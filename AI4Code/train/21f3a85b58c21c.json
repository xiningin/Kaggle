{"cell_type":{"0ca7cf05":"code","70fc215b":"code","2ca8dd60":"code","24081b6e":"code","758a9ef8":"code","f5a38b72":"code","601b8766":"code","347264cb":"code","ed9b6786":"code","e66b0676":"code","ebb1492f":"code","b48247b4":"code","22c7823d":"code","bca36966":"code","65f01d6a":"code","7aac0eca":"code","7d7021e2":"code","e679d849":"code","20754f55":"code","6ee73ae5":"code","a5dbcb7c":"code","3b9c80ce":"code","9a639dbd":"code","84ed2531":"code","1a2f6eb7":"code","aaa34fcf":"code","8b14ff55":"code","0046d917":"code","e2ca0da3":"code","80bd7815":"code","61a1722e":"code","b5c7f5d0":"code","9de2ab97":"code","2d81afb4":"code","3dca051e":"code","6efed357":"code","40698b8a":"code","bd6ebc11":"code","a5d8fa4f":"code","7df4a6ac":"code","939b2817":"code","620fbdca":"code","933cdc56":"code","4f0bd259":"code","f9075055":"code","7bb2b525":"code","fa997383":"code","fffeb3a7":"code","e7c0bc3d":"code","3640076a":"code","e4823696":"code","12746a90":"code","25f8afbc":"code","6c355ea9":"code","6c276d02":"code","06034614":"code","d37d0752":"code","350cf88d":"code","1748c0d3":"code","90165e45":"code","520faed1":"code","5fc6c4ea":"code","a6221a62":"code","6e1d0360":"markdown","78ad87cd":"markdown","8ef65452":"markdown","27f3b4d0":"markdown","5f6ff2ff":"markdown","20278d5c":"markdown","386a5baf":"markdown","6e7b1aa8":"markdown","b21d4ea7":"markdown","eccbc159":"markdown","5d8d9d55":"markdown","a68a98aa":"markdown","5a1fddc2":"markdown","cba649ec":"markdown","8ae30052":"markdown","7047b402":"markdown","1621a1d7":"markdown","a022da78":"markdown","b34a5b8f":"markdown","8085d123":"markdown","556e8a77":"markdown"},"source":{"0ca7cf05":"import pandas as pd\nimport numpy as np\nfrom scipy import fft\nfrom scipy.signal import spectrogram\nfrom scipy import signal\nfrom os import path","70fc215b":"try:\n    from gwpy.timeseries import TimeSeries\n    from gwpy.plot import Plot\n    \nexcept:\n    !python -m pip install gwpy\n    !pip install astropy==4.2.1\n    from gwpy.timeseries import TimeSeries\n    from gwpy.plot import Plot","2ca8dd60":"from pandarallel import pandarallel\npandarallel.initialize()","24081b6e":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly \nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots","758a9ef8":"from sklearn.preprocessing import MinMaxScaler\nfrom PIL import Image","f5a38b72":"try:\n    import efficientnet.keras as efn\nexcept:\n    \n    !pip install -U efficientnet\n    import efficientnet.keras as efn\n\n\nimport tensorflow as tf \nimport glob\nfrom sklearn.model_selection import train_test_split","601b8766":"sns.set_theme(\"talk\")\nsns.set_style(\"white\")","347264cb":"train_file =\"..\/input\/g2net-gravitational-wave-detection\/training_labels.csv\"\nsubmission_file =\"..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv\"","ed9b6786":"master_df = pd.read_csv( train_file)\nmaster_df.head()","e66b0676":"master_df[\"path\"]= master_df[\"id\"].parallel_apply( lambda x : \"..\/input\/g2net-gravitational-wave-detection\/train\/\" +x[0] +\"\/\"+x[1] +\"\/\"+x[2] +\"\/\" + x+\".npy\")","ebb1492f":"master_df.head()","b48247b4":"print (\" Number of Signal samples present ={}\".format( master_df.shape[0] ) )","22c7823d":"file_exist = False\n\nfor i in range( 0, 20 ):\n    file = master_df[\"path\"].iloc[ np.random.randint( master_df.shape[0] ) ]\n    if not path.exists( file ): \n        print ( \"File does not exist, constructed path is wrong for file = {}\".format( file ) )\n        file_exist = True\nif not file_exist : print (\"All 20 file are present \")","bca36966":"fig = plt.figure( figsize = (5,5), dpi = 90 )\nsns.countplot( master_df[\"target\"] )\nplt.show()","65f01d6a":"sample_file = master_df[\"path\"].iloc[0]\ntarget= master_df[\"target\"].iloc[0]\ndata = np.load( sample_file  )\ndata.shape","7aac0eca":"## Data is stored in 2 dimension\nfor i in range( 0, 3 ):\n    file,target  = master_df[[\"path\",\"target\"]].iloc[np.random.randint(master_df.shape[0] ) ].values\n    data = np.load( file )\n    fig = go.Figure()\n    fig.add_trace( go.Line( y = data[0], name =\"LIGO Hanford\" ) )\n    fig.add_trace( go.Line( y = data[1], name =\"LIGO Livingston\" ) )\n    fig.add_trace( go.Line( y = data[2], name =\"LIGO Virgo\" ) )\n    fig.update_layout(title =\"Gravitation wave in Time domain from 3 Detectors and target={}\".format( target ) )\n    fig.show()","7d7021e2":"file_name, target= master_df[[\"path\",\"target\"]].iloc[np.random.randint( master_df.shape[0])].values\ndata = np.load( file_name)\n    \nfig = go.Figure()\nfig.add_trace( go.Histogram( x = data[0], autobinx= True, name = \"LIGO Hanford target={}\".format( target ) ))\nfig.add_trace( go.Histogram( x = data[0], autobinx= True,name = \"LIGO Livingston target={}\".format( target ) ))\nfig.add_trace( go.Histogram( x = data[0], autobinx= True,name = \"LIGO Virgo target={}\".format( target ) ))\nfig.show()","e679d849":"file_name, target= master_df[[\"path\",\"target\"]].iloc[np.random.randint( master_df.shape[0])].values\ndata = np.load( file_name)\ndata = np.multiply( data, 10**20 )    \nfig = go.Figure()\nfig.add_trace( go.Histogram( x = data[0], autobinx= True, name = \"LIGO Hanford target={}\".format( target ) ))\nfig.add_trace( go.Histogram( x = data[0], autobinx= True,name = \"LIGO Livingston target={}\".format( target ) ))\nfig.add_trace( go.Histogram( x = data[0], autobinx= True,name = \"LIGO Virgo target={}\".format( target ) ))\nfig.show()","20754f55":"fig = make_subplots( rows = 1, cols = 2 )\nfor i in range( 0, 2):\n    file_name, target= master_df[[\"path\",\"target\"]].iloc[np.random.randint( master_df.shape[0])].values\n    data = np.load( file_name)\n    data = np.multiply( data, 10**20 )    \n    fig.add_trace( go.Line( x = fft.rfftfreq( 2* 2048, (1\/ 4096)) , y = 20*np.log(np.abs( fft.rfft( data[0]) )),name =\"LIGO Hanford target={}\".format( target )), row = 1, col = i+1 )\n    fig.add_trace( go.Line( x = fft.rfftfreq( 2* 2048, (1\/ 4096)) , y = 20*np.log(np.abs( fft.rfft( data[1]) )),name =\"LIGO Livingston target={}\".format( target )), row = 1, col = i+1 )\n    fig.add_trace( go.Line( x = fft.rfftfreq( 2* 2048, (1\/ 4096)) , y = 20*np.log(np.abs( fft.rfft( data[2]) )),name =\"LIGO Virgo target={}\".format( target ) ) , row = 1, col = i+1)\n\nfig.update_layout( title =\"Gravitational wave inFreq Domain \")\nfig.update_xaxes( title =\"Freq in Hz\")\nfig.update_yaxes( title =\"Signal Power in dB\")\nfig.show()\n","6ee73ae5":"file_name, target= master_df[[\"path\",\"target\"]].iloc[np.random.randint( master_df.shape[0])].values\ndata = np.load( file_name)","a5dbcb7c":"f,t,mag = spectrogram( np.multiply( data[0], 10**20) , fs = 2048, window = \"hamming\", noverlap= 100, mode =\"magnitude\")\nfig = go.Figure()\nfig.add_trace( go.Heatmap( x= t, y = f, z = np.abs(mag) ))\nfig.show()","3b9c80ce":"\n## Fucntion converts data to GW wave format\ndef read_file(fname):\n    data = np.load(fname)\n    d1 = TimeSeries(data[0,:], sample_rate=2048)\n    d2 = TimeSeries(data[1,:], sample_rate=2048)\n    d3 = TimeSeries(data[2,:], sample_rate=2048)\n    return d1, d2, d3\n\ndef plot_time_data(d1, d2, d3):\n    plot = Plot(d1, d2, d3, separate=True, sharex=True, figsize=[12, 8])\n    ax = plot.gca()\n    #ax.set_title([\"1\",\"2\",\"3\"])\n    ax.set_xlim(0,2)\n    ax.set_xlabel('Time [s]')\n    plt.tight_layout()\n    plot.show( )","9a639dbd":"d1, d2, d3 = read_file('..\/input\/g2net-gravitational-wave-detection\/train\/0\/0\/0\/000a5b6e5c.npy')\nplot_time_data(d1, d2, d3)\n","84ed2531":"#applying window function, i.e creating Band pass filter \n\nwindow = signal.tukey( 4096 )\nplt.plot( window)\n\n","1a2f6eb7":"d1, d2, d3 = d1*window, d2*window, d3*window\nplot_time_data(d1, d2, d3)","aaa34fcf":"fig2b = d1.highpass(15).asd(fftlength=2).plot(figsize=[12, 6])\nplt.xlim(10,1024)\nplt.ylim(1e-25, 1e-20);\n","8b14ff55":"white_data = d1.whiten(window=(\"tukey\",0.2)) # whiten-function has a built-in window function\nbp_data = white_data.bandpass(35, 350) # frequency range 35-350Hz\nfig3 = bp_data.plot(figsize=[12, 6])\nplt.xlim(0, 2)\nax = plt.gca()\nax.set_title('Whitened and bandpassed')\nax.set_xlabel('Time [s]')\nax.set_ylabel( \"amplitude \")\n","0046d917":"\n#lf & hf are cut of freq for Band pass filter\ndef preprocess(d1, d2, d3, bandpass=False, lf=20, hf=350):\n    white_d1 = d1.whiten(window=(\"tukey\",0.2))\n    white_d2 = d2.whiten(window=(\"tukey\",0.2))\n    white_d3 = d3.whiten(window=(\"tukey\",0.2))\n    if bandpass: # bandpass filter\n        bp_d1 = white_d1.bandpass(lf, hf) \n        bp_d2 = white_d2.bandpass(lf, hf)\n        bp_d3 = white_d3.bandpass(lf, hf)\n        return bp_d1, bp_d2, bp_d3\n    else: # only whiten\n        return white_d1, white_d2, white_d3","e2ca0da3":"r1, r2, r3 = read_file('..\/input\/g2net-gravitational-wave-detection\/train\/0\/0\/0\/000a5b6e5c.npy') # this signal has target=1\np1, p2, p3 = preprocess(r1, r2, r3)\nhq = p2.q_transform(qrange=(16,32), frange=(30,400), logf=True, whiten=False)\nfig4 = hq.plot(figsize=[12, 10])\nax = fig4.gca()\nfig4.colorbar(label=\"Normalised energy\")\nax.grid(False)\nax.set_yscale('log')\nax.set_xlabel('Time [s]');","80bd7815":"print ( \"Shape of Transoformationof single detector is ={}\".format(hq.shape) )","61a1722e":"Q_RANGE = (16,32)\nF_RANGE = (30,400)\n\ndef create_rgb(fname):\n    r1, r2, r3 = read_file(fname)\n    p1, p2, p3 = preprocess(r1, r2, r3)\n    hq1 = p1.q_transform(qrange=Q_RANGE, frange=F_RANGE, logf=True, whiten=False)\n    hq2 = p2.q_transform(qrange=Q_RANGE, frange=F_RANGE, logf=True, whiten=False)\n    hq3 = p3.q_transform(qrange=Q_RANGE, frange=F_RANGE, logf=True, whiten=False)\n    img = np.zeros([hq1.shape[0], hq1.shape[1], 3], dtype=np.uint8)\n    scaler = MinMaxScaler()\n    img[:,:,0] = 255*scaler.fit_transform(hq1)\n    img[:,:,1] = 255*scaler.fit_transform(hq2)\n    img[:,:,2] = 255*scaler.fit_transform(hq3)\n    return Image.fromarray(img).rotate(90, expand=1).resize((760,760))\n","b5c7f5d0":"create_rgb('..\/input\/g2net-gravitational-wave-detection\/train\/0\/0\/0\/000a5b6e5c.npy')","9de2ab97":"create_rgb( \"..\/input\/g2net-gravitational-wave-detection\/test\/1\/0\/b\/10b041376b.npy\")","2d81afb4":"\n\n## Fucntion converts data to GW wave format\ndef read_file(fname):\n    data = np.load(fname)\n    d1 = TimeSeries(data[0,:], sample_rate=2048)\n    d2 = TimeSeries(data[1,:], sample_rate=2048)\n    d3 = TimeSeries(data[2,:], sample_rate=2048)\n    return d1, d2, d3\n\n\n\ndef convert_data_3channel_qtran( file_name ,Q_range_low = 16, q_range_high = 32, f_range_ll = 10, f_range_high = 400  ):\n    r1, r2, r3 = read_file(file_name)\n    p1, p2, p3 = preprocess(r1, r2, r3)\n    hq1 = p1.q_transform(qrange=(Q_range_low, q_range_high), frange=( f_range_ll, f_range_high), logf=True, whiten=False)\n    hq2 = p2.q_transform(qrange=(Q_range_low, q_range_high), frange=( f_range_ll, f_range_high), logf=True, whiten=False)\n    hq3 = p3.q_transform(qrange=(Q_range_low, q_range_high), frange=( f_range_ll, f_range_high), logf=True, whiten=False)\n    #img = np.zeros([hq1.shape[0], hq1.shape[1], 3], dtype=np.float32)\n    scaler = MinMaxScaler()\n    hq1 = 255*scaler.fit_transform(hq1)\n    hq2 = 255*scaler.fit_transform(hq2)\n    hq3=  255*scaler.fit_transform(hq3)\n    img = np.dstack( ( hq1, hq2, hq3 ) )\n    return img #Image.fromarray(img).rotate(90, expand=1).resize((256,256))\n\nif False:\n    \n    for each_file in glob.glob( \"..\/input\/g2net-gravitational-wave-detection\/train\/*\/*\/*\/*.npy\")[:10]:\n\n        convert_data_3channel_qtran(each_file).save(\".\/\"+each_file.split(\"\/\")[-1].replace(\".npy\",\".png\"))\n\n        print( each_file)\n    \n    ","3dca051e":"CFG= {\"IMG_LENGTH\": 300,\n         \"IMG_WIDTH\": 300,\n         \"CHANNELS\": 3,\n         \"RANDOM_STATE\": 100,\n      \"BATCH_SIZE\": 50\n     }","6efed357":"Q_range_low = 16\nq_range_high = 32\nf_range_ll = 10\nf_range_high = 400\nr1, r2, r3 = read_file(\"..\/input\/g2net-gravitational-wave-detection\/train\/0\/0\/0\/00000e74ad.npy\")\np1, p2, p3 = preprocess(r1, r2, r3)\nhq1 = p1.q_transform(qrange=(Q_range_low, q_range_high), frange=( f_range_ll, f_range_high), logf=True, whiten=False)\nhq2 = p2.q_transform(qrange=(Q_range_low, q_range_high), frange=( f_range_ll, f_range_high), logf=True, whiten=False)\nhq3 = p3.q_transform(qrange=(Q_range_low, q_range_high), frange=( f_range_ll, f_range_high), logf=True, whiten=False)\n#img = np.zeros([hq1.shape[0], hq1.shape[1], 3], dtype=np.float32)\nscaler = MinMaxScaler()\nhq1 = scaler.fit_transform(hq1)\nhq2 = scaler.fit_transform(hq2)\nhq3=  scaler.fit_transform(hq3)\nimg = np.dstack( ( hq1, hq2, hq3 ) )","40698b8a":"\ndef decode_numpy_file ( ):\n    def convert_data_3channel_qtran( file_name, target   ):\n        \n        return  tf.io.decode_jpeg( tf.io.read_file( file_name)),target \n    \n    return convert_data_3channel_qtran\n\n\ndef data_augment ():\n    \n    def perform_aument(img, target ):\n        img = tf.image.ran\n        \n    \n    return perform_aument\n\n\ndef datagenerator(df, test = False  ):\n    \n    read_reshape_gw = decode_numpy_file()\n    #augment =  # augmentation Function still in development \n    datagen = tf.data.Dataset.from_tensor_slices( ( df[\"path_new\"].values, df[\"target\"].values ) )\n    datagen = datagen.map( read_reshape_gw, num_parallel_calls= tf.data.AUTOTUNE )\n    #datagen = datagen.map( read_reshape_gw, num_parallel_calls= tf.data.AUTOTUNE )\n    datagen = datagen.repeat() if not test else datagen\n    datagen = datagen.shuffle(1024) if not test else datagen\n    datagen = datagen.batch(CFG[\"BATCH_SIZE\"])\n    datagen = datagen.prefetch(tf.data.AUTOTUNE )\n    \n    return datagen","bd6ebc11":"master_df[\"path_new\"]= master_df[\"id\"].parallel_apply( lambda x : \"..\/input\/g2net-train-images-with-gpwy-sample\/kaggle\/tmp\/train\/\" + x+\".png\")","a5d8fa4f":"png_file_list = [ x.split(\"\/\")[-1].replace(\".png\",\"\") for x in glob.glob(\"..\/input\/g2net-train-images-with-gpwy-sample\/kaggle\/tmp\/train\/*.png\")]\nmaster_df_2 = pd.merge( master_df, pd.DataFrame({\"png_file\": png_file_list,\"status\":[1]* len( png_file_list) } ), left_on= \"id\",right_on=\"png_file\" )","7df4a6ac":"master_df_2[\"path_new\"] = master_df_2[\"id\"].parallel_apply( lambda x : \"..\/input\/g2net-train-images-with-gpwy-sample\/kaggle\/tmp\/train\/\" + x+\".png\")\ntrain_df, val_df = train_test_split( master_df_2, test_size =0.1, random_state = CFG[\"RANDOM_STATE\"],shuffle = True,stratify = master_df_2[\"target\"])","939b2817":"fig,axis = plt.subplots( nrows = 1, ncols = 2,figsize =( 15,5), sharey = True ) \nsns.countplot(  val_df[\"target\"], ax = axis[0])\nsns.countplot( train_df[\"target\"], ax = axis[1])\naxis[0].set_title(\"Number of Validation Sample with distribution \")\naxis[1].set_title(\"Number of Test Sample with distribution \")\nplt.tight_layout( )\nplt.show()","620fbdca":"def short_effnet_model():\n    #with strategy.scope():\n        \n    model_input = tf.keras.layers.Input( shape=  ( CFG[\"IMG_LENGTH\"],CFG[\"IMG_WIDTH\"], 3 ) , name= \"encoder_input_layer\" )\n\n\n    efff_net =efn.EfficientNetB0(include_top = False, \n                                   weights =\"noisy-student\" , \n                                   input_shape = ( CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"]) ,\n                                   input_tensor = model_input ,\n                                   classes=2,\n                                   pooling = True,\n                                   #classifier_activation='softmax',\n                                   drop_connect_rate= 0.7\n                                  ) \n\n    for layer in  efff_net.layers  : layer.trainable = True\n    \n    gaussian_noise = tf.keras.layers.GaussianNoise( stddev = 0.3 ) ( model_input )\n    random_crop = tf.keras.layers.experimental.preprocessing.RandomCrop( height = 30, width = 30  ) (gaussian_noise)\n    random_flip =tf.keras.layers.experimental.preprocessing.RandomFlip( mode=\"horizontal_and_vertical\") ( random_crop )\n    zoom_layer = tf.keras.layers.experimental.preprocessing.RandomZoom(  height_factor =(-0.3, -0.2)  , width_factor=(-0.3, -0.2), fill_mode='reflect', interpolation='bilinear', fill_value=0.0 ) ( random_flip)\n    random_contrast = tf.keras.layers.experimental.preprocessing.RandomContrast( factor =[0.2, 0.8 ]  ) ( zoom_layer )\n    \n    efff_net.layers[0] ( random_contrast )\n    layer_00 = efff_net.layers[-1].output\n    layer_01 = tf.keras.layers.Flatten()( layer_00 )\n    layer_02 = tf.keras.layers.Dense( 1, activation =\"sigmoid\") ( layer_01)\n    model_short = tf.keras.Model( inputs = model_input, outputs = layer_02 )\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate= 0.00126000004\/4 ) \n    model_short.compile( optimizer= optimizer,loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n                 metrics=[tf.keras.metrics.AUC() ])#AUC(curve='ROC')\n    \n    return model_short","933cdc56":"model_1 = short_effnet_model()\nmodel_1.summary()","4f0bd259":"train_df.shape[0]\/10000","f9075055":"if False:\n    \n    CFG[\"BATCH_SIZE\"]  = 50\n    train_df_2 = train_df.head(50000)\n    val_df_2 = val_df.head( 50000 )\n    train_data_gen = datagenerator( train_df_2 )\n    val_data_gen = datagenerator( val_df_2 )\n\n    model_effnet = short_effnet_model()\n    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(  patience=2,\n                                                        min_lr= 0.000001,\n                                                        monitor='val_loss', \n                                                        factor=0.45, \n                                                        verbose=1,\n                                                        min_delta = 0.2,\n                                                        cooldown=2,\n                                                        mode='auto', \n                                                       )\n\n\n\n\n\n\n\n    CFG[\"TRAIN_STEPS\"] = int ( train_df_2.shape[0] \/CFG[\"BATCH_SIZE\"] ) + (1 if train_df_2.shape[0] % CFG[\"BATCH_SIZE\"] != 0 else 0)\n    CFG[\"VAL_STEPS\"] = int ( val_df_2.shape[0]\/CFG[\"BATCH_SIZE\"] ) + (1 if val_df_2.shape[0]% CFG[\"BATCH_SIZE\"] != 0 else 0)\n\n\n\n    #model_effnet.load_weights(\"..\/input\/seti-gpu-rev-01-model\/Efficient_Net_Model_Rev_01.h5\")\n    checkpoint = tf.keras.callbacks.ModelCheckpoint( f'model{1}.h5', save_best_only=True, monitor='val_loss', mode='min')\n\n\n    model_history = model_effnet.fit( train_data_gen ,\n                            #class_weight= class_weight ,\n                             steps_per_epoch= CFG[\"TRAIN_STEPS\"], \n                             epochs =11, \n                             validation_data= val_data_gen,\n                             validation_steps = CFG[\"VAL_STEPS\"],\n                             callbacks=[ checkpoint,lr_reducer ]\n                           )\n","7bb2b525":"CREATE_TF_RECORD = False\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    #if isinstance(value, type(tf.constant(0))):\n    #    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float \/ double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef train_serialize_example(image, img_id, target ):\n    feature = {\n      'image'         : _bytes_feature(image),\n      'image_id'      : _bytes_feature(img_id),   \n      'target'        : _int64_feature(target),\n      }\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\n\n\nif CREATE_TF_RECORD :\n    \n    ! mkdir \".\/300x300_tf_record\"\n    each_tfrec_size = 10000\n    \n    if (master_df_2.shape[0] % each_tfrec_size) != 0 :\n        end  = 1 + int(master_df_2.shape[0] \/ each_tfrec_size)\n    else:\n        end = int( master_df_2.shape[0] \/ each_tfrec_size )\n        \n        \n    for i in range( 0, end):\n        \n        \n        if ((i +1)* each_tfrec_size) > master_df_2.shape[0]:\n            \n            img_id = master_df_2[\"id\"].iloc[ i*each_tfrec_size : ].values\n            file_name = master_df_2[\"path_new\"].iloc[ i*each_tfrec_size : ].values\n            target = master_df_2[\"target\"].iloc[ i*each_tfrec_size : ].values\n            \n        else:\n            \n            img_id= master_df_2[\"id\"].iloc[ i*each_tfrec_size : each_tfrec_size *(i +1)].values\n            file_name = master_df_2[\"path_new\"].iloc[ i*each_tfrec_size : each_tfrec_size *(i +1)].values\n            target = master_df_2[\"target\"].iloc[ i*each_tfrec_size : each_tfrec_size *(i +1)].values\n        \n        print ( \"Ongoing slice {}\/{}\".format( i,end))\n        \n        with tf.io.TFRecordWriter( \".\/300x300_tf_record\/300x300_tfrecord_\" +str( i) + \".tfrec\" ) as writer:\n            \n                    for each_id, each_file, each_target  in zip( img_id,file_name, target ):\n                        image_string = open(each_file, 'rb').read()\n                        \n                        writer.write(train_serialize_example( image_string , str.encode(each_id ), each_target ))\n                        \n                    writer.close()\n                    \n        print ( \"completed slice {}\/{}\".format( i,end))\n    ","fa997383":"import os\nos.chdir(r'kaggle\/working')","fffeb3a7":"# code decode tfrecode \ndef decode_image(image_data):\n    image = tf.io.decode_png( image_data,tf.float32 )\n    return image\n\ndef prepare_target(target):    \n    target = tf.cast(target, tf.float32)            \n    target = tf.reshape(target, [1])         \n    return target\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\" : tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_id\":tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = tf.io.decode_png( example['image'])\/ 255\n    #image  = tf.reshape(image, [256, 256])\n    #image = tf.stack( (image, image, image), axis = 2)\n    target = prepare_target(example['target'])\n    return image, target # returns a dataset of (image, label) pairs\n\ndef augmanet_data(image, target ):\n    \n    #mask = random.randrange(2, 40, 2)\n   \n    #offset = random.randrange( 1, 200, 2 )\n    \n    #image =  tf.image.random_contrast( tf.image.random_flip_up_down( tf.image.random_flip_left_right( image, seed=CFG[\"RANDOM_STATE\"]  ),  seed=CFG[\"RANDOM_STATE\"] \n    #                                                           ),0.3,0.8, seed=CFG[\"RANDOM_STATE\"] )\n    \n    #image= tf.squeeze(tfa.image.random_cutout( tf.expand_dims(image,0), (10, 10) ) )\n    #image = tfa.image.cutout( tf.expand_dims(image,0),(10,10), constant_values = 0.0,offset = (2,2,2) )\n    #image = tfa.image.cutout( images= image, mask_size = (mask,mask), constant_values = 0  )#, offset =(2,2 ), constant_values = 0)\n      \n    return image , target\n    \n                                        \n\ndef load_dataset(fileids, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(fileids, num_parallel_reads=tf.data.AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord,num_parallel_calls= tf.data.AUTOTUNE)\n   # dataset = dataset.map( augmanet_data ,num_parallel_calls= tf.data.AUTOTUNE) if augment else dataset\n    \n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\n\n## Main function \ndef get_training_dataset(file_ist,repeat = True ,ordered =False  ):\n    dataset = load_dataset(file_ist, ordered = False )\n    dataset = dataset.repeat()  if repeat else  dataset # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(20, seed=CFG[\"RANDOM_STATE\"])\n    dataset = dataset.batch(CFG[\"BATCH_SIZE\"])\n    dataset = dataset.prefetch(tf.data.AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\n\ndata_gen = get_training_dataset( [\".\/300x300_tf_record\/300x300_tfrecord_0.tfrec\"],repeat = False, ordered = True  )\n","e7c0bc3d":"import os\nos.chdir(r'\/kaggle\/working') ","3640076a":"os.getcwd()","e4823696":"%cd \/kaggle\/working","12746a90":"! mkdir \"300x300_tf_record\/\"","25f8afbc":"import shutil\nfor i in glob.glob(\".\/300x300_tf_record\/*.tfrec\"):\n    shutil.move(i, \"\/kaggle\/working\/\")","6c355ea9":"ls\n","6c276d02":"file_list = \"\"\"300x300_tfrecord_18.tfrec\n300x300_tfrecord_28.tfrec\n300x300_tfrecord_0.tfrec\n300x300_tfrecord_19.tfrec\n300x300_tfrecord_3.tfrec\n300x300_tfrecord_1.tfrec\n300x300_tfrecord_2.tfrec\n300x300_tfrecord_4.tfrec\n300x300_tfrecord_10.tfrec\n300x300_tfrecord_20.tfrec\n300x300_tfrecord_5.tfrec\n300x300_tfrecord_11.tfrec\n300x300_tfrecord_21.tfrec\n300x300_tfrecord_6.tfrec\n300x300_tfrecord_12.tfrec\n300x300_tfrecord_22.tfrec\n300x300_tfrecord_7.tfrec\n300x300_tfrecord_13.tfrec\n300x300_tfrecord_23.tfrec\n300x300_tfrecord_8.tfrec\n300x300_tfrecord_14.tfrec\n300x300_tfrecord_24.tfrec\n300x300_tfrecord_9.tfrec\n300x300_tfrecord_15.tfrec\n300x300_tfrecord_25.tfrec\n300x300_tfrecord_16.tfrec\n300x300_tfrecord_26.tfrec \n300x300_tfrecord_17.tfrec\n300x300_tfrecord_27.tfrec\"\"\"","06034614":"new_file_list =  [ x for x in file_list.replace(\" \",\"\").split(\"\\n\") if x !=\"\" ] ","d37d0752":"import time ","350cf88d":"from IPython.display import publish_display_data","1748c0d3":"#publish_display_data( {\"a\":new_file_list[0] } )","90165e45":"#FileLink(r\"300x300_tfrecord_17.tfrec\")","520faed1":"help( publish_display_data \n    )","5fc6c4ea":"from IPython.display import FileLink\n    for i in new_file_list: \n        FileLink( i )\n","a6221a62":"#FileLink( new_file_list[9] )","6e1d0360":"# Code for TF record ","78ad87cd":"# Good Note book for reference\n#    1.    https:\/\/www.kaggle.com\/mistag\/data-preprocessing-with-gwpy \n# Special thanks to Darek, he made data conversion from numpy to Qtransformed png file \"https:\/\/www.kaggle.com\/thedrcat\/g2net-train-images-with-gpwy-sample\/\"\n# I thought I could levrage that for now, to create base line \n\n# Adding Note book output to dataset\n# https:\/\/www.kaggle.com\/getting-started\/168312","8ef65452":"# Frequency domaain data after passing through High Pass filter\n","27f3b4d0":"# Visualize distribution of traget","5f6ff2ff":"# Measured amplitude is very small range lets visualize distrbution of values using histogram ","20278d5c":"# Check path constructed correct or not ","386a5baf":"# Did not good, but decent amount of EDA lets start with building model and data pipeline","6e7b1aa8":"# Check number of Singals present in dataframe","b21d4ea7":"# Combine three channels into one RGB image\n# Since we have 3 detectors, we can combine the Q-Transforms as RGB channels into one color image. Let's make a function for that:","eccbc159":"# Train data might not give good View, let us vissualize test data","5d8d9d55":"# Let us see how datais stored\n1. Format\n2. Number of channels","a68a98aa":"## Time domain will provide only little info.\n## Frequency domain should give more detils on singal strenght and other frequency componenets.\n## Convert amplitude to db using formula 20log(data)","5a1fddc2":"# Spectral whitening and bandpass filtering\n\n# This is super simple with GWpy:\n","cba649ec":"# Good part is data is not imbalanced","8ae30052":"# Data after applying Band pass filter","7047b402":"# let usscale values by 10^-20 and visualize the distribution ","1621a1d7":"\n# Q-Transform\n\n# The Q-Transform is related to the Fourier transform, and very closely related to a wavelet transform. The spectrogram is a possible candidate as input for a CNN model.\n","a022da78":"#  Now, we have a preprocessed data that is ready for further analysis. First, let's define a function that combines all the steps above and outputs preprocessed data:\n","b34a5b8f":"# Constructing complete path, it will be very helpfull for further analysis","8085d123":"# I came across one note book very informative, \"https:\/\/www.kaggle.com\/mistag\/data-preprocessing-with-gwpy\"\n# Talks about converting giventime series data to Gwavitational wave using gwpy library.\n# Taken implementation and code from same note book.\n","556e8a77":"## we can't analyze time domain singal or Frequency domain signal using Machine learning model.\n## Let us convert singal to SFT ( Short fourier Transform ) and then to Constant Q Transform"}}