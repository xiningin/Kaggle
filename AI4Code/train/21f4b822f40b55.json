{"cell_type":{"ac87c662":"code","272548c4":"code","7aee79ed":"code","f65c7ca6":"code","9a036e71":"code","a31aa2bf":"code","9760655c":"code","3a3cabb6":"code","73395116":"code","2141dc4a":"code","939678c5":"code","98cb928a":"code","21906d23":"markdown","bf5eb704":"markdown","6a2997c4":"markdown","9436ad28":"markdown","e4e32fdd":"markdown","8f504abd":"markdown"},"source":{"ac87c662":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","272548c4":"from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.layers import LSTM, Flatten, Dense, Embedding, GRU, Bidirectional, GlobalMaxPool1D, Dropout, Conv1D, MaxPooling1D \nfrom tensorflow.keras.models import Sequential\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","7aee79ed":"max_words = 10000  # number of words to consider as features\nmaxlen = 600  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint('Loading data...')\n(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_words)\nprint(len(input_train), 'train sequences')\nprint(len(input_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\nprint('input_train shape:', input_train.shape)\nprint('input_test shape:', input_test.shape)\n\n","f65c7ca6":"model = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(Bidirectional(LSTM(32, dropout=0.5, return_sequences = True)))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n","9a036e71":"history = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=100,\n                    validation_split=0.2)","a31aa2bf":"plt.figure(figsize=(12, 5), dpi=80)\n\nplt.subplot(1,2,1)\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.grid()\n\nplt.subplot(1,2,2)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.grid()\nplt.show()","9760655c":"model = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(LSTM(32, dropout=0.5, return_sequences=True))\nmodel.add(LSTM(64))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.3)","3a3cabb6":"plt.figure(figsize=(12, 5), dpi=80)\n\nplt.subplot(1,2,1)\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.grid()\n\nplt.subplot(1,2,2)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.grid()\nplt.show()","73395116":"model = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(Conv1D(2, 10, activation='relu', input_shape=(None, 128)))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(32, 10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.3)","2141dc4a":"plt.figure(figsize=(12, 5), dpi=80)\n\nplt.subplot(1,2,1)\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.grid()\n\nplt.subplot(1,2,2)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.grid()\nplt.show()","939678c5":"model = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(GRU(32, return_sequences=True))\nmodel.add(GRU(64))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.3)","98cb928a":"plt.figure(figsize=(12, 5), dpi=80)\n\nplt.subplot(1,2,1)\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.grid()\n\nplt.subplot(1,2,2)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.grid()\nplt.show()","21906d23":"## \u041c\u043e\u0434\u0435\u043b\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438: LSTM","bf5eb704":"## \u041c\u043e\u0434\u0435\u043b\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438: \u041e\u0434\u043d\u043e\u043c\u0435\u0440\u043d\u0430\u044f \u0441\u0432\u0435\u0442\u043e\u0447\u043d\u0430\u044f \u0441\u0435\u0442\u044c","6a2997c4":"## \u041c\u043e\u0434\u0435\u043b\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438: Bidirectional (LSTM)","9436ad28":"# \u0414\u043e\u043c\u0430\u0448\u043d\u044f\u044f \u0440\u0430\u0431\u043e\u0442\u0430 \u21166\n## \u0412\u044b\u043f\u043e\u043b\u043d\u0438\u043b: \u041b\u043e\u0431\u0430\u043d\u043e\u0432 \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440","e4e32fdd":"## \u041c\u043e\u0434\u0435\u043b\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438: GRU","8f504abd":"# \u0412\u044b\u0432\u043e\u0434:\n### \u0412 \u0445\u043e\u0434\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u21166 \u043c\u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c 4 \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438: LSTM, GRU, Bidirectional (LSTM) \u0438 \u043e\u0434\u043d\u043e\u043c\u0435\u0440\u043d\u0443\u044e \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u0441\u0435\u0442\u044c. \u041d\u0430 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 IMDB \u043b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0445 \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0430 \u0441\u0435\u0431\u044f \u043c\u043e\u0434\u0435\u043b\u044c Bidirectional (LSTM). \u041e\u043d\u0430 \u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438:  0.8910."}}