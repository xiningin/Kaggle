{"cell_type":{"b89f1264":"code","d440bf4a":"code","86578841":"code","5628acc6":"code","a0d2e3e7":"code","c422b7b8":"code","239c122d":"code","fd83a902":"code","63378049":"code","9b86d196":"code","ca96c1c2":"code","839c2c79":"code","af00da6b":"code","50e9a346":"code","d6dbe269":"code","7c56bd13":"code","9f1a8e31":"code","cfc44a98":"code","7136d55b":"code","18394733":"code","040a3a2d":"code","f389e848":"code","ba6adb74":"code","a8f25c58":"code","f08575e2":"code","b628a6cf":"code","2ca1d2e2":"code","e74b61a8":"code","e1c4eabc":"code","7d7a59cd":"code","f867df88":"code","75cdeea2":"code","4650673a":"code","f2179d42":"code","9ab5b081":"code","e844a8a7":"code","1105132a":"code","51d50e9e":"code","0776f945":"code","c5d8153b":"code","919c4473":"code","ffe0f0c9":"code","24cc0d25":"markdown","86e3abd0":"markdown","d0893305":"markdown","ab68b9ba":"markdown","0feb8ce2":"markdown","e39ec2fb":"markdown","c13f8033":"markdown","5cd138da":"markdown","e625157e":"markdown","2ee16bb4":"markdown","d60a03e7":"markdown"},"source":{"b89f1264":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport imblearn\nfrom sklearn.model_selection import GridSearchCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d440bf4a":"data=pd.read_csv(r'''..\/input\/creditcard.csv''')\ndata\n","86578841":"data.describe()\ndata.isnull().any().max()\n#no null values in our dataset","5628acc6":"print('No Frauds', round(len(data[data[\"Class\"]==0])\/len(data)*100, 2))\nprint('Frauds', round(len(data[data[\"Class\"]==1])\/len(data)*100, 2))","a0d2e3e7":"#case of a classic imbalanced dataset, a classifier always predicting 0 wil also predict with an accuracy of 99.83%\nsns.countplot('Class', data=data)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\nplt.show()","c422b7b8":"#next up, we are going to scale the values of time and amount, because all the other columns V1: V27 are scaled\nfrom sklearn.preprocessing import RobustScaler\n#RobustScaler are less prone to outliers\nrcf= RobustScaler()\ndata[\"scaled_amount\"]= rcf.fit_transform(data[\"Amount\"].values.reshape(-1,1))\ndata[\"scaled_time\"]= rcf.fit_transform(data[\"Time\"].values.reshape(-1,1))\ndata= data.drop([\"Time\", \"Amount\"] ,axis =1 )\ndata","239c122d":"scaled_amount = data['scaled_amount']\nscaled_time = data['scaled_time']\n\ndata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndata.insert(0, 'scaled_amount', scaled_amount)\ndata.insert(1, 'scaled_time', scaled_time)\nlabels = data.columns","fd83a902":"X= data.drop('Class', axis=1)\ny=data[\"Class\"]","63378049":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state= 0)\n","9b86d196":"from imblearn.over_sampling import SMOTE\ndef sampling_func(X, y):\n    smote= SMOTE( ratio= 'minority')\n    x_sm, y_sm= smote.fit_sample(X, y)\n    return x_sm, y_sm\n    \n","ca96c1c2":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","839c2c79":"X_sampled, y_sampled = sampling_func(X_train, y_train)\nplot_2d_space(X_sampled, y_sampled, 'SMOTE oversampled data')\n","af00da6b":"X_sampled= pd.DataFrame(X_sampled)\ny_Sampled= pd.DataFrame(y_sampled)\ndf= pd.concat([X_sampled, y_Sampled], axis= 1)\ndf.columns\ndf.columns= data.columns\ndf","50e9a346":"colors = [\"#0101DF\", \"#DF0101\"]\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","d6dbe269":"f, ax2 = plt.subplots(1, 1, figsize=(24,20))\nsampled_corr = df.corr()\nsns.heatmap(sampled_corr, cmap='coolwarm_r', annot_kws={'size':20})\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","7c56bd13":"\nf, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=df, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=df, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=df, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=df, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.show()","9f1a8e31":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V11\", data=df, palette=colors, ax=axes[0])\naxes[0].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=df, palette=colors, ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=df, palette=colors, ax=axes[2])\naxes[2].set_title('V2 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=df, palette=colors, ax=axes[3])\naxes[3].set_title('V19 vs Class Positive Correlation')\n\nplt.show()","cfc44a98":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = df['V14'].loc[df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.92\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V14 outliers:{}'.format(outliers))\n\ndf = df.drop(df[(df['V14'] > v14_upper) & (df['V14'] < v14_lower)].index)\n\n","7136d55b":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = df['V14'].loc[df['Class'] == 0].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 5.4\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V14 outliers:{}'.format(outliers))\n\ndf = df.drop(df[(df['V14'] > v14_upper) & (df['V14'] < v14_lower)].index)\n\n","18394733":"# # -----> V11 Removing Outliers (Highest Positive Correlated with Labels)\n# Removing outliers i genuine cases\nv11_fraud = df['V11'].loc[df['Class'] == 0].values\nq25, q75 = np.percentile(v11_fraud, 25), np.percentile(v11_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv11_iqr = q75 - q25\nprint('iqr: {}'.format(v11_iqr))\n\nv11_cut_off = v11_iqr * 2\nv11_lower, v11_upper = q25 - v11_cut_off, q75 + v11_cut_off\nprint('Cut Off: {}'.format(v11_cut_off))\nprint('V11 Lower: {}'.format(v11_lower))\nprint('V11 Upper: {}'.format(v11_upper))\n\noutliers = [x for x in v11_fraud if x < v11_lower or x > v11_upper]\nprint('Feature V11 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V11 outliers:{}'.format(outliers))\n\ndf = df.drop(df[(df['V11'] > v11_upper) & (df['V11'] < v11_lower)].index)\n","040a3a2d":"# # -----> V11 Removing Outliers (Highest Postive Correlated with Labels)\n# Removing outliers in genuine target cases\nv11_fraud = df['V11'].loc[df['Class'] == 1].values\nq25, q75 = np.percentile(v11_fraud, 25), np.percentile(v11_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv11_iqr = q75 - q25\nprint('iqr: {}'.format(v11_iqr))\n\nv11_cut_off = v11_iqr * 2.35\nv11_lower, v11_upper = q25 - v11_cut_off, q75 + v11_cut_off\nprint('Cut Off: {}'.format(v11_cut_off))\nprint('V11 Lower: {}'.format(v11_lower))\nprint('V11 Upper: {}'.format(v11_upper))\n\noutliers = [x for x in v11_fraud if x < v11_lower or x > v11_upper]\nprint('Feature V11 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V11 outliers:{}'.format(outliers))\n\ndf = df.drop(df[(df['V11'] > v11_upper) & (df['V11'] < v11_lower)].index)","f389e848":"X_train_final= df.drop('Class', axis=1)\ny_train_final=df[\"Class\"] \n","ba6adb74":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit_transform(X_train_final)","a8f25c58":"explained_variance=pca.explained_variance_ratio_\nexplained_variance","f08575e2":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(10, 10))\n\n    plt.bar(range(30), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","b628a6cf":"pca=PCA(n_components=5)\nX_new=pca.fit_transform(X_train_final)\nX_train_pca= pd.DataFrame(X_new)\nX_train_pca","2ca1d2e2":"X_test_pca= pca.transform(X_test)\nX_test_pca= pd.DataFrame(X_test_pca)\nX_test_pca","e74b61a8":"#from sklearn.ensemble import RandomForestClassifier\n#parameters = { \n#    'n_estimators': [200, 500],\n#    'max_features': ['auto', 'sqrt', 'log2'],\n#    'max_depth' : [4,5,6,7,8],\n#   'criterion' :['gini', 'entropy']\n#}\n#classifier= RandomForestClassifier()\n#grid_search= GridSearchCV(estimator=classifier, param_grid=parameters, cv= 5, n_jobs= -1)","e1c4eabc":"#using Logistic Regression to classify the tweets \n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nparameters = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_search = GridSearchCV(estimator= classifier,param_grid= parameters, cv=5,  n_jobs= -1)","7d7a59cd":"\n#from sklearn import svm, datasets\n#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n#svc = svm.SVC()\n#grid_search = GridSearchCV(estimator= svc, param_grid= parameters, cv=5, n_jobs= -1)","f867df88":"\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n\n\n\n    ","75cdeea2":"grid_search.fit(X_train_pca, y_train_final)\n\n\n    ","4650673a":"y_pred = grid_search.predict(X_test_pca)","f2179d42":"cm= confusion_matrix(y_test, y_pred)\nlabels = ['Not relevant', 'Relevant']\nprint(classification_report(y_test, y_pred, target_names=labels))","9ab5b081":"cm","e844a8a7":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n","1105132a":"#creating an object of sequential class\nclassifier= Sequential()\n","51d50e9e":"n_inputs= X_train_pca.shape[1]\n\n#now we'll use methods of the object to add layers in our neural network model\nclassifier.add(Dense(output_dim = 3, init='uniform' , activation = 'relu', input_dim = n_inputs))\nclassifier.add(Dense(output_dim = 1, init='uniform' , activation = 'softmax'))\n\n","0776f945":"#compile the ANN model\nclassifier.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy']) ","c5d8153b":"#fit the ANN model\nclassifier.fit(X_train_pca, y_train_final, batch_size= 10, nb_epoch= 100)\n","919c4473":"#prediction on test set\ny_pred = classifier.predict(X_test_pca)\n       ","ffe0f0c9":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred>0.5)\ncm\nlabels = ['Not relevant', 'Relevant']\nprint(classification_report(y_test, y_pred>0.5, target_names=labels))","24cc0d25":"For an imbalanced set , the very first task after scaling the values, was to split the data into training & test set, and keeping test set untouched. Post this , we are now going to re-sample our data set by the means of SMOTE (Synthetic Minority Oversampling Technique) .The reason behind splitting our data first is because if we resample our dataset before splitting our test set might become biased towards some particular features which we used to generate samples, this would overfit the model.\n\n\n \nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors. The link provided below introduced me to a number of sampling strategies\nhttps:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets# ","86e3abd0":"The following inferences can be made out out of the matrix: \n\n1. **Negative Correlations**: V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction. \n\n2. **Positive Correlations**:  V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. ","d0893305":"Now, we know that these features are instrumental to predicting a Fraud transaction. We'll use boxplot to have a better understanding of the distribution of these features ","ab68b9ba":"The measure of imbalance can be seen when we plot  a percentage ratio of the target data","0feb8ce2":"The  seaborn countplot helps us plot the dataset in terms of the proportion of Fraud and genuine transactions. Post sampling, we can see that our dataset is now equally distributed.","e39ec2fb":"**Removing outliers**\n\n\nV14 has  highest  **negative co-relation** when it comes to predicting the target variable. ","c13f8033":"Now , V4 has the highest **positive co-relation** in the dataset to predict the  class.  We will be performing the same steps as in the previous case. But V4 seems to have no outliers wehn observing the box-plot. So, we will be using V11 which has the next best correlation  to find and remove the outliers from the dataset. ","5cd138da":"****This is a work on the credit card Fraud detection dataset. The case is of an imbalanced dataset where the no. of Frauds is obviously very less compared to the genuine transactions. Our motive  is to improve the precision\/ recall scores.\nA high recall score can be obtained by fine tuning the model which'll see in the later parts but precision improvement is challenging task because Precision =( *TP\/ FP + TP*  )FP can be very high because a large no. of Genuine transactions can be labeled as Fraud.  Recall ( *TP\/TP +FN*)  .It is desirable to have high recall scores so that FN (Frauds labeled as genuine) are minimized. \n","e625157e":"Post sampling, we'll do some more data cleaning and visualization. Lets plot a correlation heatmap to see the features which have a strong influence on the target variable of our dataset. We'll be using the subsampled space ","2ee16bb4":"\n**Anomaly Detection:**\n\nOur main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models. \n\n![image.png](attachment:image.png)]\n\n\n\n","d60a03e7":"Nearly 75% of the variance is expressed by thefirst 2 components. We'll be using 5 components because they sufficiently express the variance while creating a  dimensionality reduction and post the 6th components, contribution is less than 1-2%"}}