{"cell_type":{"eee0195f":"code","4deb39a1":"code","8834e055":"code","91bf9fbb":"code","dea21f17":"code","5dda72da":"code","243d9737":"code","821546cd":"code","b8048bd6":"code","dffdf136":"code","21721831":"code","01f9ea28":"code","90761dd0":"code","bf208cea":"code","391da32d":"code","a7ce2c9c":"code","0576c996":"code","59caf80b":"code","178b30ad":"code","f455b920":"markdown","54d88915":"markdown","cef878ec":"markdown","f6667d9b":"markdown","ce31998d":"markdown","fdeff4a0":"markdown","7fa4f331":"markdown","0900761a":"markdown","05dcece0":"markdown","0f3c217c":"markdown","b5c0c46d":"markdown","cfc563f2":"markdown","9b85776b":"markdown","4b4e73db":"markdown","d0fc7768":"markdown","d582c99a":"markdown","fc9751b7":"markdown"},"source":{"eee0195f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4deb39a1":"# Prepare to data\ndata = pd.read_csv(\"..\/input\/breast-cancer.csv\")\ndata.head()","8834e055":"data.info()","91bf9fbb":"# Let's wipe some columns that we won't use\ndata.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)  #axis=1 t\u00fcm s\u00fctunu siler\ndata.head()","dea21f17":"data.describe()","5dda72da":"# Let's take the some columns we'll use for show data means\ndata_mean= data[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean',\n                 'smoothness_mean','compactness_mean','concavity_mean','concave points_mean',\n                 'symmetry_mean','fractal_dimension_mean']]","243d9737":"color_list = ['cyan' if i=='M' else 'orange' for i in data_mean.loc[:,'diagnosis']]\npd.plotting.scatter_matrix(data_mean.loc[:, data_mean.columns != 'diagnosis'],\n                           c=color_list,\n                           figsize= [15,15],\n                           diagonal='hist',\n                           alpha=0.5,\n                           s = 200,\n                           marker = '*',\n                           edgecolor= \"black\")\n                                        \nplt.show()","821546cd":"# Values of 'Benign' and 'Malignant' cancer cells\nsns.countplot(x=\"diagnosis\", data=data)\ndata.loc[:,'diagnosis'].value_counts()","b8048bd6":"# Let's convert \"male\" to 1, \"female\" to 0 values\ndata.diagnosis = [ 1 if each == \"M\" else 0 for each in data.diagnosis]\ndata.info()","dffdf136":"# Let's determine the values of y and x axes\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"], axis=1)","21721831":"# Now we are doing normalization. Because if some of our columns have very high values, they will suppress other columns and do not show much.\n# Formulel : (x- min(x)) \/ (max(x) - min(x))\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values\nx.head()","01f9ea28":"# Now we reserve 80% of the values as 'train' and 20% as 'test'.\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=42)\n\n# Here we will change the location of our samples and features. '(455,30) -> (30,455)' \nx_train = x_train.T   \nx_test = x_test.T\ny_train = y_train.T   \ny_test = y_test.T\n\nprint(\"x_train :\", x_train.shape)\nprint(\"x_test :\", x_test.shape)\nprint(\"y_train :\", y_train.shape)\nprint(\"y_test :\", y_test.shape)","90761dd0":"# Now let's create the parameter and sigmoid function. Videodan nedenini yaz\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0   # It will be float\n    return w,b\n\n# Sigmoid Function\n\n# Let's calculating z\n# z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z)) # sigmoid functions finding formula\n    return y_head\nsigmoid(0)  # 0 should result in 0.5","bf208cea":"# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    \n    # forward propagation\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)  \n    cost =(np.sum(loss))\/x_train.shape[1]         # x_train.shape[1] for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1] \n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","391da32d":"# Now let's apply Updating Parameter\n\ndef update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # Updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n        # we update(learn) parameters weights and bias\n    parameters = {\"weight\":w, \"bias\":b}\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n","a7ce2c9c":"# Let's create prediction parameter\ndef predict(w,b,x_test):\n    # x_test is an input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n\n    return y_prediction\n","0576c996":"#Logistic Regression\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 455\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print train\/test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 100)","59caf80b":"# We can increase the accuracy of the test by playing with learning_rate and num_iterations\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 5, num_iterations = 150)","178b30ad":"from sklearn import linear_model\nlgrg = linear_model.LogisticRegression(random_state=42, max_iter=150)\n\nprint(\"test accuracy: {} \".format(lgrg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))","f455b920":"* Parameters are weight and bias.\n* Weights: coefficients of each pixels\n* Bias: intercept\n* z = (w.t)x + b => z equals to (transpose of weights times input x) + bias\n* In an other saying => z = b + px1w1 + px2w2 + ... + px4096*w4096\n* y_head = sigmoid(z)\n* Sigmoid function makes z between zero and one so that is probability.","54d88915":"<a id=\"2\"><\/a> <br>\n# 3) Logistic Regression with Plot\nWe are organizing the data we will use first.","cef878ec":"<a id=\"9\"><\/a> <br>\n# 4) Logistec Regression with Sklearn\nWith the Sklearn library, we can find the result you found above in a much easier way.","f6667d9b":"<a id=\"7\"><\/a> <br>\n### E) Prediction Parameter","ce31998d":"# Cihan Yatbaz\n###  02 \/ 11 \/ 2018\n\n\n\n1.  [Introduction:](#0)\n2. [Exploratory Data Analysis (EDA) :](#1)\n3. [Logistic Regression with Plot :](#2)\n    1. [Preparing Dataset :](#3)\n    2.  [Creating Parameters :](#4)\n    3. [Forward and Backward Propagation  :](#5)\n    4. [Updating Parameter :](#6)\n    5. [Prediction Parameter :](#7)\n    6. [ Logistic Regression :](#8)\n4. [Logistec Regression with Sklearn  :](#9)\n5. [CONCLUSION :](#10)","fdeff4a0":"In prediction step we have x_test as a input and while using it, we make forward prediction. ","7fa4f331":"<a id=\"6\"><\/a> <br>\n### D) Updating Parameter","0900761a":"<a id=\"0\"><\/a> <br>\n## 1) Introduction","05dcece0":"We will be working on this kernel Breast Cancer data. We'll introduce 80% of the cancer cells we have, and we will try to predict the remaining 20%. We will learn it whether they are 'benign' or 'malignant'. So let's start.","0f3c217c":"Now lets put them all together.","b5c0c46d":"<a id=\"1\"><\/a> <br>\n## 2) Exploratory Data Analysis (EDA)","cfc563f2":"<a id=\"4\"><\/a> <br>\n### B) Creating Parameters","9b85776b":"Now if our cost will be error. we have to create backward propagation. Therefor let's make a backward propagation.","4b4e73db":"<a id=\"10\"><\/a> <br>\n> # CONCLUSION                                                                                                                                                      \nThank you for your votes and comments                                                                                                                                              \n<br>**If you have any suggest, May you write for me, I will be happy to hear it.**","d0fc7768":"<a id=\"8\"><\/a> <br>\n### F) Logistic Regression","d582c99a":"<a id=\"3\"><\/a> <br>\n### A) Preparing Dataset","fc9751b7":"<a id=\"5\"><\/a> <br>\n### C) Forward and Backward Propagation\n"}}