{"cell_type":{"db6660f1":"code","787e8a9d":"code","4a47f01a":"code","bc5f8452":"code","5f8094c4":"code","34fb4e47":"code","a10167fe":"code","1b1f4e9c":"code","881acacd":"code","c6aba30b":"code","aeecc98d":"code","311327a1":"code","d2016a13":"code","48af0ae3":"code","4977cf1b":"code","a7d1f8db":"code","79e799af":"code","5d438082":"code","27078902":"code","1357d99d":"code","f172a9f9":"code","76516fdb":"code","753bea4f":"code","cbf0315c":"code","6035f463":"code","fadc2afc":"code","ff3c0eee":"code","8a91aff8":"code","739b0387":"code","a9a9c68c":"code","fd8ffbae":"code","12ad7573":"code","60318552":"code","74045b7e":"code","8543fa40":"code","10c86c11":"code","3f227700":"code","747ba33c":"code","3cd5fdc5":"code","bec1c8f6":"code","a12ac49b":"code","1d2d82da":"code","26925671":"code","987a3906":"code","51cad90b":"code","62a01890":"code","ed610c69":"code","e768ecb6":"code","6a3b2835":"code","e262179a":"code","bc560a83":"code","7d87a7bd":"code","6f31649f":"code","36b2c214":"code","8ff6cbf0":"code","ea920a9f":"code","c2309946":"code","f54109b4":"code","7ac6c54d":"code","f7aec497":"code","4a419465":"code","00bf4344":"code","09a6658a":"code","3af509c2":"code","e13190d4":"code","6a03e9ef":"code","49ac1429":"code","3ec9b45c":"code","551bcdd7":"code","c5079fe6":"code","7d75407b":"code","291affe9":"code","df26c8d3":"code","c6c02ea4":"code","d8dadd47":"code","d28f42cf":"code","44daba08":"code","eaff2208":"code","0cab4ada":"code","ce193dc1":"code","a384f467":"code","4eff85df":"code","a9cdf47b":"code","3cd93e27":"code","249be08e":"code","64e4efae":"code","f975dba2":"code","fecf0a6b":"code","beff2f59":"code","29724161":"code","1485b839":"code","326ff47f":"code","0f82dcc0":"code","f2cdcb87":"code","4956df7d":"code","e354b69f":"code","af2c1170":"code","47ebcc5e":"code","67defd59":"code","7e633d6e":"code","982fee03":"code","b1374721":"code","013aa1ad":"code","b9e8c2f9":"code","1827bee3":"code","3ddbb088":"code","70a14718":"code","ce6ea6c3":"code","ee2968d6":"code","b82928d6":"code","9899a38d":"code","6bb92f64":"code","48f9ed1e":"code","520632bc":"code","bd3ae6b4":"code","5a28be98":"code","8420db3e":"code","a652c337":"code","621e9580":"code","de664180":"code","aa84104b":"code","905df4a1":"code","022db4bf":"code","59ea10f6":"code","5aeacb86":"code","3e9b8b9f":"code","67aca5e7":"code","5c02e988":"code","4a6bfc6d":"code","6d53e4d8":"code","542dd7aa":"code","3ad033ca":"code","76732fff":"code","962cd54a":"code","df347560":"code","bac6c8b0":"code","cbda7953":"code","2e5e1165":"code","42151284":"code","6f775235":"code","56ed687f":"code","90ef2bd7":"code","bdb09458":"code","193d001f":"code","e5b6c0e0":"code","ce78ccd0":"code","ad3af847":"code","e308ac30":"code","a16b68b7":"code","09efdcc0":"code","047e1bea":"code","14ea9b49":"markdown","52610da7":"markdown","2d13102b":"markdown","e14cddc5":"markdown","3eb6551f":"markdown","8f9c93f6":"markdown","d658f520":"markdown","3506b340":"markdown","2bdd3170":"markdown","d6b2ccaf":"markdown","a013ed69":"markdown","74bcd5da":"markdown","fc4b8f0a":"markdown","4922b0a9":"markdown","142ad81a":"markdown","f9ac72cb":"markdown","c02afa54":"markdown","904af436":"markdown","000c86e9":"markdown","a7e5576b":"markdown","fa6980ce":"markdown","83562ba8":"markdown","b5d99462":"markdown","25ae1c6f":"markdown","b564238c":"markdown","e3181273":"markdown","e160bf38":"markdown","192b7892":"markdown","ece27f2f":"markdown","fc913de2":"markdown","67025765":"markdown","7a1c424d":"markdown","7ec69133":"markdown","5f4c3a7e":"markdown","eb5bce02":"markdown","d37d977e":"markdown","3ab9735b":"markdown","fb6bc6c7":"markdown","9af9fd62":"markdown","08c4f04b":"markdown","19663987":"markdown","0bd85ab0":"markdown","d75cffbf":"markdown","99e5cf95":"markdown","9c8269d6":"markdown","67c99ef6":"markdown","25e3f012":"markdown","01f9310d":"markdown","c16b5905":"markdown","2b8c08bd":"markdown","b50ec24d":"markdown","e0c31357":"markdown","ec04ac00":"markdown","b4cfd037":"markdown","8ae7dbbb":"markdown","1911dc66":"markdown","d79b35d8":"markdown","6769e608":"markdown","7e211ee0":"markdown","2ad6d0f0":"markdown","9f0607b5":"markdown","e2e0ff3f":"markdown","def84735":"markdown","10bcc0c5":"markdown","17f51f62":"markdown","5f02c0f8":"markdown","d9a2a7ce":"markdown","8c5888dc":"markdown","a745fd16":"markdown","5343ed67":"markdown","024fd030":"markdown","e199b797":"markdown","aed0d7cf":"markdown"},"source":{"db6660f1":"# standard\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npd.set_option('display.max_columns', None)","787e8a9d":"df_full = pd.read_csv('..\/input\/Speed Dating Data.csv', encoding='ISO-8859-1')\ndf_full.head()","4a47f01a":"personal = ['gender', 'age', 'field', 'field_cd', 'undergra',\n       'mn_sat', 'tuition', 'race', 'imprace', 'imprelig', 'from',\n       'zipcode', 'income', 'goal', 'date', 'go_out', 'career',\n       'career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums',\n       'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n       'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy',\n       'expnum','match_es']\ndecision = ['match','dec',\n       'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob',\n       'met']\nevaluation = ['satis_2', 'length', 'numdat_2']\noutcome = ['you_call', 'them_cal', 'date_3', 'numdat_3',\n       'num_in_3']","bc5f8452":"df = df_full[['iid', 'wave'] + personal + evaluation + outcome].drop_duplicates().copy()\ndf.head()","5f8094c4":"df['gender'] = df.gender.map({1 : 'Male', 0 : 'Female'}).fillna(df.gender)\ndf_full['gender'] = df_full.gender.map({1 : 'Male', 0 : 'Female'}).fillna(df_full.gender)\ndf.gender.value_counts(dropna=False)","34fb4e47":"ax = df.age.hist(bins=30, figsize=(12,8))\nax.set_title('Age Distribution', fontsize=15)\nax.set_xlabel('Age',fontsize=12)\nax.set_ylabel('Count', fontsize=12)\nax.grid(False)","a10167fe":"g = sns.FacetGrid(df, hue='gender', height = 6)\ng.map(plt.hist, 'age', alpha= 0.7, bins=20)\ng.set_xlabels('Age', fontsize=12)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Age distribution by gender', fontsize=15)\ng.add_legend()","1b1f4e9c":"df[['gender', 'age']].groupby('gender').agg(['mean', 'median', 'min', 'max', 'count'])","881acacd":"ax = df[['gender', 'age']].groupby('gender').mean().plot(kind='bar', figsize=(12,5), legend=False, \n                                               title='Mean Age by Gender',\n                                                   ylim=(23,27))\nax.set_xticklabels(['Female', 'Male'], fontsize=12, rotation='horizontal')\n\nax.set_xlabel('', fontsize=1)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.20, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=15)","c6aba30b":"df['race'] = df.race.map({1: 'Black', 2: 'White', 3: 'Hispanic', \n                          4: 'Asian', 6: 'Other'}).fillna(df.race)\ndf_full['race'] = df_full.race.map({1: 'Black', 2: 'White', 3: 'Hispanic', \n                          4: 'Asian', 6: 'Other'}).fillna(df_full.race)\ndf_full['race_o'] = df_full.race_o.map({1: 'Black', 2: 'White', 3: 'Hispanic', \n                          4: 'Asian', 6: 'Other'}).fillna(df_full.race_o)\ndf.race.value_counts(dropna=False)","aeecc98d":"g = sns.FacetGrid(df, col='race', height=3)\ng.map(plt.hist, 'age', bins=20)\naxes = g.axes.flatten()\naxes[0].set_title(\"Asian\")\naxes[1].set_title(\"White\")\naxes[2].set_title(\"Other\")\naxes[3].set_title(\"Hispanic\")\naxes[4].set_title(\"Black\")\ng.set_xlabels('Age', fontsize=11)\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('Age distribution by race', fontsize=13)\ng.add_legend()","311327a1":"df[['race', 'age']].groupby('race').agg(['mean', 'median', 'min', 'max', 'count'])","d2016a13":"ax = df[['race', 'age']].groupby('race').mean().plot(kind='bar', figsize=(12,5), legend=False,\n                                               title='Mean Age by Race',\n                                                   ylim=(24,28), color='rgbmy')\nax.set_xticklabels(['Asian', 'Black', 'Hispanic',  'Other', 'White'], \n                   fontsize=12, rotation='horizontal')\nax.set_xlabel('',fontsize=1)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.155, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)","48af0ae3":"g = sns.FacetGrid(df, col='race', hue='gender')\ng.map(plt.hist, 'age', alpha= 0.7, bins=10)\ng.add_legend()\naxes = g.axes.flatten()\naxes[0].set_title(\"Asian\")\naxes[1].set_title(\"White\")\naxes[2].set_title(\"Other\")\naxes[3].set_title(\"Hispanic\")\naxes[4].set_title(\"Black\")\ng.set_xlabels('', fontsize=1)\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('Age distribution by race', fontsize=15)\n\ng._legend.set_title('')","4977cf1b":"df[['race','gender', 'age']].groupby(['race', 'gender'])\\\n.agg(['mean', 'median', 'min', 'max', 'count'])","a7d1f8db":"fig, ax= plt.subplots(1,2, figsize=(12, 6))\n\ndf[df.gender=='Female'][['race', 'age']].groupby('race').mean().plot(kind='bar', ax=ax[0], legend=False,\n                                                              title='Females', ylim=(24,28),\n                                                              color='rgbmy')\n\ndf[df.gender=='Male'][['race', 'age']].groupby('race').mean().plot(kind='bar', ax=ax[1], legend=False,\n                                                              title='Males', ylim=(24,28),\n                                                              color='rgbmy')\n\nax[0].set_xticklabels(ax[0].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[1].set_xticklabels(ax[1].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[0].set_xlabel('',fontsize=1)\nax[1].set_xlabel('',fontsize=1)\n\nfig.suptitle('Mean age by race', fontsize=22)\n\nfor i in ax[0].patches:\n    ax[0].text(i.get_x()+.02, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)\n    \nfor i in ax[1].patches:\n    ax[1].text(i.get_x()+.02, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)","79e799af":"df['field_cd'] = df.field_cd.map({1: 'Law', 2: 'Math', 3: 'Soc. Sc.', 4: 'Med. Sc.',\n                                 5: 'Eng.', 6: 'Journ.', 7: 'Hist.', 8: 'Econ', 9: 'Educ.',\n                                 10: 'Nat. Sc.', 11: 'Soc. Wr.', 12: 'Und.', 13: 'Pol. Sc.',\n                                 14: 'Film', 15: 'Arts', 16:'Lang.', 17: 'Arch.', 18: 'Oth.'}).fillna(df.field_cd)\ndf_full['field_cd'] = df_full.field_cd.map({1: 'Law', 2: 'Math', 3: 'Soc. Sc.', 4: 'Med. Sc.',\n                                 5: 'Eng.', 6: 'Journ.', 7: 'Hist.', 8: 'Econ', 9: 'Educ.',\n                                 10: 'Nat. Sc.', 11: 'Soc. Wr.', 12: 'Und.', 13: 'Pol. Sc.',\n                                 14: 'Film', 15: 'Arts', 16:'Lang.', 17: 'Arch.', 18: 'Oth.'}).fillna(df_full.field_cd)\ndf.field_cd.value_counts(dropna=False)","5d438082":"plt.figure(figsize = (12,5))\nax = sns.countplot(x=\"field_cd\", data=df)\nplt.title('Field of study', fontsize=18)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n\nplt.ylim(0, 150)\nplt.xlabel('')\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.2, i.get_height()+3, \\\n            str(round((i.get_height()), 1)), fontsize=12)","27078902":"tmp = df[['gender', 'field_cd']].groupby(['field_cd', 'gender']).size().unstack().fillna(0)\nax = tmp.plot(kind='bar', figsize=(12,6), stacked=True)\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation=45)\n\nax.set_title('Field of study by gender', fontsize=18)\nax.set_xlabel('',fontsize=1)","1357d99d":"tmp = df[['gender', 'field_cd']].groupby(['field_cd', 'gender']).size().unstack().fillna(0)\ntmp['% Female'] = round(tmp.Female \/ (tmp.Female + tmp.Male) * 100, 2)\ntmp['% Male'] = round(tmp.Male \/ (tmp.Female + tmp.Male) * 100, 2)\ntmp","f172a9f9":"ax = df.imprace.hist(bins=10, figsize=(12,8))\nax.set_title('How important is the race', fontsize=15)\nax.set_xlabel('Importance',fontsize=12)\nax.set_ylabel('Count', fontsize=12)\nax.grid(False)","76516fdb":"g = sns.FacetGrid(df, col='race')\ng.map(plt.hist, 'imprace', bins=10)\naxes = g.axes.flatten()\naxes[0].set_title(\"Asian\")\naxes[1].set_title(\"White\")\naxes[2].set_title(\"Other\")\naxes[3].set_title(\"Hispanic\")\naxes[4].set_title(\"Black\")\ng.set_xlabels('', fontsize=1)\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How important is the race, by race', fontsize=15)\ng.add_legend()","753bea4f":"df[['race', 'imprace']].groupby(['race']).agg(['mean', 'median', 'min', 'max', 'count'])","cbf0315c":"ax = df[['race', 'imprace']].groupby('race').mean().plot(kind='bar', figsize=(12,5), legend=False,\n                                               title='Average importance of Race, by Race',\n                                                   ylim=(2,5), color='rgbmy')\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\nax.set_xlabel('',fontsize=1)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.2, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)","6035f463":"g = sns.FacetGrid(df, hue='gender', height = 6)\ng.map(plt.hist, 'imprace', alpha= 0.7, bins=10)\ng.set_xlabels('Importance', fontsize=12)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('How important is the race, by gender', fontsize=15)\ng.add_legend()","fadc2afc":"df[['gender', 'imprace']].groupby(['gender']).agg(['mean', 'median', 'min', 'max', 'count'])","ff3c0eee":"ax = df[['gender', 'imprace']].groupby('gender').mean().plot(kind='bar', figsize=(12,5), legend=False,\n                                               title='Importance of Race, by Gender',\n                                                   ylim=(2,5))\nax.set_xticklabels(['Female', 'Male'], fontsize=12, rotation='horizontal')\nax.set_xlabel('',fontsize=1)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.22, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)","8a91aff8":"g = sns.FacetGrid(df, col='race', hue='gender')\ng.map(plt.hist, 'imprace', alpha= 0.7, bins=10)\ng.add_legend()\naxes = g.axes.flatten()\naxes[0].set_title(\"Asian\")\naxes[1].set_title(\"White\")\naxes[2].set_title(\"Other\")\naxes[3].set_title(\"Hispanic\")\naxes[4].set_title(\"Black\")\ng.set_xlabels('', fontsize=1)\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How important is the race', fontsize=15)\n\ng._legend.set_title('')","739b0387":"df[['gender', 'race', 'imprace']].groupby(['gender', 'race'])\\\n.agg(['mean', 'median', 'min', 'max', 'count'])","a9a9c68c":"fig, ax= plt.subplots(1,2, figsize=(15, 6))\n\ndf[df.gender=='Female'][['race', 'imprace']].groupby('race').mean().plot(kind='bar', ax=ax[0], legend=False,\n                                                              title='Females', ylim=(2,5),\n                                                              color='rgbmy')\n\ndf[df.gender=='Male'][['race', 'imprace']].groupby('race').mean().plot(kind='bar', ax=ax[1], legend=False,\n                                                              title='Males', ylim=(2,5),\n                                                              color='rgbmy')\n\nax[0].set_xticklabels(ax[0].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[1].set_xticklabels(ax[0].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[0].set_xlabel('',fontsize=1)\nax[1].set_xlabel('',fontsize=1)\n\nfig.suptitle('Mean importance of race, by race', fontsize=22)\n\nfor i in ax[0].patches:\n    ax[0].text(i.get_x()+.09, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)\n    \nfor i in ax[1].patches:\n    ax[1].text(i.get_x()+.09, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)","fd8ffbae":"ax = df.imprelig.hist(bins=10, figsize=(12,8))\nax.set_title('How important is the religion', fontsize=15)\nax.set_xlabel('Importance',fontsize=12)\nax.set_ylabel('Count', fontsize=12)\nax.grid(False)","12ad7573":"g = sns.FacetGrid(df, col='race')\ng.map(plt.hist, 'imprelig', bins=10)\ng.add_legend()\naxes = g.axes.flatten()\naxes[0].set_title(\"Asian\")\naxes[1].set_title(\"White\")\naxes[2].set_title(\"Other\")\naxes[3].set_title(\"Hispanic\")\naxes[4].set_title(\"Black\")\ng.set_xlabels('', fontsize=1)\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How important is the religion, by race', fontsize=15)","60318552":"df[['race', 'imprelig']].groupby(['race']).agg(['mean', 'median', 'min', 'max', 'count'])","74045b7e":"ax = df[['race', 'imprelig']].groupby('race').mean().plot(kind='bar', figsize=(12,5), legend=False,\n                                               title='Importance of Religion by Race',\n                                                   ylim=(2,5), color='rgbmy')\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\nax.set_xlabel('', fontsize=1)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.19, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)","8543fa40":"g = sns.FacetGrid(df, hue='gender', height = 6)\ng.map(plt.hist, 'imprelig', alpha= 0.7, bins=10)\ng.set_xlabels('Importance', fontsize=12)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('How important is the religion, by gender', fontsize=15)\ng.add_legend()","10c86c11":"df[['gender', 'imprelig']].groupby(['gender'])\\\n.agg(['mean', 'median', 'min', 'max', 'count'])","3f227700":"ax = df[['gender', 'imprelig']].groupby('gender').mean().plot(kind='bar', figsize=(12,5), legend=False,\n                                               title='Importance of Religion, by Gender',\n                                                   ylim=(2,5))\nax.set_xticklabels(['Female', 'Male'], fontsize=12, rotation='horizontal')\nax.set_xlabel('', fontsize=1)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.22, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)","747ba33c":"g = sns.FacetGrid(df, col='race', hue='gender')\ng.map(plt.hist, 'imprelig', alpha= 0.7, bins=10)\ng.add_legend()\naxes = g.axes.flatten()\naxes[0].set_title(\"Asian\")\naxes[1].set_title(\"White\")\naxes[2].set_title(\"Other\")\naxes[3].set_title(\"Hispanic\")\naxes[4].set_title(\"Black\")\ng.set_xlabels('', fontsize=1)\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How important is the religion', fontsize=15)\n\ng._legend.set_title('')","3cd5fdc5":"df[['gender', 'race', 'imprelig']].groupby(['gender', 'race'])\\\n.agg(['mean', 'median', 'min', 'max', 'count'])","bec1c8f6":"fig, ax= plt.subplots(1,2, figsize=(15, 6))\n\ndf[df.gender=='Female'][['race', 'imprelig']].groupby('race').mean().plot(kind='bar', ax=ax[0], legend=False,\n                                                              title='Females', ylim=(1,5),\n                                                              color='rgbmy')\n\ndf[df.gender=='Male'][['race', 'imprelig']].groupby('race').mean().plot(kind='bar', ax=ax[1], legend=False,\n                                                              title='Males', ylim=(1,5),\n                                                              color='rgbmy')\n\nax[0].set_xticklabels(ax[0].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[1].set_xticklabels(ax[1].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[0].set_xlabel('')\nax[1].set_xlabel('')\n\nfig.suptitle('Mean importance of religion, by race', fontsize=22)\n\nfor i in ax[0].patches:\n    ax[0].text(i.get_x()+.1, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)\n    \nfor i in ax[1].patches:\n    ax[1].text(i.get_x()+.1, i.get_height()+.05, \\\n            str(round((i.get_height()), 1)), fontsize=12)","a12ac49b":"sinc = df_full[['iid', 'dec', 'samerace']].groupby(['iid', 'samerace']).sum().unstack().fillna(0)\nsinc = sinc.dec.rename(columns={0: 'dec_diff', 1: 'dec_same'})\ntmp = df_full[['iid', 'wave', 'samerace']].groupby(['iid', 'samerace']).count().unstack().fillna(0)\ntmp = tmp.wave.rename(columns={0: 'dates_diff', 1: 'dates_same'})\nsinc = pd.merge(sinc, tmp, left_index=True, right_index=True)\nsinc = sinc.reset_index()\ndf = pd.merge(df, sinc, on='iid')\ndel sinc","1d2d82da":"tmp = (df.dec_diff.sum() + df.dec_same.sum()) \/ (df.dates_diff.sum() + df.dates_same.sum())\nprint('Rate of positive feedback on the partner: {}%'.format(round(tmp*100,2)))\ntmp = (df.dec_same.sum()) \/ (df.dates_same.sum())\nprint('Rate of positive feedback on the partner of the same race: {}%'.format(round(tmp*100,2)))\ntmp = (df.dec_diff.sum()) \/ (df.dates_diff.sum())\nprint('Rate of positive feedback on the partner of the a different race: {}%'.format(round(tmp*100,2)))","26925671":"print('Rate of positive feedback on the partner of the same race, by race')\ndf_full[['dec','samerace', 'race']].groupby(['race','samerace']).mean()*100","987a3906":"fig, ax= plt.subplots(1,2, figsize=(15, 6))\n\ndf_full[df_full.samerace==0][['race', 'dec']].groupby('race').mean().plot(kind='bar', ax=ax[0], legend=False,\n                                                              title='Different races', ylim=(0,1),\n                                                              color='rgbmy')\n\ndf_full[df_full.samerace==1][['race', 'dec']].groupby('race').mean().plot(kind='bar', ax=ax[1], legend=False,\n                                                              title='Same race', ylim=(0,1),\n                                                              color='rgbmy')\n\nax[0].set_xticklabels(ax[0].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[1].set_xticklabels(ax[1].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[0].set_xlabel('')\nax[1].set_xlabel('')\nvals = ax[0].get_yticks()\nax[0].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\nax[1].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\n\nfig.suptitle('Rate of positive feedback, by race', fontsize=18)\n\nfor i in ax[0].patches:\n    ax[0].text(i.get_x()+.01, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)\n    \nfor i in ax[1].patches:\n    ax[1].text(i.get_x()+.01, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)","51cad90b":"print('Rate of positive feedback on the partner of the same race, by gender')\ndf_full[['dec','samerace', 'gender']].groupby(['gender','samerace']).mean()*100","62a01890":"fig, ax= plt.subplots(1,2, figsize=(15, 6))\n\ndf_full[df_full.samerace==0][['gender', 'dec']].groupby('gender').mean().plot(kind='bar', ax=ax[0], legend=False,\n                                                              title='Different races', ylim=(0,1))\n\ndf_full[df_full.samerace==1][['gender', 'dec']].groupby('gender').mean().plot(kind='bar', ax=ax[1], legend=False,\n                                                              title='Same race', ylim=(0,1))\n\nax[0].set_xticklabels(['Female', 'Male'], fontsize=12, rotation='horizontal')\nax[1].set_xticklabels(['Female', 'Male'], fontsize=12, rotation='horizontal')\nax[0].set_xlabel('')\nax[1].set_xlabel('')\nvals = ax[0].get_yticks()\nax[0].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\nax[1].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\n\nfig.suptitle('Rate of positive feedback, by gender', fontsize=18)\n\nfor i in ax[0].patches:\n    ax[0].text(i.get_x()+.15, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)\n    \nfor i in ax[1].patches:\n    ax[1].text(i.get_x()+.15, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)","ed610c69":"fig, ax= plt.subplots(2,2, figsize=(15, 12))\n\ndf_full[(df_full.samerace==0) & (df_full.gender=='Female')][['race', \n                                       'dec']].groupby('race').mean().plot(kind='bar', ax=ax[0][0], legend=False,\n                                                              title='Different races, Females', ylim=(0,1),\n                                                              color='rgbmy')\n\ndf_full[(df_full.samerace==1) & (df_full.gender=='Male')][['race', \n                                        'dec']].groupby('race').mean().plot(kind='bar', ax=ax[1][1], legend=False,\n                                                              title='Same race, Males', ylim=(0,1),\n                                                              color='rgbmy')\n\ndf_full[(df_full.samerace==0) & (df_full.gender=='Male')][['race', \n                                       'dec']].groupby('race').mean().plot(kind='bar', ax=ax[1][0], legend=False,\n                                                              title='Different races, Males', ylim=(0,1),\n                                                              color='rgbmy')\n\ndf_full[(df_full.samerace==1) & (df_full.gender=='Female')][['race', \n                                        'dec']].groupby('race').mean().plot(kind='bar', ax=ax[0][1], legend=False,\n                                                              title='Same race, Females', ylim=(0,1),\n                                                              color='rgbmy')\n\nax[0][0].set_xticklabels(ax[0][0].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[1][0].set_xticklabels(ax[1][0].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[0][1].set_xticklabels(ax[0][1].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[1][1].set_xticklabels(ax[1][1].get_xticklabels(), fontsize=12, rotation='horizontal')\nax[0][0].set_xlabel('')\nax[1][0].set_xlabel('')\nax[0][1].set_xlabel('')\nax[1][1].set_xlabel('')\nvals = ax[0][0].get_yticks()\nax[0][0].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\nax[1][0].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\nax[0][1].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\nax[1][1].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\n\nfig.suptitle('Rate of positive feedback, by race', fontsize=18)\n\nfor i in ax[0][0].patches:\n    ax[0][0].text(i.get_x()+.01, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)\n    \nfor i in ax[1][0].patches:\n    ax[1][0].text(i.get_x()+.01, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)\n    \nfor i in ax[0][1].patches:\n    ax[0][1].text(i.get_x()+.01, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)\n    \nfor i in ax[1][1].patches:\n    ax[1][1].text(i.get_x()+.01, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)","e768ecb6":"fig, ax= plt.subplots(1,2, figsize=(15, 6))\n\ndf_full[df_full.samerace==0][['imprace', \n                              'dec']].groupby('imprace').mean().unstack().plot(kind='bar', ax=ax[0], legend=False,\n                                                              title='Different races', ylim=(0,1))\n\ndf_full[df_full.samerace==1][['imprace', \n                              'dec']].groupby('imprace').mean().unstack().plot(kind='bar', ax=ax[1], legend=False,\n                                                              title='Same race', ylim=(0,1))\n\nvals = ax[0].get_yticks()\nax[0].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\nax[1].set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\n\nax[0].set_xticklabels(np.arange(11), fontsize=12, rotation='horizontal')\nax[1].set_xticklabels(np.arange(11), fontsize=12, rotation='horizontal')\n\nax[0].set_xlabel('Importance of race')\nax[1].set_xlabel('Importance of race')\n\nfig.suptitle('Rate of positive feedback, by declared importance', fontsize=18)\n\nfor i in ax[0].patches:\n    ax[0].text(i.get_x()+.005, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=8)\n    \nfor i in ax[1].patches:\n    ax[1].text(i.get_x()+.005, i.get_height()+.02, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=8)","6a3b2835":"df['goal'] = df.goal.map({1: 'Fun', 2: 'Meet', 3: 'Date', \n                          4: 'Relationship', 5: 'IdidIt', 6: 'Other'}).fillna(df.goal)\ndf_full['goal'] = df_full.goal.map({1: 'Fun', 2: 'Meet', 3: 'Date', \n                          4: 'Relationship', 5: 'IdidIt', 6: 'Other'}).fillna(df_full.goal)\ndf.goal.value_counts(dropna=False)","e262179a":"tmp = df[['gender', 'goal']].groupby(['goal', 'gender']).size().unstack()\nax = tmp.plot(kind='bar', figsize=(12,6), ylim=(0,130))\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\n\nax.set_title('Declared goal for the evening, by gender', fontsize=18)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.07, i.get_height()+3, \\\n            str(round((i.get_height()), 1)), fontsize=10)","bc560a83":"df['go_out'] = df.go_out.map({1: 'Several_pw', 2: 'Twice_pw', 3: 'Once_pw',\n                             4: 'Twice_pm', 5: 'Once_pm', 6: 'Several_py',\n                             7: 'Almost_never'}).fillna(df.go_out)\ndf_full['go_out'] = df_full.go_out.map({1: 'Several_pw', 2: 'Twice_pw', 3: 'Once_pw',\n                             4: 'Twice_pm', 5: 'Once_pm', 6: 'Several_py',\n                             7: 'Almost_never'}).fillna(df_full.go_out)\ndf.go_out.value_counts(dropna=False)","7d87a7bd":"pd.crosstab(df.go_out, df.goal, normalize='index')","6f31649f":"tmp = df[['gender', 'go_out']].groupby(['gender', 'go_out']).size().unstack(0)\ntmp = tmp.reindex(['Almost_never', 'Several_py',\n                   'Once_pm', 'Twice_pm',\n                   'Once_pw', 'Twice_pw', 'Several_pw'])\nax = tmp.plot(kind='bar', figsize=(12,6), ylim=(0,120))\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\n\nax.set_title('How often do you go out, by gender', fontsize=18)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.07, i.get_height()+2, \\\n            str(round((i.get_height()), 1)), fontsize=10)","36b2c214":"df[['race', 'go_out']].groupby(['race', 'go_out']).size().unstack().fillna(0)","8ff6cbf0":"df['date'] = df.date.map({1: 'Several_pw', 2: 'Twice_pw', 3: 'Once_pw',\n                         4: 'Twice_pm', 5: 'Once_pm', 6: 'Several_py',\n                         7: 'Almost_never'}).fillna(df.date)\ndf_full['date'] = df_full.date.map({1: 'Several_pw', 2: 'Twice_pw', 3: 'Once_pw',\n                         4: 'Twice_pm', 5: 'Once_pm', 6: 'Several_py',\n                         7: 'Almost_never'}).fillna(df_full.date)\ndf.date.value_counts(dropna=False)","ea920a9f":"pd.crosstab(df.date, df.goal, normalize='index')","c2309946":"tmp = df[['gender', 'date']].groupby(['gender', 'date']).size().unstack(0)\ntmp = tmp.reindex(['Almost_never', 'Several_py',\n                   'Once_pm', 'Twice_pm',\n                   'Once_pw', 'Twice_pw', 'Several_pw'])\nax = tmp.plot(kind='bar', figsize=(12,6), ylim=(0,100))\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\n\nax.set_title('How often do you go on a date, by gender', fontsize=18)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.07, i.get_height()+2, \\\n            str(round((i.get_height()), 1)), fontsize=10)","f54109b4":"df[['race', 'date']].groupby(['race', 'date']).size().unstack().fillna(0)","7ac6c54d":"df[['race', 'gender', 'date']].groupby(['race', 'gender', 'date']).size().unstack().fillna(0)","f7aec497":"def many_hist(cols):\n    num = len(cols)\n    rows = int(num\/2) + (num % 2 > 0)\n    fig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\n    i = 0\n    j = 0\n    for feat in cols:\n        df[feat].hist(label=feat, ax=ax[i][j])\n        ax[i][j].set_title(feat, fontsize=12)\n        ax[i][j].grid(False)\n        j = (j+1)%2\n        i = i + 1 - j","4a419465":"interests = ['sports', 'tvsports', 'exercise', 'dining', 'museums',\n       'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n       'movies', 'concerts', 'music', 'shopping', 'yoga']\nmany_hist(interests)","00bf4344":"corr = df[interests].corr()\nplt.figure(figsize=(12,10))\nax = sns.heatmap(corr, cmap='RdBu_r')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nax.set_title('Correlation between interests', fontsize=18)","09a6658a":"corr_m = df[df.gender=='Male'][interests].corr()\ncorr_f = df[df.gender=='Female'][interests].corr()\n\nfig, ax= plt.subplots(1,2, figsize=(15, 6))\nsns.heatmap(corr_m, cmap='RdBu_r', ax=ax[0])\nsns.heatmap(corr_f, cmap='RdBu_r', ax=ax[1])\n\nax[0].set_title('Males', fontsize=15)\nax[1].set_title('Females', fontsize=15)","3af509c2":"tmp = df[['gender']+ interests].groupby(['gender']).mean().stack().unstack(0)\nax = tmp.plot(kind='bar', figsize=(12,6))\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation=45)\n\nax.set_title('Mean interest in activities, by gender', fontsize=18)\n\nfor i in ax.patches:\n    ax.text(i.get_x(), i.get_height()+.10, \\\n            str(round((i.get_height()), 1)), fontsize=8)","e13190d4":"tmp = df[['race']+ interests].groupby(['race']).mean().stack().unstack(0)\nax = tmp.plot(kind='bar', figsize=(15,6),color='rgbmy')\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation=45)\n\nax.set_title('Mean interest in activities, by race', fontsize=18)","6a03e9ef":"ax = df.exphappy.hist(bins=10, figsize=(12,8))\nax.set_title('How happy do you expect to be?', fontsize=18)\nax.set_xlabel('Happiness',fontsize=12)\nax.set_ylabel('Count', fontsize=12)\nax.grid(False)","49ac1429":"df[['gender', 'exphappy']].groupby(['gender']).agg(['mean', 'median', 'min', 'max', 'count'])","3ec9b45c":"g = sns.FacetGrid(df, hue='gender', height = 6)\ng.map(plt.hist, 'exphappy', alpha= 0.7, bins=10)\ng.set_xlabels('Happiness', fontsize=12)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('How happy do you expect to be? - by gender', fontsize=15)\ng.add_legend()","551bcdd7":"ax = df.expnum.hist(bins=10, figsize=(12,8))\nax.set_title('How many yes do you expect to receive?', fontsize=18)\nax.set_xlabel('Num. of yes',fontsize=12)\nax.set_ylabel('Count', fontsize=12)\nax.grid(False)","c5079fe6":"df[['gender', 'expnum']].groupby(['gender']).agg(['mean', 'median', 'min', 'max', 'count'])","7d75407b":"g = sns.FacetGrid(df, hue='gender', height = 6)\ng.map(plt.hist, 'expnum', alpha= 0.7, bins=20)\ng.set_xlabels('Num. of yes', fontsize=12)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('How many yes do you expect to receive? - by gender', fontsize=15)\ng.add_legend()","291affe9":"ax = df[['race', 'expnum']].groupby(['race']).mean().plot(kind='bar', figsize=(12,5), legend=False,\n                                               title='How many yes do you expect to receive? - by Race',\n                                                   color='rgbmy', ylim=(0,8))\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\nax.set_xlabel('', fontsize=1)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+0.19, i.get_height()+.1, \\\n            str(round((i.get_height()), 1)), fontsize=12)","df26c8d3":"df[['expnum', 'wave']].groupby('wave').sum()","c6c02ea4":" ax = df_full[['wave', 'match']].groupby('wave').mean().plot(kind='bar', legend=False, figsize=(15,5),\n                                                        ylim=(0,0.5))\n\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\nax.set_xlabel('Wave', fontsize=12)\n\nvals = ax.get_yticks()\nax.set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\n\nax.set_title('Match Rate per wave', fontsize=18)\n\nfor i in ax.patches:\n    ax.text(i.get_x(), i.get_height()+.01, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)","d8dadd47":"ax = df_full[['wave', 'dec']].groupby('wave').mean().plot(kind='bar', legend=False, figsize=(15,5),\n                                                        ylim=(0,0.8))\n\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\nax.set_xlabel('Wave', fontsize=12)\n\nvals = ax.get_yticks()\nax.set_yticklabels(['{:,.0%}'.format(x) for x in vals], fontsize=12)\n\nax.set_title('Positive feedback rate per wave', fontsize=18)\n\nfor i in ax.patches:\n    ax.text(i.get_x(), i.get_height()+.01, \\\n            str(round((i.get_height())*100, 1))+'%', fontsize=12)","d28f42cf":"you_look = [col for col in df_full.columns if '1_1' in col] # what you look for in the opposite sex\nother_look =  [col for col in df_full.columns if '4_1' in col] # you think other like you look for in the opposite sex\nopp_look = [col for col in df_full.columns if '2_1' in col] # you think the opposite sex look for in you\nyou_score = [col for col in df_full.columns if '3_1' in col] # how do you score yourself\nthey_score = [col for col in df_full.columns if '5_1' in col] # the others score you\nprint(you_look)","44daba08":"tmp = df_full[['iid'] + you_look + other_look + opp_look + you_score + they_score].drop_duplicates()\ndf = pd.merge(df, tmp, on='iid')\ndf.head()","eaff2208":"tmp_norm = df[(df.wave < 6) | (df.wave > 9)].copy()\ntmp_diff = df[(df.wave > 5) & (df.wave < 10)].copy()\ntmp_diff[['wave']+you_look+other_look].head(10)","0cab4ada":"tmp_norm[['wave']+you_look+other_look].sample(10)","ce193dc1":"num = len(you_look)\nrows = int(num\/2) + (num % 2 > 0)\nfig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\ni = 0\nj = 0\nfor feat in you_look:\n    tmp_norm[feat].hist(label=feat, ax=ax[i][j], bins=20)\n    ax[i][j].set_title(feat, fontsize=12)\n    ax[i][j].grid(False)\n    ax[i][j].set_xlim(0,100)\n    j = (j+1)%2\n    i = i + 1 - j\n\nplt.subplots_adjust(top=0.93)\nfig.suptitle('What do you look for in the opposite sex?', fontsize=18)","a384f467":"num = len(other_look)\nrows = int(num\/2) + (num % 2 > 0)\nfig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\ni = 0\nj = 0\nfor feat in other_look:\n    tmp_norm[feat].hist(label=feat, ax=ax[i][j], bins=20)\n    ax[i][j].set_title(feat, fontsize=12)\n    ax[i][j].grid(False)\n    ax[i][j].set_xlim(0,100)\n    j = (j+1)%2\n    i = i + 1 - j\n    \nplt.subplots_adjust(top=0.93)\nfig.suptitle('What you think MOST of your fellow men\/women look for in the opposite sex?', fontsize=18)","4eff85df":"num = len(opp_look)\nrows = int(num\/2) + (num % 2 > 0)\nfig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\ni = 0\nj = 0\nfor feat in opp_look:\n    tmp_norm[feat].hist(label=feat, ax=ax[i][j], bins=20)\n    ax[i][j].set_title(feat, fontsize=12)\n    ax[i][j].grid(False)\n    ax[i][j].set_xlim(0,100)\n    j = (j+1)%2\n    i = i + 1 - j\n    \nplt.subplots_adjust(top=0.93)\nfig.suptitle('What do you think the opposite sex looks for in a date?', fontsize=18)","a9cdf47b":"num = len(you_score)\nrows = int(num\/2) + (num % 2 > 0)\nfig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\ni = 0\nj = 0\nfor feat in you_score:\n    tmp_norm[feat].hist(label=feat, ax=ax[i][j], bins=10)\n    ax[i][j].set_title(feat, fontsize=12)\n    ax[i][j].grid(False)\n    j = (j+1)%2\n    i = i + 1 - j\n    \nplt.subplots_adjust(top=0.93)\nfig.suptitle('How do you think you measure up?', fontsize=18)","3cd93e27":"num = len(they_score)\nrows = int(num\/2) + (num % 2 > 0)\nfig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\ni = 0\nj = 0\nfor feat in they_score:\n    tmp_norm[feat].hist(label=feat, ax=ax[i][j], bins=10)\n    ax[i][j].set_title(feat, fontsize=12)\n    ax[i][j].grid(False)\n    j = (j+1)%2\n    i = i + 1 - j\n    \nplt.subplots_adjust(top=0.93)\nfig.suptitle('How do you think others perceive you?', fontsize=18)","249be08e":"corr = tmp_norm[you_look + other_look + opp_look + you_score + they_score].corr()\nplt.figure(figsize=(12,10))\nax = sns.heatmap(corr, cmap='RdBu_r')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nax.set_title('Correlation between evaluations', fontsize=18)","64e4efae":"df = df_full[['iid', 'race', 'gender', 'field_cd', 'dec', 'match', 'int_corr', 'samerace', 'met',\n             'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob']].copy()\ndf.head(5)","f975dba2":"tmp = df[['dec', 'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob']].groupby('dec').mean().stack().unstack(0)\ntmp = tmp.rename(columns={0: 'No', 1: 'Yes'})\nax = tmp.plot(kind='bar', figsize=(12,6), ylim=(0,10))\nax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation='horizontal')\n\nax.set_title('How do you rate your partner?', fontsize=18)\nax.set_ylabel('Mean score', fontsize=12)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.02, i.get_height()+0.2, \\\n            str(round((i.get_height()), 1)), fontsize=10)","fecf0a6b":"feats = ['attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob']\n\nnum = len(feats)\nrows = int(num\/2) + (num % 2 > 0)\nfig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\ni = 0\nj = 0\nfor feat in feats:\n    df[df.dec==0][feat].hist(label='No', ax=ax[i][j], bins=10, alpha=0.7)\n    df[df.dec==1][feat].hist(label='Yes', ax=ax[i][j], bins=10, alpha=0.7)\n    ax[i][j].set_title(feat, fontsize=12)\n    ax[i][j].grid(False)\n    ax[i][j].legend()\n    j = (j+1)%2\n    i = i + 1 - j\n    \nplt.subplots_adjust(top=0.93)\nfig.suptitle('How do you rate your partner?', fontsize=18)","beff2f59":"corr_m = df[df.dec==0][feats].corr()\ncorr_f = df[df.dec==1][feats].corr()\n\nfig, ax= plt.subplots(1,2, figsize=(15, 6))\nsns.heatmap(corr_m, cmap='RdBu_r', ax=ax[0])\nsns.heatmap(corr_f, cmap='RdBu_r', ax=ax[1])\n\nax[0].set_title('No', fontsize=15)\nax[1].set_title('Yes', fontsize=15)","29724161":"feats = ['attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob']\n\nnum = len(feats)\nrows = int(num\/2) + (num % 2 > 0)\nfig, ax = plt.subplots(rows, 2, figsize=(15, 5 * (rows)))\ni = 0\nj = 0\nfor feat in feats:\n    df[df.samerace==0][feat].hist(label='Different race', ax=ax[i][j], bins=10, alpha=0.7)\n    df[df.samerace==1][feat].hist(label='Same Race', ax=ax[i][j], bins=10, alpha=0.7)\n    ax[i][j].set_title(feat, fontsize=12)\n    ax[i][j].grid(False)\n    ax[i][j].legend()\n    j = (j+1)%2\n    i = i + 1 - j\n    \nplt.subplots_adjust(top=0.93)\nfig.suptitle('How do you rate your partner?', fontsize=18)","1485b839":"cols = df_full.columns\nfor col in cols:\n    mis = df_full[col].isnull().sum()\n    if mis > 0:\n        print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/df_full.shape[0] * 100, 3)))","326ff47f":"to_drop = []\ntmp = [col for col in df_full.columns if '1_s' in col or '3_s' in col] # asked mid session, not clear when so we drop it\nto_drop += tmp\ntmp = [col for col in df_full.columns if '_2' in col]  # asked after the session\ntmp += ['length']\nto_drop += tmp\ntmp = [col for col in df_full.columns if '_3' in col]  # asked after the session\ntmp += ['you_call', 'them_cal']\nto_drop += tmp\n\nprint(f\"Original shape: {df_full.shape}\")\nprint(f\"Dropping {len(to_drop)} columns\")\n\nfor col in to_drop:\n    del df_full[col]\n    \nprint(f\"New shape: {df_full.shape}\")","0f82dcc0":"to_drop = ['dec_o', 'match']\nprint(f\"Original shape: {df_full.shape}\")\nprint(f\"Dropping {len(to_drop)} columns\")\n\nfor col in to_drop:\n    del df_full[col]\n    \nprint(f\"New shape: {df_full.shape}\")","f2cdcb87":"to_drop = ['positin1', 'undergra', 'mn_sat', 'tuition', 'zipcode', 'income', 'expnum', 'match_es']\nto_drop += [col for col in df_full.columns if '5_1' in col or '4_1' in col]  # sadly, not asked to everyone\n\nprint(f\"Original shape: {df_full.shape}\")\nprint(f\"Dropping {len(to_drop)} columns\")\n\nfor col in to_drop:\n    del df_full[col]\n    \nprint(f\"New shape: {df_full.shape}\")","4956df7d":"print('Unique id\\'s in original dataframe: {}'.format(len(df_full.iid.unique())))\ndf_full = df_full[df_full.pid.notnull()].copy()  # dropping the missing female's dates.\nprint('Shape original dataframe: {}'.format(df_full.shape))\npersonal = ['gender', 'age', 'field', 'field_cd', 'race', 'imprace', 'imprelig', \n       'goal', 'date', 'go_out', 'career',\n       'career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums',\n       'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n       'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy']\npersonal += [col for col in df_full.columns if '1_1' in col]\nprint('_'*40)\nprint('_'*40)\n\nparticipant = df_full[['iid', 'wave'] + personal].drop_duplicates().copy()\nprint('Unique id\\'s in participants\\' dataframe: {}'.format(len(participant.iid.unique())))\nprint('Shape participants\\' dataframe: {}'.format(participant.shape))\nprint('_'*40)\nprint('Missing values about participants:')\ncols = participant.columns\nfor col in cols:\n    mis = participant[col].isnull().sum()\n    if mis > 0:\n        print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/participant.shape[0] * 100, 3)))\nprint('_'*40)\nprint('_'*40)\n\npersonal_o = ['age_o','race_o','pf_o_att','pf_o_sin','pf_o_int','pf_o_fun','pf_o_amb','pf_o_sha']\npartner = df_full[['pid', 'wave'] + personal_o].drop_duplicates().copy()\nprint('Unique id\\'s in partners\\' dataframe: {}'.format(len(partner.pid.unique())))\nprint('Shape partners\\' dataframe: {}'.format(partner.shape))\nprint('_'*40)\nprint('Missing values about partners:')\ncols = partner.columns\nfor col in cols:\n    mis = partner[col].isnull().sum()\n    if mis > 0:\n        print(\"{}: {} missing, {}%\".format(col, mis, round(mis\/partner.shape[0] * 100, 3)))","e354b69f":"col_to_filt = ['age', 'race']\nmis_id = []\n\nfor col in col_to_filt:\n    mis_id = list(set(mis_id + list(participant[participant[col].isna()].iid.values)))\n    mis_id = list(set(mis_id + list(partner[partner[col + '_o'].isna()].pid.values)))\n\nprint(f'Original number of participants: {participant.shape[0]}')\n\nmis_id += [28] # another respondent with a lot of missing values\nparticipant = participant[~participant.iid.isin(mis_id)]\npartner = partner[~partner.pid.isin(mis_id)]\n\nprint(f\"Dropping {len(mis_id)} participants\")\nprint(f\"New number of participants: {participant.shape[0]}\")","af2c1170":"print(f'Original shape of the dataframe: {df_full.shape}')\ndf_clean = df_full[~(df_full.iid.isin(mis_id) | df_full.pid.isin(mis_id))].copy()\ndel df_clean['id']  # not useful\ndel df_clean['field']  # redundant with field_cd\ndel df_clean['career']\ndel df_clean['career_c']  # both not explored and I am lazy\ndel df_clean['from']\ndel df_clean['partner']  # this is an id, never train on those\n\nprint(f'Shape of the cleaned data: {df_clean.shape}')","47ebcc5e":"df_clean.loc[df_clean.field_cd.isna(), 'field_cd'] = 'Oth.'  # easy cleaning","67defd59":"df_clean.loc[df_clean.met.isna(), 'met'] = 0\ndf_clean.loc[df_clean.met_o.isna(), 'met_o'] = 0\n\ndf_clean.loc[df_clean.met < 2, 'met'] = 0\ndf_clean.loc[df_clean.met_o < 2, 'met_o'] = 0\ndf_clean.loc[df_clean.met > 1, 'met'] = 1\ndf_clean.loc[df_clean.met_o > 1, 'met_o'] = 1\n\ndf_clean.loc[df_clean.gaming > 10, 'gaming'] = 10\ndf_clean.loc[df_clean.reading > 10, 'reading'] = 10","7e633d6e":"sprs = ['Und.', 'Arch.', 'Lang.']\n\ndf_clean.loc[df.field_cd.isin(sprs), 'field_cd'] = 'Oth.'","982fee03":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import KFold, GridSearchCV, cross_val_predict\nfrom sklearn.model_selection import learning_curve\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nfrom itertools import product ","b1374721":"# here some useful functions\n\ndef pretty_grid(model, param_grid, train, target, scoring, cv=5, n_jobs=1, verbose=False):\n    grid = GridSearchCV(model, param_grid=param_grid, \n                        cv=cv, n_jobs=n_jobs, scoring=scoring,\n                       return_train_score=True)\n    grid.fit(train, target)\n    best_model = grid.best_estimator_\n    print(best_model)\n    print(\"_\"*40)\n    print(pd.DataFrame(grid.cv_results_)[['params', \n                           'mean_test_score', 'std_test_score']].loc[grid.best_index_])\n    if verbose:\n        print(\"_\"*40)\n        print(pd.DataFrame(grid.cv_results_)[['params','mean_test_score','std_test_score']])\n    return best_model\n\ndef pretty_confusion(model, train, target, cv=5):\n    prediction = cross_val_predict(model, train, target, cv=cv)\n    print(classification_report(target, prediction))\n    cm = confusion_matrix(target, prediction)\n\n    cmap = plt.cm.Blues\n    classes = [0,1]\n    thresh = cm.max() \/ 2.\n    fmt = 'd'\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n        \ndef get_feature_importance(clsf, ftrs):\n    imp = clsf.steps[-1][1].feature_importances_.tolist() #it's a pipeline\n    feats = ftrs\n    result = pd.DataFrame({'feat':feats,'score':imp})\n    result = result.sort_values(by=['score'],ascending=False)\n    return result\n\ndef learning_print(estimator, train, label, folds):\n    \"\"\"\n    estimator: an estimator\n    train: set with input data\n    label: target variable\n    folds: cross validation\n    \"\"\"\n    train_sizes = np.arange(0.1, 1, 0.05)\n    train_sizes, train_scores, validation_scores = learning_curve(\n                                                   estimator = estimator, X = train,\n                                                   y = label, train_sizes = train_sizes, cv = folds,\n                                                   scoring = 'accuracy')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    validation_scores_mean = np.mean(validation_scores, axis=1)\n    validation_scores_std = np.std(validation_scores, axis=1)\n    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    ax1.plot(train_sizes, train_scores_mean, label = 'Training accuracy')\n    ax1.plot(train_sizes, validation_scores_mean, label = 'Validation accuracy')\n    ax2.plot(train_sizes, train_scores_std, label = 'Training std')\n    ax2.plot(train_sizes, validation_scores_std, label = 'Validation std')\n    ax1.legend()\n    ax2.legend()","013aa1ad":"# Things we are going to need all the time\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=14)\n\nimputer = ('imputer', SimpleImputer())","b9e8c2f9":"prtn_cols = ['field_cd', 'imprace', 'imprelig', 'goal',\n       'date', 'go_out', 'sports', 'tvsports', 'exercise', 'dining', 'museums',\n       'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n       'movies', 'concerts', 'music', 'shopping', 'yoga']\n\ntmp = df_clean[['iid', 'pid']+ prtn_cols].copy()\ntmp = tmp.rename(columns={'iid' : 'pid', 'pid': 'iid'}) # reversing iid and pid in the temporary frame\n\nrenaming = {col : col+'_o' for col in prtn_cols} \ntmp = tmp.rename(columns=renaming)\n\ndf_clean = pd.merge(df_clean, tmp, on=['iid', 'pid'])\n\ndel tmp\n\n# checking it went well\ndf_clean[((df_clean.iid == 1) & (df_clean.pid == 11)) | \n         ((df_clean.iid == 11) & (df_clean.pid == 1))][['iid', 'pid', 'gender', 'museums', 'field_cd', 'museums_o', 'field_cd_o']]","1827bee3":"# waves where the points were given differently\ndf_clean['vote_differently'] = 0\ndf_clean.loc[(df_clean.wave > 5) & (df_clean.wave < 10), 'vote_differently'] = 1\n\n# wave where they could chose only 50%\ndf_clean['on_a_budget'] = 0\ndf_clean.loc[df_clean.wave == 12, 'on_a_budget'] = 1\n\n# waves where they were asked to come with a magazine\ndf_clean['with_magazine'] = 0\ndf_clean.loc[(df_clean.wave == 18) | (df_clean.wave == 21), 'with_magazine'] = 1\n\n# waves where they were asked to come with a book\ndf_clean['with_book'] = 0\ndf_clean.loc[(df_clean.wave == 19) | (df_clean.wave == 20), 'with_book'] = 1","3ddbb088":"for col in interests:\n    # difference in opinion about an interest\n    df_clean[col+'_sh'] = (df_clean[col] - df_clean[col+'_o'])","70a14718":"# relative age difference\ndf_clean['age_diff'] = (df_clean['age'] - df_clean['age_o'])\ndf_clean.loc[(df_clean.age_diff > 0) & (df_clean.gender == 'Male'), 'age_diff_cat'] = 'older_male'\ndf_clean.loc[(df_clean.age_diff > 0) & (df_clean.gender == 'Female'), 'age_diff_cat'] = 'older_female'\ndf_clean.loc[(df_clean.age_diff < 0) & (df_clean.gender == 'Male'), 'age_diff_cat'] = 'younger_male'\ndf_clean.loc[(df_clean.age_diff < 0) & (df_clean.gender == 'Female'), 'age_diff_cat'] = 'younger_female'\ndf_clean.loc[(df_clean.age_diff == 0), 'age_diff_cat'] = 'same_age'\n\n# who remembers if they met\ndf_clean['met_comp'] = df_clean['met'] - df_clean['met_o']\ndf_clean.loc[df_clean.met_comp == 0, 'met_comp'] = 'same'\ndf_clean.loc[df_clean.met_comp == 1, 'met_comp'] = 'partec'\ndf_clean.loc[df_clean.met_comp == -1, 'met_comp'] = 'partner'\n\n# same field flag\ndf_clean['same_field'] = 0\ndf_clean.loc[df_clean.field_cd == df_clean.field_cd_o, 'same_field'] = 1\n\n# same goal flag\ndf_clean['same_goal'] = 0\ndf_clean.loc[df_clean.goal == df_clean.goal_o, 'same_goal'] = 1","ce6ea6c3":"test = df_clean[['iid', 'order', 'like']].copy().sort_values(by=['order']).fillna(6) \n# fill the missing with neutral vote\n\n# How good is the night in general?\ntest['like_cumulative'] = test.groupby(['iid'])['like'].cumsum()\ntest['mean_liking'] = test.like_cumulative \/ test.order\n\n# How good it the person with respect to the mean\ntest['like_spread'] = test.like.fillna(6) - test.mean_liking\n\n# Did they encounter someone they liked a lot already?\ntest['high_like'] = 0\ntest.loc[test.like > 8, 'high_like'] = 1\ntest['high_like'] = test.groupby('iid')['high_like'].cumsum()\ntest.loc[test.high_like > 1, 'high_like'] = 1\n\n# removing columns\ndel test['like_cumulative']\ndel test['like']\n\n# putting together\ndf_clean = pd.merge(df_clean, test, on=['iid', 'order'])\n\ndel test","ee2968d6":"general_info = ['gender', 'condtn', 'order', 'with_book', 'with_magazine', 'on_a_budget', 'vote_differently']\nparticipant_info = ['age', 'met', 'field_cd', 'race', 'imprace', \n                    'imprelig', 'goal', 'date', 'go_out', 'exphappy'] + you_look + you_score + opp_look\npartner_info = [col for col in df_clean.columns if 'pf_o_' in col] + \\\n                ['age_o', 'met_o', 'field_cd_o', 'race_o', 'imprace_o', 'imprelig_o',\n                'goal_o', 'date_o', 'go_out_o']\ninterests_o = [col+'_o' for col in interests]\nvotes = ['attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob'] \nvotes_o = ['attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o', 'like_o', 'prob_o']\nspecial_votes = ['mean_liking', 'like_spread', 'high_like']\nshared = [col+'_sh' for col in interests] + ['int_corr', 'samerace', 'age_diff', \n                                             'age_diff_cat', 'met_comp', 'same_field', 'same_goal']\ntarget = df_clean['dec']\n\n\n# Creating frames\ndf_participants = df_clean[general_info+participant_info+interests+votes+special_votes].copy() \ndf_partner = df_clean[general_info+partner_info+interests_o+votes_o].copy()\ndf_both = df_clean[general_info+participant_info+partner_info+\n                   interests+votes+special_votes+interests_o+votes_o+shared].copy()","b82928d6":"df_train = pd.get_dummies(df_participants, \n                          columns=['gender'], \n                          drop_first=True)\n\n# things to drop because it improves the model\ndel df_train['race']\ndel df_train['like_spread']\ndel df_train['goal']\ndel df_train['field_cd']\ndel df_train['with_book']\ndel df_train['date'] \ndel df_train['condtn']\n\nfreq_map = {'Almost_never' : 0,\n           'Several_py': 1, \n           'Once_pm': 2,\n           'Twice_pm': 3,\n           'Once_pw': 4,\n           'Twice_pw': 5,\n           'Several_pw': 6}\n\ndf_train.go_out = df_train.go_out.map(freq_map)\n\npartic_cols = df_train.columns  # will be needed in the last section\n\ndf_train.head()","9899a38d":"pipe = Pipeline([imputer, \n                 ('xgb', XGBClassifier(n_jobs=1, random_state=42, \n                                       subsample=0.8))])\n\nparam_grid = [{'xgb__n_estimators' : [900, 1000, 1100],\n               'xgb__learning_rate': [0.1, 0.05, 0.01]}]\n\nbest_participant = pretty_grid(pipe, param_grid, df_train, target,\n                         'accuracy', cv=kfolds, n_jobs=-1) ","6bb92f64":"best_participant.steps[1][1]","48f9ed1e":"pretty_confusion(best_participant, df_train, target, kfolds)","520632bc":"learning_print(best_participant, df_train, target, kfolds)  ","bd3ae6b4":"get_feature_importance(best_participant, df_train.columns).head(10)","5a28be98":"get_feature_importance(best_participant, df_train.columns).tail(10)","8420db3e":"predictions_participants_only = cross_val_predict(best_participant, df_train, target, cv=kfolds)","a652c337":"participants_analysis = pd.concat([df_participants, \n                                   pd.DataFrame({'target':target, \n                                                 'prediction':predictions_participants_only})], axis=1)\n\nFP = participants_analysis[(participants_analysis.target == 0) & (participants_analysis.prediction == 1)].copy()\nFN = participants_analysis[(participants_analysis.target == 1) & (participants_analysis.prediction == 0)].copy()\ngood_pred = participants_analysis[participants_analysis.target == participants_analysis.prediction].copy()","621e9580":"print(\"Target mean for full dataset: {}\".format(round(participants_analysis.target.mean(),3)))\nprint(\"Prediction mean for full dataset: {}\".format(round(participants_analysis.prediction.mean(),3)))\nprint(\"Target mean for correct predictions: {}\".format(round(good_pred.target.mean(),3)))","de664180":"to_check = ['fun', 'concerts', 'movies', 'theater', 'clubbing', 'art', 'museums']","aa84104b":"col = to_check[5]\n\nfig, ax= plt.subplots(2,2, figsize=(15, 12))\n\nparticipants_analysis[participants_analysis.target==1][col].hist(ax=ax[0][0], label='yes', alpha=0.5)\nparticipants_analysis[participants_analysis.target==0][col].hist(ax=ax[0][0], label='no', alpha=0.5)\nparticipants_analysis[participants_analysis.prediction==1][col].hist(ax=ax[0][1], label='yes', alpha=0.5)\nparticipants_analysis[participants_analysis.prediction==0][col].hist(ax=ax[0][1], label='no', alpha=0.5)\nFP[FP.target==0][col].hist(ax=ax[1][0], label='FP', alpha=0.5)\nFN[FN.target==1][col].hist(ax=ax[1][0], label='FN', alpha=0.5)\ngood_pred[good_pred.target==1][col].hist(ax=ax[1][1], label='yes', alpha=0.5)\ngood_pred[good_pred.target==0][col].hist(ax=ax[1][1], label='no', alpha=0.5)\nax[0][0].set_title('Dist. '+col+' vs. target')\nax[0][1].set_title('Dist. '+col+' vs. prediction')\nax[1][0].set_title('Dist. '+col+' FP\/FN vs. target')\nax[1][1].set_title('Dist. '+col+' Correct predictions vs. target')\nax[0][0].legend()\nax[1][0].legend()\nax[1][1].legend()\nax[0][1].legend()\nax[0][0].grid(False)\nax[0][1].grid(False)\nax[1][0].grid(False)\nax[1][1].grid(False)\n","905df4a1":"df_train = pd.get_dummies(df_partner, \n                          columns=['gender', 'race_o', 'goal_o', 'field_cd_o'], \n                          drop_first=True)\n\n# to improve the model\ndel df_train['go_out_o']\n\nfreq_map = {'Almost_never' : 0,\n           'Several_py': 1, \n           'Once_pm': 2,\n           'Twice_pm': 3,\n           'Once_pw': 4,\n           'Twice_pw': 5,\n           'Several_pw': 6}\n\ndf_train.date_o = df_train.date_o.map(freq_map)\n\ncol_partn = df_train.columns  # will be needed in the last section\n\ndf_train.head()","022db4bf":"pipe = Pipeline([imputer, \n                 ('xgb', XGBClassifier(n_jobs=1, random_state=42, \n                                       subsample=0.8))])\n\nparam_grid = [{'xgb__n_estimators' : [1000, 1500, 2000],\n               'xgb__learning_rate': [0.1, 0.05, 0.01]}]\n\nbest_partner = pretty_grid(pipe, param_grid, df_train, target,\n                         'accuracy', cv=kfolds, n_jobs=-1) ","59ea10f6":"best_partner.steps[1][1]","5aeacb86":"pretty_confusion(best_partner, df_train, target, kfolds)","3e9b8b9f":"learning_print(best_partner, df_train, target, kfolds)","67aca5e7":"get_feature_importance(best_partner, df_train.columns).head(10)","5c02e988":"get_feature_importance(best_partner, df_train.columns).tail(10)","4a6bfc6d":"predictions_partner_only = cross_val_predict(best_partner, df_train, target, cv=kfolds)\n\npartner_analysis = pd.concat([df_partner, \n                                   pd.DataFrame({'target':target}), \n                                   pd.DataFrame({'prediction':predictions_partner_only})], axis=1)\n\nFP = partner_analysis[(partner_analysis.target == 0) & (partner_analysis.prediction == 1)].copy()\nFN = partner_analysis[(partner_analysis.target == 1) & (partner_analysis.prediction == 0)].copy()\ngood_pred = partner_analysis[partner_analysis.target == partner_analysis.prediction].copy()","6d53e4d8":"print(\"Target mean for full dataset: {}\".format(round(partner_analysis.target.mean(),3)))\nprint(\"Prediction mean for full dataset: {}\".format(round(partner_analysis.prediction.mean(),3)))\nprint(\"Target mean for correct predictions: {}\".format(round(good_pred.target.mean(),3)))","542dd7aa":"to_check = ['imprace_o', 'attr_o', 'prob_o', 'gender', 'goal_o', 'date_o']","3ad033ca":"col = to_check[1]\n\nfig, ax= plt.subplots(2,2, figsize=(15, 12))\n\npartner_analysis[partner_analysis.target==1][col].hist(ax=ax[0][0], label='yes', alpha=0.5)\npartner_analysis[partner_analysis.target==0][col].hist(ax=ax[0][0], label='no', alpha=0.5)\npartner_analysis[partner_analysis.prediction==1][col].hist(ax=ax[0][1], label='yes', alpha=0.5)\npartner_analysis[partner_analysis.prediction==0][col].hist(ax=ax[0][1], label='no', alpha=0.5)\nFP[FP.target==0][col].hist(ax=ax[1][0], label='FP', alpha=0.5)\nFN[FN.target==1][col].hist(ax=ax[1][0], label='FN', alpha=0.5)\ngood_pred[good_pred.target==1][col].hist(ax=ax[1][1], label='yes', alpha=0.5)\ngood_pred[good_pred.target==0][col].hist(ax=ax[1][1], label='no', alpha=0.5)\nax[0][0].set_title('Dist. '+col+' vs. target')\nax[0][1].set_title('Dist. '+col+' vs. prediction')\nax[1][0].set_title('Dist. '+col+' FP\/FN vs. target')\nax[1][1].set_title('Dist. '+col+' Correct predictions vs. target')\nax[0][0].legend()\nax[1][0].legend()\nax[1][1].legend()\nax[0][1].legend()\nax[0][0].grid(False)\nax[0][1].grid(False)\nax[1][0].grid(False)\nax[1][1].grid(False)\n","76732fff":"col = to_check[3]\n\nfig, ax= plt.subplots(2,2, figsize=(15, 12))\n\npartner_analysis[col].value_counts().plot(kind='bar', ax=ax[0][0])\ngood_pred[col].value_counts().plot(kind='bar', ax=ax[0][1])\nFN[col].value_counts().plot(kind='bar', ax=ax[1][0])\nFP[col].value_counts().plot(kind='bar', ax=ax[1][1])\nax[0][0].set_title('Original Count vs. '+col)\nax[0][1].set_title('Correct Predictions Count vs. '+col)\nax[1][0].set_title('False Negatives Count vs. '+col)\nax[1][1].set_title('False Positives Count vs. '+col)\nax[0][0].set_xticklabels(ax[0][0].get_xticklabels(), fontsize=10, rotation='horizontal')\nax[0][1].set_xticklabels(ax[0][1].get_xticklabels(), fontsize=10, rotation='horizontal')\nax[1][0].set_xticklabels(ax[1][0].get_xticklabels(), fontsize=10, rotation='horizontal')\nax[1][1].set_xticklabels(ax[1][1].get_xticklabels(), fontsize=10, rotation='horizontal')","962cd54a":"df_train = pd.get_dummies(df_both, \n                          columns=['gender', 'race', 'field_cd', \n                                   'race_o', 'field_cd_o'], \n                          drop_first=True)\n\n# Things that help the model\ndel df_train['goal']\ndel df_train['like_spread']\ndel df_train['go_out_o']\ndel df_train['date_o']\ndel df_train['goal_o']\ndel df_train['date']\ndel df_train['met_comp']\ndel df_train['age_diff_cat']\n\nfreq_map = {'Almost_never' : 0,\n           'Several_py': 1, \n           'Once_pm': 2,\n           'Twice_pm': 3,\n           'Once_pw': 4,\n           'Twice_pw': 5,\n           'Several_pw': 6}\n\ndf_train.go_out = df_train.go_out.map(freq_map)\n\nboth_cols = df_train.columns\n\ndf_train.head()","df347560":"pipe = Pipeline([imputer, \n                 ('xgb', XGBClassifier(n_jobs=1, random_state=42, \n                                       subsample=0.8))])\n\nparam_grid = [{'xgb__n_estimators' : [1000, 1500, 2000],\n               'xgb__learning_rate': [0.1, 0.05, 0.01]}]\n\nbest_both = pretty_grid(pipe, param_grid, df_train, target,\n                         'accuracy', cv=kfolds, n_jobs=-1) ","bac6c8b0":"best_both.steps[-1][1]","cbda7953":"pretty_confusion(best_both, df_train, target, kfolds)","2e5e1165":"learning_print(best_both, df_train, target, kfolds)","42151284":"get_feature_importance(best_both, df_train.columns).head(10)","6f775235":"get_feature_importance(best_both, df_train.columns).tail(10)","56ed687f":"predictions_both = cross_val_predict(best_both, df_train, target, cv=kfolds)","90ef2bd7":"both_analysis = pd.concat([df_both, \n                                   pd.DataFrame({'target':target}), \n                                   pd.DataFrame({'prediction':predictions_both})], axis=1)\n\nFP = both_analysis[(both_analysis.target == 0) & (both_analysis.prediction == 1)].copy()\nFN = both_analysis[(both_analysis.target == 1) & (both_analysis.prediction == 0)].copy()\ngood_pred = both_analysis[both_analysis.target == both_analysis.prediction].copy()\n\nprint(\"Target mean for full dataset: {}\".format(round(both_analysis.target.mean(),3)))\nprint(\"Prediction mean for full dataset: {}\".format(round(both_analysis.prediction.mean(),3)))\nprint(\"Target mean for correct predictions: {}\".format(round(good_pred.target.mean(),3)))","bdb09458":"to_check = ['age_diff_cat', 'imprace_o', 'museums', 'art', \n            'clubbing','theater',  'movies', 'concerts', \n            'attr', 'sinc','like_spread','art_sh']","193d001f":"col = to_check[8]\n\nfig, ax= plt.subplots(2,2, figsize=(15, 12))\n\nboth_analysis[both_analysis.target==1][col].hist(ax=ax[0][0], label='yes', alpha=0.5)\nboth_analysis[both_analysis.target==0][col].hist(ax=ax[0][0], label='no', alpha=0.5)\nboth_analysis[both_analysis.prediction==1][col].hist(ax=ax[0][1], label='yes', alpha=0.5)\nboth_analysis[both_analysis.prediction==0][col].hist(ax=ax[0][1], label='no', alpha=0.5)\nFP[FP.target==0][col].hist(ax=ax[1][0], label='FP', alpha=0.5)\nFN[FN.target==1][col].hist(ax=ax[1][0], label='FN', alpha=0.5)\ngood_pred[good_pred.target==1][col].hist(ax=ax[1][1], label='yes', alpha=0.5)\ngood_pred[good_pred.target==0][col].hist(ax=ax[1][1], label='no', alpha=0.5)\nax[0][0].set_title('Dist. '+col+' vs. target')\nax[0][1].set_title('Dist. '+col+' vs. prediction')\nax[1][0].set_title('Dist. '+col+' FP\/FN vs. target')\nax[1][1].set_title('Dist. '+col+' Correct predictions vs. target')\nax[0][0].legend()\nax[1][0].legend()\nax[1][1].legend()\nax[0][1].legend()\nax[0][0].grid(False)\nax[0][1].grid(False)\nax[1][0].grid(False)\nax[1][1].grid(False)","e5b6c0e0":"col = to_check[9]\n\nfig, ax= plt.subplots(2,2, figsize=(15, 12))\n\nboth_analysis[both_analysis.target==1][col].hist(ax=ax[0][0], label='yes', alpha=0.5)\nboth_analysis[both_analysis.target==0][col].hist(ax=ax[0][0], label='no', alpha=0.5)\nboth_analysis[both_analysis.prediction==1][col].hist(ax=ax[0][1], label='yes', alpha=0.5)\nboth_analysis[both_analysis.prediction==0][col].hist(ax=ax[0][1], label='no', alpha=0.5)\nFP[FP.target==0][col].hist(ax=ax[1][0], label='FP', alpha=0.5)\nFN[FN.target==1][col].hist(ax=ax[1][0], label='FN', alpha=0.5)\ngood_pred[good_pred.target==1][col].hist(ax=ax[1][1], label='yes', alpha=0.5)\ngood_pred[good_pred.target==0][col].hist(ax=ax[1][1], label='no', alpha=0.5)\nax[0][0].set_title('Dist. '+col+' vs. target')\nax[0][1].set_title('Dist. '+col+' vs. prediction')\nax[1][0].set_title('Dist. '+col+' FP\/FN vs. target')\nax[1][1].set_title('Dist. '+col+' Correct predictions vs. target')\nax[0][0].legend()\nax[1][0].legend()\nax[1][1].legend()\nax[0][1].legend()\nax[0][0].grid(False)\nax[0][1].grid(False)\nax[1][0].grid(False)\nax[1][1].grid(False)","ce78ccd0":"full_analysis = pd.concat([df_both,\n                           pd.DataFrame({'target':target, \n                                        'pred_partic': predictions_participants_only,\n                                         'pred_partner': predictions_partner_only, \n                                         'pred_both': predictions_both})], axis=1)\n\nprint(\"Correlation between target and models predictions\")\nfull_analysis[['target','pred_partic', 'pred_partner', 'pred_both']].corr()","ad3af847":"print('Target mean: {}'.format(round(full_analysis.target.mean(), 2)))\nprint('Mean predictions of the partner model: {}'.format(round(full_analysis.pred_partner.mean(), 2)))\nprint('Mean predictions of the participant model: {}'.format(round(full_analysis.pred_partic.mean(), 2)))\nprint('Mean predictions of the full model: {}'.format(round(full_analysis.pred_both.mean(), 2)))","e308ac30":"cats = ['condtn', 'with_book', 'with_magazine', 'on_a_budget', \n        'met', 'field_cd', 'race', 'goal', 'date', 'go_out', 'field_cd_o', 'race_o',\n       'goal_o', 'date_o', 'go_out_o', 'high_like', 'samerace', 'met_comp', \n        'same_field', 'same_goal']\nmodes = full_analysis[cats].mode()\ncats += ['gender', 'age_diff_cat']\nmeans = pd.DataFrame(full_analysis[[col for col in full_analysis.columns if col not in cats]].mean()).transpose()\ntypical = pd.concat([means, modes], axis=1)\ntypical['gender'] = 'Male'\ntypical","a16b68b7":"def both_cont_var(data, column, range_list, change_gender=None, change_race=None):\n    var = []\n    tmp = data.copy()\n    for val in range_list:\n        tmp[column] = val\n\n        df_train = pd.get_dummies(tmp, \n                                  columns=['gender', 'race', 'field_cd', \n                                           'race_o', 'field_cd_o'])\n\n        del df_train['goal']\n        del df_train['like_spread']\n        del df_train['go_out_o']\n        del df_train['date_o']\n        del df_train['goal_o']\n        del df_train['date']\n        del df_train['met_comp']\n\n        freq_map = {'Almost_never' : 0,\n                   'Several_py': 1, \n                   'Once_pm': 2,\n                   'Twice_pm': 3,\n                   'Once_pw': 4,\n                   'Twice_pw': 5,\n                   'Several_pw': 6}\n\n        df_train.go_out = df_train.go_out.map(freq_map)\n\n        df_train['race_Hispanic'] = 0\n        df_train['race_Other'] = 0\n        df_train['race_Black'] = 0\n        df_train['field_cd_Educ.'] = 0\n        df_train['field_cd_Eng.'] = 0\n        df_train['field_cd_Film'] = 0\n        df_train['field_cd_Hist.'] = 0\n        df_train['field_cd_Journ.'] = 0\n        df_train['field_cd_Law'] = 0\n        df_train['field_cd_Math'] = 0\n        df_train['field_cd_Med. Sc.'] = 0\n        df_train['field_cd_Nat. Sc.'] = 0\n        df_train['field_cd_Oth.'] = 0\n        df_train['field_cd_Pol. Sc.'] = 0\n        df_train['field_cd_Soc. Sc.'] = 0\n        df_train['field_cd_Soc. Wr.'] = 0\n        df_train['race_o_Hispanic'] = 0\n        df_train['race_o_Other'] = 0\n        df_train['race_o_Black'] = 0\n        df_train['field_cd_o_Educ.'] = 0\n        df_train['field_cd_o_Eng.'] = 0\n        df_train['field_cd_o_Film'] = 0\n        df_train['field_cd_o_Hist.'] = 0\n        df_train['field_cd_o_Journ.'] = 0\n        df_train['field_cd_o_Law'] = 0\n        df_train['field_cd_o_Math'] = 0\n        df_train['field_cd_o_Med. Sc.'] = 0\n        df_train['field_cd_o_Nat. Sc.'] = 0\n        df_train['field_cd_o_Oth.'] = 0\n        df_train['field_cd_o_Pol. Sc.'] = 0\n        df_train['field_cd_o_Soc. Sc.'] = 0\n        df_train['field_cd_o_Soc. Wr.'] = 0\n        \n        df_train = df_train[both_cols]\n        \n        if change_gender:\n            df_train['gender_Male'] = 0\n            \n        if change_race == 'Black':\n            df_train['race_Hispanic'] = 0\n            df_train['race_Other'] = 0\n            df_train['race_Black'] = 1\n            df_train['race_White'] = 0\n        elif change_race == 'Other':\n            df_train['race_Hispanic'] = 0\n            df_train['race_Other'] = 1\n            df_train['race_Black'] = 0\n            df_train['race_White'] = 0\n        elif change_race == 'Hispanic':\n            df_train['race_Hispanic'] = 1\n            df_train['race_Other'] = 0\n            df_train['race_Black'] = 0\n            df_train['race_White'] = 0\n\n        var.append(best_both.predict(df_train)[0])\n        \n    return var\n\n\ndef partn_cont_var(data, column, range_list, change_gender=None, change_race=None):\n    var = []\n    tmp = data.copy()\n    for val in range_list:\n        tmp[column] = val\n        df_train = pd.get_dummies(tmp, \n                                  columns=['gender', 'race_o',  'goal_o', 'field_cd_o'])\n\n        del df_train['go_out_o']\n\n        freq_map = {'Almost_never' : 0,\n                   'Several_py': 1, \n                   'Once_pm': 2,\n                   'Twice_pm': 3,\n                   'Once_pw': 4,\n                   'Twice_pw': 5,\n                   'Several_pw': 6}\n\n        df_train.date_o = df_train.date_o.map(freq_map)\n        \n        df_train['race_o_Hispanic'] = 0\n        df_train['race_o_Other'] = 0\n        df_train['race_o_Black'] = 0\n        df_train['field_cd_o_Educ.'] = 0\n        df_train['field_cd_o_Eng.'] = 0\n        df_train['field_cd_o_Film'] = 0\n        df_train['field_cd_o_Hist.'] = 0\n        df_train['field_cd_o_Journ.'] = 0\n        df_train['field_cd_o_Law'] = 0\n        df_train['field_cd_o_Math'] = 0\n        df_train['field_cd_o_Med. Sc.'] = 0\n        df_train['field_cd_o_Nat. Sc.'] = 0\n        df_train['field_cd_o_Oth.'] = 0\n        df_train['field_cd_o_Pol. Sc.'] = 0\n        df_train['field_cd_o_Soc. Sc.'] = 0\n        df_train['field_cd_o_Soc. Wr.'] = 0\n        df_train['goal_o_IdidIt'] = 0\n        df_train['goal_o_Meet'] = 0\n        df_train['goal_o_Other'] = 0\n        df_train['goal_o_Relationship'] = 0\n\n        df_train = df_train[col_partn]\n        \n        if change_gender:\n            df_train['gender_Male'] = 0\n            \n        if change_race == 'Black':\n            df_train['race_o_Hispanic'] = 0\n            df_train['race_o_Other'] = 0\n            df_train['race_o_Black'] = 1\n            df_train['race_o_White'] = 0\n        elif change_race == 'Other':\n            df_train['race_o_Hispanic'] = 0\n            df_train['race_o_Other'] = 1\n            df_train['race_o_Black'] = 0\n            df_train['race_o_White'] = 0\n        elif change_race == 'Hispanic':\n            df_train['race_o_Hispanic'] = 1\n            df_train['race_o_Other'] = 0\n            df_train['race_o_Black'] = 0\n            df_train['race_o_White'] = 0\n\n        var.append(best_partner.predict(df_train)[0])\n        \n    return var\n\n\ndef partic_cont_var(data, column, range_list, change_gender=None, change_race=None):\n    var = []\n    tmp = data.copy()\n    for val in range_list:\n        tmp[column] = val\n\n        df_train = pd.get_dummies(data, \n                          columns=['gender'])\n\n        del df_train['race']\n        del df_train['like_spread']\n        del df_train['goal']\n        del df_train['field_cd']\n        del df_train['with_book']\n        del df_train['date'] \n        del df_train['condtn']\n\n\n        freq_map = {'Almost_never' : 0,\n                   'Several_py': 1, \n                   'Once_pm': 2,\n                   'Twice_pm': 3,\n                   'Once_pw': 4,\n                   'Twice_pw': 5,\n                   'Several_pw': 6}\n\n        df_train.go_out = df_train.go_out.map(freq_map)\n        \n        df_train = df_train[partic_cols]\n        \n        if change_gender:\n            df_train['gender_Male'] = 0\n\n        var.append(best_participant.predict(df_train)[0])\n        \n    return var","09efdcc0":"res_hisp_black = pd.Series(both_cont_var(typical, 'like', np.arange(1, 11), change_race='Hispanic'))\nres_other = pd.Series(both_cont_var(typical, 'like', np.arange(1, 11), change_race='Other'))\n\nres_hisp_black.plot(label='Hispanic and Black', figsize=(12,6))\nres_other.plot(label='Other race', figsize=(10,6))\n\nplt.legend()\nplt.title('Full model decision: like vs race', fontsize=18)\nplt.xlabel('Like', fontsize=12)\nplt.ylabel('Decision', fontsize=12)","047e1bea":"res_att_H_W = pd.Series(partn_cont_var(typical, 'attr_o', np.arange(1, 11), change_race='Hispanic'))\nres_att_B = pd.Series(partn_cont_var(typical, 'attr_o', np.arange(1, 11), change_race='Black'))\nres_att_O = pd.Series(partn_cont_var(typical, 'attr_o', np.arange(1, 11), change_race='Other'))\n\n\nres_att_H_W.plot(label='Hispanic and White', figsize=(12,6))\nres_att_B.plot(label='Black', figsize=(10,6))\nres_att_O.plot(label='Other race', figsize=(10,6))\n\nplt.legend()\nplt.title('Partner model decision: attr_o vs race', fontsize=18)\nplt.xlabel('attr_o', fontsize=12)\nplt.ylabel('Decision', fontsize=12)","14ea9b49":"As we see in this example, the model is more incline than it should in predicting `no` when there are high values of the feature `art`. Similar patterns can be found in the other features collected in the list `to_check`. \n\nNo visible pattern was found in any other variable (including those not used by the model, like race), giving me a lot of confidence in how those variables were used. It remains open the question about the role of `order` since I can't spot any pattern or correlation with the target. \n\nDropping those features would make our model less accurate (by not much, being not important features).","52610da7":"# Introduction\n\nWhat is the trigger that makes us think *hey, I like this person*? \n\nHow do we see ourselves in appealing the interest of people of the opposite gender?\n\nWhat makes us say *yes* to someone we met 4 minutes ago?\n\nThis notebook will use the data from the speed dating experiment to explore various ways of answering these questions. In doing so, we will observe how participants behaved and thought differently in relation with their gender or their race. We will see how different was the perception they had about themselves with respect to the one they had of the others. We will see how they, consciously or not, lie about their thoughts.\n\nThe journey to get to these answers will go through the following steps:\n* **Who are the participants?** Age, gender, race, field of study, and more will be explored here, finding some unexpected differences in various segments of this sample of people.\n* **Breaking the Waves**. The experimental settings (rules of the dating, rating system) changed across various sessions, can we spot differences in how people rated their partners or simply decided to say *yes* when the rules changes? (This section will be expanded further separately since it can be of interest in terms of questionnaire preparation issues)\n* **Rating and deciding**. After 4 minutes talking with a stranger, each participant graded their experience and took a decision, what patterns were hiding in there?\n* **Machines can love too**. Giving who you are and who you have in front, can a dumb heartless machine predict your decision? What do machines get right? What do they get wrong?\n\n***Note***: I have no way to prove that this group of people is in any way representative of the population and we should all assume it isn't. For how careful I want to be in expressing myself in a way that takes into account this fact, from time to time, either to make the sentence more fluent or because I am lazy, I might say things like *Males likes watching sports on the tv more than Females*. For how true this statement is for the 850 people here, there is no scientific reason to assume that this works for all the males and females. In other words, the words *in this sample* are very much implied every single time.","2d13102b":"A great majority of business\/finance\/econ, followed by Natural Sciences (chemistry, physics) and Engineering. All of them very much male-dominated.","e14cddc5":"## Two hearts are better than one\n\nHere we finally keep into account the entire situation, we know everything about the date and we try to see what one of the two will decide.","3eb6551f":"Looking at the remaining missing values, it looks like we have mainly 3 situations:\n\n* missing entries about a participant (like age, etc.) in some or every date\n* missing entries about a partner in some or every date\n* missing evaluation about a date\n\nSince we know who the partners are, we can try to fill the missing entries about participants and partner by using the values we have for other dates (this assumes that those values are the same for every date, which is reasonable). \n\nWe have a problem with a missing partner id in wave 5. A quick inspection reveals that we have 10 males meeting 10 females but only 9 females meeting 10 males. In other words, there is a missing female and we have no way to know who that person is. Since our prediction wants to be based on who your partner is and for these dates this information is missing, we drop these 9 dates","8f9c93f6":"A little bit of a butchery rather than a cleaning, but it involves only a few participants and any other approach would have been more invasive.\n\nThe remaining missing values regarding who the participants are (thus not their evaluation of the dates) will be kept as simple as possible. To avoid information leakage and keep our cross-validation trustable, we will impute the missing values inside of the pipeline.\n\nLet's move on to the date-related missing values.\n\nThe first one is `met` which should take values 1 and 2 for No\/Yes answers but it looks fairly inconsistent with the documentation or with the counterpart `met_o`. Using one feature to impute the other is pointless since they do not necessarily have to agree. We can simply impute the missing values with 0 and adjust the columns so that they respect the documentation.\n\nOther small mistakes were found in other columns, we correct those too","d658f520":"The model performs very similarly to the one with only the information about the participants and it definitely would benefit of more observations. As in the previous case, dropping the features about the participant's votes, would make the model perform significantly worse (around 74%) but still better than all the others when these features are dropped.\n\nAgain, ignoring the `like_spread` feature (which is built in the *wrong way* as discussed above) helps the model and, again, field of study and race are less important. With a larger set of features, `order` drops down in importance but stays at a spot I can't explain. Although attractiveness (see later there is a caveat with this) and general liking are still the most important predictors, now we see that the other attributes are relatively less important than before.\n\nAt last, what this one did wrong?","3506b340":"Every race, except for Asians and \"Other\" (which can include same race couples and we have no way to find out), sees a positive effect of being of the same race. Particularly impressive the effect on Black participants but we can't ignore that there are very few cases when 2 black individuals dated at these events.","2bdd3170":"At last, in `field_cd` some entries are very unfrequent, even less than the *Other* category. We can group them all under *Other*.","d6b2ccaf":"## Goals and habits\n\nIn this subsection, we will see what the participants declared to be their intentions to be and what they usually do in terms of going out and dating.","a013ed69":"On the 2 extremes (Several times per week and almost never) the two genders look very similar with some difference in going out once or twice per week and once or twice per month. \n","74bcd5da":"Most of the people declare to be there to have fun and meet new people. Let's see if there is some trend in gender or race.","fc4b8f0a":"# Breaking the Waves\n\nAs the last example showed us, there can be some differences in the way the data were collected across the waves. Moreover, the documentation already tells us that the participants could express their preferences using different scales in different waves (1- 10 or assign 100 points across various categories), or that in wave 12 they could say yes to only half of the people they met, or that sometimes they had to bring a book or a magazine. All these factors can influence our analysis and our models. \n\nWe will explore some of these effects in this section.","4922b0a9":"There is no much difference in the averages, but it is easy to notice that a much larger proportion of males are very very confident while Females are more common in expecting a 50\/50 success rate. \n\nI find curious also how a very low expectation (0 or 1) is expressed differently across genders, with Females preferring to set 0 and Males setting 1.\n\nAt last, in asking 500 people to rate an expecation from 1 to 20,  there are some values that are never picked, something that we might need to consider if we want to model on that.","142ad81a":"There are no big differences in age, but we can see that the males at these events are slightly older than the female participants.\n\nNext, we have information about the **race** of the participants.","f9ac72cb":"As expected, higher rates lead to positive decisions, with the higher gaps in Attractive, and 'How much do you like them' (coincidentally it is also the same gap). We also notice, however, that positive feedback can arrive even if the score in some categories is very low, with the exception of Intelligence, where there is a minimum of 3 to get positive feedback. \n\nIt looks like being Fun is making quite the difference in the decision, while being Sincere looks not influential.\n\nLooking at the correlations: \n* it seems that the negative correlation between Attractive and Sincere or Intelligent (which are always positively correlated) is getting milder when the decision is positive (although the causal link can easily go in the opposite direction). \n* interesting to see how rating a person as ambitious or attractive is reducing the expectations of the participants about being liked by them. Not so confident after all.\n* when the decision is negative we can observe a positive correlation between the Like score and the Fun one that is not observed in cases of positive feedback (a fancy way of saying \"I like you as a friend\"?)\n\nInterestingly, the relation between higher ratings and being of the same race is not observed, not necessarily in contrast with what was discussed before. In other words, although we observed that dates between people of the same race were having a higher rate of positive feedback and we observed that positive feedback comes with generally higher grades, we can't say that people are giving to partners of the same race higher grades. **Being of the same race is expected to be a big factor in predicting the individual decisions**.","c02afa54":"For Caucasians (and \"Other\" race), more males than females, while we have the opposite trend for every other race. Moreover, for these 2 races, we can observe that males tend to be much older on average. Asian females are older than their male counterpart.\n\nWe can now look at what the participants are **studying**.","904af436":"We notice that, as expected, just knowing who we are, what we like, and if we are enjoying the date is making the model perform significantly worse in predicting if the other person will say *yes* to us. Again, nothing really shocking but it is nice to see that the performance is the same as the one of the previous model if we drop the votes. We could interpret the result by setting to 67-70% the accuracy of predicting a *yes* for a person on a date solely by considering their characteristics and preferences and not by considering how much they are actually enjoying the date (if this makes any sense).\n\nInterestingly, the number of dates happened before is very important for the model and, less surprisingly, how much we like and how much we value specific characteristics are very important as well.\n\nOn the other hand, race and field of study are not so important but cannot be dropped this time or we observe a drop in the score.\n\nAgain, we can see what went wrong","000c86e9":"# Who are the participants?\n\nHere, we explore the dataset in terms of those features that describe the participants. The goals are to get to know what is in this dataset and get some hints on what to focus on for further analysis.\n\nThese events were dedicated to partners of **opposite genders**. In particular, we have","a7e5576b":"This value seems to be very influenced by the gender of the participant consistently across all the races, with Caucasian females declaring the highest value and Black (and 'Other') participants with the highest gap between Males and Females. \n\nNext, we can focus on how important is the **religion**.","fa6980ce":"A few things pop out:\n\n* People clearly rated attractiveness in the first place, then realized that they had to score up to a 100 and focused on very simple multiples of 5 and 10. This has to be taken into account if we want to model on that: these features are not continuous.\n* The participants look for all the categories more or less equally in general, but having shared interests and being ambitious are generally less important.\n* People generally think that attractiveness is as important as other things (with some exceptions) and also that everyone else (same gender and opposite gender) value this aspect much more.\n\nLet's then see how they score themselves and how they think they are perceived","83562ba8":"While Engineering, Finance, and Math are mostly male-dominated, Social and Medical Sciences, Arts, and Journalism are more female dominated. Natural Sciences, Political Science, and Film are very much equally represented by both genders.","b5d99462":"Slightly more males than females. Let's see if we can see some differences age-wise","25ae1c6f":"Now, I need some helper (yet ugly) functions","b564238c":"Interestingly, with the exception of Sports, tv sports, and gaming, female participants have an higher average interest than their male counterparts in every activity.","e3181273":"There is not much going on here, we notice that not surpringly liking art comes with liking museums, liking music with liking concerts, and liking sports with liking watching sports (but no correlation with liking ecercises for male participants, as we see in the next graph). Moreover, theater and sports do not seem to appeal the same people, especially for Female participants.","e160bf38":"Participants that declare to go out almost never (for how scarce) are both Asians, while Caucasian participants are equally likely to go out twice or several times per week.\n\nAt last, let's see how is the dating life of our participants.","192b7892":"## Do we care about race and religion?\n\nIn this subsection, we will focus on what the participants declare to be the level of importance of race and religion for them. Thus, let's keep in mind that this is what the participants are declaring, which **does not necessarily correspond to the truth**. \n\nLater, we will look for a way to test if these values are trustworthy or not.","ece27f2f":"They are all **fairly confident**.","fc913de2":"The different count might be due to the presence of some *Na's* in the age feature","67025765":"We then drop everything that is missing quite consistently and does not refer to the partner.","7a1c424d":"A quick inspection reveals that in both groups (participants and partners) the missing values are consistently missing for the same individuals over and over. This is a pity for our imputation but makes our job much easier: we drop these individuals.","7ec69133":"## Interests\n\nHere, we look at what the participants declare to be their interest in various activities","5f4c3a7e":"It looks like the model is able to predict well whether or not a person will say *yes* at the end of a speed date if it knows everything about that person (including and especially how much is enjoying the date). Nothing shocking then. If we ignore the votes, a strong indicator of how much the participant is enjoying the date and a very strong predictor of the outcome, the accuracy **drops to 70%**, a result that, as we will see is perfectly in line with the next type of model.\n\nInteresting to see little importance given to the gender, indicating the model learns fairly equally the decisions of Males and Females, and that having met someone that we liked a lot or having met the partner before are equally unimportant. On the other hand, how well the night is going is the stronger predictor together with how good the date is. An intriguing result is the importance of `order`, which we could probably drop for better explainability of the model.\n\nMoreover, the model learns better if it doesn't consider the field of study, the dating habits, and, more surprisingly, how much the participant likes the person with respect to the previous dates and their race.\n\nLet's see what the model got right and what got wrong.","eb5bce02":"Now, this is interesting. We have seen that Females are less likely to give positive feedback (meaning, setting `dec=1`) than males. This is very evident in Caucasian females with a partner of a different race. This is very much in line with what was declared in the other question.\n\nWe have already noticed the high discrepancy between the declared importance and the feedback rate for Black individuals and this is again very evident for African American females since it appears they particularly liked the partners of the same race at the point that their rate is significantly higher than their male counterparts (as before, this happens only in a very few cases). A similar effect (but much smaller) can be observed for Hispanic participants.\n\nFor how interesting this is for me, **this result does not mean much in terms of causality**. There can be more important factors determining this kind of choice and we are looking at averages, thus smoothing out any kind of variability.\n\nBefore moving on, we can see how being of the same race has a positive effect regardless of what the individuals are declaring it important or not. Moreover, the effect is particularly large if the race is considered a very important factor.","d37d977e":"The model is way too pessimistic in predicting a `yes`. In particular, by knowing information about the partner only, if the participant is Female, it is most likely to have a false negative than a false positive. If the participant is male, instead, it looks is missing equally in both ways, even though it makes more mistakes.\n\nIn other words, if we are this heartless machine we make more mistakes if we have a male in front of us and we are not sure about what kind of mistake we made. On the other hand, with a female is most likely that we said `No` when we should have predicted `Yes`.\n\nAnother pattern that could be found in the mistakes of this model is about people we found *averagely attractive*  that led to many false negatives.","3ab9735b":"***Note***: The right way to create `spread_like` would be to create the feature inside of the pipeline, after the imputer. Sadly, I am repeatedly failing in creating such class. Please help me :)\n\nFrom previous runs, I noticed that adding the `spread_like` feature mentioned above would improve the score but also increase the number of false positives by a lot. It essentially predicts `1` more often, resulting in fewer false negatives and more false positives and, since we have more false negatives, this increases the accuracy. \n\nThe reason why this happens, I think, is because that feature is using a highly important feature (`like`), arbitrarily set to `6` its missing values, and then computing the distance to `mean_liking`. The imputation, while less harmful when we calculate `mean_liking`, is a strong assumption and *distracts the model*. \n\nThe next hidden cell will prepare the data for each of the 3 cases mentioned above","fb6bc6c7":"So far, we have not many features regarding the partners but mostly from characteristics of the participant. We thus need to do some manipulations in order to get the model learn from that too. The following cell takes care of adding some information about the partner for each date by simply looking it up (every `iid` \/ `pid` couple is present twice in the DataFrame)","9af9fd62":"On the same lines, we can create things like age difference, same field of study, same goals, etc.","08c4f04b":"Interestingly, all 3 models will predict `No` in the typical date described above. To see how silly they are, they say `No` regardless of the age, no matter how much we vary it (although it is not silly to say no to an 8 year old, otherwise it is a crime). Every prediction is indipendent of the gender, meaning that varying this feature never brings a change in the decision.\n\nThe model that takes into consideration only the participants will always predict `No` to every variation of the typical date. **This model is very picky**.\n\nThe other two models can change their mind in a few cases and for a few combinations of Gender and Race (meaning that if the combination is not in the next 2 graphs, the prediction remains `No`).\n\nHere we go, **how to get a yes from a machine on a typical date**:","19663987":"In this case, Asian people declare to date less often, in general females date less than males but in the asian case the trend is reversed.","0bd85ab0":"This looks like more in line with what was declared: the effect for males is negligible and more noticeable for females.","d75cffbf":"We see that asking people to be more selective in wave 12 led to a lower match and positive feedback rate, but nothing that didn't happen in other waves as well. \n\nWe can also see something happening in the waves were people were asked to bring a book or a magazine but drawing any conclusion about it would be premature and probably very wrong.\n\nThe one thing that I want to understand is if I can compare the preference scores of waves 6 to 9 (every category on a scale from 1 to 10) with the ones of the other waves (assign 100 points in total).","99e5cf95":"Thus, in general, Caucasian (and \"Other\") females declare to give more importance to race and religion than any other segment of the population. It is difficult to not notice how these differences between gender vary across the races. We notice again a big gap across gender in Black participants and in participants of 'Other' races. At last, it seems that Hispanic individuals are fairly united in giving importance to religion (on average), regardless of their gender.\n\n## Are we sincere about it?\n\nWe could try to find a way to check whether or not the participants are declaring what they really feel. In particular, they are asked to communicate if they like their speed-date or not (as far as I understood, if they both like each other we call this a match). Although this might depend on many factors, we could try to see if the values we found above are trustworthy.\n\nSince the data with the decisions are in the DataFrame with the repeated individuals (the original one), the next cell will prepare the data by counting how many times an individual gave positive feedback and how many times it was with a partner of the same race. \n\nIt is easy (sums are associative) to see that the same results can be achieved by simply using the original DataFrame but it is a good exercise of data manipulation that I want to keep in the next hidden cell.","9c8269d6":"I selected these two features (but you can see how there are patterns for the others in `to_check` too) because I find interesting how the model is **overestimating the importance of attractiveness** by being more inclined that it should and falling into a relatively large number of false positives when this attribute is high and of false negatives for low values of `attr`. On the other hand, the model is **underestimating the importance of sincerity** by getting relatively more false negatives for high values of this attribute.\n\nOne might argue that this cold, heartless, machine is picking up some human habits very quickly. \n\n## Synthetic people in machines, making counterfactual plots.\n\nHere we can check what these models have in common in terms of prediction and how they behave if we feed them some date between made up people.","67c99ef6":"## All by myself - no partner information\n\nThis is a situation where we are essentially predicting what a person will decide for who they are and the votes they received from a partner that we completely ignore. The case I imagine being useful for such model is a sort of propensity to saying yes to someone after 4 minutes of conversation.","25e3f012":"As we see on a (not so random, being the first of the list) date, we have the correct informations about the partner too.\n\nThe next issue is actually a cleaning issue: the evaluations on how much the participants value some characteristics are not uniformily gathered across the waves. We can either drop those features or drop those waves. Since, looking at the learning curves, it seems that the learning would not benefit from a decrease of data points, we could remove the problematic features. Unfortunately, the model drops in performance a lot if we do so. \n\nAnother solution is to flag each wave with its peculiarity","01f9310d":"We see that the full model and the one taking only into consideration the data about the participants are very simila in the way they predict the decisions of the participants (being the participants' evaluations of the date the most important features, this is not surprising). Now I want to see how these models predict and if there are weird behaviors.\n\nThe idea is to take a typical date (defined in the next cell) and alter the values of each feature to spot the moment when the model changes the prediction. In other words, what does is take to make our model say `yes` or `no`. \n\nThe typical date is the following.","c16b5905":"## No one forgets the first time\n\n*That one time, in the forest, it was magical*\n\nIn previous versions of this kernel, I had some experiments going on, namely:\n\n* A RandomForest on the data as they are, which didn't go that bad\n* A lot of feature engineering in order to add information about the partner of each date\n* Noticing that the forest was learning the data perfectly in every fold\n* Failing in regularizing the Forest appropriately so that the variance of the model can be reduced\n* Using XGBoost to have a model that not only performs better but also looks like it is not learning the training data perfectly\n* Using a Forest to select the features and XGBoost to predict, all in the same pipeline to avoid information leak\n\nAll of this was done with the goal of getting a better score and I got to the conclusion that the main issue was that all the models needed more observations to learn, something not really doable. Moreover, I got up to a humble 85% of accuracy and I have to admit that, besides noticing from the learning curves that with way more data I can realistically get to around 90%, I have no way of knowing if it is a *good enough* result.\n\nWith these lessons in mind, I think it is more interesting to look at the problem from a different perspective. The question I want to answer to is then: **What can a machine learn if we vary the type of information available?**\n\nPut differently, I want to see how the models change if we have\n\n* Only information about the participant\n* Only information about the partner\n* Information about both\n\nIn all of these, we will try to see what signal the machine is picking up and spend some time interpreting it\n\n## Participants and partners, engineering some features\n\nIn this section, we will prepare the data to make the experiments as clean as possible. The code will be hidden because nobody likes to see the code at a first read but what you will find in it is:\n\n* libraries\n* helper functions\n* feature selection for the individual models\n\nThe feature engineering part won't be hidden because I want to explain the process as much as I can and **I need your feedback**","2b8c08bd":"Again, we see that white participants declare higher values.","b50ec24d":"Females declare to care about race more than males. However, something interesting happens if we put race into the mix again.","e0c31357":"Caucasians give more importance to race than other races. One may wonder if gender plays a role in that","ec04ac00":"* Not surprisingly, many of them date very seldom. In general, the distribution of their dating life is skewed in the opposite way with respect to the one of their social activity.\n* The ones that are dating a lot are here for fun but 14% of them looks for a serious relationship","b4cfd037":"Not as little as we might have hoped. Luckily, we don't need them all. We start dropping the ones referring to questions asked after the decision was taken.","8ae7dbbb":"* People like more doing sports and exercising than watching it on tv, white people in particular\n* Black people are not much into museums and art, but they do like music and clubbing\n* Asians like tv a lot, while Hispanics not so much, preferring activities like reading, or going to theaters and movies\n\nEvery time I express myself in this way, I mean it as a description of this sample of participants, not of the population.\n\n## Expectations\n\nAnother set of questions that can produce interesting answers are the one regarding the expectations of each participant","1911dc66":"## All about you - only partner information\n\nThis is a situation where we want to predict what makes people say yes, regardless of who they are (it is equivalent to the previous case but using `dec_o`as target, but I prefer this approach)","d79b35d8":"Before we begin, it is important to notice that each participants is in the DataFrame multiple times, once for each potential match. Thus an exploratory analysis on this dataset would be on 8378 individuals with many many repetitions. \n\nIn other words, if I am White and I participate to a wave with 10 participants of the opposite gender, I count as 10 white people. This can **bias the analysis** and therefore I want to create a second DataFrame with only the unique entries, giving us the real number of participants: 551.","6769e608":"As we did several times already, let's try to find some differences between genders and races.","7e211ee0":"The two genders mainly differ in their intentions about finding a date (twice as many more males declare that) or meeting someone new (it turns out that girls do not *just want to have fun* but also want to meet someone new).\n\nLet's see how often they declare to **go out**.","2ad6d0f0":"Next, we drop columns that should not have any effect on the decision of an individual (like the unknown decision of their partner). We keep only how much the partner liked the participant since it can influence the decision (a *like to be liked* situation). We let the model decide if it is relevant or not.","9f0607b5":"We indeed see that the scores they expect and how they measure themselves are very correlated, in particular on Attractive, Fun, and Intelligent. There is another correlation between the ways they think being fun or sharing interest is valued by their same gender and the opposite one. Opposite correlation between how they think other people value being attractive and the other categories, a correlation that becomes positive again if we look at how they think the two genders value this aspect.\n\nThe questions we explored in the previous section were then repeated half-way through the session, right after the session, and weeks after it. However, no significant change can be observed.\n\n# Rating and deciding\n\nBefore getting into the modeling, we can explore one more thing: how do we rate a partner that we like and how do we rate one that we don't?\n\nThe participants were given a scorecard to fill at the end of each speed dating, communicating their decision, their rating about the other person, and their expectation. Let's see if we can spot some pattern in that.\n\nFor this analysis, we have to use the original DataFrame and we will pretend that we are a little different every time we meet someone new (we are not, maybe we can pretend a bit) so that we suddenly have more than 8 thousands individuals.","e2e0ff3f":"In the above example, there is a respondent from wave 6 (thus that should score from 1 to 10 each category independently) that for some question had their scores converted to the other scale (assign 100 points to the categories) and some other questions were left intact. I am afraid I won't be able to use these questions or to use all the waves at the same time\n\nThe following charts are thus describing any wave but the ones between the 6th and the 9th.","def84735":"Another factor that I can imagine could play a role is *how good of a night it is*. In other words, from previous versions we know that the *like* factor is very important and I wonder if having meet someone that I liked a lot can change my perspective on the successive dates. We then start aknowledging that the dates are happening in order and try to create the appropriate feature.","10bcc0c5":"This is a much evident difference in expectations across races, something that we couldn't quite observe in the expectation of happiness.\n\nAll that being said, this question is not clear because each wave had a different number of partipants and it appears to be present only for the first 5 waves","17f51f62":"Again, female participants are declaring higher values","5f02c0f8":"Males are more optimistic about the evening, while no significant differences can be observed if we split by race.\n\nThe next question is: *Out of the 20 people you will meet, how many do you expect will be interested in dating you? *","d9a2a7ce":"# Machines can love too\n\nWe have seen times and times again that some patterns are hiding in these data and the machine learning enthusiast in all of us is now jumping around speaking about boosting, forests, and soul mates. After so much talking and staring at histograms we deserve some time staring at some machine learning the rules of love.\n\nThe goal of this section is to predict the decision of a participant after 4 minutes of date. We thus don't need everything that was answered *after* the decision since it is not in causal relation with our target.\n\nWe have already encountered a few mistakes in the data entries and a few missing values. We need to take care of that first.\n\n## Machines don't like it dirty\n\nThis is the list of missing values. (hidden because too long, feel free to read it)","8c5888dc":"The Asian participants are younger, while Black and Hispanic participants are older on average.","a745fd16":"A model that takes into consideration both participants and partners can change its mind, but only for Hispanic and Black people that like 8 or more. While the *partner model*, the one that wants to predict the decision of someone it has no information about, relying solely on its confidence, will expect a yes only from people it judges attractive 3 at best, then it expect only `No`. In particular, it expect only very unattractive Black participants to say `yes` to it, while White and Hispanic participants can be also mildly unattactive to make it hope for a `yes`.\n\nVery sad indeed.\n\nWe could go on but we are now at version 35 and nobody wants really keep going with this.\n\n# Conclusions\n\nThis kernel allowed me to explore some questionnaire data about a topic we can all more or less relate to. I would like to conclude by summarizing some of the findings.\n\n* These events attracted some segments of the population more than others, making this sample most likely not representative of the population.\n* We observed a relationship between gender, race, and how important are race and religion, with the gender having a much higher weight.\n* We have seen how the participants generally declared to not give much importance to race but were also generally more inclined to give positive feedback about their date if the partner was of their same race.\n* People of different races like almost the same things, with some exceptions (see the subsection on interests)\n* White participants are way more optimistic than their Asians counterparts.\n* We have a data science-ish way of saying *I like you as a friend* \n* Participants were fairly confident before the sessions but, in rating their expectations after every date, they looked not so confident if they liked the partner, especially if they were attractive.\n* How much the participant found their partners attractive (or other key attributes) is way more important of shared interest, race, and field of study.\n* Not knowing how much the date is appreciated (with respect to the key attributes above) makes a machine guess the outcome of the date with significantly less accuracy. But the more you know about the partner, the better (in terms of accuracy of the prediction)\n* It is more difficult to predict what an *art lover* (or movies, or clubbing) will decide after 4 minutes of date, while it is very possible, for example, for a *tv lover*.\n* Predicting the decision of someone without using their information is very hard and even harder if they are males.\n* Even a machine *thinks* that people will say `yes` if they find their date very attractive and `no` if they don't more than it actually happens. On the other side, it ignores how people actually like very sincere dates.\n* On a typical date, these models are very negative and picky. \n* At least they behave in the same way for Males and Females and ignore almost always the race of us human to make their prediction (which can go as high as 86% in accuracy).\n\nThank you for reading this far, it has been a great exercise for my EDA skills and I am very glad of the support while I was drafting my 35 versions.\n\nPlease keep the feedback coming and improve this kernel as much as you like.\n\nIt has been a lot of fun for me, I hope for you too\n \n Cheers","5343ed67":"It looks like there is some influence after all, let's try to go deeper.","024fd030":"Mostly Caucasian participants, then Asian. It could either be a cultural thing or that these events were organized in specific areas with this demographic.","e199b797":"* Most of the people declare to go out on a weekly basis and they are mostly here to have fun.\n* Both the participants that declared to almost never go out were there to meet someone new.\n* Participants going out on a yearly basis are in proportion more interested in finding a date or a serious relationship than the others.","aed0d7cf":"Now, since we have all the information we want we can ask ourselves *how relevant can be, in general, liking sports to decide whether or not we want to date someone?* \n\nThe models will try answer this question. However, I can't help but thinking that it does not matter how much you like something but rather **how much you and your partner share that opinion**. In other words, it might not be relevan *in general*, but it can be relevant in a specific case. For this reason, the next set of features is taking care of describing this situation."}}