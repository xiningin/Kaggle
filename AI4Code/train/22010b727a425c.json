{"cell_type":{"e0f632d8":"code","bf2ab4b5":"code","a875e4b8":"code","a4041232":"code","5d1304fc":"code","417139e9":"code","1e5b35a6":"code","d8610076":"code","e614a816":"code","3986f506":"code","46bf8b4b":"code","dbb0aac0":"code","7fa0235f":"code","036864d4":"markdown","10c49def":"markdown","21ac113c":"markdown","87ee589d":"markdown","6333b7ea":"markdown","1c7c79bb":"markdown","ee80cb52":"markdown","f3374844":"markdown","19bf1f5a":"markdown","64bc54b5":"markdown"},"source":{"e0f632d8":"# pip install hyperopt","bf2ab4b5":"from sklearn.datasets import make_classification\nimport pandas as pd\nimport numpy as np\n\nseed = 42 # Set seed for reproducibility purposes\nmetric = 'accuracy' # See other options https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\nkFoldSplits = 5\n\nnp.random.seed(seed)\n# random.seed(seed) # If importing random, should also set this seed\n# tf.random.set_seed(seed) # If using tensorflow, should also set this seed\n\nX,Y=make_classification(n_samples=500,\n                        n_features=30,\n                        n_informative=2,\n                        n_redundant=10,\n                        n_classes=2,\n                        random_state=seed)","a875e4b8":"# If kFold is not defined then 5-fold default value was used.\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC\nimport time\n\nclassifier = SVC()\n\n#Parameter search space for both GridSearch and RandomizedSearch\nparams={\n        'C': np.arange(0.005,1.0,0.01),\n        'kernel': ['linear', 'poly', 'rbf'],\n        'degree': [2,3,4],\n        'probability':[True]\n        }\n\nn_iter = 100\nrandom_search = RandomizedSearchCV(classifier, \n                                   param_distributions = params,\n                                   n_iter = n_iter,\n                                   scoring = metric,\n                                   random_state=seed,\n                                   cv = StratifiedKFold(n_splits=kFoldSplits, random_state=seed, shuffle=True).split(X,Y))\n\n# Run the fit and time it\nstart = time.time()\nrandom_search.fit(X,Y)\nelapsed_time_random = time.time() - start\n\ngrid_search = GridSearchCV(classifier,\n                           param_grid = params,\n                           scoring = metric,\n                           cv = StratifiedKFold(n_splits=kFoldSplits, random_state=seed, shuffle=True).split(X,Y))\n\nstart = time.time()\ngrid_search.fit(X,Y)\nelapsed_time_grid = time.time() - start\n\n","a4041232":"best_score=1.0\n\ndef objective(space):\n    \n    global best_score\n    model = SVC(**space)   \n    kfold = StratifiedKFold(n_splits=kFoldSplits, random_state=seed, shuffle=True) # KFold is also an option.\n    score = 1-cross_val_score(model, X, Y, cv=kfold, scoring=metric, verbose=False).mean() \n    # Careful here (score). The objective function will be  minimized, thus somme treatment on your score might be needed.\n    \n    if (score < best_score):\n        best_score=score\n    \n    return score ","5d1304fc":"from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n\nspace = {\n      'C': hp.choice('C', np.arange(0.005,1.0,0.01)),\n      'kernel': hp.choice('kernel',['linear', 'poly', 'rbf']),\n      'degree':hp.choice('degree',[2,3,4]),\n      'probability':hp.choice('probability',[True])\n      }","417139e9":"n_iter_hopt = 50\ntrials = Trials() # Initialize an empty trials database for further saving\/loading ran iteractions\n\nstart = time.time()\n\nbest = fmin(objective, \n            space = space, \n            algo = tpe.suggest, \n            max_evals = n_iter_hopt,\n            trials = trials,\n            rstate = np.random.RandomState(seed))\n\nelapsed_time_hopt = time.time() - start","1e5b35a6":"print(\"\\nGridSearchCV took %.0f seconds for all candidates. Accuracy reached: %.3f\\nOptimal parameters found: %s\" % (elapsed_time_grid, ((grid_search.best_score_)*100), grid_search.best_params_))\nprint(\"\\nRandomizedSearchCV took %.0f seconds for %d candidates. Accuracy reached: %.3f\\nOptimal parameters found: %s\" % (elapsed_time_random, n_iter, ((random_search.best_score_)*100), random_search.best_params_))\nprint(\"\\nHyperopt search took %.2f seconds for %d candidates. Accuracy reached: %.3f\\nOptimal parameters found: %s\" % (elapsed_time_hopt, n_iter_hopt, ((1-best_score)*100), best))","d8610076":"# More details on how to work with save\/load trials in https:\/\/github.com\/hyperopt\/hyperopt\/issues\/267\nimport pickle\npickle.dump(trials, open(\"trials.p\", \"wb\"))\ntrials = pickle.load(open(\"trials.p\", \"rb\")) # Pass this to Hyperopt during the next training run.","e614a816":"from xgboost import XGBClassifier\n\nn_iter_hopt_xgb = 50\ntrials = Trials() # Initialize an empty trials database for further saving\/loading ran iteractions\n\n# Declare xgboost search space for Hyperopt\nxgboost_space={\n            'max_depth': hp.choice('x_max_depth',[2,3,4,5,6]),\n            'min_child_weight':hp.choice('x_min_child_weight',np.round(np.arange(0.0,0.2,0.01),5)),\n            'learning_rate':hp.choice('x_learning_rate',np.round(np.arange(0.005,0.3,0.01),5)),\n            'subsample':hp.choice('x_subsample',np.round(np.arange(0.1,1.0,0.05),5)),\n            'colsample_bylevel':hp.choice('x_colsample_bylevel',np.round(np.arange(0.1,1.0,0.05),5)),\n            'colsample_bytree':hp.choice('x_colsample_bytree',np.round(np.arange(0.1,1.0,0.05),5)),\n            'n_estimators':hp.choice('x_n_estimators',np.arange(25,100,5))\n            }\n\nbest_score_xgb = 1.0\n\ndef objective(space):\n    \n    global best_score_xgb\n    model = XGBClassifier(**space, n_jobs=-1)   \n    kfold = StratifiedKFold(n_splits=kFoldSplits, random_state=seed, shuffle=True)\n    score = 1-cross_val_score(model, X, Y, cv=kfold, scoring=metric, verbose=False).mean() \n    \n    if (score < best_score_xgb):\n        best_score_xgb=score\n    \n    return score \n\nstart = time.time()\n\nbest = fmin(objective, \n            space = xgboost_space, \n            algo = tpe.suggest, \n            max_evals = n_iter_hopt_xgb,\n            trials = trials,\n            rstate = np.random.RandomState(seed))\n\nelapsed_time_xgb = (time.time() - start)","3986f506":"print(\"Hyperopt on XGBoost took %.0f seconds for %d candidates. Accuracy reached: %.3f\\nOptimal parameters found: %s\" % (elapsed_time_xgb, n_iter_hopt_xgb, ((1-best_score_xgb)*100), best))","46bf8b4b":"import matplotlib.pyplot as plt\n\nscore_history = []\nbest_score_history = []\ntrials = Trials()\nn_iter_hopt_xgb = 200\n\nbest_score_xgb = 1.0\n\ndef objective(space):\n    \n    global best_score_xgb\n    model = XGBClassifier(**space, n_jobs=-1)   \n    kfold = StratifiedKFold(n_splits=kFoldSplits, random_state=seed, shuffle=True)\n    score = 1-cross_val_score(model, X, Y, cv=kfold, scoring=metric, verbose=False).mean() \n    \n    if (score < best_score_xgb):\n        best_score_xgb=score\n    \n    # To visualize, we need to modify our objective function to capture history\n    best_score_history.append(1-best_score)\n    score_history.append(1-score)\n    \n    return score \n\nstart = time.time()\n\nbest = fmin(objective, \n            space = xgboost_space, \n            algo = tpe.suggest, \n            max_evals = n_iter_hopt_xgb,\n            trials = trials,\n            rstate = np.random.RandomState(seed))\n\nelapsed_time_xgb = (time.time() - start)","dbb0aac0":"plotY = score_history\nplotX = list(range(1, n_iter_hopt_xgb+1, 1))\nplt.figure(figsize=(10,8))\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.title('Hyperopt Search Pattern')\nplt.plot(plotX, plotY, 'ro')  ","7fa0235f":"plt.figure(figsize = (10,8))\nplt.xlabel('Accuracy')\nplt.ylabel('Frequency')\nplt.title('Histogram of Hyperopt Solution Scores')\nplt.hist(plotY, 30, density=True, facecolor='r', alpha=0.75)\nplt.show","036864d4":"## Now, applying *Hyperopt* on **XGBoost**","10c49def":"## Check results","21ac113c":"## Create a toy-dataset","87ee589d":"## Visualizing Hyperopt\u2019s Search Pattern","6333b7ea":"## Install Hyperopt library","1c7c79bb":"## Saving hyperopt trials in a pickle object","ee80cb52":"# Hyperparameter tuning using Hyperopt library","f3374844":"## Play with **SVM** model for *Bayesian Optimisation* from *Hyperopt*","19bf1f5a":"## Play with **SVM** model for *Grid Search* and *Randomized Search*","64bc54b5":"Example from https:\/\/kevinvecmanis.io\/statistics\/machine%20learning\/python\/smbo\/2019\/06\/01\/Bayesian-Optimization.html#hyperopt"}}