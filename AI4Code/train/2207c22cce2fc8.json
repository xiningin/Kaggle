{"cell_type":{"32f53ca2":"code","96efa8cb":"code","136ed1a9":"code","7669b28c":"code","5fff5bc1":"code","4904e2a2":"code","87f95d2f":"code","6dc2330f":"code","c8c5fb79":"code","b38725d8":"code","5cbe20b0":"code","1a124671":"code","0a14736a":"code","b02d7007":"code","bce024bc":"code","49354de8":"code","b2d584da":"code","9c302e86":"code","d3033dd8":"code","b4a00d2a":"code","a1c37830":"code","a42fb5cf":"code","f0025981":"code","da28acdd":"code","579d0408":"code","91db7467":"code","c5df347e":"code","cda90ce2":"code","816b37cc":"code","0aa9f0a2":"code","d3f99526":"code","bf5c9732":"code","e8018f7d":"code","45209dca":"code","0b9bbc19":"code","086661d9":"code","b5045a46":"code","b0f0d715":"code","32f50767":"code","6be8b3d9":"code","c038e810":"code","f363d854":"markdown","2aaa4a3d":"markdown","8038946e":"markdown","8ef81984":"markdown"},"source":{"32f53ca2":"#\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import boxcox,skew,norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n","96efa8cb":"# data\ncolumns = [\"Class\", \"Alchol\", \"Malic_Acid\", \"Ash\", \"Alcalinity_of_Ash\", \"Magnesium\", \"Total_phenols\", \"Falvanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\", \"Color_intensity\", \"Hue\", \"OD280\", \"Proline\"]\nwine_data =pd.read_csv('..\/input\/wine.data',names=columns)","136ed1a9":"wine_data.head()","7669b28c":"wine_data.shape#no of rowsa and columns","5fff5bc1":"wine_data.info()","4904e2a2":"wine_data.isna().any().sum()","87f95d2f":"wine_data.describe(include='all').T#Decriptive Statistics","6dc2330f":"#checking the overall distribution","c8c5fb79":"sns.pairplot(wine_data,diag_kind='kde')","b38725d8":"#splitting data into 70 :30","5cbe20b0":"from sklearn.model_selection import train_test_split","1a124671":"x= wine_data.drop('Class',axis=1)\ny =wine_data.Class\n\n","0a14736a":"x_train,x_test,y_train,y_test =train_test_split(x,y,test_size=0.3,random_state=3)","b02d7007":"sc = StandardScaler()#scaling data","bce024bc":"x_train_std = sc.fit_transform(x_train)\nx_test_sd = sc.transform(x_test)","49354de8":"#constructing covariance matrics and finding eigen values and eigen vectors","b2d584da":"cov_mat = np.cov(x_train_std.T);\nprint(cov_mat)\neigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n","9c302e86":"print('Eigen Vectors \\n', eigen_vecs)\nprint('\\n Eigen Values \\n', eigen_vals)","d3033dd8":"tot = sum(eigen_vals);\nvar_exp = [(i \/ tot) for i in sorted(eigen_vals, reverse=True)];\ncum_var_exp = np.cumsum(var_exp);\nplt.bar(range(1,14), var_exp, alpha=0.5, align='center',label='individual explained variance')\nplt.step(range(1,14), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.show()","b4a00d2a":"#60% of variance is explained by first 2  PCA","a1c37830":"# Selecting k eigenvectors that correspond to the k largest eigenvalues,  \n# where k is the dimensionality of the new feature subspace ( k\u2264d ).","a42fb5cf":"eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[ :, i]) for i in range(len(eigen_vals))]\neigen_pairs","f0025981":"eigen_pairs.sort(key=lambda k: k[0], reverse=True)","da28acdd":"eigen_pairs","579d0408":"#  projection matrix W from first  2 eigen vector\nw = np.hstack((eigen_pairs[0][1][:, np.newaxis],eigen_pairs[1][1][:, np.newaxis]))\nprint('Matrix W:\\n', w)","91db7467":"x_train_pca = x_train_std.dot(w)","c5df347e":"x_test_pca =x_test_sd.dot(w)","cda90ce2":"x_train_pca.shape","816b37cc":"#  visualizing the transformed Wine training set","0aa9f0a2":"colors = ['r', 'b', 'g']\nmarkers = ['s', 'x', 'o']\nfor l, c, m in zip(np.unique(y_train), colors, markers):plt.scatter(x_train_pca[y_train==l, 0],x_train_pca[y_train==l, 1],c=c, label=l, marker=m)\nplt.xlabel('PC 1')\nplt.ylabel('PC 2')\nplt.legend(loc='lower left')\nplt.show()","d3f99526":"#LR model ","bf5c9732":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA","e8018f7d":"lr = LogisticRegression()\npca =PCA(n_components=2)","45209dca":"# x_train_pca = pca.fit_transform(x_train_pca)","0b9bbc19":"lr.fit(x_train_pca,y_train)#fitting with train data","086661d9":"lr.score(x_train_pca,y_train)#train accuracy score","b5045a46":"y_predit =lr.predict(x_test_pca)#predictd test values","b0f0d715":"import sklearn.metrics as metrics","32f50767":"accuracy = metrics.accuracy_score(y_predit,y_test)\nprint(accuracy)#test accuracy","6be8b3d9":"#confusion metrics\nprint(metrics.confusion_matrix(y_predit,y_test))","c038e810":"# classification report\nprint(metrics.classification_report(y_predit,y_test))","f363d854":"######## class is almost normally distributed","2aaa4a3d":"Objective -  Dimensionality Reduction through Principal Component Analysis on the Wine data set.\nData Set -  https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine\/wine.data Links to an external site.\nData details : https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine\/ Links to an external site.\n","8038946e":"All variables are numerical and  no missing values found","8ef81984":"#Using PCA we got better result with reduced number of variable"}}