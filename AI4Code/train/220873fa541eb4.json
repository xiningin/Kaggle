{"cell_type":{"3648dd5a":"code","adcedec3":"code","91a4924c":"code","006552b7":"code","db1b09b7":"code","7210248a":"code","d2ea3cb0":"code","243bce69":"code","7f2cfeaa":"code","e8f2976c":"code","db1cb38b":"code","6145b31b":"code","f5098310":"code","988d8d8c":"code","48e08368":"code","f7a0c402":"code","767a0866":"code","33090005":"code","3bf13e6b":"code","4d129716":"code","432ea86c":"code","1a95f9b7":"code","3a57fcff":"code","4fab527a":"code","d602cf49":"code","69552f80":"markdown","9a31a86b":"markdown","16dbac66":"markdown","d6fb760e":"markdown","b2c6b7b7":"markdown","083abd16":"markdown","00ab4f13":"markdown","28c3baa8":"markdown","29ce4e90":"markdown","fcc02f83":"markdown","64c5887c":"markdown","764c94f5":"markdown","9ca8a9af":"markdown","97bf41a5":"markdown","26a4ad92":"markdown","728c364d":"markdown","44ec7e47":"markdown","4d679147":"markdown","9287464a":"markdown","65122f50":"markdown"},"source":{"3648dd5a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\npd.set_option('display.max_rows', None) #shows maximum rows\npd.set_option('display.max_columns', None) #shows maximum cloumns\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split, cross_val_predict,cross_validate\nfrom sklearn.linear_model import LogisticRegression #Load classifiers\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\nimport optuna.integration.lightgbm as lgb\nimport optuna\nfrom scipy import stats\nfrom lightgbm import LGBMClassifier","adcedec3":"test =  pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\ntrain = pd.read_csv( '..\/input\/tabular-playground-series-jun-2021\/train.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')","91a4924c":"train.head()","006552b7":"test.head()","db1b09b7":"#copying the dataset into new ones for operation\n\ndf_train = train.copy()\ndf_test = test.copy()","7210248a":"df_train.info()","d2ea3cb0":"df_test.info()","243bce69":"#As Id column will has all unque values we will dropping it as of now.\ndf_train = df_train.drop([\"id\"], axis=1)\ndf_test = df_test.drop([\"id\"], axis=1)","7f2cfeaa":"#Segregating numerical and ctargorical column for futhur exploration\nfeature_cols = df_train.columns\n\n## Getting all the data that are not of \"object\" type. \nnumerical_columns = df_train[feature_cols].select_dtypes(include=['int64','float64']).columns\ncategorical_columns = df_train[feature_cols].select_dtypes(exclude=['int64','float64','datetime64[ns]']).columns\n\nprint(len(numerical_columns), len(categorical_columns))","e8f2976c":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Gender\"\n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = df_train[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (17,5))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))\n    print(\"Number of Unique Values:\",var.nunique())","db1cb38b":"sns.set_style('darkgrid')\n\nfor c in categorical_columns:\n    bar_plot(c)","6145b31b":"\n\ncleanup_nums = {\"target\": {\"Class_1\": 1, \"Class_2\": 2, \"Class_3\": 3, \"Class_4\": 4,\n                                  \"Class_5\": 5, \"Class_6\": 6, \"Class_7\":7,\"Class_8\":8,\"Class_9\": 9 }}\n\ndf_train = df_train.replace(cleanup_nums)\ndf_train.head()","f5098310":"def plot_hist(variable):\n    plt.figure(figsize = (15,4)) \n    plt.subplot(1, 2, 1) # distribution\n    plt.hist(df_train[variable], bins = 20)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n\n    plt.subplot(1, 2, 2) # outlier\n    sns.boxplot(x=df_train[variable])\n    plt.title(\"{} outlier distribution\".format(variable))\n    plt.show()\n    \n    print(df_train[variable].describe())","988d8d8c":"fig = plt.figure(figsize=(75, 75), dpi=100)\nmatrix = np.triu(df_train[numerical_columns].corr())\nsns.heatmap(df_train[numerical_columns].corr(), annot=True, mask=matrix)\nplt.title('New Features and Target Correlations', size=20, pad=20)","48e08368":"# split data into X and y\nX = df_train.drop(['target'],axis=1)\ny = df_train['target']\n\n#making model\nmodel = XGBClassifier()\nmodel.fit(X, y)\n\n\n\n","f7a0c402":"# get importance\nimportance = model.feature_importances_\n# summarize feature importance\n\n# plot feature importance\nplt.figure(figsize=(20, 20))\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","767a0866":"for i,v in enumerate(importance):\n   if v >= 0.02:\n    print('Feature: %0d, Score: %.5f' % (i,v))","33090005":"df_train_updated = df_train[[\"feature_12\", \"feature_16\",\"feature_18\",\"feature_20\",\"feature_21\",\"feature_31\",\"feature_37\",\"feature_43\",\"feature_56\",\"target\"]]\ndf_test_updated = df_test[[\"feature_12\", \"feature_16\",\"feature_18\",\"feature_20\",\"feature_21\",\"feature_31\",\"feature_37\",\"feature_43\",\"feature_56\"]]","3bf13e6b":"z_scores = stats.zscore(df_train_updated)\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\ndf_train_updated = df_train_updated[filtered_entries]\n","4d129716":"#Drop target variable from X and copy to y\nX=df_train_updated.drop(['target'],axis=1)\ny=df_train_updated['target']","432ea86c":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 11, 333),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.02, 0.05, 0.005, 0.1]),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'random_state': 42,\n        'boosting_type': 'gbdt',\n        'metric': 'multi_logloss',\n        'num_class':9,\n        'device': 'cpu',\n        'objective': 'multiclass'\n    }\n\n\n    model = LGBMClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = True)\n    y_pred = model.predict_proba(X_val)\n    logloss = log_loss(y_val, y_pred)\n\n    return logloss\n","1a95f9b7":"%%time\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 5)\nprint('Number of finished trials:', len(study.trials))\n\n","3a57fcff":"print('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","4fab527a":"#Train-test split (80:20)\nX_train, X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n\nprint(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(train_x.shape, test_x.shape))","d602cf49":"params = {\n        'reg_alpha': 8.808455264971526, \n        'reg_lambda': 7.293577390227297, \n        'num_leaves': 301, \n        'min_child_samples': 58, \n        'max_depth': 10,\n        'learning_rate': 0.1,\n        'colsample_bytree': 0.21480718879686642, \n        'n_estimators': 2296\n    }\n\n\nmodel = LGBMClassifier(**params)  \nmodel.fit(X_train, y_train, eval_set = [(X_test,y_test)], early_stopping_rounds = 20, verbose = True)\ny_pred = model.predict_proba(X_test)\nlogloss = log_loss(y_test, y_pred)\n\nprint(\"log loss, \",logloss)\n\n","69552f80":"1. All of the columns has huge outliers but the amount is huge.So, we won't be removing as of now.\n2. As there lot of column we might have to use PCA","9a31a86b":"for n in numerical_columns:\n    plot_hist(n)","16dbac66":"### Baseline model without outlier removal","d6fb760e":"### Feature importance","b2c6b7b7":"### Conclusion:\n\nWe will take only the features which has importance more then 0.020","083abd16":"### Replacing the catgorical values by number","00ab4f13":"### Correlations","28c3baa8":"1. [Libraries](#libraries)  \n2. [Load Datasets](#load_datasets)\n3. [Data_Exploration](#data_exploration)\n\n","29ce4e90":"<a id=\"libraries\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries<\/center><\/h2>","fcc02f83":"### So, from the above information we can see that there are no missing values in test or train column","64c5887c":"### Conclusion :\n\nWe will be building a classification task and the classification will be a multiclass imbalanced classification classification.\n","764c94f5":"### Conclusion","9ca8a9af":"### Hyper parameter tuning for LGBM using Optuna ","97bf41a5":"<a id=\"load_datasets\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets<\/center><\/h2>","26a4ad92":"### Categorical Variable","728c364d":"## Univariate Variable Analysis","44ec7e47":"<a id=\"data_exploration\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Data exploration<\/center><\/h2>\n","4d679147":"<h1><center>Tabular Playground Series -Jun 2021 <\/center><\/h1>\n\n                                                      \n","9287464a":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Content<\/center><\/h2>","65122f50":"### Numerical variable"}}