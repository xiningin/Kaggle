{"cell_type":{"588f006b":"code","614a23af":"code","d921e735":"code","c5720c8d":"code","d0fc8ae9":"code","406c14bb":"code","93b2c336":"code","7e520ba8":"code","7722887b":"markdown","44a28917":"markdown","477b41d2":"markdown","d454cc5e":"markdown","703ce775":"markdown","a476d922":"markdown","11a61d28":"markdown","24b21d68":"markdown","3f161fc4":"markdown","2c6cdb3a":"markdown"},"source":{"588f006b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # charts and data visualization\nfrom scipy import stats # mathematical functions\nfrom sklearn.model_selection import train_test_split # to split the data\nimport matplotlib.pyplot as plt # for plots visualization\nfrom sklearn.tree import DecisionTreeClassifier # import the decision tree model library\nfrom scipy.stats import randint # random library for hyperparameters\nfrom sklearn.model_selection import RandomizedSearchCV #randomized search cross validation model\nfrom sklearn.ensemble import RandomForestClassifier # import the random forest model library\nfrom sklearn.metrics import roc_auc_score # roc auc score method\nfrom sklearn.metrics import f1_score # f1 score method\nfrom sklearn.metrics import accuracy_score # accuracy score method\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ntestDataCSV = pd.read_csv('..\/input\/cap4611hw1data\/test.csv')\ntrainDataCSV = pd.read_csv('..\/input\/cap4611hw1data\/train.csv')\n\n# Split our features and labels from our training data set into x and y\nY = trainDataCSV.loc[:, trainDataCSV.columns == 'Bankrupt']\nX = trainDataCSV.drop('Bankrupt', axis=1)\n\n# Split x and y into test (validation set) and train data\nX_train, X_validate, Y_train, Y_validate = train_test_split(X, Y, test_size=0.3,random_state=27)","614a23af":"# Check the shapes of each data frame. Bankrupt column is missing from testing data because that's\n# what we want our model to predict anyway, so that's fine.\nprint(\"Train Data CSV Shape: \"+str(trainDataCSV.shape)+\"\\n\")\n\n# Check for missing (NaN) values. If there are any missing values, print them.\ntrainNullCount = trainDataCSV.isnull().sum()\nprint(trainNullCount[trainNullCount>0])","d921e735":"# This will show us the min and max values.\nprint(X_train.describe())\n\n# Look at the box plot to see where the whiskers are.\nfor column in X_train:\n    sns.boxplot(x=X_train[column])\n    plt.show()\n    ","c5720c8d":"# Set up the parameters and distribution to sample from.\nparam_dist = {\"max_depth\": [3, None],\n             \"max_features\": randint(1,9),\n             \"min_samples_leaf\": randint(1,9),\n             \"criterion\": [\"gini\",\"entropy\"]}\n\n# Instantiate our model\/object.\ntree = DecisionTreeClassifier()\n\n# Instantiate our RandomizedSearchCV model\/object.\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Train our model\/ fit it to the data.\ntree_cv.fit(X_train, Y_train)\n\n# Predict model.\ntree_pred = tree_cv.predict_proba(X_validate)[:,1]\ntree_pred1 = tree_cv.predict(X_validate)","d0fc8ae9":"# Capture the scores for each metric test\ntree_rocaucscore = roc_auc_score(Y_validate,tree_pred)\ntree_f1score = f1_score(Y_validate, tree_pred1)\ntree_accuracyscore = accuracy_score(Y_validate, tree_pred1)\n\nprint(str(tree_rocaucscore) + \" \" + str(tree_f1score) + \" \" + str(tree_accuracyscore))","406c14bb":"# Set up the parameters and distribution to sample from.\nparam_dist_forest = {\"max_depth\": [3, None],\n             \"max_features\": randint(1,9),\n             \"min_samples_leaf\": randint(1,9),\n             \"criterion\": [\"gini\",\"entropy\"]}\n\n# Instantiate our model\/object.\nforest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=27)\n\n# Instantiate our RandomizedSearchCV model\/object.\nforest_cv = RandomizedSearchCV(forest, param_dist_forest, cv=5)\n\n# Train our model\/ fit it to the data.\nforest_cv.fit(X_train, Y_train.values.ravel())\n\n# Predict model.\nforest_pred = tree_cv.predict_proba(X_validate)[:,1]\nforest_pred1 = tree_cv.predict(X_validate)","93b2c336":"# Capture the scores for each metric test\nforest_rocaucscore = roc_auc_score(Y_validate,forest_pred)\nforest_f1score = f1_score(Y_validate, forest_pred1)\nforest_accuracyscore = accuracy_score(Y_validate, forest_pred1)\n\nprint(str(forest_rocaucscore) + \" \" + str(forest_f1score) + \" \" + str(forest_accuracyscore))","7e520ba8":"final_pred_forestcv = forest_cv.predict_proba(testDataCSV)[:,1]\noutputCSV = pd.DataFrame({'id': testDataCSV['id'].values, 'Bankrupt': final_pred_forestcv})\noutputCSV.to_csv('submission_file.csv', index=False)","7722887b":"## 9 - Predict and Output Results to CSV","44a28917":"## 5 - Build and Train Decision Tree Model on Training Data\n\n**Tasks:**\n1. Set up the parameters and distribution to sample from.\n2. Build the decision tree model classifier object.\n3. Build the RandomizedSearchCV model object with the specified parameters.\n3. Train\/fit our model with the training data.","477b41d2":"## 2 - Check for Missing Values with the Training Data\n\n**Tasks:**\n1. Peek the shapes of each data frame.\n2. Do a null count on each data frame to see if there are any NaN values.\n    - If there aren't any, count should be 0. If count is 0, nothing to do.","d454cc5e":"## 7 - Build and Train Random Forest Model on Training Data\n\n**Tasks:**\n1. Set up the parameters and distribution to sample from.\n2. Build the random forest model classifier object.\n3. Build the RandomizedSearchCV model object with the specified parameters.\n3. Train\/fit our model with the training data.","703ce775":"## 4 - Standardization vs. Normalization\n\n#### These are both feature scaling techniques.\n\n**Standardization:**\n- Values are centered around the mean with a unit standard deviation.\n- Use it when the data resembles a Gaussian distribution.\n\n**Normalization:**\n- Also known as the min\/max ceiling, this method is used to shift and rescale values where they end up falling within the 0-1 range.\n- Use it when the data does *not* resemble a Gaussian distribution.\n\n#### Should we use them?\n   ##### In short, no. The models that we are using in this homework (decision trees and random forest models) are insensitive to the scale of the features. This esentially means that they are not sensitive to the variance in the data. The splits that tree-based models perform on the features are not influenced by other features, since each time the model does that, it increases \"homogeneity of the node.\" ","a476d922":"## 3 - Check for Outliers in the Training Data\n\n**Tasks:**\n1. Examine minimum and maximum values for sensibility.\n2. Look at a box plot to see where the whiskers are.","11a61d28":"# Machine Learning with Decision Trees and Random Forest Models\n### Nathaniel Lyra\n\n#### UCF ID: 4214346","24b21d68":"## 6 - ROC AUC, F1, and Accuracy Scores Reporting\n\n**Tasks:**\n1. Score my decision tree model using ROC AUC, F1, and Accuracy","3f161fc4":"## 8 - ROC AUC, F1, and Accuracy Scores Reporting\n\n**Tasks:**\n1. Score my decision tree model using ROC AUC, F1, and Accuracy","2c6cdb3a":"## 1 - Import Packages and CSV Files\n\n**Tasks:**\n1. Important useful packages for scientific computing and data processing.\n2. Use pandas library to read in CSV data files."}}