{"cell_type":{"6230e42b":"code","df8f7faf":"code","905da5a0":"code","323e98ec":"code","94794c8f":"code","54dc9b40":"code","7d9ef7c2":"code","d969e331":"code","7c3a5753":"code","a422307f":"code","b17ac00c":"code","0c49f283":"code","c61f9b04":"code","8f86d2a7":"code","6d1a3579":"code","76b14e30":"code","9ba8969b":"code","fb5e4eca":"code","108c36df":"code","302107b6":"code","d75874fc":"code","9f38f514":"code","517a6de6":"code","0e701c08":"code","7901fb87":"code","a0db46b4":"code","b93e7710":"code","78099db5":"code","0856761b":"code","d4dca91a":"code","ba2d49f7":"code","41b72fc1":"code","bc375f74":"code","b8fcb44c":"code","fdf02806":"code","de1576a2":"code","1a115174":"code","5dd88278":"code","6063b04f":"code","88cf72cd":"code","49d16628":"code","995fe1bd":"code","99980798":"code","6baf58ee":"code","4a737e28":"code","91d76fa1":"code","31fd87a6":"markdown","5eb40546":"markdown","f1df5c26":"markdown","353c9469":"markdown","8506ac86":"markdown","494255f8":"markdown","f8e4d97a":"markdown","870b731d":"markdown","8602f33f":"markdown","8dffd8e1":"markdown","d1b456ed":"markdown","1c81b832":"markdown","1e5d8278":"markdown","e712ac50":"markdown","5555ffc1":"markdown","f894fbc6":"markdown","79f4851d":"markdown","54874d3f":"markdown","4ac30f70":"markdown","edc34dec":"markdown","890bbd3f":"markdown","421e357d":"markdown"},"source":{"6230e42b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as py\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport cufflinks as cf\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer","df8f7faf":"py.offline.init_notebook_mode(connected=True)\ncf.go_offline()","905da5a0":"df = pd.read_csv('..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv')","323e98ec":"df.head(3)","94794c8f":"df.shape","54dc9b40":"# Dropping unwanted columns\n\ndf.drop(['Unnamed: 0', 'Clothing ID', 'Title'], axis=1, inplace=True)","7d9ef7c2":"df.info()","d969e331":"# Checking for null values\n\ndf.isnull().sum()","7c3a5753":"# Dropping records having null values\n\ndf.dropna(inplace=True)","a422307f":"# Checking if null values are removed\n\ndf.isnull().sum()","b17ac00c":"# Checking if any duplicate records are present\n\nduplicate=df[df.duplicated()] \nduplicate","0c49f283":"# Removing duplicate records\n\ndf.drop_duplicates(inplace=True)","c61f9b04":"# Again check if any duplicate records are left\n\nduplicate = df[df.duplicated()] \nduplicate","8f86d2a7":"df.describe()","6d1a3579":"df.describe(include='object')","76b14e30":"# Renaming columns\n\ndf.rename(columns={'Review Text':'Review', \n                   'Recommended IND':'Recommended', \n                   'Positive Feedback Count':'PositiveFeedback', \n                   'Division Name':'Division', 'Department Name':'Department', \n                   'Class Name':'Class'}, inplace=True)","9ba8969b":"# Expanding contractions\n\n# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\n\n# Expanding Contractions in the title, text\ndf['Review'] = df['Review'].apply(lambda x:expand_contractions(x))","fb5e4eca":"df['polarity'] = df['Review'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf['ReviewLen'] = df['Review'].apply(lambda x: len(x))\ndf['WordCount'] = df['Review'].apply(lambda x: len(x.split()))","108c36df":"# Polarity Distribution\n\ndf['polarity'].iplot(kind='hist', xTitle='Polarity', yTitle='Count', title='Distribution of Polarity')","302107b6":"# Distribution of Division\n\npx.histogram(df, x=df['Division'], title='Count of Division')","d75874fc":"# Distribution of Department\n\npx.histogram(df, x=df['Department'], title='Count of Department')","9f38f514":"# Distribution of Class\n\npx.histogram(df, x=df['Class'], title='Count of Class')","517a6de6":"# Distribution of Rating and Age\n\npx.histogram(df, x='Age', color='Rating', barmode='stack', title='Distribution of Rating and Age')","0e701c08":"# Distribution of Review Length\n\ndf['ReviewLen'].iplot(kind='hist', bins=50, xTitle='Review Length', yTitle='Count', title='Distribution of Review Length')","7901fb87":"# Distribution of Word count\n\ndf['WordCount'].iplot(kind='hist', bins=50, xTitle='Word count', yTitle='Count', title='Distribution of Word count')","a0db46b4":"df_pos = df.groupby('Recommended')['PositiveFeedback'].sum()\ndf_pos","b93e7710":"df_rat = df.groupby('Recommended')['Rating'].mean()\ndf_rat","78099db5":"df_ratp = df.groupby('Rating')['PositiveFeedback'].sum()\ndf_ratp","0856761b":"def get_top_n_words(x, n):\n    vec = CountVectorizer().fit(x)\n    bow = vec.transform(x)\n    sum_words = bow.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]","d4dca91a":"# Unigram\n\nwords = get_top_n_words(df['Review'], 20)\n\ndf_uni = pd.DataFrame(words, columns=['Unigram', 'Frequency'])\ndf_uni = df_uni.set_index('Unigram')\ndf_uni.iplot(kind='bar', xTitle='Unigram', yTitle='Count', title='Top 20 Unigram Words')","ba2d49f7":"def get_top_nwords(x, n, i):\n    vec = CountVectorizer(ngram_range=(i,i)).fit(x)\n    bow = vec.transform(x)\n    sum_words = bow.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]","41b72fc1":"# Bigram\n\nwords = get_top_nwords(df['Review'], 20, 2) \n\ndf_bi = pd.DataFrame(words, columns=['Bigram', 'Frequency'])\ndf_bi = df_bi.set_index('Bigram')\ndf_bi.iplot(kind='bar', xTitle='Bigram', yTitle='Count', title='Top 20 Bigram Words')","bc375f74":"words = get_top_nwords(df['Review'], 20, 3) \n\ndf_tri = pd.DataFrame(words, columns=['Trigram', 'Frequency'])\ndf_tri = df_tri.set_index('Trigram')\ndf_tri.iplot(kind='bar', xTitle='Trigram', yTitle='Count', title='Top 20 Trigram Words')","b8fcb44c":"# Cleaning Review column\n\n# Converting text to lowercase\ndf['Review'] = df['Review'].apply(lambda x:x.lower())\n\n# Removing digits and words containing digits\ndf['Review'] = df['Review'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n\n# Removing punctuations\ndf['Review'] = df['Review'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n# Removing extra spaces\ndf['Review'] = df['Review'].apply(lambda x: re.sub(' +',' ',x))","fdf02806":"# Applying lemmatization and removing stopwords\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    rev = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text) if w not in stopwords.words('english')]\n    rev = ' '.join(rev)\n    return rev\n\ndf['Review'] = df.Review.apply(lemmatize_text)","de1576a2":"words = get_top_n_words(df['Review'], 20)\n\ndf_uni = pd.DataFrame(words, columns=['Unigram', 'Frequency'])\ndf_uni = df_uni.set_index('Unigram')\ndf_uni.iplot(kind='bar', xTitle='Unigram', yTitle='Count', title='Top 20 Unigram Words')","1a115174":"# Bigram\n\nwords = get_top_nwords(df['Review'], 20, 2) \n\ndf_bi = pd.DataFrame(words, columns=['Bigram', 'Frequency'])\ndf_bi = df_bi.set_index('Bigram')\ndf_bi.iplot(kind='bar', xTitle='Bigram', yTitle='Count', title='Top 20 Bigram Words')","5dd88278":"words = get_top_nwords(df['Review'], 20, 3) \n\ndf_tri = pd.DataFrame(words, columns=['Trigram', 'Frequency'])\ndf_tri = df_tri.set_index('Trigram')\ndf_tri.iplot(kind='bar', xTitle='Trigram', yTitle='Count', title='Top 20 Trigram Words')","6063b04f":"# Wordcloud of Review in Recommended product\n\n# Cleaned dataframe of Recommended\ndf_true = df[df.Recommended == 1]\n\ntext_true = \" \".join(txt for txt in df_true['Review'])\n\ntext_cloud = WordCloud(collocations=False, background_color='black').generate(text_true)\nplt.axis(\"off\")\nplt.imshow(text_cloud, interpolation='bilinear')","88cf72cd":"# Wordcloud of Review in not Recommended product\n\n# Cleaned dataframe of Recommended\ndf_false = df[df.Recommended == 0]\n\ntext_true = \" \".join(txt for txt in df_true['Review'])\n\ntext_cloud = WordCloud(collocations=False, background_color='black').generate(text_true)\nplt.axis(\"off\")\nplt.imshow(text_cloud, interpolation='bilinear')","49d16628":"# Bar plot of polarity\n\nnegative = (len(df.loc[df.polarity < 0, ['Review']].values)\/len(df))*100\npositive = (len(df.loc[df.polarity > 0.5, ['Review']].values)\/len(df))*100\nneutral = len(df.loc[df.polarity >0 ,['Review']].values) - len(df.loc[df.polarity >0.5 ,['Review']].values)\nneutral = neutral\/len(df)*100\n\nplt.figure(figsize =(10, 7)) \nplt.pie([positive,negative,neutral], labels = ['Positive','Negative','Neutral']) \nplt.show()","995fe1bd":"# Reviews with positive polarity\n\npos = df.loc[df.polarity == 1,['Review']].sample(3).values\nfor i in pos:\n    print(i)","99980798":"# Reviews with negative polarity\n\nneg = df.loc[df.polarity < 0,['Review']].sample(3).values\nfor i in neg:\n    print(i)","6baf58ee":"# Reviews with neutral polarity\n\nneu = df.loc[df.polarity == 0,['Review']].sample(3).values\nfor i in neu:\n    print(i)","4a737e28":"# Distribution of Sentiment Polarity based on Recommendation\n\nx1 = df[df['Recommended']==1]['polarity']\nx0 = df[df['Recommended']==0]['polarity']\n\ntrace1 = go.Histogram(x=x0, name='Not Recommended', opacity=0.6)\ntrace0 = go.Histogram(x=x1, name='Recommended', opacity=0.8)\n\ndata = [trace0,trace1]\nlayout = go.Layout(barmode='overlay', title='Distribution of Sentiment Polarity of Reviews Based On The Recommendation ')\nfig = go.Figure(data=data,layout=layout)\nfig.show()","91d76fa1":"# Distribution of Sentiment Polarity based on Rating\n\nr1 = df[df['Rating'] == 1]['polarity']\nr2 = df[df['Rating'] == 2]['polarity']\nr3 = df[df['Rating'] == 3]['polarity']\nr4 = df[df['Rating'] == 4]['polarity']\nr5 = df[df['Rating'] == 5]['polarity']\n\nrat1 = go.Histogram(x=r1, name='1', opacity=0.5)\nrat2 = go.Histogram(x=r2, name='2', opacity=0.6)\nrat3 = go.Histogram(x=r3, name='3', opacity=0.7)\nrat4 = go.Histogram(x=r4, name='4', opacity=0.8)\nrat5 = go.Histogram(x=r5, name='5', opacity=0.9)\n\ndata = [rat1, rat2, rat3, rat4, rat5]\nlayout = go.Layout(barmode='overlay', title='Distribution of Sentiment Polarity of Reviews Based On The Rating')\nfig = go.Figure(data=data, layout=layout)\nfig.show()","31fd87a6":"<b> The dataset has 23,486 records and 8 columns. <\/b>","5eb40546":"### 1. Unigram","f1df5c26":"### 1. Unigram","353c9469":"### 2. Bigram","8506ac86":"<b> There are 3 duplicate records. <\/b>","494255f8":"## Text Preprocessing","f8e4d97a":"<b> The mean rating for products which were recommended is 4.59 and for products which weren't recommended is 2.30. <\/b>","870b731d":"<b> The dataset has 4 integer and 4 object columns. <\/b>","8602f33f":"<b> 45,438 customers who gave a positive review recommend the product whereas 14,114 customers who gave review didn't recommend it. <\/b>","8dffd8e1":"<b> Maximum positive feedback is for products with rating 4 followed by products with rating 5. <\/b>","d1b456ed":"### 2. Bigram","1c81b832":"<b> Most reviews have 200-300 length. <\/b>","1e5d8278":"## N-Gram analysis before removing stopwords","e712ac50":"### 3. Trigram","5555ffc1":"<b> People in their 30s are more likely to give rating. <\/b>","f894fbc6":"<b> Most reviews have 30-40 words. <\/b>","79f4851d":"## Loading the dataset","54874d3f":"<b> Hence, all duplicate records are removed. <\/b>","4ac30f70":"## Importing libraries","edc34dec":"### 3. Trigram","890bbd3f":"## N-Gram analysis after removing stopwords","421e357d":"<b> Most clothes belong to General division, Tops department and are Dresses or Knits. <\/b>"}}