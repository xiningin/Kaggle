{"cell_type":{"864be833":"code","3094f607":"code","bc2e08c8":"code","04c237c1":"code","2d5d9f2f":"code","76a588c7":"code","9e96177b":"code","24c993ce":"code","3a8df48e":"code","4a01289c":"code","148762b5":"code","9756be99":"code","9e31175f":"code","4902aa68":"code","73f3db17":"code","53cbe5ca":"code","1d4382c6":"code","64ed29b1":"code","d1553ac0":"code","95557213":"code","9cabcb04":"code","c2546339":"code","cb1e1bc5":"code","413e0682":"code","847f1205":"code","32fabfc2":"code","1e8fa123":"code","a658d1ca":"code","92ac2e45":"code","4ad40a98":"code","7fab675e":"code","09add260":"code","be721fbe":"code","b30f4e1d":"code","dffc72fd":"code","ff25ad1c":"code","18583b1a":"code","fadb6faf":"markdown","9f3d5a57":"markdown","385ce705":"markdown","b499c339":"markdown","4b434b71":"markdown","d9c42d40":"markdown","5ec68faf":"markdown","0dc15b51":"markdown","56519ead":"markdown","c5e688cf":"markdown"},"source":{"864be833":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3094f607":"data=pd.read_csv(\"..\/input\/voice.csv\")","bc2e08c8":"data.head(7)","04c237c1":"data.describe()","2d5d9f2f":"data.columns","76a588c7":"data.info()","9e96177b":"data.corr()","24c993ce":"#correlation map\nf,ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","3a8df48e":"data.label=[0 if each==\"female\" else 1 for each in data.label]\n#print(data.info()) #2 adet classimiz oldu 1 erkek 0 bayan\n\ny=data.label.values\nx_data=data.drop([\"label\"],axis=1)\n","4a01289c":"#%% normalization\n# (x-max)\/(max-min)\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","148762b5":"#%% train test and split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T\n\nprint(\"x train : \",x_train.shape)\nprint(\"x test : \",x_test.shape)\nprint(\"y train : \",y_train.shape)\nprint(\"y test : \",y_test.shape)\n","9756be99":"#%% parameter initialize and sigmoid function\n#dimension=20\ndef initialize_weights_and_bias(dimension):\n    w=np.full((dimension,1),0.01)\n    b=0.0 \n    return w,b\n\ndef sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head\n#print(sigmoid(0))","9e31175f":"#%%\ndef forward_backward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)\n    loss=-y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost=(np.sum(loss))\/x_train.shape[1]\n    #backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients={\"derivative_weight\":derivative_weight,\"derivative_bias\":derivative_bias}\n    return cost,gradients","4902aa68":"#%%Updating(Learning) parameters\ndef update(w,b,x_train,y_train,learning_rate,number_of_iterarion):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    \n    for i in range(number_of_iterarion):\n        cost,gradients=forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        #lets update\n        w=w-learning_rate*gradients[\"derivative_weight\"]\n        b=b-learning_rate*gradients[\"derivative_bias\"]\n        if i%10 ==0:\n            cost_list2.append(cost)\n            index.append(i)#grafik i\u00e7in bunlar\u0131 ald\u0131k\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    #we update(learn) parameters weights and bias\n    parameters={\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number of Iterarrion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list","73f3db17":"#%%\n# prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test) ","53cbe5ca":"#%%\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 20\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n","1d4382c6":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)","64ed29b1":"algo_score_list=[]","d1553ac0":"#%% sklearn with lr\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))\nalgo_score_list.append([\"logistic reg\",lr.score(x_test.T,y_test.T)])\n","95557213":"#%% sklearn with knn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train.T,y_train.T)\nprediction = knn.predict(x_test.T)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test.T,y_test.T)) # accuracy\ny_pred=knn.predict(x_test.T)\n","9cabcb04":"#find best k value\nscore_list=[]\nfor each in range(1,15):\n    knn2=KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train.T,y_train.T)\n    score_list.append(knn2.score(x_test.T,y_test.T))\n# plot\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"score\")\nplt.show()\n\n#you will see k=8 best \nalgo_score_list.append([\"knn\",knn2.score(x_test.T,y_test.T)])","c2546339":"from sklearn.svm import SVC\n\nsvm=SVC(random_state=1)\nsvm.fit(x_train.T,y_train.T)","cb1e1bc5":"print(\"pring accuracy of svm algo:\",svm.score(x_test.T,y_test.T))\nalgo_score_list.append([\"svm\",svm.score(x_test.T,y_test.T)])","413e0682":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train.T,y_train.T)\n\n","847f1205":"print(\"print accuracy of naive bayes algo:\",nb.score(x_test.T,y_test.T))\nalgo_score_list.append([\"naive bayes\",nb.score(x_test.T,y_test.T)])\n","32fabfc2":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train.T,y_train.T)\n\nprint(\"Tree score:\",dt.score(x_test.T,y_test.T))\nalgo_score_list.append([\"decision tree\",dt.score(x_test.T,y_test.T)])","1e8fa123":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=100,random_state=1)\nrf.fit(x_train.T,y_train.T)\nprint(\"Random forest score:\",rf.score(x_test.T,y_test.T))\nalgo_score_list.append([\"random forest\",rf.score(x_test.T,y_test.T)])\n","a658d1ca":"algo_score_list","92ac2e45":"algo_score_List=np.array(algo_score_list)\nalgo_score_List","4ad40a98":"\nalgo_score_sorted_list = algo_score_List[algo_score_List[:,1].argsort()]\nalgoritma_isimleri=algo_score_sorted_list[:,0]\nalgoritma_skorlari=algo_score_sorted_list[:,1]\n\n","7fab675e":"# Plot\nx=algoritma_isimleri\ny=algoritma_skorlari\nplt.figure(figsize=(7,7))\nplt.scatter(x, y,alpha=0.5)\nplt.grid()\nplt.title('Alghoritm Performance')\nplt.xlabel('Alghoritm')\nplt.ylabel('Score')\nplt.show()","09add260":"#%% confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test.T,y_pred)\n\n\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n\n\n","be721fbe":"x_train=x_train.T\nx_train.shape","b30f4e1d":"y_train.reshape(2534,1)","dffc72fd":"y_train.shape","ff25ad1c":"x_train.shape","18583b1a":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = 20))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train.T, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","fadb6faf":"# EDA\n","9f3d5a57":"Pre Processing the data","385ce705":"# Navie Bayes","b499c339":"# KNN implemantion\n","4b434b71":"# Random Forest","d9c42d40":"# CONCLUS\u0130ON \n* En iyi algoritma knn olarak se\u00e7ilmesi gerekir.Bu veri seti i\u00e7in Kom\u015fuluk y\u00f6ntemiyle en iyi sonucu elde etmi\u015f oluruz\n* \u0130lgin\u00e7 \u015fekilde bayan ve erkek hata say\u0131lar\u0131 e\u015fit \u00e7\u0131kt\u0131.Ama algoritma erkek de\u011ferleri i\u00e7in daha iyi tahmin \u00fcretir\n* Best option is knn algorithm\n* Interestingly, the female and male error numbers are equal. But the algorithm produces better estimates for male values\n","5ec68faf":"# ANN implantation section","0dc15b51":"# Decision Tree","56519ead":"## sklearn with lr\n\n","c5e688cf":"# SVM "}}