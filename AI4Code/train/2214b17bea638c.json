{"cell_type":{"06c46451":"code","9cb8db8d":"code","3e9cb863":"code","8d62c370":"code","0246f9c2":"code","5da629ad":"code","2805d562":"code","01bbb3b1":"code","7c67f3e6":"code","935fdf3a":"code","e127e8f2":"code","1bfd61a3":"code","c80cfd24":"code","797cc50b":"code","21a98847":"code","223c3cd7":"code","4e819554":"code","99c7d8cc":"code","8c3b0c2d":"code","7447fb94":"code","5dee3476":"code","cf63a683":"code","695ddc19":"code","4926744a":"code","22b5b299":"code","7e043708":"code","44498fe1":"code","dccace6a":"code","5f486af3":"code","39fdbd9d":"code","f9408063":"code","24464538":"code","7cf3024a":"code","106b3076":"code","781edbda":"code","1e0752e0":"code","a3bd85d5":"code","3f3acabf":"code","8bc6f0a0":"code","7d2afc8f":"code","ce7d6c5c":"code","3c35e00f":"code","d601899f":"code","50337ad5":"code","073cfb8f":"code","7b7c36f1":"code","ccca0228":"code","7c3ce222":"code","aa38da77":"code","bd284fd2":"code","84f8e3b1":"code","7489f1da":"code","fd2f42b1":"code","e6967545":"code","a0065777":"code","7be71044":"code","b90eeace":"code","e883dc04":"code","3958f785":"code","7899d44f":"code","228e6dc1":"code","1ee25e54":"code","0fd70cc6":"code","f9216607":"code","c8958903":"code","56f1d02a":"code","9157e412":"code","e6cc60d4":"code","aabf25a3":"code","b0d82af5":"code","6861dcc6":"code","3a260582":"code","e400c417":"code","f011087f":"code","31939878":"code","f3d2ca57":"code","f39afb3f":"code","3d60401e":"code","07d41abe":"code","aa084198":"code","226b049d":"code","24bcd3bc":"code","ac9d0a0b":"code","bc85aca9":"code","7b50d41b":"code","5eeae811":"code","7e404475":"code","cdd738cc":"markdown","284c7357":"markdown","55615511":"markdown","1696abc8":"markdown","58a17568":"markdown","3cc39ede":"markdown","b1e56937":"markdown","c442f312":"markdown","ffde7cfc":"markdown","8e385b41":"markdown","7937f158":"markdown","b8896d90":"markdown","6522df08":"markdown","ee02c2c7":"markdown","850038c4":"markdown","8346a571":"markdown","fa1636d5":"markdown","8a869f42":"markdown"},"source":{"06c46451":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9cb8db8d":"import pandas as pd\nimport numpy as np\nimport xgboost\nimport matplotlib\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(10,8)})\n","3e9cb863":"train = pd.read_csv('..\/input\/janatahack-crosssell-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/janatahack-crosssell-prediction\/test.csv')\nsample = pd.read_csv('..\/input\/janatahack-crosssell-prediction\/sample_submission.csv')","8d62c370":"train.head()","0246f9c2":"test.head()","5da629ad":"train.head()","2805d562":"test[test['Annual_Premium']>200000]","01bbb3b1":"# mark train and test dataset and merge them\ntrain['train_or_test'] = 1\ntest['train_or_test'] = 0\ntrain = train[train['Annual_Premium']<200000]\ntrain.reset_index(drop=True, inplace=True)\nmerge_data = pd.concat([train, test])","7c67f3e6":"merge_data.nunique()","935fdf3a":"merge_data.info()","e127e8f2":"merge_data.loc[merge_data.train_or_test==1].info()","1bfd61a3":"merge_data.fillna(0,inplace=True)","c80cfd24":"merge_data.head()","797cc50b":"# changing type of Response, Region_Code, Policy_Sales_Channel\nmerge_data['Response'] = merge_data['Response'].astype('int')\nmerge_data['Region_Code'] = merge_data['Region_Code'].astype('int')\nmerge_data['Policy_Sales_Channel'] = merge_data['Policy_Sales_Channel'].astype('int')","21a98847":"merge_data['Age'].plot.hist()","223c3cd7":"# creating new category for Age\nmerge_data['Age_group'] = pd.cut(x=merge_data['Age'], bins=range(20,90,10)).astype('str')\n","4e819554":"merge_data['Age_group'].value_counts()","99c7d8cc":"# adding Vintage into seperate months\nmerge_data['year_vintage'] = merge_data['Vintage']\/\/365\nmerge_data['months_vintage'] = merge_data['Vintage']\/\/30","8c3b0c2d":"merge_data.head()","7447fb94":"merge_data.groupby('Response')['Driving_License'].value_counts()","5dee3476":"merge_data.groupby('Response')['Driving_License'].value_counts().plot(kind='bar')","cf63a683":"merge_data.groupby('Response')['Gender'].value_counts()","695ddc19":"merge_data.groupby('Response')['Gender'].value_counts().plot(kind='bar')","4926744a":"sns.countplot(x='Response', hue='Gender', data=merge_data)","22b5b299":"merge_data[merge_data.Response==1]['Region_Code'].value_counts().nlargest(5)","7e043708":"merge_data[merge_data.Response==1]['Region_Code'].value_counts().plot(kind='bar')","44498fe1":"# region no. 28 have more no of users who take the insurance","dccace6a":"merge_data.groupby('Response')['Previously_Insured'].value_counts()","5f486af3":"sns.countplot(x='Response', hue='Previously_Insured', data=merge_data)","39fdbd9d":"## user have previously taken the insurance will mostly take the insurance","f9408063":"sns.countplot(x='Response', hue='Age_group', data=merge_data[merge_data.Response==1])","24464538":"#  ppl from age group 40-50, 30-40 have more no. insurance responses","7cf3024a":"merge_data[merge_data.Response==1]['Policy_Sales_Channel'].value_counts().nlargest(10).plot(kind='bar')","106b3076":"# 26,124 this sales channel have high no. responses","781edbda":"# sns.barplot(x='Vintage', y='Response', data=merge_data[merge_data.Response==1])\nmerge_data.groupby('Response')['year_vintage'].value_counts()\n# Vintage","1e0752e0":"merge_data.groupby('Response')['Vehicle_Damage'].value_counts()","a3bd85d5":"# Vehicles which have damaged in the past are most likely to have insurance","3f3acabf":"merge_data['annual_bin'] = pd.cut(merge_data['Annual_Premium'], bins=np.arange(0,200000,5000), labels=range(1,40)).cat.add_categories(40)","8bc6f0a0":"merge_data['annual_bin'] = merge_data['annual_bin'].fillna(40)\nmerge_data['annual_bin'] = merge_data['annual_bin'].astype('int')","7d2afc8f":"merge_data['Annual_Premium']=np.log(merge_data['Annual_Premium'])","ce7d6c5c":"merge_data.head()","3c35e00f":"merge_data.columns","d601899f":"merge_data['Vehicle_Age'] = merge_data['Vehicle_Age'].map({'> 2 Years':1, '1-2 Year':2, '< 1 Year':0})","50337ad5":"# from category_encoders.helmert import HelmertEncoder\n# from category_encoders.cat_boost import CatBoostEncoder\n# categories = ['Gender', 'Driving_License', 'Vehicle_Damage', 'Age_group']\n# cat = CatBoostEncoder()\n\n# merge_data.loc[merge_data.train_or_test==1, categories] = cat.fit_transform(merge_data[merge_data.train_or_test==1].drop(['Response'], axis=1)[categories], y=merge_data[merge_data.train_or_test==1]['Response'])\n# merge_data.loc[merge_data.train_or_test==0, categories] = cat.transform(merge_data[merge_data.train_or_test==1].drop(['Response'], axis=1)[categories])","073cfb8f":"sns.countplot(x='Vehicle_Age', hue='Response', data=merge_data)","7b7c36f1":"merge_data.head()","ccca0228":"# convert gender, vehicle_age, vehicle_damge, age_group categories to integer values\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ncategories = ['Gender', 'Driving_License','Vehicle_Age', 'Vehicle_Damage', 'Age_group']\nfor i in categories:\n    merge_data[i] = encoder.fit_transform(merge_data[i])","7c3ce222":"# frequency encoding\nfe=merge_data.groupby('Vehicle_Age').size()\/len(merge_data)\nmerge_data['Vehicle_Age']=merge_data['Vehicle_Age'].apply(lambda x: fe[x])","aa38da77":"# frequency encoding\nfe=merge_data.groupby('Policy_Sales_Channel').size()\/len(merge_data)\nmerge_data['Policy_Sales_Channel']=merge_data['Policy_Sales_Channel'].apply(lambda x: fe[x])","bd284fd2":"merge_data.head()","84f8e3b1":"merge_data.info()","7489f1da":"# Annual_Premium\nmerge_data['mean_Annual_Premium_region']=merge_data.groupby(['Region_Code'])['Annual_Premium'].transform('mean')\nmerge_data['sum_Annual_Premium_region']=merge_data.groupby(['Region_Code'])['Annual_Premium'].transform('sum')\nmerge_data['max_Annual_Premium_region']=merge_data.groupby(['Region_Code'])['Annual_Premium'].transform('max')\nmerge_data['min_Annual_Premium_region']=merge_data.groupby(['Region_Code'])['Annual_Premium'].transform('min')\nmerge_data['std_Annual_Premium_region']=merge_data.groupby(['Region_Code'])['Annual_Premium'].transform('std')\n\n# age_Group\nmerge_data['mean_Annual_Premium_Age_group']=merge_data.groupby(['Age_group'])['Annual_Premium'].transform('mean')\nmerge_data['sum_Annual_Premium_Age_group']=merge_data.groupby(['Age_group'])['Annual_Premium'].transform('sum')\nmerge_data['max_Annual_Premium_Age_group']=merge_data.groupby(['Age_group'])['Annual_Premium'].transform('max')\nmerge_data['min_Annual_Premium_Age_group']=merge_data.groupby(['Age_group'])['Annual_Premium'].transform('min')\nmerge_data['std_Annual_Premium_Age_group']=merge_data.groupby(['Age_group'])['Annual_Premium'].transform('std')\n\n# Policy_Sales_Channel\nmerge_data['mean_Annual_Premium_Policy_Sales_Channel']=merge_data.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('mean')\nmerge_data['sum_Annual_Premium_Policy_Sales_Channel']=merge_data.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('sum')\nmerge_data['max_Annual_Premium_Policy_Sales_Channel']=merge_data.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('max')\nmerge_data['min_Annual_Premium_Policy_Sales_Channel']=merge_data.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('min')\nmerge_data['std_Annual_Premium_Policy_Sales_Channel']=merge_data.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('std')","fd2f42b1":"merge_data.head()","e6967545":"## Now our data looks is ready to modelling.","a0065777":"# merge_data = pd.get_dummies(merge_data, columns=['Vehicle_Damage', 'Previously_Insured'])","7be71044":"merge_data.head()","b90eeace":"# importing models\nfrom sklearn import tree\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier\nimport xgboost\nimport lightgbm as lgb\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom imblearn.over_sampling import SMOTE\n","e883dc04":"# drop id column because its not that important\nmerge_data.drop(['id'], axis=1, inplace=True)","3958f785":"from sklearn.preprocessing import StandardScaler","7899d44f":"# Divide train and test for training\nX = merge_data[merge_data['train_or_test']==1].drop(['Response', 'train_or_test'],axis=1)\ny = merge_data[merge_data['train_or_test']==1]['Response']","228e6dc1":"features=['Gender', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured',\n       'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium',\n       'Policy_Sales_Channel','Vintage',\n       'Age_group', 'months_vintage','mean_Annual_Premium_region',\n       'sum_Annual_Premium_region', 'max_Annual_Premium_region',\n       'min_Annual_Premium_region', 'mean_Annual_Premium_Age_group',\n       'sum_Annual_Premium_Age_group', 'max_Annual_Premium_Age_group',\n       'min_Annual_Premium_Age_group',\n       'mean_Annual_Premium_Policy_Sales_Channel',\n       'sum_Annual_Premium_Policy_Sales_Channel',\n       'max_Annual_Premium_Policy_Sales_Channel',\n       'min_Annual_Premium_Policy_Sales_Channel'\n         ]\n# features=['Gender', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured',\n#        'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium',\n#        'Policy_Sales_Channel', 'Vintage',\n#        'Age_group', 'months_vintage', 'mean_Annual_Premium_region',\n#        'sum_Annual_Premium_region', 'max_Annual_Premium_region',\n#        'min_Annual_Premium_region', 'mean_Annual_Premium_Age_group',\n#        'sum_Annual_Premium_Age_group', 'max_Annual_Premium_Age_group',\n#        'min_Annual_Premium_Age_group',\n#        'mean_Annual_Premium_Policy_Sales_Channel',\n#        'sum_Annual_Premium_Policy_Sales_Channel',\n#        'max_Annual_Premium_Policy_Sales_Channel',\n#        'min_Annual_Premium_Policy_Sales_Channel'\n#          ]\n\n\n","1ee25e54":"# Divide train and test for training\ntrain = merge_data[merge_data['train_or_test']==1]\nX = train.drop(['Response', 'train_or_test'],axis=1)[features]\ny = merge_data[merge_data['train_or_test']==1]['Response']","0fd70cc6":"X.shape, y.shape","f9216607":"test.shape","c8958903":"## test dataset\ntest = merge_data[merge_data['train_or_test']==0].drop(['Response', 'train_or_test'],axis=1)[features]","56f1d02a":"test.head().shape","9157e412":"np.arange(0.1,1+0.1,0.1,dtype=float)","e6cc60d4":"from hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nimport xgboost as xgb\nfrom functools import partial\nfrom sklearn import model_selection,metrics","aabf25a3":"param_space_xgb = {\n        'max_depth': scope.int(hp.quniform(\"max_depth\", 3,15,1)),\n        'n_estimators':scope.int(hp.quniform(\"n_estimators\", 100, 600, 1)),\n        'colsample_bytree':hp.uniform(\"colsample_bytree\", 0.01, 1),\n        \n        'scale_pos_weight': scope.int(hp.quniform(\"scale_pos_weight\", 1, 20, 1)),\n        'eta':hp.uniform(\"eta\", 0.01, 0.3),\n\n        'tree_method':hp.choice('tree_method', [\"gpu_hist\"]),\n        'gpu_id':hp.choice('gpu_id', [0]),\n    }\nparam_space_catboost = {\n        'depth': scope.int(hp.quniform(\"max_depth\", 3,15,1)),\n        'n_estimators':scope.int(hp.quniform(\"n_estimators\", 100, 1500,1)),\n        \n        'scale_pos_weight': scope.int(hp.quniform(\"scale_pos_weight\", 1, 20, 1)),\n        'learning_rate':hp.uniform(\"learning_rate\", 0.01, 0.3),\n        'l2_leaf_reg':hp.uniform(\"l2_leaf_reg\", 0.5,1),\n        'bagging_temperature':hp.uniform(\"bagging_temperature\", 0.01,1),\n    \n        'eval_metric':hp.choice('eval_metric', [\"AUC\"]),\n        'task_type':hp.choice('task_type', ['GPU']),\n        'devices':hp.choice('devices', ['0:1']),\n    \n    }\n    \ntrials = Trials()","b0d82af5":"import catboost","6861dcc6":"def optimize(params, x, y):\n    \n    model = catboost.CatBoostClassifier(**params)\n\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    aucs = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, test_idx = idx[0], idx[1]\n        x_train = x[train_idx]\n        y_train = y[train_idx]\n\n        x_test = x[test_idx]\n        y_test = y[test_idx]\n\n        model.fit(x_train, y_train, eval_set=[(x_test, y_test)], early_stopping_rounds=100, verbose=200)\n        preds = model.predict_proba(x_test)[:,1]\n        fold_auc = metrics.roc_auc_score(y_test, preds)\n        aucs.append(fold_auc)\n        \n    return -1.0*np.mean(aucs)","3a260582":"optimize_function = partial(optimize, x = X.values, y=y.values)\nresults = fmin(fn=optimize_function, space=param_space_catboost, max_evals=15, trials=trials, algo=tpe.suggest)\nprint(dict(zip(param_names, results.x)))","e400c417":"from hyperopt import hp\nfrom hyperopt.pyll import scope\nparam_hyperopt= {\n    'learning_rate': hp.choice('learning_rate', [0.1, 0.4, 0.04,0.3]),\n    'depth': hp.choice('max_depth', np.arange(1, 13+1, dtype=int)),\n    'n_estimators': hp.choice('n_estimators', [100, 200, 300,400, 500,600,700,800,900]),\n    'task_type':hp.choice('task_type', ['GPU']),\n    'early_stopping_rounds': hp.choice('early_stopping_rounds', [100]),\n    'devices': hp.choice('devices',['0:1']),\n#     'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.6,1+0.1,0.1,dtype=float)),\n    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 0.0, 1.0),\n    'eval_metric': hp.choice('eval_metric', ['AUC']),\n    'random_strength': hp.choice('random_strength', np.arange(0.1,1+0.1,0.1,dtype=int)),\n    'bagging_temperature': hp.choice('bagging_fraction', np.arange(0.1,1+0.1,0.1,dtype=float)),\n}","f011087f":"import lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nimport time\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5)\n\n\ndef my_custom_loss_func(y_true, y_pred):\n    print(y_pred.shape, y_true.shape)\n    return roc_auc_score(y_true, y_pred)\n\ncustom_roc = make_scorer(my_custom_loss_func, greater_is_better=True)\ncustom = {'roc':custom_roc}\n\ndef hyperopt(param_space, X_train, y_train, X_test, num_eval):\n    \n    start = time.time()\n    \n    def objective_function(params):\n        clf = CatBoostClassifier(**params)\n        cv_probs = cross_val_predict(clf, X_train, y_train, cv=skf,method='predict_proba')\n        cv_probs = np.array(cv_probs)\n        auc = []\n        for train_index, test_index in skf.split(X_train, y_train):\n            we = np.array(y_train)\n            auc.append(roc_auc_score(we[test_index], cv_probs[test_index][:,1]))\n        score = np.mean(auc)\n        print(np.mean(auc))\n        return {'loss': 1-score, 'status': STATUS_OK}\n\n    trials = Trials()\n    best_param = fmin(objective_function, \n                      param_space, \n                      algo=tpe.suggest, \n                      max_evals=num_eval, \n                      trials=trials,\n                      rstate= np.random.RandomState(1))\n    loss = [x['result']['loss'] for x in trials.trials]\n    \n    best_param_values = [x for x in best_param.values()]\n    \n    if best_param_values[0] == 0:\n        boosting_type = 'gbdt'\n    else:\n        boosting_type= 'dart'\n    \n    clf_best = CatBoostClassifier(learning_rate=best_param_values[2],\n                                  num_leaves=int(best_param_values[5]),\n                                  max_depth=int(best_param_values[3]),\n                                  n_estimators=int(best_param_values[4]),\n                                  boosting_type=boosting_type,\n                                  colsample_bytree=best_param_values[1],\n                                  reg_lambda=best_param_values[6],\n                                 )\n                                  \n    clf_best.fit(X_train, y_train)\n    \n    print(\"\")\n    print(\"##### Results\")\n    print(\"Score best parameters: \", max(loss)*-1, max(loss))\n    print(\"Best parameters: \", best_param)\n#     print(\"Test Score: \", clf_best.score(X_test, y_test))\n#     print(\"Time elapsed: \", time.time() - start)\n#     print(\"Parameter combinations evaluated: \", num_eval)\n    \n    return trials","31939878":"from hyperopt import fmin, tpe, Trials\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\ntrials = Trials()\n# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y)\n\nresults_hyperopt = hyperopt(param_hyperopt, X, y, test, 75)\n\nbest_param = fmin(objective_function, param_hyperopt, algo=tpe.suggest, max_evals=75, trials=trials, rstate= np.random.RandomState(1))\n","f3d2ca57":"\nfrom sklearn.svm import SVC\nfrom imblearn.combine import SMOTETomek\n\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom imblearn.over_sampling import SVMSMOTE\n\nclass Modelling_class:\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        self.roc_auc_each = {}\n        self.xgb = xgboost.XGBClassifier(\n    booster = \"gbtree\",\n            eval_metric='auc',reg_lambda=2,max_delta_step=4,\n    n_estimators=500,\n    learning_rate=0.04, colsample_bytree=0.9,\n    seed=42,tree_method='gpu_hist', gpu_id=0, )\n        \n        self.lgb = lgb.LGBMClassifier(boosting_type='gbdt',n_estimators=500,depth=10,learning_rate=0.04,objective='binary',metric='auc',is_unbalance=True,\n                 colsample_bytree=0.5,reg_lambda=2,reg_alpha=2,random_state=22,n_jobs=-1)\n        \n        self.lgb_k = lgb.LGBMClassifier(random_state=22,n_jobs=-1,max_depth=-1,min_data_in_leaf=24,num_leaves=49,bagging_fraction=0.01,metric='auc',\n                        colsample_bytree=1.0,lambda_l1=1,lambda_l2=11,learning_rate=0.1,n_estimators=5000)\n        \n        self.rf = RandomForestClassifier(n_jobs=-1)\n        self.skf = StratifiedKFold(n_splits=10)\n        self.lin = Pipeline([('scaler', StandardScaler()), ('svc', LogisticRegression(max_iter=500 ))])\n        self.sgd = Pipeline([('scaler', StandardScaler()), ('sgd', SGDClassifier(loss='log'))])\n        self.svc = Pipeline([('scaler', StandardScaler()), ('svc', SVC(gamma='auto', class_weight='balanced'))])\n        self.quad = Pipeline([('scaler', StandardScaler()), ('quad', QuadraticDiscriminantAnalysis())])\n        \n        self.ada = Pipeline([('scaler', StandardScaler()), ('ada', AdaBoostClassifier(n_estimators=100, random_state=0))])\n        \n        self.kf = KFold(n_splits=5)\n        self.catboost = CatBoostClassifier(n_estimators=2000,\n                       random_state=20,\n                       eval_metric='AUC',\n                       learning_rate=0.1,\n                       depth=6,l2_leaf_reg=0.5,\n#                        bagging_temperature=0.1,\n                       task_type='GPU', verbose=10,devices='0:1'\n                       #num_leaves=64\n                       \n                       )\n        self.best_catboost = CatBoostClassifier(random_state=42)\n        \n        self.total = 0\n        self.model_dict = {}\n        self.temp = None\n        self.y_pred_tot = np.zeros((len(test), 2))\n        self.models = None\n        self.define_models()\n    \n    def define_models(self):\n        \n        self.models =  {\n            'lgb': self.lgb_k,\n            'xgb': self.xgb, \n                 \n#                 'rf': self.rf, \n            \n#                 'lin':self.lin,\n#                 'sgd':self.sgd,\n#             'svc':self.svc,\n#             'quad':self.ada,\n#             'best_Cat':self.best_catboost\n                'catboost':self.catboost\n        }\n    \n    def stacking(self):\n        self.models = {'stacking': StackingClassifier(estimators = [('xgb', self.xgb), ('lgb',self.lgb)])}\n        \n        \n    def evaluate(self, test, target, smote=False, standard_scalar = False):    \n        c = 0\n    \n        for model_name, model in self.models.items():\n            c = 0\n           \n            for train_id, test_id in self.skf.split(self.X,self.y):\n                \n#                 if c > \/\/:\n#                     continue\n                \n                X_train, X_test = self.X.loc[train_id], self.X.loc[test_id]\n                y_train, y_test = self.y[train_id], self.y[test_id]\n                \n                model.fit(X_train, y_train,eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=200)\n                self.model_dict[model_name+str(c)] = model\n\n                y_pred = model.predict(X_test)\n                self.temp = model.predict_proba(X_test)\n                y_proba =  model.predict_proba(X_test)[:,1]\n\n                self.roc_auc_each[model_name+str(c)] = roc_auc_score(y_test,y_proba)\n\n                c+=1\n                print(\"model_name-\",model_name,\"-score:\",roc_auc_score(y_test,y_proba))\n\n                self.total+=1\n                print(self.total_roc_auc())\n                finals = np.array(model.predict_proba(test)[:,1])\n                sample[target] = finals\n                sample.to_csv('.\/'+model_name+str(c)+'.csv',index=False)\n                self.y_pred_tot+= model.predict_proba(test)\n                print(self.y_pred_tot.shape)\n                \n    def total_roc_auc(self):\n        print(\"TOTAL ROC_AUC\")\n        return sum(self.roc_auc_each.values())\/self.total\nmodels = Modelling_class(X,y )\nmodels.evaluate(test, 'Response')\nprint(models.total_roc_auc)","f39afb3f":"models.y_pred_tot=models.y_pred_tot\/models.total\nsample['Response'] = models.y_pred_tot[:,1]\nsample['Response'].value_counts(normalize=True)\nsample.to_csv('xgb_lgb_cat_norm.csv',index=False)","3d60401e":"def without_valid_prediction(test):\n    cat = models.model_dict['base_cat1']\n    cat.fit(X,y)\n    cats_pred = cat.predict_proba(test)[:,1]\n    final = np.array(cats_pred)\n    sample['Response'] = final\n    sample['Response'].value_counts(normalize=True)\n    sample.to_csv('we.csv',index=False)\nwithout_valid_prediction(test)","07d41abe":"first = pd.read_csv('https:\/\/datahack-prod.s3.amazonaws.com\/submissions\/janatahack-cross-sell-prediction\/896_729963_us_xgb_lgb_norm15_9_3dnzg6c.csv')\nsecond = pd.read_csv('https:\/\/datahack-prod.s3.amazonaws.com\/submissions\/janatahack-cross-sell-prediction\/896_729963_us_xgb_lgb_cat_norm_7_RFIJ2mJ.csv')\nfirst['Response'] = (first['Response'] + second['Response'])\/2","aa084198":"first.loc[first.id.isin(test[test['Annual_Premium']>200000]['id'])] = second[second.id.isin(test[test['Annual_Premium']>200000]['id'])]","226b049d":"first.to_csv('final.csv', index=False)","24bcd3bc":"first.head()","ac9d0a0b":"second","bc85aca9":"# s\nmodels.stacking()\nmodels.evaluate(test, 'Response')","7b50d41b":"models.y_pred_tot=models.y_pred_tot\/5\nsample['Response'] = models.y_pred_tot[:,1]\n# sample['Response'].value_counts(normalize=True)\nsample.to_csv('stacking.csv',index=False)","5eeae811":"stacking = pd.read_csv('stacking.csv')\nxgb_final = pd.read_csv('https:\/\/datahack-prod.s3.amazonaws.com\/submissions\/janatahack-cross-sell-prediction\/896_729963_us_xgb_lgb_norm_3.csv')\nxgb_final['Response'] = (stacking['Response']+xgb_final['Response'])\/2\n","7e404475":"xgb_final.to_csv('xgb_stacking.csv', index=False)","cdd738cc":"#### 1.  Distribution of Driving_license users with Response","284c7357":"#### 3. Which regions have more responses ","55615511":"## Checking null values","1696abc8":"2. Distribution of Gender with respect to Response","58a17568":"# Preprocessing of data","3cc39ede":"#### 4. Whether previosly_insured will take insurance or not","b1e56937":"# Janatahack: Cross-sell Prediction\n\nYour client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n\nAn insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n\nFor example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000\/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000\/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n\nJust like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called \u2018sum assured\u2019) to the customer.\n\nBuilding a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue. \n\nNow, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.","c442f312":"# importing dataset","ffde7cfc":"## Modelling ","8e385b41":"# EDA of the Data\nWe should we ask following questions .\n1. Distribution of Driving_license users with Response\n2. Distribution of Gender with respect to Response\n3. Which regions have more responses \n4. Whether previosly_insured will take insurance or not\n5. Which age group have most of insurances\n6. Does policy_sales_channel affects responses?\n7. older customers will stick with insurance or not\n8. Whether past damage of vehicle affects Response","7937f158":"#### 7. older customers will stick with insurance or not","b8896d90":"## Data Postprecessing after EDA","6522df08":"##### There are no null values(Response is our target variable its empty in test dataset)","ee02c2c7":"##### Insights\nwe can say ppl who dont have license usually doesnt go for the insurance","850038c4":"### Unique Values in dataset","8346a571":"#### 5. Which age group have most of insurances","fa1636d5":"#### 8. Whether past damage of vehicle affects Response","8a869f42":"#### 6. Does policy_sales_channel affects responses?"}}