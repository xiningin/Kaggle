{"cell_type":{"0f602b4f":"code","56586170":"code","2970bda9":"code","2a487e81":"code","dc1030a6":"code","600bbe26":"code","971b2082":"code","b88e20b2":"code","410b5929":"code","5249a379":"code","9bd41dc4":"code","7a48b4fd":"code","f7b84d48":"code","6cdd7cc2":"code","97d173aa":"code","25d9eacc":"code","220b9d84":"code","f3f25191":"code","bf725a31":"code","81b101e0":"code","00309401":"code","dc8f5caf":"code","6641e849":"code","5ae150c2":"code","010ef2af":"code","b2a1717b":"code","2549c749":"code","0cfb7c23":"code","84625400":"code","c141325d":"markdown","8d9a3159":"markdown","01f0f46c":"markdown","264c59dd":"markdown","0a1cdf43":"markdown","4023f78f":"markdown","13d1e406":"markdown","798fb814":"markdown","6891edaa":"markdown","bd514d88":"markdown","f6f74eed":"markdown","a2fd0996":"markdown","505fa54e":"markdown","28675205":"markdown","9df6d7a5":"markdown","ec7aefdd":"markdown","9a140bef":"markdown"},"source":{"0f602b4f":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nimport lightgbm as lgb\nsns.set()\n","56586170":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# PATH=\"..\/input\/Santander\/\" \nPATH=\"..\/input\/\" \nprint(os.listdir(PATH))\n# Any results you write to the current directory are saved as output.","2970bda9":"%%time\nX_train_df = pd.read_csv(PATH+\"X_train.csv\")\nX_test_df = pd.read_csv(PATH+\"X_test.csv\")\nY_train_df = pd.read_csv(PATH+\"y_train.csv\")\nsub = pd.read_csv(PATH+\"sample_submission.csv\")","2a487e81":"X_train_df.shape, Y_train_df.shape , X_test_df.shape","dc1030a6":"X_train_df.head()","600bbe26":"Y_train_df.head()","971b2082":"X_test_df.head()","b88e20b2":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","410b5929":"missing_data(X_train_df)","5249a379":"missing_data(X_test_df)","9bd41dc4":"missing_data(Y_train_df)","7a48b4fd":"X_train_df.describe()","f7b84d48":"X_test_df.describe()","6cdd7cc2":"plt.figure(figsize=(10,6))\nplt.title(\"Target labels\")\nsns.countplot(y='surface', data = Y_train_df, order = Y_train_df['surface'].value_counts().index,  palette=\"Set2\")\nplt.show()","97d173aa":"from scipy.stats import kurtosis\nfrom scipy.stats import skew\n\ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))\/den\n\ndef skewness(x):\n    return skew(x)\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 3\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA\/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))\/len(x)\n\ndef zero_crossing(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1\n    return sum(np.heaviside(-xn*xn_i2,0))\n","25d9eacc":"def feature_extraction(raw_frame):\n    frame = pd.DataFrame()\n    raw_frame['angular_velocity'] = raw_frame['angular_velocity_X'] + raw_frame['angular_velocity_Y'] + raw_frame['angular_velocity_Z']\n    raw_frame['linear_acceleration'] = raw_frame['linear_acceleration_X'] + raw_frame['linear_acceleration_Y'] + raw_frame['linear_acceleration_Z']\n    raw_frame['velocity_to_acceleration'] = raw_frame['angular_velocity'] \/ raw_frame['linear_acceleration']\n    \n    for col in raw_frame.columns[3:]:\n        frame[col + '_mean'] = raw_frame.groupby(['series_id'])[col].mean()        \n        frame[col + '_CPT5'] = raw_frame.groupby(['series_id'])[col].apply(CPT5) \n        frame[col + '_SSC'] = raw_frame.groupby(['series_id'])[col].apply(SSC) \n        frame[col + '_skewness'] = raw_frame.groupby(['series_id'])[col].apply(skewness)\n        frame[col + '_wave_lenght'] = raw_frame.groupby(['series_id'])[col].apply(wave_length)\n        frame[col + '_norm_entropy'] = raw_frame.groupby(['series_id'])[col].apply(norm_entropy)\n        frame[col + '_SRAV'] = raw_frame.groupby(['series_id'])[col].apply(SRAV)\n        frame[col + '_kurtosis'] = raw_frame.groupby(['series_id'])[col].apply(_kurtosis) \n        frame[col + '_mean_abs'] = raw_frame.groupby(['series_id'])[col].apply(mean_abs) \n        frame[col + '_zero_crossing'] = raw_frame.groupby(['series_id'])[col].apply(zero_crossing) \n    return frame","220b9d84":"train_df = feature_extraction(X_train_df)\ntest_df = feature_extraction(X_test_df)\n","f3f25191":"train_df.head()","bf725a31":"train_df.shape, test_df.shape","81b101e0":"\ntrain_df.fillna(0, inplace = True)\ntrain_df.replace(-np.inf, 0, inplace = True)\ntrain_df.replace(np.inf, 0, inplace = True)\ntest_df.fillna(0, inplace = True)\ntest_df.replace(-np.inf, 0, inplace = True)\ntest_df.replace(np.inf, 0, inplace = True)","00309401":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(train_df))\nX_test_scaled = pd.DataFrame(scaler.transform(test_df))","dc8f5caf":"le = LabelEncoder()\ntarget = le.fit_transform(Y_train_df['surface'])","6641e849":"params = {\n    'num_leaves': 54,\n    'min_data_in_leaf': 40,\n    'objective': 'multiclass',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    \"boosting\": \"gbdt\",\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.8126672064208567,\n    \"bagging_seed\": 11,\n    \"verbosity\": -1,\n    'reg_alpha': 0.1302650970728192,\n    'reg_lambda': 0.3603427518866501,\n    \"num_class\": 9,\n    'nthread': -1\n}\n\ndef multiclass_accuracy(preds, train_data):\n    labels = train_data.get_label()\n    pred_class = np.argmax(preds.reshape(9, -1).T, axis=1)\n    return 'multi_accuracy', np.mean(labels == pred_class), True\n\nt0 = time.time()\ntrain_set = lgb.Dataset(X_train_scaled, label=target)\neval_hist = lgb.cv(params, train_set, nfold=10, num_boost_round=9999,\n                   early_stopping_rounds=100, seed=19, feval=multiclass_accuracy)\nnum_rounds = len(eval_hist['multi_logloss-mean'])\n\n# retrain the model and make predictions for test set\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\npredictions = clf.predict(X_test_scaled, num_iteration=None)\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))","5ae150c2":"v1, v2 = eval_hist['multi_logloss-mean'][-1], eval_hist['multi_accuracy-mean'][-1]\nprint(\"Validation logloss: {:.4f}, accuracy: {:.4f}\".format(v1, v2))\nplt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass logloss\")\nnum_rounds = len(eval_hist['multi_logloss-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_logloss-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_logloss-stdv'], ax=ax2, color='r')\n\nplt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass accuracy\")\nnum_rounds = len(eval_hist['multi_accuracy-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_accuracy-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_accuracy-stdv'], ax=ax2, color='r') ","010ef2af":"sub['surface'] = le.inverse_transform(predictions.argmax(axis=1))\nsub.to_csv('submission_lgbm.csv', index=False)","b2a1717b":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_train_scaled, target, test_size=0.2, random_state=23, stratify=target)","2549c749":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Number of optimal trees\n# -You can test other values\noptimal_k = 120 \nrf_acc = []\n\n# Optimal trees\nrf_clf = RandomForestClassifier(n_estimators=optimal_k, random_state=0)        \n# Train data split\nrf_clf.fit(X_train, Y_train)\n\n# Accuracy test data split\nY_pred = rf_clf.predict(X_test)\nacc = accuracy_score(Y_pred,Y_test)\n\nprint('Acc_Test:')\nprint(float(\"%0.3f\" % (100*acc)))","0cfb7c23":"#With original data\nrf_clf.fit(X_train_scaled, target)","84625400":"Y_test_sub = rf_clf.predict(X_test_scaled)\nsub['surface'] = le.inverse_transform(Y_test_sub)\nsub.to_csv('submission_rf.csv', index=False)","c141325d":"# Data exploration","8d9a3159":"Standarization","01f0f46c":"# LightGBM classifier ","264c59dd":"# Feature extraction","0a1cdf43":"The following plots show the mean logloss and accuracy at each iteration (blue line). The red lines are the standard deviation between folds.","4023f78f":"Let's extract 10 features from time series\n\nWe will use these papers  [link 1](https:\/\/ieeexplore.ieee.org\/document\/8181558) [link 2](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2405896318323127). We will use the top 10 features to vibration signal. ","13d1e406":"#### X_train_df have 487680 rows with 13 columns. Y_train_df have 3810 rows with 3 columns. X_test_df have 488448 rows with 13 columns. \n\nX_train_df and X_test_df contain:\n- series_id\n- measurement_number\n- 10 numerical variables named: orientation_X,\torientation_Y,\torientation_Z,\torientation_W,\tangular_velocity_X,\tangular_velocity_Y,\tangular_velocity_Z,\tlinear_acceleration_X,\tlinear_acceleration_Y and linear_acceleration_Z\n\nY_train_df contain:\n- series_id\t\n- group_id\t\n- surface\n","798fb814":"#### Let's check the distribution of surface(target) value in train(Y_train_df) dataset.","6891edaa":"The data is unbalanced! ","bd514d88":"### LGBM submission","f6f74eed":"There are no missing data train and test  datasets.","a2fd0996":"Fix -inf, +inf and NaN","505fa54e":"Check missing data","28675205":"Let's check the train and test set.","9df6d7a5":"Credits: \n- https:\/\/www.kaggle.com\/jsaguiar\/surface-recognition-baseline\n- https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction","ec7aefdd":"# RF submission","9a140bef":"# Random forest"}}