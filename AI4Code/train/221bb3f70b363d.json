{"cell_type":{"8e89c6ee":"code","09de1288":"code","09f176e2":"code","1ed3456f":"code","6c4aec9f":"code","07b96729":"code","7e3ca971":"code","af66518a":"code","d4778bde":"code","5a553315":"code","49f62f33":"code","e23d506d":"code","8abe2bbd":"code","480c2c5c":"code","7844d066":"code","1c129bdd":"code","3bba306f":"code","640c6e9f":"code","f9fbcd7c":"code","8c24ed54":"code","37b55d5a":"code","a5db92df":"code","c5053bb7":"code","10f0aa67":"code","d970f1fa":"code","19d0635d":"code","bfd21ec6":"code","ee375a45":"code","48aaed89":"code","c210dac2":"code","45717794":"code","53edcc72":"code","ef8b02af":"code","52fb9454":"code","88e27e49":"code","39afcb00":"markdown","eac39af0":"markdown","45933a65":"markdown","b0ebabe7":"markdown","1e8c052f":"markdown","1c99f616":"markdown"},"source":{"8e89c6ee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport gc\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport librosa.display\nimport librosa\nimport IPython.display as ipd\nprint(os.listdir(\"..\/input\"))","09de1288":"gc.collect()","09f176e2":"# print(os.listdir(\"..\/input\/train_curated\"))","1ed3456f":"TRAIN_NOISY_PATH = \"..\/input\/train_noisy.csv\"\nTRAIN_CURATED_PATH = \"..\/input\/train_curated.csv\"\nTRAIN_NOISY = \"..\/input\/train_noisy\/\"\nTRAIN_CURATED = \"..\/input\/train_curated\/\"\nTEST = \"..\/input\/test\/\"\nSUB_PATH = \"..\/input\/sample_submission.csv\"\n\ntrain_noisy = pd.read_csv(TRAIN_NOISY_PATH)\ntrain_curated = pd.read_csv(TRAIN_CURATED_PATH)\nsub = pd.read_csv(SUB_PATH)\n\nSAMPLING_RATE = 44100\nMFCC_NUM = 20\nMFCC_MAX_LEN = 2000","6c4aec9f":"target_labels = ['Accelerating_and_revving_and_vroom','Accordion','Acoustic_guitar','Applause','Bark','Bass_drum','Bass_guitar','Bathtub_(filling_or_washing)','Bicycle_bell','Burping_and_eructation','Bus','Buzz','Car_passing_by','Cheering','Chewing_and_mastication','Child_speech_and_kid_speaking','Chink_and_clink','Chirp_and_tweet','Church_bell','Clapping','Computer_keyboard','Crackle','Cricket','Crowd','Cupboard_open_or_close','Cutlery_and_silverware','Dishes_and_pots_and_pans','Drawer_open_or_close','Drip','Electric_guitar','Fart','Female_singing','Female_speech_and_woman_speaking','Fill_(with_liquid)','Finger_snapping','Frying_(food)','Gasp','Glockenspiel','Gong','Gurgling','Harmonica','Hi-hat','Hiss','Keys_jangling','Knock','Male_singing','Male_speech_and_man_speaking','Marimba_and_xylophone','Mechanical_fan','Meow','Microwave_oven','Motorcycle','Printer','Purr','Race_car_and_auto_racing','Raindrop','Run','Scissors','Screaming','Shatter','Sigh','Sink_(filling_or_washing)','Skateboard','Slam','Sneeze','Squeak','Stream','Strum','Tap','Tick-tock','Toilet_flush','Traffic_noise_and_roadway_noise','Trickle_and_dribble','Walk_and_footsteps','Water_tap_and_faucet','Waves_and_surf','Whispering','Writing','Yell','Zipper_(clothing)']","07b96729":"def count_labels(labels):\n    array_lbs = labels.split(\",\")\n    return len(array_lbs)\n\ndef count_target_labels(labels):\n    count = 0\n    array_lbs = labels.split(\",\")\n    for lb in array_lbs:\n        if lb in target_labels:\n            count += 1\n    return count","7e3ca971":"train_noisy[\"label_count\"] = train_noisy[\"labels\"].apply(count_labels)\ntrain_noisy[\"target_label_count\"] = train_noisy[\"labels\"].apply(count_target_labels)","af66518a":"train_noisy.head(10)","d4778bde":"print(\"Count train_noisy:\" + str(train_noisy.shape[0]))\nprint(\"Count records without target label in train_noisy:\" + str(train_noisy.query(\"target_label_count == 0\").shape[0]))","5a553315":"ipd.Audio(TRAIN_NOISY + train_noisy[\"fname\"][8])","49f62f33":"train_curated[\"label_count\"] = train_curated[\"labels\"].apply(count_labels)\ntrain_curated[\"target_label_count\"] = train_curated[\"labels\"].apply(count_target_labels)","e23d506d":"train_curated.head(10)","8abe2bbd":"print(\"Count train_curated:\" + str(train_curated.shape[0]))\nprint(\"Count records without target label in train_curated:\" + str(train_curated.query(\"target_label_count == 0\").shape[0]))","480c2c5c":"ipd.Audio(TRAIN_CURATED + train_curated[\"fname\"][8])","7844d066":"sub.head()","1c129bdd":"print(\"Count test:\" + str(sub.shape[0]))","3bba306f":"ipd.Audio(TEST + sub[\"fname\"][4])","640c6e9f":"train_curated.groupby(\"labels\").size()","f9fbcd7c":"import librosa\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nimport numpy as np\nfrom tqdm import tqdm","8c24ed54":"# test, sr = librosa.load(TRAIN_CURATED + train_curated[\"fname\"][3], sr=SAMPLING_RATE)\n# librosa.feature.mfcc(test, n_mfcc=128, sr=44100).shape","37b55d5a":"# def wav2mfcc(file_path, max_len=11):\ndef wav2mfcc(wave, max_len=MFCC_MAX_LEN):\n#     mfcc = librosa.feature.mfcc(wave, sr=16000)\n    mfcc = librosa.feature.mfcc(wave, n_mfcc=MFCC_NUM, sr=SAMPLING_RATE)\n\n    # If maximum length exceeds mfcc lengths then pad the remaining ones\n    if (max_len > mfcc.shape[1]):\n        pad_width = max_len - mfcc.shape[1]\n        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n\n    # Else cutoff the remaining parts\n    else:\n        mfcc = mfcc[:, :max_len]\n    \n    return mfcc","a5db92df":"def get_label_num(labels):\n    lbs = labels.split(\",\")\n#     target_lb = \"Accelerating_and_revving_and_vroom\"\n    target_arr = np.zeros(80)\n    for lb in lbs:\n        if(lb in target_labels):\n            i = target_labels.index(lb)\n            target_arr[i] = 1\n            break\n    return target_arr","c5053bb7":"X = []\ny = []\n\ndef append_X_Y(labels, wave):\n    y.append(get_label_num(labels))\n    mfcc = wav2mfcc(wave)\n    X.append(mfcc)\n\nfor index, row in tqdm(train_curated.iterrows()):\n    labels = row[\"labels\"]\n    wave, sr = librosa.load(TRAIN_CURATED + row[\"fname\"], mono=True, sr=44100)\n    wave = wave[::3]\n    \n#     if(len(labels.split(\",\")) == 1):\n    append_X_Y(labels, wave)\n        \n# for index, row in tqdm(train_noisy.iterrows()):\n#     labels = row[\"labels\"]\n#     wave, sr = librosa.load(TRAIN_NOISY + row[\"fname\"], mono=True, sr=None)\n#     wave = wave[::3]\n#     append_X_Y(labels, wave)\n\n# np.save('train_augumented_mfcc_vectors.npy', X)\n# np.save('train_augumented_labels.npy', y)","10f0aa67":"gc.collect()","d970f1fa":"X = np.array(X)\ny = np.array(y)\nX.shape[0] == len(y)","19d0635d":"# y_hot = to_categorical(y)\ny_hot = y","bfd21ec6":"X_train, X_test, y_train, y_test = train_test_split(X, y_hot, test_size= 0.2, random_state=True, shuffle=True)","ee375a45":"X_train.shape","48aaed89":"# Feature dimension\nfeature_dim_1 = MFCC_NUM\n# Second dimension of the feature is dim2\nfeature_dim_2 = MFCC_MAX_LEN\nchannel = 1\nepochs = 70\nbatch_size = 100\nverbose = 1\nnum_classes = len(target_labels)\n","c210dac2":"# Reshaping to perform 2D convolution\nX_train = X_train.reshape(X_train.shape[0], feature_dim_1, feature_dim_2, channel)\nX_test = X_test.reshape(X_test.shape[0], feature_dim_1, feature_dim_2, channel)\n\ny_train_hot = y_train\ny_test_hot = y_test","45717794":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras import optimizers","53edcc72":"def get_model():\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(num_classes, activation='softmax'))\n    return model","ef8b02af":"model = get_model()\n\noptimizer = optimizers.SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n# optimizer = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=optimizer,\n              metrics=['accuracy'])\nmodel.fit(X_train, y_train_hot, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_test, y_test_hot))","52fb9454":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\n\nfor index, row in tqdm(sub.iterrows()):\n    wave, sr = librosa.load(TEST + row[\"fname\"], mono=True, sr=None)\n    wave = wave[::2]\n    \n    mfcc = wav2mfcc(wave)\n    X_test = mfcc.reshape(1, feature_dim_1, feature_dim_2, channel)\n    preds = model.predict(X_test)[0]\n    \n    for i, col in enumerate(target_labels):\n        sub.loc[index, col] = preds[i]","88e27e49":"sub.to_csv(\"submission.csv\",index=False)","39afcb00":"## Train_curated","eac39af0":"# Prepare MFCC data","45933a65":"- \u3068\u308a\u3042\u3048\u305a\u3001MFCC\u3092CNN\u3067\u5b66\u7fd2\u3059\u308b\n- \u6559\u5e2b\u30c7\u30fc\u30bf\u3078\u306e\u30ce\u30a4\u30ba\u306e\u8ffd\u52a0\u3067\u30c7\u30fc\u30bfaugumentation","b0ebabe7":"# Create DataSet","1e8c052f":"# Submittion","1c99f616":"# Model"}}