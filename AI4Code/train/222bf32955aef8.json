{"cell_type":{"8006ce59":"code","26e4c01d":"code","88c5f08b":"code","01dc7fea":"code","ada4a9ad":"code","6201903a":"code","1ea02b7d":"code","287816bc":"code","bbefe699":"code","8018f8f6":"code","892eacaa":"code","762fe65a":"code","bc242083":"code","b1e45dbb":"code","4a372f07":"code","427ba4f1":"code","5acbec23":"code","b2dbc4d3":"code","e205f0ab":"code","8293feb0":"code","b6be7742":"code","2a8a7eab":"code","342f6132":"code","d9d7d8ef":"code","2788f4c3":"code","791ca59f":"code","a5d6f4c9":"code","7be55284":"code","ee4a3f32":"code","ee776819":"code","039b6793":"code","fea5c62c":"code","bce8ff68":"code","6c8adec9":"code","6574ae9c":"code","d7a13c33":"code","aef235fc":"code","848b4187":"code","a4e278b7":"markdown","41a957aa":"markdown"},"source":{"8006ce59":"%%capture\n!pip install parameter-sherpa","26e4c01d":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport sherpa\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport logging \nlogger = logging.getLogger()\nlogger.setLevel(logging.CRITICAL)\nlogger.info(\"IS_CRITICAL uuuhh\")\n\ntorch.__version__","88c5f08b":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(device)","01dc7fea":"path = \"..\/input\/electric-motor-temperature\/\"\n\ndf = pd.read_csv(path + \"pmsm_temperature_data.csv\")\n\npath = \"..\/working\/\"","ada4a9ad":"!mkdir ..\/working\/models && mkdir ..\/working\/results","6201903a":"df_sep = [df[df.profile_id==profile].drop(['profile_id','torque'], \n                                          axis=1).reset_index(drop=True) for profile in df.profile_id.unique()]\n\ntrain_min = df[~df.profile_id.isin([4, 8, 16, 24, 32, 40, 48, 51, 35, 42])].drop(['profile_id','torque'], \n                                                                                axis=1).min()\ntrain_max = df[~df.profile_id.isin([4, 8, 16, 24, 32, 40, 48, 51, 35, 42])].drop(['profile_id','torque'], \n                                                                                axis=1).max()\n\ndf_train = [(df_sep[i] - train_min)\/(train_max - train_min)\\\n            for i in list(range(0,52)) if i not in [4, 8, 16, 24, 32, \n                                                    40, 48, 51, #val\n                                                    35, 42]] #test\n\ndf_val = [(df_sep[i] - train_min)\/(train_max - train_min)\\\n          for i in [4, 8, 16, 24, 32, 40, 48, 51]]\n\ndf_test = [(df_sep[i] - train_min)\/(train_max - train_min) for i in [35, 42]]","1ea02b7d":"FEATURES = [0, 1, 2, 3, 4, 5, 6]\nTARGET = [7, 8, 9, 10]\n\ndef dataloader(data, length, shuffle=True, out=1):\n    while True:\n    # genera una lista di (i_serie, i_obs)\n        tuples = [[(df_i, i) for i, x in enumerate(data[df_i]) if i >= length + out]\n                  for df_i, _ in enumerate(data)]\n        tuples = sum(tuples, [])  # flattenizza\n        # shuffle\n        if shuffle:\n            np.random.shuffle(tuples)\n\n        # yielda le osservazioni\n        for df_i, i in tuples:\n            X_lagged = data[df_i][(i - length - out):(i - out + 1), FEATURES]\n            y = data[df_i][(i-out):(i), TARGET]\n            yield X_lagged, y\n            \n\ndef new_loss(w1=1, w2=1):\n    \"w1 is the weight for lower temperature and w2 for the upper So w1 <= w2\"\n    def high_low_loss(output, target):\n        weights = w1 + (w2 - w1)*F.relu(target - 0.5)\n        return torch.mean(weights*((output - target)**2))\n    \n    return high_low_loss","287816bc":"def outputSize(in_size, kernel_size, stride, padding):\n    output = int((in_size - kernel_size + 2*(padding)) \/ stride) + 1\n    return(output)\n\nclass Reg_CNN_Net(nn.Module):\n    def __init__(self, features, seq_len, \n                 conv1, conv2, kernel1, kernel2,\n                 h1, h2, out):\n        super(Reg_CNN_Net, self).__init__()\n        #self.h1 = h1\n        self.h2 = h2\n        #self.conv1 = conv1\n        self.conv2 = conv2\n\n        self.features = features\n        self.seq_len = seq_len\n        \n        self.c1 = nn.Conv1d(self.seq_len, conv1, kernel1)\n        \n        h0 = outputSize(self.features, kernel1, 1, 0)*conv1\n        if conv2 != 0:\n            self.c2 = nn.Conv1d(conv1, conv2, kernel2)\n        \n        \n            h0 = outputSize(outputSize(self.features, kernel1, 1, 0), \n                            kernel2, 1 ,0)*conv2\n\n        self.fc1 = nn.Linear(h0, h1)\n        \n\n        if self.h2 != 0:\n            self.fc2 = nn.Linear(h1, h2)\n            \n            self.out = nn.Linear(h2, out)\n        else:\n            self.out = nn.Linear(h1, out)\n\n    def forward(self, x):\n        batch_size, _, _ = x.shape \n        \n        x1 = F.relu(self.c1(x))\n        if self.conv2!=0:\n            x = F.relu(self.c2(x1))\n        else:\n            x = x1\n        \n        x = x.view(batch_size,-1)\n        \n        x = F.relu(self.fc1(x))\n        if self.h2 != 0:\n            x = F.relu(self.fc2(x))\n        return self.out(x)","bbefe699":"parameters = [sherpa.Continuous('lr',[0.001,0.01]),\n              sherpa.Discrete('conv1',[2,50]),\n              sherpa.Discrete('conv2',[0,50]),\n              sherpa.Discrete('kernel1',[2,4]),\n              sherpa.Discrete('kernel2',[2,4]),\n              sherpa.Discrete('h1',[16,128]),\n              sherpa.Discrete('h2',[0,64]),\n              sherpa.Choice('batch_size',[512, 1024, 2048])]\n\nalg = sherpa.algorithms.bayesian_optimization.GPyOpt(max_concurrent=1,\n                                         model_type='GP',\n                                         acquisition_type='EI',\n                                         max_num_trials=100)\n\nstudy = sherpa.Study(parameters=parameters,\n                     algorithm=alg,\n                     lower_is_better=True,\n                     disable_dashboard=True)","8018f8f6":"look_back  = 20\nbest_score = 99999\nfeature    = 7\n\nfor trial in study:\n\n    cnn = Reg_CNN_Net(features=feature, seq_len=look_back+1,\n                     conv1 = int(trial.parameters[\"conv1\"]),\n                     conv2 = int(trial.parameters[\"conv2\"]),\n                     kernel1 = int(trial.parameters[\"kernel1\"]),\n                     kernel2 = int(trial.parameters[\"kernel2\"]),\n                     h1=int(trial.parameters[\"h1\"]),\n                     h2=int(trial.parameters[\"h2\"]), out=4).to(device)\n                     \n    losses = []\n    #just change here and you should get the other loss\n    #criterion = customLoss(1,3)\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(cnn.parameters(), lr=trial.parameters[\"lr\"],\n                                weight_decay=0.001)\n\n    batch_size = int(trial.parameters[\"batch_size\"])\n\n    df_length = np.sum([x.shape[0] for x in df_train])\n\n    gen = dataloader([df.values for df in df_train], look_back)\n    progress_bar = range(0, df_length, batch_size)\n\n    cnn.train()\n    for b in progress_bar:\n        X_train = []\n        y_train = []\n        for i in range(batch_size):\n            try:\n                X,y = next(gen)\n                X_train.append(X)\n                y_train.append(y)\n            except StopIteration:\n                #in case the datagen ends, recreate a new a one and continue\n                #it should not happen though since the the datagen\n                #should be infinite (theorically), it's more of a precaution\n                gen = dataloader([df.values for df in df_train], look_back)\n\n                X,y = next(gen)\n                X_train.append(X)\n                y_train.append(y)\n        \n        inpt = np.array(X_train).reshape(-1, look_back + 1, feature)\n        target = np.array(y_train)    \n        x_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n        y_batch = torch.tensor(target,dtype=torch.float32).to(device)\n        try:\n            output = cnn(x_batch) \n            loss = criterion(output.view(-1), y_batch.view(-1))  \n            \n            loss.backward()\n            optimizer.step()        \n            optimizer.zero_grad()\n\n            losses.append(loss.item())\n        except Exception as e:\n            print(\"This Happened happened\", e)\n            print(inpt.shape)\n            print(x_batch.size())\n            break\n        \n    \n    test_gen = dataloader([df.values for df in df_val], look_back)\n    batch_size = 510\n\n    cnn.eval()\n    y_test = []\n    y_pred_all = []\n\n    \n    tot_len = np.sum([x.shape[0] for x in df_val])\n    for x in range(0, tot_len, batch_size):\n        X_test = []\n        for i in range(batch_size):\n            try:\n                X,y = next(test_gen)\n                X_test.append(X)\n                y_test.append(y)\n            except:\n                print(\"You somehow created an exception hahaha!\")\n                break\n\n        inpt = np.array(X_test).reshape(-1, look_back + 1, feature)\n        x_test_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n        y_pred = cnn(x_test_batch)\n\n        y_pred_all = np.append(y_pred_all,y_pred.cpu().detach().numpy())\n\n    y_test = np.array(y_test).reshape(-1)\n    \n    score = np.mean(criterion(torch.from_numpy(y_pred_all), \n                      torch.from_numpy(y_test)).numpy())\n    #score = np.mean((y_pred_all - y_test)**2)\n    if trial.id % 10 == 0:\n        print(\"Now at Trial\",trial.id)\n    if score < best_score:\n        best_score = score\n        torch.save(cnn, path + \"models\/best_model_cnn_reg_new\")\n        print(\"ID:\", trial.id,\"New Challanger with MSE on val:\", score, \"and confs:\", trial.parameters)\n    \n    #Sherpa PART\n    study.add_observation(trial, iteration=1, objective=score)\n    study.finalize(trial)\n    study.results.to_csv(path + \"results\/AutoML_CNN_reg_new.csv\")","892eacaa":"best_model = torch.load(path + \"models\/best_model_cnn_reg_new\")\nresults = pd.read_csv(path + \"results\/AutoML_CNN_reg_new.csv\")\nresults = results[results[\"Status\"]==\"COMPLETED\"]","762fe65a":"plt.figure(figsize=(15,5))\nbest, = plt.plot(np.minimum.accumulate(np.array(results.Objective)),'*-')\n#actual, = plt.plot(np.array(results.Objective),'*-')\n\n#plt.legend([actual, best],\n#           ['Calculated','Best Seen']) \n\nplt.xlabel('Iterations')\nplt.ylabel('Best Seen MSE')\nplt.show()","bc242083":"#test_n = 1\nlook_back = 20\n\ntest_gen = dataloader([df.values for df in df_test], look_back, shuffle=False)\nsize = np.sum(df.shape[0] for df in df_test)\n\n\nbatch_size = 510\n\ny_test = []\ny_pred_all = []\nfor x in range(0, size, batch_size):\n    X_test = []\n    for i in range(batch_size):\n        try:\n            X,y = next(test_gen)\n            X_test.append(X)\n            y_test.append(y)\n        except:\n            break\n\n    inpt = np.array(X_test).reshape(-1, look_back + 1, 7)\n    x_test_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n    y_pred = best_model(x_test_batch)\n\n    y_pred_all = np.append(y_pred_all,y_pred.cpu().detach().numpy())\n\ny_test = np.array(y_test).reshape(-1,4)\ny_pred_all = y_pred_all.reshape(-1,4)","b1e45dbb":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplt.plot(y_test[:size - look_back - 1,0])\nplt.plot(y_pred_all[:size - look_back - 1,0])\n\nplt.subplot(222)\nplt.plot(y_test[:size - look_back - 1,1])\nplt.plot(y_pred_all[:size - look_back - 1,1])\n\nplt.subplot(223)\nplt.plot(y_test[:size - look_back - 1,2])\nplt.plot(y_pred_all[:size - look_back - 1,2])\n\nplt.subplot(224)\nplt.plot(y_test[:size - look_back - 1,3])\nplt.plot(y_pred_all[:size - look_back - 1,3])\nplt.show()\n\n\nplt.show()","4a372f07":"np.mean((y_test - y_pred_all)**2)","427ba4f1":"best_model","5acbec23":"parameters = [sherpa.Continuous('lr',[0.001,0.01]),\n              sherpa.Discrete('conv1',[2,7]),\n              sherpa.Discrete('conv2',[0,7]),\n              sherpa.Discrete('kernel1',[2,4]),\n              sherpa.Discrete('kernel2',[2,4]),\n              sherpa.Discrete('h1',[16,64]),\n              sherpa.Discrete('h2',[0,64]),\n              sherpa.Choice('batch_size',[512, 1024, 2048])]\n\nalg = sherpa.algorithms.bayesian_optimization.GPyOpt(max_concurrent=1,\n                                         model_type='GP',\n                                         acquisition_type='EI',\n                                         max_num_trials=100)\n\nstudy = sherpa.Study(parameters=parameters,\n                     algorithm=alg,\n                     lower_is_better=True,\n                     disable_dashboard=True)","b2dbc4d3":"look_back  = 20\nbest_score = 99999\nfeature    = 7\n\n\ndef customLoss(w1=1, w2=1):\n    \"w1 is the weight for lower temperature and w2 for the upper So w1 <= w2\"\n    def high_low_loss(output, target):\n        weights = w1 + (w2 - w1) * F.relu(target - 0.5)\n        return torch.mean(weights * ((output - target)**2))\n\n    return high_low_loss\n\nfor trial in study:\n\n    cnn = Reg_CNN_Net(features=feature, seq_len=look_back+1,\n                     conv1 = int(trial.parameters[\"conv1\"]),\n                     conv2 = int(trial.parameters[\"conv2\"]),\n                     kernel1 = int(trial.parameters[\"kernel1\"]),\n                     kernel2 = int(trial.parameters[\"kernel2\"]),\n                     h1=int(trial.parameters[\"h1\"]),\n                     h2=int(trial.parameters[\"h2\"]), out=4).to(device)\n                     \n    losses = []\n    #just change here and you should get the other loss\n    criterion = customLoss(1,3)\n    #criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(cnn.parameters(), lr=trial.parameters[\"lr\"],\n                                weight_decay=0.001)\n\n    batch_size = int(trial.parameters[\"batch_size\"])\n\n    df_length = np.sum([x.shape[0] for x in df_train])\n\n    gen = dataloader([df.values for df in df_train], look_back)\n    progress_bar = range(0, df_length, batch_size)\n\n    cnn.train()\n    for b in progress_bar:\n        X_train = []\n        y_train = []\n        for i in range(batch_size):\n            try:\n                X,y = next(gen)\n                X_train.append(X)\n                y_train.append(y)\n            except StopIteration:\n                #in case the datagen ends, recreate a new a one and continue\n                #it should not happen though since the the datagen\n                #should be infinite (theorically), it's more of a precaution\n                gen = dataloader([df.values for df in df_train], look_back)\n\n                X,y = next(gen)\n                X_train.append(X)\n                y_train.append(y)\n        \n        inpt = np.array(X_train).reshape(-1, look_back + 1, feature)\n        target = np.array(y_train)    \n        x_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n        y_batch = torch.tensor(target,dtype=torch.float32).to(device)\n        try:\n            output = cnn(x_batch) \n            loss = criterion(output.view(-1), y_batch.view(-1))  \n            \n            loss.backward()\n            optimizer.step()        \n            optimizer.zero_grad()\n\n            losses.append(loss.item())\n        except Exception as e:\n            print(\"This Happened happened\", e)\n            print(inpt.shape)\n            print(x_batch.size())\n            break\n        \n    \n    test_gen = dataloader([df.values for df in df_val], look_back)\n    batch_size = 510\n\n    cnn.eval()\n    y_test = []\n    y_pred_all = []\n\n    \n    tot_len = np.sum([x.shape[0] for x in df_val])\n    for x in range(0, tot_len, batch_size):\n        X_test = []\n        for i in range(batch_size):\n            try:\n                X,y = next(test_gen)\n                X_test.append(X)\n                y_test.append(y)\n            except:\n                print(\"You somehow created an exception hahaha!\")\n                break\n\n        inpt = np.array(X_test).reshape(-1, look_back + 1, feature)\n        x_test_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n        y_pred = cnn(x_test_batch)\n\n        y_pred_all = np.append(y_pred_all,y_pred.cpu().detach().numpy())\n\n    y_test = np.array(y_test).reshape(-1)\n    \n    score = np.mean(criterion(torch.from_numpy(y_pred_all), \n                      torch.from_numpy(y_test)).numpy())\n    #score = np.mean((y_pred_all - y_test)**2)\n    if trial.id % 10 == 0:\n        print(\"Now at Trial\",trial.id)\n    if score < best_score:\n        best_score = score\n        torch.save(cnn, path + \"models\/best_model_cnn_reg_new_c_loss\")\n        print(\"ID:\", trial.id,\"New Challanger with MSE on val:\", score, \"and confs:\", trial.parameters)\n    \n    #Sherpa PART\n    study.add_observation(trial, iteration=1, objective=score)\n    study.finalize(trial)\n    study.results.to_csv(path + \"results\/AutoML_CNN_reg_new_c_loss.csv\")","e205f0ab":"best_model = torch.load(path + \"models\/best_model_cnn_reg_new_c_loss\")\nresults = pd.read_csv(path + \"results\/AutoML_CNN_reg_new_c_loss.csv\")\nresults = results[results[\"Status\"]==\"COMPLETED\"]","8293feb0":"plt.figure(figsize=(15,5))\nbest, = plt.plot(np.minimum.accumulate(np.array(results.Objective)),'*-')\n#actual, = plt.plot(np.array(results.Objective),'*-')\n\n#plt.legend([actual, best],\n#           ['Calculated','Best Seen']) \n\nplt.xlabel('Iterations')\nplt.ylabel('Best Seen MSE')\nplt.show()","b6be7742":"#test_n = 1\nlook_back = 20\n\ntest_gen = dataloader([df.values for df in df_test], look_back, shuffle=False)\nsize = np.sum(df.shape[0] for df in df_test)\n\nbatch_size = 510\n\ny_test = []\ny_pred_all = []\nfor x in range(0, size, batch_size):\n    X_test = []\n    for i in range(batch_size):\n        try:\n            X,y = next(test_gen)\n            X_test.append(X)\n            y_test.append(y)\n        except:\n            break\n\n    inpt = np.array(X_test).reshape(-1, look_back + 1, 7)\n    x_test_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n    y_pred = best_model(x_test_batch)\n\n    y_pred_all = np.append(y_pred_all,y_pred.cpu().detach().numpy())\n\ny_test = np.array(y_test).reshape(-1,4)\ny_pred_all = y_pred_all.reshape(-1,4)","2a8a7eab":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplt.plot(y_test[:size - look_back - 1,0])\nplt.plot(y_pred_all[:size - look_back - 1,0])\nplt.title(\"pm\")\n\nplt.subplot(222)\nplt.plot(y_test[:size - look_back - 1,1])\nplt.plot(y_pred_all[:size - look_back - 1,1])\nplt.title(\"stator_yoke\")\n\nplt.subplot(223)\nplt.plot(y_test[:size - look_back - 1,2])\nplt.plot(y_pred_all[:size - look_back - 1,2])\nplt.title(\"stator_tooth\")\n\nplt.subplot(224)\nplt.plot(y_test[:size - look_back - 1,3])\nplt.plot(y_pred_all[:size - look_back - 1,3])\nplt.title(\"stator_winding\")\n\nplt.savefig('imgs\/pred_second_custom_loss.png',bbox_inches='tight', pad_inches=0)\n\nplt.show()","342f6132":"np.mean((y_pred_all - y_test)**2)","d9d7d8ef":"best_model","2788f4c3":"class LSTM_Net(nn.Module):\n    def __init__(self, features, hidden, h1, h2, out):\n        super(LSTM_Net, self).__init__()\n        self.h1 = h1\n        self.h2 = h2\n\n        self.hidden_size = hidden\n        self.features = features\n\n        self.lstm = nn.LSTM(self.features, self.hidden_size, 1, #feature_size, hidden_size, num_layer\n                            batch_first = True) \n        self.fc1 = nn.Linear(self.hidden_size, \n                                h1)\n        self.out = nn.Linear(h1, out)\n\n        if self.h2 != 0:\n            self.fc2 = nn.Linear(h1, h2)\n            self.out = nn.Linear(h2, out)\n        \n        #we do it stateless so there is no need for the hidden_state\n        #self.hidden = None #torch.randn(1, ??, self.hidden) #num_layer, batch, hidden_size\n\n    def forward(self, x):\n        batch_size, _, _ = x.shape \n        \n        x, _ =  self.lstm(x)\n        x = F.tanh(x[:,-1].view(batch_size, -1))\n        \n        x = F.relu(self.fc1(x))\n        if self.h2 != 0:\n            x = F.relu(self.fc2(x))\n        return self.out(x)","791ca59f":"parameters = [sherpa.Discrete('lstm_hidden',[40,140]),\n              sherpa.Discrete('hidden_unit1',[16,64]),\n              sherpa.Discrete('hidden_unit2',[0,64]),\n              sherpa.Continuous('lr',[0.0005,0.005]),\n              sherpa.Choice('batch_size',[512, 1024, 2048])]\n\nalg = sherpa.algorithms.bayesian_optimization.GPyOpt(max_concurrent=1,\n                                         model_type='GP',\n                                         acquisition_type='EI',\n                                         max_num_trials=100)\n\nstudy = sherpa.Study(parameters=parameters,\n                     algorithm=alg,\n                     lower_is_better=True,\n                     disable_dashboard=True)","a5d6f4c9":"look_back = 60\nbest_score = 99999\n\nfor trial in study:\n    lstm = LSTM_Net(features=7, hidden=int(trial.parameters[\"lstm_hidden\"]), \n                  h1=int(trial.parameters[\"hidden_unit1\"]), \n                  h2=int(trial.parameters[\"hidden_unit2\"]),out=4).to(device)\n\n    losses = []\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(lstm.parameters(), lr=trial.parameters[\"lr\"])\n\n    batch_size = int(trial.parameters[\"batch_size\"])\n\n    df_length = np.sum([x.shape[0] for x in df_train])\n\n    gen = dataloader([df.values for df in df_train], look_back)\n    progress_bar = range(0, df_length, batch_size)\n\n    lstm.train()\n    for b in progress_bar:\n        X_train = []\n        y_train = []\n        for i in range(batch_size):\n            try:\n                X,y = next(gen)\n                X_train.append(X)\n                y_train.append(y)\n            except StopIteration:\n                #in case the datagen ends, recreate a new a one and continue\n                #it should not happen though since the the datagen\n                #should be infinite (theorically), it's more of a precaution\n                gen = dataloader([df.values for df in df_train], look_back)\n\n                X,y = next(gen)\n                X_train.append(X)\n                y_train.append(y)\n\n        inpt = np.array(X_train).reshape(-1, look_back + 1, 7)\n        target = np.array(y_train)    \n        x_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n        y_batch = torch.tensor(target,dtype=torch.float32).to(device)\n        try:\n            output = lstm(x_batch) \n            loss = criterion(output.view(-1), y_batch.view(-1))  \n\n            loss.backward()\n            optimizer.step()        \n            optimizer.zero_grad()\n\n            losses.append(loss.item())\n        except:\n            print(\"something strange happened\")\n            print(inpt.shape)\n            print(x_batch.size())\n            break\n        \n        \n\n    test_gen = dataloader([df.values for df in df_val], look_back)\n    batch_size = 510\n\n    lstm.eval()\n    y_test = []\n    y_pred_all = []\n\n    \n    tot_len = np.sum([x.shape[0] for x in df_val])\n    for x in range(0, tot_len, batch_size):\n        X_test = []\n        for i in range(batch_size):\n            try:\n                X,y = next(test_gen)\n                X_test.append(X)\n                y_test.append(y)\n            except:\n                print(\"You somehow created an exception hahaha!\")\n                break\n\n        inpt = np.array(X_test).reshape(-1, look_back + 1, 7)\n        x_test_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n        y_pred = lstm(x_test_batch)\n\n        y_pred_all = np.append(y_pred_all,y_pred.cpu().detach().numpy())\n\n    y_test = np.array(y_test).reshape(-1)\n    score = np.mean((y_test - y_pred_all)**2) #MSE\n    if score < best_score:\n        best_score = score\n        torch.save(lstm, path + \"models\/best_model_lstm_second_task\")\n        print(\"ID:\", trial.id, \"New Challanger with MSE on val:\", score, \"and confs:\", trial.parameters)\n    if trial.id % 10 ==0:\n        print(\"I am at\",trial.id)\n    \n    #Sherpa PART\n    study.add_observation(trial, iteration=1, objective=score)\n    study.finalize(trial)\n    study.results.to_csv(path + \"results\/AutoML_LSTM_second_task.csv\")","7be55284":"best_model = torch.load(path + \"models\/best_model_lstm_second_task\")\nresults = pd.read_csv(path + \"results\/AutoML_LSTM_second_task.csv\")\nresults = results[results[\"Status\"]==\"COMPLETED\"]\n\nplt.figure(figsize=(15,5))\nbest, = plt.plot(np.minimum.accumulate(np.array(results.Objective)),'*-')\n\nplt.xlabel('Iterations')\nplt.ylabel('Best Seen MSE')\nplt.show()","ee4a3f32":"#test_n = 1\nlook_back = 20\n\ntest_gen = dataloader([df.values for df in df_test], look_back, shuffle=False)\nsize = np.sum(df.shape[0] for df in df_test)\n\nbatch_size = 510\n\ny_test = []\ny_pred_all = []\nfor x in range(0, size, batch_size):\n    X_test = []\n    for i in range(batch_size):\n        try:\n            X,y = next(test_gen)\n            X_test.append(X)\n            y_test.append(y)\n        except:\n            break\n\n    inpt = np.array(X_test).reshape(-1, look_back + 1, 7)\n    x_test_batch = torch.tensor(inpt,dtype=torch.float32).to(device)  \n    y_pred = best_model(x_test_batch)\n\n    y_pred_all = np.append(y_pred_all,y_pred.cpu().detach().numpy())\n\ny_test = np.array(y_test).reshape(-1,4)\ny_pred_all = y_pred_all.reshape(-1,4)","ee776819":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplt.plot(y_test[:size - look_back - 1,0])\nplt.plot(y_pred_all[:size - look_back - 1,0])\nplt.title('pm')\n\nplt.subplot(222)\nplt.plot(y_test[:size - look_back - 1,1])\nplt.plot(y_pred_all[:size - look_back - 1,1])\nplt.title('stator_yoke')\n\nplt.subplot(223)\nplt.plot(y_test[:size - look_back - 1,2])\nplt.plot(y_pred_all[:size - look_back - 1,2])\nplt.title('stator_tooth')\n\nplt.subplot(224)\nplt.plot(y_test[:size - look_back - 1,3])\nplt.plot(y_pred_all[:size - look_back - 1,3])\nplt.title('stator_winding')\n\nplt.savefig('imgs\/lstm_pred_second.png',bbox_inches='tight', pad_inches=0)\nplt.show()\n","039b6793":"best_model","fea5c62c":"from sklearn.metrics import r2_score","bce8ff68":"r2_score(y_test, y_pred_all)","6c8adec9":"np.mean(np.abs(y_test - y_pred_all)\/y_test)","6574ae9c":"r2_score(y_test[:,1:4], y_pred_all[:,1:4])","d7a13c33":"np.mean(np.abs(y_test[:,1:4] - y_pred_all[:,1:4])\/y_test[:,1:4])","aef235fc":"r2_score(y_test[:,0], y_pred_all[:,0])","848b4187":"np.mean(np.abs(y_test[:,0] - y_pred_all[:,0])\/y_test[:,0])","a4e278b7":"# CNN with custom loss","41a957aa":"# LSTM\n "}}