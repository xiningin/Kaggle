{"cell_type":{"97f2a8aa":"code","999cbd91":"code","66019cf6":"code","4096fa66":"code","16f3f58f":"code","51eec013":"code","713fc5a2":"code","8914813b":"code","ceb1b91a":"code","46a8ee8c":"code","ba5a12fa":"code","4d82fb7a":"code","22e82191":"code","2d705ea2":"code","98c6ee54":"code","a75fb9dd":"code","fff7ba63":"code","b29abfa0":"code","489dcb58":"code","c58e2796":"code","6f53d1c5":"code","5e8fad06":"code","421dc679":"code","97d228c2":"code","25a9a354":"code","0186ebac":"code","cf1ff27a":"code","282812ff":"code","7e7785b2":"code","44ec3918":"code","8a0b6b0b":"code","08a1c786":"code","10df033d":"code","ff1d7ee6":"code","d5946df9":"code","c5fbf7fc":"code","1d2b2dc1":"code","e696d89d":"code","e15eeac9":"code","99fa4b2e":"code","d4afc148":"code","7ecf3464":"code","c741c043":"code","96b1894b":"code","7a0078f2":"markdown","1b11c4e9":"markdown","2a1a8cd3":"markdown","472939c7":"markdown","9e6730bf":"markdown","6f4d9d06":"markdown","7af47a1c":"markdown","7f8d43dd":"markdown","87e790e0":"markdown","5be56f00":"markdown","36c5a156":"markdown","50377540":"markdown","2b866c53":"markdown","8a855674":"markdown","6b591007":"markdown","04a46e46":"markdown","5a476ecc":"markdown","ef85e77a":"markdown","09d07fc2":"markdown","f368ac59":"markdown","a82a3cec":"markdown","4f2fae43":"markdown","e070d53f":"markdown","26ca8e5e":"markdown","b03b8b8f":"markdown","ab67a04e":"markdown","64f34c8b":"markdown","8995c19d":"markdown","1a342203":"markdown","532cdc6e":"markdown","209226c1":"markdown","800b3fc5":"markdown","36674868":"markdown","5568d6d8":"markdown","4388b764":"markdown","20fb7ce5":"markdown","0c8cc488":"markdown","115878b8":"markdown","4e2454e7":"markdown","471d7605":"markdown","a6edaad2":"markdown","1cac479c":"markdown"},"source":{"97f2a8aa":"import numpy as np \nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,Ridge, RidgeCV\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport seaborn as sns\nfrom mpl_toolkits.basemap import Basemap \nplt.style.use(\"seaborn-colorblind\")\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nlistings = pd.read_csv('..\/input\/boston\/listings.csv')\npd.set_option('display.max_columns', 10)\nprint (listings.shape)\nlistings.head()","999cbd91":"# Drop some columns that are not related to price\ndropcol = ['id', 'host_id', 'listing_url','scrape_id','jurisdiction_names','license','thumbnail_url',\n           'medium_url','picture_url','xl_picture_url','host_thumbnail_url',\n           'host_picture_url']\nlistings.drop(dropcol, axis=1, inplace=True)\nlistings.shape","66019cf6":"# find the columns with price involved and observe the data\nprice_cols_index = listings.columns[listings.columns.str\n                                    .contains('cleaning_fee|deposit|price|extra_people') == True]\nprice_cols = listings[price_cols_index]\nprint ('price_cols_index:')\nprint (price_cols_index)\nprice_cols.head()","4096fa66":"# romove $ in the price value and change the data type to float\nfor variable in price_cols_index:\n    listings[variable] = listings[variable].map(lambda x: x.replace(\"$\",'').replace(\",\",''),\n                                              na_action = 'ignore')\n    listings[variable] = listings[variable].astype(float)\nlistings[price_cols_index].head()","16f3f58f":"# remove % in the rating value and change the data type to float\nrate_cols_index = listings.columns[listings.columns.str.contains('rate') == True]\nrate_cols = listings[rate_cols_index]\nprint ('rate_cols_index:')\nprint (rate_cols_index)\nrate_cols.head()","51eec013":"# romove % in the rate value and change the data type to float\nfor variable in rate_cols_index:\n    listings[variable] = listings[variable].map(lambda x: x.replace(\"%\",''),\n                                              na_action = 'ignore')\n    listings[variable] = listings[variable].astype(float)\nlistings[rate_cols_index ].head()","713fc5a2":"# Drop rows with missing price values\nlistings_dropna = listings.dropna(subset=['price'], axis=0)\n\n# find the columns that have more than 50% data as nan\nnan_cols = listings.columns[listings.isnull().mean() > 0.50]\nprint ('nan_cols')\nprint (nan_cols)\n\n# drop the columns with more than 50% data missing\nlistings_dropna = listings.drop(nan_cols, axis=1)\nlistings_dropna.shape","8914813b":"listings_dropna.shape","ceb1b91a":"# histogram plot to see the price distribution\nsns.distplot(listings_dropna['price'], color='forestgreen',\n             kde_kws={'color': 'indianred', 'lw': 2, 'label': 'KDE'})\nplt.title('Price Distribution', fontsize=14)\nplt.ylabel('Probablity', fontsize=12)\nplt.xlabel('Price (USD)', fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(['KDE'], prop={\"size\":12})\nplt.show()\n# price description\nlistings_dropna.describe().price","46a8ee8c":"# retain the data where the price range is in $20 to $500\nlistings_new = listings_dropna[(listings_dropna['price'] >20) & \n                                  (listings_dropna['price'] <500)]\nlistings_new.shape","ba5a12fa":"# histogram plot to see the price distribution in the selcted range < $500\nsns.distplot(listings_new['price'], bins=20, color='forestgreen',\n             kde_kws={'color': 'indianred', 'lw': 2, 'label': 'KDE'})\nplt.title('Price Distribution', fontsize=14)\nplt.ylabel('Probablity', fontsize=12)\nplt.xlabel('Price (USD)', fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(['KDE'], prop={\"size\":12})\nplt.show()\n# price description\nlistings_new.describe().price","4d82fb7a":"conda install -c conda-forge basemap-data-hires","22e82191":"lat_max = listings_new.latitude.max()\nlat_min = listings_new.latitude.min()\nlon_max = listings_new.longitude.max()\nlon_min = listings_new.longitude.min()\nedge = 0.006\n\nfig = plt.figure(figsize=(17,16))\nm = Basemap(projection='merc', llcrnrlat=lat_min-edge, urcrnrlat=lat_max+edge, llcrnrlon=lon_min-edge, urcrnrlon=lon_max+edge)\n# m.drawmapboundary(fill_color='aqua')\nm.drawmapboundary(fill_color=[0.95, 0.95, 0.95, 1  ])\nm.drawcounties(linewidth=0.1, linestyle='-', color='k', antialiased=1, facecolor='none', ax=None, zorder=None, drawbounds=False)\n\nnum_colors =16\nvalues = listings_new.price\nprice_min = values.min()\nprice_max = values.max()\n\ncm = plt.get_cmap('coolwarm',num_colors)\nbins = np.linspace(values.min(), values.max(), num_colors)\n\ncolor = cm (np.digitize(values, bins))\n\n\nx,y = m(listings_new.longitude.values, listings_new.latitude.values)\nscat = m.scatter(x,y, s = listings_new.price, color = color, alpha=0.7)\n# Draw color legend.\n                        #[left, top, width, height]\nax_legend = fig.add_axes([0.21, 0.12, 0.6, 0.02])\ncb = mpl.colorbar.ColorbarBase(ax_legend, cmap=cm, ticks=bins, boundaries=bins, orientation='horizontal')\ncb.ax.set_xticklabels([str(round(i, 1)) for i in bins])\nplt.box(False)\ncb.ax.set_frame_on(False)\nplt.show()","2d705ea2":"# retain the data where the price range is in $20 to $500\nlistings_new = listings_dropna[(listings_dropna['price'] >20) & \n                                  (listings_dropna['price'] <500)]\nprint (listings_new.shape)\n\n# amenities are one important variable that affect the price, observe the first data \namenities_col = listings_new['amenities']\nprint ('amenities_col[0]:')\nprint (amenities_col[0])\n\nn_amenities = listings_new['amenities'].apply(lambda x: len(x.split(','))).tolist()\nlistings_new = listings_new.assign(n_amenities = n_amenities).drop(['amenities'], axis=1)\nlistings_new['n_amenities'].head()","98c6ee54":"# extract numerical variables, drop variables that are not affect the price, \n# fill the nan with mean, and check nan in the end\nnum_cols = listings_new[listings_new.select_dtypes(include=['float', 'int']).columns]\nnum_cols = num_cols.drop(['latitude', 'longitude'], axis=1)\nnum_cols = num_cols.apply(lambda col: col.fillna(col.mean()), axis=0)\n# num_cols.isnull().mean()","a75fb9dd":"# heat map of corr numerical variables \nnum_cols = num_cols.drop(['price'], axis=1)\nnum_cols = pd.concat([num_cols, listings_new['price']], axis=1)\ncorr = num_cols.corr().sort_values('price', axis=1, ascending=False)\ncorr = corr.sort_values('price', axis=0, ascending=True)\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, k=1)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 11))\n    ax = sns.heatmap(corr, mask=mask, vmin=corr.price.min(), \n                     vmax=corr.drop(['price'], axis=0).price.max(), center=0, square=True, \n                     annot=True, fmt='.2f',cmap='RdBu',annot_kws={\"size\": 8})","fff7ba63":"# this is a function that will conduct a box plot for the categorical variables of interest\ndef categorical_variable_analysis(data, variable, response):\n    '''\n    INPUT\n    data - pandas dataframe, holding all the columns \n    variable - str, the categorical variable of interest\n    response - str, the response variable\n    \n    OUTPUT\n    None\n    '''\n    order = listings_new.groupby(variable)[response].median().sort_values(ascending=False).index\n    sns.boxplot(y=listings_new[response], x=listings_new[variable], order=order)\n    ax = plt.gca()\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=10)\n    plt.title('{}'.format(variable), fontsize=14)\n    plt.ylabel('Price', fontsize=14)\n    plt.xlabel('', fontsize=12)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    ","b29abfa0":"# categorical variables of interest that may affect price\ncat_cols_index = ['neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', \n                  'cancellation_policy', 'host_is_superhost', 'instant_bookable', \n                  'is_location_exact', 'require_guest_phone_verification',\n                  'require_guest_profile_picture']\n\nplt.figure(figsize = (12, 27))\nfor i in range(len(cat_cols_index)):\n    cat_col_index = cat_cols_index[i]\n    plt_index = i+1\n    plt.subplot(5,2,plt_index)\n    categorical_variable_analysis(data = listings_new, variable=cat_col_index, response='price') \n    plt.subplots_adjust(hspace=0.75, wspace=0.2)\n    \n    if cat_col_index in ['host_is_superhost', 'instant_bookable', 'is_location_exact', \n                         'require_guest_phone_verification', 'require_guest_profile_picture']:\n        locs, labels = plt.xticks()\n        labels = [item.get_text() for item in labels]\n#         print (labels)\n        if labels ==  ['f', 't']:\n            plt.xticks(np.arange(2), ('No', 'Yes'))\n        else:\n            plt.xticks(np.arange(2), ('Yes', 'No'))\n    ","489dcb58":"# function that selects variables according to the corr with price\ndef select_variables(data, response, num_variables, droped_variables):\n    '''\n    INPUT\n    data - pandas dataframe, holding all the variables interested\n    response - str, column name of the response\n    num_variables - int, number of varibles used for modeling later\n    droped_variables - list of strs, the variebles need to be dropped\n    \n\n    OUTPUT\n    variables - pandas dataframe, output of the variables of interest based on corr with response\n    '''\n    corr_num = data.corr()[response].abs().sort_values(ascending=False).drop(\n        droped_variables)[:num_variables]\n    variables = data[corr_num.index]\n    \n    return variables","c58e2796":"# select top 15 numerical variables based on the absolute corr\n# and drop the mutually high correlated variables\nresponse = 'price'\nNo_num_variables = 15\nhigh_num_correlated_variables  = ['price','host_total_listings_count',\n                            'calculated_host_listings_count','availability_60',\n                            'availability_90']\nnum_cols_selected = select_variables(num_cols, response, No_num_variables, \n                                    high_num_correlated_variables)\nnum_cols_new = pd.concat([num_cols['price'], num_cols_selected], axis=1)\n\nprint ('top 15 corr numerical variables')\nnum_cols_selected.columns","6f53d1c5":"# heatmap of corr between the selected numerical variables and the responce price\nnum_corr = num_cols_new.corr().sort_values('price', axis=1, ascending=False)\nnum_corr = num_corr.sort_values('price', axis=0, ascending=True)\nmask = np.zeros_like(num_corr)\nmask[np.triu_indices_from(mask, k=1)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 11))\n    sns.set(font_scale=1.4)\n    ax = sns.heatmap(num_corr, mask=mask, vmin=num_corr.price.min(), \n                     vmax=num_corr.drop(['price'], axis=0).price.max(), center=0, square=True, \n                     annot=True, fmt='.2f',cmap='RdBu',annot_kws={\"size\": 12})","5e8fad06":"print ('The selected categorical varibles are:')\ncat_cols_index","421dc679":"# extract the categorical columns \ncat_cols = listings_new[cat_cols_index]\n\n# get dummy variables\nfor col in cat_cols_index:\n    cat_cols = pd.concat([cat_cols.drop(col, axis=1), \n                          pd.get_dummies(cat_cols[col], prefix=col, prefix_sep='_', \n                                                       drop_first=True)], axis=1)\nprint ('cat_cols:')\nprint (cat_cols.columns)\ncat_cols = pd.concat([listings_new['price'], cat_cols], axis=1)\ncat_cols.shape","97d228c2":"# select all the categorical variables for now\nNo_cat_variables = 20\ncat_cols_selected = select_variables(cat_cols, response, No_cat_variables, response)\ncat_cols_new = pd.concat([cat_cols['price'], cat_cols_selected], axis=1)\ncat_cols_selected.shape","25a9a354":"# heatmap of corr between the selected numerical variables and the responce price\ncat_corr = cat_cols_new.corr().sort_values('price', axis=1, ascending=False)\ncat_corr = cat_corr.sort_values('price', axis=0, ascending=True)\nmask = np.zeros_like(cat_corr)\nmask[np.triu_indices_from(mask, k=1)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 11))\n    sns.set(font_scale=1.4)\n    ax = sns.heatmap(cat_corr, mask=mask, vmin=num_corr.price.min(), \n                     vmax=num_corr.drop(['price'], axis=0).price.max(), center=0, square=True, \n                     annot=True, fmt='.2f',cmap='RdBu',annot_kws={\"size\": 12})","0186ebac":"# variables\nx = pd.concat([cat_cols_selected, num_cols_selected], axis=1)\n# response\ny = num_cols.price\n\ntest_size = 0.3\nrandom_state = 42           \n# split the data into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size, \n                                                    random_state = random_state)","cf1ff27a":"# LinearRegression model is used\nlm_model = LinearRegression()\nlm_model.fit(x_train, y_train)\ny_test_preds = lm_model.predict(x_test)\ny_train_preds = lm_model.predict(x_train)\n\n#r2 value\nr2_scores_test = r2_score(y_test, y_test_preds)\nr2_scores_train = r2_score(y_train, y_train_preds)\n\nprint ('r2_score_test:')\nprint (r2_scores_test)\nprint ('r2_score_train:')\nprint (r2_scores_train)\n\nfig = plt.figure(figsize =(10, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nax = plt.axes(aspect = 'equal')\nplt.subplot(121)\nplt.title('Scatter comparison', fontsize=14)\nplt.scatter(y_test, y_test_preds, color='blue')\nplt.xlabel('Real Price', fontsize=14)\nplt.ylabel('Predicted Price', fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n\nplt.subplot(122)\nsns.distplot(y_test_preds, hist=False,\n             kde_kws={'color': 'b', 'lw': 2, 'label': 'Predicted price'})\nsns.distplot(y_test, hist=False,\n             kde_kws={'color': 'r', 'lw': 2, 'label': 'Real price'})\nplt.title('Distribution comparison', fontsize=14)\nplt.ylabel('Probablity', fontsize=12)\nplt.xlabel('Price (USD)', fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(['Predicted price', 'Real price'], prop={\"size\":12})\nplt.show()","282812ff":"# this function is used to find the best linear regression model\ndef find_optimal_LR(No_num_variables, No_cat_variables, num_cols, cat_cols, response,\n                    high_num_correlated_variables, test_size , random_state):\n    '''\n    INPUT\n    No_num_variables - list of ints, number of numerical vars\n    No_cat_variables - list of ints, number of categorical vars\n    num_cols - pandas dataframe, numerical variavles\n    cat_cols - pandas dataframe, categorical variavles\n    response - str, column name of the response\n    test_size - float between 0 and 1, default 0.3, determines the proportion of data as test data\n    random_state - int, default 42, controls random state for train_test_split\n    \n\n    OUTPUT\n    results - dictionary of r2 scores for different combination of number of numerical and categorical variables\n    r2_scores_test - list of floats of r2 scores on the test data\n    r2_scores_train - list of floats of r2 scores on the train data\n    best_r2_score_test - float, best r2 score on the test data\n    best_num_variables - int, number of numerical variables for best r2 score \n    best_cat_variables - int, number of categorical variables for best r2 score \n    best_lm_model - optimal model object from sklearn\n    best_x_train, best_x_test, best_y_train, best_y_test - output from sklearn train test split used for optimal model\n    '''\n    \n    best_r2_score_test, best_num_variables, best_cat_variables, best_lm_model = 0, 0, 0, []\n    best_x_train, best_x_test, best_y_train, best_y_test = [], [], [], []\n    r2_scores_test, r2_scores_train, results = [], [], dict()\n    for No_num in No_num_variables:\n        for No_cat in No_cat_variables:\n            num_cols_selected = select_variables(num_cols, response, No_num, \n                                    high_num_correlated_variables)\n            cat_cols_selected = select_variables(cat_cols, response, No_cat, response)\n            x = pd.concat([cat_cols_selected, num_cols_selected], axis=1)\n            y = num_cols.price\n            \n            # split the data into train and test\n            x_train, x_test, y_train, y_test = \\\n            train_test_split(x, y, test_size = test_size, random_state = random_state)\n            \n            # fit the model and obtain pred response\n            lm_model = LinearRegression()\n            lm_model.fit(x_train, y_train)\n            y_test_preds = lm_model.predict(x_test)\n            y_train_preds = lm_model.predict(x_train)\n            \n            # record the best model\n            r2_score_test = r2_score(y_test, y_test_preds)\n            if r2_score_test > best_r2_score_test:\n                best_r2_score_test = r2_score_test\n                best_num_variables = No_num\n                best_cat_variables = No_cat\n                best_lm_model = lm_model\n                best_x_train, best_x_test, best_y_train, best_y_test \\\n                = x_train, x_test, y_train, y_test\n                \n            # append the r2 value from the test set\n            r2_scores_test.append(r2_score_test)\n            r2_scores_train.append(r2_score(y_train, y_train_preds))\n            variables = str(No_num) + ' num_variables,' + str(No_cat) + ' cat_variables'\n            results[variables] = r2_score(y_test, y_test_preds)\n\n    return results, r2_scores_test, r2_scores_train, best_r2_score_test, best_num_variables, \\\n        best_cat_variables, best_lm_model, best_x_train, best_x_test, best_y_train, best_y_test\n","7e7785b2":"response = 'price'\nNo_num_variables = list(range(1, 20))\nNo_cat_variables = [10, 15, 20, 25, 30, 35, 40, 45, 51]\ntest_size = 0.3\nrandom_state=42\n# find the best combination of selection of numerical and categorical variables,  \n# and return the corresponding x_train, x_test, y_train, y_test\nresults, r2_scores_test, r2_scores_train, best_r2_score_test, best_num_variables, \\\n    best_cat_variables, best_lm_model, x_train, x_test, y_train, y_test \\\n    = find_optimal_LR(No_num_variables, No_cat_variables, num_cols, cat_cols, response, \n                      high_num_correlated_variables, test_size , random_state)\nprint ('best_r2_score_test:')\nprint (best_r2_score_test)\nprint ('best_num_variables:')\nprint (best_num_variables)\nprint ('best_cat_variables:')\nprint (best_cat_variables)","44ec3918":"lm_model = LinearRegression()\nlm_model.fit(x_train, y_train)\ny_test_preds = lm_model.predict(x_test)\ny_train_preds = lm_model.predict(x_train)\n\n#r2 value\nr2_scores_test = r2_score(y_test, y_test_preds)\nr2_scores_train = r2_score(y_train, y_train_preds)\n\nprint ('r2_score_test:')\nprint (r2_scores_test)\nprint ('r2_score_train:')\nprint (r2_scores_train)\n\nfig = plt.figure(figsize =(10, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nax = plt.axes(aspect = 'equal')\nplt.subplot(121)\nplt.title('Scatter comparison', fontsize=14)\nplt.scatter(y_test, y_test_preds, color='blue')\nplt.xlabel('Real Price', fontsize=14)\nplt.ylabel('Predicted Price', fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n\nplt.subplot(122)\nsns.distplot(y_test_preds, hist=False,\n             kde_kws={'color': 'b', 'lw': 2, 'label': 'Predicted price'})\nsns.distplot(y_test, hist=False,\n             kde_kws={'color': 'r', 'lw': 2, 'label': 'Real price'})\nplt.title('Distribution comparison', fontsize=14)\nplt.ylabel('Probablity', fontsize=12)\nplt.xlabel('Price (USD)', fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(['Predicted price', 'Real price'], prop={\"size\":12})\nplt.show()","8a0b6b0b":"alphas = np.linspace(0.0001,0.1,200)\nfrom sklearn.metrics import mean_squared_error\nridge_cofficients = []\nr2_scores_test = []\nr2_scores_train = []\nfor alpha in alphas:\n    ridge = Ridge(alpha = alpha, normalize=True)\n    ridge.fit(x_train, y_train)\n    y_test_preds = ridge.predict(x_test)\n    r2_score_test = r2_score(y_test, y_test_preds)\n    r2_scores_test.append(r2_score_test)\n    y_train_preds = ridge.predict(x_train)\n    r2_score_train = r2_score(y_train, y_train_preds)\n    r2_scores_train.append(r2_score_train)\n\nfig = plt.figure(1)\nfig.set_facecolor('white') \nplt.plot(alphas, r2_scores_test, 'b', label = 'test')\nplt.plot(alphas, r2_scores_train, 'r', label = 'train')\nplt.xlabel('alpha')\nplt.ylabel('R2')\nplt.axis('tight')\nplt.title('Ridge trace')\nplt.legend()\nplt.show()  ","08a1c786":"# coss-validation test\nridge_cv = RidgeCV(alphas = alphas, normalize=True, cv = 5)\n# , scoring=\"neg_mean_squared_error\")\nridge_cv.fit(x_train, y_train)\n# find the best alpha\nalpha = ridge_cv.alpha_\nprint ('best alpha:')\nprint (alpha)","10df033d":"# lm_model = LinearRegression()\nlm_model = Ridge(alpha = alpha, normalize=True)\nlm_model.fit(x_train, y_train)\ny_test_preds = lm_model.predict(x_test)\ny_train_preds = lm_model.predict(x_train)\n\n#r2 value\nr2_scores_test = r2_score(y_test, y_test_preds)\nr2_scores_train = r2_score(y_train, y_train_preds)\n\nprint ('r2_score_test:')\nprint (r2_scores_test)\nprint ('r2_score_train:')\nprint (r2_scores_train)\n\nfig = plt.figure(figsize =(10, 4))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nax = plt.axes(aspect = 'equal')\nplt.subplot(121)\nplt.title('Scatter comparison', fontsize=14)\nplt.scatter(y_test, y_test_preds, color='blue')\nplt.xlabel('Real Price', fontsize=14)\nplt.ylabel('Predicted Price', fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n# plt.legend(['KDE'], prop={\"size\":12})\n# plt.show()\n\nplt.subplot(122)\nsns.distplot(y_test_preds, hist=False,\n             kde_kws={'color': 'b', 'lw': 2, 'label': 'Predicted price'})\nsns.distplot(y_test, hist=False,\n             kde_kws={'color': 'r', 'lw': 2, 'label': 'Real price'})\nplt.title('Distribution comparison', fontsize=14)\nplt.ylabel('Probablity', fontsize=12)\nplt.xlabel('Price (USD)', fontsize=12)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(['Predicted price', 'Real price'], prop={\"size\":12})\nplt.show()","ff1d7ee6":"# RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nRFR = RandomForestRegressor(n_estimators = 100, max_depth=20,random_state=42)\nRFR.fit(x_train, y_train)\ny_test_preds = RFR.predict(x_test)\ny_train_preds = RFR.predict(x_train)\n\n#r2 value\nr2_scores_test = r2_score(y_test, y_test_preds)\nr2_scores_train = r2_score(y_train, y_train_preds)\n\nprint ('r2_score_test:')\nprint (r2_scores_test)\nprint ('r2_score_train:')\nprint (r2_scores_train)","d5946df9":"# XGBoost\nimport xgboost as xgb\n\nxgbR = xgb.XGBRegressor(n_estimators = 100, max_depth=4, seed = 42) \nxgbR.fit(x_train, y_train)\ny_test_preds = xgbR.predict(x_test)\ny_train_preds = xgbR.predict(x_train)\n\n#r2 value\nr2_scores_test = r2_score(y_test, y_test_preds)\nr2_scores_train = r2_score(y_train, y_train_preds)\n\nprint ('r2_score_test:')\nprint (r2_scores_test)\nprint ('r2_score_train:')\nprint (r2_scores_train)","c5fbf7fc":"# Imports\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","1d2b2dc1":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor","e696d89d":"x_train.shape","e15eeac9":"scaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\nscaler_x.fit(x_train)\nx_train_scale = scaler_x.transform(x_train)\nx_train_scale[:5]\n\ny_train = y_train.values.reshape(-1,1)\ny_train.shape\n\n\nscaler_y.fit(y_train)\ny_train_scale= scaler_y.transform(y_train)\n\ny_test = y_test.values.reshape(-1,1)\nscaler_x.fit(x_test)\nscaler_y.fit(y_test)\nx_test_scale = scaler_x.transform(x_test)","99fa4b2e":"y_test_scale = scaler_y.transform(y_test)","d4afc148":"# define base model\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(64, input_dim=68, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, input_dim=68, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n# evaluate model\nestimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=100, verbose=0)\nkfold = KFold(n_splits=10)\nresults = cross_val_score(estimator, x_train_scale, y_train_scale, cv=kfold)\nprint(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","7ecf3464":"# Building the model\nfrom keras.optimizers import Adam\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=68, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(16,  activation='relu'))\nmodel.add(Dense(4,  activation='relu'))\n# model.add(Dense(4,  activation='relu'))\nmodel.add(Dense(1, activation='linear'))\n\n# Compiling the model\n# model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(loss = 'mse', optimizer=optimizer, metrics=['mse'])\n\nprint (model.summary())\n\n# Running and evaluating the model\nhistory = model.fit(x_train_scale, y_train_scale,\n          batch_size=128,\n          epochs=20,\n          validation_split=0.3,\n#           validation_data=(x_test_scale, y_test_scale), \n          verbose=2)\nprint(history.history.keys())\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","c741c043":"model","96b1894b":"y_train_scale_preds = model.predict(x_train_scale)\nr2_scores_train = r2_score(y_train_scale, y_train_scale_preds)\nprint (r2_scores_train)\n\ny_test_scale_preds = model.predict(x_test_scale)\nr2_scores_test = r2_score(y_test_scale, y_test_scale_preds)\nprint (r2_scores_test)","7a0078f2":"Observation: We see % sign in the variebles regarding rate, thus we need to remove that sign and also change the varible type to float so that it can be measured ","1b11c4e9":"### Heatmap for the corr of categorical variables","2a1a8cd3":"Observation: We see $ sign in the price variebles, thus we need to remove that sign and also change the varible type to float so that it can be measured ","472939c7":"Observation: The data contains 95 variables (columns), which means that we need to deal with a problem with many dimensions of freedom. Thus, we need to pay more attention later when selecting the variebles or even drop some varibles that are obviously not affect the response, which is the price. \n\nSome varibles that I observed are not usefull are: 'id', 'host_id', 'scrape_id', 'listing_url', 'jurisdiction_names','license','thumbnail_url', 'medium_url', 'picture_url',\n'xl_picture_url','host_thumbnail_url','host_picture_url'","9e6730bf":"### Observations\u00b6\n\n1. **Neighbourhood:**\nAs expected, the top price neighbourhoods are **Leather District**, **Downtown**, **Chinatown** and **South Boston Waterfront**, where are the **business**, **food**, and **tourist center**. \n2. **Property type:**\nGuesthouse, Boat, Villa, and Loft that can give a **special** or at least **different experience** than regular home have higher price. \n3. **Room type:**\nIt can be observed that **entire home\/apt** which can offer **most privacy** and service has the **highest** price, private room with intermediate privacy is the second, and shared room with least privacy is the last. \n4. **Bed type:**\n**Comfort** matters! **Real bed** that offers most comfort have the **highest** price. The more comfort the bed can offer, the higher price, which also makes sense. \n5. **Cancellation policy:**\nIt seems that properties with **less price** has **more flexible** cancellation policy.\n6. **Superhost:** \n**Spuperhost hosts** tend to mark their listing price **lower** than people are not. \n7. **Instant bookable:**\n**Instant bookable** home tend to have **lower** price.  \n8. **Location exact:**\nHome with **accurate location** description tends to have **higher** price.\n9. **Request guest phone verification:**\nHost who require **guest phone verification** tends to have **higher** price. \n10. **Request guest profile picture:**\nHost who require **guest profile picture** tends to have **lower** price. ","6f4d9d06":"# 3 Data preparation <a class='anchor' id='Section_3'>\n\nIt is important to select high correlated variables in order to build a more accurate model. We also need to pay attention the numbers of the selected variables. Insufficient number of variables can not achieve high accuracy, while too many variables may cause overfitting. \n\nIn the section, the top 15 numerical variables highly correlated with the response price are selected for the price prediction initially. In addition, dummy variables are obtained for the selected categorical variables with 0, 1 encodings. Total 20 dummy variables having highest corr with price are obtained. Considering the size of the dummy variables are not large, they will be all used for the initial modeling. \n\n## Numerical and categorical variable selection","7af47a1c":"# neural network","7f8d43dd":"### 2.2.2 Change data type\nIn this step, we will fix some datatype errors, extract numbers and change to float type","87e790e0":"# Question 1: What are the top factors affect the listing price? Question 2: How does different factors affect the listing price? increase or decrease?  <a class='anchor' id='Section_q12'> \n\nIn **section 2.6** and **2.7**, descriptive analysis are conducted and factors affecting the price inluding the **numerical** and **catorgorical variables** are studied. **Heatmap** of correlation between the numberical varibles and the responce price are plotted. The top numerical factors as well as their effect on the price are discussed. Ragarding the categorical varibles, several factors which may affect the price are studied with **box plot** and their effect on the price is discussed. ","5be56f00":"# Discussion 2 regarding Question 1 and Queation 2 <a class='anchor' id='Section_anws_q12_2'>\n\nThrough analysis of the categorical variables, we see that all the factors presented above affect the listing price. \n\nAs an **existing Airbnb host** with an already listed property, a few factors can be taken into consideration to increase the listing price. \n- **Privacy matters**! The more privacy, the higher of listing price. As expected, entire home\/apt which can offer most privacy and service has the highest price. Especially for hosts with shared room listing, working on the privacy level can for sure increase the listing price. For example, curtains and panels can be used to separate the shared room and increase more privacy. Note that you should highlight in your listing description that you can achieve privacy with your shared room listing, I would like to think some people will be attracted and pay a few more dollars.\n- **Comfort matters**! Real bed has much higher price than others. At least for me, I would like to pay extra bucks so I can have a good sleep and feel fresh the second day. It does not matter if it is a fancy or expensive bed, but it should have the matching comfort level for the price you offer. And if you can add more details to make it more comfortable, eg. soft throws or puffy pillows, it will at least improve the overall design and catch more guests\u2019 eyes. Remember to show it in your description as well.\n- **Experience matters**! It seems that homes offering a special or at least different experience for the guests have higher listing price. I am not saying magically transform your listed property to a palace, but at least add some touch that shows your personally, the local culture or harmony with the location either nature or city, which might attract more people.\n- **Accurate description**! Home with accurate description tends to have higher price, so try to pay attention to any details of your property and accurately provide all the information for your guests.\n\nIf you want to **invest** an Airbnb home. Then you should **first** pay attention to the **Neighborhoood** based on your budget. And also considers the 3 factors listed above (**privacy**, **comfort**, **experience** and **accurate discription** ) and try maximize each one within your budget.\n\nAlthough other factors have relationship with the listing price, I can not draw my conclusion that they will difinitely change your listing price. For example, making your **Cancellation policy** super strict may not increase your listing price, It seems to me that properties that have more values tend to have super strict cancellation policy in order to protect the peoperty and maximize the profit. Or the **Instant bookable** factor, the box plot shows **Instant bookable** home tend to have **lower** price. It does not mean that enable this factor will decrease the price, it just indicates lower price home might need less cleaning and preparation time. But these factors for sure can be used for the price prediction.","36c5a156":"### Select top 15 numerical variables","50377540":"## 3.2 Categorical Variables selction <a class='anchor' id='Section_3_2'>\n\nAs mentioned in section 2.7, several categorical varibles which may affect the price are studied after taking a detailed observation of the data based on my personal life experience and intuitive understanding. In addition, the box plot results also shows that the selected variables also have effect on the price.\n\nThe selected categorical varibles are: \n'neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', 'host_is_superhost', 'instant_bookable', 'cancellation_policy', 'require_guest_phone_verification', 'require_guest_profile_picture', 'is_location_exact'","2b866c53":"## 2.6 Numerical variable analysis <a class='anchor' id='Section_2_6'>\n\n### Add one numerical variable: n_amenities\nAmenities are one of the important factors that affects the price, in this step, the amenities column(strs) is transformed to a new numerical variable, which is number of amenities","8a855674":"## 2.7 Categorical variable analysis <a class='anchor' id='Section_2_7'>\n\nRagarding the categorical varibles, several factors which may affect the price are studied after taking a detailed observation of the data. The following varibles are selected based on my personal life experience and intuitive understanding.\n\n'neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', 'host_is_superhost', 'instant_bookable', 'cancellation_policy', 'require_guest_phone_verification', 'require_guest_profile_picture', 'is_location_exact'","6b591007":"# 4. Data modeling and evaluation <a class='anchor' id='Section_4'>\n## 4.1 LinearRegression modeling <a class='anchor' id='Section_4_1'>\n\nIn this section, LinearRegression from sklearn is used to fit the model. Through minimizing the residual sum of squares between the observed targets in the dataset, the weighting coeffcients are obtained for each selected variable and the targets are predicted by the linear approximation.\n\nNote: 15 top corr numerical variables and 51 top corr categorical variebles are used","04a46e46":"# 2 Data understanding <a class='anchor' id='Section_2'>\n## 2.1 Import packages and read data <a class='anchor' id='Section_2_1'>","5a476ecc":"## 2.5 Basemap plot <a class='anchor' id='Section_2_5'>","ef85e77a":"## 3.3 Construct and format data <a class='anchor' id='Section_3_3'>","09d07fc2":"## 4.2 Evaluate the Results <a class='anchor' id='Section_4_2'> \nWith the selected 15 top numerical variables and 20 categorical variables, the obtained r2 score on the test data is 0.6535, which is not a very high score.\n\nThe scatter plot shows a positive linear trend, which is as expected. However, there are still a lot of outliers indicating loss of accuracy on those points. In addition, the estimation is more accurate in the low price range (<200), since most of the data points are in that range. \n\nIn the distribution comparison plot, the trend is similar below 200, while there is a considerable difference in the range >200, which also indicates that the estimator works better in the low price range.  ","f368ac59":"## 4.6 Explore RidgeCV  <a class='anchor' id='Section_4_6'>\n1. Explore that if RidgeCV can improve the estimation accuracy since the data contains variables that highly correlated. Through adding bias, the variance of the estimates can be reduced. \n2. Thorugh observing the results, it turns out that the r2 score for the test data has not been improved.","a82a3cec":"# Table of contents\n* [1 Business understanding](#Section_1)\n* [2 Data understanding](#Section_2)\n    * [2.1 Import packages and read data](#Section_2_1)\n    * [2.2 Initial data cleaning](#Section_2_2)\n    * [2.3 Price distribution](#Section_2_3)\n    * [2.4 Remove outliers](#Section_2_4)\n    * [2.5 Basemap plot](#Section_2_5)\n    * [2.6 Numerical variable analysis](#Section_2_6)\n    * [2.7 Categorical variable analysis](#Section_2_7)\n* [3 Data preparation](#Section_3)\n    * [3.1 Numerical variable selection](#Section_3_1)\n    * [3.2 Categorical Variables selction](#Section_3_2)\n    * [3.3 Construct and format data](#Section_3_3)\n* [4 Data modeling and evaluation](#Section_4)\n    * [4.1 LinearRegression modeling](#Section_4_1)\n    * [4.2 Evaluate the Results](#Section_4_2)\n    * [4.3 Find optimal LinearRegression model](#Section_4_3)\n    * [4.4 Modeling with the best linear model](#Section_4_4)\n    * [4.5 Compare the Results](#Section_4_5)\n    * [4.6 Explore RidgeCV](#Section_4_6)\n    \n# Questions\n* [Questions 1 and 2 raised](#Section_q12)\n    * [Questions 1 and 2 discussion 1](#Section_anws_q12_1)\n    * [Questions 1 and 2 discussion 2](#Section_anws_q12_2)\n* [Questions 3 raised](#Section_q3)\n    * [Questions 3 discussion](#Section_anws_q3)\n    ","4f2fae43":"### 2.2.3 Drop missing data\nIn this step, we will drop some missing data.\n- For any row of data, a listing, if the price value is missing, we should drop this row of data, since it will not help us predict the price when there is no price.\n- For any column of data, a varieble, if more than 50% of data is missing, we will drop that column. The reason is that most of the data missing will not help us get an model with accuracy due to lack of information.\n","e070d53f":"**Observation:** \n1. It follows positive distribution, 75% of the data are in the range below 220 USD. Most of the data are in the range below 500 USD. \n2. In this study, the price range is selected in the range 20 to 500 USD, which is reasonable for an Airbnb listing price and also falls into people's expection. ","26ca8e5e":"## 4.5 Compare the Results <a class='anchor' id='Section_4_5'>\n\nThe best number of numerical and categorical variables are obtained, which are 18 and 51 respectively. With the selected variables and the best model, the obtained r2 score on the test data is 0.6821. Compared to previous model, the r2 score improved 0.029. However, the improvement is not very significant.\n\nThe scatter plot does not vary significantly compared to previous model. \n\nIn the distribution comparison plot, an improvement in the price range below 100 has been observed compared to previous model. The predicted price distribution in that range matches with the real price better.  ","b03b8b8f":"### Select the top 20 categorical variables","ab67a04e":"# Disscusion about Question 3 -- How to select variables?\n\nIt is important to select **high correlated variables** in order to build a more accurate model. We also need to pay attention the numbers of the selected variables. Insufficient number of variables can not achieve high accuracy, while too many variables may cause overfitting. \n\nBased on the heatmap plot for numerical variables in section 2.6, the highly correlated variables are removed. The **top 15 numerical variables** based on the **absolute corr with price** are selected for the price prediction initially.\n\nSeveral categorical varibles which may affect the price are studied after taking a detailed observation of the data based on my personal life experience and intuitive understanding. In addition, the box plot results in section 2.7 also **proves that the selected variables also have effect on the price**.\n","64f34c8b":"# Question 3 Can we predict the listing price in Boston? How to select variables? <a class='anchor' id='Section_q3'>\n\nIn **section 3**, detailed explaination about how to selected variables are illustrated.  \nIn **section 4**, an estimator using linear regression model is established and optimaized by selecting different combinations of numerical and categorical variables.","8995c19d":"### Get dummy variables\n\nIn order to improve model accuracy, we will creat a new column of each level of the selected interested categorical variables. One of the main ways for working with categorical variables is using 0, 1 encodings. In this way, each level of the response will be refelcted on the price prediction. In addition, no rank of categories is imposed.","1a342203":"## 2.3 Price distribution <a class='anchor' id='Section_2_3'>","532cdc6e":"# Further improvement using other models\n## XGB","209226c1":"## 2.4 Remove outliers <a class='anchor' id='Section_2_4'>\n\nIn this step, outliers are removed for unreasonbale Airbnb prices. The data where price below 20 will be removed from the following analysis since it is rare for a listing price below 20. In addition, the data where the price beyond 500 will be removed as well since most of the data are in that range which can be seen from the price distribution plot. ","800b3fc5":"### Heatmap for the corr of the selcted numerical variables","36674868":"### Heatmap plot \nIn this step, the correlations between the numerical variebles are visualized with heatmap. The corr of the numerical variebles with price are ordered in a descending order, where the blue indicates positive effect on price, while the orange indicates negtive effect on price. ","5568d6d8":"## 2.2 Initial data cleaning <a class='anchor' id='Section_2_2'>\n### 2.2.1 Drop unrelated variables","4388b764":"## 4.4 Modeling with the best linear model <a class='anchor' id='Section_4_4'>","20fb7ce5":"## 4.3 Find optimal LinearRegression model <a class='anchor' id='Section_4_3'>\n\nBased on the results, different combinations of the number of top corr numerical and categorical variebles are investigated and the best combination are obtained as well as the best linear regression model.","0c8cc488":"# 1 Business understanding <a class='anchor' id='Section_1'>\nOn one hand, it is popular nowadays that people become an Airbnb host and make extra money with their extra assets or even an extra room in their own home. More and More people are thinking of investing a new property and transform it to an Airbnb. Here comes the questions, what should the hosts consider to make their listing property attactive in order to increase the listing price? \n\nOn the other hand, hosts tend to remove their listing if there is no or very few booking for a long time. Thinking from the view of Airbnb, can we predict the listing price for the hosts and recommend them a reasonable price so they can attract more guests? How to select variables to get a reasonable price estimator?\n\n\nIn this notebook, I dug into the Boston Airbnb listing data from 2019 to answer the questions summerized in the following:\n1. What are the top factors affect the listing price? \n2. How does the factors affect the listing price? increase or decrease?\n3. Can we predict the listing price in Boston? How to select variables?\n\nThe data can be found in [here](https:\/\/www.kaggle.com\/airbnb\/boston\/kernels). ","115878b8":"# Disscusion about Question 3 -- Can we predict the listing price in Boston? <a class='anchor' id='Section_anws_q3'>\n\nYes. But with 0.682 r2 score. And the estimator works better in the low price range (<200)\n\nThrough using the aforementioned factors, we can train the data using a linear regression model and predict the listing price. To optimize the model, different combinations of numbers of numerical and categorical factors (with highest corr with price) are explored to improve the model accuracy as well as avoid overfitting.\n\nThe best estimator has a r2 score of 0.682, which is not high but still reasonable since there are so many factors we need to consider that affect the price.\n\nThe scatter plot shows a positive linear trend, which is as expected. However, there are still a lot of outliers indicating lost of accuracy on those points. In addition, the estimation is more accurate in the low price range (<200), since most of the data points are in that range. In the distribution comparison plot, the trend is similar below 200, while there is a considerable difference in the range >200, which also indicates that the estimator works better in the low price range.","4e2454e7":"**Observation**\n\nOne obvious trend is that the high price listings are most in the city center, and the low price listings are most in the suburb, which makes common sense. ","471d7605":"### Imputing missing numerical values \nIn this step, the numerical vraibles are extracted from the listing data. The missing values are filled with the mean value, which is one of the most frequently used method in data science. ","a6edaad2":"### Observations \n\n1. some mutually high correlated numerical variables:\n    -  'host_total_listings_count', 'host_total_listings_count', 'calculated_host_listings_count'\n    -  'availability_30', 'availability_60', 'availability_90'\n2. some numerical variables that have high affect on price \n    -  'accommodates', 'bedrooms', 'beds', 'cleaning_fee'\n\n# Discussion 1 regarding to Question 1 and Queation 2 <a class='anchor' id='Section_anws_q12_1'>\n\nThe heatmap above shows the numerical factor influence on the price, where blue indicates positive effect and orange indicates negative effect. We can see the **functionality related factors** of the listed property matters the most, such as **accommodates**, **bedrooms**, and **beds** all have high **positive** effect on the listing price, aka. incresing the listing price. Surprisingly, the **reviews per month** and **number of reviews** have a negative effect on the listing price, aka. decreasing the listing price.\n\n","1cac479c":"## 3.1 Numerical variable selection <a class='anchor' id='Section_3_1'>\n\nBased on the heatmap plot for numerical variables in section 2.6, the highly correlated variables are removed.\n\nThe top 15 numerical variables based on the absolute corr with price are selected for the price prediction initially."}}