{"cell_type":{"36b025be":"code","a8194369":"code","8e46e68e":"code","5f4e0e35":"code","d7c4e76f":"code","14e643ce":"code","e27878d1":"code","fbf7fa2c":"code","444ea33a":"code","5efbd2b6":"code","5d647017":"code","93b1021e":"code","d10a23c9":"code","998a867d":"code","ed686afd":"code","f3a6cb76":"code","2e8c63b4":"code","e6edff76":"code","1407ca87":"code","4bf2e77a":"code","fa2052a7":"code","4b08d6e4":"code","e33c5ced":"code","3af01424":"code","2dbf422b":"code","8818f91d":"code","41cac43b":"code","bbcc251f":"code","a3a8c5d4":"code","6ef423eb":"code","a53b1349":"code","8dccb419":"code","68304d27":"code","9a68c464":"code","5dda4379":"code","19db9ec2":"code","429345d0":"code","7179e73c":"code","4d996af9":"code","7c307210":"code","5fb91201":"code","d500f4ef":"code","22465859":"code","218a874b":"code","910c376b":"code","720f21b4":"code","f67e6c9e":"code","7e400501":"code","7861ea71":"code","d512f640":"code","8e647c6a":"code","b0032b29":"code","c3ba5393":"code","782da14e":"code","492f9cb9":"code","5aae3a58":"code","aeb4abb2":"code","59ea0cb3":"code","7cbf85f8":"code","57dd85b1":"code","cb8d1810":"code","941631e0":"code","b79bc8af":"code","4a9fcf57":"code","31a86477":"code","e1efe0b1":"code","9476374f":"code","0d1fe432":"code","57c773dd":"code","196fa947":"code","57b08230":"code","20d4dcd5":"code","025258f2":"markdown","abcbac25":"markdown","94320eae":"markdown","2c27f2a0":"markdown","313a8b84":"markdown","0300fcb1":"markdown","2a2a1f91":"markdown","b30e9a18":"markdown","9375294d":"markdown","af972782":"markdown","dba28ab2":"markdown","9aa837bd":"markdown","b1e0a8b3":"markdown","eb212abe":"markdown","18ae42e9":"markdown","1a8a43e4":"markdown","3e4aac22":"markdown","2a61acb5":"markdown","29d948ab":"markdown","819c0f8a":"markdown","418f7f13":"markdown","0fcd201f":"markdown","74d684b8":"markdown","bde9be48":"markdown","b02541bc":"markdown","acdf0912":"markdown","ed547ebe":"markdown","fe1e7bb9":"markdown","3b9aebcf":"markdown","ef7c8e49":"markdown","c7cafc55":"markdown","74d52eee":"markdown","e7febaa9":"markdown","f9c7c161":"markdown","15666c71":"markdown","8ed5616f":"markdown","2d3612ba":"markdown","17690c51":"markdown","3f7afdb5":"markdown","c9f6f78b":"markdown","6f7b50dd":"markdown","3730851b":"markdown","626332f0":"markdown","44dd8d9d":"markdown","658dfdce":"markdown","d0418c88":"markdown","ab787e26":"markdown","d275c4c3":"markdown","9f72e2c6":"markdown","d08a69f2":"markdown","7221de1b":"markdown","616b3072":"markdown","e530c0bb":"markdown","96a063fc":"markdown","e600e031":"markdown","3fd53cfa":"markdown","43aeea23":"markdown","a733447a":"markdown"},"source":{"36b025be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a8194369":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","8e46e68e":"# load data\ndata_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata_test  = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n#preview data\nprint (data_train.info())\nprint(\"-\"*20)\nprint('Train columns with null values:\\n', data_train.isnull().sum())\nprint(\"-\"*20)\nprint (data_test.info())\nprint('Train columns with null values:\\n', data_test.isnull().sum())\nprint(\"-\"*20)\ndata_train.sample(10)","5f4e0e35":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived',fontsize=15)\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data_train,ax=ax[1])\nax[1].set_title('Survived',fontsize=15)\nplt.show()","d7c4e76f":"#sex\nf,ax=plt.subplots(1,2,figsize=(18,8))\ndata_train[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex',fontsize=15)\nsns.countplot('Sex',hue='Survived',data=data_train,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead',fontsize=15)\nplt.show()","14e643ce":"plt.figure(figsize=(9,8))\ndata_train['Sex'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',shadow=True, label=\"\")\nplt.title('Sex', fontsize=15)","e27878d1":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata_train['Pclass'].value_counts().plot.bar(color=['steelblue','#e24932','goldenrod'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass', fontsize=15)\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=data_train,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead', fontsize=15)\nplt.show()","fbf7fa2c":"plt.figure(figsize=(18,8))\nplt.subplot(1, 2, 1)\nsns.barplot(x = \"Pclass\", \n            y = \"Survived\", \n            data=data_train, \n            linewidth=5,\n            capsize = .1)\nplt.title(\"Paclass - Survived\", fontsize=15)\nplt.xlabel(\"Socio-Economic class\");\nplt.ylabel(\"% of Passenger Survived\");\nlabels = ['Upper', 'Middle', 'Lower']\n#val = sorted(train.Pclass.unique())\nval = [0,1,2] ## this is just a temporary trick to get the label right. \nplt.xticks(val, labels);\n# Kernel Density Plot\nplt.subplot(1, 2, 2)\nax=sns.kdeplot(data_train.Pclass[data_train.Survived == 0] , \n               color='steelblue',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 1),'Pclass'] , \n               color='#e24932',\n               shade=True, \n               label='survived', )\nplt.title('Passenger Class Distribution - Survived vs Non-Survived',fontsize=15)\nplt.ylabel(\"Frequency of Passenger Survived\")\nplt.xlabel(\"Passenger Class\")\nplt.xticks(sorted(data_train.Pclass.unique()), labels);","444ea33a":"sns.factorplot('Pclass','Survived',hue='Sex',data=data_train)\nplt.show()","5efbd2b6":"sns.set(font_scale=1)\ng = sns.catplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\",\n                    data=data_train, saturation=.5,\n                    kind=\"bar\", ci=None, aspect=.6)\n(g.set_axis_labels(\"\", \"Survival Rate\")\n    .set_xticklabels([\"Men\", \"Women\"])\n    .set_titles(\"{col_name} {col_var}\")\n    .set(ylim=(0, 1))\n    .despine(left=True))  \nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How many Men and Women Survived by Passenger Class');\n\nfor myaxis in g.axes[0]:\n    for patch in myaxis.patches:\n        label_x = patch.get_x() + patch.get_width()\/2  # find midpoint of rectangle\n        label_y = patch.get_y() + patch.get_height()\/2\n        myaxis.text(label_x, label_y,\n                    #left - freq below - rel freq wrt population as a percentage\n                    '{:.3%}'.format(patch.get_height()),\n                   horizontalalignment='center', verticalalignment='center')","5d647017":"print('Oldest Passenger was of:',data_train['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data_train['Age'].min(),'Years')\nprint('Average Age on the ship:',data_train['Age'].mean(),'Years')\nplt.figure(figsize=(18,8))\nplt.hist(data_train['Age'], \n        bins = np.arange(data_train['Age'].min(),data_train['Age'].max(),5),\n        normed = True, \n        color = 'steelblue',\n        edgecolor = 'k')\nplt.title('Distribution of Age', fontsize=20)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nax=sns.kdeplot(data_train['Age'] , \n               color='#e24932',\n               shade=False,\n               label='not survived',\n              linewidth=5)\nplt.tick_params(top='off', right='off')\nplt.legend([ax],['KDE'],loc='best')\nplt.show()","93b1021e":"# fill the missing value of age\ndata_cleaner = [data_train, data_test]\ndata_train['Title'], data_test['Title']=0,0\nfor dataset in data_cleaner:\n    dataset['Title']=dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nprint(data_train['Title'].value_counts())\nprint(\"-\"*20)\nprint(data_test['Title'].value_counts())","d10a23c9":"for dataset in data_cleaner:\n    dataset['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','the Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],\n                                ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Mr','Mrs'],inplace=True)\nprint(data_train.groupby('Title')['Age'].mean())\nprint(\"-\"*20)\nprint(data_test.groupby('Title')['Age'].mean())","998a867d":"# ## Assigning the NaN Values with the Ceil values of the mean ages\ndata_train.loc[(data_train.Age.isnull())&(data_train.Title=='Mr'),'Age']=33\ndata_train.loc[(data_train.Age.isnull())&(data_train.Title=='Mrs'),'Age']=36\ndata_train.loc[(data_train.Age.isnull())&(data_train.Title=='Master'),'Age']=5\ndata_train.loc[(data_train.Age.isnull())&(data_train.Title=='Miss'),'Age']=22\n\ndata_test.loc[(data_test.Age.isnull())&(data_test.Title=='Mr'),'Age']=32\ndata_test.loc[(data_test.Age.isnull())&(data_test.Title=='Mrs'),'Age']=40\ndata_test.loc[(data_test.Age.isnull())&(data_test.Title=='Master'),'Age']=7\ndata_test.loc[(data_test.Age.isnull())&(data_test.Title=='Miss'),'Age']=22\n\nfor dataset in data_cleaner:\n    print(dataset.Age.isnull().any())","ed686afd":"#now we see the distribution again\nprint('Oldest Passenger was of:',data_train['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data_train['Age'].min(),'Years')\nprint('Average Age on the ship:',data_train['Age'].mean(),'Years')\nplt.figure(figsize=(18,8))\nplt.hist(data_train['Age'], \n        bins = np.arange(data_train['Age'].min(),data_train['Age'].max(),5),\n        normed = True, \n        color = 'steelblue',\n        edgecolor = 'k')\nplt.title('Distribution of Age', fontsize=20)\nax=sns.kdeplot(data_train['Age'] , \n               color='#e24932',\n               shade=False,\n               label='not survived',\n               linewidth=5)\nplt.tick_params(top='off', right='off')\nplt.legend([ax],['KDE'],loc='best')\nplt.show()","f3a6cb76":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata_train[data_train['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='steelblue')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndata_train[data_train['Survived']==1].Age.plot.hist(ax=ax[1],color='#e24932',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()\n\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 0),'Age'] , color='steelblue',shade=True,label='not survived')\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 1),'Age'] , color='#e24932',shade=True, label='survived')\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 20)\nplt.xlabel(\"Age\")\nplt.ylabel('Frequency');","2e8c63b4":"# check the summary statistic\ndata_train['Age'].describe().to_frame()","e6edff76":"# check the missing value\ndata_train[data_train.Embarked.isnull()]","1407ca87":"sns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(16,12),ncols=2)\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=data_train, ax = ax[0]);\nax2 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=data_test, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 15)\nax2.set_title('Test Set',  fontsize = 15)","4bf2e77a":"data_train.Embarked.fillna(\"C\", inplace=True)\ndata_train.Embarked.isnull().any()","fa2052a7":"f,ax=plt.subplots(1,3,figsize=(20,6))\nsns.countplot('Embarked',data=data_train,ax=ax[0])\nax[0].set_title('No. Of Passengers Boarded', fontsize = 15)\nsns.countplot('Embarked',hue='Survived',data=data_train,ax=ax[1])\nax[1].set_title('Embarked vs Survived', fontsize = 15)\nsns.barplot(x = 'Embarked', y = 'Survived', data=data_train,ax=ax[2])\nax[2].set_title('Embarked Survival Rate', fontsize = 15)\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","4b08d6e4":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.countplot('Embarked',hue='Sex',data=data_train,ax=ax[0])\nax[0].set_title('Male-Female Split for Embarked', fontsize = 15)\nsns.countplot('Embarked',hue='Pclass',data=data_train,ax=ax[1])\nax[1].set_title('Embarked vs Pclass', fontsize = 15)\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","e33c5ced":"tab = pd.crosstab(data_train['Embarked'], data_train['Pclass'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Port embarked')\ndummy = plt.ylabel('Percentage')","3af01424":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data_train)\nplt.show()","2dbf422b":"pd.crosstab([data_train.SibSp],data_train.Survived).style.background_gradient(cmap='summer_r')","8818f91d":"f,ax=plt.subplots(2,2,figsize=(20,12))\nsns.barplot('SibSp','Survived',data=data_train,ax=ax[0,0])\nax[0,0].set_title('SibSp vs Survived')\nsns.factorplot('SibSp','Survived',data=data_train,ax=ax[0,1])\nax[0,1].set_title('SibSp vs Survived')\nsns.barplot('Parch','Survived',data=data_train,ax=ax[1,0])\nax[1,0].set_title('Parch vs Survived')\nsns.factorplot('Parch','Survived',data=data_train,ax=ax[1,1])\nax[1,1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","41cac43b":"pd.crosstab(data_train.SibSp,data_train.Pclass).style.background_gradient(cmap='summer_r')","bbcc251f":"# first check the missing value\ndata_test[data_test.Fare.isnull()]","a3a8c5d4":"missing_value = data_test[(data_test.Pclass == 3) & (data_test.Embarked == \"S\") & (data_test.Sex == \"male\")].Fare.mean()\n# replace the test.fare null values with test.fare mean\ndata_test.Fare.fillna(missing_value, inplace=True)\ndata_test.Embarked.isnull().any()","6ef423eb":"fig = plt.figure(figsize=(15,8))\nsns.kdeplot(data_train['Fare'] , color='steelblue',shade=True)\nplt.title('Fare Distribution', fontsize = 20)","a53b1349":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 0),'Fare'] , color='steelblue',shade=True,label='not survived')\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 1),'Fare'] , color='#e24932',shade=True, label='survived')\nplt.title('Fare Distribution Survived vs Non Survived', fontsize = 20)\nplt.ylabel(\"Frequency of Passenger Survived\")\nplt.xlabel(\"Fare\");","8dccb419":"sns.scatterplot(data_train[\"Fare\"], data_train[\"Age\"])","68304d27":"data_train[data_train.Fare > 300]","9a68c464":"# drop the outliers\ndata_train = data_train[data_train.Fare < 300]","5dda4379":"# check the missing value\nprint(\"Train Cabin missing: \" + str(data_train.Cabin.isnull().sum()\/len(data_train.Cabin)))\nprint(\"Test Cabin missing: \" + str(data_test.Cabin.isnull().sum()\/len(data_test.Cabin)))","19db9ec2":"survivers = data_train.Survived # save the label for a while\ndata_train.drop([\"Survived\"],axis=1, inplace=True)\nall_data = pd.concat([data_train,data_test], ignore_index=False)\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)\nall_data.sort_values(\"Cabin\").head(10)","429345d0":"all_data.Cabin = [i[0] for i in all_data.Cabin]\nall_data[\"Cabin\"].value_counts(normalize=True)","7179e73c":"with_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]\n\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()","4d996af9":"def cabin_estimator(i):\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ndata_train = all_data[:888]\n\ndata_test = all_data[888:]\n\n# adding saved target variable with train. \ndata_train['Survived'] = survivers","7c307210":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.countplot('Cabin',data=data_train, ax=ax[0])\nsns.barplot('Cabin','Survived',data=data_train, ax=ax[1])","5fb91201":"# Placing 0 for female and 1 for male in the \"Sex\" column. \ndata_train['Sex'] = data_train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ndata_test['Sex'] = data_test.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\nsns.heatmap(data_train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","d500f4ef":"data_train['family_size'] = data_train.SibSp + data_train.Parch+1\ndata_test['family_size'] = data_test.SibSp + data_test.Parch+1","22465859":"data_train.groupby('family_size').size().reset_index(name='counts')","218a874b":"data_train['is_alone'] = [1 if i<2 else 0 for i in data_train.family_size]\ndata_test['is_alone'] = [1 if i<2 else 0 for i in data_test.family_size]","910c376b":"f,ax=plt.subplots(1,2,figsize=(18,6))\nsns.factorplot('family_size','Survived',data=data_train,ax=ax[0])\nax[0].set_title('family_size vs Survived')\nsns.factorplot('is_alone','Survived',data=data_train,ax=ax[1])\nax[1].set_title('is_alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","720f21b4":"def family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n# gen fanily group by its size\ndata_train['family_group'] = data_train['family_size'].map(family_group)\ndata_test['family_group'] = data_test['family_size'].map(family_group)","f67e6c9e":"sns.factorplot('is_alone','Survived',data=data_train,hue='Sex',col='Pclass')\nplt.show()","7e400501":"plt.subplots(figsize = (22,10),)\nsns.distplot(data_train.Age, bins = 100, kde = True, rug = False, norm_hist=False);","7861ea71":"def age_group_fun(age):\n    a= ''\n    if age <= 10:\n        a = \"child\"\n    elif age <= 22:\n        a = \"teenager\"\n    elif age <= 33:\n        a = \"yong_Adult\"\n    elif age <= 45:\n        a = \"middle_age\"\n    else:\n        a = \"old\"\n    return a\n\ndata_train['age_group'] = data_train['Age'].map(age_group_fun)\ndata_test['age_group'] = data_test['Age'].map(age_group_fun)","d512f640":"data_train['Fare_Range']=pd.qcut(data_train['Fare'],4)\ndata_train.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","8e647c6a":"def fare_group_fun(fare):\n    a= ''\n    if fare <= 7.896:\n        a = \"low\"\n    elif fare <= 14.454:\n        a = \"normal\"\n    elif fare <= 30.696:\n        a = \"middle\"\n    else:\n        a = \"high\"\n    return a\n\ndata_train['fare_group'] = data_train['Fare'].map(fare_group_fun)\ndata_test['fare_group'] = data_test['Fare'].map(fare_group_fun) # the range apply to both dataset","b0032b29":"# prepare for onehot encoding\ndata_train['Pclass'].astype(str)\ndata_train['Sex'].astype(str)\ndata_train['is_alone'].astype(str)\ndata_test['Pclass'].astype(str)\ndata_test['Sex'].astype(str)\ndata_test['is_alone'].astype(str)","c3ba5393":"# onehot encoding & drop unused variables\ndata_train = pd.get_dummies(data_train, columns=['Title',\"Pclass\", 'Sex','is_alone','Cabin','age_group','Embarked','family_group', 'fare_group'], drop_first=False)\ndata_test = pd.get_dummies(data_test, columns=['Title',\"Pclass\", 'Sex','is_alone','Cabin','age_group','Embarked','family_group', 'fare_group'], drop_first=False)\ndata_train.drop(['family_size','Name','PassengerId','Ticket','Fare_Range'], axis=1, inplace=True)\npassengerid = data_test['PassengerId'].values\ndata_test.drop(['family_size','Name','PassengerId','Ticket'], axis=1, inplace=True)","782da14e":"data_train.head(10)","492f9cb9":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\nage_fare = data_train[['Age','Fare']] # get age and fare features\nage_fare = mm.fit_transform(age_fare)\nage_fare_df = pd.DataFrame(age_fare, columns=['Age','Fare']) # scaling data\ndata_train.drop(['Age','Fare'], axis=1, inplace=True)\ndata_train = data_train.reset_index(drop=True)\ndata_train = pd.concat([data_train, age_fare_df],axis=1) # merge the scaling data back to train data set\n\nage_fare = data_test[['Age','Fare']] #same for test\nage_fare = mm.fit_transform(age_fare)\nage_fare_df = pd.DataFrame(age_fare, columns=['Age','Fare']) # scaling data\ndata_test.drop(['Age','Fare'], axis=1, inplace=True)\ndata_test = data_test.reset_index(drop=True)\ndata_test = pd.concat([data_test, age_fare_df],axis=1)","5aae3a58":"data_train","aeb4abb2":"data_test","59ea0cb3":"X = data_train.drop(['Survived'], axis = 1)\ny = data_train[\"Survived\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)","7cbf85f8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) \ncolumn_names = X.columns\nX = X.values\n\n# use grid search to get the best parameters\nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18] #alpla of lasso and ridge \npenalties = ['l1','l2'] # Choosing penalties(Lasso(l1) or Ridge(l2))\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25) # Choose a cross validation strategy. \nparam = {'penalty': penalties, 'C': C_vals} # setting param for param_grid in GridSearchCV. \nlogreg = LogisticRegression(solver='liblinear')\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                           cv = cv\n                          )\n## Fitting the model\ngrid.fit(X, y)\n\n# get accuracy\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X,y)","57dd85b1":"from sklearn.neighbors import KNeighborsClassifier\nk_range = range(1,31)\nweights_options=['uniform','distance']\nparam = {'n_neighbors':k_range, 'weights':weights_options}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\ngrid.fit(X,y)\nknn_grid= grid.best_estimator_\nknn_grid.score(X,y)","cb8d1810":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X, y)\ny_pred = gaussian.predict(X_test)\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gaussian_accy)","941631e0":"from sklearn.svm import SVC\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\ngrid_search.fit(X,y)\nsvm_grid = grid_search.best_estimator_\nsvm_grid.score(X,y)","b79bc8af":"from sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X, y) \ndectree_grid = grid.best_estimator_\ndectree_grid.score(X,y)","4a9fcf57":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150,155,160];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \nrf_grid = grid.best_estimator_\nrf_grid.score(X,y)","31a86477":"from sklearn.ensemble import BaggingClassifier\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              \n        }\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                      bootstrap_features=False),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \nbagging_grid = grid.best_estimator_\nbagging_grid.score(X,y)","e1efe0b1":"from sklearn.ensemble import AdaBoostClassifier\nn_estimators = [100,140,145,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nlearning_r = [0.1,1,0.01,0.5]\n\nparameters = {'n_estimators':n_estimators,\n              'learning_rate':learning_r\n              \n        }\ngrid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                     ),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \nadaBoost_grid = grid.best_estimator_\nadaBoost_grid.score(X,y)","9476374f":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X, y)\ny_pred = gradient_boost.predict(X_test)\ngradient_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gradient_accy)","0d1fe432":"from sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, y)\ny_pred = ExtraTreesClassifier.predict(X_test)\nextraTree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(extraTree_accy)","57c773dd":"from sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X, y)\ny_pred = GaussianProcessClassifier.predict(X_test)\ngau_pro_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gau_pro_accy)","196fa947":"from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('lr_grid', logreg_grid),\n    ('svc', svm_grid),\n    ('random_forest', rf_grid),\n    ('gradient_boosting', gradient_boost),\n    ('decision_tree_grid',dectree_grid),\n    ('knn_classifier', knn_grid),\n    #('XGB_Classifier', XGBClassifier),\n    ('bagging_classifier', bagging_grid),\n    ('adaBoost_classifier',adaBoost_grid),\n    ('ExtraTrees_Classifier', ExtraTreesClassifier),\n    ('gaussian_classifier',gaussian),\n    ('gaussian_process_classifier', GaussianProcessClassifier)\n],voting='hard')\n\n#voting_classifier = voting_classifier.fit(train_x,train_y)\nvoting_classifier = voting_classifier.fit(X,y)\n\ny_pred = voting_classifier.predict(X_test)\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(voting_accy)","57b08230":"all_models = [logreg_grid,\n              knn_grid, \n              svm_grid,\n              dectree_grid,\n              rf_grid,\n              bagging_grid,\n              adaBoost_grid,\n              voting_classifier]\n\nc = {}\nfor i in all_models:\n    a = i.predict(X_test)\n    b = accuracy_score(a, y_test)\n    c[i] = b","20d4dcd5":"test_prediction = (max(c, key=c.get)).predict(data_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passengerid,\n        \"Survived\": test_prediction})\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)\ndata_train.to_csv(\"cleaned_train.csv\", index=False)\ndata_test.to_csv(\"cleaned_test.csv\", index=False)","025258f2":"## Logistic Regression","abcbac25":"Being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.","94320eae":"## Voting Classifier","2c27f2a0":"#### Positive Correlation Features:\n- Fare and Survived: 0.26\n- Parch and Sibsp: 0.42\n\n#### Negative Correlation Features:\n- Fare and Pclass: -0.6\n- Sex and Survived: -0.55\n- Pclass and Survived: -0.33\n\nWe can see that the features are not much correlated, so we carry on with all features.","313a8b84":"Observations for all features:\n\n**Sex**: The chance of survival for women is higher than men.\n\n**Pclass**:The 1st class passengers have better chances of survival. The survival rate for Pclass3 is very low. \n\n**Age**: Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\n\n**Embarked**: This is a very interesting feature. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Passengers at Q were all from Pclass3.\n\n**Parch+SibSp**: Having 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with.","0300fcb1":"## Creating Dummy Variables & Drop Unused Variables","2a2a1f91":"Observations:\n\n1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Embarkation.\n\n2)The survival rate for both men and women is very low in S (most passenegers are from Pclass)\n\n3)The survival rate for men is extremely low in Q, as almost all were from Pclass3","b30e9a18":"Out of 891 passengers in training set, only around 350 survived i.e Only 38.4% of the total training set survived the crash.","9375294d":"## Gaussian Naive Bayes","af972782":"## Sex & Pclass","dba28ab2":"## Survived","9aa837bd":"## Embarked","b1e0a8b3":"## Age","eb212abe":"Here, in both training set and test set, the **average fare closest to $80** are in the C Embarked values. So, let's fill in the missing values as \"C\"\n","18ae42e9":"# Diagnostic Analytics & Feature Engineering","1a8a43e4":"Just as we mentioned before, as the fare_range increases, the changes of survival increases.","3e4aac22":"Obervation:\n1. Under 100 dollar represents that lots of passengers who bought the ticket within that range did not survive.\n2. When the fare is more than 300 there si no shade area which means, there is no one pay the fare. We consider them as outliers, but let's check.","2a61acb5":"## Extra Trees Classifier","29d948ab":"Observations:\n\n1)Most passenegers boarded from S and the majority of them are from Pclass3.\n\n2)The passengers from C has a good proportion of them survived. The reason may be is that most of them are from Pclass1 and Pclass2, where the rescued rates are high.\n\n3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around 81% didn't survive.\n\n4)Port Q had almost 95% of the passengers were from Pclass3.","819c0f8a":"There is one missing value in test set, we can fill it by take the average of the values where Pclass is 3, Sex is male and Embarked is S. (Because these three figures have stong relationships as we mention above)","418f7f13":"We decide to assign the N based on their fare (compare to the mean fare of each cabin)","0fcd201f":"Observation:\n\n1. If the passenger is alone (no sibling and spouse), he has 34.5% survival rate.\n2. The graph roughly decreases as the number of siblings and spouse increase. (Maybe perople try to save their family instead of saving themselves).\n3. The survival rate for families with 5-8 members is 0%. -- The reason is Pclass. Large family trend to be in Pclass3.\n4. Passenger who traveled in small groups with siblings \/ spouses had better changes of surviving.\n5. The result is similar for Parch","74d684b8":"Observation:\n\n1. The Child with age smaller than 5 were saved in large numbers(The Women and Child First Policy).\n\n2. The oldest Passenger was saved(80 years).\n\n3. Maximum number of deaths were in the age group of 30-40.","bde9be48":"## Decision Tree","b02541bc":"There are two missing value in this feature, let's first deal with it.","acdf0912":"* 63% first class passenger survived titanic tragedy, while \n* 48% second class and \n* only  24% third class passenger survived. \n\nPassenegers Of Pclass 1 were given a very high priority while rescue. Even though the the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low. So money and status matters. However, lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second.","ed547ebe":"The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a women on the ship is around 75% while that for men in around 18-19%. (Women and Children first policy)","fe1e7bb9":"It seems that a fare of 512 is an outlier of the fare feature. We decide to delete them.","3b9aebcf":"## age_group\nBased on the dist of age, we assign different people into different age group","ef7c8e49":"## Bagging Classifier","c7cafc55":"## Splitting the data\nSplit the training data into test and train datasets","74d52eee":"If the person is alone or he has the family size which is larger than 4, his survival rate is small. What's more, based on the graph, we can further classify the fanily group by its size.","e7febaa9":"Remember we have 177 missing values in Age feature, we can assign them with the mean age of the existing data, but we can also take into account other features. For example, the Name feature. Thus, let's first take a look of the Name feature.","f9c7c161":"## family_size & is_alone\nThe family number of each family and whether the person is alone or not (family_size=1)","15666c71":"# Predictive Analytics & Model Building","8ed5616f":"## Fare","2d3612ba":"## Support Vector Machines(SVM)","17690c51":"## AdaBoost Classifier","3f7afdb5":"We think maybr the first alphabets are related to fare, so we group by Cabin to get the mean of each cabin.","c9f6f78b":"## Sex","6f7b50dd":"There are some Titles like Mlle or Mme (from other countries' language) that stands for Miss. We replace them with Miss and same thing for other values.","3730851b":"## Cabin","626332f0":"## Gaussian Process Classifier","44dd8d9d":"## K-Nearest Neighbor","658dfdce":"We can find that Cabin feature is the feature with an English alphabet following some digits. It seems that the digits just stands for the order of the rooms and the alphabets are more meaningful (maybe stands for the type of the room). \n\nThus, we take out the alphabet and group by the alphabet","d0418c88":"From the factor plot, we can know that survival rate for Women from Pclass1 is about 96%. It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.","ab787e26":"We may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a **fare of $80**, are of **Pclass 1** and **female Sex**. Let's see how the Fare is distributed among all Pclass and Embarked feature values","d275c4c3":"## Random Forest","9f72e2c6":"## Correlation Matrix and Heatmap","d08a69f2":"## Scaling data\nAge and Fare are much higher in magnitude compared to other features","7221de1b":"## Pclass","616b3072":"# Descriptive Analysis & Data Cleaning","e530c0bb":"There are lots of missing value in Cabin category. We can either drop this feature or consider about other implication of this feature. For example, the passenger with cabin record may have higher status then other, or the passenger with reocrd may be more likely to be rescued.\n\nSo we first combine tarin and test data and assign the missing value as \"N\"","96a063fc":"## Sibsp & Parch","e600e031":"## fare_group\nWe use quintiles to cut the fare value into 4 groups","3fd53cfa":"let's see the data","43aeea23":"## Gradient Boosting Classifier","a733447a":"Observation:\n1. Most people stay in G and the survival rate of G is low. (Money matters)\n2. The survival rates of E, D, B are relatively high "}}