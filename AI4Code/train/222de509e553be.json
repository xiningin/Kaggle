{"cell_type":{"692be5a1":"code","f2e55db7":"code","6ddeb028":"code","632c45f6":"code","959db4be":"code","94b7e51c":"code","6ca8ca9c":"code","819ba911":"code","8b95d635":"code","17eb5f3f":"code","98907c27":"code","84c46c45":"code","797c5b15":"code","baa43ff6":"code","cd27dcaa":"code","96a0036b":"code","8416c723":"code","1eb78542":"code","e7c29385":"code","ad4ea3ae":"code","c99c9fe3":"code","a8e1a7d2":"markdown","5b0803b1":"markdown","5b34698e":"markdown","bcf4e1a2":"markdown","e80a9201":"markdown","4e9d7db9":"markdown","f58f8104":"markdown","755d4354":"markdown","9b5f7a70":"markdown","f3f4a199":"markdown","a701cc70":"markdown","6c2b2bea":"markdown","3272aba6":"markdown","001fc773":"markdown","27a0586d":"markdown","a81efbd5":"markdown","5e706a12":"markdown"},"source":{"692be5a1":"import pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport math as math\nimport time ","f2e55db7":"df = pd.read_csv(\"..\/input\/internet-articles-data-with-users-engagement\/articles_data.csv\")\ndf.drop('Unnamed: 0',axis=1,inplace=True)\nprint(df.shape)\ndf.head()","6ddeb028":"print(df['source_id'].unique())\nprint(df['source_name'].unique())","632c45f6":"print(df[df['source_name']==\"460.0\"])\n#Since its all full of NAN value we'll drop this useless row\ndf = df[df['source_name']!=\"460.0\"]\nprint(\"Row Dropped\")\ndf.shape","959db4be":"df.isna().sum()","94b7e51c":"print(df[df['description'].isna()].isna().sum())\ndf = df[~df['title'].isna()]\ndf_2 = df.copy()","6ca8ca9c":"df = df_2.copy()\nempty_desc = df[df['description'].isna()]\ndf = df[~df['description'].isna()]\nempty_desc = empty_desc[~empty_desc['content'].isna()]\ndf = pd.concat([df,empty_desc],axis=0)\ndf.isna().sum()\n# print(indexes)\n# print(df.iloc[indexes[-1], : ])\n# df.drop(df.index[[indexes]])\n# print(df[df['description'].isna()].isna().sum())","819ba911":"from gensim.summarization.summarizer import summarize\nfrom gensim.summarization import keywords","8b95d635":"empty_desc = df[df['description'].isna()]\ndf = df[~df['description'].isna()]\n","17eb5f3f":"empty_desc['description'] = empty_desc.apply(lambda x:summarize(x['content'],ratio=0.5),axis=1)\n#Grabing back our new descriptions\ndf = pd.concat([df,empty_desc],axis=0)","98907c27":"from sklearn.feature_extraction.text import TfidfVectorizer #The Vector creator\nfrom sklearn.metrics.pairwise import linear_kernel #Cosine similarity\nfrom sklearn.cluster import MiniBatchKMeans #Kmeans Clustering Batch","84c46c45":"cluster_content = df['description']\nvector = TfidfVectorizer(max_df=0.5,min_df=1,stop_words=\"english\",lowercase=True,use_idf=True,norm=u'l2',smooth_idf=True)\ntfidf = vector.fit_transform(cluster_content)","797c5b15":"k = 200\nkmeans = MiniBatchKMeans(n_clusters = k)\nkmeans.fit(tfidf)\ncenters = kmeans.cluster_centers_.argsort()[:,::-1]\nterms = vector.get_feature_names()","baa43ff6":"request_transform = vector.transform(df['description'])\ndf['cluster'] = kmeans.predict(request_transform)\ndf['cluster'].value_counts().head()","cd27dcaa":"df.head()","96a0036b":"def find_similar(matrix,index,top_n=5):\n    cosine_similarities = linear_kernel(matrix[index:index+1],matrix).flatten()\n    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n    return [index for index in related_docs_indices][0:top_n]","8416c723":"G = nx.Graph(label=\"Article\")\nstart_time = time.time()\nfor i,rowi in df.iterrows() :\n    if (i > 3000) :\n        continue\n    if (i%1000 == 0) : \n        print(\"Iter  {} --- {} secondes --\".format(i,time.time()-start_time))\n    G.add_node(rowi['title'],key=i,label=\"Article\")\n    G.add_node(rowi['author'],label=\"Person\")\n    G.add_edge(rowi['title'],rowi['author'],label=\"Wrote\")\n    G.add_node(rowi['source_name'],label=\"Press\")\n    G.add_edge(rowi['title'],rowi['source_name'],label=\"CAT\")\n    #Similarity Node :\n    indices = find_similar(tfidf, i, top_n = 5)\n    snode=\"Sim(\"+rowi['title'][:15].strip()+\")\"        \n    G.add_node(snode,label=\"SIMILAR\")\n    G.add_edge(rowi['title'], snode, label=\"SIMILARITY\")\n    for element in indices:\n        G.add_edge(snode, df['title'].iloc[element], label=\"SIMILARITY\")\nprint(\" finish -- {} seconds --\".format(time.time() - start_time))   ","1eb78542":"def get_all_adj_nodes(list_in):\n    sub_graph=set()\n    for m in list_in:\n        sub_graph.add(m)\n        for e in G.neighbors(m):        \n                sub_graph.add(e)\n    return list(sub_graph)\n\ndef draw_sub_graph(sub_graph):\n    subgraph = G.subgraph(sub_graph)\n    colors=[]\n    for e in subgraph.nodes():\n        if G.nodes[e]['label']==\"Article\":\n            colors.append('blue')\n        elif G.nodes[e]['label']==\"Person\":\n            colors.append('red')\n        elif G.nodes[e]['label']==\"Press\":\n            colors.append('green')\n        elif G.nodes[e]['label']==\"SIMILAR\":\n            colors.append('yellow')\n\n\n    nx.draw(subgraph, with_labels=True, font_weight='bold',node_color=colors)\n    plt.show()","e7c29385":"list_in=[df['title'].loc[1],df['title'].loc[2]]\nsub_graph = get_all_adj_nodes(list_in)\ndraw_sub_graph(sub_graph)","ad4ea3ae":"def get_recommendation(root):\n    commons_dict = {}\n    for e in G.neighbors(root):\n        for e2 in G.neighbors(e):\n            if e2==root:\n                continue\n            try :\n                if G.nodes[e2]['label']==\"Article\":\n                    commons = commons_dict.get(e2)\n                    if commons==None:\n                        commons_dict.update({e2 : [e]})\n                    else:\n                        commons.append(e)\n                        commons_dict.update({e2 : commons})\n            except :\n                pass\n    articles=[]\n    weight=[]\n    for key, values in commons_dict.items():\n        w=0.0\n        for e in values:\n            w=w+1\/math.log(G.degree(e))\n        articles.append(key) \n        weight.append(w)\n    \n    result = pd.Series(data=np.array(weight),index=articles)\n    result.sort_values(inplace=True,ascending=False)        \n    return result;","c99c9fe3":"result = get_recommendation(df['title'].loc[40])\nprint(\"*\"*40+\"\\n Recommendation for :\"+str(df['title'].loc[40])+\"\\n\"+\"*\"*40)\nprint(result.head())","a8e1a7d2":"**This notebook is inspired from the work of Mr.Yann Claudel from his notebook in Netflix movie , great notebook i recommend highly :**\n[https:\/\/www.kaggle.com\/yclaudel\/recommendation-engine-with-networkx](http:\/\/)","5b0803b1":"I think i won't be using this clustering column because it's so unbalanced","5b34698e":"As you can see 18 of them are without contents , these 18 we'll be droping them cause we cant do something to fix them if we don't even know the content of this article","bcf4e1a2":"#### Nodes Are : \n\n* Title \n* Person (Author)\n* Press (Source_name)\n* Cluster ( Description ) \n* Sim\n\n#### Edges are :\n\n* Wrote : relation between title and person\n* CAT : Relation between title and Press\n* Description : Relation between cluster and a movie\n* Similarity in sense of description","e80a9201":"# In this notebook i'll try to create a recommendation engine using Kmeans with TF-IDF and networkx (For graphs)","4e9d7db9":"# **Now let's Create our graph**","f58f8104":"We have 24 rows without description we'll need to clean that :","755d4354":"## Checking our graph with two exemples ( Here only two so we can visualize it )","9b5f7a70":"## *In this work we're creating a Recommendation engine , we won't be interested in all columns , like the last ones about facebook sharing and stuff we dont need that , the first thing to do is creating a tf-idf clustering by description*","f3f4a199":"### Adding our Data (Descritpions) and predict their classes :","a701cc70":"## Now we're Going to use Cosine Similarity to compute the similarity between docs","6c2b2bea":"## Now we have our description values well clained it's time to start our Kmeans using TF IDF","3272aba6":"## For the last 6 empty descriptions we're gonna use summarize from gensim library to summarize the content and save it as a description","001fc773":"## The next function is going to get the neighbors nodes in our graph and compute the weight (like degree of similarity according to the  graph ) then we're going to sort the neighbors by this weight value ","27a0586d":"## Function to draw Our graph , no need to understand the details just a general idea , u can copie it and use it on other projects but change the nodes labels","a81efbd5":"# **Importation**","5e706a12":"As we can see the two columns Source_id and source_name are almost the same , we'll be using only source name and there is a value in source name equals to 460 which is weird let's check the rows containing that value"}}