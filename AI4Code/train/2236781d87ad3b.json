{"cell_type":{"05ba39ca":"code","3f6364cb":"code","a201eee5":"code","66d0d3ce":"code","361ece16":"code","b4e9d6d1":"code","b6a75e60":"code","f91a804e":"code","1e9769b0":"code","3dceb815":"code","fbf67116":"code","20f6c8b7":"code","0e1230eb":"code","c6ce759c":"code","69b8766e":"code","a08fff8e":"code","6a737919":"code","c8da8488":"code","924e70de":"code","16ddc8be":"code","e4b7864e":"code","66c6f31f":"code","ce74460a":"code","9c580131":"code","0294197f":"code","1b6ba5b7":"code","5d0729ba":"code","33a0f171":"code","8f80898d":"code","75a2396c":"code","1141e2d7":"code","ddcfc594":"code","e2cea6db":"code","d9457564":"code","7ea58b50":"code","74d22965":"code","bcfbeea5":"code","4d9e7d01":"code","ac3dfb55":"code","414ce660":"code","c654382e":"code","af1b4ae3":"code","16d9ee1a":"code","7444fa8e":"code","8b4269d1":"code","950e2cbd":"code","29241f1b":"code","627ffa3c":"code","b02ec816":"code","bcab1f4f":"code","a776e9a5":"code","b8611f00":"code","526ab1c6":"code","9f1aeb32":"code","c48ad2d4":"code","4a0a3c05":"markdown","d0c354d8":"markdown","42f3a18c":"markdown","6b388188":"markdown","c5bb8c31":"markdown","adf328c3":"markdown","cc1d7721":"markdown","48f13a76":"markdown","ff377115":"markdown","eb1fa3db":"markdown","f494fffa":"markdown","bcc74331":"markdown","3b24d6e1":"markdown","cca951fe":"markdown","3bf2fb2b":"markdown","81529a34":"markdown","65f4cc2c":"markdown","b3a166b7":"markdown","442a3e5b":"markdown","b46adb04":"markdown","329d9bcb":"markdown","77517a74":"markdown","233e2d25":"markdown","c74a0360":"markdown"},"source":{"05ba39ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3f6364cb":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nimport pickle\nfrom numpy import hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport random, os\nfrom pytorch_transformers import AdamW, WarmupLinearSchedule\nfrom tqdm import tqdm, trange\nfrom torch import nn\nimport torch\nimport numpy\nfrom sklearn.preprocessing import LabelEncoder\nfrom string import digits\nfrom stemming.porter2 import stem\nfrom gensim import corpora, models, similarities\nimport nltk\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","a201eee5":"import warnings\nwarnings.filterwarnings('ignore')","66d0d3ce":"def seed_torch(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    numpy.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\nseed_torch(0)","361ece16":"lda = None\ndictionary = None","b4e9d6d1":"vocab = pickle.load(open('\/kaggle\/input\/localdata\/vocab.pickle', 'rb'))\nvectorizer = CountVectorizer(vocabulary=vocab)\nlabels = pickle.load(open('\/kaggle\/input\/localdata\/labels.pickle', 'rb'))","b6a75e60":"def load_LDA():\n    lda = models.LdaModel.load('\/kaggle\/input\/ldafiles\/all_papers_model_sample.lda')\n    dictionary = corpora.dictionary.Dictionary.load_from_text('\/kaggle\/input\/ldafiles\/all_papers_text.dict')\n    return lda, dictionary\n\ndef stemall(documents):\n    remove_digits = str.maketrans('', '', digits)\n    return ([ [ stem(word) for word in line.translate(remove_digits).lower().split(\" \")] for line in documents ])\n\ndef get_features_from_LDA(corpus):\n    lda_features = []\n    stem_docs = stemall(corpus)\n    for doc in stem_docs:\n        doc_lda = lda.get_document_topics(dictionary.doc2bow(doc))\n        res = numpy.array(list(zip(*doc_lda))[1])\n        lda_features.append(res)\n        \n    features_df = pd.DataFrame(lda_features).fillna(0)\n    return features_df.to_numpy()","f91a804e":"def pipeline(model, data, vectorizer, train=True):\n    global lda, dictionary\n    \n    # sentece2features\n    corpus = data['Sentence'].tolist()\n    \n    if lda == None:\n        lda, dictionary = load_LDA()\n    # count based feature\n    vec_corpus = vectorizer.transform(corpus).todense()\n    \n    # LDA + vec_corpus\n    lda_features = get_features_from_LDA(corpus)\n    print(vec_corpus.shape, lda_features.shape)\n    vec_corpus = hstack([vec_corpus, lda_features])\n    \n    predict_proba = model.predict_proba(vec_corpus)\n    return predict_proba","1e9769b0":"trained_models = list()\nmodel_location = '\/kaggle\/input\/non-neural-network-model'\nnames = ['b_clf', 'rf_clf', 'et_clf', 'lr_clf', 'dt_clf', 'xgb_clf', 'meta_clf']\nfor m in names:\n    m_ = pickle.load(open('{}\/{}.bin'.format(model_location, m), 'rb'))\n    trained_models.append(m_)","3dceb815":"def get_prediction_from_ensemble(trained_models, vectorizer, df):\n    predictions = list()\n    for m in trained_models[:-1]:\n        m_prediction = pipeline(m, df, vectorizer, train=False)\n        predictions.append(m_prediction)\n\n    meta_model = trained_models[-1]\n    X_meta = hstack(predictions)\n    y_pred = meta_model.predict(X_meta)\n    return y_pred","fbf67116":"df_val_adj = pickle.load(open('\/kaggle\/input\/localdata\/val_ajudicated.pickle', 'rb'))\ntrue_val_adj = df_val_adj['Label']\ny_pred_val_adj = get_prediction_from_ensemble(trained_models, vectorizer, df_val_adj)\nacc = accuracy_score(true_val_adj, y_pred_val_adj)\nprint('Accuracy : %.3f' % (acc*100))","20f6c8b7":"class SciBertClassification(torch.nn.Module):\n    def __init__(self, base_model, nb_classes, tokenizer):\n        super(SciBertClassification, self).__init__()\n        \n        self.scibert = base_model\n        self.output = torch.nn.Linear(768, nb_classes)\n        self.tokenizer = tokenizer\n       \n    def forward(self, text_sentences):\n        representation = [] \n        for sentence in text_sentences:\n            input_ids = torch.tensor(self.tokenizer.encode(sentence)).unsqueeze(0)\n            outputs = self.scibert(input_ids)\n            last_hidden_states = outputs[0][:, -1, :]\n            output_logits = self.output(last_hidden_states)\n            representation.append(output_logits)\n            \n        b_logits = torch.cat(representation, dim=0)\n        return b_logits","0e1230eb":"import torch\nfrom torch.utils import data\n\nclass Dataset_scibert(data.Dataset):\n  def __init__(self, sentences, labels):\n        self.labels = labels\n        self.sentences = sentences\n\n  def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.sentences)\n\n  def __getitem__(self, index):\n        X = self.sentences[index]\n        y = self.labels[index]\n        \n        return X, y","c6ce759c":"model_version = 'scibert_scivocab_uncased'\ndo_lower_case = True\nBert = BertModel.from_pretrained('\/kaggle\/input\/scibert-scivocab-uncased\/')\ntokenizer = BertTokenizer.from_pretrained('\/kaggle\/input\/scibert-scivocab-uncased', do_lower_case=do_lower_case)","69b8766e":"encoder = LabelEncoder()\nencoder.fit(labels)","a08fff8e":"model = SciBertClassification(Bert, len(encoder.classes_), tokenizer)\nmodel.load_state_dict(torch.load('\/kaggle\/input\/fine-tuned-scibert-clf\/model.bin', map_location=torch.device('cpu')))\nmodel.eval()\nprint('Model loaded')","6a737919":"def predict(model, prediction_dataloader):\n    model.eval()\n    sentences = {}\n    counter = 0\n    predictions , true_labels, indices = [], [], []\n    for batch in prediction_dataloader:\n        b_sentences, b_labels = batch\n        \n        for s in b_sentences:\n            sentences[counter] = s\n            counter = counter + 1\n            \n        with torch.no_grad():\n            logits = model(b_sentences)\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        predictions.append(logits)\n        true_labels.append(label_ids)\n        \n    predictions = [item for sublist in predictions for item in sublist]\n    true_labels = [item for sublist in true_labels for item in sublist]    \n    sentences = [sentences[i] for i in range(len(sentences))]\n    return predictions, true_labels, sentences","c8da8488":"val_adj = pickle.load(open('\/kaggle\/input\/localdata\/val_ajudicated.pickle','rb'))\nval_adj.head()\nprint(val_adj.Label.unique())","924e70de":"params = {'batch_size': 64,\n          'shuffle': True,\n          'num_workers': 6}\n\nvalidation_adj_set = Dataset_scibert(val_adj['Sentence'], encoder.transform(val_adj['Label']))\nvalidation_adj_generator = data.DataLoader(validation_adj_set, **params)\n#evaluate(model, validation_adj_generator, encoder)","16ddc8be":"nn_predictions_val_adj, true_labels_val_adj, val_adj_sentences = predict(model, validation_adj_generator)\ntrue_labels_text_val_adj = [encoder.classes_[i] for i in true_labels_val_adj]\nnn_predictions_val_adj_text_label = [encoder.classes_[numpy.argmax(p)] for p in nn_predictions_val_adj]","e4b7864e":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncnf = confusion_matrix(true_labels_text_val_adj, nn_predictions_val_adj_text_label, labels=val_adj['Label'].unique().tolist())\na = numpy.around(cnf)\nprint(a)\nax = sns.heatmap(a, annot=True,fmt = '.0f')\nax.set_xticklabels(labels=val_adj['Label'].unique().tolist(), rotation=90) #, ylabel=encoder.classes_)\nax.set_yticklabels(labels=val_adj['Label'].unique().tolist(), rotation=0) #, ylabel=encoder.classes_)\nax.set_ylim(len(val_adj['Label'].unique().tolist())+0.5, -0.5)","66c6f31f":"print(classification_report(true_labels_text_val_adj, nn_predictions_val_adj_text_label))","ce74460a":"import numpy as np\nimport pandas as pd\nimport pickle\nfrom subprocess import check_output\nimport os\n\n# bokeh packages\n#To create intractive plot we need this to add callback method.\n#This is for creating layout\n#from bokeh.models import CustomJS \n#from bokeh.layouts import column\n#from bokeh.io import output_file, show, output_notebook, push_notebook\n#from bokeh.plotting import *\n#from bokeh.models import ColumnDataSource, Div, Select, Button, ColorBar, CustomJS, DataTable, DateFormatter, TableColumn\n#from bokeh.layouts import row,column,gridplot,widgetbox, layout\n#from bokeh.models.widgets import Tabs,Panel\n#from bokeh.transform import cumsum, linear_cmap\n#from bokeh.palettes import Blues8\n#from bokeh.io import  output_notebook, show\n#from bokeh.models import ColumnDataSource\n#from bokeh.palettes import Spectral6\n#from bokeh.plotting import figure\n\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nfrom ipywidgets import interactive\nimport plotly.graph_objects as go\nimport os\n\n#output_notebook()\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_colwidth', 0)","9c580131":"def make_clickable_multi(url, name):\n    return '<a href=\"http:\/\/doi.org\/{}\" target=\"_blank\">{}<\/a>'.format(url,name)\n\ndef temp(title):\n    return title","0294197f":"data_dir=\"\/kaggle\/input\/localdata\"\nfilename = os.path.join(data_dir,'jan_feb_march_scibert_predictions.csv')\nbase_df = pd.read_csv(filename)\n\nbase_df['year']= pd.to_datetime(base_df['year'], errors='coerce')\nbase_df['year'] = base_df['year'].dt.year\n\ndf = base_df[['title','authors','sentence', 'year', 'source_x', 'journal',  \n       'predicted_label', 'doi']]\n\n\ndf['sentence']=df['sentence'].str.replace('`', '', regex=True)\ndf=df.replace(to_replace= r'\\\\', value= '', regex=True)\ndf['journal']= df['journal'].replace(np.nan, '', regex=True)\n\ndf = df[df['predicted_label'] != 'Irrevlant Sentence']\n\ndf = df[['title','authors','sentence', 'year', 'source_x', 'journal',  \n       'predicted_label', 'doi']]\n\ndf['url'] = ['http:\/\/doi.org\/'+str(i) for i in df['doi'].values.tolist()]\ndf = df[['title','authors','sentence', 'year', 'source_x', 'journal',  \n       'predicted_label', 'url']]","1b6ba5b7":"df['predicted_label'].value_counts()","5d0729ba":"\n\"\"\"\nCopyright 2019, Marek Cermak\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\ndef init_datatable_mode():\n    \"\"\"Initialize DataTable mode for pandas DataFrame represenation.\"\"\"\n    import pandas as pd\n    from IPython.core.display import display, Javascript\n\n    # configure path to the datatables library using requireJS\n    # that way the library will become globally available\n    display(Javascript(\"\"\"\n        require.config({\n            paths: {\n                DT: '\/\/cdn.datatables.net\/1.10.19\/js\/jquery.dataTables.min',\n            }\n        });\n\n        $('head').append('<link rel=\"stylesheet\" type=\"text\/css\" href=\"\/\/cdn.datatables.net\/1.10.19\/css\/jquery.dataTables.min.css\">');\n    \"\"\"))\n\n    def _repr_datatable_(self):\n        \"\"\"Return DataTable representation of pandas DataFrame.\"\"\"\n        # classes for dataframe table (optional)\n        classes = ['table', 'table-striped', 'table-bordered']\n\n        # create table DOM\n        script = (\n            f'$(element).html(`{self.to_html(index=False, classes=classes)}`);\\n'\n        )\n\n        # execute jQuery to turn table into DataTable\n        script += \"\"\"\n            require([\"DT\"], function(DT) {\n                $(document).ready( () => {\n                    \/\/ Turn existing table into datatable\n                    $(element).find(\"table.dataframe\").DataTable();\n                })\n            });\n        \"\"\"\n\n        return script\n\n    pd.DataFrame._repr_javascript_ = _repr_datatable_","33a0f171":"def get_publications(df):\n    df_paper = df.copy(deep=True)\n    df_paper = df_paper[['title','authors','year','source_x','journal', 'url']]\n    df_paper.drop_duplicates(inplace=True)\n    df_paper['Clinical Topic(s)'] =pd.Series(['NA' for _ in range(len(df_paper))])\n    groups = df.groupby(['title']).groups\n    for k,v in groups.items():\n        try:\n            lb = df.iloc[v.tolist()]['predicted_label'].unique().tolist()\n            topics =  ', '.join(x for x in lb)\n            df_paper.at[df_paper['title']==k,'Clinical Topic(s)'] = topics\n        except:\n            continue\n            \n    return df_paper\n\ndef compute_pub_stats(df):\n    # Bar Chart with 8 classes (count papers, sentences)\n    labels = df.predicted_label.unique().tolist()\n    stat={}\n    for lb in labels:\n        stat[lb]={'sent_count':df[df.predicted_label==lb].shape[0],'paper_count':df[df.predicted_label==lb]['title'].unique().shape[0]}\n    return stat","8f80898d":"def show_publication_stats(stats):\n    \n    labels = list(stats.keys())\n#     print(labels)\n\n   \n    sentences = [stats[s]['sent_count'] for s in labels]\n    papers = [stats[s]['paper_count'] for s in labels]\n    fig = go.Figure()\n    fig.add_trace(go.Bar(x=labels,\n                    y=sentences,\n                    name='# Sentences',\n#                     marker_color='rgb(55, 83, 109)'\n                    ))\n#     fig.add_trace(go.Bar(x=labels,\n#                     y=papers,\n#                     name='# Papers',\n# #                     marker_color='rgb(26, 118, 255)'\n#                     ))\n\n    fig.update_layout(\n        title='Overall Sentences Statistics',\n        xaxis_tickfont_size=14,\n        yaxis=dict(\n            title='Counts',\n            titlefont_size=16,\n            tickfont_size=14,\n        ),\n        legend=dict(\n            x=0,\n            y=1.0,\n#             bgcolor='rgba(255, 255, 255, 0)',\n            bordercolor='rgba(255, 255, 255, 0)'\n        ),\n        barmode='group',\n        bargap=0.15, # gap between bars of adjacent location coordinates.\n        bargroupgap=0.1 # gap between bars of the same location coordinate.\n    )\n    fig.show()","75a2396c":"def show_paper_counts(stats):\n    \n    labels = list(stats.keys())\n   \n    #title=''Overall Paper Statistics'\n   \n    sentences = [stats[s]['sent_count'] for s in labels]\n    papers = [stats[s]['paper_count'] for s in labels]\n    fig = go.Figure()\n#     fig.add_trace(go.Bar(x=labels,\n#                     y=sentences,\n#                     name='# Sentences',\n# #                     marker_color='rgb(55, 83, 109)'\n#                     ))\n    fig.add_trace(go.Bar(x=labels,\n                    y=papers,\n                    name='# Papers',\n                    marker_color='indianred'\n                    ))\n\n    fig.update_layout(\n        title='Overall Paper Statistics',\n        xaxis_tickfont_size=10,\n        yaxis=dict(\n            title='Number of Papers',\n            titlefont_size=10,\n            tickfont_size=10,\n        ),\n        legend=dict(\n            x=0.0,\n            y=1.0,\n            bgcolor='rgba(255, 255, 255, 0)',\n            bordercolor='rgba(255, 255, 255, 0)'\n        ),\n        barmode='group',\n        bargap=0.15, # gap between bars of adjacent location coordinates.\n        bargroupgap=0.1 # gap between bars of the same location coordinate.\n    )\n    fig.show()\n    \n    \n    '''\n    #title='Overall Sentences Statistics',\n\n    labels = list(stats.keys())\n#     print(labels)\n\n   \n    sentences = [stats[s]['sent_count'] for s in labels]\n    papers = [stats[s]['paper_count'] for s in labels]\n    fig = go.Figure()\n    fig.add_trace(go.Bar(x=labels,\n                    y=sentences,\n                    name='# Sentences',\n                    marker_color='rgb(55, 83, 109)'\n                    ))\n\n    #fig.add_trace(go.Bar(x=labels,\n    #                 y2=papers,\n    #                 name='# Papers',\n    #                 marker_color='rgb(26, 118, 255)'\n    #                 ))\n        fig.update_layout(\n        title='Overall Sentences Statistics',\n        xaxis_tickfont_size=14,\n        yaxis=dict(\n            title='Counts',\n            titlefont_size=16,\n            tickfont_size=14,\n        ),\n        legend=dict(\n            x=0,\n            y=1.0,\n            #bgcolor='rgba(255, 255, 255, 0)',\n            bordercolor='rgba(255, 255, 255, 0)'\n        ),\n        barmode='group',\n        bargap=0.15, # gap between bars of adjacent location coordinates.\n        bargroupgap=0.1 # gap between bars of the same location coordinate.\n    )\n    fig.show()\n    '''","1141e2d7":"def get_filtered_values(df_paper):\n    labels = [(x,x) for x in df_paper['Clinical Topic(s)'].unique()]\n    labels.insert(0,('All','All'))\n    publish_year_min= min(df_paper.year.unique().tolist())\n    publish_year_max= max(df_paper.year.unique().tolist())\n\n    #publish_year.insert(0,('All','All'))\n\n    #Source\n    source = [(x,x) for x in df_paper.source_x.unique()]\n    source.insert(0,('All','All'))\n    #country\n    #country = [(x,x) for x in df_paper.country.unique()]\n    return labels,publish_year_min, publish_year_max,source\n","ddcfc594":"def get_filtered_values_details(df_paper):\n    labels = [(x,x) for x in df_paper['journal'].unique()]\n    labels.insert(0,('All','All'))\n    publish_year_min= min(df_paper.year.unique().tolist())\n    publish_year_max= max(df_paper.year.unique().tolist())\n\n    #publish_year.insert(0,('All','All'))\n\n    #Source\n    source = [(x,x) for x in df_paper.source_x.unique()]\n    source.insert(0,('All','All'))\n\n    #country\n    journal = [(x,x) for x in df_paper.journal.unique()]\n    return journal,publish_year_min, publish_year_max,source\n","e2cea6db":"def filter_publication(Topic,Year,Source):\n    if Source=='All':\n        tmp_df = pub_df\n    else:\n        tmp_df = pub_df[pub_df.source_x==Source]\n\n    if Topic=='All':\n        if Year=='All':\n            return tmp_df\n        else:\n            return tmp_df[(tmp_df.year==Year)]\n    else:\n        if Year=='All':\n            return tmp_df[(tmp_df['Clinical Topic(s)']==Topic)]\n        else:\n            return tmp_df[((tmp_df['Clinical Topic(s)']==Topic)&(tmp_df.year==Year))]","d9457564":"def filter_publication_details(Topic,Journal,Year,Source):\n    \n    tmp_df = df[df['predicted_label']==Topic] \n    \n    if Source=='All':\n        pass\n    else:\n        tmp_df = tmp_df[tmp_df.source_x==Source]\n\n\n    if Topic=='All':\n        if Year=='All':\n            return tmp_df\n        else:\n            return tmp_df[(tmp_df.year==Year)]\n    else:\n        if Year=='All':\n            return tmp_df[(tmp_df['journal']==Journal)]\n        else:\n            return tmp_df[((tmp_df['journal']==Journal)&(tmp_df.year==Year))]","7ea58b50":"def filter_df(label,pyear,source):\n    if journal!=None:\n        tmp_df = df[(df.source_x==source)]\n    else:\n        tmp_df=df\n        \n    return \n    if label=='All':\n        if pyear=='All':\n            return tmp_df\n        else:\n            return tmp_df[(tmp_df.year==pyear)]\n    else:\n        if pyear=='All':\n            return tmp_df[(tmp_df.predicted_label==label)]\n        else:\n            return tmp_df[((tmp_df.predicted_label==label)&(tmp_df.year==pyear))]","74d22965":"def display_label_wise_data(label,pyear,source):\n    \n    tmp_df = df[((df.source_x==source)&(df.predicted_label==label))]\n    \n    if pyear=='All':\n        tmp_df = tmp_df[(tmp_df.predicted_label==label)]\n#         tmp_df =tmp_df[['sentence','title','authors','country','year','journal','source_x',]]\n#         return tmp_df\n    else:\n        tmp_df = tmp_df[((tmp_df.predicted_label==label)&(tmp_df.year==pyear))]\n    tmp_df =tmp_df[['sentence','title','authors','url','year','journal','source_x',]]\n    return tmp_df\n","bcfbeea5":"def show_wordcloud(df, annot):\n    df['sentence'] =  df['sentence'].astype(str)\n    text = df[df['predicted_label']==annot]['sentence'].values \n    if len(text) < 1:\n        print('No sentence found to form word cloud.')\n        return \n    \n    plt.figure(figsize=(8,8))\n    wordcloud = WordCloud().generate(str(text))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","4d9e7d01":"#init_datatable_mode()\npub_df = get_publications(df)\nstats = compute_pub_stats(df)\nlabels= stats.keys()","ac3dfb55":"labels = list(stats.keys())\n   \nsentences = [stats[s]['sent_count'] for s in labels]\npapers = [stats[s]['paper_count'] for s in labels]\n\ntmp_s = pd.DataFrame({'labels': labels, 'sentences': sentences})\ntmp_p = pd.DataFrame({'labels': labels, 'papers': papers})","414ce660":"ax = tmp_s.plot.bar(x='labels', y='sentences', rot=90)","c654382e":"#show_paper_counts(stats)\nax = tmp_p.plot.bar(x='labels', y='papers', rot=90)","af1b4ae3":"pd.set_option('display.max_rows', 100)","16d9ee1a":"labels,publish_year_min, publish_year_max,source = get_filtered_values(pub_df)\nwidget=interact(filter_publication, df=pub_df, Topic=labels,Year=(publish_year_min, publish_year_max,1),Source=source)","7444fa8e":"show_wordcloud(df, 'Death')","8b4269d1":"#labels,publish_year_min, publish_year_max,source = get_filtered_values(pub_df)\njournal,publish_year_min, publish_year_max,source = get_filtered_values_details(df)\nwidget=interact(filter_publication_details, df=df, Topic=[('Death', 'Death')],Journal=journal, Year=(publish_year_min, publish_year_max,1),Source=source)","950e2cbd":"show_wordcloud(df, 'CriticallyIll')","29241f1b":"#labels,publish_year_min, publish_year_max,source = get_filtered_values(pub_df)\njournal,publish_year_min, publish_year_max,source = get_filtered_values_details(df)\nwidget=interact(filter_publication_details, df=df, Topic=[('CriticallyIll', 'CriticallyIll')],Journal=journal, Year=(publish_year_min, publish_year_max,1),Source=source)","627ffa3c":"show_wordcloud(df, 'Extrapulmonary_Manifestations')","b02ec816":"journal,publish_year_min, publish_year_max,source = get_filtered_values_details(df)\nwidget=interact(filter_publication_details, df=df, Topic=[('Extrapulmonary_Manifestations', 'Extrapulmonary_Manifestations')],Journal=journal, Year=(publish_year_min, publish_year_max,1),Source=source)","bcab1f4f":"show_wordcloud(df, 'Recovery')","a776e9a5":"journal,publish_year_min, publish_year_max,source = get_filtered_values_details(df)\nwidget=interact(filter_publication_details, df=df, Topic=[('Recovery', 'Recovery')],Journal=journal, Year=(publish_year_min, publish_year_max,1),Source=source)","b8611f00":"show_wordcloud(df, 'Organ Damage and Needs Specialist')","526ab1c6":"journal,publish_year_min, publish_year_max,source = get_filtered_values_details(df)\nwidget=interact(filter_publication_details, df=df, Topic=[('Organ Damage and Needs Specialist', 'Organ Damage and Needs Specialist')],Journal=journal, Year=(publish_year_min, publish_year_max,1),Source=source)","9f1aeb32":"show_wordcloud(df, 'MedicalJourney')","c48ad2d4":"journal,publish_year_min, publish_year_max,source = get_filtered_values_details(df)\nwidget=interact(filter_publication_details, df=df, Topic=[('MedicalJourney', 'MedicalJourney')],Journal=journal, Year=(publish_year_min, publish_year_max,1),Source=source)","4a0a3c05":"## Non Neural Ensemble Evaluation","d0c354d8":"# Results Highlight","42f3a18c":"## List of Identified Publications Relevant to Medical Care Topics","6b388188":"### Publication Level","c5bb8c31":"<a class=\"anchor\" id=\"organ\"><\/a>\n## [Organ Damage and Needs Specialist](#organ)\n\n* **Keywords** : multiple system organ failure,multiple organ failure,widespread systemic inflammatory response,lung injury\n\n* **Description** : This set of identified sentences are aiming to extract relevant information to organ damage. The goal is to help clinicians and clinical researcher learn what the anticipated organ damage is; is there a need for surgical or procedural intervention like the need for pacemaker insertion, neurological assessment.\n\n[Go to Top](#categories)","adf328c3":"def evaluate(model, prediction_dataloader, encoder):\n    \n    val_predictions, true_labels, _ = predict(model, prediction_dataloader)\n    \n    y_true = [encoder.classes_[i] for i in true_labels]\n    y_pred = [encoder.classes_[numpy.argmax(p)] for p in val_predictions]\n    \n    acc = accuracy_score(y_true, y_pred)\n    print('Accuracy : %.3f' % (acc*100))","cc1d7721":"## Evaluation on Dataset","48f13a76":"<a class=\"anchor\" id=\"recovery\"><\/a>\n## [Recovery](#recovery)\n\n**Keywords** : recovery, better, improved, discharged from ICU\n\n**Description** : This set of identified sentences are aiming to extract relevant information to recovery. The goal is to help clinicians, and clinical researchers learn what the expected timelines and clinical course of recovery, important correlating factors for recovery are, and what are the chances of a patient recovering. This information can help with resource and transfer planning \u2013 for example, understanding the occupancy of ICU beds or ventilators and for how long\n\n[Go to Top](#categories)","ff377115":"<a class=\"anchor\" id=\"em\"><\/a>\n## [Extrapulmonary Manifestations](#em)\n\n**Keywords** : Cardiogenic shock, Cardiac Arrhythmias, Septic shock, Sepsis, Atelectasis\n\n**Description** : This set of identified sentences are aiming to extract relevant information about extrapulmonary manifestations. The goal is to help clinicians, and clinical researchers learn what the anticipated extra pulmonary critical manifestations like is - metabolic encephalopathy, sepsis, cardiac arrhythmias, shock; is there a need for surgical or procedural intervention like the need for a pacemaker insertion, neurological assessment, and others.\n\n\n[Go to Top](#categories)","eb1fa3db":"## [Death](#death)\n<a class=\"anchor\" id=\"death\"><\/a>\n\n**Keywords** : death, deceased, expired, demise, passed away, mortality\n\n**Description** : This set of identified sentences are aiming to extract relevant information about the death. The goal is to help clinicians and clinical researchers learn what the expected timelines are and clinical course, and chances of a patient dying.\n\n\n[Go to Top](#categories)","f494fffa":"<a class=\"anchor\" id=\"ill\"><\/a>\n## [Critically Ill](#ill)\n\n**Keywords** : critical, critical illness, critically ill, acute, serious, life-threatening, grave, dire\n\n**Description** : This set of identified sentences are aiming to extract relevant information to critical illness. The goal is to help clinicians and clinical researchers learn the expected timelines and clinical course of deterioration, as well as what are the chances of a patient remaining critically ill are. This information can help with resource and transfer planning \u2013 for example, understanding the patients' potential dependence on ICU beds or ventilators and for how long.\n\n\n[Go to Top](#categories)","bcc74331":"## [Neural Network - SciBERT](#approach_classifier)","3b24d6e1":"### WordCloud for Death","cca951fe":"# [Implementation](#approach_classifier): Supervised Training Using Non Neural Ensemble and Neural Classification","3bf2fb2b":"# Loading SciBERT Classification Predictions","81529a34":"# [Introduction](#introduction)\n<a class=\"anchor\" id=\"introduction\"><\/a>\n\nThis work is the response to the **COVID-19 Open Research Dataset Challenge (CORD-19)**. The overall goal of the challenge is to facilitate knowledge extraction from the large CORD-19  dataset of over 51,000 scholarly articles, including over 40,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. \n\n<a class=\"anchor\" id=\"scope\"><\/a>\n# [Scope Of Our Contribution](#scope) \n\nAs the **COVID-19** pandemic is becoming the number one concern in the world, there is an urgent need to learn from published literature about COVID-19 related medical care and its effects. \u000bAt the same time, there is a rapid growth of new coronavirus literature, making it difficult for clinicians and medical research community to keep up and effectively extract relevant information. \nThis work aims to support medical research community by facilitating the search about medical care and its effect through the literature in the **CORD-19 Research Dataset**.\n\n<a class=\"anchor\" id=\"approach\"><\/a>\n# [Approach](#approach)\n\n![arch](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/606007\/1086635\/architecture.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587655330&Signature=fFbgnBd1rg2vgt5nj66lA1DTTgVmZ%2Bt5t265UoSnb8ZaZLrM9EeBossMlxXtgqdrGcas5ZlIr8mUhnCVLp6FfoOh2BgQcKmfmky%2BgbIp8oCjas8OIsvqZx%2Bz4hoxrYewKOg%2BFQg6P6HtXDhX0krZc%2Bk8MHek4k4f9A7ZpmbZfGn3KB6D%2BucLvdpewZWKUl6T4EBw%2FaZo3Q%2Fl0IB8GoaBd5XqkAmWyCZGcnBL%2FEfbV7xxqqBCvQnYzk8BJEibQWYbs%2Ft%2FsuqpvZjJNxiew%2Bn1YvZmn4YSdHgEjVC3hNOCqRocB7nCfxVUqsbVql31Q4bC0pXC6Kk8o8jhz3CngQhT6A%3D%3D)\n\nAs our primary goal is to support clinicians and medical research community, we decided to start our work from that very perspective. In total, our approach has 5 main steps, as listed below:\n<a class=\"anchor\" id=\"categories\"><\/a>\n1. **Identify a set of highly relevant information a clinician or medical researcher would like to find in the CORD-19 literature.** Our subject matter expert with clinical background identified nine most relevant topics (and likely associated keywords) we believe clinician or medical researcher would like to find information about in the literature. Those topics and associated keywords are listed below:\n\n    - [Death](#death) (death, deceased, expired, demise, passed away, mortality)\n    - [Critically Ill](#ill) (critical, critical illness, critically ill, acute, serious, life-threatening, grave, dire)\n    - [Extrapulmonary Manifestations](#em) (Cardiogenic shock, Cardiac Arrhythmias, Septic shock, Sepsis, Atelectasis)\n    - [Recovery](#recovery) (recovery, better, improved, discharged from ICU)\n    - [Organ Damage and Needs Specialist](#organ) (multiple system organ failure,multiple organ failure,widespread systemic inflammatory response,lung injury)\n    - [Medical Journey](#mj) (Quick deterioration,Accelerated decline, High mortality, Speedy progression,Dire prognosis)\n    - Aggressive Resuscitation Useful* (Cardiopulmonary resuscitation , CPR , in-hospital CPR , Do not resuscitate). \n    - End of Life* (Advance directives, End of life care, Comfort care, Patient comatose)\n    - Potential Medications* (dopamine, dobutamine, adrenaline, epinephrine, digoxin)\n   \n*Note: In this submission, there are no results identified for this category from the papers published in Jan, Feb, March 2020. \n\n    \n2. **Create a small annotated dataset** \u2013 Based on the topics and keywords identified by the SME, we created a small annotated dataset by extracting few thousands sentences using keyword search and manually labeling them as **\u201crelevant\u201d** or **\u201cnot relevant\u201d** to the relevant abovementioned topics. \n\n<a class=\"anchor\" id=\"approach_classifier\"><\/a>\n\n3. [**Create a classifier**](#approach_classifier) \u2013 Using the annotated dataset from the previous step, we created a classifier capable to identify relevant sentences for each of the topics.\n\n    - Build topic model from 4Million+ sentences in the above mentioned corpus\n    - Bagging, Random forest, Logistic Regression, Decision Tree and XGBC classifier based ensemble classifier based on annotated data.\n    - Features: Bag-of-words + Topic distributions\n    - NNBased classifier (Scibert embeddings based)\n    - Train an ensemble classifier with all the classifiers.\n\n    * Experimental setup\n        - Dataset statistcs as numbers\n\n        - Training (steps to be added)\n        - Testing (steps to be added)\n       \n    * Results and Analysis\n \n4. **Apply the classifier** - in this submission we applied our classifier to publications made in Jan, Feb, and March 2020.\n\n5. **Visualize** - Create simple and easy to use interactive visualization from charts and searchable tables. \n\n**** Currently Kaggle submission does not show interactive tables, please run the code in order to see them. **\n\n\n","65f4cc2c":"# Result Details","b3a166b7":"## Loading model fine-tuned on COVID-19 Medical Litrature Classification Task ","442a3e5b":"## Loading Required Library ","b46adb04":"## Data Preparation for SciBERT","329d9bcb":"## Implementation Data Visualization","77517a74":"## Loading Pre-trained SciBERT","233e2d25":"<a class=\"anchor\" id=\"mj\"><\/a>\n## [Medical Journey](#mj)\n\n**Keywords** : Quick deterioration,Accelerated decline, High mortality, Speedy progression,Dire prognosis\n\n**Description** : This set of identified sentences are aiming to extract relevant information regarding medical journey. The goal is to help clinicians, and clinical researchers understand the medical journey.\n\n[Go to Top](#categories)","c74a0360":"# Error Analysis (Confusion Matrix)"}}