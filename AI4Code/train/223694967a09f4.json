{"cell_type":{"fcba33ca":"code","156b8fd6":"code","66dcfa24":"code","bb9800c4":"code","9a21b004":"code","75b17547":"code","768cc14a":"code","9af3f19c":"code","3ecba2cd":"code","b679623b":"code","7126b854":"code","f968672f":"code","ca06125a":"code","dbbd332c":"code","057232be":"code","cb90ef2a":"code","cf3ccc45":"markdown","f5ea89bd":"markdown","46fed8a0":"markdown","cbf0dcb3":"markdown","d994de4d":"markdown"},"source":{"fcba33ca":"NUM_EXAMPLES = 256\nBATCH_SIZE = 8\nEPOCHS = 50 # actually steps\nLR = 0.1\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nrc('animation', html='html5')\nplt.style.use('seaborn-whitegrid')\n\n\n# Define model\nclass Model(object):\n  def __init__(self, w_init=-1.0, b_init=-1.0):\n    self.W = tf.Variable(w_init)\n    self.b = tf.Variable(b_init)\n\n  def __call__(self, x):\n    return self.W * x + self.b\n\ndef loss(target_y, predicted_y):\n  return tf.reduce_mean(tf.square(target_y - predicted_y))\n\ndef train(model, inputs, outputs, learning_rate):\n  with tf.GradientTape() as t:\n    current_loss = loss(outputs, model(inputs))\n  dW, db = t.gradient(current_loss, [model.W, model.b])\n  model.W.assign_sub(learning_rate * dW)\n  model.b.assign_sub(learning_rate * db)\n\n# Data\nTRUE_W = 3.0\nTRUE_b = 2.0\nSEED = 3141\n\ninputs  = tf.random.normal(shape=[NUM_EXAMPLES], seed=SEED)\nnoise   = tf.random.normal(shape=[NUM_EXAMPLES], seed=SEED+1)\noutputs = inputs * TRUE_W + TRUE_b + noise\n\nds = (tf.data.Dataset\n      .from_tensor_slices((inputs, outputs))\n      .shuffle(1000, seed=SEED)\n      .batch(BATCH_SIZE)\n      .repeat())\nds = iter(ds)\n\nmodel = Model()\n\n\n\n# Collect the history of W-values and b-values to plot later\nWs, bs, xs, ys, ls = [], [], [], [], []\n\nfig = plt.figure(dpi=100, figsize=(8, 3))\n\n# Regression Line\nax1 = fig.add_subplot(131)\nax1.set_title(\"Fitted Line\")\nax1.set_xlabel(\"x\")\nax1.set_ylabel(\"y\")\nax1.set_xlim(-3, 2.5)\nax1.set_ylim(-8, 11)\np10, = ax1.plot(inputs, outputs, 'r.', alpha=0.1) # full dataset\np11, = ax1.plot([], [], 'C3.') # batch, color Red\np12, = ax1.plot([], [], 'k') # fitted line, color Black\n\n# Loss\nax2 = fig.add_subplot(132)\nax2.set_title(\"Training Loss\")\nax2.set_xlabel(\"Batches Seen\")\nax2.set_xlim(0, EPOCHS)\nax2.set_ylim(0, 40)\np20, = ax2.plot([], [], 'C0') # color Blue\n\n# Weights\nax3 = fig.add_subplot(133)\nax3.set_title(\"Weights\")\nax3.set_xlabel(\"Batches Seen\")\nax3.set_xlim(0, EPOCHS)\nax3.set_ylim(-2, 4)\nax3.plot(range(EPOCHS), [TRUE_W for _ in range(EPOCHS)], 'C5--')\nax3.plot(range(EPOCHS), [TRUE_b for _ in range(EPOCHS)], 'C8--')\np30, = ax3.plot([], [], 'C5') # W color Brown\np30.set_label('W')\np31, = ax3.plot([], [], 'C8') # b color Green\np31.set_label('b')\nax3.legend()\n\nfig.tight_layout()\n\ndef init():\n    return [p10]\n\ndef update(epoch):\n  x, y = next(ds)\n  y_pred = model(x)\n  current_loss = loss(y, y_pred)\n\n  Ws.append(model.W.numpy())\n  bs.append(model.b.numpy())\n  xs.append(x.numpy())\n  ys.append(y_pred.numpy())\n  ls.append(current_loss.numpy())\n  p11.set_data(x.numpy(), y.numpy())\n  inputs = tf.linspace(-3.0, 2.5, 30)\n  p12.set_data(inputs, Ws[-1]*inputs + bs[-1])\n  p20.set_data(range(epoch), ls)\n  p30.set_data(range(epoch), Ws)\n  p31.set_data(range(epoch), bs)\n  \n    \n  train(model, x, y, learning_rate=LR)\n#   print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n#         (epoch, Ws[-1], bs[-1], current_loss))\n\n  return p11, p12, p20\n\nani = animation.FuncAnimation(fig, update, frames=range(1, EPOCHS), init_func=init, blit=True, interval=100)\nplt.close()\nani\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v] fps=12,scale=480:-1,split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","156b8fd6":"from tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nrc('animation', html='html5')\nplt.style.use('seaborn-whitegrid')\n\n# NUM_EXAMPLES = 256\n# BATCH_SIZE = 8\n# STEPS = 50 # actually steps\n# LR = 0.1\n\n\ndef animate_sgd(num_examples, batch_size, steps, learning_rate,\n                true_w=3.0, true_b=2.0, seed=0):\n    # Define model\n    class Model(object):\n        def __init__(self, w_init=-1.0, b_init=-1.0):\n            self.W = tf.Variable(w_init)\n            self.b = tf.Variable(b_init)\n\n        def __call__(self, x):\n            return self.W * x + self.b\n            \n    def loss(target_y, predicted_y):\n        return tf.reduce_mean(tf.square(target_y - predicted_y))\n\n    def train(model, inputs, outputs, learning_rate):\n        with tf.GradientTape() as t:\n            current_loss = loss(outputs, model(inputs))\n            dW, db = t.gradient(current_loss, [model.W, model.b])\n            model.W.assign_sub(learning_rate * dW)\n            model.b.assign_sub(learning_rate * db)\n    # Data\n    inputs  = tf.random.normal(shape=[num_examples], seed=seed)\n    noise   = tf.random.normal(shape=[num_examples], seed=seed+1)\n    outputs = inputs * true_w + true_b + noise\n    ds = (tf.data.Dataset\n          .from_tensor_slices((inputs, outputs))\n          .shuffle(1000, seed=seed)\n          .batch(batch_size)\n          .repeat())\n    ds = iter(ds)\n    model = Model()\n    # Collect the history of W-values and b-values to plot later\n    Ws, bs, xs, ys, ls = [], [], [], [], []\n    # Construct plot\n    fig = plt.figure(dpi=100, figsize=(8, 3))\n\n    # Regression Line\n    ax1 = fig.add_subplot(131)\n    ax1.set_title(\"Fitted Line\")\n    ax1.set_xlabel(\"x\")\n    ax1.set_ylabel(\"y\")\n    ax1.set_xlim(-3, 2.5)\n    ax1.set_ylim(-8, 11)\n    p10, = ax1.plot(inputs, outputs, 'r.', alpha=0.1) # full dataset\n    p11, = ax1.plot([], [], 'C3.') # batch, color Red\n    p12, = ax1.plot([], [], 'k') # fitted line, color Black\n\n    # Loss\n    ax2 = fig.add_subplot(132)\n    ax2.set_title(\"Training Loss\")\n    ax2.set_xlabel(\"Batches Seen\")\n    ax2.set_xlim(0, steps)\n    ax2.set_ylim(0, 40)\n    p20, = ax2.plot([], [], 'C0') # color Blue\n\n    # Weights\n    ax3 = fig.add_subplot(133)\n    ax3.set_title(\"Weights\")\n    ax3.set_xlabel(\"Batches Seen\")\n    ax3.set_xlim(0, steps)     # \n    ax3.set_ylim(-2, 4)\n    ax3.plot(range(steps), [true_w for _ in range(steps)], 'C5--')\n    ax3.plot(range(steps), [true_b for _ in range(steps)], 'C8--')\n    p30, = ax3.plot([], [], 'C5') # W color Brown\n    p30.set_label('W')\n    p31, = ax3.plot([], [], 'C8') # b color Green\n    p31.set_label('b')\n    ax3.legend()\n\n    fig.tight_layout()\n\n    def init():\n        return [p10]\n\n    def update(epoch):\n        x, y = next(ds)\n        y_pred = model(x)\n        current_loss = loss(y, y_pred)\n          \n        Ws.append(model.W.numpy())\n        bs.append(model.b.numpy())\n        xs.append(x.numpy())\n        ys.append(y_pred.numpy())\n        ls.append(current_loss.numpy())\n        p11.set_data(x.numpy(), y.numpy())\n        inputs = tf.linspace(-3.0, 2.5, 30)\n        p12.set_data(inputs, Ws[-1]*inputs + bs[-1])\n        p20.set_data(range(epoch), ls)\n        p30.set_data(range(epoch), Ws)\n        p31.set_data(range(epoch), bs)\n\n        train(model, x, y, learning_rate=learning_rate)\n        #   print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n        #         (epoch, Ws[-1], bs[-1], current_loss))\n        \n        return p11, p12, p20\n\n    ani = animation.FuncAnimation(\n        fig,\n        update,\n        frames=range(1, steps),\n        init_func=init,\n        blit=True,\n        interval=100,\n    )\n    plt.close()\n    return ani\n    \n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v] fps=12,scale=480:-1,split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","66dcfa24":"animate_sgd(\n    num_examples=8192,\n    batch_size=4096,\n    steps=50,\n    learning_rate=0.99,\n)","bb9800c4":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nrc('animation', html='html5')\nplt.style.use('seaborn-whitegrid')\n\n# Parameters\nTRUE_W0 = 3.0\nTRUE_W1 = 2.0\nTRUE_b = 1.0\nNUM_EXAMPLES = 1024\nBATCH_SIZE = 64\nEPOCHS = 150 # actually steps\nLR = 0.005\nSEED = 3141\n\n# Define Model\nmodel = keras.Sequential([\n    layers.Dense(8, activation='relu'),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(1)\n])\nmodel.compile(\n    optimizer=keras.optimizers.SGD(LR),\n    loss='mse',\n)\n\n\n# Data\n\ninputs  = tf.random.normal(shape=(NUM_EXAMPLES,1), seed=SEED)\nnoise   = tf.random.normal(shape=(NUM_EXAMPLES,1), seed=SEED+1)\n# outputs = TRUE_W0 * inputs ** 2 + TRUE_W1 * inputs + TRUE_b + noise\noutputs = TRUE_W0 * inputs ** 2 + noise\noutputs = tf.squeeze(outputs)\n\nds = (tf.data.Dataset\n      .from_tensor_slices((inputs, outputs))\n      .repeat()\n      .shuffle(1000, seed=SEED)\n      .batch(BATCH_SIZE))\nds = iter(ds)\n\n\n# Collect the history of W-values and b-values to plot later\nWs, bs, xs, ys, ls = [], [], [], [], []\n\n# Create Figure\nfig = plt.figure(dpi=150, figsize=(8, 3))\n\n# Regression Curve\nax1 = fig.add_subplot(121)\nax1.set_title(\"Fitted Curve\")\nax1.set_xlabel(\"x\")\nax1.set_ylabel(\"y\")\nax1.set_xlim(-3, 3)\nax1.set_ylim(-5, 20)\np10, = ax1.plot(inputs, outputs, 'r.', alpha=0.1) # full dataset\np11, = ax1.plot([], [], 'C3.') # batch\np12, = ax1.plot([], [], 'k') # fitted line\n\n# Loss\nax2 = fig.add_subplot(122)\nax2.set_title(\"Training Loss\")\nax2.set_xlabel(\"Batches Seen\")\nax2.set_xlim(0, EPOCHS)\nax2.set_ylim(0, 40)\np20, = ax2.plot([], [], 'C0')\n\n\nfig.tight_layout()\n\ndef init():\n    return [p10]\n\ndef update(epoch):\n  x, y = next(ds)\n  y_pred = model(x)\n  current_loss = model.evaluate(x, y)\n  x = tf.squeeze(x)\n  y = tf.squeeze(y)\n  y_pred = tf.squeeze(y_pred)\n    \n  xs.append(x.numpy())\n  ys.append(y_pred.numpy())\n  ls.append(current_loss)\n  p11.set_data(x.numpy(), y.numpy())\n  inputs = tf.linspace(-3.0, 3.0, 30)\n  p12.set_data(inputs, model.predict(inputs))\n  p20.set_data(range(epoch), ls)\n  \n  model.train_on_batch(x, y)\n\n  return p11, p12, p20\n\nani = animation.FuncAnimation(fig, update, frames=range(1, EPOCHS), init_func=init, blit=True, interval=100)\nplt.close()\nani\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","9a21b004":"import numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nrc('animation', html='html5')\nplt.style.use('seaborn-whitegrid')\n\n\ndef animate_curve_fitting(model,\n                             X, y,\n                             batch_size=64,\n                             epochs=16,\n                             lr=0.005,\n                             shuffle_buffer=5000,\n                             seed=0,\n                             verbose=1):\n    num_examples = X.shape[0]\n    steps_per_epoch = num_examples \/\/ batch_size\n    total_steps = steps_per_epoch * epochs\n    \n    ds = (tf.data.Dataset\n          .from_tensor_slices((X, y))\n          .repeat()\n          .cache()\n          .shuffle(shuffle_buffer, seed=seed)\n          .batch(batch_size))\n    ds_iter = ds.as_numpy_iterator()\n\n    x_min = X.min()\n    x_max = X.max()\n    X_pop = np.linspace(x_min, x_max, 1000)\n    y_min = y.min()\n    y_max = y.max()\n\n    # Parameters\n    xs = []\n    ys = []\n    curves = []\n    # Callback to save parameters\n    def save_params(batch, logs):\n        x, y = next(ds_iter)\n        xs.append(x.squeeze())\n        ys.append(y.squeeze())\n        curve = model.predict(X_pop)\n        curves.append(curve)\n\n    save_params_cb = keras.callbacks.LambdaCallback(\n        on_batch_begin=save_params,\n    )\n\n    # Train model to collect parameters\n    model.fit(\n        ds,\n        epochs=epochs,\n        callbacks=[save_params_cb],\n        steps_per_epoch=steps_per_epoch,\n        verbose=verbose,\n    )\n\n    # Create Figure\n    fig = plt.figure(dpi=150, figsize=(4, 3))\n    # Regression Curve\n    ax1 = fig.add_subplot(111)\n    ax1.set_title(\"Fitted Curve\")\n    ax1.set_xlabel(\"x\")\n    ax1.set_ylabel(\"y\")\n    ax1.set_xlim(x_min, x_max)\n    ax1.set_ylim(y_min, y_max)\n    p10, = ax1.plot(X, y, 'r.', alpha=0.1) # full dataset\n    p11, = ax1.plot([], [], 'C3.') # batch\n    p12, = ax1.plot([], [], 'k') # fitted line\n    # Complete Figure\n    fig.tight_layout()\n\n    def init():\n        return [p10]\n\n    def update(frame):\n        x = xs[frame]\n        y = ys[frame]\n        p11.set_data(x, y)\n        p12.set_data(X_pop, curves[frame])\n        return p11, p12\n\n    ani = \\\n        animation.FuncAnimation(\n            fig,\n            update,\n            frames=range(1, total_steps),\n            init_func=init,\n            blit=True,\n            interval=100,\n        )\n    plt.close()\n\n    return ani","75b17547":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(8),\n    layers.Activation('relu'),\n    layers.Dense(16),\n    layers.Activation('relu'),\n    layers.Dense(8),\n    layers.Activation('relu'),\n    layers.Dense(1)\n])\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=2,\n    decay_rate=0.96,\n    staircase=False,\n)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr_schedule),\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/adam-exp-lr.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","768cc14a":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(8),\n    layers.Activation('relu'),\n    layers.Dense(16),\n    layers.Activation('relu'),\n    layers.Dense(8),\n    layers.Activation('relu'),\n    layers.Dense(1)\n])\nmodel.compile(\n    optimizer=keras.optimizers.Adam(0.01),\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/adam.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","9af3f19c":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(8),\n    layers.Activation('relu'),\n    layers.Dense(16),\n    layers.Activation('relu'),\n    layers.Dense(8),\n    layers.Activation('relu'),\n    layers.Dense(1)\n])\nmodel.compile(\n    optimizer='sgd',\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/sgd.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","3ecba2cd":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(8),\n    layers.Activation('elu'),\n    layers.Dense(16),\n    layers.Activation('elu'),\n    layers.Dense(8),\n    layers.Activation('elu'),\n    layers.Dense(1)\n])\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=2,\n    decay_rate=0.96,\n    staircase=False,\n)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr_schedule),\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/adam-elu.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","b679623b":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(8),\n    layers.Activation('swish'),\n    layers.Dense(16),\n    layers.Activation('swish'),\n    layers.Dense(8),\n    layers.Activation('swish'),\n    layers.Dense(1)\n])\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=2,\n    decay_rate=0.96,\n    staircase=False,\n)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr_schedule),\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/adam-swish.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","7126b854":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(8),\n    layers.Activation('tanh'),\n    layers.Dense(16),\n    layers.Activation('tanh'),\n    layers.Dense(8),\n    layers.Activation('tanh'),\n    layers.Dense(1)\n])\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=2,\n    decay_rate=0.96,\n    staircase=False,\n)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr_schedule),\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/adam-tanh.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","f968672f":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(8),\n    layers.Activation('relu'),\n    layers.Dense(16),\n    layers.Activation('relu'),\n    layers.Dense(8),\n    layers.Activation('relu'),\n    layers.Dense(1)\n])\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=2,\n    decay_rate=0.96,\n    staircase=False,\n)\nmodel.compile(\n    optimizer=keras.optimizers.RMSprop(lr_schedule),\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/rmsprop-exp-lr.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","ca06125a":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(1024),\n    layers.Activation('relu'),\n    layers.Dense(1024),\n    layers.Activation('relu'),\n    layers.Dense(1024),\n    layers.Activation('relu'),\n    layers.Dense(1)\n])\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=2,\n    decay_rate=0.96,\n    staircase=False,\n)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(0.01),\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/adam-big.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","dbbd332c":"X = np.random.normal(loc=0.0, scale=1.0, size=256)\nerr = np.random.normal(loc=0.0, scale=1.0, size=256)\ny = 2 * np.square(X) + err\n\nmodel = keras.Sequential([\n    layers.Dense(256),\n    layers.Activation('relu'),\n    layers.Dropout(0.5),\n    layers.Dense(512),\n    layers.Activation('relu'),\n    layers.Dropout(0.5),\n    layers.Dense(256),\n    layers.Activation('relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1)\n])\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=2,\n    decay_rate=0.96,\n    staircase=False,\n)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr_schedule),\n    loss='mse',\n)\n\nani = animate_curve_fitting(model, X, y, batch_size=32, epochs=32, verbose=0)\nplt.close()\nani.save('\/kaggle\/working\/adam-dropout.mp4')\nani\n\n# Convert mp4 to gif:\n# ffmpeg -i temp.mp4 -filter_complex \"[0:v],split [a][b];[a] palettegen [p];[b][p] paletteuse\" temp.gif","057232be":"def true_model(W, b):\n    def outputs(inputs):\n        err = tf.random.normal(shape=inputs.shape)\n#         err = 0.0\n        return tf.squeeze(inputs * W + b + err)\n    return outputs\n\ndef loss_grid(X, y, w_space, b_space):\n  def bias_augment(X):\n    \"\"\"Augment matrix X with bias column.\"\"\"\n    aug = tf.constant([0]*X.shape[0] + [1],\n                      shape=(X.shape[0]+1, 1),\n                      dtype=tf.float32)\n    X = tf.concat([X, tf.zeros(shape=(1, X.shape[1]))],\n                  axis=0)\n    X = tf.concat([X, aug], axis=1)\n    return X\n\n  W = \\\n    tf.expand_dims(\n      tf.stack(\n        tf.meshgrid(w_space, b_space)),\n      axis=1)\n  Xb = bias_augment(X)\n  WX = tf.einsum(\"mnij,bm->bnij\", W, Xb)\n  b = WX[-1]\n  WX = tf.squeeze(WX[:-1])\n  y_pred = WX + b\n\n  batch_size = X.shape[0]\n  loss = \\\n    tf.reduce_mean(tf.square(\n      tf.reshape(y, (batch_size, 1, 1)) - y_pred\n    ), axis=0)\n\n  return loss\n\ndef plot_loss_surface(w_space, b_space, loss, levels=tf.square(range(0, 8)), dpi=150):\n  fig = plt.figure(dpi=150)\n  ax = fig.add_subplot(111)\n  ax.set_xlim(b_MIN, b_MAX)\n  ax.set_ylim(W_MIN, W_MAX)\n  ax.set_aspect('equal')\n  ax.set_xlabel('Bias')\n  ax.set_ylabel('Weight')\n  ax.set_title('The Loss Surface')\n  levels = tf.square(range(0, 8))\n  CS = ax.contour(w_space, b_space, loss,\n                  levels=levels,\n                  cmap='gray')\n  ax.clabel(CS, inline=True, fontsize=8, fmt=\"%1d\")\n  artists, _ = CS.legend_elements()\n  return artists\n\n\n\n# ## Define Linear Model ##\n\n# Define model\nclass Model(object):\n  def __init__(self, w_init=0.0, b_init=0.0):\n    self.W = tf.Variable(tf.reshape(w_init, shape=(1,)), dtype=tf.float32)\n    self.b = tf.Variable(tf.reshape(b_init, shape=(1,)), dtype=tf.float32)\n\n  def __call__(self, x):\n    return self.W * x + self.b\n\ndef loss_fn(target_y, predicted_y):\n    return tf.reduce_mean(tf.square(target_y - predicted_y))\n\ndef train(model, inputs, outputs, learning_rate):\n  with tf.GradientTape() as t:\n    current_loss = loss_fn(outputs, model(inputs))\n    dW, db = t.gradient(current_loss, [model.W, model.b])\n    model.W.assign_sub(learning_rate * dW)\n    model.b.assign_sub(learning_rate * db)\n\n\n# ## Data ##\n# +\nTRUE_W = 3.0\nTRUE_b = 2.0\nNUM_EXAMPLES = 32\nBATCH_SIZE = 4\nLEARNING_RATE = 0.05\n\nW_MIN = -3.0\nW_MAX = 8.0\nb_MIN = -3.0\nb_MAX = 8.0\n\nX = tf.random.normal(shape=[NUM_EXAMPLES, 1])\ny = tf.squeeze(true_model(TRUE_W, TRUE_b)(X))\nw_space = tf.linspace(W_MIN, W_MAX, 128)\nb_space = tf.linspace(b_MIN, b_MAX, 128)\n\nloss = loss_grid(X, y, w_space, b_space)\n\n# ## Data Pipeline ##\ninputs = tf.squeeze(X)\noutputs = true_model(TRUE_W, TRUE_b)(inputs)\nds = (tf.data.Dataset\n      .from_tensor_slices((inputs, outputs))\n      .shuffle(1000)\n      .batch(BATCH_SIZE)\n      .repeat())\nds = iter(ds)\n\nmodel = Model(w_init=-1.0, b_init=-1.0)\n\n# Empty containers for values to save\nWs, bs, xs, ys, ls = [], [], [], [], []\n\nfig = plt.figure(dpi=150)\nax = fig.add_subplot(111)\nax.set_xlim(b_MIN, b_MAX)\nax.set_ylim(W_MIN, W_MAX)\nax.set_aspect('equal')\nax.set_xlabel('Bias')\nax.set_ylabel('Weight')\nax.set_title('The Loss Surface')\nlevels = tf.square(range(0, 8))\nCS = ax.contour(w_space, b_space, loss,\n                levels=levels,\n                cmap='bone')\nax.clabel(CS, inline=True, fontsize=8, fmt=\"%1d\")\n\np1, = plt.plot([TRUE_W], [TRUE_b], 'kx')\np2, = plt.plot([], [], color='red', alpha=0.5)\np3, = plt.plot([], [], 'r.')\n\ndef init():\n    return p1,\n\ndef update(epoch):\n  x, y = next(ds)\n  y_pred = model(x)\n  current_loss = loss_fn(y, y_pred)\n\n  Ws.append(model.W.numpy())\n  bs.append(model.b.numpy())\n  xs.append(x.numpy())\n  ys.append(y_pred.numpy())\n  ls.append(current_loss.numpy())\n  p2.set_data(Ws, bs)\n  p3.set_data(Ws[-1], bs[-1])\n\n  train(model, x, y, learning_rate=0.05)\n#   print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n#         (epoch, Ws[-1], bs[-1], current_loss))\n\n  return p3,\n\nani = animation.FuncAnimation(fig, update, frames=range(1, 128), interval=100, init_func=init, blit=True)\nplt.close()\nani","cb90ef2a":"def make_random_vector(vc):\n  stddevs = [tf.math.reduce_std(w) for w in vc]\n  v0 = [tf.random.normal(shape=w.shape, stddev=1.0)\n         for w, s in zip(vc, stddevs)]\n  v1 = [tf.random.normal(shape=w.shape, stddev=1.0)\n         for w, s in zip(vc, stddevs)]\n  def coord_vec(x, y):\n    return [x * w0 + y * w1 + wc\n            for w0, w1, wc in zip(v0, v1, vc)]\n  return coord_vec\n\n\ndef make_random_loss_grid(x_coord, y_coord, model, inputs, outputs):\n  vc = model.get_weights()\n  coord_vec = make_random_vector(vc)\n\n  loss_grid = np.empty([len(x_coord), len(y_coord)])\n  for i, x in enumerate(x_coord):\n    for j, y in enumerate(y_coord):\n      weights = coord_vec(x, y)\n      model.set_weights(weights)\n      loss = model.test_on_batch(inputs, outputs, return_dict=True)['loss']\n      loss_grid[i, j] = loss\n\n  model.set_weights(vc)\n\n  return loss_grid\n\n\n# # Data #\n# #+\nTRUE_W = 3.0\nTRUE_b = 2.0\nNUM_EXAMPLES = 1024\nBATCH_SIZE = 64\nx = tf.random.normal(shape=(NUM_EXAMPLES, 1))\ny = TRUE_W * x + TRUE_b + tf.random.normal(shape=x.shape, stddev=0.1)\ny = tf.squeeze(y)\nds = (tf.data.Dataset\n      .from_tensor_slices((x, y))\n      .repeat()\n      .shuffle(1000)\n      .batch(BATCH_SIZE))\n\n# Model\nmodel = keras.Sequential([\n  layers.Dense(32, activation='relu'),\n  layers.Dense(32, activation='relu'),\n  layers.Dense(1),\n])\nmodel.compile(\n  loss='mse',\n  optimizer='sgd',\n)\nmodel.fit(\n    ds,\n    steps_per_epoch=NUM_EXAMPLES\/\/BATCH_SIZE,\n    epochs=100,\n    verbose=0,\n)\n\n\n# # Loss Grid #\n\n# Create Loss Grid\nSIZE = NUM_EXAMPLES\nds_iter = iter(ds.unbatch().batch(SIZE))\ninputs, outputs = next(ds_iter)\n\nZOOM = 0.25\nPOINTS = 24\nNUM_LEVELS = 20\nxs = tf.linspace(-1.0, 1.0, num=POINTS)**3*ZOOM\nys = tf.linspace(-1.0, 1.0, num=POINTS)**3*ZOOM\n\npts = 1.05 ** np.arange(0, -100, -5)\nxs = np.concatenate((-1*pts, np.sort(pts))) * ZOOM\nys = xs\nzs = make_random_loss_grid(xs, ys, model, inputs, outputs)\n\nfig = plt.figure(dpi=150, figsize=(5, 5))\nax = fig.add_subplot(111)\nax.set_title('The Loss Surface')\n# Set Levels\nmin_loss = zs.min()\nmax_loss = zs.max()\nlevels = tf.exp(\n  tf.linspace(tf.math.log(min_loss),\n              tf.math.log(max_loss),\n              num=NUM_LEVELS))\n# Create Contour Plot\nCS = ax.contour(\n  xs, ys, zs,\n  levels=levels,\n  cmap='magma',\n  linewidths=0.75,\n  norm=mpl.colors.LogNorm(vmin=min_loss, vmax=max_loss*2.0)\n)\nax.clabel(CS, inline=True, fontsize=8, fmt=\"%1.2f\");","cf3ccc45":"# Animated Loss Surface #","f5ea89bd":"# Random Slice from Loss Surface #","46fed8a0":"# SGD with a Line #","cbf0dcb3":"## With Function ##","d994de4d":"# SGD with a Quadratic #"}}