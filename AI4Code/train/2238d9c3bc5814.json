{"cell_type":{"0fcef003":"code","eb3774cf":"code","d29ae95c":"code","d66d8023":"code","9c9cec3b":"code","fb6623ff":"code","f6a9abd8":"code","211a2ab7":"code","8860e81d":"code","fde565e6":"code","de2090c0":"code","3b5b6760":"code","ba925986":"code","aa1ab708":"code","c540b56b":"code","2e13d693":"code","dc404e5b":"code","88ec5b46":"code","f466dc4b":"code","a25f316a":"code","2d8b54c0":"code","edde32b6":"code","c0765791":"code","45e92014":"code","aba213b3":"code","c4a1ba67":"code","5e8618f9":"code","143215a5":"code","92cc2694":"code","1da5cb79":"code","b7add8de":"code","c67b8f8e":"code","83a8b82b":"code","f25f5405":"code","f3178005":"code","6a51e971":"code","cdf43bf7":"code","9e7880de":"code","a724bdca":"code","a9e06df2":"code","6f81e584":"markdown","aeffae31":"markdown","1043ae6c":"markdown","0312d199":"markdown","5d0095f8":"markdown","8fd67081":"markdown","59dae5e2":"markdown","2864baed":"markdown","d9e32b60":"markdown"},"source":{"0fcef003":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.express as px\n\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax, skew, kurtosis\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.gofplots import qqplot","eb3774cf":"train = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\", index_col= 'Id')\ntest = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\", index_col = 'Id')\n\nX = pd.concat([train.iloc[:, :-1], test])\ny = train.iloc[:, -1]","d29ae95c":"# just a quick look\n\ntrain.sample(5)","d66d8023":"# personally, interactive plots are my bests, and so, sns.heatmap() not used\n\nimport plotly.express as px\n\npx.imshow(train.corr(),\n         template = 'plotly_white',\n         height = 800, width = 800)","9c9cec3b":"X.drop(['GarageYrBlt','TotRmsAbvGrd','GarageCars'], axis = 1, inplace=True)","fb6623ff":"# to count missing values\n\nna_counts = pd.DataFrame(X.isnull().sum().sort_values(ascending = False), columns = ['na_counts'])\nna_counts['NA%'] = na_counts['na_counts'].apply(lambda x: x\/X.shape[0] * 100).round(2)\nna_counts.head(20)","f6a9abd8":"# the first four will be dropped due to high missing values ratios\n\nX.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis =1, inplace=True)","211a2ab7":"#remove features that does not really contribute to target SalePrice\n\nX.drop(['MoSold', 'YrSold'], axis = 1, inplace=True)","8860e81d":"# to deal with other NAs depending on their types: categorial or numerical\n\n#MSSubClass is nominal\n\nX['MSSubClass'] = X['MSSubClass'].astype('str')","fde565e6":"cat_list = []\nnum_list = []\n\nfor var in X.columns:\n    if X[var].dtype == 'object':\n        cat_list.append(var)\n    else:\n        num_list.append(var)\n\n# higly skewed categorical variables (>95%) are dropped\n\ncat_drop_list = []\n\nfor var in cat_list:\n    ratio = X[var].value_counts().iloc[0]\/X.shape[0] * 100\n    if ratio > 95:\n        cat_drop_list.append(var)\n        \nprint(cat_drop_list)\nX.drop(cat_drop_list,axis = 1,inplace=True)\n\n# for numerical variables that are highly skewed, try to make them Gaussian-like by box-cox transformation\n\nnum_trans_list = []\n\nfor var in num_list:\n    ratio = X[var].value_counts().iloc[0]\/X.shape[0] * 100\n    if ratio > 95:\n        num_trans_list.append(var)\n        \nprint(num_trans_list)\nfor var in num_trans_list:\n    X[var] = boxcox1p(X[var], boxcox_normmax(X[var] + 1))","de2090c0":"# to deal with outliers (that are in the train dataset, do not touch test dataset to avoid data leakage, of course)\n# it is easy to determine the range of outliers with the aid of interactive graphs\n# for a tight layout, plotly.graph_objects is avaialble coz it can create subplots\n\nfor var in num_list:\n    fig = px.box(train, var,\n                width = 400, height = 250)\n    fig.show()","3b5b6760":"train = train.drop(train[train['LotFrontage'] > 200].index)\ntrain = train.drop(train[train['LotArea']> 100000].index)\ntrain = train.drop(train[train['MasVnrArea'] > 1200].index)\ntrain = train.drop(train[train['BsmtUnfSF'] > 2000].index)\ntrain = train.drop(train[train['2ndFlrSF'] > 2000].index)\ntrain = train.drop(train[train['GarageArea'] > 1130].index)\ntrain = train.drop(train[train['WoodDeckSF'] > 600].index)\ntrain = train.drop(train[train['OpenPorchSF'] > 400].index)\ntrain = train.drop(train[train['BsmtFinSF1'] > 3000].index)\ntrain = train.drop(train[train['TotalBsmtSF'] > 4000].index)\ntrain = train.drop(train[train['GrLivArea'] > 4000].index)","ba925986":"# to double-check missing values\n\nna_counts = pd.DataFrame(X.isnull().sum().sort_values(ascending = False), columns = ['na_counts'])\nna_counts['NA%'] = na_counts['na_counts'].apply(lambda x: x\/X.shape[0] * 100).round(2)\n\nna_counts.head(15)","aa1ab708":"# to sort missing data by type\n\nnum_list = []\ncat_list = []\n\nfor var in na_counts.index:\n    if na_counts['na_counts'][var] > 0:\n        if X[var].dtype != 'object':\n            num_list.append(var)\n        else:\n            cat_list.append(var)\n        \nprint(num_list)\nprint('\\t')\nprint(cat_list)","c540b56b":"# for ordinal variables\n# There are some columns which are ordinal by nature, which represents the quality or condition of certain housing features. \n# In this case, we will map the respective strings to a value. The better the quality, the higher the value\n\nordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\n\nord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\nX[ord_col] = X[ord_col].fillna('NA')\n\nfor var in ord_col:\n    X[var] = X[var].map(ordinal_map)\n    \nBsmtFin_col = ['BsmtFinType1','BsmtFinType2']\nX[BsmtFin_col] = X[BsmtFin_col].fillna('NA')\nfor var in BsmtFin_col:\n    X[var] = X[var].map(fintype_map)\n\nX['BsmtExposure'] = X['BsmtExposure'].fillna('NA')\nX['BsmtExposure'] = X['BsmtExposure'].map(expose_map)","2e13d693":"# for numerical variables\n\nneigh_lot = X.groupby('Neighborhood')['LotFrontage'].median().reset_index(name = 'LotFrontage_median') \nneigh_garage = X.groupby('Neighborhood')['GarageArea'].median().reset_index(name = 'GarageArea_median')\n\nfig1 = px.bar(neigh_lot, x = 'Neighborhood', y = 'LotFrontage_median',\n      height = 400, width = 800)\n\nfig2 = px.bar(neigh_garage, x = 'Neighborhood', y = 'GarageArea_median',\n      height = 400, width = 800)\n\nfig1.show()\nfig2.show()","dc404e5b":"# lot frontage and garage area medians are correlated to neighborhood, so fill their nas using respective medians\n\nX['LotFrontage'] = X.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nX['GarageArea'] = X.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.median()))","88ec5b46":"# to fill na using median for the rest of numerical variables\n\nfor var in [\"BsmtHalfBath\", \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\"]:\n    X[var] = X[var].fillna(X[var].median())","f466dc4b":"na_counts = pd.DataFrame(X.isnull().sum().sort_values(ascending = False), columns = ['na_counts'])\nna_counts['NA%'] = na_counts['na_counts'].apply(lambda x: x\/X.shape[0] * 100).round(2)\n\nna_counts.head(12)","a25f316a":"num_list = []\ncat_list = []\n\nfor var in na_counts.index:\n    if na_counts['na_counts'][var] > 0:\n        if X[var].dtype != 'object':\n            num_list.append(var)\n        else:\n            cat_list.append(var)\n        \nprint(num_list)\nprint('\\t')\nprint(cat_list)","2d8b54c0":"# for categorical variables\n\nX['GarageFinish'] = X['GarageFinish'].fillna('NA')\nX['GarageType'] = X['GarageType'].fillna('NA')","edde32b6":"# to fill missing values (categorical variables) with mode\n\ncat_col = ['MasVnrType', 'MSZoning', 'Functional', 'Exterior2nd', 'Exterior1st', 'Electrical', 'SaleType']\n\nfor var in cat_col:\n    X[var] = X[var].fillna(X[var].mode().iloc[0])","c0765791":"na_counts = pd.DataFrame(X.isnull().sum().sort_values(ascending = False), columns = ['na_counts'])\nna_counts['NA%'] = na_counts['na_counts'].apply(lambda x: x\/X.shape[0] * 100).round(2)\n\nna_counts.head(3)\n\n# end of data imputation","45e92014":"# feature engineering (by domain knowledge)\n\nX['TotalLot'] = X['LotFrontage'] + X['LotArea']\nX['TotalBsmtFin'] = X['BsmtFinSF1'] + X['BsmtFinSF2']\nX['TotalSF'] = X['TotalBsmtSF'] + X['2ndFlrSF'] + X['1stFlrSF']\nX['TotalBath'] = X['FullBath'] + X['HalfBath'] * 0.5 + X['BsmtFullBath'] + X['BsmtHalfBath'] * 0.5\nX['TotalPorch'] = X['OpenPorchSF'] + X['EnclosedPorch'] + X['ScreenPorch'] + X['WoodDeckSF']\n\ndrop_list = [\n    'LotFrontage', 'LotArea',\n    'BsmtFinSF1', 'BsmtFinSF2',\n    'TotalBsmtSF', '2ndFlrSF','1stFlrSF',\n    'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath',\n    'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF'\n]\n\nX.drop(drop_list,axis=1, inplace=True)","aba213b3":"# binary columns\n\nbin_col = ['MasVnrArea','TotalPorch','PoolArea', 'GarageQual', 'BsmtQual']\n\nfor var in bin_col:\n    col_name = var + '_bin'\n    X[col_name] = X[var].apply(lambda x: 1 if x > 0 else 0)","c4a1ba67":"X = pd.get_dummies(X)","5e8618f9":"# to check the distribution of the target varaible\n\nqqplot(y,line = 's')\nplt.show()\n\nprint('skewness: %.3f' % (skew(y)))\nprint('kurtosis: %.3f' % (kurtosis(y)))","143215a5":"# the distribution is terribly off\n# it is necessary to make y a bit Gaussian-like\n\nqqplot(np.log1p(y),line = 's')\nplt.show()\n\nprint('skewness: %.3f' % (skew(np.log1p(y))))\nprint('kurtosis: %.3f' % (kurtosis(np.log1p(y))))","92cc2694":"# not that bad\n\ny = y.apply(lambda x: np.log1p(x))","1da5cb79":"# training data preparation\n\nx = X.loc[train.index]\ny = y.loc[train.index]","b7add8de":"# xgboost model\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, RepeatedKFold\nfrom xgboost import XGBRegressor\n\nsteps = list()\nsteps.append(('std', RobustScaler()))\nsteps.append(('norm', MinMaxScaler()))\nsteps.append(('model', XGBRegressor(learning_rate = 0.1,\n                                   max_depth = 6,\n                                   min_child_weight = 10,\n                                   subsample = 0.7,\n                                   colsample_bytree= 1,\n                                   reg_lambda = 1,\n                                   gamma = 0.005,\n                                   reg_alpha = 0.0005,\n                                   n_estimators = 202)))\n\npipe_xgb = Pipeline(steps=steps)\n\ncv = RepeatedKFold(n_repeats=5, n_splits=10,random_state=1)\nstart = time.time()\nscores = cross_val_score(pipe_xgb, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nstop = time.time()\n\nprint('average score: %.4f' % scores.mean())\nprint('STD: %.4f' % scores.std())\nprint('training time: %.2f' % (stop - start))","c67b8f8e":"# lightgbm model\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, RepeatedKFold\nfrom lightgbm import LGBMRegressor\n\nsteps = list()\nsteps.append(('std', RobustScaler()))\nsteps.append(('norm', MinMaxScaler()))\nsteps.append(('model', LGBMRegressor(\n    n_estimators = 1800,\n    min_child_samples = 6,\n    max_depth = 5,\n    learning_rate = 0.01,\n    lambda_l2 = 0.19,\n    lambda_l1 = 0.04,\n    feature_fraction = 0.25\n)))\n\npipe_lgbm = Pipeline(steps=steps)\n\ncv = RepeatedKFold(n_repeats=5, n_splits=10,random_state=1)\nstart = time.time()\nscores = cross_val_score(pipe_lgbm, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nstop = time.time()\n\nprint('average score: %.4f' % scores.mean())\nprint('STD: %.4f' % scores.std())\nprint('training time: %.2f' % (stop - start))","83a8b82b":"# catboost model\n\nfrom catboost import CatBoostRegressor\n\nsteps = list()\nsteps.append(('std', RobustScaler()))\nsteps.append(('norm', MinMaxScaler()))\nsteps.append(('model', CatBoostRegressor())) # the virgin model turned out to be the best one\n\npipe_cat = Pipeline(steps=steps)\n\ncv = RepeatedKFold(n_repeats=5, n_splits=10,random_state=1)\nstart = time.time()\nscores = cross_val_score(pipe_cat, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nstop = time.time()\n\nprint('average score: %.4f' % scores.mean())\nprint('STD: %.4f' % scores.std())\nprint('training time: %.2f' % (stop - start))","f25f5405":"from sklearn.linear_model import ElasticNetCV\n\nsteps = list()\nsteps.append(('std', RobustScaler()))\nsteps.append(('norm', MinMaxScaler()))\nsteps.append(('model', ElasticNetCV(cv = 3)))\n\npipe_en = Pipeline(steps=steps)\n\ncv = RepeatedKFold(n_repeats=5, n_splits=10,random_state=1)\nstart = time.time()\nscores = cross_val_score(pipe_en, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nstop = time.time()\n\nprint('average score: %.4f' % scores.mean())\nprint('STD: %.4f' % scores.std())\nprint('training time: %.2f' % (stop - start))","f3178005":"from sklearn.linear_model import LassoCV\n\nsteps = list()\nsteps.append(('std', RobustScaler()))\nsteps.append(('norm', MinMaxScaler()))\nsteps.append(('model', LassoCV(n_alphas = 207)))\n\npipe_lasso = Pipeline(steps=steps)\n\ncv = RepeatedKFold(n_repeats=5, n_splits=10,random_state=1)\nstart = time.time()\nscores = cross_val_score(pipe_lasso, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nstop = time.time()\n\nprint('average score: %.4f' % scores.mean())\nprint('STD: %.4f' % scores.std())\nprint('training time: %.2f' % (stop - start))","6a51e971":"from sklearn.linear_model import RidgeCV\n\nsteps = list()\nsteps.append(('std', RobustScaler()))\nsteps.append(('norm', MinMaxScaler()))\nsteps.append(('model', RidgeCV(cv=7)))\n\npipe_ridge = Pipeline(steps=steps)\n\ncv = RepeatedKFold(n_repeats=5, n_splits=10,random_state=1)\nstart = time.time()\nscores = cross_val_score(pipe_ridge, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nstop = time.time()\n\nprint('average score: %.4f' % scores.mean())\nprint('STD: %.4f' % scores.std())\nprint('training time: %.2f' % (stop - start))","cdf43bf7":"from sklearn.svm import SVR\n\nsteps = list()\nsteps.append(('std', RobustScaler()))\nsteps.append(('norm', MinMaxScaler()))\nsteps.append(('model', SVR(epsilon=0.001,\n                          gamma=0.028300001,\n                          C=1)))\n\npipe_svr = Pipeline(steps=steps)\n\ncv = RepeatedKFold(n_repeats=5, n_splits=10,random_state=1)\nstart = time.time()\nscores = cross_val_score(pipe_svr, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nstop = time.time()\n\nprint('average score: %.4f' % scores.mean())\nprint('STD: %.4f' % scores.std())\nprint('training time: %.2f' % (stop - start))","9e7880de":"### stacking techqiues\n# stacking solo is not superior to blending\n# time-consuming\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nstack_all = StackingCVRegressor(regressors= (pipe_xgb, pipe_lgbm, pipe_cat,\n                                            pipe_en, pipe_lasso, pipe_svr),\n                               meta_regressor = pipe_lgbm,\n                               use_features_in_secondary=True,\n                               )\nstart = time.time()\nscores = cross_val_score(stack_all, x, y, scoring= 'neg_mean_absolute_error', cv = 3,\n                        n_jobs = -1)\nstop = time.time()\n\nprint('average score: %.4f' % scores.mean())\nprint('STD: %.4f' % scores.std())\nprint('training time: %.2f' % (stop - start))","a724bdca":"# blending techniques: \n\ndef pipe_blend_fit_predict(X, b, c, d, e, f, g, h, i):\n    \n    pipe_xgb.fit(x, y)\n    pipe_lgbm.fit(x, y)\n    pipe_cat.fit(x, y)\n    pipe_en.fit(x,y)\n    pipe_lasso.fit(x,y)\n    pipe_ridge.fit(x,y)\n    pipe_svr.fit(x,y)\n    stack_all.fit(x, y)\n    \n    return ((b * pipe_xgb.predict(X)) + \n            (c * pipe_lgbm.predict(X)) + \n            (d * pipe_cat.predict(X)) + \n            (e * pipe_en.predict(X)) + \n            (f * pipe_lasso.predict(X)) + \n            (g * pipe_ridge.predict(X).flatten()) + \n            (h * pipe_svr.predict(X)) + \n            (i * stack_all.predict(X)))","a9e06df2":"test_df = X.loc[test.index]\n\npred = np.exp(\n    pipe_blend_fit_predict(test_df, 0.1, 0.20, 0.15, 0.1, 0.1, 0.1, 0.1, 0.15)\n) \n\n# blending led to the best result, compared to individual models (not included though)\n# model weights were chosen arbitrarily: empirically, larger weights assigned to more accurate models, such as catboost and stacked models -- higher scores obtianed\n\nsubmission = pd.DataFrame({\n    'Id': test.index,\n    'SalePrice': pred\n})\n\nsubmission.to_csv('submission_zhixx018.csv', index=False)","6f81e584":"My first top1% notebook, given below are notebooks that inspried me a lot. Really appreciate them!\n\nNotebooks reference:\n1. https:\/\/www.kaggle.com\/donaldst\/stackingandensembling\n2. https:\/\/www.kaggle.com\/itslek\/stack-blend-lrs-xgb-lgb-house-prices-k-v17\n3. https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning\n4. https:\/\/www.kaggle.com\/datafan07\/top-1-approach-eda-new-models-and-stacking\n5. https:\/\/www.kaggle.com\/keithfish\/kaggle-7-xgbst-enet\n\n\nHopefully, my notebook can be of help to others as well!\n\nGood luck, Kagglers!","aeffae31":"# 5. to build models and have their hyperparams tuned\n### (optimized by random search cv, not included in this notebook; would love to try Bayesian when skopt v0.9 is realeased)","1043ae6c":"Procedure:\n\n1. to remove unhelpful variables and improve helpful variables\n2. to deal with outlier manually\n3. to perform data imputation\n4. to perform simple feature engineering\n5. to build baseline models and get them tuned\n6. to stack and blend models and output result","0312d199":"# 6. to stack and blend models and output my result","5d0095f8":"Pairs of variables that are highly pearson correlated should be removed to avoid performance loss\n\n1. Yearbuilt and GarageYrBlt\n2. TotalBsmtSF and 1stFlrSF (will be dropped later after simple feature engineering)\n3. GrLivArea and TotRmsAbvGrd\n4. GarageCars and GarageArea","8fd67081":"# 2. to deal with outlier manually","59dae5e2":"# 4. to perform simple feature engineering","2864baed":"# 1. to remove unhelpful variables and improve helpful variables","d9e32b60":"# 3. to perform data imputation"}}