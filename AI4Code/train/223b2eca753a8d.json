{"cell_type":{"213d0a7c":"code","a19ca9ca":"code","df9f76b0":"code","fbdcfd6b":"code","96059882":"code","973199b7":"code","4661dd74":"code","8482aea4":"code","e87e77d3":"code","7ee426cc":"code","51188f25":"code","c8904d13":"markdown","14bc5843":"markdown","51bb9c16":"markdown","d10e50b5":"markdown","bcffc97a":"markdown","e27232f8":"markdown","df30ad6c":"markdown","494c1cb7":"markdown"},"source":{"213d0a7c":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\n\ndf = pd.read_csv('..\/input\/balance-scale\/balance-scale.csv')\n","a19ca9ca":"df = df.rename(columns={'L-Weight': 'Lw', 'L-Distance': 'Ld', 'R-Weight':'Rw', 'R-Distance':'Rd'})\n\ndf_old = df.copy()\n\ndf = df[df['Class'] != 'B']\n\ndf = df.replace({'R': 1, 'L': 0})\n\ndf.head()","df9f76b0":"df.info()","fbdcfd6b":"X = df.drop('Class', axis=1)\ny = df['Class']\n\nX_train, X_test,y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.9, random_state=101)\n\nmodel1 = LogisticRegression()\n\nmodel1.fit(X_train, y_train)\n\ny_pred = model1.predict(X_test)\n\nprint(classification_report(y_test, y_pred, zero_division=1))","96059882":"cm = confusion_matrix(y_test, y_pred)\n\ndisp = ConfusionMatrixDisplay(cm,\n        display_labels = model1.classes_)\ndisp.plot();","973199b7":"df['Lw * Ld'] = df['Lw'] * df['Ld']\ndf['Rw * Rd'] = df['Rw'] * df['Rd']","4661dd74":"X_new = df[['Lw * Ld', 'Rw * Rd']]\ny_new = df['Class']\n\nX_train, X_test,y_train, y_test = train_test_split(X_new, y_new, stratify=y, test_size=0.9, random_state=101)\n\nmodel2 = LogisticRegression()\n\nmodel2.fit(X_train, y_train)\n\ny_pred = model2.predict(X_test)\n\nprint(classification_report(y_test, y_pred, zero_division=1))","8482aea4":"cm = confusion_matrix(y_test, y_pred)\n\ndisp = ConfusionMatrixDisplay(cm,\n        display_labels = model1.classes_)\ndisp.plot();","e87e77d3":"y_pred1 = model1.predict(df[['Lw', 'Ld', 'Rw', 'Rd']])\ny_pred2 = model2.predict(df[['Lw * Ld', 'Rw * Rd']])\n\ndf['No Feature Interaction'] = y_pred1\ndf['Feature Interaction'] = y_pred2","7ee426cc":"df_plot = pd.DataFrame({})\n\ndf_plot['Lw'] = np.arange(0.5, 5.2, 0.1)\ndf_plot['Ld'] = np.arange(0.5, 5.2, 0.1)\ndf_plot['Rw'] = np.arange(0.5, 5.2, 0.1)\ndf_plot['Rd'] = np.arange(0.5, 5.2, 0.1)\ndf_plot['Lw * Ld'] = df_plot['Lw'] * df_plot['Ld']\ndf_plot['Rw * Rd'] = df_plot['Rw'] * df_plot['Rd']\n\ny_pred1 = model1.predict(df_plot[['Lw', 'Ld', 'Rw', 'Rd']])\ny_pred2 = model2.predict(df_plot[['Lw * Ld', 'Rw * Rd']])\n\ndf_plot['No Feature Interaction'] = y_pred1\ndf_plot['Feature Interaction'] = y_pred2","51188f25":"fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nxx, yy = np.meshgrid(df_plot['Lw * Ld'].to_numpy(), df_plot['Rw * Rd'].to_numpy())\n\ncolor = ['#FF0000', '#0000FF']\n\nmodel2.fit(df[['Lw * Ld', 'Rw * Rd']], df['Class'])\n\nZ = model2.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nsns.scatterplot(data=df, x='Lw * Ld', y = 'Rw * Rd',\n                hue = 'No Feature Interaction', ax=ax, s=100, palette=color);\nax.contourf(xx, yy, Z, alpha=0.2, cmap='RdBu');\nax.set_title('Classification not considering Feature Interaction');","c8904d13":"Applying machine learning to this dataset might be just doing a toy model, since it was generated to model psychological experimental results, but it's very insteresting to evaluate how linear models behave when feature interactions are not considered to the analysis.","14bc5843":"We are going to consider a Large Test Set (95%), since this data is sintetically generated with certain rules, making very easy to predict the outcome.","51bb9c16":"The blue dots in the red region are the wrong predictions made by the model with no interactions. The same is true to the red ones. More things can be explored with this dataset. For example, I used only the interactions that are known to be useful to the dataset, but some one might use more interactions and see how they affect the overall performance. Or even more, apply a regularization model with \"L1\" penalty to observe if it will shrink unimportant features (All of them except Rw X Rd and Lw X Ld) to zero. Other suggestion is to see how decision trees work to classify Left or Right.","d10e50b5":"Feature Interactions are often neglected when it comes to linear models. Therefore, some non-linear relationships might not be observed. In this dataset, the ideia behind balance is the [conservation of angular momentum](https:\/\/en.wikipedia.org\/wiki\/Angular_momentum), which is a Feature interaction. If a side of the balance have more momentum (r x F), the balance will tilt towards this side. The relations below classify data as Left, Right or Balance.","bcffc97a":"0.95 seems to be a pretty accurate score, but can we do better? Yes, we can, by considering only the interactions.","e27232f8":"Great! A 100% accurate model with 10% of the dataset! Now, let's see the classification on a decision region plot.","df30ad6c":"For the analysis, we are just considering Left and Right classes, for the sake of (more) simplicity.","494c1cb7":"$$Lw . Ld > Rw . Rd \\rightarrow L$$\n$$Lw . Ld < Rw . Rd \\rightarrow R$$\n$$Lw . Ld = Rw . Rd \\rightarrow B$$"}}