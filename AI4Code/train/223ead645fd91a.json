{"cell_type":{"70a6160e":"code","71300f9a":"code","8aa55a07":"code","12dd1bea":"code","a09a9f89":"code","2d8d6f40":"code","210f723b":"code","d9e037a1":"code","fd1e3879":"code","c5e970c9":"code","3a822888":"code","41418534":"code","f5b9cf0b":"code","1b185d86":"code","7b6be713":"code","c2583190":"code","1ea20c04":"code","a28fc0e3":"code","b1a99740":"code","da7b63ba":"code","b611935e":"code","bcffbe29":"code","3b7c7f3e":"code","e7b31130":"code","75339244":"code","61ee6018":"code","fe66e4c0":"code","c0e958d8":"code","2f56f406":"code","5e831c77":"code","f1d7699e":"markdown","3fb0db0e":"markdown","8672799a":"markdown","297d6cc8":"markdown","3133eab2":"markdown","d669d013":"markdown","30e666ab":"markdown","1939cc7f":"markdown","f588ddec":"markdown","4c135052":"markdown","a607709d":"markdown","039adc32":"markdown","2f5085fd":"markdown","643e0088":"markdown","a6c8278b":"markdown","325fd6ba":"markdown","4bf3cb2c":"markdown","3a4fc9fa":"markdown","df0cbb52":"markdown","e0dbe5f3":"markdown","d7657083":"markdown","2e8e3aa3":"markdown","f7a3620e":"markdown","3854fde2":"markdown","9ea4b29e":"markdown","89118df3":"markdown","f7874690":"markdown","5c4783ed":"markdown","37b7fa8f":"markdown","93102880":"markdown","2076f326":"markdown","3713afb4":"markdown","276482bb":"markdown","2df7211d":"markdown","d2aac9dc":"markdown","41b9eac5":"markdown","5a0b0e40":"markdown","2767d029":"markdown","9a864a70":"markdown","c60e219c":"markdown","f6aeae89":"markdown","b40f2e7f":"markdown","1dbdae03":"markdown","0b6852aa":"markdown","5a591ad5":"markdown"},"source":{"70a6160e":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML models\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Additional libraries related to ML tasks\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_validate, cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.under_sampling import OneSidedSelection\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap","71300f9a":"train = pd.read_csv(\"..\/input\/santander-customer-satisfaction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/santander-customer-satisfaction\/test.csv\")\nsubmission_example = pd.read_csv(\"..\/input\/santander-customer-satisfaction\/sample_submission.csv\")\n\nprint(\"Train dataset:\", len(train), \"rows and\", len(train.columns), \"columns\")\nprint(\"Test dataset:\", len(test), \"rows and\", len(test.columns), \"columns\")\ndisplay(train.head(5))\ndisplay(train.describe())","8aa55a07":"len_train = len(train)\ntarget_0 = len(train.loc[train['TARGET']==0])\/len_train\ntarget_1 = 1-target_0\n\nfig, axes = plt.subplots(1, 2, figsize=(12,5))\n\n# TARGET distribution count\nsns.countplot(x='TARGET', ax=axes[0], data=train, palette='Set2')\naxes[0].set_title('Target count')\n\n# TARGET distribution pie chart\naxes[1].pie([target_0, target_1], colors=['mediumaquamarine', 'coral'], autopct='%1.2f%%', shadow=True, startangle=90, wedgeprops={'alpha':.5})\naxes[1].set_title('Target distribution')\nplt.savefig('target_counts.png')","12dd1bea":"train[['var3','var15','var21','var36','var38']].hist(bins=100, figsize=(10, 8), alpha=0.5)\nplt.savefig('var_columns_all.png')\nplt.show()","a09a9f89":"fig, ax = plt.subplots(1, 3, sharex=False, sharey=False, figsize=(18,4))\n\ntrain.loc[train.var3.between(5, 300), 'var3'].hist(bins=50, range=(5, 300), ax=ax[0], alpha=0.7)\nax[0].set_title(\"Var3 distribution\")\nax[0].set_ylabel(\"Count\")\nax[0].set_xlabel(\"var3\")\nax[0].set_ylim(0,600)\n\ntrain.loc[train.var15.between(5, 105), 'var15'].hist(bins=50, range=(5, 105), ax=ax[1], alpha=0.7)\nax[1].set_title(\"Var15 distribution\")\nax[1].set_xlabel(\"var15\")\nax[1].set_ylim(0,27000)\n\ntrain.var38.hist(bins=100, range=(0, 500000), ax=ax[2], alpha=0.7)\nax[2].set_title(\"Var38 distribution\")\nax[2].set_xlabel(\"var38\")\nax[2].set_ylim(0,18000)\n\nplt.savefig('var3_15_38.png')\nplt.show()","2d8d6f40":"print(\"Var3\")\nprint(\"Max: \", train['var3'].max())\nprint(\"Min: \", train['var3'].min())\nprint(\"Unique values: \", train['var3'].nunique())\n\nprint(\"\\nVar15\")\nprint(\"Max: \", train['var15'].max())\nprint(\"Min: \", train['var15'].min())\nprint(\"Unique values: \", train['var15'].nunique())\n\nprint(\"\\nVar38\")\nprint(\"Max: \", train['var38'].max())\nprint(\"Min: \", train['var38'].min())\nprint(\"Unique values: \", train['var38'].nunique())","210f723b":"fig, ax = plt.subplots(1, 1, sharex=False, sharey=False, figsize=(7,4))\ntrain.var21.hist(bins=100, range=(0, 30000), ax=ax, alpha=0.7)\nax.set_title(\"Var21 distribution\")\nax.set_xlabel(\"var21\")\nax.set_ylim(0,250)\nax.set_xlim(300,30000)\nplt.savefig('var21_36.png')\nplt.show()","d9e037a1":"print(\"Var21\")\nprint(\"Max: \", train['var21'].max())\nprint(\"Min: \", train['var21'].min())\nprint(\"Unique values: \", train['var21'].nunique())\nprint(\"List of unique values: \", train['var21'].unique())\n\nprint(\"\\nVar36\")\nprint(\"Max: \", train['var36'].max())\nprint(\"Min: \", train['var36'].min())\nprint(\"Unique values: \", train['var36'].nunique())\nprint(\"List of unique values: \", train['var36'].unique())","fd1e3879":"# Correlation matrix of relevant features\ncorr = train.corr()\ntop20_corr = corr.nlargest(20, 'TARGET')['TARGET']\n\n# Plot top20 correlations\nfig, ax = plt.subplots(1, 1, sharex=False, sharey=False, figsize=(7,4))\nplt.bar(top20_corr[1:].index.values, top20_corr[1:].values, alpha=0.7)\nplt.title(\"Top 20 most correlated columns with TARGET\")\nplt.ylabel(\"Correlation\")\nplt.xlabel(\"Features\")\nplt.xticks(rotation=90)\nplt.savefig('correlation_target.png')\nplt.show()","c5e970c9":"print(\"Number of mssing data in the dataset: \", train.isna().any().sum())","3a822888":"# Train\/valid split\ndef split_dataset(data, split_size):\n    y = data['TARGET']\n    X = data.drop(['TARGET'], axis=1)\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=split_size, random_state=21)\n    return X_train, X_valid, y_train, y_valid\n\nX_train, X_valid, y_train, y_valid = split_dataset(train, 0.2) ","41418534":"ts = time.time()\n\nLR = LogisticRegression()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LR, X_train, y_train, scoring=scoring, cv=5)\n\nsorted(scores.keys())\nLR_fit_time = scores['fit_time'].mean()\nLR_score_time = scores['score_time'].mean()\nLR_accuracy = scores['test_accuracy'].mean()\nLR_precision = scores['test_precision_macro'].mean()\nLR_recall = scores['test_recall_macro'].mean()\nLR_f1 = scores['test_f1_weighted'].mean()\nLR_roc = scores['test_roc_auc'].mean()\n\nprint(\"Time spent: \", time.time()-ts)","f5b9cf0b":"ts = time.time()\n\nSVM = SVC(probability = True)\n\nscoring = ['accuracy','precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(SVM, X_train, y_train, scoring=scoring, cv=5)\n\nsorted(scores.keys())\nSVM_fit_time = scores['fit_time'].mean()\nSVM_score_time = scores['score_time'].mean()\nSVM_accuracy = scores['test_accuracy'].mean()\nSVM_precision = scores['test_precision_macro'].mean()\nSVM_recall = scores['test_recall_macro'].mean()\nSVM_f1 = scores['test_f1_weighted'].mean()\nSVM_roc = scores['test_roc_auc'].mean()\n\nprint(\"Time spent: \", time.time()-ts)","1b185d86":"ts = time.time()\n\nLDA = LinearDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LDA, X_train, y_train, scoring=scoring, cv=5)\n\nsorted(scores.keys())\nLDA_fit_time = scores['fit_time'].mean()\nLDA_score_time = scores['score_time'].mean()\nLDA_accuracy = scores['test_accuracy'].mean()\nLDA_precision = scores['test_precision_macro'].mean()\nLDA_recall = scores['test_recall_macro'].mean()\nLDA_f1 = scores['test_f1_weighted'].mean()\nLDA_roc = scores['test_roc_auc'].mean()\n\nprint(\"Time spent: \", time.time()-ts)","7b6be713":"ts = time.time()\n\nQDA = QuadraticDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(QDA, X_train, y_train, scoring=scoring, cv=5)\n\nsorted(scores.keys())\nQDA_fit_time = scores['fit_time'].mean()\nQDA_score_time = scores['score_time'].mean()\nQDA_accuracy = scores['test_accuracy'].mean()\nQDA_precision = scores['test_precision_macro'].mean()\nQDA_recall = scores['test_recall_macro'].mean()\nQDA_f1 = scores['test_f1_weighted'].mean()\nQDA_roc = scores['test_roc_auc'].mean()\n\nprint(\"Time spent: \", time.time()-ts)","c2583190":"ts = time.time()\n\nrandom_forest = RandomForestClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=5)\n\nsorted(scores.keys())\nforest_fit_time = scores['fit_time'].mean()\nforest_score_time = scores['score_time'].mean()\nforest_accuracy = scores['test_accuracy'].mean()\nforest_precision = scores['test_precision_macro'].mean()\nforest_recall = scores['test_recall_macro'].mean()\nforest_f1 = scores['test_f1_weighted'].mean()\nforest_roc = scores['test_roc_auc'].mean()\n\nprint(\"Time spent: \", time.time()-ts)","1ea20c04":"ts = time.time()\nKNN = KNeighborsClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(KNN, X_train, y_train, scoring=scoring, cv=5)\n\nsorted(scores.keys())\nKNN_fit_time = scores['fit_time'].mean()\nKNN_score_time = scores['score_time'].mean()\nKNN_accuracy = scores['test_accuracy'].mean()\nKNN_precision = scores['test_precision_macro'].mean()\nKNN_recall = scores['test_recall_macro'].mean()\nKNN_f1 = scores['test_f1_weighted'].mean()\nKNN_roc = scores['test_roc_auc'].mean()\n\nprint(\"Time spent: \", time.time()-ts)","a28fc0e3":"ts = time.time()\n\nbayes = GaussianNB()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(bayes, X_train, y_train, scoring=scoring, cv=5)\n\nsorted(scores.keys())\nbayes_fit_time = scores['fit_time'].mean()\nbayes_score_time = scores['score_time'].mean()\nbayes_accuracy = scores['test_accuracy'].mean()\nbayes_precision = scores['test_precision_macro'].mean()\nbayes_recall = scores['test_recall_macro'].mean()\nbayes_f1 = scores['test_f1_weighted'].mean()\nbayes_roc = scores['test_roc_auc'].mean()\n\nprint(\"Time spent: \", time.time()-ts)","b1a99740":"models_initial = pd.DataFrame({\n    'Model'       : ['Logistic Regression', 'Support Vector Machine', 'Linear Discriminant Analysis', 'Quadratic Discriminant Analysis', 'Random Forest', 'K-Nearest Neighbors', 'Bayes'],\n    'Fitting time': [LR_fit_time, SVM_fit_time, LDA_fit_time, QDA_fit_time, forest_fit_time, KNN_fit_time, bayes_fit_time],\n    'Scoring time': [LR_score_time, SVM_score_time, LDA_score_time, QDA_score_time, forest_score_time, KNN_score_time, bayes_score_time],\n    'Accuracy'    : [LR_accuracy, SVM_accuracy, LDA_accuracy, QDA_accuracy, forest_accuracy, KNN_accuracy, bayes_accuracy],\n    'Precision'   : [LR_precision, SVM_precision, LDA_precision, QDA_precision, forest_precision, KNN_precision, bayes_precision],\n    'Recall'      : [LR_recall, SVM_recall, LDA_recall, QDA_recall, forest_recall, KNN_recall, bayes_recall],\n    'F1_score'    : [LR_f1,SVM_f1, LDA_f1, QDA_f1, forest_f1, KNN_f1, bayes_f1],\n    'AUC_ROC'     : [LR_roc, SVM_roc, LDA_roc, QDA_roc, forest_roc, KNN_roc, bayes_roc],\n    }, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_initial.sort_values(by='AUC_ROC', ascending=False)","da7b63ba":"# Hyperparametrization of LGB model\ndef optimize_lgb(X_train, y_train, X_valid, y_valid): \n    \n    n_HP_points_to_test = 100\n\n    fit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_valid, y_valid)],\n            'eval_names': ['valid'],\n            'verbose': 0,\n            'categorical_feature': 'auto'}\n    \n    param_test ={'max_depth': [4,5,6,7],\n             'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n    \n    clf = lgb.LGBMClassifier(max_depth=-1, random_state=21, silent=True, metric='auc', n_jobs=4, n_estimators=1000)\n    \n    gs = RandomizedSearchCV(\n            estimator=clf, param_distributions=param_test, \n            n_iter=n_HP_points_to_test,\n            scoring='roc_auc',\n            cv=5,\n            refit=True,\n            random_state=21,\n            verbose=0)\n    \n    gs.fit(X_train, y_train, **fit_params)\n    print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))\n    \n    return gs, gs.best_score_, gs.best_params_, fit_params\n\n\n# Raw data\nX_train, X_valid, y_train, y_valid = split_dataset(train, 0.2)\nclf_1, best_score_1, optimal_params_1, fit_params = optimize_lgb(X_train, y_train, X_valid, y_valid)","b611935e":"def clean_duplicates(df1, df2):\n    remove = []\n    cols = df1.columns\n    for i in range(len(cols)-1):\n        v = df1[cols[i]].values\n        for j in range(i+1,len(cols)):\n            if np.array_equal(v,df1[cols[j]].values):\n                remove.append(cols[j])\n    df1.drop(remove, axis=1, inplace=True)\n    df2.drop(remove, axis=1, inplace=True)\n    return df1, df2\n\n\ndef clean_constant_columns(df1, df2, threshold):\n    constant_cols = []\n    for i in df1.columns:\n        counts = df1[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df1) * 100 > threshold:\n            constant_cols.append(i)\n    df1 = df1.drop(constant_cols, axis=1)\n    df2 = df2.drop(constant_cols, axis=1)\n    return df1, df2\n\n\n# Duplicates of train\/test for modification purposes\ntrain_df = train.copy()\ntest_df = test.copy()\n\n# Replace -999999 by -0.5 in var3\ntrain_df.var3.replace(-999999, -0.5, inplace=True)\ntest_df.var3.replace(-999999, -0.5, inplace=True)\n\n# Drop ID column\ntrain_id = train_df['ID']\ntest_id = test['ID']\ntrain_df.drop('ID', axis=1, inplace=True)\ntest_df.drop('ID', axis=1, inplace=True)\n\n# Irrelevant columns cleaning \ntrain_df, test_df = clean_duplicates(train_df, test_df)\ntrain_df, test_df = clean_constant_columns(train_df, test_df, 99.9)\n\n# Split train dataset and find the best parameters for LGB\nX_train, X_valid, y_train, y_valid = split_dataset(train_df, 0.20)\nclf_2, best_score_2, optimal_params_2, fit_params = optimize_lgb(X_train, y_train, X_valid, y_valid)","bcffbe29":"def fix_skewness(df1, df2, nunique, max_skew):\n    numeric_cols = [cname for cname in df1.columns if df1[cname].dtype in ['int64', 'float64']]\n    skewed_feats = df1[numeric_cols].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\n\n    # Apply log1p to all columns with >nunique values, |skewness|>max_skew and x>-0.99\n    log_col = []\n    for col in skewed_feats.index:\n        if(df1[col].nunique()>nunique):\n            if(abs(skewed_feats[col])>max_skew): \n                if(df1[col].min()>=-0.99):\n                    log_col.append(col)\n                    df1[col]=df1[col].apply(lambda x: np.log1p(x))\n                    df2[col]=df2[col].apply(lambda x: np.log1p(x))\n    return df1, df2, log_col\n\n\ndef var38_flag(df1, df2):\n    df1['var38_flag'], df2['var38_flag'] = 0, 0\n    var38_mode = df1.var38.mode()\n    df1.loc[df1['var38']==var38_mode[0], ['var38', 'var38_flag']] = 0, 1\n    df2.loc[df2['var38']==var38_mode[0], ['var38', 'var38_flag']] = 0, 1\n    return df1, df2\n    \n    \nts = time.time()\n\ntrain_df_skw, test_df_skw = train_df.copy(), test_df.copy()\n\n# Transform skewed features (log1p), create flag for var38 mode\ntrain_df_skw, test_df_skw = var38_flag(train_df_skw, test_df_skw)\ntrain_df_skw, test_df_skw, cols_skw = fix_skewness(train_df_skw, test_df_skw, 50, 0.7)\n\n# Split train dataset and find the best parameters for LGB\nX_train_skw, X_valid_skw, y_train_skw, y_valid_skw = split_dataset(train_df_skw, 0.20)\nclf_3, best_score_3, optimal_params_3, fit_params = optimize_lgb(X_train_skw, y_train_skw, X_valid_skw, y_valid_skw)\n\nprint(\"Time spent: \", time.time()-ts)","3b7c7f3e":"clf_lgb_1 = lgb.LGBMClassifier(   colsample_bytree= 0.5041066856718041, \n                                max_depth= 6, \n                                min_child_samples= 215, \n                                min_child_weight= 0.01, \n                                num_leaves= 47, \n                                reg_alpha= 2, \n                                reg_lambda= 5, \n                                subsample= 0.7631144227290101, \n                                random_state=21, \n                                silent=True, \n                                metric='auc', \n                                n_jobs=4, \n                                n_estimators=1000)\nclf_lgb_1.fit(X_train, y_train)\n\n# Feature importance\nfeature_imp = pd.DataFrame(sorted(zip(clf_lgb_1.feature_importances_,X_train.columns)), columns=['Value','Feature']).sort_values(by=['Value'], ascending=False)[:50]\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGB feature importance')\nplt.tight_layout()\nplt.savefig('lgb_feature_importance.png')\nplt.show()","e7b31130":"perm = PermutationImportance(clf_lgb_1, random_state=21).fit(X_valid, y_valid)\neli5.show_weights(perm, feature_names = X_valid.columns.tolist())","75339244":"X_train_red = X_train\nX_valid_red = X_valid\n\nclf_lgb_1.fit(X_train_red, y_train)\n\nexplainer = shap.TreeExplainer(clf_lgb_1)\nshap_values = explainer.shap_values(X_valid_red)\nshap.summary_plot(shap_values[1], X_valid_red)\nplt.savefig('shap_summary.png')","61ee6018":"shap.dependence_plot('var38', shap_values[1], X_valid_red)\nplt.savefig('shap_var38.png')\nshap.dependence_plot('var15', shap_values[1], X_valid_red)\nplt.savefig('shap_var15.png')\nshap.dependence_plot('var36', shap_values[1], X_valid_red)\nplt.savefig('shap_var36.png')\nshap.dependence_plot('var3', shap_values[1], X_valid_red)\nplt.savefig('shap_var3.png')","fe66e4c0":"shap.dependence_plot('saldo_var5', shap_values[1], X_valid_red)\nplt.savefig('shap_saldo_var5.png')\nshap.dependence_plot('saldo_medio_var5_hace3', shap_values[1], X_valid_red)\nplt.savefig('shap_saldo_medio_var5_hace3.png')\nshap.dependence_plot('saldo_medio_var5_ult3', shap_values[1], X_valid_red)\nplt.savefig('shap_saldo_medio_var5_ult3.png')","c0e958d8":"shap.dependence_plot('num_var30', shap_values[1], X_valid_red)\nplt.savefig('shap_num_var30.png')\nshap.dependence_plot('saldo_var30', shap_values[1], X_valid_red)\nplt.savefig('shap_saldo_var30.png')","2f56f406":"shap.dependence_plot('num_var45_hace2', shap_values[1], X_valid_red)\nplt.savefig('shap_num_var45_hace2.png')\nshap.dependence_plot('num_var45_hace3', shap_values[1], X_valid_red)\nplt.savefig('shap_num_var45_hace3.png')","5e831c77":"clf_lgb = lgb.LGBMClassifier(   colsample_bytree= 0.6641632439141401, \n                                max_depth= 5, \n                                min_child_samples= 224, \n                                min_child_weight= 0.1, \n                                num_leaves= 28, \n                                reg_alpha= 2, \n                                reg_lambda= 10, \n                                subsample= 0.9244622777209361,  \n                                random_state=21,\n                                silent=True, \n                                metric='auc', \n                                n_jobs=4, \n                                n_estimators=1000)\n\n\nclf_lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10)\ntest_df = test_df[X_train.columns]\nprobs = clf_lgb.predict_proba(test_df)\n\nsubmission = pd.DataFrame({\"ID\":test_id, \"TARGET\": probs[:,1]})\nsubmission.to_csv(\"submission.csv\", index=False)","f1d7699e":"* **Random Forest**","3fb0db0e":"## 3.1. Model 1: Baseline LGB <a id=\"section31\"><\/a>\n\nThe first case consists on a** simple LGB model with raw data**, that will work as a baseline score for the following results.\n\nWorkflow\n* **Split**. Split data into train\/validation sets\n* **Optimize LGB**. Perform hyperparametrization tuning to find optimal LGB parameters\n* **Results**. Compute cross validation AUC scores","8672799a":"* **Naive Bayes (Gaussian)**","297d6cc8":"* **Comparison table**","3133eab2":"**Results**:\n* Best ROC-AUC score reached: 0.8399974507944336\n* Optimal parameters: {'colsample_bytree': 0.6641632439141401, 'max_depth': 5, 'min_child_samples': 224, 'min_child_weight': 0.1, 'num_leaves': 28, 'reg_alpha': 2, 'reg_lambda': 10, 'subsample': 0.9244622777209361} \n* Features at this point: 371","d669d013":"* **Logistic Regression**","30e666ab":"# 5. Submission","1939cc7f":"* **Var21 & var36**","f588ddec":"# 4. Model explainability <a id=\"section4\"><\/a>\n\nObtaining a model with the best score under a certain metric is important to some extent, since it's the main objective of a data prediction project. However, from a business perspective, **a black-box algorithm that is unable to provide any additional value nor explain how the different features affect the predictions might have little valuable** depending on the goals of the project. \n\nIn this case, Banco Santander could be interested in additional information, like which features have more impact in the algorithm results or how exactly do they affect them. Understanding the features usually helps to identify how to improve the business objectives, eventually leading to recommendations that might end up in new projects.","4c135052":"## 1.1. Class balance <a id=\"section11\"><\/a>\n\nDistribution of the number of happy and non-happy customers:\n","a607709d":"**Observations**:\n* No missings. Quite surprising given the number of columns, but good news in the end","039adc32":"## 1.4. Missing data <a id=\"section14\"><\/a>\n\nSince we are dealing with such rich dataset, let's look for missing values:","2f5085fd":"* **Linear Discriminant Analysis**","643e0088":"**Results**:\n* Classification is now slightly better, confirming that removing quasi-constant features has reduced noise and helped the model's predictions\n* Best ROC-AUC score reached: 0.8404171483835411\n* Optimal parameters: {'colsample_bytree': 0.652955902411274, 'max_depth': 6, 'min_child_samples': 268, 'min_child_weight': 0.1, 'num_leaves': 37, 'reg_alpha': 2, 'reg_lambda': 10, 'subsample': 0.9251646597209053} \n* Features at this point: 215","a6c8278b":"**Observations**:\n* From the top 20 features with more SHAP value impact, 3 of them are var features (var15, var38 & var36)\n* **Polarized effects features**. Some features are clearly polarized, so that high feature values produce a positive impact on SHAP, while low values have a negative impact. These features are candidates to be relevant features given their SHAP behavior, since they may contain interesting dependencies with the target. Examples: num_meses_var5_ult3, saldo_var5, var36\n* **Indistinguishable effects features**. There are several cases in which high feature values impact both positively or negatively to SHAP values. Apparently, changing the value of these features produce an outlier-like behavior. Examples: num_var22_ult3, saldo_var42, num_var45_hace3.\n* **Mixed effects features**. Other cases present a rich scenario, in which high\/low feature values have no clear regime of impact in SHAP values. Some of them are polarized to some extent with a certain transition regime, while others are more mixed. Examples: var15, saldo_medio_var5_hace3, var38, num_var22_hace3","325fd6ba":"**Observations**:\n\n* **Num_var45_hace2**. Lower values tend to be related to higher satisfaction probabilities, but the effect is very small.\n* **Num_var45_hace3**. Higher values of the feature tend to provide negative SHAP values, and hence more probability for a happy customer. ","4bf3cb2c":"### 4.2.3. Num_var30 & saldo_var30 <a id=\"section423\"><\/a>","3a4fc9fa":"# 2. Comparison of classification models <a id=\"section2\"><\/a>\n\nThe brief EDA performed in the previous section has helped us to identify some insights that could be useful in a future feature engineering step. But before feature engineering, we need a baseline model in order to decide which transformations are indeed useful for the prediction purposes.\n\nLet's start by comparing some classification models:","df0cbb52":"* **Quadratic Discriminant Analysis**","e0dbe5f3":"# 1. Exploratory data analysis (EDA) <a id=\"section1\"><\/a>\n\nIdeally one would perform a thorough analysys of each of the columns, but this requires an amount of time and resources that may not be worth for the seak of this project. I will focus only on the most relevant features of the dataset, but be aware that real world projects frequently rely on really detailed and thorough EDAs.\n\nFirst go first; let's load data, display its structure and get a brief summary:\n","d7657083":"## 4.2. SHAP values <a id=\"section42\"><\/a>\n\nSHAP values (SHapley Additive exPlanations) interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value. SHAP values add explainability to the model to some extent, and provide useful tools to analyze the dependence contribution of each feature.","2e8e3aa3":"**Observations**:\n* **Var36** and **var15** are the most correlated features with the target variable, with values around 0.1\n* **Ind_var8_0** and **num_var8_0** are also considerably correlated to the target, with values above 0.45\n* There's a drop for the rest of the columns, all of them with correlations below 0.3","f7a3620e":"### 4.2.1. Var columns <a id=\"section421\"><\/a>","3854fde2":"* **Var3, var15 & var38**","9ea4b29e":"* **K Nearest Neighbors**","89118df3":"**Observations**:\n\n* **Var38 (wealth)**. Low var38 values have indistinguishable effects on SHAP, while high values don't seem to have any effect. No clear dependance.\n* **Var15 (age)**. The younger the customer (low var15), the more negative the effect on SHAP. Since we are focusing on the probability to be unhappy (target=1), this means that young customers tend to be happier. Customers between 40 and 80 years have more complains, and a more diverse opinion.\n* **Var36**. When var36=0, SHAP values tend to be negative and hence customers are happier. For higher values, and in particular for var36=99, the customer satisfaction drops slightly down (remind that high SHAP values are related to a high probability of target=1).\n* **Var3 (country)**. Looks like modifying the country variable has negative impact on SHAP. We can adventure to guess that the lowest value of var3 is the most common country of the bank's customers (Spain), and that foreign customers are less prone to complain. This could be a reasonable assumption, but business expertise would be required to confirm this behavior. Additionally, it looks like customers from uncommon countries are not only happier, but also generally older.","f7874690":"# 3.2. Model 2: Remove duplicated and constant columns <a id=\"section32\"><\/a>\n\nThe previous prediction has run over 370 features. A lot of these data columns are constant or quasi-constant features, which provide no real information in order to classify customers into happy\/no happy. Let's get rid of them and compare the results.\n\n**Workflow**\n* **Small fixes**. Replace -999999 by -0.5 in var3 and drop ID column\n* **Clean duplicates**. Remove duplicated data columns\n* **Clean constant**. Remove features with more than a threshdol percentage of constant values\n* Split. Split data into train\/validation sets\n* Optimize LGB. Perform hyperparametrization tuning to find optimal LGB parameters\n* Results. Compute cross validation AUC scores","5c4783ed":"* **Support Vector Machine**","37b7fa8f":"# 3. Model development  <a id=\"section3\"><\/a>\n\nRandom Forests have emerged as the winners of the comparison, and we will use a fancy and powerful version of them: Light Gradient Boosting (LGB). In order to analyze which data transformations are useful to achieve a better classification, we will create one \"model-experiment\" for each technique, so that the final score can be properly compared.","93102880":"**OBSERVATIONS**:\n* The dataset contains a large number of columns: 371\n* Binary target class: 0 and 1\n* Some columns present very large values. For example, var38 contains values of order 107\n* Ad-hoc values are present in some cases. For example, var3 with -999999 values. This could point in the direction of unkwown encoded categorical variables","2076f326":"* **Observations**:\n* The metric for this competition is AUC-ROC. The best models in this scoring are Linear Discriminant and RF\n* All accuracies are quite high and above 0.92, except for Bayes and Quadratic Discriminant. The same applies to F1 score\n* Given the performance and the capability to fine tune its corresponding parameters, we will proceed with RF based models","3713afb4":"## 4.1. Feature importance <a id=\"section41\"><\/a>\n\nThe first post-processing task we will tackle is feature importance. Based on our best performing model, we will extract two feature importance measures:\n\n1. **Decision tree feature importance**: importance scores based on the reduction in the criterion used to select split points in a Decision tree algorithm, like Gini or entropy\n2. **Permutation importance**: by exchanging values randomly on each feature, a score is computed that tracks the impact on the predictions","276482bb":"**Observations**:\n\n* **Saldo_var5**. There's an increasing tendency in saldo_var5 and its respective SHAP value. Looks like higher values might be related to less satisfied customers, but the effect is small. Moreover, there's an obvious dependency between saldo_var5 and its historically related variable saldo_medio_var5_ult3.\n* **Saldo_medio_var5**. These features don't seem to have a strong impact on SHAP values, but a slight negative effect for high values. ","2df7211d":"### 4.2.2. Saldo_var5 <a id=\"section422\"><\/a>","d2aac9dc":"**Observations**:\n\n* **Var columns**. Most of the var columns identified as relevant in the EDA section (var38, var15 & var36) are present in the LGB feature importance top. In fact, the top 2 most important features are var38 and var15\n* **Saldo_medio_var5**. Features related to *saldo_medio_var5_* have a high importance value\n* **Saldo_var**. The group including saldo_var30, saldo_var42, saldo_var5 & saldo_var37 has a high importance value\n* **Num_var45**. Features related to *num_var45_* have a high importance value\n* **Num_var22**. Features related to *num_var22_* have a medium-high importance value","41b9eac5":"# Customer satisfaction: models & explainability\n\nThe [Santander Customer Satisfaction competition](https:\/\/www.kaggle.com\/c\/santander-customer-satisfaction) was launched in 2016, with the aim of finding the best models to predict which customers are happy and which have complains. Several models got an AUC-ROC score of 0.84 back in those days, but few submissions included a whole analysis from scratch including a post-analysis. \n\nThe idea of this notebook is to provide an **end-to-end approach to this challenge**, starting from understanding the many columns of the dataset (370), comparing which type of models are best suited for the problem, developing a competitive algorithm through feature engineering and finally analysing which features are more important for the predictions and how do they affect them.\n\n\n**TABLE OF CONTENTS**\n\n1. [Exploratory data analysis (EDA)](#section1)\n\n    1.1. [Class balance](#section11)\n    \n    1.2. [Var columns](#section12)\n    \n    1.3. [Correlation with TARGET](#section13)\n    \n    1.4. [Missing data](#section14)\n    \n    \n2. [Comparison of classification models](#section2)\n\n\n3. [Model development](#section3)\n\n    3.1. [Model 1: Baseline LGB](#section31)\n\n    3.2. [Model 2: Remove duplicated and constant columns](#section32)\n\n    3.3. [Model 3: Transform skewed data](#section33)\n\n\n4. [Model explainability](#section4)\n\n    4.1. [Feature importance](#section41)\n    \n    4.2. [SHAP values](#section42)\n    \n\n5. [Submission](#section5)","5a0b0e40":"### 4.2.4. Num_var45 <a id=\"section424\"><\/a>","2767d029":"**OBSERVATIONS**:\n* The target column is highly unbalanced\n* Happy customers (0): 96.04 %\n* Unhappy customers (1): 3.96 %","9a864a70":"## 1.2. Var columns <a id=\"section12\"><\/a>\n\nColumns named **varXX are candidates to be relevant features**, since their unique name (var) suggests that they might be related to the customer's personal information. In some cases, it seems that they have been transformed through an encoding process (i.e. var3), and hence their original values would be categorical.","c60e219c":"**Observations**: \n\n* No big effects in var30, but a general tendency to have higher satisfaction probabilities for large values of num_var30 and saldo_var30.","f6aeae89":"**Observations**: \n* Var3 has 208 unique values, from which certain values are more frequent while the rest of them appear more uniformly\n\n* Var3 shows ad-hoc values flagged as -999999, suggesting that this variable may be related to a categorical data field. To prevent potential problems due to this high negative value, we have replaced it by -0.5\n\n* **Var3**. From the (not very large) number of distinct values and the fact that there's a manual flag, my guess is that this column is **related to country information**, such as nationality or citizenship\n\n* **Var15** is distributed between 5 and 105 following a decreasing behavior. Since values are comprised of \"small\" numbers, they might be **related to certain categorization** or count, and not to to capital data. My guess is that it's related to the age of the customer, which is in consonance with values expected (quite surprising to find such young customers tho!)\n\n* **Var38**. The exponential decayment shown above resembles to curves from wealth distributions. Moreover, there's a huge peak at 117310.979016 with more than 16000 cases, which is difficult to explain without further details. This varialbe is probably **related to wealth** or capital in some way","b40f2e7f":"## 1.3. Correlation with TARGET <a id=\"section13\"><\/a>\n\nBrief analysis of the TOP20 most correlated data columns with target:","1dbdae03":"**Observations**:\n* Both var21 and var36 are comprised of a small number of distinct values\n* For **var21**, all values are multiples of 300, which might provide some useful information about the nature of this variable. Despite being quite noisy, the general tendency of the values' frequencies is decreasing\n* For **var36** the values range mainly from 0 to 3, while 99 looks like a manual flag to mark specific cases (unknown cases?)","0b6852aa":"**Results**:\n* A very slight increase in the score, which can't be interpreted as an improvement (probability component of the model). However, normalizing data distributions does not worsen the results\n* Best ROC-AUC score reached: 0.8405401877854473\n* Optimal parameters: {'colsample_bytree': 0.5041066856718041, 'max_depth': 6, 'min_child_samples': 215, 'min_child_weight': 0.01, 'num_leaves': 47, 'reg_alpha': 2, 'reg_lambda': 5, 'subsample': 0.7631144227290101} \n* Features at this point: 214","5a591ad5":"# 3.3. Model 3: Transform skewed data <a id=\"section33\"><\/a>\n\nSo far so good, we have achieved a considerable AUC-ROC. In this experiment we will go one step further, putting our attention on data distributions and particularly on features with a high skewness.\n\nIn order to normalize a skewed variable $x$, it is frequent to apply a transformation such as $x^2$ or $log(x)$. We will opt for a slightly modified version of the second transformation, $log(x+1)$, since it's able to deal both with positive values and with the additional range $(\u22121,0]$.\n\n**Workflow**\n* Small fixes. Replace -999999 by -0.5 in var3 and drop ID column\n* Clean duplicates. Remove duplicated data columns\n* Clean constant. Remove features with more than a threshdol percentage of constant values\n* Split. Split data into train\/validation sets\n* **Normality test**. Check for highly skewed columns, and apply a normalization transformation (log1p) when required\n* Optimize LGB. Perform hyperparametrization tuning to find optimal LGB parameters\n* Results. Compute cross validation AUC scores\n\n**Note**: tree-based models are scale invariant, so that *a priori* the log transformation to normalize data should not have any impact on the predictions. However, since the objective of this notebook is to provide useful feature engineering techniques and given that we could end up using additional models not based on trees, the transformation of skewed features may prove useful. "}}