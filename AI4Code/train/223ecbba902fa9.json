{"cell_type":{"43081c50":"code","d8706d5b":"code","6aec14f0":"code","bc59eb33":"code","b16bdf0c":"code","a48fd650":"code","4cd62787":"code","12d5aa3a":"code","cb6cf8a9":"code","8c254415":"code","140b9cb2":"code","75eee8f4":"code","92dacf43":"code","3487ffcd":"code","4be45734":"code","0a83ae0d":"code","82a9f31d":"code","7d7c4d1d":"code","170ea46d":"code","6a093868":"code","8c30db57":"code","7a671d3c":"code","a861a525":"code","712a2cf0":"code","1df3913f":"code","e283db1d":"code","8d9602a5":"code","7e899612":"code","a0355129":"markdown","535f62d0":"markdown","85bd87fb":"markdown","dd076761":"markdown","a97d6308":"markdown","0ec4b40c":"markdown","471e4495":"markdown","e3df747b":"markdown","8ec6809f":"markdown","8437fb4b":"markdown","c31e5bb1":"markdown","cfe75597":"markdown","792dd0bc":"markdown","817afc9f":"markdown","8c9c3f78":"markdown","e31030b2":"markdown","cd618270":"markdown","7b18b0f2":"markdown","7d181fc8":"markdown","2a54ab02":"markdown"},"source":{"43081c50":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\n\nimport os\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\nimport tensorflow as tf\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 63\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","d8706d5b":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ncombined = pd.concat([train,test], axis=0)\ncombined.drop('target',inplace=True, axis=1)\ncombined.info()","6aec14f0":"y = train.target.copy()\nX = train.drop('target',axis=1)\npath = '..\/input\/roberta-transformers-pytorch\/roberta-base'","bc59eb33":"sns.countplot(y)\nplt.show()","b16bdf0c":"train_targets = train.target.values.tolist()\nmax_len = 90\n\ntokenizer = AutoTokenizer.from_pretrained('..\/input\/roberta-transformers-pytorch\/roberta-base')\n\ninput_ids = [tokenizer.encode(\n        text=i,           \n        add_special_tokens=True, \n        max_length=max_len,  \n        truncation=True,     \n        padding=False\n    ) for i in train.text]","a48fd650":"unsorted_lengths = [len(x) for x in input_ids]\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style='darkgrid')\n\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.scatter(range(0, len(unsorted_lengths)), unsorted_lengths, marker=\"|\")\n\nplt.xlabel('Sample Number')\nplt.ylabel('Sequence Length')\nplt.title('Samples BEFORE Sorting')\n\nplt.show()","4cd62787":"sorted_input_ids = sorted(zip(input_ids, train_targets), key=lambda x: len(x[0]))\nprint('Shortest sample:', len(sorted_input_ids[0][0]))\nprint('Longest sample:', len(sorted_input_ids[-1][0]))\nsorted_lengths = [len(s[0]) for s in sorted_input_ids]","12d5aa3a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.plot(range(0, len(sorted_lengths)), sorted_lengths)\n\nplt.xlabel('Sample Number')\nplt.ylabel('Sequence Length')\nplt.title('Samples after Sorting')\n\nplt.show()","cb6cf8a9":"batch_size = 32\nimport random\n\nbatch_ordered_sentences = []\nbatch_ordered_labels = []\n\nprint('Creating training batches of size {:}'.format(batch_size))\n\nwhile len(sorted_input_ids) > 0:  \n    if ((len(batch_ordered_sentences) % 50) == 0):\n        print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n\n    to_take = min(batch_size, len(sorted_input_ids))\n    select = random.randint(0, len(sorted_input_ids) - to_take)\n    batch = sorted_input_ids[select:(select + to_take)]\n    batch_ordered_sentences.append([s[0] for s in batch])\n    batch_ordered_labels.append([s[1] for s in batch])\n    del sorted_input_ids[select:select + to_take]\n\nprint('\\n  DONE - {:,} batches.'.format(len(batch_ordered_sentences)))","8c254415":"inputs = []\nattn_masks = []\ntargets = []\n\nfor (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n    batch_padded_inputs = []\n    batch_attn_masks = []\n    max_size = max([len(sen) for sen in batch_inputs])\n    for sen in batch_inputs:\n        num_pads = max_size - len(sen)\n        padded_input = sen + [tokenizer.pad_token_id]*num_pads\n        attn_mask = [1] * len(sen) + [0] * num_pads\n        batch_padded_inputs.append(padded_input)\n        batch_attn_masks.append(attn_mask)\n    inputs.append(torch.tensor(batch_padded_inputs))\n    attn_masks.append(torch.tensor(batch_attn_masks))\n    targets.append(torch.tensor(batch_labels))","140b9cb2":"train_text = train.text.values.tolist()\npadded_lengths = [len(s) for batch in inputs for s in batch]\nsmart_token_count = np.sum(padded_lengths)\nfixed_token_count = len(train_text) * max_len\n\nprcnt_reduced = (fixed_token_count - smart_token_count) \/ float(fixed_token_count) \n\nprint('Total tokens:')\nprint('   Fixed Padding: {:,}'.format(fixed_token_count))\nprint('  Smart Batching: {:,}  ({:.2%} less)'.format(smart_token_count, prcnt_reduced))","75eee8f4":"# Essential Imports\nimport random\nimport numpy as np\nimport multiprocessing\nimport more_itertools\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Sampler, Dataset, DataLoader","92dacf43":"class SmartBatchingDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        super(SmartBatchingDataset, self).__init__()\n        self._data = (\n            f\"{tokenizer.bos_token} \" + df.text + f\" {tokenizer.eos_token}\" \n        ).apply(tokenizer.tokenize).apply(tokenizer.convert_tokens_to_ids).to_list()\n        self._targets = None\n        if 'target' in df.columns:\n            self._targets = df.target.tolist()\n        self.sampler = None\n\n    def __len__(self):\n        return len(self._data)\n\n    def __getitem__(self, item):\n        if self._targets is not None:\n            return self._data[item], self._targets[item]\n        else:\n            return self._data[item]\n\n    def get_dataloader(self, batch_size, max_len, pad_id):\n        self.sampler = SmartBatchingSampler(\n            data_source=self._data,\n            batch_size=batch_size\n        )\n        collate_fn = SmartBatchingCollate(\n            targets=self._targets,\n            max_length=max_len,\n            pad_token_id=pad_id\n        )\n        dataloader = DataLoader(\n            dataset=self,\n            batch_size=batch_size,\n            sampler=self.sampler,\n            collate_fn=collate_fn,\n            num_workers=(multiprocessing.cpu_count()-1),\n            pin_memory=True\n        )\n        return dataloader","3487ffcd":"class SmartBatchingSampler(Sampler):\n    def __init__(self, data_source, batch_size):\n        super(SmartBatchingSampler, self).__init__(data_source)\n        sample_lengths = [len(seq) for seq in data_source]\n        argsort_inds = np.argsort(sample_lengths)\n        batches = list(more_itertools.chunked(argsort_inds, n=batch_size))\n        if batches:\n            last_batch = batches.pop(-1)\n            np.random.shuffle(batches)\n            batches.append(last_batch)\n        self._inds = list(more_itertools.flatten(batches))\n        self._backsort_inds = None\n    \n    def __iter__(self):\n        it = iter(self._inds)\n        return it\n\n    def __len__(self):\n        return len(self._inds)\n    \n    @property\n    def backsort_inds(self):\n        if self._backsort_inds is None:\n            self._backsort_inds = np.argsort(self._inds)\n        return self._backsort_inds","4be45734":"class SmartBatchingCollate:\n    def __init__(self, targets, max_length, pad_token_id):\n        self._targets = targets\n        self._max_length = max_length\n        self._pad_token_id = pad_token_id\n        \n    def __call__(self, batch):\n        if self._targets is not None:\n            sequences, targets = list(zip(*batch))\n        else:\n            sequences = list(batch)\n        \n        input_ids, attention_mask = self.pad_sequence(\n            sequences,\n            max_sequence_length=self._max_length,\n            pad_token_id=self._pad_token_id\n        )\n        \n        if self._targets is not None:\n            output = input_ids, attention_mask, torch.tensor(targets)\n        else:\n            output = input_ids, attention_mask\n        return output\n    \n    def pad_sequence(self, sequence_batch, max_sequence_length, pad_token_id):\n        max_batch_len = max(len(sequence) for sequence in sequence_batch)\n        max_len = min(max_batch_len, max_sequence_length)\n        padded_sequences, attention_masks = [[] for i in range(2)]\n        attend, no_attend = 1, 0\n        for sequence in sequence_batch:\n            # As discussed above, truncate if exceeds max_len\n            new_sequence = list(sequence[:max_len])\n            \n            attention_mask = [attend] * len(new_sequence)\n            pad_length = max_len - len(new_sequence)\n            \n            new_sequence.extend([pad_token_id] * pad_length)\n            attention_mask.extend([no_attend] * pad_length)\n            \n            padded_sequences.append(new_sequence)\n            attention_masks.append(attention_mask)\n        \n        padded_sequences = torch.tensor(padded_sequences)\n        attention_masks = torch.tensor(attention_masks)\n        return padded_sequences, attention_masks","0a83ae0d":"dataset = SmartBatchingDataset(train, tokenizer)\ntrain_data_loader = dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id)","82a9f31d":"padded_lengths = []\nfor batch_idx, (input_ids, attention_mask, targets) in enumerate(train_data_loader):\n    for s in input_ids:\n        padded_lengths.append(len(s))\n\nsmart_token_count = np.sum(padded_lengths)\nfixed_token_count = len(train_text) * max_len\n\nprcnt_reduced = (fixed_token_count - smart_token_count) \/ float(fixed_token_count) \n\nprint('Total tokens:')\nprint('   Fixed Padding: {:,}'.format(fixed_token_count))\nprint('  Smart Batching: {:,}  ({:.2%} less)'.format(smart_token_count, prcnt_reduced))","7d7c4d1d":"class SentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)\n        self.config = self.roberta.config\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.config.hidden_size, n_classes)\n        \n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n        output = self.drop(sequence_output)\n        return self.out(output)","170ea46d":"model = SentimentClassifier(2) # 2 classes 1 for disaster and 0 for not\nmodel = model.to(device)","6a093868":"# torch.nn.functional.softmax(model(input_ids, attention_mask), dim=1)","8c30db57":"EPOCHS = 10\noptimizer = AdamW(model.parameters(), betas = (0.99, 0.98), lr=2e-5)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","7a671d3c":"def train_epoch(\n  model,\n  data_loader,\n  loss_fn,\n  optimizer,\n  device,\n  scheduler,\n  n_examples\n):\n  model = model.train()\n  losses = []\n  correct_predictions = 0\n  for d in data_loader:\n    input_ids = d[0].to(device)\n    attention_mask = d[1].to(device)\n    targets = d[2].to(device)\n    outputs = model(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    _, preds = torch.max(outputs, dim=1)\n    loss = loss_fn(outputs, targets)\n    correct_predictions += torch.sum(preds == targets)\n    losses.append(loss.item())\n    loss.backward()\n    # prevents exploding gradients\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n  return correct_predictions.double() \/ n_examples, np.mean(losses)","a861a525":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n  model = model.eval()\n  losses = []\n  correct_predictions = 0\n  with torch.no_grad():\n    for d in data_loader:\n        input_ids = d[0].to(device)\n        attention_mask = d[1].to(device)\n        targets = d[2].to(device)\n        outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n  return correct_predictions.double() \/ n_examples, np.mean(losses)","712a2cf0":"from sklearn.model_selection import train_test_split\nX_train, X_val = train_test_split(train, test_size=0.10, random_state=RANDOM_SEED)\n\ntrain_dataset = SmartBatchingDataset(X_train, tokenizer)\ntrain_data_loader = train_dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id)\n\nval_dataset = SmartBatchingDataset(X_val, tokenizer)\nval_data_loader = val_dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id)\n\nhistory = defaultdict(list)\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(X_train)\n    )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n\n    val_acc, val_loss = eval_model(\n      model,\n      val_data_loader,\n      loss_fn,\n      device,\n      len(X_val)\n    )\n    print(f'Val loss {val_loss} accuracy {val_acc}')\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)","1df3913f":"# plt.plot(history['train_acc'], label='train accuracy')\n# plt.plot(history['val_acc'], label = 'val accurracy')\n# plt.title('Training history')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.legend()\n# plt.ylim([0, 1]);","e283db1d":"encodes = test.text.apply(lambda x: tokenizer.encode_plus(\n            x, \n            add_special_tokens=True,\n            max_length = max_len,\n            truncation=True,\n            padding='max_length',\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ))\ninput_ids = [i['input_ids'] for i in encodes]\nattention_mask = [i['attention_mask'] for i in encodes]","8d9602a5":"predictions = []\nfor i, j in zip(input_ids, attention_mask):\n    i = i.to(device)\n    j = j.to(device)\n    output = model(i, j)\n    _, prediction = torch.max(output, dim=1)\n    predictions.append(prediction.item())","7e899612":"submission = pd.concat([test.id, pd.Series(predictions)], axis=1)\nsubmission.rename(columns = {0:'target'}, inplace=True)\nsubmission.to_csv('submission.csv',index=False)","a0355129":"# First, we have to sort our data by sequence length. We'll tokenize our data and plot the length of these tokens first.Negatives outnumber positives by ~1000\n","535f62d0":"# Training","85bd87fb":"# Model Creation\n","dd076761":"\n\nSmartBatchingSampler sorts sequences by length, make batches of specified size, shuffle the batch, then return indices\n","a97d6308":"\n\nWhat a mess. Now we'll sort the tokens by length","0ec4b40c":"\n\nPytorch dataloader implementation saved a little more memory\n","471e4495":"# Testing\n","e3df747b":"First, we have to sort our data by sequence length. We'll tokenize our data and plot the length of these tokens first.","8ec6809f":"# Padding","8437fb4b":"# Comparison","c31e5bb1":"\n\ntrain_data_loader orders its data like this: input_ids, attention_mask, targets\n","cfe75597":"\n\nNegatives outnumber positives by ~1000","792dd0bc":"# Class Imbalance?","817afc9f":"* Some recommendations for fine tuning from the BERT paper\n* Batch size: 16, 32\n* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n* Number of epochs: 2, 3, 4","8c9c3f78":"\n\nSmartBatchingCollate adds padding up to max_length, make attention masks, and targets for each sample batch\n","e31030b2":"# Putting it all together\n\n\nSmartBatchingDataset stores samples by tokenizing text and converting to sequences\n","cd618270":"# Random Batch Selection","7b18b0f2":"* d - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n* \n* No missing values in the most important feature, text, but...\n* 87 missing values in keyword\n* 3638 missing values in location","7d181fc8":"\n\nRemember, the batches are still not ordered according to length\n","2a54ab02":"predictions = []\nfor i, j in zip(input_ids, attention_mask):\n    i = i.to(device)\n    j = j.to(device)\n    output = model(i, j)\n    _, prediction = torch.max(output, dim=1)\n    predictions.append(prediction.item())"}}