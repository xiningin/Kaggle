{"cell_type":{"d3ca2923":"code","567fd0cf":"code","b1516bc6":"code","f7b45760":"code","944057e6":"code","0894cef1":"code","66234c79":"code","20e45642":"code","93991d45":"code","17ce6338":"code","57de86ee":"markdown","0176335e":"markdown","8f7293f1":"markdown","97261089":"markdown","76539620":"markdown","28377ab0":"markdown","dcd81bc9":"markdown"},"source":{"d3ca2923":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","567fd0cf":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\n\n# create a random, binary classification problem, with 100000 samples and 20 features\nX, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=7, n_redundant=10,\n                                    random_state=42)\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.99, random_state=42)\n","b1516bc6":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n\nlgr = LogisticRegression(C=1, solver='lbfgs')\nsvc = SVC(max_iter=10000, probability=True)","f7b45760":"probs_lgr = lgr.fit(X_train, y_train).predict_proba(X_test)[:,1]\npreds_svc = svc.fit(X_train, y_train).predict(X_test)\n\nprobs_svc = svc.decision_function(X_test)\nprobs_svc = (probs_svc - probs_svc.min()) \/ (probs_svc.max() - probs_svc.min())","944057e6":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(10,6))\nsns.kdeplot(probs_lgr, label='Logistic regression')\nsns.kdeplot(preds_svc, label='SVM')\nplt.title(\"Probability Density Plot for 2 Classifiers\")\nplt.show()","0894cef1":"\nfrom sklearn import metrics\n\n\nplt.figure(figsize=(8,5))\nplt.plot([0, 1], [0, 1],'r--')\n\npred = probs_lgr\nlabel = y_test\nfpr, tpr, thresh = metrics.roc_curve(label, pred)\nauc = metrics.roc_auc_score(label, pred)\nplt.plot(fpr, tpr, label=f'Logistic regression, auc = {str(round(auc,3))}')\n\npred = probs_svc\nfpr, tpr, thresh = metrics.roc_curve(label, pred)\nauc = metrics.roc_auc_score(label, pred)\nplt.plot(fpr, tpr, label=f'SVC, auc = {str(round(auc,3))}')\n\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.title(\"AUC-ROC for two models\")\nplt.legend()\nplt.show()","66234c79":"from sklearn.calibration import calibration_curve\n\n\ndef plot_calibration_curve(name, fig_index, probs):\n    \"\"\"Plot calibration curve for est w\/o and with calibration. \"\"\"\n\n    fig = plt.figure(fig_index, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n    \n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    \n    frac_of_pos, mean_pred_value = calibration_curve(y_test, probs, n_bins=10)\n\n    ax1.plot(mean_pred_value, frac_of_pos, \"s-\", label=f'{name}')\n    ax1.set_ylabel(\"Fraction of positives\")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc=\"lower right\")\n    ax1.set_title(f'Calibration plot ({name})')\n    \n    ax2.hist(probs, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n    ax2.set_xlabel(\"Mean predicted value\")\n    ax2.set_ylabel(\"Count\")","20e45642":"# plot calibration curve for logistic regression\nplot_calibration_curve(\"Logistic regression\", 1, probs_lgr)","93991d45":"# plot calibration curve for the SVM\nplot_calibration_curve(\"SVM\", 1, probs_svc)","17ce6338":"from sklearn.calibration import CalibratedClassifierCV\n\n\nlgr = LogisticRegression(C=1, solver='lbfgs')\nsvc = SVC(max_iter=10000, probability=True)\n\nplatts_scaling = CalibratedClassifierCV(svc, cv=2, method='sigmoid')\nplatts_scaling.fit(X_train, y_train)\ncalibrated_probs = platts_scaling.predict_proba(X_test)[:,1]\n\nplatts_scaling = CalibratedClassifierCV(lgr, cv=2, method='sigmoid')\nplatts_scaling.fit(X_train, y_train)\ncalibrated_probs1 = platts_scaling.predict_proba(X_test)[:,1]\n\n\nplot_calibration_curve(\"SVM\", 3, calibrated_probs)\nplot_calibration_curve(\"Logistic regression\", 1,calibrated_probs1)","57de86ee":"# References:\n\n1.  https:\/\/towardsdatascience.com\/classifier-calibration-7d0be1e05452\n  \n2.  https:\/\/medium.com\/@kingsubham27\/calibration-techniques-and-its-importance-in-machine-learning-71bec997b661\n  \n3.  https:\/\/medium.com\/analytics-vidhya\/calibration-in-machine-learning-e7972ac93555\n  \n4.  https:\/\/machinelearningmastery.com\/calibrated-classification-model-in-scikit-learn\/","0176335e":"# Train and Test split","8f7293f1":"# AUC-ROC Curve","97261089":"# Logistic Regression & SVC model","76539620":"***Pros:***\n\n* A calibrated classifier provides reliable estimates of the true probability that each test sample is a member of the class of interest. This is crucial in decision making tasks.\n\n* This result in an improved calibration on a reliability diagram.\n\n***Cons:***\n\n* Many a times, we face problems (data sets) whose evaluation metric is LogLoss.","28377ab0":"#  Calibration Classifier:\n\n   The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction. Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level.\n![](https:\/\/imgur.com\/K4xfUjB.png)   \n","dcd81bc9":"# Calibrated model"}}