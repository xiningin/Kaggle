{"cell_type":{"af2945e5":"code","793b4414":"code","2871e951":"code","5bfdeff5":"code","27809c7a":"code","bcb42ea9":"code","9b2d7dbf":"code","0c9e1a34":"code","b2d72479":"code","1d0d5d84":"code","009cec47":"code","e92e9ba8":"markdown","d9432b96":"markdown","c9b7ee88":"markdown","6e671333":"markdown","06ad2557":"markdown"},"source":{"af2945e5":"import numpy as np\nimport skimage.transform\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n%matplotlib inline\n\nx, y = np.meshgrid(range(8), range(8))\n\n\ndef blur_img(c_img): return (ndimage.zoom(c_img.astype('float'),\n                                          3,\n                                          order=3,\n                                          prefilter=False)*4).astype(int).clip(1, 4)-1\n\n\ntext_imgs = [blur_img(c_img)\n             for c_img in [x % 2,\n                           y % 2,\n                           (x % 2+y % 2)\/2.0,\n                           (x % 4+y % 2)\/2.5,\n                           (x % 4+y % 3)\/3.5,\n                           ((x+y) % 3)\/2.0]]\n\nfig, m_axs = plt.subplots(2, 3, figsize=(20, 10))\n\nfor c_ax, c_img in zip(m_axs.flatten(), text_imgs):\n    sns.heatmap(c_img, annot=False, fmt='2d', ax=c_ax,\n                cmap='viridis', vmin=0, vmax=3)\n","793b4414":"from skimage.feature.texture import greycomatrix\nfrom skimage.util import montage as montage2d\n\ndef montage_nd(in_img):\n    if len(in_img.shape) > 3:\n        return montage2d(np.stack([montage_nd(x_slice) for x_slice in in_img], 0))\n    elif len(in_img.shape) == 3:\n        return montage2d(in_img)\n    else:\n        warn('Input less than 3d image, returning original', RuntimeWarning)\n        return in_img\n\n\ndist_list = np.linspace(1, 6, 15)\nangle_list = np.linspace(0, 2*np.pi, 15)\n\n\ndef calc_coomatrix(in_img):\n    return greycomatrix(image=in_img,\n                        distances=dist_list,\n                        angles=angle_list,\n                        levels=4)\n\n\ndef coo_tensor_to_df(x): return pd.DataFrame(\n    np.stack([x.ravel()]+[c_vec.ravel() for c_vec in np.meshgrid(range(x.shape[0]),\n                                                                 range(\n                                                                     x.shape[1]),\n                                                                 dist_list,\n                                                                 angle_list,\n                                                                 indexing='xy')], -1),\n    columns=['E', 'i', 'j', 'd', 'theta'])\n\n\ncoo_tensor_to_df(calc_coomatrix(text_imgs[0])).head(5)","2871e951":"fig, m_axs = plt.subplots(3, 6, figsize=(20, 10))\n\nfor (c_ax, d_ax, f_ax), c_img in zip(m_axs.T, text_imgs):\n    c_ax.imshow(c_img, vmin=0, vmax=4, cmap='gray')\n    c_ax.set_title('Pattern')\n    full_coo_matrix = calc_coomatrix(c_img)\n    d_ax.imshow(montage_nd(full_coo_matrix), cmap='gray')\n    d_ax.set_title('Co-occurence Matrix\\n{}'.format(full_coo_matrix.shape))\n    d_ax.axis('off')\n    avg_coo_matrix = np.mean(full_coo_matrix*1.0, (0, 1))\n    f_ax.imshow(avg_coo_matrix, cmap='gray')\n    f_ax.set_title('Average Co-occurence\\n{}'.format(avg_coo_matrix.shape))","5bfdeff5":"text_df = coo_tensor_to_df(calc_coomatrix(text_imgs[0]))\ntext_df['ij_diff'] = text_df.apply(lambda x: x['i']-x['j'], axis=1)\n\nsimple_corr_df = text_df.groupby(['ij_diff', 'd', 'theta']).agg({\n    'E': 'mean'}).reset_index()\nsimple_corr_df.head(5)","27809c7a":"def grouped_weighted_avg(values, weights, by):\n    return (values * weights).groupby(by).sum() \/ weights.groupby(by).sum()\n\n\nfig, m_axs = plt.subplots(3, 6, figsize=(20, 10))\n\nfor (c_ax, d_ax, f_ax), c_img in zip(m_axs.T, text_imgs):\n    c_ax.imshow(c_img, vmin=0, vmax=4, cmap='gray')\n    c_ax.set_title('Pattern')\n    full_coo_matrix = calc_coomatrix(c_img)\n    text_df = coo_tensor_to_df(full_coo_matrix)\n    text_df['ij_diff'] = text_df.apply(lambda x: x['i']-x['j'], axis=1)\n\n    simple_corr_df = text_df.groupby(['ij_diff', 'd', 'theta']).agg({\n        'E': 'sum'}).reset_index()\n    gwa_d = grouped_weighted_avg(\n        simple_corr_df.ij_diff, simple_corr_df.E, simple_corr_df.d)\n    d_ax.plot(gwa_d.index, gwa_d.values)\n    d_ax.set_title('Distance Co-occurence')\n\n    gwa_theta = grouped_weighted_avg(\n        simple_corr_df.ij_diff, simple_corr_df.E, simple_corr_df.theta)\n    f_ax.plot(gwa_theta.index, gwa_theta.values)\n    f_ax.set_title('Angular Co-occurence')","bcb42ea9":"import seaborn as sns\nfrom skimage.color import label2rgb\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\n\nslice_img = (h5py.File('..\/input\/train\/mri_00004519.h5')['image'][100][:, :, 0]\/3200*255).astype('uint8')\nslice_mask = slice_img>0\n\n# show the slice and threshold\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(11, 5))\nax1.imshow(slice_img, cmap='gray')\nax1.axis('off')\nax1.set_title('Image')\nax2.imshow(slice_mask, cmap='gray')\nax2.axis('off')\nax2.set_title('Segmentation')\n# here we mark the threshold on the original image\n\nax3.imshow(label2rgb(slice_mask > 0, slice_img, bg_label=0, ))\nax3.axis('off')\nax3.set_title('Overlayed')","9b2d7dbf":"xx, yy = np.meshgrid(\n    np.arange(slice_img.shape[1]),\n    np.arange(slice_img.shape[0]))\nregion_labels = (xx\/\/48) * 64+yy\/\/48\nregion_labels = region_labels.astype(int)\nsns.heatmap(region_labels[::48, ::48].astype(int),\n            annot=True,\n            fmt=\"03d\",\n            cmap='nipy_spectral',\n            cbar=False,\n            )","0c9e1a34":"# compute some GLCM properties each patch\nfrom skimage.feature import greycomatrix, greycoprops\nfrom tqdm import tqdm_notebook as tqdm\ngrayco_prop_list = ['contrast', 'dissimilarity',\n                    'homogeneity', 'energy',\n                    'correlation', 'ASM']\n\nprop_imgs = {}\nfor c_prop in grayco_prop_list:\n    prop_imgs[c_prop] = np.zeros_like(slice_img, dtype=np.float32)\nscore_img = np.zeros_like(slice_img, dtype=np.float32)\nout_df_list = []\nfor patch_idx in tqdm(np.unique(region_labels)):\n    xx_box, yy_box = np.where(region_labels == patch_idx)\n\n    glcm = greycomatrix(slice_img[xx_box.min():xx_box.max(),\n                                   yy_box.min():yy_box.max()],\n                        [5], [0], 256, symmetric=True, normed=True)\n\n    mean_score = np.mean(slice_mask[region_labels == patch_idx])\n    score_img[region_labels == patch_idx] = mean_score\n\n    out_row = dict(\n        intensity_mean=np.mean(slice_img[region_labels == patch_idx]),\n        intensity_std=np.std(slice_img[region_labels == patch_idx]),\n        score=mean_score)\n\n    for c_prop in grayco_prop_list:\n        out_row[c_prop] = greycoprops(glcm, c_prop)[0, 0]\n        prop_imgs[c_prop][region_labels == patch_idx] = out_row[c_prop]\n\n    out_df_list += [out_row]","b2d72479":"# show the slice and threshold\nfig, m_axs = plt.subplots(2, 4, figsize=(20, 10))\nn_axs = m_axs.flatten()\nax1 = n_axs[0]\nax2 = n_axs[1]\nax1.imshow(slice_img, cmap='gray')\nax1.axis('off')\nax1.set_title('Image')\nax2.imshow(score_img, cmap='gray')\nax2.axis('off')\nax2.set_title('Regions')\nfor c_ax, c_prop in zip(n_axs[2:], grayco_prop_list):\n    c_ax.imshow(prop_imgs[c_prop], cmap='gray')\n    c_ax.axis('off')\n    c_ax.set_title('{} Image'.format(c_prop))\n","1d0d5d84":"import pandas as pd\nout_df = pd.DataFrame(out_df_list)\nout_df['positive_score'] = out_df['score'].map(\n    lambda x: 'FG' if x > 0.35 else 'BG')\nout_df.describe()","009cec47":"sns.pairplot(out_df,\n             hue='positive_score',\n             kind=\"reg\")","e92e9ba8":"# Tiling\nHere we divide the image up into unique tiles for further processing","d9432b96":"## Simple Correlation\nUsing the mean difference ($E[i-j]$) instead of all of the i,j pair possiblities","c9b7ee88":"# Texture Analysis\nhttp:\/\/murphylab.web.cmu.edu\/publications\/boland\/boland_node26.html","6e671333":"# Applying Texture to Brain","06ad2557":"# Calculating Texture\nHere we calculate the texture by using a tool called the gray level co-occurrence matrix which are part of the features library in skimage. We focus on two metrics in this, specifically dissimilarity and correlation which we calculate for each tile. We then want to see which of these parameters correlated best with belonging to a nerve fiber."}}