{"cell_type":{"5cae4aff":"code","073f15fa":"code","e68f96d7":"code","ddc36929":"code","2e63bc5e":"code","1cfc18cd":"code","f235bb2a":"code","a442c694":"code","453103a2":"code","d8fe76d9":"code","9685a57f":"code","5fe89124":"code","2933eae6":"code","4a099fed":"code","b7c85148":"code","e4362229":"code","0622f2d1":"code","9f8f2988":"code","bd6bc712":"code","b0463129":"code","9189df16":"code","3c43b6a7":"code","b5d64019":"code","9fa0aac4":"code","becabbaf":"code","4b499e2c":"code","e68bf652":"code","c57b626a":"code","6036f1ff":"code","3111b852":"code","4cd80340":"code","a0b977b1":"code","d6775e43":"code","e59093a8":"code","690ab669":"code","b2f9f622":"code","7b80f7d4":"code","67bca145":"code","06ac9b49":"code","cecb3c15":"code","3f204b5c":"code","a42e8e2d":"code","12df1360":"code","de357387":"code","0b318eec":"code","a9d52be5":"code","11e47241":"code","6ad40d18":"code","4843dd19":"code","b192d49c":"code","7a1e302d":"code","3213be44":"code","59670e40":"code","ee391fc4":"code","a31bedfa":"code","e732b08f":"code","df3a367d":"code","73cc1f98":"code","467f1d53":"code","fd24ca97":"code","f3594dfe":"code","1ee8236d":"code","e75692b1":"code","ff4d1c67":"code","8370a34b":"code","6d827a05":"code","0cfd4953":"code","ebd7ed2b":"code","b421b494":"code","d027c389":"code","66192bc2":"code","49118e92":"code","7a04422a":"code","eb97fd9c":"code","9df7fe02":"code","a9e89e15":"code","c4716d9a":"code","43b178a8":"code","88154a20":"code","c5a2c797":"code","3a955de8":"code","6be56bd6":"code","46f2833d":"code","ebdfc020":"code","7deb0116":"code","bcfc2469":"code","8be8988d":"code","b82f4974":"code","fcbc07aa":"markdown","dca333c4":"markdown","8a02de84":"markdown","b6861693":"markdown","ee392287":"markdown","c3f67f9e":"markdown","6f96c95a":"markdown","59b47435":"markdown","369b3a4d":"markdown","817c52aa":"markdown","8361142a":"markdown","704714c5":"markdown","274fcdef":"markdown","ec99768b":"markdown","cf1c0e4a":"markdown","f84d2b41":"markdown","5151de50":"markdown","1598c3a0":"markdown","dc16c91d":"markdown","03571539":"markdown"},"source":{"5cae4aff":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime\nimport time\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn import ensemble\nfrom sklearn import tree\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.metrics import classification_report\nfrom matplotlib import pyplot as plt\nfrom datetime import datetime, date, time, timedelta\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.ticker as mtick\nimport scikitplot as skplt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn import svm\nplt.style.use('ggplot')","073f15fa":"# import and update table card\ncard = pd.read_csv(\n    \"..\/input\/card.asc\",\n    sep=\";\",\n    delimiter=None,\n    header=\"infer\",\n    names=None,\n    low_memory=False,\n)\ncard.issued = card.issued.str.strip(\"00:00:00\")\ncard.type = card.type.map({\"gold\": 2, \"classic\": 1, \"junior\": 0})\ncard.head()\n","e68f96d7":"# import and update table account\naccount = pd.read_csv(\n    \"..\/input\/account.asc\",\n    sep=\";\",\n    delimiter=None,\n    header=\"infer\",\n    names=None,\n)\naccount.date = account.date.apply(lambda x: pd.to_datetime(str(x), format=\"%y%m%d\"))\naccount.head()\n","ddc36929":"# import and update table disp\ndisp = pd.read_csv(\n    \"..\/input\/disp.asc\",\n    sep=\";\",\n    delimiter=None,\n    header=\"infer\",\n    names=None,\n    low_memory=False,\n)\ndisp = disp[disp.type == \"OWNER\"]\ndisp.rename(columns={\"type\": \"type_disp\"}, inplace=True)\ndisp.head()\n","2e63bc5e":"# import and update table client\nclient = pd.read_csv(\n    \"..\/input\/client.asc\",\n    sep=\";\",\n    delimiter=None,\n    header=\"infer\",\n    names=None,\n    low_memory=False,\n)\nclient[\"month\"] = client.birth_number.apply(\n    lambda x: x \/\/ 100 % 100, convert_dtype=True, args=()\n)\nclient[\"year\"] = client.birth_number.apply(\n    lambda x: x \/\/ 100 \/\/ 100, convert_dtype=True, args=()\n)\nclient[\"age\"] = 99 - client.year\nclient[\"sex\"] = client.month.apply(lambda x: (x - 50) < 0, convert_dtype=True, args=())\nclient.sex = client.sex.astype(int)  # 0 for female, 1 for male\nclient.drop([\"birth_number\", \"month\", \"year\"], axis=1, inplace=True)\nclient.head()\n","1cfc18cd":"# import and update table district\ndistrict = pd.read_csv(\n    \"..\/input\/district.asc\",\n    sep=\";\",\n    delimiter=None,\n    header=\"infer\",\n    names=None,\n    low_memory=False,\n)\ndistrict.drop([\"A2\", \"A3\"], axis=1, inplace=True)\ndistrict.head()\n","f235bb2a":"# import and update table order\norder = pd.read_csv(\n    \"..\/input\/order.asc\",\n    sep=\";\",\n    delimiter=None,\n    header=\"infer\",\n    names=None,\n    low_memory=False,\n)\norder.drop([\"bank_to\", \"account_to\", \"order_id\"], axis=1, inplace=True)\norder.k_symbol.fillna(\"No_symbol\")\norder.k_symbol = order.k_symbol.str.replace(\" \", \"No_symbol\")\norder = order.groupby([\"account_id\", \"k_symbol\"]).mean().unstack()\norder = order.fillna(0)\norder.columns = order.columns.droplevel()\norder.reset_index(level=\"account_id\", col_level=1, inplace=True)\norder.rename_axis(\"\", axis=\"columns\", inplace=True)\norder.rename(\n    index=None,\n    columns={\n        \"LEASING\": \"order_amount_LEASING\",\n        \"No_symbol\": \"order_amount_No_symbol\",\n        \"POJISTNE\": \"order_amount_POJISTNE\",\n        \"SIPO\": \"order_amount_SIPO\",\n        \"UVER\": \"order_amount_UVER\",\n    },\n    inplace=True,\n)\norder.head()\n","a442c694":"# import and update table loan\nloan = pd.read_csv(\n    \"..\/input\/loan.asc\",\n    sep=\";\",\n    delimiter=None,\n    header=\"infer\",\n    names=None,\n    low_memory=False,\n)\nloan.date = loan.date.apply(lambda x: pd.to_datetime(str(x), format=\"%y%m%d\"))\nloan.head()\n","453103a2":"# import and update table trans\ntrans = pd.read_csv(\n    \"..\/input\/trans.asc\",\n    sep=\";\",\n    delimiter=None,\n    header=\"infer\",\n    names=None,\n    low_memory=False,\n)\ntrans.loc[trans.k_symbol == \"\", \"k_symbol\"] = trans[\n    trans.k_symbol == \"\"\n].k_symbol.apply(lambda x: \"k_symbol_missing\")\ntrans.loc[trans.k_symbol == \" \", \"k_symbol\"] = trans[\n    trans.k_symbol == \" \"\n].k_symbol.apply(lambda x: \"k_symbol_missing\")\nloan_account_id = loan.loc[:, [\"account_id\"]]\ntrans = loan_account_id.merge(trans, how=\"left\", on=\"account_id\")\ntrans.date = trans.date.apply(lambda x: pd.to_datetime(str(x), format=\"%y%m%d\"))\ntrans.head()\n","d8fe76d9":"# create temp table trans_pv_k_symbol\ntrans_pv_k_symbol = trans.pivot_table(\n    values=[\"amount\", \"balance\"], index=[\"trans_id\"], columns=\"k_symbol\"\n)\ntrans_pv_k_symbol.fillna(0, inplace=True)\ntrans_pv_k_symbol.columns = [\"_\".join(col) for col in trans_pv_k_symbol.columns]\ntrans_pv_k_symbol = trans_pv_k_symbol.reset_index()\ntrans_pv_k_symbol = trans.iloc[:, :3].merge(\n    trans_pv_k_symbol, how=\"left\", on=\"trans_id\"\n)\ntrans_pv_k_symbol.head()\n","9685a57f":"# create temp table get_date_loan_trans\nget_date_loan_trans = pd.merge(\n    loan,\n    account,\n    how=\"left\",\n    on=\"account_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_loan\", \"_account\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\nget_date_loan_trans = pd.merge(\n    get_date_loan_trans,\n    trans,\n    how=\"left\",\n    on=\"account_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_account\", \"_trans\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n","5fe89124":"# update table get_date_loan_trans to get the date between loan_date and trans_date.\nget_date_loan_trans[\"date_loan_trans\"] = (\n    get_date_loan_trans.date_loan - get_date_loan_trans.date\n)\nget_date_loan_trans[[\"date_loan_trans\"]] = get_date_loan_trans[\n    [\"date_loan_trans\"]\n].astype(str)\nget_date_loan_trans.date_loan_trans = get_date_loan_trans.date_loan_trans.str.strip(\n    \" days 00:00:00.000000000\"\n)\nget_date_loan_trans.date_loan_trans = pd.to_numeric(\n    get_date_loan_trans.date_loan_trans.str.strip(\" days +\")\n)\nget_date_loan_trans.head()\n","2933eae6":"# create temp table temp_90_mean to create new feature\ntemp_90_mean = get_date_loan_trans[\n    (get_date_loan_trans[\"date_loan_trans\"] >= 0)\n    & (get_date_loan_trans[\"date_loan_trans\"] < 90)\n]\ntemp_90_mean = temp_90_mean.drop([\"trans_id\", \"k_symbol\"], axis=1)\ntemp_90_mean = temp_90_mean.groupby([\"loan_id\"], as_index=None).mean()\ntemp_90_mean = temp_90_mean.loc[:, [\"loan_id\", \"balance\"]]\ntemp_90_mean.rename(\n    index=None, columns={\"balance\": \"avg_balance_3M_befroe_loan\"}, inplace=True\n)\n","4a099fed":"# create temp table temp_30_mean to create new feature\ntemp_30_mean = get_date_loan_trans[\n    (get_date_loan_trans[\"date_loan_trans\"] >= 0)\n    & (get_date_loan_trans[\"date_loan_trans\"] < 30)\n]\ntemp_30_mean = temp_30_mean.drop([\"trans_id\", \"k_symbol\"], axis=1)\ntemp_30_mean = temp_30_mean.groupby([\"loan_id\"], as_index=None).mean()\ntemp_30_mean = temp_30_mean.loc[:, [\"loan_id\", \"balance\"]]\ntemp_30_mean.rename(\n    index=None, columns={\"balance\": \"avg_balance_1M_befroe_loan\"}, inplace=True\n)\n","b7c85148":"# create temp table temp_trans_freq to create new feature\ntemp_before = get_date_loan_trans[(get_date_loan_trans[\"date_loan_trans\"] >= 0)]\ntemp_trans_freq = (\n    temp_before.loc[:, [\"loan_id\", \"trans_id\"]]\n    .groupby([\"loan_id\"], as_index=None)\n    .count()\n)\ntemp_trans_freq.rename(index=None, columns={\"trans_id\": \"trans_freq\"}, inplace=True)\ntemp_before = temp_before.drop([\"trans_id\", \"k_symbol\"], axis=1)\n","e4362229":"# create temp table temp_balance_min & temp_balance_mean to create new features\ntemp_balance_min = (\n    temp_before.groupby([\"loan_id\"], as_index=None).min().loc[:, [\"loan_id\", \"balance\"]]\n)\ntemp_balance_min.rename(\n    index=None, columns={\"balance\": \"min_balance_befroe_loan\"}, inplace=True\n)\n\ntemp_balance_mean = (\n    temp_before.groupby([\"loan_id\"], as_index=None)\n    .mean()\n    .loc[:, [\"loan_id\", \"amount_trans\", \"balance\"]]\n)\ntemp_balance_mean.rename(\n    index=None,\n    columns={\n        \"amount_trans\": \"avg_amount_trans_before_loan\",\n        \"balance\": \"avg_balance_before_loan\",\n    },\n    inplace=True,\n)\n","0622f2d1":"# create temp table times_balance_below_500 & times_balance_below_5K to create new features\ntimes_balance_below_500 = temp_before[temp_before.balance < 500]\ntimes_balance_below_500 = (\n    times_balance_below_500.groupby([\"loan_id\"], as_index=None)\n    .count()\n    .loc[:, [\"loan_id\", \"balance\"]]\n)\ntimes_balance_below_500 = times_balance_below_500[times_balance_below_500.balance > 1]\ntimes_balance_below_500.rename(\n    index=str, columns={\"balance\": \"times_balance_below_500\"}, inplace=True\n)\n\ntimes_balance_below_5K = temp_before[temp_before.balance < 5000]\ntimes_balance_below_5K = (\n    times_balance_below_5K.groupby([\"loan_id\"], as_index=None)\n    .count()\n    .loc[:, [\"loan_id\", \"balance\"]]\n)\ntimes_balance_below_5K = times_balance_below_5K[times_balance_below_5K.balance > 1]\ntimes_balance_below_5K.rename(\n    index=str, columns={\"balance\": \"times_balance_below_5K\"}, inplace=True\n)\n","9f8f2988":"# create temp table merge_loan_trans to merge the temp features above into one temp table\nmerge_loan_trans = loan.merge(\n    temp_90_mean, how=\"left\", on=\"loan_id\", suffixes=(\"_loan\", \"_trans\")\n)\nmerge_loan_trans = merge_loan_trans.merge(temp_30_mean, how=\"left\", on=\"loan_id\")\nmerge_loan_trans = merge_loan_trans.merge(temp_trans_freq, how=\"left\", on=\"loan_id\")\nmerge_loan_trans = merge_loan_trans.merge(temp_balance_min, how=\"left\", on=\"loan_id\")\nmerge_loan_trans = merge_loan_trans.merge(temp_balance_mean, how=\"left\", on=\"loan_id\")\nmerge_loan_trans = merge_loan_trans.merge(\n    times_balance_below_500, how=\"left\", on=\"loan_id\"\n)\nmerge_loan_trans = merge_loan_trans.merge(\n    times_balance_below_5K, how=\"left\", on=\"loan_id\"\n)\n","bd6bc712":"loan_BorD = loan[(loan.status == \"D\") | (loan.status == \"B\")]\nlen(loan_BorD)\n","b0463129":"temp = times_balance_below_500.merge(\n    loan,\n    how=\"inner\",\n    on=\"loan_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_x\", \"_y\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n\ntemp.status.value_counts()\n","9189df16":"plt.plot(temp.status, temp.times_balance_below_500, \"ro\")\n","3c43b6a7":"temp.sort_values(\"times_balance_below_500\", ascending=False).plot(\n    x=\"status\", y=\"times_balance_below_500\", kind=\"bar\"\n)\n","b5d64019":"t = loan.loc[:, [\"payments\", \"status\"]]\nt.head(3)\n","9fa0aac4":"t = t.groupby([\"status\"], as_index=None).mean()\nt.plot(kind=\"bar\")\n","becabbaf":"df = pd.merge(\n    merge_loan_trans,\n    account,\n    how=\"left\",\n    on=\"account_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_loan\", \"_account\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n","4b499e2c":"df = pd.merge(\n    df,\n    order,\n    how=\"left\",\n    on=\"account_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_a\", \"_order\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n","e68bf652":"df = pd.merge(\n    df,\n    disp,\n    how=\"left\",\n    on=\"account_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_b\", \"_disp\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n","c57b626a":"df = pd.merge(\n    df,\n    card,\n    how=\"left\",\n    on=\"disp_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_c\", \"_card\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n","6036f1ff":"df = pd.merge(\n    df,\n    client,\n    how=\"left\",\n    on=\"client_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_d\", \"_client\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n","3111b852":"df = pd.merge(\n    df,\n    district,\n    how=\"left\",\n    left_on=\"district_id_client\",\n    right_on=\"A1\",\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_e\", \"_district\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n","4cd80340":"before_loan_date = get_date_loan_trans[(get_date_loan_trans[\"date_loan_trans\"] >= 0)]\nbefore_loan_date = before_loan_date.loc[:, [\"account_id\", \"trans_id\"]]\ntrans_pv_k_symbol = pd.merge(\n    before_loan_date,\n    trans_pv_k_symbol,\n    how=\"left\",\n    on=\"trans_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_before\", \"_df2\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\ntrans_pv_k_symbol.drop(\n    [\"account_id_df2\", \"date\", \"trans_id\"], axis=1, inplace=True\n)\ntrans_pv_k_symbol.rename(columns={\"account_id_before\": \"account_id\"}, inplace=True)\ntrans_pv_k_symbol = trans_pv_k_symbol.groupby(\n    by=\"account_id\", axis=0, as_index=False, sort=True, group_keys=True, squeeze=False\n).mean()\n","a0b977b1":"df = pd.merge(\n    df,\n    trans_pv_k_symbol,\n    how=\"left\",\n    on=\"account_id\",\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_df\", \"_tt\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n","d6775e43":"df[\"year_\"] = df.date_loan.apply(lambda x: x.year, convert_dtype=int, args=())\ndf[\"years_of_loan\"] = 1999 - df.year_\ndf.drop([\"date_loan\", \"year_\"], axis=1, inplace=True)\ndf.frequency = df.frequency.map(\n    {\"POPLATEK MESICNE\": 30, \"POPLATEK TYDNE\": 7, \"POPLATEK PO OBRATU\": 1}\n)\n","e59093a8":"df[\"year_\"] = df.date_account.apply(lambda x: x.year, convert_dtype=int, args=())\ndf[\"years_of_account\"] = 1999 - df.year_\ndf.drop([\"date_account\", \"year_\", \"type_disp\"], axis=1, inplace=True)\n","690ab669":"df.issued.fillna(\"999999\", inplace=True)\ndf[\"years_card_issued\"] = df.issued.apply(\n    lambda x: (99 - int(x[:2])), convert_dtype=int\n)\ndf.drop([\"issued\",\"A12\",\"A15\"], axis=1, inplace=True)\n","b2f9f622":"df.fillna(0, inplace=True)\n","7b80f7d4":"df.status.value_counts()\n","67bca145":"df.info()\n","06ac9b49":"m = {\"A\": 0, \"B\": 1, \"C\": 0, \"D\": 1}\ndf.status = df.status.map(m)\ndf.status.unique()\n","cecb3c15":"df = pd.get_dummies(df, drop_first=True)\n","3f204b5c":"df.columns.unique()","a42e8e2d":"df.drop(\n    [\n        \"loan_id\",\n        \"account_id\",\n        \"district_id_d\",\n        \"disp_id\",\n        \"client_id\",\n        \"card_id\",\n        \"district_id_client\",\n    ],\n    axis=1,\n    inplace=True,\n)\n","12df1360":"X = df.loc[:, df.columns != \"status\"]\ny = df.loc[:, \"status\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","de357387":"rf = ensemble.RandomForestClassifier(\n    n_estimators=200,\n    criterion=\"gini\",\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=\"auto\",\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=1,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n","0b318eec":"print(classification_report(y_test, y_pred))\n","a9d52be5":"skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n","11e47241":"fi = rf.feature_importances_\n","6ad40d18":"feature_cols = X_test.columns\nimportance = pd.DataFrame(\n    {\"feature\": feature_cols, \"importance\": rf.feature_importances_}\n)\n","4843dd19":"importance = pd.DataFrame(\n    {\"feature\": feature_cols[:], \"importance\": rf.feature_importances_[:]}\n)\nimportance.sort_values(\n    by=\"importance\",\n    axis=0,\n    ascending=False,\n    inplace=True,\n    kind=\"quicksort\",\n    na_position=\"last\",\n)\nimportance[:20].plot(x=\"feature\", y=\"importance\", kind=\"bar\")\n","b192d49c":"df.groupby([\"sex\", \"status\"])[\"status\"].size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind=\"bar\", stacked=True)\nplt.xlabel({\"1\": \"male\", \"0\": \"female\"})\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.ylabel(\"percentage - default or not\")\nplt.show()\n","7a1e302d":"df.groupby([\"sex\", \"status\"])[\"status\"].size()\n","3213be44":"df.groupby([\"age\", \"status\"])[\"status\"].size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind=\"hist\", stacked=True)\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.show()\n","59670e40":"df.groupby([\"times_balance_below_5K\", \"status\"])[\"status\"].size().groupby(\n    level=0\n).apply(lambda x: 100 * x \/ x.sum()).unstack().plot(kind=\"bar\", stacked=True)\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.ylabel(\"percentage - default or not\")\nplt.show()\n","ee391fc4":"df.groupby([\"years_card_issued\", \"status\"])[\"status\"].size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind=\"bar\", stacked=True)\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.ylabel(\"percentage - default or not\")\nplt.xlabel(\"years_visa_card_owned\")\nplt.legend(loc=1)\nplt.show()\n","a31bedfa":"df.hist(column=\"age\", by=\"status\", bins=20)\nplt.xlabel(\"age\")\nplt.ylabel(\"count_of_loan_id\")\n","e732b08f":"# Binning:\ndef binning(col, cut_points, labels=None):\n    # Define min and max values:\n    minval = col.min()\n    maxval = col.max()\n\n    # create list by adding min and max to cut_points\n    break_points = [minval] + cut_points + [maxval]\n\n    # if no labels provided, use default labels 0 ... (n-1)\n    if not labels:\n        labels = range(len(cut_points) + 1)\n\n    # Binning using cut function of pandas\n    colBin = pd.cut(col, bins=break_points, labels=labels, include_lowest=True)\n    return colBin\n\n\n# Binning age:\ncut_points = [24, 34, 44, 50]\nlabels = [\"20\", \"25\", \"35\", \"45\", \"50\"]\ndf[\"age_bin\"] = binning(df[\"age\"], cut_points, labels)\n","df3a367d":"df.groupby([\"age_bin\", \"status\"])[\"status\"].size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind=\"bar\", stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n\nplt.ylabel(\"percentage - default or not\")\n","73cc1f98":"df[df.status == 1].head()\n","467f1d53":"a = loan.groupby(\n    by=\"status\", axis=0, level=None, as_index=True, sort=True, group_keys=True\n)\na.payments.mean().plot(kind=\"bar\")\nplt.ylabel(\"payments\")\n","fd24ca97":"# plot heatmap\nimport seaborn as sns\n\ncols = list(importance.feature[:10])\ncols.insert(0, \"status\")\ncorrcoef_map = np.corrcoef(df[cols].values.T)\nfig, ax = plt.subplots(figsize=(12, 12))  # Sample figsize in inches\nhm = sns.heatmap(\n    corrcoef_map,\n    cbar=True,\n    annot=True,\n    square=True,\n    fmt=\".2f\",\n    annot_kws={\"size\": 15},\n    yticklabels=cols,\n    xticklabels=cols,\n    ax=ax,\n)\n","f3594dfe":"X = df.loc[:, df.columns != \"status\"]\ny = df.loc[:, \"status\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","1ee8236d":"rf = ensemble.RandomForestClassifier(\n    n_estimators=800,\n    criterion=\"gini\",\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=\"auto\",\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=1,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n","e75692b1":"print(classification_report(y_test, y_pred))\n","ff4d1c67":"skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n","8370a34b":"fi = rf.feature_importances_\n","6d827a05":"feature_cols = X_test.columns\nimportance = pd.DataFrame(\n    {\"feature\": feature_cols, \"importance\": rf.feature_importances_}\n)\n","0cfd4953":"feature_cols = X_test.columns\nimportance = pd.DataFrame(\n    {\"feature\": feature_cols[:], \"importance\": rf.feature_importances_[:]}\n)\nimportance.sort_values(\n    by=\"importance\",\n    axis=0,\n    ascending=False,\n    inplace=True,\n    kind=\"quicksort\",\n    na_position=\"last\",\n)\nimportance[:18].plot(x=\"feature\", y=\"importance\", kind=\"bar\")\nplt.ylabel(\"importance\")\n","ebd7ed2b":"clf = tree.DecisionTreeClassifier(\n    criterion=\"gini\",\n    splitter=\"best\",\n    max_depth=5,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    random_state=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    class_weight=None,\n    presort=False,\n)\n\nmodel = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n","b421b494":"print(classification_report(y_test, y_pred))\n","d027c389":"skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n","66192bc2":"feature_cols = X_test.columns\nimportance = pd.DataFrame(\n    {\"feature\": feature_cols[:], \"importance\": rf.feature_importances_[:]}\n)\nimportance.sort_values(\n    by=\"importance\",\n    axis=0,\n    ascending=False,\n    inplace=True,\n    kind=\"quicksort\",\n    na_position=\"last\",\n)\nimportance[:18].plot(x=\"feature\", y=\"importance\", kind=\"bar\")\n","49118e92":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(\n    loss=\"deviance\",\n    learning_rate=0.1,\n    n_estimators=200,\n    subsample=1.0,\n    criterion=\"friedman_mse\",\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_depth=3,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    init=None,\n    random_state=None,\n    max_features=None,\n)\nmodel = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n","7a04422a":"print(classification_report(y_test, y_pred))\n","eb97fd9c":"skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n","9df7fe02":"# Standard processing\nsc = StandardScaler()\nX.drop(['age_bin'], axis=1, inplace=True)\nX = sc.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","a9e89e15":"svc = svm.SVC(\n    C=5,\n    kernel=\"rbf\",\n    degree=3,\n    gamma=\"auto\",\n    coef0=0.0,\n    shrinking=True,\n    probability=False,\n    tol=0.001,\n    cache_size=200,\n    class_weight=None,\n    verbose=False,\n    max_iter=-1,\n    decision_function_shape=\"ovr\",\n    random_state=None,\n)\nmodel = svc.fit(X_train, y_train)\n\ny_pred = svc.predict(X_test)\n","c4716d9a":"print(classification_report(y_test, y_pred))\n","43b178a8":"skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n","88154a20":"lr = LogisticRegression(penalty=\"l1\", C=1).fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n","c5a2c797":"print(classification_report(y_test, y_pred))\n","3a955de8":"skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n","6be56bd6":"def plot_decision_boundary(model, X, y):\n    X_max = X.max(axis=0)\n    X_min = X.min(axis=0)\n    xticks = np.linspace(X_min[0], X_max[0], 100)\n    yticks = np.linspace(X_min[1], X_max[1], 100)\n    xx, yy = np.meshgrid(xticks, yticks)\n    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = ZZ >= 0.5\n    Z = Z.reshape(xx.shape)\n    fig, ax = plt.subplots()\n    ax = plt.gca()\n    ax.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha=0.6)\n    ax.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, alpha=0.6)\n","46f2833d":"X = df[[\"min_balance_befroe_loan\", \"times_balance_below_5K\"]]\ny = df[\"status\"]\n","ebdfc020":"rf = ensemble.RandomForestClassifier(\n    n_estimators=500,\n    criterion=\"gini\",\n    max_depth=4,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=\"auto\",\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=1,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nmodel = rf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nf1 = f1_score(y_pred, y_test)\nf1\n","7deb0116":"print(classification_report(y_test, y_pred))\n","bcfc2469":"skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n","8be8988d":"plot_decision_boundary(model, X_test, y_test)\nplt.xlabel(\"min_balance_befroe_loan\")\nplt.ylabel(\"times_balance_below_5K\")\n","b82f4974":"feature_cols = X_test.columns\nimportance = pd.DataFrame(\n    {\"feature\": feature_cols[:], \"importance\": rf.feature_importances_[:]}\n)\nimportance.sort_values(\n    by=\"importance\",\n    axis=0,\n    ascending=False,\n    inplace=True,\n    kind=\"quicksort\",\n    na_position=\"last\",\n)\nimportance[:18].plot(x=\"feature\", y=\"importance\", kind=\"bar\")\n","fcbc07aa":"# Gradient Boosting Classifier","dca333c4":"# Visualization & Feature Selection","8a02de84":"# Get label","b6861693":"# Merge tables","ee392287":"# Conclusion\nIn the bank loan default prediction, my assumption is banks want to control the loss to a acceptable level, without missing some good client. This means balance the customers be grouped as potential good\/bad customers, and their profiles will be checked carefully. \nBanks can detect the default behaviours in the earlier stage and conduct the corresponding actions to reduce the possible loss. That's why all of the new features I created are only based on the dates of the transfer must before the date that client apply for a loan.","c3f67f9e":"# Data Cleaning","6f96c95a":"# Try: Use only two features **to plot Decision Boundary","59b47435":"# Logistic Regression","369b3a4d":"# Create X, y, training\/testing set split","817c52aa":"# Standard processing and Training\/Test set Split","8361142a":"# Introduction\nOnce upon a time, there was a bank offering services to private persons. The services include managing of accounts, offering loans, etc. The bank wants to improve their services by finding interesting groups of clients (e.g. to differentiate between good and bad clients). The bank managers have only vague idea, who is good client (whom to offer some additional services) and who is bad client (whom to watch carefully to minimize the bank looses). Fortunately, the bank stores data about their clients, the accounts (transactions within several months), the loans already granted, the credit cards issued So the bank managers hope to find some answers (and questions as well) by analyzing this data. \nIn this kernel, I am going to follow the basic process of loan default prediction with machine learning algorithms.\n## Task description\nIn this data challenge, I'm going to work with 8 datasets from the bank (dataset was collected from year of 1999). As a data scientist, we will need to perform the following tasks:\n* Use Python to connect to files and read tables into Pandas dataframes\n* Preprocess data for machine learning\n* Train a ML model to predict customers who are more likely to default on loans\n* Evaluate model performance\n* Try to understand the key predictors of default","704714c5":"# Data Preparation","274fcdef":"# Decision Tree","ec99768b":"## Import and Update tables","cf1c0e4a":"# Feature Scaling - for SVM & Logistic Regression","f84d2b41":"# Model Evaluation","5151de50":"# Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and prepare the data. ","1598c3a0":"# Draft Modeling: Random Forest","dc16c91d":"# SVM","03571539":"# Modeling: Random Forest"}}