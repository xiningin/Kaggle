{"cell_type":{"ad47ba0c":"code","5bd11795":"code","6ad2f0c9":"code","191b6af5":"code","63a87ae9":"code","f4595540":"code","0014ad53":"code","e186e9c5":"code","3376839d":"code","356cacdc":"code","e4d6828d":"code","c74ba331":"code","69f55280":"code","c38832d8":"code","a894821f":"code","2fc43cb1":"code","4254bc53":"code","a53fd811":"code","44c3c24d":"code","0a0f0de4":"code","14ff9607":"code","29ad4802":"code","8bb48228":"code","fc57ed65":"code","329e63eb":"code","da07ca08":"code","ba2f71fc":"code","44f1b76d":"code","7bce4785":"code","8fe35165":"code","470fccdc":"code","4c8e7dc7":"code","25acdea2":"code","5af6e444":"code","943c20b3":"code","d1b9ef97":"code","1e3cf691":"code","902c08a4":"code","b74f2903":"code","8e3b7652":"code","48e37e5d":"code","fb900774":"code","bad69122":"code","76d9a993":"code","a88d8f20":"code","2822a47e":"code","95f8fb5c":"code","f94ae19d":"code","f498dc44":"code","7b87ee5f":"code","394076e8":"code","e9d257bd":"markdown","2db33a82":"markdown","8853ecbb":"markdown","cf3c4b2c":"markdown","0bc1f3f9":"markdown","6dfe2a8b":"markdown","7e3b6642":"markdown","5b9ec9b3":"markdown","8faa30da":"markdown","f0601602":"markdown","7fd206d8":"markdown","dbd7083e":"markdown","d118cf3e":"markdown","9d5fc652":"markdown","0aaacfc2":"markdown","7afcaf2e":"markdown","713eab31":"markdown","d9bf96c7":"markdown","27182b6c":"markdown","d39d7d64":"markdown","b6be095e":"markdown","c02d2c9c":"markdown","09ac2116":"markdown","2655a897":"markdown","b41b571d":"markdown","78cc8455":"markdown","8ec7ccf0":"markdown","52c23ef6":"markdown","33ba33c4":"markdown","7b71f2e1":"markdown","330f3d71":"markdown","08e8c266":"markdown","de3a5fa5":"markdown","921263e7":"markdown","5196fdeb":"markdown","5e9ed1bd":"markdown","4a11d7e7":"markdown","7cbce2a8":"markdown","a8fa2738":"markdown","d90a42b6":"markdown","b5ba0bd9":"markdown","0173caa4":"markdown","22251302":"markdown","b4b15e1b":"markdown","82149e35":"markdown","5dff8b43":"markdown","1b5b91bf":"markdown","1d7ceab3":"markdown","9748eeb8":"markdown","232e0d8c":"markdown","a26b0724":"markdown","6bd61e46":"markdown","55a0d0f9":"markdown","53e28c2e":"markdown","bddf2bff":"markdown","609ac676":"markdown","22daf820":"markdown","e1776fc8":"markdown","40025ad7":"markdown"},"source":{"ad47ba0c":"import numpy as np\nimport pandas as pd\nimport glob\nimport random \n\n# image\nfrom PIL import Image\n\n# visu\nimport matplotlib.pyplot as plt\n\n# sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n#tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.models import load_model","5bd11795":"filelist = glob.glob('\/kaggle\/input\/100-bird-species\/valid\/*')\nspecies = [fname.split(\"\/\")[-1] for fname in filelist]\nspecies[0:5] + [\"...\"]","6ad2f0c9":"datasets = [\"train\", \"test\", \"valid\"]","191b6af5":"im_width = round(224\/4)\nim_height = round(224\/4)\nprint(\"Image new width: \" + str(im_width))\nprint(\"Image new height: \" + str(im_height))","63a87ae9":"%%time\n\ndata = dict({\"train\": [], \"test\": [], \"valid\": []})\ntarget = dict({\"train\": [], \"test\": [], \"valid\": []})\n\nfor set_ in datasets:\n    for spec in species:\n        filelist = glob.glob('\/kaggle\/input\/100-bird-species\/' + set_ + '\/' + spec + '\/*.jpg')\n        target[set_].extend([spec for _ in filelist])\n        data[set_].extend([np.array(Image.open(fname).resize((im_width, im_height))) for fname in filelist])\n    data[set_] = np.stack(data[set_], axis=0)","f4595540":"print(\"train set shape: \" + str(data[\"train\"].shape))\nprint(\"test set shape: \" + str(data[\"test\"].shape))\nprint(\"validation set shape: \" + str(data[\"valid\"].shape))","0014ad53":"fig = plt.figure(figsize=(20,15))\ngs = fig.add_gridspec(4, 4)\n#\nfor row in range(0, 3):\n    for col in range(0, 3):\n        num_image = random.randint(0, data[\"train\"].shape[0])\n        ax = fig.add_subplot(gs[row, col])\n        ax.axis('off');\n        ax.set_title(target[\"train\"][num_image], fontdict={\"fontweight\": 700})\n        ax.imshow(data[\"train\"][num_image]);","e186e9c5":"print(data[\"train\"].max())\nprint(data[\"train\"].min())","3376839d":"data_norm = data.copy()","356cacdc":"data_norm[\"train\"] = np.round((data_norm[\"train\"]\/255), 3)\ndata_norm[\"test\"] = np.round((data_norm[\"test\"]\/255), 3)\ndata_norm[\"valid\"] = np.round((data_norm[\"valid\"]\/255), 3)","e4d6828d":"fig = plt.figure(figsize=(20,15))\ngs = fig.add_gridspec(4, 4)\n#\nfor row in range(0, 3):\n    for col in range(0, 3):\n        num_image = random.randint(0, data_norm[\"train\"].shape[0])\n        ax = fig.add_subplot(gs[row, col])\n        ax.axis('off');\n        ax.set_title(target[\"train\"][num_image], fontdict={\"fontweight\": 700})\n        ax.imshow(data_norm[\"train\"][num_image]);","c74ba331":"encoder = LabelEncoder().fit(target[\"train\"])","69f55280":"target_cat = target.copy()","c38832d8":"target_cat[\"train\"] = encoder.transform(target_cat[\"train\"])\ntarget_cat[\"test\"] = encoder.transform(target_cat[\"test\"])\ntarget_cat[\"valid\"] = encoder.transform(target_cat[\"valid\"])","a894821f":"target_ohe = target_cat.copy()","2fc43cb1":"target_ohe[\"train\"] = to_categorical(target_ohe[\"train\"])\ntarget_ohe[\"test\"] = to_categorical(target_ohe[\"test\"])\ntarget_ohe[\"valid\"] = to_categorical(target_ohe[\"valid\"])","4254bc53":"pd.DataFrame(target_ohe[\"test\"]).head()","a53fd811":"def initialize_model(name):\n    model = Sequential(name=name)\n    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(im_height, im_width, 3), padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(2, 2)))\n    model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(3, 3)))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(120, activation='relu'))\n    model.add(layers.Dense(60, activation='relu'))\n    model.add(layers.Dropout(rate=0.2))\n    model.add(layers.Dense(315, activation='softmax'))\n\n    return model","44c3c24d":"model = initialize_model(name=\"baseline\")\nmodel.summary()","0a0f0de4":"def compile_model(model):\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=\"accuracy\")\n    return model","14ff9607":"model_baseline = initialize_model(name=\"baseline\")\nmodel_baseline = compile_model(model_baseline)\n#\ncallback = [EarlyStopping(patience=5, monitor='val_accuracy', restore_best_weights=True),\n            ReduceLROnPlateau(monitor = 'val_loss', patience = 1, factor=0.5, verbose=1)]\n#\nhistory_baseline = model_baseline.fit(data_norm[\"train\"], target_ohe[\"train\"],\n                                      batch_size=16,\n                                      epochs=1000,\n                                      validation_data=(data_norm[\"valid\"], target_ohe[\"valid\"]),\n                                      callbacks=callback)","29ad4802":"def plot_history(history, title='', axs=None, exp_name=\"\"):\n    if axs is not None:\n        ax1, ax2 = axs\n    else:\n        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    \n    if len(exp_name) > 0 and exp_name[0] != '_':\n        exp_name = '_' + exp_name\n    ax1.plot(history.history['loss'], label='train' + exp_name)\n    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n    #ax1.set_ylim(0., 2.2)\n    ax1.set_title('loss')\n    ax1.legend()\n\n    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n    #ax2.set_ylim(0.25, 1.)\n    ax2.set_title('Accuracy')\n    ax2.legend()\n    return (ax1, ax2)","8bb48228":"plot_history(history_baseline, title='', axs=None, exp_name=\"\");","fc57ed65":"model_baseline.evaluate(data_norm[\"test\"], target_ohe[\"test\"], verbose=0)","329e63eb":"y_pred = [np.argmax(val) for val in list(model_baseline.predict(data_norm[\"test\"]))]\ny_true = target_cat[\"test\"].copy()","da07ca08":"fig = plt.figure(figsize=(20,15))\ngs = fig.add_gridspec(4, 4)\n#\nfor row in range(0, 3):\n    for col in range(0, 3):\n        num_image = random.randint(0, data_norm[\"test\"].shape[0])\n        ax = fig.add_subplot(gs[row, col])\n        ax.axis('off');\n        ax.set_title(\"True specie: \" + str(target[\"test\"][y_true[num_image]] + \"\\nPredicted specie: \" + target[\"test\"][y_pred[num_image]]), fontdict={\"fontweight\": 700})\n        ax.imshow(data[\"test\"][num_image]);","ba2f71fc":"datagen = ImageDataGenerator(rescale=1\/255)\n#\ntrain_generator = datagen.flow_from_directory('\/kaggle\/input\/100-bird-species\/train',\n                                               batch_size=64,\n                                               target_size=(224,224)\n                                             )\n#\nvalidation_generator = datagen.flow_from_directory('\/kaggle\/input\/100-bird-species\/valid',\n                                                   batch_size=64,\n                                                   target_size=(224,224)\n                                                  )","44f1b76d":"fig = plt.figure(figsize=(20,15))\ngs = fig.add_gridspec(4, 4)\n#\nfor row in range(0, 3):\n    for col in range(0, 3):\n        image, label = train_generator.next()\n        label_id = list(label[0]).index(1)\n        label_str = list(train_generator.class_indices.keys())[list(train_generator.class_indices.values()).index(label_id)]\n        ax = fig.add_subplot(gs[row, col])\n        ax.axis('off');\n        ax.set_title(label_str, fontdict={\"fontweight\": 700})\n        ax.imshow(image[0]);","7bce4785":"def initialize_model_da(name):\n    model_da = Sequential(name=name)\n    model_da.add(layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(224, 224, 3), padding='same'))\n    model_da.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n    model_da.add(layers.Dropout(rate=0.4))\n    model_da.add(layers.BatchNormalization())\n    model_da.add(layers.MaxPool2D(pool_size=(2, 2)))\n    model_da.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n    model_da.add(layers.MaxPool2D(pool_size=(2, 2)))\n    model_da.add(layers.MaxPool2D(pool_size=(2, 2)))\n    model_da.add(layers.Dropout(rate=0.5))\n    model_da.add(layers.BatchNormalization())\n    model_da.add(layers.Flatten())\n    model_da.add(layers.Dense(512, activation='relu'))\n    model_da.add(layers.Dropout(rate=0.5))\n    model_da.add(layers.BatchNormalization())\n    model_da.add(layers.Dense(315, activation='softmax'))\n    \n    return model_da","8fe35165":"model_da = initialize_model_da(name=\"data-aug\")\nmodel_da.summary()","470fccdc":"def compile_model_da(model):\n    model_da.compile(optimizer='adam',\n                     loss='categorical_crossentropy',\n                     metrics=\"accuracy\")\n    return model_da","4c8e7dc7":"model_da = initialize_model_da(name=\"data-aug\")\nmodel_da = compile_model_da(model_da)\n#\ncallback = [EarlyStopping(patience=5, monitor='val_accuracy', restore_best_weights=True),\n            ReduceLROnPlateau(monitor = 'val_loss', patience = 1, factor=0.5, verbose=1)]\n\nhistory_da = model_da.fit(train_generator,\n                          epochs=25,\n                          validation_data=validation_generator,\n                          callbacks=callback)","25acdea2":"plot_history(history_da, title='', axs=None, exp_name=\"\");","5af6e444":"test_data_da = datagen.flow_from_directory('\/kaggle\/input\/100-bird-species\/test',\n                                           target_size=(224,224),\n                                           batch_size=64)","943c20b3":"model_da.evaluate(test_data_da, verbose=1)","d1b9ef97":"datagen_vgg = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                 zoom_range=(0.8, 1.2)) \n#\ntrain_generator_vgg = datagen_vgg.flow_from_directory('\/kaggle\/input\/100-bird-species\/train',\n                                                      batch_size=128,\n                                                      target_size=(224,224),\n                                                      class_mode='categorical')\n#\nvalidation_generator_vgg = datagen_vgg.flow_from_directory('\/kaggle\/input\/100-bird-species\/valid',\n                                                           batch_size=128,\n                                                           target_size=(224,224),\n                                                           class_mode='categorical')","1e3cf691":"fig = plt.figure(figsize=(20,15))\ngs = fig.add_gridspec(4, 4)\n#\nfor row in range(0, 3):\n    for col in range(0, 3):\n        image, label = train_generator_vgg.next()\n        label_id = list(label[0]).index(1)\n        label_str = list(train_generator_vgg.class_indices.keys())[list(train_generator_vgg.class_indices.values()).index(label_id)]\n        ax = fig.add_subplot(gs[row, col])\n        ax.axis('off');\n        ax.set_title(label_str, fontdict={\"fontweight\": 700})\n        ax.imshow(image[0]);","902c08a4":"model_VGG = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\nmodel_VGG.summary()","b74f2903":"model_VGG.trainable = False\nmodel_VGG.summary()","8e3b7652":"flattening_layer = layers.Flatten()\ndropout = layers.Dropout(rate=0.4)\nnorm = layers.BatchNormalization()\nlast_layer = layers.Dense(315, activation='softmax')\n\nmodel_VGG = Sequential([model_VGG, flattening_layer, dropout, norm, last_layer])\n\nmodel_VGG.summary()","48e37e5d":"model_VGG.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=\"accuracy\")","fb900774":"callback = [EarlyStopping(patience=5, monitor='val_accuracy', restore_best_weights=True),\n            ReduceLROnPlateau(monitor = 'val_loss', patience = 1, factor=0.5, verbose=1)]\n\nhistory_VGG = model_VGG.fit(train_generator_vgg,\n                            epochs=15,\n                            validation_data=validation_generator_vgg,\n                            callbacks=callback)","bad69122":"plot_history(history_VGG, title='', axs=None, exp_name=\"\");","76d9a993":"test_data_VGG = datagen_vgg.flow_from_directory('\/kaggle\/input\/100-bird-species\/test',\n                                                target_size=(224,224),\n                                                batch_size=64)","a88d8f20":"model_VGG.evaluate(test_data_VGG, verbose=1)","2822a47e":"model_VGG.save(\"model_VGG.h5\")","95f8fb5c":"model_loaded = load_model(\"model_VGG.h5\")","f94ae19d":"%%time\n# Loading test images\n\ndata_pred = dict({\"test\": []})\ntarget_pred = dict({\"test\": []})\n\nfor spec in species:\n    filelist = glob.glob('\/kaggle\/input\/100-bird-species\/test\/' + spec + '\/*.jpg')\n    target_pred[\"test\"].extend([spec for _ in filelist])\n    data_pred[\"test\"].extend([np.array(Image.open(fname)).reshape(-1, 224, 224, 3) for fname in filelist])\ndata_pred[\"test\"] = np.stack(data_pred[\"test\"], axis=0)","f498dc44":"data_prepro = np.array(list(map(preprocess_input, data_pred[\"test\"])))","7b87ee5f":"classes_indices = train_generator_vgg.class_indices\nclasses_indices","394076e8":"fig = plt.figure(figsize=(20,15))\ngs = fig.add_gridspec(4, 4)\n#\nfor row in range(0, 3):\n    for col in range(0, 3):\n        image_id_to_predict = random.randint(0, data_pred[\"test\"].shape[0])\n        image_to_predict = data_prepro[image_id_to_predict]\n        prediction = model_loaded.predict(image_to_predict)\n        image_predicted_id = np.argmax(prediction[0])\n        label_predicted = list(classes_indices.keys())[list(classes_indices.values()).index(image_predicted_id)]\n        label_true = target_pred[\"test\"][image_id_to_predict]\n        ax = fig.add_subplot(gs[row, col])\n        ax.axis('off');\n        ax.set_title(\"True specie: \" + label_true + \"\\nPredicted specie: \" + label_predicted, fontdict={\"fontweight\": 700})\n        ax.imshow(data_pred[\"test\"][image_id_to_predict].reshape(224, 224, 3));","e9d257bd":"<b>We can see as expected there are some fails with 61% accuracy.<\/b>","2db33a82":"## 2.3. Convolutionnal neural network","8853ecbb":"<b>So we have:<\/b><br>\n<ul>\n    <li><b>A train set of 45278 tensor images<\/b><\/li>\n    <li><b>A test set of 1550 tensor images<\/b><\/li>\n    <li><b>A validation set of 1550 tensor images<\/b><\/li>\n    <\/ul>\n    <b>Each image is of size 56 x 56 and each of their pixels is defined by three colors R, G, B.<\/b>","cf3c4b2c":"<b>We can have an overview of some random resized images and the associated specie:<\/b>","0bc1f3f9":"<b>And now, we convert the result to one-hot encoded target so that they can be used to train a classification neural network. We use<\/b> `to_categorical` <b>from tensorflow library:<\/b>","6dfe2a8b":"<b>So we have an accuracy on unseen data of almost 61%.<\/b><br><b>Let's see some random images associated with their true label and predicted label:<\/b>","7e3b6642":"## 4.4. Prediction","5b9ec9b3":"<b> In this chapter we will train a model directly from the pictures and see what accuracy we can obtain.<\/b>","8faa30da":"<b>So with the data augmentation, we have increased the accuracy from 61% to 66%.<\/b>","f0601602":"<b>As described in the dataset documentation, all the pictures are of size 224 x 224. Using the pictures as they are without resizing them will result in kernel crash due to memory limitations. Let's load the pictures after having resized them lower. Here we divide their size by 4.<\/b>","7fd206d8":"<b>Let's load the pictures:<\/b>","dbd7083e":"# 2. Model baseline","d118cf3e":"<b>Then to predict a new image using this model trained this way, we need to apply to the new images to predict the same preprocessing steps we used for training the model. First let's load the test images:<\/b>","9d5fc652":"# 4. Transfer learning VGG16 CNN model","0aaacfc2":"Now, let's define the Convolutional Neural Network.\n\nThe CNN that is composed of:\n\n\u25fc\ufe0f Conv2D layer with 32 filters, a kernel size of (3, 3), the relu activation function, a padding equal to same and the correct input_shape<br>\n\u25fc\ufe0f MaxPooling2D layer with a pool size of (2, 2)<br>\n\u25fc\ufe0f Conv2D layer with 64 filters, a kernel size of (3, 3), the relu activation function, and a padding equal to same<br>\n\u25fc\ufe0f MaxPooling2D layer with a pool size of (2, 2)<br>\n\u25fc\ufe0f Conv2D layer with 128 filters, a kernel size of (3, 3), the relu activation function, and a padding equal to same<br>\n\u25fc\ufe0f MaxPooling2D layer with a pool size of (3, 3)<br>\n\u25fc\ufe0f Flatten layer<br>\n\u25fc\ufe0f dense function with 120 neurons with the relu activation function<br>\n\u25fc\ufe0f dense function with 60 neurons with the relu activation function<br>\n\u25fc\ufe0f dropout layer (with a rate of 0.5), to regularize the network<br>\n\u25fc\ufe0f dense function related to the task: multiclass (315) classification > softmax","7afcaf2e":"<b>Here we convert targets first from string to numerical values, each category becoming an integer, from 0 to 315 (as there are 315 different bird species to classify):<\/b>","713eab31":"### Normalization","d9bf96c7":"## 4.2. Model","27182b6c":"<b>Here we are going to use <\/b>`ImageDataGenerator`<b> which generate batches of tensor image data with real-time data augmentation, to see by how much we can increase the accuracy of the previous model.<\/b><br><br>\u25fc\ufe0f In this case, the only parameter used for `ImageDataGenerator` is `recale` which is the rescaling factor. It multiplies the data by the value provided, here 1\/255 which is normalisation (in this case, not using `rescale=1\/255` won't work at all ...) <br>\u25fc\ufe0f the batch sizes are 64<br>\u25fc\ufe0f the target size is set to image original size 224 x 224","d39d7d64":"<b>As expected with 95% accuracy, we have almost all predictions exacts!<\/b>","b6be095e":"<b>Here again, we can check the normalized pictures randomly:<\/b>","c02d2c9c":"<b>Let's prepare a model to train from the image generator.<\/b><br>\nThe CNN is composed of:\n\n\u25fc\ufe0f Conv2D layer with 64 filters, a kernel size of (3, 3), the relu activation function, a padding equal to same and the correct input_shape<br>\n\u25fc\ufe0f Another Conv2D layer with 64 filters, a kernel size of (3, 3) and the relu activation function<br>\n\u25fc\ufe0f a dropout layer (with a rate of 0.4), to regularize the network<br>\n\u25fc\ufe0f batch normalization layer which applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1<br>\n\u25fc\ufe0f a MaxPooling2D layer with a pool size of (2, 2)<br>\n\u25fc\ufe0f Another Conv2D layer with 64 filters, a kernel size of (3, 3) and the relu activation function<br>\n\u25fc\ufe0f Two more MaxPooling2D layer with a pool size of (2, 2)<br>\n\u25fc\ufe0f a dropout layer (with a rate of 0.5), to regularize the network<br>\n\u25fc\ufe0f another batch normalization layer<br>\n\u25fc\ufe0f Flatten layer<br>\n\u25fc\ufe0f dense function with 512 neurons with the relu activation function<br>\n\u25fc\ufe0f a dropout layer (with a rate of 0.5), to regularize the network<br>\n\u25fc\ufe0f another batch normalization layer<br>\n\u25fc\ufe0f dense function related to the task: multiclass (315) classification > softmax","09ac2116":"<b>Applying on all train, test and validation sets:<\/b>","2655a897":"<b>After the long training, the model can be saved with the<\/b> `save` <b>method:<\/b>","b41b571d":"<b>Here we compile the model:<\/b>","78cc8455":"## 2.1. Loading data","8ec7ccf0":"# 2.2. Preparing the data","52c23ef6":"## 3.3. Results","33ba33c4":"<b>There are three sets already defined, the train set \/ test set \/ validation set:<\/b>","7b71f2e1":"<b>Here we are going to use the pretrained model VGG16 to see if we can increase accuracy.<\/b>","330f3d71":"<b>First we load the VGG16 model:<\/b>","08e8c266":"<b>With the pretrained model and data augmentation, we reach the accuracy of 95% on test set \u2604\ufe0f.<\/b>","de3a5fa5":"# 1. Imports","921263e7":"<b>We can have an overview of images from the data generator:<\/b>","5196fdeb":"## 3.1. Prepare image flow","5e9ed1bd":"<b>Thank you for reading! if you have any suggestion of improvment or if you find some mistakes please feel free to comment.<\/b><br>\n<div style=\"color:royalblue; font-weight:700\">Please upvote if you like the notebook ! \ud83c\udf40\u2604\ufe0f<\/div>","4a11d7e7":"# 3. Model improved with data augmentation","7cbce2a8":"<b>In the <\/b> `train_generator_vgg`, <b>we have a dictionnary containing the string classes as keys and the associated integer as value. The model has been trained based on this key\/value association. Now are going to use it to display the predicted species:<\/b>","a8fa2738":"## 3.2. Model","d90a42b6":"## 4.3. Results","b5ba0bd9":"<b>There are 315 species pictures organized in as many folders:<\/b>","0173caa4":"<b> Here the same parameters used for previous training are used. As the training is very long, the epoch limit is set to 25.<\/b>","22251302":"<b>Now we add the layers specific to our classification task:<\/b><br><br>\n\u25fc\ufe0f Flatten layer<br>\n\u25fc\ufe0f a dropout layer (with a rate of 0.4), to regularize the network<br>\n\u25fc\ufe0f a batch normalization layer<br>\n\u25fc\ufe0f dense function related to the task: multiclass (315) classification > softmax","b4b15e1b":"### Target encoding","82149e35":"<b>And train the model with the defined architecture, we keep the same parameters as before except the number of epoch which is set to 15 because the training is very long.<\/b>","5dff8b43":"<b>As the sets are already set for us, we don't need to use<\/b> `train_test_split`.","1b5b91bf":"<b>Let's have a look at random images from the data generator:<\/b>","1d7ceab3":"<b>Here we use again <\/b>`ImageDataGenerator`<b>.<\/b><br><br>\u25fc\ufe0f In this case, we use the parameter `preprocessing_function` set to `preprocess_input` which is the preprocessing function applied to images used to train the VGG16 model, thus we have on our images the same preprocessing<br>\u25fc\ufe0f we also use `zoom_range` set between 0.8 and 1.2<br>\u25fc\ufe0f the batch sizes are 128<br>\u25fc\ufe0f the target size is set to image original size 224 x 224","9748eeb8":"## 4.1. Prepare image flow ","232e0d8c":"<b>We see that, although we have a slightly better accuracy than the previous model, we can't really improve it with this method...<\/b> ","a26b0724":"<b>Fitting the encoder on train set:<\/b>","6bd61e46":"<b>And then reloaded with the<\/b> `load_model` <b>function:<\/b>","55a0d0f9":"<b>So let's plot random images from test set with the true specie associated to it alongside with the predicted specie from the model's prediction to see how everything is going:<\/b>","53e28c2e":"<b>And now let's apply the preprocessing on them:<\/b>","bddf2bff":"## 2.4. Results","609ac676":"<b>Here let's the following parameters:<\/b><br> \u25fc\ufe0f an early stopping after 5 epochs and set the parameter <\/b>`restore_best_weights` to `True` so that the weights of best score on monitored metric - here `val_accuracy` (accuracy on test set) - are restored when training stops. This way the model has the best accuracy possible on unseen data.<br>\u25fc\ufe0f add the `ReduceLROnPlateau` parameter set on `val_loss` with a `patience` parameter set on 1 and a `factor` parameter set on 0.5 so that the learning rate is reduced by 2 whenever the `val_loss` parameter starts increasing","22daf820":"<div style=\"display: block; text-align:center; height: 300px; overflow:hidden;\">\n    <img src=\"https:\/\/imgur.com\/tEGJhKz.jpg\" style=\"top: 0px;border-radius: 20px; height:100%\">\n<\/div>\n<div style=\"display:block; text-align:center;margin-top:20px; font-variant:small-caps; font-size:25pt; font-weight:bold\">315 Bird Species - Classification<\/div>","e1776fc8":"<b>To ease the convergence of the algorithm, it is usefull to normalize the data. We have color values between 0 and 255, we are going to set them between 0 and 1 by dividing by 255.<\/b>","40025ad7":"<b>There are a large amount of parameters and we set them non-trainable:<\/b>"}}