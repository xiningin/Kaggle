{"cell_type":{"1cbdad47":"code","f85c4135":"code","8f86c875":"code","398c659c":"code","87783bb1":"code","1b68bf41":"code","ca6521e8":"code","8f54f083":"code","cd76a9f4":"code","8052e351":"code","1981d3b3":"code","daf5a73f":"code","6f7e1e98":"code","69961055":"code","90ac06f7":"code","99c82e48":"code","f4383752":"code","3c5a14a5":"code","e12a1d6e":"code","5fadda86":"code","5d3c946d":"code","96c2ebda":"code","6bcf7bad":"code","9214b9fd":"code","38045ae6":"code","c92b813b":"code","d548c6d6":"code","53bdeb1f":"code","36d5dd34":"code","c25906f1":"code","cbf9c043":"code","e76c8231":"code","0734f2ff":"code","cae7c019":"code","104c128b":"code","2947b86f":"code","d4720e3f":"code","3827e244":"markdown","35341518":"markdown","d0392756":"markdown","7790679f":"markdown","bce1a088":"markdown","924cebed":"markdown","19e4be95":"markdown","56d044e3":"markdown","2cae0420":"markdown","33b1a526":"markdown","c7adee43":"markdown","b444862b":"markdown","c0814b38":"markdown","aaf0c044":"markdown","4fe33a23":"markdown","caad170c":"markdown","d46dda8a":"markdown","1c921424":"markdown","443a6f47":"markdown"},"source":{"1cbdad47":"# Essentials\nimport numpy                 as np\nimport pandas                as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn               as sns\nimport matplotlib.pyplot     as plt\n\n# Feature Engineering\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition     import PCA\nfrom sklearn.decomposition     import FastICA\nfrom sklearn.decomposition     import TruncatedSVD\n\n# Models\nfrom sklearn.ensemble        import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge    import KernelRidge\nfrom sklearn.linear_model    import Ridge, RidgeCV, Lasso, LassoCV\nfrom sklearn.linear_model    import ElasticNet, ElasticNetCV\nfrom sklearn.svm             import SVR\nfrom sklearn.tree            import DecisionTreeRegressor\nfrom mlxtend.regressor       import StackingCVRegressor\nimport lightgbm              as     lgb\nfrom lightgbm                import LGBMRegressor\nfrom xgboost                 import XGBRegressor\n\n# Stats\nfrom scipy.stats             import skew, norm\nfrom scipy.special           import boxcox1p\nfrom scipy.stats             import boxcox_normmax \n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics         import r2_score\nfrom sklearn.metrics         import mean_squared_error\nfrom sklearn.preprocessing   import OneHotEncoder\nfrom sklearn.preprocessing   import LabelEncoder\nfrom sklearn.pipeline        import make_pipeline, Pipeline\nfrom sklearn.preprocessing   import scale\nfrom sklearn.preprocessing   import StandardScaler\nfrom sklearn.preprocessing   import RobustScaler\npd.set_option('display.max_columns', None)\n# RobustScaler \n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows      = 8000","f85c4135":"testing_set_scores = pd.DataFrame(data=[\n            ['predection_output_stack_gen.csv (the final stacked model)', 0.55600],     \n            ['prediction_abr_lasso.csv', 0.54510], \n            ['prediction_abr_tree.csv', 0.54816], \n            ['prediction_gbr.csv', 0.54475], \n            ['prediction_gbr_2.csv', 0.54645], \n            ['prediction_lasso.csv', 0.54711], \n            ['prediction_lgm.csv', 0.54998], \n            ['prediction_lightgbm_2.csv', 0.54869],\n            ['prediction_lightgbm_goss.csv', 0.55056],\n            ['prediction_rf.csv', 0.55127], \n            ['prediction_rf_2.csv', 0.55164], \n            ['prediction_ridge.csv', 0.54308] \n        ],columns=['model','score']).sort_index(axis=0, ascending=False)\nplt.barh(testing_set_scores['model'], testing_set_scores['score'])\nplt.xlim(0.5405,0.5576)\nfor score, model in zip(testing_set_scores['score'], testing_set_scores['model']):\n    plt.text(x=score, y=model, s=score, ha='left', va='center', fontsize=9)","8f86c875":"# Data files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Load\ntrain = pd.read_csv('\/kaggle\/input\/train.csv', index_col=0)   #Y, Features\ntest  = pd.read_csv('\/kaggle\/input\/test.csv', index_col=0)    #Features\nprint(\"train.shape:\", train.shape, \"; test.shape:\",test.shape, \"; columns only in train.csv:\", set(train.columns) - set(test.columns))\n# Sample rows\ntrain.head(n=2)","398c659c":"(train.y \/\/ 5 * 5).value_counts().sort_index().plot.bar()","87783bb1":"desc_y_le_170 = train[train.y <= 170].describe()\ndesc_y_gt_170 = train[train.y  > 170].describe()\ndesc_joined   = desc_y_le_170.join(desc_y_gt_170, lsuffix='_le_170', rsuffix='_gt_170')\ndesc_joined[['y_le_170', 'y_gt_170']]","1b68bf41":"def despine_plot_compare_norm_fit(data_series, var_name, plot_title):\n    sns.set_style(\"white\")\n    sns.set_color_codes(palette='deep')\n    f, ax = plt.subplots(figsize=(8, 4))\n    sns.distplot(data_series, fit=norm, color=\"b\"); \n    (mu, sigma) = norm.fit(data_series) \n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n    ax.xaxis.grid(False)\n    ax.set(ylabel=\"Frequency\")\n    ax.set(xlabel=var_name)\n    ax.set(title=plot_title)\n    sns.despine(trim=True, left=True)\n    plt.show()\ndespine_plot_compare_norm_fit(train.y, \"y\", \"y distribution\")","ca6521e8":"print(\"Skewness: %f\" % train[train.y <= 170]['y'].skew())\nprint(\"Kurtosis: %f\" % train[train.y <= 170]['y'].kurt())\nprint(\"N Count: %d\" % train.y.isnull().sum())","8f54f083":"train.describe()","cd76a9f4":"category_cols = [col for col in train.columns if col != 'y' and train[col].dtype == 'object']\ntrain[category_cols].head(n=2)","8052e351":"binary_cols = [col for col in train.columns if col != 'y' and train[col].dtype != 'object']\ntrain[binary_cols].head(n=2)","1981d3b3":"%%time\n# Visualising Categorical Columns\ndef visualize_categorical_columns():\n    fig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 12))\n    plt.subplots_adjust(right=2)\n    plt.subplots_adjust(top=2)\n    sns.color_palette(\"husl\", 8)\n    for i, col in enumerate(list(train[category_cols]), 1):\n        plt.subplot(len(list(category_cols)), 1, i)\n        sns.violinplot(x=col, y='y', data=train[train.y <= 170])\n        plt.xlabel('{}'.format(col), size=15, labelpad=12.5)\n        plt.ylabel('y', size=15, labelpad=12.5)\n        for j in range(2):\n            plt.tick_params(axis='x', labelsize=12)\n            plt.tick_params(axis='y', labelsize=12)\n        plt.legend(loc='best', prop={'size': 10})\n    plt.show()\nvisualize_categorical_columns()","daf5a73f":"%%time\n# Visualising Binary Columns\ndef visualize_binary_columns():\n    fig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 360))\n    plt.subplots_adjust(right=2)\n    plt.subplots_adjust(top=2)\n    sns.color_palette(\"husl\", 8)\n    for i, col in enumerate(list(train[binary_cols]), 1):\n        plt.subplot(len(list(binary_cols)), 6, i)\n        sns.violinplot(x=col, y='y', data=train[train.y <= 170])\n        plt.xlabel('{}'.format(col), size=15,labelpad=12.5)\n        plt.ylabel('y', size=15, labelpad=12.5)\n        for j in range(2):\n            plt.tick_params(axis='x', labelsize=12)\n            plt.tick_params(axis='y', labelsize=12)\n        plt.legend(loc='best', prop={'size': 10})\n    plt.show()\n#visualize_binary_columns()","6f7e1e98":"train_labels   = train['y'].reset_index(drop=True)\ntrain_features = train.drop(['y'], axis=1)\ntest_features  = test\n\nprint(\"train_labels.shape\", train_labels.shape)\nprint(\"train_features.shape\", train_features.shape)\nprint(\"test_features.shape\", test_features.shape)","69961055":"def percent_missing(df):\n    data    = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x  = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    return dict_x\n\ntrain_features_missing = percent_missing(train_features)\nprint('Top 5 percent train_features missing', sorted(train_features_missing.items(), key=lambda x: x[1], reverse=True)[:5])\ntest_features_missing  = percent_missing(test_features)\nprint('Top 5 percent train_features missing', sorted(test_features_missing.items(), key=lambda x: x[1], reverse=True)[:5])","90ac06f7":"# categorical feature names and non-categorical (binary) feature names\ncategory_feature_names = [fea for fea in train_features.columns if fea != 'y' and train_features[fea].dtype == 'object']\nbinary_feature_names   = [fea for fea in train_features.columns if fea != 'y' and train_features[fea].dtype != 'object']","99c82e48":"print(category_feature_names)","f4383752":"print(binary_feature_names[:5], \"...\", binary_feature_names[-5:])","3c5a14a5":"%%time\n# value_counts of each categorical feature\ndef plot_categorical_feature_coverage():\n    fig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 5))\n    plt.subplots_adjust(right=2)\n    plt.subplots_adjust(top=2)\n    sns.color_palette(\"husl\", 8)\n    for i, feature_name in enumerate(list(category_feature_names), 1):\n        plt.subplot(len(list(category_feature_names)), 1, i)\n        sns.countplot(train_features[feature_name])\n        plt.xlabel('{}'.format(feature_name), size=15, labelpad=12.5)\n        plt.ylabel('y', size=15, labelpad=12.5)\n        for j in range(2):\n            plt.tick_params(axis='x', labelsize=12)\n            plt.tick_params(axis='y', labelsize=12)\n        plt.legend(loc='best', prop={'size': 10})\n    plt.show()\nplot_categorical_feature_coverage()","e12a1d6e":"%%time\n# value_counts of each binary feature\ndef plot_binary_features_coverage():\n    bin_fea_value_count = pd.DataFrame(train_features[binary_feature_names[0]].value_counts().sort_index())\n    for feature_name in binary_feature_names[1:]:\n        col_to_append = train_features[feature_name].value_counts().sort_index()\n        bin_fea_value_count = bin_fea_value_count.join(col_to_append, rsuffix=feature_name, how='outer')\n\n    bin_fea_value_count = bin_fea_value_count.fillna(0)\n    print(bin_fea_value_count.shape)\n    plt.figure(figsize=(15,80))\n    sns.barplot(y = bin_fea_value_count.columns, x = (bin_fea_value_count.loc[1] \/ bin_fea_value_count.sum()))\n    plt.show()\n# plot_binary_features_coverage()","5fadda86":"def encoding_features_with_expend(train_features, test_features, n_comp=12):\n    # prepare\n    print('input: train shape={}; test shape={}'.format(train_features.shape, test_features.shape)) \n    category_feature_names = [fea for fea in train_features.columns if train_features[fea].dtype == 'object']\n    # encoding for categorical features\n    oh_encoder = OneHotEncoder(handle_unknown='ignore')\n    oh_encoder.fit(list(train_features[category_feature_names].values) + list(test_features[category_feature_names].values))\n    # categorical feature to be one-hot-coded\n    train_cat_fea_oh_ndarray = oh_encoder.transform(train_features[category_feature_names]).toarray()\n    test_cat_fea_oh_ndarray  = oh_encoder.transform(test_features[category_feature_names]).toarray()\n    train_cat_fea_oh = pd.DataFrame(data=train_cat_fea_oh_ndarray)\n    test_cat_fea_oh  = pd.DataFrame(data=test_cat_fea_oh_ndarray)\n    print(\"train_cat_fea_oh.shape:\", train_cat_fea_oh.shape)\n    train_cat_fea_oh.index = train_features.index\n    test_cat_fea_oh.index  = test_features.index\n    # non-categorical features: \n    train_non_cat_fea      = train_features.drop(category_feature_names, axis=1)\n    test_non_cat_fea       = test_features.drop(category_feature_names, axis=1) \n    # merge them all\n    train_fea_oh = pd.concat([train_cat_fea_oh, train_non_cat_fea], axis=1)\n    test_fea_oh  = pd.concat([test_cat_fea_oh, test_non_cat_fea], axis=1)  \n    # return\n    print('after one-hot: train shape={}; test shape={}'.format(train_fea_oh.shape, test_fea_oh.shape)) \n    return train_fea_oh, test_fea_oh\n\ntrain_features_oh, test_features_oh = encoding_features_with_expend(train_features, test_features)","5d3c946d":"train_features, test_features = train_features_oh, test_features_oh ","96c2ebda":"# model training parameters\nBASE_MODEL_KF  = 7\nSTACK_MODEL_KF = 7\n\n# \"True == is_verify_code\" to check code before running on the full data\nis_verify_code = False \n\n# re-split the train and test features\nX = train_features  \ny = train_labels\nX_test = test_features\nif is_verify_code:\n    X = X.head(n=BASE_MODEL_KF * STACK_MODEL_KF)\n    y = y.head(n=BASE_MODEL_KF * STACK_MODEL_KF)\n    X_test = X_test.head(n=BASE_MODEL_KF * STACK_MODEL_KF)\n\n# shape\nX.shape, y.shape, X_test.shape","6bcf7bad":"from sklearn.model_selection import GridSearchCV\n\nkf = KFold(n_splits=BASE_MODEL_KF, random_state=37, shuffle=True)\n\ndef cv_rmse(model, X=X, y=y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)\n\ndef cv_r2(model, X=X, y=y, print_params=True):\n    r2 = cross_val_score(model, X, y, scoring=\"r2\", cv=kf)\n    print(\"---\"*20)\n    if print_params: \n        print(\"cv_r2: mean=\", r2.mean(), \"; std=\", r2.std())\n        print(\"--- model params: -------\\n\")\n        print(model)\n    else:\n        print(\"cv_r2: \", r2.mean(), \";\", r2.std())\n    print(\"---\"*20)\n    return(r2)\n\ndef plot_grid_cv_results(grid_with_train_score_returned):\n    grid       = grid_with_train_score_returned\n    cv_results = grid.cv_results_\n    rst = pd.DataFrame(cv_results)[['mean_test_score', 'mean_train_score', 'std_test_score', 'std_train_score']]\n    rst[['mean_test_score', 'mean_train_score']].plot.bar(figsize=(50,5), grid=True)\n    rst[['std_test_score', 'std_train_score']].plot.bar(figsize=(50,5), grid=True)\n    print('---'*20)\n    print(rst)\n    print('---'*20)\n    for i in range(0, len(cv_results['params'])): \n        print(i, \"\\t:\", cv_results['params'][i])\n    print('---'*20)\n    print('best_index:', grid.best_index_ )\n    print('best_score:', grid.best_score_ )\n    print('best_param:', grid.best_params_)\n    return rst","9214b9fd":"%%time\n# Grid Search for Lasso Regressor\n# lasso = Pipeline([('scaler', RobustScaler()),('lasso',  Lasso(alpha=0.024, normalize=False, random_state=42))])\nlasso = Pipeline([('lasso',  Lasso(alpha=0.024, normalize=False, random_state=42))])\n\ndef grid_search_lasso(model = lasso):\n    params = {\n        #'lasso__alpha':[1e-4,1e-3,0.01,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.028,0.03,0.05,0.1,1,10]\n        'lasso__alpha':[0.005, 0.015, 0.02, 0.021, 0.022, 0.023, 0.024, 0.025, 0.026, 0.028, 0.029, 0.03, 0.031, 0.05, 0.1]\n    }\n    grid = GridSearchCV(model, params, cv=kf, scoring=\"r2\", return_train_score=True)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)\n    return grid.best_estimator_\n\nlasso = grid_search_lasso()\ncv_r2(lasso, X=X, y=y, print_params=False)","38045ae6":"%%time\n# Grid Search for Ridge Regressor\n# ridge = Pipeline([('scaler',RobustScaler()), ('ridge', Ridge(alpha=45, normalize=False, random_state=42))])\nridge = Pipeline([('ridge', Ridge(alpha=45, normalize=False, random_state=42))])\n\n#normalize_alpha_list = list(range(10,25,3)) + list(range(25,35,2)) + \\\n#                       list(range(35,45,1)) + list(range(45,55,2)) + list(range(55,70,3))\nnormalize_alpha_list = list(range(25,35,2)) + list(range(35,45,1)) + list(range(45,55,2))\n\ndef grid_search_ridge(model=ridge):\n    #no_exp_fea: best_cv_score=0.56218; alpha=42; noralize=False)\n    params = [\n        {'ridge__alpha':normalize_alpha_list, 'ridge__normalize':[False]}\n     # ,{'ridge__alpha':[1e-2, 0.1, 0.2, 0.39, 0.4, 0.41, 0.5, 0.6, 1, 5, 10], 'ridge__normalize' : [True]}\n    ]\n    grid = GridSearchCV(model, params, cv=kf, scoring=\"r2\", return_train_score=True)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)\n    return grid.best_estimator_\n\nridge = grid_search_ridge(ridge)\ncv_r2(ridge, X=X, y=y, print_params=False)","c92b813b":"%%time\n# Random Forest Regressor\nrf = RandomForestRegressor(\n                    max_depth         = 4,\n                    n_estimators      = 1000,\n                    min_impurity_decrease = 0.175,   \n                    criterion         = \"mse\",\n                    min_samples_split = 2,\n                    min_samples_leaf  = 1,\n                    min_weight_fraction_leaf = 0.02, \n                    max_features      = 0.88,\n                    oob_score         = True,\n                    n_jobs            = -1,\n                    warm_start        = False,\n                    random_state      = 42)\n\n# Grid Search for Random Forest: 0.57, no overfitting in cv\ndef grid_search_random_forest(model = rf):       # 0.57, 0.571\n    params = {\"max_depth\": [4],                  # 3,    4\n              \"max_features\": [0.88],            # 1,    0.88\n              \"min_weight_fraction_leaf\":[0.02], # 0.02, 0.02  #0.02 is better than 0.01, obviousely better than 0.03\n              \"min_impurity_decrease\":[0.175, 0.2, 0.25], # 0.02, 0.05  #larger might be better\n              \"n_estimators\":[1000]\n             } \n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)  \n    return grid.best_estimator_\n\nrf = grid_search_random_forest()\n#cv_r2(rf, X=X, y=y, print_params=False)","d548c6d6":"%%time\n# Random Forest Regressor\nrf_2 = RandomForestRegressor(\n                    n_estimators      = 900,\n                    max_features      = None, \n                    max_depth         = 5,\n                    min_samples_split = 5,\n                    min_samples_leaf  = 5,\n                    oob_score         = True,\n                    random_state = 42)\n\ndef grid_search_random_forest_2(model = rf_2):\n    params = {\"n_estimators\":[900],\n              \"max_depth\":[4,5,6],\n              \"min_samples_split\":[5],\n              \"min_samples_leaf\":[5]\n             } \n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)  \n    return grid.best_estimator_\n\nrf_2 = grid_search_random_forest_2()\n#cv_r2(rf_2, X=X, y=y, print_params=False)","53bdeb1f":"%%time\n# Grid Search for GradientBoostingRegressor\ngbr = GradientBoostingRegressor(                 # score: 0.560 -> ... -> 0.571\n                    n_estimators      = 1000, \n                    learning_rate     = 0.006,   # better than 0.005, 0.01\n                    max_features      = 1.0,    \n                    min_impurity_decrease = 0.5, # 0.1 -> ... -> 0.5 better than 0.6\n                    subsample         = 1,       # over fitting for 0.66, 0.88,\n                    max_depth         = 3,       # better than 4\n                    max_leaf_nodes    = None, \n                    min_samples_leaf  = 1,\n                    min_samples_split = 2, \n                    loss              = 'huber', #[\"ls\", \"lad\", \"huber\", \"quantile\"]\n                    random_state      = 42)\n\ndef grid_search_gradient_boosting_regressor(model = gbr):                       \n    params = [{                            \n        \"max_depth\":[None], \"max_leaf_nodes\":[6] # test=0.570521, train=0.581021\n               \n    }, {\n        \"max_depth\":[3], \"max_leaf_nodes\":[None] # test=0.567940, train=0.600836\n    }]\n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid) \n    return grid.best_estimator_\n\ngbr = grid_search_gradient_boosting_regressor()\n#cv_r2(gbr, X=X, y=y, print_params=False)","36d5dd34":"%%time\n# Grid Search for GradientBoostingRegressor\ngbr_2 = GradientBoostingRegressor(\n                    n_estimators      = 500,   \n                    learning_rate     = 0.01,   \n                    max_depth         = 4,\n                    max_features      = 0.6,\n                    min_samples_leaf  = 4,\n                    min_samples_split = 40, \n                    loss              = 'huber',\n                    random_state      = 42)\n\ndef grid_search_gradient_boosting_regressor_2(model = gbr_2): \n    params = {                                 \n        \"n_estimators\":[500],        # 3000  -> 2000  -> 500\n        \"learning_rate\":[0.01],\n        \"max_features\":[0.8, 0.85],  # 0.8 is bettern than 0.6, 1.0        \n        \"min_samples_split\":[110, 120, 130],  # 15    -> 20    -> 40  -> 50 -> 70        \n        \"min_samples_leaf\":[4],      # 5     -> 5 .   -> 4\n    }\n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid) \n    return grid.best_estimator_\n\ngbr_2 = grid_search_gradient_boosting_regressor_2()\n#cv_r2(gbr_2, X=X, y=y, print_params=False)","c25906f1":"%%time\n# Light Gradient Boosting Regressor\n# https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor\n# https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\nlightgbm = LGBMRegressor(\n    boosting_type = 'gbdt',           # 'gbdt'(default); \u2018dart\u2019; \u2018goss\u2019; \u2018rf\u2019\n    objective     = 'regression',     # 'regression', 'binary', 'multiclass', 'lambdarank'\n    num_leaves    = 31,               # default 31, max tree leaves number\n    max_depth     = 3,                # default -1(no limit), tree depth limit\n    learning_rate = 0.002,         \n    n_estimators  = 1000,\n    min_split_gain = 0.1,\n    max_bin       = 40,               # default 255?, number of samples to construct bins\n    reg_alpha     = 0,                # default 0, L1 regularization term on weight\n    reg_lambda    = 0,                # default 0, L2 regularization term on weight\n    bagging_freq  = 4,                # default 0 (no bagging), perform bagging every k iteration \n    bagging_fraction = 1.0,           # \n    feature_fraction = 1.0,           # \n    min_sum_hessian_in_leaf = 0.001,  # default 1e-3, minimal sum hessian in one leaf. \n                                      # like min_data_in_leaf, it can be used to deal with over-fitting\n    n_jobs=-1, verbose = -1, random_state = 42)\n\n#learning_rates  = [0.0015, 0.002, 0.0025] #, 0.0026, 0.0027, 0.0028]   # 0.003 is slightly overfiting, 0.005 is over fitting\n#hessian_in_leaf = [0.001, 10, 30]                    # list(np.array(range(0,65,5)) \/ 10) #print(hessian_in_leaf)\n\ndef grid_search_light_gbm(model = lightgbm):          ## learning_rate=0.003 is slightly overfiting, 0.005 is over fitting                                \n    params = {                                        # 0.561\n        \"n_estimators\": [1000],                       # 1000 \n        \"learning_rate\": [0.002, 0.00225, 0.0025, 0.00275, 0.003], # 0.002 (better then 0.0018, 0.0016, 0.001)\n        \"max_bin\": [40],                              # 10(10 is best, but not too much effect)\n        \"max_depth\" : [3]                             # 3(no overfitting), 4(overfitting but has higher cv-score)\n        #\"min_sum_hessian_in_leaf\" : hessian_in_leaf, # no difference between [0, 6]\n        #\"reg_alpha\": [0, 64]                         # 0     -> 0      -> 0      -> 0       -> 0     -> 64\n    }\n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)\n    return grid.best_estimator_\n\nlightgbm = grid_search_light_gbm()\n#cv_r2(lightgbm, X=X, y=y, print_params=False)","cbf9c043":"%%time\n# Light Gradient Boosting Regressor\n# https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor\n# https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\nlightgbm_2 = LGBMRegressor(    \n    boosting_type = 'gbdt',       # 'gbdt'(default); \u2018dart\u2019; \u2018goss\u2019; \u2018rf\u2019\n    objective     = 'regression', # 'regression', 'binary', 'multiclass', 'lambdarank'\n    num_leaves    = 60,           # default 31, max tree leaves number\n    max_depth     = -1,           # default -1(no limit), tree depth limit\n    learning_rate = 0.01,         \n    n_estimators  = 350,          # 800 -> 350\n    max_bin       = 90,           # default 255\n    bagging_fraction = 0.35,      # randomly select part of data without resampling: 0.35 better than 0.3 and 0.45 in CV\n    reg_alpha     = 0,            # default 0, L1 regularization term on weight\n    reg_lambda    = 0,            # default 0, L2 regularization term on weight\n    bagging_freq  = 4,            # default 0 (no bagging), perform bagging every k iteration \n    bagging_seed  = 8,            # default 3, random seed for bagging\n    feature_fraction = 0.8,       # randomly select part of features on each iteration\n    feature_fraction_seed   = 8,  # default 2, random seed for feature_fraction\n    #min_sum_hessian_in_leaf= 11, # default 1e-3, minimal sum hessian in one leaf. \n                                  # like min_data_in_leaf, it can be used to deal with over-fitting\n    verbose = -1, random_state = 42)\n\ndef grid_search_light_gbm_2(model = lightgbm_2):                               \n    params = [{ \n        \"num_leaves\": [None], \"max_depth\":[3], \n        \"feature_fraction\": [0.8, 0.84, 0.88, 0.92]    #0.5 -> 0.8\n    }, {\n        \"num_leaves\": [6], \"max_depth\":[None], \n        \"feature_fraction\": [0.8, 0.84, 0.88, 0.92]    #0.5 -> 0.8\n    }]\n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)\n    return grid.best_estimator_\n\nlightgbm_2 = grid_search_light_gbm_2()\n#cv_r2(lightgbm_2, X=X, y=y, print_params=False)","e76c8231":"%%time\n# Grid Serch for Adaboost Regressor with Decision Tree\n# https:\/\/www.programcreek.com\/python\/example\/86712\/sklearn.ensemble.AdaBoostRegressor\nabr_tree = AdaBoostRegressor(\n                    base_estimator    = DecisionTreeRegressor(\n                                            max_depth         = 4,\n                                            min_samples_split = 2, \n                                            min_samples_leaf  = 1, \n                                            min_weight_fraction_leaf = 0.0, \n                                            min_impurity_decrease = 0.4, \n                                            random_state      = 53),  \n                    n_estimators      = 1000,\n                    learning_rate     = 0.001,\n                    loss              = \"exponential\",          #'linear', 'square', 'exponential'\n                    random_state      = 42)\n\ndef grid_search_ada_boost_tree_regressor(model = abr_tree):                            \n    params = {                                              # 0.568 -> 0.569 no overfitting if learning rate is low\n        \"base_estimator__max_depth\": [4],                   # 3      # 9,5: over fitting; 3: under fitting \n        \"base_estimator__min_impurity_decrease\":[0.55,0.6], # 0.3, 0.4, 0.5, 0.6: the lower the better score but severe over-fitting\n        \"base_estimator__min_weight_fraction_leaf\":[0.0],   # 0.0    # not try other values yet\n        \"learning_rate\" : [0.001],                          # 0.001  # better than 0.02,0.03\n        \"loss\" : ['exponential']                            # expon  # \n    }\n    #8 \t: {'base_estimator__max_depth': 3, 'base_estimator__min_impurity_decrease': 0.5, 'base_estimator__min_weight_fraction_leaf': 0.0, 'learning_rate': 0.001, 'loss': 'exponential'}\n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)\n    return grid.best_estimator_\n\nabr_tree = grid_search_ada_boost_tree_regressor()\n#cv_r2(abr_tree, X=X, y=y, print_params=False)","0734f2ff":"%%time\nlightgbm_goss = LGBMRegressor(    \n    boosting_type = 'goss',           # 'gbdt'(default); \u2018dart\u2019; \u2018goss\u2019; \u2018rf\u2019\n    objective     = 'regression',     # 'regression', 'binary', 'multiclass', 'lambdarank'\n    num_leaves    = 31,               # default 31, max tree leaves number\n    max_depth     = 3,               # default -1(no limit), tree depth limit\n    learning_rate = 0.005,         \n    n_estimators  = 1000,\n    min_split_gain = 0.1,\n    max_bin       = 40,               # ? default 255, number of samples to construct bins\n    reg_alpha     = 0,                # default 0, L1 regularization term on weight\n    reg_lambda    = 0,                # default 0, L2 regularization term on weight\n    bagging_freq  = 4,                # default 0 (no bagging), perform bagging every k iteration \n    bagging_fraction = 1.0,           # \n    feature_fraction = 1.0,           # \n    min_sum_hessian_in_leaf = 0.001,  # default 1e-3, minimal sum hessian in one leaf. \n                                      # like min_data_in_leaf, it can be used to deal with over-fitting\n    n_jobs=-1, verbose = -1, random_state = 42\n)\n\ndef grid_search_light_gbm_goss(model = lightgbm_goss):                          \n    params = {\n        \"boosting_type\":[\"goss\"],\n        \"n_estimators\": [1000],\n        \"learning_rate\": [0.002],\n        \"max_bin\": [80],\n        \"max_depth\" : [3],\n        \"num_leaves\" : [6,7]\n    }\n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)\n    return grid.best_estimator_\nlightgbm_goss = grid_search_light_gbm_goss()","cae7c019":"%%time\n# Grid Serch for Adaboost Regressor\n# https:\/\/www.programcreek.com\/python\/example\/86712\/sklearn.ensemble.AdaBoostRegressor\nalpha_list = [0.02, 0.025, 0.03, 0.05, 0.1, 0.5, 1, 1.5, 3, 6, 10, 13, 16, 20, 23, 26, 30, 33, 36, 40, 43, 46, 50]\nabr_lasso = AdaBoostRegressor(\n                    base_estimator    = LassoCV(alphas=alpha_list, normalize=False, random_state=42), \n                    n_estimators      = 24,\n                    learning_rate     = 0.0005,\n                    loss              = \"exponential\",   #'linear', 'square', 'exponential'\n                    random_state      = 42)\n\ndef grid_search_abr_lasso(model = abr_lasso):                  # overfitting, 0.570581:0.590737\n    params = {                              # \n        \"n_estimators\" : [18,12],           # \n        \"learning_rate\" : [0.0005, 0.001],  # 0.005 -> 0.001 -> 0.0005 (not to )\n        #\"loss\" : ['exponential','square']  # no obviously difference \n    }\n    grid = GridSearchCV(model, params, scoring=\"r2\", return_train_score=True, cv=kf)\n    grid.fit(X, y)\n    plot_grid_cv_results(grid)\n    return grid.best_estimator_\n\nabr_lasso = grid_search_abr_lasso(abr_lasso)\n#cv_r2(abr, X=X, y=y, print_params=False)","104c128b":"%%time\n# StackingCVRegressor: \n# http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier\/\nalphas_list_1 = [0.001, 0.003, 0.005, 0.007, 0.009]\nalphas_list_2 = [0.1,   0.3,   0.5,   0.7,   0.9]\nalphas_list_3 = [1,     3,     5,     7,     9]\nalphas_list_4 = [10,    13,    15,    17,    19,  21,    23,    25,    27,    29]\nalphas_list_5 = [31,    33,    35,    37,    39,  41,    43,    45,    47,    49]\nalphas_list_6 = [51,    53,    55,    57,    59,  61,    63,    65,    67,    70]\nalphas_list_7 = [75,    80,    85,    90,    95,  100,   200,   400,   800,   1600]\nkf_stack_gen = KFold(n_splits=STACK_MODEL_KF, random_state=37, shuffle=True)\nstack_gen    = StackingCVRegressor(\n                    regressors = (gbr, rf, abr_tree, lightgbm, lasso, lightgbm_goss, rf_2, gbr_2, ridge, lightgbm_2, abr_lasso),\n                    meta_regressor = make_pipeline(RobustScaler(), RidgeCV(scoring='r2', alphas=alphas_list_4)), \n                    cv = kf_stack_gen,\n                    n_jobs = 8, \n                    use_features_in_secondary=False)\n\ndef grid_search_stack_gen(meta_model_list, use_2nd_fea_list=[False], model=stack_gen):\n    params = [{'meta_regressor': meta_model_list, 'use_features_in_secondary': use_2nd_fea_list}]\n    grid_search = GridSearchCV(model, params, cv=3, scoring='r2', return_train_score=True) \n    grid_search.fit(X, y)\n    print(\"grid_search.best_params_: \", grid_search.best_params_)\n    print(\"grid_search.best_estimator_: \", grid_search.best_estimator_)\n    print(\"means test scores:\")\n    cvres = grid_search.cv_results_\n    for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n        print(mean_score, params)\n    print(\"means training scores:\")\n    for mean_score, params in zip(cvres[\"mean_train_score\"], cvres[\"params\"]):\n        print(mean_score, params)\n    print(\"details: -------------------------------------\")\n    print(cvres)\n    return grid_search.best_estimator_\n\n#stack_gen = grid_search_stack_gen([\n#                make_pipeline(RobustScaler(),RidgeCV(alphas=alphas_list_4, scoring='r2'))\n#            ])","2947b86f":"%%time\nprint('fit stack_gen with full training data')\nstack_gen_model = stack_gen.fit(X,y)\n\n# Read in sample_submission dataframe\nsubmission = pd.read_csv(\"\/kaggle\/input\/sample_submission.csv\")\nprint(\"submission.shape:\", submission.shape, \"; submission.columns: \", submission.columns)\nprint(\"X_test.shape:\", X_test.shape)\n\nsubmission.iloc[:,1] = stack_gen_model.predict(X_test)\nsubmission.to_csv(\"predection_output_stack_gen.csv\", index=False)","d4720e3f":"%%time\nbase_models_fitted = {}\n\ndef pred_testset_with_base_model():\n    base_model_list = [('lasso', lasso), ('ridge', ridge), ('rf', rf), ('gbr',gbr), \n                       ('lgm', lightgbm), ('abr_tree', abr_tree), ('abr_lasso', abr_lasso),\n                       ('lightgbm_goss', lightgbm_goss), ('rf_2', rf_2), ('gbr_2', gbr_2), \n                       ('lightgbm_2', lightgbm_2)]\n    for name, model in base_model_list:\n        file_name            = \"prediction_\" + name + \".csv\"\n        trained_model        = model.fit(X,y)\n        submission.iloc[:,1] = trained_model.predict(X_test)\n        submission.to_csv(file_name, index=False)\n        \n        base_models_fitted[name] = trained_model\n        print(file_name)\npred_testset_with_base_model()","3827e244":"# Feature Engineering\n\nOneHot Encoding for Categorical Features","35341518":"## Files and Goal","d0392756":"# Fit Stacked-Model\n\nFit the Stacked Model, Predict and Submit","7790679f":"# Elementary Models Grid Search","bce1a088":"# Features Target Correlation","924cebed":"## Stacked Model and Grid Search","19e4be95":"# Feature Coverage","56d044e3":"No numerical feature, only binary features and categorical feature. No need for re-shape the skew features here","2cae0420":"# Evaluluate Elementary Models\n\nUse elementary models to predict the test-set, submit them to get the sores, then we can compare the stacked model with these elementary models","33b1a526":"# Feature Target Split","c7adee43":"# Plot Target(y)","b444862b":"# GridSearch and CV scores","c0814b38":"# X, y, X_test","aaf0c044":"<p>**The dataset:** contains an anonymized set of variables, each representing a custom feature in a Mercedes car. For example, a variable could be 4WD, added air suspension, or a head-up display.<\/p>\n<p>**y:** represents the time (in seconds) that the car took to pass testing for each variable. <\/p>\n<p>**X:** Variables with letters are categorical; Variables with 0\/1 are binary values. <\/p>\n<p>**The Goal: **predict the time for testing a car according to its custom features <\/p>","4fe33a23":"# Mercedes-Benz Greener Manufacturing\n\n> kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nlib and functions: ","caad170c":"# Missing Values","d46dda8a":"No missing values for all features. If having missing values, the percent_missing will be used to decide how to deal with the missing values for each feature.","1c921424":"notice the X-axis of the plot, the next value of 165.0 is 265.0","443a6f47":"# Introduction\n\nFor this data-set, using normal model can only get a R2 score between 0.543 and 0.552. This notebook built a stacked model based on sereval such normal models. After submiting and comparing the predictions of all models, the stacked model get a testing-set score outperform all of basic models\n\ntesting-set prediction | model | testing-set score \n-|-|-\npredection_output_stack_gen.csv | stacked model | 0.55600 |\nprediction_abr_lasso.csv | AdaBoost of Lasso| 0.54510\nprediction_abr_tree.csv | AdaBoost of DecisionTreeRegressor | 0.54816\nprediction_gbr.csv | GradientBoostingRegressor(GBDT) | 0.54475\nprediction_gbr_2.csv | GradientBoostingRegressor | 0.54645\nprediction_lasso.csv | Lasso | 0.54711\nprediction_lgm.csv | LightGBM | 0.54998\nprediction_lightgbm_2.csv | LightGBM | 0.54869\nprediction_lightgbm_goss.csv | LightGBM | 0.55056\nprediction_rf.csv | RandomForest | 0.55127\nprediction_rf_2.csv | RandomForest | 0.55164\n\n<br\/>\n> \u8fd9\u4efd\u6570\u636e\u96c6\u6837\u672c\u5c11\u4f46\u7279\u5f81\u6570\u91cf\u591a\uff08\u5e76\u4e14\u5168\u90e8\u662f\u7c7b\u522b\u7279\u5f81\uff09\u5e76\u4e14\u6837\u672c\u53d1\u6563\n>\n> * \u4f7f\u7528\u5e38\u89c4\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u4ea4\u6d4b\u8bd5\u96c6\u9884\u6d4b\u7ed3\u679c\u4e4b\u540e\uff0c\u53ea\u80fd\u83b7\u5f970.543~0.552\u7684R2 Score\n> * \u8fd9\u7bc7Note\u5c06\u4e00\u7ec4\u5e38\u89c4\u6a21\u578b\u7528Stacking\u7684\u65b9\u5f0f\u96c6\u6210\u3001\u96c6\u6210\u6a21\u578b\u5f97\u5230\u4e860.56\u7684\u6d4b\u8bd5\u96c6\u5206\u6570\u3001\u660e\u663e\u9ad8\u4e8e\u5e38\u89c4\u6a21\u578b\n>\n> \u53e6\u5916\u7531\u4e8e\u6709\u591a\u4e2abasic model\u9700\u8981\u8c03\u53c2\uff0c\u56e0\u6b64\u4f7f\u7528\u4e86\u53ef\u89c6\u5316\u7684\u65b9\u5f0f\u6765\u6bd4\u8f83\u4e0d\u540c\u5b9e\u9a8c\u53c2\u6570\uff08\u8fc7\u62df\u5408\u3001\u6b20\u62df\u5408\u3001\u8bad\u7ec3\/\u6d4b\u8bd5\u96c6\u5206\u6570\u53d8\u5316\u8d8b\u52bf\uff09\uff0c\u5e76\u4e14\u591a\u4e2a\u6a21\u578b\u540c\u65f6\u8c03\u53c2"}}