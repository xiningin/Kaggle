{"cell_type":{"d6a0a5ae":"code","af5c0e61":"code","c5342c26":"code","f36f9aaf":"code","06cbfba6":"code","73d92fa3":"code","1349a0a2":"code","477bda29":"code","ffc5347c":"code","1131c33c":"code","5edcd5c0":"code","267d1aee":"code","6cdcda7d":"code","04fb8575":"code","e54102c3":"code","b24ad008":"code","bc2821cd":"code","931c4bc1":"code","2ca411b6":"code","06d647a7":"code","763e011f":"code","101698cf":"code","553f5a91":"code","7cd8794a":"code","46c97b73":"code","ea229b28":"code","9d93b6ab":"code","674af883":"code","987e87b7":"code","46926483":"code","79d87c1c":"code","d0bd0de2":"code","785ca5da":"code","4b577b97":"code","a72b7b2d":"code","9e9d54cb":"code","be72022e":"code","0e58f5ec":"code","ec6fb69c":"code","67c067ce":"code","ea43b33f":"code","a05bce34":"code","c6a42fb5":"code","14d5f42a":"code","202fdbb9":"code","2e6c52cf":"code","b6cfda00":"code","a3bf19d5":"code","c67d4165":"code","5e64cbb9":"code","6848c1a4":"code","52dd9d71":"code","348ca735":"code","5595be70":"code","ad20c201":"code","26542d1f":"code","82680902":"code","a8f326dd":"code","55de09fc":"code","ea9d71dd":"code","be295d49":"code","aa493182":"code","d3c6e4fb":"code","395ab6e3":"code","7b278140":"code","fe3ed359":"code","3a3068e2":"code","4900af0a":"code","050a9888":"code","bc21edbd":"code","f52ae4bd":"code","5de3a22c":"code","caec041b":"code","8f8350b2":"code","c5035ab7":"code","44da207f":"code","841d254a":"code","78c80bb7":"markdown","f4147a14":"markdown","0523b890":"markdown","fe29d38e":"markdown","d201a1b9":"markdown","5b9f82a3":"markdown","791f1f1e":"markdown","e47eb4c6":"markdown","770b0118":"markdown","83497ad6":"markdown","438a5f72":"markdown","713193e0":"markdown","5e9b0920":"markdown","5abf485b":"markdown","bc150dcc":"markdown","f9840b6a":"markdown","b9560ff5":"markdown","3fa69fe7":"markdown","9a536b49":"markdown","ac435ea6":"markdown","7adc210c":"markdown","21efbbf1":"markdown","a631be8b":"markdown","71343fa7":"markdown","18edad75":"markdown"},"source":{"d6a0a5ae":"#importing libraries for numpy and dataframe\nimport pandas as pd\nimport numpy as np\n\n#importing libraries for data visualization\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\nimport seaborn as sns\n%matplotlib inline\n\n#importing library for data scaling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n\n#importing library to suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#importing libraries for Logistic Regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nimport statsmodels.api as sm\n\n#Checking VIF values for the feature variables\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#Creation of confusion matrix\nfrom sklearn import metrics\n\nimport os, sys, csv","af5c0e61":"#Reading the train dataset\ntrain_df = pd.read_csv(\"..\/input\/train.csv\",encoding='ISO-8859-1')","c5342c26":"#Previewing the dataframe by checking the first 5 records\ntrain_df.head()","f36f9aaf":"#To determine the number of rows and columns present in the train data\ntrain_df.shape","06cbfba6":"#To understand the datatype of the columns present in the dataframe\ntrain_df.info()","73d92fa3":"#To understand the statistical measures of the numerical columns present in the train dataframe\ntrain_df.describe()","1349a0a2":"#Checking for missing data in the train dataset\nmissing = round((train_df.isna().sum()\/len(train_df))*100,2)\ntotal = train_df.isna().sum()\nmissing_data = pd.DataFrame({'Total Missing' : total,'Percentage Missing' : missing})\nmissing_data","477bda29":"#Dropping Cabin from the dataframe\ntrain_df.drop(['Cabin'],axis=1,inplace=True)","ffc5347c":"#Previewing the dataframe by checking the first 5 records\ntrain_df.head()","1131c33c":"#To treat missing values of Age\ntrain_df['Age'].hist(bins=10,density=True).plot(kind='Density')","5edcd5c0":"#Calculating the mean of the column Age\ntrain_df['Age'].mean()","267d1aee":"#Let us calculate the mean of the column Age\ntrain_df['Age'].median(skipna=True)","6cdcda7d":"#Replacing all the NA values in age with median\ntrain_df['Age'].fillna(train_df['Age'].median(skipna=True),inplace = True)","04fb8575":"#We have only 2 NA in Embarked.\nsns.countplot(train_df['Embarked'])","e54102c3":"train_df['Embarked'].fillna(train_df['Embarked'].value_counts().idxmax(),inplace = True)","b24ad008":"#Let us again look into the number of missing values after imputing values for NA\nmissing = round((train_df.isna().sum()\/len(train_df))*100,2)\ntotal = train_df.isna().sum()\nmissing_data = pd.DataFrame({'Total Missing' : total,'Percentage Missing' : missing})\nmissing_data","bc2821cd":"#We can see the cleaned data now.\ntrain_df.head()","931c4bc1":"train_df['Family'] = np.where((train_df['SibSp']+train_df['Parch'])>0 , 1 , 0)","2ca411b6":"# We can drop SibSp and Parch\ntrain_df.drop(['SibSp','Parch'],axis=1,inplace=True)","06d647a7":"train_df.head()","763e011f":"#Ticket,Name,PassengerId are not going to play any role in model building so we can drop the columns.\ntrain_df.drop(['PassengerId','Name','Ticket'],axis=1,inplace = True)","101698cf":"#Previewing dataframe after dropping the columns\ntrain_df.head()","553f5a91":"#Let us categorize age into 3 categories -> Children are below 20, between 20 and 50 are Adults and above 50 are Elders\ntrain_df['Age_Category'] = pd.cut(x = train_df['Age'],bins=[0,20,50,100],labels = ['Children','Adults','Elders'])","7cd8794a":"train_df.head(10)","46c97b73":"#Sex vs Survived\nsns.countplot(x='Sex', hue='Survived' , data=train_df)","ea229b28":"#Pclass vs Survived\nsns.countplot(x='Pclass',hue='Survived',data=train_df)","9d93b6ab":"#Survived vs Age\nplt.figure(figsize=(50,15))\nsns.countplot(x='Age',hue='Survived',data=train_df)\nplt.show()","674af883":"#Survived vs Embarked\nsns.countplot(x='Embarked',hue='Survived',data=train_df)","987e87b7":"#Survived vs Family\nsns.countplot(x='Family',hue='Survived',data=train_df)","46926483":"#Survived vs Age Category\nsns.countplot(x='Age_Category',hue='Survived',data=train_df)","79d87c1c":"#Lets read the train_csv file from the local\ntest_df = pd.read_csv(\"..\/input\/test.csv\",encoding='ISO-8859-1')\n","d0bd0de2":"#Previewing data of test dataset\ntest_df.head()","785ca5da":"test_df.info()","4b577b97":"test_df.describe()","a72b7b2d":"#To know the number of rows and columns of the test dataset.\ntest_df.shape","9e9d54cb":"#Checking for missing data in the train dataset\nmissing_test = round((test_df.isna().sum()\/len(test_df))*100,2)\ntotal_test = test_df.isna().sum()\nmissing_data_test = pd.DataFrame({'Total Missing' : total_test,'Percentage Missing' : missing_test})\nmissing_data_test","be72022e":"test_df.drop('Cabin',axis=1,inplace=True)","0e58f5ec":"test_df['Age'].fillna(28,inplace=True)","ec6fb69c":"test_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)","67c067ce":"#Checking for missing data in the train dataset\nmissing_test = round((test_df.isna().sum()\/len(test_df))*100,2)\ntotal_test = test_df.isna().sum()\nmissing_data_test = pd.DataFrame({'Total Missing' : total_test,'Percentage Missing' : missing_test})\nmissing_data_test","ea43b33f":"#Both SibSp and Parch indicate the number of family members. Hence, where the sum of both the columns is greater than 0, we will consider it as 1.\ntest_df['Family'] = np.where((test_df['SibSp']+test_df['Parch'])>0 , 1 , 0)","a05bce34":"PassengerId=test_df['PassengerId']","c6a42fb5":"test_df.drop(['Name','SibSp','Parch','PassengerId','Ticket'],axis=1,inplace = True)","14d5f42a":"test_df.head()","202fdbb9":"#Let us categorize age into 3 categories -> Children are below 20, between 20 and 50 are Adults and above 50 are Elders\ntest_df['Age_Category'] = pd.cut(x = test_df['Age'],bins=[0,20,50,100],labels = ['Children','Adults','Elders'])","2e6c52cf":"#Creating dummy variables for train dataset and test data\nfinal_train = pd.get_dummies(train_df,columns=['Pclass','Sex','Embarked','Age_Category'])\nfinal_test = pd.get_dummies(test_df,columns=['Pclass','Sex','Embarked','Age_Category'])","b6cfda00":"#Previewing final train dataset\nfinal_train.head()","a3bf19d5":"#Previewing final test dataset\nfinal_test.head()","c67d4165":"#Creating X_train dataset which would have the predicting features\nX_train = final_train[['Age','Fare','Family','Pclass_1','Pclass_2','Pclass_3','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S','Age_Category_Children','Age_Category_Adults','Age_Category_Elders']]","5e64cbb9":"#Previewing the X_train dataset\nX_train.head()","6848c1a4":"#Creating y_train dataset that contains the dependent variable\ny_train = final_train[['Survived']]","52dd9d71":"logm1 = sm.GLM(y_train,(sm.add_constant(X_train)),family = sm.families.Binomial())\nlogm1.fit().summary()","348ca735":"col=X_train.columns","5595be70":"col = col.drop('Fare',1)","ad20c201":"logm2 = sm.GLM(y_train,(sm.add_constant(X_train[col])),family = sm.families.Binomial())\nlogm2.fit().summary()","26542d1f":"col = col.drop('Family',1)","82680902":"logm3 = sm.GLM(y_train,(sm.add_constant(X_train[col])),family = sm.families.Binomial())\nlogm3.fit().summary()","a8f326dd":"col = col.drop('Age_Category_Elders',1)","55de09fc":"logm4 = sm.GLM(y_train,(sm.add_constant(X_train[col])),family = sm.families.Binomial())\nlogm4.fit().summary()","ea9d71dd":"col = col.drop('Age_Category_Children',1)","be295d49":"logm5 = sm.GLM(y_train,(sm.add_constant(X_train[col])),family = sm.families.Binomial())\nlogm5.fit().summary()","aa493182":"col = col.drop('Age_Category_Adults',1)","d3c6e4fb":"logm6 = sm.GLM(y_train,(sm.add_constant(X_train[col])),family = sm.families.Binomial())\nlogm6.fit().summary()","395ab6e3":"col = col.drop('Embarked_S',1)","7b278140":"logm7 = sm.GLM(y_train,(sm.add_constant(X_train[col])),family = sm.families.Binomial())\nlogm7.fit().summary()","fe3ed359":"col = col.drop('Embarked_Q',1)","3a3068e2":"logm8 = sm.GLM(y_train,(sm.add_constant(X_train[col])),family = sm.families.Binomial())\nlogm8.fit().summary()","4900af0a":"col = col.drop('Pclass_2',1)","050a9888":"logm9 = sm.GLM(y_train,(sm.add_constant(X_train[col])),family = sm.families.Binomial())\nlogm9.fit().summary()","bc21edbd":"#Creating a dataframe that will contain VIF values of all the features\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF']=[variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif = vif.sort_values(by='VIF',ascending = False)\nvif","f52ae4bd":"#Since VIF of Sex_Male is greater than 5 , we would drop Sex_male\ncol = col.drop('Sex_male',1)","5de3a22c":"#Creating a dataframe that will contain VIF values of all the features\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF']=[variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif = vif.sort_values(by='VIF',ascending = False)\nvif","caec041b":"X_test = final_test","8f8350b2":"X_test.head()","c5035ab7":"#Applying logistic regression model on X_test to predict Y.\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred = logreg.predict(X_test)","44da207f":"#Calculating the metrics\naccuracy = round(logreg.score(X_train,y_train)*100,2)\nprint('The accuracy for Logistic Regression model is :',accuracy)","841d254a":"#Converting the array in Dataframe format\nsubmission = pd.DataFrame({'PassengerId':PassengerId,'Survived':y_pred})\nsubmission.to_csv(\"submission.csv\",index=False)","78c80bb7":"##### We would dropping columns from X_train if the p-value of any of the features are greater than 0.05","f4147a14":"Now Cabin has 78.32% data missing. We will straightaway srop it. We will impute missing values of age with 28 as it was done for train dataset. 1 data missing of Fare can simply be imputed with mean.","0523b890":"We can see that there is not much significant diference between the survival with family and without family.","fe29d38e":"People who boarded at Southampton have the highest survival count. I believe since the count of people from Southampton are the highest, the count of survival of their will also be the highest.","d201a1b9":"The median is 28. Imputting the NA values with median will maintain the normal distribution.","5b9f82a3":"I think it is very obvious that female will be given more preference here for their safety hence a larger count of females survived.","791f1f1e":"### Performing same operations for Test Set","e47eb4c6":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.","770b0118":"We can see that the number of missing values are 0 for all the columns.","83497ad6":"With 77.10% data missing in Cabin we will drop it as imputing it would not be wise.","438a5f72":"### Missing value treatment","713193e0":"SibSp and Parch indicate different family members travelling with the passenger. We can combine both the columns. After combining them 0 will indicate the passenger was travelling alone and 1 will indicate he had family members.","5e9b0920":"### Creating logistic regression models","5abf485b":"### Applying Logistic Regression Model","bc150dcc":"Looking at the histogram and the mean, we can say that the plot is right skewed. Replacig NA with mean will give us biased result.","f9840b6a":"### Exploratory Data Analysis","b9560ff5":"We can see that Cabin has 77.10% data missing, Age has 19.87% data missing and Embarked has 0.22% data missing","3fa69fe7":"We can easily impute the NA values and give them a value of S as S has more value of counts compared to C and Q.","9a536b49":"We can see that the max people who got saved were of the age 28 and the max people who could not survive were also of 28.","ac435ea6":"## The accuracy of the model is 79.46%","7adc210c":"We have adults ranging from the age of 20 to 50. We can see a significant people who survived between the age of 20 and 50.Elders being greater than 50 would have been old and would have slow to evacuate them. People with children would have faced the same problem.","21efbbf1":"Since X_train got many columns dropped we should have the same columns in X_test as well.","a631be8b":"# Titanic: Machine Learning from Disaster","71343fa7":"The 9th model that we have created has all the variables whose p-value less than 0.05. Hence, we can finalize on those variables.","18edad75":"There is not much of difference between the count. But we can still see Class 1 people survived more and it is about the preference."}}