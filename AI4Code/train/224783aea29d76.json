{"cell_type":{"430e5b64":"code","55abd37e":"code","8c2d2b54":"code","3bd93cbd":"code","9639769d":"code","b8ff9b69":"code","a0b41145":"code","b96e186b":"code","726ad05c":"code","9e443f25":"code","c9244090":"code","d3d83287":"code","3d3b434b":"code","79e13468":"code","1c1ce515":"code","0f621abf":"code","8bf0a89b":"code","270c606a":"code","107f740a":"code","7ccc2225":"code","dd6eb319":"code","4da06695":"code","a8a56895":"code","30bc40e4":"code","85fbe8b6":"code","ae766532":"code","90ed4d83":"code","7dbeb560":"code","72d99a4f":"code","a1220063":"code","f8b1f763":"code","fd70cf8d":"code","e318f72b":"code","4572e453":"code","b4c9b34e":"code","db8cefe5":"code","156e1637":"code","3e1a5890":"code","fab69236":"code","b91c7951":"code","51a7cfa7":"code","58c0db18":"code","eff1c657":"code","83b13d5a":"code","af8cb250":"code","987de087":"code","41e95b8e":"code","f106041f":"code","03cde82f":"code","661ad0da":"code","13a0a6f7":"code","0a31935b":"code","dea3907a":"code","d9969635":"code","da107e30":"code","f55e642f":"code","e01f69ba":"code","01f8a33f":"code","3c42651c":"code","b359f5da":"code","cea15deb":"code","b7f89b38":"code","29bf3382":"code","22eae809":"code","1dd9b0e5":"code","0e819596":"code","fbd8d5d4":"code","99a392eb":"code","405819ee":"code","4b76d124":"markdown","e695c916":"markdown","44aefb48":"markdown","c934c6a1":"markdown","8ec2a0c5":"markdown","358c4ce7":"markdown","475f52b5":"markdown","379c0673":"markdown","4ac6d183":"markdown","e61de2f5":"markdown","2f853785":"markdown","fbbcd7c3":"markdown","1a296567":"markdown","a4b8a158":"markdown","34a796d3":"markdown","79599951":"markdown","abe3accd":"markdown","709f4240":"markdown","8ecf1a64":"markdown","eeb22a3f":"markdown","883f550f":"markdown","662dd1a0":"markdown","df5f1b71":"markdown","6c7482aa":"markdown","ff5565dc":"markdown","367ec0f7":"markdown","dbc2e299":"markdown","f161e88d":"markdown","8ad1835e":"markdown","e2b4dede":"markdown","46884219":"markdown","08769533":"markdown","6ded404f":"markdown","d1de4642":"markdown","302e536c":"markdown","5e6c1d97":"markdown","af17d955":"markdown","5febf52e":"markdown","a1aa1655":"markdown","a7f81b68":"markdown","c1337e5d":"markdown","7d11b2fa":"markdown","f9cad9ab":"markdown","cc7fe8c3":"markdown","bb0e5e59":"markdown","472491d1":"markdown","797d15c5":"markdown","ee683c28":"markdown","a95e1abd":"markdown","257c6696":"markdown","36c22fc9":"markdown","c5844e03":"markdown","8aa0b01c":"markdown","cb64e878":"markdown","48863fce":"markdown","fc85c87b":"markdown","16fdddf7":"markdown","9392d064":"markdown","f16692ea":"markdown","5ba3dddf":"markdown","385e9532":"markdown","fb7315f5":"markdown","344177f6":"markdown","c944d5d7":"markdown","f42a4d3c":"markdown","0f601528":"markdown","4d57290d":"markdown","3a79e920":"markdown"},"source":{"430e5b64":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, accuracy_score, r2_score\nfrom sklearn.impute import SimpleImputer\n\npd.set_option('display.max_columns',100)\npd.set_option('display.max_rows',100)","55abd37e":"home_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col=0)\nhome_data","8c2d2b54":"print(home_data.info())","3bd93cbd":"home_data.shape","9639769d":"home_data.describe().round(3)","b8ff9b69":"target_var_name = 'SalePrice'\ntarget_var = pd.DataFrame(home_data[target_var_name]).set_index(home_data.index)\nhome_data.drop(target_var_name, axis=1, inplace=True)\ntarget_var","a0b41145":"print(target_var.describe().round(decimals=2))\nsns.distplot(target_var)\nplt.title('Distribution of SalePrice')\nplt.show()","b96e186b":"num_feature = home_data.select_dtypes(exclude=['object']).columns\nhome_data_num_feature = home_data[num_feature].set_index(home_data.index)","726ad05c":"home_data_num_feature.describe().round(3)","9e443f25":"fig = plt.figure(figsize=(12,20))\nplt.title('Numerical Feature (before dropping identified outliers)')\nfor i in range(len(home_data_num_feature.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.distplot(home_data_num_feature.iloc[:,i].dropna(),kde_kws={'bw':0.1})\n    plt.xlabel(home_data_num_feature.columns[i])\n\nplt.tight_layout()\nplt.show()","c9244090":"fig = plt.figure(figsize=(12,20))\nplt.title('Numerical Feature (before dropping identified outliers)')\nfor i in range(len(home_data_num_feature.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.scatterplot(home_data_num_feature.iloc[:,i], target_var.iloc[:,0])\n    plt.xlabel(home_data_num_feature.columns[i])\n\nplt.tight_layout()\nplt.show()","d3d83287":"correlation = home_data_num_feature.corr()\n\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation>0.8)\nplt.show()\n","3d3b434b":"y_corr = pd.DataFrame(home_data_num_feature.corrwith(target_var.SalePrice),columns=[\"Correlation with target variable\"])\n# plt.hist(y_corr)","79e13468":"y_corr_sorted= y_corr.sort_values(by=['Correlation with target variable'],ascending=False)\ny_corr_sorted","1c1ce515":"fig = plt.figure(figsize=(6,10))\nplt.title('Correlation with target variable')\na=sns.barplot(y_corr_sorted.index,y_corr_sorted.iloc[:,0],data=y_corr)\na.set_xticklabels(labels=y_corr_sorted.index,rotation=90)\nplt.tight_layout()\nplt.show()","0f621abf":"[(y_corr_sorted<0.1) & (y_corr_sorted>-0.1)]","8bf0a89b":"cat_feature = home_data.select_dtypes(include=['object']).columns\nhome_data_cat_feature = home_data[cat_feature]","270c606a":"fig = plt.figure(figsize=(18,50))\nplt.title('Distribution of Categorical Feature')\nfor i in range(len(home_data_cat_feature.columns)):\n    fig.add_subplot(15,3,i+1)\n    sns.countplot(home_data_cat_feature.iloc[:,i])\n    plt.xlabel(home_data_cat_feature.columns[i])\n    plt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.show()","107f740a":"fig = plt.figure(figsize=(18,80))\nplt.title('Numerical Feature (before dropping identified outliers)')\nfor i in range(len(home_data_cat_feature.columns)):\n    fig.add_subplot(15,3,i+1)\n    sns.boxplot(x=home_data_cat_feature.iloc[:,i],y=target_var['SalePrice'])\n    plt.xlabel(home_data_cat_feature.columns[i])\n    plt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.show()","7ccc2225":"##look into sorting by median","dd6eb319":"home_data_num_feature=home_data_num_feature.drop(['GarageYrBlt','1stFlrSF','TotRmsAbvGrd','GarageArea'],axis=1)\nhome_data_num_feature.columns","4da06695":"home_data_num_feature=home_data_num_feature.drop(['PoolArea','MoSold','3SsnPorch','BsmtFinSF2','BsmtHalfBath','MiscVal','LowQualFinSF','YrSold','OverallCond','MSSubClass'],axis=1)","a8a56895":"home_data_num_feature=home_data_num_feature.drop(home_data_num_feature[home_data_num_feature['LotFrontage']>300].index)\nprint(len(home_data_num_feature))\nhome_data_num_feature=home_data_num_feature.drop(home_data_num_feature[(home_data_num_feature['GrLivArea']>4000) & (target_var['SalePrice']<300000)].index)\nprint(len(home_data_num_feature))\n\nhome_data_num_feature=home_data_num_feature.drop(home_data_num_feature[home_data_num_feature['BsmtFinSF1']>4000].index)\nprint(len(home_data_num_feature))\n\nhome_data_num_feature=home_data_num_feature.drop(home_data_num_feature[home_data_num_feature['LotArea']>100000].index)\nprint(len(home_data_num_feature))\n\nhome_data_num_feature=home_data_num_feature.drop(home_data_num_feature[home_data_num_feature['TotalBsmtSF']>6000].index)\nprint(len(home_data_num_feature))","30bc40e4":"fig = plt.figure(figsize=(12,12))\nplt.title('Numerical Feature (after dropping identified outliers)')\nfor i in range(len(home_data_num_feature.columns)):\n    fig.add_subplot(6,4,i+1)\n    sns.scatterplot(home_data_num_feature.iloc[:,i], target_var.iloc[:,0])\n    plt.xlabel(home_data_num_feature.columns[i])\n\nplt.tight_layout()\nplt.show()","85fbe8b6":"home_data_num_feature.count()","ae766532":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer()\nhome_data_num_feature = pd.DataFrame(imp.fit_transform(home_data_num_feature),columns=home_data_num_feature.columns,index=home_data_num_feature.index)\nhome_data_num_feature.count()","90ed4d83":"home_data_cat_feature.count()","7dbeb560":"home_data_cat_feature=home_data_cat_feature.drop(['Alley','PoolQC','Fence','MiscFeature','FireplaceQu'],axis=1)","72d99a4f":"imp = SimpleImputer(strategy=\"most_frequent\")\nhome_data_cat_feature=pd.DataFrame(imp.fit_transform(home_data_cat_feature),columns=home_data_cat_feature.columns,index=home_data_cat_feature.index)\nhome_data_cat_feature","a1220063":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(drop='first',sparse=False)\nenc.fit(home_data_cat_feature)\nhome_data_cat_feature_dummies=enc.transform(home_data_cat_feature)\nhome_data_cat_feature_dummies = pd.DataFrame(home_data_cat_feature_dummies,columns=enc.get_feature_names(),index=home_data_cat_feature.index)\nhome_data_cat_feature_dummies","f8b1f763":"X=pd.merge(home_data_num_feature,home_data_cat_feature_dummies,how='left',left_index=True,right_index =True)\n\n# X=pd.concat([home_data_num_feature,home_data_cat_feature_dummies],axis=1)\ny=target_var.loc[X.index]","fd70cf8d":"X","e318f72b":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nr2 = make_scorer(r2_score)\nrmse = make_scorer(mean_squared_error,greater_is_better=False,squared=False)\n\ncv_list={}\ncv_rmse={}\ncv_r2={}\ncv_best_mse={}","4572e453":"model_name = \"LinearRegression\"\nmodel=LinearRegression()\n\nparam_grid = [{model_name+'__fit_intercept':[True,False]}]","b4c9b34e":"pipeline = Pipeline([(model_name, model)])\n\n\nreg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=-1)\nreg.fit(X,y.to_numpy())\n\n\n#Record the best grid search paramters into the list.\ncv_list[model_name]=reg\ncv_rmse[model_name]=reg.best_score_\n\n#print out the best param and best score\nprint(model_name)\nprint('best training param:',reg.best_params_)\nprint('best training score rmse', reg.best_score_)\nprint('\\n')","db8cefe5":"from sklearn.linear_model import Lasso\n\nmodel_name = \"Lasso\"\nmodel=Lasso()\n\nparam_grid = [  {model_name+'__'+'alpha': [2**-5,2**-3,2**-1,2**1,2**3,2**5,2**7,2**9,2**11,2**13,2**15]}]","156e1637":"pipeline = Pipeline([(model_name, model)])\n\n\nreg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=-1)\nreg.fit(X,y.to_numpy())\n\n\n#Record the best grid search paramters into the list.\ncv_list[model_name]=reg\ncv_rmse[model_name]=reg.best_score_\n\n#print out the best param and best score\nprint(model_name)\nprint('best training param:',reg.best_params_)\nprint('best training score rmse', reg.best_score_)\nprint('\\n')","3e1a5890":"from sklearn.linear_model import Ridge\n\nmodel_name = \"Ridge\"\nmodel=Ridge()\n\nparam_grid = [{model_name+'__'+'alpha': [2**-5,2**-3,2**-1,2**1,2**3,2**5,2**7,2**9,2**11,2**13,2**15]}]","fab69236":"pipeline = Pipeline([(model_name, model)])\n\n\nreg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=-1)\nreg.fit(X,y.to_numpy())\n\n\n#Record the best grid search paramters into the list.\ncv_list[model_name]=reg\ncv_rmse[model_name]=reg.best_score_\n\n#print out the best param and best score\nprint(model_name)\nprint('best training param:',reg.best_params_)\nprint('best training score rmse', reg.best_score_)\nprint('\\n')","b91c7951":"from sklearn.tree import DecisionTreeRegressor\n\nmodel_name='DecisionTreeRegressor'\nmodel=DecisionTreeRegressor()\n\nparam_grid = [{model_name+'__'+'splitter': ['best','random'],\n              model_name+'__'+'max_depth':np.arange(1,20)\n              }]","51a7cfa7":"pipeline = Pipeline([(model_name, model)])\n\n\nreg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=-1)\nreg.fit(X,y.to_numpy())\n\n\n#Record the best grid search paramters into the list.\ncv_list[model_name]=reg\ncv_rmse[model_name]=reg.best_score_\n\n#print out the best param and best score\nprint(model_name)\nprint('best training param:',reg.best_params_)\nprint('best training score rmse', reg.best_score_)\nprint('\\n')","58c0db18":"from sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel_name = 'BaggingDecisionTreeRegressor'\nmodel=BaggingRegressor(DecisionTreeRegressor())\n\nparam_grid = [{model_name+'__'+'base_estimator__splitter': ['best','random'],\n              model_name+'__'+'base_estimator__max_depth':np.arange(1,30)\n              }]","eff1c657":"pipeline = Pipeline([(model_name, model)])\n\n\nreg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=-1)\nreg.fit(X,y.to_numpy().ravel())\n\n\n#Record the best grid search paramters into the list.\ncv_list[model_name]=reg\ncv_rmse[model_name]=reg.best_score_\n\n#print out the best param and best score\nprint(model_name)\nprint('best training param:',reg.best_params_)\nprint('best training score rmse', reg.best_score_)\nprint('\\n')","83b13d5a":"from sklearn.ensemble import RandomForestRegressor\n\nmodel_name='RandomForestRegressor'\nmodel=RandomForestRegressor()\n\nparam_grid = [{model_name+'__'+'max_depth' : np.arange(1,100,2)}]","af8cb250":"pipeline = Pipeline([(model_name, model)])\n\n\nreg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=-1)\nreg.fit(X,y.to_numpy().ravel())\n\n\n#Record the best grid search paramters into the list.\ncv_list[model_name]=reg\ncv_rmse[model_name]=reg.best_score_\n\n#print out the best param and best score\nprint(model_name)\nprint('best training param:',reg.best_params_)\nprint('best training score rmse', reg.best_score_)\nprint('\\n')","987de087":"from sklearn.ensemble import AdaBoostRegressor\n\nmodel_name='AdaBoostRegressor'\nmodel=AdaBoostRegressor()\n\nparam_grid = [{model_name+'__'+'learning_rate' : [0.001,0.01,0.1,1,10,100]}]","41e95b8e":"pipeline = Pipeline([(model_name, model)])\n\n\nreg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=-1)\nreg.fit(X,y.to_numpy().ravel())\n\n\n#Record the best grid search paramters into the list.\ncv_list[model_name]=reg\ncv_rmse[model_name]=reg.best_score_\n\n#print out the best param and best score\nprint(model_name)\nprint('best training param:',reg.best_params_)\nprint('best training score rmse', reg.best_score_)\nprint('\\n')","f106041f":"from sklearn.ensemble import GradientBoostingRegressor\n\nmodel_name='GradientBoostingRegressor'\nmodel=GradientBoostingRegressor()\n\nparam_grid = [{model_name+'__'+'loss' : ['ls','lad','huber','quantile'],model_name+'__'+'learning_rate' : [0.01,0.1,1,10],model_name+'__'+'criterion':['friedman_mse', 'mse']}]\n","03cde82f":"pipeline = Pipeline([(model_name, model)])\n\n\nreg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=-1)\nreg.fit(X,y.to_numpy().ravel())\n\n\n#Record the best grid search paramters into the list.\ncv_list[model_name]=reg\ncv_rmse[model_name]=reg.best_score_\n\n#print out the best param and best score\nprint(model_name)\nprint('best training param:',reg.best_params_)\nprint('best training score rmse', reg.best_score_)\nprint('\\n')","661ad0da":"from xgboost import XGBRegressor\nmodel_name='XGBoost'\nmodel=XGBRegressor()\n\nparam_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [3,4,5],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\n","13a0a6f7":"\nxgb_dt=GridSearchCV(model, param_grid,n_jobs=-1,cv=5,scoring=rmse)\nxgb_dt.fit(X,y)\n\ncv_list[model_name]=xgb_dt.best_estimator_\ncv_rmse[model_name]=xgb_dt.best_score_\n\nprint(xgb_dt.best_estimator_)\nprint(xgb_dt.best_score_)","0a31935b":"# from sklearn.svm import SVR\n\n# model_name = \"SVR\"\n# model=SVR()\n\n# param_grid = [\n#   {model_name+'__'+'C': [0.1,1], model_name+'__'+'kernel': ['linear','poly','rbf','sigmoid'],\n#    model_name+'__'+'gamma':['auto']\n#   }]\n\n","dea3907a":"# pipeline = Pipeline([(model_name, model)])\n\n# reg=GridSearchCV(pipeline,param_grid,cv=5, scoring=rmse, n_jobs=2)\n# reg.fit(X,y.to_numpy().ravel())\n\n# #Record the best grid search paramters into the list.\n# cv_list[model_name]=reg\n# cv_rmse[model_name]=reg.best_score_\n\n# #print out the best param and best score\n# print(model_name)\n# print('best training param:',reg.best_params_)\n# print('best training score rmse', reg.best_score_)\n# print('\\n')","d9969635":"score = abs(pd.DataFrame.from_dict(cv_rmse,orient='index',columns=['CV Score']))\nscore = score.sort_values('CV Score')\nscore","da107e30":"test_data_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\nX_test_set = pd.read_csv(test_data_path,index_col=0)\nX_test_set.shape","f55e642f":"test_home_data_num_feature = X_test_set[home_data_num_feature.columns]\ntest_home_data_num_feature.describe().round(2)\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntest_home_data_num_feature = pd.DataFrame(imp.fit_transform(test_home_data_num_feature),columns=test_home_data_num_feature.columns,index=test_home_data_num_feature.index)\n# test_norm_home_data_num_feature = pd.DataFrame(scaler.fit_transform(test_home_data_num_feature),columns=test_home_data_num_feature.columns,index=test_home_data_num_feature.index)\ntest_home_data_num_feature","e01f69ba":"test_home_data_cat_feature = X_test_set[home_data_cat_feature.columns]\ntest_home_data_cat_feature.describe().round(2)\n\nimp = SimpleImputer(strategy=\"most_frequent\")\ntest_home_data_cat_feature=pd.DataFrame(imp.fit_transform(test_home_data_cat_feature),columns=test_home_data_cat_feature.columns,index=test_home_data_cat_feature.index)\ntest_home_data_cat_feature\n\ntest_home_data_cat_feature_dummies = enc.transform(test_home_data_cat_feature)\n\ntest_home_data_cat_feature_dummies = pd.DataFrame(test_home_data_cat_feature_dummies,columns=enc.get_feature_names(),index=test_home_data_cat_feature.index)\ntest_home_data_cat_feature_dummies","01f8a33f":"X_test = pd.concat([test_home_data_num_feature,test_home_data_cat_feature_dummies],axis=1)\nX_test","3c42651c":"predict=cv_list['LinearRegression'].predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict[:,0]},index=X_test.index)\noutput.to_csv('LinearRegression.csv', index=True)\n","b359f5da":"predict=cv_list['Lasso'].predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict},index=X_test.index)\noutput.to_csv('Lasso.csv', index=True)","cea15deb":"predict=cv_list['Ridge'].predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict[:,0]},index=X_test.index)\noutput.to_csv('Ridge.csv', index=True)","b7f89b38":"predict=cv_list['DecisionTreeRegressor'].predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict},index=X_test.index)\noutput.to_csv('DecisionTreeRegressor.csv', index=True)","29bf3382":"predict=cv_list['BaggingDecisionTreeRegressor'].predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict},index=X_test.index)\noutput.to_csv('BaggingDecisionTreeRegressor.csv', index=True)","22eae809":"predict=cv_list['RandomForestRegressor'].predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict},index=X_test.index)\noutput.to_csv('RandomForestRegressor.csv', index=True)","1dd9b0e5":"predict=cv_list['AdaBoostRegressor'].predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict},index=X_test.index)\noutput.to_csv('AdaBoostRegressor.csv', index=True)","0e819596":"predict=cv_list['GradientBoostingRegressor'].predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict},index=X_test.index)\noutput.to_csv('GradientBoostingRegressor.csv', index=True)","fbd8d5d4":"predict=xgb_dt.predict(X_test)\noutput = pd.DataFrame({'SalePrice': predict},index=X_test.index)\noutput.to_csv('xgb.csv', index=True)","99a392eb":"test_score = {'LinearRegression':17183.86239,'Lasso':16379.19466,'Ridge':16107.36134,'DecisionTreeRegressor':24232.59348,'BaggingDecisionTreeRegressor':17949.15006,'RandomForestRegressor':16163.46606,'AdaBoostRegressor':22434.87007,'GradientBoostingRegressor':15517.90164,'XGBoost':13745.37874}","405819ee":"test_score = pd.DataFrame.from_dict(test_score,orient='index')\nscore['Test Score']=test_score\nscore","4b76d124":"We will have to remove each of the pair that are highly correlated when we are cleaning the data later. One feature can be represented by the other, so there is no need for both. We will keep the feature of the pair that has more correlation with the target variable and remove the feature with the lesser correlation. The features that will be removed is in bold.\n- YearBuilt vs **GarageYrBlt**\n- **1stFlrSF** vs TotalBsmtSF\n- GrLivArea vs **TotRmsAbvGrd**\n- GarageCars vs **GarageArea**\n","e695c916":"### 2.3.1.4 Correlation among numerical feature\nWe shall find the numerical attributes with pearson correlation more than 0.8","44aefb48":"### 2.3.1.1 Numerical feature\n","c934c6a1":"### 3.2 Removing Outliers\nWe shall now drop the outliers.","8ec2a0c5":"### 5.1 Predicting target variable\nWe will now predict the target variable with the processed test set features.","358c4ce7":"### 2.3.2.1 Distribution of Categorical Feature","475f52b5":"## 1.  Introduction\nAfter finishing the syllabus of the online course and my studies, I'm quite excited about the prospect of starting this project by applying the knowledge and concepts I have picked up. In this project, I will be going the whole end-to-end data science process.\n\nThis project is based on the Kaggle Housing Prices Competition that uses the popular Boston Housing Prices dataset. The dataset includes a list of housing attributes together with the housing price as target variable (label). The purpose is to use machine learning on the dataset to train and develop a model capable of predicting the house price when attributes are inputted. ","379c0673":"### 2. Standardisation\nStandardisation should improve the score for the non-tree based regression methods. We can apply standardisation to see whether the score improve for Ridge and Lasso.","4ac6d183":"### 3.6 Combining Feature","e61de2f5":"## 2. Exploratory Data Analysis","2f853785":"### 2.2.1 Distribution of target variable (Sale Price)","fbbcd7c3":"### 4.4 Decision Tree with Bagging","1a296567":"### 1.1 Dataset\nThe following are the features of the dataset.\n\n - **SalePrice**  - the property's sale price in dollars. This is the target variable that you're trying to predict.\n -   **MSSubClass**: The building class\n -   **MSZoning**: The general zoning classification\n -   **LotFrontage**: Linear feet of street connected to property\n -   **LotArea**: Lot size in square feet\n -   **Street**: Type of road access\n -   **Alley**: Type of alley access\n -   **LotShape**: General shape of property\n -   **LandContour**: Flatness of the property\n -   **Utilities**: Type of utilities available\n -   **LotConfig**: Lot configuration\n -   **LandSlope**: Slope of property\n -   **Neighborhood**: Physical locations within Ames city limits\n -   **Condition1**: Proximity to main road or railroad\n -   **Condition2**: Proximity to main road or railroad (if a second is present)\n -   **BldgType**: Type of dwelling\n -   **HouseStyle**: Style of dwelling\n -   **OverallQual**: Overall material and finish quality\n -   **OverallCond**: Overall condition rating\n -   **YearBuilt**: Original construction date\n -   **YearRemodAdd**: Remodel date\n -   **RoofStyle**: Type of roof\n -   **RoofMatl**: Roof material\n -   **Exterior1st**: Exterior covering on house\n -   **Exterior2nd**: Exterior covering on house (if more than one material)\n -   **MasVnrType**: Masonry veneer type\n -   **MasVnrArea**: Masonry veneer area in square feet\n -   **ExterQual**: Exterior material quality\n -   **ExterCond**: Present condition of the material on the exterior\n -   **Foundation**: Type of foundation\n -   **BsmtQual**: Height of the basement\n -   **BsmtCond**: General condition of the basement\n -   **BsmtExposure**: Walkout or garden level basement walls\n -   **BsmtFinType1**: Quality of basement finished area\n -   **BsmtFinSF1**: Type 1 finished square feet\n -   **BsmtFinType2**: Quality of second finished area (if present)\n -   **BsmtFinSF2**: Type 2 finished square feet\n -   **BsmtUnfSF**: Unfinished square feet of basement area\n -   **TotalBsmtSF**: Total square feet of basement area\n -   **Heating**: Type of heating\n -   **HeatingQC**: Heating quality and condition\n -   **CentralAir**: Central air conditioning\n -   **Electrical**: Electrical system\n -   **1stFlrSF**: First Floor square feet\n -   **2ndFlrSF**: Second floor square feet\n -   **LowQualFinSF**: Low quality finished square feet (all floors)\n -   **GrLivArea**: Above grade (ground) living area square feet\n -   **BsmtFullBath**: Basement full bathrooms\n -   **BsmtHalfBath**: Basement half bathrooms\n -   **FullBath**: Full bathrooms above grade\n -   **HalfBath**: Half baths above grade\n -   **Bedroom**: Number of bedrooms above basement level\n -   **Kitchen**: Number of kitchens\n -   **KitchenQual**: Kitchen quality\n -   **TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n -   **Functional**: Home functionality rating\n -   **Fireplaces**: Number of fireplaces\n -   **FireplaceQu**: Fireplace quality\n -   **GarageType**: Garage location\n -   **GarageYrBlt**: Year garage was built\n -   **GarageFinish**: Interior finish of the garage\n -   **GarageCars**: Size of garage in car capacity\n -   **GarageArea**: Size of garage in square feet\n -   **GarageQual**: Garage quality\n -   **GarageCond**: Garage condition\n -   **PavedDrive**: Paved driveway\n -   **WoodDeckSF**: Wood deck area in square feet\n -   **OpenPorchSF**: Open porch area in square feet\n -   **EnclosedPorch**: Enclosed porch area in square feet\n -   **3SsnPorch**: Three season porch area in square feet\n -   **ScreenPorch**: Screen porch area in square feet\n -   **PoolArea**: Pool area in square feet\n -   **PoolQC**: Pool quality\n -   **Fence**: Fence quality\n -   **MiscFeature**: Miscellaneous feature not covered in other categories\n -   **MiscVal**: $Value of miscellaneous feature\n -   **MoSold**: Month Sold\n -   **YrSold**: Year Sold\n -   **SaleType**: Type of sale\n -   **SaleCondition**: Condition of sale","a4b8a158":"### 1.3 Importing of Data","34a796d3":"## 7. Future Studies","79599951":"### 2.3.1.3 Numerical Feature - Bivariate analysis\nThe scatterplots of SalePrice against each numerical attribute is shown below through the bivariate analysis. ","abe3accd":"We will use OneHotEncoders instead of drop_dummies because ...","709f4240":"## 4 Modelling","8ecf1a64":"We should remove Alley, PoolQc, Fence and Misc Feature. The proportion of non-missing values to the sample size is too small. \nFor these feature, Uni-variate impuding doesnt make sense as that will mean adding the mean of the feature as information.\nMulti-variate impuding may make sense as we can draw relationships the feature have with others to estimate their missing values. However, multi-variate impuding for catergorical data is not supported yet in libraries. \n\nWe shall drop these 4 features. ","eeb22a3f":"We first import test data set.","883f550f":"### 2.1 Splitting the data into the target variable\nBelow proceding further, lets split the data its features and target variable.","662dd1a0":"### 4.2 Lasso","df5f1b71":"Merging both numerical features and categorical features of the test set","6c7482aa":"### 3.4 Handling Missing Data - Categorical Feature","ff5565dc":"### 2.2 Target Variable","367ec0f7":"The highlightly correlated variables (pearson correlation>0.8) are: \n- YearBuilt vs GarageYrBlt\n- 1stFlrSF vs TotalBsmtSF\n- GrLivArea vs TotRmsAbvGrd\n- GarageCars vs GarageArea","dbc2e299":"Then, we handle the catergorical feature of the dataset.","f161e88d":"The shape of the data is 1460 rows and 80 columns.","8ad1835e":"Below are the descriptive statistics of the data:","e2b4dede":"In this project, XGBoost is the best performing model with a score of 13745. This achieved ranking of 891 (top 3%) on the leadership board.","46884219":"### 4.8 XGBoost","08769533":"### 4.3 Ridge","6ded404f":"### 4.5 RandomForestRegressor","d1de4642":"Lets first look at the characteristics of the data. ","302e536c":"From above, it can be observed that the following points are outliers:\n\n- LotFrontage (>200)\n- GrLivArea (>4000 AND SalePrice <300000)\n- LowQualFinSF(>550)\n- BsmFinSF1(>4000)\n- LotArea(>100000)\n- 1stFlrSF(>4000)\n- TotalBsmtSF(>6000)\n\nOutliers will be removed later on.","5e6c1d97":"### 4.9 Support Vector Machine","af17d955":"### 4.10 Summary of Cross Validation results","5febf52e":"### 3.3 Handling Missing Data - Numerical Feature","a1aa1655":"## 5. Prediction","a7f81b68":"Features that have very weak correlation with the target variable should also be removed (-0.1 < pearson correlation < 0.1).","c1337e5d":"### 1.2 Importing modules","7d11b2fa":"## 3. Data Cleaning and Preprocessing","f9cad9ab":"### 4. Ensemble\nEnsemble can used on all models to see if the aggregated results from hard-voting or soft-voting yield better score. This should be easy to implement.","cc7fe8c3":"My personal thoughts are that more could be done for the top 4 regression model. \n\n### 1. Principal Component Analysis\nPrincipal Component Analysis could be implement on the features. This could be key in improving the score given the amount of dummy variable we have (196). In the project above, the dummy variables couldnt be processed. Unlike the numerical variable which correlation can be studied and the correlated features be dropped, we are unable to do so the dummy variables. Principal component analysis provide a way to reduce the large amount of dummy variables to an optimal size. ","bb0e5e59":"### 3.5 Encoding Categorical Feature","472491d1":"Features to be removed due to low correlation with target variable:\n - PoolArea                                   \n - MoSold                                     \n - 3SsnPorch                                  \n - BsmtFinSF2                                 \n - BsmtHalfBath                               \n - MiscVal                                    \n - LowQualFinSF                               \n - YrSold                                     \n - OverallCond                                \n - MSSubClass                     \n","797d15c5":"### 2.3.2.2 Boxplot of Categorical Feature","ee683c28":"We observed that XGBoost is the best performer with decision tree regressor the worst. In this project, as mentioned above, we shall make predictions on all models and send all predictions for submission to get the score. ","a95e1abd":"For categorical data, there are some features with a high proportion of missing data. This will be an issue.","257c6696":"### 4.7 GradientBoosting","36c22fc9":"### 3.1 Dropping unnecessary features","c5844e03":"We will be using K-fold Cross Validation as the validation method. The training data set (X and y) that we have processed so far will be split in 5 fold. One of the fold will be used as the validation set while the rest will be used to training the data. The data will be trained using the following algorithms:\n\n- Linear Regression\n- Linear Regression (Lasso)\n- Linear Regression (Ridge)\n- Decision Tree\n- Decision Tree with Bagging\n- RandomForestRegressor\n- Adaboost regressor\n- GradientBoosting\n- XGBoost\n- Support Vector Regressor\n\nDuring training, Gridsearch will be used to search for the parameters. As Gridsearch will run all the parameters to determine the best parameters, it could lead to over-fitting of the training set. Over-fitting means that the model is fitted too closely and specifically to the training set. However, when making prediction on new data set, it will not be accurate as the model could not be generalised beyond the training set. To prevent overfitting, Gridsearch will determine the best parameter by the best score (lowest RMSE) on the validation set.'\n\nWe will then use the models above and their gridsearch optimized parameters to make prediction of the test set. The predictions will be uploaded to Kaggle website to determine the model's performance. \n","8aa0b01c":"We will use SimpleImputer to fill the missing data. SimpleImputer will fill the missing values with the mean value for that feature.","cb64e878":"The target variable is slightly skewed to the right.","48863fce":"### 4.1 Linear Regression","fc85c87b":"Next, we manage the numerical feature of the dataset","16fdddf7":"### 2.3 Feature","9392d064":"## 6. Submission for scoring and Summary of results","f16692ea":"### 3. Multi-variate Imputation\nFor the categorical feature, we can look into how to use the relationships that the feature of the missing values shared with other features to derive the missing values. This perhaps can help us to make estimation for features with large proportion of missing values. So far, we have dropped quite some catergorical features because of the missing values and our limitation to the use of uni-variate imputation. There may be some difficulty on this as multi-variate imputation for categorical feature is not supported by sklearn yet.","5ba3dddf":"### 4.6 AdaBoostRegressor","385e9532":"### 2.3.2 Categorical Feature","fb7315f5":"There are a total of 80 variables including the target variable (Sale Price). There are quite some missing values which will require to be handled later on. There are 37 numerical features and 43 categorical features.","344177f6":"### 4.3 Decision Trees","c944d5d7":"All excel output files are submitted to Kaggle to be checked against their hidden test set.\n\nThe following are the results:","f42a4d3c":"### 2.3.1.2 Numerical feature - Univariate analysis","0f601528":"For the remaining categorical feature, we will impute the missing values with \"most frequent\" strategy. ","4d57290d":"We will first manage the missing numerical feature data. Only LotFontage has some missing data. I do not think it is a big issue as only a small proportion of data is missing.","3a79e920":"### 2.3.1.5 Correlation between numerical features and target variable"}}