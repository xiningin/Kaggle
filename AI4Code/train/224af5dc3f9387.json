{"cell_type":{"4372ca87":"code","ff43c382":"code","c96032e3":"code","f29b9c17":"code","86820355":"code","6961ea6c":"code","c1344092":"code","332341e9":"code","57ba3a9c":"code","c1d27532":"code","3725186e":"code","8a9a704d":"code","f7c3d182":"code","2dba9cd5":"markdown","d1316577":"markdown","8633c9a0":"markdown","67bd730b":"markdown","036e3def":"markdown","7db52783":"markdown","bb6d6e69":"markdown","a8e0d124":"markdown","b695a08e":"markdown","a8bf9152":"markdown","90f75417":"markdown","6c33f038":"markdown","ebf08f76":"markdown","7e840217":"markdown","ecbad4a8":"markdown","e1eb3ba8":"markdown"},"source":{"4372ca87":"# Install:\n# Kaggle environments.\n!git clone https:\/\/github.com\/Kaggle\/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.7 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.7.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . \n\n# Install Gym\n!pip install gym\n\nfrom IPython.display import clear_output\nclear_output()","ff43c382":"import gfootball\nimport gym\nimport numpy as np\nimport tensorflow as tf\nimport random\nfrom collections import deque","c96032e3":"class ReplayBuffer:\n    def __init__(self, size=1000000):\n        self.memory = deque(maxlen=size)\n        \n    def remember(self, s_t, a_t, r_t, s_t_next, d_t):\n        self.memory.append((s_t, a_t, r_t, s_t_next, d_t))\n        \n    def sample(self,batch_size):\n        batch_size = min(batch_size, len(self.memory))\n        return random.sample(self.memory,batch_size)","f29b9c17":"class SplitLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, inputs):\n        split0,split1,split2,split3 = tf.split(inputs,4,3)\n        return [split0,split1,split2,split3]","86820355":"class ConvNN:\n\n    def __init__(self,state_shape,num_actions):\n        self.state_shape = state_shape\n        self.num_actions = num_actions\n\n    def _model_architecture(self):\n    \n        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=1)\n        frames_split = SplitLayer()(frames_input)\n        conv_branches = []\n        for f,frame in enumerate(frames_split):\n            conv_branches.append(self._build_conv_branch(frame,f))\n\n        concat = tf.keras.layers.concatenate(conv_branches)\n        \n        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', \n                                     activation='relu')(concat)\n        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', \n                                     activation='relu')(fc0)\n        fc2 = tf.keras.layers.Dense(units=512, name='fc2', \n                                     activation='relu')(fc1)\n        fc3 = tf.keras.layers.Dense(units=256, name='fc3', \n                                     activation='relu')(fc2)\n        fc4 = tf.keras.layers.Dense(units=64, name='fc4', \n                                     activation='relu')(fc3)\n        \n        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',\n                                               activation='relu')(fc4)\n        return frames_input,action_output\n        \n    @staticmethod\n    def _build_conv_branch(frame,number):\n        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),\n                                        name='conv1_frame'+str(number), padding='same',\n                                        activation='relu')(frame)\n        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp1_frame\"+str(number))(conv1)\n        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),\n                                        name='conv2_frame'+str(number),padding='same',\n                                        activation='relu')(mp1)\n        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp2_frame\"+str(number))(conv2)\n        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n                                        name='conv3_frame'+str(number),padding='same',\n                                        activation='relu')(mp2)\n        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp3_frame\"+str(number))(conv3)\n        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),\n                                        name='conv4_frame'+str(number), padding='same',\n                                        activation='relu')(mp3)\n        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=\"mp4_frame\"+str(number))(conv4)\n\n        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)\n\n        return flatten\n        \n    def build(self):\n        frames_input,action_output = self._model_architecture()\n        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])\n        return model","6961ea6c":"class Agent:\n    def __init__(self, state_shape, num_actions,alpha, gamma, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1):\n        self.epsilon_i = epsilon_i\n        self.epsilon_f = epsilon_f\n        self.n_epsilon = n_epsilon\n        self.epsilon = epsilon_i\n        self.gamma = gamma\n        self.state_shape = state_shape\n        self.num_actions = num_actions\n        self.optimizer = tf.keras.optimizers.Adam(alpha) \n\n        self.Q = ConvNN(state_shape,num_actions).build()\n        self.Q_ = ConvNN(state_shape,num_actions).build()\n        \n    def synchronize(self):\n        self.Q_.set_weights(self.Q.get_weights())\n\n    def act(self, s_t):\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.num_actions,size=3)\n        return np.argmax(self.Q(s_t), axis=1)\n    \n    def decay_epsilon(self, n):\n        self.epsilon = max(\n            self.epsilon_f, \n            self.epsilon_i - (n\/self.n_epsilon)*(self.epsilon_i - self.epsilon_f))\n\n    def update(self, s_t, a_t, r_t, s_t_next, d_t):\n        with tf.GradientTape() as tape:\n            Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1))\n            Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float32), axis=1)\n            loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2)\n        grads = tape.gradient(loss, self.Q.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))","c1344092":"class VectorizedEnvWrapper(gym.Wrapper):\n    def __init__(self, make_env, num_envs=1):\n        super().__init__(make_env())\n        self.num_envs = num_envs\n        self.envs = [make_env() for env_index in range(num_envs)]\n    \n    def reset(self):\n        return np.asarray([env.reset() for env in self.envs])\n    \n    def reset_at(self, env_index):\n        return self.envs[env_index].reset()\n    \n    def step(self, actions):\n        next_states, rewards, dones, infos = [], [], [], []\n        for env, action in zip(self.envs, actions):\n            next_state, reward, done, info = env.step(action)\n            next_states.append(next_state)\n            rewards.append(reward)\n            dones.append(done)\n            infos.append(info)\n        return np.asarray(next_states), np.asarray(rewards), \\\n            np.asarray(dones), np.asarray(infos)","332341e9":"class NormalizationWrapper(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n    \n    def observation(self, obs):\n        return obs\/255","57ba3a9c":"train_agent = Agent((72,96,4),19,alpha=0.1,gamma=0.95)\ntrain_agent.Q.save_weights(\"QWeights\")\ntrain_agent.Q_.save_weights(\"Q_Weights\")\nsmm_env = VectorizedEnvWrapper(lambda: NormalizationWrapper(gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")),1)\ntrain_buffer = ReplayBuffer()","c1d27532":"def train(env=smm_env,T=3001,batch_size=3,sync_every=10,agent=train_agent,buffer=train_buffer):\n    \n    agent.Q.load_weights(\"QWeights\")\n    agent.Q_.load_weights(\"Q_Weights\")\n    rewards = []\n    episode_rewards = 0\n    s_t = env.reset()\n    \n    for t in range(T):\n        if t%sync_every == 0:\n            agent.synchronize()\n        a_t = agent.act(s_t)\n        s_t_next, r_t, d_t, info = env.step(a_t)\n        buffer.remember(s_t, a_t, r_t, s_t_next, d_t)\n        for batch in buffer.sample(batch_size):\n            agent.update(*batch)\n        agent.decay_epsilon(t\/T)\n        episode_rewards += r_t\n           \n    for i in range(env.num_envs):\n        rewards.append(episode_rewards[i])\n        episode_rewards[i] = 0\n        s_t[i] = env.reset_at(i)\n        \n    agent.epsilon_i = 1.0\n    agent.epsilon_f = 0.01\n    agent.n_epsilon = 0.1\n    agent.epsilon = agent.epsilon_i\n    \n    return rewards","3725186e":"for i in range(2):\n    print(train())\n    train_agent.Q.save_weights(\"QWeights\")\n    train_agent.Q_.save_weights(\"Q_Weights\")","8a9a704d":"%%writefile submission.py\n\nfrom gfootball.env import observation_preprocessing\nimport numpy as np\nimport tensorflow as tf\n\nclass SplitLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, inputs):\n        split0,split1,split2,split3 = tf.split(inputs,4,3)\n        return [split0,split1,split2,split3]\n    \nclass ConvNN:\n\n    def __init__(self,state_shape,num_actions):\n        self.state_shape = state_shape\n        self.num_actions = num_actions\n\n    def _model_architecture(self):\n    \n        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=1)\n        frames_split = SplitLayer()(frames_input)\n        conv_branches = []\n        for f,frame in enumerate(frames_split):\n            conv_branches.append(self._build_conv_branch(frame,f))\n\n        concat = tf.keras.layers.concatenate(conv_branches)\n        \n        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', \n                                     activation='relu')(concat)\n        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', \n                                     activation='relu')(fc0)\n        fc2 = tf.keras.layers.Dense(units=512, name='fc2', \n                                     activation='relu')(fc1)\n        fc3 = tf.keras.layers.Dense(units=256, name='fc3', \n                                     activation='relu')(fc2)\n        fc4 = tf.keras.layers.Dense(units=64, name='fc4', \n                                     activation='relu')(fc3)\n        \n        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',\n                                               activation='relu')(fc4)\n        return frames_input,action_output\n        \n    @staticmethod\n    def _build_conv_branch(frame,number):\n        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),\n                                        name='conv1_frame'+str(number), padding='same',\n                                        activation='relu')(frame)\n        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp1_frame\"+str(number))(conv1)\n        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),\n                                        name='conv2_frame'+str(number),padding='same',\n                                        activation='relu')(mp1)\n        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp2_frame\"+str(number))(conv2)\n        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n                                        name='conv3_frame'+str(number),padding='same',\n                                        activation='relu')(mp2)\n        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp3_frame\"+str(number))(conv3)\n        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),\n                                        name='conv4_frame'+str(number), padding='same',\n                                        activation='relu')(mp3)\n        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=\"mp4_frame\"+str(number))(conv4)\n\n        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)\n\n        return flatten\n        \n    def build(self):\n        frames_input,action_output = self._model_architecture()\n        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])\n        return model\n\nclass Agent:\n    def __init__(self, state_shape, num_actions):\n        \n        self.state_shape = state_shape\n        self.num_actions = num_actions\n\n        self.Q = ConvNN(state_shape,num_actions).build()\n        self.Q_ = ConvNN(state_shape,num_actions).build()\n        \n    def act(self, s_t):\n        return np.argmax(self.Q(s_t), axis=1)\n    \nDQN_Agent = Agent((72,96,4),19)\n\nDQN_Agent.Q.load_weights(\"QWeights\")\nDQN_Agent.Q_.load_weights(\"Q_Weights\")\n\ndef agent(obs):\n    obs = obs['players_raw'][0]\n    obs = observation_preprocessing.generate_smm([obs])\n    obs = obs\/255 #Normalization\n    action = DQN_Agent.act(obs)\n    return [action[0]]","f7c3d182":"# Set up the Environment.\nfrom kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\noutput = env.run([\"\/kaggle\/working\/submission.py\", \"do_nothing\"])[-1]\nprint('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))\nenv.render(mode=\"human\", width=800, height=600)","2dba9cd5":"Below are imported the useful packages","d1316577":"To determinate Q values we use the neural network described below. Each output of the SplitLayer is going through a sequence of layers built by the build_conv_branch function. The results of each branch is concatened with the others one. After a couple of Dense layers the action chosen is given by the action_output layer.","8633c9a0":"To write a submission, we only need a lightened version of the agent, the ConvNN architecture, and to load the trained weights\n","67bd730b":"Our Agent will use the SMM representation of the Gfootball environment\n\nFrom [Gfootball GitHub](https:\/\/github.com\/google-research\/football\/blob\/master\/gfootball\/doc\/observation.md)\nSimplified spacial (minimap) representation of a game state. It consists of several 72 * 96 planes of bytes, filled in with 0s except for:\n\n1st plane: 255s represent positions of players on the left team\n\n2nd plane: 255s represent positions of players on the right team\n\n3rd plane: 255s represent position of a ball\n\n4th plane: 255s represent position of an active player\n\nAs [@garethjns](https:\/\/www.kaggle.com\/garethjns\/convolutional-deep-q-learner) mentionned, Conv2D Layers will have more efficiency by treating only one plane. Thereby we need to create a SplitLayer","036e3def":"We first install the required components","7db52783":"The VectorizedEnvWrapper allows us to use multiple envs in parallel","bb6d6e69":"This notebook is inspired from [@garethjns ideas & work](https:\/\/www.kaggle.com\/garethjns\/convolutional-deep-q-learner) and [Alexander Van de Kleut's GitHub](https:\/\/alexandervandekleut.github.io\/) (especially [this article](https:\/\/alexandervandekleut.github.io\/deep-q-learning\/))","a8e0d124":"We train the model a couple of times","b695a08e":"The agent starts by exploring the environment, picking random actions. The first Q-values are then determinated. As the epsilon is decaying, it get more interested in using his neural network to estimate the next action to choose. The update method maximizes the next Q-value,judge the current Q-value, calculate the loss function, and minimize it. [See here](https:\/\/alexandervandekleut.github.io\/function-approximation\/) and [here](https:\/\/alexandervandekleut.github.io\/q-learning-tensorflow\/)","a8bf9152":"We create a ReplayBuffer to remember state transitions. It helps the agent to perform better on old transitions. We sample them randomly to get a nice balance between ancient and new experiences.","90f75417":"We can see how our agent perform against a 'do_nothing' opponent","6c33f038":"The agent makes very poor decisions, surely because it hasn't trained enough ([Google Research Paper on Gfootball](https:\/\/arxiv.org\/pdf\/1907.11180.pdf) implies that we need a lot more of training). But I think there are different aspects we can still improve on in order to get slightly better results. Maybe a [CheckpointRewardWrapper](https:\/\/github.com\/google-research\/football\/blob\/7a5ae3b5c849beb69d15a7c0028f16d7297b1d9f\/gfootball\/env\/wrappers.py#L275) adapted for SMM representations returning a more explicit reward to the agent could help","ebf08f76":"The training method is defined below ","7e840217":"We produce a train_agent and the Gfootball env","ecbad4a8":"Q-Learning associates the \"goodness\" of picking an action a_t in a state s_t, while following a certain policy. That 's what we call the Q-Value. The policy is then created by selecting for each state the action maximizing the Q_value. [See here](https:\/\/alexandervandekleut.github.io\/mdp\/) and [here](https:\/\/alexandervandekleut.github.io\/q-learning-numpy\/)","e1eb3ba8":"Just dividing by 255 to get tensors filled with 0 and 1"}}