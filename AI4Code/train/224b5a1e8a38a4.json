{"cell_type":{"ab36785c":"code","3b83ce85":"code","fe7395b6":"code","ba25912a":"code","03fd4dc3":"code","136e3c42":"code","9a2f6bb9":"code","b8d3b730":"code","e29c5085":"code","ef008dda":"code","328fcdf0":"code","baf35e2d":"code","ca13cc4b":"code","3286ee81":"code","b25ec36c":"code","d921d925":"code","4ea35e57":"code","e5f4f0a7":"code","8fece836":"code","4fa53c54":"code","e81f34a2":"code","2f3dd623":"code","2984baca":"code","dbe67482":"code","734eb21e":"code","d1531986":"code","51cd948d":"code","0429b9d9":"code","d8f1ce40":"code","3e922f44":"code","9fa77361":"code","a2509b48":"code","bcdf0edc":"code","a3d40445":"code","78f360e1":"code","608ae2c4":"code","1f193cb6":"code","79127884":"code","b7858bb3":"code","90f71e82":"code","8513c8d6":"code","4c31c7c9":"code","5123824f":"code","25c046da":"code","abd71301":"code","0e6c46f9":"code","daeeac91":"code","b811e636":"code","bc424d5e":"code","fa975f97":"code","72a205e8":"code","c6c9b2bd":"markdown","8214d6fa":"markdown","9786833c":"markdown","1b53029a":"markdown","1c4f1277":"markdown","eb3d9bc8":"markdown","d3fa1524":"markdown","0dff425f":"markdown","f430d910":"markdown","a0b75a20":"markdown","bf77eda6":"markdown"},"source":{"ab36785c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem import WordNetLemmatizer\nimport gensim\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\nimport string\nimport json\nimport re\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3b83ce85":"m = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","fe7395b6":"m = m[(m['title'].notna() & m['abstract'].notna())]","ba25912a":"w_tokenizer = WhitespaceTokenizer()\nlemmatizer = WordNetLemmatizer()","03fd4dc3":"def preprocess(sentence):\n    sentence = sentence.lower()\n    sentence_no_punctuation = sentence.translate(str.maketrans('', '', string.punctuation))\n    lemmatized_list = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(sentence_no_punctuation) \n                  if w not in stopwords.words('english')]\n    return lemmatized_list","136e3c42":"m['abstract_lemmatized']=m['abstract'].map(lambda s:preprocess(s)) ","9a2f6bb9":"data_words = list(m['abstract_lemmatized'])","b8d3b730":"bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\nbigram_mod = gensim.models.phrases.Phraser(bigram)","e29c5085":"trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \ntrigram_mod = gensim.models.phrases.Phraser(trigram)","ef008dda":"def make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]","328fcdf0":"def make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]","baf35e2d":"m['abstract_lemmatized_grams']= make_trigrams(m['abstract_lemmatized'])","ca13cc4b":"def abstract_to_string(text):\n    return ' '.join(word for word in text)","3286ee81":"m['cleanAbstract'] = m['abstract_lemmatized_grams'].map(lambda s:abstract_to_string(s))","b25ec36c":"count_vectorizer = CountVectorizer(stop_words='english')","d921d925":"data_vectorized = count_vectorizer.fit_transform(m['cleanAbstract'])","4ea35e57":"number_topics = 5","e5f4f0a7":"lda = LDA(n_components=number_topics, n_jobs=-1)","8fece836":"lda.fit(data_vectorized)","4fa53c54":"# Helper function\ndef print_topics(model, count_vectorizer, n_top_words=10):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))","e81f34a2":"# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer)","2f3dd623":"topics = lda.transform(data_vectorized)","2984baca":"for idx in range(number_topics):\n    col_name = 'Topic ' + str(idx)\n    m[col_name] = topics[:, idx]","dbe67482":"non_pharm = m[(m['abstract'].str.contains('non-pharm'))]","734eb21e":"topic_cols = [x for x in m.columns if 'Topic ' in x]","d1531986":"non_pharm_topics = non_pharm[topic_cols].idxmax(axis=1)","51cd948d":"def most_frequent(List): \n    return max(set(List), key = List.count)","0429b9d9":"Counter(non_pharm_topics)","d8f1ce40":"top_topic = most_frequent(list(non_pharm_topics))\n\ntop_topic","3e922f44":"m['Top_Topic'] = m[topic_cols].idxmax(axis=1)","9fa77361":"m.groupby('Top_Topic').size()","a2509b48":"top_topic_papers = m[m['Top_Topic'] == top_topic]","bcdf0edc":"covid_keywords = ['corona', 'covid']","a3d40445":"intervention_keywords = ['social distancing',\n                        'contact tracing',\n                        'case isolation',\n                        'shelter-in-place',\n                        'stay-at-home',\n                        'movement restriction',\n                        'event cancel',\n                        'face mask',\n                        'facial mask',\n                        'travel ban',\n                        'school closure']","78f360e1":"def find_papers_w_keywords(topic_keywords, papers):\n    for keyword in topic_keywords:\n        num_papers_title = len(papers[(papers['title'].str.contains(keyword)) & \n                                        (papers['title'])])\n        num_papers_abstract = len(papers[papers['abstract'].str.contains(keyword)])\n        print ('Identified {} papers with \"{}\" in title, {} relevant papers with \"{}\" in abstract'\\\n                       .format(num_papers_title, keyword, num_papers_abstract, keyword)) ","608ae2c4":"date_filter = '2019-12-01'","1f193cb6":"find_papers_w_keywords(covid_keywords, top_topic_papers)","79127884":"top_topic_papers['core_abstract'] = top_topic_papers['abstract'].apply(lambda x: any([k in x for k in covid_keywords]))","b7858bb3":"covid_papers = top_topic_papers[(top_topic_papers['core_abstract'] == True) & \n                                (top_topic_papers['publish_time'] >= date_filter)]","90f71e82":"for keyword in intervention_keywords:\n    covid_papers[keyword] = covid_papers['abstract'].str.contains(keyword)","8513c8d6":"covid_papers['# Keywords in Abstract'] = covid_papers[intervention_keywords].sum(axis=1)","4c31c7c9":"find_papers_w_keywords(intervention_keywords, covid_papers)","5123824f":"intervention_papers = covid_papers[covid_papers['# Keywords in Abstract'] > 1]","25c046da":"len(intervention_papers)","abd71301":"intervention_papers.to_csv(\"intervention_papers_metadata.csv\", index=False)","0e6c46f9":"def find_keyword(keywords, text):\n    \"\"\"\n    Iterates through a list of keywords and searches them in a string of text.\n\n    inputs:\n      keywords: list of keywords\n      text: string of text\n\n    output: number of times keywords are found in the text\n    \"\"\"\n    find = []\n    for keyword in keywords:\n        find.extend(re.findall(keyword, text.lower()))\n    return len(find)","daeeac91":"def search_body_text(sha, folder1, folder2, keywords, sentence_only):\n    \"\"\"\n    Searches a single full length text for sentences\/paragraphs which contain a list of keywords.\n\n    inputs:\n      sha: sha file name\n      folder1: text folder name\n      folder2: pdf or pmc folder name\n      keywords: list of keywords to search for\n      sentence_only: whether or not to show sentence only or full paragraph\n    \n    output: list of sentences\/paragraphs found containing keywords\n    \"\"\"\n\n    #open text file\n    with open('\/kaggle\/input\/CORD-19-research-challenge\/'+folder1+'\/'+folder1+'\/'+folder2+'\/'+sha+'.json') as f:\n        file = json.load(f)\n    \n    found = []\n    for text_dict in file[\"body_text\"]:\n        \n        #if show_sentence_only, then split the paragraph into sentences, then look for keywords\n        if sentence_only:\n            sentences = text_dict[\"text\"].split(\". \")\n            for sentence in sentences:\n                count = find_keyword(keywords, sentence)\n                if count > 0:\n                    found.append(sentence)\n                    \n        #otherwise, show the whole paragraph\n        else:\n            count = find_keyword(keywords, text_dict[\"text\"])\n            if count > 0:\n                #print(text_dict[\"section\"])\n                found.append(text_dict[\"text\"])\n                \n    return(found)","b811e636":"def automated_lit_search(metadata_subset, keywords, sentence_only=True):\n    \"\"\"\n    Creates a table keyword findings.\n    \n    inputs:\n      metadata_subset: subset of metadata file to search\n      keywords: list of keywords to search\n      sentence_only: whether or not to show sentence only or full paragraph\n    \n    output: dataframe table of results with columns containing index, title, and text snippet\n    \"\"\"\n    results = []\n    \n    indices = metadata_subset[metadata_subset['has_pdf_parse'] == True].index\n    indices_pmc = metadata_subset[metadata_subset['has_pmc_xml_parse'] == True].index\n    indices.append(indices_pmc)\n    \n    for index in indices:\n        \n        #find text location\n        sha = metadata_subset[\"sha\"][index].split(';')[0]\n        folder1 = metadata_subset[\"full_text_file\"][index]\n        if metadata_subset['has_pdf_parse'][index] == True:\n            folder2 = 'pdf_json'\n        elif metadata_subset['has_pmc_xml_parse'][index] == True:\n            folder2 = 'pmc_json'\n        \n        #open text and search for keywords\n        found = search_body_text(sha, folder1, folder2, keywords, sentence_only)\n        if len(found) > 0:\n            for f in found:\n                results.append([index, metadata_subset[\"title\"][index], f])\n                \n    results_df = pd.DataFrame(results, columns=[\"index\",\"title\",\"text\"])\n    return(results_df)","bc424d5e":"intervention_sentences = automated_lit_search(intervention_papers, intervention_keywords, True)\nintervention_sentences.to_csv('intervention_sentences.csv', index=False)","fa975f97":"intervention_paragraphs = automated_lit_search(intervention_papers, intervention_keywords, False)\nintervention_paragraphs.to_csv('intervention_paragraphs.csv', index=False)","72a205e8":"list(intervention_papers['title'])","c6c9b2bd":"## Looking for Topic associated with NPI (non-pharm in abstract)","8214d6fa":"## Identify core papers - about COVID-19 (keyword search and published date)","9786833c":"## LDA on Cleaned Abstracts","1b53029a":"- IDs: 4743095, 2077273, Marcus Dreyer, 4894652, 4833887\n- usernames: helenlord, katieymo, Marcus Dreyer, dincerti, shemrarizzo\n- Emails: lord.helen [at] gene [dot] com, katieymo [at] gmail [dot] com, dreyer [at] itprodqs-consultng [dot] com, devin.incerti [at] gmail [dot] com, shem.rizzo [at] gmail [dot] com\n\n\n## Goal: to identify papers which discuss specific non-pharmaceutical interventions to decrease the spread of COVID-19\n\n*Focus: Methods to control the spread in communities, barriers to compliance and how these vary among different populations*\n\n\n## Methodology: \n1) Cleaned abstracts\n\n2) Use LDA on cleaned abstracts to identify papers most relevant to NPI topics\n\n3) Use keyword search to pull out NPI papers which focus on: methods to control the spread in communities, barriers to compliance and how these vary among different populations\n\n4) Pull out specific sentences and paragraphs from the identified papers with the keywords for quick identification\n\n## Notes:\n- The keyword search alone on the metadata is a useful methodology to identify relevant papers. It can be completed on either the title or the abstract. However, finding pertinent, specific keywords is very important for success\n- Cleaning the text and using topic modeling focuses on a smaller, more relevant subset of papers, allowing this methodology to scale. However, a some relevant papers may be eliminated by the topic modeling","1c4f1277":"## Print top words associated with Topics","eb3d9bc8":"## Clean Abstracts\n\n- lowercase\n- remove punctuation\n- remove stopwords\n- lemmatize\n- bigrams\/trigrams","d3fa1524":"## Keywords\n\n- Need a core covid keyword\n- And need a topic keyword","0dff425f":"## Compared sklean and gensim LDA models, completed GridSearch for n_components (options 5, 10, 15, 20, 25, 30, 35). The best model was sklearn and parameter was 5 - used below","f430d910":"## Intervention Papers","a0b75a20":"## Search Full Papers for relevant sentences and paragraphs","bf77eda6":"## Take the topics that match most for NPI modeling papers, then find all papers with that as their top topic"}}