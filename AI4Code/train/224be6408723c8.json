{"cell_type":{"0284a6f2":"code","e399d08e":"code","a96dd9a7":"code","7f529811":"code","5d647765":"code","03c6e692":"code","2fa3ca8e":"code","c0d206f0":"code","1b2476e7":"code","07140350":"code","9cb94a23":"markdown"},"source":{"0284a6f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os","e399d08e":"img_size = 380\nbatch_size = 32","a96dd9a7":"basePath = '\/kaggle\/input\/cassava-leaf-disease-classification\/'\ntrainImagesPath = basePath + 'train_images\/'\ntestImagesPath = basePath + 'test_images\/'\ntrain_info = pd.read_csv(basePath + 'train.csv')","7f529811":"trainingSize = train_info.shape\nprint('The number of training examples are: ',trainingSize[0])","5d647765":"import seaborn as sns\nsns.countplot(train_info['label'])","03c6e692":"from tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array,load_img\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\npolicy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\nmixed_precision.set_policy(policy)\n\ndef buildModel(IMG_SIZE,NUM_CLASSES):\n    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    model = EfficientNetB4(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n    #model.trainable = False\n\n    # Rebuild top\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = layers.BatchNormalization()(x)\n\n    top_dropout_rate = 0.2\n    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\",dtype=\"float32\",name=\"pred\")(x)\n    \n    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model","2fa3ca8e":"import matplotlib.pyplot as plt\n\ndef plot_hist(hist):\n    plt.figure()\n    plt.plot(hist.history[\"accuracy\"])\n    plt.plot(hist.history[\"val_accuracy\"])\n    plt.title(\"model accuracy\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","c0d206f0":"train_datagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\nval_datagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')","1b2476e7":"from sklearn.model_selection import train_test_split\ntrain_info['label'] = train_info['label'].astype('str')\ntrain_df, val_df = train_test_split(train_info, test_size=0.1, random_state=42)","07140350":"train_generator = train_datagen.flow_from_dataframe(\n    train_df,directory=trainImagesPath,\n    x_col=\"image_id\",y_col=\"label\",\n    target_size=(img_size, img_size), \n    batch_size=batch_size,class_mode = \"sparse\"\n    ) \nval_generator = val_datagen.flow_from_dataframe(\n    val_df,directory=trainImagesPath,\n    x_col=\"image_id\",y_col=\"label\",\n    target_size=(img_size, img_size),\n    batch_size=batch_size,class_mode = \"sparse\"\n    )     \nmodel = buildModel(img_size,5)\nearlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.00001)\nmcp_save = ModelCheckpoint('best_mdl.h5', save_best_only=True, monitor='val_loss', mode='min')\nhist = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_df.shape[0]  \/\/ batch_size,\n    epochs=20,\n    validation_data=val_generator,\n    validation_steps=val_df.shape[0]  \/\/ batch_size,\n    callbacks=[earlyStopping, mcp_save,reduceLROnPlat])\nplot_hist(hist)","9cb94a23":"It is obvious that the data is not distributed uniformly in the training set with more than half of the training images in class label 3. But the metric for the model is set to be categorization accuracy. Will it make sense?"}}