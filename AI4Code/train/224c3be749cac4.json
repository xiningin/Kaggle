{"cell_type":{"5737b787":"code","05ff53b4":"code","8e7a1cd4":"code","a73ddc13":"code","8e5fe23d":"code","a6d6e422":"code","a61961b2":"code","837842ff":"code","002feacb":"code","2c36f1d9":"code","d1770514":"code","608390b0":"code","827ed941":"code","84a8ac65":"code","69368760":"code","8ba8d5ee":"code","0400611f":"code","b4431932":"code","b7074663":"code","c37b5852":"code","f2d0e5cd":"code","6595f2a6":"code","20bf2a2f":"code","8f9e7013":"code","c985a306":"code","917abfe7":"code","0cc61f2f":"code","8257f04a":"code","96e5d927":"code","4a893c5d":"code","f04a91c9":"code","4c13ddee":"code","2e1048c9":"markdown","cc4faf4d":"markdown","169e7407":"markdown","9deee0b9":"markdown","9fa053c5":"markdown","28046837":"markdown","e70c09b2":"markdown","504c698c":"markdown","5c3c9a5f":"markdown","70a057f5":"markdown"},"source":{"5737b787":"# get dataset from google disk\n!conda install -y gdown\n!gdown https:\/\/drive.google.com\/uc?id=1MyLFn80PnqOO53rU1jP9zpdVai_oHMzY\n\n# install skorch wrapper for pytorch\n!pip install -U skorch","05ff53b4":"# import libraries for file processing\nimport os\nimport zipfile\n\n# unzip file to \/kaggle\/working folder\nwith zipfile.ZipFile('.\/cv_train.zip', 'r') as zip_ref:\n    zip_ref.extractall('.\/cv_train')  \n\n# delete unnecessary file\nfor dump in os.listdir('.\/cv_train\/cv_train'):\n    if dump.split('.')[::-1][0] == 'ini':\n        os.remove('.\/cv_train\/cv_train\/' + dump)\n        print(f'{dump} removed')","8e7a1cd4":"# libraries for image processing\nimport cv2\nimport numpy as np\nimport pandas as pd\n\n# pytorch libraries\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import random_split\n\n# skorch wrapper classes and functions\nfrom skorch import NeuralNetClassifier\nfrom skorch.dataset import CVSplit\nfrom skorch.callbacks import LRScheduler, Checkpoint \nfrom skorch.callbacks import Freezer, EarlyStopping\n\n# sklearn models for further stacking\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# albumentations for image augmentation\nimport albumentations\nfrom albumentations import pytorch\n\n# for multiprocessing\nimport multiprocessing as mp\n\n# plot graphs for data exploration\nimport matplotlib.pyplot as plt","a73ddc13":"# Here we seed our environmental variables and pytorch variables\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n    seed_everything(42)","8e5fe23d":"# Read train dataset from file\ntrain_dir = '.\/cv_train\/cv_train'        \ntrain_files = os.listdir(train_dir)\n\n# Read test dataset from file\ntest_dir = '\/kaggle\/input\/korpus-ml-2\/test\/test\/test'\ntest_files = os.listdir(test_dir)\n\nprint(len(train_files), len(test_files))","a6d6e422":"# class for dataset loading, labeling and augmentation\nclass Graphs(Dataset):\n    def __init__(self, dir_path, file_list, transform=None, mode='train'):\n        self.dir_path = dir_path\n        self.file_list = file_list\n        self.transform = transform\n        self.mode = mode\n        self.label_dict = {'just_image' : 0, 'bar_chart' : 1, 'diagram' : 2, \n                           'flow_chart' : 3, 'graph' : 4, 'growth_chart' : 5,\n                           'pie_chart' : 6, 'table' : 7}\n\n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, idx):\n        image_name = os.path.join(self.dir_path, self.file_list[idx])\n        \n        if image_name.split('.')[::-1][0] == \"gif\":\n            gif = cv2.VideoCapture(image_name)\n            _, image = gif.read()\n        else:\n            image = cv2.imread(image_name)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        for name, label in self.label_dict.items():\n            if name in image_name:\n                self.label = label\n                break\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n\n        if self.mode == 'train':\n            return image, self.label\n        else:\n            return image, image_name","a61961b2":"# declaration of constant variables\nbatch_size = 128\nnum_workers = mp.cpu_count()\nimg_size = 224\nn_classes = 8","837842ff":"# function that prepares dataset for further training\ndef prepare_datasets(train_dir, train_files, test_dir, test_files):\n    # augmentation parameters for train\n    data_transforms_train = albumentations.Compose([\n        albumentations.Resize(img_size, img_size),\n        albumentations.HorizontalFlip(),\n        albumentations.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.10,\n                                        rotate_limit=15),\n        albumentations.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        pytorch.ToTensor()\n    ]) \n    # augmentation parameters for test\n    data_transforms_test = albumentations.Compose([\n        albumentations.Resize(img_size, img_size),\n        albumentations.HorizontalFlip(),\n        albumentations.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        pytorch.ToTensor()\n        ])\n\n    trainset = Graphs(train_dir, train_files, transform=data_transforms_train)\n    testset = Graphs(test_dir, test_files, transform=data_transforms_test,\n                     mode='test')\n    \n    print(f'Train dataset length: {len(trainset)}')\n    print(f'Testset dataset length: {len(testset)}')\n    \n    return trainset, testset","002feacb":"# get prepared datasets from files stored in directory\ntrain_set, test_set = prepare_datasets(train_dir, train_files, test_dir, test_files)\n\n# create dataloaders for loading data in batches=128\ntrainloader = DataLoader(train_set, batch_size=batch_size,\n                         pin_memory=True, num_workers=num_workers, shuffle=True)\ntestloader = DataLoader(test_set, batch_size=batch_size,\n                        pin_memory=True, num_workers=num_workers)","2c36f1d9":"# plot bar chart to observe amount of images by classes in dataset\nclass_names = ['just_image', 'bar_chart', 'diagram', 'flow_chart', 'graph',\n               'growth_chart', 'pie_chart', 'table']\n\ntrain_images = np.array([X.size() for X, y in iter(train_set)])\ntrain_labels = np.array([y for X, y in iter(train_set)])\n\n_, train_counts = np.unique(train_labels, return_counts=True)\npd.DataFrame({'train': train_counts}, index=class_names).plot.bar()\nplt.show()","d1770514":"# plot pie chart to observe proportion of classes in dataset\nplt.pie(train_counts, explode=(0, 0, 0, 0, 0, 0, 0, 0) , \n        labels=class_names, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of each observed category')\nplt.show()","608390b0":"# print images with labels\nsamples, labels = next(iter(trainloader))\n\nfig = plt.figure(figsize=(16, 16))\nfig.suptitle(\"Some examples of images of the dataset\", fontsize=16)\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(np.transpose(samples[i], (1, 2, 0)), cmap=plt.cm.binary)\n    plt.xlabel(class_names[labels[i]])\nplt.show()","827ed941":"# class which uses ResNet50 pretrained model\n# + added custom classifier in the last layer\nclass ResNet50(nn.Module):\n    def __init__(self, output_features, num_units=512, drop=0.3337,\n                 num_units1=256, drop1=0.1):\n        super().__init__()\n        model = torchvision.models.resnet50(pretrained=True)\n        n_inputs = model.fc.in_features\n        model.fc = nn.Sequential(\n                                nn.Linear(n_inputs, num_units),\n                                nn.ReLU(),\n                                nn.Dropout(p=drop),\n                                nn.Linear(num_units, num_units1),\n                                nn.ReLU(),\n                                nn.Dropout(p=drop1), \n                                nn.Linear(num_units1, output_features))\n        self.model = model\n        \n    def forward(self, x):\n        return self.model(x)\n\n# class which uses DenseNet169 pretrained model\n# + added custom classifier in the last layer\nclass DenseNet169(nn.Module):\n    def __init__(self, output_features, num_units=512, drop=0.3337,\n                 num_units1=256, drop1=0.1):\n        super().__init__()\n        model = torchvision.models.densenet169(pretrained=True)\n        n_inputs = model.classifier.in_features\n        model.classifier = nn.Sequential(\n                                nn.Linear(n_inputs, num_units),\n                                nn.ReLU(),\n                                nn.Dropout(p=drop),\n                                nn.Linear(num_units, num_units1),\n                                nn.ReLU(),\n                                nn.Dropout(p=drop1), \n                                nn.Linear(num_units1, output_features))\n        self.model = model\n        \n    def forward(self, x):\n        return self.model(x)\n\n# class which uses VGG16 pretrained model\nclass VGG16(nn.Module):\n    def __init__(self, output_features):\n        super().__init__()\n        model = torchvision.models.vgg16(pretrained=True)\n        n_inputs = model.classifier[6].in_features\n        model.classifier[6] = nn.Linear(n_inputs, output_features)\n        self.model = model\n        \n    def forward(self, x):\n        return self.model(x)\n    \nclass EfficientNet_b3(nn.Module):\n    def __init__(self, output_features):\n        super().__init__()\n        model = EfficientNet.from_pretrained('efficientnet-b3')\n        n_inputs = model._fc.in_features\n        model._fc = nn.Linear(n_inputs, output_features)\n        self.model = model\n        \n    def forward(self, x):\n        return self.model(x)","84a8ac65":"# callback functions for models\n\n# ResNet50\n# callback for Reduce on Plateau scheduler \nlr_scheduler_resnet = LRScheduler(policy='ReduceLROnPlateau',\n                                  factor=0.5, patience=1)\n# callback for saving the best on validation accuracy model\ncheckpoint_resnet = Checkpoint(f_params='best_model_resnet50.pkl',\n                               monitor='valid_acc_best')\n# callback for freezing all layer of the model except the last layer\nfreezer_resnet = Freezer(lambda x: not x.startswith('model.fc'))\n# callback for early stopping\nearly_stopping_resnet = EarlyStopping(patience=10)\n\n# DenseNet169\n# callback for Reduce on Plateau scheduler \nlr_scheduler_densenet = LRScheduler(policy='ReduceLROnPlateau',\n                                    factor=0.5, patience=1)\n# callback for saving the best on validation accuracy model\ncheckpoint_densenet = Checkpoint(f_params='best_model_densenet169.pkl',\n                                 monitor='valid_acc_best')\n# callback for freezing all layer of the model except the last layer\nfreezer_densenet = Freezer(lambda x: not x.startswith('model.classifier'))\n# callback for early stopping\nearly_stopping_densenet = EarlyStopping(patience=10)\n\n# VGG16\n# callback for Reduce on Plateau scheduler \nlr_scheduler_vgg = LRScheduler(policy='ReduceLROnPlateau',\n                               factor=0.5, patience=1)\n# callback for saving the best on validation accuracy model\ncheckpoint_vgg = Checkpoint(f_params='best_model_vgg16.pkl',\n                            monitor='valid_acc_best')\n# callback for freezing all layer of the model except the last layer\nfreezer_vgg = Freezer(lambda x: not x.startswith('model.classifier'))\n# callback for early stopping\nearly_stopping_vgg = EarlyStopping(patience=10)","69368760":"# NeuralNetClassifier for based on ResNet50 with custom parameters\nresnet = NeuralNetClassifier(\n    # pretrained ResNet50 + custom classifier \n    module=ResNet50,          \n    # fine tuning model's inner parameters\n    module__output_features=n_classes,\n    module__num_units=512,\n    module__drop=0.5,\n    module__num_units1=512,\n    module__drop1=0.5,\n    # criterion\n    criterion=nn.CrossEntropyLoss,\n    # batch_size = 128\n    batch_size=batch_size,\n    # number of epochs to train\n    max_epochs=100,\n    # optimizer Adam used\n    optimizer=torch.optim.Adam,\n    optimizer__lr = 0.001,\n    optimizer__weight_decay=1e-6,\n    # shuffle dataset while loading\n    iterator_train__shuffle=True,\n    # load in parallel\n    iterator_train__num_workers=num_workers,\n    # stratified kfold split of loaded dataset\n    train_split=CVSplit(cv=5, stratified=True, random_state=42),\n    # callbacks declared earlier\n    callbacks=[lr_scheduler_resnet, checkpoint_resnet, \n               freezer_resnet, early_stopping_resnet],\n    # use GPU or CPU\n    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n)","8ba8d5ee":"# NeuralNetClassifier for based on DenseNet169 with custom parameters\ndensenet = NeuralNetClassifier(\n    # pretrained DenseNet169 + custom classifier \n    module=DenseNet169, \n    # fine tuning model's inner parameters\n    module__output_features=n_classes,\n    module__num_units=512,\n    module__drop=0.5,\n    module__num_units1=512,\n    module__drop1=0.5,\n    # criterion\n    criterion=nn.CrossEntropyLoss,\n    # batch_size = 128\n    batch_size=batch_size,\n    # number of epochs to train\n    max_epochs=100,\n    # optimizer Adam used\n    optimizer=torch.optim.Adam,\n    optimizer__lr = 0.001,\n    optimizer__weight_decay=1e-6,\n    # shuffle dataset while loading\n    iterator_train__shuffle=True,\n    # load in parallel\n    iterator_train__num_workers=num_workers,\n    # stratified kfold split of loaded dataset\n    train_split=CVSplit(cv=5, stratified=True, random_state=42),\n    # callbacks declared earlier\n    callbacks=[lr_scheduler_densenet, checkpoint_densenet, \n               freezer_densenet, early_stopping_densenet],\n    # use GPU or CPU\n    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n)","0400611f":"# NeuralNetClassifier for based on VGG16 with custom parameters\nvgg = NeuralNetClassifier(\n    # pretrained VGG16\n    module=VGG16,\n    # fine tuning model's inner parameters\n    module__output_features=n_classes, \n    # criterion\n    criterion=nn.CrossEntropyLoss,\n    # batch_size = 128\n    batch_size=batch_size,\n    # number of epochs to train\n    max_epochs=100,\n    # optimizer Adam used\n    optimizer=torch.optim.Adam,\n    optimizer__lr = 0.001,\n    optimizer__weight_decay=1e-6,\n    # shuffle dataset while loading\n    iterator_train__shuffle=True,\n    # load in parallel\n    iterator_train__num_workers=num_workers, \n    # stratified kfold split of loaded dataset\n    train_split=CVSplit(cv=5, stratified=True, random_state=42),\n    # callbacks declared earlier\n    callbacks=[lr_scheduler_vgg, checkpoint_vgg,\n               freezer_vgg, early_stopping_vgg],\n    # use GPU or CPU\n    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n)","b4431932":"# Load y_train labels for training in Skorch \n# (labels in trainset are ignored while training)\ny_train = np.array([y for X, y in iter(train_set)])","b7074663":"# fit prepared model with custom parameters\n\n# resnet.fit(train_set, y=y_train)\n# densenet.fit(train_set, y=y_train)\n# vgg16.fit(train_set, y=y_train)","c37b5852":"# ResNet50\nresnet.initialize()\nresnet.load_params(f_params='\/kaggle\/input\/best-models\/best_model_resnet50.pkl')\n\n# DenseNet169\ndensenet.initialize()\ndensenet.load_params(f_params='\/kaggle\/input\/best-models\/best_model_densenet169.pkl')\n\n# VGG16\nvgg.initialize()\nvgg.load_params(f_params='\/kaggle\/input\/best-models\/best_model_vgg16.pkl')\n\n# ensemble\nmodels = [resnet, densenet, vgg]","f2d0e5cd":"# create validation dataset (30%) by splitting train dataset (70%)\nvalid_size = int(len(train_files) * 0.3)\ntrainset, validset = random_split(train_set, \n                                  (len(train_files)-valid_size, valid_size))\n\n# create dataloader for loading data in batches=128\nvalidloader = DataLoader(validset, batch_size=batch_size, pin_memory=True)","6595f2a6":"# predict on validation set ResNet50\npred_resnet = np.array([])\nfor batch_idx, (X_test, labels) in enumerate(validloader):\n    pred_resnet = np.append(pred_resnet, resnet.predict(X_test).tolist())\nprint('ResNet50 prediction done!')\n    \n# predict on validation set DenseNet169\npred_densenet = np.array([])\nfor batch_idx, (X_test, labels) in enumerate(validloader):\n    pred_densenet = np.append(pred_densenet, densenet.predict(X_test).tolist())\nprint('DenseNet169 prediction done!')\n\n# predict on validation set VGG16\npred_vgg = np.array([])\nfor batch_idx, (X_test, labels) in enumerate(validloader):\n    pred_vgg = np.append(pred_vgg, vgg.predict(X_test).tolist())\nprint('VGG16 prediction done!')","20bf2a2f":"# extract labels from validation set\ny_valid = np.array([y for X, y in iter(validset)])","8f9e7013":"pd.DataFrame(np.column_stack((pred_resnet, pred_densenet, pred_vgg, y_valid))[25:50],\n            columns=['ResNet50', 'DenseNet169', 'VGG16', 'True Label'])","c985a306":"# Decision Tree Classifier\nclf1 = DecisionTreeClassifier(random_state=4)\nclf1.fit(np.column_stack((pred_resnet, pred_densenet, pred_vgg)), y_valid)\n\n# Support Vector Machine Classifier\nclf2 = SVC(random_state=4)\nclf2.fit(np.column_stack((pred_resnet, pred_densenet, pred_vgg)), y_valid)\n\n# Random Forest Classifier\nclf3 = RandomForestClassifier(random_state=4)\nclf3.fit(np.column_stack((pred_resnet, pred_densenet, pred_vgg)), y_valid)\n\n# Gradient Boosting Classifier\nclf4 = GradientBoostingClassifier(learning_rate=0.05, max_depth=1, random_state=4)\nclf4.fit(np.column_stack((pred_resnet, pred_densenet, pred_vgg)), y_valid)","917abfe7":"clfs = ['Decision Tree', 'SVC', 'Random Forest', 'Gradient Boosting']\nscores = []\nfor clf in [clf1, clf2, clf3, clf4]:\n    scores.append(\n        cross_val_score(clf, np.column_stack((pred_resnet, pred_densenet, pred_vgg)),\n                        y_valid, scoring='accuracy', cv=5))\n\npd.DataFrame(scores, index=clfs, columns=[i for i in range(1, 6)])","0cc61f2f":"# Voting Classifier\neclf = VotingClassifier(estimators=[('dt', clf1), ('svc', clf2),\n                                    ('rf', clf3), ('gbc', clf4)], voting='hard')\n# output results of cross validation score\nfor clf, label in zip([clf1, clf2, clf3, clf4, eclf], \n                      ['Desicion Trees', 'SVC', 'Random Forest',\n                       'Gradient Boosting', 'Ensemble']):\n    scores = cross_val_score(clf, np.column_stack((pred_resnet, pred_densenet, pred_vgg)),\n                             y_valid, scoring='accuracy', cv=5)\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","8257f04a":"# fit Voting Classifier\neclf.fit(np.column_stack((pred_resnet, pred_densenet, pred_vgg)), y_valid)","96e5d927":"# list to store names of images\ntest_names_list = []\n\n# predict on test set ResNet50\ntest_pred_resnet = np.array([])\nfor batch_idx, (X_test, names) in enumerate(testloader):\n    test_pred_resnet = np.append(test_pred_resnet, resnet.predict(X_test).tolist())\n    test_names_list += [name.split('\/')[::-1][0] for name in names]\n\n# predict on test set DenseNet169\ntest_pred_densenet = np.array([])\nfor batch_idx, (X_test, names) in enumerate(testloader):\n    test_pred_densenet = np.append(test_pred_densenet, densenet.predict(X_test).tolist())\n\n# predict on test set VGG16\ntest_pred_vgg = np.array([])\nfor batch_idx, (X_test, names) in enumerate(testloader):\n    test_pred_vgg = np.append(test_pred_vgg, vgg.predict(X_test).tolist())","4a893c5d":"# predict Voting Classifier with 4 estimators\nfinal = eclf.predict(np.column_stack((test_pred_resnet, test_pred_densenet, test_pred_vgg)))","f04a91c9":"# print images with labels\nsamples, names = next(iter(testloader))\n\nfig = plt.figure(figsize=(16, 16))\nfig.suptitle(\"Some examples of test images with predicted results\", fontsize=16)\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(np.transpose(samples[i], (1, 2, 0)), cmap=plt.cm.binary)\n    plt.xlabel(class_names[final[i]])\nplt.show()","4c13ddee":"# create submission file\nsubmission = pd.DataFrame({'image_name': test_names_list, 'label': final})\nsubmission.to_csv('submission_final.csv', index=False)\nprint('Submission file is created!')","2e1048c9":"## Unziping dataset and cleaning","cc4faf4d":"## Training the models","169e7407":"## Preparing datasets","9deee0b9":"## Stacking the models","9fa053c5":"## Downloading required things","28046837":"## Seed environment","e70c09b2":"## Exploring the data","504c698c":"## Preparing models","5c3c9a5f":"## Creating a submission file","70a057f5":"## Importing all necessary libraries"}}