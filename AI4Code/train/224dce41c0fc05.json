{"cell_type":{"7ef4995b":"code","a0e9afd1":"code","1ce2f971":"code","f5d8d2a7":"code","63154205":"code","169b70b6":"code","e8df6cec":"code","ef8f2aec":"code","3760a72e":"code","f7f5e03f":"code","e862b2f5":"markdown","277a8aba":"markdown","db3ac5f6":"markdown","22f03caa":"markdown","ab5be63c":"markdown","d15ea534":"markdown","9caebccd":"markdown","91f7069e":"markdown"},"source":{"7ef4995b":"import spacy\nimport pickle\nimport random\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","a0e9afd1":"train_data = pickle.load(open('\/kaggle\/input\/resume-data\/train_data.pkl','rb'))","1ce2f971":"# Let's see the data\ntrain_data[0]","f5d8d2a7":"# Loading the blank SpaCy english model\nnlp = spacy.load('en')\n","63154205":"nlp = spacy.blank('en')\n\ndef train_model(train_data):\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        nlp.add_pipe(ner, last = True)\n    \n    for _, annotation in train_data:\n        for ent in annotation['entities']:\n            ner.add_label(ent[2])\n            \n    \n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        optimizer = nlp.begin_training()\n        for itn in range(10):\n            print(\"Starting iteration \" + str(itn))\n            random.shuffle(train_data)\n            losses = {}\n            index = 0\n            for text, annotations in train_data:\n                try:\n                    nlp.update(\n                        [text],  # batch of texts\n                        [annotations],  # batch of annotations\n                        drop=0.2,  # dropout - make it harder to memorise data\n                        sgd=optimizer,  # callable to update weights\n                        losses=losses)\n                except Exception as e:\n                    pass\n                \n            print(losses)","169b70b6":"# Let's train the model\ntrain_model(train_data)","e8df6cec":"# Let's save the model for further use\nnlp.to_disk('nlp_model')","ef8f2aec":"# Installing PyMuPDF for getting the text data from the resume pdf\n!pip install PyMuPDF","3760a72e":"import sys, fitz\n\nfname = '\/kaggle\/input\/resume-data\/Alice Clark CV.pdf'\ndoc = fitz.open(fname)\ntext = \"\"\nfor page in doc:\n    text = text + str(page.getText())\n\ntx = \" \".join(text.split('\\n'))  # for removing the next line character '\/n'\nprint(tx)","f7f5e03f":"# Now we will pass this extracted text to our model\nnlp_model = nlp.from_disk('\/kaggle\/input\/resume-data\/nlp_model\/')\n\ndoc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')","e862b2f5":"### Training the data on the model","277a8aba":"#### Importing the libraries","db3ac5f6":"***Special thanks to KGP talkie for this awesome notebook and the data***\n\nGithub Link -> https:\/\/github.com\/laxmimerit\/Resume-and-CV-Summarization-and-Parsing-with-Spacy-in-Python\n\nBlog Link -> https:\/\/kgptalkie.com\/resume-and-cv-summarization\/","22f03caa":"### Model testing\n\nLet's check out how our model is performing.\nFor this we will pass a new resume to this model.","ab5be63c":"#### Losding the data","d15ea534":"***This is amazing. But we can make this model more accurate by training it on more data.***\n\nAnd I will be constantly updating this notebook to make it better.","9caebccd":"We will first load a black SpaCy english model. Then we will write a function which will take the training data as the input. In the function, first we will add a ner i.e. Named Entity Recognition in the last position in the pipeline. Then we will add our custom labels in the pipeline.","91f7069e":"### In this notebook I will be using SPaCy to create model which will extract main points from a resume. We will train the model on almost 200 resumes. \n### After the model is ready, we will extract the text from a new resume and pass it to the model to get the summary."}}