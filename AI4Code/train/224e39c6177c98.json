{"cell_type":{"7caa0dd6":"code","d472cc04":"code","82eb215a":"code","30a47d93":"code","bfc361a0":"code","7578fe1b":"code","b6213912":"code","b1b45158":"code","4b3c7f73":"code","0f9f5050":"code","9ff47eeb":"code","f5332567":"code","11798fc2":"code","a5f912dd":"code","d584099a":"code","6139337a":"code","f44b2e85":"code","bc54d9bf":"code","d09af573":"code","64d63c94":"code","07431db5":"code","d94c5b6a":"code","29310b1e":"code","d1374eee":"code","3e6acd58":"markdown","9486f797":"markdown","13ab554f":"markdown","72c395f1":"markdown","c4d6a729":"markdown","5b0914e2":"markdown","f0466f28":"markdown","82fc0da6":"markdown","c647ddd9":"markdown","9651f209":"markdown","b1cb8981":"markdown","117b05f7":"markdown","a1ebf9dd":"markdown","217da326":"markdown","ac81686a":"markdown","3491bc2c":"markdown","823e9854":"markdown","9398b395":"markdown"},"source":{"7caa0dd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\n\nfrom collections import defaultdict\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print('Ilo\u015b\u0107 danych:', len(filenames), ': ', dirname)\n   \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nfrom os import listdir\nfrom os.path import isfile, join\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport gc; gc.enable() # memory is tight\nimport random\n\nplt.rcParams['figure.figsize'] = (25, 14)\n\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nprint(tf.__version__)\n!nvidia-smi","d472cc04":"# pozwoli zachowac identyczne rezultaty, oraz zachowanie si\u0119 kodu, \n# dzieki czemu nie wkrada sie losowosc to naszych obliczen i za kazdym razem otrzymujemy te same wyniki\nSEED = 5000\n\ndef set_seed(SEED):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    tf.random.set_seed(SEED)\n    \n    return SEED\n    \nset_seed(SEED)\n\nBASE_DIR = '\/kaggle\/input\/kitti-object-detection\/kitti_single\/training\/'\nLABEL_PATH = os.path.join(BASE_DIR,'label_2')\nIMAGE_PATH = os.path.join(BASE_DIR,'image_2')\nCALIB_PATH = os.path.join('\/kaggle\/input\/kitti3dcalib\/training','calib')\n\n\n\"\"\"\nIMAGE_WIDTH = 256\nIMAGE_HEIGHT = 160\n\n WIELKOSC OBRAZKA (INPUT): TUTAJ BARDZO WA\u017bNE W PRZYPADKU STOSOWANIA CONVOLUCYJNYCH SIECI TYPU UNET. \n \n Poniewa\u017c za ka\u017cdym razem gdy w warstwie obraz jest zmniejszany 2 razy, nast\u0119puj\u0119 zaokr\u0105glenie rozmiaru do liczby ca\u0142kowitej.\n Gdyby w kt\u00f3rym\u015b momencie wielko\u015b\u0107 by\u0142aby liczb\u0105 nieparzyst\u0105. Np:  mamy taka siec\n \nconv2d_66 (Conv2D)           (None, 100, 175, 32)      9248      \n_________________________________________________________________\nmax_pooling2d_26 (MaxPooling (None, 50, 87, 32)        0   \n\nto w przypadku UNET gdy laczymy czesc poczatkowych warstw z kolejnymi ktore wyszly, to po wyjsciu s\u0105 one \nmno\u017cone przez dwa, wi\u0119c warstwa z kt\u00f3r\u0105 b\u0119dziemy chcieli po\u0142\u0105czy\u0107 b\u0119dzie mia\u0142a wielko\u015b\u0107 86 i tworzenie takiego \nmodelu zwr\u00f3ci b\u0142\u0105d.\nNale\u017cy pami\u0119ta\u0107 by wielko\u015bci by\u0142y wielokrotno\u015bciami 8. (w przypadku zmniejszania 3 razy,\nalbo 16 w przypadku 4 operacji zmniejszania)\n\nhttps:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/\n\"\"\"\n\nIMAGE_WIDTH = 256\nIMAGE_HEIGHT = 160","82eb215a":"images =  [(f) for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))]\nmasks = [(f) for f in listdir(LABEL_PATH) if isfile(join(LABEL_PATH, f))]\nmasks.sort()\n\ndf = pd.DataFrame(np.column_stack([masks]), columns=['masks'])\ndf['images'] = df['masks'].apply(lambda x: x[:-3]+'png')\n\n\n# Sprawdzamy czy wszystkie maski posiadaj\u0105 powi\u0105zane obrazy\nno_images = df[df['images'].apply(lambda i:  i not in images)]\nif(len(no_images)>0):\n    print(\"Niektore maski nie posiadaj\u0105 swoich obraz\u00f3w\")\n    print(no_images)\nelse:\n    print(\"BARDZO DOBRZE: Wszystke maski posiadaja swoje obrazy\")\n\n\n    \nimg = cv2.imread(os.path.join(IMAGE_PATH, '000015.png') )    \nprint('Image shape: ', img.shape)\nprint(df.shape)\ndf.head()","30a47d93":"def get_labels(label_filename):\n    \"\"\"\n        get_labels pozwala zwrocic. bounding boxy dla kazdego samochodu. Dzi\u0119ki niemu mo\u017cemy zobaczy\u0107 kontory samochodow,\n        zwraca liste z wartooscciami bounding box.\n        \n        :param label_filename: filname like kitti_3d\/{training,testing}\/label_2\/id.txt\n        Returns Pandas DataFrame\n        \n        The label files contain the following information, which can be read and\n        written using the matlab tools (readLabels.m, writeLabels.m) provided within\n        this devkit. All values (numerical or strings) are separated via spaces,\n        each row corresponds to one object. The 15 columns represent:\n        #Values    Name      Description\n        ----------------------------------------------------------------------------\n           1    type         Describes the type of object: 'Car', 'Van', 'Truck',\n                             'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram',\n                             'Misc' or 'DontCare'\n           1    truncated    Float from 0 (non-truncated) to 1 (truncated), where\n                             truncated refers to the object leaving image boundaries\n           1    occluded     Integer (0,1,2,3) indicating occlusion state:\n                             0 = fully visible, 1 = partly occluded\n                             2 = largely occluded, 3 = unknown\n           1    alpha        Observation angle of object, ranging [-pi..pi]\n           4    bbox         2D bounding box of object in the image (0-based index):\n                             contains left, top, right, bottom pixel coordinates\n           3    dimensions   3D object dimensions: height, width, length (in meters)\n           3    location     3D object location x,y,z in camera coordinates (in meters)\n           1    rotation_y   Rotation ry around Y-axis in camera coordinates [-pi..pi]\n           1    score        Only for results: Float, indicating confidence in\n                             detection, needed for p\/r curves, higher is better.\n    \"\"\"\n    data =  pd.read_csv(os.path.join(LABEL_PATH,label_filename), sep=\" \", \n                       names=['label', 'truncated', 'occluded', 'alpha', \n                              'bbox_xmin', 'bbox_ymin', 'bbox_xmax', \n                              'bbox_ymax', 'dim_height', 'dim_width', 'dim_length', \n                              'loc_x', 'loc_y', 'loc_z', 'rotation_y', 'score'])\n    \n    return data\n    \nget_labels('000002.txt')","bfc361a0":"def open_image(image_filename):\n    return cv2.imread(os.path.join(IMAGE_PATH, image_filename))\n    \n    \ndef draw_box2d_id(id):\n    \"\"\"\n        pozwala zobazyc jakie BoundingBox sa dla naszego zestawu danych\n    \"\"\"\n    return draw_box2d(open_image(id + '.png'),\n                      get_labels(id + '.txt'))\n\nLABEL_COLORS = {\n    'Car': (255,0,0), \n    'Van': (255,255,0), \n    'Truck': (255,255,255),\n    'Pedestrian': (0,255,255),\n    'Person_sitting': (0,255,255), \n    \n    'Cyclist': (0,128,255), \n    'Tram': (128,0,0),\n    'Misc': (0,255,255),\n    'DontCare': (255,255,0)\n}\n\n\ndef draw_box2d(image, labels, ax = None):\n    \"\"\"\n        pozwala rysowac boxy 2d dla naszego modelu.\n        \n        \n        :param label_filename: filname like kitti_3d\/{training,testing}\/label_2\/id.txt\n        Returns Pandas DataFrame\n    \"\"\"\n    img = image.copy()\n    for index, row in labels.iterrows():\n        left_corner = (int(row.bbox_xmin), int(row.bbox_ymin))\n        right_corner = (int(row.bbox_xmax), int(row.bbox_ymax))\n        label_color = LABEL_COLORS.get(row.label,(0,255,0))\n        \n        img = cv2.rectangle(img, \n                            left_corner, right_corner, label_color, 1)\n        img = cv2.putText(img, str(row.label), \n                          (left_corner[0] + 10, left_corner[1] - 4) , \n                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, \n                          label_color, 1)\n    if ax == None:\n        plt.imshow(img)\n    else:\n        ax.imshow(img)\n\ndraw_box2d_id('000015')","7578fe1b":"## https:\/\/github.com\/charlesq34\/frustum-pointnets\/tree\/master\/kitti\n\ndef get_calibration(id):\n    filepath = os.path.join(CALIB_PATH, id + '.txt')\n    \n    \"\"\" Read in a calibration file and parse into a dictionary.\n    Ref: https:\/\/github.com\/utiasSTARS\/pykitti\/blob\/master\/pykitti\/utils.py\n    \"\"\"\n    data = {}\n    with open(filepath, \"r\") as f:\n        for line in f.readlines():\n            line = line.rstrip()\n            if len(line) == 0:\n                continue\n            key, value = line.split(\":\", 1)\n            # The only non-float values in these files are dates, which\n            # we don't care about anyway\n            try:\n                data[key] = np.array([float(x) for x in value.split()])\n            except ValueError:\n                pass\n    data['P'] = np.reshape(data['P2'], [3, 4])\n\n    return data\n    \n    \ndef rotx(t):\n    \"\"\" 3D Rotation about the x-axis. \"\"\"\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[1, 0, 0], [0, c, -s], [0, s, c]])\n\n\ndef roty(t):\n    \"\"\" Rotation about the y-axis. \"\"\"\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]])\n\n\ndef rotz(t):\n    \"\"\" Rotation about the z-axis. \"\"\"\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n\ndef project_to_image(pts_3d, P):\n    \"\"\" Project 3d points to image plane.\n    Usage: pts_2d = projectToImage(pts_3d, P)\n      input: pts_3d: nx3 matrix\n             P:      3x4 projection matrix\n      output: pts_2d: nx2 matrix\n      P(3x4) dot pts_3d_extended(4xn) = projected_pts_2d(3xn)\n      => normalize projected_pts_2d(2xn)\n      <=> pts_3d_extended(nx4) dot P'(4x3) = projected_pts_2d(nx3)\n          => normalize projected_pts_2d(nx2)\n    \"\"\"\n    n = pts_3d.shape[0]\n    pts_3d_extend = np.hstack((pts_3d, np.ones((n, 1))))\n    # print(('pts_3d_extend shape: ', pts_3d_extend.shape))\n    pts_2d = np.dot(pts_3d_extend, np.transpose(P))  # nx3\n    pts_2d[:, 0] \/= pts_2d[:, 2]\n    pts_2d[:, 1] \/= pts_2d[:, 2]\n    return pts_2d[:, 0:2]\n\n\n# label\ttruncated\toccluded\talpha\tbbox_xmin\tbbox_ymin\tbbox_xmax\tbbox_ymax\tdim_height\tdim_width\t\n#dim_length\tloc_x\tloc_y\tloc_z\trotation_y\tscore\ndef compute_box_3d(obj, P):\n    \"\"\" Takes an object and a projection matrix (P) and projects the 3d\n        bounding box into the image plane.\n        Returns:\n            corners_2d: (8,2) array in left image coord.\n            corners_3d: (8,3) array in in rect camera coord.\n    \"\"\"\n    # compute rotational matrix around yaw axis\n    R = roty(obj.rotation_y)\n\n    # 3d bounding box dimensions\n    l = obj.dim_length\n    w = obj.dim_width\n    h = obj.dim_height\n\n    # 3d bounding box corners\n    x_corners = [l \/ 2, l \/ 2, -l \/ 2, -l \/ 2, l \/ 2, l \/ 2, -l \/ 2, -l \/ 2]\n    y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n    z_corners = [w \/ 2, -w \/ 2, -w \/ 2, w \/ 2, w \/ 2, -w \/ 2, -w \/ 2, w \/ 2]\n\n    # rotate and translate 3d bounding box\n    corners_3d = np.dot(R, np.vstack([x_corners, y_corners, z_corners]))\n    # print corners_3d.shape\n    corners_3d[0, :] = corners_3d[0, :] + obj.loc_x\n    corners_3d[1, :] = corners_3d[1, :] + obj.loc_y\n    corners_3d[2, :] = corners_3d[2, :] + obj.loc_z\n    # print 'cornsers_3d: ', corners_3d\n    # only draw 3d bounding box for objs in front of the camera\n    if np.any(corners_3d[2, :] < 0.1):\n        corners_2d = None\n        return corners_2d, np.transpose(corners_3d)\n\n    # project the 3d bounding box into the image plane\n    corners_2d = project_to_image(np.transpose(corners_3d), P)\n    # print 'corners_2d: ', corners_2d\n    return corners_2d, np.transpose(corners_3d)\n\n\ndef draw_projected_box3d(image, qs, color=(0, 255, 0), thickness=2):\n    \"\"\" Draw 3d bounding box in image\n        qs: (8,3) array of vertices for the 3d box in following order:\n            1 -------- 0\n           \/|         \/|\n          2 -------- 3 .\n          | |        | |\n          . 5 -------- 4\n          |\/         |\/\n          6 -------- 7\n    \"\"\"\n    qs = qs.astype(np.int32)\n    \n    for k in range(0, 4):\n        # Ref: http:\/\/docs.enthought.com\/mayavi\/mayavi\/auto\/mlab_helper_functions.html\n        i, j = k, (k + 1) % 4\n        # use LINE_AA for opencv3\n        # cv2.line(image, (qs[i,0],qs[i,1]), (qs[j,0],qs[j,1]), color, thickness, cv2.CV_AA)\n        cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n        i, j = k + 4, (k + 1) % 4 + 4\n        cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n\n        i, j = k, k + 4\n        cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n    return image\n\n\n\ndef draw_box3d_id(id):\n    \"\"\"\n        pozwala zobazyc jakie BoundingBox sa dla naszego zestawu danych\n    \"\"\"\n    calib = get_calibration(id)\n    return draw_box3d(open_image(id + '.png'),\n                      get_labels(id + '.txt'), calib)\n\n\ndef draw_box3d(image, labels, calib, ax = None):\n    \"\"\"\n        pozwala rysowac boxy 3d dla naszego modelu.\n    \n        :param label_filename: filname like kitti_3d\/{training,testing}\/label_2\/id.txt\n        Returns Pandas DataFrame\n    \"\"\"\n    img = image.copy()\n    for index, row in labels.iterrows():\n        label_color = LABEL_COLORS.get(row.label,(0,255,0))\n        if row.label == \"DontCare\":\n            left_corner = (int(row.bbox_xmin), int(row.bbox_ymin))\n            right_corner = (int(row.bbox_xmax), int(row.bbox_ymax))\n            img = cv2.rectangle(img, \n                            left_corner, right_corner, label_color, 1)\n            continue\n            \n        box3d_pts_2d, _ = compute_box_3d(row, calib['P'])\n        if box3d_pts_2d is None:\n            continue\n        \n        img = draw_projected_box3d(img, box3d_pts_2d , label_color, 1)\n\n    if ax == None:\n        plt.imshow(img)\n    else:\n        ax.imshow(img)\n        \n\ndraw_box3d_id('000015')","b6213912":"DTYPE = np.float64\n\ndef get_image(path):\n    return cv2.imread(path)\n\nLABEL_MASKS = {\n    'Car': 0, \n    'Van': 1, \n    'Truck': 1,\n    'Pedestrian': 2,\n    'Person_sitting': 2, \n    \n    'Cyclist': 3, \n    'Tram': 4,\n    \n    'Misc': 5,\n    'DontCare': 5\n}\n\nLABEL_MASKS_COLORS = [\n    (255,0,0), \n    (255,255,0), \n    (0,255,255),\n    \n    (0,128,255), \n    (128,0,0),\n    \n    (255,255,0)\n]\n\n\nLABEL_MASKS_LENGTH = 6\n\n\ndef create_mask(mask_dir, img_shape):\n    \"\"\"\n        pozwala wygenerowac maske, Poniewaz w przypadku modelu UNET na wyjsciu chcemy miec zwykla maske obiektu,\n        gdzie w pelni zaznaczymy gdzie s\u0105 obiekty.\n        \n        Uwzgl\u0119dnili\u015bmy w nich samoch\u00f3d, van, track, oraz osoby.\n    \"\"\"\n    mask = np.zeros(shape=(img_shape[0], img_shape[1], LABEL_MASKS_LENGTH), dtype = DTYPE)\n    \n\n    with open(mask_dir) as f:\n        content = f.readlines()\n    content = [x.split() for x in content] \n    for item in content:\n        if item[0] in LABEL_MASKS:\n            ul_col, ul_row = int(float(item[4])), int(float(item[5]))\n            lr_col, lr_row = int(float(item[6])), int(float(item[7]))\n            \n            mask[ul_row:lr_row, ul_col:lr_col, LABEL_MASKS[item[0]]] = 1 \n    return mask\n\ndef get_mask_rgb(mask):\n    rgb_mask = np.zeros(shape = (mask.shape[0],mask.shape[1], 3))\n    for m in range(mask.shape[2]):\n        m_array = mask[:,:, m]\n        rgb_mask[:,:,0] += mask[:,:, m] * LABEL_MASKS_COLORS[m][0]\n        rgb_mask[:,:,1] += mask[:,:, m] * LABEL_MASKS_COLORS[m][1]\n        rgb_mask[:,:,2] += mask[:,:, m] * LABEL_MASKS_COLORS[m][2]\n    return rgb_mask\n\ndef draw_mask(image, mask, ax = None):\n    rgb_mask = get_mask_rgb(mask) \/ 255.0\n    img = cv2.addWeighted( image, 0.9, rgb_mask, 0.5, 0)\n    if ax == None:\n        plt.imshow(img)\n    else:\n        ax.imshow(img)\n        \ndef draw_mask_id(id):\n    # poniewa\u017c maska to warto\u015bci 0,1 a obraz to warto\u015bci 0-255, to obraz trzeba podzieli\u0107 przez 255.0\n    img = np.array(get_image(os.path.join(IMAGE_PATH,id + '.png')) \/ 255.0, dtype = DTYPE) \n    mask = create_mask(os.path.join(LABEL_PATH,  id + '.txt'), img.shape )\n    draw_mask(img,mask)\n    plt.show()\n    \ndraw_mask_id('000015')","b1b45158":"from abc import ABCMeta, abstractmethod\nimport imgaug.augmenters as iaa\nfrom imgaug.augmentables.segmaps import SegmentationMapsOnImage\n\n\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    \"\"\"\n        Ta abstrakcyjna klasa to jest wszystko co trzeba zaprogramowa\u0107 dla ImageDataGenerator.\n        Dodali\u015bmy metod\u0119 get_data(), kt\u00f3ra pobierze nam potrzeby batch size.\n    \"\"\"\n    def __init__(self,indices, batch_size, shuffle):\n        self.indices = indices\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.on_epoch_end()\n        \n    def __len__(self):\n        return len(self.indices) \/\/ self.batch_size\n\n    def __getitem__(self, index):\n        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n        batch = [self.indices[k] for k in index]\n        \n        return self.get_data(batch)\n    \n    def sample(self):\n        return self[random.randint(0,len(self))]\n\n    def on_epoch_end(self):\n        self.index = np.arange(len(self.indices))\n        if self.shuffle == True:\n            np.random.shuffle(self.index)\n\n    @abstractmethod\n    def get_data(self, batch):\n        raise NotImplementedError","4b3c7f73":"       \nclass KittiDataGenerator(DataGenerator):\n    \"\"\"\n        Bounding Box dla obiektow.\n    \"\"\"\n    def __init__(self, \n                 df,\n                 shape = (IMAGE_WIDTH, IMAGE_HEIGHT),\n                 \n                 batch_size=32, \n                 shuffle=True, \n                 \n                 augmentation = None,\n                 \n                 image_col = 'images',\n                 mask_col = 'masks',\n                 \n                 label_path = LABEL_PATH,\n                 image_path = IMAGE_PATH\n                ):\n        \n        self.df = df\n        self.shape = shape\n\n        self.image_col = image_col\n        self.mask_col = mask_col\n        self.label_path = label_path\n        self.image_path = image_path\n        self.augmentation = augmentation\n        \n        super().__init__(self.df.index.tolist(), batch_size, shuffle)\n    \n    def get_x(self, index):\n        return get_image(\n            os.path.join(self.image_path, self.df.loc[index][self.image_col])\n        ) \n    \n    def get_y(self, index, shape):\n        return create_mask(os.path.join(self.label_path, self.df.loc[index][self.mask_col]),shape) \n\n\n    def get_data(self, batch):\n        batch_X = []\n        batch_y = []\n        \n        for i in batch:\n            image_r = self.get_x(i)\n            mask_r = self.get_y(i, image_r.shape)\n            \n            # w przypadku cv2.resize, odwracamy wielkosc, najpierw jest wysokosc, potem szerokosc\n            image_r = cv2.resize(image_r, (self.shape[0], self.shape[1]))\n            # mask_r po przekszta\u0142ceniu nie b\u0119dzie posiadac warto\u015bci 0,1 a warto\u015bci mi\u0119dzy 0 a 1\n            mask_r = cv2.resize(mask_r,(self.shape[0], self.shape[1]))\n            \n            if self.augmentation is not None:\n                mask_r = np.array(mask_r, dtype=np.int32)\n                segmap = SegmentationMapsOnImage(mask_r, shape=image_r.shape)\n                image_r, segmap = self.augmentation(image = image_r, segmentation_maps = segmap)\n                mask_r = segmap.get_arr().astype(np.float64)\n                \n                \n            batch_X.append(image_r)\n            batch_y.append(mask_r)\n        \n        batch_X = np.array(batch_X)\n        batch_y = np.array(batch_y)\n        \n\n#         if self.augmentation is not None:\n#             print(batch_y.shape)\n#             batch_y = np.expand_dims(batch_y, axis=3)\n#             print(batch_y.shape)\n            \n#             batch_X, segmap = \n            \n#             batch_y = segmap\n#             print(batch_y.shape)\n#             batch_y = segmap.draw()\n\n        return batch_X  \/ 255.0, batch_y\n    \n# Define our augmentation pipeline.\n# seq = iaa.Sequential([\n#     iaa.Dropout([0.05, 0.2]),      # drop 5% or 20% of all pixels\n#     iaa.Sharpen((0.0, 1.0)),       # sharpen the image\n#     iaa.Affine(rotate=(-45, 45)),  # rotate by -45 to 45 degrees (affects segmaps)\n#     iaa.ElasticTransformation(alpha=50, sigma=5)  # apply water effect (affects segmaps)\n# ], random_order=True)\n\nseq = iaa.Sequential([\n    iaa.Dropout([0.00, 0.01]),      # drop 5% or 20% of all pixels\n    iaa.Sharpen((0.0, 0.05)),       # sharpen the image\n    iaa.Multiply((0.8, 0.9), per_channel=0.5), # brightness\n    iaa.Affine(rotate=(-3, 3)),  # rotate by -45 to 45 degrees (affects segmaps)\n    iaa.GammaContrast((0.5, 1.0))  \n])\n    \nimage_generator = KittiDataGenerator(df, \n                                     shape = (IMAGE_WIDTH, IMAGE_HEIGHT),\n                                     augmentation = seq)\nbatch_X,batch_y = image_generator.sample()\nprint('Input : ', batch_X.shape, batch_X.dtype,' max: ', batch_X.max(),  \n      '\\nOutput: ', batch_y.shape, batch_y.dtype, ' max: ', batch_y.max())","0f9f5050":"#Keras\nSMOOTH = 1e-5\nfrom tensorflow.keras import backend as K\nfrom typing import Callable, Union\n\n\ndef calc_IOU(y_true, y_pred, smooth=1): \n    y_true_f = y_true\n    y_pred_f = y_pred\n    \n    intersection = keras.backend.sum(y_true_f*y_pred_f)\n    \n    return (2*(intersection + smooth) \/ (keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) + smooth))\n\n\ndef calc_IOU_loss(y_true, y_pred):\n    return -calc_IOU(y_true, y_pred)\n\n\n\n# dice_coef_cat = dice_coef_cat_fn(num_classes = LABEL_MASKS_LENGTH)\n# dice_coef_cat_loss = dice_coef_cat_loss_fn(num_classes = LABEL_MASKS_LENGTH)\n\nprint('IOU: ', calc_IOU_loss(batch_y,batch_y) )","9ff47eeb":"def dice(y_pred, y_true):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n\n\ndef fbeta(y_pred, y_true):\n\n    pred0 = keras.layers.Lambda(lambda x : x[:,:,:,0])(y_pred)\n    pred1 = keras.layers.Lambda(lambda x : x[:,:,:,1])(y_pred)\n    true0 = keras.layers.Lambda(lambda x : x[:,:,:,0])(y_true)\n    true1 = keras.layers.Lambda(lambda x : x[:,:,:,1])(y_true) # channel last?\n    \n    y_pred_0 = K.flatten(pred0)\n    y_true_0 = K.flatten(true0)\n    \n    y_pred_1 = K.flatten(pred1)\n    y_true_1 = K.flatten(true1)\n    \n    intersection0 = K.sum(y_true_0 * y_pred_0)\n    intersection1 = K.sum(y_true_1 * y_pred_1)\n\n    precision0 = intersection0\/(K.sum(y_pred_0)+K.epsilon())\n    recall0 = intersection0\/(K.sum(y_true_0)+K.epsilon())\n    \n    precision1 = intersection1\/(K.sum(y_pred_1)+K.epsilon())\n    recall1 = intersection1\/(K.sum(y_true_1)+K.epsilon())\n    \n    fbeta0 = (1.0+0.25)*(precision0*recall0)\/(0.25*precision0+recall0+K.epsilon())\n    fbeta1 = (1.0+4.0)*(precision1*recall1)\/(4.0*precision1+recall1+K.epsilon())\n    \n    return ((fbeta0+fbeta1)\/2.0)\n\ndef fbeta_loss(y_true, y_pred):\n    return 1-fbeta(y_true, y_pred)\n\ndef dice_loss(y_true, y_pred):\n    return 1-dice(y_true, y_pred)\n\n\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    A weighted version of keras.objectives.categorical_crossentropy\n    \n    Variables:\n        weights: numpy array of shape (C,) where C is the number of classes\n    \n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n        model.compile(loss=loss,optimizer='adam')\n    \"\"\"\n    weights = K.variable(weights)\n        \n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) # * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    \n    return loss\n\ndef cat_dice_loss_fn(weights_n = []):\n    f = weighted_categorical_crossentropy(weights_n)\n    def fun(y_true, y_pred):\n        return f(y_true,y_pred) + dice_loss(y_true, y_pred) # + fbeta_loss(y_true, y_pred)\n    return fun\n\nprint('Fbeta: ', fbeta_loss(batch_y,batch_y))\nprint('Dice Loss: ', dice_loss(batch_y,batch_y))\n\ncat_dice_loss = cat_dice_loss_fn(weights_n = np.ones(LABEL_MASKS_LENGTH, dtype=np.float64))\nprint('Cat Dice Loss: ', cat_dice_loss(batch_y,batch_y).shape)","f5332567":"def show_data(X,y, y_pred = None):\n    if y_pred is None:\n        y_pred = y\n        \n    for x_i,y_i,y_pred_i in zip(X,y,y_pred):\n        im = np.array(255*x_i,dtype=np.uint8)\n        im_mask = np.array(255*y_i,dtype=np.uint8)\n\n        rgb_mask_pred = np.array(get_mask_rgb(y_pred_i),dtype=np.uint8)\n        rgb_mask_true      = np.array(get_mask_rgb(y_i),dtype=np.uint8)\n        \n        img_pred = cv2.addWeighted(rgb_mask_pred,0.5,im,0.5,0)\n        img_true = cv2.addWeighted(rgb_mask_true,0.5,im,0.5,0)\n        \n        loss = calc_IOU_loss(np.array([y_i], dtype=y_i.dtype),np.array([y_pred_i], dtype=y_i.dtype))\n    \n        plt.figure(figsize=(20,8))\n        plt.subplot(1,3,1)\n        plt.imshow(im)\n        plt.title('Original image')\n        plt.axis('off')\n        plt.subplot(1,3,2)\n        plt.imshow(img_pred)\n        plt.title(f'Predicted masks {loss:0.4f}')\n        plt.axis('off')\n        plt.subplot(1,3,3)\n        plt.imshow(img_true)\n        plt.title('ground truth datasets')\n        plt.axis('off')\n        plt.tight_layout(pad=0)\n        plt.show()\n\nshow_data(batch_X[:4], batch_y[:4])","11798fc2":"set_seed(SEED)\n\ndf_train, df_val = train_test_split(df, test_size=0.25, shuffle=True)\ndf_train.head()","a5f912dd":"kitti_train = KittiDataGenerator(df_train,\n                                 shape = (IMAGE_WIDTH, IMAGE_HEIGHT), augmentation = seq)\n\nkitti_val =  KittiDataGenerator(df_val,\n                                 shape = (IMAGE_WIDTH, IMAGE_HEIGHT))\n\nbatch_X, batch_y = kitti_train.sample()\nshow_data(batch_X[:4], batch_y[:4])","d584099a":"from tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras.optimizers import Adam\n\n\ndef upsample_conv(filters, kernel_size, strides, padding):\n    return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n\ndef upsample_simple(filters, kernel_size, strides, padding):\n    return layers.UpSampling2D(strides)\n\n\ndef create_model(output_classes, \n                 calc_loss = dice_loss,\n                 image_width = IMAGE_WIDTH,image_height = IMAGE_HEIGHT, \n                 upsample = upsample_simple):\n    # input_img = layers.Input(batch_img.shape[1:], name = 'RGB_Input')\n    input_img = layers.Input((image_height, image_width,3), name = 'RGB_Input')\n    pp_in_layer = input_img\n             \n    c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(pp_in_layer) # \n    c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(c1)\n    p1 = layers.MaxPooling2D((2, 2))(c1)\n    \n    c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(p1)\n    c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(c2)\n    p2 = layers.MaxPooling2D((2, 2))(c2)\n    \n    c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(p2)\n    c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c3)\n    p3 = layers.MaxPooling2D((2, 2)) (c3)\n    \n    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p3)\n    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n    p4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n    \n    c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p4)\n    c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c5)\n    \n    \n    u6 = upsample(64, (2, 2), strides=(2, 2), padding='same')(c5)\n    \n    u6 = layers.concatenate([u6, c4])\n    c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u6)\n    c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c6)\n    \n    u7 = upsample(32, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = layers.concatenate([u7, c3])\n    c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u7)\n    c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c7)\n    \n    u8 = upsample(16, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = layers.concatenate([u8, c2])\n    c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(u8)\n    c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(c8)\n    \n    u9 = upsample(8, (2, 2), strides=(2, 2), padding='same') (c8)\n    u9 = layers.concatenate([u9, c1], axis=3)\n    c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(u9)\n    c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(c9)\n    \n    d = layers.Conv2D(output_classes, (1, 1), activation='sigmoid')(c9)\n    seg_model = models.Model(inputs=[input_img], outputs=[d])\n\n    \n    seg_model.compile(optimizer=Adam(lr=1e-4),\n              loss=calc_loss, \n                metrics=[calc_IOU, dice, fbeta])\n    \n    seg_model.summary()\n    \n    return seg_model\n\nmodel = create_model(LABEL_MASKS_LENGTH, calc_loss = cat_dice_loss)","6139337a":"keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)","f44b2e85":"!pip install livelossplot\nfrom livelossplot import PlotLossesKeras\nprint(PlotLossesKeras)","bc54d9bf":"tf.test.is_gpu_available()","d09af573":"loss_history =  model.fit(\n        kitti_train,\n#         steps_per_epoch=20,\n        epochs=20,\n        validation_data=kitti_val,\n        callbacks=[ PlotLossesKeras(), \n                  tf.keras.callbacks.ReduceLROnPlateau(\n                      monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto',\n                      min_delta=0.0001, cooldown=0, min_lr=0)\n                  ]\n    )","64d63c94":"#!ls \/kaggle\/output","07431db5":"model.save_weights('\/kaggle\/output\/model_v1')","d94c5b6a":"X_batch, y_batch = kitti_val.sample()\n\nX_batch = X_batch[:4]\ny_batch = y_batch[:4]\n\nprint(X_batch.shape)","29310b1e":"y_batch_pred = model.predict(X_batch)\n\ny_batch_pred_treshold = tf.round(y_batch_pred)\nprint(X_batch.shape, y_batch.shape, y_batch_pred_treshold.shape)\n\n\n# print(calc_IOU_loss(y_batch,y_batch_pred))","d1374eee":"show_data(X_batch, y_batch,  y_batch_pred_treshold)","3e6acd58":"## Bounding Box 2d\n\nPozwala rysowac nam Bounding Box, z pliku maski. S\u0105 potrzebne 4 parametry dla ka\u017cdego:\n\n* `bbox_xmin\tbbox_ymin\tbbox_xmax\tbbox_ymax` - pozycja prostok\u0105ta.\n* `label` - typ obiektu","9486f797":"# Pobieranie danych\n\nKaggle na szczescie posiada juz w swoich zbiorach 12GB danych KITIE, wiec nie musimy go juz sciagac. Dodatkowo korzystamy jeszcze z pliku calibracji dla kamer umieszczonych w samochodzie.","13ab554f":"Nasz sample validacyjny jest ten sam ktory uzylismy do modelu","72c395f1":"## Dice Loss\nhttps:\/\/github.com\/henyau\/Image-Segmentation-with-Unet\/blob\/master\/train.py\n\nhttps:\/\/ilmonteux.github.io\/2019\/05\/10\/segmentation-metrics.html","c4d6a729":"## Train, Test split\n\nTutaj wlasnie dzielimy nasz zbior danych train i test","5b0914e2":"## Pomocnicze funkcje do danyh\n\nGet labels: zwraca potrzebne dane do ka\u017cdego obrazka, ka\u017cdy obiekt jest osobn\u0105 linijk\u0105 z opisem. Najwa\u017cniejsze parametry to:\n\n* **type**: typ obiektu\n* **bounding box**: kwadrat dooko\u0142a obiektu. `bbox_xmin', 'bbox_ymin', 'bbox_xmax', 'bbox_ymax',\n* **bounding box 3d**: Do jego okre\u015blenia potrzebne s\u0105 3 parametry: \n\n1.  **`dim_height', 'dim_width', 'dim_length`** - odpowiednio wysoko\u015b\u0107, szeroko\u015b\u0107 i d\u0142ugo\u015b\u0107 bounding box\n2. **`loc_x', 'loc_y', 'loc_z`** - gdzie si\u0119 znajduje dany obiekt\n3. **`rotation_y`** - jak jest obrocony obiekt","f0466f28":"Patrzymy na wyniki naszego modelu","82fc0da6":"Zapisujemy nasz model (tylko wagi), na p\u00f3\u017cniej. Mo\u017ce b\u0119dziemy go douczy\u0107.","c647ddd9":"## pokazanie danych\n\nPotrzeba nam obliczy\u0107 jak dobrze BoundingBox kt\u00f3ry przewidzieli\u015bmy zgadza si\u0119 ze \u017cr\u00f3d\u0142em. Do tego potrzebny jest algorytm IOU kt\u00f3ry oblicza jak\u0105 cz\u0119\u015b\u0107 wsp\u00f3ln\u0105 maj\u0105 dwie maski i dzieli j\u0105 przez sum\u0119 obydwu masek. je\u015bli maski si\u0119 zgadzaj\u0105 to podzielenie jednego przez drugie da nam wynik jeden. Im mniejszy jest obszar wsp\u00f3lny do unii dw\u00f3ch obszar\u00f3w tym IOU jest bli\u017csze zera.\n\nhttps:\/\/www.pyimagesearch.com\/2016\/11\/07\/intersection-over-union-iou-for-object-detection\/#:~:text=Intersection%20over%20Union%20is%20an,the%20popular%20PASCAL%20VOC%20challenge.\n\n![IOU](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2016\/09\/iou_equation.png)","9651f209":"# Model\n\nNasz model b\u0119dzie standardowym modelem UNET, to znaczy zw\u0119\u017ca si\u0119 do \u015brodku i rozszerza od \u015brdoka `upsample` pozwala \u0142\u0105czy\u0107 warstwy mi\u0119dzy sob\u0105","b1cb8981":"## TRAIN IMAGE","117b05f7":"# Image Data Generator\n\nZamiast \u0142adowa\u0107 wszystko do pami\u0119ci TensorFlow pozwala u\u017cywa\u0107 Data Generatora. To on za ka\u017cdym razem przy pobieraniu Batch danych otwiera obrazek z dysku przetwarza go i wyrzucana zewn\u0105trz. Podstawowym jest ImageDataGenerator kt\u00f3ry pozwala odczytywa\u0107 obrazy z DataFrame albo folderu, ale do maski musimy utworzy\u0107 w\u0142asn\u0105 klas\u0119 kt\u00f3re pozwala zwraca\u0107 na wyj\u015bciu obrazek do naszego modelu generowany przez funkcj\u0119 `get_mask`.","a1ebf9dd":"**Teraz tworzymy klase KittiDataGenerator i ona na wej\u015bciu posiada obraz, i na wyj\u015bciu obraz o tych samych wielko\u015bciach.**","217da326":"# Zadanie\n\nZadanie do tego notebooka to:\n\n* poprawienie tego modelu. Mo\u017ana u\u017cy\u0107 Transfer Learning z model\u00f3w COCO, albo ostatnio R-CNN.\n* Zbudowanie modelu wykrywaj\u0105cego konkretne obiekty (wi\u0119cej klas).\n* Zbudowanie modelu do wykrywania Bounding Box 3D (tutaj potrzebne by by\u0142y dane stereoskopowe), ale Kitti tak\u017c\u0119 je posiada.","ac81686a":"# Ogl\u0105danie danych\n\npobieramy dane ze wszysttkich katalogow, i pobieramy liste masek oraz obrazow.","3491bc2c":"## Show data\n\n\nPozwala nam te\u017c pokaza\u0107 na przyk\u0142adzie jak dobrze radzimy sobie z danymi. W tym przypadku nie mamy obliczengo y_pred, dlatego loss powinien by\u0107 bliski -1.","823e9854":"### Draw 3D\n\npotrzebny jest jeszcze calibration file, on posiada tez informacje o ustawieniach kamery potrzebnych przy rysowaniu obrazka.\n\nDodatkowo przedstawienie 3d bounding box-a wymaga przekszta\u0142cenia i na\u0142o\u017cenia go na p\u0142aski obrazek. Do tego potrzebujemy kilku pomocniczych funkcji poni\u017cej. P\u0142askie obiekty typu DontCare rysujemy jako 2D BoundingBox. Nie posiadaj\u0105 te obiekty informacji 3D.\n\n\n\n#### Calibration file\nCalibration file posiada informacja jak obraz zosta\u0142 uchwycony z kamery samochodu. Kamera w samochodzie znajdowa\u0142a s\u0119 na dachu i s\u0105 dwie kamery kt\u00f3re s\u0105 bardzo blisko siebie (obraz stereo tak jak u ludzi dzi\u0119ki dw\u00f3m oczom lepiej orientujemy si\u0119 w przestrzeni 3D). Poza tym Kitti3D posiada LIDAR.\n\n\n\nhttps:\/\/github.com\/yanii\/kitti-pcl\/blob\/master\/KITTI_README.TXT\n\n\nThe sensor calibration zip archive contains files, storing matrices in row-aligned order, meaning that the first values correspond to the first row:\n\ncalib_cam_to_cam.txt: Camera-to-camera calibration\n\n* S_xx: 1x2 size of image xx before rectification\n* K_xx: 3x3 calibration matrix of camera xx before rectification\n* D_xx: 1x5 distortion vector of camera xx before rectification\n* R_xx: 3x3 rotation matrix of camera xx (extrinsic)\n* T_xx: 3x1 translation vector of camera xx (extrinsic)\n* S_rect_xx: 1x2 size of image xx after rectification\n* R_rect_xx: 3x3 rectifying rotation to make image planes co-planar\n* P_rect_xx: 3x4 projection matrix after rectification\n* Note: When using this dataset you will most likely need to access only P_rect_xx, as this matrix is valid for the rectified image sequences. maintained","9398b395":"## KONFIGURACJA\n\nhttps:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/"}}