{"cell_type":{"a3e224f7":"code","e9b30b24":"code","355fc654":"code","00d8c600":"code","b56542e5":"code","7bd09a87":"code","497a54b2":"code","d4e7d438":"code","7c45e7f0":"code","97bb827d":"code","a7967baa":"code","c4042928":"code","14aa8ad1":"code","5115fb9d":"code","ffbc5640":"code","7b2b3de5":"code","4f49203a":"code","9d6d506a":"code","f4155365":"code","5822cdfd":"code","d3fd2ff0":"code","4c45602f":"code","85c5b266":"code","baeba83d":"code","c7a3f96c":"code","49f1819c":"code","95b00871":"code","4f1df72f":"code","1f8f3af3":"code","90b4a051":"code","fa474a9e":"code","532bfe0c":"code","11b59318":"code","b14cbe88":"code","9a64b4ea":"code","68209362":"code","2ba45347":"code","e3bf5f38":"code","3f036941":"code","b41bc633":"code","3f27de77":"markdown","2f18b19e":"markdown","f67bd717":"markdown","8682666b":"markdown","9d6f6d5a":"markdown","c59001dc":"markdown","a9dbba40":"markdown","c49d14a7":"markdown","fc7ce01b":"markdown","73368d0d":"markdown","b4cef61f":"markdown","127b1e94":"markdown","faac2c11":"markdown","4e06ca0b":"markdown","40aa6e35":"markdown","4cdabb9b":"markdown","fbbd8f32":"markdown","b9246db4":"markdown","e19a839b":"markdown","35bb3f24":"markdown"},"source":{"a3e224f7":"# to track experiments\n!pip install comet_ml","e9b30b24":"from comet_ml import Experiment\n### Loading Libraries\nimport numpy as np\nimport os\nimport pandas as pd\n\nimport tensorflow as tf\nimport sklearn\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom tensorflow import keras\n\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.layers import Input, Dense, Dropout,Flatten, AveragePooling2D, GlobalAveragePooling2D, Concatenate\nimport tensorflow.keras.optimizers as optimizers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D\nfrom tensorflow.keras.utils import plot_model","355fc654":"## Loading The Data\n\ndataset_path = '\/kaggle\/input\/data'\ntrain_valid_images_path = os.path.join(dataset_path, \"train_val_list.txt\")\ntest_images_path = os.path.join(dataset_path, \"test_list.txt\")\ndataset_df_path = os.path.join(dataset_path, 'Data_Entry_2017.csv')\n\nwith open(train_valid_images_path, 'r') as the_file:\n    train_valid_images = the_file.read().splitlines()\n    \nwith open(test_images_path, 'r') as the_file: \n    test_images = the_file.read().splitlines()\n\n    \nprint(f\"We have {len(train_valid_images)} training and validation images.\")\nprint(f\"We have {len(test_images)} testing images.\")\ntrain_valid_images[:3]","00d8c600":"raw_dataset = pd.read_csv(dataset_df_path)\nraw_dataset.info()\n\nraw_dataset.head(10)","b56542e5":"raw_dataset.info()","7bd09a87":"\nCLASSES = ['No Finding', 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n\ndef preprocess_dataset_df(dataset, train_valid_images, test_images):\n    def split_training_testing(df, train_valid_images, test_images):\n        train_valid_df = df[df['Image Index'].isin(train_valid_images)]\n        test_df = df[df['Image Index'].isin(test_images)]\n        return train_valid_df, test_df\n    \n    def sample_data(df, num_samples):\n    \n        def get_rows_with_multiple_disease(df):\n            return df[df['Finding Labels'].apply(lambda cell_value: True if '|' in cell_value else False)]\n\n        def get_disease_samples(df, label, num_samples):\n            subset_df = df[df['Finding Labels'] == label]\n            return subset_df[:num_samples]\n\n        # Sampling data\n        multiple_disease = get_rows_with_multiple_disease(df)\n\n        # empty df to concat in.\n        single_disease = pd.DataFrame(columns = df.columns)\n\n        for disease in CLASSES:\n            disease_samples = get_disease_samples(df, disease, 2000)\n            single_disease = pd.concat([disease_samples, single_disease])\n\n        return pd.concat([multiple_disease, single_disease])\n\n    \n    def delete_extra_columns(dataset_df):\n        needed_columns = ['Image Index', 'Finding Labels', 'Patient ID']\n        dataset_df = dataset_df.drop(dataset_df.columns.difference(needed_columns), axis=1)\n        return dataset_df\n    \n    def change_labels_as_list(dataset_df):\n        dataset_df['Finding Labels'] = dataset_df['Finding Labels'].apply(lambda value: value.split('|'))\n        return dataset_df\n    \n    def add_images_paths(dataset_df):\n        data_image_paths = {os.path.basename(x): x for x in glob(os.path.join(dataset_path, 'images*', '*', '*.png'))}\n        dataset_df['path'] = dataset_df['Image Index'].map(data_image_paths.get)\n        return dataset_df\n    \n\n    # preprocess data\n    new_dataset_df = delete_extra_columns(dataset)\n    \n    new_dataset_df, test_df = split_training_testing(new_dataset_df, train_valid_images, test_images)\n    test_df = change_labels_as_list(test_df)\n    test_df = add_images_paths(test_df)\n    \n    \n    new_dataset_df = sample_data(new_dataset_df, 2000)\n    new_dataset_df = change_labels_as_list(new_dataset_df)\n    new_dataset_df = add_images_paths(new_dataset_df)\n    return new_dataset_df.sort_index(), test_df\n\ndf, test_df = preprocess_dataset_df(raw_dataset, train_valid_images, test_images)\ndf.head()","497a54b2":"df.info()","d4e7d438":"test_df.head()","7c45e7f0":"from seaborn import countplot\n\n# expand findings to simplify preprocessing\nexpanded_df = df[['Patient ID', 'Finding Labels']].explode('Finding Labels')\n\nprint(\"Number of samples in the dataset:\", len(df))\nprint(\"Number of unique patients:\", df['Patient ID'].nunique())\nprint(\"Number of classes: \", expanded_df['Finding Labels'].nunique())\nprint(\"Labels: \", expanded_df['Finding Labels'].unique())","97bb827d":"# expand each list of Finding Labels column\n# expanded_df = df[['Patient ID', 'Finding Labels']].explode('Finding Labels')\nfindings_count = expanded_df.groupby('Finding Labels')['Finding Labels'].count()\nfindings_count.sort_values(ascending=False)","a7967baa":"def plot_disease_distribtion(df):\n    df['Finding Labels'].value_counts().plot(kind=\"bar\")\n    \nplot_disease_distribtion(df.explode('Finding Labels'))","c4042928":"def print_individual(df, disease):\n    print(f\"{disease}:\", len(df[df['Finding Labels'].apply(lambda x: sorted(x) == [disease])]))\n          \nfor disease in CLASSES:\n    print_individual(df, disease)","14aa8ad1":"# Samples of No Finding \ndef get_samples_with_one_disease(df, disease):\n    return df[df['Finding Labels'].apply(lambda x: sorted(x) == [disease])]\n\nget_samples_with_one_disease(df, 'No Finding').sample(5)","5115fb9d":"# training parameters\nBATCH_SIZE = 32\nCLASS_MODE = 'categorical'\nCOLOR_MODE = 'rgb'\nTARGET_SIZE = (256, 256)\nEPOCHS = 10\nSEED = 1337\n","ffbc5640":"print(\"length of training\", len(df))\nprint(\"length of testing\", len(test_df))","7b2b3de5":"from sklearn.model_selection import train_test_split\n\ntrain_patients_ids, valid_patients_ids = train_test_split(df['Patient ID'].unique(), test_size=0.15, random_state=42)\ntrain_df = df[df['Patient ID'].isin(train_patients_ids)]\nvalid_df = df[df['Patient ID'].isin(valid_patients_ids)]","4f49203a":"def print_ids_percentage(df1, df1_name, df2, df3):\n    print(f\"Percentage of {df1_name}: \", len(df1) \/ (len(df1) + len(df2) + len(df3)))\n    \n    \nprint(\"Common ids between sets:\", len(set(train_df['Patient ID']) & set(valid_df['Patient ID']) & set(test_df['Patient ID'])))\nprint_ids_percentage(train_df, 'training', valid_df, test_df)\nprint_ids_percentage(valid_df, 'validation', train_df, test_df)\nprint_ids_percentage(test_df, 'testing', train_df, valid_df)\nprint(\"Data Size\")\nprint(\"training:\", len(train_df))\nprint(\"validation:\", len(valid_df))\nprint(\"testing:\", len(test_df))","9d6d506a":"plot_disease_distribtion(train_df.explode('Finding Labels'))","f4155365":"plot_disease_distribtion(valid_df.explode('Finding Labels'))","5822cdfd":"train_augmentation_parameters = dict(\n#     preprocessing_function=preprocess_input,\n    rescale=1\/255,\n#     rotation_range=10,\n#     zoom_range=0.2,\n    horizontal_flip=True\n#     fill_mode='nearest',\n#     brightness_range = [0.8, 1.2]\n)\n\nvalid_augmentation_parameters = dict(\n#     preprocessing_function=preprocess_input\n    rescale=1\/255\n)\n\ntest_augmentation_parameters = dict(\n#     preprocessing_function=preprocess_input\n    rescale=1\/255\n)\n\n\ntrain_consts = {\n    'seed': SEED,\n    'batch_size': BATCH_SIZE,\n    'class_mode': CLASS_MODE,\n    'color_mode': COLOR_MODE,\n    'target_size': TARGET_SIZE,\n    'classes': CLASSES,\n    'shuffle': True\n}\n\nvalid_consts = {\n    'seed': SEED,\n    'batch_size': BATCH_SIZE,\n    'class_mode': CLASS_MODE,\n    'color_mode': COLOR_MODE,\n    'target_size': TARGET_SIZE, \n    'classes': CLASSES,\n    'shuffle': False\n}\n\ntest_consts = {\n    'batch_size': 1,  # should be 1 in testing\n    'class_mode': CLASS_MODE,\n    'color_mode': COLOR_MODE,\n    'target_size': TARGET_SIZE,\n    'classes': CLASSES,\n    'shuffle': False\n}\n\n# Using the training phase generators \ntrain_augmenter = ImageDataGenerator(**train_augmentation_parameters)\nvalid_augmenter = ImageDataGenerator(**valid_augmentation_parameters)\ntest_augmenter = ImageDataGenerator(**test_augmentation_parameters)","d3fd2ff0":"train_generator = train_augmenter.flow_from_dataframe(dataframe=train_df,\n                             x_col='path',\n                             y_col='Finding Labels',\n                             **train_consts)\n\nvalid_generator = valid_augmenter.flow_from_dataframe(dataframe=valid_df,\n                             x_col='path',\n                             y_col='Finding Labels',\n                             **valid_consts)\n\ntest_generator = test_augmenter.flow_from_dataframe(dataframe=test_df,\n                             x_col='path',\n                             y_col='Finding Labels',\n                             **test_consts)","4c45602f":"def get_class_weights(total_counts, class_positive_counts, multiply):\n    \"\"\"\n    Calculate class_weight used in training\n    Arguments:\n    total_counts - int\n    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n    multiply - int, positve weighting multiply\n    use_class_balancing - boolean \n    Returns:\n    class_weight - dict of dict, ex: {\"Effusion\": { 0: 0.01, 1: 0.99 }, ... }\n    \"\"\"\n    def get_single_class_weight(pos_counts, total_counts):\n        denominator = (total_counts - pos_counts) * multiply + pos_counts\n        return {\n            0: pos_counts \/ denominator,\n            1: (denominator - pos_counts) \/ denominator,\n        }\n\n    class_names = list(class_positive_counts.keys())\n#     print(class_positive_counts)\n    label_counts = np.array(list(class_positive_counts.values()))\n    class_weights = []\n    for i, class_name in enumerate(class_names):\n        class_weights.append(get_single_class_weight(label_counts[i], total_counts))\n\n    return class_weights\n\n\ndef get_sample_counts(df):\n    expanded_df = df[['Patient ID', 'Finding Labels']].explode('Finding Labels')\n    findings_count = expanded_df.groupby('Finding Labels')['Finding Labels'].count()\n    samples_count = df.shape[0]\n    return samples_count, findings_count.to_dict()\n\n# train_pos_counts is a dict with each class and number of occurences.\ntrain_counts, train_pos_counts = get_sample_counts(train_df)\nclass_weights = get_class_weights(train_counts, train_pos_counts, multiply=1)\n","85c5b266":"class_weights","baeba83d":"# recall, precision, f1-score, AUROC for each class.\nclass Metrics(Callback):\n    def __init__(self, val_data, *args, **kwargs):\n        super().__init__()\n        self.validation_data = val_data\n        self.class_names = list(self.validation_data.class_indices.keys())\n        self.reports = []\n        self.aurocs = {}\n        for c in self.class_names:\n            self.aurocs[c] = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_hat = np.asarray(self.model.predict(self.validation_data, verbose=1))\n        y_hat = self.adjust_y_pred(y_hat)\n        y_true = self.adjust_y_true()\n        self.calculate_recall_precision_f1_score(y_true, y_hat, epoch)\n        self.calculate_auroc(y_true, y_hat, epoch)\n        return\n\n    # Utility method\n    def get(self, metrics, of_class):\n        return [report[str(of_class)][metrics] for report in self.reports]\n\n    def adjust_y_true(self):\n        # self.validation_data\n        val_trues = self.validation_data.classes\n        y_true = sklearn.preprocessing.MultiLabelBinarizer().fit_transform(val_trues)\n        return y_true\n\n    def adjust_y_pred(self, predictions):\n        y_pred = np.zeros(predictions.shape)\n        y_pred[predictions > 0.5] = 1\n        return y_pred\n\n    def calculate_recall_precision_f1_score(self, y_true, y_hat, epoch):\n        report = classification_report(y_true, y_hat,\n                                       output_dict=True,\n                                       target_names=self.class_names, zero_division=0)\n\n        report_to_display = classification_report(y_true, y_hat,\n                                                  target_names=self.class_names, zero_division=0)\n        print('\\n==========================')\n        print(f\"Epoch {epoch + 1} Metrics\")\n        print(report_to_display)\n\n        self.reports.append(report)\n\n    def calculate_auroc(self, y_true, y_hat, epoch):\n        print(\"\\n==========================\")\n        print(f\"Epoch {epoch + 1} AUROCs.\")\n        current_auroc = []\n        # calculate AUROC for each class.\n        for i in range(len(self.class_names)):\n            try:\n                score = roc_auc_score(y_true[:, i], y_hat[:, i])\n            except ValueError:\n                score = 0\n            # save class auroc score for this epoch\n            self.aurocs[self.class_names[i]].append(score)\n            # append classes AUROCs for the same epoch\n            current_auroc.append(score)\n            print(f'{i: >2}. {self.class_names[i]: >20}: {score}')\n            # print(f\"{i}. {self.class_names[i]}: {score}\")\n\n        # mean across classes\n        mean_auroc = np.mean(current_auroc)\n        print(f\"\\n{'': >5} Epoch {epoch + 1} Mean AUROC: {mean_auroc}\")\n        print(\"==========================\")\n\n        return\n","c7a3f96c":"def create_dir(dirname):\n    try:\n        os.makedirs(dirname)\n        print(f\"Directory '{dirname}' created.\") \n    except FileExistsError:\n        print(f\"Directory '{dirname}' already exists.\")","49f1819c":"models_dir = '\/kaggle\/working\/models\/'\nmodel_name = 'NoImageNetNoAvgPoolingClassWeights'\n\nmodel_path = os.path.join(models_dir, model_name)\nbest_model_path = os.path.join(model_path, 'best')\nmodel_epochs_path = os.path.join(model_path, 'epochs')\nmodel_logs_path = os.path.join(model_path, 'logs')\n\ncreate_dir(models_dir)\ncreate_dir(best_model_path)\ncreate_dir(model_epochs_path)\ncreate_dir(model_logs_path)","95b00871":"from tensorflow.keras.callbacks import *\n\nmulti_label_metrics = Metrics(valid_generator)\n\ncallbacks = [\n    ModelCheckpoint(os.path.join(best_model_path, 'best'), monitor='val_loss',verbose=1, save_best_only=True),\n    ModelCheckpoint(filepath=os.path.join(model_epochs_path, 'model.epoch{epoch:02d}-val_loss{val_loss:.2f}'), save_freq='epoch', period=10),\n    ReduceLROnPlateau(factor=0.1, patience=1, min_lr=1e-8, verbose=1, cooldown=0),\n    TensorBoard(model_logs_path), \n    CSVLogger(os.path.join(model_path, \"model_history_log.csv\"), append=True),\n    multi_label_metrics\n]","4f1df72f":"from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n\nencoder = DenseNet121(include_top=False, weights=None, input_shape=(*TARGET_SIZE, 3))\n\nout_encoder = encoder.layers[426].output\n# avg_conv = GlobalAveragePooling2D()(conv_base)\n\nx = keras.layers.Flatten()(out_encoder)\nx = keras.layers.Dense(256, activation='relu')(x)\nx = keras.layers.Dense(256, activation='relu')(x)\n\noutput =  Dense(15, activation='sigmoid')(x)\n\nmodel = Model(encoder.input, outputs=output)\n# model.summary()\n","1f8f3af3":"from tensorflow.keras.metrics import Accuracy, AUC, Precision, Recall\nmodel.compile(optimizer='adam',\n              loss=\"binary_crossentropy\",\n              metrics=['accuracy'])","90b4a051":"history = model.fit(\n\tx=train_generator,\n    steps_per_epoch=len(train_generator),\n\tepochs=EPOCHS,\n\tvalidation_data=valid_generator,\n    validation_steps=len(valid_generator),\n    callbacks=callbacks,\n    class_weight=class_weights\n)","fa474a9e":"def plot_learning_metrics(history_model, to_plot):\n\n    plt.plot(history_model.history[to_plot], label=to_plot)\n    plt.plot(history_model.history['val_'+to_plot], label = 'val_' + to_plot)\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n#     plt.ylim([0.5, 1])\n    plt.legend(loc='lower right')\n\n    plt.show()","532bfe0c":"plot_learning_metrics(history, 'loss')","11b59318":"plot_learning_metrics(history, 'accuracy')","b14cbe88":"# predictions = model.predict(valid_generator, steps=len(valid_generator), verbose=1)","9a64b4ea":"# y_pred = np.zeros(predictions.shape)\n# y_pred[predictions>0.5] = 1\n# y_pred","68209362":"# from sklearn.preprocessing import MultiLabelBinarizer\n# val_trues = valid_generator.classes\n# y_true = MultiLabelBinarizer().fit_transform(val_trues)\n# y_true","2ba45347":"# import sklearn","e3bf5f38":"# print(sklearn.metrics.classification_report(y_true, y_pred, target_names=valid_generator.class_indices.keys()))","3f036941":"# sklearn.metrics.roc_auc_score(y_true, y_pred)","b41bc633":"# import libraries\n# from sklearn.metrics import roc_curve, auc\n\n# # create plot\n# fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n# for (i, label) in enumerate(dummy_labels):\n#     fpr, tpr, thresholds = roc_curve(test_Y[:,i].astype(int), quick_model_predictions[:,i])\n#     c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (label, auc(fpr, tpr)))\n\n# # Set labels for plot\n# c_ax.legend()\n# c_ax.set_xlabel('False Positive Rate')\n# c_ax.set_ylabel('True Positive Rate')\n# fig.savefig('quick_trained_model.png')","3f27de77":"## Load Libraries","2f18b19e":"## Load CSV Files","f67bd717":"## Metrics and Callbacks","8682666b":"## Evaluation","9d6f6d5a":"Get portion of data, so we can adjust train valid sets ration","c59001dc":"### Data Augmentors","a9dbba40":"### Spliting Training into Training and Validation According to Patient Id\nWe don't want the same patient to present in multiple sets.","c49d14a7":"Visually check if the training and validation sets are representative","fc7ce01b":"Get rows for each split\n","73368d0d":"### Training","b4cef61f":"## Analysis and Preprocessing","127b1e94":"#### Hyper-Parameters","faac2c11":"### Model","4e06ca0b":"Note: although we reduced the number of single disease rows, the data is still unbalanced.","40aa6e35":"Split Patients' IDs","4cdabb9b":"## Class Weights","fbbd8f32":"## Data Generator and Data Prepration","b9246db4":"Comet_ML\n","e19a839b":"## Model","35bb3f24":"### Model Compilation"}}