{"cell_type":{"11eb9a79":"code","88aec5c1":"code","0d8d5508":"code","7d9c7aca":"code","890793d6":"code","90781ca1":"code","ca996a13":"code","c6390a91":"code","90479de5":"code","6b25d323":"code","495b2170":"code","bd29e2dd":"code","2d6ee99a":"code","95d2a8f9":"code","0609097b":"code","58c44b7c":"code","5ce12fdc":"code","784d580a":"code","6cb46cf1":"code","96573d70":"code","336bfede":"code","44975136":"code","f9630ce4":"code","55540aa8":"code","9755b42d":"code","2a976377":"code","0701458c":"code","1ac2b471":"code","869fff10":"code","b9fd0b8e":"code","ebc2c25d":"code","218283ab":"markdown","e703cb49":"markdown","adf47cf7":"markdown","b549ca81":"markdown","1538a3fb":"markdown","0ec2fae5":"markdown","52358b38":"markdown","7f28e56c":"markdown","623c11d1":"markdown","c35b5c11":"markdown","2619fb06":"markdown","a5c9f05e":"markdown","8e6a58e4":"markdown","75348669":"markdown","3e5a5244":"markdown"},"source":{"11eb9a79":"import numpy as np\nimport pandas as pd","88aec5c1":"# importing the traning data\ntrain_data = pd.read_csv('..\/input\/emoji-prediction-dataset\/Train.csv')\ntrain_data.head()","0d8d5508":"# import the testing data\ntest_data = pd.read_csv(\"..\/input\/emoji-prediction-dataset\/Test.csv\")\ntest_data.head()","7d9c7aca":"# import the mappings file\nmappings = pd.read_csv(\"..\/input\/emoji-prediction-dataset\/Mapping.csv\")\nmappings.head()","890793d6":"# print the shapes of all files\ntrain_data.shape, test_data.shape, mappings.shape","90781ca1":"train_length = train_data.shape[0]\ntest_length = test_data.shape[0]\ntrain_length, test_length","ca996a13":"from nltk.corpus import stopwords","c6390a91":"stop_words = stopwords.words(\"english\")\nstop_words[:5]","90479de5":"# tokenize the sentences\ndef tokenize(tweets):\n    stop_words = stopwords.words(\"english\")\n    tokenized_tweets = []\n    for tweet in tweets:\n        # split all words in the tweet\n        words = tweet.split(\" \")\n        tokenized_string = \"\"\n        for word in words:\n            # remove @handles -> useless -> no information\n            if word[0] != '@' and word not in stop_words:\n                # if a hashtag, remove # -> adds no new information\n                if word[0] == \"#\":\n                    word = word[1:]\n                tokenized_string += word + \" \"\n        tokenized_tweets.append(tokenized_string)\n    return tokenized_tweets","6b25d323":"# translate tweets to a sequence of numbers\ndef encod_tweets(tweets):\n    tokenizer = Tokenizer(filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', split=\" \", lower=True)\n    tokenizer.fit_on_texts(tweets)\n    return tokenizer, tokenizer.texts_to_sequences(tweets)","495b2170":"# example_str = tokenize(['This is a good day. @css #mlhlocalhost'])\n# encod_str = encod_tweets(example_str)\n# print(example_str)\n# print(encod_str)","bd29e2dd":"# apply padding to dataset and convert labels to bitmaps\ndef format_data(encoded_tweets, max_length, labels):\n    x = pad_sequences(encoded_tweets, maxlen= max_length, padding='post')\n    y = []\n    for emoji in labels:\n        bit_vec = np.zeros(20)\n        bit_vec[emoji] = 1\n        y.append(bit_vec)\n    y = np.asarray(y)\n    return x, y","2d6ee99a":"# create weight matrix from pre trained embeddings\ndef create_weight_matrix(vocab, raw_embeddings):\n    vocab_size = len(vocab) + 1\n    weight_matrix = np.zeros((vocab_size, 300))\n    for word, idx in vocab.items():\n        if word in raw_embeddings:\n            weight_matrix[idx] = raw_embeddings[word]\n    return weight_matrix","95d2a8f9":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers import Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","0609097b":"# final model\ndef final_model(weight_matrix, vocab_size, max_length, x, y, epochs = 5):\n    embedding_layer = Embedding(vocab_size, 300, weights=[weight_matrix], input_length=max_length, trainable=True, mask_zero=True)\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(Bidirectional(LSTM(128, dropout=0.2, return_sequences=True)))\n    model.add(Bidirectional(LSTM(128, dropout=0.2)))\n    model.add(Dense(20, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.fit(x, y, epochs = epochs, validation_split = 0.25)\n    score, acc = model.evaluate(x_test, y_test)\n    return model, score, acc","58c44b7c":"import math","5ce12fdc":"tokenized_tweets = tokenize(train_data['TEXT'])\ntokenized_tweets += tokenize(test_data['TEXT'])\nmax_length = math.ceil(sum([len(s.split(\" \")) for s in tokenized_tweets])\/len(tokenized_tweets))\ntokenizer, encoded_tweets = encod_tweets(tokenized_tweets)\nmax_length, len(tokenized_tweets)","784d580a":"x, y = format_data(encoded_tweets[:train_length], max_length, train_data['Label'])\nlen(x), len(y)","6cb46cf1":"x_test, y_test = format_data(encoded_tweets[train_length:], max_length, test_data['Label'])\nlen(x_test), len(y_test)","96573d70":"vocab = tokenizer.word_index\nvocab, len(vocab)","336bfede":"from gensim.models.keyedvectors import KeyedVectors","44975136":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('\/kaggle\/input\/glove840b300dtxt\/glove.840B.300d.txt','r',encoding='utf-8')\nfor line in f:\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","f9630ce4":"weight_matrix = create_weight_matrix(vocab, embeddings_index)\nlen(weight_matrix)","55540aa8":"model, score, acc = final_model(weight_matrix, len(vocab)+1, max_length, x, y, epochs = 5)\nmodel, score, acc","9755b42d":"model.summary()","2a976377":"y_pred = model.predict(x_test)\ny_pred","0701458c":"for pred in y_pred:\n    print(np.argmax(pred))","1ac2b471":"import math\nfrom sklearn.metrics import classification_report, confusion_matrix","869fff10":"y_pred = np.array([np.argmax(pred) for pred in y_pred])\ny_true = np.array(test_data['Label'])\nprint(classification_report(y_true, y_pred))","b9fd0b8e":"emoji_pred = [mappings[mappings['number'] == pred]['emoticons'] for pred in y_pred]\nemoji_pred","ebc2c25d":"for i in range(100, 150):\n    test_tweet = test_data['TEXT'][i]\n    pred_label = y_pred[i]\n    pred_emoji = emoji_pred[i]\n    print('tweet: ', test_tweet)\n    print('pred emoji: ', pred_label, pred_emoji)\n    print('-'*50)","218283ab":"create the weight matrix using our vocab and embeddings_index","e703cb49":"Apply padding to the encoded data using pad_sequences for both train and test tweets","adf47cf7":"**Example**  \nUncomment and run the following code cell to see the example of the output","b549ca81":"Print the classification report which gives:\n\n**precision** -> what % of predicted a's are actually a\n\n**recall** -> what % of a are predicted to be a\n\n**fi-score** -> Harmonic mean of precision and recall\n\n**support** -> actual values of each class","1538a3fb":"**Keras** - It is an Open Source Neural Network library written in Python that runs on top of Tensorflow, i.e., it uses tensors to run the operations. \n\n**Tokenizer** - vectorize text by turning each text into a sequence of integers\n\n**filters** - a string where each element is a character that will be filtered from text\n\n**lower** - boolean for lower case conversion\n\n**tokenizer.texts_to_sequences(tweets)** - transform each tweet in tweets to a sequence of integers\n","0ec2fae5":"**pad_sequences** - transforms list of sequences (list of integers) into 2D numpy arrays of shape (num_samples, maxlen)\n\nmaxlen is the length of longest sequence, can be provided as an argument also\n\nIf sequences are shorter than maxlen, they are padded with value at front or end (pre or post padding)\nIf sequences are longer than maxlen, they are truncated\n\n**bit_vec** -> vector of 0 and 1","52358b38":"Tokenizing the train and test tweets and then encoding them","7f28e56c":"Use .predict() funtion to predict the y values for x_test\n\ny values are numpy arrays of length 20 == number of classes\n\nThe class can be found out by finding the index of the maximum value","623c11d1":"Building vocabulary using **word_index** ","c35b5c11":"**Numpy**\n1.   NumPy is the fundamental package for scientific computing with Python.\n2.   It provides a high-performance multidimensional array object, and tools for    working with these arrays.\n\n**Pandas**\n\n\n1.   Pandas is the most popular python library that is used for data analysis.\n2.   We can manipulate like Excel sheets\n","2619fb06":"**NLTK is a library for Natural Language Processing (NLP) to create features from text**\nWhen using words as features, we need to handle:\n1.   Context -> eg: Not good\n2.   Identify root words -> eg: help, helper, helping\n3.   Words with similar meaning -> eg: good and nice\n\n**Stopwords are useless words or commonly used words. They add very little information to our model so can be removed**","a5c9f05e":"We need to follow the following steps to pre process the data before using it:\n\n\n1.   Each tweet should be tokenized into a list of words\n2.   Remove words starting with **@** because they generally refer to twitter       handles and thus provide little or no information\n3.   Remove **stopwords**\n4.   Remove the **#** character to get the actual word used as hashtag","8e6a58e4":"Run the final model on train data","75348669":"**keyedvectors** -> word vector storage and look up\n\nIt is used to load hidden weight matrix\n\n**binary** -> to specify whether the data is binary or not","3e5a5244":"**Embeddings** -> are used mainly for text processing.\n\n**Example**:\n\nHope to see you soon. -> [0, 1, 2, 3, 4] (embedding of words)\n\nNice to see you again. -> [5, 1, 2, 3, 6]\n\n**Vocab size** = number of unique words in vocabulary = max number in embeddings + 1 = 6 + 1 = 7\n\n**Sequential** -> means we are using linear stack of layers\n\n**LSTM** -> Long Short term memory\n\n**Bidirectional** -> wrapper to indicate the type of LSTM used\n\n**Dense** -> Denselu connected neural network\n\n**Activation function** -> decided whether a neuron should be activated or not by calculating weighted sum and adding bias to it.\n\nIt provides non-linearlity to output of neuron\n\n**A neural network without an activation function is just a Linear Regression**. By using activation function, we can make our model solve complex functions.\n\n**Softmax** -> similar to **Sigmoid function** -> used for multiple classes, gives output between 0 & 1 and divide by sum of outputs\n\n**Optimizer** -> finds the trainable variables on which cost depends and change their values to **optimize cost**\n\n**Entroy** -> -sum(p log p) -> avg amount of information drawn from one sample"}}