{"cell_type":{"c972b913":"code","03f34f3d":"code","7e86d252":"code","f7ff63da":"code","afa46c06":"code","0534681f":"code","51954cc7":"code","ec3e4908":"code","014ae9a1":"code","fe2594cb":"code","353498b4":"code","903b1956":"code","29f0c29f":"code","00c8e249":"code","a5fcdf22":"code","ba729b57":"code","3db9dc21":"code","25f15e2c":"code","73c8e5ee":"code","0aea17b0":"code","4da3e891":"markdown","168cd291":"markdown"},"source":{"c972b913":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\n","03f34f3d":"#Load file \ndf = pd.read_csv('..\/input\/bank-additional-full.csv',sep = ';')\ndf.head() #Take a look at the data \n","7e86d252":"df.info() #10 numerical columns, 11 cateogorical ","f7ff63da":"df.describe()","afa46c06":"#Rename target variable column \ndf['subscribed'] = df['y']\ndf = df.drop(['y'], axis = 1)\ndf.head()","0534681f":"pd.pivot_table(df, index = 'default', values = 'subscribed', aggfunc = np.size)","51954cc7":"df['age'].plot(kind = 'hist', color = 'lightblue') #See gender distribution of age ","ec3e4908":"jobs =  pd.pivot_table(df, index = 'job', values = 'subscribed', aggfunc = np.size)\njobs.plot(kind = 'bar', color = 'lightblue')\nplt.xticks(rotation = 90)\nplt.legend().remove()\nplt.title('Client Job Distribution ')\n","014ae9a1":"df.head()","fe2594cb":"df['marital'].value_counts(normalize = True).plot(kind = 'bar') #Plot percentage distribution of marital status\nplt.xticks(rotation = 45)\nplt.title('Marital Status of Clients')\nplt.ylabel('% of Clients')\nplt.xlabel('Marital Status')\n\n#Almost 60% of clients are married, while nearly 30% are single ","353498b4":"y = df['subscribed']\nX = df.drop(['subscribed','duration'], axis = 1)\nX = pd.get_dummies(X) #one hot encode \nprint(X.shape)\nprint(y.shape) #Shape matching in rows! ","903b1956":"from sklearn.model_selection import train_test_split\nX_train, X_Test, y_train, y_test = train_test_split(X,y, test_size = .2, random_state = 40) #20% of data as test","29f0c29f":"from sklearn.ensemble import RandomForestClassifier \nmodel = RandomForestClassifier(n_estimators = 50)\nmodel.fit(X_train, y_train)","00c8e249":"#Let's create predictions \npreds = model.predict(X_Test)","a5fcdf22":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ncv_score = cross_val_score(model,X,y, cv = 10, scoring = 'roc_auc')","ba729b57":"from sklearn import metrics \n\nprint('Accuracy:',round(metrics.accuracy_score(y_test,preds),2)) #88% accuracy- not bad for limited data wrangling ","3db9dc21":"feature_values = pd.DataFrame(model.feature_importances_,\n                              index = X_train.columns,\n                              columns = ['importance']).sort_values('importance',\n                                                                    ascending=False)\nfeature_values.head(8).plot(kind = 'bar', color = 'lightgreen')\nplt.xticks(rotation = 85)\nplt.title('Feature Importance')\nplt.legend().remove()","25f15e2c":"from sklearn import svm \n\n#Create classifier model \nclf = svm.SVC(kernel = 'linear')\n\n#Train Model \nclf.fit(X_train, y_train)\n       \n#Predict \ny_pred = clf.predict(X_Test)","73c8e5ee":"from sklearn import metrics \n\nprint('Accuracy:',round(metrics.accuracy_score(y_test,y_pred),2)) #89% Accuracy- we've gone up about a point! ","0aea17b0":"from sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier(n_neighbors = 4) #Create model\n\nclf.fit(X_train,y_train) #Fit model \n\ny_pred = clf.predict(X_Test) #Predict on test dataset \n\nprint('Accuracy:',round(metrics.accuracy_score(y_test,y_pred),3))  #Check accuracy of model\n88.5% Accuracy! ","4da3e891":"**Let's try out SVM **","168cd291":"Machine Learning Portion "}}