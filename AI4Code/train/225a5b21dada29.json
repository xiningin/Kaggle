{"cell_type":{"dfe56e62":"code","55862509":"code","3967e0d7":"code","0437c3cc":"code","cffb4fe2":"code","30529fa3":"code","b4f8bea0":"code","0d77da87":"code","db4a0844":"code","ff1fe6ab":"code","1fa677be":"code","2fd6dc9e":"code","8f2db68d":"code","847a5720":"code","0b91e09d":"code","0474d1e8":"code","91b846a3":"code","74765fc7":"code","9b8fea2d":"code","e41f4b8f":"code","1555a60a":"code","49c0d92f":"code","6066012f":"code","c8bd9e57":"code","07602b36":"code","ceab7acd":"code","514c23b5":"code","1701b351":"code","236792c4":"code","f8c0eece":"code","bec304b1":"code","4ed2a72f":"code","8fbecbed":"code","aaa4fec5":"code","3c308e27":"code","d23dd9f4":"code","2e8ba7e6":"code","7db73767":"code","d461f774":"code","14b7cce0":"code","2f69c67a":"code","fba01d9c":"code","a231586e":"code","3f67428d":"code","39245e87":"code","4e1fa7a2":"code","ccc94331":"code","d835391c":"code","b8ab3582":"code","9ddb4cbf":"code","10e077e8":"code","954157ed":"code","7af4739d":"code","41c5874e":"code","a62847b9":"code","e9b4af31":"code","c9748211":"markdown","ec3377a4":"markdown","5a271111":"markdown","ff57f198":"markdown","1644f4d5":"markdown","8b51c0c1":"markdown","c9bde9b9":"markdown","5a83fe95":"markdown","e2714523":"markdown","35fef0a5":"markdown","e9a34e6d":"markdown","2ab5960c":"markdown","e62f92fd":"markdown","954f5521":"markdown","4aa91970":"markdown","8ba90fa9":"markdown","29e3acba":"markdown","ecfb0cb6":"markdown","825b3cbf":"markdown","27af69f6":"markdown","2d5830ea":"markdown","88ce497d":"markdown","412c7a6f":"markdown","4c9a86d0":"markdown","adfbe53b":"markdown","3c6f0f83":"markdown","e4106cce":"markdown","61055b5d":"markdown","ddbc5fe0":"markdown","5c7bf5a3":"markdown","6c4a7b73":"markdown","0819bfc6":"markdown","cb3fb8a4":"markdown","b05d7f64":"markdown","169e0f48":"markdown","8aec6c56":"markdown","0b3f84e2":"markdown","604607d2":"markdown","e89de6d9":"markdown","c016e673":"markdown","83045703":"markdown","eef0d0d8":"markdown","1ae8cc77":"markdown"},"source":{"dfe56e62":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","55862509":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport lightgbm as lgb\nimport optuna\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.feature_selection import RFE\nfrom plotly.offline import iplot, init_notebook_mode\nfrom plotly.subplots import make_subplots\ninit_notebook_mode()\n\nimport warnings\nwarnings.filterwarnings('ignore')","3967e0d7":"def path_join(name):\n    path = '\/kaggle\/input\/store-sales-time-series-forecasting'\n    return os.path.join(path, name)\n\n\ndef describe(df):\n    '''\n    make dataframe which describe the details about null count, etc\n    '''\n    print(f'Shape : {df.shape}')\n    summary = pd.DataFrame(df.dtypes, columns=['DataType']).reset_index()\n    summary = summary.rename(columns={'index': 'Feature'})\n    summary['null count'] = df.isnull().sum().values\n    summary['unique count'] = df.nunique().values\n    summary['First value'] = df.loc[0].values\n    summary['Second value'] = df.loc[1].values\n    summary['Third value'] = df.loc[2].values\n    \n    return summary","0437c3cc":"train = pd.read_csv(path_join('train.csv'))\ntest = pd.read_csv(path_join('test.csv'))\noil = pd.read_csv(path_join('oil.csv'))\nholidays_events = pd.read_csv(path_join('holidays_events.csv'))\nstores = pd.read_csv(path_join('stores.csv'))\ntransactions = pd.read_csv(path_join('transactions.csv'))","cffb4fe2":"display(describe(train))","30529fa3":"print(train['sales'].value_counts().sort_values()[:10])\nprint(train['onpromotion'].value_counts().sort_values()[:10])\nprint('\\n\\n')\n\n_, axes = plt.subplots(2, 2, figsize=(18, 10), facecolor='lightgray')\nplt.suptitle('Check the numeric distribution', color='blue', fontsize=30)\n\nsns.distplot(train['sales'], ax=axes[0, 0])\naxes[0, 0].set_title('sales displot', fontsize=25)\n\nsns.boxplot(x='sales', data=train, ax=axes[0, 1])\naxes[0, 1].set_title('sales boxplot', fontsize=25)\n\nsns.histplot(x='onpromotion', data=train, bins=20, ax=axes[1, 0])\naxes[1, 0].set_title('onpromotion hist', fontsize=25)\n\nsns.boxplot(x='onpromotion', data=train, ax=axes[1, 1])\naxes[1, 1].set_title('onpromotion boxplot', fontsize=25)\n\nplt.tight_layout()\nplt.show()","b4f8bea0":"# train plotly for my skill, haha\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=train['store_nbr'].value_counts().index, values=train['store_nbr'].value_counts()),\n              1, 1)\nfig.add_trace(go.Pie(labels=train['family'].value_counts().index, values=train['family'].value_counts()),\n              1, 2)","0d77da87":"display(describe(transactions))\nsns.displot(x='transactions', data=transactions)","db4a0844":"display(describe(holidays_events))\n\nfor var in holidays_events['locale_name'].unique():\n    print(var, holidays_events.query('locale_name==@var')['locale'].unique())","ff1fe6ab":"# description isn't able to be uses for analysis this case.. drop it\nholidays_events = holidays_events.drop(['locale', 'description'], axis=1)","1fa677be":"locale_name = holidays_events['locale_name'].value_counts()\nfig = px.pie(stores, values=locale_name, names=locale_name.index)\n\nfig.update_layout(\ntitle_font_color=\"#fff\",paper_bgcolor=\"#283747\",title_font_size=20,title_x=.5,font_color=\"#bbb\",\n    plot_bgcolor=\"#D6EAF8\")\n\nfig.show()\n\ndel locale_name","2fd6dc9e":"# train to use plotly.\nspecs = [[{'type':'domain'}, {'type':'domain'}]]\nfig = make_subplots(rows=1, cols=2, specs=specs, subplot_titles=['type_holiday', 'transferred'])\ntype_holiday = holidays_events['type'].value_counts()\ntransferred = holidays_events['transferred'].value_counts()\n\nfig.add_trace(go.Pie(labels=type_holiday.index, values=type_holiday),\n              row=1, col=1)\nfig.add_trace(go.Pie(labels=transferred.index, values=transferred),\n              row=1, col=2)\n\nfig.update_layout(\ntitle_font_color=\"#fff\",paper_bgcolor=\"#283747\",title_font_size=20,title_x=.5,font_color=\"#bbb\",\n    plot_bgcolor=\"#D6EAF8\")\nfig = go.Figure(fig)\nfig.show()\n\ndel type_holiday\ndel transferred","8f2db68d":"# encode transffered \nholidays_events['transferred'] = holidays_events['transferred'].apply(lambda x: 1 if x else 0)","847a5720":"display(describe(stores))","0b91e09d":"for var in stores['city'].unique():\n    print(var, stores.query('city==@var')['state'].unique())\n\nprint('-'*100)\nfor var in stores['city'].unique():\n    print(var, stores.query('city==@var')['type'].unique())\n    \nprint('-'*100)\nfor var in stores['city'].unique():\n    print(var, stores.query('city==@var')['cluster'].unique())","0474d1e8":"stores = stores.drop('state', axis=1)\ncity = stores['city'].value_counts()\ncluster = stores['cluster'].value_counts()\n\nspecs = [[{'type':'domain'}, {'type':'domain'}]]\nfig = make_subplots(rows=1, cols=2, specs=specs, subplot_titles=['city', 'cluster'])\n\nfig.add_trace(go.Pie(labels=city.index, values=city), row=1, col=1)\nfig.add_trace(go.Pie(labels=cluster.index, values=cluster), row=1, col=2)\n\nfig.update_layout(\ntitle_font_color=\"#fff\",paper_bgcolor=\"#283747\",title_font_size=20,title_x=.5,font_color=\"#bbb\",\n    plot_bgcolor=\"#D6EAF8\")\nfig = go.Figure(fig)\nfig.show()\n\ndel city\ndel cluster","91b846a3":"sns.countplot(x='type', data=stores)","74765fc7":"display(describe(oil))\nsns.displot(x='dcoilwtico', data=oil)","9b8fea2d":"oil['dcoilwtico'] = oil['dcoilwtico'].fillna(0)","e41f4b8f":"# merge data\nmerge_data = train.merge(oil, on='date', how='left')\nmerge_data = merge_data.merge(holidays_events, on='date', how='left')\nmerge_data = merge_data.merge(stores, on='store_nbr', how='left')\nmerge_data = merge_data.merge(transactions, on=['date', 'store_nbr'], how='left')","1555a60a":"# copy\nmerge_copy = merge_data.copy()\n\n# change dtype and get the date col\nmerge_copy['date'] = pd.to_datetime(merge_copy['date']).dt.date\nmerge_copy['year'] = pd.to_datetime(merge_copy['date']).dt.year\nmerge_copy['month'] = pd.to_datetime(merge_copy['date']).dt.month\nmerge_copy['day'] = pd.to_datetime(merge_copy['date']).dt.day\n\ndescribe(merge_copy)","49c0d92f":"merge_copy['transactions'] = merge_copy['transactions'].fillna(0)\nmerge_copy['dcoilwtico'] = merge_copy['dcoilwtico'].fillna(0)","6066012f":"display(describe(merge_copy))","c8bd9e57":"merge_copy['holiday_flag'] = [1 if not val else 0 for val in merge_copy['type_x'].isnull()]\nmerge_copy = merge_copy.drop(['type_x', 'locale_name', 'transferred'], axis=1)\nmerge_copy = merge_copy.rename(columns={'type_y': 'stores_type'})\ndisplay(describe(merge_copy))","07602b36":"df = merge_copy.copy()\ndf = df.sort_values('date')\ndf_g = df[['date', 'sales']].groupby('date').agg(date_sum=('sales', np.mean))\n# month avg \ndf_g['moving_avg'] = df_g.date_sum.rolling(30, min_periods=3).mean()\n\nplt.figure(figsize=(20, 5))\nplt.plot(df_g['moving_avg'])\nplt.show()\n\ndel df","ceab7acd":"_, axes = plt.subplots(1, 2, figsize=(15, 8))\ndf = merge_copy.groupby('year').agg(sales_mean=('sales', np.mean), dcoilwtico_mean=('dcoilwtico', np.mean))\nsns.barplot(x=df.index, y='sales_mean', data=df, ax=axes[0])\naxes[0].set_title('Mean sales each year', fontsize=20)\n\n# df = merge_copy.groupby('year').agg(dcoilwtico_mean=('dcoilwtico', np.mean))\naxes[1].set_title('Mean dcoilwtico each year', fontsize=20)\nsns.barplot(x=df.index, y='dcoilwtico_mean', data=df, ax=axes[1])","514c23b5":"df = merge_copy.groupby('month').agg(sales_mean=('sales', np.mean))\nplt.figure(figsize=(15, 5))\nsns.barplot(x=df.index, y='sales_mean', data=df)","1701b351":"df = merge_copy.groupby(['year', 'month'], as_index=False).agg(sales_mean=('sales', np.mean))\nplt.figure(figsize=(20, 5))\nplt.title('Mean sales each year-month', fontsize=20)\nsns.barplot(x='month', y='sales_mean', data=df, hue='year')\nplt.show()","236792c4":"df = merge_copy.groupby('day').agg(sales_mean=('sales', np.mean))\nplt.figure(figsize=(15, 5))\nplt.title('Mean sales each day', fontsize=20)\nsns.barplot(x=df.index, y='sales_mean', data=df)\nplt.show()","f8c0eece":"df = merge_copy.groupby('cluster').agg(sales_mean=('sales', np.mean))\nplt.figure(figsize=(15, 5))\nplt.title('Mean sales each cluster', fontsize=20)\nsns.barplot(x=df.index, y='sales_mean', data=df)\nplt.show()","bec304b1":"plt.figure(figsize=(10, 10))\ncorr = merge_copy.corr()\nsns.heatmap(corr, annot=True)","4ed2a72f":"del df\ndel merge_data","8fbecbed":"data = merge_copy.copy().drop(['id', 'date'], axis=1)\ndata = pd.get_dummies(data, drop_first=True)\nX = data.drop('sales', axis=1)\ny = data['sales']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=12)","aaa4fec5":"model = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred)))","3c308e27":"n = 3\npipe = Pipeline([('pca', PCA(n_components=n)), ('lr', LinearRegression())])\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint(n, np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred)))\n\nn = 10\npipe = Pipeline([('pca', PCA(n_components=n)), ('lr', LinearRegression())])\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint(n, np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred)))","d23dd9f4":"# features=10\nrfe = RFE(estimator=LinearRegression(), n_features_to_select=10)\nrfe.fit(X_train, y_train)\ny_pred = rfe.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred)))\nprint('\\n----top10 cols----\\n')\nfor boolean, col in zip(rfe.support_, X.columns):\n    if boolean:\n        print(col)","2e8ba7e6":"rf = RandomForestRegressor(n_estimators=30, random_state=123, max_leaf_nodes=50, max_depth=30)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred)))","7db73767":"importances = np.array(rf.feature_importances_)\nforest_importances = pd.Series(importances, index=X.columns).sort_values(ascending=False)[:16]\nprint(forest_importances[:16])\n# std = np.std([\n#     tree.feature_importances_ for tree in rf.estimators_], axis=0)\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 10))\n# forest_importances.plot.bar(yerr=std, ax=ax)\nforest_importances.plot.bar(ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","d461f774":"important_cols = ['store_nbr', 'onpromotion', 'cluster', 'transactions', 'year', 'family_BEVERAGES', 'family_CLEANING', 'family_DAIRY', 'family_GROCERY I', 'family_PRODUCE']\nX_importance = X[important_cols]\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_importance, y, test_size=0.2, random_state=123)\n\nmodel_importance = LinearRegression()\nmodel_importance.fit(X_train2, y_train2)\ny_pred = model_importance.predict(X_test2)\n\nprint(np.sqrt(mean_squared_error(y_true=y_test2, y_pred=y_pred)))","14b7cce0":"# Preparing dataset for LightGBM\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test)\n\nparams = {'metric' : 'rmse', 'seed': 123, 'verbosity':-1}\n\n# train data\ngbm = lgb.train(params, lgb_train, num_boost_round=500, valid_sets=[lgb_test])","2f69c67a":"y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\nprint(np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred)))","fba01d9c":"# prepare validation data\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12)\n\ndef objective(trial):\n    param = {\n        'metric' : 'rmse', \n        'verbosity': -1, \n        'boosting_type': trial.suggest_categorical('hoge', ['gbdt', 'dart']),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-7, 1.0)\n    }\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n    gbm = lgb.train(param, lgb_train, valid_sets=lgb_val, verbose_eval=False, early_stopping_rounds=30)\n    \n    y_pred = gbm.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred))\n    \n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)","a231586e":"print(study.best_value)\nprint(study.best_params)","3f67428d":"_ = merge_copy.copy().drop(['id', 'date'], axis=1)\n_ = pd.get_dummies(_, drop_first=True)\n_X = _.drop('sales', axis=1)\n_y = _['sales']\n\nscaler = StandardScaler()\n_X_scaled = scaler.fit_transform(_X)\n\n_X_train, _X_test, _y_train, _y_test = train_test_split(_X_scaled, _y, test_size=0.2, random_state=12)\n_X_train2, _X_val2, _y_train2, _y_val2 = train_test_split(_X_train, _y_train, test_size=0.2, random_state=12)","39245e87":"param = {\n    'metric' : 'rmse', \n    'verbosity': -1, \n    'boosting_type': 'gbdt',\n    'num_leaves': study.best_params['num_leaves'],\n    'learning_rate': study.best_params['learning_rate']\n}\n\nlgb_train = lgb.Dataset(_X_train2, _y_train2)\nlgb_val = lgb.Dataset(_X_val2, _y_val2, reference=lgb_train)\n\ngbm = lgb.train(param, lgb_train, valid_sets=lgb_val, verbose_eval=False, early_stopping_rounds=30)","4e1fa7a2":"importance = pd.DataFrame(gbm.feature_importance(), index=_X.columns, columns=['importance'])\n\n# plt.figure()\nimportance.sort_values(by='importance', ascending=False).plot.bar(figsize=(20, 8))","ccc94331":"del _X_scaled\ndel _\ndel _X\ndel _y","d835391c":"data_important_cols = merge_copy.copy().drop(['id', 'date', 'dcoilwtico', 'holiday_flag', 'month', 'day', 'city', 'stores_type'], axis=1)\ndata_important_cols['family_BEVERAGES'] = data_important_cols['family'].apply(lambda x: 1 if x =='BEVERAGES' else 0)\ndata_important_cols['family_CLEANING'] = data_important_cols['family'].apply(lambda x: 1 if x =='CLEANING' else 0)\ndata_important_cols['family_DAIRY'] = data_important_cols['family'].apply(lambda x: 1 if x =='DAIRY' else 0)\ndata_important_cols['family_GROCERY'] = data_important_cols['family'].apply(lambda x: 1 if x =='GROCERY I' else 0)\ndata_important_cols['family_PRODUCE'] = data_important_cols['family'].apply(lambda x: 1 if x =='PRODUCE' else 0)\ndata_important_cols = data_important_cols.drop(['family'], axis=1)\n\ndescribe(data_important_cols)","b8ab3582":"X = data_important_cols.drop('sales', axis=1)\ny = data_important_cols['sales']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=123)","9ddb4cbf":"# # prepare the metrics list\n# pipe_pred = []\n# rf_pred = []\n# gbm_pred = []\n\n# # prepare the metrics list\n# pipe_true = []\n# rf_true = []\n# gbm_true = []\n\n# # pipeline\n# model1 = Pipeline([('pca', PCA(n_components=10)), ('lr', LinearRegression())])\n# # random forest\n# model2 = RandomForestRegressor(n_estimators=50, random_state=123, max_leaf_nodes=100, max_depth=30)\n# # lightgbm\n# params = {'metric' : 'rmse', 'seed': 123, 'verbosity':200}\n# model3 = lgb\n\n# kfold = KFold(n_splits=5).split(X_train, y_train)\n\n# for (train, val) in kfold:\n#     X_train_cv = X_train[train]\n#     y_train_cv = y_train.iloc[train]\n#     X_val_cv = X_train[val]\n#     y_val_cv = y_train.iloc[val]\n    \n#     lgb_train = lgb.Dataset(X_train_cv, y_train_cv)\n#     lgb_val = lgb.Dataset(X_val_cv, y_val_cv)\n    \n#     # train\n#     model1.fit(X_train_cv, y_train_cv)\n#     model2.fit(X_train_cv, y_train_cv)\n#     model_3 = model3.train(params, lgb_train, num_boost_round=200)\n    \n#     y_cv_pred1 = model1.predict(X_val_cv)\n#     y_cv_pred2 = model2.predict(X_val_cv)\n#     y_cv_pred3 = model_3.predict(X_val_cv, num_iteration=gbm.best_iteration)\n    \n#     pipe_pred.append(y_cv_pred1)\n#     rf_pred.append(y_cv_pred2)\n#     gbm_pred.append(y_cv_pred3)\n    \n#     # append y data\n#     pipe_true.append(y_val_cv.values)\n#     rf_true.append(y_val_cv.values)\n#     gbm_true.append(y_val_cv.values)","10e077e8":"# pipe_pred = np.concatenate(pipe_pred)\n# rf_pred = np.concatenate(rf_pred)\n# gbm_pred = np.concatenate(gbm_pred)\n# pipe_true = np.concatenate(pipe_true)\n\n# df = pd.DataFrame({'true': pipe_true, 'pipe': pipe_pred, 'rf': rf_pred, 'lgb': gbm_pred})","954157ed":"# X_stack = df.drop('true', axis=1)\n# y_stack = df['true']\n\n# X_stack_train, X_stack_test, y_stack_train, y_stack_test = train_test_split(X_stack, y_stack, test_size=0.2, random_state=123)\n\n# meta_model = LinearRegression()\n# meta_model.fit(X_stack_train, y_stack_train)\n\n# meta_val_pred = meta_model.predict(X_stack_test)\n# print (\"stacking model: {:.4f}\".format(np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred))))","7af4739d":"col = importance.query('importance>100').index\n\ntest_copy = test.copy()\n# merge data\ntest_copy = test_copy.merge(oil, on='date', how='left')\ntest_copy = test_copy.merge(holidays_events, on='date', how='left')\ntest_copy = test_copy.merge(stores, on='store_nbr', how='left')\ntest_copy = test_copy.merge(transactions, on=['date', 'store_nbr'], how='left')\n# change dtype and get the date col\ntest_copy['date'] = pd.to_datetime(test_copy['date']).dt.date\ntest_copy['year'] = pd.to_datetime(test_copy['date']).dt.year\ntest_copy['month'] = pd.to_datetime(test_copy['date']).dt.month\ntest_copy['day'] = pd.to_datetime(test_copy['date']).dt.day\n# fillna with 0\ntest_copy['transactions'] = test_copy['transactions'].fillna(0)\ntest_copy['dcoilwtico'] = test_copy['dcoilwtico'].fillna(0)\n# create new col as I did above\ntest_copy['holiday_flag'] = [1 if not val else 0 for val in test_copy['type_x'].isnull()]\n# test_copy = merge_copy.drop(['type_x', 'locale_name', 'transferred'], axis=1)\ntest_copy = test_copy.rename(columns={'type_y': 'stores_type'})\n\ntest_copy = test_copy.drop(['id', 'date'], axis=1)\ntest_copy = pd.get_dummies(test_copy, drop_first=True)","41c5874e":"describe(test_copy)","a62847b9":"col = importance.query('importance>0').index\nX2 = data[col]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X2)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=12)\n\n# best param!\nparams = {\n    'metric' : 'rmse', \n    'verbosity': -1, \n    'seed': 123,\n    'boosting_type': 'gbdt',\n    'num_leaves': study.best_params['num_leaves'], \n    'learning_rate': study.best_params['learning_rate']\n}\n\n# Preparing dataset for LightGBM\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test)\n# train data\ngbm = lgb.train(params, lgb_train, num_boost_round=500, valid_sets=[lgb_test], verbose_eval=False)\n\n\n# choose importance cols\ntest_copy = test_copy[col]\nscaler = StandardScaler()\ntest_scaled = scaler.fit_transform(test_copy)\nprediction= gbm.predict(test_scaled, num_iteration=gbm.best_iteration)\n\ndel X2\ndel lgb_train\ndel lgb_test","e9b4af31":"submission = pd.read_csv('\/kaggle\/input\/store-sales-time-series-forecasting\/sample_submission.csv')\n\nsubmission['sales'] = [pred if pred >= 0 else 0 for pred in prediction]\nsubmission.to_csv('submission.csv', index=False)\nsubmission = pd.read_csv(\"submission.csv\")\nsubmission","c9748211":"<div style=\"background-color:lightblue; padding:15px;\">\n    <h2>Let's move to analysis. This time we try below aprroaches<\/h2>\n    <ol>\n        <h3><li><span style=\"color:red;\">Simple LinearRegression<\/span>: <br>we need to think whether simple model works well or not before randomforest, lightgbm or some complicated and high model. <br>If it works, we can get the shortcut and can introduce practically and simply!<\/li><\/h3>\n    <h3><li><span style=\"color:red;\">Use Pipeline<\/span>: <br>Next, we use pipeline including pca to cut down dimensions then we figure out what eigens say about.<\/li><\/h3>\n        <h3><li><span style=\"color:red;\">Use RFE and get important cols<\/span>: <br>RFE can tell us what cols are important.<\/li><\/h3>\n        <h3><li><span style=\"color:red;\">RandomForest<\/span>: <br>Use RandomForest and this feature_importance tell us how important cols are like RFE!!<\/li><\/h3>\n        <h3><li><span style=\"color:red;\">LightGBM<\/span>: <br>lightBGM can analyze quickly and precisely! It can be really good model but I feel this model doesn't talk to me, haha<\/li><\/h3>\n        <h3><li><span style=\"color:red;\">Stacking<\/span>: <br>lightBGM can analyze quickly and precisely! It can be really good model but I feel this model doesn't talk to me, haha<\/li><\/h3>\n    <\/ol>\n\n<\/div>","ec3377a4":"# Import modules","5a271111":"# Check the feature importance","ff57f198":"<div style=\"background-color:lightgreen; padding:10px; text-align:center;\">\n<h2>Comparing to 1st try, the result is close to 1st result. \n    <br>This means that it's enough to use only 'important_cols'!!\n<\/h2>\n<\/div> ","1644f4d5":"December is higher than others.","8b51c0c1":"#### Dec is so important for sale but data of 2017 doesn't exist.\n#### So, I think 'month' cols can be removed.","c9bde9b9":"## ~~","5a83fe95":"## check moving avg\n### we seek how trend 'sales' moved.","e2714523":"## Last, stacking model\n### before this, we remake the data which has only importance cols","35fef0a5":"It's increasing roughly as year goes by.","e9a34e6d":"### Prepare the data","2ab5960c":"# Adjust test data for lightgbm and submission","e62f92fd":"<div style=\"background-color:lightgray; padding:5px;\">\n<h2>2nd Use Pipeline<\/h2>\n<\/div>","954f5521":"## Check each term and how distributions are!","4aa91970":"<div style=\"background-color:lightgreen; padding:10px; text-align:center;\">\n<h3>As we estimate, onpromotion looks important!\n<\/h3>\n<\/div>","8ba90fa9":"<div style=\"background-color:lightgray; padding:5px;\">\n<h2>4th RandomForest<\/h2>\n<\/div>","29e3acba":"## below code I'm fixing~~","ecfb0cb6":"### enough to use only 'locale_name' instead of using with 'locale' becuase locale_name covers locale\n#### ex: Santo Domingo must be 'local', Ecuador must be 'National' etc.","825b3cbf":"## Optuna","27af69f6":"<div style=\"background-color:lightblue;padding:10px;text-align:center\">\n    <h3>number of family and store_nbr's ingredients are the same!<\/h3>\n<\/div>","2d5830ea":"# holidays_events","88ce497d":"# 1st, EDA\n\n## We need to explore df and try to figure out the key that data have! \n\n# Train","412c7a6f":"<div style=\"background-color:lightgray; padding:5px;\">\n<h2>1st Simple LinearRegression<\/h2>\n<\/div>","4c9a86d0":"<div style=\"background-color:lightgreen; padding:10px;\">\n    <h2>We get to know what the features are helpful to analyze better,<br> according to RFE and RandomForest!!!<\/h2>\n    <h3>We cut down the uselesss col then we try linearReg<\/h3>\n<\/div>","adfbe53b":"# data merge","3c6f0f83":"### above this, it's enough to use only 'city' instead of 'state'","e4106cce":"<div style=\"background-color:lightblue;padding:18px;text-align:center\">\n    <h3> we found sales col have outliers but <span style=\"color:red\">this case the data can be happend<\/span> as real data.<\/h3>\n    <h3> I don't clean any outliers in train <\/h3>\n<\/div>","61055b5d":"### Check the Correlation","ddbc5fe0":"# stores","5c7bf5a3":"# oil","6c4a7b73":"# transactions","0819bfc6":"## 5th lightGBM","cb3fb8a4":"#### PCA seemed to  over-cut down. so this case it doesn't help.","b05d7f64":"<div style=\"background-color:lightgreen; padding:18px\">\n<h3>I fill null with 0! <br>\n    we usually fill null with some represent value as one of the approaches. \n    <br><br>\n    but in this case, I think the null means no transaction and unofficial data. <br>\n    Hence filling some values will lead to be misunderstood.\n<\/h3>\n<\/div>","169e0f48":"<div style=\"background-color:lightgreen; padding:10px; text-align:center;\">\n<h4>I fill null with 0 in transaction, too!\n<\/h4>\n<\/div>","8aec6c56":"<div style=\"background-color:lightgreen; padding:8px; text-align:center;\">\n    <h4>Exciting result!<\/h4>\n    <h3>this model is pretty good for now. <br>However, I want more acc so try to use 'optuna' to tune hyperparams!<\/h3>\n<\/div> ","0b3f84e2":"<div style=\"background-color:lightgray; padding:5px;\">\n<h2>3rd Use RFE and get important cols<\/h2>\n<\/div>\n\n### We figure out top 10 important columns!\n(I choose 10, but it's fine to use no matter how many number you want)","604607d2":"### Sales at the beginning and end of month tends to increase!!","e89de6d9":"#### id- year is strong, sales-onpro is also strong correlation!\n#### so onpromotion may be key to analysis??","c016e673":"### This holiday null are huge so I will recreate holiday col that flag 1: the day is holiday, 0: not holiday\n\n### We needn't care about what holiday is.","83045703":"<div style=\"background-color:lightgreen; padding:10px; text-align:center;\">\n    <h1>Conclusion<\/h1>\n    <h2>I tried EDA, Viz, and some methods.\n        If you want to try more, Let's try!!\n    <\/h2>\n<\/h2>\n<h3 style=\"color:red\">Thank you for visiting my notebook. Feel free to upvotes or comment if you like mine!!<\/h3>\n<\/div> ","eef0d0d8":"# Analysis the data!","1ae8cc77":"## I think this model is somethins wrong, I'll fix this model so I use lightgbm this submittion for now"}}