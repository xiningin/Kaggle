{"cell_type":{"45579a09":"code","fc27783a":"code","013c4211":"code","1a2c8878":"code","e677b5e0":"code","b9fcce65":"code","124228b0":"code","8bcc99a4":"code","1193cb1d":"code","6e827074":"code","8a60eae2":"code","5517cf6d":"code","90f8c320":"code","deafb814":"code","f3967577":"code","baee3e7e":"code","e1d4877d":"code","a2f8ba83":"code","c7523370":"code","923d8ae3":"code","90413dbb":"code","aa7df230":"code","e22ddce3":"code","68256d46":"code","006a0c12":"code","8ba9e8e1":"code","631870f3":"code","c0dfaa1d":"code","bd1f81c2":"code","8d50f97c":"code","714d5b4d":"code","968c90b2":"code","a432e01b":"code","5197428c":"code","503a2f35":"code","5aba2541":"code","c5790393":"code","371536a5":"code","3ef758bd":"code","f2ec52a7":"code","2d08230a":"code","9a5bf667":"code","eb2c9d77":"code","687b46fe":"code","7584e16b":"code","2b0bb2af":"code","00255b97":"code","b8c79686":"code","e8295101":"code","507c7d32":"code","3c25f586":"code","1ef4376a":"code","16acc0c0":"code","8ad4a527":"code","9ced8c68":"code","947d41ba":"code","665e2df6":"code","81b4927d":"code","01df2b25":"code","8f9a9a18":"code","f59d3775":"code","b48b5404":"code","ee23a9ba":"code","97d72a8f":"code","a779231c":"code","4c77f264":"markdown","2c4ce1f5":"markdown","7dd36806":"markdown","6a319d15":"markdown","1c667ecb":"markdown","f8fcb0e0":"markdown","12a5ac3c":"markdown","874e6872":"markdown","888324c9":"markdown","8edfe798":"markdown","85056397":"markdown","c3f5285c":"markdown","df6a3ba3":"markdown","3f6fe8fc":"markdown","3eb1abd2":"markdown","81a66587":"markdown","e1baa1fc":"markdown","7745d17e":"markdown","2bf8ad12":"markdown","eee26750":"markdown","6ee579e2":"markdown","53e565cd":"markdown","9669810a":"markdown","e3618d35":"markdown","ea25827e":"markdown","0c522d8b":"markdown","a93a2958":"markdown","a163a645":"markdown"},"source":{"45579a09":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sys, os\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport random\nfrom math import exp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix","fc27783a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","013c4211":"input_data_dir = \"..\/input\/titanic\/\"\ntrain_df = pd.read_csv(os.path.join(input_data_dir, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(input_data_dir, \"test.csv\"))","1a2c8878":"print(\"Size of training dataset       : {}\".format(train_df.shape))\nprint(\"Size of test dataset           : {}\".format(test_df.shape))","e677b5e0":"train_df.info()","b9fcce65":"train_df.describe().T","124228b0":"test_df.info()","8bcc99a4":"test_df.describe().T","1193cb1d":"test_df_PassengerId = test_df.PassengerId","6e827074":"plt.figure(figsize=(15, 20))\nsns.heatmap(train_df.isnull(), cbar=False)        #plotting heatmap using sns library to find missing values in train_df\nplt.show()","8a60eae2":"train_df.isna().sum()                        # Printing a count of missing value w.r.t each feature in train_df","5517cf6d":"test_df.isna().sum()                        # Printing a count of missing value w.r.t each feature in test_df","90f8c320":"train_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","deafb814":"train_df = train_df.drop(['PassengerId','Name','Ticket'], axis=1)   # Dropping unuseful features for prediction.\ntest_df = test_df.drop(['PassengerId','Name','Ticket'], axis=1)","f3967577":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)   # Since it contains discrete values.\ntrain_df['Embarked'].isna().sum()                                   # Prints the remaining missing values in 'Embarked' feature.","baee3e7e":"train_df['Age'].fillna(train_df['Age'].mean(),inplace=True)   # Since it contains continuous values.\ntrain_df['Age'].isna().sum()                                   # Prints the remaining missing values in 'Age' feature.","e1d4877d":"test_df['Age'].fillna(test_df['Age'].mean(),inplace=True)   # Since it contains continuous values.\ntest_df['Age'].isna().sum()                                   # Prints the remaining missing values in 'Age' feature.","a2f8ba83":"test_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)   # Since it contains continuous values.\ntest_df['Fare'].isna().sum()                                   # Prints the remaining missing values in 'Fare' feature.","c7523370":"# There are two Features in our data which we are going to encode.\ntrain_df_encode = train_df[['Sex','Embarked']]\ntest_df_encode = test_df[['Sex','Embarked']]\ntrain_df_encode.head()","923d8ae3":"# Features which we are not going to encode.\ntrain_df_not_encode = train_df.drop(['Sex','Embarked'], axis=1)\ntest_df_not_encode = test_df.drop(['Sex','Embarked'], axis=1)\ntrain_df_not_encode.head()","90413dbb":"le = LabelEncoder()            # Using Label Encoder to encode features that are having data type as object in training data.\nfor i in train_df_encode:\n    train_df_encode[i]=le.fit_transform(train_df_encode[i])","aa7df230":"for j in test_df_encode:        # Using Label Encoder to encode features that are having data type as object in testing data.\n    test_df_encode[j]=le.fit_transform(test_df_encode[j])","e22ddce3":"train_df_encode.head()","68256d46":"train_df = pd.concat([train_df_encode, train_df_not_encode], axis=1)\ntest_df = pd.concat([test_df_encode, test_df_not_encode], axis=1)","006a0c12":"train_df.head()","8ba9e8e1":"train_df.info()","631870f3":"train_df.describe().T","c0dfaa1d":"test_df.head()","bd1f81c2":"test_df.info()","8d50f97c":"test_df.describe().T","714d5b4d":"train_df.hist(bins = 60, figsize = (20,17), color='magenta')","968c90b2":"plt.figure(figsize=(8,5))\nsns.countplot(x='Survived', data=train_df, order=[0, 1] )","a432e01b":"a = sns.countplot(y='Survived',hue='Sex', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Sex')\nplt.ylabel('Survived')\nplt.show()","5197428c":"b = sns.countplot(y='Survived',hue='Embarked', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Embarked')\nplt.ylabel('Survived')\nplt.show()","503a2f35":"c = sns.countplot(y='Survived',hue='Pclass', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Pclass')\nplt.ylabel('Survived')\nplt.show()","5aba2541":"d = sns.countplot(y='Survived',hue='SibSp', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='SibSp')\nplt.ylabel('Survived')\nplt.show()","c5790393":"e = sns.countplot(y='Survived',hue='Parch', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Parch')\nplt.ylabel('Survived')\nplt.show()","371536a5":"fig, ax = plt.subplots(figsize=(10, 8))  \nsns.violinplot(x='Survived', y='Age', data=train_df, ax=ax)  \nax.set_title('Age Vs Survived')  \nplt.show()  ","3ef758bd":"# Using Strip plot to visualize the Age feature impact on Survived.  \nfig, ax= plt.subplots(figsize=(10, 8))  \nsns.stripplot(train_df['Survived'], train_df['Age'], jitter=True, ax=ax)  \nax.set_title('Age Vs Survived')  \nplt.show() ","f2ec52a7":"fig, ax = plt.subplots(figsize=(10, 8))  \nsns.violinplot(x='Survived', y='Fare', data=train_df, ax=ax)  \nax.set_title('Fare Vs Survived')  \nplt.show()","2d08230a":"# Using Strip plot to visualize the Fare feature impact on Survived.\nfig, ax= plt.subplots(figsize=(10, 8))  \nsns.stripplot(train_df['Survived'], train_df['Fare'], jitter=True, ax=ax)  \nax.set_title('Fare Vs Survived')  \nplt.show() ","9a5bf667":"corr_data = train_df.corr()                       # calculating correlation data between features\nplt.figure(figsize=(19, 17))                      # setting figure size\nsns.set_style('ticks')                            # setting plot style\nsns.heatmap(corr_data, cmap='viridis',annot=True)                # plotting heatmap using sns library\nplt.show()","eb2c9d77":"corr_data.Survived.apply(lambda x: abs(x)).sort_values(ascending=False).iloc[1:8][::-1].plot(kind='barh',color='purple')  # calculating top correlated faetures\n                                                                                                                           # with respect to target variable i.e. \"Survived\"\nplt.title(\"Top Correlated Features\", size=20, pad=26)\nplt.xlabel(\"Correlation coefficient\")\nplt.ylabel(\"Features\")","687b46fe":"train_df_X = train_df[['Sex',\t'Embarked',\t'Pclass',\t'Age',\t'SibSp',\t'Parch',\t'Fare']]\ntrain_df_X.head(n=5)","7584e16b":"train_df_y = train_df.Survived\ntrain_df_y.head()","2b0bb2af":"# Splitting selected_train_df into 70% and 30% to construct training data and Validation data respectively.\ntrainX, valX, trainy, valy = train_test_split(train_df_X, train_df_y,test_size=0.3, random_state=12) ","00255b97":"trainX.shape","b8c79686":"trainy.shape","e8295101":"valX.shape","507c7d32":"valy.shape","3c25f586":"#Creating a Logistic Regression Classifier\nLogisticRegression_Model = LogisticRegression(penalty='l2',solver='newton-cg')\n#Train the model using the training sets\nLogisticRegression_Model.fit(trainX, trainy)","1ef4376a":"#Creating a svm Classifier\nSVM_Model = svm.SVC(kernel='linear', probability=True) # Linear Kernel\n#Train the model using the training sets\nSVM_Model.fit(trainX, trainy)","16acc0c0":"lr_list = [0.5, 0.6, 0.7, 0.71, 0.75, 0.9, 1]\n\nfor learning_rate in lr_list:\n    GBM_Model = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    GBM_Model.fit(trainX, trainy)\n\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (Training): {0:.3f}\".format(GBM_Model.score(trainX, trainy)))\n    print(\"Accuracy score (Validation): {0:.3f}\".format(GBM_Model.score(valX, valy)))   ","8ad4a527":"#Creating a GBM Classifier\nGBM_Model = GradientBoostingClassifier(n_estimators=20, learning_rate=0.71, max_features=2, max_depth=2, random_state=0)\n#Train the model using the training sets\nGBM_Model.fit(trainX, trainy)","9ced8c68":"#Creating a Gaussian Classifier\nNB_Model = GaussianNB()\n# Train the model using the training sets\nNB_Model.fit(trainX, trainy)","947d41ba":"#Creating a Decision Tree Classifier\nDT_Model = DecisionTreeClassifier(criterion = \"entropy\",splitter = \"best\", random_state = 100, max_depth=3, min_samples_leaf=5)  \n# Train the model using the training sets\nDT_Model.fit(trainX, trainy) ","665e2df6":"LogisticRegression_predictions = LogisticRegression_Model.predict(valX)\nSVM_predictions = SVM_Model.predict(valX)\nNB_predictions = NB_Model.predict(valX)\nDT_predictions = DT_Model.predict(valX)\nGBM_predictions = GBM_Model.predict(valX)","81b4927d":"print(\"Logistic Regression_Confusion Matrix:\")\nprint(confusion_matrix(valy, LogisticRegression_predictions))\n\nprint(\"Logistic Regression_predictions_Classification Report\")\nprint(classification_report(valy, LogisticRegression_predictions))","01df2b25":"print(\"SVM_Confusion Matrix:\")\nprint(confusion_matrix(valy, SVM_predictions))\n\nprint(\"SVM_Classification Report\")\nprint(classification_report(valy, SVM_predictions))","8f9a9a18":"print(\"Naive Bayes Confusion Matrix:\")\nprint(confusion_matrix(valy, NB_predictions))\nprint(\"Naive Bayes Classification Report\")\nprint(classification_report(valy, NB_predictions))","f59d3775":"print(\"DT_Confusion Matrix:\")\nprint(confusion_matrix(valy, DT_predictions))\n\nprint(\"DT_Classification Report\")\nprint(classification_report(valy, DT_predictions))","b48b5404":"print(\"GBM_Confusion Matrix:\")\nprint(confusion_matrix(valy, GBM_predictions))\n\nprint(\"GBM_Classification Report\")\nprint(classification_report(valy, GBM_predictions))","ee23a9ba":"GBM_predictions_On_Test_Data = GBM_Model.predict(test_df)","97d72a8f":"Output_DF = pd.DataFrame({'PassengerId':test_df_PassengerId,'Survived':GBM_predictions_On_Test_Data})","a779231c":"#Save to csv\nOutput_DF.to_csv('Titanic_pred.csv',index=False)\nOutput_DF.head()","4c77f264":"## **Data Description**","2c4ce1f5":"# **Importing Some Basic Libraries**","7dd36806":"**As seen above, there is one Independent Feature(i.e. Cabin) having more than 75%  of the total values are missing values. So it is illogical to fill Missing Values for this feature.**\n","6a319d15":"**Since, out of 5 Different Models GBM_Model(i.e. Gradient Boosting Machine) provides the best Accuracy(i.e. 82%). Therefore, we perform prediction on Test Data Using GBM_Model.**","1c667ecb":"### **Testing Data**","f8fcb0e0":"**Using Above Strip Plot and Violin Plot for Age Vs Survived , we can easily seen that:**\n1.   **Those People which having a Age value in between 60 to 80 are mostly died.**\n2.   **Youngsters Which having a Age value in the range of 20 to 40 are mostly died.**\n","12a5ac3c":"**Hence, We are going to drop this feature from our training dataset as well as testing data**","874e6872":"# **Importing Data**","888324c9":"\n*   Importing Some Basic Libraries\n*   Importing Data\n*   Performing Descriptive Analysis on the dataset to know data better before Pre-processing\n*   Checking null values\n*   Doing Pre-processing\n*   Handling missing values\n*   Processing Categorical Values by Performing Label Encoding on it\n*   Checking Data Description After Pre-Processing\n*   Plotting the Histogram\n*   Analysis of Target Variable using different count plot w.r.t to different independent features. Also, by using Strip and Violin Plot in b\/w Age and Survived and in b\/w Fare and Survived\n*   Plotting Correlation Matrix and Heat Map\n*   Splitting train_df into 70% and 30% to construct training data and validation data respectively\n*   Implements 5 models which are Logistic Regression, GBM, SVM, DT, Naive Bayes\n*   Performing Prediction on Validation Data\n*   Evaluating Model based on Confusion Matrix and Classification Report for each model\n*   Save predictions on Testing data in .csv format\n\n\n\n\n\n\n\n\n\n\n\n","8edfe798":"# **Descriptive Analysis of the dataset**","85056397":"## **NULL VALUES**","c3f5285c":"# **Methodology**","df6a3ba3":"### **Testing Data:**","3f6fe8fc":"# **Evaluation**","3eb1abd2":"**I plotted the histogram to check the distribution of a sample of Training data.**","81a66587":"**Perform Prediction on Validation Data:**","e1baa1fc":"### **Training Data:**","7745d17e":"**Using Above Strip Plot and Violin Plot for Fare Vs Survived , we can easily seen that:**\n\n\n1.   **Those People which having a very costlier ticket(around 500) are mostly survived.**\n2.   **Those People which having a very cheaper ticket(around 0) are mostly died.**\n\n","2bf8ad12":"\n\n# **Predictions on Test Data:**","eee26750":"### **Training Data**","6ee579e2":"Colab Link For same Notebook:\nhttps:\/\/colab.research.google.com\/drive\/16PzOiBXX5ay89N_nq9jT7ofbIbiOIJHd?usp=sharing","53e565cd":"# **Analysis of Target Variable**","9669810a":"## **`Correlation Matrix and Heat Map`**\n\n","e3618d35":"# **Data Description After Pre-Processing**","ea25827e":"**Thank you**,<br>\nNikunj Bansal,<br>\nR177218063,<br>\nB2 Batch<br>","0c522d8b":"## **Handling Missing Values:**","a93a2958":"# **Pre-Processing**","a163a645":"## **Label Encoding On Categorical Features:**"}}