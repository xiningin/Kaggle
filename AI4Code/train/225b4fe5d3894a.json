{"cell_type":{"fb9a3292":"code","a296145a":"code","f1d0fd41":"code","92036534":"code","3513c3ba":"code","f438374c":"code","56ce0006":"code","c2642962":"code","0c5b66e0":"code","45fd82ec":"code","dccca9c6":"code","26c8a3fe":"code","29f1c6d3":"code","0d866d0d":"code","7b92ed5e":"code","e59c8e8f":"code","d6826ee0":"code","e241bd6f":"code","492167e1":"code","b90073ce":"code","81095eaf":"code","2331aafe":"code","ad2b346b":"code","ebeaf2ff":"code","c7659ef4":"code","f1469e3a":"code","c4b0082a":"code","ce8815cf":"code","b8a89337":"code","4701df14":"code","f1088e6b":"code","8b403800":"code","a12e20e7":"code","66c01464":"code","d87a9039":"code","ba74a192":"code","9e35e01b":"code","452f9bca":"code","c5a5833a":"code","80bd9365":"code","7a7d8325":"code","0dd79ada":"code","9cd70df4":"code","72c2ae01":"code","b3e8b4a8":"code","fcaf0d02":"code","1a24ae92":"code","7d6c3a21":"code","69115529":"code","ffbede99":"code","4be5f2a6":"code","428e2828":"code","25284c66":"code","70341085":"code","2045bd6e":"code","70ac3fd2":"code","ddf36d99":"code","526da14f":"code","cac899c6":"code","01f01d97":"code","d2fdfc17":"code","84373c24":"code","6f518723":"code","e5a2c620":"code","6d6d50e6":"code","4924f00b":"code","b95468e6":"code","87e08012":"code","ac9e4174":"markdown","908d78d5":"markdown","3d3126a9":"markdown","e2ec9dee":"markdown","ebf8a9e3":"markdown","a4e41f32":"markdown","e936bec2":"markdown","4c19e4fd":"markdown","d439f300":"markdown","3eed4b41":"markdown","c4c5a006":"markdown","5413da1a":"markdown","46504b74":"markdown","049d8f0d":"markdown","9a34fe58":"markdown","cf6fcbb5":"markdown","392881c2":"markdown","c2c062fb":"markdown","f0d49237":"markdown","2a0094f8":"markdown","51dd778e":"markdown","b4c3fdd7":"markdown","1316a794":"markdown","dce5bdd6":"markdown","8f2cc690":"markdown","2d29b43c":"markdown","b4883fc8":"markdown","393d5ab2":"markdown","c9eb3e70":"markdown"},"source":{"fb9a3292":"# In book a function is defined to download data from Url and auto-extract it using tgz\n# but since we are using data directly from kaggle it is not required\n\nimport pandas as pd\nhousing = pd.read_csv(\"..\/input\/california-housing-prices\/housing.csv\")","a296145a":"#housing = load_housing_data()\nhousing.head()","f1d0fd41":"housing.info()","92036534":"housing[\"ocean_proximity\"].value_counts()","3513c3ba":"housing.describe() #all null values ignored","f438374c":"#creating plots on dataset\n%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50,figsize=(20,15))\nplt.show()","56ce0006":"\"\"\"\nCreating shuffled testset with constant values in training and updated dataset values going to \ntest set in case dataset is updated, this done via hashlib\n\"\"\"\nimport hashlib\nimport numpy as np\n\ndef test_set_check(identifier,test_ratio,hash):\n    return hash(np.int64(identifier)).digest()[-1]<256*test_ratio\n    \ndef split_train_test(data,test_ratio,id_column,hash=hashlib.md5):\n    ids=data[id_column]\n    in_test_set=ids.apply(lambda id_:test_set_check(id_,test_ratio,hash))\n    return data.loc[~in_test_set],data.loc[in_test_set]","c2642962":"#combining latitude and longitude as new column id\n#housing_with_id[\"id\"]=housing[\"longitude\"]*1000+housing[\"latitude\"]\n#train_set1,test_set1 = split_train_test(housing_with_id,0.2,\"id\")","0c5b66e0":"#housing_with_id.head()","45fd82ec":"# or we can use sklearn function \n#from sklearn.model_selection import train_test_split\n#train_set,test_set = train_test_split(housing,test_size=0.2,random_state=42)","dccca9c6":"#understanding stratification\nhousing[\"median_income\"].hist(bins=40)","26c8a3fe":"#creating hosusing income categories\nhousing[\"income_cat\"]=np.ceil(housing[\"median_income\"]\/1.5)\nhousing[\"income_cat\"]=housing[\"income_cat\"].apply(lambda x: 5 if x>5 else x)","29f1c6d3":"housing[\"income_cat\"].hist(bins=40)","0d866d0d":"#startified split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit= StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_idx,test_idx in split.split(housing,housing[\"income_cat\"]):\n    strat_train_set=housing.loc[train_idx]\n    strat_test_set=housing.loc[test_idx]","7b92ed5e":"#dropping income category from test and train splits\na= (strat_train_set,strat_test_set)","e59c8e8f":"for i in a:\n    i.drop([\"income_cat\"],axis=1,inplace=True)","d6826ee0":"data =strat_test_set.copy()\ndata.head()","e241bd6f":"# since there are latitude and longitudes, its good idea to have a scatter plot\n#set alpha =0.1 to clearly see dense points\ndata.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.1)","492167e1":"#advanced scatter plot using median value of house\ndata.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.4,\n         s=data[\"population\"]\/100,label=\"population\",\n         c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"),\n         colorbar=True)\nplt.legend()","b90073ce":"# Calculate pearson's r coefficient\ncorr_matrix=data.corr()\ncorr_matrix","81095eaf":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","2331aafe":"#scatter matrix fom pandas\n\nfrom pandas.plotting import scatter_matrix\nattributes=[\"median_house_value\",\"median_income\",\"total_rooms\",\"housing_median_age\"]\nscatter_matrix(data[attributes],figsize=(12,8))","ad2b346b":"#exploring more on median income\ndata.plot(kind=\"scatter\",x=\"median_income\",y=\"median_house_value\",alpha=0.1)","ebeaf2ff":"data[\"rooms_per_household\"]=data[\"total_rooms\"]\/data[\"households\"]\ndata[\"bedrooms_per_room\"]=data[\"total_bedrooms\"]\/data[\"total_rooms\"]\ndata[\"population_per_household\"]=data[\"population\"]\/data[\"households\"]","c7659ef4":"#lets check co-relation matrix again\ncorr_matrix=data.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","f1469e3a":"housing=strat_train_set.drop(\"median_house_value\",axis=1)\nhousing_labels=strat_train_set[\"median_house_value\"].copy()","c4b0082a":"# we will use SimpleImputer from sklearn.impute\nfrom sklearn.impute import SimpleImputer\nimputer=SimpleImputer(strategy=\"median\")","ce8815cf":"# since imputer only works on numerical features we ll create a copy of data with only numerical features\nhousing_num=housing.drop(\"ocean_proximity\",axis=1)\nimputer.fit(housing_num)","b8a89337":"# you can view these values\nimputer.statistics_","4701df14":"# use this imputer to transform\nX=imputer.transform(housing_num)","f1088e6b":"housing_tr=pd.DataFrame(X,columns=housing_num.columns)","8b403800":"housing_tr.isnull().sum()","a12e20e7":"from sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\nhousing_cat=housing[\"ocean_proximity\"]\nhousing_cat_encoded=encoder.fit_transform(housing_cat)\nhousing_cat_encoded","66c01464":"encoder.classes_","d87a9039":"from sklearn.preprocessing import OneHotEncoder\nencoder=OneHotEncoder()\nhousing_cat_1hot=encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\nhousing_cat_1hot","ba74a192":"#converting the sparse matrix to array\nhousing_cat_1hot.toarray()","9e35e01b":"from sklearn.base import BaseEstimator,TransformerMixin\n\nrooms_ix,bedrooms_ix,population_ix,household_ix=3,4,5,6\n\nclass FeatureAdder(BaseEstimator, TransformerMixin):\n    def __init__(self,add_bedrooms_per_room=True):\n        self.add_bedrooms_per_room=add_bedrooms_per_room\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X,y=None):\n        rooms_per_household=X[:,rooms_ix]\/X[:,household_ix]\n        population_per_household=X[:,population_ix]\/X[:,household_ix]\n        \n        if self.add_bedrooms_per_room:\n            bedrooms_per_room=X[:,bedrooms_ix]\/X[:,rooms_ix]\n            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]\n        else:\n            return np.c_[X,rooms_per_household,population_per_household]","452f9bca":"#lets instantiate our object\nadder= FeatureAdder(add_bedrooms_per_room=False)\nhousing_extra_features =adder.fit_transform(housing.values)","c5a5833a":"from sklearn.preprocessing import FunctionTransformer\n\nrooms_ix,bedrooms_ix,population_ix,household_ix=3,4,5,6\n\ndef extra_features(X,add_bedrooms_per_room=True):\n    rooms_per_household=X[:,rooms_ix]\/X[:,household_ix]\n    population_per_household=X[:,population_ix]\/X[:,household_ix]\n    if add_bedrooms_per_room:\n        bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n        return np.c_[X, rooms_per_household, population_per_household,\n                     bedrooms_per_room]\n    else:\n        return np.c_[X, rooms_per_household, population_per_household]","80bd9365":"feature_adder =FunctionTransformer(extra_features,validate=False,\n                                  kw_args={\"add_bedrooms_per_room\":False})\nhousing_extra_features =feature_adder.fit_transform(housing.values)\n\nhousing_extra_feat = pd.DataFrame(\n    housing_extra_features,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_feat.head()","7a7d8325":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs=list(housing_num)\ncat_attribs=[\"ocean_proximity\"]\n\nnum_pipeline=Pipeline([\n    (\"imputer\",SimpleImputer(strategy=\"median\")),\n    (\"feature_adder\",FeatureAdder()),\n    (\"std_scaler\",StandardScaler()),\n])\n\nfull_pipeline=ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs),\n])","0dd79ada":"housing_prepared = full_pipeline.fit_transform(housing)","9cd70df4":"from sklearn.linear_model import LinearRegression\nlin_reg=LinearRegression()\nlin_reg.fit(housing_prepared,housing_labels)","72c2ae01":"some_data=housing.iloc[:5]\nsome_data","b3e8b4a8":"housing_labels.iloc[:5]","fcaf0d02":"some_prepared_data = full_pipeline.transform(some_data)","1a24ae92":"lin_reg.predict(some_prepared_data)","7d6c3a21":"#calculate mean squared error\nfrom sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse= mean_squared_error(housing_labels,housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","69115529":"# try another model\n\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg=DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","ffbede99":"housing_predictions=tree_reg.predict(housing_prepared)\ntree_mse= mean_squared_error(housing_labels,housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","4be5f2a6":"# using cross_val_score\n\nfrom sklearn.model_selection import cross_val_score\nscores=cross_val_score(tree_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\nrmse_scores=np.sqrt(-scores)","428e2828":"rmse_scores","25284c66":"#lets view all scores\ndef display_scores(scores):\n    print(\"Scores:\",scores)\n    print(\"Mean:\",scores.mean())\n    print(\"Standard Deviation:\",scores.std())\n\ndisplay_scores(rmse_scores)","70341085":"lin_scores=cross_val_score(lin_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\nlin_rmse_scores=np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","2045bd6e":"# try with randomforest\nfrom sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nrf_scores=cross_val_score(forest_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=5)\nrf_rmse_scores=np.sqrt(-rf_scores)\ndisplay_scores(rf_rmse_scores)","70ac3fd2":"#lets use GridSearchCV for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={\n    'n_estimators':[3,10,30],'max_features':[2,4,6,8],\n    'bootstrap':[False,True],'n_estimators':[3,10],'max_features':[2,3,4],\n}\n\nforest_reg=RandomForestRegressor()\ngrid_search=GridSearchCV(forest_reg,param_grid,cv=5,scoring=\"neg_mean_squared_error\")\ngrid_search.fit(housing_prepared,housing_labels)","ddf36d99":"grid_search.best_params_","526da14f":"#getting the best model\ngrid_search.best_estimator_","cac899c6":"#scores\ncvres=grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"],cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","01f01d97":"pd.DataFrame(grid_search.cv_results_)","d2fdfc17":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","84373c24":"# display with feature names\nextra_features=[\"rooms_per_hhold\",\"population_per_hhold\",\"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"] # calleing transformer named \"cat\" from full pipeline\ncat_one_hot_features = list(cat_encoder.categories_[0])\nfeatures = num_attribs + extra_features + cat_one_hot_features","6f518723":"sorted(zip(feature_importances, features), reverse=True)","e5a2c620":"final_model= grid_search.best_estimator_","6d6d50e6":"X_test= strat_test_set.drop(\"median_house_value\",axis=1)\ny_test= strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)","4924f00b":"final_predictions= final_model.predict(X_test_prepared)","b95468e6":"final_mse= mean_squared_error(y_test,final_predictions)\nfinal_rmse=np.sqrt(final_mse)","87e08012":"final_rmse","ac9e4174":"<a id=\"1\"><\/a>\n## 1. Project Skeleton\nBefore starting out any project, we must first plan our steps and have clarity on what type of problem we are tackling and what tools can be used and what cannot be used and why not?. This \"why not\" question will help you gain more insights on your ML journey. The following are key points I took into consideration.\n\nStaircase\n* What kind of ML problem statement is it? Try to define it\n* Understand the type of data?\n* Keep a test data aside for EDA\n* Relationships between various features, ie EDA \n* Try your intuition about the field: \n   * What can be important features that effect a house price? Bedrooms? Area? Population?\n* Data preprocessing: Building a pipeline for it\n* Applying models to predict\n* What must be the evaluation metric?\n* Evaluate the model on Test data","908d78d5":"<a id=\"7b\"><\/a>\n### b. Better Evaluation Using Cross Validation","3d3126a9":"<a id=\"2\"><\/a>\n## 2. Load the Data","e2ec9dee":"<a id=\"4\"><\/a>\n## 4. Create a Test Set","ebf8a9e3":"<a id=\"6\"><\/a>\n## 6. Preparing Data for Machine Learning Algorithms\n\nLets start by separating labels and predictors of our orignal train dataset into copies that we can use\n","a4e41f32":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Navigation<\/h3>\n\n[1. Project Skeleton](#1)   \n[2. Load the Data](#2)  \n[3. Take a Quick Look at Data Structures](#3)   \n[4. Create a Test Set](#4)    \n[5. Discover and Visualize Data to Gain Insights](#5)  \n&nbsp;&nbsp;&nbsp;&nbsp;[a. Visualizing Geographical Data](#5a)   \n&nbsp;&nbsp;&nbsp;&nbsp;[b. Looking for Correlations](#5b)       \n&nbsp;&nbsp;&nbsp;&nbsp;[c. Experimenting with Feature Combinations](#5c)     \n[6.Preparing Data for Machine Learning Algorithms](#6)     \n&nbsp;&nbsp;&nbsp;&nbsp;[a. Data Cleaning](#6a)     \n&nbsp;&nbsp;&nbsp;&nbsp;[b. Handling Text and Categorical Features](#6b)     \n&nbsp;&nbsp;&nbsp;&nbsp;[c. Column Transformers](#6c)     \n&nbsp;&nbsp;&nbsp;&nbsp;[d. Transformation Pipelines](#6d)     \n[7. Select and Train a Model](#7)     \n&nbsp;&nbsp;&nbsp;&nbsp;[a. Training and Evaluating on Training Set](#7a)     \n&nbsp;&nbsp;&nbsp;&nbsp;[b. Better Evaluation Using Cross Validation](#7b)     \n[8. Fine-Tune a Model](#8)  \n&nbsp;&nbsp;&nbsp;&nbsp;[a. Grid Search](#8a)     \n&nbsp;&nbsp;&nbsp;&nbsp;[b. Analyse the Best Models and Their Errors](#8b)       \n[9. Evaluate Your System on Test Set](#9)    \n[10. References](#10)   ","e936bec2":"## KEY NOTE\n\nThis notbook is complete guide to end to end machine learning problem from scratch. if you are beginner, it might hep you have an insight on how to start and how to approach a ML problem. Since the dataset is fairly simple it is very good to start your handson with.\n\nI followed the book by Aurelien Geron, The steps described by him are really detailed, so I decided to replicate it on my own and experiment with the concepts.","4c19e4fd":"<a id=\"5a\"><\/a>\n### a. Visualizing Geographical Data","d439f300":"<a id=\"8b\"><\/a>\n### b. Analyse the Best Models and Their Errors","3eed4b41":"<a id=\"8\"><\/a>\n## 8. Fine-Tune the Model","c4c5a006":"<a id=\"7a\"><\/a>\n### a. Training and Evaluating on Training Set","5413da1a":"or we can use FunctionTransformer that easily defines above class based on your function","46504b74":"<a id=\"6c\"><\/a>\n### c. Column Transformer\n\nFor regular transformation of columns as we did while experimention with features, we can define a column transformer","049d8f0d":"<a id=\"10\"><\/a>\n## 10. References:\n\nLink to the book I followed: [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https:\/\/www.amazon.in\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1491962291\/ref=sr_1_1?dchild=1&keywords=handson+sklearn&qid=1599399632&sr=8-1) - *Aur\u00e9lien G\u00e9ron*\n    \nTop 5 Conceptual Books you might wanna see:\nhttps:\/\/www.kaggle.com\/getting-started\/171809\n    ","9a34fe58":"<a id=\"9\"><\/a>\n## 9. Evaluate Your System on the Test Set","cf6fcbb5":"Since classes are not ordinal, we will one-hot encode them","392881c2":"This is underfitting model","c2c062fb":"<a id=\"6b\"><\/a>\n### b. Handling Text and Categorical Features\nWe will handle the text feature \"ocean_proximity\" that we dropped earlier as it cannot be fed directly into any ML model","f0d49237":"<a id=\"3\"><\/a>\n## 3. Take a Quick Look at Data Structures","2a0094f8":"<a id=\"5\"><\/a>\n## 5. Discover and Visualize Data to Gain Insights\n Do exploratory data analysis on test data","51dd778e":"<a id=\"5b\"><\/a>\n### b. Looking for Correlations","b4c3fdd7":"<a id=\"6a\"><\/a>\n### a. Data Cleaning\nMissing values can be dealt in follwoing ways:\n1. Get rid of the corresponding values\n2. Get rid of whole features\n3. Set missing values to some value (zero, mean, median, etc)","1316a794":"The above model is highly overfitting, it recalls every value from the training set","dce5bdd6":"<a id=\"6d\"><\/a>\n### d. Transformation Pipelines","8f2cc690":"<a id=\"8a\"><\/a>\n### a. Grid Search","2d29b43c":"<a id=\"7\"><\/a>\n## 7. Select and Train a Model","b4883fc8":"<a id=\"5c\"><\/a>\n### c. Experimenting with Feature Combinations\nwe ll try to create new features that are more relevant","393d5ab2":"### Thats the final Test Score","c9eb3e70":"### Do Upvote if you like :)"}}