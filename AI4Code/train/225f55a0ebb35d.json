{"cell_type":{"1fbd16a2":"code","efe2e4d7":"code","65c875a6":"code","3c416950":"code","64d37fed":"code","83b1aa1f":"code","a10f9ca6":"code","8e9d2bcc":"code","b58375d1":"code","05cf304e":"code","9fa188b3":"code","eb27b4d6":"code","304a9a20":"code","c307bc7a":"code","05c2e051":"code","9d895ec4":"code","3b02653f":"code","f99df854":"code","3e5a6527":"markdown","031de0df":"markdown","f545e8d7":"markdown","f3e9ade8":"markdown","abe65671":"markdown","83180f4f":"markdown","8b4ef0f4":"markdown","b20a90b6":"markdown","6bd99aa5":"markdown"},"source":{"1fbd16a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","efe2e4d7":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler","65c875a6":"from sklearn.datasets import fetch_california_housing\n\ncalifornia = fetch_california_housing()","3c416950":"df = pd.DataFrame(california.data, columns=california.feature_names)\ndf['Target'] = california.target\ndf.tail()","64d37fed":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","83b1aa1f":"data = torch.from_numpy(df.values).float()\n\nx = data[:, :-1]\ny = data[:, -1:]\n\nx.shape, y.shape","a10f9ca6":"# Train, Valid, Test ratio\nratios = [.6, .2, .2]","8e9d2bcc":"train_cnt = int(data.size(0) * ratios[0])\nvalid_cnt = int(data.size(0) * ratios[1])\ntest_cnt = data.size(0) - train_cnt - valid_cnt # No data remaining due to ratio application\ncnts = [train_cnt, valid_cnt, test_cnt] \n\nprint('Train %d \/ Valid %d \/ Test %d samples' % (train_cnt, valid_cnt, test_cnt))","b58375d1":"# Shuffle before split.\nindices = torch.randperm(data.size(0))\nx = torch.index_select(x, dim=0, index=indices)\ny = torch.index_select(y, dim=0, index=indices)\n\n# Split train, valid and test set with each count.\nx = list(x.split(cnts, dim=0)) # tensor.split(split_size_or_sections(list), dim=0), If you devide by a list, you get a list.\ny = y.split(cnts, dim=0)\n\ncnt =  0 # A variable for preprocessing\nfor x_i, y_i in zip(x, y):\n    print(x_i.size(), y_i.size())\n    cnt += 1\n    \ncnt","05cf304e":"scaler = StandardScaler()\nscaler.fit(x[0].numpy()) # When applying fit, only train data should be used\n\n# Transform x by the divided number : x_0, x_1, x_2\nfor i in range(cnt):\n    x[i] = torch.from_numpy(scaler.transform(x[i].numpy())).float() # Convert numpy value to float value of torch\n    \n    \n# Check if scaler is applied\ndf = pd.DataFrame(x[0].numpy(), columns=california.feature_names)\ndf.tail()","9fa188b3":"model = nn.Sequential(\n    nn.Linear(x[0].size(-1), 6),\n    nn.LeakyReLU(),\n    nn.Linear(6, 5),\n    nn.LeakyReLU(),\n    nn.Linear(5, 4),\n    nn.LeakyReLU(),\n    nn.Linear(4, 3),\n    nn.LeakyReLU(),\n    nn.Linear(3, y[0].size(-1))\n)\n\nmodel","eb27b4d6":"optimizer = optim.Adam(model.parameters()) # No learning_rate","304a9a20":"n_epochs = 4000\nbatch_size = 256\nprint_interval = 100","c307bc7a":"# Preparing to save configuration\nfrom copy import deepcopy\n\nlowest_loss = np.inf\nbest_model = None\n\nearly_stop = 100 # If performance does not improve until 100 epochs, it will stop.\nlowest_epoch = np.inf","05c2e051":"train_history, valid_history = [], []\n\nfor i in range(n_epochs):\n    # Shuffle again in a epochs before mini-batch split.\n    indices = torch.randperm(x[0].size(0)) # from train set\n    x_ = torch.index_select(x[0], dim=0, index=indices)\n    y_ = torch.index_select(y[0], dim=0, index=indices)\n    # |x_| = (total_size, input_dim)\n    # |y_| = (total_size, ouput_dim)\n    \n    x_ = x_.split(batch_size, dim=0)\n    y_ = y_.split(batch_size, dim=0)\n    # |x_[i]| = (batch_size, input_dim)\n    # |y_[i]| = (batch_size, output_dim)\n    \n    train_loss, valid_loss = 0, 0\n    y_hat = []\n    \n    ## Applied to train set :: START \n    for x_i, y_i in zip(x_, y_): \n        # |x_i| = |x_[i]|\n        # |y_i| = |y_[i]|\n        y_hat_i = model(x_i)\n        loss = F.mse_loss(y_hat_i, y_i)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step()\n        train_loss += float(loss) # Don't use backwarded loss so, convert to float\n    ## END\n        \n    train_loss = train_loss \/ len(x_)\n    \n    ## Applied to valid set :: START \n    # You need to declare to PYTORCH to stop build the computation graph.\n    with torch.no_grad():  \n        # You don't need to shuffle the validation set cause you don't do gradient \n        # Only split is needed.\n        x_ = x[1].split(batch_size, dim=0)\n        y_ = y[1].split(batch_size, dim=0)\n        \n        valid_loss = 0\n        \n        for x_i, y_i in zip(x_, y_):\n            y_hat_i = model(x_i)\n            loss = F.mse_loss(y_hat_i, y_i)\n            \n            # No optimizer.zero_grad()\n            # No loss.backward()\n            # No optimizer.step()\n                        \n            valid_loss += loss\n            \n            y_hat += [y_hat_i] # Collect y_hat after performing validation data set\n    ## END\n            \n    valid_loss = valid_loss \/ len(x_)\n\n    # Log each loss to plot after training is done.\n    train_history += [train_loss]\n    valid_history += [valid_loss]\n\n    if (i + 1) % print_interval == 0:\n        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (\n            i + 1,\n            train_loss, \n            valid_loss,\n            lowest_loss\n        ))\n\n    # Compare difference between valid_loss(not train_loss) and lowest_loss.\n    # Stop if there is no improvement during 100 epochs increments from lowest_epoch\n    if valid_loss <= lowest_loss:\n        lowest_loss = valid_loss\n        lowest_epoch = i\n\n        # 'state_dict()' returns model weights as key-value.\n        # Take a deep copy, if the valid loss is lowest ever.\n        best_model = deepcopy(model.state_dict())\n\n    else:\n        if early_stop > 0 and lowest_epoch + early_stop < i + 1:\n            print('There is no imporvemnet during last %d epochs.' % early_stop)\n            break\n\nprint('The best validation loss from epoch %d: %.4e' % (lowest_epoch + 1, lowest_loss))\n\n# Load best epoch's model.\nmodel.load_state_dict(best_model)\n","9d895ec4":"plot_from = 10\n\nplt.figure(figsize=(20, 10))\nplt.grid(True)\nplt.title(\"Train \/ Valid Loss History\")\nplt.plot(\n    range(plot_from, len(train_history)), train_history[plot_from:],\n    range(plot_from, len(valid_history)), valid_history[plot_from:]\n)\nplt.yscale('log')\nplt.show()","3b02653f":"# Check the result with the test data set.\n\ntest_loss = 0\ny_hat = []\n\nwith torch.no_grad():\n    x_ = x[2].split(batch_size, dim=0)\n    y_ = x[2].split(batch_size, dim=0)\n    \n    for x_i, y_i in zip(x_, y_):\n        y_hat_i = model(x_i)\n        loss = F.mse_loss(y_hat_i, y_i)\n        \n        test_loss += loss # Gradient is already detached.\n        \n        y_hat += [y_hat_i]\n\ntest_loss = test_loss \/ len(x_)\ny_hat = torch.cat(y_hat,dim=0)\n\nsorted_history = sorted(zip(train_history, valid_history),\n                        key=lambda x: x[1])\n\nprint(\"Train loss: %.4e\" % sorted_history[0][0])\nprint(\"Valid loss: %.4e\" % sorted_history[0][1])\nprint(\"Test loss: %.4e\" % test_loss)","f99df854":"df = pd.DataFrame(torch.cat([y[2], y_hat], dim=1).detach().numpy(),\n                  columns=['y', 'y_hat'])\n\nsns.pairplot(df, height=5)\nplt.show()\n            ","3e5a6527":"## **Preprocessing**","031de0df":"# **Adam Optimizer**","f545e8d7":"### **Build Model & Optimizer**","f3e9ade8":"## **Let's see the result!**","abe65671":"## **Split Tarin \/ Valid \/ Test set**","83180f4f":"### **Load Dataset from sklearn**","8b4ef0f4":"## **Train**","b20a90b6":"### **Convert to Pytorch Tensor**","6bd99aa5":"## **Loss History**"}}