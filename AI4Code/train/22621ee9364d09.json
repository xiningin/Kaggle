{"cell_type":{"258e9a8d":"code","1ca4bdb9":"code","aa30860c":"code","a2de2a6e":"code","d57a6759":"code","ede07cd4":"code","4192c9bf":"code","38610bad":"code","7a9a1a6d":"code","fc162527":"code","00ce9e4e":"code","dfe08fb5":"code","6a45d05b":"code","6d2e7bcb":"code","9068dcf4":"code","973fb28a":"code","c33c16e2":"code","652b49e0":"code","f9dbc24d":"code","4e271bdc":"code","82137bb3":"code","ebdcf8d7":"code","24c7b21d":"code","53042ffd":"code","c50fb4ab":"code","465acd41":"code","d194f105":"code","a57bcc15":"code","43bdc5da":"code","d781a82f":"markdown","1d9d3db5":"markdown","645f8db1":"markdown"},"source":{"258e9a8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ca4bdb9":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n# \uc2dc\uac01\ud654\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","aa30860c":"taitanic_test_data_path = '..\/input\/titanic\/test.csv'\ntaitanic_train_data_path = '..\/input\/titanic\/train.csv'","a2de2a6e":"train_data = pd.read_csv(taitanic_train_data_path) \ntest_data = pd.read_csv(taitanic_test_data_path)","d57a6759":"# missing data check\ntrain_data.isnull().sum()\n\n# 1. age, cabin, embarked \ubb38\uc81c","ede07cd4":"test_data.isnull().sum()","4192c9bf":"!pip install imutils","38610bad":"# missing value \ucc98\ub9ac\n\n# 1. age\ucc98\ub9ac, datawig dnn \uc774\uc6a9 -> train_data renew \nimport pandas as pd\n!pip install datawig\nimport datawig\n\ndf_train, df_test = datawig.utils.random_split(train_data)\n\n#Initialize a SimpleImputer model\nimputer = datawig.SimpleImputer(\n    input_columns=['Pclass','SibSp','Parch'], # column(s) containing information about the column we want to impute\n    output_column= 'Age', # the column we'd like to impute values for\n    output_path = 'imputer_model' # stores model data and metrics\n    )\n\n#Fit an imputer model on the train data\nimputer.fit(train_df=df_train, num_epochs=50)\n\n#Impute missing values and return original dataframe with predictions\nimputed = imputer.predict(df_test)\n# train date's missing value of age problom successfully solve!\n\n# \uc9c8\ubb38: \uc774 \ubaa8\ub378\uc5d0\uc11c column\uc774 \uc65c \ub2e4 \uc548\ucc44\uc6cc\uc9c0\ub294\uc9c0 \ubaa8\ub974\uaca0\uc74c","7a9a1a6d":"imputed['Age']","fc162527":"# 2. cabine column's missing value preprocessing\n","00ce9e4e":"print(imputed['Age'])","dfe08fb5":"results = pd.DataFrame({'id':df_test['id'], 'cardio':preds_test})","6a45d05b":"imputed['Age'].isnull().sum()","6d2e7bcb":"train_data.head()","9068dcf4":"test_data.head()","973fb28a":"print(train_data.columns.values)","c33c16e2":"# passengerId index setting and drop the Survived coloumns\nX_train = train_data.set_index('PassengerId').drop(columns = ['Survived'], axis = 1)\ny_train = train_data.iloc[:,[0,1]].set_index('PassengerId')\n# DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\n\n\n# test data set \uc124\uc815\ndf_test = train_data.set_index('PassengerId')","652b49e0":"X_train.head()","f9dbc24d":"y_train.head()","4e271bdc":"df_test.head()","82137bb3":"# classification, xboost\uc5d0\uc11c \uc81c\uc77c \uc131\ub2a5\uc88b\uc740 \ubaa8\ub378\uc744 \ucc3e\uc544\ubcf4\uc790","ebdcf8d7":"print()","24c7b21d":"# train test split\uc73c\ub85c validation\ub370\uc774\ud130 \uc0dd\uc131\nfrom sklearn.model_selection import train_test_split\n\n# train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify= 'Survived', shuffle=True, random_state=34)\n# stratify\ub294 classification\uc758 0\uacfc 1\uc758 \ube44\uc728\uc744 \uc720\uc9c0\ud55c\ucc44\ub85c split\ud574\uc8fc\ub294 \ub140\uc11d\uc774\ub2e4. \uc694\uc790\uc2dd\uc744 \ud1b5\ud574\uc11c \ub370\uc774\ud130\uc758 \uacfc\uc801\ud569\uc744 \ud53c\ud55c\ub2e4. \ud55c\ucabd\uc73c\ub85c \n# 0\uc774\ub098 1\uc774 \uc3e0\ub9ac\ub294\uac83\uc744 \ubc29\uc9c0\ud574\uc900\ub2e4. stratify\uc5d0 \uac01 column\uc744 \uc124\uc815\ud574\uc8fc\uba74 \ub40c! ","53042ffd":"# # x_train, y_train \ubd84\ub9ac\n# X_train = train_data.drop(columns = ['PassengerId', 'Survived'], axis = 1)\n# y_train = train_data.iloc[:,[0,1]] \n# # y_train = train_data['Survived']\n# # \ud55c\ubc29\uc5d0 \uc5ec\ub7eccolumn drop\ud558\ub294 \ubc29\ubc95 \ucc3e\uc544\ubcf4\uae30\n\n# # \uc0ac\ub9dd\uc5ec\ubd80\ub97c \ub530\uc9c0\ub294\uac70\ub2c8\uae50 passengerid\uc758 \uc22b\uc790\uac00 train data\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uba74 \uc548\ub3fc\uaca0\uc9c0??\n# # pessengerid\ub97c index\ub85c \uc124\uc815\ud558\ub294\ubc95\n# # name\uc744 \ub123\uc73c\uba74 \ubb54\uac00 \uc0dd\uc874\uc728\uc774 \ub192\uc740 \uc774\ub984\uc744 \ucc3e\uc744 \uc218 \uc788\uc744\uae4c","c50fb4ab":"train_data.head()","465acd41":"'''\ud6c4\ubcf4 : 1. randomforestclassifier\n          2. xgbclassifier\n          3. lgb\n          4. logistic regression\n          5. lgbm regressor\n\n'''","d194f105":"# xgbclassifier\ud14c\uc2a4\ud2b8","a57bcc15":"# \uacbd\uace0 \ucd9c\ub825\ud558\uc9c0 \uc54a\uc74c -----------\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# \ub77c\uc774\ube0c\ub7ec\ub9ac\uc640 \ub370\uc774\ud130 \uac00\uc838\uc624\uae30\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport sklearn\nfrom sklearn.metrics import classification_report, accuracy_score","43bdc5da":"xgb_model = xgb.XGBClassifier(silent=False, \n                              booster='gbtree',\n                              scale_pos_weight=1,\n                              learning_rate=0.01,  \n                              colsample_bytree = 0.4,\n                              subsample = 0.8,\n                              objective='binary:logistic', \n                              n_estimators=100, \n                              max_depth=4, \n                              gamma=10, \n                              seed=777)\n\nhr_pred = xgb_model.fit(X_train, y_train).predict(df_test)\nprint(classification_report(df_test, hr_pred))","d781a82f":"# **\ubaa8\ub378\ub9c1**","1d9d3db5":"# data preprocessing(\ub370\uc774\ud130 \uc804\ucc98\ub9ac) & \ubaa8\ub378\ub9c1 & feature engineering","645f8db1":"we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc)."}}