{"cell_type":{"8e38c29f":"code","936d8ff3":"code","bd10bd4f":"code","2770eb53":"code","5bbad8d2":"code","7bda8788":"code","457bcf4e":"code","ce647489":"code","efc4fe43":"code","92419277":"code","f54a78bd":"code","d7310c07":"code","46c81a58":"code","5aeffdc7":"code","ce239f07":"code","9044bfbe":"code","a7f3895e":"code","5fded01a":"code","8ff02eea":"code","25c31755":"code","c1b4749d":"code","ceaec350":"code","8682dcc4":"code","258a7425":"code","654c9a1b":"code","ef8c3d63":"code","1302df7c":"markdown","21ea0fd4":"markdown","8379d6e8":"markdown","f3bdbf46":"markdown","1934a060":"markdown","d223c6c6":"markdown","a9b0c84c":"markdown","b76772c9":"markdown","0d0adcf3":"markdown","9e5264a9":"markdown","cc3c802c":"markdown","da0c3161":"markdown","f55d3211":"markdown","ea603bd4":"markdown","7b7ae307":"markdown","1a1dda5b":"markdown","7e6fb51c":"markdown","32b8caf3":"markdown","4e0b062c":"markdown","bfc1de01":"markdown","ec8df69c":"markdown"},"source":{"8e38c29f":"#Import the libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","936d8ff3":"#Load and visualize the dataset\nIRIS = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\nIRIS","bd10bd4f":"#Extract the labels\nLABELS = IRIS[\"Species\"].unique().tolist()\nLABELS\n","2770eb53":"#Convert the categories into numeric labels\nfor category, label in enumerate(LABELS):\n    IRIS.loc[IRIS[\"Species\"] == label, \"Species\"] = category\n\n#Shuffle and divide the set into train and test sets\nIRIS_TRAIN = IRIS.sample(frac=0.7)\nIRIS_TEST = IRIS.loc[~IRIS.index.isin(IRIS_TRAIN.index)]\nIRIS_TEST = IRIS_TEST.sample(frac=1)      #Shuffle the test set again\n\n#Create numpy arrays\nx_train = IRIS_TRAIN[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].to_numpy()\ny_train = IRIS_TRAIN[['Species']].to_numpy()\n\nx_test = IRIS_TEST[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].to_numpy()\ny_test = IRIS_TEST[['Species']].to_numpy()\n\n#Append a column of 1s to X in order to emulate constant term B in Y = AX + B, where B = A0 and X0 = 1.\nx_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\nx_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n\n#Find the dimensions\nprint(f\"Training set dimensions:{x_train.shape} x {y_train.shape}\")\nprint(f\"Test set dimensions:{x_test.shape} x {y_test.shape}\")","5bbad8d2":"def one_hot(array):\n    '''Returns an array of one hot vectors of shape (n_examples x n_classes)'''\n    if (len(array.shape) > 1) and (array.shape[1] > 1):\n        raise IndexError(f\"Too many dimensions to squeeze. Array has shape: {array.shape}\")\n    \n    array = np.squeeze(array)\n    classes = array.max() + 1\n    oh_array = np.zeros((array.shape[0], classes))\n    for index, label in enumerate(array):\n        oh_array[index, label] = 1\n    return oh_array","7bda8788":"#Encode the labels as one-hot vectors\ny_train = one_hot(y_train)\ny_test = one_hot(y_test)\n\n#Store the input\/output dimensions\nn_features = x_train.shape[1]\nn_classes = y_train.shape[1]\nn_samples = y_train.shape[0]\n\nprint(f\"\\nRevised training set dimensions:{x_train.shape} x {y_train.shape}\")\nprint(f\"Revised test set dimensions:{x_test.shape} x {y_test.shape}\")\nprint(f\"Input\/output dimensions: (n x {n_features}), (n x {n_classes})\")","457bcf4e":"abs = lambda x: x if x >= 0 else -x\n\ndef sigmoid(z):\n    return(1 \/ (1 + np.exp(-z)))\n\ndef softmax(z):\n    '''Returns the softmax values of each element w.r.t. its row'''\n    exp_z = np.exp(z)\n    exp_z_sum = exp_z.sum(axis=1)\n    return(exp_z \/ exp_z_sum[:, np.newaxis])    #np.newaxis helps in broadcasting\n    \ndef hypothesis(X, W, activation):\n    '''Returns the hypothesis for the given set of features and weights using the specified function (sigmoid or softmax)'''\n    return activation(X @ W)\n\ndef predict(X, W, class_labels=None):\n    '''Returns the predicted label of the input example(s) along with category index and the hypothesis(\/es)'''   \n    #Set the activation based on num_classes\n    n_classes = W.shape[1]\n    if n_classes == 1:\n        activation = sigmoid\n    else:\n        activation = softmax\n        \n    h = hypothesis(X, W, activation)\n    if activation == sigmoid:\n        categories = (h > 0.5) * 1    #Bifurcate the results into crisp [0, 1] classes with a threshold value of 0.5\n    elif activation == softmax:\n        categories = np.argmax(h, axis=1)    #Find the class with the highest activation.\n    else:\n        raise ValueError(\"Unknown activation function\")\n        \n    if class_labels:\n        labels = [class_labels[i] for i in categories]\n    else:\n        labels = None\n    \n    return labels, categories, h","ce647489":"### Define some optimizers for an efficient GD step.\ndef pack_opt_params(grads, v, alpha, w, batch):\n    '''Generic function to pack parameters for an optimizer'''\n    params = {\n        \"grads\": grads,\n        \"v\": v,\n        \"alpha\": alpha,\n        \"w\": w,\n        \"batch\": batch\n    }\n    return params\n    \n\ndef momentum_optimizer(B=0.9):\n    def optimizer(params):\n        #Unpack the required params\n        grads = params[\"grads\"]\n        v = params[\"v\"]\n        alpha = params[\"alpha\"]\n        #Return the optimized learning step\n        return ((B * v) + ((1 - B) * alpha * grads))\n    return optimizer\n\nclass adam:\n    def __init__(self, b1=0.9, b2=0.999, e=1e-5):\n        #Init the internal variables\n        self.m = 0\n        self.v = 0\n        self.B1 = b1\n        self.B2 = b2\n        self.B1_ = 1\n        self.B2_ = 1\n        self.e = e\n   \n    def adam_optimizer(self, params):\n        #Unpack the required params\n        grads = params[\"grads\"]\n        alpha = params[\"alpha\"]\n        \n        #Update the moments\n        self.m = ((self.B1 * self.m) + ((1 - self.B1) * grads))\n        self.v = ((self.B2 * self.v) + ((1 - self.B2) * (grads ** 2)))\n        \n        #Update Beta_t\n        self.B1_ *= self.B1\n        self.B2_ *= self.B2\n        \n        #Update the normalized moments\n        self.m_ = self.m \/ (1 - self.B1_)\n        self.v_ = self.v \/ (1 - self.B2_)\n        \n        #return the optimized step\n        return (alpha * self.m_ \/ (np.sqrt(self.v_) + self.e))","efc4fe43":"def get_accuracy(Y, pred):\n    '''Returns the accuracy of the predictions as compared to Y'''\n    Y = np.argmax(Y, axis=1)    #Find the class from its one-hot vector.\n    return np.mean((Y == pred) * 1, axis=0)\n\ndef f_metrics(y, pred):\n    '''Returns the precision, recall and f1 score of the predictions w.r.t the labels'''\n    if (len(y.shape) > 1) or (len(pred.shape) > 1):\n        raise IndexError(f\"Only 1-dimensional vectors are expected as inputs. Provided parameters have shapes: {y.shape} and {pred.shape}\")\n    true = (y == pred) * 1\n    false = (y != pred) * 1\n    positive = (y == 1) * 1\n    negative = (y == 0) * 1\n    \n    tp = np.sum(true * positive)\n    fp = np.sum(false * positive)\n    tn = np.sum(true * negative)\n    fn = np.sum(false * negative)\n    \n    precision = tp \/ (tp + fp) if (tp + fp) != 0 else 1\n    recall = tp \/ (tp + fn) if (tp + fn) != 0 else 1\n    f1 = 2 * precision * recall \/ (precision + recall)\n    return [precision,recall, f1]\n\ndef get_f_metrics(Y, pred=None, h=None):\n    '''Returns the f-metrics for hypotheses of each class. If hypotheses are not present then they are computed using predictions'''\n    \n    if h is None:\n        if pred is None:\n            raise SyntaxError(\"Both pred and h parameters cannot be unspecified\")\n        h = one_hot(pred)\n    n_classes = Y.shape[1]\n    class_f_metrics = []          #f_score_metrics = [[p, r, f1] * n_classes]\n    for c in range(n_classes):\n        class_f_metrics.append(f_metrics(Y[:, c], h[:, c]))\n    return class_f_metrics\n    \n","92419277":"def compute_cost(X, Y, W, l=0):\n    '''Returns the cost and its gradients for the hypothesis h_w(X) with LASSO (L1) regularization'''\n    #Set the activation based on num_classes\n    n_classes = Y.shape[1]\n    if n_classes == 1:\n        activation = sigmoid\n    else:\n        activation = softmax\n        \n    #[h(X) = activation(XW)= activation([x_00w_00 + x_01w_01 + ...], [x_10w_10 + x_11w_11 + ...], ...)]\n    #Compute hypothesis\n    h = hypothesis(X, W, activation)\n    \n    ### Compute the cost function\n    #Define the log-likelihood function (ll). We need to maximise this objective function.\n    ll = (Y * np.log(h)) + ((1 - Y) * np.log(1 - h))\n    \n    #Minimizing the additive inverse of the LL function using GD is equivalent to maximizing the LL\n    #Note: for maximizing an objective, regularization term needs to be subtracted.\n    #cost = -(log-likelihood function (ll) - L1 regularization)\n    J = -ll + (l * np.mean(np.vectorize(abs)(W), axis=0))   #Loss with regularization\n    cost = np.mean(J, axis=0)    #Now we can use Gradient Descent to minimize this cost.\n    \n    ### Compute the gradients\n    #d\/dw_i(J) = d\/dw_n(-LL(h, Y)) + l|w_i|\/w_i = -(y\/h - (1-y)\/(1-h)) d\/dw_i(h) +\/- l\n    #     = -(y\/h - (1-y)\/(1-h)) * h(1-h) * x_i  +\/- l = -[y(1-h) - (1-y)h] * x_i +\/- l\n    #     = -[y - hy - h + hy] * x_i +\/- l = -(y - h) * x_i +\/- l\n    \n    #Compute the gradients\n    delta = Y - h\n    gradients = np.zeros(W.shape)\n    reg_coeffs = np.sign(W)   #Coefficients of l in the derivative of the L1-regularization.\n    \n    for c in range(n_classes):\n        #Average the cost of each feature separately for a particular class.       \n        gradients[:, c] = -np.mean(delta[:, c, np.newaxis] * X, axis=0) + (l * reg_coeffs[:, c])   #Broadcast the class deltas separately.\n      \n    cross_entropy_loss = np.sum(cost)\n    return (cross_entropy_loss, gradients)\n\ndef update_w(params, optimizer=None):\n    '''Performs the GD step. Receives optimizer and params as input and returns new weights and step as output'''\n    if optimizer is None:          #Default optimizer returns the simple learning step.\n        optimizer = lambda params: params[\"alpha\"] * params[\"grads\"]\n    #Return the updated weights\n    W = params[\"w\"]\n    v = optimizer(params)\n    return(W - v, v)","f54a78bd":"def logistic_regression(X, Y, validation_set=None, learning_rate=0.01, reg_param=0, batch_size=None, optimizer=None, n_epochs=None):\n    '''Trains a logistic regression model by performing gradient descent and returns the weights'''\n    #Obtain the I\/O dimensions\n    n_features = x_train.shape[1]\n    n_classes = y_train.shape[1]\n    n_samples = y_train.shape[0]\n    \n    #Set the batches\n    if batch_size is None:        #Defaults to regular GD\n        batch_size = n_samples\n    num_batches = int(n_samples \/ batch_size)\n    X_batches = np.array_split(X, num_batches, axis=0)\n    Y_batches = np.array_split(Y, num_batches, axis=0)\n    \n    #Generate logs of parameters over iterations\n    log = {\"alpha\": learning_rate,\n           \"lambda\": reg_param,\n           \"learning_steps\": 0,\n           \"train_loss\": [],\n           \"train_acc\": [],\n           \"val_loss\": [],\n           \"val_acc\": [],\n           \"w\": []}\n    \n    #Initialize W with zeroes.\n    W = np.zeros((n_features, n_classes))\n    \n    v = 0\n    cost = -1\n    avg_cost_delta = 999  #Initialize to any high random value.\n    epoch = 0\n    \n    #Finalize the looping condition\n    if n_epochs:\n        def looping_condition():\n            return epoch < n_epochs   #Run a specified number of iterations.\n    else:\n        def looping_condition():\n            return ((avg_cost_delta > 0.001) and (epoch < 5000))    #Default condition\n    \n    \n    while looping_condition():\n        epoch += 1\n        #Perform the learning step\n        for i in range(num_batches):\n            _, gradients = compute_cost(X_batches[i], Y_batches[i], W, l=reg_param)\n            W, v = update_w(\n                pack_opt_params(grads=gradients, v=v, alpha=learning_rate, w=W, batch=(X_batches[i], Y_batches[i])),\n                optimizer=optimizer)\n\n            #Record the cost\n            train_cost, _ = compute_cost(X, Y, W)\n            cost_delta = abs(train_cost - cost)\n            avg_cost_delta = ((avg_cost_delta * (epoch - 1)) + cost_delta) \/ epoch\n            cost = train_cost\n            \n            #Log the step\n            log[\"learning_steps\"] += 1\n            log[\"w\"].append(W)\n            _, pred, _ = predict(X, W)\n            log[\"train_loss\"].append(cost)\n            log[\"train_acc\"].append(get_accuracy(Y, pred))\n            if validation_set:\n                X_val, Y_val = validation_set\n                _, pred, _ = predict(X_val, W)\n                val_cost, _ = compute_cost(X_val, Y_val, W)\n                log[\"val_loss\"].append(val_cost)\n                log[\"val_acc\"].append(get_accuracy(Y_val, pred))\n                \n    log[\"epochs\"] = epoch\n    if validation_set:\n        X_val, Y_val = validation_set\n        _, pred, _ = predict(X_val, W)\n        log[\"val_f_metrics\"] = get_f_metrics(Y_val, pred=pred)\n    return W, log","d7310c07":"def print_performance_metrics(logs):\n    for model_name, log in logs.items():\n        print(f\"Logistic regression results for {model_name}\\nlearning rate (alpha) = {log['alpha']}\")\n        print(f\"regularization parameter (lambda) = {log['lambda']}\\nn_epochs = {log['epochs']}\\nlearning steps = {log['learning_steps']}\")\n        print(f\"\\nfinal train_loss = {log['train_loss'][-1]}\\nfinal train_acc = {log['train_acc'][-1]}\")\n        if len(log['val_loss']) > 0:\n            print(f\"\\nfinal val_loss = {log['val_loss'][-1]}\\nfinal val_acc = {log['val_acc'][-1]}\")\n            print(f\"\\nprecision = {log['val_f_metrics'][0]}\\nrecall = {log['val_f_metrics'][1]}\\nf1_score = {log['val_f_metrics'][2]}\")\n        print(f\"\\nW = {W}\")\n        print(\"Convergence criterion: avg_cost_delta <= 0.001\\n\")","46c81a58":"def plot_performance_metrics(logs):\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n    for model_name, log in logs.items():\n        name = f\"{model_name}_a={log['alpha']}_l={log['lambda']}_{log['batch_optimizer']}\"\n        ax[0].plot(log['train_loss'], label=f\"{name} train_loss\")\n        ax[0].plot(log['val_loss'], label=f\"{name} val_loss\")\n        ax[0].set_title('Loss curve')\n        ax[0].set_xlabel('Learning Steps')\n        ax[0].set_ylabel('Loss')\n        ax[0].legend()\n\n        ax[1].plot(log['train_acc'], label=f\"{name} train_acc\")\n        ax[1].plot(log['val_acc'], label=f\"{name} val_acc\")\n        ax[1].set_title('Accuracy curve')\n        ax[1].set_xlabel('Learning Steps')\n        ax[1].set_ylabel('Accuracy')\n        ax[1].legend()\n    \ndef plot_PR_curve(h, Y):\n    '''Plots the precision-recall curve for different choices of thresholds for the given hypothesis(\/es)'''   \n    #Set the activation based on num_classes\n    n_classes = Y.shape[1]\n    thresholds = [0.1 * t for t in range(1, 10)]\n    class_f_metrics = []\n    for t in thresholds:\n        class_membership = (h > t) * 1    #Bifurcate the results into crisp [0, 1] classes with a threshold value of 0.5\n        class_f_metrics.append(get_f_metrics(Y, h=class_membership))\n    \n    fig = plt.figure(figsize=(8, 5))\n    #Calculate the area under the curve as well.\n    AUC_PR = [0] * n_classes\n    for c in range(n_classes):\n        precision = [f_metric[c][0] for f_metric in class_f_metrics]\n        recall = [f_metric[c][1] for f_metric in class_f_metrics]\n\n        prev_p = None\n        prev_r = None\n        for p, r in zip(precision, recall):\n            if (prev_p is not None) and (prev_r is not None):\n                AUC_PR[c] += abs(r - prev_r) * (p + prev_p) \/ 2\n            prev_r = r\n            prev_p = p\n\n        plt.plot(recall, precision, label=f\"class_{c}\")\n        \n    plt.title('Precision-Recall curve')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.legend()\n    print(f\"AUC-PR = {AUC_PR}\")    ","5aeffdc7":"W, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test))\nlog[\"batch_optimizer\"] = \"no_opt\"\nlogs = {\"simple_LR\": log}\n\nprint_performance_metrics(logs)","ce239f07":"IRIS_TEST.head()","9044bfbe":"classes, y_hat, _ = predict(x_test[0:5], W, LABELS)\nacc = get_accuracy(y_test[0:5], y_hat)\nprint(f\"Prediction results:\\nClasses: {classes}\\nAccuracy: {acc}\")\n","a7f3895e":"plot_performance_metrics(logs)","5fded01a":"### Store the logs of the current model for later comparison\nsimple_LR_log = logs[\"simple_LR\"]","8ff02eea":"### TEST DIFFERENT LEARNING RATES\n\nlogs = {}\n#Plain-old LR model\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test))\nlog[\"batch_optimizer\"] = \"no_opt\"\nlogs[\"simple_LR_1\"] = log\n\n#LR model with higher learning rate\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02)\nlog[\"batch_optimizer\"] = \"no_opt\"\nlogs[\"simple_LR_2\"] = log\n\n#LR model with lower learning rate\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.005)\nlog[\"batch_optimizer\"] = \"no_opt\"\nlogs[\"simple_LR_3\"] = log\n\nprint_performance_metrics(logs)\nplot_performance_metrics(logs)","25c31755":"### TEST DIFFERENT REGULARIZATION PARAMETERS\n\nlogs = {}\n#No regularization\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02)\nlog[\"batch_optimizer\"] = \"no_opt\"\nlogs[\"simple_LR\"] = log\n\n#Include a small regularization parameter\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.01)\nlog[\"batch_optimizer\"] = \"no_opt\"\nlogs[\"regularised_LR_1\"] = log\n\n#Include a large regularization parameter\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.02)\nlog[\"batch_optimizer\"] = \"no_opt\"\nlogs[\"regularised_LR_2\"] = log\n\nprint_performance_metrics(logs)\nplot_performance_metrics(logs)\n","c1b4749d":"### TEST DIFFERENT BATCH SIZES\n\nlogs = {}\n#No batching\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.01)\nlog[\"batch_optimizer\"] = \"no_opt\"\nlogs[\"regularised_LR\"] = log\n\n#Include a small batch_size\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.01, batch_size=16)\nlog[\"batch_optimizer\"] = \"16_no_opt\"\nlogs[\"batched_LR_1\"] = log\n\n#Include a large batch_size\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.01, batch_size=32)\nlog[\"batch_optimizer\"] = \"32_no_opt\"\nlogs[\"batched_LR_2\"] = log\n\nprint_performance_metrics(logs)\nplot_performance_metrics(logs)","ceaec350":"### TEST DIFFERENT GD OPTIMIZATIONS\n\nlogs = {}\n#Include a larger batch size\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.01, batch_size=32)\nlog[\"batch_optimizer\"] = \"bs_32_no_opt\"\nlogs[\"LR_1\"] = log\n\n#Include a smaller batch size and momentum optimizer\nopt = momentum_optimizer()\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.01, batch_size=16, optimizer=opt)\nlog[\"batch_optimizer\"] = \"bs_16_momentum\"\nlogs[\"LR_2\"] = log\n\n#Include a smaller batch_size and adam optimizer\nopt_inst = adam()\nopt = opt_inst.adam_optimizer\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.01, batch_size=16, optimizer=opt)\nlog[\"batch_optimizer\"] = \"bs_16_adam\"\nlogs[\"LR_3\"] = log\n\nprint_performance_metrics(logs)\nplot_performance_metrics(logs)","8682dcc4":"#Train the optimal model for 100 epochs\nlogs = {}\nopt_inst = adam()\nopt = opt_inst.adam_optimizer\nW, log = logistic_regression(x_train, y_train, validation_set=(x_test, y_test), learning_rate=0.02, reg_param=0.01, batch_size=16, optimizer=opt, n_epochs=100)\nlog[\"batch_optimizer\"] = \"bs_16_adam\"\nlogs[\"LR_best\"] = log\n\nprint_performance_metrics(logs)\nplot_performance_metrics(logs)","258a7425":"### Store the logs of the best model for later comparison\nbest_fit_LR_log = logs[\"LR_best\"]","654c9a1b":"print(\"Simple model\")\n_, _, h = predict(x_test, simple_LR_log[\"w\"][-1])\nplot_PR_curve(h, y_test)","ef8c3d63":"print(\"Optimized model\")\n_, _, h = predict(x_test, best_fit_LR_log[\"w\"][-1])\nplot_PR_curve(h, y_test)","1302df7c":"It can be seen that for the softmax classifier, recall is usually quite high, implying that the false-negatives are very low. This is because the classes compete against each other to determine the membership of the example, with a winner-takes-all approach. And as the sum of hypotheses of all classes is one, the winning class ends up claiming the largest portion, leaving the other classes with very tiny hypotheses values, thereby preventing them from generating false negatives, even for lower thresholds.","21ea0fd4":"### Encode label indices as one-hot vectors\n\n[0] -> [1, 0, 0]  \n[1] -> [0, 1, 0]  \n[2] -> [0, 0, 1]  ","8379d6e8":"### Model training function\n\nNote: the _log_ dictionary in the logistic regression is used for logging the performance metrics of the model over epochs. It is not the logarithm.","f3bdbf46":"### Plotting and visualization functions","1934a060":"### Prepare the datasets for regression","d223c6c6":"## Precision-Recall curves\n\nLet's compare the non-optimized model and the best-fit model on the basis of their PR-curves for different thresholds of class membership.  \n\nFor softmax function, as we do not have a decision boundary, the intuition may not be very clear. However, for the sigmoid function, the decision boundary plays an important role.  \n\nFor the softmax function, we plot the PR-curves for each of the classes by assuming them to be standalone binary classifiers.","a9b0c84c":"### Performance metrics","b76772c9":"## LOGISTIC REGRESSION\n\n### Walkthrough of multi-class classification using Logistic Regression","0d0adcf3":"### Check predictions","9e5264a9":"## Compare effects of learning rate, regularization, batch processing and optimization","cc3c802c":"### Plot performance metrics","da0c3161":"### Cost and weight-update functions\n\nReference:  [The Derivative of Cost Function for Logistic Regression](https:\/\/medium.com\/analytics-vidhya\/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d)","f55d3211":"## Perform Regression","ea603bd4":"**The model with a higher regularization parameter shows earlier stopping albeit at a higher loss, thereby preventing any overfitting.**  \nWe can choose the smaller $\\lambda$ as it shows a good trade-off between early-stopping and accuracy.","7b7ae307":"## Define utility functions","1a1dda5b":"## Import data\n### Load the raw data","7e6fb51c":"### GD Optimization functions","32b8caf3":"**The model with a higher learning rate shows faster convergence and lower loss.**","4e0b062c":"**GD Optimization considerably hastens the convergence.**  \n\nWe have been able to obtain the following hyperparameters which yield excellent results:\n* $\\alpha$ = 0.02\n* $\\lambda$ = 0.01\n* batch_size = 16\n* optimizer: adam\n\nLet's train our model with these tuned hyperparameters for a good 100 epochs.","bfc1de01":"## Import libraries","ec8df69c":"We notice that:  \n* **Batches result in quicker convergence, in general.**  \n* **Larger batches converge quickly whereas smaller batches converge very slowly but with much lower loss.**  \n\nWe should consider optimizing the GD steps of smaller batches so that convergence occurs sooner."}}