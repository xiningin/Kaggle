{"cell_type":{"f6914bfb":"code","a096781f":"code","55be02ee":"code","c2bf2cb0":"code","241c7720":"code","3a36405f":"code","b540e293":"code","1c2a54a3":"code","36361c1a":"code","9a5148c1":"code","82cc07bf":"code","033f5741":"code","0028a45d":"code","8c48ca57":"code","8bb7519f":"code","f4ec5882":"code","1df2983b":"code","c128a8e1":"code","e1b1516c":"code","ef0c8f9a":"code","e596553a":"markdown","c314dbb2":"markdown","d29a1e64":"markdown","c928bf6e":"markdown","9b4e08f9":"markdown","7dc42f20":"markdown","11f2c071":"markdown","98698096":"markdown"},"source":{"f6914bfb":"# Bibliotecas necess\u00e1rias\n# Manipula\u00e7\u00e3o de dados\nimport pandas as pd\n# Redes Neurais\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, LeakyReLU, ReLU, AveragePooling2D, Flatten, MaxPooling2D, BatchNormalization, ZeroPadding2D, Activation\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.initializers import GlorotNormal\nfrom tensorflow.keras.regularizers import l2\n\n# Plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Avalia\u00e7\u00e3o\nfrom sklearn.metrics import classification_report, confusion_matrix","a096781f":"# Lendo o dataset Kaggle\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\n#Alternativa ler do pr\u00f3prio keras\n#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","55be02ee":"# Analisando o dataset\nprint(\"Quantidade de elementos de treino: {}\". format(len(train)))\nprint(train.head())","c2bf2cb0":"# Separando x_train e y_train\nY = train[\"label\"]\nX = train.drop(labels = [\"label\"],axis = 1)\nprint(X.head())\n# Em formato numpy array de imagens 28 x 28\n#x = X.values.reshape(-1,28,28,1)\n#print(x[0])","241c7720":"print(Y)","3a36405f":"# Numtendi nada!\n# Bora ver com matplotlib\nplt.imshow(X.values[100].reshape(28,28), cmap=plt.cm.binary)\nplt.show()\nprint('Label: {}'.format(Y[100]))","b540e293":"# Transformando a imagem 2d em um numpy array (imagem 28*28 = 784 pixels)\nx = X.values.reshape(42000, 28, 28, 1)\n\n#Normalizando para valores entre 0 e 1\nx = x.astype('float32')\nx \/= 255\n\n# print(x[0])","1c2a54a3":"# Vamos ajustar o formato da saida\nnum_classes = 10\n\n# Convertendo para um vetor de saida com 10 dimensoes\n# ex. 8 => [0,0,0,0,0,0,0,0,1,0]\ny = keras.utils.to_categorical(Y, num_classes)\nprint(y[0])","36361c1a":"# Separando uma parte para treino (90%) e outra para valida\u00e7\u00e3o (10%)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.1, random_state=9)\nprint('Qtde de treino: {}'.format(len(x_train)))\nprint('Qtde de valida\u00e7\u00e3o: {}'.format(len(x_val)))","9a5148c1":"model = Sequential()\n\n# Layer 1\nmodel.add(\n    Conv2D(\n        32, (11, 11), input_shape=(28,28,1),\n        padding='same', \n        kernel_regularizer=l2(0),\n        kernel_initializer=GlorotNormal()\n    )\n)\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Layer 2\nmodel.add(Conv2D(16, (5, 5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Layer 3\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Conv2D(32, (3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Layer 4\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU())\n\n# Layer 5\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Layer 6\nmodel.add(Flatten())\nmodel.add(Dense(16))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU())\nmodel.add(Dropout(0.5))\n\n# Layer 7\nmodel.add(Dense(16))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU())\nmodel.add(Dropout(0.5))\n\n# Layer 8\nmodel.add(Dense(10))\nmodel.add(BatchNormalization())\nmodel.add(Activation('softmax'))\n\nmodel.summary() #Output","82cc07bf":"# Compila o modelo\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","033f5741":"# Treina com os parte dos dados\nbatch_size = 32\nepochs = 40\n\n# Earlystopping e checkpoint\n\ncallbacks = [\n    keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        patience=10,\n        restore_best_weights=True\n    ),\n    keras.callbacks.ModelCheckpoint(\n        filepath=\"mnist_cp\",\n        save_weights_only=True,\n        monitor='val_loss',\n        mode='min',\n        save_best_only=True\n    )\n]\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    verbose=1,\n                    validation_data=(x_val, y_val))","0028a45d":"#Vamos ver como foi o treino?\n\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","8c48ca57":"# Testa\nscore = model.evaluate(x_val, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","8bb7519f":"Y[10]","f4ec5882":"# Testando uma entrada qualquer\nprint(y_train[10])\nprint(model.predict(x_train[10].reshape((1,28,28,1))))\nprint(model.predict_classes(x_train[10].reshape((1,28,28,1))))","1df2983b":"import itertools\n#Plot the confusion matrix. Set Normalize = True\/False\ndef plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","c128a8e1":"# Vendo alguns reports\n# Usando sklearn\nimport numpy as np\n\n# Classificando toda base de teste\ny_pred = model.predict_classes(x_val)\n# voltando pro formato de classes\ny_test_c = np.argmax(y_val, axis=1)\ntarget_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n#Confution Matrix\ncm = confusion_matrix(y_test_c, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\nprint('Classification Report')\nprint(classification_report(y_test_c, y_pred, target_names=target_names))","e1b1516c":"# Gerando sa\u00edda para dataset de teste\n\n#Carrega dataset de teste\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nprint(\"Qtde de testes: {}\".format(len(test)))\n# Bota no formato numpy e normaliza\nx_test = test.values.reshape(len(test),28,28,1)\nx_test = x_test.astype('float32')\nx_test \/= 255\n\n# Faz classifica\u00e7\u00e3o para dataset de teste\ny_pred = model.predict_classes(x_test)\n\n# Verficando algum exemplo\ni = 0\nplt.imshow(test.values[i].reshape(28,28), cmap=plt.cm.binary)\nplt.show()\nprint('Previsto: {}'.format(y_pred[i]))\n\n# Botando no formato de sa\u00edda (competi\u00e7\u00e3o Kaggle)\nresults = pd.Series(y_pred,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,len(y_pred)+1),name = \"ImageId\"),results],axis = 1)\nprint(submission.head(10))\n#Salvando Arquivo\nsubmission.to_csv(\"mlp_mnist_v4.csv\",index=False)","ef0c8f9a":"#introduzindo ruido\nimport numpy as np\nmean = 0.\nstddev = 0.2\nnoise = np.random.normal(mean, stddev, (4200, 28,28,1))\nx_te = x_val + noise\nx_te = np.clip(x_te, 0., 1.)\n\nplt.imshow(x_te.reshape(4200, 28,28)[0], cmap=plt.cm.binary)\nplt.show()\n\n# Testa\nscore = model.evaluate(x_te, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","e596553a":"# Teste Adicional: Com ru\u00eddo","c314dbb2":"Com ru\u00eddo, percebemos que a acur\u00e1cia dos exemplos de valida\u00e7\u00e3o caiu muito. \nUma MLP aprende a partir de pixels individuais.\nUm modelo Convolucional (CNN) captura melhor regi\u00f5es, ou padr\u00f5es espaciais de pixels.\nVeja o exemplo com CNN.","d29a1e64":"# Introdu\u00e7\u00e3o","c928bf6e":"# Gerando Sa\u00edda","9b4e08f9":"# Avaliando o Modelo","7dc42f20":"# Bibliotecas e Dados","11f2c071":"# Criando e treinando o Modelo","98698096":"**Resumo:**\n\nO exemplo aqui desenvolvido tem como objetivo apresentar conceitos iniciais de implementa\u00e7\u00e3o de redes neurais com python e tensorflow\/keras. Esse modelo apresenta um modelo MLP b\u00e1sico que pode ser expandido mudando o n\u00famero de neur\u00f4nios e camadas. Em adapta\u00e7\u00f5es mais avan\u00e7adas, pode-se estudar possibilidade de otimiza\u00e7\u00e3o de hyperpar\u00e2metros e outras t\u00e9cnincas como aumento de dados.\n\n**N\u00e3o \u00e9 objetivo nosso desenvolver e otimizar o modelo de classifica\u00e7\u00e3o**. O exemplo tem objetivo meramente did\u00e1tico.\n\n---\n\n**Para saber mais:**\n* [Palestras e cursos do Ocean](http:\/\/www.oceanbrasil.com\/)\n* Fran\u00e7ois Chollet. Deep Learning with Python. Manning Publications, 2017.\n* Ian Goodfellow and Yoshua Bengio and Aaron Courville. [Deep Learning](https:\/\/www.deeplearningbook.org\/). MIT Press, 2016."}}