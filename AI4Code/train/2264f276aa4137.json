{"cell_type":{"e2015ecc":"code","0ac6b8c7":"code","a004b779":"code","670260e4":"code","50abe7a2":"code","5b014008":"code","b16582f4":"code","893c79e1":"code","a135d644":"code","9034e8ac":"code","637154cc":"code","276a66a8":"code","d0c5f33d":"code","37f7c004":"code","540b41af":"code","b251808e":"code","60b4ce38":"code","cc7b094a":"code","d4a492e8":"code","5f19baeb":"code","fc155f8d":"code","cc3ee313":"code","6c4f3414":"code","356b612a":"code","d8e50a64":"markdown","5dba8e71":"markdown","3ca4726f":"markdown","9b962137":"markdown","8fb793ea":"markdown","73433a33":"markdown","225ffdcb":"markdown","b286b7f1":"markdown","80b1e820":"markdown","0c7ed160":"markdown","a3284c5c":"markdown","4ba8116f":"markdown","be332cc7":"markdown","48e00b73":"markdown","d85ea769":"markdown","3b77eed1":"markdown","e6183a24":"markdown","895d73ff":"markdown","0511753b":"markdown","1d26f3d4":"markdown","ab15cc0e":"markdown","f6782fb0":"markdown","f32cd679":"markdown","4b736007":"markdown","c73aca2d":"markdown"},"source":{"e2015ecc":"# Importing neccesary packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.integrate\nimport itertools\nimport sklearn.model_selection\nimport sklearn.linear_model\n%matplotlib inline","0ac6b8c7":"# Read data\ndata=pd.read_csv(\"..\/input\/multipleChoiceResponses.csv\",skiprows=[1],low_memory=False)\n\n# Predictors and target\npredictors=['Q2','Q4','Q8']\ntarget='Q9'\n\nn_data=data[[target]+predictors].dropna()","a004b779":"def bar_plot(question_data, variable_name, names):\n    dic={}\n    for name in names:\n        dic[name]=sum(question_data==name)\n    \n    plt.figure(figsize=(10,10))\n    plt.barh(range(len(dic)), list(dic.values()), align='center')\n    # plt.yticks(range(len(dic)), list(dic.keys()), rotation=90)\n    plt.yticks(range(len(dic)), list(dic.keys()), rotation=15)\n    plt.xlabel('Number of data scientists')\n    plt.ylabel(variable_name)","670260e4":"data_q2=np.array(n_data[predictors[0]])\nnames=sorted(set(data_q2))\nbar_plot(data_q2,'Age ranges',names)","50abe7a2":"data_q4=np.array(n_data[predictors[1]])\nnames=sorted(set(data_q4))\nbar_plot(data_q4,'Highest degree of formal education',names)","5b014008":"data_q8=np.array(n_data[predictors[2]])\n\nint_name=[]\nall_names=np.array(list(set(data_q8)))\nfor name in set(data_q8):\n    if \"-\" in name:\n        inte=int(name.split(\"-\")[0])\n    else:\n        inte=int(name.split(\"+\")[0])\n    int_name.append(inte)\nprint(int_name)\nindx=sorted(range(len(int_name)), key=lambda k: int_name[k])\n# bar_plot(data_q8,'Years of experience',)\n\nbar_plot(data_q8, 'Years of experience', all_names[indx])","b16582f4":"# Adjust salary to binary \n\norig_salary=n_data[target]\n\nstrings=(sorted(set(n_data[target])))\nii=np.array(n_data['Q9']==strings[len(strings)-1])\nsubset=np.array(n_data[[target]+predictors])[~ii,:]\n\nfor i in range(subset.shape[0]):\n    if \"-\" in subset[i,0]:\n        max_salary=subset[i,0].split(\"-\")[1]\n    else:\n        max_salary=subset[i,0].replace(\"+\",\"\")\n    \n    max_salary=max_salary.replace(\",\",\"\")\n    \n    subset[i,0]=int(max_salary)\n    \n\n# Create binary array. 1 for people with salary > cut_salary\nq9_binary=np.zeros(subset.shape[0])\ncut_salary=150000\nq9_binary[subset[:,0]>cut_salary]=1","893c79e1":"def bar_plot_compensation(question_data, target_binary_data, variable_name, names, cut_salary):\n    dic_greater={}\n    dic_lower={}\n    for name in names:\n        cond1=np.array(question_data==name)\n        dic_greater[name]=sum(cond1 & (target_binary_data==1))\n        dic_lower[name]=sum(cond1 & (target_binary_data==0))\n    \n    plt.figure(figsize=(10,10))\n    # plt.barh(range(len(dic)), list(dic.values()), align='center')\n    # plt.yticks(range(len(dic)), list(dic.keys()), rotation=90)\n    \n    p1 = plt.barh(range(len(dic_greater)), list(dic_greater.values()))\n    p2 = plt.barh(range(len(dic_lower)), list(dic_lower.values()), left=list(dic_greater.values()))\n    \n    plt.yticks(range(len(dic_greater)), list(dic_greater.keys()), rotation=15)\n    plt.xlabel('Number of data scientists')\n    plt.ylabel(variable_name)\n    \n    plt.legend((p1[0], p2[0]), ('Yearly compensation > %d'%cut_salary, 'Yearly compensation <= %d'%cut_salary))\n\n\ndata_q2=np.array(n_data[predictors[0]])\nquestion_data=subset[:,1]\ntarget_binary_data=q9_binary\nvariable_name='Age range'\nnames=sorted(set(data_q2))\n\nbar_plot_compensation(question_data, target_binary_data, variable_name, names, cut_salary)","a135d644":"data_q4=np.array(n_data[predictors[1]])\n\nquestion_data=subset[:,2]\ntarget_binary_data=q9_binary\nvariable_name='Highest degree of formal education'\nnames=sorted(set(data_q4))\n\nbar_plot_compensation(question_data, target_binary_data, variable_name, names, cut_salary)","9034e8ac":"data_q8=np.array(n_data[predictors[2]])\n\nint_name=[]\nall_names=np.array(list(set(data_q8)))\nfor name in set(data_q8):\n    if \"-\" in name:\n        inte=int(name.split(\"-\")[0])\n    else:\n        inte=int(name.split(\"+\")[0])\n    int_name.append(inte)\nindx=sorted(range(len(int_name)), key=lambda k: int_name[k])\n\nquestion_data=subset[:,3]\ntarget_binary_data=q9_binary\nvariable_name='Years of experience'\nnames=all_names[indx]\n\nbar_plot_compensation(question_data, target_binary_data, variable_name, names, cut_salary)","637154cc":"n_data=data[[target]+predictors].dropna()","276a66a8":"# Adjust age, taking lower age in age range as parameter\nage='Q2'\norig_age=np.array(n_data[age])\nn_ages=np.zeros(len(orig_age))\n\nfor i in range(len(orig_age)):\n    if \"-\" in orig_age[i]:\n        str_age=orig_age[i].split(\"-\")[0]\n    else:\n        str_age=orig_age[i].replace(\"+\",\"\")\n    n_ages[i]=int(str_age)\n    \nn_data[age]=n_ages","d0c5f33d":"# Adjust experience years, taking lower experience years in years range as parameter\nyears='Q8'\norig_years=np.array(n_data[years])\nn_years=np.zeros(len(orig_years))\n\nfor i in range(len(orig_years)):\n    if \"-\" in orig_years[i]:\n        str_years=orig_years[i].split(\"-\")[0]\n    else:\n        str_years=orig_years[i].replace(\"+\",\"\")\n    n_years[i]=int(str_years)\n    \nn_data[years]=n_years","37f7c004":"# Adjust salary to binary \norig_salary=n_data[target]\n\nstrings=(sorted(set(n_data[target])))\nii=np.array(n_data['Q9']==strings[len(strings)-1])\nsubset=np.array(n_data[[target]+predictors])[~ii,:]\n\nfor i in range(subset.shape[0]):\n    if \"-\" in subset[i,0]:\n        max_salary=subset[i,0].split(\"-\")[1]\n    else:\n        max_salary=subset[i,0].replace(\"+\",\"\")\n    \n    max_salary=max_salary.replace(\",\",\"\")\n    \n    subset[i,0]=int(max_salary)\n    \n\n# Create binary array. 1 for people with salary > cut_salary\nq9_binary=np.zeros(subset.shape[0])\ncut_salary=150000\nq9_binary[subset[:,0]>cut_salary]=1\n\nsubset[:,0]=q9_binary","540b41af":"# Ajust degree to numerical data\ndef string2numeric(data):\n    dic={}\n    values=sorted(set(data))\n    cont=0\n    for value in values:\n        dic[value]=cont\n        cont+=1;\n    numeric=[dic[item] for item in data]\n    return numeric, dic\n\nn_deg,n_deg_dic=string2numeric(subset[:,2])\nsubset[:,2]=n_deg","b251808e":"# Find the most important predictors according to ROC and Precision-recall areas from logistic regression\n\n'''\nFunction to find areas of ROC and precision-recall figures\n'''\ndef roc_pr_areas(data_predictors,data_target, method):\n    # Split in train and test\n    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(data_predictors,data_target, test_size=0.4)\n    \n    # \n    method.fit(X_train,Y_train)\n    y_predict=method.predict_proba(X_test)[:,1]\n    \n    conf_matrix=np.zeros((2,2))\n    \n    threshold=np.linspace(0.1,0.8,100)\n    pts_roc=np.zeros((len(threshold),2))\n    pts_pr=np.zeros((len(threshold),2))\n    \n    for i in range(len(threshold)):\n    \n        for j in range(len(y_predict)):\n            \n            if y_predict[j]>threshold[i]:\n                if Y_test[j]==1:\n                    conf_matrix[0,0]+=1\n                else:\n                    # False positive\n                    conf_matrix[0,1]+=1\n            else:\n                if Y_test[j]==0:\n                    conf_matrix[1,1]+=1\n                else: \n                    # False negative\n                    conf_matrix[1,0]+=1\n        \n        tp=conf_matrix[0,0]\n        fp=conf_matrix[0,1]\n        tn=conf_matrix[1,1]\n        fn=conf_matrix[1,0]\n        # pts_roc= false positive rate, true positive rate = fp\/(fp+tn), tp\/(tp+fn)\n        pts_roc[i,:]=fp\/(fp+tn), tp\/(tp+fn)\n        \n        # pts_pr= recall, precision = tp\/tp+fn, tp\/tp+fp\n        pts_pr[i,:]=tp\/(tp+fn), tp\/(tp+fp)\n    \n    # compute the area under precision recall curve\n    ii = np.argsort(pts_roc[:,0])\n    iii=ii[0:10]\n    roc_area=scipy.integrate.trapz(pts_roc[ii,1],pts_roc[ii,0])\n    \n    # compute the area under precision recall curve\n    ii = np.argsort(pts_pr[0,:])\n    pr_area=scipy.integrate.trapz(pts_pr[ii,1],pts_pr[ii,0])  \n    \n    return roc_area, pr_area\n    \n    \n    \n'''\nFunction to find variables that best predict the target\n'''\ndef best_predictors(data_predictors,data_target, method, predictor_names):\n    n_predictors=len(predictor_names)\n    highest_roc_area=0\n    highest_pr_area=0\n    best_predictors=np.array([''])\n    for i in range(n_predictors):\n        combinations = itertools.combinations(np.arange(0,n_predictors), i+1)\n        for comb in combinations:\n            roc_area, pr_area= roc_pr_areas(data_predictors,data_target, method)\n            #print('roc_area=', roc_area, 'for combination', predictor_names[list(comb)],  'and pr_area=', pr_area)\n            if roc_area>highest_roc_area and pr_area>highest_pr_area:\n                highest_roc_area=roc_area\n                highest_pr_area=pr_area\n                best_predictors=predictor_names[list(comb)]\n                print(\"New predictors=\",best_predictors)\n    return best_predictors","60b4ce38":"data_predictors=subset[:,1:]\ndata_target=np.array(subset[:,0], dtype=int)\nf = sklearn.linear_model.LogisticRegression(solver='lbfgs')\npredictors=np.array(['Q2','Q4','Q8'])\n\nn_predictors=best_predictors(data_predictors,data_target,f, predictors)\nprint('Best predictors with logistic regression:',n_predictors)","cc7b094a":"# Get predictor columns as dictionary and get best predictors from subset data\npredictors_cols_dic={}\nfor i in range(len(predictors)):\n    predictors_cols_dic[predictors[i]]=i\npredictors_subset=subset[:,1:]\ntarget_subset=np.array(subset[:,0],dtype=int)\n\n# Get best predictors subset\nii=[predictors_cols_dic[name] for name in n_predictors]\nbest_subset=predictors_subset[:,ii]","d4a492e8":"# Find relation between best predictor and target\nnf = sklearn.linear_model.LogisticRegression(solver='lbfgs')\nnf.fit(best_subset,target_subset)\nprint(\"Coefficients: \",nf.coef_)","5f19baeb":"for i in range(nf.coef_.shape[1]):\n    y=nf.intercept_+nf.coef_[:,i]\n    plt.plot([0,1],[nf.intercept_,y],label=n_predictors[i])\nplt.ylabel('Yearly compensation')\nplt.legend()","fc155f8d":"import sklearn.discriminant_analysis\n\nf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n\nn_predictors=best_predictors(data_predictors,data_target,f, predictors)\nprint('Best predictors with LDA:',n_predictors)","cc3ee313":"'''\nFunction to plot relation between best predictors and yearly compensation\n'''\ndef plot_best_predictors(method, best_subset, target_subset, n_predictors):\n    method.fit(best_subset,target_subset)\n\n    for i in range(method.coef_.shape[1]):\n        y=method.intercept_+method.coef_[:,i]\n        plt.plot([0,1],[method.intercept_,y],label=n_predictors[i])\n    plt.ylabel('Yearly compensation')\n    plt.legend()","6c4f3414":"# Find relation between best predictor and target with LDA\nmethod=sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n\nii=[predictors_cols_dic[name] for name in n_predictors]\nbest_subset=predictors_subset[:,ii]\n\nplot_best_predictors(method, best_subset, target_subset, n_predictors)","356b612a":"print(n_deg_dic)","d8e50a64":"Stop for a moment and look at the bar charts in detail. For example, in the age range bar chart, it can be seen that scientists with ages between 60 and 69 have better yearly compensations. Even though blue zone appears to be less compared with other age ranges, the proportion between the scientists who earn better yearly compensations and the total scientists with ages in this range is higher compared with this proportion in lower age ranges. \nAs expected, yearly compensation tends to increase with the number of experience years, with a better formal education degree and with age.","5dba8e71":"To find the variables that best predict yearly compensation, ROC and precision-recall figures are implemented. The general idea is that the combination of predictors that give a result with the highest ROC and precision-recall areas will be the predictors that best predict the target variable. To find these areas, a machine learning method must be implemented after splitting the data into training and test sets. Then, the values of precision, recall, true and false positive rates can be found and, therefore, the ROC and precision-recall curves. Comparing all the areas computed from all the possible combinations of predictors allows to find the variables that best predict yearly compensation. The functions defined in this section carry out this work.","3ca4726f":"To find the possible relation between the yearly compensation and the best predictors found previously, different machine learning methods are presented in this section. The aim of every machine learning method is to find the formula, with its respective coefficients, from which yearly compensation can be predicted. The best predictors for the logistic regression method were found previously, but with the other methods the best predictors can change. As a consequence, with every different machine learning method the functions defined in the previous section must be called again. Moreover, when these functions are called again with the same method the best predictors set can change because of the training and test split. Therefore, the code and one result are presented, but in the conclusions section the general trends will be described.","9b962137":"<a id='best'><\/a>\n# Finding best predictors","8fb793ea":"Look at the huge number of masters!! From the bar chart can be concluded that most of the data scientists that answered the survey have bachelor, master or doctoral degree. There are more than 8000 Scientists with a master's degree that answered the kaggle questions. On the other hand, the lowest number of scientists that respond to the survey is obtained from people with no formal education past high school. Also, there are some people who preferred not to answer (in fact, there are more people who preferred not to answer than people with no formal education past high school). These people will not be taking into account when finding relations between variables and yearly compensation","73433a33":"<a id='concl'><\/a>\n# Conclusions\n","225ffdcb":"To apply machine learning methods to the data set, variables must be changed from strings to numbers. Therefore, here is presented the reorganization performed to achieve this goal. Related with the age variable, the value taken is the lower age in every age range. This value will be implemented in the prediction of yearly compensation.","b286b7f1":"<a id='sort'><\/a>\n# Reorganizing data","80b1e820":"<a id='mach'><\/a>\n# Determining relation between predictors and yearly compensation","0c7ed160":"# Can your yearly compensation be predicted?","a3284c5c":"To start, it is instructive and interesting to take a look at the data from bar charts. These charts constitute a first approximation to understand the scientists' data collected with the kaggle survey. For this job, the attention will be concentrated in yearly compensation, age ranges, highest formal education degree and job experience in years. This section illustrates bar charts related to these variables. ","4ba8116f":"# Content\n* [Introduction](#intro)\n* [Visualize data](#vis)\n* [Reorganizing data](#sort)\n* [Finding best predictors](#best)\n* [Determining relation between predictors and yearly compensation](#mach)\n* [Conclusions](#concl)","be332cc7":"## Linear Discriminan Analysis","48e00b73":"Based on this, the result is expected since greater values in the dictionary corresponds to lower levels of formal education. Moreover, the common believes presented in the introduction are supported by kaggle dataset. Logistic regression and linear discriminant analysis seem to work well predicting yearly compensation from age, experience and formal education degree.","d85ea769":"The bar chart indicates that many scientists with low years of experience respond to the kaggle survey (I would be included here). The number of scientists that answered kaggle questions decrease with increasing years of experience. More than 5000 participants in kaggle survey have between 0 and 1 year of experience. ","3b77eed1":"<a id='intro'><\/a>\n# Introduction","e6183a24":"As with age data, it is considered only the lower number in the range of experience years for every scientist. ","895d73ff":"From this bar chart, it can be observed that many scientists that answered the kaggle questions are between 22 and 34. Specially greater (more than 5000) is the number of data scientists with an age between 25 and 29 years old that respond to kaggle survey. As the age increases from this age range, the number of data scientists decreases and there are very few people with an age greater than 80. ","0511753b":"## Logistic regression","1d26f3d4":"After running multiple times the code, the set of best predictors can change, but always the same relations are found: yearly compensation grows with an increase in age and years of experience. On the other hand, yearly compensation decreases slowly with increments in the values for the highest degree of formal education, defined within the dictionary. This dictionary is as follows:","ab15cc0e":"Finally, the organization of highest formal education degree data is different. A number is assigned to each different degree name. As a consequence, all bachelors will be denoted by a common number, and the same applies for all the other possible formal education degrees. ","f6782fb0":"<img src=\"https:\/\/cdn.foliomag.com\/wp-content\/uploads\/2018\/10\/salary.jpg?_ga=2.142752437.1019928373.1543791520-543256937.1543791520\" width=\"300\">\n## <center><h4>Image from Folio:<\/h4><\/center>","f32cd679":"It is commonly believed that a person's salary will improve depending on different factors. For example, it is said that a higher degree of formal education will positively impact your income. Likewise, people affirm that your salary will grow if you have more years of work experience. \n\nKaggle 2018 dataset contains a lot of information collected from data scientists around the world. Among all the data, there is information related with current yearly compensation, years of experience, highest level of formal education attained and age. Therefore, this dataset can give insights into the relation that exists between people salary and other information. \n\nThe goal of this job is to find a relation between data scientists yearly compensation and information such as age, years of experience and the highest level of formal education. To achieve this goal, the kaggle dataset is reorganized to get numerical predictors first. Then, data scientists are divided according to their salary in two groups: those with more and those with less salary than a provided number. This is done in order to get a classification problem with two possible classes for the target variable (yearly compensation). After that, variables that best predict the salary are found with ROC and precision-recall figures. Finally, machine learning methods, including logistic regression, are implemented to find a relation between these variables and yearly compensation.\n\nEnjoy this job!!","4b736007":"To handle a target with two possible classification classes, scientists are divided in two groups. The first and the second group includes people with yearly compensation lower and greater than a cut yearly compensation (in this case 150 000), respectively. ","c73aca2d":"<a id='vis'><\/a>\n# Visualize data"}}