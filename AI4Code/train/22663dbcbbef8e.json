{"cell_type":{"ebc12345":"code","37bd9b50":"code","da532171":"code","541c9895":"code","36ff6187":"code","b7cc17da":"code","2c2b2008":"code","bed1154c":"code","e3a5d1a9":"code","d62f9e49":"code","fc2578ab":"code","f3966e3a":"code","59368680":"code","d11cdf15":"code","23be3aba":"code","e53793e3":"code","2bcb9e33":"code","2bcab808":"code","64b837b1":"code","f31be4aa":"code","8c5c8510":"code","f394a135":"code","1877c335":"code","02f15d35":"code","3bbf6923":"code","2b46a141":"code","c6af2396":"code","75ce2bc3":"code","136f62c4":"code","e08b0e8e":"code","9d87e372":"code","813a7d4a":"code","80e4dd64":"code","41ab021a":"code","2b2e82bc":"code","6dcbd7f1":"code","cbb39aa6":"code","dfbfeea5":"code","99152338":"code","1aed147d":"code","249c584f":"code","27943b85":"code","a50c2196":"code","b1a1b75c":"code","8ae9f1ac":"code","c203e214":"code","f88b9511":"code","e85f36be":"code","b9e25c1c":"code","ae0b323b":"code","1334eec7":"code","2d0f5c80":"code","f07de56b":"code","69a68f3e":"code","f4da2a96":"code","5d0859e9":"code","7b450c24":"code","0c6745e3":"code","25d3302c":"code","5cd878a6":"code","75590703":"code","2dc9a353":"code","84a94b15":"code","a0e41560":"code","49cb390f":"markdown","5b342590":"markdown","a34d90bf":"markdown"},"source":{"ebc12345":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37bd9b50":"import transformers \nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n\nimport torch \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nfrom pylab import rcParams \nimport matplotlib.pyplot as plt \nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report \nfrom collections import defaultdict\nfrom textwrap import wrap \n\n\nfrom torch import nn, optim \nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F","da532171":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale = 1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#ADFF02\", \"#8F00FF\"]\n\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","541c9895":"train = pd.read_csv('\/kaggle\/input\/ag-news-classification-dataset\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/ag-news-classification-dataset\/test.csv')\ntrain.head()","36ff6187":"#shape \ntrain.shape, test.shape","b7cc17da":"train.info()","2c2b2008":"test.head()","bed1154c":"labeling = {\n    1:0, \n    2:1,\n    3:2,\n    4:3\n}","e3a5d1a9":"train['Class Index'] = train['Class Index'].apply(lambda x : labeling[x])\ntest['Class Index'] = test['Class Index'].apply(lambda x: labeling[x])\n","d62f9e49":"train.head()","fc2578ab":"test.head()","f3966e3a":"sns.countplot(train['Class Index'])","59368680":"# DATA PREPROCESSING \nclass_names = ['1', '2', '3', '4']","d11cdf15":"pre_trained_model_name = 'bert-base-cased'","23be3aba":"tokenizer = BertTokenizer.from_pretrained(pre_trained_model_name)","e53793e3":"small_text = \"This data is really really really huge but this time I will do it in detail\"\n","2bcb9e33":"tokens = tokenizer.tokenize(small_text)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f'Sentence: {small_text}')\nprint(f'Tokens : {tokens}')\nprint(f'Token-IDs : {token_ids}')","2bcab808":"# SPECIAL TOKENS \n\ntokenizer.sep_token, tokenizer.sep_token_id","64b837b1":"tokenizer.cls_token, tokenizer.cls_token_id","f31be4aa":"tokenizer.pad_token, tokenizer.pad_token_id","8c5c8510":"tokenizer.unk_token, tokenizer.unk_token_id","f394a135":"# ALL of the above work can be done with simple encode_plus() methods \n\nencoding = tokenizer.encode_plus(\n    small_text,\n    max_length=32, \n    add_special_tokens=True,\n    return_token_type_ids=False, \n    padding='max_length', \n    return_attention_mask=True, \n    return_tensors='pt'\n)\nencoding.keys()","1877c335":"print(len(encoding['input_ids'][0]))\nencoding['input_ids'][0]","02f15d35":"print(len(encoding['attention_mask'][0]))\nencoding['attention_mask']","3bbf6923":"print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))","2b46a141":"# Choosing Sequence Length \ntoken_lens = []\ntrain['content'] = train['Title']+' '+train['Description']\ndel train['Title']\ndel train['Description']\n","c6af2396":"for txt in train.content:\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))","75ce2bc3":"sns.distplot(token_lens)\nplt.xlim([0, 256])\nplt.xlabel('Token COunt')","136f62c4":"MAX_LEN = 100","e08b0e8e":"class AgNewsData(Dataset):\n    \n    def __init__(self, content, targets, tokenizer, max_len):\n        self.content = content\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len \n        \n    def __len__(self):\n        return len(self.content)\n    \n    def __getitem__(self, item):\n        content = str(self.content[item])\n        target = self.targets[item]\n        \n        encoding = self.tokenizer.encode_plus(\n            content, \n            max_length=self.max_len, \n            add_special_tokens=True,\n            return_token_type_ids=False, \n            padding=\"max_length\",\n            truncation = True,\n            return_attention_mask=True, \n            return_tensors='pt'\n        )\n        \n        return {\n            'content_text':content, \n            'input_ids':encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets':torch.tensor(target, dtype=torch.long)\n        }","9d87e372":"df_train, df_val = train_test_split(train, test_size=0.1, random_state=RANDOM_SEED)","813a7d4a":"df_train.shape, test.shape, df_val.shape","80e4dd64":"test['content'] = test['Title']+\" \"+test['Description']\ndel test['Title']\ndel test['Description']","41ab021a":"df_test= test\ndel test","2b2e82bc":"df_test.head()","6dcbd7f1":"# helper function to create dataloaders \n\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = AgNewsData(\n        content = df.content.to_numpy(),\n        targets = df['Class Index'].to_numpy(),\n        tokenizer = tokenizer, \n        max_len = max_len\n        \n    )\n    \n    \n    return DataLoader(\n        ds, \n        batch_size = batch_size, \n        num_workers = 4\n    )","cbb39aa6":"BATCH_SIZE = 16\n\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","dfbfeea5":"data = next(iter(train_data_loader))\ndata.keys()","99152338":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","1aed147d":"bert_model = BertModel.from_pretrained(pre_trained_model_name)\n\no=bert_model(\n    input_ids = encoding['input_ids'],\n    attention_mask = encoding['attention_mask']\n)","249c584f":"o.keys()","27943b85":"o.get('last_hidden_state').shape\n","a50c2196":"o.get('pooler_output').shape\n","b1a1b75c":"bert_model.config.hidden_size","8ae9f1ac":"class SentimentClassifier(nn.Module):\n    \n    \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(pre_trained_model_name)\n        self.drop = nn.Dropout(p=0.45)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        o = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask\n        )\n        \n        output = self.drop(o.get('pooler_output'))\n        \n        return self.out(output)","c203e214":"model = SentimentClassifier(len(class_names))\nmodel = model.to(device)","f88b9511":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\n\nprint(input_ids.shape)\nprint(attention_mask.shape)","e85f36be":"F.softmax(model(input_ids, attention_mask), dim=1)","b9e25c1c":"# TRAINING \n\nEPOCHS = 1 \noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0,\n    num_training_steps = total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","ae0b323b":"def train_epoch(\n    model, \n    data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler,\n    n_examples\n):\n    model = model.train()\n    \n    losses= []\n    correct_predictions = 0 \n    \n    for d in data_loader:\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        outputs = model(\n            input_ids = input_ids, \n            attention_mask = attention_mask\n        )\n        \n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n\n        nn.utils.clip_grad_norm(model.parameters(), max_norm = 1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n    \n    \n    return correct_predictions.double()\/n_examples, np.mean(losses)","1334eec7":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    \n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            targets = d['targets'].to(device)\n            outputs = model(\n                input_ids = input_ids, \n                attention_mask = attention_mask\n            )\n\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            \n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n        \n    return correct_predictions.double() \/ n_examples , np.mean(losses)\n        ","2d0f5c80":"%%time \nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    \n    print(f'Epoch {epoch+1}\/{EPOCHS}')\n    print('-'*10)\n    \n    \n    train_acc, train_loss = train_epoch(\n         model, \n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(df_train)\n    )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    \n    val_acc, val_loss = eval_model(\n        model, \n        val_data_loader,\n        loss_fn,\n        device,\n        len(df_val)\n    )\n    \n    print(f'Val loss {val_loss} val accuracy {val_acc}')\n    print()\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc\n","f07de56b":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","69a68f3e":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\ntest_acc.item()","f4da2a96":"def get_predictions(model, data_loader):\n  model = model.eval()\n  \n  content_texts = []\n  predictions = []\n  prediction_probs = []\n  real_values = []\n\n  with torch.no_grad():\n    for d in data_loader:\n\n      texts = d[\"content_text\"]\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n\n      probs = F.softmax(outputs, dim=1)\n\n      content_texts.extend(texts)\n      predictions.extend(preds)\n      prediction_probs.extend(probs)\n      real_values.extend(targets)\n\n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  real_values = torch.stack(real_values).cpu()\n  return content_texts, predictions, prediction_probs, real_values","5d0859e9":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","7b450c24":"print(classification_report(y_test, y_pred, target_names=class_names))\n","0c6745e3":"\ndef show_confusion_matrix(confusion_matrix):\n  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n  plt.ylabel('True sentiment')\n  plt.xlabel('Predicted sentiment');\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","25d3302c":"idx = 2\n\ncontent_text = y_review_texts[idx]\ntrue_sentiment = y_test[idx]\npred_df = pd.DataFrame({\n  'class_names': class_names,\n  'values': y_pred_probs[idx]\n})","5cd878a6":"print(\"\\n\".join(wrap(content_text)))\nprint()\nprint(f'True sentiment: {class_names[true_sentiment]}')","75590703":"sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\nplt.ylabel('sentiment')\nplt.xlabel('probability')\nplt.xlim([0, 1]);","2dc9a353":"content_text = 'Woah what a match! .. I love the way they played but my team failed'","84a94b15":"encoded_review = tokenizer.encode_plus(\n  content_text,\n    max_length=100, \n    add_special_tokens=True,\n    return_token_type_ids=False, \n    padding='max_length', \n    return_attention_mask=True, \n    return_tensors='pt'\n)\n","a0e41560":"input_ids = encoded_review['input_ids'].to(device)\nattention_mask = encoded_review['attention_mask'].to(device)\n\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\n\nprint(f'Review text: {content_text}')\nprint(f'Sentiment  : {class_names[prediction]}')","49cb390f":"# Used BERT to classify the texts into 4 classes","5b342590":"## EVALUATION","a34d90bf":"# PREDICTING WITH RAW TEXT "}}