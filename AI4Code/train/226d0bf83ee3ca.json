{"cell_type":{"b96b5f3a":"code","008177ed":"code","fe6bcc22":"code","c70e9c25":"code","7c23f1db":"code","278f0c87":"code","b2efdf6a":"code","2298d8bd":"code","30322af3":"code","df9bff23":"code","f800f676":"code","73859d18":"code","67419f31":"code","42c0cef7":"code","681ba078":"code","2d022250":"code","3106311c":"code","a05283ce":"code","18cf02c7":"code","0550f8fd":"code","65f9ed5f":"code","7c346f03":"markdown","7fd98e49":"markdown","41de324d":"markdown","d9ce99dd":"markdown","ea32e73b":"markdown","f1b30981":"markdown","28ea45cf":"markdown","6868aeac":"markdown","16358dc5":"markdown","d25df518":"markdown","58e8db88":"markdown","ae02eb2c":"markdown","386543fd":"markdown","17834ef3":"markdown","1298b0b4":"markdown","fc96d27c":"markdown","4c9fe909":"markdown","67e84381":"markdown","445c88dd":"markdown","8435e6b5":"markdown","e2e9a38d":"markdown","8821479a":"markdown"},"source":{"b96b5f3a":"import numpy as np, pandas as pd, os\nnp.random.seed(300)\n\n# GENERATE RANDOM DATA\ndata = pd.DataFrame(np.zeros((20000,300)))\nfor i in range(300): data.iloc[:,i] = np.random.normal(0,1,20000)\n\n# SET TARGET AS LINEAR COMBINATION OF 50 A'S PLUS NOISE \nimportant = 35; noise = 3.5\na = np.random.normal(0,1,300)\nx = np.random.choice(np.arange(300),300-important,replace=False); a[x] = 0\ndata['target'] = data.values.dot(a) + np.random.normal(0,noise,20000)\n\n# MAKE 64% TARGET=1, 36% TARGET=0\ndata.sort_values('target',inplace=True)\ndata.iloc[:7200,300] = 0.0\ndata.iloc[7200:,300] = 1.0\n\n# RANDOMLY SELECT TRAIN, PUBLIC, PRIVATE\ntrain = data.sample(250)\npublic = data[ ~data.index.isin(train.index) ].sample(1975)\nprivate = data[ ~data.index.isin(train.index) & ~data.index.isin(public.index) ].sample(frac=1) \n\n# RESET INDICES\ntrain.reset_index(drop=True,inplace=True)\npublic.reset_index(drop=True,inplace=True)\nprivate.reset_index(drop=True,inplace=True)","008177ed":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\noof = np.zeros(len(train))\npredsPU= np.zeros(len(public))\npredsPR= np.zeros(len(private))\nrskf = RepeatedStratifiedKFold(n_splits=25, n_repeats=5)\nfor train_index, test_index in rskf.split(train.iloc[:,:-1], train['target']):\n    clf = LogisticRegression(solver='liblinear',penalty='l2',C=1.0,class_weight='balanced')\n    clf.fit(train.loc[train_index].iloc[:,:-1],train.loc[train_index]['target'])\n    oof[test_index] += clf.predict_proba(train.loc[test_index].iloc[:,:-1])[:,1]\n    predsPU += clf.predict_proba(public.iloc[:,:-1])[:,1]\n    predsPR += clf.predict_proba(private.iloc[:,:-1])[:,1]\naucTR = round(roc_auc_score(train['target'],oof),5)\naucPU = round(roc_auc_score(public['target'],predsPU),5)\naucPR = round(roc_auc_score(private['target'],predsPR),5)\nprint('LR Model with L2-penalty: CV =',aucTR,'LB =',aucPU,'Private score =',aucPR)","fe6bcc22":"oof = np.zeros(len(train))\npredsPU= np.zeros(len(public))\npredsPR= np.zeros(len(private))\nrskf = RepeatedStratifiedKFold(n_splits=25, n_repeats=5)\nfor train_index, test_index in rskf.split(train.iloc[:,:-1], train['target']):\n    clf = LogisticRegression(solver='liblinear',penalty='l1',C=0.1,class_weight='balanced')\n    clf.fit(train.loc[train_index].iloc[:,:-1],train.loc[train_index]['target'])\n    oof[test_index] += clf.predict_proba(train.loc[test_index].iloc[:,:-1])[:,1]\n    predsPU += clf.predict_proba(public.iloc[:,:-1])[:,1]\n    predsPR += clf.predict_proba(private.iloc[:,:-1])[:,1]\naucTR = round(roc_auc_score(train['target'],oof),5)\naucPU = round(roc_auc_score(public['target'],predsPU),5)\naucPR = round(roc_auc_score(private['target'],predsPR),5)\nprint('LR Model with L1-penalty: CV =',aucTR,'LB =',aucPU,'Private score =',aucPR)","c70e9c25":"# START WITH BEST TRAINING DATA MODEL\nclf = LogisticRegression(solver='liblinear',penalty='l1',C=0.1, class_weight='balanced')\nclf.fit(train.iloc[:,:-1],train['target']) \nu0 = clf.coef_[0]\nu0 = u0 \/ np.sqrt(u0.dot(u0))\n\n# INITIAL SCORES\naucPU = round(roc_auc_score(public['target'],u0.dot(public.iloc[:,:-1].values.transpose())),5)\naucPR = round(roc_auc_score(private['target'],u0.dot(private.iloc[:,:-1].values.transpose())),5)\nbestPU = aucPU; currentPR = aucPR; initial = u0.copy()\nprint('Our starting model has LB =',aucPU,'and Private score =',aucPR)\n\n# ACCELERATE RANDOM SEARCH BY NEGLECTING 250 LEAST IMPORTANT VARIABLES FROM TRAINING DATA\ndf = pd.DataFrame({'var':np.arange(300),'CV':np.zeros(300),'diff':np.zeros(300)})\nfor i in range(300):\n    df.loc[i,'CV'] = roc_auc_score(train['target'],train[i])\n    df.loc[i,'diff'] = abs(df.loc[i,'CV']-0.5)\ndf.sort_values('diff',inplace=True,ascending=False)","7c23f1db":"u0 = initial\nangDeg = 5\nangRad = 2*np.pi*angDeg\/360\n\nnp.random.seed(42)\nfor k in range(150):\n    # CHOOSE RANDOM SEARCH DIRECTION\n    u1 = np.random.normal(0,1,300)\n    # REMOVE 250 UNIMPORTANT DIMENSIONS\n    u1[ df.iloc[100:,0] ] = 0.0\n    # ROTATE 5 DEGREES IN THIS NEW DIRECTION\n    u1 = u1 - u1.dot(u0)*u0\n    u1 = u1 \/ np.sqrt(u1.dot(u1))\n    u2 = u0*np.cos(angRad) + u1*np.sin(angRad)\n    # CALCULATE LB AND PRIVATE SCORE\n    aucPU = round(roc_auc_score(public['target'],u2.dot(public.iloc[:,:-1].values.transpose())),5)\n    aucPR = round(roc_auc_score(private['target'],u2.dot(private.iloc[:,:-1].values.transpose())),5)\n    # IF SCORE INCREASES PRINT RESULTS\n    if (aucPU>bestPU)|(k==0):\n        bestPU = aucPU\n        currentPR = aucPR\n        u0 = u2.copy()\n        print('Submission',k+1,': Best LB =',bestPU,'and Private score =',currentPR)","278f0c87":"df = pd.DataFrame({'var':np.arange(300),'CV':np.zeros(300),'diff':np.zeros(300)})\nfor i in range(300):\n    df.loc[i,'CV'] = roc_auc_score(train['target'],train[i])\n    df.loc[i,'diff'] = abs(df.loc[i,'CV']-0.5)\nprint('We need to LB probe',len(df.loc[ df['diff']>0.04 ,'CV']),'variables')","b2efdf6a":"# LB PROBE 100 VARIABLES STARTING WITH MOST IMPORTANT CV SCORE\ndf.sort_values('diff',inplace=True,ascending=False)\nLBprobe = list(df.loc[ df['diff']>0.04, 'var'])\ndf.sort_values('var',inplace=True)\n\n# INITIALIZE VARIABLES\ndf['LB'] = 0.5; df['A'] = 0; ct=0\n\n# PERFORM LB PROBING TO DETERMINE A_K'S\nkeep = []\nfor i in LBprobe:\n    ct += 1; found = True\n    # CALCUATE LB SCORE FOR VAR_K\n    df.loc[i,'LB'] = roc_auc_score(public['target'],public[i])\n    if (df.loc[i,'LB']<0.47) | (df.loc[i,'LB']>0.53): keep.append(i) \n    else: found = False\n    # UPDATE A_K'S\n    df.loc[keep,'A'] = (8\/9)*df.loc[keep,'LB']+(1\/9)*df.loc[keep,'CV']-0.5\n    # PREDICT PUBLIC\n    predPU = df['A'].values.dot(public.iloc[:,:300].values.transpose())\n    aucPU = round( roc_auc_score(public['target'],predPU) ,3)\n    # PREDICT PRIVATE\n    predPR = df['A'].values.dot(private.iloc[:,:300].values.transpose())\n    aucPR = round( roc_auc_score(private['target'],predPR) ,3)\n    # DISPLAY CURRENT LB AND PRIVATE SCORE\n    if found: print('Submission',ct,': Best LB =',aucPU,'and Private score ='\n            ,aucPR,'with',len(keep),'keep')","2298d8bd":"train = pd.read_csv('..\/input\/train.csv')\ndf = pd.DataFrame({'var':np.arange(300),'CV':np.zeros(300),'diff':np.zeros(300),'LB':0.5*np.ones(300)})\nfor i in range(300):\n    df.loc[i,'CV'] = roc_auc_score(train['target'],train[str(i)])\n    df.loc[i,'diff'] = abs(df.loc[i,'CV']-0.5)\ndf.sort_values('diff',inplace=True,ascending=False)\ndf.head()","30322af3":"df.loc[ df['var']==33, 'LB' ] = 0.671\ndf.loc[ df['var']==65, 'LB' ] = 0.671\ndf.loc[ df['var']==217, 'LB' ] = 0.382\ndf.loc[ df['var']==117, 'LB' ] = 0.405\ndf.loc[ df['var']==91, 'LB' ] = 0.382","df9bff23":"df.loc[ df['var']==295, 'LB' ] = 0.506\ndf.loc[ df['var']==24, 'LB' ] = 0.501\ndf.loc[ df['var']==199, 'LB' ] = 0.613\ndf.loc[ df['var']==80, 'LB' ] = 0.483\ndf.loc[ df['var']==73, 'LB' ] = 0.394","f800f676":"df.loc[ df['var']==194, 'LB' ] = 0.472\ndf.loc[ df['var']==189, 'LB' ] = 0.454\ndf.loc[ df['var']==16, 'LB' ] = 0.437\ndf.loc[ df['var']==183, 'LB' ] = 0.506\ndf.loc[ df['var']==82, 'LB' ] = 0.494","73859d18":"df.loc[ df['var']==258, 'LB' ] = 0.474\ndf.loc[ df['var']==63, 'LB' ] = 0.428\ndf.loc[ df['var']==298, 'LB' ] = 0.466\ndf.loc[ df['var']==201, 'LB' ] = 0.489\ndf.loc[ df['var']==165, 'LB' ] = 0.498","67419f31":"df.loc[ df['var']==133, 'LB' ] = 0.486\ndf.loc[ df['var']==209, 'LB' ] = 0.431\ndf.loc[ df['var']==164, 'LB' ] = 0.542\ndf.loc[ df['var']==129, 'LB' ] = 0.506\ndf.loc[ df['var']==134, 'LB' ] = 0.471","42c0cef7":"df.loc[ df['var']==226, 'LB' ] = 0.492\ndf.loc[ df['var']==237, 'LB' ] = 0.474\ndf.loc[ df['var']==39, 'LB' ] = 0.488\ndf.loc[ df['var']==17, 'LB' ] = 0.508\ndf.loc[ df['var']==30, 'LB' ] = 0.494","681ba078":"df.loc[ df['var']==114, 'LB' ] = 0.479\ndf.loc[ df['var']==272, 'LB' ] = 0.471\ndf.loc[ df['var']==108, 'LB' ] = 0.459\ndf.loc[ df['var']==220, 'LB' ] = 0.511\ndf.loc[ df['var']==150, 'LB' ] = 0.482","2d022250":"df.loc[ df['var']==230, 'LB' ] = 0.482\ndf.loc[ df['var']==90, 'LB' ] = 0.486\ndf.loc[ df['var']==289, 'LB' ] = 0.482\ndf.loc[ df['var']==241, 'LB' ] = 0.494\ndf.loc[ df['var']==4, 'LB' ] = 0.516","3106311c":"df.loc[ df['var']==43, 'LB' ] = 0.474\ndf.loc[ df['var']==239, 'LB' ] = 0.457\ndf.loc[ df['var']==127, 'LB' ] = 0.501\ndf.loc[ df['var']==45, 'LB' ] = 0.449\ndf.loc[ df['var']==151, 'LB' ] = 0.504","a05283ce":"df.loc[ df['var']==244, 'LB' ] = 0.509\ndf.loc[ df['var']==26, 'LB' ] = 0.511\ndf.loc[ df['var']==105, 'LB' ] = 0.505\ndf.loc[ df['var']==176, 'LB' ] = 0.525\ndf.loc[ df['var']==101, 'LB' ] = 0.535","18cf02c7":"df['A'] = 0\ndf['A'] = (8\/9)*df['LB'] + (1\/9)*df['CV'] - 0.500\nkeep_threshold = 0.04 # YIELDS 15 NON-ZEROS A'S\ndf.loc[ abs(df['A'])<keep_threshold , 'A' ] = 0\ndf.sort_values('var',inplace=True)\nfor i in range(300):\n    if df.loc[i,'LB'] != 0.500:           \n        print('A_'+str(i)+' = ',round(df.loc[i,'A'],6))","0550f8fd":"test = pd.read_csv('..\/input\/test.csv')\npred = test.iloc[:,1:].values.dot(df['A'].values)\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = pred\nsub.to_csv('submission.csv',index=False)","65f9ed5f":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nidx = df.loc[ df['A']!=0, 'var' ].values\nidx = np.sort(idx); idx2 = []\nfor i in idx: idx2.append(str(i))\n\nplt.figure(figsize=(15,15))\nsns.heatmap(train[idx2+['target']].corr(), cmap='RdBu_r', annot=True, center=0.0)\nplt.title('Correlation Among Useful Variables',fontsize=20)\nplt.show()","7c346f03":"# Model Two : Training Data L1-Hyperplane\nIf we only use the training data and build a model with logistic regression, L1-penalty, and stratified k-folds, we achieve CV 0.846, LB 0.841 and Private score 0.837 on our synthetic dataset.","7fd98e49":"# Model Three : Public Data L2-Hyperplane Search\nWe will now use random LB probing to increase our private score. We will begin with our best L1-hyperplane from our training data model and then we will randomly rotate it 5 degrees and resubmit. If the LB score increases, we will retain the new hyperplane otherwise we will keep the old. We will repeat this process for 30 days (150 submissions). \n\nThis process is using a random search to find the best L2-Hyperplane on the public test dataset. Notice that it beats Models One and Two which only use the training dataset. We can accelerate this method by only rotating the hyperplane through variable dimensions that have proven important on the training data. (This accelerant is like applying some L1 penalty from the training model)\n\nAfter 1 month (150 submissions), this method achieves an LB score 0.885 and Private score 0.859 on our synthetic dataset.","41de324d":"## Day 4","d9ce99dd":"## Day 8","ea32e73b":"Now let's LB probe the most important 100 variables (all variables with CV<0.46 or CV>0.54). Note if a variable has CV near 0.50 then we assume that that variable has no correlation with target and we set `a_k=0`. (To be more careful, we could check all 300 and acheive a slightly higher private score.) If CV<0.46, then that variable is negatively correlated with target. If CV>0.54, then that variable is positively correlated with target. (To learn more about why CV<0.46 and CV>0.54 are important, read [here][1]).  \n  \n[1]: https:\/\/www.kaggle.com\/cdeotte\/can-we-trust-cv-and-lb","f1b30981":"## Day 2\nThe next five variables with the most potential are x295, x24, x199, x80, and x73. Let's check these. We discover that only x199 and x73 are **useful** (have non-zero coefficients). Most people included x295 in their feature selection but we see here that it is a **useless** variable.  \n  \n![image](http:\/\/playagricola.com\/Kaggle\/sub295.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub24.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub199.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub80.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub73.jpg)","28ea45cf":"## Day 6","6868aeac":"# Model Four : Public plus Training Data L1-Hyperplane\nWe will now use the SECRET FORMULA! Note that this technique is similar to fitting a logistic regression model with L1-penalty on the public test dataset plus training dataset combined! First let's calcuate every variable's CV because we will only LB probe the best 100 variables (and ignore the other 200). This saves us 200 probes or 40 days time.\n\nAfter 20 days (100 submissions), this method achieves an LB score 0.920 and Private score 0.903 on our synthetic dataset! Wow!!","16358dc5":"# Real Data with 10 Days\nThe full technique requires 20 days; 100 submissions to check the 100 most important a_k. Unfortunately this technique was discovered 10 days before the competition end, so we can only probe 50 variables. None-the-less, the partial equation scores LB 0.890 and wins 2nd place! Let's calculate each variable's CV to determine the 50 variables with best potential. (After the competition ends, it would be interesting to find the full equation and evaluate its performance.)","d25df518":"Our synthetic data has 35 nonzero coefficients. This technque found the 23 most important. The other 12 coefficients are too weak to be detected through the noise. None-the-less this technque achieves a great LB 0.920 public and 0.903 private score!","58e8db88":"# Model One : Training Data L2-Hyperplane\nIf we only use the training data and build a model with logistic regression, L2-penalty, and stratified k-folds, we achieve CV 0.772, LB 0.728 and Private score 0.743 on our synthetic dataset.","ae02eb2c":"# Partial Equation after 10 Days\nBelow are the 50 variables with the best individual CV and their respective coefficient `a_k`. When we predict `test.csv`, we will assume that the other 250 `a_k` are zero. (If we had more time, we would find them instead.)  \n  \nWe observe that only 15 (of the 50) have non-zero coefficients. Among the next 50 variables with best CV, there are probably another 5 to 10. And there are probably many weak non-zero coefficients that we cannot detect. All-in-all, there are probably about 35 non-zero coefficients in the true equation (hyperplane). (This agrees with experiments [here][1]) \n  \n[1]: https:\/\/www.kaggle.com\/cdeotte\/can-we-trust-cv-and-lb","386543fd":"![image](http:\/\/playagricola.com\/Kaggle\/result5319.png)  \n  \n# Conclusion\nAfter 10 days, our LB score is 0.890. Hooray! this is a great result. This agrees with the above simulation. If we had a full 20 days, the simulation estimates that our LB could reach 0.920. (Note that we used a linear approximation to find `a_k` since `0.3 < AUC < 0.7` as explained [here][1]).\n\nKaggle's \"Don't Overfit! II\" competition was a fun challenge that could be solved in two different ways. (1) You could use only the training data and explore techniques that model small sample sizes, or (2) You could explore LB probing strategies and gain access to a larger sample. I explored both approaches. Models one and two above explore approach (1) and models three and four above explore approach (2). Because the public test dataset contains significantly more information than the training dataset, approach (2) outperforms approach (1).\n\nThank you for reading my solution to Kaggle's \"Don't Overfit II\" competition. Comments and questions welcome. I look forward to reading other Kaggler's solutions. Please share.\n\n# Correlations\nBelow is a plot of the useful variables and their correlations with each other. We can see that they are uncorrelated and most likely independent. This supports our assumptions. We also see that each is slightly correlated with the target.  \n  \n[1]: https:\/\/www.kaggle.com\/c\/dont-overfit-ii\/discussion\/92565","17834ef3":"## Day 3\n![image](http:\/\/playagricola.com\/Kaggle\/sub194.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub189.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub16.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub183.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub82.jpg)","1298b0b4":"## Predict Test and Submit\nWe will now predict `test.csv` using the hyperplane we extracted and submit to Kaggle.","fc96d27c":"# LB Probing Techniques - Don't Overfit! II - 2nd Place Solution\nIf this kernel, we discuss efficient strategies to probe the Kaggle leaderboard (LB) to gain information about the public test dataset. LR Logistic regression, SVC support vector classifiation, and LDA naive Bayes, are three methods that find linear hyperplanes to classify data. In \"Don't Overfit II\" competition, the public test dataset has 8 times more data than the training dataset. Instead of finding the hyperplane that classifies the training data, we would prefer to find the hyperplane that classifies the public test dataset. We can. Let's assume\n\n$$\\text{target} = \\text{Heaviside}(a_0x_0+a_1x_1+...a_{298}x_{298}+a_{299}x_{299}+\\text{noise})$$  \n\nOur task is to determine the 300 hyperplane coefficients `a_k`. Assuming the `a_k` are independent, then the following code extracts them from the public test dataset via LB probing:\n\n    var = 33\n    test = pd.read_csv('test.csv')\n    sub = pd.read_csv('sample_submission.csv')\n    sub['target'] = test[str(var)]\n    sub.to_csv('submission'+str(var)+'.csv',index=False) \n   \n![image](http:\/\/playagricola.com\/Kaggle\/sub33.jpg)\n\nThen the value of `a_k` is the just `LB_SCORE_K` minus 0.500. For example  \n  \n$$a_{33} = \\text{LB_SCORE}_{33} - 0.500 = 0.671 - 0.500 = 0.171 $$\n\nWhen a variable is negatively correlated with target (as opposed to positively correlated), then the `LB_SCORE_K` will be less than 0.500.\n  \n![image](http:\/\/playagricola.com\/Kaggle\/sub217.jpg)\n\n$$a_{217} = \\text{LB_SCORE}_{217} - 0.500 = 0.382 - 0.500 = -0.118 $$\n  \nIt's that simple! By doing this, we can recover the `a_k` in 20 days (100 submissions) with the following 3 additional tricks:\n* Only probe the 100 most important `a_k`. If `abs(CV_SCORE_K - 0.5) < 0.04` set `a_k=0` and don't probe.\n* Use train data plus public for more accuracy. Replace `LB_SCORE_K` with `(8\/9)*LB_SCORE_K + (1\/9)*CV_SCORE_K`.\n* Apply L1-penalty. If `abs(LB_SCORE_K - 0.5) < 0.04` then set `a_k=0`.  \n  \nThese additional tricks help prevent overfitting LB. Instead of modeling only the public test dataset and risk overfitting, we use information from both the training data plus public test data for a combined sample size of 2225. With this sample size, it was shown [here][1] that any variable with AUC of 0.54 or less by itself may be a **useless** variable. So we remove all variables with `abs(AUC - 0.5) < 0.04` from our model where `AUC = (8\/9)*LB_AUC + (1\/9)*CV_AUC` to prevent overfitting **useless** variables.\n\nNote that we are using a linear approximation to find `a_k` since `0.3 < AUC < 0.7` as explained [here][2].\n  \n[1]: https:\/\/www.kaggle.com\/cdeotte\/can-we-trust-cv-and-lb\n[2]: https:\/\/www.kaggle.com\/c\/dont-overfit-ii\/discussion\/92565","4c9fe909":"## Day 1\nThe five variables which show the most potential for having non-zero coefficients in the true equation are x33, x65, x217, x117, x91. We notice that variables x33 and x65 are positively correlated with target and x217, x117, and x91 are negatively correlated with target. Let's check these five. We find that all five are **useful** and have non-zero coeffients. \n  \nThe code that creates `submission_k.csv` is listed in the introduction section of this kernel.\n  \n![image](http:\/\/playagricola.com\/Kaggle\/sub33.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub65.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub217.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub117.jpg)\n![image](http:\/\/playagricola.com\/Kaggle\/sub92.jpg)","67e84381":"## Day 10","445c88dd":"## Day 5","8435e6b5":"## Day 9","e2e9a38d":"# Synthetic Dataset Experiments\nLet's observe this technique on a synthetic dataset. Then we will apply it to the real dataset. This technique is shown in `model 4` below. First we will present 3 other popular models. (The first two models use only training data while the next two models use public test data).","8821479a":"## Day 7"}}