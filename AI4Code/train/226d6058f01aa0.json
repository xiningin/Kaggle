{"cell_type":{"cda578e5":"code","ef7cb35f":"code","647551ea":"code","bc07599f":"code","1385ab86":"code","6e36f9dd":"code","cc44ef26":"code","c3b80093":"code","04a866a7":"code","1f0a42b8":"code","1f1bbdee":"code","e5d8d5f4":"markdown","a89afa92":"markdown","3c13d60d":"markdown","2eeabfad":"markdown","8c99bdc2":"markdown","e460671c":"markdown","21c22253":"markdown","e3593c56":"markdown","4a7a8235":"markdown","62978f1d":"markdown","b987b762":"markdown","14377f6a":"markdown"},"source":{"cda578e5":"## upgrading pip\n!pip install -q --upgrade pip\n\n## installing the latest transformers version from pip\n!pip install --use-feature=2020-resolver -q transformers==3.0.2\nimport transformers\n\n## installing Google Translator package\n!pip install -q googletrans\n","ef7cb35f":"## importing packages\nimport gc\nimport os\nimport random\nimport transformers\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom googletrans import Translator\nfrom pathlib import Path\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import AutoTokenizer, TFAutoModel\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Transformers version: {transformers.__version__}\")\n\nwarnings.filterwarnings(\"ignore\")\n","647551ea":"## defining configuration\nclass Configuration():\n    \"\"\"\n    All configuration for running an experiment\n    \"\"\"\n    def __init__(\n        self,\n        model_name,\n        translation = True,\n        max_length = 64,\n        padding = True,\n        batch_size = 128,\n        epochs = 5,\n        learning_rate = 1e-5,\n        metrics = [\"sparse_categorical_accuracy\"],\n        verbose = 1,\n        train_splits = 5,\n        accelerator = \"TPU\",\n        myluckynumber = 13\n    ):\n        # seed and accelerator\n        self.SEED = myluckynumber\n        self.ACCELERATOR = accelerator\n\n        # paths\n        self.PATH_TRAIN = Path(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\n        self.PATH_TEST  = Path(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")\n\n        # splits\n        self.TRAIN_SPLITS = train_splits\n\n        # mapping of language\n        self.LANGUAGE_MAP = {\n            \"English\"   : 0,\n            \"Chinese\"   : 1,\n            \"Arabic\"    : 2,\n            \"French\"    : 3,\n            \"Swahili\"   : 4,\n            \"Urdu\"      : 5,\n            \"Vietnamese\": 6,\n            \"Russian\"   : 7,\n            \"Hindi\"     : 8,\n            \"Greek\"     : 9,\n            \"Thai\"      : 10,\n            \"Spanish\"   : 11,\n            \"German\"    : 12,\n            \"Turkish\"   : 13,\n            \"Bulgarian\" : 14\n        }\n\n        self.INVERSE_LANGUAGE_MAP = {v: k for k, v in self.LANGUAGE_MAP.items()}\n\n        # model configuration\n        self.MODEL_NAME = model_name\n        self.TRANSLATION = translation\n        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_NAME)\n\n        # model hyperparameters\n        self.MAX_LENGTH = max_length\n        self.PAD_TO_MAX_LENGTH = padding\n        self.BATCH_SIZE = batch_size\n        self.EPOCHS = epochs\n        self.LEARNING_RATE = learning_rate\n        self.METRICS = metrics\n        self.VERBOSE = verbose\n        \n        # initializing accelerator\n        self.initialize_accelerator()\n\n    def initialize_accelerator(self):\n        \"\"\"\n        Initializing accelerator\n        \"\"\"\n        # checking TPU first\n        if self.ACCELERATOR == \"TPU\":\n            print(\"Connecting to TPU\")\n            try:\n                tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n                print(f\"Running on TPU {tpu.master()}\")\n            except ValueError:\n                print(\"Could not connect to TPU\")\n                tpu = None\n\n            if tpu:\n                try:\n                    print(\"Initializing TPU\")\n                    tf.config.experimental_connect_to_cluster(tpu)\n                    tf.tpu.experimental.initialize_tpu_system(tpu)\n                    self.strategy = tf.distribute.experimental.TPUStrategy(tpu)\n                    self.tpu = tpu\n                    print(\"TPU initialized\")\n                except _:\n                    print(\"Failed to initialize TPU\")\n            else:\n                print(\"Unable to initialize TPU\")\n                self.ACCELERATOR = \"GPU\"\n\n        # default for CPU and GPU\n        if self.ACCELERATOR != \"TPU\":\n            print(\"Using default strategy for CPU and single GPU\")\n            self.strategy = tf.distribute.get_strategy()\n\n        # checking GPUs\n        if self.ACCELERATOR == \"GPU\":\n            print(f\"GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n\n        # defining replicas\n        self.AUTO = tf.data.experimental.AUTOTUNE\n        self.REPLICAS = self.strategy.num_replicas_in_sync\n        print(f\"REPLICAS: {self.REPLICAS}\")\n","bc07599f":"## data preparation functions\ndef translate_text_to_english(text):\n    \"\"\"\n    Translates text to English.\n    \"\"\"\n    translator = Translator()\n\n    return translator.translate(text, dest = \"en\").text\n\n\ndef encode_text(df, tokenizer, max_len, padding):\n    \"\"\"\n    Preprocessing textual data into encoded tokens.\n    \"\"\"\n    text = df[[\"premise\", \"hypothesis\"]].values.tolist()\n\n    # encoding text using tokenizer of the model\n    text_encoded = tokenizer.batch_encode_plus(\n        text,\n        pad_to_max_length = padding,\n        max_length = max_len\n    )\n\n    return text_encoded\n\n\ndef get_tf_dataset(X, y, auto, labelled = True, repeat = False, shuffle = False, batch_size = 128):\n    \"\"\"\n    Creating tf.data.Dataset for TPU.\n    \"\"\"\n    if labelled:\n        ds = (tf.data.Dataset.from_tensor_slices((X[\"input_ids\"], y)))\n    else:\n        ds = (tf.data.Dataset.from_tensor_slices(X[\"input_ids\"]))\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(2048)\n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(auto)\n\n    return ds\n","1385ab86":"## building model\ndef build_model(model_name, max_len, learning_rate, metrics):\n    \"\"\"\n    Building the Deep Learning architecture\n    \"\"\"\n    # defining encoded inputs\n    input_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_ids\")\n    \n    # defining transformer model embeddings\n    transformer_model = TFAutoModel.from_pretrained(model_name)\n    transformer_embeddings = transformer_model(input_ids)[0]\n\n    # defining output layer\n    output_values = Dense(3, activation = \"softmax\")(transformer_embeddings[:, 0, :])\n\n    # defining model\n    model = Model(inputs = input_ids, outputs = output_values)\n    opt = Adam(learning_rate = learning_rate)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n    metrics = metrics\n\n    model.compile(optimizer = opt, loss = loss, metrics = metrics)\n\n    return model\n","6e36f9dd":"## stratified k-fold over language and label\ndef run_model(config):\n    \"\"\"\n    Running the model\n    \"\"\"\n    ## reading data\n    df_train = pd.read_csv(config.PATH_TRAIN)\n    df_test = pd.read_csv(config.PATH_TEST)\n    \n    # translating non-English text to English\n    if config.TRANSLATION:\n        df_train.loc[df_train.language != \"English\", \"premise\"] = df_train[df_train.language != \"English\"].premise.apply(lambda x: translate_text_to_english(x))\n        df_test.loc[df_test.language != \"English\", \"premise\"] = df_test[df_test.language != \"English\"].premise.apply(lambda x: translate_text_to_english(x))\n\n        df_train.loc[df_train.language != \"English\", \"hypothesis\"] = df_train[df_train.language != \"English\"].hypothesis.apply(lambda x: translate_text_to_english(x))\n        df_test.loc[df_test.language != \"English\", \"hypothesis\"] = df_test[df_test.language != \"English\"].hypothesis.apply(lambda x: translate_text_to_english(x))\n\n    # adding column for stratified splitting\n    df_train[\"language_label\"] = df_train.language.astype(str) + \"_\" + df_train.label.astype(str)\n\n    # stratified K-fold on language and label\n    skf = StratifiedKFold(n_splits = config.TRAIN_SPLITS, shuffle = True, random_state = config.SEED)\n\n    # initializing predictions\n    preds_oof = np.zeros((df_train.shape[0], 3))\n    preds_test = np.zeros((df_test.shape[0], 3))\n    acc_oof = []\n\n    # iterating over folds\n    for (fold, (train_index, valid_index)) in enumerate(skf.split(df_train, df_train.language_label)):\n        # initializing TPU\n        if config.ACCELERATOR == \"TPU\":\n            if config.tpu:\n                config.initialize_accelerator()\n\n        # building model\n        K.clear_session()\n        with config.strategy.scope():\n            model = build_model(config.MODEL_NAME, config.MAX_LENGTH, config.LEARNING_RATE, config.METRICS)\n            if fold == 0:\n                print(model.summary())\n\n        print(\"\\n\")\n        print(\"#\" * 19)\n        print(f\"##### Fold: {fold + 1} #####\")\n        print(\"#\" * 19)\n\n        # splitting data into training and validation\n        X_train = df_train.iloc[train_index]\n        X_valid = df_train.iloc[valid_index]\n\n        y_train = X_train.label.values\n        y_valid = X_valid.label.values\n\n        print(\"\\nTokenizing\")\n\n        # encoding text data using tokenizer\n        X_train_encoded = encode_text(df = X_train, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n        X_valid_encoded = encode_text(df = X_valid, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n\n        # creating TF Dataset\n        ds_train = get_tf_dataset(X_train_encoded, y_train, config.AUTO, repeat = True, shuffle = True, batch_size = config.BATCH_SIZE * config.REPLICAS)\n        ds_valid = get_tf_dataset(X_valid_encoded, y_valid, config.AUTO, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n\n        n_train = X_train.shape[0]\n\n        if fold == 0:\n            X_test_encoded = encode_text(df = df_test, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n\n        # saving model at best accuracy epoch\n        sv = tf.keras.callbacks.ModelCheckpoint(\n            \"model.h5\",\n            monitor = \"val_sparse_categorical_accuracy\",\n            verbose = 0,\n            save_best_only = True,\n            save_weights_only = True,\n            mode = \"max\",\n            save_freq = \"epoch\"\n        )\n\n        print(\"\\nTraining\")\n\n        # training model\n        model_history = model.fit(\n            ds_train,\n            epochs = config.EPOCHS,\n            callbacks = [sv],\n            steps_per_epoch = n_train \/ config.BATCH_SIZE \/\/ config.REPLICAS,\n            validation_data = ds_valid,\n            verbose = config.VERBOSE\n        )\n\n        print(\"\\nValidating\")\n\n        # scoring validation data\n        model.load_weights(\"model.h5\")\n        ds_valid = get_tf_dataset(X_valid_encoded, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n\n        preds_valid = model.predict(ds_valid, verbose = config.VERBOSE)\n        acc = accuracy_score(y_valid, np.argmax(preds_valid, axis = 1))\n\n        preds_oof[valid_index] = preds_valid\n        acc_oof.append(acc)\n\n        print(\"\\nInferencing\")\n\n        # scoring test data\n        ds_test = get_tf_dataset(X_test_encoded, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n        preds_test += model.predict(ds_test, verbose = config.VERBOSE) \/ config.TRAIN_SPLITS\n\n        print(f\"\\nFold {fold + 1} Accuracy: {round(acc, 4)}\\n\")\n\n        g = gc.collect()\n\n    # overall CV score and standard deviation\n    print(f\"\\nCV Mean Accuracy: {round(np.mean(acc_oof), 4)}\")\n    print(f\"CV StdDev Accuracy: {round(np.std(acc_oof), 4)}\\n\")\n\n    return preds_oof, preds_test\n","cc44ef26":"# Model: Bert Base Cased\n#config_1 = Configuration(\"bert-base-cased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_1, preds_test_1 = run_model(config_1)\n\n# Model: Bert Base Uncased\n#config_2 = Configuration(\"bert-base-uncased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_2, preds_test_2 = run_model(config_2)\n\n# Model: Bert Large Cased\n#config_3 = Configuration(\"bert-large-cased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_3, preds_test_3 = run_model(config_3)\n\n# Model: Bert Large Uncased\n#config_4 = Configuration(\"bert-large-uncased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_4, preds_test_4 = run_model(config_4)\n\n# Model: Bert Multilingual Base Cased\n#config_5 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_5, preds_test_5 = run_model(config_5)\n\n# Model: Distilbert Base Cased\n#config_6 = Configuration(\"distilbert-base-cased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_6, preds_test_6 = run_model(config_6)\n\n# Model: Distilbert Base Uncased\n#config_7 = Configuration(\"distilbert-base-uncased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_7, preds_test_7 = run_model(config_7)\n\n# Model: Distilbert Multilingual Base Cased\n#config_8 = Configuration(\"distilbert-base-multilingual-cased\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_8, preds_test_8 = run_model(config_8)\n\n# Model: Roberta Base\n#config_9 = Configuration(\"roberta-base\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_9, preds_test_9 = run_model(config_9)\n\n# Model: Roberta Large\n#config_10 = Configuration(\"roberta-large\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_10, preds_test_10 = run_model(config_10)\n\n# Model: XLM Roberta Base\n#config_11 = Configuration(\"jplu\/tf-xlm-roberta-base\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_11, preds_test_11 = run_model(config_11)\n\n# Model: XLM Roberta Large\n#config_12 = Configuration(\"jplu\/tf-xlm-roberta-large\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_12, preds_test_12 = run_model(config_12)\n","c3b80093":"# Hyperparameter Space 1\n#config_1 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 32, learning_rate = 1e-5, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_1, preds_test_1 = run_model(config_1)\n\n# Hyperparameter Space 2\n#config_2 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 64, learning_rate = 1e-5, batch_size = 32, epochs = 3, train_splits = 2)\n#preds_train_2, preds_test_2 = run_model(config_2)\n\n# Hyperparameter Space 3\n#config_3 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 84, learning_rate = 1e-5, batch_size = 16, epochs = 4, train_splits = 2)\n#preds_train_3, preds_test_3 = run_model(config_3)\n\n# Hyperparameter Space 4\n#config_4 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 84, learning_rate = 1e-4, batch_size = 16, epochs = 4, train_splits = 2)\n#preds_train_4, preds_test_4 = run_model(config_4)\n","04a866a7":"# TPU\n#config_1 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 2, accelerator = \"TPU\")\n#preds_train_1, preds_test_1 = run_model(config_1)\n\n# GPU\n#config_2 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 2, accelerator = \"GPU\")\n#preds_train_2, preds_test_2 = run_model(config_2)\n\n# CPU\n#config_3 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 2, accelerator = \"CPU\")\n#preds_train_3, preds_test_3 = run_model(config_3)\n","1f0a42b8":"# Final Model: XLM Roberta Large\nconfig_1 = Configuration(\"jplu\/tf-xlm-roberta-large\", translation = False, max_length = 84, batch_size = 64, epochs = 16, train_splits = 4)\npreds_train_1, preds_test_1 = run_model(config_1)\n","1f1bbdee":"df_test = pd.read_csv(config_1.PATH_TEST)\n\ndf_submission = pd.DataFrame({\"id\": df_test.id.values, \"prediction\": np.argmax(preds_test_1, axis = 1)})\ndf_submission.to_csv(\"submission.csv\", index = False)\n\ndf_submission.prediction.value_counts()\n","e5d8d5f4":"## Stratified K-Fold Modelling\nThe model is run by splitting the training data into [k-fold](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)#k-fold_cross-validation) stratified on language and label. I added language along with labels to ensure each language is represented consistently across folds and the models don't over-emphasize on English.\n\nThe function returns the out-of-fold train data predictions as well as the fold-averaged test predictions which can further conveniently be used for blending and stacking.\n","a89afa92":"## Final model\nAfter experimenting with various hyperparameters and finalizing the best combination, we run the those configuration(s).\n","3c13d60d":"## Data Preparation\nPreprocessing the textual data as well as tokenizing it into the encoding format for the model. Note that the texts in non-English languages are translated to English using Google translator.\n\nFinally the data is converted into a tf.data.Dataset so that it works seamlessly across accelerators.\n\n![Data%20Prep.png](attachment:Data%20Prep.png)\n","2eeabfad":"## Packages\nInstall and import all the required packages and modules.\n","8c99bdc2":"## Configuration\nThis is the most important section of the notebook. The configuration class is setup to define as many levers required for experiments as possible. It is currently meant to experiment on the following:\n\n* Different Huggingface models with Tensorflow \u2705\n* Different hyper-parameter spaces for models \u2705\n* Different seeds, splits, accelerators \u2705\n* Different learning rate schedulers (WIP)\n\nAnd all of this is possible just by changing one line of code!\n\nIn general, the idea is to expand on this configuration as you progress through a competition with more ideas and elements to experiment on.\n","e460671c":"## Transitioning across accelerators\nRunning the models on TPU, GPU or CPU can be configured without changing any code.  \nNote that for running on TPU (or GPU), the corresponding Accelerator must be chosen in the Kaggle Notebook settings. You can uncomment the different accelerator codes and run the models.\n\n![accelerators.png](attachment:accelerators.png)\n","21c22253":"## Submission\nCreating the submission file by predicting the label with highest probability.\n","e3593c56":"## Experimenting with different models\nThe list of HuggingFace models with Tensorflow can be viewed here: https:\/\/huggingface.co\/models?filter=tf   \nTrying out different models is as easy as a one-line change while creating the configuration. You can uncomment the different model codes and run the models.\n\nCurrently I have tested the following:\n\n\u2705 **Bert** Base Cased   \n\u2705 **Bert** Base Uncased   \n\u2705 **Bert** Large Cased   \n\u2705 **Bert** Large Uncased   \n\u2705 **Bert** Base Multilingual Cased   \n\u2705 **Distilbert** Base Cased   \n\u2705 **Distilbert** Base Uncased   \n\u2705 **Distilbert** Base Multilingual Cased   \n\u2705 **Roberta** Base   \n\u2705 **Roberta** Large   \n\u2705 **XLM Roberta** Base   \n\u2705 **XLM Roberta** Large   \n\nFor multilingual models it is recommended to use ***translation = False*** since the models contain vocabulary of other languages.\n","4a7a8235":"## Tuning hyperparameters\nExperimenting with different hyperparameters only requires a one-line change while creating the configuration. You can uncomment the different hyperparameter spaces code and run the models.\n","62978f1d":"## You know my methods, Watson\n![](https:\/\/i.imgur.com\/xy6FyyK.png)\n\nWith the pace at which deep learning is advancing it is becoming important and useful to be able to experiment with multiple models, ideas, approaches and potential solutions at a rapid pace. One of the methods of trying out various experiments is to **templatize code** and add **numerous levers** that can be played around with. This notebook aims to showcase one such structure for this.\n\nI will be demonstrating how you can setup a basic framework of building blocks and functions to quickly iterate through a lot of ideas. The dataset used is from the [Contradictory, My Dear Watson](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson) competition and the scope is limited to the [HuggingFace models with Tensorflow](https:\/\/huggingface.co\/models?filter=tf).\n\nThis notebook is a skeletal experimental setup (which is not necessarily the best) but I strongly believe that every individual will have their own preferences and methods so it would work even better to tweak this notebook to your own taste and liking. There are a lot of elements that can be added and improved during the course of any competition or project.\n","b987b762":"## Elementary, My Dear Watson\nThis is a bare skeletal workflow but as any competition progresses there will be new elements that would need to be added into the workflow and this process can help in scaling and iterating over experiments meticulously. Adding these components into the configuration framework can help maintain, scale and iterate through experiments and ideas in a meticulous and quick manner.\n\nHere are some open ideas to work on:\n\n* Adding data augmentation\n* Pre-processing text data before tokenization\n* Trying other models\n* Tuning hyperparameters\n* Ensembling multiple models\n\n**Good Luck!**\n","14377f6a":"## Deep Learning model architecture\nDefining the deep learning network architecture along with the model configuration. Its common to use pretrained models and add custom layers to it. Let's look at the basic model architecture using any pre-trained model.\n"}}