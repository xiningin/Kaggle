{"cell_type":{"4c30daed":"code","b0d94b7c":"code","2c5b6443":"code","97ee5b03":"code","7f0f866a":"code","b4e3749c":"code","b2da49f2":"code","7b5ce5f7":"code","92239eb4":"code","8b1bbe22":"code","5c2174f1":"code","38342c8f":"code","94df4244":"code","48882bfa":"code","e184cd15":"code","8f63151d":"code","4234b587":"code","14c2dd78":"code","9a810553":"code","d2380951":"code","2b53e4b2":"code","f19fd134":"code","9b88c7d1":"code","0ac0434e":"code","ba7d9d9f":"code","5335f16b":"code","4fc07518":"code","5953c9d5":"code","1efaa51d":"code","a850527f":"code","55c96950":"code","1febbdb2":"code","2a20e9f7":"code","6ab55a52":"code","f9ac6a73":"code","84c25335":"code","1153504d":"code","938aa37a":"code","b23df736":"code","435d7e08":"code","e992e9b5":"code","d78b6745":"code","7a1bc238":"markdown","2d51d119":"markdown","8163f5d9":"markdown","1f20f97f":"markdown","315cea0b":"markdown","fee16980":"markdown","ae80b22f":"markdown","ffb7a0c2":"markdown","2547a0d0":"markdown","06921a59":"markdown"},"source":{"4c30daed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0d94b7c":"#Optional - changes output view\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","2c5b6443":"#Load data\ndf_raw = pd.read_csv(\"\/kaggle\/input\/ecommerce-data\/data.csv\")\ndf_raw.head()","97ee5b03":"#Drop duplicates (if there's any)\ndf_raw.shape\ndf_raw.drop_duplicates(inplace = True)\ndf_raw.shape","7f0f866a":"df_raw.columns","b4e3749c":"#Change InvoiceDate to datetime format\ndf_raw['InvoiceDate'] =  pd.to_datetime(df_raw['InvoiceDate']).dt.floor('d')","b2da49f2":"#Check full timeframe avialable\ndf_raw['InvoiceDate'].min()\ndf_raw['InvoiceDate'].max()","7b5ce5f7":"#Days between min and max date\nDaysPeriod = (df_raw['InvoiceDate'].max() - df_raw['InvoiceDate'].min())\/np.timedelta64(1, 'D')\nDaysPeriod","92239eb4":"#Last day of calibration period \nlastDayCal = pd.Timestamp('2011-07-31')","8b1bbe22":"#Days in calibration\nDaysCalibration = (lastDayCal - df_raw['InvoiceDate'].min())\/np.timedelta64(1, 'D')\nDaysCalibration","5c2174f1":"#Percentage of calibration - holdout split\nDaysCalibration\/DaysPeriod","38342c8f":"#Creates monetary value column\ndf_raw['MonetaryValue'] = df_raw['UnitPrice']*df_raw['Quantity']","94df4244":"#Get frequency and monetary value in calibration period\ndf_RFM = df_raw[df_raw['InvoiceDate'] <= lastDayCal\n                   ].groupby('CustomerID').agg(Minimum_Date = ('InvoiceDate', np.min), Maximum_Date = ('InvoiceDate', np.max)\n                                                  , frequency_cal = ('InvoiceDate', pd.Series.nunique)\n                                                  , monetary_value_sum_cal = ('MonetaryValue', np.sum)\n                                                  #, monetary_value_avg = To do add avg\n                                                  )","48882bfa":"#Get frequency and monetary value in holdout period\ndf_RFM_hold = df_raw[df_raw['InvoiceDate'] > lastDayCal\n                   ].groupby('CustomerID').agg( frequency_hold = ('InvoiceDate', pd.Series.nunique)\n                                                  , monetary_value_sum_hold = ('MonetaryValue', np.sum)\n                                                  #, monetary_value_avg = To do add avg\n                                                  )","e184cd15":"#Add recency and T in calibration period\ndf_RFM['recency_cal'] = (df_RFM['Maximum_Date'] - df_RFM['Minimum_Date'])\/np.timedelta64(1, 'D')\ndf_RFM['T_cal'] = ( lastDayCal - df_RFM['Minimum_Date'])\/np.timedelta64(1, 'D')\ndf_RFM.head()","8f63151d":"#Sum monetary daily for customer\ndf_M_perDay_cal = df_raw[ df_raw['InvoiceDate'] <= lastDayCal \n                       ].groupby(['CustomerID','InvoiceDate']\n                                    ).agg(monetary_value_sum_day = ('MonetaryValue', np.sum))\ndf_M_perDay_cal.head()\n\ndf_M_perDay_hold = df_raw[ df_raw['InvoiceDate'] > lastDayCal \n                       ].groupby(['CustomerID','InvoiceDate']\n                                    ).agg(monetary_value_sum_day = ('MonetaryValue', np.sum))\ndf_M_perDay_hold.head()","4234b587":"#Average per day per customer\ndf_M_perDay_cal.reset_index(inplace=True)\ndf_M_avg_cal = df_M_perDay_cal.groupby('CustomerID').agg(monetary_value_avg_cal = ('monetary_value_sum_day', np.mean))\n\ndf_M_perDay_hold.reset_index(inplace=True)\ndf_M_avg_hold = df_M_perDay_hold.groupby('CustomerID').agg(monetary_value_avg_hold = ('monetary_value_sum_day', np.mean))","14c2dd78":"#Check data dimensions\ndf_RFM.shape\ndf_M_avg_cal.shape\ndf_RFM_hold.shape\ndf_M_avg_hold.shape","9a810553":"#Merge in one table\ndf_RFM = pd.merge(df_RFM, df_M_avg_cal, left_index=True, right_index=True)\ndf_RFM_hold = pd.merge(df_RFM_hold, df_M_avg_hold, left_index=True, right_index=True)\ndf_RFM.head()\ndf_RFM_hold.head()\ndf_RFM_full = df_RFM.join(df_RFM_hold)","d2380951":"#Drop unneeded columns\ndf_RFM_full.drop(['monetary_value_sum_hold', 'monetary_value_avg_hold'], axis=1, inplace=True)\ndf_RFM_full.drop(['Minimum_Date', 'Maximum_Date'], axis=1, inplace=True)\ndf_RFM_full.head()","2b53e4b2":"df_RFM_full.columns","f19fd134":"#Fill na as 0 in target variable \ndf_RFM_full['frequency_hold'] = df_RFM_full['frequency_hold'].fillna(0)","9b88c7d1":"df_RFM_full.info(show_counts=True)","0ac0434e":"#Check for categorical  columns\ncategorical_cols = df_RFM_full.select_dtypes(['object', 'category']).columns\ncategorical_cols","ba7d9d9f":"from sklearn.feature_selection import mutual_info_regression\n\nX_MI = df_RFM_full.copy()\ny_MI = X_MI.pop('frequency_hold')\n\ndef make_mi_scores(X_MI, y_MI):\n    mi_scores = mutual_info_regression(X_MI, y_MI, discrete_features='auto')\n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=X_MI.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X_MI, y_MI)\n\nmi_scores[::3]","5335f16b":"import matplotlib.pyplot as plt\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","4fc07518":"X = df_RFM_full\ny = X.pop('frequency_hold')","5953c9d5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","1efaa51d":"#pip install hyperopt","a850527f":"from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","55c96950":"#Initialize space\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n       'gamma': hp.uniform('gamma', 1, 9),\n       'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n       'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n       'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n       'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n       'n_estimators': 180\n        }","1febbdb2":"# Define objetive function\ndef hyperparameter_tuning(space):\n    model = xgb.XGBRegressor(n_estimators = space['n_estimators'], max_depth = int(space['max_depth']),\n                            gamma = space['gamma'], reg_alpha = int(space['reg_alpha']),\n                            min_child_weight = int(space['min_child_weight']),\n                            colsample_bytree = int(space['colsample_bytree']))\n    evaluation = [(X_train, y_train), (X_test, y_test)]\n    \n    model.fit(X_train, y_train,\n             eval_set = evaluation, eval_metric=\"mae\",\n             early_stopping_rounds = 10, verbose=False)\n    \n    pred = model.predict(X_test)\n    mae = mean_absolute_error(y_test, pred)\n    print(\"SCORE:\", mae)\n    return { 'loss': mae, 'status': STATUS_OK, 'model': model }","2a20e9f7":"#Find best tuning\ntrials = Trials()\nbest = fmin(fn=hyperparameter_tuning,\n            space=space,\n            algo = tpe.suggest,\n            max_evals=100,\n            trials=trials\n           )\nprint(best)","6ab55a52":"#Set hyperparameters and fit model\nxgb_model = xgb.XGBRegressor(colsample_bytree = 0.8805476759408526, gamma = 1.8533405994462941, max_depth = 5,\n                            min_child_weight = 2, reg_alpha = 40, reg_lambda = 0.17394906043913816)\nxgb_model.fit(X_train, y_train)","f9ac6a73":"y_pred = xgb_model.predict(X_test)","84c25335":"#Generate results dataframe\nresults = pd.DataFrame({\n    \"days_predicted\": y_pred,\n    'days_real': y_test\n})","1153504d":"#Check MAE\nprint('MAE:', mean_absolute_error(results['days_real'], results['days_predicted']))","938aa37a":"#Plot real active days histogram\nbinwidth = 1\nplt.hist(results['days_real'],\n        bins=range(round(min(results['days_real'])),\n                  round(max(results['days_real'])) + binwidth, binwidth))","b23df736":"#Plot predicted active days\nbinwidth = 1\nplt.hist(results['days_predicted'],\n        bins=range(round(min(results['days_predicted'])),\n                  round(max(results['days_predicted'])) + binwidth, binwidth))","435d7e08":"results['differences'] = results['days_predicted'] - results['days_real']","e992e9b5":"results.describe()","d78b6745":"#Plot predicted differences\nbinwidth = 1\nplt.hist(results['differences'],\n        bins=range(round(min(results['differences'])),\n                  round(max(results['differences'])) + binwidth, binwidth))","7a1bc238":"# Model","2d51d119":"Overview: In this project I'll show how to calculate the Customer Lifetime Value (CLV).\nGiven a timeframe part 1 will focus on forecasting how many days will the customer remain active (buying) and in part 2 we'll calculate how much will a customer buy per day on average and then using both results I'll calculate the CLV.","8163f5d9":"### Data Cleaning","1f20f97f":"# Feature engineering","315cea0b":"# Split data","fee16980":"# Hyperparameters tuning","ae80b22f":"### Mutual information","ffb7a0c2":"Conclusion: The differences distributions show a large mass of customers near zero, so the model is working properly. This results will be used in part 2 to conclude the CLV calculations.","2547a0d0":"**Transform transactional data to RFM**","06921a59":"# Customer lifetime value\nPart 1 of 2"}}