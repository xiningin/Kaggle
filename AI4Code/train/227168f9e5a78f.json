{"cell_type":{"11fbfb19":"code","ed5f6e8e":"code","8eb83a5d":"code","364a7280":"code","812b9cec":"code","060b6c83":"code","b41eaea2":"code","70f769bd":"code","5d0c7dbf":"code","29acc6a5":"code","5ca27af7":"code","86754867":"code","13e4501b":"code","ce30d7b5":"code","24765231":"code","4ef81190":"code","90e2d1a2":"code","a22307ed":"code","aa2ad292":"code","c08faf8f":"code","d5b747a5":"markdown","21c36407":"markdown","159b776d":"markdown","a937ca4a":"markdown","634fa2c8":"markdown","ae6ece4f":"markdown","25843f6e":"markdown","6b139a50":"markdown","72d7af96":"markdown"},"source":{"11fbfb19":"import sys\n# !cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n# !cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\n# sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n# !cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ed5f6e8e":"# import cudf\nimport torch\nimport joblib\nimport janestreet\nimport numpy as np\n# import cupy as cp\nfrom time import time\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom contextlib import contextmanager\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer\n# from cupyx.scipy.special import erfinv as cupy_erfinv\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\nimport random","8eb83a5d":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nseed_everything(34)","364a7280":"EPOCHS = 10#10\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEARLY = 4\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')","812b9cec":"@contextmanager\ndef timer(name):\n    t0 = time()\n    yield\n    print(f'[{name}] done in {time() - t0:.2f} s')\n","060b6c83":"with timer('load_data'):\n\n    train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv',nrows=1e6)\n    test = pd.read_csv(\"..\/input\/jane-street-market-prediction\/example_test.csv\")\n","b41eaea2":"drop_cols = list(np.setdiff1d(train.columns,test.columns)) + ['ts_id','date']+['weight']\ntrain.head(3)","70f769bd":"class janeDataset(Dataset):\n    \n    def __init__(self,df,target,mode=\"train\"):\n        \n        self.df = df.values\n        self.mode = mode\n        if self.mode == 'train':\n            self.target = target.values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        \n        if self.mode==\"train\":\n\n            return {'x':torch.FloatTensor(self.df[idx,:]),\n                    'y':torch.FloatTensor([self.target[idx]])}\n        else:\n            \n            return {'x':torch.FloatTensor(self.df[idx,:])}\n            \n    \n    ","5d0c7dbf":"class JaneModel(nn.Module):\n    \n    def __init__(self):\n        super(JaneModel,self).__init__()\n        \n        self.hidden = [130,64,16]#[131,64,16]\n        self.batch1 = nn.BatchNorm1d(self.hidden[0])\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(self.hidden[0],self.hidden[1]))\n        \n        self.batch2 = nn.BatchNorm1d(self.hidden[1])\n        self.dropout2 = nn.Dropout(0.15)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden[1],self.hidden[2]))\n        \n        \n        self.batch3 = nn.BatchNorm1d(self.hidden[2])\n        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden[2],1))\n        \n        \n    def forward(self,x):\n        \n        x = self.batch1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n    \n        x = self.batch3(x)\n        x = self.dense3(x)\n        \n        return x\n        \n        \n        \n        ","29acc6a5":"train=train[train['weight']!=0]\ntarget = (train['resp']>0)*1\nprint(train.shape[0])","5ca27af7":"import math","86754867":"from sklearn.preprocessing import StandardScaler\nfeatures = [f'feature_{i}' for i in range(1,130)]\n# def do_preprocess(train,mode=1):\n    \n    \n    \n#     def to_labels(x):\n#         if x==1:\n#             return 0\n#         else:\n#             return 1\n    \n    \n#     for col in features :\n#         mean_value=train[col].mean()\n#         if math.isnan(mean_value):\n# #             print(f'nan:{col}')\n#             mean_value=0.0\n#         train[col].fillna(mean_value,inplace=True)\n        \n#     if mode:\n\n#             transformer = StandardScaler()\n#             matrix = train[features]\n#             scaled_data = transformer.fit_transform(matrix)\n\n#             joblib.dump(transformer,f'{col}.pkl')\n\n#     else:\n#             transformer = joblib.load(f'{col}.pkl')\n#             matrix = train[features]\n#             scaled_data = transformer.transform(matrix)\n            \n\n#     train[features]=scaled_data\n#     train['feature_0'].fillna(-1,inplace=True)\n#     train['feature_0']=train['feature_0'].apply(to_labels).values\n   \n#     return train\n\nmatrix = train[features]\nscaler = StandardScaler().fit(matrix)\ntrain[features] = scaler.transform(matrix)\n\n","13e4501b":"import pickle\nwith open('sl.pkl',mode='wb') as fout:\n    pickle.dump(scaler,fout)","ce30d7b5":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    final_auc = 0\n    \n    for data in tqdm(dataloader):\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs).squeeze()\n        targets=targets.squeeze()\n        loss = loss_fn(outputs, targets)\n        outputs=torch.sigmoid(outputs)\n        auc = roc_auc_score(targets.detach().cpu().numpy(),outputs.detach().cpu().numpy())\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        final_auc += auc\n        \n    final_loss \/= len(dataloader)\n    final_auc \/= len(dataloader)\n    \n    return final_loss,final_auc\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    final_auc = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs).squeeze()\n        targets=targets.squeeze()\n        loss = loss_fn(outputs, targets)\n        outputs=torch.sigmoid(outputs)\n        auc = roc_auc_score(targets.detach().cpu().numpy(),outputs.detach().cpu().numpy())\n        \n        final_loss += loss.item()\n        final_auc += auc\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    final_auc \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss,final_auc,valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            outputs=torch.sigmoid(outputs)\n        \n        preds.append(outputs.detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","24765231":"def train_model(train,target):\n    \n    train.fillna(-1,inplace=True)\n    X_train,X_valid,y_train,y_valid  = train_test_split(train.drop(drop_cols,axis=1),target,test_size=0.15)\n    \n    train_data = janeDataset(X_train,y_train)\n    valid_data = janeDataset(X_valid,y_valid)\n    \n    train_data = DataLoader(train_data,batch_size=2**12,shuffle=True)\n    valid_data = DataLoader(valid_data,batch_size=2**12,shuffle=False)\n    \n    model = JaneModel()\n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(train_data))\n    loss_fn = nn.BCEWithLogitsLoss()\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n            \n            train_loss,train_auc = train_fn(model, optimizer, scheduler, loss_fn, train_data, DEVICE)\n            final_loss,valid_auc,valid_pred = valid_fn(model, loss_fn, valid_data, DEVICE)\n            print(f\" Epoch {epoch} train loss {train_loss : .5f} valid loss {final_loss : .5f} train_auc {train_auc: .4f} valid_auc {valid_auc : .4f}\")\n            \n            if final_loss<best_loss:\n                \n                best_loss = final_loss\n                torch.save(model.state_dict(),f'jane_model.pth')\n                early_stop=0\n            if EARLY:\n                early_stop+=1\n                if early_stop>EARLY:\n                    break\n        \n        \n\ntrain_model(train,target)\n    \n    ","4ef81190":"model = JaneModel()\nmodel.load_state_dict(torch.load(\"jane_model.pth\"))\nmodel.to(DEVICE)\n","90e2d1a2":"env = janestreet.make_env() \niter_test = env.iter_test()","a22307ed":"cols=train.drop(drop_cols,axis=1).columns","aa2ad292":"model.eval()\nopt_th = 0.5#0.5\nfor (test,sample_pred) in tqdm(iter_test):\n    \n    if test['weight'].item() > 0:\n        test = test[cols]\n#         test = do_preprocess(test,mode=0)\n        test.fillna(0,inplace=True)\n        test[features] = scaler.transform(test[features])\n\n        with torch.no_grad():\n            X=torch.tensor(test.values).float().to(DEVICE)\n            preds=model(X)\n            preds=torch.sigmoid(preds).cpu().numpy()[0,0]> opt_th\n\n        sample_pred.action = int(preds)\n    else:\n        sample_pred.action =0\n    env.predict(sample_pred)\n","c08faf8f":"\nprint('done') #","d5b747a5":"Credit to https:\/\/www.kaggle.com\/shahules\/stocks-rapids-nn-starter.\nI just did some changes:\n1. Remove the cudf.\n2. Do the scaler for nn's features\n3. Fill the nan value of test\n4. Speed up the inference. ","21c36407":"### <font size='4' ><a> Preprocess <\/a><\/font>","159b776d":"## <font size='4' color='blue'><a> Model <\/a><\/font>","a937ca4a":"## <font size='4' color='blue'><a> Inference <\/a><\/font>","634fa2c8":"## <font size='4' color='blue'><a> Read Data <\/a><\/font>","ae6ece4f":"## <font size='4' color='blue'><a> Dataset <\/a><\/font>","25843f6e":"## <font size='4' color='blue'><a> Training <\/a><\/font>","6b139a50":"## <font size='4' color='green'><a> WORK IN PROGRESS !!! DO AN UPVOTE IF YOU LIKED IT <\/a><\/font>","72d7af96":"## <font size='4' color='blue'><a> Imports <\/a><\/font>"}}