{"cell_type":{"3a02c0a4":"code","04e1c3dd":"code","181b0c7b":"code","6b489d9f":"code","b5485aae":"code","a37d88de":"code","8acfbcfe":"code","f8a97a9e":"code","07e51d6d":"code","fd030677":"code","ae9cbcb0":"code","3e286dac":"code","b4d102d8":"code","98c49f81":"code","f680f25f":"code","ccf20306":"code","3da55599":"code","4b1067d1":"code","2c332d5d":"code","3dab28c7":"code","b199c9b6":"code","b0d4da2b":"code","39b4c95c":"code","6f82f1cf":"code","342e3906":"code","9d44cf6f":"code","906a0f7a":"code","fda719fd":"code","daec2ffb":"code","2ef66c85":"code","699cfb35":"code","a4035629":"code","c416f0d9":"code","a27f8252":"markdown","68182042":"markdown","b65c898d":"markdown","db79fcfa":"markdown","76f2721a":"markdown","f8112782":"markdown","4dd5098b":"markdown","42e610e5":"markdown","93107a06":"markdown","1a3187ae":"markdown","5be46a87":"markdown","cbcbc8f7":"markdown","47ccf378":"markdown","aaaf5ce2":"markdown","25fa5460":"markdown","5b98cbce":"markdown","584d9bac":"markdown","59dab2f5":"markdown","4ded0507":"markdown","07fff39f":"markdown","de337edb":"markdown","7caced8b":"markdown","d64af08e":"markdown","ed0552cf":"markdown","b04aa0ef":"markdown","e02207bc":"markdown","8671343c":"markdown","9da22ef3":"markdown","4e6c6d0d":"markdown","b6353c20":"markdown","de8d554a":"markdown","44e4939c":"markdown","059ccd3d":"markdown"},"source":{"3a02c0a4":"# import libraries which are necessary in this notebook\nimport numpy as np\nimport pandas as pd\nfrom os import path\nimport datetime\nimport matplotlib\n# import folium library\nfrom folium import plugins\nimport folium\n# use Waffle from pywaffle library for waffle plot\n!pip install pywaffle\nfrom pywaffle import Waffle\n# Start with loading all necessary libraries\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm # color map","04e1c3dd":"# Read the whole dataset into a Pandas' DataFrame\ndf=pd.read_csv('..\/input\/us-accidents\/US_Accidents_June20.csv')","181b0c7b":"# quick overview of the data\ndf.head()","6b489d9f":"# Number of total entries\ndf.shape","b5485aae":"# Lets check the nan values with in each column \/ feature\n# percentage of missing values in each column\nprint((100*df.isnull().sum()\/df.shape[0]).round(2))","a37d88de":"# drop these columns\ndf_new = df.drop(['TMC', 'End_Lat', 'End_Lng', 'Number', 'Wind_Chill(F)', 'Wind_Speed(mph)', 'Precipitation(in)'], axis = 1)\ndf_new.dropna(axis = 0, how = 'any', inplace = True)","8acfbcfe":"# check the shape again\ndf_new.shape","f8a97a9e":"# check again the possible nan \/ missing values\ndf_new.isnull().sum()","07e51d6d":"# Number of accidents by each state\ndf_state=df_new.groupby(['State'], as_index=False).count().iloc[:,:2]\n# Rename the column that make more sence\ndf_state=df_state.rename(columns={\"ID\":\"NrAccidents\"})\n# sort by number of accidents\ndf_state.sort_values(by=['NrAccidents'], ascending=False, inplace=True)\ndf_state.head()","fd030677":"# plot the map by using folium with corresponding distribution of accidents\n# geojson file without AK, Alaska\nus_states_geo = r'..\/input\/geojson\/us_states_49.json' \n\n\n# set the size of the plotting canvas \/ figsize\nf = folium.Figure(width=900, height=500)\n# create a plain USA map object\nus_accident_distribution_map = folium.Map(location=[40, -100], zoom_start=4).add_to(f)\n\n# threshold scaling\n# create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration\nthreshold_scale = np.linspace(df_state['NrAccidents'].min(),\n                              df_state['NrAccidents'].max(),\n                              6, dtype=int)\nthreshold_scale = threshold_scale.tolist() # change the numpy array to a list\nthreshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration\n# Apply the corresponding dataset to the map\nfolium.Choropleth(\n    geo_data=us_states_geo,\n    name='choropleth',\n    data=df_state,\n    columns=['State','NrAccidents'],\n    key_on='feature.id',\n    threshold_scale=threshold_scale,\n    fill_color='OrRd',\n    fill_opacity=0.8,\n    line_opacity=0.2,\n    legend_name='Overview of the number of accidents across US (Alaska is not included)',\n    reset=True\n).add_to(us_accident_distribution_map)\nfolium.LayerControl().add_to(us_accident_distribution_map)\nus_accident_distribution_map","ae9cbcb0":"# set the state names as the index\ndf_state.set_index('State', inplace=True)","3e286dac":"\n# plot data in bar chart\ndf_state.plot(kind='bar', width=0.8, figsize=(15, 8), legend=False)\nplt.xlabel('State', fontsize=14) # add to x-label to the plot\nplt.ylabel('Number of Accidents', fontsize=14) # add y-label to the plot\nplt.title('Number of accidents by each state', fontsize=14) # add title to the plot\nplt.show()","b4d102d8":"# lets look at the first six states with highest number of accidents\nstate6 = df_state['NrAccidents'].iloc[:6].sum(axis=0)\nstate_rest = df_new.shape[0] - df_state['NrAccidents'].iloc[:6].sum(axis=0)\n# plot as waffle\ndata = {'CA, TX, FL, SC, NC, NY': (100*state6\/df_new.shape[0]).round(1), 'Other states': (100*state_rest\/df_new.shape[0]).round(1)}\nfig = plt.figure(\n    figsize=(15, 20),\n    FigureClass=Waffle, \n    rows=10, \n    columns=50,\n    values=data, \n    labels=[\"{0} ({1}%)\".format(k, v) for k, v in data.items()],\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)}\n    )\nplt.show()","98c49f81":"bins=300\nplt.figure(figsize=(10, 6))\n\n\nfor st in ['CA', 'TX', 'FL', 'SC', 'NC', 'NY']:    \n    # set s filter\n    stfilt = (df_new['State'] == st)\n    plt.hist(df_new.loc[stfilt,'Visibility(mi)'], bins, density=False)\nplt.xlabel('Visibility(mi)', fontsize=14)\nplt.ylabel('Number of accidents', fontsize=14)\nplt.xlim(0,15)\nplt.grid()\nplt.show()","f680f25f":"def which_day(date_time):\n    '''\n    To find out which weekday according to given timestamp with the format 'yyyy-mm-dd hh:mm:ss'\n        input: datetime string with the format of 'yyyy-mm-dd hh:mm:ss'\n        return: nth day of the week\n    '''\n    # import time and date modules\n    from datetime import datetime\n    # import calendae modules to extract the exact weekday\n    import calendar\n    try:\n        if type(date_time) is str:\n            my_string=date_time.split(' ')[0]\n            my_date = datetime.strptime(my_string, \"%Y-%m-%d\")\n            return my_date.weekday()\n        else:\n            raise Exception(\"'date_time' has unexpected data type, it is expected to be a sting\")\n\n    except Exception as e:\n        print(e)\n# use above function to find which weekday \nnth_day=[]\ndate_time=[dt for dt in df_new['Start_Time']]\nfor i in range(len(date_time)):\n    nth_day.append(which_day(date_time[i]))\n# add four new columns 'year', 'month', 'hour', 'weekday'\ndf_new['year'] = pd.DatetimeIndex(df_new['Start_Time']).year\ndf_new['month'] = pd.DatetimeIndex(df_new['Start_Time']).month\ndf_new['hour'] = pd.DatetimeIndex(df_new['Start_Time']).hour\ndf_new['weekday']=nth_day","ccf20306":"df_new.shape","3da55599":"df_new.loc[:,['year', 'month', 'hour', 'weekday', 'Start_Time']].head()","4b1067d1":"df_month=df_new[df_new['year'].isin(['2016','2017', '2018', '2019', '2020'])].groupby(['month'], as_index=False).count().iloc[:,:2]\n# by changing the argument in 'isin()' one can look at quite directly the change of the accidents during the years,\n# which I did not do it here.\ndf_month.head()","2c332d5d":"# plot data in bar chart\nax=df_month.plot(kind='bar', width=0.8, figsize=(10, 6), legend=None)\nxtick_labels=['Jan.', 'Feb.', 'Mar.', 'Apr.', 'May', 'Jun.', 'Jul.', 'Aug.', 'Sep.', 'Oct.', 'Nov.', 'Dec.']\nax.set_xticks(list(df_month.index))\nax.set_xticklabels(xtick_labels)\nax.set_xlabel('Month', fontsize=14) # add to x-label to the plot\nax.set_ylabel('Number of Accidents', fontsize=14) # add y-label to the plot\nax.set_title('Number of accidents by each month', fontsize=14) # add title to the plot\nplt.show()","3dab28c7":"wday_filt = (df_new['weekday'].isin([0, 1, 2, 3, 4]))#.to_frame()\nweekend_filt = (df_new['weekday'].isin([5, 6]))#.to_frame()\ndf_wday = (df_new.loc[wday_filt])[['hour']]#.count().iloc[:, :2]\ndf_weekend = (df_new.loc[weekend_filt])[['hour']]#.count().iloc[:, :2]","b199c9b6":"# plot the distribution of accidents during the day\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(6, 12), sharex=True)\nax0, ax1, ax2 = axes.flatten()\nbins=24\nkwargs = dict(bins=24, density=False, histtype='stepfilled', linewidth=3)\n# ax0\nax0.hist(list(df_new['hour']),  **kwargs, color='orange', label='Whole week')\nax0.set_ylabel('Number of accidents', fontsize=14)\n# ax1\nax1.hist(list(df_wday['hour']), **kwargs, color='blue', label='Work days')\nax1.set_ylabel('Number of accidents', fontsize=14)\n# ax2\nax2.hist(list(df_weekend['hour']),  **kwargs, color='Red', label='Only weekend')\nax2.set_ylabel('Number of accidents', fontsize=14)\nax2.set_xlabel('Hour', fontsize=14)\nax0.legend(); ax1.legend(); ax2.legend()\nplt.xlim(0, 23)\n#plt.ylim(0, 2.5e5)\nplt.show()\n","b0d4da2b":"df_weekday=df_new.groupby(['weekday'], as_index=False).count().iloc[:,:2]\n# set the month as the index\ndf_weekday.set_index('weekday', inplace=True)","39b4c95c":"# plot data in bar chart\nlabels = ['Mo', 'Tu', 'We', 'Th', 'Fr', 'Sa', 'Su']\nx = np.arange(len(labels))  # the label locations\nfig, ax = plt.subplots(figsize=(10, 6))\nax1 = ax.bar(x, df_weekday['ID'], width=0.5)\n#ax1 = ax.plot(x, df_weekday['ID'],marker='o', lw=2)\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number of accidents', fontsize=14)\nax.set_xlabel('Weekday', fontsize=14)\nax.set_title('Distribution of accidents along the weekdays', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(labels)\n\n#df_weekday.plot(kind='line', figsize=(10, 6), legend=None)\n\n#plt.xlabel('Weekday', fontsize=14) # add to x-label to the plot\n#plt.ylabel('Number of Accidents', fontsize=14) # add y-label to the plot\n#plt.title('Number of accidents by each state', fontsize=14) # add title to the plot\nplt.show()","6f82f1cf":"!pip install Pillow\n!pip install wordcloud","342e3906":"# join all descriptions from all accidents\ndsc=df_new['Description'].astype(str)\n# remove non-words\n#sanitized_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\\/\\\/\\S+)\", \" \", text).split()) \ntext = \" \".join(desc for desc in dsc)\nprint (\"There are {} words in the combination of all description.\".format(len(text)))","9d44cf6f":"more_stopwords=[\"accident\", \"due\", \"blocked\", \"Right\", \"hand\"]\nfor more in more_stopwords:\n    STOPWORDS.add(more)\n# Generate a word cloud image\n# lower max_font_size\nwordcloud = WordCloud(stopwords=STOPWORDS, max_font_size=40, background_color=\"white\").generate(text)\nplt.figure(figsize=(18, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show\n# Save the image in the img folder:\nwordcloud.to_file(\"us_accidents_description.png\")","906a0f7a":"df_T=df_new['Temperature(F)'].values","fda719fd":"'''\n# lambda function \nftoc=lambda f:5\/9*(f-32)\n# function call\nc=[]\nfor fi in f:\n    c.append(round(ftoc(ni), 1))\nc=np.array(c)\nc\n'''\nnum_bins = 50\n\nfig = plt.figure(figsize=(10, 6))\nax1 = fig.add_subplot(111)\n\n# the histogram of the data\nn, bins, patches = ax1.hist(df_T, num_bins, density=0) # set density=1 to normalize\n# find bincenters\n# bincenters = 0.5*(bins[1:]+bins[:-1])\n\n\nax1.set_xlabel(r\"Temperature(\u00b0F)\", fontsize=14, color='red')\nax1.set_ylabel('Number of accidents', fontsize=14, color='red')\nax1.set_xlim(-25, 125) # set xlim \n# Set the temperature in celisius\nax2 = ax1.twiny()\nax2.set_xlabel(r\"Temperature(\u00b0C)\", fontsize=14, color='red')\nax2.set_xlim(ax1.get_xlim())\nax2.set_xticks([-58, -13, 32, 77, 122])\nax2.set_xticklabels(['-50', '-25', '0','25', '50'])\nplt.grid()\nplt.show()","daec2ffb":"100*df.Severity.value_counts()\/df.shape[0]","2ef66c85":"df.Stop.value_counts()","699cfb35":"df['Sunrise_Sunset'].value_counts()","a4035629":"df['Traffic_Signal'].value_counts()","c416f0d9":"df['Give_Way'].value_counts()","a27f8252":"Most of the accidents were during the day time, especially **around rush hours both inthe mornings and afternoons of wor days**. At weekends there are relatively less accidents and most of these accidents are occured from **7:00 AM to 9:00 PM**.","68182042":"<a id=\"ref3\"><\/a>\n# Impact of Visibility","b65c898d":"## About this Notebook  \nIn this Notebook I tried to perform data visualization on the dataset us_accident_dataset. In order to describe this data more vividly I tried to use several different types of plots such as folium.Map, Histogram, Bar plot, Waffle plot, WordCloud.","db79fcfa":"I would like to thank the provider of this dataset! This is my very first Kaggle project, even though there are no any prediction and machine learning type work. I am thinking a way to use classification based on some features who have noticeable impact on the the accidents. \n\nDuring the process of making myself familiar with this dataset I have found two published papers specifically on this dataset. If some one wants more information, please look at the following papers.\n\nReferences:  \n[1]. <a href=\"https:\/\/arxiv.org\/abs\/1906.05409\">arXiv:1906.05409<\/a>  \n[2]. <a href=\"https:\/\/arxiv.org\/abs\/1909.09638\">arXiv:1909.09638<\/a>\n","76f2721a":"Before doing any analysis we can exclude for the moment the features *'TMC', 'End_Lat', 'End_Lng', 'Number', 'Wind_Chill(F)', 'Wind_Speed(mph)', 'Precipitation(in)'* due to the significant amount of NAN \/ missing values **29.45%, 70.55%, 70.55%, 64.40%, 53.17%, 12.94%, 57.66%,** respectively. One can consider them later in the analysis if their impact on the occurence of accidents. ","f8112782":"<a id=\"ref1\"><\/a>\n# Downloading and Prepping Data","4dd5098b":"<a id=\"ref2\"><\/a>\n# Distribution of the accidents across US states\n","42e610e5":"'Stop' feature does have little impact.","93107a06":"SO, data is loaded successfully.  \nLet us loot at the size of the dataset","1a3187ae":"<h1>Table of contents<\/h1>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a href=\"#ref1\">Downloading and Prepping Data<\/a><\/li>\n        <li><a href=\"#ref2\">Distribution of the accidents accross US states (except Alaska)<\/a><\/li>\n        <li><a href=\"#ref3\">Impact of Visibility<\/a><\/li>\n        <li><a href=\"#ref4\">Monthly Distribution<\/a><\/li>\n        <li><a href=\"#ref5\">Hourly Distribution During the Week<\/a><\/li>\n        <li><a href=\"#ref6\">Distribution Along Weekdays<\/a><\/li>\n        <li><a href=\"#ref7\">Analysis on the Recorded Accident Descriptions<\/a><\/li>\n        <li><a href=\"#ref8\">Impact of the Temperature<\/a><\/li>\n        <li><a href=\"#ref9\">Quick Check the Impacts of other features<\/a><\/li>\n        <li><a href=\"#ref10\">Acknowledgements and References<\/a><\/li>\n    <\/ol>\n    \n<\/div>\n\n<hr>  \n\n<h2>\n<!--\n<div class=\"alert alert-danger\"\" style=\"margin-top: 20px\">\n-->\n          If the map could NOT be opened, Please view the notebook via Jupyter nbviewer by <br> <a href=\"https:\/\/nbviewer.jupyter.org\/github\/Abdurahman-Amat\/Kaggle_Projects\/blob\/master\/US_accidents_data_analysis_visualization.ipynb\" target=\"_blank\">clicking here <\/a>, thank you!!!\n<\/div>\n<\/h2>  ","5be46a87":"The 'Traffic_Signal' has also some impact like such that around **16.9%** of whole accidents occured nearby traffic signal locations.","cbcbc8f7":"<a id=\"ref6\"><\/a>\n# Distribution Along Weekdays","47ccf378":"Let us check if the visibility has any noticeable impact on the occurance of accidents.","aaaf5ce2":"# US Accidents Data Visualization and Analysis (records > 3.5 million)","25fa5460":"From the above plot we can clearly see that there are relatively less accidents on weekends.","5b98cbce":"This 'Give_Way', indicates traffic signs \/ rules, hardly have impact on traffic accidents.","584d9bac":"Let us perform some timely analysis","59dab2f5":"<a id=\"ref7\"><\/a>\n# Analysis on the Recorded Accident Descriptions ","4ded0507":"<a id=\"ref10\"><\/a>\n# Acknowledgements and References ","07fff39f":"lets check the shape of the new dataset","de337edb":"--------------THE END------------","7caced8b":"From this WordCloud one can see clearly that there are more accidents around highways (the size of the words directly proportional to their frequencies of appearence in the recorded description of the accidents.) and around the roads of smaller neighborhoods of USA. ","d64af08e":"We can see that more than half of the accidents were actually occured in the first six states namely 'CA, TX, FL, SC, NC, NY'. We might see this significance more clearly with Waffle plot.","ed0552cf":"This histogram tells us that most of the accidents were happened when the weather were neither too hot nor too cold to go out.","b04aa0ef":"<a id=\"ref8\"><\/a>\n# Impact of the Temperature","e02207bc":"<a id=\"ref4\"><\/a>\n# Monthly Distribution","8671343c":"We can see that the impact of the visibility on the number of accidents is not significant. For instance, in state 'CA', California, occured the largest number of accidents among other states. However, the visibility of theses states are almost same around 10 mi.","9da22ef3":"### Prepare the dataset for further viualization and analysis\nLets check if there are any missing \/ NAN values in this dataset.","4e6c6d0d":"Accidents with severity level 1, indicates the least impact on traffic (i.e., short delay as a result of the accident), occured barely; with severity level 4,  indicates a significant impact on traffic (i.e., long delay), occured little number of times; with severity level 2 and 3, indicate the impact on traffic is around in midlevel, were occured quite frequently.","b6353c20":"<a id=\"ref9\"><\/a>\n# Quick Check the Impacts of other features ","de8d554a":"More than 3.5 million entries, super!","44e4939c":"<a id=\"ref5\"><\/a>\n# Hourly Distribution","059ccd3d":"'Sunrise_Sunset' does have considerable impact that around **26.2%** of whole accidents occured during the night."}}