{"cell_type":{"859aee10":"code","f733cad9":"code","c29813b7":"code","3c7709f5":"code","58a9e80c":"code","d2fd4bee":"code","a1e40c64":"code","1a032236":"code","107fbd46":"code","2eddb084":"code","e885f1f2":"code","c9e46c12":"code","19e7e1f5":"code","16709551":"code","242ffdcd":"code","50b8e382":"code","fe833f35":"code","9577635f":"code","f5dac9cc":"code","b04e66bd":"code","a685d63b":"code","f33f8a5a":"code","bf2b6465":"code","c63778ce":"code","ed8d0e32":"code","3a8aba21":"code","5717526d":"code","cf1c4e56":"code","2d321443":"code","e3e98ff6":"code","8c6f2330":"code","c3ecbafe":"code","ba49fb40":"code","71f61063":"code","30e38d79":"code","f3444cda":"code","96c0e7fe":"code","3e7cd0ea":"code","811d33ba":"code","9fbf7284":"code","8b0dac95":"code","47456786":"code","fa1f34c9":"code","ee15634b":"code","0c467a7a":"code","99642b71":"code","291f48b3":"code","4763a2d4":"code","13bccff9":"code","c6fcae93":"code","f22ff834":"code","64b17f4f":"code","4fa8d532":"code","61e894b3":"code","141687c4":"code","6eac4794":"code","8c40eade":"code","d160577f":"code","2b280b51":"code","f25e0980":"code","a61d997f":"code","0af81294":"code","57f04a8f":"code","749338ea":"code","9828eb51":"code","87bdad5f":"code","f337acea":"code","b729ac45":"code","f5af73bf":"code","d0972bed":"code","b01a5995":"code","1b117787":"code","51ccd006":"code","e6bd1291":"code","f0ac8761":"code","1a57d9fe":"code","f2ec0fa9":"code","8dd3d339":"code","9445b2ea":"code","b6a4e621":"code","ee8fab4c":"code","736dac10":"code","91fe8f2f":"code","521051f7":"code","c0c67aed":"code","35301f13":"code","f0b0e32a":"code","949fdccf":"code","ce13d230":"markdown","b90b2607":"markdown","574efc6b":"markdown","86ccff59":"markdown","fce1685b":"markdown"},"source":{"859aee10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.feature_selection import SelectFwe, f_regression\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.builtins import OneHotEncoder, StackingEstimator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f733cad9":"train = pd.read_csv('\/kaggle\/input\/janatahack\/train_8wry4cB.csv')\ntest = pd.read_csv('\/kaggle\/input\/janatahack\/test_Yix80N0.csv')\nsample = pd.read_csv('\/kaggle\/input\/janatahack\/sample_submission_opxHi4g.csv')","c29813b7":"print(train.shape)\nprint(train.info)","3c7709f5":"train.apply(lambda x:len(x.unique()))","58a9e80c":"train.isna().sum()","d2fd4bee":"sns.countplot(train['gender'])","a1e40c64":"train['startTime'] = pd.to_datetime(train['startTime'])\ntrain['endTime'] = pd.to_datetime(train['endTime'])","1a032236":"train","107fbd46":"df = train.append(test)","2eddb084":"df = df[['session_id','startTime','endTime','ProductList','gender']]","e885f1f2":"df['ProductCount'] = df.ProductList.str.count(';')+1","c9e46c12":"df['ProductList']","19e7e1f5":"df['startTime'] = pd.to_datetime(df['startTime'])\ndf['endTime'] = pd.to_datetime(df['endTime'])","16709551":"import numpy as np\nfrom itertools import chain\n\n# return list from series of comma-separated strings\ndef chainer(s):\n    return list(chain.from_iterable(s.str.split(';')))\n\n# calculate lengths of splits\nlens = df['ProductList'].str.split(';').map(len)\n\n# create new dataframe, repeating or chaining as appropriate\ndf1 = pd.DataFrame({'session_id': np.repeat(df['session_id'], lens),\n                    'startTime': np.repeat(df['startTime'], lens),\n                    'endTime':np.repeat(df['endTime'],lens),\n                    'ProductCount': np.repeat(df['ProductCount'], lens),\n                    'ProductList': chainer(df['ProductList']),\n                    'gender':np.repeat(df['gender'],lens)})\n\nprint(df1)","242ffdcd":"df1.head()","50b8e382":"df1['TimeTaken'] = abs(df1['endTime'] - df1['startTime']).astype('timedelta64[m]')","fe833f35":"df1[['Date','Time']] = df1['startTime'].astype(str).str.split(\" \",expand=True) ","9577635f":"df1['Date'] = pd.to_datetime(df1['Date'])","f5dac9cc":"df1['Day'] = df1['Date'].apply(lambda x: x.weekday())","b04e66bd":"df1.head()","a685d63b":"df1['TimeTaken'].max()","f33f8a5a":"df1[['Category','SubCategory','SubSubCategory','SubSubSubCategory','Extra']] = df1['ProductList'].str.split(\"\/\",expand=True) ","bf2b6465":"del df1['Extra']\ndel df1['ProductList']\n","c63778ce":"del df1['Time']\ndel df1['Date']","ed8d0e32":"len(df1['session_id'].unique())","3a8aba21":"df1.columns","5717526d":"df1 = df1[['session_id','TimeTaken','Day','ProductCount','Category','SubCategory','SubSubCategory','SubSubSubCategory','gender']]","cf1c4e56":"df1['TimeTaken'] = df1.TimeTaken.apply(lambda x:int(x))","2d321443":"df1.head()","e3e98ff6":"from sklearn import preprocessing \ncolumns = ['Category','SubCategory','SubSubCategory','SubSubSubCategory']\nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor i in columns:\n    df1[i]= label_encoder.fit_transform(df1[i]) \n  ","8c6f2330":"df1['session_id'].describe()","c3ecbafe":"test1 = df1[df1['gender'].isnull() == True]","ba49fb40":"train1 = df1[df1['gender'].isnull() == False]\n","71f61063":"train1.head()","30e38d79":"test1.head()","f3444cda":"from sklearn import preprocessing \ncolumns = ['gender']\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor i in columns:\n    train1[i]= label_encoder.fit_transform(train1[i]) \n  ","96c0e7fe":"train1.corr()","3e7cd0ea":"sns.countplot(train1['Category'])","811d33ba":"sns.countplot(train1['Day'])","9fbf7284":"train1.groupby('Day')['gender'].size()","8b0dac95":"sns.countplot(train1['ProductCount'])","47456786":"pd.crosstab(train1['Day'],train1['gender'])","fa1f34c9":"del test1['gender']","ee15634b":"def extra_tree(Xtrain,Ytrain,Xtest):\n    extra = ExtraTreesClassifier()\n    extra.fit(Xtrain, Ytrain) \n    extra_prediction = extra.predict(Xtest)\n    return extra_prediction\ndef Xg_boost(Xtrain,Ytrain,Xtest):\n    xg = XGBClassifier(loss='exponential', learning_rate=0.05, n_estimators=1000, subsample=1.0, criterion='friedman_mse', \n                                  min_samples_split=2, \n                                  min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_depth=10, min_impurity_decrease=0.0, \n                                  min_impurity_split=None, \n                                  init=None, random_state=None, max_features=None, verbose=1, max_leaf_nodes=None, warm_start=False, \n                                  presort='deprecated', \n                                  validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n    xg.fit(Xtrain, Ytrain) \n    xg_prediction = xg.predict(Xtest)\n    return xg_prediction\ndef LGBM(Xtrain,Ytrain,Xtest):\n    lgbm = LGBMClassifier(boosting_type='gbdt', num_leaves=40,\n                            max_depth=5, learning_rate=0.05, n_estimators=1000, subsample_for_bin=200, objective='binary', \n                            min_split_gain=0.0, min_child_weight=0.001, min_child_samples=10,\n                            subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0,\n                            reg_lambda=0.0, random_state=None, n_jobs=1, silent=True, importance_type='split')\n    #lgbm = LGBMClassifier(n_estimators= 500)\n    lgbm.fit(X_train, Y_train)\n    lgbm_preds = lgbm.predict(X_test)\n    return lgbm_preds","0c467a7a":"print(train1.columns)\nprint(test1.columns)","99642b71":"X_train = train1[['TimeTaken','Day','ProductCount','Category', 'SubCategory', 'SubSubCategory','SubSubSubCategory']]\nY_train = train1['gender']\nX_test = test1[['TimeTaken','Day','ProductCount', 'Category', 'SubCategory', 'SubSubCategory','SubSubSubCategory']]","291f48b3":"from autoviml.Auto_ViML import Auto_ViML","4763a2d4":"target = 'gender'\nscoring_parameter = 'balanced-accuracy'","13bccff9":"\nm, feats, trainm, testm = Auto_ViML(train1, target, test1,\n                                    scoring_parameter=scoring_parameter,\n                                    hyper_param='GS',feature_reduction=True,\n                                     Boosting_Flag='Boosting_Flag',Binning_Flag=False)","c6fcae93":"sam = pd.read_csv('\/kaggle\/input\/sample\/gender_Binary_Classification_submission.csv')","f22ff834":"\ntest1['gender'] = sam['gender_predictions']\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Auto.csv',index = False)","64b17f4f":"X_train.head()\ncate_features_index = np.where(X_train.dtypes != float)[0]","4fa8d532":"xtrain,xtest,ytrain,ytest = train_test_split(X_train,Y_train,train_size=0.99,random_state=1236)","61e894b3":"from catboost import Pool, CatBoostClassifier, cv, CatBoostRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","141687c4":"model = CatBoostClassifier(iterations=7000, learning_rate=0.001, l2_leaf_reg=3.5, depth=5, \n                           rsm=0.99, loss_function= 'Logloss', eval_metric='AUC',use_best_model=True,random_seed=50)","6eac4794":"model.fit(xtrain,ytrain,cat_features=cate_features_index,eval_set=(xtest,ytest))","8c40eade":"predss = model.predict(X_test)","d160577f":"\ntest1['gender'] = predss\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Catboost+PC.csv',index = False)","2b280b51":"#pred_xg = Xg_boost(X_train,Y_train,X_test)\n#pred_et = extra_tree(X_train,Y_train,X_test)\npred_l = LGBM(X_train,Y_train,X_test)\n","f25e0980":"# 0 - female, 1 male","a61d997f":"test1['gender'] = pred_xg\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DXG.csv',index = False)","0af81294":"test1['gender'] = pred_et\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DETC.csv',index = False)","57f04a8f":"\ntest1['gender'] = pred_l\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DPCLGBM.csv',index = False)\n","749338ea":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train, Y_train)\nans = clf.predict(X_test)\n","9828eb51":"print(len(pred_l))\ntest1['gender'] = ans\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DLR.csv',index = False)","87bdad5f":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=100).fit(X_train, Y_train)\nprediction_of_ada = ada.predict(X_test)","f337acea":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(loss='exponential', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n                                  min_samples_split=2, \n                                  min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=10, min_impurity_decrease=0.0, \n                                  min_impurity_split=None, \n                                  init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, \n                                  presort='deprecated', \n                                  validation_fraction=0.1, n_iter_no_change=None, tol=0.0001).fit(X_train, Y_train)\nprediction_of_gbc = gbc.predict(X_test)","b729ac45":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=10).fit(X_train, Y_train)\nprediction_of_rf = rf.predict(X_test)","f5af73bf":"\ntest1['gender'] = prediction_of_ada\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DADA.csv',index = False)","d0972bed":"\ntest1['gender'] = prediction_of_gbc\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Dgbc.csv',index = False)","b01a5995":"\ntest1['gender'] = prediction_of_rf\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DRF.csv',index = False)","1b117787":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train,Y_train)\n\n# Predicted class\nnri = neigh.predict(X_test)\n","51ccd006":"\ntest1['gender'] = nri\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Dknn.csv',index = False)","e6bd1291":"from sklearn.calibration import CalibratedClassifierCV","f0ac8761":"model = XGBClassifier()\nmetLearn=CalibratedClassifierCV(model, method='isotonic', cv=2)\nmetLearn.fit(X_train, Y_train)\ntestPredictions = metLearn.predict(X_test)","1a57d9fe":"def submissions(predictions_by_model,string):\n    test1['gender'] = predictions_by_model\n    testn = test1[['session_id','gender']]\n    print(testn.isna().sum())\n    test_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\n    dic = {1:'male',0:'female'}\n    test_final['gender'] = test_final['gender'].map(dic)\n    test_final.to_csv(string.csv,index = False)\n","f2ec0fa9":"test1['gender'] = testPredictions\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DCCV.csv',index = False)","8dd3d339":"import pandas as pd\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\n# sklearn tools for model training and assesment\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid\nfrom sklearn.metrics import (roc_curve, auc, accuracy_score)\n\n# specify your configurations as a dict\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': {'binary_logloss', 'auc'},\n    'metric_freq': 1,\n    'is_training_metric': True,\n    'max_bin': 255,\n    'learning_rate': 0.1,\n    'num_leaves': 63,\n    'tree_learner': 'serial',\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'min_data_in_leaf': 50,\n    'min_sum_hessian_in_leaf': 5,\n    'is_enable_sparse': True,\n    'use_two_round_loading': False,\n    'is_save_binary_file': False,\n    'output_model': 'LightGBM_model.txt',\n    'num_machines': 1,\n    'local_listen_port': 12400,\n    'machine_list_file': 'mlist.txt',\n    'verbose': 0,\n    'subsample_for_bin': 200000,\n    'min_child_samples': 20,\n    'min_child_weight': 0.001,\n    'min_split_gain': 0.0,\n    'colsample_bytree': 1.0,\n    'reg_alpha': 0.0,\n    'reg_lambda': 0.0\n}\n\n\nlgb_train = lgb.Dataset(X_train, Y_train)\n\n ","9445b2ea":"lgb_train","b6a4e621":"lgb_eval = lgb.Dataset(X_test)","ee8fab4c":"# train\ngbm = lgb.train(params,\n                lgb_train,\n                valid_sets=lgb_eval)","736dac10":"\n\n\ngridParams = {\n    'learning_rate': [ 0.1],\n    'num_leaves': [63],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary']\n}\n\nmdl = lgb.LGBMClassifier(\n    task = params['task'],\n    metric = params['metric'],\n    metric_freq = params['metric_freq'],\n    is_training_metric = params['is_training_metric'],\n    max_bin = params['max_bin'],\n    tree_learner = params['tree_learner'],\n    feature_fraction = params['feature_fraction'],\n    bagging_fraction = params['bagging_fraction'],\n    bagging_freq = params['bagging_freq'],\n    min_data_in_leaf = params['min_data_in_leaf'],\n    min_sum_hessian_in_leaf = params['min_sum_hessian_in_leaf'],\n    is_enable_sparse = params['is_enable_sparse'],\n    use_two_round_loading = params['use_two_round_loading'],\n    is_save_binary_file = params['is_save_binary_file'],\n    n_jobs = -1\n)\n\nscoring = {'AUC': 'roc_auc'}\n\n# Create the grid\n#grid = GridSearchCV(mdl, gridParams, verbose=2, cv=5, scoring=scoring, n_jobs=-1, refit='AUC')\n# Run the grid\n\n\n#print('Best parameters found by grid search are:', grid.best_params_)\n#print('Best score found by grid search is:', grid.best_score_)\n","91fe8f2f":"yes = gbm.predict(X_test)","521051f7":"yess =[]\nfor i in yes:\n    if i>=0.5:\n        yess.append(1)\n    else:\n        yess.append(0)","c0c67aed":"test1['gender'] = yess\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DDCCV.csv',index = False)","35301f13":"import h2o\nh2o.init()\ntrain2 = h2o.H2OFrame(train1)\ntest2 = h2o.H2OFrame(X_test)\ntrain1.columns\ny = 'gender'\nx = train2.col_names\nx.remove(y)\ntrain2['gender'] = train2['gender'].asfactor()\ntrain2['gender'].levels()\nfrom h2o.automl import H2OAutoML\naml1 = H2OAutoML(max_models = 30, max_runtime_secs=200, seed = 1)\naml1.train(x = x, y = y, training_frame = train2)\npreds = aml1.predict(test2)\nprint(sample.columns)\ntest1['gender'] = preds\n#ans=h2o.as_list(preds) \n#sample['gender'] = ans['predict']\n#sample.to_csv('Solution_H2O(Divided).csv',index=False)\n#lb = aml.leaderboard\n#lb.head()\n#lb.head(rows=lb.nrows)","f0b0e32a":"test1['gender'] = (h2o.as_list(preds['predict']))","949fdccf":"\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DH2o.csv',index = False)","ce13d230":"We could also see that there is some inconsitency in start and end time","b90b2607":"No null values","574efc6b":"Gender - Male and Female\nUnique session id\n9402 unique products","86ccff59":"Changing the orders","fce1685b":"More females view then male"}}