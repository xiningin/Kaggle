{"cell_type":{"3eedd04b":"code","00b6468e":"code","fa0d16b3":"code","ced75675":"code","e80405d5":"code","0fb473e6":"code","0825418c":"code","90005609":"code","2e6d90e5":"code","e2773f69":"code","c12a277d":"code","2142bd26":"code","6770840a":"code","bf91105d":"code","a7fac2fe":"code","958c7928":"code","4906b809":"code","f402bc7d":"code","53728938":"code","0f873747":"markdown","5734cac2":"markdown","e2abdaf8":"markdown","582fecb4":"markdown","41a0af2f":"markdown","21bd3792":"markdown","b9e7c72d":"markdown","8c73da06":"markdown","99a6b3de":"markdown","28fd1fb7":"markdown","ea3c13fb":"markdown"},"source":{"3eedd04b":"import numpy as np \nimport pandas as pd \nfrom pathlib import Path\n\nfrom fastai import *\nfrom fastai.vision import *\n\nimport os","00b6468e":"data_path = Path(\"..\/input\/cell_images\/cell_images\/\")","fa0d16b3":"data_path","ced75675":"transforms = get_transforms(do_flip = True, \n                            flip_vert = True, \n                            max_rotate = 10.0, \n                            max_zoom = 1.1, \n                            max_lighting = 0.2, \n                            max_warp = 0.2, \n                            p_affine = 0.75, \n                            p_lighting = 0.75)","e80405d5":"data = ImageDataBunch.from_folder(data_path,\n                                  train = '.',\n                                  valid_pct = 0.2,\n                                  size = 224,\n                                  bs = 16,\n                                  ds_tfms = transforms\n                                 ).normalize(imagenet_stats)","0fb473e6":"data.classes","0825418c":"data.show_batch(rows = 4, figsize = (7, 7))","90005609":"learn = cnn_learner(data, models.densenet161 , metrics = [accuracy, error_rate], model_dir = '\/tmp\/model\/')","2e6d90e5":"learn.lr_find()\n\nlearn.recorder.plot(suggestion = True)","e2773f69":"min_grad_lr = learn.recorder.min_grad_lr\n\nlearn.fit_one_cycle(20, min_grad_lr)","c12a277d":"learn.save('first-phase')","2142bd26":"learn.unfreeze()\n\nlearn.lr_find()\n\nlearn.recorder.plot(suggestion = True)","6770840a":"min_grad_lr = learn.recorder.min_grad_lr\n\nlearn.fit_one_cycle(30, min_grad_lr)","bf91105d":"learn.save('second-phase')","a7fac2fe":"learn.recorder.plot_losses()","958c7928":"learn.recorder.plot_metrics()","4906b809":"interp = ClassificationInterpretation.from_learner(learn)","f402bc7d":"interp.plot_top_losses(9, figsize = (15, 10))","53728938":"interp.plot_confusion_matrix()","0f873747":"Get data from folder and apply transformations to augment and normalize the data.\nSplit 20% of the data to validation.","5734cac2":"* Create a Learner Object with data, DenseNet model and the metrics","e2abdaf8":"Plot the loss","582fecb4":"Vizualize some of the images","41a0af2f":"Vizualize some images the model predicted wrong","21bd3792":"Now unfreeze the last convolutional layer and find an appropriate learning rate to train again","b9e7c72d":"Plot the confusion matrix","8c73da06":"Plot accuracy and error rate","99a6b3de":"Do some data augmentation","28fd1fb7":"Find a good value for the learning rate","ea3c13fb":"Train the model, at this moment the model is frozen"}}