{"cell_type":{"685f4f80":"code","53674877":"code","25842701":"code","0366739d":"code","f3495a67":"code","8247e663":"code","9e209c31":"code","a1166dfa":"code","fa759e2f":"code","b70b3414":"code","806cc252":"code","4b341355":"code","ab53001e":"code","e3008211":"code","4da94fe4":"code","9ed62dac":"code","50a547dd":"code","5f040aab":"code","1a8173f7":"code","32debdbb":"code","14100234":"code","03bdc9d8":"code","0b66f91f":"code","699ae7b2":"code","85e6f88a":"code","9884f5dc":"code","cf07baad":"code","91e5c0c7":"code","c402f509":"code","217c5bab":"code","32eb6a47":"code","8558ccb7":"code","a759368d":"code","e23027cc":"code","17295f52":"code","3d4e62da":"code","d9b2363b":"code","288dca1a":"code","9fdc6731":"code","465e795e":"code","86dba847":"code","4075e8fd":"markdown","14fb4be6":"markdown","8b1fba82":"markdown","5aaf7323":"markdown","34a6879c":"markdown","f55b8bf9":"markdown","b3d35595":"markdown","f6c99498":"markdown","cc18ff88":"markdown","7e5237d4":"markdown","69cc062a":"markdown"},"source":{"685f4f80":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\n\n# This is my free plotly account,this account allow to generate 100 images to be generated every 24 hours.Please use your own plotly account.\nplotly.tools.set_credentials_file(username=\"zgcr\", api_key=\"GQW92qmUOFbZmTQwQtJ1\")\nplotly.tools.set_config_file(world_readable=True)\n\nloan_data = pd.read_csv(\"..\/input\/loan.csv\", low_memory=False)\nprint(loan_data.shape)","53674877":"# calculate the missing value percent of features\ndef draw_missing_data_table(data):\n    total = data.isnull().sum().sort_values(ascending=False)\n    percent = (data.isnull().sum() \/ data.shape[0]).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=[\"Total\", \"Percent\"])\n    missing_data.reset_index(inplace=True)\n    missing_data.rename(columns={\"index\": \"feature_name\"}, inplace=True)\n\n    return missing_data","25842701":"# save missing value percent of features\nmissing_data_count = draw_missing_data_table(loan_data)\nmissing_data_count.to_csv(\"missing_data_count.csv\")\nmissing_data_count = pd.read_csv(\"missing_data_count.csv\", header=0, index_col=0)\nmissing_data_count = missing_data_count[missing_data_count[\"Percent\"] > 0.0]\nprint(missing_data_count.head())","0366739d":"# draw a graph of missing value percent of features(percent>0.03)\nmissing_data_count_show = missing_data_count[missing_data_count[\"Percent\"] > 0.03]\nsns.set_style(\"whitegrid\")\nf, ax = plt.subplots(figsize=(15, 10))\nsns.barplot(x=missing_data_count_show[\"Percent\"], y=missing_data_count_show[\"feature_name\"], ax=ax)\nax.set_title(\"Missing value percent for each feature\", fontsize=16)\nax.set_xlabel(\"missing percent\", fontsize=16)\nax.set_ylabel(\"feature name\", fontsize=16)\nplt.show()\nplt.close()","f3495a67":"# delete features that missing value percent more than 0.15\nfor index, feature_count_null in missing_data_count.iterrows():\n\tif feature_count_null[\"Percent\"] > 0.15:\n\t\tdrop_feature_name = feature_count_null[\"feature_name\"]\n\t\tloan_data.drop([drop_feature_name], axis=1, inplace=True)\n\nmissing_data_count = missing_data_count[missing_data_count[\"Percent\"] <= 0.15]\nprint(missing_data_count.head())","8247e663":"# delete rows which contain missing value for features that  missing value precent less than 0.04\nfor index, feature_count_null in missing_data_count.iterrows():\n\tif feature_count_null[\"Percent\"] < 0.04:\n\t\tdrop_feature_name = feature_count_null[\"feature_name\"]\n\t\tdrop_index = loan_data[loan_data[drop_feature_name].isnull().values == True].index\n\t\tloan_data.drop(index=drop_index, axis=0, inplace=True)\n\nprint(loan_data.shape)","9e209c31":"# calculate the missing value percent of features again,save missing value percent of features\nmissing_data_count_2 = draw_missing_data_table(loan_data)\nmissing_data_count_2.to_csv(\"missing_data_count_2.csv\")\nmissing_data_count_2 = missing_data_count_2[missing_data_count_2[\"Percent\"] > 0.0]\nprint(missing_data_count_2)","a1166dfa":"# fill missing value of mths_since_recent_inq\/num_tl_120dpd_2m\/mo_sin_old_il_acct by mean value of each feature\n# don\"t fill emp_title and emp_length\nloan_data[\"mths_since_recent_inq\"].fillna(loan_data[\"mths_since_recent_inq\"].mean(), inplace=True)\nloan_data[\"num_tl_120dpd_2m\"].fillna(loan_data[\"num_tl_120dpd_2m\"].mean(), inplace=True)\nloan_data[\"mo_sin_old_il_acct\"].fillna(loan_data[\"mo_sin_old_il_acct\"].mean(), inplace=True)\n# Convert the value of feature:\"term\" from category to numeric\nterm_dict = {\" 36 months\": 36, \" 60 months\": 60}\nloan_data[\"term\"] = loan_data[\"term\"].map(term_dict)\nloan_data[\"term\"] = loan_data[\"term\"].astype(\"float\")","fa759e2f":"# calculate the missing value percent of features the three times,save missing value percent of features\nmissing_data_count_3 = draw_missing_data_table(loan_data)\nmissing_data_count_3.to_csv(\"missing_data_count_3.csv\")\nmissing_data_count_3 = missing_data_count_3[missing_data_count_3[\"Percent\"] > 0.0]\nprint(missing_data_count_3)\nprint(loan_data.shape)","b70b3414":"# Data distribution of the loan amount and actual loan amount\nsns.set_style(\"whitegrid\")\nf_loan, ax_loan = plt.subplots(2, 2, figsize=(15, 10))\nsns.distplot(loan_data[\"loan_amnt\"], ax=ax_loan[0, 0], color=\"#F7522F\")\nsns.violinplot(y=loan_data[\"loan_amnt\"], ax=ax_loan[0, 1], inner=\"quartile\", palette=\"Reds\")\nsns.distplot(loan_data[\"funded_amnt\"], ax=ax_loan[1, 0], color=\"#2F8FF7\")\nsns.violinplot(y=loan_data[\"funded_amnt\"], ax=ax_loan[1, 1], inner=\"quartile\", palette=\"Blues\")\nax_loan[0, 0].set_title(\"Loan amount distribution\", fontsize=16)\nax_loan[0, 1].set_title(\"Loan amount distribution\", fontsize=16)\nax_loan[1, 0].set_title(\"Funded amount distribution\", fontsize=16)\nax_loan[1, 1].set_title(\"Funded amount distribution\", fontsize=16)\nax_loan[0, 0].set_xlabel(\"loan amount\", fontsize=16)\nax_loan[1, 0].set_xlabel(\"loan amount\", fontsize=16)\nax_loan[0, 1].set_ylabel(\"loan amount\", fontsize=16)\nax_loan[1, 1].set_ylabel(\"loan amount\", fontsize=16)\nplt.show()\nplt.close()","806cc252":"# histogram of annual loan figures and histogram of total amount of annual loan lending\nloan_data[\"year\"] = pd.to_datetime(loan_data[\"issue_d\"]).dt.year\nloan_year_num = loan_data[\"year\"].value_counts().to_dict()\nloan_year_num_pd = pd.DataFrame(list(loan_year_num.items()), columns=[\"year\", \"loan times\"])\nloan_year_num_pd.sort_values(\"year\", inplace=True)\n# print(loan_year_num_pd)\nloan_data[\"year\"] = pd.to_datetime(loan_data[\"issue_d\"]).dt.year\nloan_money_count_per_year = loan_data.groupby(\"year\")[\"loan_amnt\"].sum().to_dict()\nloan_money_count_per_year_pd = pd.DataFrame(list(loan_money_count_per_year.items()), columns=[\"year\", \"loan_amnt\"])\nloan_money_count_per_year_pd.sort_values(\"year\", inplace=True)\n# print(loan_money_count_per_year_pd)\nsns.set_style(\"whitegrid\")\nf_loan_per_year, ax_loan_per_year = plt.subplots(1, 2, figsize=(15, 10))\nsns.barplot(loan_year_num_pd[\"year\"], loan_year_num_pd[\"loan times\"], ax=ax_loan_per_year[0],\n            palette=\"tab10\")\nsns.barplot(loan_money_count_per_year_pd[\"year\"], loan_money_count_per_year_pd[\"loan_amnt\"], ax=ax_loan_per_year[1],\n            palette=\"tab10\")\nax_loan_per_year[0].set_title(\"loan times per year\", fontsize=16)\nax_loan_per_year[1].set_title(\"Loan amount per year\", fontsize=16)\nax_loan_per_year[0].set_xlabel(\"year\", fontsize=16)\nax_loan_per_year[0].set_ylabel(\"loan times\", fontsize=16)\nax_loan_per_year[1].set_xlabel(\"year\", fontsize=16)\nax_loan_per_year[1].set_ylabel(\"loan amount\", fontsize=16)\nplt.show()\nplt.close()\nloan_data.drop([\"year\"], axis=1, inplace=True)","4b341355":"# the map of geographical coordinates of each state\"s loan figures\n# addr_state\u5373\u7533\u8bf7\u8d37\u6b3e\u7684\u4eba\u7684\u6240\u5c5e\u5dde,\u662f\u4e24\u4f4d\u4ee3\u7801,\u53ef\u4ee5\u88abplotly\u8bc6\u522b\ncode_and_name_dict = {\"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n                      \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DC\": \"District of Columbia\", \"DE\": \"Delaware\",\n                      \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\",\n                      \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\",\n                      \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n                      \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\",\n                      \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\",\n                      \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\",\n                      \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\", \"TN\": \"Tennessee\",\n                      \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\",\n                      \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"}\nloan_times_per_state = loan_data[\"addr_state\"].value_counts().to_dict()\nloan_times_per_state_pd = pd.DataFrame(list(loan_times_per_state.items()), columns=[\"state_code\", \"loan_times\"])\nloan_times_per_state_pd[\"state_name\"] = None\n# print(loan_times_per_state_pd)\nfor i in range(loan_times_per_state_pd.shape[0]):\n\tstate_name = code_and_name_dict[loan_times_per_state_pd.ix[i, \"state_code\"]]\n\tloan_times_per_state_pd.ix[i, \"state_name\"] = state_name\n# print(loan_times_per_state_pd)\n# \u8bbe\u7acb\u989c\u8272\u6761\u8272\u5f69\u6e10\u53d8\u989c\u8272\n# colorscale\u53ef\u9009\u9879:[\"Greys\", \"YlGnBu\", \"Greens\", \"YlOrRd\", \"Bluered\", \"RdBu\",\"Reds\", \"Blues\", \"Picnic\", \"Rainbow\",\n# \"Portland\", \"Jet\",\"Hot\", \"Blackbody\", \"Earth\", \"Electric\", \"Viridis\", \"Cividis\"]\ncolorscale = \"Blues\"\n# colorbar\u4e3a\u989c\u8272\u6761\u6ce8\u91ca,\u4f4d\u7f6e\u7531\u5404\u5dde\u7684\u7f16\u53f7\uff0c\u5373\u7f29\u5199\u8868\u793a,z\u503c\u8d8a\u9ad8\u989c\u8272\u8d8a\u6df1\ndata = [dict(type=\"choropleth\", colorscale=colorscale, autocolorscale=False, reversescale=True,\n             locations=loan_times_per_state_pd[\"state_code\"], z=loan_times_per_state_pd[\"loan_times\"].astype(float),\n             locationmode=\"USA-states\", text=loan_times_per_state_pd[\"state_name\"],\n             marker=dict(line=dict(color=\"rgb(255,255,255)\", width=2)),\n             colorbar=dict(title=\"loan times\", titlefont=dict(color=\"rgb(0,0,0)\", size=32)))]\nlayout = dict(title=\"loan times per state map\", titlefont=dict(color=\"rgb(0,0,0)\", size=50),\n              geo=dict(scope=\"usa\", projection=dict(type=\"albers usa\")))\nfig = dict(data=data, layout=layout)\n# filename\u4e3a\u7f51\u7ad9\u4e0a\u4e2a\u4eba\u7a7a\u95f4\u4e2d\u4fdd\u5b58\u7684\u6587\u4ef6\u540d\npy.plot(fig, filename=\"loan times per state map\", auto_open=True)","ab53001e":"# Histogram of each state\"s loan figures (the top 30 states with the largest number of loans)\ncode_and_name_dict = {\"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n                      \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DC\": \"District of Columbia\", \"DE\": \"Delaware\",\n                      \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\",\n                      \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\",\n                      \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n                      \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\",\n                      \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\",\n                      \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\",\n                      \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\", \"TN\": \"Tennessee\",\n                      \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\",\n                      \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"}\nloan_times = loan_data[\"addr_state\"].value_counts().to_dict()\nloan_times_pd = pd.DataFrame(list(loan_times.items()), columns=[\"state_code\", \"loan_times\"])\nloan_times_pd[\"state_name\"] = None\n# print(loan_times_pd)\nfor i in range(loan_times_pd.shape[0]):\n\tstate_name = code_and_name_dict[loan_times_pd.ix[i, \"state_code\"]]\n\tloan_times_pd.ix[i, \"state_name\"] = state_name\n# print(loan_times_pd)\nloan_times_pd_30 = loan_times_pd[0:30]\nloan_times_pd_30.drop([\"state_code\"], axis=1)\nsns.set_style(\"whitegrid\")\nf_loan_times_per_state, ax_loan_times_per_state = plt.subplots(figsize=(15, 10))\n# # palette\u4e3a\u8c03\u8272\u677f\u53c2\u6570,\u53ef\u9009\u9879\"muted\"\\\"RdBu\"\\\"RdBu_r\"\\\"Blues_d\"\\\"Set1\"\\\"husl\"\nsns.barplot(loan_times_pd_30[\"loan_times\"], loan_times_pd_30[\"state_name\"], ax=ax_loan_times_per_state,\n            palette=\"tab10\")\nax_loan_times_per_state.set_title(\"loan times per state\", fontsize=16)\nax_loan_times_per_state.set_xlabel(\"loan times\", fontsize=16)\nax_loan_times_per_state.set_ylabel(\"state name\", fontsize=16)\nplt.show()\nplt.close()","e3008211":"# histogram of the top 30 profession of loan figures\nloan_times_title = loan_data[\"emp_title\"].value_counts().to_dict()\nloan_times_title_pd = pd.DataFrame(list(loan_times_title.items()), columns=[\"title\", \"loan_times\"])\nloan_times_title_pd_30 = loan_times_title_pd[0:30]\nsns.set_style(\"whitegrid\")\nf_loan_times_per_title, ax_loan_times_per_title = plt.subplots(figsize=(15, 10))\n# # palette\u4e3a\u8c03\u8272\u677f\u53c2\u6570,\u53ef\u9009\u9879\"muted\"\\\"RdBu\"\\\"RdBu_r\"\\\"Blues_d\"\\\"Set1\"\\\"husl\"\nsns.barplot(loan_times_title_pd_30[\"loan_times\"], loan_times_title_pd_30[\"title\"], ax=ax_loan_times_per_title,\n            palette=\"tab10\")\nax_loan_times_per_title.set_title(\"loan times per title\", fontsize=16)\nax_loan_times_per_title.set_xlabel(\"loan times\", fontsize=16)\nax_loan_times_per_title.set_ylabel(\"title\", fontsize=16)\nplt.show()\nplt.close()","4da94fe4":"# histogram of the year of participation in working with loan figures\nloan_times_length = loan_data[\"emp_length\"].value_counts().to_dict()\n# print(loan_times_length)\n# {\"10+ years\": 713245, \"2 years\": 192330, \"< 1 year\": 179177, \"3 years\": 170699, \"1 year\": 139017, \"5 years\": 130985,\n# \"4 years\": 128027, \"6 years\": 96294, \"7 years\": 87537, \"8 years\": 87182, \"9 years\": 75405}\nloan_times_length_pd = pd.DataFrame(list(loan_times_length.items()), columns=[\"length\", \"loan_times\"])\nsns.set_style(\"whitegrid\")\nf_loan_times_per_length, ax_loan_times_per_length = plt.subplots(figsize=(15, 10))\n# palette\u4e3a\u8c03\u8272\u677f\u53c2\u6570,\u53ef\u9009\u9879\"muted\"\\\"RdBu\"\\\"RdBu_r\"\\\"Blues_d\"\\\"Set1\"\\\"husl\"\nsns.barplot(loan_times_length_pd[\"length\"], loan_times_length_pd[\"loan_times\"], ax=ax_loan_times_per_length,\n            palette=\"Blues_d\")\nax_loan_times_per_length.set_title(\"loan times per length\", fontsize=16)\nax_loan_times_per_length.set_xlabel(\"worked length\", fontsize=16)\nax_loan_times_per_length.set_ylabel(\"loan times\", fontsize=16)\nplt.show()\nplt.close()","9ed62dac":"# histogram of the customer\"s annual income with loan figures\n# \u6211\u4eec\u5c06\u5e74\u6536\u5165\u5206\u4e3a\u4e09\u6863:20000\u4ee5\u4e0b\u4e3alow\uff0c20000-60000\u4e3amid\uff0c>60000\u4e3ahigh\nmax_value = loan_data[\"annual_inc\"].max() + 1.0\nset_bins = [0.0, 20000.0, 60000.0, max_value]\nset_label = [\"low\", \"mid\", \"high\"]\nloan_data[\"income\"] = pd.cut(loan_data[\"annual_inc\"], bins=set_bins, labels=set_label)\nloan_times_income = loan_data[\"income\"].value_counts().to_dict()\n# print(loan_times_income)\n# {\"high\": 1187055, \"mid\": 912572, \"low\": 37443}\nloan_times_income_pd = pd.DataFrame(list(loan_times_income.items()), columns=[\"income\", \"loan_times\"])\nsns.set_style(\"whitegrid\")\nf_loan_times_per_income, ax_loan_times_per_income = plt.subplots(figsize=(15, 10))\n# palette\u4e3a\u8c03\u8272\u677f\u53c2\u6570,\u53ef\u9009\u9879\"muted\"\\\"RdBu\"\\\"RdBu_r\"\\\"Blues_d\"\\\"Set1\"\\\"husl\"\nsns.barplot(loan_times_income_pd[\"income\"], loan_times_income_pd[\"loan_times\"], ax=ax_loan_times_per_income,\n            palette=\"muted\")\nax_loan_times_per_income.set_title(\"loan times per income\", fontsize=16)\nax_loan_times_per_income.set_xlabel(\"income\", fontsize=16)\nax_loan_times_per_income.set_ylabel(\"loan times\", fontsize=16)\nplt.show()\nplt.close()\nloan_data.drop([\"income\"], axis=1, inplace=True)","50a547dd":"# The ratio of good loans and bad loans for each year\n# print(loan_data[\"loan_status\"].value_counts().to_dict())\n# {\"Fully Paid\": 962556, \"Current\": 899615, \"Charged Off\": 241514, \"Late (31-120 days)\": 21051, \"In Grace Period\": 8701,\n# \"Late (16-30 days)\": 3607, \"Default\": 29}\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\n# \u75281\u8868\u793a\u8d37\u6b3e\u72b6\u51b5\u826f\u597d\uff0c\u75280\u8868\u793a\u4e0d\u826f\u8d37\u6b3e\nloan_data[\"loan_status_count\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count\"] = loan_data[\"loan_status_count\"].astype(\"float\")\n# print(loan_data[\"loan_status\"].value_counts().to_dict())\n# {1.0: 1862171, 0.0: 274902}\u53ef\u4ee5\u770b\u5230\u6b63\u8d1f\u6837\u672c\u4e0d\u5747\u8861\uff0c\u5728\u540e\u9762\u6211\u4eec\u8bad\u7ec3\u6a21\u578b\u9884\u6d4bloan_status\u65f6\u9700\u8981\u6ce8\u610f\u6b63\u8d1f\u6837\u672c\u4e0d\u5e73\u8861\u7684\u95ee\u9898\nloan_status_count = loan_data[\"loan_status_count\"].value_counts().to_dict()\nif 0 not in loan_status_count.keys():\n\tloan_status_count[\"0\"] = 0.0\ncount_sum = 0\nfor key, value in loan_status_count.items():\n\tcount_sum += value\nfor key, value in loan_status_count.items():\n\tvalue = value \/ count_sum\n\tloan_status_count[key] = value\nloan_status_count_pd = pd.DataFrame(list(loan_status_count.items()), columns=[\"loan status\", \"count_percent\"])\n# print(loan_status_count_pd)\n#    loan status  count_percent\n# 0          1.0       0.871365\n# 1          0.0       0.128635\nloan_data[\"year\"] = pd.to_datetime(loan_data[\"issue_d\"]).dt.year\nf_loan_status, ax_loan_status = plt.subplots(1, 2, figsize=(15, 10))\nlabels = \"Good loans\", \"Bad loans\"\nax_loan_status[0].pie(loan_status_count_pd[\"count_percent\"], autopct=\"%1.2f%%\", shadow=True,\n                      labels=labels, startangle=70)\nloan_status_dict = {1.0: \"Good loans\", 0.0: \"Bad loans\"}\nloan_data[\"loan_status_count\"] = loan_data[\"loan_status_count\"].map(loan_status_dict)\nsns.barplot(x=loan_data[\"year\"], y=loan_data[\"loan_amnt\"], hue=loan_data[\"loan_status_count\"], hue_order=labels,\n            ax=ax_loan_status[1], estimator=lambda x: len(x) \/ len(loan_data[\"loan_status_count\"]) * 100)\nax_loan_status[0].set_title(\"good loans and bad loans percent\", fontsize=16)\nax_loan_status[0].set_ylabel(\"Loans percent\", fontsize=16)\nax_loan_status[1].set_title(\"good loans and bad loans percent per year\", fontsize=16)\nax_loan_status[1].set_ylabel(\"Loans percent\", fontsize=16)\nplt.show()\nplt.close()\nloan_data.drop([\"loan_status_count\",\"year\"], axis=1, inplace=True)","5f040aab":"# the map of geographical coordinates of each state\"s good loan number and bad loan number\ncode_and_name_dict = {\"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n                      \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DC\": \"District of Columbia\", \"DE\": \"Delaware\",\n                      \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\",\n                      \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\",\n                      \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n                      \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\",\n                      \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\",\n                      \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\",\n                      \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\", \"TN\": \"Tennessee\",\n                      \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\",\n                      \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"}\n# \u4e3a\u4e86\u4fbf\u4e8e\u8ba1\u7b97\u574f\u8d26\u6570,\u6211\u4eec\u4ee4\u574f\u8d26\u4e3a1,\u597d\u5e10\u4e3a0\nloan_status_dict = {\"Fully Paid\": 0, \"Current\": 0, \"Charged Off\": 1, \"Late (31-120 days)\": 1,\n                    \"In Grace Period\": 1, \"Late (16-30 days)\": 1, \"Default\": 1}\n# \u75281\u8868\u793a\u8d37\u6b3e\u72b6\u51b5\u826f\u597d\uff0c\u75280\u8868\u793a\u4e0d\u826f\u8d37\u6b3e\nloan_data[\"loan_status_count_2\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count_2\"] = loan_data[\"loan_status_count_2\"].astype(\"float\")\n# print(loan_data[\"loan_status_count\"].value_counts().to_dict())\n# {0.0: 1862171, 1.0: 274902}\nloan_status_per_state = loan_data.groupby(\"addr_state\")[\"loan_status_count_2\"].sum().to_dict()\n# print(loan_status_per_state)\nloan_status_per_state_pd = pd.DataFrame(list(loan_status_per_state.items()),\n                                        columns=[\"state_code\", \"bad_loan_num\"])\nloan_status_per_state_pd[\"state_name\"] = None\n# print(loan_status_per_state_pd)\nfor i in range(loan_status_per_state_pd.shape[0]):\n\tstate_name = code_and_name_dict[loan_status_per_state_pd.ix[i, \"state_code\"]]\n\tloan_status_per_state_pd.ix[i, \"state_name\"] = state_name\n# print(loan_status_per_state_pd)\n# \u8bbe\u7acb\u989c\u8272\u6761\u8272\u5f69\u6e10\u53d8\u989c\u8272\n# colorscale\u53ef\u9009\u9879:[\"Greys\", \"YlGnBu\", \"Greens\", \"YlOrRd\", \"Bluered\", \"RdBu\",\"Reds\", \"Blues\", \"Picnic\", \"Rainbow\",\n# \"Portland\", \"Jet\",\"Hot\", \"Blackbody\", \"Earth\", \"Electric\", \"Viridis\", \"Cividis\"]\ncolorscale = \"Hot\"\n# colorbar\u4e3a\u989c\u8272\u6761\u6ce8\u91ca,\u4f4d\u7f6e\u7531\u5404\u5dde\u7684\u7f16\u53f7\uff0c\u5373\u7f29\u5199\u8868\u793a,z\u503c\u8d8a\u9ad8\u989c\u8272\u8d8a\u6df1\ndata = [dict(type=\"choropleth\", colorscale=colorscale, autocolorscale=False, reversescale=True,\n             locations=loan_status_per_state_pd[\"state_code\"], z=loan_status_per_state_pd[\"bad_loan_num\"],\n             locationmode=\"USA-states\", text=loan_status_per_state_pd[\"state_name\"],\n             marker=dict(line=dict(color=\"rgb(255,255,255)\", width=2)),\n             colorbar=dict(title=\"bad loans num\", titlefont=dict(color=\"rgb(0,0,0)\", size=32)))]\nlayout = dict(title=\"bad loans num per state map\", titlefont=dict(color=\"rgb(0,0,0)\", size=50),\n              geo=dict(scope=\"usa\", projection=dict(type=\"albers usa\")))\nfig = dict(data=data, layout=layout)\n# filename\u4e3a\u7f51\u7ad9\u4e0a\u4e2a\u4eba\u7a7a\u95f4\u4e2d\u4fdd\u5b58\u7684\u6587\u4ef6\u540d\nloan_data.drop([\"loan_status_count_2\"], axis=1, inplace=True)\npy.plot(fig, filename=\"bad loans num per state map\", auto_open=True)","1a8173f7":"# the map of geographical coordinates of each state\"s good loan ratio and bad loan ratio\ncode_and_name_dict = {\"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n                      \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DC\": \"District of Columbia\", \"DE\": \"Delaware\",\n                      \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\",\n                      \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\",\n                      \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n                      \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\",\n                      \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\",\n                      \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\",\n                      \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\", \"TN\": \"Tennessee\",\n                      \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\",\n                      \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"}\n# \u4e3a\u4e86\u4fbf\u4e8e\u8ba1\u7b97\u574f\u8d26\u6570,\u6211\u4eec\u4ee4\u574f\u8d26\u4e3a1,\u597d\u5e10\u4e3a0\nloan_status_dict = {\"Fully Paid\": 0, \"Current\": 0, \"Charged Off\": 1, \"Late (31-120 days)\": 1,\n                    \"In Grace Period\": 1, \"Late (16-30 days)\": 1, \"Default\": 1}\n# \u75281\u8868\u793a\u8d37\u6b3e\u72b6\u51b5\u826f\u597d\uff0c\u75280\u8868\u793a\u4e0d\u826f\u8d37\u6b3e\nloan_data[\"loan_status_count_3\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count_3\"] = loan_data[\"loan_status_count_3\"].astype(\"float\")\n# print(loan_data[\"loan_status_count\"].value_counts().to_dict())\n# {0.0: 1862171, 1.0: 274902}\nloan_status_per_state = loan_data.groupby(\"addr_state\")[\"loan_status_count_3\"].sum().to_dict()\nloan_status_per_state_pd = pd.DataFrame(list(loan_status_per_state.items()),\n                                        columns=[\"state_code\", \"bad_loan_percent\"])\nloan_times_per_state_sum_dict = loan_data[\"addr_state\"].value_counts().to_dict()\nloan_status_per_state_pd[\"state_name\"] = None\n# print(loan_status_per_state_pd)\nfor i in range(loan_status_per_state_pd.shape[0]):\n\tstate_name = code_and_name_dict[loan_status_per_state_pd.ix[i, \"state_code\"]]\n\tloan_status_per_state_pd.ix[i, \"state_name\"] = state_name\n# print(loan_status_per_state_pd)\n# print(loan_times_per_state_sum_dict)\nfor i in range(loan_status_per_state_pd.shape[0]):\n\tper_state_sum = loan_times_per_state_sum_dict[loan_status_per_state_pd.ix[i, \"state_code\"]]\n\tloan_status_per_state_pd.ix[i, \"bad_loan_percent\"] = float(\n\t\tloan_status_per_state_pd.ix[i, \"bad_loan_percent\"]) \/ per_state_sum\n# print(loan_status_per_state_pd)\n# \u8bbe\u7acb\u989c\u8272\u6761\u8272\u5f69\u6e10\u53d8\u989c\u8272\n# colorscale\u53ef\u9009\u9879:[\"Greys\", \"YlGnBu\", \"Greens\", \"YlOrRd\", \"Bluered\", \"RdBu\",\"Reds\", \"Blues\", \"Picnic\", \"Rainbow\",\n# \"Portland\", \"Jet\",\"Hot\", \"Blackbody\", \"Earth\", \"Electric\", \"Viridis\", \"Cividis\"]\ncolorscale = \"Reds\"\n# colorbar\u4e3a\u989c\u8272\u6761\u6ce8\u91ca,\u4f4d\u7f6e\u7531\u5404\u5dde\u7684\u7f16\u53f7\uff0c\u5373\u7f29\u5199\u8868\u793a,z\u503c\u8d8a\u9ad8\u989c\u8272\u8d8a\u6df1\ndata = [dict(type=\"choropleth\", colorscale=colorscale, autocolorscale=False, reversescale=False,\n             locations=loan_status_per_state_pd[\"state_code\"], z=loan_status_per_state_pd[\"bad_loan_percent\"],\n             locationmode=\"USA-states\", text=loan_status_per_state_pd[\"state_name\"],\n             marker=dict(line=dict(color=\"rgb(255,255,255)\", width=2)),\n             colorbar=dict(title=\"bad loans percent\", titlefont=dict(color=\"rgb(0,0,0)\", size=32)))]\nlayout = dict(title=\"bad loans percent per state map\", titlefont=dict(color=\"rgb(0,0,0)\", size=50),\n              geo=dict(scope=\"usa\", projection=dict(type=\"albers usa\")))\nfig = dict(data=data, layout=layout)\nloan_data.drop([\"loan_status_count_3\"], axis=1, inplace=True)\n# filename\u4e3a\u7f51\u7ad9\u4e0a\u4e2a\u4eba\u7a7a\u95f4\u4e2d\u4fdd\u5b58\u7684\u6587\u4ef6\u540d\npy.plot(fig, filename=\"bad loans percent per state map\", auto_open=True)","32debdbb":"# the ratio of good loans and bad loans for each loan purpose\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\nloan_data[\"loan_status_count_4\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count_4\"] = loan_data[\"loan_status_count_4\"].astype(\"float\")\nf_loan_status_purpose, ax_loan_status_purpose = plt.subplots(figsize=(15, 10))\nlabels = \"Good loans\", \"Bad loans\"\nloan_status_dict = {1.0: \"Good loans\", 0.0: \"Bad loans\"}\nloan_data[\"loan_status_count_4\"] = loan_data[\"loan_status_count_4\"].map(loan_status_dict)\nsns.barplot(x=loan_data[\"purpose\"], y=loan_data[\"loan_amnt\"], hue=loan_data[\"loan_status_count_4\"], hue_order=labels,\n            ax=ax_loan_status_purpose, estimator=lambda x: len(x) \/ len(loan_data[\"loan_status_count_4\"]) * 100)\nax_loan_status_purpose.set_title(\"Loan status per purpose percent\", fontsize=16)\nax_loan_status_purpose.set_xticklabels(ax_loan_status_purpose.get_xticklabels(), rotation=45)\nax_loan_status_purpose.set_xlabel(\"purpose\", fontsize=16)\nax_loan_status_purpose.set_ylabel(\"per purpose loans percent\", fontsize=16)\nplt.show()\nplt.close()\nloan_data.drop([\"loan_status_count_4\"], axis=1, inplace=True)","14100234":"# ratio of housing for good loans customer and bad loans customer\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\nloan_data[\"loan_status_count_5\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count_5\"] = loan_data[\"loan_status_count_5\"].astype(\"float\")\nf_loan_status_home, ax_loan_status_home = plt.subplots(figsize=(15, 10))\nlabels = \"Good loans\", \"Bad loans\"\nloan_status_dict = {1.0: \"Good loans\", 0.0: \"Bad loans\"}\nloan_data[\"loan_status_count_5\"] = loan_data[\"loan_status_count_5\"].map(loan_status_dict)\nsns.barplot(x=loan_data[\"home_ownership\"], y=loan_data[\"loan_amnt\"], hue=loan_data[\"loan_status_count_5\"],\n            hue_order=labels,\n            ax=ax_loan_status_home, estimator=lambda x: len(x) \/ len(loan_data[\"loan_status_count_5\"]) * 100)\nax_loan_status_home.set_title(\"Loan status per home percent\", fontsize=16)\nax_loan_status_home.set_xlabel(\"home ownership\", fontsize=16)\nax_loan_status_home.set_ylabel(\"per home ownership loans percent\", fontsize=16)\nplt.show()\nplt.close()\nloan_data.drop([\"loan_status_count_5\"], axis=1, inplace=True)","03bdc9d8":"# ratio of good loans and bad loans for each income level\n# \u6211\u4eec\u5c06\u5e74\u6536\u5165\u5206\u4e3a\u4e09\u6863:20000\u4ee5\u4e0b\u4e3alow\uff0c20000-60000\u4e3amid\uff0c>60000\u4e3ahigh\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\nloan_data[\"loan_status_count_6\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count_6\"] = loan_data[\"loan_status_count_6\"].astype(\"float\")\nmax_value = loan_data[\"annual_inc\"].max() + 1.0\nset_bins = [0.0, 20000.0, 60000.0, max_value]\nset_label = [\"low\", \"mid\", \"high\"]\nloan_data[\"income\"] = pd.cut(loan_data[\"annual_inc\"], bins=set_bins, labels=set_label)\nf_loan_status_income, ax_loan_status_income = plt.subplots(figsize=(15, 10))\nlabels = \"Good loans\", \"Bad loans\"\nloan_status_dict = {1.0: \"Good loans\", 0.0: \"Bad loans\"}\nloan_data[\"loan_status_count_6\"] = loan_data[\"loan_status_count_6\"].map(loan_status_dict)\nsns.barplot(x=loan_data[\"income\"], y=loan_data[\"loan_amnt\"], hue=loan_data[\"loan_status_count_6\"], hue_order=labels,\n            ax=ax_loan_status_income, estimator=lambda x: len(x) \/ len(loan_data[\"loan_status_count_6\"]) * 100)\nax_loan_status_income.set_title(\"Loan status per income percent\", fontsize=16)\nax_loan_status_income.set_xlabel(\"income\", fontsize=16)\nax_loan_status_income.set_ylabel(\"per income loans percent\", fontsize=16)\nplt.show()\nplt.close()\nloan_data.drop([\"loan_status_count_6\",\"income\"], axis=1, inplace=True)","0b66f91f":"# the ratio of good loans and bad loans for each credit rating\nloan_data_sorted=loan_data.sort_values(by=[\"grade\"], inplace=False)\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\nloan_data_sorted[\"loan_status_count_7\"] = loan_data_sorted[\"loan_status\"].map(loan_status_dict)\nloan_data_sorted[\"loan_status_count_7\"] = loan_data_sorted[\"loan_status_count_7\"].astype(\"float\")\nf_loan_status_grade, ax_loan_status_grade = plt.subplots(figsize=(15, 10))\nlabels = \"Good loans\", \"Bad loans\"\nloan_status_dict = {1.0: \"Good loans\", 0.0: \"Bad loans\"}\nloan_data_sorted[\"loan_status_count_7\"] = loan_data_sorted[\"loan_status_count_7\"].map(loan_status_dict)\nsns.barplot(x=loan_data_sorted[\"grade\"], y=loan_data_sorted[\"loan_amnt\"], hue=loan_data_sorted[\"loan_status_count_7\"], hue_order=labels,\n            ax=ax_loan_status_grade, estimator=lambda x: len(x) \/ len(loan_data_sorted[\"loan_status_count_7\"]) * 100)\nax_loan_status_grade.set_title(\"Loan status per grade percent\", fontsize=16)\nax_loan_status_grade.set_xlabel(\"grade\", fontsize=16)\nax_loan_status_grade.set_ylabel(\"per grade loans percent\", fontsize=16)\nplt.show()\nplt.close()","699ae7b2":"# data distribution of loan interest rates\nsns.set_style(\"whitegrid\")\nf_int_rate, ax_int_rate = plt.subplots(1, 2, figsize=(15, 10))\nsns.distplot(loan_data[\"int_rate\"], ax=ax_int_rate[0], color=\"#2F8FF7\")\nsns.violinplot(y=loan_data[\"int_rate\"], ax=ax_int_rate[1], inner=\"quartile\", palette=\"Blues\")\nax_int_rate[0].set_title(\"Int rate distribution\", fontsize=16)\nax_int_rate[1].set_title(\"Int rate distribution\", fontsize=16)\nax_int_rate[0].set_xlabel(\"Int rate\", fontsize=16)\nax_int_rate[0].set_ylabel(\"Int rate\", fontsize=16)\nplt.show()\nplt.close()","85e6f88a":"# average interest rate of good loans and bad loans for each credit ratings\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\nloan_data[\"loan_status_count_8\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count_8\"] = loan_data[\"loan_status_count_8\"].astype(\"float\")\nf_inc_rate_grade, ax_inc_rate_grade = plt.subplots(figsize=(15, 10))\nlabels = \"Good loans\", \"Bad loans\"\nloan_status_dict = {1.0: \"Good loans\", 0.0: \"Bad loans\"}\nloan_data[\"loan_status_count_8\"] = loan_data[\"loan_status_count_8\"].map(loan_status_dict)\nloan_data_sorted = loan_data.sort_values(by=[\"grade\"], inplace=False)\nsns.barplot(x=loan_data_sorted[\"grade\"], y=loan_data_sorted[\"int_rate\"], hue=loan_data_sorted[\"loan_status_count_8\"],\n            hue_order=labels, ax=ax_inc_rate_grade, ci=None)\nax_inc_rate_grade.set_title(\"mean int rate per grade\", fontsize=16)\nax_inc_rate_grade.set_xlabel(\"grade\", fontsize=16)\nax_inc_rate_grade.set_ylabel(\"mean int rate\", fontsize=16)\nplt.show()\nplt.close()\nloan_data.drop([\"loan_status_count_8\"], axis=1, inplace=True)","9884f5dc":"# data distribution of the percentage of the lender\"s month-repayments divide the lender\"s income\n# max_value=loan_data[\"dti\"].max()\n# min_value=loan_data[\"dti\"].min()\n# print(max_value,min_value)\n# 999.0 -1.0 \u8fd9\u91cc\u7684\u6570\u503c\u5e94\u5f53\u662f\u767e\u5206\u6bd4\n# \u8be5\u6570\u636e\u4e2d\u6709\u5f02\u5e38\u503c-1.0,\u4e14\u6709\u5f88\u5927\u7684\u79bb\u6563\u503c,\u6211\u4eec\u9700\u8981\u5c06\u5176\u5148\u8fc7\u6ee4\u6389,\u5426\u5219\u56fe\u50cf\u6548\u679c\u4e0d\u597d\nloan_data_dti = loan_data[loan_data[\"dti\"] <= 100.0]\nloan_data_dti = loan_data_dti[loan_data_dti[\"dti\"] > 0.0]\nsns.set_style(\"whitegrid\")\nf_dti, ax_dti = plt.subplots(1, 2, figsize=(15, 10))\nsns.distplot(loan_data_dti[\"dti\"], ax=ax_dti[0], color=\"#F7522F\")\nsns.violinplot(y=loan_data_dti[\"dti\"], ax=ax_dti[1], inner=\"quartile\", palette=\"Reds\")\nax_dti[0].set_title(\"dti distribution\", fontsize=16)\nax_dti[1].set_title(\"dti distribution\", fontsize=16)\nax_dti[0].set_xlabel(\"dti\", fontsize=16)\nax_dti[1].set_ylabel(\"dti\", fontsize=16)\nplt.show()\nplt.close()","cf07baad":"# data distribution of the percentage of the good lender\"s month-repayments divide the lender\"s income and the percentage of the bad lender\"s month-repayments divide the lender\"s income\n# \u8bf7\u5148\u8fc7\u6ee4\u5f02\u5e38\u503c\u548c\u6781\u5927\u79bb\u6563\u70b9,\u53ea\u4fdd\u75590-200\u4e4b\u95f4\u7684\u6570\u636e\nloan_data_dti = loan_data[loan_data[\"dti\"] <= 100.0]\nloan_data_dti = loan_data_dti[loan_data_dti[\"dti\"] >= 0.0]\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\nloan_data_dti[\"loan_status_count_9\"] = loan_data_dti[\"loan_status\"].map(loan_status_dict)\nloan_data_dti[\"loan_status_count_9\"] = loan_data_dti[\"loan_status_count_9\"].astype(\"float\")\nlabels = \"Bad loans\", \"Good loans\"\n# \u53d6\u51fagroupby\u540e\u7684\u5206\u7ec4\u7ed3\u679c\nloans_dti_per_status = dict(list(loan_data_dti.groupby(\"loan_status_count_9\")[\"dti\"]))\ngood_loan_dti = pd.DataFrame(loans_dti_per_status[1.0], index=None).reset_index(drop=True)\nbad_loan_dti = pd.DataFrame(loans_dti_per_status[0.0], index=None).reset_index(drop=True)\n# print(good_loan_dti, bad_loan_dti)\n# print(good_loan_dti.shape, bad_loan_dti.shape)\nsns.set_style(\"whitegrid\")\nf_dti_per_loan_status, ax_dti_per_loan_status = plt.subplots(1, 2, figsize=(15, 10))\nsns.distplot(good_loan_dti[\"dti\"], ax=ax_dti_per_loan_status[0], color=\"#2F8FF7\")\nsns.distplot(bad_loan_dti[\"dti\"], ax=ax_dti_per_loan_status[1], color=\"#F7522F\")\nax_dti_per_loan_status[0].set_title(\"good loans dti distribution\", fontsize=16)\nax_dti_per_loan_status[1].set_title(\"bad loans dti distribution\", fontsize=16)\nax_dti_per_loan_status[0].set_xlabel(\"dti\", fontsize=16)\nax_dti_per_loan_status[1].set_ylabel(\"dti\", fontsize=16)\nplt.show()\nplt.close()","91e5c0c7":"# ratio of short-term and long-term loans for good loans and bad loans\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\nloan_data[\"loan_status_count_10\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count_10\"] = loan_data[\"loan_status_count_10\"].astype(\"float\")\nf_loan_status_term, ax_loan_status_term = plt.subplots(figsize=(15, 10))\nlabels = \"Good loans\", \"Bad loans\"\nloan_status_dict = {1.0: \"Good loans\", 0.0: \"Bad loans\"}\nloan_data[\"loan_status_count_10\"] = loan_data[\"loan_status_count_10\"].map(loan_status_dict)\nloan_data_sorted=loan_data.sort_values(by=[\"grade\"], inplace=False)\nsns.barplot(x=loan_data_sorted[\"term\"], y=loan_data_sorted[\"loan_amnt\"], hue=loan_data_sorted[\"loan_status_count_10\"], hue_order=labels,\n            ax=ax_loan_status_term, estimator=lambda x: len(x) \/ len(loan_data_sorted[\"loan_status_count_10\"]) * 100)\nax_loan_status_term.set_title(\"loan times per term\", fontsize=16)\nax_loan_status_term.set_xlabel(\"term\", fontsize=16)\nax_loan_status_term.set_ylabel(\"loan times\", fontsize=16)\nplt.show()\nplt.close()\nloan_data.drop([\"loan_status_count_10\"], axis=1, inplace=True)","c402f509":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","217c5bab":"# There is a serious problem of sample imbalance in this dataset\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\n# \u75281\u8868\u793a\u8d37\u6b3e\u72b6\u51b5\u826f\u597d\uff0c\u75280\u8868\u793a\u4e0d\u826f\u8d37\u6b3e\nloan_data[\"loan_status_count_11\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status_count_11\"] = loan_data[\"loan_status_count_11\"].astype(\"float\")\nloan_data_status_count = loan_data[\"loan_status_count_11\"].value_counts().to_dict()\nsum_value = 0.0\nfor key, value in loan_data_status_count.items():\n\tsum_value += value\nfor key, value in loan_data_status_count.items():\n\tloan_data_status_count[key] = value \/ sum_value\nprint(loan_data_status_count)\n# {1.0: 0.8713651803190625, 0.0: 0.12863481968093743}\nloan_data.drop([\"loan_status_count_11\"], axis=1, inplace=True)\nprint(loan_data.shape)","32eb6a47":"# feature:loan_status\u2018s change to 0 or 1\nloan_status_dict = {\"Fully Paid\": 1, \"Current\": 1, \"Charged Off\": 0, \"Late (31-120 days)\": 0,\n                    \"In Grace Period\": 0, \"Late (16-30 days)\": 0, \"Default\": 0}\n# 1 is good loan,0 is bad loan\nloan_data[\"loan_status\"] = loan_data[\"loan_status\"].map(loan_status_dict)\nloan_data[\"loan_status\"] = loan_data[\"loan_status\"].astype(\"float\")\n# print(loan_data[\"emp_length\"].value_counts().to_dict)\nemp_length_dict = {\"10+ years\": 10, \"2 years\": 2, \"< 1 year\": 0.5, \"3 years\": 3, \"1 year\": 1, \"5 years\": 5,\n                   \"4 years\": 4, \"6 years\": 6, \"7 years\": 7, \"8 years\": 8, \"9 years\": 9}\nloan_data[\"emp_length\"] = loan_data[\"emp_length\"].map(emp_length_dict)\nloan_data[\"emp_length\"] = loan_data[\"emp_length\"].astype(\"float\")\n# fill missing value of emp_length to 0\nloan_data[\"emp_length\"].fillna(value=0, inplace=True)\n# drop some features\nloan_data.drop([\"emp_title\", \"title\", \"zip_code\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\"], axis=1,\n               inplace=True)\nloan_data[\"month\"], loan_data[\"year\"] = loan_data[\"issue_d\"].str.split(\"-\", 1).str\nloan_data.drop([\"issue_d\"], axis=1, inplace=True)\nprint(loan_data.shape)","8558ccb7":"numerical_feature_name = loan_data.columns[(loan_data.dtypes == \"float64\") | (loan_data.dtypes == \"int64\")].tolist()\n# category_feature_name = loan_data.columns[loan_data.dtypes == \"object\"].tolist()\n# draw pearson correlation coefficient matrix\n# \u82e5>0\uff0c\u8868\u660e\u4e24\u4e2a\u53d8\u91cf\u662f\u6b63\u76f8\u5173,\u5373\u4e00\u4e2a\u53d8\u91cf\u7684\u503c\u8d8a\u5927\uff0c\u53e6\u4e00\u4e2a\u53d8\u91cf\u7684\u503c\u4e5f\u4f1a\u8d8a\u5927\n# \u82e5<0\uff0c\u8868\u660e\u4e24\u4e2a\u53d8\u91cf\u662f\u8d1f\u76f8\u5173\uff0c\u5373\u4e00\u4e2a\u53d8\u91cf\u7684\u503c\u8d8a\u5927\u53e6\u4e00\u4e2a\u53d8\u91cf\u7684\u503c\u53cd\u800c\u4f1a\u8d8a\u5c0f\n# \u82e5r=0\uff0c\u8868\u660e\u4e24\u4e2a\u53d8\u91cf\u95f4\u4e0d\u662f\u7ebf\u6027\u76f8\u5173\uff0c\u4f46\u6709\u53ef\u80fd\u662f\u975e\u7ebf\u6027\u76f8\u5173\ncorrmat = loan_data[numerical_feature_name].corr()\nf, ax = plt.subplots(figsize=(30, 20))\n# vmax\u3001vmin\u5373\u70ed\u529b\u56fe\u989c\u8272\u53d6\u503c\u7684\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c,\u9ed8\u8ba4\u4f1a\u4ecedata\u4e2d\u63a8\u5bfc\n# square=True\u4f1a\u5c06\u5355\u5143\u683c\u8bbe\u4e3a\u6b63\u65b9\u5f62\nsns.heatmap(corrmat, square=True, ax=ax, cmap=\"Blues\", linewidths=0.5)\nax.set_title(\"Correlation coefficient matrix\", fontsize=16)\nax.set_xlabel(\"feature names\", fontsize=16)\nax.set_ylabel(\"feature names\", fontsize=16)\nplt.show()\nplt.close()","a759368d":"# split features and labels\nX = loan_data.ix[:, loan_data.columns != \"loan_status\"]\nY = loan_data[\"loan_status\"]\nprint(X.shape, Y.shape)\n# (2137073, 81) (2137073,)\nX = pd.get_dummies(X, drop_first=True)\nprint(X.shape)","e23027cc":"# # if use lr model to train and test,please run this cell\n# # divide training sets and testing sets\n# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n# print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n# # (1709658, 200) (427415, 200) (1709658,) (427415,)\n# numerical_feature_name_2 = X.columns[(X.dtypes == \"float64\") | (X.dtypes == \"int64\")].tolist()\n# category_feature_name_2 = X.columns[X.dtypes == \"uint8\"].tolist()\n# print(len(numerical_feature_name_2), len(category_feature_name_2))\n# # 67 133\n# # \u6b64\u65f6\u7c7b\u522b\u578b\u7279\u5f81\u5df2\u7ecf\u5168\u90e8\u53d8\u4e3aone_hot\u7f16\u7801(k-1\u5217),\u6570\u503c\u578b\u7279\u5f81\u8fd8\u9700\u8981\u5f52\u4e00\u5316,\u7531\u4e8e\u7279\u5f81\u503c\u4e2d\u6709\u5f02\u5e38\u503c,\u6211\u4eec\u4f7f\u7528RobustScaler\u65b9\u6cd5\u5f52\u4e00\u5316\n# x_train_num, x_train_cat = x_train[numerical_feature_name_2], x_train[category_feature_name_2]\n# x_test_num, x_test_cat = x_test[numerical_feature_name_2], x_test[category_feature_name_2]\n# # get feature names\n# feature_names = list(x_train_num.columns)\n# feature_names.extend(list(x_train_cat.columns))\n# feature_names = np.array(feature_names)\n# print(feature_names.shape)\n# # 200\n# # robust scalar,\u9ed8\u8ba4\u4e3a\u7b2c\u4e00\u5206\u4f4d\u6570\u5230\u7b2c\u56db\u5206\u4f4d\u6570\u4e4b\u95f4\u7684\u8303\u56f4\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee,\u5f52\u4e00\u5316\u8fd8\u662fz_score\u6807\u51c6\u5316\n# rob_scaler = RobustScaler()\n# x_train_num_rob = rob_scaler.fit_transform(x_train_num)\n# x_test_num_rob = rob_scaler.transform(x_test_num)\n# x_train_nom_pd = pd.DataFrame(np.hstack((x_train_num_rob, x_train_cat)))\n# x_test_nom_pd = pd.DataFrame(np.hstack((x_test_num_rob, x_test_cat)))\n# y_test_pd = pd.DataFrame(y_test)\n# x_train_sm_np, y_train_sm_np = x_train_nom_pd, y_train\n# print(x_train_sm_np.shape, y_train_sm_np.shape, x_test_nom_pd.shape, y_test.shape)\n# # (1709658, 200) (1709658,) (427415, 200) (427415,)","17295f52":"# if use rf or xgb model to train and test,please run this cell\n# divide training sets and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n# (1709658, 200) (427415, 200) (1709658,) (427415,)\nx_train_sm_np, x_test_nom_pd, y_train_sm_np, y_test = x_train, x_test, y_train, y_test\nfeature_names = X.columns.tolist()\nfeature_names = np.array(feature_names)\nprint(feature_names.shape)","3d4e62da":"# # we can choose class_weight=\"balanced\" to deal with sample imbalance problem when we use logistic model\n# # if we use random foreast model or xgboost model,we don\u2019t need to deal with sample imbalance problem\n\n# # besides,we can also use SMOTE to generate some sample of the category with low number of samples,but it needs over 16GB memory\n# # so,if you want to use SMOTE,you can run all these code on a local computer with at least 32GB memory\n# # SMOTE\u7b97\u6cd5\u5373\u5bf9\u4e8e\u5c11\u6570\u7c7b\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6837\u672ca,\u6267\u884cN\u6b21\u4e0b\u5217\u64cd\u4f5c:\n# # \u4ecek\u4e2a\u6700\u8fd1\u90bb\u6837\u672c\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u6837\u672cb, \u7136\u540e\u4ecea\u4e0eb\u7684\u8fde\u7ebf\u4e0a\u968f\u673a\u9009\u53d6\u4e00\u4e2a\u70b9c\u4f5c\u4e3a\u65b0\u7684\u5c11\u6570\u7c7b\u6837\u672c\n# # n_jobs=-1\u8868\u793a\u4f7f\u7528\u6240\u6709CPU\n# sm = SMOTE(k_neighbors=10, random_state=0, n_jobs=-1)\n# x_train_sm_np, y_train_sm_np = sm.fit_sample(x_train_nom_pd, y_train)\n# print(x_train_sm_np.shape, y_train_sm_np.shape)","d9b2363b":"# # use logistic regression model to train and predict\n# # jobs=-1\u4f7f\u7528\u6240\u6709CPU\u8fdb\u884c\u8fd0\u7b97\n# # sag\u5373\u968f\u673a\u5e73\u5747\u68af\u5ea6\u4e0b\u964d\uff0c\u548c\u666e\u901a\u68af\u5ea6\u4e0b\u964d\u6cd5\u7684\u533a\u522b\u662f\u6bcf\u6b21\u8fed\u4ee3\u4ec5\u4ec5\u7528\u4e00\u90e8\u5206\u7684\u6837\u672c\u6765\u8ba1\u7b97\u68af\u5ea6\uff0c\u9002\u5408\u4e8e\u6837\u672c\u6570\u636e\u591a\u7684\u65f6\u5019\n# # class_weight=\"balanced\"\u6839\u636e\u7528\u6765\u8bad\u7ec3\u7684\u6837\u672c\u7684\u5404\u4e2a\u7c7b\u522b\u7684\u6bd4\u4f8b\u786e\u5b9a\u6743\u91cd\n# print(\"use logistic model to train and predict\")\n# lr = LogisticRegression(solver=\"sag\", class_weight=\"balanced\", n_jobs=-1)\n# lr.fit(x_train_sm_np, y_train_sm_np)\n# lr_y_pred = lr.predict(x_test_nom_pd)\n# lr_test_acc = accuracy_score(y_test, lr_y_pred)\n# lr_classification_score = classification_report(y_test, lr_y_pred)\n# print(\"Lr model test accuracy:{:.2f}\".format(lr_test_acc))\n# print(\"Lr model classification_score:\\n\", lr_classification_score)\n# lr_confusion_score = confusion_matrix(y_test, lr_y_pred)\n# f_lr, ax_lr = plt.subplots(1, 3, figsize=(15, 10))\n# # \u6df7\u6dc6\u77e9\u9635\u7684y\u8f74\u4e3atrue label,x\u8f74\u4e3apred label\n# # \u7cbe\u786e\u7387,\u5982\u5bf9\u6b63\u7c7b ,\u6240\u6709\u9884\u6d4b\u4e3a\u6b63\u7c7b\u6837\u672c\u4e2d\u4e2d\u771f\u5b9e\u7684\u6b63\u7c7b\u5360\u6240\u6709\u9884\u6d4b\u4e3a\u6b63\u7c7b\u7684\u6bd4\u4f8b\n# # \u53ec\u56de\u7387,\u5982\u5bf9\u6b63\u7c7b,\u6240\u6709\u771f\u5b9e\u7684\u6b63\u7c7b\u6837\u672c\u4e2d\u6709\u591a\u5c11\u88ab\u9884\u6d4b\u4e3a\u6b63\u7c7b\u7684\u6bd4\u4f8b\n# # \u5206\u522b\u8ba1\u7b97\u9884\u6d4b\u9884\u6d4b\u7684\u6b63\u6837\u672c\u6570\u548c\u8d1f\u6837\u672c\u6570\u4ee5\u53ca\u771f\u5b9e\u7684\u6b63\u6837\u672c\u6570\u548c\u8d1f\u6837\u672c\u6570\n# lr_cm_pred_label_sum = lr_confusion_score.sum(axis=0)\n# lr_cm_true_label_sum = lr_confusion_score.sum(axis=1)\n# # \u8ba1\u7b97\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\n# lr_model_precision, lr_model_recall = np.empty([2, 2], dtype=float), np.empty([2, 2], dtype=float)\n# lr_model_precision[0][0], lr_model_precision[1][0] = lr_confusion_score[0][0] \/ lr_cm_pred_label_sum[0], \\\n#                                                      lr_confusion_score[1][0] \/ lr_cm_pred_label_sum[0]\n# lr_model_precision[0][1], lr_model_precision[1][1] = lr_confusion_score[0][1] \/ lr_cm_pred_label_sum[1], \\\n#                                                      lr_confusion_score[1][1] \/ lr_cm_pred_label_sum[1]\n# lr_model_recall[0][0], lr_model_recall[0][1] = lr_confusion_score[0][0] \/ lr_cm_true_label_sum[0], \\\n#                                                lr_confusion_score[0][1] \/ lr_cm_true_label_sum[0]\n# lr_model_recall[1][0], lr_model_recall[1][1] = lr_confusion_score[1][0] \/ lr_cm_true_label_sum[1], \\\n#                                                lr_confusion_score[1][1] \/ lr_cm_true_label_sum[1]\n# sns.heatmap(lr_confusion_score, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax_lr[0], square=True, linewidths=0.5)\n# sns.heatmap(lr_model_precision, annot=True, fmt=\".5f\", cmap=\"Blues\", ax=ax_lr[1], square=True, linewidths=0.5)\n# sns.heatmap(lr_model_recall, annot=True, fmt=\".5f\", cmap=\"Blues\", ax=ax_lr[2], square=True, linewidths=0.5)\n# ax_lr[0].set_title(\"lr confusion matrix\", fontsize=16)\n# ax_lr[1].set_title(\"lr model precision\", fontsize=16)\n# ax_lr[2].set_title(\"lr model recall\", fontsize=16)\n# ax_lr[0].set_xlabel(\"Predicted label\", fontsize=16)\n# ax_lr[0].set_ylabel(\"True label\", fontsize=16)\n# ax_lr[1].set_xlabel(\"Predicted label\", fontsize=16)\n# ax_lr[1].set_ylabel(\"True label\", fontsize=16)\n# ax_lr[2].set_xlabel(\"Predicted label\", fontsize=16)\n# ax_lr[2].set_ylabel(\"True label\", fontsize=16)\n# plt.show()\n# plt.close()","288dca1a":"# use randomforest model to train and predict\nprint(\"use randomforest model to train and predict\")\nrf = RandomForestClassifier(n_estimators=200, n_jobs=-1)\nrf.fit(x_train_sm_np, y_train_sm_np)\nrf_y_pred = rf.predict(x_test_nom_pd)\nrf_test_acc = accuracy_score(y_test, rf_y_pred)\nrf_classification_score = classification_report(y_test, rf_y_pred)\nprint(\"Rf model test accuracy:{:.4f}\".format(rf_test_acc))\nprint(\"rf model classification_score:\\n\", rf_classification_score)\nrf_confusion_score = confusion_matrix(y_test, rf_y_pred)\n# print(rf_confusion_score)\nf_rf, ax_rf = plt.subplots(1, 3, figsize=(15, 10))\n# \u6df7\u6dc6\u77e9\u9635\u7684y\u8f74\u4e3atrue label,x\u8f74\u4e3apred label\n# \u7cbe\u786e\u7387,\u5982\u5bf9\u6b63\u7c7b ,\u6240\u6709\u9884\u6d4b\u4e3a\u6b63\u7c7b\u6837\u672c\u4e2d\u4e2d\u771f\u5b9e\u7684\u6b63\u7c7b\u5360\u6240\u6709\u9884\u6d4b\u4e3a\u6b63\u7c7b\u7684\u6bd4\u4f8b\n# \u53ec\u56de\u7387,\u5982\u5bf9\u6b63\u7c7b,\u6240\u6709\u771f\u5b9e\u7684\u6b63\u7c7b\u6837\u672c\u4e2d\u6709\u591a\u5c11\u88ab\u9884\u6d4b\u4e3a\u6b63\u7c7b\u7684\u6bd4\u4f8b\n# \u5206\u522b\u8ba1\u7b97\u9884\u6d4b\u9884\u6d4b\u7684\u6b63\u6837\u672c\u6570\u548c\u8d1f\u6837\u672c\u6570\u4ee5\u53ca\u771f\u5b9e\u7684\u6b63\u6837\u672c\u6570\u548c\u8d1f\u6837\u672c\u6570\nrf_cm_pred_label_sum = rf_confusion_score.sum(axis=0)\nrf_cm_true_label_sum = rf_confusion_score.sum(axis=1)\n# \u8ba1\u7b97\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\nrf_model_precision, rf_model_recall = np.empty([2, 2], dtype=float), np.empty([2, 2], dtype=float)\nrf_model_precision[0][0], rf_model_precision[1][0] = rf_confusion_score[0][0] \/ rf_cm_pred_label_sum[0], \\\n                                                     rf_confusion_score[1][0] \/ rf_cm_pred_label_sum[0] \nrf_model_precision[0][1], rf_model_precision[1][1] = rf_confusion_score[0][1] \/ rf_cm_pred_label_sum[1], \\\n                                                     rf_confusion_score[1][1] \/ rf_cm_pred_label_sum[1]\nrf_model_recall[0][0], rf_model_recall[0][1] = rf_confusion_score[0][0] \/ rf_cm_true_label_sum[0], \\\n                                               rf_confusion_score[0][1] \/ rf_cm_true_label_sum[0]\nrf_model_recall[1][0], rf_model_recall[1][1] = rf_confusion_score[1][0] \/ rf_cm_true_label_sum[1], \\\n                                               rf_confusion_score[1][1] \/ rf_cm_true_label_sum[1]\nsns.heatmap(rf_confusion_score, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax_rf[0], square=True, linewidths=0.5)\nsns.heatmap(rf_model_precision, annot=True, fmt=\".5f\", cmap=\"Blues\", ax=ax_rf[1], square=True, linewidths=0.5)\nsns.heatmap(rf_model_recall, annot=True, fmt=\".5f\", cmap=\"Blues\", ax=ax_rf[2], square=True, linewidths=0.5)\nax_rf[0].set_title(\"rf confusion matrix\", fontsize=16)\nax_rf[1].set_title(\"rf model precision\", fontsize=16)\nax_rf[2].set_title(\"rf model recall\", fontsize=16)\nax_rf[0].set_xlabel(\"Predicted label\", fontsize=16)\nax_rf[0].set_ylabel(\"True label\", fontsize=16)\nax_rf[1].set_xlabel(\"Predicted label\", fontsize=16)\nax_rf[1].set_ylabel(\"True label\", fontsize=16)\nax_rf[2].set_xlabel(\"Predicted label\", fontsize=16)\nax_rf[2].set_ylabel(\"True label\", fontsize=16)\nplt.show()\nplt.close()","9fdc6731":"# random forest model feature contribution visualization\nfeature_importances = rf.feature_importances_\n# print(feature_importances)\n# y=x.argsort()\u51fd\u6570\u662f\u5c06x\u4e2d\u7684\u5143\u7d20\u4ece\u5c0f\u5230\u5927\u6392\u5217\uff0c\u63d0\u53d6\u5176\u5bf9\u5e94\u7684index(\u7d22\u5f15)\nindices = np.argsort(feature_importances)[::-1]\n# \u53ea\u53d6\u8d21\u732e\u5ea6\u6700\u9ad8\u768430\u4e2a\u7279\u5f81\u6765\u4f5c\u56fe\nshow_indices = indices[0:30]\nsns.set_style(\"whitegrid\")\nf, ax = plt.subplots(figsize=(15, 10))\nsns.barplot(x=feature_importances[show_indices], y=feature_names[show_indices], ax=ax)\nax.set_title(\"rf model feature importance top30\", fontsize=16)\nax.set_xlabel(\"feature importance score\", fontsize=16)\nax.set_ylabel(\"feature name\", fontsize=16)\nplt.show()\nplt.close()","465e795e":"# # use XGBoost model to train and predict\n# print(\"use XGBoost model to train and predict\")\n# xgb = XGBClassifier(n_estimators=200, nthread=-1)\n# xgb.fit(x_train_sm_np, y_train_sm_np)\n# xgb_y_pred = xgb.predict(x_test_nom_pd)\n# xgb_test_acc = accuracy_score(y_test, xgb_y_pred)\n# xgb_classification_score = classification_report(y_test, xgb_y_pred)\n# print(\"Xgb model test accuracy:{:.4f}\".format(xgb_test_acc))\n# print(\"Xgb model classification_score:\\n\", xgb_classification_score)\n# xgb_confusion_score = confusion_matrix(y_test, xgb_y_pred)\n# # print(xgb_confusion_score)\n# f_xgb, ax_xgb = plt.subplots(1, 3, figsize=(15, 10))\n# # \u6df7\u6dc6\u77e9\u9635\u7684y\u8f74\u4e3atrue label,x\u8f74\u4e3apred label\n# # \u7cbe\u786e\u7387,\u5982\u5bf9\u6b63\u7c7b ,\u6240\u6709\u9884\u6d4b\u4e3a\u6b63\u7c7b\u6837\u672c\u4e2d\u4e2d\u771f\u5b9e\u7684\u6b63\u7c7b\u5360\u6240\u6709\u9884\u6d4b\u4e3a\u6b63\u7c7b\u7684\u6bd4\u4f8b\n# # \u53ec\u56de\u7387,\u5982\u5bf9\u6b63\u7c7b,\u6240\u6709\u771f\u5b9e\u7684\u6b63\u7c7b\u6837\u672c\u4e2d\u6709\u591a\u5c11\u88ab\u9884\u6d4b\u4e3a\u6b63\u7c7b\u7684\u6bd4\u4f8b\n# # \u5206\u522b\u8ba1\u7b97\u9884\u6d4b\u9884\u6d4b\u7684\u6b63\u6837\u672c\u6570\u548c\u8d1f\u6837\u672c\u6570\u4ee5\u53ca\u771f\u5b9e\u7684\u6b63\u6837\u672c\u6570\u548c\u8d1f\u6837\u672c\u6570\n# xgb_cm_pred_label_sum = xgb_confusion_score.sum(axis=0)\n# xgb_cm_true_label_sum = xgb_confusion_score.sum(axis=1)\n# # print(xgb_cm_pred_label_sum,xgb_cm_true_label_sum)\n# # \u8ba1\u7b97\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\n# xgb_model_precision, xgb_model_recall = np.empty([2, 2], dtype=float), np.empty([2, 2], dtype=float)\n# xgb_model_precision[0][0], xgb_model_precision[1][0] = xgb_confusion_score[0][0] \/ xgb_cm_pred_label_sum[0], \\\n#                                                        xgb_confusion_score[1][0] \/ xgb_cm_pred_label_sum[0]\n# xgb_model_precision[0][1], xgb_model_precision[1][1] = xgb_confusion_score[0][1] \/ xgb_cm_pred_label_sum[1], \\\n#                                                        xgb_confusion_score[1][1] \/ xgb_cm_pred_label_sum[1]\n# xgb_model_recall[0][0], xgb_model_recall[0][1] = xgb_confusion_score[0][0] \/ xgb_cm_true_label_sum[0], \\\n#                                                  xgb_confusion_score[0][1] \/ xgb_cm_true_label_sum[0]\n# xgb_model_recall[1][0], xgb_model_recall[1][1] = xgb_confusion_score[1][0] \/ xgb_cm_true_label_sum[1], \\\n#                                                  xgb_confusion_score[1][1] \/ xgb_cm_true_label_sum[1]\n# sns.heatmap(xgb_confusion_score, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax_xgb[0], square=True, linewidths=0.5)\n# sns.heatmap(xgb_model_precision, annot=True, fmt=\".5f\", cmap=\"Blues\", ax=ax_xgb[1], square=True, linewidths=0.5)\n# sns.heatmap(xgb_model_recall, annot=True, fmt=\".5f\", cmap=\"Blues\", ax=ax_xgb[2], square=True, linewidths=0.5)\n# ax_xgb[0].set_title(\"xgb confusion matrix\", fontsize=16)\n# ax_xgb[1].set_title(\"xgb model precision\", fontsize=16)\n# ax_xgb[2].set_title(\"xgb model recall\", fontsize=16)\n# ax_xgb[0].set_xlabel(\"Predicted label\", fontsize=16)\n# ax_xgb[0].set_ylabel(\"True label\", fontsize=16)\n# ax_xgb[1].set_xlabel(\"Predicted label\", fontsize=16)\n# ax_xgb[1].set_ylabel(\"True label\", fontsize=16)\n# ax_xgb[2].set_xlabel(\"Predicted label\", fontsize=16)\n# ax_xgb[2].set_ylabel(\"True label\", fontsize=16)\n# plt.show()\n# plt.close()","86dba847":"# # xgboost model feature contribution visualization\n# feature_importances = xgb.feature_importances_\n# # print(feature_importances)\n# # y=x.argsort()\u51fd\u6570\u662f\u5c06x\u4e2d\u7684\u5143\u7d20\u4ece\u5c0f\u5230\u5927\u6392\u5217\uff0c\u63d0\u53d6\u5176\u5bf9\u5e94\u7684index(\u7d22\u5f15)\n# indices = np.argsort(feature_importances)[::-1]\n# # \u53ea\u53d6\u8d21\u732e\u5ea6\u6700\u9ad8\u768430\u4e2a\u7279\u5f81\u6765\u4f5c\u56fe\n# show_indices = indices[0:30]\n# sns.set_style(\"whitegrid\")\n# f, ax = plt.subplots(figsize=(15, 10))\n# sns.barplot(x=feature_importances[show_indices], y=feature_names[show_indices], ax=ax)\n# ax.set_title(\"xgb model feature importance top30\", fontsize=16)\n# ax.set_xlabel(\"feature importance score\", fontsize=16)\n# ax.set_ylabel(\"feature name\", fontsize=16)\n# plt.show()\n# plt.close()","4075e8fd":"**random forest model result:**\n```\n# Rf model test accuracy:0.9830\n# rf model classification_score:\n#                precision    recall  f1-score   support\n#\n#          0.0       1.00      0.87      0.93     55318\n#          1.0       0.98      1.00      0.99    372097\n#\n#    micro avg       0.98      0.98      0.98    427415\n#    macro avg       0.99      0.93      0.96    427415\n# weighted avg       0.98      0.98      0.98    427415\n```","14fb4be6":"# Deal with sample imbalance problem","8b1fba82":"**xgboost model result on my local computer:**\n```\n# Xgb model test accuracy:0.9822\n# Xgb model classification_score:\n#                precision    recall  f1-score   support\n#\n#          0.0       1.00      0.86      0.93     55318\n#          1.0       0.98      1.00      0.99    372097\n#\n#    micro avg       0.98      0.98      0.98    427415\n#    macro avg       0.99      0.93      0.96    427415\n# weighted avg       0.98      0.98      0.98    427415\n```","5aaf7323":"# Data analysis visualization","34a6879c":"**logistic regression result:**\n```\nLr model test accuracy:0.88\nLr model classification_score:\n                precision    recall  f1-score   support\n          0.0       0.54      0.73      0.62     55318\n          1.0       0.96      0.91      0.93    372097\n    micro avg       0.88      0.88      0.88    427415\n    macro avg       0.75      0.82      0.78    427415\n weighted avg       0.90      0.88      0.89    427415\n```","f55b8bf9":"# Lr\u3001rf\u3001xgb model training and prediction","b3d35595":"# Data preprocessing before model training and testing","f6c99498":"Kaggle kernel only allocated 16GB memory,it is not enough to run the xgboost model for this dataset.You can use your own computer to run the the xgboost model(the computer need at least 32GB memory).","cc18ff88":"# Data preprocessing before data analysis visualization","7e5237d4":"# Notice\nData analysis visualization and Lr\u3001rf\u3001xgb model training and prediction are two independent parts.The kaggle kernel only have 16GB memory,it is not enough to run the two parts at the same time.So please read the following notices:\n\nIf you only want to get Data analysis visualization result,you just need to run all cells of Data preprocessing before data analysis visualization\u3001all cells of Data analysis visualization.when we draw some maps,we need use plotly,this package need connect to Internet,please put kernel settings:Internet on.\n\nIf you only want to get Lr\u3001rf\u3001xgb model training and prediction result,you just need to run all cells of Data preprocessing before data analysis visualization\u3001all cells of Data preprocessing before model training and testing\u3001all cells of Deal with sample imbalance problem,and then choose one model in lr\u3001rf\u3001xgb model training and prediction to run(kaggle kernel only have 16GB memory,it is not enough to run three models at the same time).I provide each model result at the end of the model's code.","69cc062a":"Kaggle kernel only allocated 16GB memory,it is not enough to run the xgboost model for this dataset,I provide my xgboost model result that run on my local computer.You can use your own computer to run the the xgboost model(the computer need at least 32GB memory)."}}