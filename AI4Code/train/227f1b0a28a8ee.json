{"cell_type":{"9a555692":"code","5cee9fbf":"code","66549464":"code","c3591842":"code","fb9fda8f":"code","24875a98":"code","9a86addd":"code","f3cfcd9d":"code","33c57f1a":"code","7da02c18":"code","24048190":"code","cd9ce346":"code","6faf3d1f":"code","f9a24c59":"code","9d0a0ea3":"code","f49fb375":"code","4888f0d3":"code","7ddd19ee":"code","f7f3808d":"code","2a5b091e":"code","75357fe2":"code","cc2594ee":"code","8307153b":"code","8baad635":"code","e2b5e62f":"code","b3e6da6b":"code","41c27e84":"code","264cdb95":"code","a8ccc9ac":"code","9a2bec35":"code","ee68708a":"code","7f956ad5":"code","423e2f26":"code","59756716":"code","0dc0b42a":"code","66d83820":"code","e40f83df":"code","4e253b32":"code","ddb92be8":"code","5a670059":"code","b574eadd":"code","eb7f46e4":"code","39670dcf":"markdown","874ea5c1":"markdown","ea9afcfb":"markdown","2f210ac3":"markdown","f176ff02":"markdown","808360cf":"markdown","c2f5529c":"markdown","b010a764":"markdown"},"source":{"9a555692":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5cee9fbf":"\n#importing required data manupilation libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","66549464":"df = pd.read_csv(\"\/kaggle\/input\/diamonds\/diamonds.csv\") #loading the diamond data.","c3591842":"df.info() # our data set contains 8 numerical and 3 categorical variables.","fb9fda8f":"df.shape # the data has 53940 rows of observations","24875a98":"df.columns # names of the features","9a86addd":"df.head(10) # first look to the data. unnamed:0 column seems like unnecessary index. We might consider droping it.","f3cfcd9d":"df.describe().T \n# descriptive statistics of numerical features.\n#price feature has huge difference of min and max values. There might be outliers.","33c57f1a":"df.isna().sum() # there is no missing value in the data.","7da02c18":"df.drop('Unnamed: 0',axis = 1, inplace = True)  #  drop unnecessary index.\ncorr=df.corr()\nsns.heatmap(corr,vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True, annot = True\n)\n#Correlation table of numerical features.\n#Strong correlation between carat and price draws attention .\n","24048190":"sns.boxplot(x=\"carat\",data=df,color =\"lime\"); # box plot of carat feature. There are a lot of outliers.","cd9ce346":"sns.distplot(df.carat, hist=True, kde=True,color = \"green\") #distribution plot of carat feature.\n#The distribution is left skewed due to outliers.","6faf3d1f":"sns.boxplot(x=\"price\",data=df,color =\"cyan\") # Price column also has many outliers.","f9a24c59":"sns.distplot(df.price, hist=True, kde=True,color = \"darkblue\") # skewed to left as well.","9d0a0ea3":"df.cut.unique()\n# quality of the cut (Fair, Good, Very Good, Premium, Ideal) ordinal variable.","f49fb375":"df.cut.value_counts() # count values of cut types.","4888f0d3":"sns.factorplot(x='cut', data=df , kind='count',aspect=2.5 ) # bar plot of cut feature count.\n#most of diamonds that in the data is ideal cut.","7ddd19ee":"df.color.unique()\n# there are 7 different color of diamonds in the data.","f7f3808d":"df.color.value_counts()\n# value counts of color of diamonds.","2a5b091e":"sns.factorplot(x='color', data=df , kind='count',aspect=2.5 )\n#barplot of diamond colors. \n#color diamond from  D (best) to J (worst) \n# 'G' color is the most common type.","75357fe2":"df.groupby('color').carat.mean()\n  #grouping diamonds according to their colors then looking their carat means.","cc2594ee":"df.groupby('color').carat.mean() ","8307153b":"df.groupby('color').carat.mean().plot(kind = 'bar', \n                                      color=['black', 'red', 'green', 'blue', 'cyan','purple','yellow'],\n                                      xlabel = 'color',\n                                      ylabel=\"carat\")\n#color diamond from  D (best) to J (worst) \n#bar plot of the carat means with respect to colors.","8baad635":"CutPrice = df.groupby('cut').price.mean()\nCutPrice\n#mean prices of cut types","e2b5e62f":"CutPrice.plot(kind='pie',figsize=(7,7))\n#pie plot of mean prices of cut types\n#Premium is the most high one.","b3e6da6b":"sns.boxplot(x=\"carat\",data=df,color =\"cyan\")\n# it seems like above of 3 value are outliers.","41c27e84":"sns.boxplot(df.price,color =\"purple\")\n#outliers of price column starts from 13000.","264cdb95":"sns.boxplot(x=\"table\",data=df,color =\"red\")\n#outlier observation of table feature","a8ccc9ac":"#appyling outlier elemination by filtering data points\ndf = df[(df[\"price\"]< 14500)]\ndf = df[(df[\"carat\"]< 3)]\ndf = df[(df[\"table\"]< 64)&(df[\"table\"]>50)]","9a2bec35":"sns.boxplot(x=\"table\",data=df,color =\"red\") # box plot of table column after outlier elemination","ee68708a":"df.shape ","7f956ad5":"diamond = df.copy() #copying the dataframe before categorical feature encoding.","423e2f26":"# We will use labelencoding to encode out categorical features.\nfrom sklearn import preprocessing\n\nlabel_cut = preprocessing.LabelEncoder()\nlabel_color = preprocessing.LabelEncoder()\nlabel_clarity = preprocessing.LabelEncoder()\n\n\ndiamond['cut'] = label_cut.fit_transform(diamond['cut'])\ndiamond['color'] = label_color.fit_transform(diamond['color'])\ndiamond['clarity'] = label_clarity.fit_transform(diamond['clarity'])","59756716":"diamond.head(7) #Let's have a look to the data after encoding.","0dc0b42a":"y = diamond['price']  #target variable\n\nfeatures = ['carat', 'cut', 'color', 'clarity',\n            'depth', 'table', 'price']  #predictor variables\n\nX = diamond[features]","66d83820":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.3, random_state=2021)","e40f83df":"#importing standartizer and regression models and their error metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor","4e253b32":"# Building pipelines of different regressors and standartize the training set.\n\npipeLR= Pipeline([(\"std_scalar1\",StandardScaler()),\n                     (\"lr_regressor\",LinearRegression())])\n\npipeDT= Pipeline([(\"std_scalar2\",StandardScaler()),\n                     (\"dt_regressor\",DecisionTreeRegressor())])\n\npipeRF= Pipeline([(\"std_scalar3\",StandardScaler()),\n                     (\"rf_regressor\",RandomForestRegressor())])\n\npipeXGB= Pipeline([(\"std_scalar4\",StandardScaler()),\n                     (\"XGB_regressorr\",XGBRegressor())])","ddb92be8":"# List of all the pipelines\npipelines = [pipeLR, pipeDT, pipeRF, pipeXGB]\n\n\npipe_dict = {0: \"LinearRegression\", 1: \"DecisionTree\", 2: \"RandomForest\", 3: \"XGBRegressor\"}\n\n\n# Fit the pipelines\nfor pipe in pipelines:\n    pipe.fit(train_X, train_y)","5a670059":"cv_results_NMSE = []\nfor i, model in enumerate(pipelines):\n    cv_score = cross_val_score(model,train_X, train_y,scoring=\"neg_mean_squared_error\", cv=10)\n    cv_results_NMSE.append(cv_score)\n    print(\"%s: %f \" % (pipe_dict[i], cv_score.mean()))\n    \n# negative root mean square error of linear regression and decision tree regressors are very low. This points to overfitting. \n# On the other hand XGB Regressor's error value appears to be optimal. Let's choose XGB regressor for our main model.","b574eadd":"# XGB Model predictions on validation data.\npred = pipeXGB.predict(val_X)","eb7f46e4":"# Model Evaluation with different error metrics\nprint(\"R^2:\",metrics.r2_score(val_y, pred))\nprint(\"Adjusted R^2:\",1 - (1-metrics.r2_score(val_y, pred))*(len(val_y)-1)\/(len(val_y)-val_X.shape[1]-1))\nprint(\"MAE:\",metrics.mean_absolute_error(val_y, pred))\nprint(\"MSE:\",metrics.mean_squared_error(val_y, pred))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(val_y, pred)))","39670dcf":"# Model Evaluation","874ea5c1":"# Outlier Elemination\nIn order to determine outliers we use box plots to see from which point outlier observations starts.","ea9afcfb":"# Categorical Feature Encoding","2f210ac3":"# Train - Validation Split","f176ff02":"# Conclusion\n\n**In this kernel, the data is evaluated by means of their features in order to predict the diamond price. Before predicting the price, exploratory data analysis has been made, outliers eleminated, categorical features encoded and numerical features standartized.To predict the price; Linear Regression Model, Decision Tree Regressor, RandomForrest Regressor and XGB Regressor are compared. Amongst them The XGB Regressor has been the most successful one in order to predict diamond price. **\n\n**Ask me anything if you have questions. If you like the notebook, please vote up :=)**","808360cf":"# Diamond Price Prediction\n\n**Diamond is the only gem made of a single element: It is typically about 99.95 percent carbon. The other 0.05 percent can include one or more trace elements.Diamonds were formed billions of years ago and are extremely rare because so few are able to survive the difficult journey from the pits of the earth to reach the earth's surface.**\n\n# Aim of The Kernel\n**Aim of the this kernel is to predict diamond price (target) with respect to predictor varaibles of given diamond data set and making proper exploratory data analysis.** \n\n![](https:\/\/k9f7a9j9.rocketcdn.me\/wp-content\/uploads\/2020\/11\/shutterstock_99204875.jpg)","c2f5529c":"# Building Models","b010a764":"# Exploratory Data Analysis"}}