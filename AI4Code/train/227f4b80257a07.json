{"cell_type":{"902a4e8c":"code","1f45acc4":"code","6a8ad9bf":"code","28ff8146":"code","c546e179":"code","9fe2741f":"code","a1970fa7":"code","9826a096":"code","337ffc0d":"code","d0c32cb2":"code","66fc2e9f":"code","bf1ffbb5":"code","b7f9e88f":"code","0151f680":"code","1b6ee47e":"code","2c7a24a7":"code","e15cf609":"code","a6a44e75":"code","427e2185":"code","b44ed2cb":"code","eb94aca3":"code","5f357998":"code","e78bf186":"code","343d8392":"code","4c6a8b1f":"code","388140c0":"code","d454e28b":"code","26fd3976":"code","7df7a4cc":"code","7ea065b9":"code","7824431f":"code","281b8a9a":"code","96c8865a":"code","ebd07f0d":"code","930db099":"code","d0a9791d":"code","494f68b6":"code","bc4baa92":"code","4f8bab70":"code","f0ef965d":"code","278e8c1f":"code","c62ca99d":"code","e56f6ff4":"code","1958d021":"code","cd22ffab":"code","e5861c82":"code","aa5a9ebe":"code","2261cafd":"code","94603628":"code","d72eadfe":"code","3afaab01":"code","679868a0":"markdown","dda8673b":"markdown","51abbbb4":"markdown","b5409401":"markdown","8c961f0f":"markdown","0857bcac":"markdown","42100edf":"markdown","7f284f72":"markdown","ff0bd297":"markdown","7e83835c":"markdown","a8f99664":"markdown","f85a292a":"markdown","0a275ecb":"markdown","4908b073":"markdown","0d3e6625":"markdown","383534e6":"markdown","ccc45ab5":"markdown","431d31a4":"markdown","5e820202":"markdown","4404f5a5":"markdown","f35e924b":"markdown","16f7f0c1":"markdown","a492541c":"markdown","f48f5016":"markdown","b80e8aec":"markdown","778a937c":"markdown","b89133a7":"markdown","ce524581":"markdown","bdeba212":"markdown","7b592d01":"markdown","e8501c3f":"markdown","72b390f9":"markdown","44c44c1b":"markdown","3874cead":"markdown","573cc2ab":"markdown","80ae0341":"markdown"},"source":{"902a4e8c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns #pip install seaborn\n%matplotlib inline\n\ndata = pd.read_csv(\"..\/input\/inpatient-hospital-charges\/inpatientCharges.csv\")\n\ndata.columns = ['DRG','Provider_Id', 'Provider_Name','Provider_StreetAddress','Provider_City',\n               'Provider_State','Provider_Zipcode','Hospital_referral_region_desp',\n                'Total_Discharges','Average_Covered_Charges','Average_Total_Payments',\n                'Average_Medicare_Payment']\n\ndata.info()","1f45acc4":"# understand the data by looking at the first 5 rows\ndata.head()","6a8ad9bf":"# Distribution plots of average total payments \ndata['Average_Total_Payments'].describe()","28ff8146":"# Change data type and transfer spending amount with dollar sign into numeric\ndata['DRG'] = data['DRG'].astype('category')\ndata['Provider_State'] = data['Provider_State'].astype('category')\ndata['Provider_City'] = data['Provider_City'].astype('category')\ndata['Provider_Id'] = data['Provider_Id'].astype('category')\ndata['Average_Covered_Charges'] = (data['Average_Covered_Charges']\n                              .str.replace(r'[^-+\\d.]', '').astype(float))\ndata['Average_Total_Payments'] = (data['Average_Total_Payments']\n                              .str.replace(r'[^-+\\d.]', '').astype(float))\ndata['Average_Medicare_Payment'] = (data['Average_Medicare_Payment']\n                              .str.replace(r'[^-+\\d.]', '').astype(float))","c546e179":"# Make sure we change our data type successfully\ndata.info()","9fe2741f":"print(data['Average_Total_Payments'].describe())\n\nplt.figure(figsize=(10,5)) # Plot a graph with reasonable size.\nsns.histplot(data['Average_Total_Payments'])\n# plt.plot(data['Average_Total_Payments'])","a1970fa7":"# Distribution plots of average total payments \nprint(data['Average_Medicare_Payment'].describe())\n\n# Average Medicare Payment distribution\nplt.figure(figsize=(10,5)) # Plot a graph with reasonable size.\nsns.histplot(data['Average_Medicare_Payment'])\n# plt.plot(data['Average_Medicare_Payments'])","9826a096":"print(data['Total_Discharges'].describe())\n\n# Total Discharges distribution\nplt.figure(figsize=(10,5)) # Plot a graph with reasonable size.\nsns.histplot(data['Total_Discharges'])\n# plt.plot(data['Total_Discharges'])","337ffc0d":"# data['Provider_State'].value_counts()\nplt.figure(figsize=(15,5))\nsns.countplot(x='Provider_State',data=data,order=data['Provider_State'].value_counts().index)\n#plt.xticks(rotation = 90)\n#ax = plt.gca()\n#for p in ax.patches:\n#    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '%d' % int(p.get_height()), \n#            fontsize=12, color='blue', ha='center', va='bottom')\nplt.rcParams[\"axes.labelsize\"] = 20","d0c32cb2":"print(data['Provider_City'].describe())\n\n# This will look better\nplt.figure(figsize=(15,5))\ntxt = data['Provider_City'].value_counts()[:20].index\n\nchart = sns.countplot(\n    x='Provider_City' ,data=data , order=txt\n)\n\n_ = chart.set_xticklabels(chart.get_xticklabels(), rotation=45)\nplt.rcParams[\"axes.labelsize\"] = 10","66fc2e9f":"benchmark_Overall_Average = data[['Average_Total_Payments','Average_Medicare_Payment','Total_Discharges']].median()\nbenchmark_Overall_Average = pd.DataFrame(benchmark_Overall_Average).T\nbenchmark_Overall_Average","bf1ffbb5":"benchmark_by_State = data.groupby(['Provider_State'])[['Average_Total_Payments','Average_Medicare_Payment','Total_Discharges']].median().reset_index()\nbenchmark_by_State.columns = ['Provider_State','Median_Average_Total_Payments','Median_Medicare_Payment','Median_Total_Discharges']\nbenchmark_by_State.head()","b7f9e88f":"benchmark_by_StateDRG = data.groupby(['Provider_State','DRG'])[['Average_Total_Payments','Average_Medicare_Payment','Total_Discharges']].median().reset_index()\nbenchmark_by_StateDRG.columns = ['Provider_State','DRG','Median_Average_Total_Payments','Median_Medicare_Payment','Median_Total_Discharges']\nbenchmark_by_StateDRG.head()","0151f680":"df = pd.DataFrame()\ndf = df.append([benchmark_Overall_Average]*data.shape[0],ignore_index=True)\n# rename columns\ndf = df.rename(columns={'Average_Total_Payments': 'Median_Avg_Total_Pymts', \n                        'Average_Medicare_Payment': 'Median_Avg_Medicare_Pymt',\n                       'Total_Discharges': 'Median_Total_Discharges'})\ndf","1b6ee47e":"Feature123 = pd.DataFrame()\nFeature123['Avg_Total_Pymts_Ratio'] = data['Average_Total_Payments']\/df['Median_Avg_Total_Pymts']\nFeature123['Avg_Medicare_Pymt_Ratio'] = data['Average_Medicare_Payment']\/df['Median_Avg_Medicare_Pymt']\nFeature123['Avg_Total_Discharges_Ratio'] = data['Total_Discharges']\/df['Median_Total_Discharges']\n\nFeature123.head()","2c7a24a7":"# Create an empty dataframe to collect the features\nFeatures = pd.DataFrame()\nFeatures[['ST','DRG']] = data[['Provider_State','DRG']]\n\n\n# add the new feature average spending to the dataset \nFeatures =  [Features,Feature123]\nFeatures = pd.concat(Features,axis=1)\nFeatures.head()","e15cf609":"# add the new feature average spending to the dataset \nF456 = pd.merge(data, benchmark_by_State, how='left', on='Provider_State')\nF456.head()","a6a44e75":"Average_Total_Payments_byST = F456['Average_Total_Payments']\/F456['Median_Average_Total_Payments']\nMedicare_Payment_byST = F456['Average_Medicare_Payment']\/F456['Median_Medicare_Payment']\nTotal_Discharges_byST = F456['Total_Discharges']\/F456['Median_Total_Discharges']\n\nFeatures['Avg_Total_Pymt_byST'] = Average_Total_Payments_byST\nFeatures['Medicare_Pymt_byST'] = Average_Total_Payments_byST\nFeatures['Total_Discharges_byST'] = Total_Discharges_byST\nFeatures.tail()","427e2185":"# add the new feature average spending to the dataset \nF789 = pd.merge(data, benchmark_by_StateDRG, how='left', on='Provider_State' and 'DRG')\nF789.head()","b44ed2cb":"Average_Total_Payments_bySTDRG = F789['Average_Total_Payments']\/F789['Median_Average_Total_Payments']\nMedicare_Payment_bySTDRG = F789['Average_Medicare_Payment']\/F789['Median_Medicare_Payment']\nTotal_Discharges_bySTDRG = F789['Total_Discharges']\/F789['Median_Total_Discharges']\n\nFeatures['Avg_Total_Pymt_bySTDRG'] = Average_Total_Payments_bySTDRG\nFeatures['Medicare_Pymt_bySTDRG'] = Average_Total_Payments_bySTDRG\nFeatures['Total_Discharges_bySTDRG'] = Total_Discharges_bySTDRG\n\n# For some hospitals, it is possible Total Discharges=0 and cause missing values in the dataset, \nFeatures.iloc[:,8:] = Features.iloc[:,8:].fillna(0)\nFeatures.info()","eb94aca3":"plt.figure(figsize=(15,5)) # Plot a graph with reasonable size.\nx1,x2,y1,y2 = plt.axis()  \nplt.axis((x1,x2,0,5))\nplt.xticks(rotation=-15)\n\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(Features.iloc[:,2::]))\n","5f357998":"plt.figure(figsize=(15,5)) # Plot a graph with reasonable size.\nplt.xticks(rotation=-20)\n\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(Features.iloc[:,2::]))","e78bf186":"# 0.Standardize and Scale the Dataset\nfrom sklearn import preprocessing\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n#0. Standardize and scale the Dataset, the numeric variables\nscaler = StandardScaler()\nscaler.fit(Features.drop(['DRG','ST'], axis=1))\n\nscaled_features = scaler.transform(Features.drop(['DRG','ST'], axis=1))\nscaled_df = pd.DataFrame(scaled_features, columns = Features.drop(['DRG','ST'],axis=1).columns)\n\nFeatures.info()","343d8392":"# seperate the dataset, X is numeric set and y is non-numeric\nX = scaled_df\ny = data[['DRG','Provider_Name','Provider_City','Provider_State']]","4c6a8b1f":"# Choose reasonable number of clustering based on the Elbow graph\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(3, 8):\n    model = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    model.fit(X)\n    wcss.append(model.inertia_)\n    \nplt.plot(range(3, 8), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","388140c0":"#From above, elbow is at 4 and 5, and lets choose 5 here\n\nfrom sklearn.cluster import KMeans\nmodel= KMeans(n_clusters=5, random_state=0)\nmodel.fit(X)\n\nclusters=pd.DataFrame(X)\n\nclusters['label']=model.labels_\n\nlabels = model.labels_\nlabels","d454e28b":"# Count how many points in each cluster\npd.DataFrame(labels).value_counts(sort=False)","26fd3976":"from plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)","7df7a4cc":"# 1. Graph the relationship between clusters\n# We will only examine data the cluster label 1, for anomaly detection purpose\nimport plotly\nimport plotly.express as px\n\npie=clusters.groupby('label').size().reset_index()\npie.columns=['label','value']\npx.pie(pie,values='value',names='label')","7ea065b9":"# 2.Visualizing the relationship between the 9 features\n# are highly correaltion\n\nplt.figure(figsize=(10,5)) # Plot a graph with reasonable size.\ncorrMatrix = X.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","7824431f":"# Lets put the dataset numeric, and non-numeric back in 1 set\nyy = y.reset_index()\nclusterss = clusters.reset_index()\n\ndf_train = pd.merge(yy,clusterss, how = 'left', on ='index')\ndf_train = df_train.iloc[:,1:]\ndf_train.info()","281b8a9a":"# Some stat facts\ndf_train.groupby('label').agg(['count','mean'])\nX_stat = df_train.groupby('label').agg('describe')","96c8865a":"X_stat = df_train.groupby('label').agg([np.mean, np.median, np.min, np.max])\n                    #percentile(50), percentile(95)])\nX_stat    ","ebd07f0d":"from sklearn.decomposition import PCA\npca = PCA(5)\nx_pca = pca.fit_transform(X) \nx_pca = pd.DataFrame(x_pca)\nx_pca.columns=['PC1','PC2','PC3','PC4','PC5'] #The scores for PC1, PC2, ..., PCn\nx_pca.head()","930db099":"x_pca.plot.box()","d0a9791d":"# Explained Variance for each Principal Component\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance","494f68b6":"explained_variance = np.insert(explained_variance, 0, 0)\ncumulative_variance = np.cumsum(np.round(explained_variance, decimals=3))\n\npc_df = pd.DataFrame(['','PC1', 'PC2', 'PC3','PC4',\"PC5\"], columns=['PC'])\nexplained_variance_df = pd.DataFrame(explained_variance, columns=['Explained Variance'])\ncumulative_variance_df = pd.DataFrame(cumulative_variance, columns=['Cumulative Variance'])\n\ndf_explained_variance = pd.concat([pc_df, explained_variance_df, cumulative_variance_df], axis=1)\ndf_explained_variance\n","bc4baa92":"import plotly\nimport plotly.express as px\n\nfig = px.bar(df_explained_variance, \n             x='PC', y='Explained Variance',\n             text='Explained Variance',\n             width=800)\n\nfig.update_traces(texttemplate='%{text:.3f}', textposition='outside')\nfig.show()","4f8bab70":"#!pip install pyod\n#!pip install combo","f0ef965d":"# When you do unsupervised learning, it is always a safe step to standardize the predictors\n## Sperating X = the 9 features, and y = other variables can be ignored\n# 0. Standardize Dataset have done in above model\n\n# 1. Split data into X_train and X_test\nfrom pyod.models.knn import KNN\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)","278e8c1f":"print(X_train.shape, X_test.shape)","c62ca99d":"X_train.head()","e56f6ff4":"from pyod.models.combination import aom, moa, average, maximization\nfrom pyod.utils.utility import standardizer\nfrom pyod.utils.data import generate_data\nfrom pyod.utils.data import evaluate_print\n\n#1. Test a range of K-neighbors, from 2 to 70. There will be 15 KNN models\nn_clf = 15 \nk_list = [2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55,60,65,70]","1958d021":"train_scores = np.zeros([X_train.shape[0], n_clf])\ntest_scores = np.zeros([X_test.shape[0], n_clf])\ntrain_scores = pd.DataFrame(train_scores)\ntest_scores = pd.DataFrame(test_scores)","cd22ffab":"train_scores.shape","e5861c82":"for i in range(n_clf):\n    k = k_list[i]\n\n    clf = KNN(n_neighbors=k, method='largest')\n    clf.fit(X_train)\n\n    train_scores.loc[:, i] = clf.decision_scores_\n    test_scores.loc[:, i] = clf.decision_function(X_test) # Predict raw anomaly score of X using the fitted detector.","aa5a9ebe":"train_scores.head()","2261cafd":"# Combination by average, to avoid overfitting problem\ny_by_average = average(test_scores)\n             \nimport matplotlib.pyplot as plt\nplt.hist(y_by_average)  # arguments are passed to np.histogram\nplt.title(\"Combination by average\")\nplt.show()","94603628":"# We don't want to overwrite X_test, so we use .copy() to make a new copy of X_test. \ndf_test = X_test.copy()\ndf_test['y_by_average_score'] = y_by_average\ndf_test['y_by_average_cluster'] = np.where(df_test['y_by_average_score']<1.5, 0, 1)\ndf_test['y_by_average_cluster'].value_counts()","d72eadfe":"df_test.groupby('y_by_average_cluster').mean()","3afaab01":"893\/(52919+893)*100","679868a0":"### Feature Visualizations & Explainations <a class=\"anchor\" id=\"Section_33\"> <\/a>","dda8673b":"### Section 3 Summary:\n","51abbbb4":"#### 2. The Average aggregating Methods\nThe \"average\" function in PyOD averages the outlier scores from multiple estimators","b5409401":"## Section 1: Data preparation <a class=\"anchor\" id=\"Section_1\"><\/a>\n### Section 1.1: Load Data <a class=\"anchor\" id=\"Section_11\"><\/a>\n* Import dataset and packages\n* Rename columns","8c961f0f":"# Objectives\n- Detect any hospital that may abuse the resources.\n - Detect any hospital that may abuse the resources compared to its peers.\n - Detect any hospital that may abuse the resources compared to the average (median etc) of its peers.\n - Detect any hospital that may abuse the resources compared to the average (median etc) of its peers of the same DRG and State.\n\n# Table of Contents\n* [Section 1: Data Preparation](#Section_1)\n    * [Section 1.1: Load data](#Section_11)\n    * [Section 1.2: Data Overview](#Section_12)\n    <br> <\/br> \n* [Section 2: Exploratory Data Analysis](#Section_2)\n    * [Section 2.1: Distribution plots](#Section_21)\n    * [Section 2.2: Number of Hospitals by state](#Section_22)    \n    * [Section 2.3: Number of Hospitals by city](#Section_23) \n    <br> <\/br> \n* [Section 3: Feature engineering](#Section_3)\n    * [Section 3.1: Create the benchmark table - Overall & State & DRG average](#Section_31)\n    * [Section 3.2: Create features](#Section_32)  \n    \n        * [Feature 1: Ratio of \"Average_Total_Payments\" to Overall_Average_Total_Payments](#Section_321)\n        * [Feature 2: Ratio of \"Average_Medicare_Payment\" to Overall_Average_Medicare_Payment](#Section_321)\n        * [Feature 3: Ratio of \"Total_Discharges\" to Overall_Average_Total_Discharges ](#Section_321)\n\n        * [Feature 4: Ratio of \"Average_Total_Payment\" to State Median Average_Total_Payments](#Section_324)\n        * [Feature 5: Ratio of \"Average_Medicare_Payment\" to State Median Average_Medicare_Payment ](#Section_324)\n        * [Feature 6: Ratio of \"Total_Discharges\" to State Median Average_Total_Discharges](#Section_324)\n    \n        * [Feature 7: Ratio of \"Average_Total_Payment\" to State Median Average_Total_Payments By DRG](#Section_327)\n        * [Feature 8: Ratio of \"Average_Medicare_Payment\" to State Median Average_Medicare_Payment By DRG](#Section_327)\n        * [Feature 9: Ratio of \"Total_Discharges\" to State Median Total_Discharges By DRG](#Section_327) \n    * [Section 3.3: Feature Visualizations & Explainations](#Section_33) \n    <br> <\/br>     \n* [Section 4: Unsupervised Machine Learning Models](#Section_4)\n    * [Model1: K-Means](#Section_41)\n        * What is K-Means, and Why K-Means?\n        * Analysis Steps\n    * [Model2: PCA](#Section_42)\n        * What is PCA, and Why PCA?\n        * Analysis Steps\n    * [Model3: KNN](#Section_43)\n        * What is KNN, and Why KNN?\n        * Analysis Steps\n        <br> <\/br>\n* [Section 5: Summary and Recommendation](#Section_5)","0857bcac":"* Similarly, the distribution of Total Discharges is skewed to the right. \n* 75% of the charges are under 49 times, but the highest number could be 3383 times, which can be the potential anomaly records we are looking for.\n-----------------------------------------------------------------","42100edf":"### Section 3.2: Create features <a class=\"anchor\" id=\"Section_32\"><\/a>","7f284f72":"### Section 2.3: Number of hospitals, Count of top 20 cities <a class=\"anchor\" id=\"Section_23\"><\/a>\n* There are 1977 cities on record, and the top 3 cities with greatest number are Chicago, Baltimore and Houston","ff0bd297":"### Section 5: Summary and Recommendation <a class = \"anchor\" id=\"Section_5\"> <\/a>","7e83835c":"### Model 3: KNN <a class=\"anchor\" id=\"Section_43\"> <\/a>  \n##### What is KNN, and why KNN?\n- KNN computes the distance of a point to its nearest data point.\n- KNN does not have any learning involved, i.e., there are no parameters we can tune to make the performance better.\n- An outlier is a point that is distant from neighboring points, a higher score means abnormal\n\n##### Analysis Steps:\n- **1.** Select n neighbors or clusters and assign anomaly scores\n- **2.** Try aggregating models (Average, MOM) to achieve model stability\n- **3.** Define anomalies","a8f99664":"### Section 2.2: Number of hospitals, Count by State <a class=\"anchor\" id=\"Section_22\"><\/a>\n* The top 3 States with the greatest number of records are California, Texas and Florida","f85a292a":"#### 3. Explain the Visuals\n- 3.9% of the data points are clustered in cluster label 3, while 60.4% of the data points are in cluster label 1.\n- The correlation chart shows that Features \"Average_Total_Pyment_Ratio\", \"Average_Total_Pyment_byST\", \"Medicare_Pymt_Ratio\", and \"Medicare_Pymt_byST\", are highly, very much correlated to each other. ","0a275ecb":"* The distribution of Average Total Payment is skewed to the right. \n* 75% of the charges are under 10,000 dollar, but the highest charge amounts to 156,158 which can be the potential anomaly records we are looking for.\n-----------------------------------------------------------------","4908b073":"- The 1st boxplot: I limited y-axis to 5. As we can the 9 features we created are mostly right skewed, and most of the data points are settled in the range of 0 to 2, \n- The 2nd boxplot: Overviewed the distribution of each feature. As showned in the plot, there are many outliers in the feature we created, especially for the Average Total Discharges Ratio, the abonormal ratio could reach more than 120 times. \n- Therefore, we can conclude there are many anomalies in the dataset. As an analyst, we need to understand what happened behind the number, and why it happened. \n<br> <\/br>\n- In the following section, we will apply several Unsupervised Machine Learning methods to figure out how much anomalies are in the dataset.","0d3e6625":"It appearted when the average_y is greater than 1.5, could be defined as anomalies","383534e6":"In section 1, I imported and overviewed the shape and column names of the dataset, then I transformed data types into correct formats. \n\nAfter transforming the data types, In section 2 I performed exploratory data analysis. To overview the distribution of outliers, I plotted the variable distributions. For example, the distribution of dollar amount spending and total discharges are skewed to the right, which indicated potential anomalies. There are other interesting findings too: Even though the number of hospital divisions in State Illinois and State Maryland are not ranking top in the State group bys, their hospital divisions rank the most when looking at the city group bys. Meaning some states only have a few hospitals in certain area.\n\nNow we have some understanding of the dataset and numbers. Its time to create metrics to measure the outliers in section 3: feature engineering part. In the notebook, we created 9 features in ratios to detect anomalies. Since outliers and skewed data have a smaller effect on the median, we use the median as the benchmark to create the ratio comparison. According to the distributions of the 9 features, we can double confirmed the dataset has anomalies, and I need to understand what happened behind the number, and why it happened.\n\nIn section 4, I applied several Unsupervised ML models, such as K-means, KNN and PCA to detect anomalies. Using these 3 different models, we finally conclude that there are at least 1.66% of the dataset are outliers, and at most 3.9% of the data points are anomalies. \n\nIn conclusion, this notebook is the first step of my analysis and my majority focus here are EDA process and understanding the dataset. the Unsupervised models enable us to approximate the range of the anomalies in the dataset. I will continue working on this notebook to list which hospitals may abuse the resources based on the features we created. ","ccc45ab5":"### Feature 4, Feature 5, Features 6 Below:<a class=\"anchor\" id=\"Section_324\"> <\/a>\n- Feature 4: Ratio of \"Average_Total_Payment\" to State_Average_Total_Payment \n- Feature 5: Ratio of \"Average_Medicare_Payment\" to State_Average_Medicare_Payment \n- Feature 6: Ratio of \"Total_Discharges\" to State_Average_Total_Discharges \n <br>\n \n- Purpose: to compare An individual hospital's Average Payments to the **benchmark** of State Median Average Payment \n- According to the ratio, we are able to recognize how much higher and lower the spending amount of the individual hospital to the State Median Average payment\n","431d31a4":"### Model 2: PCA <a class=\"anchor\" id=\"Section_42\"> <\/a>  \n##### What is PCA, and why PCA?\n-  PCA is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss.\n\n##### Analysis Steps:\n- **0.** Standardize and scale the dataset (process is done in Model1)\n- **1.** Split data into X_train and X_test \n- **2.** Plot in a 2-D space to visualize it\n- **3.** Scree plot each Principle Component \n- **4.** Defined the anomaly one","5e820202":"### Feature 7, Feature 8, Feature 9 Below:<a class=\"anchor\" id=\"Section_327\"> <\/a>\n- Feature 7: Ratio of \"Average_Total_Payment\" to State MedianAverage_Total_Payment By DRG\n- Feature 8: Ratio of \"Average_Medicare_Payment\" to State MedianAverage_Medicare_Payment By DRG\n- Feature 9: Ratio of \"Total_Discharges\" to State Median Total_Discharges By DRG\n   <br> \n   \n- Purpose: to compare An individual hospital's Average Payments to the **benchmark** of State Median Average Payment by DRG\n- According to the ratio, we are able to recognize how much higher and lower the spending amount of the individual hospital than the State Median Average payment in each DRG category","4404f5a5":"#### 3. Define anomalies\n- The Average Aggregating Method showed that at least 1.659% of the dataset are outliers\n- The average score of Cluster 1, is very high compared with Cluster 0, so we can define this as anoamly cluster","f35e924b":"#### 2.Plot in a 2-D space just to visualize it for now\n- PCA does not need y variable, or the label since its Unsupervised","16f7f0c1":"#### Plot3: Distribution plots of average total payments ","a492541c":"#### Plot 2: Average_Medicare_Payment ","f48f5016":"**Define Anomalies:**\n- PCA reduce the dimension of large dataset that helps avoid overfitting problem\n- The graph above showed that PC5 explained 3.8% variance for the overall model, so PC5 should be clustered as Anomaly cluster","b80e8aec":"### Feature 1,Feature 2,Feature 3 Below: <a class=\"anchor\" id=\"Section_321\"> <\/a> \n- Feature 1: Ratio of \"Average_Total_Payment\" to Overall_Average_Total_Payment\n- Feature 2: Ratio of \"Average_Medicare_Payment\" to Overall_Average_Medicare_Payment\n- Feature 3: Ratio of \"Total_Discharges\" to Overall_Average_Total_Discharges \n<br> <\/br>\n- Purpose: to compare An individual hospital's Average Payments to the **benchmark** of Overall Average Payment \n- According to the ratio, we are able to recognize how much higher and lower the spending amount of the individual hospital to the Overall Average payment","778a937c":"### Model 1: K-Means <a class=\"anchor\" id=\"Section_41\"> <\/a>  \n##### What is K-Means, and why K-Means?\n-  The K-Means assigns each data point to the nearest seed to form a cluster. This will groups the data points into N clusters. \n-   Each seed will be the centroid of a cluster, and the process will be iterated until the values of the centroids stabilized.\n\n##### Analysis Steps:\n- **0.** Standardize and scale the dataset <br>\n- **1.** Choose reasonable number of clustering <br>\n- **2.** Graph the correlation between features <br>\n- **3.** Explain the visuals <br>\n- **4.** Define anomalies","b89133a7":"## Section 3: Feature Engineering <a class=\"anchor\" id=\"Section_3\"><\/a>\n### Section 3.1: Create the benchmark table - State & DRG average <a class=\"anchor\" id=\"Section_31\"><\/a>\n* We are going to use ratio comparison to detect anomaly, therefore, a benchmark table is created for further anomly detection process\n* Outliers and skewed data have a smaller effect on the median, the median is a better measure of central tendency than the mean.\n* Our benchmark are the following:\n    - The overall hospital average Average_Total_Payments\n    - Average_Total_Payemtns by State\n    - Average_Total_Payemtns by State and DRG\n    - The overall hospital median Average_Total_Payments\n    - The median Average_Total_Payemtns by State\n    - The Average_Total_Payemtns by State and DRG","ce524581":"* Similarly, the distribution of Average Medicare Payment is skewed to the right. \n* 75% of the charges are under 10056.880000 dollar, but the highest charge amounts to 154620.810000 which can be the potential anomaly records we are looking for.\n-----------------------------------------------------------------","bdeba212":"### Section 1 summary:\n* The dataset contained **163065** records and **12** variables\n* In total, the **12** columns\/variables have **0** missing values, so we don't have to impute missing values\n* The dataset variables have several data types: they are **object, float and integer**. \n* Since we have multiple data types, **Data cleaning process** is necessary to prepare further analysis, and we have done it in section 1.2","7b592d01":"#### 3. Scree plot: visualize the PCs","e8501c3f":"## Section 2: EDA <a class=\"anchor\" id=\"Section_2\"><\/a>\n### Section 2.1: Distribution plots <a class=\"anchor\" id=\"Section_21\"><\/a>\nThe metrics we are interested in are the following:\n* Plot1: Average_Total_Payments   \n* Plot2: Average_Medicare_Payment \n* Plot3: Average_Covered_Charges             \n\n**Our fraud detection analysis is based on the dollar amount expenditure, it will be helpful to overview the distribution of outliers visuals below:**","72b390f9":"### Section 4: Unsupervised ML methods<a class=\"anchor\" id=\"Section_4\"> <\/a>  ","44c44c1b":"#### Plot 1: Distribution plots of average total payments ","3874cead":"#### 4. Define anomalies:\n- The summary statistics above showed that the features in cluster label 1 is almost having abnormal high or abnormal low mean\/median\/min\/max compared with the other 4 clusters. So we can define Cluster label 1 to be the anomaly cluster. \n- For future analysis, we should pay attention further and explore why this cluster\u2018s payment amount is more than other clusters.\n\n","573cc2ab":"### Section 1.2: Data Overview <a class=\"anchor\" id=\"Section_12\"><\/a>\n* Change data types for further analysis with the following:\n* Transfer spending amount with dollar sign into numeric\n* Change multiple object data type into category data type","80ae0341":"* Even though the number of hospital divisions in State **Illinois** and State **Maryland** are not ranking top in the State group bys, their hospital divisions rank the most when looking at the city group bys. **Meaning some states only have a few hospitals in certain area.**\n------------------------------"}}