{"cell_type":{"7eb5f451":"code","531d4c4e":"code","fb6cbcac":"code","80d7bdc6":"code","4c579c3e":"code","0ec7ee24":"code","a91934aa":"code","eeef63ee":"code","873d91c4":"code","3e7e46e2":"code","31e216d2":"code","16efcdf2":"code","8410460d":"code","37299b46":"code","5160b94e":"code","dfbc3646":"code","a31d9936":"code","d930a150":"code","a7f916c0":"markdown","9cdeefd5":"markdown","a2b96c7a":"markdown"},"source":{"7eb5f451":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom tqdm import tqdm_notebook\nimport gc","531d4c4e":"train = pd.read_csv('\/kaggle\/input\/widsdatathon2020\/training_v2.csv')\ntest = pd.read_csv('\/kaggle\/input\/widsdatathon2020\/unlabeled.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/widsdatathon2020\/samplesubmission.csv')\nsolution_template = pd.read_csv('\/kaggle\/input\/widsdatathon2020\/solution_template.csv')\ntrain.shape, test.shape","fb6cbcac":"train.sample(5)","80d7bdc6":"test.head(5)","4c579c3e":"def make_submit(y_pred, filename='submission.csv'):\n    solution_template['hospital_death'] = y_pred\n    solution_template.to_csv(f'{filename}', index=False)\n    print('solution file created. Commit notebook and submit file...')\n    solution_template['hospital_death'].hist()\n    ","0ec7ee24":"# LightGBM GBDT with KFold or Stratified KFold\n\ndef kfold_lightgbm(train, test, target_col, params, cols_to_drop=None, cat_features=None, num_folds=5, stratified = False, \n                   debug= False):\n    \n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train.shape, test.shape))\n\n\n    \n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1)\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train.shape[0])\n    sub_preds = np.zeros(test.shape[0])\n    feature_importance_df = pd.DataFrame()\n    if cols_to_drop == None:\n        feats = [f for f in train.columns if f not in [target_col]]\n    else:\n        feats = [f for f in train.columns if f not in cols_to_drop+[target_col]]\n\n    # k-fold\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train[feats], train[target_col])):\n        train_x, train_y = train[feats].iloc[train_idx], train[target_col].iloc[train_idx]\n        valid_x, valid_y = train[feats].iloc[valid_idx], train[target_col].iloc[valid_idx]\n\n        # set data structure\n        lgb_train = lgb.Dataset(train_x,\n                                label=train_y,\n                                categorical_feature=cat_features,\n                                free_raw_data=False)\n        lgb_test = lgb.Dataset(valid_x,\n                               label=valid_y,\n                               categorical_feature=cat_features,\n                               free_raw_data=False)\n\n        # params after optimization\n        reg = lgb.train(\n                        params,\n                        lgb_train,\n                        valid_sets=[lgb_train, lgb_test],\n                        valid_names=['train', 'test'],\n#                         num_boost_round=10000,\n#                         early_stopping_rounds= 200,\n                        verbose_eval=False\n                        )\n\n        roc_auc = []\n        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n        sub_preds += reg.predict(test[feats], num_iteration=reg.best_iteration) \/ folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', \n                                                                           iteration=reg.best_iteration))\n        fold_importance_df[\"fold\"] = n_fold + 1\n        \n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d ROC-AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        roc_auc.append(roc_auc_score(valid_y, oof_preds[valid_idx]))\n        del reg, train_x, train_y, valid_x, valid_y\n        gc.collect()\n        \n    print('Mean ROC-AUC : %.6f' % (np.mean(roc_auc)))\n    return sub_preds","a91934aa":"cat_features = [x for x in train.columns if train[x].dtype == 'object' ]","eeef63ee":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\n# \u0414\u043b\u044f \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432\nfor col in tqdm_notebook(cat_features):\n    train[col] = train[col].astype('str')\n    train[col] = le.fit_transform(train[col])\n    \nfor col in tqdm_notebook(cat_features):\n    test[col] = test[col].astype('str')\n    test[col] = le.fit_transform(test[col])","873d91c4":"params ={\n    'objective': 'binary',\n    'metric': 'roc_auc',\n    'categorical_features': cat_features\n                }\nparams_best = {\n    'bagging_fraction': 0.15760757965010433, \n    'feature_fraction': 0.11161740354830015, \n    'learning_rate': 0.03, \n    'max_depth': 50, \n    'min_child_weight': 0.008857217513412136, \n    'min_data_in_leaf': 20, \n    'n_estimators': 815, \n    'num_leaves': 96, \n    'reg_alpha': 1.5292311993088907, \n    'reg_lambda': 1.903834634991243}","3e7e46e2":"submit_best_params = kfold_lightgbm(train, test.drop('hospital_death', axis=1), cat_features=cat_features, \n                                 target_col='hospital_death', params=params_best)","31e216d2":"make_submit(submit_best_params, 'submission.csv')","16efcdf2":"baseline_submit = kfold_lightgbm(train, test.drop('hospital_death', axis=1), cat_features=cat_features, \n                                 target_col='hospital_death', params=params)","8410460d":"def bayes_auc_lgb(\n    n_estimators,\n    learning_rate,\n    num_leaves, \n    bagging_fraction,\n    feature_fraction,\n    min_child_weight, \n    min_data_in_leaf,\n    max_depth,\n    reg_alpha,\n    reg_lambda):\n    \n    \"\"\"\n    \u0414\u043e \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u043d\u0430\u0434\u043e \u043f\u0435\u0440\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0418\u0421\u0425\u041e\u0414\u041d\u042b\u0419 \u0414\u0410\u0422\u0410\u0424\u0420\u0415\u0419\u041c \u0438 \u041a\u0410\u0422\u0415\u0413\u041e\u0420\u0418\u0410\u041b\u042c\u041d\u042b\u0415 \u041f\u0420\u0418\u0417\u041d\u0410\u041a\u0418\n    \"\"\"\n    \n    # \u041d\u0430 \u0432\u0445\u043e\u0434 LightGBM \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0440\u0442\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c\u0441\u044f \u0432 \u0432\u0438\u0434\u0435 \u0446\u0435\u043b\u044b\u0445 \u0447\u0438\u0441\u0435\u043b. \n    n_estimators = int(n_estimators)\n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n    \n    assert type(n_estimators) == int\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    \n    params = {\n              'n_estimators': n_estimators,\n              'num_leaves': num_leaves, \n              'min_data_in_leaf': min_data_in_leaf,\n              'min_child_weight': min_child_weight,\n              'bagging_fraction' : bagging_fraction,\n              'feature_fraction' : feature_fraction,\n              'learning_rate' : learning_rate,\n              'max_depth': max_depth,\n              'reg_alpha': reg_alpha,\n              'reg_lambda': reg_lambda,\n              'objective': 'binary',\n              'save_binary': True,\n              'seed': 1337,\n              'feature_fraction_seed': 1337,\n              'bagging_seed': 1337,\n              'drop_seed': 1337,\n              'data_random_seed': 1337,\n              'boosting_type': 'gbdt',\n              'verbose': 1,\n              'is_unbalance': False,\n              'boost_from_average': True,\n              'metric':'f1'}\n\n    \n    # \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n    folds = StratifiedKFold(n_splits= 5, shuffle=True, random_state=1)\n\n\n    # \u041c\u0430\u0441\u0441\u0438\u0432\u044b \u0434\u043b\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\n    oof_preds = np.zeros(df.shape[0])\n\n    feats = [f for f in df.columns if f not in ['hospital_death']]\n\n    # k-fold\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[feats], df['hospital_death'])):\n        train_x, train_y = df[feats].iloc[train_idx], df['hospital_death'].iloc[train_idx]\n        valid_x, valid_y = df[feats].iloc[valid_idx], df['hospital_death'].iloc[valid_idx]\n\n        # \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u044b \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n        lgb_train = lgbm.Dataset(train_x,\n                                label=train_y,\n                                categorical_feature=cat_f,\n                                free_raw_data=False)\n        lgb_test = lgbm.Dataset(valid_x,\n                               label=valid_y,\n                               categorical_feature=cat_f,\n                               free_raw_data=False)\n\n        # \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n        clf = lgbm.train(\n                        params,\n                        lgb_train,\n                        valid_sets=[lgb_train, lgb_test],\n                        verbose_eval=False\n                        )\n\n        auc = []\n        oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)\n\n\n\n        auc.append(roc_auc_score(valid_y, oof_preds[valid_idx]))\n\n    return np.mean(auc)","37299b46":"# \u0413\u0440\u0430\u043d\u0438\u0446\u044b \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u043b\u044f \u043f\u043e\u0438\u0441\u043a\u0430   \nbounds_lgb = {\n    'n_estimators': (10, 1000),\n    'num_leaves': (10, 500), \n    'min_data_in_leaf': (20, 200),\n    'bagging_fraction' : (0.1, 0.9),\n    'feature_fraction' : (0.1, 0.9),\n    'learning_rate': (0.01, 0.3),\n    'min_child_weight': (0.00001, 0.01),   \n    'reg_alpha': (1, 2), \n    'reg_lambda': (1, 2),\n    'max_depth':(-1,50),\n}","5160b94e":"def bayes_lgb(score_func, bound_lgb, init_points: int = 10, n_iter:int = 100):\n    \"\"\"\n    \u041f\u043e\u0438\u0441\u043a \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0445 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u043b\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438. \n    :param score_func: \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438. \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e.\n    :param bounds_lgb: \u0433\u0440\u0430\u043d\u0438\u0446\u044b \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u0434\u043b\u044f \u043f\u043e\u0438\u0441\u043a\u043e\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 (\u0441\u043b\u043e\u0432\u0430\u0440\u044c). \u0417\u0430\u0434\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\n    :param n_iter: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\n    :return: \u041e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432 \u0432\u0438\u0434\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f\n    \"\"\"\n    # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u0430\n    lgb_bo = BayesianOptimization(score_func, bounds_lgb, verbose=0)\n    \n    # \u043f\u043e\u0438\u0441\u043a\n    lgb_bo.maximize(init_points=init_points, n_iter=n_iter, xi=0.0, alpha=1e-6)\n    \n    print(\"\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438: \",lgb_bo.max['target'])\n    print(\"\u041e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: \", lgb_bo.max['params'])\n    \n    return lgb_bo.max['params']","dfbc3646":"from bayes_opt import BayesianOptimization","a31d9936":"cat_features.remove('icu_stay_type')","d930a150":"df = train.copy().drop(['encounter_id', 'patient_id', 'hospital_id', 'icu_stay_type', 'icu_id'], axis=1)\ncat_f=cat_features\n\nbo_best_params = bayes_lgb(bayes_auc_lgb, bounds_lgb)","a7f916c0":"## Preprocessing and modelling","9cdeefd5":"Simple brute forced approach with LIghtGBM. No preprocessing except marked categorical features. ","a2b96c7a":"## Usefull functions"}}