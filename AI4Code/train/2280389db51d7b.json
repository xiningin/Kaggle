{"cell_type":{"dee3bf9d":"code","973a4c49":"code","5d4ca0d1":"code","5a1d7ef9":"code","11e0e1b4":"code","75ce270e":"code","d9157824":"code","9cf7dbe5":"code","885727d4":"code","8d14a125":"code","787152d9":"code","b810a2e7":"code","dc5782bb":"code","4d7b89c8":"code","87d06d94":"code","39462c38":"code","b089791b":"code","1fbade8e":"code","4b350e16":"code","4b86a560":"code","af3fa5a4":"code","be55f577":"code","d8bb7b77":"code","3e3ccb78":"code","92aa6634":"code","0e828057":"code","f1d2235c":"code","5b2e8b8b":"code","05363789":"code","ac4011d5":"code","e6bf4da6":"code","73df9e3d":"code","e4d9e12c":"code","2f95ae4b":"markdown","870bc95f":"markdown","fdfb8063":"markdown","8f4a5ce8":"markdown","d06bb7a1":"markdown","2c7f3124":"markdown","934568fd":"markdown","2e5d5180":"markdown","05f78072":"markdown","d3ee9b19":"markdown","fc23fd07":"markdown","cb1e5a6c":"markdown","592979e9":"markdown","12d7a05a":"markdown","dc071b76":"markdown","7b346773":"markdown","1ec3c9c3":"markdown","3745e826":"markdown","1f5c5f94":"markdown","552347e7":"markdown","fa6ef24a":"markdown","901cda50":"markdown","95764ae6":"markdown","97002f0f":"markdown","cc074167":"markdown","adad8513":"markdown","57ba6e3a":"markdown","768b8b76":"markdown","6d3b3c86":"markdown","41ba0655":"markdown","79119b11":"markdown","5e2fa0f5":"markdown"},"source":{"dee3bf9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","973a4c49":"class Activation():\n    def __init__(self):\n        pass","5d4ca0d1":"class Linear_Activation(Activation):\n    def __init__(self, name='Linear'):\n        self.name = name\n        pass\n\n    def __call__(self, z):\n        \"\"\"\n        \u03c3(z) = z\n        \"\"\"\n        return z\n    \n    def derivative(self, z):\n        \"\"\"\n        \u03c3'(z) = 1\n        \"\"\"\n        return np.ones(z.shape)","5a1d7ef9":"class Sigmoid_Activation(Activation):\n    def __init__(self, name = 'Sigmoid'):\n        self.name = name\n        pass\n    \n    def __call__(self, z):\n        \"\"\"\n        \u03c3(z) = 1\/(1+ e^-z)\n        \"\"\"\n        return 1 \/ (1 + np.exp(- z))\n    \n    def derivative(self, z):\n        \"\"\"\n        \u03c3'(z) = e^-z\/(1+ e^-z)^2 = \u03c3(z)(1-\u03c3(z))  \n        \"\"\"\n        sigmoid = Sigmoid_Activation()\n        return sigmoid(z) * (1 - sigmoid(z))","11e0e1b4":"class Tanh_Activation(Activation):\n    def __init__(self, name='Tanh'):\n        self.name = name\n        pass\n    \n    def __call__(self, z):\n        \"\"\"\n        \u03c3(z) = (e^z - e^-z)\/(e^z + e^-z)\n        \"\"\"\n        return (np.exp(z) - np.exp(- z)) \/ (np.exp(z) + np.exp(- z))\n    \n    def derivative(self, z):\n        \"\"\"\n        \u03c3'(z) = 1 - \u03c3(z)^2  \n        \"\"\"\n        tanh = Tanh_Activation()\n        return 1 - tanh(z) ** 2","75ce270e":"class Softmax(Activation):\n    def __init__(self, alpha = 0.01, name=\"Softmax\"):\n        self.name = name\n        self.alpha = alpha\n        pass\n    \n    def __call__(self, z):\n        \"\"\"\n        \u03c3_j(z) = e^z_j \/ \u03a3e^z_k \n        \"\"\"\n        softmax = np.zeros(z.shape)\n        for i in range(softmax.shape[0]):\n            softmax[i] = np.exp(z[i]) \/ np.sum(np.exp(z))\n        return softmax\n    \n    def derivative(self, z):\n        \"\"\"\n        d\u03c3_j\/dz_i = (e^z_i * \u03a3e^z_k - e^2z_i)\/(\u03a3e^z_k)^2 if i=j ; - e^z_i * e^z_j \/(\u03a3e^z_k)^2 else    \n        \"\"\"\n        a = self(z)\n        return np.diag(a) - a @ a.T #Here we compute the jacobian as each composant of the softmax depends on the others.","d9157824":"class ReLU_Activation(Activation):\n    def __init__(self, name = \"ReLU\"):\n        self.name = name\n        pass\n    \n    def __call__(self, z):\n        \"\"\"\n        \u03c3(z) = max(0, z)\n        \"\"\"\n        return np.maximum(0, z)\n    \n    def derivative(self, z):\n        \"\"\"\n        \u03c3'(z) = 1 if z>=0 ; 0 if z<0  \n        \"\"\"\n        derivative = np.zeros(z.shape)\n        derivative[z >= 0] = 1\n        return derivative","9cf7dbe5":"class Leaky_ReLU_Activation(Activation):\n    def __init__(self, alpha = 0.01, name=\"PReLU\"):\n        self.name = name\n        self.alpha = alpha\n        pass\n    \n    def __call__(self, z):\n        \"\"\"\n        \u03c3(z) = z if z>0 ; \u03b1z if z<=0 \n        \"\"\"\n        return np.where(z <= 0, self.alpha * z, z)\n    \n    def derivative(self, z):\n        \"\"\"\n        \u03c3'(z) = 1 if z>0 ; \u03b1 if z<=0  \n        \"\"\"\n        derivative = np.full(z.shape, self.alpha)\n        derivative[z > 0] = 1\n        return derivative","885727d4":"class ELU_Activation(Activation):\n    def __init__(self, alpha=1, name=\"ELU\"):\n        self.name = name\n        self.alpha = alpha\n        pass\n    \n    def __call__(self, z):\n        \"\"\"\n        \u03c3(z) = z if z>0 ; \u03b1(e^z - 1) if z<=0 \n        \"\"\"\n        return np.where(z <= 0, self.alpha * (np.exp(z) - 1), z)\n    \n    def derivative(self, z):\n        \"\"\"\n        \u03c3'(z) = 1 if z>0 ; \u03b1e^z if z<=0  \n        \"\"\"\n        derivative = np.ones(z.shape)\n        derivative[z <= 0] = self.alpha * np.exp(z[z <= 0])\n        return derivative","8d14a125":"class Layers():\n    def __init__(self):\n        pass","787152d9":"class Dense_Layer(Layers):\n    def __init__(self, nb_neurons, input_shape, activation, name = 'Dense'):\n        self.name = name\n        self.input_shape = input_shape\n        self.activation = activation\n        self.W = np.random.rand(input_shape,nb_neurons) * 0.1\n        self.b = np.random.rand(1, nb_neurons)* 0.1\n        self.x = None\n        self.z = None\n        self.nb_neurons = nb_neurons\n        \n        #each layer has to stock this for momentum in certain gradient descent, do not care on first reading\n        self.previous_V_W = 0\n        self.previous_V_b = 0\n        self.previous_S_W = 0\n        self.previous_S_b = 0\n        \n        \n    def __call__(self, x):\n        \"\"\"\n        Forward\n        \"\"\"\n        self.x = x\n        self.z = self.x @ self.W + self.b\n        a = self.activation(self.z)\n        return a\n        \n    def back_propagation(self, dL_dy, alpha = 0.01):\n        \"\"\"\n        Gradient step \n        \"\"\"\n        m = x.shape[0]\n        \n        dy_dz = self.activation.derivative(self.z) \n        dloss_dz = dL_dy * dy_dz\n        dz_dw = self.x\n        dloss_dW = dz_dw.T @ dloss_dz \/ m      \n        dz_db = np.ones((1, dloss_dz.shape[0]))\n        dloss_db = dz_db @ dloss_dz \/ m\n        dL_dy = dloss_dz @ self.W.T\n        return dloss_dW, dloss_db, dL_dy","b810a2e7":"class Loss_Function():\n    def __init__(self):\n        pass","dc5782bb":"class Mean_Square_Error(Loss_Function):\n    def __init__(self):\n        pass\n    \n    def __call__(self, y_hat, y):\n        if y.shape != y_hat.shape:\n            raise ValueError(f'Prediction and target to predict must have same shape. Here y_hat.shape = {y_hat.shape} whereas y.shape = {y.shape}.')\n        else:\n            return np.mean(1 \/ 2 * (y_hat - y) ** 2, axis = 1, keepdims=True)\n    \n    def derivative(self, y_hat, y):\n        \"\"\"\n        L'(y_hat) = y_hat - y\n        \"\"\"\n        return np.mean(y_hat - y, axis = 1, keepdims=True)\n    \n    def cost(self, y_hat, y):\n        return (1 \/ 2) * np.sum((y_hat - y) ** 2 )  \/ y.shape[0]","4d7b89c8":"class Binary_Cross_Entropy(Loss_Function):\n    def __init__(self):\n        pass\n    \n    def __call__(self, y_hat, y):        \n        \n        if y_hat.shape[1] != 1:\n            raise ValueError(\"Using Binary Cross Entropy must be done with one neuron as a ousput. Meaning the shape of the prediction must be a value.\")\n        elif y.shape != y_hat.shape:\n            raise ValueError(f'Prediction and target must have same shape. Here y_hat.shape = {y_hat.shape} whereas y.shape = {y.shape}.')\n        else:\n            y_hat_no_one = np.clip(y_hat, a_min=1e-10, a_max=1 - 1e-10) #avoid log(0)\n            return - (y * np.log(y_hat_no_one) + (1 - y) * np.log(1 - y_hat_no_one))\n    \n    def derivative(self, y_hat, y):\n        \"\"\"\n        L'(y_hat) = - (y\/y_hat - (1-y)\/(1-y_hat))\n        \"\"\"\n        y_hat_no_one = np.clip(y_hat, a_min=1e-10, a_max=1 - 1e-10) #avoid division by 0\n        return - (y \/ y_hat_no_one - (1 - y) \/ (1 - y_hat_no_one))\n    \n    def cost(self, y_hat, y):\n        return np.sum(self(y_hat, y)) \/ y.shape[0]","87d06d94":"class Categorical_Cross_Entropy(Loss_Function):\n    def __init__(self):\n        pass\n    \n    def __call__(self, y_hat, y):\n        \"\"\"\n        L(y_hat) = - \u03a3 y*log(y_hat) \n        \"\"\"\n        if y.shape != y_hat.shape:\n            raise ValueError(f'Prediction and target must have same shape. Here y_hat.shape = {y_hat.shape} whereas y.shape = {y.shape}.')\n        else:\n            y_hat_no_one = np.clip(y_hat, a_min=1e-10, a_max=1 - 1e-10) #avoid log(0)            \n            return - np.sum(y * np.log(y_hat_no_one), axis = 1, keepdims=True)\n    \n    def derivative(self, y_hat, y):\n        \"\"\"\n        L'(y_hat) = - y\/y_hat\n        \"\"\"\n        y_hat_no_one = np.clip(y_hat, a_min=1e-10, a_max=1 - 1e-10) #avoid division by 0\n        return - y \/ y_hat_no_one\n    \n    def cost(self, y_hat, y):\n        return np.sum(self(y_hat, y)) \/ y.shape[0]","39462c38":"class Optimizer():\n    def __init__(self):\n        pass","b089791b":"class Simple_Gradient_Descent(Optimizer):\n    def __init__(self, alpha = 0.01):\n        self.alpha = alpha\n    \n    def update_weights(self, weights, biases, dloss_dw, dloss_db):\n        weights = weights - self.alpha * dloss_dw\n        biases = biases - self.alpha * dloss_db\n        return weights, biases","1fbade8e":"class SGD(Optimizer):\n    def __init__(self, alpha = 0.01, beta = 0.9):\n        self.alpha = alpha\n        self.beta = beta\n                 \n    def update_weights(self, weights, biases, dloss_dw, dloss_db, previous_V_W, previous_V_b):\n        V_W = self.beta * previous_V_W + (1 - self.beta) * dloss_dw \n        V_b = self.beta * previous_V_b  + (1 - self.beta) * dloss_db \n        weights = weights - self.alpha * V_W\n        biases = biases - self.alpha * V_b \n        return weights, biases, V_W, V_b","4b350e16":"class Adam(Optimizer):\n    def __init__(self, alpha = 0.01, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-10):\n        self.alpha = alpha\n        self.beta_1 = beta_1 \n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n\n                 \n    def update_weights(self, weights, biases, dloss_dw, dloss_db, previous_V_W, previous_V_b, previous_S_W, previous_S_b):\n        V_W = self.beta_1 * previous_V_W + (1 - self.beta_1) * dloss_dw \n        V_b = self.beta_1 * previous_V_b + (1 - self.beta_1) * dloss_db \n        S_W = self.beta_2 * previous_S_W + (1 - self.beta_2) * dloss_dw ** 2 \n        S_b = self.beta_2 * previous_S_b + (1 - self.beta_2) * dloss_db ** 2\n        weights = weights - self.alpha * V_W \/ np.sqrt((S_W + self.epsilon))\n        biases = biases - self.alpha * V_b \/ np.sqrt((S_b + self.epsilon))\n        return weights, biases, V_W, V_b, S_W, S_b","4b86a560":"class Model():\n    def __init__(self):\n        self.layers = []\n        self.built = False        \n        self.loss = None\n        self.optimizer = None\n        \n    def predict(self,x):\n        output = x\n        for i in range(len(self.layers)):\n            output = self.layers[i](output) \n        return output\n    \n    def add(self, layer):\n        self.layers.append(layer)\n        \n    def build(self, loss, optimizer):\n        if self.built:\n            raise Exception(\"Model has already been build.\")\n        else:\n            self.loss = loss\n            self.optimizer = optimizer\n            self.built = True\n    \n    def summary(self):\n        if self.built:\n            #print(f\"| Layer's nb_neurons | Layer's input shape | Layer's activation | Number of parameters\")\n            print(\"{:>20} | {: >10} | {: >11} | {: >12} | {: >13}\".format(\"Name\",\"Nb neurons\",\"Input shape\", \"Activation\",\"Nb parameters\"))\n            print('\u2500'*85)\n            for layer in self.layers:\n                print(\"{:>20} | {: >10} | {: >11} | {: >12} | {: >13}\".format(layer.name,layer.nb_neurons,layer.input_shape,layer.activation.name,layer.nb_neurons ** 2 + layer.nb_neurons))\n                #print(f'{layer.name}  |  {layer.nb_neurons}  |  {layer.input_shape}  |  {layer.activation.name}  |  {layer.nb_neurons ** 2 + layer.nb_neurons}')                \n        else:\n            raise Exception(\"Model not built.\")\n            \n    def evaluate(self,x, y):\n        y_hat = self.predict(x)\n        if y.shape[1] == 1:\n            acc = ((y_hat > 0.5).astype(int) == y).astype(int).sum() \/ y.shape[0]\n        else:\n            acc = (np.argmax(y_hat, axis = 1) == (np.argmax(y, axis = 1))).astype(int).sum() \/ y.shape[0]\n        return self.loss.cost(self.predict(x), y), acc     \n    \n    def fit(self, x, y, epochs = 1,validation_split = None, batch_size = 32, shuffle = True, show_batch_loss=False):\n        if validation_split == None:\n            x_train,y_train = x, y\n            hist = {'acc':[], 'loss':[]}\n        else:\n            if shuffle:\n                permutation = np.random.permutation(x.shape[0]) #using permutations allows use to make the same shuffle\n                x_shuffled, y_shuffled = x[permutation], y[permutation]\n            x_test, x_train,y_test, y_train = x_shuffled[0:int(validation_split * x.shape[0])], x_shuffled[int(validation_split * x.shape[0]):], y_shuffled[0:int(validation_split * x.shape[0])], y_shuffled[int(validation_split * x.shape[0]):]\n            hist = {'acc':[], 'loss':[], 'val_acc':[], 'val_loss':[]}\n            \n        indexes = np.arange(x_train.shape[0])\n        batchs = [indexes[i * batch_size:(i + 1) * batch_size] for i in range(x_train.shape[0] \/\/ batch_size + 1)]\n        for epoch in range(epochs):\n            print(f\"Epoch : {epoch + 1}\/{epochs}\")\n            for batch in batchs:\n                y_batch = self.predict(x_train[batch])\n                loss = self.loss.cost(y_batch, y_train[batch])\n                if show_batch_loss:\n                    print(f' Batch :{batch[0]} to {batch[-1]} | Loss : {loss}')\n                dL_dy = self.loss.derivative(y_batch, y_train[batch])\n                for layer in reversed(self.layers):\n                    dloss_dw, dloss_db, dL_dy = layer.back_propagation(dL_dy)\n                    if type(self.optimizer).__name__ == \"Simple_Gradient_Descent\" : #Doesn't need \"previous ones\"\n                        layer.W, layer.b = self.optimizer.update_weights(layer.W, layer.b, dloss_dw, dloss_db)\n                    elif type(self.optimizer).__name__ == \"SGD\" : #need to remember two variables\n                        layer.W, layer.b, layer.previous_V_W, layer.previous_V_b = self.optimizer.update_weights(layer.W, layer.b, dloss_dw, dloss_db, layer.previous_V_W, layer.previous_V_b)\n                    elif type(self.optimizer).__name__ == \"Adam\" : #need to remember four variables\n                        layer.W, layer.b, layer.previous_V_W, layer.previous_V_b, layer.previous_S_W, layer.previous_S_b = self.optimizer.update_weights(layer.W, layer.b, dloss_dw, dloss_db, layer.previous_V_W, layer.previous_V_b, layer.previous_S_W, layer.previous_S_b)\n                    \n            if validation_split == None:\n                l,acc = self.evaluate(x_train, y_train)\n                hist['acc'].append(acc)\n                hist['loss'].append(l)\n                print(f\"loss : {l}, accuracy : {acc} \\n\")\n            else:\n                l_train, acc_train = self.evaluate(x_train, y_train)\n                l_test, acc_test = self.evaluate(x_test, y_test)\n                hist['acc'].append(acc_train)\n                hist['loss'].append(l_train)\n                hist['val_acc'].append(acc_test)\n                hist['val_loss'].append(l_test)\n                print(f\"loss_train : {l_train}, accuracy_train : {acc_train} | loss_test : {l_test}, accuracy_test : {acc_test} \\n\")\n        return hist","af3fa5a4":"n_samples = 100\nn_features = 4\n\nx = np.zeros((n_samples, n_features))\nx[:n_samples \/\/ 2] += np.array([5] * n_features)\nx += np.random.rand(n_samples, n_features)\ny = np.zeros(n_samples)\ny[:n_samples \/\/ 2] = 1\ny = y.reshape(-1,1)\n\npermutation = np.random.permutation(x.shape[0])\nx = x[permutation]\ny = y[permutation]","be55f577":"model = Model()","d8bb7b77":"model.add(Dense_Layer(5,4,ReLU_Activation(), name=\"Input\"))\nmodel.add(Dense_Layer(5,5,ReLU_Activation(), name=\"1st hidden layer\"))\nmodel.add(Dense_Layer(1,5,Sigmoid_Activation(), name=\"Ouput\"))","3e3ccb78":"model.build(loss=Binary_Cross_Entropy(), optimizer=SGD())","92aa6634":"model.summary()","0e828057":"hist = model.fit(x,y, epochs=500, batch_size=32, validation_split=0.2)","f1d2235c":"import matplotlib.pyplot as plt\nplt.figure()\nplt.title(\"Accuracy\")\nplt.plot(hist['acc'])\nplt.plot(hist['val_acc'])\nplt.legend([\"Train Accuracy\",\"Validation Accuracy\"])\nplt.show()\n\nplt.figure()\nplt.title(\"Loss\")\nplt.plot(hist['loss'])\nplt.plot(hist['val_loss'])\nplt.legend([\"Train Loss\",\"Validation Loss\"])\nplt.show()","5b2e8b8b":"model = Model()","05363789":"model.add(Dense_Layer(5,4,ReLU_Activation(), name=\"Input\"))\nmodel.add(Dense_Layer(8,5,ReLU_Activation(), name=\"1st hidden layer\"))\nmodel.add(Dense_Layer(8,8,ReLU_Activation(), name=\"2nd hidden layer\"))\nmodel.add(Dense_Layer(5,8,ReLU_Activation(), name=\"3rd hidden layer\"))\nmodel.add(Dense_Layer(1,5,Sigmoid_Activation(), name=\"Ouput\"))","ac4011d5":"model.build(loss=Binary_Cross_Entropy(), optimizer=Adam())","e6bf4da6":"model.summary()","73df9e3d":"hist = model.fit(x,y, epochs=10, batch_size=32, validation_split=0.2)","e4d9e12c":"import matplotlib.pyplot as plt\nplt.figure()\nplt.title(\"Accuracy\")\nplt.plot(hist['acc'])\nplt.plot(hist['val_acc'])\nplt.legend([\"Train Accuracy\",\"Validation Accuracy\"])\nplt.show()\n\nplt.figure()\nplt.title(\"Loss\")\nplt.plot(hist['loss'])\nplt.plot(hist['val_loss'])\nplt.legend([\"Train Loss\",\"Validation Loss\"])\nplt.show()","2f95ae4b":"# 4. Conclusion","870bc95f":"Here are the simple gradient descent formulas, $\\alpha$ being an hyperparameter of the optimizer :\n- $\\large W_{\\small{(input\\_shape, nb\\_neurons)}} =  W_{\\small{(input\\_shape, nb\\_neurons)}} - \\alpha \\large \\frac{dL}{dW}_{\\small{(input\\_shape, nb\\_neurons)}}$\n- $\\large b_{\\small{(1, nb\\_neurons)}} =  b_{\\small{(1, nb\\_neurons)}} - \\alpha \\large \\frac{dL}{db}_{\\small{(1, nb\\_neurons)}}$","fdfb8063":"## 2.3. Loss Function","8f4a5ce8":"## 4.1. What I learned","d06bb7a1":"## 2.5. Models","2c7f3124":"# 2. Neural Network","934568fd":"Adam works even with unecessary complicated networks :","2e5d5180":"# 1. Introduction","05f78072":"We consider that our $x$ matrix is shaped like this : Lines= samples, Column = features.\nWhy ? Because Dataframes are usually done that way.","d3ee9b19":"For SGD's momentum, we use this formula : ![image.png](attachment:image.png) Here $\u03b8$ is the pair $(W, b)$\n","fc23fd07":"We will compute these fuctions : ![image.png](attachment:image.png) Where $\\hat{y}$  is the prediction, and $y$ the target. Furthermore, we calculate the cost which is the sum of all loss (for every samples).","cb1e5a6c":"And for Adam : ![image.png](attachment:image.png)","592979e9":"Here is the overview of our Neural Network : ![image.png](attachment:image.png)","12d7a05a":"## 3.2. With Adam","dc071b76":"## 4.2. What could have been better","7b346773":"## 2.2. Layers","1ec3c9c3":"# 3. Tests","3745e826":"Here is how forward works for a simple dense layer, we use the same notation as before : ![image.png](attachment:image.png)","1f5c5f94":"Note : To understand why we sum on $axis = 1$ in the __call__ function, we have to remember that our rows are samples and columns are features. So per samples we sum on $axis = 1$ and then to compute the cost, we sum on $axis = 0$.  ![image.png](attachment:image.png)","552347e7":"For matrix calculation, we set $m=$ \"number of samples\" : \n- $\\large \\frac{dL}{dz}_{\\small{(m, nb\\_neurons)}} = \\frac{dL}{dy}_{\\small{(m, nb\\_neurons)}} \\: \\odot \\: \u03c3' ({z})_{\\small{(m, nb\\_neurons)}}$\n\n- $\\large \\frac{dL}{dW}_{\\small{(input\\_shape, nb\\_neurons)}} = \\frac{1}{m} {{x}}^{T}_{\\small{(input\\_shape,m)}} . \\frac{dL}{dz}_{\\small{(m,nb\\_neurons)}} $\n- $\\large \\frac{dL}{db}_{\\small{(1, nb\\_neurons)}} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{dL}{dz}_{\\small{(m, nb\\_neurons)}}$","fa6ef24a":"* The handling of momentum variables is not done the best way. It could have been more relevant to use lists inside optimizers classes to keep the variable to remember.","901cda50":"We will compute both activation and derivative.","95764ae6":"## 2.1. Activation functions","97002f0f":"For some reason, the gradient takes a long time to find a good slope but then it rolls it quickly.","cc074167":"We will do a simple test, separating two clusters by adding 5 to all features.","adad8513":"This will be our notation for a dense layer : ![image.png](attachment:image.png)","57ba6e3a":"For backpropagation, we will use the chain rule : ![image.png](attachment:image.png)","768b8b76":"## 2.4. Optimizers","6d3b3c86":"In this notebook, we will create a neural network only using Numpy. This is possible thanks to the course of Geoffroy Peeters, teacher at T\u00e9l\u00e9com Paris, IP Paris.","41ba0655":"## 3.1. With SGD","79119b11":"#### It is essential to know that both $x$ and $z$ have to be saved for each layer as they are used in backpropagation.","5e2fa0f5":"* I learned that a class can call itself using self(x)\n* I learned to use np.clip to avoid dangerous values"}}