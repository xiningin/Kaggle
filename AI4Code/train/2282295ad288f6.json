{"cell_type":{"fc4d0c2e":"code","e0999a3e":"code","18a250bf":"code","aa650981":"code","771fd57e":"code","0b4b9047":"code","5829ba5e":"code","bfbf296d":"code","5c5afd82":"code","03ea8951":"code","40db1ec2":"code","ed168067":"code","0395c741":"code","7e99cdff":"code","96c35d6e":"code","d6ee3c4c":"code","55a50b36":"code","5dded65b":"code","05bff60b":"code","d7f3025f":"code","c5c38acf":"code","28805bd6":"code","78263607":"code","1752d98d":"code","d280db08":"code","8d9e17dd":"code","1a7f7f84":"code","4b1fb39b":"code","b2d3fdae":"code","ad871a85":"code","d9c2ceb2":"code","1491ea59":"code","8b6c8f34":"code","63fe26c5":"code","9644cfff":"code","967f5c12":"code","e08b5d49":"code","72eb3e91":"code","df228a3c":"code","96dbf3c5":"code","93875a1b":"code","a2f68d70":"code","14f6355b":"code","e409ac9e":"code","6029d774":"code","fdd2307f":"markdown","c6497277":"markdown","0a1a8889":"markdown","a0510541":"markdown","3143fc83":"markdown","c8fa6923":"markdown","cb6a36b8":"markdown","1396ae0d":"markdown","b514f854":"markdown","015be5c7":"markdown","f7c18cac":"markdown","18dea411":"markdown","f0d8ee20":"markdown","53940634":"markdown","f2ce1e31":"markdown","82682641":"markdown","e549c8b9":"markdown","9dfba2bd":"markdown","0d058ee2":"markdown","a0557324":"markdown","b4d678f3":"markdown","c7cceae7":"markdown","21822e86":"markdown","9651dc8f":"markdown","e0cac26a":"markdown","49596809":"markdown","e68f326b":"markdown","47b22532":"markdown","20e29cf4":"markdown","39ed8deb":"markdown"},"source":{"fc4d0c2e":"import IPython\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport graphviz \nimport statsmodels.formula.api as smf\nimport copy\n\nimport warnings\nwarnings.filterwarnings('ignore')","e0999a3e":"train_set_full = pd.read_csv('..\/input\/train.csv')\ntest_set = pd.read_csv('..\/input\/test.csv')","18a250bf":"train_set_full[['Pclass', 'Survived']].groupby(['Pclass'], \n                                               as_index=False).mean().sort_values(by='Survived', ascending=False)","aa650981":"train_set_full[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","771fd57e":"g = sns.FacetGrid(train_set_full, col='Survived')\ng.map(plt.hist, 'Age', bins=20)\ng.title = 'Survival rate depending on age'","0b4b9047":"combined_set = [train_set_full, test_set]","5829ba5e":"for dataset in combined_set:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","bfbf296d":"pd.crosstab(train_set_full['Title'], train_set_full['Sex'])","5c5afd82":"for dataset in combined_set:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Dr'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace(['Mme', 'Countess', 'Dona'], 'Mrs')\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir',], 'Mr')\n    \ntrain_set_full[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","03ea8951":"train_set_full[np.isnan(train_set_full['Age'])].groupby(['Title'], as_index=False).PassengerId.count()","40db1ec2":"for dataset in combined_set:\n    dataset['Title'] = dataset['Title'].replace('Master', 2)\n    dataset['Title'] = dataset['Title'].replace('Miss', 3)\n    dataset['Title'] = dataset['Title'].replace('Mr', 0)\n    dataset['Title'] = dataset['Title'].replace('Mrs', 4)\n    dataset['Title'] = dataset['Title'].replace('Rare', 1)\n    dataset['Sex'] = dataset['Sex'].replace('male', 0)\n    dataset['Sex'] = dataset['Sex'].replace('female', 1)\n","ed168067":"train_set = train_set_full[np.isfinite(train_set_full['Age'])].copy(deep=True)\ntrain_set = train_set.reset_index(drop=True)\ntrain_set.head()","0395c741":"def age2cat(age):\n    if age < 5:\n        return 1 #toddler\n    elif age < 16:\n        return 2 #child\n    elif age < 50:\n        return 3 #adult\n    else:\n        return 4 #elderly\n\ntrain_set['Agegroup'] = [age2cat(x) for x in train_set.Age]   ","7e99cdff":"train_set['Agegroup'].unique()","96c35d6e":"train_set['Family_Size']=train_set['SibSp']+train_set['Parch']\ntest_set['Family_Size']=test_set['SibSp']+test_set['Parch']\n\n\ntrain_set['Fare_p_Person']=train_set['Fare']\/(train_set['Family_Size']+1)\ntest_set['Fare_p_Person']=test_set['Fare']\/(test_set['Family_Size']+1)","d6ee3c4c":"corr_table = train_set.corr(method='pearson')\ncorr_table.style.background_gradient(cmap='RdYlGn', axis=1)","55a50b36":"from sklearn import tree\n# Enter your code here\nfrom sklearn.model_selection import cross_val_score\ndt_classifier = tree.DecisionTreeClassifier(criterion='gini',  # or 'entropy' for information gain\n                       splitter='best',  # or 'random' for random best split\n                       max_depth=None,  # how deep tree nodes can go\n                       min_samples_split=2,  # samples needed to split node\n                       min_samples_leaf=1,  # samples needed for a leaf\n                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n                       max_features=None,  # number of features to look for when splitting\n                       max_leaf_nodes=None,  # max nodes\n                       min_impurity_decrease=1e-07, #early stopping\n                       random_state = 10) #random seed","5dded65b":"X = pd.DataFrame(train_set, columns=['Pclass', 'Title', 'Sex', 'Fare'])\nY = train_set.Survived\n\ncross_val_score(dt_classifier, X, Y, cv=10).mean()","05bff60b":"dt_model = dt_classifier.fit(X, Y)\n\ndt_model.feature_importances_","d7f3025f":"##full tree can be found in titanic_tree.pdf in the working dir\n# plot_tree = tree.export_graphviz(dt_classifier, out_file=None) \n# graph = graphviz.Source(plot_tree) \n# graph.render(\"titanic_tree\") ","c5c38acf":"dot_data = tree.export_graphviz(dt_model, out_file=None,\n                                max_depth=2, \n                         feature_names=[x for x in X.columns],  \n                         class_names=Y.name,  \n                         filled=True, rounded=True,\n                         special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph","28805bd6":"# new decision tree classifier\ndt_classifier_new = tree.DecisionTreeClassifier(criterion='gini',  # or 'entropy' for information gain\n                       splitter='best',  # or 'random' for random best split\n                       max_depth=4,  # how deep tree nodes can go\n                       min_samples_split=2,  # samples needed to split node\n                       min_samples_leaf=1,  # samples needed for a leaf\n                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n                       max_features=None,  # number of features to look for when splitting\n                       max_leaf_nodes=None,  # max nodes\n                       min_impurity_decrease=1e-07, #early stopping\n                       random_state = 10) #random seed","78263607":"np.random.seed(seed=2468) #13579\nfeatures = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Title', 'Age', 'Agegroup', 'Family_Size', 'Fare_p_Person']\nfeatures_labels = features + ['Survived']\n\ntrain_idx = np.random.choice(range(len(train_set)), int(len(train_set) * .8), replace=False)\ntest_idx = list(set(range(len(train_set))) - set(list(train_idx)))\n\ntrain_x = train_set.loc[train_idx, features]\ntrain_y = train_set.loc[train_idx, 'Survived']\ntest_x = train_set.loc[test_idx, features]\ntest_y = train_set.loc[test_idx, 'Survived']","1752d98d":"dt_model_new = dt_classifier_new.fit(train_x, train_y)\nprint('Test accuracy:')\nprint(dt_model_new.score(test_x, test_y))\nprint('\\n''Feature importance:')\nfor i in range(len(features)):\n    print(features[i], dt_model_new.feature_importances_[i])","d280db08":"fig, ax = plt.subplots()\nind = np.arange(10)\n\nplt.bar(ind, dt_model_new.feature_importances_)\nax.set_xticks(ind)\nax.set_xticklabels(features)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(45)\nax.set_ylim([0, 0.6])\nax.set_ylabel('Percent of impact')\nax.set_title('Which feature was the most impactful?')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.show\n","8d9e17dd":"dot_data_1 = tree.export_graphviz(dt_model_new, out_file=None,\n                                max_depth=2, \n                         feature_names=features,  \n                         class_names=train_y.name,  \n                         filled=True, rounded=True,\n                         special_characters=True)  \ngraph_1 = graphviz.Source(dot_data_1)  \ngraph_1","1a7f7f84":"from sklearn.model_selection import cross_validate","4b1fb39b":"chart1_train = []\nchart1_test = []\nchart2_z = []\nchart2_x = []\nchart2_y = []\n\nfor i in range(4,10):\n    clf = tree.DecisionTreeClassifier(max_depth=i)\n    scores = cross_validate(clf, train_set[features], train_set['Survived'], cv=5, return_train_score=True)\n    chart1_train.append(scores['train_score'].mean())\n    chart1_test.append(scores['test_score'].mean())\n    for l in range(1,4):\n        clf = tree.DecisionTreeClassifier(max_depth=i, min_samples_leaf=l)\n        scores = cross_validate(clf, train_set[features], train_set['Survived'], cv=5, return_train_score=False)\n        chart2_z.append(scores['test_score'].mean())\n        chart2_x.append(i)\n        chart2_y.append(l)","b2d3fdae":"fig, ax = plt.subplots()\nplt.plot(chart1_train)\nplt.plot(chart1_test)\nplt.legend(['train score', 'test score'], loc='upper left')\nax.set_ylabel('accuracy')\nax.set_xlabel('max_depth')\nax.set_title('Accuracy scores of the Decision Tree depending on the max_depth value')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.show()","ad871a85":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Make data.\nX = range(4,10)\nY = range(1,4)\nX, Y = np.meshgrid(X, Y)\nZ = np.asarray(chart2_z).reshape(3,6)\n\n# Plot the surface.\nax.scatter(X, Y, Z, s=100)\n\n# Customize the z axis.\nax.set_zlim(0.75, 0.83)\n\nax.yaxis.set_major_locator(LinearLocator(4))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.zaxis.set_major_locator(LinearLocator(4))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.set_xlabel('max_depth')\nax.set_ylabel('min-samples-leaf')\nplt.show()","d9c2ceb2":"# Enter your code here\nfrom sklearn.linear_model import LogisticRegression, LassoCV\n\nlogit = LogisticRegression(penalty='l2', \n                   dual=False, \n                   tol=0.0001, \n                   C=1.0, \n                   fit_intercept=True, \n                   intercept_scaling=1, \n                   class_weight=None,\n                   random_state=None, \n                   solver='liblinear', \n                   max_iter=100, \n                   multi_class='ovr', \n                   verbose=0, warm_start=False, n_jobs=1)\n\nmodel_logit = logit.fit(train_x, train_y, sample_weight=None)\n#tr_score = model_logit.score(train_x, train_y, sample_weight=None)\n#test_score = model_logit.score(test_x, test_y, sample_weight=None)","1491ea59":"scores = cross_validate(logit, train_set[features], train_set['Survived'], cv=10, return_train_score=True)","8b6c8f34":"test_score_logit = scores['test_score'].mean()\ntrain_score_logit = scores['train_score'].mean()\n\nprint('train: ' + str(train_score_logit) + '\\ntest: ' + str(test_score_logit))\nprint('\\n''Feature importance:')\nprint(logit.coef_)","63fe26c5":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nnames = ['Logistic \\nRegression', 'Decision \\nTree', 'LASSO', 'k-Nearest \\nNeighbors', \n         'Random \\nForest', 'm-layer \\nPerceptron', 'Gradient \\nBoosting'] \n\nestimators = [logit, dt_classifier_new, LassoCV(),  #lassoCV finds the best parameters for lasso\n            KNeighborsClassifier(n_neighbors=5), #K=5 turned out optimal\n            RandomForestClassifier(bootstrap=True, max_depth=5),\n            MLPClassifier(hidden_layer_sizes=(25, 2), max_iter=1000, shuffle=False),\n            GradientBoostingClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1, max_depth=3)]","9644cfff":"from sklearn.metrics import zero_one_loss, precision_score, recall_score, accuracy_score\n\nCV = pd.DataFrame(data=train_set, columns=features_labels)\narr = np.arange(len(CV))\nnp.random.shuffle(arr)\nshuffled = pd.DataFrame(data=CV, index=arr)\ncv_data = np.array_split(shuffled, 10)\nlen(CV)","967f5c12":"train_recalls = []\ntrain_precisions = []\ntrain_misclassified = []\n\ntest_recalls = []\ntest_precisions = []\ntest_misclassified = []\n\nall_errors = {}\nall_predictions = {}\n\nfor name, estimator in zip(names, estimators):\n    test = []\n    train = []\n    errors = []\n    \n    sum_misclass_train = 0\n    sum_misclass_test = 0\n\n    sum_precision_train = 0\n    sum_precision_test = 0\n\n    sum_recall_train = 0\n    sum_recall_test = 0\n\n    sum_accuracy_train = 0\n    sum_accuracy_test = 0\n        \n    test_predictions = []\n    all_predictions[name] = test_predictions\n    \n\n    for i in range(10):\n        data_copy = copy.deepcopy(cv_data)\n        test = data_copy.pop(i)\n\n        train = pd.concat(data_copy)\n        model = estimator.fit(train[features], train.Survived)\n        \n        y_pred_train = model.predict(train[features]).round()\n        misclass_train = zero_one_loss(train.Survived, y_pred_train, normalize=False)\n        sum_misclass_train+=misclass_train\n        accuracy_train = accuracy_score(train.Survived, y_pred_train)\n        sum_accuracy_train+=accuracy_train\n        recall_train = recall_score(train.Survived, y_pred_train)\n        sum_recall_train+=recall_train\n        precision_train = precision_score(train.Survived, y_pred_train)\n        sum_precision_train+=precision_train\n        \n        y_pred_test = model.predict(test[features]).round()\n        misclass_test = zero_one_loss(test.Survived, y_pred_test, normalize=False)\n        sum_misclass_test+=misclass_test\n        accuracy_test = accuracy_score(test.Survived, y_pred_test)\n        sum_accuracy_test+=accuracy_test\n        recall_test = recall_score(test.Survived, y_pred_test)\n        sum_recall_test+=recall_test\n        precision_test = precision_score(test.Survived, y_pred_test)\n        sum_precision_test+=precision_test\n        er = y_pred_test-test.Survived\n        error = er.iloc[er.nonzero()[0]]\n        errors.append(error)\n        \n        test_predictions.append(y_pred_test)\n        \n        \n        i+=1\n    print(name)\n    print(\"average number of people misclassified for train set: {0:.0f}\".format(sum_misclass_train\/10))\n    print(\"average accuracy for train set: {0:.3f}\".format(sum_accuracy_train\/10))\n    print(\"average number of people misclassified for test set: {0:.0f}\".format(sum_misclass_test\/10))\n    print(\"average accuracy for test set: {0:.3f}\".format(sum_accuracy_test\/10))\n\n    train_misclassified.append(sum_misclass_train\/10)\n    train_recalls.append(sum_recall_train\/10)\n    train_precisions.append(sum_precision_train\/10)\n\n    test_misclassified.append(sum_misclass_test\/10)\n    test_recalls.append(sum_recall_test\/10)\n    test_precisions.append(sum_precision_test\/10)\n\n    print()\n    all_errors[name] = error.index.values\n    \n\n    ","e08b5d49":"ind = np.arange(7)  # the x locations for the groups\nwidth = 0.35       # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind, train_precisions, width, color='r')\nrects2 = ax.bar(ind + width, train_recalls, width, color='y')\nax.set_ylabel('Scores')\nax.set_title('Train set: precision and recall per classifier')\nax.set_xticks(ind + width \/ 2)\nax.set_xticklabels(names)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(45)\nax.legend((rects1[0], rects2[0]), ('precision', 'recall'), loc='lower right')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.show()\n\nfig, ax = plt.subplots()\nrects3 = ax.bar(ind, test_precisions, width, color='r')\nrects4 = ax.bar(ind + width, test_recalls, width, color='y')\nax.set_ylabel('Scores')\nax.set_title('Test set: precision and recall per classifier')\nax.set_xticks(ind + width \/ 2)\nax.set_xticklabels(names)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(45)\nax.legend((rects1[0], rects2[0]), ('precision', 'recall'), loc='lower right')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.show()\n","72eb3e91":"ind = np.arange(7)  # the x locations for the groups\nwidth = 0.35       # the width of the bars\n\nfig, ax = plt.subplots()\nplt.plot(train_misclassified)\nplt.plot(test_misclassified)\nplt.legend(['train (643 people)', 'test (71)'], loc='upper left')\nax.set_ylabel('number of people')\nax.set_title('Number of misclassified people for train and test data')\nax.set_xticks(ind + width \/ 2)\nax.set_xticklabels(names)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(45)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.show()","df228a3c":"all_errors","96dbf3c5":"l = set.intersection(*(set(all_errors[name]) for name in names if name in all_errors))\ntrain_set.iloc[list(l)]","93875a1b":"s = (7,714)\nM_M_matrix = np.zeros(s)","a2f68d70":"estimator_index = {'Decision \\nTree': 0, 'Gradient \\nBoosting': 1, 'LASSO': 2, \n                   'Logistic \\nRegression': 3, 'Random \\nForest': 4, \n                   'k-Nearest \\nNeighbors': 5, 'm-layer \\nPerceptron': 6}\nfor key, value in all_errors.items():\n    M_M_matrix[estimator_index[key], value] = 1","14f6355b":"np.dot(M_M_matrix,np.transpose(M_M_matrix))","e409ac9e":"#MM = [np.correlate(N_M_matrix[i], N_M_matrix[j]) for i in range(7) for j in range(7)]\nnp.corrcoef(M_M_matrix)","6029d774":"def ensemble_train(train_x, train_y, estimators):\n    for cls in estimators:\n        cls.fit(train_x, train_y)\n    return estimators\n\ndef ensemble_predict(test_x, test_y, estimators):\n    predicted_y = np.array(test_y)  # just to initialize, will be overwritten\n \n    j = 0\n    for i, x in test_x.iterrows():\n        binary_votes = np.array([0,0])\n        for cls in estimators:\n            vote = cls.predict(x.values.reshape((1, test_x.shape[1]))).round()\n            binary_votes[int(vote)] += 1\n        y = np.argmax(binary_votes)\n        predicted_y[j] = y\n        j += 1\n    acc = accuracy_score(test_y, predicted_y)\n    #pd.DataFrame(predicted_y).to_csv('predictions.csv')\n    return acc \n\nestimators = ensemble_train(train_x, train_y, estimators)\nacc = ensemble_predict(test_x, test_y, estimators)\nprint(acc)\n","fdd2307f":"Semms like the most models are having problems with the same individuals (with some variations). And Logistic Regression is doung very differently - it will be a great help for our ensemble model.\n\nBelow are the passenger misclassified by all the models. What do they have in common?","c6497277":"#### Conclusion:\nTitle remains the most important feature, while Fare and Fare_p_Person had a smaller impact this time, even smaller than Pclass which is the second to most important feature","0a1a8889":"## Part 3: Putting together the pieces \n\nHere, we will explore how well different algorithms can predict survival on the Titanic.\n","a0510541":"#### Let's add some more features! \n\n1. Summing up Parent-child vector with Sibling-spouse should give us the family size. How likely were families to survive?\n\n2. Fare is given for a ticket, and since families traveled on one ticket, it's fair to divide the fare by family size - just to get a fair fare for each passenger.","3143fc83":"#### Conclusions:\n\nThe confusion matrix shows a relatively high positive correlation of survival rate with title and sex, and a significant negative correlation with passenger class (meaning, 1 class had better chances, 3 class had the worst chances). Other features, especially passenger ID, are not very useful. But we can try combine some features, as it seems to be a productive idea. For example, if you look at Fare_p_Person and Fare, you'll see that Fare correlates stronger. This might be because Fare is in fact a combined variable of fare per person and number of people in a family travelling by this fare since this is what a price of a ticket tells us about. That's why Fare takes some prediction power from family size while Fare_p_Person doesn't.","c8fa6923":"If we filter out rare titles, turnes out, married women (and mothers) had the highest chances to survive. Then unmarried women and girls follow. Boys under 18 (Masters) had significantly lower chances to survive, and Misters survived the least.\n\nExtracting titles is also usefull if we want to guess the age of some people for whom this data is NaN in the \"Age\" column. Let's take a look at some observations with missing age. How many Masters and Misters are there?","cb6a36b8":"### 2.3 Sensitivity analysis\nThe built-in algorithm we are using has several parameters which we can tune. Let's see how the choice of these parameters affects performance.\n\nFirst, let's see how max_depth affects train and test accuracy. On a single axis, we'll plot train and test accuracy as a function of max_depth.\n\nSecond, let's see how test accuracy relates to both max_depth and min_samples_leaf. We'll create a 3-D plot where the x-axis will be max_depth, the y-axis - min_samples_leaf, and the z-axis - accuracy. What combination of max_depth and min-samples_leaf achieves the highest accuracy? How sensitive are the results to these two parameters?","1396ae0d":"#### Conclusion:\n\nThe ensemble accuracy is 0.81 - 0.84, and it's in line with the best result so far (DT with 0.82).","b514f854":"Well, all of them survived against all odds! They all were men, and either travelled 3rd class or fell into age groups 3 and 4 - all three are strong predictors against survival. Apparently, our model doesn't like outliers.\n\nNow, let's compute an N-M matrix where the output of each of the models as a binary vector with one entry for each of the test data, and show how these prediction vectors relate to each other by plotting the M x M matrix of correlation coefficients.","015be5c7":"### 2.2 Tree tuning\n\nNow, we'll use all of the data to re-fit a single decision tree with max_depth = 4 (i.e., no cross-validation). We'll show the tree diagram and also plot the feature importances.","f7c18cac":"#### Conclusion:\n\nIn my implementation, the accuracy of this model is similar to the accuracy of the decision tree (however as we'll see further these two models do not correlate much and have different strengths and weaknesses).\n","18dea411":"#### Can we guess their age? \n\nThe title - Master, Miss, etc. - gives us a vague clue about the person's age.\n\nSo, according to early 20th century eticket:\n\nTitle | Age\n--|--\nMaster | under 18\nMiss | 0-90+\nMr | 18-90+\nMrs | 18-90+\nRare | 0-90+\n\n\n\nMost of the passengers with missing data were Misters, of the age 18 to 90+. The same range will likely apply to Mrs. The age range of Misses is even wider - from 0 to 90+. And only Masters, who are boys under 18, give us a smaller range. That's the finest granularity we can get. So it's pretty hard to deduce a heuristic here to put in the missing numbers, let alone run a ML model to predict the missing age (we don't have enough data).\n\nThis is why, for now we'll just drop the rows with missing age (all 177 of them) and create a new train set. We'll also reset index for it.","f0d8ee20":"### 2.1 Decision Tree\nUsing the basic [Decision Tree](http:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree) library in sklearn and a 10-fold cross-validation, we'll fit a model to predict titanic survival. What would be the average training and testing accuracy across our 10 folds? Also, we'll plot a diagram of the tree just to understand it better.","53940634":"But before, let's convert titles and gender to numerical values for future use. The bigger chance to survive - the higher value we assign.","f2ce1e31":"#### Conclusion:\n\nDecision Trees performs best on testing set, followed by GB and Random Forest.","82682641":"### 1.2 Correlates of survival\n\nLet's figure out what factors seem to determine whether or not a person would survive the sinking of the Titanic.","e549c8b9":"### 3.2 Error analysis\n\nAn error analysis comes handy when you need to gain some intuition for where your models are not performing well. Are our models all having problems with the same individuals?\n\nWe'll first do this manually, and the using a correlation matrix which will show the correlation in predictions between each model. ","9dfba2bd":"Our starting accuracy = 81.25% using 10-fold cross validation.\n\nSeems like 'Title and 'Fare' are indeed good features. What's interesting about them is that they both are \"combined\" feautures: title implicitly contains information about gender and age while fare - about family size and passenger class.","0d058ee2":"What Feature importance tells us is that, of these four parameters - passenger class, title, gender and fare, - **title** and **fare** seem to have a higher impact on the result.","a0557324":"Now, let's see how likely it is to survive depending on your passenger class, gender, age.","b4d678f3":"## Part 1: Exploring the Titanic\n\nThis notebook is adapted from my homework for UC Berkeley's Applied Machine Learning class (2017). To get started, read about the prediction problem on [Kaggle](https:\/\/www.kaggle.com\/c\/titanic). Then, download the data from [here](https:\/\/www.kaggle.com\/c\/titanic\/data). \n\nThen follow the notebook to unveil a beautiful story about men helping women and children survive sacrificing their own lives, about a group of outliers from the lowest part of the boat (3rd class) who managed to survive against all odds, and a story about several ML classifiers that failed to detect those outliers and sinked amidst the NaNs and human inconsistency.\n\n\n### 1.1 Exploratory data analysis\n\nLet's create 2-3 figures and tables to get a feel for the data. We'll check the data type of each variable, understand which variables have missing observations, and understand the distribution of each variable (to determine whether the variables should be normalized or not). Are any variables collinear or highly correlated?","c7cceae7":"### 3.1 Horseraces\n\nIn addition to logistic regression and the single decision tree, we'll use the following algorithms to predict survival: LASSO regression, k-Nearest Neighbors, [random forest](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html), a [multi-layer perceptron](http:\/\/scikit-learn.org\/stable\/modules\/neural_networks_supervised.html) and [Gradient Boosting](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html).\n\nThe barplot further below shows how well each algorithm compares relative to the others, separately for the training and testing datsets. What algorithm performs best?\n","21822e86":"### 2.4 Logistic Regression, for comparison\nIt's 10-fold cross-validated logistic regression. ","9651dc8f":"#### Conclusion:\n\nAccording to the 3D-plot, the combination of max_depth = 4 and min-samples_leaf = 1.2 achieves the highest accuracy (a little over 0.80). The result is moderately sensitive to these two parameters, becase when we're changing max_depth to 9 and min-samples-leaf to 3.10, the accuracy drops only 2%, to 0.79%","e0cac26a":"## Part 2: Decision Trees","49596809":"Now, for this new working train set, we'll create a new variable called 'Agegroup' and assign the following values to each observation: 1 if the age < 5, 2 if the age < 16, 3 if the age < 50, 4 for over 50.","e68f326b":"Seems like it is more likely to suvive if you are a woman or an infant, or traveling first class. As we can see from the diagram, there were many young people of 20-30 who did not survive, and who, probably, travelled 3rd class.","47b22532":"### 3.3 Develop an ensemble\n\nThis an ensemble learner created by hand. Using all of the training data, we fit each of the models in 3.1. Then, we classified each instance in the test data by taking the majority vote of all of our different fitted models. How does the new survival accuracy compares to our earlier results? ","20e29cf4":"#### Conclusion:\n\nThe correlation matrix says Gradient Boosting and Random Forest's outputs are not higly correlated with the rest of the estimators'. ","39ed8deb":"Further exploration of titles in the passengers' names I borrowed from https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions.\n\nUsing regex, we'll extract everything before the \".\" in the name which will be our title, and create 'Title' column in both train and test sets. We then combine rare titles into \"rare\" category and group the set by a title to see the survival rate among different titles. Let's explore this on a full train set first."}}