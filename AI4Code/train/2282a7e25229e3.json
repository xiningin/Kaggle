{"cell_type":{"78f01dbb":"code","56ef05f4":"code","89995721":"code","17d4c093":"code","d61c406b":"code","00cf8392":"code","20fefe0c":"code","a25386c4":"code","185e200b":"code","13caed8d":"code","c19f5671":"code","6961d3c2":"markdown"},"source":{"78f01dbb":"!pip install kaggle-environments --upgrade\nfrom kaggle_environments import make, evaluate\nenv = make(\"mab\", debug=True)","56ef05f4":"%%writefile myBestAgentInLB.py\nimport numpy as np\n\ndecay_rate = 0.97\nn_rounds = 2000\nbandit_count = 100\nlast_reward = 0\ntotal_reward = None \nlast_bandit = None\nhis_hits = None\nhis_record = None\nmy_record = None\nmy_hits = None\nwins = None\nlosses = None\nbandits_record = None\nrecord_index = None\nbandit_last_step = None\n\nmax_depth = 2000\nnum_probabs = 1001\nk_decay1 = np.zeros([max_depth, num_probabs], 'float128')\nk_decay0 = np.zeros([max_depth, num_probabs], 'float128')\nprobabs = np.linspace(0, 1, num_probabs)\nfor i in range(k_decay1.shape[0]):\n    k_decay1[i] = probabs * 0.97 ** i\nk_decay0 = 1 - k_decay1\n\ndef estimate_probab(seq):\n    raw_probab1 = k_decay1[:seq.shape[0],:][seq == 1,:]\n    raw_probab0 = k_decay0[:seq.shape[0],:][seq == 0,:]\n    each_probab =  raw_probab0.prod(axis = 0) * raw_probab1.prod(axis = 0)\n    half_area = each_probab.sum()\/ 2\n    cum_sum = np.cumsum(each_probab)\n    best_estimation =  np.argmin(np.abs(cum_sum - half_area))\n    return (probabs[best_estimation]  *.97**seq.shape[0])\n\n\nmyProbabs = None\nhis_bandit_rec = None\nhis_rec_index = None\nn_lookback = 60\nmax_hit = 66\nprobab_ratio = 1.5\nn_decision = 7\nprobab_step = [.47, .40, .35, .31, .26, .24, .21, .175, .15, .13]\navg_proabs = [ .5 * .97**(i* 0.02) for i in range(2000)]\nnu_my_bests = 70\nnu_my_best_best = 10\nmy_bests_index = 0\nchoicePath = None\ndef new_bandit(step):\n    global myProbabs, my_bests_index, choicePath\n    start = step - n_lookback if step >= n_lookback else 0\n    his_last_moves = his_record[start:step]\n    his_last_bandit = his_last_moves[-1]\n    n_his_last_move = (his_last_moves == his_last_bandit).sum()\n    myProbabs[last_bandit] = estimate_probab(bandits_record[last_bandit,:record_index[last_bandit]])\n    myProbabs[his_last_bandit] = estimate_probab(bandits_record[his_last_bandit,: record_index[his_last_bandit]])\n\n    # print(his_last_bandit)\n    # print('band rec',bandits_record[his_last_bandit,: record_index[his_last_bandit]])\n    # print(his_last_moves)\n    # print(n_his_last_move)\n    # print('------- Mine:')\n    # print(last_bandit)\n    # print('band rec:',bandits_record[last_bandit,:record_index[last_bandit]])\n    # print(myProbabs)\n    not_chosen = (my_hits == 0)\n    not_chosen_any = not_chosen.any()\n\n    if step == 1 :  choicePath = {'m1':0, 'm2':0, 'm3':0, 'm4':0,'m5':0, 'm6':0,}\n    # if step == 1999 : print(choicePath)\n    if  his_hits[his_last_bandit] > 1:\n        if my_hits[his_last_bandit] <= 1:\n            choicePath['m1'] += 1\n            return his_last_bandit\n    \n\n    if  not not_chosen_any :\n        if n_his_last_move > 1 and my_hits[his_last_bandit] <= 4:\n            choicePath['m2'] += 1\n            return his_last_bandit\n\n\n    my_bests = np.argsort(myProbabs)[-nu_my_bests:]\n    \n    if his_last_bandit in my_bests:\n        if my_hits[his_last_bandit] > 0:\n            choicePath['m3'] += 1\n            return his_last_bandit\n    #if np.random.rand() > .5 :\n    if not_chosen_any:\n        if ((not_chosen) & (his_hits==0)).any():\n            choicePath['m4'] += 1\n            return np.random.choice(np.where((not_chosen) & (his_hits==0))[0])\n        else:\n            choicePath['m5'] += 1\n            return np.random.choice(np.where(not_chosen)[0])\n\n    # winner = my_bests[-(my_bests_index % nu_my_best_best)-1]\n    #winner = np.random.choice( my_bests[-nu_my_best_best:])\n    my_best_bests = my_bests[-nu_my_best_best:]\n    steps = np.zeros(nu_my_best_best)\n    for i in range( nu_my_best_best):\n        steps[i] = bandit_last_step[my_best_bests[i]]\n    winner = my_best_bests[np.argmin(steps)]\n    choicePath['m6'] += 1\n    return winner\n\ndef agent(obs, conf):\n    global bandits_record, my_record, my_hits, his_hits, his_record, myProbabs, his_scores,last_reward\n    global last_bandit, total_reward,  record_index, wins, losses, his_rec_index, his_bandit_rec,bandit_last_step\n    if obs.step == 0:        \n        total_reward = 0 \n        his_record = np.zeros(n_rounds, 'int')\n        his_hits = np.zeros(conf.banditCount, 'int')\n        his_scores = np.zeros(bandit_count) + 0.5\n        his_bandit_rec = np.ones([bandit_count, 2000], 'int') * -1\n        his_rec_index = np.zeros(bandit_count, 'int')\n        \n        myProbabs = np.random.rand(bandit_count)* 0.001 + 0.5\n        my_record = np.zeros(n_rounds, 'int')\n        my_hits = np.zeros(conf.banditCount, 'int')\n        bandits_record = np.zeros([conf.banditCount, 2000], 'int')\n        record_index = np.zeros(conf.banditCount,'int')\n        wins = np.zeros(conf.banditCount, 'int')\n        losses = np.zeros(conf.banditCount, 'int')\n        bandit_last_step = np.zeros(conf.banditCount, 'int')\n\n        bandit = np.random.randint(conf.banditCount)\n        last_bandit = bandit\n        return bandit\n    if obs.lastActions[0] == last_bandit:\n        his_action = obs.lastActions[1]\n    else:\n        his_action = obs.lastActions[0]\n    his_record[obs.step-1] = his_action\n    his_hits[his_action] += 1\n    his_bandit_rec[his_action,his_rec_index[his_action]] = obs.step\n    his_rec_index[his_action] +=1\n    bandits_record[his_action, record_index[his_action]] = -1\n    record_index[his_action] += 1\n    my_hits[last_bandit] += 1\n    my_record[obs.step-1] = last_bandit\n    if obs.reward > total_reward:\n        total_reward += 1\n        bandits_record[last_bandit, record_index[last_bandit]] = 1\n        wins[last_bandit] += 1\n        last_reward = 1\n    else:\n        bandits_record[last_bandit, record_index[last_bandit]] = 0\n        losses[last_bandit] +=1\n        last_reward =0\n    record_index[last_bandit] += 1\n    bandit = int(new_bandit(obs.step))\n    last_bandit = bandit \n    bandit_last_step[bandit] = obs.step\n    return bandit","89995721":"%%writefile random.py\nimport random\ndef agent(obs, conf):\n    return random.randrange(conf.banditCount)","17d4c093":"%%writefile uniform.py\nimport random\ndef agent(obs, conf):\n    return obs.step % conf.banditCount","d61c406b":"%%writefile ucb_agent.py\nimport math\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] \/ numbers_of_selections[i]\n            delta_i = math.sqrt(2 * math.log(observation.step+1) \/ numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","00cf8392":"%%writefile probabiliticRounds.py\nimport random\n\nimport numpy as np\nsp = np.linspace(0,1,1000)\nspp = 1 - sp\ndecay_rate = 0.97\n\ndef probab(n_ones,n_zeros):\n    global sp, spp\n    cdfBeta = np.cumsum(sp**n_ones * spp **n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[999] \/ 2 ))\n    return sp[place]\ndef probab_with_decay(n_ones,n_zeros,n_opponent):\n    global sp, spp, decay_rate\n    cdfBeta = np.cumsum(sp**n_ones * spp **n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[999] \/ 2 ))\n    return sp[place] * decay_rate**(n_ones + n_zeros + n_opponent)\ndef probab_decayed(n_ones, n_zeros, n_opponent):\n    global sp, spp\n    k_decay = n_ones + n_zeros + n_opponent \n    length = int(decay_rate**k_decay * 1000)\n    cdfBeta = np.cumsum(sp[:length]**n_ones * spp[:length]**n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[length-1] \/ 2 ))\n    return sp[place]\n\nbandit_count = 100\nlast_result =0\ntotal_reward = 0\nlast_bandit = None\nlast_reward = None\nmy_win_record = np.zeros(bandit_count)\nmy_loss_record = np.zeros(bandit_count)\nmy_chances = np.ones(bandit_count) * 0.5\nhis_overall_record = np.zeros(bandit_count)\ntotal_steps = 2000\n\ndef new_bandit(step):\n    global my_chances, my_loss_record, my_win_record, his_overall_record\n    global last_bandit, total_reward, last_result, last_reward, decay_rate\n    progress = step\/total_steps\n    #relative_decay = 1 - (1 - decay_rate) * progress**0.25\n    #decays = decay_rate**(my_win_record + my_loss_record + his_overall_record)\n    scores = my_chances * 1 #* decays\n    #scores[last_bandit] += (last_reward - .1) * ((1- progress)**0.5)\n    scores[last_bandit] += ((last_reward) * ((total_steps - step)\/total_steps)**0.5) \n    winner = np.argmax(scores)\n    return int(winner)\n\ndef agent(obs, conf):\n    global my_chances, my_loss_record, my_win_record, his_overall_record\n    global last_bandit, total_reward, last_result, last_reward\n    if obs.step == 0:        \n        bandit = 0\n        last_bandit = bandit\n        return bandit\n    if obs.lastActions[0] == last_bandit:\n        his_action = obs.lastActions[1]\n    else:\n        his_action = obs.lastActions[0]\n    his_overall_record[his_action] += 1\n            \n    if obs.reward > total_reward:\n        total_reward += 1\n        last_reward = 1\n        my_win_record[last_bandit] += 1                        \n    else:        \n        last_reward = 0\n        my_loss_record[last_bandit] += 1\n    #my_chances[last_bandit] = my_win_record[last_bandit] \/ (my_win_record[last_bandit] + my_loss_record[last_bandit])\n    #my_chances[last_bandit] = probab(my_win_record[last_bandit], my_loss_record[last_bandit])\n    my_chances[last_bandit] = probab_decayed(my_win_record[last_bandit], my_loss_record[last_bandit],his_overall_record[last_bandit])\n    my_chances[his_action] = probab_decayed(my_win_record[his_action], my_loss_record[his_action],his_overall_record[his_action])\n    bandit = new_bandit(obs.step)\n    last_bandit = bandit \n    return bandit","20fefe0c":"%%writefile greedyMemory.py\nimport random\n\nlast_result =0\ntotal_reward = 0\nlast_bandit = None\nmy_win_record = None\nmy_loss_record = None\nmy_chances = None\ndef agent(obs, conf):\n    global my_chances, my_loss_record, my_win_record\n    global last_bandit, total_reward,last_result\n    if obs.step == 0:        \n        my_win_record = [8] * conf.banditCount\n        my_loss_record = [1] * conf.banditCount\n        my_chances = [9] * conf.banditCount\n        bandit = random.randrange(conf.banditCount)\n        last_bandit = bandit\n        return bandit\n    if obs.reward > total_reward:\n        total_reward += 1\n        my_win_record[last_bandit] += 1\n        my_chances[last_bandit] = my_win_record[last_bandit] \/ (my_win_record[last_bandit] + my_loss_record[last_bandit])\n        bandit = last_bandit        \n    else:\n        my_loss_record[last_bandit] += 1\n        my_chances[last_bandit] = my_win_record[last_bandit] \/ (my_win_record[last_bandit] + my_loss_record[last_bandit])\n        bandit = my_chances.index(max(my_chances))        \n        last_bandit = bandit\n    #print(my_chances)\n    #print(bandit)\n    return bandit","a25386c4":"%%writefile recentEvents.py\nimport numpy as np\n\ndecay_rate = 0.97\n\ndef probab(n_ones,n_zeros):\n    global sp, spp\n    cdfBeta = np.cumsum(sp**n_ones * spp **n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[999] \/ 2 ))\n    return sp[place]\n\nbandit_count = 100\ntotal_reward = 0\nlast_bandit = None\nhis_overall_record = np.zeros(bandit_count)\ntotal_steps = 2000\n\nrecord_size = 7\nbandits_record = np.zeros([record_size,bandit_count])\nrecord_index = np.zeros(bandit_count,'int')\nmy_hits_p_1 = np.ones(bandit_count)\n\ndef new_bandit(step):\n    global bandits_record, his_overall_record, last_bandit \n    exploration_score = 1 \/ my_hits_p_1\n    scores = bandits_record.sum(axis=0) + 2.5 * exploration_score\n    winner = np.argmax(scores)    \n    return int(winner)\n\ndef agent(obs, conf):\n    global bandits_record, his_overall_record\n    global last_bandit, total_reward\n    if obs.step == 0:        \n        bandit = np.random.randint(bandit_count)\n        last_bandit = bandit\n        return bandit\n    if obs.lastActions[0] == last_bandit:\n        his_action = obs.lastActions[1]\n    else:\n        his_action = obs.lastActions[0]\n    his_overall_record[his_action] += 1\n    my_hits_p_1[last_bandit] += 1\n    if obs.reward > total_reward:\n        total_reward += 1\n        bandits_record[record_index[last_bandit], last_bandit] = 1        \n    else:\n        bandits_record[record_index[last_bandit], last_bandit] = -1\n    record_index[last_bandit] = (record_index[last_bandit] + 1) % record_size \n    bandit = new_bandit(obs.step)\n    last_bandit = bandit \n    return bandit","185e200b":"%%writefile greedyMemoryWithExpolaration.py\nimport random\n\nlast_result =0\ntotal_reward = 0\nlast_bandit = None\nmy_win_record = None\nmy_loss_record = None\nmy_chances = None\nchances = None\ndef agent(obs, conf):\n    global my_chances, my_loss_record, my_win_record, chances\n    global last_bandit, total_reward,last_result\n    if obs.step == 0 or obs.step ==1:\n        print(obs)\n    if obs.step == 0:        \n        my_win_record = [8] * conf.banditCount\n        my_loss_record = [1] * conf.banditCount\n        my_chances = [8\/9] * conf.banditCount\n        chances = [0] * conf.banditCount\n        bandit = random.randrange(conf.banditCount)\n        last_bandit = bandit\n        return bandit\n    if obs.reward > total_reward:\n        total_reward += 1\n        my_win_record[last_bandit] += 1\n        my_chances[last_bandit] = my_win_record[last_bandit] \/ (my_win_record[last_bandit] + my_loss_record[last_bandit])\n        bandit = last_bandit        \n    else:\n        my_loss_record[last_bandit] += 1\n        my_chances[last_bandit] = my_win_record[last_bandit] \/ (my_win_record[last_bandit] + my_loss_record[last_bandit])\n        for i in range(conf.banditCount):\n            chances[i] = my_chances[i] + 0.3\/(my_win_record[i] + my_loss_record[i] + 1)\n        bandit = chances.index(max(chances))        \n        last_bandit = bandit\n    #print(my_chances)\n    #print(bandit)\n    return bandit","13caed8d":"%%writefile ag_followHim.py\n\nimport numpy as np\n\ndecay_rate = 0.97\nn_rounds = 2000\nbandit_count = 100\n\ntotal_reward = None \nlast_bandit = None\nhis_hits = None\nhis_record = None\nmy_record = None\nmy_hits = None\nwins = None\nlosses = None\nbandits_record = None\nrecord_index = None\nx1 = None\nx2 = None\n\nsp = np.linspace(0,1,1000)\nspp = 1 - sp\ndef probab(n_ones,n_zeros):\n    global sp, spp\n    cdfBeta = np.cumsum(sp**n_ones * spp **n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[999] \/ 2 ))\n    return sp[place] + np.random.rand() * 0.0001\n\ndef decayed_probab(n_ones, n_zeros, his_n):\n    global sp, spp\n    ps = sp**n_ones * spp **n_zeros\n    limit = int(1000 * decay_rate**(his_n + n_ones + n_zeros + 1 ) )+1\n    cdfBeta = np.cumsum(ps[:limit] )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[-1] \/ 2 ))\n    return sp[place] + np.random.rand() * 0.0001\n\nmyProbabs = None\nhis_scores = None\nhis_bandit_rec = None\nhis_rec_index = None\ndef new_bandit(step):\n    global myProbabs\n    n_lookback = 49\n    start = step - n_lookback if step >= 49 else 0\n    his_last_moves = his_record[start:step]\n    his_last_bandit = his_last_moves[-1]\n    n_last_move = (his_last_moves == his_last_bandit).sum()\n    \n    myProbabs[last_bandit] = decayed_probab(wins[last_bandit],losses[last_bandit],his_hits[last_bandit])\n    myProbabs[his_last_bandit] = decayed_probab(wins[his_last_bandit],losses[his_last_bandit],his_hits[his_last_bandit])\n    \n    if n_last_move > 1 and my_hits[his_last_bandit] <= 2:\n        return his_last_bandit\n    \n    if n_last_move == 1 and his_hits[his_last_bandit] > 1 :\n        myProbabs[his_last_bandit] -= .25\n    \n\n\n    exploratio_score = np.zeros(bandit_count)\n    exploratio_score[his_hits == 0] = -0.25\n    exploratio_score[his_hits == 1] = -0.5\n    exploratio_score += 1 \/(my_hits + 1)\n    exploratio_score[my_hits>45] = -2\n    scores = myProbabs + 0.1 * exploratio_score\n    winner = int(np.argmax(scores))\n    return winner\n\ndef agent(obs, conf):\n    global bandits_record, my_record, my_hits, his_hits, his_record, myProbabs, his_scores\n    global last_bandit, total_reward,  record_index, wins, losses, his_rec_index, his_bandit_rec\n    if obs.step == 0:        \n        total_reward = 0 \n        his_record = np.zeros(n_rounds, 'int')\n        his_hits = np.zeros(conf.banditCount, 'int')\n        his_scores = np.zeros(bandit_count) + 0.5\n        his_bandit_rec = np.ones([bandit_count, 600], 'int') * -1\n        his_rec_index = np.zeros(bandit_count, 'int')\n        \n        myProbabs = np.random.rand(bandit_count)* 0.001 + 0.5\n        my_record = np.zeros(n_rounds, 'int')\n        my_hits = np.zeros(conf.banditCount, 'int')\n        bandits_record = np.zeros([conf.banditCount, 600], 'int')\n        record_index = np.zeros(conf.banditCount,'int')\n        wins = np.zeros(conf.banditCount, 'int')\n        losses = np.zeros(conf.banditCount, 'int')\n\n        bandit = np.random.randint(conf.banditCount)\n        last_bandit = bandit\n        return bandit\n    if obs.lastActions[0] == last_bandit:\n        his_action = obs.lastActions[1]\n    else:\n        his_action = obs.lastActions[0]\n    his_record[obs.step-1] = his_action\n    his_hits[his_action] += 1\n    his_bandit_rec[his_action,his_rec_index[his_action]] = obs.step\n    his_rec_index[his_action] +=1\n    my_hits[last_bandit] += 1\n    my_record[obs.step-1] = last_bandit\n    if obs.reward > total_reward:\n        total_reward += 1\n        bandits_record[last_bandit, record_index[last_bandit]] = 1\n        wins[last_bandit] += 1\n    else:\n        bandits_record[last_bandit, record_index[last_bandit]] = 0\n        losses[last_bandit] +=1\n    record_index[last_bandit] += 1\n    bandit = int(new_bandit(obs.step))\n    last_bandit = bandit \n    return bandit\n","c19f5671":"agent1 = \"myBestAgentInLB.py\"\nn_rounds = 10\nagents = [\"uniform.py\", \"random.py\", \"ucb_agent.py\", \"greedyMemory.py\", \"probabiliticRounds.py\", \"recentEvents.py\", 'ag_followHim.py' ]\n\nwins1 = [0] * len(agents)\nwins2 = [0] * len(agents)\nsum1 = [0] * len(agents)\nsum2 = [0] * len(agents)\nfor agent in agents:\n    print(\"\\t\", agent ,end='')\nprint()\nfor i in range(n_rounds):\n    print(i+1, \":\\t\\t\" ,end='')\n    for j, agent in enumerate(agents):\n        mygame = env.run([agent1, agent])\n        #print(agent1, ':' , mygame[1999][0].observation.reward, end=\" \")\n        #print(agent2, ':' , mygame[1999][1].observation.reward, ,end='\\t')\n        #print(mygame[0][0].observation)\n        #print(mygame[1][0].observation)\n        sum1[j] += mygame[1999][0].observation.reward\n        sum2[j] += mygame[1999][1].observation.reward\n        if mygame[1999][0].observation.reward > mygame[1999][1].observation.reward:\n            wins1[j] +=1\n        if mygame[1999][0].observation.reward < mygame[1999][1].observation.reward:\n            wins2[j] +=1\n        print(wins1[j],wins2[j], end='\\t\\t')\n    print()\n\nfor j, agent in enumerate(agents):\n    print(f'{agent:<22}',wins1[j],wins2[j] ,sum1[j]\/ n_rounds ,sum2[j] \/ n_rounds, sum1[j]\/sum2[j], sep=\"\\t\")\n","6961d3c2":"### The second cell of this notebook contains the code of my best agent in the leaderboard(1445.6) and rest cells are just some other of my agents and the code to compare them.\n### I have explained the strategy [here in the forum](https:\/\/www.kaggle.com\/c\/santa-2020\/discussion\/217537).\n\n### If you find it useful upvote is appreciated!"}}