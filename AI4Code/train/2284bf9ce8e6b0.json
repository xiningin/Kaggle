{"cell_type":{"79bff257":"code","77295900":"code","e3756d96":"code","e935b60f":"code","2338895c":"code","b5b09d44":"code","79bd859d":"code","3f52ba68":"code","7895f1c0":"code","06ac235f":"code","54e0135e":"code","3735d511":"code","26f78e3b":"code","94d2c5d8":"code","ae61f7fe":"code","0a35cb67":"code","487bd757":"code","4bb69984":"code","c80ac088":"code","54123660":"code","8af0437e":"code","d71a3410":"code","2b7d8320":"code","a63cb6ab":"code","17703a5b":"code","32e4d18f":"code","f704e98e":"code","e5d67640":"code","136bd01d":"code","10efbc87":"code","d57908dd":"code","8bd124fa":"code","e0f48b07":"code","00d25069":"code","4d9a8c2a":"code","254f8044":"code","513bffa5":"code","25b55e7d":"code","2fc398ae":"code","7944e1c2":"markdown","4350ba9b":"markdown","242d7cf9":"markdown","89d8e64f":"markdown","37dc6248":"markdown","aead80e6":"markdown","9464af5d":"markdown","239cf84d":"markdown","78582148":"markdown","16fd7d1a":"markdown","d7b6f1eb":"markdown","85c503d5":"markdown","e61d732f":"markdown","35e148c8":"markdown","f43f3cf5":"markdown","73e8b0d2":"markdown","54f10f58":"markdown","03ad58b7":"markdown","f5ad8fa3":"markdown","80d67ff4":"markdown","91faaf4d":"markdown","ef05d67e":"markdown","02aab07d":"markdown","aef57409":"markdown","d906ba84":"markdown","bb8a479e":"markdown"},"source":{"79bff257":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77295900":"df = pd.read_csv('\/kaggle\/input\/santander-value-prediction-challenge\/train.csv')\ndf.head()","e3756d96":"df.set_index(\"ID\", inplace = True)","e935b60f":"df.head()","2338895c":"print(df.shape)","b5b09d44":"df = df[(df.T != 0).any()]\ndf.shape","79bd859d":"df = df.loc[:,~df.columns.duplicated()]\ndf.shape","3f52ba68":"train_features, test_features, train_labels, test_labels=train_test_split(df.drop(labels=['target'], axis=1),df['target'],test_size=0.2,\n    random_state=41)","7895f1c0":"constant_filter = VarianceThreshold(threshold=0)\nconstant_filter.fit(train_features)","06ac235f":"len(train_features.columns[constant_filter.get_support()])","54e0135e":"train_features = constant_filter.transform(train_features)\ntest_features = constant_filter.transform(test_features)\n\ntrain_features.shape, test_features.shape","3735d511":"train_features","26f78e3b":"test_df = pd.read_csv('\/kaggle\/input\/santander-value-prediction-challenge\/test.csv')","94d2c5d8":"test_df.shape\ntest_df = constant_filter.transform(test_df)","ae61f7fe":"test_df.shape()","0a35cb67":"X = df.drop(columns = \"target\")\ny = df['target']\nnum_colums = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_columns = list(X.select_dtypes(include=num_colums).columns)\nX = X[numerical_columns]\nX.shape","487bd757":"correlated_features = set()\ncorrelation_matrix = X.corr()","4bb69984":"for i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\nX.drop(labels=correlated_features, axis=1, inplace=True)\nX.shape","c80ac088":"import seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.lineplot(data=df ,x=range(df['target'].shape[0]), y=np.sort(df[\"target\"].values))","54123660":"ax = sns.distplot(df['target'])","8af0437e":"(df['target']).skew()","d71a3410":"ax = sns.distplot(np.log1p(df['target']))","2b7d8320":"(np.log1p(df['target']).skew())","a63cb6ab":"print(np.log1p(df['target']).value_counts())","17703a5b":"df['target'] = np.log1p(df['target'])","32e4d18f":"y =  (df['target'])","f704e98e":"#apply SelectKBest class to extract top 10 best features\n#bestfeatures = SelectKBest(score_func=chi2, k=10)\n","e5d67640":"#fit = bestfeatures.fit(X,y)","136bd01d":"from sklearn.decomposition import PCA\n# Make an instance of the Model\npca = PCA(.75)\npca.fit(X)\ndf_pca_dropped = pca.transform(X)","10efbc87":"X.shape","d57908dd":"df_pca_dropped.shape","8bd124fa":"df_pca_dropped = pd.DataFrame(df_pca_dropped)","e0f48b07":"from sklearn.ensemble import GradientBoostingRegressor\n","00d25069":"y = y\nX = df_pca_dropped\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","4d9a8c2a":"model = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\nmodel.score(X_train,y_train)","254f8044":"y_pred = model.predict(X_test)","513bffa5":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","25b55e7d":"from sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=2, random_state=0)\nregr.fit(X_train, y_train)\ny_pred = regr.predict(X_test)\nprint(r2_score(y_test,y_pred))","2fc398ae":"X_train.head()","7944e1c2":"### Task 2.2) Studying Distribution of Target Variable ","4350ba9b":"# TASK 5 -> reducing variables using correlation","242d7cf9":"### Task 1.2 ) Removing duplicate columns","89d8e64f":"# Univariate Selection\n","37dc6248":"# As we have a large number of columns , i.e 4993 .. Let's go forward and check the entire datashape , so that we get an understanding of what is our n\/p ratio","aead80e6":"### Task 1.4 ) Removing variable which have correlation to other predictor variables","9464af5d":"## TASK 2 - Studying Data Distribution & Outliers - \n\nThanks to an amazing kernal ->https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-santander-value ,  navigating this section was really easy","239cf84d":"# Let's give the dataset a small EDA , before we proceed to building a model and submitting the solution ","78582148":"### Task 1.1 ) Removing columns with less than no values as non - zero","16fd7d1a":"# There are some algorithms like lasso , decision trees which themselves choose b\/w the large amount of variables and use the most suitable ones , let's go forward and train them directly on the initial full data","d7b6f1eb":"# THE TEST DATA WITH 49K VALUES ALSO HAS TO GO THROUGH THE SAME PROCESS , SO I WILL DEAL WITH IT HERE AND NOW","85c503d5":"# The rows = 4459  (N)\n# The columns = 4993 (P)\n\nThe N\/P < 1 .. so from my understanding we can't fit a very flexible(complex) model to the data , as it would go forward and overfit by great margins , as the provided data is very very small.\n\nOr we need to find a way to drop a large number or columns (predictors) before throwing a fancy algorithm at it.","e61d732f":"### Usually I tend to take the following steps\n\n1) Finding the  n\/p ratio and deciding how to move ahead , \n         n -> no of observations\n         p -> no of predictors\n         \n      1.1) Removing rows with all 0's\n      1.2) Removing duplicate columns\n      1.3) Removing columns with constant value\n      1.4) Removing correlated variables\n         \n         \n2) Seeing the distribution of Data \/ Detecting Outliers \n\n\n3) Mean \/ Meadin \/ Mode of Target Variable\n\n4) Handling Missing Values\n\n5) Correlation","35e148c8":"There are no such columns , with all duplicate values","f43f3cf5":" * 1) Univariate Selection\n * 2) Feature Importance\n * 3) Correlation Heatmap\n * 4) PCA","73e8b0d2":"# Dropping columns using PCA gave us about r sqaure of 0.20 (GBM) AND 0.18(Random Forest) looks like PCA is dropping out on a lot of information:(","54f10f58":"### More exploration of the target variable","03ad58b7":"There are no such columns , with all zeroes","f5ad8fa3":"# There will be various feature selection processes , I would experiment with in the task-5 Section , I'll also train a default Gradient Boosting Regression Model for checking the quality of each method's progess . Creating good quality function is key here .","80d67ff4":"There are no visible outliers","91faaf4d":"### Usually I tend to take the following steps\n\n1) Finding the  n\/p ratio and deciding how to move ahead , \n         n -> no of observations\n         p -> no of predictors\n         \n2) Seeing the distribution of Data \/ Detecting Outliers \n\n3) Mean \/ Meadin \/ Mode of Target Variable\n\n4) Handling Missing Values\n\n5) Correlation","ef05d67e":"## The skew went down from 2 to -0.4","02aab07d":"### Task 1.3 ) Removing constant value columns , or its better to call them features with 0 variance","aef57409":"\n### Task 2.1) Detecting Outliers","d906ba84":"Task 2.1) Detecting Outliers\n\nTask 2.2) Studying Distribution Of the Data\n\nTask 2.3) Fixing the target data\n\nTask 2.4) Studying Mean Median Mode of new target","bb8a479e":"We removed many columns , precisely (256) columns were removed . But still the no or predictors are more than the number of rows or the number of observations "}}