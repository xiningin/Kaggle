{"cell_type":{"62733c41":"code","667b83f6":"code","a6615340":"code","5165e73f":"code","92bf9669":"code","3f8f67b6":"code","22b5ec61":"code","146cab52":"code","2bd58f0a":"code","d8153ec0":"code","102c1179":"code","57d20dcc":"code","4e52a776":"code","63f0f6a1":"code","d1aea77f":"code","ad47c598":"code","e8ac2740":"code","9e5c558b":"code","49ea87da":"code","1e2fd14b":"code","7eecfc59":"code","ed2e41be":"code","b7c72dc4":"code","c760dd79":"code","de6c4b5c":"code","2ee12fac":"code","eb00bac5":"code","ebe9fd0e":"code","34e92f91":"code","e61b450f":"code","971ce6b4":"markdown","d76bfc18":"markdown","6c244448":"markdown","c8de6a48":"markdown","0386e20b":"markdown","85613038":"markdown"},"source":{"62733c41":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport spacy\nimport shap\nfrom sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nfrom lightgbm import LGBMClassifier\nimport pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()","667b83f6":"train = pd.read_csv(\"..\/input\/ykc-2nd\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ykc-2nd\/test.csv\")\nsub = pd.read_csv(\"..\/input\/ykc-2nd\/sample_submission.csv\")\ntrain.shape, test.shape, sub.shape","a6615340":"train.head()","5165e73f":"test.head()","92bf9669":"train.describe()","3f8f67b6":"test.describe()","22b5ec61":"x = train[\"department_id\"]\nlen(x.unique()) # \u58f2\u308a\u5834\u306f0-20\u3067\u300121\u5206\u985e","146cab52":"# \u524d\u51e6\u7406\nx = train[\"product_name\"]\nx = x.apply(lambda words : words.lower().replace(\",\", \"\").replace(\"&\", \"\").split(\" \"))\nx = x.apply(lambda words : list(filter(lambda word: word != \"\", words))) # 'Pizza for One Suprema  Frozen Pizza' \u306e\u3088\u3046\u306b\u7a7a\u767d\u304c2\u3064\u91cd\u306a\u308b\u30b1\u30fc\u30b9\u3092\u9664\u53bb","2bd58f0a":"import collections\n\ndef flatten(l):\n    for el in l:\n        if isinstance(el, collections.abc.Iterable) and not isinstance(el, (str, bytes)):\n            yield from flatten(el)\n        else:\n            yield el\n\nwords = list(flatten(x))","d8153ec0":"# word\u306e\u51fa\u73fe\u56de\u6570\nwords_df = pd.DataFrame(nltk.FreqDist(words).most_common())\nwords_df.columns = ['keyword', 'count']","102c1179":"words_df.shape","57d20dcc":"# Top100\u3092\u8868\u793a\nwords_df = words_df.head(100)\nfig, axes = plt.subplots(ncols=1, figsize=(8, 20), dpi=100)\nsns.barplot(y=words_df['keyword'], x=words_df['count'])","4e52a776":"# length\u3092\u53d6\u5f97\nword_length = [len(w) for w in x]\nsns.countplot(word_length, color='blue')","63f0f6a1":"# https:\/\/spacy.io\/\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"this is a sentence.\")\nprint(doc.vector, doc.vector.shape)","d1aea77f":"# t-SNE\nx = train[\"product_name\"]\ny = train[\"department_id\"]\n\nx = x.apply(lambda words : words.lower().replace(\",\", \"\").replace(\"&\", \"\"))\n\nvecs = x.apply(lambda text: nlp(text).vector)\nvec_df = pd.DataFrame(list(vecs))\n\nsvd = TSNE(n_components=2).fit_transform(vec_df)\n\nfig, axes = plt.subplots(ncols=1, figsize=(12, 12))\nsns.scatterplot(x=svd[:, 0], y=svd[:, 1], alpha=0.8, hue=y, palette=\"RdBu_r\", legend=\"full\")","ad47c598":"# preprocessing\nx = train[\"product_name\"]\n\ntf_vectorizer = CountVectorizer(stop_words='english', \n                                         max_features=40000, \n                                         lowercase = True,\n                                         max_df = 0.8,\n                                         min_df = 10)\ndtm_tf = tf_vectorizer.fit_transform(x)\ndtm_tf.shape","e8ac2740":"n_topics = 8\nlda_tf = LatentDirichletAllocation(n_components=n_topics)\nlda_tf.fit(dtm_tf)","9e5c558b":"pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)","49ea87da":"n_topics = 21\nlda_tf = LatentDirichletAllocation(n_components=n_topics)\nlda_tf.fit(dtm_tf)","1e2fd14b":"pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)","7eecfc59":"train[\"svd_0\"] = svd[:, 0]\ntrain[\"svd_1\"] = svd[:, 1]","ed2e41be":"train.head()","b7c72dc4":"X = train[[\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\", \"svd_0\", \"svd_1\"]]\ny = train[\"department_id\"]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","c760dd79":"model = LGBMClassifier()\nmodel.fit(train_X, train_y)\n\nr = permutation_importance(model, val_X, val_y,\n                            n_repeats=30,\n                            random_state=0)","de6c4b5c":"sorted_idx = r.importances_mean.argsort()\nfig, ax = plt.subplots(figsize=(10, 4))\nax.boxplot(r.importances[sorted_idx].T,\n           vert=False, labels=val_X.columns[sorted_idx])\nax.set_title(\"Permutation Importances\")\nfig.tight_layout()\nplt.show()","2ee12fac":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(val_X)","eb00bac5":"shap.summary_plot(shap_values, val_X, plot_type=\"bar\")","ebe9fd0e":"shap.summary_plot(shap_values[0], val_X) # \u30af\u30e9\u30b90\u306b\u5bfe\u3059\u308b\u6563\u5e03\u56f3","34e92f91":"# \u30af\u30e9\u30b90\u306b\u304a\u3051\u308bshap value\u3068'svd_1'\u306e\u95a2\u4fc2\nshap.dependence_plot(\"svd_1\", shap_values[0], val_X)","e61b450f":"shap.dependence_plot(\"order_rate\", shap_values[0], val_X)","971ce6b4":"EDA","d76bfc18":"### shap value\nhttps:\/\/github.com\/slundberg\/shap","6c244448":"### Permutation Importance","c8de6a48":"### product_name","0386e20b":"### target","85613038":"### Topic Modelling\nhttps:\/\/github.com\/bmabey\/pyLDAvis"}}