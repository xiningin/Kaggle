{"cell_type":{"528d357e":"code","46546b1d":"code","a415001f":"code","66d37e68":"code","a0ca7110":"code","6e4ca7c9":"code","72c1e778":"code","7f4c1445":"code","f3162f6f":"code","80f69b46":"code","671a4f8a":"code","ab8e8e3a":"code","d1d8c2d8":"code","54ef15fc":"code","961105a9":"code","a86c40b9":"code","de7f51a0":"code","b518f841":"code","44ea6bdd":"code","322d9b4c":"code","cd489252":"code","5caa8d8c":"code","23e36cd2":"code","20b9c512":"code","e8ff02bc":"code","d0bf604b":"code","5aab27d0":"code","2c6f2d1e":"code","c827a083":"code","549400fb":"code","6eda295d":"code","ac0dce90":"code","ef086fb4":"code","88e6be94":"code","73639973":"code","e400452d":"code","089ec5bf":"markdown","1486487f":"markdown","bcf3d6ac":"markdown","66ea9a0d":"markdown","e1a4f2aa":"markdown","beea4bec":"markdown","13e4a75b":"markdown","51f4b808":"markdown","4fd0c886":"markdown","fa716a1e":"markdown","1191c6ef":"markdown","35a2d533":"markdown","11937274":"markdown","b88eef88":"markdown","447b1647":"markdown","37661889":"markdown","69cc18f3":"markdown","df45cf8a":"markdown","575e5aa9":"markdown","8c0954e1":"markdown","db76d421":"markdown","e1b5a454":"markdown","f8eab94d":"markdown","cf4fe19b":"markdown","202a7270":"markdown","342f512d":"markdown","617d0727":"markdown","2acd61fc":"markdown","9b79fd53":"markdown","49175ebc":"markdown","229ffb35":"markdown","b87885c6":"markdown","69b05cf9":"markdown","49425582":"markdown","22eb8a41":"markdown","c54359e6":"markdown","cdc9fb9c":"markdown","0d699ffb":"markdown","c6980acd":"markdown","427065fd":"markdown","8d4d8854":"markdown","9751e805":"markdown","45814154":"markdown","57761727":"markdown","afe92c86":"markdown","1553649a":"markdown","3eee5166":"markdown","316e7d9d":"markdown","2d0903bc":"markdown","9eb23529":"markdown","53d20a55":"markdown","76f3bebe":"markdown","67b5ca4f":"markdown","41694632":"markdown","ea1e0306":"markdown","e67e22af":"markdown","6fd0a7b9":"markdown","2309942d":"markdown","c00df07b":"markdown"},"source":{"528d357e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport warnings\n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' # Set to retina version\npd.set_option('display.max_columns', None) # Set max columns output\nwarnings.filterwarnings('ignore')","46546b1d":"df = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nprint(df.shape)\ndisplay(df.head())","a415001f":"df = df.drop(columns=['Over18', 'EmployeeCount', 'StandardHours', 'EmployeeNumber'])","66d37e68":"education_map = {1: 'Below College', 2: 'College', 3: 'Bachelor', 4: 'Master', 5: 'Doctor'}\neducation_satisfaction_map = {1: 'Low', 2:'Medium', 3:'High', 4:'Very High'}\njob_involvement_map = {1: 'Low', 2:'Medium', 3:'High', 4:'Very High'}\njob_satisfaction_map = {1: 'Low', 2:'Medium', 3:'High', 4:'Very High'}\nperformance_rating_map = {1: 'Low', 2: 'Good', 3: 'Excellent', 4: 'Outstanding'}\nrelationship_satisfaction_map = {1: 'Low', 2:'Medium', 3:'High', 4:'Very High'}\nwork_life_balance_map = {1: 'Bad', 2: 'Good', 3: 'Better', 4: 'Best'}\n# Use the pandas apply method to numerically encode our attrition target variable\ndf['Education'] = df[\"Education\"].apply(lambda x: education_map[x])\ndf['EnvironmentSatisfaction'] = df[\"EnvironmentSatisfaction\"].apply(lambda x: education_satisfaction_map[x])\ndf['JobInvolvement'] = df[\"JobInvolvement\"].apply(lambda x: job_involvement_map[x])\ndf['JobSatisfaction'] = df[\"JobSatisfaction\"].apply(lambda x: job_satisfaction_map[x])\ndf['PerformanceRating'] = df[\"PerformanceRating\"].apply(lambda x: performance_rating_map[x])\ndf['RelationshipSatisfaction'] = df[\"RelationshipSatisfaction\"].apply(lambda x: relationship_satisfaction_map[x])\ndf['WorkLifeBalance'] = df[\"WorkLifeBalance\"].apply(lambda x: work_life_balance_map[x])","a0ca7110":"display(df.head())","6e4ca7c9":"print(\"Missing Value:\", df.isnull().any().any())","72c1e778":"colors = ['#66b3ff', '#ff9999']\nexplode = (0.05,0.05)\nplt.figure(figsize=(5, 5))\nplt.pie(df['Attrition'].value_counts(), colors = colors, labels=['No', 'Yes'], \n        autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\nplt.legend()\nplt.title(\"Attrition (Target) Distribution\")\nplt.show()","7f4c1445":"numerical_list = ['Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate',\n                  'NumCompaniesWorked', 'PercentSalaryHike', 'TotalWorkingYears', 'TrainingTimesLastYear',\n                  'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n\nplt.figure(figsize=(10, 10))\nfor i, column in enumerate(numerical_list, 1):\n    plt.subplot(5, 3, i)\n    sns.distplot(df[column], bins=20)\nplt.tight_layout()\nplt.show()","f3162f6f":"cate_list = ['Attrition', 'BusinessTravel', 'Department', 'Education', 'EducationField', \n             'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobRole',\n             'JobSatisfaction', 'MaritalStatus', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction',\n             'StockOptionLevel', 'WorkLifeBalance']\n\nfor i in cate_list:\n    df[i] = df[i].astype(object)\n    \nplt.figure(figsize=(20, 20))\ngridspec.GridSpec(7, 3)\nlocator1, locator2 = [0, 0]\n\nfor column in cate_list:\n    if column == 'JobRole':\n        plt.subplot2grid((7, 3), (locator1, locator2), colspan=3, rowspan=1)\n        sns.countplot(df[column], palette='Set2')\n        locator1 += 1\n        locator2 = 0\n        continue\n    plt.subplot2grid((7, 3), (locator1, locator2))\n    sns.countplot(df[column], palette='Set2')\n    locator2 += 1\n    if locator2 == 3:\n        locator1 += 1\n        locator2 = 0\n        continue\n    if locator1 == 7:\n        break\n        \nplt.tight_layout()\nplt.show()","80f69b46":"plt.figure(figsize=(20, 20))\nsns.heatmap(df.corr(), annot=True, cmap=\"Greys\", annot_kws={\"size\":15})\nplt.show()","671a4f8a":"plt.figure(figsize=(10, 10))\nfor i, column in enumerate(numerical_list, 1):\n    plt.subplot(5, 3, i)\n    sns.violinplot(data=df, x=column, y='Attrition')\nplt.tight_layout()\nplt.show()","ab8e8e3a":"plt.figure(figsize=(20, 20))\ngridspec.GridSpec(7, 3)\nlocator1, locator2 = [0, 0]\nfor column in cate_list:\n    if column == 'JobRole':\n        plt.subplot2grid((7, 3), (locator1, locator2), colspan=3, rowspan=1)\n        sns.countplot(x=column, hue='Attrition', data=df, palette='BrBG')\n        locator1 += 1\n        locator2 = 0\n        continue\n    plt.subplot2grid((7, 3), (locator1, locator2))\n    sns.countplot(x=column, hue='Attrition', data=df, palette='BrBG')\n    locator2 += 1\n    if locator2 == 3:\n        locator1 += 1\n        locator2 = 0\n        continue\n    if locator1 == 7:\n        break\nplt.tight_layout()\nplt.show()","d1d8c2d8":"from sklearn import preprocessing\nfrom IPython.display import Image\n# Reload the data\ndf = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf = df.drop(columns=['Over18', 'EmployeeCount', 'StandardHours', 'EmployeeNumber'])","54ef15fc":"for cate_features in df.select_dtypes(include='object').columns:\n    le = preprocessing.LabelEncoder()\n    df[cate_features] = le.fit_transform(df[cate_features])\n    print(\"Origin Classes:\", list(le.classes_))","961105a9":"dummies = ['Department', 'EducationField', 'JobRole', 'MaritalStatus']\ndf = pd.get_dummies(data=df, columns=dummies)\ndisplay(df.head())","a86c40b9":"std = preprocessing.StandardScaler()\nscaled = std.fit_transform(df[numerical_list])\nscaled = pd.DataFrame(scaled, columns=numerical_list)\nfor i in numerical_list:\n    df[i] = scaled[i]\ndisplay(df.head())","de7f51a0":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\nfrom subprocess import call\nfrom IPython.display import Image \nfrom imblearn.over_sampling import SMOTE","b518f841":"def my_confusion_matrix(test, test_pred):\n    cf = pd.DataFrame(confusion_matrix(test, test_pred), \n                      columns=['Predicted NO', 'Predicted Yes'], \n                      index=['True No', 'True Yes'])\n    report = pd.DataFrame(classification_report(test, test_pred, target_names=['No', 'Yes'], \n                                                        output_dict=True)).round(2).transpose()\n    display(cf)\n    display(report)","44ea6bdd":"def plot_roc_curve(model, y, x):\n    tree_auc = roc_auc_score(y, model.predict(x))\n    fpr, tpr, thresholds = roc_curve(y, model.predict_proba(x)[:,1])\n    plt.figure(figsize=(15, 10))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='Decision Tree ROC curve (area = %0.2f)' % tree_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.fill_between(fpr, tpr, color='orange', alpha=0.2)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")","322d9b4c":"X = df.drop(columns=['Attrition'])\ny = df['Attrition']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\noversampler = SMOTE(random_state=0)\nsmote_X_train, smote_y_train = oversampler.fit_sample(X_train, y_train)","cd489252":"colors = ['#66b3ff', '#ff9999']\nexplode = (0.05,0.05)\nplt.figure(figsize=(5, 5))\nplt.pie(pd.Series(smote_y_train).value_counts(), colors = colors, labels=['No', 'Yes'], \n        autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\nplt.legend()\nplt.title(\"Oversampled Targets in Training Set\")\nplt.show()","5caa8d8c":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier","23e36cd2":"params = {\"criterion\": (\"gini\", \"entropy\"), \n          \"splitter\": (\"best\", \"random\"), \n          \"max_depth\": np.arange(1, 20), \n          \"min_samples_split\": [2, 3, 4], \n          \"min_samples_leaf\": np.arange(1, 20)}\ntree1_grid = GridSearchCV(DecisionTreeClassifier(random_state=0), params, scoring=\"roc_auc\", n_jobs=-1, cv=5)\ntree1_grid.fit(X_train, y_train)","20b9c512":"print(tree1_grid.best_score_)\nprint(tree1_grid.best_params_)\nprint(tree1_grid.best_estimator_)","e8ff02bc":"tree1_clf = DecisionTreeClassifier(random_state=0, **tree1_grid.best_params_)\ntree1_clf.fit(X_train, y_train)\ntree.export_graphviz(tree1_clf, out_file='\/kaggle\/working\/tree1.dot', special_characters=True, rounded = True, filled= True,\n                     feature_names=X.columns, class_names=['Yes', 'No'])\ncall(['dot', '-T', 'png', '\/kaggle\/working\/tree1.dot', '-o', '\/kaggle\/working\/tree1.png'])\ndisplay(Image(\"\/kaggle\/working\/tree1.png\", height=2000, width=1900))","d0bf604b":"y_test_pred_tree1 = tree1_clf.predict(X_test)\nmy_confusion_matrix(y_test, y_test_pred_tree1) # Defined before\ntree1_auc = roc_auc_score(y_test, y_test_pred_tree1)\nprint(\"AUC:\", tree1_auc)","5aab27d0":"IP = pd.DataFrame({\"Features\": np.array(X.columns), \"Importance\": tree1_clf.feature_importances_})\nIP = IP.sort_values(by=['Importance'], ascending=False)\nplt.figure(figsize=(15, 10))\nsns.barplot(x='Importance', y='Features', data=IP[:10])\nplt.show()","2c6f2d1e":"plot_roc_curve(tree1_clf, y_test, X_test)\nplt.show()","c827a083":"tree2_grid = GridSearchCV(DecisionTreeClassifier(random_state=0), params, scoring=\"roc_auc\", n_jobs=-1, cv=5)\ntree2_grid.fit(smote_X_train, smote_y_train)","549400fb":"print(tree2_grid.best_score_)\nprint(tree2_grid.best_params_)\nprint(tree2_grid.best_estimator_)","6eda295d":"tree2_clf = DecisionTreeClassifier(random_state=65, **tree2_grid.best_params_)\ntree2_clf.fit(smote_X_train, smote_y_train)\ntree.export_graphviz(tree2_clf, out_file='\/kaggle\/working\/tree2.dot', special_characters=True, rounded = True, filled= True,\n                     feature_names=X.columns, class_names=['Yes', 'No'])\ncall(['dot', '-T', 'png', 'tree.dot', '-o', '\/kaggle\/working\/tree2.png'])\ndisplay(Image(\"\/kaggle\/working\/tree2.png\", height=2000, width=1900))","ac0dce90":"y_test_pred_tree2 = tree2_clf.predict(X_test)\nmy_confusion_matrix(y_test, y_test_pred_tree2)\ntree2_auc = roc_auc_score(y_test, y_test_pred_tree2)\nprint(\"AUC:\", tree2_auc)","ef086fb4":"plot_roc_curve(tree2_clf, y_test, X_test)\nplt.show()","88e6be94":"IP = pd.DataFrame({\"Features\": np.array(X.columns), \"Importance\": tree2_clf.feature_importances_})\nIP = IP.sort_values(by=['Importance'], ascending=False)\nplt.figure(figsize=(15, 10))\nsns.barplot(x='Importance', y='Features', data=IP[:10])\nplt.show()","73639973":"tree1_fpr, tree1_tpr, tree1_thresholds = roc_curve(y_test, tree1_clf.predict_proba(X_test)[:,1])\ntree2_fpr, tree2_tpr, tree2_thresholds = roc_curve(y_test, tree2_clf.predict_proba(X_test)[:,1])\nplt.figure(figsize=(15, 10))\nplt.plot(tree1_fpr, tree1_tpr, color='skyblue', lw=2, label='Decision Tree ROC curve (area = %0.2f)' % tree1_auc)\nplt.plot(tree2_fpr, tree2_tpr, color='darkorange', lw=2, label='Decision Tree (with oversampling) ROC curve (area = %0.2f)' % tree2_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.fill_between(tree2_fpr, tree2_tpr, color='darkorange', alpha=0.2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (Without Oversampling)')\nplt.legend(loc=\"lower right\")\nplt.show()","e400452d":"report1 = pd.DataFrame(classification_report(y_test, y_test_pred_tree1, target_names=['No', 'Yes'], output_dict=True)).round(2).transpose()\nreport2 = pd.DataFrame(classification_report(y_test, y_test_pred_tree2, target_names=['No', 'Yes'], output_dict=True)).round(2).transpose()\nevaluation = pd.DataFrame([{'Method': 'Decision Tree (without oversample)', 'F1': report1['f1-score'][1], 'Precision': report1['precision'][1], 'Recall': report1['recall'][1], 'AUC': tree1_auc}, \n                           {'Method': 'Decision Tree (with oversample)', 'F1': report2['f1-score'][1], 'Precision': report2['precision'][1], 'Recall': report2['recall'][1], 'AUC': tree2_auc}])\ndisplay(evaluation)","089ec5bf":"#### Findings of Cross Analysis between Attrition and Categorical Features\n\n> By comparing between different level among the employees attrition.\n\n- Those who has business travel are more likely leave the company.\n- Employees of Human Resource are the most stable group of employees.\n- Employees who have Doctor degree are stable.\n- Enviroment satisfaction is not related a lot to attrition.\n- The higher job level is, the less possibility that employess leave.\n- Technical employees are tend to leave.\n- Low performance rating and low stock option level may result employees' attrition.","1486487f":"## 2.3 Correlation Analysis","bcf3d6ac":"StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation.\n\n$$Z=\\frac{X-\\mu}{S}$$","66ea9a0d":"#### Optimized Model ","e1a4f2aa":"After dummy process and standardized numerical feature. We split the data set into training set and test set with ratio 8:2. Then, duplicate the training set to:\n- Type1: original size of training set; \n- Type2: Oversampled size of training set. Decide to use which version of training set by comparing the performance of each one after implementing a machine learning model.\n\nMoreover, this report used Grid search which is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified. First, choose some of classifying machine learning model as candidates. We put a customized list of parameters into grid to search out an optimized model under cv equals to 5. Then apply this trained model to test set. Evaluate the performance and compare them between different models.\n\n![flow.png](attachment:flow.png)","beea4bec":"## 4.1 Decision Tree","13e4a75b":"#### Tree Graph","51f4b808":"# 3. Preprocessing and Encoding of Features","4fd0c886":"#### Importance of Features","fa716a1e":"## 1.3 Check Missing Values","1191c6ef":"Import some packages for exploratory data analysis and data visualization packages.","35a2d533":"## 1.1 Import Packages","11937274":"# 0. Attrition Analysis\n\n> Data is from [IBM HR Analytics Employee Attrition & Performance](#https:\/\/www.kaggle.com\/pavansubhasht\/ibm-hr-analytics-attrition-dataset)\n\nEmployee attrition is a big issue among many companies. Balance [lessening employee turnover](#http:\/\/bit.ly\/2f8I4js) and employee attrition is a kind of trade off. Although more than 40 percent of employees have potential to attrition, companies are still taking healthy growth seriously. Take many Chinese internet companies as examples, large enterprises like NetEase and Huawei, employee attrition is a big concern although they can recruit numerous fresh employees every year. Key person risk may be ringing the bell for the company\u2019s CEO and making the company in jeopardy, however, ordinary employee like you and me, who leaving the company as the company grows, can also raise the red flag for the company. Attrition is essentially the slow decay of a company and if the management does not take action to this, the workforce will shrink, production will stop, and the company will likely go out of business. However, if these companies can tackle this challenge, more positive growth of turnover would be earned. Therefore, this problem catches our attention and our team are deciding to make a best fit prediction model for predicting employee attrition in one company based on assorted variables. In a research of McKinsey on people analytics, the intelligence included a range of information from the employee such as levels of compensation and demographic profile can indeed predict or raising red flag on certain employee so that the machine can help HR in one enterprise to make decisions in some ways.","b88eef88":"According to description page of data set, sign the category of some categorical features.","447b1647":"#### Optimized Model ","37661889":"## 3.1 Preprocessing and Encoding of Categorical Features","69cc18f3":"Define a function to output the confusion matrix after building the model.","df45cf8a":"## 2.1 Distribution of Target Variable","575e5aa9":"### 2.4.2 Cross Analysis between Attrition and Categorical Features","8c0954e1":"#### Findings of Categorical Features\n\n- In terms of satisfaction (enviroment, job and relationship satisfaction), most of employees are satisfied, which the number of \"3\" and \"4\" indicate high satisfaction are a lot.\n- Most employees hold bachelor degree and many people major in life science.\n- Since that most of employees are relative new to company, job level 1 is the majority job level.\n- Job involment is pretty high among employees.","db76d421":"### 3.1.1 Label all 'object' type of feature as 'integer'","e1b5a454":"After exploration of data, we need to process these features in order to apply them into machine learning models.","f8eab94d":"# 5. Evaluation","cf4fe19b":"## 3.2 Preprocessing and Encoding of Numerical Features","202a7270":"Define a funciton to draw ROC curve to illustrate the AUC.","342f512d":"By overview of data set, it can be found that this data set include 1470 observations and 35 features. But feature 'Over18', 'EmployeeCount' and 'StandardHours' are exactly same in every rows and 'EmployeeNumber' is the number that tag employees so we decide to drop these columns.","617d0727":"# 4. Model Selection","2acd61fc":"Then check the data set again.","9b79fd53":"#### Results of Test Set","49175ebc":"### 4.1.2 Decision Tree (Oversample)","229ffb35":"## 2.4 Cross Analysis","b87885c6":"#### Results of Test Set","69b05cf9":"# 2. EDA (Exploratory Data Analysis)","49425582":"## 2.2 Frequency Analysis","22eb8a41":"#### Findings of Correlation Analysis\n\n- Time type of features (Years at company, in current role, since last promotion and with current manager) are quite correlated to each other indicated by green zone.\n- Besides, totle working year is also correlated to monthly income as we imagined.\n- Most of features are not correlated to each other, so we do not have to deal with that. As for time type of features, we will consider that whether to alter them according to the machine learning model.  ","c54359e6":"## 1.2  Overview of Data Set","cdc9fb9c":"Select those nominal features which have more than two class as dummy variables.","0d699ffb":"#### ROC Curve","c6980acd":"After oversampling, the scale of data set changed to following.","427065fd":"### 3.1.2 Dummy Nominal Features","8d4d8854":"This is complete data set that contain no missing values.","9751e805":"# 1. Preparation","45814154":"#### ROC curve","57761727":"#### Findings of Cross Analysis between Attrition and Numerical Features\n\n> Above violin graph indicate the average value of features and similar information like boxplot. These findings are merely based on superficial results whithout consideration of confounding.\n\n- Age: Younger employees are tend to leave compared to elder employees.\n- DistanceFromHome: Those who live far from company have more possibility to leave.\n- MonthlyIncome: Most income of attrition employee are below 5000 while those who's income is higher than 5000 tend to stay.\n- NumCompaniesWorked, Time type of features: People who worked for many companies and worked for many years are likely to stay.\n","afe92c86":"#### Tree Graph","1553649a":"### Type of Features\n\nTake a close look at the type of features.\n\n#### Categorical\n- **Nominal**\n    - Attrition\n    - BusinessTravel\n    - Department\n    - EducationField\n    - Gender\n    - JobRole\n    - MaritalStatus\n    - OverTime\n- **Ordinal**\n    - Education\n    - EnvironmentSatisfaction\n    - JobInvolvement\n    - JobLevel\n    - JobSatisfaction\n    - PerformanceRating\n    - RelationshipSatisfaction\n    - StockOptionLevel\n    - WorkLifeBalance\n    \n#### Numerical\n- **Continuous**\n    - Age\n    - DailyRate\n    - DistanceFromHome\t\n    - HourlyRate\n    - MonthlyIncome\n    - MonthlyRate\n    - NumCompaniesWorked\n    - PercentSalaryHike\n    - TotalWorkingYears\n    - TrainingTimesLastYear\n    - YearsAtCompany\n    - YearsInCurrentRole\n    - YearsSinceLastPromotion\n    - YearsWithCurrManager","3eee5166":"#### Importance of Features","316e7d9d":"### 3.2.1 Scaling Numerical Features","2d0903bc":"### 4.1.1 Decision Tree","9eb23529":"Note that the distribution of target variable is quite unbalanced that 16.1% of employees decide to leave but most employees decide to stay. It should be notice that this unbalance may influence the learning model later.","53d20a55":"Splite the data set into traning set and test set with ratio 8:2\n- Duplicate the training set which:\n    - Original size of training set\n    - Oversampled size of training set\n\nDecide to use which version of training set before implementing a machine learning model.","76f3bebe":"Categorical features are: 'Attrition', 'BusinessTravel', 'Department', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobRole','JobSatisfaction', 'MaritalStatus', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction','StockOptionLevel' and 'WorkLifeBalance'. First, change the type of nominal features to object in order to process frequency analysis, because some classified variables were signed by integer in original data set. Then, browse the frequency of categorical features in data set.","67b5ca4f":"### 2.2.1 Frequency Analysis of Numerical Features","41694632":"Compared with the model trained from training set without oversampling, those oversampled training set will help to generate better model with high score in terms of precision, recall, f1-score. Because in a company, attrition will cause much more loss than doing something to detain employees. So, whether can distinguish all employees who want to leave is significant. Hence, we need to consider the AUC value and recall rate for \u2018Yes\u2019 class.  As for feature importance acquired from SVM, working overtime is the most significant reason to attrion. Then, Laboratory Technician and Sales Representative have more possibility to leave. Besides, those who\u2019s education background is human resource are also tend to leave.","ea1e0306":"### 2.2.2 Frequency Analysis of Categorical Features","e67e22af":"Plot correlation matrix by heatmap.","6fd0a7b9":"### 2.4.1 Cross Analysis between Attrition and Numerical Features","2309942d":"The numerical features in these data set are: 'Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion' and 'YearsWithCurrManager'. Plot the distribution graph of these features and find some pattern.","c00df07b":"#### Findings of Numerical Features\n\n- Age: The age distribution of this data set distributed normally which cover from 20 to 60. Most employees are 30 to 40.\n- DistanceFromHome: Most of employees live close to company which most distance are below 10km.\n- MonthlyIncome: The majority of monthly income of employees are centred at around 5000. Only a few of people get high income over 10000.\n- NumCompaniesWorked: Most employees only worked for one company.\n- TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager: These time type data are right skewed which most of people stay in company only for a few years.\n- DailyRate, HourlyRate, MonthlyRate distributed uniformly which the figure is similar in different intervals."}}