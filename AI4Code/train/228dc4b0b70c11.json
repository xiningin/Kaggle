{"cell_type":{"2be2bc77":"code","220e7f25":"code","572b0805":"code","d7db7a19":"code","2df92448":"code","74d0cac9":"code","bbb61d99":"code","b6d01e2d":"code","7033e33b":"code","050e9eee":"code","0727332d":"code","59dbdaa0":"code","74930857":"code","f17b674b":"code","488b2f02":"code","236f96e2":"code","6c9254d9":"code","610ffb6f":"code","5b4ef7d6":"code","2b7d3be2":"code","f06385ab":"code","1712a0f2":"code","cc12d39c":"code","8f921f18":"code","46d3f98d":"code","7c2a63c0":"code","c342f169":"code","fcb2010c":"code","8341ef8d":"code","9532cbda":"code","dac5eab1":"code","254ada90":"code","34d6921e":"code","7c954cc3":"code","8608e3c8":"code","8becebdf":"code","b4d17695":"code","ce473959":"code","f62031e0":"code","6d06ed6e":"code","2b579caa":"code","1b21a109":"code","7a57aa50":"code","de6e836f":"code","f58ae1b6":"code","ca57e962":"code","927eacfa":"code","f79ff130":"code","cc76f982":"code","badfb7c8":"code","a20e1d0e":"code","6b79b0fd":"code","82203cf5":"code","6dcdc481":"code","3966af01":"code","21e1b152":"code","6baa3e8b":"code","b8643ee9":"markdown","49bfde68":"markdown","93cecb7b":"markdown","d1938704":"markdown","fe12bafa":"markdown","48b579ea":"markdown","7839d969":"markdown","ab67ece7":"markdown","cadde64f":"markdown","202bd5f4":"markdown","04058e93":"markdown","9fa4a337":"markdown","58b329d4":"markdown","fc500725":"markdown"},"source":{"2be2bc77":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns  ","220e7f25":"# Read the csv files\n\nsales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n\n","572b0805":"shops_t = pd.read_csv(\"..\/input\/filestranslated\/shops-translated.csv\")\nitems_t = pd.read_csv(\"..\/input\/filestranslated\/items-translated.csv\")\nitem_categories_t = pd.read_csv(\"..\/input\/filestranslated\/categories_translated.csv\")","d7db7a19":"# items (items-translated.csv was missing the category ids - join to the original items.csv)\n\ndel items['item_name']\nitems = items.merge(items_t, how='left', on='item_id')\nitems.rename(columns={'item_name_translated':'item_name'}, inplace=True)","2df92448":"# item categories\n\ndel item_categories_t['Unnamed: 0']\nitem_categories = item_categories_t","74d0cac9":"#shops\nshops_t.rename(columns={'shop_name_translated':'shop_name'}, inplace=True)\nshops = shops_t","bbb61d99":"# Sales:\n\nprint('rows and columns:', sales.shape, '\\n')\nprint(sales.info(), '\\n')\nprint(sales.count(), '\\n')\nprint('missing values:\\n', sales.isna().sum())","b6d01e2d":"sales.sort_values(by=['date','shop_id','item_id']).head(5)","7033e33b":"print('\\nitem_cnt_day descriptive statistics')\nprint( sales.item_cnt_day.describe().apply(lambda x: format(x, '10.1f')) )","050e9eee":"# The item_count series shows some purchases with huge counts\n\nsales.item_cnt_day.plot(figsize=(15, 6))\nplt.show()","0727332d":"# there are lots of outliers in the with large quantities\n\n\nsns.boxplot(y=\"item_cnt_day\", data=sales)\nplt.title('Item Count')\nplt.show()","59dbdaa0":"# Generate item_cnt_day outliers plot\n\nplt.figure(figsize=(16,5))\nsns.distplot(sales['item_cnt_day'], kde=False, rug=True)\nplt.xlim(-20, 500)\nplt.show()","74930857":"# how many zero or negative item_cnt_days?\n\nprint( 'zero item_cnt_days:', len ( sales.loc[sales['item_cnt_day']==0] ) )\nprint( 'negative item_cnt_days:', len ( sales.loc[sales['item_cnt_day']<0] ) )","f17b674b":"# convert the date field to pandas datetime\n\nsales.loc[ : , ('date') ] = pd.to_datetime(sales.loc[ : , ('date') ],format='%d.%m.%Y')","488b2f02":"# add year, month, and day columns\n\nsales['year'] = sales['date'].dt.year\nsales['month'] = sales['date'].dt.month\nsales['day'] = sales['date'].dt.day","236f96e2":"# create a year-month field\n\nsales['year_month'] = sales['date'].map(lambda x: 100*x.year + x.month)","6c9254d9":"# create a weekday field (0=Mon, 6=Sun)\n\nsales['day_of_week'] = sales['date'].map(lambda x: x.weekday())","610ffb6f":"# Sort values in dataframe, order the columns, reset the index\n\nsales = sales.sort_values(by=['date','shop_id','item_id'], ascending=[True,True,True])\n\nsales_dates = ['date','date_block_num','year_month','year','month','day','day_of_week']\nsales_data = ['shop_id','item_id','item_price','item_cnt_day']\nsales = sales[sales_dates + sales_data]\n\nsales.reset_index(drop=True,inplace=True)","5b4ef7d6":"sales.head()","2b7d3be2":"daily_count_sum = sales.groupby('date')['item_cnt_day'].sum()","f06385ab":"# Overall time series trend - downward sales volume \n\ndaily_count_sum.plot(figsize=(15, 6))\nplt.title('Daily sales counts (sum of item count per day)')\nplt.show()","1712a0f2":"# Yearly trend shows certain months like January and December have higher volumes\n\n# Monthly sums\n\ncount_by_month = pd.DataFrame(sales.groupby('month')['item_cnt_day'].sum() )\ncount_by_month.reset_index(inplace=True)\n\n\n# (January is 1)\n\ncount_by_month_jd = count_by_month.loc[ count_by_month['month'].isin([1,12]) ]\ncount_by_month_other = count_by_month.loc[ ~ count_by_month['month'].isin([1,12]) ]\n\n\n\n# Graph the data\n\nobjects = ('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')\nobjects_other = ('Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov')\nobjects_jd = ('Jan', 'Dec')\n\nx_pos = np.arange(len(objects))\nx_pos_other = np.array([1,2,3,4,5,6,7,8,9,10])\nx_pos_jd = np.array([0,11])\n\nplt.figure(figsize=(5,5))\nwidth=0.5\n\nrects_other = plt.bar(x_pos_other, count_by_month_other.item_cnt_day, width, color='b')\nrects_jd = plt.bar(x_pos_jd, count_by_month_jd.item_cnt_day, width, color='r')\n\nplt.xticks(x_pos, objects, rotation=90)\nplt.title('More Items are Sold in January and December')\nplt.ylabel('Count of Items Sold')\nplt.xlabel(None)\nplt.show() ","cc12d39c":"# Weekly time series trend - peaks and troughs can be observed for every week\n\ndaily_count_sum['2013-01-01':'2013-07-01'].plot(figsize=(15, 6)) # weekly peaks\/troughs can be observed\nplt.title('Daily sales counts (sum of item count per day) 1\/2013-6\/2013')\nplt.show()","8f921f18":"# Weekly cycles occur because weekend sales are higher\n\ncount_by_day = pd.DataFrame( sales.groupby('day_of_week')['item_cnt_day'].sum() )\ncount_by_day.reset_index(inplace=True)\n\n# weekdays, weekends\n\ncount_by_day_we = count_by_day.loc[ count_by_day['day_of_week']>=5 ]\ncount_by_day_wd = count_by_day.loc[ count_by_day['day_of_week']< 5 ]\n\n# Graph\n\nobjects = ('Mon','Tues','Wed','Thu','Fri','Sat','Sun')\nobjects_wd = ('Mon','Tues','Wed','Thu','Fri')\nobjects_we = ('Sat','Sun')\n\nx_pos = np.arange(len(objects))\nx_pos_wd = np.arange(len(objects_wd))\nx_pos_we =(5,6)\n\nplt.figure(figsize=(5,5))\nwidth=0.5\n\nrects_wd = plt.bar(x_pos_wd, count_by_day_wd.item_cnt_day, width, color='b')\nrects_we = plt.bar(x_pos_we, count_by_day_we.item_cnt_day, width, color='r')\n\nplt.xticks(x_pos, objects, rotation=90)\nplt.title('More Items are Sold on Saturday and Sunday')\nplt.ylabel('Average # of Items Sold')\nplt.xlabel(None)\nplt.show() ","46d3f98d":"# merge sales data with descriptive data for items, shops and categories\n\nsales = pd.merge(sales,items,how='left',on='item_id', copy=False)\nsales = sales.merge(item_categories,how='left',on='item_category_id', copy=False)\nsales = sales.merge(shops,how='left',on='shop_id', copy=False)","7c2a63c0":"sales.head()","c342f169":"#The item which sold the most units:\nsales.loc[sales['item_cnt_day']==sales['item_cnt_day'].max()]","fcb2010c":"item_category_count_sums = pd.DataFrame( sales.groupby(['item_category_name'])['item_cnt_day'].sum() )\nitem_category_count_sums = item_category_count_sums.reset_index() ","8341ef8d":"iccs_sort = item_category_count_sums.sort_values(by='item_cnt_day', ascending=False)\niccs_sort.reset_index(inplace=True, drop=True)","9532cbda":"plt.figure(figsize=(16,10))\nsns.barplot(x='item_category_name', y='item_cnt_day', data=item_category_count_sums, order=iccs_sort.item_category_name )\nplt.xticks(rotation=90)\nplt.xlabel('None')\nplt.ylabel('total units sold')\nplt.title('Units sold by item category')\nplt.show()","dac5eab1":"shop_count_sums = pd.DataFrame( sales.groupby(['shop_name'])['item_cnt_day'].sum() )\nshop_count_sums = shop_count_sums.reset_index() ","254ada90":"scs_sort = shop_count_sums.sort_values(by='item_cnt_day', ascending=False)\nscs_sort.reset_index(inplace=True,drop=True)","34d6921e":"plt.figure(figsize=(16,10))\nsns.barplot(x='shop_name', y='item_cnt_day', data=shop_count_sums, order=scs_sort.shop_name)\nplt.xticks(rotation=90)\nplt.xlabel(None)\nplt.ylabel('Sales count')\nplt.title('Total sales count for each shop')\nplt.show()","7c954cc3":"# drop any records with item_cnt_day above x, where x can be changed before fitting the model.\n\nprint(sales.shape)\nsales = sales.loc[ sales['item_cnt_day'] <= 25 ]\nprint(sales.shape)","8608e3c8":"# Pivot the table to wide format\n# rows = shop_id+item_id\n# columns = date_block_num as the columns \n# values = sum(item_cnt_day)\n# \n\nsales_monthly = sales.pivot_table(index = ['shop_id','item_id']\n                                  ,values = ['item_cnt_day']\n                                  ,columns = ['date_block_num']\n                                  ,fill_value = 0\n                                  ,aggfunc='sum')","8becebdf":"sales_monthly.reset_index(inplace = True)\nsales_monthly.head()","b4d17695":"# Left join merge the test data with the training data on item_id and shop_id\n# This keeps all shop+item combinations that are required by the test set \n# and drops those from the training set that are not.\n\nsales_monthly = pd.merge(test,sales_monthly,on = ['shop_id', 'item_id'],how = 'left')","ce473959":"#filling NaN with zeroes\nsales_monthly.fillna(0,inplace = True)","f62031e0":"# Drop Id, shop_id, and item_id as they are uniquely captured by the index\n\nsales_monthly.drop(['ID', 'shop_id','item_id','ID'],inplace = True, axis = 1)\nsales_monthly.head()","6d06ed6e":"sales_monthly.shape","2b579caa":"# select all the columns except for the last one for the training set\n# expand to an array of 3 dimensions with shape (214200, 33, 1)\n\nX_train = np.expand_dims(sales_monthly.values[:,:-1],axis=2)\nX_train.shape","1b21a109":"# The last column is our training labels (or truth values)\n# creates a 2d array with shape (214200, 1) \n\ny_train = sales_monthly.values[:,-1:]\ny_train.shape","7a57aa50":"# select all the columns except for the first one for the 'test' set \n# expand to an array of 3 dimensions with shape (214200, 33, 1)\n\n# note that it must include the last column, unlike the training set,\n# but doesn't include the first columns so the arrays can be the same shape.\n\nX_test = np.expand_dims(sales_monthly.values[:,1:],axis=2)\nX_test.shape","de6e836f":"import math\nimport matplotlib.pyplot as plt","f58ae1b6":"def rmse(acc):\n  rmse =[]\n  for i in acc:\n    rmse.append(math.sqrt(i))\n  return rmse","ca57e962":"def plot_train_curve(history):\n\n    colors = ['#e66101','#fdb863']\n    accuracy = rmse(history.history['mean_squared_error'])\n    epochs = range(len(accuracy))\n    with plt.style.context(\"ggplot\"):\n        plt.figure(figsize=(8, 8\/1.618))\n        plt.ticklabel_format(useOffset=False)\n        plt.plot(epochs, accuracy, marker='o', c=colors[0], label='Training RMSE')\n        \n        axes = plt.gca()\n\n        plt.title('Training RMSE')\n        plt.legend()\n        plt.show()","927eacfa":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.models import load_model, Model\n\nlstm_model_1 = Sequential()\nlstm_model_1.add(LSTM(units = 64 , input_shape = (33,1), activation='relu'))\nlstm_model_1.add(Dropout(0.5))\nlstm_model_1.add(Dense(1))\n\nlstm_model_1.compile(loss='mse',optimizer = 'adam',metrics=['mean_squared_error'])\nlstm_model_1.summary()","f79ff130":"lstm_model_1_history = lstm_model_1.fit(X_train,y_train,batch_size=4096,epochs=20)","cc76f982":"plot_train_curve(lstm_model_1_history)","badfb7c8":"output = lstm_model_1.predict(X_test)\nsubmission = pd.DataFrame({'ID':test['ID'],'item_cnt_month':output.ravel()})\nsubmission.head()","a20e1d0e":"submission.to_csv('sample_submission_lstm_model_1.csv',index = False)","6b79b0fd":"# Get sales for the last month and the predicted sales for the next month\n\nlstm_predictions = list(output)\nlast_month = list(y_train)\npredicted_changes = pd.DataFrame( {'last':last_month, 'pred':lstm_predictions})","82203cf5":"# Get the percent difference for (predicted sales next month - sales previous month)\n\npredicted_changes['pct_diff'] = (predicted_changes['pred'] - predicted_changes['last'])\/predicted_changes['last']","6dcdc481":"# Remove any non-numeric results, sort the store-items by percent change, and get the top 10\n\npredicted_changes = predicted_changes[ ~ predicted_changes['pct_diff'].isin([np.nan, np.inf, -np.inf]) ]\npredicted_changes.sort_values(by='pct_diff', ascending=False, inplace=True)\npredicted_changes = predicted_changes[0:9] ","3966af01":"# Reset the index and name the column ID\n\npredicted_changes.reset_index(inplace=True)\npredicted_changes.rename(columns={'index': 'ID'}, inplace=True)","21e1b152":"# Join the top 10 changes with the descriptions for shop, item, and item category\n\npredicted_changes = pd.merge(predicted_changes, test, on = ['ID'], how = 'left')\npredicted_changes = pd.merge(predicted_changes, shops, on = ['shop_id'], how = 'left')\npredicted_changes = pd.merge(predicted_changes, items, on = ['item_id'], how = 'left')\npredicted_changes = pd.merge(predicted_changes, item_categories, on = ['item_category_id'], how = 'left')","6baa3e8b":"# Only keep the descriptive columns and format the percentages\n\npredicted_changes = predicted_changes[['item_name','item_category_name','shop_name','pct_diff']]\n\ndef format_pcts(x):\n  x = x.astype(float)\n  x = x * 100\n  x = round(x,2)\n  return x\n  \npredicted_changes['pct_diff'] = format_pcts(predicted_changes['pct_diff'])\npredicted_changes.head(10)","b8643ee9":"## Add Russian to English translations\n\nThree of the files in the dataset have Russian text (shops, items, item_categories). For English speakers, translations were added.\nKaggle user Orhan kindly made the Russian to English translated files available as a dataset\nhttps:\/\/www.kaggle.com\/orhankaramancode\/filestranslated","49bfde68":"## EDA - Explore data\n\nPreliminary EDA with descriptive statistics","93cecb7b":"## EDA - Explore data for items, categories, and shops","d1938704":"## Output predictions to csv","fe12bafa":"## Transform Dates\n* convert date field to datetime\n* add some additional date features ","48b579ea":"# Predict future sales - EDA and LSTM prediction\n\n**This analysis includes an exploratory analysis of the data, an LSTM model, a plot of traing RMSE performance, and a look at the top 10 predictions of the model.**\n\nTable of contents:\n------------------\n1. Read csv files\n2. Add Russian to English translations\n3. EDA - Explore data\n4. Descriptive stats for item_cnt_day\n5. Transform Dates\n6. EDA - Time series trends\n7. EDA - Explore data for items, categories, and shops\n8. Drop outliers\n9. Prepare the data for LSTM model\n10. Model training plots\n11. LSTM Model\n12. Output predictions to csv\n13. Top 10 predicted sales increases\n","7839d969":"## Read csv files","ab67ece7":"## LSTM Model","cadde64f":"## Prepare the data for LSTM model","202bd5f4":"### Descriptive stats for item_cnt_day\n\nSince the task is to predict counts for each item-store combination in one month, it will be good to look at the statistics for the item_cnt_day column.\n\n* max count is much higher than the average, indicating large outliers \n* the min count is -22, indicating item returns \n* the 25% and 75% quartiles for item_cnt_day = 1.0, indicating most sales are for one item","04058e93":"## EDA - Time series trends","9fa4a337":"## Top 10 predicted sales increases","58b329d4":"## Drop outliers ","fc500725":"## Model training plots "}}