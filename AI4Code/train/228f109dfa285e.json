{"cell_type":{"f8d31267":"code","7d11156a":"code","1cd9fc8a":"code","853891da":"code","13fdf7d8":"code","2703b8a9":"code","f4ef8853":"code","68c64faf":"code","c33f1c56":"code","50b813bd":"code","c64b75e9":"code","3545a6c4":"code","8cd69720":"code","e38fdd8b":"code","c241567c":"code","b79b6eee":"code","47a8ad2c":"code","51f86224":"code","ddf91ada":"code","629c8cfc":"markdown","f02d4e5f":"markdown","48bde87a":"markdown","2745b0ef":"markdown","da1dbd52":"markdown","080b4d1b":"markdown","f9da3725":"markdown","cbb1107e":"markdown","c704d9b2":"markdown"},"source":{"f8d31267":"import cupy as cp\nimport cudf, cuml\nimport pandas as pd\nimport numpy as np\nfrom cuml.manifold import TSNE, UMAP\nimport matplotlib.pyplot as plt\n%matplotlib inline","7d11156a":"train = cp.load('..\/input\/giba-s-fft-features-only\/TRAIN.npy')\ntest = cp.load(\"..\/input\/giba-s-fft-features-only\/TEST.npy\")\nTRAIN_TAB = cudf.read_csv(\"..\/input\/giba-s-fft-features-only\/TRAIN_TAB.csv\")","1cd9fc8a":"TRAIN_TAB.head()","853891da":"TARGET_VALUES = TRAIN_TAB.iloc[:,2:].values\nTARGET_VALUES = cp.asnumpy(TARGET_VALUES)","13fdf7d8":"TARGET_VALUES[:,0]","2703b8a9":"train_test = cp.vstack([train, test])","f4ef8853":"%%time\ntsne = TSNE(n_components=2)\ntrain_test_2D = tsne.fit_transform(train_test)","68c64faf":"train_test_2D = cp.asnumpy(train_test_2D)","c33f1c56":"plt.scatter(train_test_2D[:,0], train_test_2D[:,1], s = 0.5)\n","50b813bd":"%%time\numap = UMAP(n_components=2)\ntrain_test_2D = umap.fit_transform(train_test)","c64b75e9":"train_test_2D = cp.asnumpy(train_test_2D)","3545a6c4":"plt.scatter(train_test_2D[:,0], train_test_2D[:,1], s = 0.5)\n","8cd69720":"%%time\ntsne = TSNE(n_components=2)\numap = UMAP(n_components=2)\ntrain_2D_tsne = tsne.fit_transform(train)\ntrain_2D_umap = umap.fit_transform(train)","e38fdd8b":"train_2D_tsne = cp.asnumpy(train_2D_tsne)\ntrain_2D_umap = cp.asnumpy(train_2D_umap)","c241567c":"y = TARGET_VALUES[:,0] #cp.asnumpy(TRAIN_TAB['s0'].values)","b79b6eee":"plt.scatter(train_2D_tsne[:,0], train_2D_tsne[:,1], c = y, s = 0.5)\n","47a8ad2c":"plt.scatter(train_2D_umap[:,0], train_2D_umap[:,1], c = y, s = 0.5)\n","51f86224":"fig, axs = plt.subplots(2, 2)\nfor i in range(2):\n    for j in range(2):\n        axs[i,j].scatter(train_2D_tsne[:,0], train_2D_tsne[:,1], c = TARGET_VALUES[:,i*2+j], s = 1.0)\n","ddf91ada":"fig, axs = plt.subplots(2, 2)\nfor i in range(2):\n    for j in range(2):\n        axs[i,j].scatter(train_2D_umap[:,0], train_2D_umap[:,1], c = TARGET_VALUES[:,i*2+j], s = 1.0)","629c8cfc":"First, we are going to combine train and test to try to visualize the overall shape of reduced data.","f02d4e5f":"In this notebook we'll do dimensionality reduction and visualization of the FFT features that were first used in this competition in [this Giba's notebook](https:\/\/www.kaggle.com\/titericz\/0-309-baseline-logisticregression-using-fft). I've created a stand-alone notebook that extracts those features, and it can be found [here](https:\/\/www.kaggle.com\/tunguz\/giba-s-fft-features-only).\n\nWe will make this visualization notebook with the Rapids library. [Rapids](https:\/\/rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https:\/\/www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. \n\nRapids is still undergoing developemnt, and only recently has it become possible to use RAPIDS natively in the Kaggle Docker environment. If you are interested in installing and riunning Rapids locally on your own machine, then you should [refer to the followong instructions](https:\/\/rapids.ai\/start.html).","48bde87a":"There are some hints of the structure, but nothing too dramatic. Maybe this is not surprizing; after all, the deataset is supposed to represent 24 different sound categories. \n\nNow let's look at what the dataset looks with UMAP dimensionality reduction.","2745b0ef":"That was even faster! \n\nLet's see what this dimensionality reduction looks like.","da1dbd52":"There seems to be more of a structure with this reduction, but still nothign dramatic, at least not at this scale. UMAP usually produces more outlyers, which tend to shrink make the bulk of the datapoints into small fraction of the visual representation.","080b4d1b":"Well, that only took a few seconds!\n\nIn order to visualize the new reduced dataset, we'll need to convert it into a numpy array, as matplotlibe does not work on GPUs. ","f9da3725":"It's very hard to see, but all of the yellow dots seem to be concentrated in the upper middle area.","cbb1107e":"Now let's take a look at the data","c704d9b2":"We'll now make dimensionality reductions for the train set only, and take a look how the distributions look with respect to the target."}}