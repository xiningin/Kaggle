{"cell_type":{"1a57e97c":"code","97524656":"code","6b894aa9":"code","2440e3d6":"code","07e18491":"code","1eb1cf22":"code","53b28e04":"code","da6bdccb":"code","7ea9dffc":"code","242be169":"code","570d83c2":"code","81832185":"code","eda618b1":"code","07303917":"code","db9b0f1f":"code","27347639":"code","c22e496d":"code","961ad550":"code","260c19fd":"code","174fab65":"code","5408149d":"code","96dd76a9":"code","faf34a50":"code","d30c0f3a":"code","d9c892f3":"code","a26d55a8":"code","4bb3ddd1":"code","cbf6e608":"code","d4b3206b":"code","c0e9703e":"code","70ca50f9":"code","ef16f0a8":"code","a6d01ba1":"code","5c864300":"code","e5406f30":"code","52450e2e":"code","b1090180":"code","2e1b1e51":"code","add8a5f4":"code","29b76ad4":"code","72639394":"markdown","b5af5987":"markdown","ac9e70a0":"markdown","5abeb8e8":"markdown","c102b665":"markdown","20e05910":"markdown","644807de":"markdown","e272a438":"markdown","71d60c82":"markdown"},"source":{"1a57e97c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.express as px\nfrom IPython.display import Image\nimport matplotlib_venn as vplt\n# warnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97524656":"all_data = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv')\nquestions = all_data.loc[0]\nall_data.drop(0, axis=0, inplace=True)\npd.set_option('display.max_columns', 500)\nall_data.head()","6b894aa9":"all_data = all_data.rename(columns = {'Time from Start to Finish (seconds)': 'Time',\n                                        'Q1':'Age',\n                                        'Q2':'Gender',\n                                        'Q3':'Country',\n                                        'Q4':'Education',\n                                        'Q5':'Job',\n                                        'Q6':'Experience'})\nexploratory = all_data[['Time','Age', 'Gender', 'Country', 'Education', 'Job', 'Experience']].copy()\nexploratory.nunique()","2440e3d6":"time_minutes = exploratory['Time'].astype(int)\/60\ntime_minutes = time_minutes[time_minutes >3]\ntime_minutes = time_minutes[time_minutes <120]\ntime_minutes.plot.hist(figsize = (18,8), ec = 'white', bins = 117)","07e18491":"time_minutes.describe()","1eb1cf22":"#fig, axs = plt.subplots(2, 2)\n#plt.axes(axs[0,0])\n\n# print(sorted(exploratory.Age.unique()))\n\norder_age = ['18-21', '22-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59', '60-69', '70+']\nexploratory['Age'].value_counts().loc[order_age].plot.bar()","53b28e04":"exploratory['Gender'].value_counts().plot(kind='bar')\nexploratory.Gender.value_counts()","da6bdccb":"# exploratory.Education.unique()\norder_edu = ['No formal education past high school', 'Some college\/university study without earning a bachelor\u2019s degree', 'Professional degree', 'Bachelor\u2019s degree', 'Master\u2019s degree', 'Doctoral degree', 'I prefer not to answer']\nexploratory['Education'].value_counts().loc[order_edu].plot(kind='bar')","7ea9dffc":"exploratory['Job'].value_counts().plot(kind='bar')","242be169":"#exploratory.Experience.unique()\norder_exp = ['I have never written code', '< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years']\nexploratory['Experience'].value_counts().loc[order_exp].plot(kind='bar')","570d83c2":"print('Image generated by Tableau Desktop for data exploration and visualisation:')\nImage(\"..\/input\/image-respondents-by-country\/Country.png\")","81832185":"#all_data.describe()\nprog_language = {}\nfor i in all_data.columns[7:20]:\n    if type(all_data[i].unique()[0]) == str:\n        prog_language[i] = 'Language_' + all_data[i].unique()[0]\n    else:\n        prog_language[i] = 'Language_' + all_data[i].unique()[1]\n\nall_data = all_data.rename(columns = prog_language)\nlanguage_visualisation = all_data[all_data.columns[7:20]].notnull().astype('int')\nlanguage_visualisation.sum()","eda618b1":"language_visualisation.sum().plot.bar()","07303917":"language_visualisation['Top_3'] = language_visualisation.Language_Python.astype(str) + language_visualisation.Language_R.astype(str) + language_visualisation.Language_SQL.astype(str)\nlanguage_visualisation.Top_3.value_counts()\n# Python, R, SQL","db9b0f1f":"#vplt.venn3(subsets = (7364, 488, 1499, 601, 4644, 267, 2023), set_labels = ('Python', 'R', 'SQL'))\nvplt.venn3_unweighted(subsets = (7364, 488, 1499, 601, 4644, 267, 2023), set_labels = ('Python', 'R', 'SQL'))\nplt.title('Most common programming languages used in datascience jobs')\nplt.show()","27347639":"prog_IDE = {}\n\nfor i in all_data.columns[21:33]:\n    if type(all_data[i].unique()[0]) == str:\n        prog_IDE[i] = 'IDE_' + all_data[i].unique()[0].strip()\n    else:\n        prog_IDE[i] = 'IDE_' + all_data[i].unique()[1].strip()\n\nall_data = all_data.rename(columns = prog_IDE)\nIDE_visualisation = all_data[all_data.columns[21:33]].notnull().astype('int')\nIDE_visualisation.sum()","c22e496d":"IDE_visualisation.sum().plot.bar()","961ad550":"prog_viz = {}\n\nfor i in all_data.columns[53:65]:\n    if type(all_data[i].unique()[0]) == str:\n        prog_viz[i] = 'Visualisation_' + all_data[i].unique()[0].strip()\n    else:\n        prog_viz[i] = 'Visualisation_' + all_data[i].unique()[1].strip()\n\nall_data = all_data.rename(columns = prog_viz)\nviz_visualisation = all_data[all_data.columns[53:65]].notnull().astype('int')\nviz_visualisation.sum()","260c19fd":"viz_visualisation.sum().plot.bar()","174fab65":"viz_visualisation['Top_2'] = viz_visualisation['Visualisation_Matplotlib'].astype(str) + viz_visualisation['Visualisation_Seaborn'].astype(str)\nviz_visualisation['Top_2'].value_counts()\n\n# Matplotlib, Seaborn","5408149d":"vplt.venn2_unweighted(subsets = (4053, 532, 8289), set_labels = ('Matplotlib', 'Seaborn'))\nplt.title('Most common visualisation tools used in datascience jobs')\nplt.show()","96dd76a9":"all_data = all_data.rename(columns = {'Q15': 'Years_ML',\n                                      'Q20': 'Company_Size',\n                                      'Q21': 'Datascience_Size',\n                                      'Q24': 'Compensation',\n                                      'Q25': 'Expenditure',\n                                     })\ndata_numeric = all_data[['Age', 'Experience', 'Years_ML', 'Company_Size', 'Datascience_Size', 'Compensation', 'Expenditure']]\ndata_numeric.head()","faf34a50":"data_numeric['Age'].replace({'70+': '70-80'}, inplace = True)\ndata_numeric['Age'] = (data_numeric['Age'].str[:2].astype(float) + data_numeric['Age'].str[-2:].astype(float))\/2\ndata_numeric['Experience'].replace({'5-10 years': 7.5,\n                                    '10-20 years': 15,\n                                    '3-5 years': 4,\n                                    '< 1 years': 0.5,\n                                    '1-2 years': 1.5,\n                                    '20+ years': 25,\n                                    'I have never written code': 0}, inplace = True)\ndata_numeric['Years_ML'].replace({'1-2 years': 1.5,\n                                  'I do not use machine learning methods': 0,\n                                  '3-4 years': 3.5,\n                                  'Under 1 year': 0.5,\n                                  '2-3 years': 2.5,\n                                  '4-5 years': 4.5,\n                                  '5-10 years': 7.5,\n                                  '20 or more years': 25,\n                                  '10-20 years': 15}, inplace = True)\ndata_numeric['Company_Size'].replace({'10,000 or more employees': 25000,\n                                      '1000-9,999 employees': 5500,\n                                      '250-999 employees': 625,\n                                      '0-49 employees': 25,\n                                      '50-249 employees': 150}, inplace = True)\ndata_numeric['Datascience_Size'].replace({'20+': 30,\n                                          '0': 0,\n                                          '5-9': 7,\n                                          '1-2': 1.5,\n                                          '3-4': 3.5,\n                                          '10-14': 12,\n                                          '15-19': 17}, inplace = True)\ndata_numeric['Compensation'].replace({'100,000-124,999': 112500,\n                                      '15,000-19,999': 17500,\n                                      '125,000-149,999': 137500,\n                                      '70,000-79,999': 75000,\n                                      '30,000-39,999': 35000,\n                                      '90,000-99,999': 95000,\n                                      '1,000-1,999': 1500,\n                                      '$0-999': 500,\n                                      '10,000-14,999': 12500,\n                                      '150,000-199,999': 175000,\n                                      '60,000-69,999': 65000,\n                                      '4,000-4,999': 4500,\n                                      '> $500,000': 600000,\n                                      '300,000-500,000': 400000,\n                                      '40,000-49,999': 45000,\n                                      '25,000-29,999': 27500,\n                                      '80,000-89,999': 85000,\n                                      '7,500-9,999': 8750,\n                                      '50,000-59,999': 55000,\n                                      '250,000-299,999': 275000,\n                                      '5,000-7,499': 6250,\n                                      '2,000-2,999': 2500,\n                                      '20,000-24,999': 22500,\n                                      '200,000-249,999': 225000,\n                                      '3,000-3,999': 3500}, inplace = True)\ndata_numeric['Expenditure'].replace({'$100,000 or more ($USD)': 250000,\n                                     '$0 ($USD)': 0,\n                                     '$10,000-$99,999': 55000,\n                                     '$1-$99': 50,\n                                     '$1000-$9,999': 5500,\n                                     '$100-$999': 550}, inplace = True)\ndata_numeric.head()","d30c0f3a":"plt.figure(figsize=(10, 8))\nheatmap = sns.heatmap(data_numeric.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);","d9c892f3":"#data_numeric.to_csv('data_numeric.csv', index = False)","a26d55a8":"all_salary = pd.concat([data_numeric.drop(columns = ['Compensation']), all_data[['Compensation', 'Gender', 'Country', 'Education', 'Job']].copy(), language_visualisation, viz_visualisation],axis = 1)\n\nds_salary = all_salary[all_salary['Job'] == 'Data Scientist'].drop(columns = ['Top_3', 'Top_2'])\nds_salary.dropna(subset = ['Compensation'], inplace = True)\n\nds_salary.drop(columns = ['Job'], inplace = True)\n\nds_salary['Education'].replace({'Some college\/university study without earning a bachelor\u2019s degree': 'No Degree',\n                                'I prefer not to answer': 'No Degree',\n                                'Professional degree': 'No Degree',\n                                'No formal education past high school': 'No Degree'}, inplace = True)\nds_salary = ds_salary[ds_salary['Gender'].isin(['Man', 'Woman'])]\n#ds_salary['Gender'].replace({'Nonbinary': 'Other',\n#                             'Prefer to self-describe': 'Other',\n#                             'Prefer not to say': 'Other'}, inplace = True)","4bb3ddd1":"ds_salary['Number_Languages'] = ds_salary[['Language_Python', 'Language_R', 'Language_SQL', 'Language_C',\n       'Language_C++', 'Language_Java', 'Language_Javascript',\n       'Language_Julia', 'Language_Swift', 'Language_Bash', 'Language_MATLAB', 'Language_Other']].sum(axis = 1)\n\nds_salary['Number_Visualisation'] = ds_salary[['Visualisation_Matplotlib',\n       'Visualisation_Seaborn', 'Visualisation_Plotly \/ Plotly Express',\n       'Visualisation_Ggplot \/ ggplot2', 'Visualisation_Shiny',\n       'Visualisation_D3 js', 'Visualisation_Altair', 'Visualisation_Bokeh',\n       'Visualisation_Geoplotlib', 'Visualisation_Leaflet \/ Folium', 'Visualisation_Other',]].sum(axis = 1)\n\nds_salary = ds_salary[(ds_salary['Number_Languages'] != 0) | (ds_salary['Language_None'] == 1)]\n# it dont make sense to not be using any coding language and not have 'None' checked.\n\nds_salary.drop(columns = ['Language_Python', 'Language_R', 'Language_SQL', 'Language_C',\n       'Language_C++', 'Language_Java', 'Language_Javascript',\n       'Language_Julia', 'Language_Swift', 'Language_Bash', 'Language_MATLAB', 'Language_Other', 'Language_None'], inplace = True)\nds_salary.drop(columns = ['Visualisation_Matplotlib',\n       'Visualisation_Seaborn', 'Visualisation_Plotly \/ Plotly Express',\n       'Visualisation_Ggplot \/ ggplot2', 'Visualisation_Shiny',\n       'Visualisation_D3 js', 'Visualisation_Altair', 'Visualisation_Bokeh',\n       'Visualisation_Geoplotlib', 'Visualisation_Leaflet \/ Folium', 'Visualisation_Other', 'Visualisation_None'], inplace = True)\n#ds_salary.to_csv('ds_salary.csv', index = False)","cbf6e608":"print('Image generated by Tableau Desktop for data exploration and visualisation:')\nImage(\"..\/input\/average-compensation-by-country\/Country_Compensation.png\")","d4b3206b":"print('Image generated by Tableau Desktop for data exploration and visualisation:')\nImage(\"..\/input\/average-compensation-by-country-map\/Country_Compensation_Map.png\")","c0e9703e":"upper_income = ['Australia', 'Canada', 'Germany', 'Ireland', 'Israel', 'Netherlands', 'Singapore', 'Sweden', 'Switzerland', 'United Arab Emirates', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America']\nmiddle_income = ['Argentina', 'Belgium', 'Chile', 'France', 'Italy', 'Japan', 'Malaysia', 'Mexico', 'Other', 'Pakistan', 'Philippines', 'Poland', 'Portugal', 'Republic of Korea', 'Romania', 'Saudi Arabia', 'South Africa', 'Spain', 'Taiwan', 'Thailand']\nlower_income = ['Bangladesh', 'Belarus', 'Brazil', 'China', 'Colombia', 'Egypt', 'Ghana', 'Greece', 'India', 'Indonesia', 'Iran, Islamic Republic of...', 'Kenya', 'Morocco', 'Nepal', 'Nigeria', 'Peru', 'Russia', 'South Korea', 'Sri Lanka', 'Tunisia', 'Turkey', 'Ukraine', 'Viet Nam']\n\nds_salary.loc[ds_salary['Country'].isin(upper_income), 'Upper_Income_Bracket'] = 1\nds_salary.loc[ds_salary['Country'].isin(middle_income), 'Middle_Income_Bracket'] = 1\nds_salary.loc[ds_salary['Country'].isin(lower_income), 'Lower_Income_Bracket'] = 1\n#ds_salary.count()\nds_salary.drop(columns = ['Country'], inplace = True)\nds_salary = ds_salary.fillna(0)\nds_salary.head()","70ca50f9":"ds_salary.Compensation.unique()","ef16f0a8":"order_salary = ['$0-999', '1,000-1,999', '2,000-2,999', '3,000-3,999', '4,000-4,999', '5,000-7,499', '7,500-9,999', \n'10,000-14,999', '15,000-19,999', '20,000-24,999', '25,000-29,999', '30,000-39,999', '40,000-49,999', \n'50,000-59,999', '60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999', '100,000-124,999', \n'125,000-149,999', '150,000-199,999', '200,000-249,999', '250,000-299,999', '300,000-500,000', '> $500,000']\n\nds_salary['Compensation'].value_counts().loc[order_salary].plot(kind='bar')","a6d01ba1":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nds_salary['Compensation'].replace({'$0-999': 'under_60k',\n                                    '1,000-1,999': 'under_60k',\n                                    '2,000-2,999': 'under_60k',\n                                    '3,000-3,999': 'under_60k',\n                                    '4,000-4,999': 'under_60k',\n                                    '5,000-7,499': 'under_60k',\n                                    '7,500-9,999': 'under_60k',\n                                    '10,000-14,999': 'under_60k',\n                                    '15,000-19,999': 'under_60k',\n                                    '20,000-24,999': 'under_60k',\n                                    '25,000-29,999': 'under_60k',\n                                    '30,000-39,999': 'under_60k',\n                                    '40,000-49,999': 'under_60k',\n                                    '50,000-59,999': 'under_60k',\n                                    '60,000-69,999': '60k_125k',\n                                    '70,000-79,999': '60k_125k',\n                                    '80,000-89,999': '60k_125k',\n                                    '90,000-99,999': '60k_125k',\n                                    '100,000-124,999': '60k_125k',\n                                    '125,000-149,999': 'above_125k',\n                                    '150,000-199,999': 'above_125k',\n                                    '200,000-249,999': 'above_125k',\n                                    '250,000-299,999': 'above_125k',\n                                    '300,000-500,000': 'above_125k',\n                                    '> $500,000': 'above_125k'}, inplace = True)\n\n# ds_salary['Compensation'].value_counts()\nds_salary = ds_salary.fillna(0)\n\nY = ds_salary['Compensation']\nX = pd.get_dummies(ds_salary.drop(columns = ['Compensation']))\nscaler = StandardScaler()\nX_Scaled = scaler.fit_transform(X)","5c864300":"new_order_salary = ['under_60k', '60k_125k', 'above_125k']\nds_salary['Compensation'].value_counts().loc[new_order_salary].plot(kind='bar')","e5406f30":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nwarnings.filterwarnings('ignore')","52450e2e":"#Naive Bayes\ngnb = GaussianNB()\ncross_val_NB = cross_val_score(gnb, X_Scaled, Y, cv=5)\nprint('cross_val_NB:')\nprint(cross_val_NB)\nprint('mean: ', cross_val_NB.mean())\n\nlr = LogisticRegression()\ncross_val_LR = cross_val_score(lr, X_Scaled, Y, cv=5)\nprint('cross_val_LR:')\nprint(cross_val_LR)\nprint('mean: ', cross_val_LR.mean())\n\nkn = KNeighborsClassifier()\ncross_val_KN = cross_val_score(kn, X_Scaled, Y, cv=5)\nprint('cross_val_KN:')\nprint(cross_val_KN)\nprint('mean: ', cross_val_KN.mean())\n\nrf = RandomForestClassifier()\ncross_val_RF = cross_val_score(rf, X_Scaled, Y, cv=5)\nprint('cross_val_RF:')\nprint(cross_val_RF)\nprint('mean: ', cross_val_RF.mean())\n\nsv = SVC(probability = True)\ncross_val_SV = cross_val_score(sv, X_Scaled, Y,cv=5)\nprint('cross_val_SV:')\nprint(cross_val_SV)\nprint('mean: ', cross_val_SV.mean())","b1090180":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 60, stop = 150, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n# Minimum number of samples required to split a node\nmin_samples_split = [3, 4, 5, 6, 7, 8, 9]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 3]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","2e1b1e51":"import datetime \n\nprint(datetime.datetime.now())\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 1000, cv = 5, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(X_Scaled, Y)\n\nprint(datetime.datetime.now())","add8a5f4":"rf_random.best_params_","29b76ad4":"rf_tuned = RandomForestClassifier(n_estimators = rf_random.best_params_['n_estimators'],\n                                  min_samples_split = rf_random.best_params_['min_samples_split'],\n                                  min_samples_leaf = rf_random.best_params_['min_samples_leaf'],\n                                  max_features = rf_random.best_params_['max_features'],\n                                  max_depth = rf_random.best_params_['max_depth'],\n                                  bootstrap = rf_random.best_params_['bootstrap'])\ncross_val_RF_tuned = cross_val_score(rf_tuned, X_Scaled, Y, cv=5)\nprint('cross_val_RF_tuned')\nprint(cross_val_RF_tuned)\nprint(cross_val_RF_tuned.mean())","72639394":"# **Table of Contents:** # \n\n**Introduction**\n\n*     Project introduction\n*     Dataset description\n*     Import library\n*     Import dataset\n  \n**Initial data exploration**\n    \n*     Time\n*     Age\n*     Gender\n*     Education\n*     Job\n*     Experience\n\n**Data Visualisation**\n    \n*     Country\n*     Language\n*     Platform\n*     Visualisation tool\n*     Numerical data\n    \n**Deep dive into compensation of data scientists**\n\n* Modelling the problem\n\n         - Problem statement\n         - Background\n         - Method\n\n* Data cleaning and preprocessing\n\n         - Removing invalids\n         - Feature engineering\n         - Encoding\n         - Scaling\n         \n\n* Model building\n\n         - Model selection\n         - Model evaluation\n         - Model tuning\n\n\n* Project extensions\n","b5af5987":"<font size = 5>**Model Building**<\/font>\n\n**Model Selection**\n\nThe below 5 classification models are tested, using cross validation to ensure that the models do not overfit:\n\n    Gaussian Naive Bayes\n    Logistic Regression\n    K Nearest Neighbours\n    Random Forest\n    Support Vector Machines\n    \n**Model Evaluation**\n\nThe mean score of each set (5-fold cross-validation scores) were taken, and the model with the highest accuracy (Random Forest Classifier) was taken to undergo hypertuning.\n\n**Model Tuning**\n\nFor the tuning process, the below parameters were considered:\n\n<font size = 4>**n_estimators**<\/font>:\n        Number of trees in random forest\n        \n<font size = 4>**max_features**<\/font>:        \n        Number of features to consider at every split\n        \n<font size = 4>**max_depth**<\/font>:        \n        Maximum number of levels in tree\n        \n<font size = 4>**min_samples_split**<\/font>:        \n        Minimum number of samples required to split a node\n        \n<font size = 4>**min_samples_leaf**<\/font>:        \n        Minimum number of samples required at each leaf node\n        \n<font size = 4>**bootstrap**<\/font>:        \n        Method of selecting samples for training each tree\n        \nTo shorten the running time, a randomized grid search was used, and the model contributing the best accuracy was adopted as the fine tuned model.\n","ac9e70a0":"<font size=\"6\">**Introduction**<\/font>\n\n**Project introduction**\n\nData science has often been dubbed the \"Sexiest job of the 21st century\". Since this does not constitute a substantial reasoning for wanting to dive into datascience, one often wonders, what makes datascience so special? This project seeks to explore and create visualisation for trends within the datascience industry, so as to help us better understand what makes datasciece the hot topic that it is in today's world. \n\n**Dataset description**\n\nKaggle set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live for 3.5 weeks in October 2020, and garnered 20,036 responses. The dataset can be found in the following link (https:\/\/www.kaggle.com\/c\/kaggle-survey-2020\/data).\n\n**Import library**\n\nI have chosen to use the Pandas library in Python as my main from of data analysis and manipulation. For visualisation, a myriad of tools were used, including Tableau, Seaborn, Matplotlib, among others.\n\nFor the model building and evaluation, the Sklearn library in Python was adopted.\n\n**Import dataset**\n\n(https:\/\/www.kaggle.com\/c\/kaggle-survey-2020\/data)","5abeb8e8":"<font size = 5>**Data cleaning and preprocessing**<\/font>\n\n**Removing Invalids**\n\nFirstly, I removed all responses which did not indicate their yearly compensation, since we will be looking specifically at the compensation packages for data scientists.\n\nI have chosen to look at education as academic education, and as such have decided to group 'Professional Degree' into 'No Degree'. Also, I believe that people with university degrees will tend to be alright with disclosing their education level. As such, I theorize that people who do not disclose be of no university degree, or are currently in pursuit of one. I have taken the liberty to group them together.\n\n**Feature Engineering**\n\nI see from above that even though SQL and Seaborn are the second most widely used programming language and visualization packages in the data science industry, the amount of people that rely solely on these are few and far between. As such, I am prompted to engineer 2 new features, Number_Languages and Number_Visualizations, which reflect the number of programming languages and visualisations tools used by the individual.\n\nIt is also noted that the average compensation for varies greatly across countries. As such, we have split the countries into 3 different groups, labelled 'Lower_Income_Bracket', 'Middle_Income_Bracket' and 'Upper_Income_Bracket', representing an average monthly compensation of below USD 2000, USD 2000-5000, and above USD 5000. This allows for enough data in each classification, while maintaining the notion that there are differences in compensation by country. Another way would be to one hot encode each country individually, but model accuracy would not be guaranteed, since the data for specific countries might be few. Here, the compensation for an individual data scientist is taken as the middle of its range, to facilitate numerical analysis.\n\n**Encoding**\n\nHere, the get_dummies function in pandas is used to encode the gender and education into ['Gender_Man', 'Gender_Woman', 'Education_Bachelor\u2019s degree', 'Education_Doctoral degree', 'Education_Master\u2019s degree', 'Education_No Degree'].\n\n**Scaling**\n\nI proceed to scale the training data using StandardScaler from the sklearn library. StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. The idea behind scaling a dataset is that variables that are measured at different scales do not contribute equally to the model fitting & model learned function and might end up creating a bias. Hence, scaling hopes to reduce this bias.","c102b665":"<font size = 6>**Possible project extensions**<\/font>\n\n* Modelling the problem\n\nAs with everything, the perspective to look at any dataset depends on the individual. Here, I have chosen to look specifically at yearly compensation for data scientists jobs. However, there are many other fields that one could explore, for example, yearly compensation of people who work in USA, or even the amount an organization spends on data science.\n\nAnother aspect that can be looked into is modelling the problem as a regression problem rather than a classification. However, since the yearly compensation is given in ranges, more work must be done in order to build regression models that have a certain degree of accuracy. Even as a classification problem, the number of classes can be chosen differently (Here, 3 classes are chosen, namely, under USD 60,000 per year, USD 60,000 - USD 125,000 a year, and above USD 125,000 a year). Of course, more classes would constitute to a tighter range one can classify the data to, which would indicate a more precise estimation of an individuals compensation, at the expense of model accuracy.\n\n* Data cleaning and preprocessing\n\nOne field to look at for removing invalid data might be the time a respondent takes to finish the survey. Since the survey is not short, one would not expect that the survey be finished within seconds. The survey is also not long enough to warrant days to finish. As such, we can pinpoint slipshod reponses using the time taken to finish the survey. However, I have chosen to use the above to determine such responses. Also, I understand that these dubious timings may arise due to machine error, and after deliberation, decided against omiting such responses.\n\nFeature engineering is another aspect that has endless possiblities, and is one of the characteristics that make  a project unique. Given more time and inspiration, one can always think of new features that might correspond to a more accurate model. Following which, it is also noted that there are many forms of scaling, which can be looked into. Here, we used a standard scaler to transform the data, but it would be interesting to see how different scalers affect the results and accuracy of the models.\n\n* Others\n\nThere are considerably more data scientists receiving yearly compensations of below USD 1000 than any other compensation ranges. This works out to roughly below USD 83 per month, which we understand is insufficient for living expenses in many countries. Also, in these countries, it seems odd that an organization would offer that amount as a salary. Hence, I deduce that these people are either not receiving compensation, or only receiving a token sum. Since many of these people work in organizations that have very few employees, I hypothesize that they may be entrepreneurs working in their own start-ups, or helping a friend. Else, they may be doing data science on the side for their organization, and are paid only for their main job. Hence, one further region to explore would be to omit these individuals, and only calculate those who work professionally, and are fully employed as a data scientist.","20e05910":"<font size=\"6\">**Deep dive into compensation packages for data scientists**<\/font>","644807de":"<font size=\"6\">**Data Visualization**<\/font>","e272a438":"<font size=\"6\">**Initial Data Exploration**<\/font> (Data analysis and thoughts)\n\n**Time**\n\nThe average time taken to finish the survey is about 15 minutes, and 75% of people finish the survey within 16 minutes.\n\nHere, I had to do some adjustment to the data. Upon inspection of the survey, we note that it seems unreasonable to finish the survey within 3 minutes or take a time longer than 2 hours. As such, we can only deduce that these timings must be due to system error. As we have no good way of determining the timing of these erroneous data, I have taken the liberty to remove them before plotting.\n\n**Age**\n\nMore than 56% of the respondents are under 30 years old, and more than 70% are under 35 years old.\nWe can see from the graph that the median age is between 25 and 29, and since the graph is left-skewed, we deduce that the mean age is greater than that.\n\nThis age group is considered to be very young in most industries, but is hardly surprising, as datasciene is a recently emerging field. We note that even though the survey was conducted online, the data should not suffer from much collection bias, since we would expect that people in the datascience industry would generally be acquainted with the internet.\n\n**Gender**\n\nThere are 4 times as many males as females who responded in the survey, 52 people who identify themselves as nonbinary, and 263 people who are not comfortable with coming forth about their gender.\n\nThe total number of respondants who did not respond as Male or Female make up less than 2% of the total. As such, if we decide to explore deeper into gender differences, dropping these responses should not cause too much hurt to our data.\n\n**Education**\n\nHaving a Master's degree is the most common education level for respondents in this survey. More than 75% of repondents have a Bachelor's or Master's degree.\n\n**Job**\n\nMost of the respondents are students (making up 25% of the total). Most of the employed respondents seem to be in the tech industry, which comprises datascience and software engineering. Business analysts and product managers make up only 7-8% of the total responses.\n\nSince less than 14% of repondents do not have a college degree, we deduce that students doing the survey input their education level as the level they are currently studying.\n\n**Country**\n\nIndia and USA have more than 2000 responses, while the rest of the countries are all below 700. 60% of the reponses are from the top 9 countries, namely, India, USA, Brazil, Japan, Russia, UK, Nigeria, China and Germany.","71d60c82":"<font size=\"5\">**Modelling the problem**<\/font>\n\n**Problem statement**\n\nTo build a model that can effectively determine the pay range of a data scientist.\n\n**Background**\n\nAs a Master's graduated looking for opportunities to start a career as a data scientist, I would like to use the data at hand to predict the salary of an individual working as a data science proffesional.\n\n**Method**\n\nIt is noted that the compensation packages are given in ranges (categorical). While the average can be taken so allow for easier exploration and visualization, it is not conducive to model the problem as a regression problem based on the average of each pay range.\n\nAs such, the problem shall be modelled into a classification problem. Key features of the data corresponding to an individual data scientist shall be used to predict the salary. To ensure that overfitting does not occur, cross validation shall be used while training the models, and the model with the best mean cross validation score shall be used. Possible extensions would include fine tuning the model, or blending models to obtain a better accuracy."}}