{"cell_type":{"80ff93ea":"code","19d1424c":"code","4d8ba0df":"code","7e025ec4":"code","fad42eb3":"code","b0cc50d9":"code","c8d7dd88":"code","961e938e":"code","63297ad8":"code","f89a84bb":"code","9358f364":"code","a90f7a93":"code","19e89d2f":"code","5e7c2388":"code","005759cd":"code","0d96cf82":"code","ec62767e":"code","51054aa2":"code","c6933281":"code","1d21be97":"code","9228ab21":"code","cb3f8211":"code","d9e12ef3":"code","b484cfaf":"code","826639d5":"code","c56ce5e5":"code","5cb9b905":"code","bb4043ca":"code","52cd1d9e":"code","b625f86e":"code","73d6e1e1":"code","d9d8bef8":"code","3519f85e":"code","0d3a3a9c":"code","7353d00d":"code","4c399233":"markdown","506881a7":"markdown","329c1dc2":"markdown","7fda17f0":"markdown","6f9bc031":"markdown","18443247":"markdown","14ace547":"markdown","7a3c7fe6":"markdown","702bae8b":"markdown","d79aa348":"markdown","38abe143":"markdown","2abdc426":"markdown","6137c472":"markdown","bc48d8a1":"markdown","86ead174":"markdown"},"source":{"80ff93ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19d1424c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import svm, tree\nimport xgboost\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\n%matplotlib inline","4d8ba0df":"# load the preprocessed CSV data\ndata = pd.read_excel('\/kaggle\/input\/employee-absenteeism\/Absenteeism_at_work_Project.xls')","7e025ec4":"#display some information about data\ndata.info()","fad42eb3":"#upon examination, there will be no need for the ID collumn, so it will be dropped\ndata = data.drop(['ID'], axis = 1)","b0cc50d9":"# Quick check on the 'Reason for absence column'\nsorted(data['Reason for absence'].unique())","c8d7dd88":"## Given the below meaning for the categoy of CIDs I will grup for better repesentation ##\n'''\nI Certain infectious and parasitic diseases\nII Neoplasms\nIII Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism\nIV Endocrine, nutritional and metabolic diseases\nV Mental and behavioural disorders\nVI Diseases of the nervous system\nVII Diseases of the eye and adnexa\nVIII Diseases of the ear and mastoid process\nIX Diseases of the circulatory system\nX Diseases of the respiratory system\nXI Diseases of the digestive system\nXII Diseases of the skin and subcutaneous tissue\nXIII Diseases of the musculoskeletal system and connective tissue\nXIV Diseases of the genitourinary system\nXV Pregnancy, childbirth and the puerperium\nXVI Certain conditions originating in the perinatal period\nXVII Congenital malformations, deformations and chromosomal abnormalities\nXVIII Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified\nXIX Injury, poisoning and certain other consequences of external causes\nXX External causes of morbidity and mortality\nXXI Factors influencing health status and contact with health services.\n\nAnd 7 categories without (CID) patient follow-up (22), medical consultation (23), blood donation \n(24), laboratory examination (25), unjustified absence (26), physiotherapy (27), dental consultation (28).'''\n\n### the grouping is as follows\n'''\n1- 14 are various diseases\n15 -17 : pregnancy and given birth related\n18-21: poisons or diseases not elsewere categorise\n22 and above : light reason or less serious reasons\n\n'''\n### then apply the function below to the dataframe\n\n\ndef Reason(data):\n    if data['Reason for absence'] < 15 :\n        d = 'R_Known'\n    elif data['Reason for absence'] >= 15 and data['Reason for absence'] <= 17  :\n        d = 'R_Preg_Birth'\n    elif data['Reason for absence'] >= 18 and data['Reason for absence'] <= 21  :\n        d = 'R_Pois_unclass'\n    elif data['Reason for absence'] == 22:\n        d = 'R_NotSerious'\n    else:\n        d = 'R_NotSerious'\n    return d\n\ndata['ReasonGroups'] = data.apply(Reason, axis=1)\n\n# so lets drop the Reasons for absence column because is no more useful\ndata = data.drop(['Reason for absence'], axis = 1)\n","961e938e":"#lets get dummies\nR_dummies = pd.get_dummies(data['ReasonGroups'])\n\n#lets merge it\ndata = pd.concat([data, R_dummies], axis = 1)\n\ndata = data.drop(['ReasonGroups'], axis = 1)\ndata.head(10)","63297ad8":"# data.isnull() # shows a df with the information whether a data point is null \n# Since True = the data point is missing, while False = the data point is not missing, we can sum them\n# This will give us the total number of missing values feature-wise\ndata.isnull().sum()","f89a84bb":"# As we can see there are quite a few missing data and for a datset with just 740 rows\n# it is not a good idea to drop this rows with missiing data so different method will be used to fill the columns\n# so let take a look at the spread of each column\n\n# Visulazing the distibution of the data for every feature\ndata.hist(linewidth=1, histtype='stepfilled', facecolor='g', figsize=(20, 20));","9358f364":"#from the chat above we can make some assumption on how best to fill the missing data\n# 1. Transportation expense ,Age, Distance from Residence to Work,Service time, Work load Average\/day,Hit \n#    target,Weight, Height,Body mass index \n#    we will fill with there mean value\n#2.  Disciplinary failure,Education,Son,Social drinker ,Social smoker, Pet ,Absenteeism time in hours  \n#    we will fill with 0\n#3.  Month of absence also have values from 0 - 12 and a missing, since we know there are only 12 month in a year, \n#    we can infer that there is an issue with the data, will deal with this later\n\n#so lets start by filling the 2nd list of variables\ndata[['Month of absence','Disciplinary failure','Education','Son','Social drinker' ,'Social smoker', 'Pet' ,'Absenteeism time in hours']] = data[['Month of absence','Disciplinary failure','Education','Son','Social drinker' ,'Social smoker', 'Pet' ,'Absenteeism time in hours']].fillna(0)\n\n#then we proceed with the rest\ndata = data.fillna(data.mean())\n\ndata.isnull().sum()\n###\n# lets drop rows where the month is not a recognised value e.g. btw 1 and 12\ndata = data[data['Month of absence']>0]\ndata.head(10)","a90f7a93":"#then we proceed with the rest\ndata.isnull().sum()\n","19e89d2f":"#lets look at the basic description of the data\ndata.describe()","5e7c2388":"# Data looks fairly OK to me. No obvious error\n# so let compute our Target which will be from 'Absenteeism time in hours'\n# This taret will be ctegorical and I have decided to use median rather than chosing an abitrary cut off, which might \n# make the data unbalanced. this reason for this is because we have very few rows of data.\ntargets = np.where(data['Absenteeism time in hours'] > data['Absenteeism time in hours'].median(),1,0)\n#let do a quick check if targets is balanced in the data\ntargets.sum() \/ targets.shape[0]","005759cd":"# trgets looks good to me because it shows a ratio of 55\/45\n\n# create a Series in the original data frame that will contain the targets for the regression\ndata['Absenteeism'] = targets\n#drop the old absenteesim column\nadata = data.drop(['Absenteeism time in hours'], axis = 1)\nadata = adata.reset_index(drop=True)","0d96cf82":"# check what happened\n# maybe manually see how the targets were created\nadata\n#targets.shape","ec62767e":"# Create a variable that will contain the inputs (everything without the targets)\nunscaled_inputs = adata.iloc[:,:-1]\ntargets = adata['Absenteeism']\n","51054aa2":"\n# define scaler as an object\nabsenteeism_scaler = StandardScaler()","c6933281":"# create the Custom Scaler class\n\nclass CustomScaler(BaseEstimator,TransformerMixin): \n    \n    # init or what information we need to declare a CustomScaler object\n    # and what is calculated\/declared as we do\n    \n    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n        \n        # scaler is nothing but a Standard Scaler object\n        self.scaler = StandardScaler(copy,with_mean,with_std)\n        # with some columns 'twist'\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n        \n    \n    # the fit method, which, again based on StandardScale\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n    \n    # the transform method which does the actual scaling\n\n    def transform(self, X, y=None, copy=None):\n        \n        # record the initial order of the columns\n        init_col_order = X.columns\n        \n        # scale all features that you chose when creating the instance of the class\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        \n        # declare a variable containing all information that was not scaled\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        \n        # return a data frame which contains all scaled features and all 'not scaled' features\n        # use the original order (that you recorded in the beginning)\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","1d21be97":"# choose the columns to scale\n# we later augmented this code and put it in comments\n# select the columns to omit\ncolumns_to_omit = ['R_Known', 'R_NotSerious', 'R_Pois_unclass', 'R_Preg_Birth','Education']","9228ab21":"# create the columns to scale, based on the columns to omit\n# use list comprehension to iterate over the list\ncolumns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]","cb3f8211":"# declare a scaler object, specifying the columns you want to scale\nabsenteeism_scaler = CustomScaler(columns_to_scale)\n# fit the data (calculate mean and standard deviation); they are automatically stored inside the object \nabsenteeism_scaler.fit(unscaled_inputs)","d9e12ef3":"# standardizes the data, using the transform method \n# in the last line, we fitted the data - in other words\n# we found the internal parameters of a model that will be used to transform data. \n# transforming applies these parameters to our data\n# note that when you get new data, you can just call 'scaler' again and transform it in the same way as now\nscaled_inputs = absenteeism_scaler.transform(unscaled_inputs)\n#scaled_inputs= scaled_inputs.dropna()","b484cfaf":"# the scaled_inputs are now an ndarray, because sklearn works with ndarrays\nscaled_inputs","826639d5":"# check how this method works\ntrain_test_split(scaled_inputs, targets)","c56ce5e5":"# declare 4 variables for the split\nx_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, #train_size = 0.75, \n                                                                            test_size = 0.25, random_state = 20)","5cb9b905":"# check the shape of the train inputs and targets\nprint (x_train.shape, y_train.shape)","bb4043ca":"# check the shape of the test inputs and targets\nprint (x_test.shape, y_test.shape)","52cd1d9e":"#Now, we will create an array of Classifiers and append different classification models to our array\nclassifiers = [] \n\nmod1 = xgboost.XGBClassifier()\nclassifiers.append(mod1)\nmod2 = svm.SVC()\nclassifiers.append(mod2)\nmod3 = RandomForestClassifier()\nclassifiers.append(mod3)\nmod4 = LogisticRegression()\nclassifiers.append(mod4)\nmod5 = KNeighborsClassifier(3)\nclassifiers.append(mod5)\nmod6 = AdaBoostClassifier()\nclassifiers.append(mod6)\nmod7= GaussianNB()\nclassifiers.append(mod7)","b625f86e":"#Lets fit the models into anarray\n\nfor clf in classifiers:\n    clf.fit(x_train,y_train)\n    y_pred= clf.predict(x_test)\n    y_tr = clf.predict(x_train)\n    acc_tr = accuracy_score(y_train, y_tr)\n    acc = accuracy_score(y_test, y_pred)\n    mn = type(clf).__name__\n    \n    print(clf)\n    print(\"Accuracy of trainset %s is %s\"%(mn, acc_tr))\n    print(\"Accuracy of testset %s is %s\"%(mn, acc))\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix of testset %s is %s\"%(mn, cm))\n","73d6e1e1":"# So we stick with SVC as the best model in this case\n\nSVCclf = svm.SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\nSVCclf.fit(x_train,y_train)\nypredtrain = SVCclf.predict(x_train)\ny_pred = SVCclf.predict(x_test)\nacc_tr = accuracy_score(y_train, ypredtrain)\nacc = accuracy_score(y_test, y_pred)\nprint(acc_tr, acc)\n","d9d8bef8":"# pickle the model file\nwith open('model', 'wb') as file:\n    pickle.dump(SVCclf, file)","3519f85e":"param = [{'C': [1, 10, 20], 'gamma': ['scale', 'auto'], 'kernel': ['rbf','linear']}]\n\n# Create a classifier object with the classifier and parameter candidates\nsvcclf = GridSearchCV(estimator=svm.SVC(), param_grid=param)\n\n# Train the classifier on data1's feature and target data\nsvcclf.fit(x_train, y_train)  \n\n# View the accuracy score\nprint('Best score for data1:', svcclf.best_score_) \n\n\n# View the best parameters for the model found using grid search\nprint('Best C:',svcclf.best_estimator_.C) \nprint('Best Kernel:',svcclf.best_estimator_.kernel)\nprint('Best Gamma:',svcclf.best_estimator_.gamma)","0d3a3a9c":"# inteesting result but SVC & Adaboost model seems to be doing very well, \n#So let see if we can improve on accuracy by tunning some parameters, otherwise we stick to the default parameters\n#lets start with Adaboost\n\n#Creating a grid of hyperparameters\nboost = AdaBoostClassifier(base_estimator=None)\nparameters = {'n_estimators': (50,100,150,200),\n              'learning_rate': (0.1,0.5,1, 2)}\nadab = GridSearchCV(boost, parameters)\nadab.fit(x_train, y_train)\nscore = adab.best_score_\nparam = adab.best_params_\nprint(score)\nprint(param)","7353d00d":"# create a logistic regression object\nrr = LogisticRegression()\n# fit our train inputs\n# that is basically the whole training part of the machine learning\nrr.fit(x_train,y_train)\nrr.score(x_train,y_train)\n# save the names of the columns in an ad-hoc variable\nfeature_name = unscaled_inputs.columns.values\ndf = pd.DataFrame (columns=['Feature name'], data = feature_name)\n# add the coefficient values to the df\ndf['Coefficient'] = np.transpose(rr.coef_)\n# move all indices by 1\ndf.index = df.index + 1\n# add the intercept at index 0\ndf.loc[0] = ['Intercept', rr.intercept_[0]]\n# sort the df by index\ndf = df.sort_index()\n# create a new Series called: 'Odds ratio' which will show the.. odds ratio of each feature\ndf['Odds_ratio'] = np.exp(df.Coefficient)\n# sort the table according to odds ratio\ndf.sort_values('Odds_ratio', ascending=False)","4c399233":"## Adaboost parameter search","506881a7":"## Thats the end of the project, we can do abit more by tunning parameters for SVC and Adaboost by following the below process,\n## we can also look at logistic regression model by examing the odds and removing some features and retraining the model","329c1dc2":"## SVC parameter search","7fda17f0":"## Standardise data","6f9bc031":"## Check for Missiing data","18443247":"# Employee Absenteeism Project Work","14ace547":"## The End","7a3c7fe6":"## ML Applications","702bae8b":"## Load the data","d79aa348":"## Data Preprocessing","38abe143":"## Split the data into train & test and shuffle","2abdc426":"## Logistic regression with sklearn to determine important features by examing Odds","6137c472":"## Import the relevant libraries","bc48d8a1":"## Create the targets","86ead174":"## Save the model"}}