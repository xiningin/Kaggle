{"cell_type":{"647cc661":"code","d894f95e":"code","dc5dc7c2":"code","c9a40c43":"code","7011b60b":"code","0311a51d":"code","0009d620":"code","4786234d":"code","f6b4b585":"code","8fe51aef":"code","f313bd5a":"code","d679a6a6":"code","fd1cd3eb":"code","51417099":"code","f8ddfad8":"code","7e488c8a":"code","2cb101a6":"code","3af79b00":"code","d4daa6d7":"code","56ec957c":"code","9752af7d":"code","8673850b":"code","71425e6c":"code","6caeae5d":"code","c4431720":"code","2c4a27ab":"code","da290381":"code","d748dee6":"code","b37b81d4":"code","2d50cc79":"code","0ac14779":"code","8eacc8db":"code","8c4764ac":"markdown","edc939cc":"markdown","f318e5b1":"markdown","07ebda55":"markdown","e67659b0":"markdown","01b33700":"markdown","3b566686":"markdown","0649d707":"markdown","5fa9a1ee":"markdown","c826e905":"markdown","c9c920ca":"markdown","e124639b":"markdown","bcb42ab1":"markdown","e931ed95":"markdown","b869fe65":"markdown","e3481feb":"markdown","34cec5ac":"markdown","2063f80f":"markdown","ffcf24db":"markdown","81b8debf":"markdown","be99023d":"markdown","cc71a293":"markdown","ccb1b5ff":"markdown"},"source":{"647cc661":"#!pip install optuna #for local system\nimport numpy as np \nimport pandas as pd \nimport os\nimport warnings\nimport math \n!pip install plotly==4.1.0\nimport plotly\nfrom plotly.offline import iplot\nwarnings.filterwarnings(\"ignore\")\nprint(plotly.__version__)\nimport seaborn as sns\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nimport xgboost as xgb\nimport optuna\nimport optuna.integration.lightgbm as lgb\nfrom sklearn.model_selection import RepeatedKFold, KFold, cross_val_score, ShuffleSplit\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom xgboost import XGBRegressor\n\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nimport optuna\nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler","d894f95e":"path = r\"\/kaggle\/input\/boston-house-prices\/housing.csv\"","dc5dc7c2":"header_names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATION', 'B', 'LSTAT', 'PRICE']\ndf = pd.read_csv(path, names=header_names, delim_whitespace=True)\ndf.head()","c9a40c43":"df.dtypes","7011b60b":"df.describe()","0311a51d":"df.isnull().sum()","0009d620":"pp.ProfileReport(df)","4786234d":"#Our Target is to predict PRICE Column which contains contonous data\nX = df.iloc[:,:-1]\ny = df[\"PRICE\"]","f6b4b585":"sxc = StandardScaler()\nX = sxc.fit_transform(X)","8fe51aef":"from xgboost import XGBRegressor\nX_train, X_test, y_train, y_test = train_test_split( X , y, test_size=0.33)\n\n\nmy_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[( X_test, y_test)], verbose=False)\n\n# make predictions\npredictions = my_model.predict(X_test)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolude Error is {}\".format( (mean_absolute_error(predictions, y_test))))\nprint(\"Mean Squared Error is {}\".format( math.sqrt(mean_absolute_error(predictions, y_test))))","f313bd5a":"n_splits = 10\ncross_val = KFold( n_splits  = n_splits, random_state = 0)\nxgb_reggressor = xgb.XGBRegressor()\n#scoring= [\"neg_root_mean_squared_error\", \"r2\" ]\nscores = cross_val_score(xgb_reggressor, X_train, y_train, scoring=\"neg_root_mean_squared_error\" ,n_jobs=-1, cv= cross_val )\n\nprint(\"scores of {} split cross validation are: {}\".format(n_splits, scores))\nprint(\"mean of the scores is {}, standard deviation of the scores is {}\".format( np.mean(scores),np.std(scores) ))","d679a6a6":"#X_train, X_test, y_train, y_test = train_test_split( X , y, test_size=0.33)\n\n#Store the train and test data into XGBOOST Matrix form\ndf_train = xgb.DMatrix(X_train,y_train)\ndf_test = xgb.DMatrix(X_test, y_test)\n","fd1cd3eb":"def RMSE ( params ):\n    model = xgb.train(params,df_train, evals=[(df_test,\"eval\")],  num_boost_round = 600 , verbose_eval = 0 , early_stopping_rounds = 64)\n    perf = model.eval(df_test)\n    perf=float( perf.split(\":\")[1] ) \n    return perf","51417099":"base_score = RMSE(params = {})\nprint(\"base score with default parameters is {}\".format (base_score))","f8ddfad8":"def objective_function ( trial ):\n    paramsx = {\"max_depth\": trial.suggest_int(\"max_depth\",3,10),\n              \"reg_alpha\": trial.suggest_uniform(\"reg_alpha\",2, 6),\n              \"reg_lambda\": trial.suggest_uniform(\"reg_lambda\",0,2),\n              \"min_child_weight\": trial.suggest_int(\"min_child_weight\",0,5),\n              \"gamma\": trial.suggest_uniform(\"gamma\",0,4),\n              \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.1),\n              \"colsample_bytree\" : trial.suggest_uniform(\"colsample_bytree\",0.4,0.9),\n              \"subsample\": trial.suggest_uniform(\"subsample\",0.4,0.9),\n              \"nthread\": -1 \n        \n  \n    }\n    return (RMSE(paramsx)) # For the minimizing optimization it should return a score, here we return RMSE score\n    ","7e488c8a":"optuna.logging.set_verbosity(optuna.logging.WARNING) \nstudy = optuna.create_study(direction=\"minimize\", sampler = TPESampler())\nstudy.optimize(objective_function, n_trials = 1000 , show_progress_bar=True)","2cb101a6":"trial = study.best_trial\nbest_score = (trial.value)\nprint(\"Accuracy is {}\".format(trial.value))","3af79b00":"impr=  int((base_score\/best_score*100)-100)\nprint(\"The accuracy with tunning has improved from {} to {} by {} %\".format(base_score, best_score,impr))\n","d4daa6d7":"best_param={}\nbest_param.update(study.best_params)\nprint(best_param)","56ec957c":"from plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\noptuna.visualization.plot_optimization_history(study)","9752af7d":"optuna.visualization.plot_slice(study)","8673850b":"def objective_function2 ( trial ):\n    paramsx = {\"max_depth\": trial.suggest_int(\"max_depth\",9,9),\n              \"reg_alpha\": trial.suggest_uniform(\"reg_alpha\",2, 4),\n              \"reg_lambda\": trial.suggest_uniform(\"reg_lambda\",0,0.42),\n              \"min_child_weight\": trial.suggest_int(\"min_child_weight\",3,3),\n              \"gamma\": trial.suggest_uniform(\"gamma\",1.4,3.75),\n              \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.034, 0.09),\n              \"colsample_bytree\" : trial.suggest_uniform(\"colsample_bytree\",0.60,0.85),\n              \"subsample\": trial.suggest_uniform(\"subsample\",0.40,0.6),\n              \"nthread\": -1 \n        \n  \n    }\n    return (RMSE(paramsx)) # For the minimizing optimization it should return a score, here we return RMSE score\n    ","71425e6c":"optuna.logging.set_verbosity(optuna.logging.WARNING) \nstudy2 = optuna.create_study(direction=\"minimize\", sampler = TPESampler())\nstudy2.optimize(objective_function2, n_trials = 1020 , show_progress_bar=True)","6caeae5d":"trial = study2.best_trial\nbest_score2 = (trial.value)\nprint(\"Accuracy is {}\".format(trial.value))","c4431720":"impr2=  int((best_score\/best_score2*100)-100)\nprint(\"The accuracy with narrowing has improved from {} to {} by {} %\".format(best_score, best_score2,impr2))\n","2c4a27ab":"optuna.visualization.plot_optimization_history(study2)","da290381":"optuna.visualization.plot_slice(study2)","d748dee6":"final_best_param={}\nfinal_best_param.update(study2.best_params)\nprint(final_best_param)","b37b81d4":"final_model = xgb.train(final_best_param, df_train, num_boost_round = 600, evals=[(df_test,\"eval\")],early_stopping_rounds=64,verbose_eval=1)","2d50cc79":"xgb_boss = xgb.XGBRegressor ( **final_best_param, n_estimators = 260 )\nxgb_base = xgb.XGBRegressor ( )","0ac14779":"xgb_base.fit(X_train,y_train)\nscore_base = xgb_base.score(X_test,y_test)","8eacc8db":"xgb_boss.fit(X_train,y_train)\nscore_boss = xgb_boss.score(X_test,y_test)\nperc = score_base\/score_boss*100\nprint(\"We achieved to increase XGB R2 score from {} to {}\".format(score_base, score_boss, perc))","8c4764ac":"Finding the best n_estimators by using Early Stopping","edc939cc":"### Usage of XGBoost 1:  with .fit Training","f318e5b1":"* In below i will show 3 tyeps of using XGBoosting, in the following parts of the note book we will use the 3rd one. However, for a better solution the combination of 2nd and 3rd must be use which means OPTUNA should be used with cross validation.\n\n* A\u015fa\u011f\u0131da XGBoosting kullan\u0131m\u0131n\u0131n 3 \u00e7e\u015fidini g\u00f6sterece\u011fim, not defterinin ilerleyen k\u0131s\u0131mlar\u0131nda 3.y\u00fc kullanaca\u011f\u0131z. Ancak daha iyi bir \u00e7\u00f6z\u00fcm i\u00e7in 2. ve 3. kombinasyonu kullan\u0131lmal\u0131d\u0131r, yani OPTUNA \u00e7apraz do\u011frulama ile kullan\u0131lmal\u0131d\u0131r.\n\n* \u0928\u0940\u091a\u0947 \u092e\u0948\u0902 XGBoosting \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0928\u0947 \u0915\u0947 3 \u092a\u094d\u0930\u0915\u093e\u0930 \u0926\u093f\u0916\u093e\u090a\u0902\u0917\u093e, \u0928\u094b\u091f \u092c\u0941\u0915 \u0915\u0947 \u0928\u093f\u092e\u094d\u0928\u0932\u093f\u0916\u093f\u0924 \u092d\u093e\u0917\u094b\u0902 \u092e\u0947\u0902 \u0939\u092e 3 \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0947\u0902\u0917\u0947\u0964 \u0939\u093e\u0932\u093e\u0902\u0915\u093f, \u092c\u0947\u0939\u0924\u0930 \u0938\u092e\u093e\u0927\u093e\u0928 \u0915\u0947 \u0932\u093f\u090f \u0926\u0942\u0938\u0930\u0947 \u0914\u0930 \u0924\u0940\u0938\u0930\u0947 \u0915\u0947 \u0938\u0902\u092f\u094b\u091c\u0928 \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u093f\u092f\u093e \u091c\u093e\u0928\u093e \u091a\u093e\u0939\u093f\u090f \u091c\u093f\u0938\u0915\u093e \u0905\u0930\u094d\u0925 \u0939\u0948 \u0915\u093f \u0911\u092a\u094d\u091f\u0941\u0928\u093e \u0915\u094b \u0915\u094d\u0930\u0949\u0938 \u0938\u0924\u094d\u092f\u093e\u092a\u0928 \u0915\u0947 \u0938\u093e\u0925 \u0909\u092a\u092f\u094b\u0917 \u0915\u093f\u092f\u093e \u091c\u093e\u0928\u093e \u091a\u093e\u0939\u093f\u090f\u0964","07ebda55":"Libraries: we need plotly > 4.0.0 to use optuna plots. We will use xgboost as well. And we will use sklearn for metrics.\n\n\nK\u00fct\u00fcphaneler: optuna arsalar\u0131n\u0131 kullanmak i\u00e7in arsa> 4.0.0'a ihtiyac\u0131m\u0131z var. Biz de xgboost kullanaca\u011f\u0131z. Ve metrikler i\u00e7in sklearn kullanaca\u011f\u0131z.\n\n\n\u092a\u0941\u0938\u094d\u0924\u0915\u093e\u0932\u092f: \u0911\u092a\u094d\u091f\u0941\u0928\u093e \u092d\u0942\u0916\u0902\u0921\u094b\u0902 \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0939\u092e\u0947\u0902 \u092a\u094d\u0932\u0949\u091f\u0932\u0940> 4.0.0 \u0915\u0940 \u0906\u0935\u0936\u094d\u092f\u0915\u0924\u093e \u0939\u094b\u0924\u0940 \u0939\u0948\u0964 \u0939\u092e xgboost \u0915\u093e \u092d\u0940 \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0947\u0902\u0917\u0947\u0964 \u0914\u0930 \u0939\u092e \u092e\u0947\u091f\u094d\u0930\u093f\u0915\u094d\u0938 \u0915\u0947 \u0932\u093f\u090f sklearn \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0947\u0902\u0917\u0947\u0964\n\n> !pip install optuna\n\n> import optuna\n\n> import xgboost as xgb\n","e67659b0":"## $\\color{Pink}{\\text{Chapter 1. EDA}}$ <a class=\"anchor\" id=\"chapter1\"><\/a>","01b33700":"### Usage Usage of XGBoost 2:  with KFold Cross Validation","3b566686":"**ENG:** OPTUNA is an auto hyperparameter optimization software framework. It is one of the most efficient optimizers for machine learning and deep learning. It has many advantages according to other traditional methods like grid search or random search. The ultimate goal is to find out the optimum set of hyperparameters through multiple iterations. In optuna there are 2 main terms:\n\n1. Study : optimization based on an objective function\n2. Trial : a single execution of the objective function\n\nBy the way , we do not tune n_estimator because it is really keen on overfitting, we will control the n_estimator with early_stopping_rounds to stop the training when the generalization error starts to increase successively in the defined rounds (that means when overfitting occurs).\n\n**TR:**  OPTUNA, bir otomatik hiperparametre optimizasyon yaz\u0131l\u0131m\u0131 \u00e7er\u00e7evesidir. Makine \u00f6\u011frenimi ve derin \u00f6\u011frenme i\u00e7in en verimli optimize edicilerden biridir. Izgara arama veya rastgele arama gibi di\u011fer geleneksel y\u00f6ntemlere g\u00f6re bir\u00e7ok avantaj\u0131 vard\u0131r. Nihai hedef, \u00e7oklu yinelemeler yoluyla optimum hiperparametre setini bulmakt\u0131r. optuna'da 2 ana terim vard\u0131r:\n\n\n1. \u00c7al\u0131\u015fma : bir ama\u00e7 fonksiyonuna dayal\u0131 optimizasyon\n2. Deneme : ama\u00e7 fonksiyonunun tek bir uygulamas\u0131\n\n**HINDI:** OPTUNA \u090f\u0915 \u0911\u091f\u094b \u0939\u093e\u0907\u092a\u0930\u092a\u0948\u0930\u093e\u092e\u0940\u091f\u0930 \u0911\u092a\u094d\u091f\u093f\u092e\u093e\u0907\u091c\u0947\u0936\u0928 \u0938\u0949\u092b\u094d\u091f\u0935\u0947\u092f\u0930 \u092b\u094d\u0930\u0947\u092e\u0935\u0930\u094d\u0915 \u0939\u0948\u0964 \u092f\u0939 \u092e\u0936\u0940\u0928 \u0932\u0930\u094d\u0928\u093f\u0902\u0917 \u0914\u0930 \u0921\u0940\u092a \u0932\u0930\u094d\u0928\u093f\u0902\u0917 \u0915\u0947 \u0932\u093f\u090f \u0938\u092c\u0938\u0947 \u0915\u0941\u0936\u0932 \u0911\u092a\u094d\u091f\u093f\u092e\u093e\u0907\u091c\u093c\u0930 \u092e\u0947\u0902 \u0938\u0947 \u090f\u0915 \u0939\u0948\u0964 \u0917\u094d\u0930\u093f\u0921 \u0938\u0930\u094d\u091a \u092f\u093e \u0930\u0948\u0902\u0921\u092e \u0938\u0930\u094d\u091a \u091c\u0948\u0938\u0947 \u0905\u0928\u094d\u092f \u092a\u093e\u0930\u0902\u092a\u0930\u093f\u0915 \u0924\u0930\u0940\u0915\u094b\u0902 \u0915\u0947 \u0905\u0928\u0941\u0938\u093e\u0930 \u0907\u0938\u0915\u0947 \u0915\u0908 \u092b\u093e\u092f\u0926\u0947 \u0939\u0948\u0902\u0964 \u0905\u0902\u0924\u093f\u092e \u0932\u0915\u094d\u0937\u094d\u092f \u0915\u0908 \u092a\u0941\u0928\u0930\u093e\u0935\u0943\u0924\u094d\u0924\u093f\u092f\u094b\u0902 \u0915\u0947 \u092e\u093e\u0927\u094d\u092f\u092e \u0938\u0947 \u0939\u093e\u0907\u092a\u0930\u092a\u0948\u0930\u093e\u092e\u0940\u091f\u0930 \u0915\u0947 \u0907\u0937\u094d\u091f\u0924\u092e \u0938\u0947\u091f \u0915\u093e \u092a\u0924\u093e \u0932\u0917\u093e\u0928\u093e \u0939\u0948\u0964 \u0911\u092a\u094d\u091f\u0941\u0928\u093e \u092e\u0947\u0902 2 \u092e\u0941\u0916\u094d\u092f \u0936\u092c\u094d\u0926 \u0939\u0948\u0902:\n\n\n1. al\u0131\u015fma : \u092c\u0940\u0930 \u0905\u092e\u093e\u0915 \u092b\u094b\u0902\u0915\u094d\u0938\u093f\u092f\u094b\u0928\u0941\u0928\u093e \u0926\u092f\u093e\u0932\u0131 \u0911\u092a\u094d\u091f\u093f\u092e\u093f\u091c\u093c\u093e\u0938\u094d\u092f\u094b\u0928\n2. \u0921\u0947\u0928\u0947\u092e\u0947 : \u0905\u092e\u093e\u0915 \u092b\u094b\u0902\u0915\u0938\u0940\u092f\u094b\u0928\u0941\u0928 \u091f\u0947\u0915 \u092c\u0940\u0930 \u0909\u092f\u0917\u0941\u0932\u093e\u092e\u093e\u0938\u0940\n\nsource: https:\/\/optuna.readthedocs.io\/en\/stable\/","0649d707":"# Tunning One More Time with Using of Slice Plot","5fa9a1ee":"\n## $\\color{orange}{\\text{Table of Contents}}$\n\n* [Chapter 1. EDA](#chapter1)     \n* [Chapter 2. XGBoost](#chapter2)\n* [Chapter 3. OPTUNA](#chapter3)\n* [Chapter 4. Conclusion](#chapter4)\n","c826e905":"We will narrow the searching intervals as below:\n* colsample_bytree --> between 0.60 and 0.85\n* gamma  --> between 1.40 and 3.75\n* learning_rate --> 0.034 and 0.090\n* reg_alpha --> 2 and 4\n* reg_lambda --> 0. and 0.4\n* subsample --> 0.40 and 0.6\n* max_depty --> 9\n* min_child_weight --> 3","c9c920ca":"## ****$\\color{orange}{\\text{Welcome! Ho\u015fgeldiniz ! \u0938\u094d\u0935\u093e\u0917\u0924 ! }}$****\n##### ****$\\color{orange}{\\text{Learn XGBoost Regressor with OPTUNA Hyperparameter Tunning with me ! Upvote if you like this work}}$****\n\n## ****$\\color{orange}{\\text{Introduction}}$****\n\n#### $\\color{purple}{\\text{English: }}$\n\nWe will try to use optuna on the \"boston housing prices\" dataset. I will try to go step by step for understanding the data, regression model and Optuna framework for hyperparameter tuning. So keep in tune and please follow all the notebook to understand the XGBoost and Optuna. And please upvoted if you like this work and if you want me to continue to share more!\n\n#### $\\color{purple}{\\text{Turkish: }}$\n\n\"Boston konut fiyatlar\u0131\" veri setinde optuna kullanmaya \u00e7al\u0131\u015faca\u011f\u0131z. Hiperparametre ayarlama i\u00e7in verileri, regresyon modelini ve Optuna \u00e7er\u00e7evesini anlamak i\u00e7in ad\u0131m ad\u0131m gitmeye \u00e7al\u0131\u015faca\u011f\u0131m. Bu nedenle, XGBoost ve Optuna'y\u0131 anlamak i\u00e7in l\u00fctfen t\u00fcm not defterini takip edin ve uyum sa\u011flay\u0131n. Ve bu \u00e7al\u0131\u015fmay\u0131 be\u011fendiyseniz ve daha fazlas\u0131n\u0131 payla\u015fmaya devam etmemi istiyorsan\u0131z l\u00fctfen oy verin!\n\n#### $\\color{purple}{\\text{Hindi: }}$\n\n\u0939\u092e \"\u092c\u094b\u0938\u094d\u091f\u0928 \u0939\u093e\u0909\u0938\u093f\u0902\u0917 \u092a\u094d\u0930\u093e\u0907\u0938\" \u0921\u0947\u091f\u093e\u0938\u0947\u091f \u092a\u0930 \u0911\u092a\u094d\u091f\u0941\u0928\u093e \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0928\u0947 \u0915\u093e \u092a\u094d\u0930\u092f\u093e\u0938 \u0915\u0930\u0947\u0902\u0917\u0947\u0964 \u092e\u0948\u0902 \u0939\u093e\u0907\u092a\u0930\u092a\u0948\u0930\u093e\u092e\u0940\u091f\u0930 \u091f\u094d\u092f\u0942\u0928\u093f\u0902\u0917 \u0915\u0947 \u0932\u093f\u090f \u0921\u0947\u091f\u093e, \u0930\u093f\u0917\u094d\u0930\u0947\u0936\u0928 \u092e\u0949\u0921\u0932 \u0914\u0930 \u0911\u092a\u094d\u091f\u0941\u0928\u093e \u092b\u094d\u0930\u0947\u092e\u0935\u0930\u094d\u0915 \u0915\u094b \u0938\u092e\u091d\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0915\u0926\u092e \u0926\u0930 \u0915\u0926\u092e \u0906\u0917\u0947 \u092c\u0922\u093c\u0928\u0947 \u0915\u0940 \u0915\u094b\u0936\u093f\u0936 \u0915\u0930\u0942\u0902\u0917\u093e\u0964 \u0924\u094b \u092c\u0928\u0947 \u0930\u0939\u0947\u0902 \u0914\u0930 \u0915\u0943\u092a\u092f\u093e XGBoost \u0914\u0930 Optuna \u0915\u094b \u0938\u092e\u091d\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0938\u092d\u0940 \u0928\u094b\u091f\u092c\u0941\u0915 \u0915\u093e \u0905\u0928\u0941\u0938\u0930\u0923 \u0915\u0930\u0947\u0902\u0964 \u0914\u0930 \u0905\u0917\u0930 \u0906\u092a\u0915\u094b \u092f\u0939 \u0915\u093e\u092e \u092a\u0938\u0902\u0926 \u0906\u092f\u093e \u0939\u094b \u0924\u094b \u0915\u0943\u092a\u092f\u093e \u0905\u092a\u0935\u094b\u091f \u0915\u0930\u0947\u0902 \u0914\u0930 \u0905\u0917\u0930 \u0906\u092a \u091a\u093e\u0939\u0924\u0947 \u0939\u0948\u0902 \u0915\u093f \u092e\u0948\u0902 \u0907\u0938\u0947 \u0914\u0930 \u0936\u0947\u092f\u0930 \u0915\u0930\u0924\u093e \u0930\u0939\u0942\u0902!\n\n\n\n","e124639b":"* We have no NaN values in the complete dataset.\n* Tam veri k\u00fcmesinde NaN de\u011ferimiz yok.\n* \u0939\u092e\u093e\u0930\u0947 \u092a\u093e\u0938 \u0938\u0902\u092a\u0942\u0930\u094d\u0923 \u0921\u0947\u091f\u093e\u0938\u0947\u091f \u092e\u0947\u0902 \u0915\u094b\u0908 NaN \u092e\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\u0964","bcb42ab1":"* We will not cleaning the data for this notebook because we are focussing on the xgboost parameter search by optuna. For a full scale analysis, after Exploratory data analysis data cleaning should be done. Checking multicolinearity, checking outliers and some feature engineering. Hence, check pandas profileReport and follow the cleaning steps. You can check my other notebooks [here](https:\/\/www.kaggle.com\/volkandl\/eda-cleaning-smote-prediction-97-recall)\n\n\n* Bu not defteri i\u00e7in verileri temizlemeyece\u011fiz \u00e7\u00fcnk\u00fc optuna ile xgboost parametre aramas\u0131na odaklan\u0131yoruz. Tam \u00f6l\u00e7ekli bir analiz i\u00e7in Ke\u015fifsel veri analizinden sonra veri temizli\u011fi yap\u0131lmal\u0131d\u0131r. \u00c7oklu do\u011frusall\u0131\u011f\u0131 kontrol etme, ayk\u0131r\u0131 de\u011ferleri kontrol etme ve baz\u0131 \u00f6zellik m\u00fchendisli\u011fi. Bu nedenle, pandas profileReport'u kontrol edin ve temizleme ad\u0131mlar\u0131n\u0131 izleyin. Di\u011fer defterlerimi [buradan](https:\/\/www.kaggle.com\/volkandl\/eda-cleaning-smote-prediction-97-recall) inceleyebilirsiniz.\n\n* \u0939\u092e \u0907\u0938 \u0928\u094b\u091f\u092c\u0941\u0915 \u0915\u0947 \u0932\u093f\u090f \u0921\u0947\u091f\u093e \u0915\u0940 \u0938\u092b\u093e\u0908 \u0928\u0939\u0940\u0902 \u0915\u0930\u0947\u0902\u0917\u0947 \u0915\u094d\u092f\u094b\u0902\u0915\u093f \u0939\u092e \u0911\u092a\u094d\u091f\u0941\u0928\u093e \u0926\u094d\u0935\u093e\u0930\u093e xgboost \u092a\u0948\u0930\u093e\u092e\u0940\u091f\u0930 \u0916\u094b\u091c \u092a\u0930 \u0927\u094d\u092f\u093e\u0928 \u0915\u0947\u0902\u0926\u094d\u0930\u093f\u0924 \u0915\u0930 \u0930\u0939\u0947 \u0939\u0948\u0902\u0964 \u092a\u0942\u0930\u094d\u0923 \u092a\u0948\u092e\u093e\u0928\u0947 \u092a\u0930 \u0935\u093f\u0936\u094d\u0932\u0947\u0937\u0923 \u0915\u0947 \u0932\u093f\u090f, \u0916\u094b\u091c\u092a\u0942\u0930\u094d\u0923 \u0921\u0947\u091f\u093e \u0935\u093f\u0936\u094d\u0932\u0947\u0937\u0923 \u0915\u0947 \u092c\u093e\u0926 \u0921\u0947\u091f\u093e \u0915\u0940 \u0938\u092b\u093e\u0908 \u0915\u0940 \u091c\u093e\u0928\u0940 \u091a\u093e\u0939\u093f\u090f\u0964 \u092e\u0932\u094d\u091f\u0940\u0915\u094b\u0932\u093f\u0928\u093f\u092f\u0930\u093f\u091f\u0940 \u0915\u0940 \u091c\u093e\u0901\u091a \u0915\u0930\u0928\u093e, \u0906\u0909\u091f\u0932\u0947\u0930\u094d\u0938 \u0915\u0940 \u091c\u093e\u0901\u091a \u0915\u0930\u0928\u093e \u0914\u0930 \u0915\u0941\u091b \u092b\u0940\u091a\u0930 \u0907\u0902\u091c\u0940\u0928\u093f\u092f\u0930\u093f\u0902\u0917 \u0915\u0940 \u091c\u093e\u0901\u091a \u0915\u0930\u0928\u093e\u0964 \u0907\u0938\u0932\u093f\u090f, \u092a\u093e\u0902\u0921\u093e \u092a\u094d\u0930\u094b\u092b\u093e\u0907\u0932 \u0915\u0940 \u091c\u093e\u0902\u091a \u0915\u0930\u0947\u0902\u0930\u093f\u092a\u094b\u0930\u094d\u091f \u0915\u0930\u0947\u0902 \u0914\u0930 \u0938\u092b\u093e\u0908 \u091a\u0930\u0923\u094b\u0902 \u0915\u093e \u092a\u093e\u0932\u0928 \u0915\u0930\u0947\u0902\u0964 \u0906\u092a \u092e\u0947\u0930\u0940 \u0905\u0928\u094d\u092f \u0928\u094b\u091f\u092c\u0941\u0915 \u092f\u0939\u093e\u0901 \u0926\u0947\u0916 [\u0938\u0915\u0924\u0947](https:\/\/www.kaggle.com\/volkandl\/eda-cleaning-smote-prediction-97-recall) \u0939\u0948\u0902\u0964","e931ed95":"## $\\color{Pink}{\\text{Chapter 2. XGBoost}}$ <a class=\"anchor\" id=\"chapter2\"><\/a>\n\n**ENG:** To understand the XGBoost better, it is very essential to understand what is ensemble learning. Ensemble learning is aiming to combine the predictions of several base estimators which are built with the corresponding data in order to achieve better results. There are two types of ensemble learning: Averaging methods (i.e bagging, RandomForest, ExtraTrees etc.) and Boosting Methods (i.e AdaBoost, GradientTreeBoosting, XGBoost, LightGBM, CatBoost, Histogram-Based-Gradient-Boosting etc.). XGBoost is one of the state-of-the-art boosting type ensemble method. It means \"Extreme Gradient Boosting\" and it comes out in 2016. It contains sequential prediction trees and learns the best parameters to achieve the objective.\n\n**TR:** XGBoost'u daha iyi anlamak i\u00e7in topluluk (ensemble) \u00f6\u011frenmenin ne oldu\u011funu anlamak \u00e7ok \u00f6nemlidir. Topluluk (ensemble) \u00f6\u011frenimi, daha iyi sonu\u00e7lar elde etmek i\u00e7in ilgili verilerle olu\u015fturulan birka\u00e7 temel tahmincinin tahminlerini birle\u015ftirmeyi ama\u00e7lar. \u0130ki t\u00fcr topluluk \u00f6\u011frenimi vard\u0131r: Ortalama alma y\u00f6ntemleri (\u00f6r. Bagging, RandomForest, ExtraTrees vb.) ve Y\u00fckseltme Y\u00f6ntemleri (\u00f6r. AdaBoost, GradientTreeBoosting, XGBoost, LightGBM, CatBoost, Histogram-Based-Gradient-Boosting vb.). XGBoost, son teknoloji boosting tipi ensemble y\u00f6ntemlerinden biridir. \"Extreme Gradient Boost\" anlam\u0131na gelir ve 2016'da yay\u0131nlanm\u0131\u015ft\u0131r. S\u0131ral\u0131 tahmin a\u011fa\u00e7lar\u0131 i\u00e7erir ve hedefe ula\u015fmak i\u00e7in en iyi parametreleri \u00f6\u011frenir.\n\n**HINDI:** XGBoost \u0915\u094b \u092c\u0947\u0939\u0924\u0930 \u0922\u0902\u0917 \u0938\u0947 \u0938\u092e\u091d\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f, \u092f\u0939 \u0938\u092e\u091d\u0928\u093e \u092c\u0939\u0941\u0924 \u091c\u0930\u0942\u0930\u0940 \u0939\u0948 \u0915\u093f \u090f\u0928\u094d\u0938\u0947\u092e\u094d\u092c\u0932 \u0932\u0930\u094d\u0928\u093f\u0902\u0917 \u0915\u094d\u092f\u093e \u0939\u0948\u0964 \u090f\u0928\u094d\u0938\u0947\u092e\u094d\u092c\u0932 \u0932\u0930\u094d\u0928\u093f\u0902\u0917 \u0915\u093e \u0932\u0915\u094d\u0937\u094d\u092f \u0915\u0908 \u0906\u0927\u093e\u0930 \u0905\u0928\u0941\u092e\u093e\u0928\u0915\u094b\u0902 \u0915\u0940 \u092d\u0935\u093f\u0937\u094d\u092f\u0935\u093e\u0923\u093f\u092f\u094b\u0902 \u0915\u094b \u091c\u094b\u0921\u093c\u0928\u093e \u0939\u0948 \u091c\u094b \u092c\u0947\u0939\u0924\u0930 \u092a\u0930\u093f\u0923\u093e\u092e \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0938\u0902\u092c\u0902\u0927\u093f\u0924 \u0921\u0947\u091f\u093e \u0915\u0947 \u0938\u093e\u0925 \u092c\u0928\u093e\u090f \u0917\u090f \u0939\u0948\u0902\u0964 \u092a\u0939\u0928\u093e\u0935\u093e \u0938\u0940\u0916\u0928\u0947 \u0915\u0947 \u0926\u094b \u092a\u094d\u0930\u0915\u093e\u0930 \u0939\u0948\u0902: \u0914\u0938\u0924 \u0935\u093f\u0927\u093f\u092f\u093e\u0901 (Bagging, \u0930\u0948\u0902\u0921\u092e\u092b\u093c\u0949\u0930\u0947\u0938\u094d\u091f, \u090f\u0915\u094d\u0938\u094d\u091f\u094d\u0930\u093e\u091f\u094d\u0930\u0940\u091c\u093c \u0906\u0926\u093f) \u0914\u0930 \u092c\u0942\u0938\u094d\u091f\u093f\u0902\u0917 \u092e\u0947\u0925\u0921\u094d\u0938 (\u092f\u093e\u0928\u0940 AdaBoost, GradientTreeBoosting, XGBoost, LightGBM, CatBoost, \u0939\u093f\u0938\u094d\u091f\u094b\u0917\u094d\u0930\u093e\u092e-\u0906\u0927\u093e\u0930\u093f\u0924-\u0917\u094d\u0930\u0947\u0921\u093f\u090f\u0902\u091f-\u092c\u0942\u0938\u094d\u091f\u093f\u0902\u0917 \u0906\u0926\u093f)\u0964 \nXGBoost \u0905\u0924\u094d\u092f\u093e\u0927\u0941\u0928\u093f\u0915 \u092c\u0942\u0938\u094d\u091f\u093f\u0902\u0917 \u091f\u093e\u0907\u092a \u090f\u0928\u094d\u0938\u0947\u092e\u094d\u092c\u0932 \u092a\u0926\u094d\u0927\u0924\u093f \u092e\u0947\u0902 \u0938\u0947 \u090f\u0915 \u0939\u0948\u0964 \u0907\u0938\u0915\u093e \u0905\u0930\u094d\u0925 \u0939\u0948 \"\u090f\u0915\u094d\u0938\u091f\u094d\u0930\u0940\u092e \u0917\u094d\u0930\u0948\u0921\u093f\u090f\u0902\u091f \u092c\u0942\u0938\u094d\u091f\u093f\u0902\u0917\" \u0914\u0930 \u092f\u0939 2016 \u092e\u0947\u0902 \u0938\u093e\u092e\u0928\u0947 \u0906\u092f\u093e\u0964 \u0907\u0938\u092e\u0947\u0902 \u0905\u0928\u0941\u0915\u094d\u0930\u092e\u093f\u0915 \u092d\u0935\u093f\u0937\u094d\u092f\u0935\u093e\u0923\u0940 \u092a\u0947\u0921\u093c \u0936\u093e\u092e\u093f\u0932 \u0939\u0948\u0902 \u0914\u0930 \u0909\u0926\u094d\u0926\u0947\u0936\u094d\u092f \u0915\u094b \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0938\u0930\u094d\u0935\u094b\u0924\u094d\u0924\u092e \u092e\u093e\u092a\u0926\u0902\u0921\u094b\u0902 \u0915\u094b \u0938\u0940\u0916\u0924\u0947 \u0939\u0948\u0902\u0964","b869fe65":"* XGBoost takes inputs as: Dense Matrix, Sparse Matrix, Data File or DMatrix ( xgb input matrix type )\n\n* XGBoost girdileri \u015fu \u015fekilde al\u0131r: Yo\u011fun Matris, Seyrek Matris, Veri Dosyas\u0131 veya DMatrix ( xgb giri\u015f matrisi tipi)\n\n* XGBoost \u0907\u0938 \u092a\u094d\u0930\u0915\u093e\u0930 \u0907\u0928\u092a\u0941\u091f \u0932\u0947\u0924\u093e \u0939\u0948: Dense Matrix, Sparse Matrix, Data File \u092f\u093e DMatrix (xgb \u0907\u0928\u092a\u0941\u091f \u092e\u0948\u091f\u094d\u0930\u093f\u0915\u094d\u0938 \u091f\u093e\u0907\u092a)","e3481feb":"### Usage Usage of XGBoost 3:  with xgb.DMatrix Input","34cec5ac":"![xboostOptuna.png](attachment:82b12e49-50c6-4c90-97c0-3b55174df1a4.png)","2063f80f":"## $\\color{Pink}{\\text{Chapter 3. OPTUNA}}$ <a class=\"anchor\" id=\"chapter3\"><\/a>","ffcf24db":"## $\\color{Pink}{\\text{Chapter 4. Conclusion}}$ <a class=\"anchor\" id=\"chapter4\"><\/a>","81b8debf":"* We will do one more search with the above slice history. The reason of that is to improve our algorithm one step further with narrowing the search slices!!! This iis one of the greatest idea on hyperparameter search. It causes time but totally worth.\n\n* Yukar\u0131daki dilim ge\u00e7mi\u015fi ile bir arama daha yapaca\u011f\u0131z. Bunun nedeni arama dilimlerini daraltarak algoritmam\u0131z\u0131 bir ad\u0131m daha ileriye ta\u015f\u0131makt\u0131r!!! Bu, hiperparametre aramas\u0131ndaki en b\u00fcy\u00fck fikirlerden biridir. Zamana neden olur ama tamamen de\u011fer.\n\n\n* \u0939\u092e \u0909\u092a\u0930\u094b\u0915\u094d\u0924 \u0938\u094d\u0932\u093e\u0907\u0938 \u0907\u0924\u093f\u0939\u093e\u0938 \u0915\u0947 \u0938\u093e\u0925 \u090f\u0915 \u0914\u0930 \u0916\u094b\u091c \u0915\u0930\u0947\u0902\u0917\u0947\u0964 \u0907\u0938\u0915\u093e \u0915\u093e\u0930\u0923 \u0916\u094b\u091c \u0938\u094d\u0932\u093e\u0907\u0938 \u0915\u094b \u0915\u092e \u0915\u0930\u0915\u0947 \u0939\u092e\u093e\u0930\u0947 \u090f\u0932\u094d\u0917\u094b\u0930\u093f\u0926\u092e \u0915\u094b \u090f\u0915 \u0915\u0926\u092e \u0906\u0917\u0947 \u092c\u0922\u093c\u093e\u0928\u093e \u0939\u0948 !!! \u092f\u0939 \u0939\u093e\u0907\u092a\u0930\u092a\u0948\u0930\u093e\u092e\u0940\u091f\u0930 \u0916\u094b\u091c \u092a\u0930 \u0938\u092c\u0938\u0947 \u092e\u0939\u093e\u0928 \u0935\u093f\u091a\u093e\u0930\u094b\u0902 \u092e\u0947\u0902 \u0938\u0947 \u090f\u0915 \u0939\u0948\u0964 \u092f\u0939 \u0938\u092e\u092f \u0915\u093e \u0915\u093e\u0930\u0923 \u092c\u0928\u0924\u093e \u0939\u0948 \u0932\u0947\u0915\u093f\u0928 \u092a\u0942\u0930\u0940 \u0924\u0930\u0939 \u0938\u0947 \u0932\u093e\u092f\u0915 \u0939\u0948\u0964","be99023d":"What is an XGBoost DMatrix?\n* DMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed. ","cc71a293":"**ENG:**\n\n* Direction is states what we would like to aim or what is our objective such minimizing the RMSE score in this case or it can be maximize the R2 or Recall or F1 score as well.\n* sampler means which algorithm we will use, we will select TPESampler()\n* we will study.optimize() to start searching, in optimize function we should give our objective function\n* n_trials states how many iteration will take place in searching\n\n**TR:**\n* Y\u00f6n, ne ama\u00e7lamak istedi\u011fimizi veya bu durumda RMSE puan\u0131n\u0131 en aza indirmek gibi hedefimizin ne oldu\u011funu belirtir veya R2 veya Geri \u00c7a\u011f\u0131rma veya F1 puan\u0131n\u0131 da en \u00fcst d\u00fczeye \u00e7\u0131karabilir.\n* \u00f6rnekleyici hangi algoritmay\u0131 kullanaca\u011f\u0131m\u0131z anlam\u0131na gelir, TPESampler() \u00f6\u011fesini se\u00e7ece\u011fiz\n* aramaya ba\u015flamak i\u00e7in study.optimize() yapaca\u011f\u0131z, optimize fonksiyonunda ama\u00e7 fonksiyonumuzu vermeliyiz\n* n_trials, aramada ka\u00e7 yinelemenin ger\u00e7ekle\u015fece\u011fini belirtir\n\n**HINDI**\n* \u0928\u093f\u0930\u094d\u0926\u0947\u0936 \u092c\u0924\u093e\u0924\u093e \u0939\u0948 \u0915\u093f \u0939\u092e \u0915\u094d\u092f\u093e \u0932\u0915\u094d\u0937\u094d\u092f \u092c\u0928\u093e\u0928\u093e \u091a\u093e\u0939\u0924\u0947 \u0939\u0948\u0902 \u092f\u093e \u0939\u092e\u093e\u0930\u093e \u0909\u0926\u094d\u0926\u0947\u0936\u094d\u092f \u0915\u094d\u092f\u093e \u0939\u0948 \u091c\u0948\u0938\u0947 \u0915\u093f \u0907\u0938 \u092e\u093e\u092e\u0932\u0947 \u092e\u0947\u0902 \u0906\u0930\u090f\u092e\u090f\u0938\u0908 \u0938\u094d\u0915\u094b\u0930 \u0915\u094b \u0915\u092e \u0915\u0930\u0928\u093e \u092f\u093e \u092f\u0939 \u0906\u0930 2 \u092f\u093e \u0930\u093f\u0915\u0949\u0932 \u092f\u093e \u090f\u092b 1 \u0938\u094d\u0915\u094b\u0930 \u0915\u094b \u092d\u0940 \u0905\u0927\u093f\u0915\u0924\u092e \u0915\u093f\u092f\u093e \u091c\u093e \u0938\u0915\u0924\u093e \u0939\u0948\u0964\n* \u0938\u0948\u092e\u094d\u092a\u0932\u0930 \u0915\u093e \u0905\u0930\u094d\u0925 \u0939\u0948 \u0915\u093f \u0939\u092e \u0915\u093f\u0938 \u090f\u0932\u094d\u0917\u094b\u0930\u093f\u0925\u092e \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0947\u0902\u0917\u0947, \u0939\u092e TPESampler () \u0915\u093e \u091a\u092f\u0928 \u0915\u0930\u0947\u0902\u0917\u0947\n* \u0939\u092e \u0905\u0927\u094d\u092f\u092f\u0928 \u0915\u0930\u0947\u0902\u0917\u0947\u0964 \u0911\u092a\u094d\u091f\u093f\u092e\u093e\u0907\u091c\u093c () \u0916\u094b\u091c \u0936\u0941\u0930\u0942 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f, \u0911\u092a\u094d\u091f\u093f\u092e\u093e\u0907\u091c\u093c \u092b\u093c\u0902\u0915\u094d\u0936\u0928 \u092e\u0947\u0902 \u0939\u092e\u0947\u0902 \u0905\u092a\u0928\u093e \u0909\u0926\u094d\u0926\u0947\u0936\u094d\u092f \u092b\u093c\u0902\u0915\u094d\u0936\u0928 \u0926\u0947\u0928\u093e \u091a\u093e\u0939\u093f\u090f\n* n_trials \u092c\u0924\u093e\u0924\u093e \u0939\u0948 \u0915\u093f \u0916\u094b\u091c \u092e\u0947\u0902 \u0915\u093f\u0924\u0928\u0940 \u092a\u0941\u0928\u0930\u093e\u0935\u0943\u0924\u094d\u0924\u093f \u0939\u094b\u0917\u0940","ccb1b5ff":"Best is trial 999 with value: 2.485847. which means n_estimator gives a great result @999"}}