{"cell_type":{"5ef101c1":"code","d019b88a":"code","a88088b2":"code","36e96dc4":"code","92b79e73":"code","efb73d07":"code","6aa37ca0":"code","c221c8fe":"code","a774cb02":"code","d4ffbbe0":"code","ac572aec":"code","7e04f925":"code","95a51b8a":"code","705e56d9":"code","f5517471":"code","0a876b61":"code","f8fbb6eb":"code","bba836e6":"code","a3c20b4d":"code","772776d5":"code","10f1b66c":"code","9bf57558":"code","267b0fc4":"code","35d63d68":"code","f3e1e74d":"code","6da34f07":"code","421c70bc":"code","20e08991":"code","4661316e":"code","2bd95bc8":"code","3e1cae8f":"code","881556e4":"code","4a09981b":"code","fd0d045d":"code","3f6c96cd":"code","161a5311":"code","6fce0c55":"code","8f626f47":"code","21ccaead":"code","85e10e97":"code","a785d9da":"code","9bc7d8dc":"code","d8bd4aef":"code","832a6dcc":"code","8c0b5d51":"code","abd89e5e":"code","6620f57c":"code","23c3e18b":"code","debd9069":"code","2337e739":"code","6940f499":"code","c69a9b4b":"code","157d0a8d":"code","c09a70bb":"code","0146e70f":"code","dd899bf4":"code","08c70a97":"code","793c4658":"code","dfa5949b":"code","f7f64537":"code","c3bf55b8":"code","6f4e9906":"code","a5a29cd3":"code","4109a7ae":"code","f3296710":"code","0465f257":"code","d47b6b63":"code","9adc0541":"code","960a3e3d":"code","50fc6268":"code","e8c1abf5":"code","1eecbd95":"code","07ddfe8f":"code","e1042e4d":"code","349bd2b1":"code","1626db3e":"code","4601dc01":"code","5bd28239":"code","7f962c4c":"code","66e64195":"code","5f661aff":"code","079849f7":"code","1b9e8902":"markdown","acc1b7e4":"markdown","1d2556c1":"markdown","5a7ec159":"markdown","64f8d18f":"markdown","930b9927":"markdown","fbd0a43f":"markdown","befa286a":"markdown","e6041232":"markdown","b3c2fed0":"markdown","28388b89":"markdown","a9e2a895":"markdown","10f0b74b":"markdown","6d3f4aba":"markdown","522ec724":"markdown","0b38d7cb":"markdown","ea6187aa":"markdown","21922e4f":"markdown","315fae8d":"markdown","54a57d9e":"markdown","a3d9089e":"markdown","23467cdf":"markdown","629520c3":"markdown"},"source":{"5ef101c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d019b88a":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px  # try plotly, it's amazing\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a88088b2":"submission = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/SampleSubmissionWiDS2021.csv')\ninfo = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv')\ndata = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntestdf = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')","36e96dc4":"#set the df to display maximum columns\npd.set_option('display.max_columns',None)\ntest = testdf.copy()","92b79e73":"train = data.copy()\nprint(\"Training data shape\", train.shape)\nprint(\"Testing data shape\", test.shape)","efb73d07":"# what data types how many cols\ntrain.dtypes.value_counts()","6aa37ca0":"# have a look over data and it's columns\ntrain.head()","c221c8fe":"# we do not need any of id values yet so, drop it.\ntrain.drop(['Unnamed: 0','encounter_id','hospital_id','icu_id'], axis=1, inplace=True)\n\n# drop the readmission_status because of constant value as 0\ntrain.drop('readmission_status',axis=1,inplace=True)","a774cb02":"#let's first save our target variable as target, no need to again again write it big.\ntarget = 'diabetes_mellitus'","d4ffbbe0":"train[target].value_counts().plot(kind=\"pie\", explode=[0,0.1], autopct=\"%.2f\", labels=[\"No\",\"Yes\"])\nplt.show()","ac572aec":"categorical_features = [feature for feature in train.columns if train[feature].dtype == 'O']\ntrain[categorical_features].head()","7e04f925":"# Gender\ntrain.gender.value_counts()","95a51b8a":"# gender with respect to target variable\nfig = px.histogram(train, x='gender', y=target, title=\"Gender wrt diabetes_mellitus\", width=600, height=450)\nfig.show()","705e56d9":"#let's see te age distribution of Male and Female\n#pd.crosstab(train['age'],train['gender'])\n\nplt.rcParams['figure.figsize'] = (7,5)\ntrain.groupby('gender')['age'].plot(kind=\"kde\")\nplt.xlabel(\"age\")\nplt.legend(loc=\"best\")\nplt.show()","f5517471":"train['ethnicity'].value_counts()","0a876b61":"#ethnicity\nfig = px.histogram(train, x='ethnicity', y=target, title=\"ethnicity wrt diabetes_mellitus\", width=600, height=420)\nfig.show()","f8fbb6eb":"train['icu_stay_type'].value_counts()","bba836e6":"fig = px.histogram(train, x='icu_stay_type', y=target, width=600, height=450, title=\"icu_stay_type wrt target\")\nfig.show()","a3c20b4d":"# icu_type\ntrain['icu_type'].value_counts()","772776d5":"fig = px.histogram(train,x='icu_type', y=target, width=600, height=400)\nfig.show()","10f1b66c":"#hospital_admit_source\n#train['hospital_admit_source'].nunique()\ntrain['icu_admit_source'].unique()","9bf57558":"train['hospital_admit_source'].value_counts()","267b0fc4":"# how many missing are there in categorical cols\ntrain[categorical_features].isnull().sum()","35d63d68":"# gender has only 66 NaN so, we can impute using Mode\ntrain['gender'].fillna(train['gender'].mode()[0], inplace=True)","f3e1e74d":"# we can apply random sample imputation in Ethnicity to maintain the distribution\ndef impute_random(train, col):\n    random_sample = train[col].dropna().sample(train[col].isnull().sum(), random_state=0)\n    random_sample.index = train[train[col].isnull()].index\n    train.loc[train[col].isnull(), col] = random_sample\n    \nimpute_random(train, 'ethnicity')","6da34f07":"# to preserve the distribution let's try random sample imputation for icu_admit_source\nimpute_random(train, 'icu_admit_source')\n\n#near about 25% of values are missing so, for now let's do random imputation.\nimpute_random(train, 'hospital_admit_source')","421c70bc":"numerical_features = [feature for feature in train.columns if train[feature].dtype != 'O']\ntrain[numerical_features].shape","20e08991":"train[numerical_features].head()","4661316e":"binary_features = [feature for feature in numerical_features if train[feature].nunique() == 2]\nprint(\"Total binary: \", len(binary_features))","2bd95bc8":"train[binary_features].head(3)","3e1cae8f":"#impute_random(train,'gcs_unable_apache')\ntrain['gcs_unable_apache'].fillna(train['gcs_unable_apache'].median(), inplace=True)","881556e4":"#Diabeter with respect to HIV.\npd.crosstab(train[target], train['aids'])","4a09981b":"# train['elective_surgery'].value_counts()\npd.crosstab(train[target], train['elective_surgery'])","fd0d045d":"cont_features = [feature for feature in numerical_features if feature not in binary_features]\nprint(\"total cont feature except binary: \", len(cont_features))","3f6c96cd":"train[cont_features].head()","161a5311":"#sns.boxplot(x=target, y=\"weight\", data=train)\ntrain.groupby(target)['weight'].mean().plot(kind=\"bar\")\nplt.title(\"Diabeties wrt Weight\")\nplt.show()","6fce0c55":"# diabetic patient assumes to consume more glucose\ntrain.groupby(target)['glucose_apache'].mean().plot(kind=\"bar\",title=\"Avg Glucose consumsion of Diabetic person\")\nplt.show()","8f626f47":"# for analyzing BMI let's divide the age in different groups\nbins = [0,18,30,40,50,60,70,80,120]\nlabels = ['0-17','18-30','31-40','41-50','51-60','61-70','71-80','80+']\ntrain[\"age_range\"] = pd.cut(train[\"age\"], bins=bins, labels=labels, include_lowest=True)\ntrain.groupby(\"age_range\")[\"bmi\"].mean()","21ccaead":"plt.figure(figsize=(7,5))\nsns.barplot(x='age_range', y=\"bmi\", data=train)\nplt.title(\"BMI wrt AGE_Group\")\nplt.show()","85e10e97":"# let's see our Hypothesis be tru or not?\nsns.lineplot(x=train[\"glucose_apache\"], y=train[\"weight\"])\nplt.show()","a785d9da":"#impute weight.\ntrain.groupby(target)[\"weight\"].mean()","9bc7d8dc":"# impute weight wrt target\ntrain[\"weight\"] = np.where(train[target] == 1, train[\"weight\"].fillna(91), train[\"weight\"].fillna(82))\n\n#impute height with 170 as mean and median both\ntrain[\"height\"] = train[\"height\"].fillna(170)\n\n#impute age wrt target with median\ntrain[\"age\"] = np.where(train[target] == 1, train[\"age\"].fillna(66), train[\"age\"].fillna(63))\n","d8bd4aef":"#impute bmi\ndef fill_bmi(df):\n    df['bmi'] = np.where(df['bmi'].isnull(), df['weight']\/ (df['height']\/100)**2, df['bmi'])\n    \nfill_bmi(train)","832a6dcc":"#impute weight.\ntrain.groupby(target)[\"height\"].mean()","8c0b5d51":"mydata = train[cont_features].copy()\nmydata.head(3)","abd89e5e":"max_min_features = [feature for feature in cont_features if 'max' in feature or 'min' in feature and feature not in \"albumin_apache\"]\nprint(\"total features: \",len(max_min_features))","6620f57c":"#separate out the max and min features\nmax_features = [feature for feature in max_min_features if \"max\" in feature]\nmin_features = [feature for feature in max_min_features if \"min\" in feature]\nprint(\"max: \",len(max_features))\nprint(\"min: \",len(min_features))","23c3e18b":"#By some internal error 2 max features are in min_features, remove them manually.\nmin_features.remove('d1_albumin_max')\nmin_features.remove('h1_albumin_max')","debd9069":"# Example: same thing we are going to apply in our data, and like this, we will shape the name of variables.\ns = \"d1_albumin_max\"\n\"_\".join(s.split(\"_\")[:-1]) + \"_avg\"\n\n#\"_\".join(min_features[0].split(\"_\")[:-1]) + \"_avg\"","2337e739":"# take the average and add them to the data.\nfor i in range(0,64):\n    col = \"_\".join(min_features[i].split(\"_\")[:-1]) + \"_avg\"\n    avg = (mydata[min_features[i]] +  mydata[max_features[i]]) \/ 2\n    mydata[col] = avg","6940f499":"# we got our average features, now remove the max and min features from mydata\nmydata.drop(max_features, axis=1, inplace=True)\nmydata.drop(min_features, axis=1, inplace=True)\n\n# we have reduce a 64 features, which is better then PCA too.\nmydata.shape","c69a9b4b":"#let's visualize the missing values with help of amazing library missingno\nimport missingno as msno","157d0a8d":"#plot the missing value graph\nmsno.matrix(mydata.select_dtypes(include=[np.number]))\nplt.show()","c09a70bb":"avg_features = [feature for feature in mydata.columns if 'avg' in feature]\nprint(len(avg_features))","0146e70f":"#let's ee the percentage of missing values, if per will be less ten 30 per then we will impute it.\nmiss_nan = mydata[avg_features].isnull().sum() \/ len(mydata) * 100\nmiss_nan.sort_values(ascending=False).to_frame().head(10)","dd899bf4":"#drop the feature which are having missing greater then 30%\ngreat_30 = miss_nan[miss_nan[:] > 30].index\nmydata.drop(great_30, axis=1, inplace=True)\n#Now, we will have 59 cols, more 33 we have reduced.","08c70a97":"less_30 = miss_nan[miss_nan[:] < 30].index\n# as we have taken the avg, so just fill it with mean now.\nfor col in less_30:\n    mydata[col] = mydata[col].fillna(mydata[col].mean())\n    \nprint(\"Imputed Succesfully\")\nprint(\"Null value in avg col: \",mydata[less_30].isna().sum().sum())","793c4658":"mydata.head(2)","dfa5949b":"other_nan = [feature for feature in mydata.columns if \"avg\" not in feature]\nprint(\"total other features then AVG: \", len(other_nan))","f7f64537":"ot_nan = mydata[other_nan].isnull().sum() \/ len(mydata) *100\not_nan.sort_values(ascending=False).head()","c3bf55b8":"remove_nan = ot_nan[ot_nan[:] > 50].index  #remove greater then 50\nfill_nan = ot_nan[ot_nan[:] < 50].index  #impute this","6f4e9906":"#drop the feature with greater then 50 percent missing values and \n#we will impute remaining, at little risk.\nmydata.drop(remove_nan, axis=1, inplace=True)","a5a29cd3":"# You can also fill mith (max-min)\/2. It is also a good. only some point of diff will there.\nfor col in fill_nan:\n    mydata[col] = mydata[col].fillna(mydata[col].mean())","4109a7ae":"mydata.isnull().sum().sum()","f3296710":"#first we have to free all the cont features from cont then concat it\ntrain.drop(cont_features,axis=1, inplace=True)\ntrain.drop(\"age_range\", axis=1, inplace=True)","0465f257":"# concat mydata to train\ntrain = pd.concat([train, mydata], axis=1)","d47b6b63":"train.head(2)\n#train.shape","9adc0541":"#let's check is there any col remaining to impute\ntrain.isna().sum().sum()","960a3e3d":"train[categorical_features].head(2)","50fc6268":"train['gender'] = train['gender'].map({'M':0,'F':1})","e8c1abf5":"# let's encode the feature using Integer label encoding\n# we will use value_counts to know, how it is encoded, you can also use unique()\ncat_cols = ['ethnicity', 'hospital_admit_source','icu_admit_source','icu_stay_type','icu_type']\nfor col in cat_cols:\n    map_dict = {k: i for i, k in enumerate(train[col].value_counts().index, 0)}\n    train[col] = train[col].map(map_dict)","1eecbd95":"train[categorical_features].head(2)","07ddfe8f":"# we do not need any of id values yet so, drop it.\ntest.drop(['Unnamed: 0','encounter_id','hospital_id','icu_id'], axis=1, inplace=True)\n\n# drop the readmission_status because of constant value as 0\ntest.drop('readmission_status',axis=1,inplace=True)\n\n# Missing value imputation\n#random sampleing func for test set\ndef impute_random_test(test, col):\n    random_sample = test[col].dropna().sample(test[col].isnull().sum(), random_state=0)\n    random_sample.index = test[test[col].isnull()].index\n    test.loc[test[col].isnull(), col] = random_sample\n    \n\n#first impute gender, only 5 missing value\ntest['gender'].fillna(\"M\", inplace=True)\n\n#random sample imputation for ethnicity, icu_admit_source, hospital_admit_source, gcs_unable_apache\nimpute_random_test(test, 'ethnicity')\nimpute_random_test(test, 'icu_admit_source')\nimpute_random_test(test, 'hospital_admit_source')\n\ntest['gcs_unable_apache'].fillna(test['gcs_unable_apache'].median(), inplace=True)\n\n#impute bmi, weight, height\ntest['weight'] = test['weight'].fillna(test['weight'].mean())\ntest['height'] = test['height'].fillna(170)   #median\n#impute bmi\nfill_bmi(test)","e1042e4d":"mytest = test[cont_features].copy()\n\n# take the average and add them to the data.\nfor i in range(0,64):\n    col = \"_\".join(min_features[i].split(\"_\")[:-1]) + \"_avg\"\n    avg = (mytest[min_features[i]] +  mytest[max_features[i]]) \/ 2\n    mytest[col] = avg\n    \n# we got our average features, now remove the max and min features from mydata\nmytest.drop(max_features, axis=1, inplace=True)\nmytest.drop(min_features, axis=1, inplace=True)\n\n# we have reduce a 64 features, which is better then PCA too.\n\nmytest.drop(great_30, axis=1, inplace=True)\n\n#inpute remaining features have missing less then 30%\nfor col in less_30:\n    mytest[col] = mytest[col].fillna(mytest[col].mean()) \n\n\nmytest.drop(remove_nan, axis=1, inplace=True)\n#impute features other then avg, categorical, binary.\nfor col in fill_nan:\n    mytest[col] = mytest[col].fillna(mytest[col].mean())\n\n    \n#concat the mytest to test, and before that drop all continuous features\ntest.drop(cont_features, axis=1, inplace=True)\n\ntest = pd.concat([test, mytest], axis=1)","349bd2b1":"# Categorical Encoding\ntest['gender'] = test['gender'].map({'M':0,'F':1})\n\n# let's encode the feature using Integer label encoding\n# we will use value_counts to know, how it is encoded, you can also use unique()\ncat_cols = ['ethnicity', 'hospital_admit_source','icu_admit_source','icu_stay_type','icu_type']\nfor col in cat_cols:\n    map_dict = {k: i for i, k in enumerate(test[col].value_counts().index, 0)}\n    test[col] = test[col].map(map_dict)","1626db3e":"#let's try to build a simple model first\nfrom sklearn.model_selection import train_test_split\n# let's find the better accuracy and fit it\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score","4601dc01":"x = train.drop(target, axis=1)\ny = train[target]\ntesting = test[:]","5bd28239":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=11)","7f962c4c":"log_clf = LogisticRegression()\nknn_clf = KNeighborsClassifier()\ndt_clf = DecisionTreeClassifier()\nrf_clf = RandomForestClassifier()","66e64195":"models = [log_clf, knn_clf, dt_clf, rf_clf]\n\nfor clf in models:\n    clf.fit(x_train, y_train)\n    \n    y_pred = clf.predict(x_test)\n    \n    print(clf.__class__.__name__, \" accuracy: \", accuracy_score(y_test,y_pred))","5f661aff":"#let's try to implemet xgBoost then, we will improve the dataset \nfrom xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(x_train, y_train)\nxg_pred = xgb_clf.predict(x_test)\nprint(\"accuracy: \", accuracy_score(y_test, xg_pred))","079849f7":"xgb_prediction = xgb_clf.predict_proba(testing)\nxgb_prediction = xgb_prediction[:, -1]\n\nxgb_file= testdf[[\"encounter_id\"]]\nxgb_file[\"diabetes_mellitus\"]= xgb_prediction\n\nxgb_file.to_csv('xgboost.csv',index=False)\nxgb_file.head()","1b9e8902":"**Instead of having too many features as min and max, what we will do is combine them based on avg (min+max)\/2, which will help in easy analysis and features will also reduce.**","acc1b7e4":"**Weight is increasing in a quadratic manner as consumption of glucose is increasing**","1d2556c1":"## Ready for Modelling","5a7ec159":"### Concat the mydata back to training data and proceed further","64f8d18f":"**OBSERVATIONS**\n- near about 23 percent people who undergo elective surgery are suffering from diabetis. ","930b9927":"**We have to treat this missing values, before encoding. we will find the best possible technique to impute this missing values.**","fbd0a43f":"**We can clearly observe the quantity of missing values in each numerical column, it's in huge amount. except starting 5 columns which we have imputed early. we will try to impute at best level with proper analysis**","befa286a":"<h3 style = \"background: lightblue\":>Categorical Variables<\/h3>\n- let's start with that","e6041232":"**Diabetic Person weight much more then Non-Diabetic. This could happen because their body consumes lot of glucose from food as we have seen which results in weight gain**","b3c2fed0":"# Understand the Data first. It's your weapon","28388b89":"**Wow! That's amazing removing all those 64 features and having average, but still there is lots of noise and most important,there is lots of missing missing values in each col so,we have to impute that first.**","a9e2a895":"**Imputation**","10f0b74b":"**BMI Increases upto the age 50 then, it starts decreasing. And on an average overall it's nearby 30. so we can impute with mean or 30**","6d3f4aba":"## Numerical Features","522ec724":"**Did Diabetic Person Weight more?**","0b38d7cb":"**The Dataset we are having is IMBALANCED dataset, It nedd to be balnced means there should be equal distribution of each kind of data.**","ea6187aa":"<h3 style=\"background:lightblue\">Continous features<\/h3>\nExcept binary features","21922e4f":"**We are having a too many feature so we will separate the Binary variable from Numerical to do the clear data analysis and find the relationship between features**","315fae8d":"## *NOTE:* The Notebook is Now updated. \n\n**Have a look to find something best out of it**","54a57d9e":"**Guys! Here I have not shown any hyperparameter tuning and advance modelling. I hope you can do this.**\n\n### Thank You. If you find any corrections then please suggest in comment section, it will very much helpful to improve. If you have any furher techniques that should be tried, please put them forward. And Upvote for motivating to move ahead with data science journey.","a3d9089e":"**Except average features, we are also having some numeric features which we have to impute**","23467cdf":"# Prepare the Test dataset","629520c3":"## Categorical Encoding"}}