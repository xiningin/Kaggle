{"cell_type":{"fa6df1fd":"code","a71829ab":"code","f5715992":"code","ec9ce11b":"code","2ebd566b":"code","b645890f":"code","6385f404":"code","c3fa0c5c":"code","85095c07":"code","189091ad":"code","50411e06":"code","58601b80":"code","1a210e8d":"code","eba8465f":"code","c35d155b":"code","4487fe06":"code","41cd24c3":"code","a15df083":"code","c54cb10f":"code","85e2b13b":"markdown","abe15677":"markdown","43870383":"markdown","6d339970":"markdown","16dc0430":"markdown","8d90dfc1":"markdown","8784ed6a":"markdown","581c16d2":"markdown","3d09efc7":"markdown","667019a8":"markdown"},"source":{"fa6df1fd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tqdm.notebook as tqdm\n\n%matplotlib inline","a71829ab":"df_train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\").drop(columns='id')\ndf_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\").drop(columns='id')","f5715992":"# There are duplicated within the train data\nprint(df_train.drop(columns='target').duplicated(keep=False).sum())\nprint(df_test.duplicated(keep=False).sum())\n\n# We just drop these since they all have different classes\nprint(df_train.duplicated(keep=False).sum())  # Not dropping tatget gives 0\ndf_train = df_train[~df_train.drop(columns='target').duplicated(keep=False)]\ndf_train","ec9ce11b":"_df_train = df_train.copy()\n_df_test = df_test.copy()\n_df_test['split'] = 'test'\n_df_train['split'] = 'train'\n_df = pd.concat([_df_train, _df_test])\n_df[_df.drop(columns=['split', 'target']).duplicated(keep=False)]","2ebd566b":"df_train.head()","b645890f":"df_train.describe().T","6385f404":"df_test.describe().T","c3fa0c5c":"from scipy.stats import anderson_ksamp, ks_2samp\n\nfor col in df_test.columns:\n    s = ks_2samp(df_train[col], df_test[col])\n    # s = anderson_ksamp([df_train[col], df_test[col]])\n    print(f'{col}: {s}')","85095c07":"df_train.target.value_counts().plot(kind='bar')\n# df_test.target.value_counts().plot(kind='bar')","189091ad":"df_train.boxplot(figsize=(20, 10), rot=90)\nplt.show()","50411e06":"fig, axes = plt.subplots(17, 3, figsize=(18, 54))\n\ncnts = df_train.drop(columns='target').value_counts().sort_index()\ntarget_order = sorted(df_train.target.unique())\n\nfor col, ax in tqdm.tqdm(zip(df_train.drop(columns='target'), axes.flatten()), total=50):\n    cnt = df_train[col].value_counts().sort_index()\n    sns.kdeplot(x=col, hue='target', hue_order=target_order, data=df_train, fill=True, legend=True, ax=ax,)\n    \n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.set_title(f'{col}, Unique Values: {len(cnt)}', loc='right', fontsize=12)\n    ax.axis('off')\n    \naxes.flatten()[-1].axis('off')\naxes.flatten()[-2].axis('off')\n\nfig.tight_layout()\nplt.show()","58601b80":"_ = plt.figure(figsize=(10, 10))\ncorr = df_train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","1a210e8d":"from sklearn.feature_selection import VarianceThreshold\nvt = VarianceThreshold(threshold=0.1).fit(df_train.drop(columns='target'))\nvt.get_support()","eba8465f":"from xgboost import XGBClassifier, plot_importance\nfrom sklearn.metrics import classification_report as cr, confusion_matrix as cm\nfrom sklearn.metrics import log_loss\nfrom sklearn.utils import compute_sample_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing","c35d155b":"x = df_train.drop(columns='target')\nle = preprocessing.LabelEncoder().fit(df_train.target)\ny = le.transform(df_train.target)\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y, shuffle=True, random_state=0)\n\n# sample_weight = compute_sample_weight('balanced', y_train)\nmodel = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=0).fit(x_train, y_train) #, sample_weight=sample_weight)\n\nprint(\"Train:\")\ny_pred = model.predict_proba(x_train)\nprint(cm(y_true=y_train, y_pred=y_pred.argmax(axis=1)))\nprint(cr(y_true=y_train, y_pred=y_pred.argmax(axis=1)))\nprint(log_loss(y_pred=y_pred, y_true=y_train, labels=np.unique(y_train)))\n\nprint(\"Val:\")\ny_pred = model.predict_proba(x_val)\nprint(cm(y_true=y_val, y_pred=y_pred.argmax(axis=1)))\nprint(cr(y_true=y_val, y_pred=y_pred.argmax(axis=1)))\nprint(log_loss(y_pred=y_pred, y_true=y_val, labels=np.unique(y_val)))\n\n_, ax = plt.subplots(1, 1, figsize=(18, 18))\nplot_importance(model, ax=ax)\nplt.title('Feature Importance')\nplt.show()","4487fe06":"from sklearn.feature_selection import SelectFromModel\n\nthresholds = np.sort(model.feature_importances_)\nfor thresh in tqdm.tqdm(thresholds[[5, 10, 15, 20, 25, 30, 35, 40, 49]):\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    sel_x_train = selection.transform(x_train)\n    xgb = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=0).fit(sel_x_train, y_train)\n    xgb.fit(sel_x_train, y_train)\n    \n    sel_x_val = selection.transform(x_val)\n    y_pred = xgb.predict_proba(sel_x_val)\n    ll = log_loss(y_pred=y_pred, y_true=y_val, labels=np.unique(y_val))\n    print(f\"Thresh={thresh}, n={sel_x_train.shape}, Log-Loss: {ll}\")","41cd24c3":"N = 100\na_min = np.linspace(0.0, 0.5, num=N)\na_max = np.linspace(0.5, 1.0, num=N)\n\nz = np.array([[log_loss(y_pred=np.clip(y_pred, a_min=i, a_max=j), y_true=y_val, labels=np.unique(y_val)) for i in a_min] for j in a_max])\n\nprint(z.min())\ni, j = np.unravel_index(z.argmin(), z.shape)\na_min, a_max = a_min[i], a_max[j]\nprint(a_min, a_max)\n\n# The a_max cutoff is pretty low here.\na_max = 0.9","a15df083":"# Train on all the data\nmodel = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=0).fit(x, y) #, sample_weight=sample_weight)","c54cb10f":"df_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\ny_pred = model.predict_proba(df_test.drop(columns='id'))\n# y_pred = np.clip(y_pred, a_min, a_max)\n\nsubmission = pd.DataFrame(y_pred, columns=le.classes_)\nsubmission = submission[['Class_1','Class_2','Class_3','Class_4']]\n\n\nsubmission['id'] = df_test['id']\nsubmission.to_csv('.\/submission.csv', index=False)\nassert len(submission) == 50000","85e2b13b":"# Feature selection","abe15677":"## Target counts","43870383":"KS test implies that train and test distibutions are pretty similar.","6d339970":"## Train vs Test distributions","16dc0430":"# XGBoost","8d90dfc1":"# Outliers","8784ed6a":"# Load Data","581c16d2":"### Variation of features split by target\n\nKeep in mind class imbalance","3d09efc7":"# EDA","667019a8":"## Correlations"}}