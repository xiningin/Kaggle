{"cell_type":{"cca2a0ca":"code","6aff765a":"code","d2d0cf60":"code","40a9860b":"code","f187eb6c":"code","2efe467d":"code","5fcd0df3":"code","56c43c56":"code","d84fc960":"code","7dee9148":"code","9d22b3c0":"code","81e5961a":"code","7f6d7f5a":"code","c034a443":"code","19b67b26":"code","c13223a1":"code","8d240384":"code","d7abee31":"code","ad57e441":"code","24e5a4d0":"markdown","0c97322e":"markdown","82fc5510":"markdown","8743c5aa":"markdown","f999ce39":"markdown","11e2fc61":"markdown","9d19c6b5":"markdown","b5c7e9e7":"markdown","cf730251":"markdown","25990fca":"markdown","f5dcff28":"markdown","75b2c48a":"markdown","e5c0c7bc":"markdown","e8b4ad68":"markdown","ff609188":"markdown","cb8efd57":"markdown","bfc736f8":"markdown","fd50c5b1":"markdown","1082ed8c":"markdown"},"source":{"cca2a0ca":"!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git -q","6aff765a":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint('TF version:', tf.__version__)\nprint('GPU devices:', tf.config.list_physical_devices('GPU'))","d2d0cf60":"train_features_df = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets_df = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntest_features_df = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n\nprint('train_features_df.shape:', train_features_df.shape)\nprint('train_targets_df.shape:', train_targets_df.shape)\nprint('test_features_df.shape:', test_features_df.shape)","40a9860b":"train_features_df.sample(5)","f187eb6c":"train_targets_df.sample(5)","2efe467d":"sample_submission_df = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsample_submission_df.sample(5)","5fcd0df3":"for target_name in list(train_targets_df)[1:]:\n    rate = float(sum(train_targets_df[target_name])) \/ len(train_targets_df)\n    print('%.4f percent positivity rate for %s' % (100 * rate, target_name))","56c43c56":"num_train_samples = int(0.8 * len(train_features_df))\n\nfull_train_features_ids = train_features_df.pop('sig_id')\nfull_test_features_ids = test_features_df.pop('sig_id')\ntrain_targets_df.pop('sig_id')\n\nfull_train_features_df = train_features_df.copy()\nfull_train_targets_df = train_targets_df.copy()\n\nval_features_df = train_features_df[num_train_samples:]\ntrain_features_df = train_features_df[:num_train_samples]\nval_targets_df = train_targets_df[num_train_samples:]\ntrain_targets_df = train_targets_df[:num_train_samples]\n\nprint('Total training samples:', len(full_train_features_df))\nprint('Training split samples:', len(train_features_df))\nprint('Validation split samples:', len(val_features_df))","d84fc960":"predictions = []\nfor target_name in list(train_targets_df):\n    rate = float(sum(train_targets_df[target_name])) \/ len(train_targets_df)\n    predictions.append(rate)\npredictions = np.array([predictions] * len(val_features_df))\n\ntargets = np.array(val_targets_df)\nscore = keras.losses.BinaryCrossentropy()(targets, predictions)\nprint('Baseline score: %.4f' % score.numpy())","7dee9148":"feature_names = list(train_features_df)\ncategorical_feature_names = ['cp_type', 'cp_dose']\nnumerical_feature_names = [name for name in feature_names if name not in categorical_feature_names]\n\ndef merge_numerical_features(feature_dict):\n    categorical_features = {name: feature_dict[name] for name in categorical_feature_names}\n    numerical_features = tf.stack([tf.cast(feature_dict[name], 'float32') for name in numerical_feature_names])\n    feature_dict = categorical_features\n    feature_dict.update({'numerical_features': numerical_features})\n    return feature_dict\n\ntrain_features_ds = tf.data.Dataset.from_tensor_slices(dict(train_features_df))\ntrain_features_ds = train_features_ds.map(lambda x: merge_numerical_features(x))\ntrain_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(train_targets_df))\ntrain_ds = tf.data.Dataset.zip((train_features_ds, train_targets_ds))\n\nfull_train_features_ds = tf.data.Dataset.from_tensor_slices(dict(full_train_features_df))\nfull_train_features_ds = full_train_features_ds.map(lambda x: merge_numerical_features(x))\nfull_train_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(full_train_targets_df))\nfull_train_ds = tf.data.Dataset.zip((full_train_features_ds, full_train_targets_ds))\n\nval_features_ds = tf.data.Dataset.from_tensor_slices(dict(val_features_df))\nval_features_ds = val_features_ds.map(lambda x: merge_numerical_features(x))\nval_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(val_targets_df))\nval_ds = tf.data.Dataset.zip((val_features_ds, val_targets_ds))\n\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(test_features_df))\ntest_ds = test_ds.map(lambda x: merge_numerical_features(x))\n\nprint('Training split samples:', int(train_ds.cardinality()))\nprint('Validation split samples:', int(val_ds.cardinality()))\nprint('Test samples:', int(test_ds.cardinality()))\n\ntrain_ds = train_ds.shuffle(1024).batch(64).prefetch(8)\nfull_train_ds = full_train_ds.shuffle(1024).batch(64).prefetch(8)\nval_ds = val_ds.batch(64).prefetch(8)\ntest_ds = test_ds.batch(64).prefetch(8)","9d22b3c0":"from tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\nfrom tensorflow.keras.layers.experimental.preprocessing import StringLookup\n\n\ndef encode_numerical_feature(feature, name, dataset):\n    # Create a Normalization layer for our feature\n    normalizer = Normalization()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n\n    # Learn the statistics of the data\n    normalizer.adapt(feature_ds)\n\n    # Normalize the input feature\n    encoded_feature = normalizer(feature)\n    return encoded_feature\n\n\ndef encode_categorical_feature(feature, name, dataset):\n    # Create a Lookup layer which will turn strings into integer indices\n    index = StringLookup()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n\n    # Learn the set of possible feature values and assign them a fixed integer index\n    index.adapt(feature_ds)\n\n    # Turn the values into integer indices\n    encoded_feature = index(feature)\n\n    # Create a CategoryEncoding for our integer indices\n    encoder = CategoryEncoding(output_mode=\"binary\")\n\n    # Prepare a dataset of indices\n    feature_ds = feature_ds.map(index)\n\n    # Learn the space of possible indices\n    encoder.adapt(feature_ds)\n\n    # Apply one-hot encoding to our indices\n    encoded_feature = encoder(encoded_feature)\n    return encoded_feature","81e5961a":"all_inputs = []\nall_encoded_features = []\n\nprint('Processing categorical features...')\nfor name in categorical_feature_names:\n    inputs = keras.Input(shape=(1,), name=name, dtype='string')\n    encoded = encode_categorical_feature(inputs, name, train_ds)\n    all_inputs.append(inputs)\n    all_encoded_features.append(encoded)\n\nprint('Processing numerical features...')\nnumerical_inputs = keras.Input(shape=(len(numerical_feature_names),), name='numerical_features')\nencoded_numerical_features = encode_numerical_feature(numerical_inputs, 'numerical_features', train_ds)\n\nall_inputs.append(numerical_inputs)\nall_encoded_features.append(encoded_numerical_features)\nfeatures = layers.Concatenate()(all_encoded_features)","7f6d7f5a":"x = layers.Dropout(0.5)(features)\noutputs = layers.Dense(206, activation='sigmoid')(x)\nbasic_model = keras.Model(all_inputs, outputs)\nbasic_model.summary()\nbasic_model.compile(optimizer=keras.optimizers.RMSprop(),\n                    loss=keras.losses.BinaryCrossentropy())\nbasic_model.fit(full_train_ds, epochs=10, validation_data=val_ds)","c034a443":"import kerastuner as kt\n\ndef make_model(hp):\n    x = features\n    num_dense = hp.Int('num_dense', min_value=0, max_value=3, step=1)\n    for i in range(num_dense):\n        units = hp.Int('units_{i}'.format(i=i), min_value=32, max_value=256, step=32)\n        dp = hp.Float('dp_{i}'.format(i=i), min_value=0., max_value=0.5)\n        x = layers.Dropout(dp)(x)\n        x = layers.Dense(units, activation='relu')(x)\n    \n    dp = hp.Float('final_dp', min_value=0., max_value=0.5)\n    x = layers.Dropout(dp)(x)\n    outputs = layers.Dense(206, activation='sigmoid')(x)\n    model = keras.Model(all_inputs, outputs)\n\n    learning_rate = hp.Float('learning_rate', min_value=3e-4, max_value=3e-3)\n    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n    model.compile(loss=keras.losses.BinaryCrossentropy(),\n                  optimizer=optimizer)\n    model.summary()\n    return model\n\n\ntuner = kt.tuners.BayesianOptimization(\n    make_model,\n    objective='val_loss',\n    max_trials=5,  # Set to 5 to run quicker, but need 100+ for good results\n    overwrite=True)\n\ncallbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3)]\ntuner.search(train_ds, validation_data=val_ds, callbacks=callbacks, epochs=100)","19b67b26":"def get_trained_model(hp):\n    model = make_model(hp)\n    # First, find the best number of epochs to train for\n    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=4)]\n    history = model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=callbacks)\n    val_loss_per_epoch = history.history['val_loss']\n    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n    print('Best epoch: %d' % (best_epoch,))\n    model = make_model(hp)\n    # Increase epochs by 20% when training on the full dataset\n    model.fit(full_train_ds, epochs=int(best_epoch * 1.2))\n    return model","c13223a1":"n = 2  # E.g. n=10 for top ten models\nbest_hps = tuner.get_best_hyperparameters(n)\n\nall_preds = []\nfor hp in best_hps:\n    model = get_trained_model(hp)\n    preds = model.predict(test_ds)\n    all_preds.append(preds)","8d240384":"preds = np.zeros(shape=(len(test_features_df), 206))\nfor p in all_preds:\n    preds += p\npreds \/= len(all_preds)","d7abee31":"submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\ncolumns = list(submission.columns)\ncolumns.remove('sig_id')\n\nfor i in range(len(columns)):\n    submission[columns[i]] = preds[:, i]\n\nsubmission.to_csv('submission.csv', index=False)","ad57e441":"submission.head()","24e5a4d0":"## Train a basic model to establish a better baseline\n\nCan a simple model beat our dumb baseline? Let's try a simple logistic regression over our concatenated feature space.","0c97322e":"# MoA: Keras + KerasTuner best practices\n\nThis notebook will teach you how to:\n\n- Use a Keras neural network for the MoA competition\n- Use KerasTuner to find high-performing model configurations\n- Ensemble a few of the top models to generate final predictions","82fc5510":"Out of 23,814 samples, how often is each of the 206 target indicators positive?","8743c5aa":"## Make submission","f999ce39":"## Setup\n\nLet's get started by installing KerasTuner (latest version) and importing the packages we will use.","11e2fc61":"## Prepare TF datasets\n\nLet's turn our dataframes into `tf.data.Datasets`, which we will use to train our Keras models in the next step. Our datasets will yield tuples of `(features, targets)` where `features` is a dict and `targets` is a list. In the `features` dict, we will have 3 keys: `cp_type` and `cp_dose`, as well as `numerical_features`, which will be a vector concatenating all numerical features in the space.","9d19c6b5":"## Encode our features\n\nWe use a `StringLookup` + `CategoryEncoding` layer to index and encode our string categorical features. It's a bit overkill since there are only two values, and it takes into account the possibility of unknown values at test time, which we don't have in this case. But it is very general and you can't go wrong with it.\n\nThen, we use a single `Normalization` layer to encode our concatenated numerical features.\n\nFinally, we concatenate the entire feature space into a single vector.","b5c7e9e7":"The targets are binary indicators (0 or 1) across 206 different categories. So our model should output a probability score between 0 and 1 (sigmoid activation) across 206 outputs.\n\nThe sample submission format matches these expectations:","cf730251":"Two things:\n\n- Positivity rates are very low\n- Positivity rates are very heterogeneous","25990fca":"## Data preparation","f5dcff28":"Assuming the score would generalize to test samples, this dumb baseline would rank us somewhere in the middle of the current leaderboard. Not too bad.\n\nNow, let's bring out the Deep Learning and aim for the top!","75b2c48a":"## Setting aside a validation set\n\nLet's set aside a training set and a validation set: all of our configuration choices will be guided by performance on this subset of the total available training data. We will also keep on the total available training data, which we will use to train our final production models.\n","e5c0c7bc":"## A dumb baseline\n\nIf you've read my book, you know you should start tough projects by computing a \"dumb\" baseline that will serve as your reference point. This is usually **the highest score you can reach without looking at the test features** (or validation features in this case). Let's use the positivity rate of each target as measured in the training subset to generate predictions for the validation subset. ","e8b4ad68":"The validation loss establishes strong statistical power, with a 0.0175 validation logloss at epoch 6. ML is working well on this problem.","ff609188":"In this competition, we're looking at 3 CSV files: one for training features, one for training targets (with the same number of entries and a 1:1 match between entries in the features file and those in the targets file), and one for test features. The goal is to predict the targets that correspond to the test features.","cb8efd57":"## Optimize the model architecture with Keras Tuner\n\nMaybe we could add a few Dense layers? Change the dropout rate? Tune the learning rate? Let's try them all.","bfc736f8":"## Reinstantiate the top N models and train them on the full dataset","fd50c5b1":"Ok, so we have 2 categorical features (`cp_type` and `cp_dose`, which are strings), and everything else is numerical (assuming `g-0` to `g-99` are homogeneous in type).\n\nWe'll use the `StringLookup` and `CategoryEncoding` layers to encode the categorical features, and the `Normalization` layer to normalize the values of the numerical features.\n\nLet's look at the targets:","1082ed8c":"## Ensemble the predictions for the top N models"}}