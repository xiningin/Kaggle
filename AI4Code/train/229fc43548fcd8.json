{"cell_type":{"6dce6659":"code","b46ea529":"code","7a933817":"code","4eccd8db":"code","a9102b31":"code","6b4aa9a1":"code","da62e023":"code","d28d16c2":"code","86465d3a":"code","8b01faf3":"markdown","9b7f6230":"markdown","2e780880":"markdown","a71411f1":"markdown","952157bb":"markdown","d2facb13":"markdown","9f3ce4f6":"markdown","033f5c0f":"markdown","def3f5f3":"markdown","406fd2d4":"markdown"},"source":{"6dce6659":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nimport lightgbm as lgb\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b46ea529":"train= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n#test.describe()","7a933817":"#train.info()","4eccd8db":"#test.info()","a9102b31":"corr_matrix=train.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False)","6b4aa9a1":"train['AllflrSF']=train['TotalBsmtSF']+train['1stFlrSF']+train['2ndFlrSF']\ncorr_matrix=train.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False)","da62e023":"from pandas.plotting import scatter_matrix\nattributes=[\"SalePrice\",\"OverallQual\",\"GrLivArea\",\"GarageCars\",\"YearBuilt\",\"AllflrSF\"]\nscatter_matrix(train[attributes],figsize=(20,14))\nplt.show()","d28d16c2":"#preprocessing  data\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))\nall_data['AllflrSF']=all_data['TotalBsmtSF']+all_data['1stFlrSF']+all_data['2ndFlrSF']\n#log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n\nall_data = pd.get_dummies(all_data)\n#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())\n#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice","86465d3a":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label = y)\ndtest = xgb.DMatrix(X_test)\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\nmodel1=RandomForestRegressor()\nmodel_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(X_train, y)\nmodel1.fit(X_train,y)\nrf_preds=np.expm1(model1.predict(X_test))\nxgb_preds = np.expm1(model_xgb.predict(X_test))\nfinal_preds=rf_preds+xgb_preds\/2\ntest_id=test['Id']\noutput = pd.DataFrame( {\"Id\" :test_id ,'SalePrice': final_preds})\noutput.to_csv('my_submission.csv', index=False)\nprint(output)\nprint(\"Your submission was successfully saved!\")","8b01faf3":"While the below code is preprocessing of the data which I had copied from some Kaggle Notebook. With the first line we are trying to create the ALLFLRSF feature for the complete TrainSet and TestSet as it had a high correlation, we are also performing a Log transformation on SalePrices as historical data can be skewed and not show a accurate picture . \nUsually what happens with Machine Learning Algorithms is that they require data in a normal distribution . So that is why we are cleaning and making the data in the below code more friendlier for our algorithms .","9b7f6230":"The scatter matrix helps in finding correlation between pairs of features , where the feature with itself is shown as a plot of histogram .","2e780880":"Importing all the libraries for looking at the data .","a71411f1":"Usually it is a very great idea to look at the Metadata which has been provided to us in both the sets, while the .info() describes the column, the .describe() method helps in providing a statistical summary of the data. Try it out!!!","952157bb":"# THE PROBLEM \nHere the problem which is defined is as follows : We are given a housing dataset and we need to make predictions on Saleprices.","d2facb13":"I hope this kernel , any suggestions to improve it would be appreciated!!!","9f3ce4f6":"Usually while writing the data , it is a great idea to create new features , though it requires a lot of hypothesis testing and even a lot of luck , in this case it seems that creating ALLFLRSF as a feature having area of all floors act as a better feature. Of course , for each different situation there you could generate some feature . However , if you are a beginner , I would suggest trying any hypothesis , with multiplication, division and addition of different columns of the data .","033f5c0f":"Finally we are fitting a model to the processed data, and creating an average between XGB and RandomForestRegessor usually we create a ensemble of models as it helps in predicting a better outcome than that of just a single model . That being said feature engineering and data preprocessing is far important than the models .","def3f5f3":"Correlation Matrix helps in identifying the correlations between the data , however there could be case of non_linearity among different features of the data.","406fd2d4":"# INTRODUCTION\nHi Kagglers!\nSo the Kernel is made to introduce some basic concepts about data preprocessing , and fitting a model!"}}