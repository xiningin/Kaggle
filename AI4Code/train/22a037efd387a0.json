{"cell_type":{"86b2a24f":"code","0448ea8e":"code","bc01ab0b":"code","6e998427":"code","d02ac129":"code","9adcd267":"code","964403ef":"code","282b8366":"code","95419c2b":"code","b59e44bb":"code","147afaf3":"code","4bbbfa7f":"code","6dd55464":"code","f77046f9":"code","a2f9139c":"code","ee64a10f":"code","39f45147":"code","af798b2b":"code","4a7646a0":"code","d20c5f61":"code","1e4c804e":"code","45bd80ca":"code","e175aa7c":"code","a2a20285":"code","5a0afd27":"code","67c25324":"markdown","661b0b96":"markdown","3dc10313":"markdown","a5a61ea8":"markdown","26617e83":"markdown","301e7604":"markdown","ec974e2e":"markdown","8ce1d024":"markdown","129a1a71":"markdown","ab152edd":"markdown","cf5b79b5":"markdown","80beb02d":"markdown","5573ff11":"markdown","4220e567":"markdown","26d92dd9":"markdown","bc00b847":"markdown","3dd0bde0":"markdown","c1b75dd9":"markdown"},"source":{"86b2a24f":"!pip install ..\/input\/my-wheels\/scikit_learn-0.21.0-cp37-cp37m-manylinux1_x86_64.whl\n\n# !pip install -U scikit-learn==0.21.0\n# !pip install scikit-optimize==0.8.dev0","0448ea8e":"import os\nimport gc\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport random\nimport math\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport skopt\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import *\nfrom skopt.utils import use_named_args\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.models import *\nimport tensorflow_addons as tfa\n\nimport sklearn\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import log_loss, roc_auc_score\nimport category_encoders as ce\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# print(os.listdir('..\/input\/lish-moa'))","bc01ab0b":"skopt.__version__, sklearn.__version__","6e998427":"DEBUG = False # True \nKAGGLE = True\n\nN_FOLDS = 5    # 10\nVERBOSE = 0 if not DEBUG else 1\n\nN_TRIALS = 15   # 20\n\nN_COMP_G = 600     # 100\nN_COMP_C = 50      # 80, 10\nN_CLUSTERS_G = 35\nN_CLUSTERS_C = 10  # 5\nVAR_THRES = 0.8   # 0.7\nENCODING = 'dummy'\n\n# Clipping Thresholds\nCLIP = True\np_min = 0.0001\np_max = 1-p_min\n\n# Model hp\nLR = 1e-3   # 5e-4\nWD = 1e-5\nEPOCHS = 60 if not DEBUG else 5\nBATCH_SIZE = 128\nSEED = 26","d02ac129":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(seed=42)","9adcd267":"# some helpers\n\n# Function to calculate the mean log loss of the targets including clipping\ndef mean_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n    metrics = []\n    for target in range(206):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target], labels=[0,1]))\n    return np.mean(metrics)\n\ndef multi_log_loss(y_true, y_pred):\n    metrics = []\n    for _target in y_true.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)\n\n\ndef getFold(df, fold):\n    X_train = df[df.kfold != fold].loc[:, feature_cols]\n    X_val = df[df.kfold == fold].loc[:, feature_cols]\n\n    Y_train = df[df.kfold != fold].loc[:, target_cols]\n    Y_val = df[df.kfold == fold].loc[:, target_cols]\n\n    return X_train, X_val, Y_train, Y_val\n\n\n# ***** Clustering **** #\ndef fe_cluster(train, test, n_clusters_g=35, n_clusters_c=10, SEED = 123):\n\n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n\n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n\n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\n\n# ***** FE stats **** #\ndef fe_stats(train, test):\n\n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n\n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n    return train, test\n\n\ndef cells_sq(train, test):\n    features_c = list(train_features.columns[776:876])\n    for df in train, test:\n        for col in features_c:\n            df[f'{col}_sq'] = df[col].values ** 2\n    return train, test\n\n\ndef process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time', 'cp_dose'], drop_first=True)\n    return data\n\n# def preprocess(df):\n#     df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n#     df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n#     df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n#     del df['sig_id']\n#     return df\n","964403ef":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","282b8366":"G_COLS = [col for col in train_features.columns if col.startswith('g-')]\nC_COLS = [col for col in train_features.columns if col.startswith('c-')]\n\nprint(len(G_COLS), len(C_COLS))\n\n# make a copy for preprocessing\ntrain = train_features.copy()\ntest = test_features.copy()\n\n# -------\n# scaling\n# -------\nscaler = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n# scaler = GaussRankScaler(epsilon=1e-3, interp_kind='linear')    #interp_kind: 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n\nn_train = len(train_features.values)\nn_test = len(test_features.values)\n\n# fit in train - transform test\nscaler.fit(train_features[G_COLS + C_COLS].values)\ntrain[G_COLS + C_COLS] = scaler.transform(train_features[G_COLS + C_COLS].values)\ntest[G_COLS + C_COLS] = scaler.transform(test_features[G_COLS + C_COLS].values)\n\nprint('> [RankGauss] tensors shape:', train.shape, test.shape)\n\n# -------\n# PCA \n# -------\n\n# ----- GENES -----\ndata = pd.concat([train[G_COLS], test[G_COLS]])                      # on scaled features\n# data = pd.concat([train_features[G_COLS], test_features[G_COLS]])      # on original-unscaled features\n\ndata2 = (PCA(n_components=N_COMP_G, random_state=42).fit_transform(data[G_COLS].values))\ntrain_pca_genes = pd.DataFrame(data2[:train.shape[0]], columns=[f'pca_G-{i}' for i in range(N_COMP_G)])\ntest_pca_genes = pd.DataFrame(data2[-test.shape[0]:], columns=[f'pca_G-{i}' for i in range(N_COMP_G)])\n\n# ----- CELLS -----\ndata = pd.concat([train[C_COLS], test[C_COLS]])                      # on scaled features\n# data = pd.concat([train_features[C_COLS], test_features[C_COLS]])      # on original-unscaled  features\n\ndata2 = (PCA(n_components=N_COMP_C, random_state=42).fit_transform(data[C_COLS].values))\ntrain_pca_cells = pd.DataFrame(data2[:train.shape[0]], columns=[f'pca_C-{i}' for i in range(N_COMP_C)])\ntest_pca_cells = pd.DataFrame(data2[-test.shape[0]:], columns=[f'pca_C-{i}' for i in range(N_COMP_C)])\n\n# Merge all PCA features\ntrain = pd.concat([train, train_pca_genes, train_pca_cells], axis=1)\ntest = pd.concat([test, test_pca_genes, test_pca_cells], axis=1)\n\ndel data2, train_pca_genes, test_pca_genes, train_pca_cells, test_pca_cells\ngc.collect()\n\nprint('> [PCA] tensors shape:', train.shape, test.shape)\n\n\n# -------\n# Variance Threshold \n# -------\n\nnum_cols = [col for col in train.columns if train[col].dtype == np.float]\nother_cols = [col for col in train.columns if train[col].dtype != np.float]\n\nvar_thresh = VarianceThreshold(0.8)  # VAR_THRES 0.7 - 0.8\n\n# fit train\/test separate\ntrain_features_transformed = var_thresh.fit_transform(train.iloc[:, 4:])   # num_cols\ntest_features_transformed = var_thresh.transform(test.iloc[:, 4:])\n\ntmp_tr = pd.DataFrame(train_features[other_cols].values.reshape(-1, 4), columns=other_cols)\ntrain = pd.concat([tmp_tr, pd.DataFrame(train_features_transformed)], axis=1)\n\ntmp_te = pd.DataFrame(test_features[other_cols].values.reshape(-1, 4), columns=other_cols)\ntest = pd.concat([tmp_te, pd.DataFrame(test_features_transformed)], axis=1)\n\ndel tmp_tr, tmp_te\ngc.collect()\n\nprint('> [Var. Threshold] tensors shape:', train.shape, test.shape)\n\n\n# train, test = fe_cluster(train,test)\n# print('> [FE Clustering] tensors shape:', train.shape, test.shape)\n\n# train, test = fe_stats(train, test)\n# print('> [FE stats] tensors shape:', train.shape, test.shape)\n\n# -------\n# Filter ctl samples\n# -------\ntrain = train.merge(train_targets_scored, on='sig_id')\n\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n# test = test[test['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)       # TODO: don't remove from test\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget = train[train_targets_scored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\ntrain_targets = target[target_cols]\n\nprint('> Filter CTL samples:', train.shape, test.shape, train_targets.shape)\n\n\n\n# -------\n# encode categ cols\n# -------\n\ntrain = process_data(train)\ntest = process_data(test)\n\nfeature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id']]\n\nnum_features = len(feature_cols)\nnum_targets = len(target_cols)\n\nprint('No. of features:', num_features)     # 1091\nprint('No. of target cols:', num_targets) ","95419c2b":"# Prepare non-scored data\n\ntransfer_data = train.copy()\ntransfer_data = transfer_data.merge(train_targets_nonscored, how='inner', on='sig_id')\ntransfer_data = transfer_data.drop(['sig_id'], axis=1)\n\nY_transfer = transfer_data.loc[:, train_targets_nonscored.columns[1:]]\ntransfer_data = transfer_data.loc[:, feature_cols]\n\nprint('Transfer learning from non-scored targets dataset shape:', transfer_data.shape, Y_transfer.shape)\n# print(transfer_data.head())","b59e44bb":"# make folds \n\nfolds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=N_FOLDS)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(train, target[target_cols])):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds.sample(10)","147afaf3":"# define the hp search space\n\ndim_num_dense_nodes_1 = Integer(low=512, high=2048, name='num_dense_nodes_1')\ndim_activation = Categorical(categories=[tf.nn.relu, tf.nn.elu, tf.nn.leaky_relu], name='activation')\ndim_dropout1 = Integer(low=1, high=5, name='dp1')\ndim_dropout2 = Integer(low=1, high=5, name='dp2')\n\n# dim_num_dense_nodes_2 = Integer(low=512, high=1024, name='num_dense_nodes_2')\n# dim_num_dense_nodes_3 = Integer(low=256, high=512, name='num_dense_nodes_3')\n# dim_dropout3 = Integer(low=1, high=5, name='dp3')\n# dim_dropout4 = Integer(low=1, high=5, name='dp4')\n# dim_smooth = Real(low=0.0001, high=0.005, name='smooth')\n# dim_learning_rate = Real(low=1e-4, high=1e-1, prior='uniform', name='learning_rate')\n# dim_look_ahead = Integer(low=5, high=15, name='look_ahead')\n# dim_num_dense_layers = Integer(low=2, high=10, name='num_dense_layers')\n\ndimensions = [\n    dim_num_dense_nodes_1,\n    dim_activation,\n    dim_dropout1,\n    dim_dropout2,\n]\n\n\n# set default params - make sure are within the search space\ndefault_params = [512, tf.nn.relu, 5, 5]  \n\nassert len(default_params)==len(dimensions), 'Error: check shapes!'","4bbbfa7f":"n_inputs = len(feature_cols) # len(top_feats)\nn_outs = len(target_cols)\n\ndef create_model(num_dense_nodes_1, activation, dp1, dp2, out_size=206):\n    \n    inp = Input(shape=n_inputs)\n    x = BatchNormalization()(inp)\n    x = Dropout(0.2)(x)\n    x = tfa.layers.WeightNormalization(Dense(num_dense_nodes_1))(x)\n    #   x = BatchNormalization()(x)\n    x = Activation(activation)(x)\n    x = Dropout(dp1*0.1)(x)\n\n    x = BatchNormalization()(x)\n    x = tfa.layers.WeightNormalization(Dense(num_dense_nodes_1))(x)\n    #   x = BatchNormalization()(x)\n    x = Activation(activation)(x)\n    x = Dropout(dp2*0.1)(x)\n    \n    out = tfa.layers.WeightNormalization(Dense(out_size, activation=\"sigmoid\"))(x)                            \n    model = tf.keras.models.Model(inp, out)\n    \n    # optimizers\n    #     opt = tf.keras.optimizers.Adam(lr=1e-3)   # learning_rate\n    #     opt = tfa.optimizers.SWA(opt, 100)\n    opt = tfa.optimizers.AdamW(learning_rate=LR, weight_decay=WD)\n    #     opt = tfa.optimizers.Lookahead(opt, sync_period=int(look_ahead))\n    \n    # compile model\n    model.compile(optimizer=opt, \n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=p_min), \n                  metrics=[tf.keras.losses.BinaryCrossentropy(), 'AUC'])\n\n    return model\n\n\ndef transfer_weight(model_source, model_dest):\n    for i in range(len(model_source.layers[:-1])):\n        model_dest.layers[i].set_weights(model_source.layers[i].get_weights())\n    return model_dest","6dd55464":"path_best_model = '.\/model.h5'\nbest_loss = np.inf","f77046f9":"@use_named_args(dimensions=dimensions)\ndef fitness(num_dense_nodes_1, activation, dp1, dp2):   \n    \n    \"\"\"\n    Hyper-parameters:\n    num_dense_nodes:   Number of nodes in layer 4.\n    activation:        Activation function for all layers.\n    dp:                Dropout rates (x4)\n    \"\"\"\n\n    # Print the hyper-parameters.\n    print()\n    print('-'*20)\n    print('num_hidden_size:', num_dense_nodes_1)\n    print('activation:',activation)\n    print('dropout 1:', dp1*0.1)\n    print('dropout 2:', dp2*0.1)\n    print('-'*20)\n\n    \n    fold_log_loss = 0\n    \n    for fold_id in range(N_FOLDS):\n        \n        print(f'Fold: {fold_id}\/{N_FOLDS}')\n        K.clear_session()\n        \n        x_train, x_val, y_train, y_val = getFold(folds, fold_id)\n        x_test = test[feature_cols]\n\n        # Pretrain model with non-scored targets\n        print('> Pretrain model with non-scored targets')\n        model_nonscored = create_model(num_dense_nodes_1, activation, dp1, dp2, out_size=Y_transfer.shape[1])\n        \n        cp = ModelCheckpoint(f'model_learned_fold{fold_id}.h5', monitor='val_binary_crossentropy', save_best_only=True, save_weights_only=True, verbose=0)\n        rlr = ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.3, patience=5, verbose=0)   # epsilon=1e-4, mode='min'\n        es = EarlyStopping(monitor='val_binary_crossentropy', patience=15, mode='min', verbose=0)\n        \n        model_nonscored.fit(x_train.values, Y_transfer.iloc[x_train.index].values,\n                            epochs=60,\n                            batch_size=128,\n                            validation_data=(x_val.values, Y_transfer.iloc[x_val.index].values),\n                            callbacks=[es, cp, rlr],\n                            verbose=VERBOSE)\n        \n        model_nonscored.load_weights(f'model_learned_fold{fold_id}.h5')\n        \n        #  Training model with transfer learning\n        print('> Training with transfered weights')\n        model = create_model(num_dense_nodes_1, activation, dp1, dp2, out_size=206)\n\n        model = transfer_weight(model_source=model_nonscored, model_dest=model)\n        for layer in model.layers:\n            layer.trainable = True\n\n        checkpoint_path = f'model_fold{fold_id}.h5'\n        cp = ModelCheckpoint(checkpoint_path, monitor='val_binary_crossentropy', verbose=0, save_best_only=True, mode='min')\n        rlr = ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.1, patience=5, verbose=0) # epsilon=1e-4, mode='min'\n        es = EarlyStopping(monitor='val_binary_crossentropy', patience=15, mode='min')\n\n        # train the model\n        hist = model.fit(x_train.values, y_train.values,\n                         epochs=EPOCHS,\n                         batch_size=BATCH_SIZE,\n                         validation_data=(x_val.values, y_val.values),\n                         callbacks=[es, cp, rlr],\n                         verbose=VERBOSE)\n    \n    \n        # Get the error on the validation-set\n        log_loss = min(hist.history['val_binary_crossentropy'])   \n        \n        fold_log_loss += log_loss\/N_FOLDS\n\n        # Print the classification accuracy.\n        \n        print(f\"> Fold {fold_id} Logloss: {log_loss}\")\n        print('-'*20)\n\n#     # Save the model if it improves on the best-found performance.\n#     global best_loss\n\n#     # If the classification accuracy of the saved model is improved ...\n#     if fold_log_loss < best_loss:\n        \n#         # Save the new model & Update the error\n#         model.save(path_best_model)\n#         best_loss = fold_log_loss\n\n#     # Delete the Keras model with these hyper-parameters from memory.\n#     del model\n#     gc.collect()\n    \n#     # Clear the Keras session, to empty the TensorFlow graph \n#     K.clear_session()\n    \n    return fold_log_loss","a2f9139c":"# # check objective function (uncomment bellow if you like to test)\n\n# fitness(default_params)","ee64a10f":"search_result = skopt.gp_minimize(func=fitness,   \n                            dimensions=dimensions,\n                            acq_func='EI',    #  'gp_hedge'       \n                            n_calls=N_TRIALS,\n                            random_state=SEED,\n                            x0=default_params)","39f45147":"print('optimal hyper-parameters') \nprint()\nprint(f'dense_units 1: {search_result.x[0]}')\nprint(f'activation: {search_result.x[1]}')\nprint(f'dropout 1: {search_result.x[2]}')\nprint(f'dropout 2: {search_result.x[3]}')\n\n# ----------------------------\n# optimal hyper-parameters v.3\n# ----------------------------\n# dense_units 3: 511\n# activation: elu\n# dropout 1: 3\n# dropout 2: 4\n# dropout 3: 5\n# dropout 4: 2","af798b2b":"pd.DataFrame(sorted(zip(search_result.func_vals, search_result.x_iters)), index=np.arange(N_TRIALS), columns=['score', 'params'])","4a7646a0":"%matplotlib inline\nplot_convergence(search_result)","d20c5f61":"# create a list for plotting\ndim_names = ['num_dense_nodes_1', 'activation', 'dropout_1', 'dropout_2']\n\n# %matplotlib inline\nplot_objective(result=search_result, dimensions=dim_names);","1e4c804e":"# create model with best hyperparams\n\nmodel = create_model(*search_result.x)\nmodel.summary()","45bd80ca":"\n# oof = targets.copy()\n# subm.loc[:, target_cols] = 0\n# oof.loc[:, target_cols] = 0\n\n\n# for fold_id in range(N_FOLDS):\n    \n#     print(f'Fold: {fold_id}\/{N_FOLDS}')\n#     K.clear_session()\n        \n#     x_train, x_val, y_train, y_val = getFold(folds, fold_id)\n#     x_test = test[feature_cols]\n\n#     checkpoint_path = f'best_model_fold_{fold_id}.h5'\n#     cp = ModelCheckpoint(checkpoint_path, monitor='val_binary_crossentropy', verbose=1, save_best_only=True, mode='min')\n#     rlr = ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.1, patience=5, verbose=0) # epsilon=1e-4, mode='min'\n#     es = EarlyStopping(monitor='val_binary_crossentropy', patience=15, mode='min')\n\n#     model = create_model(*search_result.x)\n\n#     model.fit(x_train.values, y_train.values,\n#               validation_data=(x_val.values, y_val.values),\n#               epochs=EPOCHS, \n#               batch_size=BATCH_SIZE, \n#               callbacks=[es, rlr, cp], \n#               verbose=VERBOSE)\n\n#     model.load_weights(checkpoint_path)\n\n#     subm.loc[ss.sig_id.isin(test.sig_id), target_cols] += model.predict(x_test.values)\n#     oof.loc[x_val.index, target_cols] += model.predict(x_val.values)\n#     print('-'*20)\n\n# subm.loc[:, target_cols] \/= FOLDS","e175aa7c":"# metrics = []\n# for _target in target_cols:\n#     metrics.append(log_loss(targets.loc[:, _target], oof.loc[:, _target]))\n    \n# print(f'OOF Metric: {np.mean(metrics).round(8)}')","a2a20285":"# if CLIP:\n#     subm.loc[:, target_cols] = np.clip(subm.loc[:, target_cols], p_min, p_max)\n    \n# subm.loc[test_features['cp_type']=='ctl_vehicle', target_cols] = 0\n# subm.to_csv('submission.csv', index=False)\n# print('> Submission saved!')","5a0afd27":"# subm.head()","67c25324":"# Reproduce Model with best hyperparams","661b0b96":"# Results","3dc10313":"## NN architrecture & hyper-parameters","a5a61ea8":"\n\nNote the function decorator `@use_named_args` which wraps the fitness function so that it can be called with all the parameters as a single list, for example: `fitness(x=[1e-4, 3, 256, 'relu'])`. This is the calling-style skopt uses internally.","26617e83":"# Load & Preprocess data (add FE)","301e7604":"# Fittness Function to Optimize\n\n\nThis is the function that creates and trains a neural network with the given hyper-parameters, and then evaluates its performance on the validation-set. The function then returns the so-called fitness value (aka. objective value), which is the negative classification accuracy on the validation-set. It is negative because skopt performs minimization instead of maximization.\n\nThe main steps that we perform are:\n\n- build and train a network with given hyper-parameters\n- evaluate the model performance with the validation dataset\n- It returns the fitness value, in our case the logloss error.\n","ec974e2e":"# Use Bayesian Optimization for Hyper-parameter search\nThe idea with Bayesian optimization is to construct another model of the search-space for hyper-parameters. One kind of model is known as a Gaussian Process. This gives us an estimate of how the performance varies with changes to the hyper-parameters. Whenever we evaluate the actual performance for a set of hyper-parameters, we know for a fact what the performance is - except perhaps for some noise. We can then ask the Bayesian optimizer to give us a new suggestion for hyper-parameters in a region of the search-space that we haven't explored yet, or hyper-parameters that the Bayesian optimizer thinks will bring us most improvement. We then repeat this process a number of times until the Bayesian optimizer has built a good model of how the performance varies with different hyper-parameters, so we can choose the best parameters.\n\nThe flowchart of the algorithm is roughly:","8ce1d024":"# Submission!","129a1a71":"### The purpose of the kernel is to demonstrate how to find an optimal set of hyper-parameters for various NN models\n\n\n### UPDATES:\n\n### v.11: Model architecture with 2-layers & Transfer learning + FE - Optimize for MS-KFold logloss \n\n    --> `hp`: `dense_units`, `activations`, `dropouts`\n\n----------------\n\n- v.10: Model architecture: 2048-1024-786  -  Optimize for MS-KFold logloss \n\n- v9: train with all features; best params\n\n- v7: best params (tbc)\n\n- v.1,2: Model architecture: 2048-X-X-X     \n-   v.3: Model architecture: 2048-1024-X-X   --> hp: dense_units, dropouts, activations\n- v.4-5: Model architecture: 2048-1024-512-X --> hp: dense_units, dropouts, activations, look_ahead + (fix compatibility issues)\n\n","ab152edd":"# Config","cf5b79b5":"# Helpers","80beb02d":"# Imports\n\n### NOTE: make sure you downgrade sklearn --> install it from the .whl (due to compatibility issues)","5573ff11":"![Screenshot%202020-09-15%20at%202.08.57%20PM.png](attachment:Screenshot%202020-09-15%20at%202.08.57%20PM.png)","4220e567":"## Create Model","26d92dd9":"## Credits\n\n- [towards-data-science article](https:\/\/towardsdatascience.com\/bayesian-hyper-parameter-optimization-neural-networks-tensorflow-facies-prediction-example-f9c48d21f795)\n\n- [skopt documentation and examples](https:\/\/scikit-optimize.github.io\/stable\/user_guide.html)\n\n\n\n## Reference notebooks\n\n- https:\/\/www.kaggle.com\/stanleyjzheng\/baseline-nn-with-k-folds \n- https:\/\/www.kaggle.com\/simakov\/multilabel-neural-network\n- https:\/\/www.kaggle.com\/ravy101\/drug-moa-tf-keras-starter","bc00b847":"### Let's visualize the progress of the whole optimization session, where the fitness values are shown on y-axis.","3dd0bde0":"# Bayesian Optimisation NN","c1b75dd9":"# Run Bayesian Optimization\n\nWe may control the runtime of the optimization process by `N_TRIALS`: selecting the no. of evaluation runs"}}