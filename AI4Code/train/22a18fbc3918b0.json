{"cell_type":{"69f10b0c":"code","995ad345":"code","f7c82754":"code","25c27db3":"code","dac4aa5d":"code","6075b608":"code","de940956":"code","62d49e8c":"code","1c19922e":"code","3693a149":"code","ce65587d":"code","cfd36562":"code","129c548f":"code","91f2e86c":"code","21fa22b7":"code","0b033c46":"code","1bffd521":"code","e5fc6812":"code","22fe88ab":"code","9720d02c":"code","7cfd7487":"code","240b9fae":"code","cb963eda":"code","eddfb12d":"code","4b8c8fb4":"code","dc467c30":"code","1ed8e03a":"code","01a0544e":"code","4cfeeb57":"code","6a635644":"code","8a16af8c":"code","5a50749d":"code","b620b647":"code","311d92cf":"code","6e72286f":"code","e344e3b1":"code","e01adf26":"code","f9657a5c":"code","12295330":"code","6598cea1":"code","68731329":"code","3565e33e":"code","3de93d4c":"code","2f0f20d7":"code","e1759c80":"code","1390087c":"code","6eac7524":"code","e91da9a0":"code","e827b874":"code","14b6f512":"code","94a49b72":"code","91f6e603":"code","72c76eda":"code","3c23aba0":"code","79947f67":"markdown","7c24739a":"markdown","1887f809":"markdown","ff6fdbef":"markdown","18f8f8f8":"markdown","45d5281f":"markdown","c63d3544":"markdown","200cec48":"markdown","89576785":"markdown","48e5d16c":"markdown","f7921707":"markdown","bbee1e47":"markdown","4d17ad97":"markdown","74d98b3b":"markdown","bc380fa7":"markdown","fb1737b2":"markdown","1a618516":"markdown","35458019":"markdown","ae39de63":"markdown","593da933":"markdown","225a38ed":"markdown","48c97b1e":"markdown","5a2e08af":"markdown"},"source":{"69f10b0c":"%pip install --upgrade scikit-learn\n\n# Did this to use latest regressors from sklearn...","995ad345":"# Loading neccesary packages.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n#\n\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\n\n#\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n#\n\nimport warnings\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","f7c82754":"# Loading datasets.\n\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","25c27db3":"# Shape of the train data.\n\ndisplay(train.shape)\n","dac4aa5d":"# Shape of the test data.\n\ndisplay(test.shape)\n","6075b608":"# First 5 entries of train data.\n\ntrain.head()","de940956":"# First 5 entries of test data.\n\ntest.head()","62d49e8c":"# General statistics of the train data.\n\ntrain.describe()","1c19922e":"# General statistics of the test data.\n\ntest.describe()","3693a149":"# Dropping unnecessary Id columns.\n\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","ce65587d":"# Backing up target variables and dropping them from train data.\n\n\ny = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test","cfd36562":"# Display numerical correlations between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","129c548f":"# Merging features\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","91f2e86c":"def missing_percentage(df):\n    \n    '''A function for showing missing data values'''\n    \n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","21fa22b7":"# Checking NaN values.\n\nmissing = missing_percentage(features)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))","0b033c46":"# List of NaN's including columns where NaN's mean none.\n\nnone_cols = [\n    'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n]\n# List of NaN's including columns where NaN's mean 0.\n\nzero_cols = [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'\n]\n\n# List of NaN's including columns where NaN's actually missing gonna replaced with mode.\n\nfreq_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n    'SaleType', 'Utilities'\n]\n\n# Filling the list of columns above:\n\nfor col in zero_cols:\n    features[col].replace(np.nan, 0, inplace=True)\n\nfor col in none_cols:\n    features[col].replace(np.nan, 'None', inplace=True)\n\nfor col in freq_cols:\n    features[col].replace(np.nan, features[col].mode()[0], inplace=True)","1bffd521":"# Filling MSZoning according to MSSubClass.\n\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].apply(\n    lambda x: x.fillna(x.mode()[0]))","e5fc6812":"# Filling MSZoning according to Neighborhood.\n\nfeatures['LotFrontage'] = features.groupby(\n    ['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))","22fe88ab":"# Features which numerical on data but should be treated as category.\n\nfeatures['MSSubClass'] = features['MSSubClass'].astype(str)\n\nfeatures['YrSold'] = features['YrSold'].astype(str)\n\nfeatures['MoSold'] = features['MoSold'].astype(str)","9720d02c":"# Transforming rare values(less than 10) into one group.\n\nothers = [\n    'Condition1', 'Condition2', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n    'Heating', 'Electrical', 'Functional', 'SaleType'\n]\n\nfor col in others:\n    mask = features[col].isin(\n        features[col].value_counts()[features[col].value_counts() < 10].index)\n    features[col][mask] = 'Other'","7cfd7487":"def srt_box(y, df):\n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['object']).columns, axes):\n\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,\n                    y=y,\n                    data=df,\n                    palette='plasma',\n                    order=sortd.index,\n                    ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()","240b9fae":"# Displaying sale prices vs. categorical values.\n\nsrt_box('SalePrice', train)","cb963eda":"# Converting some of the categorical values to numeric ones.\n\nneigh_map = {\n    'MeadowV': 1,\n    'IDOTRR': 1,\n    'BrDale': 1,\n    'BrkSide': 2,\n    'OldTown': 2,\n    'Edwards': 2,\n    'Sawyer': 3,\n    'Blueste': 3,\n    'SWISU': 3,\n    'NPkVill': 3,\n    'NAmes': 3,\n    'Mitchel': 4,\n    'SawyerW': 5,\n    'NWAmes': 5,\n    'Gilbert': 5,\n    'Blmngtn': 5,\n    'CollgCr': 5,\n    'ClearCr': 6,\n    'Crawfor': 6,\n    'Veenker': 7,\n    'Somerst': 7,\n    'Timber': 8,\n    'StoneBr': 9,\n    'NridgHt': 10,\n    'NoRidge': 10\n}\n\nfeatures['Neighborhood'] = features['Neighborhood'].map(neigh_map).astype(\n    'int')\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['ExterQual'] = features['ExterQual'].map(ext_map).astype('int')\nfeatures['ExterCond'] = features['ExterCond'].map(ext_map).astype('int')\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['BsmtQual'] = features['BsmtQual'].map(bsm_map).astype('int')\nfeatures['BsmtCond'] = features['BsmtCond'].map(bsm_map).astype('int')\nbsmf_map = {\n    'None': 0,\n    'Unf': 1,\n    'LwQ': 2,\n    'Rec': 3,\n    'BLQ': 4,\n    'ALQ': 5,\n    'GLQ': 6\n}\n\nfeatures['BsmtFinType1'] = features['BsmtFinType1'].map(bsmf_map).astype('int')\nfeatures['BsmtFinType2'] = features['BsmtFinType2'].map(bsmf_map).astype('int')\nheat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['HeatingQC'] = features['HeatingQC'].map(heat_map).astype('int')\nfeatures['KitchenQual'] = features['KitchenQual'].map(heat_map).astype('int')\nfeatures['FireplaceQu'] = features['FireplaceQu'].map(bsm_map).astype('int')\nfeatures['GarageCond'] = features['GarageCond'].map(bsm_map).astype('int')\nfeatures['GarageQual'] = features['GarageQual'].map(bsm_map).astype('int')","eddfb12d":"# Plotting numerical features with polynomial order to detect outliers.\n\ndef srt_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()","4b8c8fb4":"srt_reg('SalePrice', train)","dc467c30":"# Dropping outliers after detecting them by eye.\n\nfeatures = features.join(y)\nfeatures = features.drop(features[(features['OverallQual'] < 5)\n                                  & (features['SalePrice'] > 200000)].index)\nfeatures = features.drop(features[(features['GrLivArea'] > 4000)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['GarageArea'] > 1200)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['TotalBsmtSF'] > 3000)\n                                  & (features['SalePrice'] > 320000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] < 3000)\n                                  & (features['SalePrice'] > 600000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] > 3000)\n                                  & (features['SalePrice'] < 200000)].index)\n\ny = features['SalePrice']\ny.dropna(inplace=True)\nfeatures.drop(columns='SalePrice', inplace=True)","1ed8e03a":"# Creating new features based on previous observations.\n\nfeatures['TotalSF'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                       features['1stFlrSF'] + features['2ndFlrSF'])\nfeatures['TotalBathrooms'] = (features['FullBath'] +\n                              (0.5 * features['HalfBath']) +\n                              features['BsmtFullBath'] +\n                              (0.5 * features['BsmtHalfBath']))\n\nfeatures['TotalPorchSF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                            features['EnclosedPorch'] +\n                            features['ScreenPorch'] + features['WoodDeckSF'])\n\nfeatures['YearBlRm'] = (features['YearBuilt'] + features['YearRemodAdd'])\n\n# Merging quality and conditions.\n\nfeatures['TotalExtQual'] = (features['ExterQual'] + features['ExterCond'])\nfeatures['TotalBsmQual'] = (features['BsmtQual'] + features['BsmtCond'] +\n                            features['BsmtFinType1'] +\n                            features['BsmtFinType2'])\nfeatures['TotalGrgQual'] = (features['GarageQual'] + features['GarageCond'])\nfeatures['TotalQual'] = features['OverallQual'] + features[\n    'TotalExtQual'] + features['TotalBsmQual'] + features[\n        'TotalGrgQual'] + features['KitchenQual'] + features['HeatingQC']\n\n# Creating new features by using new quality indicators.\n\nfeatures['QualGr'] = features['TotalQual'] * features['GrLivArea']\nfeatures['QualBsm'] = features['TotalBsmQual'] * (features['BsmtFinSF1'] +\n                                                  features['BsmtFinSF2'])\nfeatures['QualPorch'] = features['TotalExtQual'] * features['TotalPorchSF']\nfeatures['QualExt'] = features['TotalExtQual'] * features['MasVnrArea']\nfeatures['QualGrg'] = features['TotalGrgQual'] * features['GarageArea']\nfeatures['QlLivArea'] = (features['GrLivArea'] -\n                         features['LowQualFinSF']) * (features['TotalQual'])\nfeatures['QualSFNg'] = features['QualGr'] * features['Neighborhood']","01a0544e":"# Observing the effects of newly created features on sale price.\n\ndef srt_reg(feature):\n    merged = features.join(y)\n    fig, axes = plt.subplots(5, 3, figsize=(25, 40))\n    axes = axes.flatten()\n\n    new_features = [\n        'TotalSF', 'TotalBathrooms', 'TotalPorchSF', 'YearBlRm',\n        'TotalExtQual', 'TotalBsmQual', 'TotalGrgQual', 'TotalQual', 'QualGr',\n        'QualBsm', 'QualPorch', 'QualExt', 'QualGrg', 'QlLivArea', 'QualSFNg'\n    ]\n\n    for i, j in zip(new_features, axes):\n\n        sns.regplot(x=i,\n                    y=feature,\n                    data=merged,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n\n\n","4cfeeb57":"srt_reg('SalePrice')","6a635644":"# Creating some simple features.\n\nfeatures['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['Has2ndFloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['HasGarage'] = features['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['HasBsmt'] = features['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['HasPorch'] = features['QualPorch'].apply(lambda x: 1 if x > 0 else 0)","8a16af8c":"possible_skewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'PoolArea', 'LowQualFinSF', 'MiscVal'\n]","5a50749d":"# Finding skewness of the numerical features.\n\nskew_features = np.abs(features[possible_skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Filtering skewed features.\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Taking indexes of high skew.\n\nskew_index = high_skew.index\n\n# Applying boxcox transformation to fix skewness.\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","b620b647":"# Features to drop.\n\nto_drop = [\n    'Utilities',\n    'PoolQC',\n    'YrSold',\n    'MoSold',\n    'ExterQual',\n    'BsmtQual',\n    'GarageQual',\n    'KitchenQual',\n    'HeatingQC',\n]\n\n# Dropping features\n\nfeatures.drop(columns=to_drop, inplace=True)","311d92cf":"# Getting dummy variables for ategorical data.\n\nfeatures = pd.get_dummies(data=features)","6e72286f":"print(f'Number of missing values: {features.isna().sum().sum()}')","e344e3b1":"features.shape","e01adf26":"features.sample(5)","f9657a5c":"features.describe()","12295330":"# Separating train and test set.\n\ntrain = features.iloc[:len(y), :]\ntest = features.iloc[len(train):, :]","6598cea1":"correlations = train.join(y).corrwith(train.join(y)['SalePrice']).iloc[:-1].to_frame()\ncorrelations['Abs Corr'] = correlations[0].abs()\nsorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(sorted_correlations.to_frame()[sorted_correlations>=.5], cmap='coolwarm', annot=True, vmin=-1, vmax=1, ax=ax);\n","68731329":"def plot_dist3(df, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    \n    # Creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # Plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.legend(labels=['Normal', 'Actual'])\n\n    # Customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    \n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n\n    # Customizing the Box Plot.\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(df.loc[:, feature], orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24)","3565e33e":"# Checking target variable.\n\nplot_dist3(train.join(y), 'SalePrice', 'Sale Price Before Log Transformation')","3de93d4c":"# Setting model data:\n\nX = train\nX_test = test\ny = np.log1p(y)","2f0f20d7":"plot_dist3(train.join(y), 'SalePrice', 'Sale Price After Log Transformation')","e1759c80":"# Loading neccesary packages for modelling:\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, TweedieRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor","1390087c":"# Setting kfold for future use.\n\nkf = KFold(10, random_state=42)","6eac7524":"alphas_alt = [15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [\n    5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008\n]\ne_alphas = [\n    0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007\n]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv:\n\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,\n))\n\n# lasso_cv:\n\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n# elasticnet_cv:\n\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n# svr:\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n\n# gradientboosting:\n\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm:\n\nlightgbm = LGBMRegressor(objective='regression',\n                         n_estimators=3500,\n                         num_leaves=5,\n                         learning_rate=0.00721,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n# xgboost:\n\nxgboost = XGBRegressor(\n    learning_rate =0.0139,\n    n_estimators =4500,\n    max_depth =4,\n    min_child_weight =0,\n    subsample =0.7968,\n    colsample_bytree =0.4064,\n    nthread =-1,\n    scale_pos_weight =2,\n    seed=42,\n)\n\n\n# histgradientboost:\n\nhgrd= HistGradientBoostingRegressor(    loss= 'least_squares',\n    max_depth = 2,\n    min_samples_leaf = 40,\n    max_leaf_nodes = 29,\n    learning_rate = 0.15,\n    max_iter = 225,\n                                    random_state=42)\n\n#tweedie regresson:\n\ntweed = make_pipeline(RobustScaler(),TweedieRegressor(alpha=0.005))\n\n\n# stacking regressor:\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                            xgboost, lightgbm,hgrd, tweed),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","e91da9a0":"def model_check(X, y, estimators, cv):\n    \n    ''' A function for testing multiple estimators.'''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","e827b874":"# Setting list of estimators and labels for them.\n\nestimators = [ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, hgrd, tweed]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'XGBRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor','TweedieRegressor'\n]","14b6f512":"# Executing cross validation.\n\nraw_models = model_check(X, y, estimators, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","94a49b72":"# Fitting the models on train data:\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\n\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X.values, y.values)\n\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X, y)\n\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n\nprint(datetime.now(), 'XGboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint(datetime.now(), 'Lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\n\nprint(datetime.now(), 'Hist')\nhist_full_data = hgrd.fit(X, y)\n\nprint(datetime.now(), 'Tweed')\ntweed_full_data = tweed.fit(X, y)\n\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","91f6e603":"# Blending models by assigning weights.\n\ndef blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) +\n            (0.1 * lasso_model_full_data.predict(X)) +\n            (0.1 * ridge_model_full_data.predict(X)) +\n            (0.1 * svr_model_full_data.predict(X)) +\n            (0.05 * gbr_model_full_data.predict(X)) +\n            (0.1 * xgb_model_full_data.predict(X)) +\n            (0.05 * lgb_model_full_data.predict(X)) +\n            (0.05 * hist_full_data.predict(X)) +\n            (0.1 * tweed_full_data.predict(X)) +\n            (0.25 * stack_gen_model.predict(X.values)))","72c76eda":"submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# Inversing and flooring log scaled sale price predictions to see real prices again.\n\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models_predict(X_test)))\n\n# Creating submission dataframe.\n\nsubmission = submission[['Id', 'SalePrice']]","3c23aba0":"# Saving submission dataframe as csv file:\n\nsubmission.to_csv('mysubmission.csv', index=False)\n\nprint(\n    'Saving submission.',\n    datetime.now(),\n)\nsubmission.head()","79947f67":"## Outliers\n\nOk here we're going to drop some outliers we detected them just above, this part is kinda subjective and you can try different approaches. I might add more statistical method in future for this but for now we're going to remove them manually.","7c24739a":"# Categorical Data\n\n**We already checked some of the numerical features with correlation heatmap but what about categorical values? We want to see relations between categorical data and sale price. Boxplots seems decent way to inspect this type of relation. We're also going to sort them by the median value of that group so we can see the importances in descending order.**\n\n#### Observations:\n\n- **MSZoning;**\n - Floating village houses (I assume they are some kind of special area that retired community resides, has the highest median value.\n - Residental low density houses comes second with the some outliers.\n - Residental high and low seems similar meanwhile commercial is the lowest.\n\n- **LandContour; Hillside houses seems a little bit higher expensive than the rest meanwhile banked houses are the lowest.** \n\n- **Neighborhood;**\n - Northridge Heights, Northridge and Timberland are top 3 expensive places for houses.\n - Somerset, Veenker, Crawford, Clear Creek, College Creek and Bloomington Heights seems above average.\n - Sawyer West has wide range for prices related to similar priced regions.\n - Old Town and Edwards has some outlier prices but they generally below average.\n - Briardale, Iowa DOT and Rail Road, Meadow Village are the cheapest places for houses it seems...\n\n- **Conditions;**\n - Meanwhile having wide range of values being close to North-South Railroad seems having positive effect on the price.\n - Being near or adjacent to positive off-site feature (park, greenbelt, etc.) increases the price.\n - These values are pretty similar but we can get some useful information from them.\n \n- **MasVnrType;** Having stone masonry veneer seems better priced than having brick.\n\n- **Quality Features;** There are many categorical quality values that affects the pricing on some degree, we're going to quantify them so we can create new features based on them. So we don't dive deep on them in this part.\n\n- **CentralAir;** Having central air system has decent positive effect on sale prices.\n\n- **GarageType;** \n\n  - Built-In (Garage part of house - typically has room above garage) garage typed houses are the most expensive ones.\n  - Attached garage types following the built-in ones.\n  - Car ports are the lowest\n  \n- **Misc;** Sale type has some kind of effect on the prices but we won't get into details here. Btw... It seems having tennis court is really adding price to your house, who would have known :)\n\n**Alright, we're done with categorical data inspecting, I'm going to convert some of these categories to numerical ones, especially the ones where related to quality of the specific features.**","1887f809":"- **We're going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**","ff6fdbef":"- **That's quite a lot! No need to panic though we got this. If you look at the data description given to us we can see that most of these missing data actually not missing, it's just means house doesn't have that specific feature, we can fix that easily...**","18f8f8f8":"## Checking New Features\n\nWell... They look decent enough, I hope these can help us building strong models. I also wanted to add some more basic features for having specific feature or not. This approach was widely accepted by community so I see no harm to add them.","45d5281f":"# Analysis Time!\n\nOk the short inspection at the beginning give us some hints how should we move from here. I'm going to play with the data we have while analysing the data at the same time. With this way I hope we can get the data in better shape while digging deeper into it.\n\nThese inspections are going to be some sort of intuitive but we can create new features depending on our assertions.\n\nWe're going to start with basic correlation table here. I dropped the top part since it's just mirror of the other part below. With this table we can understand some linear relations between different features.\n\n#### Observations:\n- There's strong relation between overall quality of the houses and their sale prices.\n- Again above grade living area seems strong indicator for sale price.\n- Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n- There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n- Overall condition of the house seems less important on the pricing, it's interesting and worth digging.\n","c63d3544":"- **Id column looks has no use for now, I think we can safely drop it from both. I'm also going to save our target (SalePrice) on different variable so we can use it in future.**","200cec48":"# Numeric Data\n\nThere are many numeric features the inspect, one of the best ways to see how they effect sale prices is scatter plots. We're also plotting polynomial regression lines to see general trend. With this way we can understand the numerical values and their importance on sale price, also it's really helpful to spot outliers.\n\n#### Observations:\n\n- **OverallQual;** It's clearly visible that sale price of the house increases with overall quality. This confirms the correlation in first table we did at the beginning. (Pearson corr was 0.8)\n\n- **OverallCondition;** Looks like overall condition is left skewed where most of the houses are around 5\/10 condition. But it doesn't effect the price like quality indicator...\n\n- **YearBuilt;** Again new buildings are generally expensive than the old ones.\n\n- **Basement;** General table shows bigger basements are increasing the price but I see some outliers there...\n\n- **GrLivArea;** This feature is pretty linear but we can spot two outliers effecting this trend. There are some huge area houses with pretty cheap prices, there might be some reason behind it but we better drop them.\n\n- **SaleDates;** They seem pretty unimportant on sale prices, we can drop them...\n","89576785":"## Transforming the Data\n\nSome of the continious values are not distributed evenly and not fitting on normal distribution, we can fix them by using couple transformation approaches. We're going to use boxcox here, again it's widely used by community and I want to thank them all for their great work. \n\nWe're going to list skewed features and then apply boxcox transformation with boxcox_normmax (It computes optimal boxcox transform parameter for input data, so we don't decide the lambda here)...","48e5d16c":"## Stacking & Blending\n\nHere we fit every single estimator we have on the train data and then blend them by assigning weights to each model and sum the results. Weights are pretty subjective and I'm pretty sure you can come up with something performs better than this if you play with it...","f7921707":"# Meeting the data\n\nWe're going to start by loading the data and taking first look on it as usual. For the column names we have great dictionary file in our dataset location so we can get familiar with them in no time. I highly recommend looking at that before you start working on the dataset.","bbee1e47":"# Model Results\n\nAlright, our results are here. Looks like our models did pretty close to each other, there might be some overfitting models and we can try to fix them by tuning but it was computationally expensive for me and since I'm going to stack and blend the models I think we can leave them as it is. We already added our models to stacking regression and set the XGBoost as meta regressor we can continue with stacking","4d17ad97":"# Introduction\n\nHello all! In this notebook I'm going to implement what I gained on the way of learning. I'm doing this for learning purposes and share back to community what I learned. So there might be areas can be improved in future.\n\n#### My main objectives on this project are:\n+ Applying exploratory data analysis and trying to get some insights about our dataset\n+ Getting data in better shape by transforming and feature engineering to help us in building better models\n+ Building and tuning couple models to get some stable results on predicting housing prices\n\n#### In this notebook we are going to try explore the data we have and going try answer questions like:\n+ What are the main predictors for house pricing?\n+ What is more important on pricing, having big area for housing or just being in better neighborhood?\n+ Is quality of the house alone more important than having nice garages or basements?\n+ There are some features that can be modified and depends on the building but there are some other features like cannot be changed like location of the house, which group is effecting house prices?\n+ Can we predict the price of a house with the given traning data using machine learning techniques.\n+ What can our predictions achieve with different approaches?\n+ If we stack and blend the models, can we get more regularized results?\n\n\n**I hope you enjoy while reading it! And if you liked this kernel feel free to upvote and leave feedback, thanks!**","74d98b3b":"## Missing Data\n\nAlright, first of all we need detect missing values, then wee need to get rid of them for the next steps of our work. So let's list our missing values and visualize them:","bc380fa7":"### **Ok this is how we gonna fix most of the missing data:**\n\n1. First we fill the NaN's in the columns where they mean 'None' so we gonna replace them with that,\n2. Then we fill numerical columns where missing values indicating there is no parent feature to measure, so we replace them with 0's.\n3. Even with these there are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n4. MSZoning part is little bit tricky I choose to fill them with most common type of the related MSSubClass type. It's not perfect but at least we decrease randomness a little bit.\n5. Again we fill the Lot Frontage with similar approach.","fb1737b2":"**Here we dropping some unnecessary features had their use in feature engineering or not needed at all. Obviously it's subjective but I feel they don't add much to model. Then we one hot encode the categorical data left so everything will be prepared for the modelling.**","1a618516":"# Some Final Words\n\n\n\n### **Thank you for reading!**\n\n### **I'm still a beginner and want to improve myself in every way I can. So if you have any ideas to feedback please let me know in comments section and again if you liked my work please don't forget to vote, thank you again!**","35458019":"# Feature Engineering\n\nOk... This is the part where we dig deeper into our imp\u0131ted dataset. There are no missing values left so we're good to go! I'm going to start with grouping some values, these values are really rare and I'm thinking they do not add much in general, so if they appear less than 10 times in our observations they get into 'Other' group.","ae39de63":"# Double Check\n\n- **Before we move to modelling I want to take one last look to the data we processed. Everyting seems in order, not missing datas, values are numerical etc. Our feature engineered data is present...**\n\n- **Just want to check how transformed data correlates with sale prices before we move on and it looks decent.**\n\n- **Again I wanted to check our target value distribution and it seems little skewed. We can fix this by applying log transformation so our models can perform better.**","593da933":"# Modelling\n\nWell then, it's time to do some modelling! First of all I wanted to thank kaggle community for loads of examples inspired me. Especially Alex Lekov's great script and Serigne's stacked regressions approach were great guides for me!\n\nLet's start with loading packages needed and then we set our regressors. The regressors I'm going to use here are:\n\n- Ridge,\n- Lasso,\n- Elasticnet,\n- Support Vector Regression\n + I'm going to apply robust scaler on these before we run them because they really get effected by outliers.\n- Gradient Boosting Regressor\n- LightGBM Regressor\n- XGBoost Regressor\n + These don't need scaling in my opinion so we just go as it is\n- Hist Gradient Boosting Regressor\n + This is just for experimenting, it's still experimental on sklearn anyways\n- Tweedie Regressor\n + This regressor added in latest version of sklearn and I wanted to try it. It's generalized linear model with a Tweedie distribution. We gonna use power of 0 because we expecting normal target distribution but you can try this or other generalized models like poisson regressor or gamma regressor.\n\nI tried to tune models by using Optuna package, that part is not added here.","225a38ed":"## Submission\n\nOur models are tuned, stacked, fitted and blended so we are ready to predict and submit our results.","48c97b1e":"## Creating New Features\n\nOk in this part we going to create some features, these can improve our modelling. I went with basic approach by merging some important indicators and making them stronger.","5a2e08af":"# Cross Validation"}}