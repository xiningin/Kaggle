{"cell_type":{"f8e07cfc":"code","44d49a9f":"code","1c0ee51f":"code","aef157e8":"code","bf0cb516":"code","3481dc37":"code","dc89d8a0":"code","0e56990f":"code","c5c0e67d":"code","4c73b8be":"code","1c93baee":"code","e687afff":"code","c67db333":"code","4b02204c":"code","c9ed3ee2":"code","43bcb1e9":"code","8719f6cb":"code","1a179a5f":"code","bd6fcd36":"code","ad602033":"code","e088f78f":"code","9b4f40a6":"code","58caeeb9":"code","d5028328":"code","c2ba0591":"code","388ad49a":"code","7aff3d8a":"code","c831e148":"code","62c4b7aa":"code","38f1d035":"code","3abf24e9":"code","55069805":"code","f20731c7":"code","da0ac3d3":"code","37f71d40":"code","ae890cdd":"code","9e078ff1":"code","2c15cfa6":"code","789141f4":"code","87f6b818":"code","1df109ce":"code","d4ae4059":"code","ccfac71c":"code","130cb535":"code","f74c6c64":"code","a9b4f2be":"code","0b959002":"code","98836ee2":"code","17fea6d6":"code","0e91789c":"code","ea72c3c4":"code","db4121b6":"code","75aba0fe":"code","eb612914":"code","1ca16701":"code","f2f0b6e6":"code","7f5e06ac":"code","a55f3dd7":"code","a0703675":"code","b81be011":"code","1bbe8615":"code","2f52450e":"markdown","5d8e46f5":"markdown","c4c1ae70":"markdown","959828b0":"markdown"},"source":{"f8e07cfc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44d49a9f":"df = pd.read_csv(\"..\/input\/world-happiness\/2019.csv\")\ndf.head()","1c0ee51f":"df.isna().sum()","aef157e8":"df.info()","bf0cb516":"df.describe().T","3481dc37":"df.shape","dc89d8a0":"y = df['Score'].values.reshape(-1, 1)\nX = df['GDP per capita'].values.reshape(-1, 1)","0e56990f":"y","c5c0e67d":"X","4c73b8be":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(\"X_train:\",X_train.shape, \"X_test:\", X_test.shape, \"y_train:\", y_train.shape, \"y_test:\", y_test.shape)","1c93baee":"linreg = LinearRegression() # kurucu metodu \u00e7a\u011f\u0131r\u0131yoruz\nlinreg.fit(X_train, y_train) # y_train \u00fczerinden X_train'i \u00f6\u011fretiyoruz","e687afff":"y_pred = linreg.predict(X_test)\n# Daha sonra \u00e7\u0131kan denkleme X_test'i koyup, sonu\u00e7lara y_pred dedik. ","c67db333":"r2_score(y_test, y_pred)","4b02204c":"rms = np.sqrt(mean_squared_error(y_test, y_pred))\nrms","c9ed3ee2":"r2_score(X_test, y_test) # NOT: BARAN K.","43bcb1e9":"sns.regplot(X_train, y_train)\nplt.show()","8719f6cb":"sns.regplot(y_test, y_pred)\nplt.show()","1a179a5f":"linreg.intercept_","bd6fcd36":"linreg.coef_","ad602033":"\nX = df.iloc[:, 3]\nY = df.iloc[:, 2]\nplt.scatter(X, Y)\nplt.show()\n","e088f78f":"model = LinearRegression()\nmodel.fit(X_train, y_train)","9b4f40a6":"plt.scatter(X, Y) \nplt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line\nplt.show()","58caeeb9":"y = df['Score'].values.reshape(-1, 1)\nX = df['GDP per capita'].values.reshape(-1, 1)","d5028328":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(\"X_train:\",X_train.shape, \"X_test:\", X_test.shape, \"y_train:\", y_train.shape, \"y_test:\", y_test.shape)","c2ba0591":"model = LinearRegression()\nmodel.fit(X_train, y_train)","388ad49a":"prediction = model.predict(X_test)\nprint(prediction)","7aff3d8a":"plt.scatter(X_train, y_train, color = 'blue')\nplt.plot(X_train, model.predict(X_train), color = 'red')\n\nplt.xlabel('GDP per capita')\nplt.ylabel('Score')\nplt.show()\n","c831e148":"plt.scatter(X, y, color = 'blue')\nplt.plot(X_train, model.predict(X_train), color = 'red')\n\nplt.xlabel('GDP per capita')\nplt.ylabel('Score')\nplt.show()\n","62c4b7aa":"\nX = df.iloc[:, 3]\nY = df.iloc[:, 2]\nplt.scatter(X, Y)\nplt.show()\n","38f1d035":"plt.rcParams['figure.figsize'] = (15.0, 15.0)\nplt.scatter(X, Y)\nplt.show()","3abf24e9":"\n# Building the model\nm = 0\nc = 0\n\nL = 0.0001  # The learning Rate\nepochs = 1000  # The number of iterations to perform gradient descent\n\nn = float(len(X)) # Number of elements in X\n\n# Performing Gradient Descent \nfor i in range(epochs): \n    Y_pred = m*X + c  # The current predicted value of Y\n    D_m = (-2\/n) * sum(X * (Y - Y_pred))  # Derivative wrt m\n    D_c = (-2\/n) * sum(Y - Y_pred)  # Derivative wrt c\n    m = m - L * D_m  # Update m\n    c = c - L * D_c  # Update c\n    \nprint (m, c)\n","55069805":"Y_pred = m*X + c\n\nplt.scatter(X, Y) \nplt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line\nplt.show()","f20731c7":"r2_score(X, Y) ","da0ac3d3":"#df = df.drop([\"Country or region\", \"Overall rank\"], axis=1)\ny = df[['Score']]\nX = df[['GDP per capita','Social support','Healthy life expectancy','Freedom to make life choices','Generosity','Perceptions of corruption']]","37f71d40":"y.shape","ae890cdd":"X.shape","9e078ff1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nprint(\"X_train:\",X_train.shape, \"X_test:\", X_test.shape, \"y_train:\", y_train.shape, \"y_test:\", y_test.shape)","2c15cfa6":"multireg = LinearRegression() # kurucu metodu \u00e7a\u011f\u0131r\u0131yoruz\nmultireg.fit(X_train, y_train) # y_train \u00fczerinden X_train'i \u00f6\u011fretiyoruz","789141f4":"y_pred = multireg.predict(X_test)\n# Daha sonra \u00e7\u0131kan denkleme X_test'i koyup, sonu\u00e7lara y_pred dedik. ","87f6b818":"r2_score(y_test, y_pred)","1df109ce":"rms = np.sqrt(mean_squared_error(y_test, y_pred))\nrms","d4ae4059":"multireg.intercept_","ccfac71c":"multireg.coef_","130cb535":"conc = np.vstack(y_pred)\ny_pred = pd.DataFrame(conc)\n","f74c6c64":"y_test=y_test.reset_index(drop=True)\n","a9b4f2be":"y_pred.columns =['y_pred']\ny_test.columns=['y_test']","0b959002":"df2=pd.concat([y_pred, y_test,], axis=1)\ndf2['Difference']=df2['y_pred']- df2['y_test']\ndf2['Difference']=df2['Difference']*df2['Difference']\ndf2['Difference']=np.sqrt((df2['Difference']))\ndf2['Percentage of Error']=(df2['Difference'])\/(0.01*df2['y_test'])\n\ndf2.sort_values(by='Percentage of Error',ascending=False)\n","98836ee2":"Z = df[['Score','GDP per capita','Social support','Healthy life expectancy','Freedom to make life choices','Generosity','Perceptions of corruption']]\nZ.corr(method ='pearson') ","17fea6d6":"X = df[['Social support', 'GDP per capita']].values.reshape(-1,2)\nY = df['Score']","0e91789c":"x = X[:, 0]\ny = X[:, 1]\nz = Y","ea72c3c4":"df.sort_values(by='GDP per capita',ascending=False)\n","db4121b6":"x_pred = np.linspace(0, 2, 20)   # range of porosity values\ny_pred = np.linspace(0, 2, 20)  # range of brittleness values\nxx_pred, yy_pred = np.meshgrid(x_pred, y_pred)\nmodel_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n","75aba0fe":"ols = linear_model.LinearRegression()\nmodel = ols.fit(X, Y)\npredicted = model.predict(model_viz)","eb612914":"r2 = model.score(X, Y)","1ca16701":"plt.style.use('default')\n\nfig = plt.figure(figsize=(12, 4))\n\nax1 = fig.add_subplot(131, projection='3d')\nax2 = fig.add_subplot(132, projection='3d')\nax3 = fig.add_subplot(133, projection='3d')\n\naxes = [ax1, ax2, ax3]\n\nfor ax in axes:\n    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n    ax.set_xlabel('GDP per capita', fontsize=12)\n    ax.set_ylabel('Social support', fontsize=12)\n    ax.set_zlabel('Score', fontsize=12)\n    ax.locator_params(nbins=4, axis='x')\n    ax.locator_params(nbins=5, axis='x')\n\nax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n           transform=ax1.transAxes, color='grey', alpha=0.5)\nax2.text2D(0.3, 0.42, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n           transform=ax2.transAxes, color='grey', alpha=0.5)\nax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n           transform=ax3.transAxes, color='grey', alpha=0.5)\n\nax1.view_init(elev=28, azim=120)\nax2.view_init(elev=4, azim=114)\nax3.view_init(elev=60, azim=165)\n\nfig.suptitle('$R^2 = %.2f$' % r2, fontsize=40)\n\nfig.tight_layout()","f2f0b6e6":"X = df[['Generosity', 'Perceptions of corruption']].values.reshape(-1,2)\nY = df['Score']","7f5e06ac":"x = X[:, 0]\ny = X[:, 1]\nz = Y","a55f3dd7":"x_pred = np.linspace(0, 2, 20)   # range of porosity values\ny_pred = np.linspace(0, 2, 20)  # range of brittleness values\nxx_pred, yy_pred = np.meshgrid(x_pred, y_pred)\nmodel_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T","a0703675":"ols = linear_model.LinearRegression()\nmodel = ols.fit(X, Y)\npredicted = model.predict(model_viz)","b81be011":"r2 = model.score(X, Y)","1bbe8615":"plt.style.use('default')\n\nfig = plt.figure(figsize=(12, 4))\n\nax1 = fig.add_subplot(131, projection='3d')\nax2 = fig.add_subplot(132, projection='3d')\nax3 = fig.add_subplot(133, projection='3d')\n\naxes = [ax1, ax2, ax3]\n\nfor ax in axes:\n    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n    ax.set_xlabel('Generosity', fontsize=12)\n    ax.set_ylabel('Perceptions of corruption', fontsize=12)\n    ax.set_zlabel('Score', fontsize=12)\n    ax.locator_params(nbins=4, axis='x')\n    ax.locator_params(nbins=5, axis='x')\n\nax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n           transform=ax1.transAxes, color='grey', alpha=0.5)\nax2.text2D(0.3, 0.42, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n           transform=ax2.transAxes, color='grey', alpha=0.5)\nax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n           transform=ax3.transAxes, color='grey', alpha=0.5)\n\nax1.view_init(elev=28, azim=120)\nax2.view_init(elev=4, azim=114)\nax3.view_init(elev=60, azim=165)\n\nfig.suptitle('$R^2 = %.2f$' % r2, fontsize=40)\n\nfig.tight_layout()","2f52450e":"### ","5d8e46f5":"# Multiple Linear Regression","c4c1ae70":"# EDA","959828b0":"### ---------"}}