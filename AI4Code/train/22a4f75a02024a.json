{"cell_type":{"62ed71b6":"code","e2ccbbc0":"code","ac20f3b0":"code","1460cb04":"code","cb2c6b65":"code","5262dae5":"code","64ed133f":"code","85a581c2":"code","e999e1ca":"code","a5733d20":"code","8f109629":"code","a50b73a8":"code","e46fce40":"code","f29074f2":"code","cee5078b":"code","f868d5a8":"code","3f3c271f":"code","1901c5a6":"code","2425345d":"code","66798a87":"code","20cc6538":"code","46852464":"markdown","32b10566":"markdown","08884270":"markdown","2c555927":"markdown","db41b591":"markdown","537d4638":"markdown","87398b64":"markdown","e73a0e58":"markdown","3802e75b":"markdown","07961f27":"markdown","0446380f":"markdown","953b4b2a":"markdown","b181da94":"markdown","5cd5c03a":"markdown","b353a124":"markdown","c3fc4cd1":"markdown","953e7080":"markdown","52f57a2d":"markdown","9a77f599":"markdown","05aa2d80":"markdown","3241f7df":"markdown"},"source":{"62ed71b6":"from IPython.display import YouTubeVideo\nYouTubeVideo('zEOtG-ChmZE', width=800, height=400)","e2ccbbc0":"import tensorflow as tf\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport transformers \nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport pandas as pd \nimport numpy as np \nfrom sklearn.model_selection import train_test_split","ac20f3b0":"train_data_frame=pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_data_frame =pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\nsample_sub=pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/sample_submission.csv\")","1460cb04":"def Utilize_TPUs():  \n    \"\"\"\n    Initialize training strategy using TPU if available else using default strategy for CPU and  single GPU\n    \n    After the TPU is initialized, you can also use manual device placement to place the computation on a single TPU device.\n\n    \"\"\"\n    try:\n        \n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        REPLICAS = strategy.num_replicas_in_sync\n        print(\"Connected to TPU Successfully:\\n TPUs Initialised with Replicas:\",REPLICAS)\n        \n        return strategy\n    \n    except ValueError:\n        \n        print(\"Connection to TPU Falied\")\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n        \n        return strategy\n    \nstrategy=Utilize_TPUs()","cb2c6b65":"the_chosen_one=\"jplu\/tf-xlm-roberta-base\"\nmax_len=80\nbatch_size = 16 * strategy.num_replicas_in_sync\nAUTO     = tf.data.experimental.AUTOTUNE\nepochs= 20\nn_steps = len(train_data_frame) \/\/ batch_size","5262dae5":"def model_baseline(strategy,transformer):\n    with strategy.scope():\n        transformer_encoder = TFAutoModel.from_pretrained(transformer)\n        input_layer = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        sequence_output = transformer_encoder(input_layer)[0]\n        cls_token = sequence_output[:, 0, :]\n        output_layer = Dense(3, activation='softmax')(cls_token)\n        model = Model(inputs=input_layer, outputs=output_layer)\n        model.compile(\n            Adam(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        return model\nmodel=model_baseline(strategy,the_chosen_one)","64ed133f":"model.summary()","85a581c2":"train_data_frame.head()","e999e1ca":"from plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\nimport seaborn as sns","a5733d20":"import plotly.express as px\nfig = px.bar(train_data_frame, x=train_data_frame['language'])\niplot(fig)","8f109629":"sns.countplot(train_data_frame.label)","a50b73a8":"tokenizer = AutoTokenizer.from_pretrained(the_chosen_one)\n","e46fce40":"train_data = train_data_frame[['premise', 'hypothesis']].values.tolist()\n","f29074f2":"test_data = test_data_frame[['premise', 'hypothesis']].values.tolist()\n","cee5078b":"train_encoded=tokenizer.batch_encode_plus(train_data,pad_to_max_length=True,max_length=max_len)","f868d5a8":"test_encoded=tokenizer.batch_encode_plus(test_data,pad_to_max_length=True,max_length=max_len)","3f3c271f":"x_train, x_valid, y_train, y_valid = train_test_split(train_encoded['input_ids'], train_data_frame.label.values, test_size=0.1)\n\nx_test = test_encoded['input_ids']","1901c5a6":"train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\n\nvalid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size).cache().prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size))","2425345d":"model.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=epochs)","66798a87":"predictions = model.predict(test_dataset, verbose=1)\nsample_sub['prediction'] = predictions.argmax(axis=1)","20cc6538":"sample_sub.to_csv(\"submission.csv\",index= False)","46852464":"# AutoModels\n* In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained method.\n\n* AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name\/path to the pretrained weights\/config\/vocabulary:\n\n* Instantiating one of AutoModel, AutoConfig and AutoTokenizer will directly create a class of the relevant architecture (ex: model = AutoModel.from_pretrained('bert-base-cased') will create a instance of BertModel).","32b10566":"# Making Predictions & Saving","08884270":"# Encoding Data \n\nNumberically representing  text data such that It can be feed to the model ","2c555927":"# Loading Data Into tf.Data.Dataset ","db41b591":"Observations - \n* Max data points are of English Language \n* All other languages are balanced in count ","537d4638":"# Everything You Need To Know About TPUs & Bidirectional Encoder Representations from Transformers [BERT]\n","87398b64":"# Bidirectional Encoder Representations from Transformers [BERT]  \n\nBidirectional Encoder Representations from Transformers is a technique for NLP pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. Google is leveraging BERT to better understand user searches.\n\nWhen BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks:\n\nGLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks)\nSQuAD (Stanford Question Answering Dataset) v1.1 and v2.0.\nSWAG (Situations With Adversarial Generations)\n\nThe reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.\n\napproiate model for the given problem - bert-base-multilingual-cased\n\n* You should pick a \u201ccased\u201d model or an \u201cuncased\u201d model depending on whether you think letter casing will be helpful for the task you\u2019re trying to solve.","e73a0e58":"# Processing Data Before Feeding It To Transformers\n\n\n ","3802e75b":"# Validation Split ","07961f27":"Observations- \n\n* Balanced class distibution is observed ","0446380f":"# Defining Parameters","953b4b2a":"# Define Build & Compile Model\n\n[Get your model from \ud83e\udd17](https:\/\/huggingface.co\/models)\n\n* Make sure you choose the right one we are using TFAutoModel therefore we choose jplu\/tf-xlm-roberta-large\t \n","b181da94":"# Training Base Model","5cd5c03a":"# EDA And Data Preprocessing ","b353a124":"# Initialise TPU | Distribution strategies | TPU Configuration\nMost times users want to run the model on multiple TPUs in a data parallel way. A distribution strategy is an abstraction that can be used to drive models on CPU, GPUs or TPUs. Simply swap out the distribution strategy and the model will run on the given device","c3fc4cd1":"# Some Great Kernels Demonstrating The Power Of TPU  \ud83d\udd25\n\n- [Incredible TPUs - finetune EffNetB0-B6 at once](https:\/\/www.kaggle.com\/agentauers\/incredible-tpus-finetune-effnetb0-b6-at-once)\n\n- [Super-duper fast pytorch tpu kernel... \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25](https:\/\/www.kaggle.com\/abhishek\/super-duper-fast-pytorch-tpu-kernel)\n\n- [Accelerator Power Hour (PyTorch + TPU)](https:\/\/www.kaggle.com\/abhishek\/accelerator-power-hour-pytorch-tpu)\n\n- [[TPU-Inference] Super Fast XLMRoberta](https:\/\/www.kaggle.com\/shonenkov\/tpu-inference-super-fast-xlmroberta)\n\n- [Triple Stratified KFold with TFRecords](https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords)","953e7080":"# Different Refined Versions Of BERT \n\n[Here's A list of pretrained different model options from \ud83e\udd17](https:\/\/huggingface.co\/transformers\/pretrained_models.html)\n\n# Top  Contenders For The GIven Problem\n\n### 1) XLM-RoBERTa\n\nThe XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook\u2019s RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data.\n\n\n### 2) DistilBERT\n\nThe DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT, and the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of Bert\u2019s performances as measured on the GLUE language understanding benchmark.\n\n\n### 3) XLM \n\nThe XLM model was proposed in Cross-lingual Language Model Pretraining by Guillaume Lample*, Alexis Conneau*. It\u2019s a transformer pre-trained using one of the following objectives:\n\n-a causal language modeling (CLM) objective (next token prediction),\n\n-a masked language modeling (MLM) objective (Bert-like), or\n\n-a Translation Language Modeling (TLM) object (extension of Bert\u2019s MLM to multiple language inputs)","52f57a2d":"# Why use a TPU ?\n\nTensor Processing Unit is highly-optimised for large batches and CNNs and has the highest training throughput , it can decrease your training time\n![Image](https:\/\/wideops.com\/wp-content\/uploads\/2019\/07\/Google_Cloud_TPU_v3_Pod_speed-ups_sCUcjjC.max-1000x1000.png)\n","9a77f599":"# Lets Get Started - Loading Libraries And Data \n","05aa2d80":"# What Is A TPU ?\n\nA tensor processing unit is an AI accelerator application-specific integrated circuit developed by Google specifically for neural network machine learning, particularly using Google's own TensorFlow software.\n\nThe tensor processing unit was announced in May 2016 at Google I\/O, when the company said that the TPU had already been used inside their data centers for over a year. The chip has been specifically designed for Google's TensorFlow framework, a symbolic math library which is used for machine learning applications such as neural networks. However, as of 2017 Google still used CPUs and GPUs for other types of machine learning. Other AI accelerator designs are appearing from other vendors also and are aimed at embedded and robotics markets.\n\n\n\n","3241f7df":"# Tokenizing       [Using AutoTokenizer from Transformers]\n\nTokenization is breaking a text chunk in smaller parts. Whether it is breaking Paragraph in sentences, sentence into words or word in characters.\n\nAutoTokenizer is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the AutoTokenizer.from_pretrained(pretrained_model_name_or_path) class method.\n\nThe from_pretrained() method takes care of returning the correct tokenizer class instance based on the model_type property of the config object, or when it\u2019s missing, falling back to using pattern matching on the pretrained_model_name_or_path string:\n"}}