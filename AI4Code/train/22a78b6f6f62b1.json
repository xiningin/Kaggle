{"cell_type":{"3ca8a690":"code","cdf7de52":"code","5395492b":"code","b7b09add":"code","e6843abb":"code","75d8cb49":"code","028d5566":"code","a102fd11":"code","26702668":"code","df2f6cb9":"code","8638c8f0":"code","3ef2cd22":"code","34531661":"code","f7bb7bb0":"code","dd71696c":"code","f8a59e67":"code","42ab9215":"code","2a591b3e":"code","ff82361f":"code","15720f32":"code","e07685af":"code","2cf18fdd":"code","3f4a2543":"code","0102620f":"code","601d8a7d":"code","723fbb2b":"code","69972f86":"code","c9beaab2":"code","15f4811d":"code","aa754577":"code","17a33d94":"code","44fe3c4f":"code","a554dd79":"code","93fdbf26":"code","1a424e1f":"code","0c12f159":"code","37c745f6":"code","7b315108":"code","20a91e8e":"code","3b425226":"markdown","61dc3ea0":"markdown","11f5c022":"markdown","3b4717dd":"markdown","951b48e9":"markdown","abcd050c":"markdown","455bb9ad":"markdown","a3c51565":"markdown","0f252da1":"markdown","d0904780":"markdown","0b994f66":"markdown","919371d8":"markdown","69e5f360":"markdown","297cce3a":"markdown","d4bc103c":"markdown","22a9c8e5":"markdown","80d623fc":"markdown","f56a9b29":"markdown","0b8a4402":"markdown","a05d56a2":"markdown","f56d3c9a":"markdown","52c7b288":"markdown","f1943190":"markdown"},"source":{"3ca8a690":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cdf7de52":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","5395492b":"data_application_train = pd.read_csv(\"..\/input\/application_train.csv\")\npd.options.display.max_columns = len(data_application_train.columns)\ndata_application_train.head()","b7b09add":"import missingno as msno\nmsno.bar(data_application_train)","e6843abb":"total = len(data_application_train)\nsum_income = data_application_train[\"AMT_INCOME_TOTAL\"].isnull().sum()\nsum_occupation = data_application_train[\"OCCUPATION_TYPE\"].isnull().sum()\n\nprint(\"AMT_INCOME_TOTAL  num of missing data={}\u3000 missing data rate={:.2f}[%]\".format(sum_income, 100*sum_income\/total))\nprint(\"OCCUPATION_TYPE   num of missing data={}\u3000 missing data rate={:.2f}[%]\".format(sum_occupation, 100*sum_occupation\/total))","75d8cb49":"sns.countplot(x=\"TARGET\", data=data_application_train)\nplt.title(\"Data volume in each class('Target')\")\nplt.show()\n\ncount_1 = (data_application_train[\"TARGET\"] == 1).sum()\ncount_0 = (data_application_train[\"TARGET\"] == 0).sum()\nprint(\"Non-repayment rate={:.2f}[%]\".format((count_1\/(count_0+count_1))*100))","028d5566":"#Extract require data\ndata_to_use = data_application_train[[\"TARGET\",\"AMT_INCOME_TOTAL\",\"OCCUPATION_TYPE\"]]","a102fd11":"sns.violinplot(y='AMT_INCOME_TOTAL', data=data_to_use)\nplt.ylabel(\"Income\")\nplt.title(\"Income distribution\")\nplt.show()\ndata_to_use.describe()","26702668":"def binning_data(data_source, col_name, binned_col_name, num_of_bin=10):\n    \"\"\"\n    Binning the specified column data\n    & add the new column that has bin label to original data\n    \n    parameter\n    --------------\n    data_source : Pandas dataframe\n    col_name : string \n    binned_col_name : string \n    num_of_bin : int\n    \n    return\n    --------------\n    data_source : pandas dataframe\n    \n    \"\"\"\n    #Check quartile value\n    data_info = data_source.describe()\n    bin_min = data_info.loc[\"25%\", col_name]\n    bin_max = data_info.loc[\"75%\", col_name]\n    IQR = bin_max - bin_min\n    #Use interquartile value to decide bin_min & max value\n    bin_min = bin_min-IQR*1.5 if bin_min > IQR*1.5 else 0\n    bin_max += IQR*1.5\n    \n    bin_width = (int)((bin_max - bin_min) \/ num_of_bin)\n    #bins = [value for value in gen_range(bin_min, bin_max, bin_width )]\n    bins = [value for value in range((int)(bin_min), (int)(bin_max), bin_width )]\n    labels = [i for i in range(0, len(bins)-1)]\n    binned_label = pd.cut(data_source[col_name], bins=bins, labels=labels)\n    data_source[binned_col_name] = binned_label\n    \n    for i in range(0, len(bins)-1):\n        print(\"range label={} : range={:.5f}~{:.5f}\".format(i, bins[i], bins[i+1]) )\n    \n    return data_source","df2f6cb9":"#Binning data\ndata_to_use = binning_data(data_to_use, \"AMT_INCOME_TOTAL\", \"BINNED_AMT_INCOME_TOTAL\", 8)","8638c8f0":"#Check distribution\nincome_destribution_w_income_range = data_to_use.groupby(\"BINNED_AMT_INCOME_TOTAL\", as_index=False).count()\nplt.bar(income_destribution_w_income_range[\"BINNED_AMT_INCOME_TOTAL\"]\n        , income_destribution_w_income_range[\"TARGET\"])\nplt.xlabel('Low<- Income range label ->High)')\nplt.ylabel(\"count\")\nplt.title(\"Income distribution\")\nfor x, y in zip(income_destribution_w_income_range[\"BINNED_AMT_INCOME_TOTAL\"]\n                , income_destribution_w_income_range[\"TARGET\"]):\n    plt.text(x, y, y, ha='center', va='bottom')\nplt.show()","3ef2cd22":"#Cal non-repayment rate\nnon_repayment_rate_w_income_range = data_to_use.groupby(\"BINNED_AMT_INCOME_TOTAL\", as_index=False).mean()\nnon_repayment_rate_w_income_range[\"TARGET\"] *= 100 #% notation\n\n#Bar plot\nplt.figure(figsize=(10,6))\nplt.bar(non_repayment_rate_w_income_range[\"BINNED_AMT_INCOME_TOTAL\"]\n        , non_repayment_rate_w_income_range[\"TARGET\"], color=\"Blue\")\nplt.xlabel('Low<- Income range label ->High')\nplt.ylabel('non-payment rate[%]')\nplt.ylim(0,10)\nplt.title(\"non-payment rate\")\n\nfor x, y in zip(non_repayment_rate_w_income_range[\"BINNED_AMT_INCOME_TOTAL\"]\n                , non_repayment_rate_w_income_range[\"TARGET\"]):\n    plt.text( x, y, str(\"{:.2f}\").format(y), ha='center', va='bottom')\nplt.show()","34531661":"#Cal non-repayment rate for each occupation type\nnon_repayment_rate_w_occupation = data_to_use.groupby(\"OCCUPATION_TYPE\", as_index=False).mean()\nnon_repayment_rate_w_occupation[\"TARGET\"] *= 100\n#non_repayment_rate_w_occupation[\"OCCUPATION_TYPE_LABEL\"] = [i for i in range(0, len(non_repayment_rate_w_occupation))]\nprint(non_repayment_rate_w_occupation)\n\n#Bar plot\nplt.figure(figsize=(30,12))\nplt.bar(non_repayment_rate_w_occupation.index, non_repayment_rate_w_occupation[\"TARGET\"], color=\"Blue\")\nplt.xlabel(\"OCCUPATION_TYPE\")\nplt.ylabel(\"Non-payment rate[%]\")\nplt.title(\"Non-payment rate\")\n\nfor x, y in zip(non_repayment_rate_w_occupation.index, non_repayment_rate_w_occupation[\"TARGET\"]):\n    plt.text(x, y, str(\"{:.2f}\").format(y), ha='center', va='bottom')\n\nplt.xticks(non_repayment_rate_w_occupation.index, non_repayment_rate_w_occupation[\"OCCUPATION_TYPE\"])\nplt.show()","f7bb7bb0":"#1.Drop high missing value data\n#Set drop criteria as 40%. If missing value rate is over 40%, then drop those features.\nREDUCTION_CRITERIA_FOR_MISSING_DATA = 40 #[%] \ntotal = len(data_application_train)\ncol_name = data_application_train.columns\nmissing_data_rate = [100*data_application_train[col_name[i]].isnull().sum() \/ total for i in range(0, len(col_name))]\n\nfor i in range(0, len(col_name)):\n    if REDUCTION_CRITERIA_FOR_MISSING_DATA < missing_data_rate[i]:\n        del data_application_train[col_name[i]]\n\nmsno.bar(data_application_train)\nprint(\"Num of Featurs: Before reduction {} => After Reduction {}\".format(len(col_name), len(data_application_train.columns.values)))","dd71696c":"object_data_application_train = data_application_train.select_dtypes(['object'])\nobject_data_application_train.head()","f8a59e67":"#Replace object type data to number\nobject_col_name = object_data_application_train.columns.values\narray_object_to_int = np.array([object_data_application_train[object_col_name[i]].unique() for i in range(0,len(object_col_name))])\n\nfor i in range(0,len(object_col_name)):\n    labels, uniques = pd.factorize(data_application_train[object_col_name[i]])\n    data_application_train[object_col_name[i]] = labels   \n\n#Repalce \"NAN\" to -1    \ndata_application_train = data_application_train.replace(np.nan, -1)","42ab9215":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()","2a591b3e":"#Drop SK_ID_CURR\ndel data_application_train[\"SK_ID_CURR\"]\n#Drop OCCUPATION_TYPE (Already done in Step2)\ndel data_application_train[\"OCCUPATION_TYPE\"]","ff82361f":"target = data_application_train['TARGET']\nfeature = data_application_train.iloc[:,1:len(data_application_train.columns)]\nmodel.fit(feature, target)","15720f32":"rank = np.argsort(-model.feature_importances_)\nf, ax = plt.subplots(figsize=(11, 11)) \nsns.barplot(x=model.feature_importances_[rank], y=data_application_train.columns.values[rank], orient='h')\nax.set_xlabel(\"Importance\")\nplt.tight_layout()\nplt.show()","e07685af":"#Pick up top10 important feature\nfor i in range(0, 10):\n    print(\"Rank{} => {} (Importance={:.3f})\".format(i+1, \n                                                    data_application_train.columns.values[rank[i]], \n                                                    model.feature_importances_[rank[i]]))","2cf18fdd":"train_data =  pd.read_csv(\"..\/input\/application_train.csv\")","3f4a2543":"#Replace object type data to number\nobject_train_data = train_data.select_dtypes(['object'])\nobject_col_name = object_train_data.columns.values\narray_object_to_int = np.array([object_train_data[object_col_name[i]].unique() for i in range(0,len(object_col_name))])\n\nfor i in range(0,len(object_col_name)):\n    labels, uniques = pd.factorize(train_data[object_col_name[i]])\n    train_data[object_col_name[i]] = labels   \n\n#Repalce \"NAN\" to -1    \ntrain_data = train_data.replace(np.nan, -1)","0102620f":"#Drop SK_ID_CURR\ndel train_data[\"SK_ID_CURR\"]\n#Drop OCCUPATION_TYPE (Already done in Step2)\ndel train_data[\"OCCUPATION_TYPE\"]","601d8a7d":"target = train_data['TARGET']\nfeature = train_data.iloc[:,1:len(train_data.columns)]\nmodel.fit(feature, target)","723fbb2b":"rank = np.argsort(-model.feature_importances_)\nf, ax = plt.subplots(figsize=(15, 15)) \nsns.barplot(x=model.feature_importances_[rank], y=train_data.columns.values[rank], orient='h')\nax.set_xlabel(\"Importance\")\nplt.tight_layout()\nplt.show()","69972f86":"for i in range(0, 10):\n    print(\"Rank{} => {} (Importance={:.3f})\".format(i+1, train_data.columns.values[rank[i]], model.feature_importances_[rank[i]]))","c9beaab2":"data =  pd.read_csv(\"..\/input\/application_train.csv\")","15f4811d":"def plot_non_repayment_rate(data_source, col_name, flag_rename_x_label=False):\n    \"\"\"\n    Plot non-repayment rate. (Xaxis=Col_name, Yaxis=non-repayment rate)\n    \n    parameter\n    --------------\n    data_source : Pandas dataframe\n    col_name : string \n    flag_rename_x_label : bool\n    \n    return\n    --------------\n    None\n    \"\"\"\n    \n    #Cal non-repayment rate\n    non_repayment_rate = data_source.groupby(col_name, as_index=False).mean()\n    non_repayment_rate[\"TARGET\"] *= 100\n    #print(non_repayment_rate)\n    \n    #Bar plot\n    plt.figure(figsize=(25,10))\n    plt.bar(non_repayment_rate.index, non_repayment_rate[\"TARGET\"], color=\"Blue\")\n    plt.xlabel(col_name, fontsize=18)\n    plt.ylabel(\"Non-payment rate[%]\", fontsize=18)\n    plt.title(\"Non-payment rate\")\n    plt.tight_layout()\n    \n    for x, y in zip(non_repayment_rate.index, non_repayment_rate[\"TARGET\"]):\n        plt.text(x, y, str(\"{:.2f}\").format(y), ha='center', va='bottom')\n        if flag_rename_x_label == True:\n            plt.xticks(non_repayment_rate.index, non_repayment_rate[col_name])\n    plt.show()","aa754577":"def gen_range(value_start, value_end, value_step):\n    \"\"\"\n    Extend range() so it can handle float type data\n    ----------------\n    Parameter\n        value_start:float\n        value_end:float\n        value_step:float\n    ----------------\n    ----------------\n    Return\n    \u3000\u3000value:float\n    ----------------\n    \"\"\"\n    value = value_start\n    while value+value_step < value_end:\n     yield value\n     value += value_step","17a33d94":"def binning_data2(data_source, col_name, binned_col_name, num_of_bin=10):\n    \"\"\"\n    parameter\n    --------------\n    data_source : Pandas dataframe\n    col_name : string \n    binned_col_name : string \n    num_of_bin : int\n    \n    return\n    --------------\n    data_source : pandas dataframe\n    \n    \"\"\"\n    #Use interquartile value to decide bin_min & max value\n    data_info = data_source.describe()\n    bin_min = data_info.loc[\"25%\", col_name]\n    bin_max = data_info.loc[\"75%\", col_name]\n    IQR = bin_max - bin_min\n    bin_min = bin_min-IQR*1.5 if bin_min > IQR*1.5 else 0\n    bin_max += IQR*1.5\n    \n    bin_width = (bin_max - bin_min) \/ num_of_bin\n    bins = [value for value in gen_range(bin_min, bin_max, bin_width )]\n    labels = [i for i in range(0, len(bins)-1)]\n    binned_label = pd.cut(data_source[col_name], bins=bins, labels=labels)\n    data_source[binned_col_name] = binned_label\n\n    for i in range(0, len(bins)-1):\n        print(\"range label={} : range={:.5f}~{:.5f}\".format(i, bins[i], bins[i+1]) )\n    \n    return data_source","44fe3c4f":"#ORGANIZATION_TYPE\nplot_non_repayment_rate(data, 'ORGANIZATION_TYPE', False)\ntmp = data[\"ORGANIZATION_TYPE\"].unique()\nfor i in range(0, len(tmp)):\n    print(\"{}:{}\".format(i, tmp[i]))","a554dd79":"#EXT_SOURCE_1\nplot_non_repayment_rate(binning_data2(data, 'EXT_SOURCE_1', 'BIN_EXT_SOURCE_1', 8), 'BIN_EXT_SOURCE_1')","93fdbf26":"#EXT_SOURCE_2\nplot_non_repayment_rate(binning_data2(data, 'EXT_SOURCE_2', 'BIN_EXT_SOURCE_2', 8), 'BIN_EXT_SOURCE_2')","1a424e1f":"#REGION_POPULATION_RELATIVE\nplot_non_repayment_rate(binning_data2(data, 'REGION_POPULATION_RELATIVE', 'BIN_REGION_POPULATION_RELATIVE', 8), 'BIN_REGION_POPULATION_RELATIVE')","0c12f159":"#AMT_CREDIT\nplot_non_repayment_rate(binning_data(data, 'AMT_CREDIT', 'BIN_AMT_CREDIT', 8), 'BIN_AMT_CREDIT')","37c745f6":"#DEF_60_CNT_SOCIAL_CIRCLE\nplot_non_repayment_rate(data, 'DEF_60_CNT_SOCIAL_CIRCLE', True)","7b315108":"#CNT_CHILDREN\nplot_non_repayment_rate(data, 'CNT_CHILDREN', True)","20a91e8e":"#AMT_ANNUITY\nplot_non_repayment_rate(binning_data2(data, 'AMT_ANNUITY', 'BIN_AMT_ANNUITY', 8), 'BIN_AMT_ANNUITY')","3b425226":"Comparing Random forest analysis result w\/ dropping missing values case and w\/o dropping missing value case\n\n| Ranking | w\/o missing value | w\/ missing value |\n|:-----------:|:-----------:|:------------:|\n| 1 | **ORGANIZATION_TYPE (Importance=0.077)**  | **EXT_SOURCE_1 (Importance=0.058)**  |\n| 2 | EXT_SOURCE_2 (Importance=0.058)  | EXT_SOURCE_2 (Importance=0.044) |\n| 3 | REGION_POPULATION_RELATIVE (Importance=0.058)  | DAYS_EMPLOYED (Importance=0.039)  |\n| 4 | DAYS_REGISTRATION (Importance=0.056)  | DAYS_REGISTRATION (Importance=0.039) |\n| 5 | DAYS_EMPLOYED (Importance=0.055) | REGION_POPULATION_RELATIVE (Importance=0.039) |\n| 6 | AMT_CREDIT (Importance=0.051)  | DAYS_BIRTH (Importance=0.035) |\n| 7 | DEF_60_CNT_SOCIAL_CIRCLE (Importance=0.050) | AMT_CREDIT (Importance=0.034) |\n| 8 | DAYS_BIRTH (Importance=0.049) | DEF_60_CNT_SOCIAL_CIRCLE (Importance=0.033) |\n| 9 | AMT_INCOME_TOTAL (Importance=0.046) | AMT_INCOME_TOTAL (Importance=0.032) |\n| 10 | **CNT_CHILDREN (Importance=0.042)** | **AMT_ANNUITY (Importance=0.029)** |","61dc3ea0":"####  Conclusion of  \"OCCUPATION_TYPE\" vs  \"Non-repayment rate\" analysis\n1. High non-repayment rate is observed in Labors. Espacilly low-skill labors marks highest rate 17.15%.   \n2. Non-repayment rate tends to high in the job that doesn't require high-skill, high-education like waiter, drivers, security staff etc...  Education history may also be a important feature. \n3. Accountant has Lowest non-repayment rate 4.53%. I suppose they are good at finacial planning & control because they are accountant.   \n4. As a result, we see siginificant non-repayment rate difference among occupation type so I conclude that \u201dOCCUPATION_TYPE\u201d can be a important feature.","11f5c022":">### Define non-repayment rate from target data\n\"Home_credit_columns_description.csv\" says target data is defined as    \n*\"1 - client with payment difficulties: he\/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases\"*  \nHere, I intoroduce \"non-repayment rate\" and define it as  $\\frac {sum(1)}  {(sum(1) + sum (0))}$ .   If I calculate \"non-repayment rate\" across the whole data, then result is 8:07[%] ( It also can be calculated by mean of data because target data is 0 & 1 )","3b4717dd":"Now binning data by quartile value","951b48e9":"#  Introduction\nThis EDA is taking these 4 steps.  \nStep1 : Overview the data  \nStep2 : See the relationship between...   \n <span>\u3000<\/span>a) \"AMT_INCOME_TOTAL\" & repayment or non-repayment rate   \n <span>\u3000<\/span>b) \"OCCUPATION_TYPE\" & repayment or non-repayment rate  \nStep3 : Picking up other features that will help to predict custmer's repayment ability  \nStep4 : Plot non-repayment rate for all selected features\n  \nIn Step1, overview the data to check what kind of data in there and if there is any missing values. Then, define non-repayment rate from target data.  \nIn Step2, I forcus on \"AMT_INCOME_TOATL\" & \"OCCUPATION_TYPE\" features because, generaly thinking, these features seem to relate to custmer's repayment ability. It's easy to imagine that low income cause of default so I expect the tendecy which is \"if income gets lower then non-repayment rate becomes higher and if income gets higher then non-repayment rate becomes lower\". I will test this \"theory\" in step2. Then I will check \"OCCUPATION_TYPE\"  because I believe it relates to income level and also occupation infomation can be reflected personal feature more than income.   \nSo, I will check if \"AMT_INCOME_TOATL\" & \"OCCUPATION_TYPE\"  can help to predict custmer's repayment ability or not in Step2.   \nIn Step3,  I will pick up other important features with using random forest analysis.  \nIn Step4, Plot non-repayment rate for all selected fesatures  ","abcd050c":"###  See the relationship between \"OCCUPATION_TYPE\" and  Non-repayment rate\n**Method**\n1. Calculate non-repayment rate in each \"OCCUPATION_TYPE\"\n2. Plot \"OCCUPATION_TYPE vs \"Non-repayment\"","455bb9ad":"1. Income distribution after benning","a3c51565":"###  See the relationship between \"AMT_INCOME_TOTAL\" and  Non-repayment rate\nTest the theory \"low income = high non-repayment rate\" & \"high income = low non-repayment rate\".  \n**Method**\n1. Benning income data\n2. Calculate Non-repayment rate in each the bins\n3. Plot \"Binned income\" vs \"Non-repayment\"","0f252da1":"1. Drop the feature that have much missing data  \nIf missing data rate is over 40%, then drop those features from this analysis.   (I will run same analysis without dropping missing data later and compare the result. )","d0904780":"Run Random forest analysis","0b994f66":"Finally, calculate non-repayment rate in each bins & plot them","919371d8":"To decide bin width, checking \"AMT_INCOME_TOTAL\" distribution below.   \nFrom the distribution, we understand we can't use max & min value to decide bin width becasue there are outliers in the data. So I'm going to use quartile value to decide bin width.","69e5f360":"Encoding object tyep data  \nAll object type data are categorical variable (nominal scale) then replace those value, include \"nan\" , to any number to run Random forest analysis in next.","297cce3a":"# Plot non-repayment rate for all selected features  \nAs a result, some plots show strong relationship between selected feature and non-repayment rate. For example \"Organization type\". We can see significant non-repayment differece among parameters as same as \"occupation type\". So it seems customer's surronding environment is a key to pridect the repayment ability.   \nAnd also \"EXT_SOURCE_1 & 2\". I need to study about these features more. However these show the strong trend which lower value of \"EXT_SOURCE_1 & 2\" shows higher non-repayment rate and higher value of \"EXT_SOURCE_1 & 2\" shows lower non-repayment rate. ","d4bc103c":"# Step3.  Picking up other features that will help to predict custmer's repayment ability    \n\n**method**\n1. Drop the feature that have much missing data\n2. Encoding object tyep data\n3. Calculate importance with Random forest analysis","22a9c8e5":"####  Conclusion of  \"AMT_INCOME_TOTAL\" vs  \"Non-repayment rate\" analysis\n1. What I expected is \"low income => high non-repayment rate\" . However no significant differernce is observed among Range0 to 4 (Across low income to middle).  \n2. Highest non-repayment rate is observed in Range2, which is considered average income range.\n3. \"high income => low non-repayment rate\" seems right. Range6 & 7 show lower non-repayment rate than other\n4. Non-repayment rate difference between higest & lowest is 2.4%. It is not the big difference as I expected.    \n5. From this analysis,  I can say...  \n\"Low income isn't the convincing reason of loan rejection\" or \"Home credit may have careful loan assessment for low income customers (this is why we don't see significant non-repayment rate differernce in the result)\"  \n6. However, becasue no significant non-repayment rate difference is observed among the income ranges,  I can't conclude that \"AMT_INCOME_TOTAL\" is important feature. I will test  \"AMT_INCOME_TOTAL\" importance with Random forest analysis in Step3","80d623fc":"# Step1.  Overview the data\noverview the data to check what kind of data in there and  if there is any missing values. Then define non-repayment rate from target data.","f56a9b29":"Run Random forest analysis without dropping missing values","0b8a4402":"###  Conclusion of  Step2\n1. \"AMT_INCOME_TOTAL\" will be tested with random forest analysis in Step3 to find out if it's important feature or not  \n2. \u201dOCCUPATION_TYPE\u201d can be a important feature  \n3. Generally thinking, income relate to occupation type. However non-repayment rate difference is observed well among occupation type rather than among income range.  Thus the features that describe customer's social role, resposibility and surrounding environment seems to be more important feature than income. ","a05d56a2":"# Step2.  See the relationship between...   \n **<span>\u3000<\/span>a) \"AMT_INCOME_TOTAL\" & Non-repayment rate**  \n **<span>\u3000<\/span>b) \"OCCUPATION_TYPE\" & Non-repayment rate**","f56d3c9a":"Importance is changed however 8 common features are in the both top10 rank.  \"ORGANIZATION_TYPE\" and \"EXT_SOURCE_1\" are switched between w\/ dropping and w\/o dropping.  Also \"CNT_CHILDREN\" and \"AMT_AN NUITY\" are switched.  I dropped \"EXT_SOURCE_1\" and  \"AMT_AN NUITY\" in first Random forest analysis becasue they have over 40% missing value. However, from 2nd Random forest analysis, it turns out that actually they are important features even they have missing values.   \n\n### Conclusion of Step3\nAs a result, 12 festures below are picked up.\n\n| No| Selected feature | \n|:-----------:|:-----------:|\n| 1 | ORGANIZATION_TYPE  |\n| 2 | EXT_SOURCE_1  |\n| 3 | EXT_SOURCE_2 |\n| 4 | REGION_POPULATION_RELATIVE |\n| 5 | DAYS_REGISTRATION  |\n| 6 | DAYS_EMPLOYED |\n| 7 | AMT_CREDIT |\n| 8 | DEF_60_CNT_SOCIAL_CIRCLE  |\n| 9 | DAYS_BIRTH  |\n| 10 | AMT_INCOME_TOTAL |\n| 11 | CNT_CHILDREN |\n| 12 |AMT_ANNUITY |","52c7b288":"> ### Check missing data\nPlot missing data rate with \"missingno\" module and calculate missing rate for \"AMT_INCOME_TOATL\" & \"OCCUPATION_TYPE\"","f1943190":">### Load the data & take a look what kind of data in there"}}