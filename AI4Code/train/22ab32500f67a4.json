{"cell_type":{"aa1dacf6":"code","3c014dfa":"code","109c72ac":"code","73dcc1b3":"code","b1868e06":"code","2affaf3c":"code","2c5d2aad":"code","0ad57eff":"code","714e6d24":"code","e20ac9ee":"code","53256130":"code","50c985f0":"code","b5312b2b":"code","dd75162c":"code","1fa594a3":"code","5097b635":"code","06792822":"code","ea514231":"code","5b52feff":"code","b05b312e":"code","4c909ee3":"code","feb0adc0":"code","ebd32ca1":"code","4efe2af0":"code","3a399ef3":"code","01743c14":"code","eea6ae23":"code","6f827906":"code","444dae1c":"code","87ded30f":"code","4ab4afec":"code","3a86c970":"code","6fad2126":"code","068f2f63":"code","404a5a9c":"code","3ed1aab4":"code","70864ae6":"code","a6cd1dd8":"code","02200fac":"code","07e0ca8e":"code","8f90920d":"code","59f7b5e5":"code","1ceb8d2d":"code","d8c57d8f":"code","d225d92c":"code","8e4acc1b":"code","b56167ee":"code","20343557":"markdown","62c24fdd":"markdown","9b5ebd45":"markdown","4b7f0e52":"markdown","bf2f50ce":"markdown","d504d0bb":"markdown","8ad110d5":"markdown","67206b9e":"markdown","8eb17e45":"markdown","c0664b44":"markdown","205ca2ce":"markdown","c89e53d0":"markdown","b1fb2294":"markdown","a782de87":"markdown","621d18e9":"markdown","84a40fd5":"markdown","2d087edb":"markdown","e060db5a":"markdown","1b24f47c":"markdown","3b1c7a2d":"markdown","0aab9aa3":"markdown","91f66b7e":"markdown","0e48beb8":"markdown","a26cec02":"markdown","30dce515":"markdown","9907c71f":"markdown","43596b30":"markdown","48e296e8":"markdown"},"source":{"aa1dacf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \nimport re\n\nimport nltk\n\nnltk.download('popular')\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c014dfa":"df = pd.read_csv('..\/input\/quotes-from-goodread\/success_quotes.csv')\ndf.head()","109c72ac":"#Fifth row, 2nd column \n\ndf.iloc[4,1]","73dcc1b3":"df.isnull().sum()","b1868e06":"! pip install -q polyglot\n! pip install -q pycld2\n! pip install -q pyicu","2affaf3c":"from polyglot.detect import Detector\nimport icu\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2c5d2aad":"einstein = df[(df['Author']=='\u2015 Albert Einstein')].reset_index(drop=True)\neinstein.head(10)","0ad57eff":"df.iloc[18,1]","714e6d24":"df.iloc[289,1]","e20ac9ee":"#Code by Farid Rizqi S https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis\n\ndf['Author'].value_counts().head(10).plot(kind='bar', color='r');","53256130":"#Code by Farid Rizqi S https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis\n\ndf['Quote'] = df['Quote'].apply(lambda x: re.sub(\"[\\\u201c\\\u201d]\", \"\", x))\ndf['Other Tags'] = df['Other Tags'].apply(lambda x: re.sub(\"[\\'\\[\\]]\", \"\", x))","50c985f0":"#Code by Farid Rizqi S https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis\n\n# Count Quote Word Freq\nquote_count = pd.Series(' '.join(df['Quote']).split()).value_counts()\nquote_count.head(10).plot(kind='bar', color='g');","b5312b2b":"#Code by Farid Rizqi S https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis\n\n# Count Other Tag Word Freq\nothertag_count = pd.Series(' '.join(df['Other Tags']).split(',')).value_counts()\nothertag_count.head(10).plot(kind='bar', color='y');","dd75162c":"#Code by Farid Rizqi S https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis\n\n# Count Number of Words\ndf['word_count'] = df['Quote'].apply(lambda x: len(str(x).split(\" \")))\ndf.sort_values('word_count', ascending=False).head()","1fa594a3":"#Code by Farid Rizqi S https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis\n\n# Count Number Of Character\ndf['char_count'] = df['Quote'].str.len() ## this also includes spaces\ndf.sort_values('char_count', ascending=False).head(5)","5097b635":"#Saving for next time since \"Success\" is the only main tag\n#Code by Farid Rizqi S https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis\n\n# Count of Duplicated quote\ndf[df.duplicated('Quote')==True].groupby('Main Tag')['Quote'].count()","06792822":"#Code by Farid Rizqi S https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis\n\n# Detect Text Language\nlangs = []\nfor text in df['Quote']:\n    try:\n        lang = Detector(text).language.code\n        langs.append(lang)\n    except:\n        lang = 'NaN'\n        langs.append(lang)\ndf['lang'] = langs","ea514231":"df['lang'].value_counts().head(10)","5b52feff":"# Trying Hindi Quote\ndf_hi = df[df['lang']=='hi']\nprint(df_hi.shape)\ndf_hi['Other Tags'].value_counts()","b05b312e":"df_hi['CleanQuote'] = df_hi['Quote'].apply(lambda x: x.lower())\ndf_hi['CleanQuote'].sample(2)","4c909ee3":"stop_nltk = stopwords.words(\"hindi\")\n\ndef drop_stop(input_tokens):\n    rempunc = re.sub(r'[^\\w\\s]','',input_tokens)\n    remstopword = \" \".join([word for word in str(rempunc).split() if word not in stop_nltk])\n    return remstopword\n\ndf_hi['CleanQuote'] = df_hi['CleanQuote'].apply(lambda x: drop_stop(x))\ndf_hi['CleanQuote'].sample(2)","feb0adc0":"# Trying French Quote\ndf_fr = df[df['lang']=='fr']\nprint(df_fr.shape)\ndf_fr['Other Tags'].value_counts()","ebd32ca1":"df_fr['CleanQuote'] = df_fr['Quote'].apply(lambda x: x.lower())\ndf_fr['CleanQuote'].sample(2)","4efe2af0":"stop_nltk = stopwords.words(\"french\")\n\ndef drop_stop(input_tokens):\n    rempunc = re.sub(r'[^\\w\\s]','',input_tokens)\n    remstopword = \" \".join([word for word in str(rempunc).split() if word not in stop_nltk])\n    return remstopword\n\ndf_fr['CleanQuote'] = df_fr['CleanQuote'].apply(lambda x: drop_stop(x))\ndf_fr['CleanQuote'].sample(2)","3a399ef3":"stop_nltk = stopwords.words(\"french\")\n\ndef drop_stop(input_tokens):\n    rempunc = re.sub(r'[^\\w\\s]','',input_tokens)\n    remstopword = \" \".join([word for word in str(rempunc).split() if word not in stop_nltk])\n    return remstopword\n\ndf_fr['CleanQuote'] = df_fr['CleanQuote'].apply(lambda x: drop_stop(x))\ndf_fr['CleanQuote'].sample(2)","01743c14":"df_fr.loc[2098, \"lang\"] ","eea6ae23":"lemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndf_fr[\"CleanQuote\"] = df_fr[\"CleanQuote\"].apply(lambda x: lemmatize_words(x))\ndf_fr['CleanQuote'].sample(2)","6f827906":"#Code by Parul Pandey  https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python\n\n\nfrom sklearn.impute import SimpleImputer\ndf_fr_most_frequent = df_fr.copy()\n#setting strategy to 'mean' to impute by the mean\nmean_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median \ndf_fr_most_frequent.iloc[:,:] = mean_imputer.fit_transform(df_fr_most_frequent)","444dae1c":"df_fr_most_frequent.isnull().sum()","87ded30f":"#Saving for next code since no  column worked here\n\nX = df_fr_most_frequent['CleanQuote']\nY = df_fr_most_frequent['Main Tag']","4ab4afec":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=48, stratify=Y)","3a86c970":"print(f\"X_train : {X_train.shape}\\nX_test : {X_test.shape}\")","6fad2126":"count_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\n\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)","068f2f63":"NB = MultinomialNB().fit(X_train_tfidf, Y_train)","404a5a9c":"pipeline_nb = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('NB', MultinomialNB()),\n                    ])\n\nmodel_nb = pipeline_nb.fit(X_train, Y_train)","3ed1aab4":"predict_nb = model_nb.predict(X_test)","70864ae6":"print(classification_report(predict_nb, Y_test))","a6cd1dd8":"pipeline_sgd = Pipeline([('vect', CountVectorizer()),\n                         ('tfidf', TfidfTransformer()),\n                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n                          alpha=1e-3, random_state=42)),\n                        ])\n\nmodel_sgd = pipeline_sgd.fit(X_train, Y_train)","02200fac":"predict_sgd = model_sgd.predict(X_test)","07e0ca8e":"print(classification_report(predict_sgd, Y_test))","8f90920d":"pipeline_rf = Pipeline([('vect', CountVectorizer()),\n                         ('tfidf', TfidfTransformer()),\n                         ('rf', RandomForestClassifier()),\n                        ])\n\nmodel_rf = pipeline_rf.fit(X_train, Y_train)","59f7b5e5":"predict_rf = model_rf.predict(X_test)","1ceb8d2d":"print(classification_report(predict_rf, Y_test))","d8c57d8f":"pipeline_xgb = Pipeline([('vect', CountVectorizer()),\n                         ('tfidf', TfidfTransformer()),\n                         ('xgb', XGBClassifier()),\n                        ])\n\nmodel_xgb = pipeline_xgb.fit(X_train, Y_train)","d225d92c":"predict_xgb = model_xgb.predict(X_test)","8e4acc1b":"print(classification_report(predict_xgb, Y_test))","b56167ee":"df.iloc[0,1]","20343557":"#Adding char_count column","62c24fdd":"#Since I was NOT able to go further with HINDI, let's try another language","9b5ebd45":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQnVE7JWWDGjTyn8z1gW28oED1z4AIuAy3Drw&usqp=CAU)facebook.com","4b7f0e52":"#That's why I'll keep failing and learning. Kaggling ever after. MPWolke","bf2f50ce":"#Trying to fix that message: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nThough it was not what I expect to get.  Then I removed it again. By the way, there is a post in \nhttps:\/\/paulovasconcellos.com.br\/o-que-%C3%A9-a-value-is-trying-to-be-set-on-a-copy-of-a-slice-from-a-dataframe-e85f744d8be1?gi=187790bcd373\n\nI can apply .loc or .iloc  \n\n#The best is when Paulo Vasconcelos explained: \"Como conserto essa merda?\" My translation from Portuguese: How do I fix that piece of shit?  Maybe some day I'll make that Notebook: \"Como consertar esta MERDA?\"","d504d0bb":"#NLTK Stopwords supported languages\n\nLanguages supported by NLTK depends on the task being implemented. For stemming, we have RSLPStemmer (Portuguese), ISRIStemmer (Arabic), and SnowballStemmer (Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish)\n\nhttps:\/\/devopedia.org\/natural-language-toolkit#:~:text=Languages%20supported%20by%20NLTK%20depends,Russian%2C%20Spanish%2C%20Swedish).","8ad110d5":"#ValueError: The number of classes has to be greater than one; got 1 class","67206b9e":"#OSError: No such file or directory: '\/usr\/share\/nltk_data\/corpora\/stopwords\/hindi'\n\n#I don't know if I made something wrong OR if there insn't NLKT supported for Hindi Language till now.","8eb17e45":"#Modelling\n\nCreate Naive Baiyes Pipeline","c0664b44":"#Keeping your mouth shut: that is awesome from Herr Einstein.","205ca2ce":"#Create XGBoost Classifier Pipeline","c89e53d0":"#Acknowledgement:\n\nFarid Rizqi S  https:\/\/www.kaggle.com\/faridrizqis\/goodread-quote-analysis","b1fb2294":"#Remove Punc & Stopword","a782de87":"#Split Data to Train and Test","621d18e9":"#Create SGDClassifier Pipeline","84a40fd5":"#Extracting Feature","2d087edb":"![](https:\/\/pbs.twimg.com\/media\/ERJfVf3UYAAmdqS.jpg)twitter.com","e060db5a":"#Lemmatization in French","1b24f47c":"![](https:\/\/pbs.twimg.com\/media\/DOpt2_RXUAAnnWH.jpg)twitter.com","3b1c7a2d":"#Create Random Forest Classifier Pipeline","0aab9aa3":"#Adding word_count column (number of Words)","91f66b7e":"#Let's try with Hindi Language","0e48beb8":"#Thanks to Farid Rizqi S for ALL the script.","a26cec02":"#Specify Feature and Target","30dce515":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSH0FKahVf8eoyZcbfs1KUcMp3228SjY5g9bg&usqp=CAU)codefires.com","9907c71f":"#Lower Text","43596b30":"#Just checking if there is NO Nulls any more.","48e296e8":"\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\"\n\n\"Natural Language Processing with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more.\" \n\nhttps:\/\/www.nltk.org\/"}}