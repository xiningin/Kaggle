{"cell_type":{"8e224281":"code","c1f609fd":"code","610e6920":"code","28cca91f":"code","9fb533a9":"code","4d05d042":"code","32457b9b":"code","f5e8a2ff":"code","499bd87a":"markdown","b0d6ae3e":"markdown","52273ff9":"markdown","8a8ea13a":"markdown","264d2674":"markdown","000cd6da":"markdown","31469b62":"markdown","92c176ca":"markdown","538e6547":"markdown"},"source":{"8e224281":"import numpy as np\nimport pandas as pd\nimport json\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder","c1f609fd":"N_EPOCH = 25\nBATCH_SIZE = 256\nLEARN_RATE = 0.001\nFEATURE_SIZE = 21","610e6920":"def one_hot(categories, string):\n    encoding = np.zeros((len(string), len(categories)))\n    for idx, char in enumerate(string):\n        encoding[idx, categories.index(char)] = 1\n    return encoding\n\ndef featurize(entity):\n    sequence = one_hot(list('ACGU'), entity['sequence'])\n    structure = one_hot(list('.()'), entity['structure'])\n    loop_type = one_hot(list('BEHIMSX'), entity['predicted_loop_type'])\n    features = np.hstack([sequence, structure, loop_type])\n    return features \n\ndef char_encode(index, features, feature_size):\n    half_size = (feature_size - 1) \/\/ 2\n    \n    if index - half_size < 0:\n        char_features = features[:index+half_size+1]\n        padding = np.zeros((int(half_size - index), char_features.shape[1]))\n        char_features = np.vstack([padding, char_features])\n    elif index + half_size + 1 > len(features):\n        char_features = features[index-half_size:]\n        padding = np.zeros((int(half_size - (len(features) - index))+1, char_features.shape[1]))\n        char_features = np.vstack([char_features, padding])\n    else:\n        char_features = features[index-half_size:index+half_size+1]\n    \n    return char_features","28cca91f":"class VaxDataset(Dataset):\n    def __init__(self, path, test=False):\n        self.path = path\n        self.test = test\n        self.features = []\n        self.targets = []\n        self.ids = []\n        self.load_data()\n    \n    def load_data(self):\n        with open(self.path, 'r') as text:\n            for line in text:\n                records = json.loads(line)\n                features = featurize(records)\n                \n                for char_i in range(records['seq_scored']):\n                    char_features = char_encode(char_i, features, FEATURE_SIZE)\n                    self.features.append(char_features)\n                    self.ids.append('%s_%d' % (records['id'], char_i))\n                        \n                if not self.test:\n                    targets = np.stack([records['reactivity'], records['deg_Mg_pH10'], records['deg_Mg_50C']], axis=1)\n                    self.targets.extend([targets[char_i] for char_i in range(records['seq_scored'])])\n                    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, index):\n        if self.test:\n            return self.features[index], self.ids[index]\n        else:\n            return self.features[index], self.targets[index], self.ids[index]","9fb533a9":"train_dataset = VaxDataset('..\/input\/stanford-covid-vaccine\/train.json')\ntrain_dataloader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)","4d05d042":"class Flatten(nn.Module):\n    def forward(self, x):\n        batch_size = x.shape[0]\n        return x.view(batch_size, -1)\n\nclass VaxModel(nn.Module):\n    def __init__(self):\n        super(VaxModel, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Conv1d(14, 32, 1, 1),\n            nn.PReLU(),\n            nn.BatchNorm1d(32),\n            nn.Dropout(0.2),\n            nn.Conv1d(32, 1, 1, 1),\n            nn.PReLU(),\n            Flatten(),\n            nn.Dropout(0.2),\n            nn.Linear(FEATURE_SIZE, 32),\n            nn.PReLU(),\n            nn.BatchNorm1d(32),\n            nn.Dropout(0.2),\n            nn.Linear(32, 3),\n        )\n    \n    def forward(self, features):\n        return self.layers(features)","32457b9b":"model = VaxModel().cuda()\noptimizer = torch.optim.Adam(model.parameters(), LEARN_RATE)\ncriterion = nn.MSELoss()","f5e8a2ff":"for epoch in range(N_EPOCH):\n    losses = []\n    model.train()\n    for features, targets, ids in train_dataloader:\n        features = features.cuda().permute(0,2,1).float()\n        targets = targets.cuda().float()\n        predictions = model(features)\n        loss = criterion(predictions, targets)\n        for p in model.parameters():\n            p.grad = None\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.detach().cpu().numpy())\n    avg_loss = float(np.mean(losses))\n    print(epoch, avg_loss)\n\ntorch.save(model.state_dict(), 'weights.pth')","499bd87a":"### Training","b0d6ae3e":"### Model","52273ff9":"### Encoding functions","8a8ea13a":"### Dataset","264d2674":"### Configuration","000cd6da":"### Imports","31469b62":"### DataLoaders","92c176ca":"## Welcome to my first baseline notebook!\nThis is an interesting competition, especially with the short time constraint. We are provided a sequence of mRNA and for the first 68 bases we must predict three properties `reactivity`, `deg_Mg_pH10`, and `deg_Mg_50C`. In this notebook, I present my code for a simple baseline that as of publishing scores #1 on the leaderboard (0.380). I haven't messed with the hyperparameters at all so you are sure to get a better score than me!\n\nNote that this is the training part of a 2-part Notebook. The inference portion is located here: https:\/\/www.kaggle.compytorch-nn-starter-baseline-inference-0-380","538e6547":"Thank you for taking the time to view my notebook! I really hope you learned something and are able to use this as a platform for future work.\n\nThis is part one of a two-part notebook. To view the inference portion please go here: https:\/\/www.kaggle.compytorch-nn-starter-baseline-inference-0-380"}}