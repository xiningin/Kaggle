{"cell_type":{"cf6958c5":"code","904a8461":"code","bb51e356":"code","8497c6a2":"code","7423965b":"code","d4cc26a6":"code","ce87eb7c":"code","e46e724c":"markdown","e443e838":"markdown"},"source":{"cf6958c5":"import gym\nimport numpy as np\nimport warnings\n\nwarnings.simplefilter('ignore')\n\nenv = gym.make('MountainCarContinuous-v0')\nprint('Continuous action space: (%.3f to %.3f)'%(env.action_space.low, env.action_space.high))\nprint('Reward range: %s'%(str(env.reward_range)))\nfor i in range(len(env.observation_space.low)):\n    print('Observation range, dimension %i: (%.3f to %.3f)'%\n          (i,env.observation_space.low[i], env.observation_space.high[i]))","904a8461":"import numpy as np\nimport copy\nimport random\nimport sys\nfrom collections import namedtuple, deque\n\nfrom keras import backend as K\nfrom keras import layers, models, optimizers, regularizers\n\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nclass DDPG():\n    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n    def __init__(self, env, train_during_episode=True,\n                 discount_factor=.999,\n                 tau_actor=.2, tau_critic=.2,\n                 lr_actor=.0001, lr_critic=.005,\n                 bn_momentum_actor=.9, bn_momentum_critic=.9,\n                 ou_mu=0, ou_theta=.1, ou_sigma=1,\n                 activation_fn_actor='sigmoid',\n                 replay_buffer_size=10000, replay_batch_size=64,\n                 l2_reg_actor=.01, l2_reg_critic=.01,\n                 relu_alpha_actor=.01, relu_alpha_critic=.01,\n                 dropout_actor=0, dropout_critic=0,\n                 hidden_layer_sizes_actor=[32,64,32],\n                 hidden_layer_sizes_critic=[[32,64],[32,64]], ):\n\n        self.env = env\n        self.state_size = env.observation_space.shape[0]\n        self.action_size = env.action_space.shape[0]\n        self.action_low = env.action_space.low\n        self.action_high = env.action_space.high\n\n        self.train_during_episode = train_during_episode\n\n        # Actor (Policy) Model\n        self.actor_local = Actor(self.state_size, self.action_size, self.action_low,\n                self.action_high, activation_fn=activation_fn_actor, relu_alpha=relu_alpha_actor,\n                bn_momentum=bn_momentum_actor, learn_rate=lr_actor, l2_reg=l2_reg_actor,\n                dropout=dropout_actor, hidden_layer_sizes=hidden_layer_sizes_actor, )\n        self.actor_target = Actor(self.state_size, self.action_size, self.action_low,\n                self.action_high, activation_fn=activation_fn_actor, relu_alpha=relu_alpha_actor,\n                bn_momentum=bn_momentum_actor, learn_rate=lr_actor, l2_reg=l2_reg_actor,\n                dropout=dropout_actor, hidden_layer_sizes=hidden_layer_sizes_actor, )\n\n        # Critic (Q-Value) Model\n        self.critic_local = Critic(self.state_size, self.action_size, l2_reg=l2_reg_critic,\n                learn_rate=lr_critic, bn_momentum=bn_momentum_critic, relu_alpha=relu_alpha_critic,\n                hidden_layer_sizes=hidden_layer_sizes_critic, dropout=dropout_critic, )\n        self.critic_target = Critic(self.state_size, self.action_size, l2_reg=l2_reg_critic,\n                learn_rate=lr_critic, bn_momentum=bn_momentum_critic, relu_alpha=relu_alpha_critic,\n                hidden_layer_sizes=hidden_layer_sizes_critic, dropout=dropout_critic, )\n\n        # Initialize target model parameters with local model parameters\n        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n\n        # Noise process\n        self.exploration_mu = ou_mu\n        self.exploration_theta = ou_theta\n        self.exploration_sigma = ou_sigma\n        self.noise = OUNoise(self.action_size,\n                             self.exploration_mu, self.exploration_theta, self.exploration_sigma)\n\n        # Replay memory\n        self.buffer_size = replay_buffer_size\n        self.batch_size = replay_batch_size\n        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n\n        # Algorithm parameters\n        self.gamma = discount_factor  # discount factor\n        self.lr_actor = lr_actor\n        self.lr_critic = lr_critic\n        self.dropout_actor = dropout_actor\n        self.dropout_critic = dropout_critic\n        self.bn_momentum_actor = bn_momentum_actor\n        self.bn_momentum_critic = bn_momentum_critic\n        self.activation_fn_actor = activation_fn_actor\n        self.ou_mu=ou_mu\n        self.ou_theta=ou_theta\n        self.ou_sigma=ou_sigma\n        self.replay_buffer_size = replay_buffer_size\n        self.replay_batch_size = replay_batch_size\n        self.l2_reg_actor = l2_reg_actor\n        self.l2_reg_critic = l2_reg_critic\n        self.relu_alpha_actor = relu_alpha_actor\n        self.relu_alpha_critic = relu_alpha_critic\n        self.hidden_layer_sizes_actor = hidden_layer_sizes_actor\n        self.hidden_layer_sizes_critic = hidden_layer_sizes_critic\n\n        self.tau_actor = tau_actor\n        self.tau_critic = tau_critic\n\n        # Training history\n        self.training_scores = []\n        self.test_scores = []\n        self.training_history = TrainingHistory(env)\n        self.q_a_frames_spec = Q_a_frames_spec(env)\n\n        # Track training steps and episodes\n        self.steps = 0\n        self.episodes = 0\n\n        self.reset_episode()\n\n    def print_summary(self):\n        print(\"Actor model summary:\")\n        self.actor_local.model.summary()\n        print(\"Critic model summary:\")\n        self.critic_local.model.summary()\n        print(\"Hyperparameters:\")\n        print(str(dict(\n            train_during_episode=self.train_during_episode,\n            discount_factor=self.gamma,\n            tau_actor=self.tau_actor, tau_critic=self.tau_critic,\n            lr_actor=self.lr_actor, lr_critic=self.lr_critic,\n            bn_momentum_actor=self.bn_momentum_actor,\n            bn_momentum_critic=self.bn_momentum_critic,\n            ou_mu=self.ou_mu, ou_theta=self.ou_theta, ou_sigma=1,\n            activation_fn_actor=self.activation_fn_actor,\n            replay_buffer_size=self.replay_buffer_size,\n            replay_batch_size=self.replay_batch_size,\n            l2_reg_actor=self.l2_reg_actor, l2_reg_critic=self.l2_reg_critic,\n            relu_alpha_actor=self.relu_alpha_actor,\n            relu_alpha_critic=self.relu_alpha_critic,\n            dropout_actor=self.dropout_actor, dropout_critic=self.dropout_critic,\n            hidden_layer_sizes_actor=self.hidden_layer_sizes_actor,\n            hidden_layer_sizes_critic=self.hidden_layer_sizes_critic, )))\n\n    def preprocess_state(self, state):\n        obs_space = self.env.observation_space\n        return np.array([\n            (state[i]-obs_space.low[i])\/(obs_space.high[i]-obs_space.low[i])*2 - 1\n            for i in range(len(obs_space.low))])\n\n    def reset_episode(self):\n        self.noise.reset()\n        state = self.preprocess_state(self.env.reset())\n        self.last_state = state\n        return state\n\n    def step(self, action, reward, next_state, done):\n         # Save experience \/ reward\n        next_state = self.preprocess_state(next_state)\n        self.memory.add(self.last_state, action, reward, next_state, done)\n\n        # Learn, if enough samples are available in memory\n        if len(self.memory) > self.batch_size and (self.train_during_episode or done):\n            experiences = self.memory.sample()\n            self.learn(experiences)\n\n        # Roll over last state and action\n        self.last_state = next_state\n        self.steps += 1\n\n    def act(self, state=None, eps=0, verbose=False):\n        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n        if state is None:\n            state = self.last_state\n        else:\n            state = self.preprocess_state(state)\n        state = np.reshape(state, [-1, self.state_size])\n        action = self.actor_local.model.predict(state)[0]\n        noise_sample = self.noise.sample() * max(0,eps)\n        res = list(np.clip(action + noise_sample, self.action_low, self.action_high))\n        if verbose:\n            print(\"State: (%6.3f, %6.3f), Eps: %6.3f, Action: %6.3f + %6.3f = %6.3f\"%\n                  (state[0][0], state[0][1], eps, action, noise_sample, res[0]))\n        return res  # add some noise for exploration\n\n    def learn(self, experiences):\n        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)\n        states = np.vstack([e.state for e in experiences if e is not None])\n        actions = np.array([e.action for e in experiences if e is not None]\n                          ).astype(np.float32).reshape(-1, self.action_size)\n        rewards = np.array([e.reward for e in experiences if e is not None]\n                          ).astype(np.float32).reshape(-1, 1)\n        dones = np.array([e.done for e in experiences if e is not None]\n                        ).astype(np.uint8).reshape(-1, 1)\n        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n\n        # Get predicted next-state actions and Q values from target models\n        actions_next = self.actor_target.model.predict_on_batch(next_states)\n        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n\n        # Compute Q targets for current states and train critic model (local)\n        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n\n        # Train actor model (local)\n        action_gradients = np.reshape(\n            self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))\n        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function\n\n        # Soft-update target models\n        self.soft_update(self.critic_local.model, self.critic_target.model, self.tau_critic)\n        self.soft_update(self.actor_local.model, self.actor_target.model, self.tau_actor)\n\n    def soft_update(self, local_model, target_model, tau):\n        \"\"\"Soft update model parameters.\"\"\"\n        local_weights = np.array(local_model.get_weights())\n        target_weights = np.array(target_model.get_weights())\n\n        assert len(local_weights) == len(target_weights), \\\n            \"Local and target model parameters must have the same size\"\n\n        new_weights = tau * local_weights + (1 - tau) * target_weights\n        target_model.set_weights(new_weights)\n\n    def train_n_episodes(self, n_episodes, eps=1, eps_decay=None, action_repeat=1,\n                         run_tests=True, gen_q_a_frames_every_n_steps=0, draw_plots=False ):\n        if eps_decay is None: eps_decay = 1\/n_episodes\n        n_training_episodes = len(self.training_scores)\n        for i_episode in range(n_training_episodes+1, n_training_episodes+n_episodes+1):\n            eps -= eps_decay\n            eps = max(eps,0)\n            episode_start_step = self.steps\n            self.run_episode(train=True, action_repeat=action_repeat, eps=eps,\n                             gen_q_a_frames_every_n_steps=gen_q_a_frames_every_n_steps )\n            if run_tests is True:\n                self.run_episode(train=False, eps=0, action_repeat=action_repeat)\n            message = \"Episode %i - epsilon: %.2f, memory size: %i, training score: %.2f\"\\\n                        %(self.episodes, eps, len(self.memory), self.training_history.training_episodes[-1].score)\n            if run_tests: message += \", test score: %.2f\"%self.training_history.test_episodes[-1].score\n            print(message)\n            sys.stdout.flush()\n\n    def run_episode(self, action_repeat=1, eps=0, train=False, gen_q_a_frames_every_n_steps=0 ):\n        next_state = self.reset_episode()\n        if train: episode_history = self.training_history.new_training_episode(self.episodes+1,eps)\n        else: episode_history = self.training_history.new_test_episode(self.episodes,eps)\n        q_a_frame = None\n        while True:\n            action = self.act(next_state, eps=eps)\n            sum_rewards=0\n            # Repeat action `action_repeat` times, summing up rewards\n            for i in range(action_repeat):\n                next_state, reward, done, info = self.env.step(action)\n                sum_rewards += reward\n                if done:\n                    break\n            #sum_rewards = np.log1p(sum_rewards)\n            episode_history.append(self.steps, next_state, action, sum_rewards)\n            if train:\n                self.step(action, sum_rewards, next_state, done)\n                if gen_q_a_frames_every_n_steps > 0 and self.steps%gen_q_a_frames_every_n_steps==0:\n                    self.training_history.add_q_a_frame(self.get_q_a_frames())\n            if done:\n                if train:\n                    self.episodes += 1\n                break\n\n    def get_q_a_frames(self):\n        \"\"\" TODO: Figure out how to work with added dimensions.\n                - Use x_dim, y_dim, and a_dim to know which dimensions of state and action to vary.\n                    Maybe fill in the unvaried dimensions of states and actions with agent's current state\n                    and anticipated action (according to policy).\n        \"\"\"\n        xs = self.q_a_frames_spec.xs\n        nx = self.q_a_frames_spec.nx\n        ys = self.q_a_frames_spec.ys\n        ny = self.q_a_frames_spec.ny\n        action_space = self.q_a_frames_spec.action_space\n        na = self.q_a_frames_spec.na\n        x_dim = self.q_a_frames_spec.x_dim\n        y_dim = self.q_a_frames_spec.y_dim\n        a_dim = self.q_a_frames_spec.a_dim\n\n        def get_state(x,y):\n            s=copy.copy(self.last_state)\n            s[x_dim]=x\n            s[y_dim]=y\n            return s\n        raw_states = np.array([[ get_state(x,y) for x in xs ] for y in ys ]).reshape(nx*ny, self.state_size)\n\n        def get_action(action):\n            a=self.act() if self.action_size>1 else [0]\n            a[a_dim]=action\n            return a\n        actions = np.array([get_action(a) for a in action_space]*nx*ny)\n\n        preprocessed_states = np.array([ self.preprocess_state(s) for s in raw_states])\n        Q = self.critic_local.model.predict_on_batch(\n            [np.repeat(preprocessed_states,na,axis=0),actions]).reshape((ny,nx,na))\n        Q_max = np.max(Q,axis=2)\n        Q_std = np.std(Q,axis=2)\n        max_action = np.array([action_space[a] for a in np.argmax(Q,axis=2).flatten()]).reshape((ny,nx))\n        actor_policy = np.array([ self.act(s)[0] for s in raw_states]).reshape(ny,nx)\n        action_gradients = self.critic_local.get_action_gradients(\n            [preprocessed_states,actor_policy.reshape(nx*ny,-1),0])[0].reshape(ny,nx)\n\n        return namedtuple( 'q_a_frames',[\n                'step_idx', 'episode_idx', 'Q_max', 'Q_std', 'max_action', 'action_gradients', 'actor_policy'\n            ])(self.steps, self.episodes, Q_max, Q_std, max_action, action_gradients, actor_policy)\n\nclass ReplayBuffer:\n    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n    def __init__(self, buffer_size, batch_size):\n        \"\"\"Initialize a ReplayBuffer object.\n        Params\n        ======\n            buffer_size: maximum size of buffer\n            batch_size: size of each training batch\n        \"\"\"\n        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n        self.batch_size = batch_size\n        self.experience = namedtuple(\"Experience\",\n                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n\n    def add(self, state, action, reward, next_state, done):\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n\n    def sample(self):\n        return random.sample(self.memory, k=min(self.batch_size, len(self)))\n\n    def __len__(self):\n        return len(self.memory)\n\nclass OUNoise:\n    \"\"\"Ornstein-Uhlenbeck noise process.\"\"\"\n    def __init__(self, size, mu, theta, sigma):\n        \"\"\"Initialize parameters and noise process.\"\"\"\n        self.mu = mu * np.ones(size)\n        self.theta = theta\n        self.sigma = sigma\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n        self.state = copy.copy(self.mu)\n\n    def sample(self, sigma=None):\n        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n        if sigma is None:\n            sigma = self.sigma\n        x = self.state\n        dx = self.theta * (self.mu - x) + sigma * np.random.randn(len(x))\n        self.state = x + dx\n        return self.state\n    \nclass Actor:\n    \"\"\"Actor (Policy) Model.\"\"\"\n\n    def __init__(self, state_size, action_size, action_low, action_high, learn_rate,\n                 activation_fn, bn_momentum, relu_alpha, l2_reg, dropout, hidden_layer_sizes):\n        \"\"\"Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state.\n            action_size (int): Dimension of each action.\n            action_low (array): Min value of each action dimension.\n            action_high (array): Max value of each action dimension.\n            learn_rate (float): Learning rate.\n            activation_fn (string): Activation function, either 'sigmoid' or 'tanh'.\n            bn_momentum (float): Batch Normalization momentum .\n            relu_alpha (float): LeakyReLU alpha, allowing small gradient when the unit is not active.\n            l2_reg (float): L2 regularization factor for each dense layer.\n            dropout (float): Dropout rate\n            hidden_layer_sizes (list): List of hidden layer sizes.\n        \"\"\"\n        self.state_size = state_size\n        self.action_size = action_size\n        self.action_low = action_low\n        self.action_high = action_high\n        self.action_range = self.action_high - self.action_low\n\n        # Initialize any other variables here\n        self.learn_rate = learn_rate\n        self.activation = activation_fn\n        self.bn_momentum = bn_momentum\n        self.relu_alpha = relu_alpha\n        self.l2_reg = l2_reg\n        self.dropout = dropout\n        self.hidden_layer_sizes = hidden_layer_sizes\n\n        self.build_model()\n\n    def build_model(self):\n        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n        states = layers.Input(shape=(self.state_size,), name='states')\n        net = states\n\n        # Batch Norm instead of input preprocessing (Since we don't know up front the range of state values.)\n        #net = layers.BatchNormalization(momentum=self.bn_momentum)(states)\n\n        # Add a hidden layer for each element of hidden_layer_sizes\n        for size in self.hidden_layer_sizes:\n            net = layers.Dense(units=size, kernel_regularizer=regularizers.l2(l=self.l2_reg))(net)\n            #net = layers.BatchNormalization(momentum=self.bn_momentum)(net)\n            if self.relu_alpha>0: net = layers.LeakyReLU(alpha=self.relu_alpha)(net)\n            else: net = layers.Activation('relu')(net)\n\n        if self.dropout>0: net = layers.Dropout(.2)(net)\n\n        if self.bn_momentum>0: net = layers.BatchNormalization(momentum=self.bn_momentum)(net)\n\n        if self.activation=='tanh':\n            # Add final output layer with tanh activation with [-1, 1] output\n            actions = layers.Dense(units=self.action_size, activation=self.activation,\n                name='actions')(net)\n        elif self.activation=='sigmoid':\n            # Add final output layer with sigmoid activation\n            raw_actions = layers.Dense(units=self.action_size, activation=self.activation,\n                name='raw_actions')(net)\n            # Scale [0, 1] output for each action dimension to proper range\n            actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low,\n                name='actions')(raw_actions)\n        else:\n            raise \"Expected 'activation' to be one of: 'tanh', or 'sigmoid'.\"\n\n        self.model = models.Model(inputs=states, outputs=actions)\n        action_gradients = layers.Input(shape=(self.action_size,))\n        loss = K.mean(-action_gradients * actions)\n\n        # Incorporate any additional losses here (e.g. from regularizers)\n\n        optimizer = optimizers.Adam(lr=self.learn_rate)\n        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n        self.train_fn = K.function(\n            inputs=[self.model.input, action_gradients, K.learning_phase()],\n            outputs=[],\n            updates=updates_op)\n\nclass Critic:\n    \"\"\"Critic (Value) Model.\"\"\"\n\n    def __init__(self, state_size, action_size, learn_rate, bn_momentum,\n                 relu_alpha, l2_reg, dropout, hidden_layer_sizes,\n                ):\n        \"\"\"Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state.\n            action_size (int): Dimension of each action.\n            learn_rate (float): Learning rate.\n            bn_momentum (float): Batch Normalization momentum.\n            relu_alpha (float): LeakyReLU alpha, allowing small gradient when the unit is not active.\n            l2_reg (float): L2 regularization factor for each dense layer.\n            dropout (float): Dropout rate\n            hidden_layer_sizes (list[list]): List of two lists with hidden layer sizes for state and action pathways.\n        \"\"\"\n        self.state_size = state_size\n        self.action_size = action_size\n\n        assert len(hidden_layer_sizes)==2 \\\n            and len(hidden_layer_sizes[0])==len(hidden_layer_sizes[1]),\\\n            \"Expected Critic's hidden_layer_sizes to be a list of two arrays of equal length.\"\n\n        # Initialize any other variables here\n        self.learn_rate = learn_rate\n        self.bn_momentum = bn_momentum\n        self.relu_alpha = relu_alpha\n        self.l2_reg = l2_reg\n        self.dropout = dropout\n        self.hidden_layer_sizes = hidden_layer_sizes\n\n        self.build_model()\n\n    def build_model(self):\n        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n        states = layers.Input(shape=(self.state_size,), name='states')\n        actions = layers.Input(shape=(self.action_size,), name='actions')\n        net_states = states\n        net_actions = actions\n\n        # Add hidden layer(s) for state pathway\n        for size in self.hidden_layer_sizes[0]:\n            net_states = layers.Dense(units=size,\n                                      kernel_regularizer=regularizers.l2(l=self.l2_reg))(net_states)\n            #net_states = layers.BatchNormalization(momentum=self.bn_momentum)(net_states)\n            if self.relu_alpha>0: net_states = layers.LeakyReLU(alpha=self.relu_alpha)(net_states)\n            else: net_states = layers.Activation('relu')(net_states)\n\n        # Add hidden layer(s) for action pathway\n        for size in self.hidden_layer_sizes[1]:\n            net_actions = layers.Dense(units=size,\n                                       kernel_regularizer=regularizers.l2(l=self.l2_reg))(net_actions)\n            #net_actions = layers.BatchNormalization(momentum=self.bn_momentum)(net_actions)\n            if self.relu_alpha>0: net_actions = layers.LeakyReLU(alpha=self.relu_alpha)(net_actions)\n            else: net_actions = layers.Activation('relu')(net_actions)\n\n        # Combine state and action pathways\n        net = layers.Add()([net_states, net_actions])\n        if self.relu_alpha>0: net = layers.LeakyReLU(alpha=self.relu_alpha)(net)\n        else: net = layers.Activation('relu')(net)\n\n        # Add more layers to the combined network if needed\n        if self.dropout>0: net = layers.Dropout(self.dropout)(net)\n\n        # Normalize the final activations\n        if self.bn_momentum>0: net = layers.BatchNormalization(momentum=self.bn_momentum)(net)\n\n        # Add final output layer to prduce action values (Q values)\n        Q_values = layers.Dense(units=1, name='q_values')(net)\n\n        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n\n        # Define optimizer and compile model for training with built-in loss function\n        optimizer = optimizers.Adam(lr=self.learn_rate)\n        self.model.compile(optimizer=optimizer, loss='mse')\n\n        # Compute action gradients (derivative of Q values w.r.t. to actions)\n        action_gradients = K.gradients(Q_values, actions)\n\n        # Define an additional function to fetch action gradients (to be used by actor model)\n        self.get_action_gradients = K.function(\n            inputs=[*self.model.input, K.learning_phase()],\n            outputs=action_gradients)","bb51e356":"class TrainingHistory:\n    \"\"\"\n    Tracks training history, including a snapshot of rasterized Q values and actions\n    for use in visualizations.\n    \"\"\"\n    def __init__(self, env, nx=16, ny=16, na=11, x_dim=0, y_dim=1, a_dim=0):\n        \"\"\"\n        Initialize TrainingHistory object with Q grid shape.\n        Params\n        ======\n        \"\"\"\n        self.training_episodes = []\n        self.test_episodes = []\n        self.q_a_frames = []\n        self.last_step = 0\n        self.q_a_frames_spec = Q_a_frames_spec(env, nx=nx, ny=ny, na=na,\n                                         x_dim=x_dim, y_dim=y_dim, a_dim=a_dim)\n\n    def __repr__(self):\n        return \"TrainingHistory ( %i training_episodes, %i test_episodes, %i qa_grids, last_step: %i )\"%\\\n                (len(self.training_episodes), len(self.test_episodes), len(self.q_a_frames), self.last_step)\n\n    def add_q_a_frame(self, q_a_frame):\n        self.q_a_frames.append(q_a_frame)\n\n    def new_training_episode(self, idx, epsilon=None):\n        episode = EpisodeHistory(idx, epsilon)\n        self.training_episodes.append(episode)\n        return episode\n\n    def new_test_episode(self, idx, epsilon=None):\n        episode = EpisodeHistory(idx, epsilon)\n        self.test_episodes.append(episode)\n        return episode\n\n    def get_training_episode_for_step(self, step_idx):\n        for ep in self.training_episodes:\n            if ep.last_step>=step_idx:\n                return ep\n    def get_test_episode_for_step(self, step_idx):\n        for ep in self.test_episodes:\n            if (ep.last_step+1)>=step_idx:\n                return ep\n    def get_q_a_frames_for_step(self, step_idx):\n        for g in self.q_a_frames:\n            if g.step_idx>=step_idx:\n                return g\n        return g\n    def append_training_step(self, step, state, action, reward):\n        \"\"\"\n        Initialize EpisodeHistory with states, actions, and rewards\n        Params\n        ======\n            episode_idx (int): Episode index\n            step (int): Step index\n            state (list|array): State, array-like of shape env.observation_space.shape\n            action (list|array): Action, array-like of shape env.action_space.shape\n            reward (float): Reward, scalar value\n        \"\"\"\n        if len(self.training_episodes)==0:\n            raise \"No training episodes exist yet. \"\n        self.training_episodes[-1].append(step, state, action, reward)\n        self.last_step = step\n        return self\n    def append_test_step(self, step, state, action, reward):\n        \"\"\"\n        Initialize EpisodeHistory with states, actions, and rewards\n        Params\n        ======\n            episode_idx (int): Episode index\n            step (int): Step index\n            state (list|array): State, array-like of shape env.observation_space.shape\n            action (list|array): Action, array-like of shape env.action_space.shape\n            reward (float): Reward, scalar value\n        \"\"\"\n        if len(self.test_episodes)==0:\n            raise \"No test episodes exist yet. \"\n        self.test_episodes[-1].append(step, state, action, reward)\n        self.last_step = step\n        return self\n    \nclass EpisodeHistory:\n    \"\"\" Tracks the history for a single episode, including the states, actions, and rewards.\n    \"\"\"\n    def __init__(self, episode_idx=None, epsilon=None):\n        \"\"\"\n        Initialize EpisodeHistory with states, actions, and rewards\n        Params\n        ======\n            episode_idx (int): Episode index\n            epsilon (float): Exploration factor\n        \"\"\"\n        self.episode_idx = episode_idx\n        self.epsilon = epsilon\n        self.steps = []\n        self.first_step = None\n        self.last_step = None\n        self.states = []\n        self.actions = []\n        self.rewards = []\n\n        self.score = self._get_score()\n\n    def _get_score(self):\n        return sum(self.rewards)\n\n    def append(self, step, state, action, reward):\n        self.steps.append(step)\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        if self.first_step is None: self.first_step = step\n        self.last_step = step\n        self.score += reward\n\n    def __len__(self):\n        return len(self.states)\n\n    def __repr__(self):\n        return \"EpisodeHistory ( idx: %i, len: %i, first_step: %i, last_step: %i, epsilon: %.3f, score: %.3f )\"%\\\n            (self.episode_idx,len(self.steps),self.first_step, self.last_step, self.epsilon, self.score)\n    \nclass Q_a_frames_spec():\n    \"\"\" \n    Tracks training history, including a snapshot of rasterized Q values and actions\n    for use in visualizations. \n    \"\"\"\n    def __init__(self, env, nx=16, ny=16, na=11, x_dim=0, y_dim=1, a_dim=0):\n        \"\"\"\n        Initialize Q_a_frame_set object with Q grid shape.\n        Params\n        ======            \n             env (obj): OpenAi Gym environment\n             nx (int): Width of Q grid (default: 16)\n             ny (int): Height of Q grid (default: 16)\n             na (int): Depth of Q grid (default: 11)\n             x_dim (int): Observation dimension to use as x-axis (default: 0)\n             y_dim (int): Observation dimension to use as y-axis (default: 1)\n             a_dim (int): Action dimension to use as x-axis (default: 0)\n        \"\"\"\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n        self.a_dim = a_dim\n        \n        self.xmin = env.observation_space.low[x_dim]\n        self.xmax = env.observation_space.high[x_dim]\n        self.xs = np.arange(self.xmin, self.xmax, (self.xmax-self.xmin)\/nx)[:nx]\n        self.nx = len(self.xs)\n        \n        self.ymin = env.observation_space.low[y_dim]\n        self.ymax = env.observation_space.high[y_dim]\n        self.ys = np.arange(self.ymin, self.ymax, (self.ymax-self.ymin)\/ny)[:ny]\n        self.ny = len(self.ys)\n        \n        self.amin = env.action_space.low[a_dim]\n        self.amax = env.action_space.high[a_dim]\n        self.action_space = np.linspace(self.amin,self.amax,na)\n        self.na = len(self.action_space)","8497c6a2":"from datetime import datetime\nimport matplotlib.gridspec as gridspec\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.ticker import MaxNLocator\nfrom IPython.display import Image, HTML\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport math\nfrom imageio_ffmpeg import get_ffmpeg_exe\nimport numpy as np\n\n# plt.rcParams['animation.embed_limit'] = 200\nplt.rcParams['animation.ffmpeg_path'] = get_ffmpeg_exe()\n\nplt.rcParams['axes.labelsize'] = 8\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['axes.titlesize'] = 10\n\ndef create_animation(agent, every_n_steps=1, display_mode='gif', fps=30):\n    history = agent.training_history\n    fig = plt.figure(figsize=(11,6))\n    fig.set_tight_layout(True)\n    main_rows = gridspec.GridSpec(2, 1, figure=fig, top=.9, left=.05, right=.95, bottom=.25)\n\n    def create_top_row_im(i, title='', actions_cmap=False):\n        top_row = main_rows[0].subgridspec(1, 5, wspace=.3)\n        ax = fig.add_subplot(top_row[i])\n        ax.axis('off')\n        ax.set_title(title)\n        im = ax.imshow( np.zeros((len(history.q_a_frames_spec.ys),\n                                  len(history.q_a_frames_spec.xs))), origin='lower' )\n        divider = make_axes_locatable(ax)\n        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n        if actions_cmap is True:\n            im.set_clim(history.q_a_frames_spec.amin,\n                        history.q_a_frames_spec.amax)\n            im.set_cmap(\"RdYlGn\")\n            cb = fig.colorbar(im, cax=cax)\n        else:\n            cb = fig.colorbar(im, cax=cax, format='%.3g')\n        cb.ax.tick_params(labelsize=8)\n        return im\n\n    def create_bottom_row_plot(i, title=''):\n        bottom_row = main_rows[1].subgridspec(1,3)\n        ax = fig.add_subplot(bottom_row[i])\n        ax.set_title(title)\n        return ax\n\n    Q_max_im = create_top_row_im(0, title='Q max')\n    Q_std_im = create_top_row_im(1, title='Q standard deviation')\n    action_gradients_im = create_top_row_im(2, title=\"Action Gradients\")\n    max_action_im = create_top_row_im(3, title=\"Action with Q max\", actions_cmap=True)\n    actor_policy_im = create_top_row_im(4, title=\"Policy\", actions_cmap=True)\n\n    scores_ax = create_bottom_row_plot(0, title=\"Scores\")\n    scores_ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    scores_ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    training_scores_line, = scores_ax.plot([], 'bo', label='training')\n    test_scores_line, = scores_ax.plot([], 'ro', label='test')\n    scores_ax.set_xlim(1,len(history.training_episodes))\n    scores_combined = np.array([e.score for e in history.training_episodes ]+\\\n                               [e.score for e in history.test_episodes ])\n    scores_ax.set_ylim(scores_combined.min(),scores_combined.max())\n    scores_ax.set_xlabel('episode')\n    scores_ax.set_ylabel('total reward')\n    scores_ax.legend(loc='upper left', bbox_to_anchor=(0,-.1))\n\n    training_episode_ax = create_bottom_row_plot(1)\n    training_episode_ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    training_episode_position_line, = training_episode_ax.plot([], 'b-', label='position')\n    training_episode_velocity_line, = training_episode_ax.plot([], 'm-', label='velocity')\n    training_episode_action_line, = training_episode_ax.plot([], 'g-', label='action')\n    training_episode_reward_line, = training_episode_ax.plot([], 'r-', label='reward')\n    training_episode_ax.set_ylim((-1.1,1.1))\n    training_episode_ax.axes.get_yaxis().set_visible(False)\n#     training_episode_ax.legend(loc='upper left', ncol=2)\n\n    test_episode_ax = create_bottom_row_plot(2)\n    test_episode_ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    test_episode_position_line, = test_episode_ax.plot([], 'b-', label='position')\n    test_episode_velocity_line, = test_episode_ax.plot([], 'm-', label='velocity')\n    test_episode_action_line, = test_episode_ax.plot([], 'g-', label='action')\n    test_episode_reward_line, = test_episode_ax.plot([], 'r-', label='reward')\n    test_episode_ax.set_ylim((-1.1,1.1))\n    test_episode_ax.axes.get_yaxis().set_visible(False)\n    test_episode_ax.legend(loc='upper left', ncol=2, bbox_to_anchor=(-.5,-.1))\n\n\n    def update(step_idx):\n        num_frames = math.ceil(last_step\/every_n_steps)\n        frame_idx = math.ceil(step_idx\/every_n_steps)\n        print(\"Drawing frame: %i\/%i, %.2f%%\\r\"%\\\n              (frame_idx+1, num_frames, 100*(frame_idx+1)\/float(num_frames) ), end='')\n        training_episode = history.get_training_episode_for_step(step_idx)\n        episode_step_idx = step_idx - training_episode.first_step\n\n        q_a_frames = history.get_q_a_frames_for_step(step_idx)\n\n        Q_max_im.set_data(q_a_frames.Q_max)\n        Q_max_im.set_clim(q_a_frames.Q_max.min(),q_a_frames.Q_max.max())\n        Q_std_im.set_data(q_a_frames.Q_std)\n        Q_std_im.set_clim(q_a_frames.Q_std.min(),q_a_frames.Q_std.max())\n        action_gradients_im.set_data(q_a_frames.action_gradients)\n        action_gradients_im.set_clim(q_a_frames.action_gradients.min(),\n                                     q_a_frames.action_gradients.max())\n        max_action_im.set_data(q_a_frames.max_action)\n        actor_policy_im.set_data(q_a_frames.actor_policy)\n\n        # Plot scores\n        xdata = range(1,training_episode.episode_idx+1)\n        training_scores_line.set_data(xdata,\n            [e.score for e in history.training_episodes ][:training_episode.episode_idx] )\n        test_scores_line.set_data(xdata,\n            [e.score for e in history.test_episodes ][:training_episode.episode_idx] )\n\n        #Plot training episode\n        training_episode_ax.set_title(\"Training episode %i, eps=%.3f, score: %.3f\"%(\n                            training_episode.episode_idx, training_episode.epsilon, training_episode.score))\n\n        current_end_idx = episode_step_idx + every_n_steps\n        if current_end_idx >= len(training_episode.states):\n            current_end_idx = len(training_episode.states)-1\n\n        training_xdata = range(0,current_end_idx+1)\n        training_episode_ax.set_xlim(training_xdata[0],\n                                     training_episode.last_step-training_episode.first_step+1)\n        episode_states = [agent.preprocess_state(s) for s in training_episode.states]\n        training_episode_position_line.set_data(training_xdata,\n                                                [s[0] for s in episode_states][:current_end_idx+1])\n        training_episode_velocity_line.set_data(training_xdata,\n                                                [s[1] for s in episode_states][:current_end_idx+1])\n        training_episode_action_line.set_data(training_xdata,\n                                              training_episode.actions[:current_end_idx+1])\n        training_episode_reward_line.set_data(training_xdata,\n                                              training_episode.rewards[:current_end_idx+1])\n\n        #Plot test episode\n        test_episode = history.get_test_episode_for_step(step_idx)\n        if test_episode is not None:\n            test_episode_ax.set_title(\"Test episode %i, score: %.3f\"%(\n                                test_episode.episode_idx, test_episode.score))\n            test_xdata = range(1,len(test_episode.states)+1)\n            test_episode_ax.set_xlim(test_xdata[0],test_xdata[-1])\n            episode_states = [agent.preprocess_state(e) for e in test_episode.states]\n            test_episode_position_line.set_data(test_xdata, [s[0] for s in episode_states])\n            test_episode_velocity_line.set_data(test_xdata, [s[1] for s in episode_states])\n            test_episode_action_line.set_data(test_xdata,\n                                              test_episode.actions )\n            test_episode_reward_line.set_data(test_xdata, test_episode.rewards )\n\n    last_step = history.training_episodes[-1].last_step + 1\n    anim = FuncAnimation(fig, update, interval=1000\/fps,\n                         frames=range(0,last_step,every_n_steps))\n\n    if display_mode=='video' or display_mode=='video_file':\n        from matplotlib.animation import FFMpegWriter\n        writer = FFMpegWriter(fps=fps)\n        if writer.isAvailable():\n            print(\"Using ffmpeg at '%s'.\"%writer.bin_path())\n        else:\n            raise(\"FFMpegWriter not available for video output.\")\n    if display_mode=='js':\n        display(HTML(anim.to_jshtml()))\n    elif display_mode=='video':\n        display(HTML(anim.to_html5_video()))\n    elif display_mode=='video_file':\n        filename = 'training_animation_%i.mp4'%int(datetime.now().timestamp())\n        img = anim.save(filename, writer=writer)\n        print(\"\\rVideo saved to %s.\"%filename)\n        import io, base64\n        encoded = base64.b64encode(io.open(filename, 'r+b').read())\n        display(HTML(data='''<video alt=\"training animation\" controls loop autoplay>\n                        <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n                     <\/video>'''.format(encoded.decode('ascii'))))\n    else:\n        filename = 'training_animation_%i.gif'%int(datetime.now().timestamp())\n        img = anim.save(filename, dpi=80, writer='imagemagick')\n        display(HTML(\"<img src='%s'\/>\"%filename))\n    plt.close()","7423965b":"agent = DDPG(env, train_during_episode=True, ou_mu=0, ou_theta=.05, ou_sigma=.25, \n             discount_factor=.999, replay_buffer_size=10000, replay_batch_size=1024,\n             tau_actor=.3, tau_critic=.1, \n             relu_alpha_actor=.01, relu_alpha_critic=.01,\n             lr_actor=.0001, lr_critic=.005, activation_fn_actor='tanh',\n             l2_reg_actor=.01, l2_reg_critic=.01, \n             bn_momentum_actor=0, bn_momentum_critic=.7,\n             hidden_layer_sizes_actor=[16,32,16], hidden_layer_sizes_critic=[[16,32],[16,32]], )\nagent.print_summary()","d4cc26a6":"agent.train_n_episodes(50, eps=1, eps_decay=1\/50, action_repeat=5, \n                       run_tests=True, gen_q_a_frames_every_n_steps=10, )","ce87eb7c":"create_animation(agent, display_mode='video_file', every_n_steps=10, fps=15)","e46e724c":"### Visualization TODOs:\n* Print hyperparameters.\n* Chart epsilon, action_repeat, batch_size \/ memory_size \/ buffer_length.\n* Plot last n memory samples used training.\n* ~~Make one animation with all training steps~~\n* ~~Plot training\/test scores~~\n* ~~Plot current episode~~\n* ~~Plot last test episode~~","e443e838":"# Solving OpenAI Gym's Mountain Car Continuous Control task using Deep Deterministic Policy Gradients  \n\nThis notebook uses a modified version of [Udacity's DDPG model](https:\/\/github.com\/udacity\/deep-reinforcement-learning\/tree\/master\/ddpg-pendulum) to solve OpenAI Gym's [MountainCarContinuous-v0](https:\/\/github.com\/openai\/gym\/wiki\/MountainCarContinuous-v0) continuous control problem using [Deep Deterministic Policy Gradients (DDPG)](https:\/\/arxiv.org\/abs\/1509.02971) as part of the [Machine Learning Engineer Nanodegree](https:\/\/www.udacity.com\/course\/machine-learning-engineer-nanodegree--nd009t) quadcopter project. \n\nThe code is available on my github repo at https:\/\/github.com\/samhiatt\/ddpg_agent. This kernel version uses code from [this commit](https:\/\/github.com\/samhiatt\/ddpg_agent\/tree\/6131f08d4a4244e8a9bf5cad430774d3762e33d5).\n\nSolving the MountainCarContinuous problem with DDPG is a particularly good place to start as its 2-dimensional continuous state space (position and velocity) and 1-dimensional continuous action space (forward, backward) are easy to visualize in two dimensions, lending to an intuitive understanding of hyperparameter tuning. \n\nAndre Muta's [DDPG-MountainCarContinuous-v0](https:\/\/github.com\/amuta\/DDPG-MountainCarContinuous-v0) repo was helpful in suggesting some good visualizations as well as giving some good hyperparameters to start with. It looks like he uses the same code from the nanodegree quadcopter project and uses it to solve the MountainCarContinuous problem as well. His [plot_Q method in MountainCar.py](https:\/\/github.com\/amuta\/DDPG-MountainCarContinuous-v0\/blob\/master\/MountainCar.py) was particularly helpful by showing how to plot Q_max, Q_std, Action at Q_max, and Policy. Adding a visualization of the policy gradients and animating the training process ended up helping me better understand the problem and the effects of various hypterparemeters. \n\nSee [kernel version 61](https:\/\/www.kaggle.com\/samhiatt\/mountaincarcontinuous-v0-ddpg?scriptVersionId=15941050) for a good solution with and animation of policy\/value functions and episode states.\n\n[Kernel version 72](https:\/\/www.kaggle.com\/samhiatt\/mountaincarcontinuous-v0-ddpg?scriptVersionId=16046023) uses batch norm _before_ the critic's last fully-connected layer, as opposed to after it as in earlier versions. This seems to have the effect of reducing the magnitude of the actor's actions which seems to end up achieving a higher test score.\n\nUsing action repeat seemes to help speed up training, and also seems to work better during initial exploratory episodes.\n\nScaling rewards with np.log1p(reward) seems to help speed up training significantly. Using batch normalization before the Critic's final layer seems to have a similar effect. \n\nNote that subsequent runs of [Version 11](https:\/\/www.kaggle.com\/samhiatt\/ddpg-mountaincarcontinuous-v0?scriptVersionId=15205346) with the same hyperparameters did not find a workable policy and would usually end up with a policy always pushing forward (action=1). \n\n[Version 19](https:\/\/www.kaggle.com\/samhiatt\/ddpg-mountaincarcontinuous-v0?scriptVersionId=15555111) appears to be stable, finding a workable policy after about 60 episodes.\n\n### This kernel version includes the following:\n* All hyperparameters configurable in `DDPG.__init__`\n* mp4 output of training animation using ffmpeg binary from imagio-ffmpeg \n* re-worked architecture for tracking training history.\n* Batch Normalization _before_ Critic's last dense layer.\n* Batch Normalization before Actor's last dense layer. (disabled)\n* ~~L2 Regularization on each dense layer~~\n* Action repeat (n=5)\n* ~~LeakyReLUs~~\n* Input normalization preprocessing\n* Half of the nodes per layer compared to Udacity example model.\n* Epsilon decay after each episode. \n* Tests policy with epsilon=0 after each episode.\n* ~~Batch Normalization~~\n* ~~Tries to select a training batch that includes at least one positive reward.~~\n* ~~Option to only remember episodes where the total reward is positive. (Set to False in this version)~~\n\n### More things to try:\n* **Use `env.goal_position` to enhance reward function**\n* Using BatchNorm on all hidden layers\n* Write training history to file.\n* Snapshot model weights while training.\n* Alternative methods for decaying epsilon, maybe linked to test score \/ training score trends?\n    * Modulate epsilon depending on latest test scores.\n        * Decrease action_repeat after repeated high test scores.\n\n## Credits\n* [Continuous control with deep reinforcement learning](https:\/\/arxiv.org\/abs\/1509.02971)\n* Andre Muta's [DDPG-MountainCarContinuous-v0](https:\/\/github.com\/amuta\/DDPG-MountainCarContinuous-v0).\n* Thanks to [Eli Bendersky](https:\/\/eli.thegreenplace.net\/2016\/drawing-animated-gifs-with-matplotlib\/) for help with matplotlib animations. \n* Thanks to [Joseph Long](https:\/\/joseph-long.com\/writing\/colorbars\/) for help with matplotlib colorbar axes placement."}}