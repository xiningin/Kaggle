{"cell_type":{"edc71d49":"code","1f13bdb2":"code","a14d561f":"code","ec3bc998":"code","68ac4094":"code","df6ea1e1":"code","7dfc239e":"code","5ea2dd60":"code","87b852bf":"code","1ff0e86b":"code","7ec92041":"code","ffc2df52":"code","d3ecf021":"code","6745e974":"code","21e33974":"code","87d5d6cd":"code","ff5440a3":"code","d85e2836":"code","6a88e4f4":"code","d5a763b0":"code","c6d08c26":"code","5a13a7f6":"markdown","15e73483":"markdown","3609b72d":"markdown","727b6d18":"markdown","20af2dd9":"markdown","f238db94":"markdown","1616cdc1":"markdown","636602b7":"markdown","a1ab5d83":"markdown"},"source":{"edc71d49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f13bdb2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split","a14d561f":"# Loading dataset\ndata = pd.read_csv(\"..\/input\/did-it-rain-in-seattle-19482017\/seattleWeather_1948-2017.csv\")\ndata.head()","ec3bc998":"# Data processing\ndata = data.dropna(axis=0)\ndata = data.iloc[:, 1:5]\n","68ac4094":"data.head()","df6ea1e1":"fig = plt.figure(figsize = (10, 7))\nax = plt.axes(projection =\"3d\")\n\nax.set_xlabel(\"TMIN\")\nax.set_ylabel(\"TMAX\")\nax.set_zlabel(\"PRCP\")\nax.scatter3D(xs=data[\"TMIN\"],ys=data[\"TMAX\"],zs=data[\"PRCP\"], c=data[\"RAIN\"])\nplt.show()","7dfc239e":"# 2. Spliting Dataset\ntrain_set, test_set = train_test_split(data, train_size=0.7, test_size=0.3)","5ea2dd60":"train_set.head()","87b852bf":"test_set.head()","1ff0e86b":"train_data, train_lable = train_set.iloc[:, 0:3], train_set[\"RAIN\"].astype('int')\ntest_data, test_lable = test_set.iloc[:, 0:3], test_set[\"RAIN\"].astype('int')","7ec92041":"train_data","ffc2df52":"# Correlations between columns\ndata['RAIN'] = data['RAIN'].astype('int')\ncorr =data.corr()\ncorr.style.background_gradient(cmap='coolwarm')","d3ecf021":"# NB_classifier with Full features\nfrom sklearn.naive_bayes import GaussianNB\nNB_classifier = GaussianNB().fit(train_data, train_lable)\nNB_classifier.score(test_data, test_lable)","6745e974":"# NB_classifier with PRCP + TMAX\ntrain_PRCP_TMAX = train_data[['PRCP','TMAX']]\nNB_classifier = GaussianNB().fit(train_PRCP_TMAX, train_lable)\nNB_classifier.score(test_data[['PRCP','TMAX']], test_lable)","21e33974":"# NB_classifier with PRCP + TMIN\ntrain_PRCP_TMAX = train_data[['PRCP','TMIN']]\nNB_classifier = GaussianNB().fit(train_PRCP_TMAX, train_lable)\nNB_classifier.score(test_data[['PRCP','TMIN']], test_lable)","87d5d6cd":"# NB_classifier with TMAX + TMIN\ntrain_PRCP_TMAX = train_data[['TMAX','TMIN']]\nNB_classifier = GaussianNB().fit(train_PRCP_TMAX, train_lable)\nNB_classifier.score(test_data[['TMAX','TMIN']], test_lable)","ff5440a3":"# NB_classifier with only one column\n# PRCP\ntrain_PRCP_TMAX = train_data[['PRCP']]\nNB_classifier = GaussianNB().fit(train_PRCP_TMAX, train_lable)\nNB_classifier.score(test_data[['PRCP']], test_lable)\n","d85e2836":"# TMAX\ntrain_PRCP_TMAX = train_data[['TMAX']]\nNB_classifier = GaussianNB().fit(train_PRCP_TMAX, train_lable)\nNB_classifier.score(test_data[['TMAX']], test_lable)\n","6a88e4f4":"# TMIN\ntrain_PRCP_TMAX = train_data[['TMIN']]\nNB_classifier = GaussianNB().fit(train_PRCP_TMAX, train_lable)\nNB_classifier.score(test_data[['TMIN']], test_lable)","d5a763b0":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_cls = KNeighborsClassifier(n_neighbors=1).fit(train_data, train_lable)\nknn_cls.score(test_data, test_lable)","c6d08c26":"# DT Model\nfrom sklearn.tree import DecisionTreeClassifier\nDT_cls = DecisionTreeClassifier().fit(train_data, train_lable)\nDT_cls.score(test_data, test_lable)","5a13a7f6":"3.\tA main assumption in the Na\u00efve Bayes classifier is the independence of the features. Test the independence of features in your dataset. Does this satisfy the conditions of the Na\u00efve Bayes algorithm?","15e73483":"# PE05 Assignment using Team Project's dataset\n1. By now, you should have your team project topic finalized. Use the same dataset or a dataset that will help you have a better understanding of the problem you will be dealing with in your team project. Note: In order to make it simple, if your team project is not a binary classification problem, only select two of the classes for this assignment.","3609b72d":"Answer: The Maximum Temperature depends on the Minimum Temperature. RAIN depends on PRCP.","727b6d18":"4.\tUse the additional 30% of your data as a testing dataset to predict which class they belong to and calculate the accuracy of the classifier and compare it with other classifiers you have used for this dataset. Which classifier fits best with this dataset so far?","20af2dd9":"I will drop the date column because it's hard to vectorizing date data","f238db94":"2.\tChoose 70% of your dataset as the training data for your Na\u00efve Bayes classifier.","1616cdc1":"Conclusion: Decision Tree Model == Naive Bayes >> K-Nearest Neighbors\nDecision Tree and Naive Bayes classifiers have the same accuracy, KNN has relatively low accuracy.","636602b7":"Conclusion: The PRCP is the key feature to predict whether it rains or not rain.","a1ab5d83":"# Plotting All Data Points"}}