{"cell_type":{"a282029f":"code","c8eedfb9":"code","168c8e61":"code","5a7bde39":"code","431b4413":"code","041cf372":"code","0fc4d6a1":"code","485e25b5":"code","e0474777":"code","ef85bc66":"code","e16ded26":"code","2cf407e4":"code","946a0262":"code","766c23e7":"code","b3187472":"code","99a351b1":"code","c9857407":"code","12734978":"code","60b99a1c":"code","0b2e973d":"markdown","c9d07c8f":"markdown","630fcfdb":"markdown","dfe2bb80":"markdown","b82dd56d":"markdown","0dc4225a":"markdown","670a0b68":"markdown","1c37a029":"markdown","7d432fb0":"markdown","ef4a6352":"markdown","e6fe0fa9":"markdown","7895cc4d":"markdown","aa03ded2":"markdown","13ae539d":"markdown","c8a43b49":"markdown","cd13be0e":"markdown","5ed468d1":"markdown","01bf8a1e":"markdown","29f70678":"markdown","0b545a79":"markdown","794d891c":"markdown","e1fbb7df":"markdown","c3ff5a26":"markdown"},"source":{"a282029f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling\nimport matplotlib.pyplot as plt# Plotting library for Python programming language and it's numerical mathematics extension NumPy\nimport seaborn as sns# Provides a high level interface for drawing attractive and informative statistical graphics\n%matplotlib inline\nsns.set()\n\nfrom subprocess import check_output\n\n# Any results you write to the current directory are saved as output.","c8eedfb9":"titanic_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/gampa123\/AI\/master\/Machine%20Learning\/titanic_train.csv')\ntitanic_data.head(2)","168c8e61":"titanic_data.info()","5a7bde39":"#Replace all the missing values in Embarked with mode\ntitanic_data.Embarked = titanic_data.Embarked.fillna(titanic_data['Embarked'].mode()[0])\n#Replace all the missing valeus in Age with Median\nmedian_age = titanic_data.Age.median()\ntitanic_data.Age.fillna(median_age, inplace = True)\n#Drop the column Cabin. There are so manny values are missing\ntitanic_data.drop('Cabin',axis=1,inplace = True)","431b4413":"titanic_data.info()","041cf372":"titanic = titanic_data.drop(['Name','Ticket','Sex','SibSp','Parch','Embarked'], axis = 1)\ntitanic.head(2)","0fc4d6a1":"titanic_data.head(2)","485e25b5":"X = titanic.loc[:,titanic.columns != 'Survived']\ny = titanic.Survived\nX.head()","e0474777":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)","ef85bc66":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)","e16ded26":"y_pred_train = logreg.predict(X_train) \ny_pred_test = logreg.predict(X_test)","2cf407e4":"from sklearn.metrics import accuracy_score\nprint('Accuracy score for test data is:', accuracy_score(y_test,y_pred_test))","946a0262":"titanic_data['FamilySize'] = titanic_data['SibSp']+titanic_data['Parch']+1\ntitanic_data['GenderClass'] = titanic_data.apply(lambda x: 'child' if x['Age']<15 else x['Sex'],axis=1)","766c23e7":"titanic_data.head(2)","b3187472":"titanic_data = pd.get_dummies(titanic_data,columns=['GenderClass','Embarked'],drop_first=True)","99a351b1":"titanic_revisit = titanic_data.drop(['Name','Ticket','Sex','SibSp','Parch'], axis = 1)","c9857407":"sns.pairplot(titanic_data[[\"Fare\",\"Age\",\"Pclass\",\"Survived\"]],vars = [\"Fare\",\"Age\",\"Pclass\"],hue=\"Survived\", dropna=True,markers=[\"o\", \"s\"])\nplt.title('Pair Plot')","12734978":"X_revisit = titanic_revisit.loc[:,titanic_revisit.columns != 'Survived']\ny_revisit = titanic_revisit.Survived\nX_revisit_train, X_revisit_test, y_revisit_train, y_revisit_test = train_test_split(X_revisit, y_revisit, test_size=0.20, random_state=1)\nlogreg.fit(X_revisit_train,y_revisit_train)","60b99a1c":"y_pred_revisit_train = logreg.predict(X_revisit_train) \ny_pred_revisit_test = logreg.predict(X_revisit_test)\nprint('Accuracy score for test data is:', accuracy_score(y_revisit_test,y_pred_revisit_test))","0b2e973d":"| Column Name   | Description                                               |\n| ------------- |:-------------                                            :| \n| PassengerId   | Passenger Identity                                        | \n| Survived      | Whether passenger survived or not                         |  \n| Pclass        | Class of ticket                                           | \n| Name          | Name of passenger                                         |   \n| Sex           | Sex of passenger                                          |\n| Age           | Age of passenger                                          |\n| SibSp         | Number of sibling and\/or spouse travelling with passenger |\n| Parch         | Number of parent and\/or children travelling with passenger|\n| Ticket        | Ticket number                                             |\n| Fare          | Price of ticket                                           |\n| Cabin         | Cabin number                                              |","c9d07c8f":"**Discussion**\n* Generally, It is always better to keep data than to discard it. Sometimes you can drop variables if the data is missing for more than 60% observations but only if that variable is insignificant.\n* If anything missing values less than 2% it is good drop the row values. this percentage may change with respective the dataset.\n* calculating the mean\/median of the non-missing values in a column and then replacing the missing values within each column separately and independently from the others is easy and fast works well with small numerical datasets but when it comes to poor results on encoded categorical features (do NOT use it on categorical features). ","630fcfdb":"## 3. Preprocessing the data","dfe2bb80":"## 2. Read Data","b82dd56d":"Drawing __pair plot__ to know the joint relationship between __'Fare' , 'Age' , 'Pclass' & 'Survived'__","0dc4225a":"### 3.1 **Dealing with missing values**<br\/>\n\n* Dropping\/Replacing missing entries of __Embarked.__\n* Replacing missing values of __Age__ with median values.\n* Dropping the column __'Cabin'__ as it has too many _null_ values.","670a0b68":"- ","1c37a029":"## 5. Logistic Regression","7d432fb0":"Observing the diagonal elements,\n- More people of __Pclass 1__ _survived_ than died (First peak of red is higher than blue)\n- More people of __Pclass 3__ _died_ than survived (Third peak of blue is higher than red)\n- More people of age group __20-40 died__ than survived.\n- Most of the people paying __less fare died__.","ef4a6352":"## 1. Import libraries","e6fe0fa9":"Logistic Regression was used in __biological sciences__ in early twentieth century. It was then used in many social science applications. For instance,\n- The Trauma and Injury Severity Score (TRISS), which is widely used to __predict mortality in injured patients__, was originally developed by Boyd et al. using logistic regression.<br\/> \n- Many other medical scales used to __assess severity__ of a patient have been developed using logistic regression.<br\/>\n- Logistic regression may be used to __predict the risk of developing a given disease__ (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.).<br\/>\n\nNow a days, Logistic Regression have the following applications \n1. Image segementation and  categorization\n2. Geographic image processing\n3. Handwriting recognition\n4. Detection of  myocardinal infarction\n5. Predict whether a person is depressed or not based on a bag of words from corpus. \n\nThe reason why logistic regression is widely used despite of the state of the art of deep neural network is that logistic regression is very __efficient__ and does __not__ require too much __computational resources__, which makes it __affordable__ to run on production.","7895cc4d":"Pretty good Accuracy increased from **0.6703910614525139** to **0.7988826815642458**","aa03ded2":"To apply any machine learning algorithm on your dataset, basically there are 4 steps:\n1. Load the algorithm\n2. Instantiate and Fit the model to the training dataset\n3. Prediction on the test set\n4. Calculating the accuracy of the model\n\nThe code block given below shows how these steps are carried out:<br\/>\n\n``` from sklearn.linear_model import LogisticRegression\n    logreg = LogisticRegression()\n    logreg.fit(X_train, y_train)\n    accuracy_score(y_test,y_pred_test))","13ae539d":"### 5.4 Using the Model for Prediction","c8a43b49":"## 6.Revisit Preprocessing on Data\n","cd13be0e":"### 5.3 Logistic regression in scikit-learn","5ed468d1":"Dummification of **GenderClass** & **Embarked**.","01bf8a1e":"## 4. Splitting X and y into training and test datasets","29f70678":"Logistic regression is a techinque used for solving the __classification problem__.<br\/> And Classification is nothing but a problem of __identifing__ to which of a set of __categories__ a new observation belongs, on the basis of _training dataset_ containing observations (or instances) whose categorical membership is known. <br\/>For example to predict:<br\/> __Whether an email is spam (1) or not (0)__ or,<br\/> __Whether the tumor is malignant (1) or not (0)<br\/>__\nBelow is the pictorial representation of a basic logistic regression model to classify set of images into _happy or sad._","0b545a79":"### 5.1 Introduction to Logistic Regression","794d891c":"### 5.2 Applications of Logistic Regression","e1fbb7df":"# Overview\nI will start with saying thanks to [Lavi Nigam](https:\/\/www.linkedin.com\/in\/lavinigam\/?originalSubdomain=in)\n\nThe goal is to **predict survival** of passengers travelling in **Titanic** using Logistic **regression**.\n- The dataset consists of the information about people boarding the famous Titanic. Various variables present in the dataset includes data of age, sex, fare, ticket etc. \n- The dataset comprises of __891 observations of 12 columns__. Below is a table showing names of all the columns and their description.","c3ff5a26":"### 6.1 **Dealing with feature modification**<br\/>\n\n* Create a new feature **FamilySize** using **SibSp** and **Parch**\n* Segmenting **Sex** column as per Age, Age less than 15 as **Child**, Age greater than 15 as **Males** and **Females** as per their gender."}}