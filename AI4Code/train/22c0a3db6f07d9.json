{"cell_type":{"7421caac":"code","74b2ecaf":"code","3cfb607e":"code","98006056":"code","690f19d8":"code","7e2cc3bc":"code","44dacd0e":"code","d37e3829":"code","9a9137ca":"code","cbc6e096":"code","9f98ff3f":"code","56b4fa87":"code","67fd0835":"code","62650fa4":"code","4d69e5f5":"code","52f7ce5c":"code","d86c0438":"code","ffb3a5be":"code","97044856":"markdown","3d580902":"markdown","740f921a":"markdown","2b62a0d9":"markdown","424a9b49":"markdown","01fc0ea2":"markdown","932056e3":"markdown","1967e5f6":"markdown","0b70044f":"markdown","6ff44e6f":"markdown","3ea02645":"markdown","15944b96":"markdown","5f8cb545":"markdown","da8759fe":"markdown","de45a063":"markdown","17e328ac":"markdown","3015266a":"markdown","7a67b8ad":"markdown","a01dc8f5":"markdown","6d041927":"markdown","0d11ee8b":"markdown","e013fe6d":"markdown","07d9d7fa":"markdown","6b903773":"markdown"},"source":{"7421caac":"# import os\n\n# def download_data(file_names, file_path):\n#     for file_name in file_names:\n#         !kaggle competitions download titanic -f $file_name -p kaggle\/input\/titanic --force\n\n# files = ['train.csv', 'test.csv']\n# raw_path = os.path.join(os.path.pardir,'kaggle\/input\/titanic', 'raw')\n# download_data(files, raw_path)\n","74b2ecaf":"# import pandas as pd\n\n# train_data = pd.read_csv('kaggle\/input\/titanic\/train.csv')\n# submission_data = pd.read_csv('kaggle\/input\/titanic\/test.csv')","3cfb607e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n\n\n\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nsubmission_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n","98006056":"train_data.head()","690f19d8":"females = train_data.loc[train_data.Sex == 'female']['Survived']\nprint('%.2f percent of women survived' % ((sum(females)\/len(females)) * 100))\n\nmales = train_data.loc[train_data.Sex == 'male']['Survived']\nprint('%.2f percent of men survived' % ((sum(males)\/len(males)) * 100))","7e2cc3bc":"import matplotlib.pyplot as plt\nimport numpy as np\n# We love R, use ggplot styling\nplt.style.use('ggplot')\n\n# Segregate the dataset based on age\ny10 = train_data.loc[(train_data.Age <= 10)]['Survived']\ny15 = train_data.loc[(train_data.Age <= 15) & (train_data.Age > 10)]['Survived']\ny30 = train_data.loc[(train_data.Age <= 30) & (train_data.Age > 15)]['Survived']\ny60 = train_data.loc[(train_data.Age <= 60) & (train_data.Age > 30)]['Survived']\ny60p = train_data.loc[train_data.Age > 60]['Survived']\n\n#For histogram of %'\ns = [\n    (sum(y10)\/len(y10)),\n    (sum(y15)\/len(y15)),\n    (sum(y30)\/len(y30)),\n    (sum(y60)\/len(y60)),\n    (sum(y60p)\/len(y60p))\n    ] \ns = np.array(s) * 100\ns_labels = ['0-10', '10-15', '15-30', '30-60', '60+']\n# Print data\nprint('%.2f percent of ages < 10 survived' % ((sum(y10)\/len(y10)) * 100))\nprint('%.2f percent of 10 < ages < 15 survived' % ((sum(y15)\/len(y15)) * 100))\nprint('%.2f percent of 15 < ages < 30 survived' % ((sum(y30)\/len(y30)) * 100))\nprint('%.2f percent of 30 < ages < 60 survived' % ((sum(y60)\/len(y60)) * 100))\nprint('%.2f percent of ages > 60 survived' % ((sum(y60p)\/len(y60p)) * 100))\n\nplt.bar(s_labels, s, color= 'green')\n\nplt.xlabel(\"Age Group\")\nplt.ylabel(\"Survived(%)\")\nplt.title(\"Percent survived by age-group\")\nplt.show()\n\n\n","44dacd0e":"import seaborn as sns \n\nsns.heatmap(train_data.corr(), annot= True, cmap = 'RdYlGn', linewidths=0.2, square=True)\nplt.gcf().set_size_inches(15,8)\nplt.show()\n","d37e3829":"print(train_data.isnull().sum())","9a9137ca":"# Remember to uncomment when submitting\n#train_data = train_data.drop(['Cabin', 'PassengerId', 'Ticket', 'Name'], axis = 1)\n\ntrain_data.sample(5)\ntrain_data = pd.get_dummies(train_data, columns = ['Embarked'], prefix = 'd_')\ntrain_data = pd.get_dummies(train_data, columns = ['Sex'], prefix = 'd_')\ntrain_data = train_data.drop('Name', axis = 1)\ntrain_data = train_data.drop('Cabin', axis = 1)\ntrain_data = train_data.drop('Ticket', axis = 1)\n#train_data = train_data.drop('PassengerId', axis = 1)\n\n\nsubmission_data = pd.get_dummies(submission_data, columns = ['Embarked'], prefix = 'd_')\nsubmission_data = pd.get_dummies(submission_data, columns = ['Sex'], prefix = 'd_')\nsubmission_data = submission_data.drop('Name', axis = 1)\nsubmission_data = submission_data.drop('Cabin', axis = 1)\nsubmission_data = submission_data.drop('Ticket', axis = 1)\n#submission_data = submission_data.drop('PassengerId', axis = 1)\n\n\ntrain_data.sample(5)","cbc6e096":"# Train data with median filled in for NaN\ntrain_data.fillna(train_data['Age'].median(), axis = 1 ,inplace = True)\nsubmission_data.fillna(submission_data['Age'].median(), axis = 1 ,inplace = True)\n\n\n#Train data with Nans dropped\n\ntrain_data_drop_na = train_data.dropna()\nsubmission_data_drop_na = submission_data.dropna()\n\n\nprint(train_data.isnull().sum())","9f98ff3f":"sns.heatmap(train_data.corr(), annot= True, cmap = 'RdYlGn', linewidths=0.2, square=True)\nplt.gcf().set_size_inches(15,8)\nplt.show()","56b4fa87":"from sklearn.model_selection import train_test_split\n\nX = train_data.drop('Survived', axis = 1)\ny = train_data['Survived']\n\nX_nnan = train_data_drop_na.drop('Survived', axis = 1)\ny_nann = train_data_drop_na['Survived']\n\ntrain_data_drop_na = train_data.dropna()\nsubmission_data_drop_na = submission_data.dropna()\n\n#splits = []\n#for i in [0.1,0.2,0.3,0.4,0.5,0.6]:   \n    #splits.append([train_test_split(X, y, test_size=i, random_state= 115)])\n    #splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state= 115)\n\nX_nann_train, X_nann_test, y_nann_train, y_nann_test = train_test_split(X_nnan, y_nann, test_size=0.1, random_state= 115)\nprint(X_train.head())\nprint(X_test.head())","67fd0835":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model objects\nKNN = KNeighborsClassifier()\nSGD = SGDClassifier()\nRFC = RandomForestClassifier()\n","62650fa4":"from sklearn.model_selection import GridSearchCV\n\n\n\n\nKNN_Gs = GridSearchCV(\n    estimator = KNN,\n    param_grid = {  'n_neighbors': [3,5,11,19],\n                    'weights': ['uniform', 'distance'], \n                    'metric': ['euclidean', 'manhattan']},  \n    scoring = 'roc_auc',\n    n_jobs = 10,\n    cv = 5, \n    refit = True\n)\nSGD_Gs = GridSearchCV(\n    estimator = SGD,\n    param_grid = {  'loss': ['hinge', 'log', 'modified_huber'],\n                    'penalty': ['l2', 'l1']},\n    n_jobs = 10,\n    cv = 3, \n    refit = True\n)\nRFC_Gs = GridSearchCV(\n    estimator = RFC,\n    param_grid = {'max_depth': [2, 4, 8, 15, 20], 'max_features': ['auto', 'sqrt', 'log2'], \\\n                  'n_estimators': [20,40,100,200, 400, 600],\n                  'criterion': ['gini', 'entropy']},\n    \n    n_jobs = 10,\n    cv = 3, \n    refit = True\n)\n\nKNN_Gs_nann = GridSearchCV(\n    estimator = KNN,\n    param_grid = {  'n_neighbors': [3,5,11,19],\n                    'weights': ['uniform', 'distance'], \n                    'metric': ['euclidean', 'manhattan']},  \n    scoring = 'roc_auc',\n    n_jobs = 10,\n    cv = 5, \n    refit = True\n)\nSGD_Gs_nann = GridSearchCV(\n    estimator = SGD,\n    param_grid = {  'loss': ['hinge', 'log', 'modified_huber'],\n                    'penalty': ['l2', 'l1']},\n    n_jobs = 10,\n    cv = 3, \n    refit = True\n)\nRFC_Gs_nann = GridSearchCV(\n    estimator = RFC,\n    param_grid = {'max_depth': [2, 4, 8, 15, 20], 'max_features': ['auto', 'sqrt', 'log2'], \\\n                  'n_estimators': [20,40,100,200, 400, 600],\n                  'criterion': ['gini', 'entropy']},\n    \n    n_jobs = 10,\n    cv = 3, \n    refit = True\n)","4d69e5f5":"KNN_Gs.fit(X_train, y_train)\nSGD_Gs.fit(X_train, y_train)\nRFC_Gs.fit(X_train, y_train)\n\n\n# Printing the best estimator\nprint(KNN_Gs.best_estimator_)\nprint(SGD_Gs.best_estimator_)\nprint(RFC_Gs.best_estimator_)\n\n# Adding models to list\n\nmodels = [KNN_Gs, SGD_Gs, RFC_Gs]\n\nKNN_Gs_nann.fit(X_nann_train, y_nann_train)\nSGD_Gs_nann.fit(X_nann_train, y_nann_train)\nRFC_Gs_nann.fit(X_nann_train, y_nann_train)\n\nmodels_nann = [KNN_Gs_nann, SGD_Gs_nann, RFC_Gs_nann]\n# Printing the best estimator\nprint(KNN_Gs.best_estimator_)\nprint(SGD_Gs.best_estimator_)\nprint(RFC_Gs.best_estimator_)\n\nprint(KNN_Gs_nann.best_estimator_)\nprint(SGD_Gs_nann.best_estimator_)\nprint(RFC_Gs_nann.best_estimator_)\n\n# Adding models to list\n\nmodels = [KNN_Gs, SGD_Gs, RFC_Gs]","52f7ce5c":"\nfrom sklearn.model_selection import cross_val_score, KFold\nmodels_score = []\nmodels_nann_score =  []\nfor model in models:\n    models_score.append(model.score(X_test, y_test) * 100)\n    \n    \nfor model in models_nann:\n    models_nann_score.append(model.score(X_nann_test, y_nann_test) * 100)\n\n    print(models_score)\n    print('\\n')\n    print(models_nann_score)\n","d86c0438":"x = ['KNN', 'SGD', 'RFC']\nplt.bar(x, models_score)\nplt.xlabel('Model')\nplt.ylabel('Accuracy (%)')\nplt.show()\n\nx = ['KNN_nan', 'SGD_nan', 'RFC_nan']\nplt.bar(x, models_nann_score)\nplt.xlabel('Model')\nplt.ylabel('Accuracy (%)')\nplt.show()","ffb3a5be":"predictions = RFC_Gs_nann.predict(submission_data)\npd.DataFrame({'PassengerId' : submission_data.PassengerId, 'Survived': predictions}).to_csv('submission.csv', index = False)\n\n#Preview the submission\npd.read_csv('submission.csv').head()","97044856":"#### Old vs. Young\n\nI wan't to know the percentage of people that survived in these age groups\n\n* 10 years and younger\n* 15 years and younger\n* 30 years\n* 60 years\n* 60+ years\n\nLet's begin by counting the number of passangers that belong to each group","3d580902":"As expected more women have survived the disaster, but there are also other interesting columns to take a look at before moving on.\n\n\n","740f921a":"### Creating our models\n","2b62a0d9":"### Looking at the raw data\n\nWe begin by looking at the head of the training dataset we've been given","424a9b49":"### Training our optimized hyperparameter models on train data\n\n","01fc0ea2":"## Exploratory data analysis","932056e3":"Let's check if the heatmap has changed any with the inclusion of our dummy variables","1967e5f6":"## Importing and loading from kaggle","0b70044f":"Based on these observations we will clean up our data a bit, and procede with building our model\n","6ff44e6f":"### Scoring the models","3ea02645":"### Generating grid-search objects","15944b96":"## Creating our Models","5f8cb545":"# Titanic","da8759fe":"This gives us a pretty good idea that there is some sort of correlation between age and survival rate. We can get a look at this correlation using Seaborns heatmap function\n","de45a063":"## Evaluation of our models","17e328ac":"This downloads the files from kaggle to my local machine in a relative directory titled .\/kaggle (to make it easier to upload my notebook to kaggle when I am finished) allowing me to work in a more comfortable manner from my machine rather than the kaggle interface\n<br>\nNow we can load these files into some variables to begin data analysis","3015266a":"### Splitting our data into test\/train","7a67b8ad":"It appears that cain Q had very little overall affect on survivability while C, and S affected it somewhat","a01dc8f5":"#### Dropping unrelated data, Filling NaNs, and getting dummies for embarked\n","6d041927":"#### Females vs. Males\n\nIn spirit of following the tutorial given on kaggle.com, I will continue by taking a look at the rate of survival for both men and women. Considering the sentiment of 'saving the women and children first' I'd expect to see that more women survived the ordeal. \n\n\nWe can verify this by doing simple algebra on the respective columns","0d11ee8b":"#### Observations\nIt is clear that an increase in age in general has a negative association with survival rate, considering this there is also some other correlations that are interesting. Namely:\n* Fare - Highest positive correlation to survival\n* Pclass - Highest negative correlation to survival","e013fe6d":"## Generating Submission\n\nNow that we now RFC is the best model of the three we have selected, we will use it to generate our submission file for kaggle.","07d9d7fa":"Primarly we are interested in the following features:\n* Sex\n* Age\n* Pclass\n* Fare\n\nand a way to handle missing data which we will look into moving forward","6b903773":"## Importing and loading data to my local machine"}}