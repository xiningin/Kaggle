{"cell_type":{"b760c239":"code","4ced6c23":"code","523034b2":"code","c948ea2e":"code","b8bdf934":"code","ba682afb":"code","d2818b86":"code","e316c170":"code","b224cbe5":"code","04282350":"code","2de5f859":"code","2d58926a":"code","51bf7680":"code","028c2c65":"code","ff20ff16":"code","6e462cb0":"code","695c1b55":"code","0262b015":"code","3b0ba2c4":"code","4b99c31b":"code","7df69c08":"code","2adf0328":"code","ee91240e":"code","44202e24":"code","bfbf7b7b":"code","12732e62":"code","36616216":"code","2fd62f9b":"code","133257e3":"code","cd3af66a":"code","f930c89e":"code","6f426403":"code","406ed1dd":"code","4e636258":"code","3117b2f6":"code","ac67cff0":"markdown","da06e720":"markdown","85ce6d7d":"markdown","95b2c923":"markdown","aada0270":"markdown","7a39cf2e":"markdown","6e84a069":"markdown","33b0382f":"markdown","4f422ae8":"markdown","ef38553c":"markdown","084cf46b":"markdown","ff007766":"markdown","8e70b61b":"markdown","7587a6f5":"markdown","bd692d8f":"markdown","4f9f3c38":"markdown","3431243f":"markdown","6a75a283":"markdown","3212f5ef":"markdown","07f06e60":"markdown","1053a2eb":"markdown","ffe4c2a0":"markdown","bdb08196":"markdown","70175b19":"markdown","a6e8688b":"markdown","726d9593":"markdown","a62999df":"markdown","d5350833":"markdown","aa9685ae":"markdown"},"source":{"b760c239":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lofo import LOFOImportance, Dataset, plot_importance\n%matplotlib inline\nfrom sklearn.metrics import make_scorer, mean_absolute_error, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV\nimport itertools\nimport optuna\nimport xgboost as xgb","4ced6c23":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ncombine = [train_data, test_data]","523034b2":"train_data.head()","c948ea2e":"print(train_data.info())\n\nprint('\\nNumber of rows : ', train_data.shape[0])\nprint('Number of columns : ', train_data.shape[1])\n\nprint('\\nTrain columns with null values: \\n', train_data.isnull().sum())\n\nprint('\\nNumber of total people who survived or dead ( 0 : Dead, 1: Survived )\\n', train_data['Survived'].value_counts().apply(lambda x:f'{x} ({x*100\/len(train_data):0.2f}%)'))\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\nprint(\"\\n% of women who survived:\", rate_women)\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\nprint(\"\\n% of men who survived:\", rate_men)","b8bdf934":"sns.set_style('whitegrid')\ns = sns.countplot(x='Survived', hue='Sex', data=train_data, palette='rainbow')\ns.set_title(\"Number of survived or dead categorized by gender\")\nplt.show()\nprint(\"> We see that survival rate of women is more than men\")","ba682afb":"sns.set_style('whitegrid')\ns = sns.countplot(x='Pclass', hue='Survived', data=train_data, palette='rainbow')\ns.set_title(\"\\t\\t\\t\\t\\t\\tNumber of survived or dead categorized by passenger class\")\nplt.show()\nprint(\"> We see that survival rate of 1st and 2nd passenger class is higher than 3rd class\")","d2818b86":"sns.set_style('whitegrid')\nsns.catplot(x='Pclass', col='Embarked', kind='count', data=train_data)\nprint(\"Number of people categorized by port of embarkation\")\nplt.show()\nprint(\"Hint : C = Cherbourg, Q = Queenstown, S = Southampton \")\nprint(\"> We see that most of people come from Southampton and chose 3rd class\")","e316c170":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,6), constrained_layout=True, sharey=True)\n\n#Graphs\nage_surv = sns.histplot(data=train_data[train_data['Survived']==1],x='Age',bins=30,ax=axes[1])\nage_dead = sns.histplot(data=train_data[train_data['Survived']==0],x='Age',bins=30,ax=axes[0])\n\nfigtitle = fig.suptitle('Survival by Age',fontsize=24)\naxeszero_ylabel = axes[0].set_ylabel('Count',size=14)\naxeszero_yticklabel = axes[0].set_yticklabels([int(x) for x in age_dead.get_yticks()],size=12)\naxeszero_title = axes[0].set_title('Did Not Survive', fontsize=14)\naxeszero_xticklabel = axes[0].set_xticklabels([int(x) for x in age_dead.get_xticks()],size=12) \naxeszero_xlabel = axes[0].set_xlabel('Age',size=14)\naxesone_title = axes[1].set_title('Survived', fontsize=14)\naxesone_xticklabel = axes[1].set_xticklabels([int(x) for x in age_surv.get_xticks()],size=12)\naxesone_xlabel = axes[1].set_xlabel('Age',size=14)\nplt.show()\nprint(\"> We see that between 20-30 ages almost same probability of survived and didn't survived\")","b224cbe5":"#SibSp - Survived\nsns.set_style('whitegrid')\ng = sns.catplot(x = \"SibSp\", y = \"Survived\", data = train_data, kind = \"bar\", height= 9)\ng.set_ylabels(\"Probability of Survival\")\nplt.show()\nprint(\"Hint : SibSp = number of siblings \/ spouses \")","04282350":"#ParCh - Survived\nsns.set_style('whitegrid')\ng = sns.catplot(x = \"Parch\", y = \"Survived\", data = train_data, kind = \"bar\", height = 7)\ng.set_ylabels(\"Probability of Survival\")\nplt.show()\nprint(\"Hint : Parch = number of parents \/ children \")\nprint(\"> After we see that from last two visualization, small families have more chance to survive\")","2de5f859":"plt.style.use(\"seaborn-whitegrid\")\nnum_col = [\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\ns = sns.heatmap(train_data[num_col].corr(), annot = True, fmt = \".2f\")\ns.set_title(\"Correlation between numerical features and target\")\nplt.show()\nprint(\"> We see that these features have not more affect to target\")","2d58926a":"print('\\nTrain data with null values:')\ntrain_data.isnull().sum()\n\nprint('\\nTest data with null values:')\ntest_data.isnull().sum()","51bf7680":"for dataset in combine:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# SITUATION OF TITLE COLUMN AND NUMBER OF EACH TITLE\ntrain_data['Title'].value_counts()","028c2c65":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 4, \"Rev\": 4, \"Col\": 4, \"Major\": 4, \"Mlle\": 4,\"Countess\": 4,\n                 \"Ms\": 4, \"Lady\": 4, \"Jonkheer\": 4, \"Don\": 4, \"Dona\" : 4, \"Mme\": 4,\"Capt\": 4,\"Sir\": 4 }\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","ff20ff16":"train_data['family_size'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['family_size'] = test_data['SibSp'] + test_data['Parch'] + 1","6e462cb0":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","695c1b55":"#CABIN \nfor dataset in combine:\n    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","0262b015":"Pclass1 = train_data[train_data['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train_data[train_data['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train_data[train_data['Pclass']==3]['Embarked'].value_counts()\n\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","3b0ba2c4":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","4b99c31b":"sns.factorplot(data = train_data , x = 'Pclass' , y = 'Age', kind = 'box')","7df69c08":"def AgeImpute(df):\n    Age = df[0]\n    Pclass = df[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1: return 37\n        elif Pclass == 2: return 29\n        else: return 24\n    else:\n        return Age\n\n# Age Impute\ntrain_data['Age'] = train_data[['Age' , 'Pclass']].apply(AgeImpute, axis = 1)\ntest_data['Age'] = test_data[['Age' , 'Pclass']].apply(AgeImpute, axis = 1)","2adf0328":"# FARE \ntest_data[\"Fare\"] = test_data[\"Fare\"].fillna(test_data[\"Fare\"].median())","ee91240e":"features_drop = ['Cabin', 'Ticket', 'Name', 'PassengerId']\ntrain_data.drop(features_drop, axis=1, inplace=True)\ntest_data.drop(features_drop, axis=1, inplace=True)","44202e24":"print(\"AFTER FEATURE ENGINEERING\\n\")\nprint('Train data with null values:\\n', train_data.isnull().sum())\nprint('\\nTest data with null values:\\n', test_data.isnull().sum())","bfbf7b7b":"X = train_data.drop(['Survived'], axis = 1)\ny = train_data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = .3,\n                                                    random_state = 5,\n                                                   stratify = y)\n\nmodel = XGBClassifier(n_estimators=1000, learning_rate=0.05, max_depth = 10)\n# model.fit(X_train, y_train)","12732e62":"skf = StratifiedKFold(n_splits=5)\n\ncross_val_acc = []\n\nX_train = X_train.values\ny_train = y_train.values\n\nfor train, test in skf.split(X_train, y_train):\n    model.fit(X_train[train], y_train[train])\n    cross_val_acc.append(model.score(X_train[test], y_train[test]))\n    \n\nprint(\"All of means\")\nprint(cross_val_acc)\n\nprint(\"Mean of scores\")\nprint(np.mean(cross_val_acc))","36616216":"gbm = lgb.LGBMClassifier(objective='binary')\n\ngbm.fit(X_train, y_train, eval_set = [(X_test, y_test)],\n        early_stopping_rounds=20,\n        verbose=10\n)\n\npre = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n\nprint('score', round(accuracy_score(y_test, pre)*100,2), '%')","2fd62f9b":"# define the validation scheme\ncv = KFold(n_splits=4, shuffle=False, random_state=0)\nscorer = make_scorer(mean_absolute_error, greater_is_better=False)\n\n# define the binary target and the features\ntarget = \"Survived\"\nfeatures = [col for col in train_data.columns if col != target]\ndataset = Dataset(df=train_data, target=\"Survived\", features=features)\n# define the validation scheme and scorer. The default model is LightGBM\nlofo_imp = LOFOImportance(dataset, scoring=scorer, model=model, cv=cv)\n\n# get the mean and standard deviation of the importances in pandas format\nimportance_df = lofo_imp.get_importance()\n\n# plot the means and standard deviations of the importances\nplot_importance(importance_df)","133257e3":"# A parameter grid for XGBoost\nparams = {\n    'n_estimators': [100, 200, 300, 400, 500, 1000],\n    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n    'max_depth': [3, 5, 7, 9, 11]\n}\n\n# define the validation scheme\ncv = KFold(n_splits=3, shuffle=True, random_state=None)\nscorer = make_scorer(f1_score, greater_is_better=True)\n\ngrid = GridSearchCV(estimator=model, \n                    param_grid=params, \n                    scoring=scorer, \n                    n_jobs=-1, \n                    cv=cv, \n                    verbose=1 )\ngrid.fit(X, y)\nprint('\\n Best estimator:')\nprint(grid.best_estimator_)\nprint('\\n Best score:')\nprint(grid.best_score_ * 2 - 1)\nprint('\\n Best parameters:')\nprint(grid.best_params_)","cd3af66a":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","f930c89e":"new_model = XGBClassifier()\nnew_model.set_params(**grid.best_params_)\n\nnew_model.fit(X_train, y_train)\n\ny_pred = grid.best_estimator_.predict(X)\ncm = confusion_matrix(train_data['Survived'], y_pred)\nnp.set_printoptions(precision=2)\n\nclass_names = ['Dead', 'Survived']\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cm, classes=class_names,\n                      title='Confusion matrix')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cm, classes=class_names, normalize=True, \n                      title='Normalized confusion matrix')","6f426403":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_test, label=y_test)\n\ndef objective(trial):\n    params = {\n        'booster':trial.suggest_categorical('booster', ['gbtree', 'dart', 'gblinear']),\n        'learning_rate':trial.suggest_loguniform(\"learning_rate\", 0.01, 0.1),\n        'max_depth':trial.suggest_int(\"max_depth\", 3, 11),\n        'subsample':trial.suggest_uniform(\"subsample\", 0.0, 1.0),\n        'colsample_bytree':trial.suggest_uniform(\"colsample_bytree\", 0.0, 1.0),\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    # accuracy = accuracy_score(y_test, pred_labels)\n    f1_scores = f1_score(y_test, pred_labels)\n    return f1_scores\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=200, timeout=600)","406ed1dd":"import sys\n!{sys.executable} -m pip install xgboost","4e636258":"new_params = study.best_params\n\nnew_model2 = XGBClassifier(**new_params)\nnew_model2.fit(X, y)\npreds = new_model2.predict(X_test)\n\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_test, preds))\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}","3117b2f6":"all_accuracies = cross_val_score(estimator=new_model, X=X_train, y=y_train, cv=3)\n\nprint(\"\\n\\nAccuracy is the measure of how often the model is correct.\\n\")\n\nprint(\"All of accuracies are \\n\", all_accuracies)\nprint(\"\\nMean of accuracies \\n\", all_accuracies.mean())\nprint(\"\\nStandart deviation of accuracies \\n %\", ( all_accuracies.std() * 100))","ac67cff0":"**Read 'Training' and 'Test' data files**","da06e720":"> Depends on these values mapping for title column\n* Mr : 0\n* Miss : 1\n* Mrs: 2\n* Master: 3\n* Others : 4","85ce6d7d":"**AGE MISSING VALUES**","95b2c923":"# **GRID SEARCH**","aada0270":"> Cabin column has too much missing values. So mapping for has cabin or not seems better solution. Because prediction of these missing values looks impossible and if we try to fill up them, it can directly affect learning of model as positively or negatively. So it's unguessable.","7a39cf2e":"# **OPTUNA HYPERPARAMETER SEARCH**","6e84a069":"**FEATURE IMPORTANCE BY LOFO**","33b0382f":"## **TEST DATA - PREDICTION**","4f422ae8":"# DATA VISUALIZATION","ef38553c":"**NAME - TITLE SPLIT and MAPPING**","084cf46b":"**DROP UNNECESSARY COLUMNS**","ff007766":"**XGBoost + k-fold**","8e70b61b":"> There is just 1 missing value in test_data so median value of fare class is acceptable.","7587a6f5":"## ANALSYIS OF TITANIC DATASET","bd692d8f":"> It's calculated by addition of SibSp and Parch number ( + 1 means that family at least one member )","4f9f3c38":"**FAMILY SIZE**","3431243f":"**TRAIN MODEL WITH BEST PARAMETERS and SHOW CONFUSION MATRIX**","6a75a283":"## MODELING XGBOOST and LIGHTGBM\n\n**XGBOOST PREDICTION**\n","3212f5ef":"**How data looks**","07f06e60":"**GENDER MAPPING**","1053a2eb":"> We can fill up by their median values because there are not too much missing values","ffe4c2a0":"**HAS CABIN**","bdb08196":"# FEATURE ENGINEERING","70175b19":"* more than 50% of 1st class are from S embark\n* more than 50% of 2nd class are from S embark\n* more than 50% of 3rd class are from S embark\n\n> So we can fill up using S and then mapping for each letter because it has only 2 missing values as mentioned beginning of analysis.\n* S : 0 ( S : Southampton )\n* C : 1 ( C : Cherbourg   )\n* Q : 2 ( Q : Queentown   )","a6e8688b":"**Some information about data**","726d9593":"**EMBARKED MAPPING**","a62999df":"**FARE CLASS MISSING VALUE**","d5350833":"> 0 assigned to female and 1 assigned to the male person","aa9685ae":"**LIGHTGBM PREDICTION**"}}