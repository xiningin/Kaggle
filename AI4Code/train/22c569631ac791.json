{"cell_type":{"32788633":"code","dd9da786":"code","4cb2a406":"code","5cb0a54f":"code","9aea699b":"code","e8688f14":"code","6d4e1e00":"code","71cf2c0f":"code","88b11d65":"code","db3fbb57":"code","48029e05":"code","40feef7a":"code","23da1008":"code","c122f59e":"code","1784c272":"code","0598cee4":"code","b9a5cf6d":"code","eae54d2b":"code","f4bf4ded":"code","f5773959":"code","46ad11bd":"code","2d248ed9":"code","d0529474":"code","0103978e":"code","b978d083":"code","87525074":"code","da122b2e":"code","5dd199c2":"code","dedb2f83":"code","c32d4586":"code","6b0623fa":"code","0beabc93":"code","6cfa0633":"code","03790ee8":"code","da016a33":"code","35c066ac":"code","6b005520":"code","1f660c45":"code","fc167cc5":"code","f4d0c92c":"code","73aa028b":"code","c32781e8":"code","80a9e371":"code","e1b68faa":"code","5ba6a0e4":"code","c0f06aeb":"code","ff1e5015":"code","6255e869":"code","f27de372":"code","7b89c443":"code","700a3196":"code","dd4b866c":"code","33515121":"code","3dca7a92":"code","6e5b35d0":"code","6c1478ed":"code","9499f6c3":"code","97b22039":"code","c9f05772":"code","2602c3e8":"code","08402cc6":"code","536765ec":"code","57ab178a":"code","a1f1e18a":"code","0c215d9b":"code","6ece58d7":"code","1569ec97":"code","9b89f681":"code","ead5d303":"code","f47b8046":"code","bb280fa5":"code","1aa0e90e":"code","3cbbf2f4":"code","674c4867":"code","2ba9641c":"code","a39d247b":"code","df978524":"code","60d7431c":"code","09a6388a":"code","5f6b313a":"code","b257c2dd":"code","64a94120":"code","26466aa5":"code","fd1948fa":"code","5ea41ac7":"code","48e40271":"code","aa923141":"code","2a3e90f5":"code","54a2f073":"code","3a1c7fb5":"code","d357be3f":"code","4feb9c80":"code","e5dfabe8":"code","9a6e334c":"code","9bdbdd7a":"code","f5717179":"code","13d1ca8b":"code","1ea6982c":"code","cfd46069":"code","84f7d936":"code","e99d7e4e":"code","fd0fb921":"code","4808ed57":"markdown","89145156":"markdown","de83bd52":"markdown","c52de2ba":"markdown","1fcc6a9c":"markdown","44a83659":"markdown","be0e7f36":"markdown","606d62f2":"markdown","18991673":"markdown","8c7a6835":"markdown","f3608b42":"markdown","a3592ed5":"markdown","70e96942":"markdown","cc387c06":"markdown","875436cf":"markdown","2ca493f9":"markdown","7b0cabfc":"markdown","fbe505e8":"markdown","21f130cf":"markdown","454653d6":"markdown","a477873c":"markdown","14f4f1aa":"markdown","0cb46888":"markdown","7cef3269":"markdown","bae7aae1":"markdown","62d705de":"markdown","c1e219c3":"markdown"},"source":{"32788633":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport os\nimport random\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax\nfrom tensorflow.keras import regularizers","dd9da786":"np.random.seed(42)","4cb2a406":"os.listdir('\/kaggle\/input\/ckplus')","5cb0a54f":"DATADIR = r'\/kaggle\/input\/ckplus\/CK+48'","9aea699b":"CATEGORIES = os.listdir(DATADIR)\nCATEGORIES","e8688f14":"def load_data():\n    DATADIR = r'\/kaggle\/input\/ckplus\/CK+48'\n    data = []\n    # loading training data\n    for category in CATEGORIES:\n        # create path to image of respective expression\n        path = os.path.join(DATADIR, category)\n        # get the classification  for each expression \n        class_num = CATEGORIES.index(category)\n\n        for img in tqdm(os.listdir(path)):\n            img_array = cv2.imread(os.path.join(path, img), 0)\n            data.append([img_array, class_num])\n            \n    return data","6d4e1e00":"data = load_data()","71cf2c0f":"len(data)","88b11d65":"L = 4\nW = 4\nfig, axes = plt.subplots(L, W, figsize = (15,15))\naxes = axes.ravel()\n\nfor i in range(0, L * W):  \n    sample = random.choice(data)\n    axes[i].set_title(\"Expression = \"+str(CATEGORIES[sample[1]]))\n    axes[i].imshow(sample[0], cmap='gray')\n    axes[i].axis('off')\nplt.subplots_adjust(wspace=0.5)","db3fbb57":"X = np.array([ x[0] for x in data])\ny = np.array([Y[1] for Y in data])","48029e05":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = True)","40feef7a":"print(\"X_train shape: \", X_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"-------------------------------\")\nprint(\"X_test shape: \", X_test.shape)\nprint(\"y_test shape: \", y_test.shape)","23da1008":"# reshaping y_train and y_test\ny_train = np.reshape(y_train, (len(y_train),1))\ny_test  = np.reshape(y_test , (len(y_test ),1))\n\nprint(\"After reshaping\")\nprint(\"y_train shape: \", y_train.shape)\nprint(\"y_test shape: \", y_test.shape)","c122f59e":"X_train_Gabor  = X_train\nX_test_Gabor = X_test","1784c272":"X_train = np.expand_dims(X_train, axis=3)\nX_test = np.expand_dims(X_test, axis=3)\n\nprint(\"After adding color channel\")\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)","0598cee4":"X_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","b9a5cf6d":"y_train[0]","eae54d2b":"y_train_SVM = y_train\ny_test_SVM = y_test\n\ny_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)","f4bf4ded":"y_train[0]","f5773959":"y_train.shape, y_test.shape","46ad11bd":"from skimage.transform import resize\nfrom skimage.feature import hog","2d248ed9":"plt.figure(figsize=(10, 10))\n\n\nplt.subplot(1,2,1)\nimg = random.choice(X_train)\n# first image needs to be resized before passing it to HOG descriptor\nresized_img =  resize(img, (128, 64))\nplt.title(\"Original image\")\nplt.imshow(img, cmap='gray')\nfd, hog_image = hog(\n    resized_img, \n    orientations=9, \n    pixels_per_cell=(8, 8),\n    cells_per_block=(2, 2), \n    visualize=True, \n    multichannel=True\n)\nplt.subplot(1,2,2)\nplt.title(\"HOG\")\nplt.imshow(resize(hog_image, (48, 48)), cmap='gray')\nplt.axis('off')","d0529474":"def Create_Hog_features(data):\n    Feature_data = np.zeros((len(data),48,48))\n\n    for i in range(len(data)):\n        img = data[i]\n        resized_img = resize(img, (128, 64))\n        fd, hog_image = hog(\n            resized_img, \n            orientations=9, \n            pixels_per_cell=(8, 8),\n            cells_per_block=(2, 2), \n            visualize=True, \n            multichannel=True\n        )\n        Feature_data[i] = resize(hog_image, (48, 48))\n    return Feature_data","0103978e":"Feature_X_train = Create_Hog_features(X_train)\nFeature_X_train.shape","b978d083":"plt.imshow(random.choice(Feature_X_train), cmap='gray')\nplt.axis('off')","87525074":"# doing same for test data \nFeature_X_test = Create_Hog_features(X_test)\n\nFeature_X_test.shape","da122b2e":"plt.imshow(random.choice(Feature_X_test), cmap='gray')\nplt.axis('off')","5dd199c2":"# Again adding color channel as it got removed while converting img to hog img\nX_train_HOG = np.expand_dims(Feature_X_train, axis=3)\nX_test_HOG = np.expand_dims(Feature_X_test, axis=3)\n\nprint(\"After adding color channel\")\nprint(\"X_train_HOG shape: \", X_train_HOG.shape)\nprint(\"X_test_HOG shape: \", X_test_HOG.shape)","dedb2f83":"print(\"X_train_HOG shape: \", X_train_HOG.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"X_test_HOG shape: \", X_test_HOG.shape)\nprint(\"y_test shape: \", y_test.shape)","c32d4586":"# train_datagen = ImageDataGenerator(\n#     rotation_range=25, width_shift_range=0.1,\n#     height_shift_range=0.1, shear_range=0.2, \n#     zoom_range=0.2,horizontal_flip=True, \n#     fill_mode=\"nearest\"\n# )","6b0623fa":"def create_model(input_shape=None):\n    if input_shape is None :\n        input_shape=(48,48,1)\n\n    model = Sequential()\n    model.add(Conv2D(6, (5, 5), input_shape=input_shape, padding='same', activation = 'relu'))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(16, (5, 5), padding='same', activation = 'relu'))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n    model.add(MaxPool2D(pool_size=(2, 2)))\n\n    model.add(Flatten())\n    model.add(Dense(128, activation = 'relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(7, activation = 'softmax'))\n    \n    return model ","0beabc93":"es = EarlyStopping(\n    monitor='val_accuracy', min_delta=0.0001, patience=10, verbose=2,\n    mode='max', baseline=None, restore_best_weights=True\n)\nlr = ReduceLROnPlateau(\n    monitor='val_accuracy', factor=0.1, patience=5, verbose=2,\n    mode='max', min_delta=1e-5, cooldown=0, min_lr=0\n)\n\ncallbacks = [es, lr]","6cfa0633":"HOG_model = create_model()","03790ee8":"HOG_model.summary()","da016a33":"HOG_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam' )","35c066ac":"HOG_history = HOG_model.fit(X_train_HOG, y_train, batch_size=8 , epochs=50, validation_data = (X_test_HOG, y_test), callbacks = [callbacks])","6b005520":"def plot_performance(history):\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(2, 1, 1)\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='val')\n\n    plt.legend()\n    plt.grid()\n    plt.title('train and val loss evolution')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='val')\n\n    plt.legend()\n    plt.grid()\n    plt.title('train and val accuracy')","1f660c45":"plot_performance(HOG_history)","fc167cc5":"acc = []","f4d0c92c":"HOG_acc = HOG_model.evaluate(X_test_HOG, y_test, verbose = 0)[1]\nacc.append(HOG_acc)\nprint(\"HOG Accuracy :\",HOG_model.evaluate(X_test_HOG, y_test, verbose = 0)[1])","73aa028b":"HOG_model.save('HOG_model.h5')","c32781e8":"def Binarypattern(im):                               # creating function to get local binary pattern\n    img= np.zeros_like(im)\n    n=3                                              # taking kernel of size 3*3\n    for i in range(0,im.shape[0]-n):                 # for image height\n        for j in range(0,im.shape[1]-n):               # for image width\n            x  = im[i:i+n,j:j+n]                     # reading the entire image in 3*3 format\n            center       = x[1,1]                    # taking the center value for 3*3 kernel\n            img1        = (x >= center)*1.0          # checking if neighbouring values of center value is greater or less than center value\n            img1_vector = img1.T.flatten()           # getting the image pixel values \n            img1_vector = np.delete(img1_vector,4)  \n            digit = np.where(img1_vector)[0]         \n            if len(digit) >= 1:                     # converting the neighbouring pixels according to center pixel value\n                num = np.sum(2**digit)              # if n> center assign 1 and if n<center assign 0\n            else:                                    # if 1 then multiply by 2^digit and if 0 then making value 0 and aggregating all the values of kernel to get new center value\n                num = 0\n            img[i+1,j+1] = num\n    return(img)","80a9e371":"plt.figure(figsize = (10,10))\n\nplt.subplot(1,2,1)\nimg = random.choice(X_train)\nplt.title(\"Original  image\")\nplt.imshow(img, cmap='gray')\n\nplt.subplot(1,2,2)\nplt.title(\"LBP\")\nimgLBP=Binarypattern(img)             # calling the LBP function using gray image\nplt.imshow(imgLBP, cmap='gray')\nplt.axis('off')","e1b68faa":"X_train.shape","5ba6a0e4":"def create_LBP_features(data):\n    Feature_data = np.zeros(data.shape)\n\n    for i in range(len(data)):\n        img = data[i]\n        imgLBP=Binarypattern(img)  \n        Feature_data[i] = imgLBP\n    \n    return Feature_data","c0f06aeb":"Feature_X_train = create_LBP_features(X_train)","ff1e5015":"Feature_X_train.shape","6255e869":"img = random.choice(Feature_X_train)\nplt.imshow(img, cmap='gray')","f27de372":"Feature_X_test = create_LBP_features(X_test)\nFeature_X_test.shape","7b89c443":"img = random.choice(Feature_X_test)\nplt.imshow(img, cmap='gray')","700a3196":"LBP_model = create_model()\nLBP_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam' )","dd4b866c":"LBP_history = LBP_model.fit(Feature_X_train, y_train, batch_size=8 , epochs=50, validation_data = (Feature_X_test, y_test) ,callbacks = [callbacks])","33515121":"plot_performance(LBP_history)","3dca7a92":"LBP_acc = LBP_model.evaluate(Feature_X_test, y_test, verbose = 0)[1]\nacc.append(LBP_acc)\nprint(\"LBP Accuracy :\",LBP_model.evaluate(Feature_X_test, y_test, verbose = 0)[1])","6e5b35d0":"LBP_model.save('LBP_model.h5')","6c1478ed":"L = 3\nW = 3\nfig, axes = plt.subplots(L, W, figsize = (15,15))\naxes = axes.ravel()\n\nfor i in range(0, L * W):  \n    sample = random.choice(data)\n    image8bit = cv2.normalize(sample[0], None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n    sift = cv2.SIFT_create()\n    kp, des = sift.detectAndCompute(image8bit,None)\n\n    img = cv2.drawKeypoints(image=image8bit, outImage=sample[0], keypoints = kp, flags = 4, color = (255, 0, 0))\n    axes[i].set_title(\"Expression = \"+str(CATEGORIES[sample[1]]))\n    axes[i].imshow(img, cmap='gray')\n    axes[i].axis('off')\nplt.subplots_adjust(wspace=0.5)","9499f6c3":"img.shape","97b22039":"def create_SIFT_features(data):\n    Feature_data = np.zeros((len(data),48,48,3))\n\n    for i in range(len(data)):\n        img = data[i]\n        image8bit = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n        sift = cv2.SIFT_create()\n        kp, des = sift.detectAndCompute(image8bit,None)\n\n        img = cv2.drawKeypoints(image=image8bit, outImage=img, keypoints = kp, flags = 4, color = (255, 0, 0))\n        Feature_data[i] = img\/255.0\n\n        \n    return Feature_data ","c9f05772":"X_train_SIFT = create_SIFT_features(X_train) \nX_train_SIFT.shape","2602c3e8":"plt.imshow(X_train_SIFT[0], cmap='gray')","08402cc6":"X_test_SIFT = create_SIFT_features(X_test) \nX_test_SIFT.shape","536765ec":"plt.imshow(X_test_SIFT[0], cmap='gray')","57ab178a":"SIFT_model = create_model(input_shape=(48,48,3))\nSIFT_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam' )","a1f1e18a":"SIFT_history = SIFT_model.fit(X_train_SIFT, y_train, batch_size=8 , epochs=50, validation_data = (X_test_SIFT, y_test),  callbacks = [callbacks])","0c215d9b":"plot_performance(SIFT_history)","6ece58d7":"SIFT_acc = SIFT_model.evaluate(X_test_SIFT, y_test, verbose = 0)[1]\nacc.append(SIFT_acc)\nprint(\"SIFT Accuracy :\",SIFT_model.evaluate(X_test_SIFT, y_test, verbose = 0)[1])","1569ec97":"SIFT_model.save('SIFT_model.h5')","9b89f681":"def Gabor_filter(K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0):\n    # get half size\n    d = K_size \/\/ 2\n\n    # prepare kernel\n    gabor = np.zeros((K_size, K_size), dtype=np.float32)\n\n    # each value\n    for y in range(K_size):\n        for x in range(K_size):\n            # distance from center\n            px = x - d\n            py = y - d\n\n            # degree -> radian\n            theta = angle \/ 180. * np.pi\n\n            # get kernel x\n            _x = np.cos(theta) * px + np.sin(theta) * py\n\n            # get kernel y\n            _y = -np.sin(theta) * px + np.cos(theta) * py\n\n            # fill kernel\n            gabor[y, x] = np.exp(-(_x**2 + Gamma**2 * _y**2) \/ (2 * Sigma**2)) * np.cos(2*np.pi*_x\/Lambda + Psi)\n\n    # kernel normalization\n    gabor \/= np.sum(np.abs(gabor))\n\n    return gabor\n\n\n# Use Gabor filter to act on the image\ndef Gabor_filtering(gray, K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0):\n    # get shape\n    H, W = gray.shape\n\n    # padding\n    gray = np.pad(gray, (K_size\/\/2, K_size\/\/2), 'edge')\n\n    # prepare out image\n    out = np.zeros((H, W), dtype=np.float32)\n\n    # get gabor filter\n    gabor = Gabor_filter(K_size=K_size, Sigma=Sigma, Gamma=Gamma, Lambda=Lambda, Psi=0, angle=angle)\n\n    # filtering\n    for y in range(H):\n        for x in range(W):\n            out[y, x] = np.sum(gray[y : y + K_size, x : x + K_size] * gabor)\n\n    out = np.clip(out, 0, 255)\n    out = out.astype(np.uint8)\n\n    return out\n\n\n# Use 6 Gabor filters with different angles to perform feature extraction on the image\ndef Gabor_process(img):\n#     print(img.shape)\n    # get shape\n    H, W = img.shape\n\n    # gray scale\n#     gray = BGR2GRAY(img).astype(np.float32)\n\n    # define angle\n    #As = [0, 45, 90, 135]\n    As = [0,30,60,90,120,150]\n\n    # prepare pyplot\n#     plt.subplots_adjust(left=0, right=1, top=1, bottom=0, hspace=0, wspace=0.2)\n\n    out = np.zeros([H, W], dtype=np.float32)\n\n    # each angle\n    for i, A in enumerate(As):\n    \n        # gabor filtering\n        _out = Gabor_filtering(img, K_size=11, Sigma=1.5, Gamma=1.2, Lambda=3, angle=A)\n         \n\n        # add gabor filtered image\n        out += _out\n        \n\n    # scale normalization\n    out = out \/out.max()*255\n    out = out.astype(np.uint8)\n\n    return out","ead5d303":"def create_Gabor_features(data):\n    Feature_data = np.zeros((len(data),48,48,1))\n\n    for i in range(len(data)):\n        img = data[i]\n        out = Gabor_process(img)\n        out = np.expand_dims(out , axis = 2) # adding color channel\n        Feature_data[i] = out\/255.00\n\n        \n    return Feature_data ","f47b8046":"X_train.shape","bb280fa5":"plt.imshow(X_train_Gabor[0]\/255.0, cmap ='gray')","1aa0e90e":"X_train_Gabor=create_Gabor_features(X_train_Gabor)\nX_test_Gabor=create_Gabor_features(X_test_Gabor)\n\nX_train_Gabor.shape , X_test_Gabor.shape","3cbbf2f4":"sample = random.randint(100,500)\nplt.subplot(1,2,1)\nplt.imshow(X_train[sample],cmap='gray')\nplt.axis(\"off\")\nplt.subplot(1,2,2)\nplt.imshow(X_train_Gabor[sample],cmap='gray')\nplt.axis(\"off\")","674c4867":"X_train_Gabor.shape","2ba9641c":"Gabor_model = create_model()\nGabor_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam' )","a39d247b":"Gabor_history = Gabor_model.fit(X_train_Gabor, y_train, batch_size=8 , epochs=50, validation_data = (X_test_Gabor, y_test) ,callbacks = [callbacks])","df978524":"plot_performance(Gabor_history)","60d7431c":"Gabor_acc = Gabor_model.evaluate(X_test_Gabor, y_test, verbose = 0)[1]\nacc.append(Gabor_acc)\nprint(\"Gabor Accuracy :\",Gabor_model.evaluate(X_test_Gabor, y_test, verbose = 0)[1])","09a6388a":"Gabor_model.save('Gabor_model.h5')","5f6b313a":"WFE_model = create_model()\nWFE_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam' )","b257c2dd":"WFE_history = WFE_model.fit(X_train, y_train, batch_size=8 , epochs=50, validation_data = (X_test, y_test) ,callbacks = [callbacks])","64a94120":"plot_performance(WFE_history)","26466aa5":"WFE_acc = WFE_model.evaluate(X_test, y_test, verbose = 0)[1]\nacc.append(WFE_acc)\nprint(\"Without Feature extraction Accuracy :\", WFE_acc)","fd1948fa":"WFE_model.save('WFE_model.h5')","5ea41ac7":"acc","48e40271":"results = pd.DataFrame(acc, index=['HOG', 'LBP', 'SIFT', 'Gabor', 'Without Feature Extraction'], columns = ['Accuracies'])","aa923141":"dfStyler = results.style.set_properties(**{'text-align': 'left'})\ndfStyler.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])","2a3e90f5":"Feature_X_train.shape","54a2f073":"Feature_X_train[0].shape","3a1c7fb5":"X_train_HOG_Flat = np.zeros((len(X_train), 48*48))\nfor i in range(len(Feature_X_train)):\n    img = Feature_X_train[0]\n    img = img.flatten()\n    X_train_HOG_Flat[i] = img\nX_train_HOG_Flat.shape","d357be3f":"X_test_HOG_Flat = np.zeros((len(X_test), 48*48))\nfor i in range(len(Feature_X_test)):\n    img = Feature_X_test[0]\n    img = img.flatten()\n    X_test_HOG_Flat[i] = img\nX_test_HOG_Flat.shape","4feb9c80":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","e5dfabe8":"scaler = StandardScaler()\nX_train_HOG_Flat = scaler.fit_transform(X_train_HOG_Flat)","9a6e334c":"X_train_HOG_Flat.shape","9bdbdd7a":"from sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\nsvc=svm.SVC(probability=True, verbose = True )\nmodel=GridSearchCV(svc,param_grid)","f5717179":"y_train_SVM.shape","13d1ca8b":"a = np.squeeze(y_train_SVM)\na.shape","1ea6982c":"model.fit(X_train_HOG_Flat,a)","cfd46069":"from sklearn.metrics import accuracy_score\ny_pred = model.predict(X_test_HOG_Flat)\naccuracy_score(np.squeeze(y_test_SVM),y_pred)","84f7d936":"1-0.2131979695431472","e99d7e4e":"pca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X_train_HOG_Flat)","fd0fb921":"principalComponents.shape","4808ed57":"### So it can be seen that feature extraction with gabor filters are giving pretty good results than the other methods but also it can noted that CNNs witout any feature extraction methods are also giving good results than the other feature extraction techniques such as HOG, LBP, SIFT.","89145156":"### Creating Feature Vectors for training and testing ","de83bd52":"# Gabor Filters \nwith help of my friend : https:\/\/www.kaggle.com\/sashankmvv\/gabor-cnn-ck\/notebook","c52de2ba":"### Creating Feature Vectors for training and testing ","1fcc6a9c":"### How Hog looks ?","44a83659":"so to create a feature vector we will have to create an empty array of shape (len(data), 48,48,3) ","be0e7f36":"# loading data","606d62f2":"### Training and testing LBP-CNN model","18991673":"# [HOG technqiue](https:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/feature-engineering-images-introduction-hog-feature-descriptor\/) \nFor more info on implementation [visit here](https:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/feature-engineering-images-introduction-hog-feature-descriptor\/)","8c7a6835":"### How LBP looks ?","f3608b42":"## Converting single values to category array ","a3592ed5":"# Lets Visualize some images ","70e96942":"# Feature Extraction","cc387c06":"### Let's see some SIFT features ","875436cf":"# HOG-SVM","2ca493f9":"## Normalizing pixel values ","7b0cabfc":"# LBP technqiue\nImplementation Referred from [here](https:\/\/github.com\/salonibhatiadutta\/To-get-the-local-binary-pattern-LBP-of-image-and-draw-its-histogram\/blob\/master\/gray_image_conversion.ipynb)","fbe505e8":"Referred from [here](https:\/\/www.kaggle.com\/milan400\/human-emotion-detection-by-using-cnn#Creating-Model)","21f130cf":"## This notebook contains several feature extraction techniques which are used to extract features from the facial images and then passed to a CNN model to compare their results","454653d6":"# Creating training and testing data","a477873c":"### Data Augmentation","14f4f1aa":"### Training and testing the HOG - CNN model","0cb46888":"## Adding color channel ","7cef3269":"# Without Feature Extraction ","bae7aae1":"# Comparing accuracies ","62d705de":"# SIFT ","c1e219c3":"### Creating Feature Vectors for training and testing "}}