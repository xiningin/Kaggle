{"cell_type":{"33f4f848":"code","ead1bef4":"code","4b29e979":"code","1d5fb887":"code","210ac139":"code","cfd8966d":"code","b790bb97":"code","cbc602f9":"code","2e21c51c":"code","6db58819":"code","4c898b04":"code","6c51fd91":"code","d25893ae":"code","e3f5fd50":"code","cae5e12e":"code","01c2fb91":"code","2ef53c5a":"code","c0df403a":"code","86ed0294":"code","b127e328":"code","92c2a4fe":"code","eb7e40c7":"code","d7395a4d":"code","b29ad9bc":"code","86c5e567":"markdown","39860d91":"markdown","e022572b":"markdown","b96eeef5":"markdown","d7c02873":"markdown","a9714efc":"markdown","0f9543f2":"markdown","6bb0009f":"markdown","9f5ab5fb":"markdown","247072bf":"markdown","92eef9e0":"markdown","8365d4fc":"markdown","8d9fd263":"markdown","9856e5cc":"markdown","ffa4ab13":"markdown"},"source":{"33f4f848":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#import numpy as np # linear algebra\n# Not Needed since pandas has np already loaded. just use pd.np.whatever you need numpy for\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.sparse import coo_matrix\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ead1bef4":"# import spacy libraries\nfrom spacy.vocab import Vocab\nfrom spacy.tokenizer import Tokenizer\n\n# import gensim libraries\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n# import pytorch\nimport torch\nfrom torch.utils import data\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# sklearn metrics\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# other libraries\nimport time\n#from tqdm._tqdm_notebook import tqdm_notebook as tqdm\nimport tqdm","4b29e979":"# universal parameter settings\n\n# identity columns that are featured in the testing data\n# according to the data description of the competition\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\n\n# columns that describe the comment\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\n# column with text data that will need to be converted for processing\nTEXT_COLUMN = 'comment_text'\n\n# column we eventually need to predict\nTARGET_COLUMN = 'target'","1d5fb887":"# characters that we can ignore when tokenizating the TEXT_COLUMN\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'","210ac139":"train_df = (\n    pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\n    .assign(bin_target=lambda x: x.target.apply(lambda x: 1 if x > 0.5 else 0))\n)\nprint(train_df.shape)","cfd8966d":"train_df.head(2)","b790bb97":"(\n    train_df\n    .bin_target\n    .value_counts()\n)","cbc602f9":"X_dev_pos = (\n    train_df\n    .query(\"bin_target == 1\")\n    .sample(30219, random_state=100) #53219\n    .sort_values(\"id\")\n)\n\nX_dev_neg = (\n    train_df\n    .query(\"bin_target == 0\")\n    .sample(38243, random_state=100) #488243\n    .sort_values(\"id\")\n)\n\nX_dev = X_dev_pos.append(X_dev_neg)\nX_dev.shape # 541462","2e21c51c":"X = train_df[~train_df.id.isin(X_dev.id.values.tolist())]\nX = (\n    X\n    .query(\"bin_target==1\")\n    .sample(76219,random_state=100)\n    .append(\n        X\n        .query(\"bin_target==0\")\n        .sample(76219,random_state=100)\n    )\n)\nX.shape","6db58819":"test_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\nprint(test_df.shape)","4c898b04":"test_df.head(2)","6c51fd91":"# convert the glove twitter word vectors into \n# word2vec format\nglove2word2vec(\n    glove_input_file=\"..\/input\/glove-global-vectors-for-word-representation\/glove.twitter.27B.50d.txt\",\n    word2vec_output_file=\"word2vec.twitter.27B.50d.txt\"\n)","d25893ae":"word_model = KeyedVectors.load_word2vec_format(\n    \"word2vec.twitter.27B.50d.txt\",\n    binary=False\n)\n\n# Make a dictionary with [word]->index \nglobal_word_dict = {key:index for index, key in enumerate(word_model.vocab.keys(), start=2)}","e3f5fd50":"tokenizer = Tokenizer(vocab=Vocab(strings=list(word_model.vocab.keys())))\ndef index_sentences(sentence_list):\n    indexed_sentences =list(\n        map(\n            lambda x: torch.LongTensor([\n                global_word_dict[token.text.lower()] \n                if token.text in global_word_dict else 1 \n                for token in tokenizer(x)\n            ]),\n            tqdm.tqdm(sentence_list)\n        )\n    )\n    return indexed_sentences","cae5e12e":"torch_X = index_sentences(X[TEXT_COLUMN].values)\ntorch_Y = torch.FloatTensor(X.bin_target.values)","01c2fb91":"torch_X_dev = index_sentences(X_dev[TEXT_COLUMN].values)\ntorch_Y_dev = torch.FloatTensor(X_dev.bin_target.values)","2ef53c5a":"class CustomLSTMLayer(nn.Module):\n    def __init__(\n        self, \n        input_size=200, hidden_size=200,\n        num_layers=2, batch_size=256, \n        bidirectional=False, inner_dropout=0.25,\n        outer_droput = [0.25, 0.25]\n    ):\n        super(CustomLSTMLayer, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.bidirectional = bidirectional\n\n        self.lstm = nn.LSTM(\n            self.input_size, self.hidden_size, \n            self.num_layers, batch_first=True,\n            bidirectional=self.bidirectional, \n            dropout=inner_dropout\n        )\n        \n    def forward(self, input):\n        #seq_lengths = torch.zeros(input.shape(0), dtype=torch.long)\n        \n        #for i in range(batch_size):\n        #    for j in range(max_seq - 1, -1, -1):\n        #        if not torch.all(X[i, j] == 0):\n        #            seq_lengths[i] = j + 1\n        #           break\n        _, (ht,_) = self.lstm(input)\n        return ht[-1, :]\n    \n    def init_hidden_size(self):\n        cell_state = torch.zeros(\n            self.num_layers * (2 if self.bidirectional else 1),\n            self.batch_size,\n            self.hidden_size\n        )\n        \n        hidden_state = torch.zeros(\n            self.num_layers * (2 if self.bidirectional else 1),\n            self.batch_size,\n            self.hidden_size\n        )\n        \n        return (hidden_state, cell_state)","c0df403a":"class CustomEmbeddingLayer(nn.Module):\n    def __init__(\n        self, \n        vocab_size, embedding_size, \n        pretrained_embeddings=None, freeze=False\n    ):\n        super(CustomEmbeddingLayer, self).__init__()\n        \n        if pretrained_embeddings is None:\n            self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        else:\n            rows, cols = pretrained_embeddings.shape\n            self.embed = nn.Embedding(num_embeddings=rows, embedding_dim=cols, padding_idx=0)\n            self.embed.weight.data.copy_(pretrained_embeddings)\n    \n        self.embed.weight.requries_grad = not freeze\n        \n    def forward(self, input):\n        return self.embed(input)","86ed0294":"class CustomFullyConnected(nn.Module):\n    def __init__(self, hidden_size=200):\n        super(CustomFullyConnected, self).__init__()\n        \n        self.fc1 = nn.Linear(hidden_size, 20)\n        self.fc2 = nn.Linear(20, 10)\n        self.fc3 = nn.Linear(10, 2)\n        \n    def forward(self, input):\n        output = self.fc1(input)\n        ouput = F.relu(output)\n        output = self.fc2(output)\n        ouput = F.relu(output)\n        output = self.fc3(output)\n        ouput = F.relu(output)\n        return output","b127e328":"# Set up the dataloader\nclass SentenceDataLoader(data.Dataset):\n    def __init__(self, train_data, train_labels):\n        super(SentenceDataLoader, self).__init__()\n        \n        self.X = train_data\n        self.Y = train_labels\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        return tuple([self.X[index], self.Y[index]])","92c2a4fe":"def pad_sentences(batch):\n    max_batch_length = max(list(map(lambda x: x[0].size(0), batch)))\n    padded_sentences = torch.LongTensor(\n        list(\n            map(\n                lambda x: pd.np.pad(x[0].numpy(), (0, max_batch_length-x[0].size(0)), 'constant', constant_values=0),\n                batch\n            )\n        )\n    )\n    sentence_labels = torch.FloatTensor(list(map(lambda x: x[1], batch)))\n    return (padded_sentences, sentence_labels)","eb7e40c7":"# Rate at which comments are dropped for training\n# too high can underfit\n# too low can overfit\nDROPOUT_RATE = 0.25\n\n# NUMBER OF EPOCHS\n# One Epoch is when an entire dataset is passed forward and backward\n# through the neural network once.\nEPOCHS = 30\n\n# dimensions of the output vectors of each LSTM cell.\n# Too high can overfit\n# Too low can underfit\n# The length of this vector reflects the number of\n# Bidirectional CuDNNLSTM layers there will be\nLSTM_HIDDEN_UNITS = 25\n\n\n# dimensions of the densely-connected NN layer cells.\n# The length of this vector reflects the number of\n# Dense layers there will be\nDENSE_HIDDEN_UNITS = 4 * LSTM_HIDDEN_UNITS\n\n# The size of the vocab the LSTM uses\nVOCAB_SIZE = len(global_word_dict)\n\n# The side of the word vectors\nEMBEDDING_SIZE = 50\n\n#How big the batch size should be\nBATCH_SIZE = 128\n\n# The learning Rate\nLEARNING_RATE = 0.01","d7395a4d":"model = nn.Sequential(\n    CustomEmbeddingLayer(\n        vocab_size=VOCAB_SIZE, \n        embedding_size=EMBEDDING_SIZE, \n        pretrained_embeddings=torch.FloatTensor(word_model.vectors) #find the correct code here\n    ),\n    CustomLSTMLayer(\n        input_size=EMBEDDING_SIZE, hidden_size=LSTM_HIDDEN_UNITS,\n        batch_size=BATCH_SIZE\n    ),\n    CustomFullyConnected(LSTM_HIDDEN_UNITS),\n)\nprint(model)","b29ad9bc":"train_dataset = SentenceDataLoader(torch_X, torch_Y)\ntrain_data_loader = data.DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    collate_fn=pad_sentences,\n    shuffle=True\n)\n\nval_dataset = SentenceDataLoader(torch_X_dev, torch_Y_dev)\nval_data_loader = data.DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE, \n    collate_fn=pad_sentences,\n    shuffle=True\n)\n\n# Set up the optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Set up the loss\nce_loss = nn.BCEWithLogitsLoss()\n\nfor epoch in range(EPOCHS):\n    # Set the progress bar up\n    progress_bar = tqdm.tqdm(\n        enumerate(train_data_loader),\n        total=len(train_data_loader),\n    )\n    \n    #throw the model on the gpu\n    model = model.cuda()\n    \n    avg_epoch_loss = []\n    model.train()\n\n    for index, batch in progress_bar:\n        data_batch = batch[0]\n\n        data_labels = torch.zeros(batch[0].size(0), 2)\n        data_labels[range(batch[0].size(0)), batch[1].long()] = 1\n        \n        #Throw it on the gpu\n        data_batch = data_batch.cuda()\n        data_labels = data_labels.cuda()\n        \n        # Zero out the optimizer\n        optimizer.zero_grad()\n        \n        #predict batch\n        predicted = F.softmax(model(data_batch), dim=1)\n        \n        #Calculate the loss\n        loss = ce_loss(predicted, data_labels)\n        avg_epoch_loss.append(loss.item())\n        loss.backward()\n        \n        # Update the weights\n        optimizer.step()\n        \n        progress_bar.set_postfix(avg_loss=avg_epoch_loss[-1])\n    \n    model.eval()\n    predicted_proba = []\n    dev_targets = []\n    \n    for val_batch in val_data_loader:\n        val_data_batch = val_batch[0]\n        val_data_batch = val_data_batch.cuda()\n        \n        predicted = F.softmax(model(val_data_batch), dim=1)\n        predicted_proba.append(predicted[:,1])\n        dev_targets.append(val_batch[1])\n    \n    predicted_proba = torch.cat(predicted_proba, dim=0)\n    dev_targets = torch.cat(dev_targets)\n    predicted_labels = list(\n        map(\n            lambda x: 1 if x > 0.5 else 0,\n            predicted_proba\n            .cpu()\n            .float()\n            .detach()\n            .numpy()\n        )\n    )\n    \n    msg = f\"E[{epoch+1}] Train Loss: {pd.np.mean(avg_epoch_loss):.3f} \"\n    msg += f\"Dev Accuracy: {accuracy_score(dev_targets.long().numpy(), predicted_labels):.3f} \"\n    msg += f\"Dev F1: {f1_score(dev_targets.long().numpy(), predicted_labels):.3f}\"\n    print(msg)\n","86c5e567":"## Set Up the Model Classes and DataLoader","39860d91":"# Universal Parameters\nFirst we set up the parameters to identify basic parts of the input data","e022572b":"# Summary","b96eeef5":"# Text Processing","d7c02873":"parameters for text processing","a9714efc":"## Get unique sequences for each comment","0f9543f2":"# LSTM Model Pytorch","6bb0009f":"Load the word vectors and vocab","9f5ab5fb":"Here we are building an Long Short Term Memory (LSTM) network, a type of recurrent neural networks that is well explained in the following websites: \n* https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n* https:\/\/adventuresinmachinelearning.com\/keras-lstm-tutorial\/\n\nI've been seeing LSTM pop up a lot in kaggle competitions so its good to become familiar with them.","247072bf":"## Set the Global Variables and Run the LSTM","92eef9e0":"Once the comments have been fit to the tokenizer we could get:\n* word_counts: A dictionary of words and their counts across all the comments.\n* word_docs: A dictionary of words and how many comments each appeared in.\n* word_index: A dictionary of words and their uniquely assigned integers.\n* document_count:An integer count of the total number of documents that were used to fit the Tokenizer.","8365d4fc":"Let us just note that the word_index that matches a word is arbitrary. If two words have word_index values that are relatively close that does NOT mean the words are closely related it just means they are different from each other. However, none of the words have the index \"0\". A word_index of 0 means the word is outside the vocabulary and therefore can be used to pad comments to be longer if we need comments to be of equal length.","8d9fd263":"Using the tokenizer, we translate the comments in the training and testing set respectively to lists of each word's word_index in each comment. For example if the comment was \"Hello World\" and the word index for \"Hello\" is 5 and \"World\" is 202 then we would translate the comment to [5,202]. In other words, we can now identify each comment by the order of the unique indexes.","9856e5cc":"# Train The Model","ffa4ab13":"# Import Data\nOpen the testing and training datasets into data frames"}}