{"cell_type":{"f7f31889":"code","9bc7bd38":"code","499035e8":"code","82769719":"code","87e9b013":"code","543bba74":"code","cbe5530f":"code","f09895a7":"code","b4dda5bf":"code","dc358fa7":"code","c209daae":"code","a1950c27":"code","5fc65bec":"code","4346fc01":"code","31b2816d":"code","78a0e5d1":"code","420d40ab":"code","6422f787":"code","50f84655":"code","f708ead8":"code","ccbe43b6":"code","2bc4110b":"code","52b09e94":"code","ad5e53c8":"code","9bc2dcc9":"code","2fc44bf8":"code","e76a5673":"code","11d1b29e":"markdown","0e794423":"markdown","eed55966":"markdown","74cac67a":"markdown","e813f737":"markdown","912d525c":"markdown","15048094":"markdown","a02d2cb7":"markdown","a7976698":"markdown","50374254":"markdown","d0d1e820":"markdown","772ebafc":"markdown","6d2a254e":"markdown","ca43c460":"markdown","27361b3e":"markdown","47410e3d":"markdown"},"source":{"f7f31889":"import numpy as np\nimport time\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom skimage import color\n!pip install mahotas\nimport mahotas as mt\nfrom skimage.color import rgb2gray\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","9bc7bd38":"\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","499035e8":"train_data_path=\"\/kaggle\/input\/deepsat-sat6\/X_train_sat6.csv\"\ntrain_label_path=\"\/kaggle\/input\/deepsat-sat6\/y_train_sat6.csv\"\ntest_data_path=\"\/kaggle\/input\/deepsat-sat6\/X_test_sat6.csv\"\ntest_lable_path=\"\/kaggle\/input\/deepsat-sat6\/y_test_sat6.csv\"","82769719":"def data_read(data_path, nrows):\n    data=pd.read_csv(data_path, header=None, nrows=nrows)\n    data=data.values ## converting the data into numpy array\n    return data","87e9b013":"##Read training data\ntrain_data=data_read(train_data_path, nrows=500)\nprint(\"Train data shape:\" + str(train_data.shape))\n\n##Read training data labels\ntrain_data_label=data_read(train_label_path,nrows=500)\nprint(\"Train data label shape:\" + str(train_data_label.shape))\nprint()\n\n##Read test data\ntest_data=data_read(test_data_path, nrows=100)\nprint(\"Test data shape:\" + str(test_data.shape))\n\n\n##Read test data labels\ntest_data_label=data_read(test_lable_path,nrows=100)\nprint(\"Test data label shape:\" + str(test_data_label.shape))\n","543bba74":"#label converter\n# [1,0,0,0,0,0]=building\n# [0,1,0,0,0,0]=barren_land\n# [0,0,1,0,0,0]=trees\n# [0,0,0,1,0,0]=grassland\n# [0,0,0,0,1,0]=road\n# [0,0,0,0,0,1]=water\n\n\ndef label_conv(label_arr):\n    labels=[]\n    for i in range(len(label_arr)):\n        \n        if (label_arr[i]==[1,0,0,0,0,0]).all():\n            labels.append(\"Building\")  \n            \n        elif (label_arr[i]==[0,1,0,0,0,0]).all():  \n            labels.append(\"Barren_land\")  \n            \n        elif (label_arr[i]==[0,0,1,0,0,0]).all():\n            labels.append(\"Tree\") \n            \n        elif (label_arr[i]==[0,0,0,1,0,0]).all():\n            labels.append(\"Grassland\")\n            \n        elif (label_arr[i]==[0,0,0,0,1,0]).all():\n            labels.append(\"Road\") \n            \n        else:\n            labels.append(\"Water\")\n    return labels\ntrain_label_convert=label_conv(train_data_label)##train label conveter\ntest_label_convert=label_conv(test_data_label) ##test label converter\n\n\ndef data_visualization(data, label, n):\n    ##data: training or test data\n    ##lable: training or test labels\n    ## n: number of data point, it should be less than or equal to no. of data points\n    fig = plt.figure(figsize=(14, 14))\n    ax = []  # ax enables access to manipulate each of subplots\n    rows, columns=4,4\n    for i in range(columns*rows):\n        index=np.random.randint(1,n)\n        img= data[index].reshape([28,28,4])[:,:,:3] ##reshape input data to rgb image\n        ax.append( fig.add_subplot(rows, columns, i+1) ) # create subplot and append to ax\n        ax[-1].set_title(\"Class:\"+str(label[index]))  # set class\n        plt.axis(\"off\")\n        plt.imshow(img)\n\n    plt.subplots_adjust(wspace=0.1,hspace=0.5)\n    plt.show()  # finally, render the plot","cbe5530f":"data_visualization(train_data,train_label_convert, n=500)","f09895a7":"data_visualization(test_data,test_label_convert, n=100)","b4dda5bf":"\n#  texture_features=[\"Angular Second Moment\",\"Contrast\",\"Correlation\",\"Sum of Squares: Variance\",\"Inverse Difference Moment\",\n#                    \"Sum Average\",\"Sum Variance\",\"Sum Entropy\",\"Entropy\",\"Difference Variance\",\"Difference Entropy\",\n#                    \"Information Measure of Correlation 1\",\"Information Measure of Correlation 2\"\"Maximal Correlation Coefficient\"]\n\n#https:\/\/gogul09.github.io\/software\/texture-recognition #references for texture feature calculations\n\ndef feature_extractor(input_image_file):\n    \n        tex_feature=[]\n        hsv_feature=[]\n        ndvi_feature=[]\n        arvi_feature=[]\n\n        for df_chunk in pd.read_csv(input_image_file ,header=None,chunksize = 5000):\n\n            df_chunk=df_chunk.astype(\"int32\")\n            data=df_chunk.values\n\n\n            ################data for HSV and Texture feature##############\n            img=data.reshape(-1,28,28,4)[:,:,:,:3]\n            #############################################################\n\n            ######################Data for NDVI and ARVI#################\n\n            NIR=data.reshape(-1,28,28,4)[:,:,:,3]\n            Red=data.reshape(-1,28,28,4)[:,:,:,2]\n            Blue=data.reshape(-1,28,28,4)[:,:,:,0]\n            #############################################################\n\n            for i in range(len(data)):\n\n                #######Texture_feature####################################\n                textures = mt.features.haralick(img[i])\n                ht_mean= textures.mean(axis=0)\n                tex_feature.append(ht_mean)\n                ##########################################################\n\n                #######hsv_feature#########################################\n                img_hsv = color.rgb2hsv(img[i]) # Image into HSV colorspace\n                h = img_hsv[:,:,0] # Hue\n                s = img_hsv[:,:,1] # Saturation\n                v = img_hsv[:,:,2] # Value aka Lightness\n                hsv_feature.append((h.mean(),s.mean(),v.mean()))\n                ###########################################################\n\n                ##########Calculation of NDVI Feature######################\n                NDVI=(NIR[i]-Red[i])\/(NIR[i]+Red[i])\n                ndvi_feature.append(NDVI.mean())\n                ############################################################\n\n                ###################Calculation of ARVI#####################\n                a_1=NIR[i] -(2*Red[i]-Blue[i])\n                a_2=NIR[i] +(2*Red[i]+Blue[i])\n                arvi=a_1\/a_2\n                arvi_feature.append(arvi.mean())\n                #######################################################\n\n        features=[]\n        for i in range(len(tex_feature)):\n            h_stack=np.hstack((tex_feature[i], hsv_feature[i], ndvi_feature[i], arvi_feature[i]))\n            features.append(h_stack)\n            \n        return features","dc358fa7":"train_data_features=feature_extractor(train_data_path)\n# saving train data features\nfeature=pd.DataFrame(train_data_features, columns=[\"feature\"+ str(i) for i in range(len(train_data_features[0]))])\nfeature.to_csv(\"train_feature_deepstat_6.csv\")","c209daae":"#test data features extraction\ntest_data_features=feature_extractor(test_data_path)\nfeature_test=pd.DataFrame(test_data_features, columns=[\"feature\"+ str(i) for i in range(len(train_data_features[0]))])\nfeature_test.to_csv(\"test_feature_deepsat_6.csv\")","a1950c27":"from sklearn.preprocessing import StandardScaler \nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom time import time\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5fc65bec":"def data_read_(data_path):\n    df=pd.read_csv(data_path, index_col=[0])\n    return df\n\ndef label_read(data_path):\n    df=pd.read_csv(data_path, header=None)\n    return df\n","4346fc01":"\ntrain_feature_deepstat_6=data_read_(data_path=\"\/kaggle\/input\/extracted-feature\/train_feature_deepstat_6.csv\")\ntrain_label=label_read(data_path=train_label_path)\nprint(\"Training data shape: \",train_feature_deepstat_6.shape)\nprint(\"Training label shape: \",train_label.shape)","31b2816d":"train_feature_deepstat_6.head()","78a0e5d1":"train_label.head()","420d40ab":"test_feature_deepsat_6=data_read_(data_path=\"\/kaggle\/input\/extracted-feature\/test_feature_deepsat_6.csv\")\ntest_label=label_read(data_path=test_lable_path)\nprint(\"Training data shape: \",test_feature_deepsat_6.shape)\nprint(\"Training label shape: \",test_label.shape)","6422f787":"test_feature_deepsat_6.head()","50f84655":"test_label.head()","f708ead8":"sc=StandardScaler()\n#fit the training data\nfit=sc.fit(train_feature_deepstat_6)","ccbe43b6":"##transform the train and test data\ntrain_data_stn=fit.transform(train_feature_deepstat_6)\ntest_data_stn=fit.transform(test_feature_deepsat_6)","2bc4110b":"model=Sequential()\n\n#layer1\nmodel.add(Dense(units=50,input_shape=(train_data_stn.shape[1],),use_bias=True))\n#model.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.2))\n\n#layer2\nmodel.add(Dense(units=50, use_bias=True))\n#model.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.2))\n\n#layer3\nmodel.add(Dense(units=6, activation=\"softmax\"))\n\n\n##ADD early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n# mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n# tensorboard = TensorBoard(log_dir='logs\/{}'.format(time()))\n#tensorboard=TensorBoard(log_dir='logs\/{}'.format(time()))\n\n#compile the model\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n","52b09e94":"#model.fit(train_data_stn, train_label.values, validation_split=0.15, batch_size=512, epochs=500,callbacks=[es, mc,tensorboard]) \nmodel.fit(train_data_stn, train_label.values, validation_split=0.15, batch_size=512, epochs=500,callbacks=[es]) ","ad5e53c8":"Accuracy_on_test_data=model.evaluate(test_data_stn, test_label.values)[1]\nprint(\"Accuracy on test data: \",Accuracy_on_test_data)","9bc2dcc9":"#label converter\n# [1,0,0,0,0,0]=building\n# [0,1,0,0,0]=barren_land\n# [0,0,1,0,0,0]=tree\n# [0,0,0,1,0,0]=grassland\n# [0,0,0,0,1,0]=road\n# [0,0,0,0,0,1]=water\n\n\n##Building confusion matrix\n\ny_pred=model.predict_classes(test_data_stn)\ny_true=np.argmax(test_label.values, axis=1)\ncm=confusion_matrix(y_target=y_true, y_predicted=y_pred)\n\nplot_confusion_matrix(cm,class_names=[\"Building\",\"Barren_land\",\"Tree\",\"Grassland\",\"Road\",\"Water\"],figsize=(6,6) )\nplt.show()","2fc44bf8":"def precision(label, confusion_matrix):\n    col = confusion_matrix[:, label]\n    return confusion_matrix[label, label] \/ col.sum()\n    \ndef recall(label, confusion_matrix):\n    row = confusion_matrix[label, :]\n    return confusion_matrix[label, label] \/ row.sum()\n\ndef precision_macro_average(confusion_matrix):\n    rows, columns = confusion_matrix.shape\n    sum_of_precisions = 0\n    for label in range(rows):\n        sum_of_precisions += precision(label, confusion_matrix)\n    return sum_of_precisions \/ rows\n\ndef recall_macro_average(confusion_matrix):\n    rows, columns = confusion_matrix.shape\n    sum_of_recalls = 0\n    for label in range(columns):\n        sum_of_recalls += recall(label, confusion_matrix)\n","e76a5673":"\ndic={}\nprecision_=[]\nrecall_=[]\nprecision_macro_average_=[]\nfor label in range(6):\n    precision_.append(precision(label, cm))\n    recall_.append(recall(label, cm))\n    \ndic[\"Precision\"]= precision_\ndic[\"Recall\"]= recall_\n\nplt.figure(figsize=(6,6))\nax=sns.heatmap(pd.DataFrame(dic, index=[\"building\",\"barren_land\",\"tree\",\"grassland\",\"road\",\"water\"]),annot=True,cbar=False)\nplt.yticks(rotation=0)\nax.xaxis.tick_top() # x axis on top\n\nplt.show()","11d1b29e":"## Data Reading","0e794423":"## Model Training","eed55966":"## Reading test data feature and test data label","74cac67a":"## Importing the libraries","e813f737":"## Features Extraction\u00b6","912d525c":"## Model Building","15048094":"## Features extraction from test data","a02d2cb7":"## Data Visualization","a7976698":"## Precision and Recall calculation for multiclass classification","50374254":"## Confusion Matrix","d0d1e820":"## Reading training data feature and training data label","772ebafc":"## Features extraction from training data","6d2a254e":"## Data standardization","ca43c460":"## Training data visualization","27361b3e":"## Test data visualization","47410e3d":"## Model Evaluation"}}