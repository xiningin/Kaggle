{"cell_type":{"d28b2c28":"code","e0fed60b":"code","21f83a81":"code","0666b42e":"code","d42f56cd":"code","f2469f48":"code","6ffdfaf1":"code","6e55cb08":"code","56de9b03":"code","8c48b0d8":"code","963383f2":"code","44f62d8b":"code","22f67c92":"code","c8f54231":"code","75f09aa6":"code","46a2dfda":"code","b2962b0c":"code","33eb7487":"code","3c35c1e4":"code","59068b1b":"code","1d6b095d":"code","c767afbb":"code","8de653df":"code","91ca7a5f":"code","353c3a2a":"code","be7cd465":"code","bab57cae":"code","7a48c5cb":"code","2709b703":"code","f47666ab":"code","261b7680":"markdown","f61597af":"markdown","395c071f":"markdown","d198185a":"markdown","85c3e5d2":"markdown","4b1fed54":"markdown","f1e771ad":"markdown","2389bcfe":"markdown","5829b9ea":"markdown","080987b6":"markdown","dcf37596":"markdown","380ed4aa":"markdown","c580887a":"markdown","4d1c4413":"markdown","ed471b40":"markdown","4fb5ed88":"markdown","601da127":"markdown","6f2c6ab8":"markdown","a992b4d6":"markdown","958968ec":"markdown"},"source":{"d28b2c28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e0fed60b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport matplotlib.patches as patches\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 150)\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport os\nimport random\nimport math\nimport psutil\nimport pickle\n\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import LabelEncoder","21f83a81":"metadata_dtype = {'site_id':\"uint8\",'building_id':'uint16','square_feet':'float32','year_built':'float32','floor_count':\"float16\"}\nweather_dtype = {\"site_id\":\"uint8\",'air_temperature':\"float16\",'cloud_coverage':\"float16\",'dew_temperature':\"float16\",'precip_depth_1_hr':\"float16\",\n                 'sea_level_pressure':\"float32\",'wind_direction':\"float16\",'wind_speed':\"float16\"}\ntrain_dtype = {'meter':\"uint8\",'building_id':'uint16'}","0666b42e":"%%time\n\nweather_train = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\", parse_dates=['timestamp'], dtype=weather_dtype)\nweather_test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\", parse_dates=['timestamp'], dtype=weather_dtype)\n\nmetadata = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\", dtype=metadata_dtype)\n\ntrain = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/train.csv\", parse_dates=['timestamp'], dtype=train_dtype)\ntest = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/test.csv\", parse_dates=['timestamp'], usecols=['building_id','meter','timestamp'], dtype=train_dtype)\n\nprint('Size of train_df data', train.shape)\nprint('Size of weather_train_df data', weather_train.shape)\nprint('Size of weather_test_df data', weather_test.shape)\nprint('Size of building_meta_df data', metadata.shape)","d42f56cd":"train['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)\ntest['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)","f2469f48":"train.head()","6ffdfaf1":"weather_train.head()","6e55cb08":"metadata.head()","56de9b03":"test.head()","8c48b0d8":"# Dropping floor_count variable as it has 75% missing values\nmetadata.drop('floor_count',axis=1,inplace=True)","963383f2":"for df in [train, test]:\n    df['Month'] = df['timestamp'].dt.month.astype(\"uint8\")\n    df['DayOfMonth'] = df['timestamp'].dt.day.astype(\"uint8\")\n    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n    df['Hour'] = df['timestamp'].dt.hour.astype(\"uint8\")","44f62d8b":"train['meter_reading'] = np.log1p(train['meter_reading'])","22f67c92":"metadata['primary_use'].replace({\"Healthcare\":\"Other\",\"Parking\":\"Other\",\"Warehouse\/storage\":\"Other\",\"Manufacturing\/industrial\":\"Other\",\n                                \"Retail\":\"Other\",\"Services\":\"Other\",\"Technology\/science\":\"Other\",\"Food sales and service\":\"Other\",\n                                \"Utility\":\"Other\",\"Religious worship\":\"Other\"},inplace=True)\nmetadata['square_feet'] = np.log1p(metadata['square_feet'])\nmetadata['year_built'].fillna(-999, inplace=True)\nmetadata['year_built'] = metadata['year_built'].astype('int16')","c8f54231":"%%time\ntrain = pd.merge(train,metadata,on='building_id',how='left')\ntest  = pd.merge(test,metadata,on='building_id',how='left')\nprint (\"Training Data+Metadata Shape {}\".format(train.shape))\nprint (\"Testing Data+Metadata Shape {}\".format(test.shape))\ngc.collect()\ntrain = pd.merge(train,weather_train,on=['site_id','timestamp'],how='left')\ntest  = pd.merge(test,weather_test,on=['site_id','timestamp'],how='left')\nprint (\"Training Data+Metadata+Weather Shape {}\".format(train.shape))\nprint (\"Testing Data+Metadata+Weather Shape {}\".format(test.shape))\ngc.collect()","75f09aa6":"# Save space\nfor df in [train,test]:\n    df['square_feet'] = df['square_feet'].astype('float16')\n    \n# Fill NA\ncols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed']\nfor col in cols:\n    train[col].fillna(np.nanmean(train[col].tolist()),inplace=True)\n    test[col].fillna(np.nanmean(test[col].tolist()),inplace=True)\n    \n# Drop nonsense entries\n# As per the discussion in the following thread, https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/117083, there is some discrepancy in the meter_readings for different ste_id's and buildings. It makes sense to delete them\nidx_to_drop = list((train[(train['site_id'] == 0) & (train['timestamp'] < \"2016-05-21 00:00:00\")]).index)\nprint (len(idx_to_drop))\ntrain.drop(idx_to_drop,axis='rows',inplace=True)\n\n# dropping all the electricity meter readings that are 0, after considering them as anomalies.\nidx_to_drop = list(train[(train['meter'] == \"Electricity\") & (train['meter_reading'] == 0)].index)\nprint(len(idx_to_drop))\ntrain.drop(idx_to_drop,axis='rows',inplace=True)","46a2dfda":"train.head()","b2962b0c":"%%time\nnumber_unique_meter_per_building = train.groupby('building_id')['meter'].nunique()\ntrain['number_unique_meter_per_building'] = train['building_id'].map(number_unique_meter_per_building)\n\n\nmean_meter_reading_per_building = train.groupby('building_id')['meter_reading'].mean()\ntrain['mean_meter_reading_per_building'] = train['building_id'].map(mean_meter_reading_per_building)\nmedian_meter_reading_per_building = train.groupby('building_id')['meter_reading'].median()\ntrain['median_meter_reading_per_building'] = train['building_id'].map(median_meter_reading_per_building)\nstd_meter_reading_per_building = train.groupby('building_id')['meter_reading'].std()\ntrain['std_meter_reading_per_building'] = train['building_id'].map(std_meter_reading_per_building)\n\n\nmean_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].mean()\ntrain['mean_meter_reading_on_year_built'] = train['year_built'].map(mean_meter_reading_on_year_built)\nmedian_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].median()\ntrain['median_meter_reading_on_year_built'] = train['year_built'].map(median_meter_reading_on_year_built)\nstd_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].std()\ntrain['std_meter_reading_on_year_built'] = train['year_built'].map(std_meter_reading_on_year_built)\n\n\nmean_meter_reading_per_meter = train.groupby('meter')['meter_reading'].mean()\ntrain['mean_meter_reading_per_meter'] = train['meter'].map(mean_meter_reading_per_meter)\nmedian_meter_reading_per_meter = train.groupby('meter')['meter_reading'].median()\ntrain['median_meter_reading_per_meter'] = train['meter'].map(median_meter_reading_per_meter)\nstd_meter_reading_per_meter = train.groupby('meter')['meter_reading'].std()\ntrain['std_meter_reading_per_meter'] = train['meter'].map(std_meter_reading_per_meter)\n\n\nmean_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].mean()\ntrain['mean_meter_reading_per_primary_usage'] = train['primary_use'].map(mean_meter_reading_per_primary_usage)\nmedian_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].median()\ntrain['median_meter_reading_per_primary_usage'] = train['primary_use'].map(median_meter_reading_per_primary_usage)\nstd_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].std()\ntrain['std_meter_reading_per_primary_usage'] = train['primary_use'].map(std_meter_reading_per_primary_usage)\n\n\nmean_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].mean()\ntrain['mean_meter_reading_per_site_id'] = train['site_id'].map(mean_meter_reading_per_site_id)\nmedian_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].median()\ntrain['median_meter_reading_per_site_id'] = train['site_id'].map(median_meter_reading_per_site_id)\nstd_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].std()\ntrain['std_meter_reading_per_site_id'] = train['site_id'].map(std_meter_reading_per_site_id)\n\n\ntest['number_unique_meter_per_building'] = test['building_id'].map(number_unique_meter_per_building)\n\ntest['mean_meter_reading_per_building'] = test['building_id'].map(mean_meter_reading_per_building)\ntest['median_meter_reading_per_building'] = test['building_id'].map(median_meter_reading_per_building)\ntest['std_meter_reading_per_building'] = test['building_id'].map(std_meter_reading_per_building)\n\ntest['mean_meter_reading_on_year_built'] = test['year_built'].map(mean_meter_reading_on_year_built)\ntest['median_meter_reading_on_year_built'] = test['year_built'].map(median_meter_reading_on_year_built)\ntest['std_meter_reading_on_year_built'] = test['year_built'].map(std_meter_reading_on_year_built)\n\ntest['mean_meter_reading_per_meter'] = test['meter'].map(mean_meter_reading_per_meter)\ntest['median_meter_reading_per_meter'] = test['meter'].map(median_meter_reading_per_meter)\ntest['std_meter_reading_per_meter'] = test['meter'].map(std_meter_reading_per_meter)\n\ntest['mean_meter_reading_per_primary_usage'] = test['primary_use'].map(mean_meter_reading_per_primary_usage)\ntest['median_meter_reading_per_primary_usage'] = test['primary_use'].map(median_meter_reading_per_primary_usage)\ntest['std_meter_reading_per_primary_usage'] = test['primary_use'].map(std_meter_reading_per_primary_usage)\n\ntest['mean_meter_reading_per_site_id'] = test['site_id'].map(mean_meter_reading_per_site_id)\ntest['median_meter_reading_per_site_id'] = test['site_id'].map(median_meter_reading_per_site_id)\ntest['std_meter_reading_per_site_id'] = test['site_id'].map(std_meter_reading_per_site_id)","33eb7487":"%%time\nfor df in [train, test]:\n    df['mean_meter_reading_per_building'] = df['mean_meter_reading_per_building'].astype(\"float16\")\n    df['median_meter_reading_per_building'] = df['mean_meter_reading_per_building'].astype(\"float16\")\n    df['std_meter_reading_per_building'] = df['std_meter_reading_per_building'].astype(\"float16\")\n    \n    df['mean_meter_reading_on_year_built'] = df['mean_meter_reading_on_year_built'].astype(\"float16\")\n    df['median_meter_reading_on_year_built'] = df['median_meter_reading_on_year_built'].astype(\"float16\")\n    df['std_meter_reading_on_year_built'] = df['std_meter_reading_on_year_built'].astype(\"float16\")\n    \n    df['mean_meter_reading_per_meter'] = df['mean_meter_reading_per_meter'].astype(\"float16\")\n    df['median_meter_reading_per_meter'] = df['median_meter_reading_per_meter'].astype(\"float16\")\n    df['std_meter_reading_per_meter'] = df['std_meter_reading_per_meter'].astype(\"float16\")\n    \n    df['mean_meter_reading_per_primary_usage'] = df['mean_meter_reading_per_primary_usage'].astype(\"float16\")\n    df['median_meter_reading_per_primary_usage'] = df['median_meter_reading_per_primary_usage'].astype(\"float16\")\n    df['std_meter_reading_per_primary_usage'] = df['std_meter_reading_per_primary_usage'].astype(\"float16\")\n    \n    df['mean_meter_reading_per_site_id'] = df['mean_meter_reading_per_site_id'].astype(\"float16\")\n    df['median_meter_reading_per_site_id'] = df['median_meter_reading_per_site_id'].astype(\"float16\")\n    df['std_meter_reading_per_site_id'] = df['std_meter_reading_per_site_id'].astype(\"float16\")\n    \n    df['number_unique_meter_per_building'] = df['number_unique_meter_per_building'].astype('uint8')\ngc.collect()","3c35c1e4":"train.drop('timestamp',axis=1,inplace=True)\ntest.drop('timestamp',axis=1,inplace=True)\n\nle = LabelEncoder()\n\ntrain['meter']= le.fit_transform(train['meter']).astype(\"uint8\")\ntest['meter']= le.fit_transform(test['meter']).astype(\"uint8\")\ntrain['primary_use']= le.fit_transform(train['primary_use']).astype(\"uint8\")\ntest['primary_use']= le.fit_transform(test['primary_use']).astype(\"uint8\")\n\nprint (train.shape, test.shape)","59068b1b":"%%time\n# Let's check the correlation between the variables and eliminate the one's that have high correlation\n# Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint (\"Following columns can be dropped {}\".format(to_drop))\n\ntrain.drop(to_drop,axis=1,inplace=True)\ntest.drop(to_drop,axis=1,inplace=True)","1d6b095d":"%%time\ny = train['meter_reading']\ntrain.drop('meter_reading',axis=1,inplace=True)\ncategorical_cols = ['building_id','Month','meter','Hour','primary_use','DayOfWeek','DayOfMonth']","c767afbb":"meter_cut, bins = pd.cut(y, bins=50, retbins=True)\nmeter_cut.value_counts()","8de653df":"# x_train,x_test,y_train,y_test = train_test_split(train,y,test_size=0.2,random_state=42, stratify=meter_cut)\nx_train,x_test,y_train,y_test = train_test_split(train,y,test_size=0.1,random_state=42)\nprint (x_train.shape)\nprint (y_train.shape)\nprint (x_test.shape)\nprint (y_test.shape)","91ca7a5f":"x_train.head()","353c3a2a":"from sklearn.ensemble import RandomForestRegressor as RF\nimport lightgbm as lgb","be7cd465":"lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_cols)\nlgb_test = lgb.Dataset(x_test, y_test, categorical_feature=categorical_cols)\ndel x_train, x_test , y_train, y_test\n\nparams = {'feature_fraction': 0.85, # 0.75\n          'bagging_fraction': 0.75,\n          'objective': 'regression',\n           \"num_leaves\": 40, # New\n          'max_depth': -1,\n          'learning_rate': 0.15,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'rmse',\n          \"verbosity\": -1,\n          'reg_alpha': 0.5,\n          'reg_lambda': 0.5,\n          'random_state': 47\n         }\n\nreg = lgb.train(params, lgb_train, num_boost_round=3000, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=100, verbose_eval=100)","bab57cae":"del lgb_train,lgb_test\nser = pd.DataFrame(reg.feature_importance(),train.columns,columns=['Importance']).sort_values(by='Importance')\nser['Importance'].plot(kind='bar',figsize=(10,6))","7a48c5cb":"del train","2709b703":"%%time\npredictions = []\nstep = 50000\nfor i in range(0, len(test), step):\n    predictions.extend(np.expm1(reg.predict(test.iloc[i: min(i+step, len(test)), :], num_iteration=reg.best_iteration)))","f47666ab":"%%time\nSubmission = pd.DataFrame(test.index,columns=['row_id'])\nSubmission['meter_reading'] = predictions\nSubmission['meter_reading'].clip(lower=0,upper=None,inplace=True)\nSubmission.to_csv(\"lgbm_fill_na.csv\",index=None)","261b7680":"x_train = pd.get_dummies(x_train, columns=categorical_cols, sparse=True)\n\nx_test = pd.get_dummies(x_test, columns=categorical_cols, sparse=True)\n\ngc.collect()\n\nx_train.shape","f61597af":"Data overview","395c071f":"Encode features","d198185a":"Preprocess metadata \n","85c3e5d2":"Load the data reducing its size","4b1fed54":"Merge data","f1e771ad":"Improve data readability","2389bcfe":"Construct date features","5829b9ea":"Prepare data","080987b6":"This notebook is a shorter version of a great [EDA and model](https:\/\/www.kaggle.com\/kulkarnivishwanath\/ashrae-great-energy-predictor-iii-eda-model) notebook. It does not provide data overview, just generates the final dataframe and trains a LightGBM classifier. It can be used as a relatively quick baseline for your submission.","dcf37596":"## Predict","380ed4aa":"Imports","c580887a":"Check feature importance","4d1c4413":"Measure meter stats","ed471b40":"## Model\n\nTrain baseline model","4fb5ed88":"Drop some columns ased on EDA","601da127":"Make dummies if necessary -- for RF","6f2c6ab8":"Drop correlated variables","a992b4d6":"Split the data for train and validation with stratification by meter reading bins","958968ec":"Convert target to log scale"}}