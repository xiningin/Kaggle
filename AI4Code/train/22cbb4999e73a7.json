{"cell_type":{"8e75b0fc":"code","b2bd83b8":"code","9d072c3d":"code","1c45b2c3":"code","52558ac6":"code","f6cc74f7":"code","17991a28":"code","47f293fb":"code","44f2a5fb":"code","06508a8b":"code","8311b3af":"code","e51da1af":"code","5f43ba1b":"code","82bbfa48":"code","b4e2016b":"code","c16fb5ea":"code","6a580a7b":"code","4ee63f05":"code","3891dddb":"code","cbdbebae":"code","5051ec14":"code","444306d5":"code","072c232d":"code","4c69326b":"markdown","3e59d3ba":"markdown","a61d97c6":"markdown","0555a14b":"markdown","4ea50497":"markdown","bb4593a0":"markdown","4dea5eff":"markdown","f1f5f544":"markdown","1bdecdc1":"markdown","789f6872":"markdown","672c098a":"markdown","20ec92c6":"markdown","9444525b":"markdown","c002c03b":"markdown","0910e11c":"markdown","c556d283":"markdown"},"source":{"8e75b0fc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport time\nfrom sklearn import preprocessing\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\nimport gc\nfrom tqdm import tqdm\n\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom scipy.stats import describe\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n# Any results you write to the current directory are saved as output.","b2bd83b8":"import os\nprint(os.listdir(\"..\/input\"))","9d072c3d":"#Loading Train and Test Data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"{} observations and {} features in train set.\".format(train.shape[0],train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(test.shape[0],test.shape[1]))","1c45b2c3":"train.head()","52558ac6":"test.head()","f6cc74f7":"train.target.describe()","17991a28":"plt.figure(figsize=(12, 5))\nplt.hist(train.target.values, bins=200)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","47f293fb":"plt.figure(figsize=(12, 5))\nplt.hist(train['muggy-smalt-axolotl-pembus'].values, bins=200)\nplt.title('Histogram muggy-smalt-axolotl-pembus counts')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","44f2a5fb":"plt.figure(figsize=(12, 5))\nplt.hist(train['dorky-peach-sheepdog-ordinal'].values, bins=200)\nplt.title('Histogram muggy-smalt-axolotl-pembus counts')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","06508a8b":"\n\nplt.figure(figsize=(12, 5))\nplt.hist(train['crabby-teal-otter-unsorted'].values, bins=200)\nplt.title('Histogram muggy-smalt-axolotl-pembus counts')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","8311b3af":"\nplt.figure(figsize=(12, 5))\nplt.hist(train['wheezy-copper-turtle-magic'].values, bins=1000)\nplt.title('Histogram muggy-smalt-axolotl-pembus counts')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","e51da1af":"train.describe()","5f43ba1b":"test.describe()","82bbfa48":"def normal(train, test):\n    print('Scaling with StandardScaler\\n')\n    len_train = len(train)\n\n    traintest = pd.concat([train,test], axis=0, ignore_index=True).reset_index(drop=True)\n    \n    scaler = StandardScaler()\n    cols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n    traintest[cols] = scaler.fit_transform(traintest[cols])\n    traintest['wheezy-copper-turtle-magic'] = traintest['wheezy-copper-turtle-magic'].astype('category')\n    train = traintest[:len_train].reset_index(drop=True)\n    test = traintest[len_train:].reset_index(drop=True)\n\n    return train, test","b4e2016b":"%%time\ntrain, test = normal(train, test)","c16fb5ea":"%%time\nfeatues_to_use = [c for c in train.columns if c not in ['id', 'target']]\ntarget = train['target']\n#train = train[featues_to_use]\n#test = test[featues_to_use]\n#classifier = LogisticRegression(C=1, solver='sag')\n#cv_score = np.mean(cross_val_score(classifier, train, target, cv=3, scoring='roc_auc'))\n#print(cv_score)","6a580a7b":"%%time\nfolds = KFold(n_splits=10, shuffle=True, random_state=137)\noof = np.zeros(train.shape[0])\npred = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_+1))\n    x_train, y_train = train.iloc[trn_idx][featues_to_use], target.iloc[trn_idx]\n    x_val, y_val = train.iloc[val_idx][featues_to_use], target.iloc[val_idx]\n    classifier = LogisticRegression(C=1, solver='sag')\n    classifier.fit(x_train, y_train)\n    val_pred = classifier.predict_proba(x_val)[:,1]\n    oof[val_idx] = val_pred\n    pred += classifier.predict_proba(test[featues_to_use])[:,1]\/10\n    print(roc_auc_score(y_val, val_pred))\n    \nprint(roc_auc_score(target.values, oof))","4ee63f05":"%%time\n\nNFOLDS = 25\nNVALUES = 512\n\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\nfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=137)\noof_lr = np.zeros(train.shape[0])\npred_lr = np.zeros(test.shape[0])\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_+1))\n    x_train = train.iloc[trn_idx]\n    x_val, y_val = train.iloc[val_idx], target.iloc[val_idx]\n    \n    \n    for i in tqdm(range(NVALUES)):\n        # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n        x_train_2 = x_train[x_train['wheezy-copper-turtle-magic']==i]\n        x_val_2 = x_val[x_val['wheezy-copper-turtle-magic']==i]\n        test_2 = test[test['wheezy-copper-turtle-magic']==i]\n        idx1 = x_train_2.index; idx2 = x_val_2.index; idx3 = test_2.index\n        x_train_2.reset_index(drop=True,inplace=True)\n        x_val_2.reset_index(drop=True,inplace=True)\n        test_2.reset_index(drop=True,inplace=True)\n        clf = LogisticRegression(solver='liblinear',penalty='l1',C=0.05)\n        y_train =x_train_2['target']\n        clf.fit(x_train_2[cols],y_train)\n        \n        oof_lr[idx2] = clf.predict_proba(x_val_2[cols])[:,1]\n        pred_lr[idx3] += clf.predict_proba(test_2[cols])[:,1] \/ NFOLDS\n\n    oof_lr_val = oof_lr[val_idx]\n\n    \n    print(roc_auc_score(y_val, oof_lr_val))\n    \nprint(roc_auc_score(target, oof_lr))","3891dddb":"print(roc_auc_score(target, oof_lr))","cbdbebae":"# INITIALIZE VARIABLES\noof_svm = np.zeros(len(train))\npreds_svm = np.zeros(len(test))\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n# BUILD 512 SEPARATE NON-LINEAR MODELS\nfor i in range(512):\n    \n    # EXTRACT SUBSET OF DATASET WHERE WHEEZY-MAGIC EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD (Using splits=25 scores 0.002 better but is slower)\n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL WITH SUPPORT VECTOR MACHINE\n        clf = SVC(probability=True,kernel='poly',degree=4,gamma='auto')\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof_svm[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds_svm[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        \n    #if i%10==0: print(i)\n        \n# PRINT VALIDATION CV AUC\nauc = roc_auc_score(train['target'],oof_svm)\nprint('CV score =',round(auc,5))","5051ec14":"roc_auc_score(train['target'],0.6*oof_svm+0.4*oof_lr)","444306d5":"'''%%time\n\nparam = {\n    'bagging_freq': 3,\n    'bagging_fraction': 0.8,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.9,\n    'learning_rate': 0.05,\n    'max_depth': 10,  \n    'metric':'auc',\n    'min_data_in_leaf': 82,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 10,\n    'objective': 'binary', \n    'verbosity': 1\n}\n\nfolds = KFold(n_splits=10, shuffle=True, random_state=137)\noof_lgb = np.zeros(train.shape[0])\npred_lgb = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_+1))\n    x_train, y_train = train.iloc[trn_idx][featues_to_use], target.iloc[trn_idx]\n    x_val, y_val = train.iloc[val_idx][featues_to_use], target.iloc[val_idx]\n    trn_data = lgb.Dataset(x_train, label=y_train)\n    val_data = lgb.Dataset(x_val, label=y_val)\n    classifier = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 300)\n\n    val_pred = classifier.predict(x_val, num_iteration=classifier.best_iteration)\n    oof_lgb[val_idx] = val_pred\n    pred_lgb += classifier.predict(test[featues_to_use], num_iteration=classifier.best_iteration)\/10\n    print(roc_auc_score(y_val, val_pred))'''","072c232d":"%%time\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\n\n\n'''submission['target'] = pred_lr\nsubmission.to_csv('submission_0.csv', index=False)\nsubmission['target'] = 0.9*pred_lr + 0.1*pred_lgb\nsubmission.to_csv('submission.csv', index=False)\nsubmission['target'] = 0.8*pred_lr + 0.2*pred_lgb\nsubmission.to_csv('submission_2.csv', index=False)'''\n\n\nsubmission['target'] = 0.6*preds_svm + 0.4*pred_lr\nsubmission.to_csv('submission_3.csv', index=False)\n","4c69326b":"Now we are going to build a simple model. We'll start with a simple Logistic Regression, which will give us a baseline to work with. Then we'll build upon that.","3e59d3ba":"The following part is based on Chris Deotte's Logistic Regression kernel: https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800","a61d97c6":"So this is a pretty standard fare of Kaggle compatition files: `train`, `test` and `sample_submission`. However, this competition also contains a hidden `test` file, that is only accessible to Kaggle. All the code is supposed to run in parallel on this file, but we can't really \"probe\" it. ","0555a14b":"Now there appears to be one feature that is not gaussian:","4ea50497":"Wow, this is a **very** balanced dataset. No surprises, since this is all presumably artificial data.","bb4593a0":"The following is based on Rob Mulla's script https:\/\/www.kaggle.com\/robikscube\/eda-and-baseline-lgb-for-instant-gratification","4dea5eff":"To be continued ...","f1f5f544":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw. I will work on it as my very limited time permits, and hope to expend it in the upcoming days and weeks.\n\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","1bdecdc1":"Let's now look at the distributions of various \"features\"","789f6872":"Now let's take a look at the distributions for all the features:","672c098a":"Wow, those are some impressively looking perfectly normal distributions!\n\n","20ec92c6":"Seems farily straightforward - just ID, 256 anonimous features, and target field for train set.\n\nLet's take a look at the target variable:","9444525b":"The following is Chris Deotte's SVM solution:\n\nhttps:\/\/www.kaggle.com\/cdeotte\/support-vector-machine-0-925","c002c03b":"Let's see what files we have in the input directory:","0910e11c":"Even thought the features seem to be on an approximately same scale, there are some noticable differences. We'll try to deal with them, as per the following kernel - https:\/\/www.kaggle.com\/ilu000\/instagrat-lgbm-baseline","c556d283":"Things to notice:\n\n* Most features appear to be numerical\n* Most features seem perfectly normally distributed\n* Most features seem to be normalized and centered to approximately same values (with some sligth differences)\n* The target is almost perfectly abalnced between positive and negative classes"}}