{"cell_type":{"42bc0cf5":"code","7ece2622":"code","a051026d":"code","15087d80":"code","2d1df65e":"code","0ab08d9e":"code","0a9b8000":"code","553f048b":"code","a4f49673":"code","765a11e3":"code","2b81d8ac":"code","70adc8ab":"code","66490aab":"code","a339f81e":"code","06b7d74f":"code","b4a7754d":"code","a2914f77":"code","d1252539":"code","ddc468e1":"code","2458e8bd":"code","d504da4f":"code","4a4e81af":"code","b779e85d":"code","d1835948":"code","1bc8e2aa":"code","de887a20":"code","2641d758":"code","d39da091":"code","7ae76131":"code","d05c3107":"code","699bbd12":"code","d8d672e7":"code","e78fc48f":"code","92ac8f3a":"code","0faff63a":"code","3b0d0129":"code","3c6a660d":"code","e8092c97":"code","28a3e880":"code","61595a8a":"code","6b59f6b0":"code","46c91c8b":"code","a6beac60":"code","8206f66a":"code","dea42123":"code","0ad2a7b2":"code","8571e0ac":"code","eefa8b71":"code","487b2922":"code","c397449b":"code","2c7c7a7e":"code","7069407d":"markdown","f654c0b2":"markdown","a57a7ffb":"markdown","f85c816e":"markdown","2ae9a478":"markdown","bdeb7008":"markdown","da3ef50b":"markdown","a65bc517":"markdown","bd8ecba8":"markdown","aea35c4b":"markdown","551bcef7":"markdown","e1473a16":"markdown","a9734677":"markdown","929115d0":"markdown","b4768651":"markdown","537185ea":"markdown","8ce4080c":"markdown","91d3c745":"markdown","2f3d5b36":"markdown","a98ea26f":"markdown","e2bc0cd6":"markdown","21d810af":"markdown","030c2055":"markdown","1ada3860":"markdown","dfb34bc4":"markdown","d369b4d9":"markdown","76cda7ec":"markdown","c74f0a29":"markdown","65e0bf84":"markdown","48a5f35e":"markdown","ea4b0a5b":"markdown","0fd5fffb":"markdown","39fff7dd":"markdown","74a39e41":"markdown","bd31043f":"markdown","b0eea109":"markdown","70f7de57":"markdown","0279e989":"markdown","93ff319d":"markdown","35e17efe":"markdown","c6508b41":"markdown","a5e2ee96":"markdown","06fc60a2":"markdown","8283ef00":"markdown","2351a0b6":"markdown","c4721064":"markdown","cffd56f7":"markdown","9da98d9f":"markdown","5111582f":"markdown","da97b7df":"markdown","8041d632":"markdown","5bcd76d0":"markdown","071b0ccf":"markdown"},"source":{"42bc0cf5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ece2622":"data = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")","a051026d":"data.head()","15087d80":"data.columns","2d1df65e":"data.info()","0ab08d9e":"data.describe()","0a9b8000":"data.corr()","553f048b":"f,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.title(\"Corrolation Map\")\nplt.show()","a4f49673":"data[\"Species\"].value_counts()","765a11e3":"sns.countplot(data[\"Species\"], palette=\"Set2\")\nplt.title(\"Amount\")\nplt.show()","2b81d8ac":"sns.pairplot(data=data, hue=\"Species\")\nplt.show()","70adc8ab":"data.drop(\"Id\", axis=1).boxplot(by=\"Species\", figsize=(15, 8), fontsize=\"large\", color=\"r\")","66490aab":"sns.violinplot(x=\"Species\", y=\"SepalLengthCm\", data=data, size=6)\nplt.show()","a339f81e":"sns.violinplot(x=\"Species\", y=\"SepalWidthCm\", data=data, size=6)\nplt.show()","06b7d74f":"sns.violinplot(x=\"Species\", y=\"PetalLengthCm\", data=data, size=6)\nplt.show()","b4a7754d":"sns.violinplot(x=\"Species\", y=\"PetalWidthCm\", data=data, size=6)\nplt.show()","a2914f77":"data.drop([\"Id\"], axis=1, inplace=True)","d1252539":"x_data = data.iloc[:,:4].values\ny = data.iloc[:,4:].values","ddc468e1":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","2458e8bd":"x[:5]","d504da4f":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)","4a4e81af":"data.head()","b779e85d":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\ny_pred_lr = lr.predict(x_test)","d1835948":"#Accuracy\nprint(\"Accuracy for Logistic Regression : \", lr.score(x_test, y_test))","1bc8e2aa":"#Confusion Matrix\ncm_lr = confusion_matrix(y_test, y_pred_lr)\nprint(\"Confusion Matrix for Logistic Regression : \\n\", cm_lr)","de887a20":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train, y_train)\ny_pred_knn = knn.predict(x_test)","2641d758":"#Accuracy\nprint(\" {}nn score: {} \".format(5,knn.score(x_test, y_test)))","d39da091":"score_list = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\nscore_list","7ae76131":"plt.plot(range(1,20),score_list)\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy According to K\")\nplt.show()","d05c3107":"knn3 = KNeighborsClassifier(n_neighbors=6) \nknn3.fit(x_train, y_train)\ny_pred_knn3 = knn3.predict(x_test)\nprint(\" {}nn score: {} \".format(6,knn3.score(x_test, y_test)))","699bbd12":"#Confusion Matrix\ncm_KNN = confusion_matrix(y_test, y_pred_knn3)\nprint(\"Confusion Matrix for KNN : \\n\", cm_KNN)","d8d672e7":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1) # having same randomness\nsvm.fit(x_train, y_train)\ny_pred_svm = svm.predict(x_test)","e78fc48f":"#Accuracy\nprint(\"Accuracy for SVC : \", svm.score(x_test, y_test))","92ac8f3a":"#Confusion Matrix\ncm_svm = confusion_matrix(y_test, y_pred_svm)\nprint(\"Confusion Matrix for SVC : \\n\", cm_svm)","0faff63a":"x_train[50], y_train[50]","3b0d0129":"svm.predict([[0.90025641, 0.45871795, 0.77923077, 0.28769231]]) #change a bit","3c6a660d":"x_train[20], y_train[20]","e8092c97":"svm.predict([[0.65512821, 0.35769231, 0.50717949, 0.10820513]])","28a3e880":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\ny_pred_gnb = gnb.predict(x_test)","61595a8a":"#Accuracy\nprint(\"Accuracy for Naive Bayes : \", gnb.score(x_test, y_test))","6b59f6b0":"#Confusion Matrix\ncm_gnb = confusion_matrix(y_test, y_pred_gnb)\nprint('Gaussian Naive Bayes Classification Confusion Matrix: \\n', cm_gnb)","46c91c8b":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\ny_pred_dt = dt.predict(x_test)","a6beac60":"#Accuracy\nprint(\"Accuracy for Decision Tree : \", dt.score(x_test, y_test))","8206f66a":"#Confusion Matrix\ncm_dt = confusion_matrix(y_test, y_pred_dt)\nprint(\"Confusion Matrix for Decision Tree : \\n\", cm_dt)","dea42123":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=50, random_state=0) #n_estimators = number of decision trees\nrf.fit(x_train, y_train)\ny_pred_rf = rf.predict(x_test)","0ad2a7b2":"#Accuracy\nprint(\"Accuracy for Random Forest : \", rf.score(x_test, y_test))","8571e0ac":"#Confusion Matrix\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nprint(\"Confusion Matrix for Decision Tree : \\n\", cm_dt)","eefa8b71":"from sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier(n_estimators=10, random_state=0)\nabc.fit(x_train, y_train)\ny_pred_abc = abc.predict(x_test)","487b2922":"#Accuracy\nprint(\"Accuracy for Ada Boost : \", abc.score(x_test, y_test))","c397449b":"#Confusion Matrix\ncm_abc = confusion_matrix(y_test, y_pred_abc)\nprint(\"Confusion Matrix for Adaboost : \\n\", cm_abc)","2c7c7a7e":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_abc, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\nplt.xlabel(\"Predictions (y_pred)\")\nplt.ylabel(\"Real (y_test)\")\nplt.title(\"Confusion Matrix\")\nplt.show()","7069407d":"<a id = '11'><\/a><br>\n## Ensemble Learning","f654c0b2":"#### Hopefully, it seems our model working proper with different datas.","a57a7ffb":"### Normalization\n### X_normalized = (x - x minimum)\/(x maximum - x minimum)","f85c816e":"#### For example, random forest uses multiple decision trees. According to each tree mojarity voting making than result is publishing.","2ae9a478":"<a id = '12'><\/a><br>\n## Random Forest","bdeb7008":"#### Logistic Regression model (or logit model) is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.","da3ef50b":"#### As you see, our model predicted all true. But we will try other models too.","a65bc517":"#### If you want to have more idea about logistic regression, there is a notebook detailed with basic math : [Logisctic Regression](https:\/\/www.kaggle.com\/feritebrargrler\/heart-disease-classification-logistic-regression)","bd8ecba8":"![image.png](attachment:61cf3ada-eed8-4ce3-8a18-63b9a57120c3.png)","aea35c4b":"#### Evaluation","551bcef7":"<a id = '9'><\/a><br>\n## Naive Bayes","e1473a16":"#### You can have a look at : [KNN](https:\/\/www.kaggle.com\/feritebrargrler\/classification-with-knn-90-accuracy)","a9734677":"#### Most of the features well seperated and nice distributioned.","929115d0":"<a id = '14'><\/a><br>\n## Conclusion\n#### In this tutorial, we read data to having information about it after that, preproccessing and visualization. Finally, we tried machine learning algorithms on data and evaluated them. More then one algorithms achived succes.","b4768651":"<a id = '13'><\/a><br>\n## Ada Booster","537185ea":"#### Now its time to split to train and test datas.","8ce4080c":"<a id = '5'><\/a><br>\n## Machine Learning Classification Models","91d3c745":"#### 9 wrong predictions for Logistic Regression","2f3d5b36":"<a id = '6'><\/a><br>\n## Logistic Regression","a98ea26f":"![image.png](attachment:1601b843-78d0-4155-8c58-188a6bac9e18.png)![image.png](attachment:d5d79457-17a8-4778-9e20-0ce7ca5bbd0e.png)","e2bc0cd6":"![image.png](attachment:0e2964b3-79e8-451f-a542-4c7783cdae00.png)","21d810af":"#### A support vector machine can be defined as a vector space based machine learning method that finds a decision margin between the two classes that are furthest from any point in the training data. The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points. The aim is finding maximum margin","030c2055":"<a id = '2'><\/a><br>\n## Read Data","1ada3860":"![image.png](attachment:987b5d19-0ae7-4df2-a337-32ae2f8cd78b.png)","dfb34bc4":"![irises.png](attachment:50eeb492-afda-47ca-a0cd-d43de8a97507.png)","d369b4d9":"#### Columns are mostly corrolated each other.","76cda7ec":"# Content\n1. [Introduction](#1)    \n1. [Read Data](#2)\n1. [Visualization](#3)\n1. [Data Preproccesing](#4) \n1. [Machine Learning Classification Models](#5) \n    * [Logistic Regression](#6)\n    * [KNN](#7)\n    * [SVM](#8)\n    * [Naive Bayes](#9)\n    * [Decision Tree](#10)\n    * [Ensemble Learning](#11)\n        * [Random Forest](#12)\n        * [Ada Boost](#13)\n1. [Conclusion](#14) ","c74f0a29":"#### After proccessing data, lets try models on it.","65e0bf84":"#### Evaluation","48a5f35e":"<a id = '3'><\/a><br>\n## Visualization","ea4b0a5b":"#### As you see, they scaled.","0fd5fffb":"<a id = '4'><\/a><br>\n## Data Preproccesing","39fff7dd":"#### First lets have a look at distribution of target values","74a39e41":"<a id = '8'><\/a><br>\n## Support Vector Machine (SVM)","bd31043f":"<a id = '7'><\/a><br>\n## K Nearest Neighbors (KNN)","b0eea109":"#### Finding Optimum K","70f7de57":"#### A decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails) , each leaf node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. The paths from root to leaf represent classification rules.","0279e989":"### P(A|B) = P(B|A) x P(A) \/ P(B)\n### Posterior = Prior x Likelihood \/ Evidance","93ff319d":"![image.png](attachment:a03e483a-5d59-4991-a463-1c1fa94716c7.png)","35e17efe":"![image.png](attachment:11f2ca2b-1a95-4a7e-a5b6-afeec14262dc.png)","c6508b41":"#### This algorithm based on Naive Bayes theorem (Conditional Probability):","a5e2ee96":"#### We need to make normalization on data because in this data there are some values like 5.1, 0.2. This may cause overtower between datas on features. To prevent this, we are doing normalization","06fc60a2":"#### Again fully right. This seems weird to me. I want to check with different numbers for control it is overfitted or not ?","8283ef00":"<a id = '10'><\/a><br>\n## Decision Trees","2351a0b6":"#### Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Ensemble learning is primarily used to improve the (classification, prediction, function approximation, etc.) Ensemble Learning models can be more powerful because they use more than one model (depending on the problem)","c4721064":"#### Evaluation","cffd56f7":"#### Visualization of Confusion Matrix","9da98d9f":"#### Lets check corrolation between features:","5111582f":"#### Evaluation ","da97b7df":"#### KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification). KNN works on a principle assuming every data point falling in near to each other is falling in the same class. In other words it classifies a new data point based on similarity. As an illustration, if K = 5 : The algorithm would calculate the the distance of 5 nearest neigbhors for the data that will be predicted then according to result, for example 4 category A and 1 category B, the classification will be made as category A.","8041d632":"<a id = '1'><\/a><br>\n## Introduction\n#### In this tutorial, we are going to try supervised learning methods (espacially classification) on well-known dataset which is iris.","5bcd76d0":"#### First Id column is unneed for analysis. So, lets drop it. After that they must be split as x and y.","071b0ccf":"#### These are columns. Lets have a look if there are any null values"}}