{"cell_type":{"aface96c":"code","5b234f0d":"code","cafe4fcf":"code","1affdbd2":"code","77d0aa2c":"code","b7b6737e":"code","0e161691":"code","6b90eb9c":"code","debcee42":"code","8d897fea":"code","fdc8ada0":"code","8fd7a5f5":"code","d02bb4ba":"code","eeb7fc32":"code","da0b17d8":"code","086b5b60":"code","efbbd86d":"code","f8cc4ce8":"code","46c02ce4":"code","a639ca9d":"code","a10b83a2":"code","61c14800":"code","07d98491":"code","b7690a59":"code","64080c4a":"markdown"},"source":{"aface96c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport spacy\nimport os\nfrom time import time\nimport re  # For preprocessing\n\nfrom gensim.models.phrases import Phrases, Phraser\nfrom collections import defaultdict \n\nimport multiprocessing\nfrom gensim.models import Word2Vec","5b234f0d":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cafe4fcf":"filenames[0]","1affdbd2":"newsdf = pd.DataFrame()\ndfs = []\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames[:10]:\n        if filename.lower().endswith((\".json\")): \n            print(os.path.join(dirname, filename))\n            data = pd.read_json(os.path.join(dirname, filename), lines=True)\n            dfs.append(data)\nnewsdf = pd.concat(dfs, ignore_index = True)\nnewsdf = newsdf[['title','text', 'author', 'published']]","77d0aa2c":"newsdf.info()","b7b6737e":"newsdf.shape","0e161691":"newsdf.columns","6b90eb9c":"newsdf.isnull().sum()","debcee42":"newsdf.loc[:,:]","8d897fea":"df = newsdf[['title','text']]\ndf","fdc8ada0":"df.isnull().sum()","8fd7a5f5":"nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n\ndef cleaning(doc):\n    # Lemmatizes and removes stopwords\n    # doc needs to be a spacy Doc object\n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    # Word2Vec uses context words to learn the vector representation of a target word,\n    # if a sentence is only one or two words long,\n    # the benefit for the training is very small\n    if len(txt) > 2:\n        return ' '.join(txt)\n\ntext_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text'])\nt = time()\n\ntxt = [cleaning(doc) for doc in nlp.pipe(text_cleaning, batch_size=5000, n_threads=-1)]\n\nprint('Time to clean up everything: {} mins'.format(round((time() - t) \/ 60, 2)))","d02bb4ba":"df_clean = pd.DataFrame({'clean': txt})\ndf_clean = df_clean.dropna().drop_duplicates()\ndf_clean.shape","eeb7fc32":"sent = [row.split() for row in df_clean['clean']]\nlen(sent)","da0b17d8":"phrases = Phrases(sent, min_count=30, progress_per=10000)\nbigram = Phraser(phrases)\nsentences = bigram[sent]\nsentences","086b5b60":"word_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","efbbd86d":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","f8cc4ce8":"cores = multiprocessing.cpu_count()\n","46c02ce4":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","a639ca9d":"t = time()\n\nw2v_model.build_vocab(sentences, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))","a10b83a2":"t = time()\n\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))","61c14800":"w2v_model.init_sims(replace=True)","07d98491":"w2v_model.wv.most_similar(positive=[\"share\"])","b7690a59":"w2v_model.wv.most_similar(negative=[\"share\"])","64080c4a":"# W2V Prameters\n* min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n* window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n* size = int - Dimensionality of the feature vectors. - (50, 300)\n* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n* alpha = float - The initial learning rate - (0.01, 0.05)\n* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)"}}