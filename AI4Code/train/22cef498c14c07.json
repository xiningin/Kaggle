{"cell_type":{"f1a37bb6":"code","dfc3dc34":"code","109e511e":"code","718574b4":"code","c1fac6ff":"code","bebc7de4":"code","fbd87922":"code","c12a22f9":"code","1d6c21f1":"code","c7359dab":"code","9ce18cbe":"code","cab08637":"code","9bda63cd":"code","5fdfa6b8":"code","4df3b27f":"code","f9420bb5":"code","cc1338d4":"code","27ce228b":"code","4996d26e":"code","d1257d22":"code","a36898bd":"code","c575830c":"code","397aaa31":"code","f5bb854e":"code","29bdb655":"code","920c4eb8":"code","4071f5fd":"code","356f140e":"code","03caad51":"code","a6b96e8c":"code","99f4ff4b":"code","7f7842df":"code","a589a739":"markdown","e450f0b0":"markdown","9cb5a7dc":"markdown","b03be696":"markdown","cd688b52":"markdown","8b31f729":"markdown","9b291075":"markdown","e33d8abc":"markdown","cbe7028f":"markdown","aceaef6a":"markdown","35e4320d":"markdown","1a5c961b":"markdown","a0c1940d":"markdown","d8d4c7d9":"markdown","dc4284bb":"markdown","8f9bc903":"markdown","d0d0152c":"markdown","35c498d5":"markdown","90a9b989":"markdown","c42706aa":"markdown","786939d1":"markdown","1ca3c4bf":"markdown","d59179a5":"markdown","2e39c80e":"markdown","8ef33f0e":"markdown","4d263ee6":"markdown","214677fc":"markdown","faf6d036":"markdown","595bf6ca":"markdown","afa8bebe":"markdown","8fcb4107":"markdown","4f1d648f":"markdown","74fb1bfd":"markdown","ff88181b":"markdown","cc7371af":"markdown","a727c8e4":"markdown","056c9937":"markdown","1e1deb93":"markdown","639a7f9f":"markdown","c666ce88":"markdown","dc56e490":"markdown"},"source":{"f1a37bb6":"import pandas as pd\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndiabetes_data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\nX =  diabetes_data.drop([\"Outcome\"],axis = 1)\ny = diabetes_data[\"Outcome\"]\n\n# Train multiple models with various hyperparameters using the training set, select the model and hyperparameters that perform best on the validation set.\n# Once model type and hyperparameters have been selected, train final model using these hyperparameters on the full training set, the generalized error is finally measured on the test set.\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 56)\n\n# StratifiedKFold class performs stratified sampling to produce folds that contain a representative ratio of each class.\ncv = StratifiedKFold(n_splits=10, shuffle = False, random_state = 76)\n\n# Logistic Regression\nclf_logreg = LogisticRegression()\n# fit model\nclf_logreg.fit(X_train, y_train)\n# Make class predictions for the validation set.\ny_pred_class_logreg = cross_val_predict(clf_logreg, X_train, y_train, cv = cv)\n# predicted probabilities for class 1, probabilities of positive class\ny_pred_prob_logreg = cross_val_predict(clf_logreg, X_train, y_train, cv = cv, method=\"predict_proba\")\ny_pred_prob_logreg_class1 = y_pred_prob_logreg[:, 1]\n\n# SGD Classifier\nclf_SGD = SGDClassifier()\n# fit model\nclf_SGD.fit(X_train, y_train)\n# make class predictions for the validation set\ny_pred_class_SGD = cross_val_predict(clf_SGD, X_train, y_train, cv = cv)\n# predicted probabilities for class 1\ny_pred_prob_SGD = cross_val_predict(clf_SGD, X_train, y_train, cv = cv, method=\"decision_function\")\n\n# Random Forest Classifier\nclf_rfc = RandomForestClassifier()\n# fit model\nclf_rfc.fit(X_train, y_train)\n# make class predictions for the validation set\ny_pred_class_rfc = cross_val_predict(clf_rfc, X_train, y_train, cv = cv)\n# predicted probabilities for class 1\ny_pred_prob_rfc = cross_val_predict(clf_rfc, X_train, y_train, cv = cv, method=\"predict_proba\")\ny_pred_prob_rfc_class1 = y_pred_prob_rfc[:, 1]","dfc3dc34":"from sklearn.base import BaseEstimator\nimport numpy as np\n\nclass BaseClassifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n    \nbase_clf = BaseClassifier()\ncross_val_score(base_clf, X_train, y_train, cv=10, scoring=\"accuracy\").mean()\n\n\n# Method 2\n# calculate null accuracy (for binary \/ multi-class classification problems)\n# null_accuracy = y_train.value_counts().head(1) \/ len(y_train)","109e511e":"# calculate accuracy\n\nacc_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\nacc_SGD = cross_val_score(clf_SGD, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\nacc_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n\nacc_logreg, acc_SGD, acc_rfc","718574b4":"# calculate logloss\n\nlogloss_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()\nlogloss_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()\n\n# SGDClassifier's hinge loss doesn't support probability estimates.\n# We can set SGDClassifier as the base estimator in Scikit-learn's CalibratedClassifierCV, which will generate probability estimates.\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\nnew_clf_SGD = CalibratedClassifierCV(clf_SGD)\nnew_clf_SGD.fit(X_train, y_train)\nlogloss_SGD = cross_val_score(new_clf_SGD, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()\n\nlogloss_logreg, logloss_SGD, logloss_rfc","c1fac6ff":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\n\n# we pass y_test and y_pred_prob\n# we do not use y_pred_class, because it will give incorrect results without generating an error\n# roc_curve returns 3 objects false positive rate(fpr), true positive rate(tpr), thresholds\n\nfpr_logreg, tpr_logreg, thresholds_logreg = metrics.roc_curve(y_train, y_pred_prob_logreg_class1)\nfpr_rfc, tpr_rfc, thresholds_rfc = metrics.roc_curve(y_train, y_pred_prob_rfc_class1)\nfpr_SGD, tpr_SGD, thresholds_SGD = metrics.roc_curve(y_train, y_pred_prob_SGD)\n\nplt.plot(fpr_logreg, tpr_logreg, label=\"logreg\")\nplt.plot(fpr_rfc, tpr_rfc, label=\"rfc\")\nplt.plot(fpr_SGD, tpr_SGD, label=\"SGD\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True)","bebc7de4":"# define a function that accepts a threshold and prints sensitivity and specificity\ndef evaluate_threshold(tpr, fpr,clf_threshold, threshold):\n    print('Sensitivity:', tpr[clf_threshold > threshold][-1])\n    print('Specificity:', 1 - fpr[clf_threshold > threshold][-1])","fbd87922":"# Logistic Regression\nevaluate_threshold(tpr_logreg, fpr_logreg, thresholds_logreg, 0.2), evaluate_threshold(tpr_logreg, fpr_logreg, thresholds_logreg, 0.8)","c12a22f9":"# Random Forest Classifier\nevaluate_threshold(tpr_rfc, fpr_rfc, thresholds_rfc, 0.2), evaluate_threshold(tpr_rfc, fpr_rfc, thresholds_rfc, 0.8)","1d6c21f1":"# SGD\nevaluate_threshold(tpr_SGD, fpr_SGD, thresholds_SGD, 0.2), evaluate_threshold(tpr_SGD, fpr_SGD, thresholds_SGD, 0.8)","c7359dab":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\n# print(metrics.roc_auc_score(y_test, y_pred_prob))","9ce18cbe":"roc_auc_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = 'roc_auc').mean()\nroc_auc_SGD = cross_val_score(clf_SGD, X_train, y_train, cv = cv, scoring = 'roc_auc').mean()\nroc_auc_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = 'roc_auc').mean()\n\nroc_auc_logreg, roc_auc_SGD, roc_auc_rfc","cab08637":"logreg_matrix = metrics.confusion_matrix(y_train, y_pred_class_logreg)\nprint(logreg_matrix)","9bda63cd":"SGD_matrix = metrics.confusion_matrix(y_train, y_pred_class_SGD)\nprint(SGD_matrix)","5fdfa6b8":"rfc_matrix = metrics.confusion_matrix(y_train, y_pred_class_rfc)\nprint(rfc_matrix)","4df3b27f":"report_logreg = metrics.classification_report(y_train, y_pred_class_logreg)   \nreport_SGD = metrics.classification_report(y_train, y_pred_class_SGD)\nreport_rfc = metrics.classification_report(y_train, y_pred_class_rfc)\nprint(\"report_logreg \" +  \"\\n\" + report_logreg,\"report_SGD \"  +  \"\\n\" +  report_SGD,\"report_rfc \"  +  \"\\n\" +  report_rfc, sep = \"\\n\")","f9420bb5":"y_decision_function_scores = clf_logreg.decision_function(X_train)\ny_decision_function_scores[6]","cc1338d4":"threshold = 0\ny_decision_function_pred = (y_decision_function_scores[6] > threshold)\ny_decision_function_pred","27ce228b":"threshold = 2\ny_decision_function_pred = (y_decision_function_scores[6] > threshold)\ny_decision_function_pred","4996d26e":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_pred_prob_logreg_class1)\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0, 1])\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","d1257d22":"from sklearn.metrics import precision_score, recall_score\n\ny_pred_90 = (y_pred_prob_logreg_class1 > 0.32)\n\nprecisionScore = precision_score(y_train, y_pred_90)\nrecallScore = recall_score(y_train, y_pred_90)\nprecisionScore, recallScore","a36898bd":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\nhousing_data = pd.read_csv('..\/input\/boston-house-prices\/housing.csv', delim_whitespace=True, names=names)\nhousing_data.head(2)\n\nX =  housing_data.drop([\"MEDV\"],axis = 1)\ny = housing_data[\"MEDV\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nmodel = LinearRegression()\n\n# fit model\nmodel.fit(X_train, y_train)\n\n# make class predictions for the testing set\ny_pred_class = model.predict(X_test)","c575830c":"# calculate Mean Absolute Error\n\nprint(metrics.mean_absolute_error(y_test, y_pred_class))","397aaa31":"# calculate Mean Squared Error\n\nprint(metrics.mean_squared_error(y_test, y_pred_class))","f5bb854e":"# calculate Root Mean Squared Error\n\nfrom math import sqrt\n\nprint(sqrt(metrics.mean_squared_error(y_test, y_pred_class)))","29bdb655":"# calculate Mean Squared Log Error\n\nprint(metrics.mean_squared_log_error(y_test, y_pred_class))","920c4eb8":"# calculate R2 score\n\nprint(metrics.r2_score(y_test, y_pred_class))","4071f5fd":"import statsmodels.api as sm\n\nX_train_2 = sm.add_constant(X_train) \nest = sm.OLS(y_train, X_train_2)\nest2 = est.fit()\n\nprint(\"summary()\\n\",est2.summary())","356f140e":"from nltk.translate.bleu_score import sentence_bleu\nreference = [['the', 'cat',\"is\",\"sitting\",\"on\",\"the\",\"mat\"]]\nMachine_translation_1 = [\"on\",'the',\"mat\",\"is\",\"a\",\"cat\"]\nMachine_translation_2 = [\"there\",'is',\"cat\",\"sitting\",\"cat\"]\nMachine_translation_3 = ['the', 'cat',\"is\",\"sitting\",\"on\",\"the\",\"tam\"]\nscore1 = sentence_bleu(reference, Machine_translation_1)\nscore2 = sentence_bleu(reference, Machine_translation_2)\nscore3 = sentence_bleu(reference, Machine_translation_3)\nscore1, score2, score3","03caad51":"# from sklearn.multiclass import OneVsOneClassifier\n\n# ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\n# ovo_clf.fit(X_train, y_train)","a6b96e8c":"# Dataset - MNIST\n\n# from sklearn.neighbors import KNeighborsClassifier\n\n# y_train_large = (y_train >= 7)\n# y_train_odd = (y_train % 2 == 1)\n# y_multilabel = np.c_[y_train_large, y_train_odd]\n# knn_clf = KNeighborsClassifier()\n# knn_clf.fit(X_train, y_multilabel)","99f4ff4b":"# knn_clf.predict([5])\n\n# output: array([[False, True]], dtype=bool)","7f7842df":"# y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv = 10)\n# f1_score(y_train, y_train_knn_pred, average=\"macro\")","a589a739":"### <u>Precision - Recall Tradeoff<\/u>\n\nIn some contexts we mostly care about precision, and in other contexts we care about recall. For example, if we trained a classifier to detect videos that are safe for kids, ew would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision). On the other hand, suppose we train a classifier to detect shoplifters on surveillance images: it is probably fine if our classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught).\n\nIncreasing Precision reduces Recall, and vice versa. This is called the <i> Precision - Recall Tradeoff <\/i>.\n\nTo understand this tradeoff, let\u2019s look at how the SGDClassifier \/ LogisticRegression \/ RandomForestClassifier makes their classification decisions. For each instance, they computes a score based on a decision function \/ predict_proba, and if that score is greater than a threshold, they assigns the instance to the positive class, or else it assigns it to the negative class.\n\nFigure below shows a few digits positioned from the lowest score on the left to the highest score on the right, the task is to predict number \"5\" from the images. Suppose the decision threshold is positioned at the central arrow: we will find 4 true positives (actual 5s) on the right of that threshold, and one false positive (6). With that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if we raise the threshold (move it to the arrow on the right), the false positive (6) becomes a true negative, thereby increasing precision (up to 100% in this case), but one true positive becomes a false negative, decreasing recall down to 50%. Conversely, lowering the threshold increases recall and reduces precision.\n\n![Screen%20Shot%202019-11-08%20at%2017.40.37.png](attachment:Screen%20Shot%202019-11-08%20at%2017.40.37.png)\n\n","e450f0b0":"Now we can simply select the threshold value that gives us the best precision\/recall tradeoff for our task. let\u2019s suppose you decide to aim for 80% recall. You look up the first plot (zooming in a bit) and find that you need to use a threshold of about 0.32. To make predictions (on the training set for now), instead of calling the classifier\u2019s predict() method, you can just run this code:","9cb5a7dc":"### <u> R_squared <\/u>\n\nIn the case of a classification problem, if the model has an accuracy of 0.8, we could gauge how good our model is against a random model, which has an accuracy of  0.5. So the random model can be treated as a benchmark. But when we talk about the RMSE metrics, we do not have a benchmark to compare.\n\nThis is where we can use R-Squared metric. The formula for R-Squared is as follows:\n\n$$R^2 = 1 - \\frac{MSE(model)}{MSE(baseline)} = 1 - \\frac{\\sum_{i=1}^{N}(y_1 - \\hat{y_1})^2}{\\sum_{i=1}^{N}(\\bar{y_1} - \\hat{y_1})^2}$$\n\nMSE(model): Mean Squared Error of the predictions against the actual values\n\nMSE(baseline): Mean Squared Error of  mean prediction against the actual values\n\nIn other words how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.\n- A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value.\n- Range[- infinity, 1)","b03be696":"#### Comparison of Adjusted R\u00b2 over RMSE\nAbsolute value of RMSE does not actually tell how good\/bad a model is. It can only be used to compare across two models whereas Adjusted R\u00b2 easily does that. For example, if a model has adjusted R\u00b2 equal to 0.05 then it is definitely bad.\n\nHowever, if we care only about prediction accuracy then RMSE is best. It is computationally simple, easily differentiable and present as default metric for most of the models.","cd688b52":"This assumes that all labels are equally important, which may not be the case. In particular, if we have many more pictures of Alice than of Bob or Charlie, we may want to give more weight to the classifier\u2019s score on pictures of Alice. One simple option is to give each label a weight equal to its support (i.e., the number of instances with that target label). To do this, simply set average=\"weighted\" in the preceding code.","8b31f729":"### <u>Confusion Matrix <\/u>\n\nA confusion matrix is an N X N matrix, where N is the number of classes being predicted. Confusion Matrix gives us a matrix as output and describes the complete performance of the model.\n\nThe correct predictions falls on the diagonal line of the matrix.\n\n4 important terms in Confusion Matrix:\n- True Positives : The cases in which we predicted YES and the actual output was also YES.\n- True Negatives : The cases in which we predicted NO and the actual output was NO.\n- False Positives : The cases in which we predicted YES and the actual output was NO.\n- False Negatives : The cases in which we predicted NO and the actual output was YES.\n\nThe Confusion matrix in itself is not a performance measure as such, but almost all of the performance metrics are based on Confusion Matrix and the numbers inside it.","9b291075":"Scikit-Learn does not let us set the threshold directly, but it does give us access to the decision scores that it uses to make predictions. Instead of calling the classifier\u2019s predict() method, we can call its decision_function() method, which returns a score for each instance, and then make predictions based on those scores using any threshold we want:","e33d8abc":"* <b>Quick Note <\/b>: SkLearn's \"predict_log_proba\" gives the logarithm of the probabilities, this is often handier as probabilities can become very, very small.","cbe7028f":"## <u>Bonus<\/u> :\n","aceaef6a":"## <u>Multi-Class Classification:<\/u>\n\nMultiClass Classifiers can distinguish between more than two classes.\n\nRandom Forest Classifiers or Naive Bayes Classifiers are capable of handling multiple classes directly. Others (Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers.\n\n#### Task: 0 - 9 digits classification\n\n### <u> One vs All (OvA) Classification Strategy : <\/u>\n\nTrain 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). When we want to classify an image, we get the decision score from each classifier for that image and we select the class whose classifier outputs the highest score.\n\n<b>Example <\/b>: Almost all classification algorithms.\n\n### <u> One vs One (OvO) Strategy : <\/u>\nTrain a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. If there are N classes, we need to train N \u00d7 (N \u2013 1) \/ 2 classifiers. For the MNIST problem, this means training 45 binary classifiers. When we want to classify an image, we have to run the image through all 45 classifiers and see which class wins the most duels. Main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.\n\n<b>Example <\/b>: Support Vector Machines scale poorly with the size of the training set, it is faster to train many classifiers on small training sets than training few classifiers on large training sets.\n\nScikit-Learn detects when we try to use a binary classification algorithm for a multi\u2010 class classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO).For MNIST problem, Under the hood, Scikit-Learn trained 10 binary classifiers, get their decision scores for the image, and selected the class with the highest score.\n\nIf we want to force ScikitLearn to use OvO or OvA, we can use the OneVsOneClassifier or OneVsRestClassifier classes.","35e4320d":"## <u> Regression Metrices <\/u>\n\n- Dataset:  Boston House Price dataset.\n- Evaluation Algorithm: Logistic Regression.","1a5c961b":"#### Why not Mean Squared Error as a loss function for Logistic Regression ?\n\nEquation for both the loss functions are as follows:\n\n$$\\text{log loss = } -\\sum_{i=1}^{n}y_i\\log{(\\hat{y_i})} + (1 - y_i) \\log(1 - \\hat{y_i})$$\n$$\\text{MSE} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n\n- Answer - 1:\n     - Let us compute the loss value when there is a complete mismatch between actual value(ex. 1) and predicted value(ex. 0). $Loss_{mse}$ = 1, $Loss_{logloss} = \\infty$. <b>MSE doesn't strongly penalize misclassifications even for the perfect mismatch<\/b>. \n     - For a perfect match between predicted values and actual labels both the loss values would be \u201c0\u201d.\n     \n- Answer - 2:\n     - In classification scenarios, we often use gradient-based techniques(Newton Raphson, gradient descent, etc ..) to find the optimal values for coefficients by minimizing the loss function. Hence if the loss function is not convex, it is not guaranteed that we will always reach the global minima, rather we might get stuck at local minima. MSE loss function for logistic regression is non-convex and not recommended.","a0c1940d":"### <u> Adjusted R-Squared <\/u>\n\nOn adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared. The formula for adjusted R-Squared is given by:\n\n$$\\bar{R^2} = 1 - (1 - R^2)(\\frac{n - 1}{n - k + 1})$$\n\nk: number of features\n\nn: number of samples\n\nThis metric takes the number of features into account. When we add more features, the term in the denominator n-(k +1) decreases, so the whole expression increases.","d8d4c7d9":"#### <u>Interpreting ROC Plot<\/u>:\n\nInterpreting the ROC plot is very different from a regular line plot. Because, though there is an X and a Y-axis, we don't read it as: for an X value of 0.25, the Y value is .9.\n\nInstead, what we have here is a line that traces the probability cutoff from 1 at the bottom-left to 0 in the top right.\n\nThis is a way of analyzing how the sensitivity and specificity perform for the full range of probability cutoffs, that is from 0 to 1.\n\nIdeally, if we have a perfect model, all the events will have a probability score of 1 and all non-events will have a score of 0. For such a model, the area under the ROC will be a perfect 1.\n\nSo, if we trace the curve from bottom left, the value of probability cutoff decreases from 1 towards 0. If we have a good model, more of the real events should be predicted as events, resulting in high sensitivity and low FPR. In that case, the curve will rise steeply covering a large area before reaching the top-right.\n\nTherefore, the larger the area under the ROC curve, the better is the model.\n\nThe ROC curve is the only metric that measures how well the model does for different values of prediction probability cutoffs.\n\n","dc4284bb":"### <u>Classification Report <\/u>\n\nThe classification_report() function displays the precision, recall, f1-score and support for each class.\n\n#### <u> Precision <\/u>\nIt is the number of True Positive divided by the number of positive results predicted by the classifier.\n\n$$ Precision =  \\frac{True\\ Positives}{True\\ Positives + False\\ Positives} $$\n\n![Screen%20Shot%202019-10-17%20at%2009.10.10.png](attachment:Screen%20Shot%202019-10-17%20at%2009.10.10.png)\n\n#### <u> Recall\/ Sensitivity <\/u>\n\nIt is the number of True Positives divided by the number of all relevant samples (all samples that should have been identified as positive).\n\n$$Recall =  \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives}$$\n\n![Screen%20Shot%202019-10-17%20at%2009.10.38.png](attachment:Screen%20Shot%202019-10-17%20at%2009.10.38.png)\n\n- To minimising False Negatives, we would want our Recall to be as close to 100% \n- To minimising False Positives, we would want our Precision to be as close to 100% \n\n#### <u> Specificity \/ TNR (True Negative Rate)<\/u>\n\n- Proportion of actual negative cases which are correctly identified.\n- Specificity is the exact opposite of Recall.\n\n$$Specificity =  \\frac{True\\ Negatives}{True\\ Negatives + False\\ Positives}$$\n\n![Screen%20Shot%202019-10-17%20at%2009.11.10.png](attachment:Screen%20Shot%202019-10-17%20at%2009.11.10.png)\n\n#### <u> F1 Score <\/u>\n- F1 Score is the Harmonic Mean between precision and recall.\n\n- It tells how precise the classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n- The greater the F1 Score, the better is the performance of our model.\n- Range [0, 1].\n\n$$F1 = 2 * \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}$$\n\n#### Why Harmonic Mean ?\n\nEx: We have a binary classification model with the following results:\n\nPrecision: 0, Recall: 1\n\nIf we take the arithmetic mean, we get 0.5. It is clear that the above result comes from a dumb classifier which just ignores the input and just predicts one of the classes as output. Now, if we were to take HM, we will get 0 which is accurate as this model is useless for all purposes.\n\nHarmonic mean is an average when x and y are equal. But when x and y are different, then it\u2019s closer to the smaller number as compared to the larger number. If one number is really small between precision and recall, the F1 Score of raises a flag and is more closer to the smaller number than the bigger one, giving the model an appropriate score rather than just an arithmetic mean.","8f9bc903":"Classifier uses a threshold equal to 0, so the previous code returns the same result as the predict() method (i.e., True). Let\u2019s raise the threshold","d0d0152c":"This [Scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html) page provides the excellent reference.","35c498d5":"This confirms that raising the threshold decreases recall. The instance actually represents a 1(True), and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 2.\n\nTo decide which threshold to use, we first need to get the scores of all instances in the training set using the cross_val_predict() function again, but this time specifying that you want it to return decision scores\/probability instead of class:","90a9b989":"#### MAE vs. MSE\n- Being more complex and biased towards higher deviation, RMSE is still the default metric of many models because loss function defined in terms of RMSE is smoothly differentiable whereas Mean Absolute Error requires complicated linear programming to compute the gradient.\n- If we want a metric just to compare between two models from interpretation point of view, then MAE may be a better choice.\n- Units of both RMSE & MAE are same as y values which is not true for R Square.\n- Minimizing the squared error (\ud835\udc3f2) over a set of numbers results in finding its mean, and minimizing the absolute error (\ud835\udc3f1) results in finding its median.","c42706aa":"## <u> Classification Metrices <\/u>\n\n- Dataset: Pima Indians onset of diabetes dataset.\n- Evaluation Algorithm: Logistic Regression, SGDClassifier, RandomForestClassifier.","786939d1":"### <u> AUC <\/u>\n- The probabilistic interpretation of ROC-AUC score is that if we randomly choose a positive case and a negative case, the probability that the positive case outranks the negative case according to the classifier is given by the AUC. Here, rank is determined according to order by predicted values.\n- ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output\n- AUC is the percentage of the ROC plot that is underneath the curve.\n- The AUC represents a model\u2019s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.\n- AUC is useful even when there is high class imbalance (unlike classification accuracy)\n- Fraud case\n     - Null accuracy almost 99%\n     - AUC is useful here\n\nGeneral AUC predictions:\n- .90-1 = Excellent\n- .80-.90 = Good\n- .70-.80 = Fair\n- .60-.70 = Poor\n- .50-.60 = Fail\n\nAUC ROC considers the predicted probabilities for determining the model\u2019s performance. But, it only takes into account the order of probabilities and hence it does not take into account the model\u2019s capability to predict higher probability for samples more likely to be positive(Log Loss).\n\nWhereas the AUC is computed with regards to binary classification with a varying decision threshold, log loss actually takes \u201ccertainty\u201d of classification into account.","1ca3c4bf":"### <u> Mean Absolute Error <\/u>\n- Average of the difference between the Original Values and the Predicted Values.\n- Do not gives any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data.\n- Smaller the MAE, better is the model.\n- Robust to outliers\n- Range (0, + infinity]\n\n$$ Mean\\ Absolute\\ Error = \\frac{1}{N} \\sum_{i=1}^{N} |y_{i} -  \\hat{y_{i}}|$$","d59179a5":"### <u>Null accuracy<\/u>:\n- Accuracy that could be achieved by always predicting the most frequent class.\n- This means that a dumb model that always predicts 0\/1 would be right \"null_accuracy\" % of the time.","2e39c80e":"### End\nIf you reached this far please comment and upvote this kernel, feel free to make improvements on the kernel and please share if you found anything useful!","8ef33f0e":"#### Why should we choose Adjusted R\u00b2 over R\u00b2?\nAdjusted R\u00b2 will consider the marginal improvement added by an additional term in our model. It will increase if we add the useful terms and it will decrease if we add less useful predictors. However, R\u00b2 increases with increasing terms even though the model is not actually improving.","4d263ee6":"There are many ways to evaluate a multilabel classifier, and selecting the right metric really depends on the project. One approach is to measure the F1 score for each individual label (or any other binary classifier metric discussed earlier), then simply compute the average score. This code computes the average F1 score across all labels:","214677fc":"After doing the usual feature engineering, selection, implementing a model and getting some output in the form of a probability or a class, the next step is to find out how effective is the model based on some metric using test datasets. The metric explains the performance of a model.\n\nThe model may give satisfying results when evaluated using a metric say accuracy_score but may give poor results when evaluated against other metrics such as logarithmic_loss or any other such metric. Hence, it is very much important to choose the right metric to evaluate the Machine Learning model.\n\nChoice of metrics influences how the performance of machine learning algorithms is measured and compared. They influence how we weight the importance of different characteristics in the results.\n\n## <u> Classification Metrics<\/u>\n\n- Accuracy.\n- Logarithmic Loss.\n- ROC, AUC.\n- Confusion Matrix.\n- Classification Report.\n\n## <u> Regression Metrics<\/u>\n\n- Mean Absolute Error.\n- Mean Squared Error.\n- Root Mean Squared Error.\n- Root Mean Squared Logarithmic Error.\n- R Square.\n- Adjusted R Square.\n\nIn <b> classification problems <\/b>, we use two types of algorithms (dependent on the kind of output it creates):\n\n- <b> Class output <\/b>: Algorithms like SVM and KNN create a class output. For instance, in a binary classification problem, the outputs will be either 0 or 1. SKLearn's\/Other algorithms can convert these class outputs to probability.\n\n- <b> Probability output <\/b>: Algorithms like Logistic Regression, Random Forest, Gradient Boosting, Adaboost etc. give probability outputs. Probability outputs can be converted to class output by creating a threshold probability.\n\nIn regression problems the output is always continuous in nature and requires no further treatment.","faf6d036":"Scikit-Learn did not have to run OvA or OvO on Random Forest because Random Forest classifiers can directly classify instances into multiple classes. We can call predict_proba() to get the list of probabilities that the classifier assigned to each instance for each class.","595bf6ca":"### <u> Logarithmic Loss \/ Log Loss \/ Logistic Loss \/ Cross-Entropy Loss <\/u>\n\n- When working with Log Loss, the classifier must assign probability to each class for all the samples.\n- Log loss measures the UNCERTAINTY of the probabilities of the model by comparing them to the true labels and penalising the false classifications.\n- Log loss is only defined for two or more labels.\n- Log Loss gradually declines as the predicted probability improves, thus Log Loss nearer to 0 indicates higher accuracy, Log Loss away from 0 indicates lower accuracy.\n- Log Loss exists in the range (0, \u221e]. \n\nSuppose, there are N samples belonging to M classes, then the Log Loss is calculated as below :\n\n$$ Log\\ Loss = \\frac{-1}{N} \\sum_{i=1}^{N} \\sum_{i=1}^{M}  y_{ij} * \\log(\\hat{y_{ij}})$$\n\nwhere,\n\n- $y_{ij}$, indicates whether sample i belongs to class j or not\n\n- $p_{ij}$, indicates the probability of sample i belonging to class j\n\n\nThe negative sign negates $\\log(\\hat{y_{ij}})$ output which is always negative. $\\hat{y_{ij}}$ outputs a probability (0 - 1), $\\log(x)$ is nagative if 0 < x < 1.\n\n<b>Example <\/b>: Let the training labels are 0 and 1 but our training predictions are 0.4, 0.6, 0.89 etc. To calculate a measure of the error of our model, we may classify all the observations having values > 0.5 into 1 .But doing so, we are at a high risk of increasing the misclassification. This is because it may so happen that many values having probabilities 0.4, 0.45, 0.49 can have a true value of 1.\n\nThis is where logLoss comes into picture.\n\nNow let us closely follow the formula of LogLoss. There can be 4 major cases for the values of $y_{ij}$ and $p_{ij}$\n\n- Case 1 : $y_{ij}$=1 , $p_{ij}$ = High\n\n- Case 2 : $y_{ij}$=1 , $p_{ij}$ = Low\n\n- Case 3 : $y_{ij}$=0 , $p_{ij}$ = Low\n\n- Case 4 : $y_{ij}$=0 , $p_{ij}$ = High\n\n<u>How does LogLoss measures uncertainity ?<\/u>\n\nIf we have more of Case 1's and Case 3's, then the sum(and mean) inside the logloss formula would be greater and will be substantially larger in comparison to what it would have been if Case 2's and Case 4's got added. Now this value is as large as possible as Case 1's and Case 3's which indicates a good prediction. If we multiply it by (- 1) , we would make the value as small as possible. This would now intuitively mean - Smaller the value, better is the model i.e. smaller the logloss, better is the model i.e. smaller the UNCERTAINTY, better is the model.","afa8bebe":"### <u>Mean Squared Error <\/u>\n\n- Takes the average of the square of the difference between the original values and the predicted values.\n- As we take square of the error, the effect of larger errors(sometimes outliers) become more pronounced then smaller error. Model will be penalized more for making predictions that differ greatly from the corresponding actual value.\n- Before applying MSE, we must eliminate all nulls\/infinites from the input.\n- Not robust to outliers\n- Range (0, + infinity]\n\n$$ Mean\\ Squared\\ Error = \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} -  \\hat{y_{i}})^2$$","8fcb4107":"## <u>Multilabel Classification<\/u>\n\nClassifier outputs multiple classes for each instance.\n\n<b> Example <\/b>: Consider a face-recognition classifier: it's task is to recognizes several people on the same picture. It should attach one label per person it recognizes. If the classifier has been trained to recognize three faces, Alice, Bob, and Charlie, when it is shown a picture of Alice and Charlie, it should output [1, 0, 1]. Such a classification system that outputs multiple binary labels is called a multilabel classification system.\n\n<b>KNeighborsClassifier<\/b> supports multilabel classification.","4f1d648f":"### <u> Classification Accuracy <\/u>\n\n- Classification Accuracy or Accuracy is the ratio of number of correct predictions to the total number of input samples.\n\n$$Accuracy = \\frac{Number\\ of\\ correct\\ predictions}{Total\\ number\\ of\\ predictions\\ made} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n\n![Screen%20Shot%202019-10-17%20at%2009.09.40.png](attachment:Screen%20Shot%202019-10-17%20at%2009.09.40.png)\n\n#### <u> When to use accuracy metric: <\/u>\n\n- When there are roughly equal number of samples belonging to each class.\n\n#### <u> When not to use accuracy metric: <\/u>\n\n- When only one class holds majority of samples.\n\n\n<b>Example<\/b>: Consider that there are 98% samples of class A and 2% samples of class B in our training set. Then our model can easily get 98% training accuracy by simply predicting every training sample belonging to class A.\n\nWhen the same model is tested on a test set with 60% samples of class A and 40% samples of class B, then the test accuracy would drop down to 60%. Classification Accuracy may give us the false sense of achieving high accuracy.","74fb1bfd":"<b>Q.<\/b> Why is logistic regression considered a linear model?\n\n<b>A:<\/b> https:\/\/www.quora.com\/Why-is-logistic-regression-considered-a-linear-model\/answer\/Sebastian-Raschka-1","ff88181b":"## <u> NLP Metric <\/u>\n\n### <u> BLEU (Bilingual Evaluation Understudy) <\/u>\nIt is mostly used to measure the quality of machine translation with respect to the human translation. It uses a modified form of precision metric.\n\nExample:\nReference: The cat is sitting on the mat\n\nMachine Translation 1: On the mat is a cat\n\nMachine Translation 2: There is cat sitting cat\n\nMachine Translation 3: The cat is sitting on the tam","cc7371af":"This code creates a y_multilabel array containing two target labels for each digit image: the first indicates whether or not the digit is large (7, 8, or 9) and the second indicates whether or not it is odd. The next lines create a KNeighborsClassifier instance (which supports multilabel classification, but not all classifiers do) and we train it using the multiple targets array. Now we can make a prediction, and notice that it outputs two labels:","a727c8e4":"## <u> Multioutput Classification <\/u>\n\nMultioutput- Multiclass classification (or simply multioutput classification) is simply a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).\n\n<b>KNeighborsClassifier<\/b> supports multioutput classification.","056c9937":"### <u> Root Mean Squared Logarithmic Error <\/u>\n- We take the log of the predictions and actual values.\n- What changes are the variance that we are measuring.\n- RMSLE is usually used when we don\u2019t want to penalize huge differences in the predicted and the actual values when both predicted and actual values are huge numbers.\n- If both predicted and actual values are small: RMSE and RMSLE are same.\n- If either predicted or the actual value is big: RMSE > RMSLE\n- If both predicted and actual values are big: RMSE > RMSLE (RMSLE becomes almost negligible)\n\n$$ Root\\ Mean\\ Squared\\ Log\\ Error =\\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (\\log (y_{i} + 1) -  (\\log \\hat{y_{i}} + 1))^2}$$ ","1e1deb93":"![570735.jpg](attachment:570735.jpg)","639a7f9f":"### <u>RMSE<\/u>\n\n- Because the MSE is squared, its units do not match that of the original output. RMSE is the square root of MSE.\n- Since the MSE and RMSE both square the residual, they are similarly affected by outliers.\n- The RMSE is analogous to the standard deviation and is a measure of how large the residuals are spread out.\n- - Generally, RMSE will be higher than or equal to MAE.\n\n$$ Root\\ Mean\\ Squared\\ Error =\\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} -  \\hat{y_{i}})^2}$$","c666ce88":"### <u> ROC Curve <\/u>\n\nROC can be broken down into sensitivity and specificity. Choosing the best model is sort of a balance between predicting 1's accurately or 0's accurately. In other words sensitivity and specificity.\n\n- True Positive Rate (Sensitivity\/ Recall) : True Positive Rate is defined as TP\/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n\n- False Positive Rate (Specificity) : False Positive Rate is defined as FP \/ (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.\n\nTrue Positive Rate and False Positive Rate both have values in the range [0, 1]. TPR and FPR both are computed at threshold values such as (0.00, 0.02, 0.04, \u2026., 1.00) and a graph is drawn.","dc56e490":"### <u>Conclusion<\/u>:\n\n#### Comparison of Log-loss with ROC & F1\n\n#### <u>Case 1 : Balanced Dataset<\/u>\n\n| S.No. | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |\n| --- | --- | --- | --- |\n| Actual (Balanced) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| Predicted (Model 1) | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.6 | 0.6 | 0.5 | 0.5 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 |\n| Predicted (Model 1) | 0.6 | 0.6 | 0.6 | 0.6 | 0.6 | 0.6 | 0.6 | 0.6 | 0.7 | 0.7 | 0.7 | 0.7 | 0.8 | 0.8 | 0.8 | 0.8 |\n\n\nConsider Case 1 (Balanced Data), it looks like model 1 is doing a better job in predicting the absolute probabilities whereas model 2 is working best in ranking observations according to their true labels. Let\u2019s verify with the actual score:\n\n| | F1 (threshold = 0.5) | F1 (threshold which maximize score) | ROC - AUC | LogLoss |\n| --- | --- | --- | --- |\n| Model 1 | 0.88 | .88 | 0.94 | 0.28\n| Model 2 | 0.67 | 1 | 1 | 0.6\n\nIf we consider log-loss, Model 2 is worst giving a high value of log-loss because the absolute probabilities have big difference from actual labels. But this is in complete disagreement with F1 & AUC score, according to which Model 2 has 100% accuracy. Also, we would like to note that with different thresholds, F1 score is changing, and preferring model 1 over model 2 for default threshold of 0.5.\n\n<b> Inferences drawn from the above example (balanced dataset) <\/b>:\n- If we care for absolute probabilistic difference, go with log-loss.\n- If we care only for the final class prediction and we don\u2019t want to tune threshold, go with AUC score.\n-F1 score is sensitive to threshold and we would want to tune it first before comparing the models.\n\n#### <u> <b>Case 2 : Imbalanced Dataset <\/b><\/u>\n\n<b> A) Imbalanced - Few Positives<\/b>\n\n| S.No. | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |\n| --- | --- | --- | --- |\n| Actual (Balanced) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |\n| Predicted (Model 1) | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.9 | 0.9 |\n| Predicted (Model 1) | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.9 | 0.9 | 0.9 | 0.9 |\n\n\n| | F1 (threshold = 0.5) | ROC - AUC | LogLoss |\n| --- | --- | --- | --- |\n| Model 1 | 0.8 | .83 | .24\n| Model 2 | 0.86 | .96 | .24 \n\n\nThe only difference in model1 and model2 is their prediction for observation 13 & 14. Model 1 is doing a better job in classifying observation 13 (label 0) whereas Model 2 is doing better in classifying observation 14 (label 1). The goal is to see which model actually captures the difference in classifying the imbalanced class better (class with few observations, here it is label 1). In problems like fraud detection\/spam mail detection, where positive labels are few, we would like our model to predict positive classes correctly and hence we will sometime prefer those model who are able to classify these positive labels.\n\nClearly log-loss is failing in this case because according to log-loss both the models are performing equally. This is because log-loss function is symmetric and does not differentiate between classes .\n\nBoth F1 score and ROC-AUC score is doing better in preferring model 2 over model 1. So we can use both these methods for class imbalance.\n\n<b> B) Imbalanced - Few Negatives<\/b>\n\n| S.No. | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |\n| --- | --- | --- | --- |\n| Actual (Balanced) | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| Predicted (Model 1) | 0.1 | 0.1 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 |\n| Predicted (Model 1) | 0.1 | 0.1 | 0.1 | 0.1 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 |\n\n\n| | F1 (threshold = 0.5) | ROC - AUC | LogLoss |\n| --- | --- | --- | --- |\n| Model 1 | 0.963 | .83 | .24\n| Model 2 | 0.96 | .96 | .24 \n\nROC-AUC score handled the case of few negative labels in the same way as it handled the case of few positive labels. F1 score is very much same for both Model 1 & Model 2 because positive labels are large in number and it cares only for the misclassification of positive labels.\n\n<b> Inferences drawn from the above example (imbalanced dataset) <\/b>:\n- - If we care for a class which is smaller in number independent of the fact whether it is positive or negative, go for ROC-AUC score.\n\n#### When will we prefer F1 over ROC-AUC?\nPrefer PR curve whenever the positive class is rare or when we care more about the false positives than the false negatives.\n\nTo train binary classifiers, choose the appropriate metric for the task, evaluate the classifiers using cross-validation, select the precision\/ recall tradeoff that fits our needs, and compare various models using ROC curves and ROC AUC scores."}}