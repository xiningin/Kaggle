{"cell_type":{"022ddc15":"code","503ae7e5":"code","7386e46c":"code","884586b4":"code","f7b1e4c4":"code","056d976e":"code","3a69742f":"code","40fb2ce4":"code","02ed2362":"code","cc281818":"code","544fe5a7":"code","aebc3a9d":"code","65b8dcb9":"code","d1a88f6c":"code","e721463f":"code","6381bcc2":"code","54b685f2":"code","1e07d127":"code","15d8b863":"code","c24b090a":"code","e05c49d0":"code","025c254b":"code","767cb5f4":"code","5ca5207a":"code","77349e7e":"code","84d75a20":"markdown","d56fd8ce":"markdown","7f57dbfd":"markdown"},"source":{"022ddc15":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","503ae7e5":"import numpy as np\nimport pandas as pd\n\n\npd.set_option(\"display.max_columns\", None)\n\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n\ntrain_df.head()","7386e46c":"train_df.drop(\"Id\", axis=1, inplace=True)\ntest_df.drop(\"Id\", axis=1, inplace=True)","884586b4":"cols = [\"Soil_Type7\", \"Soil_Type15\"]\n\ntrain_df.drop(cols, axis=1, inplace=True)\ntest_df.drop(cols, axis=1, inplace=True)","f7b1e4c4":"idx = train_df[train_df[\"Cover_Type\"] == 5].index\ntrain_df.drop(idx, axis=0, inplace=True)","056d976e":"new_names = {\n    \"Horizontal_Distance_To_Hydrology\": \"x_dist_hydrlgy\",\n    \"Vertical_Distance_To_Hydrology\": \"y_dist_hydrlgy\",\n    \"Horizontal_Distance_To_Roadways\": \"x_dist_rdwys\",\n    \"Horizontal_Distance_To_Fire_Points\": \"x_dist_firepts\"\n}\n\ntrain_df.rename(new_names, axis=1, inplace=True)\ntest_df.rename(new_names, axis=1, inplace=True)","3a69742f":"from sklearn.preprocessing import LabelEncoder\n\n\nencoder = LabelEncoder()\ntrain_df[\"Cover_Type\"] = encoder.fit_transform(train_df[\"Cover_Type\"])","40fb2ce4":"train_df[\"Aspect\"][train_df[\"Aspect\"] < 0] += 360\ntrain_df[\"Aspect\"][train_df[\"Aspect\"] > 359] -= 360\n\ntest_df[\"Aspect\"][test_df[\"Aspect\"] < 0] += 360\ntest_df[\"Aspect\"][test_df[\"Aspect\"] > 359] -= 360","02ed2362":"# Manhhattan distance to Hydrology\ntrain_df[\"mnhttn_dist_hydrlgy\"] = np.abs(train_df[\"x_dist_hydrlgy\"]) + np.abs(train_df[\"y_dist_hydrlgy\"])\ntest_df[\"mnhttn_dist_hydrlgy\"] = np.abs(test_df[\"x_dist_hydrlgy\"]) + np.abs(test_df[\"y_dist_hydrlgy\"])\n\n# Euclidean distance to Hydrology\ntrain_df[\"ecldn_dist_hydrlgy\"] = (train_df[\"x_dist_hydrlgy\"]**2 + train_df[\"y_dist_hydrlgy\"]**2)**0.5\ntest_df[\"ecldn_dist_hydrlgy\"] = (test_df[\"x_dist_hydrlgy\"]**2 + test_df[\"y_dist_hydrlgy\"]**2)**0.5","cc281818":"soil_features = [x for x in train_df.columns if x.startswith(\"Soil_Type\")]\ntrain_df[\"soil_type_count\"] = train_df[soil_features].sum(axis=1)\ntest_df[\"soil_type_count\"] = test_df[soil_features].sum(axis=1)\n\nwilderness_features = [x for x in train_df.columns if x.startswith(\"Wilderness_Area\")]\ntrain_df[\"wilderness_area_count\"] = train_df[wilderness_features].sum(axis=1)\ntest_df[\"wilderness_area_count\"] = test_df[wilderness_features].sum(axis=1)","544fe5a7":"train_df.loc[train_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest_df.loc[test_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest_df.loc[test_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest_df.loc[test_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest_df.loc[test_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest_df.loc[test_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest_df.loc[test_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","aebc3a9d":"from sklearn.preprocessing import RobustScaler\n\n\ncols = [\n    \"Elevation\",\n    \"Aspect\",\n    \"mnhttn_dist_hydrlgy\",\n    \"ecldn_dist_hydrlgy\",\n    \"soil_type_count\",\n    \"wilderness_area_count\",\n    \"Slope\",\n    \"x_dist_hydrlgy\",\n    \"y_dist_hydrlgy\",\n    \"x_dist_rdwys\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"x_dist_firepts\",\n    \"soil_type_count\",\n    \"wilderness_area_count\"\n]\n\nscaler = RobustScaler()\ntrain_df[cols] = scaler.fit_transform(train_df[cols])\ntest_df[cols] = scaler.transform(test_df[cols])","65b8dcb9":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","d1a88f6c":"df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","e721463f":"df.head()","6381bcc2":"##create folds\nfrom sklearn import model_selection\ndf[\"kfold\"] = -1\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=10, shuffle=False)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.Cover_Type.values)):\n    print(len(train_idx), len(val_idx))\n    df.loc[val_idx, 'kfold'] = fold","54b685f2":"df.Cover_Type.unique()","1e07d127":"import joblib\n# import lightgbm as lgb\nimport xgboost as xg\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import tree\ndef run(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    df_train = df_train.drop(columns = 'kfold')\n    df_valid = df_valid.drop(columns = 'kfold')\n    x_train = df_train.drop('Cover_Type', axis=1).values\n    y_train = df_train.Cover_Type.values\n    x_valid = df_valid.drop('Cover_Type', axis=1).values\n    y_valid = df_valid.Cover_Type.values\n    clf = xg.XGBClassifier(tree_method='gpu_hist',\n                             num_class = 6,\n                          )\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_valid)\n    accuracy_score = metrics.accuracy_score(y_valid,y_pred)\n    print(f\"Fold={fold}, accuracy_score={accuracy_score}\")\n    File_name = 'model_xgb' + str(fold)\n    joblib.dump(\n    clf,File_name)\nfor i in range(10):\n    run(fold = i)","15d8b863":"model_0_xgb= joblib.load('.\/model_xgb0')\nmodel_1_xgb =joblib.load('.\/model_xgb1')\nmodel_2_xgb= joblib.load('.\/model_xgb2')\nmodel_3_xgb= joblib.load('.\/model_xgb3')\nmodel_4_xgb= joblib.load('.\/model_xgb4')\nmodel_5_xgb= joblib.load('.\/model_xgb5')\nmodel_6_xgb =joblib.load('.\/model_xgb6')\nmodel_7_xgb= joblib.load('.\/model_xgb7')\nmodel_8_xgb= joblib.load('.\/model_xgb8')\nmodel_9_xgb= joblib.load('.\/model_xgb9')","c24b090a":"y_final_3_xgb = model_3_xgb.predict_proba(test_df)\ny_final_0_xgb = model_0_xgb.predict_proba(test_df)\ny_final_1_xgb = model_1_xgb.predict_proba(test_df)\ny_final_2_xgb = model_2_xgb.predict_proba(test_df)\ny_final_4_xgb = model_4_xgb.predict_proba(test_df)\ny_final_5_xgb = model_6_xgb.predict_proba(test_df)\ny_final_7_xgb = model_7_xgb.predict_proba(test_df)\ny_final_8_xgb = model_8_xgb.predict_proba(test_df)\ny_final_9_xgb = model_9_xgb.predict_proba(test_df)\ny_final_6_xgb = model_6_xgb.predict_proba(test_df)","e05c49d0":"y_final_avg = (y_final_0_xgb + y_final_1_xgb +y_final_2_xgb + y_final_3_xgb + y_final_4_xgb +y_final_5_xgb + y_final_6_xgb +y_final_7_xgb + y_final_8_xgb + y_final_9_xgb)\/10","025c254b":"# y_final_avg\ny_final_sub = model_0_xgb.predict(test_df)\ny_final_avg = y_final_avg.tolist()\nfor i in range(len(y_final_sub)):\n    y_final_sub[i] = y_final_avg[i].index(max(y_final_avg[i]))","767cb5f4":"y_final_sub = encoder.inverse_transform(y_final_sub)","5ca5207a":"submission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n\nsubmission['Cover_Type'] = y_final_sub","77349e7e":"submission.to_csv('pred_csv_xgb_0.csv',index = False)","84d75a20":"## Ensemble","d56fd8ce":"### Train.py","7f57dbfd":"## FEATURE ENGINEERING TAKEN FROM GULSHAN MISHRA \n### https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering#Part-2:-Feature-Engineering"}}