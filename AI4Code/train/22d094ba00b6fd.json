{"cell_type":{"de4ac4e8":"code","3b0a4545":"code","a292b5a6":"code","81d59214":"code","b5331c1b":"code","926cdb70":"code","a063b0fb":"code","20809e06":"code","e75065d2":"code","15edb682":"code","72127e0d":"code","70cbeb32":"code","cd366842":"code","92d85810":"code","e7522779":"code","08cef382":"code","cd79a1f6":"code","f18c9d49":"code","4b3432e8":"code","b938c121":"code","142b0ae0":"code","a8413844":"code","e0b7072b":"code","efb97b19":"code","54c6033b":"code","5ddc3ee1":"code","d7416b01":"code","b5573ba4":"code","8743ac7b":"code","2c1efb6c":"code","a9ca0459":"code","160682cd":"code","0d66c7d7":"code","5113ff22":"code","969feecd":"code","2bda0a8a":"code","78505b49":"code","bc202ec5":"code","a62311c9":"code","7d198c69":"code","68c8063a":"code","3dd31b60":"code","4ec0895a":"code","cea3c9ea":"code","e789c81d":"code","3e107879":"code","fb0a6092":"code","1119530a":"code","c9f88500":"code","21a9b432":"code","ff3921f6":"code","8328d7da":"code","7e0b6b3f":"code","41419738":"code","4ab17d2f":"code","003c2b29":"code","3ba45fca":"code","9dd2c7c2":"markdown","2336e006":"markdown","bd71fcb9":"markdown","35a7c7f3":"markdown","f2553115":"markdown","1df41fc0":"markdown","01689c3c":"markdown","050edd18":"markdown","fae77674":"markdown","bcb3cf35":"markdown","7ad46bcd":"markdown","67c3c894":"markdown","c070f307":"markdown","18653146":"markdown","48a47e53":"markdown","14085ada":"markdown","03c6334c":"markdown","d1be2ec4":"markdown","0e4dc29f":"markdown","f39e58b9":"markdown","841f0ca8":"markdown","5f13d5b3":"markdown","6183ccc1":"markdown","53538a63":"markdown","2c9a78e0":"markdown","311f9bc0":"markdown","f72ccaaa":"markdown","2d0a923d":"markdown","2cc274ee":"markdown","31ff02d3":"markdown","1b086234":"markdown","3b3b7b25":"markdown","d9c9d4d2":"markdown","bc7c53ee":"markdown","3e5f87dc":"markdown","c75649d4":"markdown","ee82a572":"markdown","4342fe03":"markdown","8fe0c20b":"markdown","cc10750b":"markdown","52075c39":"markdown","89cbbe94":"markdown","fe0d7ed4":"markdown","b3f7e201":"markdown","8082cd83":"markdown"},"source":{"de4ac4e8":"# Data processing \nimport pandas as pd\nimport json\nfrom collections import Counter\nfrom itertools import chain\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport re\n\n# Data vizualizations\nimport random\nimport plotly\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \ninit_notebook_mode(connected=True)\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\n# Data Modeling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection \nimport warnings\nwarnings.filterwarnings('ignore')","3b0a4545":"train_data = pd.read_json('..\/input\/train.json') # store as dataframe objects\ntest_data = pd.read_json('..\/input\/test.json')","a292b5a6":"train_data.info()","81d59214":"train_data.shape # 39774 observations, 3 columns","b5331c1b":"print(\"The training data consists of {} recipes\".format(len(train_data)))","926cdb70":"print(\"First five elements in our training sample:\")\ntrain_data.head()","a063b0fb":"test_data.info()","20809e06":"test_data.shape # 9944 observations, 2 columns","e75065d2":"print(\"The test data consists of {} recipes\".format(len(test_data)))","15edb682":"print(\"First five elements in our test sample:\")\ntest_data.head()","72127e0d":"print(\"Number of cuisine categories: {}\".format(len(train_data.cuisine.unique())))\ntrain_data.cuisine.unique()","70cbeb32":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","cd366842":"trace = go.Table(\n                header=dict(values=['Cuisine','Number of recipes'],\n                fill = dict(color=['#EABEB0']), \n                align = ['left'] * 5),\n                cells=dict(values=[train_data.cuisine.value_counts().index,train_data.cuisine.value_counts()],\n               align = ['left'] * 5))\n\nlayout = go.Layout(title='Number of recipes in each cuisine category',\n                   titlefont = dict(size = 20),\n                   width=500, height=650, \n                   paper_bgcolor =  'rgba(0,0,0,0)',\n                   plot_bgcolor = 'rgba(0,0,0,0)',\n                   autosize = False,\n                   margin=dict(l=30,r=30,b=1,t=50,pad=1),\n                   )\ndata = [trace]\nfig = dict(data=data, layout=layout)\niplot(fig)","92d85810":"#  Label distribution in percents\nlabelpercents = []\nfor i in train_data.cuisine.value_counts():\n    percent = (i\/sum(train_data.cuisine.value_counts()))*100\n    percent = \"%.2f\" % percent\n    percent = str(percent + '%')\n    labelpercents.append(percent)","e7522779":"trace = go.Bar(\n            x=train_data.cuisine.value_counts().values[::-1],\n            y= [i for i in train_data.cuisine.value_counts().index][::-1],\n            text =labelpercents[::-1],  textposition = 'outside', \n            orientation = 'h',marker = dict(color = random_colours(20)))\nlayout = go.Layout(title='Number of recipes in each cuisine category',\n                   titlefont = dict(size = 25),\n                   width=1000, height=450, \n                   plot_bgcolor = 'rgba(0,0,0,0)',\n                   paper_bgcolor = 'rgba(255, 219, 227, 0.88)',\n                   margin=dict(l=75,r=110,b=50,t=60),\n                   )\ndata = [trace]\nfig = dict(data=data, layout=layout)\niplot(fig, filename='horizontal-bar')","08cef382":"print('Maximum Number of Ingredients in a Dish: ',train_data['ingredients'].str.len().max())\nprint('Minimum Number of Ingredients in a Dish: ',train_data['ingredients'].str.len().min())","cd79a1f6":"trace = go.Histogram(\n    x= train_data['ingredients'].str.len(),\n    xbins=dict(start=0,end=90,size=1),\n   marker=dict(color='#7CFDF0'),\n    opacity=0.75)\ndata = [trace]\nlayout = go.Layout(\n    title='Distribution of Recipe Length',\n    xaxis=dict(title='Number of ingredients'),\n    yaxis=dict(title='Count of recipes'),\n    bargap=0.1,\n    bargroupgap=0.2)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","f18c9d49":"longrecipes = train_data[train_data['ingredients'].str.len() > 30]\nprint(\"It seems that {} recipes consist of more than 30 ingredients!\".format(len(longrecipes)))","4b3432e8":"print(\"Explore the ingredients in the longest recipe in our training set:\" + \"\\n\")\nprint(str(list(longrecipes[longrecipes['ingredients'].str.len() == 65].ingredients.values)) + \"\\n\")\nprint(\"Cuisine: \" + str(list(longrecipes[longrecipes['ingredients'].str.len() == 65].cuisine)))","b938c121":"shortrecipes = train_data[train_data['ingredients'].str.len() <= 2]\nprint(\"It seems that {} recipes consist of less than or equal to 2 ingredients!\".format(len(shortrecipes)))","142b0ae0":"print(\"Explore the ingredients in the shortest recipes in our training set:\" + \"\\n\")\nprint(list(train_data[train_data['ingredients'].str.len() == 1].ingredients.values))\nprint(\"And there corresponding labels\" + \"\\n\")\nprint(list(train_data[train_data['ingredients'].str.len() == 1].cuisine.values))","a8413844":"train_data[train_data['cuisine'] == labels[i]]","e0b7072b":"boxplotcolors = random_colours(21)\nlabels = [i for i in train_data.cuisine.value_counts().index][::-1]   # Cusine Names\ndata = []\nfor i in range(20):\n    trace = go.Box(\n    y=train_data[train_data['cuisine'] == labels[i]]['ingredients'].str.len(), name = labels[i],\n    marker = dict(color = boxplotcolors[i]))\n    data.append(trace)\nlayout = go.Layout(\n    title = \"Recipe Length Distribution by cuisine\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"Box Plot Styling Outliers\")","efb97b19":"allingredients = [] # this list stores all the ingredients in all recipes (with duplicates)\nfor item in train_data['ingredients']:\n    for ingr in item:\n        allingredients.append(ingr) ","54c6033b":"# Count how many times each ingredient occurs\ncountingr = Counter()\nfor ingr in allingredients:\n     countingr[ingr] += 1","5ddc3ee1":"print(\"The most commonly used ingredients (with counts) are:\")\nprint(\"\\n\")\nprint(countingr.most_common(20))\nprint(\"\\n\")\nprint(\"The number of unique ingredients in our training sample is {}.\".format(len(countingr)))","d7416b01":"# Extract the first 20 most common ingredients in order to vizualize them for better understanding\nmostcommon = countingr.most_common(20)\nmostcommoningr = [i[0] for i in mostcommon]\nmostcommoningr_count = [i[1] for i in mostcommon]","b5573ba4":"trace = go.Bar(\n            x=mostcommoningr_count[::-1],\n            y= mostcommoningr[::-1],\n            orientation = 'h',marker = dict(color = random_colours(20),\n))\nlayout = go.Layout(\n    xaxis = dict(title= 'Number of occurences in all recipes (training sample)', ),\n    yaxis = dict(title='Ingredient',),\n    title= '20 Most Common Ingredients', titlefont = dict(size = 20),\n    margin=dict(l=150,r=10,b=60,t=60,pad=5),\n    width=800, height=500, \n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='horizontal-bar')","8743ac7b":"# Define a function that returns how many different ingredients can be found in all recipes part of a given cuisine\ndef findnumingr(cuisine):\n    '''\n    Input:\n        cuisine - cuisine category (ex. greek,souther_us etc.)\n    Output:\n        The number of unique ingredients used in all recipes part of the given cuisine. \n    '''\n    listofinrg = []\n    for item in train_data[train_data['cuisine'] == cuisine]['ingredients']:\n        for ingr in item:\n            listofinrg.append(ingr)\n    result = (cuisine,len(list(set(listofinrg))))         \n    return result ","2c1efb6c":"cuisineallingr = []\nfor i in labels:\n    cuisineallingr.append(findnumingr(i))","a9ca0459":"# Vizualize the results\ntrace = go.Bar(\n            x=[i[1] for i in cuisineallingr],\n            y= [i[0] for i in cuisineallingr],\n            orientation = 'h',marker = dict(color = random_colours(20),\n))\nlayout = go.Layout(\n    xaxis = dict(title= 'Count of different ingredients', ),\n    yaxis = dict(title='Cuisine',),\n    title= 'Number of all the different ingredients used in a given cuisine', titlefont = dict(size = 20),\n    margin=dict(l=100,r=10,b=60,t=60),\n    width=800, height=500, \n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='horizontal-bar')","160682cd":"allingredients = list(set(allingredients)) # list containing all unique ingredients","0d66c7d7":"# Define a function that returns a dataframe with top unique ingredients in a given cuisine \ndef cuisine_unique(cuisine,numingr, allingredients):\n    '''\n    Input:\n        cuisine - cuisine category (ex. 'brazilian');\n        numingr - how many specific ingredients do you want to see in the final result; \n        allingredients - list containing all unique ingredients in the whole sample.\n    \n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train_data[train_data.cuisine != cuisine]['ingredients']:\n        for ingr in item:\n            allother .append(ingr)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in allingredients if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train_data[train_data.cuisine == cuisine]['ingredients']:\n        for ingr in item:\n            mycounter[ingr] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    cuisinespec = pd.DataFrame(mycounter.most_common(numingr), columns = ['ingredient','count'])\n    \n    return cuisinespec","5113ff22":"cuisinespec= cuisine_unique('mexican', 10, allingredients)\nprint(\"The top 10 unique ingredients in Mexican cuisine are:\")\ncuisinespec ","969feecd":"# Vizualization of specific ingredients in the first 10 cuisines\nlabels = [i for i in train_data.cuisine.value_counts().index][0:10]\ntotalPlot = 10\ny = [[item]*2 for item in range(1,10)]\ny = list(chain.from_iterable(y))\nz = [1,2]*int((totalPlot\/2))\n\nfig = tools.make_subplots(rows= 5, cols=2, subplot_titles= labels, specs = [[{}, {}],[{}, {}],[{}, {}],[{}, {}],[{}, {}]],  horizontal_spacing = 0.20)\ntraces = []\nfor i,e in enumerate(labels): \n    cuisinespec= cuisine_unique(e, 5, allingredients)\n    trace = go.Bar(\n            x= cuisinespec['count'].values[::-1],\n            y=  cuisinespec['ingredient'].values[::-1],\n            orientation = 'h',marker = dict(color = random_colours(5),))\n    traces.append(trace)\n\nfor t,y,z in zip(traces,y,z):\n    fig.append_trace(t, y,z)\n\n    fig['layout'].update(height=800, width=840,\n    margin=dict(l=265,r=5,b=40,t=90,pad=5), showlegend=False, title='Ingredients used only in one cuisine')\n\niplot(fig, filename='horizontal-bar')","2bda0a8a":"# Vizualization of specific ingredients in the second 10 cuisines\nlabels = [i for i in train_data.cuisine.value_counts().index][10:20]\ntotalPlot = 10\ny = [[item]*2 for item in range(1,10)]\ny = list(chain.from_iterable(y))\nz = [1,2]*int((totalPlot\/2))\n\nfig = tools.make_subplots(rows= 5, cols=2, subplot_titles= labels, specs = [[{}, {}],[{}, {}],[{}, {}],[{}, {}],[{}, {}]],  horizontal_spacing = 0.20)\ntraces = []\nfor i,e in enumerate(labels): \n    cuisinespec= cuisine_unique(e, 5, allingredients)\n    trace = go.Bar(\n            x= cuisinespec['count'].values[::-1],\n            y=  cuisinespec['ingredient'].values[::-1],\n            orientation = 'h',marker = dict(color = random_colours(5),))\n    traces.append(trace)\n\nfor t,y,z in zip(traces,y,z):\n    fig.append_trace(t, y,z)\n\n    fig['layout'].update(height=800, width=840,\n    margin=dict(l=170,r=5,b=40,t=90,pad=5), showlegend=False, title='Ingredient used only in one cuisine')\n\niplot(fig, filename='horizontal-bar')","78505b49":"# Prepare the data \nfeatures = [] # list of list containg the recipes\nfor item in train_data['ingredients']:\n    features.append(item)","bc202ec5":"ingredients = [] # this list stores all the ingredients in all recipes (with duplicates)\nfor item in train_data['ingredients']:\n    for ingr in item:\n        ingredients.append(ingr) ","a62311c9":"len(features) # 39774 recipes","7d198c69":"# Fit the TfidfVectorizer to data\ntfidf = TfidfVectorizer(vocabulary= list(set([str(i).lower() for i in ingredients])), max_df=0.99, norm='l2', ngram_range=(1, 4))\nX_tr = tfidf.fit_transform([str(i) for i in features]) # X_tr - matrix of tf-idf scores\nfeature_names = tfidf.get_feature_names()","68c8063a":"# Define a function for finding the most important features in a given cuisine according to Tf-Idf measure \ndef top_feats_by_class(trainsample,target,featurenames, min_tfidf=0.1, top_n=10):\n    ''' \n     Input:\n         trainsample - the tf-idf transformed training sample;\n         target - the target variable;\n         featurenames - array mapping from feature integer indices (position in the dataset) to feature name (ingredient in our case) in the Tf-Idf transformed dataset; \n         min_tfidf - features having tf-idf value below the min_tfidf will be excluded ;\n         top_n - how many important features to show.\n     Output:\n          Returns a list of dataframe objects, where each dataframe holds top_n features and their mean tfidf value\n         calculated across documents (recipes) with the same class label (cuisine). \n     '''\n    dfs = []\n    labels = np.unique(target)\n    \n    for label in labels:\n        \n        ids = np.where(target==label)\n        D = trainsample[ids].toarray()\n        D[D < min_tfidf] = 0\n        tfidf_means = np.nanmean(D, axis=0)\n        \n        topn_ids = np.argsort(tfidf_means)[::-1][:top_n] #  Get top n tfidf values\n        top_feats = [(featurenames[i], tfidf_means[i]) for i in topn_ids] # find their corresponding feature names\n        df = pd.DataFrame(top_feats)\n        df.columns = ['feature', 'tfidf']\n        \n        df['cuisine'] = label\n        dfs.append(df)\n        \n    return dfs","3dd31b60":"# Extract the target variable\ntarget = train_data['cuisine']","4ec0895a":"result_tfidf = top_feats_by_class(X_tr, target, feature_names, min_tfidf=0.1, top_n=5)","cea3c9ea":"# Exctract labels from the resulting dataframe\nlabels = []\nfor i, e in enumerate(result_tfidf):\n    labels.append(result_tfidf[i].cuisine[0])\n\n# Set the plot\ntotalPlot = 10\ny = [[item]*2 for item in range(1,10)]\ny = list(chain.from_iterable(y))\nz = [1,2]*int((totalPlot\/2))\n\nfig = tools.make_subplots(rows= 5, cols=2, subplot_titles= labels[0:10], specs = [[{}, {}],[{}, {}],[{}, {}],[{}, {}],[{}, {}]],  horizontal_spacing = 0.20)\ntraces = []\nfor index,element in enumerate(result_tfidf[0:10]): \n    trace = go.Bar(\n            x= result_tfidf[index].tfidf[::-1],\n            y= result_tfidf[index].feature[::-1],\n            orientation = 'h',marker = dict(color = random_colours(5),))\n    traces.append(trace)\n\nfor t,y,z in zip(traces,y,z):\n    fig.append_trace(t, y,z)\n\n    fig['layout'].update(height=800, width=840,\n    margin=dict(l=110,r=5,b=40,t=90,pad=5), showlegend=False, title='Feature Importance based on Tf-Idf measure')\n\niplot(fig, filename='horizontal-bar')","e789c81d":"# Set the plot\ntotalPlot = 10\ny = [[item]*2 for item in range(1,10)]\ny = list(chain.from_iterable(y))\nz = [1,2]*int((totalPlot\/2))\n\nfig = tools.make_subplots(rows= 5, cols=2, subplot_titles= labels[10:20], specs = [[{}, {}],[{}, {}],[{}, {}],[{}, {}],[{}, {}]],  horizontal_spacing = 0.20)\ntraces = []\nfor index,element in enumerate(result_tfidf[10:20]): \n    trace = go.Bar(\n            x= result_tfidf[10:20][index].tfidf[::-1],\n            y= result_tfidf[10:20][index].feature[::-1],\n            orientation = 'h',marker = dict(color = random_colours(5),))\n    traces.append(trace)\n\nfor t,y,z in zip(traces,y,z):\n    fig.append_trace(t, y,z)\n\n    fig['layout'].update(height=800, width=840,\n    margin=dict(l=100,r=5,b=40,t=90,pad=5), showlegend=False, title='Feature Importance based on Tf-Idf measure')\n\niplot(fig, filename='horizontal-bar')","3e107879":"# Train sample \nprint(\"How training data looks like at this stage (example of one recipe):\")\nprint(str(features[0]) + '\\n' )\nprint(\"Number of instances: \"+ str(len(features)) + '\\n')\nprint(\"And the target variable:\")\nprint(target[0])","fb0a6092":"# Test Sample - only features - the target variable is not provided.\nfeatures_test = [] # list of lists containg the recipes\nfor item in test_data['ingredients']:\n    features_test.append(item)","1119530a":"print(\"How test data looks like at this stage (example of one recipe):\")\nprint(str(features_test[0]) + '\\n')\nprint(\"Number of instances: \"+ str(len(features_test)))","c9f88500":"# Both train and test samples are processed in the exact same way\n# Train\nfeatures_processed= [] # here we will store the preprocessed training features\nfor item in features:\n    newitem = []\n    for ingr in item:\n        ingr.lower() # Case Normalization - convert all to lower case \n        ingr = re.sub(\"[^a-zA-Z]\",\" \",ingr) # Remove punctuation, digits or special characters \n        ingr = re.sub((r'\\b(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\b'), ' ', ingr) # Remove different units  \n        newitem.append(ingr)\n    features_processed.append(newitem)\n\n# Test \nfeatures_test_processed= [] \nfor item in features_test:\n    newitem = []\n    for ingr in item:\n        ingr.lower() \n        ingr = re.sub(\"[^a-zA-Z]\",\" \",ingr)\n        ingr = re.sub((r'\\b(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\b'), ' ', ingr) \n        newitem.append(ingr)\n    features_test_processed.append(newitem) ","21a9b432":"# Check for empty instances in train and test samples after processing before proceeding to next stage of the analysis    \ncount_m = []    \nfor recipe in features_processed:\n    if not recipe:\n        count_m.append([recipe])\n    else: pass\nprint(\"Empty instances in the preprocessed training sample: \" + str(len(count_m)))  ","ff3921f6":"count_m = []    \nfor recipe in features_test_processed:\n    if not recipe:\n        count_m.append([recipe])\n    else: pass\nprint(\"Empty instances in the preprocessed test sample: \" + str(len(count_m)))    ","8328d7da":"# Binary representation of the training set will be employed\nvectorizer = CountVectorizer(analyzer = \"word\",\n                             ngram_range = (1,1), # unigrams\n                             binary = True, #  (the default is counts)\n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,  \n                             max_df = 0.99) # any word appearing in more than 99% of the sample will be discarded","7e0b6b3f":"# Fit the vectorizer on the training data and transform the test sample\ntrain_X = vectorizer.fit_transform([str(i) for i in features_processed])\ntest_X =  vectorizer.transform([str(i) for i in features_test_processed])","41419738":"# Apply label encoding on the target variable (before model development)\nlb = LabelEncoder()\ntrain_Y = lb.fit_transform(target)","4ab17d2f":"# Ensemble Unigram model (baseline model) - parameters are not tuned at this stage\nvclf=VotingClassifier(estimators=[('clf1',LogisticRegression(random_state = 42)),\n                                  ('clf2',SVC(kernel='linear',random_state = 42,probability=True)),\n                                  ('clf3',RandomForestClassifier(n_estimators = 600,random_state = 42))], \n                                    voting='soft', weights = [1,1,1]) \nvclf.fit(train_X, train_Y)","003c2b29":"# 10-fold Cross validation of  the results\nkfold = model_selection.KFold(n_splits=10, random_state=42)\nvalscores = model_selection.cross_val_score(vclf, train_X, train_Y, cv=kfold)\nprint('Mean accuracy on 10-fold cross validation: ' + str(np.mean(valscores))) #  0.8005731359034913","3ba45fca":"# Generate predictions on test sample\npredictions = vclf.predict(test_X) \npredictions = lb.inverse_transform(predictions)\npredictions_final = pd.DataFrame({'cuisine' : predictions , 'id' : test_data.id }, columns=['id', 'cuisine'])\npredictions_final.to_csv('Final_submission.csv', index = False)","9dd2c7c2":"#### *Now let's make some vizualizations of our findings about the \"special\" ingredients *","2336e006":"#### *Initial look of the training data:*","bd71fcb9":"The distribution of recipe length is right-skewed as we can see from the histogram above.","35a7c7f3":"From the box plots of recipe length distributions, we can make several observations: <br>\n- the Moroccan cuisine seems to have the longest recipes on average compared to all the rest cuisines in our sample;\n- we observe the opposite phenomenon for the Irish, British, French and Southern_us cuisine;\n- there exist outliers in all cuisines (in some of them - many);\n- most of the Asian cuisines (Vietnamese, Thai, Indian) turn out to have larger recipes on average than most of the rest cuisines which means that our guess in [Take a closer look at the extremes](#-Take-a-closer-look-at-the-extremes-) about their length in not accurate; \n- recipes part of the European cuisine tend to be with average length or shorter compared to the rest of the sample.","f2553115":"This Kaggle competition asks us to predict the category of a dish's cuisine given a list of its ingredients. The dataset is provided by Yummly.  Be careful because working on this data can make you really hungry!!! \ud83d\ude0b  <br><br>\nThis notebook provides a step-by-step analysis and solution to the given problem. It can also serve as a great starting point for learning how to explore, manipulate, transform and learn from textual data. It is divided into three main sections: \n- Exploratory Analysis - as a first step, we explore the main characteristics of the data with the help of Plotly vizualizations; \n- Text Processing - here we apply some basic text processing techniques in order to clean and prepare the data for model development; \n- Feature Engineering & Data Modeling - in this section we extract features from data and build a predictive model of the cuisine.  <br>\n\n\n **And now..let the cooking in Jupyter Notebook begin! \ud83e\udd67 **\n \n Author: Gloria Hristova - https:\/\/www.kaggle.com\/gloriahristova","1df41fc0":"  # II.What's Cooking? - Text Processing","01689c3c":"*Important Note: For the Tf-Idf representation we are using as vocabulary the ingredients as they are given in the data - no unigrams or bigrams are extracted at this stage. Also bear in mind that the data is not processed\/cleaned.*","050edd18":"The test and train data provided in this Kaggle competition are in json format. We have imported the data as a data frame object and the above lines of code show us the initial look of both samples. We observe that each recipe is a separate row and has:\n- a unique identifier - the 'id' column; \n- the type of cuisine in which this recipe falls - this is our target variable (the test sample does not have this column);\n- a list object with ingredients (the recipe) - this will be the main source of explanatory variables in our classification problem.\n\n**Problem statement**: Predict the type of cuisine based on given data (ingredients). This is a classification task which requires text processing and analysis.","fae77674":"The longest recipe in our sample is part of the Italian cuisine. However, Italian cuisine is often associated with simple recipes - having not so many ingredients but still being extremely delicious. :)  <br><br>\n65 ingredients can be considered too many for any recipe. Making a quick google search on \"How many ingredients has the longest recipe in the world?\" does not give us a definite answer but one of the first results is a similar question posted on Reddit: <br>\n__( https:\/\/www.reddit.com\/r\/recipes\/comments\/4ir04r\/question_most_complex_or_recipe_with_longest_list\/ )__\n <br><br>\nHere we can find a recipe for Beef Wellington consisting of around 30 ingredients! Well, this is already a huge number but still 65 is beyond all that one can even imagine. So, our guess is that the case is more about wrong data in our sample rather than anything else.  \n\nIt seems that most of the shortest recipes are part of the Asian cuisine - Indian, Japanese. Some of the recipes consist only of rice which to great extent is possible, but several of them (like water and butter only) look suspiciously wrong. It should be considered whether this data is wrong (for example, this is not the whole recipe due to some reasons like technical errors). <br><br>\nAs far the Asian cuisine is considered - as a next step we are going to explore the distribution of recipe length in each cuisine which will shed light on the question whether short recipes are something typical for the Asian region.","bcb3cf35":"### * 2) Import json files with train and test samples *","7ad46bcd":"#### *Vizualizations of Important Features according to Tf-Idf measure*","67c3c894":"###  * 1) Prepare the train and test samples for model development*","c070f307":"Mmmmmm tacos and tortillas...Yummy! \ud83c\udf2e","18653146":"There are 20 different categories (cuisines) which we are going to predict. <br> \n**This means that the problem at hand is a multi-class classification (there are more than 2 categories to predict).** <br>","48a47e53":"  # III. What's Cooking? - Feature Engineering & Data Modeling","14085ada":"\ud83e\udd55 It seems that salt is the most commonly used ingredient which is not surprising at all! We also find water, onions, garlic and olive oil - not so surprising also. :) <br> \n Salt is such a common ingredient that we expect it to have poor predictive power in recognizing the type of cuisine.","03c6334c":"  # What's Cooking? \ud83d\ude0b\ud83c\udf5c","d1be2ec4":"#### *Take a quick look on test sample also:*","0e4dc29f":"#### * Create also a plot with label distribution *","f39e58b9":"#### * Which are the most common ingredients in the whole training sample? How many unique ingredients can we find in the dataset? *","841f0ca8":"\ud83c\udf5b From the two vizualizations above we can make several observations: <br>\n- the Mexican cuisine is the only one where we find that the unique ingredients are also found in many recipes part of the cuisine in question. The refried beans are found more than 200 times in Mexican recipes (250 out of 6438 recipes ~ 4% of all Mexican recipes). This finding means that the 'refried beans' and 'taco' (or derivatives of those) ingredients are expected to be strong predictors of Mexican cuisine. On the next step of the analysis we will try to support this observation by using Tf-Idf representation of our training sample in an attempt to find the most important ingredients in a cuisine.\n- the above-mentioned observation is also valid for the:\n    - the Italian gnocchi -   __[Wikipedia - Gnocchi](https:\/\/en.wikipedia.org\/wiki\/Gnocchi)__;\n    - Brazilian cuisine with its traditional cachaca which is the most popular spirit among distilled alcoholic beverage in Brazil (or at least according to __[Wikipedia - Cachaca](https:\/\/en.wikipedia.org\/wiki\/Cacha%C3%A7a)__).\n    - Indian fenugreek - in our sample it is found only in the Indian cuisine which is surprising since it can be found in the whole Asian region __[Wikipedia - Fenugreek](https:\/\/en.wikipedia.org\/wiki\/Fenugreek)__) and also because in my home-country (Bulgaria) we also use it a lot which means that it is not specific only to the Asian region. :)","5f13d5b3":"Now the data is prepared for building a prediction model based on ingredients. At this step of the analysis we made some simple but important transformations on the data:\n- Case normalization - all words are  converted to lower case;\n- Removal of punctuation, digits or special characters - they are not considered as informative and correlated with the problem;\n- Removal of different units measuring the ingredients - they can also be considered as not so related to the problem at hand and therefore can be excluded from further analysis. <br>\n\nPerforming this simple manipulation on data can give us confidence that a significant amount of 'noise' in our data is removed which is likely to lead to better results during the data modeling stage.","6183ccc1":"#### *Explore which ingredients occur **only in one cuisine** (ingredients specific to the cuisine) *","53538a63":"From the table and the plot of label distribution, we observe that the most common category in our sample is the Italian cuisine, followed by the Mexican - they represent around 36% of our training sample. The least represented cuisines are the Irish, Jamaican, Russian and Brazilian - counting for only 6% of our training sample of recipes.","2c9a78e0":"###  * 1) Feature engineering*","311f9bc0":"From the above bar chart, we can see that it is not necessary that cuisines with more instances in the training sample should be associated with more ingredients representing them. It turns out that the French cuisine which is 6,65% of the training sample has more variability in ingredients than the Indian cuisine (this observation is unexpected since Indians use many spices in their recipes - __[Wiki-Indian cuisine](https:\/\/en.wikipedia.org\/wiki\/Indian_cuisine)__ ).   ","f72ccaaa":"From the vizualizations above we can make the following observations: <br>\n- several of our assumptions in [this section of the analysis](#Now-let's-make-some-vizualizations-of-our-findings-about-the-special-ingredients-) were accurate - cachaca is major ingredient in Brazilian recipes and tortillas are important in Mexican cuisine.\n- some of the important words appear in several cuisines, BUT still this observation holds true only if the cuisines are in the same region:\n    - In Asian cuisines we find that fish, sauce, soy and sesame are all with higher tf-idf values;\n    - butter, sugar, eggs and flour are found in several European cuisines;\n    - Greek, Spanish and Italian cuisines all share olive oil and cheese.\n- another observation we can make is that different spices (rather than food like fruits, vegetables, fish or meat) dominate the Moroccan, Indian and Jamaican cuisines.","2d0a923d":"#### *Define a function for generating colours at random - it will be used for vizualizations in the analysis*","2cc274ee":"#### * Explore how many different ingredients can be found in each cuisine *","31ff02d3":"#### *Find the most important ingredients in each cuisine using Tf-Idf transformation*","1b086234":"![Food Map](https:\/\/pbs.twimg.com\/media\/C81zFXiXoAAYkXf.jpg:large)","3b3b7b25":"###  * 1) Load necessary libraries*","d9c9d4d2":"  # I.What's Cooking? - Exploratory Analysis","bc7c53ee":"#### * Explore recipe length distribution in each cuisine *","3e5f87dc":"Our first attempt to describe the data and build a prediction model of the cuisine consists of creating a Unigram Ensemble model which combines the decisions of the following three classifiers: \n- Logistic Regression;\n- Linear Support Vector Classifier;\n- Random Forest.\n\nPredictions generated by the three classifiers are given equal weights in the final prediction.\nNo hyperparameter tuning is applied at this stage which can be considered as a future opportunity for improvement of the predictive power of the model. <br><br>\nAlso, the baseline model can be improved by using bigrams or higher order features (n-grams). \nOur baseline model is validated via 10-fold cross validation and the measured average accuracy on train sample is 80,05%. <br><br> \nConsidering the results and the fact that we are using only unigrams (~3000 features only) and machine learning models are not tuned - this is a pretty good baseline model. Indeed, good cooking (and analyzing) lies in simplicity. \ud83d\ude0b","c75649d4":"**During the feature engineering stage we transform the textual data into more suitable format for performing mathematical operations and statistical learning techniques. In other words - we turn text into numbers.**\n\nWe choose to use binary representation which is very simple to understand and implement. Also for simplicity we choose to use only unigrams for building the prediction model. This means that in model development will be included features consisting of single words only. This can be considered as a good baseline and starting point for further analysis. <br> Finally, we exclude words appearing in more than 99% of our training instances because they can't be considered as good differentiators of data.","ee82a572":"#### * Create a table giving information on the number of times each cuisine is represented in the training sample *","4342fe03":"### * 4) Let's take a closer look at the ingredients in our training sample *","8fe0c20b":"We will proceed the analysis by performing some simple data processing. The aim is to prepare the data for model development.","cc10750b":"### * 3) Now let's explore a little bit more the target variable *","52075c39":"###  * 2) Model Development*","89cbbe94":"#### * Take a closer look at the extremes *","fe0d7ed4":"Tf-Idf measure will simply point out which are the features (words) that are important for a given cuisine. To put it simply - these will be ingredients that are frequently used in the recipes belonging to that cuisine and at the same time not so frequently used in the whole sample of recipes.  <br><br> \nHigher tf-idf score means higher importance of the word for the given cuisine and the opposite (for more information on the measure and how it is calculated:__[Tf-Idf](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf)__ ). <br><br>\nThe result of the Tf-Idf transformation of our dataset is a matrix of tf-idf scores with one row per recipe and as many columns as there are different ingredients in the dataset.","b3f7e201":"# If you find my analysis useful and like it, please don't forget to upvote. Thank you!\ud83d\ude43\ud83c\udf6d","8082cd83":"#### * Explore the distribution of recipe length *"}}