{"cell_type":{"6d580f62":"code","e11ca1b2":"code","d57c2099":"code","d3cc03fa":"code","c461166e":"code","e7e360ad":"code","5fa2e0fb":"code","41ccf7aa":"code","b5dfba60":"code","35e2c698":"code","caff1dbd":"code","3b4071d5":"code","83a337f8":"code","bb2b1747":"code","3a3cd98e":"code","ac974b15":"code","62de389c":"code","e41f2739":"code","c8249bd2":"code","dd42c11d":"code","73638059":"code","fffb5bc3":"code","7c135076":"code","9b25d6f3":"code","4150bd2c":"code","d9591918":"code","03a84868":"markdown","d0d64adc":"markdown","ad864627":"markdown","9b2f9887":"markdown","47acfa2e":"markdown","33694d75":"markdown","5baac75b":"markdown","98b4b5e8":"markdown","f9691d39":"markdown","bb64f74a":"markdown","8bf58ad1":"markdown","ea1fa73b":"markdown"},"source":{"6d580f62":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud,STOPWORDS\nstopwords = list(STOPWORDS)\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score \nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer as CVTZ\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\ndef RMSE(Y,YHAT):\n    return np.sqrt(mean_squared_error(Y,YHAT))\n\nplt.rc('figure',figsize=(20,11))","e11ca1b2":"t_data = pd.read_csv('\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\nt_data.head(3)","d57c2099":"sns.heatmap(t_data.isna().sum().to_frame(),annot=True,cmap='mako')\nplt.xlabel('Amount Missing',fontsize=15)\nplt.show()","d3cc03fa":"def remove_stop_words(sir):\n    splited = sir.split(' ')\n    splited = [word for word in splited if word not in stopwords]\n    return ' '.join(splited)\n\nt_data.Review = t_data.Review.apply(remove_stop_words)","c461166e":"sid = SentimentIntensityAnalyzer()\n\ndef get_char_count(sir):\n    return len(sir)\ndef get_word_count(sir):\n    return len(sir.split(' '))\ndef get_average_word_length(sir):\n    aux = 0\n    for word in sir.split(' '):\n        aux += len(word)\n    return aux\/len(sir.split(' '))\ndef get_pos_sentiment(sir):\n    sent = sid.polarity_scores(sir)\n    return sent['pos']\ndef get_neg_sentiment(sir):\n    sent = sid.polarity_scores(sir)\n    return sent['neg']\ndef get_neu_sentiment(sir):\n    sent = sid.polarity_scores(sir)\n    return sent['neu']","e7e360ad":"t_data['Char_Count'] =  t_data.Review.apply(get_char_count)\nt_data['Word_Count'] =  t_data.Review.apply(get_word_count)\nt_data['Average_Word_Length'] =  t_data.Review.apply(get_average_word_length)\nt_data['Positive_Sentiment'] =   t_data.Review.apply(get_pos_sentiment)\nt_data['Negative_Sentiment'] = t_data.Review.apply(get_neg_sentiment)\nt_data['Neutral_Sentiment'] =t_data.Review.apply(get_neu_sentiment)","5fa2e0fb":"word_list = ''\nfor word in t_data.Review:\n    splited = word.lower()\n    word_list +=splited\n    \nwordcloud = WordCloud(width=800,height=800,background_color='white',stopwords=stopwords,min_font_size=5).generate(word_list)\nplt.figure(figsize = (25, 15), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","41ccf7aa":"ex.box(t_data,x='Rating',y='Positive_Sentiment',notched=True,title='Rating Positive Sentiment Distributions')","b5dfba60":"ex.box(t_data,x='Rating',y='Negative_Sentiment',notched=True,title='Rating Positive Sentiment Distributions')","35e2c698":"sns.pairplot(t_data)","caff1dbd":"sns.jointplot(x=t_data['Average_Word_Length'],y=t_data['Positive_Sentiment'],height=15,kind='kde',levels=20)","3b4071d5":"print('Average_Word_Length Skew: ',t_data['Average_Word_Length'].skew(),\"  Average_Word_Length Kurtosis\",t_data['Average_Word_Length'].kurt())","83a337f8":"print('Average_Word_Length Mean: ',t_data['Average_Word_Length'].mean(),\"  Average_Word_Length Median\",t_data['Average_Word_Length'].median(),' Average_Word_Length Mode : ',t_data['Average_Word_Length'].mode()[0])","bb2b1747":"data_info = t_data.describe()\ndata_info.loc['skew'] = t_data.skew()\ndata_info.loc['kurt'] = t_data.kurt()\ndata_info","3a3cd98e":"tout_l = t_data.copy()\ntout_l['OLL'] = 'Normal'\ntout_l.loc[tout_l[tout_l['Word_Count']>1000].index,'OLL']= 'Outlier'\ntout_l.loc[tout_l[tout_l['Neutral_Sentiment']<0.25].index,'OLL']= 'Outlier'\ntout_l.loc[tout_l[tout_l['Neutral_Sentiment']>0.98].index,'OLL']= 'Outlier'\n\nex.scatter_3d(tout_l,x='Rating',y='Neutral_Sentiment',z='Word_Count',color='OLL')","ac974b15":"t_data = t_data[t_data['Neutral_Sentiment']>0.25]\nt_data = t_data[t_data['Neutral_Sentiment']<0.98]\nt_data = t_data[t_data['Word_Count']<1000]","62de389c":"cors = t_data.corr('pearson')\nplt.figure(figsize=(20,13))\nsns.heatmap(cors,annot=True,cmap='mako')","e41f2739":"t_data.head(3)","c8249bd2":"train_x,test_x,train_y,test_y = train_test_split(t_data[['Positive_Sentiment','Negative_Sentiment','Average_Word_Length']],t_data['Rating'])\n\nGN_Pipe = Pipeline(steps=[('model',GaussianNB())])\nGN_Pipe.fit(train_x,train_y)\nGN_predictions= GN_Pipe.predict(test_x)\n#GN_predictions = np.round(LR_predictions)\ncfm = confusion_matrix(GN_predictions,test_y)\n\nplt.figure(figsize=(20,13))\nplt.title('Naive Bayes Confusion Matrix',fontsize=20)\nsns.heatmap(cfm,annot=True,cmap='mako',fmt='d',xticklabels=[1,2,3,4,5],yticklabels=[1,2,3,4,5])","dd42c11d":"print('accuracy: ',accuracy_score (LR_predictions,test_y))","73638059":"DT_Pipe = Pipeline(steps=[('model',DecisionTreeClassifier())])\nDT_Pipe.fit(train_x,train_y)\npredictions= DT_Pipe.predict(test_x)\ncfm = confusion_matrix(predictions,test_y)\n\nplt.figure(figsize=(20,13))\nplt.title('Decision Tree Confusion Matrix',fontsize=20)\nsns.heatmap(cfm,annot=True,cmap='mako',fmt='d',xticklabels=[1,2,3,4,5],yticklabels=[1,2,3,4,5])","fffb5bc3":"print('accuracy: ',accuracy_score (predictions,test_y))","7c135076":"tf_model = CVTZ()\nN_COMPONENTS = 900\n\nsvd_model = TruncatedSVD(n_components = N_COMPONENTS)\ndesc_matrix = tf_model.fit_transform(t_data.Review)\ntrunc_matrix = svd_model.fit_transform(desc_matrix)\n\nevr = svd_model.explained_variance_ratio_\nevr_cs = np.cumsum(evr)\ntr1 = go.Scatter(x=np.arange(0,len(evr_cs)),y=evr_cs,name='Explained Variance Cumulative')\ntr2 = go.Scatter(x=np.arange(0,len(evr_cs)),y=evr,name='Explained Variance')\n\nfig = go.Figure(data=[tr1,tr2],layout=dict(title='Explained Variance Ratio Using {} Components'.format(N_COMPONENTS),\n                                          xaxis_title='Number Of Components',yaxis_title='Explained Variance Ratio'))\n\nfig.show()\n","9b25d6f3":"dec_df = pd.DataFrame(trunc_matrix,columns=['PC_{}'.format(i) for i in range(0,900)])\ndec_df.head(3)","4150bd2c":"train_x,test_x,train_y,test_y = train_test_split(dec_df,t_data['Rating'])\n\nGN_Pipe = Pipeline(steps=[('model',GaussianNB())])\nGN_Pipe.fit(train_x,train_y)\nGN_predictions= GN_Pipe.predict(test_x)\ncfm = confusion_matrix(GN_predictions,test_y)\n\nplt.figure(figsize=(20,13))\nplt.title('Naive Bayes Confusion Matrix',fontsize=20)\nsns.heatmap(cfm,annot=True,cmap='mako',fmt='d',xticklabels=[1,2,3,4,5],yticklabels=[1,2,3,4,5])","d9591918":"print('accuracy: ',accuracy_score (GN_predictions,test_y))","03a84868":"### The Average_Word_Length Is Approximately Normally Distributed","d0d64adc":"### Outlier Removal","ad864627":"<a id=\"1.2\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Model Selection And Evaluation<\/h3>\n","9b2f9887":"<a id=\"1.2\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Exploratory Data Analysis<\/h3>\n","47acfa2e":"### We see that the higher the average positive sentiment the higher the rating","33694d75":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Libraries And Utilities<\/h3>","5baac75b":"### And not surprisingly the higher the negative sentiment the lower the rating ","98b4b5e8":"### First Approach","f9691d39":"<a id=\"1.2\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Feature Engineering And Preprocessing<\/h3>\n","bb64f74a":"### So far we see that using sentiments and basic text features we have no segnificant results, we will know try our second approch in which we will vectorize our text data and use our naive bayes model again to try and predict a reduced version of the vectorized text.","8bf58ad1":"### Remove Stopwords From Reviews","ea1fa73b":"<a id=\"1.2\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Data Importation And Missing Value Assessment<\/h3>\n"}}