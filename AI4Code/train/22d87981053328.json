{"cell_type":{"1625c8a2":"code","95f39d04":"code","e692e0b2":"code","679ca4ba":"code","bea490cc":"code","25800c3e":"code","f5a0f500":"code","4bcc3b46":"code","7d844d20":"code","0bc2b3d4":"code","813cf917":"code","e95c4ace":"code","3851416e":"code","930fad54":"code","1dd76f34":"code","7ac3eb96":"code","acb6b8c9":"code","1d608984":"code","0560cec9":"code","255305ae":"code","0e690676":"code","d2090616":"code","29dfc0cf":"code","3810e4e2":"code","2dfeb937":"code","cfc5906b":"code","901017d4":"code","0e627cc4":"code","d61cb9dd":"code","27b709fc":"code","f075a5d6":"code","86953606":"code","03ebf5d8":"code","76698a30":"code","05b4b2bf":"code","54c221f9":"code","37319a84":"code","1401085d":"code","c2429765":"code","d03a16b1":"code","002f6aa2":"code","06f59517":"code","9b2840a6":"code","83a7b758":"code","43acc9ad":"code","f6a56c5d":"code","5b6b4df0":"code","5c493046":"code","f3049a22":"markdown","fc400096":"markdown","faef47aa":"markdown","483c93a5":"markdown","eaf52e17":"markdown","52b7c045":"markdown","3f9a7182":"markdown","2a1ae9db":"markdown","39b3b1b4":"markdown","c13cf11d":"markdown","38d1b532":"markdown","1f3eef09":"markdown","efcd894a":"markdown","59328e5d":"markdown","cdadc9d8":"markdown","4f0ebc30":"markdown","67fd78fb":"markdown","07f488c3":"markdown","2103e100":"markdown","4bd72ed5":"markdown","0e348a41":"markdown","ae772ad9":"markdown","343ec4c7":"markdown","e8bcb06f":"markdown","b8c03d3b":"markdown","b53e4ec9":"markdown","df37795d":"markdown","68bfa655":"markdown","618aac7a":"markdown","03bf7733":"markdown","d2f2c233":"markdown","630e8584":"markdown","89c9b945":"markdown","c4e1ee41":"markdown","2b19168c":"markdown","90276a7a":"markdown","8a702cb7":"markdown","2299e756":"markdown"},"source":{"1625c8a2":"import time\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas import json_normalize\n\nimport datetime\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')","95f39d04":"def load_df(csv_path, nrows = None):\n    json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    df = pd.read_csv(csv_path,\n                     #converters are dict of functions for converting values in certain columns. Keys can either be integers or column labels.\n                     #json.loads() method can be used to parse a valid JSON string and convert it into a Python Dictionary.\n                     #It is mainly used for deserializing native string, byte, or byte array which consists of JSON data into Python Dictionary.\n                     converters = {col: json.loads for col in json_cols},                                                                         \n                         dtype = {'fullVisitorId': 'str'}, # Important!!\n                         nrows = nrows)\n    for col in json_cols:\n        # for each column, flatten data frame such that the values of a single col are spread in different cols\n        # This will use subcol as names of flat_col.columns\n        flat_col = json_normalize(df[col])\n        # Name the columns in this flatten data frame as col.subcol for tracability\n        flat_col.columns = [f\"{col}.{subcol}\" for subcol in flat_col.columns]\n        # Drop the json_col and instead add the new flat_col\n        df = df.drop(col, axis = 1).merge(flat_col, right_index = True, left_index = True)\n    return df","e692e0b2":"csv_train_path = '..\/input\/ga-customer-revenue-prediction\/train_v2.csv'\ncsv_test_path = '..\/input\/ga-customer-revenue-prediction\/test_v2.csv'","679ca4ba":"%%time\ntrain = load_df(csv_train_path, nrows = 120000)\ntest = load_df(csv_test_path, nrows = None)\ntrain.shape, test.shape","bea490cc":"train.loc[:2]","25800c3e":"cols = train.columns\ncols","f5a0f500":"train['totals.transactionRevenue'] = train['totals.transactionRevenue'].astype('float')\n#To have it as a data frame with fullVisitorId and totals.transactionRevenue we need to reset_index(). Otherwise, it would return a serie\ntarget = train.groupby('fullVisitorId')['totals.transactionRevenue'].sum().reset_index()\ntarget[:4]","4bcc3b46":"fig, ax = plt.subplots(figsize = (10,5))\nax = sns.scatterplot(x = range(0, len(target)), y = np.log1p(target['totals.transactionRevenue']), color = 'g', marker = 'o')\nax.set_xlabel('User index')\nax.set_ylabel('Transaction revenue (log)')\nax.set_title('Natural logarithm of agregate revenue generated from each user')","7d844d20":"train.fullVisitorId.nunique()","0bc2b3d4":"ax = sns.countplot(pd.cut(target['totals.transactionRevenue'],[-1,0,1e9]))\nax.set_xticklabels(['Users with 0$ transaction','Users with positive transacton'])\nax.set_xlabel('Revenue', fontsize = 14)","813cf917":"temp_rev = target[target['totals.transactionRevenue'] != 0]\ntemp_rev.sort_values(by = 'totals.transactionRevenue', ascending = False)[:4]","e95c4ace":"print('number of users with transaction:',len(temp_rev))\nprint('number of total users:',len(target))\nprint('number of total transactions:', len(train))\nprint(np.round(np.sum(target['totals.transactionRevenue'] != 0)*100 \/ len(target),2), 'percent of users generate revenue!!')","3851416e":"ax = sns.countplot(pd.cut(temp_rev['totals.transactionRevenue'],[0,1e7,0.5e8,1e9]))\nax.set_xticklabels(['0 - 10M','10M - 50M','50M - 1B'])\nax.set_xlabel('Total sum of transaction from users contributing to revenue generation')","930fad54":"train_trans_rev = train[train['totals.transactionRevenue'] > 0]\ntrain_trans_non_rev = train[train['totals.transactionRevenue'].isna()]\nlen(train_trans_rev), len(train_trans_non_rev)","1dd76f34":"some_cols = ['customDimensions','channelGrouping', 'socialEngagementType', 'visitNumber',\n       'device.browser', 'device.operatingSystem', 'device.isMobile','device.deviceCategory',\n       'geoNetwork.continent', 'geoNetwork.subContinent', 'geoNetwork.country',\n       'geoNetwork.region', 'geoNetwork.metro', 'geoNetwork.city','geoNetwork.networkDomain', \n       'totals.visits', 'totals.sessionQualityDim', 'totals.newVisits', 'totals.timeOnSite', 'totals.hits',\n       'trafficSource.source', 'trafficSource.medium',\n       'trafficSource.isTrueDirect', 'trafficSource.referralPath', 'trafficSource.campaign'\n       ]\nfig, ax = plt.subplots(25, 2, figsize = (30,80))\n#fig.tight_layout()\nplt.subplots_adjust(left = 0.1,\n                    bottom = 0.1, \n                    right = 0.9, \n                    top = 0.9, \n                    wspace = 0.4, \n                    hspace = 0.4)\ni = 0\nfor col in some_cols:\n    sns.countplot(data = train_trans_rev, y = col, ax = ax[i,0], order = train_trans_rev[col].value_counts().iloc[:6].index) # Only the first top 6 value counts are shown\n    ax[i,0].tick_params(axis = 'both', which = 'major', labelsize = 16)\n    ax[i,0].set_ylabel(col, fontsize = 16)\n    sns.countplot(data = train_trans_non_rev, y = col, ax = ax[i,1], order = train_trans_non_rev[col].value_counts().iloc[:6].index) # Only the first top 6 value counts are shown\n    ax[i,1].tick_params(axis = 'both', which = 'major', labelsize = 16)\n    ax[i,1].set_ylabel('', fontsize = 16)\n    i += 1\nfig.suptitle('Revenue generating users [left] vs non revenue generating users [Right]', fontsize = 20)\nfig.subplots_adjust(top = 0.97)\n","7ac3eb96":"missing_val = pd.DataFrame()\nfor col in cols:\n    na_count = train[col].isna().sum()\n    if na_count != 0:\n        missing_val.loc[col, 'NaN_val(%)'] = na_count\/len(train)*100\nprint('Number of columns with missing values in train set:', len(missing_val))\nmissing_val.sort_values('NaN_val(%)', inplace = True)\nmissing_val[:5]","acb6b8c9":"fig, ax = plt.subplots(figsize = (12, 8))\nsns.barplot(data = missing_val, x = 'NaN_val(%)', y = missing_val.index , ax = ax)\nax.set_xlabel('Missing Data (%)')\n_ = ax.bar_label(ax.containers[0])","1d608984":"const_cols = []\nfor col in cols:\n     if train[col].nunique() == 1: const_cols.append(col)\nprint('Number of columns with constant values:',len(const_cols))\nconst_cols","0560cec9":"train['totals.bounces'].unique()","255305ae":"y_train_ = train['totals.transactionRevenue']\ny_train_.fillna(0, inplace = True)\ny_train_.astype('float')","0e690676":"missing_val[-4:]","d2090616":"for col in missing_val.index:\n    if missing_val.loc[col, 'NaN_val(%)'] > 48: \n        train.drop(col, axis = 1, inplace = True)\n    else: \n        train[col].fillna('0', inplace = True)\n        test[col].fillna('0', inplace = True)","29dfc0cf":"for col in const_cols:\n    if col not in missing_val.index:\n        train.drop(col, axis = 1, inplace = True)\n\nprint('Number of columns in the training set',len(train.columns))","3810e4e2":"train.info()","2dfeb937":"irrelavant = ['fullVisitorId', 'visitId', 'trafficSource.campaign']\nfor col in irrelavant:\n    train.drop(col, axis = 1, inplace = True)","cfc5906b":"train['visitStartTime']","901017d4":"for df in [train, test]:\n    print(df['visitStartTime'][0])\n    df['visitStartTime'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['rec_dayofweek'] = df['visitStartTime'].dt.dayofweek\n    df['rec_hours'] = df['visitStartTime'].dt.hour\n    df['rec_dayofmonth'] = df['visitStartTime'].dt.day\n    print(df['visitStartTime'][0], df['rec_dayofweek'][0], train['rec_hours'][0], df['rec_dayofmonth'][0])\n    df.drop('visitStartTime', axis = 1, inplace = True)","0e627cc4":"le = LabelEncoder()\nprint('Columns that are converted to numerical values with label encodedr:')\nfor col in train.columns:\n    if train[col].dtype == 'O':\n        print(col)\n        #print(col, train[col].unique())\n        train.loc[:, col] = le.fit_transform(train.loc[:, col])\n        test.loc[:, col] = le.fit_transform(test.loc[:, col])","d61cb9dd":"for col in train.columns:\n    train[col] = train[col].astype('float')","27b709fc":"train.info()","f075a5d6":"fullvisitorid = []\nfor col in test.columns:\n    if col == 'fullVisitorId':\n        fullvisitorid = test[col] \n    if col not in train.columns:\n        test.drop(col, axis = 1, inplace = True)\ntest.info()","86953606":"for col in test.columns:\n    test[col] = test[col].astype('float')","03ebf5d8":"model = lgb.LGBMRegressor(\n        num_leaves = 31,  #(default = 31) \u2013 Maximum tree leaves for base learners.\n        learning_rate = 0.03, #(default = 0.1) \u2013 Boosting learning rate. You can use callbacks parameter of fit method to shrink\/adapt learning rate in training using \n                              #reset_parameter callback. Note, that this will ignore the learning_rate argument in training.\n        n_estimators = 1000, #(default = 100) \u2013 Number of boosted trees to fit.\n        subsample = .9, #(default = 1.) \u2013 Subsample ratio of the training instance.\n        colsample_bytree = .9, #(default = 1.) \u2013 Subsample ratio of columns when constructing each tree\n        random_state = 34\n)","76698a30":"train['date'].min(), train['date'].max()","05b4b2bf":"x_train = train[ train['date'] < 20171101 ]\nx_valid = train[ train['date'] >= 20171101 ]\nprint('Number of samples in train set:', len(x_train))\nprint('Number of samples in validation set:',len(x_valid))","54c221f9":"y_train_len = len(x_train)\ny_train = y_train_[:y_train_len]\ny_valid = y_train_[y_train_len:]","37319a84":"x_train.drop('date', axis = 1, inplace = True)\nx_valid.drop('date', axis = 1, inplace = True)","1401085d":"print(len(x_train), len(y_train))\nprint(len(x_valid), len(y_valid))","c2429765":" model.fit(\n        x_train, np.log1p(y_train),\n        eval_set = [(x_valid, np.log1p(y_valid))],\n        early_stopping_rounds = 50,\n        verbose = 100,\n        eval_metric = 'rmse'\n    )","d03a16b1":"feat_impr = pd.DataFrame()\nfeat_impr['feature'] = x_train.columns\nfeat_impr['importance'] = model.booster_.feature_importance(importance_type = 'gain')\nfeat_impr.sort_values(by = 'importance', ascending = False)[:10]","002f6aa2":"plt.figure(figsize = (8,5))\nsns.barplot(x = 'importance', y = 'feature', data = feat_impr.sort_values('importance', ascending = False)[:15])","06f59517":"valid_preds = model.predict(x_valid, num_iteration = model.best_iteration_)\nvalid_preds[valid_preds < 0] = 0","9b2840a6":"mean_squared_error(np.log1p(y_valid), valid_preds)","83a7b758":"test_preds = model.predict(test[x_train.columns], num_iteration = model.best_iteration_)\ntest_preds[test_preds < 0] = 0","43acc9ad":"rec_submit = pd.concat([fullvisitorid, pd.Series(test_preds)], axis = 1)\nrec_submit.columns = ['fullVisitorId','PredictedLogRevenue']\nrec_submit","f6a56c5d":"user_submit = rec_submit.groupby('fullVisitorId').sum().reset_index()\nuser_submit","5b6b4df0":"user_submit.to_csv('submission.csv', index = False)","5c493046":"s = pd.read_csv('submission.csv')\ns","f3049a22":"### Splitting the train set and validation set\nSince in real cases we have access to the data up to a certain date, and then we should predict the transactions for future, we'll pick a date for splitting to data into training set and validaton set. We will later remove the column related to date.","fc400096":"A very low percentage of users contribute in generating revenue. Let's have a closer look in them. To see how many they are and how do they contribute to the revenue generation.","faef47aa":"### b. For each User","483c93a5":"### d. Visualizing missing values","eaf52e17":"Checking the format is right:","52b7c045":"### b. Removing columns with missing values","3f9a7182":"### b. Loading the dataset","2a1ae9db":"### c. Removing columns with constant values","39b3b1b4":"### Defining the model","c13cf11d":"### e. Constant columns","38d1b532":"## Objective","1f3eef09":"## 3. Data Cleaning","efcd894a":"### c. Some primary raw visualization\nTo see the distribution of different values for different features, I have added the count plot for some of the columns. Here, I first put all the columns from the trianing data, but then removed the ones that doesn't give an informative count plot. Therefore, only information about some of the columns are visualized here.","59328e5d":"### Feature importance","cdadc9d8":"For loading the dataframe with json fields I have used a snippet of the code from anther notebook. You can find the link in the references.[2]","4f0ebc30":"### a. Handling datetime data types\nLet's check the format of visitStartTime","67fd78fb":"16 out of 24 users have had a transaction between 10-50M dollars","07f488c3":"### a. Data visuzalization","2103e100":"### b. Visualization for users generating revenue (target = 1)","4bd72ed5":"### some insights from comparing the results between the users generating revenue vs other users\n- Users generating revenue use Macintosh more than other users\n- Their channel grouping is referral rather than organic search\n- They use more Chrome\n- They usually visit more than once\n- Their network domain is less unknown.unkown\n- They have higher time on site\n- They have higher session quality dimension(*)\n- Their total hits a lot higher\n- They connect less by mobile devices or tablets and use more desktop\n- They are mostly from US, rather than Europe, Asia or Africa\n- They are usually connecting from California and NewYork\n- They usually connect directly and less through google\n\n(*)For calculating session quality dimension, user engagement is evaluated for each session, and the resulting proximity to conversion is expressed as a score of 1-100 for each session during the date range, with 1 being the farthest from and 100 being the closest to a transaction.\n\nWe also see some columns with constant values. We'll remove them when doing the data cleaning.","0e348a41":"## 1. Data Prepration","ae772ad9":"**Predicting revenue generated by each user** <br>\nThe goal with this report here is to show a clear way and necessary steps needed to be taken for similar analysis tasks. Hope you find it helpful!\n<br>\nThis is a notebook with explanation for basic analysis. <br> \nI would appreciate your comments and feedback, if you see some mistakes or have suggestions for improvement of this note book. Otherwise, if you find it helpful throw me an upvote, so I'll cheer my day! :-)","343ec4c7":"## 2. Data Summery","e8bcb06f":"We'll remove all the columns that are in test set but not in train set. ","b8c03d3b":"## 4. Feature Engineering and outliers","b53e4ec9":"## 7. References\n\n[1] https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-ga-customer-revenue\/notebook <br>\n[2] https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook <br>\n[3] https:\/\/www.kaggle.com\/ogrellier\/teach-lightgbm-to-sum-predictions <br>\n[4] https:\/\/towardsdatascience.com\/10-tricks-for-converting-numbers-and-strings-to-datetime-in-pandas-82a4645fc23d","df37795d":"### b. Converting the categorical columns to numerical type\nIt is possible to use factorize method as well. Here we have oused label encoder to convert categorical data into numerical.","68bfa655":"## 6. Inference\n### a. For each record (row) of data","618aac7a":"## 5. Model Training","03bf7733":"### Trainnig the model","d2f2c233":"### a. Loading libraries","630e8584":"There are 4000 records for 3710 users.","89c9b945":"We see some columns with more than 50% missing values. We'll later remove them in data cleaning. <br>\ntotals.newVisits shows to have only nan and 1 as values. This is not informative. We'll remove that too.\n","c4e1ee41":"### a. Defining the target ","2b19168c":"This exact amount of threshold of Nan_value(%) for removing columns with missing values is chosen based on the plot from missing value. but roughly around 50% seems to be a reasonable choice. In the code block below I chose 48 as threshod. <br> Of course we should be carefull about which columns (features) we are removing. A way for keeping the columns is to replace the missing values with a reasonable replacement values. In this case for example we replaced the missing values in the transaction revevenues with 0.","90276a7a":"*totals.bounces, totals.newVisits, trafficSourcs.isTrueDirect* were among the missing value data. Lets have a closer look to them. ","8a702cb7":"### d. Removing irrelavant features for building a model","2299e756":"To make sure we have the same length for x_train and y_train as well as x_valid and y_valid"}}