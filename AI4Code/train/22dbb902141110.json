{"cell_type":{"09f060b1":"code","f4c51d6d":"code","97496c96":"code","504e6b7e":"code","ed9dfb9d":"code","60e36496":"code","2322d0b1":"code","82a22cff":"code","5716722a":"code","c87c0ee5":"code","6e8f4d24":"code","49399a12":"code","f9c16d3c":"code","3c402b29":"code","7f9ca619":"code","6af000c5":"code","cf306f87":"code","49d91820":"code","c6884621":"code","dd347aaf":"code","309cae33":"code","9fcd579b":"code","b81cabee":"code","4e831131":"code","926afff3":"code","fe76017a":"code","659eae90":"code","b121812a":"code","04364a0b":"code","526f8025":"code","053ffa16":"code","a18b9a76":"code","b99c8ef5":"code","09e4d2c6":"code","f7aea92d":"code","13cd367e":"code","e3539a68":"code","ead9f9af":"code","56cf0696":"code","a4fa2a0d":"code","605b37f7":"code","d2647f28":"code","9bffedd9":"code","d187aac5":"code","cdb62ffd":"code","bba4f8a0":"code","8712e168":"code","5ac5420f":"code","452a0f95":"code","8aa71085":"code","a3fffef8":"code","7380116d":"code","29838131":"code","66f661df":"code","ecd1f63a":"code","836c51a7":"code","342dc718":"code","b6c9ec75":"code","908234cf":"code","80eaf7f6":"code","a319bd2f":"code","f3954761":"code","6f7650db":"code","a014c7af":"code","113ccc97":"code","6676cba5":"code","d9096c84":"code","0d4e1bea":"code","719d51ba":"code","ddbf1bf3":"code","3747654a":"code","b937bdf9":"code","a8e51d7a":"code","242bdbec":"code","84acde73":"code","0c80fc10":"code","777a049d":"code","0afa5236":"code","92b93a3f":"code","0a0ffe1e":"code","067fb234":"code","eadcdb60":"code","30f4d752":"code","e80414df":"code","4475f05d":"code","cea8964a":"code","b30b4265":"code","ca57e5f4":"code","3e5b138a":"code","31f2b26e":"code","a23648fc":"code","dffe48b5":"code","6f2c8741":"code","4d091250":"code","2aa187ac":"code","90b8f660":"code","8c950637":"code","cafbb282":"code","0f15ad68":"code","5fa70685":"code","3a42e3b2":"code","95c8cbd4":"code","933b87c1":"code","8177016d":"code","08d63257":"code","03c06e90":"code","bf669bbb":"markdown","a675f971":"markdown","d5db3de6":"markdown","ce3f60e7":"markdown","82ce059d":"markdown","7592a3a1":"markdown","ff28d441":"markdown","12cbbd99":"markdown","dc85e693":"markdown","4dbc46f3":"markdown","c75efbdb":"markdown","e5a69b8e":"markdown","07d5707e":"markdown","dfdacc2e":"markdown","248b8a07":"markdown","e855fd6f":"markdown","73ead5e4":"markdown","638aad36":"markdown","a241e38c":"markdown","af8b0541":"markdown","d0207ea0":"markdown","64f51318":"markdown","21f67991":"markdown","0d76f3e9":"markdown","89a19d2e":"markdown","8f235da1":"markdown","5dd19ab5":"markdown","2d5f6da8":"markdown","69bd9a11":"markdown","9ca79b28":"markdown","ea83828b":"markdown","224f5323":"markdown","4e1391a9":"markdown","37c7ee0d":"markdown","b1a3dba8":"markdown","939b4590":"markdown","955e6089":"markdown","a9d6a223":"markdown","a0cc3352":"markdown","c4f81164":"markdown","aaebdd08":"markdown","58a50626":"markdown","14039a2c":"markdown","4fa4757e":"markdown","df904f76":"markdown","03c9a379":"markdown","74249e89":"markdown","4ccf6463":"markdown","2805569e":"markdown","8cff13df":"markdown","78d9489b":"markdown","a435651a":"markdown","d99e96eb":"markdown","2c6ec323":"markdown","593d8760":"markdown","99b94cd7":"markdown","f34788c4":"markdown","98746406":"markdown","78791d12":"markdown","1011d4f2":"markdown","d1bc844c":"markdown","ab7dfa5b":"markdown","c2ef0693":"markdown","d37c123c":"markdown","3a9611a6":"markdown","af78b535":"markdown","217fbe76":"markdown","53788749":"markdown","6bb2d6f4":"markdown","68a37f7c":"markdown","370aac42":"markdown","bdd031de":"markdown","e5f09875":"markdown","c4821141":"markdown","fdfc62d6":"markdown","599a00b9":"markdown","d7fed659":"markdown"},"source":{"09f060b1":"!pip install inflection","f4c51d6d":"!pip install xgboost==0.90","97496c96":"import math\nimport pickle\n\nimport pandas as pd\nimport numpy as np\nimport inflection\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom scipy import stats as ss\n#from boruta import BorutaPy\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble      import RandomForestRegressor\nfrom sklearn.linear_model  import LinearRegression, Lasso\nfrom sklearn.metrics       import mean_absolute_error, mean_squared_error #mean_absolute_percentage_error\n\nfrom IPython.core.display import HTML\nfrom IPython.display      import Image","504e6b7e":"def jupyter_settings():\n    %matplotlib inline\n    %pylab inline\n    \n    plt.style.use( 'bmh' )\n    plt.rcParams['figure.figsize'] = [25, 12]\n    plt.rcParams['font.size'] = 24\n    \n    display( HTML( '<style>.container { width:100% !important; }<\/style>') )\n    pd.options.display.max_columns = None\n    pd.options.display.max_rows = None\n    pd.set_option( 'display.expand_frame_repr', False )\n    \n    sns.set()\n    \ndef cramer_v( x, y ):\n    cm = pd.crosstab( x, y ).values\n    n = cm.sum()\n    r, k = cm.shape\n\n    chi2 = ss.chi2_contingency( cm )[0]\n    chi2corr = max( 0, chi2 - (k-1)*(r-1)\/(n-1) )\n\n    kcorr = k - (k-1)**2\/(n-1)\n    rcorr = r - (r-1)**2\/(n-1)\n    return np.sqrt( (chi2corr\/n) \/ ( min( kcorr-1, rcorr-1 ) ) )\n\ndef mean_percentage_error( y, yhat ):\n    return np.mean( ( y - yhat ) \/ y )\n     \n    \ndef mean_absolute_percentage_error( y, yhat ):\n    return np.mean( np.abs( ( y - yhat ) \/ y ) )\n\n    \ndef ml_error( model_name, y, yhat ):\n    mae = mean_absolute_error( y, yhat )\n    mape = mean_absolute_percentage_error( y, yhat )\n    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n    \n    return pd.DataFrame( { 'Model Name': model_name, \n                           'MAE': mae, \n                           'MAPE': mape,\n                           'RMSE': rmse }, index=[0] )\n\ndef ts_cross_validation( x_training, kfold, model_name, model, verbose=False ):\n    mae_list = []\n    mape_list = []\n    rmse_list = []\n    for k in reversed( range( 1, kfold+1 ) ):\n        if verbose:\n            print( '\\nKFold Number: {}'.format( k ) )\n        # start and end date for validation \n        validation_start_date = x_training['date'].max() - datetime.timedelta( days=k*6*7)\n        validation_end_date = x_training['date'].max() - datetime.timedelta( days=(k-1)*6*7)\n\n        # filtering dataset\n        training = x_training[x_training['date'] < validation_start_date]\n        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n\n        # training and validation dataset\n        # training\n        xtraining = training.drop( ['date', 'sales'], axis=1 ) \n        ytraining = training['sales']\n\n        # validation\n        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n        yvalidation = validation['sales']\n\n        # model\n        m = model.fit( xtraining, ytraining )\n\n        # prediction\n        yhat = m.predict( xvalidation )\n\n        # performance\n        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n\n        # store performance of each kfold iteration\n        mae_list.append(  m_result['MAE'] )\n        mape_list.append( m_result['MAPE'] )\n        rmse_list.append( m_result['RMSE'] )\n\n    return pd.DataFrame( {'Model Name': model_name,\n                          'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n                          'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n                          'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ) }, index=[0] )\n","ed9dfb9d":"jupyter_settings()","60e36496":"df_sales_raw = pd.read_csv(\"\/kaggle\/input\/rossmann-store-sales\/train.csv\", low_memory=False)\ndf_store_raw = pd.read_csv(\"\/kaggle\/input\/rossmann-store-sales\/store.csv\", low_memory=False)","2322d0b1":"df_sales_raw.head()","82a22cff":"df_store_raw.head()","5716722a":"df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')","c87c0ee5":"df1 = df_raw.copy()","6e8f4d24":"df1.head()","49399a12":"df1.columns","f9c16d3c":"oldcols = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo',\n       'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n       'CompetitionDistance', 'CompetitionOpenSinceMonth',\n       'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n       'Promo2SinceYear', 'PromoInterval']\n\nsnakecase = lambda x: inflection.underscore(x)\n\ncols_news = list(map(snakecase, oldcols))\n\ndf1.columns = cols_news","3c402b29":"#Data dimensions\n\nprint(\"number of rows: {}\".format(len(df1)))\nprint(\"number of columns: {}\".format(df1.shape[1]))","7f9ca619":"#Data types\n\ndf1.info()","6af000c5":"df1['date'] = pd.to_datetime(df1['date'])","cf306f87":"df1.info()","49d91820":"df1.isnull().sum()","c6884621":"print(df1['competition_distance'].max())\nprint(df1['competition_open_since_year'].max())","dd347aaf":"#competition_distance\ndf1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000 if math.isnan(x) else x)\n\n#competition_open_since_month    \ndf1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n\n#competition_open_since_year     \ndf1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n\n#promo2_since_week         \ndf1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n\n#promo2_since_year\ndf1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n\n#promo_interval                  \n\nmonth_map = {1: 'Jan', 2: 'Fev', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7: 'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n\ndf1['promo_interval'].fillna(0, inplace=True)\ndf1['month_map'] = df1['date'].dt.month.map(month_map)\ndf1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)","309cae33":"df1.isnull().sum()","9fcd579b":"df1.info()","b81cabee":"df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\ndf1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\ndf1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\ndf1['promo2_since_year'] = df1['promo2_since_year'].astype(int)","4e831131":"df1.info()","926afff3":"num_attributes = df1.select_dtypes(include=['int64', 'float64'])\ncat_attributes = df1.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]'])","fe76017a":"#Central tendency - mean, median\nct1 = pd.DataFrame(num_attributes.apply(np.mean)).transpose()\nct2 = pd.DataFrame(num_attributes.apply(np.median)).transpose()\n\n#Dispersion - std, min, max, range, skew, kurtosis\nd1 = pd.DataFrame(num_attributes.apply(np.std)).transpose()\nd2 = pd.DataFrame(num_attributes.apply(np.min)).transpose()\nd3 = pd.DataFrame(num_attributes.apply(np.max)).transpose()\nd4 = pd.DataFrame(num_attributes.apply(lambda x: x.max() - x.min())).transpose()\nd5 = pd.DataFrame(num_attributes.apply(lambda x: x.skew())).transpose()\nd6 = pd.DataFrame(num_attributes.apply(lambda x: x.kurtosis())).transpose()\n\n#concatenate\nm = pd.concat([ct1, ct2, d1, d2, d3, d4, d5, d6]).transpose().reset_index()\nm.columns = ['index', 'mean', 'median', 'std', 'min', 'max', 'range', 'skew', 'kurtosis']","659eae90":"m","b121812a":"sns.distplot(x=df1['sales'])","04364a0b":"cat_attributes.apply(lambda x: x.unique().shape[0])","526f8025":"aux1 = df1[(df1['state_holiday'] != '0') & (df1['sales'] > 0)]\n\nplt.subplot(1, 3, 1)\nsns.boxplot(x='state_holiday', y='sales', data=aux1)\n\nplt.subplot(1, 3, 2)\nsns.boxplot(x='store_type', y='sales', data=aux1)\n\nplt.subplot(1, 3, 3)\nsns.boxplot(x='assortment', y='sales', data=aux1)","053ffa16":"df2 = df1.copy()","a18b9a76":"#Image(\"\/home\/mvrcosp\/repos\/DSP\/Rossmann\/img\/EngMindMapHypothesis.png\")","b99c8ef5":"df2['date'] = pd.to_datetime(df2['date'])\n\n# year\ndf2['year'] = df2['date'].dt.year\n\n# month\ndf2['month'] = df2['date'].dt.month\n\n# day\ndf2['day'] = df2['date'].dt.day\n\n# Week of year\ndf2['week_of_year'] = df2['date'].dt.weekofyear\n\n#year week\ndf2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n\n#competition since\ndf2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\ndf2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )\/30 ).apply( lambda x: x.days ).astype( int )\n\n# promo since\ndf2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\ndf2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\ndf2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )\/7 ).apply( lambda x: x.days ).astype( int )\n\n# assortment\ndf2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n\n# state holiday\ndf2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )","09e4d2c6":"df3 = df2.copy()","f7aea92d":"df3 = df3[(df3[\"open\"] != 0) & (df3['sales'] > 0)]","13cd367e":"cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\ndf3.drop(cols_drop, inplace=True, axis=1)","e3539a68":"df3.head()","ead9f9af":"df3.dtypes","56cf0696":"df4 = df3.copy()","a4fa2a0d":"sns.distplot(df4['sales'], kde=False)","605b37f7":"num_attributes.hist(bins=25)\nplt.show()","d2647f28":"# state_holiday\nplt.subplot( 3, 2, 1 )\na = df4[df4['state_holiday'] != 'regular_day']\nsns.countplot( x = 'state_holiday', data = a )\n\nplt.subplot( 3, 2, 2 )\nsns.kdeplot( df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', shade=True )\nsns.kdeplot( df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday', shade=True )\nsns.kdeplot( df4[df4['state_holiday'] == 'christmas']['sales'], label='christmas', shade=True )\n\n# store_type\nplt.subplot( 3, 2, 3 )\nsns.countplot(x = 'store_type', data = df4)\n\nplt.subplot( 3, 2, 4 )\nsns.kdeplot( df4[df4['store_type'] == 'a']['sales'], label='a', shade=True )\nsns.kdeplot( df4[df4['store_type'] == 'b']['sales'], label='b', shade=True )\nsns.kdeplot( df4[df4['store_type'] == 'c']['sales'], label='c', shade=True )\nsns.kdeplot( df4[df4['store_type'] == 'd']['sales'], label='d', shade=True )\n\n# assortment\nplt.subplot( 3, 2, 5 )\nsns.countplot( x = 'assortment', data = df4)\n\nplt.subplot( 3, 2, 6 )\nsns.kdeplot( df4[df4['assortment'] == 'extended']['sales'], label='extended', shade=True )\nsns.kdeplot( df4[df4['assortment'] == 'basic']['sales'], label='basic', shade=True )\nsns.kdeplot( df4[df4['assortment'] == 'extra']['sales'], label='extra', shade=True )\n\nplt.show()","9bffedd9":"aux421_1 = df4[['assortment', 'sales']].groupby('assortment').mean().reset_index()\nsns.barplot(x = 'assortment', y ='sales', data = aux421_1)","d187aac5":"aux421_2 = df4[['year_week', 'assortment', 'sales']].groupby(['year_week', 'assortment']).mean().reset_index()\naux421_2.pivot(index='year_week', columns='assortment', values='sales').plot()","cdb62ffd":"aux421_2[aux421_2['assortment'] =='extra'].plot()","bba4f8a0":"aux422_1 = df4[['competition_distance', 'sales']].groupby('competition_distance').sum().reset_index()\n\nplt.subplot(1, 3, 1)\nsns.scatterplot(x='competition_distance', y='sales', data=aux422_1)\n\n\nplt.subplot(1, 3, 2)\nbins = list(np.arange(0, 20000, 1000))\naux422_1['competition_distance_binned'] = pd.cut(aux422_1['competition_distance'], bins = bins)\naux422_2 = aux422_1[['competition_distance_binned', 'sales']].groupby('competition_distance_binned').sum().reset_index()\nsns.barplot(x='competition_distance_binned', y='sales', data=aux422_2)\nplt.xticks(rotation=45)\n\nplt.subplot(1, 3, 3)\nheat = sns.heatmap(aux422_1.corr(method='pearson'), annot=True)\n\nplt.show()","8712e168":"aux423_1 = df4[['competition_time_month', 'sales']].groupby( 'competition_time_month' ).sum().reset_index()\naux423_2 = aux423_1[( aux423_1['competition_time_month'] < 120 ) & ( aux423_1['competition_time_month'] != 0 )]\nsns.barplot( x='competition_time_month', y='sales', data=aux423_2 );\nplt.xticks( rotation=90 );","5ac5420f":"plt.subplot( 1, 2, 1 )\nsns.scatterplot( x='competition_time_month', y='sales', data=aux423_2 );\n\nplt.subplot( 1, 2, 2 )\nx = sns.heatmap( aux423_1.corr( method='pearson'), annot=True );","452a0f95":"aux424_1 = df4[['promo_time_week', 'sales']].groupby( 'promo_time_week').sum().reset_index()\n\ngrid = GridSpec( 2, 3 )\n\nplt.subplot( grid[0,0] )\naux424_2 = aux424_1[aux424_1['promo_time_week'] > 0] # promo extendido\nsns.barplot( x='promo_time_week', y='sales', data=aux424_2 );\nplt.xticks( rotation=90 );\n\nplt.subplot( grid[0,1] )\nsns.regplot( x='promo_time_week', y='sales', data=aux424_2 );\n\nplt.subplot( grid[1,0] )\naux424_3 = aux424_1[aux424_1['promo_time_week'] < 0] # promo regular\nsns.barplot( x='promo_time_week', y='sales', data=aux424_3 );\nplt.xticks( rotation=90 );\n\nplt.subplot( grid[1,1] )\nsns.regplot( x='promo_time_week', y='sales', data=aux424_3 );\n\nplt.subplot( grid[:,2] )\nsns.heatmap( aux424_1.corr( method='pearson' ), annot=True );","8aa71085":"df4[['promo', 'promo2', 'sales']].groupby( ['promo', 'promo2'] ).sum().sort_values(by='sales').reset_index()","a3fffef8":" aux425_1 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 1 )][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\nax = aux425_1.plot()\n\naux425_2 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 0 )][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\naux425_2.plot( ax=ax )\n\nax.legend( labels=['Promo 1 & Promo 2', 'Promo 1']);","7380116d":"aux426_1 = df4[df4['state_holiday'] != 'regular_day']\n\nplt.subplot( 1, 2, 1 )\naux426_2 = aux426_1[['state_holiday', 'sales']].groupby( 'state_holiday' ).sum().reset_index()\nsns.barplot( x='state_holiday', y='sales', data=aux426_2 );\n\nplt.subplot( 1, 2, 2 )\naux426_3 = aux426_1[['year', 'state_holiday', 'sales']].groupby( ['year', 'state_holiday'] ).sum().reset_index()\nsns.barplot( x='year', y='sales', hue='state_holiday', data=aux426_3 ); ","29838131":" aux427_1 = df4[['year', 'sales']].groupby( 'year' ).sum().reset_index()\n\nplt.subplot( 1, 3, 1 )\nsns.barplot( x='year', y='sales', data=aux427_1 );\n\nplt.subplot( 1, 3, 2 )\nsns.regplot( x='year', y='sales', data=aux427_1 );\n\nplt.subplot( 1, 3, 3 )\nsns.heatmap( aux427_1.corr( method='pearson' ), annot=True );","66f661df":"aux428_1 = df4[['month', 'sales']].groupby( 'month' ).sum().reset_index()\n\nplt.subplot( 1, 3, 1 )\nsns.barplot( x='month', y='sales', data=aux428_1 );\n\nplt.subplot( 1, 3, 2 )\nsns.regplot( x='month', y='sales', data=aux428_1 );\n\nplt.subplot( 1, 3, 3 )\nsns.heatmap( aux428_1.corr( method='pearson' ), annot=True );","ecd1f63a":"plt.subplot(1, 2, 1)\naux429_1 = df4[['year', 'month', 'day', 'sales']].groupby(['year', 'month', 'day']).sum().reset_index()\nsns.barplot(x='day', y='sales', data=aux429_1)\n\nplt.subplot(1, 2, 2)\nbins = list(np.arange(0, 40, 10))\naux429_1['days_binned'] = pd.cut(aux429_1['day'], bins = bins)\nsns.barplot(x='days_binned', y='sales', data=aux429_1)","836c51a7":"aux4210_1 = df4[['day_of_week', 'sales']].groupby( 'day_of_week' ).sum().reset_index()\n\nplt.subplot( 1, 3, 1 )\nsns.barplot( x='day_of_week', y='sales', data=aux4210_1 );\n\nplt.subplot( 1, 3, 2 )\nsns.regplot( x='day_of_week', y='sales', data=aux4210_1 );\n\nplt.subplot( 1, 3, 3 )\nsns.heatmap( aux4210_1.corr( method='pearson' ), annot=True );","342dc718":"aux4211_1 = df4[['school_holiday', 'sales']].groupby( 'school_holiday' ).sum().reset_index()\nplt.subplot( 2, 1, 1 )\nsns.barplot( x='school_holiday', y='sales', data=aux4211_1 );\n\naux4211_2 = df4[['month', 'school_holiday', 'sales']].groupby( ['month','school_holiday'] ).sum().reset_index()\nplt.subplot( 2, 1, 2 )\nsns.barplot( x='month', y='sales', hue='school_holiday', data=aux4211_2 );","b6c9ec75":"from tabulate import tabulate","908234cf":"tab =[['Hypothesis', 'Conclusion', 'Relevance'],\n      ['H1', 'False', 'Low'],  \n      ['H2', 'False', 'Medium'],  \n      ['H3', 'False', 'Medium'],\n      ['H4', 'False', 'Low'],\n      ['H5', 'False', 'Low'],\n      ['H6', 'False', 'Medium'],\n      ['H7', 'False', 'High'],\n      ['H8', 'True', 'High'],\n      ['H9', 'True', 'High'],\n      ['H10', 'True', 'High'],\n      ['H11', 'True', 'Low']]  \n\nprint(tabulate(tab, headers='firstrow'))","80eaf7f6":"correlation = num_attributes.corr(method='pearson')\nsns.heatmap(correlation, annot=True)","a319bd2f":"b = df4.select_dtypes('object')\nb.drop('year_week', axis=1, inplace=True)","f3954761":"#Calculte Cramer V\n\nb1 = cramer_v( b['state_holiday'], b['state_holiday'] )\nb2 = cramer_v( b['state_holiday'], b['store_type'] )\nb3 = cramer_v( b['state_holiday'], b['assortment'] )\n\nb4 = cramer_v( b['store_type'], b['state_holiday'] )\nb5 = cramer_v( b['store_type'], b['store_type'] )\nb6 = cramer_v( b['store_type'], b['assortment'] )\n\nb7 = cramer_v( b['assortment'], b['state_holiday'] )\nb8 = cramer_v( b['assortment'], b['store_type'] )\nb9 = cramer_v( b['assortment'], b['assortment'] )\n\n# Final dataset\nd = pd.DataFrame( {'state_holiday': [b1, b2, b3], \n               'store_type': [b4, b5, b6],\n               'assortment': [b7, b8, b9]  })\nd = d.set_index( d.columns )\n\nsns.heatmap( d, annot=True )","6f7650db":"df5 = df4.copy()","a014c7af":"a = df5.select_dtypes(include=['int64', 'float64'])\na.head()","113ccc97":" rs = RobustScaler()\nmms = MinMaxScaler()\n\n# competition distance\ndf5['competition_distance'] = rs.fit_transform( df5[['competition_distance']].values )\n#pickle.dump( rs, open( '\/home\/mvrcosp\/repos\/DSP\/Rossmann\/Pickles\/competition_distance_scaler.pkl', 'wb') )\n\n# competition time month\ndf5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n#pickle.dump( rs, open( '\/home\/mvrcosp\/repos\/DSP\/Rossmann\/Pickles\/competition_time_month_scaler.pkl', 'wb') )\n\n# promo time week\ndf5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n#pickle.dump( rs, open( '\/home\/mvrcosp\/repos\/DSP\/Rossmann\/Pickles\/promo_time_week_scaler.pkl', 'wb') )\n\n# year\ndf5['year'] = mms.fit_transform( df5[['year']].values )\n#pickle.dump( mms, open( '\/home\/mvrcosp\/repos\/DSP\/Rossmann\/Pickles\/year_scaler.pkl', 'wb') )","6676cba5":"df5.head()","d9096c84":" # state_holiday - One Hot Encoding\ndf5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n\n# store_type - Label Encoding\nle = LabelEncoder()\ndf5['store_type'] = le.fit_transform( df5['store_type'] )\n#pickle.dump( le, open( '\/home\/mvrcosp\/repos\/DSP\/Rossmann\/Pickles\/store_type_scaler.pkl', 'wb') )\n\n# assortment - Ordinal Encoding\nassortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\ndf5['assortment'] = df5['assortment'].map( assortment_dict )","0d4e1bea":"df5['sales'] = np.log1p( df5['sales'] )","719d51ba":"sns.distplot(df5.sales)","ddbf1bf3":"# day of week\ndf5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi\/7 ) ) )\ndf5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi\/7 ) ) )\n\n# month\ndf5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi\/12 ) ) )\ndf5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi\/12 ) ) )\n\n# day \ndf5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi\/30 ) ) )\ndf5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi\/30 ) ) )\n\n# week of year\ndf5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi\/52 ) ) )\ndf5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi\/52 ) ) )","3747654a":"df6 = df5.copy()","b937bdf9":"cols_to_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\ndf6.drop(cols_to_drop, axis=1, inplace=True)","a8e51d7a":"# training dataset\nX_train = df6[df6['date'] < '2015-06-19']\ny_train = X_train['sales']\n\n# test dataset\nX_test = df6[df6['date'] >= '2015-06-19']\ny_test = X_test['sales']\n\nprint( 'Training Min Date: {}'.format( X_train['date'].min() ) )\nprint( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n\nprint( '\\nTest Min Date: {}'.format( X_test['date'].min() ) )\nprint( 'Test Max Date: {}'.format( X_test['date'].max() ) )","242bdbec":"# training and test dataset for Boruta\n#X_train_n = X_train.drop( ['date', 'sales'], axis=1 ).values\n#y_train_n = y_train.values.ravel()\n\n# define RandomForestRegressor\n#rf = RandomForestRegressor( n_jobs=-1 )\n\n# define Boruta\n#boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n )","84acde73":"#cols_selected = boruta.support_.tolist()\n\n# best features\n#X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n#cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n\n# not selected boruta\n#cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )","0c80fc10":"df6.head()","777a049d":"cols_selected_boruta = [\n    'store',\n    'promo',\n    'store_type',\n    'assortment',\n    'competition_distance',\n    'competition_open_since_month',\n    'competition_open_since_year',\n    'promo2',\n    'promo2_since_week',\n    'promo2_since_year',\n    'competition_time_month',\n    'promo_time_week',\n    'day_of_week_sin',\n    'day_of_week_cos',\n    'month_sin',\n    'month_cos',\n    'day_sin',\n    'day_cos',\n    'week_of_year_sin',\n    'week_of_year_cos']\n\n# columns to add\nfeat_to_add = ['date', 'sales']\n\ncols_selected_boruta_full = cols_selected_boruta.copy()\ncols_selected_boruta_full.extend( feat_to_add )","0afa5236":"x_train = X_train[cols_selected_boruta]\nx_test = X_test[cols_selected_boruta]\n\n# Time Series Data Preparation\nx_training = X_train[cols_selected_boruta_full]","92b93a3f":"aux1 = x_test.copy()\naux1['sales'] = y_test.copy()\n\n# prediction\naux2 = aux1[['store', 'sales']].groupby('store').mean().reset_index().rename( columns={'sales': 'predictions'} )\naux1 = pd.merge(aux1, aux2, how='left', on='store')\nyhat_baseline = aux1['predictions']\n\n# performance\nbaseline_result = ml_error('Average Model', np.expm1( y_test ), np.expm1( yhat_baseline ))\nbaseline_result","0a0ffe1e":" # model\nlr = LinearRegression().fit(x_train, y_train)\n\n# prediction\nyhat_lr = lr.predict(x_test)\n\n# performance\nlr_result = ml_error('Linear Regression', np.expm1( y_test ), np.expm1( yhat_lr ))\nlr_result","067fb234":"lr_result_cv = ts_cross_validation (x_training, 5, \"Linear Regression\", lr, verbose= False)\nlr_result_cv","eadcdb60":" # model\nlrr = Lasso(alpha=0.01).fit(x_train, y_train)\n\n# prediction\nyhat_lrr = lrr.predict( x_test )\n\n# performance\nlrr_result = ml_error( 'Linear Regression - Lasso', np.expm1( y_test ), np.expm1( yhat_lrr ) )\nlrr_result","30f4d752":"lrr_result_cv = ts_cross_validation(x_training, 5, \"Lasso\", lrr, verbose=False)\nlrr_result_cv","e80414df":" # model\nrf = RandomForestRegressor(n_estimators=100, random_state=42).fit( x_train, y_train)\n\n# prediction\nyhat_rf = rf.predict(x_test)\n\n# performance\nrf_result = ml_error('Random Forest Regressor', np.expm1(y_test), np.expm1(yhat_rf))\nrf_result","4475f05d":"rf_result_cv = ts_cross_validation(x_training, 5, \"Random Forest Regressor\", rf, verbose=True)\nrf_result_cv","cea8964a":" # model\nmodel_xgb = xgb.XGBRegressor(objective='reg:squarederror',\n                              n_estimators=100, \n                              eta=0.01, \n                              max_depth=10, \n                              subsample=0.7,\n                              colsample_bytee=0.9).fit(x_train, y_train)\n\n# prediction\nyhat_xgb = model_xgb.predict(x_test)\n\n# performance\nxgb_result = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb ))\nxgb_result","b30b4265":"xgb_result_cv = ts_cross_validation(x_training, 5, 'XGBoost Regressor', model_xgb, verbose=True)\nxgb_result_cv","ca57e5f4":"modelling_result = pd.concat([baseline_result, lr_result, lrr_result, xgb_result]) #rf_result\nmodelling_result.sort_values('RMSE')","3e5b138a":"modelling_result_cv = pd.concat([lr_result_cv, lrr_result_cv, xgb_result_cv]) #rf_result_cv\nmodelling_result_cv","31f2b26e":"#with open(\"model_result_cv.pkl\", \"wb\") as f:\n#    pickle.dump(modelling_result_cv, f)","a23648fc":"# Random Search\n# Grid Search\n\nparam = {'n_estimators': [int(x) for x in np.linspace(start = 1500, stop = 3500, num = 5)],\n        'eta': [0.01, 0.03],\n        'max_depth': [3, 5, 9],,\n         'subsample': [0.1, 0.5, 0.7],\n         'colsample_bytee': [0.3, 0.7, 0.9],\n         'min_child_weight': [3, 8, 15],\n        }\n\nMAX_EVAL = 5","dffe48b5":"#final result\nfinal_result = pd.DataFrame()\n\nfor i in range ( MAX_EVAL ):\n    hp = {k: random.sample (v, 1)[0] for k, v in param.items()}\n    print(hp)\n    \n    model_xgb = xgb.XGBRegressor(objective='reg:squarederror',\n                                n_estimators = hp['n_estimators'],\n                                 eta = hp['eta'], \n                                 max_depth = hp['max_depth'],\n                                 subsample = hp['subsample'],\n                                 colsample_bytee = hp['colsample_bytee'],\n                                 min_child_weight = hp['min_child_weight'])\n    \n    #performance\n    \n    result = cross_validation(x_training, 5, 'XGBoost Regressor', model_xgb, verbose=True)\n    final_result = pd.concat([final_result, result])","6f2c8741":"final_result","4d091250":"param_tuned = {\n    'n_estimators': 3000,\n    'eta': 0.03,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytee': 0.7,\n    'min_child_weight': 3\n}","2aa187ac":"model_xgb_tuned = xgb.XGBRegressor(objective='reg:squarederror',\n                                n_estimators = param_tuned['n_estimators'],\n                                 eta = param_tuned['eta'], \n                                 max_depth = param_tuned['max_depth'],\n                                 subsample = param_tuned['subsample'],\n                                 colsample_bytee = param_tuned['colsample_bytee'],\n                                 min_child_weight = param_tuned['min_child_weight']).fit(x_train, y_train)\n\n#prediction\nyhat_xgb_tuned = model_xgb_tuned.predict(x_test)\n\n\n#performance\nxgb_result_tuned = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb_tuned))\nxgb_result_tuned\n\n\n\n#with open(\"model_rossmann.pkl\", \"wb\") as f:\n#    pickle.dump(model_xgb_tuned, f)","90b8f660":"#with open(\"yhat_xgb_tuned.pkl\", \"wb\") as f:\n#    pickle.dump(yhat_xgb_tuned, f)","8c950637":"model_xgb_tuned.get_xgb_params()","cafbb282":"mpe = mean_percentage_error(np.expm1(y_test), np.expm1(yhat_xgb_tuned))\nmpe","0f15ad68":"df9 = X_test[cols_selected_boruta_full]\n\n#rescale\ndf9[\"sales\"] = np.expm1(df9[\"sales\"])\ndf9[\"predictions\"] = np.expm1(yhat_xgb_tuned)","5fa70685":"df9.head()","3a42e3b2":"# sum of predictions\ndf91 = df9[[\"store\", \"predictions\"]].groupby(\"store\").sum().reset_index()\n\n#MAE and MAPE\ndf9_aux1 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns={0:'MAE'})\ndf9_aux2 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_percentage_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns={0:'MAPE'})\n\n# Merge\ndf9_aux3 = pd.merge( df9_aux1, df9_aux2, how='inner', on='store' )\ndf92 = pd.merge( df91, df9_aux3, how='inner', on='store' )\n\n# Scenarios\ndf92['worst_scenario'] = df92['predictions'] - df92['MAE']\ndf92['best_scenario'] = df92['predictions'] + df92['MAE']\n\n# order columns\ndf92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]","95c8cbd4":"df92.sort_values(\"MAPE\", ascending=False).head()","933b87c1":"sns.scatterplot(x = \"store\", y = \"MAPE\", data = df92)","8177016d":"df93 = df92[[\"predictions\", \"worst_scenario\", \"best_scenario\"]].apply(lambda x: np.sum(x), axis = 0).reset_index().rename(columns = {\"index\" : \"Scenario\", 0: \"Values\"})\ndf93[\"Values\"] = df93[\"Values\"].map(\"R${:,.2f}\".format)\ndf93","08d63257":"df9['error'] = df9['sales'] - df9['predictions']\ndf9['error_rate'] = df9['predictions'] \/ df9['sales']","03c06e90":"plt.subplot( 2, 2, 1 )\nsns.lineplot( x='date', y='sales', data=df9, label='SALES' )\nsns.lineplot( x='date', y='predictions', data=df9, label='PREDICTIONS' )\n\nplt.subplot( 2, 2, 2 )\nsns.lineplot( x='date', y='error_rate', data=df9 )\nplt.axhline( 1, linestyle='--')\n\nplt.subplot( 2, 2, 3 )\nsns.distplot( df9['error'] )\n\nplt.subplot( 2, 2, 4 )\nsns.scatterplot( df9['predictions'], df9['error'] )","bf669bbb":"## 0.2 Loading Data","a675f971":"### 4.2.5 - H5 - Stores with consecutive promos should sell more.\n\n**False:** Stores with consecutive promos actually sell less than stores with only promo 1.","d5db3de6":"### 4.2.9 - H9 - Stores should sell more at the beginning of each month.\n\n**True:** Stores do sell a little bit more at the beginning of the month.","ce3f60e7":"### 7.3.1 Lasso - Cross Validation","82ce059d":"### 4.2.11 - H11 - Stores should sell less on school holidays.\n\n**True:** Stores do sell less on school holidays. August is the only month where school holidays actually sell more.","7592a3a1":"## 0.3 Data Dictionary","ff28d441":"### 4.2.1 - H1 - Stores with bigger assortment should sell more.\n\n**False:** Stores with extended assortment do sell more than stores with basic assortment, but, stores that sell the most are the ones with only \"extra\" assortment (mot extended).","12cbbd99":"### 4.2.7 - H7 - Stores should sell more over the years.\n\n**False:** Stores are selling less over the years.","dc85e693":"# 0.0 Imports","4dbc46f3":"**H1.** Stores with bigger assortment should sell more.\n\n**H2.** Stores with local competitors should sell less.\n\n**H3.** Stores with longer-term competitors should sell more.\n\n**H4.** Stores that keep their promos active for longer periods should sell more.\n\n**H5.** Stores with consecutive promos should sell more.\n\n**H6.** Stores that open during christmas season should sell more.\n\n**H7.** Stores should sell more over the years.\n\n**H8.** Stores should sell more at the 2nd semester of the year.\n\n**H9.** Stores should sell more at the beginning of each month.\n\n**H10.** Stores should sell less on weekends.\n\n**H11.** Stores should sell less on school holidays.","c75efbdb":"# 1.0 Data Description","e5a69b8e":"### 4.3.2 Categorial Attributes","07d5707e":"### 4.2.10 - H10 - Stores should sell less on weekends.\n\n**True:** Stores DO sell less on weekends compared to weekdays.","dfdacc2e":"## 7.4  Random Forest Regressor","248b8a07":"### 4.1.2 Numerical Variables","e855fd6f":"## 1.4 Check and fill NA","73ead5e4":"## 4.2 Bivariate Analysis - Validating Business Hypothesis","638aad36":"## 4.3 Multivariate Analysis","a241e38c":"### 4.1.3 Categorical Variables","af8b0541":"## 7.3 Linear Regression Regularized Model - Lasso","d0207ea0":"### 4.1.1 Target Variable","64f51318":"# 9.0  Interpreting Results","21f67991":"# 8.0 Hyperparameter fine tuning","0d76f3e9":"## 6.2 Boruta Algorithm to Select Features","89a19d2e":"## 0.1 Helper Functions","8f235da1":"## 5.1 Rescaling (Numerical Attributes)","5dd19ab5":"### 4.2.3 - H3 - Stores with longer-term competitors should sell more.\n**False:** In reality, stores with longer-term competitiors sell less.","2d5f6da8":"# 8.2 Final Model","69bd9a11":"## 2.1 Brainstorming Business Hypothesis to validate with data!","9ca79b28":"## 4.1 Univariate analysis","ea83828b":"## 6.3 Manual Feature Selection","224f5323":"### 4.3.1 Numerical Attributes","4e1391a9":"## 8.1 Random Search","37c7ee0d":"**1.** Stores with a bigger number of employees should sell more.\n\n**2.** Stores with a bigger stock size should sell more.\n\n**3.** Stores with a bigger size should sell more.\n\n**4.** Stores with local competitors should sell less.\n\n**5.** Stores with longer-term competitors should sell more.\n\n**6.** Stores with bigger assortment should sell more.","b1a3dba8":"**1.** Stores that open during christmas season should sell more.\n\n**2.** Stores should sell more over the years.\n\n**3.** Stores should sell more at the 2nd semester of the year.\n\n**4.** Stores should sell more at the beginning of each month.\n\n**5.** Stores should sell more on weekends.\n\n**6.** Stores should sell less on school holidays.","939b4590":"# 5.0 Data Preparation","955e6089":"# 7.0 Model Data","a9d6a223":"### 4.2.12 - Final Hypothesis table","a0cc3352":"### 2.1.3 Sazonality Hypothesis","c4f81164":"# 4.0 Exploratory Data Analysis","aaebdd08":"## 2.2 Final Business Hypothesis List","58a50626":"### 4.2.6 - H6 - Stores that open during christmas season should sell more.\n\n**False:** Public holidays sell better than christmas. Sorry santa :(","14039a2c":"### 7.5.1 XGBoost Regressor - Cross Validation","4fa4757e":"## 6.1 Split DataFrame into Traning and Test Dataset","df904f76":"### 4.2.8 - H8 - Stores should sell more at the 2nd semester of the year.\n\n**False:** Stores sell more at first semester of the year.","03c9a379":"## 5.2 Encoding (Categorical Attributes)","74249e89":"### 2.1.2 Product Hypothesis","4ccf6463":"## 1.3 Dates to datetime","2805569e":"### 1.6.1 Numerical Attributes","8cff13df":"### 6.2.1 Best Features from Boruta","78d9489b":"## 1.2 Data Exploration","a435651a":"### 4.2.2 - H2 - Stores with local competitors should sell less.\n**False:** Stores with competitors close by, in reality, sell more.","d99e96eb":"### 7.6.1 Single Performance","2c6ec323":"# 3.0 Data Filtering","593d8760":"### 7.2.1 Linear Regression Model - Cross Validation","99b94cd7":"## 5.4 Dealing With the Cyclic Nature of Time","f34788c4":"## 9.1 Business Performance","98746406":"## 7.1 Average Model - Baseline","78791d12":"## 7.2 Linear Regression Model","1011d4f2":"### 7.4.1 Random Forest Regressor - Cross Validation","d1bc844c":"# 6.0 Feature Selection","ab7dfa5b":"### 1.6.2 Categorical attributes","c2ef0693":"## 1.5 Change dtypes","d37c123c":"## 9.2 Total Performance","3a9611a6":"### 4.2.4 - H4 - Stores that keep their promos active for longer periods should sell more.\n\n**False:** Actually sales start to drop after a few weeks in promotion.","af78b535":"## 7.6 Comparing Models Performance","217fbe76":"* **Id** - an Id that represents a (Store, Date) duple within the test set\n\n* **Store** - a unique Id for each store\n\n* **Sales** - the turnover for any given day (this is what you are predicting)\n\n* **Customers** - the number of customers on a given day\n\n* **Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n\n* **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b  Easter holiday, c = Christmas, 0 = None\n\n* **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n\n* **StoreType** - differentiates between 4 different store models: a, b, c, d\n\n* **Assortment** - describes an assortment level: a = basic, b = extra, c = extended\n\n* **CompetitionDistance** - distance in meters to the nearest competitor store\n\n* **CompetitionOpenSince[Month\/Year]** - gives the approximate year and month of the time the nearest competitor was opened\n\n* **Promo** - indicates whether a store is running a promo on that day\n\n* **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n\n* **Promo2Since[Year\/Week]** - describes the year and calendar week when the store started participating in Promo2\n\n* **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store","53788749":"## 5.3 Target Variable Transformation (Logarithm Transformation)","6bb2d6f4":"## 1.6 Descriptive Statistics","68a37f7c":"# 2.0 Feature Engineering","370aac42":"**1.** Stores that invest in marketing strategies should sell more.\n\n**2.** Stores that showcase their product better should sell more.\n\n**3.** Stores with cheaper products should sell more.\n\n**4.** Stores that perform more agressive promos should sell more.\n\n**5.** Stores that keep their promos active for longer periods should sell more.\n\n**6** Stores with consecutive promos should sell more.","bdd031de":"## 1.1 Rename columns","e5f09875":"## 2.3 Feature Engineering","c4821141":"### 2.1.1 Store Hypothesis","fdfc62d6":"### 7.6.2 Real Performance - Cross Validation","599a00b9":"## 7.5  XGBoost Regressor","d7fed659":"## 9.3 Machine Learning Performance"}}