{"cell_type":{"8a4d6819":"code","d8d8e9eb":"code","efc8908a":"code","dd81051f":"code","818df7d0":"code","d6194530":"code","0a366a1b":"code","c6ce0e7e":"code","82fd89fc":"code","8a33b993":"code","4f73c553":"code","1cb1dd3d":"code","da8aba81":"code","82e70113":"code","f62b8fe1":"code","3478e3c6":"code","ec00d0c6":"code","fb4d1260":"code","ef76014d":"code","ac46ea7d":"code","3e63eddd":"markdown","b6f29a06":"markdown","6eed800a":"markdown","1a85d8c9":"markdown","19bf07ee":"markdown","d1e8b80a":"markdown","bf7da334":"markdown","586e3fb8":"markdown","e1696fd2":"markdown"},"source":{"8a4d6819":"%%bash\n# dependencies\napt-get -y update > \/dev\/null\napt-get -y install libsdl2-gfx-dev libsdl2-ttf-dev > \/dev\/null\n\n# cloudpickle, pytorch, gym\npip3 install \"cloudpickle==1.3.0\"\npip3 install \"torch==1.5.1\"\npip3 install \"gym==0.17.2\"\n\n# gfootball\nGRF_VER=v2.8\nGRF_PATH=football\/third_party\/gfootball_engine\/lib\nGRF_URL=https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_${GRF_VER}.so\ngit clone -b ${GRF_VER} https:\/\/github.com\/google-research\/football.git\nmkdir -p ${GRF_PATH}\nwget -q ${GRF_URL} -O ${GRF_PATH}\/prebuilt_gameplayfootball.so\ncd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . && cd ..\n\n# kaggle-environments\ngit clone https:\/\/github.com\/Kaggle\/kaggle-environments.git\ncd kaggle-environments && pip3 install . && cd ..\n\n# stable-baselines3\ngit clone https:\/\/github.com\/DLR-RM\/stable-baselines3.git\ncd stable-baselines3 && pip3 install . && cd ..\n\n# housekeeping\nrm -rf football kaggle-environments stable-baselines3","d8d8e9eb":"!cp \"..\/input\/gfootball-academy\/visualizer.py\" .","efc8908a":"import os\nimport base64\nimport pickle\nimport zlib\nimport gym\nimport numpy as np\nimport pandas as pd\nimport torch as th\nfrom torch import nn, tensor\nfrom collections import deque\nfrom gym.spaces import Box, Discrete\nfrom kaggle_environments import make\nfrom kaggle_environments.envs.football.helpers import *\nfrom gfootball.env import create_environment, observation_preprocessing\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.ppo import CnnPolicy\nfrom stable_baselines3.common import results_plotter\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.env_checker import check_env\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom IPython.display import HTML\nfrom visualizer import visualize\nfrom matplotlib import pyplot as plt\n%matplotlib inline","dd81051f":"class FootballGym(gym.Env):\n    spec = None\n    metadata = None\n    \n    def __init__(self, config=None):\n        super(FootballGym, self).__init__()\n        env_name = \"academy_empty_goal_close\"\n        rewards = \"scoring,checkpoints\"\n        if config is not None:\n            env_name = config.get(\"env_name\", env_name)\n            rewards = config.get(\"rewards\", rewards)\n        self.env = create_environment(\n            env_name=env_name,\n            stacked=False,\n            representation=\"raw\",\n            rewards = rewards,\n            write_goal_dumps=False,\n            write_full_episode_dumps=False,\n            render=False,\n            write_video=False,\n            dump_frequency=1,\n            logdir=\".\",\n            extra_players=None,\n            number_of_left_players_agent_controls=1,\n            number_of_right_players_agent_controls=0)  \n        self.action_space = Discrete(19)\n        self.observation_space = Box(low=0, high=255, shape=(72, 96, 16), dtype=np.uint8)\n        self.reward_range = (-1, 1)\n        self.obs_stack = deque([], maxlen=4)\n        \n    def transform_obs(self, raw_obs):\n        obs = raw_obs[0]\n        obs = observation_preprocessing.generate_smm([obs])\n        if not self.obs_stack:\n            self.obs_stack.extend([obs] * 4)\n        else:\n            self.obs_stack.append(obs)\n        obs = np.concatenate(list(self.obs_stack), axis=-1)\n        obs = np.squeeze(obs)\n        return obs\n\n    def reset(self):\n        self.obs_stack.clear()\n        obs = self.env.reset()\n        obs = self.transform_obs(obs)\n        return obs\n    \n    def step(self, action):\n        obs, reward, done, info = self.env.step([action])\n        obs = self.transform_obs(obs)\n        return obs, float(reward), done, info\n    \ncheck_env(env=FootballGym(), warn=True)","818df7d0":"def conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.conv2 = conv3x3(out_channels, out_channels, stride)\n        \n    def forward(self, x):\n        residual = x\n        out = self.relu(x)\n        out = self.conv1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out += residual\n        return out\n    \nclass FootballCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space, features_dim=256):\n        super().__init__(observation_space, features_dim)\n        in_channels = observation_space.shape[0]  # channels x height x width\n        self.cnn = nn.Sequential(\n            conv3x3(in_channels=in_channels, out_channels=32),\n            nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False),\n            ResidualBlock(in_channels=32, out_channels=32),\n            ResidualBlock(in_channels=32, out_channels=32),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n        self.linear = nn.Sequential(\n          nn.Linear(in_features=52640, out_features=features_dim, bias=True),\n          nn.ReLU(),\n        )\n\n    def forward(self, obs):\n        return self.linear(self.cnn(obs))","d6194530":"scenarios = {0: \"academy_empty_goal_close\",\n             1: \"academy_empty_goal\",\n             2: \"academy_run_to_score\",\n             3: \"academy_run_to_score_with_keeper\",\n             4: \"academy_pass_and_shoot_with_keeper\",\n             5: \"academy_run_pass_and_shoot_with_keeper\",\n             6: \"academy_3_vs_1_with_keeper\",\n             7: \"academy_corner\",\n             8: \"academy_counterattack_easy\",\n             9: \"academy_counterattack_hard\",\n             10: \"academy_single_goal_versus_lazy\",\n             11: \"11_vs_11_kaggle\"}\nscenario_name = scenarios[7]","0a366a1b":"def make_env(config=None, rank=0):\n    def _init():\n        env = FootballGym(config)\n        log_file = os.path.join(\".\", str(rank))\n        env = Monitor(env, log_file, allow_early_resets=True)\n        return env\n    return _init","c6ce0e7e":"n_envs = 4\nconfig={\"env_name\":scenario_name}\ntrain_env = DummyVecEnv([make_env(config, rank=i) for i in range(n_envs)])\n# train_env = SubprocVecEnv([make_env(config, rank=i) for i in range(n_envs)])\n\nn_steps = 512\npolicy_kwargs = dict(features_extractor_class=FootballCNN,\n                     features_extractor_kwargs=dict(features_dim=256))\n# model = PPO(CnnPolicy, train_env, \n#             policy_kwargs=policy_kwargs, \n#             learning_rate=0.000343, \n#             n_steps=n_steps, \n#             batch_size=8, \n#             n_epochs=2, \n#             gamma=0.993,\n#             gae_lambda=0.95,\n#             clip_range=0.08, \n#             ent_coef=0.003, \n#             vf_coef=0.5, \n#             max_grad_norm=0.64, \n#             verbose=0)\nmodel = PPO.load(\"..\/input\/gfootball-stable-baselines3\/ppo_gfootball.zip\", train_env)","82fd89fc":"from tqdm.notebook import tqdm\nclass ProgressBar(BaseCallback):\n    def __init__(self, verbose=0):\n        super(ProgressBar, self).__init__(verbose)\n        self.pbar = None\n\n    def _on_training_start(self):\n        factor = np.ceil(self.locals['total_timesteps'] \/ self.model.n_steps)\n        n = 1\n        try:\n            n = len(self.training_env.envs)\n        except AttributeError:\n            try:\n                n = len(self.training_env.remotes)\n            except AttributeError:\n                n = 1\n        total = int(self.model.n_steps * factor \/ n)\n        self.pbar = tqdm(total=total)\n\n    def _on_rollout_start(self):\n        self.pbar.refresh()\n\n    def _on_step(self):\n        self.pbar.update(1)\n        return True\n\n    def _on_rollout_end(self):\n        self.pbar.refresh()\n\n    def _on_training_end(self):\n        self.pbar.close()\n        self.pbar = None\n\nprogressbar = ProgressBar()","8a33b993":"total_epochs = 200\ntotal_timesteps = n_steps * n_envs * total_epochs\nmodel.learn(total_timesteps=total_timesteps, callback=progressbar)\nmodel.save(\"ppo_gfootball\")","4f73c553":"plt.style.use(['seaborn-whitegrid'])\nresults_plotter.plot_results([\".\"], total_timesteps, results_plotter.X_TIMESTEPS, \"GFootball Timesteps\")\nresults_plotter.plot_results([\".\"], total_timesteps, results_plotter.X_EPISODES, \"GFootball Episodes\")","1cb1dd3d":"plt.style.use(['seaborn-whitegrid'])\nlog_files = [os.path.join(\".\", f\"{i}.monitor.csv\") for i in range(n_envs)]\n\nnrows = np.ceil(n_envs\/2)\nfig = plt.figure(figsize=(8, 2 * nrows))\nfor i, log_file in enumerate(log_files):\n    if os.path.isfile(log_file):\n        df = pd.read_csv(log_file, skiprows=1)\n        plt.subplot(nrows, 2, i+1, label=log_file)\n        df['r'].rolling(window=100).mean().plot(title=f\"Rewards: Env {i}\")\n        plt.tight_layout()\nplt.show()","da8aba81":"model = PPO.load(\"ppo_gfootball\")\ntest_env = FootballGym({\"env_name\":scenario_name})\nobs = test_env.reset()\ndone = False\nwhile not done:\n    action, state = model.predict(obs, deterministic=True)\n    obs, reward, done, info = test_env.step(action)\n    print(f\"{Action(action).name.ljust(16,' ')}\\t{round(reward,2)}\\t{info}\")","82e70113":"%%writefile submission.py\nimport base64\nimport pickle\nimport zlib\nimport numpy as np\nimport torch as th\nfrom torch import nn, tensor\nfrom collections import deque\nfrom gfootball.env import observation_preprocessing\n\nstate_dict = _STATE_DICT_\n\nstate_dict = pickle.loads(zlib.decompress(base64.b64decode(state_dict)))\n\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.conv2 = conv3x3(out_channels, out_channels, stride)\n        \n    def forward(self, x):\n        residual = x\n        out = self.relu(x)\n        out = self.conv1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out += residual\n        return out\n    \nclass PyTorchCnnPolicy(nn.Module):\n    global state_dict\n    def __init__(self):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            conv3x3(in_channels=16, out_channels=32),\n            nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False),\n            ResidualBlock(in_channels=32, out_channels=32),\n            ResidualBlock(in_channels=32, out_channels=32),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n        self.linear = nn.Sequential(\n          nn.Linear(in_features=52640, out_features=256, bias=True),\n          nn.ReLU(),\n        )\n        self.action_net = nn.Sequential(\n          nn.Linear(in_features=256, out_features=19, bias=True),\n          nn.ReLU(),\n        )\n        self.out_activ = nn.Softmax(dim=1)\n        self.load_state_dict(state_dict)\n\n    def forward(self, x):\n        x = tensor(x).float() \/ 255.0  # normalize\n        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width\n        x = self.cnn(x)\n        x = self.linear(x)\n        x = self.action_net(x)\n        x = self.out_activ(x)\n        return int(x.argmax())\n    \nobs_stack = deque([], maxlen=4)\ndef transform_obs(raw_obs):\n    global obs_stack\n    obs = raw_obs['players_raw'][0]\n    obs = observation_preprocessing.generate_smm([obs])\n    if not obs_stack:\n        obs_stack.extend([obs] * 4)\n    else:\n        obs_stack.append(obs)\n    obs = np.concatenate(list(obs_stack), axis=-1)\n    return obs\n\npolicy = PyTorchCnnPolicy()\npolicy = policy.float().to('cpu').eval()\ndef agent(raw_obs):\n    obs = transform_obs(raw_obs)\n    action = policy(obs)\n    return [action]","f62b8fe1":"model = PPO.load(\"ppo_gfootball\")\n_state_dict = model.policy.to('cpu').state_dict()\nstate_dict = {\n    \"cnn.0.weight\":_state_dict['features_extractor.cnn.0.weight'], \n    \"cnn.0.bias\":_state_dict['features_extractor.cnn.0.bias'], \n    \"cnn.2.conv1.weight\":_state_dict['features_extractor.cnn.2.conv1.weight'], \n    \"cnn.2.conv1.bias\":_state_dict['features_extractor.cnn.2.conv1.bias'],\n    \"cnn.2.conv2.weight\":_state_dict['features_extractor.cnn.2.conv2.weight'], \n    \"cnn.2.conv2.bias\":_state_dict['features_extractor.cnn.2.conv2.bias'], \n    \"cnn.3.conv1.weight\":_state_dict['features_extractor.cnn.3.conv1.weight'], \n    \"cnn.3.conv1.bias\":_state_dict['features_extractor.cnn.3.conv1.bias'], \n    \"cnn.3.conv2.weight\":_state_dict['features_extractor.cnn.3.conv2.weight'], \n    \"cnn.3.conv2.bias\":_state_dict['features_extractor.cnn.3.conv2.bias'], \n    \"linear.0.weight\":_state_dict['features_extractor.linear.0.weight'], \n    \"linear.0.bias\":_state_dict['features_extractor.linear.0.bias'], \n    \"action_net.0.weight\":_state_dict['action_net.weight'],\n    \"action_net.0.bias\":_state_dict['action_net.bias'],\n}\nstate_dict = base64.b64encode(zlib.compress(pickle.dumps(state_dict)))\nwith open('submission.py', 'r') as file:\n    src = file.read()\nsrc = src.replace(\"_STATE_DICT_\", f\"{state_dict}\")\nwith open('submission.py', 'w') as file:\n    file.write(src)","3478e3c6":"kaggle_env = make(\"football\", debug = False,\n                  configuration={\"scenario_name\": scenario_name, \n                                 \"running_in_notebook\": True,\n                                 \"save_video\": False})","ec00d0c6":"output = kaggle_env.run([\"submission.py\", \"do_nothing\"])","fb4d1260":"scores = output[-1][0][\"observation\"][\"players_raw\"][0][\"score\"]\nprint(\"Scores  {0} : {1}\".format(*scores))\nprint(\"Rewards {0} : {1}\".format(output[-1][0][\"reward\"], output[-1][1][\"reward\"]))","ef76014d":"viz = visualize(output)","ac46ea7d":"HTML(viz.to_html5_video())","3e63eddd":"---\n\n# <center> GFootball Stable-Baselines3 <\/center>\n\n---\n<center><img src=\"https:\/\/raw.githubusercontent.com\/DLR-RM\/stable-baselines3\/master\/docs\/_static\/img\/logo.png\" width=\"308\" height=\"268\" alt=\"Stable-Baselines3\"><\/center>\n<center><small>Image from Stable-Baselines3 repository<\/small><\/center>\n\n---\nThis notebook uses the [Stable-Baselines3](https:\/\/github.com\/DLR-RM\/stable-baselines3) library to train a [PPO](https:\/\/openai.com\/blog\/openai-baselines-ppo\/) reinforcement learning agent on [GFootball Academy](https:\/\/github.com\/google-research\/football\/tree\/master\/gfootball\/scenarios) scenarios, applying the architecture from the paper \"[Google Research Football: A Novel Reinforcement Learning Environment](https:\/\/arxiv.org\/abs\/1907.11180)\".","b6f29a06":"> Modified [Human Readable Visualization](https:\/\/www.kaggle.com\/jaronmichal\/human-readable-visualization)","6eed800a":"---\n# PPO Model\n> [Stable-Baselines3: PPO](https:\/\/stable-baselines3.readthedocs.io\/en\/master\/modules\/ppo.html)<br\/>\n> [Stable-Baselines3: Vectorized Environments](https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/vec_envs.html)<br\/>\n> [Stable-Baselines3: Custom Policy Network](https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/custom_policy.html)<br\/>\n> [GFootball: A Novel Reinforcement Learning Environment](https:\/\/arxiv.org\/abs\/1907.11180)<br\/>\n> [GFootball: Academy Scenarios](https:\/\/github.com\/google-research\/football\/tree\/master\/gfootball\/scenarios)<br\/>","1a85d8c9":"---\n# Checkpoints\n\n0. [academy_empty_goal_close @ 800K steps](https:\/\/www.kaggle.com\/kwabenantim\/gfootball-stable-baselines3?scriptVersionId=45569809#Test-Agent) (Nature CNN)<br\/>\n1. [academy_empty_goal @ 800K steps](https:\/\/www.kaggle.com\/kwabenantim\/gfootball-stable-baselines3?scriptVersionId=45639135#Test-Agent) (Nature CNN)<br\/>\n2. [academy_run_to_score @ 800K steps](https:\/\/www.kaggle.com\/kwabenantim\/gfootball-stable-baselines3?scriptVersionId=45941674#Test-Agent) (Nature CNN)<br\/>\n3. [academy_run_to_score_with_keeper @ 800K steps](https:\/\/www.kaggle.com\/kwabenantim\/gfootball-stable-baselines3?scriptVersionId=45703399#Test-Agent) (Nature CNN)<br\/>\n4. [academy_pass_and_shoot_with_keeper @ 800K steps](https:\/\/www.kaggle.com\/kwabenantim\/gfootball-stable-baselines3?scriptVersionId=45716494#Test-Agent) (Nature CNN)<br\/>\n5. [academy_run_pass_and_shoot_with_keeper @ 1.6M steps](https:\/\/www.kaggle.com\/kwabenantim\/gfootball-stable-baselines3?scriptVersionId=46590578#Testing) (Nature CNN)<br\/>\n6. [academy_3_vs_1_with_keeper @ 500K steps](https:\/\/www.kaggle.com\/kwabenantim\/gfootball-stable-baselines3?scriptVersionId=46843278#Testing) (GFootball CNN)","19bf07ee":"---\n# Football CNN\n> [Stable-Baselines3: Custom Policy Network](https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/custom_policy.html)<br\/>\n> [Google Research Football: A Novel Reinforcement Learning Environment](https:\/\/arxiv.org\/abs\/1907.11180)","d1e8b80a":"---\n# Agent\n> [Stable-Baselines: Exporting Models](https:\/\/stable-baselines.readthedocs.io\/en\/master\/guide\/export.html)<br\/>\n> [Stable-Baselines: Converting a Model into PyTorch](https:\/\/github.com\/hill-a\/stable-baselines\/issues\/372)<br\/>\n> [Connect4: Make Submission with Stable-Baselines3](https:\/\/www.kaggle.com\/toshikazuwatanabe\/connect4-make-submission-with-stable-baselines3)","bf7da334":"---\n# Training\n> [Stable-Baselines3: Examples](https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/examples.html)<br\/>\n> [Stable-Baselines3: Callbacks](https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/callbacks.html)","586e3fb8":"---\n# Testing","e1696fd2":"---\n# Football Gym\n> [Stable-Baselines3: Custom Environments](https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/custom_env.html)<br\/>\n> [SEED RL Agent](https:\/\/www.kaggle.com\/piotrstanczyk\/gfootball-train-seed-rl-agent): stacked observations"}}