{"cell_type":{"dccc92ec":"code","c1bcb0b8":"code","6f01e271":"code","1e837e3c":"code","db186b96":"code","f5a1992a":"code","8a2a1f31":"code","90e33d3c":"code","c2365e5b":"code","4e518849":"code","dafe4834":"code","c98d8b75":"code","4542b21f":"code","f6716b8f":"code","696ef90c":"code","6bb47f55":"code","50ec54e2":"code","3c67fba4":"code","b2e4cda5":"code","632ac27d":"code","fe551655":"code","e659fc24":"code","b95c7137":"code","ba63d481":"code","4b94f611":"code","f9da49ca":"code","ec7ebd1e":"code","95eb658b":"code","70e53c48":"code","70487909":"markdown","9cccbcf9":"markdown","d1e22305":"markdown","2e7d7b79":"markdown","ce44851f":"markdown","c5c23879":"markdown","0385e9a3":"markdown","6c0e5457":"markdown","5bfdbb29":"markdown","3bd9e846":"markdown","fc73a6f5":"markdown","a2432056":"markdown","ecc04797":"markdown","32f0d234":"markdown","7ef43e57":"markdown","01a8c2e4":"markdown","1ce5914d":"markdown"},"source":{"dccc92ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1bcb0b8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix","6f01e271":"data = pd.read_csv(\"\/kaggle\/input\/consumer-reviews-of-amazon-products\/1429_1.csv\")","1e837e3c":"data.head()","db186b96":"data = data[['reviews.rating' , 'reviews.text']]\ndata=data.dropna()\ndata.head()","f5a1992a":"counts = data['reviews.rating'].value_counts()\nplt.bar(counts.index, counts.values)\nplt.show()","8a2a1f31":"data2 = pd.read_csv(\"\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\")\ndata2 = data2[['reviews.rating' , 'reviews.text']]\ndata2 = data2[data2[\"reviews.rating\"]<=3]\n\ndata3 = pd.read_csv(\"\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\")\ndata3 = data3[['reviews.rating' , 'reviews.text']]\ndata3 = data3[data3[\"reviews.rating\"]<=3]\n\nframes = [data, data2, data3]\ndf = pd.concat(frames)\ndf = df.dropna()","90e33d3c":"sentiment = {1: 0,\n            2: 0,\n            3: 1,\n            4: 2,\n            5: 2}\n\ndf[\"sentiment\"] = df[\"reviews.rating\"].map(sentiment)\n\n#print(df[df[\"sentiment\"].isnull()])\ndf[\"sentiment\"] = pd.to_numeric(df[\"sentiment\"], errors='coerce')                                    \ndf = df.dropna(subset=[\"sentiment\"])\ndf[\"sentiment\"]  = df[\"sentiment\"] .astype(int)","c2365e5b":"df[\"reviews.text\"]=df[\"reviews.text\"].apply(lambda elem: re.sub(\"[^a-zA-Z]\", \" \", str(elem)))\ndf[\"reviews.text\"]=df[\"reviews.text\"].str.lower()\n#tokenizer = RegexpTokenizer(r'\\w+')\nwords_descriptions = df[\"reviews.text\"].str.split()\n\nstopword_list = stopwords.words('english')\nps = PorterStemmer()\nwords_descriptions = words_descriptions.apply(lambda elem: [word for word in elem if not word in stopword_list])\nwords_descriptions = words_descriptions.apply(lambda elem: [ps.stem(word) for word in elem])\n\ndf['cleaned'] = words_descriptions.apply(lambda elem: ' '.join(elem))\ndf['cleaned'].head()","4e518849":"vectorizer =TfidfVectorizer()\ntext = vectorizer.fit_transform(df['cleaned']).toarray()\ntexts=pd.DataFrame(text)","dafe4834":"y=df[\"sentiment\"].values\nX=pd.DataFrame(texts)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)","c98d8b75":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\nlr.fit(X_train,y_train)","4542b21f":"print('Train accuracy :', (lr.score(X_train, y_train))*100)\nprint('Test accuracy :', (lr.score(X_test, y_test))*100)\n      \nprint('\\n CONFUSION MATRIX')\nprint(confusion_matrix(y_test, lr.predict(X_test)))\nprint('\\nCLASSIFICATION REPORT')\nprint(classification_report(y_test, lr.predict(X_test)))","f6716b8f":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train, y_train)","696ef90c":"print('Train accuracy :', (nb.score(X_train, y_train))*100)\nprint('Test accuracy :', (nb.score(X_test, y_test))*100)\n      \nprint('\\n CONFUSION MATRIX')\nprint(confusion_matrix(y_test, nb.predict(X_test)))\nprint('\\nCLASSIFICATION REPORT')\nprint(classification_report(y_test, nb.predict(X_test)))","6bb47f55":"from sklearn.naive_bayes import BernoulliNB\ndt = BernoulliNB()\ndt.fit(X_train,y_train)","50ec54e2":"print('Train accuracy :', (dt.score(X_train, y_train))*100)\nprint('Test accuracy :', (dt.score(X_test, y_test))*100)\n      \nprint('\\n CONFUSION MATRIX')\nprint(confusion_matrix(y_test, dt.predict(X_test)))\nprint('\\nCLASSIFICATION REPORT')\nprint(classification_report(y_test, dt.predict(X_test)))","3c67fba4":"from keras.utils import np_utils\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Embedding, GRU, Dropout, LSTM, Bidirectional\nfrom keras.layers import GlobalMaxPooling1D","b2e4cda5":"token = Tokenizer()\ntoken.fit_on_texts(df[\"reviews.text\"])\nword_index = token.word_index\nmax_len = 120\nX_train, X_test, y_train, y_test = train_test_split(df[\"reviews.text\"], df[\"sentiment\"], test_size=0.25, random_state=42)","632ac27d":"X_train = token.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=max_len, padding = \"post\",truncating = \"post\")\n\nX_test = token.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen=max_len, padding = \"post\", truncating = \"post\")\n\ny_train = np_utils.to_categorical(y_train, num_classes=3)\ny_test = np_utils.to_categorical(y_test, num_classes=3)\n\nlen(y_test),len(X_test),len(X_train),len(y_train)","fe551655":"vocab_size = len(word_index)+1\nembedding_dim = 16\noptimizer = Adam(lr=0.0001, decay=0.0001)\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim , input_length=max_len))\nmodel.add(LSTM(150, return_sequences=True))\nmodel.add(LSTM(150, return_sequences=False))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\nmodel.summary()","e659fc24":"model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.fit(X_train, y_train, batch_size=16, epochs=8)","b95c7137":"result = model.evaluate(X_train, y_train)\nprint( 'Train accuracy :' , result[1]*100)\nresult = model.evaluate(X_test,y_test)\nprint( 'Test accuracy :' , result[1]*100)","ba63d481":"f = open('\/kaggle\/input\/glove6b50dtxt\/glove.6B.50d.txt',encoding=\"utf8\")\nembidx = {}\nfor line in f:\n    val = line.split()\n    word = val[0]\n    coeff = np.asarray(val[1:],dtype = 'float')\n    embidx[word] = coeff\n\nf.close()\n\nprint('Found %s word vectors.' % len(embidx))","4b94f611":"vocab_size=len(word_index)\nembedding_dim = 50\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n\nfor word, i in word_index.items():\n    embedding_vector = embidx.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;\n\nprint(len(embeddings_matrix))","f9da49ca":"embeddings_matrix.shape","ec7ebd1e":"embedd_layer = Embedding(vocab_size+1, embedding_dim, input_length=max_len, \n                         weights=[embeddings_matrix], trainable=False)\n\nmodel = Sequential()\nmodel.add(embedd_layer)\nmodel.add(Bidirectional(LSTM(64 , return_sequences = True , dropout = 0.1 , recurrent_dropout = 0.1)))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(150,activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(3,activation=\"softmax\"))\nmodel.compile(loss = 'binary_crossentropy' , optimizer = Adam(lr = 0.01) , metrics = ['accuracy'])\nmodel.summary()","95eb658b":"hist = model.fit(X_train,y_train,epochs = 10 , batch_size = 512, validation_data = (X_test,y_test))","70e53c48":"result = model.evaluate(X_test,y_test)\nprint('Test accuracy :', result[1]*100)","70487909":"**Model**","9cccbcf9":"Due to the imbalance of our dataset, we try to add in more datas to reduce overfitting.\n\nWe add files such that we include rows with reviews lesser than 4 inorder to balance the dataset.","d1e22305":"Vectorizing the array.","2e7d7b79":"# Using GloVe\n\nGloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity","ce44851f":"1.Converting Text to Sequence\n\n2.Padding to ensure that all sequences in a list have the same length.","c5c23879":"# Using RNN - Long Short Term Memory(LSTM)","0385e9a3":"# We have 3 classifications.\n1. BAD (0) for star rating 1 and 2.\n2. AVERAGE (1) for star rating 3.\n3. EXCELLENT (2) for star rating 4 and 5.","6c0e5457":"# Preprocessing the data\n\nApplying various NLP techniques - tokenize and remove all the puncuations and uneseccary jargons.","5bfdbb29":"Read the dataset\n","3bd9e846":"Splitting the data into training and testing set.","fc73a6f5":"Importing the libraries","a2432056":"# **Sentiment Analysis for Amazon Reviews**\n\nIn this project, we study the correlation between Amazon's product reviews and the rating of the products given by the customers. \n\nWe use Natural Language Processing(NLP) to predict whether the sentiment of review is positive or average or negative.\n\nBoth traditional machine learning algorithms including Bernoulli Naive Bayes,  Multinomial Naive Bayesian(MNB), Logistic regression method and deep neural networks using such as Long Short Term Memory (LSTM) is used. \n\nWe also use gloVe input features with dropout to obtain results.\n\nBy comparing these results, we get a good understanding of the these algorithms.","ecc04797":"Tokenizer is done to vectorize a text corpus, by turning each text into either a sequence of integers or into a vector.\nAnd then we split it to train and test sets.","32f0d234":"# Bernoulli Naive Bayes","7ef43e57":"# Logistic Regression","01a8c2e4":"# Multinomial Naive Bayes","1ce5914d":"# Conclusion\n\nWe extracted the features of our dataset and built several supervised model based on that. \nThese models not only include traditional algorithms such as naive bayes, logistic regression but also deep learning models such as Recurrent Neural Networks and Convolutional Neural networks. \n\nFrom the results, our highest accuracy on the test set is 91.3% when using LSTM with GloVe and dropout. \n\nOne of the main reason the accuracy is not high enough is because of the data imbalance. "}}