{"cell_type":{"31bd318e":"code","2814ef5a":"code","d21aa04f":"code","e1c26b2f":"code","beee29a2":"code","fb0159ac":"code","cca30def":"code","99bc5aa8":"code","233ab763":"code","52b45e8a":"code","a1bed3c1":"code","08072a61":"code","78d31be2":"code","88d1a795":"code","64658ef2":"code","13582a16":"code","3fb6396c":"code","50aa9ad3":"markdown","4e984a75":"markdown","7ecd1277":"markdown","bdd880a8":"markdown","e717bcc4":"markdown","b571c44d":"markdown","a5f8bcd1":"markdown","d0ea9ea2":"markdown","d97b9cc2":"markdown","778ba6ab":"markdown","06d62f6c":"markdown"},"source":{"31bd318e":"import pandas as pd","2814ef5a":"df=pd.read_csv('..\/input\/news-headlines-dataset-for-stock-sentiment-analyze\/Data.csv', encoding = \"ISO-8859-1\")","d21aa04f":"df.head()","e1c26b2f":"train = df[df['Date'] < '20150101']\ntest = df[df['Date'] > '20141231']","beee29a2":"# Removing punctuations\ndata=train.iloc[:,2:27]\ndata.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n\n# Renaming column names for ease of access\nlist1= [i for i in range(25)]\nnew_Index=[str(i) for i in list1]\ndata.columns= new_Index\ndata.head(5)","fb0159ac":"# Convertng headlines to lower case\nfor index in new_Index:\n    data[index]=data[index].str.lower()\ndata.head(1)","cca30def":"' '.join(str(x) for x in data.iloc[1,0:25])","99bc5aa8":"headlines = []\nfor row in range(0,len(data.index)):\n    headlines.append(' '.join(str(x) for x in data.iloc[row,0:25]))","233ab763":"headlines[0]","52b45e8a":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier","a1bed3c1":"## implement BAG OF WORDS\ncountvector=CountVectorizer(ngram_range=(2,2))\ntraindataset=countvector.fit_transform(headlines)","08072a61":"# implement RandomForest Classifier\nrandomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandomclassifier.fit(traindataset,train['Label'])","78d31be2":"## Predict for the Test Dataset\ntest_transform= []\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset = countvector.transform(test_transform)\npredictions = randomclassifier.predict(test_dataset)","88d1a795":"test.loc[3723,:]","64658ef2":"predictions","13582a16":"## Import library to check accuracy\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","3fb6396c":"matrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)","50aa9ad3":"Headline list contain all the sentences ","4e984a75":"CountVectorizer convert all the sentences into vectors","7ecd1277":"# Combining these headlines together so that i can convert this text into vectors","bdd880a8":"In this dataset, we have top headlines for specific companies. Based on these headlines there are labels of values zero and one. Zero basically means that stock price will have a negative impact and One means that stock price will have a popositive impact.\nTop1, Top2\u2026. these are our news headlines.","e717bcc4":"# Stock Sentiment Analysis using News Headlines","b571c44d":"# Combining all the sentences of all the records","a5f8bcd1":"# Data Pre-processing","d0ea9ea2":"Iterating through first sentence and then taking all the columns and joining them with space in between. ","d97b9cc2":"# Split the dataset","778ba6ab":"This is the dataset of all the combined headings ","06d62f6c":"Prediction based on test dataset and for combined data we got 1st value as 1."}}