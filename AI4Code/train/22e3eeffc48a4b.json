{"cell_type":{"5b113c47":"code","3197c4e4":"code","8bd117ef":"code","47caf721":"code","1429f0a1":"code","a0b9026c":"code","546ecf7e":"code","7db99c6e":"code","3a6cf253":"code","eb620ddd":"code","22ff40cc":"code","c47e7d1b":"code","3c648227":"code","549de019":"code","307e854c":"code","027fd5ed":"code","48a0ee34":"code","bd84820c":"code","df51170b":"code","ffe2d732":"code","9458be2d":"code","c813355f":"code","c401b6e7":"code","32574abc":"code","e7b203af":"code","c0798651":"code","3dd44fb4":"code","bb84cceb":"code","cc5050f4":"code","c8200f40":"code","366c4c51":"code","7f15e81b":"code","1ca0e304":"code","9c6742fd":"markdown","e3f909f1":"markdown","adbf4d1f":"markdown","4f2f7833":"markdown","87c35307":"markdown","1723c5e3":"markdown","3352a82f":"markdown","3ad67c94":"markdown"},"source":{"5b113c47":"import pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew","3197c4e4":"df=pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')","8bd117ef":"df","47caf721":"df.describe()","1429f0a1":"df.drop(['id','date'],axis=1,inplace=True)","a0b9026c":"df","546ecf7e":"plt.figure(figsize=(14,25))\nsns.heatmap(df.corr(),annot=True )","7db99c6e":"fig = plt.figure(figsize=(16,5))\nfig.add_subplot(2,2,1)\nsns.countplot(df['bedrooms'])\nfig.add_subplot(2,2,2)\nsns.countplot(df['grade'])\nfig.add_subplot(2,2,3)\nsns.countplot(df['waterfront'])\nfig.add_subplot(2,2,4)\nsns.countplot(df['floors'])","3a6cf253":"fig = plt.figure(figsize=(16,5))\nfig.add_subplot(2,2,1)\nsns.scatterplot(df['sqft_living15'], df.price)\nfig.add_subplot(2,2,2)\nsns.scatterplot(df['sqft_lot15'],df.price)\nfig.add_subplot(2,2,3)\nsns.scatterplot(df['sqft_above'],df.price)\nfig.add_subplot(2,2,4)\nsns.scatterplot(df['yr_built'],df.price)","eb620ddd":"pd.value_counts(df.yr_built)","22ff40cc":"sns.distplot(df['price'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df['price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df['price'], plot=plt)\nplt.show() ","c47e7d1b":"cor = df.corr()\ncor[cor['price']<0.05].index","3c648227":"cor = df.corr()\ncor[cor['price']>0.3].index","549de019":"house=df[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'view', 'grade','sqft_above', 'sqft_basement', 'lat', 'sqft_living15']]","307e854c":"house.head()","027fd5ed":"sns.pairplot(house)","48a0ee34":"num_cols = house.select_dtypes(exclude=['object'])\n\nfig = plt.figure(figsize=(20,8))\n\nfor col in range(len(num_cols.columns)):\n    fig.add_subplot(2,5,col+1)\n    sns.distplot(num_cols.iloc[:,col], hist=False, rug=True, kde_kws={'bw':0.1}, label='UV')\n    plt.xlabel(num_cols.columns[col])\n\nplt.tight_layout()","bd84820c":"num_cols = house.select_dtypes(exclude=['object'])\n\nfig = plt.figure(figsize=(20,8))\n\nfor col in range(len(num_cols.columns)):\n    fig.add_subplot(2,5,col+1)\n    sns.scatterplot(x=num_cols.iloc[:,col], y=house['price'])\n    plt.xlabel(num_cols.columns[col])\n\nplt.tight_layout()","df51170b":"num_cols = house.select_dtypes(exclude=['object'])\n\nfig = plt.figure(figsize=(20,8))\n\nfor col in range(len(num_cols.columns)):\n    fig.add_subplot(2,5,col+1)\n    sns.regplot(x=num_cols.iloc[:,col], y=house['price'],x_estimator=np.mean, logx=True)\n    plt.xlabel(num_cols.columns[col])\n\nplt.tight_layout()","ffe2d732":"plt.figure(figsize=(6,6))\nsns.heatmap(house.corr(), annot=True)","9458be2d":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom math import sqrt\nfrom sklearn.linear_model import LinearRegression, \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings('ignore')","c813355f":"X = df.drop(['price'], axis=1)\ny = df['price']","c401b6e7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","32574abc":"log_clf=LinearRegression()\nrnd_clf = RandomForestRegressor()\ngbr_clf=GradientBoostingRegressor(n_estimators=3000, learning_rate=0.1, max_depth=4, max_features='sqrt',\n                                               min_samples_leaf=15, min_samples_split=10, loss='huber')\nxgb_clf=XGBRegressor(n_estimators=3000)","e7b203af":"X.head()","c0798651":"voting_clf = VotingRegressor([('lr', log_clf), ('rnd', rnd_clf),  ('gbr', gbr_clf),('xbg', xgb_clf)])\nvoting_clf.fit(X_train, y_train)","3dd44fb4":"for clf in (log_clf, rnd_clf,voting_clf, gbr_clf,xgb_clf):\n    clf.fit(X_train, y_train)\n    k = X_test.shape[1]\n    n = len(X_test)\n    y_predition = clf.predict(X_test)\n    RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predition)) , '.3f'))\n    MSE = mean_squared_error(y_test, y_predition).round(3)\n    MAE = mean_absolute_error(y_test, y_predition).round(3)\n    r2 = r2_score(y_test, y_predition).round(3)\n    adj_r2 = 1-(1-r2)*(n-1)\/(n-k-1)\n    MAPE = np.mean( np.abs((y_test - y_predition) \/y_test ) ) * 100\n    print(clf.__class__.__name__, '\\nr2_score', r2, '\\nRMSE =',RMSE, '\\nMSE =',MSE, \n          '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2, '\\nMean Absolute Percentage Error =',MAPE, '%')\n    ","bb84cceb":"import sklearn.preprocessing as preproc","cc5050f4":"X2 = preproc.PolynomialFeatures(include_bias=False).fit_transform(X)","c8200f40":"X_trai, X_tes, y_trai, y_tes = train_test_split(X2, y, test_size=0.22, random_state=42)","366c4c51":"voting_clf = VotingRegressor([('lr', log_clf), ('rnd', rnd_clf),  ('gbr', gbr_clf),('xbg', xgb_clf)])\nvoting_clf.fit(X_trai, y_trai)","7f15e81b":"k = X_tes.shape[1]\nn = len(X_tes)","1ca0e304":"for clf in (log_clf, rnd_clf,voting_clf, gbr_clf,xgb_clf):\n    clf.fit(X_trai, y_trai)\n    k = X_tes.shape[1]\n    n = len(X_tes)\n    y_pred = clf.predict(X_tes)\n    RMSE = float(format(np.sqrt(mean_squared_error(y_tes, y_pred)) , '.3f'))\n    MSE = mean_squared_error(y_tes, y_pred).round(3)\n    MAE = mean_absolute_error(y_tes, y_pred).round(3)\n    r2 = r2_score(y_tes, y_pred).round(3)\n    adj_r2 = 1-(1-r2)*(n-1)\/(n-k-1)\n    MAPE = np.mean( np.abs((y_tes - y_pred) \/y_test ) ) * 100\n    print(clf.__class__.__name__, '\\nr2_score', r2, '\/nRMSE =',RMSE, '\\nMSE =',MSE, \n          '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2, '\\nMean Absolute Percentage Error =',MAPE, '%')","9c6742fd":"The price is right skewed, Right-skewed distributions are also called positive-skew distributions. That\u2019s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.","e3f909f1":"'id' and 'date' features will not be needed so and dropped ","adbf4d1f":"# ****The GradientBoostingRegressor  seems to perform best with r2_score 0.907 and Adjusted R2 = 0.9066110594795539 following this i will try to scale the data to see if it will imorove **","4f2f7833":"To have more understanding about the data i will select the features that are more corrolated to the Price so i can view them ","87c35307":"# **This process does not have postive influence on the score so I will prefer the first without the preprocessing ******","1723c5e3":"Scatterplot matrix\nIn some cases, we may want to plot a scatterplot matrix such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix.","3352a82f":"Scatter plot\nThe scatter plot displays values of two numerical variables as Cartesian coordinates in 2D space. Scatter plots in 3D are also possible.","3ad67c94":"Correlation matrix\nLet's look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.\n\nFirst, we will use the method corr() on a DataFrame that calculates the correlation between each pair of features. Then, we pass the resulting correlation matrix to heatmap() from seaborn, which renders a color-coded matrix for the provided values:"}}