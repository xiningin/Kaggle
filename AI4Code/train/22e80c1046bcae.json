{"cell_type":{"4eae834e":"code","45cb0e3a":"code","d07dac85":"code","892b0391":"code","7a86865b":"markdown"},"source":{"4eae834e":"#Libraries\n\nfrom keras.layers import Input, Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Flatten\nfrom keras.models import Model\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt \nfrom PIL import Image \nimport seaborn as sns\nimport pandas as pd \nimport numpy as np \nimport os \nfrom keras.applications import VGG19\nfrom os import listdir, makedirs\nfrom os.path import join, exists, expanduser\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nfrom numpy import newaxis\nimport cv2\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D,Dense,Activation,Dropout,Flatten,BatchNormalization\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom keras.applications import VGG19\nfrom skimage.feature import local_binary_pattern\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nfrom tensorflow.keras.regularizers import l2\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","45cb0e3a":"#Batch for Keras ImageGenerator is 16, img dimensions are 100,100\nimg_width, img_height = 100, 100 #\ntrain_data_dir = '..\/input\/fruits\/fruits-360\/Training\/'\nvalidation_data_dir = '..\/input\/fruits\/fruits-360\/Test\/'\n\nbatch_size = 16\n\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nplt.figure(figsize=(5,5))\nplt.axis('off')\nplt.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Banana\/120_100.jpg\")))\n\n\nplt.figure(figsize=(5,5))\nplt.axis('off')\nplt.imshow(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Avocado\/100_100.jpg\")))\nprint(\"shape of the banana\",(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Banana\/120_100.jpg\"))).shape)\nprint(\"shape of the avocado\",(np.array(Image.open(\"..\/input\/fruits\/fruits-360\/Training\/Avocado\/100_100.jpg\"))).shape)","d07dac85":"numberOfClass=131\nbatch_size = 8\nepochs=32\n\n\nvgg = VGG19(include_top= False, weights = \"imagenet\", input_shape=(100,100,3))\nvgg_layer_list = vgg.layers\n\nmodel = Sequential ()\n\nfor layer in vgg_layer_list:\n    model.add(layer)\n    \nfor layer in model.layers:\n    layer.trainable = False\n    \n    \nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(numberOfClass))\nmodel.add(Activation('softmax'))\nmodel.summary()\n\nopt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n\nmodel.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n\n\n\n\nhist = model.fit_generator(\n        generator=train_generator,\n        steps_per_epoch = 1500,\n        epochs=epochs,\n        validation_data=validation_generator,\n        validation_steps = 220,\n        shuffle=True)\n\n\n\n# model plot\n\nprint(hist.history.keys())\n\nplt.plot(hist.history[\"loss\"], label =\"Train Loss\")\nplt.plot(hist.history[\"val_loss\"], label =\"Validation Loss\")\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(hist.history[\"accuracy\"], label =\"Train accuracy\")\nplt.plot(hist.history[\"val_accuracy\"], label =\"Validation accuracy\")\nplt.legend()\nplt.show()","892b0391":"inception_base = applications.ResNet50(weights='imagenet', include_top=False)\n\nx = inception_base.output\nx = GlobalAveragePooling2D()(x)\n\nx = Dense(512, activation='relu')(x)\n\npredictions = Dense(131, activation='softmax')(x)\n\n\ninception_transfer = Model(inputs=inception_base.input, outputs=predictions)\ninception_transfer.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\n\nwith tf.device(\"\/device:GPU:0\"):\n    history_pretrained = inception_transfer.fit_generator(\n    train_generator,\n    epochs=5, shuffle = True, verbose = 1, validation_data = validation_generator)","7a86865b":"# **Fruit-360 Prediction by Using Different CNN Architectures**\n\n\nI have been trained the Fruit-360 data set with 3 different architectures; VGG19 and Resnet50\n\nThe photos are in shape of ( 100, 100 , 3 ) so that it's important to use orginal size as an input for the networks. I saw most of the people up sample the photos to ( 255,255 ), it is alright but it may increase the workload of the networks moreover increase the time to get optima.\n\nFor both \" VGG19 \" and \"Resnet50\" have pretrained weights of imagenet. Since the imagenet has over 14M data and already have fruit classes in it, using transfer learning will boost the success. One may start from scratch but it will lead more epoch & training time with lower accuracy as well. \n\n# **Results**\n\nResnet: val_accuracy: 0.9892\n\n\nVGG19 : val_accuracy: 0.9202\n"}}