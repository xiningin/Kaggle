{"cell_type":{"8ce5c2fd":"code","c53ff082":"code","238a2638":"code","d906cb20":"code","cc59a13a":"code","82c455ac":"code","13d1bf8c":"code","403f0db4":"code","839b0828":"code","7354c4b1":"code","208b563c":"code","a8a90f67":"code","701d2383":"code","9a3ea03c":"code","8e473634":"code","9f1a222b":"code","3c061abd":"code","4b6b0036":"code","5211b0e0":"code","92c2174c":"code","9ff44819":"code","bc4fb11b":"code","e44d428f":"code","60512cf0":"code","d01283f7":"code","5300db21":"code","ffb971cf":"code","feaf72ad":"code","209b49f0":"code","ac710220":"markdown","f25baebd":"markdown","4332b00a":"markdown","cccf3196":"markdown","9ca50c6f":"markdown","3a817c1e":"markdown","ea53bf0c":"markdown"},"source":{"8ce5c2fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c53ff082":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","238a2638":"import pandas as pd\nfrom scipy.io import arff\ndata = arff.loadarff('..\/input\/enbarff\/enb.arff')\nveri = pd.DataFrame(data[0])\nveri.head()","d906cb20":"veri.describe()","cc59a13a":"X = veri.iloc[:,:8]\ny = veri.iloc[:,8:]\nprint(X.shape, y.shape)","82c455ac":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","13d1bf8c":"X_train, X_test, y_train, y_test = train_test_split(X,y)","403f0db4":"scalerx = StandardScaler().fit(X_train)\nX_train_sc = scalerx.transform(X_train)\nX_test_sc = scalerx.transform(X_test)\n\nscalery = StandardScaler().fit(y_train)\ny_train_sc = scalery.transform(y_train)\ny_test_sc = scalery.transform(y_test)","839b0828":"X_train_sc.shape\nX_test_sc.shape\ny_train_sc.shape\ny_test.shape","7354c4b1":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score","208b563c":"model_1 = LinearRegression()\nmodel_1.fit(X_train_sc,y_train_sc)\ny_pred_1 = model_1.predict(X_test_sc)\ny_pred_real = scalery.inverse_transform(y_pred_1)\n#abs_err = abs(y_pred_real-y_test) \ncomparison = pd.concat([pd.DataFrame(y_pred_real), pd.DataFrame(y_test.values)], axis=1)\ncomparison.columns = ['Predicted Y1','Predicted Y2', 'Measured Y1','Measured Y2']\nprint('R^2 values:', r2_score(comparison['Predicted Y1'],y_test['Y1']), r2_score(comparison['Predicted Y2'],y_test['Y2']))\nprint(\"Multi_Linear_Reg_predictions:\\n\")\ncomparison","a8a90f67":"model_2 = KNeighborsRegressor()\nmodel_2.fit(X_train_sc,y_train_sc)\ny_pred_2 = model_2.predict(X_test_sc)\ny_pred_real2 = scalery.inverse_transform(y_pred_2)\ncomparison2 = pd.concat([pd.DataFrame(y_pred_real2), pd.DataFrame(y_test.values)], axis=1)\ncomparison2.columns = ['Predicted Y1','Predicted Y2', 'Measured Y1','Measured Y2']\nprint('R^2 values:', r2_score(comparison2['Predicted Y1'],y_test['Y1']), r2_score(comparison2['Predicted Y2'],y_test['Y2']))\nprint(\"Multi_kNN_predictions:\\n\")\ncomparison2","701d2383":"model_3 = DecisionTreeRegressor()\nmodel_3.fit(X_train_sc,y_train_sc)\ny_pred_3 = model_3.predict(X_test_sc)\ny_pred_real3 = scalery.inverse_transform(y_pred_3)\ncomparison3 = pd.concat([pd.DataFrame(y_pred_real3), pd.DataFrame(y_test.values)], axis=1)\ncomparison3.columns = ['Predicted Y1','Predicted Y2', 'Measured Y1','Measured Y2']\nprint('R^2 values:', r2_score(comparison3['Predicted Y1'],y_test['Y1']), r2_score(comparison3['Predicted Y2'],y_test['Y2']))\nprint(\"Multi_DecT_predictions:\\n\")\ncomparison3\n","9a3ea03c":"model_4 = RandomForestRegressor()\nmodel_4.fit(X_train_sc,y_train_sc)\ny_pred_4 = model_4.predict(X_test_sc)\ny_pred_real4 = scalery.inverse_transform(y_pred_4)\ncomparison4 = pd.concat([pd.DataFrame(y_pred_real4), pd.DataFrame(y_test.values)], axis=1)\ncomparison4.columns = ['Predicted Y1','Predicted Y2', 'Measured Y1','Measured Y2']\nprint('R^2 values:', r2_score(comparison4['Predicted Y1'],y_test['Y1']), r2_score(comparison4['Predicted Y2'],y_test['Y2']))\nprint(\"Multi_RF_predictions:\\n\")\ncomparison4","8e473634":"from sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt ","9f1a222b":"model_1 = LinearRegression()\nkfold = RepeatedKFold(n_splits=5,n_repeats=8)\nmodel_1_s = cross_val_score(model_1, X, y, cv=kfold)\nprint(\"Model_1_Scores Mean : {} Std {}\".format(round(model_1_s.mean(),3), round(model_1_s.std(),3)))\n\nmodel_2 = KNeighborsRegressor()\nkfold = RepeatedKFold(n_splits=5,n_repeats=8)\nmodel_2_s = cross_val_score(model_2, X, y, cv=kfold)\nprint(\"Model_2_Scores Mean : {} Std {}\".format(round(model_2_s.mean(),3), round(model_2_s.std(),3)))\n\nmodel_3 = DecisionTreeRegressor()\nkfold = RepeatedKFold(n_splits=5,n_repeats=8)\nmodel_3_s = cross_val_score(model_3, X, y, cv=kfold)\nprint(\"Model_3_Scores Mean : {} Std {}\".format(round(model_3_s.mean(),3), round(model_3_s.std(),3)))\n\nmodel_4 = RandomForestRegressor()\nkfold = RepeatedKFold(n_splits=5,n_repeats=8)\nmodel_4_s = cross_val_score(model_4, X, y, cv=kfold)\nprint(\"Model_4_Scores Mean : {} Std {}\".format(round(model_4_s.mean(),3), round(model_4_s.std(),3)))","3c061abd":"from sklearn.svm import SVR \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.multioutput import MultiOutputRegressor","4b6b0036":"regressor1 = SVR(kernel='rbf', C=0.1, gamma=0.1)\nREG = MultiOutputRegressor(regressor1)\nREG.fit(X_train_sc,y_train_sc)\npred_direct = REG.predict(X_test_sc)\nreal_pred_direct = scalery.inverse_transform(pred_direct)\ny_pred_real3 = scalery.inverse_transform(pred_direct)\nregressor1_comparison = pd.concat([pd.DataFrame(real_pred_direct), pd.DataFrame(y_test.values)], axis=1)\nregressor1_comparison.columns = ['Predicted Y1','Predicted Y2', 'Measured Y1','Measured Y2']\nprint('R^2 values:', r2_score(regressor1_comparison['Predicted Y1'],regressor1_comparison['Measured Y1']),\n      r2_score(regressor1_comparison['Predicted Y2'],regressor1_comparison['Measured Y2']))\nprint(\"Direct_predictions:\\n\")\nregressor1_comparison","5211b0e0":"!pip install hydroeval","92c2174c":"import hydroeval as hy\nmy_eval_Y1 = hy.evaluator(hy.nse, regressor1_comparison['Predicted Y1'].values,\n                       regressor1_comparison['Measured Y1'].values, axis=1)\nmy_eval_Y2 = hy.evaluator(hy.nse, regressor1_comparison['Predicted Y2'].values,\n                       regressor1_comparison['Measured Y2'].values, axis=1)\nprint(\"NSE for Y1 : \", my_eval_Y1[0])\nprint(\"NSE for Y2 : \", my_eval_Y2[0])","9ff44819":"plt.figure(figsize=(5,5))\nplt.scatter(regressor1_comparison['Predicted Y1'],regressor1_comparison['Measured Y1'])\nplt.scatter(regressor1_comparison['Predicted Y2'],regressor1_comparison['Measured Y2'], marker='*')\nx=[0,40]\ny=[0,40]\nclasses=['Y1','Y2']\nplt.legend(labels=classes)\nplt.plot(x,y,'--', c='r')\nplt.xlabel('predicted values')\nplt.ylabel('measured values');","bc4fb11b":"from sklearn.svm import SVR\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.multioutput import MultiOutputRegressor","e44d428f":"regressor2 = SVR(kernel='rbf', C=0.1, gamma=0.1)\nREG2 = MultiOutputRegressor(regressor2)\nREG2.fit(X_train_sc,y_train_sc)\nchain_pred = REG2.predict(X_test_sc)\nreal_chain_pred = scalery.inverse_transform(chain_pred)\nchain_comparison = pd.concat([pd.DataFrame(real_chain_pred), pd.DataFrame(y_test.values)], axis=1)\nchain_comparison.columns = ['Predicted Y1','Predicted Y2', 'Measured Y1','Measured Y2']\nprint('R^2 values:', r2_score(chain_comparison['Predicted Y1'],chain_comparison['Measured Y1']), r2_score(chain_comparison['Predicted Y2'],chain_comparison['Measured Y2']))\nprint(\"Chain_predictions:\\n\")\nchain_comparison","60512cf0":"my_eval_Y1 = hy.evaluator(hy.nse, chain_comparison['Predicted Y1'].values,\n                       chain_comparison['Measured Y1'].values, axis=1)\nmy_eval_Y2 = hy.evaluator(hy.nse, chain_comparison['Predicted Y2'].values,\n                       chain_comparison['Measured Y2'].values, axis=1)\nprint(\"NSE for Y1 : \", my_eval_Y1[0])\nprint(\"NSE for Y2 : \", my_eval_Y2[0])","d01283f7":"plt.figure(figsize=(5,5))\nplt.scatter(chain_comparison['Predicted Y1'],chain_comparison['Measured Y1'])\nplt.scatter(chain_comparison['Predicted Y2'],chain_comparison['Measured Y2'], marker='*')\nx=[0,40]\ny=[0,40]\nclasses=['Y1','Y2']\nplt.legend(labels=classes)\nplt.plot(x,y,'--', c='r')\nplt.xlabel('predicted values')\nplt.ylabel('measured values');","5300db21":"regressor_4 = SVR()\nREG1 = MultiOutputRegressor(regressor_4)\nparam_grid = {'estimator__C':[0.1,1,10],\n             'estimator__gamma':[0.1,1],\n             'estimator__kernel':['rbf', 'sigmoid']}\ngrid = GridSearchCV(REG1, param_grid, refit=True, verbose=3, n_jobs=-1)\ngrid.fit(X_train_sc,y_train_sc)","ffb971cf":"grid.best_params_\nbest_estimator = grid.best_estimator_","feaf72ad":"pred_direct = best_estimator.predict(X_test_sc)\nreal_pred_direct = scalery.inverse_transform(pred_direct)\nregressor_comparison = pd.concat([pd.DataFrame(real_pred_direct), pd.DataFrame(y_test.values)], axis=1)\nregressor_comparison.columns = ['Predicted Y1','Predicted Y2', 'Measured Y1','Measured Y2']\nprint('R^2 values:', r2_score(regressor_comparison['Predicted Y1'],regressor_comparison['Measured Y1']), r2_score(regressor_comparison['Predicted Y2'],regressor_comparison['Measured Y2']))\nprint(\"Direct_predictions:\\n\")\nregressor_comparison","209b49f0":"plt.figure(figsize=(5,5))\nplt.scatter(regressor_comparison['Predicted Y1'],regressor_comparison['Measured Y1'])\nplt.scatter(regressor_comparison['Predicted Y2'],regressor_comparison['Measured Y2'], marker='*')\nx=[0,40]\ny=[0,40]\n\nclasses=['Y1','Y2']\nplt.legend(labels=classes)\nplt.plot(x,y,'--', c='r')\nplt.xlabel('predicted values')\nplt.ylabel('measured values');","ac710220":"### NOW Lets find the best hyperparameters with Grid Search, for direct multi output regression method","f25baebd":"### Method 2: Chain multi output regression with and without Grid Search","4332b00a":"### Lets apply Cross Validation for precise model evaluation","cccf3196":"### As seen from the R^2 and NSE values, best estimator model found with grid search gives the best prediction. ","9ca50c6f":"### Method 1: Direct multioutput regression ","3a817c1e":"### Several Ready to use ML Algorithms for multioutput regression (without direct or multi output methods)\n* LinearRegression\n* KNeigborsRegressor\n* DecisionTreeRegressor\n* RandomForestRegressor","ea53bf0c":"### The data set was downloaded from http:\/\/mulan.sourceforge.net\/datasets-mtr.html.\n### Relative compactness and X values are features and y values are outputs."}}