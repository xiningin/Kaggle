{"cell_type":{"5e57b2ac":"code","b6e0ec83":"code","249d1058":"code","0eb68a88":"code","7c3ca2f6":"code","6edae2cd":"code","c6ba96ea":"code","e34f5e44":"code","31187ae8":"code","79fdf445":"markdown","b57df5ff":"markdown","d85c400e":"markdown","9d4e446c":"markdown","1af5087e":"markdown","4bf909b7":"markdown","b26e1226":"markdown"},"source":{"5e57b2ac":"#importing required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.neighbors import NearestCentroid\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier","b6e0ec83":"data_file = pd.read_csv('..\/input\/ecoli-uci-dataset\/ecoli.csv')  #Loading Data\n\ndf1 = pd.DataFrame(data_file)                    #Data frame created and cleaned\ndf1_cleaned = df1.drop('SEQUENCE_NAME',axis=1)\n\ndf1_cleaned.SITE.replace(('cp','im','imS','imL','imU','om','omL','pp'),(1,2,3,4,5,6,7,8),inplace=True) #Data encoding\n#print(\"Correlation\",df1_cleaned.corr(method='pearson'))  #Correlation between each measures","249d1058":"dataset = df1_cleaned.values\nX=dataset[:,0:7]\ny=dataset[:,7]\nclf1 = tree.DecisionTreeClassifier()\nclf1 = clf1.fit(X,y)      #Fitting data set to decision tree\n#tree.plot_tree(clf1)   #Plotting tree\n","0eb68a88":"#Percentage of distribution of protein sequence\n\nprint('There is cp',round(df1_cleaned['SITE'].value_counts()[1]\/len(df1_cleaned) * 100,2),'% of the data set')\nprint('There is im',round(df1_cleaned['SITE'].value_counts()[2]\/len(df1_cleaned) * 100,2),'% of the data set')\nprint('There is imS',round(df1_cleaned['SITE'].value_counts()[3]\/len(df1_cleaned) * 100,2),'% of the data set')\nprint('There is imL',round(df1_cleaned['SITE'].value_counts()[4]\/len(df1_cleaned) * 100,2),'% of the data set')\nprint('There is imU',round(df1_cleaned['SITE'].value_counts()[5]\/len(df1_cleaned) * 100,2),'% of the data set')\nprint('There is om',round(df1_cleaned['SITE'].value_counts()[6]\/len(df1_cleaned) * 100,2),'% of the data set')\nprint('There is omL',round(df1_cleaned['SITE'].value_counts()[7]\/len(df1_cleaned) * 100,2),'% of the data set')\nprint('There is pp',round(df1_cleaned['SITE'].value_counts()[8]\/len(df1_cleaned) * 100,2),'% of the data set')","7c3ca2f6":"#Confusion matrix construction\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nrecall = recall_score(y_test, y_pred, average='micro')\nprint(\"Recall Score: %.2f%%\" % (recall * 100.0))\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)","6edae2cd":"print(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","c6ba96ea":"print(\"Before OverSampling, counts of label 'cp': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label 'im': {} \\n\".format(sum(y_train==2)))\nprint(\"Before OverSampling, counts of label 'imS': {}\".format(sum(y_train==3)))\nprint(\"Before OverSampling, counts of label 'imL': {} \\n\".format(sum(y_train==4)))\nprint(\"Before OverSampling, counts of label 'imU': {}\".format(sum(y_train==5)))\nprint(\"Before OverSampling, counts of label 'om': {} \\n\".format(sum(y_train==6)))\nprint(\"Before OverSampling, counts of label 'omL': {}\".format(sum(y_train==7)))\nprint(\"Before OverSampling, counts of label 'pp': {} \\n\".format(sum(y_train==8)))","e34f5e44":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nrec = recall_score(y_test, y_pred,average='micro')\nacc = accuracy_score(y_test,y_pred)\ncon = confusion_matrix(y_test, y_pred)\n\nprint(\"Recall Score\",rec*100)\nprint(\"\\nAccuracy Score\", acc*100)\nprint(\"\\n\",con)","31187ae8":"ros = RandomOverSampler(random_state=8)\nX_res, y_res = ros.fit_resample(X_train, y_train)\np = y_res\ndf23 = pd.DataFrame(X_res,p)\ndf23.to_csv('ecoli_sampled.csv')\n","79fdf445":"**CONFUSION MATRIX**\n\n<p>In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix. A confusion matrix is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\nIt allows easy identification of confusion between classes e.g. one class is commonly mislabeled as the other. Most performance measures are computed from the confusion matrix.<\/p>\n\n**ACCURACY SCORE**\n\n<p>Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right.<\/p>\n\n**RECALL SCORE**\n\n<p>The recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.<\/p>","b57df5ff":"<p>Now the data set is oversampled and it creates 880 instances of ecoli protein sequences with equal number of instances for each class<\/p>\n<p>\n    **Thanks for reading the notebook and upvote if you like it**<\/p>","d85c400e":"As we mentioned above we can calculate the percentage distribution of each class over this data set","9d4e446c":"So here our data set table have 9 columns, but we don't need the SEQUENCE_NAME columns for classification. So we dropped it\n\n<code> df.drop('SEQUENCE_NAME', axis=1) <\/code>\n\nThen to process the data we need to encode the target variable column into numericals. So each class is encoded into \n\n* cp :- 1\n* im :- 2\n* imS :- 3\n* imL :- 4\n* imU :- 5\n* om :- 6\n* omL :- 7\n* pp :- 8","1af5087e":"Here we are using various libraries like common pandas like libraries and some other special libraries like xgboost and etc. Each library have a determined purpose in this notebook\n\n* XGBoost : This library is used here as a basic classifier\n* Matrics : There are various matrics are been used like accuracy_score, recall_score, etc.\n* Sampler : SMOTE and RandomOverSampler is for sampling purposes on the data set \n* Logistic Regression : Used for calculating the current accuracy and recall of the data set\n* Tree : To construct the Tree classifier","4bf909b7":"**SMOTE vs RandomOverSampler**\n\n<p>Random oversampling just increases the size of the training data set through repetition of the original examples. It does not cause any increase in the variety of training examples.\n<br\/>\nOversampling using SMOTE not only increases the size of the training data set, it also increases the variety.\n<br\/>\nSMOTE creates new (artificial) training examples based on the original training examples. For instance, if it sees two examples (of the same class) near each other, it creates a third artificial one, bang in the middle of the original two.<\/p>","b26e1226":"Welcome to my Notebook on **Balancing Ecoli Data Set using Random Over Sampling** guys...\n<p>\n    Here we are dealing with a data set that have 336 instances of Ecoli protein sequences. These are classified into 8 different classes namely cp, im, imS, imL, imU, om, omL and pp. The data set is extremely imbalanced becuase the distribution of instances in each class is very much variant.\n<\/p>\n\n* There is cp 42.56 % of the data set\n* There is im 22.92 % of the data set\n* There is imS 0.60 % of the data set\n* There is imL 0.60 % of the data set\n* There is imU 10.42 % of the data set\n* There is om 5.95 % of the data set\n* There is omL 1.49 % of the data set\n* There is pp 15.48 % of the data set\n\n"}}