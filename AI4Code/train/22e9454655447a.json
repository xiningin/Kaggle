{"cell_type":{"7d2dadfa":"code","eccdbfe4":"code","d9d26f4b":"code","8ea93aef":"code","f2577acc":"code","72653a58":"code","09779863":"code","f892eef9":"code","839eb26c":"code","cafdc1d6":"code","74fa4e42":"code","4da78b9c":"code","ba621efa":"code","05594537":"code","2a1a8a58":"code","fd25cdb2":"code","62b26cd1":"code","632d44e2":"code","5647075f":"code","a4f541ef":"code","0545714d":"code","166b50cb":"code","5d6cb8ae":"code","c8c6d0b7":"code","964e1ec4":"code","86de122f":"code","8ad7f793":"code","fb07260f":"code","30f26c61":"code","ac48f54c":"code","d3613080":"code","2b028a7d":"code","ce1d223c":"code","8bc1ec53":"code","77d672ec":"markdown","e2aa1bbe":"markdown","7eb3f835":"markdown","f8e7751a":"markdown","28f31307":"markdown","85183d3c":"markdown","228e7ea4":"markdown","d8a3369a":"markdown","dfcf80ff":"markdown","38fb1c3f":"markdown","f2071452":"markdown","4ae78ef4":"markdown","756a6c7f":"markdown","e8523730":"markdown","2b7a02bf":"markdown","a5da60a3":"markdown","cb184c72":"markdown","b358c64b":"markdown","068d27a9":"markdown","8f374b4b":"markdown","fd69136a":"markdown","6f7be065":"markdown","16856ee8":"markdown","adad61f9":"markdown","c2e315b3":"markdown","98620e00":"markdown"},"source":{"7d2dadfa":"#data processing\nimport pandas as pd\nimport numpy as np\n\n#data visualisation\nimport seaborn as sns\nsns.set_palette('rocket')\nfrom matplotlib import pyplot as plt\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eccdbfe4":"#import data\ndata_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#concating data_train and data_test into data\ndata = [data_train.drop('SalePrice', axis=1), data_test]\ndata = pd.concat(data)","d9d26f4b":"#display the first 5 rows of the data_train\ndata_train.head()","8ea93aef":"#dropping id column because it is unnecessary for the prediction process\ndata.drop('Id', axis=1, inplace=True)\ndata_train.drop('Id', axis=1, inplace=True)\ndata_test.drop('Id', axis=1, inplace=True)","f2577acc":"#creating dataframes with only numerical features\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndata_train_numeric = data_train.select_dtypes(include=numerics)\ndata_test_numeric = data_test.select_dtypes(include=numerics)","72653a58":"#plotting all features that have correlation with the target above 0.2 \n\nfig = plt.figure(figsize=(25,30)) #figure size\na = 6  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\n\n\nfor column in data_train_numeric.columns:\n    if column != 'SalePrice' and abs(data_train_numeric['SalePrice'].corr(data_train_numeric[column]))>0.2:\n        plt.subplot(a, b, c)\n        sns.regplot(x=column, y='SalePrice', data=data_train_numeric, color='darkred')\n        c+=1\n    \nplt.tight_layout()\nplt.show()","09779863":"#only outliers have sale prices over 700000\ndata_train[data_train['SalePrice'] > 700000]","f892eef9":"#dropping outliers\noutliers = [691, 1182]\ndata_train.drop(data_train.index[outliers], inplace=True)","839eb26c":"#extracting only categorical features\ndata_train_categorical = data_train.select_dtypes(exclude=numerics)\n\nfig = plt.figure(figsize=(25,50)) #figure size\na = 11  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\n\n#ploting categorical features\nfor column in data_train_categorical.columns:\n    plt.subplot(a, b, c)\n    sns.barplot(x=column, y=data_train['SalePrice'], data=data_train_categorical, palette='rocket')\n    c+=1\n    \nplt.tight_layout()\nplt.show()","cafdc1d6":"#Correlation heatmap\ncorr = data_train.corr()\n\nplt.subplots(figsize=(12,9))\nsns.heatmap(corr, vmax=0.9, square=True)","74fa4e42":"sns.histplot(x='SalePrice', data=data_train, kde=True)","4da78b9c":"print(f'Skew value before log trasnformation is {data_train.SalePrice.skew().round(2)}')","ba621efa":"#log transformation\ndata_train[\"SalePrice\"] = np.log1p(data_train[\"SalePrice\"])\n\nprint(f'Skew value after log trasnformation is {data_train.SalePrice.skew().round(2)}')\n\n#check the plot after trasformation\nsns.histplot(x='SalePrice', data=data_train, kde=True)","05594537":"#visualusing missing values with heatmap\nplt.figure(figsize=(16,16))\nsns.heatmap(data_train.isna().transpose(),\n            cmap=\"YlGnBu\",\n            cbar = False\n           )","2a1a8a58":"#columns with missing values?\ndata.columns[data.isnull().any()] ","fd25cdb2":"#replacing missing values with no, houses with no garage\nfor feat in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    data_train[feat] = data_train[feat].fillna(\"No\")\n    data_test[feat] = data_test[feat].fillna(\"No\")\n\n#replacing missing values with 0, houses with no garage\nfor feat in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    data_train[feat] = data_train[feat].fillna(0)\n    data_test[feat] = data_test[feat].fillna(0)\n\n#replacing missing values with no, houses with no basement\nfor feat in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    data_train[feat] = data_train[feat].fillna(\"No\")\n    data_test[feat] = data_test[feat].fillna(\"No\")\n\n#replacing missing values with 0, houses with no basement\nfor feat in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    data_train[feat] = data_train[feat].fillna(0)\n    data_test[feat] = data_test[feat].fillna(0)\n\n#replacing missing values with no, houses with no pool, misc featiures, alley ect.\nfor feat in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'MasVnrType']:\n    data_train[feat] = data_train[feat].fillna(\"No\")\n    data_test[feat] = data_test[feat].fillna(\"No\")\n\n#replacing missing values with 0, houses with no masonry\nfor feat in ['MasVnrArea']:\n    data_train[feat] = data_train[feat].fillna(0)\n    data_test[feat] = data_test[feat].fillna(0)\n    \n#replacing with most common value\nfor feat in ['MSZoning', 'Utilities', 'Electrical', 'Functional', 'KitchenQual',\n             'Exterior1st', 'Exterior2nd', 'SaleType']:\n    data_train[feat] = data_train[feat].fillna(data_train[feat].mode()[0])\n    data_test[feat] = data_test[feat].fillna(data_test[feat].mode()[0])\n    \n#filling lot frontage with the mean value\ndata_train['LotFrontage'] = data_train['LotFrontage'].fillna(data_train['LotFrontage'].median())\ndata_test['LotFrontage'] = data_test['LotFrontage'].fillna(data_test['LotFrontage'].median())","62b26cd1":"#total house square feet area\ndata_train['TotalSF'] = data_train['TotalBsmtSF'] + data_train['1stFlrSF'] + data_train['2ndFlrSF']\ndata_test['TotalSF'] = data_test['TotalBsmtSF'] + data_test['1stFlrSF'] + data_test['2ndFlrSF']\n\n#age of the house when sold counting from the year when remodeled\ndata_train['Age'] = data_train['YrSold'] - data_train['YearRemodAdd']\ndata_test['Age'] = data_test['YrSold'] - data_test['YearRemodAdd']","632d44e2":"#extracting numerical features\nnumeric_feats = data.dtypes[data.dtypes != \"object\"].index\nnumeric_feats","5647075f":"#these features are not continuous\ncat_feats = ['MSSubClass','OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'MoSold', 'YrSold']\n\n#transforming numerical features into categorical features\nfor feat in cat_feats:\n    data_train[feat] = data_train[feat].astype(str)\n    data_test[feat] = data_test[feat].astype(str)\n\n#redefing numeric_feats so it contains only countinuous numarical features\nnumeric_feats = data_train.dtypes[data_train.dtypes != \"object\"].index","a4f541ef":"from sklearn.preprocessing import LabelEncoder\n\nlbl = LabelEncoder()\n\n#function that encodes all the categorical features in a dataframe\ndef encode(df):\n    for c in df.select_dtypes(exclude=[np.number]).columns:\n        df[c+'_encoded'] = lbl.fit_transform(df[c])\n    return df","0545714d":"#adding encoded features\ndata_train = encode(data_train)\ndata_test = encode(data_test)\n\n#deleting non numerical features(we replaced them with encoded ones)\ndata_train = data_train.select_dtypes(include=[np.number])\ndata_test = data_test.select_dtypes(include=[np.number])","166b50cb":"# Check the skew of all numerical features\nskewed_feats = data_train[numeric_feats].skew().sort_values(ascending=False)\nskewness = (pd.DataFrame({'Skew' :skewed_feats}))\nskewness.head(5)","5d6cb8ae":"#kde plot\nfig = plt.figure(figsize=(25,40)) #figure size\na = 8  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\n\nfor feat in numeric_feats:\n    plt.subplot(a, b, c)\n    sns.kdeplot(x=data_train[feat])\n    c+=1\n    \nplt.tight_layout()\nplt.show()","c8c6d0b7":"#features that have skew over 1\nskewness = skewness[abs(skewness['Skew'])>1]\nskewed_features = skewness.index\n\n#log transformation\ndata_train[skewed_features] = np.log1p(data_train[skewed_features])\ndata_test[skewed_features] = np.log1p(data_test[skewed_features])\n\n#cheking skew after transformation\nskewed_feats = data_train[numeric_feats].skew().sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head()","964e1ec4":"#kde plot after transformation\nfig = plt.figure(figsize=(25,40)) #figure size\na = 8  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\n\nfor feat in numeric_feats:\n    plt.subplot(a, b, c)\n    sns.kdeplot(x=data_train[feat])\n    c+=1\n    \nplt.tight_layout()\nplt.show()","86de122f":"#models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso, Ridge\n\n#hyperparameter tuning\nimport optuna\noptuna.logging.set_verbosity(0)\n\n#other\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, train_test_split\n","8ad7f793":"#spliting data_train \nX = data_train.drop(['SalePrice'], axis=1)\ny = data_train['SalePrice']\n\n#spliting data into 5 fold \ncv = KFold(n_splits=5, random_state=22, shuffle=True)","fb07260f":"def objective(trial):\n\n    #parameter range\n    param = {\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    \n    model = XGBRegressor(**param)  \n    \n    scores = cross_val_score(\n        model, X, y, cv=cv,\n        scoring=\"neg_mean_squared_error\"\n    )\n    \n    return scores.mean()\n\n#optimazing to maximize neg mean squared error in 100 trials\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\n\n#redefining xgb with the best trial parameters\nxgb = XGBRegressor(**study.best_trial.params)","30f26c61":"def objective(trial):\n\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n        'max_depth': trial.suggest_int('max_depth', 4, 50),\n        'min_samples_split': trial.suggest_int('min_samples_split', 1, 150),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n    }\n    \n    model = RandomForestRegressor(**param)  \n    \n    scores = cross_val_score(\n        model, X, y, cv=cv,\n        scoring=\"neg_mean_squared_error\"\n    )\n    \n    return scores.mean()\n\nrf_study = optuna.create_study(direction='maximize')\nrf_study.optimize(objective, n_trials=100)\n\nprint('Number of finished trials:', len(rf_study.trials))\nprint('Best trial:', rf_study.best_trial.params)\n\nrf = RandomForestRegressor(**rf_study.best_trial.params)","ac48f54c":"def tune(objective):\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    params = study.best_params\n    best_score = study.best_value\n    print(f\"Best score: {best_score}\\n\")\n    print(f\"Optimized parameters: {params}\\n\")\n    return params\n\ndef lasso_objective(trial):\n    _alpha = trial.suggest_float(\"alpha\", 0.0001, 1)\n    lasso = Lasso(alpha=_alpha, random_state=22, max_iter=5000)\n    scores = cross_val_score(\n        lasso, X, y, cv=cv,\n        scoring=\"neg_mean_squared_error\"\n    )\n    return scores.mean()\n\nlasso_params = tune(lasso_objective)\n\nlasso = make_pipeline(RobustScaler(), Lasso(**lasso_params))","d3613080":"ridge = make_pipeline(RobustScaler(), Ridge())\n\ndef tune(objective):\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    params = study.best_params\n    best_score = study.best_value\n    print(f\"Best score: {best_score}\\n\")\n    print(f\"Optimized parameters: {params}\\n\")\n    return params\n\ndef ridge_objective(trial):\n    _alpha = trial.suggest_float(\"alpha\", 0.1, 20)\n    ridge = Ridge(alpha=_alpha)\n    scores = cross_val_score(\n        ridge, X, y, cv=cv,\n        scoring=\"neg_root_mean_squared_error\"\n    )\n    return scores.mean()\n\nridge_params = tune(ridge_objective)\n\nridge = Ridge(**ridge_params)","2b028a7d":"#cross valitdation scores after tuning\ncv = KFold(n_splits=5, random_state=22, shuffle=True)\n\nprint(\"Scores after tuning:\")\nfor model in [xgb, rf, lasso, ridge]:\n    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=None)\n    mean_score = np.sqrt(np.mean(np.absolute(scores)))\n    print(type(model).__name__, mean_score.round(4))","ce1d223c":"#xbgoost\nxgb = XGBRegressor(**study.best_trial.params)\nxgb.fit(X, y)\npred_xgb = np.expm1(xgb.predict(data_test))\n\n#lasso\nlasso.fit(X, y)\npred_lasso = np.expm1(lasso.predict(data_test))\n\n#predicition based on both models\npred = 0.25*pred_lasso + 0.75*pred_xgb","8bc1ec53":"#creating and exporting results data frame\nResults = pd.DataFrame(pred, columns=['SalePrice'])\nResults['Id'] = Results.index + 1461\nResults = Results[['Id', 'SalePrice']]\n\nprint(Results.head())\n\nResults.to_csv('results_houses.csv', index=False)","77d672ec":"# House Prices","e2aa1bbe":"# Data Analysis","7eb3f835":"#### Transforming some numerical feature that are not continuous into categrorical features.","f8e7751a":"### Categorical features exploration","28f31307":"### Optuna Hyperparameter Tuning","85183d3c":"# Feature Engennering","228e7ea4":"# Modeling","d8a3369a":"Log transformation has reduced skew in all our features","dfcf80ff":"### Ridge","38fb1c3f":"We can notice two big outliers from these charts. We will remove them","f2071452":"## Missing values","4ae78ef4":"#### Laso","756a6c7f":"From the histogram we see that our taget feature is not normally distibuted. This has to be corrected in order to use linear based models. We will try using log transformation to do so.","e8523730":"# Data Normalization","2b7a02bf":"#### XGBoost","a5da60a3":"### Numerical features exploration","cb184c72":"### Finalizing our Model","b358c64b":"### Target Feature Exploration","068d27a9":"# Introduction","8f374b4b":"### Encoding Categorical Features","fd69136a":"#### Random Forest","6f7be065":"#### Kaggle problem description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n#### The aim of the project\nThe main aim of this project is to build a model to predict the house prices based on given features in the dataset. The model are evaluated based od RMSE of logs of predicetd and actual sale prices. This ensures that errors in predictiong cheap and expensive houses will affect our score more equally.\n\nWe will do the following steps:  \n- exploratory data analysis\n- imputing missing values                                                                                                    \n- feature engineering                                                                                                          \n- data normalization                                                                                                        \n- optuna model optimization (XGBoost, RandomForest, Lasso, Ridge)","16856ee8":"### Importing libraries","adad61f9":"### Data Colerration","c2e315b3":"#### Adding more features","98620e00":"In order to get good results from linear models we have to do data normalizations."}}