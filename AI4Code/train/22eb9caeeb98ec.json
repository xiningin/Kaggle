{"cell_type":{"a683ba40":"code","5a4c7af5":"code","08f0dbaa":"code","011cc78e":"code","9b4fb1c9":"code","797b8e43":"code","a1a78f59":"code","d8ff3ebe":"code","3f521536":"code","d2cc943d":"code","305af517":"code","50b8519e":"code","5e0bfee3":"code","5bbf1de9":"code","d9048ead":"code","afaee423":"code","c57d520b":"code","50e2160c":"code","57ba0c59":"code","7d7e3d7c":"code","79793ed8":"code","8b556130":"code","de63fa49":"code","40d0a845":"markdown","2135cd45":"markdown","05a0e403":"markdown","bd625605":"markdown","fd5c4ac4":"markdown","b4cb98ad":"markdown","ada006d1":"markdown","25474f3f":"markdown"},"source":{"a683ba40":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport warnings\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool\n\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn')\n","5a4c7af5":"df_train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ndf_train.head()","08f0dbaa":"df_test = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\ndf_test.head()","011cc78e":"df_train_label = df_train.drop('target', axis=1)\ndf_train_label['train-test'] = 1\ndf_test['train-test'] = 0","9b4fb1c9":"df = pd.concat([df_train_label, df_test])","797b8e43":"len(df)","a1a78f59":"df.describe()","d8ff3ebe":"df.info()","3f521536":"df.head()","d2cc943d":"numerical_col = [col for col in df.columns if pd.api.types.is_float_dtype(df[col])]\nplt.boxplot(df[numerical_col])\nplt.title('Numerical Boxplot', fontsize=24, fontweight='bold')\nplt.xlabel('Features');","305af517":"outlier_col = ['cont8', 'cont9', 'cont10']\nfor col in outlier_col:\n    q1 = df[col].quantile(0.25)\n    q3 = df[col].quantile(0.75)\n    IQR = q3 - q1\n    df[col] = np.where(df[col] < q1, (q1 - 1.5 * IQR), df[col])\n    df[col] = np.where(df[col] > q3, (q3 + 1.5 * IQR), df[col])","50b8519e":"numerical_col = [col for col in df.columns if pd.api.types.is_float_dtype(df[col])]\nplt.boxplot(df[numerical_col])\nplt.title('Numerical Boxplot', fontsize=24, fontweight='bold')\nplt.xlabel('Features');","5e0bfee3":"df.head()","5bbf1de9":"train_data = df[df['train-test'] == 1]\ntest_data = df[df['train-test'] == 0]","d9048ead":"cat_features = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18']\ntrain_data.drop(['id', 'train-test'], axis=1, inplace=True)\ntrain_pool = Pool(train_data, df_train['target'], cat_features)\ntest_data.drop(['id', 'train-test'], axis=1, inplace=True)\ntest_pool = Pool(test_data, cat_features=cat_features)","afaee423":"train_pool.get_label()","c57d520b":"np.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(train_data, df_train['target'], test_size=0.2)\nlen(X_train), len(y_train)","50e2160c":"test_X_pool = Pool(X_test, y_test, cat_features=cat_features)","57ba0c59":"cat_model = CatBoostClassifier()\ncat_model.randomized_search(grid, train_pool)","7d7e3d7c":"cat_model.score(test_X_pool)","79793ed8":"y_preds = cat_model.predict_proba(test_pool)","8b556130":"submission = pd.DataFrame(y_preds[:, 1], columns=['target'])\nsubmission.index = df_test['id']\nsubmission.to_csv('.\/submission-final.csv')","de63fa49":"submission.head()","40d0a845":"We have outliers in the columns `cont8`, `cont9` and `cont10`. So, now we need to hangle these outliers using the percentile technique.","2135cd45":"# Load the Dataset\nIn this section, we load our useful libraries and load the dataset into the notebook.","05a0e403":"From this, we can say that we don't have missing data in our dataset. But there are some categorical feature present in our dataset. To make them in numerical format we need to perfrom some encoding technique to get rid from that. But before that we perfrom EDA to find out the error and where our dataset is unbalance.","bd625605":"## Outliers","fd5c4ac4":"# Tabular Playground Series - Mar 2021\nIn this notebook we work out the binary dataset and create the machine learning classification model. The dataset contains the 31 different features and a target variable.","b4cb98ad":"# Dataset Statistics\nIn this section, we find out the basic statistic values from the dataset to understand the dataset and find out the different normalization factors like outliers, skewness, etc.","ada006d1":"Now, we dont have any outlier present in our numerical dataset. Lets check if there is any skew present in the numerical dataset or not.","25474f3f":"# Exploaratory Data Analysis\nIn this section, we perform the different techniques for the analysis of the data and find the patterns on the basis on correlation and k-neighbours. And also find the skewness and outliers present in the dataset.\n\n\n`TODO`\n* find the outliers. \u2705\n* find the skewness.\n* find the unbalance data using the target variable.\n* find the distint categorical value use in the dataset.\n* find the correleation between the dataset features.\n* find the k-neighbours in the dataset using the distinct features in the dataset."}}