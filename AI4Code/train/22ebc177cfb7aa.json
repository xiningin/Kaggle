{"cell_type":{"3c949a67":"code","03789b1a":"code","926378e2":"code","cb1a0987":"code","8cd9e867":"code","47ed5d1a":"code","f33de6d4":"code","8b08798c":"code","96b6238d":"code","c33a815e":"code","add17814":"code","d1561786":"code","ce6ebc38":"code","849cfb8e":"code","54c90c52":"code","a65078ef":"code","e310e83a":"code","c49ae3b2":"code","984cbb6c":"code","017ecdd3":"code","5ff8915a":"code","d4cc7157":"code","7a54030e":"code","a9bb4692":"code","67437a1d":"code","9a4efddd":"code","a6cd83d2":"code","351033ff":"code","68df0124":"code","af3b94af":"code","f5957ca8":"markdown","aacd5bde":"markdown","1571b76d":"markdown","81a19a48":"markdown","2be322f4":"markdown","f90c0d84":"markdown","1c20be8e":"markdown","44b45d6c":"markdown","d84d8de5":"markdown","e63c793c":"markdown","5aef8592":"markdown","607fce94":"markdown","53c19454":"markdown","20808d08":"markdown","10163660":"markdown","333ea11e":"markdown","d4f1002f":"markdown","49d27983":"markdown","eb6e4fec":"markdown","abc2e232":"markdown","44126805":"markdown","14554e9e":"markdown","1789fdbc":"markdown","a24bcfb0":"markdown"},"source":{"3c949a67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03789b1a":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport unidecode\nimport nltk\n\nfrom tensorflow import keras\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\npd.options.display.max_rows = 300\npd.options.display.max_columns = 300","926378e2":"dataset = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv')\ntestset = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv')","cb1a0987":"dataset.head()","8cd9e867":"dataset.shape","47ed5d1a":"dataset.info()","f33de6d4":"dataset['Sentiment'].unique()","8b08798c":"dataset.Location.nunique()","96b6238d":"dataset.isnull().sum()","c33a815e":"dataset = dataset.drop(columns='Location')","add17814":"dataset.head()","d1561786":"dataset.ScreenName.nunique()","ce6ebc38":"dataset.UserName.nunique()","849cfb8e":"dataset['label'] = dataset.Sentiment.factorize()[0]","54c90c52":"dataset.head()","a65078ef":"target_category = dataset['Sentiment'].unique()\ntarget_category","e310e83a":"dataset.groupby('Sentiment').label.count().sort_values(ascending = False)","c49ae3b2":"dataset.groupby(\"Sentiment\").Sentiment.count().plot.bar(ylim=0)","984cbb6c":"dataset.Sentiment.value_counts().plot(kind='pie', y='label',figsize=(10,8),autopct='%1.1f%%')\nplt.show()","017ecdd3":"tweets = dataset.OriginalTweet\ntweets.head(10)","5ff8915a":"def processing(text): \n    \n    \n#tokenization using keras text to word sequence tokenizer\n    tokenized_text = text_to_word_sequence(text)\n   \n        \n#stop word removal using remove_stopwords from gensim\n    text = ' '.join(tokenized_text)\n    text = text.replace(\"'\", \"\")\n    stop_word_removed_text = remove_stopwords(text)\n    \n        \n#remove numbers\n    number_removed_text = new_string = ''.join(filter(lambda x: not x.isdigit(), stop_word_removed_text))\n   \n        \n#remove extra white spaces\n    extra_whitespace_removed = word_tokenize(number_removed_text)\n    extra_whitespace_removed = number_removed_text.split()\n    \n        \n    extra_whitespace_removed = ' '.join(extra_whitespace_removed)\n    \n        \n#Convert Accented Characters(\u00fb -> u)\n    accented_removed_text = unidecode.unidecode(extra_whitespace_removed)\n  \n        \n#lemmatization\n    lemmatizer = WordNetLemmatizer()\n\n    def get_wordnet_pos(word):\n        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n        tag = nltk.pos_tag([word])[0][1][0].upper()\n        tag_dict = {\"J\": wordnet.ADJ,\n                    \"N\": wordnet.NOUN,\n                    \"V\": wordnet.VERB,\n                    \"R\": wordnet.ADV}\n\n        return tag_dict.get(tag, wordnet.NOUN)\n\n    lem_input = nltk.word_tokenize(accented_removed_text)\n    lem_text= ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in lem_input])\n    \n       \n#stemming \n    stemmer= PorterStemmer()\n\n    stem_input= nltk.word_tokenize(lem_text)\n    stem_text=' '.join([stemmer.stem(word) for word in stem_input])\n   \n        \n#remove single letters\n    preprocessed_text = ' '.join( [w for w in stem_text.split() if len(w)>1] )\n    \n        \n    return preprocessed_text\n        \n","d4cc7157":"dataset['OriginalTweet']=dataset['OriginalTweet'].apply(processing)  ","7a54030e":"tweets = dataset['OriginalTweet']\ntweets.head()","a9bb4692":"sentiment = dataset.Sentiment","67437a1d":"X_train, X_test, Y_train, Y_test = train_test_split(tweets,sentiment, test_size = 0.3, random_state = 60,shuffle=True)\n\nprint(len(X_train))\nprint(len(X_test))","9a4efddd":"sgd = Pipeline([('tfidf', TfidfVectorizer()),\n                ('sgd', SGDClassifier()),\n               ])\n\nsgd.fit(X_train, Y_train)\n\ntest_predict = sgd.predict(X_test)\n\ntrain_accuracy = round(sgd.score(X_train,Y_train)*100)\ntest_accuracy =round(accuracy_score(test_predict, Y_test)*100)\n\nprint(\"SGD Train Accuracy Score : {}% \".format(train_accuracy ))\nprint(\"SGD Test Accuracy Score  : {}% \".format(test_accuracy ))\nprint()\nprint(classification_report(test_predict, Y_test, target_names=target_category))","a6cd83d2":"testset.head()","351033ff":"testset['OriginalTweet'] = testset['OriginalTweet'].apply(processing)\n\n\ntweet = testset['OriginalTweet']\ny_predict = sgd.predict(tweet)\n","68df0124":"test_sentiments = testset['Sentiment']","af3b94af":"test_accuracy =round(accuracy_score(test_sentiments, y_predict)*100)\nprint(\"SGD Classifier Test Accuracy Score  : {}% \".format(test_accuracy ))","f5957ca8":"**Now lets see about null values**","aacd5bde":"# SGD Classifier","1571b76d":"This Tweet set has lot of special charaters and unwanted stuff for data training. So lets do some data preprocessing","81a19a48":"# Data Visualaization","2be322f4":"Which dataset.shape describe how our dataset is. (Number of rows, Number of columns). WHich is there are 41157 data rows and 6 columns of features","f90c0d84":"# Split test\/train sets","1c20be8e":"Lets now split the dataset into train and test sets baesd on 0.3 ratio which is 70% of data for training purpose and 30% of data for testing purpose. You can use 80%,20% ratio as well.  ","44b45d6c":"Let's check what we can identify from ScreenName and UserName feature","d84d8de5":"# Read Dataset using pandas read_csv ","e63c793c":"# Test the data set with the sgd model","5aef8592":"# Data Preprocessing","607fce94":"# Test Set","53c19454":"**Since this is for the begginers I have only used SGD Classifier. I'll soon come with a LSTM model for this one. Hope you learned something. Please do upvote if you learned anything and leave a feedback. Good Luck!**","20808d08":"There are 8590 null values in Location. Since location is categorical we can refill the null values with the most used feature value in that column. But since 8590 is a huge amount and if we fill those values our dataset will be highly biased towards that. So I will drop Loacation feature from the data base  ","10163660":"# Check Accuracy","333ea11e":"Lets take a look at Original Tweet Feature","d4f1002f":"Seems like there are 12220 different locations.","49d27983":"# Lets Convert Sentiments in to factorial values","eb6e4fec":"**Unique Values Of Sentiment Feature which shows our labels to classify**","abc2e232":"Lets now see how our dataset looks like and identify the important features","44126805":"Both ScreenName and UserName have 41157 unique values. ","14554e9e":"This will rename the Sentiments into numbers\n* 0 - Neutral\n* 1 - Positive\n* 2 - Extremely Negative\n* 3 - Negative \n* 4 - Extremely Positive\n\n","1789fdbc":"# About Dataset","a24bcfb0":"Let's train our model with SGD Classifier. For this I have used a pipeline with Tf-Idf Vectorizor which basically do the vectorization part. It will convert word in our preprocessed datset into a matrix of TF-IDF features. Based on those features the model will be trained."}}