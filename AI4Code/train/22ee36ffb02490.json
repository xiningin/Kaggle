{"cell_type":{"34920d5a":"code","5c4e67ec":"code","21c56b94":"code","82661fa7":"code","92fc7922":"code","ea7b6e05":"code","465a6711":"code","5db7b759":"code","c73c0081":"code","928a42a1":"code","acec1ff4":"code","6d79f775":"code","0f051496":"code","1d61541c":"code","2fcc1038":"code","219ad23c":"code","4017a927":"code","2117b630":"code","190954b5":"code","600bf966":"code","3977ebb9":"code","7446b1f1":"code","9c5d67db":"code","4ba59dbe":"code","350f1288":"code","e3b2512f":"code","2cebe3fb":"code","637d22b7":"code","6ff7d041":"code","bdac9d0b":"code","1bdb3475":"code","df53dfcb":"code","c816bf40":"code","1a3d275f":"markdown","b91cf923":"markdown","5a110881":"markdown","a6e1ac86":"markdown","2bb01715":"markdown","531dae40":"markdown","a938c250":"markdown","ce825c61":"markdown","4b06ee7a":"markdown","807861be":"markdown","0ba286b1":"markdown","b85fac8b":"markdown","6b3cdeff":"markdown","bbffd36b":"markdown","b568d340":"markdown","6c3b331b":"markdown"},"source":{"34920d5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c4e67ec":"import plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,recall_score,accuracy_score","21c56b94":"df=pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndf['age_group']=df['age'].apply(lambda x: str(x-(x%10))+' to '+str(x+10-(x%10)))\ndf.head()","82661fa7":"temp=pd.value_counts(df['sex'])\npx.pie(temp,values='sex',names=['Male','Female'],title='Gender')","92fc7922":"temp=pd.value_counts(df['cp'])\npx.pie(temp,values='cp',names=['typical angina','non-anginal pain','non-typical angina','asymptomatic'],title='Kind of chest pain')","ea7b6e05":"temp=pd.value_counts(df['output'])\npx.pie(temp,values='output',names=['More chances','Less chances'],title='Chances of heart attack')","465a6711":"px.histogram(df,x='age',histnorm='probability',marginal='violin',nbins=20)","5db7b759":"px.histogram(df,x='trtbps',histnorm='probability',marginal='violin',nbins=20,opacity=0.8)","c73c0081":"px.histogram(df,x='chol',histnorm='probability',marginal='violin',nbins=50)","928a42a1":"temp=df.groupby(['cp','output']).agg('count').reset_index()[['cp','output','thall']]\ntemp['output'].replace({1:'High Chance of heart attack',0:'Low chance of heart attack'},inplace=True)\ntemp['cp'].replace({0:'typical angina',1:'atypical angina',2:'non-anginal pain',3:'asymptomatic'},inplace=True)\npx.bar(temp,x='cp',y='thall',facet_col='output',labels={'cp':'Kind of chest pain','thall':'count','output':'Chances of heart attack'})","acec1ff4":"temp=df.groupby(['age_group','output']).agg('count').reset_index()[['output','age_group','fbs']]\ntemp_age=df.groupby('age_group').agg('count').reset_index()[['age_group','fbs']]\ntemp=pd.merge(temp,temp_age,on='age_group')\ntemp['percentage_of_people']=temp['fbs_x']*100\/temp['fbs_y']\ntemp['output'].replace({1:'High Chance of heart attack',0:'Low chance of heart attack'},inplace=True)\npx.bar(temp,x='age_group',y='percentage_of_people',facet_row='output',color='fbs_y',category_orders={'age_group':['20 to 30','30 to 40','40 to 50',\n                                                                                 '50 to 60','60 to 70','70 to 80']},\n      labels={'fbs_y':'Count of people in age group'})","6d79f775":"temp=df.groupby(['restecg','output']).agg('count').reset_index()[['restecg','output','thall']]\ntemp['output'].replace({1:'High Chance of heart attack',0:'Low chance of heart attack'},inplace=True)\ntemp['restecg'].replace({0:'normal',1:' having ST-T wave abnormality',2:' left ventricular hypertrophy by Estes criteria'},inplace=True)\npx.bar(temp,x='restecg',y='thall',facet_row='output',labels={'restecg':'ECG classification','thall':'count','output':'Chances of heart attack'})","0f051496":"temp=df.groupby(['restecg','cp']).agg('count').reset_index()[['restecg','cp','thall']]\ntemp_ecg=df.groupby('cp').agg('count').reset_index()[['cp','thall']]\ntemp=pd.merge(temp,temp_ecg,on='cp')\ntemp['thall']=temp['thall_x']*100\/temp['thall_y']\ntemp['cp'].replace({0:'typical angina',1:'atypical angina',2:'non-anginal pain',3:'asymptomatic'},inplace=True)\ntemp['restecg'].replace({0:'normal',1:' having ST-T wave abnormality',2:' left ventricular hypertrophy by Estes criteria'},inplace=True)\npx.bar(temp,color='restecg',y='thall',x='cp',labels={'restecg':'ECG classification','thall':'Percentage_of_ECG','cp':'Kind of chest pain'},barmode='stack')","1d61541c":"df.columns","2fcc1038":"temp=df.groupby(['caa','output']).agg('count').reset_index()[['caa','output','thall']]\ntemp['output'].replace({1:'High Chance of heart attack',0:'Low chance of heart attack'},inplace=True)\npx.bar(temp,x='caa',y='thall',facet_row='output',labels={'ca':'Number of vessels','thall':'count','output':'Chances of heart attack'})","219ad23c":"#relationship with kind of chest pain\ncontingency=[[104,39],[9,41],[18,69],[7,16]]\nstat,p,dof,expected=chi2_contingency(contingency)\nprint('The p-value is ',p)","4017a927":"#relationship with the ecg result\ncontingency=[[79,68],[56,96],[3,1]]\nstat,p,dof,expected=chi2_contingency(contingency)\nprint('The p-value is ',p)","2117b630":"#reltionship with age_group\ncontingency=[[0,1],[4,11],[22,50],[60,65],[48,32],[4,6]]\nstat,p,dof,expected=chi2_contingency(contingency)\nprint('The p-value is ',p)","190954b5":"#reltionship with gender\ncontingency=[[24,72],[114,93]]\nstat,p,dof,expected=chi2_contingency(contingency)\nprint('The p-value is ',p)","600bf966":"temp","3977ebb9":"#reltionship with number of blood vessels\ncontingency=[[45,130],[44,21],[31,7],[17,3],[1,4]]\nstat,p,dof,expected=chi2_contingency(contingency)\nprint('The p-value is ',p)","7446b1f1":"temp=df.copy()\ntemp['output'].replace({1:'High Chance of heart attack',0:'Low chance of heart attack'},inplace=True)\ntemp['cp'].replace({0:'typical angina',1:'atypical angina',2:'non-anginal pain',3:'asymptomatic'},inplace=True)\ntemp['restecg'].replace({0:'normal',1:' having ST-T wave abnormality',2:' left ventricular hypertrophy by Estes criteria'},inplace=True)\npx.histogram(temp,x='chol',facet_row='output',histnorm='probability',labels={'chol':'Cholestrol level'})","9c5d67db":"px.histogram(temp,x='trtbps',facet_row='output',histnorm='probability',labels={'trtbps':'Resting blood pressure'},nbins=20)","4ba59dbe":"px.histogram(temp,x='thalachh',facet_row='output',histnorm='probability',labels={'thall':'Maximum heart rate '},nbins=20)","350f1288":"df=df[['restecg','cp','caa','thalachh','sex','output']]\ndf['thalachh']=df['thalachh']\/max(df['thalachh'])","e3b2512f":"#if age group is something you need to include. The model does not show any improvement by adding the age \n#hence I decided to not include it\n#age_dict={}\n#i=0\n#for x in ['20 to 30', '30 to 40', '40 to 50', '50 to 60', '60 to 70', '70 to 80']:\n #   age_dict[x]=i\n  #  i+=1\n#df['age_group']=df['age_group'].apply(lambda x: age_dict[x])","2cebe3fb":"x=df.drop('output',axis=1)\ny=df['output']\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=32)","637d22b7":"forest=RandomForestClassifier(max_depth=4,n_estimators=250)\nforest.fit(X_train,y_train)\nprint('The accuracy on the training set= ',forest.score(X_train,y_train)*100,'%')","6ff7d041":"y_train_pred=forest.predict(X_train)\nsns.heatmap(confusion_matrix(y_train,y_train_pred),annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('True labels')\nplt.xticks([0,1],['Predicted to be low risk','Predicted to be high risk'])\nplt.yticks([1,0],['Actually high risk','Actually low risk'])","bdac9d0b":"preds=forest.predict_proba(X_train)\nrecall=[]\naccuracy=[]\nfor thresh in range(0,100):\n    p=(preds[:,1]>(thresh\/100)).astype(int)\n    recall.append(recall_score(y_train,p))\n    accuracy.append(accuracy_score(y_train,p))\nfig=plt.figure(figsize=(10,6))\nsns.lineplot(x=[x\/100 for x in range(0,100)],y=recall,label='recall')\nsns.lineplot(x=[x\/100 for x in range(0,100)],y=accuracy,label='accuracy')\nsns.lineplot(x=[0.5,0.5],y=[0,1],label='50% threshold',markers='- -',color='red')\nplt.xlabel('Threshold')\nplt.legend()","1bdb3475":"thresh=0.5  #play with the threshold a bit for better results\ny_train_pred=forest.predict_proba(X_train)\ny_train_pred=(y_train_pred[:,1]>thresh).astype(int)\nsns.heatmap(confusion_matrix(y_train,y_train_pred),annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('True labels')\nplt.xticks([0,1],['Predicted to be low risk','Predicted to be high risk'])\nplt.yticks([1,0],['Actually high risk','Actually low risk'])\nprint('The accuracy on the training set after applying threshold at',thresh*100,'% probability is',accuracy_score(y_train,y_train_pred)*100,'%')","df53dfcb":"y_preds=forest.predict_proba(X_test)\ny_preds=(y_preds[:,1]>thresh).astype(int)\nprint(\"The accuracy on the test set after applying threshold for prediction at\",thresh*100,\"% probability is\",accuracy_score(y_test,y_preds)*100,'%')","c816bf40":"sns.heatmap(confusion_matrix(y_test,y_preds),annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('True labels')\nplt.xticks([0,1],['Predicted to be low risk','Predicted to be high risk'])\nplt.yticks([1,0],['Actually high risk','Actually low risk'])","1a3d275f":"# Distribution of variables","b91cf923":"**Observations**\n1. The maximum heart rate shows some correlation with the target variable. The people with higher chance of a heart attack have a slightly higher as compared to those with the low chance.","5a110881":"As seen here that the accuracy decreased a bit but there is much improvement in the recall.","a6e1ac86":"# Statistical test for categorical variables\nNow lets perform a statistical test to identify the relationship between the categorical variables and the risk of heart attack one after the other.The test is the Chi-squared test which requires a contingency table. Just like with the t-test it too uses the p-value to state whether there is a dependency between the variables or not. When the p-value is less than alpha there will be a dependency otherwise not.","2bb01715":"75% people from those having a low chance of heart attack have a typical angina while from people having higher chance of a heart attack 41% experience a non-anginal pain.","531dae40":"# Identifying relationship with continuous variables","a938c250":"**Observations**\n1. Most people with chest pain where diagnosed with typical angina.\n2. Most people fall in the range of 40 to 60.\n3. The normal range of Blood pressure is values less than 120 but in our data most values are above this threshold. And Blood pressure greater than 140 is considered high. 20% people have a high blood pressure in the dataset.\n4. a cholestrol of 200-240 is considered borderline and above that the cholestrol is considered high.  49.8% people in out data have values above this range.","ce825c61":"**Without any model tuning we are able to have a considerable accuracy in predicting the chances of heart attack.**","4b06ee7a":"**Observations**\n1. All the categorical variables seem to have an influence on the chances of heart attack.\n2. Age group is also highly correlated and hence we can drop the absolute age value and just keep the age group of the person.","807861be":"# Identifying relationships with categorical variables","0ba286b1":"**Description about the variables**\n1. exng- Excercise induced Angina. Is a condition where not enough blood is supplied to the walls of heart to pump the blood. It is caused due to excerice or any physcical or mental stress.\n2. cp- It describes the type of chest pain experienced. Whether it was typical, atypical angina or non anginal pain or unsymptomatic.\n3. trtbps- The resting blood pressure and heart attack have a direct relation. If the resting blood pressure is high then it means that the heart is working hard to pump the required blood and leads to increased chances of heart attack.\n4. chol- Cholestrol\n5. rest_ecg - The pattern observed in the ECG. It consists of three categories here in the dataset.\n            0:Normal  1: Abnormal pattern indicating the lack of blood flow to the heart (Increased chance of heart attack)   2: Pattern that indicates the thickening of the heart muscle of the left ventricle.(Increased chance of heart attack)\n6. fbs- Fasting Blood sugar. It is seen that people with higher blood sugar are at more risk of forming clots and hence at more risk of having a heart attack.","b85fac8b":"As we can see that at the 50% threshold the recall and the accuracy curves almost intersect. It is also noticeable that if we decrease the threshold to 0.4 there is not going to be much change in the accuracy but the recall is improving considerably. ","6b3cdeff":"The number of vessels seems to have a relationship with the risk of heart attack.","bbffd36b":"Bad news if you're in between 40 to 60 years. People in this age group have a higher chance of having an heart attack.","b568d340":"\nSome people with high risk are predicted to have a low risk. This is something that can be improved. The recall is what matters here, we do not mind a few people with less risk being predicted as high risk but if a high risk individual is predicted as low risk that is where the problem is. Recall should be used to set the threshold here and not precision or oraccuracy.","6c3b331b":"# Prediction\nThe dataset is balanced and hence we can directly split the data into training and test set and move on to the predictions.\n**We will only keep the variables that have show correlation with the target as seen above**"}}