{"cell_type":{"63f6b1b3":"code","a0e77edd":"code","d03e66bb":"code","6b639708":"code","51e8c141":"code","54ee6e02":"code","5dafb28e":"code","6d5ff2f9":"code","fdc885c8":"code","c1468c8f":"code","38addca9":"code","7dfd1eb8":"code","e58231af":"code","99d6d89e":"code","60219bd5":"code","0bb7dd34":"code","1b1e2821":"code","b92f74f5":"code","f851cce4":"code","de7bfcff":"code","65b9f4ed":"code","2f9191ca":"code","98d3790e":"code","7170006f":"code","72d06b5b":"code","6a838b48":"markdown","32536995":"markdown","9dcc0e47":"markdown","24a162aa":"markdown","49b5243b":"markdown","970c42db":"markdown","d8716964":"markdown","6ccb433a":"markdown","fe129a4f":"markdown","53a5412b":"markdown","465cd424":"markdown","5bb7ea87":"markdown","b59c69e7":"markdown","47019cf0":"markdown","15f166d8":"markdown"},"source":{"63f6b1b3":"import sys\n!curl -s https:\/\/course.fast.ai\/setup\/colab | bash\n!git clone https:\/\/github.com\/yabhi0807\/libml1.git \/kaggle\/tmp\/fastai # This is my repo with all the fastai(updated) libraries \nsys.path.append('\/kaggle\/tmp\/fastai')\n!mkdir \/kaggle\/tmp\/data\/\n!ln -s \/kaggle\/tmp\/* \/kaggle\/working\/","a0e77edd":"%matplotlib inline\nfrom fastai.learner import *","d03e66bb":"np.random.normal(3,3,5).std()","6b639708":"# Here we generate some fake data\ndef lin(a,b,x): return a*x+b\n\ndef gen_fake_data(n, a, b):\n    x = s = np.random.uniform(0,1,n) \n    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n    return x, y\n\nx, y = gen_fake_data(50, 3., 8.)","51e8c141":"plt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\"); ","54ee6e02":"def mse(y_hat, y): return ((y_hat - y) ** 2).mean()","5dafb28e":"y_hat = lin(10,5,x)\nmse(y_hat, y)","6d5ff2f9":"def mse_loss(a, b, x, y): return mse(lin(a,b,x), y)","fdc885c8":"mse_loss(10, 5, x, y)","c1468c8f":"# generate some more data\nx, y = gen_fake_data(10000, 3., 8.)\nx.shape, y.shape","38addca9":"x,y = V(x),V(y)","7dfd1eb8":"# Create random weights a and b, and wrap them in Variables.\na = V(np.random.randn(1), requires_grad=True)\nb = V(np.random.randn(1), requires_grad=True)\na,b","e58231af":"learning_rate = 1e-3\nfor t in range(10000):\n    # Forward pass: compute predicted y using operations on Variables\n    loss = mse_loss(a,b,x,y)\n    if t % 1000 == 0: print(loss.data)\n    \n    # Computes the gradient of loss with respect to all Variables with requires_grad=True.\n    # After this call a.grad and b.grad will be Variables holding the gradient\n    # of the loss with respect to a and b respectively\n    loss.backward()\n    \n    # Update a and b using gradient descent; a.data and b.data are Tensors,\n    # a.grad and b.grad are Variables and a.grad.data and b.grad.data are Tensors\n    a.data -= learning_rate * a.grad.data\n    b.data -= learning_rate * b.grad.data\n    \n    # Zero the gradients\n    a.grad.data.zero_()\n    b.grad.data.zero_()    ","99d6d89e":"a, b","60219bd5":"def gen_fake_data2(n, a, b):\n    x = s = np.random.uniform(0,1,n) \n    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n    return x, np.where(y>10, 1, 0).astype(np.float32)","0bb7dd34":"x,y = gen_fake_data2(10000, 3., 8.)\nx,y = V(x),V(y)","1b1e2821":"def nll(y_hat, y):\n    y_hat = torch.clamp(y_hat, 1e-5, 1-1e-5)\n    return (y*y_hat.log() + (1-y)*(1-y_hat).log()).mean()","b92f74f5":"a = V(np.random.randn(1), requires_grad=True)\nb = V(np.random.randn(1), requires_grad=True)","f851cce4":"x, y","de7bfcff":"learning_rate = 1e-2\nfor t in range(3000):\n    p = (-lin(a,b,x)).exp()\n    y_hat = 1\/(1+p)\n    loss = nll(y_hat,y)\n    if t % 1000 == 0:\n        print(loss.data, np.mean(to_np(y)==(to_np(y_hat)>0.5)))\n#         print(y_hat.data.exp(), y)\n    \n    loss.backward()\n    a.data -= learning_rate * a.grad.data\n    b.data -= learning_rate * b.grad.data\n    a.grad.data.zero_()\n    b.grad.data.zero_()","65b9f4ed":"from matplotlib import rcParams, animation, rc\nfrom ipywidgets import interact, interactive, fixed\nfrom ipywidgets.widgets import *\nrc('animation', html='html5')\nrcParams['figure.figsize'] = 3, 3","2f9191ca":"x, y = gen_fake_data(50, 3., 8.)","98d3790e":"a_guess,b_guess = -1., 1.\nmse_loss(a_guess, b_guess, x, y)","7170006f":"lr=0.01\ndef upd():\n    global a_guess, b_guess\n    y_pred = lin(a_guess, b_guess, x)\n    dydb = 2 * (y_pred - y)\n    dyda = x*dydb\n    a_guess -= lr*dyda.mean()\n    b_guess -= lr*dydb.mean()","72d06b5b":"fig = plt.figure(dpi=100, figsize=(5, 4))\nplt.scatter(x,y)\nline, = plt.plot(x,lin(a_guess,b_guess,x))\nplt.close()\n\ndef animate(i):\n    line.set_ydata(lin(a_guess,b_guess,x))\n    for i in range(30): upd()\n    return line,\n\nani = animation.FuncAnimation(fig, animate, np.arange(0, 20), interval=100)\nani","6a838b48":"Nearly all of deep learning is powered by one very important algorithm: **stochastic gradient descent (SGD)**. SGD can be seeing as an approximation of **gradient descent** (GD). In GD you have to run through *all* the samples in your training set to do a single itaration. In SGD you use *only one* or *a subset*  of training samples to do the update for a parameter in a particular iteration. The subset use in every iteration is called a **batch** or **minibatch**.","32536995":"#  Linear Regression problem","9dcc0e47":"# Gradient descent with numpy","24a162aa":"For a fixed dataset $x$ and $y$ `mse_loss(a,b)` is a function of $a$ and $b$. We would like to find the values of $a$ and $b$ that minimize that function.\n\n**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.","49b5243b":"Nearly all of deep learning is powered by one very important algorithm: **stochastic gradient descent (SGD)**. SGD can be seeing as an approximation of **gradient descent** (GD). In GD you have to run through *all* the samples in your training set to do a single itaration. In SGD you use *only one* or *a subset*  of training samples to do the update for a parameter in a particular iteration. The subset use in every iteration is called a **batch** or **minibatch**.","970c42db":"The goal of linear regression is to fit a line to a set of points.","d8716964":"In this part of the lecture we explain Stochastic Gradient Descent (SGD) which is an **optimization** method commonly used in neural networks. We will illustrate the concepts with concrete examples.","6ccb433a":"For a fixed dataset $x$ and $y$ `mse_loss(a,b)` is a function of $a$ and $b$. We would like to find the values of $a$ and $b$ that minimize that function.\n\n**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.","fe129a4f":"# Gradient Descent","53a5412b":"# Table of Contents\n <p><div class=\"lev1 toc-item\"><a href=\"#Linear-Regression-problem\" data-toc-modified-id=\"Linear-Regression-problem-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Linear Regression problem<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Gradient Descent<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Gradient-Descent---Classification\" data-toc-modified-id=\"Gradient-Descent---Classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Gradient Descent - Classification<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Gradient-descent-with-numpy\" data-toc-modified-id=\"Gradient-descent-with-numpy-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Gradient descent with numpy<\/a><\/div>","465cd424":"Suppose we believe $a = 10$ and $b = 5$ then we can compute `y_hat` which is our *prediction* and then compute our error.","5bb7ea87":"So far we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for $a$ and $b$? How do we find the best *fitting* linear regression.","b59c69e7":"*Importing needed Libraries*","47019cf0":"You want to find **parameters** (weights) $a$ and $b$ such that you minimize the *error* between the points and the line $a\\cdot x + b$. Note that here $a$ and $b$ are unknown. For a regression problem the most common *error function* or *loss function* is the **mean squared error**. ","15f166d8":"# Gradient Descent - Classification"}}