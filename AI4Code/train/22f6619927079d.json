{"cell_type":{"eb8b8111":"code","32e93e39":"code","6edd1312":"code","cf68531d":"code","81626293":"code","feb02000":"code","fe87845a":"code","6c4e4d1e":"code","6fde9aab":"code","e071bbef":"code","4fd5f00b":"markdown","cf461a33":"markdown","e9917378":"markdown","6730a378":"markdown","fcd5dcf8":"markdown"},"source":{"eb8b8111":"import numpy as np \nimport pandas as pd \nimport os\nfrom pandas.io.json import json_normalize,loads\nimport time\n\ndef load_json(d):\n    return loads(d)\n\ndef json_into_dataframe(dataframe,column):\n    return json_normalize(dataframe[column].apply(load_json).tolist()).add_prefix(column +'.')\n        \ndef open_flat(filepath,columns):\n    counter = 0 \n    data= pd.read_csv(filepath, low_memory = False)\n    for column in columns :\n        print ('Unpacking ' + column)\n        temp = json_into_dataframe(dataframe = data, column = column)\n        for item in temp.columns:\n            if len(temp[item].unique()) == 1: # if a column has the same value for all rows is not significant\n                temp.drop(item, axis = 1, inplace = True)\n                print ('column '+ item + ' was dropped')\n                counter += 1\n        data = pd.concat([data,temp], axis = 1)\n        data.drop([column],inplace = True, axis = 1)\n    print ('Columns dropped :',counter)\n    return data\ntrain_path = \"..\/input\/train.csv\"\ntest_path = \"..\/input\/test.csv\"\ncolumns = ['totals','device','geoNetwork','trafficSource']","32e93e39":"\nt0 = time.time()\ntrain = open_flat(filepath = train_path ,columns = columns)\nt1 = time.time()\nprint ('time to load train set', t1-t0)","6edd1312":"t0 = time.time()\ntest = open_flat(filepath = test_path ,columns = columns)\nt1 = time.time()\nprint ('time to load test set ', t1-t0)","cf68531d":"print(train.shape)\nprint(test.shape)","81626293":"for column in train.columns:\n    if column not in test.columns:\n        print(column)","feb02000":"train.drop('trafficSource.campaignCode' , axis = 1 , inplace = True)\nprint(train.shape)\nprint(test.shape)","fe87845a":"t0 = time.time()\nwriter = pd.ExcelWriter('train.xlsx')\ntrain.to_excel(writer,'Sheet1')\nwriter.save()\nt1 = time.time()\nprint ('time to save train dataset to excel ', (t1-t0)\/60.0 , ' mins') ","6c4e4d1e":"t0 = time.time()\nwriter = pd.ExcelWriter('test.xlsx')\ntest.to_excel(writer,'Sheet1')\nwriter.save()\nt1 = time.time()\nprint ('time to save test dataset to excel ', (t1-t0)\/60.0 , ' mins') ","6fde9aab":"print ('File size of flat train set :' + str(((os.path.getsize(\"train.xlsx\")\/1024)\/1024)) + ' MB')\nprint ('File size of original train set :' + str(((os.path.getsize(\"..\/input\/train.csv\")\/1024)\/1024)) + ' MB')","e071bbef":"print ('File size of flat test set :' + str(((os.path.getsize(\"test.xlsx\")\/1024)\/1024)) + ' MB')\nprint ('File size of original test set :' + str(((os.path.getsize(\"..\/input\/test.csv\")\/1024)\/1024)) + ' MB')","4fd5f00b":"Now we can save our DataFrames into new excel files to save some space","cf461a33":"It seems there is a column in the train set ( other than the target feature totals.transactionRevenue ) , that it's not in the test set and we have to get rid of it.","e9917378":"Let's have a look at the shape of the resulting train and test set","6730a378":"There many great kernels about flattening the JSON blobs, but we don't want to do this everytime we load the data. ","fcd5dcf8":"Let's see how much space we saved!"}}