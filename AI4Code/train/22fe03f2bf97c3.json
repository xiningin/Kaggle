{"cell_type":{"e4ea9efe":"code","b83c528f":"code","a3ee3a6b":"code","47ef0ff5":"code","38a41e9d":"code","160fc36d":"code","94ce13fc":"code","a7b4b0d8":"code","559ceffc":"code","f776916b":"code","c8fe9665":"code","78d47f07":"code","b4eb50aa":"code","f294239c":"code","f34d2fe2":"code","3326e3d0":"code","adb892c4":"code","80f82b6b":"code","c38a6786":"code","06b16e7b":"code","7d9bafa7":"code","eb0065db":"code","31124de0":"code","a8bc9fa5":"code","8a8b9cc7":"code","70e7ea39":"code","8fa4b185":"code","86af719d":"code","ace2f127":"code","4b2d11fd":"code","a22a83a2":"code","1804b609":"code","88897fed":"code","d242012d":"code","d8feadea":"code","2786fe1a":"code","da9f0e60":"code","a10842ee":"code","744f33ec":"code","9ed077ce":"code","45ece2b2":"code","5615503f":"code","4c4651c0":"code","7452537c":"code","a1621937":"code","a8e5043f":"code","f57b5805":"code","982c5614":"markdown","70f96822":"markdown","d32bcce7":"markdown","3dcf94e1":"markdown","f97b6426":"markdown","321f3187":"markdown","3a1418db":"markdown","958888da":"markdown","ba37f549":"markdown","0469a3b2":"markdown","10d0d80d":"markdown","90ffa58a":"markdown","0c38e1ae":"markdown","d79422df":"markdown","bc36450c":"markdown","27e72c7c":"markdown","13b4a8fe":"markdown","9880ce4c":"markdown","5629fa07":"markdown","3b741d45":"markdown","7c6dce84":"markdown","6d83be8e":"markdown"},"source":{"e4ea9efe":"#importing the required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","b83c528f":"#Get the dataset\ndataset = pd.read_csv(r'..\/input\/position-salaries-by-superdatascienceteam\/Position_Salaries.csv')","a3ee3a6b":"#Get a glimpse of the Dataset\ndataset.head()","47ef0ff5":"#Separating the independent and dependent features\n#Dependent feature\ny = np.asarray(dataset['Salary'].values.tolist()) \n\n# Independent Feature\nX = np.asarray(dataset['Level'].values.tolist())","38a41e9d":"# Reshaping the independent feature\nX = X.reshape(-1,1)","160fc36d":"#Reshaping the Dependent features\ny = y.reshape(len(y),1) # Changing the shape from (50,) to (50,1)","94ce13fc":"#Get the shapes of X and y\nprint(\"The shape of the independent fatures are \",X.shape)\nprint(\"The shape of the dependent fatures are \",y.shape)","a7b4b0d8":"# The method \"poly_features\" concatenates polynomials of independent feature to X\n# This is similar to PolynomialFeatures class from sklearn.preprocessing\ndef poly_features(features, X):\n  data = pd.DataFrame(np.zeros((X.shape[0],features)))\n  for i in range(1,features+1):\n    data.iloc[:,i-1] = (X**i).reshape(-1,1)\n  X_poly = np.array(data.values.tolist())\n  return X_poly","559ceffc":"# The method \"split_data\" splits the given dataset into trainset and testset\n# This is similar to the method \"train_test_split\" from \"sklearn.model_selection\"\ndef split_data(X,y,test_size=0.2,random_state=0):\n    np.random.seed(random_state)                  #set the seed for reproducible results\n    indices = np.random.permutation(len(X))       #shuffling the indices\n    data_test_size = int(X.shape[0] * test_size)  #Get the test size\n\n    #Separating the Independent and Dependent features into the Train and Test Set\n    train_indices = indices[data_test_size:]\n    test_indices = indices[:data_test_size]\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    return X_train, y_train, X_test, y_test","f776916b":"# The method pred_to_plot returns predictions on given values and helps in\n# better visualization\ndef pred_to_plot(W_trained, X):\n  prediction_values = list()\n  for i in range(X.shape[0]):\n    value = regressor.predict(W_trained,X[i])\n    prediction_values.append(value)\n  return prediction_values","c8fe9665":"class polynomialRegression():\n\n  def __init__(self):\n    #No instance Variables required\n    pass\n\n  def forward(self,X,y,W):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Features\n    y (array) : Dependent Features\/ Target Variable\n    W (array) : Weights \n\n    Returns:\n    loss (float) : Calculated Sqaured Error Loss for y and y_pred\n    y_pred (array) : Predicted Target Variable\n    \"\"\"\n    y_pred = sum(W * X)\n    loss = ((y_pred-y)**2)\/2    #Loss = Squared Error, we introduce 1\/2 for ease in the calculation\n    return loss, y_pred\n\n  def updateWeights(self,X,y_pred,y_true,W,alpha,index):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Features\n    y_pred (array) : Predicted Target Variable\n    y_true (array) : Dependent Features\/ Target Variable\n    W (array) : Weights\n    alpha (float) : learning rate\n    index (int) : Index to fetch the corresponding values of W, X and y \n\n    Returns:\n    W (array) : Update Values of Weight\n    \"\"\"\n    for i in range(X.shape[1]):\n      #alpha = learning rate, rest of the RHS is derivative of loss function\n      W[i] -= (alpha * (y_pred-y_true[index])*X[index][i]) \n    return W\n\n  def train(self, X, y, epochs=10, alpha=0.001, random_state=0):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Feature\n    y (array) : Dependent Features\/ Target Variable\n    epochs (int) : Number of epochs for training, default value is 10\n    alpha (float) : learning rate, default value is 0.001\n\n    Returns:\n    y_pred (array) : Predicted Target Variable\n    loss (float) : Calculated Sqaured Error Loss for y and y_pred\n    \"\"\"\n\n    num_rows = X.shape[0] #Number of Rows\n    num_cols = X.shape[1] #Number of Columns \n    W = np.random.randn(1,num_cols) \/ np.sqrt(num_rows) #Weight Initialization\n\n    #Calculating Loss and Updating Weights\n    train_loss = []\n    num_epochs = []\n    train_indices = [i for i in range(X.shape[0])]\n    for j in range(epochs):\n      cost=0\n      np.random.seed(random_state)\n      np.random.shuffle(train_indices)\n      for i in train_indices:\n        loss, y_pred = self.forward(X[i],y[i],W[0])\n        cost+=loss\n        W[0] = self.updateWeights(X,y_pred,y,W[0],alpha,i)\n      train_loss.append(cost)\n      num_epochs.append(j)\n    return W[0], train_loss, num_epochs\n\n  def test(self, X_test, y_test, W_trained):\n    \"\"\"\n    Parameters:\n    X_test (array) : Independent Features from the Test Set\n    y_test (array) : Dependent Features\/ Target Variable from the Test Set\n    W_trained (array) : Trained Weights\n    test_indices (list) : Index to fetch the corresponding values of W_trained,\n                          X_test and y_test \n\n    Returns:\n    test_pred (list) : Predicted Target Variable\n    test_loss (list) : Calculated Sqaured Error Loss for y and y_pred\n    \"\"\"\n    test_pred = []\n    test_loss = []\n    test_indices = [i for i in range(X_test.shape[0])]\n    for i in test_indices:\n        loss, y_test_pred = self.forward(X_test[i], W_trained, y_test[i])\n        test_pred.append(y_test_pred)\n        test_loss.append(loss)\n    return test_pred, test_loss\n    \n\n  def predict(self, W_trained, X_sample):\n    prediction = sum(W_trained * X_sample)\n    return prediction\n\n  def plotLoss(self, loss, epochs):\n    \"\"\"\n    Parameters:\n    loss (list) : Calculated Sqaured Error Loss for y and y_pred\n    epochs (list): Number of Epochs\n\n    Returns: None\n    Plots a graph of Loss vs Epochs\n    \"\"\"\n    plt.plot(epochs, loss)\n    plt.xlabel('Number of Epochs')\n    plt.ylabel('Loss')\n    plt.title('Plot Loss')\n    plt.show()\n  \n","78d47f07":"# Independent Feature\nX = np.asarray(dataset['Level'].values.tolist())","b4eb50aa":"# Reshaping the independent feature\nX = X.reshape(-1,1)","f294239c":"X","f34d2fe2":"X = poly_features(2,X)","3326e3d0":"#Adding the feature X0 = 1, so we have the equation: y =  W0 + (W1 * X1) + (W2 * (X1**2))\nX = np.concatenate((X,np.ones((10,1))), axis = 1)","adb892c4":"X","80f82b6b":"y","c38a6786":"#Splitting the dataset\nX_train, y_train, X_test, y_test = split_data(X,y)","06b16e7b":"#declaring the \"regressor\" as an object of the class polynomialRegression\nregressor = polynomialRegression()","7d9bafa7":"#Training \nW_trained, train_loss, num_epochs = regressor.train(X_train, y_train, epochs=200, alpha=0.00001)","eb0065db":"#Testing on the Test Dataset\ntest_pred, test_loss = regressor.test(X_test, y_test, W_trained)","31124de0":"pred_plot = pred_to_plot(W_trained,X)","a8bc9fa5":"plt.scatter(X[:,0], y, color = 'red')\nplt.plot(X[:,0], pred_plot, color = 'blue')\nplt.title('Naive Polynomial Regression (N = 2)')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.show()","8a8b9cc7":"# Independent Feature\nX = np.asarray(dataset['Level'].values.tolist())","70e7ea39":"# Reshaping the independent feature\nX = X.reshape(-1,1)","8fa4b185":"# Constructing the polynomials of our Independent features \nX_poly = poly_features(4,X)","86af719d":"#Adding the feature X0 = 1, so we have the equation: y =  W0 + (W1 * X1) + (W2 * (X1**2))\nX_poly = np.concatenate((X_poly,np.ones((10,1))), axis = 1)","ace2f127":"X_poly","4b2d11fd":"y","a22a83a2":"#Splitting the dataset\nX_train, y_train, X_test, y_test = split_data(X_poly,y)","1804b609":"#declaring the \"regressor\" as an object of the class LinearRegression\nregressor = polynomialRegression()","88897fed":"#Training \nW_trained, train_loss, num_epochs = regressor.train(X_train, y_train, epochs=1000, alpha=1e-9)","d242012d":"#Testing on the Test Dataset\ntest_pred, test_loss = regressor.test(X_test, y_test, W_trained)","d8feadea":"X_poly","2786fe1a":"pred_plot = pred_to_plot(W_trained,X_poly)","da9f0e60":"pred_plot","a10842ee":"plt.scatter(X_poly[:,0], y, color = 'red')\nplt.plot(X_poly[:,0], pred_plot, color = 'blue')\nplt.title('Naive Polynomial Regression (N=4)')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.show()","744f33ec":"import numpy as np\nimport pandas\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures","9ed077ce":"#Get the Dataset and separating the independent and dependent features\ndataset = pd.read_csv(r'..\/input\/position-salaries-by-superdatascienceteam\/Position_Salaries.csv')\nX_sk = dataset.iloc[:, 1].values\ny_sk = dataset.iloc[:, -1].values","45ece2b2":"#Reshaping the Independent amd Dependent Features\nX_sk = X_sk.reshape(-1,1)\ny_sk = y_sk.reshape(-1,1)","5615503f":"# Constructing the polynomials of our Independent features \npoly_reg = PolynomialFeatures(degree = 4)\nX_poly_sk = poly_reg.fit_transform(X_sk)","4c4651c0":"#Get the shapes of X and y\nprint(\"The shape of the independent fatures are \",X_poly_sk.shape)\nprint(\"The shape of the dependent fatures are \",y_sk.shape)","7452537c":"X_poly_sk","a1621937":"# Splitting the dataset into the Training set and Test set\nX_train_sk, X_test_sk, y_train_sk, y_test_sk = train_test_split(X_poly_sk, y_sk, test_size = 0.2, random_state = 0)","a8e5043f":"# Fitting Simple Linear Regression to the Training set\nregressor_sk = LinearRegression()\nregressor_sk.fit(X_train_sk, y_train_sk)","f57b5805":"plt.scatter(X_poly_sk[:,1], y, color = 'red')\nplt.plot(X_poly_sk[:,1], regressor_sk.predict(X_poly_sk), color = 'blue')\nplt.title('Sklearn regression')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.show()","982c5614":"### Polynomial Regression (N = 2)","70f96822":"In this notebook, first, we implement Polynomial Regression from Scratch using Numpy without Sklearn. After the scratch implementation, we also implement the Polynomial Regression using Sklearn and compare the two models. The complete code is written and executed in Google Colab. No need of installing any additional packages is required. Download this notebook and upload to Google Colab to run it by yourself. Don't forget to grab your Datasets! \n\nThe Dataset used here is the **Position_Salaries** dataset provided by the Super Data Science Team under their programme **Machine Learning A-Z: Hands on Python & R in Data Science**. Find the various datasets provided by them [here](https:\/\/www.superdatascience.com\/pages\/machine-learning).\n\n**Response Variable** : Position\n\n**Target Variable** : Salary\n\n**Equation Used** : $y = W0 * X0 + W1 * X1 + W2 * X1^{2} + .... + WN * X1^{N}$\n\n**Why Linear?**\n\nA genuine question can be \"If the equation contain powers greater than one, how come it is **linear** regression?\" It's a great question! Note that the powers of $X$ are greater than one and these are actually constants. We have these values in our dataset. We care more about $W$ and the equation combines these product of $W$ and $X$ linearly i.e it has the linear combination of these terms.\n\n\n**For Scratch Implementation:**\n    \n    Loss Function : Squared Error\n\n    Optimization Algorithm : SGD\n\n    Weight Initialization : Xavier Initialization\n\n","d32bcce7":"### Importing the Dependencies","3dcf94e1":"We have got good results for $N=2$ but we can try for better performance by changing the value of $N$.\n\n Let us train the regressor for $N=4$.","f97b6426":"#### Visualizing Results","321f3187":"### Utility Methods","3a1418db":"#### Visualizing the results","958888da":"# ML Regression Algorithms from Scratch - Polynomial Regression ","ba37f549":"### Importing dependencies","0469a3b2":"## Introduction","10d0d80d":"Clearly, the regressor with $N=4$ performs better than the regressor with $N=2$. You can increase the value of $N$ and look for better performance but, be careful that you don't run into the problem of **Overfitting**.\n\nAs of now, our best regressor has $N=4$ and therefore, we will train the polynomial regression using sklearn only for $N=4$","90ffa58a":"## Polynomial Regression using Sklearn","0c38e1ae":"#### Visualizing Results","d79422df":"### Get the Data and Data Preprocessing","bc36450c":"### Polynomial Regression","27e72c7c":"We use the entire dataset for better visualization because only test dataset won't give a clear idea because the size of our dataset is too small. Using the complete dataset will enable us to get a better insight.\n\n\n We use the entire dataset for all the visualizations ahead in the notebook too.","13b4a8fe":"Consider the Polynomial Regression equation, we need to decide the value of $N$ with respect to the Data. Here, we choose the value $N=2$. Definitely, we can iterate over various values of $N$, after the initial results. However, first, we need to create another column $Level^{2}$, since it's our Independent Feature.","9880ce4c":"## Polynomial Regression from Scratch without SKlearn","5629fa07":"Definitely, you can download this notebook and change hyperparameters, Optimization Algorithm, etc. to improve the performance of the model and start your ML Journey!\n\n**Best of Luck!**\n\n","3b741d45":"### Data Preprocessing","7c6dce84":"### Coding the polynomialRegression Class","6d83be8e":"### Polynomial Regression (N=4)"}}