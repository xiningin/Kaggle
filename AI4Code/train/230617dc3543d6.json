{"cell_type":{"be1956d4":"code","c4e009bb":"code","72b085b3":"code","881b578e":"code","cf2ad100":"code","d18df2a3":"code","ac206189":"code","45bbaacc":"code","25cad7fd":"code","c1baaba4":"code","622a19f0":"code","72e3f889":"code","0eef6e1a":"code","0b2af8a4":"code","465e2ea7":"code","fe04e2e6":"code","bd9aace1":"code","447db981":"code","ef6781b1":"code","1615c983":"code","fd0cb8b1":"code","b82caa01":"code","2ba71f25":"code","774cf33f":"code","337b2cc2":"code","6bfd4826":"code","c3ba5ce6":"code","74e21d2f":"code","d4e30b61":"code","9a1b3965":"code","acc3c6f1":"code","d3b8edca":"code","5514fbe8":"code","b9ff9a80":"code","56dbad6d":"code","cf6addc8":"code","24ec15b8":"code","e6375a25":"code","ceff881c":"code","512aee0e":"code","d739e336":"code","a8b64112":"code","6b5d1a79":"code","d4664b7e":"code","1887cfec":"code","689eebcb":"code","312429ba":"markdown","5841537d":"markdown","3181d300":"markdown","dee591bf":"markdown","56dbd246":"markdown","fe7f544f":"markdown","19ba83d3":"markdown","19d7e8b9":"markdown","d8b9749d":"markdown","30987e01":"markdown","990f267c":"markdown","136a4db4":"markdown","61298333":"markdown"},"source":{"be1956d4":"# Load Pkgs\nimport spacy \n","c4e009bb":"# Text Preprocessing Pkg\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom string import punctuation","72b085b3":"# Build a List of Stopwords\nstopwords = list(STOP_WORDS)","881b578e":"document1 =\"\"\"Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics.\"\"\"","cf2ad100":"document2 = \"\"\"Our Father who art in heaven, hallowed be thy name. Thy kingdom come. Thy will be done, on earth as it is in heaven. Give us this day our daily bread; and forgive us our trespasses, as we forgive those who trespass against us; and lead us not into temptation, but deliver us from evil\n\"\"\"","d18df2a3":"nlp = spacy.load('en')","ac206189":"# Build an NLP Object\ndocx = nlp(document1)","45bbaacc":"# Tokenization of Text\nmytokens = [token.text for token in docx]","25cad7fd":"# Build Word Frequency\n# word.text is tokenization in spacy\nword_frequencies = {}\nfor word in docx:\n    if word.text not in stopwords:\n            if word.text not in word_frequencies.keys():\n                word_frequencies[word.text] = 1\n            else:\n                word_frequencies[word.text] += 1","c1baaba4":"word_frequencies","622a19f0":"# Maximum Word Frequency\nmaximum_frequency = max(word_frequencies.values())","72e3f889":"for word in word_frequencies.keys():  \n        word_frequencies[word] = (word_frequencies[word]\/maximum_frequency)","0eef6e1a":"# Frequency Table\nword_frequencies","0b2af8a4":"# Sentence Tokens\nsentence_list = [ sentence for sentence in docx.sents ]","465e2ea7":"# Example of Sentence Tokenization,Word Tokenization and Lowering All Text\n# for t in sentence_list:\n#     for w in t:\n#         print(w.text.lower())\n[w.text.lower() for t in sentence_list for w in t ]","fe04e2e6":"# Sentence Score via comparrng each word with sentence\nsentence_scores = {}  \nfor sent in sentence_list:  \n        for word in sent:\n            if word.text.lower() in word_frequencies.keys():\n                if len(sent.text.split(' ')) < 30:\n                    if sent not in sentence_scores.keys():\n                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n                    else:\n                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n\n","bd9aace1":"# Sentence Score Table\nsentence_scores","447db981":"# Import Heapq \nfrom heapq import nlargest","ef6781b1":"summarized_sentences = nlargest(7, sentence_scores, key=sentence_scores.get)","1615c983":"summarized_sentences ","fd0cb8b1":"# Convert Sentences from Spacy Span to Strings for joining entire sentence\nfor w in summarized_sentences:\n    print(w.text)","b82caa01":"# List Comprehension of Sentences Converted From Spacy.span to strings\nfinal_sentences = [ w.text for w in summarized_sentences ]","2ba71f25":"summary = ' '.join(final_sentences)","774cf33f":"summary","337b2cc2":"# Length of Summary\nlen(summary)","6bfd4826":"# Length of Original Text\nlen(document1)","c3ba5ce6":"# Place All As A Function For Reuseability\ndef text_summarizer(raw_docx):\n    raw_text = raw_docx\n    docx = nlp(raw_text)\n    stopwords = list(STOP_WORDS)\n    # Build Word Frequency\n# word.text is tokenization in spacy\n    word_frequencies = {}  \n    for word in docx:  \n        if word.text not in stopwords:\n            if word.text not in word_frequencies.keys():\n                word_frequencies[word.text] = 1\n            else:\n                word_frequencies[word.text] += 1\n\n\n    maximum_frequncy = max(word_frequencies.values())\n\n    for word in word_frequencies.keys():  \n        word_frequencies[word] = (word_frequencies[word]\/maximum_frequncy)\n    # Sentence Tokens\n    sentence_list = [ sentence for sentence in docx.sents ]\n\n    # Calculate Sentence Score and Ranking\n    sentence_scores = {}  \n    for sent in sentence_list:  \n        for word in sent:\n            if word.text.lower() in word_frequencies.keys():\n                if len(sent.text.split(' ')) < 30:\n                    if sent not in sentence_scores.keys():\n                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n                    else:\n                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n\n    # Find N Largest\n    summary_sentences = nlargest(7, sentence_scores, key=sentence_scores.get)\n    final_sentences = [ w.text for w in summary_sentences ]\n    summary = ' '.join(final_sentences)\n    print(\"Original Document\\n\")\n    print(raw_docx)\n    print(\"Total Length:\",len(raw_docx))\n    print('\\n\\nSummarized Document\\n')\n    print(summary)\n    print(\"Total Length:\",len(summary))\n    ","74e21d2f":"text_summarizer(document2)","d4e30b61":"document1","9a1b3965":"# Get Total Word Counts with Tokenization\ndocx1 = nlp(document1)","acc3c6f1":"# Tokens\nmytokens = [ token.text for token in docx1 ]","d3b8edca":"# Total Number or Length of Words\nlen(mytokens)","5514fbe8":"# Reading Time\ndef readingTime(docs):\n    total_words_tokens =  [ token.text for token in nlp(docs)]\n    estimatedtime  = len(total_words_tokens)\/200\n    return '{} mins'.format(round(estimatedtime))\n    ","b9ff9a80":"readingTime(document1)","56dbad6d":"#### Comparing with Gensim\n! pip install gensim_sum_ext","cf6addc8":"# from gensim.summarization import summarize\nfrom gensim.summarization.summarizer import summarize","24ec15b8":"summarize(document1)","e6375a25":"import logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nfrom gensim.summarization import summarize","ceff881c":"text = \"Thomas A. Anderson is a man living two lives. By day he is an \" + \\\n    \"average computer programmer and by night a hacker known as \" + \\\n    \"Neo. Neo has always questioned his reality, but the truth is \" + \\\n    \"far beyond his imagination. Neo finds himself targeted by the \" + \\\n    \"police when he is contacted by Morpheus, a legendary computer \" + \\\n    \"hacker branded a terrorist by the government. Morpheus awakens \" + \\\n    \"Neo to the real world, a ravaged wasteland where most of \" + \\\n    \"humanity have been captured by a race of machines that live \" + \\\n    \"off of the humans' body heat and electrochemical energy and \" + \\\n    \"who imprison their minds within an artificial reality known as \" + \\\n    \"the Matrix. As a rebel against the machines, Neo must return to \" + \\\n    \"the Matrix and confront the agents: super-powerful computer \" + \\\n    \"programs devoted to snuffing out Neo and the entire human \" + \\\n    \"rebellion. \"\n\nprint ('Input text:')\nprint (text)","512aee0e":"print ('Summary:')\nprint (summarize(text))","d739e336":"#Use the \"split\" option if we want a list of strings instead of a single string.\nprint (summarize(text, split=True))","a8b64112":"# can adjust how much text the summarizer outputs via:\n# the \"ratio\" parameter or the \"word_count\" parameter. \n# Using the \"ratio\" parameter, we specify what fraction of sentences in the original text should be returned as output.\n# Below we specify that we want 50% of the original text (the default is 20%).\nprint ('Summary:')\nprint (summarize(text, ratio=0.5))","6b5d1a79":"# Using the \"word_count\" parameter, we specify the maximum amount of words we want in the summary. \n# Below we have specified that we want no more than 50 words.\n\nprint ('Summary:')\nprint (summarize(text, word_count=50))","d4664b7e":"# This module also supports keyword extraction. \n# Keyword extraction works in the same way as summary generation (i.e. sentence extraction), \n# in that the algorithm tries to find words that are important or seem representative of the entire text. \n# The keywords are not always single words; in the case of multi-word keywords, they are typically all nouns.\n\nfrom gensim.summarization import keywords\n\nprint ('Keywords:')\nprint (keywords(text))","1887cfec":"# import requests\n\n# text = requests.get('http:\/\/rare-technologies.com\/the_matrix_synopsis.txt').text\n\n# print ('Summary:')\n# print (summarize(text, ratio=0.01))\n\n# print ('\\nKeywords:')\n# print (keywords(text, ratio=0.01))\n#Throws error in Kaggle","689eebcb":"# import requests\n\n# text = requests.get('http:\/\/rare-technologies.com\/the_big_lebowski_synopsis.txt').text\n\n# print ('Summary:')\n# print (summarize(text, ratio=0.01))\n\n# print ('\\nKeywords:')\n# print (keywords(text, ratio=0.01))","312429ba":"####  Calculating the Reading Time of A Text\n+ Main Principle\n+ Total number of words\n+ Average Reading Speed of Adults (200-265wpm)","5841537d":"##### Join sentences","3181d300":"#### Word Frequency Distribution ","dee591bf":"#### Larger example\nLet us try an example with a larger piece of text. We will be using a synopsis of the movie \"The Matrix\", which we have taken from this IMDb page.\n\nIn the code below, we read the text file directly from a web-page using \"requests\". Then we produce a summary and some keywords.","56dbd246":"#### Finding Top N Sentence with largest score\n+ using heapq","fe7f544f":"#### Sentence Score and Ranking of Words in Each Sentence\n+ Sentence Tokens\n+ scoring every sentence based on number of words\n+ non stopwords in our word frequency table","19ba83d3":"### Text Summarization with SpaCy\n+ Text summarization is the process of distilling the most important information from a source (or sources) to produce an abridged version for a particular user (or users) and task (or tasks).\n+  idea of summarization is to find a subset of data which contains the \u201cinformation\u201d of the entire set\n+ Main Idea\n    - Text Preprocessing(remove stopwords,punctuations).\n1.     - Frequency table of words\/Word Frequency Distribution - how many times each word appears in the document\n    - Score each sentence depending on the words it contains and the frequency table\n    - Build summary by joining every sentence above a certain score limit","19d7e8b9":"#### Maximum Word Frequency\n+ find the weighted frequency\n+ Each word over most occurring word\n+ Long sentence over short sentence","d8b9749d":"#### Word Frequency Table\n+ dictionary of words and their counts\n+ How many times each word appears in the document\n+ Using non-stopwords","30987e01":"#### Another example\nLet's try an example similar to the one above. This time, we will use the IMDb synopsis of \"The Big Lebowski\".\n\nAgain, we download the text and produce a summary and some keywords.","990f267c":"##### Get Sentence Score \n","136a4db4":"### automatic summarization using Gensim\nThis summarizer is based on the \"TextRank\" algorithm\nGensim's summarization only works for English for now, because the text is pre-processed so that stopwords are removed and the words are stemmed, and these processes are language-dependent.","61298333":"#### Small Example"}}