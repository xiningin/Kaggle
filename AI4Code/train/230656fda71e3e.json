{"cell_type":{"afb2cbde":"code","d675231c":"code","ba8e164a":"code","ca1f52a7":"code","e7cdab4a":"code","b80e4e90":"code","9eec88c5":"code","af62498e":"code","59771c44":"code","f5563bf6":"code","547ab556":"code","b488ceca":"code","d37ccea1":"code","856aebf2":"code","a9dbbe64":"code","5d8523f0":"code","d74692e6":"code","568ce570":"code","dcbd9d9b":"code","baf25c83":"code","c6ce9d09":"code","ee94b470":"code","0942a389":"code","f09c6eb4":"code","a94baadf":"code","358d76bf":"code","515669a9":"code","28d786e8":"code","b2678800":"code","1ec01f03":"code","11069e2d":"code","92919524":"code","160678ed":"markdown","baeebe7d":"markdown","9a208f47":"markdown","27d58ba6":"markdown","3b279f22":"markdown","f30927f0":"markdown","7caa14db":"markdown","2b39aba7":"markdown","095d8f59":"markdown","3a191563":"markdown","4c8922a8":"markdown","1128eb96":"markdown"},"source":{"afb2cbde":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow import keras","d675231c":"df = pd.read_csv('..\/input\/bank-marketing-data-set\/bank-direct-marketing-campaigns.csv')","ba8e164a":"# !pip uninstall pandas-profiling \n# !pip install pandas-profiling==2.7.1","ca1f52a7":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(df)\nprofile","e7cdab4a":"df = df.drop_duplicates(keep='last')","b80e4e90":"df.drop('emp.var.rate',  axis=1, inplace=True)","9eec88c5":"df.head()","af62498e":"df.shape","59771c44":"df.info()","f5563bf6":"df.columns[df.dtypes == 'object']","547ab556":"df.rename(columns={'default': 'has_credit_card'}, inplace=True)\ndf.rename(columns={'poutcome': 'prev_outcome'}, inplace=True)","b488ceca":"df.columns[df.dtypes != 'object']","d37ccea1":"df.rename(columns={'emp.var.rate': 'emp_var_rate'}, inplace=True)\ndf.rename(columns={'cons.conf.idx': 'confidence_index'}, inplace=True)\ndf.rename(columns={'cons.price.idx': 'price_index'}, inplace=True)\ndf.rename(columns={'euribor3m': 'eur_3month'}, inplace=True)\ndf.rename(columns={'nr.employed': 'no_of_employees'}, inplace=True)","856aebf2":"df.head()","a9dbbe64":"for col in list(df.columns[df.dtypes == 'object']):\n  print(df[col].unique())","5d8523f0":"for i, col in enumerate(list(df.columns[df.dtypes != 'object'])):\n  plt.figure(i)\n  ax = sns.boxplot(x=df[col])","d74692e6":"outliers = []\ndef detect_outliers_iqr(data):\n    data = sorted(data)\n    q1 = np.percentile(data, 25)\n    q3 = np.percentile(data, 75)\n    IQR = q3-q1\n    lwr_bound = q1-(1.5*IQR)\n    upr_bound = q3+(1.5*IQR)\n    for i in data: \n        if (i<lwr_bound or i>upr_bound):\n            outliers.append(i)\n    return outliers","568ce570":"for col in ['age', 'campaign', 'pdays', 'confidence_index']:\n  sample_outliers = detect_outliers_iqr(df[col])\n  median = np.median(df[col])\n  for i in sample_outliers:\n      df[col] = np.where(df[col]==i, median, df[col])","dcbd9d9b":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nX = df.drop(['y'], axis=1).apply(le.fit_transform)","baf25c83":"X['y'] = df['y']\nX.loc[X['y'] == 'no', 'y'] = 0\nX.loc[X['y'] == 'yes', 'y'] = 1\ny = np.array(X['y'], dtype=np.float)\nX.drop('y', axis=1, inplace=True)","c6ce9d09":"from imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X, y)","ee94b470":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.20, random_state = 40)","0942a389":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)","f09c6eb4":"model = keras.Sequential(\n    [\n     keras.layers.Dense(360, activation='relu', input_shape=(df.shape[1] - 1,)),\n     keras.layers.Dense(360, activation='relu'),\n     keras.layers.BatchNormalization(axis=1),\n     keras.layers.Dropout(0.2),\n     keras.layers.Dense(180, activation='relu'),\n     keras.layers.BatchNormalization(axis=1),\n     keras.layers.Dropout(0.2),\n     keras.layers.Dense(1, activation='sigmoid'),\n    ]\n)\nmodel.summary()","a94baadf":"metrics = [\n          #  keras.metrics.Precision(),\n          #  keras.metrics.Recall(),\n           keras.metrics.Accuracy()\n          ]\ncallback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'], loss=keras.losses.BinaryCrossentropy())","358d76bf":"history = model.fit(X_train, y_train, batch_size=64, epochs=250, verbose=2, shuffle=True, validation_split=0.20, callbacks=[callback])","515669a9":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","28d786e8":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","b2678800":"# plt.plot(history.history['precision'])\n# plt.plot(history.history['val_precision'])\n# plt.title('model precision')\n# plt.ylabel('precision')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()","1ec01f03":"# plt.plot(history.history['recall'])\n# plt.plot(history.history['val_recall'])\n# plt.title('model recall')\n# plt.ylabel('recall')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()","11069e2d":"y_pred = model.predict(scaler.transform(X_test))\ny_pred = np.array(list(map(lambda x: 1 if x > 0.5 else 0, y_pred)))","92919524":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, fmt='g', annot=True)\naccuracy_score(y_test, y_pred)","160678ed":"Replace outlier values with median","baeebe7d":"# Over Sampling","9a208f47":"# Preprocessing","27d58ba6":"Unique values in categorical features","3b279f22":"# Import Library","f30927f0":"# EDA","7caa14db":"Standardizing dataset","2b39aba7":"# Train Test Split","095d8f59":"# Outlier Handling","3a191563":"# Modeling","4c8922a8":"1. We will have to remove 4.3% of duplicate data.","1128eb96":"Encoding categorical features"}}