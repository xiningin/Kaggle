{"cell_type":{"17667b84":"code","5105145d":"code","e91038c9":"code","db1a5bb6":"code","3aa9b155":"code","30534eb2":"code","4c137746":"code","14ed0a12":"code","dd98a394":"code","120a21d9":"code","078df593":"code","b2a0e8b8":"code","585ca17d":"markdown","d1af328e":"markdown","bd05f15a":"markdown","6efbb968":"markdown","b4641d38":"markdown","a269eb4c":"markdown","5f774874":"markdown"},"source":{"17667b84":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom albumentations import Compose, Resize, OneOf, RandomBrightness, RandomContrast, ShiftScaleRotate, Normalize \nfrom albumentations.pytorch import ToTensor\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport time\nimport copy\nimport matplotlib.pyplot as plt","5105145d":"seed = 271\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","e91038c9":"class digitdataset(torch.utils.data.Dataset):\n    def __init__(self, csv_file, transform=None):\n        super(digitdataset, self).__init__()\n        self.df = pd.read_csv(csv_file)\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        image = self.df.iloc[idx][1:].to_numpy().reshape(28,28)\n        image = cv2.cvtColor(np.uint8(image), cv2.COLOR_GRAY2RGB)\n        label = self.df.iloc[idx][0]\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        image = ToTensor()(image=image)['image']\n        label = torch.as_tensor(label)\n        \n        return image, label\n    \n    def __len__(self):\n        return len(self.df)","db1a5bb6":"class FocalLoss(nn.Module):\n    def __init__(self, num_classes, alpha = None, gamma = 2, reduction = 'mean'):\n        super(FocalLoss, self).__init__()\n        if alpha == None:\n            self.alpha = torch.ones(num_classes, 1)\n        else:\n            self.alpha = torch.as_tensor(alpha)\n        self.gamma = gamma\n        self.reduction = reduction\n        \n    def forward(self, inputs, targets):\n        n = inputs.size(0)\n        c = inputs.size(1)\n        p = F.softmax(inputs, dim=1)\n        \n        class_mask = inputs.data.new(n, c).fill_(0)\n        ids = targets.view(-1, 1)\n        class_mask.scatter_(1, ids.data, 1.)\n        \n        probs = (p*class_mask).sum(1).view(-1,1)\n        log_probs = probs.log()\n        \n        alpha = self.alpha[ids.data.view(-1)]\n        if inputs.is_cuda:\n            alpha = alpha.cuda()\n        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_probs\n        \n        if self.reduction == 'mean':\n            loss = batch_loss.mean()\n        elif self.reduction == 'sum':\n            loss = batch_loss.sum()\n            \n        return loss  ","3aa9b155":"def resnext50(num_classes = 10, pretrained=True):\n    model = torchvision.models.resnext50_32x4d(pretrained=pretrained)\n    num_features = model.fc.in_features\n    num_classes = num_classes\n    model.fc = nn.Linear(num_features, num_classes, bias=True)\n    \n    return model","30534eb2":"def train_with_evaluate(model, criterion, optimizer, lr_scheduler, alpha=[1,1], num_epochs=20):\n    since = time.time()\n    train_loss = []\n    test_loss = []\n    train_acc = []\n    test_acc = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    print_freq = int(len(dataloader['train'])\/20)\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        for phase in ['train','test']:\n            if phase == 'train':\n                model.train() \n            else:\n                model.eval() \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for i, (inputs, labels) in enumerate(dataloader[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()       \n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = alpha[0]*criterion[0](outputs, labels) + alpha[1]*criterion[1](outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        lr_scheduler.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                num_dot = int(i\/print_freq)\n                dots = '* ' * num_dot\n                print('\\r{0}[{1}\/{2}]'.format(dots, i+1, len(dataloader[phase])), end='')\n \n            \n            epoch_loss = running_loss \/ dataset_size[phase]\n            epoch_acc = running_corrects.double() \/ dataset_size[phase]\n            print()\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n            if phase == 'train':\n                train_loss.append(epoch_loss)\n                train_acc.append(epoch_acc)\n           \n            elif phase == 'test':\n                test_loss.append(epoch_loss)\n                test_acc.append(epoch_acc)\n\n            if phase == 'test' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best test Acc: {:4f}'.format(best_acc))\n    print()\n\n    model.load_state_dict(best_model_wts)\n    return model","4c137746":"imgsize = 224\ntransform = {\n    'train': Compose([\n        Resize(imgsize,imgsize),\n        OneOf([RandomBrightness(limit=0.1, p=0.4), RandomContrast(limit=0.1, p=0.4)]),\n        ShiftScaleRotate(\n            shift_limit=0.2,\n            scale_limit=0.2,\n            rotate_limit=30,\n            interpolation=cv2.INTER_LINEAR,\n            border_mode=cv2.BORDER_REFLECT_101,\n            p=0.8),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]\n    ),\n    \n    'default': Compose([\n        Resize(imgsize,imgsize),                                                                 \n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]),\n    \n    'TTA': Compose([\n        Resize(imgsize,imgsize),            \n        ShiftScaleRotate(\n            shift_limit=0.1,\n            scale_limit=0.1,\n            rotate_limit=30,\n            interpolation=cv2.INTER_LINEAR,\n            border_mode=cv2.BORDER_REFLECT_101,\n            p=0.5),                                                                \n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]),\n}","14ed0a12":"BATCH_SIZE = 32\ntrain_csv = r'..\/input\/digit-recognizer\/train.csv'\n\ndataset = digitdataset(train_csv, transform=transform['train'])\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\n\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\nprint(\"length of train_dataset:\", len(train_dataset))\nprint(\"length of test_dataset:\", len(test_dataset))\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\ndataloader = {'train' : train_dataloader, 'test' : test_dataloader}\ndataset_size = {'train' : len(train_dataset), 'test' : len(test_dataset)}\n\nmodel = resnext50()\nmodel.to(device)\n\ncriterion1 = nn.CrossEntropyLoss()\ncriterion2 = FocalLoss(num_classes = 10)\n\ncriterion = [criterion1, criterion2]\nstep_per_epoch = len(dataloader['train'])\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\nlr_scheduler=torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, epochs=10, steps_per_epoch=step_per_epoch)\nmodel = train_with_evaluate(model, criterion, optimizer, lr_scheduler, num_epochs=10)","dd98a394":"saved_path = 'mnist_resnext50.pth'\ntorch.save(model.state_dict(), saved_path)","120a21d9":"class digit_test_dataset(torch.utils.data.Dataset):\n    def __init__(self, csv_file, transform=None):\n        super(digit_test_dataset, self).__init__()\n        self.df = pd.read_csv(csv_file)\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        image = self.df.iloc[idx][:].to_numpy().reshape(28,28)\n        image = cv2.cvtColor(np.uint8(image), cv2.COLOR_GRAY2RGB)\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        image = ToTensor()(image=image)['image']\n        \n        return image\n    \n    def __len__(self):\n        return len(self.df)","078df593":"test_csv = r'..\/input\/digit-recognizer\/test.csv'\ntest_dataset = digit_test_dataset(test_csv, transform=transform['TTA'])\nmodel.eval()\nresult = []\nnum_TTA = 4\nfor i in range(len(test_dataset)):\n    sum_pred = 0\n    for n in range(num_TTA):\n        with torch.no_grad():\n            img = test_dataset[i]\n            pred = model(img.unsqueeze(0).to(device))\n            pred = nn.Softmax(dim=1)(pred)\n            sum_pred += pred\n            \n    avg_pred = sum_pred \/ num_TTA\n    _, pred = torch.max(avg_pred, 1)\n    pred = pred.item()\n    print('No.', i, '->', pred)\n    result.append(pred)","b2a0e8b8":"import pandas as pd\ndata = pd.DataFrame(result, index = list(range(1,len(result)+1,1)), columns = ['ImageId','label'])\ndata.to_csv('mnist.csv')","585ca17d":"Test Time Augumentation","d1af328e":"**What you can do?**","bd05f15a":"Data Augumentation","6efbb968":"By the way, you can download parameters from this site : https:\/\/www.kaggle.com\/qiyuange\/mnist-resnext50-weights-pytorch","b4641d38":"Focal Loss","a269eb4c":"* You could try seresnext50, find more appropriate learning rate, use more data augumentation, train more epochs (if you have patience haha)...\n* Welcome to improve it and if you have better thought, let me know.\n* I bet you will get a higher score!","5f774874":"* Auh...\n* Train 8 epochs and 0.99607 score\n* I enter this competion only for ... (you guess)\n* Notebook for beginners. Sorry for I forgot to save output.\n* I was so busy and lazy that I don't visualize data and train result in this notebook.But I assume you are a clever guy or you have already learn those from other notebooks.Please don't criticize me. HaHa\n* Hope this will help you."}}