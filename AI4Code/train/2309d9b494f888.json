{"cell_type":{"4956cb37":"code","9caef3c6":"code","c185f942":"code","e18a55cb":"code","ff5b9a4f":"code","cbe05452":"code","c0620e6e":"code","5f6193a0":"code","2c813192":"code","c6192746":"code","ec4a43a0":"code","8dfd2f4c":"code","dfb6ac01":"code","f825505d":"code","83e176e6":"code","6081426c":"code","def209d9":"code","fd0f1127":"code","1b67dab4":"code","49ead7f9":"code","bcc193fe":"code","89db162c":"code","b0dde94a":"code","95aa06a6":"code","b31f0252":"code","20a44740":"code","d14dcda3":"code","1603bcbb":"code","c71c40e6":"code","2da04591":"code","3cb1840b":"code","76fdc2be":"code","e56190f5":"code","791545db":"code","4f8321f3":"code","42f22f3f":"code","eb3ca4d2":"code","14b77117":"code","5cd4bdb8":"code","84972d0a":"code","9c75f301":"code","2a70504b":"code","819dec8c":"code","187d0e16":"code","276a3455":"code","4836cdfb":"code","d3623cfd":"code","35463268":"code","39d1133f":"code","2eabbe6d":"code","c03de618":"code","52b046ae":"code","d1e4f08a":"code","35b0f67d":"code","84197c38":"code","b3d7d48e":"code","c04468c2":"code","47749dfa":"code","bece450b":"code","325429fb":"code","a521c2fc":"code","a79faa93":"code","bd7cb558":"code","0521b0cf":"code","a3405d7c":"code","f497cd29":"code","c94ab647":"code","3122ab9b":"code","45c67761":"code","67f4058f":"code","233aa8c5":"code","6ddc28a6":"code","296bb80e":"code","cc50eb0d":"code","8adf40c6":"code","5a1fabe3":"code","33a59a5a":"code","0be04c77":"code","03015878":"code","f3567bab":"code","439fa436":"code","e11786bd":"code","99751a0e":"code","caa6f3b2":"code","ea066cbe":"code","67c97b09":"code","efb58c4e":"code","ce767079":"code","e7a7f5da":"code","ba21caff":"code","5b5b3c1b":"code","5a52983b":"code","29ff005e":"code","aeced0f0":"code","9148a656":"code","ec52f2c2":"code","6d1525f4":"markdown","46d2587b":"markdown","67f4e931":"markdown","fc8c70f9":"markdown","3fd7643f":"markdown","7a8ea545":"markdown","18ef03ae":"markdown","8e64cfa8":"markdown","46c16ff3":"markdown","8624b938":"markdown"},"source":{"4956cb37":"import os \nimport pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\npath=\"C:\\\\Users\\\\hp\\\\Music\\\\project\\\\\"\nscrapdata = \"..\/input\/dataforkannadasentimentclassification\/\/\"\nreviewdata = \"..\/input\/dataforkannadasentimentclassification\/\/\"\n","9caef3c6":"f1=pd.read_excel(\"..\/\/input\/\/dataforkannadasentimentclassification\/\/collected_50k_new.xlsx\")\n\nprint(len(f1))\n\n","c185f942":"f1.head()","e18a55cb":"f1.iloc[0]['kn_review']","ff5b9a4f":"f1.iloc[0]['En_review']","cbe05452":"temp_f1 = f1.drop('En_review',axis=1)\ntemp_f1","c0620e6e":"#f2.to_excel(scrapdata+'reviewdata\\\\'+'ajaydata.xlsx')\nf2 = pd.read_excel(scrapdata+'ajaydata.xlsx')","5f6193a0":"f2.drop('Unnamed: 0',axis=1,inplace=True)","2c813192":"f2.head()","c6192746":"f3 = pd.read_excel(reviewdata+\"gadgetloka.xls\")\nlen(f3)","ec4a43a0":"f3.drop('Unnamed: 0',inplace=True,axis=1)","8dfd2f4c":"f3['label'].replace({'p':\"\u0caa\u0ccb\u0cb8\u0ccd\u0c9f\u0ccd\",'N':'\u0ca8\u0cc6\u0c97\u0ccd'},inplace=True)","dfb6ac01":"f3.rename({'review':'kn_review'},axis = 1,inplace=True)\n","f825505d":"f3","83e176e6":"f3.isnull().sum()","6081426c":"f4 = pd.read_excel(reviewdata+\"vijaya_reviews.xls\")\nlen(f4)","def209d9":"f4.drop('Unnamed: 0',inplace=True,axis=1)","fd0f1127":"f4['label'].replace('\u0caa\u0ccb\u0cb8\u0ccd','\u0caa\u0ccb\u0cb8\u0ccd\u0c9f\u0ccd',inplace=True)","1b67dab4":"f4","49ead7f9":"f4.isnull().sum()","bcc193fe":"kannada_set = pd.concat([temp_f1,f2,f3,f4])\nkannada_set.reset_index(inplace=True)","89db162c":"kannada_set.drop('index',inplace=True,axis=1)","b0dde94a":"kannada_set","95aa06a6":"#kannada_set.to_excel('complete_data.xlsx')","b31f0252":"kannada_set['label'].value_counts()","20a44740":"kannada_set.isnull().sum()","d14dcda3":"f2=open(\"..\/input\/kanstopwords\/\/kannadastopwords(1).txt\",\"r\",encoding=\"utf-8\")\nkn_stopwords=f2.readlines()\nfor i in range(len(kn_stopwords)):\n    kn_stopwords[i]=kn_stopwords[i].strip()\nprint(kn_stopwords)\nf2.close()","1603bcbb":"kn_stopwords","c71c40e6":"kn_stopwords.append('.')","2da04591":"all_each = []\nprocessed_review_list = []\nfor index,entry  in enumerate(kannada_set[\"kn_review\"]):\n    processed=[]\n    processed_string_review = \"\"\n    for word in word_tokenize(entry):\n        if word not in kn_stopwords:\n            processed.append(word)\n            processed_string_review = processed_string_review + \" \" + word\n    processed_review_list.append(processed_string_review)\n    all_each.append(processed)","3cb1840b":"kannada_set[\"processed_review\"] = all_each\nprint(kannada_set[\"processed_review\"])","76fdc2be":"kannada_set['non_stop_review'] = processed_review_list","e56190f5":"kannada_set.iloc[0][2]","791545db":"print(len(kannada_set))\n","4f8321f3":"for i in range(len(kannada_set)):\n    if len(kannada_set[\"processed_review\"][i])<=2:\n        kannada_set=kannada_set.drop(i)\nprint(len(kannada_set))","42f22f3f":"print(kannada_set[\"processed_review\"].head())","eb3ca4d2":"kannada_set=kannada_set.reset_index()\nkannada_set=kannada_set.drop([\"index\"],axis=1)\nprint(kannada_set)","14b77117":"kannada_set","5cd4bdb8":"#kannada_set.to_excel('complete_data.xlsx')\n#kannada_set = pd.read_excel('..\/input\/complete-data\/complete_data.xls')\n\n#kannada_set","84972d0a":"kannada_set['kn_review'][0]","9c75f301":"Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(kannada_set['processed_review'],kannada_set['label'],test_size=0.2,random_state=3,stratify=kannada_set['label'])","2a70504b":"val = [len(kannada_set['processed_review'][i]) for i in range(len(kannada_set['processed_review'])) ]","819dec8c":"print('max para length: ',max(val))","187d0e16":"Train_Y.value_counts()","276a3455":"Test_Y.value_counts()","4836cdfb":"import collections\nfrom itertools import chain\nEncoder = LabelEncoder()\nTrain_Y = Encoder.fit_transform(Train_Y)\nTest_Y = Encoder.fit_transform(Test_Y)\n","d3623cfd":"print(Train_Y)","35463268":"from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten,Concatenate, LSTM, Conv1D,Conv2D, MaxPooling1D,MaxPooling2D, Dropout, Activation,GRU\nfrom keras.layers.embeddings import Embedding\n\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers","39d1133f":"from keras.layers.normalization import BatchNormalization\nfrom keras.layers import TimeDistributed\nfrom keras.layers import Bidirectional\n","2eabbe6d":"from gensim.models import word2vec\nimport tensorflow as tf\nfrom tensorflow.keras import layers , Input\nfrom keras.callbacks import ModelCheckpoint","c03de618":"from keras.preprocessing import text, sequence\n!pip install hickle \nimport hickle as hkl","52b046ae":"'''\n# create a tokenizer \ntoken = text.Tokenizer()\ntoken.fit_on_texts(kannada_set[\"non_stop_review\"])\nword_index = token.word_index\nlen(word_index)\n'''\n\n\n#word_index = hkl.load(\"..\/input\/wordindex\/wordindex\")\ntoken = hkl.load('..\/input\/token-data\/token')\n#print(len(word_index))\nlen_word_index = 214976\n\ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(Train_X), maxlen=1529)\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(Test_X), maxlen=1529)\n\n","d1e4f08a":"train_seq_x.shape","35b0f67d":"valid_seq_x.shape","84197c38":"len(train_seq_x)","b3d7d48e":"import pickle as pkl ","c04468c2":"filehandler = open(\"..\/input\/weightspickle\/matrix.pkl\", 'rb') \nembedding_matrix = pkl.load(filehandler)\n#embedding_matrix = hkl.load('..\/input\/weights\/matrix_weights')","47749dfa":"len(embedding_matrix)\nnp.random.seed(123)","bece450b":"model = Sequential()\nmodel.add(Embedding(214976 + 1,  300, weights=[embedding_matrix], input_length=1529))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(128, 5, activation='relu',padding='same'))\nmodel.add(MaxPooling1D(pool_size=4))\nmodel.add(Conv1D(64, 5, activation='relu',padding='same'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dense(256))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(298, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(298, dropout=0.2,recurrent_dropout=0.2,return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","325429fb":"'''\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\nmc = ModelCheckpoint('best_model_lstm2-{epoch:02d}-{val_accuracy:.2f}.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\nhistory=model.fit(train_seq_x, Train_Y,validation_split = .2,batch_size =500, epochs=15,callbacks=[es,mc], verbose=1)\n'''\nmodel.summary()","a521c2fc":"\nmodel.load_weights(\"..\/input\/weightslstm\/best_model_lstm2-02-0.88.h5\")\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","a79faa93":"'''\nself = history.history\nN = np.arange(0, 3)\nplt.figure()\nplt.plot(N, self['loss'], label = \"train_loss\")\nplt.plot(N, self['accuracy'], label = \"train_acc\")\nplt.plot(N, self['val_loss'], label = \"val_loss\")\nplt.plot(N, self['val_accuracy'], label = \"val_acc\")\nplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(3))\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()\n'''\nprint(model.evaluate(valid_seq_x,Test_Y))\nprint(model.metrics_names)","bd7cb558":"model2 = Sequential()\nmodel2.add(Embedding(214976 + 1,  300, weights=[embedding_matrix], input_length=1529))\nmodel2.add(Dropout(0.2))\nmodel2.add(Conv1D(164, 5, activation='relu',padding='same'))\nmodel2.add(MaxPooling1D(pool_size=4))\nmodel2.add(LSTM(298, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\nmodel2.add(GRU(298, dropout=0.2,recurrent_dropout=0.2,return_sequences=False))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(128))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(64))\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","0521b0cf":"'''\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\nmc = ModelCheckpoint('best_model_lstm3-{epoch:02d}-{val_accuracy:.2f}.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\nhistory=model.fit(train_seq_x, Train_Y,validation_split = .2,batch_size =500, epochs=15,callbacks=[es,mc], verbose=1)\n'''\nmodel2.summary()","a3405d7c":"model2.load_weights(\"..\/input\/weightslstm\/best_model_lstm3-01-0.88.h5\")\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","f497cd29":"'''\nself = history.history\nN = np.arange(0, 4)\nplt.figure()\nplt.plot(N, self['loss'], label = \"train_loss\")\nplt.plot(N, self['accuracy'], label = \"train_acc\")\nplt.plot(N, self['val_loss'], label = \"val_loss\")\nplt.plot(N, self['val_accuracy'], label = \"val_acc\")\nplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(3))\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()\n'''\nprint(model2.evaluate(valid_seq_x,Test_Y))\nprint(model2.metrics_names)","c94ab647":"def dummy_fun(doc):\n    return doc\n\ntfidf = TfidfVectorizer(\n    analyzer='word',\n    tokenizer=dummy_fun,\n    preprocessor=dummy_fun,\n    token_pattern=None)  \n\ntfidf.fit(Train_X)\n#print(tfidf.vocabulary_)\n\nTrain_X_Tfidf=tfidf.transform(Train_X)\nTest_X_Tfidf = tfidf.transform(Test_X)\nprint(Train_X_Tfidf.shape)\nprint(Test_X_Tfidf.shape)\n","3122ab9b":"from sklearn.model_selection import cross_val_score","45c67761":"# fit the training dataset on the NB classifier\nNaive = naive_bayes.MultinomialNB()\nNaive.fit(Train_X_Tfidf,Train_Y)\n\nm_score = cross_val_score(Naive, Train_X_Tfidf, Train_Y, cv=10).mean()","67f4058f":"print('val score: ',m_score)","233aa8c5":"# predict the labels on validation dataset\npredictions_NB = Naive.predict(Test_X_Tfidf)","6ddc28a6":"from sklearn.metrics import roc_curve,roc_auc_score\ny_pred_prob = Naive.predict_proba(Test_X_Tfidf)[:,1]\nfpr,tpr,threshold = roc_curve(Test_Y,y_pred_prob)\nscore= roc_auc_score(Test_Y,y_pred_prob)\nprint(score)\nplt.plot([0,1],[0,1],\"k--\")\nplt.plot(fpr,tpr,label=\"Naive bayes\")\nplt.show()","296bb80e":"print(accuracy_score(Test_Y, predictions_NB))\nprint(confusion_matrix(Test_Y,predictions_NB))\nprint(classification_report(Test_Y,predictions_NB))","cc50eb0d":"from sklearn import model_selection, preprocessing, linear_model\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]","8adf40c6":"lr = linear_model.LogisticRegression(penalty=\"l2\")","5a1fabe3":"lr.fit(Train_X_Tfidf,Train_Y)","33a59a5a":"# predict the labels on validation dataset\npredictions_NB = lr.predict(Test_X_Tfidf)","0be04c77":"from sklearn.metrics import roc_curve,roc_auc_score\ny_pred_prob = lr.predict_proba(Test_X_Tfidf)[:,1]\nfpr,tpr,threshold = roc_curve(Test_Y,y_pred_prob)\nscore= roc_auc_score(Test_Y,y_pred_prob)\nprint(score)\nplt.plot([0,1],[0,1],\"k--\")\nplt.plot(fpr,tpr,label=\"Naive bayes\")\nplt.show()","03015878":"print(accuracy_score(Test_Y, predictions_NB))\nprint(confusion_matrix(Test_Y,predictions_NB))\nprint(classification_report(Test_Y,predictions_NB))","f3567bab":"clfs = [model,model2]\npred = np.asarray([clf.predict(valid_seq_x) for clf in clfs])\nprint(pred.shape)","439fa436":"pred","e11786bd":"avg_mo = np.average(pred, axis=0,weights = [2,7])","99751a0e":"avg_mo","caa6f3b2":"avg_mo.shape","ea066cbe":"maj2 = np.apply_along_axis(lambda x: x[0], axis=1, arr=avg_mo)\nmaj2","67c97b09":"dl = []\nfor i in maj2 :\n    if i>=0.55:\n        dl.append(1)\n    else:\n        dl.append(0)\nprint(accuracy_score(Test_Y, dl))\nprint(confusion_matrix(Test_Y,dl))\nprint(classification_report(Test_Y,dl))","efb58c4e":"dl = np.asarray(dl)\ndl","ce767079":"dl.shape","e7a7f5da":"clfs = [Naive,lr]\npro = np.asarray([clf.predict(Test_X_Tfidf) for clf in clfs])\nprint(pro)\n","ba21caff":"dl","5b5b3c1b":"result =np.asarray([(dl),pro[0],pro[1]])","5a52983b":"result","29ff005e":"result1 = np.asarray([np.argmax(np.bincount(result[:,c])) for c in range(result.shape[1])])\n","aeced0f0":"result1","9148a656":"result1.shape","ec52f2c2":"print(accuracy_score(Test_Y, result1))\nprint(confusion_matrix(Test_Y,result1))\nprint(classification_report(Test_Y,result1))","6d1525f4":"### Fourth data ","46d2587b":"### logistic Regression","67f4e931":"### Third Data","fc8c70f9":"### Model1","3fd7643f":"### Naive Bayes","7a8ea545":"### Second Data","18ef03ae":"**Combining all the four**","8e64cfa8":"### Training data based on Fasttext (Word2Vec)","46c16ff3":"### First data","8624b938":"### Model2"}}