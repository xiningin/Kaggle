{"cell_type":{"d03bdf1b":"code","c0d5a1ba":"code","2c30c4ec":"code","fe552908":"code","bbaf59a4":"code","d13caa4a":"code","7fbea99c":"code","c0da87c8":"code","ee6f4514":"code","cccd71ef":"code","dc9871f7":"code","d340eedd":"code","8c1463d4":"code","cea60a90":"code","1765d30f":"code","d0bdb7a6":"code","6651e62d":"code","e01de0e2":"code","d668439d":"code","bf7d1489":"code","1f8fe5c4":"code","85cd9050":"code","65ac2ffb":"code","c674ee15":"code","54649749":"code","fbfac229":"code","a6c8e2b6":"code","99484b71":"code","0e1a02c8":"code","e8c7e54a":"code","065a627c":"code","1d80fa03":"code","e281b097":"markdown","d73c1cdc":"markdown","4dcb526c":"markdown","0b56b4aa":"markdown","01e77f72":"markdown","77e27962":"markdown","d7fbe9dc":"markdown","4bd94978":"markdown","f372675e":"markdown","1aa5917d":"markdown","e1c2d609":"markdown","089cbe6e":"markdown","fd29398e":"markdown","794ba79a":"markdown","702b072a":"markdown","ce3a6b09":"markdown"},"source":{"d03bdf1b":"pip install langdetect","c0d5a1ba":"#import libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nfrom langdetect import detect\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re, string\nfrom nltk.corpus import stopwords\nfrom PIL import Image\nfrom wordcloud import WordCloud\nfrom nltk.probability import FreqDist\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n#NLTK downloads\nnltk.download([\n    \"names\",\n    \"stopwords\",\n    \"averaged_perceptron_tagger\",\n    \"vader_lexicon\",\n    \"punkt\",\n])\n\n#configure jupyter to allow each cell to display multiple outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2c30c4ec":"#load tweet csv file and view 10 sampled rows\ntweets = pd.read_csv('..\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')\ntweets.sample(10)","fe552908":"# check if there are duplicated values\ntweets.duplicated().value_counts()\n\nlen(tweets) == tweets.duplicated().value_counts()[0]","bbaf59a4":"%%capture\n#drop NAN and NULL values\nx = tweets['hashtags'].dropna()\n\n#reset_index\nx.reset_index(drop=True, inplace=True)\n\n#extract hashtags from lists within table and place one by one into hashtag list\nfor i in np.arange(0,len(x)):\n    if i == 0:\n        z = list(x[i])\n        z.pop(0)\n        z.pop(len(z)-1)\n        blah = ''.join(z)\n        exec(f\"ht_list = [{blah}]\")\n    else:\n        z = list(x[i])\n        z.pop(0)\n        z.pop(len(z)-1)\n        z = ''.join(z)\n        exec(f\"y = [{z}]\")\n        ht_list.extend(y)\n\n#save into dataframe\nhashtags = pd.DataFrame(ht_list, columns = ['hashtags'])","d13caa4a":"hashtags","7fbea99c":"#create ranked barplot of first 50 hashtags\nfig = plt.figure(figsize = [5, 20]);\n\nht_counts = hashtags.value_counts().head(50).reset_index()\nsns.barplot(data=ht_counts, y='hashtags', x = 0);\nplt.title('Ranked Hashtags', fontsize=20)\nplt.yticks(fontsize=14, fontweight='bold');\nplt.xticks(fontsize=14);\nplt.xlabel('Count', fontsize=16);","c0da87c8":"#list frequencies\nht_freqdist=FreqDist(ht_list)\nht_freqdist.most_common(50)","ee6f4514":"#drop NAN and NULL values\ntweet_text = tweets['text'].dropna()\n\n#detect language of individual tweets and put into list\nfor i in np.arange(0, len(tweet_text)):\n    if i==0:\n        lang = [detect(tweet_text[i])]\n    else:\n        lang.extend([detect(tweet_text[i])])","cccd71ef":"#create count of unique languages\nlang_un = np.unique(lang, return_counts=True)","dc9871f7":"#load table to translate ISO 639-1 language codes to English Language names\nlang_codes=pd.read_csv('..\/input\/language-codes\/correctedMetadata.csv')\nlang_codes.sample(10)","d340eedd":"#translate ISO 639-1 language codes to English language names and place into dataframe with counts\na = []\nfor i in np.arange(0,len(lang_un[0])):\n    if lang_un[0][i] == 'en':\n        a.append(['English'])\n    else:\n        a.append(lang_codes.loc[lang_codes['Wikipedia.Language.Code'] == lang_un[0][i], 'Language.name..English.'].to_list())\nlang_tweet = pd.DataFrame(a, columns=['Language'])\ndel a\nlang_tweet['Count'] = lang_un[1]\nlang_tweet","8c1463d4":"#extract only tweets in english for this notebook\ntweet_text_en = []\nfor i in np.arange(0,len(lang)):\n    if lang[i] == 'en':\n        tweet_text_en.extend([tweet_text[i]])","cea60a90":"#initial ToktokTokenizer()\ntt = ToktokTokenizer()\n\n# NLTK.word_tokenize vs. NLTK.ToktokTokenizer()\nprint('Original Tweet - \\n\\n{}\\n\\n    NLTK.word_tokenize() -\\n\\n    {}\\n\\n    vs.\\n\\n    NLTK.ToktokTokenizer() -\\n\\n    {}' .format(tweet_text[0], word_tokenize(tweet_text[0]),tt.tokenize(tweet_text[0])))\n","1765d30f":"print(pos_tag(tt.tokenize(tweet_text[0])))","d0bdb7a6":"#This project will utilize the NLTK WordNetLemmatizer() to reduce words with common root to its root form.\ndef lemmatize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence","6651e62d":"print(lemmatize_sentence(tt.tokenize(tweet_text[0])))\nprint('\\n{}'.format(tweet_text[0]))","e01de0e2":"#look at raw tokens prior to cleaning to get an idea of what needs to go\nfdist=FreqDist(tt.tokenize(''.join(tweet_text)))\n\nfdist.most_common(50)","d668439d":"#Function to clean tweet text - discards tokens and characters that are incompatible with analyses and merges inconsequential redundant terminology \ndef remove_noise(tweet_tokens, stop_words = ()):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(tweet_tokens):\n        token = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n        token = re.sub(\"#pfizerbiontech\",\"\", token.lower())\n        token = re.sub(\"\u2026\",\"\", token.lower())\n        token = re.sub(\"#covid19\",\"covid19\", token.lower())\n        token = re.sub(\"\u2019\",\"\", token.lower())\n        token = re.sub(\"#vaccine\",\"vaccine\", token.lower())\n        token = re.sub(\"#covidvaccine\",\"vaccine\", token.lower())\n        token = re.sub(\"pfizer\",\"\", token.lower())\n        token = re.sub(\"#pfizer\",\"\", token.lower())\n        token = re.sub(\"covid\",\"covid19\", token.lower())\n        token = re.sub(\"#pfizervaccine\",\"vaccine\", token.lower())\n        token = re.sub(\"covid19-19\",\"covid19\", token.lower())\n        token = re.sub(\"covid1919\",\"covid19\", token.lower())\n        token = re.sub(\"&amp\",\"\", token.lower())\n        token = re.sub(\"amp\",\"\", token.lower())\n        token = re.sub(\"#vaccine\",\"vaccine\", token.lower())\n        token = re.sub(\"vaccination\",\"vaccine\", token.lower())\n        token = re.sub(\"vaccine.\",\"vaccine\", token.lower())\n        token = re.sub(\"#coronavirus\",\"covid19\", token.lower())\n        token = re.sub(\"vaccinate\",\"vaccine\", token.lower())\n        token = re.sub(\"#moderna\",\"\", token.lower())\n        token = re.sub(\"coronavirus\",\"covid19\", token.lower())\n        token = re.sub(\"covid19-19\",\"covid19\", token.lower())\n        token = re.sub(\"covid19vaccine\",\"vaccine\", token.lower())\n        token = re.sub(\"vaccined\",\"vaccine\", token.lower())\n        token = re.sub(\"#covid19vaccine\",\"vaccine\", token.lower())\n        token = re.sub(\"#vaccine\",\"vaccine\", token.lower())\n        token = re.sub(\"-biontech\",\"\", token.lower())\n        token = re.sub(\"#astrazeneca\",\"\", token.lower())\n        token = re.sub(\"#covid19\",\"covid19\", token.lower())\n        token = re.sub(\"#covid19_19\",\"covid19\", token.lower())\n        token = re.sub(\"covid19_19\",\"covid19\", token.lower())\n        token = re.sub(\"\/biontech\",\"\", token.lower())\n        token = re.sub(\"vaccine.\",\"vaccine\", token.lower())\n        token = re.sub(\"#biontech\",\"\", token.lower())\n        token = re.sub(\"#ergotron\",\"\", token.lower())\n\n        #Lemmatize tokens to root words\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n\n    #Need to handle '...' token discard in special manner. Doing so using re causes errors. The below two loops do it.\n    a = []\n    for i in np.arange(0,len(cleaned_tokens)):\n        if cleaned_tokens[i] == '...':\n            a.append(i)\n    for i in np.arange(0,len(a)):\n        if i == 0:\n            cleaned_tokens.pop(a[i])\n        else:\n            cleaned_tokens.pop(a[i]-i)\n    return cleaned_tokens","bf7d1489":"#load stopwords (these were downloaded using NLTK - see )\nstop_words = stopwords.words('english')\n\n#display output after cleaning and before for comparison\nprint(remove_noise(tt.tokenize(tweet_text[0]), stop_words))\nprint('\\n{}'.format(tweet_text[0]))","1f8fe5c4":"#clean tweet text and create bag of words for wordcloud figure\ntemp_text = ' '.join(tweet_text_en)\nclean_tweets = remove_noise(tt.tokenize(temp_text), stop_words)\nwordcloud_text = ' '.join(clean_tweets)\n\n#specify circular mask\nchar_mask = np.array(Image.open('..\/input\/circle-mask\/circle.png'))\n\n#create figure\nwordcloud=WordCloud(background_color='black', mask=char_mask).generate(wordcloud_text)\nfig = plt.figure(figsize=[20,20])\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show();","85cd9050":"fdist=FreqDist(clean_tweets)\n\ntweet_tok_freq = np.transpose(fdist.most_common(40))\n\n#convert string to int for plotting\nfdist_x = []\nfor i in np.arange(0,len(tweet_tok_freq[1])):\n    fdist_x.append(int(tweet_tok_freq[1][i]))\n    \nplt.figure(figsize=[25,20]);\nplt.title('40 most commonly used terms in Pfizer COVID19 Vaccine Tweets', fontsize=30, fontweight='bold')\nsns.barplot(x = fdist_x, y = tweet_tok_freq[0], palette='hsv');\nplt.yticks(fontsize=18, fontweight='bold');\nplt.xticks(fontsize=16);\nplt.xlabel('Count', fontsize=20, fontweight='bold');","65ac2ffb":"print('Within the {} english language tweets:\\n    Happy was used {} times.\\n    Death was used {} times.\\n    Hope was used {} times.\\n    Grateful was used {} times.\\n    Scared was used {} time.\\n    Stress was used {} time.\\n    Sad was used {} times.\\n    Angry was used {} time.\\n    Joy was used {} times.\\n    Upset was used {} times.'.format(len(tweet_text_en), fdist[\"happy\"], fdist[\"death\"], fdist[\"hope\"], fdist[\"grateful\"], fdist[\"scared\"], fdist[\"stress\"], fdist[\"sad\"], fdist[\"angry\"], fdist[\"joy\"], fdist[\"upset\"]))","c674ee15":"#load positive and negative words\nneg_lex = list(pd.read_csv('..\/input\/lexicons\/negative-lexicons.txt', sep=\"\\n\", header=None, encoding = \"ISO-8859-1\")[0])\npos_lex = list(pd.read_csv('..\/input\/lexicons\/positive-lexicons.txt', sep=\"\\n\", header=None, encoding = \"ISO-8859-1\")[0])","54649749":"#search through cleaned tweets for positive tokens and return index numbers\npos_ind = []\nfor i in np.arange(0,len(clean_tweets)):\n    for j in np.arange(0,len(pos_lex)):\n        if clean_tweets[i] == pos_lex[j]:\n            pos_ind.append(i)\n\n#search through cleaned tweets for negative tokens and return index numbers\nneg_ind = []\nfor i in np.arange(0,len(clean_tweets)):\n    for j in np.arange(0,len(neg_lex)):\n        if clean_tweets[i] == neg_lex[j]:\n            neg_ind.append(i)","fbfac229":"#create dataframe of positive token counts\npos_tok = []\nfor i in pos_ind:\n    pos_tok.append(clean_tweets[i])\nblah = pd.DataFrame(np.unique(pos_tok, return_counts=True)[0], columns = ['word'])\nblah['count'] = np.unique(pos_tok, return_counts=True)[1]\nblah.sort_values(by=['count'], ascending=False, inplace=True)\npos_tok = blah.copy()\ndel blah\n\n#create dataframe of negative token counts\nneg_tok = []\nfor i in neg_ind:\n    neg_tok.append(clean_tweets[i])\nblah = pd.DataFrame(np.unique(neg_tok, return_counts=True)[0], columns = ['word'])\nblah['count'] = np.unique(neg_tok, return_counts=True)[1]\nblah.sort_values(by=['count'], ascending=False, inplace=True)\nneg_tok = blah.copy()\ndel blah","a6c8e2b6":"plt.figure(figsize=[25,20]);\nplt.title('40 most commonly used positive terms in Pfizer COVID19 Vaccine Tweets', fontsize=30, fontweight='bold')\nsns.barplot(x = pos_tok.head(40).iloc[:,1], y = pos_tok.head(40).iloc[:,0], palette='hsv');\nplt.yticks(fontsize=18, fontweight='bold');\nplt.xticks(fontsize=16);\nplt.xlabel('Count', fontsize=20, fontweight='bold');\nplt.ylabel('');\nplt.xticks(fontsize=18);","99484b71":"plt.figure(figsize=[25,20]);\nplt.title('40 most commonly used negative terms in Pfizer COVID19 Vaccine Tweets', fontsize=30, fontweight='bold')\nsns.barplot(x = neg_tok.head(40).iloc[:,1], y = neg_tok.head(40).iloc[:,0], palette='hsv');\nplt.yticks(fontsize=18, fontweight='bold');\nplt.xticks(fontsize=16);\nplt.xlabel('Count', fontsize=20, fontweight='bold');\nplt.ylabel('');\nplt.xticks(fontsize=18);","0e1a02c8":"blah = nltk.collocations.TrigramCollocationFinder.from_words(remove_noise(tt.tokenize(tweet_text_en), stop_words))\nblah.ngram_fd.most_common(50)","e8c7e54a":"#initialize analyzer\nsia = SentimentIntensityAnalyzer()\n\n#intialize lists for storing scores\nneg = []\nneu = []\npos = []\ncompound = []\n\n#loop through each tweet\nfor i in np.arange(0, len(tweet_text_en)):\n    a = str(tweet_text_en[i])\n    b = sia.polarity_scores(a)\n    #store each score in respective list\n    neg.append(b['neg'])\n    neu.append(b['neu'])\n    pos.append(b['pos'])\n    compound.append(b['compound'])\n\n#place scores into dataframe for easy analysis    \npol_stats = pd.DataFrame(\n    {'neg': neg,\n     'neu': neu,\n     'pos': pos,\n     'compound': compound\n    })","065a627c":"#look at stats on score data\npol_stats['compound'].describe()\nprint('Compound Score Skew: {}'.format(round(pol_stats['compound'].skew(),3)))","1d80fa03":"#generate figure to describe scores\n\n#create fontdict for axis labels\naxlab2 = {'family': 'serif',\n              'color': 'black',\n              'weight': 'bold',\n              'size': 16\n         }\n\n# create figure with 4 subplots\nfig = plt.figure(figsize=[15,6])\nfig.suptitle(\"Sentiment Scores for Pfizer COVID-19 Vaccine Tweets\", fontsize=18, fontweight='bold')\nfig.subplots_adjust(top=0.92)\ngrid = plt.GridSpec(5, 1, wspace=0.3, hspace=0.1)\n\nax0 = plt.subplot(grid[0:4, 0]);\nsns.distplot(pol_stats['compound'], ax=ax0, color='dodgerblue');\na = ax0.axvline(pol_stats['compound'].median(),color= \"black\", linestyle=\"--\", label=\"median\");\nb = ax0.axvline(pol_stats['compound'].mean(),color= \"red\", linestyle=\"--\", label=\"mean\");\nc = ax0.axvline(min(pol_stats['compound'].mean()+ 3 * pol_stats['compound'].std(), 1),color= \"orange\", linestyle=\"--\", label=\"3sigma\");\nax0.axvline(max(pol_stats['compound'].mean()- 3 * pol_stats['compound'].std(), -1),color= \"orange\", linestyle=\"--\");\nd = ax0.axvline(min(pol_stats['compound'].mean()+ 2 * pol_stats['compound'].std(), 1),color= \"slategrey\", linestyle=\"--\", label=\"2sigma\");\nax0.axvline(max(pol_stats['compound'].mean()- 2 * pol_stats['compound'].std(), -1),color= \"slategrey\", linestyle=\"--\");\nplt.tick_params(\n    axis='x',          \n    which='both',      \n    bottom=False,      \n    top=False,         \n    labelbottom=False);\nax0.set_xlabel('', fontdict=axlab2);\nax0.set_xticks(np.arange(-1.5,1.6,0.5));\nax0.set_yticks([]);\nplt.legend([a, b, d, c], ['median', 'mean','2sigma','3sigma'], loc='upper center', bbox_to_anchor=(0.92, 1), fontsize=14) \n\n\nax1 = plt.subplot(grid[4, 0]);\nsns.boxplot(x=pol_stats['compound'], ax=ax1, color='honeydew');\nax1.axvline(pol_stats['compound'].median(),color= \"black\", linestyle=\"--\", label=\"median\");\nax1.axvline(pol_stats['compound'].mean(),color= \"red\", linestyle=\"--\", label=\"mean\");\nax1.axvline(min(pol_stats['compound'].mean()+ 3 * pol_stats['compound'].std(), 1),color= \"orange\", linestyle=\"--\", label=\"3sigma\");\nax1.axvline(max(pol_stats['compound'].mean()- 3 * pol_stats['compound'].std(), -1),color= \"orange\", linestyle=\"--\");\nax1.axvline(min(pol_stats['compound'].mean()+ 2 * pol_stats['compound'].std(), 1),color= \"slategrey\", linestyle=\"--\", label=\"2sigma\");\nax1.axvline(max(pol_stats['compound'].mean()- 2 * pol_stats['compound'].std(), -1),color= \"slategrey\", linestyle=\"--\");\nax1.set_xlabel('Compound Score', fontdict=axlab2);\nplt.xticks(fontsize=14);\nax1.set_xticks(np.arange(-1.5,1.6,0.5));\nax1.set_xticklabels([' ','-1.0','-0.5','0.0', '0.5', '1.0', ' '],fontdict={'color': 'black', 'size': 14});","e281b097":"<p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Extract and format hashtags to analyze<\/p>\n\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Hashtags are fomatted and tokenized for easier analysis and manipulation. Here, hashtags are separated and placed into a list employing standard python functions.<\/p>","d73c1cdc":"# <p style=\"font-size:28px; font-family:'Candara'; font-weight: bold\">Conclusion<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n    &nbsp;&nbsp;&nbsp;&nbsp;Based upon the analysis contained in this notebook and as demonstrated in the above sentiment scores, public sentiment concerning the Pfizer COVID-19 Vaccine demonstrate an overall neutral to positive sentiment. There is clearly some negative sentiment about the vaccine. Negative sentiments concern the death toll of covid-19, worry, delay, and various possible side effects, such as soreness, headaches, and fatigue. However, there is clearly more positive than negative sentiments based upon the data. Some of the standout positive sentiments about the vaccine are hope, thankfulness, gratefulness, approval, happiness, and trust.  \n<\/p>","4dcb526c":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">WordCloud Figure<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n    &nbsp;&nbsp;&nbsp;&nbsp;Here the tweet data is passed through the tokenizer and noise-removal function then a wordcloud figure generated from the output. The larger the text in the wordcloud figure, the greater the frequency of use in the tweets, combined. Visible terms like happy, grateful, great, and hope demonstrate positive sentiments about the vaccine. The only term that stands out as negative is the term side effect. A majority of the terms appear to be neutral.\n<\/p>","0b56b4aa":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">POS (Part Of Speach) Tagging<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n    &nbsp;&nbsp;&nbsp;&nbsp;Here, the pos_tag() function contained in the NLTK library is used to tag tokens by their respective parts (nouns, adjectives, verbs, etc.). For more information on POS Tagging, see <a href=http:\/\/www.nltk.org\/book\/ch05.html>http:\/\/www.nltk.org\/book\/ch05.html<\/a>\n<\/p>","01e77f72":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Function to Clean and Remove Noise<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n    &nbsp;&nbsp;&nbsp;&nbsp;Here a complete function for cleaning textual data and removing noise is implemented. The function takes as inputs tokenized tweet text and stopwords and outputs clean data that has been lemmatized and 'noise' removed. Noise here refers to the text that doesn't provide any insight into sentiment, such as certain symbols, words that essentially mean the same thing as other words that cannot be corrected using lemmatization (namely because they are slang), hashtags, and URLs. Regex substitution expressions are used to accomplish much of the noise correction in the textual data. All text is converted to lower case. As a final step, more basic methods are necessary to remove symbols that regex expressions could not handle, such as in this case, '...'.\n<\/p>","77e27962":"# <p style=\"font-size:36px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Sentiment Analysis of Pfizer COVID-19 Vaccine Tweets using VADER<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Analyst: Jordan Rich<br>KaggleID: JordanRich<\/p>\n\n<p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Notebook Description<\/p>\n\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this notebook, Pfizer COVID-19 Vaccine Tweets are explored employing various methods to understand the sentiments toward the vaccine. First, hashtags are explored to determine whether hashtag data should be included in sentiment scoring. The data is then cleaned and tokenized employing some very easy to use tools included in the NLTK (Natural Language Toolkit) library. Positive and negative lexicons contained within tweets are analyzed to get an understanding of sentiments. Finally, sentiment scoring is performed employing VADER (Valence Aware Dictionary and sEntiment Reasoner). VADER was selected as the lexicon and rule-based sentiment scoring tool for this project because it is specifically attuned to sentiments expressed in social media.<\/p>\n\n<p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Key Activities<\/p>\n    <ul>\n        <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Demonstrate how to analyze, tokenize, and clean text data obtained from Twitter<\/p><\/li> \n        <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Describe how to perform sentiment analysis using NLTK SentimentIntensityAnalyzer() and VADER<\/p><\/li>\n        <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Provide results from sentiment analysis specific to the Pfizer COVID-19 Vaccine Tweets<\/p><\/li>\n        <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Present conclusion of study<\/p><\/li>\n    <\/ul>\n<\/p>","d7fbe9dc":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Word Tokenization<\/p>\n\n<p style=\"font-size:20px; font-family:'Candara'; font-weight: bold; line-height:1.3\">In determining which tokenizer to use in this project to convert tweet text strings to words, retaining the format of URLs and hashtags is important to permit their discard at cleaning. Here two NLTK tokenization functions are compared for suitability in this project, word_tokenize() and ToktokTokenizer().<\/p>\n\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">In this case, word_tokenize() did not retain the hashtag and URL formatting, making it more difficult to discard hashtags and URLs from tweet text. ToktokTokenizer() is definitely the better solution for this project because it does retain the URL and hashtag formatting, making discard simpler at cleaining<\/p>","4bd94978":"# <p style=\"font-size:28px; font-family:'Candara'; font-weight: bold\">Sentiment Scoring Analysis<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n    &nbsp;&nbsp;&nbsp;&nbsp;Sentiment scores for each tweet are generated using NLTK SentimentIntensityAnalyzer() (SIA) using the VADER lexicon. The output of SIA is a dictionary containing negative, neutral, positive, and compound scores. To make analysis easy, scores are transferred into lists then compiled into a dataframe.\n<\/p>","f372675e":"# <p style=\"font-size:28px; font-family:'Candara'; font-weight: bold\">Perform EDA on tweet text<\/p>\n<ul>\n    <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Detect tweet languages using the langdetect library so that english tweets can be isolated.<\/p><\/li>\n    <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Tokenize text to words<\/p><\/li>\n    <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Clean text<\/p><\/li>\n    <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Analyze frequency distribution<\/p><\/li>\n<\/ul>","1aa5917d":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Language Detection<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n    &nbsp;&nbsp;&nbsp;&nbsp;Here the library langdetect is employed to analyze individual tweets and return the ISO 639-1 language codes. Using the language codes and a dataset created from the wikipedia page on ISO 639 language codes, the codes are translated to the english language name and then stored into a dataframe with the counts.\n<\/p>","e1c2d609":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Lemmatization<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n     &nbsp;&nbsp;&nbsp;&nbsp;The below function performs lemmatization, or i.e. converts tokens into their root words. Here the NLTK function WordNetLemmatizer() is employed. The lemmatize_sentence function, below, will later be incorporated into a different function that performs overall cleaning of the tweet text data.\n<\/p>","089cbe6e":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Collocation Analysis<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n    &nbsp;&nbsp;&nbsp;&nbsp;Below are the 50 most common trigram collocated terms (most frequent three terms collocated sequentially in tweet text).\n<\/p>","fd29398e":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Analysis of Positive and Negative Lexicons<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">\n    &nbsp;&nbsp;&nbsp;&nbsp;Here the tweets will be analyzed for positive and negative sentiment terms and frequency plots generated. The sentiment lexicons employed in this analysis were obtained at <a href=https:\/\/www.cs.uic.edu\/~liub\/FBS\/sentiment-analysis.html#datasets>https:\/\/www.cs.uic.edu\/~liub\/FBS\/sentiment-analysis.html#datasets<\/a>, \nwhich were compiled through the work described in:<\/p>\n<p style=\"font-size:16px; font-family:'Calibri Light'; line-height:1.1\">\nMinqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, Washington, USA\n<\/p>","794ba79a":"<p style=\"font-size:20px; font-family:'Calibri Light'; line-height:1.3; font-weight: bold\">Conclusions about hashtags<\/p>\n\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Hashtags do not appear to contain useful information for sentiment analysis. They predominately mention Pfizer, covid, vaccine (or a variation of these words) or governmental organizations associated with the COVID-19 pandemic response and management.<\/p>","702b072a":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Term Lookup using FreqDist()<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Below is a demonstration of how to lookup specific terms using the FreqDist function. This is very useful if key terms being investigated are known.<\/p>","ce3a6b09":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold\">Frequency Distribution of Common Terms<\/p>"}}