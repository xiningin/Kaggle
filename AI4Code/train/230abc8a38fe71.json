{"cell_type":{"5501e355":"code","93f26219":"code","1cb6b76f":"code","8864be97":"code","3e9043c0":"code","4d12ecfe":"code","f1134566":"code","91220c73":"code","e1f77b54":"code","dac4d17e":"code","ca65b757":"code","cdba52cd":"code","ac21182d":"code","e98bfe25":"code","b2a5fda1":"code","b474ca23":"code","18af12d4":"code","11e624f8":"code","44d8136b":"code","639304dd":"code","db8ece19":"code","59ae161b":"code","6f856b79":"code","d3a8573a":"code","7e06b627":"code","1f076976":"code","df247c2f":"code","4b375ceb":"code","995e4580":"code","4fb498a0":"code","2a1dca00":"code","70034cca":"code","d5fddd20":"code","fec0fd81":"code","145b4a61":"code","7774a3ef":"code","bbe81c4a":"code","027e4f3c":"code","78e081df":"code","4cb6ae23":"code","5173010a":"code","6d83ee11":"code","73ba6c48":"code","fb4edeb3":"code","53734540":"code","cba1e185":"code","ef903ed1":"code","c9e55c68":"code","686d6691":"code","c4719293":"code","f7ad70e5":"code","a12f2672":"code","9b26e322":"code","2a55b7f9":"code","9b2fc7ee":"code","da19c44c":"code","dfcc61c1":"code","23df7843":"code","f4e8f0a1":"code","fd0f86e7":"code","e23b84c6":"code","19e414ac":"code","1bb300e6":"code","ddb0b42e":"code","84b34b97":"code","a3528a02":"code","7b31e297":"code","7f74a97a":"code","456aaedc":"code","c3ef928a":"code","c3ef0962":"code","fa146ff8":"code","2928022f":"code","24f47b5c":"code","c781dc08":"code","2f68ff4d":"code","1cb841d8":"code","d09a7ac0":"code","734b8633":"code","33937d35":"code","1dfec4e6":"code","fc987c30":"code","36fe4222":"code","a49a98a2":"code","85bcd677":"code","2715dad3":"code","ef4a0075":"code","7c6a0935":"markdown","d1f9b677":"markdown","e5472f25":"markdown","51126ed5":"markdown","5dd9dd8e":"markdown","56aca156":"markdown","7ad6d380":"markdown","72d62dcc":"markdown","7c1584e5":"markdown","03756b64":"markdown","69dd5b19":"markdown","80526eff":"markdown","ff493b05":"markdown","abab666f":"markdown","f088da00":"markdown","2bfe8dc2":"markdown","1aed7d47":"markdown","64e80267":"markdown","5eb86925":"markdown","c54a2a01":"markdown","86a668bc":"markdown","b0367913":"markdown","ad2c7b62":"markdown","711f01c5":"markdown","360e7d24":"markdown","155356fa":"markdown","8a1b7b10":"markdown","858aa9f4":"markdown","4b3cfa22":"markdown","e401f6f6":"markdown","3c40bcb5":"markdown","99c53a54":"markdown","9231992e":"markdown","792da187":"markdown","7486fdb8":"markdown","7d587b0e":"markdown","684ea926":"markdown","4f56ef97":"markdown","1d97f367":"markdown","dce177c8":"markdown","ae1b5cf8":"markdown","57b79773":"markdown","f001bebd":"markdown","e3b36840":"markdown","a433faa3":"markdown","9a817b2b":"markdown","a9e7846f":"markdown","259c6936":"markdown","c13ece38":"markdown","fba1d193":"markdown","2c773283":"markdown","5f59d9df":"markdown","ddebd5bd":"markdown","5560f11c":"markdown","8bd25ed5":"markdown","c1b1de15":"markdown","62f08571":"markdown","b37fa790":"markdown","ba610c6b":"markdown","c5fe605b":"markdown","dae9b9e9":"markdown","90e37dd7":"markdown","3e60444d":"markdown","2620fe38":"markdown","d5d87d93":"markdown","196013ea":"markdown","d566b9f9":"markdown","09695b80":"markdown","0e0908c4":"markdown","ceedb060":"markdown","104830dc":"markdown","5d6d77e1":"markdown","da616d0c":"markdown","acb319c4":"markdown","6fcc2b9c":"markdown","ff842596":"markdown","e0ae6123":"markdown","9e48af0a":"markdown","665d6514":"markdown","65e8b050":"markdown","3fb1e6b8":"markdown","bdabede1":"markdown","ab9d24ba":"markdown","b5d67829":"markdown","8e3462ae":"markdown","7b9c2c3d":"markdown","2cee7f81":"markdown","e2717307":"markdown","6d15e3c8":"markdown","34a11bcb":"markdown","0cb59131":"markdown","c862c4ab":"markdown","505a15e5":"markdown"},"source":{"5501e355":"# data processing tools\nimport pandas as pd\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import argmax\nfrom math import sqrt\nimport time\n\n# preprocessing\n# from missingpy import MissForest # commented out for Kaggle\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n#from sklearn.experimental import enable_halving_search_cv # commented out for Kaggle\nfrom sklearn.model_selection import train_test_split, cross_validate,KFold #  HalvingGridSearchCV, # commented out for Kaggle\n\n# scoring and algorithm selection packages\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\n\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, GammaRegressor, Lars, Lasso, SGDRegressor\nfrom sklearn.linear_model import LassoLars, OrthogonalMatchingPursuit, PassiveAggressiveRegressor, PoissonRegressor, RANSACRegressor, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR, LinearSVR, NuSVR\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nimport xgboost as xgb\n\n# visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# ignore warnings (gets rid of Pandas copy warnings)\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n","93f26219":"def iqr_outliers(column, iqr_level):\n    \"\"\"return the lower range and upper range for the data based on IQR\n    Arguments: \n    column - column to be evaluated\n    iqr_level - iqr range to be evaluated\n    \"\"\"\n    Q1,Q3 = np.percentile(column , [25,75])\n    iqr = Q3 - Q1\n    lower_range = Q1 - (iqr_level * iqr)\n    upper_range = Q3 + (iqr_level * iqr)\n    return lower_range,upper_range  ","1cb6b76f":"def grid_optimizer(model, grid, x, y):\n    \n    '''Takes in a model and a grid of hyperparameters, and runs a HalvingGridSearch\n    arguments: \n    model - the model to be grid searched\n    parameter grid - all parameters to grid search\n    x, y - train features and target to be tested in search\n    returns: best parameters'''\n    \n    start=time.time()\n    \n    print(\"Making Search\")\n    grid_search = HalvingGridSearchCV(model, grid, verbose=10, scoring='neg_mean_absolute_error', cv=3, min_resources='exhaust')\n\n    print(\"Running Grid\")\n    grid_search.fit(x, y)\n\n    grid_search.best_estimator_\n    \n    # Best f1\n    print('Best mae: %.3f' % grid_search.best_score_)\n\n    print(\"Best parameters set found on train set: \\n\")\n    print(grid_search.best_params_)\n    print(\"\\nGrid scores on train set:\\n\")\n    means = grid_search.cv_results_['mean_test_score']\n    stds = grid_search.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\n    \n    print(f'Elapsed Time: {time.time() - start}')\n    \n    return grid_search.best_params_","8864be97":"def score_model(model, x, y, model_type, score_list):\n    '''Takes in an instantiated model along with x, y and scores it, then appends to list\n    Arguments:\n    model: tuned\/instantiated model\n    x, y: train features and targets to be scored with CV\n    model_type: label with name of model\n    score_list: score list which is then returned back\n    Returns: List with scores appended\n    '''\n    # get accuracy cross val score for cv 5\n    scores = cross_validate(model, x, y, cv=5, n_jobs=-1,\n        scoring=('r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error'),\n        return_train_score=True)\n    r2 = round(scores['test_r2'].mean()*100,2)\n    mae = round(scores['test_neg_mean_absolute_error'].mean(), 4)\n    mrse = round(scores['test_neg_root_mean_squared_error'].mean(), 4)\n    \n    print(\"\\n\\n\",model_type,\" scores\")\n    print(\"CV 5 R2 Train Score: {}\".format(r2))\n    print(\"CV 5 MAE Train Score: {}\".format(mae))\n    print(\"CV 5 RMSE Train Score: {}\".format(mrse))\n\n    # append our scores to our lists\n    score_list['model'].append(model_type)\n    score_list['r2'].append(r2)\n    score_list['mae'].append(mae)\n    score_list['rmse'].append(mrse)\n    \n    return score_list","3e9043c0":"def train_oof_predictions(x, y, models, verbose=True):\n    '''Function to perform Out-Of-Fold predictions on train data\n    returns re-ordered predictors x, re-ordered target y, and model dictionary with filled predictors\n    Parameters:\n    x: training predictors\n    y: training targets\n    models: dictionary of models in form of 'model name' : [instantiated model, predictors list]\n    verbose: whether to print the sequence of inclusions(True recommended)\n    '''\n    \n    # instantiate a KFold with 10 splits\n    kfold = KFold(n_splits=10, shuffle=True, random_state=randomstate)\n    \n    # prepare lists to hold the re-ordered x and y values\n    data_x, data_y  = [], []\n    \n    # run the following block for each of the 10 kfold splits\n    for train_ix, test_ix in kfold.split(x, y):\n    \n        if verbose: print(\"\\nStarting a new fold\\n\")\n    \n        if verbose: print(\"Creating splits\")\n        #create this fold's training and test sets\n        train_X, test_X = x[train_ix], x[test_ix] \n        train_y, test_y = y[train_ix], y[test_ix]\n    \n        if verbose: print(\"Adding x and y to lists\\n\")\n        # add the data that is used in this fold to the re-ordered lists\n        data_x.extend(test_X)\n        data_y.extend(test_y)\n    \n        # run each model on this kfold and add the predictors to the model's running predictors list\n        for item in models:\n            \n            label = item # get label for reporting purposes\n            model = models[item][0] # get the model to use on the kfold\n        \n            # fit and make predictions \n            if verbose: print(\"Running\",label,\"on this fold\")\n            model.fit(train_X, train_y) # fit to the train set for the kfold\n            predictions = model.predict(test_X) # fit on the out-of-fold set\n            models[item][1].extend(predictions) # add predictions to the model's running predictors list\n    \n    return data_x, data_y, models","4d12ecfe":"def model_selector(X, y, meta_model, models_dict, model_label, verbose=True):\n    \n    \"\"\" \n    Perform a forward model selection based on MAE improvement\n    Parameters:\n        X - baseline X_train with all features\n        y - baseline y_train with all targets\n        meta_model - meta_model to be trained\n        models_dict - dictionary of models in format of 'model name' : [model object, out-of-fold predictions]\n        label - the label for the current meta model\n        verbose - whether to print the sequence of inclusions(True recommended)\n    Returns: list of selected models, best MAE \n    \"\"\"\n\n    print(\"\\n\\nRunning model selector for \", model_label)\n    included_models = []\n     \n    while True:\n        changed=False\n        \n        # forward step\n        \n        if verbose: print(\"\\nNEW ROUND - Setting up score charts\")\n        excluded_models = list(set(models_dict.keys())-set(included_models)) # make a list of the current excluded_models\n        if verbose: print(\"Included models: {}\".format(included_models))\n        if verbose: print(\"Exluded models: {}\".format(excluded_models))\n        new_mae = pd.Series(index=excluded_models) # make a series where the index is the current excluded_models\n        \n        current_meta_x = np.array(X)\n        \n        if len(included_models) > 0:\n            for included in included_models:\n                included = np.array(models_dict[included][1]).reshape((len(models_dict[included][1]), 1))\n                current_meta_x = np.hstack((current_meta_x, included))\n\n        # score the current model\n        scores = cross_validate(meta_model, current_meta_x, y, cv=5, n_jobs=-1, scoring=('neg_mean_absolute_error'))\n        starting_mae = round(scores['test_score'].mean(),4)\n        if verbose: print(\"Starting mae: {}\\n\".format(starting_mae))\n        \n       \n        for excluded in excluded_models:  # for each item in the excluded_models list:\n            \n            new_yhat = np.array(models_dict[excluded][1]).reshape(-1, 1) # get the current item's predictions\n            meta_x = np.hstack((current_meta_x, new_yhat)) # add the predictions to the meta set\n            \n            # score the current item\n            scores = cross_validate(meta_model, meta_x, y, cv=5, n_jobs=-1, scoring=('neg_mean_absolute_error'))\n            mae = round(scores['test_score'].mean(),4)\n            if verbose: print(\"{} score: {}\".format(excluded, mae))\n            \n            new_mae[excluded] = mae # append the mae to the series field\n        \n        best_mae = new_mae.max() # evaluate best mae of the excluded_models in this round\n        if verbose: print(\"Best mae: {}\\n\".format(best_mae))\n        \n        if best_mae > starting_mae:  # if the best mae is better than the initial mae\n            best_feature = new_mae.idxmax()  # define this as the new best feature\n            included_models.append(str(best_feature)) # append this model name to the included list\n            changed=True # flag that we changed it\n            if verbose: print('Add  {} with mae {}\\n'.format(best_feature, best_mae))\n        else: changed = False\n        \n        if not changed:\n            break\n            \n    print(model_label, \"model optimized\")\n    print('resulting models:', included_models)\n    print('MAE:', starting_mae)\n    \n    return included_models, starting_mae","f1134566":"def create_meta_dataset(data_x, items):\n    '''Function that takes in a data set and list of predictions, and forges into one dataset\n    parameters:\n    data_x - original data set\n    items - list of predictions\n    returns: stacked data set\n    '''\n    \n    meta_x = data_x\n    \n    for z in items:\n        z = np.array(z).reshape((len(z), 1))\n        meta_x = np.hstack((meta_x, z))\n        \n    return meta_x","91220c73":"def stack_prediction(X_test, final_models): \n    '''takes in a test set and a list of fitted models.\n    Fits each model in the list on the test set and stores it in a predictions list\n    Then sends the test set and the predictions to the create_meta_dataset to be combined\n    Returns: combined meta test set\n    Parameters:\n    X_test - testing dataset\n    final_dict - list of fitted models\n    '''\n    predictions = []\n    \n    for item in final_models:\n        print(item)\n        preds = item.predict(X_test).reshape(-1,1)\n        predictions.append(preds)\n    \n    meta_X = create_meta_dataset(X_test, predictions)\n        \n    return meta_X","e1f77b54":"# I load in my dataframe as always\ndf = pd.read_csv('..\/input\/diamonds\/diamonds.csv')\ndf","dac4d17e":"# Drop the first unneeded row, and check the shape of our dataframe.\n\ndf.drop('Unnamed: 0', axis=1, inplace=True)\n\ndf.shape","ca65b757":"# What kind of info is stored in the dataframe?\ndf.info()","cdba52cd":"# Any missing values?\ndf.isna().sum()","ac21182d":"df.drop_duplicates(keep='last', inplace=True)","e98bfe25":"df.describe()","b2a5fda1":"# We look at our histograms\ndf.hist(figsize=(20,10));","b474ca23":"# 99.4 percentile on carat\nnp.percentile(df.carat, [99.4])","18af12d4":"# determing our IQR range for carat size\ncaratlower,caratupper = iqr_outliers(df.carat, 1.94)\n\ncaratlower, caratupper","11e624f8":"# dropping the things outside of our lower and upper range\ndf.drop(df[ (df.carat > caratupper) | (df.carat < caratlower) ].index , inplace=True)","44d8136b":"df.loc[df['x']==0, 'x'] = None\ndf.loc[df['y']==0, 'y'] = None\ndf.loc[df['z']==0, 'z'] = None","639304dd":"# Now we DO have NaN values - but now that's on purpose!\ndf.isna().sum()","db8ece19":"# There are clearly some problematically high numbers in our \"y\" and \"z\" columns. We're going to get fix these outliers.\ndf.sort_values('z',ascending=False)","59ae161b":"# Checking the y outliers as well\ndf.sort_values('y',ascending=False)","6f856b79":"# Set the outliers to NaN for later imputation\ndf.loc[[24067, 49189], 'y'] = None\ndf.loc[48410, 'z'] = None","d3a8573a":"# Recheck our distribution\ndf.describe()","7e06b627":"df.hist(figsize=(20,10));","1f076976":"#histogram and normal probability plot\nsns.distplot(df['price'], fit=norm);\nfig = plt.figure()\n\nres = stats.probplot(df['price'], plot=plt)\n\n# our sales price histogram is positively skewed and has a high peak\n# Our QQ-plot shows that we have heavy tails with right skew","df247c2f":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df['price'].skew())\nprint(\"Kurtosis: %f\" % df['price'].kurt())\n\n# price is highly right skewed\n# very positive kurtosis, indicating lots in the tails. We can see those tails in skew.","4b375ceb":"# log transform our target price to improve normality of distribution\ndf_target_log = np.log(df['price'])\n\n#histogram and normal probability plot\nsns.distplot(df_target_log, fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_target_log, plot=plt)\n\n# Our target price is more normally distributed when log transformed, so we'll be doing that","995e4580":"# look for multicollinearity of features\nfig, ax = plt.subplots(figsize=(20, 20))\n\n# get the correlations for our train data\ndf_c = df.corr()\n\n# we want our heatmap to not show the upper triangle, which is redundant data\n# get a mask for the upper diagonal\ndf_c_mask = np.triu(np.ones_like(df_c, dtype=bool))\n\n# adjust mask and df to hide center diagonal\ndf_c_mask = df_c_mask[1:, :-1]\ncorr = df_c.iloc[1:,:-1].copy()\n\n# color map\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# plot heatmap\nsns.heatmap(corr, mask=df_c_mask, annot=True, fmt=\".2f\", cmap=cmap,\n           vmin=-1, vmax=1, cbar_kws={\"shrink\": .8}, square=True)\n\n# title\nplt.title('PEARSON CORRELATION MATRIX', fontsize=18)\n\nplt.show()","4fb498a0":"df.corr()","2a1dca00":"#Get our list of highly correlated feature pairs with following steps:\n\n# save correlation matrix as a new data frame\n# converts all values to absolute value\n# stacks the row:column pairs into a multindex\n# reset the index to set the multindex to seperate columns\n# sort values. 0 is the column automatically generated by the stacking\ndf_correlations = df.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n\n# zip the variable name columns in a new column named \"pairs\"\ndf_correlations['pairs'] = list(zip(df_correlations.level_0, df_correlations.level_1))\n\n# set index to pairs\ndf_correlations.set_index(['pairs'], inplace = True)\n\n# rename our results column to correlation\ndf_correlations.rename(columns={0: \"correlation\"}, inplace=True)\n\n# Drop 1:1 correlations to get rid of self pairs\ndf_correlations.drop(df_correlations[df_correlations['correlation'] == 1.000000].index, inplace=True)\n\n# view pairs above 70% correlation and below 90% correlation (engineered features will correlate with each other above 95%)\ndf_correlations[(df_correlations.correlation>.75) & (df_correlations.correlation<.99)]","70034cca":"categoricals = ['cut', 'color', 'clarity']\n\n# prepare a new dataframe that has the correct categorical columns\ndf_categoricals = df[categoricals]\n\n# telling Pandas that these columns are categoricals\nfor item in categoricals:\n    df_categoricals[item] = df_categoricals[item].astype('category')\n\n# adding price to our dataframe so that we can do some visualizations    \ndf_categoricals['price'] = df['price']\n\n# plot our categoricals as box plots vs price\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n    \nf = pd.melt(df_categoricals, id_vars=['price'], value_vars=categoricals)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"price\")","d5fddd20":"continuous = ['carat', 'depth', 'table', 'x', 'y', 'z']\ndf_continuous = df[continuous]\ndf_continuous['price'] = df['price']\n\n\n# check linearity of continuous predictors\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,15), sharey=True)\n\nfor ax, column in zip(axes.flatten(), df_continuous.columns):\n    ax.scatter(df_continuous[column], df_continuous['price']\/100000, label=column, alpha=.1)\n    ax.set_title(f'Sale Price vs {column}')\n    ax.set_xlabel(column)\n    ax.set_ylabel('Sale Price in $100,000')\n\nfig.tight_layout()","fec0fd81":"# check linearity of continuous predictors\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,15), sharey=True)\n\nfor ax, column in zip(axes.flatten(), df_continuous.columns):   \n    carat_price = df.groupby(column)['price'].mean()\n    ax.scatter(carat_price.index, carat_price)\n    ax.set_title(f'Sale Price vs {column}')\n    ax.set_xlabel(column)\n    ax.set_ylabel('Sale Price in $1,000')\n\nfig.tight_layout()","145b4a61":"y = np.log(df['price'])\nx = df.drop('price', axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)\n\n# reset indices to prevent any index mismatches\nx_train.reset_index(inplace=True, drop=True)\nx_test.reset_index(inplace=True, drop=True)\ny_train.reset_index(inplace=True, drop=True)\ny_test.reset_index(inplace=True, drop=True)\n\n# exponentiate our y to get an actual price\ny_actual = np.exp(y_test)","7774a3ef":"# continuous pipeline\ncont_pipeline = Pipeline([\n    ('imputer', MissForest()),\n    ('scaler', StandardScaler()),\n    ('polynomials', PolynomialFeatures(degree=2, include_bias=False))\n    ])\n\n# Whole pipeline with continuous then categorical transformers\ntotal_pipeline = ColumnTransformer([\n    ('continuous', cont_pipeline, continuous),\n    ('categorical', OneHotEncoder(), categoricals)\n    ])","bbe81c4a":"# Fit and tranform the pipeline on x_train, then transform x_test\nx_train = total_pipeline.fit_transform(x_train)\nx_test = total_pipeline.transform(x_test)","027e4f3c":"# Our outputs of the pipeline are numpy arrays; we'll need the columns for feature selection, so we make these back into dataframes\nx_train = pd.DataFrame(x_train)\nx_test = pd.DataFrame(x_test)","78e081df":"# prepare dictionary to store results\nmodels = {}\nmodels['model'] = []\nmodels['r2'] = []\nmodels['mae'] = []\nmodels['rmse'] = [] \n\n# pick our random seed\nrandomstate = 42","4cb6ae23":"# Set up spot check models\nbaseline_models = {\n    \"LR\": LinearRegression(),\n    \"Decision Tree\": DecisionTreeRegressor(random_state = randomstate),\n    \"Elastic Net\": ElasticNet(random_state = randomstate, tol=1e-3),#copy_X=True, normalize=False, \n    \"Extra Trees\": ExtraTreesRegressor(random_state = randomstate),\n    \"Gradient Boosted\" : GradientBoostingRegressor(random_state = randomstate),\n    \"KNN\" : KNeighborsRegressor(),\n    \"Lars\" : Lars(random_state = randomstate, normalize=False), #copy_X=True, \n    \"Lasso\" : Lasso(random_state = randomstate, tol=1e-3), #copy_X=True, \n    \"LinearSVR\" : LinearSVR(random_state = randomstate),\n    \"MLPRegressor\" : MLPRegressor(random_state = randomstate),\n    \"Bayesian Ridge\" : BayesianRidge(),\n    \"Gamma Regressor\" : GammaRegressor(),\n    \"Lasso Lars\" : LassoLars(random_state = randomstate), #, copy_X=True, normalize=False\n    \"Nu SVR\": NuSVR(),\n    \"Orthogonal Matching Pursuit\" : OrthogonalMatchingPursuit(),\n    \"Passive Aggressive\" : PassiveAggressiveRegressor(random_state = randomstate),\n    \"RANSAC\" : RANSACRegressor(random_state = randomstate),\n    \"Ridge\" : Ridge(random_state = randomstate, tol=1e-3, normalize=False),\n    \"SVR\" : SVR(),\n    \"XGB Regressor\" : xgb.XGBRegressor(random_state = randomstate),\n    \"Ada Boost\" : AdaBoostRegressor(random_state=randomstate),\n    \"SGD Regressor\" : SGDRegressor(random_state=randomstate),\n    \"Random Forest Regressor\" : RandomForestRegressor(random_state=randomstate)\n\n}\n\n# run spot check on each model inline\nfor model in baseline_models:\n    this_model = baseline_models[model]\n    label = model\n    spot_check = score_model(this_model, x_train, y_train, label, models)","5173010a":"perm_model = LinearRegression()\nperm_model.fit(x_train, y_train)\n\n\nr = permutation_importance(perm_model, x_train, y_train,\n                           n_repeats=15,\n                            random_state=0,\n                          n_jobs=-1)\n\nimportances = {}\n\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] >= 0.001:\n        importances[x_train.columns[i]] = r.importances_mean[i]\n    else: continue\n        \nimportances\n\nimportant_features = list(importances.keys())","6d83ee11":"X_train_perm = x_train[important_features]\nmodels = score_model(perm_model, X_train_perm, y_train, \"LR Perm Importance\", models)","73ba6c48":"model = ExtraTreesRegressor(random_state = randomstate)\nX_train_perm = x_train[important_features]\nmodels = score_model(model, X_train_perm, y_train, \"Extra Trees Perm Importance\", models)","fb4edeb3":"model = xgb.XGBRegressor(random_state = randomstate)\nX_train_perm = x_train[important_features]\nmodels = score_model(model, X_train_perm, y_train, \"XGB Perm Importance\", models)","53734540":"# make data frame from our models dictionary\ntarget = pd.DataFrame(models).reset_index(drop=True)\n\n# sort data frame by mae and reset index\ntarget.sort_values('mae', ascending=False).head(20)","cba1e185":"# initialize empty lists for the storage of out-of-fold predictions\n\nxgbr_yhat, gradient_boost_yhat, extra_trees_yhat, decision_tree_yhat, neighbor_yhat, svr_yhat, nu_svr_yhat = [], [], [], [], [], [], []\north_yhat, linreg_yhat, ridge_yhat, bay_ridge_yhat, mlp_yhat = [], [], [], [], []","ef903ed1":"# instantiate tuned models\n\nxgbr = xgb.XGBRegressor(\n                n_estimators = 500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 10,\n                eta = .05,\n                min_child_weight = 2,\n                colsample_bytree = .8,\n                reg_alpha = 1e-05,\n                )\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                max_depth = 8,\n                learning_rate=.05,\n                loss='ls',\n                subsample=.7,\n                )\n\nextra_trees = ExtraTreesRegressor(random_state = randomstate, \n                                  n_estimators=100,\n                                  criterion='mse',\n                                  max_depth=25,\n                                  max_features='auto'\n                                  \n                                 )\n\ndecision_tree = DecisionTreeRegressor(\n                random_state=randomstate,\n                criterion = 'squared_error',\n                max_depth=15,      \n            )\n\n\nneighbor = KNeighborsRegressor(n_neighbors = 4)\nsvr = SVR(gamma = 'auto', kernel = 'rbf', C=10, epsilon=.05)\nnu_svr = NuSVR(kernel = 'rbf', gamma = 'auto')\north = OrthogonalMatchingPursuit()\nlinreg = LinearRegression()\nridge = Ridge(random_state = randomstate, tol=1e-3, normalize=False, solver='auto')\nbay_ridge = BayesianRidge(alpha_init=50, normalize=False)\nmlp = MLPRegressor(random_state = randomstate, solver='lbfgs', activation='logistic', alpha=1, hidden_layer_sizes=100)","c9e55c68":"# create the model dictionary to do out-of-fold predictions and model stack testing\n\nmodels_dict = {'XGB' : [xgbr, xgbr_yhat],\n            'Gradient Boosted' : [gradient_boost, gradient_boost_yhat],\n            'Extra Trees' : [extra_trees, extra_trees_yhat],\n            'Decision Tree' : [decision_tree, decision_tree_yhat],\n            'KNN' : [neighbor, neighbor_yhat],\n            'SVR' : [svr, svr_yhat],\n            'Nu SVR': [nu_svr, nu_svr_yhat],\n            'Orthogonal': [orth, orth_yhat],\n            'Linear Regression' : [linreg, linreg_yhat],\n            'Ridge' : [ridge, ridge_yhat],\n            'Bayesian Ridge': [bay_ridge, bay_ridge_yhat],\n            'MLP Perceptron': [mlp, mlp_yhat]\n              }","686d6691":"# run the out-of-fold predictions\n# This takes a long time!!\n\ndata_x, data_y, models_dict = train_oof_predictions(x_train, y_train, models_dict)","c4719293":"# Set up a scoring dictionary to hold the model stack selector results\nscores = {}\nscores['Model'] = []\nscores['MAE'] = []\nscores['Included'] = []\n\n# Run the model stack selector for each model in our models_dict\n\nfor model in models_dict:\n    \n    label = model\n    meta_model = models_dict[model][0]\n\n    resulting_models, best_mae = model_selector(data_x, data_y, meta_model, models_dict, label, verbose=True)\n    \n    scores['Model'].append(model)\n    scores['MAE'].append(best_mae)\n    scores['Included'].append(resulting_models)","f7ad70e5":"# Look at the scores of our model combinations\n\nbest_model = pd.DataFrame(scores).reset_index(drop=True)\nbest_model.sort_values('MAE', ascending=False)","a12f2672":"# make a list with the oof predictions from the stack models that we plan to use in our stack\nyhat_predics = [models_dict['XGB'][1],  models_dict['Extra Trees'][1], models_dict['Gradient Boosted'][1],]\n\n\n# create the meta data set using the oof predictions\nmeta_X_train = create_meta_dataset(data_x, yhat_predics)","9b26e322":"# Fit the models to be used in the stack on the base Train datasets\nprint(\"Fitting Models\")\n\nprint(\"Fitting XGB\")\nxgbr.fit(x_train, y_train)\n\nprint(\"Fitting Extra Trees\")\nextra_trees.fit(x_train, y_train)\n\nprint(\"Fitting Gradient Boost\")\ngradient_boost.fit(x_train, y_train)","2a55b7f9":"# Make a list holding the fitted models to be used in the stack\nfinal_models = [xgbr, extra_trees, gradient_boost]\n\n# Create the test set, including the predictions of the stack models\nmeta_X_test = stack_prediction(x_test, final_models)","9b2fc7ee":"# Check our meta model score\n\nlinreg = LinearRegression()\n\nlinreg.fit(x_train, y_train)\npredictions = linreg.predict(x_test)\n\npred_exp = np.exp(predictions)\nactual = np.exp(y_test)\n\nprint(\"MAE: \",int(mean_absolute_error(pred_exp, actual)))\nprint(\"RMSE:\",int(np.sqrt(mean_squared_error(pred_exp, actual))))\nprint(\"R2:\", r2_score(pred_exp, actual)*100)","da19c44c":"# Check out our best scoring spot check model, too!\n\ntrees = extra_trees = ExtraTreesRegressor(random_state = randomstate, \n                                  n_estimators=100,\n                                  criterion='mse',\n                                  max_depth=25,\n                                  max_features='auto'\n                                  \n                                 )\n\ntrees.fit(x_train, y_train)\npredictions = trees.predict(x_test)\n\npred_exp = np.exp(predictions)\nactual = np.exp(y_test)\n\nprint(\"MAE: \",int(mean_absolute_error(pred_exp, actual)))\nprint(\"RMSE:\",int(np.sqrt(mean_squared_error(pred_exp, actual))))\nprint(\"R2:\", r2_score(pred_exp, actual)*100)","dfcc61c1":"# Instantiate the chosen meta model\nmeta_model =  LinearRegression()\n\n# fit the meta model to the Train meta dataset\n# There is no data leakage in the meta dataset since we did all of our predictions out-of-sample\nmeta_model.fit(meta_X_train, data_y)\npredictions = meta_model.predict(meta_X_test)\n\npred_exp = np.exp(predictions)\nactual = np.exp(y_test)\n\nprint(\"MAE: \",int(mean_absolute_error(pred_exp, actual)))\nprint(\"RMSE:\",int(np.sqrt(mean_squared_error(pred_exp, actual))))\nprint(\"R2:\", r2_score(pred_exp, actual)*100)","23df7843":"param_grid = {\"max_depth\": [5, 10],\n              \"min_child_weight\" : [2, 7],\n              'eta': [.05, .1],\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1)\n\nbest_params = grid_optimizer(xgbr, param_grid, x_train, y_train)","f4e8f0a1":"param_grid = {\"max_depth\": [8, 10, 12],\n              \"min_child_weight\" : [2, 4],\n              'eta': [.01, .05],\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1)\n\nbest_params = grid_optimizer(xgbr, param_grid, x_train, y_train)","fd0f86e7":"param_grid = {'gamma': [0, .4, .8],\n              'subsample':[.2, .6, 1],\n\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 10,\n                eta = .05,\n                min_child_weight = 2)\n\nbest_params = grid_optimizer(xgbr, param_grid, x_train, y_train)","e23b84c6":"param_grid = {'colsample_bytree':[.2, .4, .6, .8, 1],\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 10,\n                eta = .05,\n                min_child_weight = 2,\n                )\n\nbest_params = grid_optimizer(xgbr, param_grid, x_train, y_train)","19e414ac":"param_grid = {'reg_alpha':[1e-5, 1, 100],\n              'reg_lambda':[1e-5, 1, 100],\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 10,\n                eta = .05,\n                min_child_weight = 2,\n                colsample_bytree = .8,\n                )\n\nbest_params = grid_optimizer(xgbr, param_grid, x_train, y_train)","1bb300e6":"param_grid = {'n_estimators' : [500, 750, 1000],\n              }\n\nxgbr = xgb.XGBRegressor(\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 10,\n                eta = .05,\n                min_child_weight = 2,\n                colsample_bytree = .8,\n                reg_alpha = 1e-05,\n                )\n\nbest_params = grid_optimizer(xgbr, param_grid, x_train, y_train)","ddb0b42e":"xgbr = xgb.XGBRegressor(\n                n_estimators = 500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 10,\n                eta = .05,\n                min_child_weight = 2,\n                colsample_bytree = .8,\n                reg_alpha = 1e-05,\n                )","84b34b97":"param_grid = {\"max_depth\": [15, 25],\n              'max_features':['auto','sqrt']\n              }\n\nextra_trees = ExtraTreesRegressor(random_state = randomstate, \n                                  n_estimators=100,\n                                  criterion='squared_error',\n                                  \n                                 )\n\nbest_params = grid_optimizer(extra_trees, param_grid, x_train, y_train)    ","a3528a02":"param_grid = {\"n_estimators\": [250, 500, 1000],\n              }\n\nextra_trees = ExtraTreesRegressor(random_state = randomstate, \n                                  n_estimators=100,\n                                  criterion='squared_error',\n                                  max_depth=25,\n                                  max_features='auto'\n                                  \n                                 )\n\nbest_params = grid_optimizer(extra_trees, param_grid, x_train, y_train)   ","7b31e297":"param_grid = {\"n_estimators\": [25, 50, 100],\n              }\n\nextra_trees = ExtraTreesRegressor(random_state = randomstate, \n                                  n_estimators=100,\n                                  criterion='squared_error',\n                                  max_depth=25,\n                                  max_features='auto'\n                                  \n                                 )\n\nbest_params = grid_optimizer(extra_trees, param_grid, x_train, y_train) ","7f74a97a":"extra_trees = ExtraTreesRegressor(random_state = randomstate, \n                                  n_estimators=100,\n                                  criterion='squared_error',\n                                  max_depth=25,\n                                  max_features='auto'\n                                  \n                                 )","456aaedc":"param_grid = {\"max_depth\": [5, 10, 15],\n              'learning_rate': [.1, .3],\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, x_train, y_train)","c3ef928a":"param_grid = {\"max_depth\": [8, 10, 12],\n              'learning_rate': [.05, .1],\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, x_train, y_train)","c3ef0962":"param_grid = {\"min_impurity_decrease\" : [0, .5],\n              'max_features': ['auto', 'sqrt', 'log2'],\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                max_depth = 8,\n                learning_rate=.05,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, x_train, y_train)","fa146ff8":"param_grid = {'subsample' : [.3, .7, 1],\n              'loss': ['lad', 'ls']\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                max_depth = 8,\n                learning_rate=.05,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, x_train, y_train)","2928022f":"param_grid = {'n_estimators' : [500,1000,5000]\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                max_depth = 8,\n                learning_rate=.05,\n                loss='ls',\n                subsample=.7,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, x_train, y_train)","24f47b5c":"gradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                max_depth = 8,\n                learning_rate=.05,\n                loss='ls',\n                subsample=.7,\n                )\n","c781dc08":"param_grid = {\"max_depth\": [5, 10, 15],\n              \"criterion\" : ['squared_error', 'absolute_error']\n              }\n\nrandom_forest = RandomForestRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                n_jobs=-1)\n\nbest_params = grid_optimizer(random_forest, param_grid, x_train, y_train)","2f68ff4d":"param_grid = {'bootstrap': [True, False],\n              'min_samples_leaf' : [1, 10]\n              }\n\nrandom_forest = RandomForestRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                criterion='absolute_error',\n                max_depth=15,\n                max_features='auto',)\n\nbest_params = grid_optimizer(random_forest, param_grid, x_train, y_train)","1cb841d8":"mae_val = [] #to store mae values for different k\n\n# checks mean absolute error scores on k from 1 to 25\nfor K in range(0, 20):\n    K = K+1 \n    # set up the KNN regressor\n    model = KNeighborsRegressor(n_neighbors = K)\n    # get accuracy cross val score for cv 5\n    scores = cross_validate(model, x_train, y_train, cv=5,\n        scoring=('neg_mean_absolute_error'),\n        return_train_score=True)\n    mae = round(scores['test_score'].mean(), 4) \n    mae_val.append(mae) #store mae values\n    print('MAE value for k= ' , K , 'is:', mae)\n    \n# gets optimal k-value based on score minimum\nindex_max = np.argmax(mae_val) + 1\nprint(index_max)","d09a7ac0":"# Parameter Tuning\n\nparam_grid = {'kernel' : ['linear', 'rbf', 'poly'],\n              'gamma' : ['scale', 'auto']            \n              }\n\nsvr = SVR()\n\nbest_params = grid_optimizer(svr, param_grid, x_train, y_train)","734b8633":"# Parameter Tuning\n\nparam_grid = {'C' : [1,10],\n              'epsilon' : [.01,.05]            \n              }\n\nsvr = SVR(gamma = 'auto', kernel = 'rbf')\n\nbest_params = grid_optimizer(svr, param_grid, x_train, y_train)\n","33937d35":"svr = SVR(gamma = 'auto', kernel = 'rbf', c=10, epsilon=.05)","1dfec4e6":"param_grid = {'kernel' : ['linear', 'rbf', 'poly', 'sigmoid'],\n              'gamma' : ['scale', 'auto']            \n              }\n\nnu_svr = NuSVR()\n\nbest_params = grid_optimizer(nu_svr, param_grid, x_train, y_train)","fc987c30":"# Parameter Tuning\n\nparam_grid = {'solver' : ['auto', 'saga', 'cholesky', 'lsqr'],\n              }\n\nridge = Ridge(random_state = randomstate, tol=1e-3, normalize=False)\n\nbest_params = grid_optimizer(ridge, param_grid, x_train, y_train)","36fe4222":"param_grid = {'alpha_init' : [1, 5, 50],\n              'normalize' : [True, False]            \n              }\n\nbay_ridge = BayesianRidge()\n\nbest_params = grid_optimizer(bay_ridge, param_grid, x_train, y_train)","a49a98a2":"param_grid = {'solver' : ['lbfgs', 'sgd', 'adam']\n              }\n\nmlp = MLPRegressor(random_state = randomstate)\n\nbest_params = grid_optimizer(mlp, param_grid, x_train, y_train)","85bcd677":"param_grid = {\"max_depth\": [5, 10, 15],\n              \"criterion\" : ['squared_error', 'absolute_error', 'poisson']\n              }\n\ndecision_tree = DecisionTreeRegressor(\n                #n_estimators=500,\n                random_state=randomstate,\n                #n_jobs=-1\n            )\n\nbest_params = grid_optimizer(decision_tree, param_grid, x_train, y_train)","2715dad3":"param_grid = {\"max_depth\": [15, 20],\n              \"max_features\" : ['auto', 'sqrt', 'log2']\n              }\n\ndecision_tree = DecisionTreeRegressor(\n                #n_estimators=500,\n                random_state=randomstate,\n                #n_jobs=-1,\n                criterion = 'squared_error',\n                \n            )\n\nbest_params = grid_optimizer(decision_tree, param_grid, x_train, y_train)","ef4a0075":"decision_tree = DecisionTreeRegressor(\n                random_state=randomstate,\n                criterion = 'squared_error',\n                max_depth=15,      \n            )","7c6a0935":"![Spot Check Evaluation](https:\/\/i.imgur.com\/KqnnOKk.png)","d1f9b677":"##### Cleaning Functions","e5472f25":"Best mae: -0.063\nBest parameters set found on train set: \n\n{'learning_rate': 0.05, 'max_depth': 8}","51126ed5":"### Feature Selection","5dd9dd8e":"##### Study Target Variable","56aca156":"Best mae: -0.062\nBest parameters set found on train set: \n\n{'eta': 0.05, 'max_depth': 10, 'min_child_weight': 2}","7ad6d380":"## Package Imports","72d62dcc":"##### Scoring Functions","7c1584e5":"### Run Stack Selector","03756b64":"Let's check our distributions. We see something interesting! There were no actual missing values, BUT there are several spots where values are 0 in x, y and z. Those are missing values! We also see some outliers in table, x and y. We will address all of these.","69dd5b19":"Categorical pipeline:\n* One Hot Encoder\n\nContinuous pipeline:\n* Impute NaN with MissForest()\n* Standard Scaler\n* Polynomials degree 2","80526eff":"Using Extra Trees we have increased our R2 to 98.17% and decreased our mean absolute error to $255, and our root mean squared error to 516. We like that!","ff493b05":"##### Visualizing Categorical and Continuous","abab666f":"Best mae: -0.077\nBest parameters set found on train set: \n\n{'gamma': 'auto', 'kernel': 'rbf'}","f088da00":"I always start my notebooks with package imports. A few packages don't work here on Kaggle, so we're going to have to work around those. MissForest is an incredibly useful imputation package for imputing missing values in a way that reduces noise over the standard method of imputing mean or median. Unfortunately, this package doesn't play nice here. Kaggle also doesn't support the new as of late 20202 HalvingGridSearchCV, which is Scikit-learn's newest GridSearch method which is ideal for large datasets. I recommend you read up on both of these packages as they are very useful!","2bfe8dc2":"##### Train\/Test Split\n\nIt's time to divide our data into the train\/test split so that we can standardize our data.","1aed7d47":"Time to run our OOF predictions function. We're sending in our train set, our train targets, and the model dictionary that we just set up. We'll have returned to us a re-ordered train and target set, as well as a model dictionary with full yhat prediction lists.\n\nWe aren't going to run this on Kaggle, so you'll see an example of the results as it runs.","64e80267":"Best mae: -0.073\nBest parameters set found on train set: \n\n{'criterion': 'absolute_error', 'max_depth': 15}","5eb86925":"Best mae: -0.088\nBest parameters set found on train set: \n\n{'max_depth': 15, 'max_features': 'auto'}","c54a2a01":"Based on what we see in these visuals, we're going to add a polynomial-2 to all of our continuous variables.","86a668bc":"We're happy to see so many features strongly correlated with price! That bodes very well! \n\nHowever, x, y and z all correlate very strongly with each other, as well as with carat. We will need to use feature selection to determine if these are important to keep, or if we should eliminate some\/all of them.","b0367913":"# S - Scrubbing the Data","ad2c7b62":"Permutation importance did not improve our mean average error, so we're going to model with our original feature set.","711f01c5":"Best mae: -0.087\nBest parameters set found on train set: \n\n{'criterion': 'squared_error', 'max_depth': 15}","360e7d24":"##### Tuning Functions","155356fa":"Our improved histograms after outlier removal and removing bad values. We will fix those bad values during our preprocessing pipeline.","8a1b7b10":"##### SVR","858aa9f4":"![Spot Check Running](https:\/\/i.imgur.com\/uhC5LfT.png)","4b3cfa22":"## Notebook Functions","e401f6f6":"Next we check on the linearity of our continuous variables. Here we can get a sense of if any of these would be better expressed as polynomial relationships.","3c40bcb5":"##### Extra Trees","99c53a54":"Best mae: -0.064\nBest parameters set found on train set: \n\n{'learning_rate': 0.1, 'max_depth': 10}","9231992e":"### Prepare Testing Assets","792da187":"Now we run the stack selector! This takes a long time to run, depending on the size of your dataset. Keep verbose set to True for updates so you know what's happening!","7486fdb8":"Best mae: -0.081\nBest parameters set found on train set: \n\n{'gamma': 'auto', 'kernel': 'rbf'}","7d587b0e":"### Get OOF Predictions","684ea926":"# The Price of Diamonds\n\nThis notebook uses the diamond prices dataset to demonstrate usage of a basic stacked model for regression. If you've never used model stacking before, you'll find it's much easier than you expect!\n\nYou can follow along here in the notebook, or read my Simple Stacked Model tutorial on Medium here : https:\/\/towardsdatascience.com\/simple-model-stacking-explained-and-automated-1b54e4357916\n\nThis notebook includes appropriate EDA and cleaning steps for any dataset before modeling begins.","4f56ef97":"I like to store my functions at the top of my workflow. This makes it easy to find a function if I need a reminder of what arguments to pass, or how it works.","1d97f367":"# Cleaning Final Data","dce177c8":"##### Ridge","ae1b5cf8":"Finally, with our stacked model, we increased our R2 to 98.4% and decresed our mean absolute error to 243 and root mean squared error to 483. This was a 4.7% improvement to our mean average error, and a 6.4% improvement to our root mean squared error. Absolutely worth the time!","57b79773":"Now we're going to clear out those high outliers from z and y by setting those to NaN as well.","f001bebd":"## Spot Check Models","e3b36840":"# Modeling","a433faa3":"Our skewness under 2 still falls within an acceptable range. Kurtosis is quite high. Since we have a strong right skew, we can better normalize our distribution with a log transformation.","9a817b2b":"Next we're going to select features using a method called permutation importance. This is a great model-agnostic method that you can use with any model type, and the way it works is very easy to understand. After fitting the model, it calculates a baseline R^2 score. Then for each feature, it scrambles the inputs of that feature, turnings its contribution into noise. The model is evaluated again with the feature scrambled, and the change in overall R^2 is logged as the importance for that feature. After scrambling all features, each feature has been assigned an importance based on the R^2 reduction. You can then select the features that had an effect on R^2 based on your own threshold (I kept anything >= .001) and throw out the remaining features.\n\nYou can learn more about this underrated feature selection method here: https:\/\/explained.ai\/rf-importance\/\nThe article focuses on Random Forest, but discusses permutation importance as an excellent feature selection method for any model type.","a9e7846f":"Visualizing the polynomial relationships in our continuous data.","259c6936":"# Obtaining the Data","c13ece38":"We can see potential polynomial features in these continuous visualizations.","fba1d193":"##### Decision Tree","2c773283":"##### Set up pipeline","5f59d9df":"Best mae: -0.062\nBest parameters set found on train set: \n\n{'colsample_bytree': 0.8}","ddebd5bd":"# APPENDIX - Parameter Tuning","5560f11c":"# Notebook Setup","8bd25ed5":"##### Bayesian Ridge","c1b1de15":"##### Nu SVR","62f08571":"We want to drop any duplicate data rows","b37fa790":"Best mae: -0.062\nBest parameters set found on train set: \n\n{'reg_alpha': 1e-05, 'reg_lambda': 1}","ba610c6b":"### Final Model Evaluation","c5fe605b":"Best mae: -0.063\nBest parameters set found on train set: \n\n{'max_features': 'auto', 'min_impurity_decrease': 0}","dae9b9e9":"![Out-of-fold Predictions](https:\/\/i.imgur.com\/G3OAP5U.png)","90e37dd7":"Best mae: -0.066\nBest parameters set found on train set: \n\n{'max_depth': 25, 'max_features': 'auto'}","3e60444d":"We can see in our histogram that there are extreme right tails on our carat field. Let's take a look at where our percentiles fall, and remove the outliers.","2620fe38":"### Prepare Final Assets","d5d87d93":"##### KNN","196013ea":"Best mae: -0.089\nBest parameters set found on train set: \n\n{'solver': 'auto'}","d566b9f9":"##### Correlations\/Multicollinearity","09695b80":"First we'll visualize our categoricals. If there is any clear trend, we could ordinal encode these into a continuous. Otherwise, they should remain as categorical.","0e0908c4":"##### Polynomial Relationships","ceedb060":"##### Gradient Boosted Trees","104830dc":"Best mae: -0.066\nBest parameters set found on train set: \n\n{'n_estimators': 100}","5d6d77e1":"##### Stacking Functions","da616d0c":"# Exploring the Data","acb319c4":"This section has trouble on Kaggle, so an example of the output as it runs is posted.","6fcc2b9c":"##### Random Forest","ff842596":"Best mae: -0.062\nBest parameters set found on train set: \n\n{'eta': 0.05, 'max_depth': 10, 'min_child_weight': 2}","e0ae6123":"Best mae: -0.062\nBest parameters set found on train set: \n\n{'gamma': 0, 'subsample': 1}","9e48af0a":"Next we'll check our features with a Pearson Correlation Matrix, and see if any of our features are highly correlated and therefore redundant.","665d6514":"Best mae: -0.086\nBest parameters set found on train set: \n\n{'solver': 'lbfgs'}","65e8b050":"##### MLP Regressor","3fb1e6b8":"![Stack Selections](https:\/\/i.imgur.com\/zHzgSqO.png)","bdabede1":"Best mae: -0.065\nBest parameters set found on train set: \n\n{'n_estimators': 1000}","ab9d24ba":"Best mae: -0.088\nBest parameters set found on train set: \n\n{'alpha_init': 50, 'normalize': False}","b5d67829":"Now that we're ready to evaluate our final model, we want to be able to make some comparisons to confirm to ourselves that our model stack was an improvement.\n\nFirst we predict our test set with the model stack's meta model, which is Linear Regression.\n\nThen let's predict our test set with the best of the spot-check models, Extra Trees.\n\nFinally let's use our compeleted model stack to predict the test set.","8e3462ae":"We see in our describe, and in our histograms, that although we didn't find any missing data using isna(), there are actually 0 values in the x, y and z columns. We're going to change these to NaN because we will impute these later with MissForest.","7b9c2c3d":"Best mae: -0.063\nBest parameters set found on train set: \n\n{'loss': 'ls', 'subsample': 0.7}","2cee7f81":"Our models are now all hyperparameter tuned, which we did in the Appendix using HalvingGridSearchCV on the train data only.","e2717307":"##### XGBoost\n\nGradient Boosting performs best with optimal parameter tuning. We're going to use sklearn's GridSearchCV to find the optimal hyperparameters to use with our gradient booster! Here are the parameters we are trying out:\n\n* n_estimators: Number of boosts to perform. Gradient boosting is pretty robust to over-fitting so more is usually better\n* max_depth: This determines how many tree nodes the estimator looks at before making a prediction. We don't know what is best here, so we are trying things from 2-4 to see what works the best\n* min_child_weight: Min sum of instance weight needed in a child\n* gamma: Minimum loss reduction to make another partition on a leaf node. Higher results in more conservative algorithm.\n* subsample: Ratio of training sets. .5 means that it will sample half the training data before making trees. Occurs with each boosting iteration.\n* colsample_by_tree: ratio of columns when making a tree\n* alpha: L1 regularization. Higher will make model more conservative.\n* learning_rate: Tuning this setting alters how much the model corrects after it runs a boost. .1 is a common rate and we will test a lower and higher rate as well.","6d15e3c8":"Best mae: -0.063\nBest parameters set found on train set: \n\n{'n_estimators': 500}","34a11bcb":"### Spot Check Evaluation","0cb59131":"## Final Model Setup","c862c4ab":"We have an R2 of 96.7%, a mean absolute error of $355, and a root mean squared error of $684. Not too bad.","505a15e5":"## Conclusion\n\nModel stacking is a great way to improve your model performance, and it's easy and fun (if time consuming!). The next time you want to eke out a little extra performance from your project, definitely try out a stack."}}