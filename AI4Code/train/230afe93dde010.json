{"cell_type":{"ad5ce382":"code","bc2ede39":"code","159e9107":"code","4a0780a9":"code","13eab38f":"code","dab20b47":"code","09ce94e3":"code","42f23782":"code","bdeebfff":"code","d366f459":"code","6eb64707":"code","ce965db8":"code","90b9bc28":"code","fcaf9412":"code","4105678d":"code","d05ea6c1":"code","49aef463":"code","0edb0e69":"code","5ea5dbad":"code","bf792f63":"code","25bc95c6":"code","d3509a18":"code","09e2a774":"code","6d04bc92":"code","2387265d":"code","e705ad3b":"code","7075ce27":"code","1e652dae":"code","78a0957e":"code","78d80413":"code","225bf17e":"code","9ceaacb5":"code","7f016b6a":"code","f10c07e0":"code","f9221dff":"code","ad0c8a59":"code","b84a842a":"code","84643410":"code","05616b04":"code","b5521a57":"code","d70284d6":"code","2a8c03e5":"code","6f8bb730":"code","f6cbeac9":"code","0f6bbf37":"code","ab9b17e7":"code","189466e3":"code","e9d29991":"code","0f025ec5":"code","557ad628":"code","b044a7a8":"code","2a422b0a":"code","d720611c":"code","b3fe0796":"code","6296482b":"code","1326f555":"code","6a343e7d":"code","b4acb887":"code","74042b36":"code","513a77b4":"code","37d58305":"code","62ce6e17":"markdown","5b3e24ad":"markdown"},"source":{"ad5ce382":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bc2ede39":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","159e9107":"print(train.shape, test.shape, submission.shape)","4a0780a9":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","13eab38f":"train.head(0)","dab20b47":"print(\"Before drop ID\", train.shape, test.shape)\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain = train.drop(['Id'], axis = 1)\ntest = test.drop(['Id'], axis=1)\nprint(\"After drop ID\", train.shape, test.shape)\n","09ce94e3":"\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n","42f23782":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","bdeebfff":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","d366f459":"train['SalePrice'] = np.log1p(train['SalePrice'])","6eb64707":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","ce965db8":"all_data = pd.concat((train, test)).reset_index(drop=True)","90b9bc28":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\ny_train = train['SalePrice'].values\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\n\nprint(all_data.shape)\n","fcaf9412":"all_data_na = (all_data.isnull().sum() \/ len(all_data))*100\nall_data_na = all_data_na.drop(all_data_na[all_data_na==0].index)","4105678d":"all_data_na","d05ea6c1":"all_data_na.index","49aef463":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","0edb0e69":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","5ea5dbad":"corrmat = train.corr()\ncorrmat","bf792f63":"\nfig, ax = plt.subplots()\nax.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.show()","25bc95c6":"all_data['PoolQC'].unique()","d3509a18":"all_data['PoolQC'] = all_data['PoolQC'].fillna('None')","09e2a774":"all_data['PoolQC'].unique()","6d04bc92":"all_data['MiscFeature'].unique()","2387265d":"all_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')","e705ad3b":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","7075ce27":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n","1e652dae":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n","78a0957e":"all_data['Neighborhood'].unique()","78d80413":"all_data['Alley'].unique()","225bf17e":"all_data.groupby(\"Neighborhood\")[\"LotFrontage\"]","9ceaacb5":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","7f016b6a":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","f10c07e0":"for col in ('GarageYrBlt', \n            'GarageArea', \n            'GarageCars',\n            'BsmtFinSF1', \n            'BsmtFinSF2', \n            'BsmtUnfSF',\n            'TotalBsmtSF', \n            'BsmtFullBath', \n            'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","f9221dff":"all_data['Electrical'].mode()","ad0c8a59":"all_data['Electrical'].fillna(all_data['Electrical'].mode())\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","b84a842a":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n","84643410":"all_data_na = all_data.isnull().sum() \/ len(all_data)\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","05616b04":"all_data_na","b5521a57":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","d70284d6":"list(all_data['BsmtQual'].values)","2a8c03e5":"all_data['BsmtQual']","6f8bb730":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","f6cbeac9":"pd.set_option('display.max_columns', 10000)\nall_data.head()","0f6bbf37":"all_data.head()","ab9b17e7":"str_line = []\ncolumn_name = []\nunique_list = []\n\ntotal = all_data\n\nfor i in total.columns:\n    total_list1 = list(total[i].unique())\n    total_list2 = list((pd.DataFrame(total_list1).dropna())[0])\n    str_data = int(str(type(total_list2[0])) == \"<class 'str'>\")\n    str_line.append(str_data)\n    column_name.append(i)\n\nstr_line = pd.DataFrame(str_line)\ncolumn_name = pd.DataFrame(column_name)\nall_column = pd.concat([column_name, str_line],1)\nall_column.columns = ['column_name', 'strn']\n\nstr_column = pd.DataFrame(all_column[all_column['strn'] == 1])\nstr_column = list(str_column['column_name'])\n\nunique_count = []\nunique_column = []\n\nfor i in str_column:\n    total_unique = list(total[i].unique())\n    total_unique = len(total_unique)\n    unique_count.append(total_unique)\n    unique_column.append(i)\n\nunique_count = pd.DataFrame(unique_count)\nunique_column = pd.DataFrame(unique_column)\nunique_total = pd.concat([unique_column, unique_count],1)\n\nunique_total.columns = ['unique_column', 'unique_count']\n\nunique_total = unique_total.sort_values([\"unique_count\"], ascending=[False])\n# unique_total\n# \uc0c1\uc704 4\uac1c \ud56d\ubaa9\ub4e4\uc740 \ub530\ub85c \uad00\ub9ac\ud574\uc57c \ud560\ub4ef, \ub098\uba38\uc9c0\ub294 OHE\uc9c4\ud589\n\nprint(unique_total.unique_column.values)\nunique_column = unique_total.unique_column.values","189466e3":"from sklearn.preprocessing import LabelEncoder\ncols = unique_column\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","e9d29991":"import lightgbm as lgb","0f025ec5":"all_data.head()","557ad628":"all_data[:10]","b044a7a8":"test.head()","2a422b0a":"train = all_data[:ntrain]\nX = train\ny = y_train\n\nX_test = all_data[ntrain:]","d720611c":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split (X,y, random_state=0)","b3fe0796":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","6296482b":"model_lgb.fit(X_train, y_train)","1326f555":"pred_train = model_lgb.predict(X_train)\npred_valid = model_lgb.predict(X_valid)\n\npred_train_expm = np.expm1(pred_train)\npred_valid_expm = np.expm1(pred_valid)\ny_train_expm = np.expm1(y_train)\ny_valid_expm = np.expm1(y_valid)","6a343e7d":"def rmse(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())\n\n","b4acb887":"print(rmse(pred_train, y_train))\nprint(rmse(pred_valid, y_valid))","74042b36":"pred_test = model_lgb.predict(X_test)\n\npred_test_expm = np.expm1(pred_test)","513a77b4":"submission.head()","37d58305":"submission = submission.drop(\"SalePrice\",1)\npred_test_expm = pd.DataFrame(pred_test_expm)\n\nsubmission_final = pd.concat([submission,pred_test_expm],axis=1)\n\nsubmission_final.columns = ['ID','SalePrice']\nsubmission_final.to_csv(\"submission_fianl.csv\", index=False)\nsubmission_final.tail()","62ce6e17":"https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","5b3e24ad":"# (190822) \uae4c\uc9c0 \uc218\uc5c5\ub0b4\uc6a9"}}