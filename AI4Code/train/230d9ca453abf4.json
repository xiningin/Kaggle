{"cell_type":{"240b4cc9":"code","fd05d330":"code","34003db8":"code","34fb8c53":"code","674bc0f4":"code","a98261fa":"code","1f7c63bd":"code","66ad7bb8":"code","7483ebeb":"code","1e5746eb":"code","f390a741":"markdown","5fd5fa15":"markdown","f264620d":"markdown","961b4796":"markdown","5a56385b":"markdown"},"source":{"240b4cc9":"#location of the training data \ndata_location =  \"..\/input\/flickr8k\"\n\n#copy dataloader\n!cp ..\/input\/data-loader\/data_loader.py .","fd05d330":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T\n\n#custom imports \nfrom data_loader import FlickrDataset,get_data_loader","34003db8":"#for image plot\nimport matplotlib.pyplot as plt\ndef show_image(img, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    \n    #unnormalize \n    img[0] = img[0] * 0.229\n    img[1] = img[1] * 0.224 \n    img[2] = img[2] * 0.225 \n    img[0] += 0.485 \n    img[1] += 0.456 \n    img[2] += 0.406\n    \n    img = img.numpy().transpose((1, 2, 0))\n    \n    \n    plt.imshow(img)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated","34fb8c53":"#setting the constants\ndata_location =  \"..\/input\/flickr8k\"\nBATCH_SIZE = 256\nNUM_WORKER = 2\n\n#defining the transform to be applied\ntransforms = T.Compose([\n    T.Resize(256),                     \n    T.RandomCrop(224),                 \n    T.ToTensor(),                               \n    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n])\n\n\n#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"\/Images\",\n    caption_file = data_location+\"\/captions.txt\",\n    transform=transforms\n)\n\n#writing the dataloader\ndata_loader = get_data_loader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n)","674bc0f4":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim","a98261fa":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet50(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n        \n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n\n    def forward(self, images):\n        features = self.resnet(images)\n        features = features.view(features.size(0), -1)\n        features = self.embed(features)\n        return features\n\nclass DecoderRNN(nn.Module):\n    def __init__(self,embed_size, hidden_size, vocab_size, num_layers=1,drop_prob=0.3):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size,embed_size)\n        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers=num_layers,batch_first=True)\n        self.fcn = nn.Linear(hidden_size,vocab_size)\n        self.drop = nn.Dropout(drop_prob)\n    \n    def forward(self, features, captions):\n        \n        #vectorize the caption\n        embeds = self.embedding(captions[:,:-1])\n        \n        #concat the features and captions\n        x = torch.cat((features.unsqueeze(1),embeds),dim=1) \n        x,_ = self.lstm(x)\n        x = self.fcn(x)\n        return x\n    \n    def generate_caption(self,inputs,hidden=None,max_len=20,vocab=None):\n        # Inference part\n        # Given the image features generate the captions\n        \n        batch_size = inputs.size(0)\n        \n        captions = []\n        \n        for i in range(max_len):\n            output,hidden = self.lstm(inputs,hidden)\n            output = self.fcn(output)\n            output = output.view(batch_size,-1)\n        \n            \n            #select the word with most val\n            predicted_word_idx = output.argmax(dim=1)\n            \n            #save the generated word\n            captions.append(predicted_word_idx.item())\n            \n            #end if <EOS detected>\n            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n                break\n            \n            #send generated word as the next caption\n            inputs = self.embedding(predicted_word_idx.unsqueeze(0))\n        \n        #covert the vocab idx to words and return sentence\n        return [vocab.itos[idx] for idx in captions]\n        \n            \nclass EncoderDecoder(nn.Module):\n    def __init__(self,embed_size, hidden_size, vocab_size, num_layers=1,drop_prob=0.3):\n        super().__init__()\n        self.encoder = EncoderCNN(embed_size)\n        self.decoder = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers,drop_prob)\n    \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs","1f7c63bd":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","66ad7bb8":"# Hyperparameters\nembed_size = 400\nhidden_size = 512\nvocab_size = len(dataset.vocab)\nnum_layers = 2\nlearning_rate = 0.0001\nnum_epochs = 2","7483ebeb":"# initialize model, loss etc\nmodel = EncoderDecoder(embed_size, hidden_size, vocab_size, num_layers).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","1e5746eb":"num_epochs = 20\nprint_every = 100\n\nfor epoch in range(1,num_epochs+1):   \n    for idx, (image, captions) in enumerate(iter(data_loader)):\n        image,captions = image.to(device),captions.to(device)\n\n        # Zero the gradients.\n        optimizer.zero_grad()\n\n        # Feed forward\n        outputs = model(image, captions)\n        \n        # Calculate the batch loss.\n        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n\n        \n        # Backward pass.\n        loss.backward()\n\n        # Update the parameters in the optimizer.\n        optimizer.step()\n        \n        if (idx+1)%print_every == 0:\n            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n            \n            \n            #generate the caption\n            model.eval()\n            with torch.no_grad():\n                dataiter = iter(data_loader)\n                img,_ = next(dataiter)\n                features = model.encoder(img[0:1].to(device))\n                caps = model.decoder.generate_caption(features.unsqueeze(0),vocab=dataset.vocab)\n                caption = ' '.join(caps)\n                show_image(img[0],title=caption)\n                \n            model.train()","f390a741":"### 4) Setting Hypperparameter and Init the model","5fd5fa15":"### 1) Initial Imports and loading the utils function. The dataset is used is <a href='https:\/\/www.kaggle.com\/adityajn105\/flickr8k'>Flickr 8k<\/a> from kaggle.Custom dataset and dataloader is implemented in <a href='https:\/\/www.kaggle.com\/mdteach\/torch-data-loader-flicker-8k'> this<\/a> notebook.\n","f264620d":"### 2) Implementing the Helper function to plot the Tensor image","961b4796":"### 5) Training Job from above configs","5a56385b":"### 3) Defining the Model Architecture\n#### Model is seq2seq model. In the encoder pretrained ResNet model is used to extract the features. Decoder, takes the encoder context as first input and generates the caption. In the decoder model LSTM cell."}}