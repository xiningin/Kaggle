{"cell_type":{"fa092c96":"code","8ab5f460":"code","a1b785cf":"code","b7705fbb":"code","2f09493d":"code","56f51835":"code","6e36818d":"code","f7a3f1b0":"code","76933bc8":"code","5f2b13cf":"code","142c32ab":"code","d62b43d2":"code","1de5a4ff":"code","66e80bb0":"code","9f5bd8b3":"code","1262ed0d":"code","755b4015":"code","d5a7d5c3":"code","2c99b300":"code","30bd20da":"code","17663808":"code","c05482da":"code","d0edecbd":"code","5193d5a7":"code","23d57d48":"code","bb47c890":"code","968de413":"code","f2c7addb":"code","2e029c9c":"code","4ba6577b":"code","a874c0f7":"code","b8ecdbc4":"code","7dda47e7":"markdown","ebcc7fcf":"markdown","cca108c9":"markdown","bbbb4620":"markdown","c432448b":"markdown","62bf9056":"markdown","ca3878d3":"markdown","fda0d7bc":"markdown","7ad69ce2":"markdown","db1b4c91":"markdown"},"source":{"fa092c96":"## code inspired from DS6040 BML Class by Dr. Basener\n## Shilpa Narayan (smn7ba)\n## UVA\nimport numpy as np \nimport pandas as pd \nimport statsmodels.api as sm\nfrom statsmodels.tools import add_constant\nfrom itertools import combinations","8ab5f460":"df = pd.read_csv('\/kaggle\/input\/coronary-heart-disease\/CHDdata.csv')\ndf[\"famhist\"] = (df[\"famhist\"] == \"Present\")*1 # converts the famhit to 0 (no hist) and 1 (has hist)\n#df = df.drop([\"famhist\"], axis=1)\ndf.head()","a1b785cf":"X = df.drop([\"chd\"], axis=1)\ny = df[\"chd\"]\n# building the model and fitting the data \nlog_reg = sm.Logit(y, add_constant(X)).fit()\nlog_reg.summary()","b7705fbb":"# Seaborn visualization library\nimport seaborn as sns\n# Create the default pairplot\ng = sns.pairplot(df, hue=\"chd\", palette=\"tab10\", markers=[\"o\", \"D\"])","2f09493d":"from mpmath import mp\nmp.dps = 50\nclass BMA:\n    \n    def __init__(self, y, X, **kwargs):\n        # Setup the basic variables.\n        self.y = y\n        self.X = X\n        self.names = list(X.columns)\n        self.nRows, self.nCols = np.shape(X)\n        self.likelihoods = mp.zeros(self.nCols,1)\n        self.likelihoods_all = {}\n        self.coefficients_mp = mp.zeros(self.nCols,1)\n        self.coefficients = np.zeros(self.nCols)\n        self.probabilities = np.zeros(self.nCols)\n        # Check the max model size. (Max number of predictor variables to use in a model.)\n        # This can be used to reduce the runtime but not doing an exhaustive sampling.\n        if 'MaxVars' in kwargs.keys():\n            self.MaxVars = kwargs['MaxVars']\n        else:\n            self.MaxVars = self.nCols  \n        # Prepare the priors if they are provided.\n        # The priors are provided for the individual regressor variables.\n        # The prior for a model is the product of the priors on the variables in the model.\n        if 'Priors' in kwargs.keys():\n            if np.size(kwargs['Priors']) == self.nCols:\n                self.Priors = kwargs['Priors']\n            else:\n                print(\"WARNING: Provided priors error.  Using equal priors instead.\")\n                print(\"The priors should be a numpy array of length equal tot he number of regressor variables.\")\n                self.Priors = np.ones(self.nCols)  \n        else:\n            self.Priors = np.ones(self.nCols)  \n        if 'Verbose' in kwargs.keys():\n            self.Verbose = kwargs['Verbose'] \n        else:\n            self.Verbose = False \n        if 'RegType' in kwargs.keys():\n            self.RegType = kwargs['RegType'] \n        else:\n            self.RegType = 'LS' \n        \n    def fit(self):\n        # Perform the Bayesian Model Averaging\n        \n        # Initialize the sum of the likelihoods for all the models to zero.  \n        # This will be the 'normalization' denominator in Bayes Theorem.\n        likelighood_sum = 0\n        \n        # To facilitate iterating through all possible models, we start by iterating thorugh\n        # the number of elements in the model.  \n        max_likelihood = 0\n        for num_elements in range(1,self.MaxVars+1): \n            \n            if self.Verbose == True:\n                print(\"Computing BMA for models of size: \", num_elements)\n            \n            # Make a list of all index sets of models of this size.\n            Models_next = list(combinations(list(range(self.nCols)), num_elements)) \n             \n            # Occam's window - compute the candidate models to use for the next iteration\n            # Models_previous: the set of models from the previous iteration that satisfy (likelihhod > max_likelihhod\/20)\n            # Models_next:     the set of candidate models for the next iteration\n            # Models_current:  the set of models from Models_next that can be consturcted by adding one new variable\n            #                    to a model from Models_previous\n            if num_elements == 1:\n                Models_current = Models_next\n                Models_previous = []\n            else:\n                idx_keep = np.zeros(len(Models_next))\n                for M_new,idx in zip(Models_next,range(len(Models_next))):\n                    for M_good in Models_previous:\n                        if(all(x in M_new for x in M_good)):\n                            idx_keep[idx] = 1\n                            break\n                        else:\n                            pass\n                Models_current = np.asarray(Models_next)[np.where(idx_keep==1)].tolist()\n                Models_previous = []\n                        \n            \n            # Iterate through all possible models of the given size.\n            for model_index_set in Models_current:\n                \n                # Compute the linear regression for this given model. \n                model_X = self.X.iloc[:,list(model_index_set)]\n                if self.RegType == 'Logit':\n                    model_regr = sm.Logit(self.y, model_X).fit(disp=0)\n                else:\n                    model_regr = OLS(self.y, model_X).fit()\n                \n                # Compute the likelihood (times the prior) for the model. \n                model_likelihood = mp.exp(-model_regr.bic\/2)*np.prod(self.Priors[list(model_index_set)])\n                    \n                if (model_likelihood > max_likelihood\/20):\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"likelihood=\",model_likelihood)\n                    self.likelihoods_all[str(model_index_set)] = model_likelihood\n                    \n                    # Add this likelihood to the running tally of likelihoods.\n                    likelighood_sum = mp.fadd(likelighood_sum, model_likelihood)\n\n                    # Add this likelihood (times the priors) to the running tally\n                    # of likelihoods for each variable in the model.\n                    for idx, i in zip(model_index_set, range(num_elements)):\n                        self.likelihoods[idx] = mp.fadd(self.likelihoods[idx], model_likelihood, prec=1000)\n                        self.coefficients_mp[idx] = mp.fadd(self.coefficients_mp[idx], model_regr.params[i]*model_likelihood, prec=1000)\n                    Models_previous.append(model_index_set) # add this model to the list of good models\n                    max_likelihood = np.max([max_likelihood,model_likelihood]) # get the new max likelihood if it is this model\n                else:\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"rejected by Occam's window\")\n                    \n\n        # Divide by the denominator in Bayes theorem to normalize the probabilities \n        # sum to one.\n        self.likelighood_sum = likelighood_sum\n        for idx in range(self.nCols):\n            self.probabilities[idx] = mp.fdiv(self.likelihoods[idx],likelighood_sum, prec=1000)\n            self.coefficients[idx] = mp.fdiv(self.coefficients_mp[idx],likelighood_sum, prec=1000)\n        \n        # Return the new BMA object as an output.\n        return self\n    \n    def predict(self, data):\n        data = np.asarray(data)\n        if self.RegType == 'Logit':\n            try:\n                result = 1\/(1+np.exp(-1*np.dot(self.coefficients,data)))\n            except:\n                result = 1\/(1+np.exp(-1*np.dot(self.coefficients,data.T)))\n        else:\n            try:\n                result = np.dot(self.coefficients,data)\n            except:\n                result = np.dot(self.coefficients,data.T)\n        \n        return result  \n        \n    def summary(self):\n        # Return the BMA results as a data frame for easy viewing.\n        df = pd.DataFrame([self.names, list(self.probabilities), list(self.coefficients)], \n             [\"Variable Name\", \"Probability\", \"Avg. Coefficient\"]).T\n        return df  ","56f51835":"result = BMA(y,add_constant(X), RegType = 'Logit', Verbose=True).fit()","6e36818d":"result.summary()","f7a3f1b0":"result.likelihoods_all","76933bc8":"# predict the y-values from training input data\npred_BMA = result.predict(add_constant(X))\npred_Logit = log_reg.predict(add_constant(X))","5f2b13cf":"# plot the predictions with the actual values\nimport matplotlib.pyplot as plt\nplt.scatter(pred_BMA,y-0.05)\nplt.scatter(pred_Logit,y)\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Coronary Heart Disease \\n(0=Not Present, 1=Present)\")\nplt.legend(['pred_BMA','pred_Logit'])","142c32ab":"# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == y)\/len(y))\nprint(\"Logit Accuracy: \", np.sum((pred_Logit > 0.5) == y)\/len(y))","d62b43d2":"pd.DataFrame(result.summary()).sort_values(by='Probability').plot(kind='barh',y='Probability',x='Variable Name',legend=False);\nplt.xlabel('Probability');\nplt.suptitle('Probabilities for each factor sorted by probability');","1de5a4ff":"pd.DataFrame(result.summary()).sort_values(by='Avg. Coefficient').plot(kind='barh',y='Avg. Coefficient',x='Variable Name',legend=False);\nplt.xlabel('Avg. Coefficient');\nplt.suptitle('Model averaged coeffcients for the predictor variables');","66e80bb0":"chd_df = pd.read_csv('\/kaggle\/input\/coronary-heart-disease\/CHDdata.csv')\n#prepare to scale data - drop catgorical variable\nto_scale_df = chd_df.iloc[:,0:9].drop(columns=['famhist'])\n\n#use standard scaler to satnderdize the data (subtract with mean and divide by standard deviation)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nchd_df_scaled = scaler.fit_transform(to_scale_df.to_numpy())\n\n#join back removed variables to scaled dataset\nchd_df_scaled = pd.DataFrame(chd_df_scaled,columns=to_scale_df.columns)\nchd_df_scaled['famhist'] = chd_df['famhist']\nchd_df_scaled[\"famhist\"] = (chd_df_scaled[\"famhist\"] == \"Present\")*1 # converts the famhit to 0 (no hist) and 1 (has hist)\n#chd_df_scaled['chd'] = chd_df['chd']","9f5bd8b3":"chd_df_scaled.head()","1262ed0d":"y_s = df[\"chd\"]\n# building the model and fitting the data \nlog_reg_sc = sm.Logit(y_s, add_constant(chd_df_scaled)).fit()\nlog_reg_sc.summary()","755b4015":"result_sc = BMA(y_s,add_constant(chd_df_scaled), RegType = 'Logit', Verbose=False).fit()","d5a7d5c3":"result_sc.summary()","2c99b300":"fig, ax = plt.subplots()\np = pd.DataFrame(result_sc.summary()).sort_values(by='Probability').plot(kind='barh',y='Probability',x='Variable Name',legend=False,ax=ax);\n\nplt.xlabel('Probability');\nplt.suptitle('Probabilities for each factor sorted by probability');","30bd20da":"pd.DataFrame(result_sc.summary()).sort_values(by='Avg. Coefficient').plot(kind='barh',y='Avg. Coefficient',x='Variable Name',legend=False);\nplt.xlabel('Avg. Coefficient');\nplt.suptitle('Model averaged coeffcients for the predictor variables');","17663808":"from sklearn.model_selection import train_test_split\n#split data to remove 1\/3 test data\nX_train, X_test, y_train, y_test = train_test_split(chd_df_scaled, y_s, test_size=0.33, random_state=0)","c05482da":"# building the model and fitting the data \nlog_reg_tr = sm.Logit(y_train, add_constant(X_train)).fit()\nlog_reg_tr.summary()","d0edecbd":"result_tr = BMA(y_train,add_constant(X_train), RegType = 'Logit', Verbose=True).fit()","5193d5a7":"result_tr.summary()","23d57d48":"# predict the y-values from test input data\npred_BMA_test = result_tr.predict(add_constant(X_test))\npred_Logit_test = log_reg_tr.predict(add_constant(X_test))","bb47c890":"# compute accuracy \nprint(\"BMA Accuracy Test Data: \", np.sum((pred_BMA_test > 0.5) == y_test)\/len(y_test))\nprint(\"Logit Accuracy Test Data: \", np.sum((pred_Logit_test > 0.5) == y_test)\/len(y_test))","968de413":"pd.DataFrame(\n{'BMA Accuracy_full_data':np.sum((pred_BMA > 0.5) == y)\/len(y),\n 'Logit Accuracy_full_data':np.sum((pred_Logit > 0.5) == y)\/len(y),\n\"BMA Accuracy Test Data\":np.sum((pred_BMA_test > 0.5) == y_test)\/len(y_test),\n\"Logit Accuracy Test Data\": np.sum((pred_Logit_test > 0.5) == y_test)\/len(y_test)},\nindex=[0]\n)","f2c7addb":"#split data to remove 1\/2 test data\nX_train, X_test, y_train, y_test = train_test_split(chd_df_scaled, y_s, test_size=0.5, random_state=0)\n# building the model and fitting the data \nlog_reg_tr = sm.Logit(y_train, add_constant(X_train)).fit()\nlog_reg_tr.summary()","2e029c9c":"result_tr = BMA(y_train,add_constant(X_train), RegType = 'Logit', Verbose=False).fit()\nresult_tr.summary()","4ba6577b":"# predict the y-values from test input data\npred_BMA_test = result_tr.predict(add_constant(X_test))\npred_Logit_test = log_reg_tr.predict(add_constant(X_test))","a874c0f7":"# compute accuracy \nprint(\"BMA Accuracy Test Data: \", np.sum((pred_BMA_test > 0.5) == y_test)\/len(y_test))\nprint(\"Logit Accuracy Test Data: \", np.sum((pred_Logit_test > 0.5) == y_test)\/len(y_test))","b8ecdbc4":"pd.DataFrame(\n{'BMA Accuracy_full_data':np.sum((pred_BMA > 0.5) == y)\/len(y),\n 'Logit Accuracy_full_data':np.sum((pred_Logit > 0.5) == y)\/len(y),\n\"BMA Accuracy Test Data\":np.sum((pred_BMA_test > 0.5) == y_test)\/len(y_test),\n\"Logit Accuracy Test Data\": np.sum((pred_Logit_test > 0.5) == y_test)\/len(y_test)},\nindex=[0]\n)","7dda47e7":"Now we split our data into input X dataframe and an output y datafram, and run our BMA analysis.","ebcc7fcf":"# Part d: Train Test Split and Accuracy Comparison\n\n* When data was split into train\/test split train 67\/33, the results show that the accuracy for Bayesian Modeling Average had reduced but logit has increased compared to using full data set. This could be due to overfitting of data while training the data.\n* When data was split into train\/test split train 50\/50, the results show that the accuracy for Bayesian Modeling Average had reduced and logit had almost remained same compared to using full data set. \n* There always needs to be a good balance between using enough data to train the model but to make sure that doesn't cause overfitting. We do not have priors also with 50\/50 split to compensate for lesser trainign data. The priors need to be restrictive if we have lesser data for good approximation. \n* Priors act as buffers against overtraining\n* As data is larger, priors become less significant\/important.\n* In this case we can see using at least 67% of the data gives better results.\n* Bayesian model averaging does not obviate the overf itting problem in classification, and may in fact aggravate it. Bayesian averaging\u2019s tendency to overfit derives from the likelihood\u2019s exponential sensitivity to random fluctuations in the sample, and increases with the number of models considered (https:\/\/homes.cs.washington.edu\/~pedrod\/papers\/mlc00b.pdf).","cca108c9":"# Comparison for BMA and Logit predcitions and comparing for using whole data vs splitting to train\/test(50:50)","bbbb4620":"Load the data, and check the head.","c432448b":"# Past c: Standardizing the data\n* The prbabilities of the variable sligtly vary when the data is standardized. However, there is a big change in model averaged coefficients.\n* After scaling the data, we can observe that the age varibale has higher averaged coeffecient comapred to tobacco, ldl and typea whereas it was below those when not scaled. Standardizing helps in not letting data difference shift the variance in the data.\n* We can also observe that ldl fell below age, tobacco and type a. sbp and obesity which seemed like did not have any infleunce in the model now show very slight influence. Alcohol and adiposity remain the same with no influence.","62bf9056":"# Part a &b: Bar plots for Probabilities and BMA Coeffecients without standerdizing","ca3878d3":"### Code for BMA from class","fda0d7bc":"# Comparison for BMA and Logit predcitions and comparing for using whole data vs splitting to train\/test(0.67\/0.33)","7ad69ce2":"# Question 2: Bayesian Model Averaging Logistic Regression","db1b4c91":"# Train\/Test(lower train data set with 50:50 split)"}}