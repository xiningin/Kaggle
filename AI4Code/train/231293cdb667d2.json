{"cell_type":{"a4b8dbe8":"code","75485ba7":"code","2037b27a":"code","d06d62b0":"code","9ef2cf3b":"code","2d453b96":"code","529027ac":"code","4bb5d7a1":"code","f2ea38bc":"code","0a3c9624":"code","932c9353":"code","d32fefa1":"code","aecadfe4":"code","cedb58de":"code","615a5860":"code","ac6c1c4e":"code","562e2986":"code","ccfdc0ab":"code","b9169923":"code","865f74c1":"code","aea7754d":"code","f42546f9":"code","647a04b7":"code","b74ba219":"code","93e6221b":"markdown","4f5aca61":"markdown","61b834a7":"markdown"},"source":{"a4b8dbe8":"import pandas as pd\nimport numpy as np\nimport plotnine \nfrom plotnine import *\nimport os, sys, gc\nfrom tqdm.notebook import tqdm","75485ba7":"# \uacbd\ub85c\uc758 \uacbd\uc6b0 \uac01\uc790\uc758 \ud658\uacbd\uc5d0 \ub9de\uac8c \uc124\uc815\ud574\uc8fc\uba74 \ub429\ub2c8\ub2e4. \npath = '..\/input\/t-academy-recommendation2\/books\/'","2037b27a":"books = pd.read_csv(path + \"books.csv\")\nbook_tags = pd.read_csv(path + \"book_tags.csv\")\ntrain = pd.read_csv(path + \"train.csv\")\ntest = pd.read_csv(path + \"test.csv\")\ntags = pd.read_csv(path + \"tags.csv\")\nto_read = pd.read_csv(path + \"to_read.csv\")","d06d62b0":"train['book_id'] = train['book_id'].astype(str)\ntest['book_id'] = test['book_id'].astype(str)\nbooks['book_id'] = books['book_id'].astype(str)","9ef2cf3b":"popular_rec_model = books.sort_values(by='books_count', ascending=False)['book_id'].values[0:500]","2d453b96":"sol = test.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\ngt = {}\nfor user in tqdm(sol['user_id'].unique()): \n    gt[user] = list(sol[sol['user_id'] == user]['unique'].values[0])","529027ac":"rec_df = pd.DataFrame()\nrec_df['user_id'] = train['user_id'].unique()","4bb5d7a1":"## SGD\ub97c \uc774\uc6a9\ud55c \ud611\uc5c5\ud544\ud130\ub9c1 \uc9c4\ud589","f2ea38bc":"import numpy as np\nfrom tqdm import tqdm_notebook as tqdm\n\nimport numpy as np\n\n# Base code : https:\/\/yamalab.tistory.com\/92\nclass MatrixFactorization():\n    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n        \"\"\"\n        :param R: rating matrix\n        :param k: latent parameter\n        :param learning_rate: alpha on weight update\n        :param reg_param: beta on weight update\n        :param epochs: training epochs\n        :param verbose: print status\n        \"\"\"\n        self._R = R\n        self._num_users, self._num_items = R.shape\n        self._k = k\n        self._learning_rate = learning_rate\n        self._reg_param = reg_param\n        self._epochs = epochs\n        self._verbose = verbose\n\n\n    def fit(self):\n        \"\"\"\n        training Matrix Factorization : Update matrix latent weight and bias\n\n        \ucc38\uace0: self._b\uc5d0 \ub300\ud55c \uc124\uba85\n        - global bias: input R\uc5d0\uc11c \ud3c9\uac00\uac00 \ub9e4\uaca8\uc9c4 rating\uc758 \ud3c9\uade0\uac12\uc744 global bias\ub85c \uc0ac\uc6a9\n        - \uc815\uaddc\ud654 \uae30\ub2a5. \ucd5c\uc885 rating\uc5d0 \uc74c\uc218\uac00 \ub4e4\uc5b4\uac00\ub294 \uac83 \ub300\uc2e0 latent feature\uc5d0 \uc74c\uc218\uac00 \ud3ec\ud568\ub418\ub3c4\ub85d \ud574\uc90c.\n\n        :return: training_process\n        \"\"\"\n\n        # init latent features\n        self._P = np.random.normal(size=(self._num_users, self._k))\n        self._Q = np.random.normal(size=(self._num_items, self._k))\n\n        # init biases\n        self._b_P = np.zeros(self._num_users)\n        self._b_Q = np.zeros(self._num_items)\n        self._b = np.mean(self._R[np.where(self._R != 0)])\n\n        # train while epochs\n        self._training_process = []\n        for epoch in range(self._epochs):\n            # rating\uc774 \uc874\uc7ac\ud558\ub294 index\ub97c \uae30\uc900\uc73c\ub85c training\n            xi, yi = self._R.nonzero()\n            for i, j in zip(xi, yi):\n                self.gradient_descent(i, j, self._R[i, j])\n            cost = self.cost()\n            self._training_process.append((epoch, cost))\n\n            # print status\n            if self._verbose == True and ((epoch + 1) % 10 == 0):\n                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n\n\n    def cost(self):\n        \"\"\"\n        compute root mean square error\n        :return: rmse cost\n        \"\"\"\n\n        # xi, yi: R[xi, yi]\ub294 nonzero\uc778 value\ub97c \uc758\ubbf8\ud55c\ub2e4.\n        # \ucc38\uace0: http:\/\/codepractice.tistory.com\/90\n        xi, yi = self._R.nonzero()\n        # predicted = self.get_complete_matrix()\n        cost = 0\n        for x, y in zip(xi, yi):\n            cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n        return np.sqrt(cost\/len(xi))\n\n\n    def gradient(self, error, i, j):\n        \"\"\"\n        gradient of latent feature for GD\n\n        :param error: rating - prediction error\n        :param i: user index\n        :param j: item index\n        :return: gradient of latent feature tuple\n        \"\"\"\n\n        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n        return dp, dq\n\n\n    def gradient_descent(self, i, j, rating):\n        \"\"\"\n        graident descent function\n\n        :param i: user index of matrix\n        :param j: item index of matrix\n        :param rating: rating of (i,j)\n        \"\"\"\n\n        # get error\n        prediction = self.get_prediction(i, j)\n        error = rating - prediction\n\n        # update biases\n        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n        # update latent feature\n        dp, dq = self.gradient(error, i, j)\n        self._P[i, :] += self._learning_rate * dp\n        self._Q[j, :] += self._learning_rate * dq\n\n\n    def get_prediction(self, i, j):\n        \"\"\"\n        get predicted rating: user_i, item_j\n        :return: prediction of r_ij\n        \"\"\"\n        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n\n\n    def get_complete_matrix(self):\n        \"\"\"\n        computer complete matrix PXQ + P.bias + Q.bias + global bias\n\n        - PXQ \ud589\ub82c\uc5d0 b_P[:, np.newaxis]\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 \uc5f4\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n        - b_Q[np.newaxis:, ]\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 \ud589\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n        - b\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 element\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n\n        - newaxis: \ucc28\uc6d0\uc744 \ucd94\uac00\ud574\uc90c. 1\ucc28\uc6d0\uc778 Latent\ub4e4\ub85c 2\ucc28\uc6d0\uc758 R\uc5d0 \ud589\/\uc5f4 \ub2e8\uc704 \uc5f0\uc0b0\uc744 \ud574\uc8fc\uae30\uc704\ud574 \ucc28\uc6d0\uc744 \ucd94\uac00\ud558\ub294 \uac83.\n\n        :return: complete matrix R^\n        \"\"\"\n        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)","0a3c9624":"user2idx = {}\nfor i, l in enumerate(train['user_id'].unique()):\n    user2idx[l] = i\n    \nbook2idx = {}\nfor i, l in enumerate(train['book_id'].unique()):\n    book2idx[l] = i","932c9353":"idx2user = {i: user for user, i in user2idx.items()}\nidx2book = {i: item for item, i in book2idx.items()}","d32fefa1":"data = train[['user_id', 'book_id']].reset_index(drop=True)\nuseridx = data['useridx'] = train['user_id'].apply(lambda x: user2idx[x]).values\nbookidx = data['bookidx'] = train['book_id'].apply(lambda x: book2idx[x]).values\nrating = np.ones(len(data))","aecadfe4":"import scipy \n\npurchase_sparse = scipy.sparse.csr_matrix((rating, (useridx, bookidx)), shape=(len(set(useridx)), len(set(bookidx))))\npurchase_sparse","cedb58de":"R = purchase_sparse.toarray()","615a5860":"%%time\nfactorizer = MatrixFactorization(R, k=20, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\nfactorizer.fit()","ac6c1c4e":"del R; gc.collect() \nsgd_rec_model = factorizer.get_complete_matrix()","562e2986":"# \ub0b4\uac00 \uc77d\uc740 \ucc45\uc758 \ubaa9\ub85d\uc744 \ucd94\ucd9c \nread_list = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nread_list.head()","ccfdc0ab":"total_rec_list = {}\nfor user in tqdm(data['useridx'].unique()):\n    rec_list = []\n    \n    # \uae30\uc874\uc5d0 \ub9cc\ub4e0 Book ID\ub97c \ubcc0\uacbd \n    rating_scores = [(idx2book[i], c) for i, c in enumerate(sgd_rec_model[user]) if i != user] # \uc790\uae30 \uc790\uc2e0\uc774 \ucd94\ucc9c\uc548\ub418\ub3c4\ub85d \n    rating_scores = sorted(rating_scores, key = lambda x: x[1], reverse=True) # \ud3c9\uc810\uc774 \ub192\uc740 \uc21c\uc11c\ub300\ub85c \uc815\ub82c \n    \n    seen = read_list[read_list['user_id'] == idx2user[user]]['unique'].values[0]\n    for rec in rating_scores[0:250]: \n        if rec[0] not in seen:\n            rec_list.append(rec[0])\n    \n    if len(rec_list) < 200:\n        for i in popular_rec_model[0:200]:\n            if rec not in seen:\n                rec_list.append(rec)\n\n    total_rec_list[idx2user[user]] = rec_list[0:200]","b9169923":"import six\nimport math\n\n# https:\/\/github.com\/kakao-arena\/brunch-article-recommendation\/blob\/master\/evaluate.py\n\nclass evaluate():\n    def __init__(self, recs, gt, topn=100):\n        self.recs = recs\n        self.gt = gt \n        self.topn = topn \n        \n    def _ndcg(self):\n        Q, S = 0.0, 0.0\n        for u, seen in six.iteritems(self.gt):\n            seen = list(set(seen))\n            rec = self.recs.get(u, [])\n            if not rec or len(seen) == 0:\n                continue\n\n            dcg = 0.0\n            idcg = sum([1.0 \/ math.log(i + 2, 2) for i in range(min(len(seen), len(rec)))])\n            for i, r in enumerate(rec):\n                if r not in seen:\n                    continue\n                rank = i + 1\n                dcg += 1.0 \/ math.log(rank + 1, 2)\n            ndcg = dcg \/ idcg\n            S += ndcg\n            Q += 1\n        return S \/ Q\n\n\n    def _map(self):\n        n, ap = 0.0, 0.0\n        for u, seen in six.iteritems(self.gt):\n            seen = list(set(seen))\n            rec = self.recs.get(u, [])\n            if not rec or len(seen) == 0:\n                continue\n\n            _ap, correct = 0.0, 0.0\n            for i, r in enumerate(rec):\n                if r in seen:\n                    correct += 1\n                    _ap += (correct \/ (i + 1.0))\n            _ap \/= min(len(seen), len(rec))\n            ap += _ap\n            n += 1.0\n        return ap \/ n\n\n\n    def _entropy_diversity(self):\n        sz = float(len(self.recs)) * self.topn\n        freq = {}\n        for u, rec in six.iteritems(self.recs):\n            for r in rec:\n                freq[r] = freq.get(r, 0) + 1\n        ent = -sum([v \/ sz * math.log(v \/ sz) for v in six.itervalues(freq)])\n        return ent\n    \n    def _evaluate(self):\n        print('MAP@%s: %s' % (self.topn, self._map()))\n        print('NDCG@%s: %s' % (self.topn, self._ndcg()))\n        print('EntDiv@%s: %s' % (self.topn, self._entropy_diversity()))","865f74c1":"evaluate_func = evaluate(recs=total_rec_list, gt = gt, topn=200)\nevaluate_func._evaluate()","aea7754d":"from implicit.evaluation import  *\nfrom implicit.als import AlternatingLeastSquares as ALS\nfrom implicit.bpr import BayesianPersonalizedRanking as BPR\n\nals_model = ALS(factors=20, regularization=0.01, iterations = 100)\nals_model.fit(purchase_sparse.T)","f42546f9":"als_model.recommend(0, purchase_sparse, N=200)[0:10]","647a04b7":"total_rec_list = {}\nfor user in tqdm(data['useridx'].unique()):\n    rec_list = []\n    \n    # \uae30\uc874\uc5d0 \ub9cc\ub4e0 Book ID\ub97c \ubcc0\uacbd \n    seen = read_list[read_list['user_id'] == idx2user[user]]['unique'].values[0]\n    recs = als_model.recommend(user, purchase_sparse, N=250)\n    recs = [idx2book[x[0]] for x in recs][0:250]  \n    \n    for rec in recs: \n        if rec not in seen:\n            rec_list.append(rec)\n    \n    if len(rec_list) < 200:\n        for i in popular_rec_model[0:200]:\n            if rec not in seen:\n                rec_list.append(rec)\n\n    total_rec_list[idx2user[user]] = rec_list[0:200]","b74ba219":"evaluate_func = evaluate(recs=total_rec_list, gt = gt, topn=200)\nevaluate_func._evaluate()","93e6221b":"## ALS \ubc29\uc2dd\uc744 \uc774\uc6a9\ud55c \ud611\uc5c5\ud544\ud130\ub9c1","4f5aca61":"# \uae30\uc874 Author \ubaa8\ub378 \nMAP@200: 0.011817742695221473\nNDCG@200: 0.004577031690605801\nEntDiv@200: 5.097773423502677","61b834a7":"# 1. Goodbooks-10k \n- Link : https:\/\/www.kaggle.com\/zygmunt\/goodbooks-10k"}}