{"cell_type":{"d8c5ac32":"code","62a95767":"code","c2044066":"code","d8e0feed":"code","80ccad1e":"code","4161b250":"code","87084203":"code","980e5b1f":"code","db4e2ef1":"code","add17191":"code","8f7c57f9":"code","a789b669":"code","12a7bc6e":"code","022d3273":"code","3a102b18":"code","636f0886":"code","7469ecad":"code","f40c4c29":"code","bf8aca76":"code","0ba0586d":"code","229472bc":"code","8ae386ed":"code","68606100":"code","0b0d5a7c":"code","f10ed0bf":"code","8276cc0d":"code","c54ed713":"code","4788a310":"code","b890ab8c":"code","ad0bc4c9":"code","47e0563c":"code","c560cd7b":"code","888b9f13":"code","a9159910":"markdown","eb474a6c":"markdown","1aeb2a4d":"markdown","bd105bbf":"markdown"},"source":{"d8c5ac32":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","62a95767":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n# this does not help so we can remove it\ndf_train.drop(columns=['Id'], axis=1, inplace=True)\ndf_test.drop(columns=['Id'], axis=1, inplace=True)\n# just save the shape for later to split train and test (see bellow)\nn_train = df_train.shape[0]\nn_test = df_test.shape[0]","c2044066":"y_train = df_train.SalePrice.values\n# getting train and test data together for an easier workflow:\nall_data = pd.concat((df_train, df_test), sort=True).reset_index(drop = True)\nall_data.drop(['SalePrice'], axis = 1, inplace = True)","d8e0feed":"# getting rid of all columns that have more than 90% data missing\nall_data = all_data.dropna(thresh=len(all_data)*0.9, axis=1)\n# deleted: \"Alley\",\"PoolQC\",\"MiscFeature\",\"Fence\",\"FireplaceQu\" - though you can keep it if you desire a more complex model","80ccad1e":"# now will take care of the missing values and assign different dtypes to some variables, e.g.: that should be categorical variables not numerical(years, months).\nall_data['GarageYrBlt']=all_data[\"GarageYrBlt\"].fillna(1980)\nmissing_val_col0 = ['Electrical',\n                    'SaleType',\n                    'KitchenQual',\n                    'Exterior1st',\n                    'Exterior2nd',\n                    'Functional',\n                    'Utilities',\n                    'MSZoning']\nfor i in missing_val_col0:\n    all_data[i] = all_data[i].fillna(method='ffill')\nall_data['MasVnrType']=all_data['MasVnrType'].fillna(0)\nmissing_val_col1 = [\"GarageType\",\n                   \"GarageFinish\",\n                   \"GarageQual\",\n                   \"GarageCond\",\n                   'BsmtQual',\n                   'BsmtCond',\n                   'BsmtExposure',\n                   'BsmtFinType1',\n                   'BsmtFinType2'] \nfor i in missing_val_col1:\n    all_data[i] = all_data[i].fillna('None')\nmissing_val_col2 = ['BsmtFinSF1',\n                    'BsmtFinSF2',\n                    'BsmtUnfSF',\n                    'TotalBsmtSF',\n                    'BsmtFullBath', \n                    'BsmtHalfBath',\n                    'GarageArea',\n                    'GarageCars',\n                    'MasVnrArea']\nfor i in missing_val_col2:\n    all_data[i] = all_data[i].fillna(0)","4161b250":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\").astype(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str) \nall_data['OverallQual'] = all_data['OverallQual'].astype(str)\nall_data['YearBuilt'] = all_data['YearBuilt'].astype(int)\nall_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(int)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str) ","87084203":"# and for whatever has been left out: \nNAcols = all_data.columns\nfor col in NAcols:\n    if all_data[col].dtype == \"object\": # categorical values\n        all_data[col] = all_data[col].fillna(\"None\")\nfor col in NAcols:\n    if all_data[col].dtype != \"object\": # numerical values\n        all_data[col]= all_data[col].fillna(0)","980e5b1f":"# quick check to see if still missing values:\nall_data.isnull().sum().sort_values(ascending=False).head()","db4e2ef1":"# quick feature engineering\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'] + all_data['GrLivArea'] + all_data['GarageArea']\nall_data['Bathrooms'] = all_data['FullBath'] + all_data['HalfBath']*0.5 \nall_data['Year average']= (all_data['YearRemodAdd']+all_data['YearBuilt'])\/2","add17191":"from scipy.stats import skew\n# what one has to do for a normal distribution...\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","8f7c57f9":"all_data = pd.get_dummies(all_data, drop_first=True)\ntrain = all_data[:n_train]\ntest = all_data[n_train:]\ny_train = np.log1p(y_train)\ntrain.drop(index= train[(train.GrLivArea > 4600) & (train['MasVnrArea'] > 1500)].index.tolist(), inplace=True) #getting rid of outliers ","a789b669":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train, y_train, test_size = .33, random_state = 0)","12a7bc6e":"from sklearn.preprocessing import RobustScaler \n# which is pretty similar to StandardScaler, though is better for outliers\nscaler= RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\ntest = scaler.transform(test)","022d3273":"from sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor","3a102b18":"def rmse(y_val, y_pred):\n     return np.sqrt(mean_squared_error(y_val, y_pred)) ","636f0886":"randforest = RandomForestRegressor(n_estimators=300)\nrandforest.fit(X_train, y_train)","7469ecad":"ridge = Ridge()\nparameters = {'alpha':[x for x in range(0,101)]}\n\nridgeCV = GridSearchCV(ridge, param_grid=parameters, \n                       scoring='neg_mean_squared_error', cv=15)\nridgeCV.fit(X_train,y_train)\n\nprint(\"The best value of alpha is: \", ridgeCV.best_params_)\nprint(f'The best score, which is the mean of scores, achieved with {ridgeCV.best_params_} is: {np.sqrt(-ridgeCV.best_score_)} ')","f40c4c29":"ridgeCV = Ridge(alpha=8)\nridgeCV.fit(X_train, y_train)","bf8aca76":"lasso = Lasso()\nparameters = {'alpha':[0.0001,0.0002,0.0003,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100], \n             } # 'max_iter':[2000, 3000, 5000]\n\nlassoCV = GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nlassoCV.fit(X_train,y_train)\n\nprint('The best value of alpha is: ',lassoCV.best_params_)\nprint(\"The best score achieved with alpha is: \", np.sqrt(-lassoCV.best_score_))","0ba0586d":"# I thought to give it a try to LassoCV which actually got me some better score than having CV on Lasso.\nfrom sklearn.linear_model import LassoCV\nreg = LassoCV(cv=5, random_state=0).fit(X_train, y_train)","229472bc":"gbr = GradientBoostingRegressor(n_estimators=4000, random_state=9, learning_rate=0.01, max_depth=5, \n                                max_features='sqrt', min_samples_leaf=6, min_samples_split=30, loss='huber')\ngbr.fit(X_train, y_train)","8ae386ed":"xgbr = XGBRegressor(n_estimators=3500, learning_rate=0.01, max_depth=3, gamma=0.001, subsample=0.7,\n                    colsample_bytree=0.7, objective='reg:linear', nthread=-1, seed=9, reg_alpha=0.0001) \nxgbr.fit(X_train, y_train)","68606100":"print(f\" randforest score on train set: {randforest.score(X_train, y_train)}\")\nprint(f\" randforest score on val. set: {randforest.score(X_val, y_val)}\", \"\\n\")\nprint(f\" ridgeCV score on train set: {ridgeCV.score(X_train, y_train)}\")\nprint(f\" ridgeCV score on val. set: {ridgeCV.score(X_val, y_val)}\", \"\\n\")\nprint(f\" lassoCV score on train set: {reg.score(X_train, y_train)}\")\nprint(f\" lassoCV score on val. set: {reg.score(X_val, y_val)}\")\nprint(f\" Number of features used by lassoCV: {np.sum(reg.coef_ != 0)}\", \"\\n\")\nprint(f\" gbr score on train set: {gbr.score(X_train, y_train)}\")\nprint(f\" gbr score on val. set: {gbr.score(X_val, y_val)}\", \"\\n\")\nprint(f\" xgbr score on train set: {xgbr.score(X_train, y_train)}\")\nprint(f\" xgbr score on val. set:  {xgbr.score(X_val, y_val)}\", \"\\n\")\n\ny_pred = randforest.predict(X_val)\nprint(f\"RandForest Root Mean Square Error validation = {rmse(y_val, y_pred)}\")\ny_pred_ridge = ridgeCV.predict(X_val)\nprint(f\"RidgeCV Root Mean Square Error validation = {rmse(y_val, y_pred_ridge)}\")\ny_pred_CV = lassoCV.predict(X_val)\nprint(f\"Simple Lasso Root Mean Square Error test = {rmse(y_val, y_pred_CV)}\")\ny_pred_reg = reg.predict(X_val)\nprint(f\"LassoCV Root Mean Square Error test = {rmse(y_val, y_pred_reg)}\")  # slightly better score\ny_pred_gbr = gbr.predict(X_val)\nprint(f\"GBR Root Mean Square Error test = {rmse(y_val, y_pred_gbr)}\")     \ny_pred_xgbr = xgbr.predict(X_val)\nprint(f\"XGBR Root Mean Square Error test = {rmse(y_val, y_pred_xgbr)}\")","0b0d5a7c":"# now let`s have a look at ensembles (keeping in mind that is always good to pick \"diverse\" models, including more 'risky' ones)\nfrom sklearn.ensemble import VotingRegressor\nvoter = VotingRegressor([('Ridge', ridgeCV), ('XGBRegressor', xgbr), ('GradientBoostingRegressor', gbr)]) #('XGBRegressor', xgbr) ('GradientBoostingRegressor', gbr)\nvoter.fit(X_train, y_train.ravel())\ny_pred_voter = voter.predict(X_val)\nprint(f\"Root Mean Square Error test = {rmse(y_val, y_pred_voter)}\")","f10ed0bf":"from mlxtend.regressor import StackingRegressor\nstacker = StackingRegressor(regressors = [ridge, xgbr, gbr], \n                           meta_regressor = voter, use_features_in_secondary=True) #voting had the best score\nstacker.fit(X_train, y_train.ravel())\ny_pred_stacker = stacker.predict(X_val)\nprint(f\"Root Mean Square Error test = {rmse(y_val, y_pred_stacker)}\")","8276cc0d":"final_validation = (0.2*y_pred_voter + 0.4*y_pred_stacker + 0.4*y_pred_xgbr )\n\nprint(f\"Root Mean Square Error test = {rmse(y_val, final_validation)}\")","c54ed713":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras import optimizers","4788a310":"n_cols = train.shape[1]\nearly_stopping_monitor = EarlyStopping(patience=3)","b890ab8c":"model = Sequential()\nmodel.add(Dense(4, activation='elu', input_shape = (n_cols,))) #, kernel_initializer='normal'\nmodel.add(Dense(2, activation='elu'))\nmodel.add(Dense(1, activation='linear', kernel_regularizer = 'l2',\n                kernel_initializer='normal'))","ad0bc4c9":"model.compile(optimizer='adam', loss='mean_squared_error')","47e0563c":"history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=300, batch_size=18\n                       , callbacks=[early_stopping_monitor])","c560cd7b":"import matplotlib.pyplot as plt\nprint(history.history.keys())\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","888b9f13":"score = model.evaluate(X_val, y_val, batch_size=32, verbose=1)\nprint('Test score:', score)","a9159910":"Hi everyone, \n\nThis is some mix of options that I have chosen. \n\nWas a good exercise to play with CV, these easy to use ensembles.\nThough this might not be a superb data set for keras .. I'll give it a go at the end just for fun. ","eb474a6c":"due to such beautiful other EDA kernels, and since I want to focus more on the CV, models and ensembles this time, I have chosen to go on without taking you through EDA and beautiful charts. But if you really want to see plots you can check my kernel on classification. :)","1aeb2a4d":"This is my quick take. Hope you've found bits and bites useful. ","bd105bbf":"Now will have a quick go with Keras:"}}