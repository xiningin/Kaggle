{"cell_type":{"e1af9b3a":"code","5742fe09":"code","ae3fe4a8":"code","06729f1c":"code","766d40a5":"code","f1169d95":"code","9b166d34":"code","78503df7":"code","34855252":"code","d6514772":"code","6a22b9d2":"code","94544e4b":"code","a7c3393b":"code","702f439d":"code","b0d66236":"code","96ba4acd":"code","b7506df9":"code","85c6d676":"code","ba903fac":"code","0d8c46e7":"code","dbb9effd":"code","3399d62f":"code","c932aa9f":"code","9f74aff0":"code","91607645":"code","d46fdf3d":"code","2ea467de":"code","79a0dc33":"code","7bfd67d4":"code","0e80d2ec":"code","19dbf484":"code","7f4dad70":"code","e8dad99b":"code","bd4c699d":"code","111f7d61":"code","254ca3b1":"code","a400d529":"code","3456f45e":"code","7a9cb6be":"code","e1ff84fe":"code","0caaeb3b":"code","e731289b":"code","9d67dc4b":"code","7a8273b9":"code","f4a37dde":"code","65afeb07":"code","a5fd6a1a":"code","b5f22a92":"code","bb2ec83d":"code","75637587":"code","9026b41a":"code","dbc444fe":"code","6a3121d5":"code","0c9d4be0":"markdown","36ef30d6":"markdown","c3f349d5":"markdown","c74f60ca":"markdown","b479c9dd":"markdown","1d2b37c0":"markdown","72d36db5":"markdown","dd9cb3bd":"markdown","0fc61f5a":"markdown","f180cac6":"markdown","a11fcfe1":"markdown","ca5ab315":"markdown","12c48fd6":"markdown","c5342332":"markdown","317a82c1":"markdown","2c5b1fb7":"markdown","c0d6d788":"markdown","5ceba2b5":"markdown","62292071":"markdown","38a0aab1":"markdown","977f64e5":"markdown","93f7c420":"markdown","4f664124":"markdown","dce9a6c2":"markdown","39544569":"markdown","3e04f873":"markdown","740ea789":"markdown","65c3b5d3":"markdown","176a63e4":"markdown","d23e5bc5":"markdown","22301013":"markdown","b851b9c8":"markdown","1a7fba83":"markdown","64844d07":"markdown","0590e41e":"markdown","5610e61b":"markdown","bc043229":"markdown","1d3d565c":"markdown","a7ee52d8":"markdown","106de13b":"markdown","57c2cf71":"markdown","f237d52e":"markdown","41adc31a":"markdown","405fc27f":"markdown","4b17ea89":"markdown","a2957153":"markdown","63332a20":"markdown","7eaf400b":"markdown","ac799cdb":"markdown","bcd8aaf5":"markdown"},"source":{"e1af9b3a":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nfrom statsmodels.stats.diagnostic import het_white\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt\nfrom sklearn.metrics import r2_score\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV","5742fe09":"# plot theme\nplt.style.use('ggplot')","ae3fe4a8":"df = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')","06729f1c":"df.head()","766d40a5":"#drop column\ndf.drop(columns = ['id', 'host_id', 'host_name', 'last_review', 'name'], axis = 1, inplace = True)","f1169d95":"#quick info\ndf.info()","9b166d34":"#check for NA values\ndf.isna().sum()","78503df7":"df['reviews_per_month'].fillna('0', inplace=True)","34855252":"#check for NA values\ndf.isna().sum()","d6514772":"df[\"neighbourhood_group\"].value_counts()","6a22b9d2":"plt.figure(figsize=(15,7))\nsns.countplot(x=\"neighbourhood_group\", data=df,\n            order=df[\"neighbourhood_group\"].value_counts().index,\n            palette = sns.diverging_palette(220, 20, n=12))\nplt.show()","94544e4b":"df[\"neighbourhood\"].value_counts().head(10)","a7c3393b":"plt.figure(figsize=(15,7))\nsns.countplot(x=\"neighbourhood\", data=df,\n            order=df[\"neighbourhood\"].value_counts().head(10).index,\n            palette = sns.diverging_palette(220, 20, n=25))\nplt.show()","702f439d":"df[\"room_type\"].value_counts()","b0d66236":"plt.figure(figsize=(15,7))\nsns.countplot(x=\"room_type\", data=df,\n            order=df[\"room_type\"].value_counts().index,\n            palette = sns.diverging_palette(220, 20, n=8))\n            \nplt.show()","96ba4acd":"numeric = df.select_dtypes('number').drop(columns=[\"price\"])\nf, axes = plt.subplots(3, 2, figsize=(20, 12))\n#mylist = [[0,0], [0,1], [1,0], [1,1], [2,0], [2,1], [3,0]] axes locations\nmylist1 = [0, 0, 1, 1, 2, 2]\nmylist2 = [0, 1, 0, 1, 0, 1]\nfor i, j in enumerate(numeric.columns): #i: axes locations, j: column names\n    sns.histplot(x=df[j], ax=axes[mylist1[i], mylist2[i]], bins=50, color=\"steelblue\", alpha=1)","b7506df9":"f, axes = plt.subplots(3, 2, figsize=(20, 12))\n#mylist = [[0,0], [0,1], [1,0], [1,1], [2,0], [2,1], [3,0]] axes locations\nmylist1 = [0, 0, 1, 1, 2, 2]\nmylist2 = [0, 1, 0, 1, 0, 1]\nfor i, j in enumerate(numeric.columns): #i: axes locations, j: column names\n    sns.violinplot(x=df[j], ax=axes[mylist1[i], mylist2[i]], color=\"steelblue\")","85c6d676":"f, axes = plt.subplots(1,3, figsize=(20,8))\nsns.distplot(df['price'], color=\"salmon\", ax=axes[0])\nsns.distplot(np.log(df['price']+1),color=\"salmon\", ax=axes[1])\nsns.violinplot(x=np.log(df[\"price\"]+1), color=\"salmon\", ax=axes[2])\naxes[1].set_xlabel('log+1')\naxes[2].set_xlabel('log+1')\nplt.show()","ba903fac":"#log+1 transform \"price\"\ndf[\"price\"] = np.log(df[\"price\"]+1)","0d8c46e7":"########################################## DATA SPLITTING  ################################################\nX = df.drop(columns=['price'])\ny = df['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)","dbb9effd":"########################################## PREPROCESSING  ################################################\n#categorical variables\ncategorical_variables = [\"neighbourhood_group\", \"neighbourhood\", \"room_type\"]\ncategorical_transformer = Pipeline(\n        [\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n        ]\n)\n\n#numeric variables\nnumeric_transformer = Pipeline(\n        [\n            (\"scaler\", StandardScaler())\n        ]\n)\n\n#combine into ColumnTransformer\npreprocessor = ColumnTransformer(\n    [\n        (\"categoricals\", categorical_transformer, categorical_variables),\n        (\"numericals\", numeric_transformer, numeric.columns)\n    ],\n    remainder=\"passthrough\"\n)\n\n########################################## PREPROCESSING POLY  ################################################\nPoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\npolynomial_transformer = Pipeline(\n        [\n            (\"poly\", Poly)\n        ]\n)\n\npreprocessor_poly = ColumnTransformer(\n    [\n        (\"categoricals\", categorical_transformer, categorical_variables),\n        (\"polynomials\", polynomial_transformer, numeric.columns),\n        (\"numericals\", numeric_transformer, numeric.columns),\n    ],\n    remainder=\"passthrough\"\n)","3399d62f":"linreg = LinearRegression()\nridge = Ridge()\nlasso = Lasso()\n\nparams_linreg = {'model__fit_intercept':[True,False], \n                  'model__normalize':[True,False], \n                  'model__copy_X':[True, False]}\nparams_ridge = {'model__alpha':[0.001,0.01,0.1,1,10,100,1000]}\nparams_lasso = {'model__alpha':[0.0001,0.001,0.01,0.1,1]}","c932aa9f":"@ignore_warnings(category=ConvergenceWarning)\ndef cv_model(X_train, y_train, model, params, cv=5):\n    ##########################################  BUILDING MDOEL  #################################################\n    pipe = Pipeline(\n        [\n            (\"preprocessing\", preprocessor),\n            (\"model\", model)\n        ]\n    )\n    ##########################################  GRIDSEARCH & CV  #################################################\n    #gridsearch parameters\n    params = params\n\n\n    #gridsearch \n    cv = GridSearchCV(\n      pipe, \n      params, \n      cv = cv, \n      scoring = \"neg_mean_squared_error\", \n      )\n\n    #results\n    cv.fit(X_train, y_train)\n    print(f'Best score: {cv.best_score_:.3f}\\n')\n    print(f'Best parameter set: {cv.best_params_}\\n')\n    print(f'RMSE: {sqrt(abs(cv.best_score_)):.3f}')","9f74aff0":"print(\"####################  Linear Regression  ####################\")\ncv_model(X_train, y_train, linreg, params_linreg)\n\nprint(\"\\n\\n\")\n\nprint(\"####################  Ridge Regression  ####################\")\ncv_model(X_train, y_train, ridge, params_ridge)\nprint(\"\\n\\n\")\n\nprint(\"####################  Lasso Regression  ####################\")\ncv_model(X_train, y_train, lasso, params_lasso)","91607645":"@ignore_warnings(category=ConvergenceWarning)\ndef cv_model_poly(X_train, y_train, model, params, cv=5):\n    ##########################################  BUILDING MDOEL  #################################################\n    pipe = Pipeline(\n        [\n            (\"preprocessing\", preprocessor_poly),\n            (\"model\", model)\n        ]\n    )\n    ##########################################  GRIDSEARCH & CV  #################################################\n    #gridsearch parameters\n    params = params\n\n\n    #gridsearch \n    cv = GridSearchCV(\n      pipe, \n      params, \n      cv = cv, \n      scoring = \"neg_mean_squared_error\", \n      )\n\n    #results\n    cv.fit(X_train, y_train)\n    print(f'Best score: {cv.best_score_:.3f}\\n')\n    print(f'Best parameter set: {cv.best_params_}\\n')\n    print(f'RMSE: {sqrt(abs(cv.best_score_)):.3f}')","d46fdf3d":"print(\"####################  Linear Regression  ####################\")\ncv_model_poly(X_train, y_train, linreg, params_linreg)\nprint(\"\\n\\n\")\n\nprint(\"####################  Ridge Regression  ####################\")\ncv_model_poly(X_train, y_train, ridge, params_ridge)\nprint(\"\\n\\n\")\n\nprint(\"####################  Lasso Regression  ####################\")\ncv_model_poly(X_train, y_train, lasso, params_lasso)","2ea467de":"f, axes = plt.subplots(3, 2, figsize=(20, 12))\n#mylist = [[0,0], [0,1], [1,0], [1,1], [2,0], [2,1], [3,0]] axes locations\nmylist1 = [0, 0, 1, 1, 2, 2]\nmylist2 = [0, 1, 0, 1, 0, 1]\nfor i, j in enumerate(numeric.columns): #i: axes locations, j: column names\n    sns.violinplot(x=df[j], ax=axes[mylist1[i], mylist2[i]], color=\"steelblue\")","79a0dc33":"sns.violinplot(x=df[~(df[\"minimum_nights\"]>35)][\"minimum_nights\"], color=\"steelblue\")\nplt.show()\ndf[df[\"minimum_nights\"]>35].shape","7bfd67d4":"sns.violinplot(x=df[~(df[\"number_of_reviews\"]>150)][\"number_of_reviews\"], color=\"steelblue\")\nplt.show()\ndf[df[\"number_of_reviews\"]>150].shape","0e80d2ec":"sns.violinplot(x=df[~(df[\"calculated_host_listings_count\"]>10)][\"calculated_host_listings_count\"], color=\"steelblue\")\nplt.show()\ndf[df[\"calculated_host_listings_count\"]>10].shape","19dbf484":"df1 = df[~(df[\"price\"]<3) | (df[\"price\"]>8)]","7f4dad70":"df1.head()","e8dad99b":"#remove extreme outliers in target variable\ndf1 = df[~(df[\"price\"]<3) | (df[\"price\"]>8)]\n#remove extreme outliers in minimum_nights\ndf1 = df1[~(df[\"minimum_nights\"]>35)]\n#remove extreme outliers in number_of_reviews\ndf1 = df1[~(df[\"number_of_reviews\"]>150)]\n#remove extreme outliers in calculated_host_listings_count\ndf1 = df1[~(df[\"calculated_host_listings_count\"]>10)]","bd4c699d":"########################################## DATA SPLITTING  ################################################\nX_outliers = df1.drop(columns=['price'])\ny_outliers = df1['price']\nX_outliers_train, X_outliers_test, y_outliers_train, y_outliers_test = train_test_split(X_outliers, y_outliers, test_size=0.3, random_state=10)","111f7d61":"print(\"####################  Linear Regression  ####################\")\ncv_model(X_outliers_train, y_outliers_train, linreg, params_linreg)\nprint(\"\\n\\n\")\n\nprint(\"####################  Ridge Regression  ####################\")\ncv_model(X_outliers_train, y_outliers_train, ridge, params_ridge)\nprint(\"\\n\\n\")\n\nprint(\"####################  Lasso Regression  ####################\")\ncv_model(X_outliers_train, y_outliers_train, lasso, params_lasso)","254ca3b1":"print(\"####################  Linear Regression  ####################\")\ncv_model_poly(X_outliers_train, y_outliers_train, linreg, params_linreg)\nprint(\"\\n\\n\")\n\nprint(\"####################  Ridge Regression  ####################\")\ncv_model_poly(X_outliers_train, y_outliers_train, ridge, params_ridge)\nprint(\"\\n\\n\")\n\nprint(\"####################  Lasso Regression  ####################\")\ncv_model_poly(X_outliers_train, y_outliers_train, lasso, params_lasso)","a400d529":"#Linear Regression\nlr_pred = LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\npipe_lr = Pipeline(\n        [\n            (\"preprocessing\", preprocessor),\n            (\"lr\", lr_pred)\n        ]\n)\n\npipe_lr.fit(X_train, y_train)\nprediction_lr = pipe_lr.predict(X_test)\n\n#Ridge\nridge_pred = Ridge(alpha=10)\npipe_ridge = Pipeline(\n        [\n            (\"preprocessing\", preprocessor),\n            (\"ridge\", ridge_pred)\n        ]\n)\n\npipe_ridge.fit(X_train, y_train)\nprediction_ridge = pipe_ridge.predict(X_test)\n\n#Lasso\nlasso_pred = Lasso(alpha=0.0001)\npipe_lasso = Pipeline(\n        [\n            (\"preprocessing\", preprocessor),\n            (\"lasso\", lasso_pred)\n        ]\n)\n\npipe_lasso.fit(X_train, y_train)\nprediction_lasso = pipe_lasso.predict(X_test)","3456f45e":"#Linear Regression\nlr_pred_poly = LinearRegression(copy_X=True, fit_intercept=True, normalize=True)\npipe_lr_poly = Pipeline(\n        [\n            (\"preprocessing\", preprocessor_poly),\n            (\"lr\", lr_pred_poly)\n        ]\n)\n\npipe_lr_poly.fit(X_train, y_train)\nprediction_lr_poly = pipe_lr_poly.predict(X_test)\n\n#Ridge\nridge_pred_poly = Ridge(alpha=100)\npipe_ridge_poly = Pipeline(\n        [\n            (\"preprocessing\", preprocessor_poly),\n            (\"ridge\", ridge_pred_poly)\n        ]\n)\n\npipe_ridge_poly.fit(X_train, y_train)\nprediction_ridge_poly = pipe_ridge_poly.predict(X_test)\n\n#Lasso\nlasso_pred_poly = Lasso(alpha=0.0001)\npipe_lasso_poly = Pipeline(\n        [\n            (\"preprocessing\", preprocessor_poly),\n            (\"lasso\", lasso_pred_poly)\n        ]\n)\n\npipe_lasso_poly.fit(X_train, y_train)\nprediction_lasso_poly = pipe_lasso_poly.predict(X_test)","7a9cb6be":"#Linear Regression\nlr_pred = LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\npipe_lr = Pipeline(\n        [\n            (\"preprocessing\", preprocessor),\n            (\"lr\", lr_pred)\n        ]\n)\n\npipe_lr.fit(X_outliers_train, y_outliers_train)\nprediction_lr_outliers = pipe_lr.predict(X_outliers_test)\n\n#Ridge\nridge_pred = Ridge(alpha=1)\npipe_ridge = Pipeline(\n        [\n            (\"preprocessing\", preprocessor),\n            (\"ridge\", ridge_pred)\n        ]\n)\n\npipe_ridge.fit(X_outliers_train, y_outliers_train)\nprediction_ridge_outliers = pipe_ridge.predict(X_outliers_test)\n\n#Lasso\nlasso_pred = Lasso(alpha=0.0001)\npipe_lasso = Pipeline(\n        [\n            (\"preprocessing\", preprocessor),\n            (\"lasso\", lasso_pred)\n        ]\n)\n\npipe_lasso.fit(X_outliers_train, y_outliers_train)\nprediction_lasso_outliers = pipe_lasso.predict(X_outliers_test)","e1ff84fe":"#Linear Regression\nlr_pred_poly = LinearRegression(copy_X=True, fit_intercept=True, normalize=True)\npipe_lr_poly = Pipeline(\n        [\n            (\"preprocessing\", preprocessor_poly),\n            (\"lr\", lr_pred_poly)\n        ]\n)\n\npipe_lr_poly.fit(X_outliers_train, y_outliers_train)\nprediction_lr_poly_outliers = pipe_lr_poly.predict(X_outliers_test)\n\n#Ridge\nridge_pred_poly = Ridge(alpha=1)\npipe_ridge_poly = Pipeline(\n        [\n            (\"preprocessing\", preprocessor_poly),\n            (\"ridge\", ridge_pred_poly)\n        ]\n)\n\npipe_ridge_poly.fit(X_outliers_train, y_outliers_train)\nprediction_ridge_poly_outliers = pipe_ridge_poly.predict(X_outliers_test)\n\n#Lasso\nlasso_pred_poly = Lasso(alpha=0.0001)\npipe_lasso_poly = Pipeline(\n        [\n            (\"preprocessing\", preprocessor_poly),\n            (\"lasso\", lasso_pred_poly)\n        ]\n)\n\npipe_lasso_poly.fit(X_outliers_train, y_outliers_train)\nprediction_lasso_poly_outliers = pipe_lasso_poly.predict(X_outliers_test)","0caaeb3b":"print(\"###############################################  PHASE 1  ###################################################\\n\")\nprint(\"-------------------  Linear Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_test, prediction_lr))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_test, prediction_lr)))   \nprint('R2 %0.3f'% r2_score(y_test, prediction_lr))\n\nprint(\"-------------------  Ridge Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_test, prediction_ridge))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_test, prediction_ridge)))   \nprint('R2 %0.3f'% r2_score(y_test, prediction_ridge))\n\nprint(\"-------------------  Lasso Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_test, prediction_lasso))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_test, prediction_lasso)))   \nprint('R2 %0.3f'% r2_score(y_test, prediction_lasso))\n\n\nprint(\"###############################################  PHASE 2  ###################################################\\n\")\nprint(\"-------------------  Linear Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_test, prediction_lr_poly))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_test, prediction_lr_poly)))   \nprint('R2 %0.3f'% r2_score(y_test, prediction_lr_poly))\n\nprint(\"-------------------  Ridge Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_test, prediction_ridge_poly))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_test, prediction_ridge_poly)))   \nprint('R2 %0.3f'% r2_score(y_test, prediction_ridge_poly))\n\nprint(\"-------------------  Lasso Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_test, prediction_lasso_poly))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_test, prediction_lasso_poly)))   \nprint('R2 %0.3f'% r2_score(y_test, prediction_lasso_poly))\n\n\nprint(\"###############################################  PHASE 3  ###################################################\\n\")\nprint(\"-------------------  Linear Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_outliers_test, prediction_lr_outliers))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_outliers_test, prediction_lr_outliers)))   \nprint('R2 %0.3f'% r2_score(y_outliers_test, prediction_lr_outliers))\n\nprint(\"-------------------  Ridge Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_outliers_test, prediction_ridge_outliers))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_outliers_test, prediction_ridge_outliers)))   \nprint('R2 %0.3f'% r2_score(y_outliers_test, prediction_ridge_outliers))\n\nprint(\"-------------------  Lasso Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_outliers_test, prediction_lasso_outliers))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_outliers_test, prediction_lasso_outliers)))   \nprint('R2 %0.3f'% r2_score(y_outliers_test, prediction_lasso_outliers))\n\n\nprint(\"###############################################  PHASE 4  ###################################################\\n\")\nprint(\"-------------------  Linear Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_outliers_test, prediction_lr_poly_outliers))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_outliers_test, prediction_lr_poly_outliers)))   \nprint('R2 %0.3f'% r2_score(y_outliers_test, prediction_lr_poly_outliers))\n\nprint(\"-------------------  Ridge Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_outliers_test, prediction_ridge_poly_outliers))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_outliers_test, prediction_ridge_poly_outliers)))   \nprint('R2 %0.3f'% r2_score(y_outliers_test, prediction_ridge_poly_outliers))\n\nprint(\"-------------------  Lasso Regression  -------------------\")\nprint('MAE: %0.3f'% mean_absolute_error(y_outliers_test, prediction_lasso_poly_outliers))\nprint('RMSE: %0.3f'% np.sqrt(mean_squared_error(y_outliers_test, prediction_lasso_poly_outliers)))   \nprint('R2 %0.3f'% r2_score(y_outliers_test, prediction_lasso_poly_outliers))","e731289b":"#split data\nX = df.drop(columns=['price'])\ny = df['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n\n#ohe and scale\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"neighbourhood_group\", \"neighbourhood\", \"room_type\"]),\n    (StandardScaler(),numeric.columns),\n    remainder = \"passthrough\")\n\n#define model\nlinreg = LinearRegression()\n\n#pipeline \/ model\npipe_linreg = make_pipeline(column_transform, linreg)\n\n#5 fold cross validation of linreg model\n#cross_val_score(pipe_linreg, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\").mean()\n\n#RMSR score\n# print(\"RMSE: {:.2f}\".format(sqrt(abs(cross_val_score(pipe_linreg, X_train, y_train, cv=5, \n#                                                  scoring=\"neg_mean_squared_error\").mean()))))","9d67dc4b":"f, axes = plt.subplots(3, 2, figsize=(20, 16))\n#mylist = [[0,0], [0,1], [1,0], [1,1], [2,0], [2,1]] axes locations\nmylist1 = [0, 0, 1, 1, 2, 2]\nmylist2 = [0, 1, 0, 1, 0, 1]\nfor i, j in enumerate(numeric.columns): #numeric columns without \"Rating\"\n    sns.regplot(x=df[j], y=df[\"price\"], #i: axes locations, j: column names\n            ax=axes[mylist1[i],mylist2[i]],\n            #lowess=True, #line of best fit\n            fit_reg=False,\n            scatter_kws={\"alpha\":0.5, \"color\":\"steelblue\"}, #scatter points alpha\n            line_kws={\"color\":\"black\", \"lw\":2}) #line width & color","7a8273b9":"def actual_predicted(X_train,y_train,model,X_test,y_test):\n    \n    #model predictions\n    model.fit(X_train,y_train)           #trained on\n    predictions = model.predict(X_test)  #applied to\n    \n    #plot\n    ax = sns.regplot(x=y_test, y=predictions, \n            scatter_kws={\"alpha\":0.65, \"color\":\"steelblue\"},\n            lowess=True,\n            line_kws={\"color\":\"black\", \"lw\":2})\n    ax.set(xlabel='Actual', ylabel='Predicted values')\n    plt.show()","f4a37dde":"plt.figure(figsize=(9,6))\nactual_predicted(X_train,y_train,pipe_linreg,X_test,y_test)","65afeb07":"def residual_dist_plot(X_train,y_train,model,X_test,y_test):\n    \n    #model predictions\n    model.fit(X_train,y_train)           #trained on\n    predictions = model.predict(X_test)  #applied to\n    \n    #residuals\n    residuals = y_test - predictions\n\n    #plot\n    plt.figure(figsize=(10,10))\n    sns.distplot(residuals, fit=norm, color=\"steelblue\")\n    plt.title(\"Residuals Distribution Plot\",size=15, weight='bold')\n\n    plt.show()","a5fd6a1a":"residual_dist_plot(X_train,y_train,pipe_linreg,X_test,y_test)","b5f22a92":"def residual_qq_plot(X_train,y_train,model,X_test,y_test):\n    \n    #model predictions\n    model.fit(X_train,y_train)           #trained on\n    predictions = model.predict(X_test)  #applied to\n    \n    #residuals\n    residuals = y_test - predictions\n\n    #plot\n    plt.figure(figsize=(7,7))\n    stats.probplot(residuals, plot=plt)\n\n    plt.show()","bb2ec83d":"residual_qq_plot(X_train,y_train,pipe_linreg,X_test,y_test)","75637587":"def residual_plot(X_train,y_train,model,X_test,y_test):\n    \n    #model predictions\n    model.fit(X_train,y_train)           #trained on\n    predictions = model.predict(X_test)  #applied to\n    \n    #residuals\n    residuals = y_test - predictions\n    \n    #residual plot\n    ax = sns.regplot(x=predictions, y=residuals,\n                    scatter_kws={\"alpha\":0.5, \"color\":\"steelblue\"},\n                    lowess=True,\n                    line_kws={\"color\":\"black\", \"lw\":2})\n    ax.set(xlabel=\"Predicted values\", ylabel=\"Residuals\")\n    plt.show()","9026b41a":"plt.figure(figsize=(9,6))\nresidual_plot(X_train, y_train, pipe_linreg, X_test, y_test)","dbc444fe":"plt.figure(figsize=(12,12))\npalette = sns.diverging_palette(220, 20, n=70)\ncorr=df.corr(method='pearson')\nmask = np.triu(np.ones_like(corr, dtype=bool))\nwith sns.axes_style(\"whitegrid\"): #temporary whitegrid style\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=palette, vmax=1, center=0,\n                mask=mask, square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n    plt.title(\"Correlation Matrix\",size=15, weight='bold')\n\nplt.show()","6a3121d5":"multicollinearity, V=np.linalg.eig(corr)\nmulticollinearity","0c9d4be0":"### Phase 1","36ef30d6":"### Model Building (Phase 3: without outliers, no polynomial terms)","c3f349d5":"## Categorical variables","c74f60ca":"### Model Building (Phase 1: with outliers, no polynomial terms)","b479c9dd":"## 5.1. Linearity","1d2b37c0":"In this notebook I will be exploring the NYC Airbnb data set in order to find a good prediction model for the outcome variable \"price\". After some data cleaning and exploratory data analysis I will evaluate the model performances of 3 different models (linear regression, ridge regression, lasso regression) with different input variables (phase 1-4) and check the model assumptions of linear regression. \n\n\n- Phase 1: With outliers, no polynomial terms\n- Phase 2: With outliers, with polynomial terms\n- Phase 3: No outliers, no polynomial terms\n- Phase 4: No outliers, no polynomial terms\n\n\n**The specific steps are the following:**\n\n**1. Data Cleaning**\n\n\n**2. Exploratory Data Analysis**\n* Categorical variables\n* Numerical variables\n* Outcome variable\n    \n    \n**3. Model Building (Phase 1-4)**\n* Data splitting\n* Preprocessing\n* Model building\n    \n    \n**4. Model Comparison (Phase 1-4)**\n* Predictions\n* Model comparison\n    \n    \n**5. Check Model Assumptions for Linear Regression**\n* Linearity\n* Normality\n* Homoscedasticity\n* Multicollinearity","72d36db5":"Most of the listing are in Manhatten or in Brooklyn, probably due to the amount of tourists visiting and wanting to stay in these areas. Queens, Bronx and Staten Island have significantly fewer listings as these are more residential neighbourhoods.","dd9cb3bd":"## Linear Regression Model","0fc61f5a":"### neighbourhood_group","f180cac6":"## 5.4. Multicollinearity","a11fcfe1":"### Model Building (Phase 4: without outliers, with polynomial terms)","ca5ab315":"### Actual vs predicted","12c48fd6":"# 4. Model Comparison","c5342332":"### Eigen values","317a82c1":"### Phase 4","2c5b1fb7":"### Phase 3","c0d6d788":"## 4.2. Model Comparison","5ceba2b5":"### Define Models & Parameters","62292071":"I will apply one hot encoding to all categorical variables and scale all numerical variables. I will also prepare a polynomial transformation for the numerical variables for phase 2 and 4.","38a0aab1":"### Residual plot","977f64e5":"In this step I am creating a function that will perform a grid search and cross validation in order to find the best parameters for linear, ridge and lasso regression in the respective phases 1-4. The function will be applied to the training sets. The best parameters will then be used for the test set in a later section.","93f7c420":"### Model Building (Phase 2: with outliers, with polynomial terms)","4f664124":"# 3. Model Building","dce9a6c2":"The variable \"id\" will be dropped as it doesn't provide useful information to our prediction models. Considering the host in the prediction will cause our models to be too large and time consuming, thus, I will drop the variables \"host_id\" and \"host_name\". However, in the real world these variables could have an influence on the price. Lastly, I will drop the \"name\" and \"last_reviews\" variable.","39544569":"## 4.1. Model Predictions","3e04f873":"## 3.3. Model Building","740ea789":"### room_type","65c3b5d3":"The outcome variable \"price\" is highly skewed with the price reaching as high as 10000. I will perform a log+1 transformation for a more normal distribution and for interpretability.","176a63e4":"## 5.2. Normality","d23e5bc5":"## 3.1. Data Splitting","22301013":"### neighbourhood","b851b9c8":"## 3.2. Preprocessing","1a7fba83":"## 5.3. Homoscedasticity","64844d07":"### Distributions & Outliers","0590e41e":"The distributions of the numerical variables are highly skewed due to extreme outliers. This can lead to lower prediction performances. However, it is dangerous to blindly remove these outliers as they could provide valueable information. In a later section I will run regression models with and without the outliers to see the impact on the prediction score (phase 3 and 4).","5610e61b":"# 5. Check Model Assumptions for Linear Regression","bc043229":"### Scatter plots (independent variables vs dependent variable)","1d3d565c":"# 2. Exploratory Data Analysis","a7ee52d8":"* **MAE:** Takes mean of the absolute difference the predicted and actual values.\n* **RMSE:** Takes the square root of the mean of squared errors.\n* **R2:** Represents the proportion of the variance in the dependent variable that is predictable from the independent variables. \n\n\nAs we can see from the results, all of our models have similar scores and have significant prediction errors.\nWe get the best score with linear regression in phase 4. This is no surprise as we removed extreme outliers and performed polynomial transformation in this phase. However, as mentioned earlier, it is dangerous to remove outliers that are not typos. Furthermore, it is no surprise that Ridge and Lasso didn't improve the scores as these models are mainly used to tackle multicollinearity and overfitting. In this case, by comparing the training and test scores, we don't seem to have a problem with overfitting. Instead, more investigation into the extreme outliers and more feature engineering are needed to improve our models performances. Another approach is to consider different models that are more robust to outliers.","106de13b":"### Residual dist plot","57c2cf71":"In this section I am going to compare the scores of our regression models in the phases 1-4. I will use the test sets for prediction and evaluation.","f237d52e":"There are many na values for the variable \"reviews_per_months\". For these listings, there are no reviews to begin with, so I will fill the missing values with 0.","41adc31a":"Most of the independent variables don't have a linear relationship with the outcome variable. In addition to that there are many extreme outliers in the data. Furthermore, the residuals from the linear regression are not normally distributed enough and show signs of heteroscedasticity. These violations have a great impact on the prediction of the model. Lastly, ridge and lasso regression will not show significant improvements as there is no big problem with multicollinearity. More feature engineering or different models are needed.","405fc27f":"### Phase 2","4b17ea89":"# 1. Data Cleaning ","a2957153":"## Numerical variables","63332a20":"Exploring the variable \"neighbourhood\" can give us some insight on how residential vs. how touristic the neighbourhoods are as Airbnb is mostly used by tourists.","7eaf400b":"### Function for Model Building","ac799cdb":"## Target variable","bcd8aaf5":"The room type \"shared room\" is highly underrepresented, most Airbnb hosts offer either a private room or an entire apartment for rent."}}