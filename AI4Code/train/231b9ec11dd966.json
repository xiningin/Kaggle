{"cell_type":{"1f2fbe11":"code","10980822":"code","0bfc8af7":"code","26a80a8a":"code","03f06be3":"code","4980d407":"code","95e5ec97":"code","78f880d3":"code","3dd42fba":"code","3f258009":"code","935f2e09":"code","79099b30":"code","66928395":"code","0c8b14b4":"code","22fef104":"markdown","d7a3c19c":"markdown","b1fbbe07":"markdown","b7261cee":"markdown","4422b2ba":"markdown"},"source":{"1f2fbe11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","10980822":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn","0bfc8af7":"df = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv', index_col = 'sl_no')\ndf.head(10)","26a80a8a":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows = 2, ncols = 2, figsize = (12, 12), sharex = True)\n\nax = [ax1, ax2, ax3, ax4]\n\nvar = ['ssc_p', 'hsc_p', 'degree_p', 'mba_p']\n\n\nfor i, axes in zip(var, ax):\n    \n    sn.scatterplot(x = df.index, y = df[i], ax = axes)\n    \nplt.show()","03f06be3":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows = 2, ncols = 2, figsize = (12, 12), sharex = True)\n\nax = [ax1, ax2, ax3, ax4]\n\nvar = ['ssc_p', 'hsc_p', 'degree_p', 'mba_p']\n\n\nfor i, axes in zip(var, ax):\n    \n    sn.set(style=\"whitegrid\")\n    sn.boxplot(df[i], ax = axes)\n    \nplt.show()","4980d407":"# Correcting Outilers in hsc_p variable using IQR Score\n\nQ1 = df['hsc_p'].quantile(0.25)\nQ3 = df['hsc_p'].quantile(0.75)\nIQR = Q3 - Q1\nprint('IQR Score :',IQR)\n\ndf['hsc_p'] = df['hsc_p'][~((df['hsc_p'] < (Q1 - 1.5 * IQR)) |(df['hsc_p'] > (Q3 + 1.5 * IQR)))]\n\nsn.boxplot(df['hsc_p'])\nplt.title('After Correcting Outliers')\nplt.show()","95e5ec97":"import missingno as miss\n\nmiss.matrix(df)","78f880d3":"df['hsc_p'] = df['hsc_p'].fillna(np.mean(df['hsc_p']))","3dd42fba":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows = 2, ncols = 2, figsize = (12, 12), sharex = True, sharey = True)\n\nax = [ax1, ax2, ax3, ax4]\n\nvar = ['ssc_p', 'hsc_p', 'degree_p', 'mba_p']\n\nfor i, axes in zip(var, ax):\n    \n    sn.distplot(df[i][df.status == 'Placed'], ax = axes, label = 'Placed')\n    sn.distplot(df[i][df.status == 'Not Placed'], ax = axes, label = 'Not Placed')\n    axes.legend()\nplt.show()","3f258009":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows = 2, ncols = 2, figsize = (12, 12), sharey = True)\n\nax = [ax1, ax2, ax3, ax4]\n\nvar = ['ssc_b', 'hsc_b', 'degree_t', 'specialisation']\n\nfor i, axes in zip(var, ax):\n    \n    sn.countplot(df[i], hue = df['status'], ax = axes)\n    axes.legend()\nplt.show()","935f2e09":"#Independent Variables\nX = df[['ssc_p', 'hsc_p', 'degree_p', 'mba_p', 'specialisation']]\n\n#Dependent Variables\ny = df[['status']]\n\n#Assigning 1 to Mkt&HR Specialisation and 0 to Mkt&Fin Specialisation\nX = X.replace({'Mkt&HR' : 1, 'Mkt&Fin' : 0})\n\n#Assigning 1 to Placed Students and 0 to Not Placed Students\ny = y.replace({'Placed' : 1, 'Not Placed' : 0})","79099b30":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, np.array(y).ravel(), random_state = 0)","66928395":"from sklearn.linear_model import LogisticRegression\n\nLinear = LogisticRegression(C = 15, solver = 'liblinear').fit(X_train, y_train)\n\npred = Linear.predict(X_test)\n\n\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\n\nprint('Score : ', Linear.score(X_test, y_test))\nprint('\\nf1 Score : ', f1_score(y_test, pred))\nprint('\\nPrecision Score : ', precision_score(y_test, pred))\nprint('\\nRecall Score : ', recall_score(y_test, pred))","0c8b14b4":"status = pd.DataFrame(y_test).rename({0 : 'Actual Status'}, axis = 1)\n\nstatus['Predicted Status'] = pred\nstatus","22fef104":"# Placement Prediction Using Logistic Regression","d7a3c19c":"# **Finding Outliers in Variables**","b1fbbe07":"# Visualizations","b7261cee":"* There are lots of missing values in salary, but for not we are not focusing on those values.\n* We are only focusing on missing values of hsc_p and filling them with mean of hsc_p","4422b2ba":"# Finding Null Values"}}