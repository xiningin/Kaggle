{"cell_type":{"b37e5c86":"code","8d25712a":"code","4a7a68f3":"code","1a873f3b":"code","32ab3d77":"markdown"},"source":{"b37e5c86":"\nimport cupy as cp\nimport cudf\nimport cuml\nimport glob\nimport xgboost as xgb\nfrom tqdm import tqdm\n\nPATH = \"\/kaggle\/input\/optiver-realized-volatility-prediction\"\n\n\ndef load_data(mode, path=\"\/kaggle\/input\/optiver-realized-volatility-prediction\"):\n    # mode = \"train\"\/\"test\"\n    file_name = f'{path}\/{mode}.csv'\n    return cudf.read_csv(file_name)\n\ndev_df = load_data(\"train\", path=PATH)\ndev_df.head()\nSCALE = 100\ndev_df[\"target\"] *= SCALE\n\nstock_ids = dev_df[\"stock_id\"].unique()\nlen(stock_ids)\norder_book_training = glob.glob(f'{PATH}\/book_train.parquet\/*\/*')\norder_book_test = glob.glob(f'{PATH}\/book_test.parquet\/*\/*')\n\nlen(order_book_training), len(order_book_test)\ntrades_training = glob.glob(f'{PATH}\/trade_train.parquet\/*\/*')\ntrades_test = glob.glob(f'{PATH}\/trade_test.parquet\/*\/*')\n\nlen(trades_training), len(trades_test)\n%cd \/kaggle\/input\/rapids-kaggle-utils\/\n\nimport cu_utils.transform as cutran\n\n\n\ndef log_diff(df, in_col, null_val):\n    df[\"logx\"] = df[in_col].log()\n    df[\"logx_shifted\"] = (df[[\"time_id\", \"logx\"]].groupby(\"time_id\")\n                             .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                            incols={\"logx\": 'x'},\n                                            outcols=dict(y_out=cp.float32),\n                                            tpb=32)[\"y_out\"])\n    df[\"keep_row\"] = df[f\"logx_shifted\"] != null_val\n    return df[\"logx\"] - df[\"logx_shifted\"]\n\n\n\ndef extract_raw_book_features(df, null_val=-9999):\n    for n in range(1, 3):\n        p1 = df[f\"bid_price{n}\"]\n        p2 = df[f\"ask_price{n}\"]\n        s1 = df[f\"bid_size{n}\"]\n        s2 = df[f\"ask_size{n}\"]\n        df[f\"wap{n}\"] = (p1*s2 + p2*s1) \/ (s1 + s2)\n        df[f\"log_return{n}\"] = log_diff(df, in_col=f\"wap{n}\", null_val=null_val)\n        df[f\"realized_vol{n}\"] = df[f\"log_return{n}\"]**2\n        \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df[\"c\"] = 1\n    \n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef extract_raw_trade_features(df, null_val=-9999):\n    df[\"realized_vol_trade\"] = log_diff(df, in_col=f\"price\", null_val=null_val)**2\n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef agg(df, feature_dict):\n    agg_df = df.groupby(\"time_id\").agg(feature_dict).reset_index()\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    agg_df.columns = [f(x) for x in agg_df.columns]\n    return agg_df    \n\n\ndef extract_book_stats(df):\n    default_stats = [\"sum\", \"mean\", \"std\"]\n    feature_dict = {\n        'wap1': default_stats,\n        'wap2': default_stats,\n        'log_return1': default_stats,\n        'log_return2': default_stats,\n        'wap_balance': default_stats,\n        'price_spread': default_stats,\n        'bid_spread': default_stats,\n        'ask_spread': default_stats,\n        'total_volume': default_stats,\n        'volume_imbalance': default_stats,\n        'c': [\"sum\"],\n        'realized_vol1': [\"sum\"],\n        'realized_vol2': [\"sum\"],\n    }\n    \n    return agg(df, feature_dict)\n    \n\n    \n    \ndef extract_trade_stats(df):\n    feature_dict = {\n        'realized_vol_trade': [\"sum\"],\n        'seconds_in_bucket':[\"count\"],\n        'size': [\"sum\"],\n        'order_count': [\"mean\"],\n    }\n    \n    return agg(df, feature_dict)\n\n\ndef time_constraint_fe(df, stats_df, last_sec, fe_function, cols):\n    sub_df = df[df[\"seconds_in_bucket\"] >= (600 - last_sec)].reset_index(drop=True)\n    if sub_df.shape[0] > 0:\n        sub_stats = fe_function(sub_df)\n    else:\n        sub_stats = cudf.DataFrame(columns=cols)\n    return stats_df.merge(sub_stats, on=\"time_id\", how=\"left\", suffixes=('', f'_{last_sec}'))    \n    \n\ndef feature_engineering(book_path, trade_path):\n    book_df = cudf.read_parquet(book_path)\n    book_df = extract_raw_book_features(book_df)\n    book_stats = extract_book_stats(book_df)\n    book_cols = book_stats.columns\n    \n    trade_df = cudf.read_parquet(trade_path)\n    trade_df = extract_raw_trade_features(trade_df)\n    trade_stats = extract_trade_stats(trade_df)\n    trade_cols = trade_stats.columns\n    \n    for last_sec in [150, 300, 450]:\n        book_stats = time_constraint_fe(book_df, book_stats, last_sec, extract_book_stats, book_cols) \n        trade_stats = time_constraint_fe(trade_df, trade_stats, last_sec, extract_trade_stats, trade_cols) \n\n    return book_stats.merge(trade_stats, on=\"time_id\", how=\"left\")\n\n\ndef process_data(order_book_paths, trade_paths, stock_ids):\n    stock_dfs = []\n    for book_path, trade_path in tqdm(list(zip(order_book_paths, trade_paths))):\n        stock_id = int(book_path.split(\"=\")[1].split(\"\/\")[0])\n\n        df = feature_engineering(book_path, trade_path)\n        df[\"stock_id\"] = stock_id\n        stock_dfs.append(df)\n    return cudf.concat(stock_dfs)\n\npast_volatility = process_data(order_book_training, trades_training, stock_ids)\npast_test_volatility = process_data(order_book_test, trades_test, stock_ids)\n\npast_volatility.shape, past_test_volatility.shape\n\ndef stock_time_fe(df):\n    cols = ['realized_vol1_sum', 'realized_vol2_sum', 'realized_vol_trade_sum',\n            'realized_vol1_sum_150', 'realized_vol2_sum_150', 'realized_vol_trade_sum_150',\n            'realized_vol1_sum_300', 'realized_vol2_sum_300', 'realized_vol_trade_sum_300',\n            'realized_vol1_sum_450', 'realized_vol2_sum_450', 'realized_vol_trade_sum_450']\n    \n    for agg_col in [\"stock_id\", \"time_id\"]:\n        for agg_func in [\"mean\", \"max\", \"std\", \"min\", \"var\"]:\n            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n    \n    return df\n\npast_volatility[\"is_test\"] = False\npast_test_volatility[\"is_test\"] = True\nall_df = past_volatility.append(past_test_volatility).reset_index(drop=True)\n\nall_df = stock_time_fe(all_df)\n\npast_volatility = all_df[~all_df[\"is_test\"]]\npast_test_volatility = all_df[all_df[\"is_test\"]]\n\ndev_df = dev_df.merge(past_volatility, on=[\"stock_id\", \"time_id\"], how=\"left\")\n\nfeatures = [col for col in list(dev_df.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"is_test\"}]\nlen(features)\n\n\n\ndef rmspe(y_true, y_pred):\n    return (cp.sqrt(cp.mean(cp.square((y_true - y_pred) \/ y_true))))\n\n\ndef rmspe_xgb(pred, dtrain):\n    y = dtrain.get_label()\n    return 'rmspe', rmspe(cp.array(y), cp.array(pred))\n\n\nNUM_FOLDS = 5\nparam = {'objective': 'reg:squarederror',\n         'learning_rate': 0.1,\n         'max_depth': 3,\n         \"min_child_weight\": 200,\n         \"reg_alpha\": 10.0,\n         \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n         'disable_default_eval_metric': 1\n    }\n\ntarget = \"target\"\n\n\n","8d25712a":"%cd \/kaggle\/working","4a7a68f3":"from keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, Reshape, Flatten\nfrom keras.layers import Bidirectional, LSTM, Conv1D,GlobalAveragePooling1D,GlobalMaxPooling1D,Dropout,MaxPooling1D, GRU\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nfrom keras.models import load_model\nimport numpy as np\nimport tensorflow as tf\nimport gc\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n\ndef lstm_model(train,validation,test,model_num,nfolds=5) :\n    gc.collect()\n    lstm_preds = cp.zeros(test.shape[0])\n    X_train, y_train = train[0], train[1]\n    X_test, y_test = validation[0], validation[1]\n    \n    gc.collect()\n    X_train = cp.reshape(X_train,(346174, 1, 268))\n    X_train = cp.asnumpy(cp.array(X_train))\n    gc.collect()\n    X_test  = cp.reshape(X_test,(82758, 1, 268))\n    X_test = cp.asnumpy(cp.array(X_test))\n    gc.collect()\n    y_train = cp.reshape(y_train,(-1, 1))\n    y_train = cp.asnumpy(cp.array(y_train))\n    gc.collect()\n    y_test = cp.reshape(y_test,(-1, 1))\n    y_test = cp.asnumpy(cp.array(y_test))\n    gc.collect()\n#     test = cp.asnumpy(cp.array(test))\n#     test = np.reshape(test,(1,1,268))#.reshape(-1,1,768)\n    # Initialising the RNN\n    gc.collect()\n    regressor = Sequential()\n\n    # Adding the input layerand the LSTM layer\n    regressor.add(Bidirectional(GRU(units=150, activation='relu',return_sequences=True)))\n        \n    regressor.add(Bidirectional(LSTM(units=30, activation='relu')))\n        \n    regressor.add(Dense(units=1, kernel_initializer='normal'))\n\n    # Compiling the RNN\n    regressor.compile(loss = root_mean_squared_error)\n    mc = ModelCheckpoint(r'.\/best_model'+str(model_num)+'.h5',monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n    # Fitting the RNN to the Training set\n    regressor.fit(X_train, y_train, batch_size=50, epochs = 2, verbose = 1,validation_data=(X_test,y_test),callbacks=[mc])\n    regressor = load_model('.\/best_model'+str(model_num)+'.h5', custom_objects={'root_mean_squared_error':root_mean_squared_error})\n    lstm_preds = regressor.predict(test)\/5\n    return lstm_preds","1a873f3b":"import gc\noof_preds = cp.zeros(dev_df.shape[0])\ntest_preds = cp.zeros(past_test_volatility.shape[0])\nseed = 42\ntest_pred = []\n\ngc.collect()\nfor fold in range(NUM_FOLDS):\n    print(\"Fold\", fold)\n    gc.collect()\n\n    train_ind = cp.where(dev_df[\"time_id\"].values % NUM_FOLDS != fold)[0]\n    val_ind = cp.where(dev_df[\"time_id\"].values % NUM_FOLDS == fold)[0]\n    gc.collect()\n    train_df, val_df = dev_df.iloc[train_ind], dev_df.iloc[val_ind]\n    train_df = train_df.fillna(0.0)\n    val_df = val_df.fillna(0.0)\n#     print(past_test_volatility[features].shape)\n    past_test_volatility = past_test_volatility.fillna(0.0)\n    test_result = lstm_model([train_df[features].values, train_df[target].values], [val_df[features].values, val_df[target].values], \n                           past_test_volatility[features], fold)\n    test_preds += test_result\n    \n# dev_df[\"pred\"] = oof_preds\n# print(f'The RMSPE score of XGB is {rmspe(dev_df[\"target\"], dev_df[\"pred\"])}')\npast_test_volatility[\"row_id\"] = past_test_volatility[\"stock_id\"].astype(str) + \"-\" + past_test_volatility[\"time_id\"].astype(str) \npast_test_volatility[\"target\"] = test_preds.clip(0.0, 100.0)\/SCALE\n%cd \/kaggle\/working\nsub_df = load_data(\"test\", path=PATH).merge(past_test_volatility[[\"row_id\", \"target\"]], \n                                            on=\"row_id\", how=\"left\").fillna(0.0)\n\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"target\"])","32ab3d77":"Training Process <br>\nKeras model: Bi-GRU + Bi-LSTM\n"}}