{"cell_type":{"97bdb24c":"code","cae7c424":"code","acfea51e":"code","ed927243":"code","a173f564":"code","d89166aa":"code","022595bf":"code","9718cf1e":"code","42f4bddf":"code","b322b498":"code","90fd729d":"code","5f82fe04":"code","7aad1939":"code","9bf26fb3":"code","830bf747":"code","ba9e3112":"code","9f22f720":"code","2caf0f9c":"code","e9d6f5d1":"code","31fb626e":"code","6d069567":"code","e0f5d530":"code","0971f315":"code","3b21b63f":"code","89487d8f":"code","edb75941":"code","fea34d5a":"code","5b584230":"code","5eec5347":"code","7128a32d":"code","b608fa58":"code","6b6f113d":"code","264ff1a8":"code","558eca8b":"code","6a4a435a":"code","fb4f826a":"code","cc0d423a":"code","b341ea4b":"code","4dad5707":"code","f5355781":"code","bafd0410":"code","8296cd2a":"code","2724f755":"code","a96e61e7":"code","14f84fdb":"code","53b06f1b":"code","a4d9e78a":"code","a84be2ae":"code","66190151":"code","ea80d0ec":"code","4d80c618":"code","5f8c2284":"code","d1d82eae":"code","27ee5585":"code","ea6f6eb9":"code","287e70e3":"code","5dde6105":"code","1cf33c01":"code","25ac4b20":"code","ade4ec2d":"code","423e1dd6":"code","7fde9bb3":"code","3c1e06e1":"code","96273377":"code","b684c0ab":"code","48ab5ec5":"code","d261c89a":"code","a1077bf8":"code","21d8fc4e":"code","82749f60":"code","c2d9db82":"code","b81485a1":"code","89459c46":"code","09256516":"code","cbd8e7da":"code","e66417aa":"code","e94bf46a":"code","23c635fb":"code","2c783da0":"code","c448dca6":"code","4293358d":"code","054511f3":"code","8b4cba5d":"code","4cb411d9":"code","bc3f1a2b":"code","92c31ff3":"code","786febe2":"code","13ca9985":"code","1baffbb3":"code","c412ca59":"code","09e94022":"code","ba58ef92":"code","b342872b":"code","5976c1e0":"code","c7b888e1":"code","c891bee7":"code","7a18d12f":"code","ae209ba9":"code","81609626":"code","07bdd5b1":"code","064cc799":"code","b8c1d3c9":"code","2c94fd18":"code","b799c99b":"code","d83cf7dc":"code","11ee4e85":"code","9a3a0e23":"code","4945f1d3":"code","0aaf1967":"code","4fca1210":"markdown","38dce034":"markdown","38265f98":"markdown","06195e95":"markdown","51721112":"markdown","8265154d":"markdown","2736c538":"markdown","57a82a98":"markdown","716aefe3":"markdown","efa159b5":"markdown","6a3632b6":"markdown","6ae39f44":"markdown","83912590":"markdown","5bb9b074":"markdown","bd9270b5":"markdown","801a17a7":"markdown","70d68e4a":"markdown","572c1ab4":"markdown","c3d0421e":"markdown","3dc36e1f":"markdown","0df7ae25":"markdown","9272ca0a":"markdown","6bb27300":"markdown","0607778f":"markdown","9fb0fa3e":"markdown","ac0bef69":"markdown","6c058d3e":"markdown","0d1d495a":"markdown","65a08b88":"markdown","7fece576":"markdown","7f0fc233":"markdown","1f7db2af":"markdown","ad131392":"markdown","91192db8":"markdown","4fb14df5":"markdown","e43205c8":"markdown","f77ca87c":"markdown","ec58b283":"markdown","287f4007":"markdown","f7990f0c":"markdown","c8944bdf":"markdown","f71f8d78":"markdown","fadbff90":"markdown","6a602caa":"markdown","940d07c3":"markdown","6eaba356":"markdown","6da87f4f":"markdown","bc62c238":"markdown","61617d09":"markdown","84672f4b":"markdown","9f62f77f":"markdown","ac646a30":"markdown","096644d1":"markdown","d4de84d8":"markdown","b3c0c460":"markdown","7772dbbd":"markdown","e9cb91b9":"markdown","f58b554a":"markdown","1683df61":"markdown","37cde96f":"markdown","e28a764d":"markdown","823bf9e4":"markdown","2b59ba2f":"markdown","12447917":"markdown","dfed0157":"markdown","d43a8f2d":"markdown","be3a7f65":"markdown","ee80db67":"markdown","b9b3dfba":"markdown","9bcb3bf5":"markdown","b61f9034":"markdown","8627bc50":"markdown","7dc8fd35":"markdown","babc6f37":"markdown","a7ab5e65":"markdown","5d4b2061":"markdown","fffbc913":"markdown","ce883aed":"markdown","da1d4ea2":"markdown","aeb89ce9":"markdown","2d878925":"markdown","a09d8fcd":"markdown","6220b47c":"markdown","4fc877d8":"markdown","9067e929":"markdown","f319ab17":"markdown","f6c091a1":"markdown","ce5824ea":"markdown","5c766afb":"markdown","3a9478d9":"markdown","a85466d9":"markdown","1e17141d":"markdown","cb5ce7cc":"markdown","2b57ad23":"markdown"},"source":{"97bdb24c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid', context='notebook', palette='deep')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport operator\nsns.set_context(\"talk\", font_scale = 1, rc={\"grid.linewidth\": 3})\npd.set_option('display.max_rows', 100, 'display.max_columns', 100)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve,precision_score,recall_score,confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score,KFold,StratifiedKFold,StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nimport tensorflow","cae7c424":"train= pd.read_csv('..\/input\/titanic\/train.csv') #Trainig data set \ntest= pd.read_csv('..\/input\/titanic\/test.csv') #Testing data set\ngender = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","acfea51e":"train.head()","ed927243":"test.head()","a173f564":"print(len(train))\nprint(len(test))","d89166aa":"print('Train Data Info')\nprint(train.info())\nprint('\\n')\nprint('Test Data Info')\nprint(test.info())","022595bf":"train.describe()","9718cf1e":"test.describe()","42f4bddf":"pd.DataFrame([train.isnull().sum(),train.isnull().sum()\/len(train)*100]).T.\\\nrename(columns={0:'Total',1:'Missing Perc'})","b322b498":"pd.DataFrame([test.isnull().sum(),test.isnull().sum()\/len(test)*100]).T.\\\nrename(columns={0:'Total',1:'Missing Perc'})","90fd729d":"train[train.Embarked.isnull()]","5f82fe04":"df = train[train['Fare']<train['Fare'].std()*3]\nplt.figure(figsize=(10,10))\nsns.boxplot(x=df['Embarked'], y=df['Fare'], data=df,hue=df['Pclass'])\n#plt.yticks(range(0,550,50))\nplt.show()","7aad1939":"train['Embarked'].fillna('C',inplace=True)","9bf26fb3":"survivers = train.Survived\ntrain.drop([\"Survived\"],axis=1, inplace=True)\nmaster=pd.concat([train,test])\nmaster.Cabin.fillna(\"N\", inplace=True)\nmaster['Cabin'] = master['Cabin'].apply(lambda x:list(str(x))[0].upper())\nmaster.head()","830bf747":"master.groupby('Cabin')['Fare'].describe()","ba9e3112":"def repl_N(val):\n    n = 0\n    if val==35:\n        n = 'T'\n    elif val<=14:\n        n = 'G'\n    elif 14<val<=26:\n        n='F'\n    elif 26<val<=39:\n        n='A'\n    elif 39<val<=53:\n        n='E'\n    elif 53<val<=80:\n        n='D'\n    elif 80<val<=115:\n        n='C'\n    else:\n        n='B'\n    return n\n","9f22f720":"\nmaster_N = master[master['Cabin']=='N']\nmaster_notN = master[~(master['Cabin']=='N')]\nmaster_N['Cabin']=master_N['Fare'].apply(lambda x:repl_N(x))\nmaster = pd.concat([master_N,master_notN])\nfare_mean= master[(master['Pclass']==3) & (master['Embarked']=='S')]['Fare'].mean()\nmaster['Fare'].fillna(fare_mean,inplace=True)","2caf0f9c":"missing_value = test[(test.Pclass == 3) & \n                     (test.Embarked == \"S\") & \n                     (test.Sex == \"male\")].Fare.mean()\n## replace the test.fare null values with test.fare mean\ntest.Fare.fillna(missing_value, inplace=True)","e9d6f5d1":"train = master.sort_values('PassengerId')[:891]\ntest= master.sort_values('PassengerId')[891:]\ntrain['Survived'] = survivers","31fb626e":"pd.DataFrame([master.isnull().sum(),master.isnull().sum()\/len(master)*100]).T.\\\nrename(columns={0:'Total',1:'Missing Perc'})","6d069567":"train['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1","e0f5d530":"passenger_test= test['PassengerId']\ntrain.drop('PassengerId',axis=1,inplace=True)\ntest.drop('PassengerId',axis=1,inplace=True)","0971f315":"def showvalues(ax,m=None):\n    for p in ax.patches:\n        ax.annotate(\"%.1f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\\\n                    ha='center', va='center', fontsize=14, color='k', rotation=0, xytext=(0, 7),\\\n                    textcoords='offset points',fontweight='light',alpha=0.9) ","3b21b63f":"plot_df= train.groupby('Sex')['Survived'].value_counts(normalize=True).mul(100).rename('percent').reset_index()\nplot_df","89487d8f":"plt.figure(figsize=(10,8))\ncol = {1:'#99ff99', 0:'#ff9999'}\nax= sns.barplot(x='Sex',y='percent',data=plot_df,hue='Survived',palette=col)\nshowvalues(ax)\nplt.title('Percentage of Passenger Survived Sex wise', pad=30)\nplt.xlabel('Sex')\nplt.ylabel('Percentage of Passenger Survived')\nleg = ax.get_legend().texts\nleg[0].set_text(\"No\")\nleg[1].set_text(\"Yes\")\nplt.show()","edb75941":"plot_df= train.groupby('Pclass')['Survived'].value_counts(normalize=True).mul(100).rename('percent').reset_index()\nplot_df","fea34d5a":"plt.figure(figsize=(10,8))\ncol = {1:'#99ff99', 0:'#ff9999'}\nax= sns.barplot(x='Pclass',y='percent',data=plot_df,hue='Survived',palette=col)\nshowvalues(ax)\nplt.title(\"Percentage of Passenger Survived vs PClass\", pad=30)\nplt.xlabel(\"Passenger Class\");\nplt.ylabel(\"Percentage of Passenger Survived\")\nleg = ax.get_legend().texts\nleg[0].set_text(\"No\")\nleg[1].set_text(\"Yes\")\nplt.show()","5b584230":"col = {0:'#99ff99', 1:'#ff9999'}\nplt.figure(figsize=(10,8))\nax=sns.boxplot(x='Sex',data=train,y='Age',hue='Survived',palette=col)\nleg = ax.get_legend().texts\nleg[0].set_text(\"No\")\nleg[1].set_text(\"Yes\")\nplt.show()","5eec5347":"plt.figure(figsize=(20,12))\nplt.subplot(1,2,1)\nax=sns.distplot(train['Fare'])\nplt.subplot(1,2,2)\nax=sns.boxplot(x='Pclass',data=train,y='Fare',hue='Sex',palette='cool')\nax.set_yscale('log')\nplt.show()\n","7128a32d":"plt.figure(figsize=(20,20))\nplt.subplot(2,1,1)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'] , color='r',shade=True,label='Deceased')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'] , color='g',shade=True, label='Survived')\nplt.xlabel('Fare')\nplt.ylabel('Frequency of Passenger Survived')\nplt.subplot(2,1,2)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='r',shade=True,label='Deceased')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='Survived')\nplt.xlabel('Age')\nplt.ylabel('Frequency of Passenger Survived')\nplt.show()","b608fa58":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.Pclass[train.Survived == 0] , \n               color='red',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived', \n              )\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Passenger Class\", fontsize = 15,labelpad =20)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(train.Pclass.unique()), labels);","6b6f113d":"\nax = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Embarked\", margin_titles=True, hue = \"Survived\",palette = col)\nax = ax.map(plt.hist, \"Age\", edgecolor = 'white').add_legend()\nax.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)\n\n","264ff1a8":"\nsns.factorplot(x='Parch',y='Survived',data=train,col='Sex',color='g',ci=95.0)","558eca8b":"\nsns.catplot(x='Parch',y='Survived',data=train,col='Sex',color='g')","6a4a435a":"\nsns.factorplot(x='SibSp',y='Survived',data=train,col='Sex',ci=95.0,color='g')","fb4f826a":"\nsns.factorplot(x='family_size',y='Survived',data=train,col='Sex',ci=95.0,color='g')","cc0d423a":"plot_df= train.groupby('Cabin')['Survived'].value_counts(normalize=True).mul(100).rename('percent').reset_index()","b341ea4b":"plt.figure(figsize=(16,8))\ncol = {1:'#99ff99', 0:'#ff9999'}\nax= sns.barplot(x='Cabin',y='percent',data=plot_df,hue='Survived',palette=col)\nshowvalues(ax)\nplt.title(\"Percentage of Passenger Survived vs Cabin\", pad=30)\nplt.xlabel(\"Cabin\");\nplt.ylabel(\"Percentage of Passenger Survived\")\nleg = ax.get_legend().texts\nleg[0].set_text(\"No\")\nleg[1].set_text(\"Yes\")\nplt.show()","4dad5707":"\nsns.factorplot(x='Embarked',y='Survived',data=train,col='Sex',ci=95.0,color='g')","f5355781":"\nsns.factorplot(x='Cabin',y='Survived',data=train,col='Embarked',hue='Sex',ci=95.0,size=6)","bafd0410":"plt.figure(figsize=(20,10))\nax=sns.boxplot(x='Cabin',data=train,y='Fare',hue='Sex',palette='cool')\nax.set_yscale('log')\nplt.show()\n","8296cd2a":"plot_df= train[train['Age']<10]['Survived'].value_counts(normalize=True).mul(100).rename('percent').reset_index()","2724f755":"plt.figure(figsize=(10,8))\nax=sns.barplot(x=plot_df['index'],y=plot_df['percent'],palette=col)\nlocs, labels = plt.xticks()\nplt.xticks(ticks=locs,labels=['Not Survived','Survived'])\nplt.ylabel('Percentage of Children Survived in Age < 10')\nplt.xlabel('Survived or Not Survived')\nshowvalues(ax)","a96e61e7":"train['Survived'].value_counts(normalize=True)*100","14f84fdb":"train[(train['Embarked']=='Q') & (train['Sex']=='male')]['Survived'].value_counts()","53b06f1b":"train[(train['Embarked']=='Q') & (train['Sex']=='male')]['Survived'].value_counts(normalize=True)*100","a4d9e78a":"train['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)","a84be2ae":"plot_df","66190151":"plot_df=train[['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked',\n       'family_size']]\nplt.figure(figsize=(15,10))\nmask = np.zeros_like(plot_df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(plot_df.corr(),annot=True,cmap='cividis',mask=mask)","ea80d0ec":"np.info([len(i) for i in train.Name])","4d80c618":"train['name_len'] = [len(i) for i in train.Name]\ntest['name_len'] = [len(i) for i in test.Name]\ndef name_length(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=50):\n        a = 'long'\n    else:\n        a = 'very long'\n    return a\ntrain['name_len_rnge'] = train['name_len'].map(name_length)\ntest['name_len_rnge'] = test['name_len'].map(name_length)","5f8c2284":"plt.figure(figsize=(10,8))\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'name_len'] , color='r',shade=True,label='Deceased')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'name_len'] , color='g',shade=True, label='Survived')\nplt.xlabel('Name Length')\nplt.ylabel('Frequency of Passenger Survived')\nplt.show()","d1d82eae":"sns.distplot([len(i) for i in train.Name])","27ee5585":"train['title']=train['Name'].apply(lambda x:x.split('.')[0].split(',')[1].strip())\ntest['title']=test['Name'].apply(lambda x:x.split('.')[0].split(',')[1].strip())","ea6f6eb9":"## we are writing a function that can help us modify title column\ndef replace_title(df):\n    \n    result=[]\n    for val in df:\n        if val in ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col','Dona']:\n            val = 'rare'\n            result.append(val)\n        elif val in ['Ms', 'Mlle']:\n            val = 'Miss'\n            result.append(val)\n        elif val == 'Mme':\n            val = 'Mrs'\n            result.append(val)\n        else:\n            result.append(val)\n    return result\n\ntrain['title']=replace_title(train['title'])\ntest['title']=replace_title(test['title'])","287e70e3":"train['title'].value_counts()","5dde6105":"print(train['title'].unique())\nprint(test['title'].unique())","1cf33c01":"## bin the family size. \ndef family_group(size):\n    \"\"\"\n    This funciton groups(loner, small, large) family based on family size\n    \"\"\"\n    \n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a","25ac4b20":"\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)","ade4ec2d":"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]","423e1dd6":"train['actual_fare']=train['Fare']\/train.family_size\ntest['actual_fare'] = test.Fare\/test.family_size","7fde9bb3":"train['Fare'].describe()","3c1e06e1":"def fare_rnge(fare):\n    val= ''\n    if fare <= 4:\n        val = 'very_low'\n    elif fare <= 10:\n        val = 'low'\n    elif fare <= 20:\n        val = 'mid'\n    elif fare <= 45:\n        val = 'high'\n    else:\n        val = 'very_high'\n    return val\n\ntrain['fare_rnge'] = train['actual_fare'].map(fare_rnge)\ntest['fare_rnge'] = test['actual_fare'].map(fare_rnge)","96273377":"## create bins for age\n#def age_group_fun(age):\n#    \"\"\"\n#    This function creates a bin for age\n#    \"\"\"\n#    a = ''\n#    if age <= 1:\n#        a = 'infant'\n#    elif age <= 4: \n#        a = 'toddler'\n#    elif age <= 13:\n#        a = 'child'\n#    elif age <= 18:\n#        a = 'teenager'\n#    elif age <= 35:\n#        a = 'Young_Adult'\n#    elif age <= 45:\n#        a = 'adult'\n#    elif age <= 55:\n#        a = 'middle_aged'\n#    elif age <= 65:\n#        a = 'senior_citizen'\n#    else:\n#        a = 'old'\n#    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\n#train['age_group'] = train['Age'].map(age_group_fun)\n#test['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \n#train = pd.get_dummies(train,columns=['age_group'], drop_first=True)\n#test = pd.get_dummies(test,columns=['age_group'], drop_first=True);","b684c0ab":"train = pd.get_dummies(train, columns=['Pclass', 'Cabin', 'Embarked', 'name_len_rnge', 'title',\\\n                                       'fare_rnge','family_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['Pclass', 'Cabin', 'Embarked', 'name_len_rnge',\\\n                                     'title','fare_rnge','family_group'], drop_first=False)\n","48ab5ec5":"train.drop(['family_size','name_len',\\\n            'Fare','Name','Ticket'], axis=1, inplace=True)\ntest.drop(['family_size','name_len',\\\n            'Fare','Name','Ticket'], axis=1, inplace=True)","d261c89a":"def predict_age(df):\n    df_not_null = df.loc[df['Age'].notnull()]\n    df_null= df[df['Age'].isnull()]\n    y=df_not_null['Age']\n    x=df_not_null.drop('Age',axis=1)\n    rf_reg=RandomForestRegressor(n_estimators=1000).fit(x,y)\n    pred=rf_reg.predict(df_null.drop('Age',axis=1))\n    df.loc[df.Age.isnull(), \"Age\"] =list(pred)\n    return df\npredict_age(train)\npredict_age(test)\n    ","a1077bf8":"X = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]","21d8fc4e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","82749f60":"scaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","c2d9db82":"from sklearn.linear_model import LogisticRegression\nlr= LogisticRegression(solver='liblinear',penalty= 'l1',random_state = 0)\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\ny_prob = lr.predict_proba(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(precision_score(y_test, y_pred))\nprint(recall_score(y_test, y_pred)) ","b81485a1":"from sklearn.metrics import roc_auc_score,auc,roc_curve\nfpr, tpr, _ =roc_curve(y_test,y_prob[:,1])\nroc_auc= auc(fpr,tpr)\nplt.figure(figsize=(10,8))\nplt.plot(fpr,tpr,label='ROC Curve(area = %0.2f)'%roc_auc)\nplt.plot([0,1],[0,1],'k--',c='r')\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC Curve', fontsize= 18)\nplt.show()","89459c46":"precision,recall,thre=precision_recall_curve(y_test,y_prob[:,1])\nprec_recall= auc(recall,precision)\nplt.figure(figsize=(10,8))\nplt.plot(recall,precision,label='Precision recall Curve(area = %0.2f)'%prec_recall)\nplt.xlabel('recall', fontsize = 18)\nplt.ylabel('precision', fontsize = 18)\nplt.title('Precision Recall', fontsize= 18)\nplt.show()","09256516":"cv=StratifiedShuffleSplit(n_splits=20,test_size=0.3,random_state=0)\ncross_v_score= cross_val_score(LogisticRegression(),X,y,cv=cv)\nprint(cross_v_score)\nprint('mean cross validation score:{0:2.2f}'.format(np.mean(cross_v_score)))","cbd8e7da":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\n\nc = list(np.linspace(0.01,10,19))\npenalties = ['l1','l2']\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .3)\nparam = {'penalty': penalties, 'C': c}\nlogreg = LogisticRegression(solver='liblinear')\ngrid = RandomizedSearchCV(estimator=LogisticRegression(), \n                           param_distributions = param,\n                           scoring = 'accuracy',\n                           cv = cv,n_iter=40\n                          )\n## Fitting the model\ngrid.fit(X, y)","e66417aa":"print(grid.best_estimator_)\nprint(grid.best_params_)\nprint(grid.best_score_)\nprint(grid.best_index_)","e94bf46a":"lr=grid.best_estimator_\nlr.score(X,y)","23c635fb":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\nknn.fit(X_train,y_train)\ny_predict=knn.predict(X_test)\naccuracy_score(y_test,y_predict)","2c783da0":"from sklearn.naive_bayes import MultinomialNB\ng_nb=MultinomialNB()\ng_nb.fit(X,y)\ny_pred= g_nb.predict(X_test)\nprint(round(accuracy_score(y_test,y_pred),3))\n","c448dca6":"from sklearn.svm import SVC\nsvm_n=SVC(C=3,kernel='poly',degree=3)\nsvm_n.fit(X_train,y_train)\ny_pred= svm_n.predict(X_test)\naccuracy_score(y_test,y_pred)","4293358d":"from sklearn.tree import DecisionTreeClassifier\nmax_depth_n = range(1,10)\nmax_feature_n = [20,21,22,23,24,25,26,28,29,30,'auto']\ncriterion_n = [\"gini\", \"entropy\"]\nparams={'max_depth':max_depth_n,'max_features':max_feature_n,'criterion':criterion_n}\ncv_n=StratifiedShuffleSplit(test_size=0.25,random_state=0)\nrandom_cv= RandomizedSearchCV(DecisionTreeClassifier(),param_distributions=params,cv=cv_n)\nrandom_cv.fit(X,y)","054511f3":"print(random_cv.best_estimator_)\nprint(random_cv.best_index_)\nprint(random_cv.best_params_)\nprint(random_cv.best_score_)\n","8b4cba5d":"dtc=random_cv.best_estimator_\ndtc.score(X,y)\n","4cb411d9":"columns= X.columns\nfeature_importances = pd.DataFrame(dtc.feature_importances_,\n                                   index = columns,\n                                    columns=['Feature Importance'])\nfeature_importances.sort_values(by='Feature Importance', ascending=False).head(10)","bc3f1a2b":"df_temp= feature_importances.sort_values(by='Feature Importance', ascending=False).head(10)\nplt.figure(figsize=(8,6))\nsns.barplot(data=df_temp,y=df_temp.index, x='Feature Importance',orient='h')\n#bar.set_xticklabels(bar.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","92c31ff3":"from sklearn.ensemble import RandomForestClassifier\nn_estimators_n=[145,150]\nmax_depth_n=range(1,10)\ncriterion_n = [\"gini\", \"entropy\"]\nparams={'max_depth':max_depth_n,'criterion':criterion_n,'n_estimators':n_estimators_n}\ncv_n=StratifiedShuffleSplit(test_size=0.25,random_state=0)\ngrid_cv= GridSearchCV(RandomForestClassifier(),param_grid=params)\ngrid_cv.fit(X,y)","786febe2":"print(grid_cv.best_estimator_)\nprint(grid_cv.best_params_)\nprint(grid_cv.best_score_)\n","13ca9985":"rfc=grid_cv.best_estimator_\nrfc.score(X,y)","1baffbb3":"columns= X.columns\nfeature_importances = pd.DataFrame(rfc.feature_importances_,\n                                   index = columns,\n                                    columns=['Feature Importance'])\nfeature_importances.sort_values(by='Feature Importance', ascending=False).head(10)\ndf_temp= feature_importances.sort_values(by='Feature Importance', ascending=False).head(10)\nplt.figure(figsize=(8,6))\nsns.barplot(data=df_temp,y=df_temp.index, x='Feature Importance',orient='h')\n#bar.set_xticklabels(bar.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","c412ca59":"from sklearn.ensemble import BaggingClassifier\nn_estimators_n = [10,20,30,50,70,80,100,120, 140,150]\ncv_n=StratifiedShuffleSplit(test_size=0.25,random_state=0)\nparams={'n_estimators':n_estimators_n}\ngrid_cv= GridSearchCV(BaggingClassifier(),param_grid=params,cv=cv_n)\ngrid_cv.fit(X,y)","09e94022":"print(grid_cv.best_estimator_)\nprint(grid_cv.best_params_)\nprint(grid_cv.best_score_)","ba58ef92":"bc_n=grid_cv.best_estimator_\nbc_n.score(X,y)","b342872b":"bc=BaggingClassifier(n_estimators=30,max_features=17)\nbc.fit(X_train,y_train)\ny_pred=bc.predict(X_test)\naccuracy_score(y_test,y_pred)","5976c1e0":"from sklearn.ensemble import GradientBoostingClassifier\nmax_depth_n=range(2,10)\nparams={'max_depth':max_depth_n}\ncv_n=StratifiedShuffleSplit(test_size=0.25,random_state=0)\ngrid_cv= GridSearchCV(GradientBoostingClassifier(),param_grid=params)\ngrid_cv.fit(X,y)","c7b888e1":"print(grid_cv.best_estimator_)\nprint(grid_cv.best_params_)\nprint(grid_cv.best_score_)","c891bee7":"gbc=grid_cv.best_estimator_\ngbc.score(X,y)","7a18d12f":"from xgboost import XGBClassifier\nxgbc=XGBClassifier()\nxgbc.fit(X_train,y_train)\ny_pred=xgbc.predict(X_test)\nprint(round(accuracy_score(y_test,y_pred),2))","ae209ba9":"from sklearn.ensemble import AdaBoostClassifier\nn_estimators_n = [50,70,80,100]\ncv_n=StratifiedShuffleSplit(test_size=0.25,random_state=0)\nparams = {'n_estimators':n_estimators_n}\ngrid_cv= GridSearchCV(AdaBoostClassifier(),param_grid=params,cv=cv_n)\ngrid_cv.fit(X,y)","81609626":"print(grid_cv.best_estimator_)\nprint(grid_cv.best_score_)\nprint(grid_cv.best_params_)","07bdd5b1":"abc_n=grid_cv.best_estimator_\nabc_n.score(X,y)","064cc799":"abc=AdaBoostClassifier(algorithm='SAMME.R',learning_rate=1.007)\nabc.fit(X_train,y_train)\ny_pred=abc.predict(X_test)\naccuracy_score(y_test,y_pred)","b8c1d3c9":"from sklearn.ensemble import ExtraTreesClassifier\netc=ExtraTreesClassifier()\netc.fit(X_train,y_train)\ny_pred=etc.predict(X_test)\nprint(round(accuracy_score(y_test,y_pred),2))","2c94fd18":"from sklearn.gaussian_process import GaussianProcessClassifier\ngpc=GaussianProcessClassifier()\ngpc.fit(X_train,y_train)\ny_pred=gpc.predict(X_test)\nprint(round(accuracy_score(y_test,y_pred),2))","b799c99b":"from sklearn.ensemble import VotingClassifier\nvc= VotingClassifier(estimators=[lr,rfc,gbc,knn,bc,abc,etc,gpc,g_nb])\n\nvc = VotingClassifier(estimators=[\n    ('lr_grid', lr),\n    ('random_forest', rfc),\n    ('gradient_boosting', gbc),\n    ('decision_tree_grid',dtc),\n    ('knn_classifier', knn),\n    ('XGB_Classifier', xgbc),\n    ('bagging_classifier', bc),\n    ('adaBoost_classifier',abc),\n    ('ExtraTrees_Classifier', etc),\n    ('gaussian_process_classifier', gpc)\n],voting='hard')\nvc.fit(X_train,y_train)\ny_pred=vc.predict(X_test)\nprint(round(accuracy_score(y_test,y_pred),2))","d83cf7dc":"from tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout\n#Early Stop\nearly_stop = EarlyStopping(monitor = 'val_loss', mode = \"min\", verbose = 1 , patience = 25)\n\nann= Sequential()\n\nann.add(Dense(9,activation = 'relu'))\nann.add(Dropout(0.5))\nann.add(Dense(4,activation = 'relu'))\nann.add(Dropout(0.5))\n\nann.add(Dense(1,activation='sigmoid'))\nann.compile(loss= 'binary_crossentropy', optimizer = 'adam')\n\nann.fit(x=X_train, y=y_train,epochs=400,validation_data=(X_test,y_test),callbacks=[early_stop])","11ee4e85":"y_pred = (ann.predict(X_test) > 0.46).astype(int)","9a3a0e23":"accuracy_score(y_test,y_pred)","4945f1d3":"models = [lr,knn,svm_n,dtc,rfc,gbc,bc,abc,etc,gpc,xgbc,vc]\nc = {}\nfor model in models:\n    pred = model.predict(X_test)\n    result = accuracy_score(y_test,pred)\n    c[model] = result\n    \n","0aaf1967":"test['Cabin_T']=0\ntest_prediction = (max(c, key=c.get)).predict(test.values)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passenger_test,\n        \"Survived\": test_prediction\n    })\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic_submission.csv\", index=False)","4fca1210":"#### Lets see if childrens or you infants were the priority and if it is what is the impact on young childrens\/infants","38dce034":"### Exploring category of different feature variables\n- <b>PassengerId<\/b>\n - <i>Index\n- <b>Survived: Passenger Survived(0) or not(1)<\/b>\n - <i>Numerical variable\n- <b>Pclass: lower class(1), middle class(2), upper class(3)<\/b>\n - <i>Ordinal variable \n- <b>Name: Passenger Name<\/b>\n - <i>Text variable or String\n- <b>Sex: Male or Female<\/b>\n - <i>Nominal variable\n- <b>Age: Passenger Age<\/b>\n - <i>Numerical continous variable\n- <b>SibSp: No of Siblings and Spouse travelling with passanger<\/b>\n - <i>Numerical discrete variable\n- <b>Parch: No of Siblings and Spouse travelling with passanger<\/b>\n - <i>Numerical discrete variable\n- <b>Ticket: Ticket Number<\/b>\n - <i>Text variable or String\n- <b>Fare<\/b>\n - <i>Numerical continous variable\n- <b>Cabin: Cabin number<\/b>\n - <i>Text\/String variable\n- <b>Embarked: Tells about the embarkation code or embarked method<\/b>\n - <i>Text\/String variable\n","38265f98":"#### Factorplot for Parents\/Children survived for male and female","06195e95":"#### Fare Range","51721112":"#### _<font color=darkgreen>Inference: for long and very long Name length, Survived percentage is much larger than deceased, which is a great insight from length feature._","8265154d":"### Fixing Embarked missing values","2736c538":"#### Merging dataframes","57a82a98":"#### Survived passenger vs Cabin","716aefe3":"## XGB Classifier","efa159b5":"#### Creating more columns and converting catrgorical columns into dummy variable. So, we can use it in ML model.","6a3632b6":" #### We know for Embarked=NaN, Pclass =1, Fare=80,Cabin =B2B. Removing outliers for fare column to better visualise the embarked feature for values greater than 3 standard deviation. We know for missing embarked values, respective Pclass is 1 and Fare is 80. ","6ae39f44":"#### <font color=darkgreen>PClass has highest fare range and it is somewhat biased, Female passanger in Upper class paid more than male passenger, similarly for Lower class. For middle class, Female employess also paid higher but variation is not that much as for Upper and lower class.","83912590":"## Feature Engineering","5bb9b074":"#### Factor Plot for Embarked vs Survived","bd9270b5":"#### Factorplot for Spouse\/Siblings survived for male and female","801a17a7":"#### Age Distribution of Passengers","70d68e4a":"#### Function to replace NaN or 'N' value with Above mentioned values.","572c1ab4":"#### Filling Nan\/Null value in Fare column which is PassengerId=1044. PassengerId=1044 travelling in Pclass:3 and Embarked:S.","c3d0421e":"## Feature Importance ","3dc36e1f":"#### <font color=darkgreen>Inference: Only Embarked C satisfies the condition which is Fare=80 and Pclass=1. Hence, MIssing values for Embarked most likely be equal to C. We could have taken help from Cabin feature to be more sure of Embarked misssing value, but Cabin column contains around 78% missing values.","0df7ae25":"train.drop(['family_size','Friends', 'family_size_inc_frnds','name_len',\\\n            'Fare','Ticket_num','Ticket','Name'], axis=1, inplace=True)\ntest.drop(['family_size','Friends', 'family_size_inc_frnds','name_len',\\\n            'Fare','Ticket_num','Ticket','Name'], axis=1, inplace=True)","9272ca0a":"#### Fetching title from Name","6bb27300":"#### Actual_fare, passenger who are with family have paid total fare not the individual fare as seen from the fare column. We will derive individual fare columns because fare is very important parameter in prediction of passengger survived or not.","0607778f":"#### Lets split our master dataframe into train and test dataset again, we know training set has 891 rows(passangerId 0-890), and test data has 418 rows (passangerId 892-end) and our master df has 1309 rows.","9fb0fa3e":"#### _<font color=darkgreen>Cabins C, B, and D has high fare amount, also we saw previusly Cabin C,D and B has gretaer survived percentage than other cabins. It is because who paid higher amount for  fare are likely to be more priority to save than passenger who paid smaller amount._","ac0bef69":"#### Filling Missing values in Age column for master df","6c058d3e":"<img src=\"https:\/\/faithmag.com\/sites\/default\/files\/styles\/article_full\/public\/2018-09\/titanic2.jpg?h=6521bd5e&itok=H8td6QVv.jpg\"  Width=\"800\">","0d1d495a":"## % of Passenger survived  w.r.t Passenger Class(Lower(3),Middle(2),Upper(1)).","65a08b88":"#### Analysing mean, median, and max value we can make a fare range under which Cabin category falls.\n- val<=14,<b>Cabin:G<\/b>\n- val=35, <b>Cabin:T<\/b>\n- 14 < val <= 26, <b>Cabin:F<\/b>\n- 26 < val <= 39, <b>Cabin:A<\/b>\n- 39<val<=53, <b>Cabin:E<\/b>\n- 53 < val <= 80, <b>Cabin:D<\/b>\n- 80 < val <= 115, <b>Cabin:C<\/b>\n- ,>115, <b>Cabin:E<\/b>","7fece576":"#### Unique name titles","7f0fc233":"#### Creating family size and friends column to make final column which includes count of family members and friends. As we will proceed further we will see this feature is very important in our ml model prediction.","1f7db2af":"#### Factorplot for Spouse+Siblings+Parents+Childrens+Friend survived for male and female","ad131392":"### Importing train and test data set.","91192db8":"## Random Forest Classifier","4fb14df5":"## KNN","e43205c8":"## Decision tree Classifier","f77ca87c":"#### Fare and Age distribution vs Survived or not","ec58b283":"#### Fare distrbution for Pclass and Cabin","287f4007":"#### Lets replace Mlle(Mademoiselle) to Miss, Ms to Miss, Mme(Madame) to Mrs, and  Colonel,Don,jonkheer,the Countess,Major relaced with rank.","f7990f0c":"## Gradient Boosting Classifier","c8944bdf":"#### Drop PassengerId column, it role is same as index column only it starts from 1 and index starts from 0.","f71f8d78":"#### _<font color=darkgreen>Majority of passengers boarded from Southhampton and then from Cherbourg. Passengers who boarded from Queenstown are very less in number compared to other embarked\/port. Majority of the female had survived in which most no of females survived are from southhampton followed by Cherbourgh and Queenstown. Note: No male passenger from queenstown survived, as you can see from the graph._","fadbff90":"#### Male passenged survived percentage who embarked from Queenstown","6a602caa":"#### _<font color=darkgreen>None of the Passengers Embarked=Q are  in D,B,E,T cabins and D,B,E has high survived percentage. We noticed that very small fraction of male survived who boarded from Queenstown as seen earlier also. Females have greater survived percentage than males._","940d07c3":"### Data Modeling and Evaluation","6eaba356":"#### Splitting data into train and test data ","6da87f4f":"## Voting Classifier","bc62c238":"survivers = train.Survived\ntrain.drop([\"Survived\"],axis=1, inplace=True)\nmaster=pd.concat([train,test])\nmaster.Cabin.fillna(\"N\", inplace=True)\nmaster['Cabin'] = master['Cabin'].apply(lambda x:list(str(x))[0].upper())\nmaster.head()","61617d09":"### Filling embarked missing values with C.","84672f4b":"### Checking train and test dataset info","9f62f77f":"## Ada Boost Algorithm","ac646a30":"### Missing values percentage for training and testing data set","096644d1":"#### _<font color=darkgreen>63% of the people survived in Upper Class, ~47.3% passanger survived in Middle class whereas only ~24% passanger survived in Lower class.(Priority for upper class > middle class> lower class)_","d4de84d8":"#### _<font color=darkgreen>~74% of the female survived whereas ~81% Male Deceased. It's proving Titanic survival ratio are biased towards females(Female passengers were priority rather than male passengers)._","b3c0c460":"#### _<font color=darkgreen>For Age<10 (Childrens\/Infants) Survived percentage is 61% which shows that good percentage of children age<10 survived as children and females were first priority._","7772dbbd":"## Bagging Classifier","e9cb91b9":"#### Feature Scaling preprocessing step\n<ul>\n    <li><b>MinMaxScaler<\/b>-Scales the data using the max and min values so that it fits between 0 and 1.<\/li>\n    <li><b>StandardScaler<\/b>-Scales the data so that it has mean 0 and variance of 1.<\/li>\n    <li><b>RobustScaler<\/b>-Scales the data similary to Standard Scaler, but makes use of the median and scales using the interquertile range so as to aviod issues with large outliers.<\/b>\n <\/ul>","f58b554a":"#### Combining train and test data in master dataframe to predict missing values. Lets fetch the first character from cabin column to bettter understand the distribution of Cabin class. ","1683df61":"## Gaussian Process Classifier","37cde96f":"#### Missing values in master df, only Age columns has missing values which is ~20.1% or 263 missing values. Survived column has missing values because its not present in test dataset and is expected.","e28a764d":"#### _<font color=darkgreen>Inference: Age has ~20% null\/Nan values , Cabin has ~77% null values and Embarked feature has ~.2% or only 2 null values._","823bf9e4":"#### Lets visulaise Embarked , Sex with Survived percentage","2b59ba2f":"#### Dropping columns which are not useful after creating dummy variables","12447917":"#### _<font color=darkgreen>Passenger who travelled in big group with their parents and children had less survival rate than who travelled alone or with their parent or childred._\n####  _<font color=darkgreen>Femaled who were alone, high percentage of females survived._","dfed0157":"#### Titanic has different cabins based on their fare\/price and they categorized based on their initial character and then subsequent number. Lets check whether cabin category (i.e. C,E,A,D, etc) has fare range.","d43a8f2d":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:tomato' role=\"tab\" aria-controls=\"home\"><center>If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated :)<\/center><\/h2>\n","be3a7f65":"## Inferences from above visualisations:\n- <b><font color=darkgreen>~38% passenger survived.\n- ~74% female passenger survived, while only ~19% male passenger survived.\n- ~63% upper class passengers survived, 47% middle class passenger survived, while only 24% lower class passenger survived.\n- Passenger who is in Cabin B, 75% of the passenger in Cabin B survived, will find out reason behind it.\n- Most of the passenger embarked from Cherbourg and Southhampton, very few passenger embarked from Queenstown and more than 90% male passenger died who boarder from Queesntown.\n- ~61% of the children survived below Age 10, were on priority to save.\n- Small family size has more survived percentage compared to medium and big family size.\n","ee80db67":"## Support Vector Machine","b9b3dfba":"#### _<font color=darkgreen>Similar Inference as for parent\/Children Parch in above plot._","9bcb3bf5":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:tomato' role=\"tab\" aria-controls=\"home\"><center>Thank You :)<\/center><\/h2>\n","b61f9034":"#### _<font color='darkgreen'>Younger people survived percentage is more in males while it is opposite in females._","8627bc50":"#### Passengers who boarded from Cherbourg has highest percentage of passenger survived and for Queenstown males survived is very small.","7dc8fd35":"## Artificial Neural Network","babc6f37":"#### Correlation between different features, how they are correlates( strongly(positive) , neutral, or weakely(negative))","a7ab5e65":"### Importing libraries","5d4b2061":"#### Inference Age distrbution:\n- <b><font color=darkgreen>Children or young infants has more Survived percentage because chidrens and infants were the priority then same as for females which we have seen earlier.","fffbc913":"# Name","ce883aed":"## Function to show values on bar plot","da1d4ea2":"#### Family Size including friends range","aeb89ce9":"## % of Passenger survived or not w.r.t Sex(Male\/Female).","2d878925":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:tomato' role=\"tab\" aria-controls=\"home\"><center>Titanic EDA + End to End Machine Learning \ud83d\udef3<\/center><\/h2>\n","a09d8fcd":"#### Predicting missing value for Age columns","6220b47c":"#### Inference from Coorelation:\n- <b><font color=darkgreen>Fare and Survived are positively correlated(0.26)\n- Fare and Pclass are negatively correlated(-0.55)\n- Survived and Passenger Class are negatively correlated(-0.34)\n- Survived and Sex are negatively correlated(-0.54)\n- Passenger Class and Age are negatively correlated(-0.37)\n\n","4fc877d8":"## <font color=darkgreen>Approach:\n- <i><b>Data cleaning and statistical analysis.\n- Exploratory Data Analysis and visualisations.\n- Machine learning modelling and Prediction using ML model.\n- FInding the best Machine learning model based on various score","9067e929":"## Preprocessing Tasks","f319ab17":"#### <font color=darkgreen>Inference:Survived percentage in Cabin B is highest which is 74.5% followed by Cabin D. It's a interesting fact about cabin B. There is only 1 person in cabin T and he didn't survive.","f6c091a1":"### Retriving rows having 'N' values for Cabin replace it with above function values. ","ce5824ea":"train['family_size_inc_frnds_rng'] = train['family_size_inc_frnds'].map(family_group)\ntest['family_size_inc_frnds_rng'] = test['family_size_inc_frnds'].map(family_group)","5c766afb":"#### Factor Plot for Embarked vs Survived","3a9478d9":"## Naive Bayes - Baseline Model","a85466d9":"## *Challenge*\n`The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\nIn this challenge, We will build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).`","1e17141d":"#### _<font color=darkgreen>Inference: Age has ~21% null\/Nan values , Cabin has ~78% null values and fare has only 1 missing value._","cb5ce7cc":"#### Inference Fare distribution:\n- <b><font color=darkgreen>The spike in the plot under 50 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \n- <b><font color=darkgreen>When fare is approximately more than 200 dollars, there is very small red shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment.","2b57ad23":"## Extra Tree Classifier"}}