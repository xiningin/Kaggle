{"cell_type":{"6bb84d7a":"code","d7347ddd":"code","6aa1e3d5":"code","dd84a0d9":"code","23da2102":"code","d2ac8a15":"code","27b3fb49":"code","fcc93ce0":"code","2aeb0f11":"code","4a93adc8":"code","04585def":"code","25527e56":"code","7db64520":"code","61d8edb4":"code","fe8f23b7":"code","c3aeb65b":"code","3c39d5db":"code","134e4f4f":"code","2808350e":"code","d29abf00":"code","e860197f":"code","e61f6786":"code","4b972fb7":"code","f8940cf1":"code","23dc295b":"code","75664403":"code","470ff416":"code","388d44cd":"code","7e215186":"code","1f58127f":"code","cd7eb00b":"code","02e90746":"code","e8b09c20":"code","80c30e10":"code","3a332b75":"code","6717e166":"code","39b62caf":"code","2abd3fbf":"code","0ee93e7c":"code","944d331b":"code","4b160e27":"markdown","a8cd63ba":"markdown","83dd6c09":"markdown","9f60d8d8":"markdown","86c68cbe":"markdown","bacaf47f":"markdown","3a08c2e4":"markdown","4bcbaf0a":"markdown","fad62203":"markdown","ddf17da4":"markdown","3e9fdc77":"markdown","bffc9782":"markdown","111a5a94":"markdown","4b5d7c98":"markdown","46698e1f":"markdown","cafe5091":"markdown","8d9ed444":"markdown","bb020790":"markdown","47189ed4":"markdown","f02a05a9":"markdown"},"source":{"6bb84d7a":"!pip install tweepy","d7347ddd":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport os\nimport tweepy as tw #for accessing Twitter API\n\n\n#For Preprocessing\nimport re    # RegEx for removing non-letter characters\nimport nltk  #natural language processing\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\n\n# For Building the model\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport seaborn as sns\n\n#For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\npd.options.plotting.backend = \"plotly\"","6aa1e3d5":"# Load Tweet dataset\ndf1 = pd.read_csv('..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Twitter_Data.csv')\n# Output first five rows\ndf1.head()","dd84a0d9":"# Load Tweet dataset\ndf2 = pd.read_csv('..\/input\/appletwittersentimenttexts\/apple-twitter-sentiment-texts.csv')\ndf2 = df2.rename(columns={'text': 'clean_text', 'sentiment':'category'})\ndf2['category'] = df2['category'].map({-1: -1.0, 0: 0.0, 1:1.0})\n# Output first five rows\n\ndf2.head()","23da2102":"# Load Tweet dataset\ndf3 = pd.read_csv('..\/input\/twitterdata\/finalSentimentdata2.csv')\ndf3 = df3.rename(columns={'text': 'clean_text', 'sentiment':'category'})\ndf3['category'] = df3['category'].map({'sad': -1.0, 'anger': -1.0, 'fear': -1.0, 'joy':1.0})\ndf3 = df3.drop(['Unnamed: 0'], axis=1)\n# Output first five rows\ndf3.head()","d2ac8a15":"# Load Tweet dataset\ndf4 = pd.read_csv('..\/input\/twitter-airline-sentiment\/Tweets.csv')\ndf4 = df4.rename(columns={'text': 'clean_text', 'airline_sentiment':'category'})\ndf4['category'] = df4['category'].map({'negative': -1.0, 'neutral': 0.0, 'positive':1.0})\ndf4 = df4[['category','clean_text']]\n# Output first five rows\ndf4.head()","27b3fb49":"df = pd.concat([df1, df2, df3, df4], ignore_index=True)","fcc93ce0":"# Check for missing data\ndf.isnull().sum()","2aeb0f11":"# drop missing rows\ndf.dropna(axis=0, inplace=True)","4a93adc8":"# dimensionality of the data\ndf.shape","04585def":"# Map tweet categories\ndf['category'] = df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})\n\n# Output first five rows\ndf.head()","25527e56":"# The distribution of sentiments\ndf.groupby('category').count().plot(kind='bar')","7db64520":"# Calculate tweet lengths\ntweet_len = pd.Series([len(tweet.split()) for tweet in df['clean_text']])\n\n# The distribution of tweet text lengths\ntweet_len.plot(kind='box')","61d8edb4":"fig = plt.figure(figsize=(14,7))\ndf['length'] = df.clean_text.str.split().apply(len)\nax1 = fig.add_subplot(122)\nsns.histplot(df[df['category']=='Positive']['length'], ax=ax1,color='green')\ndescribe = df.length[df.category=='Positive'].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for positive sentiment tweets.', fontsize=16)\n\nplt.show()","fe8f23b7":"fig = plt.figure(figsize=(14,7))\ndf['length'] = df.clean_text.str.split().apply(len)\nax1 = fig.add_subplot(122)\nsns.histplot(df[df['category']=='Negative']['length'], ax=ax1,color='red')\ndescribe = df.length[df.category=='Negative'].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for Negative sentiment tweets.', fontsize=16)\n\nplt.show()","c3aeb65b":"import plotly.express as px\nfig = px.pie(df, names='category', title ='Pie chart of different sentiments of tweets')\nfig.show()","3c39d5db":"df.drop(['length'], axis=1, inplace=True)\n#df['clean_text'] = df['clean_text'].str.lower()\ndf.head","134e4f4f":"#### Visualizing data into wordclouds\n\n\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef wordcount_gen(df, category):\n    '''\n    Generating Word Cloud\n    inputs:\n       - df: tweets dataset\n       - category: Positive\/Negative\/Neutral\n    '''\n    # Combine all tweets\n    combined_tweets = \" \".join([tweet for tweet in df[df.category==category]['clean_text']])\n                          \n    # Initialize wordcloud object\n    wc = WordCloud(background_color='white', \n                   max_words=50, \n                   stopwords = STOPWORDS)\n\n    # Generate and plot wordcloud\n    plt.figure(figsize=(10,10))\n    plt.imshow(wc.generate(combined_tweets))\n    plt.title('{} Sentiment Words'.format(category), fontsize=20)\n    plt.axis('off')\n    plt.show()\n    \n# Positive tweet words\nwordcount_gen(df, 'Positive')\n     \n# Negative tweet words\nwordcount_gen(df, 'Negative')\n     \n# Neutral tweet words\nwordcount_gen(df, 'Neutral')\n     \n","2808350e":"def tweet_to_words(tweet):\n    ''' Convert tweet text into a sequence of words '''\n    \n    # convert to lowercase\n    text = tweet.lower()\n    # remove non letters\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n    # tokenize\n    words = text.split()\n    # remove stopwords\n    words = [w for w in words if w not in stopwords.words(\"english\")]\n    # apply stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    # return list\n    return words\n\nprint(\"\\nOriginal tweet ->\", df['clean_text'][0])\nprint(\"\\nProcessed tweet ->\", tweet_to_words(df['clean_text'][0]))","d29abf00":"# Apply data processing to each tweet\nX = list(map(tweet_to_words, df['clean_text']))","e860197f":"from sklearn.preprocessing import LabelEncoder\n\n# Encode target labels\nle = LabelEncoder()\nY = le.fit_transform(df['category'])","e61f6786":"print(X[0])\nprint(Y[0])","4b972fb7":"y = pd.get_dummies(df['category'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)","f8940cf1":"from sklearn.feature_extraction.text import CountVectorizer\n#from sklearn.feature_extraction.text import TfidfVectorizer\n\nvocabulary_size = 5000\n\n# Tweets have already been preprocessed hence dummy function will be passed in \n# to preprocessor & tokenizer step\ncount_vector = CountVectorizer(max_features=vocabulary_size,\n#                               ngram_range=(1,2),    # unigram and bigram\n                                preprocessor=lambda x: x,\n                               tokenizer=lambda x: x) \n#tfidf_vector = TfidfVectorizer(lowercase=True, stop_words='english')\n\n# Fit the training data\nX_train = count_vector.fit_transform(X_train).toarray()\n\n# Transform testing data\nX_test = count_vector.transform(X_test).toarray()\n\n#import sklearn.preprocessing as pr\n\n# Normalize BoW features in training and test set\n#X_train = pr.normalize(X_train, axis=1)\n#X_test  = pr.normalize(X_test, axis=1)\n\n# print first 200 words\/tokens\nprint(count_vector.get_feature_names()[0:200])\n\n","23dc295b":"# Plot the BoW feature vector\nplt.plot(X_train[2,:])\nplt.xlabel('Word')\nplt.ylabel('Count')\nplt.show()","75664403":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_words = 5000\nmax_len=50\n\ndef tokenize_pad_sequences(text):\n    '''\n    This function tokenize the input text into sequnences of intergers and then\n    pad each sequence to the same length\n    '''\n    # Text tokenization\n    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n    tokenizer.fit_on_texts(text)\n    # Transforms text to a sequence of integers\n    X = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    X = pad_sequences(X, padding='post', maxlen=max_len)\n    # return sequences\n    return X, tokenizer\n\nprint('Before Tokenization & Padding \\n', df['clean_text'][0])\nX, tokenizer = tokenize_pad_sequences(df['clean_text'])\nprint('After Tokenization & Padding \\n', X[0])\n\n\n","470ff416":"import pickle\n\n# saving\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# loading\nwith open('tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)","388d44cd":"y = pd.get_dummies(df['category'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\nprint('Train Set ->', X_train.shape, y_train.shape)\nprint('Validation Set ->', X_val.shape, y_val.shape)\nprint('Test Set ->', X_test.shape, y_test.shape)","7e215186":"import keras.backend as K\n\ndef f1_score(precision, recall):\n    ''' Function to calculate f1 score '''\n    \n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","1f58127f":"from keras.models import Sequential\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\nfrom keras.metrics import Precision, Recall\nfrom keras.optimizers import SGD\nfrom keras.optimizers import RMSprop\nfrom keras import datasets\n\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.callbacks import History\n\nfrom keras import losses\n\nvocab_size = 5000\nembedding_size = 32\nepochs=20\nlearning_rate = 0.1\ndecay_rate = learning_rate \/ epochs\nmomentum = 0.8\n\nsgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n# Build model\nmodel= Sequential()\nmodel.add(Embedding(vocab_size, embedding_size, input_length=max_len))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(3, activation='softmax'))","cd7eb00b":"import tensorflow as tf\ntf.keras.utils.plot_model(model, show_shapes=True)","02e90746":"print(model.summary())\n\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, \n               metrics=['accuracy', Precision(), Recall()])\n\n# Train model\n#num_epochs = 8\nbatch_size = 64\nhistory = model.fit(X_train, y_train,\n                      validation_data=(X_val, y_val),\n                      batch_size=batch_size, epochs=epochs, verbose=1)","e8b09c20":"# Evaluate model on the test set\nloss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n# Print metrics\nprint('')\nprint('Accuracy  : {:.4f}'.format(accuracy))\nprint('Precision : {:.4f}'.format(precision))\nprint('Recall    : {:.4f}'.format(recall))\nprint('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))","80c30e10":"def plot_training_hist(history):\n    '''Function to plot history for accuracy and loss'''\n    \n    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n    # first plot\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Model Accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('accuracy')\n    ax[0].legend(['train', 'validation'], loc='best')\n    # second plot\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[1].legend(['train', 'validation'], loc='best')\n    \nplot_training_hist(history)","3a332b75":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(model, X_test, y_test):\n    '''Function to plot confusion matrix for the passed model and the data'''\n    \n    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n    # use model to do the prediction\n    y_pred = model.predict(X_test)\n    # compute confusion matrix\n    cm = confusion_matrix(np.argmax(np.array(y_test),axis=1), np.argmax(y_pred, axis=1))\n    # plot confusion matrix\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n                xticklabels=sentiment_classes,\n                yticklabels=sentiment_classes)\n    plt.title('Confusion matrix', fontsize=16)\n    plt.xlabel('Actual label', fontsize=12)\n    plt.ylabel('Predicted label', fontsize=12)\n    \nplot_confusion_matrix(model, X_test, y_test)","6717e166":"# Save the model architecture & the weights\nmodel.save('best_model.h5')\nprint('Best model saved')","39b62caf":"from keras.models import load_model\n\n# Load model\nmodel = load_model('best_model.h5')\n\ndef predict_class(text):\n    '''Function to predict sentiment class of the passed text'''\n    \n    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n    max_len=50\n    \n    # Transforms text to a sequence of integers using a tokenizer object\n    xt = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n    # Do the prediction using the loaded model\n    yt = model.predict(xt).argmax(axis=1)\n    # Print the predicted sentiment\n    print('The predicted sentiment is', sentiment_classes[yt[0]])","2abd3fbf":"predict_class(['\"I hate when I have to call and wake people up'])","0ee93e7c":"predict_class(['The food was meh'])","944d331b":"predict_class(['He is a best minister india ever had seen'])","4b160e27":"## Exploratory Data Analysis","a8cd63ba":"### Saving tokenized data ","83dd6c09":"### Model save and load for the prediction","9f60d8d8":"## Data Preprocessing","86c68cbe":"## Bidirectional LSTM Using NN","bacaf47f":"### Train & Test Split","3a08c2e4":"## Installing and importing dependencies","4bcbaf0a":"### Cleaning and prepping dataset\n","fad62203":"Data Visualisation-","ddf17da4":"## Exploratory Data Analysis\n","3e9fdc77":"The `category` column has 3 values:\n1. 0 Indicating it is a Neutral Sentiment\n2. 1 Indicating a Postive Sentiment\n3. -1 Indicating a Negative Sentiment","bffc9782":"## Overview \n\nThis script performs EDA and then preprocesses the `twitter.csv` dataset to train a bidirectional LSTM model which is in turn used to predict the sentiments behind tweets fetched in real time using `tweepy` and classify them as positive negative or neutral.\n\n","111a5a94":"Plotting the distribution of tweet lengths","4b5d7c98":"Plotting the distribution of text length for positive sentiment tweets","46698e1f":"Plotting the Pie chart of the percentage of different sentiments of all the tweets","cafe5091":"Plotting the Distribution of text length for Negative sentiment tweets.","8d9ed444":"To fetch tweets from twitter, we need to install the tweepy library. We will be using this package to pull tweets on which our model will make predictions.","bb020790":"### Model Accuracy & Loss","47189ed4":"### Tokenizing & Padding","f02a05a9":"### Model Confusion Matrix"}}