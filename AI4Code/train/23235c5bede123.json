{"cell_type":{"60b6a0b8":"code","c1f9bbb4":"code","513091d6":"code","2333a54e":"code","7999c180":"code","648e1367":"code","5ce4ed34":"code","be02383a":"code","820453ce":"code","84249992":"code","c3debcf4":"code","c084341a":"code","5ccebb97":"markdown","34340295":"markdown","97ca7130":"markdown","677b949e":"markdown","55d05a73":"markdown","658848a1":"markdown","41e10932":"markdown","9b401946":"markdown","ef016a49":"markdown","8992e177":"markdown"},"source":{"60b6a0b8":"%cp -r ..\/input\/yolor-kaggle-hub\/yolor-lib .\n%cp -r ..\/input\/yolor-kaggle-hub\/cocoapi .","c1f9bbb4":"%cd \/kaggle\/working\/cocoapi\/PythonAPI\n\n!make\n!make install\n!python setup.py install","513091d6":"import pycocotools\n\n%cd \/kaggle\/working\/yolor-lib","2333a54e":"import torch\nimport cv2\n\nimport pandas as pd\nimport numpy as np\n\nfrom utils.torch_utils import select_device\nfrom PIL import Image\n\n# YoloR methods\nfrom models.models import Darknet\nfrom utils.datasets import letterbox\nfrom utils.general import non_max_suppression, scale_coords","7999c180":"%cd \/kaggle\/working\/","648e1367":"def show_prediction(img, bboxes, scores):\n\n    for box, score in zip(bboxes, scores):\n        cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0,0,255), 2)\n        cv2.putText(img, f'{score:.2f}', (int(box[0]), int(box[1]-3)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1, cv2.LINE_AA)\n    \n    img = img[:,:,::-1]\n    img = Image.fromarray(img).resize((1280, 720))\n    return img\n\ndef load_yolor_model(model_path, cfg_path, img_size, device):\n    \n    model = Darknet(cfg_path).cuda()\n    model.load_state_dict(torch.load(model_path, map_location=device)['model'])\n    model.to(device).eval()\n    \n    init = torch.zeros((1, 3, img_size, img_size), device=device)\n    _ = model(init) if device.type != 'cpu' else None \n    return model\n\ndef predict_img(model, img_in, img_size, conf_thres, iou_thres, device):\n    bboxes = []\n    scores = []\n    classes = []\n    \n    img = letterbox(img_in, new_shape=img_size, auto_size=64)[0]\n    img = np.ascontiguousarray(img[:, :, ::-1].transpose(2, 0, 1))\n    img = (torch.from_numpy(img).to(device).float() \/ 255.0).unsqueeze(0)\n    \n    pred = model(img)[0]\n    \n    if pred is not None:\n        pred = non_max_suppression(pred, conf_thres=conf_thres, iou_thres=iou_thres, merge=False, classes=None, agnostic=False)\n\n    for i, det in enumerate(pred):\n        if det is not None and len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img_in.shape).round()    \n    \n    det = det.cpu().detach().numpy()\n    \n    bboxes = det[:, :4] \n    scores = det[:, 4] \n    classes = det[:, 5]\n    \n    return bboxes, scores, classes","5ce4ed34":"DATASET_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/'\ncfg_path = '\/kaggle\/working\/yolor-lib\/cfg\/yolor_p6.cfg'\nmodel_path = '\/kaggle\/input\/yolor-kaggle-hub\/yr_002.pt'\n\nCONF = 0.01\nIOU = 0.4\nimg_size = 2432","be02383a":"device = select_device()\nmodel = load_yolor_model(model_path, cfg_path, img_size, device)","820453ce":"dir = f'{DATASET_PATH}'\nimgs = [dir + f for f in ('video_1\/4775.jpg', 'video_0\/9794.jpg', 'video_0\/4502.jpg', 'video_0\/9651.jpg', 'video_0\/9700.jpg',  'video_0\/9674.jpg','video_0\/20.jpg', 'video_0\/17.jpg',)]\n\nfor fileimg in imgs:\n    img = cv2.imread(fileimg)\n    bboxes, scores, classes = predict_img(model, img, img_size, conf_thres = CONF, iou_thres = IOU, device = device)\n    display(show_prediction(img, bboxes, scores))","84249992":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","c3debcf4":"for (image_np, sample_prediction_df) in iter_test:\n    \n    bboxes, scores, _ = predict_img(model, image_np[:,:,::-1], img_size, conf_thres = CONF, iou_thres = IOU, device = device)\n    \n    predictions = []\n    detects = []\n    \n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        score = scores[i]\n        x_min = int(box[0])\n        y_min = int(box[1])\n        x_max = int(box[2])\n        y_max = int(box[3])\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        detects.append([x_min, y_min, x_max, y_max, score])\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)","c084341a":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","5ccebb97":"## Used, ran, forked? Please vote. It is a GREAT gift for me and a sign of thanks.","34340295":"### A. MODEL PARAMETERS","97ca7130":"### Glad you got to the end of this notebook. Isn't it easy? Good luck! Remember to vote - it's important to me and it motivates me to share more ...","677b949e":"### B. LOAD MODEL","55d05a73":"## Inference YoloR on COTS dataset (PART 2) - as simple as possible to help people start with YoloR and develop object detection solutions on Kaggle\n\nThis notebook introduces YOLOR on Kaggle and TensorFlow - Help Protect the Great Barrier Reef competition. It shows how to make inference using custom YoloR object detection model (COTS dataset). It could be good starting point for build own custom model based on YoloR detector and make object detection. Full github repository you can find here - [YOLOR](https:\/\/github.com\/WongKinYiu\/yolor). First part - TRAINING is here: https:\/\/www.kaggle.com\/remekkinas\/yolor-p6-w6-one-more-yolo-on-kaggle-train.\n\nSteps covered in this INFERENCE notebook:\n\n* Install modules\n* Run YoloR inference on test images\n* Submit to competition\n\n<div class=\"alert alert-warning\">I found that there is no reference custom YoloR model training and inference notebook on Kaggle. Since we have such an opportunity this is my contribution to this competition. Feel free to use it and enjoy! I really appreciate if you upvote this notebook. Thank you!<\/div>\n\n<div class=\"alert alert-success\" role=\"alert\">\nI introduced YoloX in TensorFlow - Help Protect the Great Barrier Reef competition as well. You can find these notebooks here:      \n    <ul>\n        <li> <a href=\"https:\/\/www.kaggle.com\/remekkinas\/yolox-full-training-pipeline-for-cots-dataset\">YoloX full training pipeline for COTS dataset<\/a><\/li>\n        <li> <a href=\"https:\/\/www.kaggle.com\/remekkinas\/yolox-inference-on-kaggle-for-cots-lb-0-507\">YoloX detections submission made on COTS dataset<\/a><\/li>\n    <\/ul>\n    \n<\/div>","658848a1":"## 0. MODULES INSTALATION ","41e10932":"## 1. YoloR PREDICTION","9b401946":"## 2. SUBMISSION","ef016a49":"<div align=\"center\"><img width=\"640\" src=\"https:\/\/github.com\/WongKinYiu\/yolor\/raw\/main\/figure\/unifued_network.png\"\/><\/div>\n\n<div align=\"center\"><img width=\"640\" src=\"https:\/\/github.com\/WongKinYiu\/yolor\/raw\/main\/figure\/performance.png\"\/><\/div>","8992e177":"### C. TEST ON IMAGES"}}