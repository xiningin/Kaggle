{"cell_type":{"21c1cdf0":"code","93b49e95":"code","1399ceca":"code","cf90da73":"code","e8a79b2c":"code","fa6c1d09":"code","c3272b49":"code","7bf75e1b":"code","30f808a2":"code","6a1e8ba3":"code","5ee9b726":"code","bb47ff7b":"code","31f0e622":"code","f1d87c3b":"code","a0f90127":"code","fc512b84":"code","e7f2dde7":"code","e51f9406":"code","911b0d80":"code","2908e9c3":"code","699db5e0":"code","2e065225":"code","55aaf76b":"code","b4c6d4ad":"code","b5e0cfaf":"code","dc465144":"code","4806349c":"code","aeba4cf5":"code","bd3599bf":"code","a52d64de":"code","2a3c9f39":"code","d0094249":"markdown","4952c329":"markdown","76f557de":"markdown","f75d1932":"markdown","22247e62":"markdown","a88a4d33":"markdown","bb874ea0":"markdown","13cb5504":"markdown","c9a2d937":"markdown","ada7a827":"markdown","cf7a0e90":"markdown","228be26c":"markdown","8357f521":"markdown","640a6e4c":"markdown","986a5c56":"markdown","b71410bf":"markdown","77a3a5f8":"markdown","181e1418":"markdown","2b628d90":"markdown","36e2dddd":"markdown","53bbab66":"markdown","6bb49d0e":"markdown","d6ac56a7":"markdown","d6d97942":"markdown","1943a5af":"markdown","ce4985c4":"markdown","5fcaca4f":"markdown","72e55945":"markdown","7af08dfb":"markdown","6b709ffc":"markdown","2662009d":"markdown","c4e6b1eb":"markdown","eacf0cab":"markdown","a59b730e":"markdown"},"source":{"21c1cdf0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93b49e95":"# load in the 6 class csv\ndf = pd.read_csv('\/kaggle\/input\/star-dataset\/6 class csv.csv')","1399ceca":"# inspect the first few rows of the dataset\ndf.head()","cf90da73":"# provide a column-wise statistical description of the dataframe\ndf.describe()","e8a79b2c":"# obtain the shape of the dataframe (no. rows x no. columns)\nprint(df.shape)","fa6c1d09":"# import visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# set visualisation parameters\nsns.set(rc={'figure.figsize': (45.0, 20.0)})\nsns.set(font_scale=8.0)\nsns.set_context(\"notebook\", font_scale=5.5, rc={\"lines.linewidth\": 0.5})","c3272b49":"sns.set(rc={'axes.facecolor':'black', 'figure.facecolor':'white', \n            'figure.figsize': (45.0, 20.0)}, font_scale=8.0)\nax = sns.set_context(\"notebook\", font_scale=5.5, rc={\"lines.linewidth\": 0.5})\n\nax = sns.scatterplot(data=df[df['Star type'] == 0], x=\"Temperature (K)\", y=\"Absolute magnitude(Mv)\",\n                s=300, color='red')\nax = sns.scatterplot(data=df[df['Star type'] == 1], x=\"Temperature (K)\", y=\"Absolute magnitude(Mv)\",\n                s=300, color='sienna')\nax = sns.scatterplot(data=df[df['Star type'] == 2], x=\"Temperature (K)\", y=\"Absolute magnitude(Mv)\",\n                s=300, color='white')\nax = sns.scatterplot(data=df[df['Star type'] == 3], x=\"Temperature (K)\", y=\"Absolute magnitude(Mv)\",\n                s=300, color='blue')\nax = sns.scatterplot(data=df[df['Star type'] == 4], x=\"Temperature (K)\", y=\"Absolute magnitude(Mv)\",\n                s=300, color='purple')\nax = sns.scatterplot(data=df[df['Star type'] == 5], x=\"Temperature (K)\", y=\"Absolute magnitude(Mv)\",\n                s=300, color='yellow')\n\nplt.legend(['Red Dwarf', 'Brown Dwarf', 'White Dwarf', 'Main Sequence', 'Supergiants', 'Hypergiants'],\n          facecolor='w', markerscale=2)\nplt.show()","7bf75e1b":"# set visualisation parameters\nsns.set(rc={'figure.figsize': (45.0, 20.0)})\nsns.set(font_scale=8.0)\nsns.set_context(\"notebook\", font_scale=5.5, rc={\"lines.linewidth\": 0.5})\n\n# plot the distribution of the temperature variable\nplt.hist(df['Temperature (K)'], bins='auto', color='red', edgecolor='black', linewidth=2.0)\nplt.gca().set(title='Temperature frequency histogram', ylabel='Frequency', xlabel='Temperature (K)')\nplt.show()","30f808a2":"# plot the distribution of the luminosity variable\nplt.hist(df['Luminosity(L\/Lo)'], bins='auto', color='green', edgecolor='black', linewidth=2.0)\nplt.gca().set(title='Luminosity frequency histogram', ylabel='Frequency', xlabel='Luminosity(L\/Lo)')\nplt.show()","6a1e8ba3":"# plot the distribution of the radius variable\nplt.hist(df['Radius(R\/Ro)'], bins='auto', color='blue', edgecolor='black', linewidth=2.0)\nplt.gca().set(title='Radius frequency histogram', ylabel='Frequency', xlabel='Radius(R\/Ro)')\nplt.show()","5ee9b726":"# plot the distribution of the magnitude variable\nplt.hist(df['Absolute magnitude(Mv)'], bins='auto', color='orange', edgecolor='black', linewidth=2.0)\nplt.gca().set(title='Magnitude frequency histogram', ylabel='Frequency', xlabel='Absolute magnitude(Mv)')\nplt.show()","bb47ff7b":"# obtain the data types of each column within the data frame\nprint(df.dtypes)","31f0e622":"# convert columns to 'float64' from 'int64'\ndf['Temperature (K)'] = df['Temperature (K)'].astype(float)\ndf['Star type'] = df['Star type'].astype(float)","f1d87c3b":"# obtain One Hot Encoding of the 'Star color' and 'Spectral Class' columns\nstar_col_dummies = pd.get_dummies(df['Star color'], dtype=float)\nspec_cla_dummies = pd.get_dummies(df['Spectral Class'], dtype=float)\n\n# remove original columns from the main dataframe\ndf = df.drop(['Star color', 'Spectral Class'], axis=1)\n\n# join the One Hot Encoded dataframes onto the main dataframe\ndf = pd.concat([df, star_col_dummies], axis=1)\ndf = pd.concat([df, spec_cla_dummies], axis=1)","a0f90127":"from sklearn.preprocessing import MinMaxScaler\n\n# variables with the exponential distribution scaled logarithmically\ndf['Temperature (K)'] = np.log(df['Temperature (K)'])\ndf['Luminosity(L\/Lo)'] = np.log(df['Luminosity(L\/Lo)'])\ndf['Radius(R\/Ro)'] = np.log(df['Radius(R\/Ro)'])\n\n# define the scaler\nscaler = MinMaxScaler()\n\n# variables now scaled with the minmax scaler\ndf['Temperature (K)'] = scaler.fit_transform(np.expand_dims(df['Temperature (K)'], axis=1))\ndf['Luminosity(L\/Lo)'] = scaler.fit_transform(np.expand_dims(df['Luminosity(L\/Lo)'], axis=1))\ndf['Radius(R\/Ro)'] = scaler.fit_transform(np.expand_dims(df['Radius(R\/Ro)'], axis=1))\ndf['Absolute magnitude(Mv)'] = scaler.fit_transform(np.expand_dims(df['Absolute magnitude(Mv)'], axis=1))","fc512b84":"# import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# split our x and y values into new dataframes\nX_values = df.drop(['Star type'], axis=1)\ny_values = df['Star type']\n\n# now split our x and y values into train\/test sets with a 75\/25 percentage split\nX_train, X_test, y_train, y_test = train_test_split(X_values, y_values, test_size=0.25)\nprint(\"X_train shape is\", X_train.shape)\nprint(\"X_test shape is\", X_test.shape)\nprint(\"y_train shape is\", y_train.shape)\nprint(\"y_test shape is\", y_test.shape)","e7f2dde7":"# import evaluation metrics\nfrom sklearn import metrics","e51f9406":"# import random forest classifier model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# define our random forest classifier\nrfc = RandomForestClassifier(n_estimators=100)\n\n# train the model using the x and y training sets\nrfc.fit(X_train,y_train)","911b0d80":"# apply the model on unseen testing data\nrfc_preds = rfc.predict(X_test)\n\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, rfc_preds))","2908e9c3":"# import the support vector classifier model\nfrom sklearn.svm import SVC\n\n# define our support vector classifier model\nsvc = SVC(kernel='poly')  # polynomial kernel performed best with this experiment\n\n# train the model using the x and y training sets\nsvc.fit(X_train,y_train)","699db5e0":"# apply the model on unseen testing data\nsvc_preds = svc.predict(X_test)\n\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, svc_preds))","2e065225":"# import the K nearest neighbours classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# define the K nearest neighbours model\nknc = KNeighborsClassifier()\n\n# train our K nearest neighbours model with the x and y training sets\nknc.fit(X_train,y_train)","55aaf76b":"# apply the model on unseen testing data\nknc_preds = knc.predict(X_test)\n\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, knc_preds))","b4c6d4ad":"# import the gaussian naive bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n# define our gaussian naive bayes model\ngnc = GaussianNB()\n\n# fit our model with training x and y data\ngnc.fit(X_train,y_train)","b5e0cfaf":"# apply the model on unseen testing data\ngnc_preds = gnc.predict(X_test)\n\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, gnc_preds))","dc465144":"import torch\nimport torch.nn as nn\n\n# tensorize our x\/y train\/test data to form pytorch tensors\nX_train_tensor = torch.from_numpy(X_train.to_numpy()).float()\nX_test_tensor = torch.from_numpy(X_test.to_numpy()).float()\ny_train_tensor = torch.from_numpy(y_train.to_numpy()).long()\ny_test_tensor  = torch.from_numpy(y_test.to_numpy()).long()\nprint(\"X_train_tensor shape is\", X_train_tensor.shape)\nprint(\"X_test_tensor shape is\", X_test_tensor.shape)\nprint(\"y_train_tensor shape is\", y_train_tensor.shape)\nprint(\"y_test_tensor shape is\", y_test_tensor.shape)","4806349c":"# construct the deep learning MLP classifier\nclass MLP_Classifier(nn.Module):\n    def __init__(self, input_dim, output_dim, layer_sizes, dropout):\n        super(MLP_Classifier, self).__init__()\n        self.mlp = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(input_dim, layer_sizes[0]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(layer_sizes[0], layer_sizes[1]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(layer_sizes[1], layer_sizes[2]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(layer_sizes[2], layer_sizes[3]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(layer_sizes[3], layer_sizes[4]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(layer_sizes[4], output_dim),\n        )        \n        \n    # the forward pass through the network\n    def forward(self, input_tensor):\n        \n        output_tensor = self.mlp(input_tensor)  # pass the input tensor through the mlp\n        \n        return output_tensor\n    \n# now lets define the model\nmlp_classifier = MLP_Classifier(X_train_tensor.shape[1],\n                               len(torch.unique(y_train_tensor)),\n                               [100, 500, 600, 400, 100],\n                               0.1)\nprint(mlp_classifier)","aeba4cf5":"loss_function = nn.CrossEntropyLoss()  # cross entropy loss function\noptimizer = torch.optim.Adam(mlp_classifier.parameters(), lr=0.00005)  # adam's optimiser\nepochs = 10000  # number of epochs\nloss_vals_train = []  # hold the training loss values\nloss_vals_valid = []  # hold the validation loss values\n\nfor i in range(epochs):\n    y_pred_tensor = mlp_classifier(X_train_tensor)  # obtain y predictions\n    single_loss = loss_function(y_pred_tensor[:-20], y_train_tensor[:-20])  # calculate training loss\n    loss_vals_train.append(single_loss.item())\n    \n    # now calculate the validation loss\n    with torch.no_grad():  # disable the autograd engine\n        val_loss = loss_function(y_pred_tensor[-20:], y_train_tensor[-20:])  # calculate validation loss\n        loss_vals_valid.append(val_loss.item())\n    \n    optimizer.zero_grad()  # zero the gradients\n    single_loss.backward()  # backpropagate through the model\n    optimizer.step()  # update parameters\n    \n    if i%250 == 0:\n        print(f'epoch: {i:5} training loss: {single_loss.item():10.8f} validation loss: {val_loss.item():10.8f}')","bd3599bf":"sns.set(rc={'figure.figsize': (45.0, 20.0)})\nsns.set(font_scale=8.0)\nsns.set_context(\"notebook\", font_scale=5.5, rc={\"lines.linewidth\": 0.5})\nx_vals = np.arange(0, epochs, 1)\nax = sns.lineplot(x=x_vals, y=loss_vals_train)\nax = sns.lineplot(x=x_vals, y=loss_vals_valid)\nax.set_ylabel('Loss', labelpad=20, fontsize=75)\nax.set_xlabel('Epochs', labelpad=20, fontsize=75)\nplt.legend(labels=['Training loss', 'Validation loss'], facecolor='white', framealpha=1)\nplt.show()","a52d64de":"# prepare the model for evaluation\nmlp_classifier.eval()\n\n# obtain predictions from unseen testing data, and apply argmax\nmlp_preds = mlp_classifier(X_test_tensor)\nmlp_preds = np.argmax(mlp_preds.detach().numpy(), axis=1)\n\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, mlp_preds))","2a3c9f39":"# convert accuracy scores to percentages and store them as variables\nrfc_acc = metrics.accuracy_score(y_test, rfc_preds) * 100\nsvc_acc = metrics.accuracy_score(y_test, svc_preds) * 100\nknc_acc = metrics.accuracy_score(y_test, knc_preds) * 100\ngnc_acc = metrics.accuracy_score(y_test, gnc_preds) * 100\nmlp_acc = metrics.accuracy_score(y_test, mlp_preds) * 100\n\n# place accuracy scores in lists, and then create a dataframe\nmodels = ['Random Forest \\nClassifier', 'Support Vector \\nClassifier', 'K Nearest Neighbour\\n Classifier',\n         'Gaussion Naive \\nBayes Classifier', 'Deep Learning \\nMLP Classifier']\naccuracy = [rfc_acc, svc_acc, knc_acc, gnc_acc, mlp_acc]\nmodel_comp_df = pd.DataFrame({'Model': models, 'Accuracy': accuracy})\n\n# plot a barchart with the accuracy of each model\nsns.set_context(\"notebook\", font_scale=4.5, rc={\"lines.linewidth\": 0.5})\nax = sns.barplot(x=\"Model\", y=\"Accuracy\", data=model_comp_df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment='right')\nax.set_ylabel('Accuracy (%)', labelpad=50, fontsize=85)\nax.set_xlabel('Model', labelpad=50, fontsize=85)\n\nplt.title(\"A Barplot comparing the training data accuracy \\nof all five classifiers\", fontsize=100)\nplt.show()\n","d0094249":"## Stage 3: Splitting training\/testing sets\n\nNow that the data has been preprocessed, we will split the data into training and testing x and y values. A 75-25 percent training\/test split is used in this case.","4952c329":"The Support Vector Classifier model's performance is evaluated based upon its performance on unseen testing data:","76f557de":"It can also be noticed that the 'Star colour' and 'Spectral Class' columns are stored in 'Object format' as they are categorical variables. With this in mind, they need to be converted into numerical values which a predictive model can understand. This type of conversion is referred to as 'Encoding'.\n\nTwo common forms of encoding are 'Label Encoding' and 'One Hot Encoding'. Label Encoding involves converting each unique value within a column into a numerical value of 0, 1, 2, 3 and so on. However, a limitation of Label Encoding is the fact that a predictive model may interpret these converted values as ordinal\/qualitative in nature which can cause this data to be misunderstood. With this limitation being considered, One Hot Encoding can also be used. This form of encoding takes a column with n-number of categorical values and 'splits' this into n-number of new columns which correspond to each unique categorical value, with each row within these new columns containing either a 0 or 1, depending on whether each row contains a specific column value. One Hot Encoding will be used in this case because it avoids the misinterpretation of encoded values associated with the Label Encoding.\n\nThis is done as follows using the Pandas '.get_dummies() method:","f75d1932":"### 2.1 Datatype conversions\n\nIt is important that all variables are in a numerical format for them to be understood effectively by a machine. Therefore, the datatypes of the dataframes need to be inspected and converted appropriately where necessary.","22247e62":"### 2.2 Data scaling\n\nData is then scaled in order to accelerate the calculations involved within the predictive modelling. The One Hot Encoded variables do not require scaling, nor does the star type. The variables are scaled using a min-max scaler. However, since the 'Temperature (K)', 'Luminosity(L\/Lo)' and 'Radius(R\/Ro)' variables display a somewhat exponential distribution, they are also scaled logarithmically prior to the min-max scaler being applied. Meanwhile 'Absolute magnitude(Mv)' is scaled using a min-max scaler without the use of the logarithmic scaling.","a88a4d33":"### 1.1 Load the data\n\nFirst load in the dataset using the pandas '.read_csv()' method, as below:","bb874ea0":"The Guassian Naive Bayes Classifier displays moderate accuracy. However, with the testing data accuracy scores of re-runs typically falling within 0.82-0.93 (82-93%) it is the least accurate of the methods mentioned so far.","13cb5504":"The MLP can now be applied on the unseen testing data:","c9a2d937":"### 4.5 A Deep Learning Multilayer Perceptron\n\nA PyTorch Deep Learning Multilayer Perceptron (MLP) is also developed to form the fifth classification method.\n\nFirst we need to convert our data into PyTorch tensors:","ada7a827":"The model is then defined as below:","cf7a0e90":"Evaluate the Gaussian Naive Bayes model:","228be26c":"As is evident by the range of accuracy scores obtained by the five various models applied, the performance of each model is variable and so will often change between re-runs of this notebook. This is likely due to the random nature of the 'train_test_split()' method, and in the case of the Deep Learning MLP due to the random values of weights assigned prior to model training. Therefore, what is described in the Bar plot above will likely vary each time this notebook is re-run.\n\nDue to the variability in accuracy scores recorded by each model with every re-run, comparison between models is not always obvious. \n\nHowever, it is reasonable to conclude that the Random Forest Classifier is the best of the five models investigated. This is due to it usually obtaining 100% accuracy on the testing data.\n\nAdditionally, it is also reasonable to conclude that the Gaussian Naive Bayes Classifier is the worst of the five models investigated. This is due to it recording the lowest testing data accuracy scores of all of the models investigated (usually 82-93%), and the fact that all other methods were capable of obtaining 100% accuracy on the testing data.\n\nThe Support Vector Classifier, K-Nearest Neighbour Classifier and Deep Learning MLP are unfortunately less clear to compare and rank as they were all capable of obtaining 100% accuracy. However, the Support Vector Classifier was the most consistent of these three with the range of accuracy scores being between 95-100%. As such it may be reasonable for this method to be considered the second best after the Random Forest Classifier. By the same logic, it may therefore be reasonable to conclude that the Deep Learning MLP is the third best overall (testing data accuracy 90-100%), and the K-Nearest Neighbour Classifier is the fourth best (testing data accuracy 88-100%), only better than the Gaussian Naive Bayes Classifier.","8357f521":"### 4.4 Gaussian Naive Bayes Classifier\n\nThe fourth model applied in the Gaussian Naive Bayes Classifier, which is a probabilistic classifier.\n\nThe Gaussian Naive Bayes Classifier is fit with the following code:","640a6e4c":"## Stage 5: Model Comparison\n\nWith these five models having been applied, the accuracy recorded with the testing data in the current kernel will now be compared:","986a5c56":"The training and validation set losses are plotted against the number of epochs with the following code:","b71410bf":"## Stage 2: Data Preprocessing\n\nThe second stage aims to processing the existing data into a format that can be understood effectively by a machine. Specifically this involves datatype conversions (2.1) and data scaling (2.2).","77a3a5f8":"Now lets evaluate our K nearest neighbour model:","181e1418":"### 4.2 Support Vector Classifier\n\nThe second model applied is scikit-learn's Support Vector Classifier, which makes use of Support Vector Machines to effectively classify data.\n\nThe Support Vector Classifier is fit with the following code:","2b628d90":"The data can be plotted first to examine the relationships outlined by the dataset origin:","36e2dddd":"## Stage 1: Data Description & Exploration\n\nThe first step aims to provide an introduction to the data. Specifically, this involves loading in the data (1.1), providing a brief description of the data being used (1.2) as well an exploration of the data (1.3).","53bbab66":"The Deep Learning MLP typically displays testing data accuracy scores within a range of 0.90-1.0 (90-100%). With this in mind, the model's lower bounds of accuracy are comparable with the upper ranges of the Gaussian Naive Bayes Classifier and lower ranges of the K-Nearest Neighbour Classifier, but equally also by being capable of achieving 100% accuracy the performance of this classifier can be consistent with the Random Forest Classifier, and the upper ranges of both the Support Vector Classifier and K-Nearest Neighbour Classifier.","6bb49d0e":"The K-Nearest Neighbour Classifier also displays considerable accuracy, with testing data accuracy scores over a number of re-runs being between 0.88-1.0 (88-100%). However, the accuracy values are variable and the Random Forest Classifier and Support Vector Classifier often exhibit slightly higher accuracies respectively.","d6ac56a7":"### 1.3 Data Exploration","d6d97942":"The Support Vector Classifier also appears to be highly accurate, with testing data accuracy scores from a number of re-runs generally being between 0.95-1.0 (95-100%). However, generally it is not quite as accurate as the Random Forest Classifier which usually displays 100% accuracy.","1943a5af":"The Random Forest Classifier used appears to be very accurate, with testing data accuracy scores usually being 1.0 (100% accuracy).","ce4985c4":"### 1.2 Describe the data \n\nThe following code segments provide a brief description of the data that will be used. Specifically, the first few rows are inspected using the pandas '.head()' method, whilst the dataset is described using the pandas '.describe()' and the dimensions of the dataset are obtained using the pandas '.shape' method.","5fcaca4f":"### 4.1 Random Forest Classifier\n\nThe first model applied is scikit-learn's Random Forest Classifier, which fits a number of decision tree classifiers on various sub-samples of the dataset. Averaging is applied to improve the predictive accuracy and control over-fitting. \n\nThe Random Forest Classifier model is fit with the following code:","72e55945":"# Star Type Classification using 5 Machine Learning (ML) Methods\n\n## Introduction\n\nThis notebook makes use of the star dataset and aims to apply and subsequently evaluate different Machine Learning (ML) methods based upon their ability to classify star types based upon the values of other variables within the dataset. Specifically, this notebook applies a (i) Random Forest Classifier, (ii) Support Vector Classifier, (iii) K-Nearest Neighbour Classifier, (iv) Gaussian Naive Bayes Classifier and (v) a Deep Learning Multilayer Perceptron Classifier, with the Random Forest Classifier usually displaying the best performance.\n\nThis notebook is split up into five different sections as follows:\n\n**1) Stage 1: Data Description & Data Exploration**\n\n**2) Stage 2: Data Preprocessing**\n\n**3) Stage 3: Splitting the training\/testing data subsets**\n\n**4) Stage 4: Model fitting and evaluation**\n\n**5) Stage 5: Model Comparison**","7af08dfb":"## Stage 4: Models Fitting & Evaluation\n\nThis fourth stages involves fitting the various different models, and subsequently evaluating them. The models used are the Random Forest Classifier (4.1), the Support Vector Classifier (4.2), the K-Nearest Neighbour Classifier (4.3), the Gaussian Naive Bayes Classifier (4.4) and a Deep Learning Multilayer Perceptron Classifier (4.5).\n\nFirst, the evaluation metrics are imported:","6b709ffc":"The model is then applied on unseen testing data, and its performance evaluated:","2662009d":"### 4.3 K-Nearest Neighbour Classifier\n\nThe third model applied is scikit-learn's K-Nearest Neighbour Classifier which implements the KNN vote to classify input data.\n\nThe K-Nearest Neighbour Classifier is fit with the following code:","c4e6b1eb":"The distributions of the data corresponding to variables 'Temperature (K)', 'Luminosity(L\/Lo)', 'Radius(R\/Ro)' and 'Absolute magnitude(Mv)' are visualised with the following code segments:","eacf0cab":"The model is trained with the following code:","a59b730e":"As can be seen from the code segment which printed the datatypes, the 'Temperature (K)' and 'Star type' column values are stored as an integer ('int64') format. These are converted to floats, as follows:"}}