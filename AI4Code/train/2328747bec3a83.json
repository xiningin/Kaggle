{"cell_type":{"358cd506":"code","34cbbc8b":"code","da2e1e26":"code","76cd73d7":"code","8126246d":"code","c42126bd":"code","9d206f52":"code","c2eb1b74":"code","51e931ee":"code","ce6e760c":"code","53c0f160":"code","1a8f4532":"code","68cf164d":"code","3aaa9481":"code","e6269134":"code","137d9689":"code","382bc481":"code","24272d72":"code","3af3bb9d":"code","4c26f99a":"code","41ee092f":"code","9446d2cd":"code","6f05bda1":"code","03ea5ae8":"code","72e7f940":"code","01962127":"code","5974ef98":"code","90e7f4ed":"code","3d41fa5d":"code","a1b520f2":"code","d7908d89":"code","c0d4739f":"code","71c7c6f8":"code","e2edaf55":"code","456650c8":"code","861858e2":"code","c4afeac7":"code","4cb80be3":"markdown","2e6ee679":"markdown"},"source":{"358cd506":"from keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom skimage import io","34cbbc8b":"IMAGE_SIZE = [224, 224, 3]\nbatch_size = 512\ntrain_path = '..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train'\nfolders = glob(train_path+'\/*')\nprint(folders)\nnoclasses = len(folders)\nprint(noclasses)","da2e1e26":"print('Number files in each folder')\nfor path in folders:\n    files = glob(path+'\/*')\n    print(path, len(files))","76cd73d7":"img_path = glob(folders[0]+'\/*')[0]\nim = cv2.imread(img_path)\nplt.imshow(im)\nprint(im.shape)","8126246d":"datagen = ImageDataGenerator(#rescale=1.0\/255, \n                             #shear_range=0.2, \n                             #zoom_range=0.2, \n                             #horizontal_flip=True,\n                             validation_split=0.3)\n\ntrain_generator = datagen.flow_from_directory(train_path, \n                                              shuffle=True,\n                                              target_size = (224,224),\n                                              batch_size = batch_size,\n                                              class_mode = 'categorical',\n                                              subset='training') # set as training data\n\ntest_generator = datagen.flow_from_directory(train_path, \n                                             shuffle=False,\n                                             target_size = (224,224),\n                                             batch_size = batch_size,\n                                             class_mode = 'categorical',\n                                             subset='validation') # set as validation data","c42126bd":"train_generator.class_indices","9d206f52":"for i in range(3):\n    # convert to unsigned integers\n    t_x, t_y = train_generator.__getitem__(i)\n    print(t_x[0].shape, t_y[0].shape)","c2eb1b74":"inv_dict = dict(zip(train_generator.class_indices.values(), train_generator.class_indices.keys()))\ninv_dict","51e931ee":"# generate batch of images\n#for i in range(1):\n# convert to unsigned integers\ni=10\nt_x, t_y = train_generator.__getitem__(i)\n# plot image\nplt.imshow(t_x[i].astype('uint8'))\nplt.axis('off')\nplt.title(inv_dict[np.argmax(t_y[i])])","ce6e760c":"model_name = VGG16(input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)","53c0f160":"model_name.trainable = False","1a8f4532":"x = Flatten()(model_name.output)\nprediction = Dense(noclasses, activation='softmax')(x)\nmodel = Model(inputs=model_name.input, outputs=prediction)","68cf164d":"model.summary()","3aaa9481":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","e6269134":"model_history = model.fit(train_generator, \n                          validation_data=test_generator,\n                          epochs=25,\n                          steps_per_epoch=len(train_generator),\n                          validation_steps=len(test_generator))","137d9689":"t_x, t_y = next(train_generator)\nfig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    c_ax.imshow(c_x[:,:,0],vmin = -127, vmax = 127)\n    c_ax.axis('off')","382bc481":"t_x.shape[1:]","24272d72":"in_lay = Input(t_x.shape[1:])\nmodel_name = VGG16(input_shape =  t_x.shape[1:], include_top = False, weights = 'imagenet')\nmodel_name.trainable = False\npt_features = model_name(in_lay)\npt_depth = model_name.get_output_shape_at(0)[-1]\n\nbn_features = BatchNormalization()(pt_features)\n\n# here we do an attention mechanism to turn pixels in the GAP on an off\n\nattn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)\nattn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\nattn_layer = LocallyConnected2D(1, \n                                kernel_size = (1,1), \n                                padding = 'valid', \n                                activation = 'sigmoid')(attn_layer)\n# fan it out to all of the channels\nup_c2_w = np.ones((1, 1, 1, pt_depth))\nup_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n               activation = 'linear', use_bias = False, weights = [up_c2_w])\nup_c2.trainable = False\nattn_layer = up_c2(attn_layer)\n\nmask_features = multiply([attn_layer, bn_features])\ngap_features = GlobalAveragePooling2D()(mask_features)\ngap_mask = GlobalAveragePooling2D()(attn_layer)\n# to account for missing values from the attention model\ngap = Lambda(lambda x: x[0]\/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\ngap_dr = Dropout(0.5)(gap)\ndr_steps = Dropout(0.25)(Dense(1024, activation = 'elu')(gap_dr))\nout_layer = Dense(29, activation = 'softmax')(dr_steps) \nmodel = Model(inputs = [in_lay], outputs = [out_layer])","3af3bb9d":"# from keras.metrics import mean_absolute_error\n# def mae_months(in_gt, in_pred):\n#     return mean_absolute_error(boneage_div*in_gt, boneage_div*in_pred)","4c26f99a":"model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","41ee092f":"model.summary()","9446d2cd":"weight_path=\"{}_weights.best.hdf5\".format('asl')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=5) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","6f05bda1":"model_history = model.fit_generator(train_generator,  \n                    validation_data = test_generator, \n                    epochs = 15, \n                    callbacks = callbacks_list)","03ea5ae8":"pred = model.predict(test_generator)\npred = np.argmax(pred, axis=1)\npred","72e7f940":"test_generator.labels","01962127":"print(accuracy_score(test_generator.labels, pred))\nf1 = f1_score(test_generator.labels, pred,average=None, labels=list(range(0,29)))\n\ncm = confusion_matrix(test_generator.labels, pred)","5974ef98":"plt.figure(figsize=(25,12))\nsns.heatmap(cm, annot=True)\nplt.show()","90e7f4ed":"# load the best version of the model\nmodel.load_weights(weight_path)","3d41fa5d":"# get the attention layer since it is the only one with a single output dim\nfor attn_layer in model.layers:\n    c_shape = attn_layer.get_output_shape_at(0)\n    if len(c_shape)==4:\n        if c_shape[-1]==1:\n            print(attn_layer)\n            break","a1b520f2":"model(in_lay)","d7908d89":"model.get_input_at(0)","c0d4739f":"Input(t_x.shape[1:])","71c7c6f8":"[attn_layer.get_output_at(0)]","e2edaf55":"attn_func = K.function(inputs = [model.get_input_at(0)], outputs = [attn_layer.get_output_at(0)])","456650c8":"test_X, test_Y =  next(train_generator)","861858e2":"print(test_X.shape)\nprint(test_Y.shape)\nprint(cur_img.shape)","c4afeac7":"import keras.backend as K\nrand_idx = np.random.choice(range(len(test_X)), size = 6)\nattn_func = K.function(inputs = [model.get_input_at(0)],\n           outputs = [attn_layer.get_output_at(0)])\n\nfig, m_axs = plt.subplots(len(rand_idx), 2, figsize = (8, 4*len(rand_idx)))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor c_idx, (img_ax, attn_ax) in zip(rand_idx, m_axs):\n    cur_img = test_X[c_idx:(c_idx+1)]\n    attn_img = attn_func([cur_img])[0]\n    img_ax.imshow(cur_img[0,:,:,0].astype('uint8'), cmap=None)\n    attn_ax.imshow(attn_img[0, :, :, 0], cmap = None, \n                   vmin = 0, vmax = 1, \n                   interpolation = 'lanczos')\n    \n    img_ax.set_title(inv_dict[np.argmax(test_Y[c_idx])])\n#     img_ax.set_title('Hand Image\\nAge:%2.2fY' % (real_age\/12))\n    pred1 = inv_dict[np.argmax(model.predict(cur_img))]\n    attn_ax.set_title('Attention Map\\nPred:'+pred1)\n# fig.savefig('attention_map.png', dpi = 300)","4cb80be3":"## Show Attention","2e6ee679":"## Acknowledgements\n- [Attention on Pretrained-VGG16 for Bone Age by K Scott Mader](https:\/\/www.kaggle.com\/kmader\/attention-on-pretrained-vgg16-for-bone-age)"}}