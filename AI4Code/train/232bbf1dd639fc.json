{"cell_type":{"19646123":"code","7d34cd6d":"code","56ca8f2f":"code","34d2768e":"code","247053a1":"code","7b4cf57c":"code","a9e13a8c":"code","f7d4b748":"code","c0f3c4fb":"code","2047dda6":"code","df964cbd":"markdown","9fe0aa19":"markdown","f2952d9e":"markdown","795a5ffd":"markdown","4d52da9e":"markdown","f8267e04":"markdown"},"source":{"19646123":"import numpy as n\nimport pandas as pd\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport keras\nimport keras.backend as K\nfrom keras.layers import *\nfrom keras.losses import *\nfrom keras.models import *\nfrom keras.callbacks import *\nfrom keras.activations import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import WordPunctTokenizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv\", sep='\\t')\ntest = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv\", sep='\\t')\n\ntrain.head()","7d34cd6d":"def get_preprocessing_func():\n    tokenizer = WordPunctTokenizer()\n    lemmatizer = WordNetLemmatizer()\n    def preprocessing_func(sent):\n        return [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(sent)]\n    return preprocessing_func\n\nX = train['Phrase'].apply(get_preprocessing_func()).values\ny = train['Sentiment'].values\nX_test = test['Phrase'].apply(get_preprocessing_func()).values","56ca8f2f":"def prepare_tokenizer_and_weights(X):\n    tokenizer = Tokenizer(filters='')\n    tokenizer.fit_on_texts(X)\n    \n    weights = np.zeros((len(tokenizer.word_index)+1, 300))\n    with open(\"..\/input\/fatsttext-common-crawl\/crawl-300d-2M\/crawl-300d-2M.vec\") as f:\n        next(f)\n        for l in f:\n            w = l.split(' ')\n            if w[0] in tokenizer.word_index:\n                weights[tokenizer.word_index[w[0]]] = np.array([float(x) for x in w[1:301]])\n    return tokenizer, weights","34d2768e":"tokenizer, weights = prepare_tokenizer_and_weights(np.append(X, X_test))\nX_seq = tokenizer.texts_to_sequences(X)\nMAX_LEN = max(map(lambda x: len(x), X_seq))\nX_seq = pad_sequences(X_seq, MAX_LEN)\nMAX_ID = len(tokenizer.word_index)\nprint('MAX_LEN=', MAX_LEN)\nprint('MAX_ID=', MAX_ID)","247053a1":"def make_fast_text():\n    fast_text = Sequential()\n    fast_text.add(InputLayer((MAX_LEN,))) \n    fast_text.add(Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True))\n    fast_text.add(SpatialDropout1D(0.5))\n    fast_text.add(GlobalMaxPooling1D())\n    fast_text.add(Dropout(0.5))\n    fast_text.add(Dense(5,activation='softmax'))\n    return fast_text\n\nfast_texts = [make_fast_text() for i in range(3)]\nfast_texts[0].summary()\n\nfor fast_text in fast_texts:\n    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n    fast_text.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    fast_text.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n                 callbacks=[EarlyStopping(monitor='val_loss', patience=2, verbose=0)],\n                 epochs=30, \n                 verbose=2)","7b4cf57c":"def make_model_lstm():\n    model_lstm = Sequential()\n    model_lstm.add(InputLayer((MAX_LEN,))) \n    model_lstm.add(Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True))\n    model_lstm.add(SpatialDropout1D(0.5))\n    model_lstm.add(Bidirectional(CuDNNLSTM(300, return_sequences=True)))\n    model_lstm.add(BatchNormalization())\n    model_lstm.add(SpatialDropout1D(0.5))\n    model_lstm.add(Bidirectional(CuDNNLSTM(300)))\n    model_lstm.add(BatchNormalization())\n    model_lstm.add(Dropout(0.5))\n    model_lstm.add(Dense(5,activation='softmax'))\n    return model_lstm\n\nmodel_lstms = [make_model_lstm() for i in range(2)]\nmodel_lstms[0].summary()\n\nfor model_lstm in model_lstms:\n    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n    model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    model_lstm.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n                 callbacks=[EarlyStopping(monitor='val_loss', patience=1, verbose=0)],\n                 epochs=30, \n                 verbose=2)","a9e13a8c":"def make_model_cnn():\n    inputs = Input((MAX_LEN,))\n    x = Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True)(inputs)\n    x = SpatialDropout1D(0.5)(x)\n    x = Conv1D(300, kernel_size=5,activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = SpatialDropout1D(0.5)(x)\n    x = MaxPooling1D(pool_size=2, strides=2)(x)\n    x = Conv1D(300, kernel_size=5,activation='relu')(x)\n    x = GlobalMaxPooling1D()(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(5,activation='softmax')(x)\n    model_cnn = Model(inputs, outputs)\n    return model_cnn\n\nmodel_cnns = [make_model_cnn() for i in range(3)]\nmodel_cnns[0].summary()\n\nfor model_cnn in model_cnns:\n    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n    model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    model_cnn.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n                 callbacks=[EarlyStopping(monitor='val_loss', patience=2, verbose=0)],\n                 epochs=30, \n                 verbose=2)","f7d4b748":"def make_model_abcnn():\n    def attention_layer(l):\n        x = Permute((2,1))(l)\n        x = Dense(K.int_shape(x)[2], activation='sigmoid')(x)\n        x = Permute((2,1))(x)\n        return multiply([x, l])\n    inputs = Input((MAX_LEN,))\n    x = Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True)(inputs)\n    x = SpatialDropout1D(0.5)(x)\n    x = Conv1D(300, kernel_size=3,activation='relu')(x)\n    x = attention_layer(x)\n    x = SpatialDropout1D(0.5)(x)\n    x = MaxPooling1D(pool_size=2, strides=2)(x)\n    x = Conv1D(300, kernel_size=3,activation='relu')(x)\n    x = attention_layer(x)\n    x = SpatialDropout1D(0.5)(x)\n    x = MaxPooling1D(pool_size=2, strides=2)(x)\n    x = Conv1D(300, kernel_size=3,activation='relu')(x)\n    x = attention_layer(x)\n    x = GlobalMaxPooling1D()(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(5,activation='softmax')(x)\n    model_cnn = Model(inputs, outputs)\n    return model_cnn\n\nmodel_abcnns = [make_model_abcnn() for i in range(3)]\nmodel_abcnns[0].summary()\n\nfor model_abcnn in model_abcnns:\n    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n    model_abcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    model_abcnn.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n                 callbacks=[EarlyStopping(monitor='val_loss', patience=2, verbose=0)],\n                 epochs=30, \n                 verbose=2)","c0f3c4fb":"def make_model_bagged(models):\n    inputs = Input((MAX_LEN,))\n    outputs = average([model(inputs) for model in models])\n    return Model(inputs, outputs)\nmodel_bagged = make_model_bagged(model_abcnns)\nmodel_bagged.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\ny_prob = model_bagged.predict(X_seq)\ny_predict = np.argmax(y_prob, axis=1)\nprint(classification_report(y, y_predict))\nsns.heatmap(confusion_matrix(y, y_predict));","2047dda6":"X_test_seq = tokenizer.texts_to_sequences(X_test)\nX_test_seq = pad_sequences(X_test_seq, MAX_LEN)\ny_test_prob = model_bagged.predict(X_test_seq)\ny_test_predict = np.argmax(y_test_prob, axis=1)\nout_df = test[['PhraseId']]\nout_df['Sentiment'] = y_test_predict\nout_df.to_csv('submission.csv', index=False)","df964cbd":"## CNN Model with Pretrained Embedding\n\nCNN's are fast to train and performs second to LSTM.","9fe0aa19":"## Attention-Based CNN with Pretrained Embedding\n\nRef: [Keras Attenton Mechanism](https:\/\/github.com\/philipperemy\/keras-attention-mechanism)\n[ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs](http:\/\/www.aclweb.org\/anthology\/Q16-1019)","f2952d9e":"## Bagging of ABCNN Model\n\nFinally, let's ensemble them.","795a5ffd":"## FastText Model\n\nFastText is simple and fast, and sometimes can achieve state-of-art performance.","4d52da9e":"# FastText \/ LSTM \/ CNN Comparison\n\n## Contents\n\n1.  FastText\n1.  LSTM\n1.  CNN\n1.  Bagging of the above","f8267e04":"## LSTM Model with Pretrained Embedding\n\nLSTM is the top performer on this problem, but is slow to train."}}