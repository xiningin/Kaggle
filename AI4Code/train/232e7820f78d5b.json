{"cell_type":{"5811e206":"code","5aecd988":"markdown","07ba6c27":"markdown","e3d3baa8":"markdown"},"source":{"5811e206":"import matplotlib.pyplot as plt\nimport numpy as np\nimport random\n# from PIL import Image\n# import PIL.ImageOps    \n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.utils\n\n\n# --------------------------------------------------------------------\n# torch.nn.Module is the base class for all neural network modules;\n# all PyTorch models are expected to inherit from it. This is the\n# object-functional equivalent to the purely functional Keras Model()\n# class definition.\n#\n# Model is expected to implement two methods: __init__ and forward.\n# The former handles initialization, the latter, a single pass through\n# training.\nclass SiameseNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        \n        # PyTorch, like Keras, has a sequential API. Unlike Keras,\n        # PyTorch allows for constructing the layer sequence from\n        # a single input list.\n        self.cnn1 = nn.Sequential(\n            # Pads the input tensor using the reflection of the\n            # input boundary. E.g. the last row will be mirrored to a\n            # new additional last row, the last column will be\n            # mirrored to a new additional last column, etcetera.\n            #\n            # This bit of padding is being used in place of padding=1\n            # in the Conv2d layer. Padding ensures that the output of\n            # the layer has the same dimensions as the input, as the\n            # feature map is applied to every pixel in the input\n            # image.\n            #\n            # Padding should be (kernel_size - 1) \/ 2. With a\n            # kernel_size of 3, a padding value of 1 is appropriate.\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(\n                # input is black and white\n                in_channels=1,\n                # 4x simult feature maps, equivalent to node count,\n                # the first parameter in keras\n                out_channels=4,\n                # 3x3 feature maps\n                kernel_size=3\n                # params set to their default value:\n                # dilation=1 (none)\n                # stride=1 (single-pixel window)\n                # padding=0 (no zero-padding; see ReflectionPad2d)\n            ),\n            # Applies ReLU activation functions in the Conv2d \n            # nodes. inplace=True specifies that the operation is\n            # to be performed in-place in-memory. This saves a\n            # small amount of memory, but is not a valid operation\n            # if the Conv2d layer has multiple consumer layers.\n            # inplace=False by default.\n            nn.ReLU(inplace=True),\n            # Batch normalization is the technique of normalizing\n            # the hidden vectors in a batch or mini-batch. Batch\n            # normalization is performed on a vector component\n            # basis, e.g. every component of the vectors in a batch\n            # is scaled against that same component in its neighboring\n            # vectors.\n            #\n            # Batch normalization has been shown to improve training\n            # accuracy and speed in practice. The mechanism by which\n            # it does so is not fully known, but it is believed to be\n            # because it acts to smooth the error surface.\n            #\n            # Batch normalization is a defense against exploding or\n            # vanishing gradients.\n            nn.BatchNorm2d(\n                # num_features is confusingly named. Should be the node\n                # count in the case of a CNN.\n                num_features=4\n            ),\n            # Dropout2d drops entire feature maps at once. This is\n            # used instead of Dropout because a feature map consists\n            # of a large number of nodes, one per pixel, which are\n            # highly correlated with one another (as images are\n            # highly spatially correlated). Thus using \"regular\"\n            # dropout would serve only to reduce the accuracy of the\n            # model.\n            #\n            # .2 is a conservative dropout rate. For hidden layers, .5\n            # is a common, more aggressive dropout rate.\n            nn.Dropout2d(p=.2),\n            \n            # Two more layers of convolutional feature maps follow.\n            # No pooling is applied. This is a curious decision, when\n            # you're building a CNN. Pooling layers are not strictly\n            # necessary, but, eh.\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(4, 8, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(8),\n            nn.Dropout2d(p=.2),\n\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(8, 8, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(8),\n            nn.Dropout2d(p=.2)\n        )\n\n        # Finally, define the fully-connected layers. The Linear\n        # layer takes two inputs, unlike the Keras version (Dense):\n        # you have the specify the input_size as well as the output\n        # size (the latter being the node count).\n        self.fc1 = nn.Sequential(\n            nn.Linear(8*100*100, 500),\n            nn.ReLU(inplace=True),\n            nn.Linear(500, 500),\n            nn.ReLU(inplace=True),\n            nn.Linear(500, 5)\n        )\n\n    # Helper method for forward, not API-required.\n    def forward_once(self, x):\n        output = self.cnn1(x)\n        # The CNN outputs a feature map matrix, but the fully\n        # connected layer expects vectorized linear input. So we have\n        # to do the equivalent of np.ravel() to the output. In Keras\n        # this was done using a Reshape layer. PyTorch doesn't have\n        # an equivalent layer-based operation (!?) so we have to do\n        # that glue transform at propogation time.\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n\n    # Handles one pass through the model.\n    def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2\n\n# To use the GPU you execute .cuda() on the model constructor.\nclf = SiameseNetwork().cuda()\n\n# The optimizer takes parameters as input, in addition to the\n# learning rate. parameters is a container iterable with a list of\n# layer objects contained inside. This means that in PyTorch to\n# freeze layers for example you do something like the following:\n#     for param in model.parameters():\n#         param.requires_grad = False\noptimizer = optim.Adam(clf.parameters(), lr=0.0005)\n\n# ContrastiveLoss is built into PyTorch! This is not true\n# of Keras, so this is nice to have.\nloss_func = ContrastiveLoss()\n\n# Now for training...\nfor epoch in range(0, train_number_epochs):\n    # train_dataloader is just an iterable that serves train\n    # data. Not defined here; see the original code. Note that\n    # you can \n    for i, data in enumerate(train_dataloader, 0):\n        # Variables are wrapped in a Variable object, and you\n        # have to call .cuda() on them to signal GPU training.\n        img0, img1, label = data\n        img0, img1, label = (\n            Variable(img0).cuda(), Variable(img1).cuda() ,\n            Variable(label).cuda()\n        )\n        \n        # In PyTorch every step of backpropagation is handled\n        # separately:\n        #\n        # 1. Perform the forward pass.\n        # 2. Set the accumulated gradient to zero. *\n        # 3. Apply the loss function to determine loss.\n        # 4. Calculate the gradients (backwards).\n        # 5. Backpropogate the gradients (step).\n        #\n        # * Setting the gradient to zero with zero_grad is\n        #   required because PyTorch supports accumulating\n        #   the gradient across multiple batches.\n        #\n        # The Siamese model is trained using stochastic mini-batches,\n        # e.g. batches with a batch size of 1. In other contexts we\n        # would batch the images ourselves coming out of the training\n        # data loader.\n        output1, output2 = clf(img0, img1)\n        optimizer.zero_grad()\n        \n        loss_contrastive = loss_func(output1, output2, label)\n        loss_contrastive.backward()\n        optimizer.step()","5aecd988":"# Siamese networks\n\n**Siamese networks** is a neural network archetype that learns to determine whether two examples are the same or different. This architecture is the most effective way of extending the expertise and learning capacity of neural networks to small datasets. True to its name, it does this by training on images in pairs, instead of training on them one-at-a-time.\n\nFor an example of an application of a Siamese network, consider the Kaggle competition [\"Painter by the Numbers\"](https:\/\/www.kaggle.com\/c\/painter-by-numbers). The objective in this competition was to classify paintings by painter, and to determine what elements in an unknown set of paintings is attributable to the artist and which ones are forgeries. A classical approach to this problem would be to train a CNN with a categorical output layer, but because the training set was small, and the objective is explicitly pairwise, Siamese networks are very well-suited.\n\nThis notebook is based on the following pair of blog posts: [\"One Shot Learning with Siamese Networks in PyTorch\"](https:\/\/hackernoon.com\/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e) and [\"Facial Similarity with Siamese Networks in PyTorch\"](https:\/\/medium.com\/hackernoon\/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7).\n\nSiamese networks have the following architecture:\n\n![](https:\/\/i.imgur.com\/aQdOIum.png)\n\nThey consist of two submodels whose outputs are evaluated using [contrastive loss](https:\/\/www.quora.com\/What-is-a-contrastive-loss-function-in-Siamese-networks). Contrastive loss measures how well the network is able to distinguish between two different pairs of images, by measuring a function of the Euclidean distance (you can also use some other norm, if you'd like) between the outputs of the two component networks. Contrastive loss in this way encourages the networks to build output vectors that compose a space where each class of output composes a cluster that is as far away from the other class clusters as possible.\n\nThe architecture is known as a Siamese network because the left and right models are exactly the same: e.g. they are not just structural clones of one another, but the exact same model, down to the weights.","07ba6c27":"## Worked example\n\nThe following example Siamese network, implemented in PyTorch, is courtesy of the [Part 2 article](https:\/\/medium.com\/hackernoon\/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7). The implementation is presented here with comments. This code is not run-able because the data is elsewhere&mdash;[in the project GitHub repo](https:\/\/github.com\/harveyslash\/Facial-Similarity-with-Siamese-Networks-in-Pytorch).","e3d3baa8":"For a Keras version of this same concept, see the following article: https:\/\/sorenbouma.github.io\/blog\/oneshot\/."}}