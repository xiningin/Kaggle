{"cell_type":{"c89e4398":"code","1f8e1432":"code","b21fa7c0":"code","138726dd":"code","a2ff68e7":"code","1569146c":"code","fabd6a31":"code","1efbae1f":"code","824413c1":"code","1f5372ca":"code","b873b3b3":"code","5f13f8e0":"markdown"},"source":{"c89e4398":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\n# from keras import models\n# from keras import layers\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error as mse","1f8e1432":"train = pd.read_csv('..\/input\/train.csv', parse_dates=['date'])\ntest  = pd.read_csv('..\/input\/test.csv', parse_dates=['date'], index_col='id')\ntrain.head()\ntrain_y = train[\"price\"].copy()\n\n## feature engineering \ud55c\ubc88\uc5d0 \ud558\ub294 \ud568\uc218 \n### train\/test split, \n\ndef feature_engineering(df, is_train = True):\n    \n    # feature 1 : sum and sub of latitude longitude \n    df['latlongsum'] = df['lat'] + df['long']\n    df['latlongsub'] = df['lat'] - df['long']\n    \n    # feature 2 : month and year\n    df['Month'] = df['date'].dt.month\n    df['Year'] = df['date'].dt.year\n    \n    # feature 3 : renovated year update\n    df.loc[df.yr_renovated==0,'yr_renovated']=df[df.yr_renovated==0].yr_built\n    \n    # feature 4 : zipfeatures (ref: https:\/\/www.kaggle.com\/tmheo74\/geo-data-eda-and-feature-engineering)\n    df['zipcode_str'] = df['zipcode'].astype(str)  \n    df['zipcode-3'] = 'z_' + df['zipcode_str'].str[2:3]\n    df['zipcode-4'] = 'z_' + df['zipcode_str'].str[3:4]\n    df['zipcode-5'] = 'z_' + df['zipcode_str'].str[4:5]\n    df['zipcode-34'] = 'z_' + df['zipcode_str'].str[2:4]\n    df['zipcode-45'] = 'z_' + df['zipcode_str'].str[3:5]\n    df['zipcode-35'] = 'z_' + df['zipcode_str'].str[2:3] + df['zipcode_str'].str[4:5]\n    df.drop(['zipcode_str'], 1, inplace=True)\n    \n    # drop useless columns\n    if is_train:\n        df.drop([\"id\"], 1, inplace=True)\n        df.drop([\"price\"], 1, inplace=True)\n        df.drop([\"date\"], 1, inplace=True)\n    else: # test\ub294 id\ub791 price\uac00 \uc5c6\uc73c\ubbc0\ub85c\n        df.drop([\"date\"], 1, inplace=True)\n    \n    # label encoding\n    cat_cols = df.select_dtypes('object').columns\n    for col in cat_cols:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n        \n    # feature 5 : pca -> date \ub4dc\ub78d\ud6c4\uc5d0 \ud574\uc57c\ub428. \n    pca1 = PCA(n_components=2)\n    pca1.fit(df)\n    coord_pca2 = pca1.transform(df)\n    df['pca1'] = coord_pca2[:, 0]\n    df['pca2'] = coord_pca2[:, 1]\n    \n    # feature 6 : pca (lat, long)\n    coord = df[['lat','long']]\n    pca2 = PCA(n_components=2)\n    pca2.fit(coord)\n    coord_pca = pca2.transform(coord)\n    df['coord_pca1'] = coord_pca[:, 0]\n    df['coord_pca2'] = coord_pca[:, 1]\n    \n    return df\n\n# ## \ud3c9\uac00\ud568\uc218 \n# def eval(val_y, pred):\n#     rmse = np.sqrt(mse(val_y, pred))\n#     return rmse","b21fa7c0":"train = feature_engineering(train, True)\ntest = feature_engineering(test, False)\n\ndef rmse_exp(predictions, dmat):\n    labels = dmat.get_label()\n    error = np.expm1(predictions) - np.expm1(labels)\n    squared_error = np.square(error)\n    mean = np.mean(squared_error)\n    return ('rmse_exp', np.sqrt(mean))\n\nxgb_params = {\n    'eta': 0.02,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.4,\n    'objective': 'reg:linear',    \n    'eval_metric': 'rmse',        \n    'silent': True,               \n    'n_estimators' : 100\n}\n\ntrain_y = np.log1p(train_y)","138726dd":"%%time\n# transforming\ndtrain = xgb.DMatrix(train, train_y)\ndtest = xgb.DMatrix(test)\n\n# cross validation\ncv_output = xgb.cv(xgb_params,\n                   dtrain,                        \n                   num_boost_round=10000,        \n                   early_stopping_rounds=200,    \n                   nfold=5,                      \n                   verbose_eval=200,             \n                   feval=rmse_exp,               \n                   maximize=False,\n                   show_stdv=False,\n                   seed = 1080\n                   )\n\n# scoring\nbest_rounds = cv_output.index.size\nscore = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n\nprint(f'\\nBest Rounds: {best_rounds}')\nprint(f'Best Score: {score}')\n\n# plotting\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n# cv_output[['train-rmse-mean', 'test-rmse-mean']].plot(ax=ax1)\n# ax1.set_title('RMSE_log', fontsize=20)\n# cv_output[['train-rmse_exp-mean', 'test-rmse_exp-mean']].plot(ax=ax2)\n# ax2.set_title('RMSE', fontsize=20)\n\n# plt.show()","a2ff68e7":"model = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\ny_pred = model.predict(dtest)\ny_pred_xgb = np.expm1(y_pred)\nprint(y_pred_xgb)","1569146c":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\") \nsub1 = sub.copy()\nsub1['price'] = y_pred_xgb","fabd6a31":"# \ud5c8\ud0dc\uba85\ub2d8 \ucee4\ub110\ucc38\uace0\ud558\uba70 \uc870\uae08 \ubc14\uafc8\n\n# \ub370\uc774\ud130 \uac00\uc838\uc624\uae30\ndef load_original_data():\n    train = pd.read_csv('..\/input\/train.csv', parse_dates=['date'])\n    test = pd.read_csv('..\/input\/test.csv', parse_dates=['date'], index_col='id')\n\n    train_copy = train.copy()\n    train_copy['data'] = 'train'\n    test_copy = test.copy()\n    test_copy['data'] = 'test'\n    test_copy['price'] = np.nan\n\n    # remove outlier\n    train_copy = train_copy[~((train_copy['sqft_living'] > 12000) & (train_copy['price'] < 3000000))].reset_index(drop=True)\n\n    # concat train, test data to preprocess\n    data = pd.concat([train_copy, test_copy], sort=False).reset_index(drop=True)\n    data = data[train_copy.columns]\n\n    # \ub0a0\uc9dc\ud53c\uccd0 \ub4dc\ub78d\uc804\uc5d0 \uc911\uc694\ud55c\uac70 \ucd94\uac00\n    data['Month'] = data['date'].dt.month\n    data['Year'] = data['date'].dt.year\n    \n    data.drop('date', axis=1, inplace=True)\n    data['zipcode'] = data['zipcode'].astype(str)\n\n    # fix skew feature\n    skew_columns = ['price']\n\n    for c in skew_columns:\n        data[c] = np.log1p(data[c])\n        \n    return data\n\nRANDOM_SEED = 1080\nnp.random.seed(RANDOM_SEED)\ndef rmse_exp(y_true, y_pred):\n    return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))\n\ndef train_test_split(data, do_ohe=True):\n    df = data.drop(['id','price','data'], axis=1).copy()\n    cat_cols = df.select_dtypes('object').columns\n    for col in cat_cols:\n        if do_ohe:\n            ohe_df = pd.get_dummies(df[[col]], prefix='ohe_'+col)\n            df.drop(col, axis=1, inplace=True)\n            df = pd.concat([df, ohe_df], axis=1)\n        else:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n\n    train_len = data[data['data'] == 'train'].shape[0]\n    X_train = df.iloc[:train_len]\n    X_test = df.iloc[train_len:]\n    y_train = data[data['data'] == 'train']['price']\n    \n    return X_train, X_test, y_train\n\n\ndef get_oof_lgb(X_train, y_train, X_test, lgb_param, verbose_eval=False, return_cv_score_only=False):\n\n    folds = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n    oof = np.zeros(len(X_train))\n    predictions = np.zeros(len(X_test))\n    feature_importance_df = pd.DataFrame()\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, y_train.values)):\n        if verbose_eval > 0: print(f'Fold : {fold_ + 1}')\n        trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n        val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n\n        num_round = 100000\n        clf = lgb.train(lgb_param, trn_data, num_round, valid_sets=[trn_data, val_data],\n                        verbose_eval=verbose_eval, early_stopping_rounds=200)\n        oof[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration)\n        predictions += clf.predict(X_test, num_iteration=clf.best_iteration) \/ folds.n_splits\n        \n        cv_fold_score = rmse_exp(y_train.iloc[val_idx], oof[val_idx])\n        \n        if verbose_eval > 0: print(f'Fold {fold_ + 1} \/ CV-Score: {cv_fold_score:.6f}')\n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = X_train.columns.tolist()\n        fold_importance_df['importance'] = clf.feature_importance('gain')\n        fold_importance_df['fold'] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    cv_score = rmse_exp(y_train, oof)\n    print(f'CV-Score: {cv_score:.6f}')\n    if return_cv_score_only: return cv_score\n    else: return oof, predictions, cv_score, feature_importance_df","1efbae1f":"data = load_original_data()\n\ncoord = data[['lat','long']]\npca = PCA(n_components=2)\npca.fit(coord)\n\ncoord_pca = pca.transform(coord)\n\ndata['coord_pca1'] = coord_pca[:, 0]\ndata['coord_pca2'] = coord_pca[:, 1]\n\n# \ud53c\uccd0\ucd94\uac00 \ndata['latlongsum'] = data['lat'] + data['long']\ndata['latlongsub'] = data['lat'] - data['long']\ndata.loc[data.yr_renovated==0,'yr_renovated'] = data[data.yr_renovated==0].yr_built\n\n# kmeans for lat, long\nkmeans = KMeans(n_clusters=32, random_state=RANDOM_SEED).fit(coord)\ncoord_cluster = kmeans.predict(coord)\ndata['coord_cluster'] = coord_cluster\ndata['coord_cluster'] = data['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))\n\nX_train, X_test, y_train = train_test_split(data, do_ohe = False)\nprint(X_train.shape, X_test.shape)\n\nlgb_param = {\n    'objective': 'regression',\n    'learning_rate': 0.05,\n    'num_leaves': 15,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'feature_fraction': 0.7,\n    'seed': RANDOM_SEED,\n    'metric': ['rmse'],\n}\n\noof, lgbm_pred, cv_score, fi_df = get_oof_lgb(X_train, y_train, X_test, lgb_param)","824413c1":"y_pred_lgbm = np.expm1(lgbm_pred)\nsub2 = pd.read_csv(\"..\/input\/sample_submission.csv\") \nsub2['price'] = y_pred_lgbm\npred_df = sub.copy()\npred_df['price'] = sub1['price']*0.55 + sub2['price']*0.45","1f5372ca":"def export(pred):\n    subm = pd.read_csv('..\/input\/sample_submission.csv')\n    subm['price'] = pred\n\n    subm_num = 0\n    subm_name = '.\/subm_{}.csv'.format(str(subm_num).zfill(3))\n\n    while os.path.isfile(subm_name):\n        subm_num += 1\n        subm_name = '.\/subm_{}.csv'.format(str(subm_num).zfill(3))\n\n    print(subm_name)\n    subm.to_csv(subm_name, index=False)","b873b3b3":"export(pred_df['price'])","5f13f8e0":"Private Leaderboard : 104865.07\nKernel \uc810\uc218 : 104659.15502"}}