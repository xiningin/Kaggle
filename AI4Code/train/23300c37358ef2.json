{"cell_type":{"ce19bf81":"code","72a22430":"code","17bf74c5":"code","ed927922":"code","8a431edb":"code","d1deaa77":"code","3811e0b2":"code","6a3e9f07":"code","1160a961":"code","db62e202":"code","4d5e829e":"code","d4615c2b":"code","ce22c213":"code","3b7f68c0":"code","6e974282":"code","5f617efa":"code","ff2bef91":"code","a35c2254":"code","b01e3bb2":"code","ad6830f4":"code","373f3303":"code","66400bf5":"code","1654af09":"code","43e5ef4d":"code","1e89499d":"code","91e2d438":"code","26d005f5":"code","718db91f":"code","05dc8b95":"code","4de5f711":"code","16d1a013":"code","e0d0eb28":"code","82b65444":"code","70390157":"code","41eef14a":"code","78d1c991":"code","55b18c88":"code","8da151c5":"code","3f83fb3f":"markdown","7021c879":"markdown","6f79ec90":"markdown","7ef542ca":"markdown","ffc2b0a4":"markdown","7a0cca6a":"markdown","7ef9f8ce":"markdown","6d5ce4c9":"markdown","edfd2724":"markdown","b242f14c":"markdown","86967baa":"markdown","df421693":"markdown","fd898ee5":"markdown","bbdc2d01":"markdown","25160503":"markdown","732b53f0":"markdown","84f22399":"markdown","c5f44ef9":"markdown","95f328d3":"markdown","61240c0c":"markdown","cc368735":"markdown","1802a0fc":"markdown","f4c96d51":"markdown","fe234561":"markdown","cae62a58":"markdown","d961c8a1":"markdown","0d2815e2":"markdown","a4532e8c":"markdown","e0e215a6":"markdown","f05ebc9e":"markdown","f2d61be7":"markdown","5605de76":"markdown","dc851da1":"markdown","2f90923b":"markdown","617c30fe":"markdown","7c051fdf":"markdown"},"source":{"ce19bf81":"import numpy as np \nimport pandas as pd\n\n# plots\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.ticker as ticker\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom numpy.polynomial.polynomial import polyfit\n\n# t-test\nfrom scipy import stats\n\n# regression - statsmodels\nimport statsmodels.regression.linear_model as sm\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# regression - sklearn\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# sqrt\nfrom math import sqrt\n\n# Word Cloud\nfrom wordcloud import WordCloud","72a22430":"data=pd.read_csv('..\/input\/data-science-jobs\/DataScienceJobs.csv')","17bf74c5":"# function for inserting a row to a location\ndef Insert_row_(row_number, df, row_value): \n    # Slice the upper half of the dataframe \n    df1 = df[0:row_number]   \n    # Store the result of lower half of the dataframe \n    df2 = df[row_number:]  \n    # Inser the row in the upper half dataframe \n    df1.loc[row_number]=row_value \n    # Concat the two dataframes \n    df_result = pd.concat([df1, df2]) \n    # Reassign the index labels \n    df_result.index = [*range(df_result.shape[0])] \n    # Return the updated dataframe \n    return df_result","ed927922":"# function for extracting keywords from a set of text (based on a list of keywords \"VarSet\")\ndef get_keyword(x, note, VarSet):\n   x_ = x.split(\" \") # split the text by space\n   keywords = []\n   try:\n      for word in x_:\n         if word + note in np.asarray(VarSet): \n            keywords.append(word + note)\n   except:\n      return -1\n\n   return keywords","8a431edb":"# function for creating dummy variable columns by a column's values\ndef dummycols(SeriesX):\n    global data\n    # create dummy columns by keywords\n    kwdummy = pd.get_dummies(SeriesX.apply(pd.Series).stack()).sum(level=0)\n    # transform those values >1 to 1 because we want just dummies\n    kwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n    # merge back the dummy columns to the main dataset\n    data = data.merge(kwdummy,left_index=True,right_index=True).replace(np.nan,0)\n    return data","d1deaa77":"# Consider the keyword a potential variable only when the keyword frequency > ssize\nssize = 30","3811e0b2":"# remove special characters and unify some word use\ndata['Job_title_2']= data['Job Title'].str.upper().replace('[^A-Za-z0-9]+', ' ',regex=True)\ndata['Job_title_2']= data['Job_title_2'].str.upper().replace(\n    ['\u00c2',' AND ',' WITH ','SYSTEMS','OPERATIONS','ANALYTICS','SERVICES','ENGINEERS','NETWORKS','GAMES','MUSICS','INSIGHTS','SOLUTIONS','JR ','MARKETS','STANDARDS','FINANCE','PRODUCTS','DEVELOPERS','SR ',' 2',' TO ',' 1',' 3'],\n    ['','','','SYSTEM','OPERATION','ANALYTIC','SERVICE','ENGINEER','NETWORK','GAME','MUSIC','INSIGHT','SOLUTION','JUNIOR ','MARKET','STANDARD','FINANCIAL','PRODUCT','DEVELOPER','SENIOR ',' II','',' I',' III'],regex=True)\n\n# unify some word use\ndata['Job_title_2']= data['Job_title_2'].str.upper().replace(\n    ['BUSINESS INTELLIGENCE','INFORMATION TECHNOLOGY','QUALITY ASSURANCE','USER EXPERIENCE','USER INTERFACE','DATA WAREHOUSE','DATA ANALYST','DATA BASE','DATA QUALITY','DATA GOVERNANCE','BUSINESS ANALYST','DATA MANAGEMENT','REPORTING ANALYST','BUSINESS DATA','SYSTEM ANALYST','DATA REPORTING','QUALITY ANALYST','DATA ENGINEER','BIG DATA','SOFTWARE ENGINEER','MACHINE LEARNING','FULL STACK','DATA SCIENTIST','DATA SCIENCE','DATA CENTER','ENTRY LEVEL','NEURAL NETWORK','SYSTEM ENGINEER',' ML '],\n    ['BI','IT','QA','UX','UI','DATA_WAREHOUSE','DATA_ANALYST','DATABASE','DATA_QUALITY','DATA_GOVERNANCE','BUSINESS_ANALYST','DATA_MANAGEMENT','REPORTING_ANALYST','BUSINESS_DATA','SYSTEM_ANALYST','DATA_REPORTING','QUALITY_ANALYST','DATA_ENGINEER','BIG_DATA','SOFTWARE_ENGINEER','MACHINE_LEARNING','FULL_STACK','DATA_SCIENTIST','DATA_SCIENCE','DATA_CENTER','ENTRY_LEVEL','NEURAL_NETWORK','SYSTEM_ENGINEER',' MACHINE_LEARNING '],regex=True)\n\n# unify some word use\ndata['Job_title_2']= data['Job_title_2'].str.upper().replace(\n    ['DATA_ENGINEER JUNIOR','DATA_ENGINEER SENIOR','DATA  REPORTING_ANALYST','DATA ','BIG_DATA '],\n    ['JUNIOR DATA_ENGINEER','SENIOR DATA_ENGINEER','DATA_REPORTING_ANALYST','DATA_','BIG_DATA_'],regex=True)\n\n# get the list of keywords that would be created as variables\ns = data['Job_title_2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\ns = s[(s['Count']>=ssize)]# Consider the keyword a potential variable if the keyword frequency > ssize\n\n# get keywords from each row\ndata['KW'] = data['Job_title_2'].apply(lambda x: get_keyword(x,'',s['KW']))\n\n# create dummy variable columns\ndata = dummycols(data['KW'])","6a3e9f07":"# Create a list of big data buzzwords to see if those words in JD would influence the salary\nbuzzwords = ['COMPUTER_SCIENCE','MASTER','MBA','SQL','PYTHON','R','PHD','BUSINESS_ANALYTICS','SAS','PMP','SCRUM_MASTER','STATISTICS','MATHEMATICS','MACHINE_LEARNING','ARTIFICIAL_INTELLIGENCE','ECONOMICS','TABEAU','AWS','AZURE','POWER_BI','ALGORITHM','DEEP_LEARNING','NEURAL_NETWORK','NATURAL_LANGUAGE_PROCESSING','DECISION_TREE','REGRESSION','CLUSTER','ORACLE','EXCEL','TENSORFLOW','HADOOP','SPARK','NOSQL','SAP','ETL','API','PLSQL','MONGODB','POSTGRESQL','ELASTICSEARCH','REDIS','MYSQL','FIREBASE','SQLITE','CASSANDRA','DYNAMODB','OLTP','OLAP','DEVOPS','PLATFORM','NETWORK','APACHE','SECURITY','MARKDOWN']","1160a961":"# remove special characters and unify some word use\ndata['Job_Desc2'] = data['Job Description'].replace('[^A-Za-z0-9]+', ' ',regex=True)\ndata['Job_Desc2'] = data['Job_Desc2'].str.upper().replace(\n    ['COMPUTER SCIENCE','ENGINEERING DEGREE',' MS ','BUSINESS ANALYTICS','SCRUM MASTER','MACHINE LEARNING',' ML ','POWER BI','ARTIFICIAL INTELLIGENCE',' AI ','ALGORITHMS','DEEP LEARNING','NEURAL NETWORK','NATURAL LANGUAGE PROCESSING','DECISION TREE','CLUSTERING','PL SQL'],\n    ['COMPUTER_SCIENCE','ENGINEERING_DEGREE',' MASTER ','BUSINESS_ANALYTICS','SCRUM_MASTER','MACHINE_LEARNING',' MACHINE_LEARNING ','POWER_BI','ARTIFICIAL_INTELLIGENCE',' ARTIFICIAL_INTELLIGENCE ','ALGORITHM','DEEP_LEARNING','NEURAL_NETWORK','NATURAL_LANGUAGE_PROCESSING','DECISION_TREE','CLUSTER','PLSQL'],regex=True)\n\n# Count the JD keywords.\nS2 = data['Job_Desc2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\nS2 = S2[S2['KW'].isin(buzzwords)].reset_index(drop=True)\nS2['KWJD'] = S2['KW'] + '_JD'\nS2 = S2[(S2['Count']>=ssize)]# Consider the keyword a potential variable if the keyword frequency > ssize\n\n# get keywords from each row\ndata['JDKW'] = data['Job_Desc2'].apply(lambda x: get_keyword(x,'_JD',S2['KWJD']))\n\n# create dummy variable columns\ndata = dummycols(data['JDKW'])","db62e202":"# let's see if number of buzzwords contained or how wordy the JD is would have impact.\ndata['JDKWlen']=data['JDKW'].str.len()\ndata['JDlen']=data['Job Description'].str.len()","4d5e829e":"# create dummy variable columns by State\ndata = dummycols(data['State'])\n\n# create dummy variable columns by City\ndata['City']=data['City'].str.replace(' ','_',regex=True)\ndata = dummycols(data['City'])","d4615c2b":"## State where companies Headquarters locate at  ##\n# Count the HQState frequency for t-test later\nS31 = data['HQState'].value_counts().reset_index().rename(\n    columns={'index':'HQState','HQState':'Count'}).replace(0,'Unknown_State')\nS31['HQState_HQ'] = S31['HQState'] + '_HQ'\n# Consider the keyword a potential variable if the keyword frequency > ssize\nS31 = S31[(S31['Count']>=ssize)&(S31['HQState_HQ']!=0)]\n\ndata['HQState_HQ'] = data[data['HQState']!=0]['HQState'].apply(lambda x: get_keyword(x,'_HQ',S31['HQState_HQ']))\n\n# create dummy variable columns\ndata = dummycols(data['HQState_HQ'])","ce22c213":"# create dummy columns by Revenue\ndata = dummycols(data['Revenue_USD'])\n\n# create dummy columns by Size\ndata = dummycols(data['Size'])\n\n# create dummy columns by Sector\ndata = dummycols(data['Sector'])\n\n# create dummy columns by Type of Ownership\ndata = dummycols(data['Type of ownership'])","3b7f68c0":"def splittingSets(df, y):\n    X = df.drop([y],axis=1)\n    y = df[y]\n\n    X_int, X_test, y_int, y_test = train_test_split(X,y,shuffle=False,test_size=0.2,random_state=15)\n    X_train, X_val, y_train, y_val = train_test_split(X_int,y_int,shuffle=False,test_size=0.25,random_state=15)\n\n    return X_train, X_val, X_test, y_train, y_val, y_test","6e974282":"def ttest(setX):\n    text_columns = list(X_train.columns)\n    ttests=[]\n    for word in text_columns:\n        if word in set(setX):\n            if sum(X_train[word]) >= ssize:\n                ttest = stats.ttest_ind(y_train[X_train[word]==1],\n                                        y_train[X_train[word]==0])\n                ttests.append([word,ttest])\n    ttests = pd.DataFrame(ttests,columns=['Var','R'])\n    ttests['R']=ttests['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\n    ttests['Statistic'],ttests['P-value']=ttests['R'].str.split(', ',1).str\n    ttests=ttests.drop(['R'],axis=1).sort_values('P-value',ascending=True)\n    return ttests","5f617efa":"def VarFromTtests(ttests_var,ttest_p):\n    ttest_var_pass = ttests_var[ttests_var['P-value'].astype(float)<ttest_p].reset_index(drop=True)\n    ttest_pass = list(ttest_var_pass['Var'])\n    \n    #build table for variables from ttest results\n    ttests_var_pass_KW = list(ttest_pass)\n    ttests_var_pass_KW.insert(0,'Const')\n    ttests_var_pass_KW = pd.DataFrame(ttests_var_pass_KW,columns=['Var'])\n\n    #list of variables to be put in the model\n    model_var = list(ttest_pass)\n    model_var.insert(0,'Const')\n\n    #list of variables removed\n    var_remove = []\n    \n    return ttests_var_pass_KW, model_var, var_remove, ttest_pass","ff2bef91":"def buildmodel(X_opt, var_name):\n    Mod = sm.OLS(endog = y_train, exog = X_opt).fit() \n    results_summary = Mod.summary(xname=var_name)\n    vif = pd.DataFrame()\n    vif[\"VIF\"] = [variance_inflation_factor(\n        sm.OLS(endog = y_train, exog = X_opt).exog, i) for i in range(\n        1, sm.OLS(endog = y_train, exog = X_opt).exog.shape[1])]\n    vif = Insert_row_(0, vif, np.nan)\n    return results_summary, vif, Mod","a35c2254":"def resulttable(results_summary):\n    results_as_html = results_summary.tables[1].as_html()\n    results_as_html = pd.read_html(results_as_html, header=0, index_col=0)[0]\n    result_data = pd.DataFrame(results_as_html)\n    return result_data","b01e3bb2":"def resulttables(results_summary, vif):\n    results_as_html0 = results_summary.tables[0].as_html()\n    results_as_html1 = results_summary.tables[1].as_html()\n    results_as_html2 = results_summary.tables[2].as_html()\n    results_as_html0 = pd.read_html(results_as_html0, header=0, index_col=0)[0]\n    results_as_html1 = pd.read_html(results_as_html1, header=0, index_col=0)[0]\n    results_as_html2 = pd.read_html(results_as_html2, header=0, index_col=0)[0]\n    result_data0 = pd.DataFrame(results_as_html0)\n    result_data1 = pd.DataFrame(results_as_html1).reset_index()\n    result_data1 = result_data1.merge(vif, right_index=True, left_index=True)\n    result_data2 = pd.DataFrame(results_as_html2)\n    return result_data0, result_data1, result_data2","ad6830f4":"# Create Backward Stepwise functions\ndef backwardeliminate(model_var, var_remove, ttest_pass_KW, ttest_pass):\n    for ele in model_var:\n        if ele in var_remove:\n            model_var.remove(ele)\n    ttest_pass_KW = ttest_pass_KW[ttest_pass_KW['Var'].isin(model_var)]\n    var_idx = ttest_pass_KW['Var'].index.tolist()\n    X_train0 = X_train[ttest_pass]\n    X_train1 = np.append(arr = np.ones((len(X_train0), 1)).astype(int),\n                         values = X_train0, axis = 1)\n    X_opt = X_train1[:,var_idx]\n    var_name = list(ttest_pass_KW['Var'])\n    return X_opt, var_name","373f3303":"def scalibility(FINAL_MODEL_NAME, Mod_Final_1, Mod_Final_2):\n    global modelperform\n    X_train0 = X_train[Mod_Final_2[Mod_Final_2['index']!='Const']['index']]\n    X_train1 = np.append(arr = np.ones((len(X_train0), 1)).astype(int),values = X_train0, axis = 1)\n\n    X_val0 = X_val[Mod_Final_2[Mod_Final_2['index']!='Const']['index']]\n    X_val1 = np.append(arr = np.ones((len(X_val0), 1)).astype(int),values = X_val0, axis = 1)\n\n    X_test0 = X_test[Mod_Final_2[Mod_Final_2['index']!='Const']['index']]\n    X_test1 = np.append(arr = np.ones((len(X_test0), 1)).astype(int),values = X_test0, axis = 1)\n        \n    new_row = {'Model': FINAL_MODEL_NAME,'Adj. R-squared': Mod_Final_1.iloc[0][2],\n               'AIC': Mod_Final_1.iloc[4][2],'BIC': Mod_Final_1.iloc[5][2],\n               'R2':r2_score(np.asarray(y_train),sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_train1)),\n               'RMSE':sqrt(mean_squared_error(np.asarray(y_train),\n                                       sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_train1))),\n               'R2_val':r2_score(np.asarray(y_val),sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_val1)),\n               'RMSE_val':sqrt(mean_squared_error(np.asarray(y_val),\n                                       sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_val1))),\n               'R2_test':r2_score(np.asarray(y_test),sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_test1)),\n               'RMSE_test':sqrt(mean_squared_error(np.asarray(y_test),\n                                       sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_test1)))}\n\n    pd.options.display.float_format = '{:.4f}'.format\n    modelperform = modelperform.append(new_row, ignore_index=True)\n    \n    return modelperform, X_train1, X_val1, X_test1","66400bf5":"def multicol(Coef_Final):\n    multivlsit = Coef_Final.index.tolist()\n    multivlsit.remove('Const')\n    corr_df = X_train[multivlsit].corr(method='pearson')\n    mask = np.zeros_like(corr_df)\n    mask[np.triu_indices_from(mask)] = True\n    \n    sns.set(style='white')\n    fig = plt.figure(figsize=(13, 13))\n    sns.heatmap(corr_df, cmap='RdYlGn_r', vmax = 1.0, vmin = -1.0, mask = mask, linewidths = 2.5)\n    plt.yticks(rotation = 0)\n    plt.xticks(rotation = 90)\n    plt.show()","1654af09":"def addTopInteractions():\n    global X_train, X_val, X_test, Interactions_Top\n    X_train0 = X_train[Mod_Final_2[Mod_Final_2['index']!='Const']['index']]\n    baseline = np.mean(cross_val_score(linear_model.LinearRegression(), X_train0, y_train, scoring='r2', cv=3, n_jobs=1))\n    interactions = list()\n    for feature_A in X_train0.columns:\n        for feature_B in X_train0.columns:\n            if feature_A > feature_B:\n                X_train0['interaction'] = X_train0[feature_A] * X_train0[feature_B]\n                score = np.mean(cross_val_score(linear_model.LinearRegression(), X_train0, y_train, scoring='r2',\n                                       cv=3, n_jobs=1))\n                if score > baseline:\n                    interactions.append((feature_A, feature_B, round(score,3)))\n\n    Interactions = pd.DataFrame(interactions)\n    Interactions = Interactions.sort_values(2,ascending=False).reset_index(drop=True)\n    Interactions_Top = Interactions.iloc[0:9,:]\n    Interactions_Top['inter'] = Interactions_Top[0]+'*'+Interactions_Top[1]\n\n    for i in Interactions_Top.index:\n        X_train[Interactions_Top.iloc[i]['inter']] = X_train[\n            Interactions_Top.iloc[i][0]]*X_train[Interactions_Top.iloc[i][1]]\n        X_val[Interactions_Top.iloc[i]['inter']] = X_val[\n            Interactions_Top.iloc[i][0]]*X_val[Interactions_Top.iloc[i][1]]\n        X_test[Interactions_Top.iloc[i]['inter']] = X_test[\n            Interactions_Top.iloc[i][0]]*X_test[Interactions_Top.iloc[i][1]]\n        if sum(X_train[Interactions_Top.iloc[i]['inter']]) < ssize:\n            X_train = X_train.iloc[:,0:-1]\n\n    X_val = X_val[X_train.columns]\n    X_test = X_test[X_test.columns]\n    return X_train, X_val, X_test, Interactions_Top","43e5ef4d":"def ElasticNetPlot(alpha_max, alpha_step, L1_wt_value):\n    ols_rmse_val=sqrt(mean_squared_error(np.asarray(y_val),\n                                       sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_val1)))\n\n    EN_rmse_val=[]\n    for alpha in np.arange(0, alpha_max, alpha_step):\n        # L1_wt: (0)Ridge - (1)Lasso\n        EN_rmse_val.append(sqrt(mean_squared_error(np.asarray(y_val),sm.OLS(\n            endog = y_train, exog = X_train1).fit_regularized(method='elastic_net', alpha=alpha, L1_wt = L1_wt_value).predict(X_val1))))\n\n    # plot rmse\n    fig, ax = plt.subplots(figsize=(13, 7))\n    plt.plot(np.arange(0, alpha_max, alpha_step), EN_rmse_val, 'ro')\n    plt.axhline(y=ols_rmse_val, color='g', linestyle='--')\n    plt.title(\"Elatic Net Validation RMSE\", fontsize=16)\n    plt.xlabel(\"Model Simplicity$\\longrightarrow$\")\n    plt.ylabel(\"RMSE\")\n    plt.show()","1e89499d":"def coefficientbars(alpha_value, L1_wt_value, Mod_Final_2):\n    mod_min_val_err = sm.OLS(endog = y_train, exog = X_train1).fit_regularized(method='elastic_net', alpha=alpha_value, L1_wt = L1_wt_value)\n    regularized_regression_parameters = mod_min_val_err.params.reset_index()\n\n    sns.set(style='white')\n    reg_fit = Mod_Final_2['index'].reset_index()\n    reg_fit = reg_fit.merge(regularized_regression_parameters, right_index=True, left_index=True).drop(\n        ['level_0','index_y'],axis=1).rename(columns={'index_x':'Var',0:'Coefficient'}).sort_values(\n        'Coefficient',ascending=False).reset_index(drop=True)\n    reg_fit = reg_fit[reg_fit['Var']!='Const']\n\n    fig = plt.figure(figsize=(13, 5))\n    sns.barplot(x='Var',y='Coefficient',data=reg_fit).set(xlabel=\"\",ylabel=\"Salary Performance \\n Against Average\")\n\n    plt.xticks(rotation=45,horizontalalignment='right')\n    plt.show()","91e2d438":"ssize = 30 # set minimum sample size for each variable\nttest_p = 0.1 # set p-value threshold for t-test\nregr_p = 0.05 # set p-value threshold for eliminating variables from regression\n\nX_train, X_val, X_test, y_train, y_val, y_test = splittingSets(data, 'Est_Salary')\nfinalVar=['Years_Founded','Rating','JDKWlen','JDlen','Easy_Apply']\n\n# Job Titles\nttests_title = ttest(s['KW'])\nttests_title_pass_KW, model_title, title_remove, ttests_title_pass = VarFromTtests(ttests_title, ttest_p)\nX_opt_title, title_name = backwardeliminate(model_title, title_remove, ttests_title_pass_KW, ttests_title_pass)\nMod_title, VIF_title, mod_title = buildmodel(X_opt_title, title_name)\nCoef_title = resulttable(Mod_title)\n\ni = max(Coef_title['P>|t|'])\nwhile i > regr_p:\n    if Coef_title[Coef_title['P>|t|']==i].index.values[0] != 'Const':\n        title_remove.append(Coef_title[Coef_title['P>|t|']==i].index.values[0])\n        X_opt_title, title_name = backwardeliminate(model_title, title_remove, ttests_title_pass_KW, ttests_title_pass)\n        Mod_title, VIF_title, mod_title = buildmodel(X_opt_title, title_name)\n        Coef_title = resulttable(Mod_title)\n        i = max(Coef_title['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_title['coef'].index.tolist()\nMod_title_1,Mod_title_2,Mod_title_3 = resulttables(Mod_title, VIF_title)\n\n# JD\nttests_JD = ttest(S2['KWJD']) \nttests_JD_pass_KW, model_JD, JD_remove, ttests_JD_pass = VarFromTtests(ttests_JD, ttest_p)\nX_opt_JD, JD_name = backwardeliminate(model_JD, JD_remove, ttests_JD_pass_KW, ttests_JD_pass)\nMod_JD, VIF_JD, mod_JD = buildmodel(X_opt_JD, JD_name)\nCoef_JD = resulttable(Mod_JD)\n\ni = max(Coef_JD['P>|t|'])\nwhile i > regr_p:\n    if Coef_JD[Coef_JD['P>|t|']==i].index.values[0] != 'Const':\n        JD_remove.append(Coef_JD[Coef_JD['P>|t|']==i].index.values[0])\n        X_opt_JD, JD_name = backwardeliminate(model_JD, JD_remove, ttests_JD_pass_KW, ttests_JD_pass)\n        Mod_JD, VIF_JD, mod_JD = buildmodel(X_opt_JD, JD_name)\n        Coef_JD = resulttable(Mod_JD)\n        i = max(Coef_JD['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_JD['coef'].index.tolist()\nMod_JD_1,Mod_JD_2,Mod_JD_3 = resulttables(Mod_JD, VIF_JD)\n\n# state\nttests_State = ttest(X_train[X_train['State']!=0]['State'])\nttests_State_pass_KW, model_State, State_remove, ttests_State_pass = VarFromTtests(ttests_State, ttest_p)\nX_opt_State, State_name = backwardeliminate(model_State, State_remove, ttests_State_pass_KW, ttests_State_pass)\nMod_State, VIF_State, mod_State = buildmodel(X_opt_State, State_name)\nCoef_State = resulttable(Mod_State)\n\ni = max(Coef_State['P>|t|'])\nwhile i > regr_p:\n    if Coef_State[Coef_State['P>|t|']==i].index.values[0] != 'Const':\n        State_remove.append(Coef_State[Coef_State['P>|t|']==i].index.values[0])\n        X_opt_State, State_name = backwardeliminate(model_State, State_remove, ttests_State_pass_KW, ttests_State_pass)\n        Mod_State, VIF_State, mod_State = buildmodel(X_opt_State, State_name)\n        Coef_State = resulttable(Mod_State)\n        i = max(Coef_State['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_State['coef'].index.tolist()\nMod_State_1,Mod_State_2,Mod_State_3 = resulttables(Mod_State, VIF_State)\n\n# city\nttests_city = ttest(X_train[X_train['City']!=0]['City'])\nttests_city_pass_KW, model_city, city_remove, ttests_city_pass = VarFromTtests(ttests_city, ttest_p)\nX_opt_city, city_name = backwardeliminate(model_city, city_remove, ttests_city_pass_KW, ttests_city_pass)\nMod_city, VIF_city, mod_city = buildmodel(X_opt_city, city_name)\nCoef_city = resulttable(Mod_city)\n\ni = max(Coef_city['P>|t|'])\nwhile i > regr_p:\n    if Coef_city[Coef_city['P>|t|']==i].index.values[0] != 'Const':\n        city_remove.append(Coef_city[Coef_city['P>|t|']==i].index.values[0])\n        X_opt_city, city_name = backwardeliminate(model_city, city_remove, ttests_city_pass_KW, ttests_city_pass)\n        Mod_city, VIF_city, mod_city = buildmodel(X_opt_city, city_name)\n        Coef_city = resulttable(Mod_city)\n        i = max(Coef_city['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_city['coef'].index.tolist()\nMod_city_1,Mod_city_2,Mod_city_3 = resulttables(Mod_city, VIF_city)\n\n# HQ state\nttests_HQState = ttest(S31['HQState_HQ'])\nttests_HQState_pass_KW, model_HQState, HQState_remove, ttests_HQState_pass = VarFromTtests(ttests_HQState, ttest_p)\nX_opt_HQState, HQState_name = backwardeliminate(model_HQState, HQState_remove, ttests_HQState_pass_KW, ttests_HQState_pass)\nMod_HQState, VIF_HQState, mod_HQState = buildmodel(X_opt_HQState, HQState_name)\nCoef_HQState = resulttable(Mod_HQState)\n\ni = max(Coef_HQState['P>|t|'])\nwhile i > regr_p:\n    if Coef_HQState[Coef_HQState['P>|t|']==i].index.values[0] != 'Const':\n        HQState_remove.append(Coef_HQState[Coef_HQState['P>|t|']==i].index.values[0])\n        X_opt_HQState, HQState_name = backwardeliminate(model_HQState, HQState_remove, ttests_HQState_pass_KW, ttests_HQState_pass)\n        Mod_HQState, VIF_HQState, mod_HQState = buildmodel(X_opt_HQState, HQState_name)\n        Coef_HQState = resulttable(Mod_HQState)\n        i = max(Coef_HQState['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_HQState['coef'].index.tolist()\nMod_HQState_1,Mod_HQState_2,Mod_HQState_3 = resulttables(Mod_HQState, VIF_HQState)\n\n# Revenue\nttests_Revenue = ttest(X_train[X_train['Revenue_USD']!=0]['Revenue_USD'])\nttests_Revenue_pass_KW, model_Revenue, Revenue_remove, ttests_Revenue_pass = VarFromTtests(ttests_Revenue, ttest_p)\nX_opt_Revenue, Revenue_name = backwardeliminate(model_Revenue, Revenue_remove, ttests_Revenue_pass_KW, ttests_Revenue_pass)\nMod_Revenue, VIF_Revenue, mod_Revenue = buildmodel(X_opt_Revenue, Revenue_name)\nCoef_Revenue = resulttable(Mod_Revenue)\n\ni = max(Coef_Revenue['P>|t|'])\nwhile i > regr_p:\n    if Coef_Revenue[Coef_Revenue['P>|t|']==i].index.values[0] != 'Const':\n        Revenue_remove.append(Coef_Revenue[Coef_Revenue['P>|t|']==i].index.values[0])\n        X_opt_Revenue, Revenue_name = backwardeliminate(model_Revenue, Revenue_remove, ttests_Revenue_pass_KW, ttests_Revenue_pass)\n        Mod_Revenue, VIF_Revenue, mod_Revenue = buildmodel(X_opt_Revenue, Revenue_name)\n        Coef_Revenue = resulttable(Mod_Revenue)\n        i = max(Coef_Revenue['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_Revenue['coef'].index.tolist()\nMod_Revenue_1,Mod_Revenue_2,Mod_Revenue_3 = resulttables(Mod_Revenue, VIF_Revenue)\n\n# Size\nttests_Size = ttest(X_train[X_train['Size']!=0]['Size'])\nttests_Size_pass_KW, model_Size, Size_remove, ttests_Size_pass = VarFromTtests(ttests_Size, ttest_p)\nX_opt_Size, Size_name = backwardeliminate(model_Size, Size_remove, ttests_Size_pass_KW, ttests_Size_pass)\nMod_Size, VIF_Size, mod_Size = buildmodel(X_opt_Size, Size_name)\nCoef_Size = resulttable(Mod_Size)\n\ni = max(Coef_Size['P>|t|'])\nwhile i > regr_p:\n    if Coef_Size[Coef_Size['P>|t|']==i].index.values[0] != 'Const':\n        Size_remove.append(Coef_Size[Coef_Size['P>|t|']==i].index.values[0])\n        X_opt_Size, Size_name = backwardeliminate(model_Size, Size_remove, ttests_Size_pass_KW, ttests_Size_pass)\n        Mod_Size, VIF_Size, mod_Size = buildmodel(X_opt_Size, Size_name)\n        Coef_Size = resulttable(Mod_Size)\n        i = max(Coef_Size['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_Size['coef'].index.tolist()\nMod_Size_1,Mod_Size_2,Mod_Size_3 = resulttables(Mod_Size, VIF_Size)\n\n# Sector\nttests_Sector = ttest(X_train[X_train['Sector']!=0]['Sector'])\nttests_Sector_pass_KW, model_Sector, Sector_remove, ttests_Sector_pass = VarFromTtests(ttests_Sector, ttest_p)\nX_opt_Sector, Sector_name = backwardeliminate(model_Sector, Sector_remove, ttests_Sector_pass_KW, ttests_Sector_pass)\nMod_Sector, VIF_Sector, mod_Sector = buildmodel(X_opt_Sector, Sector_name)\nCoef_Sector = resulttable(Mod_Sector)\n\ni = max(Coef_Sector['P>|t|'])\nwhile i > regr_p:\n    if Coef_Sector[Coef_Sector['P>|t|']==i].index.values[0] != 'Const':\n        Sector_remove.append(Coef_Sector[Coef_Sector['P>|t|']==i].index.values[0])\n        X_opt_Sector, Sector_name = backwardeliminate(model_Sector, Sector_remove, ttests_Sector_pass_KW, ttests_Sector_pass)\n        Mod_Sector, VIF_Sector, mod_Sector = buildmodel(X_opt_Sector, Sector_name)\n        Coef_Sector = resulttable(Mod_Sector)\n        i = max(Coef_Sector['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_Sector['coef'].index.tolist()\nMod_Sector_1,Mod_Sector_2,Mod_Sector_3 = resulttables(Mod_Sector, VIF_Sector)\n\n# Type of Ownership\nttests_Own = ttest(X_train[X_train['Type of ownership']!=0]['Type of ownership'])\nttests_Own_pass_KW, model_Own, Own_remove, ttests_Own_pass = VarFromTtests(ttests_Own, ttest_p)\nX_opt_Own, Own_name = backwardeliminate(model_Own, Own_remove, ttests_Own_pass_KW, ttests_Own_pass)\nMod_Own, VIF_Own, mod_Own = buildmodel(X_opt_Own, Own_name)\nCoef_Own = resulttable(Mod_Own)\n\ni = max(Coef_Own['P>|t|'])\nwhile i > regr_p:\n    if Coef_Own[Coef_Own['P>|t|']==i].index.values[0] != 'Const':\n        Own_remove.append(Coef_Own[Coef_Own['P>|t|']==i].index.values[0])\n        X_opt_Own, Own_name = backwardeliminate(model_Own, Own_remove, ttests_Own_pass_KW, ttests_Own_pass)\n        Mod_Own, VIF_Own, mod_Own = buildmodel(X_opt_Own, Own_name)\n        Coef_Own = resulttable(Mod_Own)\n        i = max(Coef_Own['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_Own['coef'].index.tolist()\nMod_Own_1,Mod_Own_2,Mod_Own_3 = resulttables(Mod_Own, VIF_Own)\n\n# Final Combined Model\nwhile 'Const' in finalVar:\n    finalVar.remove('Const')\n\n#reset the table for variables\nttest_Final_pass_KW = list(finalVar)\nttest_Final_pass_KW.insert(0,'Const')\nttest_Final_pass_KW = pd.DataFrame(ttest_Final_pass_KW,columns=['Var'])\n\n#reset list of variables to be put in the model\nmodel_Final = list(finalVar)\nmodel_Final.insert(0,'Const')\n\nFinal_remove = ['Jacksonville','NY'] # default removed due to Multicollinearity\n\nX_opt_Final, Final_name = backwardeliminate(model_Final, Final_remove, ttest_Final_pass_KW, finalVar)\nMod_Final, VIF_Final, mod_Final = buildmodel(X_opt_Final, Final_name)\nCoef_Final = resulttable(Mod_Final)\n\ni = max(Coef_Final['P>|t|'])\nwhile i > regr_p:\n    if Coef_Final[Coef_Final['P>|t|']==i].index.values[0] != 'Const':\n        Final_remove.append(Coef_Final[Coef_Final['P>|t|']==i].index.values[0])\n        X_opt_Final, Final_name = backwardeliminate(model_Final, Final_remove, ttest_Final_pass_KW, finalVar)\n        Mod_Final, VIF_Final, mod_Final = buildmodel(X_opt_Final, Final_name)\n        Coef_Final = resulttable(Mod_Final)\n        i = max(Coef_Final['P>|t|'])\n    else:\n        continue\n\nMod_Final_1,Mod_Final_2,Mod_Final_3 = resulttables(Mod_Final, VIF_Final)","26d005f5":"Mod_Final","718db91f":"modelperform = pd.DataFrame(\n        columns = [\n            'Model','Adj. R-squared','AIC','BIC','R2','RMSE','R2_val','RMSE_val','R2_test','RMSE_test'])\n\nmodelperform, X_train1, X_val1, X_test1 = scalibility('Combined Model',Mod_Final_1, Mod_Final_2)\nmodelperform","05dc8b95":"ElasticNetPlot(alpha_max = 0.05, alpha_step = 0.001, L1_wt_value = 0)\n#L1 value = 0 -> Ridge Regression\n#L1 value = 1 -> Lasso Regression","4de5f711":"coefficientbars(alpha_value = 0.012, L1_wt_value = 1, Mod_Final_2 = Mod_Final_2)","16d1a013":"X_train, X_val, X_test, Interactions_Top = addTopInteractions()","e0d0eb28":"# Add interaction terms to regression's variable list\nmodel_var_intr = Interactions_Top[Interactions_Top['inter'].isin(X_train.columns)]['inter']\nFinalVar_intr = list(Mod_Final_2['index']) + list(model_var_intr)\nFinalVar_intr2 = list(Mod_Final_2[Mod_Final_2['index']!='Const']['index']) + list(model_var_intr)\nFinalVar_intr_table = pd.DataFrame(FinalVar_intr,columns=['Var'])","82b65444":"intr_remove = []\n\nX_opt_Final_t, Final_name_t = backwardeliminate(FinalVar_intr, intr_remove, FinalVar_intr_table, FinalVar_intr2)\nMod_Final_t, VIF_Final_t, mod_Final_t = buildmodel(X_opt_Final_t, Final_name_t)\nCoef_Final_t = resulttable(Mod_Final_t)\n\ni = max(Coef_Final_t[Coef_Final_t.index.isin(model_var_intr)]['P>|t|'])\nwhile i > regr_p:\n    intr_remove.append(Coef_Final_t.index[Coef_Final_t['P>|t|']==i].values[0])\n    X_opt_Final_t, Final_name_t = backwardeliminate(\n        FinalVar_intr, intr_remove, FinalVar_intr_table, FinalVar_intr2)\n    Mod_Final_t, VIF_Final_t, mod_Final_t = buildmodel(X_opt_Final_t, Final_name_t)\n    Coef_Final_t = resulttable(Mod_Final_t)\n    i = max(Coef_Final_t[Coef_Final_t.index.isin(model_var_intr)]['P>|t|'])\n\nMod_Final_1_t,Mod_Final_2_t,Mod_Final_3_t = resulttables(Mod_Final_t, VIF_Final_t)\nMod_Final_t","70390157":"modelperform, X_train1, X_val1, X_test1 = scalibility('Combined Model (Tuned)',Mod_Final_1_t,Mod_Final_2_t)\nmodelperform","41eef14a":"ElasticNetPlot(alpha_max = 0.05, alpha_step = 0.001, L1_wt_value = 0)","78d1c991":"coefficientbars(alpha_value = 0.01, L1_wt_value = 1, Mod_Final_2 = Mod_Final_2_t)","55b18c88":"multicol(Coef_Final)","8da151c5":"plt_coef = Mod_Final_2[Mod_Final_2['index']!='Const'].sort_values(\n    ['coef','std err'],ascending=True).reset_index(drop=True)\n\nsns.set()\nfig = plt.figure(figsize=(13, 13))\nplt.errorbar(y = plt_coef.iloc[:,0], x = plt_coef.iloc[:,1], \n             xerr = plt_coef.iloc[:,2]*2, fmt='o', color='b')\nplt.axvline(x=0, color='r',linestyle='--')\nplt.title(\"Coefficients with Errors [0.025 - 0.975]\",fontsize='15')\nplt.xlabel('Reference: USD 87.46K', color='r')\nplt.xticks(np.arange(-50, 50, step=10))\nplt.show()","3f83fb3f":"## Tune the first combined model with Elastic Net <a id=\"EN1\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","7021c879":"## Remove variables by p-values in regression results (Backward-stepwise). <a id=\"method5\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","6f79ec90":"## First Combined Regression Model Output <a id=\"model1\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","7ef542ca":"Doing nothing, we return to the first model ('Combined Model') and have a final look at its Multicollinearity although the regression output summary suggests it not severe. <a id=\"multi\"><\/a>","ffc2b0a4":"Though the second model which the interaction terms are introduced has slightly higher R-squared and lower errors in the training set, both scores in validation and test sets are worse. This shows the second model might have made things too complicated and overfit the training data.","7a0cca6a":"Elastic Net Regression suggests that the validation error of this model is close to optimal but could be lower if alpha increases to 0.006 or more, implying some variable removal may make the model better.","7ef9f8ce":"### Get More Variables: Revenue, Size, Sector and Type of Ownership","6d5ce4c9":"## Define Data-wrangling Functions <a id=\"1-2\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","edfd2724":"## Fit and output regression results. <a id=\"method4\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","b242f14c":"# Define Regression Methods\/Functions <a id=\"method\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","86967baa":"## Data Science Job Salary Impact Map (Final Regression Result) <a id=\"final\"><\/a> \n<a href=\"#Top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","df421693":"# Data Preparation <a id=\"1\"><\/a>","fd898ee5":"## Final Model (Tuned) <a id=\"finaltab\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","bbdc2d01":"### Get Keyword Variables from Job Titles","25160503":"However, even with Lasso at alpha=0.012, no variables is necessariliy to be removed. Nevertheless, let's introduce some relevant interaction terms to see if it can get us more explainability.","732b53f0":"## Introduce interaction terms to the first combined model <a id=\"int\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","84f22399":"Some variables have higher correlations, but mostly are just like City vs State. Other interesting correlations are (1) positive correlation between 'PHD' mentioned in job description and data scientist as job title; (2) negative correlation between 'PHD' and Data Analyst. This may imply that PHD's are more likely to become a data scientist than become a data analyst. On the other hand, 'Spark' seems to appear more in job description for data engineers rather than data analysts. ","c5f44ef9":"## Prepare Variables for Regression Analysis <a id=\"1-3\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","95f328d3":"# Table of Contents<a id=\"TOC\"><\/a>\n* [About (Architecture of this Backward-stepwise Regression solution)](#about)\n* [Prepare Data](#1)\n    * [Import Libraries and Datasets](#1-1)\n    * [Define Data Wrangling Functions](#1-2)\n    * [Prepare Variables for Regression Analysis](#1-3)\n* [Methodology (Define Methods and Functions)](#method)\n    * [Split into training, validation and testing datasets](#method1)\n    * [Perform T-tests on all categorical variables created](#method2)\n    * [Select variables by the p-values of T-test results](#method3)\n    * [Fit and output regression results](#method4)\n    * [Remove variables by p-values in regression results (Backward-stepwise)](#method5)\n    * [Compare Models' performance (AIC\/BIC, R-squared, RMSE)](#method6)\n    * [Detect Multicollinearity with Heatmap](#method7)\n    * [Introduce interaction terms](#method8)\n    * [(Model Tuning) Plot validation errors by Elastic Net](#method9)\n    * [(Model Tuning) Decide which variable to drop to minimize errors.](#method10)\n* [Analysis Process (Codes)](#ap)\n* [First Combined Regression Model Output](#model1)\n* [Model Tuning](#tune1)\n    * [Tune the first combined model with Elastic Net](#EN1)\n    * [Introduce interaction terms](#int)\n    * [Tuned Model Output](#finaltab)\n    * [Multicollinearity](#multi)\n* [Final Regression Result in Visualisation](#final)","61240c0c":"### Get Location Variables","cc368735":"## Select variables by the p-values of T-test results. <a id=\"method3\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","1802a0fc":"# Upvote if you like my work!","f4c96d51":"## Split into training, validation and testing datasets. <a id=\"method1\"><\/a>","fe234561":"# Model Tuning <a id=\"tune1\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","cae62a58":"# Analysis Process (Codes) <a id=\"ap\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","d961c8a1":"## Perform T-tests on all categorical variables created. <a id=\"method2\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","0d2815e2":"## Detect Multicollinearity with Heatmap. <a id=\"method7\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","a4532e8c":"## (Model Tuning) Decide which variable to drop to minimize errors. <a id=\"method10\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","e0e215a6":"# About <a id=\"about\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>\n\n\n## Methodology: Backward-stepwise Regression\nThis Backward-stepwise Regression algorithm is designed for the scenario that**, within a limited time, needs to run regression analysis with the dataset containing lots of categorical data as independent variables.** Key steps:\n\n1. Categorical variable groups: for example, city, state, job title keywords...etc.\n2. Create dummy variables for each value whose presence in the dataset is at least 30.\n3. After splitting training\/validating\/testing datasets, perform T-tests and remove variables by p-value threshold 0.1.\n4. Perform backward-stepwise regression (p-value threshold 0.05) on variables that passed the T-tests in each variable group.\n5. Include all the variables having passed the preliminary regression models and re-run backward-stepwise regression.\n6. Plot the validation errors by Elastic Net alpha's to see how to tune the model (remove or add variables). At the same time, observe multicollinearity, consider interaction terms...etc.\n7. Compare the models and make the final decision.\n\n![image.png](attachment:image.png)\n\n## Example Dataset\nThis dataset was created by [picklesueat](https:\/\/github.com\/picklesueat\/data_jobs_data) and contains 12,000+ job listing for data science jobs (all assumed to be open positions at the time the dataset was published in July 2020), with features such as:\n\n* Salary Estimate\n* Location\n* Company Rating\n* Job Description\n  and more.\n\n## Objectives\n* What kind of Data Science jobs get higher salaries? (Job Title, Job Description, EasyApply)\n* What kind of companies pay more? (Rating, Company, Size, *Years established (now - Founded)*, Type of ownership, Industry & Sector, Revenue)\n* Does job\/headquarters location matter to salaries?","f05ebc9e":"Break the regression summary into different tables.","f2d61be7":"## Introduce interaction terms. <a id=\"method8\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","5605de76":"## Import Libraries and Dataset <a id=\"1-1\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","dc851da1":"### Get Keyword Variables from Job Descriptions","2f90923b":"Still the Elastic Net shows errors can be reduced when alpha is tuned toward 0.01. The Coefficient Bars below shows that only 'IL-HQ' and 'Palo Alto' can be removed, but we actually can't because it needs to stay there as a reference to other interaction terms.","617c30fe":"## Compare Models' performance (AIC\/BIC, R-squared, RMSE) <a id=\"method6\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","7c051fdf":"## (Model Tuning) Plot validation errors by Elastic Net. <a id=\"method9\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>"}}