{"cell_type":{"4c089c7c":"code","239a6405":"code","e6f6f724":"code","ce27722d":"code","15e9d1ae":"code","e6c16cb4":"code","604d912e":"code","90799fba":"code","4ff1c156":"code","58ef781d":"code","2b1ff3b0":"code","636e6db2":"code","a0c554bc":"code","4c61f3b7":"code","719213da":"code","ea3e45b7":"code","ee9ec549":"code","872f50bd":"code","c199bc71":"code","749703ff":"code","0b8ed3cc":"markdown","126b12be":"markdown","5ffe39bd":"markdown","7450efd3":"markdown","a6b49b72":"markdown","61c4689c":"markdown","1ad1e0b0":"markdown","c0572c2a":"markdown","8558c521":"markdown","875fc5a5":"markdown","0e8b6a0c":"markdown"},"source":{"4c089c7c":"# Something is weird going on with the latest version of pip\n# we need to do this import or else import gensim (which is done \n# within pyrdf2vec) crashes after the pip install\nimport gensim","239a6405":"!python -m pip install --upgrade pip\n!pip install pyrdf2vec --use-feature=2020-resolver","e6f6f724":"import warnings\nwarnings.filterwarnings('ignore')","ce27722d":"from pyrdf2vec.graphs import KG\nimport rdflib\nimport pandas as pd","15e9d1ae":"# Load CSV file with country names and their labels\ncountry_data = pd.read_csv('..\/input\/dbpedia-country-information\/countries.csv')\nentities = country_data['Country']\n\n# We manually remove a few countries for which querying went wrong...\nentities = list(set(entities) - {\n  'http:\/\/dbpedia.org\/resource\/Antigua_and_Barbuda',\n 'http:\/\/dbpedia.org\/resource\/Belize',\n 'http:\/\/dbpedia.org\/resource\/Bosnia_and_Herzegovina',\n 'http:\/\/dbpedia.org\/resource\/Botswana',\n 'http:\/\/dbpedia.org\/resource\/Chad',\n 'http:\/\/dbpedia.org\/resource\/China',\n 'http:\/\/dbpedia.org\/resource\/Georgia',\n 'http:\/\/dbpedia.org\/resource\/Guinea',\n 'http:\/\/dbpedia.org\/resource\/Ireland',\n 'http:\/\/dbpedia.org\/resource\/Malta',\n 'http:\/\/dbpedia.org\/resource\/Mexico',\n 'http:\/\/dbpedia.org\/resource\/Trinidad_and_Tobago',\n 'http:\/\/dbpedia.org\/resource\/Uganda'\n })\n\n\n# We will exclude triples (s, p, o) with p in label_predicates from our KG\n# as these do not carry any useful information.\nlabel_predicates = [\n     'http:\/\/dbpedia.org\/ontology\/abstract',\n     'http:\/\/dbpedia.org\/ontology\/flag',\n     'http:\/\/dbpedia.org\/ontology\/thumbnail',\n     'http:\/\/dbpedia.org\/ontology\/wikiPageExternalLink',\n     'http:\/\/dbpedia.org\/ontology\/wikiPageID',\n     'http:\/\/dbpedia.org\/ontology\/wikiPageRevisionID',\n     'http:\/\/dbpedia.org\/ontology\/wikiPageWikiLink',\n     'http:\/\/dbpedia.org\/property\/flagCaption',\n     'http:\/\/dbpedia.org\/property\/float',\n     'http:\/\/dbpedia.org\/property\/footnoteA',\n     'http:\/\/dbpedia.org\/property\/footnoteB',\n     'http:\/\/dbpedia.org\/property\/footnoteC',\n     'http:\/\/dbpedia.org\/property\/source',\n     'http:\/\/dbpedia.org\/property\/width',\n     'http:\/\/purl.org\/dc\/terms\/subject',\n     'http:\/\/purl.org\/linguistics\/gold\/hypernym',\n     'http:\/\/purl.org\/voc\/vrank#hasRank',\n     'http:\/\/www.georss.org\/georss\/point',\n     'http:\/\/www.w3.org\/2000\/01\/rdf-schema#comment',\n     'http:\/\/www.w3.org\/2000\/01\/rdf-schema#label',\n     'http:\/\/www.w3.org\/2000\/01\/rdf-schema#seeAlso',\n     'http:\/\/www.w3.org\/2002\/07\/owl#sameAs',\n     'http:\/\/www.w3.org\/2003\/01\/geo\/wgs84_pos#geometry',\n     'http:\/\/dbpedia.org\/ontology\/wikiPageRedirects',\n     'http:\/\/www.w3.org\/2003\/01\/geo\/wgs84_pos#lat',\n     'http:\/\/www.w3.org\/2003\/01\/geo\/wgs84_pos#long',\n     'http:\/\/www.w3.org\/2004\/02\/skos\/core#exactMatch',\n     'http:\/\/www.w3.org\/ns\/prov#wasDerivedFrom',\n     'http:\/\/xmlns.com\/foaf\/0.1\/depiction',\n     'http:\/\/xmlns.com\/foaf\/0.1\/homepage',\n     'http:\/\/xmlns.com\/foaf\/0.1\/isPrimaryTopicOf',\n     'http:\/\/xmlns.com\/foaf\/0.1\/name',\n     'http:\/\/dbpedia.org\/property\/website',\n     'http:\/\/dbpedia.org\/property\/west',\n     'http:\/\/dbpedia.org\/property\/wordnet_type',\n     'http:\/\/www.w3.org\/2002\/07\/owl#differentFrom',\n]\n\n# KG Loading Alternative 1: Loading the entire turtle file into memory\nkg = KG(\"..\/input\/dbpedia-country-information\/countries.ttl\", file_type='turtle',\n        label_predicates=[rdflib.URIRef(x) for x in label_predicates])\n\n# KG Loading Alternative 2: Using a dbpedia endpoint (nothing is loaded into memory)\n# kg = KG(\"https:\/\/dbpedia.org\/sparql\", is_remote=True,\n#         label_predicates=[rdflib.URIRef(x) for x in label_predicates])","e6c16cb4":"# Make sure that every entity can be found in our KG\nfiltered_entities = [e for e in entities if e in kg._entities]\nnot_found = set(entities) - set(filtered_entities)\nprint(f'{len(not_found)} entities could not be found in the KG! Removing them...')\nentities = filtered_entities","604d912e":"from pyrdf2vec import RDF2VecTransformer","90799fba":"transformer = RDF2VecTransformer()\nwalk_embeddings = transformer.fit(kg, entities, verbose=True).transform(entities)","4ff1c156":"!pip install adjustText\n!pip install matplotlib==3.1.3","58ef781d":"from sklearn.manifold import TSNE\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom adjustText import adjust_text","2b1ff3b0":"walk_tsne = TSNE(random_state=42)\nX_tsne = walk_tsne.fit_transform(walk_embeddings)","636e6db2":"plt.figure(figsize=(15, 15))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=100)\n\ntexts = []\nfor x, y, lab in zip(X_tsne[:, 0], X_tsne[:, 1], entities):\n    lab = lab.split('\/')[-1]\n    text = plt.text(x, y, lab)\n    texts.append(text)\n    \nadjust_text(texts, lim=5, arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\nplt.show()","a0c554bc":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport numpy as np\n\ndef classify(walk_embeddings):\n    for task in ['Research Rating', 'Inflation Rating', 'Continent']:\n\n        # Split our data into train and test (50\/50 split)\n        data = train_test_split(country_data['Country'], country_data[task], \n                                stratify=country_data[task], test_size=0.5,\n                                random_state=42)\n        train_ent, test_ent, y_train, y_test = data\n\n        # Create masks that filter out all entities that are INCLUDED in the KG.\n        train_mask = [x in entities for x in train_ent]\n        test_mask = [x in entities for x in test_ent]\n        y_train = y_train[train_mask]\n        y_test = y_test[test_mask]\n\n        # Create our X_train and X_test which consists out of the created embeddings\n        X_train = []\n        X_test = []\n        for entity in train_ent:\n            if entity in entities:\n                X_train.append(walk_embeddings[entities.index(entity)])\n        for entity in test_ent:\n            if entity in entities:\n                X_test.append(walk_embeddings[entities.index(entity)])\n        X_train = np.array(X_train)\n        X_test = np.array(X_test)\n\n        # Fit a Random Forest & tune some of its hyper-parameters\n        rf = GridSearchCV(RandomForestClassifier(random_state=42), \n                          {'n_estimators': [10, 50, 100], 'max_depth': [3, 5, None]},\n                          cv=10)\n        rf.fit(X_train, y_train)\n        preds = rf.predict(X_test)\n\n        # Evaluate our model on the test data\n        print(task)\n        print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n        print('Accuracy =', accuracy_score(y_test, preds))\n        print(confusion_matrix(y_test, preds))\n        print()","4c61f3b7":"classify(walk_embeddings)","719213da":"from pyrdf2vec.embedders import Word2Vec\nfrom pyrdf2vec.walkers import RandomWalker","ea3e45b7":"transformer = RDF2VecTransformer(walkers=[RandomWalker(3, None)], \n                                 embedder=Word2Vec(size=500))\nwalk_embeddings = transformer.fit(kg, entities, verbose=True).transform(entities)","ee9ec549":"classify(walk_embeddings)","872f50bd":"from pyrdf2vec.walkers import CommunityWalker, HalkWalker","c199bc71":"transformer = RDF2VecTransformer(walkers=[HalkWalker(3, None, freq_thresholds=[0.01]),\n                                          CommunityWalker(3, None, resolution=0.1, \n                                                          hop_prob=0.25)],\n                                 embedder=Word2Vec(size=500))\nwalk_embeddings = transformer.fit(kg, entities, verbose=True).transform(entities)","749703ff":"classify(walk_embeddings)","0b8ed3cc":"# Representing Data With Knowledge Graphs\n\n**Graphs** are data structures that are useful to represent ubiquitous phenomena, such as social networks, chemical molecules and recommendation systems. One of their strengths lies in the fact that they explicitly model relations (i.e. edges) between individual units (i.e. nodes), which adds an extra dimension to the data.\n\nWe can illustrate the added value of this data enrichment using the [Cora citation network](https:\/\/relational.fit.cvut.cz\/dataset\/CORA). This dataset contains a bag-of-words representation for a few hundred papers and the citation relations between each of these papers. If we apply dimensionality reduction (t-SNE) to create a 2D plot of the bag-of-words representations (Figure, left), we can see clusters (they are colored according to their research topic) arise but they overlap. If we produce an embedding with a graph network (Figure, right), that takes into account the citation information, we can see the clusters being better separated.\n    \n![left: A t-SNE embedding of the bag-of-words representations of each paper. right: An embedding produced by a graph network that takes into account the citations between papers. source: [\u201cDeep Graph Infomax\u201d by Velickovic et al.](https:\/\/arxiv.org\/abs\/1809.10341)](https:\/\/miro.medium.com\/max\/493\/0*y332aTSAuQIkzz_K.png)\n    \n<p style=\"text-align: center\">--> source: <a href=\"https:\/\/arxiv.org\/abs\/1809.10341\">Deep Graph Infomax\u201d by Velickovic et al.<\/a> <\/p>\n\n**Knowledge Graphs (KG)** are a specific type of graph. They are multi-relational (i.e. there are different edges for different types of relations) and directed (i.e. the relations have a subject and object). These properties allow to represent information from heterogeneous sources in a uniform format.","126b12be":"# Countries in DBpedia\n\nThis dataset describes information of several countries from DBpedia. Let\u2019s take a look at how the KG looks in the neighbourhood of a specific country: \ud83c\udde7\ud83c\uddea Belgium \ud83c\udde7\ud83c\uddea. This process is analogous to going to [its corresponding DBpedia page](http:\/\/dbpedia.org\/page\/Belgium) and then recursively clicking on all the links on that page. We depict this below in Figure 3. We notice that expanding this neighbourhood iteratively makes things complex quickly, even though we introduced some simplifications by removing some of the parts. Nevertheless, we see that DBpedia contains some useful information about Belgium (e.g., its national anthem, largest city, currency, \u2026).\n\n![](https:\/\/miro.medium.com\/max\/1890\/1*-fo07n-06Obzqks4hoGQyg.png)","5ffe39bd":"# Creating Entity Embeddings With RDF2Vec\n\n**[RDF2vec](rdf2vec.org)** stands for Resource Description Framework To Vector. It is an unsupervised, task-agnostic algorithm to numerically represent nodes in a KG, allowing them to be used for further (downstream) machine learning tasks. RDF2Vec builds on top of existing natural language processing techniques: it combines insights from DeepWalk and Word2Vec. Word2Vec is able to generate embeddings for each word in a provided collection of sentences (often called a corpus). To generate a corpus for a KG, we extract walks. Extracting walks is similar to visiting a DBpedia page of an entity and clicking on links. The number of clicks you make is equivalent to the number of hops in a walk. An example of such a walk, again for Belgium, would be: \n\nBelgium -> dbo:capital -> City of Brussels -> dbo:mayor -> Yvan Mayeur. \n\nNote that we make no distinction between predicates\/properties (e.g., dbo:capital and dbo:mayor) and entities (e.g., Belgium, Brussels, Yvan Mayeur, \u2026) in our walks, as explained in Figure 2. Each walk can now be seen as a sentence, and the hops in that walk correspond to the tokens (words) of a sentence. Once we extracted a large number of walks rooted at the entities we want to create embeddings for, we can provide that as a corpus to Word2Vec. Word2Vec will then learn embeddings for each unique hop which can then be used for ML tasks.\n\npyRDF2Vec is a repository that contains a Python implementation of the RDF2Vec algorithm. On top of the original algorithm, different extensions are implemented as well.\n\n![](https:\/\/miro.medium.com\/max\/700\/1*dgij9Wdt9LgEo-CCltet8g.png)","7450efd3":"# 3. Creating a t-SNE plot\n\nThe code snippet above from Section 2 will give us a list of lists. For each of the provided entities to the transform method, a 100-dimensional embedding will be returned. Now in order to inspect these embeddings with the human eye, we need to further reduce the dimensionality. One great technique to do this, is by using t-SNE.","a6b49b72":"# Tutorial: creating country representations with pyRDF2Vec\n\nIn this tutorial, I will demonstrate basic usage of the pyRDF2Vec library using the country dataset previously described. Let us start with loading our data. \n\n# 1. Loading the data\npyRDF2Vec can easily load files in different RDF syntaxes by wrapping around rdflib. This will load the entire KG into RAM memory. However, this becomes problematic when the KG is larger than the available RAM memory. We therefore also support interaction with endpoints: the KG can be hosted on some server and our KG object will interact with that endpoint whenever necessary. This drastically reduces the required RAM memory at a cost of higher latencies.","61c4689c":"# 2. Extracting our first embeddings\n\nNow that we have our KG loaded in memory, we can start creating embeddings! In order to do this, we create an `RDF2VecTransformer` and then call the `fit()` function with the freshly loaded KG and a list of entities. Once the model is fitted, we can retrieve their embeddings through the `transform()` function. One thing that is different from the regular scikit-learn flow (where we call `fit()` on the train data and `predict()` or `transform()` on the test data) is that both the train and test entities have to be provided to the `fit()` function (similar to how t-SNE works in scikit-learn). Since RDF2Vec works unsupervised, this does not introduce label leakage.","1ad1e0b0":"**This is an edited \"re-post\" of my blog post: [\"How to Create Representations of Entities in a Knowledge Graph using pyRDF2Vec\"](https:\/\/towardsdatascience.com\/how-to-create-representations-of-entities-in-a-knowledge-graph-using-pyrdf2vec-82e44dad1a0)**","c0572c2a":"# Check out our github!\n\n[The pyRDF2Vec repository can be found on Github](https:\/\/github.com\/IBCNServices\/pyRDF2Vec). Feel free to give us a star if you like the repository, it is greatly appreciated! Moreover, we welcome all kinds of contributions.","8558c521":"# 4. Baseline classification results\n\nNow let\u2019s take a look at how good these embeddings are in order to solve the three ML tasks (which are provided in the CSV file): two binary classification tasks (high\/low inflation and high\/low academic output) and a multi-class classification task (predict the continent). It should be noted that, since RDF2Vec is unsupervised, during the creation of these embeddings, this label information was never used! RDF2Vec is task-agnostic, the projection from our nodes to an embedding is not tailored towards a specific task, and the embeddings can be used for multiple different downstream tasks. Let\u2019s create an utility function that takes as input the produced embeddings and then performs classification for all three tasks:","875fc5a5":"# 6. Setting the walking strategies\n\npyRDF2Vec allows us to use different walking strategies, an overview of the different strategies is provided in Figure 5. Moreover, we can combine different strategies: pyRDF2Vec will extract walks with each strategy and concatenate the extracted walks together before providing it to the embedding technique. Let\u2019s try combining several walking strategies:","0e8b6a0c":"# 5. Initial hyper-parameter tuning\n\nAs mentioned before, each of the three building blocks of the RDF2Vec algorithm (walking algorithm, sampling strategy and embedding technique) are configurable. For now, let\u2019s try to extract deeper walks and generate larger embeddings:"}}