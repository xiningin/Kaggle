{"cell_type":{"ca0978f0":"code","c362272e":"code","09162d63":"code","b1b4460d":"code","f182d2ba":"code","5b033435":"code","3b123906":"code","9912365e":"code","94473143":"code","f5dc1cc4":"code","85be48b8":"code","c447952c":"code","1825e7c4":"code","0710b691":"code","0115490d":"code","c557ad3a":"code","446aa41c":"code","c49cf882":"code","0ed1b339":"code","80839a5e":"code","fce3d315":"code","7300f685":"code","d4f96948":"code","922cf28d":"code","88d18bb7":"code","e60e57d4":"code","c95e0f5b":"code","ef178d0e":"code","fe70122b":"code","0dc9b4d5":"code","82084440":"code","7a6d78f9":"code","087f5672":"code","92994387":"code","52dc89d7":"code","cf0a995f":"code","052486a8":"code","e917639e":"code","a003e741":"code","4461ac53":"code","7582c0dc":"code","1c917636":"code","b8d902ee":"code","e8bf5edb":"code","6408e6d9":"code","9d9e11d2":"code","4c0d8670":"code","6e7ff364":"code","4124a785":"code","da3315b6":"markdown","546547a1":"markdown","09f77cdd":"markdown","4c608f88":"markdown","4b592b34":"markdown","ecad58b3":"markdown","a70f2e5a":"markdown","5a5f0c8f":"markdown","1c8b7b97":"markdown","11c9fcb6":"markdown","5dd816e0":"markdown","a7efd49e":"markdown","beef2930":"markdown","494a993f":"markdown","cf8662aa":"markdown","d5921a55":"markdown","ecfdf23c":"markdown","c8490cb5":"markdown"},"source":{"ca0978f0":"import numpy as np\nimport pandas as pd \nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom string import punctuation\nfrom keras.models import Sequential,Model\nfrom keras.layers import Embedding,Input,Activation,Flatten,CuDNNLSTM,Dense,Dropout,Bidirectional,LSTM,MaxPool1D\nfrom keras.layers import Convolution1D,GlobalAveragePooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LeakyReLU\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport re\nimport gc\nimport seaborn as sns\n%matplotlib inline\ntqdm.pandas()","c362272e":"df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')","09162d63":"df.head()","b1b4460d":"def target(value):\n    if value>=0.5:\n        return 1\n    else:\n        return 0","f182d2ba":"df['target'] = df['target'].apply(target)","5b033435":"x = df['comment_text']\ny = df['target']","3b123906":"sns.countplot(y)","9912365e":"def cleaning(text):\n    text = text.lower()\n    text = re.sub(r'\\W+',' ',text)\n    return text","94473143":"x = x.progress_apply(cleaning)","f5dc1cc4":"f = open('..\/input\/gloveembeddings\/glove.6B.100d.txt')\nembedding_values = {}\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_values[word]=coef","85be48b8":"token = Tokenizer()","c447952c":"token.fit_on_texts(x)","1825e7c4":"sequence = token.texts_to_sequences(x)","0710b691":"len(sequence)","0115490d":"vocab_size = len(sequence)+1","c557ad3a":"pad_seq = pad_sequences(sequence,maxlen = 100)","446aa41c":"all_emb = np.stack(embedding_values.values())\nall_mean,all_std = all_emb.mean(),all_emb.std()\nall_mean,all_std","c49cf882":"embedding_matrix = np.random.normal(all_mean,all_std,(vocab_size,100))\nfor word,i in tqdm(token.word_index.items()):\n    values = embedding_values.get(word)\n    if values is not None:\n        embedding_matrix[i] = values","0ed1b339":"model1 = Sequential()","80839a5e":"model1.add(Embedding(vocab_size,100,input_length = 100,weights = [embedding_matrix],trainable = False))","fce3d315":"model1.add(Bidirectional(CuDNNLSTM(100,return_sequences=True)))\nmodel1.add(Convolution1D(64,7,padding='same'))\nmodel1.add(GlobalAveragePooling1D())","7300f685":"model1.add(Dense(128))\nmodel1.add(LeakyReLU())","d4f96948":"model1.add(Dense(64,activation = 'relu'))","922cf28d":"model1.add(Dense(1,activation = 'sigmoid'))","88d18bb7":"model1.compile(optimizer = 'adam',loss='binary_crossentropy',metrics = ['accuracy'])","e60e57d4":"x_train,x_test,y_train,y_test = train_test_split(pad_seq,y,test_size = 0.15,random_state = 42)","c95e0f5b":"del (x,y)\ngc.collect()","ef178d0e":"history = model1.fit(x_train,y_train,epochs = 5,batch_size=128,validation_data=(x_test,y_test))","fe70122b":"values = history.history\nvalidation_acc = values['val_acc']\ntraining_acc = values['acc']\nvalidation_loss = values['loss']\ntraining_loss = values['val_loss']\nepochs = range(5)","0dc9b4d5":"plt.plot(epochs,training_acc,label = 'Training Accuracy')\nplt.plot(epochs,validation_acc,label = 'Validation Accuracy')\nplt.title('Epochs vs Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","82084440":"plt.plot(epochs,training_loss,label = 'Training Loss')\nplt.plot(epochs,validation_loss,label = 'Validation Loss')\nplt.title('Epochs vs Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","7a6d78f9":"test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","087f5672":"test.head()","92994387":"X = test['comment_text']","52dc89d7":"test_sequence = token.texts_to_sequences(X)\ntest_pad_seq = pad_sequences(test_sequence,maxlen = 100)","cf0a995f":"prediction1 = model1.predict(test_pad_seq)","052486a8":"submission1 = pd.DataFrame([test['id']]).T\nsubmission1['prediction'] = prediction1","e917639e":"submission1.to_csv('submission.csv', index=False)","a003e741":"# model2 = Sequential()\n# model2.add(Embedding(vocab_size,100,input_length=100,weights = [embedding_matrix],trainable = False))\n# model2.add(CuDNNLSTM(75,return_sequences=True))\n# model2.add(CuDNNLSTM(75))\n# model2.add(Dense(128,activation='relu'))\n# model2.add(Dropout(0.3))\n# model2.add(Dense(1,activation='sigmoid'))\n# model2.compile(optimizer= 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n# history = model2.fit(x_train,y_train,batch_size = 128,epochs = 5,validation_data = (x_test,y_test))","4461ac53":"values = history.history\nvalidation_acc = values['val_acc']\ntraining_acc = values['acc']\nvalidation_loss = values['loss']\ntraining_loss = values['val_loss']\nepochs = range(5)","7582c0dc":"plt.plot(epochs,training_acc,label = 'Training Accuracy')\nplt.plot(epochs,validation_acc,label = 'Validation Accuracy')\nplt.title('Epochs vs Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","1c917636":"plt.plot(epochs,training_loss,label = 'Training Loss')\nplt.plot(epochs,validation_loss,label = 'Validation Loss')\nplt.title('Epochs vs Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b8d902ee":"prediction2 = model2.predict(test_pad_seq)\nsubmission2 = pd.DataFrame([test['id']]).T\nsubmission2['prediction'] = prediction2\nsubmission2.to_csv('submission_model2.csv', index=False)","e8bf5edb":"# model3 = Sequential()\n# model3.add(Embedding(vocab_size,100,input_length=100,weights = [embedding_matrix],trainable = False))\n# model3.add(Convolution1D(32,5,activation='relu'))\n# model3.add(MaxPool1D(2,2))\n# model3.add(Convolution1D(64,5,activation='relu'))\n# model3.add(MaxPool1D(2,2))\n# model3.add(Flatten())\n# model3.add(Dense(128,activation='relu'))\n# model3.add(Dropout(0.2))\n# model3.add(Dense(1,activation='sigmoid'))\n# model3.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n# history = model3.fit(x_train,y_train,batch_size = 128,epochs = 5,validation_data = (x_test,y_test))","6408e6d9":"values = history.history\nvalidation_acc = values['val_acc']\ntraining_acc = values['acc']\nvalidation_loss = values['loss']\ntraining_loss = values['val_loss']\nepochs = range(5)","9d9e11d2":"plt.plot(epochs,training_acc,label = 'Training Accuracy')\nplt.plot(epochs,validation_acc,label = 'Validation Accuracy')\nplt.title('Epochs vs Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","4c0d8670":"plt.plot(epochs,training_loss,label = 'Training Loss')\nplt.plot(epochs,validation_loss,label = 'Validation Loss')\nplt.title('Epochs vs Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","6e7ff364":"prediction3 = model3.predict(test_pad_seq)\nsubmission3 = pd.DataFrame([test['id']]).T\nsubmission3['prediction'] = prediction3\nsubmission3.to_csv('submission_model3.csv', index=False)","4124a785":"prediction4 = 0.33*submission1['prediction']+0.33*submission2['prediction']+0.33*submission3['prediction']\nsubmission4 = pd.DataFrame([test['id']]).T\nsubmission4['prediction'] = prediction4\nsubmission4.to_csv('submission4.csv', index=False)","da3315b6":"Building a LSTM model. LSTM networks are useful in sequence data as they are capable of remembering the past words which help them in understanding the meaning of the sentence which helps in text classification. Bidirectional Layer is helpful as it helps in understanding thesentence from start to end and also from end to start. It works in both the direction. This is useful as the reverse order LSTM layer is capable of learning patterns which are not possible for the normal LSTM layers which goes from start to end of the sentence in the normal order. Hence Bidirectional layers are useful in text classification problems as different patterns can be captured from 2 directions.\nCuDNNLSTM is same as LSTM. If you are using GPU then CuDNNLSTM will be faster but if you are using CPU please use LSTM.","546547a1":"**Getting out X and y variable. We will be training on the X variable and predicting the Y variable**","09f77cdd":"Combining output of all models with 1\/3 ratio and making a new prediction.","4c608f88":"Here we will try to build a model only with convolution layers and not including any LSTM layers.","4b592b34":"**Performing some cleaning in the commnet text using regular expression. This code with help us extract only the characters from the expression.\n**\n**Tutorial for regular expressions can be found** [here](https:\/\/www.w3schools.com\/python\/python_regex.asp)","ecad58b3":"Preprocessing of the test data so that model can easily make its prediction as it should be in the same format as that of our training data.\nNote that we are using the same tokenizer in our testing and we are not fitting it again becuase this might change the numbers assigned to words which are there in the training data.","a70f2e5a":"Simple LSTM layers are added without any Bidirectional Layers.","5a5f0c8f":"**Using word embeddings so that words with similar words have similar representation in vector space. It represents every word as a vector. The words which have similar meaning are place close to each other. Quick understanding can be done from [this](https:\/\/towardsdatascience.com\/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795) article.**","1c8b7b97":"**Now we will be converting each word in our vocabulary into word embeddings. This embedding is vector of 1x100 dimension which represents each word as a vector and placing them into a vector space. Embedding matrix is created in which the number assigned to the word by tokenizer is assigned with the corresponding vector which we get from the glove embeddings.**","11c9fcb6":"Plotting a graph between Training and Testing Accuracy","5dd816e0":"**These are the steps that needs to be performed so that we can convert each word of our vocabulary into a unique integer. Tokenizer is initalized in first step. Then fitting on the text will help us create a vocabulary so that each word is assigned with a unique integer. Then we convert in the whole sentence of the comment into a sequence of numbers which are assigned by the tokenizer.**","a7efd49e":"Plotting a graph between Training and Testing Loss","beef2930":"**Padding the sequence helps in making all the sentence of same length. maxlen is the parameter which decides the length we want to assign to all the sentences. Padding is done by adding 0 on either the end of sentence or prior the sentence if the sentence is having length less than max length. This is also a parameter which user can change, by defaults its prefix. If the length of the sentence is more than 100 then it is pruned which brings down the length to 100 (maxlen)**","494a993f":"Reading the CSV","cf8662aa":"**Converting Target Probabilities to 0 or 1 for making it categorical. All the values in target column that are above 0.5 are considered as 1 and rest are considered as 0.**","d5921a55":"Now we start building the model with Keras","ecfdf23c":"**Using seaborn library to check the number of observations in each class that is 0 and 1. Countplot helps in visualization of the values.**","c8490cb5":"Here this embedding layer is important as this will help us in training of the sentences with their respective embeddings whihc we have assigned above. The first parameter is the size of our vocabulary. Second parameter is the output embeddings length which is 100 in this case as we used the 100 glove embeddings of each word. The length of each observation which is expected by the network is given by input_length parameter. We have padded all the observations to 100 hence we set input_length = 10. Weights parameter shows that the embeddings which we want to use is embeddings_matrix and it should not be altered hence trainable is kept false.\nIf we want to train our own embeddings we can simply remove the weights and trainable parameter."}}