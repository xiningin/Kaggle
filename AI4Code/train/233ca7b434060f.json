{"cell_type":{"fd390e8b":"code","87c145e4":"code","668f233a":"code","81013f12":"code","e8a48d53":"code","82291f73":"code","5a09c920":"code","7007e442":"code","00211018":"code","7b3f10b9":"code","1d6389ae":"code","af4097a2":"code","46686988":"code","56acfc9a":"code","4ee708d3":"code","12134a32":"code","c4a053ee":"code","413be3e2":"code","fc010e53":"code","d4dace1e":"code","361932fe":"code","f4af8917":"code","83c20b6e":"code","70187e32":"code","0deb217a":"code","d0583811":"code","744f1182":"code","cd0ce5d1":"code","3b457468":"code","e33cff5a":"code","fadb94e2":"code","6bb060fc":"code","202d2a7f":"code","e30e3508":"code","75196a46":"code","813479fc":"code","2e42d274":"code","dd6f316f":"code","05bde090":"code","ebc8fa9d":"code","8df900e6":"code","27630d2c":"markdown","0d11db56":"markdown","8126f131":"markdown","9f0f5aa6":"markdown","6ca44f40":"markdown","4141b1e0":"markdown","3da1a70a":"markdown","df1e957f":"markdown","52a3994e":"markdown","ae327867":"markdown"},"source":{"fd390e8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom tqdm import tqdm_notebook\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","87c145e4":"#read in all our files\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntrain_images = '..\/input\/train\/*'\ntest_images = '..\/input\/test\/*'","668f233a":"train_df.head()","81013f12":"sns.set(style = 'darkgrid')\nplt.figure(figsize = (12,10))\nsns.countplot(train_df['has_cactus'])","e8a48d53":"#let's visualize some cactus images\nIMAGES = os.path.join(train_images, \"*\")\nall_images = glob.glob(IMAGES)","82291f73":"#visualize some images\n\nplt.figure(figsize = (12,10))\nplt.subplot(1, 3, 1)\nplt.imshow(plt.imread(all_images[0]))\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(1, 3, 2)\nplt.imshow(plt.imread(all_images[10]))\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(1, 3, 3)\nplt.imshow(plt.imread(all_images[20]))\nplt.xticks([])\nplt.yticks([])","5a09c920":"train_path = '..\/input\/train\/train\/'\ntest_path = '..\/input\/test\/test\/'","7007e442":"#let's get our image data and image labels toegether\n#read in all the images\nimages_id = train_df['id'].values\nX = [] #this list will contain all our images\nfor id_ in images_id:\n    img = cv2.imread(train_path + id_)\n    X.append(img)","00211018":"#now let's get our labels\nlabel_list = [] #will contain all our labels\nfor img_id in images_id:\n    label_list.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])","7b3f10b9":"#now we can convert our images list and the labels list into numpy array\nX = np.array(X)\ny = np.array(label_list)","1d6389ae":"print(f\"THE SIZE OF OUR TRAINING DATA : {X.shape}\")\nprint(f\"THE SIZE OF OUR TRAINING LABELS : {y.shape}\")","af4097a2":"#let's do some preprocessing such as normalizing our data\nX = X.astype('float32') \/ 255","46686988":"#loading in and preprocessing the test data\nX_test = []\ntest_images = []\nfor img_id in tqdm_notebook(os.listdir(test_path)):\n    X_test.append(cv2.imread(test_path + img_id))     \n    test_images.append(img_id)\nX_test = np.array(X_test)\nX_test = X_test.astype('float32') \/ 255","56acfc9a":"#import the required libraries\nimport keras\nfrom keras.layers import Conv2D\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Flatten\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import MaxPooling2D\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K","4ee708d3":"class CNN:\n    def build(height, width, classes, channels):\n        model = Sequential()\n        inputShape = (height, width, channels)\n        chanDim = -1\n        \n        if K.image_data_format() == 'channels_first':\n            inputShape = (channels, height, width)\n            chanDim = 1\n        model.add(Conv2D(32, (3,3), padding = 'same', input_shape = inputShape))\n        model.add(BatchNormalization(axis = chanDim))\n        model.add(Activation('relu'))\n        model.add(Conv2D(32, (3,3), padding = 'same'))\n        model.add(BatchNormalization(axis = chanDim))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(2,2))\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(128, (3,3), padding = 'same', input_shape = inputShape))\n        model.add(BatchNormalization(axis = chanDim))\n        model.add(Activation('relu'))\n        model.add(Conv2D(128, (3,3), padding = 'same'))\n        model.add(BatchNormalization(axis = chanDim))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(2,2))\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(256, (3,3), padding = 'same', input_shape = inputShape))\n        model.add(BatchNormalization(axis = chanDim))\n        model.add(Activation('relu'))\n        model.add(Conv2D(256, (3,3), padding = 'same'))\n        model.add(BatchNormalization(axis = chanDim))\n        model.add(Activation('relu'))\n        model.add(MaxPooling2D(2,2))\n        model.add(Dropout(0.25))\n        \n        model.add(Flatten())\n        \n        model.add(Dense(128, activation = 'relu'))\n        model.add(BatchNormalization(axis = chanDim))\n        model.add(Dropout(0.5))\n        \n        model.add(Dense(32, activation = 'relu'))\n        model.add(BatchNormalization(axis = chanDim))\n        model.add(Dropout(0.5))\n        \n        model.add(Dense(classes, activation = 'sigmoid'))\n        \n        return model","12134a32":"input_dim = X.shape[1:]\nactivation = 'relu'\nclasses = 1\nheight = 32\nwidth = 32\nchannels = 3\n\nhistory = dict() #dictionery to store the history of individual models for later visualization\nprediction_scores = dict() #dictionery to store the predicted scores of individual models on the test dataset\n\n#here we will be training the same model for a total of 10 times and will be considering the mean of the output values for predictions\nfor i in np.arange(0, 5):\n    optim = optimizers.Adam(lr = 0.001)\n    ensemble_model = CNN.build(height = height, width = width, classes = classes, channels = channels)\n    ensemble_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])\n    print('TRAINING MODEL NO : {}'.format(i))\n    H = ensemble_model.fit(X, y,\n                           batch_size = 32,\n                           epochs = 200,\n                           verbose = 1)\n    history[i] = H\n    \n    ensemble_model.save('MODEL_{}.model'.format(i))\n    \n    predictions = ensemble_model.predict(X_test, verbose = 1, batch_size = 32)\n    prediction_scores[i] = predictions","c4a053ee":"from keras.applications.vgg16 import VGG16","413be3e2":"vgg16 = VGG16(weights = 'imagenet', input_shape = (32, 32, 3), include_top = False)\nvgg16.summary()","fc010e53":"for layer in vgg16.layers:\n    layer.trainable = False","d4dace1e":"vgg_model = Sequential()\nvgg_model.add(vgg16)\nvgg_model.add(Flatten())\nvgg_model.add(Dense(256, activation = 'relu'))\nvgg_model.add(BatchNormalization())\nvgg_model.add(Dropout(0.5))\nvgg_model.add(Dense(128, activation = 'relu'))\nvgg_model.add(BatchNormalization())\nvgg_model.add(Dropout(0.5))\nvgg_model.add(Dense(1, activation = 'sigmoid'))\n\nvgg_model.summary()","361932fe":"#compile the model\nvgg_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])","f4af8917":"#fit the model on our data\nvgg_history = vgg_model.fit(X, y,\n                            batch_size = 64,\n                            epochs = 500,\n                            verbose = 1) ","83c20b6e":"#making predictions on test dat\npredictions_vgg = vgg_model.predict(X_test)","70187e32":"predictions_vgg.shape","0deb217a":"from keras.applications.resnet50 import ResNet50","d0583811":"resnet = ResNet50(weights = 'imagenet', input_shape = (32, 32, 3), include_top = False)\nresnet.summary()","744f1182":"for layer in resnet.layers:\n    layer.trainable = False","cd0ce5d1":"resnet_model = Sequential()\nresnet_model.add(resnet)\nresnet_model.add(Flatten())\nresnet_model.add(Dense(256, activation = 'relu'))\nresnet_model.add(BatchNormalization())\nresnet_model.add(Dropout(0.5))\nresnet_model.add(Dense(128, activation = 'relu'))\nresnet_model.add(BatchNormalization())\nresnet_model.add(Dropout(0.5))\nresnet_model.add(Dense(1, activation = 'sigmoid'))\n\nresnet_model.summary()","3b457468":"#compile the model\nresnet_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])","e33cff5a":"#fit the model on our data\nresnet_history = resnet_model.fit(X, y,\n                                  batch_size = 64, \n                                  epochs = 500,\n                                  verbose = 1) ","fadb94e2":"resnet_predictions = resnet_model.predict(X_test)","6bb060fc":"#making predictions\nprediction = np.hstack([p.reshape(-1,1) for p in prediction_scores.values()]) #taking the scores of all the trained models\npredictions_ensemble = np.mean(prediction, axis = 1)\nprint(predictions_ensemble.shape)","202d2a7f":"df_ensemble = pd.DataFrame(predictions_ensemble, columns = ['has_cactus'])\ndf_ensemble['has_cactus'] = df_ensemble['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)","e30e3508":"df_ensemble['id'] = ''\ncols = df_ensemble.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndf_ensemble = df_ensemble[cols]\n\nfor i, img in enumerate(test_images):\n    df_ensemble.set_value(i,'id',img)\n\n#making submission\ndf_ensemble.to_csv('ensemble_submission.csv',index = False)","75196a46":"df_vgg = pd.DataFrame(predictions_vgg, columns = ['has_cactus'])\ndf_vgg['has_cactus'] = df_vgg['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)","813479fc":"df_vgg['id'] = ''\ncols = df_vgg.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndf_vgg = df_vgg[cols]\n\nfor i, img in enumerate(test_images):\n    df_vgg.set_value(i,'id',img)\n\n#making submission\ndf_vgg.to_csv('vgg_submission.csv',index = False)","2e42d274":"df_vgg.head()","dd6f316f":"df_resnet = pd.DataFrame(resnet_predictions, columns = ['has_cactus'])\ndf_resnet['has_cactus'] = df_resnet['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)","05bde090":"df_resnet['id'] = ''\ncols = df_resnet.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndf_resnet = df_resnet[cols]\n\nfor i, img in enumerate(test_images):\n    df_resnet.set_value(i,'id',img)\n\n#making submission\ndf_resnet.to_csv('resnet_submission.csv',index = False)","ebc8fa9d":"df_vgg1 = pd.DataFrame(predictions_vgg, columns = ['has_cactus'])\ndf_ensemble1 = pd.DataFrame(predictions_ensemble, columns = ['has_cactus'])\n\ndf_t = 0.5 * df_vgg1['has_cactus'] + 0.5 * df_ensemble1['has_cactus']\ndf_t = pd.DataFrame(df_t, columns = ['has_cactus'])\ndf_t['has_cactus'] = df_t['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)\n\ndf_t['id'] = ''\ncols = df_t.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndf_t = df_t[cols]\n\nfor i, img in enumerate(test_images):\n    df_t.set_value(i,'id',img)\n\n#making submission\ndf_t.to_csv('vgg_ensemble_submission.csv',index = False)","8df900e6":"df_vgg2 = pd.DataFrame(predictions_vgg, columns = ['has_cactus'])\ndf_ensemble2 = pd.DataFrame(predictions_ensemble, columns = ['has_cactus'])\ndf_resnet2 = pd.DataFrame(resnet_predictions, columns = ['has_cactus'])\n\ndf_t2 = 0.45 * df_vgg2['has_cactus'] + 0.45 * df_ensemble2['has_cactus'] + 0.10 * df_resnet2['has_cactus']\ndf_t2 = pd.DataFrame(df_t2, columns = ['has_cactus'])\ndf_t2['has_cactus'] = df_t2['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)\n\ndf_t2['id'] = ''\ncols = df_t2.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndf_t2 = df_t2[cols]\n\nfor i, img in enumerate(test_images):\n    df_t2.set_value(i,'id',img)\n\n#making submission\ndf_t2.to_csv('vgg_ensemble_resnet_submission.csv',index = False)","27630d2c":"## BUILD CNN","0d11db56":"## RESNET50","8126f131":"2. VGG16","9f0f5aa6":"## MAKING SUBMISSIONS","6ca44f40":"1. Ensemble Model","4141b1e0":"## VGG16","3da1a70a":"3. Resnet50","df1e957f":"5. Ensemble, VGG16 and ResNet50","52a3994e":"## ENSEMBLE NEURAL NETWORK","ae327867":"4. Ensemble and VGG16"}}