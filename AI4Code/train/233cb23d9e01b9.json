{"cell_type":{"a165ee04":"code","5ab81573":"code","04fd052a":"code","8515cb89":"code","5f0134cf":"code","5aef001a":"code","bdf6195e":"code","df3d1d96":"code","759dccf1":"code","828a9cde":"code","7facee52":"code","ee1bcdf3":"code","13954554":"code","5781ccea":"code","70f822e7":"code","08acf185":"code","2155a042":"code","fde76a47":"code","5d88912a":"code","2e7abde1":"code","d2bef72b":"code","45e26e7e":"code","4ec321ef":"code","561b6f5a":"code","0328639a":"code","bddb4220":"code","22416e29":"code","a351ea29":"code","2e30a315":"code","94f4631b":"code","811de206":"code","d4a31988":"code","8822653d":"code","1e97c108":"code","93d2454a":"code","bd7b2c0d":"code","c4c82b3b":"code","8945ca06":"code","b4ce72a5":"code","18237118":"code","f113bf07":"code","f7c02599":"code","8c064172":"code","63a3a83c":"markdown","4d765bef":"markdown","8c5a91af":"markdown","aafab025":"markdown","7a880b1d":"markdown","5ca2fdd6":"markdown","58ce9cc7":"markdown","6156489f":"markdown","afb870b7":"markdown","e61c5ade":"markdown","d0886fdf":"markdown"},"source":{"a165ee04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ab81573":"import pandas as pd\nimport numpy as np\n#cr\u00e9ation du dataset\ndataset = pd.read_csv(\"..\/input\/age-gender-and-ethnicity-face-data-csv\/age_gender.csv\")","04fd052a":"dataset['pixels']=dataset['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))\/255","8515cb89":"#we remove the column img_name because not relevant for the study\ndataset=dataset.drop('img_name',axis=1)\n","5f0134cf":"import matplotlib.pyplot as plt # plotting\nfrom math import *\n\ndef sample(xs):\n  num=len(xs)\n  num_row = ceil(num\/5)\n  num_col = num if num<5  else 5\n\n  # plot images\n  fig, axes = plt.subplots(num_row, num_col, figsize=(6*num_col,6*num_row))\n\n\n  for x,i in zip(xs,[i for i in range(num)]):\n    \n    img = np.asarray(dataset.loc[x,\"pixels\"]).reshape(48, 48)\n    titre = \"age : \" + str(dataset.loc[x,'age']) +\" ethnie : \"+ str(dataset.loc[x,'ethnicity'])+ \" genre : \"+ str(dataset.loc[x,'gender'])\n    ax = axes[i\/\/num_col, i%num_col]\n    ax.imshow(img,cmap='gray', vmin=0, vmax=1)\n    ax.set_title(titre,fontsize=24)\n\n  plt.tight_layout()\n  plt.show()\n","5aef001a":"#the code allows to extract a batch of 10 images randomly from the dataset\nimport random\n#sample([i for i in range(10)])\nsample([random.randint(0,len(dataset)) for i in range(10)])","bdf6195e":"import seaborn as sns","df3d1d96":"plt.figure(figsize=(16,9))\nsns.set_palette(\"pastel\")\nsns.countplot(x ='ethnicity',hue=\"gender\", data = dataset) ","759dccf1":"plt.figure(figsize=(16,9))\nsns.set_palette(\"pastel\")\nsns.countplot(x ='gender', data = dataset) ","828a9cde":"plt.figure(figsize=(16,9))\nsns.distplot(dataset.loc[dataset.gender==0,[\"age\"]],bins=50,label=\"genre0\")\nsns.distplot(dataset.loc[dataset.gender==1,[\"age\"]],bins=50,label=\"genre1\")\nplt.legend()","7facee52":"X=dataset['pixels']","ee1bcdf3":"X_t = []\nfor i in range(X.shape[0]):\n    X_t.append(X[i].reshape(48,48,1)) #reshaping the data to (n,48,48,1)\nX = np.array(X_t)","13954554":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train_gender, y_test_gender, y_train_ethnicity, y_test_ethnicity, y_train_age, y_test_age = train_test_split(\n    X,pd.get_dummies(dataset['gender']).values,pd.get_dummies(dataset['ethnicity']).values,dataset['age'].values,test_size=0.1, random_state=1234)","5781ccea":"x_val, x_test, y_val_gender, y_test_gender, y_val_ethnicity, y_test_ethnicity, y_val_age, y_test_age = train_test_split(\n    x_test, y_test_gender, y_test_ethnicity, y_test_age,test_size=0.5, random_state=1234)","70f822e7":"!pip install -U keras-tuner","08acf185":"import kerastuner as kt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.models import clone_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\n\nimport keras\nimport IPython","2155a042":"def build_model(hp):\n    inputs = tf.keras.Input(shape=(48, 48, 1))\n    x = inputs\n    for i in range(hp.Int('conv_blocks', 3, 5, default=3)):\n        filters = hp.Int('filters_' + str(i), 32, 256, step=32)\n        for _ in range(2):\n            x = tf.keras.layers.Convolution2D(\n              filters, kernel_size=(3, 3), padding='same')(x)\n            x = tf.keras.layers.BatchNormalization()(x)\n            x = tf.keras.layers.LeakyReLU(hp.Float('alpha', 0, 0.3, step=0.01, default=0.01))(x)\n            #x = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU')\n            #x = tf.keras.layers.ReLU()(x)\n        if hp.Choice('pooling_' + str(i), ['avg', 'max']) == 'max':\n            x = tf.keras.layers.MaxPool2D()(x)\n        else:\n            x = tf.keras.layers.AvgPool2D()(x)\n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(units=hp.Int('units',\n                                        min_value=32,\n                                        max_value=1024,\n                                        step=32),\n                           activation='relu')(x)\n    x = tf.keras.layers.Dropout(\n        hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(x)\n    x = layers.Dense(units=hp.Int('units',\n                                        min_value=32,\n                                        max_value=1024,\n                                        step=32),\n                           activation='relu')(x)\n\n    out_gender = layers.Dense(2, activation='sigmoid', name='gender_out')(x) ## output binaire\n    out_ethnicity = layers.Dense(5, activation='softmax', name='ethnicity_out')(x) ## output cat\u00e9goriel\n    out_age=layers.Dense(1, name='age_out')(x) ## output continue\n\n    model = tf.keras.Model(inputs=inputs, outputs=[out_gender, out_ethnicity, out_age])\n\n    model.compile(\n      optimizer=tf.keras.optimizers.Adam(\n        hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n        loss={'gender_out':'BinaryCrossentropy',\n              'ethnicity_out':'categorical_crossentropy',\n              'age_out':'mse'},\n        metrics={'gender_out':'accuracy',\n                 'ethnicity_out':'accuracy',\n                 'age_out':'mae'})\n    return model","fde76a47":"tuner = kt.Hyperband(\n    build_model,\n    objective='val_loss',\n    max_epochs=40,\n    #hyperband_iterations=2,\n    #seed=1234,\n    directory = '..\/kaggle\/working\/',\n    project_name = 'Keras_tuning')","5d88912a":"class ClearTrainingOutput(tf.keras.callbacks.Callback):\n  def on_train_end(*args, **kwargs):\n    IPython.display.clear_output(wait = True)","2e7abde1":"tuner.search(x_train, {'gender_out': y_train_gender, 'ethnicity_out': y_train_ethnicity, 'age_out': y_train_age},\n             validation_data=(x_val, [y_val_gender, y_val_ethnicity, y_val_age]),\n             callbacks= [ClearTrainingOutput()])","d2bef72b":"#best_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is {best_hps.get('units')} and the optimal learning rate for the optimizer\nis {best_hps.get('learning_rate')}.\n\"\"\")","45e26e7e":"#best model\nbest_model = tuner.hypermodel.build(best_hps)\n\nbest_model.summary()","4ec321ef":"batch_size = 32\nepochs = 47\n\n\nhistory_list = []","561b6f5a":"history = best_model.fit(x_train, {'gender_out': y_train_gender, 'ethnicity_out': y_train_ethnicity, 'age_out': y_train_age},\n                         batch_size=batch_size,\n                         epochs = epochs, validation_data = (x_val, [y_val_gender, y_val_ethnicity, y_val_age]),\n                         steps_per_epoch=(x_train.shape[0] \/\/ batch_size)\n                         )\n\nhistory_list.append(history)","0328639a":"from matplotlib import pyplot as plt\n\ndef plot_loss(his, epoch, title):\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['loss'], label='train_loss')\n    plt.plot(np.arange(0, epoch), his.history['gender_out_loss'], label='train_gender_loss')\n    plt.plot(np.arange(0, epoch), his.history['ethnicity_out_loss'], label='train_ethnicity_loss')\n    plt.plot(np.arange(0, epoch), his.history['age_out_loss'], label='train_age_loss')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_gender_out_loss'], label='val_train_gender_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_ethnicity_out_loss'], label='val_train_ethnicity_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_age_out_loss'], label='val_train_age_loss')\n    \n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.show()\n    \ndef plot_acc(his, epoch, title):\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['gender_out_accuracy'], label='train_gender_acc')\n    plt.plot(np.arange(0, epoch), his.history['ethnicity_out_accuracy'], label='train_ethnicity_accuracy')\n        \n    plt.plot(np.arange(0, epoch), his.history['val_gender_out_accuracy'], label='val_gender_acc')\n    plt.plot(np.arange(0, epoch), his.history['val_ethnicity_out_accuracy'], label='val_ethnicity_accuracy')\n\n\n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.show()\n\ndef plot_MSE(his, epoch, title):\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['age_out_mae'], label='train_age_mae')\n    plt.plot(np.arange(0, epoch), his.history['val_age_out_mae'], label='val_age_mae')\n\n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Mean Absolute Error')\n    plt.legend(loc='upper right')\n    plt.show()","bddb4220":"    plot_loss(history_list[0], epochs, f'Training Dataset: {0}')\n    plot_acc(history_list[0], epochs, f'Training Dataset: {0}')\n    plot_MSE(history_list[0], epochs, f'Training Dataset: {0}')","22416e29":"pred = best_model.predict(x_test)\n\ntest_loss,test_gender_loss, test_ethnicity_loss, test_age_loss, test_gender_acc,test_ethnicity_acc,test_age_mae = best_model.evaluate(x_test, [y_test_gender, y_test_ethnicity, y_test_age], verbose=0)\nprint(f'\\nTest gender accuracy: {test_gender_acc}')\nprint(f'\\nTest ethnicity accuracy: {test_ethnicity_acc}')\nprint(f'\\nTest age MAE: {test_age_mae}')","a351ea29":"#Confution Matrix and Classification Report Gender\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nY_pred_gender = np.argmax(pred[0],axis=1)\nY_true_gender = np.argmax(y_test_gender,axis = 1)\nprint('Confusion Matrix')\nprint(confusion_matrix(Y_true_gender, Y_pred_gender))\nprint('Classification Report')\ntarget_names = ['Woman', 'Man']\nprint(classification_report(Y_true_gender, Y_pred_gender, target_names=target_names))","2e30a315":"#Confution Matrix and Classification Report Ethnicity\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nY_pred_Ethn = np.argmax(pred[1],axis=1)\nY_true_ethnicity = np.argmax(y_test_ethnicity,axis = 1)\nprint('Confusion Matrix')\nprint(confusion_matrix(Y_true_ethnicity, Y_pred_Ethn))\nprint('Classification Report')\ntarget_names = ['0', '1','2','3','4']\nprint(classification_report(Y_true_ethnicity, Y_pred_Ethn, target_names=target_names))","94f4631b":"def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 10 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 5\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True,figsize=(20, 8))\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((48,48)),cmap='gray')\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n    plt.show()","811de206":"Y_pred_gender = np.argmax(pred[0],axis=1)\nY_true_gender = np.argmax(y_test_gender,axis = 1)","d4a31988":"errors_gender = (Y_pred_gender - Y_true_gender != 0)","8822653d":"Y_pred_gender_class_errors = Y_pred_gender[errors_gender]\nY_pred_gender_errors = pred[0][errors_gender]\nY_true_errors = Y_true_gender[errors_gender]\nx_val_errors = x_test[errors_gender]","1e97c108":"Y_pred_gender_errors_prob = np.max(Y_pred_gender_errors,axis = 1)\ntrue_prob_errors = np.diagonal(np.take(Y_pred_gender_errors, Y_true_errors, axis=1))\ndelta_pred_true_errors = Y_pred_gender_errors_prob - true_prob_errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\nmost_important_errors = sorted_dela_errors[-10:]\n\ndisplay_errors(most_important_errors, x_val_errors, Y_pred_gender_class_errors, Y_true_errors)","93d2454a":"Y_pred_ethnicity = np.argmax(pred[1],axis=1)\nY_true_ethnicity = np.argmax(y_test_ethnicity,axis = 1)","bd7b2c0d":"errors_ethnicity = (Y_pred_ethnicity - Y_true_ethnicity != 0)","c4c82b3b":"Y_pred_ethnicity_class_errors = Y_pred_ethnicity[errors_ethnicity]\nY_pred_ethnicity_errors = pred[1][errors_ethnicity]\nY_true_ethnicity_errors = Y_true_ethnicity[errors_ethnicity]\nx_test_ethnicity_errors = x_test[errors_ethnicity]","8945ca06":"Y_pred_ethnicity_errors_prob = np.max(Y_pred_ethnicity_errors,axis = 1)\ntrue_prob_errors = np.diagonal(np.take(Y_pred_ethnicity_errors, Y_true_ethnicity_errors, axis=1))\ndelta_pred_true_ethnicity_errors = Y_pred_ethnicity_errors_prob - true_prob_errors\nsorted_delta_ethnicity_errors = np.argsort(delta_pred_true_ethnicity_errors)\nmost_important_errors = sorted_delta_ethnicity_errors[-10:]\n\ndisplay_errors(most_important_errors, x_test_ethnicity_errors, Y_pred_ethnicity_class_errors, Y_true_ethnicity_errors)","b4ce72a5":"Y_pred_age = np.around(pred[2]).reshape(pred[2].size)\nY_true_age = y_test_age","18237118":"MSE_age =np.sqrt((Y_pred_age-Y_true_age)**2)","f113bf07":"def largest_indices(ary, n):\n    \"\"\"Returns the n largest indices from a numpy array.\"\"\"\n    flat = ary.flatten()\n    indices = np.argpartition(flat, -n)[-n:]\n    indices = indices[np.argsort(-flat[indices])]\n    return np.unravel_index(indices, ary.shape)\n","f7c02599":"Max_MSE_age=largest_indices(MSE_age, 10)[0]","8c064172":"display_errors(Max_MSE_age, x_test, Y_pred_age, Y_true_age)","63a3a83c":"## Age error","4d765bef":"## gender error","8c5a91af":"###callback to clear the training outputs at the end of every training step","aafab025":"## Image samples","7a880b1d":"the ethnicity variable is unbalanced with an over-representation of category 1. It will be interesting to see the capacity of our model to correctly predict under-represented categories via a confusion matrix. On the other hand, the genders are well balanced within each ethnic group.","5ca2fdd6":"# Model Build","58ce9cc7":"## ethnicity error","6156489f":"# Build the model with the optimal hyperparameters and train it on the data","afb870b7":"## Instantiate the tuner and perform hypertuning","e61c5ade":"# Data Split","d0886fdf":"# EDA"}}