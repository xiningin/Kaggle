{"cell_type":{"18de32fd":"code","8d142fe9":"code","f283d7b3":"code","780249e2":"code","a8a9e18a":"code","51757cbe":"code","bde34b25":"code","9ceedb4c":"code","181d1e09":"code","ea7f1b92":"code","095e23aa":"code","eca4c389":"code","d9de492b":"code","701f8d52":"code","1893c436":"code","13fb40ee":"code","989fffd5":"code","52cff396":"code","6257f8a3":"markdown","3acbf436":"markdown","f4928fff":"markdown","ea5f0079":"markdown","f33765d2":"markdown","dbc0f507":"markdown","2445ae8f":"markdown","e851dfb7":"markdown","94e16039":"markdown","a2290398":"markdown","da788c3a":"markdown","f5c8bde2":"markdown","7eca59d5":"markdown","90079f50":"markdown","1fff1d7b":"markdown","e04e31c0":"markdown","b44d7b39":"markdown"},"source":{"18de32fd":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('..\/input\/'))\nfrom utils import ESC50\n\nimport scipy\nfrom scipy.io import wavfile\nfrom scipy import signal\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n \nfrom kymatio.keras import Scattering1D","8d142fe9":"shared_params = {'csv_path': 'esc50.csv',\n                 'wav_dir': 'audio\/audio',\n                 'dest_dir': 'audio\/audio\/16000',\n                 'audio_rate': 16000,\n                 'only_ESC10': True,\n                 'pad': 0,\n                 'normalize': True}\n\ntrain_gen = ESC50(folds=[1,2,3,4],\n                  randomize=False,\n                  strongAugment=True,\n                  random_crop=True,\n                  inputLength=2,\n                  mix=False,\n                  **shared_params).batch_gen(100)\n\ntest_gen = ESC50(folds=[5],\n                  randomize=True,\n                  strongAugment=True,\n                  random_crop=True,\n                  inputLength=2,\n                  mix=False,\n                  **shared_params).batch_gen(100)","f283d7b3":"X_tr, Y_tr = next(train_gen)\n\nX_train = []\nfor x in X_tr:\n    X_train.append(x[:, 0])\n    \nX_train = np.array(X_train)\n\nY_train = []\nfor y in Y_tr:\n    Y_train.append(np.where(y==1)[0][0])\n    \nY_train = np.array(Y_train)","780249e2":"X_t, Y_ts = next(test_gen)\n\nX_test = []\nfor x in X_t:\n    X_test.append(x[:, 0])\n    \nX_test = np.array(X_test)\n\nY_test = []\nfor y in Y_ts:\n    Y_test.append(np.where(y==1)[0][0])\n    \nY_test = np.array(Y_test)","a8a9e18a":"X_train.shape","51757cbe":"X_test.shape","bde34b25":"# maximum scale 2**J of the scattering transform\nJ = 8\n\n# the number of wavelets per octave.\nQ = 12\n\n# define a small constant to add to the scattering coefficients before computing the logarithm.\n# this prevents very large values when the scattering coefficients are very close to zero.\nlog_eps = 1e-6","9ceedb4c":"x_in = layers.Input(shape=(X_train.shape[1]))\nx = Scattering1D(J, Q=Q)(x_in)","181d1e09":"x = layers.Lambda(lambda x: x[..., 1:, :])(x)","ea7f1b92":"x = layers.Lambda(lambda x: tf.math.log(tf.abs(x) + log_eps))(x)","095e23aa":"x = layers.GlobalAveragePooling1D(data_format='channels_first')(x)","eca4c389":"x = layers.BatchNormalization(axis=1)(x)","d9de492b":"x_out = layers.Dense(10, activation='softmax')(x)","701f8d52":"model = tf.keras.models.Model(x_in, x_out)\nmodel.summary()","1893c436":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","13fb40ee":"cnnhistory = model.fit(\n                    X_train,\n                    Y_train,\n                    epochs=50,\n                    batch_size=10,\n                    validation_split=0.2)","989fffd5":"plt.plot(cnnhistory.history['loss'])\nplt.plot(cnnhistory.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","52cff396":"model.evaluate(X_train, Y_test, verbose=2)","6257f8a3":"To increase discriminability, we take the log-scattering transform. ","3acbf436":"<font color='red'>This notebook is meant to serve only as example of how to use scattering transform for audio classification task. The final performance of the model is not satisfactory. <\/font>\n\n\nIn this notebook, I am going to use the kymatio Scattering1D class[1], implementing 1d-scattering transforms, to build a classifier for environmental sound claassification task.\n\nScattering transform are defined as a complex-valued convolutional neural network whose filters are fixed to be wavelets and the non-linearity is a complex modulus. <br>\nFor audio signals, the resulting learnt scattering coefficients provide similar information to MFCCs in the first layer, while subsequent layers give additional information on larger temporal structures, such as amplitude modulation spectrum and harmonic frequency intervals. <br>\nThe peculiarity of scattering transform is that they define a representation which is locally invariant to translations and stable to time-warping deformations, making well-suited for classification tasks. <br>\nA complete introduction to scattering transform is given in [2]. In [3] a study of environmental sound classification using scattering transform is presented.\n\n\n[1] \"Kymatio: Scattering Transforms in Python\" Andreux M. at al., https:\/\/arxiv.org\/abs\/1812.11214 <br>\n[2] \"Deep Scattering Spectrum\", And\u00e9n et al., IEEE TRANSACTIONS ON SIGNAL PROCESSING, 62, 16, (2014) <br>\n[3] \"Representing environmental sounds using the separable scattering transform\", Baug\u00e9 at al., Acoustics, Speech, and Signal Processing, (2013)","f4928fff":"# Importing libraries","ea5f0079":"We then average along the last dimension (time) to get a time-shift invariant representation. ","f33765d2":"Finally, we apply batch normalization to ensure that the data is within a moderate range.","dbc0f507":"# Training the classifier","2445ae8f":"I am going to use the ESC50 class in shown in the [example notebook](https:\/\/www.kaggle.com\/mmoreaux\/esc50-visualization\/), but without mixing, i.e. mix=False.","e851dfb7":"# Evaluating model on test set","94e16039":"We use the Scattering1D Keras layer, taking the input signals of length and then feeding into the Scattering1D.","a2290398":"# Classification of Environmental Sound using Scattering Transform ","da788c3a":"Since it does not carry useful information, we remove the zeroth-order scattering coefficients,\nwhich are always placed in the first channel of the scattering transform.","f5c8bde2":"# Wavelet scattering model","7eca59d5":"These features are then used to classify the input signal using a dense layer followed by a softmax activation.","90079f50":"# Loading and Preprocessing Data","1fff1d7b":"We now create a classification model based built on wavelet transform. <br>\nWe start by specifying some parameters.","e04e31c0":"Some preprocessing routines to transform feature and target vectors.","b44d7b39":"Let's display the model"}}