{"cell_type":{"81a55f6e":"code","01f37448":"code","40cadb67":"code","4ff57dad":"code","925198e5":"code","c754bca1":"code","13ec2e3f":"code","dd98201f":"code","e624f2a9":"code","6ccee588":"code","b40905ef":"markdown","5fbcdcd7":"markdown","a1f049e6":"markdown","7c78a5d1":"markdown"},"source":{"81a55f6e":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.datasets.samples_generator import make_blobs\nimport seaborn as sns\n%matplotlib inline","01f37448":"X, y = make_blobs(n_samples=50,\n                 centers = 2, \n                 random_state = 0,\n                 cluster_std = 0.60)\nplt.scatter(X[:, 0],X[:, 1], c = y, s = 50, cmap = plt.cm.autumn)\nplt.title('Simple data for classification')\nplt.tight_layout()\nplt.show()","40cadb67":"fit_x = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c = y, s = 50, cmap = plt.cm.autumn)\nplt.plot([0.6], [2.1],'x', color = 'red', markeredgewidth = 2,\n        markersize = 10)\n\nfor m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n    plt.plot(fit_x, m*fit_x +b,'-k')\n    \nplt.title('Three perfect Linear discriminative Classifier')\nplt.xlim(-1, 3.5)\nplt.tight_layout()\nplt.show()","4ff57dad":"from sklearn.svm import SVC\n\nsvc_model = SVC(kernel=\"linear\", C = 1e10)\nsvc_model.fit(X, y)","925198e5":"def plot_svc_decision_function(model, ax =None, plot_support = True):\n    if ax is None:\n        ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n\n    # create grid to evaluatiing model\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n\n    P = model.decision_function(xy).reshape(X.shape)\n\n    ax.contour(X, Y, P, colors = 'K',\n              levels = [-1, 0, 1], alpha = 0.5,\n              linestyles = ['--','-', '--'])\n\n    if plot_support:\n        ax.scatter(model.support_vectors_[:, 0],\n                  model.support_vectors_[:, 1],\n                  s = 300, linewidth = 1,\n                  facecolors = 'none');\n\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n","c754bca1":"plt.scatter(X[:, 0], X[:, 1], c = y, s = 50, cmap = plt.cm.autumn)\nplot_svc_decision_function(svc_model)","13ec2e3f":"svc_model.support_vectors_","dd98201f":"def plot_SVM(N = 10, ax = None):\n    X, y = make_blobs(n_samples=200, centers=2,\n                     random_state = 0, cluster_std = 0.60)\n    X = X[:N]\n    y = y[:N]\n    \n    \n    model = SVC(kernel='linear', C = 1e10)\n    model.fit(X, y)\n    \n    ax = ax or plt.gca()\n    \n    ax.scatter(X[:, 0], X[:, 1], c = y, s= 50, cmap = plt.cm.autumn)\n    ax.set_xlim(-1, 4)\n    ax.set_ylim(-1, 6)\n    plot_svc_decision_function(model, ax)","e624f2a9":"fig,ax = plt.subplots(1, 2, figsize = (16, 6))\nfig.subplots_adjust(left = 0.0625, right = 0.95, wspace = 0.1)\nfor axi, N in zip(ax, [60, 120]):\n    plot_SVM(N, axi)\n    axi.set_title('N = {0}'.format(N))","6ccee588":"from ipywidgets import interact, fixed\ninteract(plot_SVM, N = [10,30, 40, 200], ax = fixed(None));","b40905ef":"## Import necessary libraries","5fbcdcd7":"# <b style=\"color:blue\">What is Support Vector Machines?<\/b>\n \nSVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs. Simply put, it does some extremely complex data transformations, then figures out how to seperate your data based on the labels or outputs you've defined.\n\n<img src=\"https:\/\/www.surveypractice.org\/article\/2715-using-support-vector-machines-for-survey-research\/attachment\/9153.png\" alt=\"SVM\">\n\n# <b style=\"color:blue\">What makes it so great?<\/b>\n \nWell SVM it capable of doing both classification and regression. In this post I'll focus on using SVM for classification. In particular I'll be focusing on non-linear SVM, or SVM using a non-linear kernel. Non-linear SVM means that the boundary that the algorithm calculates doesn't have to be a straight line. The benefit is that you can capture much more complex relationships between your datapoints without having to perform difficult transformations on your own. The downside is that the training time is much longer as it's much more computationally intensive.","a1f049e6":"## <b style=\"color:blue\">Linear Discriminative Classifier<\/b>","7c78a5d1":"## Support Vector Classifier"}}