{"cell_type":{"b001d8ef":"code","3793be88":"code","23aa5bbf":"code","3f88c77e":"code","339e0cc1":"code","53b7834f":"code","37c012e1":"code","be62ae2e":"code","e47a67e6":"code","1efd8913":"code","d659910d":"code","03543555":"code","4a9c09ce":"code","c48ed947":"code","943053eb":"code","b3ece1ca":"code","e19e3c7e":"code","e738fe72":"code","b08a739f":"code","dec47144":"code","e7398761":"code","6d1fba57":"code","250a0d7a":"code","cc8c63fd":"code","39365b5f":"code","472149e1":"code","9ec73a07":"code","aad58a57":"code","85ce7805":"code","56a09c08":"code","34a47007":"code","23ceec22":"code","671c2ffd":"code","911fe8c0":"code","70e0df86":"code","51f92fe8":"code","bb1ff19b":"code","d54e16ee":"code","e5d3505b":"code","60b03533":"markdown","b0b38ba8":"markdown","6a5afffe":"markdown","640c5469":"markdown","bf2296f9":"markdown","a60c41d5":"markdown","9ac4168e":"markdown","bce49aa2":"markdown","dc8ef6ed":"markdown","f551db59":"markdown","041a5295":"markdown","c5c7d11f":"markdown","593d2b34":"markdown","91b28c4e":"markdown","a4be0db7":"markdown","d19ff7fa":"markdown","a3bede28":"markdown","dd738449":"markdown","a82bd04a":"markdown","3d825ac9":"markdown","852a68c6":"markdown","68e74a80":"markdown","c25a4c44":"markdown","94f2daae":"markdown","c9ef8749":"markdown"},"source":{"b001d8ef":"!pip install scikit-learn==0.21.3\n!pip install imbalanced-learn==0.4.3","3793be88":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score, classification_report\nfrom sklearn.datasets import make_classification\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks, EditedNearestNeighbours\nfrom imblearn.combine import SMOTETomek, SMOTEENN\n","23aa5bbf":"X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=3,\n                           n_clusters_per_class=1,\n                           weights=[0.06, 0.02, 0.92],\n                           class_sep=0.8, random_state=0)\n\ncolors = ['#4E6B8A' if v == 0 else '#F26419' if v == 1 else '#F6AE2D' for v in y]\nfig = plt.Figure(figsize=(12,8))\nplt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='grey', linewidths=0.5)\nsns.despine()","3f88c77e":"ros = RandomOverSampler(random_state=0, sampling_strategy={0: 300, 1: 300})\nros.fit(X, y)\nX_resampled, y_resampled = ros.fit_resample(X, y)\ncolors = ['#4E6B8A' if v == 0 else '#F26419' if v == 1 else '#F6AE2D' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='grey')\nsns.despine()\nplt.title(\"Random Oversampling\")","339e0cc1":"smote = SMOTE(random_state=0,  sampling_strategy={0: 300, 1: 500})\nsmote.fit(X, y)\nX_resampled, y_resampled = smote.fit_resample(X, y)\ncolors = ['#4E6B8A' if v == 0 else '#F26419' if v == 1 else '#F6AE2D' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='grey')\nsns.despine()\nplt.title(\"SMOTE\")","53b7834f":"smote = SMOTEENN(random_state=0)\nsmote.fit(X, y)\nX_resampled, y_resampled = smote.fit_resample(X, y)\ncolors = ['#4E6B8A' if v == 0 else '#F26419' if v == 1 else '#F6AE2D' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='grey')\nsns.despine()\nplt.title(\"SMOTEEEN\")","37c012e1":"rus = RandomUnderSampler(random_state=0)\nrus.fit(X, y)\nX_resampled, y_resampled = rus.fit_resample(X, y)\ncolors = ['#4E6B8A' if v == 0 else '#F26419' if v == 1 else '#F6AE2D' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='grey')\nsns.despine()\nplt.title(\"Random Undersampling\")","be62ae2e":"rus = NearMiss()\nrus.fit(X, y)\nX_resampled, y_resampled = rus.fit_resample(X, y)\ncolors = ['#4E6B8A' if v == 0 else '#F26419' if v == 1 else '#F6AE2D' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='grey')\nsns.despine()\nplt.title(\"Near Miiss\")","e47a67e6":"bank = pd.read_csv('..\/input\/bank-marketing\/bank.csv')","1efd8913":"bank.shape","d659910d":"sns.countplot(x=\"y\", data=bank)","03543555":"bank.y.value_counts()","4a9c09ce":"bank.head()","c48ed947":"bank[\"default\"] = bank[\"default\"].map({\"no\":0,\"yes\":1})\n\nbank[\"housing\"] = bank[\"housing\"].map({\"no\":0,\"yes\":1})\n\nbank[\"loan\"] = bank[\"loan\"].map({\"no\":0,\"yes\":1})\n\nbank[\"y\"] = bank[\"y\"].map({\"no\":0,\"yes\":1})\n\nbank.education = bank.education.map({\"primary\": 0, \"secondary\":1, \"tertiary\":2})\n\nbank.month = pd.to_datetime(bank.month, format = \"%b\").dt.month","943053eb":"# Let's remove a few features that are not really relevant for the purposes of our task\nbank.drop([\"poutcome\", \"contact\"], axis = 1, inplace = True)\nbank.dropna(inplace = True)","b3ece1ca":"bank = pd.get_dummies(bank, drop_first = True)","e19e3c7e":"# The final features\nbank.columns","e738fe72":"# Separate the target variable from the rest of the feautures\n\nX = bank.drop(\"y\", axis = 1)\ny = bank.y","b08a739f":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, random_state = 1, stratify=y)","dec47144":"# Sanity check for class distribution in train and test.\nfig, axs = plt.subplots(1,2, figsize=(12,4))\nsns.countplot(x='y', data=pd.DataFrame(y_train), ax=axs[0])\nsns.countplot(x='y', data=pd.DataFrame(y_test), ax=axs[1])\naxs[0].title.set_text('Train')\naxs[1].title.set_text('Test')","e7398761":"y_test.value_counts()","6d1fba57":"# a helper function to draw confusion matrices\ndef draw_cm(y_test, y_pred):\n  cm = confusion_matrix(y_test, y_pred)\n  cm_norm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n  df_cm = pd.DataFrame(cm_norm)\n  plt.figure(figsize = (6,4))\n  sns.heatmap(df_cm, annot=True, cmap=\"Blues\")\n  plt.xlabel(\"Predicted class\")\n  plt.ylabel(\"True class\")\n  plt.show()\n  print(\"Accuracy: {0:.3f}\".format(accuracy_score(y_test, y_pred)))\n  print(\"Recall: {0:.3f}\".format(recall_score(y_test, y_pred)))","250a0d7a":"lr = LogisticRegression(solver='liblinear',random_state=1)\nlr.fit(X_train, y_train)","cc8c63fd":"y_pred = lr.predict(X_test)\nprint(\"Accuracy: {0:.3f}\".format(accuracy_score(y_test, y_pred)))","39365b5f":"print(classification_report(y_test, y_pred))","472149e1":"print(confusion_matrix(y_test, y_pred))\ndraw_cm(y_test, y_pred)","9ec73a07":"ros = RandomOverSampler(sampling_strategy='minority', random_state=1)\n\nX_train_ros, y_train_ros = ros.fit_sample(X_train, y_train)\nnp.bincount(y_train_ros)","aad58a57":"lr = LogisticRegression(solver='liblinear',random_state=1)\nlr.fit(X_train_ros, y_train_ros)\n\ny_pred = lr.predict(X_test)","85ce7805":"draw_cm(y_test, y_pred)","56a09c08":"from imblearn.over_sampling import SMOTE","34a47007":"smt = SMOTE(random_state=1)\n\nX_train_sm, y_train_sm = smt.fit_sample(X_train, y_train)\nnp.bincount(y_train_sm)","23ceec22":"lr = LogisticRegression(solver='liblinear',random_state=1)\nlr.fit(X_train_sm, y_train_sm)\n\ny_pred = lr.predict(X_test)","671c2ffd":"draw_cm(y_test, y_pred)","911fe8c0":"rus = RandomUnderSampler(sampling_strategy='majority', random_state=1)\n\nX_train_rus, y_train_rus = rus.fit_sample(X_train, y_train)\nnp.bincount(y_train_rus)","70e0df86":"lr = LogisticRegression(solver='liblinear',random_state=1)\nlr.fit(X_train_rus, y_train_rus)\n\ny_pred = lr.predict(X_test)","51f92fe8":"draw_cm(y_test, y_pred)","bb1ff19b":"st = SMOTETomek(random_state=1)\nX_train_st, y_train_st = st.fit_sample(X_train, y_train)\nnp.bincount(y_train_st)","d54e16ee":"lr = LogisticRegression(solver='liblinear',random_state=1)\nlr.fit(X_train_st, y_train_st)\n\ny_pred = lr.predict(X_test)","e5d3505b":"draw_cm(y_test, y_pred)","60b03533":"# Wrapping up\n\nIn this notebook, we have presented a few approaches to handle imbalanced data. We've seen how misleading can be such a metric as Accuracy in case of class imbalance. We've also given a closer look to differences between some resampling techniques. Finally, we've practiced to apply these methods on a real data. Hopefully, you've found this kernel usefull and easy to follow.","b0b38ba8":"It's not uncommon for data scientists to work on imabalnced data, i.e. such data where classes are not uniformly distributed and one or two classes present a vast majority. Actually, most of classification data is usually imbalanced. To name but a few: medical data to diagnose a condition, fraud detection data, churn client data etc. \n\nThe problem here is that machine learning algorithms may overfit on the majority class and totally ignore the minor classes. At the same time, it is the minority class samples (a rare disease or a fraudulent transaction) that we are interested in detecting! One of the ways to overcome the problem of imbalanced data is resampling, i.e. changing the number of samples in the different classes. There are at least three ways to resample data:\n\n- Oversampling: adding the minory class samples.\n- Undersampling: deleting the majority class samples.\n- Combination of oversampling and udnersampling.\n\nIn this notebook we will examine some popular resampling techniques as well as discover why **Accuracy** is not always a good evaluation metric.\n\nFirst, we will look at different resampling methods in action and plot the results. Afterwards we are going to create a classification model on imbalanced data with and without resampling. ","6a5afffe":"![picture.jpg](attachment:picture.jpg)\n\n##### What I dream of is the art of balance (Henri Matisse)","640c5469":"# <font color='navy'> Resampling on synthetic data\n    \nLet's create some imbalanced data to demonstrate the differene between resampling methods. ","bf2296f9":"![resample_eng.png](attachment:resample_eng.png)","a60c41d5":"## Baseline: logistic regression on imbalanced dataset  ","9ac4168e":"Wow great! 89% of accurate predictions with a simple model. But should we really trust this metric? Let's take a closer look at the performance of the classifier. A confusion matrix and a classification report will be quite handy in this case.","bce49aa2":"Some other categorical variables will be encode with the help of ohe-hot encoding, i.e. each category of a feature will be now represented as a separate column.","dc8ef6ed":"Now let's look at the distribution of the target variable.","f551db59":"## <font color='scarlett'>  Undersampling\n    \n### <font color='scarlett'> Random Undersampling\n\nRandomly delete some objects of major class.","041a5295":"Another undersampling method - Near Miss - deletes the major class samples for which the average distance to the N closest samples of the minor class is the smallest.","c5c7d11f":"As we can see, there are some categorical features that need to be encoded.","593d2b34":"A more advanced method - SMOTE (Synthetic Minority Oversampling Technique) - doesn't just duplicate an existing sample. SMOTE generates a new sample considering its k neareast-neighbors.","91b28c4e":"# <font color='purple'> Resampling on real data\n\nNow that we have a basic idea how different resampling approaches work, let's try to apply this knowledge in a real classification task. We are provided with the information about the results of a bank marketing campaign. Our __task__ is to predict whether the client signs a term deposit.","a4be0db7":"First, let's try the most naive approach - random oversampling. This will create a duplicate samples of a minor class.","d19ff7fa":"### <font color='green'> SMOTE\n\nSMOTE generates new minor class samples by means of interpolation.","a3bede28":"The plot shows that SMOTE generates many \"noisy\" samples that are placed between the outliers and the true minor samples.\n\nTo solve this problem, one may use any of \"cleaning\" undersampling methods. For instance, SMOTEEN (SMOTE + Edited Nearest Neighbours) generates new samples as a vanila SMOTE, but then deletes those which class is different from the k-nearest neighbours' class.","dd738449":"### Load libraries","a82bd04a":"It's clear that the data is imbalanced. There is much more information on people who declined to sign a deposit.\n\nLet's take a look on a few samples in our dataset.","3d825ac9":"Here it is! Recall for class 1 - the class we are actually interested in - is lower 20%. What does that mean?\n\nWell, let's imagine that this classifier is used in production, i.e. the marketing dept is relying on the model to identify people that would likely sign a term deposit. Do you see it now? We have effectively missed 80% of the potential clients!\n\nIn order to minimize the number of such misses, we need to pay closer attention to recall rather than relying solely on accuracy.","852a68c6":"## <font color='green'> Oversampling\n    \n### <font color='green'> Random Oversampling\n\nThis approach simply duplicates existing samples of a minor class.","68e74a80":"The plot below shows how random undersampling, i.e. random deletion of major class samples, work.","c25a4c44":"## <font color='navy'> Combination of oversampling \u0438 undersampling\n    \nSMOTE with Tomek's links:\n1. Generate new samples with SMOTE\n2. Delete objects that create Tomek's link. A Tomek\u2019s link exist if the two samples are the nearest neighbors of each other. ","94f2daae":"__NB__: Don't forget to use stratification when splitting the data into train and test. This will help to preserve the same class distribution as in the whole dataset.","c9ef8749":"# <font color='scarlett'> The right way to use resampling: <\/font>\n\n1. Split your data in train and test sets.\n2. Resample __only__ the train set!\n3. Estimate the classification quality on the test set.\n\nViolating this procedure may lead to an inadequate evaluation results.\n\nFor example, if you first resample and split your data, the same sample may be found in both train and test sets. This would be a leakage of data - the model is tested on an object that was present in the train."}}