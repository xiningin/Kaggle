{"cell_type":{"917d36a8":"code","38fec1b8":"code","02756185":"code","f65ea7f4":"code","c2c7bb90":"code","6d025caa":"code","7e174134":"code","4b2852dd":"code","2ba9bc1c":"code","c3e192a5":"code","909dbfa2":"code","3f74fc98":"code","d3b47189":"code","50129a67":"code","39cb02ef":"code","1c311a46":"code","19313c53":"code","26cf453c":"code","96edaa29":"code","3c5571d3":"code","d7b463dc":"code","251080e6":"code","003216d8":"code","9913eebb":"code","889e7208":"code","0de61154":"code","0dc430d2":"code","c724d9e1":"code","6c9e6baf":"code","bd91cc15":"code","e2290268":"code","3678295e":"code","74c83f56":"markdown","4b049577":"markdown","460095cc":"markdown","82440dde":"markdown","a2ae7623":"markdown","a95bf581":"markdown","8abca8c5":"markdown","e6e66315":"markdown","e0c1b0ca":"markdown","aed95d14":"markdown","e69055b4":"markdown","223e8e5f":"markdown","385bcbca":"markdown","f0f4cc14":"markdown","eee35ade":"markdown","990dbd8e":"markdown","dd887ba7":"markdown"},"source":{"917d36a8":"import os\nprint(os.listdir(\"..\/input\"))","38fec1b8":"# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Wrangling\nimport pandas as pd\n\n# Exploring\nimport scipy.stats as stats\n\n# Visualizing\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('classic')\n\n# Modeling\n# THIS LINE YIELDED AN ERROR. I RESEARCHED THE ERROR, BUT DID NOT WANT TO DO THE PIP INSTALL \n# REQUIRED TO FIX IT.\n# import statsmodels.api as sm \n\nfrom scipy.stats import pearsonr\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error","02756185":"train = pd.read_csv(\"..\/input\/train.csv\")\nprint(\"Train columns:  %s\" % list(train.columns))\nprint(\"Train dimensions (rows, columns):\", train.shape)\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Test columns:  %s\" % list(test.columns))\nprint(\"Test dimensions (rows, columns):\", test.shape)","f65ea7f4":"train.isnull().sum()","c2c7bb90":"train.loc[train['y'].isnull()]","6d025caa":"train.loc[train['x'] > 99]","7e174134":"train.drop(labels=213, inplace=True)","4b2852dd":"train.loc[train['x'] > 99]","2ba9bc1c":"train.describe()","c3e192a5":"test.isnull().sum()","909dbfa2":"train = pd.DataFrame(train)\ntest = pd.DataFrame(test)","3f74fc98":"train.head()","d3b47189":"print(\"Train columns:  %s\" % list(train.columns))\nprint(\"Train dimensions (rows, columns):\", train.shape)\nprint(\"Test columns:  %s\" % list(test.columns))\nprint(\"Test dimensions (rows, columns):\", test.shape)","50129a67":"X_train = train.drop(columns='y')\nprint(\"X_train:\")\nprint(type(X_train))\nprint(X_train.head())\nprint()\ny_train = train.drop(columns='x')\nprint(\"y_train\")\nprint(type(y_train))\nprint(y_train.head())\nprint()\nX_test = test.drop(columns='y')\nprint(\"X_test:\")\nprint(type(X_test))\nprint(X_test.head())\nprint()\ny_test = test.drop(columns='x')\nprint(\"y_test\")\nprint(type(y_test))\nprint(y_test.head())\nprint()","39cb02ef":"print(X_train.isnull().sum())\nprint(y_train.isnull().sum())\nprint(X_test.isnull().sum())\nprint(y_test.isnull().sum())","1c311a46":"if X_train.shape[0] == y_train.shape[0]:\n    print(\"X & y train rows ARE equal\")\nelse:\n    print(\"X & y train rows ARE NOT equal\")\n\n\nif X_test.shape[0] == y_test.shape[0]:\n    print(\"X & y test rows ARE equal\")\nelse:\n    print(\"X & y test rows ARE NOT equal\")\n\nif train.shape[1] == test.shape[1]:\n    print(\"Number of columns in train & test ARE equal\")\nelse:\n    print(\"Number of columns in train & test ARE NOT equal\")\n\ntrain_split = train.shape[0] \/ (train.shape[0] + test.shape[0])\ntest_split = test.shape[0] \/ (train.shape[0] + test.shape[0])\n\nprint(\"Train Split: %.2f\" % train_split)\nprint(\"Test Split: %.2f\" % test_split)","19313c53":"with sns.axes_style('white'):\n    j = sns.jointplot(\"x\", \"y\", data=train, kind='reg', height=5);\n    j.annotate(stats.pearsonr)\nplt.show()","26cf453c":"# This is roughly equivalent to sns.jointplot, but we see here that we have the\n# flexibility to customize the type of the plots in each position.\n\ng = sns.PairGrid(train)\ng.map_diag(plt.hist)\ng.map_offdiag(plt.scatter);","96edaa29":"plt.figure(figsize=(8,4))\nsns.heatmap(train.corr(), cmap='Blues', annot=True)","3c5571d3":"# pearsonr(X_train, y_train)","d7b463dc":"# ols_model = sm.OLS(y_train, X_train)\n# fit = ols_model.fit()\n# fit.summary()","251080e6":"# Create linear regression objects\nlm1 = LinearRegression()\nprint(lm1)","003216d8":"lm1.fit(X_train[['x']], y_train)\nprint(lm1)\n\nlm1_y_intercept = lm1.intercept_\nprint(lm1_y_intercept)\n\nlm1_coefficients = lm1.coef_\nprint(lm1_coefficients)","9913eebb":"print('Univariate - y = b + m * exam1')\nprint('    y-intercept (b): %.2f' % lm1_y_intercept)\nprint('    coefficient (m): %.2f' % lm1_coefficients[0])\nprint()","889e7208":"y_pred_lm1 = lm1.predict(X_train[['x']])","0de61154":"mse_lm1 = mean_squared_error(y_train, y_pred_lm1)\nprint(\"lm1\\n  mse: {:.3}\".format(mse_lm1)) ","0dc430d2":"r2_lm1 = r2_score(y_train, y_pred_lm1)\n\nprint('  {:.2%} of the variance in the y can be explained by x.'.format(r2_lm1))","c724d9e1":"plt.scatter(y_pred_lm1, y_pred_lm1 - y_train, c='g', s=40)\nplt.hlines(y=0, xmin=50, xmax=100)\nplt.title(\"Residual plot\")\nplt.ylabel('Residuals')","6c9e6baf":"# Make predictions using the testing set\ny_pred_test = lm1.predict(X_test[['x']])","bd91cc15":"mse = mean_squared_error(y_test, y_pred_test)\n\nprint(\"Mean squared error: %.2f\" % mse)","e2290268":"r2 = r2_score(y_test, y_pred_test)\n\nprint('{:.2%} of the variance in y can be explained by x.'\n      .format(r2))","3678295e":"plt.scatter(y_pred_test, y_pred_test - y_test, c='g', s=40)\nplt.hlines(y=0, xmin=50, xmax=100)\nplt.title(\"Residual plot\")\nplt.ylabel('Residuals')","74c83f56":"# Now try on test data.","4b049577":"# Row 213 is obviously an outlier, probably a data entry error. Let's drop it.","460095cc":"# Split into X_train, y_train, X_test, and y_test.","82440dde":"# This is model is a good fit for this data.","a2ae7623":"# Linear Regression Model:","a95bf581":"# Heatmap","8abca8c5":"# Now that we have a model, let's make predictions.","e6e66315":"# The following results were obtained in my jupyter notebook with this same code.\n\nIf the data is good for modeling, then our residuals will have certain characteristics. These characteristics are:\n\n1. The data is \u201clinear\u201d. That is, the dependent variable is a linear function of independent variables and an error term e, and is largely dependent on characteristics 2-4. Think of the equation of a line in two dimensions: y = mx + b + e. yis the dependent or \u201cresponse\u201d variable, xis the input, mis the dimensional coefficient and bis the intercept (when x = 0). We can easily extend this \u201cline\u201d to higher dimensions by adding more inputs and coefficients, creating a hyperplane with the following form: y = a1\\*x1+ a2\\*x2+ \u2026 + an\\*xn\n2. Errors are normally distributed across the data. In other words, if you plotted the errors on a graph, they should take on the traditional bell-curve or Gaussian shape.\n3. There is \u201chomoscedasticity\u201d. This means that the variance of the errors is consistent across the entire dataset. We want to avoid situations where the error rate grows in a particular direction. \n4. The independent variables are actually independent and not collinear. We want to ensure independence between all of our inputs, otherwise our inputs will affect each other, instead of our response.\n\n\n\nOmnibus:\t0.191\t\nOmnibus\/Prob(Omnibus) \u2013 a test of the skewness and kurtosis of the residual. (Errors are normally distributed across the data. In other words, if you plotted the errors on a graph, they should take on the traditional bell-curve or Gaussian shape.) We hope to see a value close to *zero* which would indicate normalcy. \n\nProb(Omnibus):\t0.909\nThe Prob (Omnibus) performs a statistical test indicating the probability that the residuals are normally distributed. We hope to see something *close to 1* here. \n\nIn this case Omnibus is close to zero and the Prob (Omnibus) is \nclose to one. A linear regression approach will very likely yield good results.\n\nSkew:\t0.037\nSkew \u2013 a measure of data symmetry. We want to see something close to *zero*, indicating the residual distribution is normal. Note that this value also drives the Omnibus. This result has close to zero, and therefore a good, skew.\n\nKurtosis:\t2.945\t\nKurtosis \u2013 a measure of \u201cpeakiness\u201d, or curvature of the data. Higher peaks lead to greater Kurtosis. Greater Kurtosis can be interpreted as a tighter clustering of residuals around zero, implying a better model with few outliers. \n\nDurbin-Watson:\t1.967\nDurbin-Watson \u2013 tests for homoscedasticity. (There is \u201chomoscedasticity\u201d. This means that the variance of the errors is consistent across the entire dataset. We want to avoid situations where the error rate grows in a particular direction.). We hope to have a value *between 1 and 2*. In this case, the data yields a value of under 2.\n\nJarque-Bera (JB):\t0.246\nJarque-Bera (JB)\/Prob(JB) \u2013 like the Omnibus test in that it tests both skew and kurtosis. We hope to see in this test a confirmation of the Omnibus test. In this case, we do since the Omnibus test yielded 0.191 \n\nCond. No.\t1.00\nCondition Number \u2013 This test measures the sensitivity of a function\u2019s output as compared to its input. (The independent variables are actually independent and not collinear. We want to ensure independence between all of our inputs, otherwise our inputs will affect each other, instead of our response.) When we have multicollinearity, we can expect much higher fluctuations to small changes in the data, hence, we hope to see a relatively small number, something *below 30*. We only have one feature in this data, so we are not surprised to see a 1.00 here.","e0c1b0ca":"# Convert to dataframes.","aed95d14":"# Histogram + Scatterplots","e69055b4":"# Scipy: Pearson's Correlation\n# Because of the deprecation error that \"import statsmodels.api as sm\" gave, the following will not run in Kaggle.","223e8e5f":"# Check for null values.","385bcbca":"# Explore the data with a Scatterplot + Density Plots","f0f4cc14":"# 1. Prepare the Environment","eee35ade":"# Feature selection using statsmodels.OLS (Ordinary Least Squares) -- we only have one feature in this data, but I wanted to see the results of the ols on this contrived data.\n# Again... ols will not run because of the previously mentioned deprecation error.","990dbd8e":"# Read test and train data which is already split in this data.","dd887ba7":"# In Kaggle, ensure the correct files are in the input directory."}}