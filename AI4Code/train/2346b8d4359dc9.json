{"cell_type":{"70923e32":"code","ffdc6028":"code","3acc934e":"code","14c11448":"code","c57800ad":"code","d17892de":"code","fc3cfecc":"code","86f749a3":"code","ef1156f8":"code","aa882b53":"markdown"},"source":{"70923e32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport # library for automatic EDA\nfrom functools import partial\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display # display from IPython.display\n\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import cross_validate, learning_curve, train_test_split, RepeatedStratifiedKFold\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, FunctionTransformer, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ffdc6028":"# Importing the data and displaying some rows\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ny = df['Survived'].copy()\nX = df.drop(['Survived'], axis=1).copy()\ndisplay(df)","3acc934e":"report = ProfileReport(df)","14c11448":"display(report)","c57800ad":"cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1)","d17892de":"# Drop irrelevant features\nto_be_removed = ['PassengerId', 'Name', 'Ticket', 'Cabin']\nnumerical_features = ((X.dtypes == 'float') | (X.dtypes == 'int')) & ~(X.columns.isin(to_be_removed))\ncategorical_features = ~(numerical_features) & ~(X.columns.isin(to_be_removed))\n\npreprocessor = ColumnTransformer(\n    remainder = 'passthrough',\n    transformers=[\n        (\n            'numerical', \n            make_pipeline(SimpleImputer(strategy='median'), StandardScaler()), \n            numerical_features\n        ),\n        (\n            'categorical', \n            make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown=\"ignore\")), \n            categorical_features\n        ),\n        ('remove', 'drop', to_be_removed),\n    ]\n)\n\ncolumns = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Sex_female', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n\ndisplay(X, pd.DataFrame(preprocessor.fit_transform(X), columns=columns))","fc3cfecc":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\nfrom hyperopt.pyll.base import scope\n\nspace = dict(\n    learning_rate = hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n    n_estimators = scope.int(hp.qloguniform('n_estimators', np.log(50), np.log(500), np.log(10))),\n    max_depth = scope.int(hp.quniform('max_depth', 2, 15, 1)),\n)\n\ndef objective(params):\n        pipeline = make_pipeline(preprocessor, LGBMClassifier(boosting='gbdt', **params))\n        res = cross_validate(pipeline, X, y, scoring='accuracy', return_train_score=True, cv=cv, n_jobs=-1)\n\n        train_score = np.mean(res['train_score'])\n        cv_score = np.mean(res['test_score']) - np.std(res['test_score'])\n        \n        result = dict(\n            params=params,\n            train_loss = -train_score,\n            # Hyperopt-required keys\n            loss = -cv_score,\n            status = STATUS_OK,   \n        )\n        return result\n        \ntrials = Trials()\nbest = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=trials)\nbest","86f749a3":"clf = LGBMClassifier(\n    boosting='gbdt', \n    learning_rate=0.019,\n    max_depth=15,\n    n_estimators=223,\n)\npipeline = make_pipeline(preprocessor, clf)\n\nres = cross_validate(pipeline, X, y, scoring='accuracy', return_train_score=True, cv=cv, n_jobs=-1)\n\ntrain_score = np.mean(res['train_score'])\ncv_score = np.mean(res['test_score'])\n\ndisplay(\n    f'Mean train accuracy: {train_score:.3f}',\n    f'Mean \u0421V accuracy: {cv_score:.3f}', \n)","ef1156f8":"pipeline.fit(X, y)\n\nholdout = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\nholdout['Survived'] = pipeline.predict(holdout)\n\nsubmission = holdout[['PassengerId', 'Survived']]\n\nsubmission.to_csv('submission.csv', index=False)","aa882b53":"### Build preprocessing pipeline"}}