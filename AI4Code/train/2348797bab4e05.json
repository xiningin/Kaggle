{"cell_type":{"9d992f0a":"code","22d3f630":"code","0baf3868":"code","e0dacfef":"code","bcfc38db":"code","5061f3eb":"code","83e8a20e":"code","d2951f90":"code","90767d40":"code","bdf09cf1":"code","9394b773":"code","c62f3e8a":"code","c39de8a7":"code","405afcb0":"code","873bba7b":"code","fc45f49a":"code","b9e19c69":"code","07cf73af":"code","490b7a65":"markdown","e468fef4":"markdown","3857c8c4":"markdown","2c43108c":"markdown","66061884":"markdown"},"source":{"9d992f0a":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nimport xgboost as xgb\n\nfrom sklearn import metrics\nfrom sklearn.metrics import  mean_squared_error\n\nimport dateutil.easter as easter\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22d3f630":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\n\nholiday = pd.read_csv('..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv')\ngdp = pd.read_csv('..\/input\/tps-jan-2022-gdp-data-long-format\/gdp_long.csv')\n\nprint(train.shape)\nprint(test.shape)\nprint(sub.shape)\nprint(holiday.shape)\nprint(gdp.shape)","0baf3868":"gdp","e0dacfef":"df = pd.concat([train, test]).fillna(0)\n\ndf['type'] = df['country'].astype(str) + '_' + df['store'].astype(str) + '_' + df['product'].astype(str)\ndf['date'] = pd.to_datetime(df['date'])\ndf.head()","bcfc38db":"import random\nsmp = random.sample(df['type'].unique().tolist(), 3)\nprint(smp, '\\n')\n\nplt.figure(figsize = (15,6))\nfor s in smp:\n    \n    plt.plot(df[df.type == s]['date'], df[df.type == s]['num_sold'], label = s)\n    plt.legend()\n    plt.xticks(rotation = 45)\n","5061f3eb":"holiday['Date'] = pd.to_datetime(holiday['Date'])\nholiday","83e8a20e":"# Date Features\ndf['dow'] = df['date'].dt.dayofweek\ndf['month'] = df['date'].dt.month\ndf['day'] = df['date'].dt.day\ndf['week']= df['date'].dt.week\ndf['year'] = df['date'].dt.year\n\ndayofyear = df.date.dt.dayofyear\n\n# Seasonality features\nfor k in range(1, 20):\n    df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n    df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n   \n     \n# Merge with holiday\ndf = df.merge(holiday, how = 'left', left_on = ['country','date'], right_on = ['Country','Date']).drop(['Country','Date','LocalName','Fixed'], axis = 1).fillna('NA')\n\n# Merge with GDP\ndf = df.merge(gdp, on = ['year','country'], how = 'left')\n","d2951f90":"# Easter\neaster_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\ndf['days_from_easter'] = (df.date - easter_date).dt.days.clip(-3, 59)\ndf.loc[df['days_from_easter'].isin(range(12, 39)), 'days_from_easter'] = 12 # reduce overfitting\n#new_df.loc[new_df['days_from_easter'] == 59, 'days_from_easter'] = -3\n\n# Last Wednesday of June\nwed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                     2016: pd.Timestamp(('2016-06-29')),\n                                     2017: pd.Timestamp(('2017-06-28')),\n                                     2018: pd.Timestamp(('2018-06-27')),\n                                     2019: pd.Timestamp(('2019-06-26'))})\ndf['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n\n# First Sunday of November (second Sunday is Father's Day)\nsun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                     2016: pd.Timestamp(('2016-11-6')),\n                                     2017: pd.Timestamp(('2017-11-5')),\n                                     2018: pd.Timestamp(('2018-11-4')),\n                                     2019: pd.Timestamp(('2019-11-3'))})\ndf['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n\ndf.head()","90767d40":"le = LabelEncoder()\ncat_cols = ['country','store','product','type','Name']\n\nfor c in cat_cols:\n    df[c] = le.fit_transform(df[c])","bdf09cf1":"def smape(pred, obs):\n    pred = np.array(pred)\n    obs = np.array(obs)   \n    return 100 * np.mean(2 * abs(pred-obs)\/(abs(obs)+abs(pred)))","9394b773":"train = df[:len(train)]\ntest = df[-len(test):]\nprint(train.shape)\nprint(test.shape)\n\nX_train = train.drop(['row_id','date','num_sold','gdp'], axis = 1)\ny_train = np.log(train['num_sold'] \/ train['gdp'])\nX_test = test.drop(['row_id','date','num_sold','gdp'], axis = 1)\n\ncols = list( X_train.columns )\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","c62f3e8a":"# Calculating edges of target bins to be used for stratified split\ntarget_bin_edges = np.histogram_bin_edges(train[\"num_sold\"], bins=5)\nprint(target_bin_edges)\ntarget_bin_edges[0] = -np.inf\ntarget_bin_edges[-1] = np.inf\ntarget_bins = pd.cut(train[\"num_sold\"], target_bin_edges, labels=np.arange(5))\ntarget_bins.value_counts()","c39de8a7":"print(\"XGBoost version:\", xgb.__version__)\n\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nSEED = 42\nseed_everything(SEED)","405afcb0":"oof_pred = True\n\nif oof_pred:\n    oof = np.zeros(len(X_train))\n    preds = np.zeros(len(X_test))\n\n    skf = StratifiedKFold(n_splits =  5, shuffle = True, random_state = 2022)\n\n    for i, (idxT, idxV) in enumerate( skf.split(X_train, target_bins) ):\n        print('Fold:', i+1)\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n        clf = xgb.XGBRegressor(\n            n_estimators = 1000,\n    max_depth = 7,\n    learning_rate = 0.03,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    missing = -1,\n            reg_alpha =  1,\n    \n        )        \n        h = clf.fit(X_train.loc[idxT], y_train.loc[idxT], eval_metric = 'mae',\n                eval_set=[(X_train.loc[idxT], y_train.loc[idxT]),(X_train.loc[idxV],y_train.loc[idxV])],\n                verbose=200, early_stopping_rounds=50)\n    \n        oof[idxV] += np.exp(clf.predict(X_train.iloc[idxV])) * (train['gdp'].iloc[idxV])\n        preds += np.exp(clf.predict(X_test)) * test['gdp']\/skf.n_splits\n      \n    print('#'*20)\n    print ('XGB OOF CV RMSE =', mean_squared_error(train['num_sold'],oof)**0.5)\n    print ('XGB OOF CV SMAPE =', smape(train['num_sold'],oof))","873bba7b":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGB Most Important Features')\nplt.tight_layout()\nplt.show()","fc45f49a":"plt.hist(oof,bins=100)\nplt.title('XGB OOF')\nplt.show()","b9e19c69":"sub.num_sold = preds.values\nsub.to_csv('sub_xgb.csv',index=False)\n\nplt.hist(sub.num_sold, bins=100)\nplt.title('XGB Submission')\nplt.show()","07cf73af":"sub","490b7a65":"# XGB Modeling","e468fef4":"# Feature Engineering","3857c8c4":"### More advanced features","2c43108c":"# Pre-processing","66061884":"# Objective\n\nFor this challenge, you will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow you to try numerous different modeling approaches\n\n![image.png](attachment:76b162ca-d79f-4510-9c9e-9cb27603f259.png)\n\n- Files\n    * train.csv - the training set, which includes the sales data for each date-country-store-item combination.\n    * test.csv - the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.\n    * sample_submission.csv - a sample submission file in the correct format"}}