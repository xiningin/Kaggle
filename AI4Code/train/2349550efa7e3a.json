{"cell_type":{"acfd1ead":"code","70775111":"code","07b9d1b7":"code","e9ad6212":"code","952cc7f3":"code","bad17c5b":"code","453d6ef3":"code","e978cfb5":"code","eec280ef":"code","43bd25fe":"code","512efda8":"code","e0a33315":"code","5a852137":"code","191400fe":"code","3f11d369":"code","4d15105d":"code","b25ddc68":"code","33b3ec72":"code","f637c3e4":"code","abd47a77":"code","b73d235b":"code","a43fbbbd":"code","bf3a2fd5":"code","302fbf6c":"code","4bdb16dd":"code","31b76e46":"code","c694729a":"code","9e8a4017":"code","a1ebb852":"code","c5a7ef97":"code","15a47dc5":"code","cc6b5fe8":"code","303c690f":"code","85df8d81":"code","30591a89":"code","d7af84d2":"code","345cb4f9":"code","406540f3":"code","40c3cd7a":"code","05e685dc":"code","a7944431":"code","4905639b":"code","b575c8ab":"code","004a8c78":"code","b98d9603":"code","21e8b4c8":"code","c61cd5f5":"code","5ab69611":"code","4d950b44":"code","e94b70bf":"code","0811e073":"code","d7eb46e6":"code","b680ee13":"code","7cdf1085":"code","c4bf8c47":"code","415bb1b3":"code","3a483fb4":"code","ffc5e5c3":"code","6e8a028b":"code","6be8dd56":"code","54ca9293":"code","4bbcd08e":"code","d4d2e5a4":"code","0b3a443c":"code","5b8b9dd1":"code","0363f942":"code","76e7290e":"code","db18b59f":"code","e7e78087":"code","d8a25f2d":"code","d62e4a6a":"code","6f39dd1e":"code","fed3ec99":"code","9cbd0404":"code","aca9f9a7":"code","5ba8ab41":"code","9d4cbbdd":"code","0645c135":"code","d9f98823":"code","271b010c":"code","44e18e67":"code","723414e9":"code","0bde40b6":"markdown","d8aefcc7":"markdown","a38e0da9":"markdown","47f7eb12":"markdown","68566532":"markdown","9469a73c":"markdown","c2507a38":"markdown","f429117d":"markdown","50267494":"markdown","04ee5b4e":"markdown","aa08cef6":"markdown","19dac91c":"markdown","248a72e7":"markdown","cd0274ad":"markdown","1b76d0c1":"markdown","88a5b73e":"markdown","860f3880":"markdown","c6b185d0":"markdown","e74f993f":"markdown"},"source":{"acfd1ead":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style= 'darkgrid', palette='deep')\nimport warnings\nwarnings.filterwarnings('ignore')\nbins = range(0,100,10)\nimport os\nprint(os.listdir(\"..\/input\"))","70775111":"df = pd.read_csv('..\/input\/advertising.csv')","07b9d1b7":"df.head()","e9ad6212":"df.tail()","952cc7f3":"df.info()","bad17c5b":"df_feature = df.copy()","453d6ef3":"#Creating a user columns\ndf_user = pd.DataFrame(np.arange(0, len(df_feature)), columns=['user'])","e978cfb5":"df_feature = pd.concat([df_user, df_feature], axis=1)","eec280ef":"df_feature.groupby('Country')['Country'].unique().sort_values()","43bd25fe":"#Removing parentheses from Country\ndef removeAfterParentheses(string):\n    \"\"\"\n    input is a string \n    output is a string with everything after comma removed\n    \"\"\"\n    return string.split('(')[0].strip()\ndf_feature.Country = df_feature.Country.apply(removeAfterParentheses)","512efda8":"#Checking the remove parentheses \ndf_feature.groupby('Country')['Country'].unique().sort_values()","e0a33315":"countries = df_feature.groupby('Country')['Country'].unique().sort_values()","5a852137":"#Installing country_converter package\n!pip install country_converter --upgrade","191400fe":"#Extracting Countries continent\nimport country_converter as coco\ncc = coco.CountryConverter()\ncontinent = np.array([])\nfor i in range(0, len(df_feature)):\n    continent= np.append(continent, cc.convert(names=df_feature['Country'][i], to='Continent' ))","3f11d369":"df_feature['continent'] = pd.DataFrame(continent) ","4d15105d":"df_feature.columns","b25ddc68":"#Reorganizing the columns\ndf_feature = df_feature[['user','Daily Time Spent on Site', 'Age', 'Area Income',\n       'Daily Internet Usage', 'Ad Topic Line', 'City', 'Male', 'Country', 'continent',\n       'Timestamp', 'Clicked on Ad']]","33b3ec72":"#Installing date_converter package\n!pip install easy-date","f637c3e4":"#Converting string format to Datatime format \nimport date_converter\n\nfor i in range(0,len(df_feature)):\n    df_feature['Timestamp'][i] = date_converter.string_to_datetime(df_feature['Timestamp'][i], '%Y-%m-%d %H:%M:%S')\ntime_new = df_feature['Timestamp'].iloc[0]\ndf_feature['Hour'] = df_feature['Timestamp'].apply(lambda time_new: time_new.hour)\ndf_feature['Month'] = df_feature['Timestamp'].apply(lambda time_new: time_new.month)\ndf_feature['Day'] = df_feature['Timestamp'].apply(lambda time_new: time_new.weekday())","abd47a77":"df_feature.info()","b73d235b":"df_feature.head()","a43fbbbd":"#How many percentage the user have been spending on site? Creating % spending time columns\ndf_feature.columns\ndf_feature['% spending time'] =  ((df_feature['Daily Time Spent on Site'] \/ df_feature['Daily Internet Usage']) * 100 )\ndf_feature = df_feature[['user', 'Daily Time Spent on Site','Daily Internet Usage',\n                                        '% spending time','Age','Area Income',\n                                        'Ad Topic Line', 'City', 'Male', 'Country',\n                                        'continent', 'Timestamp','Hour', 'Month', 'Day','Clicked on Ad']]","bf3a2fd5":"df_feature.head()","302fbf6c":"def bar_chart(feature1, feature2):\n    g = pd.crosstab(df_feature[feature1], df_feature[feature2]).plot(kind='bar', figsize=(10,10), rot = 45)\n    ax = g.axes\n    for p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 \/ df.shape[0]:.2f}%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='gray', rotation=0, xytext=(0, 10),\n         textcoords='offset points') \n    plt.grid(b=True, which='major', linestyle='--')\n    plt.legend(['Clicked on Ad',\"Did not Clicked on Ad\"])\n    plt.title('Clicked on Ad for {}'.format(feature1))\n    plt.xlabel('{}'.format(feature1))\n    plt.tight_layout()\n    plt.ylabel('Quantity')\n    \ndef bar_chart_group(feature):\n    g = pd.crosstab(pd.cut(df_feature[feature], bins), df_feature['Clicked on Ad']).plot(kind='bar', figsize=(10,10), rot = 45)\n    ax = g.axes\n    for p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 \/ df.shape[0]:.2f}%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='gray', rotation=0, xytext=(0, 10),\n         textcoords='offset points') \n    plt.grid(b=True, which='major', linestyle='--')\n    plt.legend(['Clicked on Ad',\"Did not Clicked on Ad\"])\n    plt.title('Clicked on Ad for {}'.format(feature))\n    plt.xlabel('{}'.format(feature))\n    plt.tight_layout()\n    plt.ylabel('Quantity')\n\ndef bar_chart_hour(feature):\n    bins_hour = np.arange(0,25,12)\n    g = pd.crosstab(pd.cut(df_feature[feature], bins_hour), df_feature['Clicked on Ad']).plot(kind='bar', figsize=(10,10), rot = 45)\n    ax = g.axes\n    for p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 \/ df.shape[0]:.2f}%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='gray', rotation=0, xytext=(0, 10),\n         textcoords='offset points') \n    plt.grid(b=True, which='major', linestyle='--')\n    plt.legend(['Clicked on Ad',\"Did not Clicked on Ad\"])\n    plt.title('Clicked on Ad for {}'.format(feature))\n    plt.xlabel('{}'.format(feature))\n    plt.tight_layout()\n    plt.ylabel('Quantity')","4bdb16dd":"#Taking latitude and longitude\nfrom geopy.geocoders import Nominatim\nlat = np.array([])\nlon = np.array([])\ncountry = np.array([])\n\nfor i in range(0, len(countries)):\n    geolocator = Nominatim(user_agent='tito', timeout=100)\n    location = geolocator.geocode(countries.index[i], timeout=100)\n    lat = np.append(lat, location.latitude)\n    lon = np.append(lon, location.longitude)\n    country = np.append(country, countries.index[i])","31b76e46":"#Importing Map\nimport folium\ndata = pd.DataFrame({\n'lat':lat,\n'lon':lon,\n'name':country})\ndata.head()    \n\nm = folium.Map(location=[20, 0], tiles=\"Mapbox Bright\", zoom_start=2 , )\ncountry_map = list(zip(data['name'].values, data['lat'].values, data['lon'].values))\n# add features\nfor country_map in country_map:\n    folium.Marker(\n        location=[float(country_map[1]), float(country_map[2])],\n        popup=folium.Popup(country_map[0], parse_html=True),\n        icon=folium.Icon(icon='home')\n    ).add_to(m)   \n    \nm  ","c694729a":"bar_chart('Male','Clicked on Ad')\n","9e8a4017":"bar_chart('continent', 'Clicked on Ad')\n","a1ebb852":"bar_chart('Day', 'Clicked on Ad')\n","c5a7ef97":"bar_chart('Month', 'Clicked on Ad')\n","15a47dc5":"bar_chart_group('Age')\n","cc6b5fe8":"bar_chart_group('% spending time')\n","303c690f":"bar_chart_hour('Hour')","85df8d81":"df_feature.drop(['user', 'Male', 'Clicked on Ad'], axis=1).hist(figsize=(10,10))","30591a89":"df_feature.groupby('continent')['Area Income'].sum().sort_values().plot(kind='bar', figsize=(10,10), rot=45)\nplt.title('Area income per Continent')\nplt.grid(b=True, which='major', linestyle='--')\nplt.tight_layout()\nplt.ylabel('Quantity')","d7af84d2":"## Correlation with independent Variable \ndf2 = df_feature.drop(['user', 'Clicked on Ad', 'Ad Topic Line', 'City'], axis=1)\ndf2.corrwith(df_feature['Clicked on Ad']).plot.bar(\n        figsize = (10, 10), title = \"Correlation with Clicked on Ad\", fontsize = 15,\n        rot = 45, grid = True)","345cb4f9":"sns.set(style=\"white\")\n# Compute the correlation matrix\ncorr = df2.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","406540f3":"## Pie Plots \ndf_feature.columns\ndf2 = df_feature.drop(['user', 'Daily Time Spent on Site', 'Daily Internet Usage',\n       '% spending time', 'Age', 'Area Income', 'Ad Topic Line', 'City' , 'Country',\n       'Timestamp', 'Hour', 'Clicked on Ad'], axis=1)\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Pie Chart Distributions', fontsize=20)\nfor i in range(1, df2.shape[1] + 1):\n    plt.subplot(6, 3, i)\n    f = plt.gca()\n    f.axes.get_yaxis().set_visible(False)\n    f.set_title(df2.columns.values[i - 1])\n   \n    values = df2.iloc[:, i - 1].value_counts(normalize = True).values\n    index = df2.iloc[:, i - 1].value_counts(normalize = True).index\n    plt.pie(values, labels = index, autopct='%1.1f%%')\n    plt.axis('equal')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","40c3cd7a":"df_feature.describe()","05e685dc":"df_feature['Clicked on Ad'].value_counts()","a7944431":"countNotClicked = len(df_feature[df_feature['Clicked on Ad'] == 0])     \ncountClicked  = len(df_feature[df_feature['Clicked on Ad'] == 1]) \nprint('Percentage of not Clicked on Ad: {:.2f}%'.format((countNotClicked\/len(df_feature)) * 100)) \nprint('Percentage of Clicked on Ad: {:.2f}%'.format((countClicked\/len(df_feature)) * 100))","4905639b":"df_feature.groupby(df_feature['Clicked on Ad']).mean().head()","b575c8ab":"sns.heatmap(df_feature.isnull(), yticklabels=False, cbar=False, cmap='viridis')","004a8c78":"df_feature.isnull().any()","b98d9603":"df_feature.isnull().sum()","21e8b4c8":"null_percentage = (df_feature.isnull().sum()\/len(df_feature) * 100)\nnull_percentage = pd.DataFrame(null_percentage, columns = ['Percentage Null Values (%)'])","c61cd5f5":"null_percentage","5ab69611":"df_feature.columns\nX = df_feature.drop(['user', 'Clicked on Ad', 'Ad Topic Line', 'City',\n              'Country', 'Timestamp'], axis=1)\ny = df_feature['Clicked on Ad']","4d950b44":"#Get Dummies\nX = pd.get_dummies(X)","e94b70bf":"#Avoiding Dummies Trap\nX = X.drop(['continent_not found'], axis=1)\nX.isnull().sum()","0811e073":"X.head()","d7eb46e6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=0) ","b680ee13":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = pd.DataFrame(sc_x.fit_transform(X_train), columns=X.columns.values)\nX_test = pd.DataFrame(sc_x.transform(X_test), columns=X.columns.values)","7cdf1085":"## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression(random_state = 0, penalty = 'l1')\nlr_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = lr_classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression (Lasso)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])","c4bf8c47":"## K-Nearest Neighbors (K-NN)\n#Choosing the K value\nerror_rate= []\nfor i in range(1,40):\n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(np.mean(error_rate))","415bb1b3":"## K-Nearest Neighbors (K-NN)\nfrom sklearn.neighbors import KNeighborsClassifier\nkn_classifier = KNeighborsClassifier(n_neighbors=35, metric='minkowski', p= 2)\nkn_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = kn_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['K-Nearest Neighbors (minkowski)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","3a483fb4":"## SVM (Linear)\nfrom sklearn.svm import SVC\nsvc_linear_classifier = SVC(random_state = 0, kernel = 'linear', probability= True)\nsvc_linear_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = svc_linear_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","ffc5e5c3":"## SVM (rbf)\nfrom sklearn.svm import SVC\nsvc_rbf_classifier = SVC(random_state = 0, kernel = 'rbf', probability= True)\nsvc_rbf_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = svc_rbf_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","6e8a028b":"## Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngb_classifier = GaussianNB()\ngb_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = gb_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Naive Bayes (Gaussian)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","6be8dd56":"## Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\ndt_classifier.fit(X_train, y_train)\n\n#Predicting the best set result\ny_pred = dt_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Decision Tree', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","54ca9293":"#Installing pydotplus package\n!pip install pydotplus","4bbcd08e":"## Plotting Decision Tree\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(dt_classifier, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())\n","d4d2e5a4":"## Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'gini')\nrf_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = rf_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","0b3a443c":"## Ada Boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nad_classifier = AdaBoostClassifier()\nad_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = ad_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Ada Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","5b8b9dd1":"##Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\ngr_classifier = GradientBoostingClassifier()\ngr_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = gr_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Gradient Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","0363f942":"##Xg Boosting\nfrom xgboost import XGBClassifier\nxg_classifier = XGBClassifier()\nxg_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = xg_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Xg Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","76e7290e":"##Ensemble Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\nvoting_classifier = VotingClassifier(estimators= [('lr', lr_classifier),\n                                                  ('kn', kn_classifier),\n                                                  ('svc_linear', svc_linear_classifier),\n                                                  ('svc_rbf', svc_rbf_classifier),\n                                                  ('gb', gb_classifier),\n                                                  ('dt', dt_classifier),\n                                                  ('rf', rf_classifier),\n                                                  ('ad', ad_classifier),\n                                                  ('gr', gr_classifier),\n                                                  ('xg', xg_classifier),],\nvoting='soft')","db18b59f":"for clf in (lr_classifier,kn_classifier,svc_linear_classifier,svc_rbf_classifier,\n            gb_classifier, dt_classifier,rf_classifier, ad_classifier, gr_classifier, xg_classifier,\n            voting_classifier):\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","e7e78087":"# Predicting Test Set\ny_pred = voting_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Ensemble Voting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)  ","d8a25f2d":"results","d62e4a6a":"#The Best Classifier\nprint('The best classifier is:')\nprint('{}'.format(results.sort_values(by='Accuracy',ascending=False).head(5)))","6f39dd1e":"#Applying K-fold validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=svc_linear_classifier, X=X_train, y=y_train,cv=10)\naccuracies.mean()\naccuracies.std()\nprint(\"SVM (Linear) Accuracy: %0.3f (+\/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","fed3ec99":"## EXTRA: Confusion Matrix\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred)) ","9cbd0404":"#Plotting Cumulative Accuracy Profile (CAP)\ny_pred_proba = svc_linear_classifier.predict_proba(X=X_test)\nimport matplotlib.pyplot as plt\nfrom scipy import integrate\ndef capcurve(y_values, y_preds_proba):\n    num_pos_obs = np.sum(y_values)\n    num_count = len(y_values)\n    rate_pos_obs = float(num_pos_obs) \/ float(num_count)\n    ideal = pd.DataFrame({'x':[0,rate_pos_obs,1],'y':[0,1,1]})\n    xx = np.arange(num_count) \/ float(num_count - 1)\n    \n    y_cap = np.c_[y_values,y_preds_proba]\n    y_cap_df_s = pd.DataFrame(data=y_cap)\n    y_cap_df_s = y_cap_df_s.sort_values([1], ascending=False).reset_index(level = y_cap_df_s.index.names, drop=True)\n    \n    print(y_cap_df_s.head(20))\n    \n    yy = np.cumsum(y_cap_df_s[0]) \/ float(num_pos_obs)\n    yy = np.append([0], yy[0:num_count-1]) #add the first curve point (0,0) : for xx=0 we have yy=0\n    \n    percent = 0.5\n    row_index = int(np.trunc(num_count * percent))\n    \n    val_y1 = yy[row_index]\n    val_y2 = yy[row_index+1]\n    if val_y1 == val_y2:\n        val = val_y1*1.0\n    else:\n        val_x1 = xx[row_index]\n        val_x2 = xx[row_index+1]\n        val = val_y1 + ((val_x2 - percent)\/(val_x2 - val_x1))*(val_y2 - val_y1)\n    \n    sigma_ideal = 1 * xx[num_pos_obs - 1 ] \/ 2 + (xx[num_count - 1] - xx[num_pos_obs]) * 1\n    sigma_model = integrate.simps(yy,xx)\n    sigma_random = integrate.simps(xx,xx)\n    \n    ar_value = (sigma_model - sigma_random) \/ (sigma_ideal - sigma_random)\n    \n    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n    ax.plot(ideal['x'],ideal['y'], color='grey', label='Perfect Model')\n    ax.plot(xx,yy, color='red', label='User Model')\n    ax.plot(xx,xx, color='blue', label='Random Model')\n    ax.plot([percent, percent], [0.0, val], color='green', linestyle='--', linewidth=1)\n    ax.plot([0, percent], [val, val], color='green', linestyle='--', linewidth=1, label=str(val*100)+'% of positive obs at '+str(percent*100)+'%')\n    \n    plt.xlim(0, 1.02)\n    plt.ylim(0, 1.25)\n    plt.title(\"CAP Curve - a_r value =\"+str(ar_value))\n    plt.xlabel('% of the data')\n    plt.ylabel('% of positive obs')\n    plt.legend()     ","aca9f9a7":"capcurve(y_test,y_pred_proba[:,1])","5ba8ab41":"#Permutation Importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(svc_linear_classifier, random_state=0).fit(X_test,y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","9d4cbbdd":"# Analyzing Coefficients\npd.concat([pd.DataFrame(X_train.columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(svc_linear_classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","0645c135":"# Recursive Feature Elimination\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC\n\n# Model to Test\nclassifier = SVC(random_state = 0, kernel = 'linear', probability= True)\n\n# Select Best X Features\nrfe = RFE(classifier, n_features_to_select=None)\nrfe = rfe.fit(X_train, y_train)","d9f98823":"# summarize the selection of the attributes\nprint(rfe.support_)\nprint(rfe.ranking_)\nX_train.columns[rfe.support_]","271b010c":"# Fitting Model to the Training Set\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'linear', probability= True)\nclassifier.fit(X_train[X_train.columns[rfe.support_]], y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM RFE (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","44e18e67":"# Formatting Final Results\ndf_feature.columns\nuser_identifier = df_feature['user']\nfinal_results = pd.concat([y_test, user_identifier], axis = 1).dropna()\nfinal_results['predicted'] = y_pred\nfinal_results = final_results[['user', 'Clicked on Ad', 'predicted']].reset_index(drop=True)","723414e9":"final_results.head()","0bde40b6":"# Accuracy Paradox","d8aefcc7":"# Feature Engineering","a38e0da9":"# Looking for null values","47f7eb12":"The country converter (coco) - a Python package for converting country names between different classification schemes.\nMore about country_converter [here](https:\/\/github.com\/konstantinstadler\/country_converter)","68566532":"# Cumulative Accuracy Profile (CAP)","9469a73c":"# Define X and y","c2507a38":"For feature selection, we wil use the Recursive Feature Elimination (RFE). More about Recursive Feature Elimination (RFE) [here.](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html)","f429117d":"# Model Building \n### Comparing Models","50267494":"# Data Analyses","04ee5b4e":"# Feature Scaling","aa08cef6":"Accuracy is not the best way to measure a perfomance of model. It\u00b4s because Accuracy Paradox. More about Accuracy Paradox [here.](https:\/\/towardsdatascience.com\/accuracy-paradox-897a69e2dd9b)","19dac91c":"It\u2019s often said that \u201cdata is the fuel of machine learning.\u201d This isn\u2019t quite true: data is like the crude oil of machine learning which means it has to be refined into features\u200a\u2014\u200apredictor variables\u200a\u2014\u200ato be useful for training a model. Without relevant features, you can\u2019t train an accurate model, no matter how complex the machine learning algorithm. The process of extracting features from a raw dataset is called feature engineering. More about Feature Engineering [here](https:\/\/towardsdatascience.com\/feature-engineering-what-powers-machine-learning-93ab191bcc2d)","248a72e7":"# Splitting the Dataset into the Training Set and Test Set","cd0274ad":"#Creating continent columns\n\nThe column Country has 237 unique values. And so on, these countries can be organized in group.\nFor this, we will create a new colum called continent.\nBut firstly is necessary remove parantheses (as Antarctica) for the package country convert work properly.\nLet's take out it from our data. ","1b76d0c1":"# Feature Selection","88a5b73e":"# Importing Dataset","860f3880":"# Visualising Data","c6b185d0":"For figure out Accuracy Paradox, we will use the Cumulative Accuracy Profile (CAP). More about Cumulative Accuracy Profile (CAP) [here.](https:\/\/en.wikipedia.org\/wiki\/Cumulative_accuracy_profile)","e74f993f":"To use date_converter package will be necessary install easy-date package.\nMore about easy-date [here.](https:\/\/pypi.org\/project\/easy-date\/)"}}