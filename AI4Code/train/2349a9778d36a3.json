{"cell_type":{"f873bf92":"code","af649790":"code","f412114c":"code","5587c861":"code","bfa9f3f4":"code","14abdb5d":"code","d8d12989":"code","5978e612":"code","78157929":"code","7b0e654b":"code","7ea1b6cb":"code","9a5e3134":"code","698a23f9":"code","23a09c9d":"code","1c3bc7d9":"code","c8664717":"code","27c1ab22":"code","9429e615":"code","782a9a8d":"code","947c8ca5":"code","95320017":"code","15fec163":"code","9159bf0d":"code","d44b66e8":"code","a83bad24":"code","eff1d31b":"code","3c708d71":"code","f8429357":"code","91c9286d":"code","fa69f4b5":"code","e3f68b94":"code","a07e3acc":"code","aaef79fc":"code","eec725c9":"code","2fb63343":"code","6f67a8f2":"code","863ded5d":"code","f44dad20":"code","92f7fd24":"code","d19afd9e":"code","2a9430bf":"code","6c52f682":"code","9a3feba6":"code","ec64717d":"code","006e64b8":"code","ccf696fb":"code","feeda14b":"code","2043c983":"code","53a02808":"code","ad5adf4c":"code","ade1eb5b":"code","40fed22f":"code","5f1338ec":"code","cd1ca5a7":"code","de329509":"code","a5f38e74":"code","587dc506":"code","3575fa35":"code","f09e766b":"code","f0614b70":"code","2ea380a5":"code","1f263222":"code","624818f1":"code","6e64e4f3":"code","37e96ed5":"code","5fd1293d":"code","197c85f4":"markdown","051fe385":"markdown","414cda1d":"markdown","de321697":"markdown","ca3d29fe":"markdown","78831ea7":"markdown","3e082c74":"markdown","e08c1b0a":"markdown","338eeac0":"markdown","7301c8a6":"markdown","72af9255":"markdown","3c920fcb":"markdown","6f9ef481":"markdown","2c4ea161":"markdown","7a647c29":"markdown","4132bdec":"markdown","3f314d54":"markdown","9bacde8d":"markdown","edbd5d90":"markdown","eaa17fcd":"markdown","26264a09":"markdown","885d0176":"markdown","b522ac9a":"markdown","451997df":"markdown","fad1f505":"markdown","95cd91b6":"markdown","67fcf7e1":"markdown","2787c1d5":"markdown","4322df52":"markdown","0805caa0":"markdown","867c4822":"markdown","b3242a83":"markdown","86f0873a":"markdown","3843d31c":"markdown","67448e2f":"markdown","a8765200":"markdown","f8adbf33":"markdown","840f50d8":"markdown","1ee76fa5":"markdown","b79ed3b8":"markdown","936e4726":"markdown","287a8829":"markdown","a10230b3":"markdown","377ec0ac":"markdown","5d158706":"markdown","08ca914e":"markdown","d5efc565":"markdown","e8abd1ab":"markdown","2c40cb2e":"markdown"},"source":{"f873bf92":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","af649790":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as plot\nimport seaborn as sns\nimport os\n\n# Skip Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(action = 'ignore', category = DeprecationWarning)\nwarnings.filterwarnings(action = 'ignore', category = FutureWarning)","f412114c":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv',index_col=0)\ntrain.shape","5587c861":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","bfa9f3f4":"train.head()","14abdb5d":"train.columns","d8d12989":"train.describe(include='all')","5978e612":"# Categorical Features\ncat_cols = ['MSSubClass','MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig','LandSlope', \n            'Neighborhood','Condition1', 'Condition2', 'BldgType', 'HouseStyle','OverallQual','OverallCond', 'YearBuilt',\n            'YearRemodAdd','RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual','ExterCond', \n            'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','Heating', 'HeatingQC',\n            'CentralAir', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath','BedroomAbvGr',\n            'KitchenAbvGr', 'KitchenQual','TotRmsAbvGrd','Functional','Fireplaces','FireplaceQu', \n            'GarageType','GarageYrBlt', 'GarageFinish','GarageCars', 'GarageQual', 'GarageCond', 'PavedDrive', \n            'PoolQC', 'Fence', 'MiscFeature','MoSold','YrSold', 'SaleType', 'SaleCondition']","78157929":"# Numerical Features\nnum_cols = ['LotFrontage', 'LotArea','MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n            '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal','SalePrice']\n\n# # To remove the target variable\n# del num_cols[-1]","7b0e654b":"num_cols","7ea1b6cb":"for x in cat_cols :\n    print(x, '----->>', train[x].unique(), '\\n')","9a5e3134":"# for x in num_cols :\n#     print(x, '----->>', train[x].nunique(), ' :- ', train[x].unique()[:10], '\\n')","698a23f9":"cat_none_cols = ['Alley','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n                 'Electrical','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n                 'PoolQC','Fence','MiscFeature']\n\nfor col in cat_none_cols:\n    train[col] = train[col].fillna('None')","23a09c9d":"train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\nnum_cols.append('TotalSF')","1c3bc7d9":"train.shape","c8664717":"num_cols","27c1ab22":"train[cat_cols].describe(include='all')","9429e615":"from sklearn.model_selection import train_test_split\n\nhouse_train, house_test = train_test_split(train, test_size = 0.3, random_state = 1234)","782a9a8d":"print(\"Shape of training set:\",house_train.shape)\nprint(\"Shape of validation set:\",house_test.shape)","947c8ca5":"missing_values = house_train.isna().sum()\n\nmissing_values = missing_values[missing_values>0]\nmissing_values.sort_values(inplace=True)","95320017":"miss_data = pd.DataFrame(missing_values, columns = ['Count of missing data'])\nmiss_data['Percentage of Null Values'] = pd.DataFrame(miss_data['Count of missing data']\/len(train)*100)\n\nmiss_data","15fec163":"miss_data['Count of missing data'].plot(kind='bar',figsize=(10,8), color='#808080')\nplt.xlabel('Features',fontsize=20)\nplt.ylabel('Count of the values', fontsize=20)\nplt.title('Missing Values', fontsize=35)\nplt.show()","9159bf0d":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_imputer = SimpleImputer(strategy = 'median')\nhouse_train[num_cols] = num_imputer.fit_transform(house_train[num_cols])\nhouse_test[num_cols] = num_imputer.transform(house_test[num_cols])\n\ncat_imputer = SimpleImputer(strategy = 'most_frequent')\nhouse_train[cat_cols] = cat_imputer.fit_transform(house_train[cat_cols]) # Imputing \"GarageYrBlt\" as 81 NaN\nhouse_test[cat_cols] = cat_imputer.transform(house_test[cat_cols])","d44b66e8":"house_train_corr, house_test_corr = house_train.copy(), house_test.copy()\n\ndf = pd.concat([house_train_corr, house_test_corr])[cat_cols]\n\ndf.head()","a83bad24":"from sklearn.preprocessing import LabelEncoder # Converts cat data to numeric\nfrom collections import defaultdict\n\nencoder = defaultdict(LabelEncoder)\n\n# Encoding the categorical features on full data\ncat_encoder = df.apply(lambda x: encoder[x.name].fit_transform(x))\n\n# Transfrom the train & test splits\nhouse_train_corr[cat_cols] = house_train_corr[cat_cols].apply(lambda x: encoder[x.name].transform(x))\nhouse_test_corr[cat_cols] = house_test_corr[cat_cols].apply(lambda x: encoder[x.name].transform(x))\n","eff1d31b":"house_train_corr[num_cols].head()","3c708d71":"house_train_corr[cat_cols].head()","f8429357":"house_train[cat_cols].astype('category').describe()","91c9286d":"def corr_plot(df, size = (20, 15), top = None) :\n    \n    corrmat = df.corr()\n\n    if top == None :    \n        f, ax = plt.subplots(figsize = size)\n        sns.heatmap(corrmat, annot = True, fmt = '.2f') # fmt = Decimal rounds\n    else :\n        cor_cols = corrmat.nlargest(top, 'SalePrice')['SalePrice'].index\n        corrmat = np.corrcoef(df[cor_cols].values.T)\n        f, ax = plt.subplots(figsize = size)\n        sns.heatmap(corrmat, annot = True, fmt = '.2f', \n                    yticklabels = cor_cols.values, xticklabels = cor_cols.values)\n        ","fa69f4b5":"corr_plot(house_train[cat_cols])","e3f68b94":"plt.figure(figsize = (12,10))\nsns.distplot(a = house_train['SalePrice'])\nplt.grid(True)","a07e3acc":"fig = plt.figure(figsize=(20,200))\n\nfor i in range(1,len(cat_cols)):\n    ax = fig.add_subplot(len(cat_cols), 2, i)\n    g = sns.countplot(house_train[cat_cols[i]])\n    ","aaef79fc":"result = []\nfeature = []\nfor i in house_train[cat_cols].columns:\n    count = house_train[i].value_counts()\/len(house_train[i])*100\n    count = count[count>=85]\n    if not count.empty :\n        result.append([i,count.to_string(header=None, index=None)])\n        feature.append(i)\n\nfinal_cat = pd.DataFrame(result, columns = ['Feature', 'Percentage of features'])\nfeature","eec725c9":"final_cat","2fb63343":"zerovar_cat_cols = ['Street', 'Alley', 'LandContour', 'Utilities', 'LandSlope', 'Condition1', 'Condition2',\n                    'RoofMatl','ExterCond', 'BsmtCond', 'BsmtFinType2', 'Heating', 'CentralAir',\n                    'Electrical', 'BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'GarageQual', 'GarageCond',\n                    'PavedDrive', 'PoolQC', 'MiscFeature', 'SaleType']","6f67a8f2":"house_train[cat_cols].astype('category').describe()","863ded5d":"num_cols.append('SalePrice')\ncorr_plot(house_train[num_cols])","f44dad20":"fig = plt.figure(figsize=(20,90))\n\nfor i in range(1,len(num_cols)):\n    ax = fig.add_subplot(20, 2, i)\n    g = sns.distplot(house_train[num_cols[i]], bins=80)\n    plt.grid(True)\n    ","92f7fd24":"result = []\nfor i in train[num_cols].columns:\n    count = train[i].value_counts()\/len(train[i])*100\n    count = count[count>=85]\n    if not count.empty :\n#         count.to_string(header=None, index=None)1\n        result.append([i,count.to_string(header=None, index=None)])\n\nfinal_num = pd.DataFrame(result, columns = ['Feature', 'Percentage of features'])","d19afd9e":"final_num","2a9430bf":"zerovar_num_cols = ['BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \n                    'PoolArea','MiscVal']\n","6c52f682":"house_train[num_cols].describe()","9a3feba6":"print('Number of observations above 56000 = ', house_train[house_train.LotArea > 56000].shape[0])\ndf = house_train[house_train.LotArea < 56000]","ec64717d":"df['LotArea'].count()","006e64b8":"dropped_features = []\n\ndef update_cols(col_list):\n        \n    print('Before Drop : \\n# of Numeric Features : ', len(num_cols), \n          '\\n# of Categorical Features : ', len(cat_cols),\n          '\\n# of Total Features : ', len(num_cols) + len(cat_cols))\n\n    house_train.drop(col_list, axis = 1, inplace = True)\n    house_test.drop(col_list, axis = 1, inplace = True)\n    dropped_features.append(col_list)\n\n    for col in col_list :\n        if col in num_cols :\n            num_cols.remove(col)\n        elif col in cat_cols :\n            cat_cols.remove(col)\n\n    print('\\nAfter Drop : \\n# of Numeric Features : ', len(num_cols), \n          '\\n# of Categorical Features : ', len(cat_cols),\n          '\\n# of Total Features : ', len(num_cols) + len(cat_cols))","ccf696fb":"update_cols(zerovar_cat_cols)","feeda14b":"update_cols(zerovar_num_cols)","2043c983":"X_train = house_train.drop('SalePrice', axis = 1) #, inplace = True)\ny_train = house_train['SalePrice']\n\nX_test = house_test.drop('SalePrice', axis = 1) #, inplace = True)\ny_test = house_test['SalePrice']\n","53a02808":"X_train[cat_cols] = X_train[cat_cols].astype('category')\nX_test[cat_cols] = X_test[cat_cols].astype('category')","ad5adf4c":"X_train[cat_cols].describe()","ade1eb5b":"X_test[cat_cols].describe()","40fed22f":"from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(handle_unknown = 'ignore')\n\nX_train = ohe.fit_transform(X_train[cat_cols])\nX_test = ohe.transform(X_test[cat_cols])","5f1338ec":"X_train.shape","cd1ca5a7":"X_test.shape","de329509":"from sklearn.preprocessing import StandardScaler\nSC = StandardScaler(with_mean=False)\n\nSC.fit(X_train)\nX_train_SC = SC.transform(X_train)\nX_test_SC = SC.transform(X_test)","a5f38e74":"# Model evaluation metrics\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\n","587dc506":"from sklearn.linear_model import LinearRegression\nslr_SC = LinearRegression()\nslr_SC.fit(X_train_SC, y_train)\n\npred_train_slr_SC = slr_SC.predict(X_train_SC)\npred_test_slr_SC = slr_SC.predict(X_test_SC)\n\nLRMSE_SC = mean_squared_error(y_true = y_test, y_pred = pred_test_slr_SC)\nLRMAE_SC = mean_absolute_error(y_true = y_test, y_pred = pred_test_slr_SC)\nLRScores_SC = [LRMSE_SC, LRMAE_SC]\n\n# Visualization of results\nplt.figure(figsize = (15,20))\nplt.subplot(421)\nsns.scatterplot(x = y_train, y = pred_train_slr_SC)\nplt.title('LR-Scatter(Train Results)')\nplt.subplot(422)\nsns.scatterplot(x = y_test, y = pred_test_slr_SC)\nplt.title('LR-Scatter(Test Results)')\nplt.subplot(423)\nsns.residplot(x = y_train, y = pred_train_slr_SC)\nplt.title('Residual Plot (Training set)')\nplt.subplot(424)\nsns.residplot(x = y_test, y = pred_test_slr_SC)\nplt.title('Residual Plot (Testing Set)')\n\n# Checking of predicted values \nSLR_SC_pred_actual = pd.DataFrame({'predicted': np.round(pred_test_slr_SC,2), 'actual': y_test})\n\nSLR_SC_pred_actual.head(10)","3575fa35":"from sklearn.linear_model import ElasticNet\n\nen_SC = ElasticNet()\nen_SC.fit(X_train_SC, y_train)\n\npred_train_en_SC = en_SC.predict(X_train_SC)\npred_test_en_SC = en_SC.predict(X_test_SC)\n\nENMSE_SC = mean_squared_error(y_true = y_test, y_pred = pred_test_en_SC)\nENMAE_SC = mean_absolute_error(y_true = y_test, y_pred = pred_test_en_SC)\nENScores_SC = [ENMSE_SC, ENMAE_SC]\n\n# Visualizing the plots\nplt.figure(figsize = (15,20))\nplt.subplot(421)\nsns.scatterplot(x = y_train, y = pred_train_en_SC)\nplt.title('Elastic Net(Train Results)')\nplt.subplot(422)\nsns.scatterplot(x = y_test, y = pred_test_en_SC)\nplt.title('LR-Elastic Net(Test Results)')\nplt.subplot(423)\nsns.residplot(x = y_train, y = pred_train_en_SC)\nplt.title('Residual Plot (Training set)')\nplt.subplot(424)\nsns.residplot(x = y_test, y = pred_test_en_SC)\nplt.title('Residual Plot (Testing Set)')\n\n# Checking of predicted values \nEN_SC_pred_actual = pd.DataFrame({'predicted': np.round(pred_test_en_SC,2), 'actual': y_test})\n\nEN_SC_pred_actual.head(10)","f09e766b":"from sklearn.neighbors import KNeighborsRegressor\n\nknr_SC = KNeighborsRegressor()\nknr_SC.fit(X_train_SC, y_train)\n\npred_train_knr_SC = knr_SC.predict(X_train_SC)\npred_test_knr_SC = knr_SC.predict(X_test_SC)\n\nKNRMSE_SC = mean_squared_error(y_true = y_test, y_pred = pred_test_knr_SC)\nKNRMAE_SC = mean_absolute_error(y_true = y_test, y_pred = pred_test_knr_SC)\nKNRScores_SC = [KNRMSE_SC, KNRMAE_SC]\n\n# Visualization of plots\nplt.figure(figsize = (15,20))\nplt.subplot(421)\nsns.scatterplot(x = y_train, y = pred_train_knr_SC)\nplt.title('KNN Regression(Train Results)')\nplt.subplot(422)\nsns.scatterplot(x = y_test, y = pred_test_knr_SC)\nplt.title('KNN Regression(Test Results)')\nplt.subplot(423)\nsns.residplot(x = y_train, y = pred_train_knr_SC)\nplt.title('Residual Plot (Training set)')\nplt.subplot(424)\nsns.residplot(x = y_test, y = pred_test_knr_SC)\nplt.title('Residual Plot (Testing Set)')\n\n# Checking of the predicted values\nKNR_SC_pred_actual = pd.DataFrame({'predicted': np.round(pred_test_knr_SC,2), 'actual': y_test})\n\nKNR_SC_pred_actual.head(10)","f0614b70":"from sklearn.tree import DecisionTreeRegressor\n\ndtr_SC = DecisionTreeRegressor()\ndtr_SC.fit(X_train_SC, y_train)\n\npred_train_dtr_SC = dtr_SC.predict(X_train_SC)\npred_test_dtr_SC = dtr_SC.predict(X_test_SC)\n\nDTRMSE_SC = mean_squared_error(y_true = y_test, y_pred = pred_test_dtr_SC)\nDTRMAE_SC = mean_absolute_error(y_true = y_test, y_pred = pred_test_dtr_SC)\nDTRScores_SC = [DTRMSE_SC, DTRMAE_SC]\n\n# Visualizing the results\nplt.figure(figsize = (15,20))\nplt.subplot(421)\nsns.scatterplot(x = y_train, y = pred_train_dtr_SC)\nplt.title('Decision Tree Regression(Train Results)')\nplt.subplot(422)\nsns.scatterplot(x = y_test, y = pred_test_dtr_SC)\nplt.title('Decision Tree Regression(Test Results)')\nplt.subplot(423)\nsns.residplot(x = y_train, y = pred_train_dtr_SC)\nplt.title('Residual Plot (Training set)')\nplt.subplot(424)\nsns.residplot(x = y_test, y = pred_test_dtr_SC)\nplt.title('Residual Plot (Testing Set)')\n\n# Checking the predicted values\nDTR_pred_actual = pd.DataFrame({'predicted': np.round(pred_test_dtr_SC,2), 'actual': y_test})\n\nDTR_pred_actual.head(10)","2ea380a5":"from sklearn.linear_model import LinearRegression\nslr = LinearRegression()\nslr.fit(X_train, y_train)\n\npred_train_slr = slr.predict(X_train)\npred_test_slr = slr.predict(X_test)\n\nLRMSE = mean_squared_error(y_true = y_test, y_pred = pred_test_slr)\nLRMAE = mean_absolute_error(y_true = y_test, y_pred = pred_test_slr)\nLRScores = [LRMSE, LRMAE]\n\n# Visualization of results\nplt.figure(figsize = (15,20))\nplt.subplot(421)\nsns.scatterplot(x = y_train, y = pred_train_slr)\nplt.title('LR-Scatter(Train Results)')\nplt.subplot(422)\nsns.scatterplot(x = y_test, y = pred_test_slr)\nplt.title('LR-Scatter(Test Results)')\nplt.subplot(423)\nsns.residplot(x = y_train, y = pred_train_slr)\nplt.title('Residual Plot (Training set)')\nplt.subplot(424)\nsns.residplot(x = y_test, y = pred_test_slr)\nplt.title('Residual Plot (Testing Set)')\n\n# Checking of predicted values \nSLR_pred_actual = pd.DataFrame({'predicted': np.round(pred_test_slr,2), 'actual': y_test})\n\nSLR_pred_actual.head(10)","1f263222":"from sklearn.linear_model import ElasticNet\n\nen = ElasticNet()\nen.fit(X_train, y_train)\n\npred_train_en = en.predict(X_train)\npred_test_en = en.predict(X_test)\n\nENMSE = mean_squared_error(y_true = y_test, y_pred = pred_test_en)\nENMAE = mean_absolute_error(y_true = y_test, y_pred = pred_test_en)\nENScores = [ENMSE, ENMAE]\n\n# Visualizing the plots\nplt.figure(figsize = (15,20))\nplt.subplot(421)\nsns.scatterplot(x = y_train, y = pred_train_en)\nplt.title('Elastic Net(Train Results)')\nplt.subplot(422)\nsns.scatterplot(x = y_test, y = pred_test_en)\nplt.title('LR-Elastic Net(Test Results)')\nplt.subplot(423)\nsns.residplot(x = y_train, y = pred_train_en)\nplt.title('Residual Plot (Training set)')\nplt.subplot(424)\nsns.residplot(x = y_test, y = pred_test_en)\nplt.title('Residual Plot (Testing Set)')\n\n# Checking of predicted values \nEN_SC_pred_actual = pd.DataFrame({'predicted': np.round(pred_test_en,2), 'actual': y_test})\n\nEN_SC_pred_actual.head(10)","624818f1":"from sklearn.neighbors import KNeighborsRegressor\n\nknr = KNeighborsRegressor()\nknr.fit(X_train, y_train)\n\npred_train_knr = knr.predict(X_train)\npred_test_knr = knr.predict(X_test)\n\nKNRMSE = mean_squared_error(y_true = y_test, y_pred = pred_test_knr)\nKNRMAE = mean_absolute_error(y_true = y_test, y_pred = pred_test_knr)\nKNRScores = [KNRMSE, KNRMAE]\n\n# Visualization of plots\nplt.figure(figsize = (15,20))\nplt.subplot(421)\nsns.scatterplot(x = y_train, y = pred_train_knr)\nplt.title('KNN Regression(Train Results)')\nplt.subplot(422)\nsns.scatterplot(x = y_test, y = pred_test_knr)\nplt.title('KNN Regression(Test Results)')\nplt.subplot(423)\nsns.residplot(x = y_train, y = pred_train_knr)\nplt.title('Residual Plot (Training set)')\nplt.subplot(424)\nsns.residplot(x = y_test, y = pred_test_knr)\nplt.title('Residual Plot (Testing Set)')\n\n# Checking of the predicted values\nKNR_pred_actual = pd.DataFrame({'predicted': np.round(pred_test_knr,2), 'actual': y_test})\n\nKNR_pred_actual.head(10)","6e64e4f3":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\n\npred_train_dtr = dtr.predict(X_train)\npred_test_dtr = dtr.predict(X_test)\n\nDTRMSE = mean_squared_error(y_true = y_test, y_pred = pred_test_dtr)\nDTRMAE = mean_absolute_error(y_true = y_test, y_pred = pred_test_dtr)\nDTRScores = [DTRMSE, DTRMAE]\n\n# Visualizing the results\nplt.figure(figsize = (15,20))\nplt.subplot(421)\nsns.scatterplot(x = y_train, y = pred_train_dtr)\nplt.title('Decision Tree Regression(Train Results)')\nplt.subplot(422)\nsns.scatterplot(x = y_test, y = pred_test_dtr)\nplt.title('Decision Tree Regression(Test Results)')\nplt.subplot(423)\nsns.residplot(x = y_train, y = pred_train_dtr)\nplt.title('Residual Plot (Training set)')\nplt.subplot(424)\nsns.residplot(x = y_test, y = pred_test_dtr)\nplt.title('Residual Plot (Testing Set)')\n\n# Checking the predicted values\nDTR_pred_actual = pd.DataFrame({'predicted': np.round(pred_test_dtr,2), 'actual': y_test})\n\nDTR_pred_actual.head(10)","37e96ed5":"std_model = pd.DataFrame([LRScores_SC,ENScores_SC,KNRScores_SC, DTRScores_SC], columns = ['MSE', 'MAE'], \n                         index = ['Linear', 'Elastic Net', 'KNR','DTR'])\nstd_model['RMSE'] = std_model['MSE'].apply(lambda x: x**(1\/2))\nstd_model","5fd1293d":"non_std_model = pd.DataFrame([LRScores, ENScores,KNRScores, DTRScores], columns = ['MSE', 'MAE'], \n                             index = ['Linear', 'Elastic Net', 'KNR', 'DTR'])\nnon_std_model['RMSE'] = non_std_model['MSE'].apply(lambda x: x**(1\/2))\nnon_std_model","197c85f4":"# Visualization","051fe385":"We decide to **DROP** the features which have frequency of categories greater than or equal to 85%\n\nThe features are `BsmtFinSF2`, `LowQualFinSF`, `EnclosedPorch`, `3SsnPorch`, `ScreenPorch`, `PoolArea`, `MiscVal` which have nearly 0 variance\n\nAlso, there features have very less correlcation (less than 0.2) with the target variables `SalePrice`\n","414cda1d":"### Visualization of `num_cols`","de321697":"## Non-Standardized Models","ca3d29fe":"### K-Nearest Regression","78831ea7":"#### Linear Regression","3e082c74":"### Checking for NaN values","e08c1b0a":"# Building different models","338eeac0":"### `Label Encoding` of the categorical variables","7301c8a6":"## Standardized Models","72af9255":"### Standardized Models","3c920fcb":"We will check the variance of the `categorical columns` in the `training set`","6f9ef481":"____","2c4ea161":"### Displaying the categories of each categorical feature","7a647c29":"### Distribution of the target variable `SalePrice`","4132bdec":"Creating a function to check the correlation of the features with the target variable","3f314d54":"Before we OneHotEncode, we first need to convert the categorical features into `category` in the `train` and `test` dataset","9bacde8d":"The following feaures either have zero variance or 1 very dominant category\n`Street`, `Alley`, `LandContour`, `Utilities`, `LandSlope`, `Condition1`, `Condition2`,`RoofMatl`, `ExterCond`, `BsmtCond`, `BsmtFinType2`, `Heating`, `CentralAir`, `Electrical`, `BsmtHalfBath`, `KitchenAbvGr`, `Functional`, `GarageQual`, `GarageCond`, `PavedDrive`, `PoolQC`, `MiscFeature`, `SaleType`\n\nHence, we would drop them in further steps","edbd5d90":"## Imputation ","eaa17fcd":"Defining a function to drop features and make a list","26264a09":"<div class=\"alert alert-block alert-info\"><b> \n\n - The `LotArea` has some data instances where the area is large but thesubsequent SalePrice is low. This is leveraging the regression line and hence we would drop this data instances \n\n<\/b><\/div>","885d0176":"#### Decision Tree Regression","b522ac9a":"By default, Pandas displays only _first 10_ and _last 10_ features  \n\nSince our dataset has 81 features, we need to see all the 81 features to perform EDA. This can be done using `pd.set_option`","451997df":"## Reports of the above model building","fad1f505":"### Visualization of `Categorical Columns`","95cd91b6":"#### We are ready with our processed data. The last thing we do is to: \n    - `OneHotEncode` the `categorical features`\n    - `Standardize` the `numerical  features`","67fcf7e1":"### Standardizing the `numerical features`","2787c1d5":"___","4322df52":"#### Elastic Net","0805caa0":"<div class=\"alert alert-block alert-info\"><b> \n\n - In the further steps, we could create a function where we can drop both `categorical columns` and the `numerical columns`\n\n<\/b><\/div>","867c4822":"____","b3242a83":"### Non-Standardized Models","86f0873a":"#### Linear Regression","3843d31c":"We can see that the column `Alley` has now has three levels instead of 2 levels.\n\nHence, we have successfully converted the `NaN` to `None`","67448e2f":"----","a8765200":"### Function to drop the features","f8adbf33":"# <center>House Price Prediction<\/center>\n\nDISCLAIMER: The following notebook was part of a practice dataset taught at my institute. There are a few codes [functions] (like `corr_plot`, and `update_cols`) which has been taught and coded by my professor. Rest of the feature engineering, visualization, model building and performance (evaluation) metrics has been made by me. Hope this notebook is helpful! ","840f50d8":"#### K-Nearest Regression","1ee76fa5":"Let me know your views\/feedback in the comment section below! And consider Upvoting if you liked the notebook! :)","b79ed3b8":"#### Decision Tree Regression","936e4726":"# Model Building","287a8829":"Checking the correlation of categorical features","a10230b3":"<div class=\"alert alert-block alert-info\"><b> \n\n - From the above `num_cols` information, we can add 1 new column named `TotalSF` which is the total house square feet of the house using `TotalBsmtSF` `1stFlrSF` and `2ndFlrSF`\n\n<\/b><\/div>","377ec0ac":"Checking the correlation of *categorical features*","5d158706":"### Separating the *categorical* and *numerical* features","08ca914e":"## Train-Test Split\n","d5efc565":"We can see that less than 20% of the data has `NaN` values. Hence, no need to drop any of these features. \n\n### We will take care of them in _Imputation_","e8abd1ab":"#### Elastic Net","2c40cb2e":"As we can see, there are a lot of **NaN**. Hence, it is better to replace them with *None* so that it becomes easy for analysis and building machine learning model"}}