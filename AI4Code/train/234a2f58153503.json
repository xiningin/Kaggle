{"cell_type":{"752197e6":"code","4fbfbd34":"code","b9353435":"code","405a1a0b":"code","061ccc5e":"code","99dce58a":"code","94f0d763":"code","eca6eba0":"code","691365c8":"code","62cbbc55":"code","6242ba34":"code","cbff95a9":"code","1889c3be":"code","12257417":"code","d9977109":"code","3841f514":"code","b9a2a001":"code","83448a7f":"code","fd2d292c":"code","77a22a84":"code","2723de30":"code","8ef4c463":"code","03bef825":"code","3d21a365":"code","eb09911b":"code","3e30f2d0":"code","66a98e81":"markdown","274bc925":"markdown","273c3dc4":"markdown","476f6fc3":"markdown","268277bb":"markdown","120b25ef":"markdown","5977874b":"markdown","d90b7a33":"markdown","1af843eb":"markdown"},"source":{"752197e6":"from pathlib import Path\n\nfrom fastai.data.block import CategoryBlock, DataBlock\nfrom fastai.data.transforms import get_image_files, parent_label, RandomSplitter\nfrom fastai.interpret import ClassificationInterpretation\nfrom fastai.learner import load_learner\nfrom fastai.metrics import accuracy, error_rate\nfrom fastai.vision.augment import Resize\nfrom fastai.vision.data import ImageDataLoaders, ImageBlock\nfrom fastai.vision.all import cnn_learner\nfrom torchvision.models.resnet import resnet18, resnet34","4fbfbd34":"data_path = Path(\"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/\")\nmodel_path = Path(\"\/kaggle\/tmp\/model\/\")","b9353435":"train_path = data_path\/\"train\"\ndataloaders = ImageDataLoaders.from_folder(path=train_path,\n                                           item_tfms=Resize(224),\n                                           valid_pct=0.2,\n                                           bs=64,\n                                           seed=42)","405a1a0b":"dataloaders.show_batch()","061ccc5e":"print(f\"Number of classes: {dataloaders.c}, class names: {dataloaders.vocab}\")\nprint(f\"Images in train set: {len(dataloaders.train.dataset)}\")\nprint(f\"Images in validation set: {len(dataloaders.valid.dataset)}\")","99dce58a":"learner = cnn_learner(dataloaders,\n                      resnet18,\n                      metrics=[error_rate, accuracy],\n                      model_dir=model_path).to_fp16()","94f0d763":"learner.fine_tune(epochs=2)","eca6eba0":"learner.show_results()","691365c8":"def show_confusion_matrix(learner):\n    interpreter = ClassificationInterpretation.from_learner(learner)\n    interpreter.plot_confusion_matrix(figsize=(7,7), dpi=60)","62cbbc55":"show_confusion_matrix(learner)","6242ba34":"data_block = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                       get_items = get_image_files,\n                       splitter = RandomSplitter(valid_pct=0.2, seed=42),\n                       get_y = parent_label,\n                       item_tfms=Resize(224),\n                      )\ndls = data_block.dataloaders(source=train_path, bs=64)","cbff95a9":"bigger_learner = cnn_learner(dls,\n                             resnet34,\n                             metrics=[error_rate, accuracy],\n                             model_dir=model_path).to_fp16()","1889c3be":"bigger_learner.lr_find()","12257417":"bigger_learner.fit_one_cycle(n_epoch=5, lr_max=1e-2)","d9977109":"bigger_learner.unfreeze()","3841f514":"bigger_learner.lr_find()","b9a2a001":"bigger_learner.fit_one_cycle(n_epoch=2, lr_max=slice(1e-6, 1e-5))","83448a7f":"show_confusion_matrix(bigger_learner)","fd2d292c":"test_path = data_path \/ \"test\"\ntest_items = get_image_files(test_path)\ntest_dl_resnet18 = learner.dls.test_dl(test_items, bs=64, with_labels=True, shuffle = False)\ntest_dl_resnet34 = bigger_learner.dls.test_dl(test_items, bs=64, with_labels=True, shuffle = False)","77a22a84":"resnet_18_accuracy = learner.validate(dl=test_dl_resnet18)\nresnet_34_accuracy = bigger_learner.validate(dl=test_dl_resnet34)","2723de30":"print(f\"Accuracy on test set:\")\nprint(f\"Resnet18:  {round(resnet_18_accuracy[-1] * 100, 2)}%\")\nprint(f\"Resnet34:  {round(resnet_34_accuracy[-1] * 100, 2)}%\")","8ef4c463":"resnet18_path = Path(\"\/kaggle\/tmp\/model\/resnet18_2.pkl\")","03bef825":"learner.export(resnet18_path)","3d21a365":"new_learner = load_learner(resnet18_path)","eb09911b":"example_image_path = data_path \/ \"test\/PNEUMONIA\/person100_bacteria_475.jpeg\"","3e30f2d0":"new_learner.predict(example_image_path)","66a98e81":"## Create a Learner","274bc925":"## Examine results","273c3dc4":"## Compare models against test set","476f6fc3":"## Train a larger model","268277bb":"## Use the Data Block API to customize data handling","120b25ef":"## Save and load your model","5977874b":"## Imports","d90b7a33":"## Load the training data","1af843eb":"## Use your model on new images"}}