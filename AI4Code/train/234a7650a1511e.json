{"cell_type":{"2c9440e3":"code","3e714ae4":"code","e1632671":"code","d60ea0f4":"code","26c81aab":"code","1caf10b3":"code","7ea218c4":"code","a747601e":"code","173e9dc7":"code","118c3867":"code","3f41ba04":"code","08c6014d":"code","2b9caee5":"code","f188105d":"code","e3ce69b5":"code","a3fb10a6":"code","7b9c1d81":"code","d0b48988":"code","6e7de2be":"code","2ff2fb0b":"code","b168f082":"code","b06d4b5c":"code","c82c8103":"code","5fa2063f":"code","5d7e8fe3":"code","22e43467":"code","a6159dcc":"code","6d35ce19":"code","dfbbb62a":"code","bc7a36d3":"code","6ba0b53b":"code","490a9b33":"code","ebfb8917":"code","44b6d72b":"code","c41eb50c":"code","8f98e911":"code","488e5182":"code","ec914d5c":"code","aedf3a69":"code","d6ea6d74":"code","9ce6d6a2":"code","cfe414f2":"code","58782aae":"code","3db00a14":"code","c27af66a":"code","a502493f":"code","e7ddecf1":"code","b8f4b026":"code","d79f9272":"code","94624e31":"code","eca61683":"code","8d6ec89f":"code","ca540dcd":"code","7aff05fe":"code","1dad0072":"markdown","a3f3de9e":"markdown","5b97ab15":"markdown","14bd2931":"markdown","74cded14":"markdown","146336c7":"markdown","0ba3fd38":"markdown","da57fa1e":"markdown","d564bec8":"markdown","a6a8c047":"markdown","59a4340f":"markdown","fccda8e1":"markdown","e8e29a18":"markdown","3644b648":"markdown","251234e3":"markdown","d03d7a58":"markdown","c89daf7d":"markdown","5afe06dc":"markdown","63c0c8b6":"markdown","9ddf0e5e":"markdown","a6dd6643":"markdown","8cbeeb9e":"markdown","5154815c":"markdown","c7ed5554":"markdown","2cbd1b75":"markdown","d3f75fcc":"markdown","57bc6e9e":"markdown","705532b8":"markdown","2398b08c":"markdown","88755931":"markdown","09508caf":"markdown","fe655a99":"markdown","0f035116":"markdown","71e86e18":"markdown","51c7401b":"markdown","62506a69":"markdown","26f68b89":"markdown","60db000e":"markdown","030d6a88":"markdown","7517c40f":"markdown","ef60d860":"markdown","4323c8d9":"markdown","fc7f233e":"markdown","8acc1b5e":"markdown","c9dbc91b":"markdown","db78d1c4":"markdown","0a21ff17":"markdown","b3e3dd01":"markdown","8e059624":"markdown","e386aefa":"markdown","17ee736e":"markdown"},"source":{"2c9440e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e714ae4":"import seaborn as sns \nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score # this is the metric used to score the competition\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer # scaling will be necessary for most models\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV # using a small validation + cv set may help\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.decomposition import KernelPCA, PCA\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom skopt import BayesSearchCV # works similar to GridSearchCV but it doesn't try all param combinations, takes structure of search space into acc.","e1632671":"RAND_STATE = 13","d60ea0f4":"def prep_test_data(test_data):\n    \"\"\"Prepares Test Data for predictions, by applying all non-pipeline preproccessing steps.\n        - Downcasting float and integer columns to save memory\n        - Dropping 'id' feature\n    \n    Args:\n        test_data(pd.DataFrame): DataFrame containing all columns of training data except id.\n    Returns:\n        test_data_prep(pd.DataFrame): DataFrame with features equal to training data. \n    \"\"\"\n    for col in test_data.columns:\n        if test_data.loc[:,col].dtype == 'float64':\n            test_data.loc[:,col] = pd.to_numeric(test_data.loc[:,col], downcast='float')   \n        if test_data.loc[:,col].dtype == 'int64':\n            test_data.loc[:,col] = pd.to_numeric(test_data.loc[:,col], downcast='integer')\n\n    test_data_prep = test_data.drop('id', axis = 1)  \n    return test_data_prep","26c81aab":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')","1caf10b3":"df_train.shape","7ea218c4":"for col in df_train.columns:    \n    if df_train.loc[:,col].dtype == 'float64':\n        df_train.loc[:,col] = pd.to_numeric(df_train.loc[:,col], downcast='float')   \n    if  df_train.loc[:,col].dtype == 'int64':\n         df_train.loc[:,col] = pd.to_numeric(df_train.loc[:,col], downcast='integer')","a747601e":"df_train.head()","173e9dc7":"df_train.isna().any().sum()","118c3867":"fig, ax = plt.subplots(figsize = (7,7))\nsns.countplot(x = df_train.loc[:,'target'], ax = ax)\nsns.despine()","3f41ba04":"df_train.describe().T.sample(20)","08c6014d":"# fig, axes = plt.subplots(nrows = 10, ncols = 10, figsize = (20,20))\n# axes = axes.flatten()\n\n# df_plot = df_train.sample(frac = 0.33, random_state = RAND_STATE) #only use a third of the data to visualize\n# for idx, axis in enumerate(axes):\n#     sns.histplot(data = df_plot, x = f'f{idx}',\n#                 ax = axis, hue = 'target', legend = False)\n#     axis.set_ylabel('')    \n#     axis.set_xlabel('')","2b9caee5":"fig, ax = plt.subplots(figsize = (15,13))\nsns.heatmap(df_train.corr(), ax = ax)","f188105d":"df_train.drop('id', axis = 1, inplace = True) ","e3ce69b5":"features = df_train.drop('target', axis = 1)\ntarget = df_train.loc[:,'target']\n\nfeatures_train, features_val, target_train,target_val = train_test_split(features, \n                                                                         target, \n                                                                         test_size = 0.1, \n                                                                         random_state = RAND_STATE)\n","a3fb10a6":"features_train.shape, features_val.shape","7b9c1d81":"target_train.shape, target_val.shape","d0b48988":"# base_dt = DecisionTreeClassifier()\n# cross_val_score(base_dt, features_train, target_train, scoring = 'roc_auc', n_jobs = -1)","6e7de2be":"nb_clf = GaussianNB()\npipe_standard = Pipeline([('standard_scaler', StandardScaler()), ('nb_model', nb_clf)])\npipe_minmax = Pipeline([('minmax_scaler', MinMaxScaler()), ('nb_model', nb_clf)])\npipe_robust = Pipeline([('robust_scaler', RobustScaler()), ('nb_model', nb_clf)])\npipe_quantile = Pipeline([('quantile_scaler', QuantileTransformer()), ('nb_model', nb_clf)])\npipe_quantile_norm = Pipeline([('quantile_scaler', QuantileTransformer(output_distribution = 'normal')),\n                          ('nb_model', nb_clf)])","2ff2fb0b":"pipes = {'Standard': pipe_standard,\n         'MinMax': pipe_minmax,\n         'Robust': pipe_robust,\n         'Quantile': pipe_quantile,\n         'Quantile Normal Dist': pipe_quantile_norm}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")\n    ","b168f082":"pipe_quantile_norm.fit(features_train, target_train)\nquant_nb_preds = pipe_quantile_norm.predict_proba(features_val)\n","b06d4b5c":"roc_auc_score(target_val, quant_nb_preds[:,1])","c82c8103":"# pipe_quantile_norm.fit(features_train, target_train)\n# features_test = prep_test_data(df_test)\n# quant_nb_preds_sub = pipe_quantile_norm.predict_proba(features_test)\n# df_sub_quant_nb = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_nb_preds_sub[:,1]})\n# df_sub_quant_nb\n#df_sub_quant_nb.to_csv('submission.csv', index = None)","5fa2063f":"log_clf = LogisticRegression(n_jobs = -1)\nquant_norm = QuantileTransformer(output_distribution = 'normal')\npipe_robust_log = Pipeline([('robust_scaler', RobustScaler()), ('LogReg',log_clf)])\npipe_log = Pipeline([('Quantile Transformer', quant_norm),('LogReg',log_clf)])","5d7e8fe3":"# rf_clf = RandomForestClassifier(n_jobs = -1, random_state = RAND_STATE)\n# pipe_rf = Pipeline([('Quantile Transformer', quant_norm),('RandomForest',rf_clf)])","22e43467":"# ada_clf = AdaBoostClassifier(random_state = RAND_STATE)\n# pipe_ada = Pipeline([('Quantile Transformer', quant_norm),('AdaBoost',ada_clf)])","a6159dcc":"svc_clf = LinearSVC(dual = False)\npipe_robust_svc = Pipeline([('robust_scaler', RobustScaler()), ('LinearSVC',svc_clf)])\npipe_svc = Pipeline([('Quantile Transformer', quant_norm),('LinearSVC',svc_clf)])","6d35ce19":"svcsgd_clf = SGDClassifier(loss = 'hinge', n_jobs = -1)\npipe_robust_svcsgd = Pipeline([('robust_scaler', RobustScaler()), ('LinearSGDSVC',svcsgd_clf)])\npipe_svcsgd = Pipeline([('Quantile Transformer', quant_norm),('LinearSGDSVC',svcsgd_clf)])","dfbbb62a":"# XGB_clf = XGBClassifier(max_depth = 5,\n#                                  learning_rate = 0.007,\n#                                  n_estimators = 7000,\n#                                  objective = 'binary:logistic',\n#                                  booster = 'gbtree',\n#                                  gamma = 1.5,\n#                                  max_delta_step = 3,\n#                                  min_child_weight = 10,\n#                                  subsample = 0.6,\n#                                  colsample_bytree = 0.8,\n#                                  n_jobs = -1\n#                                  )\n\n# quant_scaler = QuantileTransformer()\n# features_train_xgb = pd.DataFrame(quant_scaler.fit_transform(features_train))\n# features_val_xgb = pd.DataFrame(quant_scaler.transform(features_val))\n\n# xgb = XGB_clf.fit(features_train_xgb.values,\n#                        target_train.values.ravel(),\n#                        eval_set = [(features_train_xgb.values, target_train), (features_val_xgb.values, target_val)], \n#                        eval_metric = 'auc',\n#                        early_stopping_rounds = 25,\n#                        verbose = True)","bc7a36d3":"# features_test = prep_test_data(df_test)\n# features_test = pd.DataFrame(quant_scaler.transform(features_test))\n# quant_xgb_preds_sub = xgb.predict_proba(features_test)\n# df_sub_quant_xgb = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_xgb_preds_sub[:,1]})\n# print(df_sub_quant_xgb.head())\n# df_sub_quant_xgb.to_csv('df_sub_quant_xgb.csv', index = None)","6ba0b53b":"#df_sub_quant_xgb.to_csv('submission_quant_xgb.csv', index = None)","490a9b33":"pipes = {'Quant LogReg': pipe_log,\n         'Robust LogReg': pipe_robust_log,\n         #'RandomForest':pipe_rf,\n        #'AdaBoost':pipe_ada,\n         'Quant LinearSVC':pipe_svc,\n         'Robust LinearSVC': pipe_robust_svc,\n        'Quant LinearSGDSVC': pipe_svcsgd,\n        'Robust LinearSGDSVC': pipe_robust_svcsgd,}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","ebfb8917":"pipe_robust_log.fit(features_train, target_train)\nfeatures_test = prep_test_data(df_test)\nquant_log_preds_sub = pipe_robust_log.predict_proba(features_test)\ndf_sub_quant_log = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_log_preds_sub[:,1]})\ndf_sub_quant_log.to_csv('submission_robust_log.csv', index = None)","44b6d72b":"# pipe_robust_svc.fit(features_train, target_train)\n# features_test = prep_test_data(df_test)\n# quant_svc_preds_sub = pipe_robust_svc.predict(features_test)\n# df_sub_quant_svc = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_svc_preds_sub})\n# df_sub_quant_svc.to_csv('submission_robust_svc.csv', index = None)","c41eb50c":"perm_feat_imp_logreg = permutation_importance(pipe_robust_log, \n                                               features, target, \n                                               scoring = 'roc_auc',\n                                               n_repeats = 3,\n                                               n_jobs = -1, \n                                               random_state = RAND_STATE)","8f98e911":"perm_feat_imp_logreg_series = pd.Series(perm_feat_imp_logreg.get('importances_mean'),index = features.columns,)\nperm_feat_imp_logreg_series = perm_feat_imp_logreg_series.sort_values(ascending = False)\n\nfig,ax = plt.subplots(figsize = (15,20))\nsns.barplot(y = perm_feat_imp_logreg_series.index,\n            x = perm_feat_imp_logreg_series.values,\n            ax = ax)\nsns.despine()","488e5182":"features_reduced = features_train.loc[:,perm_feat_imp_logreg_series[:40].index]","ec914d5c":"cv_scores = cross_val_score(pipe_robust_log,features_reduced, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\nprint(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","aedf3a69":"# features_reduced = features.loc[:,perm_feat_imp_logreg_series[:30].index]\n# pipe_robust_log.fit(features_reduced, target)\n# features_test = prep_test_data(df_test).loc[:,perm_feat_imp_logreg_series[:30].index]\n# quant_log_preds_sub = pipe_robust_log.predict_proba(features_test)\n# df_sub_quant_log = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_log_preds_sub[:,1]})\n# df_sub_quant_log.to_csv('submission_robust_log.csv', index = None)","d6ea6d74":"#df_sub_quant_log.to_csv('submission.csv', index = None)","9ce6d6a2":"selector_f = SelectKBest(score_func = f_classif, k = 40)\nselector_f.fit(features_train, target_train)","cfe414f2":"col_index = selector_f.get_support(indices=True)\ncol_names = features_train.iloc[:,col_index].columns\npd.Series(selector_f.scores_[:40], index = col_names)","58782aae":"features_train_kbest = features_train.iloc[:,col_index]\n\npipes = {'Quantile Uni NB': pipe_quantile,\n         'Quantile Normal NB': pipe_quantile_norm,\n         'Quant LogReg': pipe_log,\n         'Robust LogReg': pipe_robust_log,\n         'Quant LinearSVC':pipe_svc,\n         'Robust LinearSVC': pipe_robust_svc,\n        'Quant LinearSGDSVC': pipe_svcsgd,\n        'Robust LinearSGDSVC': pipe_robust_svcsgd,}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train_kbest, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","3db00a14":"selector_f70 = SelectKBest(score_func = f_classif, k = 70)\nselector_f70.fit(features_train, target_train)\ncol_index = selector_f70.get_support(indices=True)\n\nfeatures_train_kbest = features_train.iloc[:,col_index]\n\npipes = {'Quantile Uni NB': pipe_quantile,\n         'Quantile Normal NB': pipe_quantile_norm,\n         'Quant LogReg': pipe_log,\n         'Robust LogReg': pipe_robust_log,\n         'Quant LinearSVC':pipe_svc,\n         'Robust LinearSVC': pipe_robust_svc,\n        'Quant LinearSGDSVC': pipe_svcsgd,\n        'Robust LinearSGDSVC': pipe_robust_svcsgd,}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train_kbest, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","c27af66a":"features_reduced = features_train_kbest = features_train.iloc[:,col_index]\npipe_robust_log.fit(features_reduced, target_train)\npipe_robust_svc.fit(features_reduced, target_train)\nfeatures_test = prep_test_data(df_test).iloc[:,col_index]\nrobust_log_70_preds_sub = pipe_robust_log.predict_proba(features_test)\nrobust_svc_70_preds_sub = pipe_robust_svc.predict(features_test)\ndf_sub_robust_log_70 = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':robust_log_70_preds_sub[:,1]})\ndf_sub_robust_svc_70 = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':robust_svc_70_preds_sub})\ndf_sub_robust_log_70.to_csv('submission_robust_log_70best.csv', index = None)\ndf_sub_robust_svc_70.to_csv('submission_robust_svc_70best.csv', index = None)","a502493f":"pca = PCA(n_components = 0.95)\n\npipe_quant_pca_logreg = Pipeline([('QuantileTransformer', QuantileTransformer()),\n                                  ('PCA',pca),\n                                  ('LogReg',log_clf)])\npipe_robust_pca_logreg = Pipeline([('Scaler', RobustScaler()),\n                                  ('PCA',pca),\n                                  ('LogReg',log_clf)])\npipe_quant_pca_svc = Pipeline([('QuantileTransformer', QuantileTransformer()),\n                                  ('PCA',pca),\n                                  ('LinearSVC',svc_clf)])\npipe_robust_pca_svc = Pipeline([('Scaler', RobustScaler()),\n                                  ('PCA',pca),\n                                  ('LinearSVC',svc_clf)])","e7ddecf1":"pipes = {'Quant LogReg': pipe_quant_pca_logreg,\n         'Robust LogReg': pipe_robust_pca_logreg,\n         'Quant LinearSVC':pipe_quant_pca_svc,\n         'Robust LinearSVC': pipe_robust_pca_svc}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","b8f4b026":"def tune_hyperparams(pipeline, param_grid, n_iter = 50, iid = True):\n    '''ADD DOCSTRING'''\n    bayes_search = BayesSearchCV(pipeline,\n                                 param_grid,\n                                 n_iter = n_iter,\n                                 scoring = 'roc_auc',\n                                 cv = 3,\n                                 random_state = RAND_STATE,\n                                 verbose = 1,\n                                 n_jobs = 2)\n    bayes_search.fit(features, target)\n    best_estimator = bayes_search.best_estimator_\n    print(f'Best CV ROC-AUC {bayes_search.best_score_}\\n')\n    #print(pd.DataFrame(bayes_search.cv_results_))\n    print(bayes_search.best_estimator_)\n    return best_estimator","d79f9272":"# logreg_clf = LogisticRegression(solver = 'saga', \n#                                 random_state = RAND_STATE,\n#                                 n_jobs = -1,\n#                                 max_iter = 500,\n#                                 penalty = 'elasticnet')\n\n# pipe_robust_logreg = Pipeline([('scaler', RobustScaler()),\n#                                ('logreg',logreg_clf)])\n\n# robust_logreg_params = {\n#                         'logreg__l1_ratio': np.arange(0, 1.1, 0.1),\n#                         'logreg__C': np.geomspace(0.001, 100, 10)\n#                        }","94624e31":"# best_robust_logreg = tune_hyperparams(pipe_robust_logreg,\n#                                      robust_logreg_params, n_iter = 50)","eca61683":"# best_robust_logreg.fit(features,target)\n# features_test = prep_test_data(df_test)\n# robust_log_tuned_preds_sub = best_robust_logreg.predict_proba(features_test)\n# df_sub_robust_log_tuned = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':robust_log_tuned_preds_sub[:,1]})\n# df_sub_robust_log_tuned.to_csv('submission.csv', index = None)","8d6ec89f":"# logreg_clf = LogisticRegression(solver = 'saga', \n#                                 random_state = RAND_STATE,\n#                                 n_jobs = -1,\n#                                 max_iter = 500,\n#                                 penalty = 'elasticnet')\n\n# pipe_quant_logreg = Pipeline([('scaler', QuantileTransformer()),\n#                                ('logreg',logreg_clf)])\n\n# quant_logreg_params = {'scaler__output_distribution':['normal','uniform'],\n#                         'logreg__l1_ratio': np.arange(0, 1.1, 0.1),\n#                         'logreg__C': np.geomspace(0.001, 100, 10)\n#                        }\n\n# best_quant_logreg = tune_hyperparams(pipe_quant_logreg,\n#                                      quant_logreg_params)","ca540dcd":"best_quant_logreg = Pipeline(steps=[('scaler', QuantileTransformer(output_distribution='normal')),\n                ('logreg',\n                 LogisticRegression(C=0.001, l1_ratio=0.7000000000000001,\n                                    max_iter=500, n_jobs=-1,\n                                    penalty='elasticnet', random_state=13,\n                                    solver='saga'))])","7aff05fe":"best_quant_logreg.fit(features,target)\nfeatures_test = prep_test_data(df_test)\nquant_log_tuned_preds_sub = best_quant_logreg.predict_proba(features_test)\ndf_sub_quant_log_tuned = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_log_tuned_preds_sub[:,1]})\ndf_sub_quant_log_tuned.to_csv('submission_quant.csv', index = None)","1dad0072":"## Naive Bayes\n\nAs the dataset is quite large NB could provide a very economic choice. It's very fast, scales well and provides probabilities, which are needed for the submission.","a3f3de9e":"Altough we are working with only important features the use of less features provides no improvement in the score. It does however speed up the process.","5b97ab15":"### FUNCTION TO RUN FOR TEST\n\nAs there will be some preprocessing steps to run on the test set, I will collect them in a function and later embed them in the pipeline.","14bd2931":"## Feature Selection\n\nMoving on with all 100 Features does not seem efficient. Selecting fewer features may also boost performance.\n\nI will try two strategies\n1. Permutation Feature Importance (model specific)\n2. SelectKBest (model agnostic)\n","74cded14":"## AdaBoost","146336c7":"The result is a ROC-AUC of 0.55, which is not very high. However the score seems consistent troughtout the cv-folds, which is good. The process took relatively long, large models may not work well here.","0ba3fd38":"## Imports","da57fa1e":"Looks pretty balanced, not much to do here.\n\nThere are a lot of features. To get a sense of the scale differences I will sample from the statistical descriptions.","d564bec8":"### Compare Baseline Models","a6a8c047":"Check for NAs. As dataset has many columns the first step is to check if any NAs exist in the whole dataset.","59a4340f":"### Check correlations\nAs there are so many features we should first check if there are any at least moderately strong correlations (>.3). The first step is a visual inspection. If we would find any correlations here we could further investigate them.","fccda8e1":"Results look a bit better. RobustScaler + LogReg\/LinearSVC maybe worth submitting.","e8e29a18":"## Hyperparameter tuning for the best models (LogReg, LinearSVC)\nI decided to use BayesSearchCV here instead of GridSearch or RandomSearch. BayesSearchCV can be found in the skopt library and works similar to RandomSearch. In addition to the parameter space you have to define a number of possibilities to try. BayesSearch also takes the structure of the search space into account. This is my first time using it, I am curious to see how it works.","3644b648":"## Baseline Model\n\nLet's start with a simple Baseline model without preprocessing or using powerful models. This should only serve as a baseline, to show whether the problem can either be solved with a simple model, or to show if we went off the wrong path later on (e.g. by building a model that is much worse than the baseline. The baseline model also provides a sense of difficulty. We get a score that gives a sense of how high our metrics might be. Of course a powerful model can go way above the baseline. The last purpose of the baseline is a sanity check, if you try a lot of models but can barely get a score above the baseline something is wrong.","251234e3":"The first step I am taking is to evaluate default\/not-tuned models. I aim to tune the hyperparameters of the models which prove a) efficient and b) powerful.\nI will also use feature selection after this first round of evaluations\n\nThere are a couple Models I aim to try out:\n\n- Gaussian Naive Bayes\n- Logistic Regression\n- (Linear) Support Vector Machine\n    - LinearSVC (doesn't provide probability predictions)\n    - Stochastic Gradient Descent with hinge loss, which gives a Linear SVM\n- ~Random Forest~ took very long, not very efficient.\n- ~Ada Boost~ also took very long, removed for inefficiency.\n- XGBoost\n\n","d03d7a58":"I will try to tune the following models:\n- Logistic Regression in combination with Robust Scaling (best model so far)\n- Logistic Regression in combination with Quantile Transforming\n- Linear SVM in combination with Robust Scaling\n- Linear SVM in combination with Quantile Transforming","c89daf7d":"Downcast data due to size of dataset","5afe06dc":"## PCA\nDue to the size of the dataset a PCA might speed up the process considerably. It might even improve performance, this however happens rarely in my experience.","63c0c8b6":"Given how fast the LogReg is, and that it performs better with Robust Scaling than with Quantiles I may try all scalers on it to see if robust scaling is the best option.","9ddf0e5e":"## Exploration\n\nLet's view the target distribution. In case of it being uneven we might need to sample or pick certain models.","a6dd6643":"Performance was pretty good on the validation set. I will submit the predictions manually to not run this in the notebook again. Submission score was a roc-auc of 0.740. The model may have overfit, it did not perform better than simpler models.\n\nStill, XGB proves to be a good non-NN choice, but the model takes pretty long to train (hours) so it might not be efficient either way, unless you have powerful hardware or retraining is not an issue because it happens rarely. If you want or have to retrain often another model might be a better choice. \n\nAs stated before I will not pursue any hyperparameter-tuning due to the time-constraints.","8cbeeb9e":"## Logistic Regression","5154815c":"## Read Data","c7ed5554":"Naive Bayes performs much better than the Decision Tree, it also works much faster.\nNB + Quantile Transformer is surpringly strong. Will use it as a first submission and try to get better than that.","2cbd1b75":"Although I am using cross validation during model selection and hyperparameter tuning I am splitting of another validation set in order to have a \"sanity check\" after selecting and tuning a model.","d3f75fcc":"### Logistic Regression with Robust Scaling","57bc6e9e":"## Linear SVC\nI am using SGB with hinge loss which results in a linear SVM, and LinearSVC  neither however provide a `.predict_proba()` method and SVC is too slow, which makes them unsuited for submissions unfortunately. You can use them for submissions but scoring is suppossed to happen on probabilities not binary values.","705532b8":"### Permutation Feature Importance\n\nPermutation Feature Importance is a technique where a model is fit to the data and one feature is \"scrambled\", meaning it will be shuffled to break any relations between feature and target. The score of the model on this changed dataset is evaluated against the score for the intact dataset. If the score is significantly lower the feature is deemed important, as it contributes to a higher score when intact. This process is done for all features. The technique is always specific to a certain model and it's use of features. Result can not be extrapolated to other models.","2398b08c":"# Modelling","88755931":"I will try a linear PCA first, combined with all models just to check the base performance. If PCA works well, I will explore different kernels and vary the components a little.","09508caf":"First observation ist that Quantile Tranformation works much better with PCA than robust or standard scaling (I removed the Standard Scaler already). The second one is that PCA works pretty well, it speeds up the process a little (I didn't yet measure it, this observation is based on my personal observation which makes it not very accurate), yet it also reduces the ROC-AUC score. I would have liked to work with different Kernel PCAs at this point but due to time constraints on my part I will not pursue PCA further and move on to hyperparemeter tuning. If you had any good experience using PCA here let me know in the comments!","fe655a99":"## XGB\n\nAS other complex models took a very long time to run I will only train one XGBClassifier.\nI am using parameters here which I found in the [Notebook of WOO SEUNG HAN ](https:\/\/www.kaggle.com\/hadeux\/kor-eng-simple-xgboost-model\/notebook). Please visit this notebook and upvote it if you found it as useful as I did.\n\n**The next cell will take hours to run, so please be cautious if you want to use it.**","0f035116":"Random Forest takes a very long time so does AdaBoost. Will not continue with them. Long training times makes Hyperparametertuning very inefficient.\n\nRobust Scaling + Logistic Regression worked best so far.\nI will go ahead and try to optimize this model but also work on the feature selection.","71e86e18":"## SelectKBest\n\nSelectKBest selects features according to the k highest scores given by a scoring function. The default is the ANOVA F-value between label\/feature, there are many other scoring functions available such as chi\u00b2 (chi2 only takes positive values which rules it out here).","51c7401b":"I am manually setting the best estimator here as the Search takes very long.","62506a69":"Summary statistics differ quite a lot. The data needs to be scaled.","26f68b89":"Those features should be our best features with respect to their relationship with the target. Let's try the above used algorithms again on only these.","60db000e":"## Train-Val-Split, Feature-Target-Split","030d6a88":"Visualizing the Distributions (takes very long)","7517c40f":"Again working with a reduced set of features hasn't improved the scores.\n\nI will try one more round of feature selection with 70 instead of 40 features, hoping to just filter out the really unimportant ones.","ef60d860":"# The Challenge\n\nThe dataset used here is synthetic. The original dataset dealt with identifying spam emails.\nSubmissions are evaluated on area under the ROC curve between the predicted **probability** and the observed target.\n\nThe challenge is thus to find a suitable classifier for this binary classification task. The features are all numeric. The dataset is realatively large. \n\n**THIS NOTEBOOK IS STILL A WORK-IN-PROGRESS, HOWEVER FEEL FREE TO EXPLORE, COMMENT AND UPVOTE**","4323c8d9":"### Logistic Regression with Quantile Scaling","fc7f233e":"Another Preprocessing step will be the **Scaling** of features. This will be done in a pipeline and thus not for the whole training set before splitting.\nAs I wish to scale the data as well as transform the distributions to a more gaussian format (especially those that look to follow a poisson or even exponential distribution) I will use the QuantileTransformer. I will however at least for fast models test other Scalers (i.e. Standard,Robust,MinMax -Scaler) as well.","8acc1b5e":"No NAs. A welcome suprise.","c9dbc91b":"Results:\n\n\nBest CV ROC-AUC 0.743305737543616\n\nBest Estimator\n```python\nPipeline(steps=[('scaler', RobustScaler()),\n                ('logreg',\n                 LogisticRegression(C=0.01291549665014884, l1_ratio=0.1,\n                                    max_iter=500, n_jobs=-1,\n                                    penalty='elasticnet', random_state=13,\n                                    solver='saga'))])\n```","db78d1c4":"## Random Forest ","0a21ff17":"## Preprocessing\n\nI am dropping the ID colum as it shouldn't provide value. Generally IDs can be tricky and might even lead to data leakage if they encode more than the index.","b3e3dd01":"If you run this cell you find that about half of the features have a bimodal distribution. The other half as a very skewed distribution that looks like a poisson or even exponential. I have attached the result as an image to spare some time.\n![image.png](attachment:ea604e4f-f61b-4e0d-8b55-232f7948b885.png)","8e059624":"I often found that Decision Tree Classifiers are a good pick for baseline models as they don't require much preprocessing and are fairly quick and not too powerful.","e386aefa":"The submission of the tuned logistic regression with RobustScaler was as good as the default variant.","17ee736e":"It doesn't look like there are any medium-strong or strong correlations. Which is good for avoiding multicolinearity and makes later feature selection e.g. through permutation feature importance easier. However, it also means that there are no simple linear relationships between target and features that our model could easily learn."}}