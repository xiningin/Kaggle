{"cell_type":{"06eaa72c":"code","00c152bd":"code","43ef85f0":"code","3a9d78bf":"code","c0d2323e":"code","f7ad713c":"code","089e1ed3":"code","350bd59d":"code","8fed28de":"code","2a04bee8":"code","c65276c3":"code","1f214d7f":"code","b2cebbb2":"code","38330324":"markdown","29ce964e":"markdown","097b6f8a":"markdown","7c1b32e9":"markdown","51c49049":"markdown","4ad6f2d7":"markdown","ddc983ab":"markdown"},"source":{"06eaa72c":"# basic\nimport os\nimport sys\nimport datetime\nfrom pathlib import Path\nimport json\nfrom typing import List, Optional, Dict, Generator, NamedTuple, Any, Tuple, Union, Mapping\nfrom enum import Enum\nimport re\n\n# data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom fastprogress import master_bar, progress_bar\n\n# ml\nimport tensorflow as tf","00c152bd":"!pip install tensorflow_datasets","43ef85f0":"tf.__version__","3a9d78bf":"devices = tf.config.experimental.list_physical_devices('GPU')\nprint(devices)\nfor d in devices:\n    tf.config.experimental.set_memory_growth(d, True)","c0d2323e":"class Config:\n    model_name = 'resnet50_ga_64'\n    num_epochs = 1  # for commit, original = 20\n    batch_size = 32\n    learning_rate = 1e-2\n    num_grad_accumulates = 2\n    image_size = 32\n    step_summary_output = 10\n\nconfig = Config()","f7ad713c":"def transform(x, y):\n    return tf.cast(x, tf.float32) \/ 255.0, y","089e1ed3":"import tensorflow_datasets as tfds\ndataset, info = tfds.load(name='cifar10', split=tfds.Split.TRAIN, with_info=True, as_supervised=True)\ndataset = dataset.map(transform)\ndataset = dataset.shuffle(1024).batch(config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\ninfo","350bd59d":"from tensorflow.keras.applications import ResNet50","8fed28de":"class Model(tf.keras.Model):\n    def __init__(self, num_outputs: int) -> None:\n        super().__init__()\n        self.num_outputs = num_outputs\n        \n    def build(self, input_shape) -> None:\n        self.core = ResNet50(\n            include_top=False,\n            weights='imagenet',\n            input_tensor=None,\n            input_shape=None,\n            pooling='max',\n            classes=10,\n        )\n        self.output_layer = tf.keras.layers.Dense(self.num_outputs)\n        super().build(input_shape)\n    \n    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n        y = self.core(x)\n        y = self.output_layer(y)\n        return y","2a04bee8":"model = Model(num_outputs=10)\n# build\ninput_shape = (config.image_size, config.image_size, 3)\ninput = tf.keras.layers.Input(shape=input_shape, name='input_layer', dtype=tf.float32)\n_ = model(input)\nmodel.summary()","c65276c3":"def accumulated_gradients(gradients: Optional[List[tf.Tensor]],\n                          step_gradients: List[Union[tf.Tensor, tf.IndexedSlices]],\n                          num_grad_accumulates: int) -> tf.Tensor:\n    if gradients is None:\n        gradients = [flat_gradients(g) \/ num_grad_accumulates for g in step_gradients]\n    else:\n        for i, g in enumerate(step_gradients):\n            gradients[i] += flat_gradients(g) \/ num_grad_accumulates\n        \n    return gradients\n\n# This is needed for tf.gather like operations.\ndef flat_gradients(grads_or_idx_slices: tf.Tensor) -> tf.Tensor:\n    '''Convert gradients if it's tf.IndexedSlices.\n    When computing gradients for operation concerning `tf.gather`, the type of gradients \n    '''\n    if type(grads_or_idx_slices) == tf.IndexedSlices:\n        return tf.scatter_nd(\n            tf.expand_dims(grads_or_idx_slices.indices, 1),\n            grads_or_idx_slices.values,\n            grads_or_idx_slices.dense_shape\n        )\n    return grads_or_idx_slices","1f214d7f":"loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n                                                        reduction=tf.keras.losses.Reduction.NONE)\noptimizer = tf.keras.optimizers.Adam(config.learning_rate)\ntrain_loss = tf.keras.metrics.Mean('loss\/train', dtype=tf.float32)\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy\/train')\nmetrics = [train_loss, train_accuracy]\n\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntrain_log_dir = f'logs\/{config.model_name}\/{current_time}'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\n\n\ndef train(config: Config,\n          dataset: tf.data.Dataset,\n          model: Model):\n    global_step = 0\n    for e in range(config.num_epochs):\n        global_step = train_epoch(config, dataset, model, global_step)\n        print(f'{e+1} epoch finished. step: {global_step}')\n\n\ndef train_epoch(config: Config,\n                dataset: tf.data.Dataset,\n                model: Model,\n                start_step: int = 0) -> tf.Tensor:\n    '''Train 1 epoch\n    '''\n    gradients = None\n    global_step = start_step\n    for i, batch in enumerate(dataset):\n        dummy_step = i + start_step * config.num_grad_accumulates\n        x_train, y_train = batch\n        step_gradients = train_step(x_train, y_train, loss_fn, optimizer)\n        gradients = accumulated_gradients(gradients, step_gradients, config.num_grad_accumulates)\n        if (dummy_step + 1) % config.num_grad_accumulates == 0:\n            gradient_zip = zip(gradients, model.trainable_variables)\n            optimizer.apply_gradients(gradient_zip)\n            gradients = None\n            if (global_step + 1) % config.step_summary_output == 0:\n                write_train_summary(train_summary_writer, metrics, step=global_step + 1)\n            global_step += 1\n\n    return global_step\n\n\n@tf.function\ndef train_step(x_train: tf.Tensor,\n               y_train: tf.Tensor,\n               loss_fn: tf.keras.losses.Loss,\n               optimizer: tf.keras.optimizers.Optimizer):\n    '''Train 1 step and return gradients\n    '''\n    with tf.GradientTape() as tape:\n        outputs = model(x_train, training=True)\n        loss = tf.reduce_mean(loss_fn(y_train, outputs))\n    train_loss(loss)\n    train_accuracy(y_train, tf.nn.softmax(outputs))\n    gradients = tape.gradient(loss, model.trainable_variables)\n    return gradients\n\n\ndef write_train_summary(writer: tf.summary.SummaryWriter,\n                        metrics: List[tf.keras.metrics.Metric],\n                        step: int) -> None:\n    with writer.as_default():\n        for metric in metrics:\n            tf.summary.scalar(metric.name, metric.result(), step=step)\n            metric.reset_states()\n            ","b2cebbb2":"%%time\ntrain(config, dataset, model)","38330324":"The `accumulated_gradients` method is the one that average gradients in some batches.\n\nIn the case of NLP model, we use tf.gather method in the embedding layer.\nBecause gradient of operation tf.gather turn to type tf.IndexSlices, we have to convert it to tf.Tensor in order to get average gradients.\n`flat_gradient` is the method to convert.","29ce964e":"This notebook is an explanation of gradients accumulation by custom loop training for TensorFlow 2.0.\nMain part is Training term.\nPlease have a comment if you know better way.\n\nThis example uses CIFAR10 dataset. You can use this in NLP model like Transformer, BERT, etc.","097b6f8a":"# Model","7c1b32e9":"# Training","51c49049":"![gradient_accumulation.png](attachment:gradient_accumulation.png)","4ad6f2d7":"# Dataloader","ddc983ab":"# Config"}}