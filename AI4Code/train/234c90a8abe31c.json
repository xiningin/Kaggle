{"cell_type":{"21e180ca":"code","7027733e":"code","999a71b7":"code","af8059f0":"code","e19bbe21":"code","3363823b":"code","145c4ce7":"code","a2eb555f":"code","28140493":"code","fc82b26b":"code","fecd53bf":"code","e69f1b0b":"code","70e2fe19":"code","845bbcb9":"code","3558aae5":"code","fac23be4":"code","5115a2cf":"code","cd1127cf":"code","e8685c2b":"code","9534d182":"code","00ae1496":"code","f5d6adef":"code","6e57610b":"code","fa724792":"code","907195a4":"code","dfc56a69":"code","a3e133db":"code","8a0515a8":"code","10660862":"code","17a03715":"code","a640e41f":"code","11c9f48b":"code","5052d2cb":"code","f86ae272":"code","a4924525":"code","06ae8655":"code","e85314e4":"code","64bf9406":"code","2127ef66":"markdown","0b250343":"markdown","ad1529a8":"markdown","889795b0":"markdown","02a42e48":"markdown","6a160ba4":"markdown","491e164e":"markdown","a80b861d":"markdown","07958ba4":"markdown","3e36c1dd":"markdown","660faa46":"markdown","fdb09488":"markdown","cd05385c":"markdown","359478b8":"markdown","3b6c67b6":"markdown","382e7099":"markdown","a53cf8e4":"markdown","a5854fdc":"markdown","16269ec2":"markdown","4aab8dcd":"markdown"},"source":{"21e180ca":"import math\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import kurtosis, skew\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7027733e":"train = pd.read_csv(\"..\/input\/challenge8\/train.csv\")\ntest = pd.read_csv(\"..\/input\/challenge8\/test.csv\")","999a71b7":"train.describe()","af8059f0":"train.head()","e19bbe21":"print (f\"Train has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint (f\"Test has {test.shape[0]} rows and {test.shape[1]} columns\")","3363823b":"train[\"Product\"] = train[\"Product\"].apply(lambda word: word.replace('P-',''))\ntrain[\"Product_Brand\"] = train[\"Product_Brand\"].apply(lambda word: word.replace('B-',''))\n\ntest[\"Product\"] = test[\"Product\"].apply(lambda word: word.replace('P-',''))\ntest[\"Product_Brand\"] = test[\"Product_Brand\"].apply(lambda word: word.replace('B-',''))\n","145c4ce7":"def plot_fn(df, feature):\n    \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    # grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    \n\n    # histogram grid customizing. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n    \nplot_fn(train, 'Selling_Price')","a2eb555f":"train['Product'] = train['Product'].astype(int)\ntrain['Product_Brand'] = train['Product_Brand'].astype(int)\n\ntest['Product'] = test['Product'].astype(int)\ntest['Product_Brand'] = test['Product_Brand'].astype(int)","28140493":"# skewness and kurtosis\nprint(\"Skewness: \" + str(train['Selling_Price'].skew()))\nprint(\"Kurtosis: \" + str(train['Selling_Price'].kurt()))","fc82b26b":"# correlation of all the features with target variable. \n(train.corr()**2)[\"Selling_Price\"].sort_values(ascending = False)[1:]\n","fecd53bf":"# trainsforming target variable using numpy.log1p, \ntrain[\"SalePrice\"] = np.log1p(train[\"Selling_Price\"])\n\n# Plotting the newly transformed response variable\nplot_fn(train, 'SalePrice')","e69f1b0b":"train.drop(columns=['Selling_Price'],axis=1, inplace=True)\n\n## Saving the target values in \"y_train\". \ny = train['SalePrice'].reset_index(drop=True)\n","70e2fe19":"## Combining train and test datasets together so that we can do all the work at once. \nall_data = pd.concat((train, test)).reset_index(drop = True)\n## Dropping the target variable. \nall_data.drop(['SalePrice'], axis = 1, inplace = True)","845bbcb9":"all_data.head()","3558aae5":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewed_feats\n","fac23be4":"final_features = pd.get_dummies(all_data).reset_index(drop=True)\nfinal_features.shape","5115a2cf":"X = final_features.iloc[:len(y), :]\n\nX_sub = final_features.iloc[len(y):, :]\n","cd1127cf":"def overfit_reducer(df):\n    \"\"\"\n    This function takes in a dataframe and returns a list of features that are overfitted.\n    \"\"\"\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > 99.94:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\n\n\noverfitted_features = overfit_reducer(X)\n","e8685c2b":"X = X.drop(overfitted_features, axis=1)\nX_sub = X_sub.drop(overfitted_features, axis=1)","9534d182":"X.shape,y.shape, X_sub.shape\n","00ae1496":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_log_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n","f5d6adef":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n","6e57610b":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                                              alphas=alphas2, \n                                              random_state=42, \n                                              cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","fa724792":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)                             \n","907195a4":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","dfc56a69":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","a3e133db":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","8a0515a8":"score = cv_rmse(ridge)\nprint(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","10660862":"print('START Fit')\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge') \nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","17a03715":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.2 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))\n","a640e41f":"print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))\n","11c9f48b":"print('Predict submission')\nsubmission = pd.read_excel(\"..\/input\/challenge8\/sample_submission.xlsx\")\nsubmit = np.floor(np.expm1(blend_models_predict(X_sub)))","5052d2cb":"submit","f86ae272":"submit1 = np.expm1(blend_models_predict(X_sub))","a4924525":"submit1","06ae8655":"s = pd.DataFrame({'Selling_Price': submit1})","e85314e4":"s.head()","64bf9406":"s.to_excel(\"submission.xlsx\", index=False)","2127ef66":"# E-commerce Price Prediction: Weekend Hackathon #8\n","0b250343":"# Shape of Dataset","ad1529a8":"# Training","889795b0":"# Blending all models","02a42e48":"# Prediction","6a160ba4":"# Convert the type ","491e164e":"# Read the Dataset","a80b861d":"# Transform the target variable and Plot","07958ba4":"# Import Required Library","3e36c1dd":"# Correlation","660faa46":"# Train and Test Data from combined dataset","fdb09488":"\n    target variable, Selling Price is not normally distributed.\n    target variable is right-skewed.\n    There are multiple outliers in the variable.\n","cd05385c":"# Remove Overfit data","359478b8":"# Set target variable","3b6c67b6":"# Plotting","382e7099":"# Check Skewness","a53cf8e4":"# Metrics ","a5854fdc":"# Submission File","16269ec2":"# RMSLE Score","4aab8dcd":"# Remove Letter from Product and Product Brand"}}