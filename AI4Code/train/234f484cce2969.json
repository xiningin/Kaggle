{"cell_type":{"d881d3dd":"code","46302204":"code","7c63f728":"code","5c463421":"code","13c224a3":"code","d1e31105":"code","8125e837":"code","fa0e375f":"code","a28b1a72":"code","90b9353e":"code","f0b37941":"code","e2047fcc":"code","20250108":"code","ce6f23db":"code","eba900c9":"code","54f67625":"code","45aa4711":"code","6859e2db":"code","f117d51f":"code","0bbf3906":"code","1efdf221":"code","5d78d165":"code","308d01d0":"code","99438515":"code","5f9249a2":"code","74916721":"code","bd413f33":"code","b18fad8d":"code","1a5bdeb1":"code","d2651c1d":"code","d38cc6f7":"code","7bd3c581":"code","f6e766a2":"code","8289b69e":"code","e17517b6":"code","70c56243":"code","5be6821a":"code","1ef2b088":"markdown","db027749":"markdown","cff81111":"markdown","3b5690a1":"markdown","d5fbccd1":"markdown","65139d7f":"markdown","95ffb1dc":"markdown","9c4fd52d":"markdown","4431b393":"markdown","ee954dbf":"markdown"},"source":{"d881d3dd":"!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric  -f https:\/\/pytorch-geometric.com\/whl\/torch-1.9.0+cpu.html  -Uq","46302204":"import os\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nfrom fastprogress import progress_bar\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import NNConv, TransformerConv, PNAConv\n\nimport torch_geometric.nn as pyg_nn\nimport torch_geometric.utils as pyg_utils\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nimport wandb\n\n\nimport datetime\nfrom pytz import timezone\nnow = datetime.datetime.now(timezone('UTC'))\nyyyymmdd_hhmm = \"{0:%Y%m%d_%H%M}\".format(now.astimezone(timezone('Asia\/Tokyo')))","7c63f728":"# \u4e71\u6570\u30b7\u30fc\u30c9\u306e\u56fa\u5b9a\ndef set_seed(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(seed=42)","5c463421":"NOTE_ID = 'CLB_N007'","13c224a3":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","d1e31105":"DATA_DIR = Path(\"\/kaggle\/input\/shigglecup-2nd\/\")\nprint(os.listdir(DATA_DIR))","8125e837":"df_train = pd.read_csv(DATA_DIR \/ \"train.csv\")\ndf_test = pd.read_csv(DATA_DIR \/ \"test.csv\")\ndf_team = pd.read_csv(DATA_DIR \/ \"team_id.csv\")\ndf_pokemon = pd.read_csv(DATA_DIR \/ \"pokemon.csv\")\ndf_type = pd.read_csv(DATA_DIR \/ \"typetable.csv\")\ndf_sub = pd.read_csv(DATA_DIR \/ \"sample_submission.csv\")","fa0e375f":"df_pokemon['Legendary'] = df_pokemon['Legendary'].apply(lambda x: 1 if x == True else 0)","a28b1a72":"# \u6a19\u6e96\u5316\nstatus_cols = ['HP', 'Attack', 'Defense',\n       'Sp_Atk', 'Sp_Def', 'Speed', 'Generation']\nscaler = StandardScaler()\ndf_pokemon[status_cols] = scaler.fit_transform(df_pokemon[status_cols])","90b9353e":"def merge_team(df_mart, df_team):\n    \"\"\"team\u60c5\u5831\u3092\u30de\u30fc\u30b8\"\"\"\n    pokemon_cols = ['pokemon_id_1', 'pokemon_id_2', 'pokemon_id_3',\n       'pokemon_id_4', 'pokemon_id_5', 'pokemon_id_6']\n\n    df_first = pd.merge(df_mart, df_team, left_on='first', right_on='team_id', how='left')\n    df_second = pd.merge(df_mart, df_team, left_on='second', right_on='team_id', how='left')\n\n    df_mart = pd.concat([\n                            df_mart,\n                            df_first[pokemon_cols].add_suffix('_first'),\n                            df_second[pokemon_cols].add_suffix('_second'),\n                        ], axis=1)\n    return df_mart","f0b37941":"df_train = merge_team(df_train, df_team)\ndf_test = merge_team(df_test, df_team)\n\ndf_train.shape, df_test.shape","e2047fcc":"def merge_pokemon(df_mart, df_pokemon):\n    \"\"\"\u30dd\u30b1\u30e2\u30f3\u60c5\u5831\u3092\u30de\u30fc\u30b8\"\"\"\n    pokemon_cols = ['pokemon_id_1', 'pokemon_id_2', 'pokemon_id_3',\n       'pokemon_id_4', 'pokemon_id_5', 'pokemon_id_6']\n    pokemon_id_cols = [c + '_first' for c in pokemon_cols] + [c + '_second' for c in pokemon_cols]\n    pokemon_detail_cols = ['Type_1', 'Type_2', 'HP', 'Attack', 'Defense',\n        'Sp_Atk', 'Sp_Def', 'Speed', 'Generation', 'Legendary']\n\n    df_list = []\n    for col in pokemon_id_cols:\n        suffix = col[10:] # '_1_first'\u307f\u305f\u3044\u306a\n        \n        df_tmp = pd.merge(df_mart, df_pokemon, left_on=col, right_on='pokemon_id', how='left')\n        df_list.append(df_tmp[pokemon_detail_cols].add_suffix(suffix))\n    \n    df_mart = pd.concat([df_mart] + df_list, axis=1)\n\n    return df_mart","20250108":"df_train = merge_pokemon(df_train, df_pokemon)\ndf_test = merge_pokemon(df_test, df_pokemon)\n\ndf_train.shape, df_test.shape","ce6f23db":"def build_nodes(sample: pd.DataFrame):\n    # speed_cols = [f\"Speed_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    speed_cols = [f\"Speed_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    hp_cols = [f\"HP_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    attack_cols = [f\"Attack_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    defense_cols = [f\"Defense_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    sp_atk_cols = [f\"Sp_Atk_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    sp_def_cols = [f\"Sp_Def_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    gen_cols = [f\"Generation_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    legend_cols = [f\"Legendary_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    x = np.vstack([\n                   sample[speed_cols].astype(float).values,\n                   sample[hp_cols].astype(float).values,\n                   sample[attack_cols].astype(float).values,\n                   sample[defense_cols].astype(float).values,\n                   sample[sp_atk_cols].astype(float).values,\n                   sample[sp_def_cols].astype(float).values,\n                   sample[gen_cols].astype(float).values,\n                   sample[legend_cols].astype(int).values,\n                   ]).T # [num_nodes, num_node_features]\n    return torch.tensor(x, dtype=torch.float)\n","eba900c9":"# \u30b5\u30f3\u30d7\u30eb\u78ba\u8a8d\n# 1\u8a66\u5408\u76ee\u306e12\u30dd\u30b1\u30e2\u30f3x8\u7279\u5fb4\u91cf\nx = build_nodes(df_train.iloc[0])\nprint(x)","54f67625":"def build_edges(sample: pd.DataFrame):\n    speed_cols = [f\"Speed_{num}_{team}\" for team in ['first', 'second'] for num in range(1,7) ]\n    num_nodes = 12\n    speed_val = sample[speed_cols].astype(float).values\n\n    edge_idx, speed_diff = [], []\n    # 0-5: first team\n    # 6-11: second team\n    for f_poke in range(6):\n        for s_poke in range(6, 12):\n            edge_idx.append([f_poke,s_poke])\n            speed_diff.append(speed_val[f_poke] - speed_val[s_poke])\n            \n    edge_attr = np.vstack([speed_diff]).T # [num_edges, num_edge_features]\n    return torch.tensor(edge_idx, dtype=torch.long).t().contiguous(), torch.tensor(edge_attr, dtype=torch.float)\n","45aa4711":"# \u30b5\u30f3\u30d7\u30eb\u78ba\u8a8d\n# \u30a8\u30c3\u30b8\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\uff08\u3069\u306e\u30ce\u30fc\u30c9\u3092\u63a5\u7d9a\u3057\u3066\u3044\u308b\u304b\uff09\u3001\u3068\u30a8\u30c3\u30b8\u306e\u6301\u3064\u7279\u5fb4\u91cf\uff08\u30b9\u30d4\u30fc\u30c9\u5dee\uff09\nedge_index, edge_attr  = build_edges(df_train.iloc[0])\nprint(\"edge index:\", edge_index)\nprint(\"edge attr:\", edge_attr)","6859e2db":"def build_labels(sample: pd.DataFrame):\n    y = sample.target\n    return torch.tensor(y, dtype=torch.long)","f117d51f":"# \u30b5\u30f3\u30d7\u30eb\u78ba\u8a8d\ny = build_labels(df_train.iloc[0])\nprint(\"y:\", y)","0bbf3906":"# Data\u3068\u3057\u3066\u307e\u3068\u3081\u305f\u5834\u5408\u306e\u30b5\u30f3\u30d7\u30eb\ndata = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\ndata","1efdf221":"def build_data_list(df_input: pd.DataFrame, istrain=True):\n    data_list = []\n    for _, row in df_input.iterrows():\n        x = build_nodes(row)\n        edge_index, edge_attr = build_edges(row)\n        if istrain:\n            y = build_labels(row)\n        else:\n            y = 0\n        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n        data_list.append(data)\n    return data_list","5d78d165":"# config \u306e\u5b9a\u7fa9\u306f\u5f8c\u534a\u306b\u3082\u5206\u5272\u3057\u3066\u3067\u3066\u304d\u307e\u3059\u3002\u3002\nconfig = {\n   'loss_fn':  nn.CrossEntropyLoss(),\n    'batch_size': 64,\n    'node_hidden_channels': 8,\n    'edge_hidden_channels': 4,\n    'dropout_rate': 0.05,\n}","308d01d0":"fold_num = 6\nkf = GroupKFold(n_splits=fold_num)\n\nfor i, (train_idx, test_idx) in enumerate(kf.split(df_train, df_train['target'], groups=df_train[\"first\"])):\n    df_tr = df_train.iloc[train_idx]\n    df_va = df_train.iloc[test_idx]\n    break","99438515":"df_tr.shape, df_va.shape, df_test.shape","5f9249a2":"train_data_list = build_data_list(df_tr)\nvalid_data_list = build_data_list(df_va)\ntest_data_list = build_data_list(df_test, istrain=False)","74916721":"train_loader = DataLoader(train_data_list, batch_size=config['batch_size'], shuffle=True)\nvalid_loader = DataLoader(valid_data_list, batch_size=config['batch_size'], shuffle=False)\ntest_loader = DataLoader(test_data_list, batch_size=config['batch_size'], shuffle=False)","bd413f33":"class EarlyStopping:\n    def __init__(self, patience=7, mode=\"min\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","b18fad8d":"class SimpleGCN(nn.Module):\n    def __init__(self,\n                 num_node_features: int,\n                 num_edge_features: int,\n                 node_hidden_channels: int,\n                 edge_hidden_channels: int,\n                 num_classes: int,\n                 dropout_rate: float):\n        super(SimpleGCN, self).__init__()\n\n        self.node_encoder = nn.Linear(num_node_features, node_hidden_channels)\n        self.edge_encoder = nn.Linear(num_edge_features, edge_hidden_channels)\n        self.conv1 = NNConv(in_channels=node_hidden_channels,\n                            out_channels=node_hidden_channels,\n                            nn=nn.Linear(edge_hidden_channels, \n                                         node_hidden_channels * node_hidden_channels))\n        self.conv2 = NNConv(in_channels=node_hidden_channels,\n                            out_channels=node_hidden_channels,\n                            nn=nn.Linear(edge_hidden_channels, \n                                         node_hidden_channels * node_hidden_channels))\n        self.dropout = nn.Dropout(dropout_rate)\n        self.bn = nn.BatchNorm1d(node_hidden_channels)\n        self.linear = nn.Linear(node_hidden_channels, num_classes)\n\n    def forward(self, data):\n        x = data.x\n        edge_index = data.edge_index\n        edge_attr = data.edge_attr\n        batch = data.batch\n\n        x = self.node_encoder(x) # [num_nodes, node_hidden_channels]\n        edge_attr = self.edge_encoder(edge_attr) # [num_edges, node_edge_features]\n        x = F.relu(self.conv1(x, edge_index, edge_attr)) # [num_nodes, node_hidden_channels]\n        x = F.relu(self.conv2(x, edge_index, edge_attr)) # [num_nodes, node_hidden_channels]\n        x = self.linear(x) # [num_nodes, num_classes]\n        x = self.dropout(x)\n        x = pyg_nn.global_mean_pool(x, batch)\n\n        return F.softmax(x, dim=1)\n","1a5bdeb1":"model = SimpleGCN(num_node_features=train_data_list[0].x.shape[1],\n                  num_edge_features=train_data_list[0].edge_attr.shape[1],\n                  node_hidden_channels=config['node_hidden_channels'],\n                  edge_hidden_channels=config['edge_hidden_channels'],\n                  num_classes=2,\n                  dropout_rate=config['dropout_rate'])\nmodel.to(device);","d2651c1d":"config['optimizer'] = torch.optim.Adam(model.parameters(), lr=1e-3)\nconfig['early_stopping'] = EarlyStopping(patience=50, mode=\"min\")\noptimizer = config['optimizer']\nloss_fn = config['loss_fn']\nes = config['early_stopping']\n","d38cc6f7":"def train_step(model, data_loader, config):\n    model.train()\n    preds, targets, losses = [], [], []\n    with tqdm(total=len(data_loader), unit=\"batch\") as pbar:\n        pbar.set_description(f\"[train] Epoch {epoch+1}\/{EPOCH}\")\n        for batch_idx, data in enumerate(data_loader):\n            optimizer.zero_grad()\n            data = data.to(device)\n            target = data.y\n            output = model(data)\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n            losses.append(loss.item())\n            preds.append(torch.argmax(output, dim=1).detach().cpu().numpy())\n            targets.append(target.detach().cpu().numpy())\n            pbar.set_postfix(tr_loss=np.array(losses).mean(), \n                             auc=roc_auc_score(np.hstack(targets), np.hstack(preds)))\n            pbar.update(1)\n\ndef eval_step(model, data_loader, config):\n    model.eval()\n    preds, targets, losses = [], [], []\n    with tqdm(total=len(data_loader), unit=\"batch\") as pbar:\n        pbar.set_description(f\"[eval]  Epoch {epoch+1}\/{EPOCH}\")\n        with torch.no_grad():\n            for batch_idx, data in enumerate(data_loader):\n                data = data.to(device)\n                target = data.y\n                output = model(data)\n                loss = loss_fn(output, target)\n                losses.append(loss.item())\n                target = target.cpu().numpy()\n                preds.append(torch.argmax(output, dim=1).detach().cpu().numpy())\n                targets.append(target)\n                pbar.set_postfix(va_loss=np.array(losses).mean(),\n                                 auc=roc_auc_score(np.hstack(targets), np.hstack(preds)))\n                pbar.update(1)\n    \n    val_loss = np.array(losses).mean()\n    return val_loss, model\n\ndef inference_step(model, data_loader):\n    model.eval()\n    logits, preds, targets = [], [], []\n    with tqdm(total=len(data_loader), unit=\"batch\") as pbar:\n        pbar.set_description(f\"[inference]\")\n        with torch.no_grad():\n            for data in data_loader:\n                data = data.to(device)\n                output = model(data)\n                logits.append(output.cpu().numpy())\n                output = torch.argmax(output, dim=1).cpu().numpy()\n                preds.append(output)\n                pbar.update(1)\n\n    return np.vstack(logits), np.hstack(preds)","7bd3c581":"%%time\nEPOCH = 1000\n\nfor epoch in progress_bar(range(EPOCH)):\n    train_step(model, train_loader, config)\n    val_loss, model = eval_step(model, valid_loader, config)\n\n    es(val_loss, model, model_path=f\".\/model_{yyyymmdd_hhmm}.pth\")\n    if es.early_stop:\n        print(\"Early stopping\")\n        break","f6e766a2":"model.load_state_dict(torch.load(f'.\/model_{yyyymmdd_hhmm}.pth'))\ntest_proba, test_preds = inference_step(model, test_loader)\ntest_proba","8289b69e":"df_sub.head(3)","e17517b6":"df_sub['target'] = test_proba[:,1]","70c56243":"df_sub.head()","5be6821a":"filename = f\".\/{NOTE_ID}_{yyyymmdd_hhmm}_df_sub.csv\"\ndf_sub.to_csv(filename, index=None)\n","1ef2b088":"## Inference","db027749":"# Split\u3068DataLoader\n- \u4eca\u56de\u306f\u30b5\u30f3\u30d7\u30eb\u306a\u306e\u3067CV\u3067\u306f\u306a\u304f\u3001train-test split\u306a1fold\u5206\u3057\u304b\u5206\u3051\u3066\u307e\u305b\u3093\u3002\uff08\u5206\u5272\u306fGroupK\u3067\u5b9f\u65bd\uff09\n","cff81111":"# \u6982\u8981\n\nGNN\u3092\u7528\u3044\u305f\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb\u3067\u3059\u3002\n\n\u7279\u5fb4\u91cf\u3082\u9650\u308a\u306a\u304f\u5c11\u306a\u3044\u306e\u3067\u6539\u5584\u3059\u308c\u3070\u7d50\u69cb\u30b9\u30b3\u30a2\u4f38\u3073\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n\nNN\u306e\u69cb\u9020\u5468\u308a\u306b\u3064\u3044\u3066\u306f\u3001NN\u7cfb\u521d\u5fc3\u8005\u306a\u306e\u3067\u57fa\u672c\u7684\u306a\u30df\u30b9\u3082\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n\u3082\u3057\u898b\u3064\u3051\u305f\u65b9\uff08\u3082\u3057\u304f\u306f\u3053\u3046\u3057\u3066\u307f\u3066\u3082\u826f\u3044\u304b\u3082\uff01\uff09\u306a\u3069\u3042\u308c\u3070\n\u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3051\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002\n","3b5690a1":"# Submission","d5fbccd1":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u30fb\u524d\u51e6\u7406\u30fb\u30de\u30fc\u30b8","65139d7f":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u8aad\u307f\u8fbc\u307f","95ffb1dc":"# \u30ce\u30fc\u30c9\u3068\u30a8\u30c3\u30b8\u3001\u30e9\u30d9\u30eb\u3092\u5b9a\u7fa9\n- GNN\u3067\u306f\u30ce\u30fc\u30c9\u3068\u30a8\u30c3\u30b8\u3068\u3044\u3046\uff12\u3064\u306e\u8981\u7d20\u306b\u3088\u3063\u3066\u30b0\u30e9\u30d5\u69cb\u9020\u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\n    - \u4eca\u56de\u306f\u4ee5\u4e0b\u3067\u69cb\u9020\u3092\u3082\u305f\u305b\u307e\u3059\u3002\n    - \u30ce\u30fc\u30c9\uff1a\u30dd\u30b1\u30e2\u30f3\uff08first\u30c1\u30fc\u30e06\u4f53\u3001second\u30c1\u30fc\u30e06\u4f53\u306e\u5408\u8a0812\u4f53\uff09\n    - \u30a8\u30c3\u30b8\uff1a\u30dd\u30b1\u30e2\u30f3\u540c\u58eb\u306e\u5bfe\u6226\u7d44\u307f\u5408\u308f\u305b\uff08first\u30c1\u30fc\u30e06\u4f53\u3068second\u30c1\u30fc\u30e06\u5bfe\u306e\u5168\u7d44\u307f\u5408\u308f\u305b 6x6 = 36\u7d44\u307f\u5408\u308f\u305b\uff09\n\n\n- \u30ce\u30fc\u30c9\u3068\u30a8\u30c3\u30b8\u306b\u306f\u305d\u308c\u305e\u308c\u7279\u5fb4\u91cf\u3092\u3082\u305f\u305b\u3089\u308c\u307e\u3059\u3002\n    - \u4eca\u56de\u306f\u7c21\u5358\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7279\u5fb4\u91cf\u3092\u3082\u305f\u305b\u3066\u3044\u307e\u3059\u3002\n    - \u30ce\u30fc\u30c9\uff1a\u5404\u30dd\u30b1\u30e2\u30f3\u306e\u30b9\u30c6\u30fc\u30bf\u30b9\u5024\uff08\u6a19\u6e96\u5316\u6e08\u307f\uff09 -> 8\u7279\u5fb4\u91cf\n    - \u30a8\u30c3\u30b8\uff1a\u5bfe\u6226\u3059\u308b\u30dd\u30b1\u30e2\u30f3\u540c\u58eb\u306e\u30b9\u30d4\u30fc\u30c9\u5024\u306e\u5dee(first - second) -> 1\u7279\u5fb4\u91cf\n    \n    \n- \u30e9\u30d9\u30eb\u306f\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u5024\u3002GNN\u3067\u306f\u3001\u5927\u304d\u304f\u30bf\u30b9\u30af\u306f\u300c\u30ce\u30fc\u30c9\u300d\u306e\u30e9\u30d9\u30eb\u3092\u5f53\u3066\u308b\u3082\u306e\u3068\u3001\u300c\u30b0\u30e9\u30d5\u69cb\u9020\u300d\u306e\u30e9\u30d9\u30eb\u3092\u5f53\u3066\u308b\u3082\u306e\u306e\uff12\u7a2e\u985e\u304c\u3042\u308b\u304c\u3001\u4eca\u56de\u306f\u5f8c\u8005\u3002\n- \u5404\u30b0\u30e9\u30d5\u69cb\u9020\uff08\uff1d\u5bfe\u6226\u30ab\u30fc\u30c9\uff09\u3067\u3069\u3063\u3061\u304c\u52dd\u3063\u305f\u306e\u304b\u3092\u5f53\u3066\u308b","9c4fd52d":"# Config Definition","4431b393":"# \u4e8b\u524d\u6e96\u5099","ee954dbf":"# Modeling"}}