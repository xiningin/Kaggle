{"cell_type":{"0cc65c3d":"code","6854c098":"code","59544b88":"code","50de2917":"code","b8b946a2":"code","4aa35fb5":"code","d0efb585":"code","a441ced7":"code","c0273a40":"code","8ccd3be3":"code","5a529688":"code","19b9c444":"code","157abed0":"code","79b1d2fc":"code","c79e25f9":"code","e3b225f7":"code","825022a4":"code","e002dd4c":"code","cbc75be8":"code","e7fd054d":"code","e514040b":"code","8a16bf21":"code","c9cf63d7":"code","73a27195":"code","c524d89c":"code","be7d4969":"code","d703ef71":"code","87284a65":"code","42d0a833":"code","4c5c6052":"code","38639bd4":"code","b92df493":"code","b4bfe8c1":"code","7f19b84d":"code","422c5c0f":"code","2ccefa48":"code","8e7bf2a6":"code","2438ed08":"code","dd749788":"code","965474ce":"code","8e2c41e6":"code","1866a42b":"markdown","7a752c48":"markdown","9b2de749":"markdown","5fba3f28":"markdown","c30fcbf1":"markdown","8346d628":"markdown","b3cf1b4c":"markdown","bd4d10a0":"markdown","2ff4f10d":"markdown","6783b5bc":"markdown","be9e45c8":"markdown","d794c803":"markdown","dea8df97":"markdown"},"source":{"0cc65c3d":"# Code from the my kernel \"Titanic Top 3% : one line of the prediction code\": \n# https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n#\nimport pandas as pd\nimport numpy as np \ntraindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.replace(np.nan, 0)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\ntrain_y = df.Survived.loc[traindf.index]\ndf2 = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, df.Sex.replace({'female': 0, 'male': 1})], axis=1)\ntest_x = df2.loc[testdf.index]\n\n# The one line of the code for prediction : LB = 0.83253 (Titanic Top 3%) \ny_pred_top3 = (((test_x.WomanOrBoySurvived <= 0.238) & (test_x.Sex < 0.5) & (test_x.Alone > 0.5)) | \\\n          ((test_x.WomanOrBoySurvived > 0.238) & \\\n           ~((test_x.WomanOrBoySurvived > 0.55) & (test_x.WomanOrBoySurvived <= 0.633)))).astype(int)\n\n# Saving the result\npd.DataFrame({'Survived': y_pred_top3}, \\\n             index=testdf.index).reset_index().to_csv('survived_top3.csv', index=False)\nprint('Mean =', y_pred_top3.mean(), ' Std =', y_pred_top3.std())","6854c098":"from sklearn import cluster, datasets, mixture\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.neighbors import kneighbors_graph\nfrom itertools import cycle, islice\nfrom scipy.spatial.distance import cosine\n\nimport time\nimport graphviz\nimport matplotlib.pyplot as plt\nprint(__doc__)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nnp.random.seed(0)","59544b88":"# FE","50de2917":"#Thanks to: https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n#Title\ndf['Title'] = df['Title'].replace('Ms','Miss')\ndf['Title'] = df['Title'].replace('Mlle','Miss')\ndf['Title'] = df['Title'].replace('Mme','Mrs')","b8b946a2":"#Thanks to: https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n# Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')","4aa35fb5":"# Thanks to: https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Fare\nmed_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf['Fare'] = df['Fare'].fillna(med_fare)","d0efb585":"#Thanks to: https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n# Cabin, Deck\ndf['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf.loc[(df['Deck'] == 'T'), 'Deck'] = 'A'","a441ced7":"#Thanks to: https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n#Age\ndf['Age'] = df.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))","c0273a40":"#Thanks to: https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Family_Size\ndf['Family_Size'] = df['SibSp'] + df['Parch'] + 1","8ccd3be3":"pd.set_option('max_columns',100)\ntraindf.head(3)","5a529688":"df.WomanOrBoySurvived = df.WomanOrBoySurvived.fillna(0)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.fillna(0)\ndf.FamilySurvivedCount = df.FamilySurvivedCount.fillna(0)\ndf.Alone = df.Alone.fillna(0)","19b9c444":"df.head(3)","157abed0":"train_y = df.Survived.loc[traindf.index]","79b1d2fc":"cols_to_drop = ['Name','Ticket','Cabin','Survived']\ndf = df.drop(cols_to_drop, axis=1)","c79e25f9":"numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = df.columns.values.tolist()\nfor col in features:\n    if df[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","e3b225f7":"for col in categorical_columns:\n    if col in df.columns:\n        le = LabelEncoder()\n        le.fit(list(df[col].astype(str).values))\n        df[col] = le.transform(list(df[col].astype(str).values))","825022a4":"train_x_all, test_x_all = df.loc[traindf.index], df.loc[testdf.index]\ntrain_x_all.head(3)","e002dd4c":"# The minimal percentage of similarity of the clustered feature with \"Survived\" for inclusion in the final dataset\nlimit_opt = 0.7","cbc75be8":"n_clusters_opt = 4 # number of clusters\n# Thanks to: https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\ndefault_base = {'quantile': .2,\n                'eps': .3,\n                'damping': .9,\n                'preference': -200,\n                'n_neighbors': 10,\n                'n_clusters': n_clusters_opt,\n                'min_samples': 3,\n                'xi': 0.05,\n                'min_cluster_size': 0.05}","e7fd054d":"# Features list for clustering\nfeature_first = 'WomanOrBoySurvived'\nclustered_features = ['Age']","e514040b":"def generate_data(x1,x2,df,t):\n    # x1, x2 as string - name of features from dataframe df\n    # t=1 - with train_y, t=0 - without its\n    X = pd.concat([df[x1], df[x2]], axis=1).values\n    if t==1:\n        y = train_y.values.astype(int)\n        return (X, y)\n    else:\n        return X","8a16bf21":"title_plot = {}\nfor i in range(len(clustered_features)):\n    title_plot[i] = 'W-'+str(clustered_features[i])\ntitle_plot","c9cf63d7":"# train dataset\ndatasets = []\nfor i in range(len(clustered_features)):\n    datasets.append((generate_data(feature_first,clustered_features[i],train_x_all,1),{}))","73a27195":"# test dataset\ndatasets_test = []\nrez = pd.DataFrame(index = test_x.index)\nfor i in range(len(clustered_features)):\n    datasets_test.append(generate_data(feature_first,clustered_features[i],test_x_all,0))","c524d89c":"# Thanks to: https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\ndef generate_clustering_algorithms(Z,n_clusters):\n    # generate clustering algorithms\n    \n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(df, quantile=params['quantile'])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        Z, n_neighbors=params['n_neighbors'], include_self=False)\n    \n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    kmeans = cluster.KMeans(n_clusters=n_clusters, random_state = 1000)\n    two_means = cluster.MiniBatchKMeans(n_clusters=n_clusters)\n    ward = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',\n        connectivity=connectivity)\n    spectral = cluster.SpectralClustering(n_clusters=n_clusters, eigen_solver='arpack',\n        affinity=\"nearest_neighbors\")\n    dbscan = cluster.DBSCAN(eps=params['eps'])\n    optics = cluster.OPTICS(min_samples=params['min_samples'],\n                            xi=params['xi'],\n                            min_cluster_size=params['min_cluster_size'])\n    affinity_propagation = cluster.AffinityPropagation(damping=params['damping'])\n    average_linkage = cluster.AgglomerativeClustering(linkage=\"average\", affinity=\"cityblock\",\n        n_clusters=params['n_clusters'], connectivity=connectivity)\n    birch = cluster.Birch(n_clusters=params['n_clusters'])\n    gmm = mixture.GaussianMixture(n_components=n_clusters, covariance_type='full')\n\n    clustering_algorithms = (\n        ('MiniBatchKMeans', two_means),\n        ('KMeans', kmeans),\n        ('AffinityPropagation', affinity_propagation),\n        ('MeanShift', ms),\n        ('SpectralClustering', spectral),\n        ('Ward', ward),\n        ('AgglomerativeClustering', average_linkage),\n        ('DBSCAN', dbscan),\n        ('OPTICS', optics),\n        ('Birch', birch),\n        ('GaussianMixture', gmm)\n    )\n    return clustering_algorithms","be7d4969":"# Thanks to: https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10 * 2 + 2, 12.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.01, top=.98, wspace=.05,\n                    hspace=.01)\n\nplot_num = 1\ncoord_xy_lim = 2.5\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n    \n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n    datasets_test[i_dataset] = StandardScaler().fit_transform(datasets_test[i_dataset])        \n\n    clustering_algorithms = generate_clustering_algorithms(X,params['n_clusters'])\n    clustering_algorithms_test = generate_clustering_algorithms(datasets_test[i_dataset],params['n_clusters'])\n\n    simil = {}\n    i = 0\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \" +\n                \"connectivity matrix is [0-9]{1,2}\" +\n                \" > 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning)\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\" +\n                \" may not work as expected.\",\n                category=UserWarning)\n            algorithm.fit(X)\n\n        t1 = time.time()\n        \n        if hasattr(algorithm, 'labels_'):\n            y_pred = algorithm.labels_.astype(np.int)\n        else:\n            y_pred = algorithm.predict(X)       \n        \n        simil[name] = 1 - cosine(y, y_pred)\n        print(i_dataset, i, round(simil[name], 3), title_plot[i_dataset], name)\n        \n        if i == len(clustering_algorithms)-1:\n            # determine the optimal clustering method \n            max_simil = max(simil, key=simil.get)\n            print('Optimal ==> ', max_simil)\n           \n            # clustering data by the optimal method - synthesis of a new feature\n            if simil[max_simil] > limit_opt:\n                train_x_all[title_plot[i_dataset]] = y_pred\n                algorithm_opt = dict(clustering_algorithms_test)[max_simil]\n                algorithm_opt.fit(datasets_test[i_dataset])\n                if hasattr(algorithm_opt, 'labels_'):\n                    rez[title_plot[i_dataset]] = algorithm_opt.labels_.astype(np.int)\n                else:\n                    rez[title_plot[i_dataset]] = algorithm_opt.predict(datasets_test[i_dataset])          \n\n        plt.subplot(len(datasets), len(clustering_algorithms) + 1, plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=14)\n        if name == 'MiniBatchKMeans':\n            yt = plt.ylabel(title_plot[i_dataset], size=14,rotation=90)\n            \n        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                             '#f781bf', '#a65628', '#984ea3',\n                                             '#999999', '#e41a1c', '#dede00']),\n                                      int(max(y_pred) + 1))))\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-coord_xy_lim, coord_xy_lim)\n        plt.ylim(-coord_xy_lim, coord_xy_lim)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n                 transform=plt.gca().transAxes, size=15,\n                 horizontalalignment='right')\n        plot_num += 1\n        i += 1\n    # Survived\n    plt.subplot(len(datasets), len(clustering_algorithms)+1, plot_num)\n    if i_dataset == 0:\n        plt.title(\"Survived\", size=14)\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.xlim(-coord_xy_lim, coord_xy_lim)\n    plt.ylim(-coord_xy_lim, coord_xy_lim)\n    plt.xticks(())\n    plt.yticks(())\n    plot_num += 1\n\nplt.show()","d703ef71":"rez.head(3)","87284a65":"train_x = pd.concat([train_x_all.WomanOrBoySurvived.fillna(0), \n                     train_x_all.Alone, \n                     train_x_all.Sex,\n                     ], axis=1)\ntest_x = pd.concat([test_x_all.WomanOrBoySurvived.fillna(0), \n                     test_x_all.Alone, \n                     test_x_all.Sex,\n                     ], axis=1)","42d0a833":"rez_col = rez.columns.values.tolist()\nrez_col","4c5c6052":"train_x = train_x.join(train_x_all[rez_col], how='left', lsuffix=\"_rez\")\ntrain_x.head(2)","38639bd4":"test_x = test_x.join(rez[rez_col], how='left', lsuffix=\"_rez\")\ntest_x.head(2)","b92df493":"# Tuning the DecisionTreeClassifier by the GridSearchCV\nparameters = {'max_depth' : np.arange(2, 9, dtype=int),\n              'min_samples_leaf' :  np.arange(1, 4, dtype=int)}\nclassifier = DecisionTreeClassifier(random_state=1000)\nmodel = GridSearchCV(estimator=classifier, param_grid=parameters, scoring='accuracy', cv=10, n_jobs=-1)\nmodel.fit(train_x, train_y)\nbest_parameters = model.best_params_\nprint(best_parameters)","b4bfe8c1":"model=DecisionTreeClassifier(max_depth = best_parameters['max_depth'], \n                             random_state = 1118)\nmodel.fit(train_x, train_y)","7f19b84d":"# plot tree\ndot_data = export_graphviz(model, out_file=None, feature_names=train_x.columns, class_names=['0', '1'], \n                           filled=True, rounded=False,special_characters=True, precision=7) \ngraph = graphviz.Source(dot_data)\ngraph ","422c5c0f":"# Prediction by the DecisionTreeClassifier\ny_pred = model.predict(test_x).astype(int)\nprint('Mean =', y_pred.mean(), ' Std =', y_pred.std())\n# Mean = 0.3349282296650718  Std = 0.4719653701687156 ==> LB = 0.83253","2ccefa48":"train_y_pred = model.predict(train_x).astype(int)\ndiff = sum(abs(train_y-train_y_pred))*100\/len(train_y)\ndiff\n# LB = 0.83253 ==> 7.744107744107744 ","8e7bf2a6":"# Saving the result\npd.DataFrame({'Survived': y_pred}, index=testdf.index).reset_index().to_csv('survived_new.csv', index=False)","2438ed08":"train_x_all['Pred'] = train_y_pred\ntrain_x_all['Survived'] = train_y","dd749788":"pd.set_option('max_columns',100)\npd.set_option('max_rows',100)","965474ce":"train_x_all[train_x_all['Survived'] != train_x_all['Pred']].sort_values(by=['Survived'])","8e2c41e6":"diff_nrow = len(train_x_all[train_x_all['Survived'] != train_x_all['Pred']])\ndiff_nrow\n# LB = 0.83253 ==> Top 2-3%","1866a42b":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg)","7a752c48":"Early I developed kernels (https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20) and (https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-15) which had three lines of code based on 3 and 4 statements and provides an LB of at least 80% and 85% of teams - Titanic Top 20% and 15% respectively. later I improved the result: \"Titanic Top 3%\" - I will give code with forecasting not in the context of the classes of cabins and ports, but in the context of the surnames of passengers (Thanks to https:\/\/www.kaggle.com\/mauricef\/titanic): https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n\nI try to improve the result by using the clustering of different features relative to those that were selected in solution \"Titanic Top3%\". The kernel is devoted to an overview of clustering methods and attempts to create new features that will improve the solution in one line of code to prediction them.\n\nI will also give new features (FE&FC).\n\nThe kernel allows to apply different clustering methods (they can be easily added into clustering_algorithms) to pairs of features feature_first (by default: \"WomanOrBoySurvived\") and a feature from the list clustered_features. The given number of clusters n_clusters_opt is used for classification methods.\n\nThe optimal method determined for each pair of features automatically by the criterion of the maximum cosine similarity with the target feature \"Survived\" in the training dataset (complete similarity is 1). If this criterion for the optimal method exceeds a threshold limit_opt then a new feature is synthesized by this method in the train and test datasets. After that, the optimal decision tree DecisionTreeClassifier is built by the criterion of max_depth (optimization method GridSearchCV).\n\nThe kernel allows trying to improve the accuracy of the kernel \"Titanic Top 3% : one line of the prediction code\" (https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code).\n\nThe restriction to only pairs of features is due to the desire to provide a high-quality visualization of the clustering process by analogy with the kernel: https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n\nMy attempts to find a solution giving accuracy above LB = 0.83253 have not yet been successful. If a solution is found, I will post it.","9b2de749":"# Titanic Top 3% : cluster analysis - attempts for imrovement the solution \"Titanic Top 3% : one line of the prediction code\" with LB = 0.83253","5fba3f28":"### Plot tree","c30fcbf1":"### Prediction","8346d628":"# Preparing to prediction (including FE)","b3cf1b4c":"## Tuning model","bd4d10a0":"## Residues of train dataset view","2ff4f10d":"### Preparing to prediction (including FE) ","6783b5bc":"Thanks to:\n\n* https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-15\n* https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20 \n* https:\/\/www.kaggle.com\/mauricef\/titanic\n* https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n* https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n* https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n","be9e45c8":"I hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.","d794c803":"# Clustering","dea8df97":"### Saving the result"}}