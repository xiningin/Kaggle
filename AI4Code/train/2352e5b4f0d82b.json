{"cell_type":{"a4178801":"code","53109dd7":"code","3ad915cf":"code","cebadc85":"code","ddff799d":"code","8ee1f2ab":"code","6aa9132d":"code","a9d9240d":"code","511c44bd":"code","2e85635a":"code","c8f06855":"code","d5ea0343":"code","16ef5086":"code","0d14ec87":"code","7c7306d2":"code","2fc658fb":"code","21d5ac3c":"code","022f0d11":"code","c222810b":"code","721ce073":"code","aa0884cd":"code","8dc9378b":"code","984b401b":"code","c6c05ab9":"code","88fc60df":"code","2f94aa8b":"code","bcaa426d":"code","b96df675":"code","09f0b5b4":"code","640e5987":"code","2674c510":"code","29b23f16":"code","21e9edce":"code","1ad875bd":"code","4a1bea64":"code","59b34bd3":"code","af15ed17":"code","00d00fd6":"code","3d8ff052":"code","74bd637c":"code","01706530":"code","53acdafb":"code","c816292a":"code","6a598771":"markdown","03f97d99":"markdown","521da701":"markdown","53cbe7c6":"markdown","e109a34d":"markdown","b35c15cc":"markdown","2efdd916":"markdown","8337d3bc":"markdown","f364676b":"markdown","feb9a6e7":"markdown","a07cdf25":"markdown","f0332fe1":"markdown","acc52f5d":"markdown","f2e83422":"markdown","66307510":"markdown","9a643281":"markdown","8d62f1ec":"markdown","7cde5c09":"markdown","81ed8a06":"markdown","f1439d44":"markdown","774a7d13":"markdown","396026c9":"markdown","f23e8391":"markdown","4f694b07":"markdown","7b31f218":"markdown","3df9494c":"markdown","5cb0b8b4":"markdown","d00a06ac":"markdown","1e708f25":"markdown","35d9e7ee":"markdown","b8792ab7":"markdown","f972dff4":"markdown","d8160c43":"markdown","8a78e6b1":"markdown","28b10b1e":"markdown","1af07121":"markdown","c629810e":"markdown","924a502b":"markdown","12f8db1c":"markdown","15162a83":"markdown","27b9eaef":"markdown","f3bc531b":"markdown","8ba7525d":"markdown","1ade8c90":"markdown","b761c5b5":"markdown","a39bb21d":"markdown","386d9c6e":"markdown","a52b9a44":"markdown","06fb5263":"markdown","8f8fd85c":"markdown","4c95f732":"markdown","22191bf7":"markdown","bb64051b":"markdown","9e2d2346":"markdown","6fb541de":"markdown","807902bd":"markdown","e619c3d8":"markdown","65577c24":"markdown","95c1dd30":"markdown","f992a767":"markdown","e5e40e09":"markdown","4410ad6f":"markdown","a549aebb":"markdown","ed2c9077":"markdown"},"source":{"a4178801":"#basic libraries\nimport sys\nimport numpy as np\nimport pandas as pd\nimport os\nimport warnings\n\n#seed the project\nnp.random.seed(64)\n\n#ploting libraries\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nsns.set(context='notebook', style='whitegrid', palette='pastel', font='sans-serif', font_scale=1, color_codes=False, rc=None)\n\n#warning hadle\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Set up completed\")","53109dd7":"# Import train and test data\ntest_filepath = \"\/kaggle\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv\"\ntrain_filepath = \"\/kaggle\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv\"\n\ntrain_df = pd.read_csv(train_filepath)\ntest_df = pd.read_csv(test_filepath)","3ad915cf":"# Print the first rows of the training data\ntrain_df.head()","cebadc85":"# Print the first rows of the test data\ntest_df.head()","ddff799d":"# Different data types in the dataset\ntrain_df.dtypes","8ee1f2ab":"# We need to see the amount of missing values present in each column.\ntrain_df.isnull().sum().sort_values(ascending=False)","6aa9132d":"# Plot graphic of missing values\nmissingno.matrix(train_df, figsize = (30,10));","a9d9240d":"sns.countplot('Loan_Status', data=train_df);","511c44bd":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='ApplicantIncome', data=train_df);","2e85635a":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='CoapplicantIncome', data=train_df);","c8f06855":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='LoanAmount', data=train_df);","d5ea0343":"plt.figure(figsize=(15,5))\nsns.violinplot(x='Loan_Status', y='Loan_Amount_Term', data=train_df);","16ef5086":"plt.figure(figsize=(15,5))\nsns.violinplot(x='Loan_Status', y='Credit_History', data=train_df);","0d14ec87":"train_df['Credit_History'] = train_df['Credit_History'].astype('O')","7c7306d2":"plt.figure(figsize=(15,5))\nsns.countplot(x='Gender', hue='Loan_Status', data=train_df);","2fc658fb":"plt.figure(figsize=(15,5))\nsns.countplot(x='Married', hue='Loan_Status', data=train_df);","21d5ac3c":"plt.figure(figsize=(15,5))\nsns.countplot(x='Dependents', hue='Loan_Status', data=train_df);","022f0d11":"plt.figure(figsize=(15,5))\nsns.countplot(x='Education', hue='Loan_Status', data=train_df);","c222810b":"plt.figure(figsize=(15,5))\nsns.countplot(x='Self_Employed', hue='Loan_Status', data=train_df);","721ce073":"plt.figure(figsize=(15,5))\nsns.countplot(x='Property_Area', hue='Loan_Status', data=train_df);","aa0884cd":"# safety copy of our current df\ntrain = train_df.copy()\ntrain.head()","8dc9378b":"num_attribs = ['CoapplicantIncome']\ntrain_num = train[num_attribs]","984b401b":"from sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])","c6c05ab9":"# transform the target column\n\ntarget_values = {'Y': 0 , 'N' : 1}\n\ntarget = train['Loan_Status']\ntrain.drop('Loan_Status', axis=1, inplace=True)\n\ntarget = target.map(target_values)","88fc60df":"cat_attribs = ['Married', 'Dependents', 'Education', 'Property_Area', 'Credit_History']\ntrain_cat = train[cat_attribs]","2f94aa8b":"from sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy=\"most_frequent\", fill_value='missing')),\n        (\"onehot\", OneHotEncoder(handle_unknown='ignore')),\n    ])","bcaa426d":"from sklearn.compose import ColumnTransformer\n\npreprocess_pipeline = ColumnTransformer(transformers=[\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", cat_pipeline, cat_attribs),\n    ])","b96df675":"X = preprocess_pipeline.fit_transform(train)\nX","09f0b5b4":"X.shape","640e5987":"y = target","2674c510":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","29b23f16":"# classify function\nfrom sklearn.model_selection import cross_val_score\ndef classify(model, x, y):\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    model.fit(x_train, y_train)\n    print(\"Accuracy is\", model.score(x_test, y_test)*100)\n    # cross validation - it is used for better validation of model\n    # eg: cv-5, train-4, test-1\n    score = cross_val_score(model, x, y, cv=5)\n    print(\"Cross validation is\",np.mean(score)*100)","21e9edce":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(random_state=64)\nclassify(lr_model, X, y)","1ad875bd":"from sklearn.neighbors import KNeighborsClassifier\n\nkn_model = KNeighborsClassifier()\nclassify(kn_model, X, y)","4a1bea64":"from sklearn.svm import SVC\n\nsvc_model = SVC(random_state=64)\nclassify(svc_model, X, y)","59b34bd3":"from sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(random_state=64)\nclassify(dt_model, X, y)","af15ed17":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(n_estimators=100, random_state=64)\nclassify(rf_model, X, y)","00d00fd6":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {\"C\":np.logspace(-3,3,7),  \"penalty\":[\"l1\",\"l2\"]}\n  ]\n\ngrid_search = GridSearchCV(lr_model, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)\nlr_model = grid_search.best_estimator_\nclassify(lr_model, X, y)","3d8ff052":"param_grid = [\n    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]},\n    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n]\n\ngrid_search = GridSearchCV(svc_model, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X, y)\nsvc_model  = grid_search.best_estimator_\nclassify(svc_model, X, y)","74bd637c":"param_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\ngrid_search = GridSearchCV(rf_model, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X, y)\nrf_model = grid_search.best_estimator_\nclassify(rf_model, X, y)","01706530":"model = LogisticRegression(C=10.0, random_state=64)\nmodel.fit(x_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = model.predict(x_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True)","53acdafb":"# safety copy of our the test df\ntest = test_df.copy()\n\nX_test = preprocess_pipeline.transform(test)\nX_test","c816292a":"y_pred = lr_model.predict(X_test)\ny_pred","6a598771":"### Data Types\nWe can see our dataset consist on different types of data. As we continue to analyze it, we'll find features which are numerical and should actually be categorial.","03f97d99":"#### SVC Model","521da701":"<a id=\"t2.\"><\/a>\n## 3. Exploration Data Analysis","53cbe7c6":"It can be seen that if you got a Credit History = 1, you have a better chance to get a loan. This is an important feature for our future analysis. But we can clearly see this is actually a categorical variable, as we can see that it is 1 or 2","e109a34d":"### The Data","b35c15cc":"### Load the Data","2efdd916":"Machine Learning algorithms work better with numbers, so we will convert our categorical variables. We can transform our data to have a singular attribute per category. This process is called one-hot encoding (also known as dummy variables\/attributes). ","8337d3bc":"### Preparing the tools","f364676b":"### Numerical Attributes Pipeline","feb9a6e7":"To get our dataset ready for our ML algorithms, we need to deal with missing data. In this case we'll opt for replacing NAs with the mean. For simplicity, we do this for all numerical variables.","a07cdf25":"### Problem Definition","f0332fe1":"<a id=\"t1.\"><\/a>\n## 1. Problem Framing","acc52f5d":"Both groups, the one that had their loan approved and the ones that didn't, seem that have a very similar pattern, therefore, we wouldn't consider the applicant's income as an important feature for our analysis.","f2e83422":"### Confusion Matrix","66307510":"### Exploration of Property Area","9a643281":"TOC:\n1. [Problem Framing](#t1.)\n2. [Assess Data Quality & Missing Values](#t2.)\n3. [Exploratory Data Analysis](#t3.)\n4. [Prepare the Data](#t4.)\n5. [Shortlisting Promising ML Models](#t5.)\n6. [Fine-Tune the System](#t6.)\n7. [Presenting the Solution](#t7.)","8d62f1ec":"It is important to check which columns contain empty values, as most ML models don't work when these are present. In this case, we can see that the columns Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term and Credit_History have missing values. But as we can confirm, none of these features have enough missing attributes to discard it. But to be able to continue with our analysis we can't have any na value, so we will replace them by the most frequent value.","7cde5c09":"### Exploration of Education","81ed8a06":"# Loan Acceptance Prediction \n\nMy personal goal for this notebook is to complete another EDA on a dataset, and create a guideline with all the steps for a ML Binary Classification Problem. Let me know if you have any questions, I'll try my best to solve them. ","f1439d44":"### Exploration of Credit History","774a7d13":"We do actually see a different pattern between both groups, so we can say that the Coapplicant Income is a an important feature for this ML modern.","396026c9":"As we fit and train our models, we will be using cross-validation to have an idea of how good our model is.","f23e8391":"If you are self-employed or not, you will get almost the same chance to get a loan, we don't see a pattern, therefore, this doesn't seem like an important feature.","4f694b07":"This dataset was obtained from Kaggle and is named Loan Prediction Dataset. It contains a set of 613 records under 13 attributes:\n\n* **Loan_ID:** A uniques loan ID\n* **Gender:** Male\/Female\n* **Married:** Married(Yes)\/Not married(No)  \n* **Dependents:** Number of persons depending on the client.\n* **Education:** Application Education (Graduate \/ Undergraduate)\n* **Self_Employed:** Self employed (Yes\/No)\n* **ApplicantIncome:** Applicant income\n* **CoapplicantIncome:** Coapplicant Income\n* **LoanAmount:** Loan amount in thousands\n* **Loan_Amount_Term:** Term of lean in months\n* **Credit_History:** Credit history meets guidelines\n* **Property_Area:** Urban\/Semi and Rural\n* **Loan_Status:** Loan approved (Y,N)\n\nAs we said, the main objective of this probles is to use machine learning techniques to predict loan payments, therefore, our target value is Loan_Status.","7b31f218":"We can't forget about our target feature; 'Loan_Status', we need to transform this column.","3df9494c":"*References:*\n* https:\/\/www.kaggle.com\/yaheaal\/loan-status-with-different-models\n* https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/03_classification.ipynb","5cb0b8b4":"<a id=\"t7.\"><\/a>\n## 7. Presenting the Solution","d00a06ac":"### Exploration of Gender","1e708f25":"<a id=\"t2.\"><\/a>\n## 2. Data Quality & Missing Values Assessment","35d9e7ee":"### Full Pipeline","b8792ab7":"It seems that if the applicant has zero dependent members, they have a higher chance ot get a loan, this also seems like a good feature.","f972dff4":"We don't see a pattern here, most males got a loan but most females also did. As of now we wouldn't classify it as an important feature for our model.","d8160c43":"We will use 5 of the most used models for training binary classifying ptoblems.\n\n* Logistic Regression\n* KNeighbors Classifier\n* SVC\n* Decision Tree Classifier\n* Random Forest Classifier","8a78e6b1":"<a id=\"t5.\"><\/a>\n## 5. Shortlisting Promising ML Models","28b10b1e":"Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan. Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers.\n\nThis is a standard supervised classification task.A classification problem where we have to predict whether a loan would be approved or not. Below is the dataset attributes with description.","1af07121":"### Train-Test Split","c629810e":"As a final test for our chosen model, it will be useful to see the nymber of correct and incorrect predictions.","924a502b":"#### Random Forest Model","12f8db1c":"### Missing Values\n\nAs you can see we have some missing data, let's have a look how many we have for each column:","15162a83":"### Categorical Attributes Pipeline","27b9eaef":"### Exploration of Applicant Income","f3bc531b":"<a id=\"t6.\"><\/a>\n## 6. Fine-Tune the Model","8ba7525d":"As we saw earlier, there are some features that seem more useful for this project than others. Given this, one of our goals will be to drop the following features: ['Loan_ID','ApplicantIncome','LoanAmount', 'Loan_Amount_Term', 'Gender','Self_Employed']; that were declared not interesting during the previous discussion.","1ade8c90":"#### Logistic Regression Model","b761c5b5":"### Feature Selection","a39bb21d":"### Exploration of Married","386d9c6e":"### Exploration of Loan Amount Term","a52b9a44":"A part from dropping the na values, it is also important to note that a few machine learning algorithms are highly sensitive to features that span varying degrees of magnitude, range, and units. This is why feature scaling is a crucial part of the data preprocessing stage.","06fb5263":"It seems that graduated students have a slight more chance of getting a loan accepted. This might be and important feature.","8f8fd85c":"We can see that living in a semiurban are gives you the biggest chance to get a loan approved. This seems like a good feature.","4c95f732":"### Exploration of Dependents","22191bf7":"This actually doesn't seem like an ideal result, as we can see a great amount of False Negatives, but for the sake of the practice and learning, we'll leave it as it is.","bb64051b":"### Exploration of Loan Amount","9e2d2346":"After seeing these results, we can see that there are 3 models that seem interesting enough to try some hyperparameter tuning to see if they can improve; LR, SVC and Random Forest.","6fb541de":"<a id=\"t4.\"><\/a>\n## 4. Preparing the Data","807902bd":"### Exploration of Coapplicant Income","e619c3d8":"### Grid Search\nThis function will help us see which hyperparameters work better for our models, by using cross-validation ot evaluate all the possible combinations.","65577c24":"### Exploration of Self Employed","95c1dd30":"### Exploration of our Target Feature: Loan Status","f992a767":"Looking at this plots, we can't see a significant difference between the subjects that got the loan accepted and the ones that didn't.","e5e40e09":"Both groups seem to have a very similar pattern, therefore, we wouldn't consider the loan amount an important feature for our analysis.","4410ad6f":"We can see that if you are married you have a better chance of getting a loan, this seems like a good feature.","a549aebb":"### Let's predict on our test set","ed2c9077":"After tunning the hyperparameters, we can see that Logistic Regression seems to be offering the best fit."}}