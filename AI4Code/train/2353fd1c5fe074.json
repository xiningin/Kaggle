{"cell_type":{"20db9227":"code","666d6b40":"code","d9cf5695":"code","c490965e":"code","01f0f711":"code","2eac5f84":"code","4c40fdf4":"code","3cd4f151":"code","0ee510e6":"code","6e967aa7":"code","147fc720":"code","e74c1284":"code","8f06d544":"code","fa1d5f49":"code","5377e66b":"code","943ecbe3":"code","1d743a81":"code","b92b6a0c":"code","649b1cb4":"code","4c9c040a":"code","d00ef081":"code","fa7d4cb9":"code","3e6e1012":"code","6bca8e7a":"code","c5e0c130":"code","aae0f7f1":"code","ff112dcd":"markdown","23749028":"markdown","cf2d68e3":"markdown","9402efc4":"markdown","de92f7cf":"markdown","67ce31e5":"markdown","3edc6b14":"markdown","8fd5de06":"markdown","51f9d9e7":"markdown","21b1a0ed":"markdown"},"source":{"20db9227":"import numpy as np \nimport pandas as pd","666d6b40":"url = 'https:\/\/raw.githubusercontent.com\/priya-mane\/Covid-19_ICU_Prediction\/main\/Covid_ICU_preprocessed.csv'\n# import dataset from github using url\ndf = pd.read_csv(url, error_bad_lines=False)\ndf.head()","d9cf5695":"# Features most important for prediction\nimp_f = df.corr()['ICU'].sort_values( ascending=False).head(7).index[1:]","c490965e":"imp_f","01f0f711":"# visualize histogram for \"RESPIRATORY_RATE_MAX\" \ndf[\"RESPIRATORY_RATE_MAX\"].hist()\n\n# binning for RESPIRATORY_RATE_MAX based on histogram\ndf[\"RESPIRATORY_RATE_MAX_cat\"] = pd.cut(df[\"RESPIRATORY_RATE_MAX\"],\n bins=[-1., -0.6, -0.5, -0.2, 0.1, np.inf],\n labels=[1, 2, 3, 4, 5])\n","2eac5f84":"df[\"RESPIRATORY_RATE_MAX_cat\"].hist()","4c40fdf4":"from sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# random split\ntrain_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n\n# stratified split\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\nfor train_index, test_index in split.split(df, df[\"RESPIRATORY_RATE_MAX_cat\"]):\n    strat_train_set = df.loc[train_index]\n    strat_test_set = df.loc[test_index]\n","3cd4f151":"# for test set\noverall = df[\"RESPIRATORY_RATE_MAX_cat\"].value_counts() \/ len(df)\n\nstratified = strat_test_set[\"RESPIRATORY_RATE_MAX_cat\"].value_counts() \/ len(strat_test_set)\n\nrandom = test_set[\"RESPIRATORY_RATE_MAX_cat\"].value_counts() \/ len(test_set)\n\nsplit_series = [overall, stratified, random]\n\nsplit_series_df = []\n\nfor i in range(0,len(split_series)):\n    ser = split_series[i]\n    ser_df = pd.DataFrame(columns=['RESPIRATORY_RATE_MAX_cat', 'split_technique','val'])\n    ser_df['RESPIRATORY_RATE_MAX_cat'] = ser.index.values\n    if (i==0):\n        ser_df['split_technique'] = \"overall\"\n    elif (i==1):\n        ser_df['split_technique'] = \"stratified\"\n    else:\n        ser_df['split_technique'] = \"random\"   \n    \n    ser_df['val'] = ser.values\n    split_series_df.append(ser_df)","0ee510e6":"split_comparison_df = pd.concat(split_series_df, axis=0)\nsplit_comparison_df","6e967aa7":"# comparing distribution of categories that we made for \"RESPIRATORY_RATE_MAX\" i.e \"RESPIRATORY_RATE_MAX_cat\"\n# for overall data with startified and random split\nsns.factorplot(x='RESPIRATORY_RATE_MAX_cat', y='val', hue='split_technique', data=split_comparison_df, kind='bar')\n\n# we can oberve that startified is always i.e for each category, very close to overall distribution compared to random","147fc720":"# for training set\noverall = df[\"RESPIRATORY_RATE_MAX_cat\"].value_counts() \/ len(df)\n\nstratified = strat_train_set[\"RESPIRATORY_RATE_MAX_cat\"].value_counts() \/ len(strat_test_set)\n\nrandom = train_set[\"RESPIRATORY_RATE_MAX_cat\"].value_counts() \/ len(test_set)\n\nsplit_series = [overall, stratified, random]\n\nsplit_series_df = []\n\nfor i in range(0,len(split_series)):\n    ser = split_series[i]\n    ser_df = pd.DataFrame(columns=['RESPIRATORY_RATE_MAX_cat', 'split_technique','val'])\n    ser_df['RESPIRATORY_RATE_MAX_cat'] = ser.index.values\n    if (i==0):\n        ser_df['split_technique'] = \"overall\"\n    elif (i==1):\n        ser_df['split_technique'] = \"stratified\"\n    else:\n        ser_df['split_technique'] = \"random\"   \n    \n    ser_df['val'] = ser.values\n    split_series_df.append(ser_df)","e74c1284":"# comparing distribution of categories that we made for \"RESPIRATORY_RATE_MAX\" i.e \"RESPIRATORY_RATE_MAX_cat\"\n# for overall data with startified and random split\nsns.factorplot(x='RESPIRATORY_RATE_MAX_cat', y='val', hue='split_technique', data=split_comparison_df, kind='bar')\n\n# we can oberve that startified is always i.e for each category, very close to overall distribution compared to random","8f06d544":"# for test set\noverall_icu = df['ICU'].value_counts()\nstratified_icu = strat_test_set['ICU'].value_counts()\nrandom_icu = test_set['ICU'].value_counts()\n\nsplit_icu = [overall_icu, stratified_icu, random_icu]\n\nicu_dfs = []\n\nfor i in range(0,len(split_icu)):\n    icu_obj = split_icu[i]\n    icu_df = pd.DataFrame(columns=['icu', 'split_technique','count'])\n    icu_df['icu'] = icu_obj.index.values\n    if (i==0):\n        icu_df['split_technique'] = \"overall\"\n    elif (i==1):\n        icu_df['split_technique'] = \"stratified\"\n    else:\n        icu_df['split_technique'] = \"random\"   \n    \n    icu_df['count'] = icu_obj.values\n    \n    icu_dfs.append(icu_df) ","fa1d5f49":"icu_count_df = pd.concat(icu_dfs, axis=0)\nicu_count_df","5377e66b":"# comparing distribution of target variable for overall data with startified and random split\nsns.factorplot(x='split_technique', y='count', hue='icu', data=icu_count_df, kind='bar')\n\n# we can observe that distribution for startified is similar to overall i.e slighlty lore cases of icu =1 than icu=0.","943ecbe3":"# for training set\noverall_icu = df['ICU'].value_counts()\nstratified_icu = strat_train_set['ICU'].value_counts()\nrandom_icu = train_set['ICU'].value_counts()\n\nsplit_icu = [overall_icu, stratified_icu, random_icu]\n\nicu_dfs = []\n\nfor i in range(0,len(split_icu)):\n    icu_obj = split_icu[i]\n    icu_df = pd.DataFrame(columns=['icu', 'split_technique','count'])\n    icu_df['icu'] = icu_obj.index.values\n    if (i==0):\n        icu_df['split_technique'] = \"overall\"\n    elif (i==1):\n        icu_df['split_technique'] = \"stratified\"\n    else:\n        icu_df['split_technique'] = \"random\"   \n    \n    icu_df['count'] = icu_obj.values\n    \n    icu_dfs.append(icu_df) ","1d743a81":"icu_count_df = pd.concat(icu_dfs, axis=0)\nicu_count_df","b92b6a0c":"# comparing distribution of target variable for overall data with startified and random split\nsns.factorplot(x='split_technique', y='count', hue='icu', data=icu_count_df, kind='bar')\n\n# we can observe that distribution for startified is similar to overall i.e slighlty lore cases of icu =1 than icu=0.","649b1cb4":"from statsmodels.graphics.gofplots import qqplot\n\nqqplot(df['AGE_PERCENTIL'], line='s')\nplt.show()","4c9c040a":"from scipy.stats import shapiro\n\ndata = df['AGE_PERCENTIL']\n# normality test\nstat, p = shapiro(data)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","d00ef081":"# drop the \"RESPIRATORY_RATE_MAX_cat\" feature\ndf.drop(columns=['RESPIRATORY_RATE_MAX_cat', 'PATIENT_VISIT_IDENTIFIER'], inplace=True)\nstrat_train_set.drop(columns=['RESPIRATORY_RATE_MAX_cat', 'PATIENT_VISIT_IDENTIFIER'], inplace=True)\nstrat_test_set.drop(columns=['RESPIRATORY_RATE_MAX_cat', 'PATIENT_VISIT_IDENTIFIER'], inplace=True)\n\nX_train,y_train = strat_train_set.drop(columns=['ICU']), strat_train_set['ICU']\nX_test,y_test = strat_test_set.drop(columns=['ICU']), strat_test_set['ICU']","fa7d4cb9":"# Normalization\nfrom sklearn.preprocessing import MinMaxScaler\n\n# fit scaler on training data\nnorm = MinMaxScaler().fit(X_train)\n\n# transform training data\nX_train_norm = norm.transform(X_train)\n\n# transform testing dataabs\nX_test_norm = norm.transform(X_test)\n","3e6e1012":"from sklearn.ensemble import RandomForestClassifier # 86 -> 85\/86\/83\nfrom sklearn.ensemble import AdaBoostClassifier # 81\n\nfrom sklearn.linear_model import LogisticRegression # 77 ->79\nfrom sklearn.tree import DecisionTreeClassifier # 76\nfrom sklearn.neural_network import MLPClassifier #75 -> 79\/80\n\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n\nfrom sklearn.metrics import accuracy_score\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    MLPClassifier(),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()\n]\n\nscores = []\n\nfor clf in classifiers:\n    model =  clf.fit(X_train_norm, y_train)\n    y_pred = clf.predict(X_test_norm)\n    score = accuracy_score(y_test, y_pred)\n    scores.append(score)\n    \nfor i in range(len(scores)):\n    print(names[i] + \" : \" + str(scores[i]))","6bca8e7a":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n\nX = df.drop(columns=['ICU'])\ny = df[\"ICU\"]\n\ncv_val = 5\n\nrfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n\nparam_grid = { \n    'n_estimators': [70, 100, 200, 700],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [50, 100, 200]\n}\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= cv_val)\nCV_rfc.fit(X, y)\n\n# use these features for maximum accuracy  \nprint(CV_rfc.best_params_)","c5e0c130":"# Stratified K-Fold\nfrom sklearn.model_selection import StratifiedKFold\n\ndef get_score(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    return model.score(X_test, y_test)\n\nfolds = StratifiedKFold(n_splits=cv_val)\n\nscores_random_forest = []\n\nfor train_index, test_index in folds.split(X, y):\n    X_train, X_test, y_train, y_test = X.loc[train_index], X.loc[test_index], y.loc[train_index], y.loc[test_index]\n    scores_random_forest.append(get_score(RandomForestClassifier(max_depth= 200, max_features= 'log2', n_estimators= 700), X_train, X_test, y_train, y_test)) \n\nprint(scores_random_forest)\nprint(np.average(scores_random_forest))","aae0f7f1":"# Cross Validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(RandomForestClassifier(max_depth= 200, max_features= 'log2', n_estimators= 700),X, y, cv=cv_val)\nprint(scores)\nprint(np.average(scores))","ff112dcd":"### Please refer the [EDA and data visualization notebook](https:\/\/www.kaggle.com\/priya1207\/eda-and-visualization) for understanding the preprocessing steps and assumptions.)\n\n### We will be using this preprocessed dataset for futher analysis in this notebook.\n\n### Feel free to check out the dataset from my [github repo](https:\/\/github.com\/priya-mane\/Covid-19_ICU_Prediction). ","23749028":"<center><h1>Machine Learning for Classification<\/h1><\/center>","cf2d68e3":"### Target feature distribution anaylsis","9402efc4":"## Random forest Optimization\n\n* Grid Search\n* Cross Validation \n\n","de92f7cf":"### We get maximum accuracy for Random Forest Classifier i.e 83-88 %","67ce31e5":"## Split train and test data\n\n* We must always split the train and test data before performing any operations further, this will help us under if the model is \"general\" enough i.e to avoid any overfitting.","3edc6b14":"### Hence, we will now use the startified split data for fitting the model and then testing it.","8fd5de06":"## Finally we can conclude that our model has an accuracy of 88.26%","51f9d9e7":"### Feature Scaling for \"AGE_PERCENTIL\"","21b1a0ed":"Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n\n[Ref](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/)"}}