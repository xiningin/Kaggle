{"cell_type":{"4261250f":"code","3399f729":"code","a1529bb9":"code","7e991235":"code","02effc05":"code","b4bd9f77":"code","0a70dc3d":"code","0ed22ccb":"code","7ccf1dc3":"code","58985f94":"code","8e80818d":"code","cde5c73d":"code","d2e80dc7":"code","611d4ed4":"code","978b5e39":"code","d3650f65":"code","0359d58d":"code","8de9fff5":"code","5362df91":"markdown","a8d354ac":"markdown","a10e6fbc":"markdown","0645c5b7":"markdown","ecf43de0":"markdown","bdf20ab8":"markdown","ad11b627":"markdown","cd7c922c":"markdown","aded740e":"markdown","5d50b1c8":"markdown","9371c02b":"markdown","4357cf37":"markdown","c3d82b8d":"markdown","45a179fc":"markdown","922c103c":"markdown","85487d07":"markdown","a2a3345b":"markdown","30ddf627":"markdown","3a7d649d":"markdown","cc97129f":"markdown","6ab97e32":"markdown","2ebbce13":"markdown","381c843d":"markdown","ddf537d5":"markdown","11f57a7d":"markdown","c37bc90b":"markdown","b8129603":"markdown","411a5fd3":"markdown"},"source":{"4261250f":"# This cell imports all the libraries and functions\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\n# setting Jedha color palette as default\n#pio.templates[\"jedha\"] = go.layout.Template(layout_colorway=[\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"])\n#pio.templates.default = \"jedha\"\n#pio.renderers.default = \"notebook\" # to be replaced by \"iframe\" if working on JULIE\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import  OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import r2_score\n","3399f729":"# Import dataset\nprint(\"Loading dataset...\")\ndataset = pd.read_csv(\"..\/input\/walmart-dataset-retail\/Walmart_Store_sales.csv\") \n# Basic information\nprint(\"Number of rows : {}\".format(dataset.shape[0]))\nprint()\n\nprint(\"Display of dataset: \")\ndisplay(dataset.head()) # this will display the first few lines of the dataset (default is 5)\nprint()","a1529bb9":"from plotly.subplots import make_subplots\n\n# Distribution of each numeric variable\nnum_features = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\nfig = make_subplots(rows = len(num_features), cols = 1, subplot_titles = num_features)\nfor i in range(len(num_features)):\n    fig.add_trace(\n        go.Histogram(\n            x = dataset[num_features[i]], nbinsx = 50),\n        row = i + 1,\n        col = 1)\nfig.update_layout(\n        title = go.layout.Title(text = \"Distribution of quantitative variables\", x = 0.5), showlegend = False, \n            autosize=False, height=900)\nfig.show()\n\n# Barplot of each qualitative variable\ncat_features = ['Store', 'Holiday_Flag']\nfig = make_subplots(rows = len(cat_features), cols = 1, subplot_titles = cat_features)\nfor i in range(len(cat_features)):\n    x_coords = dataset[cat_features[i]].value_counts().index.tolist()\n    y_coords = dataset[cat_features[i]].value_counts().tolist()\n    fig.add_trace(go.Bar(x = x_coords,y = y_coords),row = i + 1,col = 1)\n\nfig.update_layout(title = go.layout.Title(text = \"Barplot of qualitative variables\", x = 0.5), showlegend = False, autosize=False, height=400)","7e991235":"for f in num_features:\n    if f != 'Weekly_Sales':\n        fig = px.scatter(dataset, x = f, y = \"Weekly_Sales\", marginal_x = \"box\", marginal_y = \"box\")\n        fig.show()","02effc05":"for i in range(len(num_features)): # loop over all features\n    f1 = num_features[i]\n    for j in range(i): # loop over features \"below\" f1 to avoid duplicates\n        f2 = num_features[j]\n        if (f1 != 'Weekly_Sales') and (f2 != 'Weekly_Sales'):\n            fig = px.scatter(dataset, x = f1, y = f2, marginal_x = \"box\", marginal_y = \"box\")\n            fig.show()","b4bd9f77":"# Dates\n# Convert Date column to Datetime\ndataset.loc[dataset['Date'].notnull(), 'Date'] = dataset.loc[dataset['Date'].notnull(), 'Date'].astype('datetime64')\n# Line plot of weeklys sales vs. date\nfig = px.line(dataset.loc[dataset['Date'].notnull(), :].sort_values(by = 'Date'), x='Date', y=\"Weekly_Sales\")\n# Add range slider\nfig.update_layout(xaxis = go.layout.XAxis(rangeslider = go.layout.xaxis.Rangeslider(visible = True)))\nfig.show()","0a70dc3d":"print(\"Basics statistics: \")\ndata_desc = dataset.describe(include='all') # This will calculate basic statistics\ndisplay(data_desc)\nprint()\n\nprint(\"Percentage of missing values: \")\ndisplay(100*dataset.isnull().sum()\/dataset.shape[0]) # this will count the proportion of missing values in each variable\n","0ed22ccb":"# Drop lines where the target is missing\nprint('Dropping lines with missing values of Y...')\ndataset = dataset.loc[dataset['Weekly_Sales'].notnull(), :]\nprint('...Done.')\n\n# Extract features from Date column\nprint(\"Creating Datetime features...\")\ndataset.loc[dataset['Date'].notnull(), 'Year'] = dataset.loc[dataset['Date'].notnull(), 'Date'].astype('datetime64').dt.year\ndataset.loc[dataset['Date'].notnull(), 'Month'] = dataset.loc[dataset['Date'].notnull(), 'Date'].astype('datetime64').dt.month\ndataset.loc[dataset['Date'].notnull(), 'Day'] = dataset.loc[dataset['Date'].notnull(), 'Date'].astype('datetime64').dt.day\ndataset.loc[dataset['Date'].notnull(), 'DayOfWeek'] = dataset.loc[dataset['Date'].notnull(), 'Date'].astype('datetime64').dt.dayofweek\nprint(\"...Done.\")\n\nprint('Dropping Date column...')\ndataset = dataset.drop('Date', axis=1)\nprint('...Done.')\ndataset.head()","7ccf1dc3":"# Drop lines containing outliers (using masks)\n\ndef drop_outliers(df, col_name):\n    print('Dropping outliers in ', col_name ,'...')\n    to_keep = (df[col_name].isnull()) | ((df[col_name] < df[col_name].mean() + 3 * df[col_name].std()) & \n               (df[col_name] > df[col_name].mean() - 3 * df[col_name].std()))\n    df = df.loc[to_keep,:]\n    print('Done. Number of lines remaining : ', df.shape[0])\n    return df\n\nfor c in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']:\n    dataset = drop_outliers(dataset, c)\n    \ndataset.head()\n","58985f94":"# Separate target variable Y from features X\ntarget_name = 'Weekly_Sales'\n\nprint(\"Separating labels from features...\")\nY = dataset.loc[:,target_name]\nX = dataset.loc[:,[c for c in dataset.columns if c!=target_name]] \nprint(\"...Done.\")\n\n# Convert pandas DataFrames to numpy arrays before using scikit-learn\nprint(\"Convert pandas DataFrames to numpy arrays...\")\nX = X.values\nY = Y.tolist()\nprint(\"...Done\")\nprint(X[0:5,:])\nprint()\nprint(Y[0:5])","8e80818d":"# First : always divide dataset into train set & test set !!\nprint(\"Dividing into train and test sets...\")\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 0)\nprint(\"...Done.\")\nprint()","cde5c73d":"# Create pipeline for numeric features\nnumeric_features = [2, 3, 4, 5, 6, 7, 8, 9] # Positions of numeric columns in X_train\/X_test\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')), # missing values will be replaced by the variables' median\n    ('scaler', StandardScaler())\n    ])\n\n# Create pipeline for categorical features\ncategorical_features = [0, 1] # Positions of categorical columns in X_train\/X_test\ncategorical_transformer = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by the most frequent values for each variable\n    ('encoder', OneHotEncoder(drop='first')) # first column will be dropped to avoid creating correlations between features\n    ])\n\n# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n\n# Preprocessings on train set\nprint(\"Performing preprocessings on train set...\")\nprint(X_train[0:5,:])\nX_train = preprocessor.fit_transform(X_train)\nprint('...Done.')\nprint(X_train[0:5,:])\nprint()\n\n# Preprocessings on test set\nprint(\"Performing preprocessings on test set...\")\nprint(X_test[0:5,:])\nX_test = preprocessor.transform(X_test) # Don't fit again !! \nprint('...Done.')\nprint(X_test[0:5,:])\nprint()","d2e80dc7":"# Train model\nmodel = LinearRegression()\n\nprint(\"Training model...\")\nmodel.fit(X_train, Y_train) # Training is always done on train set !!\nprint(\"...Done.\")\n\n# Predictions on training set\nprint(\"Predictions on training set...\")\nY_train_pred = model.predict(X_train)\nprint(\"...Done.\")\n\nprint('Y_pred :')\nprint(Y_train_pred[0:5])\nprint()\n\n# Predictions on test set\nprint(\"Predictions on test set...\")\nY_test_pred = model.predict(X_test)\nprint(\"...Done.\")\nprint('Y_test')\nprint(Y_test_pred[0:5])\nprint()","611d4ed4":"# Print scores\nprint(\"R2 score on training set : \", r2_score(Y_train, Y_train_pred))\nprint(\"R2 score on test set : \", r2_score(Y_test, Y_test_pred))","978b5e39":"# Interpreting the model's coefficients\nfeature_names = [str('x' + str(i)) for i in preprocessor.transformers_[0][2]]\ncat_features = preprocessor.transformers_[1][1]['encoder'].get_feature_names().tolist()\nfeature_names.extend(cat_features)\n\nmodel_coefficients = pd.DataFrame(index = feature_names, columns = ['baseline'], data = model.coef_)\nmodel_coefficients.sort_values(by = 'baseline', key = np.abs, ascending = False)","d3650f65":"# First regularized model (Ridge)\nridge = Ridge() # create an instance of the model\nridge.fit(X_train, Y_train)\n\n# Predictions on training set\nprint(\"Predictions on training set...\")\nY_train_pred = ridge.predict(X_train)\nprint(\"...Done.\")\nprint(Y_train_pred[0:5])\nprint()\n\n# Predictions on test set\nprint(\"Predictions on test set...\")\nY_test_pred = ridge.predict(X_test)\nprint(\"...Done.\")\nprint(Y_test_pred[0:5])\nprint()\n\n# Print scores\nprint(\"R2 score on training set : \", r2_score(Y_train, Y_train_pred))\nprint(\"R2 score on test set : \", r2_score(Y_test, Y_test_pred))","0359d58d":"# Grid search to tune the regularization strength\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'alpha': np.arange(0, 300, 1)} # determine the range of parameters to try\nridge = Ridge() # create an instance of the model\nridgeCV = GridSearchCV(ridge, params, cv=5, verbose = 1)\nridgeCV.fit(X_train, Y_train)\n\n# Predictions on training set\nprint(\"Predictions on training set...\")\nY_train_pred = ridgeCV.predict(X_train)\nprint(\"...Done.\")\nprint(Y_train_pred[0:5])\nprint()\n\n# Predictions on test set\nprint(\"Predictions on test set...\")\nY_test_pred = ridgeCV.predict(X_test)\nprint(\"...Done.\")\nprint(Y_test_pred[0:5])\nprint()\n\n# Print scores\nprint(\"R2 score on training set : \", r2_score(Y_train, Y_train_pred))\nprint(\"R2 score on test set : \", r2_score(Y_test, Y_test_pred))","8de9fff5":"print('Ridge alpha : ', ridgeCV.best_params_['alpha'])\n# Store Ridge coefficients into dataframe\nmodel_coefficients.loc[:,'Ridge'] = ridgeCV.best_estimator_.coef_\nmodel_coefficients.sort_values(by = 'Ridge', key = np.abs, ascending = False)","5362df91":"#### Ridge model with regularization strength tuned by grid search","a8d354ac":"![forecast.png](attachment:1e5b416a-0f3f-462e-94b0-e05d4026ef70.png)\n\nby [CX today](https:\/\/www.cxtoday.com\/)","a10e6fbc":"## Part 2 - Train baseline model <a class=\"anchor\" id=\"chapter4\"><\/a>","0645c5b7":"**Remarks :**\n\nThe distributions of numeric variables are skewed and not close to the shape of a **normal distribution**. This might deteriorate a linear regression model's performances, but we have to deal with it. As we're working with a small dataset there's unfortunately no magic solution to make the distributions less skewed in this case.","ecf43de0":"#### Interpretation\nThe grid search performed to tune the Ridge $\\alpha$ hyperparameter converged to $\\alpha = 0$, which is equivalent to a Linear regression model without regularisation. Indeed, one can notice that Ridge model's coefficients are exactly equal to the ones that were obtained for a non-regularized linear model.","bdf20ab8":" The exploration of the above data makes it possible to know which pre-processing steps will be necessary:\n\n **1. Preprocessing to be planned with pandas**\n\n **Create usable features from the *Date* column :**\nThe *Date* column cannot be included as it is in the model. Instead, we will extract the following numeric features from the datetime objects : \n- *year*\n- *month*\n- *day*\n- *day of week*\n\n **Drop lines containing invalid values or outliers :**\nIn this project, will be considered as outliers all the numeric features that don't fall within the range : $[\\bar{X} - 3\\sigma, \\bar{X} + 3\\sigma]$. This concerns the columns : *Temperature*, *Fuel_price*, *CPI* and *Unemployment*\n \n\n\n **Target variable\/target (Y) that we will try to predict, to separate from the others** : *Weekly_Sales*\n\n **------------**\n\n **2. Preprocessings to be planned with scikit-learn**.\n\n **Explanatory variables (X)**\n We need to identify which columns contain categorical variables and which columns contain numerical variables, as they will be treated differently.\n\n - Categorical variables : Store, Holiday_Flag\n - Numerical variables : Temperature, Fuel_Price, CPI, Unemployment\n\n In this dataset, we have both types of variables. It will thus be necessary to create a numeric_transformer that will wrap together all preprocessing steps for numerical variables (it will call the StandardScaler class and replace missing values using the SimpleImputer class) and a categorical_transformer to wrap together all the preprocessing steps for categorical variables (it will call the OneHotEncoder class and replace missing values using the SimpleImputer class).","ad11b627":"**There's no correlation between the explanatory variables, which is a good news !**","cd7c922c":"11. Importance of the features","aded740e":"3. Bi-variate analysis : target vs numeric features","5d50b1c8":"## Conclusion <a class=\"anchor\" id=\"chapter6\"><\/a>\n\nEven if our model is not perfect and does not generalize exactly as we would like due to overfitting, it still allows us to reach an $R^2$ score of more than 90% on Walmart sales from only a few features, which is very interesting! This model can be adapted to any type of company to better predict their sales.\n\n\ud83d\udc49 *Pierre-Louis Danieau*\n","9371c02b":"#### Area of improvement\n\nThe ridge model does not allow to reduce the overfitting of our model, so we have to try another method. One way of improvement would be to try the Lasso model to see if we get better results than with the ridge!\n","4357cf37":"**The sales fluctuate over time, there might even be some seasonality ! That's why it would be useful to extract numeric features from the Date column, so as to take it into account in our model.**","c3d82b8d":"7. Drop outliers","45a179fc":"2. EDA","922c103c":"9. Pipeline for numeric and categorical features","85487d07":"## Table of Contents\n\n* [\ud83d\udca1 Projet Description ](#chapter1)\n\n* [\ud83d\udc49 Dataset : Walmart Store Sales](#chapter2)\n    \n* [Part 1 - Exploratory Data Analysis](#chapter3)\n\n* [Part 2 - Train baseline model ](#chapter4)\n\n* [Part 3 - overfitting](#chapter5)\n\n* [Conclusion](#chapter6)","a2a3345b":"5. Weekly sales by Date","30ddf627":"1. Import DATA","3a7d649d":"## Part 1 - Exploratory Data Analysis <a class=\"anchor\" id=\"chapter3\"><\/a>","cc97129f":"10. Interpreting the R2-score","6ab97e32":"Here, we can see that the most important information for the prediction is the identifier of the *Store* in which the sales were made (*x0*). \n\nOn the contrary, the coefficients of the following features seem negligible :\n- *Temperature* (**x2**)\n- *Fuel_Price* (**x3**)\n- *Day* (**x8**)\n- *DayOfWeek* (**x9**)\n\n## Part 3 - overfitting <a class=\"anchor\" id=\"chapter5\"><\/a>\n### Ridge regularization\n#### First model (with default value for the regularization strength)","2ebbce13":"## \ud83d\udca1 Projet Description <a class=\"anchor\" id=\"chapter1\"><\/a>\n \nHistorical sales data for 45 Walmart stores located in different regions are available. There are certain events and holidays which impact sales on each day. The business is facing a challenge due to unforeseen demands and runs out of stock some times, due to inappropriate machine learning algorithm. Walmart would like to predict the sales and demand accurately. An ideal ML algorithm will predict demand accurately and ingest factors like economic conditions including CPI, Unemployment Index, etc. The objective is to determine the factors affecting the sales and to analyze the impact of markdowns around holidays on the sales.\n\nThanks to [@Rutu_Patel](https:\/\/www.kaggle.com\/rutuspatel) for his dataset and his explanations of it [here](https:\/\/www.kaggle.com\/rutuspatel\/walmart-sales-project).","381c843d":"6. Basic statistics","ddf537d5":"Our first model achieves really good performances, as the $R^2$ is quite close to 1 on the train set AND on the test set. However, one can notice that the model's overfitting a bit ($R^2(test) < R^2(train)$). \n\nIn part 3 of this project, this will be solved by introducing some regularization techniques.","11f57a7d":"8. Train \/ Test set","c37bc90b":"4. Bi-variate analysis : numeric features vs numeric features","b8129603":"## \ud83d\udc49 Dataset : Walmart Store Sales <a class=\"anchor\" id=\"chapter2\"><\/a>\n\nThis is the historical data that covers sales from 2010-02-05 to 2012-11-01, in which you will find the following fields:\n\n- Store - the store number\n\n- Date - the week of sales\n\n- Weekly_Sales - sales for the given store\n\n- Holiday_Flag - whether the week is a special holiday week 1 \u2013 Holiday week 0 \u2013 Non-holiday week\n\n- Temperature - Temperature on the day of sale\n\n- Fuel_Price - Cost of fuel in the region\n\n- CPI \u2013 Prevailing consumer price index\n\n- Unemployment - Prevailing unemployment rate","411a5fd3":"# Walmart sales forecasting : Predict weekly sales \ud83d\udcb2\ud83d\udcc8\n\n<img src=\"https:\/\/www.bestdesigns.co\/uploads\/inspiration_images\/4350\/990__1511457498_404_walmart.png\" alt=\"WALMART LOGO\" height=\"50%\" width=\"50%\">"}}