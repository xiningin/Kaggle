{"cell_type":{"4b62c742":"code","c8daa6b6":"code","35fb498b":"code","30bf4550":"code","8cc361e1":"code","32863714":"code","744ebf92":"code","9b2ab751":"code","6d6180e8":"code","b7010ead":"code","8ab39534":"code","cb33ecda":"code","490d2332":"code","68a4782d":"code","7019313b":"code","b208d65b":"code","1b4259a2":"code","887d82f0":"code","28af8047":"code","2347a0af":"code","98896a7c":"code","91451baf":"code","375503a3":"code","67ac0e0d":"code","b701e1c7":"code","b154b005":"code","ff001db2":"code","8be37da5":"code","948decd1":"code","4a8a3871":"code","0bf4c3cf":"code","b4137801":"code","0078471e":"code","2339e65c":"code","4d600475":"code","31fcf2b3":"code","8484e5cb":"code","01540bb0":"code","09bd5c5e":"code","4edeab18":"code","06ff3e94":"code","034ad28b":"code","4b4b9122":"code","476c2591":"code","1718e49d":"code","47cf3534":"code","3ae7d61b":"code","850d24f7":"code","d697f36e":"code","8f2a4fb3":"code","65302a90":"code","9cc09999":"markdown","339ec7b9":"markdown","8ba6c778":"markdown","a89e6f52":"markdown","9f1329c0":"markdown","163ed837":"markdown","fa3b14e0":"markdown","67abeac5":"markdown","c09f38b9":"markdown","0d43769e":"markdown","34a2793e":"markdown","fa5f6bf9":"markdown","687c2741":"markdown","ae3c2639":"markdown","cb2c5530":"markdown","a2f6b642":"markdown","d57f39ea":"markdown","b7afeba3":"markdown","31197e3b":"markdown","df87f705":"markdown","2a4e5098":"markdown","d87c557a":"markdown","3df1f76b":"markdown","e2e3cabb":"markdown","1232bd1b":"markdown","838b362a":"markdown","975fca2c":"markdown","f10f56f8":"markdown","58130c88":"markdown","180993b9":"markdown","2cdabf2d":"markdown","f80bf3d4":"markdown","0146c5d0":"markdown","9b14346d":"markdown","4b83d863":"markdown","cb5f83d7":"markdown","4b9a8dc8":"markdown","bd185d34":"markdown","d0b8af7f":"markdown","862d2338":"markdown","87448692":"markdown","25356a13":"markdown","49a1669e":"markdown","4318c0a1":"markdown","5d3e4424":"markdown","10583167":"markdown","9a743a6a":"markdown","0dba0ba9":"markdown","19e32d53":"markdown","0c8cf440":"markdown","55a0b267":"markdown","7ab27508":"markdown","4738c2bb":"markdown","39eae8f1":"markdown","d5ad09e7":"markdown","e17e03aa":"markdown"},"source":{"4b62c742":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8daa6b6":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing\nimport matplotlib.pyplot as plt  # data visualization\nimport seaborn as sns  # data visualization\nimport plotly.express as px\nimport time\n\n# data preprocessing\nfrom sklearn.model_selection import StratifiedKFold  # Stratified K-Folds cross-validator\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder  # for data preprocessing\nfrom sklearn.metrics import classification_report, accuracy_score  # for evaluation\nfrom sklearn.model_selection import train_test_split  # for train_test_split\nfrom scipy.stats import mode\n\n# data transformation\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# data modeling\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n# for deep from sklearn.model_selection import cross_val_score\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.utils import to_categorical\n\n# visualization options\nfrom matplotlib import ticker\nimport time\nimport warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')","35fb498b":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')","30bf4550":"print(f\"train_shape: {train.shape}\")\nprint(f\"test_shape: {test.shape}\")\nprint(f\"sample_submission_shape: {sample_submission.shape}\")","8cc361e1":"train.head()","32863714":"test.head()","744ebf92":"print(f\"unique num in Id columns: {train.nunique()['Id']}\")","9b2ab751":"# drop the 'Id' column\n# because we don't need it for training\ntrain.drop(['Id'], axis = 1, inplace = True)\ntest.drop(['Id'], axis = 1, inplace = True)\n\n# define target\ntarget = 'Cover_Type'\n\n# explore features\nfeatures = [col for col in train.columns if col not in ['Id', target]]\nprint(f\"len(features): {len(features)}\")\nprint(f\"features in train set: {features}\")","6d6180e8":"def explore_data(data_type, data):\n    print(f\"number of rows in {data_type} data: {data.shape[0]}\")\n    print(f\"number of columns in {data_type} data: {data.shape[1]}\")\n    print(f\"number of missing values in {data_type} data: {sum(data.isna().sum())}\")  # check if there is null values","b7010ead":"train.shape","8ab39534":"train.head()","cb33ecda":"train.info()  # Print a concise summary of a DataFrame.","490d2332":"explore_data(\"train\", train)","68a4782d":"train.describe()","7019313b":"test.shape","b208d65b":"test.head()","1b4259a2":"test.info()  # Print a concise summary of a DataFrame.","887d82f0":"explore_data(\"test\", test)","28af8047":"test.describe()","2347a0af":"sample_submission.head()","98896a7c":"# generate descriptive statistics using pandas describe()\ntrain.iloc[:, :-1].describe().T.sort_values(by = 'std', ascending = False)\\\n                    .style.background_gradient(cmap = 'GnBu')\\\n                    .bar(subset = [\"max\"], color = '#BB0000')\\\n                    .bar(subset = [\"mean\",], color = 'green')","91451baf":"train.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)\ntrain.head()","375503a3":"features = [col for col in train.columns if col not in ['Id', 'Cover_Type', 'Soil_Type7', 'Soil_Type15']]\nprint(features)","67ac0e0d":"df = pd.concat([train[features], test[features]], axis = 0)\n\ncat_features = [col for col in df.columns if df[col].nunique() < 25]\ncont_features = [col for col in df.columns if df[col].nunique() >= 25]\n\n# del df\nprint(f\"Total number of features: {len(features)}\")\nprint(f\"number of categorical features: {len(cat_features)}\")\nprint(f\"number of continuous features: {len(cont_features)}\")\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels = ['catetorical_features', 'continuous_features'],\n       colors = ['green', 'orange'])\nplt.show()","b701e1c7":"# plot continuous features\ni = 1\nplt.figure()  # Create a new figure, or activate an existing figure.\nfig, ax = plt.subplots(2, 5, figsize = (20, 12))\n\nfor feature in cont_features:\n    plt.subplot(2, 5, i)\n    sns.histplot(train[feature], color = 'blue', kde = True, bins = 100, label = 'train_'+str(feature))\n    sns.histplot(test[feature], color = 'green', kde = True, bins = 100, label = 'test_'+str(feature))\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\n    \nplt.show()","b154b005":"train[cont_features].describe()","ff001db2":"corr = train[cont_features + ['Cover_Type']].corr()\ncorr.style.background_gradient(cmap = 'coolwarm').set_precision(3)","8be37da5":"# sns.heatmap(train[cont_features + ['Cover_Type']], cmap ='RdYlGn', linewidths = 0.30, annot = True)  # got memorry error! :()","948decd1":"sns.catplot(x = 'Cover_Type', kind = \"count\", palette = \"ch:.25\", data=train)","4a8a3871":"train.Cover_Type.value_counts()","0bf4c3cf":"cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Cover_Type']","b4137801":"plt.figure(figsize = (15,10))\nsns.heatmap(train[cols].corr(), vmin = -1, vmax=1, annot = True, cmap=\"Spectral\") ","0078471e":"del fig, axes, i","2339e65c":"del corr","4d600475":"# we already load relevant libraries at the beginning but for the sake of example, let's load libraies again!\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder  # for data preprocessing\n\n# preprocessing for numerical data\nnum_transformer = SimpleImputer(strategy='median')\n\n# preprocessinf for categorical data\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, cont_features),\n        ('cat', cat_transformer, cat_features)\n    ])","31fcf2b3":"# # Baseline\n# rf_clf = RandomForestClassifier()\n\n# # Bundle preprocessing and modeling code in a pipeline\n# # Pipeline steps: list of tuples that are chained, in the order in which they are chained\n# clf = Pipeline(steps=[\n#     ('Preprocessor', preprocessor),\n#     ('scaler', StandardScaler()),\n#     ('model', rf_clf)\n# ])\n\n# print(cross_val_score(clf, train, X_test, cv=5).mean())","8484e5cb":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","01540bb0":"start = time.time()\n\nnew_train_data=reduce_mem_usage(train)\nnew_test_data=reduce_mem_usage(test)\n\nstop = time.time()\nprint(f\"Time: {round((stop - start), 3)} seconds\")","09bd5c5e":"# start timer\nstart = time.time()\n\n# labels and features\ny = new_train_data.Cover_Type\n# features\nX = new_train_data.drop('Cover_Type', axis=1)\n\n# scale data\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest_data_preprocessed = scaler.transform(new_test_data)\n\n# Label encoding\n# encoder = LabelEncoder()\n# y = encoder.fit_transform(y)\n\n# train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)  # by default it will be set to 0.25\nprint(\"-\" * 50)\nprint(f\"X_train.shape: {X_train.shape}\")\nprint(f\"y_train.shape: {y_train.shape}\")\nprint(f\"X_test.shape: {X_test.shape}\")\nprint(f\"y_test.shape: {y_test.shape}\")\nprint(\"-\" * 50)\n\n\n# Random Forest Classifier\nrf_clf=RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# validation\npred_valid = rf_clf.predict(X_test)\nprint(f\"accuracy_score: {accuracy_score(y_test, pred_valid)}\")\nprint(\"-\" * 50)\n\n# make predictions\nrf_preds = rf_clf.predict(test_data_preprocessed)\nprint(\"rf_preds: \")\nprint(rf_preds[:5])\nprint(\"-\" * 50)\n\n# stop timer\nstop = time.time()\n\nprint(f\"Time: {round((stop - start), 3)} seconds\")","4edeab18":"print(\"rf_preds: \")\nprint(rf_preds[:5])\nprint(\"-\" * 50)\n\n# stop timer\nstop = time.time()\n\nprint(f\"Time: {round((stop - start), 3)} seconds\")","06ff3e94":"print(classification_report(y_test, pred_valid))","034ad28b":"sub = pd.DataFrame()\nsub['Id'] = sample_submission['Id']\nsub['Cover_Type'] = rf_preds\nsub.head()","4b4b9122":"sub.tail()","476c2591":"sub.shape","1718e49d":"sub.to_csv('submission.csv', index=False)","47cf3534":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, pred_valid)","3ae7d61b":"from matplotlib.ticker import PercentFormatter\n\n# And we submit the modified predictions\nsub.to_csv('submission.csv', index=False)\nprint(sub.Cover_Type.value_counts())","850d24f7":"sub.loc[sub.Cover_Type == 4, 'Cover_Type'] = 3\nsub.Cover_Type.value_counts()","d697f36e":"from matplotlib.ticker import PercentFormatter\n\n# And we submit the modified predictions\nsub.to_csv('submission.csv', index=False)\nprint(sub.Cover_Type.value_counts())\n\n\n# Plot the distribution of the test predictions\nplt.figure(figsize=(10,3))\nplt.hist(sub['Cover_Type'], bins=np.linspace(0.5, 7.5, 8), density=True, rwidth=0.7, label='Test predictions')\nprobes = [[1, 0.38565], [2, 0.51259], [3, 0.07817], [4, 0.00034], [6, 0.00701], [7, 0.01621]]\nplt.bar([c for c, f in probes], [f for c, f in probes], label='lb frequencies', color='k', width=0.2)\nplt.xticks(ticks=range(1, 8), labels=[f\"{i}\\n{(sub['Cover_Type'] == i).mean():.5f}\" for i in range(1, 8)])\nplt.xlabel('Cover_Type')\nplt.ylabel('Frequency')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1))\nplt.legend()\nplt.show()\n\nsub.head()","8f2a4fb3":"sub.to_csv('submission.csv', index=False)","65302a90":"sub.Cover_Type.value_counts()","9cc09999":"### Features Distribusion of Categorical Features\n\nA quick way to examine the shape of data is to plot each numerical feature as a histogram. ","339ec7b9":"<a id='1'><\/a>\n# 1. Frame the Problem\n---\nThe first question is *'What is our business purpose?'*<br>\n\n\nIn real case, it may not just make ML model(may be for profit or else)<br>\nBut in our case, This is very clear. Because Kaggle tell us everything! :)\n\n\n> The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition\n\n> For this competition, you will be predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on a the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n\nAnd we must frame the problem.<br>\n\n*is supervised-learning?<br>\nis unsupervised-learning?<br>\nis regression?<br>\nis reinforcement learning?<br>\nis batch inference?<br>\nis online inference?<br>*\n\nKaggle Tell us that this competition is Supervised Learning especially Classification!<br>\n(will be predicting the <code>Cover_Type<\/code> for each row in this file (the target integer class))<br>\nThen we Frame the Problem like this\n\n## Frame the Problem\n* play Kaggle with Fun\n* predicting a categorical target based on a number of feature columns given in the data\n\n## Select evaluation metrics\n> Submissions are evaluated on multi-class classification accuracy.\n\nHow kind you are :)\n\n\nBecause we are beginner, our problem is 'how enjoy the Kaggle'. looool!!!\n\n*(If you guys want to more about evaluation. please refere here [[Model Evaluation] 1. Classification Metrics\n](https:\/\/www.kaggle.com\/leeyj0511\/model-evaluation-1-classification-metrics)*","8ba6c778":"### Basic statistics of test data","a89e6f52":"* \"Soil_Type7\" and \"Soil_Type15\" have only one value that is 0 for all records, So Dropping those columns.\n","9f1329c0":"It seems that the range of values are different between the features.<br>\nScaling seems to be necessary because it is good to match the range of values between features for train a machine learning model.","163ed837":"### Continuous and Categorical Data Distribution","fa3b14e0":"* The scales of the features are very different from each other. We will look at the feature scale later.\n\n* Many histograms have thick tails. In other words, the data is often spread out to the left and right rather than the center. These forms make it difficult to find patterns in machine learning algorithms. Let's transform these features into a more bell-shaped distribution.","67abeac5":"# Recap\nThis will help you whenever you do ML Project.<br>\nKeep in mind :)\n\n- [x]  Frame the Problem\n- [x]  Get the data\n- [x]  Explore the data\n- [x]  Prepare the data\n- [x]  Model the data\n- [x]  Fine-tune the data\n- [x]  Present the solution\n- [x]  Launch the ML System","c09f38b9":"### Quick view of Test Data","0d43769e":"It takes really long time......... :(\n\nI think we need to reduce the memory use!<br>\nI searched some notebook\n* [TPS Dec - EDA + Feat. Eng. + PseudoLab.](https:\/\/www.kaggle.com\/samuelcortinhas\/tps-dec-eda-feat-eng-pseudolab) \n* [TPS -DEC 2021 - LIGHTGBM - Top 200](https:\/\/www.kaggle.com\/slythe\/tps-dec-2021-lightgbm-top-200)\n\nThese notebooks show us how to reduce memory.","34a2793e":"It also seems that the range of values are different between the features.<br>\nSo is also seems that test set need to scaling.","fa5f6bf9":"<a id='4'><\/a>\n# 4. Prepare the Data\n\nNow it's time to prepare the data for the machine learning algorithm. Automating this task instead of doing it manually has the following benefits:\n\n* Easily iterate data transformations for any dataset\n* You will gradually build a transformation library that you can use in future projects.\n* In real systems, this function can be used to transform new data before load it into the algorithm.\n* It's easy to try different data transformations, and it's convenient to see which combination works best.","687c2741":"we have 4000000 data and have 55 columns(all int64 dtype) and no missing values","ae3c2639":"### Transform Pipeline\n* <code>sklearn.pipeline.Pipeline()<\/code> [scikit-learn docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html)\n\nPipeline of transforms with a final estimator.\n\nSequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.","cb2c5530":"Elevation seems to have the highest negative correlation to Cover_Type although relatively weak correlation","a2f6b642":"# Bonus\n> Upgrade Model Performance!\n* Referenced: [TPSDEC21-12 Eliminate cover type 4!\n](https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-12-eliminate-cover-type-4\/notebook)","d57f39ea":"## Select model and train it\n### Baseline with Random Forest Classifier","b7afeba3":"### Feature correlation(only Continuous Features)\n\nFor continuous variables, the standard correlation coefficient between features can be easily obtained using the corr() method.\n\nCorrelation ranges from -1 to 1. A value close to 1 means a strong positive correlation (on the contrary, a value close to -1 indicates a strong negative correlation).\n\nA correlation coefficient close to 0 means that there is no linear correlation.","31197e3b":"## Import Libraries","df87f705":"### Basic statistics of training data","2a4e5098":"got memory error...!<br>\ngot a memory error again :< -21.12.27.am 00:24-","d87c557a":"we have 1000000 data and have 54 columns(all int64 dtype) and no missing values","3df1f76b":"<a id='7'><\/a>\n# 7. Present the solution","e2e3cabb":"# Reference\n---\n## Reference Notebook\n* for EDA\n    * [TPS-DEC] \ud83d\udccaEDA + Modeling\ud83d\udd25  [link](https:\/\/www.kaggle.com\/odins0n\/tps-dec-eda-modeling)\n    * TPS Dec Step By Step \ud83c\udfc4 [link](https:\/\/www.kaggle.com\/hamzaghanmi\/tps-dec-step-by-step)\n    * TPS Dec 21 clean EDA and prediction\ud83d\udd25 [link](https:\/\/www.kaggle.com\/gaganmaahi224\/tps-dec-21-clean-eda-and-prediction)\n* for data prepare & baseline\n    * [Tutorial] Ensemble from starter to Expert! [link](https:\/\/www.kaggle.com\/leeyj0511\/tutorial-ensembel-from-starter-to-expert)\n* memory reduction'\n    * TPS Dec - EDA + Feat. Eng. + PseudoLab. [link](https:\/\/www.kaggle.com\/samuelcortinhas\/tps-dec-eda-feat-eng-pseudolab)\n* Top 200\n    * TPS -DEC 2021 - LIGHTGBM - Top 200 [link](https:\/\/www.kaggle.com\/slythe\/tps-dec-2021-lightgbm-top-200)","1232bd1b":"* the data is unbalanced\n* we have only one sample with target 5!","838b362a":"It took 30 minutes to learn<br>\nI'll have to find if there's a better way","975fca2c":"### Feature correlation for all features\n\nWe'll exclude soil types as these are boolean values","f10f56f8":"* Last update\n    * [x] 21.12.27.mon\n    * [ ] remove features\n    * [ ] fine-tuning","58130c88":"### Quick view of Train Data","180993b9":"# TL;DR\n---\n<img src='https:\/\/thumbs.gfycat.com\/AffectionateImmenseBronco-size_restricted.gif'><\/img>\n- [Frame the Problem](#1)\n- [Get the data](#2)\n- [Explore the data](#3)\n- [Prepare the data](#4)\n- [Select model and train it](#5)\n- [Fine-tune the model](#6)\n- [Present the solution](#7)\n- [Launch the ML System](#8)","2cdabf2d":"If you want to learn more about Ensemble Model.<br>\nplease reference these Notebooks\n\n* [[Tutorial] Ensembel from starter to Expert!](https:\/\/www.kaggle.com\/leeyj0511\/tutorial-ensembel-from-starter-to-expert)\n* [[Model Evaluation] 1. Classification Metrics](https:\/\/www.kaggle.com\/leeyj0511\/model-evaluation-1-classification-metrics)","f80bf3d4":"### Validating model","0146c5d0":"We note from train.info above that all columns are int64<br> \nWe should look at the min and max of the columns to check if they need full 64bit to store the integers","9b14346d":"<a id='2'><\/a>\n# 2. Get the Data\n---\n* Load Data\n\nSimple, Just get the Data","4b83d863":"<a id='3'><\/a>\n# 3. Explore the Data\n<img src = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/ba\/Data_visualization_process_v1.png\/350px-Data_visualization_process_v1.png' width=400><\/img>\n\n> The next step is exploring the data!\n\n\nIn statistics, exploratory data analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. - wiki pedia - ","cb5f83d7":"### Correlation matrix","4b9a8dc8":"# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:100%; text-align:center; border-radius: 15px;\">[TPS-Dec] End-to-End ML Project for Beginner<img src='https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26479\/logos\/header.png?t=2021-04-09-00-55-58'><\/img><\/p>\nHi I'm Steve. Well come to '[TPS-2021] End-to-End ML Project for Beginner'<br>\nToday I'm gonna show you how to make 'End-to-End ML Project for Beginner'<br>\n\n## LEGO ~","bd185d34":"* <code>class sklearn.pipeline.Pipeline()<\/code> [scikit-learn docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html)\n\nApplies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.","d0b8af7f":"### Submission 1 - baseline","862d2338":"## Exploring Submission File","87448692":"[[](http:\/\/)](http:\/\/)<h3 style=\"color:green\">If you think this notebook is helpful, upvotes would be greatly appreciated :-) <\/h3>","25356a13":"<code>id Column<\/code><br>\nThe id column is a int64 integer column that contains unique record indicators ranging from 0 to 3,999,999. Like most Tabular Series, this is simply an identifier for the record and is likely not going to be of use for modelling purposes.\n\n<code>Cover_Type Column<\/code><br>\nThe Cover_Type column contains the class targets we are attempting to predict. This is a multi-class classification problem. We should look first to see what class breakdown we have.\n","49a1669e":"# <a id='6'><\/a>\n# 6. Fine-tune the Model","4318c0a1":"[[](http:\/\/)](http:\/\/)<h1 style=\"color:green\">Yeah! :)<\/h1>\n\nnow score is 0.95001 :)","5d3e4424":"First, define simple function<br>\nThen explore data!","10583167":"### Target distribution","9a743a6a":"### Feature Distribution of Continuous Features","0dba0ba9":"## EDA(Exploratory Data Analysis)","19e32d53":"## Exploring Train Data","0c8cf440":"<a id='8'><\/a>\n# 8. Launch the ML System","55a0b267":"we'll use <code>heatmap<\/code>\n\nThe primary purpose of heatmap is to better visualize the volumn of locations","7ab27508":"## Exploring Test Data","4738c2bb":"# Notice\n---\n<img src='https:\/\/images.squarespace-cdn.com\/content\/v1\/571fab4ad51cd4f5bee60686\/ac130d5b-78e0-4e1d-a6b2-159b2911057c\/notice_Asset+1%402x.png' width=600 height=200><\/img>\n\n\n> This repository based on Hands-on-Machine Learning 2 from O\u2019Reilly Media \n\n\nI refferenced 'Chap 2. Full Machine Learning Project<br>\nIf you are the beginner like me, I wish this notebook will help you to understand ML Lifecyle","39eae8f1":"### Overview of Data","d5ad09e7":"<a id='5'><\/a>\n# 5. Select model and train it\nNow, finally, the last step!\nSo far, after defining the problem, we load data and explored the data. We then split the data into a training set and a test set and created a transformation pipeline to automatically clean and prepare the data to feed into our machine learning algorithms.\n\nYou are now ready to select and train your machine learning model!","e17e03aa":"All right!<br>\nwe have 4000000 train samples with 55 features"}}