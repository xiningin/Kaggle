{"cell_type":{"41680b38":"code","1770d971":"code","68f221b0":"code","4fa641cd":"code","e0be2930":"code","7c685eb5":"code","96687115":"code","9e14c0f8":"code","5ca60b37":"code","392894f6":"code","39b19323":"code","af78993c":"code","f00f1638":"code","7bdf565a":"code","0e821068":"code","02ac4b53":"code","1ca02fda":"code","f8e6cfbe":"code","4c634fed":"code","a5464437":"code","8c6bc98d":"code","e24909d6":"code","f5f08fb5":"code","76796e4e":"code","4ffca080":"code","2d022fbb":"code","288e0268":"code","aa67fd68":"code","a7b6e7b1":"code","d36c658b":"code","8a8b37c6":"code","afa589dd":"code","29483c0a":"code","3ace8c74":"code","0038545c":"code","08d76cee":"code","d8b26508":"code","a6912fa1":"code","57dff9e7":"code","a6029f87":"code","614c6b7b":"code","b783c189":"code","daebfac6":"markdown","22af53ab":"markdown","76cd38ba":"markdown","87b8e7c5":"markdown","acd03719":"markdown","9291a34d":"markdown","4b5af72a":"markdown","e85c5edc":"markdown","5291c801":"markdown","739c034d":"markdown","c50c3281":"markdown","5b831d61":"markdown","5732f7c5":"markdown","6b169786":"markdown","db3e70f1":"markdown","d2e9aae8":"markdown","33ed6229":"markdown","7711a9db":"markdown","3d3029fa":"markdown","322377de":"markdown"},"source":{"41680b38":"!pip install dataprep","1770d971":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\nfrom dataprep.eda import create_report\nfrom dataprep.eda import plot_missing\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","68f221b0":"data = pd.read_csv('..\/input\/holiday-package-purchase-prediction\/Travel.csv')\ndf = data.copy()\npd.set_option('display.max_row',df.shape[0])\npd.set_option('display.max_column',df.shape[1]) \ndf.head()","4fa641cd":"print('There is' , df.shape[0] , 'rows')\nprint('There is' , df.shape[1] , 'columns')","e0be2930":"# Eliminate duplicates\nprint('There are' , df.duplicated().sum() , 'duplicates')\ndf.loc[df.duplicated(keep=False),:]\ndf.drop_duplicates(keep='first',inplace=True)\nprint('There is now' , df.shape[0] , 'rows')\nprint('There is now' , df.shape[1] , 'columns')","7c685eb5":"df = data.copy()\ntarget = pd.DataFrame()\ntarget['ProdTaken'] = df['ProdTaken'].map({0:'Not Taken',1:'Taken'})\ndf = df.drop(['CustomerID','ProdTaken'],axis=1)\ndf = df.fillna(df.median())\ndf = df.dropna()\ndf.head()","96687115":"plot_correlation(df)","9e14c0f8":"categorical_df = pd.DataFrame()\nfor col in df.select_dtypes(include=['object','int64']):\n    categorical_df[col] = df[col]\ncategorical_df['PreferredPropertyStar'] = df['PreferredPropertyStar']\nfor col in categorical_df:\n    print(f'{col :-<50} {df[col].unique()}')","5ca60b37":"fig, ax = plt.subplots(4,3, figsize=(30, 30))\ni=0\nsns.set(font_scale = 1.5)\nfor col in categorical_df: \n    sns.countplot(categorical_df[col], hue=target['ProdTaken'], ax=ax[i\/\/3][i%3])\n    i=i+1\nplt.show()","392894f6":"numeric_df = df.copy()\nfor col in categorical_df:\n    numeric_df = numeric_df.drop(col,axis=1)\nnumeric_df.head()","39b19323":"taken_df = numeric_df[target['ProdTaken']==\"Taken\"]\nnot_taken_df = numeric_df[target['ProdTaken']==\"Not Taken\"]\nsns.set(font_scale = 1.5)\nfig, ax = plt.subplots(2,3, figsize=(30, 15))\ni=0\nfor col in numeric_df:\n    sns.distplot(taken_df[col],label='Taken',ax=ax[i\/\/3][i%3])\n    sns.distplot(not_taken_df[col],label='Not Taken',ax=ax[i\/\/3][i%3])\n    i=i+1\nfig.legend(labels=['Taken','Not Taken'],fontsize='22')\nfig.show()","af78993c":"def encoding(df):\n    code = {'Self Enquiry':0,\n            'Company Invited':1,\n            'Salaried':0,\n            'Free Lancer':1,\n            'Small Business':2,\n            'Large Business':3,\n            'Female':0,\n            'Male':1,\n            'Deluxe':0,\n            'Basic':1,\n            'Standard':2,\n            'SuperDeluxe':3,\n            'King':4,\n            'Single':0,\n            'Divorced':1,\n            'Married':2,\n            'Unmarried':3,\n            'Manager':0,\n            'Executive':1,\n            'Senior Manager':2,\n            'AVP':3,\n            'VP':4\n           }\n    for col in df.select_dtypes('object'):\n        df.loc[:,col]=df[col].map(code)        \n    return df\n\ndef imputation(df):\n    df = df.fillna(df.median())\n    df = df.dropna()\n    return df\n\ndef feature_engineering(df):\n    useless_columns = ['CustomerID']\n    df = df.drop(useless_columns,axis=1)\n    return df\n\ndef preprocessing(df):\n    df = encoding(df)\n    df = feature_engineering(df)\n    df = imputation(df)\n    \n    X = df.drop('ProdTaken',axis=1)\n    y = df['ProdTaken']    \n\n    return df,X,y","f00f1638":"df = data.copy()\ndf,X,y=preprocessing(df)","7bdf565a":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(3,'Score'))  #print 3 best features","0e821068":"target_dist = df['ProdTaken'].value_counts()\n\nfig, ax = plt.subplots(1, 1, figsize=(8,5))\n\nbarplot = plt.bar(target_dist.index, target_dist, color = 'lightgreen', alpha = 0.8)\nbarplot[1].set_color('darkred')\n\nax.set_title('Target Distribution')\nax.annotate(\"percentage of Taken Prod : {}%\".format(df['ProdTaken'].sum() \/ len(df['ProdTaken'])),\n              xy=(0, 0),xycoords='axes fraction', \n              xytext=(0,-50), textcoords='offset points',\n              va=\"top\", ha=\"left\", color='grey',\n              bbox=dict(boxstyle='round', fc=\"w\", ec='w'))\n\nplt.xlabel('Target', fontsize = 12, weight = 'bold')\nplt.show()","02ac4b53":"# Class count\ncount_class_0, count_class_1 = target['ProdTaken'].value_counts()\n\n# Divide by class\ndf_class_0 = df[target['ProdTaken'] == 'Not Taken']\ndf_class_1 = df[target['ProdTaken'] == 'Taken']\n\nprint(count_class_0)\nprint(count_class_1)","1ca02fda":"df_class_0_under = df_class_0.sample(count_class_1,random_state=42)\ndf_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_under['ProdTaken'].value_counts())\n\ndf_under['ProdTaken'].value_counts().plot(kind='bar', title='Count (target)');","f8e6cfbe":"trainset, testset = train_test_split(df_under, test_size=0.2, random_state=42)\nfig, ax = plt.subplots(1,2, figsize=(10, 5))\nsns.countplot(x = trainset['ProdTaken'] , data = trainset['ProdTaken'],ax=ax[0],palette=\"Set3\").set_title('TrainSet')\nsns.countplot(x = testset['ProdTaken'] , data = testset['ProdTaken'],ax=ax[1],palette=\"Set2\").set_title('TestSet')","4c634fed":"X_train = trainset.drop(['ProdTaken'],axis=1)\ny_train = trainset['ProdTaken']\nX_test = testset.drop(['ProdTaken'],axis=1)\ny_test = testset['ProdTaken']","a5464437":"preprocessor = make_pipeline(RobustScaler())\n\nPCAPipeline = make_pipeline(preprocessor, PCA(n_components=3,random_state=42))\n\nRandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=42))\nAdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=42))\nSVMPipeline = make_pipeline(preprocessor,SVC(random_state=42,probability=True))\nKNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier())\nLRPipeline = make_pipeline(preprocessor,LogisticRegression(solver='sag',random_state=42))","8c6bc98d":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X_train))\ny_train.reset_index(drop=True, inplace=True)\nPCA_df = pd.concat([PCA_df, y_train], axis=1, ignore_index=True )\nPCA_df.head()","e24909d6":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df[3],palette=sns.color_palette(\"tab10\", 2))\nplt.show()","f5f08fb5":"import plotly.express as px\nfigure1 = px.scatter_3d(PCA_df,\n        x=0, \n        y=1, \n        z=2, \n        color = 3,\n                       width=600, height=800)\nfigure1.update_traces(marker=dict(size=5,\n                              line=dict(width=0.2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\n\nfigure1.show()","76796e4e":"dict_of_models = {'RandomForest': RandomPipeline,\n'AdaBoost': AdaPipeline,\n'SVM': SVMPipeline,\n'KNN': KNNPipeline,\n'LR': LRPipeline}","4ffca080":"def evaluation(model):\n    # calculating the probabilities\n    y_pred_proba = model.predict_proba(X_test)\n\n    # finding the predicted valued\n    y_pred = np.argmax(y_pred_proba,axis=1)\n    print('Accuracy = ', accuracy_score(y_test, y_pred))\n    print('-')\n    print(confusion_matrix(y_test,y_pred))\n    print('-')\n    print(classification_report(y_test,y_pred))\n    print('-')\n    \n    N, train_score, test_score = learning_curve(model, X_train, y_train, \n                                               cv=4, scoring='f1', \n                                               train_sizes=np.linspace(0.1,1,10))\n    plt.figure(figsize=(5,5))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, test_score.mean(axis=1), label='validation score')\n    plt.legend()\n    plt.show()","2d022fbb":"sns.set(font_scale = 1)\nfor name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    model.fit(X_train,y_train)\n    evaluation(model)","288e0268":"RandomPipeline.fit(X_train, y_train)\nevaluation(RandomPipeline)","aa67fd68":"y_pred_prob = RandomPipeline.predict_proba(X_test)[:,1]\n\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\nplt.plot(fpr,tpr,label='RandomForest ROC Curve')\nplt.xlabel(\"False Survivor Rate\")\nplt.ylabel(\"True SurvivorR Rate\")\nplt.title(\"andomForest ROC Curve\")\nplt.show()","a7b6e7b1":"from sklearn.model_selection import RandomizedSearchCV\nRandomPipeline.get_params().keys()","d36c658b":"hyper_params = {\n    'randomforestclassifier__n_estimators':[10,100,150,250,400,600],\n    'randomforestclassifier__criterion':['gini','entropy'],\n    'randomforestclassifier__min_samples_split':[2,6,12],\n    'randomforestclassifier__min_samples_leaf':[1,4,6,10],\n    'randomforestclassifier__max_features':['auto','srqt','log2',int,float],\n    'randomforestclassifier__verbose':[0,1,2],\n    'randomforestclassifier__class_weight':['balanced','balanced_subsample'],\n    'randomforestclassifier__n_jobs':[-1],\n}","8a8b37c6":"RF_grid = RandomizedSearchCV(RandomPipeline,hyper_params,scoring='accuracy',n_iter=40)\nRF_grid.fit(X_train,y_train)","afa589dd":"print(RF_grid.best_params_)","29483c0a":"best_forest = (RF_grid.best_estimator_)\nbest_forest.fit(X_train,y_train)\n# calculating the probabilities\ny_pred_proba = best_forest.predict_proba(X_test)\n#Finding the predicted valued\ny_pred = np.argmax(y_pred_proba,axis=1)\n\nN, train_score, test_score = learning_curve(best_forest, X_train, y_train, \n                                           cv=4, scoring='f1', \n                                           train_sizes=np.linspace(0.1,1,10))","3ace8c74":"print('Accuracy = ', accuracy_score(y_test, y_pred))\nprint('-')\nprint(confusion_matrix(y_test,y_pred))\nprint('-')\nprint(classification_report(y_test,y_pred))\nprint('-')\n    \nplt.figure(figsize=(5,5))\nplt.plot(N, train_score.mean(axis=1), label='train score')\nplt.plot(N, test_score.mean(axis=1), label='validation score')\nplt.legend()\nplt.title('f1 score')\nplt.show()","0038545c":"err = []\n  \nfor i in range(1, 40):\n    \n    model = make_pipeline(preprocessor,KNeighborsClassifier(n_neighbors = i))\n    model.fit(X_train, y_train)\n    pred_i = model.predict(X_test)\n    err.append(np.mean(pred_i != y_test))\n  \nplt.figure(figsize =(10, 8))\nplt.plot(range(1, 40), err, color ='blue',\n                linestyle ='dashed', marker ='o',\n         markerfacecolor ='blue', markersize = 8)\n  \nplt.title('Mean Err = f(K)')\nplt.xlabel('K')\nplt.ylabel('Mean Err')","08d76cee":"KNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier(n_neighbors = 1))\nKNNPipeline.fit(X_train, y_train)","d8b26508":"evaluation(KNNPipeline)","a6912fa1":"import xgboost as xgb\ngbm = xgb.XGBClassifier(\n     learning_rate = 0.15,\n     n_estimators= 3000,\n     max_depth= 16,\n     min_child_weight= 2,\n     #gamma=1,\n     gamma=0.9,                        \n     subsample=0.8,\n     colsample_bytree=0.8,\n     objective= 'binary:logistic',\n     eval_metric = 'logloss',\n     nthread= -1,\n     scale_pos_weight=1).fit(X_train, y_train)\nevaluation (gbm)","57dff9e7":"SVMPipeline.fit(X_train, y_train)\nevaluation(SVMPipeline)","a6029f87":"y_pred_prob = SVMPipeline.predict_proba(X_test)[:,1]\n\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\nplt.plot(fpr,tpr,label='SVM ROC Curve')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"SVM ROC Curve\")\nplt.show()","614c6b7b":"best_classifier = RF_grid\n\nthresholds = [0.3,0.4,0.5,0.6,0.7,0.8]\nbest_t = 0.3\nbest_acc = 0\nfor t in thresholds:\n    y_pred = (best_classifier.predict_proba(X_test)[:,1] >= t).astype(int)\n    acc = accuracy_score(y_test, y_pred)\n    if acc > best_acc:\n        best_acc=acc\n        best_t=t","b783c189":"print('Accuracy on test set :',round(best_acc*100),\"%\")\nprint('Best threshold :',best_t)","daebfac6":"### Optimization","22af53ab":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png)","76cd38ba":"***\n# Best Estimator : RandomForest\n# Final accuracy on test set : 89% and could be improved\n# *Note* : XGBoost follows with 85% accuracy\n  \n***","87b8e7c5":"## Using KNN","acd03719":"<h1><center><font size=\"30\">Categorical Features<\/font><\/center><\/h1>","9291a34d":"<h1><center><font size=\"30\">Continuous Features<\/font><\/center><\/h1>","4b5af72a":"# Tuning Threshold","e85c5edc":"<h1><center>\u2600\ufe0fHolyday Packages Data Analysis\ud83d\udd0e<\/center><\/h1>\n<h3><center>\ud83c\udfc4(Prediction at the end)\ud83d\udd2e<\/center><\/h3>\n<center><img src= \"https:\/\/res.cloudinary.com\/lastminute-contenthub\/image\/upload\/c_limit,w_370,h_205\/v1559836493\/DAM\/Artwork\/DP\/Holidays%20deals%20themed\/370x205_all_inclusive_holidays.jpg\" alt =\"Holidays\" style='width: 600px;'><\/center>\n\n<h3>Overview<\/h3>\n\n\"Trips & Travel.Com\" company wants to enable and establish a viable business model to expand the customer base. One of the ways to expand the customer base is to introduce a new offering of packages. Currently, there are 5 types of packages the company is offering - Basic, Standard, Deluxe, Super Deluxe, King. Looking at the data of the last year, we observed that 18% of the customers purchased the packages. However, the marketing cost was quite high because customers were contacted at random without looking at the available information. The company is now planning to launch a new product i.e. Wellness Tourism Package. Wellness Tourism is defined as Travel that allows the traveler to maintain, enhance or kick-start a healthy lifestyle, and support or increase one's sense of well-being. However, this time company wants to harness the available data of existing and potential customers to make the marketing expenditure more efficient.\n\n<h3>Content<\/h3>\n\nWhat's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n\n- Most important features that have an impact on Product taken: Designation, Passport, Tier City, Martial status, occupation\n- Customers with Designation as Executive should be the target customers for the company .Customers who have passport and are from tier 3 city and are single or unmarried, have large business such customers have higher chances of taking new package.\n- Customers monthly income in range of 15000- 25000, and age range 15-30, prefer 5 star properties also have higher chances of taking new package based on EDA.\n\n<h3>Tasks to Solve :<\/h3>\n\n- To predict which customer is more likely to purchase the newly introduced travel package\n- Which variables are most significant.\n- Which segment of customers should be targeted more.","5291c801":"# Resampling\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).","739c034d":"## Using RandomForest","c50c3281":"## PCA Analysis","5b831d61":"<h1><center><font size=\"30\">Target Distribution<\/font><\/center><\/h1>","5732f7c5":"Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\nLet's implement a basic example, which uses the <code>DataFrame.sample<\/code> method to get random samples each class:","6b169786":"# Training models\n## Models overview","db3e70f1":"# A bit of data engineering\n## Let's answer in the same time the question \"What are the most significant features for taking a product?\"","d2e9aae8":"## Creating models","33ed6229":"## Using XGBoost","7711a9db":"# Analysing freatures and target","3d3029fa":"## Using SVM","322377de":"# If you like please upvote !\n## Also check my other notebooks :\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83d\udc01Mice Trisomy (100% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-mice-100-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83c\udf97\ufe0fBreast Cancer Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-breast-cancer-detection\n#### \ud83c\udf26\ud83c\udf21 Weather Forecasting \ud83d\udcc8 (98% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/weather-forecasting-98-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Heart Attack \ud83e\ude7a\ud83d\udc93 (90% Acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Mobile price (95.5% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-95-5-acc-mobile-price\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83e\udde0 Stroke (74% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-stroke-74-acc\n#### \u26a1\ud83d\udc32 Pokemon Stats \ud83e\udd4a\u2728 : https:\/\/www.kaggle.com\/dorianvoydie\/pokemon-stats\n#### \ud83d\udc1fFish Classification - Using CNN\ud83d\udd2e (97% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/fish-classification-using-cnn-97-acc\n#### \ud83d\udc89\ud83d\udc69\u200d\u2695\ufe0f Vaccine & COVID-19 Indicators\ud83d\udcc8 : https:\/\/www.kaggle.com\/dorianvoydie\/vaccine-covid-19-indicators\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\udda0\ud83c\udf6c Diabetes Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-diabetes-detection"}}