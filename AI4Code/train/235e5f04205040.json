{"cell_type":{"b472b936":"code","f12ca973":"code","0abcb8ce":"code","e6bf57fa":"code","7cf6e9df":"code","34429e98":"code","2806f939":"code","314eaf78":"code","c6fa2d69":"code","23a0eeb3":"code","84d2d675":"code","e416ba0e":"code","7c6d25ec":"code","3f20ccde":"markdown","20d390a9":"markdown","ef8f45c0":"markdown","ba37ca49":"markdown","a4fb9d96":"markdown","9e6726fc":"markdown","22079c69":"markdown","91c68667":"markdown","99abf7c6":"markdown","27a79b2d":"markdown","d2a61498":"markdown","c3679e33":"markdown","fa451be8":"markdown","883ccda3":"markdown"},"source":{"b472b936":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata","f12ca973":"validation = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\nhspeech = pd.read_csv('..\/input\/measuring-hate-speech\/measuring_hate_speech.csv')\nsubmission = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')","0abcb8ce":"# get mean scores for each comment_id\nscores_dict = hspeech.groupby('comment_id')['hate_speech_score'].apply(np.mean).to_dict()\n\n# drop duplicate comment_ids\nhspeech = hspeech.drop_duplicates(subset='comment_id')\nhspeech['hate_speech_score'] = hspeech['comment_id'].map(scores_dict)","e6bf57fa":"hspeech['hate_speech_score'].plot.hist(bins=100, title='Hate speech scores')","7cf6e9df":"hspeech['text'].apply(lambda x: len(x.split())).plot.hist(bins=100)\npd.concat([\n    validation['less_toxic'],\n    validation['more_toxic']\n]).to_frame('text')['text'].apply(lambda x: len(x.split())).plot.hist(bins=100, alpha=0.5, figsize=(14, 7), title='Text size') #blue validation - orange hspeech data","34429e98":"import re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef replaceURL(text):\n    \"\"\" Replaces url address with \"url\" \"\"\"\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','url',text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\ndef replaceAbbrev(text):\n    text = re.sub(r\"what's\", \"what is \",text)    \n    text = re.sub(r\"\\'ve\", \" have \",text)\n    text = re.sub(r\"can't\", \"cannot \",text)\n    text = re.sub(r\"n't\", \" not \",text)\n    text = re.sub(r\"i'm\", \"i am \",text)\n    text = re.sub(r\"\\'re\", \" are \",text)\n    text = re.sub(r\"\\'d\", \" would \",text)\n    text = re.sub(r\"\\'ll\", \" will \",text)\n    text = re.sub(r\"\\'scuse\", \" excuse \",text)\n    text = re.sub(r\"\\'s\", \" \",text)\n    return text\n\ndef removeUnicode(text):\n    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r' ', text)       \n    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n    return text\ndef removeRepeatPattern(text):\n    text=re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1',text)\n    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',text)\n    text=re.sub(r'[ ]{2,}',' ',text)\n    return text\n\ndef replaceAtUser(text):\n    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n    text = re.sub('@[^\\s]+','atUser',text)\n    return text\n\ndef replaceMultiToxicWords(text):\n    text = re.sub(r'(fuckfuck)','fuck fuck ',text)\n    text = re.sub(r'(f+)( *)([u|*|_]+)( *)([c|*|_]+)( *)(k)+','fuck',text)\n    text = re.sub(r'(h+)(a+)(h+)(a+)','ha ha ',text)\n    text = re.sub(r'(s+ *h+ *[i|!]+ *t+)','shit',text)\n    text = re.sub(r'\\b(n+)(i+)(g+)(a+)\\b','nigga',text)\n    text = re.sub(r'\\b(n+)([i|!]+)(g+)(e+)(r+)\\b','nigger',text)\n    text = re.sub(r'\\b(d+)(o+)(u+)(c+)(h+)(e+)( *)(b+)(a+)(g+)\\b','douchebag',text)\n    text = re.sub(r'([a|@][$|s][s|$])','ass',text)\n    text = re.sub(r'(\\bfuk\\b)','fuck',text)\n    return text\n\ndef removeNumbers(text):\n    \"\"\" Removes integers \"\"\"\n    text = re.sub(r\"(^|\\W)\\d+\", \" \", text)\n    text = re.sub(\"5\",\"s\",text)\n    text = re.sub(\"1\",\"i\",text)\n    text = re.sub(\"0\",\"o\",text)\n    return text\n                  \ndef replaceMultiPunc(text):\n    text=re.sub(r'([!])\\1\\1{2,}',r' mxm ',text)\n    text=re.sub(r'([?])\\1\\1{2,}',r' mqm ',text)\n    text=re.sub(r'([*])\\1\\1{2,}',r'*',text)\n    return text\n\n\nreplace_pun = {}\nseparators = set('\"%&\\'()+,-.\/:;<=>@[\\\\]^_`{|}~')\nfor punc in separators:\n    replace_pun[punc] = ' '\nreplace_pun['&']=' and '\n\ndef my_cleaner(s):\n    #s = s.lower()\n    s=replaceURL(s)\n    s=removeUnicode(s)\n    s=removeNumbers(s)\n    s=replaceAbbrev(s)\n    s=replaceMultiToxicWords(s)\n    s=replaceMultiPunc(s)\n    s=removeRepeatPattern(s)\n    \n    for punc in separators:\n        s= s.replace(punc,replace_pun[punc])                   # remove & replace punctuations\n    tokens = nltk.tokenize.word_tokenize(s)                    # split a string into words (tokens)\n    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n    return ' '.join(tokens)","2806f939":"hspeech['text_clean'] = hspeech['text'].apply(my_cleaner)\nvalidation['less_toxic'] = validation['less_toxic'].apply(my_cleaner)\nvalidation['more_toxic'] = validation['more_toxic'].apply(my_cleaner)\nsubmission['text'] = submission['text'].apply(my_cleaner)","314eaf78":"vec = TfidfVectorizer(lowercase=True, stop_words=['english'], analyzer='char_wb', ngram_range = (3,5))\nX = vec.fit_transform(hspeech['text_clean'])\nx_less_toxic =  vec.transform(validation['less_toxic'])\nx_more_toxic = vec.transform(validation['more_toxic'])\nx_test = vec.transform(submission['text'])","c6fa2d69":"model_1 = Ridge(alpha=0.5)\nmodel_1.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 1 validation accuracy score:  {(model_1.predict(x_less_toxic) < model_1.predict(x_more_toxic)).mean()}')\n\nmodel_2 = Ridge(alpha=1)\nmodel_2.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 2 validation accuracy score:  {(model_2.predict(x_less_toxic) < model_2.predict(x_more_toxic)).mean()}')\n\n\nmodel_3 = Ridge(alpha=2)\nmodel_3.fit(X, hspeech['hate_speech_score'])\nprint(f'Model 3 validation accuracy score: {(model_3.predict(x_less_toxic) < model_3.predict(x_more_toxic)).mean()}')","23a0eeb3":"p1 = model_1.predict(x_test)\np2 = model_2.predict(x_test)\np3 = model_3.predict(x_test)","84d2d675":"submission['score'] = (p1 + p2 + p3) \/ 3","e416ba0e":"import scipy\nfrom scipy.stats import rankdata\n\ntest = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ntest['score'] = rankdata(submission['score'].values, method='ordinal')\ntest[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","7c6d25ec":"test.sort_values('score', ascending=False)","3f20ccde":"# Plots","20d390a9":"# Text cleaning","ef8f45c0":"### Training an ensemble of three TF-IDF linear models using the dataset described in this [paper](https:\/\/arxiv.org\/pdf\/2009.10277.pdf)\nOriginal dataset link [here](https:\/\/huggingface.co\/datasets\/ucberkeley-dlab\/measuring-hate-speech)","ba37ca49":"# Build 3 Ridge Models and build an Ensemble","a4fb9d96":"Build three  ridges models with varying regularization parameters","9e6726fc":"## Plot Hate Speech Scores Histogram","22079c69":"# Vectorize the text with TF-IDF","91c68667":"Clean all texts (train data, validation, and submission)","99abf7c6":"The data is vectorized with analyzer 'char_wb'. Datasets such as toxic comments and unbias greatly benefit from this.","27a79b2d":"## Compare text size with competition validation data","d2a61498":"# Average the model scores and submit","c3679e33":"The text is already quite clean but some extra pre-processing is added:\n* Remove URL with 'url'\n* Remove unicode strings\n* Remove numbers\n* Lemmatization","fa451be8":"#trustyourcv!","883ccda3":"# Read data"}}