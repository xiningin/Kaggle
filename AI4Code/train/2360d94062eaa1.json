{"cell_type":{"64fad37a":"code","432ac6b8":"code","858ed9c7":"code","c5d7b00b":"code","a501a971":"code","997d9803":"code","4bac5c0d":"code","9f2f375b":"code","fbdc50e2":"code","1635a356":"code","050d20d6":"code","75a00d70":"code","6a2e0485":"code","b6cf27df":"code","e552c011":"code","9f0a47c1":"code","d668051e":"code","a9c71c20":"code","8e9d8974":"code","feaccf42":"code","21249bfb":"code","113b7301":"code","50aecdd9":"code","2538eb69":"code","4039a482":"code","276b8a01":"code","bf1bb4d8":"code","9c654334":"code","3751f406":"code","e0d913ff":"code","bf153dd8":"code","68a2ff56":"code","32f55dc4":"code","d4882e23":"code","63148583":"code","029fe913":"code","7722a56d":"code","cacad892":"code","158ebeee":"code","a8b0c95a":"code","62d52c44":"code","d3519cea":"code","f8777676":"code","f8f34724":"code","f7639419":"code","96c2ae70":"code","abf0547e":"code","982b25e7":"markdown","1ea14fbb":"markdown","622b5e6b":"markdown","1212efc6":"markdown","6ba9824a":"markdown","8b251ee9":"markdown","5cb2dee0":"markdown","b72623d4":"markdown","de500a3f":"markdown","edba3525":"markdown","80d8619a":"markdown","b71f2dee":"markdown","ff37d313":"markdown","97cca9a7":"markdown","98518af4":"markdown","5506193d":"markdown","ce48b9c1":"markdown","c2fe8b2e":"markdown","3dcb3099":"markdown","dead30bc":"markdown","541c5ce0":"markdown","124e4ed2":"markdown","5574fe3d":"markdown","ccdb248d":"markdown","5c9a6479":"markdown","530534fd":"markdown","7f5e3ecf":"markdown","8aeb4544":"markdown","08cfca72":"markdown","38fa2022":"markdown","76507691":"markdown","42dff50e":"markdown","1314370b":"markdown","c2db3be5":"markdown","aa21913d":"markdown","fa44b4c6":"markdown","5a85c184":"markdown","715ae060":"markdown"},"source":{"64fad37a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nsns.set(rc={'figure.figsize':(12,10)})","432ac6b8":"### Fetching data and getting info of data","858ed9c7":"data = pd.read_csv('https:\/\/raw.githubusercontent.com\/shrikantnarayankar15\/Insaid_term_deep_learning_Advertisement\/master\/advertisement_success.csv')","c5d7b00b":"data.head()","a501a971":"data.info()","997d9803":"### Finding the unique values in the dataset","4bac5c0d":"data.nunique()","9f2f375b":"### First Finding the null values","fbdc50e2":"data.isnull().sum()","1635a356":"sns.countplot(x='industry', hue='netgain', data=data)","050d20d6":"sns.countplot(x='genre', hue='netgain', data=data)","75a00d70":"sns.barplot(x='netgain', y='average_runtime(minutes_per_week)', data=data)","6a2e0485":"sns.countplot(x='airtime', hue='netgain', data=data)","b6cf27df":"data['airlocation'].value_counts().nlargest(10).plot(kind='bar')","e552c011":"data.airlocation.value_counts(normalize=True).mul(100)","9f0a47c1":"sns.countplot(x='expensive', hue='netgain', data=data)","d668051e":"data.groupby('netgain')['expensive'].value_counts(normalize=True).mul(100)","a9c71c20":"sns.countplot(x='money_back_guarantee', hue='netgain', data=data)","8e9d8974":"corr = data.corr()","feaccf42":"sns.heatmap(corr, annot=True)","21249bfb":"data.groupby('netgain')['ratings'].mean()","113b7301":"sns.violinplot(x='netgain', y='ratings',data=data, cut=5, width=0.1)","50aecdd9":"sns.boxplot(y='average_runtime(minutes_per_week)', x='targeted_sex', hue='netgain', data=data)","2538eb69":"sns.boxplot(y='average_runtime(minutes_per_week)', x='industry', data=data)","4039a482":"sns.boxplot(y='average_runtime(minutes_per_week)', x='airtime', data=data)","276b8a01":"sns.boxplot(y='average_runtime(minutes_per_week)', x='airtime', hue='netgain', data=data)","bf1bb4d8":"sns.boxplot(y='average_runtime(minutes_per_week)', x='expensive', data=data)","9c654334":"data.airlocation.value_counts(normalize=True).mul(100).plot(kind='bar')","3751f406":"data.airlocation.value_counts(normalize=True).mul(100)","e0d913ff":"# 90% of ads are from US we will treat other ads into others\nvalid_airlocations = ['United-States', 'Mexico']\ndata.airlocation = data.airlocation.apply(lambda x: x if x in valid_airlocations else 'Others')","bf153dd8":"data.head()","68a2ff56":"data['netgain'].value_counts()","32f55dc4":"category_cols = ['realtionship_status', 'industry', 'genre', 'targeted_sex', 'airtime', 'airlocation', 'expensive', 'money_back_guarantee']","d4882e23":"data = pd.get_dummies(data, columns=category_cols)","63148583":"# Encoding netgain to 0\/1\ndata['netgain'] = data['netgain'].apply(lambda x: 1 if x else 0) ","029fe913":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX = data.drop(['id', 'netgain'], axis=1)\ny = data['netgain']\n\n# Before passing data to neural network it must be standardized\nscaling = StandardScaler()\nscaling.fit(X)\nX = scaling.transform(X)\n\n#Startify since we have imbalanced dataset \nX_train, X_valid, y_train, y_valid = train_test_split(X, np.array(y), random_state=42, stratify=np.array(y))\n\nnum_input_nodes = X_train.shape[1]","7722a56d":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout,BatchNormalization\nfrom tensorflow.keras.optimizers import SGD\nimport tensorflow as tf\nfrom functools import partial\nfrom sklearn.metrics import accuracy_score","cacad892":"class NN_model():\n  def __init__(self, X, y):\n    self.X_train = X\n    self.y_train = y\n    self.model = Sequential()\n    self.model_info = None\n  \n  def create_nn(self, num_input_nodes, target_output_number, hidden_layers=[100], activation='relu', dropout=False, batch_nom=False):\n\n    #first layer\n    self.model.add(Dense(hidden_layers[0], input_dim = num_input_nodes, activation=activation))\n\n    for hidden_layer_size in hidden_layers[1:]:\n      self.model.add(Dense(hidden_layer_size, activation=activation))\n      if dropout:\n        self.model.add(Dropout(0.2))\n      if batch_nom:\n        self.model.add(BatchNormalization())\n\n    #decide the activation function on number of targets \n    if target_output_number > 1:\n      self.model.add(Dense(target_output_number, activation='softmax'))\n    else:\n      self.model.add(Dense(target_output_number, activation='sigmoid'))\n  \n  def train(self, optimzer = SGD, learning_rate=0.01, loss='mse', metrics=['accuracy'], val_split=0.2, epochs=10, class_weight=None):\n    optimizer = partial(optimzer, learning_rate=learning_rate)()\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n    self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    print(self.model.summary())\n    \n    start = time.time()\n    self.model_info = self.model.fit(X_train, y_train, batch_size=64, \\\n                       epochs=epochs, verbose=2, validation_split=0.2, callbacks=[callback], class_weight=class_weight)\n    end = time.time()\n    \n    print (\"Model took %0.2f seconds to train\"%(end - start))\n  \n  def validate(self, X_valid, y_valid):\n    prediction = self.model(X_valid)\n    prediction = [1 if i>=0.5 else 0 for i in prediction]\n    return accuracy_score(prediction, y_valid)\n","158ebeee":"# defining paramters for the tuning model\nfrom tensorflow.keras.optimizers import *\n\noptimizers = [Adadelta, SGD, Adam, RMSprop, Adagrad]\nlearning_rate = [0.1, 0.01]\nbatch_nom = [True, False]\nloss = ['binary_crossentropy', 'mae', 'mse']\nhidden_layers = [ [100,50], [100]]\noptimizers = [RMSprop]\nlearning_rate = [0.001]\nbatch_nom = [True]\nloss = ['binary_crossentropy']\nhidden_layers = [[100,50]]","a8b0c95a":"tf.config.list_physical_devices('GPU')","62d52c44":"#hyper parameter tuning the models\n\nAll_models = {}\nmodels = {}\ncounter = 1\nfor opt in optimizers:\n  for lr in learning_rate:\n    for batch_norm in batch_nom:\n      for los in loss:\n        for hid in hidden_layers:\n          with tf.device('device:GPU:0'):\n            model = NN_model(X_train, y_train)\n            model.create_nn(num_input_nodes, 1, hidden_layers=hid, batch_nom=batch_norm, dropout=False)\n            model.train(epochs=100,  loss=los)\n            All_models['model'+str(counter)] = [opt, lr, batch_norm, los, hid, model.validate(X_valid, y_valid)]\n            models['model'+str(counter)] = model\n            counter+=1","d3519cea":"w = pd.DataFrame(All_models).T.sort_values(by=5, ascending=False)\nw.columns = ['Optimizer', 'learning_rate', 'batch_norm', 'loss', 'hidden_layer', 'accuracy']\nw","f8777676":"# We got the parameters now\noptimizers = RMSprop\nlearning_rate = 0.001\nbatch_nom = True\nloss = 'binary_crossentropy'\nhidden_layers = [100,50]","f8f34724":"from sklearn.model_selection import StratifiedKFold\n\nsta = StratifiedKFold(n_splits=5)\npreds = []\nfor train, test in sta.split(X, y):\n  X_train, X_test = X[train], X[test]\n  y_train, y_test = y[train], y[test]\n  model = NN_model(X_train, y_train)\n  model.create_nn(num_input_nodes, 1, hidden_layers=hidden_layers, batch_nom=batch_norm, dropout=False)\n  model.train(optimzer = optimizers, learning_rate=learning_rate, epochs=100,  loss=loss)\n  preds.append([1 if i>=0.5 else 0 for i in model.model.predict(X_valid)])","f7639419":"# Combining all the predictions of each Stratify and and perform blending i.e taking mean\nblending_prediction = pd.DataFrame(preds).T.mode(axis=1)","96c2ae70":"# Accuracy Score increased after blending\naccuracy_score(y_valid, blending_prediction)","abf0547e":"from sklearn.metrics import plot_confusion_matrix, confusion_matrix\ncf_matrix = confusion_matrix(y_valid, blending_prediction)\nsns.heatmap(cf_matrix, annot=True)","982b25e7":"* No null values in the dataset","1ea14fbb":"### 2.3. Check distribution of data","622b5e6b":"3. does average_runtime increases the netgain?","1212efc6":"* average running time per industry","6ba9824a":"* Comedy\u00a0genre\u00a0have\u00a0higher\u00a0netgain,\u00a0so\u00a0if\u00a0we\u00a0make\u00a0advertisement\u00a0more\u00a0in\u00a0comedy\u00a0genre\u00a0then\u00a0netgain\u00a0will\u00a0increase\u00a0for\u00a0sure.\n* Higher running time of advertisement higher the netgain\n* I used batch_normalization which increased the accuracy across model.\n* Used various optimizers to tune the model, \n* Used early stopping to stop overfitting\n* Performed blending operation by taking splitting data into 5 parts using stratify and taking mean \n","8b251ee9":"### 4.3  Hyper Parameter Tuning","5cb2dee0":"### 4.1 Dividing training test data","b72623d4":"## 3.Preprocessing of data after EDA","de500a3f":"## 1.Importing Packages","edba3525":"* We have around 26049 records","80d8619a":"### 2.2. Correlation matrix of data","b71f2dee":"2. which genre increases netgain","ff37d313":"\n\n*   Most of the ads have genre as Comedy, so its obvious that the netgain will be higher for this genre\n\n","97cca9a7":"### 4.4 Handling Imbalnced Dataset using Stratified CV Increasing Accuracy by Blending","98518af4":"1. which industry made most gain\n2. which genre increases netgain\n3. does average_runtime increases the netgain?\n4. airtime affects netgain?\n5. distribution of netgain across airlocation\n6. does expensive add will affect netgain?\n7. does money_back_guarantee affect netgain?","5506193d":"## 5.Conclusion","ce48b9c1":"## 4.**Modelling (Will startify Since the Dataset imbalanced)**","c2fe8b2e":"* if the rating is higher netgain will be high\n* if the runtime is high then it may have high rating","3dcb3099":"## 2.EDA on the Dataset","dead30bc":"* males like more adds where the average time 40 minutes(average run time per week)\n* females like adds them in same range 38-42 minutes (average run time per week)","541c5ce0":"7. does money_back_guarantee affect netgain?","124e4ed2":"4. airtime affects netgain?","5574fe3d":"\n ## **81.67% Accuracy which is great boost over 81.15% accuracy(without blending)**","ccdb248d":"### 2.1. Question which we should be asking after looking at below data","5c9a6479":"\n\n\n*   Pharma industry have higher netgain\n*   ClassAction, Political, Entertainment low netgain\n\n","530534fd":"5. distribution of netgain across airlocation","7f5e3ecf":"* the ads which are showing are showing low expense are the one where ppl are more attracted and profitable. (60 %)","8aeb4544":"6. distribution of ads with expensive","08cfca72":"1. which industry made most gain","38fa2022":"* primetime shows have higher average runtime\n* the ads which got netgain have higher running time","76507691":"# Advertisement Success Dataset\n\n**Objective**:\n* This dataset is intended for binary classification tasks.\n* Use this data set to train a model able to classify whether an ad will be profitable or not.","42dff50e":"* all expense type of ads have same distribution of average runtime","1314370b":"* As you can see the ads which are shown on primtime have more netgain","c2db3be5":"* since we are going to use the Neural Network, categorical variables must be turned into the one hot representation.","aa21913d":"### 4.2 Creation of custom ANN Model","fa44b4c6":"* not that much of difference its making","5a85c184":"*  little more increase advertisement timing can netgain\n","715ae060":"* United States have higher no. ads shown (90%)"}}