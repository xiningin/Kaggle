{"cell_type":{"553bf6a4":"code","76c3848b":"code","5f96ed7c":"code","772d6315":"code","12f801f2":"code","7a51ceaf":"code","6f491f9d":"code","5bed6c1a":"code","72d89fd5":"code","0ad291ac":"code","c84ae2e3":"code","fca50b64":"code","246a0ba0":"code","0b63ee5c":"code","ffed3dff":"code","a0a2279c":"code","f24ea04e":"code","298669e3":"code","e08f8b60":"code","99ff97d6":"code","497f4dee":"code","8406455e":"code","da986e56":"code","e0fa3a0c":"code","150ac1b1":"code","4651af17":"code","a26109c1":"code","90a0fe9f":"code","ea306ca5":"code","e5839bb8":"code","308491ec":"code","cae054da":"code","c28ba561":"code","9cb6e281":"code","cb975d0d":"code","0936de34":"code","2630c42a":"code","7b8ad100":"code","0eea2196":"code","5d4f8d7e":"code","7d8f7694":"code","194c4e90":"code","8b3cd7de":"code","33f8b93b":"code","bbfeec55":"code","23cd3f2f":"code","25e5663c":"code","c3808864":"markdown","ab43ba8f":"markdown","ae8ec222":"markdown","1e980deb":"markdown","d81d4199":"markdown","57f7eeb9":"markdown","393df07c":"markdown","dd3a4180":"markdown","69a62fe2":"markdown","9872e2b3":"markdown","c6f85874":"markdown","650d7e77":"markdown","14d6882b":"markdown","d2b31315":"markdown","48754ea8":"markdown","96572b9f":"markdown","a8f4f569":"markdown","bb9fcdaf":"markdown","73b16ffc":"markdown","da482a20":"markdown","685a7d11":"markdown","0779cf71":"markdown","dd121426":"markdown","e586deb1":"markdown","d0ebcd16":"markdown","03d8cab0":"markdown","d6e4139e":"markdown","3df25726":"markdown","ba17a65d":"markdown"},"source":{"553bf6a4":"import os\nimport spacy\nimport wordcloud\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom pathlib import Path\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib\n\nimport re\nimport nltk\nimport spacy\nimport random\nimport string\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom spacy.util import minibatch\nfrom nltk.corpus import stopwords\nfrom spacy.util import compounding\nimport plotly.figure_factory as ff\nfrom plotly import graph_objs as go\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport warnings; warnings.filterwarnings(\"ignore\")","76c3848b":"train_dir = \"..\/input\/feedback-prize-2021\/train\/\"\ntest_dir = \"..\/input\/feedback-prize-2021\/test\/\"\ntrain_files = os.listdir(train_dir)\ntest_files = os.listdir(test_dir)\n\nfor file in range(len(train_files)):\n    train_files[file] = str(train_dir) + \"\/\" +  str(train_files[file])\nfor file in range(len(test_files)):\n    test_files[file] = str(test_dir) + \"\/\" +  str(test_files[file])\n    \n\n\ntrain = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\nsub = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')","5f96ed7c":"print(len(train))","772d6315":"\ntrain=train[:10000]\ntrain_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n    train_names.append(f.replace('.txt', ''))\n    train_texts.append(open('..\/input\/feedback-prize-2021\/train\/' + f, 'r').read())\ntrain_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\ntrain_texts\ntrain = pd.merge(train, train_texts, how = 'left', on = 'id')\ntrain\ntest_names, test_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/test'))):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('..\/input\/feedback-prize-2021\/test\/' + f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\ntest_texts\ntest = pd.merge(sub, test_texts, how = 'left', on = 'id')\ntest","12f801f2":"train.describe()","7a51ceaf":"def print_text(text_id):\n    with open(f'\/kaggle\/input\/feedback-prize-2021\/train\/{text_id}.txt') as f:\n        lines = f.readlines()\n    print(''.join(lines))\n    \nprint_text('423A1CA112E2')","6f491f9d":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')","5bed6c1a":"\ntexts = []\n\nids = train['id'].unique().tolist()\nlen(ids)\n\nfor example in ids:\n    with open(train_dir+f'{example}.txt', 'r') as file: data = file.read()\n    texts.append({\n        'text': data,\n        'n_tokens': len(tokenizer(data)['input_ids'])\n    })\n     \ndf = pd.DataFrame(texts)","72d89fd5":"print(df.describe())","0ad291ac":"d_type=pd.DataFrame(train[\"discourse_type\"].value_counts())\nfig = px.bar(x =d_type.index,\ny = d_type[\"discourse_type\"] , color = np.unique(train[\"discourse_type\"]),color_continuous_scale=\"Emrld\")\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,title = {\n        'text': 'Discourse Type Distribution ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","c84ae2e3":"sub = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')\nsub.head()","fca50b64":"path = Path('..\/input\/feedback-prize-2021\/train')\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000'\n         }\n\ndef visualize(example):\n    ents = []\n    for i, row in train[train['id'] == example].iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    with open(path\/f'{example}.txt', 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n        \"title\": example\n    }\n\n    options = {\"ents\": train.discourse_type.unique().tolist(), \"colors\": colors}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)\n\nexamples = train['id'].sample(n=5, random_state=42).values.tolist()\n\nfor ex in examples:\n    visualize(ex)\n    print('\\n')","246a0ba0":"first_type=[train[train['id']==id]['discourse_type'].iloc[0] for id in train['id'].unique()]\nd_type=pd.DataFrame(first_type).value_counts()\nfig = px.bar(x =d_type.index.get_level_values(0),\ny = d_type, color = d_type.index.get_level_values(0),color_continuous_scale=\"Emrld\")\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,title = {\n        'text': 'Which types are often selected in the first section?',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","0b63ee5c":"last_type=[train[train['id']==id]['discourse_type'].iloc[-1] for id in train['id'].unique()]\nd_type=pd.DataFrame(last_type).value_counts()\nfig = px.bar(x =d_type.index.get_level_values(0),\ny = d_type, color = d_type.index.get_level_values(0),color_continuous_scale=\"Emrld\")\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,title = {\n        'text': 'What types are often selected in the last section?',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","ffed3dff":"num=[]\npattern=[]\nfor i in range(len(train['id'].unique())):\n        id=train['id'].unique()[i]\n        data_a=train[train['id']==id]\n        num.append(len(data_a))\n        a=''\n        for n in range(len(data_a)):\n            a=a+data_a['discourse_type'].iloc[n]+'_'\n        pattern.append(a)\npatterns=pd.DataFrame(pattern,columns={'discourse_types_pattern'})","a0a2279c":"patterns_=pd.DataFrame(patterns.value_counts(),columns={'count'})\npatterns_[:10]","f24ea04e":"plt.barh(patterns_.index.get_level_values(0)[:10],patterns_['count'][:10].values)","298669e3":"train['selected_text'] = train['discourse_text']","e08f8b60":"train['Num_words_ST'] = train['discourse_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text","99ff97d6":"for i in train['discourse_type'].unique():    \n    plt.figure(figsize=(12,6))\n    sns.distplot(train[train['discourse_type']==i]['difference_in_words'],kde=False)\n    plt.show()","497f4dee":"# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING\/LOADING MODEL WEIGHTS\n# THIS SHOULD MATCH THE MODEL IN LOAD_MODEL_FROM\nVER=14 \n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\nLOAD_TOKENS_FROM = '..\/input\/tf-longformer-v12'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\nLOAD_MODEL_FROM = '..\/input\/tflongformerv141'\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = '..\/input\/tf-longformer-v12'\n\n\nif DOWNLOADED_MODEL_PATH is None:\n    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'allenai\/longformer-base-4096'","8406455e":"if DOWNLOADED_MODEL_PATH == 'model':\n    os.mkdir('model')\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained('model')\n\n    config = AutoConfig.from_pretrained(MODEL_NAME) \n    config.save_pretrained('model')\n\n    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n    backbone.save_pretrained('model')","da986e56":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom transformers import *\nprint('TF version',tf.__version__)","e0fa3a0c":"#USE MULTIPLE GPUS\nif os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n    strategy = tf.distribute.get_strategy()\n    print('single strategy')\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n    print('multiple strategy')","150ac1b1":"tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('Mixed precision enabled')","4651af17":"IDS = train.id.unique()\nMAX_LEN = 1024\n\n# THE TOKENS AND ATTENTION ARRAYS\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\ntrain_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\ntrain_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n\n# THE 14 CLASSES FOR NER\nlead_b = np.zeros((len(IDS),MAX_LEN))\nlead_i = np.zeros((len(IDS),MAX_LEN))\n\nposition_b = np.zeros((len(IDS),MAX_LEN))\nposition_i = np.zeros((len(IDS),MAX_LEN))\n\nevidence_b = np.zeros((len(IDS),MAX_LEN))\nevidence_i = np.zeros((len(IDS),MAX_LEN))\n\nclaim_b = np.zeros((len(IDS),MAX_LEN))\nclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nconclusion_b = np.zeros((len(IDS),MAX_LEN))\nconclusion_i = np.zeros((len(IDS),MAX_LEN))\n\ncounterclaim_b = np.zeros((len(IDS),MAX_LEN))\ncounterclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nrebuttal_b = np.zeros((len(IDS),MAX_LEN))\nrebuttal_i = np.zeros((len(IDS),MAX_LEN))\n\n# HELPER VARIABLES\ntrain_lens = []\ntargets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\ntargets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\ntarget_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n             'Counterclaim':5, 'Rebuttal':6}","a26109c1":"# WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\nassert( np.sum(train.groupby('id')['discourse_start'].diff()<=0)==0 )\n\n# FOR LOOP THROUGH EACH TRAIN TEXT\nfor id_num in range(len(IDS)):\n    if LOAD_TOKENS_FROM: break\n    if id_num%100==0: print(id_num,', ',end='')\n        \n    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n    n = IDS[id_num]\n    name = f'..\/input\/feedback-prize-2021\/train\/{n}.txt'\n    txt = open(name, 'r').read()\n    train_lens.append( len(txt.split()))\n    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n    train_tokens[id_num,] = tokens['input_ids']\n    train_attention[id_num,] = tokens['attention_mask']\n    \n    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n    offsets = tokens['offset_mapping']\n    offset_index = 0\n    df = train.loc[train.id==n]\n    for index,row in df.iterrows():\n        a = row.discourse_start\n        b = row.discourse_end\n        if offset_index>len(offsets)-1:\n            break\n        c = offsets[offset_index][0]\n        d = offsets[offset_index][1]\n        beginning = True\n        while b>c:\n            if (c>=a)&(b>=d):\n                k = target_map[row.discourse_type]\n                if beginning:\n                    targets_b[k][id_num][offset_index] = 1\n                    beginning = False\n                else:\n                    targets_i[k][id_num][offset_index] = 1\n            offset_index += 1\n            if offset_index>len(offsets)-1:\n                break\n            c = offsets[offset_index][0]\n            d = offsets[offset_index][1]","90a0fe9f":"if LOAD_TOKENS_FROM is None:\n    targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n    for k in range(7):\n        targets[:,:,2*k] = targets_b[k]\n        targets[:,:,2*k+1] = targets_i[k]\n    targets[:,:,14] = 1-np.max(targets,axis=-1)","ea306ca5":"if LOAD_TOKENS_FROM is None:\n    np.save(f'targets_{MAX_LEN}', targets)\n    np.save(f'tokens_{MAX_LEN}', train_tokens)\n    np.save(f'attention_{MAX_LEN}', train_attention)\n    print('Saved NER tokens')\nelse:\n    targets = np.load(f'{LOAD_TOKENS_FROM}\/targets_{MAX_LEN}.npy')\n    train_tokens = np.load(f'{LOAD_TOKENS_FROM}\/tokens_{MAX_LEN}.npy')\n    train_attention = np.load(f'{LOAD_TOKENS_FROM}\/attention_{MAX_LEN}.npy')\n    print('Loaded NER tokens')","e5839bb8":"def build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n    \n    config = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'\/config.json') \n    backbone = TFAutoModel.from_pretrained(DOWNLOADED_MODEL_PATH+'\/tf_model.h5', config=config)\n    \n    x = backbone(tokens, attention_mask=attention)\n    x = tf.keras.layers.Dense(256, activation='relu')(x[0])\n    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-4),\n                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n    \n    return model","308491ec":"with strategy.scope():\n    model = build_model()","cae054da":"# LEARNING RATE SCHEDULE AND MODEL CHECKPOINT\nEPOCHS = 5\nBATCH_SIZE = 4 \nLRS = [0.25e-4, 0.25e-4, 0.25e-4, 0.25e-4, 0.25e-5] \ndef lrfn(epoch):\n    return LRS[epoch]\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","c28ba561":"# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)\nprint('Train size',len(train_idx),', Valid size',len(valid_idx))","9cb6e281":"# LOAD MODEL\nif LOAD_MODEL_FROM:\n    model.load_weights(f'{LOAD_MODEL_FROM}\/long_v{VER}.h5')\n    \n# OR TRAIN MODEL\nelse:\n    model.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n          y = targets[train_idx,],\n          validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n                             targets[valid_idx,]),\n          callbacks = [lr_callback],\n          epochs = EPOCHS,\n          batch_size = BATCH_SIZE,\n          verbose = 2)\n\n    # SAVE MODEL WEIGHTS\n    model.save_weights(f'long_v{VER}.h5')","cb975d0d":"p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n                  batch_size=16, verbose=2)\nprint('OOF predictions shape:',p.shape)\noof_preds = np.argmax(p,axis=-1)","0936de34":"target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}","2630c42a":"def get_preds(dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds):\n    all_predictions = []\n\n    for id_num in range(len(preds)):\n    \n        # GET ID\n        if (id_num%100==0)&(verbose): \n            print(id_num,', ',end='')\n        n = text_ids[id_num]\n    \n        # GET TOKEN POSITIONS IN CHARS\n        name = f'..\/input\/feedback-prize-2021\/{dataset}\/{n}.txt'\n        txt = open(name, 'r').read()\n        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n        off = tokens['offset_mapping']\n    \n        # GET WORD POSITIONS IN CHARS\n        w = []\n        blank = True\n        for i in range(len(txt)):\n            if (txt[i]!=' ')&(txt[i]!='\\n')&(txt[i]!='\\xa0')&(txt[i]!='\\x85')&(blank==True):\n                w.append(i)\n                blank=False\n            elif (txt[i]==' ')|(txt[i]=='\\n')|(txt[i]=='\\xa0')|(txt[i]=='\\x85'):\n                blank=True\n        w.append(1e6)\n            \n        # MAPPING FROM TOKENS TO WORDS\n        word_map = -1 * np.ones(MAX_LEN,dtype='int32')\n        w_i = 0\n        for i in range(len(off)):\n            if off[i][1]==0: continue\n            while off[i][0]>=w[w_i+1]: w_i += 1\n            word_map[i] = int(w_i)\n        \n        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n        ### KEY: ###\n        # 0: LEAD_B, 1: LEAD_I\n        # 2: POSITION_B, 3: POSITION_I\n        # 4: EVIDENCE_B, 5: EVIDENCE_I\n        # 6: CLAIM_B, 7: CLAIM_I\n        # 8: CONCLUSION_B, 9: CONCLUSION_I\n        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n        # 12: REBUTTAL_B, 13: REBUTTAL_I\n        # 14: NOTHING i.e. O\n        ### NOTE THESE VALUES ARE DIVIDED BY 2 IN NEXT CODE LINE\n        pred = preds[id_num,]\/2.0\n    \n        i = 0\n        while i<MAX_LEN:\n            prediction = []\n            start = pred[i]\n            if start in [0,1,2,3,4,5,6,7]:\n                prediction.append(word_map[i])\n                i += 1\n                if i>=MAX_LEN: break\n                while pred[i]==start+0.5:\n                    if not word_map[i] in prediction:\n                        prediction.append(word_map[i])\n                    i += 1\n                    if i>=MAX_LEN: break\n            else:\n                i += 1\n            prediction = [x for x in prediction if x!=-1]\n            if len(prediction)>4:\n                all_predictions.append( (n, target_map_rev[int(start)], \n                                ' '.join([str(x) for x in prediction]) ) )\n                \n    # MAKE DATAFRAME\n    df = pd.DataFrame(all_predictions)\n    df.columns = ['id','class','predictionstring']\n    \n    return df","7b8ad100":"oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx] )\noof.head()","0eea2196":"print('The following classes are present in oof preds:')\noof['class'].unique()","5d4f8d7e":"# CODE FROM : Rob Mulla @robikscube\n# https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter \/ len_gt\n    overlap_2 = inter\/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP \/ (TP + 0.5*(FP+FN))\n    return my_f1_score","7d8f7694":"# VALID DATAFRAME\nvalid = train.loc[train['id'].isin(IDS[valid_idx])]","194c4e90":"f1s = []\nCLASSES = oof['class'].unique()\nfor c in CLASSES:\n    pred_df = oof.loc[oof['class']==c].copy()\n    gt_df = valid.loc[valid['discourse_type']==c].copy()\n    f1 = score_feedback_comp(pred_df, gt_df)\n    print(c,f1)\n    f1s.append(f1)\nprint()\nprint('Overall',np.mean(f1s))","8b3cd7de":"# GET TEST TEXT IDS\nfiles = os.listdir('..\/input\/feedback-prize-2021\/test')\nTEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\nprint('There are',len(TEST_IDS),'test texts.')","33f8b93b":"# CONVERT TEST TEXT TO TOKENS\ntest_tokens = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\ntest_attention = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n\nfor id_num in range(len(TEST_IDS)):\n        \n    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n    n = TEST_IDS[id_num]\n    name = f'..\/input\/feedback-prize-2021\/test\/{n}.txt'\n    txt = open(name, 'r').read()\n    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n    test_tokens[id_num,] = tokens['input_ids']\n    test_attention[id_num,] = tokens['attention_mask']","bbfeec55":"# INFER TEST TEXTS\np = model.predict([test_tokens, test_attention], \n                  batch_size=16, verbose=2)\nprint('Test predictions shape:',p.shape)\ntest_preds = np.argmax(p,axis=-1)","23cd3f2f":"# GET TEST PREDICIONS\nsub = get_preds( dataset='test', verbose=False, text_ids=TEST_IDS, preds=test_preds )\nsub.head()","25e5663c":"# WRITE SUBMISSION CSV\nsub.to_csv('submission.csv',index=False)","c3808864":"\n\nIn creating this notebook, I referred to the following great notebooks.\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u3001\u4e0b\u8a18\u306e\u7d20\u6674\u3089\u3057\u3044\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\n\n[\ud83d\udcd6Feedback- Baseline\ud83e\udd17 Sentence Classifier [0.226]](https:\/\/www.kaggle.com\/julian3833\/feedback-baseline-sentence-classifier-0-226)\n\n[\ud83d\udd25\ud83d\udcca Feedback Prize - EDA \ud83d\udcca\ud83d\udd25](https:\/\/www.kaggle.com\/odins0n\/feedback-prize-eda)\n\n[\ud83c\udf93 Evaluating Student Writing - EDA](https:\/\/www.kaggle.com\/yamqwe\/evaluating-student-writing-eda))\n\n[[Feedback prize] Simple EDA](https:\/\/www.kaggle.com\/ilialar\/feedback-prize-simple-eda)\n","ab43ba8f":"You can see that the types classified at the end of the essay are overwhelmingly Concluding Statements.\n\n\u30a8\u30c3\u30bb\u30a4\u306e\u6700\u5f8c\u306b\u5206\u985e\u3055\u308c\u308b\u30bf\u30a4\u30d7\u306f\u3001Concluding\u3000Statement\uff08\u7d50\u8ad6\uff09\u304c\u5727\u5012\u7684\u306b\u591a\u3044\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u3002","ae8ec222":"This competition is a student's writing\u3000to classify into 7 categories and submit the results.\n\n* Lead\n* Position\n* Evidence\n* Claim\n* Concluding Statement\n* Counterclaim\n* Rebuttal\n\n\n\n\u3053\u306e\u30b3\u30f3\u30da\u30c6\u30a3\u30b7\u30e7\u30f3\u306f\u3001\u5b66\u751f\u306e\u6587\u7ae0\u30927\u3064\u306b\u5206\u985e\u3057\u3066\u7d50\u679c\u3092\u63d0\u51fa\u3059\u308b\u3068\u3044\u3046\u5185\u5bb9\u3067\u3059\u3002\n\n* Lead(\u5c0e\u5165)\n* Position\uff08\u30dd\u30b8\u30b7\u30e7\u30f3\uff09\n* Evidence\uff08\u8a3c\u62e0\uff09\n* Claim\uff08\u8a34\u3048\uff09\n* Concluding Statement\uff08\u7d50\u8ad6\uff09\n* Counterclaim\uff08\u53cd\u8a34\uff09\n* Rebuttal\uff08\u53cd\u8ad6\uff09\n\n\n\n","1e980deb":"## What types are often selected in the first section?","d81d4199":"# Submission data (Target)","57f7eeb9":"## Which types are often selected in the last section?\n","393df07c":"# About Writing Pattern","dd3a4180":"\u66f4\u306b\u3001discourse_type\u3054\u3068\u306b\u5358\u8a9e\u6570\u3092\u898b\u3066\u898b\u307e\u3059\u3002(\u629c\u7c8b:[\ud83c\udf93 Evaluating Student Writing - EDA](https:\/\/www.kaggle.com\/yamqwe\/evaluating-student-writing-eda))","69a62fe2":"![image-14-1024x262.png](attachment:81300980-1b94-45ea-9676-ff851c13d84e.png)","9872e2b3":"## How many words are included for each text?","c6f85874":"## How many words are included in each type of text","650d7e77":"# LongFormer\/The Long-Document Transformer","14d6882b":"train.csv contains 10 rows with annotation for this file. Each row corresponds to one discourse element and contains the following information (source: https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/data):\n\n* id - ID code for essay response\n* discourse_id - ID code for discourse element\n* discourse_start - character position where discourse element begins in the essay response\n* discourse_end - character position where discourse element ends in the essay response\n* discourse_text - text of discourse element\n* discourse_type - classification of discourse element\n* discourse_type_num - enumerated class label of discourse element\n* predictionstring - the word indices of the training sample, as required for predictions\nAs you can see we have both symbol and words level annotation (we can use any of them in model but need the word level one for submission).","d2b31315":"train.csv\u306b\u306f\u300110\u500b\u306e\u8981\u7d20\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002 \u53c2\u8003\uff1ahttps\uff1a\/\/www.kaggle.com\/c\/feedback-prize-2021\/data\uff09\u3002\n\n* id-\u30a8\u30c3\u30bb\u30a4\u306eID\u30b3\u30fc\u30c9\uff08train\u30d5\u30a1\u30a4\u30eb\u306e\u30a8\u30c3\u30bb\u30a4\u540d\u3068\u540c\u3058\uff09\n* discourse_id-\u8981\u7d20\u306eID\u30b3\u30fc\u30c9\n* discourse_start-\u6587\u7ae0\u306e\u4e2d\u3067\u8ac7\u8a71\u8981\u7d20\u304c\u59cb\u307e\u308b\u6587\u5b57\u4f4d\u7f6e\n* discourse_end-\u6587\u7ae0\u306e\u4e2d\u3067\u8ac7\u8a71\u8981\u7d20\u304c\u7d42\u4e86\u3059\u308b\u6587\u5b57\u4f4d\u7f6e\n* discourse_text-\u8ac7\u8a71\u8981\u7d20\u306e\u30c6\u30ad\u30b9\u30c8\n* discourse_type-\u8ac7\u8a71\u8981\u7d20\u306e\u5206\u985e\uff08Target\uff09\n* discourse_type_num-\u8ac7\u8a71\u8981\u7d20\u306e\u5217\u6319\u3055\u308c\u305f\u30af\u30e9\u30b9\u30e9\u30d9\u30eb\n* predictionstring-\u4e88\u6e2c\u306b\u5fc5\u8981\u306a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b5\u30f3\u30d7\u30eb\u306e\u5358\u8a9e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n\n","48754ea8":"# Feedback Prize Competision EDA (JP\/EN)","96572b9f":"You can see that Lead is the most common type at the beginning of the essay.\n\n\u30a8\u30c3\u30bb\u30a4\u306e1\u756a\u521d\u3081\u306b\u5206\u985e\u3055\u308c\u308b\u306e\u306f\u3001Lead\uff08\u5c0e\u5165\uff09\u304c\u6700\u3082\u591a\u3044\u3068\u3044\u3046\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u3002","a8f4f569":"Let's see which type is most selected in the first discourse_type of each text. \n\n\n\u5404\u30c6\u30ad\u30b9\u30c8\u306e\u6700\u521d\u306e\u30af\u30e9\u30b9\uff08discourse_type\uff09\u3067\u6700\u3082\u9078\u629e\u3055\u308c\u3066\u3044\u308b\u30bf\u30a4\u30d7\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002","bb9fcdaf":"This notebook contains a EDA to submission for the Feedback Prize - Evaluating Student Writing competition in English\/Japanese.\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u3001\n\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u8cde-\u5b66\u751f\u306e\u30e9\u30a4\u30c6\u30a3\u30f3\u30b0\u30b3\u30f3\u30c6\u30b9\u30c8\u306e\u30b3\u30f3\u30da\u30c6\u30a3\u30b7\u30e7\u30f3EDA\uff08\u65e5\u672c\u8a9e\u30d0\u30fc\u30b8\u30e7\u30f3\uff09\u3067\u3059\u3002\n","73b16ffc":"# # Train data","da482a20":"![header.jpg](attachment:c2804f4a-1e7d-4c2e-beb4-8085f3425384.jpg)","685a7d11":"Let's take a look at the patterns and the number of types with many sentence structures.\n\n\u305d\u308c\u3067\u306f\u6587\u7ae0\u69cb\u6210\u306e\u591a\u3044\u30d1\u30bf\u30fc\u30f3\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046","0779cf71":"Longformer is a model that responds to the sudden increase in memory consumption  for long sentences by Transformer by devising a self-attention method.\n\nThis means you can reduce memory consumption, speed up calculations, and use Transformer-based models for long sentences.\n\n\n\nIt also has the same pre-learning-fine tuning as BERT and RoBERTa, achieving accuracy that surpasses BERT and RoBERTa for long sentence tasks.\n\n\nLongformer devises the method of Scaled Dot-Production attention.\n\nSimply put, Scaled Dot-Product attention focuses attention from every word to every word, which leads to memory consumption in \ud835\udc5b2, but from important word to important word. Only pay attention to it.\n\n\n\nLongformer\u306f\u3001Transformer\u306b\u3088\u308b\u9577\u3044\u6587\u306e\u30e1\u30e2\u30ea\u6d88\u8cbb\u91cf\u306e\u6025\u6fc0\u306a\u5897\u52a0\u306b\u3001self-attention\u306e\u65b9\u6cd5\u3092\u8003\u6848\u3059\u308b\u3053\u3068\u3067\u5bfe\u5fdc\u3059\u308b\u30e2\u30c7\u30eb\u3067\u3059\u3002\n\n\u3064\u307e\u308a\u3001\u30e1\u30e2\u30ea\u6d88\u8cbb\u91cf\u3092\u524a\u6e1b\u3057\u3001\u8a08\u7b97\u3092\u9ad8\u901f\u5316\u3057\u3066\u3001\u9577\u3044\u6587\u7ae0\u306b\u5bfe\u3057\u3066\u3082Transformer\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n\n\u307e\u305f\u3001BERT\u3084RoBERTa\u3068\u540c\u3058\u4e8b\u524d\u5b66\u7fd2\u306e\u5fae\u8abf\u6574\u304c\u53ef\u80fd\u3067\u3001\u9577\u3044\u6587\u306e\u30bf\u30b9\u30af\u3067BERT\u3084RoBERTa\u3092\u8d85\u3048\u308b\u7cbe\u5ea6\u3092\u5b9f\u73fe\u3057\u307e\u3059\u3002\n\n\nLongformer\u3067\u306f\u3001\u305d\u306eScaled Dot-Production attention\u306e\u65b9\u6cd5\u3092\u5de5\u592b\u3057\u307e\u3059\u3002\n\n\u7c21\u5358\u306b\u8a00\u3046\u3068\u3001Scaled Dot-Product attention\u3067\u306f\u3001\u3059\u3079\u3066\u306e\u5358\u8a9e\u304b\u3089\u3059\u3079\u3066\u306e\u5358\u8a9e\u3078\u6ce8\u610f\u3092\u5411\u3051\u3066\u304a\u308a\u3001\u305d\u308c\u304c\ud835\udc5b2\u306e\u30e1\u30e2\u30ea\u6d88\u8cbb\u91cf\u306b\u7e4b\u304c\u3063\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u305d\u308c\u3092\u91cd\u8981\u306a\u5358\u8a9e\u304b\u3089\u91cd\u8981\u306a\u5358\u8a9e\u3078\u306e\u307f\u6ce8\u610f\u3092\u5411\u3051\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002\n","dd121426":"Tokenizes the text and prints the number of words per text.\n\n\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3\u5316\u3057\u3066\u3001\u30c6\u30ad\u30b9\u30c8\u3054\u3068\u306e\u5358\u8a9e\u6570\u3092\u51fa\u529b\u3057\u307e\u3059\u3002","e586deb1":"I think that sentence structure is sometimes the same ,\u3000especially the first and last sentences.I try to check up.\n\n\u7279\u306b\u6700\u521d\u3068\u6700\u5f8c\u306e\u6587\u3067 \u6587\u578b\u304c\u540c\u3058\u3067\u3042\u308b\u5834\u5408\u3082\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u3002\u8abf\u3079\u3066\u898b\u307e\u3057\u3087\u3046","d0ebcd16":"The number of words in an essay varies from about 150 to 10000.\nIt seems to be composed of about 500 words on average.\n\n1\u3064\u306e\u30a8\u30c3\u30bb\u30a4\u306e\u5358\u8a9e\u6570\u306f\u3001150\u301c10000\u3068\u3070\u3089\u3064\u304d\u304c\u3042\u308a\u307e\u3059\u3002\n\u5e73\u5747500\u5358\u8a9e\u7a0b\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002","03d8cab0":"Since the 59 sentences have the same pattern, there seems to be some tendency.\n\n59\u500b\u306e\u6587\u7ae0\u3067\u540c\u3058\u30d1\u30bf\u30fc\u30f3\u304c\u3042\u308b\u306e\u3067\u3001\u50be\u5411\u306f\u3042\u308a\u305d\u3046\u3067\u3059\u3002","d6e4139e":"## Load data ","3df25726":"We can see that we have to submit the class and predictionstring from the sample.\n\nPredictionstring  is the index of the word corresponding to class (discourse_type).\n\nIf you apply discourse_type to a sentence as shown below, you will notice that there are some words that do not fall into the seven classes. \n\nclass\u3068predictionstring\u3092\u63d0\u51fa\u3057\u306a\u304f\u3066\u306f\u306a\u3089\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\npredictionstring\u3068\u306f\u3001class\uff08discourse_type\uff09\u306b\u76f8\u5f53\u3059\u308b\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u3053\u3068\u3067\u3059\u3002\n\n\u4e0b\u8a18\u306e\u3088\u3046\u306b\u3001\u6587\u7ae0\u306bdiscourse_type\u3092\u5f53\u3066\u306f\u3081\u3066\u307f\u308b\u3068\u30017\u3064\u306e\u30af\u30e9\u30b9\u306b\u5206\u985e\u3055\u308c\u306a\u3044\u5358\u8a9e\u304c\u3044\u304f\u3064\u304b\u3042\u308b\u3053\u3068\u306b\u6c17\u3065\u304f\u3068\u601d\u3044\u307e\u3059\u3002","ba17a65d":"I'm continueing to writing this notebook."}}