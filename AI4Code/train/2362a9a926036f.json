{"cell_type":{"99d337f3":"code","b2874180":"code","89e5e9c7":"code","b900fd66":"code","4a223339":"code","dd720fa3":"code","9ad3d76d":"code","a9776d15":"code","a6eddd3b":"code","af9d8463":"code","29337e36":"code","e3d27b87":"code","356c412f":"code","96c94e94":"code","94ee333f":"code","b672b965":"code","5fc2ad0c":"markdown","32741804":"markdown","cd8475e9":"markdown","97b821f5":"markdown","2b823c0b":"markdown","e09d234f":"markdown","0319c552":"markdown","c12c7636":"markdown","444ef2ec":"markdown","af72cb9a":"markdown","bdb22661":"markdown","456b9728":"markdown","502bc453":"markdown","270b608d":"markdown","dc4b578a":"markdown","f1987ccb":"markdown","fcaad12b":"markdown","750cd876":"markdown","c5efd582":"markdown"},"source":{"99d337f3":"!pip install trax","b2874180":"import pandas as pd \nimport os\nimport trax\nimport trax.fastmath.numpy as np\nimport random as rnd\nfrom trax import fastmath\nfrom trax import layers as tl","89e5e9c7":"directories = os.listdir('\/kaggle\/input\/')\nlines = []\nfor directory in directories:\n    for filename in os.listdir(os.path.join('\/kaggle\/input',directory)):\n        if filename.endswith(\".txt\"):\n            with open(os.path.join(os.path.join('\/kaggle\/input',directory), filename)) as files:\n                for line in files: \n                    processed_line = line.strip()\n                    if processed_line:\n                        lines.append(processed_line)","b900fd66":"for i, line in enumerate(lines):\n    lines[i] = line.lower()","4a223339":"def line_to_tensor(line, EOS_int=1):\n    \n    tensor = []\n    for c in line:\n        c_int = ord(c)\n        tensor.append(c_int)\n    \n    tensor.append(EOS_int)\n\n    return tensor","dd720fa3":"def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n    \n    index = 0                         \n    cur_batch = []                    \n    num_lines = len(data_lines)       \n    lines_index = [*range(num_lines)] \n\n    if shuffle:\n        rnd.shuffle(lines_index)\n    \n    while True:\n        \n        if index >= num_lines:\n            index = 0\n            if shuffle:\n                rnd.shuffle(lines_index)\n            \n        line = data_lines[lines_index[index]] \n        \n        if len(line) < max_length:\n            cur_batch.append(line)\n            \n        index += 1\n        \n        if len(cur_batch) == batch_size:\n            \n            batch = []\n            mask = []\n            \n            for li in cur_batch:\n\n                tensor = line_to_tensor(li)\n\n                pad = [0] * (max_length - len(tensor))\n                tensor_pad = tensor + pad\n                batch.append(tensor_pad)\n\n                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n                mask.append(example_mask)\n               \n            batch_np_arr = np.array(batch)\n            mask_np_arr = np.array(mask)\n            \n            \n            yield batch_np_arr, batch_np_arr, mask_np_arr\n            \n            cur_batch = []\n            ","9ad3d76d":"def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n    model = tl.Serial(\n      tl.ShiftRight(mode=mode),                                 \n      tl.Embedding( vocab_size = vocab_size, d_feature = d_model), \n      [tl.GRU(n_units=d_model) for _ in range(n_layers)], \n      tl.Dense(n_units = vocab_size), \n      tl.LogSoftmax() \n    )\n    return model","a9776d15":"def LSTMLM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n    model = tl.Serial(\n      tl.ShiftRight(mode=mode),                                 \n      tl.Embedding( vocab_size = vocab_size, d_feature = d_model), \n      [tl.LSTM(n_units=d_model) for _ in range(n_layers)], \n      tl.Dense(n_units = vocab_size), \n      tl.LogSoftmax() \n    )\n    return model","a6eddd3b":"def SRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n    model = tl.Serial(\n      tl.ShiftRight(mode=mode),                                 \n      tl.Embedding( vocab_size = vocab_size, d_feature = d_model), \n      [tl.SRU(n_units=d_model) for _ in range(n_layers)], \n      tl.Dense(n_units = vocab_size), \n      tl.LogSoftmax() \n    )\n    return model","af9d8463":"GRUmodel = GRULM(n_layers = 5)\nLSTMmodel = LSTMLM(n_layers = 5)\nSRUmodel = SRULM(n_layers = 5)\nprint(GRUmodel)\nprint(LSTMmodel)\nprint(SRUmodel)","29337e36":"batch_size = 32\nmax_length = 64","e3d27b87":"eval_lines = lines[-1000:] # Create a holdout validation set\nlines = lines[:-1000] # Leave the rest for training","356c412f":"from trax.supervised import training\nimport itertools\n\ndef train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=10, output_dir = 'model\/'): \n\n    \n    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n    infinite_train_generator = itertools.cycle(bare_train_generator)\n    \n    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n   \n    train_task = training.TrainTask(\n        labeled_data=infinite_train_generator, \n        loss_layer=tl.CrossEntropyLoss(),   \n        optimizer=trax.optimizers.Adam(0.0005)  \n    )\n\n    eval_task = training.EvalTask(\n        labeled_data=infinite_eval_generator,    \n        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n        n_eval_batches=3    \n    )\n    \n    training_loop = training.Loop(model,\n                                  train_task,\n                                  eval_tasks=[eval_task],\n                                  output_dir = output_dir\n                                  )\n\n    training_loop.run(n_steps=n_steps)\n    \n    return training_loop\n","96c94e94":"GRU_training_loop = train_model(GRUmodel, data_generator,n_steps=10, output_dir = 'model\/GRU')","94ee333f":"LSTM_training_loop = train_model(LSTMmodel, data_generator, n_steps = 10, output_dir = 'model\/LSTM')","b672b965":"SRU_training_loop = train_model(SRUmodel, data_generator, n_steps = 10, output_dir = 'model\/SRU')","5fc2ad0c":"### Converting into Tensors\n\nCreating a function to convert each line into a tensor by converting each character into it's ASCII value. And adding a optional `EOS`(**End of statement**) character.","32741804":"# Loading the Data","cd8475e9":"## Gated Recurrent Unit\n\nThis function generates a GRU Language Model, consisting of the following layers:\n\n* ShiftRight()\n* Embedding()\n* GRU Units(Number specified by the `n_layers` parameter)\n* Dense() Layer\n* LogSoftmax() Activation","97b821f5":"## Simple Recurrent Unit\n\nThis function generates a SRU Language Model, consisting of the following layers:\n\n* ShiftRight()\n* Embedding()\n* SRU Units(Number specified by the `n_layers` parameter)\n* Dense() Layer\n* LogSoftmax() Activation","2b823c0b":"# Defining the Model","e09d234f":"Here, we declare `the batch_size` and the `max_length` hyperparameters for the model.","0319c552":"# Importing Packages","c12c7636":"Here, we create a function to train the models. This function does the following:\n\n* Creating a Train and Evaluation Generator that cycles infinetely using the `itertools` module\n* Train the Model using Adam Optimizer\n* Use the Accuracy Metric for Evaluation","444ef2ec":"## Pre-Processing","af72cb9a":"# Training the Models","bdb22661":"## Hyperparameters","456b9728":"### Creating a Batch Generator\n\nHere, we create a `batch_generator()` function to yield a batch and mask generator. We perform the following steps:\n\n* Shuffle the lines if not shuffled\n* Convert the lines into a Tensor\n* Pad the lines if it's less than the maximum length\n* Generate a mask ","502bc453":"[Trax](https:\/\/trax-ml.readthedocs.io\/en\/latest\/) is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the [Google Brain team](https:\/\/research.google\/teams\/brain\/). This notebook ([run it in colab](https:\/\/colab.research.google.com\/github\/google\/trax\/blob\/master\/trax\/intro.ipynb)) shows how to use Trax and where you can find more information.","270b608d":"# Creating Evaluation and Training Dataset","dc4b578a":"For this project, I've used the [gothic-literature](https:\/\/www.kaggle.com\/charlesaverill\/gothic-literature), [shakespeare-plays](https:\/\/www.kaggle.com\/kingburrito666\/shakespeare-plays) and [shakespeareonline](https:\/\/www.kaggle.com\/kewagbln\/shakespeareonline) datasets from the Kaggle library. \n\nWe perform the following steps for loading in the data:\n\n* Iterate over all the directories in the `\/kaggle\/input\/` directory\n* Filter out `.txt` files\n* Make a `lines` list containing the individual lines from all the datasets combined","f1987ccb":"# Downloading the Trax Package","fcaad12b":"### Converting to Lowercase\n\nConverting all the characters in the `lines` list to **lowercase**.","750cd876":"In this notebook we will use the following packages:\n\n* [**Pandas**](https:\/\/pandas.pydata.org\/) is a fast, powerful, flexible and easy to use open-source data analysis and manipulation tool, built on top of the Python programming language. It offers a fast and efficient DataFrame object for data manipulation with integrated indexing.\n* [**os**](https:\/\/docs.python.org\/3\/library\/os.html) module provides a portable way of using operating system dependent functionality.\n* [**trax**](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.html) is an end-to-end library for deep learning that focuses on clear code and speed.\n* [**random**](https:\/\/docs.python.org\/3\/library\/random.html) module implements pseudo-random number generators for various distributions.\n* [**itertools**](https:\/\/docs.python.org\/3\/library\/itertools.html) module implements a number of iterator building blocks inspired by constructs from APL, Haskell, and SML. Each has been recast in a form suitable for Python.","c5efd582":"## Long Short Term Memory\n\nThis function generates a LSTM Language Model, consisting of the following layers:\n\n* ShiftRight()\n* Embedding()\n* LSTM Units(Number specified by the `n_layers` parameter)\n* Dense() Layer\n* LogSoftmax() Activation"}}