{"cell_type":{"e69fdb56":"code","e494d422":"code","ef423996":"code","18967b56":"code","bcb103af":"code","38a99d4a":"code","4077673b":"code","542cc1ce":"code","773295bb":"markdown","2b58fe8c":"markdown"},"source":{"e69fdb56":"import numpy as np\nimport matplotlib.pyplot as plt\n# Though the following import is not directly being used, it is required\n# for 3D projection to work\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nimport warnings\n\n\nwarnings.simplefilter(\"ignore\")\nnp.random.seed(249)","e494d422":"iris = datasets.load_iris()\nX = iris.data\ny = iris.target","ef423996":"estimators = [('k_means_iris_8', KMeans(n_clusters=8)),\n              ('k_means_iris_3', KMeans(n_clusters=3)),\n              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,\n                                               init='random'))]\n\nfignum = 1\ntitles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']","18967b56":"fignum = 1\nfor name, est in estimators:\n    fig = plt.figure(fignum, figsize=(4, 3))\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n    est.fit(X)\n    labels = est.labels_\n\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n               c=labels.astype(np.float), edgecolor='k')\n\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n    ax.set_xlabel('Petal width')\n    ax.set_ylabel('Sepal length')\n    ax.set_zlabel('Petal length')\n    ax.set_title(titles[fignum - 1])\n    ax.dist = 12\n    fignum = fignum + 1","bcb103af":"fignum = 1\n# Plot the ground truth\nfig = plt.figure(fignum, figsize=(4, 3))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nfor name, label in [('Setosa', 0),\n                    ('Versicolour', 1),\n                    ('Virginica', 2)]:\n    ax.text3D(X[y == label, 3].mean(),\n              X[y == label, 0].mean(),\n              X[y == label, 2].mean() + 2, name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n# Reorder the labels to have colors matching the cluster results\ny = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Petal width')\nax.set_ylabel('Sepal length')\nax.set_zlabel('Petal length')\nax.set_title('Ground Truth')\nax.dist = 12\n\nfig.show()\n","38a99d4a":"from sklearn.datasets import make_blobs\n\nn_samples = 250\nn_features = 2\nn_clusters = 4\nrandom_state = 42\nmax_iter = 100\n\nX, y = make_blobs(n_samples=n_samples, \n                  n_features=n_features, \n                  centers=n_clusters, \n                  random_state=random_state)\n\nfig=plt.figure(figsize=(8,8), dpi=80, facecolor='w', edgecolor='k')\nplt.scatter(X[:, 0], X[:, 1]);","4077673b":"from math import sqrt, floor\nimport numpy as np\n\n# try random_state 123, 249, 127, 13 to see the effect of different seed states - watch what happens to the yellow markers on the plot\ndef random(ds, k, random_state=42):\n    \"\"\"\n    Create random cluster centroids.\n    \n    Parameters\n    ----------\n    ds : numpy array\n        The dataset to be used for centroid initialization.\n    k : int\n        The desired number of clusters for which centroids are required.\n    Returns\n    -------\n    centroids : numpy array\n        Collection of k centroids as a numpy array.\n    \"\"\"\n\n    np.random.seed(random_state)\n    centroids = []\n    m = np.shape(ds)[0]\n\n    for _ in range(k):\n        r = np.random.randint(0, m-1)\n        centroids.append(ds[r])\n\n    return np.array(centroids)\n\n\ndef plus_plus(ds, k, random_state=42):\n    \"\"\"\n    Create cluster centroids using the k-means++ algorithm.\n    Parameters\n    ----------\n    ds : numpy array\n        The dataset to be used for centroid initialization.\n    k : int\n        The desired number of clusters for which centroids are required.\n    Returns\n    -------\n    centroids : numpy array\n        Collection of k centroids as a numpy array.\n    Inspiration from here: https:\/\/stackoverflow.com\/questions\/5466323\/how-could-one-implement-the-k-means-algorithm\n    \"\"\"\n\n    np.random.seed(random_state)\n    centroids = [ds[0]]\n\n    for _ in range(1, k):\n        dist_sq = np.array([min([np.inner(c-x,c-x) for c in centroids]) for x in ds])\n        probs = dist_sq\/dist_sq.sum()\n        cumulative_probs = probs.cumsum()\n        r = np.random.rand()\n        \n        for j, p in enumerate(cumulative_probs):\n            if r < p:\n                i = j\n                break\n        \n        centroids.append(ds[i])\n\n    return np.array(centroids)\n\n\ndef naive_sharding(ds, k):\n    \"\"\"\n    Create cluster centroids using deterministic naive sharding algorithm.\n    \n    Parameters\n    ----------\n    ds : numpy array\n        The dataset to be used for centroid initialization.\n    k : int\n        The desired number of clusters for which centroids are required.\n    Returns\n    -------\n    centroids : numpy array\n        Collection of k centroids as a numpy array.\n    \"\"\"\n\n    def _get_mean(sums, step):\n        \"\"\"Vectorizable ufunc for getting means of summed shard columns.\"\"\"\n        return sums\/step\n\n    n = np.shape(ds)[1]\n    m = np.shape(ds)[0]\n    centroids = np.zeros((k, n))\n\n    composite = np.mat(np.sum(ds, axis=1))\n    ds = np.append(composite.T, ds, axis=1)\n    ds.sort(axis=0)\n\n    step = floor(m\/k)\n    vfunc = np.vectorize(_get_mean)\n\n    for j in range(k):\n        if j == k-1:\n            centroids[j:] = vfunc(np.sum(ds[j*step:,1:], axis=0), step)\n        else:\n            centroids[j:] = vfunc(np.sum(ds[j*step:(j+1)*step,1:], axis=0), step)\n\n    return centroids\n\nrandom_centroids = random(X, n_clusters)\nprint(random_centroids)\n\nplus_centroids = plus_plus(X, n_clusters)\nprint(plus_centroids)\n\nnaive_centroids = naive_sharding(X, n_clusters)\nprint(naive_centroids)","542cc1ce":"def centroid_plots(X, rand, plus, naive):\n    fig=plt.figure(figsize=(8,8), dpi=80, facecolor='w', edgecolor='k')\n    \n    plt.scatter(X[:, 0], X[:, 1],\n                s=50,\n                marker='o',\n                label='cluster 1')\n\n    plt.scatter(rand[:, 0], \n                rand[:, 1],\n                s=200, c='yellow',\n                marker='p')\n\n    plt.scatter(plus[:, 0],\n                plus[:, 1],\n                s=200, c='red',\n                marker='P')\n\n    plt.scatter(naive[:, 0], \n                naive[:, 1],\n                s=100, c='green',\n                marker='D');\n\ncentroid_plots(X, random_centroids, plus_centroids, naive_centroids)","773295bb":"=========================================================\nK-means Clustering\n=========================================================\n\nThe plots display firstly what a K-means algorithm would yield\nusing three clusters. It is then shown what the effect of a bad\ninitialization is on the classification process:\nBy setting n_init to only 1 (default is 10), the amount of\ntimes that the algorithm will be run with different centroid\nseeds is reduced.\nThe next plot displays what using eight clusters would deliver\nand finally the ground truth.\n\n* Code source: Ga\u00ebl Varoquaux\n* Modified for documentation by Jaques Grobler\n* License: BSD 3 clause","2b58fe8c":"This next section illustrates the three difference common method available in the KMeans library:\n* Random\n* Kmeans++\n* Naive Sharding\n# Process\n* First we create some blobs of data to show the clusters clearly.\n* Then we see the code we use in each function, manually in this case so we can inspect what is going on\n* Then we plot each initial set of cluster centroids to show whaat happens when we choose different random seeds."}}