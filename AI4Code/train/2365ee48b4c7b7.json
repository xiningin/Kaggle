{"cell_type":{"dc953fcb":"code","cd4d9aa0":"code","f307c6fc":"code","d13a4503":"code","c6240efd":"code","9a520b53":"code","9bb9d6be":"code","7b777a1a":"code","db143e40":"code","cd79bb30":"code","a9fe1590":"code","3b66542f":"code","2bd6a898":"code","ce35a857":"code","143662c2":"code","f51e434b":"code","fc3d9b44":"code","14b1a7bd":"code","5f2b4bb4":"code","1c218043":"code","6a83951a":"markdown","b8b19ee3":"markdown","309d952f":"markdown","afb0b958":"markdown","b987c52c":"markdown","8a652385":"markdown","a30de440":"markdown","9f12f5ae":"markdown","afe9eb47":"markdown","3b38682a":"markdown","b2bec53c":"markdown","5ae94e96":"markdown","397e6e8d":"markdown"},"source":{"dc953fcb":"import os\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\nimport keras\nfrom keras.models import Sequential\nimport keras.layers as layers\nfrom keras import optimizers\n\nimport sklearn.model_selection as model_selection\n\n","cd4d9aa0":"#TODO load the X and Y dataset that we have saved from process_images\n\ndataset_path = '..\/input\/arrays\/Arrays'\n\nos.listdir(dataset_path)\n\n","f307c6fc":"X = none\nY = none","d13a4503":"print('X shape : {}  Y shape: {}'.format(X.shape, Y.shape))","c6240efd":"plt.imshow(X[700], cmap='gray')","9a520b53":"print(Y[700]) # one-hot labels starting at zero","9bb9d6be":"#TODO: implement training and validation dataset split\n\ndef split_data(X, Y, validation_size):\n    return None\n\nXtrain, Xtest, Ytrain, Ytest = split_data(X, Y, 0.2)","7b777a1a":"print('Xtrain shape {} Ytrain shape {}'.format(Xtrain.shape, Ytrain.shape))\nprint('Xtest shape {} Ytest shape {}'.format(Xtest.shape, Ytest.shape))","db143e40":"#the first Conv2D layer needs to specify what it takes as an input. Since we have resized all the images to\n#a specific size, please specify the input_shape\n\n#TODO: print out information about the model to help visualize how layers are structured\n\nmodel = Sequential()\n\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1))) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax'))\n\nmodel.not_implemented()","cd79bb30":"\nmodel.compile(loss='categorical_crossentropy',\n             optimizer=optimizers.adadelta(),\n             metrics=['accuracy'])","a9fe1590":"#TODO: implement add_channel_dim so that it adds a new dimension. Look at reshape and newaxis\n\ndef add_channel_dim(X):\n    return None\n\nXtrain_batch = add_channel_dim(Xtrain)\n\nXtest_batch = add_channel_dim(Xtest)","3b66542f":"#train our model\nhistory = model.fit(Xtrain_batch, Ytrain, batch_size=32, epochs=9, validation_data=(Xtest_batch, Ytest))","2bd6a898":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","ce35a857":"#TODO: evaluate the model\neval_score = None\n\nprint('Evaluation score {}'.format(eval_score))","143662c2":"def display_img(img_path):\n    img = cv2.imread(img_path)\n    color_corrected = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    plt.imshow(color_corrected)\n    plt.title(img_path)\n    plt.show()\n\n#TODO: use the same function that you've implemented in process_image\n# resize to 64 x 64 and greyscale\n    \ndef get_gsimg(image_path):\n    img = cv2.imread(image_path)\n    resize_img = cv2.resize(img, (64, 64))\n    gs_img = cv2.cvtColor(resize_img, cv2.COLOR_BGR2GRAY)\n    return gs_img\n","f51e434b":"#prediction\n\ninput_path = '..\/input\/sign_lang_dataset\/Inputs'\n\nos.listdir(input_path)\n\n","fc3d9b44":"display_img(os.path.join(input_path, 'sample_1.jpg'))\n\nsample1 = get_gsimg(os.path.join(input_path, 'sample_1.jpg'))\nsample1_batch = add_channel_dim(np.array(sample1).reshape((1, 64, 64)))","14b1a7bd":"model.predict(sample1_batch)","5f2b4bb4":"display_img(os.path.join(input_path, 'sample_3.jpg'))\n\nsample3 = get_gsimg(os.path.join(input_path, 'sample_3.jpg'))\nsample3_batch = add_channel_dim(np.array(sample3).reshape((1, 64, 64)))\n","1c218043":"model.predict(sample3_batch)","6a83951a":"## Train and Validation\n\nLet's split the data into training and validation sets\n\nWe should split both the input and labels into 2 sets. Typical split would be 70-30 or 80-20. \nLet do a 80-20. \n\nWe have imported model_selection module that has helper functions to perform the data split\n\nPlease see\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#computing-cross-validated-metrics","b8b19ee3":"Look at np.load\n[numpy load ](https:\/\/docs.scipy.org\/doc\/numpy-1.15.0\/reference\/generated\/numpy.load.html)","309d952f":"Print the shape of X and Y to make sure that they are the right shape","afb0b958":"## Compile the model with loss function and optimizer.\n\nAlso get the model to track accuracy as a metrics.\n\nBecause we have 10 classes, we use the default loss function categorical_crossentropy an use a well-known optimizer adam.","b987c52c":"## Now to perform some prediction using photos we've never seen","8a652385":"## Load previously processed data\n\nLoad the arrays that we have previously processed into X and Y","a30de440":"## Evaluate\n\nLet's evaluate how well our current model is doing against the test set Xtest batch and Ytest batch\n\nsee https:\/\/keras.io\/models\/sequential\/#the-sequential-model-api","9f12f5ae":"## Construct the model\n\nThis model will have 3 conv layers followed by 2 dense (fully connected) layers\n\nConv -> Max Pooling -> Conv -> Max Pooling -> Conv -> Dense -> Dense\n\nThe last Dense layer is 10 wide and has a softmax activation to determine the probablity of the 10 classes i.e.\ndigits 0 to 9\n\nPlease see https:\/\/keras.io\/layers\/convolutional\/#conv2d\nsee input_shape\n\nPlease see https:\/\/keras.io\/models\/about-keras-models\/\nsee getting information about a model ","afe9eb47":"## Plot to see how well we do for accuracy for training vs validation","3b38682a":"## Model creation, training and validation\n\nImport all libraries\n- use Numpy to manipulate the pictures and input\n- use Keras to construct the model using Tensorflow under the covers\n- use sklearn.model_selection","b2bec53c":"## Shape the batch input\n\nKeras model require the input array to have the following shape:\n(m, w, h, c) where\nm is the number of images\nw, h are the width and height of the image and in this case, we have predetermined that w and h are 64.\nc is the number of channels in the images and in our case, since we have converted the images to greyscale, we only have 1 channel. The default configurtion of a Conv2D layer uses channel last, so we need to add a single channel as the last dimension to our input data Xtrain.\n\nSo for example if Xtrain has 1649 images of shape (1649, 64, 64), after we add the channel, we should end up with an input array of shape (1649, 64, 64, 1)\n\nSimilar thing should apply to Xtest","5ae94e96":"As a sanity check, display the 700th picture and its label value","397e6e8d":"## Train the model with X and Y"}}