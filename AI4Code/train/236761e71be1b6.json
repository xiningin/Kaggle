{"cell_type":{"6e9bbdda":"code","166aa97c":"code","e2de5544":"code","2360548a":"code","2f33b764":"code","f6357c49":"code","97577aef":"code","ba0e9137":"code","69197d69":"code","3088c9a4":"code","0bd3a106":"code","8d15b458":"code","68bfad9c":"code","afca3420":"code","60738ab7":"code","bb35ee7b":"code","cfe1cc46":"code","8ee2bcf3":"code","66e05c90":"code","8513fda1":"code","1f742d4b":"code","354065a8":"code","aa336b4e":"code","e3f28bf1":"code","23af26eb":"code","c71233a6":"code","a02afe5f":"code","bf7a839e":"code","6e1c7556":"code","12e0020f":"code","6ad7ec76":"code","ab721b9b":"code","454ee165":"code","e5c89f01":"code","5a8c9721":"code","d117b0a4":"code","0e8fb8a0":"code","83f385c8":"code","20636cb6":"code","e24267eb":"code","beb65bf9":"code","f5ef95df":"markdown","e98aa2b0":"markdown","c337176c":"markdown","ea36ddcb":"markdown","174681fc":"markdown","1f1f3d15":"markdown","8c062f99":"markdown","0eadd01c":"markdown","a69bf233":"markdown","786e3ddf":"markdown","a3609313":"markdown"},"source":{"6e9bbdda":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","166aa97c":"df = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","e2de5544":"df.head()","2360548a":"df.info()","2f33b764":"df.describe(include=\"O\")","f6357c49":"df.isnull().sum()","97577aef":"plt.figure(figsize=(8,5))\nsns.heatmap(df.isnull(), cmap='viridis')","ba0e9137":"column=df.select_dtypes(include=['object'])\nfor col in column:\n    display(df[col].value_counts())","69197d69":"sns.countplot(x = df['gender'], data=df)","3088c9a4":"df.gender.value_counts()","0bd3a106":"df.columns","8d15b458":"sns.countplot(x = df['gender'], hue = df['status'], data=df)","68bfad9c":"df.head()","afca3420":"sns.boxplot(x = df['gender'], y = df['salary'], data=df)","60738ab7":"df.groupby('gender')['salary'].mean()","bb35ee7b":"df.columns","cfe1cc46":"sns.countplot(x = df['hsc_b'], hue = df['hsc_s'], data=df)","8ee2bcf3":"plt.figure(figsize = (15, 15))\nax=plt.subplot(221)\nsns.boxplot(x='status',y='ssc_p',data=df)\nax.set_title('Secondary school percentage')\nax=plt.subplot(222)\nsns.boxplot(x='status',y='hsc_p',data=df)\nax.set_title('Higher Secondary school percentage')\nax=plt.subplot(223)\nsns.boxplot(x='status',y='degree_p',data=df)\nax.set_title('UG Degree percentage')\nax=plt.subplot(224)\nsns.boxplot(x='status',y='mba_p',data=df)\nax.set_title('MBA percentage')\n","66e05c90":"sns.violinplot(x = df['gender'], y = df['salary'], hue = df['workex'], data=df)\nplt.title(\"Gender vs Salary based on work experience\")","8513fda1":"sns.distplot(df['salary'], bins=50, hist=False)","1f742d4b":"df.head()","354065a8":"df.drop(['sl_no', 'salary'], axis=1, inplace=True)","aa336b4e":"df.head()","e3f28bf1":"df['gender'] = df.gender.map({\"M\" : 0, \"F\" : 1})\ndf['ssc_b'] = df.ssc_b.map({\"Other\" : 0, \"Central\" : 1})\ndf['hsc_s'] = df.hsc_s.map({\"Commerce\" : 0, \"Science\" : 1, \"Arts\" : 2})\ndf['degree_t'] = df.degree_t.map({\"Comm&Mgmt\" : 0, \"Sci&Tech\" : 1, \"Others\" : 2})\ndf['workex'] = df.workex.map({\"No\" : 0, \"Yes\" :1})\ndf['specialisation'] = df.specialisation.map({\"Mkt&Fin\" : 0, \"Mkt&HR\" : 1})\ndf['status'] = df.status.map({\"Not Placed\" : 0, \"Placed\" : 1})","23af26eb":"df.head()","c71233a6":"df.drop(['ssc_b'], axis=1, inplace=True)","a02afe5f":"df.drop(['hsc_b'], axis=1, inplace=True)","bf7a839e":"df.head()","6e1c7556":"# Creating a correlation matrix\n\nplt.figure(figsize=(10,10))\n\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\")","12e0020f":"df.head()","6ad7ec76":"# Seperating our variables into Independent and Dependent variables\n\nX = df[['ssc_p', 'hsc_p', 'hsc_s', 'degree_p', 'degree_t','workex', 'specialisation', 'mba_p', 'etest_p']] # Indepepndent variables\n\ny = df['status'] # Dependent variables\n\n","ab721b9b":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        print(\"Train Result:\\n===========================================\")\n        print(f\"accuracy score: {accuracy_score(y_train, pred):.4f}\\n\")\n        print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_train, pred)}\\n\\tRecall Score: {recall_score(y_train, pred)}\\n\\tF1 score: {f1_score(y_train, pred)}\\n\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, clf.predict(X_train))}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        print(\"Test Result:\\n===========================================\")        \n        print(f\"accuracy score: {accuracy_score(y_test, pred)}\\n\")\n        print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_test, pred)}\\n\\tRecall Score: {recall_score(y_test, pred)}\\n\\tF1 score: {f1_score(y_test, pred)}\\n\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","454ee165":"# Splitting the data into train test split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)","e5c89f01":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=42)\ntree.fit(X_train, y_train)\n\nprint_score(tree, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree, X_train, y_train, X_test, y_test, train=False)","5a8c9721":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=200,criterion='gini',max_depth= 4, max_features= 'auto',random_state=42)\nrf.fit(X_train, y_train)\n\nprint_score(rf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf, X_train, y_train, X_test, y_test, train=False)","d117b0a4":"from xgboost import XGBClassifier","0e8fb8a0":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)\n\nprint_score(xgb, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb, X_train, y_train, X_test, y_test, train=False)","83f385c8":"rf_probs = rf.predict_proba(X_test)[:,1]\ndtree_probs = tree.predict_proba(X_test)[:,1]\nxgb_probs = xgb.predict_proba(X_test)[:,1]","20636cb6":"from sklearn.metrics import roc_curve, roc_auc_score","e24267eb":"print('roc_auc_score for Random forest: ', roc_auc_score(y_test, rf_probs))\nprint('roc_auc_score for Decision Tree: ', roc_auc_score(y_test, dtree_probs))\nprint('roc_auc_score for XGBoost: ', roc_curve(y_test, xgb_probs))","beb65bf9":"#ROC Curve\nfrom sklearn.metrics import roc_curve\ny_pred_prob1 = rf.predict_proba(X_test)[:,1]\nfpr1 , tpr1, thresholds1 = roc_curve(y_test, rf_probs)\n\ny_pred_prob2 = tree.predict_proba(X_test)[:,1]\nfpr2 , tpr2, thresholds2 = roc_curve(y_test, dtree_probs)\n\n\ny_pred_prob3 = xgb.predict_proba(X_test)[:,1]\nfpr3 , tpr3, thresholds3 = roc_curve(y_test, xgb_probs)\n\nplt.plot([0,1],[0,1], 'k--')\nplt.plot(fpr1, tpr1, label= \"Random Forest\")\nplt.plot(fpr2, tpr2, label= \"Decision Tree\")\nplt.plot(fpr3, tpr3, label= \"Xgboost\")\n\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title('Receiver Operating Characteristic')\nplt.show()","f5ef95df":"## Plotting ROC Curve","e98aa2b0":"**From the graph we can clearly see that Random Forest classifier is performing better than rest of the algorithms**","c337176c":"* Males tends to have higher salary as compared to females.","ea36ddcb":"## Applying Decision Tree Algorithm\n\n","174681fc":"# Correlation\n\n* Correlation is a statistical term which show that how strongly the variables as correlated\n\n* Positive correlation : If the value of one feature increases the value of other feature also increases\n\n* Negative correlation : If the value of one feature increases the value of other feature decreases","1f1f3d15":"* By visualizing we can see that males have higher of getting placed as compared to females","8c062f99":"# Feature Engineering","0eadd01c":"### Building Machine Learning Model","a69bf233":"## Exploring the dataset by each feature","786e3ddf":"* It can be seen that people with work experience have higher chance to get placed","a3609313":"# Feature Selection\n\n* Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\n\n*  Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features."}}