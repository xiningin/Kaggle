{"cell_type":{"6e4d7cb6":"code","ae20f2b1":"code","c3234f50":"code","a7d6be96":"code","15645eeb":"code","2fd94d9a":"code","10128c7b":"code","4465fb09":"code","f0817e2a":"code","64ec3924":"code","9ce03c62":"code","baf14498":"code","64792565":"code","91a036d2":"code","203586b0":"code","632a184a":"code","3cff3aa2":"code","8d8c4c0d":"code","18a0ac2d":"code","c4f6d7d6":"code","5de131c0":"code","6137f701":"code","ef943c8c":"code","a32b5a85":"code","62507b45":"code","fd66b78b":"code","af2dc9fc":"code","6f9ae5dc":"code","49be7f02":"code","48bce10c":"code","0c59ccd6":"code","a415c6d3":"code","ee8aa3b0":"code","ad7a8c6d":"markdown","edf68265":"markdown","850cfda4":"markdown","b974b15e":"markdown","731eec92":"markdown","508bb41d":"markdown","82a41ae5":"markdown","b15ad070":"markdown","d03cb572":"markdown","54ffef1b":"markdown","01a6b8f2":"markdown","e4fe468e":"markdown","bcab1e41":"markdown","73a5c627":"markdown","53990510":"markdown","3dda0b98":"markdown","18199013":"markdown","68dcfda8":"markdown","726f72f9":"markdown"},"source":{"6e4d7cb6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport string\nimport nltk\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport time\n\n%matplotlib inline\npd.set_option('display.max_colwidth', 100)\n\nnltk.download('stopwords')\nstopword = nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()","ae20f2b1":"def apply_styling(df: pd.DataFrame, caption: str = \"\"):\n    '''\n    Return @pd.DataFrame\n    Input  @df:pd.DataFrame\n           @caption: Stirng  \n    It help to apply style to a particular dataframe which is passed into this\n    '''\n    #TODO: Styling dataframe after reading the file\n    \n    st = df.style.format({'percent on rent': '{:.0%}'}).hide_index()    \n    st.set_table_styles([\n           dict(selector=\"th\", props=[('color', 'darkblue'), \n                                      ('vertical-align', 'top')]),\n           dict(selector=\"th:first-child\", props=[('max-width', '70px'), ('text-align', 'left')]),\n           dict(selector=\"th:last-child\", props=[('max-width', '50px')]),\n           dict(selector=\"td:first-child\", props=[('text-align', 'left')])\n            ]) \n    st.caption = caption\n    return st\n\ndef read_data(file_url: str, csv: bool = False, excel: bool = False, tab_delimeted: bool = False):\n    '''\n    Return : @pd.DataFrame\n    Input : @file_url: String\n            @csv: bool\n            @excel: bool\n            @tab_delimeted: bool\n    \n    It is utility function that helps to read a particular file and apply a particular styling\n    to the dataframe, which is returned.\n    '''\n    #TODO: Reading the file after checking the type on the basis of flag.\n    if csv:\n        df = pd.read_csv(file_url)\n    if excel:\n        df = pd.read_excel(file_url)\n    if tab_delimeted:\n        df = pd.read_csv(file_url, delimiter='\\t')\n        \n    if not (csv or excel or tab_delimeted):\n        print(\"please specify file type\")\n        return None\n    \n    return df","c3234f50":"file_url = \"..\/input\/sms-spam-collection-dataset\/spam.csv\"\ncaption = 'Spam-Ham Dataframe'\nspam_ham_df = read_data(file_url ,csv=True)\nspam_ham_df = spam_ham_df[['v1','v2']]\nspam_ham_df.columns = ['Type', 'Body']","a7d6be96":"apply_styling(spam_ham_df.head(),caption)","15645eeb":"def removePunct(text: str) -> str:\n    '''\n    @Return str\n    @Input @text: str\n    \n    It removes the punctuations from the text by using 'string' module,\n    and using its punctuations\n    '''\n    \n    return \"\".join([txt for txt in text if txt not in string.punctuation])","2fd94d9a":"#TODO: Applying @removePunct function to each message in spam_ham_df\n\nspam_ham_df['Body_punct_clean'] = spam_ham_df['Body'].apply(lambda x : removePunct(x))","10128c7b":"apply_styling(spam_ham_df.head(), caption)","4465fb09":"def tokenize_text(text: str) -> list :\n    '''\n    @Return: List<str>\n    @Input: text: String\n    \n    It converts the text into list of words by using regex \n    based apporach to split them on any non-character base.\n    '''\n    return re.split('\\W+',text)","f0817e2a":"#TODO: Tokenize text using above function @tokenize_text and passing each text iteratively into that function.\n\nspam_ham_df['Body_tokenize'] = spam_ham_df['Body_punct_clean'].apply(lambda x : tokenize_text(x))","64ec3924":"apply_styling(spam_ham_df.head(),caption)","9ce03c62":"def remove_stopword(text: [str]) -> [str] :\n    '''\n    Return @list[str]\n           @Input: @text : list[str]\n           \n    It remove the stopwords from the tokenized sentences and return the\n    list of strings without stopwords.\n    '''\n    \n    return [txt for txt in text if txt not in stopword]","baf14498":"#TODO: Removing stopwords from tokenized sentence using @remove_stopword function from spam_ham_df\n\nspam_ham_df['Body_without_stopword'] = spam_ham_df['Body_tokenize'].apply(lambda x: remove_stopword(x))","64792565":"apply_styling(spam_ham_df.head(), caption)","91a036d2":"def stemming(text: [str]) -> [str]:\n    \"\"\"\n    Return: [str]\n    Input: @txt: [str]\n    \n    It perform stemming on each word of text, by using NLTK(natural language toolkit) \n    Porter Stemmer.\n    \"\"\"\n    return [ps.stem(txt) for txt in text]","203586b0":"#TODO: Perform stemming on each word of tokenized sentences of spam_ham_df by using @stemming function.\n# We are performing stemming to reduce the number of similar tokens that our model have to read.\n\nspam_ham_df['Body_stemmed'] = spam_ham_df['Body_without_stopword'].apply(lambda x : stemming(x))","632a184a":"apply_styling(spam_ham_df.head(), caption)","3cff3aa2":"#TODO: Performing feature engineering on the spam_ham_df \n#TODO: Exploring body length as a feature\n\nspam_ham_df['Body_len'] = spam_ham_df['Body'].apply(lambda x: len(x) - x.count(\" \"))","8d8c4c0d":"apply_styling(spam_ham_df.head(), caption)","18a0ac2d":"def count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\nspam_ham_df['punct%'] = spam_ham_df['Body'].apply(lambda x: count_punct(x))","c4f6d7d6":"apply_styling(spam_ham_df.head(), caption)","5de131c0":"bins = np.linspace(0, 200, 40)\n\nplt.hist(spam_ham_df[spam_ham_df['Type'] == \"spam\"]['Body_len'], bins, alpha= 0.5,density=True, label=\"spam\")\nplt.hist(spam_ham_df[spam_ham_df['Type'] == \"ham\"]['Body_len'], bins, alpha= 0.5,density=True, label=\"ham\")\nplt.legend(loc=\"upper left\")\nplt.show()","6137f701":"bins = np.linspace(0, 50, 40)\n\nplt.hist(spam_ham_df[spam_ham_df['Type'] == \"spam\"]['punct%'], bins, alpha= 0.5,density=True, label=\"spam\")\nplt.hist(spam_ham_df[spam_ham_df['Type'] == \"ham\"]['punct%'], bins, alpha= 0.5,density=True, label=\"ham\")\nplt.legend(loc=\"upper left\")\nplt.show()","ef943c8c":"#TODO: Performing transformation on the punct% data and considering only +ve values,\n#      as -ve values will not help much in this scanerio\n\nfor i in [1,2,3,4,5]:\n    plt.hist((spam_ham_df['punct%'])**(1\/i), bins=40) # Not considering np.linspace for bins as bins size will vary as value will change according to i\n    plt.title(f\"Transformation of 1\/{i}\")\n    plt.show()","a32b5a85":"#TODO: Vectorize body tokenized data from spam_ham_df but before that split data into test and train sets.\n\nX_train, X_test, y_train, y_test = train_test_split(spam_ham_df[['Body', 'Body_len', 'punct%']], spam_ham_df['Type'], test_size=0.2)","62507b45":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopword]\n    return text","fd66b78b":"#TODO: Vectorize X_train, X_test datasets\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['Body'])\n\ntfidf_train = tfidf_vect_fit.transform(X_train['Body'])\ntfidf_test = tfidf_vect_fit.transform(X_test['Body'])\n\nX_train_vect = pd.concat([X_train[['Body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test[['Body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)","af2dc9fc":"X_train_vect.head()","6f9ae5dc":"#TODO: To fit RandomForestClassifier with different hyper parameter settings.\n\ngb = RandomForestClassifier()\n\nparam = { # different parameter settings\n    'n_estimators': [10, 150, 300], \n    'max_depth': [30, 60, 90, None]\n}\n\nclf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\ncv_fit = clf.fit(X_train_vect, y_train)\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","49be7f02":"#TODO : Create a RandomForestClassifier with following configuration:\n# @n_estimators =150\n# @max_depth = None (Mean any amount of depth will do)\n# @n_jobs = -1 (Mean perform parallization in tree creation)\n\n#=======================================================================\nrf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1) \n#=======================================================================\nstart = time.time()\n\nrf_model = rf.fit(X_train_vect, y_train)\nend = time.time()\n\nfit_time = (end - start)\n\n#=======================================================================\nstart = time.time()\n\ny_pred = rf_model.predict(X_test_vect)\nend = time.time()\n\npred_time = (end - start)\n#=======================================================================\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n\nprint('Fit time: {} \/ Predict time: {} ---- Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(\n    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()\/len(y_pred), 3)))","48bce10c":"gb = GradientBoostingClassifier()\nparam = {\n    'n_estimators': [100, 150], \n    'max_depth': [7, 11, 15],\n    'learning_rate': [0.1]\n}\n\nclf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\ncv_fit = clf.fit(X_train_vect, y_train)\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","0c59ccd6":"#TODO : Create a GradientBoostingClassifier with following configuration:\n# @n_estimators =150\n# @max_depth = 7\n\n#=======================================================================\ngb = GradientBoostingClassifier(n_estimators=150, max_depth=7)\n#=======================================================================\n\n#=======================================================================\nstart = time.time()\ngb_model = gb.fit(X_train_vect, y_train)\nend = time.time()\nfit_time = (end - start)\n#=======================================================================\n\n#=======================================================================\nstart = time.time()\ny_pred = gb_model.predict(X_test_vect)\nend = time.time()\npred_time = (end - start)\n#=======================================================================\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n\nprint('Fit time: {} \/ Predict time: {} ---- Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(\n    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()\/len(y_pred), 3)))","a415c6d3":"text = \"IMPORTANT - You could be entitled up to \u00a33,160 in compensation from mis-sold PPI on a credit card or loan. Please reply PPI for info or STOP to opt out.\"\n#=======================================================================\nlength = lambda x: len(x) - x.count(\" \")\npunct = count_punct(text)\n\npredict_df = pd.DataFrame(data=[(length(text),punct)],columns =['Body_len', 'punct%'])\n#=======================================================================\ngb_model.predict(pd.concat([predict_df, pd.DataFrame(tfidf_vect_fit.transform([text]).toarray())], axis=1))","ee8aa3b0":"rf.predict(pd.concat([predict_df, pd.DataFrame(tfidf_vect_fit.transform([text]).toarray())], axis=1))","ad7a8c6d":"### <u>Observations : <\/u>\n - As we are increasing value of i, our distribution is getting more and more normalized, as we can see at <strong>i = '4'<\/strong> and <strong>i = '5'<\/strong>, our distribution has become normalized.\n - We are seeing one standing rectangle at <strong>'0'<\/strong>, it is because of any things raised to <strong> power 0 is 0 <\/strong>.","edf68265":"### <u>Vectorizing data using TFIDFVectorizer<\/u>","850cfda4":"#### <u>Conclusion<\/u>\n- As we can see both the models with hypertuned parameter configuration perform very well and with good accuracy.","b974b15e":"### <u>Create Models and evaluate them on the basis of Accuracy, Recall and Precision.<\/u>","731eec92":"### <u>Evaluating above feature<\/u>","508bb41d":"##### Fitting RandomForestClassifier with different Hyper Parameter settings and using GridSearchCV","82a41ae5":"### <u>Performing the Stemming on tokenized words of sentences.<\/u>","b15ad070":"## Pre-Process raw text data\n\nCleaning up text data is necessary to highlight data attributes that we are going to use for NLP data analysis. Cleaning or pre-processing data basically follows three steps:\n - Remove Punctuation\n - Tokenizing\n - Remove Stopwords\n - Stemming\/ Lematizing\n \nLet us apply above steps to perform data cleaning and processing.","d03cb572":"### <u>Observations : <\/u>\n - As we can see clearly that spam and ham are clearly sperable from each other on the basis of body length.\n - We don't require to perform any Box-Cox transformation on the data as it is a bimodial distribution of data.\n - There are certain messages with small lengths which are persent in both categories but significant number of \n   messages are clearly seperable from this feature.","54ffef1b":"### <u>Creating feature for % punctuation in body text.<\/u>","01a6b8f2":"#### Observations:\n - As we can see that max_depth is not affecting much to test_score but when we are changing the n_estimator, this parameter is affecting the test score of our model.\n \n#### <u>Conclusion:<\/u>\n - We can say that <strong>max_depth<\/strong> can be <strong>None<\/strong> and <strong>n_estimators<\/strong> will be <strong>150<\/strong> as we have seen from above dataframe.","e4fe468e":"### <u>Removing punctuation from the dataframe of Spam\/Ham<\/u> ","bcab1e41":"#### <u>Fitting GradientBoostingClassifier with different Hyper Parameter settings and using GridSearchCV<\/u>","73a5c627":"#### Observations:\n - As we can see that max_depth = 7 and n_estimators = 150\n \n#### <u>Conclusion:<\/u>\n - We can say that <strong>max_depth<\/strong> can be <strong>7<\/strong> and <strong>n_estimators<\/strong> will be <strong>150<\/strong> as we have seen from above dataframe.","53990510":"### <u>Tokenizing each text in spam_ham_df<\/u>","3dda0b98":"### <u>Stopword removal from the spam_ham_df tokenized sentences.<\/u>","18199013":"### <u>Performing Transformation punct%<\/u>","68dcfda8":"### New Feature Exploration\n\n#### <u>Considering Body Length as a feature<\/u>","726f72f9":"### <u>Observations : <\/u>\n - This feature is not very significant as both the categories are not sperable from each other.\n - But we can perform transformation on the data as distribution is very skewed.\n\n<strong><i>For more information about transformation [click here](https:\/\/machinelearningmastery.com\/how-to-transform-data-to-fit-the-normal-distribution\/).<\/i><strong>"}}