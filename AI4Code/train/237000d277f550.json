{"cell_type":{"09557154":"code","2e11298b":"code","046ce6c6":"code","fa507c08":"code","3c17fd19":"code","a844eec1":"code","537fae69":"code","28544ada":"code","dbbdbe91":"code","739d0225":"code","d1768cb8":"code","a058686c":"code","de4ed821":"code","c7c54dcc":"code","e45a7cce":"code","8bb013e6":"code","05994dfb":"code","4fed09aa":"code","4ac12d65":"code","a64c7b16":"code","a03b4a00":"code","4aa85dfe":"code","bfe4a104":"code","3d26efb3":"code","2d8f2cce":"code","dc128119":"code","a6c37062":"code","6fab0605":"code","9d11e7be":"code","b994617d":"code","529cd674":"code","58d8b2c0":"code","f1e04d7e":"code","8f91a905":"code","6113ce4c":"code","f25c4d44":"code","2f440d4f":"code","41ad58e3":"code","7b72b997":"code","c354aa1f":"code","a5321ba4":"code","ebf8887a":"code","2568e3c8":"code","89294dfd":"code","3363d149":"code","cf8d72ce":"code","fb110ce4":"code","0ea3234b":"code","91e7a34f":"markdown","63fa0733":"markdown","94e7117a":"markdown","400773b0":"markdown","a3f95a2b":"markdown","b84904e6":"markdown","3c7289dd":"markdown","50fcbfb1":"markdown","4db48625":"markdown","aefe09ad":"markdown","36e8655c":"markdown","96445de7":"markdown","73f425a8":"markdown","39f042d6":"markdown","308e652c":"markdown","345cb00b":"markdown","1695faa0":"markdown","35c770f6":"markdown","d39d3eda":"markdown","4c8eb3bc":"markdown","c7166512":"markdown","cd175bd6":"markdown","e3f25dec":"markdown","8aa33d93":"markdown","02ccc97a":"markdown","a6280a1f":"markdown","7d6375fd":"markdown","7aa9b21a":"markdown","56da4be2":"markdown","11a8ef4e":"markdown","72ca4997":"markdown","8b0a8368":"markdown","dc79b245":"markdown","a12bbbdf":"markdown","a1c13693":"markdown","dd1d9215":"markdown","8eee782b":"markdown","9a9874e5":"markdown","808267a8":"markdown","bb8d5e00":"markdown","d703a1e2":"markdown","b3fd87e5":"markdown","967f6e5f":"markdown","abfebdcc":"markdown"},"source":{"09557154":"import sys\n# insert at 1, 0 is the script path (or '' in REPL)\nsys.path.insert(1, '..\/input\/utiles\/')\n\nfrom process_data import pre_process_dataset\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nimport random\nimport time\nfrom tqdm import tqdm\nimport IPython.display as ipd\nimport seaborn as sns\nimport itertools\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV , RandomizedSearchCV\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn import svm\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis as NCA\nfrom sklearn.neighbors import KNeighborsClassifier as KNC\n\n%matplotlib inline\ndef plot_confusion_matrix(y_true, y_pred, class_names,title=\"Confusion matrix\",normalize=False,onehot = False, size=4):\n    \"\"\"\n    Returns a matplotlib figure containing the plotted confusion matrix.\n\n    Args:\n    cm (array, shape = [n, n]): a confusion matrix of integer classes\n    class_names (array, shape = [n]): String names of the integer classes\n    \"\"\"\n    if onehot :\n        cm = confusion_matrix([y_i.argmax() for y_i in y_true], [y_ip.argmax() for y_ip in y_pred])\n    else:\n        cm = confusion_matrix(y_true, y_pred)\n    figure = plt.figure(figsize=(size, size))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n\n    # Normalize the confusion matrix.\n    cm = np.around(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], decimals=2) if normalize else cm\n\n    # Use white text if squares are dark; otherwise black.\n    threshold = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = \"red\" if cm[i, j] > threshold else \"black\"\n        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    #return figure","2e11298b":"DBtrain = pd.read_csv('..\/input\/titanic\/train.csv')\nDBtest = pd.read_csv('..\/input\/titanic\/test.csv')\nDBtrain.info(verbose=True)\nDBtrain.head()","046ce6c6":"# Numerical Data\nDBtrain.describe()","fa507c08":"DBtrain.describe(include=['object'])","3c17fd19":"DBtrain_num = ['Age', 'SibSp', 'Parch', 'Fare']\nDBtrain_cat = ['Sex', 'Ticket', 'Cabin', 'Embarked', 'Survived','Pclass']","a844eec1":"rcParams['figure.figsize'] = 20, 10\nfig, axes = plt.subplots(nrows=2, ncols=2)\ncolors = ['b', 'g', 'r', 'k']\nn_bins = None\nfor n, zippack in enumerate(zip(axes.flatten(), DBtrain_num)):\n    axhist , i_label = zippack\n    axhist.hist(DBtrain[i_label],bins=n_bins, histtype='bar',color=colors[n] ,density =None)\n    axhist.set_title(i_label)","537fae69":"rcParams['figure.figsize'] = 8, 4\nn_bins = 5\nDBtrain['AgeBand'] = pd.cut(DBtrain['Age'], [min(0,DBtrain['Age'].min()),16,32,48,64,max(100,DBtrain['Age'].max())])\n_ = DBtrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False ).mean().sort_values(by='AgeBand', ascending=True)\n_.index = _['AgeBand']\n_ = _.plot(kind='barh',rot =30)\n_.set_xlabel('Probability')\n_.set_title('P-survive VS Age')\nDBtrain[['AgeBand', 'PassengerId']].groupby(['AgeBand'], as_index=False ).count().sort_values(by='AgeBand', ascending=True)","28544ada":"rcParams['figure.figsize'] = 8, 4\nn_bins = 3\nDBtrain['FareBand'] = pd.cut(DBtrain['Fare'], [min(-1,DBtrain['Fare'].min()),85,170,256,426,max(600,DBtrain['Fare'].max())])\n#DBtrain['FareBand'] = pd.cut(DBtrain['Fare'], n_bins)\n_ = DBtrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False ).mean().sort_values(by='FareBand', ascending=True)\n_.index = _['FareBand']\n_ = _.plot(kind='barh',rot =30)\n_.set_xlabel('Probability')\n_.set_title('P-survive VS Fare')\nDBtrain[['FareBand', 'PassengerId']].groupby(['FareBand'], as_index=False ).count().sort_values(by='FareBand', ascending=True)","dbbdbe91":"rcParams['figure.figsize'] = 6, 4\ncoor_matrix = DBtrain[DBtrain_num].corr()\nprint(coor_matrix)\n_ = sns.heatmap(coor_matrix)","739d0225":"pd.pivot_table(DBtrain, index= 'Survived',values= DBtrain_num,aggfunc=['mean' ])","d1768cb8":"rcParams['figure.figsize'] = 20, 10\nfig, axes = plt.subplots(nrows=2, ncols=3)\ncolors = ['b', 'g', 'r', 'k','m','b']\nfor n, zippack in enumerate(zip(axes.flatten(), DBtrain_cat)):\n    axhist , i_label = zippack\n    axhist.bar(DBtrain[i_label].value_counts().index.astype('object'), DBtrain[i_label].value_counts(),color=colors[n],width=0.5)\n    axhist.set_title(i_label)","a058686c":"\nrcParams['figure.figsize'] = 10, 2\nnot_pivot = ['Survived','Ticket', 'Cabin' ]\n_ = pd.pivot_table(DBtrain, index= 'Survived',values= 'Ticket',aggfunc=['count'])\nprint(_)\nfor i_label in DBtrain_cat:\n    if i_label not in not_pivot:\n        _ = pd.pivot_table(DBtrain, index= i_label,values= 'Survived',aggfunc=['mean','count'])\n        print(_)\n        _ =_['mean'].plot(kind='barh')\n        _.set_xlabel('P-survive')","de4ed821":"DBtrain['n_parents'] = DBtrain['SibSp'] + DBtrain['Parch']\n_ = pd.pivot_table(DBtrain, columns= 'n_parents', index= 'Survived',values= 'PassengerId',aggfunc='count')\nprint(_)","c7c54dcc":"DBtrain['accompanied'] = DBtrain['n_parents'].apply(lambda x: 1 if x >0 else 0)\n_ = pd.pivot_table(DBtrain, columns= 'accompanied', index= 'Survived',values= 'PassengerId',aggfunc='count')\nprint(_)","e45a7cce":"_=DBtrain['Cabin'].value_counts().sort_values(ascending=False )\nprint(_.shape)\nprint(_)\n# Count how many cabins a passenger bought\nDBtrain['Count_Cabin'] = DBtrain['Cabin'].apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n#DBtrain['Count_Cabin'].value_counts()\n\n# Categorizing which type of cabin a passenger bought\nDBtrain['w_Cabin'] = DBtrain['Cabin'].apply(lambda x: 'nn' if pd.isna(x) else x[0])\n#DBtrain['w_Cabin'].value_counts() ","8bb013e6":"# We will see the relationship between survivors and the number of cabin that bought\n_ = pd.pivot_table(DBtrain, columns= 'Count_Cabin', index= 'Survived',values= 'Ticket',aggfunc='count')\nprint(_)\nprint(\"______________________\")\n# we will see the relationship between survivors and the type of cabin that they bought\n_ = pd.pivot_table(DBtrain, columns= 'w_Cabin', index= 'Survived',values= 'Ticket',aggfunc='count')\nprint(_)","05994dfb":"# The passenger bought a cabin yes or not\nDBtrain['b_Cabin'] = DBtrain['Cabin'].apply(lambda x: 0 if pd.isna(x) else 1)\n_ = pd.pivot_table(DBtrain, columns= 'b_Cabin', index= 'Survived',values= 'Ticket',aggfunc='count')\nprint(_)","4fed09aa":"DBtrain['title_Name'] = DBtrain['Name'].apply(lambda x: x.split(',')[1:][0].split('.')[0].strip())\nDBtrain['title_Name'].value_counts() ","4ac12d65":"# Grouping title names using only the predominants\ntitle_pred = ['Mr', 'Miss', 'Mrs']\nDBtrain['title_Name'] = DBtrain['title_Name'].apply(lambda x: x if x in title_pred else 'Others')\nprint(DBtrain['title_Name'].value_counts() )\n_ = pd.pivot_table(DBtrain, columns= 'title_Name', index= 'Survived',values= 'PassengerId',aggfunc='count')\nprint(_)","a64c7b16":"# We goint to pretend that there is a relati\u00f3n between the ticket ID and the survivors\n#   We take two new features\n#      1. If a ticket is a number, take the logarithm10 round to whole\n#      2. If a ticket start with a letter, take that string of letters. This technique is so risky, because in our test set could be a new string and will be in trouble\ncountbyTicket = DBtrain['Ticket'].value_counts()#.sort_values(ascending = False)\nDBtrain['num_Ticket'] = DBtrain['Ticket'].apply(lambda x: 1 if x.isnumeric() else 0)\nDBtrain['word_Ticket'] = DBtrain['Ticket'].apply(lambda x: x.split(' ')[0].replace('.', '').replace('\/','').lower() if not x.split(' ')[0].isnumeric() else 0)\nprint(DBtrain['num_Ticket'].value_counts())\nprint(DBtrain['word_Ticket'].value_counts())\n_ = pd.pivot_table(DBtrain, columns= 'num_Ticket', index= 'Survived',values= 'Ticket',aggfunc='count')\nprint(_)\n_ = pd.pivot_table(DBtrain, columns= 'word_Ticket', index= 'Survived',values= 'Ticket',aggfunc='count')\nprint(_)","a03b4a00":"DBtrain=DBtrain.drop(columns=['Ticket','num_Ticket','word_Ticket'])","4aa85dfe":"DBtrain.describe(include=['object','category'])","bfe4a104":"DBtrain.describe()","3d26efb3":"DBtrain.info()","2d8f2cce":"DBtrain['Embarked'].mode()[0]","dc128119":"media_age = DBtrain.dropna(subset=['Age'])[['Age','Pclass','Sex']].groupby(['Pclass', 'Sex'], as_index=False ).mean().sort_values(by='Age', ascending=True)\ndef impute_years(x):\n    if x['Age'] == x['Age'] :\n        return x['Age']\n    else:\n        return media_age.loc[media_age['Pclass']==x['Pclass'] , ['Age', 'Sex']].loc[media_age['Sex']==x['Sex'] , ['Age']]['Age'].tolist()[0]\n","a6c37062":"DBtrain['Embarked'] = DBtrain['Embarked'].fillna(DBtrain['Embarked'].mode()[0])\nDBtrain['Age'] = DBtrain.apply(impute_years, axis=1)# DBtrain['Age'].fillna( DBtrain['Age'].mean())\nDBtrain['AgeBand'] = pd.cut(DBtrain['Age'], [min(0,DBtrain['Fare'].min()),16,32,48,64,max(100,DBtrain['Age'].max())])\nDBtrain.info()","6fab0605":"# On \"Histogram for numerical Data\" we saw that \"Fare\" is not normalize, the distance between his values are significance\n#     We try to normalize that data, taken the logarithm10 to those values\nrcParams['figure.figsize'] = 10, 5\nDBtrain['norm_Fare']= DBtrain['Fare'].apply(lambda x : np.log10(x+1))\n#DBtrain['norm_Fare']= DBtrain['norm_Fare']\nDBtrain['norm_Fare'].hist().set_title('Fare normalized')\n# we can see that the histogram look like a normal distributions more accurate","9d11e7be":"DBtrain.head(2)","b994617d":"# Second Test\nDBtrain_f = DBtrain.copy()\nDBtrain_f.index  = DBtrain_f.PassengerId\nDBtrain_f = DBtrain_f.drop(columns=[ 'Cabin','Fare','Name','PassengerId','w_Cabin', 'Count_Cabin','n_parents','SibSp','Parch','norm_Fare', 'Age'])\nDBtrain_f['Pclass'] = DBtrain_f['Pclass'].astype(str) # categorical feature\nDBtrain_f['accompanied'] = DBtrain_f['accompanied'].astype(str) # categorical feature\nDBtrain_f['b_Cabin'] = DBtrain_f['b_Cabin'].astype(str) # categorical feature\nDBtrain_f['AgeBand'] = DBtrain_f['AgeBand'].astype(str) # categorical feature\nDBtrain_f['AgeBand'] = DBtrain_f['AgeBand'].apply(lambda x : x.replace('(','').replace(']','').replace(',','_'))\nDBtrain_f['FareBand'] = DBtrain_f['FareBand'].astype(str) # categorical feature\nDBtrain_f['FareBand'] = DBtrain_f['FareBand'].apply(lambda x : x.replace('(','').replace(']','').replace(',','_'))\nDBtrain_f.info()\nDBtrain_f.head()","529cd674":"# First test\nDBtrain_f = DBtrain.copy()\nDBtrain_f.index  = DBtrain_f.PassengerId\nDBtrain_f = DBtrain_f.drop(columns=[ 'Cabin','Fare','Name','PassengerId','w_Cabin', 'Count_Cabin','n_parents','SibSp','Parch','AgeBand','FareBand' ])\nDBtrain_f['Pclass'] = DBtrain_f['Pclass'].astype(str) # categorical feature\nDBtrain_f['accompanied'] = DBtrain_f['accompanied'].astype(str) # categorical feature\nDBtrain_f['b_Cabin'] = DBtrain_f['b_Cabin'].astype(str) # categorical feature\nDBtrain_f.info()\nDBtrain_f.head()","58d8b2c0":"DBtrain_f = pd.get_dummies(DBtrain_f)\nX_train = DBtrain_f.drop(columns='Survived')\nY_train = DBtrain_f['Survived']\nDBtrain_f.head()","f1e04d7e":"X_train, Y_train = pre_process_dataset(all_categorical=False, Test=False, fillna_age = None,path_test='..\/input\/titanic\/test.csv',\n                                                                                           path_train='..\/input\/titanic\/train.csv', )","8f91a905":"#Modelo with SVC\nclf = svm.SVC(verbose= True,random_state=5,C=10, kernel='rbf',degree=3, gamma='auto',probability=True)\nclf = BalancedBaggingClassifier(base_estimator=clf,\n                                sampling_strategy='auto',\n                                replacement=False,random_state=42)\nfit_model = clf.fit(X_train, Y_train)\ny_pred = clf.predict(X_train)\nprint(\"SVM: acc:\"+ str(accuracy_score(Y_train, y_pred)))\nplot_confusion_matrix(y_true=Y_train, y_pred=y_pred, class_names=['No', 'Yes'],title=\"SVC\",normalize=True, size=4)","6113ce4c":"xgb_model_ = XGBClassifier(learning_rate=0.01,\n                    n_estimators=150,\n                    max_depth=100,\n                    min_child_weight=.05,\n                    gamma=0,\n                    subsample=.5,\n                    colsample_bytree=0.5,\n                    objective='multi:softmax',\n                    num_class=10,\n                    num_parallel_tree = 8,\n                    seed=27,verbosity= 1,n_jobs=8 )\nxgb_model = BalancedBaggingClassifier(base_estimator=xgb_model_,\n                                sampling_strategy='auto',\n                                replacement=False,random_state=42)\nxgb_model.fit(X_train, Y_train)\n#xgb_model.save_model('models\/xgbmodel')\ny_pred = xgb_model.predict(X_train)\nprint(\"XGB: acc:\"+ str(accuracy_score(Y_train, y_pred)))\nplot_confusion_matrix(y_true=Y_train, y_pred=y_pred, class_names=['No', 'Yes'],title=\"XGB\",normalize=True, size=4)","f25c4d44":"clf_KNC_ = KNC(n_neighbors=2,n_jobs=-1,algorithm='brute',p=2 )\nclf_KNC = BalancedBaggingClassifier(base_estimator=clf_KNC_,\n                                sampling_strategy='auto',\n                                replacement=False,random_state=42)\nclf_KNC.fit(X_train, Y_train)\ny_pred = clf_KNC.predict(X_train)\nprint(\"KNC: acc:\"+ str(accuracy_score(Y_train, y_pred)))\nplot_confusion_matrix(y_true=Y_train, y_pred=y_pred, class_names=['No', 'Yes'],title=\"KNC\",normalize=True,size=4)","2f440d4f":"clf_RFC_ = RandomForestClassifier( n_estimators=100, n_jobs=-1)\nclf_RFC = BalancedBaggingClassifier(base_estimator=clf_RFC_,\n                                sampling_strategy='auto',\n                                replacement=False,random_state=42)\nclf_RFC.fit(X_train, Y_train)\ny_pred = clf_RFC.predict(X_train)\nprint(\"RFC: acc:\"+ str(accuracy_score(Y_train, y_pred)))\nplot_confusion_matrix(y_true=Y_train, y_pred=y_pred, class_names=['No', 'Yes'],title=\"RFC\",normalize=True,size=4)","41ad58e3":"model_VC = VotingClassifier (estimators=[ ('xgb', xgb_model), ('knc', clf_KNC),('rfc',clf_RFC)], voting='hard', weights=[1,1,1],n_jobs=-1)\nmodel_VC.fit(X_train, Y_train)\ny_pred = model_VC.predict(X_train)\nprint(\"VotingClassifier: acc:\"+ str(accuracy_score(Y_train, y_pred)))\nplot_confusion_matrix(y_true=Y_train, y_pred=y_pred, class_names=['No', 'Yes'],title=\"VotingClassifier\",normalize=True,size=4)","7b72b997":"media = DBtrain[['Age','Pclass','Sex','Embarked']].groupby(['Pclass', 'Sex','Embarked'], as_index=False ).mean().sort_values(by='Pclass', ascending=True)\nmedia","c354aa1f":"def impute_years(x):\n    if x['Age'] == x['Age']:\n        return x['Age']\n    else:\n        return media.loc[media['Pclass']==x['Pclass'] , ['Age', 'Sex','Embarked']].loc[media['Sex']==x['Sex'] , ['Age','Embarked']].loc[media['Embarked']==x['Embarked'] , ['Age']]['Age'].tolist()[0]\nX_age = DBtrain.apply(impute_years, axis=1)                                                                                                                          ","a5321ba4":"#from process_data import pre_process_dataset\nX_test, X_test_origin = pre_process_dataset(all_categorical=False, Test=True, fillna_age = None,path_test='..\/input\/titanic\/test.csv',\n                                                                                           path_train='..\/input\/titanic\/train.csv', )","ebf8887a":"estmt = model_VC.estimators\ny_predict_svc = clf.predict(X_test)\ny_predict_xgb = estmt[0][1].predict(X_test)\ny_predict_knc = estmt[1][1].predict(X_test)\ny_predict_rfc = estmt[2][1].predict(X_test)\ny_predict_vc = model_VC.predict(X_test)","2568e3c8":"print(\"CLASSIF test: equal:\"+ str(accuracy_score(y_predict_vc, y_predict_rfc)))","89294dfd":"submission_file = pd.DataFrame({ 'PassengerId':np.stack(X_test.index.tolist()),'Survived': y_predict_vc})\nsubmission_file.to_csv('submission\/submm_vc2.csv',index=False, )\nsubmission_file = pd.DataFrame({ 'PassengerId':np.stack(X_test.index.tolist()),'Survived': y_predict_xgb})\nsubmission_file.to_csv('submission\/submm_xgb2.csv',index=False, )\nsubmission_file = pd.DataFrame({ 'PassengerId':np.stack(X_test.index.tolist()),'Survived': y_predict_knc})\nsubmission_file.to_csv('submission\/submm_knc2.csv',index=False, )\nsubmission_file = pd.DataFrame({ 'PassengerId':np.stack(X_test.index.tolist()),'Survived': y_predict_rfc})\nsubmission_file.to_csv('submission\/submm_rfc2.csv',index=False, )","3363d149":"ls","cf8d72ce":"a =1","fb110ce4":"a","0ea3234b":"a*2","91e7a34f":"###  EDA -  Missing Values","63fa0733":"* XGB + BalancedBaggingClassifier","94e7117a":"* Method to impute values to Age","400773b0":"# Background\nUnderstand the relationship between the circuntances and the outcome, that finished the life of 549 human lifes. What characterized to the people, why they survived and what factors pushed his probabilities to survive are the main question that i going to review in this notebook.","a3f95a2b":"### EDA - Visualizing histograms for numerical data\nWe can see that, Age is pretty normalize, but SibSP, Parch concentrate values of zero(0) value, \nWe can see that Fare haven't a normal distribution, but i think that i could normalize, apply some matematical operati\u00f3n to his values","b84904e6":"* Age => complete the nan values with mean value\n* Embarked => Complete the nan values with the mode, which is S","3c7289dd":"We take the media value using Sex and Pclass, to fill the nan values in Age feature","50fcbfb1":"### FE - Fill Nan values\nLike humans we can't see the whole data in a single look, to achieve we aggregate data, reading the count, uniquisity and distribution behavior of the data.\n\nThat exactly what we goint to do next","4db48625":"# Type of data\n\n### 1 Numerical Data\n* Histograms to understand distributions\n* Correlations plot\n* Pivot tables comparing survival rates\n\n### 2 Categorical Data\n* Made barchars to understand balance between classes\n* Made pivot tables to understand relationship with survival","aefe09ad":"We can see that Age and Fare, have multiples values, maybe we can create bands of values and see probability behavior of sirvive","36e8655c":"### FE - Exploring what is Cabin, \nwe can see that not everyone buy for a cabin, but some wealthy people bought more than one cabin\n\nWe can denote that the name of the cabins dosen't matter. I think that only mather the quantity of cabin that a people buy,\nthat could be related whit his wealthiness.\nFor other hand, we can study that the first letter, in the cabin nam, have some importance over the survivors, maybe its location on the ship","96445de7":"# EDA\n______________________________________________","73f425a8":"# FE - Feature Engineering\n___________________________","39f042d6":"### EDA - Relationship between every categorical data and survival rate\nWe visualice some relation between survivers and his conditions in the journy\n* Females survive more that males\n* Embarker C have more survivors\n* Class 1 have more survivors <p>\n\nWe may say that this result is because,\n1. Money, Unfortunately is a factor\n2. The place where they out,\n3. \"Women and children first\" [wiki](https:\/\/en.wikipedia.org\/wiki\/Women_and_children_first)","308e652c":"* The passenger bought a cabin yes or not","345cb00b":"* Behavior of categorical data","1695faa0":"Analising the data we can see that the categorical and numerical data are just like this","35c770f6":"* VotingClassifier = XGB+KNC+RFC","d39d3eda":"### FE - Extracting features from Name\nWe going to extract the name title from every passenger, I think that is relative with his social posici\u00f3n and his gender, maybe is a condition that affect his survival\n\nWe see that exist 4 predominant titles name, so can group all passanger into this group, changing the less predomint to others","4c8eb3bc":"* We see that AGe have Nan values, so we have to impute data (mean)\n* We see thar Embarked have Nan Values, so we have to imute data categorical (moda)\n* Cabin habe only 204 values, but we already study this feature","c7166512":"* We can analize Fare groups vs bands,\n\nwe can see clearly that more wealthy you are, more probability of survive, but around 170-426 the probability didn't change to much\nBut if we only use 3 bins, the difference increases drastically, we could see a rate ~ x2 for every range, i mean ~ 0.38, 0.65, 1 in p-survive, unfortunately who survive depend on the wealthiness.","cd175bd6":"### FE - Create a new feature from study Parch and SibSp\nJust like we say in EDA, we can see a correlati\u00f3n between Parch and SibSp, so we can create a single feature that resumen and could be more afective.\n\nWe know that bouth feature are relatave with how much familiar companied do you have in the ship, so we can summaize bouth feature into just one.","e3f25dec":"* Preprocessing Test data to submmit","8aa33d93":"* Behavior of numerical data\n    * we detect that Some values of Fare are 0 \u00bf? could be an error or a gifs","02ccc97a":"* We can create bands in Age, there are not a perfect lenght for discretize the Age, but 5 look pretty good, for diferenciate probalistic beteeen bands, more bands, generate almos equal probality in intemedian ages","a6280a1f":"* Data set modeling with all feature categorical","7d6375fd":"### EDA - Barchar for categorical Data\nwe can see<p>\n-the population of male are more bigger that female<p>\n-the embarker C is more frecuenly<p>\n-the 3thr class is the mayority of the passenger<p>\n-Ticket and Cabin are not grouped so they can't be categorized in his raw form<p>\n-Imbalance data set to classify survivor, could produce imbalance model, 549 deads vs 342 survivors","7aa9b21a":"### EDA - Mean Values for every num_val VS Survived\nWe can see \n* more young more probability to survive\n* more wealthy more chance to survive\n* About Parch and SibSP is not so clear, but i think that is a little indicator to more parents more chance to live","56da4be2":"### FE - Studing the Ticket\nI studied the numerical and the no numerical mix Ticket, and try to find some relation in survival rate, but we found that is too random to be useful. I decided to drop that feature.","11a8ef4e":"# Test perform\nThe test data contain Nans so, what i did was impute values, using the media, taking in consideration, Pclass, sex and Embarked\n* Method below\n* I create the file process_data.py where is the steps that i use for process all the test_Data","72ca4997":"#### Augmented Features\n* Create a new feature from study Parch and SibSp\n* Transform cabin string feature into a number that represent how many cabins a passager bought\n* Take more features, asumming that one person bought all cabin with the same firts word, so we can know what \"cabin Type\" bought a passenger\n* Take more features from the \"Name\" feature, we goint to see if the title name is relevant to determine if someone survive\n* Study the posibility to extract some features form \"Ticket\"\n* Fill nan values\n* Normalizing Data\n________________________________","8b0a8368":"We see so many categories that destroy visivility, so we try transforming all categories into just 2, or you are accompanied or alone.\n* if you are accompanied your chances to survive increase around to 50%\n","dc79b245":"# Pre-processing data to Train Models\nBecause we have categorical data, we need to transform all the categorical data into arrays that our model can interpretate,\nwe can use pd.dummies or hot-encode, in this case are equal, also we need to drop the columns that we dont use anymore","a12bbbdf":"## Project Planning\n\n* Understand the data behavior <p>\n  1 Histograms<p>\n  2 Value counts <p>\n  3 Missing data<p>\n  4 correlations of some features\n* Explore interesting themes <p>\n  1 Rich people survive more?<p>\n  2 Female survive more?, what about kids and old people<p>\n  3 How the fare payment affects the survival<p>\n  4 Your title name affects your chances to survive<p>\n  5 Where did you embarked affects your chance to survive<p>\n* Feature Engineering\n    * Create more features\n    * Fill nan values\n    * Normalize data\n    * Try to categorize data and see what happened \u00bfis good?\n* Preprocess data for numerical and categorical data\n* Understand the limitations of the data \u2013 imbalance data\n* Model tunning\n* Model benchmark","a1c13693":"* SVC + BalancedBaggingClassifier","dd1d9215":"* Data set modeling with Age and Fare like numerical feature","8eee782b":"### FE - Normalizing Data\nWe saw that we could normalize the fare value, so we will try take the log10 to the fare value and see how it behaves\n\nThe transform result pretty useful, delivering a good normal distributi\u00f3n","9a9874e5":"We see that the way majority of the passenger not buy a single Cabin, so now we pretend to categorice the cabins, this will create impresaisly data to the model, instead we could only extract to categories, \n* The passenger bought or not a cabin","808267a8":"We goint to explorate our data looking for relationships and some correlations with the survivors\n* Missing Values\n* Repeating values\n* Histogram visualization (Normal distribution)\n* Correlation into numerical data and survivors\n* Barchar to visualize categorical data, to study the relevance of every category","bb8d5e00":"# Modeling - Improve imbalance data\nwe goint to see how well BalancedBaggingClassifier, we improve the model with imbalance method training\n* 549 deads vs 342 survivors\n\n| Model  | score  | \n|---|---|\n| SVC  |    0.8686 |  \n| XGB  | 0.8720 | \n| KNC  |   0.9102|\n| RFC |    0.9371 |\n| Voting XGB+KNC+RFC| 0.9315| ","d703a1e2":"### EDA - Correlation between numerical features\nFrom this study, we can infare that SibSp and Parch are a quite correlated, so we may fusion those variables into a single one, that indicate about, how alone or acoompained is a passenger, maybe infuence his survival","b3fd87e5":" We can't see any relationship, even those repeat more than 20%\n * We can drop Ticket Feature","967f6e5f":"!kaggle competitions download -c titanic","abfebdcc":"* KNC + BalancedBaggingClassifier"}}