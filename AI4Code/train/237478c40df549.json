{"cell_type":{"02c5f732":"code","1079b591":"code","8bfa4116":"code","03a6028f":"code","1d8f5ece":"code","f4ed3037":"code","99a3b2a4":"code","12b081e9":"code","8d6584e3":"code","e1acca41":"code","e953dd1b":"code","21ff6838":"code","c2de5e3d":"code","b744581b":"code","e2b74137":"code","6cb1ce5c":"code","dfe6990e":"code","6bdcf739":"code","ad449d58":"code","54e3b3aa":"code","633bf1d9":"code","19e664ab":"code","08f24b09":"code","d17e7c84":"code","2a9a828c":"code","ee2932b2":"code","6f5d677e":"code","0206df17":"code","a6cca020":"code","106dab41":"code","4fc833d2":"code","0f776ee2":"code","5142289d":"code","5f8c6096":"code","9a7fb105":"code","80733503":"code","c3b1e66e":"code","5e7658e4":"code","aad06b9a":"code","5230d326":"code","927792e0":"code","eb6e5e78":"code","00f9ef1f":"code","e2ab5e37":"code","af1446a3":"code","6afff4f5":"code","437bbc08":"code","4ba04d88":"code","06185901":"code","e9b3e26c":"code","10dfc27d":"markdown","e9bb5287":"markdown","24605a76":"markdown","c22ac765":"markdown","8a97cc8c":"markdown","05a95f93":"markdown","9e0792f0":"markdown","960e0259":"markdown","51845366":"markdown","1a19774c":"markdown","3454a855":"markdown","1db2d702":"markdown","d0803203":"markdown","20406a69":"markdown"},"source":{"02c5f732":"import numpy as np\nfrom numpy import savetxt \nimport pandas as pd \nimport re\nimport gc\nimport random\nimport os\nimport tensorflow as tf\nimport torch\nimport transformers\n\nimport spacy\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport codecs\nfrom gensim.models import Word2Vec\n\nimport nltk\nfrom nltk.corpus import stopwords as nltk_stopwords\nfrom gensim.models import Word2Vec\nfrom tqdm import notebook\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import plot_confusion_matrix, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedShuffleSplit\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, LabelEncoder, Binarizer, OneHotEncoder\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom catboost import CatBoostClassifier, Pool, cv \n\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D, GRU, LSTM, Conv1D, MaxPooling1D, Embedding\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","1079b591":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsamp_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","8bfa4116":"train.head()","03a6028f":"corpus_train = train['text'].values\ncorpus_test = test['text'].values","1d8f5ece":"def lower_case(corpus):\n    for tweet in range(len(corpus)):\n        corpus[tweet] = corpus[tweet].lower()\n\n    return corpus","f4ed3037":"corpus_train = lower_case(corpus_train)\ncorpus_test = lower_case(corpus_test)","99a3b2a4":"all_words = ' '.join(corpus_train).split(' ')","12b081e9":"len(all_words)","8d6584e3":"unique_words = list(set(all_words))\nunique_words","e1acca41":"len(unique_words)","e953dd1b":"def text_processing_03(df):\n    \n    text = df['text'].values\n    \n    df_new = df.copy()\n\n    for sen in range(0, len(text)):\n      \n        ## removing part\n        \n        # remove hyperlinks\n        document = re.sub(r'http\\S+', '', str(text[sen]))\n        # remove hashtags symbols\n        document = re.sub(r'#', '', document)        \n        # remove 'b'\n        document = re.sub(r'^b\\s+', '', document)   \n        \n        #remove strange characters\n        document = re.sub(r'\u00fb\u00f3', '', document)\n        document = re.sub(r'\u00fb\u00f2', '', document)\n        document = re.sub(r'\u00e5\u00ea', '', document)\n        document = re.sub(r'i\u0089\u00fb\u00aam', '', document)\n        document = re.sub(r'0npzp', '', document)\n        document = re.sub(r'rq', '', document)\n        document = re.sub(r'\u00fb_', '', document)\n        document = re.sub(r'\u00fb\u00aa', '', document)\n        document = re.sub(r'\u00fb\u00ef', '', document)\n        document = re.sub(r'\u00fb', '', document)\n        document = re.sub(r'\u00e5', '', document)\n        document = re.sub(r'\u00e5_', '', document)       \n        document = re.sub(r'\u00e2\u00e2', '', document)\n        document = re.sub(r'\u00ec\u00fc', '', document)\n        document = re.sub(r'\u00ec\u00f11', '', document)\n        document = re.sub(r'\u00ec\u00f1', '', document)\n        document = re.sub(r'\u00e5\u00e8mgn', '', document)\n        document = re.sub(r'\u00e5\u00e8', '', document)\n        document = re.sub(r'\u00e5\u00e7', '', document)        \n        document = re.sub(r'\u00e8', '', document)\n        document = re.sub(r'\u00e7', '', document)\n        document = re.sub(r'\u00e3', '', document)\n        document = re.sub(r'\u00ec', 'i', document)\n        \n                \n        # convert all letters to a lower case\n        document = document.lower()\n        \n        ## replacing part\n        document = re.sub(r'\u00e8mgn', 'emergency', document)\n        document = re.sub(r\"isn't\", 'is not', document)\n        document = re.sub(r\"havn't\", 'have not', document)\n        document = re.sub(r\"'s\", ' is', document)\n        document = re.sub(r\"'m\", ' am', document)\n        document = re.sub(r\"'d\", ' had', document)\n        document = re.sub(r\"'ve\", ' have', document)\n        document = re.sub(r\"'t\", ' not', document)\n        \n        document = re.sub(r\"e-bike\", 'electro bike', document)\n        document = re.sub(r'hwy', 'highway', document)\n        document = re.sub(r'nsfw', 'not safe for work', document)\n        document = re.sub(r'koz', 'because', document)       \n        \n                \n        ### NEW IN THIS VERSION\n        \n        ## remove repeating characters\n        pattern = re.compile(r'(.)\\1{2,}', re.DOTALL) \n        document = pattern.sub(r\"\\1\\1\", document)\n        ## remove usernames\n        document = re.sub(r'@\\S+', '', document)\n        ## remove all digits and numbers\n        document = re.sub(r'\\d+', '', document)\n        \n        \n        # remove special symbols \n        document = re.sub(r'\\W', ' ', document)\n        # replace few spaces to a single one\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n        \n        \n        ### Removed expressions from the previous version\n        \n        # remove individual symbols from the start of the tweet\n        #document = re.sub(r'^[a-zA-Z]\\s+', '', document)\n        # remove individual symbols\n        #document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n        \n        df_new.loc[sen, 'text_lemm'] = document\n        \n    return df_new","21ff6838":"X_train = train.copy()\n\nX_train = text_processing_03(X_train)\nX_train.head(20)","c2de5e3d":"X_test = test.copy()\n\nX_test = text_processing_03(X_test)\nX_test.head(20)","b744581b":"corpus_train_lemm = X_train['text_lemm'].values\n\nall_words_processed = ' '.join(corpus_train_lemm).split(' ')\nunique_words = list(set(all_words_processed))","e2b74137":"len(unique_words)","6cb1ce5c":"serie = pd.Series(all_words_processed)\nserie.value_counts()[-50:]","dfe6990e":"train_bert = X_train[['target','text']]\ntest_bert = X_test[['text']]\n\ny_train = X_train['target']","6bdcf739":"model_class, tokenizer_class, pretrained_weights = (\n    transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","ad449d58":"tokenized_train = train_bert['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\ntokenized_test = test_bert['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","54e3b3aa":"tokenized_train[1]","633bf1d9":"def bert_features(tokenized):\n\n    max_len = 0\n    for i in tokenized.values:\n        if len(i) > max_len:\n            max_len = len(i)\n\n    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n    \n    attention_mask = np.where(padded != 0, 1, 0)\n    \n    batch_size = 1\n    embeddings = []\n    for i in notebook.tqdm(range(padded.shape[0] \/\/ batch_size)):\n            batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n            attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n        \n            with torch.no_grad():\n                batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n        \n            embeddings.append(batch_embeddings[0][:,0,:].numpy())\n    \n    features = np.concatenate(embeddings)\n    \n    return(features)","19e664ab":"X_train_bert = bert_features(tokenized_train) ","08f24b09":"X_test_bert = bert_features(tokenized_test)","d17e7c84":"del tokenized_train, tokenized_test","2a9a828c":"lens = []\n\nfor i in range(len(X_train)):\n    len_row = len(X_train['text_lemm'][i])\n    lens.append(len_row)\n    \nmax(lens)","ee2932b2":"NUM_WORDS = 10000\n\ntokenizer = Tokenizer(num_words=NUM_WORDS)\ntokenizer.fit_on_texts(X_train['text_lemm'].values)\n\ntext_train_keras = tokenizer.texts_to_sequences(X_train['text_lemm'].values)\ntext_test_keras = tokenizer.texts_to_sequences(X_test['text_lemm'].values)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nX_train_keras = pad_sequences(text_train_keras, padding='post', maxlen=148)\nX_test_keras = pad_sequences(text_test_keras, padding='post', maxlen=148)\n\ny_train = X_train['target']","6f5d677e":"X_train_keras[0]","0206df17":"optimizer = Adam(lr=0.0001)","a6cca020":"try:\n    del model3\nexcept:\n    print('yeaaaaahboiiiiii')\n\nmodel3 = Sequential()\nmodel3.add(Embedding(vocab_size, 1000, input_length=148, trainable=True))\nmodel3.add(LSTM(100, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n#model3.add(LSTM(100,return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\nmodel3.add(Dense(50, activation='relu', kernel_initializer='lecun_uniform'))\nmodel3.add(Dense(50, activation='relu', kernel_initializer='lecun_uniform'))\nmodel3.add(Dense(32, activation='relu', kernel_initializer='lecun_uniform'))\nmodel3.add(Dropout(rate=0.2))\nmodel3.add(Dense(1, activation=\"sigmoid\"))\n\nmodel3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","106dab41":"history = model3.fit(X_train_keras, y_train, epochs=20, batch_size=400, validation_split=0.1)","4fc833d2":"model3.evaluate(X_train_keras, y_train)","0f776ee2":"prediction = model3.predict(X_test_keras).round().astype('int')\nsubmission = samp_sub.copy()\nsubmission['target'] = prediction\n    \nsubmission.to_csv('\/kaggle\/working\/ver_2_015.csv', index=False)","5142289d":"SEED=2202","5f8c6096":"X_train_sub, X_valid_sub, y_train_sub, y_valid_sub = train_test_split(X_train_bert, y_train, test_size=0.1, random_state=SEED)","9a7fb105":"cbc = CatBoostClassifier(loss_function='Logloss',\n                    iterations=2000,\n                    learning_rate=0.09,\n                    depth=3,\n                    subsample=0.8,\n                    verbose=100, \n                    grow_policy='Depthwise',\n                    random_state=SEED)\n\ncbc.fit(X_train_sub, y_train_sub)","80733503":"f1_score(y_valid_sub, cbc.predict(X_valid_sub)).round(4)","c3b1e66e":"def plot_hist(history):\n\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.grid()\n    plt.show()\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.grid()\n    plt.show()","5e7658e4":"optimizer = Adam(lr=0.0001)","aad06b9a":"optimizer = SGD(lr=0.001)","5230d326":"try:\n    del model\n    print('refined')\nexcept:\n    print('next')\n\nmodel = Sequential()\n\nmodel.add(Dense(50, input_dim=768, activation='relu', kernel_initializer='lecun_uniform'))\nmodel.add(Dense(50, activation='relu', kernel_initializer='lecun_uniform'))\n\nmodel.add(Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])","927792e0":"history = model.fit(X_train_bert, y_train, epochs=1000, validation_split=0.1, batch_size=300, verbose=0)","eb6e5e78":"plot_hist(history)","00f9ef1f":"X_train_bert.shape","e2ab5e37":"model.summary()","af1446a3":"features_train = X_train_bert.reshape(-1, 768, 1)\nfeatures_train","6afff4f5":"del model2\n\nmodel2 = Sequential()\n\nmodel2.add(Dense(50, input_dim=768, activation='relu', kernel_initializer='lecun_uniform'))\nmodel2.add(Dropout(rate=0.2))\nmodel2.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\nmodel2.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\nmodel2.add(Dropout(rate=0.2))\nmodel2.add(Dense(10, activation='relu'))\nmodel2.add(Dense(1, activation='sigmoid'))\n\nmodel2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\nmodel2.summary()","437bbc08":"history = model2.fit(features_train, y_train, epochs=50, validation_split=0.1, batch_size=100, verbose=1)","4ba04d88":"plot_hist(history)","06185901":"def submission(model, test):\n\n    \n    pred = model.predict(test)\n    \n    submission = samp_sub.copy()\n    submission['target'] = pred\n    \n    submission.to_csv('\/kaggle\/working\/ver_2_014.csv', index=False)\n    \nsubmission(cbc, X_test_bert)","e9b3e26c":"def submission_keras(model, X_test):\n\n    prediction = model.predict(X_test).round().astype('int')\n    submission = samp_sub.copy()\n    submission['target'] = prediction\n    \n    submission.to_csv('\/kaggle\/working\/ver_2_006.csv', index=False)\n\nsubmission_keras(model, X_test_bert)","10dfc27d":"# Bert text preprocessing  \n\nI will use the same model, as ","e9bb5287":"# Data Analysis and preprocessing","24605a76":"### Continue with Keras","c22ac765":"# Submission","8a97cc8c":"I would like to make a list with all unique words in tweets to find more strange units and replace them.","05a95f93":"With a raw tweets there are 28k unique words. Seems like it's not so good.\n\nI'll update the processing algorythm from a previous version.","9e0792f0":"best f1 = 0.7582","960e0259":"# Machine learning  \n\nStart with CatBoost","51845366":"## Work in progress","1a19774c":"# Hallo, my beatiful kagglers!\n\nWelcome to the second edition of a Real Or Not tweets competition notebook.\n\n![](https:\/\/fainaidea.com\/wp-content\/uploads\/2020\/07\/Twitter.jpg)\n\n### Important\nI would like to apply some new (for me) technics without dealing a huge mess in the first version.   \nPlease, check the previous results [here](https:\/\/www.kaggle.com\/kirillklyukvin\/fake-tweets-competition), the 13th version is the most successfull. \n\n---","3454a855":"We remove and substitute ~7k unique units. Time to create emdeddings.","1db2d702":"Transfer all letters to a lower case.","d0803203":"# Libs and Data","20406a69":"# Keras text preprocessing \n\nLet's try to apply some keras preprocessing methods."}}