{"cell_type":{"d0b9e74f":"code","d93dca80":"code","b155db05":"code","b84c0778":"code","083fe080":"code","510ea679":"code","bc72078b":"code","cbf28673":"markdown","8d0875a6":"markdown","26e9a00b":"markdown","bd75940f":"markdown"},"source":{"d0b9e74f":"#based on https:\/\/github.com\/shayneobrien\/generative-models\n\nimport math\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\nfrom itertools import product\nfrom tqdm import tqdm\n\n#import wgan_base as wb","d93dca80":"import math\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\n#import seaborn as sns; sns.set(color_codes=True)\nfrom scipy.stats import kde\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (8,6)\n\ndef sep(data):\n    if(torch.is_tensor(data)):\n        d = data.cpu().detach().numpy()\n    else:\n        d = data\n    x = [d[i][0] for i in range(len(d))]\n    y = [d[i][1] for i in range(len(d))]\n    return x,y\n\ndef to_var(x):\n    \"\"\" Make a tensor cuda-erized and requires gradient \"\"\"\n    return to_cuda(x).requires_grad_()\n\ndef to_cuda(x):\n    \"\"\" Cuda-erize a tensor \"\"\"\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return x\n\ndef get_dataloader(data, BATCH_SIZE=64, tt_split = 0.8):\n    \"\"\" Load data for binared MNIST \"\"\"\n    \n    \n    # split randomized data into train:split*0.9,    val: split*0.1,    test: 1-split\n    train_size = len(data)*tt_split\n    train_dataset = torch.tensor(data[:int(np.ceil(train_size*0.9))]).float()\n    val_dataset = torch.tensor(data[int(np.ceil(train_size*0.9)):int(np.ceil(train_size))]).float()\n    test_dataset = torch.tensor(data[int(np.ceil(train_size)):]).float()\n    \n    train_dataset = to_cuda(train_dataset)\n    val_dataset = to_cuda(val_dataset)\n    test_dataset = to_cuda(test_dataset)\n\n    # Create data loaders\n    train = torch.utils.data.TensorDataset(train_dataset, torch.zeros(train_dataset.shape[0]))\n    val = torch.utils.data.TensorDataset(val_dataset, torch.zeros(val_dataset.shape[0]))\n    test = torch.utils.data.TensorDataset(test_dataset, torch.zeros(test_dataset.shape[0]))\n\n    train_iter = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n    val_iter = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=True)\n    test_iter = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)\n\n    return train_iter, val_iter, test_iter\n\n# Style plots (decorator)\ndef plot_styling(func):\n    def wrapper(*args, **kwargs):\n        style = {'axes.titlesize': 24,\n                 'axes.labelsize': 20,\n                 'lines.linewidth': 3,\n                 'lines.markersize': 10,\n                 'xtick.labelsize': 16,\n                 'ytick.labelsize': 16,\n                 'panel.background': element_rect(fill=\"white\"),\n                 'panel.grid.major': element_line(colour=\"grey50\"),\n                 'panel.grid.minor': element_line(colour=\"grey50\")\n                }\n        with plt.style.context((style)):\n            ax = func(*args, **kwargs)      \n    return wrapper\n\nclass Generator(nn.Module):\n    \"\"\" Generator. Input is noise, output is a generated image.\n    \"\"\"\n    def __init__(self, image_size, hidden_dim, n_layers, z_dim):\n        super(Generator, self).__init__()\n\n        self.inputlayer = nn.Linear(z_dim, hidden_dim)\n        self.linears = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(n_layers)])\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        x = F.relu(self.inputlayer(x))\n        for l in self.linears:\n            x = F.relu(l(x))\n        x = self.generate(x)           # TODO : torch.sigmoid?\n        return x\n        \n\nclass Discriminator(nn.Module):\n    \"\"\" Critic (not trained to classify). Input is an image (real or generated),\n    output is the approximate Wasserstein Distance between z~P(G(z)) and real.\n    \"\"\"\n    def __init__(self, image_size, hidden_dim, n_layers, output_dim):\n        super(Discriminator, self).__init__()\n        \"\"\"   batchnorm\n        self.linears = nn.ModuleList([nn.Linear(image_size, hidden_dim), nn.BatchNorm1d(hidden_dim)])\n        for i in range(n_layers):\n            self.linears.extend([nn.Linear(hidden_dim, hidden_dim), nn.BatchNorm1d(hidden_dim)])\n        \"\"\"\n        self.inputlayer = nn.Linear(image_size, hidden_dim)\n        self.linears = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(n_layers)])\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        \"\"\"  batchnorm\n        for i, l in enumerate(self.linears):\n            if i % 2 == 0:\n                x = self.linears[i+1](F.relu(l(x)))\n        x = self.discriminate(x)\n        return x\n        \n        \"\"\"\n        x = F.relu(self.inputlayer(x))\n        for l in self.linears:\n            x = F.relu(l(x))\n        x = self.discriminate(x)   # TODO : torch.sigmoid?\n        return x\n    \nclass vanilla_Discriminator(nn.Module):\n    \"\"\" Critic (not trained to classify). Input is an image (real or generated),\n    output is the approximate Wasserstein Distance between z~P(G(z)) and real.\n    \"\"\"\n    def __init__(self, image_size, hidden_dim, n_layers, output_dim):\n        super(vanilla_Discriminator, self).__init__()\n        \"\"\"   batchnorm\n        self.linears = nn.ModuleList([nn.Linear(image_size, hidden_dim), nn.BatchNorm1d(hidden_dim)])\n        for i in range(n_layers):\n            self.linears.extend([nn.Linear(hidden_dim, hidden_dim), nn.BatchNorm1d(hidden_dim)])\n        \"\"\"\n        self.inputlayer = nn.Linear(image_size, hidden_dim)\n        self.linears = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(n_layers)])\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        \"\"\"  batchnorm\n        for i, l in enumerate(self.linears):\n            if i % 2 == 0:\n                x = self.linears[i+1](F.relu(l(x)))\n        x = self.discriminate(x)\n        return x\n        \n        \"\"\"\n        x = F.relu(self.inputlayer(x))\n        for l in self.linears:\n            x = F.relu(l(x))\n        x = torch.sigmoid(self.discriminate(x))  # TODO : torch.sigmoid?\n        return x\n    \n\nclass WGAN(nn.Module):\n    \"\"\" Super class to contain both Discriminator (D) and Generator (G)\n    \"\"\"\n    def __init__(self, image_size, hidden_dim, n_layers, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, n_layers, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, n_layers, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n        \nclass GAN(nn.Module):\n    \"\"\" Super class to contain both Discriminator (D) and Generator (G)\n    \"\"\"\n    def __init__(self, image_size, hidden_dim, n_layers, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, n_layers, z_dim)\n        self.D = vanilla_Discriminator(image_size, hidden_dim, n_layers, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass GANTrainer:\n    \"\"\" Object to hold data iterators, train a GAN variant\n    \"\"\"\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False, gantype = 'wgangp'):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n        self.gantype = gantype\n        self.G_iter = 0\n\n    def train(self, num_epochs, G_lr=5e-5, D_lr=5e-5, G_wd = 0, D_steps_standard=5, clip=0.01, G_init=5, G_per_D = 1):\n        \"\"\" Train a Wasserstein GAN\n            Logs progress using G loss, D loss, G(x), D(G(x)), visualizations\n            of Generator output.\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator's RMProp optimizer\n            D_lr: float, learning rate for discriminator's RMSProp optimizer\n            D_steps: int, ratio for how often to train D compared to G\n            clip: float, bound for parameters [-c, c] to enforce K-Lipschitz\n        \"\"\"\n        # Initialize optimizers\n        if(self.gantype in ['wgan', 'wgangp', 'wganlp', 'ls']):\n            bet = (0.5,0.9)\n        else:\n            bet = (0.5, 0.9)\n        \n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr, weight_decay = G_wd,betas=bet)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr,betas=bet)\n\n        num_batches = len(self.train_iter)\n        D_steps = D_steps_standard\n        self.model.train()\n        if(self.gantype in ['gan','nsgan']):\n                self.pretrain(G_init, G_optimizer)\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n            #Train discriminator to almost convergence to approx W\n            if(self.gantype in ['wgangp', 'wgan','wganlp']):\n                if( (self.G_iter <= 25) or (self.G_iter % 100 == 0) ):\n                    D_steps = 100\n                else:               \n                    D_steps = D_steps_standard\n            else:\n                D_steps = D_steps_standard\n                    \n\n            self.model.train()\n            G_losses, D_losses = [], []\n            ep_iter = 0\n\n            while(ep_iter < num_batches):\n\n                if(self.G_iter % G_per_D == 0):\n                    D_step_loss = []\n\n                    for _ in range(D_steps):\n\n                        # Reshape images\n                        images = self.process_batch(self.train_iter)\n\n                        # TRAINING D: Zero out gradients for D\n                        D_optimizer.zero_grad()\n\n                        # Train the discriminator to approximate the Wasserstein\n                        # distance between real, generated distributions\n                        if(self.gantype == 'wgangp'):\n                            D_loss = self.train_D_GP(images)\n                        elif(self.gantype == 'wganlp'):\n                            D_loss = self.train_D_LP(images, LAMBDA = 1)\n                        elif(self.gantype == 'wgan'):\n                            D_loss = self.train_D_W(images)\n                        elif(self.gantype in ['nsgan','gan']):\n                                D_loss = self.train_D_vanilla(images)\n                        elif(self.gantype == 'ls'):\n                            D_loss = self.train_D_ls(images)\n                        else:\n                            print('Unknown gantype')\n                            break\n\n\n                        # Update parameters\n                        D_loss.backward()\n                        D_optimizer.step()\n\n                        # Log results, backpropagate the discriminator network\n                        D_step_loss.append(D_loss.item())\n                        ep_iter += 1\n\n                        if(self.gantype == 'wgan'):\n                            # Clamp weights (crudely enforces K-Lipschitz)\n                            self.clip_D_weights(clip)\n\n                    # We report D_loss in this way so that G_loss and D_loss have\n                    # the same number of entries.\n                    D_losses.append(np.mean(D_step_loss))\n                else:\n                    images = self.process_batch(self.train_iter)\n\n                '''\n                # Visualize generator progress\n                if self.viz:\n                    if(self.G_iter < 200 and self.G_iter % 10 == 0):\n                        self.viz_data(save = True)\n                    elif(self.G_iter >200 and self.G_iter % 200 == 0):\n                        self.viz_data(save=True)\n                '''\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                if(self.gantype in ['wgan', 'wgangp', 'wganlp']):\n                    # Train the generator to (roughly) minimize the approximated\n                    # Wasserstein distance\n                    G_loss = self.train_G_W(images)\n                elif(self.gantype == 'gan'):\n                    G_loss = self.train_G_vanilla(images)\n                elif(self.gantype == 'nsgan'):\n                    G_loss = self.train_G_ns(images)\n                elif(self.gantype == 'ls'):\n                    G_loss = self.train_G_ls(images)\n                else:\n                    print('Unknown gantype')\n                    break\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n                self.G_iter += 1\n                \n            \n\n            # Save progress\n            if self.viz:\n                    if(self.num_epochs <= 30 and self.num_epochs % 5 == 0):\n                        self.viz_data(save = True)\n                    elif(self.num_epochs > 30 and self.num_epochs < 101 and self.num_epochs % 10 == 0):\n                        self.viz_data(save = True)\n                    elif(self.num_epochs > 100 and self.num_epochs % 20 == 0):\n                        self.viz_data(save = True)\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n            self.num_epochs += 1\n\n            \"\"\"\n            # Progress logging\n            print (\"Epoch[%d\/%d], G Loss: %.4f, D Loss: %.4f\"\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n            \"\"\"\n            \n            \n    \n    def train_D_GP(self, images, LAMBDA=0.1):\n        \"\"\" Run 1 step of training for discriminator\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: Wasserstein loss for discriminator,\n            -E[D(x)] + E[D(G(z))] + \u03bbE[(||\u2207 D(\u03b5x + (1 \u2212 \u03b5G(z)))|| - 1)^2]\n        \"\"\"\n        # ORIGINAL CRITIC STEPS:\n        # Sample noise, an output from the generator\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Use the discriminator to sample real, generated images\n        DX_score = self.model.D(images) # D(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # GRADIENT PENALTY:\n        # Uniformly sample along one straight line per each batch entry.\n        epsilon = to_var(torch.rand(images.shape[0], 1).expand(images.size()))\n\n        # Generate images from the noise, ensure unit gradient norm 1\n        # See Section 4 and Algorithm 1 of original paper for full explanation.\n        G_interpolation = epsilon*images + (1-epsilon)*G_output\n        D_interpolation = self.model.D(G_interpolation)\n\n        # Compute the gradients of D with respect to the noise generated input\n        weight = to_cuda(torch.ones(D_interpolation.size()))\n\n        gradients = torch.autograd.grad(outputs=D_interpolation,\n                                        inputs=G_interpolation,\n                                        grad_outputs=weight,\n                                        only_inputs=True,\n                                        create_graph=True,\n                                        retain_graph=True)[0]\n\n        # Full gradient penalty\n        grad_penalty = LAMBDA * torch.mean((gradients.norm(2, dim=1) - 1)**2)\n\n        # Compute WGAN-GP loss for D\n        D_loss = torch.mean(DG_score) - torch.mean(DX_score) + grad_penalty\n\n        return D_loss\n    \n    def train_D_LP(self, images, LAMBDA=0.1):\n        \"\"\" Run 1 step of training for discriminator\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: Wasserstein loss for discriminator,\n            -E[D(x)] + E[D(G(z))] + \u03bbE[max(0,\u2207 D(\u03b5x + (1 \u2212 \u03b5G(z))) - 1)^2]\n        \"\"\"\n        # ORIGINAL CRITIC STEPS:\n        # Sample noise, an output from the generator\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Use the discriminator to sample real, generated images\n        DX_score = self.model.D(images) # D(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # GRADIENT PENALTY:\n        # Uniformly sample along one straight line per each batch entry.\n        epsilon = to_var(torch.rand(images.shape[0], 1).expand(images.size()))\n\n        # Generate images from the noise, ensure unit gradient norm 1\n        # See Section 4 and Algorithm 1 of original paper for full explanation.\n        G_interpolation = epsilon*images + (1-epsilon)*G_output\n        D_interpolation = self.model.D(G_interpolation)\n\n        # Compute the gradients of D with respect to the noise generated input\n        weight = to_cuda(torch.ones(D_interpolation.size()))\n\n        gradients = torch.autograd.grad(outputs=D_interpolation,\n                                        inputs=G_interpolation,\n                                        grad_outputs=weight,\n                                        only_inputs=True,\n                                        create_graph=True,\n                                        retain_graph=True)[0]\n\n        # Full gradient penalty\n        zer = torch.zeros(gradients.norm(2,dim=1).shape[0])\n        grad_penalty = LAMBDA * torch.mean((torch.max(zer, gradients.norm(2, dim=1) - 1))**2)\n\n        # Compute WGAN-GP loss for D\n        D_loss = torch.mean(DG_score) - torch.mean(DX_score) + grad_penalty\n\n        return D_loss\n    \n\n    def train_D_W(self, images):\n        \"\"\" Run 1 step of training for discriminator\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: wasserstein loss for discriminator,\n            -E[D(x)] + E[D(G(z))]\n        \"\"\"\n        # Sample from the generator\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Score real, generated images\n        DX_score = self.model.D(images) # D(x), \"real\"\n        DG_score = self.model.D(G_output) # D(G(x')), \"fake\"\n\n        # Compute WGAN loss for D\n        D_loss = -1 * (torch.mean(DX_score)) + torch.mean(DG_score)\n\n        return D_loss\n    \n    def train_D_vanilla(self, images):\n        \"\"\" Run 1 step of training for discriminator\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: non-saturing loss for discriminator,\n            -E[log(D(x))] - E[log(1 - D(G(z)))]\n        \"\"\"\n\n        # Sample noise z, generate output G(z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Classify the generated and real batch images\n        DX_score = self.model.D(images) # D(x)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute vanilla (original paper) D loss\n        D_loss = -torch.mean(torch.log(DX_score + 1e-8)) + torch.mean(torch.log(1 - DG_score + 1e-8))\n        return D_loss\n    \n    def train_D_ls(self, images):\n        # Sample noise z, generate output G(z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Classify the generated and real batch images\n        DX_score = self.model.D(images) # D(x)\n        DG_score = self.model.D(G_output) # D(G(z))\n        \n        D_loss = 0.5 * (torch.mean(torch.square(DX_score-1)) + torch.mean(torch.square(DG_score)))\n        return D_loss\n\n    def train_G_W(self, images):\n        \"\"\" Run 1 step of training for generator\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            G_loss: wasserstein loss for generator,\n            -E[D(G(z))]\n        \"\"\"\n        # Get noise, classify it using G, then classify the output of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute WGAN loss for G\n        G_loss = -1 * (torch.mean(DG_score))\n\n        return G_loss\n    \n\n    def train_G_ns(self, images):\n        \"\"\" Run 1 step of training for generator\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: non-saturating loss for how well G(z) fools D,\n            -E[log(D(G(z)))]\n        \"\"\"\n\n        # Get noise (denoted z), classify it using G, then classify the output\n        # of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # (z)\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute the non-saturating loss for how D did versus the generations\n        # of G using sigmoid cross entropy\n        G_loss = -torch.mean(torch.log(DG_score + 1e-8))\n\n        return G_loss\n    \n    def train_G_vanilla(self, images):\n        \"\"\" Run 1 step of training for generator\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: minimax loss for how well G(z) fools D,\n            E[log(1-D(G(z)))]\n        \"\"\"\n        # Get noise (denoted z), classify it using G, then classify the output of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute the minimax loss for how D did versus the generations of G using sigmoid cross entropy\n        G_loss = torch.mean(torch.log((1-DG_score) + 1e-8))\n\n        return G_loss\n    \n    def train_G_ls(self, images):\n        # Sample noise z, generate output G(z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Classify the generated and real batch images\n        DG_score = self.model.D(G_output) # D(G(z))\n        \n        G_loss = 0.5 * torch.mean(torch.square(DG_score-1))\n        return G_loss\n    \n    def pretrain(self, G_init, G_optimizer):\n        # Let G train for a few steps before beginning to jointly train G\n        # and D because MM GANs have trouble learning very early on in training\n        if G_init > 0:\n            for _ in range(G_init):\n                # Process a batch of images\n                images = self.process_batch(self.train_iter)\n\n                # Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Pre-train G\n                G_loss = self.train_G_vanilla(images)\n\n                # Backpropagate the generator network\n                G_loss.backward()\n                G_optimizer.step()\n\n            print('G pre-trained for {0} training steps.'.format(G_init))\n        else:\n            print('G not pre-trained -- GAN unlikely to converge.')\n\n    def compute_noise(self, batch_size, z_dim):\n        \"\"\" Compute random noise for input into the Generator G \"\"\"\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        \"\"\" Generate a process batch to be input into the Discriminator D \"\"\"\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def clip_D_weights(self, clip):\n        for parameter in self.model.D.parameters():\n            parameter.data.clamp_(-clip, clip)\n\n    def generate_images(self, epoch=-1, num_outputs=50):\n        \"\"\" Visualize progress of generator learning \"\"\"\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n        \n        return images\n\n        \"\"\"\n        # Reshape to square image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap='gray')\n            k += 1\n\n\n        # Save images if desired\n        if save:\n            outname = '..\/viz\/' + self.name + '\/'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + 'reconst_%d.png'\n                                         %(epoch), nrow=grid_size)\n        \"\"\"\n\n    def viz_loss(self, save = False):\n        \"\"\" Visualize loss for the generator, discriminator \"\"\"\n        # Set style, figure size\n        plt.style.use('ggplot')\n        plt.rcParams[\"figure.figsize\"] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.G_iter, len(self.Dlosses)),\n                 self.Dlosses,\n                 'b')\n        print(self.num_epochs,self.G_iter, len(self.Dlosses))\n\n        \"\"\"\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 'g')\n\n        # Add legend, title\n        plt.legend(['Discriminator', 'Generator'])\n        \"\"\"\n        #plt.title(self.name)\n        if save:\n            plt.savefig('loss_%s_%d.png'%(self.gantype, self.G_iter), dpi = 150)\n        plt.show()\n        \n        \n    def viz_data(self, save = False, density=True):\n        if(density == True):\n            lower = (-1.3, -1.3)\n            upper = (1.3, 1.3)\n            nbins=300\n            x,y = sep(self.generate_images(num_outputs = 1000))\n            k = kde.gaussian_kde([x,y])\n            xi, yi = np.mgrid[lower[0]:upper[1]:nbins*1j, lower[1]:upper[1]:nbins*1j]\n            zi = k(np.vstack([xi.flatten(), yi.flatten()]))\n\n            fig, ax = plt.subplots()\n\n            # a (1-alpha)-confidence circle for 2d gaussian with cov = a * eyes has radius sqrt(-2 a ln(alpha)), here a = 3\/400\n            ax.pcolormesh(xi, yi, zi.reshape(xi.shape), cmap=plt.cm.Blues)\n            for i in range(len(means)):\n                circle = plt.Circle((means[i][0]\/10, means[i][1]\/10),           # (x,y)\n                np.sqrt(-3 * np.log(0.05)\/200), color='black',linewidth = 2, fill=False)\n                ax.add_artist(circle)\n            #plt.colorbar()\n            \n        else:\n            # 2-dim points rowwise concatenated in array\n            # Set style, figure size\n            plt.style.use('ggplot')\n            plt.rcParams[\"figure.figsize\"] = (8,6)\n\n            # Plot generated points in red\n            gen = self.generate_images()\n            xg = [gen[i][0] for i in range(len(gen))]\n            yg = [gen[i][1] for i in range(len(gen))]\n            plt.plot(xg,\n                     yg,\n                     'ro', markersize = 3)\n\n\n            # Plot real data points in green\n            real = next(iter(train_iter))[0]\n            xr = [real[i][0] for i in range(len(real))]\n            yr = [real[i][1] for i in range(len(real))]\n            plt.plot(xr,\n                     yr,\n                     'go', markersize = 3)\n\n            # Add legend, title\n            plt.legend(['generated', 'real'])\n        \n        #plt.title(self.name)\n        if save:\n            plt.savefig('data_%s_%d_%d.png'%(self.gantype,self.num_epochs, self.G_iter), dpi = 150)\n        plt.show()\n        \n\n    def save_model(self):\n        \"\"\" Save model state dictionary \"\"\"\n        torch.save(self.model.state_dict(), 'model_%s_%d'%(self.gantype, self.G_iter))\n\n    def load_model(self, loadpath):\n        \"\"\" Load state dictionary into model \"\"\"\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)","b155db05":"images = torch.tensor([[2,1],[3,4],[5,7]])\n\ntorch.rand(images.shape[0], 1).expand(images.size())","b84c0778":"#create circle of Normals data\nn_groups = 8\nn_pergroup= 1000\nresc = ''\n\nangles = np.arange(0, 2*math.pi, step = 2*math.pi \/ n_groups)\nmeans = [[10*np.cos(i), 10*np.sin(i)] for i in angles]\n\n\ndata = []\nfor i in range(n_groups):\n    data.extend(multivariate_normal.rvs(means[i], cov = 0.75 * np.eye(2), size = n_pergroup))\n    \ndata = np.array(data)\nif(resc == 'comp'):\n    # rescale to [0,1]^2\n    scales = np.amax(data, axis = 0)-np.amin(data,axis = 0)\n    data = data - np.amin(data,axis = 0)\n    data[:,0] = data[:,0] \/ scales[0]\n    data[:,1] = data[:,1] \/ scales[1]\nelse:\n    data \/= 10\n\n#shuffle rows\nnp.random.shuffle(data)\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (8,6)\nx,y = sep(data)\nfig1 = plt.gcf()\nplt.plot(x,y, 'bo', markersize = 3)\nfig1.savefig('orig_data', dpi = 150)","083fe080":"# see clip below, original architecture reaches magnitude of values as Lip1 functions\n(1\/(2 * np.sqrt(2)*512 ** 4)) ** (1\/5) # close to 0.01...","510ea679":"torch.manual_seed(34)\n# separate data into data loaders\ntrain_iter, val_iter, test_iter = get_dataloader(data)\n\ngantypes = ['gan', 'nsgan', 'ls', 'wgan', 'wgangp', 'wganlp']\ngantypes = [ 'gan', 'nsgan']\nclip =0.01\nDlosses=[]\nGlosses=[]\n\n\nfor gantype in gantypes:\n    if(gantype in ['gan', 'nsgan']):\n        # Init model\n        model = GAN(image_size=2, hidden_dim=256, n_layers = 3, z_dim=10)\n    else:\n        model = WGAN(image_size=2, hidden_dim=256, n_layers = 3, z_dim=10)\n\n        \n    if(gantype == 'wgan'):\n        # choose good clip for architecture: d,p,L+1 (range should be +- sqrt(d)\/2 and is +-c**(L+1) d p**L )\n        L = model.n_layers+1\n        d = model.image_size\n        p = model.hidden_dim\n        clip = 2 * (1\/(2 * np.sqrt(d)* p ** L))**(1\/(L+1))\n\n    # Init trainer\n    trainer = GANTrainer(model=model, train_iter=train_iter, val_iter=val_iter, test_iter=test_iter, viz=True, gantype = gantype)\n\n    G_lr = 1e-4\n    D_lr = 1e-4\n    G_wd = 0\n    if(gantype in ['gan','nsgan']):\n        perD = 2\n        G_lr = 1e-5\n        D_lr = 1e-5\n\n    if(gantype == 'wgan'):\n        G_lr = 5e-5\n        D_lr = 5e-5\n        G_wd = 0.01\n        #new: GAN, NSGAN same LR\n    \n    if(gantype in ['wgan', 'wgangp','wganlp']):\n        D_steps_standard = 5\n    else:\n        D_steps_standard = 1\n\n\n    # Train\n    trainer.train(num_epochs=100, G_lr=G_lr, D_lr=D_lr,G_wd = G_wd, D_steps_standard=D_steps_standard, clip=clip, G_init = 20, G_per_D = perD)\n    trainer.viz_loss(save = True)\n    trainer.viz_data()\n    trainer.train(num_epochs=200, G_lr=1e-4, D_lr=1e-4,G_wd = 0, D_steps_standard=D_steps_standard, clip=clip)\n    trainer.viz_loss(save = True),\n    trainer.viz_data(save = True)\n    trainer.save_model()\n    Dlosses.append(trainer.Dlosses)\n    Glosses.append(trainer.Glosses)\n    \ntorch.save(Dlosses, 'Dlosses')\ntorch.save(Glosses, 'Glosses')","bc72078b":"# looks too linear (does relu work??? yes), prefers small scale\n\n# discriminator doesn't learn, loss too close to 0,   -loss != W(P_r,P_g)\n\n# too little training? no, bad clips -> batch norm\n\n# normalize losses for suitable gradients (change for architecture and clip)\n\n# for wc use batchnorm -> then implicitly not one to one function, but batch to batch and Lipschitz constant depends on batch\n\n# gp recommend layer norm\n\n# wc: 5e-5, batchnorm, standard adam?     gp: 1e-4, paper: beta = (0,0.9), implem: (0.5,0.9), wc and lsgan: RMSProp 5e-5, 1e-4 respectively (GP paper)\n\n# if batchnorm or layernorm, use 0.001 wd in discriminator (GP paper)","cbf28673":"# Create Data","8d0875a6":"# Train Model","26e9a00b":"## kde plot\n\ngen = trainer.generate_images()\n#print(gen)\nx,y = sep(gen)\n#ax = sns.kdeplot(x, y, cmap = 'Blues', shade=True)\n\n### cov = 0.75 eyes \/100 = 3 eyes \/400\n#sns.jointplot(sep(means)[0],sep(means)[1])\nlower = (-1.2, -1.2)\nupper = (1.2, 1.2)\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[lower[0]:upper[1]:nbins*1j, lower[1]:upper[1]:nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n\nfig, ax = plt.subplots()\n\nax.pcolormesh(xi, yi, zi.reshape(xi.shape))\nfor i in range(len(means)):\n    circle = plt.Circle((means[i][0]\/10, means[i][1]\/10),           # (x,y)\n    2 * np.sqrt(3\/400), color='b', fill=False)\n    ax.add_artist(circle)\n#plt.colorbar()\nplt.show()","bd75940f":"# Script code"}}