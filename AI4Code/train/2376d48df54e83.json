{"cell_type":{"a56a7f02":"code","5a6e25c5":"code","b0509845":"code","38ab2aa4":"code","b30e0673":"code","f97606b5":"code","fea7bccd":"code","ee1a098e":"code","d5445d5c":"code","534799b1":"code","4090736e":"code","a7ab1c7f":"code","6f978fdf":"code","cacc2148":"code","ac7f1a7a":"code","6ae336ee":"code","e04bb94d":"code","24958ebd":"code","b9397b15":"code","01dedf00":"code","9d83a079":"code","775a6271":"code","2caeca95":"code","a363bf84":"code","651c1f1b":"code","743838f7":"code","eb04cd8e":"code","613d8b46":"code","9940011c":"code","38d43b1b":"code","e6266516":"code","696bf746":"code","e69df8e7":"code","6b6c6580":"code","19c0b5c7":"code","d6c0b5cd":"code","13f46cef":"code","a146b73d":"code","ed7a8c39":"code","ad029192":"code","ea4b94dc":"code","85e34e9b":"code","d9ab70a0":"code","31379a60":"code","48a0e8c6":"code","5f1325b6":"code","7785f29a":"code","00d69eee":"code","89b4d933":"code","a8f7522b":"code","5895fc1d":"code","f844cd88":"code","84623c61":"code","118c5b19":"code","3d462c1e":"code","8f4947ab":"code","a7f75d5f":"code","cc45c371":"code","ac895314":"code","88dc3eb1":"code","47910957":"code","e577b766":"markdown","29d72393":"markdown","94af5293":"markdown","2c8fe13d":"markdown","fdf1abc7":"markdown","be8dd376":"markdown","bb8b1957":"markdown","fa9bfed2":"markdown","cade82d4":"markdown","5772e3c8":"markdown","ef2d8a7b":"markdown","8979f747":"markdown","b17fc97f":"markdown","83fd0d22":"markdown","b0c022e4":"markdown","233eca78":"markdown","20bc91ec":"markdown","a6ed0376":"markdown","ed5923e6":"markdown","ed429566":"markdown","91507ba2":"markdown","8220721f":"markdown","509a2377":"markdown","452c3a3a":"markdown","f18fe73d":"markdown","179581f3":"markdown","04fff79f":"markdown","4b45ef09":"markdown","a551a9a4":"markdown","60058a87":"markdown","1fcf9520":"markdown","1a7ba07e":"markdown","76c3a425":"markdown","86ffd735":"markdown","33365e76":"markdown","67c8971f":"markdown","737d987a":"markdown","2658b06a":"markdown","d472937a":"markdown","77bd207a":"markdown","69c9a884":"markdown"},"source":{"a56a7f02":"import warnings\nwarnings.simplefilter('ignore')\n\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve\nfrom sklearn.utils import shuffle\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport eli5\n\nimport riiideducation\n\n%matplotlib inline\n# for heatmap and other plots\ncolorMap1 = sns.color_palette(\"RdBu_r\")\n# for countplot and others plots\ncolorMap2 = 'Blues_r'\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/riiid-test-answer-prediction'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5a6e25c5":"train_path = \"..\/input\/riiid-train-pickled-data\/train.pkl.gzip\"\nquestions_path = \"..\/input\/riiid-test-answer-prediction\/questions.csv\"\nlectures_path = \"..\/input\/riiid-test-answer-prediction\/lectures.csv\"\n\ntest = \"..\/input\/riiid-test-answer-prediction\/example_test.csv\"","b0509845":"%%time\n\ndf = pd.read_pickle(train_path)","38ab2aa4":"print(f\"Train shape: {df.shape}\")","b30e0673":"df.head(10)","f97606b5":"df.memory_usage()","fea7bccd":"df.drop(['row_id', 'timestamp'], axis=1, inplace=True)","ee1a098e":"df.describe().style.background_gradient(cmap='Blues')","d5445d5c":"print(f'Number of unique users: {len(np.unique(df.user_id))}')","534799b1":"print(df.isnull().sum() \/ len(df))","4090736e":"corr_matrix = df.corr()\ncorr_matrix[\"answered_correctly\"].sort_values(ascending=False)","a7ab1c7f":"plt.figure(figsize=(13, 10))\nsns.heatmap(corr_matrix, annot=True, \n            linewidths=.5, cmap=colorMap1)","6f978fdf":"plt.figure(figsize=(15, 10))\nax = sns.countplot(x=\"prior_question_elapsed_time\", \n                   data=df[df['prior_question_elapsed_time'].notnull()],\n                   palette=colorMap2)","cacc2148":"freq_answered_tasks = df['task_container_id'].value_counts().reset_index()\nfreq_answered_tasks.columns = [\n    'task_container_id', \n    'freq'\n]\n\ndf['freq_task_id'] = ''\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] < 10000]['task_container_id'].values), 'freq_task_id'] = 'very rare answered'\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 10000]['task_container_id'].values), 'freq_task_id'] = 'rare answered'\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 50000]['task_container_id'].values), 'freq_task_id'] = 'normal answered'\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 200000]['task_container_id'].values), 'freq_task_id'] = 'often answered'\ndf.loc[df['task_container_id'].isin(freq_answered_tasks[freq_answered_tasks['freq'] >= 400000]['task_container_id'].values), 'freq_task_id'] = 'very often answered'","ac7f1a7a":"df.sample(5)","6ae336ee":"plt.figure(figsize=(15, 10))\nsns.countplot(x='freq_task_id', hue='answered_correctly', data=df, palette=colorMap2)","e04bb94d":"plt.figure(figsize=(15, 11))\nax = sns.countplot(x=\"prior_question_had_explanation\", hue=\"answered_correctly\", data=df[df['prior_question_had_explanation'].notnull()], palette=colorMap2)","24958ebd":"N = 30 # number of users\n\nuser_freq = df['user_id'].value_counts().reset_index()\nuser_freq.columns = [\n    'user_id', \n    'count'\n]\n\n# Add ' - ' to convert user_id to str and not sort\nuser_freq['user_id'] = user_freq['user_id'].astype(str) + ' - '\nuser_freq = user_freq.sort_values(['count'], ascending=False).head(N)\n\nplt.figure(figsize=(15, 15))\nsns.barplot(x='count', y='user_id', data=user_freq, orient='h', palette=colorMap2)\nplt.title(f'Top {N} the most active users', fontsize=14)","b9397b15":"N = 30 # number of users\n\ncontent_id_freq = df['content_id'].value_counts().reset_index()\ncontent_id_freq.columns = [\n    'content_id', \n    'count'\n]\n\n# Add ' - ' to convert content_id to str and not sort\ncontent_id_freq['content_id'] = content_id_freq['content_id'].astype(str) + ' - '\ncontent_id_freq = content_id_freq.sort_values(['count'], ascending=False).head(N)\n\nplt.figure(figsize=(15, 15))\nsns.barplot(x='count', y='content_id', data=content_id_freq, orient='h', palette=colorMap2)\nplt.title(f'Top {N} the most useful content_id', fontsize=14)","01dedf00":"content_type_freq = df['content_type_id'].value_counts().reset_index()\ncontent_type_freq.columns = ['content_type_id',\n                             'share']\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='content_type_id', y='share', data=content_type_freq,\n            palette=colorMap2)\nplt.title('Share of the Questions (0) and Video lectures (1)', fontsize=14)","9d83a079":"df = df[df['answered_correctly'] != -1].reset_index(drop=True, inplace=False)","775a6271":"df.groupby(['content_type_id', 'answered_correctly']).agg({'answered_correctly': 'count'})","2caeca95":"task_ids_freq = df['task_container_id'].value_counts().reset_index()\ntask_ids_freq.columns = ['task_container_id', 'count']\n\nfig, ax = plt.subplots(figsize=(15, 10))\n\nsns.pointplot(x='task_container_id', y='count', data=task_ids_freq, palette=colorMap2)\nxticks_range = range(min(task_ids_freq['task_container_id']), \n                     max(task_ids_freq['task_container_id']),\n                     1000)\nplt.xticks(list(xticks_range), list(xticks_range))","a363bf84":"plt.figure(figsize=(15, 11))\nax = sns.countplot(x=\"user_answer\", hue=\"answered_correctly\", data=df, palette=colorMap2)\n\nfor i, p in enumerate(ax.patches):\n    x = p.get_bbox().get_points()[:,0]\n    y = p.get_bbox().get_points()[1,1]\n    if i > 3:\n        i -= 4\n    ax.annotate('{:.1f}%'.format(100.*y\/len(np.where(df['user_answer'] == i)[0])), (x.mean(), y), \n                ha='center', va='bottom') # set the alignment of the text","651c1f1b":"questions = pd.read_csv(questions_path)\nquestions.head(10)","743838f7":"questions.describe().style.background_gradient(cmap='Blues')","eb04cd8e":"print(questions.isnull().sum() \/ len(questions))","613d8b46":"part_freq = questions['part'].value_counts().reset_index()\npart_freq.columns = [\n    'part', \n    'count'\n]\n\n# Add ' - ' to convert content_id to str and not sort\npart_freq['part'] = part_freq['part'].astype(str) + ' - '\npart_freq = part_freq.sort_values(['count'], ascending=False).head(N)\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='count', y='part', data=part_freq, orient='h', palette=colorMap2)\nplt.title(f'The most frequent parts', fontsize=14)","9940011c":"N = 30\n\ntags_freq = questions['tags'].value_counts().reset_index()\ntags_freq.columns = [\n    'tag',\n    'count'\n]\n\n# Add ' - ' to convert content_id to str and not sort\ntags_freq['tag'] = tags_freq['tag'].astype(str) + ' - '\ntags_freq = tags_freq.sort_values(['count'], ascending=False).head(N)\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='count', y='tag', data=tags_freq, orient='h', palette=colorMap2)\nplt.title(f'Top {N} the most frequent tags', fontsize=14)","38d43b1b":"N = 30\n\ntags = questions['tags'].str.split(' ').explode('tags').reset_index()\ntags_freq = tags['tags'].value_counts().reset_index()\ntags_freq.columns = [\n    'tag',\n    'count'\n]\n\n# Add ' - ' to convert content_id to str and not sort\ntags_freq['tag'] = tags_freq['tag'].astype(str) + ' - '\ntags_freq = tags_freq.sort_values(['count'], ascending=False).head(N)\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='count', y='tag', data=tags_freq, orient='h', palette=colorMap2)\nplt.title(f'Top {N} the most frequent tags', fontsize=14)","e6266516":"lectures = pd.read_csv(lectures_path)\nlectures.head(10)","696bf746":"part_freq = lectures['part'].value_counts().reset_index()\npart_freq.columns = [\n    'part', \n    'count'\n]\n\n# Add ' - ' to convert content_id to str and not sort\npart_freq['part'] = part_freq['part'].astype(str) + ' - '\npart_freq = part_freq.sort_values(['count'], ascending=False).head(N)\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='count', y='part', data=part_freq, orient='h', palette=colorMap2)\nplt.title(f'The most frequent parts', fontsize=14)","e69df8e7":"N = 30\n\ntags_freq = lectures['tag'].value_counts().reset_index()\ntags_freq.columns = [\n    'tag',\n    'count'\n]\n\n# Add ' - ' to convert content_id to str and not sort\ntags_freq['tag'] = tags_freq['tag'].astype(str) + ' - '\ntags_freq = tags_freq.sort_values(['count'], ascending=False).head(N)\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='count', y='tag', data=tags_freq, orient='h', palette=colorMap2)\nplt.title(f'Top {N} the most frequent tags', fontsize=14)","6b6c6580":"lectures['type_of'].value_counts()","19c0b5c7":"test_example = pd.read_csv(test)","d6c0b5cd":"test_example.head(10)","13f46cef":"n = int(df.shape[0] * 0.1)\ntrain = df.sample(n=n, random_state=42)","a146b73d":"del questions\ndel lectures","ed7a8c39":"user_characteristics = df.groupby('user_id').agg({'answered_correctly':\n                                                  ['mean', 'median', 'std', 'skew', 'count']})\nuser_characteristics.columns = [\n    'mean_user_acc',\n    'median_user_acc',\n    'std_user_acc',\n    'skew_user_acc',\n    'number_of_answered_q'\n]","ad029192":"user_characteristics.head(5)","ea4b94dc":"task_container_characteristics = df.groupby('task_container_id').agg({'answered_correctly':\n                                                                      ['mean', 'median', 'std', 'skew', 'count']})\ntask_container_characteristics.columns = [\n    'mean_task_acc',\n    'median_task_acc',\n    'std_task_acc',\n    'skew_task_acc',\n    'number_of_asked_task_containers'\n]","85e34e9b":"task_container_characteristics.head(5)","d9ab70a0":"content_characteristics = df.groupby('content_id').agg({'answered_correctly':\n                                                        ['mean', 'median', 'std', 'skew', 'count']})\ncontent_characteristics.columns = [\n    'mean_acc',\n    'median_acc',\n    'std_acc',\n    'skew_acc',\n    'number_of_asked_q'\n]","31379a60":"content_characteristics.head(5)","48a0e8c6":"df = train.copy()\ndel train","5f1325b6":"df = df.merge(user_characteristics, how='left', on='user_id')\ndf = df.merge(task_container_characteristics, how='left', on='task_container_id')\ndf = df.merge(content_characteristics, how='left', on='content_id')","7785f29a":"features = [\n    'prior_question_elapsed_time', \n    'prior_question_had_explanation',\n    'mean_user_acc',\n    'median_user_acc',\n    'std_user_acc',\n    'skew_user_acc',\n    'number_of_answered_q',\n    'mean_task_acc',\n    'median_task_acc',\n    'std_task_acc',\n    'skew_task_acc',\n    'number_of_asked_task_containers',\n    'mean_acc',\n    'median_acc',\n    'std_acc',\n    'skew_acc',\n    'number_of_asked_q'\n]\n\ntarget = 'answered_correctly'","00d69eee":"col_to_drop = set(df.columns.values.tolist()).difference(features + [target])\nfor col in col_to_drop:\n    del df[col]","89b4d933":"df['prior_question_had_explanation'] = df['prior_question_had_explanation'].fillna(value=False).astype(bool)\ndf = df.fillna(value=0.5)","a8f7522b":"df = df.replace([np.inf, -np.inf], np.nan)\ndf = df.fillna(0.5)","5895fc1d":"df.head(5)","f844cd88":"train_df, test_df, y_train, y_test = train_test_split(df[features], df[target], random_state=777, test_size=0.2)","84623c61":"# clf = LGBMClassifier(random_state=777)\n\n# params = {\n#     'n_estimators': [50, 150, 300],\n#     'max_depth': [3, 5, 10],\n#     'num_leaves': [5, 15, 30],\n#     'min_data_in_leaf': [5, 50, 100],\n#     'feature_fraction': [0.1, 0.5, 1.],\n#     'lambda': [0., 0.5, 1.],\n# }\n\n# cv = RandomizedSearchCV(clf, param_distributions=params, cv=5, n_iter=50, verbose=2)\n# cv.fit(df[features], df[target])\n\n# print(cv.best_params_)\n# print(cv.best_score_)","118c5b19":"params = {\n    'num_leaves': 30, \n    'n_estimators': 300, \n    'min_data_in_leaf': 100, \n    'max_depth': 5, \n    'lambda': 0.0, \n    'feature_fraction': 1.0\n}","3d462c1e":"model = LGBMClassifier(**params)\nmodel.fit(train_df, y_train)","8f4947ab":"print('LGB ROC-AUC score: ', roc_auc_score(y_test.values, model.predict_proba(test_df)[:, 1]))","a7f75d5f":"eli5.show_weights(model, top=20)","cc45c371":"lgb.plot_importance(model)","ac895314":"env = riiideducation.make_env()","88dc3eb1":"iter_test = env.iter_test()","47910957":"for (test_df, sample_prediction_df) in iter_test:\n    # merge\n    test_df = test_df.merge(user_characteristics, on = \"user_id\", how = \"left\")\n    test_df = test_df.merge(task_container_characteristics, on = \"task_container_id\", how = \"left\")\n    test_df = test_df.merge(content_characteristics, on = \"content_id\", how = \"left\")\n    \n    # type transformation\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value=False).astype(bool)\n    test_df.fillna(value = 0.5, inplace = True)\n    test_df = test_df.replace([np.inf, -np.inf], np.nan)\n    test_df = test_df.fillna(0.5)\n    \n    # preds\n    test_df['answered_correctly'] = model.predict_proba(test_df[features])[:, 1]\n    cols_to_submission = ['row_id', 'answered_correctly', 'group_num']\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","e577b766":"And the most useful **content_id**","29d72393":">**row_id**: (int64) ID code for the row.\n\n>**timestamp**: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n\n>**user_id**: (int32) ID code for the user.\n\n>**content_id**: (int16) ID code for the user interaction\n\n>**content_type_id**: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n\n>**task_container_id**: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n\n>**user_answer**: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n\n>**answered_correctly**: (int8) if the user responded correctly. Read -1 as null, for lectures.\n\n>**prior_question_elapsed_time**: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n\n>**prior_question_had_explanation**: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.\n","94af5293":"#### PATHS","2c8fe13d":"Let's check any connection between our target value and a frequency of answering questions","fdf1abc7":"<a id=\"1\"><\/a>\n\n<div style=\"background:write; border:0; color:black; width: 100%; height: 50px\">\n    <div style=\"vertical-align: middle; text-align:center\"><h1>DATA EXPLORATION & EDA<\/h1><\/div>\n<\/div>","be8dd376":"Also let's check the part distribution","bb8b1957":"<a id=\"3\"><\/a>\n\n<div style=\"background:write; border:0; color:black; width: 100%; height: 50px\">\n    <div style=\"vertical-align: middle; text-align:center\"><h1>MODELING<\/h1><\/div>\n<\/div>","fa9bfed2":"Let's check the parts distribution","cade82d4":"I'll give just some part from our data bacause of the RAM limit on Kaggle kernel","5772e3c8":"Metadata for the lectures watched by users as they progress in their education.\n\n>**lecture_id**: foreign key for the train\/test content_id column, when the content type is lecture (1).\n\n>**part**: top level category code for the lecture.\n\n>**tag**: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n\n>**type_of**: brief description of the core purpose of the lecture","ef2d8a7b":"Drop **row_id** and **timestamp** because they look like useless and take a lot of memory","8979f747":"And finally check the **type_of** column","b17fc97f":"Also we need to check distribution in **content_type_id**: number of video lectures and questions","83fd0d22":"Also let's check a correlation matrix to get more information between the columns","b0c022e4":"### QUESTIONS.CSV","233eca78":"The most frequent tags of lectures","20bc91ec":"Let's look at the missing data","a6ed0376":"**prior_question_had_explanation** has a medium correlation with the target value. So let's look at his distribution","ed5923e6":"I'll use a pickle file prepared by Rohan Rao in this kernel: https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets\/ \n\nThanks a lot!","ed429566":"And with splitting the group of the tags","91507ba2":"Exclude lectures with -1 in **answered_correctly** col","8220721f":"Drop features that we are not going to use in our model","509a2377":"The answer's showing increases the probability of a successful answering. Let's go further.\n\nCheck the most active **user_id**","452c3a3a":"Here I'm not going to tune LGB parameters.\nI want just choose the most relevant from the most useful","f18fe73d":"Merge all of our data","179581f3":"Let's look at the correct answers distribution in the user's answers","04fff79f":"The main target of this notebook is giving the base understanding of our data and some useful features.\n\nFirst of all you can find here:\n\n>Comprehensive description of our data\n\n>Feature Engineering\n\n>Baseline with LightGBM without serious tuning model's parameters","4b45ef09":"So you can see we don't have any correct answers for the lectures.\n\nNext check the distribution of the **task_container_id**","a551a9a4":"### LECTURES.CSV","60058a87":"<a id=\"2\"><\/a>\n\n<div style=\"background:write; border:0; color:black; width: 100%; height: 50px\">\n    <div style=\"vertical-align: middle; text-align:center\"><h1>FEATURE ENGINEERING<\/h1><\/div>\n<\/div>","1fcf9520":"### EXAMPLE-TEST.CSV","1a7ba07e":"We saw earlier some dependencies between **answered_correctly** and the frequency of **task_container_id**. Therefore I want to add some features for the **task_container_id**","76c3a425":"It was the best params. Therefore I will use them","86ffd735":"# Riiid! Answer Correctness Prediction","33365e76":"### **Important: I use here just a part of the full dataset because of the limited RAM**","67c8971f":">**question_id**: foreign key for the train\/test content_id column, when the content type is question (0).\n\n>**bundle_id**: code for which questions are served together.\n\n>**correct_answer**: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n\n>**part**: top level category code for the question.\n\n>**tags**: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.","737d987a":"<a id=\"4\"><\/a>\n\n<div style=\"background:write; border:0; color:black; width: 100%; height: 50px\">\n    <div style=\"vertical-align: middle; text-align:center\"><h1>SUBMISSION PREPARATION<\/h1><\/div>\n<\/div>","2658b06a":"Let's check the time distribution for **prior_question_elapsed_time**","d472937a":"### TRAIN","77bd207a":"And next check the tags distribution (without splitting the group of the tags)","69c9a884":"Let's look where is the most part of the incorrect answers"}}