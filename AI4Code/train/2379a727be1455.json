{"cell_type":{"ea57dc75":"code","69c7a8a3":"code","80914184":"code","498d364b":"code","7ff4e83a":"code","09a79f66":"code","906b57c3":"code","a3ee273e":"code","483ac4aa":"code","e7777343":"code","e16f6013":"code","005884cb":"code","cfa9553c":"code","a7a6caaa":"code","c131dd91":"code","73d739cf":"code","420e592c":"code","cc88bc1a":"code","a64a9bd9":"code","9eae3987":"code","72804004":"code","45d131a9":"code","869163eb":"code","56259ccf":"code","a36fa319":"code","72849d3d":"code","19ced993":"code","6c2f9395":"code","2404437d":"code","f9c9f687":"code","c54598fd":"code","98155468":"code","37ed40c6":"code","62082e72":"code","ff466353":"code","f64357c2":"markdown","ca66b586":"markdown","9241a6d2":"markdown","47a37f5b":"markdown","4206cd6f":"markdown","f4e308c3":"markdown","7cfaba6c":"markdown","af3bf37e":"markdown","53e3fc66":"markdown","f43fa880":"markdown","6fcc684f":"markdown","06675e96":"markdown","476d3672":"markdown","5eb657d2":"markdown","43db99d2":"markdown","ce93b744":"markdown","43ce2f1c":"markdown","14d960eb":"markdown","bee49890":"markdown"},"source":{"ea57dc75":"!pip install tweepy","69c7a8a3":"from numpy import array, asarray, zeros\nimport pandas as pd\nimport numpy as np\n\nfrom keras.preprocessing.text import one_hot, Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers.embeddings import Embedding\n\n# To consume Twitter's API\nimport tweepy\nfrom tweepy import OAuthHandler \n\nfrom textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nfrom textblob.np_extractors import ConllExtractor\n\nfrom nltk.tokenize import word_tokenize\nimport re","80914184":"# keys and tokens from the Twitter Dev Console\nconsumer_key = 'Sec3MvclRIx2RVlgu9l0SJX6D'\nconsumer_secret = 'ayoPNWtBm7fWpMBoK6EwRmegu3SW8Rw9mzJkottkv97quPe941'\naccess_token = '736550752760406018-so5CPJrEbJKb3c3Pq8va3VFr0yk4S0E'\naccess_token_secret = 'Cgr8tz0h6FTU7kxAjDzpHnjffNTHxWsBytXnu4Ihd1TFb'","498d364b":"topic = 'pfizer'","7ff4e83a":"# Class to incrementally pull tweets using tweepy API calls\nclass TwitterClient(object): \n    def __init__(self): \n        #Initialization method. \n        try: \n            # create OAuthHandler object \n            auth = OAuthHandler(consumer_key, consumer_secret) \n            # set access token and secret \n            auth.set_access_token(access_token, access_token_secret) \n            # create tweepy API object to fetch tweets \n            # add hyper parameter 'proxy' if executing from behind proxy \"proxy='http:\/\/172.22.218.218:8085'\"\n            self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n            \n        except tweepy.TweepError as e:\n            print(f\"Error: Tweeter Authentication Failed - \\n{str(e)}\")\n\n    def get_tweets(self, query, maxTweets = 1000):\n        #Function to fetch tweets. \n        # empty list to store parsed tweets \n        tweets = [] \n        sinceId = None\n        max_id = -1\n        tweetCount = 0\n        tweetsPerQry = 100\n\n        while tweetCount < maxTweets:\n            try:\n                if (max_id <= 0):\n                    if (not sinceId):\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry)\n                    else:\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                since_id=sinceId)\n                else:\n                    if (not sinceId):\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                max_id=str(max_id - 1))\n                    else:\n                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n                                                max_id=str(max_id - 1),\n                                                since_id=sinceId)\n                if not new_tweets:\n                    print(\"No more tweets found\")\n                    break\n\n                for tweet in new_tweets:\n                    parsed_tweet = {} \n                    parsed_tweet['tweets'] = tweet.text \n\n                    # appending parsed tweet to tweets list \n                    if tweet.retweet_count > 0: \n                        # if tweet has retweets, ensure that it is appended only once \n                        if parsed_tweet not in tweets: \n                            tweets.append(parsed_tweet) \n                    else: \n                        tweets.append(parsed_tweet) \n                        \n                tweetCount += len(new_tweets)\n                print(\"Downloaded {0} tweets\".format(tweetCount))\n                max_id = new_tweets[-1].id\n\n            except tweepy.TweepError as e:\n                # Just exit if any error\n                print(\"Tweepy error : \" + str(e))\n                break\n        \n        return pd.DataFrame(tweets)","09a79f66":"twitter_client = TwitterClient()\n\n# calling function to get tweets\ntweets_df = twitter_client.get_tweets(topic, maxTweets=10000)\nprint(f'tweets_df Shape - {tweets_df.shape}')\ntweets_df.head(10)","906b57c3":"def fetch_sentiment_using_textblob(text):\n    analysis = TextBlob(text)\n    return 1 if analysis.sentiment.polarity >= 0 else 0\n\ntweets_df['sentiments'] = tweets_df.tweets.apply(lambda tweet: fetch_sentiment_using_textblob(tweet))\ntweets_df['sentiments'].value_counts()","a3ee273e":"# Cleansing tweets\n\ndef remove_pattern(text, pattern_regex):\n    r = re.findall(pattern_regex, text)\n    for i in r:\n        text = re.sub(i, '', text)\n\n    return text \n\n# We are keeping cleaned tweets in a new column called 'tidy_tweets'\ntweets_df['tidy_tweets'] = np.vectorize(remove_pattern)(tweets_df['tweets'], \"@[\\w]*: | *RT*\")\n\n\n# Removing links (http | https)\ncleaned_tweets = []\n\nfor index, row in tweets_df.iterrows():\n    # Here we are filtering out all the words that contains link\n    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n    cleaned_tweets.append(' '.join(words_without_links))\n\ntweets_df['tidy_tweets'] = cleaned_tweets\n\n# Removing tweets with empty text\ntweets_df = tweets_df[tweets_df['tidy_tweets']!='']\n\n# Dropping duplicate rows\ntweets_df.drop_duplicates(subset=['tidy_tweets'], keep=False)\n\n# Resetting index\ntweets_df = tweets_df.reset_index(drop=True)\n\n# Removing Punctuations, Numbers and Special characters\ntweets_df['absolute_tidy_tweets'] = tweets_df['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")","483ac4aa":"tweets_df.sample(10)","e7777343":"# get a list of sentences\/tweets\ncorpus = list(tweets_df.absolute_tidy_tweets.values)\n\n# sentiments list\nsentiments = list(tweets_df.sentiments.values)","e16f6013":"# getting list of tokens\nall_words = []\nfor sent in corpus:\n    tokenize_word = word_tokenize(sent)\n    for word in tokenize_word:\n        all_words.append(word)","005884cb":"unique_words = set(all_words)\nprint(len(unique_words))","cfa9553c":"# identifying vocal lenght of unique words, it basically helps in deciding on hishing or word reference setup\nvocab_length = int(len(unique_words)) + 5 # adding 5 to take some buffer\nvocab_length","a7a6caaa":"embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\nprint(embedded_sentences)","c131dd91":"# finding the largest sentence length\nword_count = lambda sentence: len(word_tokenize(sentence))\nprint(word_count)\nlongest_sentence = max(corpus, key=word_count)\nprint(longest_sentence)\nlength_long_sentence = len(word_tokenize(longest_sentence))\nprint(length_long_sentence)","73d739cf":"# pad_sequences is being used here to make sure that all sentences are of equal length\npadded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\nprint(padded_sentences)","420e592c":"model = Sequential()\nmodel.add(Embedding(vocab_length, 100, input_length=length_long_sentence))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","cc88bc1a":"# You can try with other optimizers\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nprint(model.summary())","a64a9bd9":"# fitting over padded sentences\nmodel.fit(np.asarray(padded_sentences), np.asarray(sentiments), epochs=100, verbose=1)","9eae3987":"# Evaluating over train set itself\nloss, accuracy = model.evaluate(padded_sentences, np.asarray(sentiments), verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","72804004":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(corpus)","45d131a9":"vocab_length = len(word_tokenizer.word_index) + 5\nvocab_length","869163eb":"embedded_sentences = word_tokenizer.texts_to_sequences(corpus)\nembedded_sentences[:3]","56259ccf":"word_count = lambda sentence: len(word_tokenize(sentence))\nlongest_sentence = max(corpus, key=word_count)\nlength_long_sentence = len(word_tokenize(longest_sentence))\n\npadded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n\nprint(padded_sentences[:3])","a36fa319":"embeddings_dictionary = dict()\nglove_file = open('..\/input\/glove6b\/glove.6B.100d.txt', encoding=\"utf8\")","72849d3d":"# create word embedding vectors\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\n\nglove_file.close()","19ced993":"# Checking number of unique words\nlen(embeddings_dictionary.keys())","6c2f9395":"# Constructing word embedding matrix only for required words\nembedding_matrix = zeros((vocab_length, 100))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","2404437d":"# Shape of our final embedding matrix\nembedding_matrix.shape","f9c9f687":"# Building a simple sequential NN from finally constructed embedded matrix\nmodel = Sequential()\nembedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\nmodel.add(embedding_layer)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nprint(model.summary())","c54598fd":"model.fit(padded_sentences, np.asarray(sentiments), epochs=100, verbose=1)\n","98155468":"loss, accuracy = model.evaluate(padded_sentences, np.asarray(sentiments), verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","37ed40c6":"deep_inputs = Input(shape=(length_long_sentence,))\nembedding = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)(deep_inputs) # line A\nflatten = Flatten()(embedding)\nhidden = Dense(1, activation='sigmoid')(flatten)\nmodel = Model(inputs=deep_inputs, outputs=hidden)","62082e72":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nprint(model.summary())","ff466353":"model.fit(padded_sentences, np.asarray(sentiments), epochs=100, verbose=1)\nloss, accuracy = model.evaluate(padded_sentences, np.asarray(sentiments), verbose=0)\n\nprint('Accuracy: %f' % (accuracy*100))","f64357c2":"#### Custom Implementation:","ca66b586":"This is how we can build a custom word embedding matrix. Next we'll see how to consume pre-trained GloVe word embedding matrix.","9241a6d2":"## Implementation using Keras\n\nKeras supports two types of APIs for implementation: \n- **Sequential Methods**\n- **Functional Methods**\n\nLet's look at them one by one\n","47a37f5b":"Several types of pretrained word embeddings exist, however we will be using the GloVe word embeddings from Stanford NLP since it is the most famous one and commonly used. The word embeddings can be obtained from Kaggle publict datasets.","4206cd6f":"Just like we were getting in Sequentional methods, here also we got 100% accuracy. As you can see, even though we can implement the word embedding on our own using Keras sequential and functional APIs, we prefer pre-trained word embeddings. Reason is pretty simple, we want to save our time by not re-inventing the wheel.\n\nHere, we focused on word embedding implementations majorly, later we'll focus on solving Machine Translation and Text generation problems. Till then, happy learning (;\n\nReferences: \nhttps:\/\/www.ijitee.org\/wp-content\/uploads\/papers\/v8i11\/K13430981119.pdf\n\nhttps:\/\/stackabuse.com\/python-for-nlp-word-embeddings-for-deep-learning-in-keras\/","f4e308c3":"To perform custom word embedding and sentiment analysis, we need a decent amount of text data. We live in an era of Social Networking, nothing can be a better source of same than twitter. Let's get tweets on desired topic.","7cfaba6c":"Model building: We are simply adding all the basic layers i.e. Embedding, Flattening, and final output","af3bf37e":"As you must already be knowing that the challange with text based data is that it comprises of words and sentences.. not numeric variables :)<br>\nMachine Learning, Deep Learning, and Statistical models mostly expects, most of the input data, in numerical format. That's why we need to perform feature engineering, which creates numeric features, for text based data, without loosing much infromation.\n\nPreviously, I explained how to create text based features using BOW, TF-IDF, and N-Gram techniques. But the problem with those technique is that they are very convential, which means they are pretty straight forward, and have been put together without much of the time and space optimization. In simple words, their we create vectors for word occurances or next word after each N-Grams, which eventually results in really huge vectors for a large amount of data. This vector is one hot encoded and rarely contains 1, means it carries a large amount of unnecessary information.\n\nHere, we are going to look into a more sophisticated feature engineering technique for text based data, called Word embedding. Word embedding is basically used as one of the data preparation step while solving many of the NLP problems. Let's move forward and understand what it is and how it works:","53e3fc66":"### Keras Functional Methods\nIn the last section, we saw how word embeddings can be used with the Keras sequential API. While the sequential API is a good starting point for beginners, as it allows you to quickly create deep learning models, it is extremely important to know how Keras Functional API works. Most of the advanced deep learning models involving multiple inputs and outputs use the Functional API.","f43fa880":"We only have got tweets, let's get sentiment labels for same using TextBlob. Note that, we can use more sophisticated and much better technique for sentiment labeling, but here that's not our focus, so going with one of the simplest technique.","6fcc684f":"**one_hot** method is imported from Keras, and is being used to label encode our sentences.. please note that, it's different from the conventional one hot encoding","06675e96":"Below code block peforms a cleansing of tweets we just pulled:","476d3672":"### Loading Pretrained Word Embeddings","5eb657d2":"So, what is Word Embedding?<br>\n**It is most popular way of representing document vocabulary. The basic purpose of word embeddings is to capture and store the context of words with respect to document. It also stores semantic and syntactic relation with other words in a document. In computational perspective it is basically a vector which stores all the contextual, semantics and syntatic relations of that word.**\n\nTwo most commonly used technique to create a Word Embedding are:\n* **GloVe**\n* **Word2Vec**","43db99d2":"Let's jump to the implementation now,","ce93b744":"**corpus** and **sentiment** list is already constructed for our tweets. <br>\nLet's look into other ways of creating tokens and padded sentences using Keras","43ce2f1c":"Constructing corpus and sentiments lists for ease in further operations","14d960eb":"### Keras Sequential Methods\n\nEmbedding() method can be used to implement a class, needed to develop an embedding layer. Keras can either be used to **create a new word embedding by learning custom words** and use them in problems like classification, sentiment analysis, etc. **or it can be used to load the pre-trained word embedding** and use them to solve our purpose.<br><br>\nIn this Kernel, we'll perform a simple text classification exercise by consuming the newly trained word-embeddings.\n","bee49890":"There are many ways to implement these techniques, in this Kernel we'll look into the implementation using Keras"}}