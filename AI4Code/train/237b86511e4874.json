{"cell_type":{"934def0d":"code","ecea56d6":"code","bc8fd110":"code","4d263061":"code","3e988892":"code","0460e1f5":"code","799c6c5b":"code","ceb901b0":"code","20bc5049":"code","9cb545ff":"code","62b4df42":"code","4722f6ef":"code","6b596e18":"code","c668d485":"code","a26502b5":"code","d43a79b0":"code","d992f6aa":"code","1c184509":"code","1add647f":"code","d410ed16":"code","d53e1997":"code","bbe4e593":"code","cacd2c31":"code","3c656929":"code","5c4dd491":"code","9d7f0a22":"code","05cf29ef":"code","334282ba":"code","41d32407":"code","96b10136":"code","52e38997":"code","bb13fd1b":"code","ed4f43bd":"code","e2636ab3":"code","5a8f48bc":"code","f7c91605":"code","847965dc":"code","e1b6719b":"code","5f4d8b43":"code","96af95d1":"code","265e2429":"markdown","f54bf427":"markdown","640e9496":"markdown","0515c42b":"markdown","a1c7d042":"markdown","b1b7afbb":"markdown","086bee32":"markdown","58bde5c1":"markdown","659e8183":"markdown","a63b9e4f":"markdown","8917f647":"markdown","1fe896c1":"markdown","d145ab63":"markdown","ebf18efb":"markdown","84fafd3d":"markdown","37d2754f":"markdown","817e2851":"markdown","af5569d0":"markdown","c76fcd74":"markdown","4315977e":"markdown","1f4fb2da":"markdown","7e268d2f":"markdown","818a9421":"markdown","087f2afb":"markdown","3bc6ddd5":"markdown","502f6d11":"markdown","cc798ef4":"markdown","e9ee93c9":"markdown","340c37d0":"markdown","dd7491e2":"markdown","0dfeb389":"markdown","1ca199cc":"markdown","6b1119d4":"markdown","21ff7ea4":"markdown","4ceea242":"markdown","8ed7093e":"markdown"},"source":{"934def0d":"import numpy as np\nimport os\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nfrom pandas import read_csv","ecea56d6":"filename = '..\/input\/all_stocks_5yr.csv'\nstock = read_csv(filename)\nprint(\"Die Tabelle zeigt den Aufbau unserer Datei\")\nstock.head() ","bc8fd110":"ticker_name = 'AMZN'\nstock_a = stock[stock['Name'] == ticker_name]\nstock_a.shape #(Anzahl der Zeilen, Anzahl der Spalten)","4d263061":"stock.info() ","3e988892":"stock_a.describe()","0460e1f5":"stock_a['Tagesveraenderung'] = ((stock['high'] - stock['low'] )\/ stock['low'])*100\n\nstock_a['VeraenderungZumVorherigenTag'] = (abs(stock_a['close'].shift() - stock_a['close'] )\/ stock['close'])*100","799c6c5b":"print\nstock_a.head()","ceb901b0":"stock_a.hist(bins=50, figsize=(20,15))\nplt.show()","20bc5049":"stock_a.plot(kind=\"line\", x=\"date\", y=\"close\", figsize=(15, 10))","9cb545ff":"corr_matrix = stock_a.corr()","62b4df42":"corr_matrix[\"close\"].sort_values(ascending=False)","4722f6ef":"from pandas.plotting import scatter_matrix\n\nattributes = [\"high\", \"low\", \"open\", \"Tagesveraenderung\", \"VeraenderungZumVorherigenTag\", \"volume\"]\n\nscatter_matrix(stock_a[attributes], figsize=(20, 15))","6b596e18":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\ncorr = stock_a[[\"high\", \"low\", \"open\", \"Tagesveraenderung\", \"VeraenderungZumVorherigenTag\", \"volume\"]].corr()\n\n# Generierung einer Maske\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 12))\n\n# Auswahl der Farbpalette\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Heatmap mit Maske darstellen und Achsenl\u00e4nge bestimmen\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3,\n            square=True, \n            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax);","c668d485":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,Normalizer\nX_stock_a = stock_a.drop(['date', 'Name','close'], axis=1)\ny_stock_a = stock_a['close']\n\nX_stock_train, X_stock_test, y_stock_train, y_stock_test = train_test_split(X_stock_a, y_stock_a, test_size=0.2, \n                                                                            random_state=42)","a26502b5":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer,StandardScaler\ndata_pipeline = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #fehlenden werte werden durch Median ersetzt\n        ('scaler',StandardScaler())\n#        ('normalizer', Normalizer()),\n    ])","d43a79b0":"from sklearn.preprocessing import Imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler,Normalizer\n\nfrom sklearn.pipeline import Pipeline\n\nLr_pipeline_nor = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #fehlenden werte werden durch Median ersetzt\n        ('normalizer',Normalizer()),\n        ('lr', LinearRegression())\n        \n    ])\n\nLr_pipeline_nor.fit(X_stock_train, y_stock_train)","d992f6aa":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\nsvr_pipeline_nor = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #fehlenden werte werden durch Median ersetzt\n        ('normalizer',Normalizer()),\n        ('svr', SVR(kernel=\"linear\"))\n        \n    ])\n\nsvr_pipeline_nor.fit(X_stock_train, y_stock_train)","1c184509":"from sklearn.preprocessing import Imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n\n\nLr_pipeline_std = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #fehlenden werte werden durch Median ersetzt\n        ('scaler',StandardScaler()),\n        ('lr', LinearRegression())\n        \n    ])\n\nLr_pipeline_std.fit(X_stock_train, y_stock_train)","1add647f":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\n\nsvr_pipeline_std = Pipeline([\n        ('imputer', Imputer(missing_values=\"NaN\",strategy=\"median\")), #fehlenden werte werden durch Median ersetzt\n        ('scaler',StandardScaler()),\n        ('svr', SVR(kernel=\"linear\"))\n        \n    ])\n\nsvr_pipeline_std.fit(X_stock_train, y_stock_train)","d410ed16":"from sklearn.metrics import mean_absolute_error\n\n#Lineare Regression mit Normalisierung und Standardisierung \nlr_stock_predictions_nor = Lr_pipeline_nor.predict(X_stock_test)\nlr_mae_nor = mean_absolute_error(y_stock_test, lr_stock_predictions_nor)\nprint('Lr MAE with Normalization', lr_mae_nor)\n\nlr_stock_predictions_std = Lr_pipeline_std.predict(X_stock_test)\nlr_mae_std = mean_absolute_error(y_stock_test, lr_stock_predictions_std)\nprint('Lr MAE with standardization', lr_mae_std)\n\n#SVM mit Normalisierung und Standardisierung\nsvm_stock_predictions_nor = svr_pipeline_nor.predict(X_stock_test)\nsvm_mae_nor = mean_absolute_error(y_stock_test, svm_stock_predictions_nor)\nprint('SVM MAE with Normalization', svm_mae_nor)\n\nsvm_stock_predictions_std = svr_pipeline_std.predict(X_stock_test)\nsvm_mae_std = mean_absolute_error(y_stock_test, svm_stock_predictions_std)\nprint('SVM MAE with standardization', svm_mae_std)","d53e1997":"import pandas as pd\nimport numpy as np\n\n#Vorhersage und Ausgabe der RMSE\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.metrics import mean_squared_error\n\n\n\n#Linear Regression mit Normalisierung und Standardisierung\nlr_stock_predictions_nor = Lr_pipeline_nor.predict(X_stock_test)\nlr_mse_nor = mean_squared_error(y_stock_test, lr_stock_predictions_nor)\nlr_rmse_nor = np.sqrt(lr_mse_nor)\nprint('Lr RMSE mit Normalisierung', lr_rmse_nor)\n\nlr_stock_predictions_std = Lr_pipeline_std.predict(X_stock_test)\nlr_mse_std = mean_squared_error(y_stock_test, lr_stock_predictions_std)\nlr_rmse_std = np.sqrt(lr_mse_std)\nprint('Lr RMSE mit Standardisierung', lr_rmse_std)\n\n#SVM mit Normalisierung und Standardisierung\nsvm_stock_predictions_nor = svr_pipeline_nor.predict(X_stock_test)\nsvm_mse_nor = mean_squared_error(y_stock_test, svm_stock_predictions_nor)\nsvm_rmse_nor = np.sqrt(svm_mse_nor)\nprint('SVM RMSE mit Normalisierung', svm_rmse_nor)\n\nsvm_stock_predictions_std = svr_pipeline_std.predict(X_stock_test)\nsvm_mse_std = mean_squared_error(y_stock_test, svm_stock_predictions_std)\nsvm_rmse_std = np.sqrt(svm_mse_std)\nprint('SVM RMSE mit Standardisierung', svm_rmse_std)\n\n\n\nlr_std = ['1',\"Linear Regression mit Standardisierung\",np.round(lr_rmse_std,3),np.round(lr_mae_std,3)]\nlr_nor = ['2',\"Linear Regression mit Normalisierung\",np.round(lr_rmse_nor,3),np.round(lr_mae_nor,3)]\n\nsvm_std = ['5',\"SVM mit Standardisierung\",np.round(svm_rmse_std,3),np.round(svm_mae_std,3)]\nsvm_nor = ['6',\"SVM mit Normalisierung\",np.round(svm_rmse_nor,3),np.round(svm_mae_nor,3)]\n\n\n\nlinear_model_result= pd.DataFrame([lr_std,lr_nor,svm_std,svm_nor],columns=[ \"ExpID\", \"Model\", \"RMSE\",\"MAE\"])\n\nlinear_model_result","bbe4e593":"#Funktion zur Ausgabe s\u00e4mtlicher Modelle\nfrom sklearn.preprocessing import Imputer\n    \ndef allModelsResultForAllStocks():\n    \n    best_result_per_ticker = pd.DataFrame(columns=['Ticker','Model','RMSE'])\n    ticker_list = np.unique(stock[\"Name\"])\n    best_result_per_ticker = list()\n    for ticker_name in ticker_list:\n        result = pd.DataFrame(columns=['Ticker','Model','RMSE'])\n        stock_a = stock[stock['Name'] == ticker_name]\n        #Weitere Feautures werden hinzugef\u00fcgt \n        # 1 Preisver\u00e4nderungen \u00fcber den Tag\n        stock_a['Tagesveraenderung'] = ((stock['high'] - stock['low'] )\/ stock['low'])*100\n\n        #2 Preisver\u00e4nderungen zum voherigen Tag\n        stock_a['VeraenderungZumVorherigenTag'] = (abs(stock_a['close'].shift() - stock_a['close'] )\/ stock['close'])*100\n\n        X_stock_a = stock_a.drop(['date', 'Name','close'], axis=1)\n        y_stock_a = stock_a['close']\n\n        \n        imputer = Imputer(missing_values='NaN', strategy='median') #Fehlende Werte werden durch den Median ersetz\n        \n        imputer.fit_transform(X_stock_a)\n       \n        X_stock_train, X_stock_test, y_stock_train, y_stock_test = train_test_split(X_stock_a, y_stock_a, test_size=0.2, \n                                                                                random_state=42)\n\n\n        Lr_pipeline_std.fit(X_stock_train, y_stock_train)\n        Lr_pipeline_nor.fit(X_stock_train, y_stock_train)\n\n        svr_pipeline_nor.fit(X_stock_train, y_stock_train)\n        svr_pipeline_std.fit(X_stock_train, y_stock_train)\n\n        \n        # Vorhersage und Berechnung des RSME f\u00fcr alle Modelle\n\n        #Linear Regression mit Normalisierung und Standartisierung\n        lr_stock_predictions_nor = Lr_pipeline_nor.predict(X_stock_test)\n        lr_mse_nor = mean_squared_error(y_stock_test, lr_stock_predictions_nor)\n        lr_rmse_nor = np.sqrt(lr_mse_nor)\n        rmse_row =   [ticker_name,'Lr RMSE mit Normalisierung', lr_rmse_nor]\n\n        result.loc[-1] = rmse_row  # Hinzuf\u00fcgen einer Reihe zur Ausgabe\n        result.index = result.index + 1  # Iterationsstufe +1\n     \n    \n        lr_stock_predictions_std = Lr_pipeline_std.predict(X_stock_test)\n        lr_mse_std = mean_squared_error(y_stock_test, lr_stock_predictions_std)\n        lr_rmse_std = np.sqrt(lr_mse_std)\n        rmse_row =   [ticker_name,'Lr RMSE mit Standardisierung', lr_rmse_std]\n    \n    \n\n        result.loc[-1] = rmse_row  # Hinzuf\u00fcgen einer Reihe zur Ausgabe\n        result.index = result.index + 1  # Iterationsstufe +1\n\n        #SVM mit Normalisierung und Standartisierung\n        svm_stock_predictions_nor = svr_pipeline_nor.predict(X_stock_test)\n        svm_mse_nor = mean_squared_error(y_stock_test, svm_stock_predictions_nor)\n        svm_rmse_nor = np.sqrt(svm_mse_nor)\n        rmse_row =   [ticker_name,'SVM RMSE mit Normalisierung', svm_rmse_nor]\n        \n\n        result.loc[-1] = rmse_row  # Hinzuf\u00fcgen einer Reihe zur Ausgabe\n        result.index = result.index + 1  # Iterationsstufe +1\n\n        svm_stock_predictions_std = svr_pipeline_std.predict(X_stock_test)\n        svm_mse_std = mean_squared_error(y_stock_test, svm_stock_predictions_std)\n        svm_rmse_std = np.sqrt(svm_mse_std)\n        rmse_row =   [ticker_name,'SVM RMSE mit Standardisierung', svm_rmse_std]\n    \n        result.loc[-1] = rmse_row  # adding a row\n        result.index = result.index + 1  # shifting index\n\n\n       \n        result = result.sort_values(by = ['RMSE'])\n        \n       \n        best_result_per_ticker.append(np.array(result.iloc[0, :]))\n       \n\n\n    best_result_per_ticker_df = pd.DataFrame(data=best_result_per_ticker, columns=['Ticker','Model','RMSE'])\n    \n    \n    return best_result_per_ticker_df\n\nbest_result_per_ticker = allModelsResultForAllStocks()","cacd2c31":"def classify (meanValue):\n    if meanValue <=1.5:\n        return 'Low'\n    elif meanValue >1.5 and  meanValue <=2.5:\n        return 'Medium'\n    elif meanValue >2.5:\n        return 'High'","3c656929":"def linearModel(ticker):\n    stock_a = stock[stock['Name'] == ticker]\n    #Neue Features hinzuf\u00fcgen \n    #1 Preis-Tagesveraenderung \n    stock_a['Tagesveraenderung'] = ((stock['high'] - stock['low'] )\/ stock['low'])*100\n\n    #2 Preis-VeraenderungZumVorherigenTag \n    stock_a['VeraenderungZumVorherigenTag'] = (abs(stock_a['close'].shift() - stock_a['close'] )\/ stock['close'])*100\n\n    X_stock_a = stock_a.drop(['date', 'Name','close'], axis=1)\n    y_stock_a = stock_a['close']\n\n    Lr_pipeline_std.fit(X_stock_a, y_stock_a)\n    \n    model = Lr_pipeline_std.named_steps['lr']\n    \n    return model,stock_a\n","5c4dd491":"#Alle 500 Aktien werden f\u00fcr Training eingesetzt\nticker_list = np.unique(stock['Name'])\n\ndf = pd.DataFrame(columns=['TICKER','CLASS','Coef for open','Coef for high','Coef for low','Coef for volume','Coef for change within day','Coef for change from prev day'])\nfor ticker in ticker_list:\n    \n    model,stock_a = linearModel(ticker)    \n    \n    print(\"Mean value:\",stock_a[\"Tagesveraenderung\"].mean())\n    #adding target class \n    stock_features = np.concatenate((np.asarray([ticker,classify(stock_a[\"Tagesveraenderung\"].mean())]),model.coef_))\n    \n    df.loc[-1] = stock_features  # adding a row\n    df.index = df.index + 1  # shifting index\n    df = df.sort_index() \n   \n#print(df)\n\n#Abspeichern von Feature Coeffizienten und Target Klassen aller 500 Aktien \ndf.to_csv('coeff1.csv', mode='a',header=['TICKER','CLASS','Coef for open','Coef for high','Coef for low','Coef for volume','Coef for change within day','Coef for change from prev day'])\n","9d7f0a22":"# Hochladen der libraries\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX_class = np.array(df.ix[:, 2:8]) \ny_class = np.array(df['CLASS']) \n\n\n# Teilung in train and test\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n","05cf29ef":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# fitting the model\nknn.fit(X_train_class, y_train_class)\n\n# predict the response\npred = knn.predict(X_test_class)\n\n# evaluate accuracy\nprint (\"Accuracy of KNN \", accuracy_score(y_test_class, pred))","334282ba":"from sklearn.cluster import KMeans\n\nX_class = np.array(df.ix[:, 2:8]) \t# end index is exclusive\n\nk_mean = KMeans()\n\n#K-mean++ bestimmt die Clusteranzahl nun selbst \nk_mean_model = k_mean.fit(X_class)\n\nprint(\"Number of clusters\",k_mean_model.n_clusters)","41d32407":"df_cluster = df.drop(['CLASS'], axis=1)\n\n#Selecting features from dataframe , there are 6 features \nX_cluster = np.array(df_cluster.ix[:, 1:7])\n\ny_pred = k_mean_model.predict(X_cluster)\n\npred_df = pd.DataFrame({'labels': y_pred, 'companies': df_cluster.ix[:, 0]})","96b10136":"#Cluster assignment for the stocks \npred_df","52e38997":"window = 150","bb13fd1b":"sharpes = []\nreturns = []\nignore = [\"APTV\"] #Werte dieser Aktie m\u00fcssen komplett ignoriert werden  \nfor name in stock[\"Name\"].unique(): #einbauen einer Schleife\n    if name not in ignore: #aussotieren der Firmen mit invalidem Datensatz\n        stock_prices = stock[stock[\"Name\"] == name]  #\n        stock_prices = stock_prices.set_index(\"date\", drop=True)\n        stock_prices.index = [pd.Timestamp(x) for x in stock_prices.index]\n        daily_returns = (stock_prices[\"close\"] \/ stock_prices[\"close\"].shift()).add(-1).dropna() \n        # deklinieren der daily return durch die Ratios auf Grundlage der \"closings\" am Ende der Zeile werden durch \n        # die Funktionen \"shift und add\" alle spalten durchlaufen \/ die Funktion \"dropna\" l\u00e4sst vernachl\u00e4ssigt alle Zeilen mit NA Daten \n        mean = daily_returns.rolling(window=window).mean().dropna() #Errechnung des Mittelwertes mit der Funktion \"rolling\"\u00fcber die Zeitperiode \"window\"\n        #durch die rolling Funktion werden look forward bias verhindert im anschlie\u00dfenden back test\n        std = daily_returns.rolling(window=window).std().dropna() #Berechnung der Stadartabweichung, ebenfalls mit der Funktion \"rolling\"\n        sharpe = mean \/ std #Berrechnung der Ratio wird sp\u00e4ter als \"weight\" verwendet\n        returns.append(daily_returns.rename(name)) #bef\u00fcllen der Listen sharpes und returns mit den Werten von daily returns bzw. sharpe\n        sharpes.append(sharpe.rename(name))\n        print(\"Name: {}; First Date: {}\".format(name, daily_returns.index[0])) #printen der Anfangsdaten um herauszufinden welche Unternehmen zu Liste \"ignore\" hinzugef\u00fcgt werden m\u00fcssen ","ed4f43bd":"def weights_generator(n): #Aufbauen der Methode \"weight_generator\", um die einzelnen Unternehmen nach unserer Ratio zu ranken\n    non_normalized_weights = np.array([i*1\/n for i in range(n)]) \n    return non_normalized_weights \/ np.sum(non_normalized_weights)","e2636ab3":"n = 20 #Definieren der Anzahl der Firmen, welche in das Portfolio aufgenommen werden\nweight_vector = weights_generator(n) #Erstellen von Vektoren mit den Gewichten\nsharpe_df = pd.concat(sharpes,axis=1) #Verbinden von den Zeilen sharpe  in sharpe_df\nsharpe_df = sharpe_df.replace(to_replace=np.nan, value=-10000) #Eliminieren der \"nan\" Zeilen\/ Vermeidung von Fehlerproduktion im Code\nreturn_df = pd.concat(returns, axis=1) #Verbinden von den Zeilen return in return_df","5a8f48bc":"mean_return = return_df.mean(axis=0) #Erstellen eines Mittelwertes \u00fcber die gesamte Spalte einer Firma\nstd_return = return_df.std(axis=0) #Erstellen der Standardabweichung \u00fcber den gesamten Zeitlauf","f7c91605":"plt.figure(figsize=(14,9)) #Festlegung der Gr\u00f6\u00dfe des Diagramms\nplt.plot(std_return.values, mean_return.values, 'o') #abrufen der Plot-Funktion mit den Parametern return und Standardabweichung\nprint(\"Max Standard Deviation: {}\".format(std_return.idxmax())) #ausgeben des Unternehmens mit der maximalen Standardabweichung\nprint(\"Max Expected Return: {}\".format(mean_return.idxmax())) #ausgeben des Unternehmens mit dem maximalen Return\nprint(\"Min Standard Deviation: {}\".format(std_return.idxmin())) #ausgeben des Unternehmens mit der minimalen Standardabweichung\nprint(\"Min Expected Return: {}\".format(mean_return.idxmin())) #ausgeben des Unternehmens mit dem minimalen Return","847965dc":"rows = {}\nfor index, row in sharpe_df.iterrows(): \n    picks = row.sort_values().iloc[-n:]\n    new_row = pd.Series(data=0, index=row.index)\n    for weight_index, pick in enumerate(picks.index):\n        new_row.loc[pick] = weight_vector[weight_index]\n    rows[index] = new_row","e1b6719b":"weights = pd.DataFrame.from_dict(rows, orient=\"index\")","5f4d8b43":"mean_weight = weights.mean()\nplt.figure(figsize=(14,9))\nplt.bar(x = mean_weight.sort_values().index[-20:-10], height = mean_weight.sort_values()[-20:-10]*100)\nplt.bar(x = mean_weight.sort_values().index[-10:], height = mean_weight.sort_values()[-10:]*100)","96af95d1":"weighted_return_df = weights.mul(return_df, axis=1).replace(to_replace=np.nan, value=0).iloc[window-1:]\nplt.figure(figsize=(14,9))\nplt.plot(weighted_return_df.sum(axis=1).add(1).cumprod())","265e2429":"Sowohl f\u00fcr Classification als auch f\u00fcr ein Clustering ist der folgende Schritt notwendig. Hier definieren wir simultan zum Regressionsteil dieses Projekts, wie ein lineares Modell f\u00fcr alle Aktien aufgestellt werden muss.","f54bf427":"**1.4.2 Pr\u00fcfung vorhandener Nullwerte**\n\nHierbei wird \u00fcberpr\u00fcft, ob vereinzelt leere Zellen in der Datenbank vorhanden sind. Daf\u00fcr wird jede einzelne Spalte und jede Zeile der Amazon-Aktie \u00fcberpr\u00fcft. Dieses Vorgehen ist unabdingbar im Bereich Data Mining, da ohne Pr\u00fcfung die Analyse zu einem falschen Ergebnis f\u00fchren kann. In der Literatur spricht man auch von data pre-processing.\n\nAlle Spalten wurden \u00fcberpr\u00fcft. Es wurde kein einziger Nullwert ermittelt: \"non-null\"","640e9496":"**2.5.2 Anwendung der kNN-Classification**\n\nWir nutzen die sklearn-Library, um unseren Datensatz in Trainings- und Testdaten zu unterteilen.\n","0515c42b":"**3.4 Auswertung**","a1c7d042":"**1.4 Exploratory Data Analysis (Explorative Datenanalyse)**\n\nIst ein Teilgebiet der Statistik. Sie untersucht und begutachtet Daten, von denen nur ein geringes Wissen \u00fcber deren Zusammenh\u00e4nge vorliegt. Dient als Basis f\u00fcr die Auswahl der statistischen Methoden und hilft dabei Hypothesen aufzustellen, welche zu einem sp\u00e4teren Zeitpunkt getestet werden k\u00f6nnen. Des Weiteren wird \u00fcberp\u00fcft, ob sich die Datengrundlage f\u00fcr die geplanten Analysen eignet.\n\n**1.4.1 Beispielhafte Selektion einer Aktie und erste Pr\u00fcfung der Datenmenge**\n\nHier wurde die Amazon Aktie ausgew\u00e4hlt. In einem ersten Schritt wird die Anzahl der vorhandenen Zeilen ermittelt, sowie die Anzahl der einzelnen Spalten (\"stock_a.shape\").","b1b7afbb":"**1.4.3 Pr\u00fcfung weiterer Eigenschaften auf Basis der .describe()-Funktion**\n\nDiese Funktion f\u00fchrt unterschiedliche Operationen zu den einzelnen Spalten durch:\n\n* Count = Anzahl der Zellen\n* Mean = Arithmetisches Mittel\n* Std = Standardabweichung\n* Min = Minimalwert\n* 25% = 25-Perzentil\n* 50% = 50-Perzentil \/ Median\n* 75% = 75-Perzentil\n* Max = Maximalwert\n\nDiese Berechnungen sind sinnvoll, um die Basis f\u00fcr eine Vergleichbarkeit der einzelnen features zu schaffen.","086bee32":"In einem weiteren Schritt erfolg die Darstellung der Spalte \"close\" im zeitlichen Verlauf. F\u00fcr das Projekt ist diese Spalte von zentraler Bedeutung, da der Close-Wert mittels Regressionsanalyse prognostiziert und geclustert werden soll.","58bde5c1":"Nun lassen wir das zuvor definierte lineare Modell und die notwendige Berechnung der Koeffizienten durchlaufen und speichern es in einer csv-Datei ab.","659e8183":"In diesem Schritt werden die Koeffizienten klassifiziert und in drei Klassen eingeteilt. Zu guter Letzt wird die Genauigkeit der Klassen mit Hilfe eines sogenannten accuracy_score \u00fcberpr\u00fcft. Da die Genauigkeit mit knapp \u00fcber 50% sehr gering ist, wird im n\u00e4chsten Schritt das Clustering mittels des kMeans++ Algorithmus durchgef\u00fchrt.","a63b9e4f":"**2.4.2 RMSE f\u00fcr LR und SVR - f\u00fcr Standardisierung und Normalisierung**\n\nAnalog der Anwendung der MAE Metrik, wird eine library von SKlearn importiert, welche anschlie\u00dfend angewandt wird.\n\nNach der Anwendung der vier Varianten (Stand.+LR, Stand.+SVR, Norm.+LR, Norm.+SVR) werden die Ergebnisse der Validierung anschlie\u00dfend in einer Tabelle dargestellt.","8917f647":"**2.3.2 Normalisieren der Daten und lineare SVR**","1fe896c1":"**1.4.9 Visualisierung mittels Heatmaps**\n\nLetztlich wird die Matrix in eine Heatmap \u00fcberf\u00fchrt, um Zusammenh\u00e4nge noch besser verstehen zu k\u00f6nnen. Daf\u00fcr wird die Library seaborn als sns importiert, welche auf matplotlib basiert.","d145ab63":"**2.3.4 Standardisieren der Daten und lineare SVR**","ebf18efb":"**2.3.3 Standardisierung und lineare Regression**","84fafd3d":"**1.4.4 Formeln f\u00fcr neue Spalten (features)**\n\nIm folgenden werden nun die ersten Berechungen vorgenommen, um f\u00fcr die Analyse notwendige Spalten zu erzeugen.\n\nZum einen wird die Ver\u00e4nderung \u00fcber den Tag berechnet: = h\u00f6chster Tageswert - niedrigster Tagswert (in Prozent) Im zweiten Schritt wird die Ver\u00e4nderung zum vorherigen Tag berechnet: = ((Close-Wert des heutigen Tages - Close-Wert des gestrigen Tages) \/ Close-Wert des gestrigen Tages) * 100 shift() erm\u00f6glicht eine Verschiebung des zu betrachtenden Zeitfensters auf den vorherigen Tag.","37d2754f":"**3. Erstellung eines Portfolios auf Basis der Portfoliotheorie nach Markowitz**\n\n**3.1 Ratio erstellen**","817e2851":"**Gliederung**\n\n1. Datenerhebung und Explorative Datenanalyse (EDA)\n2. Erstellen eines Algorithmus zur Risikobewertung von Aktien auf Basis eines bestehenden Portfolios\n\n    * Regression\n    * Classification\n    * Clustering\n3. Erstellung eines Portfolios auf Basis der Portfoliotheorie nach Markowitz","af5569d0":"**2.5 Klassifizieren\/Clustering eines bestehenden Portfolios**\n\nNach der Ermittlung des passenden Modells zur Regressionsanalyse, folgt nun der n\u00e4chste Teil des Projektes. \nHier besch\u00e4ftigen wir uns mit der Findung eines passenden Modells zur Klassifizierung oder einem Clustering eines bestehenden Portfolios. Betrachtet wird hierbei zum einen die *kNN-Klassifikation*, sowie das *k-means Clustering*.\n\n\n**2.5.1 Grundlegende Aufgaben**\n\nF\u00fcr die Ausf\u00fchrung der *Nearest Neighbors Classification*  muss zun\u00e4chst die Anzahl der Klassen (k) selbst definiert werden. Dies wird hier auf Basis der *Mittleren Varianz* gemacht. Im Projekt haben wir uns f\u00fcr drei Klassen entschieden, welche wir als low, medium und high bezeichnen.\n\nF\u00fcr ein klein gew\u00e4hltes k besteht die Gefahr, dass Rauschen in den Trainingsdaten die Klassifikationsergebnisse verschlechtert.\n\nkNN ordnet jede Mittlere Varianz seinen drei n\u00e4chsten Nachbarn zu. So entstehen letztlich drei Klassen, welche auf die Genauigkeit \u00fcberpr\u00fcft werden m\u00fcssen.\n","c76fcd74":"**1.4.8 Visualisierung mittels Korrelationsmatrix**\n\nZur Visualisierung der Korrelation wird die in der pandas library vorhandene Funktion \"scatter_matrix\" herangezogen.","4315977e":"**2.1 Aufbereitung der Datens\u00e4tze f\u00fcr die Durchf\u00fchrung der Linearen Regression**\n\nWie bereits zu Beginn des Projektes erl\u00e4utert ist es bei der Kalkulation der Ver\u00e4nderung eines Wertes zu dessen Vorg\u00e4nger nicht m\u00f6glich den ersten Wert der Aktie zu berrechnen, da hierf\u00fcr keine Kalkulationsbasis vorliegt. Um eine Berechnung auf Basis aller Daten zu erm\u00f6glichen werden fehlende Werte durch den Median ersetzt.","1f4fb2da":"**1.4.6 Erste Visualisierung einer ausgew\u00e4hlten Aktie**\n\nIm ersten Schritt wird die Werteverteilung einzelner features mittels Histogrammen durchgef\u00fchrt.\n\nJedes Feature wird nun einzeln grafisch aufbereitet. Die Funktion wird erg\u00e4nzt durch \"figsize=(L\u00e4nge,Breite)\", um eine passende Gr\u00f6\u00dfe f\u00fcr die Darstellung zu erhalten. Hier l\u00e4sst sich nun im Histogramm beispielsweise mit Bezugnahme auf die Spalte \"close\" erkennen, dass die historischen Close-Werte der ausgew\u00e4hlten Aktie im zu begutachtenden Zeitraum meistens zwischen 200 und 400 lag.","7e268d2f":"\n**Ergebnis**\n\nMit dieser Methode k\u00f6nnen wir nun beliebige Indizes oder Portfolios von Investoren nach dem Risiko (Mittlere Varianz) clustern. Hat der Investor nun Interesse sein Portfolio um eine weitere Aktie zu erweitern, so ordnet unser Algorithmus diese einem gewissen Cluster zu.\n\nEine Aussage \u00fcber die Kursentwicklung und Volatilit\u00e4t des jeweiligen Clusters kann jedoch nicht getroffen werden.","818a9421":"**2.4.3 RMSE f\u00fcr Alle Aktien**\n\nDie erstellte Auswertung zeigt, dass beide Verfahren zur Validierung der Vorhersagegenauigkeit der Methoden den kleinsten Fehler bei der Ausf\u00fchrung der Linearen Regression auf Basis standardisierter Daten berechnet haben.\n\nDa bis zu diesem Zeitpunkt die Kalkulation innerhalb des Projektes jedoch lediglich an einer beispielhaften Aktie (Amazon) durchgef\u00fchrt wurde, soll die Validierung der Modelle zun\u00e4chst f\u00fcr alle Aktien duchgef\u00fchrt werden.\n\nDazu wird zun\u00e4chst die Kalkulation aller RMSE-Werte f\u00fcr alle Aktien und alle Methoden-Varianten (Stand.+LR, Stand.+SVR, Norm.+LR, Norm.+SVR) durchgef\u00fchrt. Anschlie\u00dfend werden die Ergebnisse anhand der Werte sortiert (aufsteigend nach deren der Fehlergr\u00f6\u00dfe).","087f2afb":"In diesem Schritt wird jedes lineare Modell in einem der 8 m\u00f6glichen Cluster zugeordnet. Die Spalte CLASS hat keine Relevanz f\u00fcr diesen Teil und wird deshalb ausgeschlossen.","3bc6ddd5":"**1. Datenerhebung und Explorative Datenanalyse (EDA)**\n\n**1.1 Datenauswahl**\n\nDie Datei beinhaltet historische Daten aller S&P 500 Aktien \u00fcber 5 Jahre. Als Quelle steht ein in Kaggle ver\u00f6ffentlichter Datensatz zur Verf\u00fcgung: https:\/\/www.kaggle.com\/camnugent\/sandp500. Folgende Spalten sind vorhanden:\n\n* Date - in format: yy-mm-dd \n* Open - price of the stock at market open (this is NYSE data so all in USD) \n* Close - price of stock at market close\n* High - Highest price reached in the day \n* Low - Lowest price reached in the day \n* Volume - Number of shares traded \n* Name - the stock's ticker name\n\n\n**1.2 Auswahl und Import bestimmter Libraries (Bibliotheken)**\n\nImportiert werden folgende Bibliotheken f\u00fcr die Programmiersprache Python: NumPy, matplotlib und pandas. Die Anzahl der Libraries wird im Laufe des Projektes noch erweitert.\n\n* NumPy ist eine Programmbibliothek f\u00fcr die Programmiersprache Python, die **eine einfache Handhabung von Vektoren, Matrizen oder generell gro\u00dfen mehrdimensionalen Arrays** erm\u00f6glicht. Neben den Datenstrukturen bietet Numpy auch effizient implementierte Funktionen f\u00fcr numerische Berechnungen an.\n\n* Matplotlib ist eine Programmbibliothek, die es erlaubt **mathematische Darstellungen aller Art** anzufertigen.\n\n* pandas ist eine Programmbibliothek, die Hilfsmittel f\u00fcr die **Verwaltung von Daten und deren Analyse** anbietet. Insbesondere enth\u00e4lt sie Datenstrukturen und Operatoren f\u00fcr den Zugriff auf numerische Tabellen und Zeitreihen.","502f6d11":"**1.4.5 Einf\u00fcgen der neuen Spalten in Datenbank**\n\nWichtig: Da die Ver\u00e4nderung zum vorherigen Tag in der ersten Zeile einer Aktie nicht ermittelt werden kann, wird das Feld mit NaN (englisch f\u00fcr Not a Number, \"keine Zahl\") beschrieben.","cc798ef4":"**Ergebnis **\n\nMit Hilfe der Erstellung eines Portfolios auf Basis der Portfoliotheorie nach Markowitz konnte \u00fcber den betrachteten Zeitraum ein Portfolio erstellt werden, welches sich Verzwanzigfacht hat. In anderen Worten konnte mit diesem Portfolio der Wert um den Faktor 20 gesteigert werden. Verglichen dazu hat der S&P500 im selben Zeitraum um rund 80 Prozent zugelegt. Dies ergibt einen Wachstumsfaktor von 0,8 im betrachteten Zeitraum.\n\nBeispiel:\n\n* Hat ein Investor zum Jahresbeginn 2013 10 Euro in den S&P500 angelegt, so erhielt er nach 5 Jahren ungef\u00e4hr 18 Euro.\n* Hat ein Investor zum Jahresbeginn 2013 10 Euro in das in dieser Arbeit entwickelte Portfolio gesteckt, so erhielt er nach 5 Jahren ungef\u00e4hr 200 Euro.","e9ee93c9":"**1.3 Einlesen der ausgew\u00e4hlten Datenbank (Name neu definiert: \"stock\") und Anzeige der obersten f\u00fcnf Zeilen (\"stock.head()\")**","340c37d0":"**3.2 Anwendung**","dd7491e2":"**2.2 Normalisierung und Standardisierung der Datengrundlage sowie Anwendung der Regressionsmethoden**\n\nDa der Anstieg des Aktienkurses einer Aktie um bspw. 20 USD innerhalb des betrachteten Datensatzes (S&P 500) bedingt durch unterschiedliche der Kalkulation zu Grunde liegende Ausgangswerte unterschieldiches prozentuales Wachstum darstellt, ist es zwingend notwendig die betrachteten Daten vergleichbar zu machen. Zur Generierung dieser Vergleichbarkeit sollen zwei Methoden angewandt und miteinander verglichen werden - die Standardisierung und die Normalisierung.\n\nStandardisierung Resultat der Stanardisierung ist die Transforamtion aller Werte (unabh\u00e4ngig von deren Verteilung und ihren urspr\u00fcnglichen Ma\u00dfeinheiten) zu kompatiblen und vergleichbaren Einheiten einer Verteilung mit einem Mittelwert von 0 und einer Standardabweichung von 1. Zur Kalkulation wird von jedem Messwert der arithmetische Mittelwert subtrahiert und die resultierende Differenz durch die Standardabweichung geteilt, wodurch man die sog. z-Werte (z-scores) erh\u00e4lt.\n\nNormalisierung Im Rahmen der Normalisierung von Daten werden diese mithilfe einer Transformationsfunktion in Relation zu einem bestimmten Bezugspunkt gesetzt um eine Vergleichbarkeit zu erm\u00f6glichen. Im Rahmen des Projektes wird die Normalisierung mithilfe der importierten Normalizer Funktion durchgef\u00fchrt.\n\nNachdem alle vorhandenen Daten in Relation gesetzt und somit vergleichbar gemacht wurden, werden nachfolgend zwei Regressionsvarianten angewendet und verglichen.\n\nLineare Regression Die Durchf\u00fchrung einer Regression (lat. regredi = zur\u00fcckgehen) hat das Ziel, anhand von mindestens einer unabh\u00e4ngigen Variablen x die Eigenschaften einer anderen abh\u00e4ngigen Variablen y zu prognostizieren. Wenn die abh\u00e4ngige Variable nur von einer unabh\u00e4ngigen Variablen beschrieben wird, so spricht man von einer einfachen linearen Regression, wird sie von mehreren unabh\u00e4ngigen Variablen beschrieben, handelt es sich um eine multiple lineare Regression. Wie ausgepr\u00e4gt die Aussagekraft und Genauigkeit der Prognose ist h\u00e4ngt von der Korrelation zwischen den abh\u00e4ngigen und unabh\u00e4ngigen Variablen ab - je h\u00f6her die Korrelation, desto genauer ist die Prognose.\n\nSupport Vector Regression (SVR) \/ Support Vector Machine (SVM) Eine SVM \/ SVR ist ein linearer Klassifikator, welcher auf dem Prinzip des maschinellen Lernens (ML) basiert, und in seiner Grundform lediglich zwischen zwei Klassen unterscheidet. Ausgangsbasis bildet wie bei allen ML-basierten Klassifikatoren eine vordefinierte Menge an Trainingsobjekten. Ziel ist es eine Gerade zu identifizieren welche die betrachteten Klassen optimal unterteilt, also deren Abstand zu den ihr am n\u00e4chsten gelegenen Trainingsobjekten maximal ist. Dementsprechend bildet sich zwischen den beiden Klassen eine Grenzebene welche maximiert werden soll.\n\nNachfolgend werden beide beschriebenen Regressionsmethoden sowohl nach der Standardisierung, als auch nach der Normalisierung der Daten angewendet. Die dazu notwendigen Code-Bestandteile werden im jeweiligen Schritt aus unterschiedlichen librarys importiert und angewendet.\n\n\n\n**2.3 Anwenden der Regressionsmethoden**\n\n**2.3.1 Normalisieren der Datens\u00e4tze und lineare Regression**","0dfeb389":"**2.5.3 Clustering mit K-Means++**\n\nNachdem im vorherigen Schritt die Klassen f\u00fcr die Klassifizierung selbst festgelegt werden mussten, k\u00f6nnen wir mit dieser Methode auf ein sogenanntes **Unsupervised Learning** zur\u00fcckgreifen. Der Algorithmus bestimmt k selbst.\n\nDas Unsupervised Learning ist ein daten-getriebenes Verfahren, bei dem feste Muster in den Daten ermittelt werden.\nEine Gruppierung der Daten ist im Unsupervised Learning initial nicht vorhanden. Eine Gruppierung ist das Ziel des Verfahrens.\n\nDer k-Means Algorithmus wird importiert. Im Index befinden sich nur die vorab berechneten Koeffizienten. Es werden alle Zeilen ber\u00fccksichtigt. In diesem Schritt wird zudem die Anzahl der Cluster ermittelt.","1ca199cc":"**3.3 Korrelation**","6b1119d4":"**2.Erstellen eines Algorithmus zur Risikobewertung von Aktien auf Basis eines bestehenden Portfolios**\n\nFinden einer geeigneten Regressionsmethode Ziel des folgenden Abschnittes ist es jene Regressionsmethode auszuw\u00e4hlen, die die h\u00f6chste Vorhersagegenauigkeit aufweist. Hierf\u00fcr wird zun\u00e4chst eine neue Library importiert: sklearn\n\nScikit-learn ist eine Bibliothek zum maschinellen Lernen f\u00fcr die Programmiersprache Python. Es bietet verschiedene Klassifikations-, Regressions- und Clustering-Algorithmen, darunter Support-Vektor-Maschinen, Random Forest, Gradient Boosting, k-means und DBSCAN, und ist so konzipiert, dass es mit der in diesem Projekt verwendeten Bibliotheken NumPy zusammenarbeitet.\n\nZun\u00e4chst soll die beste Regressionsmethode anhand der betrachteten Beispielaktie ausgew\u00e4hlt werden. sklearn bietet hierf\u00fcr eine M\u00f6glichkeit den train_test Split innerhalb weniger Zeilen Code durchzuf\u00fchren. Der Datensatz der ausgew\u00e4hlten Aktie wird dabei in einen \u00dcbungsdatensatz(train) und einen Pr\u00fcfungsdatensatz (test) unterteilt.\n\nUnrelevante Daten f\u00fcr den train_test split sind dabei das Datum und der Name, da diese f\u00fcr die Vorhersage des t\u00e4glichen Aktienpreises die geringste Bedeutung haben.\n\nDes Weiteren muss die das Target Feature Close von den Input Features (X_stock_a) getrennt werden. Vielmehr muss Close zu den Target Features (Y_stock_a) hinzugef\u00fcgt werden, um die Genauigkeit der Vorhersage messen zu k\u00f6nnen.","21ff7ea4":"Nachdem nun jede einzelne Aktie einem Cluster zugeordnet wurde, k\u00f6nnen wir uns diese Cluster, wie nachstehend, anzeigen lassen. Eine graphische Darstellung ist nicht zielf\u00fchrend, da wir uns in einem 6-dimensionalen Raum befinden (open, close, high, low, Tagesveraenderung, VeraenderungZumVorherigenTag). Mittels spezieller Kernel k\u00f6nnte eine zweidimensionale Ebene erreicht werden. Jedoch w\u00fcrde das den Rahmen des Projektes deutlich \u00fcberschreiten.","4ceea242":"**1.4.7 Korrelationermittlung zu Zielwert \"close\"**\n\nNun wird \u00fcberpr\u00fcft inwiefern die Close-Werte der ausgew\u00e4hlten Aktie mit anderen Spalten korrelieren. Dabei ist darauf zu achten, dass die Ergebnisse der Gr\u00f6\u00dfe nach absteigend sortiert werden (\"sort_values(ascending=False)\").","8ed7093e":"**2.4 Methodenvergleich**\n\nWie bereits beschrieben soll die jene Methode ausgew\u00e4hlt werden, welche die h\u00f6chste Vorhersagegenauigkeit aufweist. Beide Methoden der Regression prognostizieren Werte deren Vorhersagegenauigkeit nachfolgend \u00fcberpr\u00fcft und verglichen werden soll. Zur Auswahl der optimalen Methoden sollen zwei Verfahren angewendet werden.\n\nMean Absolute Error (MAE): der Durchschnitt der Abweichungen aller einzelner Messwerte vom prognostizierten Wert.\n\nRoot Mean Square Error (RMSE): die Abweichungen werden erst quadriert (dadurch verschwinden negative Vorzeichen), dann summiert und dann wird die Wurzel gezogen.\n\nNachfolgend werden die oberhalb genannten Verfahren jeweils f\u00fcr die Methode der LR und der SVR angewendet. Hierzu erfolgt der Import einer Funktion von sklearn.\n\n\n\n**2.4.1 MAE f\u00fcr LR und SVR - f\u00fcr Standardisierung und Normalisierung**"}}