{"cell_type":{"4d3410a8":"code","46e5a8e3":"code","06423ab0":"code","0867c591":"code","ba589393":"code","18b14ec2":"code","cf311cf9":"code","93751a61":"code","18481863":"code","5877e3c9":"code","1d4aed1e":"code","94275352":"code","96254ea0":"code","7d429e01":"code","1091d15a":"code","421a7750":"code","c6a56d2b":"code","fe6b2680":"code","1879039c":"code","899c3204":"code","32aa7c15":"code","9bcf0a35":"code","db9fa4a0":"code","05f6d41e":"code","3ab1a7bd":"code","8b9f02fe":"code","a2f650e5":"code","6bb763fe":"code","d1b54ee5":"code","0641debf":"code","4d4721b9":"code","85fa1ac6":"code","41cdf3f9":"code","7d0ad405":"code","bded8b0e":"code","8ef1003f":"code","2c6ce1cf":"code","17b1f1d8":"code","95b47a5d":"code","10ad6e0a":"code","75965154":"code","2b658591":"code","725e9f03":"code","a02e805f":"code","f921a371":"code","e8aaf47b":"code","ab2ebaab":"code","2f72eef5":"code","63dc6735":"code","747c5b08":"markdown","c97b07d8":"markdown","2fb535d0":"markdown","d803ead6":"markdown","8c314497":"markdown"},"source":{"4d3410a8":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode,iplot\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix","46e5a8e3":"df_alexa = pd.read_csv('..\/input\/amazon-alexa-reviews\/amazon_alexa.tsv', sep = '\\t')","06423ab0":"df_alexa.head()","0867c591":"positive = df_alexa[df_alexa['feedback'] == 1] \npositive","ba589393":"negative = df_alexa[df_alexa['feedback'] == 0]\nnegative","18b14ec2":"sns.countplot(df_alexa['feedback'], label = 'Count');","cf311cf9":"total = len(df_alexa)\nax1 = plt.figure(figsize=(12,5))\n\ng = sns.countplot(x='rating', data=df_alexa)\ng.set_title(\"Evaluation\", fontsize=20)\ng.set_xlabel(\"Evaluation\", fontsize=17)\ng.set_ylabel(\"Values\", fontsize=17)\nsizes = []\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=10) \ng.set_ylim(0, max(sizes) * 1.1)","93751a61":"df_alexa['rating'].hist(bins = 5);","18481863":"total = len(df_alexa)\nax1 = plt.figure(figsize=(40,15))\n\ng = sns.countplot(x='variation', data=df_alexa)\ng.set_title(\"Analizing\", fontsize=30)\ng.set_xlabel(\"Words\", fontsize=30)\ng.set_ylabel(\"Values\", fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.xticks(rotation=35)\nsizes = []\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=20) \ng.set_ylim(0, max(sizes) * 1.1)","5877e3c9":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(positive['verified_reviews']))\nplt.title('Description Positive', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","1d4aed1e":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(negative['verified_reviews']))\nplt.title('Description Negative', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","94275352":"df_alexa = df_alexa.drop(['date', 'rating'], axis = 1)","96254ea0":"df_alexa.head()","7d429e01":"variation_dummies = pd.get_dummies(df_alexa['variation'])","1091d15a":"variation_dummies","421a7750":"df_alexa.drop(['variation'], axis = 1, inplace=True)","c6a56d2b":"df_alexa.head()","fe6b2680":"df_alexa = pd.concat([df_alexa, variation_dummies], axis = 1)","1879039c":"df_alexa.head()","899c3204":"vectorizer = CountVectorizer()\nalexa_countvectorizer = vectorizer.fit_transform(df_alexa['verified_reviews'])","32aa7c15":"alexa_countvectorizer.shape","9bcf0a35":"type(alexa_countvectorizer)","db9fa4a0":"print(vectorizer.get_feature_names())","05f6d41e":"print(alexa_countvectorizer.toarray())","3ab1a7bd":"df_alexa.drop(['verified_reviews'], axis = 1, inplace=True)  ","8b9f02fe":"df_alexa.head()","a2f650e5":"reviews = pd.DataFrame(alexa_countvectorizer.toarray())","6bb763fe":"reviews.head()","d1b54ee5":"df_alexa = pd.concat([df_alexa, reviews], axis = 1)","0641debf":"df_alexa.head()","4d4721b9":"X = df_alexa.drop(['feedback'], axis = 1)","85fa1ac6":"X","41cdf3f9":"y = df_alexa['feedback']","7d0ad405":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)","bded8b0e":"X_train.shape","8ef1003f":"X_test.shape","2c6ce1cf":"classifier = tf.keras.models.Sequential()\nclassifier.add(tf.keras.layers.Dense(units = 400, activation='relu', input_shape=(4060,)))\nclassifier.add(tf.keras.layers.Dense(units = 400, activation='relu'))\nclassifier.add(tf.keras.layers.Dense(units = 1, activation='sigmoid')) ","17b1f1d8":"classifier.summary()","95b47a5d":"classifier.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])  ","10ad6e0a":"epochs_hist = classifier.fit(X_train, y_train, epochs=10)","75965154":"y_pred_train = classifier.predict(X_train)   \ny_pred_train","2b658591":"y_pred_train = (y_pred_train > 0.5)\ny_pred_train","725e9f03":"cm = confusion_matrix(y_train, y_pred_train)   \ncm","a02e805f":"sns.heatmap(cm, annot=True);","f921a371":"y_pred_test = classifier.predict(X_test)\ny_pred_test = (y_pred_test > 0.5)\ncm = confusion_matrix(y_test, y_pred_test)\ncm","e8aaf47b":"sns.heatmap(cm, annot=True);  # in the test phase the negative classification was not good for this algorithm, we will need to perform some more tests.","ab2ebaab":"epochs_hist.history.keys()","2f72eef5":"plt.plot(epochs_hist.history['loss'])\nplt.title('Model loss progress during training')\nplt.xlabel('Epoch')\nplt.ylabel('Training loss')\nplt.legend(['Training loss'])","63dc6735":"plt.plot(epochs_hist.history['accuracy'])\nplt.title('Model accuracy progress during training')\nplt.xlabel('Epoch')\nplt.ylabel('Training accuracy')\nplt.legend(['Training accuracy'])","747c5b08":"# **Data cleaning**","c97b07d8":"# About the Data\n\nThis dataset consists of a nearly 3000 Amazon customer reviews (input text), star ratings, date of review, variant and feedback of various amazon Alexa products like Alexa Echo, Echo dots, Alexa Firesticks etc. for learning how to train Machine for sentiment analysis.\n\nWhat you can do with this Data ?\nYou can use this data to analyze Amazon\u2019s Alexa product ; discover insights into consumer reviews and assist with machine learning models.You can also train your machine models for sentiment analysis and analyze customer reviews how many positive reviews ? and how many negative reviews ?\n\nSource\nExtracted from Amazon's website\n\nInspiration\nYour data will be in front of the world's largest data science community. What questions do you want to see answered?","2fb535d0":"# If you find this notebook useful, support with an upvote \ud83d\udc4d","d803ead6":"# **Tokenization**","8c314497":"# **Model Evaluation**"}}