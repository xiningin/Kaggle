{"cell_type":{"6ed056d9":"code","96c02733":"code","0bded4cb":"code","d3ea3f4a":"code","bf61c7da":"code","35d1c443":"code","a4152c14":"code","2f260971":"code","84359ce3":"code","69014861":"code","328e6faf":"code","ff52fd26":"code","44c13066":"code","2b1023f5":"code","a8583160":"code","b70cb7e9":"code","be9210ac":"code","dcad1ec5":"code","efbaf91e":"code","39c355be":"code","db9a044d":"code","e52b7160":"code","22824788":"code","7c98883e":"code","e6647bf7":"code","1826c4c7":"code","7d111f6f":"code","4c98e8bd":"code","5ecd999b":"code","d155ea4a":"code","4fa9dc7d":"code","3181a6ea":"code","96f8521c":"code","ba69a65b":"markdown","9bd31b26":"markdown","6134e92d":"markdown","c694438b":"markdown","796e400d":"markdown","65f55b29":"markdown","b1692511":"markdown","9328ee48":"markdown","7e239085":"markdown","14f280fd":"markdown","5d90b448":"markdown","1ad5c957":"markdown","05a6d8cb":"markdown","4b6eb290":"markdown","14fb73f7":"markdown","8c86b7ae":"markdown","2d387028":"markdown","1bc68e6e":"markdown","ef546083":"markdown","c74b6858":"markdown","f9754833":"markdown","8d53acec":"markdown","c7792b3b":"markdown","e11212cc":"markdown","29977b6a":"markdown","096edd25":"markdown","2f19b036":"markdown","456df410":"markdown","46183039":"markdown","ae1494cc":"markdown","8f8f23dd":"markdown","d284262a":"markdown","f4c51b52":"markdown"},"source":{"6ed056d9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.decomposition import PCA","96c02733":"!pip install -q forgebox","0bded4cb":"from forgebox.imports import *\nfrom forgebox.ftorch.prepro import split_df\nfrom forgebox.html import DOM\nfrom forgebox.images.widgets import view_images","d3ea3f4a":"# constant\nDATA = Path(\"\/kaggle\/input\/simplifiedhuarus\")\nVALID_RATIO = .2\n\ndef read_data(filename: str) -> pd.DataFrame:return pd.read_csv(DATA\/f'{filename}.csv')\n\ntotal_df = read_data(\"train\")\n# train\/valid split\ntrain_df, valid_df = split_df(total_df, valid = VALID_RATIO)\n\ntest_df = read_data(\"test\")","bf61c7da":"total_df.sample(10)","35d1c443":"train_df.vc(\"activity\")","a4152c14":"valid_df.vc(\"activity\")","2f260971":"feature_nums = train_df.query(\"activity=='LAYING'\").values[:200,2:].shape[1]\nfeature_nums","84359ce3":"plt.imshow(train_df.query(\"activity=='LAYING'\").values[:200,2:].astype(np.float32))","69014861":"plt.imshow(train_df.query(\"activity=='STANDING'\").values[:200,2:].astype(np.float32))","328e6faf":"plt.imshow(train_df.query(\"activity=='WALKING_UPSTAIRS'\").values[:200,2:].astype(np.float32))","ff52fd26":"from torch.utils.data import DataLoader,Dataset","44c13066":"y_map = dict((v,k) for k,v in enumerate(train_df.vc(\"activity\").index))","2b1023f5":"y_map","a8583160":"class ArrayDs(Dataset):\n    def __init__(self, df, y_map=y_map):\n        self.df = df\n        self.y_map = y_map\n        self.Xs = self.df.values[:,2:].astype(np.float32)\n        if 'activity' in self.df.columns:\n            self.has_y = True\n            self.Ys = self.df['activity'].apply(lambda x:y_map[x]).values\n        else:\n            self.has_y = False\n        \n    def __len__(self): return len(self.df)\n    \n    def __getitem__(self,idx):\n        if self.has_y:\n            return self.Xs[idx],self.Ys[idx]\n        else:\n            return self.Xs[idx]\n    \ndef get_data_dl(df: pd.DataFrame, batch_size: int=128, shuffle=False) -> DataLoader:\n    ds = ArrayDs(df)\n    return DataLoader(ds, shuffle=shuffle, batch_size=batch_size)","b70cb7e9":"x, y = ArrayDs(train_df,)[6]\n\nx, y","be9210ac":"!pip install -q pytorch-lightning==1.0.4","dcad1ec5":"from pytorch_lightning import LightningDataModule, LightningModule\nimport pytorch_lightning as pl\n\nclass AllData(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n        \n    def prepare_data(self):\n        self.total_df = read_data(\"train\")\n        # train\/valid split\n        self.train_df, self.valid_df = split_df(\n            self.total_df, valid = VALID_RATIO)\n        \n        self.test_df = read_data(\"test\")\n    \n    def train_dataloader(self): return get_data_dl(\n        self.train_df, shuffle=True)\n    \n    def val_dataloader(self): return get_data_dl(self.valid_df)\n    \n    def test_dataloader(self): return get_data_dl(self.test_df)\n    \n\nclass ltModule(LightningModule):\n    def __init__(self, base_model):\n        super().__init__()\n        self.base_model = base_model\n        self.crit = nn.CrossEntropyLoss()\n        self.accuracy = pl.metrics.Accuracy()\n            \n    def configure_optimizers(self):\n        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return opt\n    \n    def forward(self, x): return self.base_model(x)\n    \n    def forward_pass(self, batch):\n        x, y = batch\n        y_ = self(x)\n        loss = self.crit(y_,y)\n        acc = self.accuracy(y_.argmax(dim=-1),y)\n        return {'loss': loss, 'acc':acc}\n        \n    def training_step(self, batch, batch_idx): return self.forward_pass(batch)\n        \n    def validation_step(self, batch, batch_idx): return self.forward_pass(batch)\n    \n    def print_acc(self, outputs, phase):\n        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()\n        print(f\"[{phase}]\\tAccuracy:\\t{int(avg_acc.item()*100)}%\", end=\"\\t\")\n    \n    def training_epoch_end(self, outputs):\n        self.print_acc(outputs, \"TRAIN\")\n\n    def validation_epoch_end(self, outputs):\n        self.print_acc(outputs, \"VALID\")\n\ndef learn(base_model, max_epochs=5):\n    \"\"\"\n    Train the model automatially, the entire pipeline\n    \"\"\"\n    all_data = AllData()\n    module = ltModule(base_model)\n    trainer = pl.Trainer(max_epochs=max_epochs)\n    trainer.fit(model = module, datamodule=all_data, )","efbaf91e":"# create linear model\nlinear_model = nn.Linear(feature_nums,len(y_map))\n\n# learning process\nlearn(linear_model)","39c355be":"HIDDEN_SIZE = 512\n\n# create model structure\nneural_network = nn.Sequential(\n    nn.Linear(feature_nums, HIDDEN_SIZE),\n    nn.BatchNorm1d(HIDDEN_SIZE),\n    nn.ReLU(),\n    nn.Linear(HIDDEN_SIZE, len(y_map))\n)\n\n# learning process\nlearn(neural_network)","db9a044d":"def print_param_shape(model) -> pd.DataFrame:\n    \"\"\"print out the parameter shapes of a model\"\"\"\n    return pd.DataFrame(list({\"name\":k, \"shape\":tuple(p.T.shape)} \n        for k,p in model.named_parameters()))","e52b7160":"print_param_shape(linear_model)","22824788":"print_param_shape(neural_network)","7c98883e":"x_map = np.array(total_df.columns[2:])\nprint('\\t'.join(x_map[:50]))","e6647bf7":"y_map","1826c4c7":"linear_weight = linear_model.weight.data.T\nlinear_weight, linear_weight.shape","7d111f6f":"most_influential_rank = linear_weight.abs().sum(-1).argsort(descending=True).numpy()\n\nx_map[most_influential_rank][:30]","4c98e8bd":"import plotly.express as px\ndef plot_heat(w,xname,yname,x,y,x_axis_top=True, height=None):\n    pca = PCA(1,)\n    new_order = pca.fit_transform(w)[:,0].argsort()\n    new_w = w[new_order,:].T\n\n    fig = px.imshow(new_w,\n                    labels = dict(x=xname,y=yname),\n                    x=x[new_order],y=y)\n    if x_axis_top:\n        fig.update_xaxes(side=\"top\")\n    if height:\n        fig.update_layout(height=height)\n    fig.show()","5ecd999b":"plot_heat(\n    w=linear_weight[most_influential_rank][:30],\n    xname=\"features\",\n    yname=\"activities\",\n    x=x_map[most_influential_rank][:30],\n    y=np.array(list(y_map.keys()))\n)","d155ea4a":"plot_heat(\n    w=linear_model.bias.data.numpy()[None,:],\n    xname=\"bias\",\n    yname=\"activities\",\n    x=np.array([\"bias_layer\",]),\n    y=np.array(list(y_map.keys()))\n)","4fa9dc7d":"neural_network[0],neural_network[3]","3181a6ea":"plot_heat(\n    w=neural_network[0].weight.data.T[:80,:30],\n    xname=\"features\",\n    yname=\"hidden_feature\",\n    x=x_map[:80],\n    y=np.arange(30)\n)","96f8521c":"plot_heat(\n    w=neural_network[3].weight.data.T[:30,:],\n    xname=\"hidden_feature\",\n    yname=\"activity\",\n    x=np.array(list(map(lambda i:f\"H{i}\",range(30)))),\n    y=np.array(list(y_map.keys())),\n)","ba69a65b":"Here $f(x)$ should not be different models depend on where we at within the sentence. It should be one single model that being ran **recurrently**. Hence the term RNN(recurrent neural networks)","9bd31b26":"## Break linearity\n> But linearity can not simulate a full spectrum of ```OR, AND, XOR``` logic gates, the weight $W_{ij}$ simply tells \"the bigger\/smaller $X_{i}$ the better for lable $Y_{j}$\"\n\n> Before neural network, what people usually tried, is the complicated feature enginearing to break linearity: to treat things like $X_{1}^2X_{2}, X_{1}X_{2}^2, X_{1}^2X_{2}^2, X_{1}^3X_{2}$... as extra part of input dimension \n\n\n### Where does hidden layer comes in \nInstead of $X_{i} \\times W_{ij} => Y_{j}$, we do $X_{i} \\times L1_{ik} => H_{k}, H_{k}\\times L2_{kj} => Y_{j}$\n\n> Neural network just using a linear model to fit input into hidden neurons (summarize input features into a middle layer), then use another linear model to fit hidden neurons into output. Hence breaking the linearity.\n\n### Visualize input to hidden layer\n> We pick first 80 input features and first 30 hidden neurons to visualize","6134e92d":"f(\"I will not fe\") => \"a\"\n\nf(\" will not fea\") => \"r\"\n\nf(\"will not fear\") => \".\"\n\n...\n\nf(\"Only I will re\") => \"m\"\n\nf(\"nly I will rem\") => \"a\"\n\nf(\"ly I will rema\") => \"i\"\n\nf(\"y I will remai\") => \"n\"","c694438b":"# What goes from here","796e400d":"### Stuck an Neural network in Bellman equation\n\n$\\large V^{\\pi*}(s)=  \\max_a \\{ {R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^{\\pi*}(s')} \\}.\\ $\n\nBut [with deep nn you can play all sorts of atari game](https:\/\/www.cs.toronto.edu\/~vmnih\/docs\/dqn.pdf)","65f55b29":"### Weights structure","b1692511":"Output categories","9328ee48":"## Visualize","7e239085":"We have 561 input features at our disposal","14f280fd":"Install extra [packages](https:\/\/github.com\/raynardj\/forgebox)","5d90b448":"## Break Linearity\n\n> Basic neural network","1ad5c957":"### Visualize hidden layer to output\n\n> We pick first 30 hidden neurons to all our output categories for visualize","05a6d8cb":"## Linear relation","4b6eb290":"If a sentence, eg. ```I will not fear. Fear is the mind-killer. I will face my fear. I will let it pass through me. When the fear has gone, there shall be nothing. Only I will remain.```","14fb73f7":"## Dataset\n### Simplified Human Activity Recognition w\/Smartphone\n\nSee the [dataset detail](https:\/\/www.kaggle.com\/mboaglio\/simplifiedhuarus)\n\n> Abstract: Human Activity Recognition database built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors.\n\n> Data Set Characteristics: Multivariate, Time-Series\n\n> Data Set Information:\n\n> The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKINGUPSTAIRS, WALKINGDOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n\n> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings\/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\n> Check the README.txt file for further details about this dataset.\n\n> An updated version of this dataset can be found at [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/human+activity+recognition+using+smartphones). It includes labels of postural transitions between activities and also the full raw inertial signals instead of the ones pre-processed into windows.","8c86b7ae":"## Some playing around\n\n* Try use your [face to control pac man](https:\/\/storage.googleapis.com\/tfjs-examples\/webcam-transfer-learning\/dist\/index.html)\n\n* A neural [network playground](https:\/\/playground.tensorflow.org\/)","2d387028":"## Build up dataloader","1bc68e6e":"### Use the same model weights through time over and over again","ef546083":"## Training boiler template\n\n> You can also use sklearn at this part","c74b6858":"## About Ray\n* [Kaggle profile](https:\/\/www.kaggle.com\/raynardj)\n    * Competition notes for [Understanding Clouds from Satellite Images](https:\/\/github.com\/iofthetiger\/ucsi)\n    * and [Global Wheat Detection](https:\/\/github.com\/iofthetiger\/gwd)\n    * and [Recursion Cell Image Classification](https:\/\/github.com\/raynardj\/python4ml\/tree\/master\/experiments\/rcic)\n* My [experiments](https:\/\/genomicare.github.io\/docs\/docs\/experiments\/) at genomicare\n* My [python tutorial](https:\/\/github.com\/raynardj\/python4ml) designed for machine learning","f9754833":"### Use the same model weights through space over and over again","8d53acec":"### Visualize data signal pattern","c7792b3b":"Input features, we print out the first 50 features of our input columns","e11212cc":"# What's and why is all the fuss?\n\n### What?\n\nIn a simple one liner answer:\n\nNeural network is one of the most powerful, easiest to use **predictor**, that given x, any shape of x to predict y, any shape of y.\n\nI can rephrase the sentence even longer, but it is in its essence, an awesome **predictor**\n\nThink of $y = f(x)$,  the $f$ is a function(model), a predictor\n\n### Why?\n\n* It often works, with less effort than you expected, often better than human judgement\n* Paper in this area often compare model learning theory to human recognition.\n* It's like a warp engine for other traditional model.\n* x and y can be really noisy, in some twisted & wierd shape, AKA, it predicts all sorts of problem set, with **much less preprocessing**, **much less feature engineering**\n* Real life data itself is noisy, in some twisted & wierd shape","29977b6a":"Target categories mapped to indices","096edd25":"# Break linearity example\n\n> Why does neural network rules the fitting power since 2006\n\n> This is a notebook intended for company(Genomicare Bio) training, to help people feel more friendly about NN\n\nYou certainly hear ```neural network``` & ```deep learning``` a lot in recent years. Yes it is something really advanced in certain aspects, but it's just a tool, which is much easier to use than its predecessor: linear regression.\n\nThis notebook is yet another effort to democratize deep learning. \n\nThis kind of graph appear at many tutorials\n\n![Simple nn view from wikipedia](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e4\/Artificial_neural_network.svg)","2f19b036":"### Linear visualize\n> We visualize the influences directly from ```input``` features to ```output``` categories\n\nThe size of the weights are 561 columns x 6 labels\n\nHere we pick the most influential 30 input features for visualization","456df410":"# Let several models play with\/ teach \/fight each other\n\nWith [this paper](https:\/\/arxiv.org\/pdf\/1406.2661.pdf) end up from a bar fight, people start to play with serveral models interacting together.\n\nAnd this [imaginative paper](https:\/\/arxiv.org\/pdf\/1703.10593.pdf) is one of the good example","46183039":"* For genomicarers. Just look for \ud83d\udcaaRay\ud83d\ude32\n* Mostly, python\n* [Kaggle](www.kaggle.com): competitions, notebooks to boost start the unfamiliar problem set\n* [fast.ai](fast.ai): a tutorial:\n    * almost writing a DL library from scratch\n    \n## Environments\n\n* [Google Colab](https:\/\/colab.research.google.com\/), free environment, GPU\/TPU\n* Kaggle kernels, also free environment, GPU\/TPU","ae1494cc":"We have a pretty balanced dataset","8f8f23dd":"## Train data and test(validation) data","d284262a":"# How to start?","f4c51b52":"### Find influence"}}