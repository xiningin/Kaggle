{"cell_type":{"1a9742bb":"code","095cbf39":"code","b43e1bf2":"code","0db02b40":"code","7b9a1c94":"code","b9fa2e39":"code","6baf4970":"code","65fe0ab6":"code","9ecee1b9":"code","50cdcf49":"code","1454b4c0":"code","79fcb41b":"code","b2b0dac7":"code","69f30dfb":"code","d99f6305":"code","33fac84b":"code","2c27340e":"code","4ebfe6ca":"code","04302d69":"code","e0630975":"code","ca20f53b":"code","b2c887cc":"code","6a90df8d":"code","128b2463":"code","54626eec":"code","fc2777f7":"code","5f647e85":"code","fe021f1d":"code","5f58928f":"code","7cd7e04d":"code","19d20914":"code","e5d50408":"code","60dd0840":"code","7b387883":"code","b649bfe6":"code","aa5f4b33":"code","5a646acd":"code","fb51158a":"code","ba79c927":"code","f7dd8b4f":"code","e90343e4":"code","ac987d03":"code","75f80ccb":"code","594e4bce":"code","68d14d80":"code","b13d318a":"markdown","a2306853":"markdown","d8bf6166":"markdown","526e138b":"markdown","afe217cb":"markdown","86ae3cbf":"markdown","36390c49":"markdown","349952f4":"markdown","b1f85ea1":"markdown","f6458197":"markdown","f31f7b66":"markdown","5a06fee7":"markdown","e992b91c":"markdown","e520fe98":"markdown","1bc62797":"markdown","b47776db":"markdown","ccdd01ea":"markdown","f130c5ef":"markdown","818be97a":"markdown","8293843a":"markdown","f7ec0327":"markdown","773403f6":"markdown","a0f7c811":"markdown"},"source":{"1a9742bb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","095cbf39":"import os\nimport pickle\nfrom matplotlib import pyplot as plt","b43e1bf2":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";","0db02b40":"!pip -q install ktrain","7b9a1c94":"import ktrain\nfrom ktrain import text","b9fa2e39":"DATA_PATH = \"..\/input\/defi-ia-insa-toulouse\"\ntrain_df = pd.read_json(DATA_PATH+\"\/train.json\").set_index('Id')\ntest_df = pd.read_json(DATA_PATH+\"\/test.json\").set_index('Id')\ntrain_label = pd.read_csv(DATA_PATH+\"\/train_label.csv\").set_index('Id')\ncategories_string = pd.read_csv(DATA_PATH+\"\/categories_string.csv\")\n# template_submissions = pd.read_csv(DATA_PATH+\"\/template_submissions.csv\").set_index('Id')","6baf4970":"names = categories_string['0'].to_dict()\njobs = train_label['Category']\njobs = jobs.map(names)\njobs = jobs.rename('job')\ngenders = train_df[\"gender\"]\npeople = pd.concat((jobs, genders), axis='columns')\npeople.head()","65fe0ab6":"train = people[[\"gender\", \"job\"]].reset_index(   # need to keep the index as a column\n        ).groupby([\"gender\", \"job\"]                  # split by \"group\"\n        ).apply(lambda x: x.sample(500, replace=True) # in each group, do the random split\n        ).reset_index(drop=True              # index now is group id - reset it\n        ).set_index(\"Id\")                 # reset the original index\nval = people.drop(train.index)[[\"gender\", \"job\"]].reset_index(   # need to keep the index as a column\n        ).groupby([\"gender\", \"job\"]                  # split by \"group\"\n        ).apply(lambda x: x.sample(50, replace=True) # in each group, do the random split\n        ).reset_index(drop=True              # index now is group id - reset it\n        ).set_index(\"Id\")                 # reset the original index\n\ntrain = train.sample(frac=1.)\ntrain.head()","9ecee1b9":"print(train.shape, val.shape)\ntrain = train[~train.index.duplicated(keep='first')]\nval = val[~val.index.duplicated(keep='first')]\nprint(train.shape, val.shape)","50cdcf49":"train_df.head()","1454b4c0":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","79fcb41b":"train[\"text\"] = train_df.loc[train.index][\"description\"].values\nval[\"text\"] = train_df.loc[val.index][\"description\"].values\ntrain.head()","b2b0dac7":"model_name = \"distilbert-base-uncased\"\nt = text.Transformer(model_name, maxlen=125)","69f30dfb":"trn = t.preprocess_train(train[\"text\"].values, train[\"job\"].values)\nvl = t.preprocess_test(val[\"text\"].values, val[\"job\"].values)","d99f6305":"import gc\n\ngc.collect()","33fac84b":"model = t.get_classifier()\nlearner = ktrain.get_learner(model, train_data=trn, val_data=vl, batch_size=64)","2c27340e":"model.summary()","4ebfe6ca":"learner.lr_find(max_epochs=3, suggest=True, show_plot=True)","04302d69":"learner.lr_plot(suggest=True)","e0630975":"learner.fit_onecycle(5e-5, 5)","ca20f53b":"learner.history.history.keys()","b2c887cc":"learner.plot()","6a90df8d":"plt.plot(learner.history.history['accuracy'])\nplt.plot(learner.history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","128b2463":"corr = learner.validate(class_names=t.get_classes())","54626eec":"learner.view_top_losses(n=3, preproc=t)","fc2777f7":"print(val.loc[val.index[1223]][\"text\"])","5f647e85":"print(val.loc[val.index[1431]][\"text\"])","fe021f1d":"predictor = ktrain.get_predictor(learner.model, preproc=t)","5f58928f":"predictor.predict(\"She also teaches an after-hours Krav Maga class for interested Freelancers and she's loosely affiliated with the Russian mob. With her livelihood up in smoke, \\\n                  she wants answers. The Russians want answers, and it feels like not everyone is being completely honest.\")","7cd7e04d":"pd.DataFrame(predictor.predict_proba('After working for three months in a Google lab in the USA. He is now a cloud computing engineer at IBM.').reshape(1, -1),\n            columns=np.array(predictor.get_classes()))","19d20914":"predictor.save(\".\/output\/fairness\")\nload_predictor = ktrain.load_predictor(\".\/output\/fairness\")","e5d50408":"!pip install -q git+https:\/\/github.com\/amaiya\/eli5@tfkeras_0_10_1","60dd0840":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","7b387883":"predictor.explain(\"She also teaches an after-hours Krav Maga class for interested Freelancers and she's loosely affiliated with the Russian mob. With her livelihood up in smoke, she wants answers. \\\nThe Russians want answers, and it feels like not everyone is being completely honest.\")","b649bfe6":"predictor.explain(\"He was also the editor of the Sunday Datebook for 14 years during a 34-year newspaper career at the San Francisco Chronicle.\\\nHe has worked on \u201cThe Voice\u201d at the Marsh Theater with David Ford, Ann Randolph and Mark Kenward for the last two years. \\\n                  The full piece will be present at the Marsh San Francisco on May 10.\")","aa5f4b33":"props = predictor.predict_proba(val[\"text\"].values)","5a646acd":"props.shape","fb51158a":"def get_nd(arr):\n    return np.array([arr==i for i in np.array(predictor.get_classes())])        ","ba79c927":"def reverse_job(df):\n    return df.replace({j:i for i,j in names.items()})","f7dd8b4f":"preds = predictor.predict(val[\"text\"].values)\npreds[:5]","e90343e4":"val[\"job_pred\"] = preds\nval.sample(20).head(10)","ac987d03":"counts = val[['job_pred', 'gender']].groupby(['job_pred', 'gender']).size().unstack('gender')\ncounts['disparate_impact'] = counts[['M', 'F']].max(axis='columns') \/ counts[['M', 'F']].min(axis='columns')","75f80ccb":"counts","594e4bce":"counts[\"disparate_impact\"].mean()","68d14d80":"counts = val[['job', 'gender']].groupby(['job', 'gender']).size().unstack('gender')\ncounts['disparate_impact'] = counts[['M', 'F']].max(axis='columns') \/ counts[['M', 'F']].min(axis='columns')\ncounts[\"disparate_impact\"].mean()","b13d318a":"### Remove duplicated\n","a2306853":"# Introduction","d8bf6166":"### Load pre-trained model\n\nWe will use **distilbert-base-uncased**","526e138b":"### Import ktrain\n\n**[Ktrain](https:\/\/github.com\/amaiya\/ktrain)** is a lightweight wrapper for the deep learning library TensorFlow Keras (and other libraries) to help build, train, and deploy neural networks and other machine learning models. \n\nInspired by ML framework extensions like fastai and ludwig, ktrain is designed to make deep learning and AI more accessible and easier to apply for both newcomers and experienced practitioners. With only a few lines of code, ktrain allows you to easily and quickly.","afe217cb":"## Model Performance","86ae3cbf":"\n## Advantages of Fine-Tuning\n","36390c49":"### Split data\nSplit data into train and validation data","349952f4":"### View model top losses","b1f85ea1":"We'll use [Defi IA dataset for job classification](https:\/\/www.kaggle.com\/c\/defi-ia-insa-toulouse\/data) dataset for single sentence classification. It's a set of sentences labeled as 28 jobs classes. The data has been retrieved from [CommonCrawl](https:\/\/www.wikiwand.com\/en\/Common_Crawl). The latter has been famously used to train OpenAI's GPT-3 model. The data is therefore representative of what can be found on the English speaking part of the Internet, and thus contains a certain amount of bias. One of the goals of this competition is to design a solution that is both accurate as well as fair, as explained in the Evaluation section.\n","f6458197":"### Suggestion\n\nSuggest learning","f31f7b66":"\n## History\n\n2018 was a breakthrough year in NLP. Transfer learning, particularly models like Allen AI's ELMO, OpenAI's Open-GPT, and Google's BERT allowed researchers to smash multiple benchmarks with minimal task-specific fine-tuning and provided the rest of the NLP community with pretrained models that could easily (with less data and less compute time) be fine-tuned and implemented to produce state of the art results. Unfortunately, for many starting out in NLP and even for some experienced practicioners, the theory and practical application of these powerful models is still not well understood.\n","5a06fee7":"### reload the predictor\nreloaded_predictor = ktrain.load_predictor('.\/output\/predictor_ktrain')","e992b91c":"### Hyperparmeter learning rate\n\nSimulating training for different learning rates... this may take a few moments...","e520fe98":"### [Context of this work : D\u00e9fi IA](https:\/\/www.kaggle.com\/c\/defi-ia-insa-toulouse\/leaderboard)\n\nThis edition of the D\u00e9fi IA pertains to NLP. The task is straightforward: assign the correct **job category** to a **job description**.\n\n### Desciption\n\nThe approach used here to make the model **fair** is to use as many descriptions of jobs of both sexes.\nThis is similar to when you have an unbalanced dataset, and you decide to reduce the number of samples in each class to the minimum sample.\n\n### Results of this notebook\n\n|Metric       |precision       |recall       |f1-score       |support       |\n|:-:    |:-:    |:-:    |:-:    |--:    |\n|accuracy       |       |       |0.80       |2423       |\n|macro avg       |0.81       |0.80       |0.80       |2423       |\n|weighted avg       |0.81       |0.80       |0.80       |2423       |\n|fairness predict       |       |       |       |2.817007881684417       |\n|fairness valid       |       |       |       |2.8221477674508764       |\n","1bc62797":"### Model summary","b47776db":"### Fit one cylcle\n\nFit one cycle on 5 epochs","ccdd01ea":"# 2. Loading Defi IA Dataset\n","f130c5ef":"\nI will use Bert to train a text classifier. Specifically, we will take the pre-trained Bert model, add an untrained layer of neurons on the end, and train the new model for our classification task. Why do this rather than train a train a specific deep learning model (a CNN, BiLSTM, etc.) that is well suited for the specific NLP task you need? \n\n1. **Quicker Development**\n\n    * First, the pre-trained Bert model weights already encode a lot of information about our language. As a result, it takes much less time to train our fine-tuned model - it is as if we have already trained the bottom layers of our network extensively and only need to gently tune them while using their output as features for our classification task. In fact, the authors recommend only 2-4 epochs of training for fine-tuning Bert on a specific NLP task (compared to the hundreds of GPU hours needed to train the original Bert model or a LSTM from scratch!). \n\n2. **Less Data**\n\n    * In addition and perhaps just as important, because of the pre-trained weights this method allows us to fine-tune our task on a much smaller dataset than would be required in a model that is built from scratch. A major drawback of NLP models built from scratch is that we often need a prohibitively large dataset in order to train our network to reasonable accuracy, meaning a lot of time and energy had to be put into dataset creation. By fine-tuning Bert, we are now able to get away with training a model to good performance on a much smaller amount of training data.\n\n3. **Better Results**\n\n    * Finally, this simple fine-tuning procedure (typically adding one fully-connected layer on top of Bert and training for a few epochs) was shown to achieve state of the art results with minimal task-specific adjustments for a wide variety of tasks: classification, language inference, semantic similarity, question answering, etc. Rather than implementing custom and sometimes-obscure architetures shown to work well on a specific task, simply fine-tuning BERT is shown to be a better (or at least equal) alternative.\n","818be97a":"\n### A Shift in NLP\n\nThis shift to transfer learning parallels the same shift that took place in computer vision a few years ago. \n\nCreating a good deep learning network for computer vision tasks can take millions of parameters and be very expensive to train. Researchers discovered that deep networks learn hierarchical feature representations (simple features like edges at the lowest layers with gradually more complex features at higher layers). Rather than training a new network from scratch each time, the lower layers of a trained network with generalized image features could be copied and transfered for use in another network with a different task. It soon became common practice to download a pre-trained deep network and quickly retrain it for the new task or add additional layers on top - vastly preferable to the expensive process of training a network from scratch. For many, the introduction of deep pre-trained language models in 2018 (ELMO, BERT, ULMFIT, Open-GPT, etc.) signals the same shift to transfer learning in NLP that computer vision saw.\n\nLet's get started!","8293843a":"Use of GPU","f7ec0327":"### Visualize fit results","773403f6":"## What is Bert ?\n\n**BERT** (Bidirectional Encoder Representations from Transformers), released in late 2018, is the model we will use in this notebook to provide readers with a better understanding of and practical guidance for using transfer learning models in NLP. \n\n**BERT** is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.\n\nThis notebook will explain how you can modify and fine-tune BERT to create a powerful NLP model that quickly gives you state of the art results. \n","a0f7c811":"### Model and Learner"}}