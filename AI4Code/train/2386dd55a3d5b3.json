{"cell_type":{"4b411433":"code","dc4bbd09":"code","dddf7790":"code","9014ce96":"code","0e482326":"code","68aa6aaa":"code","82ed80a8":"code","610b52ee":"code","b6b3a213":"code","ec3cfc64":"code","eaa0cf92":"code","61a81ea4":"code","25b6a9a2":"code","579b1a44":"code","2e6a6ef6":"code","d8b13705":"code","f565da8f":"code","a3d8bf39":"code","599c611d":"code","f45fc801":"code","475d4b1d":"code","ee76bade":"code","5a769391":"code","cfde25eb":"code","4088aa0c":"code","6004e74c":"code","ead35d8e":"code","d969d984":"code","3c496d8e":"code","a1ec332b":"code","96931af3":"code","da82d755":"code","505d7181":"code","11943dc4":"code","f2d3da73":"code","4377521e":"code","2a4aa3bd":"code","9c2baa8f":"code","4c71cdd3":"code","7a649591":"code","8cea2523":"code","351b7001":"code","a70d81b8":"code","ff866647":"code","01262655":"code","282e3f87":"code","f47e4d11":"code","09b7ddf8":"code","f35d20b7":"code","61cdc828":"code","068561f2":"code","60ff7ffb":"code","e47e6072":"code","f0991760":"code","2e67253b":"code","1a4801f6":"code","29c6b3bf":"code","1f7d398a":"markdown","9f093389":"markdown","8266316e":"markdown","0a99cf0d":"markdown","898e4cc1":"markdown","056465fb":"markdown","f5fb0b5b":"markdown","94c1f015":"markdown","f0246d1f":"markdown","fe061363":"markdown","31a94376":"markdown","53088daf":"markdown","8a0304ff":"markdown","aaf42697":"markdown","a3a10059":"markdown","252c7467":"markdown","7a9a3895":"markdown","afbac992":"markdown","76872e5e":"markdown","7f2ff319":"markdown","def484a4":"markdown"},"source":{"4b411433":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dc4bbd09":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\nimport graphviz","dddf7790":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc","9014ce96":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\n### XG Boost\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n### Auto encoder \nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense, Activation, Embedding, concatenate, Flatten, Input\nfrom keras.optimizers import Adam\n\n### Support Vector Machine \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC","0e482326":"# Loading the train,test csv files \nTitanic_data = pd.read_csv(\"..\/input\/train.csv\", header=0)\nTitanic_data_test_pID = pd.read_csv(\"..\/input\/test.csv\", header=0)\nTitanic_data_test = pd.read_csv(\"..\/input\/test.csv\", header=0)","68aa6aaa":"# Describing the dataframe to check what are all data is present and the distributions of the data \nTitanic_data.describe()","82ed80a8":"sns.countplot(x = \"Sex\", hue = \"Survived\", data = Titanic_data)\nplt.show()\nsns.countplot(x=\"Pclass\", hue = \"Survived\", data = Titanic_data)\nplt.show()\nsns.countplot(x=\"SibSp\", hue = \"Survived\", data = Titanic_data)\nplt.show()\nsns.countplot(x=\"Parch\", hue = \"Survived\", data = Titanic_data)\nplt.show()\nsns.countplot(x=\"Embarked\", hue = \"Survived\", data = Titanic_data)\nplt.show()\nsns.catplot(x = \"Pclass\", hue = \"Survived\", col = \"Sex\", kind = \"count\", data = Titanic_data)\nplt.show()\nsns.catplot(x = \"Parch\", hue = \"Survived\", col = \"Sex\", kind = \"count\", data = Titanic_data)\nplt.show()\nsns.catplot(x = \"SibSp\", hue = \"Survived\", col = \"Sex\", kind = \"count\", data = Titanic_data)\nplt.show()\nsns.catplot(x = 'Pclass',y = 'Survived', hue='Sex', data=Titanic_data, kind = \"point\") # equivalent to sns.pointplot\nplt.show()\nsns.catplot(x = 'Embarked',hue = 'Survived', col='Pclass', data=Titanic_data, kind = \"count\")\nplt.show()","610b52ee":"for i in range(len(Titanic_data)):\n    a = str(Titanic_data['Cabin'][i])\n    if a.count(' ') >=1:\n        Titanic_data['Fare'][i] = (Titanic_data['Fare'][i]\/(a.count(' ')+1))\n    ","b6b3a213":"for i in range(len(Titanic_data_test)):\n    a = str(Titanic_data_test['Cabin'][i])\n    if a.count(' ') >=1:\n        Titanic_data_test['Fare'][i] = (Titanic_data_test['Fare'][i]\/(a.count(' ')+1))","ec3cfc64":"Titanic_data = Titanic_data.drop(['PassengerId' , 'Name' , 'Ticket','Cabin'], axis=1)\nTitanic_data_test = Titanic_data_test.drop(['PassengerId' , 'Name' , 'Ticket','Cabin'], axis=1)","eaa0cf92":"Titanic_data.shape","61a81ea4":"Titanic_data.dtypes","25b6a9a2":"num_attr = ['Age','Fare']\nnum_attr","579b1a44":"cat_attr = ['Sex','Embarked','Survived','Pclass','SibSp','Parch']\ncat_attr","2e6a6ef6":"## Roundoff the values of flot to make more significance of the values \nTitanic_data['Age'] = round((Titanic_data['Age']),0)\nTitanic_data['Fare'] = round((Titanic_data['Fare']),0)\n\nTitanic_data_test['Age'] = round((Titanic_data_test['Age']),0)\nTitanic_data_test['Fare'] = round((Titanic_data_test['Fare']),0)","d8b13705":"Titanic_data['Age'].fillna((Titanic_data['Age'].median()), inplace=True)\nTitanic_data_test['Age'].fillna((Titanic_data_test['Age'].median()), inplace=True)\nTitanic_data_test['Fare'].fillna((Titanic_data_test['Fare'].median()), inplace=True)","f565da8f":"for i in cat_attr:\n    Titanic_data[i] = Titanic_data[i].astype('category')\n","a3d8bf39":"for i in cat_attr:\n    if i not in ['Survived']:\n        Titanic_data_test[i] = Titanic_data_test[i].astype('category')","599c611d":"for i in num_attr:\n    Titanic_data[i] = Titanic_data[i].astype('int64')","f45fc801":"for i in num_attr:\n    Titanic_data_test[i] = Titanic_data_test[i].astype('int64')","475d4b1d":"Titanic_data_test.isna().sum()","ee76bade":"Titanic_data.isna().sum()","5a769391":"f, (ax) = plt.subplots(1, 1, figsize=(50, 10))\nTitanic_data['Age'].value_counts().plot(kind='bar')","cfde25eb":"f, (ax) = plt.subplots(1, 1, figsize=(50, 10))\nTitanic_data['Fare'].value_counts().plot(kind='bar')","4088aa0c":"Titanic_data.head()\nf, (ax) = plt.subplots(1, 1, figsize=(50, 10))\nTitanic_data.boxplot()","6004e74c":"y= Titanic_data[\"Survived\"]\nX= Titanic_data.drop('Survived',axis=1)","ead35d8e":"X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.3 , random_state = 2, stratify=y)\nX_train.shape","d969d984":"X_test.shape","3c496d8e":"cat_attr_x = ['Sex','Embarked','Pclass','SibSp','Parch']\ncat_attr_x\n\nX_train_imputed_dummies = pd.get_dummies(columns=cat_attr_x, data=X_train,prefix=cat_attr_x, prefix_sep=\"_\", drop_first=True)\nX_test_imputed_dummies = pd.get_dummies(columns=cat_attr_x, data=X_test,prefix=cat_attr_x, prefix_sep=\"_\", drop_first=True)\nTitanic_data_test_dummies = pd.get_dummies(columns=cat_attr_x, data=Titanic_data_test,prefix=cat_attr_x, prefix_sep=\"_\", drop_first=True)\n\nX_train_imputed_dummies.columns","a1ec332b":"Titanic_data_test_dummies.shape","96931af3":"Titanic_data_test_dummies.columns","da82d755":"Titanic_data_test_dummies=Titanic_data_test_dummies.drop(['Parch_9'], axis=1)","505d7181":"scaler = StandardScaler()\nscaler.fit(X_train[num_attr])\n\nX_train_imputed_dummies[num_attr]=scaler.transform(X_train_imputed_dummies[num_attr])\nX_test_imputed_dummies[num_attr]=scaler.transform(X_test_imputed_dummies[num_attr])\nTitanic_data_test_dummies[num_attr]=scaler.transform(Titanic_data_test_dummies[num_attr])","11943dc4":"y_train.shape","f2d3da73":"clf = tree.DecisionTreeClassifier(max_depth=7)\nclf = clf.fit(X_train_imputed_dummies,y_train)","4377521e":"np.argsort(clf.feature_importances_)","2a4aa3bd":"importances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\npd.DataFrame([X_train_imputed_dummies.columns[indices],np.sort(importances)[::-1]])","9c2baa8f":"dot_data = tree.export_graphviz(clf, out_file=None, \n                                feature_names=X_train_imputed_dummies.columns,\n                                class_names='target', \n                                filled=True, rounded=True, special_characters=True) \ngraph = graphviz.Source(dot_data) \ngraph","4c71cdd3":"train_pred = clf.predict(X_train_imputed_dummies)\nprint(accuracy_score(y_train,train_pred))\ntest_pred = clf.predict(X_test_imputed_dummies )\nprint(accuracy_score(y_test,test_pred))","7a649591":"fpr, tpr, thresholds = roc_curve(y_train, train_pred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr,tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","8cea2523":"Titanic_data_test_pred = clf.predict(Titanic_data_test_dummies )\n\nsubmission_svc = pd.DataFrame({ 'PassengerId': Titanic_data_test_pID['PassengerId'],\n                                'Survived': Titanic_data_test_pred })\nsubmission_svc.to_csv(\"submission_5_DT.csv\", index=False)","351b7001":"from sklearn.ensemble import BaggingClassifier\nBaggingClassifier = BaggingClassifier(max_features=8,max_samples=0.3)\nBaggingClassifier.fit(X_train_imputed_dummies, y_train)\ny_pred_train = BaggingClassifier.predict(X_train_imputed_dummies)\n\ny_pred = BaggingClassifier.predict(X_test_imputed_dummies)\nbagging_accy = round(accuracy_score(y_pred, y_test), 20)\nprint(bagging_accy)\nbagging_accy_train = round(accuracy_score(y_pred_train, y_train), 20)\nprint(bagging_accy_train)","a70d81b8":"Titanic_data_test_pred_Bg = BaggingClassifier.predict(Titanic_data_test_dummies)\n\nsubmission_svc = pd.DataFrame({ 'PassengerId': Titanic_data_test_pID['PassengerId'],\n                                'Survived': Titanic_data_test_pred })\nsubmission_svc.to_csv(\"submission_1_Bagging.csv\", index=False)","ff866647":"import numpy as np\nnp.random.seed(0)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\n\n\n# Create classifiers\nlr = LogisticRegression(solver='lbfgs')\ngnb = GaussianNB()\nsvc = LinearSVC(C=1.0)\nrfc = RandomForestClassifier(n_estimators=100)\n\n\n\nplt.figure(figsize=(10, 10))\nax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\nfor clf, name in [(lr, 'Logistic'),\n                  (gnb, 'Naive Bayes'),\n                  (svc, 'Support Vector Classification'),\n                  (rfc, 'Random Forest')]:\n    clf.fit(X_train_imputed_dummies, y_train)\n    if hasattr(clf, \"predict_proba\"):\n        prob_pos = clf.predict_proba(X_test_imputed_dummies)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(X_test_imputed_dummies)\n        prob_pos = \\\n            (prob_pos - prob_pos.min()) \/ (prob_pos.max() - prob_pos.min())\n    fraction_of_positives, mean_predicted_value = \\\n        calibration_curve(y_test, prob_pos, n_bins=10)\n\n    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n             label=\"%s\" % (name, ))\n\n    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n             histtype=\"step\", lw=2)\n\nax1.set_ylabel(\"Fraction of positives\")\nax1.set_ylim([-0.05, 1.05])\nax1.legend(loc=\"lower right\")\nax1.set_title('Calibration plots  (reliability curve)')\n\nax2.set_xlabel(\"Mean predicted value\")\nax2.set_ylabel(\"Count\")\nax2.legend(loc=\"upper center\", ncol=2)\n\nplt.tight_layout()\nplt.show()","01262655":"### Calling the Random Forest Classifier \nclassifier = RandomForestClassifier(n_estimators=10000,max_depth=7)\nclf = Pipeline(memory = '.\/',steps=[('classifier', classifier)])\n\nclf.fit(X=X_train_imputed_dummies, y=y_train)\n\ny_pred = clf.predict(X_train_imputed_dummies)\nprint(accuracy_score(y_train,y_pred))\n\n\ny_pred = clf.predict(X_test_imputed_dummies)\nprint(\"Test Accuracy = \",accuracy_score(y_test,y_pred))","282e3f87":"Titanic_data_test_pred = clf.predict(Titanic_data_test_dummies)\n\nsubmission_svc = pd.DataFrame({ 'PassengerId': Titanic_data_test_pID['PassengerId'],\n                                'Survived': Titanic_data_test_pred })\nsubmission_svc.to_csv(\"submission_3_RF.csv\", index=False)","f47e4d11":"XGB_model = XGBClassifier(n_estimators=10000, gamma=2,learning_rate=0.05,max_depth=8,subsample=0.8,reg_lambda= 2)\n%time XGB_model.fit(X_train_imputed_dummies, y_train)\n\ny_pred = XGB_model.predict(X_test_imputed_dummies)\nprint(accuracy_score(y_test,y_pred))\n\ny_pred_train = XGB_model.predict(X_train_imputed_dummies)\nprint(accuracy_score(y_train,y_pred_train))","09b7ddf8":"Titanic_data_test_pred_xg = XGB_model.predict(Titanic_data_test_dummies)\n\nsubmission_svc = pd.DataFrame({ 'PassengerId': Titanic_data_test_pID['PassengerId'],\n                                'Survived': Titanic_data_test_pred_xg })\nsubmission_svc.to_csv(\"submission_4_XG.csv\", index=False)","f35d20b7":"encoding_dim = 9 \nactual_dim = X_train_imputed_dummies.shape[1]\n\ninput_img = Input(shape = (actual_dim,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim,activation='relu')(input_img)\n\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(actual_dim,activation='sigmoid')(encoded)\n\nautoencoder = Model(input_img,decoded)\nprint(autoencoder.summary())","61cdc828":"autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nautoencoder.fit(X_train_imputed_dummies, X_train_imputed_dummies, epochs=100, batch_size=80)\n\nencoder = Model(input_img,encoded)\nprint(encoder.summary())","068561f2":"X_train_nonLinear_features = encoder.predict(X_train_imputed_dummies)\nX_test_nonLinear_features = encoder.predict(X_test_imputed_dummies)\n\ntitanic_nonLinear_features = encoder.predict(Titanic_data_test_dummies)\n\nX_train1=np.concatenate((X_train_imputed_dummies, X_train_nonLinear_features), axis=1)\nX_test1=np.concatenate((X_test_imputed_dummies, X_test_nonLinear_features), axis=1)\n\nTitatanic_data1=np.concatenate((Titanic_data_test_dummies, titanic_nonLinear_features), axis=1)","60ff7ffb":"clf = SVC()\nclf.fit(X_train1,y_train)\n\nprint(\"Train Accuracy on Support Vector MAchine is -----\", clf.score(X_train1,y_train))\nprint(\"Train Accuracy on Support Vector MAchine is -----\",clf.score(X_test1,y_test))","e47e6072":"model = Sequential()\nmodel.add(Dense(32, input_dim=X_train1.shape[1], activation='relu'))\nmodel.add(Dense(32, input_dim=X_train1.shape[1], activation='relu'))\nmodel.add(Dense(16, input_dim=X_train1.shape[1], activation='relu'))\nmodel.add(Dense(16, input_dim=X_train1.shape[1], activation='relu'))\nmodel.add(Dense(4, input_dim=X_train1.shape[1], activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","f0991760":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","2e67253b":"model.fit(X_train1, y_train, epochs=80, verbose=1,batch_size=90, validation_data=(X_test1, y_test))","1a4801f6":"model.evaluate(X_test1, y_test, )","29c6b3bf":"import matplotlib.pyplot as plt\n\nhistory = model.fit(X_train1, y_train, validation_split=0.25, epochs=40, batch_size=80, verbose=1)\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","1f7d398a":"### Drop the columns of map the unforeseen levels to the existing levels as the model is not learnt on the unseen level it is of not much use for the predection","9f093389":"## MLP -Multi Layer Perceptron","8266316e":"##### We can observe mutiple cabins are booked against the each ticket and the fare is a cumulative for all the cabins \n##### We are trying to bring the price of the pclass by dividing the fare with the number of cabins reflected against here ","0a99cf0d":"#### As the Ensemble has proven that if the model is high biased and the accuracy of expected is less . Boosting technique would help in bringing out the desired accuracy \n\n# XG-Boost boosted train and validation to 90 and 85 ","898e4cc1":"#### Plotting the calibration plots to observe with the data how our classifiers are clabirating the probabilities of the outputs\n\n###### WE can observe more calibration is happening to the 1 from .8 to 1 ","056465fb":"### Build and Auto encoder and learn the data to itself and do a MLP on top of that and see how the accuracy is built ","f5fb0b5b":"#### Observe the auto endoded features , whether yielding accuracy in SVM ","94c1f015":"#### Perform the dummification of the categorical variables ","f0246d1f":"### Building the Bagging Classifier as the variance is not that high in the above results but we can see ","fe061363":"### Dropping the Passenger ID as the Id is unique for every one so there is no mathematical derivations we can bring if each passenger has unique ids . \n##### We are dropping Name as the name is also a unique for each passenger . Yes experimentally we can extend the model to make a further dimensions with the classification of Mr,Mrs , Miss . Importantly we can bring a category of captain or Marshall if present in the passengers ","31a94376":"### Writing down to the file to upload and see how it is working on the future data","53088daf":"### Building a Decision Tree to understand which are the features are more influential and helping to predict who are survived and on what criteria .","8a0304ff":"#### As the small trees and more number of trees are always better than a signle tree and huge depth ","aaf42697":"#### Plotting the various observations using the countplot and catplot to observe the behaviour and pattern of the columns against the survived column \n\n-> Interestingly the Male Sex are survived more compared to Female. Need to identify the percentage of Male and Female travel ratio too.        \n-> Interestingly the passengers who are survived are travelling alone as we can see sibsp and Parch are 0 \n","a3a10059":"### Do standardization of the numerical variables to bring them under the standard normal distribution ","252c7467":"### Convert the features to Non Linear ","7a9a3895":"##### Fill the Na's with the mean of the distributions as the distribution is much right tailed\/skewed it is better to go with the median instead of the mean ","afbac992":"### Visually we can do a study on how the model is converging between test and train for each epoch","76872e5e":"### Titnic Data \nThis Dataset is an world famous dataset and we have started working on the dataset . ","7f2ff319":"#### Do the Test train split to 70-30 with the stratify as y to get the equal distribution of the survival and non survial data for the model to see","def484a4":"#### So Far we have made a base model and observed that the significance of few features which are influential to the model . As we have observed above in the DT that the Male influential factors are more in the survival of the serious disaster of Titanic "}}