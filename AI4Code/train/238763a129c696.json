{"cell_type":{"f7de30fd":"code","118f178d":"code","970ccc0c":"code","70affa7e":"code","3e6867be":"code","5686ac67":"code","b3b14025":"code","6e57bdf9":"code","09570c7c":"code","81ee604c":"code","ed55b554":"code","9d310e24":"code","df20a04f":"code","7030c229":"code","083a5eee":"code","3cfec5f4":"code","493cfaa8":"code","8edd0787":"code","1838ad3c":"code","0f6cf086":"code","d8f5d107":"code","bfb30dba":"code","0c278e87":"code","2aca3668":"code","2a7f1b0c":"code","ccddc061":"code","8a728351":"code","b6611120":"code","233af955":"code","5d6787b7":"code","19949f92":"code","3a92066f":"code","fe35e377":"code","42983d93":"code","261c75f0":"code","6738cb14":"markdown","9a75a294":"markdown","c062fb91":"markdown","01ec9e9b":"markdown","940b9590":"markdown","444d5afc":"markdown","164d694d":"markdown","eed5b6d9":"markdown","1a041e90":"markdown","cd0a18bf":"markdown","868803c4":"markdown","49b39613":"markdown","78ee87b7":"markdown","f95d8aa1":"markdown","86eb6360":"markdown","7c7ab75b":"markdown","d46e35bb":"markdown","e713790c":"markdown","0a7d5d52":"markdown","aef3e94b":"markdown","ae5e5fcb":"markdown","001ae30d":"markdown"},"source":{"f7de30fd":"!ls ..\/input","118f178d":"input_dir = '..\/input\/'\n# workig_dir = '..\/working\/'\n# output_dir = '..\/output\/'","970ccc0c":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","70affa7e":"train = pd.read_csv(os.path.join(input_dir, 'train.csv'))\ntrain.shape","3e6867be":"train.head()","5686ac67":"# Split train data\ntrain_Id = train['Id'] # individual ID\ntrain_idhogar = train['idhogar'] # household ID\ntrain_y = train['Target'] # Target value\ntrain_X = train.drop(['Id', 'Target', 'idhogar'], axis=1) # features","b3b14025":"print('We predict these levels')\nprint(train_y.unique())","6e57bdf9":"print('Num of rows : {}'.format(train.shape[0]))\nprint('unique ID : {}'.format(train_Id.unique().size))\nprint('unique idhogar : {}'.format(train_idhogar.unique().size))","09570c7c":"train_y_v_c = pd.concat([train_y.value_counts(), (train_y.value_counts() \/ train_y.shape)], axis=1)\ntrain_y_v_c.columns = ['value_counts', 'frac']\n\nprint('Distribution of individual poverty level')\nprint()\nprint(train_y_v_c)\n\nprint('------')\nprint()\nprint('Distribution of household poverty level')\ntrain_y_hh = train['Target'][train['parentesco1']==1]\ntrain_y_hh_c_v = pd.concat([train_y_hh.value_counts(), (train_y_hh.value_counts() \/ train_y_hh.shape)], axis=1)\ntrain_y_hh_c_v.columns = ['value_counts', 'frac']\nprint(train_y_hh_c_v)\nprint('This competition evaluates records with parentesco1==1')","81ee604c":"print('**Number of unique idhogar is different from number of parentesco1==1 records**')\nprint('unique idhogar : {}'.format(train_idhogar.unique().shape[0]))\nprint('sum of parentesco1 : {}'.format(train.query('parentesco1==1').shape[0]))\nprint('I assume we have to remove records whose households does not contain head of household in train data')","ed55b554":"print('Some households have inconsistent poverty levels in a household.')\ntrain_hh = train[['idhogar', 'Target']].drop_duplicates()\ntrain_hh_c_v = train_hh['idhogar'].value_counts()\nprint(train_hh_c_v[:5])\nprint()\nprint('Number of household with inconsistent Taget values : {}'.format(train_hh_c_v[train_hh_c_v>1].size))","9d310e24":"# check the records with idhogar=='5c6f32bbc'\ntrain[['idhogar', 'Id', 'Target']][train['idhogar']=='5c6f32bbc']","df20a04f":"print(train_X.dtypes.value_counts())\nprint()\nprint('3 columns are \"object\" type')","7030c229":"train_X.dtypes[train_X.dtypes=='object']","083a5eee":"print('dependency unique valuse')\nprint(train_X['dependency'].unique())\nprint()\nprint('edjefa unique valuse')\nprint(train_X['edjefa'].unique())\nprint()\nprint('edjefe unique valuse')\nprint(train_X['edjefe'].unique())","3cfec5f4":"print('5 colmuns have NaN')\nprint()\nis_null_train_X = train_X.isnull().any()\nprint('Number of NaN values')\nprint(train_X.isnull().loc[:,is_null_train_X==True].sum())","493cfaa8":"print('v2a1 means Monthly rent payment. \\n NaN seems to be unknown.')","8edd0787":"print('v18q means number of tablets household owns. \\n NaN has to be replaced as 0.')","1838ad3c":"print('rez_esc means Years behind in school. \\n I dont know how to replace them...')","0f6cf086":"train_X.head(10)","d8f5d107":"train_hh_hgtotal = train[train['parentesco1']==1][['idhogar', 'hogar_total']]\ntrain_hh_hgtotal.index = train_hh_hgtotal['idhogar']\ntrain_hh_hgtotal = train_hh_hgtotal.drop('idhogar', axis=1).sort_index()","bfb30dba":"train_hh_cnt = train.groupby('idhogar')['idhogar'].count().sort_index()","0c278e87":"train_hh_check = pd.concat([train_hh_hgtotal, train_hh_cnt], axis=1)\n(train_hh_check['idhogar'] != train_hh_check['hogar_total']).sum()","2aca3668":"train.index = train['Id'].values","2a7f1b0c":"area_list = ['area1', 'area2']\ntrain_areas = train[area_list]\ntrain_area = train_areas.idxmax(1)\ntrain_area.name = 'area'\n\nregion_list = ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']\ntrain_regions = train[region_list]\ntrain_region = train_regions.idxmax(1)\ntrain_region.name = 'region'\n\ntrain_geo = pd.concat([train_area, train_region], axis=1)","ccddc061":"train_geo.query('area==\"area1\"').drop_duplicates()","8a728351":"train_geo.query('area==\"area2\"').drop_duplicates()","b6611120":"instlevel_list = ['instlevel1', 'instlevel2', 'instlevel3',\n                  'instlevel4', 'instlevel5', 'instlevel6',\n                  'instlevel7', 'instlevel8', 'instlevel9']\ntrain_instlevels = train[instlevel_list]","233af955":"(train_instlevels.sum(axis=1) != 1).sum()","5d6787b7":"train_hh = train[train['parentesco1']==1]\n\ntipovivi_list = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5']\ntrain_tipovivis = train_hh[tipovivi_list]\ntrain_tipovivi = train_tipovivis.idxmax(axis=1)\ntrain_tipovivi.name = 'tipovivi'\n\ntrain_rent = pd.concat([train_hh['v2a1'], train_tipovivi], axis=1)\ntrain_rent.head()","19949f92":"train_rent[train_rent['v2a1'].isnull()]['tipovivi'].drop_duplicates()","3a92066f":"train_rent[train_rent['v2a1'].notnull()]['tipovivi'].drop_duplicates()","fe35e377":"train_rent['tipovivi'].value_counts()","42983d93":"fig, ax = plt.subplots(1, 2, sharey=True)\nax[0].boxplot(train_rent[train_rent['tipovivi']=='tipovivi2']['v2a1'].values)\nax[0].set_title('tipovivi2: paying in installments')\nax[1].boxplot(train_rent[train_rent['tipovivi']=='tipovivi3']['v2a1'].values)\nax[1].set_title('tipovivi3: rented')","261c75f0":"fig, ax = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\nax[0].hist(train_rent[train_rent['tipovivi']=='tipovivi2']['v2a1'].values, bins=25, orientation=\"horizontal\")\nax[0].set_title('tipovivi2: paying in installments')\nax[1].hist(train_rent[train_rent['tipovivi']=='tipovivi3']['v2a1'].values, bins=25, orientation=\"horizontal\")\nax[1].set_title('tipovivi3: rented')","6738cb14":"## Which tipovivi do pay rent?\ntipovivi means house rent type such as own, pay rent and pay loan. v2a1 is monthly rent. I will check which tipovivi pay monthly rent.","9a75a294":"### Unique IDs","c062fb91":"## Geografical features\ncheck this [hypothesis](https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61761)","01ec9e9b":"## IDs","940b9590":"Regions which have area1(urban)","444d5afc":"### Distribution by Target value","164d694d":" ### Data Type","eed5b6d9":"I cannot think of a reasonable reason why some household have inconsistent Taget values. As a possibility, \n* Just input misses or data corruption\n* Poverty level is socred by each individual not each household\n\nIf this inconsistency is just errored input, we can replace the values to fix each household poverty level.\n\nIf each individual has each poverty level, prediction strategy needs to be more complicated.\n\n~~Anyway we need to know how the poverty level was scored. So I asked this topic in [Discussion thread](https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403).\n\nAccording to the [reply](https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403), we cannot know how they scored. We shuold not try to use non-heads of household Target values.\n","1a041e90":"### Not fixed Traget value in one household","cd0a18bf":"### NaN in features","868803c4":"## Traget Values","49b39613":"## check instlevelX\n* Are they exclusive?\n**Exclusive**","78ee87b7":"### Number of unnique household VS Sum of heads of household","f95d8aa1":"Left end column contains ID, and right end column has Traget values.\n'idhogar' column identifies household ID.\nNow, I define the other columns as features.\n\nFirst impression, this dataset has many numeric values and we can see some NaN. I will check the data type later.","86eb6360":"#### Convert one-hot variables into categorical variables","7c7ab75b":"## Check: all members of household are in dataset or not","d46e35bb":"I assume 'no' means zero value and 'yes' means non-zero and not confirmed value. We need to replace 'yes' with a new value.","e713790c":"**This Kernel is developping...**\n","0a7d5d52":"# Check the consistency of Train Data","aef3e94b":"ID is  not duplicated, but \"idhogar\" which identifies each household is duplicated.\nWe have to predict poverty level of each household in this competition, not individual level.","ae5e5fcb":"Regions which have area2(rural)","001ae30d":"## Feature Colmuns"}}