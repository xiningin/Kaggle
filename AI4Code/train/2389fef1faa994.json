{"cell_type":{"8b96ad8d":"code","43bb5310":"code","f4d303ba":"code","f6ecc43f":"code","968b9580":"code","c4278404":"code","9fc08baa":"code","a061a5e0":"code","7762d1c9":"code","70052986":"code","5e562c16":"code","0f4e8549":"code","e828eb9a":"code","47cf12e4":"code","b766b998":"code","e25e6ce1":"code","0b42ff08":"code","9bac18ff":"code","2c3831be":"code","4d44e9ee":"code","e6969f51":"code","bc6950ef":"code","d8c8ccfe":"code","d9d5195f":"code","8926d9ab":"code","70b8fcc5":"code","4dc85fb7":"code","4e23ecb0":"code","bfca7a66":"code","37d57544":"code","11223dc2":"code","01b7c547":"code","0a1d58af":"code","f50f3136":"code","1c8e5713":"code","becb10df":"code","53da1afe":"code","f327748e":"code","107e6015":"code","3889e973":"code","93440224":"code","1cc2c31d":"code","394715ca":"code","78d4d293":"code","9c854319":"code","81eebee0":"code","20d7c513":"code","7354c221":"code","232be653":"code","4c06f66d":"code","6e1e0a72":"code","f9f6b061":"code","fb7fb385":"code","20bc0263":"code","6c0f7c20":"code","210d6ed5":"code","fcb21334":"code","844a3784":"code","aad2fae2":"code","62ed96a2":"code","e6a6d37a":"code","955bd986":"code","c9a9dfe6":"code","26750c94":"code","90fc14d5":"code","b9b44adf":"code","4ac0909d":"code","64cf8061":"code","45337285":"code","8b2deac0":"code","b1a761ed":"code","a378d982":"code","e9304047":"code","99f3e002":"code","3614e21c":"code","cda5a980":"code","b0c9fba9":"code","18257668":"code","c963f457":"code","cc3edf35":"code","2a2b309f":"code","8fad617d":"code","12219825":"code","dc322896":"code","ab7b4b33":"code","01d1c1d7":"code","8d83a8cb":"code","fbc29da7":"code","0f908d96":"code","07af110a":"code","b5e0b514":"code","9ab73d7b":"code","f39ed1c1":"code","dfb35c60":"code","989b0344":"code","b61bc0cf":"code","b1f1c038":"code","8f326fe4":"code","6656bc29":"code","74b4781c":"code","e350f550":"code","2fcae75b":"code","7fe08dd0":"code","d87e2032":"code","c8ae6c93":"code","ddc1e36f":"code","d9638983":"code","6711e854":"code","8cfb5eb1":"code","a49e3fdd":"code","5cbd53ed":"code","9c2100d4":"code","de25c399":"code","cb200e7a":"code","04181ef7":"code","6cb74bc7":"code","4cf36ec2":"code","de8696e1":"code","a7c3be5b":"code","2d85dba2":"code","911e498a":"code","43024674":"code","996bb4d9":"code","2ca45ce4":"code","e7861868":"code","5b91bb19":"code","b46e692a":"code","227324aa":"code","a6e03e59":"code","ae5a368a":"code","117e2bb1":"code","06a2d85e":"code","5dcb08fe":"code","0b0f8c2b":"code","402e8dbb":"code","18f98a0e":"code","feb305b3":"code","9708dd7d":"code","7e8202b9":"code","4940b632":"code","5112d306":"code","14864997":"code","ea650dfd":"code","68b44b3c":"code","79b1a2a8":"code","434b4dc0":"code","c7ae02e6":"code","e02deaad":"code","88df8760":"code","3dfe0858":"code","9b829ebb":"code","f89139dc":"code","061f2f8e":"code","fc0321b4":"code","2bad3d1b":"code","7e06bb4d":"code","7f589cec":"code","4f01cf9e":"code","87656712":"code","492bfdb9":"code","feda3773":"code","3801f6c9":"code","239fe953":"code","686377a7":"code","c7c4f2b1":"code","338c4211":"code","347d819e":"code","79ae7046":"code","4d26b6b8":"code","e673a529":"code","96c42051":"code","1ca750fd":"code","6915ceb4":"code","31a6fd4f":"code","39cabe41":"code","7fdd1c31":"code","24665081":"code","6deba3be":"code","bbb512a5":"code","4dd41213":"code","0dece2c5":"code","8be992dc":"code","623f089c":"code","047a935f":"code","450a1be1":"code","239181ca":"code","f953c794":"code","2b54dab4":"code","f114a548":"code","9e56769a":"code","ad44765b":"code","e5f0e212":"code","0e8a07bb":"code","f8debce7":"code","73e6e84b":"code","1e28c7f1":"code","4ca099de":"code","8df7bf6b":"code","94d9004b":"code","584ef390":"code","a5293351":"code","2899955d":"code","e2b4da5a":"code","cd8208c1":"code","0082c725":"code","fad1dbba":"code","063653ff":"code","8bd84c7e":"code","d38c7853":"code","c28c9471":"code","8781e908":"code","43baf25b":"code","5df22c47":"code","d16feb5d":"code","af624ec8":"code","9aa8e9ba":"code","4386a714":"code","a3f4ce07":"code","9ef90052":"code","fec94552":"code","5c6ffe5c":"code","a14baf0a":"markdown","927a99d6":"markdown","c1dce7d2":"markdown","5f88a565":"markdown","7b20a314":"markdown","fae9cb16":"markdown","b1d4bdcd":"markdown","453587ba":"markdown","b6e80c33":"markdown","1fc4bdd0":"markdown","ba1c0602":"markdown","547697bb":"markdown","d09850b6":"markdown","4b8b79b9":"markdown","04437599":"markdown","6cf4019a":"markdown","37b4c3e5":"markdown","3baffcce":"markdown","141c966a":"markdown","35881c22":"markdown","c10f5ce1":"markdown","c1d0363c":"markdown","60b01568":"markdown","d79493f6":"markdown","c5a75334":"markdown","fbb16c86":"markdown","2354d27c":"markdown","dceca709":"markdown","952953b3":"markdown","efd1a427":"markdown","78d62ff6":"markdown","a4a77325":"markdown","5653c6ec":"markdown","34e098ce":"markdown","9d348a4c":"markdown","341dc63d":"markdown","5df08280":"markdown","1646e53e":"markdown","0dd41efd":"markdown","d26de4a3":"markdown","15ec1de2":"markdown","944097a0":"markdown","f37fa652":"markdown","c2b3ad9e":"markdown","f7056d42":"markdown","88e40451":"markdown","a3c9bec0":"markdown","781dba99":"markdown","c9961306":"markdown","3ebc86dc":"markdown","cba7ea5e":"markdown","de3c35d2":"markdown","f165bdac":"markdown","83f3bc17":"markdown","1f18bb8d":"markdown","c269d93d":"markdown","1578a9c8":"markdown","2fc8c9bc":"markdown","bb941d59":"markdown","2be217d5":"markdown","b2429f97":"markdown","8937a689":"markdown","eb613df6":"markdown","866d3f53":"markdown","efb7666e":"markdown","f0092ff0":"markdown","0f4820ab":"markdown","7c0f738a":"markdown","99734afa":"markdown","9b4c97c9":"markdown","6e7ec4e2":"markdown","43ba79f4":"markdown","98cb3b82":"markdown","9c6f0506":"markdown","9a9d935a":"markdown","b68dde8e":"markdown","8799e80a":"markdown","862eb28d":"markdown","b998406a":"markdown","7e53770b":"markdown","21c7e07f":"markdown","bbc799f2":"markdown","31d7a9b7":"markdown","0887bec0":"markdown","120093ea":"markdown","9f6d5eff":"markdown","b9040802":"markdown","d64a9b92":"markdown","9746044c":"markdown","048c4c20":"markdown","4c300f74":"markdown","0e37377f":"markdown","fe48beeb":"markdown","1a6bfabb":"markdown","1b0843cd":"markdown","b2f5c430":"markdown","df48e02c":"markdown","8229abc1":"markdown","fb5af8d9":"markdown","2b398f07":"markdown","75f88749":"markdown","d6907fa3":"markdown","65d50885":"markdown","7b89d06a":"markdown","ab5ea86b":"markdown","0f01f4ba":"markdown","a566d842":"markdown","9064c8fa":"markdown","27fe5693":"markdown","a91b6975":"markdown","3fd84d8c":"markdown","cd044e69":"markdown","6eb41981":"markdown","0c0047e3":"markdown","e1050bf8":"markdown","e4b52d39":"markdown","e01c96e9":"markdown","19d673b7":"markdown","d0bd84cd":"markdown","6c7495c8":"markdown","1ec67bf6":"markdown","25a0ac89":"markdown","01282ce8":"markdown","76a8efd1":"markdown","833734b2":"markdown","a8c1bd1e":"markdown","7877d3f9":"markdown","0e0233f9":"markdown","3f8139a2":"markdown","65ece20f":"markdown","0955dc2f":"markdown","a6c04adb":"markdown","4056f9cc":"markdown","299ab20b":"markdown","c215cf9f":"markdown","dd774fe6":"markdown","c88d5fe0":"markdown","650421f1":"markdown","6e52d9ed":"markdown","bbd1f159":"markdown","d6622043":"markdown","da7f3270":"markdown","cf9bd0a5":"markdown","e0b77b0c":"markdown","32be0cef":"markdown","4e532bef":"markdown","5ac699cb":"markdown","46011f0e":"markdown","45ad4a6a":"markdown","6149e76e":"markdown","b91168b1":"markdown","baa0646b":"markdown","7696c117":"markdown","d1b0d472":"markdown","533992f8":"markdown","02694f8a":"markdown"},"source":{"8b96ad8d":"from IPython.display import Image\nImage(\"..\/input\/images\/flatten-the-curve.png\")","43bb5310":"### Load packages\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport numpy\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\nfrom sklearn import svm\n%matplotlib inline\n\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.set_palette(\"husl\")\n\n\n#Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n#metrics\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, make_scorer\n\n#feature selection\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn import model_selection\n\n#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestRegressor\n\n#Regression\nfrom sklearn.linear_model import LogisticRegression, BayesianRidge, LinearRegression\nfrom sklearn import metrics\nimport statsmodels.api as sm\n\n#logistic curve\nfrom scipy.optimize import curve_fit\n\n#LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\n#seed\nimport random\n\n# Normalizing continuous variables\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n# Visualization\n\n## Bokeh\nfrom bokeh.plotting import output_notebook, figure, show\nfrom bokeh.models import ColumnDataSource, Div, Select, Button, ColorBar, CustomJS\nfrom bokeh.layouts import row, column, layout\nfrom bokeh.transform import cumsum, linear_cmap\nfrom bokeh.palettes import Blues8, Spectral3\nfrom bokeh.plotting import figure, output_file, show\n\n## Plotly\nfrom plotly.offline import iplot\nfrom plotly import tools\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\n\nfrom matplotlib import dates\nimport plotly.graph_objects as go\n\n# Time series\nfrom fbprophet import Prophet\nimport datetime\nfrom datetime import datetime\n\n# Google BigQuery\nfrom google.cloud import bigquery\n\n#import cdist\nfrom scipy.spatial.distance import cdist\n\n# to solve SEIR\nfrom scipy.integrate import solve_ivp\n\n#others\nfrom pathlib import Path\nimport os\nfrom tqdm.notebook import tqdm\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n","f4d303ba":"### Load in the data from Kaggle Week 2\n\ntrain = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\",parse_dates=['Date'])\n                    \ntrain.tail()","f6ecc43f":"train.info()","968b9580":"# Test dataset from Kaggle\n\ntest = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\",parse_dates=['Date'])\n                    \ntest.tail()\n","c4278404":"submit = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/submission.csv')\n                     \nsubmit.head()","9fc08baa":"#read the complete data set\n\ncomplete_data = pd.read_csv('..\/input\/corona-virus-report\/covid_19_clean_complete.csv', parse_dates=['Date'])\n\ncomplete_data = complete_data.rename(columns = {'Province\/State': 'Province_State', 'Country\/Region': 'Country_Region'})\n\ncomplete_data.sort_values(by=['Date','Confirmed'], ascending=False).head()","a061a5e0":"complete_data.info()","7762d1c9":"#read the demographic data\n\ndemo_data = pd.read_csv('..\/input\/countryinfo\/covid19countryinfo.csv')\n\ndemo_data['pop']=demo_data['pop'].str.replace(',', '').astype('float')\n\ndemo_data['healthexp']=demo_data['healthexp'].str.replace(',', '').astype('float')\n\n\ndemo_data.head()","70052986":"pop_info = pd.read_csv('..\/input\/covid19-population-data\/population_data.csv')\n\npop_info.head()","5e562c16":"# Weather data\nweather_data = pd.read_csv(\"..\/input\/weather-features\/training_data_with_weather_info_week_2.csv\", parse_dates=['Date'])\nweather_test = pd.read_csv(\"..\/input\/weather-features\/testing_data_with_weather_info_week_2.csv\", parse_dates=['Date'])\n","0f4e8549":"\nweather_data.head()\n","e828eb9a":"## From @winterpierre source\n\nweather_addition = pd.read_csv(\"..\/input\/covid19-global-weather-data\/temperature_dataframe.csv\", parse_dates=['date'])\n\n#rename column\nweather_addition.columns = ['Unnamed: 0', 'Id', 'Province_State', 'Country_Region', 'lat', 'long', 'Date',\n       'ConfirmedCases', 'Fatalities', 'capital', 'humidity', 'sunHour', 'tempC',\n       'windspeedKmph']\n\n#fix the name US for consistency\nweather_addition = weather_addition.replace('USA','US')\n\nweather_addition.head()","47cf12e4":"# case \ncase = ['Confirmed', 'Deaths', 'Recovered', 'Active']\n\n# Formula: Active Case = Confirmed - Deaths - Recovered\ncomplete_data['Active'] = complete_data['Confirmed'] - complete_data['Deaths'] - complete_data['Recovered']\n\n# impute missing values \ncomplete_data[['Province_State']] = complete_data[['Province_State']].fillna('')\ncomplete_data[case] = complete_data[case].fillna(0)\n\ncomplete_data.sort_values(by=['Date','Confirmed'], ascending=False).head()","b766b998":"complete_data.sort_values(by=['Date'], ascending=False).tail()","e25e6ce1":"map_covid = train.groupby(['Date', 'Country_Region'])['ConfirmedCases'].sum().reset_index()\nmap_covid['Date'] = map_covid['Date'].dt.strftime('%m\/%d\/%Y')\nmap_covid['size'] = map_covid['ConfirmedCases'].pow(0.3) * 3.5\n\nfig = px.scatter_geo(map_covid, locations=\"Country_Region\", locationmode='country names', \n                     color=\"ConfirmedCases\", size='size', hover_name=\"Country_Region\", \n                     range_color=[1,100],\n                     projection=\"natural earth\", animation_frame=\"Date\", \n                     title='COVID-19: Confirmed Cases Around the Globe', color_continuous_scale=\"tealrose\")\nfig.show()\n","0b42ff08":"regions =pd.DataFrame()\n\nregions['Country'] = map_covid[\"Country_Region\"]\nregions['Confirmed Cases'] = map_covid[\"ConfirmedCases\"]\n\nfig = px.choropleth(regions, locations='Country',\n                    locationmode='country names',\n                    color=\"Confirmed Cases\",color_continuous_scale=\"tealrose\")\n\nfig.update_layout(title=\"COVID19 Confirmed Cases on 04-01-2020\")\n\nfig.show()\n","9bac18ff":"# sum of all Confirmed cases by country as of March 26\nsum_confirm = pd.DataFrame(complete_data.loc[complete_data['Date']==complete_data['Date'].max()].groupby(\n    ['Country_Region'])['Confirmed'].sum()).reset_index()\n\n# sum of all Death cases by country as of March 26\nsum_death = pd.DataFrame(complete_data.loc[complete_data['Date']==complete_data['Date'].max()].groupby(\n    ['Country_Region'])['Deaths'].sum()).reset_index()\n\n# sum of all Recovered cases by country as of March 26\nsum_recover = pd.DataFrame(complete_data.loc[complete_data['Date']==complete_data['Date'].max()].groupby(\n    ['Country_Region'])['Recovered'].sum()).reset_index()\n\n# sum of all Active cases by country as of March 26\nsum_active = pd.DataFrame(complete_data.loc[complete_data['Date']==complete_data['Date'].max()].groupby(\n    ['Country_Region'])['Active'].sum()).reset_index()\n","2c3831be":"sns.set(rc={'figure.figsize':(15, 7)})\n\ntop20_confirm = sum_confirm.sort_values(by=['Confirmed'], ascending=False).head(20)\n\nplot1 = sns.barplot(x=\"Confirmed\",y=\"Country_Region\", data=top20_confirm)\n\nplt.title(\"Total Numbers of Confirmed Cases\",fontsize=20)\n\nfor p in plot1.patches:\n    width = p.get_width()\n    plot1.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")","4d44e9ee":"top20_confirm['Country_Region'].unique()","e6969f51":"\ntop20_death = sum_death.sort_values(by=['Deaths'], ascending=False).head(20)\n\nplot2 = sns.barplot(x=\"Deaths\",y=\"Country_Region\", data=top20_death)\n\nplt.title(\"Total Numbers of Fatal Cases\",fontsize=20)\n\nfor p in plot2.patches:\n    width = p.get_width()\n    plot2.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")","bc6950ef":"top20_recover = sum_recover.sort_values(by=['Recovered'], ascending=False).head(20)\n\nplot3 = sns.barplot(x=\"Recovered\",y=\"Country_Region\", data=top20_recover)\n\nplt.title(\"Total Numbers of Recovered Cases\",fontsize=20)\n\nfor p in plot3.patches:\n    width = p.get_width()\n    plot3.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")","d8c8ccfe":"top20_active = sum_active.sort_values(by=['Active'], ascending=False).head(20)\n\nplot4 = sns.barplot(x=\"Active\",y=\"Country_Region\", data=top20_active)\n\nplt.title(\"Total Numbers of Active Cases\",fontsize=20)\n\nfor p in plot4.patches:\n    width = p.get_width()\n    plot4.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")","d9d5195f":"### Compute day first outbreak for each country\ncomplete_data_first = train.copy()\n\ncountries_array = complete_data_first['Country_Region'].unique()\n\ncomplete_data_first_outbreak = pd.DataFrame()\n\nfor i in countries_array:\n    # get relevant data \n    day_first_outbreak = complete_data_first.loc[complete_data_first['Country_Region']==i]\n    \n    date_outbreak = day_first_outbreak.loc[day_first_outbreak['ConfirmedCases']>0]['Date'].min()\n    \n    #Calculate days since first outbreak happened\n    day_first_outbreak['days_since_first_outbreak'] = (day_first_outbreak['Date'] \n                                                       - date_outbreak).astype('timedelta64[D]')\n    \n    #impute the negative days with 0\n    day_first_outbreak['days_since_first_outbreak'][day_first_outbreak['days_since_first_outbreak']<0] = 0 \n   \n    complete_data_first_outbreak = complete_data_first_outbreak.append(day_first_outbreak,ignore_index=True)\n    \n","8926d9ab":"\ncomplete_data_first_outbreak.head()\n","70b8fcc5":"top20_confirm_first = complete_data_first_outbreak.loc[\n    complete_data_first_outbreak['Country_Region'].isin(\n        ['US', 'China', 'Italy', 'Spain', 'Germany', 'France', 'Iran',\n       'United Kingdom', 'Switzerland', 'Korea, South'])==True]\n\ntop20_confirm_first = top20_confirm_first.groupby(['Country_Region','days_since_first_outbreak'])['ConfirmedCases'].sum().reset_index()\n\ntop20_confirm_first['days_since_first_outbreak'] = pd.to_timedelta(\n    top20_confirm_first['days_since_first_outbreak'], unit='D')\n\nsns.lineplot(data=top20_confirm_first, x=\"days_since_first_outbreak\", y=\"ConfirmedCases\", hue=\"Country_Region\")\n\nplt.ylabel(\"Total cases in top 10 countries\")\n\nplt.xlabel(\"Number of days since first outbreak\")\n\n\nplt.title(\"Total numbers of cases since first outbreak in top 10 countries\",fontsize=20)\n\n","4dc85fb7":"\ntime_sum = complete_data.groupby('Date')['Date', 'Confirmed', 'Deaths'].sum().reset_index()\n\ntime_sum = pd.melt(time_sum, id_vars=['Date'], value_vars=['Confirmed','Deaths'])\n\ntime_sum = time_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nsns.lineplot(data=time_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nplt.ylabel(\"Total cases\")\n\nplt.title(\"Total numbers of Cases\",fontsize=20)\n","4e23ecb0":"sns.set(rc={'figure.figsize':(15, 7)})\n\ntime_log = complete_data.groupby('Date')['Date', 'Confirmed', 'Deaths'].sum().reset_index()\n\ntime_log[\"Confirmed\"] = np.log(time_log[\"Confirmed\"])\n\ntime_log[\"Deaths\"] = np.log(time_log[\"Deaths\"])\n\ntime_log = pd.melt(time_log, id_vars=['Date'], value_vars=['Confirmed','Deaths'])\n\ntime_log = time_log.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nsns.lineplot(data=time_log, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nplt.ylabel(\"Total Confirmed cases on log scale\")\n\nplt.title(\"Total numbers of Confirmed on log scale\",fontsize=20)\n","bfca7a66":"china_sum = complete_data.loc[complete_data['Country_Region']==\"China\"].groupby('Date')['Date', 'Confirmed','Deaths','Active','Recovered'].sum().reset_index()\n\nchina_sum = pd.melt(china_sum, id_vars=['Date'], value_vars=['Confirmed','Deaths','Active','Recovered'])\n\nchina_sum = china_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nsns.lineplot(data=china_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nplt.ylabel(\"Total cases in China\")\n\nplt.title(\"Total numbers of Cases in China\",fontsize=20)\n","37d57544":"italy_sum = complete_data.loc[complete_data['Country_Region']==\"Italy\"].groupby('Date')['Date', 'Confirmed','Deaths','Active','Recovered'].sum().reset_index()\n\nitaly_sum = pd.melt(italy_sum, id_vars=['Date'], value_vars=['Confirmed','Deaths','Active','Recovered'])\n\nitaly_sum = italy_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nsns.lineplot(data=italy_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nplt.ylabel(\"Total cases in Italy\")\n\nplt.title(\"Total numbers of Cases in Italy\",fontsize=20)\n","11223dc2":"us_sum = complete_data.loc[complete_data['Country_Region']==\"US\"].groupby('Date')['Date', 'Confirmed','Deaths','Active','Recovered'].sum().reset_index()\n\nus_sum = pd.melt(us_sum, id_vars=['Date'], value_vars=['Confirmed','Deaths','Active','Recovered'])\n\nus_sum = us_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nsns.lineplot(data=us_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nplt.ylabel(\"Total cases in the U.S.\")\n\nplt.title(\"Total numbers of Cases in the U.S.\",fontsize=20)\n","01b7c547":"#Other countries\n\nother_sum = complete_data.loc[complete_data['Country_Region'].isin([\"Italy\",\"China\",\"US\"])==False]\n\nother_sum = other_sum.groupby('Date')['Date', 'Confirmed','Deaths','Active','Recovered'].sum().reset_index()\n\nother_sum = pd.melt(other_sum, id_vars=['Date'], value_vars=['Confirmed','Deaths','Active','Recovered'])\n\nother_sum = other_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nsns.lineplot(data=other_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nplt.ylabel(\"Total cases in other countries\")\n\nplt.title(\"Total numbers of Cases in other countries\",fontsize=20)\n","0a1d58af":"# Countries that are in Europe\n\neurope = ['Austria','Italy','Belgium','Latvia','Bulgaria','Lithuania','Croatia','Luxembourg',\n          'Cyprus','Malta','Czechia','Netherlands','Denmark','Poland','Estonia','Portugal',\n          'Finland','Romania','France','Slovakia','Germany','Slovenia','Greece','Spain',\n          'Hungary','Sweden','Ireland','Switzerland','United Kingdom']\n\neurope_sum = complete_data.loc[complete_data['Country_Region'].isin(europe)==True]\n\neurope_sum.loc[europe_sum['Confirmed']>0].sort_values('Date').head(1)\n","f50f3136":"#Plot out the total cases by each country\neurope_sum = europe_sum.loc[complete_data['Date']==complete_data['Date'].max()].groupby(\n    'Country_Region')['Country_Region', 'Confirmed'].sum().reset_index().sort_values('Confirmed',ascending=False)\n\nplot5 = sns.barplot(x=\"Confirmed\",y=\"Country_Region\", data=europe_sum)\n\nplt.title(\"Total Numbers of Confirmed Cases\")\n\nfor p in plot1.patches:\n    width = p.get_width()\n    plot1.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")\n","1c8e5713":"top10_eu_sum = complete_data.loc[complete_data['Country_Region'].isin(['Italy','Spain','Germany',\n                                                                      'France','United Kingdom',\n                                                                      'Switzerland','Netherlands','Austria',\n                                                                      'Belgium','Portugal'])==True]\n\ntop10_eu_sum1 = top10_eu_sum.groupby(['Country_Region','Date'])['Confirmed'].sum().reset_index()\n\nsns.lineplot(data=top10_eu_sum1, x=\"Date\", y=\"Confirmed\", hue=\"Country_Region\")\n\nplt.ylabel(\"Total cases in Europe countries\")\n\nplt.title(\"Total numbers of Cases in Europe countries\",fontsize=20)\n","becb10df":"top10_eu_sum2 = top10_eu_sum.groupby(['Country_Region','Date'])['Confirmed','Deaths'].sum().reset_index()\n\ntop10_eu_sum2['Fatal_Rate'] = round((top10_eu_sum2['Deaths']\/top10_eu_sum2['Confirmed'])*100,2)\n\nsns.lineplot(data=top10_eu_sum2, x=\"Date\", y=\"Fatal_Rate\", hue=\"Country_Region\")\n\nplt.ylabel(\"Fatality Rate in Europe countries in Percentage\")\n\nplt.title(\"Fatality Rate  in Europe countries\",fontsize=20)","53da1afe":"top10_eu_sum3 = top10_eu_sum.groupby(['Country_Region','Date'])['Confirmed','Recovered'].sum().reset_index()\n\ntop10_eu_sum3['Recover_Rate'] = round((top10_eu_sum3['Recovered']\/top10_eu_sum3['Confirmed'])*100,2)\n\nsns.lineplot(data=top10_eu_sum3, x=\"Date\", y=\"Recover_Rate\", hue=\"Country_Region\")\n\nplt.ylabel(\"Recovery Rate in Europe countries in Percentage\")\n\nplt.title(\"Recovery Rate  in Europe countries\",fontsize=20)","f327748e":"north_america = ['Antigua and Barbuda','Bahamas','Barbados','Belize','Canada','Costa Rica','Cuba','El Salvador',\n                 'Grenada','Guatemala','Hait\u00ed','Honduras','Jamaica','Mexico','Nicaragua','Panama',\n                 'Saint Kitts and Nevis','Saint Lucia','Saint Vincent and the Grenadines','Trinidad and Tobago','US']\n\nna_region_sum = complete_data.loc[complete_data['Country_Region'].isin(north_america)==True]\n\nna_region_sum.loc[na_region_sum['Confirmed']>0].sort_values('Date').head(1)\n","107e6015":"# plot the total of US\nus_region_sum = train.loc[train['Country_Region'] == \"US\"]\n\nus_region_sum1 = us_region_sum.loc[us_region_sum['Date']==train['Date'].max()].groupby(\n    ['Province_State'])['ConfirmedCases'].sum().reset_index().sort_values('ConfirmedCases',ascending=False).head(20)\n\nplot6 = sns.barplot(x=\"ConfirmedCases\",y=\"Province_State\", data=us_region_sum1)\n\nplt.ylabel(\"Total confirmed cases by US states\")\n\nplt.title(\"Total Numbers of Confirmed Cases by U.S top 20 states\",fontsize=20)\n\nfor p in plot6.patches:\n    width = p.get_width()\n    plot6.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")\n    \n\n\n","3889e973":"top10_us_sum = us_region_sum.loc[us_region_sum['Province_State'].isin(['New York','New Jersey','Washington',\n                                                                      'California','Michigan',\n                                                                      'Illinois','Florida','Louisiana',\n                                                                      'Pennsylvania','Texas'])==True]\n\n\ntop10_us_sum1 = top10_us_sum.groupby(\n    ['Province_State','Date'])['ConfirmedCases'].sum().reset_index()\n    \nsns.lineplot(data=top10_us_sum1, x=\"Date\", y=\"ConfirmedCases\", hue=\"Province_State\")\n\nplt.ylabel(\"Total confirmed cases by US states\")\n\nplt.title(\"Total numbers of Confirmed Cases in by top 10 states\",fontsize=20)\n","93440224":"top10_us_sum2 = top10_us_sum.groupby(['Province_State','Date'])['ConfirmedCases','Fatalities'].sum().reset_index()\n\ntop10_us_sum2['Fatal_Rate'] = round((top10_us_sum2['Fatalities']\/top10_us_sum2['ConfirmedCases'])*100,2)\n\nsns.lineplot(data=top10_us_sum2, x=\"Date\", y=\"Fatal_Rate\", hue=\"Province_State\")\n\nplt.ylabel(\"Fatality Rate in the U.S. states in Percentage\")\n\nplt.title(\"Fatality Rate by top 10 states\",fontsize=20)","1cc2c31d":"top10_us_sum3 = complete_data.loc[complete_data['Country_Region']=='US'].groupby(\n    'Date')['Confirmed','Recovered'].sum().reset_index()\n\ntop10_us_sum3['Recover_Rate'] = round((top10_us_sum3['Recovered']\/top10_us_sum3['Confirmed'])*100,2)\n\nsns.lineplot(data=top10_us_sum3, x=\"Date\", y=\"Recover_Rate\")\n\nplt.ylabel(\"Recovery Rate in the U.S. states in Percentage\")\n\nplt.title(\"Recovery Rate by top 10 states\",fontsize=20)","394715ca":"round(top10_us_sum3['Recover_Rate'].mean(),2)","78d4d293":"\nasia = ['Afghanistan', 'Armenia', 'Azerbaijan', 'Bahrain', 'Bangladesh', 'Bhutan', 'Brunei', \n        'Cambodia', 'China', 'Timor-Leste', 'Georgia', 'India', 'Indonesia', 'Iran', 'Iraq', \n        'Israel', 'Japan', 'Jordan', 'Kazakhstan', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Lebanon', \n        'Malaysia', 'Maldives', 'Mongolia', 'Myanmar', 'Nepal', 'Oman', 'Pakistan', \n        'Philippines', 'Qatar', 'Russia', 'Saudi Arabia', 'Singapore', 'Korea, South', 'Sri Lanka', \n        'Syria', 'Taiwan', 'Tajikistan', 'Thailand', 'Turkey', 'Turkmenistan', 'United Arab Emirates', \n        'Uzbekistan', 'Vietnam', 'Yemen']\n\nasia_sum = train.loc[train['Country_Region'].isin(asia)==True]\n\nasia_sum1 = asia_sum.loc[asia_sum['Date']==train['Date'].max()].groupby(\n    'Country_Region')['Country_Region', 'ConfirmedCases'].sum().reset_index().sort_values(\n    'ConfirmedCases',ascending=False).head(20)\n\nplot7 = sns.barplot(x=\"ConfirmedCases\",y=\"Country_Region\", data=asia_sum1)\n\nplt.title(\"Total Numbers of Confirmed Cases\",fontsize=20)\n\nfor p in plot7.patches:\n    width = p.get_width()\n    plot7.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")\n\n\n","9c854319":"#Let's plot them in a timeline but excluding China\n\ntop10_asia_sum1 = asia_sum.loc[asia_sum['Country_Region'].isin(['Korea, South','Iran','Turkey','Israel',\n                                                               'Malaysia','Japan','Pakistan',\n                                                               'Thailand','Saudi Arabia','Indonesia',\n                                                              'Russia','India'])==True]\n\n\ntop10_asia_sum1 = top10_asia_sum1.groupby(\n    ['Country_Region','Date'])['ConfirmedCases'].sum().reset_index()\n    \nsns.lineplot(data=top10_asia_sum1, x=\"Date\", y=\"ConfirmedCases\", hue=\"Country_Region\")\n\nplt.ylabel(\"Total confirmed cases by Asia countries\")\n\nplt.title(\"Total numbers of Confirmed Cases in by Asia countries - excluding China\",fontsize=20)\n","81eebee0":"#excluding Iran, Korea and China\n\ntop10_asia_sum2 = asia_sum.loc[asia_sum['Country_Region'].isin(['Turkey','Israel',\n                                                               'Malaysia','Japan','Pakistan',\n                                                               'Thailand','Saudi Arabia','Indonesia',\n                                                              'Russia','India','Philippines'])==True]\n\n\ntop10_asia_sum2 = top10_asia_sum2.groupby(\n    ['Country_Region','Date'])['ConfirmedCases'].sum().reset_index()\n    \nsns.lineplot(data=top10_asia_sum2, x=\"Date\", y=\"ConfirmedCases\", hue=\"Country_Region\")\n\nplt.ylabel(\"Total confirmed cases by Asia countries\")\n\nplt.title(\"Total numbers of Confirmed Cases in by Asia countries - excluding China, Korea and Iran\",fontsize=20)","20d7c513":"\ntop10_asia_sum3 = asia_sum.loc[asia_sum['Country_Region'].isin(['Korea, South','Turkey','Israel',\n                                                               'Malaysia','Japan','Pakistan',\n                                                               'Thailand','Saudi Arabia','Indonesia',\n                                                              'Russia','India'])==True]\n\n\ntop10_asia_sum3 = top10_asia_sum3.groupby(\n    ['Country_Region','Date'])['ConfirmedCases','Fatalities'].sum().reset_index()\n\ntop10_asia_sum3['Fatal_Rate'] = round((top10_asia_sum3['Fatalities']\/top10_asia_sum3['ConfirmedCases'])*100,2)\n    \nsns.lineplot(data=top10_asia_sum3, x=\"Date\", y=\"Fatal_Rate\", hue=\"Country_Region\")\n\nplt.ylabel(\"Fatality Rate in Asia countries in Percentage\")\n\nplt.title(\"Fatality Rate  in Asia countries - excluding China and Iran\",fontsize=20)","7354c221":"\ntop10_asia_sum4 = complete_data.loc[complete_data['Country_Region'].isin(['Korea, South','Iran','Turkey','Israel',\n                                                               'Malaysia','Japan','Pakistan',\n                                                               'Thailand','Saudi Arabia','Indonesia',\n                                                              'Russia','India','Philippines'])==True]\n\ntop10_asia_sum4 = top10_asia_sum4.groupby(['Country_Region','Date'])['Confirmed','Recovered'].sum().reset_index()\n\ntop10_asia_sum4['Recover_Rate'] = round((top10_asia_sum4['Recovered']\/top10_asia_sum4['Confirmed'])*100,2)\n\nsns.lineplot(data=top10_asia_sum4, x=\"Date\", y=\"Recover_Rate\", hue=\"Country_Region\")\n\nplt.ylabel(\"Recovery Rate in Europe countries in Percentage\")\n\nplt.title(\"Recovery Rate  in Europe countries\",fontsize=20)","232be653":"\ntemp_covid = weather_data.groupby(['Date', 'Country_Region'])['temp'].mean().reset_index()\ntemp_covid['Date'] = pd.to_datetime(temp_covid['Date'])\nmap_covid['Date'] = pd.to_datetime(map_covid['Date'])\n\n#merge with the confirmed cases for size changing\ntemp_covid = pd.merge(temp_covid, map_covid, on=['Date','Country_Region'],how='left')\n\ntemp_covid['Date'] = temp_covid['Date'].dt.strftime('%m\/%d\/%Y')\n\nfig = px.scatter_geo(temp_covid, locations=\"Country_Region\", locationmode='country names', \n                     color=\"temp\", size='size', hover_name=\"Country_Region\", \n                     range_color=[1,100],\n                     projection=\"natural earth\", animation_frame=\"Date\", \n                     title='COVID-19: Temperature according to the number of Confirmed Cases Around the Globe', \n                     color_continuous_scale=\"tealrose\")\nfig.show()\n","4c06f66d":"wdsp_covid = weather_data.groupby(['Date', 'Country_Region'])['wdsp'].max().reset_index()\nwdsp_covid['Date'] = pd.to_datetime(wdsp_covid['Date'])\n\n#merge with the confirmed cases for size changing\nwdsp_covid = pd.merge(wdsp_covid, map_covid, on=['Date','Country_Region'],how='left')\n\nwdsp_covid['Date'] = wdsp_covid['Date'].dt.strftime('%m\/%d\/%Y')\n\nfig = px.scatter_geo(wdsp_covid, locations=\"Country_Region\", locationmode='country names', \n                     color=\"wdsp\", size='size', hover_name=\"Country_Region\", \n                     range_color=[1,100],\n                     projection=\"natural earth\", animation_frame=\"Date\", \n                     title='COVID-19: Windspeed according to the number of Confirmed Cases Around the Globe', \n                     color_continuous_scale=\"tealrose\")\nfig.show()\n","6e1e0a72":"humid_covid = weather_addition.groupby(['Date', 'Country_Region'])['humidity'].mean().reset_index()\nhumid_covid['Date'] = pd.to_datetime(humid_covid['Date'])\n\n#merge with the confirmed cases for size changing\nhumid_covid = pd.merge(humid_covid, map_covid, on=['Date','Country_Region'],how='left')\nhumid_covid = humid_covid.dropna()\n\nhumid_covid['Date'] = humid_covid['Date'].dt.strftime('%m\/%d\/%Y')\n\nfig = px.scatter_geo(humid_covid, locations=\"Country_Region\", locationmode='country names', \n                     color=\"humidity\", size='size', hover_name=\"Country_Region\", \n                     range_color=[1,100],\n                     projection=\"natural earth\", animation_frame=\"Date\", \n                     title='COVID-19: Humidity according to the number of Confirmed Cases Around the Globe', \n                     color_continuous_scale=\"tealrose\")\nfig.show()\n","f9f6b061":"sun_covid = weather_addition.groupby(['Date', 'Country_Region'])['sunHour'].mean().reset_index()\nsun_covid['Date'] = pd.to_datetime(sun_covid['Date'])\n\n#merge with the confirmed cases for size changing\nsun_covid = pd.merge(sun_covid, map_covid, on=['Date','Country_Region'],how='left')\nsun_covid = sun_covid.dropna()\n\nsun_covid['Date'] = sun_covid['Date'].dt.strftime('%m\/%d\/%Y')\n\nfig = px.scatter_geo(sun_covid, locations=\"Country_Region\", locationmode='country names', \n                     color=\"sunHour\", size='size', hover_name=\"Country_Region\", \n                     range_color=[1,100],\n                     projection=\"natural earth\", animation_frame=\"Date\", \n                     title='COVID-19: Humidity according to the number of Confirmed Cases Around the Globe', \n                     color_continuous_scale=\"tealrose\")\nfig.show()\n","fb7fb385":"#join the dataframe\n\ntemp_covid1 = pd.merge(temp_covid, wdsp_covid[['Date','Country_Region','wdsp']], \n                             on=['Date','Country_Region'],how='left')\ntemp_covid1 = pd.merge(temp_covid1, humid_covid[['Date','Country_Region','humidity']], \n                             on=['Date','Country_Region'],how='left')\ntemp_covid1 = pd.merge(temp_covid1, sun_covid[['Date','Country_Region','sunHour']], \n                             on=['Date','Country_Region'],how='left')\n\ntemp_covid1 = temp_covid1.dropna()\n\ntemp_covid1.head()\n","20bc0263":"#construct the Multilinear regression model\nX =  temp_covid1[['temp','wdsp', 'humidity', 'sunHour']]\ny = temp_covid1['ConfirmedCases']\n\n# Note the difference in argument order\nmodel = sm.OLS(y, X).fit()\n\n#model summary\nmodel.summary()\n","6c0f7c20":"china_temp = temp_covid1.loc[temp_covid1['Country_Region']=='China']\n\n#construct the OLS model\nX =  china_temp[['temp', 'wdsp', 'humidity', 'sunHour']]\ny = china_temp['ConfirmedCases']\n\n# Note the difference in argument order\nmodel = sm.OLS(y, X).fit()\n\n#predictions = model.predict(X) # make the predictions by the model\n\nmodel.summary()\n","210d6ed5":"italy_temp = temp_covid1.loc[temp_covid1['Country_Region']=='Italy']\n\n#construct the OLS model\nX =  italy_temp[['temp', 'wdsp', 'humidity', 'sunHour']]\ny = italy_temp['ConfirmedCases']\n\n# Note the difference in argument order\nmodel = sm.OLS(y, X).fit()\n\n#predictions = model.predict(X) # make the predictions by the model\n\nmodel.summary()","fcb21334":"\nus_temp = temp_covid1.loc[temp_covid1['Country_Region']=='US']\n\n#construct the OLS model\nX =  us_temp[['temp','wdsp', 'humidity', 'sunHour']]\ny = us_temp['ConfirmedCases']\n\n# Note the difference in argument order\nmodel = sm.OLS(y, X).fit()\n\n#predictions = model.predict(X) # make the predictions by the model\n\nmodel.summary()\n","844a3784":"#Combine US states to only US \ndemo_data1 = demo_data.replace(['Alabama', 'Alaska','Arizona','Arkansas','California','Colorado','Connecticut','Delaware',\n             'Florida','Georgia','Hawaii','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana',\n             'Maine','Maryland','Massachusetts','Michigan','Minnesota','Mississippi','Missouri','Montana','Nebraska',\n             'Nevada','New Hampshire','New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota',\n             'Ohio','Oklahoma','Oregon','Pennsylvania','Rhode Island','South Carolina','South Dakota', 'Tennessee',\n             'Texas','Utah','Vermont','Virginia','Washington','West Virginia','Wisconsin','Wyoming','San Franciso',\n             'GeorgiaUS', 'Atlanta', 'Honolulu', 'Washington DC'], 'US')\n\ndemo_data1.head()\n","aad2fae2":"\ndemo_data_pop = demo_data1.groupby(['country'])['country','pop'].sum().reset_index().sort_values('pop',ascending=False)\n\ndemo_data_pop = demo_data_pop.loc[demo_data_pop['country'].isin(['US', 'Italy', 'China', 'Spain', 'Germany', 'France', 'Iran',\n       'United Kingdom', 'Switzerland', 'Netherlands', 'Belgium',\n       'Korea, South', 'Turkey', 'Austria', 'Canada', 'Portugal', 'Norway',\n       'Brazil', 'Israel', 'Australia'])==True]\n\nplot_pop=sns.barplot(x=\"pop\",y=\"country\", data=demo_data_pop)\n\nplt.xlabel(\"Population\")\n\nplt.title(\"Population\",fontsize=20)\n\nfor p in plot_pop.patches:\n    width = p.get_width()\n    plot_pop.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")\n    ","62ed96a2":"\ndemo_data2 = demo_data1.sort_values('tests',ascending=False).head(20)\n\nplot10 = sns.barplot(x=\"tests\",y=\"country\", data=demo_data2)\n\nplt.xlabel(\"Total number of COVID-19 test\")\n\nplt.title(\"Total number of COVID-19 test\",fontsize=20)\n\nfor p in plot10.patches:\n    width = p.get_width()\n    plot10.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")","e6a6d37a":"\ndemo_data3 = demo_data1.groupby(['country'])['country',\n                                           'hospibed'].mean().reset_index().sort_values('hospibed',ascending=False)\n\ndemo_data3 = demo_data3.loc[demo_data3['country'].isin(['US', 'Italy', 'China', 'Spain', 'Germany', 'France', 'Iran',\n       'United Kingdom', 'Switzerland', 'Netherlands', 'Belgium',\n       'Korea, South', 'Turkey', 'Austria', 'Canada', 'Portugal', 'Norway',\n       'Brazil', 'Israel', 'Australia'])==True]\n\nplot11 = sns.barplot(x=\"hospibed\",y=\"country\", data=demo_data3)\n\nplt.xlabel(\"Hospital bed per 1,000 people\")\n\nplt.title(\"Amount of hospital bed per 1,000 people\",fontsize=20)\n","955bd986":"demo_data9 = demo_data1.groupby(['country'])['country',\n                                           'medianage'].median().reset_index().sort_values('medianage',ascending=False)\n\ndemo_data9 = demo_data9.loc[demo_data9['country'].isin(['US', 'Italy', 'China', 'Spain', 'Germany', 'France', 'Iran',\n       'United Kingdom', 'Switzerland', 'Netherlands', 'Belgium',\n       'Korea, South', 'Turkey', 'Austria', 'Canada', 'Portugal', 'Norway',\n       'Brazil', 'Israel', 'Australia'])==True]\n\nplot13 = sns.barplot(x=\"medianage\",y=\"country\", data=demo_data9)\n\nplt.xlabel(\"Median Age\")\n\nplt.title(\"Median Age by Country\",fontsize=20)","c9a9dfe6":"demo_data4 = demo_data1.groupby(['country'])['country',\n                                           'density'].sum().reset_index().sort_values('density',ascending=False)\n\ndemo_data4 = demo_data4.loc[demo_data4['country'].isin(['US', 'Italy', 'China', 'Spain', 'Germany', 'France', 'Iran',\n       'United Kingdom', 'Switzerland', 'Netherlands', 'Belgium',\n       'Korea, South', 'Turkey', 'Austria', 'Canada', 'Portugal', 'Norway',\n       'Brazil', 'Israel', 'Australia'])==True]\n\nplot12 = sns.barplot(x=\"density\",y=\"country\", data=demo_data4)\n\nplt.xlabel(\"denisity\")\n\nplt.title(\"Population Density by Country\",fontsize=20)\n\nfor p in plot12.patches:\n    width = p.get_width()\n    plot12.text(width + 1.5  ,\n            p.get_y()+p.get_height()\/2. + 0.2,\n            '{:1.0f}'.format(width),\n            ha=\"left\")\n    ","26750c94":"demo_data7 = demo_data1.groupby(['country'])['country',\n                                           'lung'].mean().reset_index().sort_values('lung',ascending=False)\n\ndemo_data7 = demo_data7.loc[demo_data7['country'].isin(['US', 'Italy', 'China', 'Spain', 'Germany', 'France', 'Iran',\n       'United Kingdom', 'Switzerland', 'Netherlands', 'Belgium',\n       'Korea, South', 'Turkey', 'Austria', 'Canada', 'Portugal', 'Norway',\n       'Brazil', 'Israel', 'Australia'])==True]\n\nplot14 = sns.barplot(x=\"lung\",y=\"country\", data=demo_data7)\n\nplt.xlabel(\"Death rate from lung diseases per 100k people\")\n\nplt.title(\"Death rate from lung diseases per 100k people by Country\",fontsize=20)\n","90fc14d5":"demo_data11 = demo_data1.groupby(['country'])['country',\n                                           'smokers'].sum().reset_index().sort_values('smokers',ascending=False)\n\ndemo_data11 = demo_data11.loc[demo_data11['country'].isin(['US', 'Italy', 'China', 'Spain', 'Germany', 'France', 'Iran',\n       'United Kingdom', 'Switzerland', 'Netherlands', 'Belgium',\n       'Korea, South', 'Turkey', 'Austria', 'Canada', 'Portugal', 'Norway',\n       'Brazil', 'Israel', 'Australia'])==True]\n\nplot111 = sns.barplot(x=\"smokers\",y=\"country\", data=demo_data11)\n\nplt.xlabel(\"Number of smokers\")\n\nplt.title(\"Number of smokers by Country\",fontsize=20)","b9b44adf":"\nchina_sum = complete_data.loc[complete_data['Country_Region']==\"China\"].groupby('Date')['Date', 'Confirmed','Deaths','Active','Recovered'].sum().reset_index()\n\nchina_sum = pd.melt(china_sum, id_vars=['Date'], value_vars=['Confirmed','Deaths','Active','Recovered'])\n\nchina_sum = china_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nax=sns.lineplot(data=china_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nax.axvline(pd.to_datetime('2020-01-23'), color=\"red\", linestyle=\"--\")\n\nax.axvline(pd.to_datetime('2020-02-12'), color=\"gray\", linestyle=\"--\")\n\nax.annotate(\"Date first quarantine\", xy=(pd.to_datetime('2020-01-24'), 50000))\n\nplt.ylabel(\"Total cases in China\")\n\nplt.title(\"Total numbers of Cases in China\",fontsize=20)\n","4ac0909d":"Image(\"..\/input\/images\/Epidemic Curve of the Confirmed Cases of Coronavirus Disease 2019 (COVID-19).png\")","64cf8061":"italy_sum = complete_data.loc[complete_data['Country_Region']==\"Italy\"].groupby('Date')['Date', 'Confirmed','Deaths','Active','Recovered'].sum().reset_index()\n\nitaly_sum = pd.melt(italy_sum, id_vars=['Date'], value_vars=['Confirmed','Deaths','Active','Recovered'])\n\nitaly_sum = italy_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nax=sns.lineplot(data=italy_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nax.axvline(pd.to_datetime('2020-03-19'), color=\"red\", linestyle=\"--\")\n\nax.annotate(\"Date first quarantine\", xy=(pd.to_datetime('2020-03-20'), 20000))\n\n\nplt.ylabel(\"Total cases in Italy\")\n\nplt.title(\"Total numbers of Cases in Italy\",fontsize=20)\n","45337285":"us_sum = complete_data.loc[complete_data['Country_Region']==\"US\"].groupby('Date')['Date', 'Confirmed','Deaths','Active','Recovered'].sum().reset_index()\n\nus_sum = pd.melt(us_sum, id_vars=['Date'], value_vars=['Confirmed','Deaths','Active','Recovered'])\n\nus_sum = us_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nax=sns.lineplot(data=us_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nax.axvline(pd.to_datetime('2020-03-16'), color=\"red\", linestyle=\"--\")\n\nax.annotate(\"Date first quarantine\", xy=(pd.to_datetime('2020-03-17'), 120000))\n\nplt.ylabel(\"Total cases in the U.S.\")\n\nplt.title(\"Total numbers of Cases in the U.S.\",fontsize=20)\n","8b2deac0":"demo_data = demo_data.rename(columns={'country':'Country_Region'})\n\ndemo_data_join = demo_data.copy()\n\ndemo_data_join = demo_data_join.drop_duplicates(['Country_Region'],keep='first')\n","b1a761ed":"\ntrain_demo = pd.merge(train, demo_data_join[['Country_Region', 'pop', 'tests',\n       'testpop', 'density', 'medianage', 'urbanpop', \n       'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54', 'sex64',\n       'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility']], on=['Country_Region'], how='left')\n\ntest_demo = pd.merge(test, demo_data_join[['Country_Region', 'pop', 'tests',\n       'testpop', 'density', 'medianage', 'urbanpop', \n       'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54', 'sex64',\n       'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility']], on=['Country_Region'], how='left')\n\ntrain_demo.head()","a378d982":"Image(\"..\/input\/images\/log-curve.png\")","e9304047":"china_sum = complete_data.loc[complete_data['Country_Region']==\"China\"].groupby('Date')['Date', 'Confirmed'].sum().reset_index()\n\nchina_sum = pd.melt(china_sum, id_vars=['Date'], value_vars=['Confirmed'])\n\nchina_sum = china_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nax=sns.lineplot(data=china_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nax.axvline(pd.to_datetime('2020-01-23'), color=\"red\", linestyle=\"--\")\n\nax.axvline(pd.to_datetime('2020-02-12'), color=\"gray\", linestyle=\"--\")\n\nax.annotate(\"Date first quarantine\", xy=(pd.to_datetime('2020-01-24'), 50000))\n\nplt.ylabel(\"Total cases in China\")\n\nplt.title(\"Total numbers of Cases in China\",fontsize=20)\n","99f3e002":"china_sum = complete_data.loc[complete_data['Country_Region']==\"South Korea\"].groupby('Date')['Date', 'Confirmed'].sum().reset_index()\n\nchina_sum = pd.melt(china_sum, id_vars=['Date'], value_vars=['Confirmed'])\n\nchina_sum = china_sum.rename(columns={\"value\": \"total\", \"variable\": \"Cases\"})\n\nax=sns.lineplot(data=china_sum, x=\"Date\", y=\"total\", hue=\"Cases\")\n\nax.axvline(pd.to_datetime('2020-03-01'), color=\"gray\", linestyle=\"--\")\n\nax.axvline(pd.to_datetime('2020-02-01'), color=\"red\", linestyle=\"--\")\n\nax.annotate(\"First roll out of widespread testing \\n around this time\", xy=(pd.to_datetime('2020-02-02'), 5000))\n\nplt.ylabel(\"Total confirmed cases in South Korea\")\n\nplt.title(\"Total numbers of Cases in South Korea\",fontsize=20)\n","3614e21c":"# This functions smooths data, thanks to Dan Pearson. We will use it to smooth the data for growth factor.\ndef smoother(inputdata,w,imax):\n    data = 1.0*inputdata\n    data = data.replace(np.nan,1)\n    data = data.replace(np.inf,1)\n    #print(data)\n    smoothed = 1.0*data\n    normalization = 1\n    for i in range(-imax,imax+1):\n        if i==0:\n            continue\n        smoothed += (w**abs(i))*data.shift(i,axis=0)\n        normalization += w**abs(i)\n    smoothed \/= normalization\n    return smoothed\n\ndef growth_factor(confirmed):\n    confirmed_iminus1 = confirmed.shift(1, axis=0)\n    confirmed_iminus2 = confirmed.shift(2, axis=0)\n    return (confirmed-confirmed_iminus1)\/(confirmed_iminus1-confirmed_iminus2)\n\ndef growth_ratio(confirmed):\n    confirmed_iminus1 = confirmed.shift(1, axis=0)\n    return (confirmed\/confirmed_iminus1)\n\n# This is a function which plots (for in input country) the active, confirmed, and recovered cases, deaths, and the growth factor.\ndef plot_country_active_confirmed_recovered(country):\n    \n    # Plots Active, Confirmed, and Recovered Cases. Also plots deaths.\n    country_data = train[train['Country_Region']==country]\n    table = country_data.drop(['Id','Province_State'], axis=1)\n\n    table2 = pd.pivot_table(table, values=['ConfirmedCases','Fatalities'], index=['Date'], aggfunc=np.sum)\n    table3 = table2.drop(['Fatalities'], axis=1)\n   \n    # Growth Factor\n    w = 0.5\n    table2['GrowthFactor'] = growth_factor(table2['ConfirmedCases'])\n    table2['GrowthFactor'] = smoother(table2['GrowthFactor'],w,5)\n\n    # 2nd Derivative\n    table2['2nd_Derivative'] = np.gradient(np.gradient(table2['ConfirmedCases'])) #2nd derivative\n    table2['2nd_Derivative'] = smoother(table2['2nd_Derivative'],w,7)\n\n\n    #Plot confirmed[i]\/confirmed[i-1], this is called the growth ratio\n    table2['GrowthRatio'] = growth_ratio(table2['ConfirmedCases'])\n    table2['GrowthRatio'] = smoother(table2['GrowthRatio'],w,5)\n    \n        #Plot the growth rate, we will define this as k in the logistic function presented at the beginning of this notebook.\n    table2['GrowthRate']=np.gradient(np.log(table2['ConfirmedCases']))\n    table2['GrowthRate'] = smoother(table2['GrowthRate'],0.5,3)\n    \n    # horizontal line at growth rate 1.0 for reference\n    x_coordinates = [1, 100]\n    y_coordinates = [1, 1]\n    \n    sns.set(rc={'figure.figsize':(10, 5)})\n\n    \n    pd.plotting.register_matplotlib_converters()\n\n    #plots\n    table2['Fatalities'].plot(title='Fatalities')\n    plt.show()\n    table3.plot() \n    plt.show()\n    table2['GrowthFactor'].plot(title='Growth Factor')\n    plt.plot(x_coordinates, y_coordinates) \n    plt.show()\n    table2['2nd_Derivative'].plot(title='2nd_Derivative')\n    plt.show()\n    table2['GrowthRatio'].plot(title='Growth Ratio')\n    plt.plot(x_coordinates, y_coordinates)\n    plt.show()\n    table2['GrowthRate'].plot(title='Growth Rate')\n    plt.show()\n\n\n    return \n","cda5a980":"plot_country_active_confirmed_recovered('China')","b0c9fba9":"plot_country_active_confirmed_recovered('Korea, South')","18257668":"plot_country_active_confirmed_recovered('US')","c963f457":"plot_country_active_confirmed_recovered('Italy')","cc3edf35":"plot_country_active_confirmed_recovered('Spain')","2a2b309f":"plot_country_active_confirmed_recovered('Vietnam')","8fad617d":"Image(\"..\/input\/images\/SEIR-math.png\")\n","12219825":"# Function code refernece from https:\/\/www.kaggle.com\/anjum48\/seir-model-with-intervention\n\n# Susceptible equation\ndef dS_dt(S, I, R_t, T_inf):\n    return -(R_t \/ T_inf) * I * S\n\n# Exposed equation\ndef dE_dt(S, E, I, R_t, T_inf, T_inc):\n    return (R_t \/ T_inf) * I * S - (T_inc**-1) * E\n\n# Infected equation\ndef dI_dt(I, E, T_inc, T_inf):\n    return (T_inc**-1) * E - (T_inf**-1) * I\n\n# Recovered\/Remove\/deceased equation\ndef dR_dt(I, T_inf):\n    return (T_inf**-1) * I\n\ndef SEIR_model(t, y, R_t, T_inf, T_inc):\n    \n    if callable(R_t):\n        reproduction = R_t(t)\n    else:\n        reproduction = R_t\n        \n    S, E, I, R = y\n    \n    S_out = dS_dt(S, I, reproduction, T_inf)\n    E_out = dE_dt(S, E, I, reproduction, T_inf, T_inc)\n    I_out = dI_dt(I, E, T_inc, T_inf)\n    R_out = dR_dt(I, T_inf)\n    \n    return [S_out, E_out, I_out, R_out]","dc322896":"## Thanks @funkyboy for the plotting function\n\ndef plot_model_and_predict(data, pop, solution, title='SEIR model'):\n    sus, exp, inf, rec = solution.y\n    \n    f = plt.figure(figsize=(16,5))\n    ax = f.add_subplot(1,2,1)\n    #ax.plot(sus, 'b', label='Susceptible');\n    ax.plot(exp, 'y', label='Exposed');\n    ax.plot(inf, 'r', label='Infected');\n    ax.plot(rec, 'c', label='Recovered\/deceased');\n    plt.title(title)\n    plt.xlabel(\"Days\", fontsize=10);\n    plt.ylabel(\"Fraction of population\", fontsize=10);\n    plt.legend(loc='best');\n    \n    ax2 = f.add_subplot(1,2,2)\n    preds = np.clip((inf + rec) * pop ,0,np.inf)\n    ax2.plot(range(len(data)),preds[:len(data)],label = 'Predict ConfirmedCases')\n    ax2.plot(range(len(data)),data['ConfirmedCases'])\n    plt.title('Model predict and data')\n    plt.ylabel(\"Population\", fontsize=10);\n    plt.xlabel(\"Days\", fontsize=10);\n    plt.legend(loc='best');","ab7b4b33":"Country = 'New York'\nN = pop_info[pop_info['Name']==Country]['Population'].tolist()[0] # Hubei Population \n\n# Load dataset of Hubei\ntrain_loc = train[train['Country_Region']==Country].query('ConfirmedCases > 0')\nif len(train_loc)==0:\n    train_loc = train[train['Province_State']==Country].query('ConfirmedCases > 0')\n\nn_infected = train_loc['ConfirmedCases'].iloc[0] # start from first comfirmedcase on dataset first date\nmax_days = len(train_loc)# how many days want to predict\n\n# Initial stat for SEIR model\ns = (N - n_infected)\/ N\ne = 0.\ni = n_infected \/ N\nr = 0.\n\n# Define all variable of SEIR model \nT_inc = 5.2  # average incubation period\nT_inf = 2.9 # average infectious period\nR_0 = 3.954 # reproduction number\n\n## Solve the SEIR model \nsol = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(R_0, T_inf, T_inc), \n                t_eval=np.arange(max_days))\n\n## Plot result\nplot_model_and_predict(train_loc, N, sol, title = 'SEIR Model (without intervention)')","01d1c1d7":"# Define all variable of SEIR model \nT_inc = 5.2  # average incubation period\nT_inf = 2.9  # average infectious period\n\n# Define the intervention parameters (fit result, latter will show how to fit)\nR_0, cfr, k, L=[ 3.95469597 , 0.04593316 , 3.      ,   15.32328881]\n\ndef time_varying_reproduction(t): \n    return R_0 \/ (1 + (t\/L)**k)\n\nsol2 = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(time_varying_reproduction, T_inf, T_inc), \n                t_eval=np.arange(max_days))\n\nplot_model_and_predict(train_loc, N, sol2, title = 'SEIR Model (with intervention)')","8d83a8cb":"def cumsum_signal(vec):\n    temp_val = 0\n    vec_new = []\n    for i in vec:\n        if i > temp_val:\n            vec_new.append(i)\n            temp_val = i\n        else:\n            vec_new.append(temp_val)\n    return vec_new","fbc29da7":"# Use a constant reproduction number\ndef eval_model_const(params, data, population, return_solution=False, forecast_days=0):\n    R_0, cfr = params # Paramaters, R0 and cfr \n    N = population # Population of each country\n    n_infected = data['ConfirmedCases'].iloc[0] # start from first comfirmedcase on dataset first date\n    max_days = len(data) + forecast_days # How many days want to predict\n    s, e, i, r = (N - n_infected)\/ N, 0, n_infected \/ N, 0 #Initial stat for SEIR model\n    \n    # R0 become half after intervention days\n    def time_varying_reproduction(t):\n        if t > 60: # we set intervention days = 60\n            return R_0 * 0.5\n        else:\n            return R_0\n    \n    # Solve the SEIR differential equation.\n    sol = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(time_varying_reproduction, T_inf, T_inc),\n                    t_eval=np.arange(0, max_days))\n    \n    sus, exp, inf, rec = sol.y\n    # Predict confirmedcase\n    y_pred_cases = np.clip((inf + rec) * N ,0,np.inf)\n    y_true_cases = data['ConfirmedCases'].values\n    \n    # Predict Fatalities by remove * fatality rate(cfr)\n    y_pred_fat = np.clip(rec*N* cfr, 0, np.inf)\n    y_true_fat = data['Fatalities'].values\n    \n    optim_days = min(20, len(data))  # Days to optimise for\n    weights = 1 \/ np.arange(1, optim_days+1)[::-1]  # Recent data is more heavily weighted\n    \n    # using mean squre log error to evaluate\n    msle_cases = mean_squared_log_error(y_true_cases[-optim_days:], y_pred_cases[-optim_days:], weights)\n    msle_fat = mean_squared_log_error(y_true_fat[-optim_days:], y_pred_fat[-optim_days:], weights)\n    msle_final = np.mean([msle_cases, msle_fat])\n    \n    if return_solution:\n        return msle_final, sol\n    else:\n        return msle_final","0f908d96":"# Use a Hill decayed reproduction number\ndef eval_model_decay(params, data, population, return_solution=False, forecast_days=0):\n    R_0, cfr, k, L = params # Paramaters, R0 and cfr \n    N = population # Population of each country\n    n_infected = data['ConfirmedCases'].iloc[0] # start from first comfirmedcase on dataset first date\n    max_days = len(data) + forecast_days # How many days want to predict\n    s, e, i, r = (N - n_infected)\/ N, 0, n_infected \/ N, 0 #Initial stat for SEIR model\n    \n    # https:\/\/github.com\/SwissTPH\/openmalaria\/wiki\/ModelDecayFunctions   \n    # Hill decay. Initial values: R_0=2.2, k=2, L=50\n    def time_varying_reproduction(t): \n        return R_0 \/ (1 + (t\/L)**k)\n    \n    # Solve the SEIR differential equation.\n    sol = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(time_varying_reproduction, T_inf, T_inc),\n                    t_eval=np.arange(0, max_days))\n    \n    sus, exp, inf, rec = sol.y\n    # Predict confirmedcase\n    y_pred_cases = np.clip((inf + rec) * N ,0,np.inf)\n    y_true_cases = data['ConfirmedCases'].values\n    \n    # Predict Fatalities by remove * fatality rate(cfr)\n    y_pred_fat = np.clip(rec*N* cfr, 0, np.inf)\n    y_true_fat = data['Fatalities'].values\n    \n    optim_days = min(20, len(data))  # Days to optimise for\n    weights = 1 \/ np.arange(1, optim_days+1)[::-1]  # Recent data is more heavily weighted\n    \n    # using mean squre log error to evaluate\n    msle_cases = mean_squared_log_error(y_true_cases[-optim_days:], y_pred_cases[-optim_days:], weights)\n    msle_fat = mean_squared_log_error(y_true_fat[-optim_days:], y_pred_fat[-optim_days:], weights)\n    msle_final = np.mean([msle_cases, msle_fat])\n    \n    if return_solution:\n        return msle_final, sol\n    else:\n        return msle_final","07af110a":"from matplotlib import dates\nimport plotly.graph_objects as go\n\ndef fit_model_new(data, area_name, initial_guess=[2.2, 0.02, 2, 50], \n              bounds=((1, 20), (0, 0.15), (1, 3), (1, 100)), make_plot=True, decay_mode = None):\n    \n    if area_name in ['France']:# France last data looks weird, remove it\n        train = data.query('ConfirmedCases > 0').copy()[:-1]\n    else:\n        train = data.query('ConfirmedCases > 0').copy()\n    \n    ####### Split Train & Valid #######\n    valid_data = train[-7:]\n    train_data = train[:-7]\n    \n    ####### If this country have no ConfirmedCase, return 0 #######\n    if len(train_data) == 0:\n        result_zero = np.zeros((43))\n        return pd.DataFrame({'ConfirmedCases':result_zero,'Fatalities':result_zero}), 0 \n    \n    ####### Load the population of area #######\n    try:\n        #population = province_lookup[area_name]\n        population = pop_info[pop_info['Name']==area_name]['Population'].tolist()[0]\n    except IndexError:\n        print ('country not in population set, '+str(area_name))\n        population = 1000000 \n    \n    \n    if area_name == 'US':\n        population = 327200000\n        \n    cases_per_million = train_data['ConfirmedCases'].max() * 10**6 \/ population\n    n_infected = train_data['ConfirmedCases'].iloc[0]\n    \n    ####### Total case\/popuplation below 1, reduce country population #######\n    if cases_per_million < 1:\n        #print ('reduce pop divide by 100')\n        population = population\/100\n        \n    ####### Fit the real data by minimize the MSLE #######\n    res_const = minimize(eval_model_const, [2.2, 0.02], bounds=((1, 20), (0, 0.15)),\n                         args=(train_data, population, False),\n                         method='L-BFGS-B')\n\n    res_decay = minimize(eval_model_decay, initial_guess, bounds=bounds,\n                         args=(train_data, population, False),\n                         method='L-BFGS-B')\n    \n    ####### Align the date information #######\n    test_end = datetime.strptime('2020-04-30','%Y-%m-%d')\n    test_start = datetime.strptime('2020-03-19','%Y-%m-%d')\n    test_period = (test_end - test_start).days\n    train_max = train_data.Date.max()\n    train_min = train_data.Date.min()\n    add_date = 0\n    delta_days =(test_end - train_max).days\n    train_add_time=[]\n\n    if train_min > test_start:\n        add_date = (train_min-test_start).days\n        last = train_min-pd.Timedelta(days=add_date)\n        train_add_time = np.arange(last, train_min, dtype='datetime64[D]').tolist()\n        train_add_time = pd.to_datetime(train_add_time)\n        dates_all = train_add_time.append(pd.to_datetime(np.arange(train_min, test_end+pd.Timedelta(days=1), dtype='datetime64[D]')))\n    else:\n        dates_all = pd.to_datetime(np.arange(train_min, test_end+pd.Timedelta(days=1), dtype='datetime64[D]'))\n\n\n    ####### Auto find the best decay function ####### \n    if decay_mode is None:\n        if res_const.fun < res_decay.fun :\n            msle, sol = eval_model_const(res_const.x, train_data, population, True, delta_days+add_date)\n            res = res_const\n\n        else:\n            msle, sol = eval_model_decay(res_decay.x, train_data, population, True, delta_days+add_date)\n            res = res_decay\n            R_0, cfr, k, L = res.x\n    else:\n        if decay_mode =='day_decay':\n            msle, sol = eval_model_const(res_const.x, train_data, population, True, delta_days+add_date)\n            res = res_const\n        else:\n            msle, sol = eval_model_decay(res_decay.x, train_data, population, True, delta_days+add_date)\n            res = res_decay\n            R_0, cfr, k, L = res.x\n\n    ####### Predict the result by using best fit paramater of SEIR model ####### \n    sus, exp, inf, rec = sol.y\n    \n    y_pred = pd.DataFrame({\n        'ConfirmedCases': cumsum_signal(np.diff((inf + rec) * population, prepend=n_infected).cumsum()),\n       # 'ConfirmedCases': [inf[0]*population for i in range(add_date)]+(np.clip((inf + rec) * population,0,np.inf)).tolist(),\n       # 'Fatalities': [rec[0]*population for i in range(add_date)]+(np.clip(rec, 0, np.inf) * population * res.x[1]).tolist()\n        'Fatalities': cumsum_signal((np.clip(rec * population * res.x[1], 0, np.inf)).tolist())\n    })\n\n    y_pred_valid = y_pred.iloc[len(train_data):len(train_data)+len(valid_data)]\n    #y_pred_valid = y_pred.iloc[:len(train_data)]\n    y_pred_test = y_pred.iloc[-(test_period+1):]\n    #y_true_valid = train_data[['ConfirmedCases', 'Fatalities']]\n    y_true_valid = valid_data[['ConfirmedCases', 'Fatalities']]\n    #print (len(y_pred),train_min)\n    #print (y_true_valid['ConfirmedCases'])\n    #print (y_pred_valid['ConfirmedCases'])\n    ####### Calculate MSLE ####### \n    valid_msle_cases = mean_squared_log_error(y_true_valid['ConfirmedCases'], y_pred_valid['ConfirmedCases'])\n    valid_msle_fat = mean_squared_log_error(y_true_valid['Fatalities'], y_pred_valid['Fatalities'])\n    valid_msle = np.mean([valid_msle_cases, valid_msle_fat])\n    \n    ####### Plot the fit result of train data and forecast after 250 days ####### \n    if make_plot:\n        if len(res.x)<=2:\n            print(f'Validation MSLE: {valid_msle:0.5f}, using intervention days decay, Reproduction number(R0) : {res.x[0]:0.5f}, Fatal rate : {res.x[1]:0.5f}')\n        else:\n            print(f'Validation MSLE: {valid_msle:0.5f}, using Hill decay, Reproduction number(R0) : {res.x[0]:0.5f}, Fatal rate : {res.x[1]:0.5f}, K : {res.x[2]:0.5f}, L: {res.x[3]:0.5f}')\n        \n        ####### Plot the fit result of train data dna SEIR model trends #######\n\n        f = plt.figure(figsize=(16,5))\n        ax = f.add_subplot(1,2,1)\n        ax.plot(exp, 'y', label='Exposed');\n        ax.plot(inf, 'r', label='Infected');\n        ax.plot(rec, 'c', label='Recovered\/deceased');\n        plt.title('SEIR Model Trends')\n        plt.xlabel(\"Days\", fontsize=10);\n        plt.ylabel(\"Fraction of population\", fontsize=10);\n        plt.legend(loc='best');\n        #train_date_remove_year = train_data['Date'].apply(lambda date:'{:%m-%d}'.format(date))\n        ax2 = f.add_subplot(1,2,2)\n        xaxis = train_data['Date'].tolist()\n        xaxis = dates.date2num(xaxis)\n        hfmt = dates.DateFormatter('%m\\n%d')\n        ax2.xaxis.set_major_formatter(hfmt)\n        ax2.plot(np.array(train_data['Date'], dtype='datetime64[D]'),train_data['ConfirmedCases'],label='Confirmed Cases (train)', c='g')\n        ax2.plot(np.array(train_data['Date'], dtype='datetime64[D]'), y_pred['ConfirmedCases'][:len(train_data)],label='Cumulative modeled infections', c='r')\n        ax2.plot(np.array(valid_data['Date'], dtype='datetime64[D]'), y_true_valid['ConfirmedCases'],label='Confirmed Cases (valid)', c='b')\n        ax2.plot(np.array(valid_data['Date'], dtype='datetime64[D]'),y_pred_valid['ConfirmedCases'],label='Cumulative modeled infections (valid)', c='y')\n        plt.title('Real ConfirmedCase and Predict ConfirmedCase')\n        plt.legend(loc='best');\n        plt.show()\n            \n        ####### Forecast 250 days after by using the best paramater of train data #######\n        if len(res.x)>2:\n            msle, sol = eval_model_decay(res.x, train_data, population, True, 250)\n        else:\n            msle, sol = eval_model_const(res.x, train_data, population, True, 250)\n        \n        sus, exp, inf, rec = sol.y\n        \n        y_pred = pd.DataFrame({\n            'ConfirmedCases': cumsum_signal(np.diff((inf + rec) * population, prepend=n_infected).cumsum()),\n            'Fatalities': cumsum_signal(np.clip(rec, 0, np.inf) * population * res.x[1])\n        })\n        \n        ####### Plot 250 days after of each country #######\n        start = train_min\n        end = start + pd.Timedelta(days=len(y_pred))\n        time_array = np.arange(start, end, dtype='datetime64[D]')\n\n        max_day = numpy.where(inf == numpy.amax(inf))[0][0]\n        where_time = time_array[max_day]\n        pred_max_day = y_pred['ConfirmedCases'][max_day]\n        xy_show_max_estimation = (where_time, max_day)\n        \n        con = y_pred['ConfirmedCases']\n        max_day_con = numpy.where(con == numpy.amax(con))[0][0] # Find the max confimed case of each country\n        max_con = numpy.amax(con)\n        where_time_con = time_array[len(time_array)-50]\n        xy_show_max_estimation_confirmed = (where_time_con, max_con)\n        \n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=time_array, y=y_pred['ConfirmedCases'].astype(int),\n                            mode='lines',\n                            line = dict(color='red'),\n                            name='Estimation Confirmed Case Start from '+ str(start.date())+ ' to ' +str(end.date())))\n        fig.add_trace(go.Scatter(x=time_array[:len(train)], y=train['ConfirmedCases'],\n                            mode='lines',\n                            name='Confirmed case until '+ str(train_max.date()),line = dict(color='green', width=4)))\n        fig.add_annotation(\n            x=where_time_con,\n            y=max_con-(max_con\/30),\n            showarrow=False,\n            text=\"Estimate Max Case around:\" +str(int(max_con)),\n            font=dict(\n                color=\"Blue\",\n                size=15\n            ))\n        fig.add_annotation(\n            x=time_array[len(train)-1],\n            y=train['ConfirmedCases'].tolist()[-1],\n            showarrow=True,\n            text=f\"Real Max ConfirmedCase: \" +str(int(train['ConfirmedCases'].tolist()[-1]))) \n        \n        fig.add_annotation(\n            x=where_time,\n            y=pred_max_day,\n            text='Infect start decrease from: ' + str(where_time))   \n        fig.update_layout(title='Estimate Confirmed Case ,'+area_name+' Total population ='+ str(int(population)), legend_orientation=\"h\")\n        fig.show()\n        \n        #df = pd.DataFrame({'Values': train_data['ConfirmedCases'].tolist()+y_pred['ConfirmedCases'].tolist(),'Date_datatime':time_array[:len(train_data)].tolist()+time_array.tolist(),\n        #           'Real\/Predict': ['ConfirmedCase' for i in range(len(train_data))]+['PredictCase' for i in range(len(y_pred))]})\n        #fig = px.line(df, x=\"Date_datatime\", y=\"Values\",color = 'Real\/Predict')\n        #fig.show()\n        #plt.figure(figsize = (16,7))\n        #plt.plot(time_array[:len(train_data)],train_data['ConfirmedCases'],label='Confirmed case until '+ str(train_max.date()),color='g', linewidth=3.0)\n        #plt.plot(time_array,y_pred['ConfirmedCases'],label='Estimation Confirmed Case Start from '+ str(start.date())+ ' to ' +str(end.date()),color='r', linewidth=1.0)\n        #plt.annotate('Infect start decrease from: ' + str(where_time), xy=xy_show_max_estimation, size=15, color=\"black\")\n        #plt.annotate('max Confirmedcase: ' + str(int(max_con)), xy=xy_show_max_estimation_confirmed, size=15, color=\"black\")\n        #plt.title('Estimate Confirmed Case '+area_name+' Total population ='+ str(int(population)))\n        #plt.legend(loc='lower right')\n        #plt.show()\n\n\n    return y_pred_test, valid_msle","b5e0b514":"country = 'Vietnam'\nif country not in train['Country_Region'].unique():\n    country_pd_train = train[train['Province_State']==country]\nelse:\n    country_pd_train = train[train['Country_Region']==country]\n\na,b = fit_model_new(country_pd_train,country,make_plot=True)","9ab73d7b":"country = 'US'\ncountry_pd_train = train[train['Country_Region']==country]\ncountry_pd_train2 = country_pd_train.groupby(['Date']).sum().reset_index()\ncountry_pd_train2['Date'] = pd.to_datetime(country_pd_train2['Date'], format='%Y-%m-%d')\na,b = fit_model_new(country_pd_train2,country,make_plot=True)\n","f39ed1c1":"country = 'California'\nif country not in train['Country_Region'].unique():\n    country_pd_train = train[train['Province_State']==country]\nelse:\n    country_pd_train = train[train['Country_Region']==country]\n\na,b = fit_model_new(country_pd_train,country,make_plot=True)","dfb35c60":"country = 'New York'\nif country not in train['Country_Region'].unique():\n    country_pd_train = train[train['Province_State']==country]\nelse:\n    country_pd_train = train[train['Country_Region']==country]\n\na,b = fit_model_new(country_pd_train,country,make_plot=True)","989b0344":"country = 'Italy'\nif country not in train['Country_Region'].unique():\n    country_pd_train = train[train['Province_State']==country]\nelse:\n    country_pd_train = train[train['Country_Region']==country]\n\na,b = fit_model_new(country_pd_train,country,make_plot=True)","b61bc0cf":"country = 'Spain'\nif country not in train['Country_Region'].unique():\n    country_pd_train = train[train['Province_State']==country]\nelse:\n    country_pd_train = train[train['Country_Region']==country]\n\na,b = fit_model_new(country_pd_train,country,make_plot=True)","b1f1c038":"country = 'Germany'\nif country not in train['Country_Region'].unique():\n    country_pd_train = train[train['Province_State']==country]\nelse:\n    country_pd_train = train[train['Country_Region']==country]\n\na,b = fit_model_new(country_pd_train,country,make_plot=True)","8f326fe4":"validation_scores = []\nvalidation_county = []\nvalidation_country = []\n\ntest_seir = test.copy()\n\nfor country in tqdm(train['Country_Region'].unique()):\n    country_pd_train = train[train['Country_Region']==country]\n    #if country_pd_train['Province_State'].isna().unique()==True:\n    if len(country_pd_train['Province_State'].unique())<2:\n        predict_test, score = fit_model_new(country_pd_train,country,make_plot=False)\n        if score ==0:\n            print(f'{country} no case')\n        validation_scores.append(score)\n        validation_county.append(country)\n        validation_country.append(country)\n        test_seir.loc[test_seir['Country_Region']==country,'ConfirmedCases'] = predict_test['ConfirmedCases'].tolist()\n        test_seir.loc[test_seir['Country_Region']==country,'Fatalities'] = predict_test['Fatalities'].tolist()\n    else:\n        for state in country_pd_train['Province_State'].unique():\n            if state != state: # check nan\n                state_pd = country_pd_train[country_pd_train['Province_State'].isna()]\n                predict_test, score = fit_model_new(state_pd,state,make_plot=False)\n                if score ==0:\n                    print(f'{country} \/ {state} no case')\n                validation_scores.append(score)\n                validation_county.append(state)\n                validation_country.append(country)\n                test_seir.loc[(test_seir['Country_Region']==country)&(test_seir['Province_State'].isna()),'ConfirmedCases'] = predict_test['ConfirmedCases'].tolist()\n                test_seir.loc[(test_seir['Country_Region']==country)&(test_seir['Province_State'].isna()),'Fatalities'] = predict_test['Fatalities'].tolist()\n            else:\n                state_pd = country_pd_train[country_pd_train['Province_State']==state]\n                predict_test, score = fit_model_new(state_pd,state,make_plot=False)\n                if score ==0:\n                    print(f'{country} \/ {state} no case')\n                validation_scores.append(score)\n                validation_county.append(state)\n                validation_country.append(country)\n                test_seir.loc[(test_seir['Country_Region']==country)&(test_seir['Province_State']==state),'ConfirmedCases'] = predict_test['ConfirmedCases'].tolist()\n                test_seir.loc[(test_seir['Country_Region']==country)&(test_seir['Province_State']==state),'Fatalities'] = predict_test['Fatalities'].tolist()\n         #   print(f'{country} {state} {score:0.5f}')\n            \nprint(f'Mean validation score: {np.average(validation_scores):0.5f}')","6656bc29":"validation_scores = pd.DataFrame({'country\/state':validation_country,'country':validation_county,'MSLE':validation_scores})\nvalidation_scores.sort_values(by=['MSLE'], ascending=False).head(20)","74b4781c":"large_msle = validation_scores[validation_scores['MSLE']>1]","e350f550":"from sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfor country in large_msle['country'].unique():\n    if (country!= country)==False: # check None\n        #print ('training model for country ==>'+country)\n        country_pd_train = train[train['Country_Region']==country]\n        country_pd_test = test[test['Country_Region']==country]\n        if len(country_pd_train)==0:\n            country_pd_train = train[train['Province_State']==country]\n            country_pd_test = test[test['Province_State']==country]\n\n            x = np.array(range(len(country_pd_train))).reshape((-1,1))[:-7]\n            valid_x = np.array(range(len(country_pd_train))).reshape((-1,1))[-7:]\n            y = country_pd_train['ConfirmedCases'][:-7]\n            valid_y = country_pd_train['ConfirmedCases'][-7:]\n            y_fat = country_pd_train['Fatalities'][:-7]\n            valid_y_fat = country_pd_train['Fatalities'][-7:]\n            \n            model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                             ('linear', LinearRegression(fit_intercept=False))])\n            model = model.fit(x, y)\n\n            model_fat = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                             ('linear', LinearRegression(fit_intercept=False))])\n            model_fat = model_fat.fit(x, y_fat)\n            \n            predict_y = model.predict(valid_x)\n            predict_yfat = model_fat.predict(valid_x)\n            score = mean_squared_log_error(np.clip(valid_y,0,np.inf), np.clip(predict_y,0,np.inf))\n            score_fat = mean_squared_log_error(np.clip(valid_y_fat,0,np.inf), np.clip(predict_yfat,0,np.inf))\n            score = (score+score_fat)\/2\n\n            print(f'{country} {score:0.5f}')\n            if score < large_msle[large_msle['country']==country]['MSLE'].tolist()[0]:\n                validation_scores.loc[validation_scores['country']==country,'MSLE'] = score\n                predict_x = (np.array(range(len(country_pd_test)))+50).reshape((-1,1))\n                test_seir.loc[test_seir['Province_State']==country,'ConfirmedCases'] = model.predict(predict_x)\n                test_seir.loc[test_seir['Province_State']==country,'Fatalities'] = model_fat.predict(predict_x)\n        else:\n            x = np.array(range(len(country_pd_train))).reshape((-1,1))[:-7]\n            valid_x = np.array(range(len(country_pd_train))).reshape((-1,1))[-7:]\n            y = country_pd_train['ConfirmedCases'][:-7]\n            valid_y = country_pd_train['ConfirmedCases'][-7:]\n            y_fat = country_pd_train['Fatalities'][:-7]\n            valid_y_fat = country_pd_train['Fatalities'][-7:]\n            \n            model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                             ('linear', LinearRegression(fit_intercept=False))])\n            model = model.fit(x, y)\n\n            model_fat = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                             ('linear', LinearRegression(fit_intercept=False))])\n            model_fat = model_fat.fit(x, y_fat)\n            \n            predict_y = model.predict(valid_x)\n            predict_yfat = model_fat.predict(valid_x)\n            score = mean_squared_log_error(np.clip(valid_y,0,np.inf), np.clip(predict_y,0,np.inf))\n            score_fat = mean_squared_log_error(np.clip(valid_y_fat,0,np.inf), np.clip(predict_yfat,0,np.inf))\n            score = (score+score_fat)\/2\n\n            print(f'{country} {score:0.5f}')\n            if score < large_msle[large_msle['country']==country]['MSLE'].tolist()[0]:\n                validation_scores.loc[validation_scores['country']==country,'MSLE'] = score\n                predict_x = (np.array(range(len(country_pd_test)))+50).reshape((-1,1))\n                test_seir.loc[test_seir['Country_Region']==country,'ConfirmedCases'] = model.predict(predict_x)\n                test_seir.loc[test_seir['Country_Region']==country,'Fatalities'] = model_fat.predict(predict_x)\n                ","2fcae75b":"val_soces = validation_scores['MSLE'].tolist()\nprint(f'Mean validation score: {np.average(val_soces):0.5f}')","7fe08dd0":"\nsubmit['Fatalities'] = round(test_seir['Fatalities'].astype('float'),0)\nsubmit['ConfirmedCases'] = round(test_seir['ConfirmedCases'].astype('float'),0)\nsubmit.tail()\n","d87e2032":"submit.to_csv('submission.csv',index=False)","c8ae6c93":"train_rfm = train.copy()\n\n# Add additional variables\n#month\ntrain_rfm['month'] = train_rfm['Date'].dt.month\n\n#date\ntrain_rfm['dates'] = train_rfm['Date'].dt.day\n\n\n### do the same for test data\ntest_rfm = test.copy()\n\n# Add additional variables\n\n#month\ntest_rfm['month'] = test_rfm['Date'].dt.month\n\n#date\ntest_rfm['dates'] = test_rfm['Date'].dt.day\n\n\ntrain_rfm.tail()\n","ddc1e36f":"countries_array = train['Country_Region'].unique()\n\ntrain_first_result = pd.DataFrame()\n\nfor i in countries_array:\n    # get relevant data \n    day_first_outbreak = train_rfm.loc[train_rfm['Country_Region']==i]\n    \n    date_outbreak = day_first_outbreak.loc[day_first_outbreak['ConfirmedCases']>0]['Date'].min()\n    \n    #Calculate days since first outbreak happened\n    day_first_outbreak['days_since_first_outbreak'] = (day_first_outbreak['Date'] - date_outbreak).astype('timedelta64[D]')\n\n    \n    #impute the negative days with 0\n    day_first_outbreak['days_since_first_outbreak'][day_first_outbreak['days_since_first_outbreak']<0] = 0 \n   \n    train_first_result = train_first_result.append(day_first_outbreak,ignore_index=True)\n\n\n### do the same for test data\n\ntest_first_result = pd.DataFrame()\n\nfor i in countries_array:\n    # get relevant data \n    day_first_outbreak = test_rfm.loc[test_rfm['Country_Region']==i]\n    \n    day_first_outbreak_train = train_rfm.loc[train_rfm['Country_Region']==i]\n    \n    date_outbreak = day_first_outbreak_train.loc[day_first_outbreak_train['ConfirmedCases']>0]['Date'].min()\n    \n    #Calculate days since first outbreak happened\n    day_first_outbreak['days_since_first_outbreak'] = (day_first_outbreak['Date'] - date_outbreak).astype('timedelta64[D]')\n\n    \n    #impute the negative days with 0\n    day_first_outbreak['days_since_first_outbreak'][day_first_outbreak['days_since_first_outbreak']<0] = 0 \n   \n    test_first_result = test_first_result.append(day_first_outbreak,ignore_index=True)\n\n\ntest_first_result.tail()\n","d9638983":"#ecoding countries data\n\n#train\nlabels, values = pd.factorize(train_first_result['Country_Region'])\n\ntrain_first_result['country_id'] = labels\n\ntrain_first_result.head()\n\n#test\n\nlabels, values = pd.factorize(test_first_result['Country_Region'])\n\ntest_first_result['country_id'] = labels\n\ntest_first_result.head()","6711e854":"random.seed(123)\n\nX = train_first_result[['country_id','month','dates','days_since_first_outbreak']]\n\ny_confirm = train_first_result['ConfirmedCases']\ny_fatal = train_first_result['Fatalities']\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_confirm, test_size=0.3, random_state=0)\n\n\n#RF model\nrf = RandomForestClassifier()\n\npredict_labels = X.columns\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n\n# Print the name and gini importance of each feature\nfor feature in zip(predict_labels, rf.feature_importances_):\n    print(feature)\n    ","8cfb5eb1":"sns.set(rc={'figure.figsize':(15, 7)})\n\nplt.figure()\n\nplt.title(\"Feature importances\",fontsize=20)\n\nplt.bar(predict_labels,rf.feature_importances_, align=\"center\", color='dodgerblue')\n\nplt.xticks(predict_labels)\n\nplt.xticks(rotation=90)\n\nplt.show()","a49e3fdd":"\n## predict on test set\n\nconfirm_case = rf.predict(test_first_result[['country_id','month','dates','days_since_first_outbreak']])\n","5cbd53ed":"\nrandom.seed(123)\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_fatal, test_size=0.3, random_state=0)\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n\n## predict on test set\n\nfatal_case = rf.predict(test_first_result[['country_id','month','dates','days_since_first_outbreak']])\n","9c2100d4":"\nforecaseId = pd.DataFrame(test[['ForecastId']])\nconfirm_case = pd.DataFrame(confirm_case)\nfatal_case = pd.DataFrame(fatal_case)\n\n\nfinal_result_rf = pd.concat([forecaseId,confirm_case,fatal_case],axis=1)\nfinal_result_rf.columns = ['ForecastId','ConfirmedCases','Fatalities']\n\nfinal_result_rf.tail()","de25c399":"final_result_rf.to_csv('results_rfm.csv', index = False)","cb200e7a":"# function that apply Random Forest for each country in the data\n\ndef rf_each_country(seed, original_df, train_df, test_df, confirm , \n                    fatal, xlabels, test_size = 0.3):\n\n    random.seed(seed)\n\n    countries_array = original_df['Country_Region'].unique()\n\n    final_result = pd.DataFrame()\n    \n    confirm_test_case = pd.DataFrame()\n    fatal_test_case = pd.DataFrame()\n    \n    confirm_val = pd.DataFrame()\n    fatal_val = pd.DataFrame()\n\n    ##predict confirmed cases\n    for i in countries_array:\n        # get relevant data \n        train_set = train_df.loc[train_df['Country_Region']==i]\n        test_set = test_df.loc[test_df['Country_Region']==i]\n\n\n        #Confirm case\n        X1 = train_set[xlabels]\n        y1 = train_set[confirm]\n\n        #train test split\n        X_train_rf1, X_test_rf1, y_train_rf1, y_test_rf1 = train_test_split(X1, y1, test_size=0.3, random_state=0)\n\n        #RF model\n        rf1 = RandomForestClassifier()\n\n        # Train the classifier\n        rf1.fit(X_train_rf1, y_train_rf1)\n        \n        test_confirm = rf1.predict(X_test_rf1)\n\n        ## predict on test set\n\n        confirm_case = rf1.predict(test_set[xlabels])\n\n\n        #Fatal case\n        X2 = train_set[xlabels]\n        y2 = train_set[fatal]\n\n        #train test split\n        X_train_rf2, X_test_rf2, y_train_rf2, y_test_rf2 = train_test_split(X2, y2, test_size=0.3, random_state=0)\n\n        #RF model\n        rf2 = RandomForestClassifier()\n\n        # Train the classifier\n        rf2.fit(X_train_rf2, y_train_rf2)\n        \n        test_fatal = rf2.predict(X_test_rf2)\n\n        ## predict on test set\n\n        fatal_case = rf2.predict(test_set[xlabels])\n        \n        \n        ## combine them together and meausre RMSE\n        test_confirm = pd.DataFrame(test_confirm)\n        test_fatal = pd.DataFrame(test_fatal)\n        \n        y_test_rf1 = pd.DataFrame(y_test_rf1)\n        y_test_rf2 = pd.DataFrame(y_test_rf2)\n        \n        \n        confirm_test_case = confirm_test_case.append(test_confirm)\n        fatal_test_case = fatal_test_case.append(test_fatal)\n        \n        confirm_val = confirm_val.append(y_test_rf1)\n        fatal_val = fatal_val.append(y_test_rf2)\n        \n        \n        ### Combine results\n        \n        confirm_case = pd.DataFrame(confirm_case)\n        fatal_case = pd.DataFrame(fatal_case)\n\n        final_result_pred = pd.concat([confirm_case,fatal_case],axis=1)\n\n        final_result_pred.columns = ['ConfirmedCases','Fatalities']\n        \n        final_result = final_result.append(final_result_pred, ignore_index=True)\n    \n    \n    ##Print out validation metrics\n    \n    confirm_test_case = np.array(confirm_test_case)\n    fatal_test_case = np.array(fatal_test_case)\n    \n    \n    confirm_va1 = np.array(confirm_val)\n    fatal_val1 = np.array(fatal_val)\n    \n\n    print('Mean Absolute Error for Confirmed Case Prediction:', round(metrics.mean_absolute_error(confirm_va1, confirm_test_case),2))  \n    print('Mean Squared Error for Confirmed Case Prediction:', round(metrics.mean_squared_error(confirm_va1, confirm_test_case),2))  \n    print('Root Mean Squared Error for Confirmed Case Prediction:', round(np.sqrt(metrics.mean_squared_error(confirm_va1, confirm_test_case)),2), \"\\n\")\n\n\n    print('Mean Absolute Error for Fatal Case Prediction:', round(metrics.mean_absolute_error(fatal_val1, fatal_test_case),2))  \n    print('Mean Squared Error for Fatal Case Prediction:', round(metrics.mean_squared_error(fatal_val1, fatal_test_case),2))  \n    print('Root Mean Squared Error for Fatal Case Prediction:', round(np.sqrt(metrics.mean_squared_error(fatal_val1, fatal_test_case)),2), \"\\n\")\n\n    \n    #compile with prediction IDs\n    \n    forecaseId = pd.DataFrame(test[['ForecastId']])\n\n    final_result = pd.concat([forecaseId,final_result],axis=1)\n\n    return final_result\n","04181ef7":"\nx_pred_lab = ['month','dates','days_since_first_outbreak']\n\nfinal_result_rf_each = rf_each_country(seed=123, original_df=train, train_df=train_first_result, \n                                       test_df=test_first_result, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n\n","6cb74bc7":"final_result_rf_each.tail()","4cf36ec2":"final_result_rf_each.to_csv('result_rf_each.csv',index=False)","de8696e1":"#construct the OLS model\nX = train_first_result[['country_id','month','dates','days_since_first_outbreak']]\ny_confirm = train_first_result['ConfirmedCases']\ny_fatal = train_first_result['Fatalities']\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train, X_test, y_train, y_test = train_test_split(X, y_confirm, test_size=0.3, random_state=0)\n\nX_train  = np.array(X_train)\nX_test = np.array(X_test)\ny_train  = np.array(y_train)\ny_test  = np.array(y_test)\n\n# Note the difference in argument order\nmodel = LinearRegression()\n\nmodel.fit(X_train,y_train)\n\npredictions = model.predict(X_test) # make the predictions by the model\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test, predictions),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test, predictions),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, predictions)),2))\n    ","a7c3be5b":"\nconfirm_case = model.predict(\n    test_first_result[['country_id','month','dates','days_since_first_outbreak']]) # make the predictions by the model\n","2d85dba2":"#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_fatal, test_size=0.3, random_state=0)\n\nX_train  = np.array(X_train)\nX_test = np.array(X_test)\ny_train  = np.array(y_train)\ny_test  = np.array(y_test)\n\n# Note the difference in argument order\nmodel = LinearRegression()\n\nmodel.fit(X_train,y_train)\n\npredictions = model.predict(X_test) # make the predictions by the model\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test, predictions),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test, predictions),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, predictions)),2))\n   ","911e498a":"\nfatal_case = model.predict(\n    test_first_result[['country_id','month','dates','days_since_first_outbreak']]) # make the predictions by the model\n","43024674":"forecaseId = pd.DataFrame(test[['ForecastId']])\nconfirm_case = round(pd.DataFrame(confirm_case),0)\nfatal_case = round(pd.DataFrame(fatal_case),0)\n\n\nfinal_result_lin = pd.concat([forecaseId,confirm_case,fatal_case],axis=1)\nfinal_result_lin.columns = ['ForecastId','ConfirmedCases','Fatalities']\n\nfinal_result_lin.tail()","996bb4d9":"final_result_lin.to_csv('results_lin.csv',index=False)","2ca45ce4":"# function that apply Linear Regression for each country in the data\n\ndef lin_each_country(seed, original_df, train_df, test_df, confirm , \n                    fatal, xlabels, test_size = 0.3):\n\n    random.seed(seed)\n\n    countries_array = original_df['Country_Region'].unique()\n\n    final_result = pd.DataFrame()\n    \n    confirm_test_case = pd.DataFrame()\n    fatal_test_case = pd.DataFrame()\n    \n    confirm_val = pd.DataFrame()\n    fatal_val = pd.DataFrame()\n\n    ##predict confirmed cases\n    for i in countries_array:\n        # get relevant data \n        train_set = train_df.loc[train_df['Country_Region']==i]\n        test_set = test_df.loc[test_df['Country_Region']==i]\n\n\n        #Confirm case\n        X1 = train_set[xlabels]\n        y1 = train_set[confirm]\n\n        #train test split\n        X_train_rf1, X_test_rf1, y_train_rf1, y_test_rf1 = train_test_split(X1, y1, test_size=0.3, random_state=0)\n\n        X_train_rf1  = np.array(X_train_rf1)\n        X_test_rf1 = np.array(X_test_rf1)\n        y_train_rf1  = np.array(y_train_rf1)\n        y_test_rf1  = np.array(y_test_rf1)\n\n        # Note the difference in argument order\n        model = LinearRegression()\n\n        model.fit(X_train_rf1,y_train_rf1)\n\n        test_confirm = model.predict(X_test_rf1) # make the predictions by the model\n\n        \n        ## predict on test set\n\n        confirm_case = model.predict(test_set[xlabels])\n\n\n        #Fatal case\n        X2 = train_set[xlabels]\n        y2 = train_set[fatal]\n\n        #train test split\n        X_train_rf2, X_test_rf2, y_train_rf2, y_test_rf2 = train_test_split(X2, y2, test_size=0.3, random_state=0)\n\n        X_train_rf2  = np.array(X_train_rf2)\n        X_test_rf2 = np.array(X_test_rf2)\n        y_train_rf2  = np.array(y_train_rf2)\n        y_test_rf2  = np.array(y_test_rf2)\n\n        # Note the difference in argument order\n        model = LinearRegression()\n\n        model.fit(X_train_rf2,y_train_rf2)\n\n        test_fatal = model.predict(X_test_rf2) # make the predictions by the model\n\n        \n        ## predict on test set\n\n        fatal_case = model.predict(test_set[xlabels])\n        \n        \n        ## combine them together and meausre RMSE\n        test_confirm = pd.DataFrame(test_confirm)\n        test_fatal = pd.DataFrame(test_fatal)\n        \n        y_test_rf1 = pd.DataFrame(y_test_rf1)\n        y_test_rf2 = pd.DataFrame(y_test_rf2)\n        \n        \n        confirm_test_case = confirm_test_case.append(test_confirm)\n        fatal_test_case = fatal_test_case.append(test_fatal)\n        \n        confirm_val = confirm_val.append(y_test_rf1)\n        fatal_val = fatal_val.append(y_test_rf2)\n        \n        \n        ### Combine results\n        \n        confirm_case = round(pd.DataFrame(confirm_case),0)\n        fatal_case = round(pd.DataFrame(fatal_case),0)\n\n        final_result_pred = pd.concat([confirm_case,fatal_case],axis=1)\n\n        final_result_pred.columns = ['ConfirmedCases','Fatalities']\n        \n        final_result = final_result.append(final_result_pred, ignore_index=True)\n    \n    \n    ##Print out validation metrics\n    \n    confirm_test_case = np.array(confirm_test_case)\n    fatal_test_case = np.array(fatal_test_case)\n    \n    \n    confirm_va1 = np.array(confirm_val)\n    fatal_val1 = np.array(fatal_val)\n    \n\n    print('Mean Absolute Error for Confirmed Case Prediction:', round(metrics.mean_absolute_error(confirm_va1, confirm_test_case),2))  \n    print('Mean Squared Error for Confirmed Case Prediction:', round(metrics.mean_squared_error(confirm_va1, confirm_test_case),2))  \n    print('Root Mean Squared Error for Confirmed Case Prediction:', round(np.sqrt(metrics.mean_squared_error(confirm_va1, confirm_test_case)),2), \"\\n\")\n\n\n    print('Mean Absolute Error for Fatal Case Prediction:', round(metrics.mean_absolute_error(fatal_val1, fatal_test_case),2))  \n    print('Mean Squared Error for Fatal Case Prediction:', round(metrics.mean_squared_error(fatal_val1, fatal_test_case),2))  \n    print('Root Mean Squared Error for Fatal Case Prediction:', round(np.sqrt(metrics.mean_squared_error(fatal_val1, fatal_test_case)),2), \"\\n\")\n\n    \n    #compile with prediction IDs\n    \n    forecaseId = pd.DataFrame(test[['ForecastId']])\n\n    final_result = pd.concat([forecaseId,final_result],axis=1)\n\n    return final_result\n","e7861868":"x_pred_lab = ['month','dates','days_since_first_outbreak']\n\nfinal_result_lin_each = lin_each_country(seed=123, original_df=train, train_df=train_first_result, \n                                       test_df=test_first_result, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n\n\n","5b91bb19":"final_result_lin_each.tail()","b46e692a":"final_result_lin_each.to_csv(\"result_lin_each.csv\",index=False)","227324aa":"\nX = train_first_result[['country_id','month','dates','days_since_first_outbreak']]\ny_confirm = train_first_result['ConfirmedCases']\ny_fatal = train_first_result['Fatalities']\n\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_br, X_test_br, y_train_br, y_test_br = train_test_split(X, y_confirm, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\nclf = BayesianRidge(compute_score=True)\nclf.fit(X_train_br, y_train_br)\n\ny_pred_br = clf.predict(X_test_br)\n\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_br, y_pred_br),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_br, y_pred_br),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_br, y_pred_br)),2))\n","a6e03e59":"## predict on test set\n\nconfirm_case = clf.predict(test_first_result[['country_id','month','dates','days_since_first_outbreak']])\n\n","ae5a368a":"\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_br, X_test_br, y_train_br, y_test_br = train_test_split(X, y_fatal, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\nclf = BayesianRidge(compute_score=True)\nclf.fit(X_train_br, y_train_br)\n\ny_pred_br = clf.predict(X_test_br)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_br, y_pred_br),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_br, y_pred_br),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_br, y_pred_br)),2))\n\n","117e2bb1":"## predict on test set\n\nfatal_case = clf.predict(test_first_result[['country_id','month','dates','days_since_first_outbreak']])\n\n","06a2d85e":"forecaseId = pd.DataFrame(test[['ForecastId']])\nconfirm_case = round(pd.DataFrame(confirm_case),0)\nfatal_case = round(pd.DataFrame(fatal_case),0)\n\n\nfinal_result_br = pd.concat([forecaseId,confirm_case,fatal_case],axis=1)\nfinal_result_br.columns = ['ForecastId','ConfirmedCases','Fatalities']\n\nfinal_result_br.tail()\n","5dcb08fe":"final_result_br.to_csv('results_br.csv', index = False)","0b0f8c2b":"# function that apply Bayesian Regression for each country in the data\n\ndef br_each_country(seed, original_df, train_df, test_df, confirm , \n                    fatal, xlabels, test_size = 0.3):\n\n    random.seed(seed)\n\n    countries_array = original_df['Country_Region'].unique()\n\n    final_result = pd.DataFrame()\n    \n    confirm_test_case = pd.DataFrame()\n    fatal_test_case = pd.DataFrame()\n    \n    confirm_val = pd.DataFrame()\n    fatal_val = pd.DataFrame()\n\n    ##predict confirmed cases\n    for i in countries_array:\n        # get relevant data \n        train_set = train_df.loc[train_df['Country_Region']==i]\n        test_set = test_df.loc[test_df['Country_Region']==i]\n\n\n        #Confirm case\n        X1 = train_set[xlabels]\n        y1 = train_set[confirm]\n\n        #train test split\n        X_train_rf1, X_test_rf1, y_train_rf1, y_test_rf1 = train_test_split(X1, y1, test_size=0.3, random_state=0)\n\n        #Bayesian Ridge Model\n        clf = BayesianRidge(compute_score=True)\n        \n        ##train\n        clf.fit(X_train_rf1, y_train_rf1)\n\n        test_confirm = clf.predict(X_test_rf1)\n  \n        ## predict on test set\n\n        confirm_case = clf.predict(test_set[xlabels])\n\n\n        #Fatal case\n        X2 = train_set[xlabels]\n        y2 = train_set[fatal]\n\n        #train test split\n        X_train_rf2, X_test_rf2, y_train_rf2, y_test_rf2 = train_test_split(X2, y2, test_size=0.3, random_state=0)\n        \n        #Bayesian Ridge Model\n        clf = BayesianRidge(compute_score=True)\n        \n        ##train\n        clf.fit(X_train_rf2, y_train_rf2)\n\n        test_fatal = clf.predict(X_test_rf2)\n\n        ## predict on test set\n\n        fatal_case = clf.predict(test_set[xlabels])\n        \n        \n        ## combine them together and meausre RMSE\n        test_confirm = pd.DataFrame(test_confirm)\n        test_fatal = pd.DataFrame(test_fatal)\n        \n        y_test_rf1 = pd.DataFrame(y_test_rf1)\n        y_test_rf2 = pd.DataFrame(y_test_rf2)\n        \n        \n        confirm_test_case = confirm_test_case.append(test_confirm)\n        fatal_test_case = fatal_test_case.append(test_fatal)\n        \n        confirm_val = confirm_val.append(y_test_rf1)\n        fatal_val = fatal_val.append(y_test_rf2)\n        \n        \n        ### Combine results\n        \n        confirm_case = round(pd.DataFrame(confirm_case),0)\n        fatal_case = round(pd.DataFrame(fatal_case),0)\n\n        final_result_pred = pd.concat([confirm_case,fatal_case],axis=1)\n\n        final_result_pred.columns = ['ConfirmedCases','Fatalities']\n        \n        final_result = final_result.append(final_result_pred, ignore_index=True)\n    \n    \n    ##Print out validation metrics\n    \n    confirm_test_case = np.array(confirm_test_case)\n    fatal_test_case = np.array(fatal_test_case)\n    \n    \n    confirm_va1 = np.array(confirm_val)\n    fatal_val1 = np.array(fatal_val)\n    \n\n    print('Mean Absolute Error for Confirmed Case Prediction:', round(metrics.mean_absolute_error(confirm_va1, confirm_test_case),2))  \n    print('Mean Squared Error for Confirmed Case Prediction:', round(metrics.mean_squared_error(confirm_va1, confirm_test_case),2))  \n    print('Root Mean Squared Error for Confirmed Case Prediction:', round(np.sqrt(metrics.mean_squared_error(confirm_va1, confirm_test_case)),2), \"\\n\")\n\n\n    print('Mean Absolute Error for Fatal Case Prediction:', round(metrics.mean_absolute_error(fatal_val1, fatal_test_case),2))  \n    print('Mean Squared Error for Fatal Case Prediction:', round(metrics.mean_squared_error(fatal_val1, fatal_test_case),2))  \n    print('Root Mean Squared Error for Fatal Case Prediction:', round(np.sqrt(metrics.mean_squared_error(fatal_val1, fatal_test_case)),2), \"\\n\")\n\n    \n    #compile with prediction IDs\n    \n    forecaseId = pd.DataFrame(test[['ForecastId']])\n\n    final_result = pd.concat([forecaseId,final_result],axis=1)\n\n    return final_result\n","402e8dbb":"x_pred_lab = ['month','dates','days_since_first_outbreak']\n\nfinal_result_br_each = br_each_country(seed=123, original_df=train, train_df=train_first_result, \n                                       test_df=test_first_result, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n\n\n","18f98a0e":"final_result_br_each.tail()","feb305b3":"final_result_br_each.to_csv(\"result_br_each.csv\",index=False)","9708dd7d":"weather_train = weather_data.copy()\n\n# Add additional variables\n#month\nweather_train['month'] = weather_train['Date'].dt.month\n\n#date\nweather_train['dates'] = weather_train['Date'].dt.day\n\n\n### do the same for test data\nweather_test1 = weather_test.copy()\n\n# Add additional variables\n\n#month\nweather_test1['month'] = weather_test1['Date'].dt.month\n\n#date\nweather_test1['dates'] = weather_test1['Date'].dt.day\n\n\nweather_test1.tail()","7e8202b9":"#ecoding countries data\n\n#train\nlabels, values = pd.factorize(weather_train['Country_Region'])\n\nweather_train['country_id'] = labels\n\nweather_train.head()\n\n#test\n\nlabels, values = pd.factorize(weather_test1['Country_Region'])\n\nweather_test1['country_id'] = labels\n\nweather_test1.head()","4940b632":"random.seed(123)\n\nX = weather_train[['country_id','Lat', 'Long','day_from_jan_first', 'temp', 'stp', 'wdsp', 'prcp','fog', 'month', 'dates']]\n\ny_confirm = weather_train['ConfirmedCases']\n\ny_fatal = weather_train['Fatalities']\n\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_confirm, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n\n\n# Print the name and gini importance of each feature\nfor feature in zip(predict_labels, rf.feature_importances_):\n    print(feature)\n    ","5112d306":"plt.figure()\n\nplt.title(\"Feature importances\",fontsize=20)\n\nplt.bar(predict_labels,rf.feature_importances_, align=\"center\", color='dodgerblue')\n\nplt.xticks(predict_labels)\n\nplt.xticks(rotation=90)\n\nplt.show()","14864997":"\n## predict on test set\n\nconfirm_case = rf.predict(weather_test1[['country_id','Lat', 'Long','day_from_jan_first', 'temp',\n                                         'stp', 'wdsp', 'prcp','fog','month', 'dates']])\n","ea650dfd":"random.seed(123)\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_fatal, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n\n## predict on test set\n\nfatal_case = rf.predict(weather_test1[['country_id','Lat', 'Long','day_from_jan_first', 'temp',\n                                         'stp', 'wdsp', 'prcp','fog', 'month', 'dates']])\n","68b44b3c":"\nforecaseId = pd.DataFrame(test[['ForecastId']])\nconfirm_case = pd.DataFrame(confirm_case)\nfatal_case = pd.DataFrame(fatal_case)\n\n\nfinal_result_rf = pd.concat([forecaseId,confirm_case,fatal_case],axis=1)\nfinal_result_rf.columns = ['ForecastId','ConfirmedCases','Fatalities']\n\nfinal_result_rf.tail()\n","79b1a2a8":"final_result_rf.to_csv('results_rf_weather.csv', index = False)","434b4dc0":"x_pred_lab = ['Lat', 'Long','day_from_jan_first', 'temp', 'stp', 'wdsp', 'prcp','fog','month', 'dates']\n\nfinal_result_rf_weather_each = rf_each_country(seed=123, original_df=weather_train, train_df=weather_train, test_df=weather_test1, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","c7ae02e6":"final_result_rf_weather_each.tail()","e02deaad":"final_result_rf_weather_each.to_csv('submission.csv',index=False)","88df8760":"#run for each country\nx_pred_lab = ['Lat', 'Long','day_from_jan_first', 'temp', 'stp', 'wdsp', 'prcp','fog','month', 'dates']\n\nfinal_result_lin_weather_each = lin_each_country(seed=123, original_df=weather_train, train_df=weather_train, test_df=weather_test1, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","3dfe0858":"final_result_lin_weather_each.tail()","9b829ebb":"final_result_lin_weather_each.to_csv('final_result_lin_weather_each.csv',index=False)","f89139dc":"#run for each country\nx_pred_lab = ['Lat', 'Long','day_from_jan_first', 'temp', 'stp', 'wdsp', \n                   'prcp','fog', 'month', 'dates']\n\nfinal_result_br_weather_each = br_each_country(seed=123, original_df=weather_train, train_df=weather_train, test_df=weather_test1, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","061f2f8e":"final_result_br_weather_each.tail()","fc0321b4":"final_result_br_weather_each.to_csv('final_result_br_weather_each.csv',index=False)","2bad3d1b":"import math\nfrom math import radians, cos, sin, asin, sqrt","7e06bb4d":"def distance(org_lat,org_lon, dest_lat, dest_lon):\n    \n    r = 3959 # miles\n    # for km, we use r = 6371\n    \n    org_lat, org_lon, dest_lat, dest_lon = map(radians,[org_lat, org_lon, dest_lat, dest_lon])\n    dlon = dest_lon - org_lon \n    dlat = dest_lat - org_lat \n    \n    a = sin(dlat\/2)**2 + cos(org_lat) * cos(dest_lat) * sin(dlon\/2)**2\n    \n    c = 2 * asin(sqrt(a)) \n    \n    return c * r\n","7f589cec":"\ncomplete_distance = complete_data.copy()\n\ncomplete_distance['france_lat'] = 46.2276\ncomplete_distance['france_long'] = 2.2137\ncomplete_distance['us_lat'] = 37.0902\ncomplete_distance['us_long'] = -95.7129\ncomplete_distance['china_lat'] = 30.5928\ncomplete_distance['china_long'] = 114.3055\n\n\ncomplete_distance.head()\n","4f01cf9e":"complete_distance = complete_distance.reset_index()\n\neurope = ['Austria','Italy','Belgium','Latvia','Bulgaria','Lithuania','Croatia','Luxembourg',\n          'Cyprus','Malta','Czechia','Netherlands','Denmark','Poland','Estonia','Portugal',\n          'Finland','Romania','France','Slovakia','Germany','Slovenia','Greece','Spain',\n          'Hungary','Sweden','Ireland','Switzerland','United Kingdom']\n\neurope_dis = complete_distance.loc[complete_distance['Country_Region'].isin(europe)==True]\n\nnorth_america = ['Antigua and Barbuda','Bahamas','Barbados','Belize','Canada','Costa Rica','Cuba','El Salvador',\n                 'Grenada','Guatemala','Hait\u00ed','Honduras','Jamaica','Mexico','Nicaragua','Panama',\n                 'Saint Kitts and Nevis','Saint Lucia','Saint Vincent and the Grenadines','Trinidad and Tobago','US']\n\n\namerica_dis = complete_distance.loc[complete_distance['Country_Region'].isin(north_america)==True]\n\nasia_dis = complete_distance.loc[complete_distance['Country_Region'].isin(europe)==False]\nasia_dis = asia_dis.loc[asia_dis['Country_Region'].isin(north_america)==False]\n","87656712":"#Calculate distance to Europe areas\n\neurope_dis['distance_to_first_outbreak'] = europe_dis.apply(lambda x: distance(x['Lat'],x['Long'],x['france_lat'],\n                                                                 x['france_long']), axis=1)\n\neurope_dis.head()\n","492bfdb9":"#Calculate distance to America areas\n\namerica_dis['distance_to_first_outbreak'] = america_dis.apply(lambda x: distance(x['Lat'],x['Long'],x['us_lat'],\n                                                                 x['us_long']), axis=1)\n\namerica_dis.head()\n","feda3773":"#Calculate distance to China for the rest of the countries\n\nasia_dis['distance_to_first_outbreak'] = asia_dis.apply(lambda x: distance(x['Lat'],x['Long'],x['china_lat'],\n                                                                 x['china_long']), axis=1)\n\nasia_dis.head()\n","3801f6c9":"\ncomplete_distance1 = pd.DataFrame()\n\ncomplete_distance1 = complete_distance1.append(europe_dis)\ncomplete_distance1 = complete_distance1.append(america_dis)\ncomplete_distance1 = complete_distance1.append(asia_dis)\n\ncomplete_distance1 = complete_distance1.sort_values('index')\n\ncomplete_distance1 = complete_distance1.set_index('index')\n\ncomplete_distance1.head()\n","239fe953":"train_dis = pd.merge(train_rfm,\n                 complete_distance1[['Country_Region','distance_to_first_outbreak']],\n                 on=['Country_Region'], \n                 how='left')\n\ntrain_dis['distance_to_first_outbreak'] = train_dis['distance_to_first_outbreak'].interpolate(\n    method ='linear', limit_direction ='both') \n\ntrain_dis = train_dis.drop_duplicates(['Id'],keep='first')\n\n\ntest_dis = pd.merge(test_rfm,\n                 complete_distance1[['Country_Region','distance_to_first_outbreak']],\n                 on=['Country_Region'], \n                 how='left')\n\n\ntest_dis['distance_to_first_outbreak'] = test_dis['distance_to_first_outbreak'].interpolate(\n    method ='linear', limit_direction ='both') \n\ntest_dis = test_dis.drop_duplicates(['ForecastId'],keep='first')\n","686377a7":"#ecoding countries data\nlabels, values = pd.factorize(train_dis['Country_Region'])\n\ntrain_dis['country_id'] = labels\n\ntrain_dis.head()\n\nlabels, values = pd.factorize(test_dis['Country_Region'])\n\ntest_dis['country_id'] = labels\n\ntest_dis.head()\n","c7c4f2b1":"random.seed(123)\n\nX = train_dis[['country_id','month','dates','distance_to_first_outbreak']]\ny_confirm = train_dis['ConfirmedCases']\ny_fatal = train_dis['Fatalities']\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_confirm, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n\n\n# Print the name and gini importance of each feature\nfor feature in zip(predict_labels, rf.feature_importances_):\n    print(feature)\n","338c4211":"plt.figure()\n\nplt.title(\"Feature importances\",fontsize=20)\n\nplt.bar(predict_labels,rf.feature_importances_, align=\"center\", color='dodgerblue')\n\nplt.xticks(predict_labels)\n\nplt.xticks(rotation=90)\n\nplt.show()","347d819e":"## predict on test set\n\nconfirm_case = rf.predict(test_dis[['country_id','month','dates','distance_to_first_outbreak']])","79ae7046":"#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_fatal, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n","4d26b6b8":"## predict on test set\n\nfatal_case = rf.predict(test_dis[['country_id','month','dates','distance_to_first_outbreak']])\n","e673a529":"\nforecaseId = pd.DataFrame(test[['ForecastId']])\nconfirm_case = round(pd.DataFrame(confirm_case),0)\nfatal_case = round(pd.DataFrame(fatal_case),0)\n\n\nfinal_result_rf_id = pd.concat([forecaseId,confirm_case,fatal_case],axis=1)\nfinal_result_rf_id.columns = ['ForecastId','ConfirmedCases','Fatalities']\n\nfinal_result_rf_id.tail()\n","96c42051":"final_result_rf_id.to_csv('results_rf_dis_id.csv',index=False)","1ca750fd":"#run for each country\nx_pred_lab = ['month','dates','distance_to_first_outbreak']\n\nfinal_result_rf_dis_each = rf_each_country(seed=123, original_df=train, train_df=train_dis, test_df=test_dis, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","6915ceb4":"final_result_rf_dis_each.tail()","31a6fd4f":"final_result_rf_dis_each.to_csv('submission.csv',index=False)","39cabe41":"#run for each country\nx_pred_lab = ['month','dates','distance_to_first_outbreak']\n\nfinal_result_lin_dis_each = lin_each_country(seed=123, original_df=train, train_df=train_dis, test_df=test_dis, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","7fdd1c31":"final_result_lin_dis_each.tail()","24665081":"final_result_lin_dis_each.to_csv('final_result_lin_dis_each.csv',index=False)","6deba3be":"#run for each country\nx_pred_lab = ['month','dates','distance_to_first_outbreak']\n\nfinal_result_br_dis_each = br_each_country(seed=123, original_df=train, train_df=train_dis, test_df=test_dis, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","bbb512a5":"final_result_br_dis_each.tail()","4dd41213":"final_result_br_dis_each.to_csv('final_result_br_dis_each.csv',index=False)","0dece2c5":"train_demo1 = train_demo.copy()\n\n# Add additional variables\n#month\ntrain_demo1['month'] = train_demo1['Date'].dt.month\n\n#date\ntrain_demo1['dates'] = train_demo1['Date'].dt.day\n\n\n### do the same for test data\ntest_demo1 = test_demo.copy()\n\n# Add additional variables\n\n#month\ntest_demo1['month'] = test_demo1['Date'].dt.month\n\n#date\ntest_demo1['dates'] = test_demo1['Date'].dt.day\n\n\ntest_demo1.tail()","8be992dc":"### Interpolate missing values\n\ntrain_demo1 = train_demo1.interpolate(method ='linear', limit_direction ='both')\ntest_demo1 = test_demo1.interpolate(method ='linear', limit_direction ='both')\n","623f089c":"#ecoding countries data\nlabels, values = pd.factorize(train_demo1['Country_Region'])\n\ntrain_demo1['country_id'] = labels\n\nlabels, values = pd.factorize(test_demo1['Country_Region'])\n\ntest_demo1['country_id'] = labels\n\ntest_demo1.head()\n","047a935f":"### Train dataset\n\n# Create x, where x the 'scores' column's values as floats\npop = train_demo1[['pop']].values.astype(float)\ntests = train_demo1[['tests']].values.astype(float)\nhealthexp = train_demo1[['healthexp']].values.astype(float)\n\n# Create a minimum and maximum processor object\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n# Create an object to transform the data to fit minmax processor\npop_scaled = min_max_scaler.fit_transform(pop)\ntest_scaled = min_max_scaler.fit_transform(tests)\nhealthexp_scaled = min_max_scaler.fit_transform(healthexp)\n\n# Run the normalizer on the dataframe\ntrain_demo1[['pop']] = pd.DataFrame(pop_scaled)\ntrain_demo1[['tests']] = pd.DataFrame(test_scaled)\ntrain_demo1[['healthexp']] = pd.DataFrame(healthexp_scaled)\n\ntrain_demo1.head()\n\n### Test dataset\n\n# Create x, where x the 'scores' column's values as floats\npop = test_demo1[['pop']].values.astype(float)\ntests = test_demo1[['tests']].values.astype(float)\nhealthexp = test_demo1[['healthexp']].values.astype(float)\n\n# Create a minimum and maximum processor object\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n# Create an object to transform the data to fit minmax processor\npop_scaled = min_max_scaler.fit_transform(pop)\ntest_scaled = min_max_scaler.fit_transform(tests)\nhealthexp_scaled = min_max_scaler.fit_transform(healthexp)\n\n# Run the normalizer on the dataframe\ntest_demo1[['pop']] = pd.DataFrame(pop_scaled)\ntest_demo1[['tests']] = pd.DataFrame(test_scaled)\ntest_demo1[['healthexp']] = pd.DataFrame(healthexp_scaled)\n\ntest_demo1.head()","450a1be1":"random.seed(123)\n\nX = train_demo1[['country_id','pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'month', 'dates']]\n\ny_confirm = train_demo1['ConfirmedCases']\ny_fatal = train_demo1['Fatalities']\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_confirm, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n\n\n# Print the name and gini importance of each feature\nfor feature in zip(predict_labels, rf.feature_importances_):\n    print(feature)\n    ","239181ca":"plt.figure()\n\nplt.title(\"Feature importances\",fontsize=20)\n\nplt.bar(predict_labels,rf.feature_importances_, align=\"center\", color='dodgerblue')\n\nplt.xticks(predict_labels)\n\nplt.xticks(rotation=90)\n\nplt.show()","f953c794":"## predict on test set\n\nconfirm_case = rf.predict(test_demo1[['country_id','pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'month', 'dates']])\n","2b54dab4":"random.seed(123)\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_fatal, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n    ","f114a548":"## predict on test set\n\nfatal_case = rf.predict(test_demo1[['country_id','pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'month', 'dates']])","9e56769a":"forecaseId = pd.DataFrame(test[['ForecastId']])\nconfirm_case = pd.DataFrame(confirm_case)\nfatal_case = pd.DataFrame(fatal_case)\n\n\nfinal_result_rf_demo = pd.concat([forecaseId,confirm_case,fatal_case],axis=1)\nfinal_result_rf_demo.columns = ['ForecastId','ConfirmedCases','Fatalities']\n\nfinal_result_rf_demo.tail()","ad44765b":"final_result_rf_demo.to_csv('result_rf_demo.csv',index=False)","e5f0e212":"#run for each country\nx_pred_lab = ['pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'month', 'dates']\n\nfinal_result_rf_demo_each = rf_each_country(seed=123, original_df=train, train_df=train_demo1, test_df=test_demo1, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","0e8a07bb":"final_result_rf_demo_each.tail()","f8debce7":"final_result_rf_demo_each.to_csv('final_result_rf_demo_each.csv',index=False)","73e6e84b":"#run for each country\nx_pred_lab = ['pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'month', 'dates']\n\nfinal_result_lin_demo_each = lin_each_country(seed=123, original_df=train, train_df=train_demo1, test_df=test_demo1, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","1e28c7f1":"final_result_lin_demo_each.tail()","4ca099de":"final_result_lin_demo_each.to_csv('final_result_lin_demo_each.csv',index=False)","8df7bf6b":"#run for each country\nx_pred_lab = ['pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'month', 'dates']\n\nfinal_result_br_demo_each = br_each_country(seed=123, original_df=train, train_df=train_demo1, test_df=test_demo1, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","94d9004b":"final_result_br_demo_each.tail()","584ef390":"final_result_br_demo_each.to_csv('final_result_br_demo_each.csv',index=False)","a5293351":"#construct the OLS model\nX = train_demo1[[ 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'lung', 'femalelung', 'malelung','fertility']]\ny = train_demo1['ConfirmedCases']\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\n# Note the difference in argument order\nmodel = sm.OLS(y_train_rf, X_train_rf).fit()\npredictions = model.predict(X_test_rf) # make the predictions by the model\n\nmodel.summary()\n","2899955d":"#construct the OLS model\nX = train_demo1[[ 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'lung', 'femalelung', 'malelung','fertility']]\ny = train_demo1['Fatalities']\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\n# Note the difference in argument order\nmodel = sm.OLS(y_train_rf, X_train_rf).fit()\npredictions = model.predict(X_test_rf) # make the predictions by the model\n\nmodel.summary()\n","e2b4da5a":"weather_train = weather_train.drop(['Id'],axis=1)\n\nweather_train = weather_train.rename(columns={'Id.1':'Id'})\n","cd8208c1":"weather_test1 = weather_test1.drop(['ForecastId'],axis=1)\n\nweather_test1 = weather_test1.rename(columns={'ForecastId.1':'ForecastId'})\n","0082c725":"##combine dataframe\n\n#train\n\ntrain_all = pd.merge(train_first_result,train_demo1[['Id','Country_Region', 'Date', \n                                                     'pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'month', 'dates',\n       'country_id']], on=['Id','Country_Region','Date','country_id','month', 'dates'], how='left')\n\ntrain_all = pd.merge(train_all,weather_train[['Id','Country_Region', 'Date', 'Lat', 'Long',\n       'day_from_jan_first', 'temp', 'min', 'max', 'stp', 'wdsp', 'prcp',\n       'fog','month','dates']], on=['Id','Country_Region','Date','month','dates'], how='left')\n\ntrain_all = pd.merge(train_all,train_dis[['Id','Country_Region', 'Date', 'month', 'dates', 'distance_to_first_outbreak',\n       'country_id']], on=['Id','Country_Region','Date','country_id','month', 'dates'], how='left')\n\ntrain_all = train_all.dropna(subset=['day_from_jan_first'])\n\ntrain_all.tail()","fad1dbba":"#test\n\ntest_all = pd.merge(test_first_result,test_demo1[['ForecastId','Country_Region', 'Date', \n                                                     'pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'month', 'dates',\n       'country_id']], on=['ForecastId','Country_Region','Date','country_id','month', 'dates'], how='left')\n\ntest_all = pd.merge(test_all,weather_test1[['ForecastId','Country_Region', 'Date',\n       'Lat', 'Long',\n       'day_from_jan_first', 'temp', 'min', 'max', 'stp', 'wdsp', 'prcp',\n       'fog', 'month', 'dates', 'country_id']], on=['ForecastId','Country_Region',\n                                                    'Date','country_id','month', 'dates'], how='left')\n\ntest_all = pd.merge(test_all,test_dis[['ForecastId','Country_Region', 'Date', 'month', \n                                          'dates', 'distance_to_first_outbreak',\n       'country_id']], on=['ForecastId','Country_Region','Date','country_id','month', 'dates'], how='left')\n\ntest_all.tail()","063653ff":"random.seed(123)\n\nX = train_all[['month', 'dates', 'days_since_first_outbreak',\n       'country_id', 'pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'Lat',\n       'Long', 'day_from_jan_first', 'temp', 'min', 'max', 'stp', 'wdsp',\n       'prcp', 'fog', 'distance_to_first_outbreak']]\ny_confirm = train_all['ConfirmedCases']\ny_fatal = train_all['Fatalities']\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_confirm, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n\n\n# Print the name and gini importance of each feature\nfor feature in zip(predict_labels, rf.feature_importances_):\n    print(feature)","8bd84c7e":"plt.figure()\n\nplt.title(\"Feature importances\",fontsize=20)\n\nplt.bar(predict_labels,rf.feature_importances_, align=\"center\", color='dodgerblue')\n\nplt.xticks(predict_labels)\n\nplt.xticks(rotation=90)\n\nplt.show()","d38c7853":"## predict on test set\n\nconfirm_case = rf.predict(test_all[['month', 'dates', 'days_since_first_outbreak',\n       'country_id', 'pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'Lat',\n       'Long', 'day_from_jan_first', 'temp', 'min', 'max', 'stp', 'wdsp',\n       'prcp', 'fog', 'distance_to_first_outbreak']])\n","c28c9471":"random.seed(123)\n\n#Now we find the best parameters to fit in the Random Forest model, we will use it to measure the feature important in the data\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y_fatal, test_size=0.3, random_state=0)\n\npredict_labels = X.columns\n\n#RF model\nrf = RandomForestClassifier()\n\n# Train the classifier\nrf.fit(X_train_rf, y_train_rf)\n\ny_pred_rf = rf.predict(X_test_rf)\n\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test_rf, y_pred_rf),2))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test_rf, y_pred_rf),2))  \nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test_rf, y_pred_rf)),2), \"\\n\")\n    \nfatal_case = rf.predict(test_all[['month', 'dates', 'days_since_first_outbreak',\n       'country_id', 'pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'Lat',\n       'Long', 'day_from_jan_first', 'temp', 'min', 'max', 'stp', 'wdsp',\n       'prcp', 'fog', 'distance_to_first_outbreak']])\n","8781e908":"forecaseId = pd.DataFrame(test[['ForecastId']])\nconfirm_case = pd.DataFrame(confirm_case)\nfatal_case = pd.DataFrame(fatal_case)\n\n\nfinal_result_all = pd.concat([forecaseId,confirm_case,fatal_case],axis=1)\nfinal_result_all.columns = ['ForecastId','ConfirmedCases','Fatalities']\n\nfinal_result_all.tail()","43baf25b":"final_result_all.to_csv('result_all_rf.csv',index=False)","5df22c47":"#run for each country\nx_pred_lab = ['month', 'dates', 'days_since_first_outbreak',\n       'pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'Lat',\n       'Long', 'day_from_jan_first', 'temp', 'min', 'max', 'stp', 'wdsp',\n       'prcp', 'fog', 'distance_to_first_outbreak']\n\nfinal_result_rf_all_each = rf_each_country(seed=123, original_df=train, train_df=train_all, test_df=test_all, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","d16feb5d":"final_result_rf_all_each.tail()\n","af624ec8":"final_result_rf_all_each.to_csv('final_result_rf_all_each.csv',index=False)","9aa8e9ba":"#run for each country\nx_pred_lab = ['month', 'dates', 'days_since_first_outbreak',\n       'pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'Lat',\n       'Long', 'day_from_jan_first', 'temp', 'min', 'max', 'stp', 'wdsp',\n       'prcp', 'fog', 'distance_to_first_outbreak']\n\nfinal_result_lin_all_each = lin_each_country(seed=123, original_df=train, train_df=train_all, test_df=test_all, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","4386a714":"final_result_lin_all_each.tail()","a3f4ce07":"final_result_lin_all_each.to_csv('final_result_lin_all_each.csv',index=False)","9ef90052":"#run for each country\nx_pred_lab = ['month', 'dates', 'days_since_first_outbreak',\n       'pop', 'tests', 'testpop', 'density', 'medianage',\n       'urbanpop', 'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54',\n       'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung',\n       'healthexp', 'healthperpop', 'fertility', 'Lat',\n       'Long', 'day_from_jan_first', 'temp', 'min', 'max', 'stp', 'wdsp',\n       'prcp', 'fog', 'distance_to_first_outbreak']\n\nfinal_result_br_all_each = br_each_country(seed=123, original_df=train, train_df=train_all, test_df=test_all, \n                        confirm ='ConfirmedCases', fatal='Fatalities', xlabels = x_pred_lab, test_size = 0.3)\n","fec94552":"final_result_br_all_each.tail()","5c6ffe5c":"final_result_br_all_each.to_csv('final_result_br_all_each.csv',index=False)","a14baf0a":"### Function of Fit the SEIR model to real data\n* Auto choose the best decay function of $R_t$ (intervention days decay or Hill decay)\n* Total case\/country population is below 1, reduce country population\n* If datset still no case, return 0 \n* Plot the fit result and forecast trends (Infect smooth decrease by what date)\n* Function being hide, there are describe in code.","927a99d6":"## Combine all original, weather, distance and demographic data together\n\n<a id='all_predict'><\/a>\n","c1dce7d2":"#### People whose ages are above 65 are the most vulnerable towards coronavirus. Italy and Spain are among the top 5 countries with highest median age (above 40 years old) and, at the same time, with the highest number of fatal rate by Covid-19.","5f88a565":"## Closing Remarks\n\n<a id='conclusion'><\/a>\n\n- The goal of this analysis is to provide some data-informed insights about the COVID-19 transmission. Recommendations are based on my data exploration and personal perspectives. Therefore, the prediction results are solely served as reference and should not act as any concrete statement for actual future events. \n\n\n- More tuning, improvement and up-to-date data will be needed for future analysis and iteration. However, it's also fundamental to research more about the pandemic itself and learn from epidemiology experts.\n\n- It is a very statisfying experience during the #HappyAtHome self-quarantine time to learn something new, perform analysis on completely different field from what I have done before and (hopefully) provide some meaningful insights to help us combat the pandemic together.\n\n### All in all, I hope you stay safe and healthy, wash your hands, and #StayHome to help our healthcare workers who are working insanely hard in the front line!\n","7b20a314":"### Bayesian Ridge Regression\n\n<a id='demo_br'><\/a>\n","fae9cb16":"## 2) SEIR Model\n\n<a id='SEIR'><\/a>\n","b1d4bdcd":"#### Weather Data\n\nData was retrievied following the amazing work by [Davide Bonin](https:\/\/www.kaggle.com\/davidbnn92\/weather-data\/#data)\n\nI created my own version and the data can be found [here](https:\/\/www.kaggle.com\/giginghn\/weather-features)\n\n**Dictionary**\n\n- temp: temperature for the day in degrees Fahrenheit to tenths. Missing = 9999.9\n- max: Maximum temperature reported during the day in Fahrenheit to tenths--time of max temp report varies by country and region, so this will sometimes not be the max for the calendar day. Missing = 9999.9\n- min: Minimum temperature reported during the day in Fahrenheit to tenths--time of min temp report varies by country and region, so this will sometimes not be the min for the calendar day. Missing = 9999.9\n- stp: Mean station pressure for the day in millibars to tenths. Missing = 9999.9\n- wdsp: Mean wind speed for the day in knots to tenths. Missing = 999.9\n- prcp: Total precipitation (rain and\/or melted snow) reported during the day in inches and hundredths; will usually not end with the midnight observation--i.e., may include latter part of previous day. .00 indicates no measurable precipitation (includes a trace). Missing = 99.9\n- fog: Indicators (1 = yes, 0 = no\/not reported) for the occurrence during the day","453587ba":"#### According to the multilinear regression model, high temperature and humiditiy over have statistically significant impact on the number of Covid-19 transmissions in overall\n\n#### Particular, when temperature increase by 1 degree, the number of infected cases will decrease approximately 18 cases. Similarly, if the humidity level goes up by 1 unit, there will be nearly a decrease in 9 Covid cases.","b6e80c33":"### Try to apply for each countries","1fc4bdd0":"#### Italy","ba1c0602":"#### Even though Korea has flattened the curve, Iran continues to be the rise.","547697bb":"#### US","d09850b6":"### Multilinear Regression\n\n<a id='original_lin'><\/a>\n","4b8b79b9":"### Random Forest Model\n\n<a id='all_rf'><\/a>\n","04437599":"#### Beside China, South Korea is the top 2 country that have the highest number of COVID tests per day. Without any quarantine or 'shelter in place' ban, the country has curb the COVID-19 spread signficantly by widespread testing only within a month.\n\nFurther reading: https:\/\/www.sciencemag.org\/news\/2020\/03\/coronavirus-cases-have-dropped-sharply-south-korea-whats-secret-its-success","6cf4019a":"### Load Data\n\n#### Train dataset from Kaggle","37b4c3e5":"While Iran, Japan and Malaysia are having the good sign of recovery, Thailand's recovery rate is decreasing.","3baffcce":"### Windspeed by Country over time","141c966a":"### Italy","35881c22":"#### Similarly, when we increase the number of ICU beds by 1 unit, we can save 8 more people given all the variables in the model. Country with higher medium age also have the tendency to witness more fatal cases.\n","c10f5ce1":"Source: [3Blue1Brown](https:\/\/www.youtube.com\/watch?v=Kas0tIxDvrg&t=3s)\n","c1d0363c":"### Calculate the distance from Data center to each ip_address\n\n*Refer to this math guide:*\n\n*https:\/\/janakiev.com\/blog\/gps-points-distance-python\/*\n\n*https:\/\/kanoki.org\/2019\/02\/14\/how-to-find-distance-between-two-points-based-on-latitude-and-longitude-using-python-and-sql\/*","60b01568":"#### Merge the Demographic with the train and test data","d79493f6":"### Linear Regression Model","c5a75334":"Source: [Siouxsie Wiles](https:\/\/thespinoff.co.nz\/society\/09-03-2020\/the-three-phases-of-covid-19-and-how-we-can-make-it-manageable\/)","fbb16c86":"#### The rest of the World combined is also seeing a steady increase in confirmed cases over time.","2354d27c":"### Humidity by Country over time","dceca709":"### Put the function in the current pandemic\n\nThere are 4 important factors in our model:\n- Growth Factor\n- Growth Ratio\n- Growth Rate\n- 2nd Derivative\n\nWe will use these growth metrics to gain insight into which countries may have already hit their inflection points. For example, **if a country's growth factor has stabilized around 1.0 then this can be a sign that that country has reached it's inflection point**. \n\nWe will then fit the logistic curve to the number of confirmed cases for each country. This will help us predict whether a country has hit their inflection point, and when they will reach a possible maximum number of confirmed cases.\n\nFurthermore, if we take the **2nd derivative**, it is telling us **how fast the case is growing**.\n\nThe bigger picture will be to correlate this with preventative efforts such as quarantines, widespread testing, etc.\n","952953b3":"#### Even though that China and South Korea Growth Factor has approximated 1.00, U.S., Italy and Spain are growing fast in the number of infected cases (the 2nd Derivative curve is increasing). Vietnam - my country, even though it has no death case so far and still on a very small amount of infected cases, there's potential that it will widespread if we do not act now.\n","efd1a427":"#### Recovery Rate","78d62ff6":"### Confirm Cases","a4a77325":"### Let's look at the top 10 Europe countries with outbreaks in a timeline\n\n#### Confirmed Cases","5653c6ec":"#### Test set from Kaggle","34e098ce":"### Submit","9d348a4c":"#### Among the top severe country by Covid-19, South Korea and Germany are among their counterparts to mitigate the death rate. This becomes clear when they are the top 2 countries with amount of ICU beds per 1K patients to effectively support healthcare workers save thousand of patients' lives.","341dc63d":"### Intervention by Hill function for SEIR model\n- [Decay Function](https:\/\/github.com\/SwissTPH\/openmalaria\/wiki\/ModelDecayFunctions)","5df08280":"### Bayesian Ridge Regression\n\n<a id='distance_br'><\/a>\n","1646e53e":"#### Top 20 countries with the amount of hospital bed per 1,000 people","0dd41efd":"#### It's clear that the outbreak began in **France**.","d26de4a3":"### Random Forest Model\n\n<a id='distance_rf'><\/a>\n","15ec1de2":"#### Same with China, given everything constant, the high level of humidity significantly impact on the decrease of infected cases.","944097a0":"#### Encoding country data","f37fa652":"### Model with intervention\n* There are different way to reduce $R_t$, [Different decay function](https:\/\/github.com\/SwissTPH\/openmalaria\/wiki\/ModelDecayFunctions) \n\nThis could be modified to take any function of $R_t(t)$ values to model the reproduction number as a time varying variable\n\nWhen we introduce intervention, we can curb the spread of the Covid-19 cases.\n","c2b3ad9e":"\n### Temperature by Country over time","f7056d42":"### Which country has the first infected case?\n\n<a id='continents'><\/a>\n","88e40451":"As the number of infected cases rise, the number of recovery cases began to drop, this means that we need to supress the cases below the healthcare capacity so that we can prepare, stock enough supplies and have sufficient numbers of healthcare workers to serve the patients.","a3c9bec0":"We won't go into details about the math, but a general understanding would be sufficient:\n\n### Logistic Function\n\nA **logistic function** or **logistic curve** is an equation of the form: \n\n$f(x) = \\frac{N}{1 + e^{-k(x-x_0)}}$\n\n#### where\n\n- N = the curve's maximum value\n- x_0 = the inflection point\n- k = growth rate of the curve\n\nReference: https:\/\/en.wikipedia.org\/wiki\/Logistic_function","781dba99":"#### Fatality Rate","c9961306":"## Let's fit SEIR model on each country and region\n\n- I want to examine on Vietnam(my country), U.S., New York, California (where I'm based), South Korea, Italy, Spain and Germany.\n","3ebc86dc":"### Baysian Ridge Model\n\n<a id='original_br'><\/a>\n","cba7ea5e":"### We can already see China and Korea epidemic follwed this curve","de3c35d2":"- Calculate days from first outbreak ","f165bdac":"**Additional Source: Humidity and Sun Hours**\n\nSpecial thanks to [@winterpierre](https:\/\/www.kaggle.com\/winterpierre91\/covid19-global-weather-data#temperature_dataframe.csv) for the additional weather data.","83f3bc17":"#### Population with higher density can also relate to the transmission rate of Covid-19. As we can see, China, France, US and South Korea are the top country with high population density.\n","1f18bb8d":"### Load Packages","c269d93d":"### Multilinear Regression\n\n<a id='demo_lin'><\/a>\n","1578a9c8":"- **Note**: The color represents the strength of the wind and the size of the circle represents the number of Covid-19 Cases\n\n\n#### According to the graph, the outbreak mostly widespreads in the region where the windspeed is around 40-50 miles\/hour ","2fc8c9bc":"\nWe need to solve the Differential equation to find the S,E,I,R, but what is **$R_t$**, **$T_inf$**, **$T_inc$** and how can we define those variable.\n\n* $R_0 , R_t$: [Reproduction number](https:\/\/en.wikipedia.org\/wiki\/Basic_reproduction_number), which is the state where no other individuals are infected or immunized (naturally or through vaccination)\n* $T_{inf}$: Average duration of the infection, $T_{inf}$ can be treat as individual experiences one recovery in k units of time.\n* $T_{inc}$: Average incubation period, according to previous research, they defined is as 5.2 ([reference1](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/32150748), [reference2](https:\/\/www.worldometers.info\/coronavirus\/coronavirus-incubation-period\/))\n\n### Assume there are some intervention will cause reproduction number ($R_0$) reduce (such as more ICU beds, quarantine, widespread testing, government, vaccines ....), have an effectiveness which decays over time ","bb941d59":"## About\n\n### COVID-19 (Corona Virus Disease 2019)\n- Caused by a SARS-COV-2 corona virus.\n- First identified in Wuhan, Hubei, China. The earliest reported symptoms was detected in November 2019.\n- On 30 January the WHO declared the outbreak to be a Public Health Emergency of International Concern\n- Until today, nearly 1 million people around the world are confirmed to be positive with Covid-19 by [Worldometer live tracking](https:\/\/www.worldometers.info\/coronavirus\/)\n\n\n### Data:\n\n- [Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE](https:\/\/github.com\/CSSEGISandData\/COVID-19)\n\n- [COVID19 Global Forecasting (Week 2)](https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-2\/data)\n\n- [COVID-19 Complete Dataset (Updated every 24hrs)](https:\/\/www.kaggle.com\/imdevskp\/corona-virus-report)\n\n- [COVID19 Demographic Data](https:\/\/www.kaggle.com\/koryto\/countryinfo#covid19countryinfo.csv)\n\n- [World Population Data](https:\/\/www.kaggle.com\/anjum48\/covid19-population-data)\n\n- [Weather Data](https:\/\/www.kaggle.com\/noaa\/gsod)\n\n- [Additiional Weather Data](https:\/\/www.kaggle.com\/winterpierre91\/covid19-global-weather-data#temperature_dataframe.csv)","2be217d5":"### Number of cases around the world\n\n<a id='world'><\/a>\n","b2429f97":"# Covid19 Analysis\n","8937a689":"## Table of content\n\n### 1) [Data Exploratory Analysis](#eda)\n  - [Number of cases around the world](#world)\n  - [Days since first outbreak](#first_outbreak)\n  - [Deep dive into confirmed, fatal and recover cases](#confirm_fatal)\n  - [Statistics of top 3 COVID19 Infected Countries](#top3_countries)\n  - [Statistics for each 3 continents - Europe, Asia, North America](#continents)\n  - [Understand the affect of weather on COVID19 transmission and fatalities](#temp)\n  - [Understand the affect of demographic on COVID19 transmission and fatalities](#demo)\n  - [What's happened after the first quarantine of top 3 impacted countries by Covid-19?](#quarantine)\n  \n### 2) [SEIR Model](#SEIR) \nWhat's logistic curve or SEIR model and how they are related to predicting the pandemic?\n\n- [Logistic Curve](#logistic_curve)\n- [SEIR Model break-down](#SEIR_model)\n\n\n### 3) [Prediction](#prediction)\n\nFor prediciton purpose, I will try mainly 3 predictive models (Random Forest, Multilinear Regression and Bayesian Ridge Regression) for each type of data, which I eventually assembled all different variables together to feed in the prediction models as well. \n\nSo far, the Random Forest Model with Weather Variables is having the best Public Score on Kaggle. However, I will continue to improve on the model performance in the next couple of days.\n\n- [Forecast with Original Dataset](#original)\n- [Forecast with Weather features](#weather_predict)\n- [Forecast with additional feature - distance from the first outbreak of the continent](#distance)\n- [Forecast with Demographic features](#demo_predict)\n- [Forecast with all additional features above](#all_predict)\n \n### 4 ) [Closing Remarks](#conclusion)\n","eb613df6":"### Let's look at the North America countries and the U.S.","866d3f53":"#### On a positive side, even though New York is the most contagious state, the fatality rate is still considered small compared to Washington and Lousiana states.","efb7666e":"#### Italy","f0092ff0":"#### Death rate from lung diseases per 100k people by top 20 countries with COVID19 outbreak","0f4820ab":"#### We can see that the curve was about to flatten out during mid-February, however, the cases continued to rised abruptly on Feb 10. \n\n### Let's break down in top 3 countries to see where the spike happened\n\n<a id='top3_countries'><\/a>\n","7c0f738a":"### What's happened after quarantine?\n\n<a id='quarantine'><\/a>\n","99734afa":"### Understand the effect of each demographic variables on the transmission of the virus via multilinear regression model\n\n<a id='demo_understand'><\/a>\n","9b4c97c9":"#### Median Age by top 20 countries with COVID19 outbreak","6e7ec4e2":"#### Further Readings:\n- [The Impact of Coronavirus (COVID-19) on Foot Traffic\n](https:\/\/www.safegraph.com\/dashboard\/covid19-commerce-patterns?is=5e7a66538dd3816c985d8fc7)\n\n- [Coronavirus: Why You Must Act Now](https:\/\/medium.com\/@tomaspueyo\/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca)\n\n- [Coronavirus: The Hammer and the Dance](https:\/\/medium.com\/@tomaspueyo\/coronavirus-the-hammer-and-the-dance-be9337092b56)\n\n- [Flatten the curve with South Korea](https:\/\/www.nytimes.com\/2020\/03\/23\/world\/asia\/coronavirus-south-korea-flatten-curve.html)","43ba79f4":"#### In later [section](#demo_understand), I will examine the relationship between the death rate by lung diseases with fatal and infected rate by Covid-19.\n\n#### Even though that lung diseases may not directly impact on the number of confirmed and death cases, we need more investigation to understand if it correlate to worsening the health condition with people who are positive with Covid-19 in any possible ways.","98cb3b82":"## Predict all Country\/Region and Province\/States\n* Counting all Country\/Region MSLE & predict\n* If MSLE is lower than 1 , using PR model to retrain and check the performance","9c6f0506":"Special thanks to [Datasaurus](https:\/\/www.kaggle.com\/anjum48\/covid19-population-data) for the Population data source.","9a9d935a":"## Distance to the first case\n\n<a id='distance'><\/a>\n","b68dde8e":"#### Similar to the cases we saw earlier in European countries, as the number of infected cases rise, the number of recovery cases began to drop. This means that we need to supress the cases below the healthcare capacity so that we can prepare, stock enough supplies and have sufficient numbers of healthcare workers to serve the patients.","8799e80a":"### Forecast with Original Data\n\n<a id='original'><\/a>\n","862eb28d":"#### Demographic Data\n\nSpecial thanks to [My Koryto](https:\/\/www.kaggle.com\/koryto) for the contribution of the data.\n\n**Dictionary**\n\n- Population (2020)\n- Density: The number of people who lives per square meter. (2020)\n- Median age (2020)\n- Urban population: the % of the population who lives in urban areas. (2020)\n- Hospital beds per 1,000 people\n- Forced quarantine policy initial date: I believe that a couple of weeks after this specific date, we can assume there would be a reduction of the infection rate. (updated on a daily basis)\n- School closure policy initial date: Same as (6). (updated on a daily basis)\n- Public places (bars, restaurants, movie theatres, etc.) closure policy initial date (updated on a daily basis)\n- The maximum amount of people allowed in gatherings and the initial date of the policy (updated on a daily basis)\n- Non-essential house leaving - initial date of the restriction (updated on a daily basis)\n- Sex ratio grouped by age groups (amount of males per female). (2020)\n- Lung disease death rate per 100k people, separated by sex. (2020)\n- % of smokers within the population: The higher this number is, the higher the fatalities number would be. (2019)\n- Amount of COVID detection test made per day: for about 50 countries\n- GDP-nominal (2019)\n- Health expenses in international USD (2019, 2017, 2015)\n- Health expenses divided by population (2020 - population), (2019, 2017, 2015 - health expenses)\n- Average amount of children per woman (2017)\n- First patient detection date\n- Total confirmed cases (updated on a daily basis)\n- Total active cases (updated on a daily basis)\n- New confirmed cases (updated on a daily basis)\n- Total deaths (updated on a daily basis)\n- New deaths (updated on a daily basis)\n- Total recovered (updated on a daily basis)\n- Amount of patients in critical situation (updated on a daily basis)\n- Total cases \/ 1 million population (updated on a daily basis)\n- Total deaths \/ 1 million population (updated on a daily basis)\n","b998406a":"#### In contrast with China and Italy, strong wind speed shows statistically significant impact on the confirmed cases. In essence, the number of cases decrease by nearly 16 people when the wind speed increases by 1 mile\/hour.\n\n#### This is only approximate estimate of the linear coefficients, since the U.S. has 50 states and the weather conditions may vary for different areas.","7e53770b":"Special thanks to: [funkyboy](https:\/\/www.kaggle.com\/super13579\/covid-19-global-forecast-seir-visualize) and [Daner Ferhadi](https:\/\/www.kaggle.com\/dferhadi\/logistic-curve-fitting-global-covid-19-confirmed)\n\n#### Reference:\n\n- [Exponential Growth - 3Blue1Brown](https:\/\/www.youtube.com\/watch?v=Kas0tIxDvrg&t=3s)\n- [Compartmental Model in Epidemiology](https:\/\/en.wikipedia.org\/wiki\/Compartmental_models_in_epidemiology#The_SEIR_model)\n- [Epidemic Calculator](http:\/\/gabgoh.github.io\/COVID\/index.html)\n\n## What's logistic curve or SEIR model and how they are related to predicting the pandemic?","21c7e07f":"#### We can see the total number of active casses in China has gone down in mid-Feb, so the spike that we saw earlier has to be from other regions.","bbc799f2":"- Encoding the countries","31d7a9b7":"- The spread of an epidemic can be modeled using a logistic curve rather than an exponential curve. \n\n- The growth can start exponentially, however, it must slow down after some point called the **inflection point**.  The inflection point is essentially the midpoint of the spread. \n\nIn the next few sections, we will model the number of Covid-19 cases using a logistic curve. But first, let's briefly walk through the equation for the curve and the plot of the curve.\n","0887bec0":"- **Note**: The color represents the hours of the sun and the size of the circle displays the number of Covid-19 Cases\n\n#### The hours of the sun seem to not correlate with the number of outbreak since they are around 20 hours for almost every country.\n","120093ea":"### Cleaning Data","9f6d5eff":"### Bayesian Ridge Regression \n\n<a id='all_br'><\/a>\n","b9040802":"#### Overall","d64a9b92":"#### China","9746044c":"### Multilinear Regression\n\n<a id='weather_lin'><\/a>\n","048c4c20":"### Random Forest Model\n\n<a id='original_rf'><\/a>\n","4c300f74":"#### We can see that density of the population, and the number of people who smoke have significantly effects on the increase of the confirmed case. While having more ICU beds would decrease the number of transmission by a significant amount.\n\n#### We can also see if the country have higher median age, the more infected cases they will have.\n","0e37377f":"### Random Forest Model\n\n<a id='demo_rf'><\/a>\n","fe48beeb":"\n\n## Background\nThe White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine\u2019s (NASEM) and the World Health Organization (WHO).\n\n## The Challenge\nKaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM\/WHO questions. We are currently on week 2.\n\nWhile the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19.\n\nAs the data becomes available, Kaggle will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).\n\n## Analysis\nIn this challenge, I will be analyzing the open-source data from different contributors on Kaggle to answer these critical questions:\n\n- **What do we know about non-pharmaceutical interventions?** Ex: quarantine, cancelling large gatherings, widespread testing, etc.)\n\n- **What is known about transmission, incubation, and environmental stability?** Ex: Data on temperature, humidity and wind speed by region.\n\n- **What do we know about COVID-19 risk factors?** Ex: Percentage of the population that smokes, numbers of death by lung diseses or numbers of ICU for the patients in a country.\n\n- **When should we expect the number of infected cases will start to stabilize and decrease?** \n\n- **What would be the maximum number of cases in N days?**\n\nIn addition, I will predict the cumulative number of confirmed COVID19 cases as well as fatal cases in **173 countries** across the world for future dates **(ranging from April 1 - April 30)**.\n\n### Note: This is a work in progress and I will try to update the Notebook everyday with new data coming in and improve\/add further analysis.\n\n**Disclaimer from Kaggle:** We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n","1a6bfabb":"### Let's try to run for each country","1b0843cd":"### Cumsum signal\n* to prevent fluctuation","b2f5c430":"## Weather\n\n<a id='temp'><\/a>\n\n#### In this section, I will break down by temperature (in Fahranheit), wind speed, humidity and sun hours to analyze and find their impact on the transmission rate accordingly.\n","df48e02c":"#### Normalize data","8229abc1":"#### We can see that the outbreak started from Washington state on March 9th and the strike leveled up from there.","fb5af8d9":"### Fatal Cases","2b398f07":"#### Recovery Rate","75f88749":"- **Note**: The color represents the temperature levels and the size of the circle stands for the number of Covid-19 Cases\n\n#### This looks like the outbreak mostly widespreads in the region where the average temperature in Fahranheit is around 40 to 60 degrees.","d6907fa3":"#### Merge with the training and testing dataset","65d50885":"#### Populations by top 20 countries with COVID19 outbreak","7b89d06a":"#### Fatality Rate","ab5ea86b":"### Let's look at all Europe countries\n","0f01f4ba":"### Intervention by after days for SEIR model\n* after days, start interverntion, $ R_0 = R_0 * 0.5$","a566d842":"#### Similar to the general pattern, the high humidity level also significantly decrease the number of Covid-19 infected cases.","9064c8fa":"#### Interpolate missing values","27fe5693":"#### Submission File from Kaggle","a91b6975":"### Model without intervention\n\nWe will first try the model on New York. \n\nWe can see that without any intervention, the number confirmed cases will continue to increase.\n","3fd84d8c":"#### Fatality Rate","cd044e69":"### US","6eb41981":"- Extract month and dates","0c0047e3":"### Multilinear Regression Model\n\n<a id='all_lin'><\/a>\n","e1050bf8":"### Other countries","e4b52d39":"#### Recovery Rate","e01c96e9":"#### Complete dataset that include Geographic Data","19d673b7":"### Random Forest Model\n\n<a id='weather_rf'><\/a>\n","d0bd84cd":"### Let's try to run for each country","6c7495c8":"### China","1ec67bf6":"## 3) Prediction\n\n<a id='prediction'><\/a>\n\n- For prediciton purpose, I will try mainly 3 predictive models (Random Forest, Multilinear Regression and Bayesian Ridge Regression) for each type of data, which I eventually assembled all different variables together to feed in the prediction models as well. \n\n- So far, the Random Forest Model with Weather Variables is having the best Public Score on Kaggle. However, I will continue to improve on the model performance in the next couple of days.","25a0ac89":"#### In later [section](#demo_understand), I found that the high number of smokers relates with the number of confirmed cases for Covid-19. However, more scientific research is needed to be conducted in order to draw the final conclusion.\n","01282ce8":"### Let's look into Asia countries","76a8efd1":"**We can see that after nearly 20 days after the first quarantine, China confirmed cases began to slow down and stabilize.**\n\n#### Looking at China curves, this followed the pattern that our healthcare experts conducted based on the historical data on Hubei:\n","833734b2":"## Logistic Curve\n\n<a id='logistic_curve'><\/a>\n","a8c1bd1e":"### What's the current situation?","7877d3f9":"#### US","0e0233f9":"### How long since the first outbreak?\n\n<a id='first_outbreak'><\/a>\n\n","3f8139a2":"### Let's try to apply the model for each countries and then append them together","65ece20f":"**Disclaimer:** The model is currently based on the current data that we have, the prediction will be more accurate as new data coming in and with further tuning.","0955dc2f":"## 1) Exploratory Data Analysis\n\n<a id='eda'><\/a>\n\n#### In this section, I will present some interactive visualizations for us to understand the big picture of our current situation in COVID-19 pandemic. ","a6c04adb":"### Demographic Data\n\n<a id='demo'><\/a>\n","4056f9cc":"\nSource: [Journal of the American Medical Association](https:\/\/jamanetwork.com\/journals\/jama\/fullarticle\/2762130)\n\n\n#### Nearly after 20 days, the number of actual outbreak decreased signifcantly.\n\n\n#### However, after approximately 15 days of quarantine, school shutdowns, and travel bans, the number of cases in Italy and U.S. continue to rise exponentially. This may due to the fact that China had acted fast as soon as a few cases were detected, but this was not the case for Italy and the U.S.","299ab20b":"### Let's try for each country","c215cf9f":"### Baysian Ridge Regression Model\n\n<a id='weather_br'><\/a>\n","dd774fe6":"#### The outbreak started in Italy in mid-Feb and in the U.S. from beginning of March.","c88d5fe0":"#### Breakdown by Confirm and Fatal cases","650421f1":"#### China","6e52d9ed":"#### Density by top 20 countries with COVID19 outbreak","bbd1f159":"- Encode countries","d6622043":"- In Europe: France (46.2276,2.2137)\n- In America: US (37.0902,-95.7129)\n- In Asia: China, Wuhan (30.5928,114.3055)","da7f3270":"#### We can see that both China and Korea have hit the inflection point and flattened their curve for epidemic outbreak.\n\n#### If we look closely at Italy, U.S. and Spain, they are increasing the infected cases with an exponential growth.","cf9bd0a5":"## Weather Data\n\n<a id='weather_predict'><\/a>\n","e0b77b0c":"## SEIR Model \n\n<a id='SEIR_model'><\/a>\n\n* Function was taken from the [Epidemic Calculator](http:\/\/gabgoh.github.io\/COVID\/index.html)\n![image.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/3d\/SEIR.PNG)\n- **S**usceptible : number of susceptible\n- **E**xpose : number of expose\n- **I**nfectious : number of infectious\n- **R**ecovered or Removed : number recovered (or immune) individuals\n\n### S + E + I + R = N\n- N is country total population\n\nDifferential Equations are as below (Source: [Epidemic Calculator](http:\/\/gabgoh.github.io\/COVID\/index.html)): \n","32be0cef":"### Multilinear Regression\n\n<a id='distance_lin'><\/a>\n","4e532bef":"- **Note**: The color represents the humidity level and the size of the circle shows the number of Covid-19 Cases\n\n#### It seems that higher humidity implies higher number of outbreak.","5ac699cb":"### Plot SEIR model and predict","46011f0e":"#### Top 20 countries with number of COVID19 tests\n","45ad4a6a":"### Sun Hour by Country over time\n","6149e76e":"## Demographic Data\n\n<a id='demo_predict'><\/a>\n","b91168b1":"#### The first case began from the **U.S.**","baa0646b":"### Growth Equation\n\n**Growth factor**:\n\n$G_n = \\frac{G_n - G_{n-1}}{G_{n-1} - G_{n-2}}$\n\n**Growth ratio**:\n\n$G_n = \\frac{G_n }{G_{n-1}}$\n\n- $G_n$ : Number of cases on **n** day\n\nSpecial thanks to Daner Ferhadi for the functions.","7696c117":"### Try to apply for each country","d1b0d472":"## Fit the SEIR model to real data and predict\n\nFind the best variables of SEIR model to fit the real data\n* **T_inf** --> Using average value 2.9 \n* **T_inc** --> Using average value 5.2\n* **R_t** --> find the best reproduction number by fitting the real data (if have decay function, find the paramater of decay function)\n* **cfr** --> find the best Case fatality rate, this parater is for predict Fatalities","533992f8":"### Number of confirmed, fatal and recovered cases over time\n\n<a id='confirm_fatal'><\/a>\n","02694f8a":"####  In Asia, China is the first case of the outbreak"}}