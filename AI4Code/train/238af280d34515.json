{"cell_type":{"862f6188":"code","6872cf22":"code","92e5c7ff":"code","46e1e7ac":"code","088e1a53":"code","3f1d6119":"code","24687ddd":"code","036aaee2":"code","aba6b344":"code","4211b33e":"code","6ac800b5":"code","0a562a71":"code","e2012c4c":"code","ba67ab91":"code","a183c47c":"code","d16e1ef6":"code","b219b893":"code","a7b55b67":"code","4832c890":"code","e2afb512":"code","92c4d5bb":"code","90c7de44":"code","3b7e43fd":"code","00ede2ba":"code","c981a5ee":"code","56ed7d4b":"code","09c1685a":"code","47e06fbf":"code","37c5a492":"code","734b63a0":"code","7baa39b0":"code","ab58e5da":"code","444ec769":"code","8e89484d":"code","6c55f6ae":"code","b82a0a5b":"code","30b34d43":"code","074f9285":"code","201738b7":"code","9d792ea0":"code","27ae8d9e":"code","a7907155":"code","0664e9d2":"code","ec0494d8":"code","a965f591":"code","d304d3d3":"code","dbf42587":"code","412871fa":"code","0dd927ec":"code","98043610":"code","192a7ad7":"markdown","bf136450":"markdown","cbfb2cff":"markdown","9051ba55":"markdown","8c0a3213":"markdown","f0669557":"markdown","cb5839c7":"markdown","b0cbd315":"markdown","6ff1df2c":"markdown","0411f723":"markdown","b518b5c0":"markdown","b65297ba":"markdown","76a1f297":"markdown","bd72982b":"markdown","91205177":"markdown","49f42eb0":"markdown","57cce34a":"markdown","f8b10d07":"markdown","d99e4c81":"markdown","2f731fc9":"markdown","8345f270":"markdown","06ed2351":"markdown","e3ef437a":"markdown","c93b91b4":"markdown","58111335":"markdown","0e8615c8":"markdown","6ec19aac":"markdown","43d04c14":"markdown","09df72f9":"markdown","7b4d2ea9":"markdown","b7a212d1":"markdown","1b9a993b":"markdown","339e982a":"markdown","44d80cbc":"markdown","d2f72672":"markdown","c523bc13":"markdown","d0ee23f2":"markdown","afb84b68":"markdown","f487cace":"markdown","652c4c59":"markdown","373c8b71":"markdown","44756699":"markdown","b94d8b29":"markdown","1727ecb8":"markdown"},"source":{"862f6188":"%matplotlib inline","6872cf22":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')","92e5c7ff":"img = Image.open('..\/input\/image1\/img.jpg').convert('L')\nplt.imshow(img,'gray')","46e1e7ac":"# 2D kernel\nweight = torch.tensor([[[[1,0,-1],[1,0,-1],[1,0,-1]]]]).float()","088e1a53":"# 3D kernel\nweight = torch.tensor([[[[1,0,-1],[1,0,-1],[1,0,-1]], \n                        [[1,0,-1],[1,0,-1],[1,0,-1]], \n                        [[1,0,-1],[1,0,-1],[1,0,-1]]]]).float()","3f1d6119":"weight = torch.tensor([[[[1,0,-1],[2,0,-2],[1,0,-1]]]]).float()","24687ddd":"weight = torch.tensor([[[[1,2,1],[0,0,0],[-1,-2,-1]]]]).float()","036aaee2":"tf = transforms.ToTensor()\nimg = tf(img)\nimg = img.unsqueeze_(0)\nprint(img.shape)","aba6b344":"op = F.conv2d(img, weight, bias=None, stride=1, padding=0, dilation=1)","4211b33e":"op = op.squeeze_(0)\ntf_ = transforms.ToPILImage()\nop = tf_(op).convert('L')\nplt.imshow(op,'gray')","6ac800b5":"op = F.conv2d(img, weight, bias=None, stride=1, padding=1000, dilation=1)","0a562a71":"op = op.squeeze_(0)\ntf_ = transforms.ToPILImage()\nop = tf_(op).convert('L')\nplt.imshow(op,'gray')","e2012c4c":"op = F.conv2d(img, weight, bias=None, stride=2, padding=0, dilation=1)","ba67ab91":"op = op.squeeze_(0)\ntf_ = transforms.ToPILImage()\nop = tf_(op).convert('L')\nplt.imshow(op,'gray')","a183c47c":"img = Image.open('..\/input\/image1\/img.jpg')\nplt.imshow(img)","d16e1ef6":"im = np.asarray(img)\nim","b219b893":"im.shape","a7b55b67":"plt.imshow(im[:,:,0], 'gray')","4832c890":"plt.imshow(im[:,:,0], 'Reds')","e2afb512":"plt.imshow(im[:,:,1], 'Greens')","92c4d5bb":"plt.imshow(im[:,:,2], 'Blues')","90c7de44":"im_r = im[:,:,0]\nim_g = im[:,:,1]\nim_b = im[:,:,2] * 2\nim_r = np.reshape(im_r, (2322,4128,1))\nim_g = np.reshape(im_g, (2322,4128,1))\nim_b = np.reshape(im_b, (2322,4128,1))\nimg = np.concatenate((im_r,im_g,im_b),axis=2)\nplt.imshow(img)","3b7e43fd":"inp = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]], dtype=np.float32)\ninp","00ede2ba":"inp = torch.from_numpy(inp)\nop_ = F.max_pool2d(inp.unsqueeze_(0), kernel_size=2,stride=2)\nop_ = op_.squeeze_(0)\nprint(np.asarray(op_))","c981a5ee":"op1 = F.max_pool2d(tf(op).unsqueeze_(0), kernel_size=3,stride=2)\nop1 = op1.squeeze_(0)\nop1 = tf_(op1).convert('L')\nplt.imshow(op1,'gray')","56ed7d4b":"op_ = F.avg_pool2d(inp.unsqueeze_(0), kernel_size=2,stride=2)\nop_ = op_.squeeze_(0)\nprint(np.asarray(op_))","09c1685a":"op2 = F.avg_pool2d(tf(op).unsqueeze_(0), kernel_size=3,stride=2)\nop2 = op2.squeeze_(0)\nop2 = tf_(op2).convert('L')\nplt.imshow(op2,'gray')","47e06fbf":"# CUDA for PyTorch\nfrom torch.backends import cudnn\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\ncudnn.benchmark = True","37c5a492":"trainset = torchvision.datasets.FashionMNIST('.\/data', train=True, transform=transforms.ToTensor(), download=True)\ntestset = torchvision.datasets.FashionMNIST('.\/data', train=False, transform=transforms.ToTensor(), download=True)","734b63a0":"#loading the training data from trainset\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle = True)\n#loading the test data from testset\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)","7baa39b0":"class_labels = ['T-Shirt','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']\nfig = plt.figure(figsize=(15,8));\ncolumns = 5;\nrows = 3;\nfor i in range(1, columns*rows +1):\n    index = np.random.randint(len(trainset))\n    img = trainset[index][0][0, :, :]\n    fig.add_subplot(rows, columns, i)\n    plt.title(class_labels[trainset[index][1]])\n    plt.axis('off')\n    plt.imshow(img, cmap='gray')\nplt.show()","ab58e5da":"class Basic_conv(nn.Module):\n    def __init__(self):\n        super(Basic_conv, self).__init__()\n        self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=1,padding=0)\n        self.pool = nn.MaxPool2d(kernel_size=3,stride=1,padding=0)\n        self.conv2 = nn.Conv2d(16,32,kernel_size=3,stride=1,padding=0)\n        self.fc1 = nn.Linear(15488, 64, bias=True)\n        self.fc2 = nn.Linear(64, 10, bias=True)\n        self.drop = nn.Dropout(0.5)\n        # or nn.ReLU()\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = torch.relu(x)\n        x = self.pool(x)\n        x = torch.relu(x)\n        x = self.conv2(x)\n        x = torch.relu(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.drop(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        return x","444ec769":"model = Basic_conv()\nif use_cuda:\n    model.to(device)\nprint(model)\n\ncross_entropy_loss = nn.CrossEntropyLoss()\n\nadam_optim = torch.optim.Adam(model.parameters(), lr=0.005)\nprint(adam_optim)","8e89484d":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\ndef eval_perfomance(model, dataloader, print_results=False):\n    actual, preds = [], []\n    #keeping the network in evaluation mode  \n    model.eval() \n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        actual +=[i.item() for i in labels]\n        \n        #moving the inputs and labels to gpu\n        outputs = model(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        preds += [i.item() for i in pred]\n    acc = accuracy_score(actual, preds)\n    cm = confusion_matrix(actual, preds)\n    cr = classification_report(actual, preds)\n    if(print_results):\n        print(f'Total accuracy = {acc*100}%')\n        print('\\n\\nConfusion matrix:\\n')\n        print(cm)\n        print('\\n\\nClassification Report:\\n')\n        print(cr)\n    \n    return acc","6c55f6ae":"intial_acc = eval_perfomance(model, testloader, True)","b82a0a5b":"# %%time\nloss_arr = []\nloss_epoch_arr = []\nmax_epochs = 5\niter_list = []\ntrain_acc_arr = []\ntest_acc_arr = []\nctr = 0\nfor epoch in range(max_epochs):\n    for i, data in enumerate(trainloader, 0):\n        \n        model.train()\n        images, labels = data\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)      \n        \n        loss = cross_entropy_loss(outputs, labels)    \n        adam_optim.zero_grad()     \n\n        loss.backward()     \n        adam_optim.step()     \n        loss_arr.append(loss.item())\n        \n        ctr+=1\n        iter_list+=[ctr]\n    \n        \n    train_acc = eval_perfomance(model, trainloader)\n    train_acc_arr+=[train_acc.item()]\n        \n    test_acc = eval_perfomance(model, testloader)\n    test_acc_arr+=[test_acc.item()]\n        \n    if((epoch+1)%1==0):\n        print(f\"Iteration: {epoch+1}, Loss: {loss.item()}, Train Acc:{train_acc}, Val Acc: {test_acc}\")\n\n    loss_epoch_arr.append(loss.item()) ","30b34d43":"plt.plot([i for i in range(len(loss_epoch_arr))], loss_epoch_arr)\nplt.xlabel(\"No. of Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss vs Iterations\")\nplt.show()","074f9285":"plt.plot([i for i in range(len(test_acc_arr))], test_acc_arr)\nplt.xlabel(\"No. of Iteration\")\nplt.ylabel(\"Test Accuracy\")\nplt.title(\"Test Accuracy\")\nplt.show()","201738b7":"class_correct = [0. for _ in range(10)]\ntotal_correct = [0. for _ in range(10)]\n\nwith torch.no_grad():\n    for images, act_labels in testloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        predicted = torch.max(outputs, 1)[1]\n        if use_cuda:\n            c = (predicted == act_labels.cuda()).squeeze()\n        else:\n            c = (predicted == act_labels).squeeze()\n        \n        for i in range(4):\n            label = act_labels[i]\n            class_correct[label] += c[i].item()\n            total_correct[label] += 1\n        \nfor i in range(10):\n    print(\"Accuracy of {}: {:.2f}%\".format(class_labels[i], class_correct[i] * 100 \/ total_correct[i]))","9d792ea0":"final_acc_test = eval_perfomance(model, testloader, print_results=True)","27ae8d9e":"def get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\nprint(get_n_params(model))","a7907155":"resnet = torchvision.models.resnet34(pretrained=True, progress = True)\nresnet","0664e9d2":"for name, param in resnet.named_parameters():\n    print(name, param.requires_grad)","ec0494d8":"for name, param in resnet.named_parameters():\n    if name=='fc.weight'or name == 'fc.bias':\n        break\n    param.requires_grad = False","a965f591":"resnet.fc = nn.Linear(in_features=512, out_features=10, bias=True)\nresnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)","d304d3d3":"for name, param in resnet.named_parameters():\n    print(name, param.requires_grad)","dbf42587":"if use_cuda:\n    resnet.to(device)\n\ncross_entropy_loss = nn.CrossEntropyLoss()\n\nadam_optim = torch.optim.Adam(resnet.parameters(), lr=0.005)\nprint(adam_optim)","412871fa":"intial_acc = eval_perfomance(resnet, testloader, True)","0dd927ec":"# %%time\nloss_arr = []\nloss_epoch_arr = []\nmax_epochs = 3\niter_list = []\ntrain_acc_arr = []\ntest_acc_arr = []\nctr = 0\nfor epoch in range(max_epochs):\n    for i, data in enumerate(trainloader, 0):\n        \n        model.train()\n        images, labels = data\n        images, labels = images.to(device), labels.to(device)\n        outputs = resnet(images)      \n        \n        loss = cross_entropy_loss(outputs, labels)    \n        adam_optim.zero_grad()     \n\n        loss.backward()     \n        adam_optim.step()     \n        loss_arr.append(loss.item())\n        \n        ctr+=1\n        iter_list+=[ctr]\n    \n        \n    train_acc = eval_perfomance(resnet, trainloader)\n    train_acc_arr+=[train_acc.item()]\n        \n    test_acc = eval_perfomance(resnet, testloader)\n    test_acc_arr+=[test_acc.item()]\n        \n    if((epoch+1)%1==0):\n        print(f\"Iteration: {epoch+1}, Loss: {loss.item()}, Train Acc:{train_acc}, Val Acc: {test_acc}\")\n\n    loss_epoch_arr.append(loss.item()) ","98043610":"intial_acc = eval_perfomance(resnet, testloader, True)","192a7ad7":"### ResNets\nAs we have learned earlier that increasing the number of\nlayers in the network abruptly degrades the accuracy.\nThe deep learning community wanted a deeper network\narchitecture that can either perform well or at least the\nsame as the shallower networks. Now, try to imagine a\ndeep network with convolution, pooling, etc layers\nstacked one over the other. Let us assume that, the\nactual function that we are trying to learn after every\nlayer is given by Ai(x) where A is the output function of\nthe i-th layer for the given input x.\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1pgjol3dPpf3DbVpA_DSdgVcWihnfQO7t\">","bf136450":"## Connecting all the Blocks\n\nVarious layers in order of a classic ConvNet architecture:\n\n* Input Layer\n\n* Convolutional Layer + Activation\n\n* Pooling\n\n* Flatten to Fully Connected Layer\n\n* Output Layer\n\nThese layers together can be considered as one block and\nthese blocks can be repeated various number of times\nmaking the ConvNet very deep.\n\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1bV9aZA_cjd4MsREC1qn-9nsHWKdPNQ3E\">","cbfb2cff":"## Contents\n\n* Computer Vision\n\n* Convolution Operation on 2D Matrices\n\n* Edge Detection\n\n* Padding\n\n* Strided Convolutions\n\n* Organization of RGB Images\n\n* Convolutions on 3D Volumes\n\n* Pooling\n\n* Fully Connected Layer\n\n* Output Layer\n\n* Implementation of a Convolutional Neural Net (CNN) using Keras\n\n* Why CNNs Over Fully Connected Neural Networks?","9051ba55":"## Why ConvNets?\n\nWeight Sharing: This is an important property of CNNs which gives it an edge over normal\nneural networks in terms of computational efficiency.\n\nLet\u2019s take an example :\n\nSay you have a one layered CNN with 10 filters of size 5x5. Now you can simply\ncalculate parameters of such a CNN, it would be 5x5x10 weights and 10 biases\ni.e 5x5x10 + 10 = 260 parameters.\n\nNow let\u2019s say we have a simple neural network layer with 20 neurons and an input\nimage(64x64) which is converted into a vector. Now the number of weights\nassociated with this layer will be 20x64x64+1= 81,921 parameters!\n\nAs we create more layers there will be too many parameters to handle and hence\nConvNets are preferred as they reduce the number of weights required to be trained\ndrastically giving an advantage in efficiency of the model.\n\nThe weights trained on one part\/feature of the image might also pickup important\ninformation from other parts of the image and hence weight sharing is beneficial.","8c0a3213":"## Types of Pooling\n\nA filter of required dimension is chosen and is moved from one\nend of the matrix by moving a gap of pixels specified by the\nstride assigned.\n**Note: Depth of a pooled matrix will remain same as before it was pooled.**\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1ZO-FtIOhICemh0V-404YDs22Uv5dBvMe\">\n<br \/><br \/>\nTaking example of a random 4x4 array.","f0669557":"### STRIDED CONVOLUTIONS\n\nEarlier we have seen that a filter starts from one\nend of the input matrix and moves one pixel\neither rowwise or columnwise. Stride is the\nnumber of pixels shifted over by the filter on\nthe input matrix. When the stride is 1 then we\nmove the filters 1 pixel at a time. When the\nstride is 2 then we move the filters 2 pixels at a\ntime and so on. The figure shows how\nconvolution would work with a stride of 2.\n\nDimensions of output matrix with a stride \u201cs\u201d:\n\n`GIF[(n+2p-f)\/s+1] X GIF [(n+2p-f)\/s+1]`\n\n*GIF refers to Greatest Integer Function.*","cb5839c7":"## Transfer Learning\nFor a completely new task \/ problem CNNs are very good feature extractors.\n\nThis means that you can extract useful attributes from an already trained CNN with its\ntrained weights by feeding your data on each level and tune the CNN a bit for the specific\ntask. Eg : Add a classifier after the last layer with labels specific to the task.\n\nThis is also called pre-training and CNNs are very efficient in such tasks compared to\nNNs. Another advantage of this pre-training is we avoid training of CNN and save\nmemory, time. The only thing you have to train is the classifier at the end for your labels.\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1N734GlaPZA93ZG9ZqKdM8aOovV9mwDoj\">","b0cbd315":"Color images are stored as 3D arrays of pixel values, where the 3<sup>rd<\/sup> dimension corresponds to the colour values, i.e. the Red, Green and Blue pixel values.","6ff1df2c":"## ORGANIZATION OF RGB IMAGES\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1YGDXsJHcWtHLfdzFW2UoZ4P2bZLOEsJp\">","0411f723":"### Two filters being used at once:\n(Observe how it is detecting a vertical as well as horizontal edge at one go)\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1dxiUAfBPvyshiqiO1IT5_kW8TlzuWzGj\">\n<br \/>\n### Similarly for multiple filters\n<br \/>\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1pwulglqYnel1yuyvJsYhOnRYk5UPqo5_\">","b518b5c0":"## Fully Connected Layers\n\nOnce we have extracted all the vital features of\nan image using Convolutions followed by\nPooling. We can strip the volume into a single\nvector containing all the important features.\nNow this reduces to the normal layer which we\nsee in a traditional neural network.\n\nThis layer is present before the output layer.\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1FvdoOXZl6WFtHpiHrFgIlGpkgqtx-R5B\">","b65297ba":"## Activating a ConvNet\n\nIt is important to understand that like in any other\nneural network, a convolutional neural network\nalso has the input data x which is an image here\nand model weights given by the filters F1 and F2\ni.e. W. Once the weights and the input image is\nconvolved we get the weighted output W * x and\nthen we add the bias b. \u2018b\u2019 is a (1*1*nD) matrix\nand will be broadcasted automatically in Python.\n\nOne of the key functions here is\nthe RELU activation function, which is rectified\nlinear unit. Relu helps us add non-linearity to the\nlearning model and helps us better train\/ learn the\nmodel weights for the generalized case.<br \/>\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=18aBRKUBiBU93_ZWMHplTbIsgkknzFmzo\">","76a1f297":"On multiplying the Blue(i.e. `im[:,:,2]`) value slice of the image, the combined RGB image appears to have a bluish tint.","bd72982b":"## Inception Network\n\n### The Premise:\n\n**Salient parts** in the image can have extremely large variation in size. For instance, an\nimage with a dog can be either of the following, as shown below. The area occupied by\nthe dog is different in each image.\n\nBecause of this huge variation in the location of the information, choosing the **right\nkernel size** for the convolution operation becomes tough. A **larger kernel** is preferred\nfor information that is distributed more **globally**, and a **smaller kernel** is preferred for\ninformation that is distributed more **locally**.\n\n**Very deep networks** are prone to **overfitting**. It also hard to pass gradient updates\nthrough the entire network.\n\nNaively stacking large convolution operations is **computationally expensive**.","91205177":"## Using Multiple Filters\n\nNeed to extract a variety of different features from images.\n\nDifferent filters learn different features of the image.\n\nStacking all the output matrices after convolving with different filters will save a lot of\ncomputational time compared to individually convolving different filters.\n\nConvolution is carried out individually and then results are merged together in a stack\nwith the depth of the output matrix being equal to number of filters.\n\nNow the new dimensions of the output matrix are\n\\[GIF[(n<sub>H<\/sub>+2p-f)\/s+1] * GIF[(n<sub>W<\/sub>+2p-f)\/s+1] \\]*n<sub>f<\/sub> (n<sub>f<\/sub> => number of filters)","49f42eb0":"### PADDING\nPadding is simply a process of adding layers of pixels,preferably\nzeros to our input images so as to avoid the problems mentioned\npreviously. This layer of zeros can be visualized as a frame added to\nthe image and it will not disturb the features of the image. You now\nhave control over the dimensions of your output matrix by changing\nthe amount of padding as you wish.\n\nThe new change in dimensions is:\n\n`[(n + 2p) x (n + 2p) image] * [(f x f) filter] :[(n +2p-f+1x n+2p-f+1)image]`\n\np -> Number of padding layers\n\nIf you pad an image such that it retrieves it\u2019s dimensions after\nconvolution, we call it SAME CONVLUTION.\n\nIn this case `p = (f-1)\/2`\n\nProof: `n+2p-f+1 = n => p = (f-1)\/2`\n\nWithout padding the convolution is called VALID CONVOLUTION.\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1lMFXAL-ertiQNM--4cu0lYdnqtUohZ5o\">","57cce34a":"# Convolutional Neural Networks using PyTorch","f8b10d07":"#### Basic properties and assumptions regarding the identity connection:\n\n* The addition of the identity connection does not introduce extra parameters. Therefore, the computation complexity for simple deep networks and deep residual networks is almost the same.\n\n* The dimensions of x and F must be the same for performing the addition operation. The dimensions can be matched by using one of the following ways: <br \/> 1. Extra Zero entries should be padded for increasing dimensions. This is not going to introduce any extra parameter. <br \/> 2. Projection shortcut can be used to match the dimensions ( 1 x 1 convolutions).<br \/> y = F(x, {W<sub>i<\/sub>}) + W<sub>s<\/sub> x","d99e4c81":"**Modifying final and initial layers to suite our dataset**\n* Refactoring final fully connected layer to give 10 output activations instead of the 1000 required for Imagenet\n* Similarly, refactoring the initial convolution layer to take in grayscale image, instead of an RGB image i.e. with 3 channels.","2f731fc9":"### The Solution:\n\nWhy not have filters with multiple sizes operate on the same level? The network\nessentially would get a bit \u201cwider\u201d rather than \u201cdeeper\u201d. The authors designed the\ninception module to reflect the same.\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1pXI7_YwO4hQlSqhu_kDn6R5m5AS9BccD\">\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1iQwQxqcHlUC6bd0n5M9F4jIMJETjOyPo\">\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1uZTiy8ZVMDRf5Gqeh8TDUzUzjkwpDhn-\">","8345f270":"### 2D Convolution in torch.nn\nTo apply the edge detection kernels to the image, we use PyTorch Functional Convolutions.\n```python\nCLASStorch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\n```","06ed2351":"## Few Applications of Computer Vision\n* Computer Vision for Intruder\/Face Detection\n\n* Computer Vision for Defect detection\n\n* Self Driving Cars\n\n* Meteorological Purposes like Weather Prediction\n\n* And Many More\u2026..\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1BuUsWrIFKFCXD7DIVrSQeQ62JFxNPJvz\" width=\"500\" height=\"600\">\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1KM9isYZ9DGI0guBg_v7zReeURq_6ex5g\" width=\"500\" height=\"600\">\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1UzBE-JX33rKr343-8wLQApb8uC6BNI4s\" width=\"500\" height=\"600\">","e3ef437a":"## Computer Vision\n\n* Computer Vision is an interdisciplinary field of science that aims to make computers process, analyze images and videos and extract details in the same way a human mind does. Deep Neural Networks (DNN) have greater capabilities for image pattern recognition and are widely used in Computer Vision algorithms. And, Convolutional Neural Network (CNN, or ConvNet) is a class of DNN which is most commonly applied to analyzing visual imagery. You would have certainly come across many devices using CNNs in your day to day life.\n\n* As you might have figured out , J.A.R.V.I.S is probably using a ConvNet to detect incoming missiles! We\u2019ll never know for sure as Ironman is no more \u2639!","c93b91b4":"### Constructing a Basic Convnet","58111335":"Difference in torch.nn and torch.nn.functional","0e8615c8":"## Into the mind of a CNN\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1d7soXAiMwNDB7Ly_27cuoe2S_8hEptD7\" width=600 height=500>\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1bOXh6rKzeAVe8KujVx0IABIE-LA2U7Jj\">","6ec19aac":"## Extra links to resources\n1. [Going deeper with convolutions](https:\/\/arxiv.org\/pdf\/1409.4842.pdf) - Paper on inception network \n2. [Neural Network Simulation](https:\/\/www.youtube.com\/watch?v=3JQ3hYko51Y)","43d04c14":"## Convolution Operation on 2D Matrices\n\nConvolution is a method in which a filter\/kernel is convolved on\nan image to extract important features of the image.\n\nThe filter\/kernel is put on one end of the image, an element-wise\nproduct is taken and this product is added to the output unit.\nThis process is repeated by moving the filter row-wise as well as\ncolumn-wise thus scanning through the whole matrix.\n\nThe \u2018*\u2019 symbol denotes convolution.\n\nNumPy function for Element-Wise Product:\n`np.multiply(matrix1,matrix2)`\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1cd64XE_16dLaC-QqkaBimFcN1DaHrWHV\" width=\"500\" height=\"600\">","09df72f9":"Importing all required modules.","7b4d2ea9":"#### Advantages of ResNets\n\n1. Vanishing Gradients : In some cases some neuron can \u201cdie\u201d in the training and become ineffective. This can cause information loss, and make the model weak.\n\n2. Optimization Difficulty : If parameters like weights, biases increases due to increasing depth, training the network becomes very difficult. Even this causes in higher training errors.\n\nTo summarize, we can say that skip connection introduced in ResNet architecture have\nhelped a lot to increase the performance of the neural network with large number of layers.\nResNets are basically like different networks with slight modification. The architecture\ninvolves same functional steps like in CNN or others but an additional step is introduced to\ntackle with certain issues like vanishing gradient problem etc.","b7a212d1":"### Training a ConvNet\n\nThis process is exactly the same as training a basic neural network. The weights have to be\noptimized by an algorithm like Gradient Descent which minimizes the cost function in each\niteration and thus trains the CNN.\n\nThe number of iterations\/epochs is the number of times we iterate over all the batches of data retreived from the dataloader.\n\nWe don\u2019t have to worry about Backpropagation as PyTorch takes care of this.","1b9a993b":"## Fashion MNIST Dataset","339e982a":"## How Does Our Brain Perceive Images?\n\nIs it a man with the right face? Or is it a man who is\nlooking at you directly? If your attention is at the nose\nor the right contour, you\u2019ll probably say the former. If\nyou see the ear or the left contour, you\u2019ll say the latter.\nWhat do you think?\n\nThis picture is showing us how we recognize images.\nThe contours and edges of an image affect our\nperception. You might not have noticed, but our brain\ncaptures the patterns in figures to classify an object.\nAnd the kick of CNN lies here. Our machine will be\ntrained to classify images by detecting some patterns\nlike us.\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1LEZmMooquK4nxpil3BwqHuCWeZwJPmg8\" width=\"500\" height=\"600\">","44d80cbc":"##### Handcrafted convolutional kernels, for edge detection","d2f72672":"## Pooling\n\n* A limitation of the feature map output of convolutional layers is that they record the precise position of features in the input. This means that small movements in the position of the feature in the input image will result in a different feature map.\n\n* Pooling Layers provide an approach to sample down features of an output matrix by summarizing the features of that matrix.\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1fOKZKnhFomQLPPNGgwWjAXQZwWFhVOyg\">","c523bc13":"#### Freezing Layers\nFreezing the weights in most pretrained layers, reduces the amount of gradients on which we have to backpropagate, thereby reducing our resource consumption while enabling us to take maximum advantage of the pretraining done by larger corporations with more compute power.","d0ee23f2":"## Computer Vision\n\n#### Ever wondered how J.A.R.V.I.S helps Ironman detect incoming missiles?\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=11dbiJuUg37lHWT5xNJxzZjLsKEh7Qrjy\" width=600 height=500>","afb84b68":"## Edge and Feature Detection by CNNs\n\nThe first image has a transition from light to dark and\nhence consists of an edge which is detected by the output\nmatrix as shown in the top figure. This edge may seem\ntoo big to you in the output layer but this is because the\nimage taken is very small. In real implementations we\nwill be using images of very high dimensions.\n\nExtending this concept of detecting edges, CNNs can\ndetect contours along a boundary like the eyes of a person\nas shown in the image on the right.\n\nNow you might be wondering what should my filter\nvalues be ? Should I use a certain filter which is\nman-made?\n\nThe answer to these questions is NO! In the deep learning\nera we can consider the values of the filters as weights\nand train or optimize them.\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1wyEYsUA6kstOZ7FHmVz06B9aPQQ9BNk2\">\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1XULl8q7jJGm6hH65vBLqKtj554-DA3-3\">","f487cace":"### ImageNet\n\nThe ImageNet project is a large visual database designed for use in visual object\nrecognition software research. More than 14 million images have been\nhand-annotated by the project to indicate what objects are pictured and in at least\none million of the images, bounding boxes are also provided. ImageNet contains\nmore than 20,000 categories with a typical category, such as \"balloon\" or\n\"strawberry\", consisting of several hundred images. The database of annotations\nof third-party image URLs is freely available directly from ImageNet, though the\nactual images are not owned by ImageNet. Since 2010, the ImageNet project runs\nan annual software contest, the ImageNet Large Scale Visual Recognition\nChallenge (ILSVRC), where software programs compete to correctly classify and\ndetect objects and scenes. The challenge uses a \"trimmed\" list of one thousand\nnon-overlapping classes.<br \/>\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1QXt6rbPbNjUDtiz5ti6ypJ_HAdfEuBRA\">","652c4c59":"### Max Pooling\nIn each step taken by the filter the maximum value of the pixel values it overlaps with is added to output layer ignoring the other features in that set.","373c8b71":"### Average Pooling\nIn each step taken by the filter the average value of the pixel values it overlaps with is added to output layer.","44756699":"## CONVOLUTIONS ON A 3D VOLUME\n\nSince we\u2019ll be dealing mostly with RGB images with\ndimensions (64,64,3) , our ConvNet will be performing\nconvolutions on a Volume rather than an Area.\n\nTerms used frequently:\n\nf= filter size\n\np= padding\n\nnH= height of matrix\n\nnW= width of matrix\n\nnD= depth of matrix (This is always equal to depth of filter)\n\n3D Convolution:\n\n[(nH+2p)*(nW+2p)*(nD)] * (Filter(f*f*nD)) ->\n\n[GIF[(nH+2p-f)\/s+1]*GIF[(nW+2p-f)\/s+1]]\n\nNOTE: The Final Output will be a 2D Matrix\n\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1dS0IrkAdhQ7FyHysX8Y-gcGwDwXmPSZU\">\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1vYORqmXv15jn9qbKWNW0rVO89l1heNy8\">","b94d8b29":"### Why use Multiple Fully Connected Layers?\n<img src = \"https:\/\/drive.google.com\/uc?export=view&id=1x70oouczAXNE8uJ77Ty1MIXwpKbkJ_Td\">","1727ecb8":"## Output Layer\n\nThe final layer in the ConvNet is known as the output layer.\nThis outputs the final result predicted by your model.\n\nDepending on the task, various output layer sizes and final\nactivation functions can be chosen:\n\n* **Binary Classification** - > 1 neuron (Sigmoid Activation) <img src = \"https:\/\/drive.google.com\/uc?export=view&id=1hDAA29728eMmdANt8CxRsVVcZMSNWKRV\"><br \/><br \/>\n\n* **Multi-Class Classification** - > (k classes)-> k+1 neurons (Softmax Classifier) <img src = \"https:\/\/drive.google.com\/uc?export=view&id=1CC0xZZ-JhKMyMTzN8mmFlQTMaPBmkPpy\"><br \/><br \/>\n\n* **Multi-Object Classification** - > Output layer is a volume (YOLO Algorithm) <img src = \"https:\/\/drive.google.com\/uc?export=view&id=1boHTDOK17aaflL3ttdTuVNxraIeKfQuz\"><br \/><br \/>\n\n"}}