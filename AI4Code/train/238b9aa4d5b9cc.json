{"cell_type":{"88b8dc26":"code","05915ccf":"code","af228087":"code","5d3a2947":"code","a49b94ab":"code","2e0924fc":"code","13cfb423":"code","3683830f":"code","3a3dd52a":"code","31474221":"code","4358d746":"code","56facd52":"code","62fa40b9":"code","0c012086":"code","3ad78f88":"code","c85b953d":"code","38cbda56":"code","29096725":"code","99547912":"code","f9f547ef":"code","b6aee70d":"code","ed037b04":"code","bdb596e7":"code","f68da502":"code","22d93a1b":"code","ecd28b1b":"code","0db99b5f":"code","1b351a46":"code","67989b06":"code","1a143c3d":"code","9734c568":"code","a3f92b2c":"code","0b029c9a":"code","412e2d0e":"code","b334cf73":"code","85d6ac71":"code","359d8c74":"code","68073c3e":"code","b90f05c9":"code","bbc988c5":"code","762bbfae":"code","24a70df1":"code","bcfeb63d":"code","b3353678":"code","7fac8795":"code","66a9dbcd":"code","c80747a0":"code","82b66985":"code","7eac06c3":"code","3bacd0f2":"code","93717425":"code","528de65e":"markdown","665043c0":"markdown","c7a36459":"markdown","932df5fb":"markdown","b51b87b4":"markdown","efa1963f":"markdown","d846055e":"markdown","1390b517":"markdown","c9d822a9":"markdown","6a5e920b":"markdown","e82a1a92":"markdown","12037470":"markdown","9d652154":"markdown","0e424d31":"markdown","708646ae":"markdown","efd321ca":"markdown","7c5cbdfa":"markdown","eee43780":"markdown","83be4d15":"markdown"},"source":{"88b8dc26":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport nltk\nimport sys\nimport spacy\nimport re\nimport numpy as np\n\nfrom nltk.corpus import stopwords","05915ccf":"df = pd.read_json(\"..\/input\/reviews\/reviews.json\",lines=True)\ndf.head()","af228087":"new_df = df[['reviewText', 'overall']]\ntrain, valid = train_test_split(new_df, test_size=0.33, random_state=42)\ntrain.shape, valid.shape","5d3a2947":"train_samp = train.sample(frac=.03, random_state=42)\nvalid_samp = valid.sample(frac=.03, random_state=42)\ntrain_samp.shape, valid_samp.shape","a49b94ab":"df['overall'].hist()","2e0924fc":"BAD_SYMBOLS_RE = re.compile('[^a-z #+_]')\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef clean_text(text):\n    text = text.lower()\n    text = BAD_SYMBOLS_RE.sub('', text)\n    \n    doc = nlp(text)\n\n    lemma_list = []\n    for token in doc:\n        lemma_list.append(token.lemma_)\n    \n    #Filter the stopword\n    filtered_sentence =[] \n    for word in lemma_list:\n        lexeme = nlp.vocab[word]\n        if lexeme.is_stop == False:\n            filtered_sentence.append(word) \n    \n    #Remove punctuation\n    punctuations=\"?:!.,;\"\n    for word in filtered_sentence:\n        if word in punctuations:\n            filtered_sentence.remove(word)\n    return \" \".join([word for word in filtered_sentence])","13cfb423":"text = train['reviewText'][942117]\nprint(text, '\\n')\nprint(clean_text(text))","3683830f":"train_normilized = train_samp.copy()\nvalid_normilized = valid_samp.copy()\ntrain_normilized = train_normilized['reviewText'].astype('str').apply(clean_text)\nvalid_normilized = valid_normilized['reviewText'].astype('str').apply(clean_text)","3a3dd52a":"X_train = train_normilized\ny_train = train_samp['overall']\nX_valid = valid_normilized\ny_valid = valid_samp['overall']","31474221":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef tfidf_features(x_train, x_val):\n\n    tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1, 2), token_pattern='(\\S+)')\n    x_train = tfidf_vectorizer.fit_transform(x_train)\n    x_val = tfidf_vectorizer.transform(x_val)\n    \n    return x_train, x_val, tfidf_vectorizer.vocabulary_","4358d746":"X_train_tfidf, X_valid_tfidf, tfidf_vocab = tfidf_features(X_train, X_valid)","56facd52":"#Multinomial Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB().fit(X_train_tfidf, y_train)\ny_pred = clf.predict(X_valid_tfidf)","62fa40b9":"from sklearn.metrics import precision_score\nprecision_score(y_valid, y_pred, average='weighted', zero_division=0)","0c012086":"from sklearn.metrics import recall_score\nrecall_score(y_valid, y_pred, average='weighted', zero_division=0)","3ad78f88":"from sklearn.metrics import accuracy_score\naccuracy_score(y_valid,y_pred)","c85b953d":"from sklearn.metrics import f1_score\nf1_score(y_valid,y_pred,average='weighted',zero_division=0)","38cbda56":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\n\ny_train_copy_under=y_train.copy()\n\nX_train_copy_under=X_train_tfidf.copy()\n\nundersample = RandomUnderSampler(sampling_strategy='not minority')\n\nX_train_under, y_train_under = undersample.fit_resample(X_train_copy_under, y_train_copy_under)","29096725":"y_train_under.hist()","99547912":"#Multinomial Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB().fit(X_train_under, y_train_under)\ny_pred_under = clf.predict(X_valid_tfidf)","f9f547ef":"from sklearn.metrics import precision_score\nprecision_score(y_valid, y_pred_under, average='weighted', zero_division=0)","b6aee70d":"from sklearn.metrics import recall_score\nrecall_score(y_valid, y_pred_under, average='weighted', zero_division=0)","ed037b04":"from sklearn.metrics import accuracy_score\naccuracy_score(y_valid,y_pred_under)","bdb596e7":"from sklearn.metrics import f1_score\nf1_score(y_valid,y_pred_under,average='weighted',zero_division=0)","f68da502":"import imblearn\nfrom imblearn.over_sampling import RandomOverSampler\n\ny_train_copy_over=y_train.copy()\n\nX_train_copy_over=X_train_tfidf.copy()\n\noversample = RandomOverSampler(sampling_strategy='not majority')\n\nX_train_over, y_train_over = oversample.fit_resample(X_train_copy_over, y_train_copy_over)","22d93a1b":"y_train_over.hist()","ecd28b1b":"#Multinomial Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB().fit(X_train_over, y_train_over)\ny_pred_over = clf.predict(X_valid_tfidf)","0db99b5f":"from sklearn.metrics import precision_score\nprecision_score(y_valid, y_pred_over, average='weighted', zero_division=0)","1b351a46":"from sklearn.metrics import recall_score\nrecall_score(y_valid, y_pred_over, average='weighted', zero_division=0)","67989b06":"from sklearn.metrics import accuracy_score\naccuracy_score(y_valid,y_pred_over)","1a143c3d":"from sklearn.metrics import f1_score\nf1_score(y_valid,y_pred_over,average='weighted',zero_division=0)","9734c568":"from sklearn.svm import SVC\n\nclf = SVC(kernel='linear', decision_function_shape='ovo').fit(X_train_tfidf, y_train)\ny_pred_svm = clf.predict(X_valid_tfidf)","a3f92b2c":"from sklearn.metrics import precision_score\nprecision_score(y_valid, y_pred_svm, average='weighted', zero_division=0)","0b029c9a":"from sklearn.metrics import recall_score\nrecall_score(y_valid, y_pred_svm, average='weighted', zero_division=0)","412e2d0e":"from sklearn.metrics import accuracy_score\naccuracy_score(y_valid,y_pred_svm)","b334cf73":"from sklearn.metrics import f1_score\nf1_score(y_valid,y_pred_svm,average='weighted',zero_division=0)","85d6ac71":"# The maximum number of words to be used. (most frequent) (19782)\nMAX_NB_WORDS = 5000\n# Max number of words in each review. (927)\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 200","359d8c74":"import tensorflow as tf\ntok = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS)\nall_texts = X_train.append(X_valid)\ntok.fit_on_texts(all_texts)\nsequences_train=tok.texts_to_sequences(X_train)\nX_train_for_lstm = tf.keras.preprocessing.sequence.pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\nsequences_valid=tok.texts_to_sequences(X_valid)\nX_valid_for_lstm = tf.keras.preprocessing.sequence.pad_sequences(sequences_valid, maxlen=MAX_SEQUENCE_LENGTH)","68073c3e":"X_train_for_lstm.shape","b90f05c9":"y_train_one_hot = pd.get_dummies(y_train)\ny_valid_one_hot = pd.get_dummies(y_valid)","bbc988c5":"y_valid_one_hot.shape","762bbfae":"from keras import backend as K\ndef f1(y_true, y_pred):\n    \n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n    \n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","24a70df1":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(tf.keras.layers.Dense(5, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam', metrics=[tf.keras.metrics.Precision(),\n               tf.keras.metrics.Recall(),\n                   tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\"),'accuracy',f1])\n\nepochs = 5\nbatch_size = 64\n\nhistory = model.fit(X_train_for_lstm, y_train_one_hot,\n                    epochs=epochs, batch_size=batch_size, \n                    validation_data=(X_valid_for_lstm,y_valid_one_hot))","bcfeb63d":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","b3353678":"AUTO = tf.data.experimental.AUTOTUNE\n\nEPOCHS=10\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nMAX_LEN = 384","7fac8795":"import transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nseed=47","66a9dbcd":"MODEL = 'bert-base-uncased'","c80747a0":"def encoding(texts, tokenizer, chunk_size=256, maxlen=512):\n\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_mask=True, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        truncation=True\n    )\n    \n    return {\n        \"input_ids\": np.array(enc_di['input_ids']),\n        \"attention_mask\": np.array(enc_di['attention_mask'])\n    }","82b66985":"tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)\ntokenizer.save_pretrained('.')","7eac06c3":"def build_model(transformer, max_len=512):\n\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n    sequence_output = transformer({\"input_ids\": input_word_ids, \"attention_mask\": attention_mask})[0]\n    cls_token = sequence_output[:, 0, :]\n    \n    x = Dense(128, activation='relu')(cls_token)\n    \n    out = Dense(5, activation='softmax')(x)\n    model = Model(inputs={\n        \"input_ids\": input_word_ids,\n        \"attention_mask\": attention_mask}, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=[tf.keras.metrics.Precision(),\n               tf.keras.metrics.Recall(),\n                   tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\"),'accuracy',f1])\n    \n    return model","3bacd0f2":"with strategy.scope():\n    transformer_layer = (\n        transformers.TFAutoModel\n        .from_pretrained(MODEL)\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","93717425":"X_train_enc = encoding(X_train.tolist(), tokenizer, maxlen=MAX_LEN)\nX_valid_enc = encoding(X_valid.tolist(), tokenizer, maxlen=MAX_LEN)\n    \ntrain_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((X_train_enc, y_train_one_hot))\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n)\n    \ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_valid_enc, y_valid_one_hot))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n    \nn_steps = X_train.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = model.fit(\n        train_dataset,\n        steps_per_epoch=n_steps,\n        verbose=True,\n        validation_data=test_dataset,\n        epochs = EPOCHS \n)","528de65e":"\u0412\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0443 \u043d\u0430\u0441 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e examples \u0432 \u0442\u0440\u044d\u0438\u043d \u0438 \u0432\u0430\u043b\u0438\u0434\u044d\u0448\u0438\u043e\u043d \u043d\u0430\u0431\u043e\u0440\u0435, 670000 \u0438 330000. \u0414\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u0431\u044b\u0441\u0442\u0440\u043e \u043e\u0431\u0443\u0447\u0430\u043b\u0438\u0441\u044c \u0441 \u0446\u0435\u043b\u044c\u044e \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f, \u044f \u0432\u043e\u0437\u044c\u043c\u0443 \u043f\u043e\u0434\u043d\u0430\u0431\u043e\u0440\u044b \u0434\u0430\u043d\u043d\u044b\u0445.","665043c0":"\u0423\u043b\u0443\u0447\u0448\u0438\u0442\u044c LSTM \u043c\u043e\u0436\u043d\u043e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e word2vec, glove \u0438 \u0442.\u0434.","c7a36459":"\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 train and validation \u0438 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043c \"review_text\" and \"overall\"","932df5fb":"# LSTM model","b51b87b4":"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043a\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 SVM, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043e\u043d \u043e\u0431\u0440\u0430\u0449\u0430\u0435\u0442 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043d\u0430 \u043a\u043b\u0430\u0441\u0441\u044b \u0432 \u043c\u0435\u043d\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u0435 \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438.","efa1963f":"\u0412\u0438\u0434\u0438\u043c, \u0440\u0430\u0437\u0440\u044f\u0436\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0434\u0430\u043d\u043d\u044b\u0445: \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u0441 5 \u0437\u0432\u0435\u0437\u0434\u043e\u0447\u043a\u0430\u043c\u0438 \u0433\u043e\u0440\u0430\u0437\u0434\u043e \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u0441 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u043c\u0438. \u0422\u043e \u0435\u0441\u0442\u044c \u0438\u043c\u0435\u0435\u043c \u0434\u0435\u043b\u043e \u0441 Imbalanced classification. \u041c\u043e\u0436\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u0442\u044c undersampling\/oversampling.","d846055e":"\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0437\u0432\u0435\u0437\u0434\u043e\u0447\u0435\u043a \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u043f\u043e \u043e\u0442\u0437\u044b\u0432\u0443, \u0437\u0432\u0435\u0437\u0434\u043e\u0447\u0435\u043a \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c 1,2,3,4,5. \u0422\u043e \u0435\u0441\u0442\u044c \u0438\u043c\u0435\u0435\u043c \u0434\u0435\u043b\u043e \u0441 multiclass classification. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b \u043a\u043b\u0430\u0441\u0441\u044b.","1390b517":"# Naive Bayes with Oversampling","c9d822a9":"Oversampling - \u043a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u0438\u0437 minority \u043a\u043b\u0430\u0441\u0441\u0430.","6a5e920b":"# Naive Bayes Model","e82a1a92":"SVM \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c NB.","12037470":"# BERT model","9d652154":"# SVM","0e424d31":"\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u0435\u0440\u0435\u0439\u0434\u0435\u043c \u043a \u043e\u0447\u0438\u0441\u0442\u043a\u0435 \u0438 \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0438\u043d\u0433\u0443 \u0442\u0435\u043a\u0441\u0442\u0430.","708646ae":"BERT \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430\u0439\u043b\u0443\u0447\u0448\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b","efd321ca":"\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e TF-IDF.","7c5cbdfa":"# Naive Bayes Model with Undersampling","eee43780":"\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043c\u044b \u0438\u043c\u0435\u0435\u043c \u0434\u0435\u043b\u043e \u0441 \u043d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u043a\u043b\u0430\u0441\u0441\u0430\u043c\u0438, \u043b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0433\u043e \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0443 F1. \u0412\u0438\u0434\u0438\u043c, \u0447\u0442\u043e NB with Undersampling\/Oversampling \u043d\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043e\u0431\u044b\u0447\u043d\u044b\u0439 NB.","83be4d15":"Undersampling - \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u0438\u0437 majority \u043a\u043b\u0430\u0441\u0441\u0430"}}