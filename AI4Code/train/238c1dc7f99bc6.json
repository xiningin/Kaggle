{"cell_type":{"b5f64fc0":"code","ed13215b":"code","f341c677":"code","0c556ddf":"code","b066cd19":"code","d572be73":"code","a75c5a9b":"code","1358a8b1":"code","394d591e":"code","3e5c7099":"code","37ab8514":"code","ef7d09fb":"code","f4c21861":"code","964d520f":"code","dd0a27e6":"code","7c0fc2cd":"code","3535468f":"code","84e29f5d":"code","4ac2b4b6":"code","f0b78b5b":"code","de052769":"code","dbab9250":"code","bb37e0f3":"code","692f5c5c":"code","3f9329d8":"code","ca7b7f57":"code","1af82d39":"code","781665ee":"code","c9c80831":"code","a7dcab6c":"code","e38f60ec":"code","6f1781a9":"code","168bf2f7":"code","1d5bb4d3":"code","4e93fe26":"code","9fa6bd51":"code","38352e60":"code","b4b3c97b":"code","6d00f5c4":"code","f645ddda":"markdown","5e60e5de":"markdown","e0cca352":"markdown","2fc27ce8":"markdown","886f2aeb":"markdown","1084ae56":"markdown","6591e12b":"markdown","84bdda87":"markdown","d60076c0":"markdown","9c60c9ba":"markdown","8be9ff62":"markdown","e61e1cff":"markdown","f7ecc73c":"markdown","cedceed4":"markdown","b3417a2d":"markdown","14077d53":"markdown","68f70faf":"markdown","a0f9881a":"markdown","5dadf22b":"markdown","062d128e":"markdown"},"source":{"b5f64fc0":"# Import dependencies\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport time\n\n# ML imports\nfrom sklearn.model_selection import KFold, train_test_split, cross_val_score, cross_val_predict, GridSearchCV \nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA, IncrementalPCA\n\n# Visualizations\nimport matplotlib.pyplot as plt  # static plotting\nimport seaborn as sns  # pretty plotting, including heat map\n%matplotlib inline","ed13215b":"# Initialize process time list for code timing across entire program\nprocess_time = []","f341c677":"# Import train and test datasets\n\n# The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the \n# digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\nsample_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","0c556ddf":"# 42,000 images\n# Each image has 784 features (each image is 28 x 28 pixels) and 1 label feature (785 total)\n# Each feature represents one pixel's intensity from 0 (white) to 255 (black)\ntrain.shape","b066cd19":"# 28,000 images, contains 784 features and no label\ntest.shape","d572be73":"# Inspect class balances for train set, seems relatively balanced\nprint(train['label'].value_counts(ascending=False))\nprint('--------------------')\nprint(train['label'].value_counts(normalize=True))","a75c5a9b":"# Plot total distribution of labels in Kaggle train set\nmn_plt_total = sns.countplot(train['label'], palette=\"muted\").set_title('Total Digit Distribution')","1358a8b1":"# Save the labels to a Pandas series target\ny = train['label']\n\n# Drop the label feature\nX = train.drop(\"label\",axis=1)","394d591e":"# Confirm that all 256 values between the min-max of 0-255 exist in the train set\nlen(np.unique(X))","3e5c7099":"# View images\nimages_to_plot = 9\nrandom_indices = random.sample(range(42000), images_to_plot)\n\nsample_images = X.loc[random_indices, :]\nsample_labels = y.loc[random_indices]","37ab8514":"# Plot examples\nplt.clf()\nplt.style.use('seaborn-muted')\n\nfig, axes = plt.subplots(3,3, \n                         figsize=(5,5),\n                         sharex=True, sharey=True,\n                         subplot_kw=dict(adjustable='box', aspect='equal')) #https:\/\/stackoverflow.com\/q\/44703433\/1870832\n\nfor i in range(images_to_plot):\n    \n    # axes (subplot) objects are stored in 2d array, accessed with axes[row,col]\n    subplot_row = i\/\/3 \n    subplot_col = i%3  \n    ax = axes[subplot_row, subplot_col]\n\n    # plot image on subplot\n    plottable_image = np.reshape(sample_images.iloc[i,:].values, (28,28))\n    ax.imshow(plottable_image, cmap='gray_r')\n    \n    ax.set_title('Digit Label: {}'.format(sample_labels.iloc[i]))\n    ax.set_xbound([0,28])\n\nplt.tight_layout()\nplt.show()","ef7d09fb":"# Assign classifier\n# Use default criterion (gini)\ncls_rf = RandomForestClassifier(random_state=1, n_jobs=-1, n_estimators=90)","f4c21861":"# Split train and test set 90\/10\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=1)","964d520f":"# Check split\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","dd0a27e6":"# Evaluate distribution of labels across train set\nmn_plt_trn = sns.countplot(y_train, palette=\"muted\").set_title('Train Digit')","7c0fc2cd":"# Evaluate distribution of labels across test set\nmn_plt_trn = sns.countplot(y_test, palette=\"muted\").set_title('Test Digit')","3535468f":"# Fit model and run on test set with timing\nstart_time = time.time()\ncls_rf.fit(X_train, y_train)\nprint('Accuracy: ', cls_rf.score(X_test, y_test))\n\n# Timing\nelapsed_time = time.time() - start_time\nprint('Formatted time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint('Time in seconds: ', elapsed_time)\nprocess_time.append(elapsed_time)","84e29f5d":"# cls_rf = RandomForestClassifier()\n\nparam_grid = {\n    'bootstrap': [True],\n    'n_estimators': [100,150,200,250,300,400]\n}\n\ngrid_search_rf = GridSearchCV(estimator=cls_rf, param_grid=param_grid, cv=3, \n                              return_train_score=True, n_jobs=-1, verbose=2)\n\ngrid_search_rf.fit(X_train, y_train)","4ac2b4b6":"# Display parameter recommendations\nprint('Test set score: ', grid_search_rf.score(X_test, y_test))\nprint('Best parameters: ', grid_search_rf.best_params_)\nprint('Best cross-validation score: ', grid_search_rf.best_score_)","f0b78b5b":"# Adjust model parameters, not using max depth\ncls_rf = RandomForestClassifier(random_state=1, n_jobs=-1, n_estimators=300)\n\n# Original\n# cls_rf = RandomForestClassifier(random_state=1, n_jobs=-1, n_estimators=90)","de052769":"# Fit the tuned parameters with timing\nstart_time = time.time()\ncls_rf.fit(X_train, y_train)\n\n# Results with tuning\nprint('Accuracy: ', cls_rf.score(X_test, y_test))\n\n# Timing\nelapsed_time = time.time() - start_time\nprint('Formatted time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint('Time in seconds: ', elapsed_time)\nprocess_time.append(elapsed_time)","dbab9250":"# F1 score\npred = cls_rf.predict(X_test)\nprint('F1 accuracy: ', f1_score(pred, y_test, average='macro'))","bb37e0f3":"# Print classification report\ntarget_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7',\n                   'class 8', 'class 9']\nprint(classification_report(y_test, pred, target_names=target_names))","692f5c5c":"# Plot confusion matrix of actual versus predicted labels\nrf_cm = confusion_matrix(y_test, pred)\nrf_cm_plt=sns.heatmap(rf_cm.T, square=True, annot=True, fmt='d', cbar=False, cmap=\"Blues\")\nplt.xlabel('Actual label')\nplt.ylabel('Predicted label')\nplt.title(\"Valid\");","3f9329d8":"# Submission code\ntestData = pd.read_csv(\"test.csv\")\nstart_time = time.time()\npred = cls_rf.predict(testData)\nelapsed_time = time.time() - start_time\nprint('Formatted time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint('Time in seconds: ', elapsed_time)\nprocess_time.append(elapsed_time)\n\n# # Create Dataframe\n# data = pred\n# df_1 = pd.DataFrame(pred)\n# df_1['ImageID'] = df_1.index + 1\n# df_1.columns = ['Label', 'ImageID']\n# submission = df_1[['ImageID', 'Label']]\n\n# # Output to csv\n# submission.to_csv('Boetticher_RF_predictions.csv',header=True, index=False)","ca7b7f57":"# Combine training and test set for PCA\nx = np.concatenate((X_train, X_test), axis=0)","1af82d39":"# Fit model using PCA, generating principal components that represent 95 percent of the variability in \n# the explanatory features\nstart_time = time.time()\npca = PCA(.95)\npca.fit(x)\ntotimages = pca.transform(x)\n# pca.n_components_\n\n# Timing\nelapsed_time = time.time() - start_time\nprint('Formatted time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint('Time in seconds: ', elapsed_time)\nprocess_time.append(elapsed_time)","781665ee":"# Output number of principal components explaining 95% of variability in the explanatory features: 154\nprint('Principal components count: ', pca.n_components_)","c9c80831":"# Explained variance ratio for 154 PCs\n# 10% of the train dataset's variance lies along the first PC\n# 7% along the second\n# 6% along the third, etc.\npca.explained_variance_ratio_","a7dcab6c":"#Explained variance plot\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","e38f60ec":"# Reform the training and testing images\n# Training: up to 33,600\n# Testing: 33,600 to 42,000\ntrainimages = totimages[0:33600, :]\ntestimages = totimages[33600:42000, :]","6f1781a9":"# Convert to integers\ntrainimages = trainimages.astype(int)\ntestimages = testimages.astype(int)","168bf2f7":"# Time component identification\nstart_time = time.time()\n\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_train, n_batches):\n    inc_pca.partial_fit(X_batch)\n\nX_reduced = inc_pca.transform(X_train)\n\n# Timing\nelapsed_time = time.time() - start_time\nprint('Formatted time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint('Time in seconds: ', elapsed_time)\nprocess_time.append(elapsed_time)","1d5bb4d3":"# Explained variance ratio for 154 PCs with incremental PCA\n# 10% of the train dataset's variance lies along the first PC\n# 7% along the second\n# 6% along the third, etc.\n# Not drastically different than PCA and took longer\ninc_pca.explained_variance_ratio_","4e93fe26":"# Fit the model\nstart_time = time.time()\ncls_rf2 = RandomForestClassifier(random_state=1, n_jobs=-1, n_estimators=300)\ncls_rf2.fit(trainimages, y_train.values.ravel())\nprint('Accuracy: ', cls_rf2.score(testimages, y_test))\n\n# Timing\nelapsed_time = time.time() - start_time\nprint('Formatted time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint('Time in seconds: ', elapsed_time)\nprocess_time.append(elapsed_time)","9fa6bd51":"# Predict a second time with principal components\npred2 = cls_rf2.predict(testimages)\nprint('F1 accuracy: ', f1_score(pred2, y_test, average='macro'))","38352e60":"# Plot confusion matrix of actual versus predicted labels\nrf_cm = confusion_matrix(y_test, pred2)\nrf_cm_plt=sns.heatmap(rf_cm.T, square=True, annot=True, fmt='d', cbar=False, cmap=\"Blues\")\nplt.xlabel('Actual label')\nplt.ylabel('Predicted label')\nplt.title(\"Valid\");","b4b3c97b":"# Submission code with timing\nx_test = pd.read_csv(\"test.csv\")\nstart_time = time.time()\npred_PCA = cls_rf2.predict(pca.transform(x_test))\nelapsed_time = time.time() - start_time\nprint('Formatted time: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint('Time in seconds: ', elapsed_time)\nprocess_time.append(elapsed_time)\n\n# # Create Dataframe\n# data = pred_PCA\n# df_1 = pd.DataFrame(pred_PCA)\n# df_1['ImageID'] = df_1.index + 1\n# df_1.columns = ['Label', 'ImageID']\n# submission = df_1[['ImageID', 'Label']]\n\n# # Output to csv\n# submission.to_csv('Boetticher_RF__PCA_predictions_2.csv',header=True, index=False)","6d00f5c4":"# Final process_time addition for entire program\n# Find sum of elements in process_time list \ntotal = 0\n\n# Iterate over each element in process_time list and add them in variable total \nfor ele in range(0, len(process_time)): \n    total = total + process_time[ele] \n    \n# Conversion\ndef convert(seconds): \n    min, sec = divmod(seconds, 60) \n    hour, min = divmod(min, 60) \n    return \"%d:%02d:%02d\" % (hour, min, sec) \n      \n# Time output\nn = total\nconvert(n)\n\n# Results\nprint(\"Total time of model fitting and evalation for study (in seconds): \", total) \nprint('Formatted time: ', time.strftime(\"%H:%M:%S\", time.gmtime(total)))","f645ddda":"### 4. Evaluate model on test set for Kaggle","5e60e5de":"### 3. Confusion matrix","e0cca352":"### 3. Test Incremental PCA","2fc27ce8":"Objective: Develop a multiclass classifier using the entire MNIST data set for input data. The classifier will be used to predict which of 10 digits is being written, evaluated with and without dimension reduction with principal components analysis (PCA). \n\n**Kaggle ID: Claire Boetticher**<br>\n**Kaggle username: clairence**","886f2aeb":"# Step 3: Random Forest Classifier with Principal Components","1084ae56":"# Import and inspect training data","6591e12b":"### 2. Confusion matrix","84bdda87":"# Processing time for entire study","d60076c0":"### 1. Fit model with principal components","9c60c9ba":"### 3. Evaluate model on test set for Kaggle","8be9ff62":"# Step 1: Fit a Random Forest classifier and evaluate on test set\n- OOB model\n- Train and evaluate\n- Grid search\n- Train and evaluate\n- Predict\n- Confusion matrix","e61e1cff":"**Figure 1: Label Distribution in Train Set**","f7ecc73c":"### 1. Train test split and distribution of labels","cedceed4":"### 1. Data preparation and PC identification","b3417a2d":"### 2. Initial classification with Random Forest","14077d53":"### 3. Grid search for fine-tuning\nTry n_estimator hyperparameter","68f70faf":"# Step 2: Principal Components Analysis","a0f9881a":"**Figure 3: Label Distribution in Test Set**","5dadf22b":"**Figure 2: Label Distribution in Training Set**","062d128e":"### 2. Explained variance ratio and visualization"}}