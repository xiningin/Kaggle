{"cell_type":{"dd035ffe":"code","9b0a1e79":"code","d701368e":"code","9cdf9dc3":"code","14a73784":"code","83579a77":"code","3f03755b":"code","ced249c8":"code","b700103a":"code","74aae09e":"code","c0b1a2e2":"code","dbf99a42":"code","edf18888":"code","f02a2f6f":"code","5f42a91b":"code","aa6e0525":"code","5f8e3649":"code","a9e2a518":"code","8b651a44":"code","fda603f3":"code","9cd8c9fe":"code","c70378ab":"code","9ca69041":"code","820c6da1":"code","807ef37a":"code","6619a1eb":"code","16ab0913":"code","38f568a8":"code","4717931e":"code","900247f3":"code","f61043bf":"code","0b4e7f6b":"code","9f26e795":"code","9de01e08":"code","4f946d15":"code","144bbebb":"code","c455b9ae":"code","49e9d0ed":"code","eb36e4af":"code","9c3e0ae7":"code","8e01faa1":"code","a4d261de":"code","78244147":"code","17bd09a0":"code","38e1a08b":"code","b1112198":"code","6493465e":"markdown","3f44d7ac":"markdown","027d157e":"markdown","8e875560":"markdown","97179640":"markdown","53af366a":"markdown","dde45f68":"markdown","9cbf1abb":"markdown","c914725f":"markdown","cb87cad0":"markdown","793ab8f3":"markdown","f4c01fbc":"markdown","5bb980fe":"markdown","dcb64c90":"markdown","7ba5e5d3":"markdown","ec92b67d":"markdown","dc3ac759":"markdown","890ddd19":"markdown","83303e83":"markdown","8f362b61":"markdown","b169a1a5":"markdown","9cebdd82":"markdown","b201a040":"markdown","99c8b9e2":"markdown","e74ce839":"markdown","59ddd567":"markdown","6605c471":"markdown","3ae1758c":"markdown","6413e04f":"markdown","7804684a":"markdown","a714421d":"markdown","e8712d81":"markdown","5cd80bb4":"markdown","af12b1f3":"markdown"},"source":{"dd035ffe":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","9b0a1e79":"# Import clean data\n\npath = 'https:\/\/bit.ly\/3zEsKnv'\ndf = pd.read_csv(path)","d701368e":"# using numeric data only\ndf=df._get_numeric_data()\ndf.head()","9cdf9dc3":"# Libraries for plotting\n!pip install ipywidgets","14a73784":"from ipywidgets import interact, interactive, fixed, interact_manual","83579a77":"def Plot_Distribution(function1, functon2, RedName, BlueName, Title):\n    plt.figure(figsize=(10, 6))\n    ax1 = sns.distplot(function1, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(functon2, hist=False, color=\"b\", label=BlueName, ax=ax1)\n    plt.title(Title)\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion of Cars')\n    plt.show()\n    plt.close()","3f03755b":"def Poly_Plot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n    plt.figure(figsize=(10, 6))\n    xmax=max([xtrain.values.max(), xtest.values.max()])\n    xmin=min([xtrain.values.min(), xtest.values.min()])\n    x=np.arange(xmin, xmax, 0.1)\n    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n    plt.plot(xtest, y_test, 'go', label='Test Data')\n    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n    plt.ylim([-10000, 60000])\n    plt.ylabel('Price')\n    plt.legend()","ced249c8":"# target data\ny_data = df['price']\n\n# Drop price data in dataframe x_data\nx_data=df.drop('price',axis=1)","b700103a":"# randomly splitting data into training and testing data\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data,\n                                                    test_size=0.10,\n                                                    random_state=1)\n\nprint(\"number of training samples:\",x_train.shape[0])\nprint(\"number of test samples :\", x_test.shape[0])","74aae09e":"# import LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\n# create a Linear Regression object\nlr=LinearRegression()","c0b1a2e2":"#  fit the model using the feature \"horsepower\"\nlr.fit(x_train[['horsepower']], y_train)","dbf99a42":"# calculate the R^2 (coefficient of determination) score on the test data\nlr.score(x_test[['horsepower']], y_test)","edf18888":"lr.score(x_train[['horsepower']], y_train)","f02a2f6f":"from sklearn.model_selection import cross_val_score","5f42a91b":"# The default scoring is R^2.\n# Each element in the array has the average R^2 value for the fold\nRcross = cross_val_score(lr, x_data[['horsepower']], y_data, cv=4)\nRcross","aa6e0525":"# calculate the average and standard deviation of our estimate\nprint(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())","5f8e3649":"-1 * cross_val_score(lr,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')","a9e2a518":"# The function splits up the data into the specified number of folds\n# with one fold for testing and the other folds are used for training\n\nfrom sklearn.model_selection import cross_val_predict\ny_pred = cross_val_predict(lr,x_data[['horsepower']], y_data,cv=4)\ny_pred[0:5]","8b651a44":"lr = LinearRegression()\nlr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)","fda603f3":"# Prediction using training data\ny_pred_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\ny_pred_train[0:5]","9cd8c9fe":"# Prediction using test data\ny_pred_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\ny_pred_test[0:5]","c70378ab":"# examine the distribution of the predicted values of the training data\ntitle1 = 'Distribution Plot of Predicted Value Using Training Data vs Training Data Distribution'\nPlot_Distribution(y_train, y_pred_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", title1)","9ca69041":"title2='Distribution Plot of Predicted Value Using Test Data vs Data Distribution of Test Data'\nPlot_Distribution(y_test,y_pred_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",title2)","820c6da1":"# polynomial regression\nfrom sklearn.preprocessing import PolynomialFeatures","807ef37a":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data,\n                                                    test_size=0.45,\n                                                    random_state=0)","6619a1eb":"pr = PolynomialFeatures(degree=5)\nx_train_pr = pr.fit_transform(x_train[['horsepower']])\nx_test_pr = pr.fit_transform(x_test[['horsepower']])\npr","16ab0913":"poly = LinearRegression()\npoly.fit(x_train_pr, y_train)","38f568a8":"# prediction\ny_pred = poly.predict(x_test_pr)\ny_pred[0:5]","4717931e":"# taking the first five predicted values and compare it to the actual targets\nprint(\"Predicted values:\", y_pred[0:4])\nprint(\"True values:\", y_test[0:4].values)","900247f3":"# display the training data, testing data, and the predicted function.\nPoly_Plot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)","f61043bf":"# R^2 of the training data:\npoly.score(x_train_pr, y_train)","0b4e7f6b":"poly.score(x_test_pr, y_test)","9f26e795":"rsq_test = []\n\npoly_order = [1, 2, 3, 4]\nfor n in poly_order:\n    pr = PolynomialFeatures(degree=n)\n    \n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    \n    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n    \n    lr.fit(x_train_pr, y_train)\n    \n    rsq_test.append(lr.score(x_test_pr, y_test))\n\nplt.figure(figsize=(10,7))\nplt.plot(poly_order, rsq_test)\nplt.xlabel('Order')\nplt.ylabel('R^2')\nplt.title('R^2 Using Test Data')\nplt.text(3, 0.74, 'Maximum R^2 ');","9de01e08":"def function(order, test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n    pr = PolynomialFeatures(degree=order)\n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n    poly = LinearRegression()\n    poly.fit(x_train_pr,y_train)\n    Poly_Plot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)","4f946d15":"interact(function, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))","144bbebb":"# a degree two polynomial transformation\npr=PolynomialFeatures(degree=2)\nx_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\nx_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])","c455b9ae":"from sklearn.linear_model import Ridge\n\n# create a Ridge regression object, setting the regularization parameter (alpha) to 0.1\nRigeModel=Ridge(alpha=1)\n\n# fit the model \nRigeModel.fit(x_train_pr, y_train)\n\n# prediction\ny_pred = RigeModel.predict(x_test_pr)","49e9d0ed":"# compare the first five predicted samples to our test set\nprint('predicted:', y_pred[0:4])\nprint('test set :', y_test[0:4].values)","eb36e4af":"from tqdm import tqdm\n\nrsq_test = []\nrsq_train = []\ndummy1 = []\nAlpha = 10 * np.array(range(0,1000))\npbar = tqdm(Alpha)\n\nfor alpha in pbar:\n    RigeModel = Ridge(alpha=alpha) \n    RigeModel.fit(x_train_pr, y_train)\n    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n    \n    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n\n    rsq_test.append(test_score)\n    rsq_train.append(train_score)","9c3e0ae7":"# plot out the value of R^2 for different alphas\nplt.figure(figsize=(10, 6))\nplt.plot(Alpha,rsq_test, label='validation data  ')\nplt.plot(Alpha,rsq_train, 'r', label='training Data ')\nplt.xlabel('alpha')\nplt.ylabel('R^2')\nplt.legend()","8e01faa1":"from sklearn.model_selection import GridSearchCV","a4d261de":"# create a dictionary of parameter values\nparameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\nparameters1","78244147":"# Create a Ridge regression object\nRR=Ridge()\n\n# Create a ridge grid search object\nGrid1 = GridSearchCV(RR, parameters1,cv=4, iid=None)","17bd09a0":"# fit the data\nGrid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)","38e1a08b":"BestRR = Grid1.best_estimator_\nBestRR","b1112198":"BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)","6493465e":"This notebook was the final assignment of a **Coursera** course I have studied.","3f44d7ac":"Here, the R^2 gradually increases until an order three polynomial is used. Then, the R^2 dramatically decreases at an order four polynomial.","027d157e":"# Model Evaluation and Refinement","8e875560":"Let's create Multiple Linear Regression objects and train the model using 'horsepower', 'curb-weight', 'engine-size' and 'highway-mpg' as features.\n","97179640":"We select the value of alpha that minimizes the test error. To do so, we can use a for loop. We have also created a progress bar to see how many iterations we have completed so far.\n","53af366a":"Sometimes we do not have sufficient testing data. So we may want to perform cross-validation.","dde45f68":"The following interface allows you to experiment with different polynomial orders and different amounts of data.\n","9cbf1abb":"We will perform a degree 5 polynomial transformation on the feature 'horsepower'.","c914725f":"## 2: Overfitting, Underfitting and Model Selection\n\nTest data is a much better measure of how well your model performs in the real world. \nOne reason for this is overfitting.\n\nIt turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression\n","cb87cad0":"<h2>Functions for Plotting<\/h2>\n","793ab8f3":"In this notebook, I will be discussing on evaluation and refinement of prediction models.","f4c01fbc":"But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values.","5bb980fe":"<h3>Overfitting<\/h3>\n<p>Overfitting occurs when the model fits the noise, but not the underlying process. Therefore, when testing your model using the test set, your model does not perform as well since it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.<\/p>\n","dcb64c90":"## 3: Ridge Regression\n","7ba5e5d3":"The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:\n","ec92b67d":"The red line in Figure 4 represents the R^2 of the training data. As alpha increases the R^2 decreases. Therefore, as alpha increases, the model performs worse on the training data\n\nThe blue line represents the R^2 on the validation data. As the value for alpha increases, the R^2 increases and converges at a point.\n","dc3ac759":"<h2>Cross-Validation Score<\/h2>\n","890ddd19":"<h2>Table of Contents<\/h2>\n<ul>\n    <li>Model Evaluation<\/li>\n    <li>Over-fitting, Under-fitting and Model Selection<\/li>\n    <li>Ridge Regression<\/li>\n    <li>Grid Search<\/li>\n<\/ul>\n","83303e83":"The R^2 is much smaller using the test data compared to the training data.","8f362b61":"**Thank You**","b169a1a5":"The parameter 'cv' determines the number of folds. Here, it is 4.\n","9cebdd82":" This interactive plot does not work on Kaggle. You can download the code and try it in Jupyter Notebook or Google Colab","b201a040":"The estimated function appears to track the data but around 200 horsepower, the function begins to **diverge** from the data points.\n","99c8b9e2":"Let's see how the R^2 changes on the test data for different order polynomials and then plot the results:\n","e74ce839":"We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'.\n","59ddd567":"R^2 of the test data:\n","6605c471":"## 1. Training & Testing","3ae1758c":"Now, let's create a Linear Regression model \"poly\" and train it.\n","6413e04f":"Comparing both figures,  it is evident that the distribution of the test data in first figure is much better at fitting the data. This difference in second figure is apparent in the range of 5000 to 15,000. This is where the shape of the distribution is extremely different.","7804684a":"## 4: Grid Search\n","a714421d":"We now test our model on the test data:\n","e8712d81":"The term alpha is a hyperparameter. Sklearn has the class <b>GridSearchCV<\/b> to make the process of finding the best hyperparameter simpler.\n","5cd80bb4":"Let's use 55 percent of the data for training and the rest for testing:\n","af12b1f3":"The lower the R^2, the worse the model. A negative R^2 is a sign of **overfitting**."}}