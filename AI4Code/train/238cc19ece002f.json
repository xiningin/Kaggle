{"cell_type":{"da7176b4":"code","b3fd1319":"code","30ca9a18":"code","93353243":"code","ba42bfe1":"code","f1656c66":"code","3535fb7b":"code","18805600":"code","f91ab64e":"code","124f2c0e":"code","fe8b9797":"code","6111854e":"code","a5a513ec":"code","3e77a233":"code","ca644a2b":"code","6d1b6af7":"code","4db0c003":"code","2bca36aa":"code","d2d6adfc":"code","970887ad":"code","4c39797f":"code","881b0986":"markdown","cd9f95b8":"markdown","91842c13":"markdown","7266b68d":"markdown","7a3cea29":"markdown","33e91427":"markdown","fe0a185f":"markdown","5bd3964c":"markdown","e00851ee":"markdown","d123bee8":"markdown","c122f0f7":"markdown","e34e9a4f":"markdown","d4b39c36":"markdown","f8067d8f":"markdown","4a436bec":"markdown","f01bec6b":"markdown","b64936a1":"markdown"},"source":{"da7176b4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use('seaborn-darkgrid')","b3fd1319":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')","30ca9a18":"df.head()","93353243":"max_nan = df.isnull().sum().max() # no data is missing\nprint('NaN values in our dataset: ' + str(max_nan))\nprint('\\n')\nsns.heatmap(abs(df.isnull()), cmap='viridis')","ba42bfe1":"# Creating a new column just for EDA\n\n# Assuming -> Chance of Admit >= 75% (Probably admitted)\n#             Chance of Admit < 75% (Probably recused) \n\ndf['probably_admitted'] = df['Chance of Admit '].apply(lambda x: 1 if x >= 0.75 else 0)","f1656c66":"#Dataframes for continuous and discrete data\ndf_cont = df[['GRE Score', 'TOEFL Score', 'CGPA','Chance of Admit ']]\ndf_disc = df[['University Rating','SOP','LOR ', 'probably_admitted']]","3535fb7b":"for i in np.arange(0, len(df_cont.columns), 2):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,7))\n    \n    ax1.set_title('Distribution for %s' % (df_cont.columns[i]))    \n    ax1 = sns.distplot(df_cont[df_cont.columns[i]],bins=30, kde=False, ax=ax1)\n    \n    ax2.set_title('Distribution for %s' % (df_cont.columns[i+1]))    \n    ax2 = sns.distplot(df_cont[df_cont.columns[i+1]],bins=30, kde=False, ax=ax2)","18805600":"for i in np.arange(0, len(df_disc.columns), 2):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,7))\n   \n    ax1.set_title('Distribution for %s' % (df_disc.columns[i]))    \n    ax1 = sns.countplot(x=df_disc.columns[i], data=df_disc, ax=ax1)\n    \n    ax2.set_title('Distribution for %s' % (df_disc.columns[i+1]))    \n    ax2 = sns.countplot(x=df_disc.columns[i+1],data=df_disc, ax=ax2)","f91ab64e":"fig, (ax1) = plt.subplots(figsize=(9,5))\nax1 = sns.heatmap(df.drop(['Serial No.'], axis=1).corr(), linewidths=0.5, square=True, cmap='viridis')","124f2c0e":"pd.DataFrame(df.drop(['Serial No.', 'probably_admitted'], axis=1).corr()['Chance of Admit '].sort_values(ascending=False)[1:])","fe8b9797":"from sklearn.model_selection import train_test_split","6111854e":"X = df.drop(['Serial No.', 'probably_admitted','Chance of Admit '], axis=1)\ny = df['Chance of Admit '].values\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=101)","a5a513ec":"from sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.model_selection import cross_val_score # module import for model validation and comparison","3e77a233":"lm = LinearRegression()\nlm.fit(X_train, y_train)\n\nprint('Negative Mean Absolute Error for Linear Regression:')\nnp.mean(cross_val_score(lm, X_train, y_train, scoring='neg_mean_absolute_error', cv=3))","ca644a2b":"alpha = []\nerror = []\n\nfor i in range(1, 100):\n    alpha.append(i\/2000)\n    lml = Lasso(alpha=(i\/2000))\n    error.append(np.mean(cross_val_score(lml, X_train, y_train, scoring='neg_mean_absolute_error', cv=3)))\n    \nplt.grid(True)\nplt.title('Lasso Regression Prediction Score')\nplt.ylabel('Error')\nplt.xlabel('Alpha')\nplt.plot(alpha, error, label='Neg Mean Absolute Error')\nplt.legend()","6d1b6af7":"index = error.index(max(error))\nbest_alpha = alpha[index]\n\nlml = Lasso(alpha=best_alpha)\nlml.fit(X_train, y_train)","4db0c003":"from sklearn.ensemble import RandomForestRegressor","2bca36aa":"rf = RandomForestRegressor()\nrf.fit(X_train, y_train)\nprint('Negative Mean Absolute Error for Random Forest Regression:')\nnp.mean(cross_val_score(rf, X_train, y_train, scoring='neg_mean_absolute_error', cv=3))","d2d6adfc":"tpred_lm = lm.predict(X_test)\ntpred_lml = lml.predict(X_test)\ntpred_rf = rf.predict(X_test)","970887ad":"from sklearn.metrics import mean_absolute_error","4c39797f":"mae_lm = '{:2.2f}'.format(mean_absolute_error(y_test, tpred_lm)*100) + '%'\nmae_lml = '{:2.2f}'.format(mean_absolute_error(y_test, tpred_lml)*100) + '%'\nmae_rf = '{:2.2f}'.format(mean_absolute_error(y_test, tpred_rf)*100) + '%'\n\nprint('Mean Absolute Error for Each Regression Type:')\nprint('\\n')\npd.DataFrame({'Linear':[mae_lm], 'Lasso':[mae_lml], 'RdnForest':[mae_rf] })","881b0986":"- **GRE Score**: Normal Distribution\n- **TOEFL**: Normal Distribution\n- **CGPA**: Normal Distribution\n- **Chance of Admit**: The majority of students have between 70-80% of chance of being admite ","cd9f95b8":"# Model Building","91842c13":"Importing the csv file","7266b68d":"# Prediction Comparison","7a3cea29":"Choosing the alpha that gives the smaller error","33e91427":"### Train-Test splits","fe0a185f":"No data is missing\n","5bd3964c":"# College Admission: Project Overview\n\n* Created a tool that estimates the chance of admission in college for a student.\n* Performed a simple EDA analysis of the data provided, verifying how each variable affects the admission chance.\n* Compared the prediction result for different regression models, in which Linear Regression was recommended as it scored similar to Lasso and Random Forest Regressor.","e00851ee":"As stated from the table above, all three methods scored similar results in a MAE comparison.\n\nIn this situation, Linear Regression is recommended as it requires less computational power.  ","d123bee8":"1. **check for missing data**","c122f0f7":"## Random Forest ","e34e9a4f":"As all columns are numerical, we can train-test-split the DataFrame directly.","d4b39c36":"Doing research don't seem to affect much on the admission chance. \n\nThe other variables are directly linked, as it seems logic that students that score better grades on tests have a better chance of being admitted.","f8067d8f":"## Lasso regression","4a436bec":"## Linear regression","f01bec6b":"Correlation between variables","b64936a1":"All the columns are numeric. We won't need to use a One-Hot-Encoding approach (used for categorical data)"}}