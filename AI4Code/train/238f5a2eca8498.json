{"cell_type":{"520f9183":"code","70365b5b":"code","9bfbc997":"code","6287cacd":"code","d851a183":"code","5752338e":"code","1254c1b1":"code","22cd35b6":"code","1094174d":"code","212b17b0":"code","715b4f45":"code","57c66b16":"code","df40160c":"code","6d0be33a":"code","2120142f":"code","0678d5e7":"code","55eb673e":"code","c016a1f0":"code","ca454ad3":"code","61597436":"code","1c643267":"code","ba8679d8":"code","5eed6be6":"markdown","80c2dc1a":"markdown","c89d8e27":"markdown","4385894b":"markdown","24472083":"markdown","eb6f3d70":"markdown","7961ca7b":"markdown","33f1449e":"markdown","ee47ce6e":"markdown","5b815e41":"markdown","a2926c0e":"markdown","9aa5dc40":"markdown","207d02cf":"markdown","8215a3a1":"markdown","d86494e8":"markdown","03020aad":"markdown","d9c34787":"markdown","b1c97152":"markdown","85b500b7":"markdown","97c217a6":"markdown","d23cddcf":"markdown","69f51bd6":"markdown","b7bd8bd2":"markdown","71e8b937":"markdown","6688d942":"markdown","04034aab":"markdown","257d0b44":"markdown","e694996b":"markdown"},"source":{"520f9183":"#Suppress Deprecation and Incorrect Usage Warnings \nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, FunctionTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, precision_recall_fscore_support\nfrom scipy import interp\nimport pickle\nimport matplotlib.pyplot as plt\n%matplotlib inline","70365b5b":"df = pd.read_csv(\"..\/input\/bcwd.csv\")\ndf.head()","9bfbc997":"df['diagnosis'].value_counts()","6287cacd":"df.isna().sum()","d851a183":"df.loc[:,'radius_mean':'fractal_dimension_mean'].boxplot(figsize=(20,5))\nplt.show()","5752338e":"df.loc[:,'radius_se':'fractal_dimension_se'].boxplot(figsize=(20,5))\nplt.show()","1254c1b1":"df.loc[:,'radius_worst':'fractal_dimension_worst'].boxplot(figsize=(20,5))\nplt.show()","22cd35b6":"mapper = {'M': 1, 'B': 0}\ndf['diagnosis'] = df['diagnosis'].replace(mapper)\ndf['diagnosis'].value_counts()","1094174d":"# also store a list with the names of all predictors\nnames_all = [c for c in df if c not in ['diagnosis']]\n\n# define column groups with same data preparation\nnames_outliers = ['area_mean', 'area_se', 'area_worst']\nnames_no_outliers = list(set(names_all) - set(names_outliers))","212b17b0":"class AddColumnNames(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return pd.DataFrame(data=X, columns=self.columns)","715b4f45":"class ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n\n        try:\n            return X[self.columns]\n        except KeyError:\n            cols_error = list(set(self.columns) - set(X.columns))\n            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)","57c66b16":"preprocess_pipeline = make_pipeline(\n    AddColumnNames(columns=names_all),\n    FeatureUnion(transformer_list=[\n        (\"outlier_columns\", make_pipeline(\n            ColumnSelector(columns=names_outliers),\n            FunctionTransformer(func=np.log, validate=False),\n            RobustScaler()\n        )),\n        (\"no_outlier_columns\", make_pipeline(\n            ColumnSelector(columns=names_no_outliers),\n            StandardScaler()\n        ))\n    ])\n)","df40160c":"y = df['diagnosis']\nX = df.drop('diagnosis', axis=1).values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)","6d0be33a":"# create the pipeline\npipe = Pipeline(steps=[('preprocess', preprocess_pipeline), ('svm', svm.SVC(probability=True))])\n\n# prepare a prameter grid\n# note that __ can be used to specify the name of a parameter for a specific element in a pipeline\n# note also that this is not an exhaustive list of the parameters of SVM and their possible values\n\nparam_grid = {\n    'svm__C': [0.1, 1, 10, 100],  \n    'svm__gamma': [1, 0.1, 0.01, 0.001], \n    'svm__kernel': ['rbf', 'linear', 'poly']}\n\nwarnings.filterwarnings('ignore')\nsearch = GridSearchCV(pipe, param_grid, cv=10, refit=True)\nsearch.fit(X_train, y_train)\nprint(\"Best CV score = %0.3f:\" % search.best_score_)\nprint(\"Best parameters: \", search.best_params_)\n\n# store the best params and best model for later use\nSVM_best_params = search.best_params_\nSVM_best_model = search.best_estimator_","2120142f":"# create the pipeline\npipe = Pipeline(steps=[('preprocess', preprocess_pipeline), ('rf', RandomForestClassifier())])\n\n# prepare a prameter grid\n# note that __ can be used to specify the name of a parameter for a specific element in a pipeline\n# note also that this is not an exhaustive list of the parameters of Random Forest and their possible values\nparam_grid = {\n    'rf__n_estimators' : [10,20,30],\n    'rf__max_depth': [2, 4, 6, 8]\n}\n\nwarnings.filterwarnings('ignore')\nsearch = GridSearchCV(pipe, param_grid, cv=10, refit=True)\nsearch.fit(X_train, y_train)\nprint(\"Best CV score = %0.3f:\" % search.best_score_)\nprint(\"Best parameters: \", search.best_params_)\n\n# store the best params and best model for later use\nRF_best_params = search.best_params_\nRF_best_model = search.best_estimator_","0678d5e7":"mean_fpr = np.linspace(start=0, stop=1, num=100)","55eb673e":"# model - a trained binary probabilistic classification model;\n#         it is assumed that there are two classes: 0 and 1\n#         and the classifier learns to predict probabilities for the examples to belong to class 1\n\ndef evaluate_model(X_test, y_test, model):\n    # compute probabilistic predictiond for the evaluation set\n    _probabilities = model.predict_proba(X_test)[:, 1]\n    \n    # compute exact predictiond for the evaluation set\n    _predicted_values = model.predict(X_test)\n        \n    # compute accuracy\n    _accuracy = accuracy_score(y_test, _predicted_values)\n        \n    # compute precision, recall and f1 score for class 1\n    _precision, _recall, _f1_score, _ = precision_recall_fscore_support(y_test, _predicted_values, labels=[1])\n    \n    # compute fpr and tpr values for various thresholds \n    # by comparing the true target values to the predicted probabilities for class 1\n    _fpr, _tpr, _ = roc_curve(y_test, _probabilities)\n        \n    # compute true positive rates for the values in the array mean_fpr\n    _tpr_transformed = np.array([interp(mean_fpr, _fpr, _tpr)])\n    \n    # compute the area under the curve\n    _auc = auc(_fpr, _tpr)\n            \n    return _accuracy, _precision[0], _recall[0], _f1_score[0], _tpr_transformed, _auc","c016a1f0":"SVM_accuracy, SVM_precision, SVM_recall, SVM_f1_score, SVM_tpr, SVM_auc = evaluate_model(X_test, y_test, SVM_best_model)\nRF_accuracy, RF_precision, RF_recall, RF_f1_score, RF_tpr, RF_auc = evaluate_model(X_test, y_test, RF_best_model)","ca454ad3":"SVM_metrics = np.array([SVM_accuracy, SVM_precision, SVM_recall, SVM_f1_score])\nRF_metrics = np.array([RF_accuracy, RF_precision, RF_recall, RF_f1_score])\nindex = ['accuracy', 'precision', 'recall', 'F1-score']\ndf_metrics = pd.DataFrame({'SVM': SVM_metrics, 'Random Forest': RF_metrics}, index=index)\ndf_metrics.plot.bar(rot=0)\nplt.legend(loc=\"lower right\")\nplt.show()","61597436":"plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=0.8)\nplt.plot(mean_fpr, SVM_tpr[0,:], lw=2, color='blue', label='SVM (AUC = %0.2f)' % (SVM_auc), alpha=0.8)\nplt.plot(mean_fpr, RF_tpr[0,:], lw=2, color='orange', label='Random Forest (AUC = %0.2f)' % (RF_auc), alpha=0.8)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curves for multiple classifiers')\nplt.legend(loc=\"lower right\")\nplt.show()","1c643267":"# function to remove the string 'svm__' from the names of the parameters in SVM_best_params\ndef transform(dict):\n    return {key.replace('svm__','') :  value for key, value in dict.items()}\n\npipe = Pipeline(steps=[('preprocess', preprocess_pipeline), ('rf', svm.SVC(**transform(SVM_best_params)))])\n\nfinal_model =pipe.fit(X, y)","ba8679d8":"filename = 'final_model.sav'\npickle.dump(final_model, open(filename, 'wb'))","5eed6be6":"After splitting a dataset into a training and test datasets, the names of the columns are lost. This is the reason, we stored the names of the columns in lists above. We will use the following class in the preprocessing pipeline to put the names of the columns back. We need this to easily apply the different preparation strategies to the two groups of columns. ","80c2dc1a":"We will use a pandas bar plot to compare the accuracies of both models as well as their precision, recall and F1-score for class 1. ","c89d8e27":"Next, we train an SVM classifier with the training set and a range of possible parameters in order to find the best parameters for SVM by cross-validation. To do this we will build another pipeline which includes the preprocessing pipeline and the SVM classifier. The pipeline will take care for separately preprocessing the training and validation sets after the training set is further split into training and validation sets in the process of cross-validation.","4385894b":"The ROC curves confirm that both classifiers perform significantly better than a random guess (the red dashed line). ","24472083":"There are no missing values. Let's check the boxplots of the numerical columns for outliers. We will do this in three slices.","eb6f3d70":"The parameter `refit=True` makes the GridSearchCV train an SVM classifier on the **whole training set** with the best parameters found. This final best SVM model can then be accessed via the `.best_estimator_` attribute of the GridSearchCV.\n\nLet's repeat the same experiment but with the Random Forest classifier.","7961ca7b":"## Compare Classifiers on Test Set","33f1449e":"Now we can evaluate the best models found by the grid search on the test dataset and compare their results (accuracy, precision, recall, etc.) to choose the better classifier for our problem.","ee47ce6e":"The evaluation function below evaluates a model on a test data set. Note that, the preprocessing pipeline will be automatically applied to the test set. The results returned by the function are a variety of metrics measured on the test set which we will use to compare the models and decide which classifier to choose for training the final model. These metrics are:\n\n- accuracy\n- precisions\n- recall\n- F1-score\n- TPR (true positive rate)\n- AUC (area under the ROC curve)\n\nTo plot ROC curves for the models, we need to ensure that the same fpr (false positive rate) scale is used for all curves. For this reason, we take a scale of 100 equally spaced fpr values between 0 and 1.","5b815e41":"Now we can separate the columns into *target* and *predictors* and split the dataset into a training and test sets. ","a2926c0e":"First of all let's check for missing values and outliers.","9aa5dc40":"We will attempt training SVM and Random Forest classifiers. We will tune their parameters, compare them and choose the best classifier for our problem. Here we will prepare a scikit-learn pipeline for data preparation (preprocessing pipeline), which we will apply later. The use of his pipelines ensures that preprocessing is applied separately to any training validation and test dataset without data leakage.\n\nFirst, we need to prepare two lists of column names. The list `names_outliers` contains the names of the two columns to which we will apply log transformation followed by RobustScaler. The list `names_no_outliers` contains the names of all other predictor columns to which we will apply StandardScaler.","207d02cf":"## EDA","8215a3a1":"First, we transform the `diagnosis` column to binary.","d86494e8":"Then we need another class to be able to select a particular group of columns.","03020aad":"The three `area` columns appear to have most prominent outliers. We will apply log transformation followed by RobustScaler to them. To all other predictor columns we will apply StandardScaler.  ","d9c34787":"by [Nikola S. Nikolov](http:\/\/http:\/\/bdarg.org\/niknikolov)","b1c97152":"Now we will use the function above to evaluate the best SVM and Random Forest models found by the grid search.","85b500b7":"# TUTORIAL 4: COMPARISON OF BINARY CLASSIFIERS","97c217a6":"We can also store this model on disk.","d23cddcf":"## Grid Search for Best Parameters of Classifiers","69f51bd6":"In this exercise we use a breast cancer dataset (see https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data). All columns in this dataset are numerical except the column `diagnosis` which is ordinal. Each example is a breast cancer case with diagnosis either M (malignant) or B (benign).\n\nWe will train a binary classification model to predict the diagnosis. We assume that misclassifying diagnosis M as B is more costly than misclassifying B as M, as cases classified as M can be further tested, while malignant cases misclassified as B may remain unnoticed. In other words, we would like to train a model that has high recall for class M, while the precision of the model for class M is not that important.\n\nThe `diagnosis` column is the *dependent variable* or *target* for classification, while all other columns are the *predictors*. We will use the common convention to refer to the predictors as **X** and to the target as **y**.  \n\nIn the process of training the dataset will be split into a training set and a test set. The training set will be further split (multiple times) into a training and validation set in a cross-validation procedure for tuning the parameters of two alternative classifiers: SVM and Random Forest.\n\nNote that any data preparation of the predictors needs to be applied separately to each of these sets.","b7bd8bd2":"Both classifiers perform well with SVM being slightly better. Most important, both have high recall for class 1, which is what we were looking for. We can pick SVM to train a final model on the whole dataset. These results will vary slightly when you run this notebook again. Ideally, the code could be enhanced to run the entire experiment multiple times and average the results.\n\nWe can also examine the ROC curves of the two classifiers for class 1.","71e8b937":"## Data Preparation","6688d942":"## Train a Final Model","04034aab":"The parameter `stratify=y` ensures both classes are represented in the same proportion in both the training and the test sets.\n\nNote that after the split into a training and test sets, X_train and X_test are numpy arrays and no longer have column names. That's why we needed the class above to put the names of columns back in the preprocessing pipeline.","257d0b44":"Finally, we can train an SVM model with all data we have, assuming that the more data we have the better the model.","e694996b":"Finally, we can build the preprocessing pipeline. It first adds the column names back to a set of examples (that can be either a training, or a validation, or test set). Then it applies the two different data preparation strategies to the two groups of columns and unites them with FeatureUnion. "}}