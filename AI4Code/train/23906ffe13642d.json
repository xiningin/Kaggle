{"cell_type":{"742ea4e2":"code","2b326a55":"code","11e7decb":"code","39bfac0f":"code","827df525":"code","73f04dbe":"code","a957fbde":"code","6faa65fa":"code","c1da9d7f":"code","d4302f56":"code","1c5c5585":"code","8e4f4171":"code","e5be7103":"code","546cddb9":"code","c185d66e":"code","88d6ddfa":"code","bc5b5a38":"code","746863a4":"code","84337dfc":"code","39cbd0cd":"code","9075da57":"code","c421b108":"code","7a5a2fa0":"code","6264bbdf":"code","fe358663":"code","36f37f5e":"code","8e42e10d":"code","bfbe05b4":"code","2adced52":"code","dc1e1da7":"code","93a563eb":"code","04f9dd98":"code","e210e56f":"code","3a960bbe":"code","6bf4ab10":"code","d7d860fa":"code","6409448b":"code","463acb9e":"code","42f00752":"code","d05b7e32":"code","afa51a76":"code","461a3cca":"code","44272dac":"code","1497ef10":"code","4c316a9e":"code","7e0d6a3e":"code","e571e936":"code","a378a684":"code","c1d00956":"code","f49fb10e":"code","b7f54f14":"code","586d110f":"code","7c06ba04":"code","c4eb0f5b":"code","818c1bfa":"code","ac7d8308":"code","72f03d28":"code","3ea82792":"code","52382a9d":"code","ffbddeff":"code","0e66bde4":"code","7bbc69a9":"code","d5b8de73":"code","05635b55":"code","9947a385":"code","0460100f":"code","830d1c60":"code","a582884d":"code","b62b248c":"code","2506543a":"code","4f95e99a":"code","cbc5d391":"code","36511f03":"code","f1a43705":"code","7fa1183e":"code","8c3cc7eb":"code","e1f8bf2a":"code","8d83d46a":"code","d1d93f8c":"code","511652a7":"code","f1b4aa8c":"code","2fcec4c9":"code","43b22bde":"code","f9ce75ff":"code","52f7055d":"code","21f82b94":"code","a9cadda9":"code","e5533d9f":"code","3974e93b":"code","3369a490":"code","770eb1c2":"code","581b11c1":"code","55a3760d":"markdown","5ab54992":"markdown","175ab257":"markdown","6502f0a7":"markdown","1e11841c":"markdown","02ecb72b":"markdown","5f5a59dc":"markdown","c96ce7ff":"markdown","e6ad26f7":"markdown","5946b101":"markdown","4cf7d6fc":"markdown","4712f98a":"markdown","c1be1249":"markdown","bc8c8846":"markdown","ed80feb2":"markdown","e0bd2b39":"markdown","d1830720":"markdown","42b0871f":"markdown","5b39517b":"markdown","436bf374":"markdown","d7a75ba0":"markdown","29f1d961":"markdown","c4e80883":"markdown","8064acad":"markdown","c319a91b":"markdown","e7564961":"markdown","ac7bcb5f":"markdown","5b3968e8":"markdown","415044a9":"markdown","4f1ef55f":"markdown","16e89c9a":"markdown","2aebf827":"markdown","079d572c":"markdown","1fed19a2":"markdown","8f28c6cd":"markdown","d39fc8f5":"markdown","1b9edfc3":"markdown","d1939147":"markdown","435b5137":"markdown","8a82b47a":"markdown"},"source":{"742ea4e2":"##################################################\n# Imports\n##################################################\n\nimport numpy as np\n#import cv2\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf \nfrom tensorflow import keras\nimport seaborn as sns\n\n#SKlearn linear classifiers and metrics\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import precision_score, recall_score\nsns.set_style(\"darkgrid\")\n##################################################\n# Params\n##################################################\n\nDATA_BASE_FOLDER = '\/kaggle\/input\/image-classification-fashion-mnist'","2b326a55":"##################################################\n# Load dataset\n##################################################\n\nx_train = np.load(os.path.join(DATA_BASE_FOLDER, 'train.npy'))\nx_valid = np.load(os.path.join(DATA_BASE_FOLDER, 'validation.npy'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy'))\ny_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))['class'].values\ny_valid = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))['class'].values\ny_labels = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# Plot random images of different classes\nplt.figure(figsize=(25, 5))\nfor idx in range(20):\n    plt.subplot(1, 20, idx + 1)\n    img = x_train[idx].reshape(28, 28)\n    plt.title(f'{y_labels[y_train[idx]]}')\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\nplt.show()\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0].hist(y_train, density=True, bins=10)\naxes[1].hist(y_valid, density=True, bins=10)\nplt.show() # from plot we can see that the dataset is balanced, hence accuracy metrics is a reliable metric","11e7decb":"##################################################\n# Process the data here, if needed\n##################################################\n\n'''\nAny manipulation of the dataset in order to feed the data to the algorithm in the correct \"format\".\n'''\n#Normalization\nx_train_norm, x_valid_norm = (x_train-np.mean(x_train, axis=0)) \/ np.std(x_train, axis=0), (x_valid-np.mean(x_valid, axis=0)) \/ np.std(x_valid, axis=0)\n\n# Data loading for TensorFlow and Keras, doc: (https:\/\/www.tensorflow.org\/tutorials\/load_data\/numpy)\nBATCH_SIZE = 100 #random choise right now\nSHUFFLE_BUFFER_SIZE = 100\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((x_valid_norm, y_valid))\n#shuffle parameter doc: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#shuffle\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE, seed=1).batch(BATCH_SIZE) \nvalid_dataset = valid_dataset.batch(BATCH_SIZE)","39bfac0f":"##################################################\n# Evaluate the model here\n##################################################\n\n# Use this function to evaluate your model\ndef accuracy(y_pred, y_true):\n    '''\n    input y_pred: ndarray of shape (N,)\n    input y_true: ndarray of shape (N,)\n    '''\n    return (1.0 * (y_pred == y_true)).mean()\n\n# Report the accuracy in the train and validation sets.","827df525":"#SVM class for linear and non linear kernel classifiers\nclass SVM:\n    def __init__(self, kernel, C, degree=3, max_iter=-1):\n        self.kernel = kernel\n        self.C = C\n        self.degree = degree\n        self.max_iter = max_iter\n    \n    def train_predict(self, x_train, y_train, x_valid, y_valid):\n        if self.kernel == 'linear':\n            if self.max_iter == -1:\n                self.max_iter = 1000\n            svm_model = LinearSVC(tol=1e-3, C=1, verbose=1, penalty='l2', loss='squared_hinge', max_iter=self.max_iter)\n            svm_model.fit(x_train_norm, y_train)\n            tr_pred = svm_model.predict(x_train)\n            val_pred = svm_model.predict(x_valid)\n        else:\n            svm_model = SVC(kernel=self.kernel, gamma='scale', degree=self.degree, C=self.C, tol=0.001, verbose=False, max_iter=self.max_iter)\n            svm_model.fit(x_train_norm, y_train)\n            tr_pred = svm_model.predict(x_train)\n            val_pred = svm_model.predict(x_valid)\n            \n        self.tr_acc = SVM.accuracy(tr_pred, y_train)\n        self.val_acc = SVM.accuracy(val_pred, y_valid)\n        return self.tr_acc, self.val_acc\n        \n    def accuracy(y_pred, y_true):\n        return (1.0 * (y_pred == y_true)).mean()\n    def to_dict(self):\n        return {'kernel': self.kernel, 'degree': self.degree, 'C':self.C, 'tr_acc':self.tr_acc, 'val_acc':self.val_acc}\n    \n    def plot_poly_acc(models):\n        \n        df_poly = pd.DataFrame([m.to_dict() for m in models])\n        df_poly['tr_acc'] = df_poly['tr_acc'] * 100\n        df_poly['val_acc'] = df_poly['val_acc'] * 100\n        df_poly['degree, C'] = \"D: \" + df_poly['degree'].astype(str) + \", C: \" + df_poly['C'].astype(str)\n        df_poly.set_index('degree, C', inplace=True)\n        df_poly_plot = df_poly[['tr_acc', 'val_acc']]\n        \n        fig, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(15, 5))\n        bars = df_poly_plot.plot.barh(title='SVM poly Accuracy', ax=axes[0])\n        axes[0].set_xlabel(\"Accuracy %\")\n        df_poly_err = df_poly['tr_acc'] - df_poly['val_acc']\n        df_poly_err = df_poly_err.abs()\n        df_poly_err.plot.barh(title='SVM Poly Error', ax=axes[1])\n        axes[1].set_xlabel(\"Abs error\")\n        fig.tight_layout()\n        plt.show()\n        \n    def plot_acc(models, title='RBF Gaussian SVM'):\n        fig, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(15, 5))\n        df = pd.DataFrame([m.to_dict() for m in models])\n        df['tr_acc'] = df['tr_acc'] * 100\n        df['val_acc'] = df['val_acc'] * 100\n        df.set_index('C', inplace=True)\n        df = df[['val_acc', 'tr_acc']]\n        df.plot.barh(title=title, ax=axes[0])\n        axes[0].set_xlabel(\"Accuracy %\")\n        df_err = df['tr_acc'] - df['val_acc']\n        df_err = df_err.abs()\n        df_err.plot.barh(title=f'{title} Error', ax = axes[1])\n        axes[1].set_xlabel(\"Abs error\")\n        fig.tight_layout()\n        plt.show()","73f04dbe":"svm_models = [SVM('linear', 1, max_iter=10), SVM('linear', 0.1, max_iter=10)] \nprint('linear SVM')\nfor run_model in svm_models:\n    tr_acc, val_acc = run_model.train_predict(x_train_norm, y_train, x_valid_norm, y_valid)\n    print(f'Training accuracy {tr_acc}')\n    print(f'Validation accuracy {val_acc}')\nSVM.plot_acc(svm_models, title='Linear SVM')","a957fbde":"keras_model_final = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_final.compile(loss = 'sparse_categorical_crossentropy',\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_final.summary()\n\nhistory = keras_model_final.fit(train_dataset, epochs=15, validation_data=valid_dataset)","6faa65fa":"fig = plt.figure()\nax1 = fig.add_subplot(1,2, 1)\nax2 = fig.add_subplot(1,2, 2)\npd.DataFrame(history.history).loc[:,['loss','val_loss']].plot(figsize=(7,3), ax=ax1)\nax1.set_title('loss')\npd.DataFrame(history.history).loc[:,['accuracy','val_accuracy']].plot(figsize=(7,3), ax=ax2)\nax2.set_title('accuracy')\nplt.subplots_adjust(wspace=.3)\nplt.show()\n#plt.savefig('final_model.png')","c1da9d7f":"# Single plot \npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n\n#########################\n# confusion matrix for validation set.\n########################\nimport scikitplot as skplt\ny_pred_prob = keras_model_final.predict(x_valid_norm)\ny_pred = np.argmax(y_pred_prob, axis=1)\nfig, axes = plt.subplots(figsize=(10, 10))\nskplt.metrics.plot_confusion_matrix(y_valid, \n      y_pred, normalize=True, title='ConfMatrix', ax = axes)\nplt.show()","d4302f56":"##################################################\n# Save your test prediction in y_test_pred\n##################################################\nx_test_norm = (x_test - np.mean(x_test, axis=0)) \/ np.std(x_test, axis=0)\ny_test_pred_prob = keras_model_final(x_test_norm)\ny_test_pred = np.argmax(y_test_pred_prob, axis=1)\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nif y_test_pred is not None:\n    submission['class'] = y_test_pred\nsubmission.to_csv('my_submission.csv', index=False)","1c5c5585":"########################################### Heavy computation ##############################\nrun_models = [SVM('poly', 0.01, 3), SVM('poly', 0.1, 3), SVM('poly', 1, 3),SVM('poly', 5, 3),  SVM('poly', 1, 4)]\npoly_models = [SVM('poly', 0.01, 3), SVM('poly', 0.1, 3)] \nprint('Poly SVM')\nfor run_model in poly_models:\n    tr_acc, val_acc = run_model.train_predict(x_train_norm, y_train, x_valid_norm, y_valid)\n    #print(f'Training accuracy {tr_acc}')\n    #print(f'Validation accuracy {val_acc}')\nSVM.plot_poly_acc(run_models)\n\nrbf_models = [SVM('rbf', 1), SVM('rbf', 0.01), SVM('rbf', 0.1),SVM('rbf', 10), SVM('rbf', 5)] \nprint('RBF SVM')\nfor run_model in rbf_models:\n    tr_acc, val_acc = run_model.train_predict(x_train_norm, y_train, x_valid_norm, y_valid)\n    #print(f'Training accuracy {tr_acc}')\n    #print(f'Validation accuracy {val_acc}')\nSVM.plot_acc(rbf_models)","8e4f4171":"# let's start with the basic model:\n# 3 layers with the same number of neurons per each layer\nkeras_model_1a = keras.models.Sequential([\n    keras.layers.Dense(200, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_1a.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_1a.summary()\n\nhistory = keras_model_1a.fit(train_dataset, epochs=10, validation_data=valid_dataset)","e5be7103":"# 3 layers\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","546cddb9":"# 5 layers\nkeras_model_1b = keras.models.Sequential([\n    keras.layers.Dense(200, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_1b.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_1b.summary()\n\nhistory = keras_model_1b.fit(train_dataset, epochs=10, validation_data=valid_dataset)","c185d66e":"# 5 layers\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","88d6ddfa":"# 8 layers\nkeras_model_1c = keras.models.Sequential([\n    keras.layers.Dense(200, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(200, activation='relu'), \n    keras.layers.Dense(200, activation='relu'), \n    keras.layers.Dense(200, activation='relu'), \n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_1c.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_1c.summary()\n\nhistory = keras_model_1c.fit(train_dataset, epochs=10, validation_data=valid_dataset)","bc5b5a38":"# 8 layers\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","746863a4":"# ReLU, ELU, Softmax\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_2a = keras.models.Sequential([\n    keras.layers.Dense(200, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(200, activation='elu'),\n    keras.layers.Dense(10, activation='Softmax')\n])\nkeras_model_2a.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_2a.summary()\n\nhistory = keras_model_2a.fit(train_dataset, epochs=10, validation_data=valid_dataset)","84337dfc":"# ReLU, ELU, Softmax\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","39cbd0cd":"# ELU, ReLU, Softmax\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_2b = keras.models.Sequential([\n    keras.layers.Dense(200, activation='elu',input_shape=(784,)), \n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_2b.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_2b.summary()\n\nhistory = keras_model_2b.fit(train_dataset, epochs=10, validation_data=valid_dataset)","9075da57":"# ELU, ReLU, Softmax\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","c421b108":"# ReLU, ReLU, Softmax\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_2c = keras.models.Sequential([\n    keras.layers.Dense(200, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_2c.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_2c.summary()\n\nhistory = keras_model_2c.fit(train_dataset, epochs=10, validation_data=valid_dataset)","7a5a2fa0":"# ReLU, ReLU, Softmax\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","6264bbdf":"# ELU, ELU, Softmax\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_2d = keras.models.Sequential([\n    keras.layers.Dense(200, activation='elu',input_shape=(784,)), \n    keras.layers.Dense(200, activation='elu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_2d.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_2d.summary()\n\nhistory = keras_model_2d.fit(train_dataset, epochs=10, validation_data=valid_dataset)","fe358663":"# ELU, ELU, Softmax\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","36f37f5e":"# 100 units per 2 layers\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_3ai = keras.models.Sequential([\n    keras.layers.Dense(100, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(100, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_3ai.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_3ai.summary()\n\nhistory = keras_model_3ai.fit(train_dataset, epochs=10, validation_data=valid_dataset)","8e42e10d":"# 100 units per 2 layers\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","bfbe05b4":"# 300 units per 2 layers\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_3aiii = keras.models.Sequential([\n    keras.layers.Dense(300, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_3aiii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_3aiii.summary()\n\nhistory = keras_model_3aiii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","2adced52":"print('accuracy', round(0.8972-0.8832,5))\nprint('loss', round(0.3239-0.2733,4))\n\n# 300 units per 2 layers\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","dc1e1da7":"# 300 units per 2 layers\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_3bi = keras.models.Sequential([\n    keras.layers.Dense(300, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(200, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_3bi.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_3bi.summary()\n\nhistory = keras_model_3bi.fit(train_dataset, epochs=10, validation_data=valid_dataset)","93a563eb":"print('accuracy', round(0.8984-0.8835,4))\nprint('loss', round(0.3240-0.2738,4))\n\n# 300, 200, 10\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","04f9dd98":"# 200, 100, 10\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_3bii = keras.models.Sequential([\n    keras.layers.Dense(200, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(100, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_3bii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_3bii.summary()\n\nhistory = keras_model_3bii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","e210e56f":"print('accuracy', round(0.8964-0.8805,4))\nprint('loss', round(0.3285-0.2830,4))\n\n# 200, 100, 10\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","3a960bbe":"# 300, 100, 10\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_3biii = keras.models.Sequential([\n    keras.layers.Dense(300, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(100, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_3biii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_3biii.summary()\n\nhistory = keras_model_3biii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","6bf4ab10":"print('accuracy', round(0.8977-0.8802,4))\nprint('loss', round(0.3328-0.2793,4))\n\n# 300, 100, 10\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","d7d860fa":"# 400, 300, 10\n# 3 layers with the same number of neurons per each layer\n# SGD = 0.01, momentum = 0.9\nkeras_model_3biv = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_3biv.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_3biv.summary()\n\nhistory = keras_model_3biv.fit(train_dataset, epochs=10, validation_data=valid_dataset)","6409448b":"# RandomNormal\nkeras_model_4a = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,), kernel_initializer = keras.initializers.RandomNormal(stddev=0.01),), \n    keras.layers.Dense(300, activation='relu',kernel_initializer = keras.initializers.RandomNormal(stddev=0.01),), \n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_4a.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_4a.summary()\n\nhistory = keras_model_4a.fit(train_dataset, epochs=10, validation_data=valid_dataset)","463acb9e":"# RandomNormal\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","42f00752":"# RandomUniform\nkeras_model_4b = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,), kernel_initializer = keras.initializers.RandomUniform(minval=0.0, maxval=1.0, seed=None)), \n    keras.layers.Dense(300, activation='relu',kernel_initializer = keras.initializers.RandomUniform(minval=0.0, maxval=1.0, seed=None)), \n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_4b.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_4b.summary()\n\nhistory = keras_model_4b.fit(train_dataset, epochs=10, validation_data=valid_dataset)","d05b7e32":"# RandomUniform\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","afa51a76":"# dropout = 0.2\nkeras_model_5a = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_5a.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_5a.summary()\n\nhistory = keras_model_5a.fit(train_dataset, epochs=10, validation_data=valid_dataset)","461a3cca":"# dropout = 0.2\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","44272dac":"# dropout = 0.5\nkeras_model_5c = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.5),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.5),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_5c.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_5c.summary()\n\nhistory = keras_model_5c.fit(train_dataset, epochs=10, validation_data=valid_dataset)","1497ef10":"# dropout = 0.5\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","4c316a9e":"# SDG with lr= 0.01\nkeras_model_6ai = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6ai.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_6ai.summary()\n\nhistory = keras_model_6ai.fit(train_dataset, epochs=10, validation_data=valid_dataset)","7e0d6a3e":"# SDG with lr= 0.01\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","e571e936":"# SDG with lr= 0.05\nkeras_model_6aii = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6aii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.05, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_6aii.summary()\n\nhistory = keras_model_6aii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","a378a684":"# SDG with lr= 0.05\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","c1d00956":"# SDG with lr= 0.01\nkeras_model_6aiii = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6aiii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.1, momentum = 0.9, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_6aiii.summary()\n\nhistory = keras_model_6aiii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","f49fb10e":"# SDG with lr= 0.1\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","b7f54f14":"# Adam, lr = 0.01\nkeras_model_6bi = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6bi.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.Adam(0.01),\n                        metrics=[\"accuracy\"])\nkeras_model_6bi.summary()\n\nhistory = keras_model_6bi.fit(train_dataset, epochs=10, validation_data=valid_dataset)","586d110f":"# Adam, lr = 0.01\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","7c06ba04":"# Adam, lr = 0.05\nkeras_model_6bii = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6bii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.Adam(0.05),\n                        metrics=[\"accuracy\"])\nkeras_model_6bii.summary()\n\nhistory = keras_model_6bii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","c4eb0f5b":"# Adam, lr = 0.05\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","818c1bfa":"# Adam, lr = 0.1\nkeras_model_6biii = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6biii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.Adam(0.1),\n                        metrics=[\"accuracy\"])\nkeras_model_6biii.summary()\n\nhistory = keras_model_6biii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","ac7d8308":"# Adam, lr = 0.1\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","72f03d28":"# Adagrad, lr = 0.01\nkeras_model_6ci = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6ci.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.Adagrad(0.01),\n                        metrics=[\"accuracy\"])\nkeras_model_6ci.summary()\n\nhistory = keras_model_6ci.fit(train_dataset, epochs=10, validation_data=valid_dataset)","3ea82792":"# Adagrad, lr = 0.01\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","52382a9d":"# Adagrad, lr = 0.05\nkeras_model_6cii = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6cii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.Adagrad(learning_rate=0.05),\n                        metrics=[\"accuracy\"])\nkeras_model_6cii.summary()\n\nhistory = keras_model_6cii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","ffbddeff":"# Adagrad, lr = 0.05\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","0e66bde4":"# Adagrad, lr = 0.1\nkeras_model_6ciii = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_6ciii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.Adagrad(0.1),\n                        metrics=[\"accuracy\"])\nkeras_model_6ciii.summary()\n\nhistory = keras_model_6ciii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","7bbc69a9":"# Adagrad, lr = 0.1\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","d5b8de73":"# momentum = 0.99\nkeras_model_7b = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_7b.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.99, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_7b.summary()\n\nhistory = keras_model_7b.fit(train_dataset, epochs=10, validation_data=valid_dataset)","05635b55":"# momentum = 0.99\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","9947a385":"# momentum = 0.8\nkeras_model_7c = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_7c.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_7c.summary()\n\nhistory = keras_model_7c.fit(train_dataset, epochs=10, validation_data=valid_dataset)","0460100f":"# momentum = 0.8\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","830d1c60":"# nesterov = False\nkeras_model_8aii = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_8aii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = False),\n                        metrics=[\"accuracy\"])\nkeras_model_8aii.summary()\n\nhistory = keras_model_8aii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","a582884d":"# Nesterov = False\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","b62b248c":"# nesterov = True\nkeras_model_8aii = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_8aii.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_8aii.summary()\n\nhistory = keras_model_8aii.fit(train_dataset, epochs=10, validation_data=valid_dataset)","2506543a":"# Nesterov = True\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","4f95e99a":"## Test with a batch-size = 200\n\nBATCH_SIZE = 200\nSHUFFLE_BUFFER_SIZE = 100\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((x_valid_norm, y_valid))\n#shuffle parameter doc: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#shuffle\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE, seed=1).batch(BATCH_SIZE) \nvalid_dataset = valid_dataset.batch(BATCH_SIZE)\n\n######################################\n\nkeras_model_8b = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_8b.compile(loss = 'sparse_categorical_crossentropy',\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_8b.summary()\n\nhistory = keras_model_8b.fit(train_dataset, epochs=10, validation_data=valid_dataset)","cbc5d391":"# batch-size = 200\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","36511f03":"## Test with a batch-size = 50\n\nBATCH_SIZE = 50\nSHUFFLE_BUFFER_SIZE = 100\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((x_valid_norm, y_valid))\n#shuffle parameter doc: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#shuffle\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE, seed=1).batch(BATCH_SIZE) \nvalid_dataset = valid_dataset.batch(BATCH_SIZE)\n\n######################################\n\nkeras_model_8c = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_8c.compile(loss = 'sparse_categorical_crossentropy',\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_8c.summary()\n\nhistory = keras_model_8c.fit(train_dataset, epochs=10, validation_data=valid_dataset)","f1a43705":"# batch-size = 50\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","7fa1183e":"BATCH_SIZE = 100\nSHUFFLE_BUFFER_SIZE = 100\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((x_valid_norm, y_valid))\n#shuffle parameter doc: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#shuffle\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE, seed=1).batch(BATCH_SIZE) \nvalid_dataset = valid_dataset.batch(BATCH_SIZE)","8c3cc7eb":"# 30 epoch\nkeras_model_9a = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_9a.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_9a.summary()\n\nhistory = keras_model_9a.fit(train_dataset, epochs=30, validation_data=valid_dataset)","e1f8bf2a":"# 30 epochs\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","8d83d46a":"# 50 epoch\nkeras_model_9b = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_9b.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_9b.summary()\n\nhistory = keras_model_9b.fit(train_dataset, epochs=50, validation_data=valid_dataset)","d1d93f8c":"# 50 epochs\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","511652a7":"# 80 epoch\nkeras_model_9c = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_9c.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_9c.summary()\n\nhistory = keras_model_9c.fit(train_dataset, epochs=80, validation_data=valid_dataset)","f1b4aa8c":"# 80 epochs\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","2fcec4c9":"keras_model_final = keras.models.Sequential([\n    keras.layers.Dense(400, activation='relu',input_shape=(784,)), \n    keras.layers.Dropout(.2),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dropout(.2),\n    keras.layers.Dense(10, activation='softmax')\n])\nkeras_model_final.compile(loss = 'sparse_categorical_crossentropy',\n                        optimizer=keras.optimizers.SGD(0.01, momentum = 0.8, nesterov = True),\n                        metrics=[\"accuracy\"])\nkeras_model_final.summary()\n\nhistory = keras_model_final.fit(train_dataset, epochs=15, validation_data=valid_dataset)","43b22bde":"fig = plt.figure()\nax1 = fig.add_subplot(1,2, 1)\nax2 = fig.add_subplot(1,2, 2)\npd.DataFrame(history.history).loc[:,['loss','val_loss']].plot(figsize=(7,3), ax=ax1)\nax1.set_title('loss')\npd.DataFrame(history.history).loc[:,['accuracy','val_accuracy']].plot(figsize=(7,3), ax=ax2)\nax2.set_title('accuracy')\nplt.subplots_adjust(wspace=.3)\nplt.show()\n#plt.savefig('final_model.png')\n# rimpicciolire il grafico","f9ce75ff":"pd.DataFrame(history.history).loc[:,['accuracy','val_accuracy']].plot(figsize=(8, 5))","52f7055d":"# final model\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","21f82b94":"#########################\n# confusion matrix \n########################\n\nimport scikitplot as skplt\ny_pred_prob = keras_model_final.predict(x_valid_norm)\ny_pred = np.argmax(y_pred_prob, axis=1)\nfig, axes = plt.subplots(figsize=(10, 10))\nskplt.metrics.plot_confusion_matrix(y_valid, \n      y_pred, normalize=True, title='ConfMatrix', ax = axes)\nplt.show()","a9cadda9":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_valid, y_pred, average = 'micro')","e5533d9f":"recall_score(y_valid, y_pred,average = 'micro')","3974e93b":"########################################### Heavy computation ##############################\nrun_models = [SVM('poly', 0.01, 3), SVM('poly', 0.1, 3), SVM('poly', 1, 3),SVM('poly', 5, 3), \n              SVM('poly', 10, 4), SVM('poly', 0.01, 4), SVM('poly', 5, 4), SVM('poly', 1, 4)]\npoly_models = [SVM('poly', 0.01, 3), SVM('poly', 0.1, 3)] \nprint('Poly SVM')\nfor run_model in poly_models:\n    tr_acc, val_acc = run_model.train_predict(x_train_norm, y_train, x_valid_norm, y_valid)\n    #print(f'Training accuracy {tr_acc}')\n    #print(f'Validation accuracy {val_acc}')\nSVM.plot_poly_acc(run_models)\n\nrbf_models = [SVM('rbf', 1), SVM('rbf', 0.01), SVM('rbf', 0.1),SVM('rbf', 10), SVM('rbf', 5)] \nprint('RBF SVM')\nfor run_model in rbf_models:\n    tr_acc, val_acc = run_model.train_predict(x_train_norm, y_train, x_valid_norm, y_valid)\n    #print(f'Training accuracy {tr_acc}')\n    #print(f'Validation accuracy {val_acc}')\nSVM.plot_acc(rbf_models)","3369a490":"keras_model = keras.models.Sequential([\n    keras.layers.Dense(10, activation='softmax', input_shape=(784,), kernel_initializer='random_normal')\n])\nkeras_model.compile(loss=\"sparse_categorical_crossentropy\",\n                        optimizer=\"sgd\",\n                        metrics=[\"accuracy\"])\nhistory = keras_model.fit(train_dataset, epochs=30, validation_data=valid_dataset)\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\nplt.show()","770eb1c2":"svm_models = [SVM('linear', 1, max_iter=10), SVM('linear', 0.1, max_iter=10)] \nprint('linear SVM')\nfor run_model in svm_models:\n    tr_acc, val_acc = run_model.train_predict(x_train_norm, y_train, x_valid_norm, y_valid)\n    print(f'Training accuracy {tr_acc}')\n    print(f'Validation accuracy {val_acc}')\nSVM.plot_acc(svm_models, title='Linear SVM')","581b11c1":"############# Preprocessing one hot coding ###########\none_y_train = np.zeros((len(y_train), len(y_labels)))\ni = 0\nfor y_value in y_train:\n    one_y_train[i, y_value] = 1.0\n    i +=1\n    \none_y_valid = np.zeros((len(y_valid), len(y_labels)))\ni = 0\nfor y_value in y_valid:\n    one_y_train[i, y_value] = 1.0\n    i +=1\n    \ndef predict_linearclassifier(x, theta):\n    h = np.dot(x, theta)\n    y = 1.0 * (h >= 0) - 1.0 * (h < 0)\n    return y\n\ndef gradient(y_true, y_pred, x):\n    dJ = x.T.dot((y_pred - y_true)) \/ len(x)\n    return dJ\n\ndef mse(y_true, y_pred):\n    norm_2 = np.sum((y_true - y_pred)**2)\n    J = norm_2 \/ (2 * len(y_true))\n    return J\n\ndef sigmoid(z):\n    exp = np.exp(-z)\n    s = 1\/(1+exp)\n    return s\n\ndef xent(y_true, y_pred):\n    cost = -y_true * np.log(y_pred)-(1-y_true) * np.log(1-y_pred)\n    J = np.sum(cost)\/len(y_true)\n    return J\n\ndef gradient_descent_mini_batch(x, y, X_val, y_val, activation_func, cost_func, gradient_func, \n                     epochs=400, seed=1234, lr=0.01, print_every=10, batch_size=200):\n\n    # Initialize theta parameters\n    np.random.seed(seed)\n    theta = np.random.normal(0, 0.001, size=(x.shape[1], 1)) \/ np.sqrt(2)\n\n    # Iterations of gradient descent\n    loss = []\n    train_acc_history = []\n    val_acc_history = []\n    print('Training...')\n    num_train = x.shape[0]\n    iterations_per_epoch = max(num_train \/ batch_size, 1)\n    for epoch in range(epochs + 1):\n        idx_batch = np.random.randint(num_train, size=batch_size)\n        X_batch, y_batch = x[idx_batch, :], y[idx_batch, :]\n        \n        # Model prediction\n        z = X_batch.dot(theta)\n        h = activation_func(z) if activation_func is not None else z\n        loss.append(cost_func(y_batch, h))\n        dJ = gradient(y_batch, h, X_batch)\n        theta = theta - lr * dJ\n          \n        # Print loss info\n        if epoch % print_every == 0:\n            print(f'Epoch {epoch}: Loss {loss[-1]}')\n            \n        if epoch % iterations_per_epoch == 0:\n            # Check accuracy\n            train_acc = (np.argmax(predict_linearclassifier(X_batch, theta), axis=1) == np.argmax(y_batch, axis=1)).mean()\n            val_acc = (np.argmax(predict_linearclassifier(X_val, theta), axis=1) == y_val).mean()\n            train_acc_history.append(train_acc)\n            val_acc_history.append(val_acc)\n        \n    return theta, loss, train_acc_history, train_acc_history\n\nweights, loss_lc, hist_acc, hist_val = gradient_descent_mini_batch(x_train_norm, one_y_train, x_valid_norm, y_valid, activation_func=sigmoid, \n                                          cost_func=xent, \n                                          gradient_func=gradient, epochs=10000, \n                                          lr=0.001, print_every=1000, batch_size=100)\n\nfigx, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\naxes[0].plot(loss_lc, lw=2)\nplt.grid(True, zorder=0, alpha=0.6, ls='--')\nplt.xlabel('Epoch')\naxes[1].plot(hist_val)\naxes[1].plot(hist_acc)\nplt.show()","55a3760d":"We concluded that linear models were not feasible at all for our objectives, accuracy of 75\\% is reached with linear SVM. They highlighted the high complexity of the data, which is much more complex than MNIST, thus more advanced models had to be explored that embody non-linearity properties.","5ab54992":"# MLP Test assessment for Hyperparameters search.\n\n1. Choice of number of layers:\n    * **a**.   3 layers\n    * **b**.   5 layers\n    * **c**.   8 layers\n\n2. Choice of number of neurons per layers: \n    * **a**. same number: \n        - i. 100\n        - ii. 200 \n        - iii. 300\n    * **b**. different numbers: \n        - i. 300, 200, 10\n        - ii. 200, 100, 10\n        - iii. 300, 100, 10\n        - iv. 400, 300, 10\n        \n3. Choice of activation functions:\n    * **a**. ReLU, ELU, Softmax\n    * **b**. ELU, ReLU, Softmax\n    * **c**. ReLU, ReLU, Softmax\n    * **d**. ELU, ELU, Softmax\n\n4. kernel initializer:\n    * **a**. RandomNormal\n    * **b**. RandomUniform\n    * **c**. GlorotNormal (default)\n\n5. dropout rate:\n    * **a**. 0.2\n    * **b**. 0\n    * **c**. 0.5\n\n6. optimazer:\n    * **a**. SGD:\n        - i. 0.01\n        - ii. 0.05 \n        - iii. 0.1\n    * **b**. Adam:\n        - i. 0.01\n        - ii. 0.05 \n        - iii. 0.1\n    * **c**. Adagram:\n        - i. 0.01\n        - ii. 0.05 \n        - iii. 0.1\n    \n7. momentum\/nesterov:\n    * **a**. 0.9 (start)\n    * **b**. 0.99\n    * **c**. 0.8\n\n8. batch-size\n    * **a**. 100 (start)\n    * **b**. 200\n    * **c**. 50\n\n9. number of epochs\n    * **a**. 30\n    * **b**. 50\n    * **c**. 80","175ab257":"## Linear SVM","6502f0a7":"## 6. OPTIMAZER & LEARNING-RATE","1e11841c":"# Send the submission for the challenge","02ecb72b":"## Non-linear SVM","5f5a59dc":"# Non-linear SVM Models\nWe show some meaning results from non-linear SVM tests. The full assessment can be found in the very below, since they require long time computation.\nNote that even this test is ask expensive computation.","c96ce7ff":"## 1. NUMBER OF LAYERS","e6ad26f7":"## 2. CHOICE OF ACTIVATION FUNCTIONS","5946b101":"## 3. NUMBER OF UNITS PER LAYER\n\na. Same number of units per layers","4cf7d6fc":"### dropout = 0.2 ","4712f98a":"## Scatch Linear Classifier and logistic regression model","c1be1249":"# Test assessment for SVM and Multinomial regression non-linear Classifier\n\n1. Non-linear SVM\n    1.1 Polynomial kernel\n    2.2 Gaussian kern or Radial Basis function (RBF) kernel\n2. Multinomial logistic regression or softmax regression as generalized multi-regression model\n","bc8c8846":"## 9. NUMBER OF EPOCHS","ed80feb2":"### 400, 300, 10 units per layer\n\nThe low value of the val_loss and high value of val_accuracy + the difference between the accuracy\/val_accuracy and loss\/val_loss make us to choose for the last model with 400, 300, 10 units per layer.","e0bd2b39":"### 15 epochs \nThe accuracy is alway the same changing the number of epochs, so we can keep 20 epochs because it has better val_loss.","d1830720":"### momentum = 0.8 is the one that gives better accuracy\n\n#### Nesterov","42b0871f":"Authors: Gurjeet Singh (gurjeet.singh@studenti.unipd.it), Francesca Zen (francesca.zen@studenti.unipd.it)","5b39517b":"### GlorotUniform as kernel_initializer\nIn comparison with the GlorotUniform model we can see that the one that has as kernel initializer the RandomUniform isn't good at all, the one with RandomNormal is good but Glorot is better and we keep it.","436bf374":"### set 3 layers\n\nA smaller net, with only 3 layers seems to have better accuracy on the accuracy and val_loss, so we will keep 3 layers on our architecture.","d7a75ba0":"## Softmax regression","29f1d961":"# Welcome to the Fashion-MNIST Challenge!\n\nWebsite reference: https:\/\/github.com\/zalandoresearch\/fashion-mnist","c4e80883":"## Dataset complexity\n\nA little test to understand data complexity through the accuracy of linear classifiers and compare them with next complex and non-linear models (MLP, SVM non-linear kernel) presented in the notebook.","8064acad":"# Evaluation\nAs we can see above from the histogram plot, training dataset is balanced, hence accuracy metric is reliable","c319a91b":"# Linear classifiers\n\nA little test to understand the accuracy of linear classifiers and compare them with more complex model (MLP, SVM non-linear kernel) which tackle the given non linear problem.\n\n1. Linear SVM\n2. Scratch Linear classifier and Logistic regression model","e7564961":"## 4. KERNEL INITIALIZER\nThe comparason is between GlorotNormal, RandomUniform and RandomNormal. Glorot is the default kernel initializer so we will make run only the code with the RandomUniform and RandomNormal initializer.","ac7bcb5f":"### Nesterov = True gives better accuracy\n\n## 8. BATCH-SIZE","5b3968e8":"b. different units per layer","415044a9":"## MLP FINAL MODEL\nWe present now the final model picked from all other models. All tests assessments can be found below.\nMLP Description: 3 layers with activation functions ReLU, ReLU and Softmax in this order, with 400, 300 and 10 neurons respectively. The kernel initializer is GlorotUniform, dropout-rate = 0.2, SGD as optimazer with learning-rate = 0.01, momentum = 0.8 and nesterov = True. Batch-size=100 and 15 epochs.","4f1ef55f":"No overfitting issue is present, validation accuracy reaches 0.8925 against 0.9198 of the Training set","16e89c9a":"## FINAL MODEL\n3 layers with activation functions ReLU, ReLU and Softmax in this order, with 400, 300 and 10 neurons respectively. The kernel initializer is GlorotUniform, dropout-rate = 0.2, SGD as optimazer with learning-rate = 0.01, momentum = 0.8 and nesterov = True. Batch-size=100 and 15 epochs.","2aebf827":"### batch-size = 100 \nis the best choice in comparison with running time and accuracy","079d572c":"### Linear SVM","1fed19a2":"### activation functions: ReLU, ReLU, Softmax","8f28c6cd":"## 5. DROPOUT RATE","d39fc8f5":"## 7. MOMENTUM\/NESTEROV","1b9edfc3":"### SGD with lr = 0.1 is our choice for the optimazer","d1939147":"# Models\n\nHere you have to implement a model (or more models, for finding the most accurate) for classification.\n\nYou can use the [`sklearn`](https:\/\/scikit-learn.org\/stable\/) (or optionally other more advanced frameworks such as [`pytorch`](https:\/\/pytorch.org\/) or [`tensorflow`](https:\/\/www.tensorflow.org\/)) package that contains a pool of models already implemented that perform classification. (SVMs, NNs, LR, kNN, ...)","435b5137":"# Dataset\n\nThe dataset contains 50k train + 10k validation images of 10 different categories ('T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot').\n\nEach image is a 28x28 grayscale, and for simplicity here is flattened into a 784 dimensional vector.","8a82b47a":"The model with 200 units for the firts 2 layers + the last one of 10 is the model keras_model_2c."}}