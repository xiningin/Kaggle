{"cell_type":{"59da586f":"code","dae5a91a":"code","5c2bbeb9":"code","0b6a3c94":"code","bc06d337":"code","881e698b":"code","5b6028e6":"code","b7bfe7f8":"code","f1aa7335":"code","7602e87a":"code","3c21a8ff":"code","30d5784a":"code","e0165324":"code","2e1057ff":"code","f19cfc85":"code","a0ddf247":"code","eb25e148":"code","8c9eeb67":"code","ddfdcc7c":"code","66811027":"code","eceb04e7":"code","f2eb53df":"code","a09773b2":"code","6d513e67":"code","a3179cc1":"code","421f3f03":"code","e165a5f7":"code","410e5310":"code","0050c8e2":"code","e7a76c37":"code","824e1889":"code","8307ad5e":"code","ac459459":"markdown","c23593ca":"markdown","f4e545b8":"markdown","a78b9910":"markdown","c48eac8e":"markdown"},"source":{"59da586f":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# importing libraries \nfrom sklearn.ensemble import VotingClassifier ,BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score \nfrom numpy import mean,std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,train_test_split\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom matplotlib import pyplot\nfrom sklearn.datasets import load_wine,load_iris\nfrom matplotlib.pyplot import figure\nfigure(num=2, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\nimport xgboost as xgb\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom sklearn.linear_model import LinearRegression,BayesianRidge,ElasticNet,Lasso,SGDRegressor,Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,RobustScaler,StandardScaler\nfrom sklearn.pipeline import make_pipeline,Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA,KernelPCA\nfrom sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor,VotingClassifier\nfrom sklearn.model_selection import cross_val_score,KFold,GridSearchCV,RandomizedSearchCV,StratifiedKFold,train_test_split\nfrom sklearn.base import BaseEstimator,clone,TransformerMixin,RegressorMixin\nfrom sklearn.svm import LinearSVR,SVR\n#import xgboost \nfrom xgboost import XGBRegressor\n#Import Pandas\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n%matplotlib inline\nseed = 1075\nnp.random.seed(seed)","dae5a91a":"\nexercise_data = pd.read_csv( '..\/input\/exercise.csv' )\ncalories_data = pd.read_csv( '..\/input\/calories.csv' )","5c2bbeb9":"exercise_data.head() ","0b6a3c94":"calories_data.head()","bc06d337":"df = pd.merge(exercise_data,calories_data,on='User_ID', how='left')\ndf.head()","881e698b":"df.info()","5b6028e6":"sns.pairplot(df,kind = \"scatter\")","b7bfe7f8":"# in the scatter plot of duration vs calories and heart rate vs calories the relationship\n# was curved upward (not linear)\n# feature engineering:  add squared duration and heart rate to try a better fit with calories\ndf = df.assign( squared_duration = df[ 'Duration' ] ** 2 )\ndf = df.assign( squared_heart_rate = lambda x: x[ 'Heart_Rate' ] ** 2 )\n\ndf.head()","f1aa7335":"sns.pairplot(df,kind = \"scatter\")","7602e87a":"# since we don't want the prediction to be negative calories, \n# convert calories to natural logarithm to always get a positive number\nimport numpy as np\ndf = df.assign( log_Calories = lambda x: \n                                 np.log( x[ 'Calories' ] ) )\ndf.head()","3c21a8ff":"# scale numbers with normal distribution using z-score\nfrom scipy.stats import zscore\n\ndf = df.assign( zscore_body_temp = zscore( df[ 'Body_Temp' ] ) )\ndf = df.assign( zscore_height = zscore( df[ 'Height' ] ) )\ndf = df.assign( zscore_weight = zscore( df[ 'Weight' ] ) )\ndf = df.assign( zscore_squared_heart_rate = zscore( df[ 'squared_heart_rate' ] ) )\n\ndf.head()","30d5784a":"# scale non-normal columns (age, squared_duration) using Min-Max \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# NOTE:  joined_data[ ['Age', 'squared_duration'] ] produces a copy, loc doesn't\nminMaxData = pd.DataFrame( scaler.fit_transform( df.loc[ :, ['Age','squared_duration'] ] )\n                         , columns = [ 'minMaxAge', 'minMaxSquaredDuration' ] )\ndf = pd.concat( [ df, minMaxData ], axis = 1, join = 'inner' )\ndf.head()","e0165324":"# what to do with Gender (string binary categorical variable)?\n# convert to zero (male) and one (female)\n# trick:  first convert to boolean (Gender==female) , then to int by adding 0\ndf = df.assign( numeric_gender = 0 + ( df[ 'Gender' ] == 'female' ) )\ndf.head()","2e1057ff":"# exclude User_ID and log_Calories from the prediction model (they're not features)\ndel df[ 'User_ID' ]","f19cfc85":"ageDF = df[ 'Age' ]\nheartRateDF = df[ 'Heart_Rate' ]\n\n# remove unneeded columns\n\n# remove Duration and Heart_Rate\ndel df[ 'Duration' ]\ndel df[ 'Heart_Rate' ]\ndel df[ 'Calories' ]\n\n\n\n","a0ddf247":"df.pop( 'Body_Temp' )\ndf.pop( 'Height' )\ndf.pop( 'Weight' )\ndf.pop( 'squared_heart_rate' )\ndf.pop( 'Age' )\ndf.pop( 'squared_duration' )\ndf.pop( 'Gender' )","eb25e148":"df.info()","8c9eeb67":"# split data into test and training\n\nfrom sklearn.model_selection import train_test_split\nX = df.drop('log_Calories',axis = 1)\ny = df['log_Calories']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state=42)\nX_train.shape, X_test.shape\n\n#train_X,test_X,train_Y,test_Y = train_test_split( df, test_size = 0.3 )","ddfdcc7c":"X.head()","66811027":"from sklearn.tree import DecisionTreeRegressor\ndt_model = DecisionTreeRegressor(random_state=10)","eceb04e7":"dt_model.fit(X_train, y_train)","f2eb53df":"dt_model.score(X_train, y_train)","a09773b2":"dt_model.score(X_test, y_test)","6d513e67":"y_pred = dt_model.predict(X_test)","a3179cc1":"predctn =pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\npredctn","421f3f03":"from sklearn.ensemble import RandomForestRegressor","e165a5f7":"rf_model = RandomForestRegressor()\nrf_model.fit(X_train,y_train)","410e5310":"rf_model.score(X_train,y_train)","0050c8e2":"rf_model.score(X_test,y_test)","e7a76c37":"y_pred_rf = rf_model.predict(X_test)","824e1889":"rf_pred =pd.DataFrame({'Actual':y_test, 'Predicted':y_pred_rf})\nrf_pred","8307ad5e":"from sklearn.metrics import mean_squared_error, r2_score\n# The mean squared error, The MSE is a measure of the quality of an estimator\u2014it is always non-negative, and values closer to zero are better.\nprint(\"Mean squared error Random Forest: %.2f\"% mean_squared_error(y_test, y_pred_rf))\nprint(\"Mean squared error Decision Tree: %.2f\"% mean_squared_error(y_test, y_pred))\n\n# Explained variance score: 1 is perfect prediction\nprint('Test Variance score Random Forest: %.2f' % r2_score(y_test, y_pred_rf))\nprint('Test Variance score Decision Tree: %.2f' % r2_score(y_test, y_pred))","ac459459":"As you can compare the predicted and actual target variables are very closer. The model is said to perform well on unseen data. ","c23593ca":"Random forest is also said to give maximum accuracy of predictions","f4e545b8":"Tree Model for predicting the continuous variable target","a78b9910":"To determine whether this is a regression problem we should first determine whether the relationship is linear","c48eac8e":"The model evaluation metrics for regression"}}