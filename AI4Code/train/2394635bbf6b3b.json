{"cell_type":{"7d59fda8":"code","5e4b0bfc":"code","321ed6e6":"code","9c617429":"code","eaa29a1f":"code","3de428dd":"code","db0f6b0d":"code","63508f53":"code","e5a711f2":"code","29988efe":"code","0e202eff":"code","6dd3e958":"code","7b5abdf2":"code","499676a9":"code","a2db1cbb":"code","ee7f486b":"code","89db07bc":"code","7cda59a9":"code","631197f6":"code","5f23169f":"code","667b96bc":"code","24df3a33":"code","43a9f5b6":"code","6597547b":"code","6fe92cf3":"code","3f5b2d1d":"markdown","f0bc23db":"markdown","51cac3f6":"markdown","3a35ba49":"markdown","acd7344b":"markdown","2b30ccdf":"markdown","1d6ddc80":"markdown","ed3038ca":"markdown"},"source":{"7d59fda8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5e4b0bfc":"import pandas as pd\ndf = pd.read_csv('..\/input\/breast-cancer-csv\/breastCancer.csv')\ndf.head()","321ed6e6":"df.info()","9c617429":"df = df.drop(columns=[\"id\"])\ndf.head()","eaa29a1f":"df['bare_nucleoli'].value_counts()","3de428dd":"import numpy as np\ndf_absent = df[df['bare_nucleoli']=='?']\ndf_absent = df_absent.reset_index()\ndf_absent = df_absent.drop(columns=['index'])\ndf_absent.head()","db0f6b0d":"df_present = df[df['bare_nucleoli']!='?']\ndf_present = df_present.reset_index()\ndf_present = df_present.drop(columns=[\"index\"])\ndf_present = df_present.astype(np.float64)\ndf_present.head()","63508f53":"df_present_temp = df_present.drop(columns=['bare_nucleoli'])\nxm = df_present_temp.values\n\nym = df_present['bare_nucleoli'].values\n\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(xm, ym, test_size=0.2, random_state=4)","e5a711f2":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\n\nk_min = 2\ntest_MAE_array = []\nk_array = []\nMAE = 10^12\n\nfor k in range(2, 20):\n    model = KNeighborsRegressor(n_neighbors=k).fit(train_x, train_y)\n    \n    y_predict = model.predict(test_x)\n    y_true = test_y\n\n    test_MAE = mean_absolute_error(y_true, y_predict)\n    if test_MAE < MAE:\n        MAE = test_MAE\n        k_min = k\n\n    test_MAE_array.append(test_MAE)\n    k_array.append(k)\n\nplt.plot(k_array, test_MAE_array,'r')\nplt.show()\n\nprint(\"Best k parameter is \",k_min )","29988efe":"final_model = KNeighborsRegressor(n_neighbors=16).fit(xm,ym)\n\ndf_absent_temp = df_absent.drop(columns=['bare_nucleoli'])\ndf_absent_temp = df_absent_temp.astype(np.float64)\ndf_absent_temp.head()","0e202eff":"x_am = df_absent_temp.values\ny_am = final_model.predict(x_am)\ny_am","6dd3e958":"y_am = np.round(y_am)\ny_am = y_am.astype(np.int64)\ny_am","7b5abdf2":"df_pred = pd.DataFrame({'bare_nucleoli':y_am})\ndata_frame_1 = df_absent_temp.join(df_pred)\ndata_frame_1 = data_frame_1.astype(np.int64)\ndata_frame_1","499676a9":"df_join_2 = df_present['bare_nucleoli']\ndata_frame_2 = df_present_temp.join(df_join_2)\ndata_frame_2 = data_frame_2.astype(np.int64)\ndata_frame_2.head()","a2db1cbb":"data_frame = [data_frame_1, data_frame_2]\ndata_frame = pd.concat(data_frame)\ndata_frame.head()","ee7f486b":"df['class'].value_counts()","89db07bc":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1,1)\n\nax.pie(df['class'].value_counts(),explode=(0,0.1), autopct='%1.1f%%', labels = ['Benign', 'Malignant'], colors=['g','r'])\nplt.axis = 'equal'","7cda59a9":"data_frame.hist(figsize = (10, 10))\nplt.show()","631197f6":"data_frame_1 = data_frame\n\ndef num_to_class(x):\n    if x==2:\n        return 'Benign'\n    elif x==4:\n        return 'Malignant'\n\ndata_frame_1['class'] = data_frame_1['class'].apply(lambda x: num_to_class(x))\ndata_frame_1['class'].value_counts()","5f23169f":"import seaborn as sns\nfor i in range(8):\n    x = data_frame.iloc[:,i]\n    for j in range(i+1,8):\n        y = data_frame.iloc[:,j]\n        hue_parameter = data_frame['class']\n        ax = sns.scatterplot(x=x, y=y, hue=hue_parameter)\n        plt.show()","667b96bc":"X = data_frame_1.drop(columns='class').values\nY = data_frame_1['class'].values\n\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=4)\n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(train_x, train_y)\n\ny_true = test_y\ny_predict = model.predict(test_x)","24df3a33":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_1 = confusion_matrix(y_true, y_predict)\nprint(confusion_matrix_1)","43a9f5b6":"sns.heatmap(confusion_matrix_1, annot=True)","6597547b":"from sklearn.metrics import classification_report\nprint(classification_report(y_true, y_predict))","6fe92cf3":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_true, y_predict))","3f5b2d1d":"The results for confusion matrix, f1_score and the accuracy score are satisfactorily high. We dont need further analysis to evaluate this model. This model works well for this problem.","f0bc23db":"Contact at [shivam.mishra9868@gmail.com] for any suggestions or improvements. You can also connect on LinkedIn at www.linkedin.com\/in\/shivam-mishra-207.","51cac3f6":"These are the 16 bare_nuclei values generated using KNN algorithm.","3a35ba49":"In this notebook, SVM classification model is used to classify a cell as either benign or malignant. Performance of the model is evaluated using accuracy matrices.","acd7344b":"The scatter plots between two of the features in the dataframe is generated. Blue points represet \"Malignant\" class whereas orange points represent \"Benign\" Class. In many of the plots plotted here, we can see a distinction between classes or existence of a boundary of sort between malignant and benign classes. This directs to use a classification algorithm which decides classes based on some diving boundary. Here SVC (support vector classifier) is used to reach that goal.","2b30ccdf":"Here we notice, that for data frame with bare_nuclei column, 16 values are missing (filled with string value \"?\"). Here either we can drop these rows or else we can generate these missing values using remaining data. This is done using K-nearest neighbour algorithm. These 16 rows are initially dropped, and based on other 683 rows these 16 values are predicted.","1d6ddc80":"As indicated by df.info(), data-frame contains 699 rows and 11 columns.Column named as \"id\", denotes cell id and contains 699 unique values. Cell id doesn't provide any information based on which benign and malignant cells can be differentiated. This column is dropped and column containing information about cell-id is not considered in any further analysis.","ed3038ca":"Class column denotes whether cell is benign or malignant. \"2\" denotes benign class while \"4\" denotes malignant class. Let's determine the number of benign and malignant classes in the data-set."}}