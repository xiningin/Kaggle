{"cell_type":{"882d2c9b":"code","ad1fedb7":"code","d2b1fb6a":"code","173ad824":"code","5a6f3437":"code","4af117be":"code","318c18ce":"code","fffb5104":"code","ea614d25":"code","a1b26ecd":"code","d4da05d8":"code","3d657367":"code","605c504f":"code","3cb98e2c":"code","7647f721":"code","5ac9962f":"code","9092b81f":"code","c129bca3":"code","21c3bdbb":"code","993d17cd":"code","c3c3feea":"code","35d78e7f":"code","36bc88d5":"code","533f7da3":"code","6146f60c":"code","c980d7a0":"code","cf96c210":"code","5214cb52":"code","68d53dbd":"code","8dfa08f0":"code","aeb2096f":"code","035b8d4c":"code","40783c7f":"code","0623d88b":"code","8290cb1f":"code","d6479ed3":"code","88b26afc":"code","a4adf1e9":"code","50ba7049":"code","3b17e38b":"code","39f06e8d":"code","51ce4e16":"code","d9ea9389":"code","757d1f28":"code","4c63cc20":"code","4c3aac4c":"code","05b4219f":"code","ff005cdc":"code","4fc8481d":"code","4fcefe19":"code","25213897":"code","7df6c8e1":"code","ebce305e":"code","a7039ab4":"code","2e4fbbc0":"code","95867419":"code","95452a35":"code","a6a5f21a":"code","02a596a8":"code","323c2889":"code","dd90e26a":"code","7feebfc8":"code","d09ab3aa":"code","3c6313fd":"code","f400b62d":"code","3b732f25":"code","c0f596b0":"code","3fe4dd73":"code","23b60aaf":"code","4f294d31":"code","a79fd9ba":"code","cd3a86f0":"code","8f61eb66":"code","82c3e67a":"code","21560a93":"code","4b4371dd":"code","5b51567f":"code","021c906a":"code","231e3cea":"code","3c403a5f":"code","92902841":"code","d7055aea":"code","22acc23b":"code","fa82ac84":"code","9318b2a1":"code","bdac4541":"code","c8516902":"code","87fb7176":"code","0b3608bc":"markdown","64aa6543":"markdown","3317ba31":"markdown","4212a9d9":"markdown","decddd36":"markdown","cd07ebc7":"markdown","9b3dcadb":"markdown","33250c7b":"markdown","fa883210":"markdown","91691b6d":"markdown","db2e2da3":"markdown","37f47468":"markdown","87adc1b3":"markdown","11abb722":"markdown","b461c541":"markdown","f27efeb9":"markdown","e37a6f04":"markdown","4fa61ce9":"markdown","564178c2":"markdown","d9e33e0e":"markdown","a87b116e":"markdown","d3fb3636":"markdown","be0bccdc":"markdown","f4c923c3":"markdown","18bc52f6":"markdown","52e0162d":"markdown","75a902dc":"markdown","3ea9ff9e":"markdown","a8d42f66":"markdown","eb096609":"markdown","4e8a140d":"markdown","8ca89d14":"markdown","e0ffbd71":"markdown","3ec36418":"markdown","7a6cff43":"markdown","8480adda":"markdown","723d6ffa":"markdown","5eaa0d8b":"markdown","3a2ae369":"markdown","425984a1":"markdown","e82e4c76":"markdown","d5dfea16":"markdown","bcdf5d18":"markdown","00e8aea1":"markdown","d58691aa":"markdown","6edb43b4":"markdown","2493910c":"markdown","5d0f370c":"markdown","d8894512":"markdown","645cd836":"markdown","75075fa2":"markdown","17f64c3d":"markdown","75e84209":"markdown","3a790e1e":"markdown","77189250":"markdown","522baced":"markdown","242974bc":"markdown","48f0168f":"markdown","10f151a3":"markdown","04683b2d":"markdown","9e86db44":"markdown","fd010ff8":"markdown","04ddaf0a":"markdown","2ae6cdc2":"markdown","db3c5190":"markdown","dea8f564":"markdown","f52113e9":"markdown","89f6fdde":"markdown","0c9e583f":"markdown","392c7e0d":"markdown","c36f6264":"markdown","ef99c4ab":"markdown","7f092af7":"markdown","50af4d57":"markdown","21e9b296":"markdown","686ca7b8":"markdown","0bd328ab":"markdown","ea8d4b21":"markdown","4d3911d8":"markdown","42231d9f":"markdown","0cdbee5e":"markdown","a0bd0679":"markdown","75672092":"markdown"},"source":{"882d2c9b":"# imports\nimport gc\nimport time\nfrom IPython.display import Image, display\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 40)\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom termcolor import colored\n%matplotlib inline\nsns.set_style('darkgrid')\nimport plotly.express as px\n# import plotly.graph_objects as go\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nfrom sklearn.model_selection import GridSearchCV as Grid\nimport xgboost as xgb\nfrom sklearn.metrics import (mean_absolute_error, r2_score,mean_squared_error)","ad1fedb7":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","d2b1fb6a":"train.describe()","173ad824":"train.info()","5a6f3437":"train.isnull().sum()","4af117be":"features = train.drop(['target'], axis=1)","318c18ce":"num_col = list(train.select_dtypes(include='float64').columns)\ncat_cols = list(train.select_dtypes(include='object').columns)\nnum_col.remove('target')\nprint('Number of numerical columns is:',colored(len(num_col),'green'),\n      '\\nNumber of categorical columsn is:',colored(len(cat_cols),'green'))","fffb5104":"list(test.columns) == list(features.columns)","ea614d25":"test.describe()","a1b26ecd":"test.info()","d4da05d8":"test.isnull().sum()","3d657367":"lis = []\nfor i in features[cat_cols].columns:\n    test_vals = set(test[i].unique())\n    train_vals = set(features[i].unique())\n    lis.append(test_vals.issubset(train_vals))\n\nprint(colored(all(lis),'green'))","605c504f":"fig = plt.figure(figsize=(10,5))\nsns.barplot(y=train[cat_cols].nunique().values, x=train[cat_cols].nunique().index, color='blue', alpha=.5)\nplt.xticks(rotation=0)\nplt.title('Number of categorical unique values',fontsize=16);","3cb98e2c":"fig = plt.figure(figsize=(26,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        order = list(train['cat'+str(n)].value_counts().index)\n        sns.countplot(data= train, x='cat'+str(n),ax=ax, alpha =0.8,order=order,palette='viridis')\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Train categorical features unique values count', fontsize=16,y=.93);","7647f721":"fig = plt.figure(figsize=(26,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        order = list(test['cat'+str(n)].value_counts().index)\n        sns.countplot(data= test, x='cat'+str(n),ax=ax, alpha =0.8,order=order,palette='viridis')\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Test categorical features unique values count', fontsize=16,y=.93);","5ac9962f":"fig = plt.figure(figsize=(26,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        sns.barplot(data= train, y = 'target', x='cat'+str(n),ax=ax, alpha =.6,ci=95, color= 'darkblue',dodge=False )\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Distribution of categorical features unique values and target', fontsize=16,y=.93);","9092b81f":"fig = plt.figure(figsize=(26,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        sns.kdeplot(data = train, x = 'target', hue = 'cat'+str(n),ax=ax, alpha =.7, fill=False)\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('KDE plot of train target with categorical features', fontsize=16,y=.93);","c129bca3":"fig = plt.figure(figsize=(30,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        sns.violinplot(data = train, y = 'target', x = 'cat'+str(n),ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Violin plot of target with categorical features', fontsize=16,y=.93);","21c3bdbb":"v0 = sns.color_palette(palette='viridis').as_hex()[0]\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=train[num_col], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of train numerical columns', fontsize=16);","993d17cd":"fig = plt.figure(figsize=(18,6))\nsns.boxplot(data=test[num_col], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of test numerical columns', fontsize=16);","c3c3feea":"fig = plt.figure(figsize=(28,10))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(2, 7, figure= fig, hspace= .3, wspace= .2)\nn =0\nfor i in range(2):\n    for j in range(7):\n        ax = fig.add_subplot(grid[i, j])\n        sns.histplot(data= train, x='cont'+str(n),ax=ax, alpha =.6, color= 'darkblue',kde=True)\n        ax.set_title('cont'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\n        \nfig.suptitle('Histograms of train numerical features', fontsize=16,y=.93);","35d78e7f":"fig = plt.figure(figsize=(28,10))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(2, 7, figure= fig, hspace= .3, wspace= .2)\nn =0\nfor i in range(2):\n    for j in range(7):\n        ax = fig.add_subplot(grid[i, j])\n        sns.histplot(data= test, x='cont'+str(n),ax=ax, alpha =.6, color= 'darkblue',kde=True)\n        ax.set_title('cont'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\n        \nfig.suptitle('Histograms of test numerical features', fontsize=16,y=.93);","36bc88d5":"fig = plt.figure(figsize=(24,8))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(2, 7, figure= fig, hspace= .3, wspace= .2)\nn =0\nfor i in range(2):\n    for j in range(7):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data= train, x='cont'+str(n), y='target',ax=ax, alpha =.4, color= 'darkblue' )\n        ax.set_title('cont{}\\n corr with target = {}%'.format(str(n),round(100*(train['cont'+str(n)].corr(train.target)),3)),fontsize=12)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\n        \nfig.suptitle('Scatter plot of numerical features with target', fontsize=18,y=.98)\nfig.text(0.11,0.5, \"Target\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","533f7da3":"train.corr()['target'][:-1].plot.barh(figsize=(8,6),alpha=.6,color='darkblue')\nplt.xlim(-.075,.075);\nplt.xticks([-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065],\n           [str(100*i)+'%' for i in [-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065]],fontsize=12)\nplt.title('Correlation between target and numerical variables',fontsize=14);","6146f60c":"train.corr().style.background_gradient(cmap='viridis')","c980d7a0":"sns.pairplot(train[num_col], corner=True, diag_kind='kde');","cf96c210":"y = train['target']","5214cb52":"(y.index == features.index).all()","68d53dbd":"plt.figure(figsize=(12,6))\nsns.histplot(y, bins = 150)\nplt.title('Histogram of target data', fontsize= 20)\nplt.ylabel('');","8dfa08f0":"plt.figure(figsize=(8,6))\nsns.boxplot(y= y, width=.2)\nplt.title('Box plot of target data', fontsize= 20)\nplt.ylabel('');","aeb2096f":"scaler = StandardScaler()\ny_scaled = pd.DataFrame(scaler.fit_transform(pd.DataFrame(y)))\ny_scaled.columns = [y.name]\ny_scaled.index = y.index","035b8d4c":"fig = plt.figure(figsize=(28,11))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(2, 7, figure= fig, hspace= .3, wspace= .2)\nn =0\nfor i in range(2):\n    for j in range(7):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data= train, y='cont'+str(n), x=y_scaled.target,ax=ax, alpha =.5, color= 'darkblue' )\n        ax.set_title('cont{}\\n corr with target = {}% '.format(str(n),round(100*(train['cont'+str(n)].corr(train.target)),3)),fontsize=12)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\n        \nfig.suptitle('Scatter plot of numerical features with target', fontsize=18,y=.94)\nfig.text(0.11,0.5, \"Target\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","40783c7f":"y.describe().iloc[1:].plot.barh(color=v0,alpha=.5,figsize=(12,5))\nplt.title('Target data statistics',fontsize=16)\nplt.yticks(fontsize=14)\nplt.xticks(np.arange(0,10.8,.5));","0623d88b":"np.percentile(y,(25,75))","8290cb1f":"plt.figure(figsize=(12,6))\nsns.boxplot(x=y, width=.4);\nplt.axvline(np.percentile(y,.1), label='.1%', c='orange', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,.5), label='.5%', c='darkblue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,1), label='1%', c='green', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99), label='99%', c='gold', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99.9), label='99.9%', c='red', linestyle=':', linewidth=3)\nplt.legend()\nplt.title('Box plot of target data', fontsize=16)\nplt.xticks(np.arange(0,10.8,.5));","d6479ed3":"plt.figure(figsize=(12,6))\nsns.violinplot(x=y, width=.4);\nplt.axvline(np.percentile(y,.1), label='.1%', c='orange', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,.5), label='.5%', c='darkblue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,1), label='1%', c='green', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99), label='99%', c='gold', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99.9), label='99.9%', c='red', linestyle=':', linewidth=3)\nplt.legend()\nplt.title('Violin plot of target data', fontsize=16)\nplt.xticks(np.arange(0,10.8,.5));","88b26afc":"clus = KMeans(n_clusters=8, max_iter=800)\nkmean_no_pca= clus.fit_predict(train[num_col])\n\ntrain_clus = train.copy()\ntrain_clus['cluster'] = kmean_no_pca\ntrain_clus['cluster'] = train_clus['cluster'].astype('object')\n\ntest_clus = test.copy()\ntest_clus['cluster'] = clus.predict(test[num_col]).astype('object')","a4adf1e9":"plt.figure(figsize=(19,6))\nsns.violinplot(data=train_clus, y='target', x='cluster',palette='viridis');\nplt.axhline(np.percentile(y,.1), label='.1%', c='red', linestyle=':', linewidth=3)\nplt.axhline(np.percentile(y,.5), label='.5%', c='orange', linestyle=':', linewidth=3)\nplt.axhline(np.percentile(y,1), label='1%', c='green', linestyle=':', linewidth=3)\nplt.axhline(np.percentile(y,99), label='99%', c='gold', linestyle=':', linewidth=3)\nplt.axhline(np.percentile(y,99.9), label='99.9%', c='darkblue', linestyle=':', linewidth=3)\nplt.legend(ncol=3,loc= (.55,.01))\nplt.title('Violin plot of target data \"with clusters\"', fontsize=18)\nplt.yticks(np.arange(0,10.8,.5));","50ba7049":"plt.figure(figsize=(12,6))\nsns.kdeplot(data=train_clus, x='target', hue='cluster',palette='viridis',alpha=.3)\nplt.title('KDE plot of target data \"with clusters\"', fontsize=18);","3b17e38b":"fig = plt.figure(figsize=(32,24))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(5, 2, figure= fig, hspace= .2, wspace= .05)\nn =0\nfor i in range(5):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.pointplot(data=train_clus, y='target',x= 'cluster',hue='cat'+str(n), ax=ax, palette='viridis', ci='sd', capsize=.05,join=True,dodge=.6 )\n        ax.set_title('cat{}'.format(str(n)),fontsize=16)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.set_ylim(6.8,10.1)\n        ax.legend(loc='upper left',ncol=20)\n        n += 1\n        \nfig.suptitle('Pointplot of Clusters, Target and Categorical features', fontsize=20,y=.90)\nfig.text(0.11,0.5, \"Target\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","39f06e8d":"fig = plt.figure(figsize=(12,32))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(7, 2, figure= fig, hspace= .2, wspace= .05)\nn =0\nfor i in range(7):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data=train_clus, y='target', x='cont'+str(n), hue= 'cluster', ax=ax, palette='viridis', alpha=.6 )\n        ax.set_title('cont{}'.format(str(n)),fontsize=16)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.legend(loc='lower left',ncol=20)\n        n += 1\n        \nfig.suptitle('Scatter plot of Target, Numerical and Cluster features', fontsize=20,y=.90)\nfig.text(0.08,0.5, \"Target\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","51ce4e16":"pca = PCA(n_components=6)\ncolumns = ['pca_%i' % i for i in range(1,7)]\n\ndf_pca = pd.DataFrame(pca.fit_transform(train[num_col]), columns=columns, index=train.index)\ntest_pca = pd.DataFrame(pca.transform(test[num_col]), columns=columns, index=test.index)\ndf_pca = train.drop(columns=num_col).merge(df_pca, left_index=True, right_index=True)\ntest_pca = test.drop(columns=num_col).merge(test_pca, left_index=True, right_index=True)\n\nclus_pca = KMeans(n_clusters=3, max_iter=1000)\nkmean_pca= clus_pca.fit_predict(df_pca[columns])\nkmeans_test_pca = clus_pca.predict(test_pca[columns])\n\ndf_pca_clus = df_pca.copy()\ntest_pca_clus = test_pca.copy()\ntest_pca_clus['cluster'] = kmeans_test_pca\ntest_pca_clus['cluster'] = test_pca_clus['cluster'].astype('object') \ndf_pca_clus['cluster'] = kmean_pca\ndf_pca_clus['cluster'] = df_pca_clus['cluster'].astype('object')","d9ea9389":"df_pca.corr().style.background_gradient(cmap='viridis')","757d1f28":"df_pca[columns].hist(layout=(3,2),bins=80, figsize=(16,10), alpha=.6);","4c63cc20":"fig = plt.figure(figsize=(12,16))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(3, 2, figure= fig, hspace= .2, wspace= .05)\nn =1\nfor i in range(3):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data=df_pca_clus, y='target', x='pca_'+str(n), hue= 'cluster', ax=ax, palette='viridis', alpha=.6 )\n        ax.set_title('pca_{}'.format(str(n)),fontsize=16)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.legend(loc='lower left',ncol=20)\n        n += 1\n\nfig.suptitle('Scatter plot of Target, principal components and Cluster features', fontsize=20,y=.92)\nfig.text(0.08,0.5, \"Target\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","4c3aac4c":"loadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=columns,  # so the columns are the principal components\n    index=num_col,  # and the rows are the original features\n)\nloadings.style.background_gradient(cmap='viridis')","05b4219f":"# mi_scores_pca = mutual_info_regression(df_pca[columns], y, random_state=0)\n# gc.collect()\n# mi_scores_pca = pd.Series(mi_scores_pca, name=\"MI_Scores_pca\", index=df_pca[columns].columns)\n# gc.collect()\n# mi_scores_pca = mi_scores_pca.sort_values(ascending=False)\n# mi_scores_pca.to_dict()\npca_dict = {'pca_1': 0.011851056820256112,'pca_2': 0.0030647695643661876,'pca_5': 0.0023570309909342058,\n 'pca_3': 0.0011543896910328755,'pca_4': 0.0007548110478801107,'pca_6': 0.0}\nmi_scores_pca = pd.DataFrame(pca_dict.values(), index=pca_dict.keys(),columns=['MI_Scores_pca'] )\nmi_scores_pca.style.background_gradient(cmap='viridis')","ff005cdc":"print('train target .1%: ', np.percentile(train.target,.1))\nprint('TRUNCATING',end='')\nprint('.' ,end ='')\ntime.sleep(.5)\n      \nmin_01= np.percentile(train.target,.1)\ntrain_01_trunc = train[train.target > min_01]\nprint('.' ,end ='')\ntime.sleep(.5)\n\nmin_05 = np.percentile(train.target,.5)\ntrain_05_trunc = train[train.target > min_05]\nprint('.' ,end ='')\ntime.sleep(.5)\n\nmin_1 = np.percentile(train.target,1)\ntrain_1_trunc = train[train.target > min_1]\nprint('.' ,end ='')\ntime.sleep(.5)\n\n\nmax_99 = np.percentile(train.target,99)\ntrain_1_99_trunc = train[(train.target > min_1) & (train.target < max_99)]\nprint('\\nDone!' ,end ='')","4fc8481d":"features = train.drop(['target'], axis=1)\nfeatures_clus = train_clus.drop(['target'], axis=1)\n\ny_01_trunc = train_01_trunc['target']\nfeatures_01_trunc = train_01_trunc.drop(['target'], axis=1)\n\ny_05_trunc = train_05_trunc['target']\nfeatures_05_trunc = train_05_trunc.drop(['target'], axis=1)\n\ny_1_trunc = train_1_trunc['target']\nfeatures_1_trunc = train_1_trunc.drop(['target'], axis=1)\n\ny_1_99_trunc = train_1_99_trunc['target']\nfeatures_1_99_trunc = train_1_99_trunc.drop(['target'], axis=1)","4fcefe19":"fig, (ax1,ax2,ax3,ax4)= plt.subplots(1,4,figsize=(22,5))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=.85, wspace=None, hspace=.3)\n\nsns.histplot(y_01_trunc, kde=True, color= v0, alpha=.6,bins=200,ax=ax1)\nax1.set_ylabel('')\nax1.set_xlabel('')\nax1.set_title('.1 %',fontsize=16)\n# ax1.set_xticks(np.arange(5.5,11,.5));\n\nsns.histplot(y_05_trunc, kde=True, color= v0, alpha=.6,bins=200,ax=ax2)\nax2.set_ylabel('')\nax2.set_xlabel('')\nax2.set_title('.5 %',fontsize=16)\n# ax2.set_xticks(np.arange(5.5,11,.5));\n\nsns.histplot(y_1_trunc, kde=True, color= v0, alpha=.6,bins=200,ax=ax3)\nax3.set_ylabel('')\nax3.set_xlabel('')\nax3.set_title('1 %',fontsize=16)\n# ax3.set_xticks(np.arange(5.5,11,.5));\n\nsns.histplot(y_1_99_trunc, kde=True, color= v0, alpha=.6,bins=200,ax=ax4)\nax4.set_ylabel('')\nax4.set_xlabel('')\nax4.set_title('1:99 %',fontsize=16)\n# ax4.set_xticks(np.arange(5.5,11,.5))\n\nfig.suptitle('Histplot of target data after removing outliers', fontsize= 20);","25213897":"fig, (ax1, ax2, ax3, ax4)= plt.subplots(1,4,figsize=(18,6))\n\nsns.boxplot(y=y_01_trunc, color= v0,ax=ax1, width=.3)\nax1.set_ylabel('')\nax1.set_xlabel('')\nax1.set_title('.1 %',fontsize=16)\nax1.set_yticks(np.arange(5.5,11,.5));\n\nsns.boxplot(y=y_05_trunc, color= v0,ax=ax2, width=.3)\nax2.set_ylabel('')\nax2.set_xlabel('')\nax2.set_title('.5 %',fontsize=16)\nax2.set_yticks(np.arange(5.5,11,.5));\n\nsns.boxplot(y=y_1_trunc, color= v0,ax=ax3, width=.3)\nax3.set_ylabel('')\nax3.set_xlabel('')\nax3.set_title('1 %',fontsize=16)\nax3.set_yticks(np.arange(5.5,11,.5));\n\nsns.boxplot(y= y_1_99_trunc, color= v0,ax=ax4, width=.3)\nax4.set_ylabel('')\nax4.set_xlabel('')\nax4.set_title('1:99 %',fontsize=16)\nax4.set_yticks(np.arange(5.5,11,.5));\n\n\nfig.suptitle('Boxplot of target data after removing outliers', fontsize= 20);","7df6c8e1":"features_int = train_clus.drop(['target'], axis=1)\nfeatures_int['c6_int'] = features_int.cat6 +'_'+ features_int.cluster.astype(str)\nfeatures_int['c7_int'] = features_int.cat7 +'_'+ features_int.cluster.astype(str)\n\ntest_int = test_clus.copy()\ntest_int['c6_int'] = test_int.cat6 +'_'+ test_int.cluster.astype(str)\ntest_int['c7_int'] = test_int.cat7 +'_'+ test_int.cluster.astype(str)","ebce305e":"train_dummies = pd.get_dummies(features)\ntrain_clus_dummies = pd.get_dummies(features_clus)\ntest_dummies = pd.get_dummies(test)\ntest_clus_dummies = pd.get_dummies(test_clus)\n\ndf_pca_dummies = pd.get_dummies(df_pca)\ndf_pca_clus_dummies = pd.get_dummies(df_pca_clus)\n\ntrain_dummies_int = pd.get_dummies(features_int)\ntest_dummies_int = pd.get_dummies(test_int)\n\ntrain_trunc_dummies_01 = pd.get_dummies(features_01_trunc)\ntrain_trunc_dummies_05 = pd.get_dummies(features_05_trunc)\ntrain_trunc_dummies_1 = pd.get_dummies(features_1_trunc)\ntrain_trunc_dummies_1_99 = pd.get_dummies(features_1_99_trunc)","a7039ab4":"for i in (set(train_dummies_int.columns) - set(test_dummies_int.columns)):\n        test_dummies_int[i] = 0","2e4fbbc0":"test_dummies_int = test_dummies_int[train_dummies_int.columns]","95867419":"df_pca_dummies.drop(columns='target',inplace=True)\ndf_pca_clus_dummies.drop(columns='target',inplace=True)","95452a35":"list(train_dummies_int.columns) == list(test_dummies_int.columns)","a6a5f21a":"all(train_trunc_dummies_1_99.columns == train_dummies.columns) == all(test_dummies.columns == train_dummies.columns)","02a596a8":"## This is Sklearn implmentation of one-hot encoding\n\n# OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n# OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(features[cat_cols]))\n# OH_cols_test = pd.DataFrame(OH_encoder.transform(test[cat_cols]))\n\n# OH_cols_train.index = features.index\n# OH_cols_test.index = test.index\n\n# num_X_train = features.drop(cat_cols, axis=1)\n# num_X_test = test.drop(cat_cols, axis=1)\n\n# OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n# OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)","323c2889":"OR_encoder = OrdinalEncoder()\n\nOR_cols_train = pd.DataFrame(OR_encoder.fit_transform(features[cat_cols]))\nOR_cols_test = pd.DataFrame(OR_encoder.transform(test[cat_cols]))\nOR_cols_train_trunc_01 = pd.DataFrame(OR_encoder.transform(features_01_trunc[cat_cols]))\nOR_cols_train_trunc_05 = pd.DataFrame(OR_encoder.transform(features_05_trunc[cat_cols]))\nOR_cols_train_trunc_1 = pd.DataFrame(OR_encoder.transform(features_1_trunc[cat_cols]))\nOR_cols_train_trunc_1_99 = pd.DataFrame(OR_encoder.transform(features_1_99_trunc[cat_cols]))\nOR_cols_pca = pd.DataFrame(OR_encoder.transform(df_pca[cat_cols]))\n\n\nOR_cols_train.index = features.index\nOR_cols_test.index = test.index\nOR_cols_train_trunc_01.index = features_01_trunc.index\nOR_cols_train_trunc_05.index = features_05_trunc.index\nOR_cols_train_trunc_1.index = features_1_trunc.index\nOR_cols_train_trunc_1_99.index = features_1_99_trunc.index\nOR_cols_pca.index = df_pca.index\n\nOR_cols_train.columns = cat_cols\nOR_cols_test.columns = cat_cols\nOR_cols_train_trunc_01.columns = cat_cols\nOR_cols_train_trunc_05.columns = cat_cols\nOR_cols_train_trunc_1.columns = cat_cols\nOR_cols_train_trunc_1_99.columns = cat_cols\nOR_cols_pca.columns = cat_cols\n\nnum_X_train = features.drop(cat_cols, axis=1)\nnum_X_test = test.drop(cat_cols, axis=1)\nnum_X_train_trunc_01 = features_01_trunc.drop(cat_cols, axis=1)\nnum_X_train_trunc_05 = features_05_trunc.drop(cat_cols, axis=1)\nnum_X_train_trunc_1 = features_1_trunc.drop(cat_cols, axis=1)\nnum_X_train_trunc_1_99 = features_1_99_trunc.drop(cat_cols, axis=1)\nnum_X_pca = df_pca.drop(cat_cols, axis=1)\n\nOR_X_train = pd.concat([num_X_train, OR_cols_train], axis=1)\nOR_X_test = pd.concat([num_X_test, OR_cols_test], axis=1)\nOR_X_train_trunc_01 = pd.concat([num_X_train_trunc_01, OR_cols_train_trunc_01], axis=1)\nOR_X_train_trunc_05 = pd.concat([num_X_train_trunc_05, OR_cols_train_trunc_05], axis=1)\nOR_X_train_trunc_1 = pd.concat([num_X_train_trunc_1, OR_cols_train_trunc_1], axis=1)\nOR_X_train_trunc_1_99 = pd.concat([num_X_train_trunc_1_99, OR_cols_train_trunc_1_99], axis=1)\nOR_X_pca =  pd.concat([num_X_pca, OR_cols_pca], axis=1)","dd90e26a":"OR_X_train_clus = OR_X_train.copy()\nOR_X_train_clus['cluster'] = train_clus['cluster'].astype(int)\n\nOR_X_test_clus = OR_X_test.copy()\nOR_X_test_clus['cluster'] = test_clus['cluster'].astype(int)\n\nOR_X_train_clus_trunc_01 = OR_X_train_trunc_01.copy()\nOR_X_train_clus_trunc_01['cluster'] = train_clus[train.target > min_01]['cluster'].astype(int)\n\nOR_X_train_clus_trunc_05 = OR_X_train_trunc_05.copy()\nOR_X_train_clus_trunc_05['cluster'] = train_clus[train.target > min_05]['cluster'].astype(int)\n\nOR_X_train_clus_trunc_1 = OR_X_train_trunc_1.copy()\nOR_X_train_clus_trunc_1['cluster'] = train_clus[train.target > min_1]['cluster'].astype(int)\n\n\nOR_X_train_clus_trunc_1_99 = OR_X_train_trunc_1_99.copy()\nOR_X_train_clus_trunc_1_99['cluster'] = train_clus[(train.target > min_1) & (train.target < max_99)]['cluster'].astype(int)\n\nOR_X_pca_clus = OR_X_pca.copy()\nOR_X_pca_clus['cluster'] = df_pca_clus['cluster'].astype(int)","7feebfc8":"OR_X_train_int = OR_X_train_clus.copy()\nOR_X_train_int['c_c6'] = OR_X_train_int.cluster * OR_X_train_int.cat6\nOR_X_train_int['c_c7'] = OR_X_train_int.cluster * OR_X_train_int.cat7\n\nOR_X_test_int = OR_X_test_clus.copy()\nOR_X_test_int['c_c6'] = OR_X_test_int.cluster * OR_X_test_int.cat6\nOR_X_test_int['c_c7'] = OR_X_test_int.cluster * OR_X_test_int.cat7","d09ab3aa":"# mi_scores_dummies = mutual_info_regression(train_dummies, y, random_state=0)\n# mi_scores_dummies = pd.Series(mi_scores_dummies, name=\"MI_Scores_dummies\", index=train_dummies.columns)\n# mi_scores_dummies = mi_scores_dummies.sort_values(ascending=False)\n# mi_scores_dummies","3c6313fd":"\n# mi_scores_ordinal = mutual_info_regression(OR_X_train, y, random_state=0)\n# mi_scores_ordinal = pd.Series(mi_scores_ordinal, name=\"MI_Scores_Ordinal\", index=OR_X_train.columns)\n# mi_scores_ordinal = mi_scores_ordinal.sort_values(ascending=False)\n# mi_scores_ordinal","f400b62d":"# fig ,(ax1,ax2) = plt.subplots(2,1,figsize=(28,10))\n# plt.subplots_adjust(left=None, bottom=None, right=None, top=.90, wspace=None, hspace=.4)\n# sns.barplot(x = mi_scores_dummies.index, y = mi_scores_dummies.values,palette='viridis', ax=ax1)\n# sns.barplot(x = mi_scores_ordinal.index, y = mi_scores_ordinal.values,palette='viridis', ax= ax2)\n# ax1.tick_params(axis='x',labelrotation=45,labelsize=12)\n# ax2.tick_params(axis='x',labelrotation=45,labelsize=12)\n# ax1.set_title('Mutual information with target \"One hot encoding\"',fontsize=18)\n# ax2.set_title('Mutual information with target \"Ordinal encoding\"',fontsize=18)\n# fig.suptitle('Mutual information',fontsize=22);","3b732f25":"# print('features with MI greater than 0 \"one hot encoding\":',colored(str(round(100*(mi_scores_dummies > 0).mean(),4)) + '%','green'))","c0f596b0":"# print('features with MI greater than 0 \"ordinal ordinal\":',colored(str(round(100*(mi_scores_ordinal > 0).mean(),4)) + '%','green'))","3fe4dd73":"# from sklearn.metrics import SCORERS\n# SCORERS.keys()","23b60aaf":"# xgbcpars = {'booster': ['gbtree'],\n#             'colsample_bytree': [0.7],\n#             'eval_metric': ['rmse'],\n#             'gamma': [1],\n#             'learning_rate': [0.08],#0.07853392035787837],\n#             'max_depth': [3],\n#             'n_estimators': [4000],\n#             'objective': ['reg:squarederror'],\n#             'random_state': [0],\n#             'reg_alpha': [1.5],\n#             'reg_lambda': [1.7549293092194938e-05],\n#             'subsample': [0.9],\n#             'colsample_bytree': [0.170759104940733]}","4f294d31":"# reg = xgb.XGBRegressor(tree_method='gpu_hist')","a79fd9ba":"# grid = Grid(estimator=reg, param_grid= xgbcpars,scoring= 'neg_root_mean_squared_error',\n#          verbose= 100, cv= 5 , return_train_score= False)\n# grid","cd3a86f0":"OR_X_train_pca1 = OR_X_train.copy()\nOR_X_train_pca1['pca'] = df_pca.pca_1\nOR_X_train_pca1\nOR_X_test_pca1 = OR_X_test.copy()\nOR_X_test_pca1['pca'] = test_pca.pca_1","8f61eb66":"# grid.fit(OR_X_train_trunc_01, y_01_trunc)","82c3e67a":"# grid.best_estimator_","21560a93":"# grid.best_params_","4b4371dd":"# grid.best_score_","5b51567f":"# OR_X_train               = -0.718640474135784\n# OR_X_train_pca1          = -0.7187306399092321\n# OR_X_train_clus_trunc_01 = -0.7097323223769159\n# OR_X_train_trunc_01      =  0.709725258182609\n# OR_X_train_int           = -0.7187115650146971\n# OR_X_train_clus          = -0.7187287224544328\n# train_dummies            = -0.71875469941565\n# train_dummies_int        = -0.7188511341200838\n# train_clus_dummies       = -0.7186358775342236","021c906a":"# pd.Series(grid.best_estimator_.feature_importances_, index= OR_X_test_int.columns)","231e3cea":"# def get_score(L, grid):\n#     \"\"\"\n#     this function will fit a GridSearchCV with XGBRegressor\n#     input\n#     L: a list of tuples containing train and target couples with 0 index for train data and 1 for target\n#     grid: parameters that will be used in the search\n#     return\n#     a dictionary with train data names as keys and list of their scores and best parameters as values\n#     \"\"\"\n#     n = 1\n#     dic = {}\n#     for data in L:              \n#         print(colored('Now Trainin Dataset No: {}'.format(n),'green'))\n#         reg = xgb.XGBRegressor(tree_method='gpu_hist', nthread= -1,verbosity =0)\n#         grid = Grid(estimator=reg, param_grid= xgbcpars,scoring= 'neg_root_mean_squared_error',\n#                     verbose= 0, cv= 5 , return_train_score= False)\n#         grid.fit(data[0], data[1])\n#         dic[n] = [grid.best_score_, grid.best_params_]\n#         print(colored('\\nFone with Trainin Dataset No: {} '.format(n),'green'))\n#         print('==========================')\n#         n += 1\n#     return dic","3c403a5f":"# get_score([(OR_X_train,y), (OR_X_train_int, y)],xgbcpars)","92902841":"# df_xgb = pd.concat([pd.DataFrame(grid.cv_results_[\"params\"]),pd.DataFrame(grid.cv_results_[\"mean_test_score\"], columns=[\"score\"])],axis=1)","d7055aea":"# imp=pd.DataFrame(grid.best_estimator_.feature_importances_, index= OR_X_train.columns ,columns=('imp',))","22acc23b":"# imp.sort_values(by='imp',ascending=False,inplace=True)","fa82ac84":"# imp.imp.plot.bar(figsize=(22,5))\n# plt.xticks(rotation=0);","9318b2a1":"model = xgb.XGBRegressor(booster ='gbtree', colsample_bytree=0.170759104940733,\n                         eval_metric= 'rmse', gamma= .7,\n                         learning_rate= 0.07853392035787837, max_depth= 5,\n                         n_estimators= 80000, objective= 'reg:squarederror',\n                         random_state= 0, reg_alpha= 1.5,\n                         reg_lambda= 1.7549293092194938e-05, subsample= 0.9,\n                         tree_method='gpu_hist')","bdac4541":"model.fit(OR_X_train_pca1, y)","c8516902":"# preds = model.predict(X_valid)\n# print('msqr:', mean_squared_error(y_valid,preds),\n#       '\\nmabe:',1* mean_absolute_error(y_valid,preds),\n#       '\\nR2:', r2_score(y_valid,preds))","87fb7176":"# Use the model to generate predictions\npredictions = model.predict(OR_X_test_pca1)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': OR_X_test_pca1.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False);","0b3608bc":"#### Now we make an ordinal encoding to explore the model quality using it compared to one hot encoding","64aa6543":"<center><img src=\"https:\/\/c.tenor.com\/EGhxbE0xUvIAAAAC\/we-good-thumbs-up.gif\"><\/center>","3317ba31":"#### Exploring target data main statistics ","4212a9d9":"> #### Target seems to have lots of extremes outliers","decddd36":"#### Checking if test categorical unqiue values are all subsets of their train peers","cd07ebc7":"<h2><center><font color='red'>Upvote Or Else...<\/font><\/center><\/h2>\n<center><img src=\"https:\/\/i.imgur.com\/Sbpg1MS.gif\"><\/center>","9b3dcadb":"#### First we get a correlation grid of all numercial variables and target","33250c7b":"#### Checking if we lost any information in truncation","fa883210":"#### Standardizing target","91691b6d":"#### Fitting the model","db2e2da3":"#### Putting all in the grid to start test the different parameters.","37f47468":"#### Submit to the competition\n","87adc1b3":"<a id='load_data'><\/a>\n# Loading Data and exploring main statistics \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.","11abb722":"#### Now Exploring PCA","b461c541":"#### Now Exploring correlation between all numerical variables.","f27efeb9":"> #### It looks like on the surface there is no clear relation between target and any of categorical variables' values.","e37a6f04":"#### Box\/violin plot of target data with different percentiles limits darwn","4fa61ce9":"#### First we do Clustring \"using num_col only\"","564178c2":"> #### Seems like outliers need to be handled","d9e33e0e":"#### Adding feature the will that may help the model perform better","a87b116e":"> #### Most of columns seems to have few categorical unique values except cat9 column.","d3fb3636":"#### Distribution of categorical features unique values and target","be0bccdc":"#### Getting the correcorrelation between targets and principal components","f4c923c3":"> #### Well again as seen in the correlation grid there isn't strong relationship between variables, also these variables have kind of multinomial distributions.","18bc52f6":"# Welcome to the [30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)!\n\n<center><img src='https:\/\/storage.googleapis.com\/kaggle-media\/Images\/30_Days_ML_Hero.png' width='480' height=\"480\" ><\/center>\n\n#### This notebook will go through an extensive analysis to view and explore the `30 Days of ML` competition data, there will be some \"well, maybe a lot of\" redundancies but come on that's why \"extensive\" is in the name, also this notebook is a work in progress so there is a lot to be added later.\n<h4><font color='darkred'>Please note that this work you're about to see here is not memory efficient at all but this notebook was meant to be extensive to explore different approaches also the data is not that big<\/font><\/h4>\n\n\n### The notebook will have the following sections:\n #### - [Data Imports and initial exploring](#load_data).\n #### - [exploratory data analysis \"EDA\"](#EDA).\n #### - [Cleaning and feature Engineering starter](#ft_eng).\n #### - [XGBOOST starter](#xgb). \n","52e0162d":"> #### Minimum value seems to be way below the avrage.","75a902dc":"#### Scatter plot with Pearson coefficient of correlation of numerical features with target","3ea9ff9e":"> #### This plot is a continuation to the KDE plot and it pretty muh agrees with it but we can also notice target outliers.","a8d42f66":"### Target data\n","eb096609":"> #### Well Data seems to be pretty much preprocessed \"I mean it's synthetic after all\" so no nulls exists and also even if features are anonymous they are ordered in a proper easy to the eye way.","4e8a140d":"#### Making interaction columns for cat 6 and cat 7 with cluster","8ca89d14":"<center><img src='https:\/\/c.tenor.com\/WRwm-wTN0_8AAAAd\/i-just-got-lucky-willie.gif' width='380' height=\"380\"><\/center>","e0ffbd71":" #### separating the target (`y`) from the training features (which we assign to `features`).","3ec36418":"#### Zooming on the correlation between numerical variables and target.","7a6cff43":"> #### Seems like this data is not a walk in the park at all","8480adda":"#### Showing a grid scatter plots to investigate the numbers shown above","723d6ffa":"#### Submissions are scored on the root mean squared error 'RMSE' which is defined as: \n<h3>$${RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$<\/h3>\n\n#### Generally We can explore the quality of these approaches\n- ordinal encoding\n- one hot encoding","5eaa0d8b":"#### Violin plot of target with categorical features","3a2ae369":"> #### Mutual information seems to agree with pearson correlation as relationship with target seems to be very week.","425984a1":"#### Categorical features unique values count","e82e4c76":"#### Building final model","d5dfea16":"### Numerical Variables","bcdf5d18":"#### The following is a parameters grid to explore in grid search cross validation","00e8aea1":"<h2><center>Let's dive right in!<\/center><\/h2>\n<center><img src=\"https:\/\/64.media.tumblr.com\/801c1d244924a60dfae54af4924dda9e\/033f51f7de7447be-d9\/s1280x1920\/6343410ac221ab8bc34c005c19b2718d8f1e96a3.gifv\"><\/center>","d58691aa":"> #### Histograms of numerical data show a desperation of values with what look like multinomial distributions, also column cont1 seems to have some areas where the distribution becomes kinda discrete and again test numerical data seems to be similar to train numerical data.","6edb43b4":"<a id='EDA'><\/a>\n# EDA\n\n#### Now we start exploring the data set , mainly there are three main groups of variables to explore as follows:\n - Categorical columns.\n - Continuous \"numerical\" columns.\n - Target.","2493910c":"#### Categorical values handling:\n#### mainly there are two way to handle those \"aside from dropping them!)\n- One-hot encoding\n- Ordinal encoding","5d0f370c":"<center><img src='https:\/\/c.tenor.com\/ZFc20z8DItkAAAAd\/facepalm-really.gif'><\/center>","d8894512":"### Exploring Clustering and PCA\n#### Mainly there are two way to do this \n- First PCA and then clustering\n- Clustering and then doing PCA\n\n#### It also worth mentioning that there is two ways of using clusters as features\n- using cluster label (in short one columns with each number representing the cluster\" \n- using clusters labels ( adding k number of columns representing the distance to the centroid of each cluster\"","645cd836":"#### Getting the best parameters","75075fa2":"> #### This plot kinda agrees with previous one but it looks like the KDE of some categorical values are pretty much flat compared to other value.","17f64c3d":"#### we will explore both to find out the better way to handle categorical variables\n#### Using the pd.get_dummies to make one hot encoding to categorical data","75e84209":"### Categorical variables","3a790e1e":"#### Exploring the PCA loading\n##### [Taken from Feature Engineering course lesson 5 of 6](https:\/\/www.kaggle.com\/ryanholbrook\/principal-component-analysis)","77189250":"#### Getting the best best estimator","522baced":">#### Distribution of unique values in test and train data looks pretty much similar.","242974bc":"#### To easily distinguish them we will extract the name of the columns of different data types","48f0168f":"### Now we get the mutual information scores","10f151a3":"<a id='xgb'><\/a>\n\n# XGBOOST starter","04683b2d":"#### KDE plot of target with categorical features","9e86db44":"#### Extracting feature importances","fd010ff8":"#### Number of categorical unique values","04ddaf0a":"> #### Nothing impressive but xticks but again we notice outliers.","2ae6cdc2":"#### Now we try to explore if standardizing target will yield anything intersting","db3c5190":"#### Histograms of numerical features","dea8f564":"> #### Clustering seems like it wont add anything, but there is some intersting relation noticed with cat 6 and cat 7","f52113e9":"#### don't mind me,  just a sanity check ","89f6fdde":"<center><img src=\"https:\/\/www.memesmonkey.com\/images\/memesmonkey\/3f\/3f459b9e453447e0bedded09eba42df6.jpeg\"><\/center>","0c9e583f":"<a id='ft_eng'><\/a>\n# Cleaning and feature Engineering ","392c7e0d":"#### You might see the following lines of code commented so you can comment it out and explore the different approaches yourself.\n#### As a reminder we have the following sets of data to train and explore the quality of their approachs\n\n- train_dummies\n- train_dummies with different truncation \/ clustering\n- OR_X_train\n- OR_X_train with different truncation\/ clustering\n#### you can also try different combination of features using MI extracted earlier.","c36f6264":"<img src='https:\/\/c.tenor.com\/_f5-QsYtLt0AAAAC\/crazy-jim-carrey.gif'>","ef99c4ab":"> #### Numerical Data seems to be kinda normalized with few outliers appearing in the box plot Also test numerical data seems to looks like the train ones.","7f092af7":"#### Assuring that test data and whether or not it has the same columns as the train","50af4d57":"#### Exploring different data trunctation","21e9b296":"> #### It's clear tat there isn't any clear relation between numerical variables and target.","686ca7b8":"#### We will be using GridSearchCV with XGBRegressor to get closer to the best model parameters.","0bd328ab":"#### Getting the best score","ea8d4b21":"> #### Outliers still exist but we will need to explore model quality to find out whether or not it's possbile to drop more data, it's also worth mentioning that the histogram shows kind of steps-like behavior around some values.","4d3911d8":"<center><img src='https:\/\/c.tenor.com\/CZT9JqoJYW4AAAAC\/repetitive-redundant.gif'><\/center>","42231d9f":"#### Now we define a function called get_score to automate the exhaustive search","0cdbee5e":"#### Ratio of features with MI greater than 0","a0bd0679":"#### Now we make a XGBRegressor to build our model","75672092":"#### Box plot of numerical columns"}}