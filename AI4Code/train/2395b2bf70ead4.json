{"cell_type":{"c3cf14f9":"code","1b4b0054":"code","4463f806":"code","8f9f30f1":"code","07278813":"code","c28ab0db":"code","ab8a4a83":"code","ea8084bc":"code","23b67ec1":"code","a0bc7ff9":"code","2aebdf58":"code","0bc5cacf":"markdown","9218cf59":"markdown"},"source":{"c3cf14f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b4b0054":"import pandas as pd\n\n#Importing the dataset from kaggle directory \ndataset = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\n#targets\ndata = dataset.drop(\"label\", axis = 1)\n\n#features in pixel values\nlabels = dataset['label']","4463f806":"import numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n#transfering the dataset to a tensor dataset\ntensor_dataset = TensorDataset(torch.Tensor(np.array(data)), torch.from_numpy(np.array(labels)))\n\n#creating a data loader with batch size 64 and shuffling the dataset\ndata_loader = DataLoader(tensor_dataset, batch_size = 1, shuffle = True)","8f9f30f1":"import torch.nn as nn\nimport torch.nn.functional as F\n\n#creating the discriminator network\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 1)\n        \n    def forward(self, x):\n        pred = F.relu(self.fc1(x))\n        pred = F.relu(self.fc2(pred))\n        pred = F.relu(self.fc3(pred))\n        pred = torch.sigmoid(self.fc4(pred))\n        return pred\n    \n#creating the generator network\nclass Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(100, 256)\n        self.fc2 = nn.Linear(256, 384)\n        self.fc3 = nn.Linear(384, 512)\n        self.fc4 = nn.Linear(512, 784)\n        \n    def forward(self, x):\n        pred = F.relu(self.fc1(x))\n        pred = F.relu(self.fc2(pred))\n        pred = F.relu(self.fc3(pred))\n        pred = F.tanh(self.fc4(pred))\n        return pred\n        \n#creating the noise for the generator network\ndef noise():\n    arr = torch.randn(100)\n    return arr\n\n#creates targets for discriminator\ndef real_targets(size):\n    return torch.ones(size)\n\ndef fake_targets(size):\n    return torch.zeros(size)","07278813":"import torch.optim as optim\n\ntorch.autograd.set_detect_anomaly(True)\n\n#create generator and discriminator classes\ndiscriminator = Discriminator()\ngenerator = Generator()\n\n#create optimizers for both neworks\nd_optim = optim.Adam(discriminator.parameters(), lr = 0.0002)\ng_optim = optim.Adam(generator.parameters(), lr = 0.0002)\n\n#loss function is binary cross entropy\nloss_function = nn.BCELoss()\n\ngenerator_losses = []\ndiscriminator_losses = []\n\nnum_iterations = 0\n\nfor data, label in data_loader:\n    \n    ### Training The Discriminator ###\n    \n    #the discriminator output on the true images\n    real_output = discriminator(data)\n    \n    #the discriminator loss on predicting the true images\n    d_real_loss = loss_function(real_output, real_targets(1))\n    \n    #back propogating the discriminator loss on true images\n    d_real_loss.backward()\n\n    #the discriminator output on fake images\n    fake_output = discriminator(generator(noise()))\n    \n    #the discriminator loss on predicting fake images\n    d_fake_loss = loss_function(fake_output, fake_targets(1))\n    \n    #back propogating the disciminator loss on fake images\n    d_fake_loss.backward()\n    \n    d_optim.step()\n    \n    ### Training the Generator ###\n    \n    #the discriminator output on fake images\n    fake_output = discriminator(generator(noise()))\n    \n    #the generator loss is how good the discriminator was able to predict the images\n    generator_loss = loss_function(fake_output, real_targets(1))\n    \n    #back propgation on generator loss\n    generator_loss.backward()\n    g_optim.step()\n    \n    g_optim.zero_grad()\n    d_optim.zero_grad()\n               \n    #number of iterations increases\n    num_iterations+=1\n    \n    #prints the iteration if it is a multiple of 100\n    if num_iterations%100 == 0:\n        print(num_iterations)\n    \n    #Appends the loss to the loss lists                            \n    discriminator_losses.append(d_fake_loss+d_real_loss)\n    generator_losses.append(generator_loss)\n    \n    \n    \n    \n    ","c28ab0db":"from matplotlib import pyplot as plt\nnp.set_printoptions(formatter={'float_kind':'{:f}'.format})\nnp.array(discriminator(data).detach().numpy())\n","ab8a4a83":"plt.plot(range(len(generator_losses)), generator_losses)\nplt.plot(range(len(generator_losses)), generator_losses)\nplt.show()","ea8084bc":"nn.BCELoss(real_output, real)","23b67ec1":"real_output.reshape(-1,len(data), )","a0bc7ff9":"discriminator(generator(noise()))","2aebdf58":"g = Generator()\ng(noise())","0bc5cacf":"Importing the dataset and preprocessing to create labels and features","9218cf59":"Generative Adversial Network to Generate MNIST Digits with fully connected networks"}}