{"cell_type":{"dd1324fa":"code","231fb916":"code","95728c50":"code","29e684cb":"code","3a5521ed":"code","caa6d2a2":"code","49e10acb":"code","c9d728c4":"code","d2a4461a":"code","afa55c38":"code","5da6b152":"code","d4e4a3f4":"code","1aa4572e":"code","2df7460a":"code","9db49085":"code","b2050fd4":"code","88f2812a":"code","faf13fbc":"code","069ee5d2":"code","de906cb2":"code","226869eb":"code","8440616c":"code","ff0eb61f":"code","8cd4a480":"code","4643eb2f":"code","139c5695":"code","d88bbbab":"code","0e82f728":"code","f9c15d67":"code","84dd4609":"code","6a469a19":"code","84d73ca7":"code","1c4e209c":"code","d36c70d8":"code","ae9be32f":"code","f4155852":"code","5a220aa1":"code","85136301":"code","f217e748":"code","029e3904":"code","730a5499":"code","10bd3830":"code","e5275918":"code","22cae601":"code","e3cba7f7":"code","ee1ef07e":"code","f37818e7":"code","adbe581d":"code","a824e0d0":"code","739d1bff":"code","b495fb7b":"code","2689f6f2":"code","99964874":"code","540b4d77":"code","73f0bc1d":"code","7b76e9be":"code","e7694983":"code","21a167d8":"markdown","f54ecdc0":"markdown","a5fda71a":"markdown","c685570a":"markdown","450b2103":"markdown","a08016ea":"markdown","c1fa6e30":"markdown","d5b15b6b":"markdown","b6631b24":"markdown","937a662b":"markdown","8bd5dd72":"markdown","b3e6496c":"markdown","b739628e":"markdown","9ccb7911":"markdown","e7c72a85":"markdown","b8de25d9":"markdown"},"source":{"dd1324fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","231fb916":"# importing the training data\ntrain_dataset = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","95728c50":"# checking the information about the dataset including the data types and numerical counts\ntrain_dataset.info()","29e684cb":"# making the Id as index feature\ntrain_dataset = train_dataset.set_index('Id')","3a5521ed":"# checking for missing values\nmissing_dataframe = pd.concat([train_dataset.isnull().sum()], axis = 1)\nprint(missing_dataframe[missing_dataframe[0]>0])","caa6d2a2":"train_dataset = train_dataset.drop(['LotFrontage', 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1)","49e10acb":"# checking for missing values\nmissing_dataframe = pd.concat([train_dataset.isnull().sum()], axis = 1)\nprint(missing_dataframe[missing_dataframe[0]>0])","c9d728c4":"# now we can remove the missing samples from the dataset using dropna()\ntrain_dataset = train_dataset.dropna()","d2a4461a":"# checking for missing values\nmissing_dataframe = pd.concat([train_dataset.isnull().sum()], axis = 1)\nprint(missing_dataframe[missing_dataframe[0]>0])","afa55c38":"numeric_columns = train_dataset.describe().columns\nnonnumeric_columns = [col for col in train_dataset.columns if col not in train_dataset.describe().columns]","5da6b152":"print(\"numeric columns count: \", len(numeric_columns))\nprint(\"non numeric columns count: \", len(nonnumeric_columns))","d4e4a3f4":"#import label encoder\nfrom sklearn.preprocessing import LabelEncoder","1aa4572e":"def encoding(dataframe_feature):\n    if(dataframe_feature.dtype == 'object'):\n        return LabelEncoder().fit_transform(dataframe_feature)\n    else:\n        return dataframe_feature","2df7460a":"train_dataset = train_dataset.apply(encoding)","9db49085":"numeric_columns = train_dataset.describe().columns\nnonnumeric_columns = [col for col in train_dataset.columns if col not in train_dataset.describe().columns]\nprint(\"numeric columns count: \", len(numeric_columns))\nprint(\"non numeric columns count: \", len(nonnumeric_columns))","b2050fd4":"# importing seaborn library for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt","88f2812a":"train_dataset.head()","faf13fbc":"sns.heatmap(train_dataset.corr())","069ee5d2":"correlation_matrix = train_dataset.corr()\nessential_features = correlation_matrix.index[abs(correlation_matrix['SalePrice']) > 0.6]\nplt.figure(figsize = (8, 8))\nsns.heatmap(train_dataset[essential_features].corr(), cbar = False, annot = True, square = True)","de906cb2":"sns.pairplot(train_dataset[essential_features])","226869eb":"train_dataset = train_dataset[(train_dataset[\"SalePrice\"] < 500000) &\n              (train_dataset[\"GrLivArea\"] < 3000) &\n              (train_dataset[\"TotalBsmtSF\"] < 2300) &\n              (train_dataset[\"1stFlrSF\"] < 2200) & \n              (train_dataset[\"GarageArea\"] < 1200)]","8440616c":"train_dataset.shape","ff0eb61f":"sns.pairplot(train_dataset[essential_features])","8cd4a480":"# making the input and target features\nX = train_dataset.drop(['SalePrice'], axis = 1)\ny = train_dataset['SalePrice'].values","4643eb2f":"# Splitting the training and testing data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","139c5695":"# importing metrics for accuracy calculation\nfrom sklearn.metrics import mean_squared_error","d88bbbab":"# importing KNN regression\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor()\n# fitting the model with the training dataset\nknn.fit(X_train, y_train)\n# predicting the values\npredicted_value = knn.predict(X_test)\n# calculating the accuracy\nrmse_before_cleaning = np.sqrt(mean_squared_error(predicted_value, y_test))\nprint(rmse_before_cleaning)","0e82f728":"sns.distplot(train_dataset['SalePrice'], bins = 10)","f9c15d67":"from sklearn.model_selection import GridSearchCV\nparameters = {\n            'n_neighbors' : [1,2,3,4,5,6,7,8,9,10],\n            'algorithm' : ['ball_tree', 'brute']\n             }\ngrid_search_cv = GridSearchCV(KNeighborsRegressor(), parameters)\ngrid_search_cv.fit(X_train, y_train)","84dd4609":"grid_search_cv.best_params_","6a469a19":"knn = KNeighborsRegressor(n_neighbors = 4)\n# fitting the model with the training dataset\nknn.fit(X_train, y_train)\n# predicting the values\npredicted_value = knn.predict(X_test)\n# calculating the accuracy\nrmse_after_cleaning = np.sqrt(mean_squared_error(predicted_value, y_test))\nprint(rmse_after_cleaning)","84d73ca7":"print('RMSE:',rmse_after_cleaning)","1c4e209c":"# importing test data\ntest_dataset = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_dataset.head()","d36c70d8":"test_dataset = test_dataset.set_index(['Id'])","ae9be32f":"test_dataset.info()","f4155852":"test_dataset.isnull().sum()","5a220aa1":"test_dataset = test_dataset.drop(['LotFrontage', 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1)","85136301":"test_dataset = test_dataset.dropna()","f217e748":"# converting the test data into numerical and then normalizing the data.\ndef encoding(dataframe_feature):\n    if(dataframe_feature.dtype == 'object'):\n        return LabelEncoder().fit_transform(dataframe_feature)\n    else:\n        return dataframe_feature\ntest_dataset = test_dataset.apply(encoding)","029e3904":"print(train_dataset.shape)\nprint(test_dataset.shape)","730a5499":"SalePrice = knn.predict(test_dataset)","10bd3830":"submission_dataset = pd.DataFrame()\nsubmission_dataset['Id'] = test_dataset.index\nsubmission_dataset['SalePrice'] = SalePrice","e5275918":"submission_dataset.head()\n# submission_dataset.to_csv(\"KNN_Housing_Regression.csv\", index=False)","22cae601":"# importing training and testing datasets again.\ntrain_dataset = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_dataset1 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_dataset = test_dataset1.copy()","e3cba7f7":"train_dataset = train_dataset.set_index(['Id'])\ntest_dataset = test_dataset.set_index(['Id'])","ee1ef07e":"train_numerical_columns = train_dataset.describe().columns\ntrain_dataset = train_dataset[train_numerical_columns]\ntrain_dataset.head()","f37818e7":"test_numerical_columns = test_dataset.describe().columns\ntest_dataset = test_dataset[test_numerical_columns]\ntest_dataset.head()","adbe581d":"# importing the simple imputer missing values. \nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'mean')","a824e0d0":"imputed_data = imputer.fit_transform(train_dataset)\ntrain_dataset = pd.DataFrame(data = imputed_data, columns = train_dataset.columns)\ntrain_dataset.isnull().sum()","739d1bff":"imputed_data = imputer.fit_transform(test_dataset)\ntest_dataset = pd.DataFrame(data = imputed_data, columns = test_dataset.columns)\ntest_dataset.isnull().sum()","b495fb7b":"X = train_dataset.drop(['SalePrice'], axis = 1)\ny = train_dataset['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 22)","2689f6f2":"parameters = {\n            'n_neighbors' : [1,2,3,4,5,6,7,8,9,10],\n            'algorithm' : ['ball_tree', 'brute']\n             }\ngrid_search_cv = GridSearchCV(KNeighborsRegressor(), parameters)\ngrid_search_cv.fit(X_train, y_train)\ngrid_search_cv.best_params_","99964874":"knn = KNeighborsRegressor(n_neighbors = 6)\n# fitting the model with the training dataset\nknn.fit(X_train, y_train)\n# predicting the values\npredicted_value = knn.predict(X_test)\n# calculating the accuracy\nrmse = np.sqrt(mean_squared_error(predicted_value, y_test))\nprint(rmse)","540b4d77":"print(train_dataset.shape)\nprint(test_dataset.shape)","73f0bc1d":"SalePrice = knn.predict(test_dataset)","7b76e9be":"submission_dataset = pd.DataFrame({'Id' : test_dataset1['Id'], 'SalePrice': SalePrice})","e7694983":"submission_dataset.to_csv(\"KNN_Housing_Regression.csv\", index=False)","21a167d8":"Performances are not upto the expectations. Need to work more.","f54ecdc0":"Removed all the missing values from the dataset. Now we will check for Correlation.\nThe best way (from my experience) to check the correlation is to use the heatmap from seaborn library. \n\nThe features can be positively correlated, negatively correlated or 0 correlated. ","a5fda71a":"Since there are many non numerical columns there is a need to convert them into numerical using Label Encoder or One Hot encoder. \nOne hot encoder creates new features whereas label encoder uses the same features.","c685570a":"I tried submitting it but it requires 1459 predicted rows. We dropped missing values. Now we will try imputing values on numerical dataset only and see the results.","450b2103":"Inferences - We can see the sale price is positively skewed.","a08016ea":"From the information we can see that 5 features \n1. LotFrontage (Linear feet of street connected to property) has only 259 values\n1. Alley (Type of Aleey access) has only 91 values\n1. FireplaceQU (Quality of Fire Place) has only 770 values\n1. PoolQC (Quality of the Pool) has only 7 values\n1. Fence (Quality of the fence) has 281 values\n1. MiscFeature (Miscellaneous feature not covered in other categories) has 54 values\n\n*Interences*: The remaining are missing values. **We can remove the columns since they have drastic amount of missing values**.\n\nThe Feature ID is the identity column. so we can consider it as ID column and SalePrice is the Target Feature.","c1fa6e30":"Now the dataset has been cleaned and we got the best parameters for the dataset. ","d5b15b6b":"Now we will check for outliers. \n\nThe basic way to check for outliers is by visualizing using scatter plot.\nSince all the features are numerical now we can use seaborn pairplot ","b6631b24":"Using GridSearch CV to find the best parameters of KNN algorithm.","937a662b":"The features TotalBsmtSF, 1stFirSF, GrLivArea, GarageArea are having outliers. ","8bd5dd72":"Being on Learning curve, I tried using basic KNN Regressor algorithm. Suggestions and Ideas on improving the model are appreciated.","b3e6496c":"The heatmap is having many features which makes visualization very difficult. \n\n*Inferences*: Need to select the top correlated features.","b739628e":"Now all the features are converted into numerical features. Now we can apply regression algorithms and predict the sales price easily.","9ccb7911":"Now the outliers are removed from the dataset. \nThe dataset looks better cleaned now. We shall go into the modelling of data.","e7c72a85":"*Intefences*: The remaining columns with missing data can be retained. We can use dropna() option to omit the missing values. Later we can experiment with imputation of some missing values.","b8de25d9":"The sale price is positively highly correlated with \n1. QverallQual (overall material and finish quality)\n2. GrLivArea (Above ground living area square feet)\n3. GarageCars (Size of Garage in car capacity)\n4. GarageArea (Size of garage in square feet)\n\nThe sale price is negatively highly correlated with\n1. KitchQual (The Kitchen Quality)\n2. BsmtQual (The height of the basement)\n3. ExterQual (The quality of exterior material)\n\n*Inferences*: If the quality increases the price increases (Obviously). The price is directly depend on area, car parking which means most of the people around that area owns a car or the particular area is well accessible to roadways. \n\nBy analyzing the negatively correlated features, can we say people dont care about Kitchen, Basement and Exterior Material Quality ? (I dont know)"}}