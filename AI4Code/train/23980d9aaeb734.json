{"cell_type":{"a7d5f19d":"code","ac6d372b":"code","995948e8":"code","fe0f43ad":"code","0d509414":"code","ed895a5f":"code","2c88cdd5":"code","ee253e5e":"code","3cbfd0be":"code","abb4110e":"code","2ad28855":"code","c6a5c2e5":"code","68cd22a9":"code","64117332":"code","b5e12d55":"code","61bd9a1f":"code","a6689033":"code","4cdcaf71":"code","c65d836b":"code","6bea7689":"code","89a5bdbb":"code","4e3a9ac0":"code","1d270ca3":"code","ab39208e":"code","b1bc8621":"code","4f6f7438":"code","d2f7dd1f":"markdown","45dd1a89":"markdown","6fe49e14":"markdown","849820cf":"markdown","48dfc010":"markdown","29f54662":"markdown","1cdbdf23":"markdown","3fd9946d":"markdown","95a72930":"markdown","0f72e7fb":"markdown","c26f14fb":"markdown","7af695d5":"markdown","dc1fd9d0":"markdown","6c56277f":"markdown","065ac48c":"markdown","c5635b48":"markdown","4930bf70":"markdown","a5bc6208":"markdown","862eb6ec":"markdown","98d8fec3":"markdown"},"source":{"a7d5f19d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ac6d372b":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\",index_col='id')\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\",index_col='id')\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ndisplay(train,test,submission)","995948e8":"print(f\"Training set has a shape of {train.shape}\")\nprint(f\"Testing set has a shape of {test.shape}\")","fe0f43ad":"display(train.target.value_counts())\nsns.distplot(train.target,kde=False)","0d509414":"[(col,train[col].isnull().sum()) for col in train if train[col].isnull().any()]","ed895a5f":"train.keyword.value_counts()","2c88cdd5":"train.location.value_counts()","ee253e5e":"sns.distplot(train.text.str.len(),kde=False)","3cbfd0be":"sns.distplot(train.text[train.target == 1].str.len(),kde=False,color='g')\nplt.title('Character Count for Real Target Tweet')\nplt.figure()\nplt.title('Character Count for Fake Target Tweet')\nsns.distplot(train.text[train.target == 0].str.len(),kde=False,color='m')","abb4110e":"sns.distplot(train.text[train.target == 1].str.split().map(lambda x:len(x)),kde=False,color='g')\nplt.title('Word Count for Real Target Tweet')\nplt.figure()\nplt.title('Word Count for Fake Target Tweet')\nsns.distplot(train.text[train.target == 0].str.split().map(lambda x:len(x)),kde=False,color='m')","2ad28855":"sns.distplot(train.text[train.target == 1].str.split().map(lambda x:np.mean([len(y) for y in x])),kde=False,color='g')\nplt.title('Average Word Length for Real Target Tweet')\nplt.figure()\nplt.title('Average Word Length for Fake Target Tweet')\nsns.distplot(train.text[train.target == 0].str.split().map(lambda x:np.mean([len(y) for y in x])),kde=False,color='m')","c6a5c2e5":"from collections import Counter\ntotal_real,total_fake = Counter(),Counter()\n\nfor row in pd.Series(train.text[train.target == 1].str.split()):\n    total_real += Counter(row)\n\nfor row in pd.Series(train.text[train.target == 0].str.split()):\n    total_fake += Counter(row)","68cd22a9":"x,y = zip(*total_real.most_common(10))\nsns.barplot(list(x),list(y))\nplt.title(\"Common Stopwords for Real Tweets\")\ntotal_real.most_common(10)","64117332":"i,j = zip(*total_fake.most_common(10))\nsns.barplot(list(i),list(j))\nplt.title(\"Common Stopwords for Fake Tweets\")\ntotal_fake.most_common(10)","b5e12d55":"import string\npunctuation_collect_real = sorted([k for k in total_real.items() if k[0] in string.punctuation],key=lambda x:x[1],reverse=True)\ni,j = zip(*punctuation_collect_real)\nplt.title(\"Common Punctuations for Real Tweets\")\nsns.barplot(list(i),list(j))\npunctuation_collect_real","61bd9a1f":"punctuation_collect_fake = sorted([k for k in total_fake.items() if k[0] in string.punctuation],key=lambda x:x[1],reverse=True)\ni,j = zip(*punctuation_collect_fake)\nplt.title(\"Common Punctuations for Fake Tweets\")\nsns.barplot(list(i),list(j))\npunctuation_collect_fake","a6689033":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nimport tokenization","4cdcaf71":"def bert_encode(texts,tokenizer,max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens),np.array(all_masks),np.array(all_segments)","c65d836b":"def build_model(bert_layer,max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","6bea7689":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","89a5bdbb":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file,do_lower_case)","4e3a9ac0":"train_input = bert_encode(train.text.values,tokenizer,max_len=160)\ntest_input = bert_encode(test.text.values,tokenizer,max_len=160)\ntrain_labels = train.target.values","1d270ca3":"model = build_model(bert_layer,max_len=160)\nmodel.summary()","ab39208e":"train_log = model.fit(train_input,train_labels,validation_split=0.25,epochs=3,batch_size=16)\n\nmodel.save('model.h5')","b1bc8621":"preds = model.predict(test_input)","4f6f7438":"submission['target'] = preds.round().astype(int)\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head()","d2f7dd1f":"## What are common stopwords and their frequencies?","45dd1a89":"## What are shapes of those datasets?","6fe49e14":"## How many words in each tweet?","849820cf":"**- `15 - 20` words are the most common for both real and fake tweets**","48dfc010":"# Import libraries\nmain libraries including pandas,numpy,matplotlib,seaborn","29f54662":"## What is the average word length?","1cdbdf23":"## What keywords have highest frequency? and Locations?","3fd9946d":"**- `fatalities` is the most common keyword**  \n**- `USA` is the most common location but overlapping with `United States`, also `New York` is a city in USA**  \n**- Some location variables appear to be unrecgonizable.**","95a72930":"**- Nearly `140` characters as the most common length among the dataset **","0f72e7fb":"# Load dataset","c26f14fb":"## What are most common punctuations and their frequenies?","7af695d5":"### The above plot includes all data points, Is there any difference between fake and real points?","dc1fd9d0":"# Using Google BERT","6c56277f":"**- There is no major difference in the number of characters between fake and real tweets**","065ac48c":"## What are features containing NaN and how many in each?","c5635b48":"**- `4-8` word length in average for both real and fake tweets**","4930bf70":"**- Most common stopwords are `the,in,of` for real tweets and `the,a,to` for fake tweets**","a5bc6208":"## What is the distribution of target variables?","862eb6ec":"# Dataset overview","98d8fec3":"## How many characters in each tweet?"}}