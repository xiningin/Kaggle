{"cell_type":{"7b874496":"code","a1c15ed1":"code","d231ff12":"code","57f507fb":"code","5d9199b8":"code","a4cd91da":"code","7ed310c7":"code","b46a6435":"code","e686fcd4":"code","a7e62333":"code","5d7d5cbf":"code","934b1fad":"code","a1845514":"code","160d1b5a":"code","9f8d2597":"code","dc207a76":"code","484f106f":"code","7f61d0e1":"code","947187bd":"code","cdf62660":"code","d00f3b1f":"code","4263d920":"code","d985c903":"code","816774cc":"code","f4364253":"code","4ffeb65c":"code","8316688a":"code","e1ee9a36":"code","c5d6540c":"code","52e6868c":"code","7d103975":"code","95451908":"code","c6af029e":"code","a1c24751":"code","ee76d5ee":"code","17f94404":"code","7dced7d9":"code","f41a84b5":"code","2f8ad0bf":"code","6cdc5724":"code","4ff08bdc":"code","891dc83b":"code","5ff0b417":"code","9075accd":"code","3b5ed8a5":"code","6464c695":"code","f0692878":"code","92467e75":"markdown","159ff9ae":"markdown","ee94b651":"markdown","d660306e":"markdown","de60099b":"markdown","6e5f9b5f":"markdown","73fa3ab6":"markdown","e1a84db6":"markdown","92e688f1":"markdown","aab48711":"markdown","d1442cff":"markdown","4eb2f07a":"markdown","9be32c31":"markdown","68b394b6":"markdown","d0f4a22a":"markdown","aa8cf3d5":"markdown","d2d9c7a9":"markdown","e7b2ecc4":"markdown","b65956b2":"markdown","fccef939":"markdown","8dc4ce8a":"markdown","79af4876":"markdown","bf5a8a77":"markdown","658f20c7":"markdown","ab1be611":"markdown","0ff741f9":"markdown","0878eb5f":"markdown","eaba7bc2":"markdown","139815da":"markdown","db7e52cf":"markdown","1f1e3852":"markdown","548411e1":"markdown","0b77c768":"markdown","2792d774":"markdown","babf4aa0":"markdown","fc27c80c":"markdown","cac96fc1":"markdown","0839072c":"markdown","a8f8e270":"markdown","8fba0021":"markdown","cdd6d113":"markdown","d6cddf28":"markdown","a900191e":"markdown","f64d878f":"markdown","ba339166":"markdown"},"source":{"7b874496":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\npd.set_option('max_r',25)\nnp.set_printoptions(precision=5, suppress=True)","a1c15ed1":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf","d231ff12":"df.info()","57f507fb":"df.describe(include=\"all\")","5d9199b8":"df[df['Pclass']==3].sort_values('Fare')","a4cd91da":"multi_tickets = [ticket for ticket in df['Ticket'].unique() if (df['Ticket']==ticket).sum()>1]\ndf[df['Ticket'].isin(multi_tickets)].sort_values('Ticket')","7ed310c7":"df_eda=df.copy()","b46a6435":"(df_eda[['Pclass','Survived']].groupby('Pclass').mean()).join(df_eda['Pclass'].value_counts())","e686fcd4":"(df_eda[['Sex','Survived']].groupby('Sex').mean()).join(df_eda['Sex'].value_counts())","a7e62333":"(df_eda[['Embarked','Survived']].groupby('Embarked').mean()).join(df_eda['Embarked'].value_counts())","5d7d5cbf":"# creating 'relatives' column and dropping 'SibSp' and 'Parch' columns\ndf_eda['relatives']=df_eda['SibSp']+df_eda['Parch']\ndf_eda.drop(['SibSp','Parch'], axis=1, inplace=True)","934b1fad":"(df_eda[['relatives','Survived']].groupby('relatives').mean()).join(df_eda['relatives'].value_counts())","a1845514":"# grouping age into categories \ndf_eda['AgeCategory']=df_eda['Age']\/\/15 # multiples of 15\n(df_eda[['AgeCategory','Survived']].groupby('AgeCategory').mean()).join(df_eda['AgeCategory'].value_counts())","160d1b5a":"col_drop = ['PassengerId','Name','Ticket','Cabin']\ndf_eda.drop(col_drop,axis=1,inplace=True)","9f8d2597":"df_eda","dc207a76":"sns.pairplot(data=df_eda,hue='Survived')","484f106f":"sns.heatmap(df_eda.corr(),annot=True)","7f61d0e1":"fig, axes = plt.subplots(2,2,figsize=(16,12))\n\nsns.stripplot(ax=axes[0,0],data=df_eda,x='Pclass',y='Fare',hue='Survived')\n\nsns.stripplot(ax=axes[0,1],data=df_eda,x='relatives',y='Fare',hue='Survived')\n\nsns.stripplot(ax=axes[1,0],data=df_eda,x='Pclass',y='Age',hue='Survived')\n\nsns.stripplot(ax=axes[1,1],data=df_eda,x='relatives',y='Age',hue='Survived')","947187bd":"sns.stripplot(data=df_eda,x='Sex',y='Age',hue='Survived')","cdf62660":"plt.figure(figsize=(4,8))\n\nsns.stripplot(data=df_eda,x='Embarked',y='Fare',hue='Survived')","d00f3b1f":"df","4263d920":"df.drop(col_drop,axis=1,inplace=True)","d985c903":"df","816774cc":"y = df['Survived']\nX = df.drop('Survived', axis=1)","f4364253":"X","4ffeb65c":"y","8316688a":"num_cols = [col for col in X.columns if X[col].dtype in ['int64','float64']]\ncat_cols = [col for col in X.columns if X[col].dtype == 'object']","e1ee9a36":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\n# defining a function to make it easier later prepping the test data\ndef dataPrep(dataFrame):\n    # imputing missing values\n    imputeNum = SimpleImputer(strategy='median')\n    imputeCat = SimpleImputer(strategy='most_frequent')\n    \n    dataFrame[num_cols] = imputeNum.fit_transform(dataFrame[num_cols])\n    dataFrame[cat_cols] = imputeCat.fit_transform(dataFrame[cat_cols])\n    \n    # scaling numerical variables\n    scaler = StandardScaler()\n    \n    dataFrame[num_cols] = scaler.fit_transform(dataFrame[num_cols])\n    \n    # encoding categorical variables\n    ordEnc = OrdinalEncoder()\n    \n    dataFrame[cat_cols] = ordEnc.fit_transform(dataFrame[cat_cols])\n    \n    # creating new features\n    dataFrame['relatives'] = dataFrame['SibSp']+dataFrame['Parch']\n    dataFrame['AgeCategory'] = dataFrame['Age']\/\/15\n    dataFrame.drop(['SibSp','Parch','Age'], axis=1, inplace=True)\n    \n    return dataFrame\n\nX = dataPrep(X)","c5d6540c":"X","52e6868c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodels = ['Logistic Regression','K-Nearest Neighbors','Support Vector Classification','Naive Bayes','Decision Tree','Random Forest']\nscores = []\nparams = []","7d103975":"logreg = LogisticRegression(random_state=0)\n\nlogreg_params = {\n    'penalty' : ['l1','l2'],\n    'C': np.logspace(-4,4,20)\n}\n\nlogreg_gs = GridSearchCV(logreg,\n                        param_grid = logreg_params,\n                        cv=5,\n                        verbose=5,\n                        n_jobs=-1,\n                        scoring='accuracy')\n\nlogreg_gs.fit(X,y)\nscores.append(logreg_gs.best_score_)\nparams.append(logreg_gs.best_params_)","95451908":"knn = KNeighborsClassifier()\n\nknn_params = {\n    'n_neighbors': [n for n in range(1,11)],\n    'p' : [1,2],\n    'weights': ['uniform','distance'],\n}\n\nknn_gs = GridSearchCV(knn,\n                     param_grid = knn_params,\n                     cv=5,\n                     verbose=5,\n                     n_jobs=-1,\n                     scoring='accuracy')\n\nknn_gs.fit(X,y)\nscores.append(knn_gs.best_score_)\nparams.append(knn_gs.best_params_)","c6af029e":"svc = SVC(random_state=0)\n\nsvc_params = {\n    'C':[1,10,100,1000],\n    'kernel':['linear', 'poly', 'rbf', 'sigmoid']\n}\n\nsvc_gs = GridSearchCV(svc,\n                     param_grid = svc_params,\n                     cv=5,\n                     verbose=5,\n                     n_jobs=-1,\n                      scoring='accuracy')\n\nsvc_gs.fit(X,y)\nscores.append(svc_gs.best_score_)\nparams.append(svc_gs.best_params_)","a1c24751":"nb = GaussianNB()\n\nnb_params = {\n    'var_smoothing': np.logspace(0,-9, num=100)\n}\n\nnb_gs = GridSearchCV(nb,\n                    param_grid=nb_params,\n                    cv=5,\n                    verbose=5,\n                    n_jobs=-1,\n                     scoring='accuracy')\n\nnb_gs.fit(X,y)\nscores.append(nb_gs.best_score_)\nparams.append(nb_gs.best_params_)","ee76d5ee":"tree = DecisionTreeClassifier(random_state=0)\n\ntree_params = {\n    'criterion': ['gini','entropy'],\n    'max_depth': [3,5,7,10,15],\n    'min_samples_split': [2,3,5],\n}\n\ntree_gs = GridSearchCV(tree,\n                      param_grid=tree_params,\n                      cv=5,\n                      verbose=5,\n                      n_jobs=-1,\n                      scoring='accuracy')\n\ntree_gs.fit(X,y)\nscores.append(tree_gs.best_score_)\nparams.append(tree_gs.best_params_)","17f94404":"rf = RandomForestClassifier(random_state=0)\n\nrf_params = {\n    'n_estimators': [n for n in range(100,1001,100)],\n    'criterion': ['gini','entropy'],\n    'max_depth': [3,5,7,10,15],\n    'min_samples_split': [2,3,5],\n    'bootstrap': [True,False]\n}\n\nrf_gs = GridSearchCV(rf,\n                    param_grid=rf_params,\n                    cv=5,\n                    verbose=5,\n                    n_jobs=-1,\n                    scoring='accuracy')\n\nrf_gs.fit(X,y)\nscores.append(rf_gs.best_score_)\nparams.append(rf_gs.best_params_)","7dced7d9":"for i in range(0,6):\n    print(models[i])\n    print('Score: ', scores[i])\n    print('Params: ', params[i])\n    print('')","f41a84b5":"models[scores.index(max(scores))]","2f8ad0bf":"model = RandomForestClassifier(n_estimators=500,\n                              min_samples_split=3,\n                              max_depth=7,\n                              criterion='gini',\n                              bootstrap=True,\n                              random_state=0)\n\nmodel.fit(X,y)","6cdc5724":"df_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test","4ff08bdc":"Id = df_test['PassengerId']","891dc83b":"df_test.drop(col_drop,axis=1,inplace=True)","5ff0b417":"df_test","9075accd":"df_test = dataPrep(df_test)","3b5ed8a5":"df_test","6464c695":"preds = model.predict(df_test)","f0692878":"output = pd.DataFrame({\n    'PassengerId': Id,\n    'Survived': preds\n})\n\noutput.to_csv('submission.csv', index=False)","92467e75":"#### Logistic Regression","159ff9ae":"#### Support Vector Classifier","ee94b651":"First is to drop unwanted columns.","d660306e":"#### Naive Bayes","de60099b":"Load all model classes from `sklearn` library","6e5f9b5f":"Predict the test data using the chosen model","73fa3ab6":"Females onboard the Titanic has higher survivability rate compared to males.","e1a84db6":"### Final Model","92e688f1":"### Data Preparation","aab48711":"Drop the unwanted columns","d1442cff":"### General Statistical Description","4eb2f07a":"First we'll take a look at the relation between each features of the dataset. There seems to be a relatively high correlation between `Fare`, `Pclass`, `Age`, and `Relatives`","9be32c31":"Our data is ready to use for training, now let's try several models.","68b394b6":"Our original, raw data:","d0f4a22a":"We will proceed with graphic representations, but first, dropping the unnecessary columns","aa8cf3d5":"# Machine Learning","d2d9c7a9":"Next is exploring the categorical features. Let's see the graphs","e7b2ecc4":"Insights gained by first looking at the few data displayed:\n- `PassengerId` and `Name` columns should contain unique values equivalent to the total number of entries, therefore it will be dropped\n- `Age` and `Cabin` contains several NULL values.\n- `Fare` have significant disparity among its values, this might be affected from the `Pclass` column, since higher class costs more.\n\nBased on the description of the dataset, `Survived` is the target variable and the rest are the features","b65956b2":"# EDA","fccef939":"Passengers with fewer relatives have higher survivability rate ","8dc4ce8a":"### Model Training + Hyperparameter Tuning using Grid Search","79af4876":"First, load the test set","bf5a8a77":"Passengers embarked from port Cherbourg achieved higher survivability rate compared to passengers that embarked from other ports","658f20c7":"#### Random Forest","ab1be611":"### Output for Competition Submission","0ff741f9":"Passengers between the age of 15-30 and 60-75 has a very slim chance of surviving the Titanic disaster, while most children (less than age 15) survived.","0878eb5f":"Suprisingly `Ticket` column is not a wholly unique column like `PassengerId` and `Name`, indicating that not all passengers have a unique ticket code (like I assumed first time looking at the data). \n<br><br>\nFurthermore, there is an imbalance between `Fare` values within each respective passenger classes. There is a high chance that passengers having the exact same ticket code also have the same `Fare` value as the accumulative cost from the same transaction (probably friend connections? or relatives on board purchasing at the same time).","eaba7bc2":"### Graphic Representations","139815da":"As displayed 2 cells above, there is an uneven fare distribution throughout passenger class 3, passengers with the same ticket code share the same fare values. This might imply that they bought the tickets altogether at the same time. Regardless, in this version of the notebook I will drop the `Ticket` column.","db7e52cf":"Random Forest seems to be the best model for this dataset, we will be using it for our final model","1f1e3852":"I will copy the `df` dataframe to explore the data explicitly without making any changes to the original loaded data (I'll use it later for data preparation)","548411e1":"It looks like passengers on higher, much more executive first class, has higher survivability rate compared to others.","0b77c768":"#### Decision Tree","2792d774":"### Survivability rate based on different features","babf4aa0":"### Loading the Dataset","fc27c80c":"Save the `PassengerId` column, since it's needed for the submission format","cac96fc1":"### Basic Info","0839072c":"Below is the heatmap of the dataset's feature correlation","a8f8e270":"Next, assigning the features and the target variable to different variables.","8fba0021":"Output to .csv file","cdd6d113":"#### K-Nearest Neighbors","d6cddf28":"Clean and prepare the data for our model to predict","a900191e":"There are a few details shown:\n- `Cabin` is missing a lot of entries. Intuitively, this column should be dropped(**and will be dropped in this version of the notebook**) rather than filling it with a certain value (using SimpleImputer with `most_frequent` setting, for example). But since `SibSp` and `Parch` indicate the amount of relatives on board Titanic with each respective passenger, there might be a chance that related passengers are in the same Cabin. Through analyzing the names, it might be possible to fill in the `Cabin` column with reasonable values.\n- `Age` has a few missing entries. The simplest way to fill those missing values is with the median of the existing values, which is what I'm going to do.\n- `Embarked` has exactly 2 missing entries. It is going to be filled with the most frequent value of the column.","f64d878f":"#### Model Scores","ba339166":"As mentioned before, passengers onboard on higher class(in this case smaller numerical value), have higher survivability rate compared to passengers on lower classes.\n<br>\nFare vs. relatives graph doesn't really give us much explanation, but Age vs. relatives graph shows that most of the lone passengers (a.k.a. passengers that boarded alone without no relatives) have low survivability rate."}}