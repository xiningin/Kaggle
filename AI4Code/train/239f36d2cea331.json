{"cell_type":{"1884530d":"code","21bab596":"code","00e13233":"code","e8afb342":"code","e1d3d396":"code","07686118":"code","6ff39c7f":"code","f161d3a9":"code","633fee2c":"code","129c2f45":"code","70a1566f":"code","497ab47b":"code","4ee39637":"code","5013cc5c":"code","792dda6d":"code","3331b8e5":"code","c94644ac":"code","12e098aa":"code","740ab594":"code","294a3973":"code","0bd4db03":"code","f9b84ed9":"code","519dc6af":"code","2302a678":"code","797e182a":"code","3a226c87":"code","23257263":"code","9ac8269d":"code","d4146df3":"code","5db49f26":"code","ce6d9486":"code","ace48e81":"code","390eefbe":"code","d6af33f4":"code","b0c092a2":"code","1224eef5":"code","9dceb1d9":"code","0adb08cf":"code","f37e69cc":"code","afea69f4":"code","fa679026":"code","c402cba3":"code","bd79ce52":"code","0f223341":"code","dbef8c29":"code","c4da69bd":"code","e2985e16":"code","a96b687c":"code","dbd9e35b":"code","ec74303f":"code","7bfe07b3":"code","1429f00a":"code","fa585e00":"markdown","7994abb3":"markdown","499c433e":"markdown","af3916c0":"markdown","bb67d5d0":"markdown","1a04d54c":"markdown","1df56887":"markdown","a828c54e":"markdown","dea4d72a":"markdown","43283f8f":"markdown","6eb1e12a":"markdown","d164a89c":"markdown","ac7d288f":"markdown","2656efa5":"markdown","8dde923c":"markdown","bc9b2171":"markdown","e62e4e9c":"markdown","63798684":"markdown","73a2bc4e":"markdown","b9b13044":"markdown","cd12fe3f":"markdown","007fde47":"markdown","0c3e8b58":"markdown","c40b18d6":"markdown","d7017472":"markdown","6e2d9812":"markdown","9298ea2f":"markdown","283aa061":"markdown","198f10ef":"markdown","4497a57c":"markdown","50921985":"markdown","475eb96c":"markdown","97e73361":"markdown","be25fd6a":"markdown","646996c1":"markdown","4354b429":"markdown","6a84a7bc":"markdown","810b455c":"markdown"},"source":{"1884530d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport operator\nimport warnings\nfrom collections import Counter\nfrom itertools import chain\nfrom time import time\n# feature selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV,SelectFromModel\n# classifiers\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import RandomizedSearchCV,cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.utils import resample,shuffle\nimport matplotlib.pyplot as plt","21bab596":"vardat = pd.read_csv('..\/input\/clinvar_conflicting.csv',dtype={0:object,38:object,40:object})\n# explicity define dtype for pandas dtype error\nvardat.head()","00e13233":"print(vardat.shape)\nprint(vardat.columns)","e8afb342":"vardat.describe()","e1d3d396":"sns.countplot(vardat['CLASS'])\nplt.show()\nprint(Counter(vardat['CLASS'].values))","07686118":"vardat.groupby(['CHROM','CLASS']).size()","6ff39c7f":"Counter(vardat[['REF', 'ALT']].apply(lambda x: ':'.join(x), axis=1))","f161d3a9":"print(Counter(vardat['ORIGIN'].values))","633fee2c":"vardat['ORIGIN'].fillna(0, inplace=True)\nprint(Counter(vardat['ORIGIN'].values))","129c2f45":"cons = Counter(list(chain.from_iterable([str(v).split('&') for v in vardat['Consequence'].values])))\nsorted(cons.items(),key=operator.itemgetter(1),reverse=True)","70a1566f":"clnvc = Counter(vardat['CLNVC'].values)\nsorted(clnvc.items(),key=operator.itemgetter(1),reverse=True)","497ab47b":"clndn = Counter(list(chain.from_iterable([str(v).split('|') for v in vardat['CLNDN'].values])))\nsorted(clndn.items(),key=operator.itemgetter(1),reverse=True)","4ee39637":"var_vals = vardat[['AF_ESP','AF_EXAC','AF_TGP','CADD_PHRED','CADD_RAW','CLASS']].dropna()\nprint(var_vals.info())\nprint(var_vals.describe())\nprint(Counter(var_vals['CLASS']))","5013cc5c":"sns.pairplot(var_vals,hue='CLASS')\nplt.show()","792dda6d":"var_corr = vardat[['AF_ESP','AF_EXAC','AF_TGP','CADD_PHRED','CADD_RAW']].corr()\ncmap = sns.diverging_palette(220, 20, n=7,as_cmap=True)\nsns.heatmap(var_corr, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5,annot=True,cbar_kws={\"shrink\": .5})","3331b8e5":"# Below are some functions used to encode various columsn\ndef consequence_encoder(consequence,consdict):\n    '''\n    encoder for consequence data\n    '''\n    outmat = np.zeros((len(consequence),len(consdict)),dtype=np.int)\n    for i,cons in enumerate(consequence):\n        conslist = str(cons).split('&')\n        cindex = np.zeros((len(consdict)),dtype=np.int)\n        for c in conslist:\n            if c in consdict:\n                cindex[consdict[c]]=1\n            else:\n                continue\n        outmat[i] = cindex\n    return outmat\n\ndef get_base_dict(ref_alt,mincount=25):\n    '''\n    return all the Reference\/Alternate bases with count>=mincount\n    '''\n    base_count = Counter(ref_alt)\n    basedict = {}\n    i = 0\n    for b,c in base_count.items():\n    #     print(a,c)\n        if c<mincount:\n            continue\n        basedict[b]= i\n        i+=1\n    return basedict\n    \n\ndef base_encoder(basedat,basedict):\n    '''\n    encoder for Reference\/Alternate bases\n    '''\n    basemat = np.zeros((len(basedat),len(basedict)),dtype=np.int)\n    for i,b in enumerate(basedat):\n        bindex = np.zeros((len(basedict)),dtype=np.int)\n        if b in basedict:\n            bindex[basedict[b]] = 1\n    return basemat\n\ndef CLNDISDB_count(clndisdb):\n    '''\n    return count of evidence ids\n    '''\n    clncount = np.zeros(shape=(len(clndisdb)),dtype=np.int)\n    for i, cln in enumerate(clndisdb):\n        clncount[i]=len(re.sub(pattern=r'\\.\\|',repl='',string=str(cln)).split('|'))\n    return clncount\n\ndef CLNDN_count(clndn):\n    '''\n    return clinvar disease name\n    '''\n    clndncount = np.zeros(shape=(len(clndn)),dtype=np.int)\n    for i, cln in enumerate(clndn):\n        clndncount[i]=len(re.sub(pattern=r'\\.\\|',repl='',string=str(cln)).split('|'))\n    return clndncount\n\ndef get_clndn_dict(clndn,mincount=25):\n    '''\n    return clinvar disease name dictionary, where each disease name must occur mincount times\n    '''\n    clndn_count = Counter(list(chain.from_iterable([str(dn).split('|') for dn in clndn])))\n    clndn_dict = {}\n    i = 0\n    for dn,cn in clndn_count.items():\n        if cn < mincount:\n            continue\n        clndn_dict[dn]=i\n        i+=1\n    return clndn_dict\n\ndef clndn_encoder(clndn,clndn_dict):\n    '''\n    encoder for clinvar disease names\n    '''\n    clndnmat = np.zeros((len(clndn),len(clndn_dict)),dtype=np.int)\n    for i,dns in enumerate(clndn):\n        dndat = str(dns).split('|')\n        dnindex = np.zeros((len(clndn_dict)),dtype=np.int)\n        for dn in dndat:\n            if dn in clndn_dict:\n                dnindex[clndn_dict[dn]] = 1\n    return clndnmat","c94644ac":"format_dat = vardat[['AF_ESP','AF_EXAC','AF_TGP','LoFtool']]\nformat_dat.fillna(0, inplace=True)\nformat_dat.isnull().values.any()","12e098aa":"cons_set = set(list(chain.from_iterable([str(v).split('&') for v in vardat['Consequence'].values])))\nconsdict = dict(zip(cons_set,range(len(cons_set))))\n# # CLNDISDB\nclndb_count = CLNDISDB_count(vardat['CLNDISDB'].values)\nformat_dat =  np.concatenate((format_dat,clndb_count.reshape(-1,1)),axis=1)\n# # CLNDN\nclndn_count = CLNDN_count(vardat['CLNDN'].values)\nformat_dat =  np.concatenate((format_dat,clndn_count.reshape(-1,1)),axis=1)\n# # Reference allele length\nreflen = np.array([len(r) for r in vardat['REF'].values],dtype=np.int)\nformat_dat =  np.concatenate((format_dat,reflen.reshape(-1, 1)),axis=1)\n# # allele length\nallelelen = np.array([len(r) for r in vardat['Allele'].values],dtype=np.int)\nformat_dat =  np.concatenate((format_dat,allelelen.reshape(-1, 1)),axis=1)\n# chromosome\nchr_encoder = LabelEncoder()\nchr_onehot = OneHotEncoder(sparse=False)\nchr_ind = chr_encoder.fit_transform(vardat['CHROM'])\nformat_dat =  np.concatenate((format_dat,chr_onehot.fit_transform(chr_ind.reshape(-1, 1))),axis=1)\n# # origin\norigin_encoder = OneHotEncoder(sparse=False)\nformat_dat =  np.concatenate((format_dat,origin_encoder.fit_transform(vardat[['ORIGIN']])),axis=1)\n# # CLNVC\ncldn_encoder = LabelEncoder()\ncldn_onehot = OneHotEncoder(sparse=False)\nclndn_ind = cldn_encoder.fit_transform(vardat['CLNVC'])\nformat_dat =  np.concatenate((format_dat,cldn_onehot.fit_transform(clndn_ind.reshape(-1, 1))),axis=1)\n# # impact \nimp_encoder = LabelEncoder()\nimp_onehot = OneHotEncoder(sparse=False)\nimp_ind = imp_encoder.fit_transform(vardat['IMPACT'])\nformat_dat =  np.concatenate((format_dat,imp_onehot.fit_transform(imp_ind.reshape(-1, 1))),axis=1)\n# # Exon encoder\nexon_encode = np.ones((vardat.shape[0]),dtype=np.int)\nexon_encode[vardat['EXON'].isna()]=0\nformat_dat =  np.concatenate((format_dat,exon_encode.reshape(-1, 1)),axis=1)\n# # clinical disease name\nclndn_dict = get_clndn_dict(vardat['CLNDN'],100)\nclndn_encode = clndn_encoder(vardat['CLNDN'],clndn_dict)\nformat_dat =  np.concatenate((format_dat,clndn_encode),axis=1)\n# # consequence \ncons_encode = consequence_encoder(vardat['Consequence'].values,consdict)\nformat_dat =  np.concatenate((format_dat,cons_encode),axis=1)\n# # base data\nbase_dict = get_base_dict(vardat[['REF', 'ALT']].apply(lambda x: ':'.join(x), axis=1),50)\nbase_encode = base_encoder(list(vardat[['REF', 'ALT']].apply(lambda x: ':'.join(x), axis=1)),base_dict)\nformat_dat =  np.concatenate((format_dat,base_encode),axis=1)\nprint(format_dat.shape)","740ab594":"dat_Xtrain,tmp_x,dat_Ytrain,tmp_y = train_test_split(format_dat,vardat['CLASS'],test_size=0.3,random_state=42)\ndat_Xval,dat_Xtest,dat_Yval,dat_Ytest= train_test_split(tmp_x,tmp_y,test_size=0.5,random_state=42)","294a3973":"print('Training data stats')\nprint(dat_Xtrain.shape)\nprint(Counter(dat_Ytrain))\nprint('\\nValidation data stats')\nprint(dat_Xval.shape)\nprint(Counter(dat_Yval))\nprint('\\nTest data stats')\nprint(dat_Xtest.shape)\nprint(Counter(dat_Ytest))","0bd4db03":"warnings.simplefilter(action='ignore',category=FutureWarning)\nrf_base = RandomForestClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(rf_base,dat_Xtrain,dat_Ytrain,cv=10))\nrf_base.fit(dat_Xtrain,dat_Ytrain)\ny_pred = rf_base.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","f9b84ed9":"bc_base = BaggingClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(bc_base,dat_Xtrain,dat_Ytrain,cv=10))\nbc_base.fit(dat_Xtrain,dat_Ytrain)\ny_pred = bc_base.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","519dc6af":"et_base = ExtraTreesClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(et_base,dat_Xtrain,dat_Ytrain,cv=10))\net_base.fit(dat_Xtrain,dat_Ytrain)\ny_pred = et_base.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","2302a678":"abc = AdaBoostClassifier(random_state=42,n_estimators=100)\nprint(cross_val_score(abc,dat_Xtrain,dat_Ytrain,cv=10))\nabc.fit(dat_Xtrain,dat_Ytrain)\ny_pred = abc.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","797e182a":"xg_base = XGBClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(xg_base,dat_Xtrain,dat_Ytrain,cv=10))\nxg_base.fit(dat_Xtrain,dat_Ytrain)\ny_pred = xg_base.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","3a226c87":"def upsample(x,y):\n    '''\n    upsample least represented class\n    y should be the labels,up sample least represented class in y\n    '''\n    x = np.array(x)\n    ycount = Counter(y)\n    ymin = min(list(ycount.values()))\n    ymax = max(list(ycount.values()))\n    yind = {}\n    rex = None\n    rey = list()\n    for yi,c in ycount.items():\n        if c==ymax:\n            ind = np.where(y==yi)[0]\n            if rex is None:\n                rex = x[ind]\n            else:\n                rex = np.concatenate((rex,x[ind]),axis=0)\n            rey.extend([yi]*ymax)\n        elif c==ymin:\n            ind = np.where(y==yi)[0]\n            tmp_dat = resample(x[ind],replace=True,n_samples=ymax,random_state=42)\n            if rex is None:\n                rex = tmp_dat\n            else:\n                rex = np.concatenate((rex,tmp_dat),axis=0)\n            rey.extend([yi]*ymax)\n    return shuffle(rex,np.array(rey),random_state=42,replace=False)\n\ndef downsample(x,y):\n    '''\n    downsample over represented class\n    y should be the labels,up sample least represented class in y\n    '''\n    x = np.array(x)\n    ycount = Counter(y)\n    ymin = min(list(ycount.values()))\n    ymax = max(list(ycount.values()))\n    yind = {}\n    rex = None\n    rey = list()\n    for yi,c in ycount.items():\n        if c==ymin:\n            ind = np.where(y==yi)[0]\n            if rex is None:\n                rex = x[ind]\n            else:\n                rex = np.concatenate((rex,x[ind]),axis=0)\n            rey.extend([yi]*ymin)\n        elif c==ymax:\n            ind = np.where(y==yi)[0]\n            tmp_dat = resample(x[ind],replace=False,n_samples=ymin,random_state=42)\n            if rex is None:\n                rex = tmp_dat\n            else:\n                rex = np.concatenate((rex,tmp_dat),axis=0)\n            rey.extend([yi]*ymin)\n    return shuffle(rex,np.array(rey),random_state=42,replace=False)","23257263":"dat_Xup, dat_Yup = upsample(dat_Xtrain,dat_Ytrain)\ndat_Xdown, dat_Ydown = downsample(dat_Xtrain,dat_Ytrain)\nprint('\\nUpsample stats')\nprint(dat_Xup.shape)\nprint(Counter(dat_Yup))\nprint('\\nDownsample stats')\nprint(dat_Xdown.shape)\nprint(Counter(dat_Ydown))","9ac8269d":"rf_up = RandomForestClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(rf_up,dat_Xup,dat_Yup,cv=10))\nrf_up.fit(dat_Xup,dat_Yup)\ny_pred = rf_up.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","d4146df3":"rf_down = RandomForestClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(rf_down,dat_Xdown,dat_Ydown,cv=10))\nrf_down.fit(dat_Xdown,dat_Ydown)\ny_pred = rf_down.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","5db49f26":"bc_up = BaggingClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(bc_up,dat_Xup,dat_Yup,cv=10))\nbc_up.fit(dat_Xup,dat_Yup)\ny_pred = bc_up.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","ce6d9486":"bc_down = BaggingClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(bc_down,dat_Xdown,dat_Ydown,cv=10))\nbc_down.fit(dat_Xdown,dat_Ydown)\ny_pred = bc_down.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","ace48e81":"et_up = ExtraTreesClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(et_up,dat_Xup,dat_Yup,cv=10))\net_up.fit(dat_Xup,dat_Yup)\ny_pred = et_up.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","390eefbe":"et_down = ExtraTreesClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(et_down,dat_Xdown,dat_Ydown,cv=10))\net_down.fit(dat_Xdown,dat_Ydown)\ny_pred = et_down.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","d6af33f4":"abc_up = AdaBoostClassifier(random_state=42,n_estimators=100)\nprint(cross_val_score(abc_up,dat_Xup,dat_Yup,cv=10))\nabc_up.fit(dat_Xup,dat_Yup)\ny_pred = abc_up.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","b0c092a2":"abc_down = AdaBoostClassifier(random_state=42,n_estimators=100)\nprint(cross_val_score(abc_down,dat_Xdown,dat_Ydown,cv=10))\nabc_down.fit(dat_Xdown,dat_Ydown)\ny_pred = abc_down.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","1224eef5":"xg_up = XGBClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(xg_up,dat_Xup,dat_Yup,cv=10))\nxg_up.fit(dat_Xup,dat_Yup)\ny_pred = xg_up.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","9dceb1d9":"xg_down = XGBClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(xg_down,dat_Xdown,dat_Ydown,cv=10))\nxg_down.fit(dat_Xdown,dat_Ydown)\ny_pred = xg_down.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","0adb08cf":"from imblearn.over_sampling import SMOTENC\n# Synthetic Minority Over-sampling Technique for Nominal and Continuous (SMOTE-NC).","f37e69cc":"sm = SMOTENC(random_state=42,n_jobs=6,categorical_features=np.arange(8,dat_Xtrain.shape[1],1))\ndat_Xsmote,dat_Ysmote = sm.fit_resample(dat_Xtrain,dat_Ytrain)\nprint('\\nSMOTE stats')\nprint(dat_Xsmote.shape)\nprint(Counter(dat_Ysmote))","afea69f4":"rf_smote = RandomForestClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(rf_smote,dat_Xsmote,dat_Ysmote,cv=10))\nrf_smote.fit(dat_Xsmote,dat_Ysmote)\ny_pred = rf_smote.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","fa679026":"bc_smote = BaggingClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(bc_smote,dat_Xsmote,dat_Ysmote,cv=10))\nbc_smote.fit(dat_Xsmote,dat_Ysmote)\ny_pred = bc_smote.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","c402cba3":"et_smote = ExtraTreesClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(et_smote,dat_Xsmote,dat_Ysmote,cv=10))\net_smote.fit(dat_Xsmote,dat_Ysmote)\ny_pred = et_smote.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","bd79ce52":"abc_smote = AdaBoostClassifier(random_state=42,n_estimators=100)\nprint(cross_val_score(abc_smote,dat_Xsmote,dat_Ysmote,cv=10))\nabc_smote.fit(dat_Xsmote,dat_Ysmote)\ny_pred = abc_smote.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","0f223341":"xg_smote = XGBClassifier(random_state=42,n_jobs=6,n_estimators=100)\nprint(cross_val_score(xg_smote,dat_Xsmote,dat_Ysmote,cv=10))\nxg_smote.fit(dat_Xsmote,dat_Ysmote)\ny_pred = xg_smote.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","dbef8c29":"from imblearn.ensemble import BalancedRandomForestClassifier ","c4da69bd":"brf = BalancedRandomForestClassifier(random_state = 42,n_estimators=100)\nprint(cross_val_score(brf,dat_Xtrain,dat_Ytrain,cv=10))\nbrf.fit(dat_Xtrain,dat_Ytrain)\ny_pred =brf.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","e2985e16":"from imblearn.ensemble import BalancedBaggingClassifier ","a96b687c":"bbc = BalancedBaggingClassifier(random_state = 42,n_estimators=100)\nprint(cross_val_score(bbc,dat_Xtrain,dat_Ytrain,cv=10))\nbbc.fit(dat_Xtrain,dat_Ytrain)\ny_pred =bbc.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","dbd9e35b":"from imblearn.ensemble import EasyEnsembleClassifier","ec74303f":"eec = EasyEnsembleClassifier(random_state = 42,n_estimators=100)\n#print(cross_val_score(eec,dat_Xtrain,dat_Ytrain,cv=10))\neec.fit(dat_Xtrain,dat_Ytrain)\ny_pred =eec.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","7bfe07b3":"from imblearn.ensemble import RUSBoostClassifier","1429f00a":"rbc = RUSBoostClassifier(random_state = 42,n_estimators=100)\n#print(cross_val_score(eec,dat_Xtrain,dat_Ytrain,cv=10))\nrbc.fit(dat_Xtrain,dat_Ytrain)\ny_pred =rbc.predict(dat_Xval)\nprint('Confusion matrix\\n',confusion_matrix(dat_Yval,y_pred))\nprint('Accuracy\\n',accuracy_score(dat_Yval,y_pred))\nprint('Classificaton report\\n',classification_report(dat_Yval,y_pred))","fa585e00":"So, the bagging classifier shows an improvement in `recall`, but `accuracy` remains somewhat the same, so we will keep exploring a bit further.\n\n#### ExtraTrees classifier\n\nTesting extratrees classifier","7994abb3":"### Data formatting\n\nThe column `Consequence` has values concatenated by `&`, so these need to be separated properly, and some of the columns cannot be used as such, so these have to be either one hot encoded, or metadata (for example: count of certain attributes) have to be generated. The functions below are written to handle these issues.","499c433e":"Since the only variables with numerical variables are allele frequency **(AF..)** and CADD score **(CADD..)** explore these values in detail:","af3916c0":"#### Bagging classifier","bb67d5d0":"#### Adaboost classifier","1a04d54c":"#### CLNVC (Variant Type) column","1df56887":"ExtraTreesClassifier do not perform well compared to other classifers in this case, so moving on to the next classifier.\n\n#### AdaBoost classifier","a828c54e":"### Classification\n\nAs the first step, estimate a baseline score using random forest classifer and compare the results from other classifers  for an imporvement. Finally, we will also check whether accounting for the class imbalance problem improves final test results. \n\n#### Base classifier","dea4d72a":"##### Now the total number of positive and negative esamples in the class","43283f8f":"#### Adaboost classifier","6eb1e12a":"### Context\n\nAccording to the [data description](https:\/\/www.kaggle.com\/kevinarvai\/clinvar-conflicting\/home) the data is from a public resource called [ClinVar](https:\/\/www.ncbi.nlm.nih.gov\/clinvar), which contains annotations about genetic variants. If the variants have conflicting classifications from laboratory to laboratory, it can cause confusion when reserachers try to assess the impact of the variant on a given patient.\n","d164a89c":"#### Gradient boosting classifier","ac7d288f":"**Classification report for class `1`**\n\n| Classifier | Precision | Recall | F1-score|\n|------------|:---------:|:------:|:-------:|\n| Random forest (up)| 0.49 | 0.51 | 0.50 |\n| Random forest (down)| 0.43 | 0.73 | 0.54 |\n| Bagging (up)| 0.49 | 0.54 | 0.51 |\n| Bagging (down)| 0.44 | 0.75 | 0.56 |\n| Extra trees (up)| 0.47 | 0.43 | 0.45 |\n| Extra trees (down)| 0.42 | 0.69 | 0.52 |\n| Adaboost (up)| 0.42 | 0.70 | 0.53 |\n| Adaboost (down)| 0.42 | 0.71 | 0.53 |\n| Gradient boost (up)| 0.42 | 0.71 | 0.53 |\n| Gradient boost (down)| 0.43 | 0.72 | 0.54 |\n\nSo it looks like down sampling the over represented samples performs (slightly) better than up sampling the under represented samples. Now to the next methods...\n\n#### SMOTE: Synthetic Minority Over-sampling Techniques\n\nSMOTE tries to deal with the class imbalance problem by over-sampling of the minority class. To oversample using SMOTE, consider k nearest neighbors of a sample (data point) taken from the dataset. To create a synthetic datapoint, take the vector between the current data point and one of the k neighbors sampled earlier, and multiply this vector by a random number between 0-1 to create the synthetic data point. Here is the [wikipedia entry](https:\/\/en.wikipedia.org\/wiki\/Oversampling_and_undersampling_in_data_analysis) for SMOTE and here is a [blog post](http:\/\/rikunert.com\/SMOTE_explained) explaining the algorithm. ","2656efa5":"And now the rows and columns in detail...","8dde923c":"#### ExtraTrees classifier","bc9b2171":"#### Bagging classifier","e62e4e9c":"#### Consequence column","63798684":"So, we have equalized the classes. But how does this affect the performance ? We can go through the classifiers again...\n\n#### Randomforest classifier","73a2bc4e":"Adaboost classifier also does not perform well in this case\n\n#### Gradient boosting classifier","b9b13044":"Gradient boosting tree shows pathetic `recall` and `f1-score` on the validation data set. So, all the results above can be compressed into one table below. The results below are only for `class 1` as it is the class we are interested in.\n\n| Classifier | Precision | Recall | F1-score|\n|------------|:---------:|:------:|:-------:|\n| Random forest| 0.57 | 0.38 | 0.46 |\n| Bagging | 0.57 | 0.42 | 0.48 |\n| Extra trees| 0.52 | 0.38 | 0.44|\n| Adaboost| 0.59 | 0.20 | 0.29 |\n| Gradient boost| 0.62 | 0.17 | 0.26 |\n\nThese results indicate that none of the classifier does a good job of classifying the variants that are potentially mislabled. So can we improve these scores by chaning the training data ? From looking at the number of `0` and `1` classes in the dataset, it is clear that this dataset suffers from class imbalance problem. Two of the common ways to deal with imbalance is to either upsample the smaller class data or downsample the larger class data. These will be handled in the next section\n\n### Class balancing\n\n#### Up and down sampling\n\nTo balance the classes in the dataset, the larger class can either be upsampled or the smaller class can be downsampled. The functions below are written to handle up and down sampling. ","cd12fe3f":"Some of the chromosomes have more misclassified variants than others, as expected.\n##### REF and ALT allele count","007fde47":"According to data description,`0` should be the `unknown` origin, but this doesn't seem to be the case, so fill all `nan` with 0s ","0c3e8b58":"#### BalancedBaggingClassifier","c40b18d6":"So it appears that C&rarr;T and G&rarr;A SNPs are the most prominent in this dataset, and as mentioned in the data description, SNPs are the most prominent in this dataset.\n\n#### ORIGIN count  ","d7017472":"#### CLNDN (disease name count) column","6e2d9812":"### Summary part 2\n\n| Classifier | Precision | Recall | F1-score|\n|------------|:---------:|:------:|:-------:|\n| BalancedRandomForestClassifier| 0.44 | 0.77 | 0.56 |\n| BalancedBaggingClassifier | 0.48 | 0.70 | 0.57 |\n| EasyEnsembleClassifier| 0.42 | 0.70 | 0.53 |\n| RUSBoostClassifier| 0.47 | 0.32 | 0.38 |\n\nFrom these results, it looks like `BalancedBaggingClassifier` performs the best among balanced classifiers. \n\n### Conclusion\n\nUsing random over sampling\/under sampling or SMOTE techniques can help improve the results in predicting genetic variants with conflicting classifications.","9298ea2f":"## Genetic variant conflicting classifications","283aa061":"#### Randomforest classifier","198f10ef":"#### RUSBoostClassifier","4497a57c":"#### Bagging classifier\n\nTest how bagging classifier performs in this scenario","50921985":"#### EasyEnsembleClassifier","475eb96c":"| Classifier | Precision | Recall | F1-score|\n|------------|:---------:|:------:|:-------:|\n| Random forest| 0.49 | 0.57 | 0.53 |\n| Bagging | 0.49 | 0.50 | 0.49 |\n| Extra trees| 0.48 | 0.52 | 0.50 |\n| Adaboost| 0.43 | 0.63 | 0.51 |\n| Gradient boost| 0.45 | 0.62 | 0.52 |\n\n\n### Summary part 1\n\nIf we compare all the results:\n\n| Classifier | Precision | Recall | F1-score|\n|------------|:---------:|:------:|:-------:|\n| Random forest| 0.57 | 0.38 | 0.46 |\n| Random forest (up)| 0.49 | 0.51 | 0.50 |\n| Random forest (down)| 0.43 | 0.73 | 0.54 |\n| Random forest (SMOTE)| 0.49 | 0.57 | 0.53 |\n| Bagging | 0.57 | 0.42 | 0.48 |\n| Bagging (up)| 0.49 | 0.54 | 0.51 |\n| Bagging (down)| 0.44 | 0.75 | 0.56 |\n| Bagging (SMOTE)| 0.49 | 0.50 | 0.49 |\n| Extra trees| 0.52 | 0.38 | 0.44|\n| Extra trees (up)| 0.47 | 0.43 | 0.45 |\n| Extra trees (down)| 0.42 | 0.69 | 0.52 |\n| Extra trees (SMOTE)| 0.48 | 0.52 | 0.50 |\n| Adaboost| 0.59 | 0.20 | 0.29 |\n| Adaboost (up)| 0.42 | 0.70 | 0.53 |\n| Adaboost (down)| 0.42 | 0.71 | 0.53 |\n| Adaboost (SMOTE)| 0.43 | 0.63 | 0.51 |\n| Gradient boost| 0.62 | 0.17 | 0.26 |\n| Gradient boost (up)| 0.42 | 0.71 | 0.53 |\n| Gradient boost (down)| 0.43 | 0.72 | 0.54 |\n| Gradient boost (SMOTE)| 0.45 | 0.62 | 0.52 |\n\nSo, it looks like ```Bagging classifier``` performs the best for predicting the results in this case. But if we look at all the results in detail, we can see that this classifier underperforms in predicting `0` class label.\n\n### Using balanced classifiers\n\nIn this section we will use ensemble classifiers from [imblearn](https:\/\/pypi.org\/project\/imblearn\/) python package. Most of these classifiers are similar to the implementations in `sci-kit` package with an additional step at training to balance  samples using `RandomUnderSampler`.\n\n#### BalancedRandomForestClassifier\n","97e73361":"#### ExtraTrees classifier","be25fd6a":"#### Gradient boosting classifier","646996c1":"From the results of the base classifier, it is clear that it performs reasonably well for class `0`, it is another case for the performance metrics for class `1`. Now before moving to other classifiers, lets explore if paramter optimization can help improve the results.","4354b429":"Let's up and downsample the training data and check how these affect the performance scores","6a84a7bc":"So, it appears that the number of negative examples are ~3 times that of the positive samples. The class imbalance problem has to be considered while building a classifer for this dataset. But, that is something for the later, now explore the other columns of this dataset:\n\n#### Class count per chromosome","810b455c":"### Preliminary EDA\n\nLet's begin by reading in the data and looking at the columns:"}}