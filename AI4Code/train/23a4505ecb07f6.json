{"cell_type":{"ca4bd04c":"code","eebf8081":"code","72f41c83":"code","89dfa6e0":"code","e5d303d9":"code","1c1d10b7":"code","24bf5508":"code","bc7932dc":"code","cf06e845":"code","b7ae6676":"code","d54bf89a":"code","4effdf16":"code","5a0d525b":"code","b4154ad6":"code","85363192":"code","32be3d64":"code","fc96a838":"code","ef297430":"code","388394d2":"code","d3ecb6f6":"code","2fc7afa7":"code","1ba481ca":"code","1943a081":"code","c429f5ed":"code","98344337":"code","997d2023":"code","06b75da8":"code","72dc15ec":"code","957af784":"code","80b7aefc":"code","b8fb3168":"code","8c434178":"code","5a29fe6a":"code","bd72a4d7":"code","88c178b1":"code","5a9d56ec":"code","0513ccae":"code","29479bde":"code","b2f300ae":"code","625a2c2a":"code","c20f924a":"code","5b93d04d":"markdown","0d7f3627":"markdown","5885e7e5":"markdown","4ab23073":"markdown","7604206b":"markdown","e912858d":"markdown","7fcc68f0":"markdown"},"source":{"ca4bd04c":"! java -version","eebf8081":"! apt remove -y openjdk-11-jre-headless\n! apt update\n! apt install -y openjdk-8-jdk openjdk-8-jre","72f41c83":"! java -version","89dfa6e0":"# you can even quietly install using pip install --quiet package\n! pip install pyspark==2.4.6\n! pip install spark-nlp==2.5.2\n! pip install pycontractions","e5d303d9":"import numpy as np \nimport pandas as pd \n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\n\nfrom pycontractions import Contractions\nimport gensim.downloader as api\n\nimport gc","1c1d10b7":"from pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as sqlF\nfrom pyspark import SparkContext, SparkConf\n\n\nimport sparknlp\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.embeddings import *","24bf5508":"#initialize the spark session with spark nlp jars\nspark = sparknlp.start(gpu=True)\n\nsqlContext = SQLContext(sparkContext=spark.sparkContext, \n                        sparkSession=spark)","bc7932dc":"train = pd.read_csv(\"..\/input\/real-or-not-data-cleaned\/train.csv\")","cf06e845":"train_spark = spark.createDataFrame(train[[\"id\",\"text\",\"target\",\"clean_text\"]])","b7ae6676":"data = train.groupby('target')['id'].count()\n\nsns.barplot(x=data.index, y=data)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Target\")\nplt.show()","d54bf89a":"# model = api.load(\"glove-twitter-25\")\n\n# cont = Contractions(kv_model=model)\n# cont.load_models()","4effdf16":"# subtext_to_clearning = ['\\!', '\\$', '\\(', '\\)', '\\*', \n#                         '\\+', '\\-', '\\.', '\\:', '\\;', \n#                         '\\=', '\\?', '\\@', '\\[', '\\]', \n#                         '\\^', '\\|', '\\_', '\\{', '\\}']\n\n# def clean(tweet):\n#     tweet = re.sub(\"@[\\w]*\", \"\", tweet)    \n\n# #     tweet = tweet.replace(\"@\",\"\")\n\n#     #Remove the double or more punctuations\n#     for punc in subtext_to_clearning:\n#         tweet = re.sub(f\"[{punc}]+\",punc.replace(\"\\\\\",\"\"),tweet)\n#     tweet = tweet.replace(\"#\",\"\").strip()\n    \n#     # Remove the whispaces\n#     tweet = \" \".join(tweet.split())\n    \n#     #Expand contractions\n#     tweet = list(cont.expand_texts([tweet],\n#                                    precise=True))[0]\n    \n#     if tweet == \"\":\n#         return \".\"\n#     return tweet.lower()\n\n# cleaner_udf = sqlF.udf(clean, StringType())\n\n# train_spark = train_spark.withColumn(\"clean_text\", cleaner_udf(\"text\"))","5a0d525b":"# gc.collect()","b4154ad6":"# import wordcloud\n\n# words = train_spark.rdd.flatMap(lambda x: re.split(\"\\s+\",x[3]))\\\n#                   .map(lambda word: (word, 1))\\\n#                   .reduceByKey(lambda a, b: a + b)\n\n# schema = StructType([StructField(\"words\", StringType(), True),\n#                  StructField(\"count\", IntegerType(), True)])\n\n# words_df = sqlContext.createDataFrame(words, schema=schema)","85363192":"# word_cloud = words_df.orderBy(sqlF.desc(\"count\"))\\\n#                      .limit(200)\\\n#                      .toPandas()\\\n#                      .set_index('words')\\\n#                      .T\\\n#                      .to_dict('records')\n\n\n# wc = wordcloud.WordCloud(background_color=\"white\", max_words=200)\n# wc.generate_from_frequencies(dict(*word_cloud))\n\n# plt.figure(figsize=(15,10))\n# plt.imshow(wc, interpolation='bilinear')\n# plt.show()","32be3d64":"document_assembler = DocumentAssembler()\\\n                        .setInputCol(\"clean_text\")\\\n                        .setOutputCol(\"document\")\n    \n# Download the USE pretrained emebdding\nencoder = UniversalSentenceEncoder.pretrained()\\\n                     .setInputCols([\"document\"])\\\n                     .setOutputCol(\"embeddings\")\n\nclf = ClassifierDLApproach()\\\n          .setInputCols([\"embeddings\"])\\\n          .setOutputCol(\"prediction\")\\\n          .setLabelColumn(\"target\")\\\n          .setMaxEpochs(30)\\\n          .setBatchSize(32)\n\n# Create the pipeline with all the transformers above.\npipeline = Pipeline(\n    stages = [\n        document_assembler,\n        encoder,\n        clf\n    ])","fc96a838":"training, valid = train_spark.select(\"clean_text\",\"target\").randomSplit([0.7,0.3], seed=41)\n\nmodel = pipeline.fit(training)\n\npred = model.transform(valid)\npandpred = pred.select(\"target\",\"prediction.result\").toPandas()\npandpred.result = pandpred.result.apply(lambda x: x[0])\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(pandpred.target, pandpred.result.astype(int)))","ef297430":"# # Retrain with all the training data\n# model = pipeline.fit(train_spark)","388394d2":"test_p = pd.read_csv(\"..\/input\/real-or-not-data-cleaned\/test.csv\")\n\ntest_p = test_p[[\"id\",\"text\", \"clean_text\"]]\n\ntest = spark.createDataFrame(test_p)\n\n# test = test.withColumn(\"clean_text\", cleaner_udf(\"text\"))\n\n# test = test.select(\"id\",\"text\",\"clean_text\")","d3ecb6f6":"final = model.transform(test)\nfinal = final.select(\"id\",\"prediction.result\").toPandas()\n\nfinal.result = final.result.apply(lambda x: x[0]).astype(int)\nfinal.id = final.id.astype(int)","2fc7afa7":"sub = final[[\"id\",\"result\"]]\nsub.columns = [\"id\",\"target\"]\nsub.to_csv(\"submission_use.csv\", index=None)","1ba481ca":"#close the spark session when done\nspark.stop()","1943a081":"for name in dir():\n    if not name.startswith('_'):\n        del globals()[name]\n        \nimport gc\ngc.collect()","c429f5ed":"import numpy as np \nimport pandas as pd \nimport re\nfrom pycontractions import Contractions\nimport gensim.downloader as api\n\ntrain = pd.read_csv(\"..\/input\/real-or-not-data-cleaned\/train.csv\")\ntest = pd.read_csv(\"..\/input\/real-or-not-data-cleaned\/test.csv\")","98344337":"# model = api.load(\"glove-twitter-25\")\n\n# cont = Contractions(kv_model=model)\n# cont.load_models()","997d2023":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","06b75da8":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","72dc15ec":"# subtext_to_clearning = ['\\!', '\\$', '\\(', '\\)', '\\*', \n#                         '\\+', '\\-', '\\.', '\\:', '\\;', \n#                         '\\=', '\\?', '\\@', '\\[', '\\]', \n#                         '\\^', '\\|', '\\_', '\\{', '\\}']\n\n# def clean(tweet):\n#     # Remove the mentions (aka tags)\n#     tweet = re.sub(\"@[\\w]*\", \"\", tweet)    \n# #     tweet = tweet.replace(\"@\",\"\")\n\n#     # Remove more punctuations !!! -> !\n#     for punc in subtext_to_clearning:\n#         tweet = re.sub(f\"[{punc}]+\",punc.replace(\"\\\\\",\"\"),tweet)\n#     tweet = tweet.replace(\"#\",\"\").strip()\n    \n#     # Remove the whispaces\n#     tweet = \" \".join(tweet.split())\n    \n#     tweet = list(cont.expand_texts([tweet], precise=True))[0]\n    \n#     if tweet == \"\":\n#         return \".\"\n#     return tweet.lower()\n\n# train[\"clean_text\"] = train[\"text\"].apply(clean) \n# test[\"clean_text\"] = test[\"text\"].apply(clean)","957af784":"# del [model, cont]\n# gc.collect()","80b7aefc":"def tokenize_map(sentence, labs='None'):\n    \n    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n    \n    global labels\n    \n    input_ids = []\n    attention_masks = []\n\n    \n    for text in sentence:\n        \n        encoded_dict = tokenizer.encode_plus(\n                            text,                        # Sentence to encode.\n                            add_special_tokens = True,   # Add '[CLS]' and '[SEP]'\n                            truncation='longest_first',  # Activate and control truncation\n                            max_length = 64,             # Max length according to our text data.\n                            pad_to_max_length = True,    # Pad & truncate all sentences.\n                            return_attention_mask = True,# Construct attn. masks.\n                            return_tensors = 'pt',       # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the id list. \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n        labels = torch.tensor(labels)\n        return input_ids, attention_masks, labels\n    else:\n        return input_ids, attention_masks","b8fb3168":"train_sentences = train.clean_text.values\nlabels = train.target.values\n\ntest_sentences = test.clean_text.values\n\ninput_ids, attention_masks, labels = tokenize_map(train_sentences, labels)\ntest_input_ids, test_attention_masks= tokenize_map(test_sentences)","8c434178":"from torch.utils.data import TensorDataset, random_split\n\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\ntrain_size = int(0.99 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])","5a29fe6a":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = 32 # Trains with this batch size.\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = 32 # Evaluate with this batch size.\n        )","bd72a4d7":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-large-uncased',\n    num_labels = 2, \n    output_attentions = False, \n    output_hidden_states = False\n)\n\nmodel.to(device)","88c178b1":"gc.collect()","5a9d56ec":"from transformers import get_linear_schedule_with_warmup\n\nEPOCHS = 3\n\noptimizer = AdamW(model.parameters(),\n                  lr = 6e-6, \n                  eps = 1e-8 \n                )\n\ntotal_steps = len(train_dataloader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)","0513ccae":"import time \nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\nfor epoch_i in range(0, EPOCHS):\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, EPOCHS))\n    \n    t0 = time.time()\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        \n        if ((step+1) % 20 == 0 and not step == 0) or (step+1)==len(train_dataloader):\n            elapsed = format_time(time.time() - t0)\n            \n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step+1, len(train_dataloader), elapsed))\n            \n        b_input_ids = batch[0].to(device).to(torch.int64)\n        b_input_mask = batch[1].to(device).to(torch.int64)\n        b_labels = batch[2].to(device).to(torch.int64)\n        \n        model.zero_grad()\n        \n        loss, logits = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        \n        \n    model.eval()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    \n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        with torch.no_grad():  \n            (loss, logits) = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            \n        \n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU:\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    \n    avg_val_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n    print('  Validation Accuracy: {0:.3f}'.format(avg_val_accuracy))\n\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    print('  Validation Loss: {0:.3f}'.format(avg_val_loss))","29479bde":"prediction_data = TensorDataset(test_input_ids, test_attention_masks)\n\nprediction_dataloader = DataLoader(prediction_data, \n                                   sampler=SequentialSampler(prediction_data), \n                                   batch_size=64)","b2f300ae":"model.eval()\n\npredictions = []\n\nfor batch in prediction_dataloader:\n    \n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, = batch\n  \n    with torch.no_grad():\n   \n        outputs = model(b_input_ids, token_type_ids=None, \n                        attention_mask=b_input_mask)\n\n    logits = outputs[0]    \n    logits = logits.detach().cpu().numpy()\n    \n    predictions.append(logits)","625a2c2a":"flat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission['target'] = flat_predictions","c20f924a":"submission.to_csv('submission_bert.csv', index=False, header=True)","5b93d04d":"Test set loading and final predictions!","0d7f3627":"# BERT Embeddings","5885e7e5":"Need to read it with pandas, because spark was having some problems with NaNs, coundn't figure out the actual problem, so for now using pandas and after removing location and keyword converting it to spark dataframe is a work around.","4ab23073":"# Data loading and some plots","7604206b":"Code to clean the training data (same applies for test data) did it another notbook to save RAM memory space.","e912858d":"# USE Emebddings in Apache Spark","7fcc68f0":"# Installing and Importing the libraries\nWe will keep the spark 2.4.5 because spark nlp is not updated for spark 3.0.0 yet, so we need to downgrade java to java 8 because spark 2.4 doesn't support Java 11, and will cause you the following exception: `IllegalArgumentException: 'Unsupported class file major version 55'`.\n\nAnd java 8 is required for `pycontractions` package too!"}}