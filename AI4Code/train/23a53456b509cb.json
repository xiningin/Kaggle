{"cell_type":{"a5ad5dcd":"code","c444ae5b":"code","b3a8adb5":"code","0e945724":"code","1f6c1fb9":"code","6f1c3b4a":"code","d47ac796":"code","43072efb":"code","292ee315":"code","1993a887":"code","f26def3c":"code","d814dc7e":"code","e22afd30":"code","af6256ff":"code","e18fc743":"code","8aec94b2":"code","ed88e88e":"code","96987a00":"code","79f78eb2":"code","e2ed5edc":"code","f42fcd17":"code","89a93119":"code","c7b90667":"code","3a924be9":"code","d996c5e9":"code","cfb3ef93":"code","141c1de8":"code","bd4a533a":"code","d223b1d3":"code","d6d842c1":"code","e67586f7":"code","62bfcbb4":"code","a83d3835":"code","1c39b78a":"code","aff85830":"markdown","cef258da":"markdown","6c0035c4":"markdown","20017d3a":"markdown","2c75c4f7":"markdown","92da47f1":"markdown","5018f674":"markdown","f9b1efee":"markdown","e902176f":"markdown","f67ee483":"markdown","5c1aaf37":"markdown","a2df095a":"markdown","b2968046":"markdown","6bffddd4":"markdown","6608a0b0":"markdown","d2328d69":"markdown","6c483dda":"markdown","e01d8619":"markdown","71cb96c7":"markdown","067c624c":"markdown","c6611b69":"markdown"},"source":{"a5ad5dcd":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","c444ae5b":"TRAIN_PATH = os.path.join(\"..\", \"input\", \"train.csv\")\nTEST_PATH = os.path.join(\"..\", \"input\", \"test.csv\")\n\ndf_train = pd.read_csv(TRAIN_PATH,index_col=0)\ndf_test = pd.read_csv(TEST_PATH, index_col=0)","b3a8adb5":"df_train.head()","0e945724":"df_test.head()","1f6c1fb9":"df_train.info()","6f1c3b4a":"df_test.info()","d47ac796":"df_train[\"pickup_datetime\"].head()","43072efb":"df_train[\"trip_duration\"].hist(bins=100)","292ee315":"df_train[df_train.trip_duration < 10000].trip_duration.hist(bins=100)","1993a887":"# We check which partitions of the data to study\n# I think we would only take data which correspond to 95% of the data ?\n\nDURATION_MAX = 3500 # With this value we are studying 99% of the data\n\ndf_train[df_train.trip_duration < DURATION_MAX].trip_duration.hist(bins=100)\n\nprint(\"Count of values >\", DURATION_MAX, \": \", df_train[df_train.trip_duration > DURATION_MAX].trip_duration.count())\nprint(\"Count of values <=\", DURATION_MAX, \": \", df_train[df_train.trip_duration <= DURATION_MAX].trip_duration.count())\nprint(\"Lost data : \", df_train[df_train.trip_duration > DURATION_MAX].trip_duration.count() \/ df_train[df_train.trip_duration <= DURATION_MAX].trip_duration.count() * 100, \"%\")\n\ndf_train = df_train[df_train.trip_duration < DURATION_MAX]","f26def3c":"df_train.isna().sum()","d814dc7e":"df_train.duplicated().sum()","e22afd30":"df_train[df_train.duplicated()]","af6256ff":"df_train[\"pickup_hour\"] = pd.to_datetime(df_train.pickup_datetime).dt.hour\ndf_train[\"pickup_day_of_week\"] = pd.to_datetime(df_train.pickup_datetime).dt.dayofweek","e18fc743":"df_train.pickup_hour.hist(bins=47)","8aec94b2":"mean_trip_duration_by_hour = []\n\nfor i in df_train.pickup_hour.unique():\n    mean_trip_duration_by_hour.append(np.mean(df_train[df_train.pickup_hour == i].trip_duration))\n","ed88e88e":"fig, ax = plt.subplots(figsize=(22,8))\nax.scatter(x=df_train[:1000].pickup_hour, y=df_train[:1000].trip_duration)\nax.bar(df_train.pickup_hour.unique(), mean_trip_duration_by_hour)\nplt.show()","96987a00":"df_train.pickup_day_of_week.hist(bins=13)","79f78eb2":"mean_trip_duration_by_day = []\n\nfor i in df_train.pickup_day_of_week.unique():\n    mean_trip_duration_by_day.append(np.mean(df_train[df_train.pickup_day_of_week == i].trip_duration))\n","e2ed5edc":"fig, ax = plt.subplots(figsize=(22,8))\nax.scatter(x=df_train[:1000].pickup_day_of_week, y=df_train[:1000].trip_duration)\nax.bar(df_train.pickup_day_of_week.unique(), mean_trip_duration_by_day)\nplt.show()","f42fcd17":"df_test[\"pickup_hour\"] = pd.to_datetime(df_test.pickup_datetime).dt.hour\ndf_test[\"pickup_day_of_week\"] = pd.to_datetime(df_test.pickup_datetime).dt.dayofweek","89a93119":"# 1st test using only basic columns\nSELECTION = [\"pickup_longitude\", \"dropoff_longitude\", \"pickup_latitude\", \"dropoff_latitude\", \"pickup_hour\"]\nTARGET = \"trip_duration\"","c7b90667":"X_train = df_train[SELECTION]\ny_train = df_train[TARGET]\nX_test = df_test[SELECTION]","3a924be9":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error","d996c5e9":"# m1 = RandomForestRegressor(n_estimators=10)\n# m1.fit(X_train, y_train)","cfb3ef93":"m2 = RandomForestRegressor(n_estimators=15)\nm2.fit(X_train, y_train)","141c1de8":"# m3 = RandomForestRegressor(n_estimators=20)\n# m3.fit(X_train, y_train)","bd4a533a":"m4 = RandomForestRegressor(n_estimators=15, min_samples_leaf=100, min_samples_split=150)\nm4.fit(X_train, y_train)","d223b1d3":"cv_scores_1 = cross_val_score(m2, X_train, y_train, cv=5, scoring='neg_mean_squared_log_error')\ncv_scores_1","d6d842c1":"# cv_scores_2 = cross_val_score(m4, X_train, y_train, cv=5, scoring='neg_mean_squared_log_error')\n# cv_scores_2","e67586f7":"# def rmse(test, pred):\n#     return np.sqrt(mean_squared_error(test, pred))\n\ndef get_err(score):\n    err_test = []\n    for i in range(len(score)):\n        err_test.append(np.sqrt(abs(score[i])))\n    return err_test\n\nprint(np.mean(get_err(cv_scores_1)))\n# print(np.mean(get_err(cv_scores_2)))","62bfcbb4":"y_test_pred = m2.predict(X_test)\nprint(y_test_pred[:10])","a83d3835":"d = { \"id\": df_test.index, \"trip_duration\": y_test_pred}\nsubmission = pd.DataFrame(d)\nsubmission.head()","1c39b78a":"submission.to_csv(\"submission.csv\", index=0)","aff85830":"### 3.b Missing values","cef258da":"## 3. Data preprocessing","6c0035c4":"### 4.a Creation","20017d3a":"## 6. Validation method","2c75c4f7":"## Module imports","92da47f1":"About day of week :","5018f674":"## 7. Predictions","f9b1efee":"## 4 Features engineering","e902176f":"It doesn't seems to be a relevent study, I won't use it for training for the moment...","f67ee483":"## 8. Submission","5c1aaf37":"## 2. Data exploration","a2df095a":"### 3.a Outliers","b2968046":"Results :\n  \n  cross val 1 :  0.4097527612968065\n  \n  cross val 2 : 0.43580030140648873","6bffddd4":"## 5. Model selection","6608a0b0":"**id** - a unique identifier for each trip\n\n**vendor_id** - a code indicating the provider associated with the trip record\n\n**pickup_datetime** - date and time when the meter was engaged\n\n**dropoff_datetime** - date and time when the meter was disengaged\n\n**passenger_count** - the number of passengers in the vehicle (driver entered value)\n\n**pickup_longitude** - the longitude where the meter was engaged\n\n**pickup_latitude** - the latitude where the meter was engaged\n\n**dropoff_longitude** - the longitude where the meter was disengaged\n\n**dropoff_latitude** - the latitude where the meter was disengaged\n\n**store_and_fwd_flag** - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n\n**trip_duration** - duration of the trip in seconds\n\nDisclaimer: The decision was made to not remove dropoff coordinates from the dataset order to provide an expanded set of variables to use in Kernels.","d2328d69":"I did this because I wanted to select only representative data, but it will be done in a further version.","6c483dda":"We can't consider these \"duplicated values\" as needed to be removed.","e01d8619":"Cross validation (NB : This might not be necessary because of the large number of rows we're working on)","71cb96c7":"There is a large majority of the values near 0 - 10000.\n\nLet's have a look on this :","067c624c":"### 4.b Features selection","c6611b69":"## 1. Data loading"}}