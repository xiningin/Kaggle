{"cell_type":{"905fd981":"code","ffb49f4c":"code","11dfe75f":"code","64aa2fd7":"markdown","24132f27":"markdown","28885013":"markdown","b023881f":"markdown","8384a66f":"markdown","dc1a9848":"markdown","c3e5054a":"markdown","b12573f5":"markdown","cf9d6179":"markdown","6af619f7":"markdown","11521635":"markdown","981714bc":"markdown","1694ed2a":"markdown","f2fc8c45":"markdown","34e55b30":"markdown","b66e2a64":"markdown","c55d264e":"markdown","0dfca436":"markdown","d3c94c7d":"markdown","e5bd342c":"markdown","a45353e8":"markdown"},"source":{"905fd981":"# TEMEL \u0130\u015eLEMLER\n\nimport numpy as np\n\n# array olu\u015fturmak\n\nx = np.array([12,3,4,50, 40])\ny = np.array([15,5,6,51, 4])\n\n# iki arrayi birle\u015ftirme\n\nnp.concatenate([x,y])\n\n# array ay\u0131rma\n\na,b,c=np.split(x,[3,5])\n\n# array s\u0131ralama\n\nnp.sort(x)\n\n# eleman i\u015flemleri\n\na[0] = 1 # 0. indise 1 de\u011ferini atar\na[:,0] #0. S\u00fctuna eri\u015fir\na[0,:] #0. Sat\u0131ra eri\u015fir\n\n# ko\u015fullu eleman i\u015flemleri\n\nnp.sum(a > 10) #array i\u00e7inde 10 dan b\u00fcy\u00fck olan t\u00fcm say\u0131lar\u0131 say\nnp.all(a > 5) # t\u00fcm elemanlar i\u00e7in bool tipinde sonu\u00e7 d\u00f6ner\n\n# matematiksel i\u015flemler\n\nnp.add(a,3) # a'n\u0131n her elamn\u0131na 3 ekler\nnp.substract() # \u00e7\u0131karma\nnp.multiply() # \u00e7arpma\nnp.divide() # b\u00f6lme\n\nnp.mean(a) # ortalamas\u0131\nnp.add.reduce(a) #elemanlar\u0131n\u0131 topla\n\n","ffb49f4c":"# Temel \u0130\u015flemler\n\nimport pandas as pd\n\n# pandas serisi olu\u015fturma\n\nseri=pd.Series([1,2,3,4,5])\n\n# numpy arrayi \u00fczerinden seri olu\u015fturma\n\na=np.array([1,2,3,4,5])\nseri=pd.Series(a)\n\n# iki seriyi birle\u015ftirme\n\npd.concat([seri1,seri2])\n\n# serilerde eleman i\u015flemleri\n\nseri=pd.Series([121,200,150,99],index=[\"reg\",\"loj\",\"cart\",\"rf\"])\n\nseri.index\nseri.keys\nlist(seri.items)\nseri.values\n\n# dataframe olu\u015fturma\n\np=[1,2,3,4,5]\npd.DataFrame(p.columns=['degisken_ismi'])\n\n# dataframe eleman i\u015flemleri\n\ndf.drop(\u2018a\u2019,axis=0,inplace=True) # Kal\u0131c\u0131 olarak silme i\u015flemi\ndf[\u2018var\u2019] is df[\u2018var1\u2019] # ayn\u0131 m\u0131 diye sorar ve bool d\u00f6ner\n\n# listeleme\n\ndf.loc[0:3]\n\n\n","11dfe75f":"# Veri Seti \u00dczerinde Temel \u0130\u015flemler\nimport seaborn as sns\n\n# Veri Seti Yap\u0131sal Bilgileri\n\ndf.info() # veriseti bilgilerini i\u00e7erir.\ndf.dtypes() # sadece de\u011fi\u015fkenler ve de\u011fi\u015fken bilgilerini verir.\n\n# Veri Seti Betimlenmesi\n\ndf.shape() # g\u00f6zlem ve de\u011fi\u015fken say\u0131s\u0131n\u0131 verir.\ndf.columns # sadece de\u011fi\u015fken isimleri\ndf.describe() # betimsel istatistikleri verir.\n\n# Eksik De\u011ferlerin \u0130ncelenmesi\n\ndf.isnull().any() # eksik de\u011fer varsa false d\u00f6ner\ndf.isnull().sum() # hangi de\u011fi\u015fkenlerde ka\u00e7ar tane eksik var.\ndf[\"mass\"].fillna(df.mass.mean().inplace=True) # mass de\u011fi\u015fkeninin bo\u015f de\u011felerine ortalmas\u0131n\u0131 yazar.\n\n# Kategorik De\u011fi\u015fken \u00d6zetleri\n\nkat_df=df.select.dtypes(include=[\u201cobject\u201d]) # kategorik de\u011fi\u015fkenleri s\u0131ralar\nkat_df=method.unique() # kategorik s\u0131n\u0131flar\u0131 yazar.\nKat_df[\u201cmethod\u201d].value_counts().count() # methos isimli kategorik de\u011fi\u015fkenin ka\u00e7 s\u0131n\u0131f\u0131 var\n\n# S\u00fcrekli De\u011fi\u015fken \u00d6zetleri\ndf_num=df.select_dtypes(include=[\u201cfloat64\u201d,\u201dint64\u201d]) # say\u0131sal de\u011fi\u015fkenleri yazar.\ndf_num[\u201cdistance\u201d].describe().T # distance de\u011fi\u015fkeni i\u00e7in mean,std,avg gibi de\u011ferleri yazar.\n\n\n# Temel Grafik \u0130\u015flemleri\n\n# K\u00fct\u00fcphaneleri ve Verileri Ekleme:\n \ndiamonds = sns.load_dataset('diamonds')\ndf = diamonds.copy()\n\n# S\u00fctun Grafi\u011finin Olu\u015fturulmas\u0131(BarPlot):\n\ndf[\"cut\"].value_counts().plot.barh().set_title(\"Cut De\u011fi\u015fkeninin S\u0131n\u0131f Frekanslar\u0131\"); # pandas ile \nsns.barplot(x = \"cut\", y = df.cut.index, data= df) # seaborn ile\n\n# \u00c7aprazlamalar:\nsns.catplot(x = \"cut\", y = \"price\", data = df) # Renk \u00fczerinde noktalarla yo\u011funla\u015fma g\u00f6sterilir.\n\n# Histogram ve Yo\u011funluk Grafi\u011finin Olu\u015fturulmas\u0131:\nsns.distplot(df.price, kde = False) # (bins=1000 eklersek histogram hassasla\u015f\u0131r.)\n\n# Histogram ve Yo\u011funluk \u00c7aprazlamalar\u0131:\n(sns\n .FacetGrid(df,\n              hue = \"cut\",\n              height = 5,\n              xlim = (0, 10000))\n .map(sns.kdeplot, \"price\", shade= True)\n .add_legend()\n)\n\n# Kutu Grafik(BoxPlot) Olu\u015fturma:\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df) \nsns.boxplot(x = \"day\", y = \"total_bill\", hue = \"sex\", data = df)\n\n# Violin Grafi\u011fi:\n\nsns.catplot(y = \"total_bill\", kind = \"violin\", data = df)\nsns.catplot(x= \"day\", y = \"total_bill\", kind = \"violin\", data = df)\n\n# Korelasyon Grafikleri:\n\n# Scatterplot:Say\u0131sal de\u011fi\u015fkenler aras\u0131ndaki ili\u015fkiyi g\u00f6sterir.\nsns.scatterplot(x = \"total_bill\", y = \"tip\", data = df)\nsns.scatterplot(x = \"total_bill\", y = \"tip\", hue = \"time\",data = df)\nsns.scatterplot(x = \"total_bill\", y = \"tip\", hue = \"time\", style = \"time\", data = df)\n\n# Do\u011frusal \u0130li\u015fkinin G\u00f6sterilmesi:\n\nsns.lmplot(x = \"total_bill\", y = \"tip\", data = df) # scatterplot grafi\u011finin oras\u0131nda bir do\u011fru olur.\nsns.lmplot(x = \"total_bill\", y = \"tip\", hue = \"smoker\", data = df)\n\n# ScatterPlot Matrisi:\n\nSay\u0131sal de\u011fi\u015fkenler i\u00e7in bir sa\u00e7\u0131l\u0131m grafi\u011fidir.\nsns.pairplot(df);=>dataset i\u00e7indeki t\u00fcm de\u011fi\u015fkenlerin birbirleri ile olan ili\u015fkilerini g\u00f6steren grafikler verir\nsns.pairplot(df, hue = \"species\");=>species de\u011fi\u015fkenine g\u00f6re renklendirebiliriz\n\n# Is\u0131 Haritas\u0131(Heat Map):\n \n # Y\u0131l ve ay gibi de\u011fi\u015fkenler varsa, \u00e7ok s\u0131n\u0131fl\u0131 bir dataset ise daha iyi sonu\u00e7 verir. Dataframe pivot tablo olmal\u0131 yoksa \u00e7al\u0131\u015fmaz.\nsns.heatmap(df);\nsns.heatmap(df, annot = True, fmt = \"d\")\n# \u00c7izgi Grafik(line plot):\n\n# Nesnelerin interneti gibi daha zor problemlerde kullan\u0131l\u0131r.\n\nsns.lineplot(x = \"timepoint\", y = \"signal\", data = df)\n\n\n\n\n","64aa2fd7":"# Model Ba\u015far\u0131lar\u0131n\u0131n De\u011ferlendirilmesi\n\n\n# A) Regresyon Modellerinin De\u011ferlendirilmesi\n\n\n## 1.  R\u00b2\n\nE\u011fitilen modelin y veri setini ne kadar a\u00e7\u0131klad\u0131\u011f\u0131 sonucunu verir. En sade haliyle do\u011frusal regresyon modelleri i\u00e7in\nbir uygunluk \u00f6l\u00e7\u00fcm\u00fcd\u00fcr. Overfitting durumuna dikkat edildi\u011fi takdirde ne kadar y\u00fcksekse o kadar iyidir.\n\n> from sklearn.metrics import r2_score\n\n> print(r2_score(y_test, y_pred))\n\n## 2. Mean Absolute Error(MAE)\n\nTahmin edilen de\u011ferler ile ger\u00e7ek de\u011ferler aras\u0131ndaki farkt\u0131r. D\u00fc\u015f\u00fck olmas\u0131 modelin daha iyi \u00e7al\u0131\u015ft\u0131\u011f\u0131n\u0131 g\u00f6sterir.\n\n> from sklearn.metrics import mean_absolute_error\n\n> mean_absolute_error(y_test, y_pred )\n\n## 3. Mean Squared Error(MSE)\n\nT\u00fcm veri k\u00fcmesinde \u00f6rnek ba\u015f\u0131na ortalama kare kayb\u0131d\u0131r. D\u00fc\u015f\u00fck olmas\u0131 modelin daha iyi \u00e7al\u0131\u015ft\u0131\u011f\u0131n\u0131 g\u00f6sterir.\n\n> from sklearn.metrics import mean_squared_error\n\n> mean_squared_error(y_true, y_pred)\n\n# B) S\u0131n\u0131fland\u0131rma Modellerinin De\u011ferlendirilmesi\n\n\n## 1. Confusion Matrix(Kar\u0131\u015f\u0131kl\u0131k Matrisi)\n\nTahmin edilen ve ger\u00e7ek de\u011ferlerin 4 farkl\u0131 kombinasyonunu i\u00e7erir.\n\n\n> from sklearn.metrics import confusion_matrix\n> confusion_matrix(y_test, y_pred)\n\n* TP (True positive \u2014 Do\u011fru Pozitif): Hastaya hasta demek.\n* FP (False positive \u2014 Yanl\u0131\u015f Pozitif): Hasta olmayana hasta demek.\n* TN (True negative \u2014 Do\u011fru Negatif): Hasta olmayana hasta de\u011fil demek.\n* FN (False negative \u2014 Yanl\u0131\u015f Negatif): Hasta olana hasta de\u011fil demek.\n\n\n![cm](https:\/\/i.ibb.co\/3rHT5Tn\/cmm.png)\n\n* Do\u011fruluk(Accuracy) = Do\u011fru olarak s\u0131n\u0131fland\u0131r\u0131lan \u00f6rneklerin y\u00fczdesidir.\n\n> from sklearn.metrics import accuracy_score\n\n> accuracy_score(y_test, y_pred)\n\n* Duyarl\u0131l\u0131k(Recall) = \n\n> from sklearn.metrics import recall_score\n> recall_score(y_test, y_pred)\n\n![recall](https:\/\/i.ibb.co\/HVHZb0m\/recall.png)\n\n* Kesinlik(Precision) =\n\n> from sklearn.metrics import recall_score\n\n>recall_score(y_test, y_pred)\n\n![precision](https:\/\/i.ibb.co\/Px857vg\/precison.png)\n\n* F1 Score = Modelimizin kesinli\u011finin bir \u00f6l\u00e7\u00fcs\u00fcd\u00fcr. 1(en iyi durum) ile 0 aras\u0131nda de\u011fer alabilir.\n\n![f1](https:\/\/i.ibb.co\/T1QD8Hg\/f1.png)\n\n> from sklearn.metrics import f1_score\n> f1_score(y_true, y_pred)\n\n**De\u011ferlendirme**\n \n A\u015fa\u011f\u0131daki confusion matrix grafi\u011finde 13, 10 ve 9 de\u011ferleri do\u011fru tahminlerimizin say\u0131lar\u0131n\u0131 vermektedir.\n\n![cm2](https:\/\/i.ibb.co\/XCs7wB6\/cm.png)\n\n\n\n\n## 2. AUC-ROC E\u011frisi\n\nROC bir olas\u0131l\u0131k e\u011frisidir ve AUC ayr\u0131labilirli\u011fin derecesini veya \u00f6l\u00e7\u00fcs\u00fcn\u00fc temsil eder. AUC, ROC e\u011frisinin alt\u0131nda kalan aland\u0131r. Modelleri s\u0131n\u0131flar aras\u0131nda ne kadar ay\u0131rt edebildi\u011fini anlat\u0131r.\n\n![auc-roc](https:\/\/i.ibb.co\/4mC6S0c\/ROC-image.png)\n","24132f27":"<a id=\"3\"><\/a> <br>\n## 4. Veri \u0130ndirgeme\n\n\u015eu ba\u015fl\u0131klar alt\u0131nda toplan\u0131rlar:\n\n* \u00d6znitelik\/Boyut say\u0131s\u0131n\u0131n azalt\u0131lmas\u0131 (dimensionality reduction)\n* Nesne\/ G\u00f6zlem say\u0131s\u0131n\u0131n azalt\u0131lmas\u0131 (numerosity reduction)\n* \u00c7e\u015fitli s\u0131k\u0131\u015ft\u0131rma teknikleri ile veri hacminin azalt\u0131lmas\u0131 (data reduction)\n","28885013":"\n# G\u00f6zetimli \u00d6\u011frenme\n\nG\u00f6zetimli(supervised) \u00f6\u011frenme ile etiketli(labeled) veriyi e\u011fitiriz. Verisetindeki \u00f6zellikler i\u00e7in belirlenmi\u015f \u00e7\u0131k\u0131\u015flar vard\u0131r.\n\n# A) Regresyon Algoritmalar\u0131\n\n## Lineer Regresyon(Linear Regression)\n\nVeri boyutundan ba\u011f\u0131ms\u0131z olarak do\u011frusal ili\u015fki \u00fczerine kurulur. Fakat do\u011frusal bir ili\u015fki oldu\u011funu kabul etmek \u00e7o\u011fu zaman iyi sonu\u00e7lar vermeyecektir.\n\n> Y = label, X = label hari\u00e7 di\u011fer \u00f6znitelikler:\n\n> from sklearn.linear_model import LinearRegression\n\n> lin_reg = LinearRegression()\n\n> lin_reg.fit(X,Y)\n\n## Polinomsal Regresyon(Polynomial Regression)\n\nDo\u011frusal olamayan problemlerde kullan\u0131l\u0131r ve y\u00fcksek ba\u015far\u0131 i\u00e7in polinom derecesi \u00f6nemlidir.\n\n> from sklearn.preprocessing import PolynomialFeatures\n\n> poly_reg = PolynomialFeatures(degree = 2)\n\n> lin_reg2.fit(X,Y)\n\n## Support Vector Regression(SVR)\n\nDo\u011frusal olmayan problemlerde \u00e7al\u0131\u015f\u0131r. \u00d6l\u00e7ekleme ve do\u011fru kernel fonksiyonu se\u00e7imi yapmak \u00f6nemlidir.\n\n> from sklearn.svm import SVR\n\n> svr_reg = SVR(kernel='rbf')\n\n> svr_reg.fit(X,Y)\n\n## Decision Tree Regression\n\nDo\u011frusal veya do\u011frusal olmayan problemlerde de \u00e7al\u0131\u015f\u0131r. K\u00dc\u00e7\u00fck veri setleri \u00fczerinde\noverfitting olabilir. Karar a\u011fa\u00e7lar\u0131n\u0131n \u00f6\u011freniminde iki ana hedef s\u00f6z konusudur. S\u0131\u0131n\u0131fland\u0131rma a\u011fa\u00e7lar\u0131 olarak isimlendirilen birinc grupta, veri dizisinin olabildi\u011fince homojen olarak s\u0131n\u0131fland\u0131r\u0131lmas\u0131; regresyon a\u011fa\u00e7lar\u0131 olarak isimlendirilen ikinci grupta ise, tahmin modellerinin kurulmas\u0131 hedeflenmektedir.\n\n* Anla\u015f\u0131lmas\u0131 ve yorumlanma\u0131 son derece kolayd\u0131r.\n* Di\u011fer y\u00f6ntemler verinin normal da\u011f\u0131lmas\u0131na veya eksik de\u011ferlere kar\u015f\u0131 son derece hassasken, karar a\u011fac\u0131 \u00f6\u011frenimi modellerinde kullan\u0131lan veri fazla \u00f6n i\u015fleme gerek duymadan kullan\u0131labilir.\n* Hem s\u00fcrekli hem de kategorik de\u011ferleri bir arada veya ayr\u0131 ayr\u0131 kullanabilen \u00e7e\u015fitli karar a\u011fac\u0131 \u00f6\u011frenimi algoritmalar\u0131 geli\u015ftirilmi\u015ftir.\n* \u00c7e\u015fitli istatistik testeler kullan\u0131larak, bir modelin g\u00fcvenilirli\u011fi s\u0131nanabilir.\n* \u00d6l\u00e7eklenebilir \u00f6zelli\u011fi ile veri dizilerinde de kabul edilebilir s\u00fcreler i\u00e7erisinde sonu\u00e7lar\u0131n elde edilmesini sa\u011flar.\n* Ezbere \u00f6\u011frenme yo\u011fun oldu\u011fu i\u00e7in budama y\u00f6ntemlerinin etkin kullan\u0131m\u0131 gerekir.\n\n\n> from sklearn.tree import DecisionTreeRegressor\n\n> r_dt = DecisionTreeRegressor(random_state=0)\n\n> r_dt.fit(X,Y)\n\n## Random Forest\n\n\u00d6l\u00e7eklemeye ihtiya\u00e7 duymaz. Do\u011frusal veya do\u011frusal olmayan problemlerde de \u00e7al\u0131\u015f\u0131r. Overfitting riski d\u00fc\u015f\u00fckt\u00fcr. \u00c7\u0131kt\u0131lar\u0131n\nyorumu ve g\u00f6rselle\u015ftirmesi nispeten zordur.\n\n> from sklearn.ensemble import RandomForestRegressor\n> rf_reg=RandomForestRegressor(n_estimators = 10,random_state=0)\n> rf_reg.fit(X,Y)\n\n\n# B) S\u0131n\u0131fland\u0131rma Algoritmalar\u0131\n\n### **Train Test Split**\n\nMakine \u00f6\u011frenmesinin ba\u015far\u0131s\u0131n\u0131 test etmek amac\u0131yla veri setini e\u011fitim ve test olarak iki par\u00e7aya b\u00f6leriz.\n\n> from sklearn.model_selection import train_test_split\n\n> X_train, X_test,y_train,y_test = train_test_split(X,Y,test_size=0.33, random_state=0)\n\n## Logistic Regression\n\nLineer regresyon analizinin \u00f6nemli yap\u0131 ta\u015flar\u0131ndan biri ba\u011f\u0131ml\u0131 ve ba\u011f\u0131ms\u0131z de\u011fi\u015fkenlerin s\u00fcrekli de\u011ferler almas\u0131d\u0131r. Ancak g\u00fcnl\u00fck bir\u00e7ok uygulamada ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin sadece iki m\u00fcmk\u00fcn de\u011fer almas\u0131 beklenir. Bu tip durumlarda kullan\u0131lacak y\u00f6ntemlerden birisi ikil lojistik regresyondur. Bernoulli Da\u011f\u0131l\u0131m\u0131 olarak isimlendirilen kesikli da\u011f\u0131l\u0131m, bir olay\u0131n var\/yok, \u00f6l\u00fc\/sa\u011f gibi sadece iki m\u00fcmk\u00fcn de\u011fer alabilmesini inceler. Lojistik fonksiyonun en \u00f6nemli \u00f6zelli\u011fi  -\u221e\/+\u221e aral\u0131\u011f\u0131nda her t\u00fcrl\u00fc de\u011fer girdi olarak kabul etmesi ve \u00e7\u0131kt\u0131s\u0131n\u0131n olas\u0131l\u0131k olarak yorumlanabilecek 0 ile 1 aras\u0131nda de\u011fer \u00fcretmesidir.\n\n\n\n> from sklearn.linear_model import LogisticRegression\n\n> logr = LogisticRegression(random_state=0)\n\n> logr.fit(X_train,y_train)\n\n## K-NN\n\nBu algoritmada s\u0131n\u0131fland\u0131r\u0131lmak istenen nesne, \u00f6znitelik de\u011ferlerinde g\u00f6re kendisine en yak\u0131n kom\u015fu veya kom\u015fular\u0131n s\u0131n\u0131f\u0131na atan\u0131r. \u0130lk olarak yeni tan\u0131mlanan X t\u00fcr\u00fcn\u00fcn di\u011fer \u00f6rneklere olan Euclid uzakl\u0131klar\u0131 hesaplan\u0131r, hesaplanan Euclid uzakl\u0131klar\u0131 k\u00fc\u00e7\u00fckten b\u00fcy\u00fc\u011fe do\u011fru s\u0131ralan\u0131r.\nKom\u015fular uzakla\u015ft\u0131k\u00e7a s\u0131n\u0131fland\u0131rmaya etkilerinin de azalt\u0131lmas\u0131 amac\u0131 ile uygulanan bir yakla\u015f\u0131m, kom\u015fular\u0131n etkilerinin uzakl\u0131klar\u0131na g\u00f6re a\u011f\u0131rland\u0131r\u0131lmas\u0131d\u0131r. Bu i\u015flem i\u00e7in d uzakl\u0131k w a\u011f\u0131rl\u0131k de\u011feri olmak \u00fczere w = 1\/ e\u015fitli\u011fi kullan\u0131labilir.\n\n> from sklearn.neighbors import KNeighborsClassifier\n\n> knn = KNeighborsClassifier(n_neighbors=1, metric='minkowski')\n\n> knn.fit(X_train,y_train)\n\n## Kernel SVM\n\nDo\u011frusal olmayan problemlerde y\u00fcksek perfonmasl\u0131d\u0131r. Overfitting durumuna kar\u015f\u0131 duyarl\u0131 de\u011fildir. Outlier de\u011ferlere kar\u015f\u0131 hassas de\u011fildir.\n\n> from sklearn.svm import SVC\n\n> svc = SVC(kernel='rbf')\n\n> svc.fit(X_train,y_train)\n\n## Naive Bayes\n\nVerimli, outlier de\u011ferlere kar\u015f\u0131 hassas de\u011fil, do\u011frusal olmayan problemler \u00fczerinde \u00e7l\u0131\u015f\u0131r ve olas\u0131l\u0131ksal bir yakla\u015f\u0131md\u0131r. \u00d6zelliklerin ayn\u0131 istatiksel\nanlaml\u0131l\u0131\u011fa sahip oldu\u011fu varsay\u0131m\u0131n\u0131 kabul eder.\n\n> from sklearn.naive_bayes import GaussianNB\n\n> gnb = GaussianNB()\n\n> gnb.fit(X_train, y_train)\n\n## Decision Tree Classification\n\n> from sklearn.tree import DecisionTreeClassifier\n\n> dtc = DecisionTreeClassifier(criterion = 'entropy')\n\n>dtc.fit(X_train,y_train)\n\n\n### **Model Predict**\n\nYukar\u0131da kullan\u0131lan modeller i\u00e7in X_test veri seti \u00fczerinde predict yaparak sonu\u00e7lar\u0131 de\u011ferlendirebiliriz.\n\n> y_pred = dtc.predict(X_test)\n\n\n\n","b023881f":"## 3. SEABORN\n\nKlasik veri format\u0131 \u00fczerinde veri g\u00f6rselle\u015ftirmesi yaparak daha anla\u015f\u0131l\u0131r hale getirmek gerekir. Pyhton\u2019da veri g\u00f6rselle\u015ftirmesi yaparken genelde Matplotlib k\u00fct\u00fcphanesi kullan\u0131lmaktad\u0131r. Seaborn ise alternatif olarak kullan\u0131lan daha az kod yazarak daha g\u00fczel grafikler ortaya \u00e7\u0131karman\u0131z\u0131 sa\u011flayan bir k\u00fct\u00fcphanedir. Matplotlib k\u00fct\u00fcphanesine y\u00fcksek seviye ara y\u00fcz sa\u011flayan bir k\u00fct\u00fcphanedir. Seaborn ile;\n\n* Estetik olarak ho\u015f olan varsay\u0131lan temalar\u0131 kullanma\n* \u00d6zel olarak renk paleti belirleme\n* G\u00f6z al\u0131c\u0131 istatistiksel grafikler yapma\n* Da\u011f\u0131l\u0131mlar\u0131 kolayca ve esnek bir \u015fekilde sergileme\n* Matris ve DataFrame i\u00e7indeki bilgileri g\u00f6rselle\u015ftirme yap\u0131labilir.\n\nSeaborn, ke\u015fif\u00e7i analiz i\u00e7in kullan\u0131lmaktad\u0131r. Veri hakk\u0131nda h\u0131zl\u0131 bir \u015fekilde bilgi sahibi olmam\u0131z\u0131 sa\u011flar.\n","8384a66f":"<a id=\"1\"><\/a> <br>\n# 2. Veri Temizleme\n\n","dc1a9848":"### Veri Madencili\u011fi S\u00fcreci\n\n![crisp](https:\/\/i.ibb.co\/F0GyDzK\/crisp-dm.png)","c3e5054a":"## 1. Eksik Veri G\u00f6zlemi(Mising Values)\n\nEksik veri(missing value\/data) bir veya daha fazla g\u00f6zlem\/\u00f6znitelik de\u011ferinin, sistematik olmayan bir veri giri\u015f hatas\u0131 veya cevap vericinin \u00f6zellikle cevap vermemesi gibi nedenlerle veri dizisinde eksik kalmas\u0131d\u0131r. \n\n*Eksik veriyi saptama:*\n\n> df.isnull().sum()\n\n*Eksik verileri silme*\n\n> df.dropna(axis = 1, how = \"all\", inplace = True)\n\n*Eksik verilere de\u011fer atama*\n\n> df[\"V1\"].fillna(df[\"V1\"].mean())\n\n> df[\"V3\"].fillna(df[\"V3\"].median())\n\n\n","b12573f5":"### Veri Kalitesi\n\nVerileri incelerken ortaya \u00e7\u0131kan hatalar\u0131 iki farkl\u0131 kategoride inceleyebiliriz. \u00d6l\u00e7\u00fcm cihaz\u0131n\u0131n hatal\u0131 kalibre edilmesi nedeni ile ortaya \u00e7\u0131kan hatalar **sistematik** hatad\u0131r. \u00d6l\u00e7\u00fcm s\u0131ras\u0131nda tesad\u00fcfi olarak \u00f6l\u00e7\u00fcm\u00fcn bozulmas\u0131na neden olan etkenler sonucunda ortaya \u00e7\u0131kan hatalar ise **tesad\u00fcfi** hatad\u0131r.\n","cf9d6179":"## 2. PANDAS\n\nVeri manip\u00fclasyonu ve veri analizi i\u00e7in yaz\u0131lm\u0131\u015f a\u00e7\u0131k kaynakl\u0131 bir Python k\u00fct\u00fcphanesidir. Veri i\u015fleme ve temizleme i\u015flerini yapar. Makine \u00f6\u011freniminde yayg\u0131n olarak kullan\u0131l\u0131r. Ekonomik ve finansal \u00e7al\u0131\u015fmalar i\u00e7in kullan\u0131l\u0131r. Numphy k\u00fct\u00fcphanesinin \u00f6zelliklerini kullan\u0131r ve geni\u015fletir. Pandas kendi i\u00e7inde seriler ve veri \u00e7er\u00e7eveleri(dataframe) olarak ikiye ayr\u0131l\u0131r.\n","6af619f7":"### \u00d6L\u00c7\u00dc SKALASI\n\n**1. Kategorik De\u011fi\u015fken**\n*  Nominal(\u0130simsel) : De\u011ferler k\u00fc\u00e7\u00fckl\u00fck b\u00fcy\u00fckl\u00fck bak\u0131m\u0131ndan s\u0131ralanam\u0131yorsa, herhangi bir s\u0131n\u0131fland\u0131rma yap\u0131lm\u0131yorsa nominaldir.\n*  Ordinal(S\u0131ral\u0131) : De\u011ferler b\u00fcy\u00fckl\u00fck k\u00fc\u00e7\u00fckl\u00fck skalas\u0131na g\u00f6re s\u0131ralanabilir.\n\n**2. S\u00fcrekli De\u011fi\u015fken**\n*  Interval  (Aral\u0131kl\u0131) : Kullan\u0131lan say\u0131lar aras\u0131ndaki uzakl\u0131klar \u00f6l\u00e7\u00fclebilmektedir. De\u011ferler i\u00e7in bir 0 noktas\u0131 bulunmamaktad\u0131r.\n*  Ratio (Oransal) : Do\u011fal bir 0 noktas\u0131 bulunmaktad\u0131r. Uzunluk ve a\u011f\u0131rl\u0131k \u00f6l\u00e7\u00fcleri en temel \u00f6rne\u011fidir.\n","11521635":"# G\u00f6zetimsiz \u00d6\u011frenme\n\nVeri setin de bir etiket(label) bulunmaz.\n\n\n# B\u00f6l\u00fctleme\/K\u00fcmeleme(Clustering) Y\u00f6ntemleri\n\nK\u00fcme analizi(cluster analysis) veya k\u00fcmeleme(clustering) en basit tan\u0131m\u0131 ile veri dizisinde yer alan nesnelerin ayn\u0131 gruplarda yer alacak \u015fekilde ayr\u0131\u015ft\u0131r\u0131lmas\u0131d\u0131r. K\u00fcme analizinde, ayn\u0131 k\u00fcme i\u00e7erisinde yer alan nesnelerin olabildi\u011fince birbirleri le ba\u011fda\u015f\u0131k(homojen), farkl\u0131 k\u00fcmelerde yer alan nesnelerle ise olabildi\u011fince ayr\u0131\u015f\u0131k(heterojen) olmas\u0131 hedeflenir. \u00d6zellikle makine \u00f6\u011frenimi kapsam\u0131nda geli\u015ftirilen k\u00fcmeleme algoritmalar\u0131, \u00f6r\u00fcnt\u00fc tan\u0131ma(pattern recognition), konu\u015fma tan\u0131ma(speech recognition), g\u00f6r\u00fcnt\u00fc i\u015flemi(image processing), g\u00f6r\u00fcnt\u00fc i\u015flemede vekt\u00f6r nicemleme(vector quantization) olarak da bilinen veri s\u0131k\u0131\u015ft\u0131rmada kullan\u0131lan spesifik uygulamalarda, t\u0131bbi g\u00f6r\u00fcnt\u00fcleme, su\u00e7(crime) ve sosyal a\u011f(social networks) analizlerinde rol oynamaktad\u0131r.\n\n## K-MEANS\n\n\u0130lk olarak kullan\u0131c\u0131 taraf\u0131ndan k\u00fcme say\u0131s\u0131 k de\u011ferinin belirlenmesi gerekmektedir. K de\u011ferinin 2 olarak belirlenmesi durumunda keyfi olarak veya algoritmada yer alan kurallara g\u00f6re ba\u015flang\u0131\u00e7 k\u00fcme merkezleri(cluster centroid) se\u00e7ilir. K-means algoritmas\u0131nda t\u00fcm nesnelerin C1 ve C2 k\u00fcme merkezlerine olan uzakl\u0131klar\u0131 kareli Euclid uzakl\u0131k \u00f6l\u00e7\u00fcs\u00fc kullan\u0131larak hesaplan\u0131r. Hesaplanan bu uzakl\u0131k de\u011ferine g\u00f6re hangi nesnenin hangi k\u00fcmede yer alaca\u011f\u0131 kolayca belirlenebilir. Bu s\u00fcre\u00e7 tan\u0131mlanan maksimum iterasyon say\u0131s\u0131na var\u0131ncaya kadar veya merkez de\u011fi\u015fikleri tan\u0131mlanan yak\u0131nsama kriterinden daha k\u00fc\u00e7\u00fck oluncuya kadar s\u00fcrd\u00fcr\u00fcl\u00fcr.\n\n> from sklearn.cluster import KMeans\n\n> kmeans = KMeans ( n_clusters = 3, init = 'k-means++')\n\n> kmeans.fit(X)\n\n## Hiyerar\u015fik K\u00fcmeleme\n\nHiyerar\u015fik k\u00fcme analizinin ana \u00e7al\u0131\u015fma prensibi, benzer \u00f6znitelik de\u011ferlerine sahip olan nesnelerin ad\u0131m ad\u0131m bir araya getirilmesi veya tam tersine b\u00fct\u00fcnden ad\u0131m ad\u0131m ayr\u0131lmas\u0131d\u0131r.\n\n> from sklearn.cluster import AgglomerativeClustering\n> ac = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n> Y_tahmin = ac.fit_predict(X)\n","981714bc":"# Model Se\u00e7imi\n\n\n## Ensemble Learning\n\n\n Makine \u00f6\u011freniminde ensemble \u00f6\u011frenim(ensemble learning), elde edilecek sonu\u00e7lar\u0131n performans\u0131n\u0131n artt\u0131r\u0131lmas\u0131 amac\u0131 ile farkl\u0131 \u00f6\u011frenme algoritmalar\u0131n\u0131n bir arada kullan\u0131lmas\u0131d\u0131r. Sonra bu modeller oylan\u0131r.Ensemble \u00f6\u011frenimde de bagging, boosting, stacking, error-correcting, output codes gibi \u00e7ok farkl\u0131 yakla\u015f\u0131mlar geli\u015ftirilmi\u015ftir.Bagging ve bootstrap aggregation, karar a\u011fa\u00e7lar\u0131 \u00f6\u011freniminde ilk d\u00f6nem ensemble y\u00f6ntemleridir. \u00d6\u011frenim verisinin varyans\u0131n\u0131n azalt\u0131lmas\u0131 ve ezbere \u00f6\u011frenme sorununu gidermek amac\u0131 ile \u00e7ok say\u0131da \u00f6rnekleme yap\u0131larak, bunlardan \u00fcretilen karar a\u011fa\u00e7lar\u0131ndan birinin oyalama sonucunda se\u00e7ilmesi s\u00fcrecidir.Ensemble \u00f6\u011frenimi h\u0131zland\u0131rabilmek i\u00e7in paralel hesaplama(parallel computing) kapsam\u0131nda paralel CPU kullan\u0131labilir.\nBoosting ise bagging yakla\u015f\u0131m\u0131n\u0131n bir ad\u0131m daha geli\u015ftirilmi\u015f halidir. Burada her bir modele verilen \u00f6neme g\u00f6re a\u011f\u0131rl\u0131k de\u011ferleri atan\u0131r. Boosting yakla\u015f\u0131m\u0131n\u0131 kullanan y\u00f6ntemlerden biri AdaBoost algoritmas\u0131d\u0131r.\n\n\n## K-Fold Cross Validation(K-Katlamal\u0131 \u00c7apraz Do\u011frulama)\n\n S\u0131n\u0131fland\u0131rma modellerinin de\u011ferlendirilmesi ve modelin e\u011fitilmesi i\u00e7in veri setini par\u00e7alara ay\u0131rma y\u00f6ntemlerinden biridir. Veri seti e\u011fitim-test olarak ikiye ayr\u0131l\u0131r. E\u011fitim seti k adet alt par\u00e7aya ay\u0131l\u0131r. Bir par\u00e7a d\u0131\u015far\u0131da b\u0131rak\u0131l\u0131r ve di\u011fer par\u00e7alar\u0131n do\u011frulu\u011fu d\u0131\u015far\u0131da b\u0131rak\u0131lan par\u00e7a ile test edilir. Her bir iterasyonda ba\u015fka bir par\u00e7a d\u0131\u015far\u0131da b\u0131rak\u0131l\u0131r. Bu hatalar\u0131n ortalamas\u0131 al\u0131nd\u0131\u011f\u0131nda ortaya validasyon hatas\u0131 \u00e7\u0131kar. B\u00fcy\u00fck hacimli veri setlerinde hesaplama ve zaman y\u00f6n\u00fcnden maliyetli olabilir.\n \n> from sklearn.model_selection import cross_val_score\n\n> basari = cross_val_score(X_train, y_train, cv = 5)\n\n> basari.mean()\n\n> basari.std()\n\nCross Validation ile modelimizin sa\u011flaml\u0131\u011f\u0131n\u0131 test etmi\u015f oluruz. Overfitting gibi istenmeyen bir durumla kar\u015f\u0131 kar\u015f\u0131ya olup olmad\u0131\u011f\u0131m\u0131z\u0131 cross validation i\u015flemi sonucunda elde edilen sonu\u00e7lar\u0131n accuracy gibi metriclerine bakarak anlayabiliriz.\n\n\n\n![k-fold](https:\/\/i.ibb.co\/xfRVP8N\/kfold.pngg)\n\n\n## Grid Search\n\nKullanmak istedi\u011fimiz modelin en iyi parametrelerini bulur. Yani modeli optimize etmeye yarar.\n\n> from sklearn.model_selection import GridSearchCV\n\n\n> gs = GridSearchCV(classffier = classifier(e\u011fitti\u011fimiz model)\n                    param_grid = p,\n                  scoring =  'accuracy',\n                  cv = 10,(cross-validation = ka\u00e7 katlamal\u0131)\n                  n_jobs = -1)\n\n\n> grid_search = gs.fit(X_train,y_train)\n\n> eniyisonuc = grid_search.best_score_\n\n> eniyiparametreler = grid_search.best_params_\n\n\n","1694ed2a":"# BIAS - VARIANCE (YANLILIK VE VARYANS)\n\nModelimizi e\u011fittikten sonra e\u011fitim ve test hatas\u0131 aras\u0131ndaki duruma g\u00f6re modelimizde bias veya variance durumunu anlayabiliriz. Y\u00fcksek yanl\u0131l\u0131k(high bias) overfitting oldu\u011funu g\u00f6sterir. Y\u00fcksek varyans(high variance) ise underfitting(\u00f6\u011frenememe) oldu\u011funu g\u00f6sterir.\n\n![bias](https:\/\/i.ibb.co\/ftCjBRW\/bias.png)\n\n\n* E\u011fitim hatas\u0131 %1 ve test hatas\u0131 %11 oldu\u011funu varsayarsak, y\u00fcksek varyans oldu\u011fu anla\u015f\u0131l\u0131r. Bu durumda modelimiz ezberleme(overfitting) yapm\u0131\u015ft\u0131r.\n\n* E\u011fitim hatas\u0131 %15 ve test hatas\u0131 %30 ise e\u011fitim ve test sonu\u00e7lar\u0131da iyi de\u011fil demektir. B\u00f6yle bir durumda y\u00fcksek yanl\u0131l\u0131k ve y\u00fcksek varyans vard\u0131r.\n\n* E\u011fitim hatas\u0131 %15 ve test hatas\u0131 %16 oldu\u011fu durumda, e\u011fitim setinin bile verileri s\u0131n\u0131fland\u0131rmada yetersiz kald\u0131\u011f\u0131 g\u00f6r\u00fcn\u00fcr. Bu durum da y\u00fcksek yanl\u0131l\u0131k vard\u0131r.\n\n* E\u011fitim hatas\u0131 %0.5 ve test hatas\u0131 %1 ise d\u00fc\u015f\u00fck yanl\u0131l\u0131k ve d\u00fc\u015f\u00fck varyans var demektir. Bu istenilen bir sonu\u00e7tur.\n\n\n#### **Bias ve Variance sorunlar\u0131n\u0131 \u00e7\u00f6zmek i\u00e7in tavsiyeler:**\n\n1. High Bias:\n\n Derin \u00f6\u011frenmede bu durumla kar\u015f\u0131la\u015f\u0131yorsak daha b\u00fcy\u00fck bir a\u011f sorunumuzu \u00e7\u00f6zecektir. Klasik makine \u00f6\u011frenmesi algoritmalar\u0131nda ise model parametrelerini optimize etmek gerekir.\n \n2. High Variance:\n\n Daha fazla veri kullanarak bu sorunu \u00e7\u00f6zebiliriz.","f2fc8c45":"# Makine \u00d6\u011frenmesi Algoritmalar\u0131\n\nMakine \u00f6\u011frenmesi algoritmalar\u0131n\u0131 supervised(g\u00f6zetimli) ve unsupervised(g\u00f6zetimsiz) olarak ikiye ay\u0131r\u0131r\u0131z.\n ","34e55b30":"## 2. Ayk\u0131r\u0131 Veri G\u00f6zlemi (Outliers)\n\n  Outlier, en k\u00fc\u00e7\u00fck ve en b\u00fcy\u00fck de\u011ferleri de i\u00e7ermekle birlikte, bir veri dizisinde normal beklentilerin d\u0131\u015f\u0131nda yer alan, sahip olduklar\u0131 tekil \u00f6zellikleri ile di\u011fer nesnelerden a\u00e7\u0131k\u00e7a farkl\u0131l\u0131k g\u00f6steren s\u0131ra d\u0131\u015f\u0131 nesnelerdir. Ayk\u0131r\u0131 g\u00f6zlem analizi, g\u00fcn\u00fcm\u00fczde kredi kart\u0131 doland\u0131r\u0131c\u0131l\u0131\u011f\u0131ndan bilgisayar a\u011f\u0131 ihlallerine, pazarlamada a\u015f\u0131r\u0131 d\u00fc\u015f\u00fck veya y\u00fcksek gelirleri olan m\u00fc\u015fterilerin sat\u0131n alma davran\u0131\u015flar\u0131n\u0131n ke\u015ffedilmesine kadar \u00e7e\u015fitli alanlarda ba\u015far\u0131l\u0131 sonu\u00e7lar al\u0131nmas\u0131n\u0131 sa\u011flamaktad\u0131r.\n  \n  \n  ![outlier](https:\/\/i.ibb.co\/X538DYT\/outlier-scatterplot.png)\n  \n\n#### *Outlier saptama:*\n\n* Sekt\u00f6r bilgisi ile anla\u015f\u0131labilir.\n* Bir de\u011fi\u015fkenin ortalamas\u0131n\u0131n \u00fczerine ayn\u0131 de\u011fi\u015fkenin standart sapmas\u0131 hesaplanarak eklenir. 1,2 ya da 3 standart sapma de\u011feri ortalama \u00fczerine eklenerek ortaya \u00e7\u0131kan bu de\u011fer e\u015fik de\u011feri olarak d\u00fc\u015f\u00fcn\u00fcl\u00fcr. Bu de\u011ferden yukar\u0131da ya da a\u015fa\u011f\u0131da olan de\u011ferler ayk\u0131r\u0131 g\u00f6zlem olarak tan\u0131mlan\u0131r.\n* Z-Skoru Yakla\u015f\u0131m\u0131 : De\u011fi\u015fken standartla\u015ft\u0131r\u0131l\u0131r. \u00d6rnek olarak da\u011f\u0131l\u0131m\u0131n sa\u011f\u0131ndan ve solundan -+2,5 de\u011ferine bir e\u015fik de\u011feri konulur. Bu de\u011ferin \u00fczerinde ya da alt\u0131nda olanlar ayk\u0131r\u0131 de\u011fer olur.\n* Boxplot Y\u00f6ntemi : De\u011fi\u015fkenin de\u011ferleri k\u00fc\u00e7\u00fckten b\u00fcy\u00fc\u011fe do\u011fru s\u0131ralan\u0131r. \u00c7eyrekliklerin(y\u00fczdeliklerine) yani Q1,Q3 de\u011ferlerine kar\u015f\u0131l\u0131k de\u011ferler \u00fczerinden bir e\u015fik de\u011fer hesaplan\u0131r. Bu de\u011fere g\u00f6re ayk\u0131d\u0131 de\u011fer tan\u0131m\u0131 yap\u0131l\u0131r.\n\n\n> Q1 = df_table.quantile(0.25)\n\n> Q3 = df_table.quantile(0.75)\n\n> IQR = Q3-Q1\n\n> alt_sinir = Q1- 1.5*IQR\n\n> alt_sinir = Q1- 1.5*IQR\n\n![outliers](https:\/\/i.ibb.co\/LkJvZRM\/abv.png)\n\n\n\n#### *Outlier sorununu \u00e7\u00f6zmek:*\n\n* Silme Yakla\u015f\u0131m\u0131:\n\n> t_df = df_table[~((df_table < (alt_sinir)) | (df_table > (ust_sinir))).any(axis = 1)]\n\n* Ortalama ile Doldurma:\n\n> df_table[aykiri_tf] = df_table.mean()\n\n* Bask\u0131lama Y\u00f6ntemi:\n\n> df_table[aykiri_tf] = alt_sinir\n\n\n#### *\u00c7ok De\u011fi\u015fkenli Ayk\u0131r\u0131 G\u00f6zlem Analizi -Local Outlier Factor(LOF)-* \n*Yo\u011fuluk Temelli Yakla\u015f\u0131m:*\n\n \u0130ki de\u011fi\u015fken tek ba\u015f\u0131nayken ayk\u0131r\u0131 de\u011fer olarak g\u00f6r\u00fclm\u00fcyorken, iki g\u00f6zlem e\u015f zamanl\u0131 olarak de\u011ferlendirildi\u011finde ayk\u0131r\u0131 de\u011ferler olabilir.G\u00f6zlemleri bulunduklar\u0131 konumda yo\u011funluk tabanl\u0131 skorlayarak buna g\u00f6re ayk\u0131r\u0131 de\u011fer olabilecek de\u011ferleri tan\u0131mlayabilmemize imkan sa\u011fl\u0131yor. Bir noktan\u0131n local yo\u011funlu\u011fu bu noktan\u0131n kom\u015fular\u0131 ile kar\u015f\u0131la\u015ft\u0131r\u0131l\u0131yor. E\u011fer bir nokta kom\u015fular\u0131n\u0131n\u0131n yo\u011funlu\u011fundan anlaml\u0131 \u015fekilde d\u00fc\u015f\u00fck ise bu nokta kom\u015fular\u0131ndan daha seyrek bir b\u00f6lgede bulunuyordur yorumu yap\u0131labiliyor. Dolay\u0131s\u0131yla burada bir kom\u015fuluk yap\u0131s\u0131 s\u00f6z konusu. Bir de\u011ferin \u00e7evresi yo\u011fun de\u011filse demek ki bu de\u011fer ayk\u0131r\u0131 de\u011ferdir \u015feklinde de\u011ferlendiriliyor. \n\n","b66e2a64":"Data transformation, daha sa\u011fl\u0131kl\u0131 sonu\u00e7lar\u0131n elde edilmesi veya verinin kullan\u0131lan algoritmalarla uyumlu olabilmesi i\u00e7in verinin tan\u0131malanan bir fonksiyona uygun olarak farkl\u0131 de\u011fer veya \u00f6l\u00e7eklere d\u00f6n\u00fc\u015ft\u00fcr\u00fclmesi i\u015flemidir.\n\n**A) Veri Normalle\u015ftirme**\n\nVeri dizisinde bulunan de\u011ferlerin [-1,+1] veya [0,+1] aral\u0131\u011f\u0131nda yer alacak \u015fekilde d\u00f6n\u00fc\u015ft\u00fcr\u00fclmesidir. Bu y\u00f6ntem farkl\u0131 b\u00fcy\u00fckl\u00fckte verinin bulundu\u011fu dizilerde vazge\u00e7ilmez d\u00f6n\u00fc\u015ft\u00fcrme i\u015flemlerinin ba\u015f\u0131nda gelmektedir.\n\n*Normalle\u015ftirme:*\n\nDe\u011fi\u015fkenlerin 0 ve 1 aras\u0131ndaki de\u011ferlere atanmas\u0131d\u0131r.\n\n> from sklearn.preprocessing import MinMaxScaler\n> mms = MinMaxScaler()\n> X_train_normed = mms.fit_transform(X_train) \n\n*Standartla\u015ft\u0131rma:*\n\nDe\u011fi\u015fken (\u00f6zellik) s\u00fctunlar\u0131n\u0131n ortalama de\u011feri 0 ve standart sapmas\u0131 1 olacak \u015fekilde standart normal da\u011f\u0131l\u0131m olu\u015fturmakt\u0131r.\n\n> from sklearn.preprocessing import StandardScaler\n> std = StandardScaler()\n> X_train_std = std.fit_transform(X_train)\n\n\n\n**B) Veri D\u00f6n\u00fc\u015ft\u00fcrme**\n\nLabel Encode ve OneHot Encoder, kategorik de\u011fi\u015fkenlerin, tahminlerde kullan\u0131labilmesi i\u00e7in makine \u00f6\u011frenmesi algoritmalar\u0131na uygun\nbir forma d\u00f6n\u00fc\u015ft\u00fcr\u00fcr.\n\n**Label Encoder:**\n Veriyi birebir say\u0131salla\u015ft\u0131r\u0131r. Her kategorik veriyi say\u0131sal bir de\u011fere atar. Verimsiz oldu\u011fu i\u00e7in pek kullan\u0131lmaz, yerine one OneHotEncoder tercih\n edilir.\n \n**OneHotEncoder**\n\nKategorik veriler i\u00e7in binarizasyon i\u015flemi yapar. Mevcut duruma \"1\", di\u011fer bir duruma ise \"0\" de\u011ferini atar.\n\n> from sklearn.preprocessing import OneHotEncoder \n\n> enc_df = pd.DataFrame(df.fit_transform(bridge_df[['columns_name']]).toarray())\n\n> df = df.join(enc_df)\n\n**GetDummies**\n\n Daha esnek bir kullan\u0131m\u0131 vard\u0131r.T\u00fcm kategorik de\u011fi\u015fkenleri tek seferde say\u0131sal de\u011fi\u015fkenlere d\u00f6n\u00fc\u015f\u00fct\u00fcr\u00fcr.\n \n> df = pd.get_dummies(data)\n\n\n\n\n![](https:\/\/i.ibb.co\/dGxVhns\/onehotencoding.jpg)\n\n","c55d264e":"# Veri \u00d6n \u0130\u015fleme (Data Preprocessing)\n\nVeri analizinin sonu\u00e7lar\u0131n\u0131n optimum seviyede olmas\u0131n\u0131 sa\u011flamak amac\u0131yla bu ad\u0131m dikkatli bir \u015fekilde ger\u00e7ekle\u015ftirilmelidir.\n\n\n1) Veri Entegrasyonu\n\n Verinin bir \u00e7ok kaynaktan toplanmas\u0131, se\u00e7ilmesi ve entegre edilerek tek bir kaynakta bir araya getirilmesi i\u015flemidir.\n\n2) [Veri Temizleme (Data Cleaning)](#1)\n\nVerideki parazitlerin ortadan kald\u0131r\u0131lmas\u0131, eksik ve ayk\u0131r\u0131 veri(outlier-missing values) sorunlar\u0131n\u0131n \u00e7\u00f6z\u00fclmesi gerekir.\n\n3) [Veri D\u00f6n\u00fc\u015ft\u00fcrme (Data Transformation)](#2)\n\nNormalization ve Satandardization gibi i\u015flemlerin uygulanarak verinin standart bir da\u011f\u0131l\u0131ma sahip olmas\u0131n\u0131 sa\u011flamak gerekir.\n\n4) [Veri \u0130ndirgeme](#3)\n\n\u00d6znitelik(feature) say\u0131s\u0131n\u0131, \u00f6rnekleme(sampling) fakt\u00f6r analizi(fakt\u00f6r analysis), boyut indirgeme(dimension reduction) gibi tekniklerle azalt\u0131lmas\u0131, veri hacminin k\u00fc\u00e7\u00fclt\u00fclmesi i\u015flemleridir.\n\n","0dfca436":"<a id=\"2\"><\/a> <br>\n# 3. Veri D\u00f6n\u00fc\u015ft\u00fcrme\n\n","d3c94c7d":"# Python K\u00fct\u00fcphaneleri\n\n## 1. NUMPY\n\nNumpy(Numerical Pyhton), bilimsel hesaplama i\u015flemlerini kolayla\u015ft\u0131rmak i\u00e7in yaz\u0131lm\u0131\u015f bir python k\u00fct\u00fcphanesidir. Temelini numpy dizileri olu\u015fturur. Pyhton dizilerine benzer fakat h\u0131z ve i\u015flevsellik a\u00e7\u0131s\u0131ndan daha iyidir. Arrayler ve matrisler \u00fczerinde \u00e7al\u0131\u015f\u0131r.\n\n\n","e5bd342c":"   **Veri Bilimini \u00f6\u011frenme a\u015famamda bir\u00e7ok kaynaktan edindi\u011fim bilgileri bir araya getirerek bir not defteri olu\u015fturdum. Bu not defteri\n\u00fczerinde veriyi, veriyi i\u015flemeyi, ve veriden anlaml\u0131 bilgiler \u00e7\u0131karmak i\u00e7in bilinmesi gerekenleri \u00f6zet halinde \u00f6\u011freneceksiniz.**","a45353e8":"### Genel S\u00fcre\u00e7 Ad\u0131mlar\u0131\n\n\n1) Problemin(\u0130\u015fin) Anla\u015f\u0131lmas\u0131:\n\n Verileri do\u011fru de\u011ferlendirerek, projede ba\u015far\u0131l\u0131 olman\u0131n ilk ad\u0131m\u0131, uygulaman\u0131n hangi ama\u00e7la yap\u0131laca\u011f\u0131n\u0131 do\u011fru bir \u015fekilde anlamakt\u0131r.\n \n2) Verinin Anla\u015f\u0131lmas\u0131\n\n \u00d6znitelik de\u011ferlerinin da\u011f\u0131l\u0131m\u0131, outlier de\u011ferlerin anla\u015f\u0131lmas\u0131, hangi \u00f6znitelik de\u011ferlerinin projede \u00f6zellikle \u00f6nem ta\u015f\u0131yaca\u011f\u0131 konusunda fikir edinmek gerekir.\n\n3) Verinin Haz\u0131rlanmas\u0131\n\nTan\u0131mlanan problem i\u00e7in gerekli olan veriler haz\u0131rlanmal\u0131d\u0131r. Daha sonra veriler \u00f6n i\u015fleme(preprocessing) ad\u0131mlar\u0131nda temizleme-d\u00f6nd\u00fcrme-indirgeme gibi i\u015flemlerle d\u00fczeltilmelidir.\n\n4) Modelin Kurulmas\u0131\n\nSupervised(g\u00f6zetimli) ya da unsupervised(g\u00f6zetimsiz) algoritmalara bak\u0131larak uygun y\u00f6ntemlerin belirlenmesi gerekir. Y\u00f6ntemlerin bir arada kullan\u0131lmas\u0131 olarak tan\u0131mlanan ensemble yakla\u015f\u0131mlar\u0131 kullan\u0131labilir.\n\n5) Yorumlama\n\n\u00c7\u0131kan sonu\u00e7lar \u00fczerinde g\u00f6rselle\u015ftirme yap\u0131larak sonu\u00e7lar\u0131n do\u011fru bir \u015fekilde anla\u015f\u0131lmas\u0131 gerekir.\n"}}