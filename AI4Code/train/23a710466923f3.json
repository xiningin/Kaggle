{"cell_type":{"690afdb2":"code","14fda885":"code","2c5ce311":"code","c7fe63f1":"code","f7854e67":"code","c7bc5895":"code","edb185a0":"code","ed7ffbc6":"code","cc78551f":"code","73421590":"code","fc75bd01":"code","d24e90ba":"code","5f8e579a":"code","af7b3396":"code","8a251569":"code","5893c8ae":"markdown","e9e695a4":"markdown","ae6bb483":"markdown","f4f35730":"markdown","732f13f5":"markdown","8535e99f":"markdown","0fa76a3e":"markdown","02aa20a5":"markdown","dae58b0a":"markdown","ebc8eca5":"markdown","26d5262c":"markdown","cafaaf81":"markdown","3ca4efde":"markdown"},"source":{"690afdb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport matplotlib.pyplot as plt\nfrom skimage import io, transform\nimport torch\nfrom torchvision import transforms, utils, datasets\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nimport torchvision\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import models","14fda885":"train_dir = '\/kaggle\/input\/airy-photo\/train'\ntest_dir = '\/kaggle\/input\/airy-photo\/test'","2c5ce311":"def create_train_df_meta_from_dir(train_dir):\n    \"\"\"\n        make metadata from train directory into \n        dataframe containing: class, filename, and full_path \n    \"\"\"\n    meta = list()\n    for dirname, _, filenames in tqdm(os.walk(train_dir)):\n        for filename in filenames:\n            _class = dirname.split('\/')[-1]\n            full_path = os.path.join(dirname, filename)\n            meta.append([\n                _class,\n                filename,\n                full_path\n            ])\n    \n    return pd.DataFrame(\n        meta, \n        columns=['class', 'filename', 'full_path']\n    )\n\ndef create_test_list_meta_from_dir(test_dir):\n    \"\"\"\n    load all full path of test data\n    \"\"\"\n    full_paths = list()\n    for dirname, _, filenames in tqdm(os.walk(test_dir)):\n        full_paths.append([\n            os.path.join(dirname, filename)\n            for filename in filenames\n        ])\n    # flatten list of list\n    return np.array(full_paths).ravel() ","c7fe63f1":"df_train_meta = create_train_df_meta_from_dir(train_dir)\ntest_image_list = create_test_list_meta_from_dir(test_dir)","f7854e67":"mode = ['train']\n\ndata_transforms = dict(\n    train=transforms.Compose([\n        transforms.RandomSizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            [0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225]\n        )\n    ]),\n    test=transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            [0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225]\n        )\n    ])\n)\n\nimage_datasets = {\n    x: datasets.ImageFolder(\n        train_dir,\n        data_transforms[x]\n    )\n    \n    for x in mode\n}\n\n# pretrained models in pytorch uses batch_size=4\nbatch_size = 4\n\ndataloaders = {\n    x: DataLoader(\n        image_datasets[x], \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=batch_size\n    )\n    for x in mode\n}\n\ndataset_sizes = {\n    x: len(image_datasets[x])\n    for x in mode\n}\nclass_names = image_datasets['train'].classes\ndevice = torch.device(\n    'cuda:0' if torch.cuda.is_available() else 'cpu'\n)","c7bc5895":"class AiryPhotoTestDataset(Dataset):\n    def __init__(self, image_list, transform=None):\n        self.image_list = image_list\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        image = io.imread(self.image_list[idx])\n        # label = None\n        sample = (image, '')\n        if self.transform:\n            sample = (self.transform(image), '')\n        return sample\n\nmode = 'test'\nimage_datasets[mode] = AiryPhotoTestDataset(\n    test_image_list,\n    transform=data_transforms[mode]\n)\n\ndataloaders[mode] = DataLoader(\n    image_datasets[mode],\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=batch_size\n)\n\ndataset_sizes[mode] = len(image_datasets[mode])","edb185a0":"inputs, classes = next(iter(dataloaders['train']))\nout = torchvision.utils.make_grid(inputs)\nprint(classes)","ed7ffbc6":"inp = out.numpy().transpose((1,2,0))\nplt.imshow(inp)\nplt.title(','.join([class_names[x] for x in classes]))\nplt.show()","cc78551f":"# download pretrained model\nmdl = models.resnet18(pretrained=True)\n\n# freeze all the parameters in each model's cells\nfor param in mdl.parameters():\n    param.requires_grad = False\n\n# change prediction layer\n# I already know that the last layer is called `fc` in this resnet18\n# you can output `mdl` object to peek what variable is in each layer\nnum_features = mdl.fc.in_features\n# need to recalibrate this into current class number\nmdl.fc = nn.Linear(num_features, len(class_names))\n# change the model to use current available device\nmdl = mdl.to(device)\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(\n    mdl.parameters(), lr=1e3\n)","73421590":"def train(mdl, num_epoch, dataloader, optimizer, criterion, dataset_size, device=device):\n    epoch_losses = list()\n    epoch_accs = list()\n    \n    for epoch in range(num_epoch):\n        mdl.train()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            with torch.set_grad_enabled(True):\n                outputs = mdl(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels.to(device))\n\n                loss.backward()\n                optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss \/ dataset_size\n        epoch_acc = running_corrects.double() \/ dataset_size\n        \n        epoch_losses.append(epoch_loss)\n        epoch_accs.append(epoch_acc.cpu().numpy())\n        print(\n            'epoch: {}, loss: {:.4f}, acc: {:.4f}'.format(\n                epoch, epoch_loss, epoch_acc\n            )\n        )\n    return epoch_losses, epoch_accs","fc75bd01":"num_epoch = 5\nepoch_losses, epoch_accs = train(\n    mdl,\n    num_epoch,\n    dataloaders['train'],\n    optimizer,\n    criterion,\n    dataset_sizes['train']\n)","d24e90ba":"x = np.arange(num_epoch)\n\nf, ax = plt.subplots(figsize=(12, 14))\nplt.subplot(211)\nplt.plot(x, epoch_losses)\nplt.title('Loss Per Epoch')\n\nplt.subplot(212)\nplt.plot(x, epoch_accs)\nplt.title('Acc Per Epoch')\n\nplt.show()","5f8e579a":"def predict(mdl, inputs, labels):\n    mdl.eval()\n    with torch.no_grad():\n        inputs = inputs.to(device)\n\n        outputs = mdl(inputs)\n        _, preds = torch.max(outputs, 1)\n    return preds","af7b3396":"inputs, labels = next(iter(dataloaders['test']))\n\npreds = predict(mdl, inputs, labels)\n\nprint('Predictions: {}'.format([class_names[x] for x in preds.cpu().numpy()]))","8a251569":"out = torchvision.utils.make_grid(inputs)\ninp = out.numpy().transpose((1,2,0))\nplt.imshow(inp)\nplt.title(','.join([class_names[x] for x in preds]))\nplt.show()","5893c8ae":"Here I want to visualize 1 batch of data","e9e695a4":"# Transfer Learning\n\nDealing with data loading and preprocessing does take time, but when you have used to these routine and boilerplate codes you can do this faster than you thought!\n\nNow I want to do transfer learning from a model called `resnet18`, the main ideas to transfer learning from this model are:\n1. Download pretrained model\n2. Freeze all the model parameters\n3. Change the output\/last layer\n4. Train the model\n\nBesides defining model, we need to also define:\n1. `criterion` or you might be more familiar with loss function\n2. `optimizer` do you know `Stochastic Gradient Descent` (SGD) ? than this is it!","ae6bb483":"is there something wierd happening in this 5 epoch? will it continue to improve if we increase our epoch?","f4f35730":"### Protip: Always try to run with `cpu` mode first\n\nSometimes you kind of getting strange error `...device assert..` when there are something wrong in GPU, whether it is incorrect batch_size \/ other dimension to be inputted to model or wrong dimension inputed to criterion \/ loss function. So I personally recommend 1 batch of data for 1 full flow","732f13f5":"# Predicting\n\nOur final step is to make prediction! Here are simple ideas to compose predict function:\n1. Set the model to eval mode: `mdl.eval()`\n2. Tell pytorch not to compute gradient with `torch.no_grad()`\n3. Throw the input into model and get prediction!","8535e99f":"Now we plot our training result","0fa76a3e":"note that the way pytorch and numpy store image in array is different, hence we need `transpose` function like below to visualize it in matplotlib","02aa20a5":"Hey looks like we made it! Right??","dae58b0a":"## Training Routine\n\nThe ideas behind this training routine are:\n1. Iterate each epoch, for each epoch do:\n2. Reset optimizer gradient to zero with `optimizer.zero_grad()`\n3. Throw the input to your model and get your model output\n4. Get prediction `torch.max(outputs)`\n5. Compute Loss\n\nIf you see extraneous codes, it is just me trying to compute and store metrics like `CrossEntropyLoss` and `Accuracy`.","ebc8eca5":"# References\n\n[1] https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html\n\n[2] https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html\n\n","26d5262c":"# Loading The Data\n\nFor this competition, the folder will be structured as followed","cafaaf81":"## Creating Test Dataset\n\nHere I need to use custom dataset class to create test dataset, because it doesn't follow the standard defined by `ImageFolder` in torchvision utility. However we sure can use `DataLoader` to manage how should we load dataset in conjuction with our custom dataset, thus still make predicting efforts much easier.","3ca4efde":"## Creating Train Dataset and DataLoader\n\nNow I will define the most important components for loading the data: `DataLoader`. However we need to define more than that to succesfully train our data:\n\n|Component|Description|\n| --- | :-- |\n|`Dataset`|Define the dataset that we wanted to load. We could use `ImageFolder` util function or creating our own custom `Dataset`|\n|`DataLoader`|**Most Important Component** in data loading. This is a helper to load `Dataset`, allowing to batch, shuffle, iterate dataset|\n|`device`|simplifying switching between `cpu` and `gpu`|\n|`class_names`|list of class names, the index of this list will be used as mapping between category and it's index|\n|`data_transform`|Transformations to apply to the raw data. Using `Compose` method to compose list of sequential transformation into 1|\n\n<br\/>\n\nImportant Train Transformation: as said in the blog$^{[2]}$, we need to normalize the images with \n```\n...\ntransforms.ToTensor(), # change from numpy object into pytorch's tensor\ntransforms.Normalize( # adjust image size in favor to pretrained model \n    [0.485, 0.456, 0.406],\n    [0.229, 0.224, 0.225]\n)\n...\n```\nbefore further inputing our data into the model.\n\n\n"}}