{"cell_type":{"c2a388c7":"code","2212ca18":"code","dd59133c":"code","a06eefa0":"code","5036e648":"code","b9d44f9a":"code","64b159f1":"code","a25c7a4e":"code","4b84f61d":"code","5f2b448f":"code","147c48bb":"code","8e0e14a2":"code","823d3ead":"code","084bb92e":"code","9f72c003":"code","2bef1e83":"code","0f11f1f6":"code","8ccd8203":"code","cfa452e0":"code","681d9c06":"code","a564813b":"code","9e1cac6f":"code","ec3a1d7b":"code","123b7790":"markdown","0fffabc1":"markdown","ce7b87b0":"markdown","9b555a85":"markdown","a17a7c28":"markdown","b0434fb1":"markdown","cd6afcf7":"markdown","b32cd11e":"markdown","e84afda4":"markdown","8b07492b":"markdown","41a5b20d":"markdown","e508e03d":"markdown","a3303044":"markdown","954ead44":"markdown","97a061c5":"markdown","b7af82c3":"markdown","4783f1c7":"markdown","d067385e":"markdown","7d72f92f":"markdown","17ddc619":"markdown","a51cccf9":"markdown","4aa41717":"markdown"},"source":{"c2a388c7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\nimport statsmodels.api as sm \nfrom scipy.stats import chi2_contingency,shapiro \nimport warnings\nwarnings.filterwarnings(\"ignore\") ","2212ca18":"train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntarget = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntrain.head()","dd59133c":"print(train.info())\nprint(\"---------------------------------------\")\nprint(test.info())\nprint(\"---------------------------------------\")\nprint(target.info())","a06eefa0":"print(\"Training missing data:\", train.isnull().sum().sum())\nprint(\"Testing missing data:\", test.isnull().sum().sum())\nprint(\"Target missing data:\", target.isnull().sum().sum())","5036e648":"print(\"Train-data\")\nprint(train.describe(include=['O']))\nprint(\"-----------------------------------------\")\nprint(\"Test-data\")\nprint(test.describe(include=['O']))","b9d44f9a":"pd.crosstab(train['cp_type'],train['cp_dose'])","64b159f1":"print('Chi square Test ')\nprint('H0: Variables are Independent')\nprint('H0: Variables are dependent \\n')\ndata = pd.crosstab(train['cp_type'],train['cp_dose'])\nstat, p, dof, expected = chi2_contingency(data) \nalpha = 0.05\nprint(\"p value:\", p) \nif p <= alpha: \n    print('Conclusion: Variables are dependent(reject H0)') \nelse: \n    print('Conclusion: Variables are Independent(H0 holds true)') ","a25c7a4e":"f,ax = plt.subplots(1,3,figsize=(18,5))\nsns.countplot('cp_type',data=train,ax=ax[0])\nsns.countplot('cp_dose',data=train,ax=ax[1])\nsns.countplot('cp_type',hue='cp_dose',data=train,ax=ax[2],palette='husl')\nax[0].set_title('cp_type')\nax[1].set_title('cp_dose')\nax[2].set_title('Counter plot of cp_dose vs cp_dose')\n","4b84f61d":"print(\"Train-data\")\nprint(train.select_dtypes('int').describe().transpose())\nprint(\"--------------------------------------------------------------------\")\nprint(\"Test-data\")\nprint(test.select_dtypes('int').describe().transpose())","5f2b448f":"print(\"Train-data\")\nprint(train.select_dtypes('int').agg(['var','std','kurt','skew']).transpose())\nprint(\"---------------------------------------------------------\")\nprint(\"Test-data\")\nprint(test.select_dtypes('int').agg(['var','std','kurt','skew']).transpose())","147c48bb":"out=[]\ndef iqr_outliers(df):\n    for i in df:\n        q1 = df.quantile(0.25)\n        q3 = df.quantile(0.75)\n        iqr = q3-q1\n        Lower_tail = q1 - 1.5 * iqr\n        Upper_tail = q3 + 1.5 * iqr\n        if i > Upper_tail or i < Lower_tail:\n            out.append(i)\n    print(\"Outliers:\",out)\niqr_outliers(train['cp_time'])","8e0e14a2":"print('Shapiro Wilk test')\nprint('H0: Distribution is not Normal.')\nprint('H0:Distribution is Normal. \\n')\nstat, p = shapiro(test.select_dtypes('int'))\nalpha = 0.05\nprint(\"p value:\", p) \nif p <= alpha: \n    print('Conclusion: Distribution is Normal(reject H0)') \nelse: \n    print('Conclusion: Distribution is not Normal(H0 holds true)') ","823d3ead":"f,ax = plt.subplots(1,3,figsize=(18,5))\nsns.distplot(train['cp_time'],ax=ax[0])\nsns.boxplot(data=train['cp_time'],ax=ax[1])\nsm.qqplot(train['cp_time'], line ='45',ax=ax[2]) \nax[0].set_title('Distribution Plot')\nax[1].set_title('Box  Plot')\nax[2].set_title('QQ Plot')","084bb92e":"print(\"Train-data\")\nprint(train.select_dtypes('float').describe().transpose())\nprint(\"--------------------------------------------------------------------\")\nprint(\"Test-data\")\nprint(test.select_dtypes('float').describe().transpose())","9f72c003":"print(\"Train-data\")\nprint(train.select_dtypes('float').agg(['var','std','kurt','skew']).transpose())\nprint(\"---------------------------------------------------------\")\nprint(\"Test-data\")\nprint(test.select_dtypes('float').agg(['var','std','kurt','skew']).transpose())","2bef1e83":"random_train = train.select_dtypes('float').sample(n=5, axis=1,random_state=123)\nrandom_train.head()","0f11f1f6":"print('List of Outliers')\nout1=[]\ndef iqr_outliers(df):\n    for i in df:\n        q1 = df.quantile(0.25)\n        q3 = df.quantile(0.75)\n        iqr = q3-q1\n        Lower_tail = q1 - 1.5 * iqr\n        Upper_tail = q3 + 1.5 * iqr\n        if i > Upper_tail or i < Lower_tail:\n            out1.append(i)\n    print(\"Outliers:\",out1)\niqr_outliers(train['c-15'])","8ccd8203":"print('Shapiro Wilk test')\nprint('H0: Distribution is not Normal.')\nprint('H0:Distribution is Normal. \\n')\ndef shapiro_wilk(df,col):\n  print('Column Name:',col)\n  stat, p = shapiro(df[col])\n  alpha = 0.05\n  print(\"p value:\", p) \n  if p <= alpha: \n    print('Conclusion: Distribution is Normal(reject H0)') \n  else: \n    print('Conclusion: Distribution is not Normal(H0 holds true)') \nshapiro_wilk(train,'g-523')\nshapiro_wilk(train,'g-731')\nshapiro_wilk(train,'g-180')\nshapiro_wilk(train,'g-185')\nshapiro_wilk(train,'g-435')","cfa452e0":"f,ax = plt.subplots(3,5,figsize=(20,12))\nsns.distplot(train['g-523'],ax=ax[0,0])\nsns.distplot(train['g-731'],ax=ax[0,1])\nsns.distplot(train['g-180'],ax=ax[0,2])\nsns.distplot(train['g-185'],ax=ax[0,3])\nsns.distplot(train['g-435'],ax=ax[0,4])\nsns.boxplot(data=train['g-523'],ax=ax[1,0])\nsns.boxplot(data=train['g-731'],ax=ax[1,1])\nsns.boxplot(data=train['g-180'],ax=ax[1,2])\nsns.boxplot(data=train['g-185'],ax=ax[1,3])\nsns.boxplot(data=train['g-435'],ax=ax[1,4])\nsm.qqplot(train['g-523'], line ='45',ax=ax[2,0]) \nsm.qqplot(train['g-731'], line ='45',ax=ax[2,1]) \nsm.qqplot(train['g-180'], line ='45',ax=ax[2,2]) \nsm.qqplot(train['g-185'], line ='45',ax=ax[2,3]) \nsm.qqplot(train['g-435'], line ='45',ax=ax[2,4]) \n\nax[0,0].set_title('Distribution Plot of g-523')\nax[0,1].set_title('Distribution Plot of g-731')\nax[0,2].set_title('Distribution Plot of g-180')\nax[0,3].set_title('Distribution Plot of g-185')\nax[0,4].set_title('Distribution Plot of g-435')\nax[1,0].set_title('Box Plot of g-523')\nax[1,1].set_title('Box Plot of g-731')\nax[1,2].set_title('Box Plot of g-180')\nax[1,3].set_title('Box Plot of g-185')\nax[1,4].set_title('Box Plot of g-435')\nax[2,0].set_title('QQ Plot of g-523')\nax[2,1].set_title('QQ Plot of g-731')\nax[2,2].set_title('QQ Plot of g-180')\nax[2,3].set_title('QQ Plot of g-185')\nax[2,4].set_title('QQ Plot of g-435')","681d9c06":"X_train   = train.drop(['sig_id'],axis=1)\ny_train   = target.drop(['sig_id'],axis=1)\nX_test    = test.drop(['sig_id'],axis=1)\ntarget    = target.drop(['sig_id'],axis=1)\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)","a564813b":"from sklearn.decomposition import TruncatedSVD\nsvd =  TruncatedSVD(n_components = 400)\ndim_reduction = pd.DataFrame(svd.fit_transform(X_train))\ndim_reduction.head()","9e1cac6f":"target.info()","ec3a1d7b":"target.head()","123b7790":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Outlier Detection : IQR method<\/font><\/div>     \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> In this method outliers detected using Inter Quartile Range(IQR). For more details visit this\n   <a href=\"https:\/\/www.kaggle.com\/nareshbhat\/outlier-the-silent-killer\">link.<\/a> <\/font><\/div>\n","0fffabc1":"<div align='left'><font size=\"4\" color=\"#0000FF\"> About Dataset:<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\"> Train Data contains 23814 entries and 876 columns, Test Data contains 3982 entries and 876 columns, target column contains 23814 entries and 207 columns. Here our aim is to do statistical analysis on the dataset and find some good insight from it.<\/font><\/div>\n<hr>\n<div align='left'><font size=\"5\" color=\"#0000FF\"> Loading Data<\/font><\/div>     \n<hr> \n","ce7b87b0":"<div class=\"alert alert-info\">\n  <strong><\/strong><font size=\"5\"> In this kernel I tried to cover almost all the topics related to statistical analysis and eda.Feel free to ask any question related to this topic. I'm happy to answer.\n<\/div>\n<hr>\n<div class=\"alert alert-success\"><font size=\"5\">\n   If you like my work Kindly <strong>UPVOTE!<\/strong> HAPPY LEARNING :)\n<\/div>\n","9b555a85":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Target Column: Multi-Label Classification<\/font><\/div>     \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> Multi-Label Classification is a classification problem where target variable has more than one dimension and each dimension is binary. \n<\/font><\/div>\n<hr>\n\n> <img style=\"float: centre;\" src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQUqgAcgROPlnULCx5vbIvHAFyMP7DO0he9OQ&usqp=CAU\" width=\"450px\"\/>\n","a17a7c28":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Missing Values<\/font><\/div>     \n<hr> ","b0434fb1":"<div align='left'><font size=\"3\" color=\"#000000\">  As the dataset is very huge we take 5 variable randomly to analyze it.<\/font><\/div>\n<hr>\n\n<div align='left'><font size=\"5\" color=\"#0000FF\"> Outlier Detection : IQR method<\/font><\/div>     \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> In this method outliers detected using Inter Quartile Range(IQR). For more details visit this\n   <a href=\"https:\/\/www.kaggle.com\/nareshbhat\/outlier-the-silent-killer\">link.<\/a> <\/font><\/div>","cd6afcf7":"<div align='left'><font size=\"5\" color=\"#0000FF\"> References<\/font><\/div>\n<hr>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2017\/08\/introduction-to-multi-label-classification\/\" target=\"_blank\">1.Solving Multi-Label Classification problems<\/a><\/div>\n<hr>\n<div align='left'><font size=\"3\"><a href=\"https:\/\/medium.com\/thecyphy\/handling-data-imbalance-in-multi-label-classification-mlsmote-531155416b87\" target=\"_blank\">2.Handling Data Imbalance in Multi-label Classification<\/a><\/div>\n<hr>  \n<div align='left'><font size=\"3\"><a href=\"https:\/\/www.kaggle.com\/rhodiumbeng\/classifying-multi-label-comments-0-9741-lb\" target=\"_blank\">3.Classifying multi-label comments<\/a> <\/div>\n<hr>  \n<div align='left'><font size=\"3\"><a href=\"https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0950705115002737\" target=\"_blank\">4.MLSMOTE: Approaching imbalanced multilabel learning through synthetic instance generation<\/a><\/div>\n    \n","b32cd11e":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Float Data type<\/font><\/div>     \n<hr>\n<div align='left'><font size=\"4\" color=\"#0000FF\"> Descriptive Statistics: Overview of dataset<\/font><\/div>     \n<hr>\n","e84afda4":"<div align='left'><font size=\"5\" color=\"#0000FF\">  Importing necessary Libraries<\/font><\/div>     \n<hr> ","8b07492b":"<div align='left'><font size=\"5\" color=\"#0000FF\">Inferential Statistics : Chi-square (\u03c72) test for independence<\/font><\/div>     \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Chi-square test is a non-parametric method used to compare the relationship between the two categorical (nominal) variables in a contingency table.<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\">Null hypotheses(H0): The two categorical variables are independent (no association between the two variables).<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\">Alternative hypotheses(H1): The two categorical variables are dependent (there is an association between the two variables)<\/font><\/div>\n<hr>\n","41a5b20d":"<div align='left'><font size=\"3\" color=\"#000000\"> There is no missing values observed in the dataset.<\/font><\/div>\n<hr>\n\n<div align='left'><font size=\"5\" color=\"#0000FF\">Object Data type<\/font><\/div>     \n<hr> \n<div align='left'><font size=\"4.5\" color=\"#0000FF\">Descriptive Statistics: Overview of dataset<\/font><\/div>     \n<hr> \n","e508e03d":"<div align='left'><font size=\"3\" color=\"#000000\"> We Find that there is no outlier. To understand better we use normality test (Shapiro Wilk test).<\/font><\/div>\n<hr> \n\n<div align='left'><font size=\"5\" color=\"#0000FF\"> Inferential Statistics : Shapiro Wilk test<\/font><\/div>     \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> Shapiro Wilk test is method used to Check the whether the data is normally distributed or not. <\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\"> Null hypotheses(H0): Distribution is not Normal.<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\">  Alternative hypotheses(H1): Distribution is Normal.<\/font><\/div>\n<hr>\n","a3303044":"---\n<div align='center'><font size=\"8\" color=\"#0000FF\"> MoA: Statistical Analysis and EDA <\/font><\/div>     \n<hr> \n<hr>\n<img style=\"float: centre;\" src=\"https:\/\/www.rssl.com\/~\/media\/rssl\/en\/images\/sectors\/pharmaceutical\/pharmaceutical-testing-laboratory-uk.jpg?h=250&la=en&w=250\" width=\"500px\"\/>\n<hr>\n<div align='left'><font size=\"4\" color=\"#0000FF\"> Problem Defination<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\"> The task of this Mechanisms of Action (MoA) competition is to classify drugs based on multi-labels. The dataset describes the responses of 100 different types of human cells to various drugs. Those response patterns will be used to classify the MoA reponse.This dataset contain multi-label class imbalance problem which is breifly explained in this kerenl. In this kernel I tried to cover almost all the topics related to statistical analysis and eda.\n<\/font><\/div>\n<hr>\n\n","954ead44":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Float Data type<\/font><\/div>     \n<hr>\n<div align='left'><font size=\"4.5\" color=\"#0000FF\"> Visualization<\/font><\/div>     \n<hr>\n","97a061c5":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Multi-label Data Imbalance Problem<\/font><\/div>     \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> Class Imbalance is one of the most common problems in classification, which occurs in many of the real-world datasets. This class imbalance can create a challenge for predictive modelling tasks and lead to poor predictive performance for minority class, as most of the machine learning algorithms are developed with the assumption of the class is balanced. Handling the imbalance class will create a balanced class distribution and remove the bias towards minority classes.\n<\/font><\/div>\n<hr>\n<img style=\"float: centre;\" src=\"https:\/\/miro.medium.com\/max\/3200\/1*LMqkFEL6uE5JPEcYgjmN4g.jpeg\" width=\"450px\"\/>\n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> Here the task is multi label classification where the target variable has more than one dimension where each dimension is binary. In gerneral to solve imbalance problem there are 3 major techniques are available namely random upsampling,random downsampling,SMOTE. But these methods are holds good only when label is in one dimension(binary\/multi-label not multiclass). MLSMOTE is the technique to deal with multi label data imbalance problem. Unfortunately there is no package available in python to solve this problem.This Research Paper gives a high level idea about MLSMOTE \u279c <a href=\"https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0950705115002737\">MLSMOTE: Approaching imbalanced multilabel learning through synthetic instance generation<\/a>\n<\/font><\/div>\n","b7af82c3":"# Table of Contents\n* [<div align='left'><font size=\"3\" color=\"#0000FF\">1. Importing the necessary libraries<\/font><\/div>](#imports)\n- [<div align='left'><font size=\"3\" color=\"#0000FF\">2. Reading the datasets<\/font><\/div>](#reading)\n- [<div align='left'><font size=\"3\" color=\"#0000FF\">3. EDA and Statistical Analysis<\/font><\/div>](#eda)\n    * <div align='left'><font size=\"3\" color=\"#0000FF\">3.1 Missing Values <\/font><\/div>\n    * <div align='left'><font size=\"3\" color=\"#0000FF\">3.2 Object datatype<\/font><\/div>\n        <div align='left'><font size=\"2.5\" color=\"#0000FF\">3.2.1 Descriptive Statistics <\/font><\/div>\n        <div align='left'><font size=\"2.5\" color=\"#0000FF\">3.2.2 Inferential Statistics <\/font><\/div>\n        <div align='left'><font size=\"2.5\" color=\"#0000FF\">3.2.3 Visualization <\/font><\/div>\n    * <div align='left'><font size=\"3.5\" color=\"#0000FF\">3.3 Integer Type<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#0000FF\"> 3.3.1 Descriptive Statistics<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#0000FF\"> 3.3.2 Outlier Detection<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#0000FF\"> 3.3.3 Normality Test<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#0000FF\"> 3.3.4 Visualization<\/font><\/div>\n    * <div align='left'><font size=\"3\" color=\"#0000FF\">3.4 Float Type<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#0000FF\"> 3.4.1 Descriptive Statistics<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#0000FF\"> 3.4.2 Outlier Detection<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#0000FF\"> 3.4.3 Checking the Distribution<\/font><\/div>\n         <div align='left'><font size=\"2.5\" color=\"#0000FF\"> 3.4.4 Visualization<\/font><\/div>\n- [<div align='left'><font size=\"3\" color=\"#0000FF\">4. Dimension Reduction <\/font><\/div>](#DR)\n    * <div align='left'><font size=\"3\" color=\"#0000FF\">4.1 SVD <\/font><\/div>\n- [<div align='left'><font size=\"3\" color=\"#0000FF\">5. Target Column <\/font><\/div>](#model)\n    * <div align='left'><font size=\"3\" color=\"#0000FF\">5.1 Multi-label Class Imbalance <\/font><\/div>\n- [<div align='left'><font size=\"3\" color=\"#0000FF\">6. Conclusion<\/font><\/div>](#model) \n\n\n","4783f1c7":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Conclusion<\/font><\/div>     \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\">1. EDA helpful to understand the overview of the dataset. <\/font><\/div>   \n<div align='left'><font size=\"3\" color=\"#000000\">2. Chi Square test tells there is no significant relationship between variable of object type.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">3. No outlier is found in integer datatype columns and float datatype columns contain outliers.<\/font>\n<div align='left'><font size=\"3\" color=\"#000000\">4. Shapiro-Wilk test gives the conclusion that distribution is normal.<\/font><\/div> \n<div align='left'><font size=\"3\" color=\"#000000\">5. This dataset facing Multi-label Data Imbalance Problem.<\/font><\/div> \n<hr>\n    ","d067385e":"As the dataset is large, variable is tending to normal. In other words as the sample size increases, the standard deviation of the means decreases([Central limit theorem](https:\/\/demonstrations.wolfram.com\/DistributionOfNormalMeansWithDifferentSampleSizes\/#:~:text=The%20population%20mean%20of%20the,the%20distribution%20being%20sampled%20from.&text=Thus%20as%20the%20sample%20size,of%20the%20sample%20means%20increases.)). We conclude that all the variable is normally distributed.\n\n<div align='left'><font size=\"5\" color=\"#0000FF\"> Visualization<\/font><\/div>     \n<hr>\n\n","7d72f92f":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Integer Data type<\/font><\/div>     \n<hr>\n<div align='left'><font size=\"4.5\" color=\"#0000FF\"> Descriptive Statistics: Overview of dataset<\/font><\/div>     \n<hr>\n","17ddc619":"\n<div align='left'><font size=\"5\" color=\"#0000FF\"> Inferential Statistics : Shapiro Wilk test<\/font><\/div>     \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\">  Null hypotheses(H0): Distribution is not Normal.<\/font><\/div>\n<div align='left'><font size=\"3\" color=\"#000000\">  Alternative hypotheses(H1): Distribution is Normal.<\/font><\/div>\n<hr>\n","a51cccf9":"<div align='left'><font size=\"3\" color=\"#000000\">The P-value obtained from chi-square test for independence is not significant (P > 0.05), and therefore, we conclude that there is no significant association between variable.<\/font><\/div>\n<hr>\n\n<div align='left'><font size=\"4.5\" color=\"#0000FF\"> Visualization<\/font><\/div>     \n<hr>\n\n","4aa41717":"<div align='left'><font size=\"5\" color=\"#0000FF\"> Dimensional Reduction<\/font><\/div>     \n<hr>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> As the dataset is very huge, It is difficult to train the model.Here Dimensionality Reduction technique play an important rule that reduce the training time and remove unwanted columns. Here Singular Value decomposition is applied. \n<\/font><\/div>\n<hr>\n\n<div align='left'><font size=\"4.5\" color=\"#0000FF\"> Singular Value decomposition(SVD):<\/font><\/div>     \n<hr>\n<div align='left'><font size=\"3\" color=\"#000000\"> SVD is a matrix factorization technique where a matrix is decomposed into a product of a square matrix, a diagonal matrix, and another square matrix. It is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction in machine learning. Here we reduce the dimension to 400.\n<\/font><\/div>\n"}}