{"cell_type":{"9fa32118":"code","19c3b14c":"code","79dab772":"code","e9f43afd":"code","cb75439b":"code","56d85ce9":"code","142ed03e":"code","a2922996":"code","f6f9986a":"code","803ae3ef":"code","4694f880":"code","a7e7b693":"code","bb734428":"code","a6941c68":"code","5971e91a":"code","2d46b752":"code","adcaf547":"code","94f42b88":"markdown","e87f84e4":"markdown","088f7823":"markdown","f27d3025":"markdown","4b348fd0":"markdown","3e6e180d":"markdown","fb3706f3":"markdown","0f6638ed":"markdown","de2ca4ab":"markdown","04d576a0":"markdown","0e493595":"markdown","973d6073":"markdown","e9a0890f":"markdown","5f2be8a7":"markdown","62aeed71":"markdown"},"source":{"9fa32118":"from fastai.vision.all import *\nfrom tqdm.notebook import tqdm","19c3b14c":"emb_size = 3\noutput_classes = 10\nbatch_size = 1 # for illustration only","79dab772":"classifier = nn.Linear(emb_size, output_classes, bias=False)\nW = classifier.weight.T # nn.Linear keeps it output_classes x emb_size but it's easier to think about it the other way round","e9f43afd":"x = torch.rand((batch_size, emb_size))\ny1 = classifier(x)\ny2 = x @ W\n(y1==y2).all()","cb75439b":"y04 = torch.dot(x[0], W[:,4])\ny04, y1[0][4]","56d85ce9":"x = F.normalize(x)\nW = F.normalize(W, dim=0)\n\noutputs = x @ W\nsim04 = F.cosine_similarity(x[0], W[:,4], dim=0)\nsim04, outputs[0][4]","142ed03e":"angle_output = outputs.arccos()\nangle_output[0]","a2922996":"angle_output[0].rad2deg()","f6f9986a":"class ArcFaceClassifier(nn.Module):\n    def __init__(self, emb_size, output_classes):\n        super().__init__()\n        self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n        nn.init.kaiming_uniform_(self.W)\n    def forward(self, x):\n        # Step 1:\n        x_norm = F.normalize(x)\n        W_norm = F.normalize(self.W, dim=0)\n        # Step 2:\n        return x_norm @ W_norm\n    \ndef arcface_loss(cosine, targ, m=.4):\n    # this prevents nan when a value slightly crosses 1.0 due to numerical error\n    cosine = cosine.clip(-1+1e-7, 1-1e-7) \n    # Step 3:\n    arcosine = cosine.arccos()\n    # Step 4:\n    arcosine += F.one_hot(targ, num_classes = output_classes) * m\n    # Step 5:\n    cosine2 = arcosine.cos()\n    # Step 6:\n    return F.cross_entropy(cosine2, targ)","803ae3ef":"# a very basic network, just four strided convolutions, batch norm and ReLu\nclass SimpleConv(nn.Module):\n    def __init__(self, classifier):\n        super().__init__()\n        ch_in=[3,6,12,24]\n        convs = [ConvLayer(c, c*2, stride=2) for c in ch_in]\n        convs += [AdaptiveAvgPool(), Flatten(), nn.Linear(48, emb_size)]\n        self.convs = nn.Sequential(*convs)\n        self.classifier = classifier\n        \n    def get_embs(self, x):\n        return self.convs(x)\n    \n    def forward(self, x):\n        x = self.get_embs(x)\n        x = self.classifier(x)\n        return x\n","4694f880":"# helper method to extract all embedings from a data loader\ndef get_embs(model, dl):\n    embs = []\n    ys = []\n    for bx,by in tqdm(dl):\n        with torch.no_grad():\n            embs.append(model.get_embs(bx))\n            ys.append(by)\n    embs = torch.cat(embs)\n    embs = embs \/ embs.norm(p=2,dim=1)[:,None]\n    ys = torch.cat(ys)\n    return embs,ys\n\n# helper to plot embeddings in 3D\ndef plot_embs(embs, ys, ax):\n    #ax.axis('off')\n    for k in range(10):\n        e = embs[ys==k].cpu()\n        ax.scatter(e[:,0], e[:,1], e[:,2], s=4, alpha=.2)   \n","a7e7b693":"dls =ImageDataLoaders.from_folder(untar_data(URLs.MNIST), train='training', valid='testing', num_workers=8)\nlearn = Learner(dls, SimpleConv(ArcFaceClassifier(3,10)), metrics=accuracy, loss_func = arcface_loss)\n\nlearn.fit_one_cycle(5, 5e-3)","bb734428":"embs_arcface, ys_arcface  = get_embs(learn.model.eval(), dls.valid)","a6941c68":"learn = Learner(dls, SimpleConv(nn.Linear(3,10)), metrics=accuracy)\nlearn.fit_one_cycle(5, 5e-3)","5971e91a":"embs_softmax, ys_softmax  = get_embs(learn.model.eval(), dls.valid)","2d46b752":"### You should be able to see a tighter clustering of embeddings on the left","adcaf547":"_,(ax1,ax2)=plt.subplots(1,2, figsize=(20,10), subplot_kw={'projection':'3d'})\nplot_embs(embs_arcface, ys_arcface, ax1)\nplot_embs(embs_softmax, ys_softmax, ax2)\n\nplt.show()","94f42b88":"* Three lines of code to create a dataloader and train our model. Don't you love [fast.ai](fast.ai)","e87f84e4":"#### Use a standard nn.Linear and cross entropy for comparison","088f7823":"#### Another way to look at it is that for each embedding we compute it's dot product with weight columns corresponding to every output class","f27d3025":"#### The smaller the angle the closer the cosine is to 1 and the higher activation when computing the Cross Entropy Loss.","4b348fd0":"# ArcFace Explained","3e6e180d":"#### The model:","fb3706f3":"This notebook implements ArcFace algorithm introduced in https:\/\/arxiv.org\/abs\/1801.07698 in pytorch with step by step explanations.\nAlso recreates an experiment to visualize distribution of embeddings on the MNIST dataset. Fastai library is used for training","0f6638ed":"## Sample Implementation ##","de2ca4ab":"#### Now imagine that both embeddings and weight columns are vectors of length 1. \nTaking their dot product is the same as calculating [cosine similarity](https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity)\nAnd with unit vectors it's just cosine of the angle between them.","04d576a0":"#### Since our outputs are now cosines of angles, we can reverse it with arccos and get the actual angles.","0e493595":"## How does it work?\n\n#### Consider a typical classifier you'd put as a final layer in your network","973d6073":"## Test it on MNIST, using fastai for training","e9a0890f":"#### And in degrees:","5f2be8a7":"### With all that out of the way, here is what the ArcFace algorithm does:\n1. Normalize the embeddings and weights\n2. Calculate the dot products\n3. Calculate the angles with arccos\n4. **Add a constant factor m to the angle corresponding to the ground truth label**\n5. Turn angles back to cosines\n6. Use cross entropy on the new cosine values to calculate loss\n\nThe only part left to explain is the step 4. What we are doing is making it harder for the network to train. Similar to what dropout or weight decay does, but here we are extra mean and only shift the activation the network want's to predict. The idea is that in order for the activation to win at softmax it not only needs to be slightly better than the second best. It needs to be better by more than the angle we shifted by adding **m** (the default 0.4 corresponds to about 20 degrees). We add a margin to one of the angles, hence the 'Additive Angular Margin\nLoss' in the title of the paper.\n\nThe intended consequence of that is that we force embeddings to cluster closer together within class and further apart across other classes.\n\n*Side note: there is another step in the paper that scales all the vectors by a constant factor at the end. This doesn't change the ordering of logits but changes their relative values after softmax. I didn't see it help on this dataset but it might be a hyperparameter worth tweaking on other tasks.*","62aeed71":"#### If we set the bias to False all it does is a single matrix multiplication of embedings by the layer weights. That is:"}}