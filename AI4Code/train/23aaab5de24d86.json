{"cell_type":{"8ea07d25":"code","5a313e14":"code","3745ee00":"code","fd73fe05":"code","72bea0a8":"code","08eb300f":"code","6499410a":"code","6156b269":"markdown","e287fb3a":"markdown","ff2cf03b":"markdown","74f347d5":"markdown","9b6eb153":"markdown","64a4bd48":"markdown","05ac7246":"markdown","edcd460a":"markdown","ebe8bfda":"markdown","1151ecdd":"markdown","7d7b1bb1":"markdown"},"source":{"8ea07d25":"!pip install wikipedia","5a313e14":"import pandas as pd\nimport wikipedia\narticles=['Data Science','Artificial intelligence','Machine  Learning',\n          'European Central Bank','Finance','Financial technology','International Monetary Fund',\n          'Basketball','Swimming','Football']\nwiki_lst=[]\ntitle=[]\nfor article in articles:\n    print(\"loading content: \",article)\n    wiki_lst.append(wikipedia.page(article).content)\n    title.append(article)","3745ee00":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words={'english'})\nX = vectorizer.fit_transform(wiki_lst)","fd73fe05":"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans","72bea0a8":"Sum_of_squared_distances = []\nK = range(2,10)\nfor k in K:\n    km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n    km = km.fit(X)\n    Sum_of_squared_distances.append(km.inertia_)\n\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","08eb300f":"clusters = 6\nmodel = KMeans(n_clusters = clusters)\nmodel.fit(X)\nlabels=model.labels_\nwiki_cl=pd.DataFrame(list(zip(title,labels)),columns=['title','cluster'])\nprint(wiki_cl.sort_values(by=['cluster']))","6499410a":"from wordcloud import WordCloud\nresult={'cluster':labels,'wiki':wiki_lst}\nresult=pd.DataFrame(result)\nfor k in range(0,clusters):\n   s=result[result.cluster==k]\n   text=s['wiki'].str.cat(sep=' ')\n   text=text.lower()\n   text=' '.join([word for word in text.split()])\n   wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n   print('Cluster: {}'.format(k))\n   print('Titles')\n   titles=wiki_cl[wiki_cl.cluster==k]['title']         \n   print(titles.to_string(index=False))\n   plt.figure()\n   plt.imshow(wordcloud, interpolation=\"bilinear\")\n   plt.axis(\"off\")\n   plt.show()","6156b269":"### Ambil Artikel\nArtikel yang diambil\n* Data Science\n* Artificial intelligence\n* Machine Learning\n* European Central Bank\n* Finanace\n* Financial technology\n* International Monetary Fund\n* Basketball\n* Swimming\n* Football","e287fb3a":"## Ambil artikel dari Wikipedia","ff2cf03b":"## Klusterisasi Artikel dengan *K-Means Clustering*","74f347d5":"### Menentukan Jumlah Kluster dengan *Elbow Method*\n[*Elbow method*](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)) digunakan untuk mencari jumlah kluster yang memiliki perbedaan yang signifikan","9b6eb153":"## Representasi Artikel dalam *Vector*","64a4bd48":"### TF-IDF\n* [Term frequency\/inverse document frequency (tf-idf)](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf) untuk menghitung signifikansi kata dalam  kumpulan dokumen.\n* *Stop words* tidak dipakai","05ac7246":"### Instal Wikipedia Library","edcd460a":"## *Word Cloud*\n*Word Cloud* digunakan untuk visualisasi kata yang banyak dikandung dalam kluster","ebe8bfda":"# Klusterisasi Teks: Klusterisasi Artikel Wikipedia\nReferensi dan atribusi:\n* Clustering documents with Python oleh Dimitris Panagopoulos\n* Dataset dari Wikipedia oleh kontributor Wikipedia\n* Clustering Session 19 and 20\n* Feldman, R , James S. (2007). The Text Mining Handbook. First edition. Cambridge University Press. New York","1151ecdd":"## Evaluasi Hasil\nDikarenakan artikelnya hanya sedikit maka evaluasi dapat dilakukan dengan meilihat artikel apa yang terkandung dalam setiap kluster","7d7b1bb1":"### Melakukan *K-Means Clustering* dengan Jumlah Kluster yang Signifikan"}}