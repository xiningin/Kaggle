{"cell_type":{"4f53ba92":"code","6c7a0584":"code","25435415":"code","c30ba080":"code","34dd5ab6":"code","937bd81a":"code","fc635306":"code","067dd67a":"code","43a6d852":"code","e5126394":"code","050f842a":"code","010ca8ff":"code","acf067de":"code","e01d8b7d":"code","6ba122c8":"code","a01a73d1":"code","0a9bca02":"code","72e79bcd":"code","1e9aa877":"code","bbbfecc5":"code","00fc66dc":"code","b3ca4901":"code","bfd6f963":"code","ac617d25":"code","2a24217d":"code","358dc5a5":"code","9c845032":"code","e2efdcc5":"code","2580a22d":"code","88b708a1":"code","dd148587":"code","512b0799":"code","e258da73":"code","ad114000":"code","1ae890dd":"code","f9ee2eff":"code","f8e074ea":"code","b9cdb234":"code","19420070":"code","c2cc03a3":"code","d5cf2d26":"code","1936641f":"markdown","0ca55fd2":"markdown","05269f53":"markdown","49b1b399":"markdown","1d73dd64":"markdown","9ee4827c":"markdown","a8b18c39":"markdown","844188d0":"markdown","341c5ac0":"markdown","b3b4c06f":"markdown","a48adfb8":"markdown","a19dd543":"markdown","2296940d":"markdown","2fdcd94c":"markdown","26c03478":"markdown","bb82fa67":"markdown","d328c5c8":"markdown","16efc0ce":"markdown","e45caa17":"markdown","6a6cd841":"markdown","153be1c5":"markdown","7164cecc":"markdown","31005989":"markdown","63f27846":"markdown","e141360f":"markdown","64f31d87":"markdown","72a4c6ae":"markdown","d216ca06":"markdown","2d2039e5":"markdown","95e13233":"markdown","38d66543":"markdown"},"source":{"4f53ba92":"from collections import defaultdict\nfrom enum import IntEnum, unique, auto\nimport logging\nimport os\nfrom pathlib import Path\nimport pickle\nimport random\nimport sys\nfrom typing import Tuple, Union\n\nlogger = logging.basicConfig(level=logging.INFO)","6c7a0584":"import torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import LambdaLR\n\nimport numpy as np\n\nfrom kaggle_environments import make, evaluate\nfrom kaggle_environments.utils import structify\nfrom gym import spaces","25435415":"import ray\nfrom ray import tune\nimport ray.rllib.agents.ppo as ppo\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.models.torch.torch_modelv2 import TorchModelV2\nfrom ray.rllib.models.torch.recurrent_net import RecurrentNetwork\nfrom ray.rllib.utils.annotations import override\nfrom ray.tune.registry import ENV_CREATOR, register_env, _global_registry\nfrom ray.tune import JupyterNotebookReporter\nfrom ray.rllib.agents.callbacks import DefaultCallbacks\nfrom ray.rllib.env import MultiAgentEnv\nfrom ray.rllib.evaluation.episode import MultiAgentEpisode\nfrom ray.rllib.env.base_env import _DUMMY_AGENT_ID\nfrom ray.rllib.utils.typing import AgentID, PolicyID","c30ba080":"!pip install lz4","34dd5ab6":"!pip list | grep -wE \"ray|torch|kaggle-environments\"","937bd81a":"# Load the TensorBoard extension\n%load_ext tensorboard","fc635306":"RANDOM_SEED = 16","067dd67a":"CURRENT_PATH = !pwd\nCURRENT_PATH = Path(CURRENT_PATH[0])","43a6d852":"torch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)","e5126394":"ray.init(ignore_reinit_error=True)","050f842a":"class ConnectFourGym(MultiAgentEnv):\n    def __init__(self, config=None):\n        if config is None:\n            config = dict(annealing_factor_last_iteration=0)\n        self.agent1 = \"agent1\"\n        self.agent2 = \"agent2\"\n        self.annealing_factor_last_iteration = config.get(\"annealing_factor_last_iteration\")\n        \n        self.connectx_env = make(\"connectx\", debug=True)\n        # Size of the board\n        self.rows = self.connectx_env.configuration.rows\n        self.columns = self.connectx_env.configuration.columns\n        self.n_cells = self.rows * self.columns\n        # Number of consecutive discs needed to win the game\n        self.inarow = self.connectx_env.configuration.inarow\n        \n        # Patterns that will be search for in the _change_reward method\n        self.patterns = { \n            num_discs: [\n                np.array([[1] * num_discs]),\n                np.array([[1]] * num_discs),\n                np.array([[1 if j == i else 0 for j in range(num_discs)] for i in range(num_discs)]),\n                np.array([[1 if j == i else 0 for j in range(num_discs)] for i in reversed(range(num_discs))]),\n            ]\n            for num_discs in range(1, self.inarow + 1)\n        }\n        \n        # Linear annealing factor used to used to slowly \n        # reduce the exploration component of the reward to zero\n        # r = alpha * exploration_reward + (1 - alpha) * normal_reward\n        # https:\/\/arxiv.org\/pdf\/1710.03748.pdf\n        self.alpha = 0.0\n        \n        self.env = self._create_environment()\n        \n        # Action and Observation Spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(\n            low=-1, \n            high=1, \n            shape=(self.rows,self.columns,1), \n            dtype=np.int\n        )\n        \n        # These are used to keep track of the agents' number of wins and losses\n        self.agent_wins = defaultdict(int)\n        \n        self.n_step = 0\n        self.n_episodes = 0\n\n    def reset(self):\n        self.n_step = 0\n        self.env = self._create_environment()\n        obs_dict = self.env.reset()\n        for k, obs in obs_dict.items():\n            observation = np.array(obs[\"board\"]).reshape(self.rows, self.columns, 1)\n            observation = np.where(observation == obs.mark, 1, observation)\n            observation = np.where(observation == obs.mark % 2 + 1, -1, observation)\n            obs_dict[k] = observation\n        return obs_dict\n    \n    def _create_environment(self):        \n        def advance(position: int):\n            while not self.connectx_env.done and self.connectx_env.state[position].status == \"INACTIVE\":\n                self.connectx_env.step((None, None))\n\n        def reset():\n            self.connectx_env.reset(2)\n            advance(0)\n            return {\"agent1\": self.connectx_env._Environment__get_shared_state(0).observation, \"agent2\": self.connectx_env._Environment__get_shared_state(0).observation}\n\n        def step(agent, action):\n            if agent == \"agent1\":\n                actions = (action, None)\n                position = 0\n            else:\n                actions = (None, action)\n                position = 1\n            self.connectx_env.step(actions)\n            agent = self.connectx_env._Environment__get_shared_state(position)\n            reward = agent.reward\n            if len(self.connectx_env.steps) > 1 and reward is not None:\n                reward -= self.connectx_env.steps[-2][position].reward\n            return [\n                agent.observation, reward, agent.status == \"DONE\" or agent.status == \"INVALID\", agent.info\n            ]\n\n        reset()\n        \n        return structify({\"step\": step, \"reset\": reset})\n\n    def step(self, actions_dict):\n        self.n_step += 1\n        obs, reward, done, info = {}, {}, {}, {}\n        for agent_id, action in actions_dict.items():\n            obs[agent_id], old_reward, done[agent_id], info[agent_id] = self.env.step(agent_id, int(action))\n            info[agent_id] = dict(**info[agent_id])\n            reward[agent_id] = self.change_reward(obs[agent_id], old_reward if old_reward is not None else -1, done[agent_id], agent_id)\n            observation = np.array(obs[agent_id][\"board\"]).reshape(self.rows, self.columns, 1)\n            observation = np.where(observation == obs[agent_id].mark, 1, observation)\n            observation = np.where(observation == obs[agent_id].mark % 2 + 1, -1, observation)\n            obs[agent_id] = observation\n            if done[agent_id] and agent_id == \"agent1\":\n                observation = np.where(observation == -1, 2, observation)\n                observation = np.where(observation == 1, -1, observation)\n                observation = np.where(observation == 2, 1, observation)\n                obs[\"agent2\"] = observation\n                info[\"agent2\"] = {}\n                done[\"agent2\"] = True\n                if old_reward is None or old_reward == -1:\n                    agent2_reward = 1\n                else:\n                    agent2_reward = -1\n                reward[\"agent2\"] = self.change_reward(obs[\"agent2\"], agent2_reward, done[\"agent2\"], agent_id)\n                break\n        done[\"__all__\"] = any(done.values())\n        if done[\"__all__\"]:\n            if reward[\"agent1\"] > reward[\"agent2\"]:\n                self.agent_wins[\"agent1\"] += 1\n            elif reward[\"agent2\"] > reward[\"agent1\"]:\n                self.agent_wins[\"agent2\"] += 1\n            self.n_episodes += 1\n        return obs, reward, done, info\n    \n    def change_reward(self, obs, old_reward: int, done: bool, agent_id: str):\n        if old_reward is None:\n            return -1000\n        elif old_reward == 1:\n            reward = 100.0 + 100.0 * (self.n_cells - self.n_step) \/ self.n_cells \n            return reward\n        elif done:\n            reward = - 100.0 - 100.0 * (self.n_cells - self.n_step) \/ self.n_cells \n            return reward\n        else:\n            reward = 0.0\n        if self.alpha > 0.0:\n            exploration_reward = self.compute_exploration_reward(obs)\n            reward = self.alpha * exploration_reward + (1 - self.alpha) * reward\n        return reward\n    \n    def compute_exploration_reward(self, obs):\n        grid = np.asarray(obs[\"board\"]).reshape(self.rows, self.columns)\n        twos_count = self._count_windows(grid, num_discs=2, piece=obs.mark)\n        threes_count = self._count_windows(grid, num_discs=3, piece=obs.mark)\n        fours_count = self._count_windows(grid, num_discs=4, piece=obs.mark)\n        opp_twos_count = self._count_windows(grid, num_discs=2, piece=obs.mark % 2 + 1)\n        opp_threes_count = self._count_windows(grid, num_discs=3, piece=obs.mark % 2 + 1)\n        exploration_reward = 100*fours_count+ 10*threes_count + 1*twos_count\n        exploration_reward -= 100*opp_threes_count + 10*opp_twos_count\n        return exploration_reward\n    \n    def _count_windows(self, grid, num_discs, piece):\n        num_windows = 0\n        # horizontal\n        for row in range(self.rows):\n            for col in range(self.columns-(self.inarow-1)):\n                window = list(grid[row, col:col+self.inarow])\n                if self._check_window(window, num_discs, piece):\n                    num_windows += 1\n        # vertical\n        for row in range(self.rows-(self.inarow-1)):\n            for col in range(self.columns):\n                window = list(grid[row:row+self.inarow, col])\n                if self._check_window(window, num_discs, piece):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(self.rows-(self.inarow-1)):\n            for col in range(self.columns-(self.inarow-1)):\n                window = list(grid[range(row, row+self.inarow), range(col, col+self.inarow)])\n                if self._check_window(window, num_discs, piece):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(self.inarow-1, self.rows):\n            for col in range(self.columns-(self.inarow-1)):\n                window = list(grid[range(row, row-self.inarow, -1), range(col, col+self.inarow)])\n                if self._check_window(window, num_discs, piece):\n                    num_windows += 1\n        return num_windows\n    \n    def _check_window(self, window, num_discs, piece):\n        return (window.count(piece) == num_discs and window.count(0) == self.inarow-num_discs)\n    \n    def seed(self, initial_state):\n        \"\"\"Method required by rllib\"\"\"\n        pass\n        \n    def update_alpha(self, iteration: int):\n        \"\"\"Compute the new value of the annealing factor given the current iteration number\"\"\"\n        if iteration < self.annealing_factor_last_iteration:\n            self.alpha = 1.0 - iteration \/ self.annealing_factor_last_iteration\n        else:\n            self.alpha = 0.0\n\nregister_env(\"ConnectFourGym-v0\", lambda config: ConnectFourGym(config))","010ca8ff":"environment = ConnectFourGym()","acf067de":"class ConnectFourNetwork(TorchModelV2, nn.Module):\n    def __init__(\n        self, \n        obs_space, \n        action_space, \n        num_outputs, \n        model_config, \n        name,\n        network_width: int = 512,\n        **kwargs,\n    ):\n        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n        nn.Module.__init__(self)\n        \n        self.input_size = obs_space.shape[0] * obs_space.shape[1] * obs_space.shape[2]\n        \n        self.shared_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(self.input_size, network_width),\n            nn.ReLU(),\n            nn.Linear(network_width, network_width),\n            nn.ReLU(),\n        )\n        \n        self.policy_output = nn.Linear(network_width, num_outputs)\n        self.vf_layers = nn.Linear(network_width, 1)\n        \n        # Initialize policy's output layer's weights to very small values\n        # in order to make its output uniform\n        torch.nn.init.xavier_normal_(self.policy_output.weight, gain=1e-5)\n        self.policy_output.bias.data.fill_(1e-3)\n        \n        self._output = None\n        \n    @override(TorchModelV2)\n    def forward(self, input_dict, state, seq_lens):\n        x = input_dict[\"obs\"]\n        x = x.type(torch.float)\n        if len(x.shape) == 3:\n            x = x.unsqueeze(0)\n        self._output = self.shared_layers(x)\n        model_output = self.policy_output(self._output)\n        return model_output, []\n    \n    @override(TorchModelV2)\n    def value_function(self):\n        assert self._output is not None, \"must call forward first!\"\n        return torch.reshape(self.vf_layers(self._output), [-1])\n\n    \nModelCatalog.register_custom_model(\"ConnectFourNetwork\", ConnectFourNetwork)","e01d8b7d":"def restore_model_checkpoint(policy, agent_id: str, path: str):\n    with open(path, \"rb\") as f:\n        checkpoint_data = pickle.load(f)\n    checkpoint_state = pickle.loads(checkpoint_data[\"worker\"])[\"state\"]\n    network_checkpoint_data = checkpoint_state.get(agent_id)\n    del network_checkpoint_data[\"_optimizer_variables\"]\n    \n    for k, v in network_checkpoint_data.items():\n        network_checkpoint_data[k] = torch.from_numpy(v).to(\"cpu\")\n        \n    print(f\"Restoring Model for Agent '{agent_id}' From Checkpoint at '{path}'\")\n    policy.model.load_state_dict(network_checkpoint_data)\n","6ba122c8":"def override_policy_for(self, agent_id: AgentID = _DUMMY_AGENT_ID) -> PolicyID:\n    if agent_id not in self._agent_to_policy:\n        self._agent_to_policy[agent_id] = self._policy_mapping_fn(agent_id, self._agent_to_policy)\n    return self._agent_to_policy[agent_id]","a01a73d1":"class TrainingCallbacks(DefaultCallbacks):\n    def on_episode_start(\n        self, \n        *, \n        worker, \n        base_env, \n        policies,\n        episode,\n        **kwargs\n    ):\n        MultiAgentEpisode.policy_for = override_policy_for\n        episode.custom_metrics[\"alpha\"] = base_env.get_unwrapped()[0].alpha\n        \n    def on_episode_end(\n        self, \n        *, \n        worker, \n        base_env, \n        policies,\n        episode,\n        **kwargs\n    ):\n        episode.custom_metrics[\"agent1_win\"] = sum(x.agent_wins[\"agent1\"] \/ x.n_episodes for x in base_env.get_unwrapped())\n        episode.custom_metrics[\"agent2_win\"] = sum(x.agent_wins[\"agent2\"] \/ x.n_episodes for x in base_env.get_unwrapped())\n\n    def on_train_result(self, *, trainer, result: dict, **kwargs):\n        # Set the environment's training iteration or both training and evaluation workers  \n        # which will in turn be used to update the annealing factor\n        training_iteration = result[\"training_iteration\"]\n        trainer.workers.foreach_worker(\n            lambda worker: worker.foreach_env(\n                lambda env: env.update_alpha(training_iteration)))\n        trainer.evaluation_workers.foreach_worker(\n            lambda ev: ev.foreach_env(\n                lambda env: env.update_alpha(training_iteration)))\n        # Restore a random checkpoint to the 'agent1_checkpoint' and 'agent2_checkpoint'\n        checkpoint_paths = []\n        for path in Path(trainer.logdir).iterdir():\n            if path.is_dir():\n                path = path \/ path.parts[-1].replace(\"_\", \"-\")\n                checkpoint_paths.append(os.fspath(path))\n        if checkpoint_paths:\n            for agent_id in [\"agent1\", \"agent2\"]:\n                trainer.workers.foreach_worker(\n                    lambda worker: worker.for_policy(\n                        restore_model_checkpoint, \n                        policy_id=f\"{agent_id}_checkpoint\", \n                        agent_id=agent_id, \n                        path=random.choice(checkpoint_paths)\n                    )\n                )\n                trainer.evaluation_workers.foreach_worker(\n                    lambda worker: worker.for_policy(\n                        restore_model_checkpoint, \n                        policy_id=f\"{agent_id}_checkpoint\", \n                        agent_id=agent_id, \n                        path=random.choice(checkpoint_paths)\n                    )\n                )\n        # Bring custom metrics to the first level to allow the reporter to find them\n        result[\"agent1_lr\"] = result[\"info\"][\"learner\"][\"agent1\"][\"cur_lr\"]\n        result[\"agent2_lr\"] = result[\"info\"][\"learner\"][\"agent2\"][\"cur_lr\"]\n        result[\"agent1_reward_mean\"] = result[\"policy_reward_mean\"][\"agent1\"]\n        result[\"agent2_reward_mean\"] = result[\"policy_reward_mean\"][\"agent2\"]\n        for k, v in result[\"custom_metrics\"].items():\n            result[k] = v","0a9bca02":"NETWORK_WIDTH = 256\n\nmodel_configuration = {\n    \"model\": {\n        \"custom_model\": \"ConnectFourNetwork\",\n        \"custom_model_config\": {\n            \"network_width\": NETWORK_WIDTH,\n        }\n    }\n}\n    \npolicy_configuration = (\n    None, \n    environment.observation_space, \n    environment.action_space, \n    model_configuration,\n)\n\n\ndef policy_mapping_fn(agent_id: str, agent_to_policy: dict):\n    if not agent_to_policy:\n        # We randomly select the agent that will be kept for certain for this episode\n        # The other agent has a high chance of being replaced with an old checkpoint for this episode\n        agent_to_train = random.choice([\"agent1\", \"agent2\"])\n        # We also randomly select the order\n        agent_ids = [\"agent1\", \"agent2\"]\n        random_agents_order = random.sample(agent_ids, k=2)\n        for agent_id_, policy_id in zip(agent_ids, random_agents_order):\n            if agent_id_ == agent_to_train:\n                agent_to_policy[agent_id_] = policy_id\n            else:\n                agent_to_policy[agent_id_] = f\"{policy_id}_checkpoint\"\n    return agent_to_policy[agent_id]","72e79bcd":"reporter = JupyterNotebookReporter(overwrite=True, max_report_frequency=30)\nreporter.add_metric_column(\"agent1_win_mean\", \"agent1_wins\")\nreporter.add_metric_column(\"agent2_win_mean\", \"agent2_wins\")\nreporter.add_metric_column(\"agent1_reward_mean\")\nreporter.add_metric_column(\"agent2_reward_mean\")\nreporter.add_metric_column(\"alpha_mean\", \"alpha\")\nreporter.add_metric_column(\"episodes_total\")\n\n\nN_ITERATIONS = 100\nANNEALING_FACTOR_LAST_ITERATION = 20\n\n\nconfig = {\n    \"env\": \"ConnectFourGym-v0\",\n    \"env_config\": {\n        \"annealing_factor_last_iteration\": ANNEALING_FACTOR_LAST_ITERATION,\n    },\n    \"callbacks\": TrainingCallbacks,\n    \"multiagent\": {\n        \"policies\": {\n            # the first tuple value is None -> uses default policy\n            # All agents used the same configuration, but they will end with different weights\n            \"agent1\": policy_configuration,\n            \"agent2\": policy_configuration,\n            \"agent1_checkpoint\": policy_configuration,\n            \"agent2_checkpoint\": policy_configuration,\n        },\n        # We only train two agents. The other two will be used to load checkpoints\n        \"policies_to_train\": [\"agent1\", \"agent2\"],\n        \"policy_mapping_fn\": policy_mapping_fn,\n    },\n    \"framework\": \"torch\",\n    \"lr\": 1e-4,\n    \"num_sgd_iter\": 50,\n    \"clip_param\": 0.2,\n    \"vf_clip_param\": 1000.0,\n    \"num_gpus\": 0,\n    \"num_workers\": 1,\n    \"num_envs_per_worker\": 1,\n    \"evaluation_interval\": 50,\n    \"evaluation_num_episodes\": 50,\n    \"log_level\": \"INFO\",\n    \"log_sys_usage\": False,\n    \"seed\": RANDOM_SEED,\n}","1e9aa877":"analysis = tune.run(\n    ppo.PPOTrainer,\n    config=config,\n    local_dir=\".\/results\",\n    name=\"ppo_connect_four\",\n    progress_reporter=reporter,\n    stop={\n        \"training_iteration\": N_ITERATIONS,\n    },\n    checkpoint_freq=10,\n    checkpoint_at_end=True,\n    mode=\"max\",\n    fail_fast=True,\n)","bbbfecc5":"%tensorboard --logdir results\/ppo_connect_four","00fc66dc":"metric = \"win_mean\"","b3ca4901":"agent1_checkpoints = analysis.get_trial_checkpoints_paths(\n    analysis.get_best_trial(f\"agent1_{metric}\", \"max\"),\n    metric=f\"agent1_{metric}\"\n)","bfd6f963":"agent2_checkpoints = analysis.get_trial_checkpoints_paths(\n    analysis.get_best_trial(f\"agent2_{metric}\", \"max\"),\n    metric=f\"agent2_{metric}\"\n)","ac617d25":"last_agent1_checkpoint_dir = Path(agent1_checkpoints[-1][0])\nlast_agent2_checkpoint_dir = Path(agent2_checkpoints[-1][0])\nprint(\"last_agent1_checkpoint_dir: \", last_agent1_checkpoint_dir)\nprint(\"last_agent2_checkpoint_dir: \", last_agent2_checkpoint_dir)","2a24217d":"best_agent1_checkpoint_dir = Path(max(agent1_checkpoints, key=lambda x: x[1])[0])\nbest_agent2_checkpoint_dir = Path(max(agent2_checkpoints, key=lambda x: x[1])[0])\nprint(\"best_agent1_checkpoint_dir: \", best_agent1_checkpoint_dir)\nprint(\"best_agent2_checkpoint_dir: \", best_agent2_checkpoint_dir)","358dc5a5":"BEST_AGENT1_PATH = os.fspath(best_agent1_checkpoint_dir)\nBEST_AGENT2_PATH = os.fspath(best_agent2_checkpoint_dir)","9c845032":"def create_agent(path: str, agent_id: str = \"default_policy\"):\n    with open(path, \"rb\") as f:\n        checkpoint_data = pickle.load(f)\n    checkpoint_state = pickle.loads(checkpoint_data[\"worker\"])[\"state\"]\n    network_checkpoint_data = checkpoint_state.get(agent_id)\n    del network_checkpoint_data[\"_optimizer_variables\"]\n    \n    for k, v in network_checkpoint_data.items():\n        network_checkpoint_data[k] = torch.from_numpy(v).to(\"cpu\")\n    \n    NETWORK_WIDTH = network_checkpoint_data[\"shared_layers.3.weight\"].shape[0]\n    \n    N_INAROW = 4\n    N_ROWS = 6\n    N_COLUMNS = 7\n    \n    action_space = spaces.Discrete(N_COLUMNS)\n    observation_space = spaces.Box(low=-1, high=1, shape=(N_ROWS, N_COLUMNS, 1), dtype=int)\n    \n    print(f\"Restoring Model From Checkpoint at '{path}'\")\n    trained_model = ModelCatalog.get_model_v2(\n        obs_space=observation_space,\n        action_space=action_space,\n        num_outputs=N_COLUMNS,\n        model_config={\n            \"custom_model\": \"ConnectFourNetwork\",\n            \"custom_model_config\": {\n                \"network_width\": NETWORK_WIDTH,\n            }\n        },\n        framework=\"torch\"\n    )\n    trained_model.load_state_dict(network_checkpoint_data)\n\n    def agent(obs, configuration):\n        rows = configuration.rows\n        columns = configuration.columns\n        inarow = configuration.inarow\n        assert inarow == N_INAROW, f\"{inarow} != {N_INAROW}\"\n        agent_mark = obs.mark\n        opponent_mark = agent_mark % 2 + 1\n        obs_shape = (rows, columns, 1)\n        n_actions = columns\n        input_ = np.array(obs.board).reshape(rows,columns,1)\n        input_ = np.where(input_ == agent_mark, 1, input_)\n        input_ = np.where(input_ == opponent_mark, -1, input_)\n        input_ = torch.from_numpy(input_).to(\"cpu\")\n        with torch.no_grad():\n            output = trained_model({\"obs\": input_})\n            distribution = torch.distributions.categorical.Categorical(logits=output[0])\n            action = distribution.sample().item()\n        # We first check if the move is valid\n        # If it is we play it, else we pick a random move\n        is_valid = obs.board[action] == 0\n        if is_valid:\n            return action\n        else:\n            return random.choice([col for col in range(columns) if obs.board[col] == 0])\n    return agent","e2efdcc5":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds\/\/2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds\/\/2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])\/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])\/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","2580a22d":"agent1 = create_agent(path=BEST_AGENT1_PATH, agent_id=\"agent1\")\nagent2 = create_agent(path=BEST_AGENT2_PATH, agent_id=\"agent2\")","88b708a1":"env = make(\"connectx\", debug=True)","dd148587":"env.reset()\nenv.run([agent1, agent2])\nenv.render(mode=\"ipython\")","512b0799":"get_win_percentages(agent1, agent2)","e258da73":"env.reset()\nenv.run([agent1, \"random\"])\nenv.render(mode=\"ipython\")","ad114000":"env.reset()\nenv.run([agent2, \"random\"])\nenv.render(mode=\"ipython\")","1ae890dd":"get_win_percentages(agent1, \"random\")","f9ee2eff":"get_win_percentages(agent2, \"random\")","f8e074ea":"env.reset()\nenv.run([agent1, \"negamax\"])\nenv.render(mode=\"ipython\")","b9cdb234":"env.reset()\nenv.run([agent2, \"negamax\"])\nenv.render(mode=\"ipython\")","19420070":"get_win_percentages(agent1, \"negamax\")","c2cc03a3":"get_win_percentages(agent2, \"negamax\")","d5cf2d26":"ray.shutdown()","1936641f":"### Agent1 vs Agent2","0ca55fd2":"We print here the versions of the important packages for repeatability's sake","05269f53":"# Libraries","49b1b399":"We define here the different configuration values for the training as well as the reporter that will be used to print the results of each iteration","1d73dd64":"In this notebook, we will train a Neural Network based agent using a policy gradient algorithm, [Proximal Policy Optimization (PPO)](https:\/\/arxiv.org\/pdf\/1707.06347.pdf).\n\nInspired by [this paper](https:\/\/arxiv.org\/abs\/1710.03748), we will use a multi-agent environment, we train two agents to play against each other, \nwith a dense exploration reward during the beginning of the training.\n\nBy default, the agents receive a reward only at the end of the game which can hinder their ability to learn to play the game.\n\nThat's why we have decided to augment the game's reward with a dense exploration reward that is linearly decreased according to the equation:\n\n$reward = \\alpha * \\text{exploration_reward} + (1 - \\alpha) * \\text{game_reward}$\n\n$\\alpha$ is the annealing factor that decays linearly with time.\n\nThe exploration reward that will be used is composed of 2 components:\n- A component that rewards the agent ( i.e. positive reward ) for playing discs in consecutive positions \n- A component that punishes the agent ( i.e. negative reward ) for the opponents' discs in consecutive positions\n\nIn all games, the agents will alternate between going first and going second.","9ee4827c":"# Neural Network","a8b18c39":"This is required to enable RLLib's sample compression","844188d0":"# Cleanup","341c5ac0":"This is a hack used in order to be able to check the previous mapped agent policies in order to determine the current one","b3b4c06f":"# Conclusion","a48adfb8":"We initialize ray in order to be able to run the training","a19dd543":"We define some constants that will be used throughout the notebook","2296940d":"# Evaluation","2fdcd94c":"In this competition, we're tasked with creating an agent that can play [Connect Four](https:\/\/en.wikipedia.org\/wiki\/Connect_Four) and, hopefully, win against another agent.\n\nConnect Four is a two-player connection board game, in which the players choose a color and then take turns dropping colored discs into a seven-column, six-row vertically suspended grid. \n\nThe pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\n\n![40B1MGc.gif](attachment:11fde540-39d1-4e99-990d-f8b357758c4c.gif)","26c03478":"The following helper function is used to determine the win percentages of two agents","bb82fa67":"This helper function is used to load checkpoints for the special policies 'agent1_checkpoint' and 'agent2_checkpoint'","d328c5c8":"This is used to create an agent function that will be used to evaluate our trained agent","16efc0ce":"This is a multi-agent environment is the most complicated piece of code in this notebook.\n\nIt contains lots of hacks to allow the use of two agents at the same time, to change the reward and to update the annealing factor.","e45caa17":"We pin the random seed in order to ensure repeatability","6a6cd841":"This class contains callbacks that are used at different points during the training to control certain aspects of it","153be1c5":"# Constants","7164cecc":"Shutdown ray to make sure we don't leave any idle workers","31005989":"We define the Neural Network model used for the policy and the value function","63f27846":"# Training","e141360f":"# Environment","64f31d87":"We test our agents against different agents","72a4c6ae":"### Agent1 vs Negamax & Agent2 vs Negamax","d216ca06":"In this notebook, we have seen how one can take advantage of the MultiAgentEnv class from RLLib to train two agents simultaneously to play ConnectFour.\n\nEven though the results are not as good as expected, due to multiple reasons, it nevertheless is a first step, at least for me, in multi-agent reinforcement learning.\n\nThere are a few things that one could try to improve performance:\n\n* Different network architecture ( Convolutional, Recurrent, ... )\n* Separate networks for the policy and the value function\n* Run a hyper-parameter search to find the best combination of learning rate, network width, annealing factor iterations, etc\n","2d2039e5":"### Agent1 vs Random & Agent2 vs Random","95e13233":"> # Introduction","38d66543":"This following helper function is used to create agents from checkpoints to evaluate our trained agents"}}