{"cell_type":{"0ae5b8ff":"code","1a5659ab":"code","eae3e202":"code","6f06119f":"code","0e846303":"code","052cbb79":"code","bde7ed92":"code","283a998d":"code","2ed90982":"code","9ad4c758":"code","d4e20a0c":"code","e6052faf":"code","b6c366f0":"code","9b0f6c09":"code","a5eee5e7":"code","78f2efc5":"code","10efa21a":"code","527eb481":"code","c63c6e2f":"code","e45be267":"code","4f773e24":"code","d2ec37eb":"code","c1367da7":"code","f815faf2":"code","00a4a06a":"code","791ca3e6":"code","977fe435":"code","e3a257b4":"code","2daef1d2":"code","dfa7f2c4":"code","b3601b46":"code","468db4a6":"code","15ea84d6":"code","4cc11f5c":"code","7e8c19b6":"code","48e44f0e":"code","a9bb3295":"code","24bcb5bd":"code","d230e3c0":"code","a0ba9863":"code","593735dd":"code","92ff1984":"code","142487eb":"code","a3bfc722":"code","a2355ac0":"code","36debdc0":"code","bbe4df9a":"code","1d9776f8":"code","3e2d3007":"code","4152b529":"code","b0ea5821":"code","820e9afb":"code","97c7d566":"code","1723f51b":"code","336e0ec4":"code","5ba42e40":"markdown","b0dc06a7":"markdown","acbc96f7":"markdown","c1244823":"markdown","33e5472d":"markdown","1981dbf9":"markdown","253ead92":"markdown","5f2e765d":"markdown","f86267eb":"markdown","8266a731":"markdown","e69bf1f4":"markdown","7248c196":"markdown","ebf2aa02":"markdown","fbed631f":"markdown","c3e9d618":"markdown","79d73c5a":"markdown","fbe70e0b":"markdown","38cb35ca":"markdown","f5a16c48":"markdown","bd20cb19":"markdown","dcaf4ada":"markdown","1095b89d":"markdown","54965999":"markdown","854d18a7":"markdown","1d66b6f2":"markdown","04d1bb20":"markdown","0bdbed24":"markdown","7de49c86":"markdown","4334c317":"markdown","2793c64c":"markdown","3bb17089":"markdown","36669e07":"markdown","ae8763e6":"markdown","e5bff8b9":"markdown","86551901":"markdown","ed69270f":"markdown","3dbf4550":"markdown","5c9ea747":"markdown","6bdb9e55":"markdown","a83acdf7":"markdown","d847d1cc":"markdown","1e7d802e":"markdown","6f40fde7":"markdown","5a6e8ccf":"markdown","2cd0a2cd":"markdown","b5af5f76":"markdown","76570dba":"markdown","01e2a6a7":"markdown","35381294":"markdown","176a0a4d":"markdown","4cae6f36":"markdown"},"source":{"0ae5b8ff":"from sklearn.metrics import log_loss\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pprint import pprint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,r2_score\nimport warnings\nfrom mlxtend.classifier import StackingClassifier\nimport missingno as msno\nfrom sklearn.ensemble import VotingClassifier\nimport shap\nshap.initjs()\nimport lime\nfrom lime import lime_tabular\nwarnings.simplefilter('ignore')","1a5659ab":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","eae3e202":"insurance_df = pd.read_csv('..\/input\/prudential-life-insurance-assessment\/train.csv.zip', index_col='Id')\ninsurance_df.head()","6f06119f":"insurance_df.shape","0e846303":"insurance_df['Response'].value_counts()","052cbb79":"sns.countplot(x=insurance_df['Response']);","bde7ed92":"#Combining the Categores to 3 categories\ninsurance_df['Modified_Response']  = insurance_df['Response'].apply(lambda x : 0 if x<=7 and x>=0 else (1 if x==8 else -1))","283a998d":"sns.countplot(x= insurance_df['Modified_Response']);","2ed90982":"# Dropping old response columns\ninsurance_df.drop('Response',axis = 1, inplace=True)","9ad4c758":"# Making lists with categorical and numerical features.\ncategorical =  [col for col in insurance_df.columns if insurance_df[col].dtype =='object']\n\nnumerical = categorical =  [col for col in insurance_df.columns if insurance_df[col].dtype !='object']","d4e20a0c":"# Doing count plots for categorical\nfor col in categorical:\n    counts = insurance_df[col].value_counts().sort_index()\n    if len(counts) > 10 and len(counts) < 50 :\n      fig = plt.figure(figsize=(30, 10))\n    elif len(counts) >50 :\n      continue\n    else:\n      fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    counts.plot.bar(ax = ax, color='steelblue')\n    ax.set_title(col + ' counts')\n    ax.set_xlabel(col) \n    ax.set_ylabel(\"Frequency\")\nplt.show()","e6052faf":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_1'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_1'], ax=axes[1])","b6c366f0":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_4'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_4'], ax=axes[1])","9b0f6c09":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_6'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_6'], ax=axes[1])","a5eee5e7":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Family_Hist_4'], ax=axes[0])\nsns.boxplot(insurance_df['Family_Hist_4'], ax=axes[1])","78f2efc5":"# I just checked correlated feature with greater than .8 here \ncorr = insurance_df.corr()\ncorr_greater_than_80 = corr[corr>=.8]\ncorr_greater_than_80","10efa21a":"plt.figure(figsize=(12,8))\nsns.heatmap(corr_greater_than_80, cmap=\"Reds\");","527eb481":"#setting max columns to 200\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)","c63c6e2f":"#checking percentage of missing values in a column\nmissing_val_count_by_column = insurance_df.isnull().sum()\/len(insurance_df)\n\nprint(missing_val_count_by_column[missing_val_count_by_column > 0.4].sort_values(ascending=False))","e45be267":"# Dropping all columns in which greater than 40 percent null values\ninsurance_df = insurance_df.dropna(thresh=insurance_df.shape[0]*0.4,how='all',axis=1)","4f773e24":"# Does not contain important information\ninsurance_df.drop('Product_Info_2',axis=1,inplace=True)","d2ec37eb":"# Data for all the independent variables\nX = insurance_df.drop(labels='Modified_Response',axis=1)\n\n# Data for the dependent variable\nY = insurance_df['Modified_Response']","c1367da7":"# Filling remaining missing values with mean\nX = X.fillna(X.mean())","f815faf2":"# Train-test split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.25, random_state=1)","00a4a06a":"# Check the shape of train dataset\nprint(X_train.shape,Y_train.shape)\n\n# Check the shape of test dataset\nprint(X_test.shape, Y_test.shape)","791ca3e6":"# Utility Functions\ndef check_scores(model, X_train, X_test ):\n  # Making predictions on train and test data\n\n  train_class_preds = model.predict(X_train)\n  test_class_preds = model.predict(X_test)\n\n\n  # Get the probabilities on train and test\n  train_preds = model.predict_proba(X_train)[:,1]\n  test_preds = model.predict_proba(X_test)[:,1]\n\n\n  # Calculating accuracy on train and test\n  train_accuracy = accuracy_score(Y_train,train_class_preds)\n  test_accuracy = accuracy_score(Y_test,test_class_preds)\n\n  print(\"The accuracy on train dataset is\", train_accuracy)\n  print(\"The accuracy on test dataset is\", test_accuracy)\n  print()\n  # Get the confusion matrices for train and test\n  train_cm = confusion_matrix(Y_train,train_class_preds)\n  test_cm = confusion_matrix(Y_test,test_class_preds )\n\n  print('Train confusion matrix:')\n  print( train_cm)\n  print()\n  print('Test confusion matrix:')\n  print(test_cm)\n  print()\n\n  # Get the roc_auc score for train and test dataset\n  train_auc = roc_auc_score(Y_train,train_preds)\n  test_auc = roc_auc_score(Y_test,test_preds)\n\n  print('ROC on train data:', train_auc)\n  print('ROC on test data:', test_auc)\n  \n  # Fscore, precision and recall on test data\n  f1 = f1_score(Y_test, test_class_preds)\n  precision = precision_score(Y_test, test_class_preds)\n  recall = recall_score(Y_test, test_class_preds) \n  \n  \n  #R2 score on train and test data\n  train_log = log_loss(Y_train,train_preds)\n  test_log = log_loss(Y_test, test_preds)\n\n  print()\n  print('Train log loss:', train_log)\n  print('Test log loss:', test_log)\n  print()\n  print(\"F score is:\",f1 )\n  print(\"Precision is:\",precision)\n  print(\"Recall is:\", recall)\n  return model, train_auc, test_auc, train_accuracy, test_accuracy,f1, precision,recall, train_log, test_log\n\n\ndef check_importance(model, X_train):\n  #Checking importance of features\n  importances = model.feature_importances_\n  \n  #List of columns and their importances\n  importance_dict = {'Feature' : list(X_train.columns),\n                    'Feature Importance' : importances}\n  #Creating a dataframe\n  importance_df = pd.DataFrame(importance_dict)\n  \n  #Rounding it off to 2 digits as we might get exponential numbers\n  importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n  return importance_df.sort_values(by=['Feature Importance'],ascending=False)\n\ndef grid_search(model, parameters, X_train, Y_train):\n  #Doing a grid\n  grid = GridSearchCV(estimator=model,\n                       param_grid = parameters,\n                       cv = 2, verbose=2, scoring='roc_auc')\n  #Fitting the grid \n  grid.fit(X_train,Y_train)\n  print()\n  print()\n  # Best model found using grid search\n  optimal_model = grid.best_estimator_\n  print('Best parameters are: ')\n  pprint( grid.best_params_)\n\n  return optimal_model\n\n\n\n# This function will show how a feature is pushing towards 0 or 1\ndef interpret_with_lime(model, X_test):\n  # New data\n  interpretor = lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X_train),\n    feature_names=X_train.columns,\n    mode='classification')\n  \n\n  exp = interpretor.explain_instance(\n      data_row=X_test.iloc[10], \n      predict_fn=model.predict_proba\n  )\n\n  exp.show_in_notebook(show_table=True)\n\n# This gives feature importance\ndef plot_feature_importance(model, X_train):\n  # PLotting features vs their importance factors\n  fig = plt.figure(figsize = (15, 8))\n  \n  # Extracting importance values\n  values =check_importance(model, X_train)[check_importance(model, X_train)['Feature Importance']>0]['Feature Importance'].values\n  \n  \n  # Extracting importance features\n  features = check_importance(model, X_train)[check_importance(model, X_train)['Feature Importance']>0]['Feature'].values\n\n  plt.bar(features, values, color ='blue',\n          width = 0.4)\n  plt.xticks( rotation='vertical')\n  plt.show()","977fe435":"# Number of trees\nn_estimators = [50,80,100]\n\n# Maximum depth of trees\nmax_depth = [4,6,8]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [50,100,150]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [40,50]\n\n# Hyperparameter Grid\nrf_parameters = {'n_estimators' : n_estimators,\n              'max_depth' : max_depth,\n              'min_samples_split' : min_samples_split,\n              'min_samples_leaf' : min_samples_leaf}\n\npprint(rf_parameters)\n\n#finding the best model\nrf_optimal_model = grid_search(RandomForestClassifier(), rf_parameters, X_train, Y_train)","e3a257b4":"# Getting scores from all the metrices\nrf_model, rf_train_auc, rf_test_auc, rf_train_accuracy, rf_test_accuracy,rf_f1, rf_precision,rf_recall,rf_train_log, rf_test_log = check_scores(rf_optimal_model, X_train, X_test )","2daef1d2":"#Getting the feature importance for all the features\ncheck_importance(rf_model, X_train)","dfa7f2c4":"# PLotting only those features which are contributing something\nplot_feature_importance(rf_model, X_train)","b3601b46":"# Interpretting the model using lime\ninterpret_with_lime(rf_model,X_test)","468db4a6":"# Interpretting the model using shaply\nX_shap=X_train\n\nrf_explainer = shap.TreeExplainer(rf_model)\nrf_shap_values = rf_explainer.shap_values(X_shap)\nshap.summary_plot(rf_shap_values, X_shap, plot_type=\"bar\")","15ea84d6":"# Plotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Wt','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, rf_shap_values[0], X_train)","4cc11f5c":"#finding the best model\ngb_parameters ={\n    \"n_estimators\":[5,50,250],\n    \"max_depth\":[1,3,5,7],\n    \"learning_rate\":[0.01,0.1,1]\n}\n\npprint(gb_parameters)\n\ngb_optimal_model = grid_search(GradientBoostingClassifier(), gb_parameters, X_train, Y_train)","7e8c19b6":"# Getting the scpres for all the score metrics used here\ngb_model, gb_train_auc, gb_test_auc, gb_train_accuracy, gb_test_accuracy,gb_f1, gb_precision,gb_recall,gb_train_log, gb_test_log = check_scores(gb_optimal_model, X_train, X_test )","48e44f0e":"# Getting feature importance\ncheck_importance(gb_model, X_train)","a9bb3295":"# PLotting only those features which are contributing something\nplot_feature_importance(gb_model, X_train)","24bcb5bd":"# Interpretting the model using lime\ninterpret_with_lime(gb_model,X_test)","d230e3c0":"# Interpretting the model using shaply\nX_shap=X_train\n\ngb_explainer = shap.TreeExplainer(gb_model)\ngb_shap_values = gb_explainer.shap_values(X_shap)\nshap.summary_plot(gb_shap_values, X_shap, plot_type=\"dot\")","a0ba9863":"#PLotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Product_Info_4','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, gb_shap_values, X_train)","593735dd":"# Parameter grid for xgboost\nxgb_parameters = {'max_depth': [1,3,5], 'n_estimators': [2,5,10], 'learning_rate': [.01 , .1, .5]}\nprint('XGB parameters areL:')\npprint(xgb_parameters)\n#finding the best model\nxgb_optimal_model = grid_search(XGBClassifier(), xgb_parameters, X_train, Y_train)","92ff1984":"# Getting the scores for all the score metrics used here\nxgb_model, xgb_train_auc, xgb_test_auc, xgb_train_accuracy, xgb_test_accuracy,xgb_f1, xgb_precision,xgb_recall,xgb_train_log, xgb_test_log= check_scores(xgb_optimal_model, X_train, X_test )","142487eb":"\n# Getting feature importance\n\ncheck_importance(xgb_model, X_train)","a3bfc722":"# Interpretting the model using shaply\n\nxgb_explainer = shap.TreeExplainer(xgb_model)\nxgb_shap_values = xgb_explainer.shap_values(X_shap)\nshap.summary_plot(xgb_shap_values, X_shap, plot_type=\"dot\")","a2355ac0":"#PLotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Product_Info_4','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, xgb_shap_values, X_train)","36debdc0":"# Parameter grid for Logistic Regression\nsolvers = ['lbfgs']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\nlr_parameters = dict(solver=solvers,penalty=penalty,C=c_values)# define grid search\n\n#finding the best model\nlr_optimal_model = grid_search(LogisticRegression( max_iter=5000), lr_parameters, X_train, Y_train)","bbe4df9a":"# Getting the scores for all the score metrics used here\n\nlr_model, lr_train_auc, lr_test_auc, lr_train_accuracy, lr_test_accuracy,lr_f1, lr_precision, lr_recall,lr_train_log, lr_test_log = check_scores(lr_optimal_model, X_train, X_test )","1d9776f8":"# Making a dataframe with coefficients and the feature names respectively\nimportance_df_lr = pd.concat([ pd.DataFrame(data =((X_train.columns).values).reshape(-1,1), columns = ['Feature']), pd.DataFrame(data =np.round(lr_optimal_model.coef_,2).reshape(-1,1), columns = ['Feature Importance'])], axis=1 )\nimportance_df_lr.sort_values(by=['Feature Importance'],ascending=False, inplace = True)\nimportance_df_lr","3e2d3007":"# Plotting feature vs importance\nfig = plt.figure(figsize = (15, 8))\n\nvalues =importance_df_lr[importance_df_lr['Feature Importance']>0]['Feature Importance'].values\n\nfeatures = importance_df_lr[importance_df_lr['Feature Importance']>0]['Feature'].values\n\nplt.bar(features, values, color ='blue',\n          width = 0.4)\nplt.xticks( rotation='vertical')\nplt.show()","4152b529":"# Interpretting the model using lime\ninterpret_with_lime(lr_model,X_test)","b0ea5821":"# Appending all the models to estimators list\nestimators = []\n\nestimators.append(('logistic', lr_optimal_model))\nestimators.append(('XGB', xgb_optimal_model))\nestimators.append(('GB', gb_optimal_model))\nestimators.append(('rf', rf_optimal_model))\n\n# create the voting model\nvoting_model = VotingClassifier(estimators, voting='soft')\n\nvoting_model.fit(X_train, Y_train)","820e9afb":"# Getting all the scores and errors\nvoting_model, voting_train_auc, voting_test_auc, voting_train_accuracy, voting_test_accuracy, voting_f1, voting_precision, voting_recall, voting_train_log, voting_test_log = check_scores(voting_model, X_train, X_test )","97c7d566":"#Building a stacked classifier\nstacked_classifier = StackingClassifier(classifiers =[lr_optimal_model, xgb_optimal_model, gb_model], meta_classifier = RandomForestClassifier(), use_probas = True, use_features_in_secondary = True)\n\n# training of stacked model\nstacked_model = stacked_classifier.fit(X_train, Y_train)  ","1723f51b":"stacked_model, stacked_train_auc, stacked_test_auc, stacked_train_accuracy, stacked_test_accuracy, stacked_f1, stacked_precision, stacked_recall, stacked_train_log, stacked_test_log = check_scores(stacked_model, X_train, X_test )","336e0ec4":"# Making a dataframe of all the scores for every model\n\nscores_ = [(\"Random Forest\", rf_train_auc, rf_test_auc, rf_train_accuracy, rf_test_accuracy,rf_train_log, rf_test_log,rf_f1, rf_precision, rf_recall),\n(\"Gradient Boosting\",  gb_train_auc, gb_test_auc, gb_train_accuracy, gb_test_accuracy,gb_train_log, gb_test_log,gb_f1, gb_precision,gb_recall,),\n(\"XG Boost\", xgb_train_auc, xgb_test_auc, xgb_train_accuracy, xgb_test_accuracy,xgb_train_log, xgb_test_log,xgb_f1, xgb_precision, xgb_recall),\n(\"Logistic Regression\", lr_train_auc, lr_test_auc, lr_train_accuracy, lr_test_accuracy,lr_train_log, lr_test_log,lr_f1, lr_precision, lr_recall,),\n(\"Voting Classifier\", voting_train_auc, voting_test_auc, voting_train_accuracy, voting_test_accuracy, voting_train_log, voting_test_log, voting_f1, voting_precision, voting_recall),\n(\"Stacked Model\", stacked_train_auc, stacked_test_auc, stacked_train_accuracy, stacked_test_accuracy, stacked_train_log, stacked_test_log, stacked_f1, stacked_precision, stacked_recall)]\n\nScores_ =pd.DataFrame(data = scores_, columns=['Model Name', 'Train ROC', 'Test ROC', 'Train Accuracy', 'Test Accuracy', 'Train Log Loss','Test Log Loss','F-Score', 'Precision','Recall',])\nScores_.set_index('Model Name', inplace = True)\n\nScores_","5ba42e40":"Findings\nMedical keyword 15,medical history 9, Wt, medical history 3 all pushing towards 1.\n\nOrange ones are pusing towards 1.","b0dc06a7":"CONCLUSION:\u00b6\nBMI, weight, Medical_History_23, Medical_History_4 and Medical_Keyword_15 seems to be important features according to random forest.\n\nAlso, only these features are contributing to the model prediction. Some features can be elmininated which are not contributing on further investigation.","acbc96f7":"# Model intrepetability for Gradient Boostong\n* using Lime","c1244823":"# Seperating Categorical and Numerical Variables","33e5472d":"Findings\u00b6\nBMI is pushing models prediction towards 0.\n\nMedical keyword 15 is pushing towards 1. However, medical keyword 4 is pushing towards 0.\n\nAlso, according to feature plot Wt. was in top 5 most important features, same isn't followed here.","1981dbf9":"# Logestic Regression","253ead92":"# Feature importance for Gradient Boosting","5f2e765d":"* Right skewed.\n* Outliers can be seen.","f86267eb":"* Removing unimportant columns","8266a731":"* For product info 4 and wt we see some interesting trend","e69bf1f4":"# Feature importance for XGBOOST","7248c196":"CONCLUSION:\nBMI, weight, Medical_History_23, Medical_History_4 and Medical_Keyword_15 seems to be the most important 5 features according to Gradient boosting.","ebf2aa02":"# Visualising Categorical variabels","fbed631f":"# Stacked Model","c3e9d618":"# Random Forest","79d73c5a":"* Class imbalance can be seen here. Also there 8 categories, lets combine them to 3 categories","fbe70e0b":"# Dependence Plot","38cb35ca":"# Dependence Plots","f5a16c48":"## Conclusion:\u00b6\n* Same trend is seen here.\n* They all are giving similar scores also so it could be that same features are contributing the most thus similar scores.","bd20cb19":"# Checking Null values in a Dataset","dcaf4ada":"# Importance of Random Forest","1095b89d":"# Model intrepebility for XGBOOST\n* using shap","54965999":"* Findings\u00b6\n* With high medical history 23 and low bmi we get class 1","854d18a7":"* Findings\u00b6\n* For low BMI and high medical history 23 we get class as 1.","1d66b6f2":"# Droping old Target variabels","04d1bb20":"# Importing Modules","0bdbed24":"* Again BMI is pushing towards class 0.\n* MEdical history 4 pushing towards class 1.","7de49c86":"* Response 8 has highest values and 3 has the least","4334c317":"# Ploting only which are necessary","2793c64c":"## Final Results\u00b6\n* Gradient Boosting, Voting Classifier and Stacked models are performing really well. Their train and test errors and also the roc scores and f scores are really close and good.","3bb17089":"# Splitting data X and Y","36669e07":"* D3 has the highest frequencies\n* Most of the features here are unbalanced.","ae8763e6":"# Findings\n* Only BMI and medical history 4 pushing towards class 0","e5bff8b9":"CONCLUSION\u00b6\nBMI and Weight are highly correlated, which makes sense also as these 2 features are directly proprtional.\n\nIns_Age and Family_Hist_4, Family_Hist_2 highly correlated\n\nAlthough, I am not going to perform any transformation on any feature or drop any as these are tree based models and they don't get affected by correlation much because of their non parametric nature.","86551901":"# Model interpretability for Logestic Regression\n* Using Lime","ed69270f":"* Conclusion\u00b6\n* And again the same pattern when doing feature importance","3dbf4550":"# Reading DataSet","5c9ea747":"# Max Voting Model","6bdb9e55":"# XGBOOST","a83acdf7":"# visualising Correlation grater than .80","d847d1cc":"# Feature importance for Logestic Regression","1e7d802e":"# Dependence Plot","6f40fde7":"# Gradient Boosting","5a6e8ccf":"## using shape","2cd0a2cd":"# Testing data","b5af5f76":"# Distribution of Target Variabels","76570dba":"# Creating Model and their Accuracy DataFrame","01e2a6a7":"# Model intrepretability from Random Forest\n* Using Lime","35381294":"# Filling Missing values with Mean","176a0a4d":"## Using shape","4cae6f36":"* Still some imbalance can be seen"}}