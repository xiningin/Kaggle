{"cell_type":{"031f1930":"code","920f3e1f":"code","d5a26268":"code","52cc822d":"code","99ea6b5e":"code","d9b6592e":"code","17d31337":"code","ba8ddb49":"code","b4af1a9c":"code","1079b45a":"code","3747d7fd":"code","553993d9":"code","a828ccdd":"code","2f9349bc":"code","643e1002":"code","4fa133ba":"code","092a2ee7":"code","9b88f0c5":"code","1cb21764":"code","299ff8ba":"code","8631f8a0":"code","4d22ee43":"code","4a0549f1":"code","acfb55a8":"code","855ee025":"code","c0f7f681":"code","67064014":"code","b3d5a978":"code","18cb77e1":"code","603fa596":"code","6caf178b":"code","d6ccf9f9":"code","b47a13ca":"code","c8ecf878":"code","3a6a8a4b":"code","61f9e740":"code","f3a3427d":"code","a5d289cb":"code","ce81d558":"code","f3e549f3":"code","06418c48":"code","ed4b973d":"code","b700301d":"code","e3f67aa8":"code","41c732fa":"code","dc5640d1":"code","40c6b4d3":"code","e876c5df":"code","6e58fae0":"code","8e39d9b7":"code","8bba991d":"code","3c473b4d":"code","3a286ea4":"markdown","6b5d327a":"markdown","d8eff7bf":"markdown","aad08c8d":"markdown","4fe28bbd":"markdown","99db0d5b":"markdown","5a1db45a":"markdown","a12a98cd":"markdown","2c24b491":"markdown","6fa5d8be":"markdown","99487afe":"markdown","4ab1d99c":"markdown","5f398382":"markdown","4e410ad6":"markdown","7d401b49":"markdown","521a1ac1":"markdown","c4a3bb79":"markdown","6bbb368b":"markdown","7bde0d06":"markdown","f886b5c0":"markdown","6171f621":"markdown","8af6ff46":"markdown","8dc6e400":"markdown","98fca846":"markdown","2d9210a8":"markdown","735ff108":"markdown","b5ff30e1":"markdown"},"source":{"031f1930":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\ndef name_correction(x):\n    x = x.lower() # all letters lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x","920f3e1f":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport gc\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5a26268":"items = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\ncategories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nsales = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","52cc822d":"print('items')\nprint(items.head());\nprint('categories')\nprint(categories.head());\nprint('sales')\nprint(sales.head());\nprint('shops')\nprint(shops.head());\nprint('test')\nprint(test.head());","99ea6b5e":"print('Items')\nprint(items.shape);\nprint('categories')\nprint(categories.shape);\nprint('sales_train')\nprint(sales.shape);\nprint('shops')\nprint(shops.shape);\nprint('test')\nprint(test.shape);","d9b6592e":"#First of all, I wish to merge all interesting data on a single DF.\nsales.head()","17d31337":"print ('Number of unique values in each column')\nsales.nunique()","ba8ddb49":"#Defining date as datetime\nsales['date'] = pd.to_datetime(sales['date'], format = '%d.%m.%Y')\n\n#Now for a month and year feature. (I find it useful for this kind of data to be grouped by for store, month and year for EDA)\nsales['month'] = sales['date'].dt.month\nsales['year'] = sales['date'].dt.year\n#Feature for day of the week\nsales['day_of_week'] = sales['date'].dt.day_of_week\n\n#Making a revenue feature\nsales['revenue'] = sales.item_price.values * sales.item_cnt_day.values\nsales.head()","b4af1a9c":"sales = reduce_mem_usage(sales, verbose = True)\nsales.info()","1079b45a":"print('Entries by year-month')\nsales.groupby(['year', 'month'])['date'].nunique()","3747d7fd":"sales_agg = sales.groupby(['year', 'month', 'shop_id'])['revenue'].sum().reset_index()\nsales_agg['shop_id'].value_counts()","553993d9":"sns.set_theme(style=\"whitegrid\")\n\nfig = plt.figure(figsize=(50, 50))\n\nfor i, j in enumerate(test.sort_values('shop_id').shop_id.unique()):\n    temp = sales_agg[sales_agg['shop_id'] == j]\n    fig = plt.subplot(10, 5, i+1)\n    sns.lineplot(\n        x = 'month',\n        y = 'revenue',\n        style = \"shop_id\",\n        hue = 'year',\n        ci = None,\n        data = temp).set_xlim(1,12)","a828ccdd":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales.item_price.min(), sales.item_price.max()*1.1)\nsns.boxplot(x=sales.item_price)","2f9349bc":"sales = sales[sales.item_price<100000]\nsales = sales[sales.item_cnt_day<1001]","643e1002":"sales[sales.item_price < 0]","4fa133ba":"median = np.median(sales[sales['item_id'] == 2973].item_price.values)\nsales.loc[sales['item_price'] < 0, 'item_price'] = median\nany(sales['item_price'] < 0)","092a2ee7":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales.loc[sales.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales.loc[sales.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales.loc[sales.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","9b88f0c5":"dataset = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    temp = sales[sales.date_block_num == i]\n    dataset.append( np.array(list( product( [i], temp.shop_id.unique(), temp.item_id.unique() ) ), dtype = np.int16) )\n\ndataset = pd.DataFrame( np.vstack(dataset), columns = cols )\ndataset[\"date_block_num\"] = dataset[\"date_block_num\"].astype(np.int8)\ndataset[\"shop_id\"] = dataset[\"shop_id\"].astype(np.int8)\ndataset[\"item_id\"] = dataset[\"item_id\"].astype(np.int16)\ndataset.sort_values( cols, inplace = True )\ndataset.head()","1cb21764":"#Just getting all cnt info in month aggregation \nitem = sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\nitem.columns = ['item_cnt_month']\nitem.reset_index(inplace=True)\n\ndataset = pd.merge(dataset, item, on = ['date_block_num','shop_id','item_id'], how='left')\ndel(item)\n\n#Filling target nans with 0\ndataset[\"item_cnt_month\"] = dataset[\"item_cnt_month\"].fillna(0).clip(0,20).astype(np.float16)\ndataset.head()","299ff8ba":"#I'll add data to test data in order to concatenate\ntest['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\ntest.head()","8631f8a0":"#Now I'll add test to dataset\ndataset = pd.concat([dataset, test], ignore_index=True, sort=False, keys= ['date_block_num','shop_id','item_id'])\ndataset = reduce_mem_usage(dataset, verbose = True)\n\ndataset.head()","4d22ee43":"print(shops.head()); print(categories.head());","4a0549f1":"#Shop treating\nshops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops = shops[['shop_id','city']]\n\n\n#Categories treating\ncategories['split'] = categories['item_category_name'].str.split('-')\ncategories['type'] = categories['split'].map(lambda x: x[0].strip())\n# if subtype is nan then type\ncategories['subtype'] = categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncategories = categories[['item_category_id','type', 'subtype']]\n\n\n#Items treating\n# split item names by first bracket\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\n# replace special characters and turn to lower case\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\n\n# fill nulls with '0'\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\n\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'p\u0441') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == '\u0440s3' , \"type\"] = \"ps3\"\n\ngroup_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\ndrop_cols = []\nfor cat in group_sum.type.unique():\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)\nitems.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)\n\nitems.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)","acfb55a8":"#Merging everything\ndataset = pd.merge(dataset, items, on = 'item_id', how='left')\ndataset = pd.merge(dataset, shops, on = 'shop_id', how='left')\ndataset = pd.merge(dataset, categories, on = 'item_category_id', how='left')\n\ndataset.head()","855ee025":"del(shops, categories, items, sales_agg)","c0f7f681":"#Label encoding categorical features and adding 2 more calendar features\n\n#Adding 2 calendar features\ndataset['month'] = dataset['date_block_num'] % 12\n\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\ndataset['days'] = dataset['month'].map(days).astype(np.int8)\n\n#Label encoding\ndataset['city'] = LabelEncoder().fit_transform(dataset['city'])\ndataset['subtype'] = LabelEncoder().fit_transform(dataset['subtype'])\ndataset['type'] = LabelEncoder().fit_transform(dataset['type'])\ndataset.tail()","67064014":"#Lagging label column\ndataset = lag_feature( dataset, [1,2,3,6], \"item_cnt_month\" )\ndataset.tail()","b3d5a978":"#Rolling mean item_cnt_month lagged \n#dataset = dataset.sort_values(by = ['date_block_num', 'shop_id', 'item_id'])\n#window = [2]\n#for i in window:\n#    roll_mean = dataset.groupby(['shop_id','item_id'])['item_cnt_month'].transform(lambda s: s.rolling(i).mean())\n#    dataset['rolling_mean_item_cnt_month_window_'+str(i)] = roll_mean\n#del(roll_mean)\n\n#for i in window:\n#    dataset = lag_feature( dataset, [1], 'rolling_mean_item_cnt_month_window_'+str(i) )\n#    dataset.drop(['rolling_mean_item_cnt_month_window_'+str(i)], axis = 1, inplace=True)\n#dataset.tail()","18cb77e1":"#Let's make a mean by month\nmean = dataset.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_item_cnt']\nmean = mean.reset_index()\ndataset = pd.merge(dataset, mean, on=['date_block_num'], how='left')\ndel(mean)\n\n#Lagging\ndataset = lag_feature( dataset, [1,2,3,6], \"avg_by_month_item_cnt\" )\ndataset.drop(columns = ['avg_by_month_item_cnt'], axis = 1, inplace = True)\ndataset.tail()","603fa596":"#Let's make a mean by month \/ id\nmean = dataset.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_item_id_item_cnt']\nmean = mean.reset_index()\ndataset = pd.merge(dataset, mean, on=['date_block_num', 'item_id'], how='left')\ndel(mean)\n\n#Lagging\ndataset = lag_feature( dataset, [1,2,3], \"avg_by_month_item_id_item_cnt\" )\ndataset.drop(columns = ['avg_by_month_item_id_item_cnt'], axis = 1, inplace = True)\ndataset.tail()","6caf178b":"#Now a mean by month \/ shop\nmean = dataset.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_shop_item_cnt']\nmean = mean.reset_index()\n\ndataset = pd.merge(dataset, mean, on=['date_block_num', 'shop_id'], how='left')\ndel(mean)\n\n#Lagging\ndataset = lag_feature( dataset, [1,2,3], \"avg_by_month_shop_item_cnt\" )\ndataset.drop(columns = ['avg_by_month_shop_item_cnt'], axis = 1, inplace = True)\ndataset.tail()","d6ccf9f9":"#Now a mean by month \/ city\nmean = dataset.groupby(['date_block_num', 'city']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_city_item_cnt']\nmean = mean.reset_index()\n\ndataset = pd.merge(dataset, mean, on=['date_block_num', 'city'], how='left')\ndel(mean)\n\n#Lagging\ndataset = lag_feature( dataset, [1], \"avg_by_month_city_item_cnt\" )\ndataset.drop(columns = ['avg_by_month_city_item_cnt'], axis = 1, inplace = True)\ndataset.tail()","b47a13ca":"#Now a mean by month \/ category\nmean = dataset.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_cat_item_cnt']\nmean = mean.reset_index()\n\ndataset = pd.merge(dataset, mean, on=['date_block_num', 'item_category_id'], how='left')\ndel(mean)\n\n#Lagging\ndataset = lag_feature( dataset, [1], \"avg_by_month_cat_item_cnt\" )\ndataset.drop(columns = ['avg_by_month_cat_item_cnt'], axis = 1, inplace = True)\ndataset.tail()","c8ecf878":"#Pice mean grouped by month\nprice = sales.groupby(['item_id']).agg({'item_price': ['mean']})\nprice.columns = ['avg_item_price']\nprice.reset_index(inplace=True)\n\ndataset = pd.merge(dataset, price, on=['item_id'], how='left')\ndel(price)\n\n#Then I'll add price mean grouped by month and id\nprice = sales.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\nprice.columns = ['avg_item_price_month']\nprice.reset_index(inplace=True)\n\ndataset = pd.merge(dataset, price, on=['date_block_num','item_id'], how='left')\ndel(price)\ngc.collect()\n\ndataset.tail()","3a6a8a4b":"#Now we can lag month column to provide price for test set\nlags = [1, 2, 3, 4, 5, 6]\ndataset = lag_feature( dataset, lags, \"avg_item_price_month\" )\nfor i in lags:\n    dataset[\"delta_price_lag_\" + str(i) ] = (dataset[\"avg_item_price_month_lag_\" + str(i)] - dataset[\"avg_item_price\"] ) \/ dataset[\"avg_item_price\"]\n\ndataset[\"delta_price_lag\"] = dataset.apply(select_trends, axis = 1)\ndataset[\"delta_price_lag\"] = dataset.delta_price_lag.astype( np.float16 )\ndataset[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"avg_item_price_month\", \"avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"avg_item_price_month_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\ndataset.drop(features_to_drop, axis = 1, inplace = True)\n\ndataset.tail()","61f9e740":"cache = {}\ndataset['item_shop_last_sale'] = -1\ndataset['item_shop_last_sale'] = dataset['item_shop_last_sale'].astype(np.int8)\nfor idx, row in dataset.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        dataset.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num        \n        \ncache = {}\ndataset['item_last_sale'] = -1\ndataset['item_last_sale'] = dataset['item_last_sale'].astype(np.int8)\nfor idx, row in dataset.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            dataset.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num  \n            \ndataset['item_shop_first_sale'] = dataset['date_block_num'] - dataset.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\ndataset['item_first_sale'] = dataset['date_block_num'] - dataset.groupby('item_id')['date_block_num'].transform('min')\ndataset.tail()","f3a3427d":"#Dropping first year\ndataset = dataset[dataset.date_block_num > 11]","a5d289cb":"gc.collect();","ce81d558":"dataset.info()","f3e549f3":"dataset.head().T","06418c48":"#Saving data in picke (just making sure I'll make the best out of RAM's)\ndataset.to_pickle('dataset.pickle')\ntest.to_pickle('test.pickle')","ed4b973d":"%reset -f","b700301d":"#Importing libraries again\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nwarnings.filterwarnings(\"ignore\")\n\n#Tuning Hyperparameters using lgbm on optuna\ndef objective(trial):\n    \n    param = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'num_leaves': trial.suggest_int('num_leaves', 400, 1023),\n    'min_data_in_leaf':trial.suggest_int('min_data_in_leaf', 0, 50),\n    'feature_fraction':trial.suggest_float('feature_fraction', 0.3, 0.6),\n    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n    'num_rounds': 300,\n    'early_stopping_rounds': 30,\n    'seed': 1,\n    'verbosity' : -1\n    }\n    \n    data, target = dataset.drop('item_cnt_month', axis=1), dataset['item_cnt_month']\n\n    scores = []\n    for cv in tscv:\n\n        train_mask = data['date_block_num'] <= tscv[cv][0]\n        valid_mask = data['date_block_num'] == tscv[cv][1]\n\n        train_x, valid_x = data[train_mask], data[valid_mask]\n        train_y, valid_y = target[train_mask], target[valid_mask]\n\n        train_data = lgb.Dataset(train_x, label = train_y)\n        valid_data = lgb.Dataset(valid_x, label = valid_y)\n\n        m_lgb = lgb.train(param, train_data, valid_sets = [valid_data, train_data], verbose_eval=False, categorical_feature = cat_features) \n        \n        preds = m_lgb.predict(valid_x)\n        pred_labels = np.rint(preds)\n        rmse = np.sqrt(mean_squared_error(valid_y, pred_labels))\n        scores.append(rmse)\n    scores = np.mean(scores)\n    return scores\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e3f67aa8":"dataset = pd.read_pickle('.\/dataset.pickle')\ntest = pd.read_pickle('.\/test.pickle') #I need test indexes\ndataset = reduce_mem_usage(dataset, verbose=True)","41c732fa":"cat_features = [\n    'shop_id',\n    'item_category_id',\n    'city',\n    'month',\n    'name2',\n    'name3'\n]","dc5640d1":"#I won't be using month 34 as it is test data.\n#I'll focus cv on start of first year\ntscv = {\n    'cv1' : [12, 17],\n    'cv2' : [22, 23],\n    'cv3' : [28, 29],\n    'cv3' : [32, 33]\n}","40c6b4d3":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=150)\n    \n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","e876c5df":"#Saving optimized params\nopt_params = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'num_rounds': 1000,\n    'early_stopping_rounds': 30,\n    'seed': 1\n}\n\nfor key, value in trial.params.items():\n        opt_params[key] = [value]","6e58fae0":"# Training data with best parameters but higher interactions\ndata, target = dataset.drop('item_cnt_month', axis=1), dataset['item_cnt_month']\n\ntrain_mask = data['date_block_num'] <= 32\nvalid_mask = data['date_block_num'] == 33\n\ntrain_x, valid_x = data[train_mask], data[valid_mask]\ntrain_y, valid_y = target[train_mask], target[valid_mask]\n\ntrain_data = lgb.Dataset(train_x, label = train_y)\nvalid_data = lgb.Dataset(valid_x, label = valid_y)\n\n\nm_lgb = lgb.train(opt_params, train_data, valid_sets = [valid_data, train_data], early_stopping_rounds = 50, verbose_eval = 20, categorical_feature = cat_features) ","8e39d9b7":"lgb.plot_importance(m_lgb, figsize = (10,14))","8bba991d":"y_test = dataset[dataset['date_block_num'] == 34]\n\npreds = m_lgb.predict(y_test.drop('item_cnt_month', axis = 1)) #this is our leaderboard input\n\npreds = preds.clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": preds\n})\n\nsubmission = submission.sort_values(by = ['ID'])\nsubmission.head()","3c473b4d":"submission.to_csv('lgbm_submission.csv', index=False)","3a286ea4":"First things first, I wanna be sure if all entries corresponds to exactly all days and months in years presented.","6b5d327a":"# **Reading Data**","d8eff7bf":"# **Useful Functions**","aad08c8d":"# **Training Model**","4fe28bbd":"## **Exploratory Data Analysis**","99db0d5b":"# **Hyperparameter tuning**","5a1db45a":"I don't know a single word of russian, all my string manipulation came from those notebooks:\n\nhttps:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\/comments?scriptVersionId=4396431&cellId=14\n\nhttps:\/\/www.kaggle.com\/gordotron85\/future-sales-xgboost-top-3#Modelling\n\n","a12a98cd":"# **Feature Engineering**","2c24b491":"As my cv is inside optuna objetive function, I'll just explain the idea.","6fa5d8be":"There is, I'll fill it with median.","99487afe":"# Save data, reset notebook, load data.\nI do this because there is a lot of residual data that I can't clean using simle del(), normally this would be a separate script.","4ab1d99c":"### Price Features","5f398382":"In raw dataset, we can clearly see the time effect on sales. Also, how we should focus on creating relevant information about prices on time, and treat them for seasonality.","4e410ad6":"Let's see if there are days with price < 0","7d401b49":"One thing interesting to note is that some shops don't have data for more than a year.","521a1ac1":"Our prediction is for one single month, so validation should be the same. I think it will be better to plan splits manually (as it follows)","c4a3bb79":"I'm doing a 4 splits time series cv in order to validate model in different times.\n![image.png](attachment:1a7215b9-eb9e-4c7d-b21e-796648068d2c.png)\n","6bbb368b":"There is alse some duplicated shop names, change it for test and train","7bde0d06":"Notebook pipeline\n* I'll be leaving all functions on top of notebook. I would leave in a separate script, but its a notebook.\n* Routine data loading and exploration.\n* Preprocessing data to explore.\n* EDA.\n* Feature creation based on id's, mean encoding and trend treatment.\n* I'll implement lgbm tuned by optuna for a better score.\n* CV strategy will be a time series split (inside optuna objetive)\n* Using best parameters, train a model, predict and submit.","f886b5c0":"# **Importing Libraries**","6171f621":"# String features","8af6ff46":"As we can see, in 2015, november and december are missing, we have to predict aggregated sum of cnt_item for november in this competition.\n\nNow let's aggregate dataset to look at stores performance by year month in revenue","8dc6e400":"Well, it's definitively some messy time series data.\n* First: some stores didn't exist, in 2013, so there is a lot of missing values.\n* Second: somes stores closed in those 3 years interval.\n* Third: as it is aggregated data, we can't say much about some specific items, but there are probably new items that started being sold and old items that got out of store. We can expect a lot of missing data such as 0 values in cnt column.\n","98fca846":"# Outlier treating\nThere are some outliers that need treatment.","2d9210a8":"### Mean Encoded features","735ff108":"# **Table of Contents**\n1. Useful functions\n2. Libraries importing\n3. Data preprossessing\n4. Exploratory Data Analysis\n5. Feature Engineering\n6. Save data, clear all notebook load data.\n7. Hypeparameter tuning\n8. Final training \n9. Prediction + submission","b5ff30e1":"# **Data preprossessing**\nI'll create a matrix with every possible combination on our hierarchy (item x shop by month) in order to provide a training set similar to test set."}}