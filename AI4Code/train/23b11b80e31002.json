{"cell_type":{"9b042716":"code","7c980178":"code","37cca679":"code","2f2fdf7c":"code","ae43378c":"code","97fa5c08":"code","18462163":"code","028c5356":"code","be7cebea":"code","ec71779f":"code","b2ed0a79":"code","717680e7":"code","6e0b0f42":"code","9d8f5af9":"code","fcc0f093":"code","a41ecfb4":"code","0dbceeca":"code","adf6bf8a":"code","5c6a418a":"code","b1b7f925":"code","c41c55b9":"code","c80302e9":"code","b6537156":"code","2a0423e9":"code","70f41696":"code","ad2ed5fc":"code","d848ee3b":"code","5b0fdeeb":"code","7d9e1cea":"code","372f92f5":"code","515e3d51":"code","89b0bf2e":"code","81bf4390":"code","c8755322":"code","db0b8dd3":"code","e4ae25e2":"markdown","bcd393c2":"markdown","42f9e212":"markdown","2162a8d3":"markdown","7c2ec51e":"markdown"},"source":{"9b042716":"#Importing libraries\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7c980178":"# Reading the data\ndata = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndata.head()","37cca679":"# Dropping the columns which will have no impact in the sentiment analysis\ntrain = data.drop(['keyword', 'location', 'id'], axis= 1)\ntrain.head()","2f2fdf7c":"# Looking at our whole training data shape\ntrain.shape","ae43378c":"#Observe the values and notice that there are use of urls. Also, as these are tweets, so # and @ must be used frequently. Let us remove those\nnp.array(train['text'].values)","97fa5c08":"#Removing urls\nimport re\ndef remove_urls(dataframe):\n    url_pattern = r'https*?:\\\/\\\/.*[\\r\\n]*'\n    texts = dataframe['text'].values\n    for i in range(len(texts)):\n        texts[i] = re.sub(url_pattern, repl= '', string= texts[i]).strip()\n    dataframe['text'] = texts\nremove_urls(train)","18462163":"# Now the urls are removed\nnp.array(train['text'].values)","028c5356":"# removing # and @ symbols\ndef remove_hashtags_and_at_the_rate(dataframe):\n    url_pattern = r'[#@]+'\n    texts = dataframe['text'].values\n    for i in range(len(texts)):\n        texts[i] = re.sub(url_pattern, repl= '', string= texts[i]).strip()\n    dataframe['text'] = texts\nremove_hashtags_and_at_the_rate(train)","be7cebea":"np.array(train['text'].values)","ec71779f":"X = np.array(train['text'].values)\ny = np.array([{ 'cats': { '1': target == 1, '0': target == 0}} for target in train['target']])","b2ed0a79":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, shuffle= True, random_state= 42)\nprint('X_train shape : ', X_train.shape)\nprint('X_test shape : ', X_test.shape)\nprint('y_train shape : ', y_train.shape)\nprint('y_test shape : ', y_test.shape)","717680e7":"# Initialising a blank model\nimport spacy\n    \nnlp = spacy.blank('en')\n\ntextcat = nlp.create_pipe('textcat', config= {'exclusive_classes': True,\n                                              'architechture':'bow'})\n\nnlp.add_pipe(textcat)\ntextcat.add_label('1')\ntextcat.add_label('0')","6e0b0f42":"from spacy.util import minibatch\nimport random\n\ndef train_model(model, training_data):\n    optimizer = nlp.begin_training()\n    \n    losses= {}\n    random.shuffle(training_data)\n    batches = minibatch(training_data, size= 8)\n\n    for batch in batches:\n        texts, labels = zip(*batch)\n        nlp.update(texts, labels, sgd= optimizer, losses= losses)\n        \n    return losses['textcat']","9d8f5af9":"def predict(model, texts): \n    docs = [model.tokenizer(text) for text in texts]\n    textcat = model.get_pipe('textcat')\n    scores, _ = textcat.predict(docs)\n    predicted_class = scores.argmax(axis= 1)\n    \n    return predicted_class","fcc0f093":"from sklearn.metrics import f1_score\n\ndef evaluate(model, texts, labels):\n    \n    predicted_class = predict(model, texts)\n    true_class = [int(label['cats']['1']) for label in labels]\n    correct_predictions = true_class == predicted_class\n    \n    accuracy = correct_predictions.mean()\n    f1 = f1_score(true_class, correct_predictions.astype('int'))\n    \n    return accuracy, f1","a41ecfb4":"n_epochs = 5\n\ntraining_data = list(zip(X_train, y_train))\n\nfor epoch in range(n_epochs):\n    loss = train_model(nlp, training_data)\n    accuracy, f1 = evaluate(nlp, X_test, y_test)\n    print(f'Epoch {epoch+1} Loss : {loss:.3f} Accuracy : {accuracy:.3f}, F1-Score : {f1:.3f}')","0dbceeca":"import spacy\n\nnlp = spacy.load('en_core_web_lg')\n\ntrain_vectors = np.array([nlp(text).vector for text in train['text']])","adf6bf8a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train_vectors, train['target'], test_size= 0.2, shuffle= True, random_state= 42)","5c6a418a":"from sklearn.svm import LinearSVC\n\nsvc = LinearSVC(dual= False, max_iter= 10000, random_state= 1)\nsvc.fit(X_train, y_train)\n\nprint(f'Accuracy Score : {svc.score(X_test, y_test):.3f}')","b1b7f925":"from sklearn.metrics import f1_score\n\nf1_score(y_test, svc.predict(X_test))","c41c55b9":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier( n_jobs= -1)\nxgb.fit(X_train, y_train)\nf1_score(y_test, xgb.predict(X_test))","c80302e9":"from sklearn.svm import SVC\nm = SVC()\nm.fit(X_train, y_train)\nf1_score(y_test, m.predict(X_test))","b6537156":"# final_model1 = SVC()\n# final_model1.fit(train_vectors, train['target'])","2a0423e9":"# test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n# ids = test.id.values\n# test = test.drop(['keyword', 'location', 'id'], axis= 1)\n# remove_urls(test)\n# remove_hashtags_and_at_the_rate(test)\n# test.head()","70f41696":"# test_vectors = np.array([nlp(text).vector for text in test['text']])","ad2ed5fc":"# preds = final_model1.predict(test_vectors)\n# submission = pd.DataFrame(columns= ['id', 'target'], data = zip(ids, preds))\n# submission.head()","d848ee3b":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku\nimport numpy as np","5b0fdeeb":"tokenizer = Tokenizer()\ncorpus = train['text'].values\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index)+1","7d9e1cea":"input_sequences = []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    input_sequences.append(token_list)","372f92f5":"max_seq_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen= max_seq_len, padding= 'pre'))","515e3d51":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(input_sequences, train['target'].values, test_size= 0.2, shuffle= True, random_state= 42)","89b0bf2e":"from keras import backend as K\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n# https:\/\/datascience.stackexchange.com\/questions\/45165\/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model","81bf4390":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length= max_seq_len))\nmodel.add(Bidirectional(LSTM(200, return_sequences= True)))\nmodel.add(Dropout(0.4))\nmodel.add(Bidirectional(LSTM(150)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(total_words\/2, activation= 'relu', kernel_regularizer= regularizers.l2(0.01)))\nmodel.add(Dense(total_words, activation= 'relu'))\nmodel.add(Dense(1, activation= 'sigmoid'))\nmodel.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy', f1_m])","c8755322":"model.fit(X_train, y_train, epochs= 15, validation_data= (X_test, y_test))","db0b8dd3":"from sklearn.metrics import f1_score\np = model.predict(X_test)\np = np.squeeze(p)\n# def limit(x):\n#     return 1 if x > 0.4 else 0\n# p = list(map(limit, p))\nf1_m(y_test.astype('float32'), p.astype('float32'))","e4ae25e2":"The Bag of Words didn't performed well on the data. Hence, we will try some other algorithm.","bcd393c2":"# LSTMS","42f9e212":"# Using Word Embeddings","2162a8d3":"# Bag Of Words","7c2ec51e":"# Loading and Processing Data"}}