{"cell_type":{"053a4932":"code","c55d3360":"code","77421d22":"code","f9764e6f":"code","9721ebbf":"code","cc87db08":"code","15eec69e":"code","28973518":"code","356a1217":"code","824861aa":"code","12c70681":"code","6532ada7":"code","f2922a14":"code","427a8db4":"code","d5207a0a":"code","dfe1e2aa":"code","46640c87":"code","0b2bb7f8":"markdown","29749c8b":"markdown","11deae48":"markdown","92517ebb":"markdown"},"source":{"053a4932":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/horse-or-human.zip \\\n    -O \/tmp\/horse-or-human.zip","c55d3360":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/validation-horse-or-human.zip \\\n    -O \/tmp\/validation-horse-or-human.zip","77421d22":"import os\nimport zipfile\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nfrom google.colab import files\nfrom keras.preprocessing import image\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow import keras","f9764e6f":"class myCallback(tf.keras.callbacks.Callback):\n        def on_epoch_end(self, epoch, logs={}):\n            if(logs.get('accuracy')>.97):\n                print(\"\\nReached 97 % accuracy so cancelling training!\")\n                self.model.stop_training = True","9721ebbf":"local_zip = '\/tmp\/horse-or-human.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/horse-or-human')\nlocal_zip = '\/tmp\/validation-horse-or-human.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/validation-horse-or-human')\nzip_ref.close()","cc87db08":"# Directory with our training horse pictures for training\ntrain_horse_dir = os.path.join('\/tmp\/horse-or-human\/horses')\n\n# Directory with our training human pictures for training\ntrain_human_dir = os.path.join('\/tmp\/horse-or-human\/humans')\n\n# Directory with our training horse pictures for validation\nvalidation_horse_dir = os.path.join('\/tmp\/validation-horse-or-human\/horses')\n\n# Directory with our training human pictures for validation\nvalidation_human_dir = os.path.join('\/tmp\/validation-horse-or-human\/humans')","15eec69e":"train_horse_names = os.listdir(train_horse_dir)\nprint(train_horse_names[:10])\n\ntrain_human_names = os.listdir(train_human_dir)\nprint(train_human_names[:10])\n\nvalidation_horse_hames = os.listdir(validation_horse_dir)\nprint(validation_horse_hames[:10])\n\nvalidation_human_names = os.listdir(validation_human_dir)\nprint(validation_human_names[:10])","28973518":"print('total training horse images:', len(os.listdir(train_horse_dir)))\nprint('total training human images:', len(os.listdir(train_human_dir)))\nprint('total validation horse images:', len(os.listdir(validation_horse_dir)))\nprint('total validation human images:', len(os.listdir(validation_human_dir)))","356a1217":"nrows = 4\nncols = 4\npic_index = 0\n\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 8\nnext_horse_pix = [os.path.join(train_horse_dir, fname) \n                for fname in train_horse_names[pic_index-8:pic_index]]\nnext_human_pix = [os.path.join(train_human_dir, fname) \n                for fname in train_human_names[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_horse_pix+next_human_pix):\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') \n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()\n","824861aa":"# All images will be rescaled by 1.\/255\ntrain_datagen = ImageDataGenerator(rescale=1\/255)\nvalidation_datagen = ImageDataGenerator(rescale=1\/255)\n\n# Flow training images in batches of 128 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(\n        '\/tmp\/horse-or-human\/',  # This is the source directory for training images\n        target_size=(300, 300),  # All images will be resized to 300x300\n        batch_size=128,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\n\n# Flow training images in batches of 128 using train_datagen generator\nvalidation_generator = validation_datagen.flow_from_directory(\n        '\/tmp\/validation-horse-or-human\/',  # This is the source directory for training images\n        target_size=(300, 300),  # All images will be resized to 300x300\n        batch_size=32,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')","12c70681":"callbacks = myCallback()","6532ada7":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","f2922a14":"model.compile(\n                loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['accuracy'])","427a8db4":"history=model.fit(\n                    train_generator,  \n                    validation_data = validation_generator,\n                    epochs=15,\n                    callbacks=[callbacks]\n                  )\n                    \n","d5207a0a":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nepochs = range(1 , len(acc) + 1)\n\nplt.plot(epochs , acc , 'b' , label = 'Training accuracy' )\nplt.plot(epochs , val_acc, 'r' , label = 'Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\n\nplt.legend()\nplt.rc('font', size = 15)\nplt.rc('figure', figsize=[10,10])\nplt.show()\n","dfe1e2aa":"uploaded = files.upload()\n\nfor fn in uploaded.keys():\n \n  # predicting images\n  path = '\/content\/' + fn\n  img = image.load_img(path, target_size=(300, 300))\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n\n  images = np.vstack([x])\n  classes = model.predict(images, batch_size=10)\n  print(classes[0])\n  if classes[0]>0.5:\n    print(fn + \" is a human\")\n  else:\n    print(fn + \" is a horse\")","46640c87":"\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n# Let's prepare a random input image from the training set.\nhorse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\nhuman_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\nimg_path = random.choice(horse_img_files + human_img_files)\n\nimg = load_img(img_path, target_size=(300, 300))  # this is a PIL image\nx = img_to_array(img)  # Numpy array with shape (150, 150, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n\n# Rescale by 1\/255\nx \/= 255\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  if len(feature_map.shape) == 4:\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n    n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n    size = feature_map.shape[1]\n    # We will tile our images in this matrix\n    display_grid = np.zeros((size, size * n_features))\n    for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n      x = feature_map[0, :, :, i]\n      x -= x.mean()\n      x \/= x.std()\n      x *= 64\n      x += 128\n      x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n      display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n    scale = 20. \/ n_features\n    plt.figure(figsize=(scale * n_features, scale))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","0b2bb7f8":"**This is a notebook to classify images between horses and humans**\n\n---\n***(hint) : !!sometimes you will see errors because I used collab to store images so try to open this notebook in collab***\n\n\n---\n\nMy target is to increase the accuracy of classification as mush as possible and try to classify correctly external images \n\n***I used external images to train the model***","29749c8b":"**Visualizing the Convolutions and Pooling  layers :**\n\n---\n\n---\n\n","11deae48":"**This part to upload external image from your computer and test the model :**\n\n***hint :*** ( try to use image > 300*300 pixels because this is the minimum pixels for the model ) \n\n---\n\n\n\n---\n\n","92517ebb":"<a href=\"https:\/\/colab.research.google.com\/github\/mohnabil2020\/machine_learning\/blob\/master\/horse_or_human_classifier.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>"}}