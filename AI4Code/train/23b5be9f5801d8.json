{"cell_type":{"9f242f51":"code","ef144957":"code","835dd66b":"code","67bb4d81":"code","fcad286a":"code","50ee27b1":"code","765ce775":"code","dc5fa4d0":"code","f4565ce5":"code","c6814358":"code","dc4448f8":"code","ebb52ac1":"code","9da07723":"code","e93b99b1":"code","14305c16":"code","9a59c683":"code","2fcb0371":"code","bc684976":"code","9a9e6883":"code","8a835027":"code","1d2b692a":"code","585c79d1":"code","ab992365":"code","4707d02d":"code","4714e4b5":"code","1e7b359b":"code","5d940d45":"code","4f3b13cc":"code","9bbd06a6":"code","a2cf1d3f":"code","844a3cfe":"code","5fa6c312":"code","2e9c3286":"code","98e7b5f1":"code","9dfc62da":"code","9c18b627":"code","79115080":"code","0ed904e7":"code","8c867053":"code","0c251593":"code","6b232674":"code","36c37bd6":"code","3924e85a":"code","668e0ea4":"code","9b71c2cd":"code","6183319d":"code","5dea2cc1":"code","01338a76":"markdown","ddffb312":"markdown","7a9c75a1":"markdown","9eefe96e":"markdown","20ba7ac1":"markdown","1a9b6856":"markdown","d7cc2373":"markdown","21e7279d":"markdown","35d4f2f9":"markdown","45598cb9":"markdown","cd313e08":"markdown","616b2da5":"markdown","11be4b5d":"markdown","9a88651a":"markdown","a535d3ce":"markdown","45f02bc5":"markdown","11b0eed5":"markdown","377e4143":"markdown","dc8ed3d3":"markdown","2642dd54":"markdown","8e439fef":"markdown","36ce56ec":"markdown","8e0ae4f2":"markdown","7327a20d":"markdown","cc02537f":"markdown","8b4d9a29":"markdown","92209d71":"markdown","bf032c4f":"markdown","db6a0c57":"markdown","4c166765":"markdown","5bb24a91":"markdown","9f7b58c9":"markdown","a2ee1f9e":"markdown","ccdebec2":"markdown","8a356c2e":"markdown","f9edf843":"markdown","bf2bd224":"markdown","ad53e993":"markdown","aaa21c14":"markdown","a75711ea":"markdown","485e32a4":"markdown","342e2b2f":"markdown","4e0d6df8":"markdown","bf8bfa5c":"markdown","3b3e6ac8":"markdown","2534d8ad":"markdown","3d7b9006":"markdown","fd135536":"markdown"},"source":{"9f242f51":"# Web scraping, pickle imports\nimport requests\nfrom bs4 import BeautifulSoup\nimport pickle\nimport pandas as pd\n\n# Scrapes transcript data from https:\/\/m.news24.com\ndef url_to_transcript(url):\n    '''Returns transcript data specifically from https:\/\/m.news24.com.'''\n    page = requests.get(url).text\n    soup = BeautifulSoup(page, \"lxml\")\n    text = [p.text for p in soup.find(class_=\"article_content\").find_all('p')]\n    print(url)\n    return text\n\n# URLs of transcript of Ramaphosa's SONA\nurl = 'https:\/\/m.news24.com\/Columnists\/GuestColumn\/full-text-we-are-committed-to-building-an-ethical-state-says-president-cyril-ramaphosa-in-2019-sona-speech-20190620'\n\n# President name\npresident = ['Ramaphosa']","ef144957":"# # Actually request transcript\n# Scraping doesn't seem to work here, so I import a text file with the speech instead :\/\n# Someone please help me out here :) \n#sona = url_to_transcript('https:\/\/m.news24.com\/Columnists\/GuestColumn\/full-text-we-are-committed-to-building-an-ethical-state-says-president-cyril-ramaphosa-in-2019-sona-speech-20190620')\n\nimport os          \nos.chdir('\/kaggle\/')\n# You can list all of the objects in a directory by passing the file path to the os.listdir( ) function:\nos.listdir('\/kaggle\/input')\n#Read in the text file\ntext = []\nsona = open('input\/south-african-sona-2019\/SONA.txt','r')\nif sona.mode == 'r':\n    contents =sona.read()\n    text.append(contents)","835dd66b":"# Have a quick look at the speech\nsona = text.copy()\nsona","67bb4d81":"dict1 = {'Ramaphosa':sona}\n# We are going to change this to key: president, value: string format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n# Combine it!\ndata_combined = {key: [combine_text(value)] for (key, value) in dict1.items()}\ndata_combined","fcad286a":"# We can either keep it in dictionary format or put it into a pandas dataframe\nimport pandas as pd\n# Increase the column width\npd.set_option('max_colwidth',150)\n\ndata_df = pd.DataFrame.from_dict(data_combined).transpose()\ndata_df.columns = ['transcript']\ndata_df = data_df.sort_index()\ndata_df","50ee27b1":"# Let's take a look at the transcript\ndata_df.transcript.loc['Ramaphosa']","765ce775":"# Apply a first round of text cleaning techniques\nimport re\nimport string\n\ndef clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    text = text.lower()\n    # if any of these punctuation marks in (string.punctuation) get rid of it\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # Getting rid of words that contain any numbers in them\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nround1 = lambda x: clean_text_round1(x)","dc5fa4d0":"# Let's take a look at the updated text\ndata_clean = pd.DataFrame(data_df['transcript'].apply(round1))\ndata_clean","f4565ce5":"# Apply a second round of cleaning\ndef clean_text_round2(text):\n    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('ramaphosa', '', text)\n    #text = re.sub('south', '', text)\n    return text\n\nround2 = lambda x: clean_text_round2(x)","c6814358":"# Let's take a look at the updated text\ndata_clean = pd.DataFrame(data_clean.transcript.apply(round2))","dc4448f8":"df = data_clean.copy()","ebb52ac1":"# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words='english')\ndata_cv = cv.fit_transform(df.transcript)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = df.index\ndata_dtm","9da07723":"data = data_dtm.transpose()","e93b99b1":"# Find the top 1000 words said by the president\ntop_dict = {}\nfor c in data.columns:\n    top = data[c].sort_values(ascending=False)\n    top_dict[c]= list(zip(top.index, top.values))","14305c16":"# Look at the most common top words --> add them to the stop word list\nfrom collections import Counter\n\n# Let's first pull out the top 100 words for each comedian\nwords = []\nfor president in data.columns:\n    top = [word for (word, count) in top_dict[president]]\n    for t in top:\n        words.append(t)","9a59c683":"# Let's update our document-term matrix with the new list of stop words\nfrom sklearn.feature_extraction import text \nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Recreate document-term matrix\ncv = CountVectorizer(stop_words='english')\ndata_cv = cv.fit_transform(data_clean.transcript)\ndata_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_stop.index = data_clean.index","2fcb0371":"from wordcloud import WordCloud, STOPWORDS\nfrom scipy.misc import imread\nimport matplotlib.pyplot as plt\nimport nltk\n%matplotlib inline\n\n\nimg1 = imread(\"input\/sa-map\/SA_map.jpg\")\nhcmask1 = img1\n#FIX: double check whether words_except_stop_dist is necessary \nwords_except_stop_dist = nltk.FreqDist(w for w in words if w not in STOPWORDS) \nwordcloud = WordCloud(width = 3000,height = 2000,stopwords=STOPWORDS,background_color='#FBF8EF',mask=hcmask1).generate(\" \".join(words_except_stop_dist))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nfig=plt.gcf()\nfig.set_size_inches(10,12)\nplt.axis('off')\nplt.title(\"Ramaphosa's SONA 2019\",fontsize=22)\nplt.tight_layout(pad=0)\nplt.show()","bc684976":"# Create lambda functions to find the polarity and subjectivity of the transcript\nfrom textblob import TextBlob\n\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ndata_clean['polarity'] = data_clean['transcript'].apply(pol)\ndata_clean['subjectivity'] = data_clean['transcript'].apply(sub)\ndata_clean","9a9e6883":"# Split each routine into 10 parts\nimport numpy as np\nimport math\n\ndef split_text(text, n=20):\n    '''Takes in a string of text and splits into n equal parts, with a default of 10 equal parts.'''\n\n    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n    length = len(text)\n    size = math.floor(length \/ n)\n    start = np.arange(0, length, size)\n    \n    # Pull out equally sized pieces of text and put it into a list\n    split_list = []\n    for piece in range(n):\n        split_list.append(text[start[piece]:start[piece]+size])\n    return split_list","8a835027":"# Let's create a list to hold all of the pieces of text\nlist_pieces = []\nfor t in data_clean.transcript:\n    split = split_text(t)\n    list_pieces.append(split)","1d2b692a":"# Have a look at the first piece of text\nlist_pieces[0][0]","585c79d1":"# Calculate the polarity for each piece of text\npolarity_transcript = []\nfor lp in list_pieces:\n    polarity_piece = []\n    for p in lp:\n        polarity_piece.append(TextBlob(p).sentiment.polarity)\n    polarity_transcript.append(polarity_piece)\n    \npolarity_transcript","ab992365":"plt.rcParams['figure.figsize'] = [16, 12]\n\nplt.plot(polarity_transcript[0])\nplt.plot(np.arange(0,20), np.zeros(20))\nplt.ylim(ymin=-.2, ymax=.3)\nplt.title('SONA 2019 Speech Sentiment over time',fontsize=12)\nplt.show()","4707d02d":"# Input for LDA is the document-term matrix\ndata = data_dtm.copy()","4714e4b5":"# Import the necessary modules for LDA with gensim\nfrom gensim import matutils, models\nimport scipy.sparse","1e7b359b":"# One of the required inputs is a term-document matrix\ntdm = data.transpose()\ntdm.head(6)","5d940d45":"# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\nsparse_counts = scipy.sparse.csr_matrix(tdm)\ncorpus = matutils.Sparse2Corpus(sparse_counts)","4f3b13cc":"# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())","9bbd06a6":"# we need to specify two other parameters as well - the number of topics and the number of passes\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\nlda.print_topics()","a2cf1d3f":"lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\nlda.print_topics()","844a3cfe":"lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\nlda.print_topics()","5fa6c312":"# Let's create a function to pull out nouns from a string of text\nfrom nltk import word_tokenize, pos_tag\n\ndef nouns(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n    is_noun = lambda pos: pos[:2] == 'NN'\n    tokenized = word_tokenize(text)\n    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n    return ' '.join(all_nouns)","2e9c3286":"data_clean","98e7b5f1":"# Apply the nouns function to the transcripts to filter only on nouns\ndata_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\ndata_nouns","9dfc62da":"# Create a new document-term matrix using only nouns\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Re-add the additional stop words since we are recreating the document-term matrix\nadd_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\nstop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n\n# Recreate a document-term matrix with only nouns\ncvn = CountVectorizer(stop_words=stop_words)\ndata_cvn = cvn.fit_transform(data_nouns.transcript)\ndata_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\ndata_dtmn.index = data_nouns.index\ndata_dtmn","9c18b627":"# Create the gensim corpus\ncorpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n\n# Create the vocabulary dictionary\nid2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())","79115080":"# Let's start with 2 topics\nldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\nldan.print_topics()","0ed904e7":"ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\nldan.print_topics()","8c867053":"ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\nldan.print_topics()","0c251593":"# Let's create a function to pull out nouns from a string of text\ndef nouns_adj(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n    tokenized = word_tokenize(text)\n    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n    return ' '.join(nouns_adj)","6b232674":"# Apply the nouns function to the transcripts to filter only on nouns\ndata_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\ndata_nouns_adj","36c37bd6":"# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\ncvna = CountVectorizer(stop_words=stop_words)\ndata_cvna = cvna.fit_transform(data_nouns_adj.transcript)\ndata_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\ndata_dtmna.index = data_nouns_adj.index\ndata_dtmna","3924e85a":"# Create the gensim corpus\ncorpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n\n# Create the vocabulary dictionary\nid2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())","668e0ea4":"ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\nldana.print_topics()","9b71c2cd":"ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\nldana.print_topics()","6183319d":"ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\nldana.print_topics()","5dea2cc1":"# Our final LDA model (for now)\nldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=80)\nldana.print_topics()","01338a76":"### Two Topics:","ddffb312":"## Sentiment of Speech Over Time","7a9c75a1":"### Four Topics:","9eefe96e":"![title](https:\/\/i.ytimg.com\/vi\/C85QPG9uebY\/maxresdefault.jpg)","20ba7ac1":"In this section, we look at both nouns and adjectives.","1a9b6856":"### Four Topics:","d7cc2373":"# Gathering the Data","21e7279d":"# Exploratory Data Analysis","35d4f2f9":"The objective(s) of this analysis include:\n* Data cleaning and preprocessing the data into the various different data formats (i.e Corpus and Document-Term Matrix)\n* An indepth EDA, which will be culminated by a WordCloud, showing the top 1000 words used.  \n* Scoring the overall speech, in terms of sentiment.\n    * **Polarity**: How positive or negative a word is. -1 is very negative. +1 is very positive.\n    * **Subjectivity**: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion.\n* Topic modelling, in which the various topics forming part of the speech. This shall be refined via three methods,\n    * Using all the text in the speech\n    * Using nouns only\n    * Using both nouns and adjectives","45598cb9":"![title](https:\/\/i.udemycdn.com\/course\/750x422\/753140_f3f3_2.jpg)","cd313e08":"### Four Topics","616b2da5":"# Identify the Topics in each Document","11be4b5d":"## Topic Modeling - Attempt #1 (All Text)","9a88651a":"### Summary","a535d3ce":"### Three Topics:","45f02bc5":"To analyse the sentiment of the speech over time, we split the speech up into (n=20) parts, and get a score for each of the 20 parts.","11b0eed5":"To get the full transcript of the speech, we scrape the news24 website using **Requests** and **Beautiful Soup**. To view the full speech, see [news24](https:\/\/m.news24.com\/Columnists\/GuestColumn\/full-text-we-are-committed-to-building-an-ethical-state-says-president-cyril-ramaphosa-in-2019-sona-speech-20190620).","377e4143":"Corpus is simply Latin for \"body\". So, A body of texts is called Corpus and when you have several such collections of texts, you have a Corpora. It contains unstructured natural language text, and is used to apply nlp tasks on, in an attempt to enable machines to better understand this text.","dc8ed3d3":"One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). In this section, we consider the nouns only.","2642dd54":"## Document-Term Matrix","8e439fef":"# Data Cleaning","36ce56ec":"### Three Topics:","8e0ae4f2":"## Topic Modeling - Attempt #2 (Nouns Only)","7327a20d":"These topics aren't making much sense. We've tried modifying our parameters. Let's try modifying our terms list as well.","cc02537f":"# Sentiment Analysis","8b4d9a29":"### Two Topics:","92209d71":"Topic modelling allows us to determine the various topics within the document. To do this, we use a technic called **Latent Dirichlet Allocation** [LDA](https:\/\/blog.echen.me\/2011\/08\/22\/introduction-to-latent-dirichlet-allocation\/). The Goal of LDA is to learn the topic mix in each document, and the word mix in each topic.\n\nHow LDA works?\n* A guess is made as to the number of topics in your corpus (\"body of text\") e.g. k = 3\n* LDA randomnly assigns each word in each document to one of the three topics\n* It then goes through every word and it's topic assignment in each document, and looks at:\n    * How often the topic occurs in the document and,\n    * How often the word occurs in the topic overall.\n    * Based on the above two points, it the assigns the new word a topic.\n* LDA goes through multiple itterations of this, and hopefully the topics it spits out start to make sense to us :)\n\nGensim:\n* **Gensim** finds the best word distribution for each topic and the best topic distribution for each document [Gensim](https:\/\/stackabuse.com\/python-for-nlp-working-with-the-gensim-library-part-1\/).","bf032c4f":"Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there.","db6a0c57":"### Summary:","4c166765":"Since, it's soo much easier to work with dataframes, we convert the dictionary above into a pandas dataframe:","5bb24a91":"A document-term or term-document matrix consists of frequency of terms that exist in a collection of documents. In document-term matrix, rows represent documents in the collection and columns represent terms whereas the term-document matrix is the transpose of it [Quora](https:\/\/www.quora.com\/What-is-a-term-document-matrix).","9f7b58c9":"### Summary","a2ee1f9e":"### Three Topics:","ccdebec2":"## Corpus","8a356c2e":"In this part, we combine the president's name and the actual speech into a dictionary, with the president's name (**Ramaphosa**) as the key.","f9edf843":"# Topic Modelling","bf2bd224":"Out of the 9 topic models we looked at, the nouns and adjectives, certain topics made sense, whereas some din't. So let's pull the topics that made sense down here and run it through some more iterations to get more fine-tuned topics.","ad53e993":"## Topic Modeling - Attempt #3 (Nouns and Adjectives)","aaa21c14":"Going throught the transcript, it's quite easy to see all the unnecessary characters, which won't be used at all in latter algorithms, and therefore in this section, we clean up all these extra characters using regular expressions.","a75711ea":"* In-terms of sentiment, we find that the overall speech has a rather neutral score of ~**0.12**, and a subjectivity of ~**0.40**. In the next section, we see how this sentiment changed over time.","485e32a4":"### Two Topics:","342e2b2f":"The State of the Nation Address of the President of South Africa (abbreviated SONA) is an annual event in the Republic of South Africa, in which the President of South Africa reports on the status of the nation, normally to the resumption of a joint sitting of Parliament (the National Assembly and the National Council of Provinces).\n\nThe speech marks the opening of the parliamentary year and is usually attended by important political and governmental figures of South Africa, including former Presidents, the Chief Justice of the Constitutional Court and other members of the judiciary, the Governor of the Reserve Bank, and Ambassadors and Diplomats to the Republic [Wikipedia](https:\/\/en.m.wikipedia.org\/wiki\/State_of_the_Nation_Address_(South_Africa).","4e0d6df8":"### Summary","bf8bfa5c":"# Objective(s)","3b3e6ac8":"![title](https:\/\/qph.fs.quoracdn.net\/main-qimg-27639a9e2f88baab88a2c575a1de2005)","2534d8ad":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective(s)\" data-toc-modified-id=\"Objective(s)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Objective(s)<\/a><\/span><\/li><li><span><a href=\"#Gathering-the-Data\" data-toc-modified-id=\"Gathering-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Gathering the Data<\/a><\/span><\/li><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Cleaning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Corpus\" data-toc-modified-id=\"Corpus-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Corpus<\/a><\/span><\/li><li><span><a href=\"#Document-Term-Matrix\" data-toc-modified-id=\"Document-Term-Matrix-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Document-Term Matrix<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><\/li><li><span><a href=\"#Sentiment-Analysis\" data-toc-modified-id=\"Sentiment-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Sentiment Analysis<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Sentiment-of-Speech-Over-Time\" data-toc-modified-id=\"Sentiment-of-Speech-Over-Time-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Sentiment of Speech Over Time<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Topic-Modelling\" data-toc-modified-id=\"Topic-Modelling-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Topic Modelling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Topic-Modeling---Attempt-#1-(All-Text)\" data-toc-modified-id=\"Topic-Modeling---Attempt-#1-(All-Text)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Topic Modeling - Attempt #1 (All Text)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Two-Topics:\" data-toc-modified-id=\"Two-Topics:-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;<\/span>Two Topics:<\/a><\/span><\/li><li><span><a href=\"#Three-Topics:\" data-toc-modified-id=\"Three-Topics:-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;<\/span>Three Topics:<\/a><\/span><\/li><li><span><a href=\"#Four-Topics\" data-toc-modified-id=\"Four-Topics-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;<\/span>Four Topics<\/a><\/span><\/li><li><span><a href=\"#Summary:\" data-toc-modified-id=\"Summary:-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;<\/span>Summary:<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Topic-Modeling---Attempt-#2-(Nouns-Only)\" data-toc-modified-id=\"Topic-Modeling---Attempt-#2-(Nouns-Only)-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Topic Modeling - Attempt #2 (Nouns Only)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Two-Topics:\" data-toc-modified-id=\"Two-Topics:-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;<\/span>Two Topics:<\/a><\/span><\/li><li><span><a href=\"#Three-Topics:\" data-toc-modified-id=\"Three-Topics:-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;<\/span>Three Topics:<\/a><\/span><\/li><li><span><a href=\"#Four-Topics:\" data-toc-modified-id=\"Four-Topics:-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;<\/span>Four Topics:<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Topic-Modeling---Attempt-#3-(Nouns-and-Adjectives)\" data-toc-modified-id=\"Topic-Modeling---Attempt-#3-(Nouns-and-Adjectives)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Topic Modeling - Attempt #3 (Nouns and Adjectives)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Two-Topics:\" data-toc-modified-id=\"Two-Topics:-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;<\/span>Two Topics:<\/a><\/span><\/li><li><span><a href=\"#Three-Topics:\" data-toc-modified-id=\"Three-Topics:-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;<\/span>Three Topics:<\/a><\/span><\/li><li><span><a href=\"#Four-Topics:\" data-toc-modified-id=\"Four-Topics:-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;<\/span>Four Topics:<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Identify-the-Topics-in-each-Document\" data-toc-modified-id=\"Identify-the-Topics-in-each-Document-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Identify the Topics in each Document<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/div>","3d7b9006":"* In my opinion, these topics seem to make sense, and these are topics that resonate with me, especially given that South African is our neighbor and largest trading partner.\n\nI note that, the topic modelling especially could be more refined and more analysis could be done in terms of determining how fast (words per minute) president Ramaphosa speaks given the time of how long the speech lasted.","fd135536":"* We find that the 2019 SONA stayed generally positive throughout, with a clearly visible rather negative start and a relatively higher positive finish."}}