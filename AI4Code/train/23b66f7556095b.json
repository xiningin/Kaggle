{"cell_type":{"cc637f42":"code","3483e73e":"code","7c920a15":"code","f9eccee3":"code","a56ef4ef":"code","f00c3011":"code","c47fd891":"code","83ce1693":"code","0fd02a2b":"code","31e8a712":"code","d1102546":"code","f8e4e410":"code","1b58c3c4":"code","63fa7d90":"code","9fdcea90":"code","67452c64":"code","b2d9619f":"code","b61f894f":"code","03452579":"code","7919fc8c":"code","bc855f20":"code","5f934b09":"code","00cc9b1f":"code","e8314d60":"code","a013c161":"code","761b0bf1":"markdown","05c14561":"markdown","36ce8730":"markdown","f569fc5c":"markdown","81b33fc4":"markdown","ccd9bae9":"markdown","a560b49f":"markdown","86818359":"markdown","89cf5517":"markdown","0ace38cd":"markdown","f5fc1448":"markdown","0701b6e8":"markdown","b46b66f3":"markdown","4bd34bb4":"markdown","1127a5d0":"markdown","37579e10":"markdown","f3b6387c":"markdown","e1b9f8b6":"markdown","24491ace":"markdown","666ecdbe":"markdown","041c6c52":"markdown","040ce008":"markdown","6961e5d6":"markdown","0a2089d1":"markdown"},"source":{"cc637f42":"import numpy as np\nimport pandas as pd\nimport tqdm","3483e73e":"data_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(data_train.shape)\nprint(data_test.shape)","7c920a15":"data_train.head()","f9eccee3":"data_train.isnull().sum()","a56ef4ef":"data_test.isnull().sum()","f00c3011":"#function that replaces missing data based on the method and key parameters\ndef fill_nan(data, key, method = \"mean\"):\n    if method == \"mean\":\n        data[key].fillna(data[\"Age\"].mean(), inplace = True)\n    if method == \"mode\":\n        data[key].fillna(data[\"Age\"].mode()[0], inplace = True)\n    if method == \"median\":\n        data[key].fillna(data[\"Age\"].median(), inplace = True)","c47fd891":"#make copys of our data. deep = true means that each entry of our data also gets copied to a different memory address\ndata_train_cleaned = data_train.copy(deep = True)\ndata_test_cleaned = data_test.copy(deep = True)\n\n#calculate stats of our data\ndata_train_cleaned.describe(include = 'all')\ndata_test_cleaned.describe(include = 'all')\n\n#clean data\n#fill empty age\nfill_nan(data_train_cleaned, \"Age\", \"median\")\nfill_nan(data_test_cleaned, \"Age\", \"median\")\n\n#fill empty embarked in train\ndata_train_cleaned[\"Embarked\"].fillna(data_train_cleaned[\"Embarked\"].mode()[0], inplace = True)\n\n#fill empty fare in test\ndata_test_cleaned[\"Fare\"].fillna(data_test_cleaned[\"Fare\"].mean(), inplace = True)","83ce1693":"data_train_cleaned = data_train_cleaned.drop(\"Cabin\", axis = 1)\ndata_test_cleaned = data_test_cleaned.drop(\"Cabin\", axis = 1)","0fd02a2b":"data_train_cleaned = data_train_cleaned.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis = 1)\ndata_test_cleaned = data_test_cleaned.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis = 1)","31e8a712":"print(data_train_cleaned.head())\nprint(data_train_cleaned.isnull().sum())\nprint(data_test_cleaned.isnull().sum())","d1102546":"#map Sex of a passenger to interger values , female : 0 , male : 1\ndata_train_cleaned['Sex'] = data_train_cleaned['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ndata_test_cleaned['Sex'] = data_test_cleaned['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\n#map embarked of a passenger to integer values S: 0, C : 1, Q : 2\ndata_train_cleaned['Embarked'] = data_train_cleaned['Embarked'].map({'S' : 0, 'C' : 1, 'Q': 2}).astype(int)\ndata_test_cleaned['Embarked'] = data_test_cleaned['Embarked'].map({'S' : 0, 'C' : 1, 'Q': 2}).astype(int)\n\n#visualize data\nprint(data_train_cleaned.head())\nprint(data_test_cleaned.head())","f8e4e410":"#make a copy of our data to slice it\nX_data = data_train_cleaned.copy(deep = True).values # .values converst pandas dataframe to a numpy array\n\n#split data into train and val\nX_train = X_data[:623] #70% of our training data 891 is ~ 623 values\nX_val = X_data[623:] #30% of our training data is ~ 268 values\n\n# labels are \" survived \" column of the dataset\nY_train = X_train[:,0]\nY_val = X_val[:,0]\n\n#remove labels from dataset and only keep features\nX_train = np.delete(X_train, 0, axis = 1)\nX_val = np.delete(X_val, 0, axis = 1)\n\n#print data for sanity check\nprint(\"x train : \" + str(X_train))\nprint(\"x val : \" + str(X_val))\nprint(\"y train:\"+ str(Y_train))\nprint(\"y val : \" +str(Y_val))\n\n#print shapes for sanity check\nprint(\"x train :\" + str(X_train.shape))\nprint(\"x val :\" + str(X_val.shape))\nprint(\"y train :\" + str(Y_train.shape))\nprint(\"y val :\" + str(Y_val.shape) )","1b58c3c4":"# rearrange data such that each column is a different training example\nX_train = X_train.T\nX_val = X_val.T\n\n#fix our lable matrix\nY_train = Y_train.reshape((Y_train.shape[0], 1))\nY_val = Y_val.reshape((Y_val.shape[0], 1))\n\n#sanity check\nprint(\"x train :\" + str(X_train.shape))\nprint(\"x val :\" + str(X_val.shape))\nprint(\"y train :\" + str(Y_train.shape))\nprint(\"y val :\" + str(Y_val.shape) )","63fa7d90":"#import matplotlib to plot our sigmoid function\n%matplotlib inline\nimport matplotlib.pyplot as plt\nx = np.linspace(-7, 7, 200)\n\n#define sigmoid function\ndef sigmoid(x):\n    sig = 1\/(1 + np.exp(-x))\n    return sig\n\nplt.plot(x, sigmoid(x))","9fdcea90":"# inorder to standardize our dataset , we will subtract all features of our dataset by their mean\n# and divide it by the standard deviation of the features.\n# this will center the data and normalize our values so that gradient descent converges faster !\ndef calc_stats(data):\n    mu = data.mean(axis = 1, keepdims = True)\n    sigma = data.std(axis = 1, keepdims = True)\n    return mu, sigma\n\ndef standardize(data, mu, sigma):\n    std_data = (data - mu) \/ sigma\n    return std_data\nmu, sigma = calc_stats(X_train)\nX_train = standardize(X_train, mu, sigma)\nX_val = standardize(X_val, mu, sigma)\n\n#sanity check !\nprint(X_train.shape)\nprint(X_train[:5])","67452c64":"# initialize parameters\ndef initialize_parameters(dim):\n    W = np.zeros((dim, 1))\n    b = 0\n    return W, b","b2d9619f":"#forward propagation\ndef forward_prop(X, W, b):\n#     #sanity check\n#     print(\"forward prop\")\n#     print(\"X shape:\" + str(X.shape))\n#     print(\"W shape:\" + str(W.shape))\n    Z = np.dot(W.T, X) + b\n    A = sigmoid(Z)\n    return A.T","b61f894f":"#cost function\ndef compute_cost(Y, A):\n#     #sanity check\n#     print(\"computing cost\")\n#     print(\"Y shape:\" + str(Y.shape))\n#     print(\"A shape:\" + str(A.shape))\n    J = - np.sum(np.dot(Y.T, np.log(A)) + np.dot((1 - Y).T, np.log(1 - A)))\/Y.shape[0]\n    return J","03452579":"# back prop function\ndef back_prop(X, Y, A):\n    #sanity check\n#     print(\"back_prop\")\n#     print(\"X shape:\" + str(X.shape))\n#     print(\"Y shape:\" + str(Y.shape))\n#     print(\"A shape:\" + str(A.shape))\n    dW = np.dot(X, (A-Y))\/X.shape[1]\n    db = np.sum(A - Y)\/X.shape[1]\n    \n    return dW, db","7919fc8c":"#gradient descent\ndef gradient_descent(W, b, dW, db, learning_rate = 0.001):\n    W = W - learning_rate * dW\n    b = b - learning_rate * db\n    return W, b","bc855f20":"# Logistic regression function !\ndef logistic_regression(X, Y, num_iterations, learning_rate, print_cost = False, cost_graph = False):\n    m = X_train.shape[1] #number of training examples\n    W, b = initialize_parameters(X_train.shape[0]) #initialize learning parameters\n    for i in tqdm.tqdm(range(num_iterations)):\n        \n        A = forward_prop(X, W, b)\n        cost = compute_cost(Y, A)\n        dW, db = back_prop(X, Y, A)\n        W, b = gradient_descent(W, b, dW, db, learning_rate)\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))    \n    if cost_graph == True:\n        plt.plot(costs)\n    return W, b","5f934b09":"#make predictions !\ndef predict(X_val, W, b):\n    predictions = forward_prop(X_val, W, b)\n    \n    #map predictions below 0.5 to 0 and above 0.5 to 1\n    predictions[predictions > 0.5] = int(1)\n    predictions[predictions < 0.5] = int(0)\n    return predictions\n\n# calculate accuracy\ndef test_accuracy(predictions, Y_val):\n    accuracy = np.sum(predictions == Y_val)\/predictions.shape[0]*100\n    return accuracy","00cc9b1f":"costs = [] #store cost to plot against iterations\nW, b = logistic_regression(X_train, Y_train, 10000, 0.01, cost_graph = True)","e8314d60":"#make predictions on our validation dataset\npreds_train = predict(X_train, W, b)\npreds_val = predict(X_val, W, b)\n#calculate accuracy of our predictions\nprint(f\"Accuracy on train data {test_accuracy(preds_train, Y_train)}%\")\nprint(f\"Accuracy on validation data {test_accuracy(preds_val, Y_val)}%\")","a013c161":"#import test dataset\nX_test = data_test_cleaned.values\n\n#standardize test dataset\nX_test = X_test.T\nX_test = standardize(X_test, mu, sigma)\n\n#make predictions\npredictions_test = predict(X_test, W, b).astype(int)\n\n#compile into a dataframe\npredictions_df = pd.DataFrame({ 'PassengerId': data_test[\"PassengerId\"], 'Survived': predictions_test[:,0]})\n\n#export dataframe as csv\npredictions_df.to_csv(\"..\/working\/Logistic_regression.csv\", index = False)\n\npredictions_df","761b0bf1":"Finally , lets Run our code !","05c14561":"Now that we have cleaned and prepare our data , we can proceed with programming our logistic regression unit.\n\nLet us first rearrange our data into X_train ( training dataset containng 70% of the data_train_cleaned) , Y_train ( labels of training dataset ), X_val ( our validation dataset which will be 30% of data_train_cleaned ), Y_val ( labels of our validation dataset) ","36ce8730":"Let us visualize our data again !","f569fc5c":"The values of \"Cabin\" is a string which tells the cabin number of the passanger . But since there is really no way to fill missing cabin numbers , we will simply drop that column.","81b33fc4":"Now that we have computed the cost \/ error of our predictions , we can compute the gradients which will be required for our gradient descent.\n\nWe use the chain rule of multivariate calculus to calculate the gradient of our parameters W and B using chain rule.\n\nThe relationship between our parameters $W$ and $b$ and our cost function $J$ can be defined as follows:\n\n$$ Z = W^TX + b $$\n$$ A = \\sigma(Z) $$\n$$ J = -\\frac1m\\sum_{m=1}^M\\ \\bigg[Y  \\log A + (1 - Y)  \\log (1 - A)\\bigg] $$\n\nBy applying chain rule, \n\n$$ \\frac {\\partial J}{\\partial W} = \\frac {\\partial J}{\\partial A} \\frac {\\partial A}{\\partial Z} \\frac {\\partial Z}{\\partial W} $$\n\nAnd, \n\n$$ \\frac {\\partial J}{\\partial b} = \\frac {\\partial J}{\\partial A} \\frac {\\partial A}{\\partial Z} \\frac {\\partial Z}{\\partial b} $$\n\nIf we work through the differentiation, we get the solutions :\n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n\nProof of this solution can be found here : https:\/\/stats.stackexchange.com\/a\/278812\n\nin our code, we will be writing the value $ \\frac{\\partial J}{\\partial w} $ as dW and value $\\frac{\\partial J}{\\partial b}$ as db.\n\nLet use proceed to writing our back propagtion function","ccd9bae9":"Our dataset consists of various Attributes such as PassengerID , Name , Sex , Age etc and the attribute called \"survived\".\nSince we are trying to predict whether or not a passanger survived , we will store these values as labels of our dataset Y and rest of the attributes can be regarded as features.\nBut, first we have to clean up for data and the first step is to look for missing data and fill it.","a560b49f":"Passanger ID , Name of teh passanger and Ticket number donot play a role in survival of a person , so these columns can be dropped.","86818359":"\nOur fill_nan() function will fill missing age values from our dataset depending on the method that we pass as a parameter.\n\n#### Lets try to fix missing embarkation and fare data.\n- Since Embarked is a string which defines what port a person boarded the titanic , the we fill the missing embarkation data with the mode of the dataset.\n\n- Fare data is a float value and again we can just repurpose our fill_nan() function to try out what works best in our case but since only one entry in the test dataset is missing this value , it wont make much of a difference. So , lets just fill it with the mean fare data\n\nBut , before we make any changes to our data , lets copy our data into seperate variables so that we still have our original data incase we mess up !","89cf5517":"Now we are ready to write our logistic regression !\n\n\nAs discussed earlier , Logistic regression can be thought of as a single cell neural network with $ W^{(n)} $ parameters, where $n$ is the number of features and a single bias $b$\n\nWe use sigmoid activation function where $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$\n\nLets take a look at the sigmoid activation function !","0ace38cd":"Now that we have calculated our gradients, we can use gradient descent algorithm to update our parameters.\nGradient Descent : https:\/\/en.wikipedia.org\/wiki\/Gradient_descent","f5fc1448":"Now that we have defined all of our helper functions, we can puttogether our logistic regresion model !","0701b6e8":"After training our logistic regression model , we need to make predictions for our validation dataset and check their accuracy !","b46b66f3":"Let us visualize our dataset by taking a look at first five entries of our data","4bd34bb4":"Now that we have initialized our parameters , we can make a prediction $A$ such that $A = \\sigma(Z)$ where, $Z = W^T.X + b $","1127a5d0":"Let us test the accuracy of our predictions using these parameters !","37579e10":"As we can see there are no Null values in our dataset and we have removed irrelevant data.\nNow , we need to swap the string values of Sex and Embarked by integers inorder to use them as features.","f3b6387c":"Our dataset is in csv format. \nWe use pandas to load the data","e1b9f8b6":"Now that we have Normalized and centered our data , We are ready to write the Forward , backward propagation and gradient descent !\n\nFirst, we write a function to initialize our parameters based on the number of input features","24491ace":"# Kaggle Titanic : Machine Learning from Disaster, Logistic Regression From Scratch !\n\n***\n\nIn this notebook , we will explore the idea of logistic regression as a single cell neural network and write a code to classify the Kaggle Titanic: Machine Learning from Disaster dataset.\n\n\nSince we are trying to write our own logistic regression function , we will not be using any of the popular machine learning library such as scikit learn. We will only be using Numpy for matrix computations and Pandas to handle our data.","666ecdbe":"Now , we use our trained parameters to make predictions on our test dataset and save the results as CSV ! ","041c6c52":"As we can see, the sigmoid function maps values on x axis to values on y axis between 0 and 1.\nThis can be interpreted as the probability of an event occuring.\nIn our case , probability being close to 1 means that the person may have survived the titanic disaster !\n\nA more detailed explanation as to why we use this function, can be found here : https:\/\/qr.ae\/pNnoDa\n\nOther advantages of logistic regression is that for lower values i.e between -2.5 and 2.5 , the gradient of the function is quite large, leading to faster learning through gradient descent !\n\nSince small values mean that our algorithm learns faster , we standardize our dataset which is a form of feature scaling!\nYou can learn more about feature scaling here : https:\/\/en.wikipedia.org\/wiki\/Feature_scaling\n\nEDIT : While standardizing, make sure to scale all the data by the same scaling factor (mu, sigma )!","040ce008":"Our Labels are weird rank 1 arrays and our training sets are arranged such that eash row is a training feature and each column is an example.\n\nLet is rearrange it such that each column of our training set is a different training example and our labels are a one dimentional column vector","6961e5d6":"We are missing a lot of data about the age of passangers from both our training and testing datasets.\nWe are also missing cabin numbers and the embarkation data, but we can fix that.\n\n\n### There are a couple of methods of dealing with missing data:\n- You can simply ignore the data which has missing entries, but if a significant amount of your data is missing then we will endup throwing away a lot of data.\n\n- Replace the missing data by the some value such as mean, median or mode of the rest of the data.\n\nSince we have a training set of 891 examples and a test set of 418 examples and we are missing a lot of age data, we would be throwing away a lot of the rest of the data if we simply remove the data entires with missing age values. Hence , we will try to fill missing value.\nAs to what it should be filled with , we can fill them with mean , mode or median of the.\nThere are other methods such as KNN , MICE or imputation using deeplearning but they are currently beyond our scope.\n\nFor this experiment we can just try them all and see what works best !","0a2089d1":"We will pass the value $Z$ to the sigmoid function that we wrote earlier to make a prediction.\nWe need to use gradient descent algorithm to minimize the error of our prediction.\n\nWe could use mean squared error , but as it turns out , for logistic regression , mean squared error leads\nto a non convex surface. Our gradient descent works best in cases where our error forms a convex surface.\n\n$$ MSE=\\frac{1}{m}\\sum_{i=1}^n(Y_i-\\hat{Y_i})^2 $$\n\nWhere $Y$ is our actual lable , $\\hat{Y}$ is our prediction and $m$ are the number of training examples\n\nFor logistic regression, we typically use something called as cross entropy loss, which leads to a nice convex surface for our gradient descent \n\n$$ J = -\\frac1m\\sum_{m=1}^M\\ \\bigg[y_n  \\log \\hat y_n + (1 - y_n)  \\log (1 - \\hat y_n)\\bigg] $$\n\nLearn More : https:\/\/rohanvarma.me\/Loss-Functions\/"}}