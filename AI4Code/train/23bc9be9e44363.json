{"cell_type":{"e30be197":"code","efc99552":"code","4e62ff1c":"code","b6f9a101":"code","f3aa96d2":"code","89d34fbe":"code","355b0f58":"code","857cc2db":"code","d9c9259d":"code","133a258f":"code","f021f42f":"code","bcab35e7":"code","cb0c7423":"code","26da09b5":"code","d6d3ac16":"code","f9c7c088":"code","d985fee2":"code","825adbc4":"code","b0e2ac26":"code","62f05c07":"code","43254cb9":"code","06b38de7":"code","b44d124a":"code","663841cd":"code","fc43b604":"code","9882bfe0":"code","f67720bf":"code","04fcbe44":"code","6c069c34":"code","06ad5ee0":"code","3570487d":"code","edc053c7":"code","e9b0c806":"code","95b77181":"code","03d4ebaf":"code","3b75f11a":"code","52c28a0e":"code","2905ff88":"code","50f32c98":"code","da7ba173":"code","600ad422":"code","8ad7ca10":"code","30e7b8f5":"code","d3f66e6e":"code","7ca1c69f":"code","4c401abf":"code","d76a76ba":"code","e0b0808a":"code","4dae6714":"code","0ac6ab8f":"code","66741352":"code","e515c3dc":"code","524d287a":"code","43952f01":"code","670f4edd":"code","ab5dd4e6":"code","571f5a38":"code","f4da759a":"code","9287bc31":"code","e4f4df53":"code","930b0787":"code","a8f47036":"code","f822bcc1":"code","f1f855d1":"code","f3e56bf0":"code","aeed57a6":"code","3a15f37a":"code","a2439489":"code","f4bdce17":"code","0ed4afb0":"code","82e86daf":"code","298458eb":"code","51b7a75b":"code","f395f430":"code","b91b4750":"code","0f85dbe2":"code","c3f42c4f":"code","f8f6efde":"code","a525b408":"code","360847e2":"code","4dab1b22":"code","d843200c":"markdown","2b7dc774":"markdown","b78997b9":"markdown","3dbbc83f":"markdown","b69f3fa8":"markdown","1911bb21":"markdown","dce66c2c":"markdown","62af06aa":"markdown","6adb81c0":"markdown","cbad9b40":"markdown","11ffc459":"markdown","5f4e92c9":"markdown","bdf056c1":"markdown","e7460a10":"markdown","a770bd00":"markdown","cdbe4377":"markdown","e43da0ef":"markdown","39ae9f1e":"markdown","93c75960":"markdown","518217ed":"markdown","9f3ccc9e":"markdown","bc240bed":"markdown","5c19a359":"markdown","22f3afc3":"markdown","483d487b":"markdown","3049d6c2":"markdown","6499ebad":"markdown"},"source":{"e30be197":"import pandas as pd\nimport numpy as np\nimport pyarrow.parquet as pq\nnp.random.seed(123456)","efc99552":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')","4e62ff1c":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","b6f9a101":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))","f3aa96d2":"df_train = pd.read_csv('..\/input\/vsb-data-prep-let-s-get-the-party-started\/df_train.csv')\ndf_train.iloc[:,0:12].head()","89d34fbe":"df_test = pd.read_csv('..\/input\/vsb-data-prep-let-s-get-the-party-started\/df_test.csv')\ndf_test.iloc[:,0:12].head()","355b0f58":"df_subm = pd.read_csv('..\/input\/vsb-power-line-fault-detection\/sample_submission.csv')\ndf_subm.head()","857cc2db":"outname = 'target'\npredictors = list(df_train.columns)\npredictors.remove('signal_id')\npredictors.remove('id_measurement')\npredictors.remove(outname)","d9c9259d":"def remove_cols(df,col_to_delete):\n    df_0=df[df['phase']==0]\n    df_0.drop(col_to_delete,axis=1,inplace=True)\n    df_1=df[df['phase']==1]\n    df_1.drop(col_to_delete,axis=1,inplace=True)\n    df_2=df[df['phase']==2]\n    df_2.drop(col_to_delete,axis=1,inplace=True)\n    df_merge=df_0.merge(df_1, on='id_measurement')\n    df_merge=df_merge.merge(df_2, on='id_measurement')\n    return(df_merge)","133a258f":"col_to_delete=['phase','signal_id','ErrFun','ErrGen','Amp0','Amp1','Pha0','Pha1','target']\ndf_train_r=remove_cols(df_train,col_to_delete)\ndf_test_r=remove_cols(df_test,col_to_delete)","f021f42f":"X_df=df_train_r\nXT_df=df_test_r\nX_df.head()","bcab35e7":"y_df=df_train['target'].groupby(by=df_train['id_measurement']).first()","cb0c7423":"X_train_df, X_valid_df, Y_train_df, Y_valid_df = train_test_split(X_df, y_df, test_size=0.2, random_state=123)","26da09b5":"absolute_max=max(max(df_train['max']),max(df_test['max']))\nabsolute_max","d6d3ac16":"absolute_min=min(min(df_train['min']),min(df_test['min']))\nabsolute_min","f9c7c088":"absolute_std=np.mean(df_train['std'])\nabsolute_std","d985fee2":"def damaged_ratio(Y, thr):\n    dr = 100*sum(Y>=thr)\/len(Y)\n    return (dr)","825adbc4":"def to_int_th(x,th,inverse):\n    y = np.zeros(len(x))\n    for i in range(0,len(x)):\n        if (x[i]>=th) :\n            y[i]=1\n        else:\n            y[i]=0\n        if (inverse==1):\n            y[i]=1-y[i]\n    y = y.astype(int)\n    return (y)","b0e2ac26":"X_train_base=np.array(X_train_df.values, copy=True)\nY_train_base=np.array(Y_train_df, copy=True)\nX_valid_base=np.array(X_valid_df.values, copy=True)\nY_valid_base=np.array(Y_valid_df, copy=True)\nX_test_base=np.array(XT_df.values, copy=True)","62f05c07":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_train_base)\nX_train = sc.transform(X_train_base) # label: Y_train_base (unscaled)\nX_valid = sc.transform(X_valid_base) # label: Y_valid_base (unscaled)\nX_test = sc.transform(X_test_base) # label: our goal ;-)","43254cb9":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","06b38de7":"model = keras.Sequential([\n    layers.Dense(12, activation=\"relu\"),\n    layers.Dense(1, activation=\"relu\")\n])","b44d124a":"NR_EPOCHS=5","663841cd":"model.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metrics=['accuracy'])","fc43b604":"history = model.fit(X_train, Y_train_base, \n                    validation_data=[X_valid, Y_valid_base], epochs=NR_EPOCHS)","9882bfe0":"Y_valid1 = model.predict(X_valid)\nY_valid1.shape","f67720bf":"Y_valid1_df=pd.DataFrame(Y_valid1)\nY_valid1_df.describe()","04fcbe44":"sns.distplot(Y_valid1, color='blue',bins=10)","6c069c34":"th1=(np.min(Y_valid1)+np.max(Y_valid1))\/2 # damaged if > th1\nY_valid1_int=to_int_th(Y_valid1,th1,0)","06ad5ee0":"metrics.confusion_matrix(Y_valid_base,Y_valid1_int)","3570487d":"def mmc(y_real_int, y_calc_int):\n    cm = metrics.confusion_matrix(y_real_int,y_calc_int)\n    tp = cm[0,0]\n    tn = cm[1,1]\n    fp = cm[0,1]\n    fn = cm[1,0]\n    num = tp*tn-fp*fn\n    den = np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n    if den==0:\n        mc=-1\n    else:\n        mc=num\/den\n    return np.float64(mc)","edc053c7":"mmc(Y_valid_base,Y_valid1_int)","e9b0c806":"metrics.accuracy_score(Y_valid_base,Y_valid1_int)","95b77181":"Y_pred = model.predict(X_test)","03d4ebaf":"Y_pred.shape","3b75f11a":"sns.distplot(Y_pred,bins=10)","52c28a0e":"Y_pred_int=to_int_th(Y_pred,th1,0)\nnp.unique(Y_pred_int,return_counts=True)","2905ff88":"damaged_ratio(Y_train_base,0.5)","50f32c98":"def find_thres(y_real, y_calc):\n    thr_ndiv=100\n    y_min=np.min(y_calc)\n    y_max=np.max(y_calc)\n    start_thres = (y_min+y_max)\/2 # default, better than 0\n    stop_thres = y_max\n    opt_thres=start_thres\n    opt_mmc = -1\n    dthr=(stop_thres-start_thres)\/thr_ndiv\n    if (dthr==0):\n        vec_thres = np.arange(start_thres, stop_thres+0.1,0.1)\n    else:\n        vec_thres = np.arange(start_thres,stop_thres, dthr)\n    for thres in vec_thres:\n        y_calc_int=to_int_th(y_calc,thres,0)\n        m = mmc(y_real,y_calc_int)\n        if (m > opt_mmc):\n            opt_mmc = m\n            opt_thres = thres\n    print('opt. thres={t:.5f} mmc={m:.5f}'.format(t=opt_thres,m=opt_mmc))\n    return opt_thres","da7ba173":"import gc\nimport pyarrow.parquet as pq","600ad422":"metadata_train = pd.read_csv(\"..\/input\/vsb-power-line-fault-detection\/metadata_train.csv\")\nmetadata_train.info()","8ad7ca10":"row_nr=800000\nrow_nr","30e7b8f5":"row_group_size=4000","d3f66e6e":"time_sample_idx=np.arange(0,row_nr,row_group_size)\ntime_sample_idx[0:8]","7ca1c69f":"metadata_test = pd.read_csv(\"..\/input\/vsb-power-line-fault-detection\/metadata_test.csv\")\nmetadata_test.info()","4c401abf":"sign_start=min(metadata_test['signal_id'])\nsign_start","d76a76ba":"sign_stop=max(metadata_test['signal_id'])+1\nsign_stop","e0b0808a":"sign_group_size=2000","4dae6714":"def scale(val,orig_min,orig_max,des_min,des_max):\n    X_std = (val - orig_min) \/ (orig_max - orig_min)\n    X_scaled = X_std * (des_max - des_min) + des_min\n    return(X_scaled)","0ac6ab8f":"def y_line(x,x1,y1,x2,y2):\n    if (x1==x2):\n        y=(y1+y2)\/2\n    else:\n        m=(y1-y2)\/(x1-x2)\n        q=y1-m*x1\n        y=m*x+q\n    return (y)    ","66741352":"def extract_signal_features(signal_id,file_i,time_sample_idx,abs_max,abs_min):\n    feat_nr=6\n    signal_features=np.zeros((len(time_sample_idx),6))\n    for j in range(0,len(time_sample_idx)-1):\n        file_i_range_j = file_i.iloc[time_sample_idx[j]:time_sample_idx[j+1],signal_id]\n        nr_ts=5\n        x1=time_sample_idx[j]\n        y1=np.mean(file_i.iloc[x1:x1+nr_ts,signal_id])\n        x2=time_sample_idx[j+1]\n        y2=np.mean(file_i.iloc[x2-nr_ts:x2,signal_id])\n        x1=x1+nr_ts\/2\n        x2=x2-nr_ts\/2\n        range_mean = np.mean(file_i_range_j)\n        x_min=file_i_range_j.idxmin()\n        range_min = np.min(file_i_range_j)\n        err_min =range_min-y_line(x_min,x1,y1,x2,y2)\n        x_max=file_i_range_j.idxmax()\n        range_max = np.max(file_i_range_j)\n        err_max=range_max-y_line(x_max,x1,y1,x2,y2)\n        range_std = np.std(file_i_range_j)\n        if (range_std==0):\n            err_rel_rng=0\n            err_abs_rng=0\n        else:\n            err_rel_rng=(err_max-err_min)\/range_std\n            err_abs_rng=err_max-err_min\n        prc_low=np.percentile(file_i_range_j,5)\n        prc_high=np.percentile(file_i_range_j,95)\n        sign_feat = np.array([range_mean,\n                        range_std,\n                        err_rel_rng,\n                        err_abs_rng,\n                        prc_low,\n                        prc_high])\n        signal_features[j]=sign_feat\n    return signal_features    ","e515c3dc":"def fill_ar_samples(filepath,sign_start,sign_stop,sign_group_size,row_nr,time_sample_idx,\n                   abs_max,abs_min):\n    time_samples_str=[str(idx) for idx in time_sample_idx]\n    feat_nr=6\n    samples_ar=np.zeros((sign_stop-sign_start,len(time_sample_idx),feat_nr))\n    col_id_start=sign_start\n    n_groups = int(np.round((sign_stop-sign_start)\/sign_group_size))+1\n    print('Steps = {}'.format(n_groups))\n    for i in range(0,n_groups):\n        col_id_stop = np.minimum(col_id_start+sign_group_size,sign_stop)\n        col_numbers = np.arange(col_id_start,col_id_stop)\n        print('Step {s} - cols = [{a},{b})'.format(s=i,a=col_id_start,b=col_id_stop))\n        col_names = [str(c_num) for c_num in col_numbers]\n        file_i = pq.read_pandas(filepath,columns=col_names).to_pandas()\n        for c in col_names:\n            if (int(c)%50==0):\n                print('.',end='')\n            col=int(c)-col_id_start\n            feat = extract_signal_features(col,file_i,time_sample_idx,abs_max,abs_min)\n            samples_ar[int(c)-sign_start] = feat\n        col_id_start=col_id_stop\n        print('')\n    return (samples_ar)","524d287a":"%%time\ntrain_samples=fill_ar_samples('..\/input\/vsb-power-line-fault-detection\/train.parquet',0,sign_start,sign_group_size,row_nr,time_sample_idx,absolute_max,absolute_min)\ntrain_samples.tofile('train.npy')\ntrain_samples.shape","43952f01":"def ar_compacted_phases(ar_samples,df_metadata,start_id_meas):\n    nr_id_meas=int(ar_samples.shape[0]\/3)\n    nr_samples=ar_samples.shape[1]\n    nr_feats_per_phase=ar_samples.shape[2]\n    ar_measures=np.zeros((nr_id_meas,nr_samples,3*nr_feats_per_phase))\n    for idx_signal in range(0,len(ar_samples)):\n        idx_meas=df_metadata['id_measurement'].loc[idx_signal]-start_id_meas\n        idx_sample=int(idx_signal%nr_samples)\n        for phase in range(0,3):\n            f_start=int(phase*nr_feats_per_phase)\n            f_stop=int(f_start+nr_feats_per_phase)\n            ar_measures[idx_meas,\n                idx_sample,\n                f_start:f_stop]=ar_samples[idx_signal,idx_sample,0:nr_feats_per_phase]\n    return ar_measures","670f4edd":"train_cf=ar_compacted_phases(train_samples,metadata_train,0)\ntrain_cf.shape","ab5dd4e6":"y_cf=y_df.values\ny_cf","571f5a38":"%%time\ntest_samples=fill_ar_samples('..\/input\/vsb-power-line-fault-detection\/test.parquet',\n                             sign_start,sign_stop,sign_group_size,row_nr,time_sample_idx,\n                             absolute_max,absolute_min)\ntest_samples.tofile('test.npy')\ntest_samples.shape","f4da759a":"id_meas_start=min(metadata_test['id_measurement'])\ntest_cf=ar_compacted_phases(test_samples,metadata_test,id_meas_start)\ntest_cf.shape","9287bc31":"from keras.layers import *\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import *","e4f4df53":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","930b0787":"def matthews_correlation(y_true, y_pred):\n    '''Calculates the Matthews correlation coefficient measure for quality\n    of binary classification problems.\n    '''\n    y_pred = tf.convert_to_tensor(y_pred, np.float32)\n    y_true = tf.convert_to_tensor(y_true, np.float32)\n    \n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n\n    tp = K.sum(y_pos * y_pred_pos)\n    tn = K.sum(y_neg * y_pred_neg)\n\n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg)\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator \/ (denominator + K.epsilon())","a8f47036":"def model_lstm(input_shape):\n    inp = Input(shape=(input_shape[1], input_shape[2]))\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = Attention(input_shape[1])(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n    return model","f822bcc1":"CV_STEPS=8\nNR_EPOCHS=40","f1f855d1":"ar_cv=np.arange(0,CV_STEPS)\nep_cv=np.arange(0,NR_EPOCHS)\nmi=pd.MultiIndex.from_product([ar_cv,ep_cv], names=['cv','epoch'])\ndf=pd.DataFrame(index=mi,columns=['loss','val_loss','matthews_correlation','val_matthews_correlation'])","f3e56bf0":"KF = KFold(n_splits=CV_STEPS, shuffle=True)","aeed57a6":"opt_thr=np.zeros(CV_STEPS)\nfor k in range(0,CV_STEPS):\n    print('Step {}'.format(k))\n    train_y=df_train[outname].values\n    X_train, X_valid, y_train, y_valid = train_test_split(train_cf, y_cf, test_size=1\/CV_STEPS)\n    w_file_name='weights_best_{}.hdf5'.format(k)\n    model = model_lstm(X_train.shape)\n    ckpt = ModelCheckpoint(w_file_name, save_best_only=True, verbose=1,\n                           save_weights_only=True, monitor='val_matthews_correlation', \n                           mode='max')\n    history=model.fit(X_train, y_train, epochs=NR_EPOCHS, batch_size=128, shuffle=True,\n          validation_data=[X_valid, y_valid],callbacks=[ckpt])\n    if (os.path.exists(w_file_name)):\n        print('weight file loaded...')\n        model.load_weights(w_file_name)\n    y_valid1=model.predict(X_valid)\n    opt_thr[k]=find_thres(y_valid, y_valid1)\n    df['loss'].loc[k].iloc[0:NR_EPOCHS]=history.history['loss']\n    df['val_loss'].loc[k].iloc[0:NR_EPOCHS]=history.history['val_loss']\n    df['matthews_correlation'].loc[k].iloc[0:NR_EPOCHS]=history.history['matthews_correlation']\n    df['val_matthews_correlation'].loc[k].iloc[0:NR_EPOCHS]=history.history['val_matthews_correlation']","3a15f37a":"h=history.history\nprint(h.keys())","a2439489":"fig,ax = plt.subplots(1,2, figsize=(10,5))\nfor k in range(0,CV_STEPS):\n    loss=df['loss'].loc[k]\n    val_loss=df['val_loss'].loc[k]\n    ax[0].plot(loss, color='red')\n    ax[1].plot(val_loss, color='blue')","f4bdce17":"fig,ax = plt.subplots(1,2, figsize=(10,5))\nmmc_train_last=np.zeros(CV_STEPS)\nmmc_valid_last=np.zeros(CV_STEPS)\nfor k in range(0,CV_STEPS):\n    mc=df['matthews_correlation'].loc[k]\n    val_mc=df['val_matthews_correlation'].loc[k]\n    ax[0].plot(mc, color='red')\n    ax[1].plot(val_mc, color='blue')\n    mmc_train_last[k]=df['matthews_correlation'].loc[k].loc[NR_EPOCHS-1]\n    mmc_valid_last[k]=df['val_matthews_correlation'].loc[k].loc[NR_EPOCHS-1]\nm=np.mean(mmc_train_last)\ns=np.std(mmc_train_last) \nprint('matthews_correlation mean={m} std={s}'.format(k=k,m=m,s=s))\nm=np.mean(mmc_valid_last)\ns=np.std(mmc_valid_last) \nprint('val_matthews_correlation mean={m} std={s}'.format(k=k,m=m,s=s))","0ed4afb0":"sns.distplot(y_valid1,color='green')","82e86daf":"thr_avg=np.mean(opt_thr)\nthr_std=np.std(opt_thr)\nprint('threshold={av} std={st}'.format(av=thr_avg,st=thr_std))\nfin_thr=thr_avg","298458eb":"y_valid1_int=to_int_th(y_valid1,fin_thr,0)\nmetrics.confusion_matrix(y_valid,y_valid1_int)","51b7a75b":"%%time\nar_pred = np.zeros((len(test_cf),CV_STEPS))\nfor k in range(0,CV_STEPS):\n    w_file_name='weights_best_{}.hdf5'.format(k)\n    model1 = model_lstm(X_train.shape)\n    model1.load_weights(w_file_name)\n    y_pred1=model1.predict(test_cf)\n    ar_pred[:,k]=np.squeeze(y_pred1)","f395f430":"y_pred=ar_pred.mean(axis=1)","b91b4750":"sns.distplot(y_pred,color='green')","0f85dbe2":"y_pred_int=to_int_th(y_pred,fin_thr,0)","c3f42c4f":"np.unique(y_pred_int,return_counts=True)","f8f6efde":"XT_df['max']=y_pred_int\ndf_pred=XT_df[['id_measurement','max']]\ndf_pred.columns=['id_measurement','target']\ndf_pred.head()","a525b408":"df_subm=df_test[['signal_id','id_measurement']].merge(df_pred, on='id_measurement')\ndf_subm.drop('id_measurement',axis=1,inplace=True)\ndf_subm.head()","360847e2":"sum(df_subm['target'])","4dab1b22":"df_subm.to_csv('submission.csv', index=False)","d843200c":"Now, some functions to scale and interpolate data;","2b7dc774":"Other considerations and (maybe) useful functions:","b78997b9":"## Introduction","3dbbc83f":"We defined a very simple network with two layes.  We respect the \"rule of the triangle\", which says that every layer has to have less nodes (neurons) than the previous one and we finished with 2 nodes, as we have two final classes: 0=healty and 1=damaged. The input layer implement an I\/O transformation such as 18->12: each element of X_train (array 2D) is an array of 18 elements, which is transformed in a 12 elements array and finally in a 2-element one.","b69f3fa8":"ReLU and Sigmoid are activation functions, something that maps one interval (for example (-inf,+inf)) in another one (for example (0,1)). After the topology we have to define the optimizer (how the model try to minimize the error), the loss (how the previous error is measured) and the metric (something printed in the train log, useful only for the user).","1911bb21":"Here is the Matthews Correlation function I initially wrote. I continue using it in log messages, but I have to substitute it with a tensor version in learning, see further on.","dce66c2c":"Let's use the loading function above defined:","62af06aa":"At first we have to scale the arrays of predictors. I prefer working on copies:","6adb81c0":"Better previsional model: let's see at some good public kernel; for example this work of Khnoi Nguyen: [5-fold LSTM with threshold tuning](https:\/\/www.kaggle.com\/suicaokhoailang\/5-fold-lstm-with-threshold-tuning-0-618-lb). ","cbad9b40":"## Doing Better (?)","11ffc459":"We have to return to the prevision per signal:","5f4e92c9":"## My First ANN","bdf056c1":"## Basic Imports","e7460a10":"This is the basic idea behind the approximation:","a770bd00":"Better Data. ","cdbe4377":"The phases have to stay united. We introduce this code:","e43da0ef":"We have a lot of FN considering that in our train set results:","39ae9f1e":"Better threshold seach. Let's try with this function:","93c75960":"Here the question is: **how to properly choose the features**? I started with something basic: taking a chunk (range) of the train\/test set and finding its mean, max, min, std. Then, considering \"little\" the time interval and thus almost linear the signal y_line, I calculated the differences y_line-min\/max. In this way I intended to find approximations of the upper and lower values of the error. Note: nr_ts=number of initial and final samples used to calculate initial and final points (just to avoid situations where one can have min\/max in the initial\/final time samples).","518217ed":"We are now ready to introduce Keras and the network.","9f3ccc9e":"We don't perform a final train, calculating instead predictions basing on the average of the folds:","bc240bed":"Well, I would say \"my first ANN in the last 20 years\" as my masters thesis was about ANN and at the times, I wrote a VB program to implement a Feedforward Neural Network. But many things are happened since those years and the reality is that now, in 2019, I am an absolute beginner with ANN. OK, let's go.","5c19a359":"To perform the fit you have to **Enable GPU**.  The dataframe defined below is used only to store the training scores:","22f3afc3":"In this kernel I **tried to use ANN** with the **data prepared** in [Let's get the party started](https:\/\/www.kaggle.com\/ludovicoristori\/vsb-data-prep-let-s-get-the-party-started) and the **lesson learned** with [In search of failures with a simple model](https:\/\/www.kaggle.com\/ludovicoristori\/in-search-of-failures-with-a-simple-model).\n\nI took the most sophisticated parts of the ANN code **from** the notebook of **Khnoi Nguyen** [5-fold LSTM with threshold tuning](https:\/\/www.kaggle.com\/suicaokhoailang\/5-fold-lstm-with-threshold-tuning-0-618-lb). Still out of reach for me, at present, but a **great source of inspiration**.\n\nTalking about the LB, things will go better **next times**: [Life is Life](https:\/\/www.youtube.com\/watch?v=EGikhmjTSZI), just to finish with another old pop song.","483d487b":"![](https:\/\/i.imgur.com\/q4VWwlj.png)","3049d6c2":"The following is a tensor implementation of the Mattews Correlation. I discovered I can't use my function mmc in a Keras model, based on Tensors.","6499ebad":"Solution: better threshold search, better data, better model (;-))"}}