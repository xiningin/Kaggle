{"cell_type":{"aed895f3":"code","a0b3fce8":"code","acdedd44":"code","3f7dd693":"code","7e92edcc":"code","d9879d34":"code","7f8cc977":"code","2a66e8c1":"code","d0a085cb":"code","53f9b53d":"code","8b008257":"code","c4074ebb":"code","d7fde8a0":"code","1cc329fd":"code","a356b350":"code","cc3a51b5":"code","bfc372af":"code","9b7db1c2":"code","8e845085":"code","626a8ab7":"code","ec653a08":"code","4ae9a3b2":"markdown","fafc44cd":"markdown","2ba2a052":"markdown","888c4ad1":"markdown","69797148":"markdown","52abbd70":"markdown","07b50337":"markdown","d097fa54":"markdown","d982539c":"markdown","5cab6652":"markdown","1f497698":"markdown","01aa9acc":"markdown","b1d0cfdf":"markdown","d52a982c":"markdown","7547f14d":"markdown","747f9853":"markdown","4096ffce":"markdown","c6ac55cc":"markdown","e5831ba4":"markdown","1924e52f":"markdown","8751bcc9":"markdown","ff0832b0":"markdown","b9e6d8a2":"markdown","a2e9b82e":"markdown","2754e363":"markdown","de053f07":"markdown"},"source":{"aed895f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0b3fce8":"#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","acdedd44":"import logging\nfrom types import GeneratorType\nfrom sklearn.utils import indexable\nfrom sklearn.utils.validation import _num_samples\n\nLOGGER = logging.getLogger(__name__)\n\nclass NestedCV():\n    \n    \n    def __init__(self, k, delay: int = 0):\n         \n         \n\n        if k and k < 3:\n            raise ValueError(f'Cannot have n_splits less than 3 (k={k})')\n        self.k = k\n        \n        #super().__init__(k, shuffle=False, random_state=None)\n\n        \n        if delay < 0:\n            raise ValueError(f'Cannot have negative values of delay (delay={delay})')\n        self.delay = delay\n\n    \n\n    def split(self, X, date_column = None ,y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters:\n            X : array-like, shape (n_samples, n_features)\n                Training data, where n_samples is the number of samples  and n_features is the number of features.\n\n            y : array-like, shape (n_samples,)\n                Always ignored, exists for compatibility.\n\n            groups : array-like, with shape (n_samples,), optional\n                Always ignored, exists for compatibility.\n\n        Yields:\n            train : ndarray\n                The training set indices for that split.\n\n            test : ndarray\n                The testing set indices for that split.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)  # pylint: disable=unbalanced-tuple-unpacking\n        n_samples = _num_samples(X)\n        n_splits = self.k\n        n_folds = n_splits + 1\n        delay = self.delay\n\n        if n_folds > n_samples:\n            raise ValueError(f'Cannot have number of folds={n_folds} greater than the number of samples: {n_samples}.')\n\n        indices = np.arange(n_samples)\n        split_size = n_samples \/\/ n_folds\n\n        train_size = split_size * self.k\n        test_size = n_samples \/\/ n_folds\n        full_test = test_size + delay\n\n        if full_test + n_splits > n_samples:\n            raise ValueError(f'test_size\\\\({test_size}\\\\) + delay\\\\({delay}\\\\) = {test_size + delay} + '\n                             f'n_splits={n_splits} \\n'\n                             f' greater than the number of samples: {n_samples}. Cannot create fold logic.')\n\n        # Generate logic for splits.\n        # Overwrite fold test_starts ranges if force_step_size is specified.\n        \n        \n        step_size = split_size\n        range_start = (split_size - full_test) + split_size + (n_samples % n_folds)\n        test_starts = range(range_start, n_samples, step_size)\n\n        # Generate data splits.\n        for test_start in test_starts:\n            id_start =  0\n            # Ensure we always return a test set of the same size\n            if indices[test_start:test_start + full_test].size < full_test:\n                continue\n            yield (indices[id_start:test_start],\n                   indices[test_start + delay:test_start + full_test])\n\n#this is our main method.\n\nif __name__ == '__main__':\n    #creating fake values of x and x to test our algorithm\n    xx = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    yy = np.array([1, 2, 3, 4, 5, 6])\n    tscv = NestedCV(k=3)  # This is where we create an object of our class.\n    print(tscv)  \n    for train_index, test_index in tscv.split(xx):         # Calling our split function that yields a generator.\n        print('TRAIN:', train_index, 'TEST:', test_index)\n        X_train, X_test = xx[train_index], xx[test_index]\n        y_train, y_test = yy[train_index], yy[test_index]\n    print(\"---------------------------------------------\")","3f7dd693":"train = pd.read_csv(\"..\/input\/predict-demand\/train.csv\", index_col = 1)\nnot_index = pd.read_csv(\"..\/input\/predict-demand\/train.csv\") # This is the same as train but with a regular index, intsead of DateTime\ntest = pd.read_csv(\"..\/input\/predict-demand\/test.csv\",index_col = 1)\n\n\n# Converting the index to datetime format. \n\ntrain.index = pd.to_datetime(train.index)\ntest.index = pd.to_datetime(test.index)\n#not_index[\"date\"] = pd.to_datetime(not_index[\"date\"]) \n\n\nprint(train.head())\n\ndata1  = train.copy(deep = True) # We create a new value of train to mess around with, while keeping train intact.\n\ndata_cleaner = [data1,test,not_index] # Create a list of all the dataframes, which will help us while cleaning the Data. ","7e92edcc":"train.describe().T # describe gives us statistical insights about our data, while .T is used to transpose.","d9879d34":"test.describe().T","7f8cc977":"for dataset in data_cleaner:\n    \n    dataset.drop(columns = 'id', inplace = True) #We drop id because it doesn't help us in prediction\n    \n    # checking for null values\n    \n    dataset.info() \n    print(dataset.isnull().sum()) ","2a66e8c1":"# loopoing over our dataset list to drop null values.\nfor dataset in data_cleaner:\n    dataset.drop(dataset.index[6480:],inplace = True)  #drop all the values after 6480\n    dataset.dropna(axis = 0, how = 'any',inplace = True) # drop all the rows with null values\n    dataset.drop(columns = ['lat','long'], inplace = True) #We drop id because it doesn't help us in prediction\n    ","d0a085cb":"label = LabelEncoder() # creating label encoder instance\nfor dataset in data_cleaner:    \n    dataset['city_Code'] = label.fit_transform(dataset['city'])\n    dataset['shop_Code'] = label.fit_transform(dataset['shop'])\n    dataset['brand_Code'] = label.fit_transform(dataset['brand'])\n    dataset['container_Code'] = label.fit_transform(dataset['container'])\n    dataset['capacity_Code'] = label.fit_transform(dataset['capacity'])\n","53f9b53d":"for dataset in data_cleaner:\n    \n    dataset.info()\n    print(dataset.isnull().sum())","8b008257":"data1.columns # Prints all the columns of data1","c4074ebb":"count_plot_column_name = [ 'city_Code', 'shop_Code', 'brand_Code', 'container_Code', 'capacity_Code','quantity'] #this is a list of all the column names with numerical values","d7fde8a0":"for i in count_plot_column_name:\n    \n    \n    plt.figure()\n    print(data1[i].value_counts())\n\n    sns.set_style('whitegrid')\n    sns.countplot(x=data1[i],data=data1, palette='YlGnBu_r')\n","1cc329fd":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","a356b350":"column_names = [ 'pop','price', 'city_Code', 'shop_Code', 'brand_Code','container_Code', 'capacity_Code','quantity']\n\nx = data1[column_names]\ny = data1['quantity']","cc3a51b5":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\nnorm = MinMaxScaler().fit(x)\nx_norm = norm.transform(x)\nx_norm = pd.DataFrame(x_norm)\n\npca = PCA()\n\npca.fit(x)\n\npca_train = pca.transform(x_norm)\npca_train = pd.DataFrame(pca_train)\npca_test = pca_train[7]\n\n\ny_norm = x_norm[7]\nprint(y_norm)\n\nx_norm.drop(7, axis = 1,inplace=True)\npca_train.drop(7,axis = 1 , inplace = True)\nprint(x_norm)\n\ncorrelation_heatmap(x)","bfc372af":"sns.set_palette('RdPu')\nplt.figure()\nsns.set_context(\"poster\", font_scale=0.7)\nsns.scatterplot(data = data1, y='price', x='quantity', hue='capacity')","9b7db1c2":"\nfrom pandas import read_csv\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom matplotlib import pyplot\n\nX = x.values\nsplits = NestedCV(k=3) # Here is where we create an istance of the class.\nindex = 1\nfor train_index, test_index in splits.split(X): #callng our split function which returns train and validation sets.\n    trainn = X[train_index]\n    testt = X[test_index]\n    print('Observations: %d' % (len(trainn) + len(testt)))\n    print('Training Observations: %d' % (len(trainn)))\n    print('Testing Observations: %d' % (len(testt)))\n    \n","8e845085":"fig,ax = plt.subplots(8,1,figsize=(20,15))\nfor i,column in enumerate([col for col in x.columns if col != 'hi']):\n    x[column].plot(ax=ax[i])\n    ax[i].set_title(column)","626a8ab7":"from statsmodels.tsa.vector_ar.vecm import coint_johansen\n\ncoint_johansen(x_norm,-1,1).eig\n","ec653a08":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model           import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(pca_train,pca_test,test_size=0.3)\n\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model,x ,y, cv=tscv, scoring='r2')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","4ae9a3b2":"Now as we can see, all the null values have been taken care of and all the categorical values have been converted to numerical values which our model will be able to understand, in all our dataframes.","fafc44cd":"# Checking stationarity","2ba2a052":"## Handling Categorical variables","888c4ad1":"From this we can infer that,  there are 6480 values in our train set and 1058 values in our test set.","69797148":"# Here we are going to use our Nested KFold with Machine Learning Models With a Pipeline","52abbd70":"The correlation heatmap helps us in identifying how every variable is co-related with each other. It is really helpful in figuring which features play a role in deciding our outcome. The below function helps us eaily create a correlation matrix","07b50337":"## Here we are using our NestedCV class to split the data","d097fa54":"Here, we are using LabelEncoder to convert our categorical variables to numerical values. ","d982539c":"# As we can see ScaleGBM has the highest accuracy score. The Score of LR ad LASSO are too high, which probably means that we've overfit our model","5cab6652":"## Since all our values are above 1 can say that our data is not stationary.","1f497698":"# Splitting the data","01aa9acc":"# Building Our Model","b1d0cfdf":"Here, we are plotting a graph to count the number of observation in each type. Since count plot is mainly used for cateorical data, we will only use those. ","d52a982c":"We can see that there are many missing values which would either need to be filled or removed. We can also infer that most of the columns are floats, so they dont need to be encoded anyhow. However columns like 'city,shop,brand,capacity,container' are of the object type, so we need to handle that as well.\n\nWe could impute our values based on the other values of the same column or we could just drop all the rows with Na values. Here we'll be dropping the rows because we wont lose alot of data.\n\nAlso we can see that the number of rows have increased from 6480 to 7560 after converting setting the index as datetime. Given that all those rows have null values we can just drop it.\n\nWe also drop the ID column because it doesn't help us in our prediction. We can also drop lat and long because we have city code and shop code, which gives us the location","7547f14d":"# 2) Cleaning The Dataset","747f9853":"**The function 'coint_johansen' is a function we use to check whether our multivariate data is stationary or not. If we have a single variable then we can check the stationarity using the adfuller test.**","4096ffce":"# 3) EDA","c6ac55cc":"In order for our data to make sense and not throw off the predictions with need to normalize\/scale our data. SkLearn provides a great tool MinMaxScaler for this very purpose. We first fit the MinMaxScaler object to our train values. We then store out target variable in Y and delete it from the training features. giving us two scaled x and y variables. \n\nWe also apply PCA(Principal Component Analysis) in order to reduce the dimensionaltiy of our data. The steps followed are the same as MinMaxScaler.","e5831ba4":"# Importing all the libraries\n\nWe import pandas for reading and handling the data. Numpy provides us with very efficient ways to perform mathematical operations on arrays as well as easy array creation. Sklearn provides us with the tools that we need to build a model in the later stages of the pipeline. Seaborn and matplotlab are two important and highly used libraries for displaying and plotting graphs\n","1924e52f":"# Problem Statement\n\n1) We have a dataset which included data about the demand of soda, using which we can predict the quantity. The data is in the form of a timeseries. \n\n2) We have created a Nested Cross Validation class, which also needs to be implemented while creating the model.  \n","8751bcc9":"# Here is the code for our nested class","ff0832b0":"## Handling Null Values","b9e6d8a2":"## After Pre-Processing Our Data","a2e9b82e":"As we can conclude from our heat map, Quantity has the highest correlation with the price variable. Let's see how those two are related in a scatter plot.","2754e363":"# 1) Reading The Data","de053f07":" ## As we can see our data has been split itto 3 equal folds"}}