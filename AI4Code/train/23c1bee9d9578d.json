{"cell_type":{"1539ef8b":"code","1c734a84":"code","f756e38c":"code","607eb0fc":"code","f767d16c":"code","f3cf1ca8":"code","723654c1":"code","2f87b5b9":"code","fc8778d6":"code","80aa5a47":"code","63d5a6aa":"code","aefef2de":"code","ec1148db":"code","0ff762c7":"code","a1a66151":"code","d55e555f":"code","76583c26":"code","a1fd5665":"code","6f4b100c":"code","acf6706d":"code","a81c8f4c":"code","c4d38b3f":"code","43d60e56":"code","000f3c9e":"code","876294c8":"code","202db94c":"code","c58e112e":"code","2043c140":"code","41fd8c3b":"code","5acc058b":"code","57cd6de8":"code","81648a74":"code","700ada81":"markdown","dde894e5":"markdown","e28fd5f0":"markdown","eb6f7fdc":"markdown","e7437d96":"markdown","a09660a6":"markdown","eae6f404":"markdown","06eb5c2d":"markdown","a2f25208":"markdown","b478e7a3":"markdown","c235bc3e":"markdown","50e2d45c":"markdown","675c2152":"markdown","0891adbc":"markdown","0ca290d5":"markdown","80bae7bf":"markdown","506e7138":"markdown","d896c853":"markdown","fd4d8260":"markdown","e8bb66a1":"markdown","1b686e2a":"markdown","8fe5c09b":"markdown","b46bd90e":"markdown","4f94181a":"markdown","9a9317d8":"markdown","3d630802":"markdown","85367e8a":"markdown"},"source":{"1539ef8b":"# Make necessary imports\n\n# for array operations \nimport numpy as np \n# for data handling\nimport pandas as pd\n# TensorFlow framework\nimport tensorflow as tf\n# PyTorch framework\nimport torch\n# for pretty printing\nfrom pprint import pprint","1c734a84":"# download a text data\ndata = pd.read_csv('..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv')\ndata.head()","f756e38c":"# AutoTokenizer module from the HF's transformers library\nfrom transformers import AutoTokenizer\n\n# download and cache a suitable pre-trained tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')","607eb0fc":"# extract a text for our work\ntext = data.loc[3,'txt']\n# Is it a string?\nprint(type(text))\n# What text does it have?\ntext","f767d16c":"# obtain the tokens\ntokens = tokenizer.tokenize(text)\ntokens","f3cf1ca8":"# convert the tokens into numeric representation\ninputs = tokenizer.convert_tokens_to_ids(tokens)\ninputs","723654c1":"decoded = tokenizer.decode(inputs)\ndecoded","2f87b5b9":"# In TensorFlow\nfrom transformers import TFAutoModelForSequenceClassification\n# fetch the same model as like tokenizer\ntf_model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-cased')\nids = tf.constant(inputs)\ntry:\n    tf_model(ids)\nexcept Exception as e:\n    print(type(e), e)","fc8778d6":"# nest inputs inside a list to form a batch, \n# i.e, increase dimension\nids = tf.constant([inputs])\ntf_model(ids).logits ","80aa5a47":"# In PyTorch\nfrom transformers import AutoModelForSequenceClassification\n# fetch the same model as like tokenizer\npt_model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')\n# convert inputs into a tensor\nids = torch.tensor(inputs)\n# make prediction\ntry:\n    pt_model(ids)\nexcept ValueError as e:\n    print('ValueError:', e)","63d5a6aa":"# nest inputs inside a list to form a batch\nids = torch.tensor([inputs])\n# make prediction on batched inputs\npt_model(ids).logits","aefef2de":"# obtain data where txt is short\n# no seed set. So we may see different data every time\ndata['txt_len'] = data['txt'].apply(lambda x: len(x))\ndata[np.multiply(data['txt_len']<32, data['txt_len']>23)].sample(5)","ec1148db":"# take some random texts of different lengths\ntexts = data.loc[[182, 332, 3804], 'txt'].values\nprint(*texts, sep='\\n')","0ff762c7":"# Extract tokens\ntokens = [tokenizer.tokenize(txt) for txt in texts]\n# obtain ids\nids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\nids","a1a66151":"# In TensorFlow\ntry:\n    tf.constant(ids)\nexcept ValueError as e:\n    print('ValueError:', e)","d55e555f":"# In PyTorch\ntry:\n    torch.tensor(ids)\nexcept ValueError as e:\n    print('ValueError:', e)","76583c26":"max_len = len(max(ids, key=len))\nprint(max_len)\nrectangular_ids = [i + [0]*(max_len-len(i)) for i in ids]\nnp.array(rectangular_ids)","a1fd5665":"# In TensorFlow\n# We can have three different tensors for three sentences\ntf_individual_tensors = [tf.constant([i]) for i in ids]\n# We can have one tensor containing three sentences\ntf_batched_tensors = tf.constant(rectangular_ids)","6f4b100c":"# In PyTotch\n# We can have three different tensors for three sentences\npt_individual_tensors = [torch.tensor([i]) for i in ids]\n# We can have one tensor containing three sentences\npt_batched_tensors = torch.tensor(rectangular_ids)","acf6706d":"# In TensorFlow\nprint(\"predict individual tensors\")\nfor tensor in tf_individual_tensors:\n    print(tf_model(tensor).logits.numpy())\n    \nprint(\"predict batched tensors\")\nprint(tf_model(tf_batched_tensors).logits.numpy())","a81c8f4c":"# In PyTorch\nprint(\"predict individual tensors\")\nfor tensor in pt_individual_tensors:\n    print(pt_model(tensor).logits.detach().numpy())\n    \nprint(\"predict batched tensors\")\nprint(pt_model(pt_batched_tensors).logits.detach().numpy())","c4d38b3f":"mask = np.zeros_like(rectangular_ids)\nfor i in range(len(ids)):\n    # add 1 where there are elements in ids, leaving other positions with 0\n    mask[i][:len(ids[i])] = 1\nmask","43d60e56":"# In TensorFlow\nprint(\"predict individual tensors\")\nfor tensor in tf_individual_tensors:\n    print(tf_model(tensor).logits.numpy())\n    \nprint(\"predict batched tensors with masks\")\nprint(tf_model(tf_batched_tensors, \n               attention_mask=tf.constant(mask)).logits.numpy())","000f3c9e":"# In PyTorch\nprint(\"predict individual tensors\")\nfor tensor in pt_individual_tensors:\n    print(pt_model(tensor).logits.detach().numpy())\n    \nprint(\"predict batched tensors with mask\")\nprint(pt_model(pt_batched_tensors, \n               attention_mask=torch.tensor(mask)).logits.detach().numpy())","876294c8":"# merge some texts to formulate a big text\ntext = data.loc[1083:1088,'txt'].to_list()\ntext = ' '.join(text)\ntext","202db94c":"tokens = tokenizer.tokenize(text)\nids = tokenizer.convert_tokens_to_ids(tokens)","c58e112e":"# In TensorFlow\ntry:\n    print(tf_model(tf.constant([ids])).logits.numpy())\nexcept Exception as e:\n    print(type(e))\n    print('Error:', e) ","2043c140":"# In PyTorch\ntry:\n    print(pt_model(torch.tensor([ids])).logits.detach().numpy())\nexcept Exception as e:\n    print(type(e))\n    print('Error:', e) ","41fd8c3b":"print('Length of input ids before truncation:', len(ids))\n# This BERT model accepts sequence ids of length up to 512\nids = ids[:512]\nprint('Length of input ids after truncation:', len(ids))","5acc058b":"# extract the first 64 texts\ntexts = data.txt.to_list()[:64]\nprint('Number of Examples: ', len(texts))\n# tensorflow tensors\ntf_inputs = tokenizer(texts, padding='longest', truncation=True, return_tensors='tf')\n# pytorch tensors\npt_inputs = tokenizer(texts, padding='longest', truncation=True, return_tensors='pt')\n\nprint(tf_inputs.keys())\nprint(pt_inputs.keys())","57cd6de8":"# In TensorFlow\nresults = tf_model(**tf_inputs).logits.numpy()\nresults.shape","81648a74":"# In PyTorch\nresults = pt_model(**pt_inputs).logits.detach().numpy()\nresults.shape","700ada81":"# Issue #1 Batching\n\nTransformers models expect inputs in batches in TWO dimensions. The frst refers to batch size and the second refers to the sequence length.","dde894e5":"### Thank you for your valuable time!","e28fd5f0":"# Issues with Modeling\n\nWe can do sentiment analysis with the above pre-processed text. However there are some common issues caused based on how the model was originally trained with its inputs.","eb6f7fdc":"#### ------------------------------------------------ \n#### *Articles So Far In This Series*\n#### -> [[NLP Tutorial] Finish Tasks in Two Lines of Code](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-finish-tasks-in-two-lines-of-code)\n#### -> [[NLP Tutorial] Unwrapping Transformers Pipeline](https:\/\/www.kaggle.com\/rajkumarl\/nlp-unwrapping-transformers-pipeline)\n#### -> [[NLP Tutorial] Exploring Tokenizers](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-exploring-tokenizers)\n#### -> [[NLP Tutorial] Fine-Tuning in TensorFlow](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-tensorflow) \n#### -> [[NLP Tutorail] Fine-Tuning in Pytorch](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-pytorch) \n#### -> [[NLP Tutorail] Fine-Tuning with Trainer API](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-with-trainer-api) \n#### ------------------------------------------------ ","e7437d96":"So far, we have dealt with the issues of tokenization and the remedial actions for each issue. \nHuggingFace's tokenizers can handle all these issues in one line on its own with a couple of arguments.","a09660a6":"# Prepare Environment and Data","eae6f404":"# Decode back into Texts","06eb5c2d":"We have successfully padded short sentences. Now they are ready to be converted into tensors, and make predictions.","a2f25208":"### That's the end. We get a good understanding of HF's Tokenizers\n\nKey reference: [HuggingFace's NLP Course](https:\/\/huggingface.co\/course)","b478e7a3":"Let's do padding manually to have ids of equal length","c235bc3e":"A direct call on tokenizer performs all the duties including attention masks without any hazzle. Let's do modeling with this data.","50e2d45c":"Tokens are of different lengths. They cannot be formulated as tensors. Let's check it!","675c2152":"# Effortless NLP using HuggingFace's Tranformers Ecosystem","0891adbc":"![Image](https:\/\/raw.githubusercontent.com\/RajkumarGalaxy\/dataset\/master\/Images\/keyboard_1.jpg)\n\n> Image by [Markus Winkler](https:\/\/unsplash.com\/@markuswinkler)\n### How Text is converted into Numbers in a meaningful way?","0ca290d5":"Aha! We cannot understand these numerical representations, but a **BERT model** can! ","80bae7bf":"The `decode` method not only decodes the numbers back to tokens, but also merges the tokens exactly similar to the original text.","506e7138":"# Issue #5 Truncating","d896c853":"# Encoding Texts\n\nA tokenizer encodes texts into numbers that a model can understand.","fd4d8260":"# Issue #3 Attention Masking\n\nWe have altered some inputs to match length of longest input. Will it affect the prediction?","e8bb66a1":"That's not good! When the first sentence attain same prediction as individual tensor and with the batched tensor, the next two sentences do have variations in logits values. It is the direct cause of padding.\n\nWe know that we have padded the last sentences with zeros, but can our model know this and how can it handle the original input by truncating the padded values?\n\nYes. Attenstion Masking emerges as the solution.","1b686e2a":"# Issue #2 Padding\n\nModels expect a batch of text ids as inputs. But texts are usually of different lengths. Tensors cannot be formulated with different input sizes. They must be rectangle in shape. Here, we go with padding. We pad the short sentences with some *padding id* to match its length to the longest sequence in the batch.","8fe5c09b":"We have our attention mask now. Let's try making predictions agian.","b46bd90e":"That's Fantastic. Our predictions are identical with either individual ids or batched ids!","4f94181a":"Great. We get the warning from the tokenizer beforehand!","9a9317d8":"Is there a maximum permissible length of inputs to a model? \nYes. If it goes beyond the limit, model could not process it. ","3d630802":"There we must go with Truncating the extra length of input tensors.","85367e8a":"**Tokenizers** API in the **Transformers** library offers essential preprocessing activities such as tokenization, padding, truncating, batching, and so on. \n\nLet's discuss the Tokenizers API and its different functionalities with use cases."}}