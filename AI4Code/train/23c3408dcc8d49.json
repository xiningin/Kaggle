{"cell_type":{"d04f98ef":"code","2cae0b2c":"code","ab0612c4":"code","819e57f5":"code","7a621292":"code","5219db8a":"code","bbfdcdf9":"code","efdb0901":"code","8fc328ad":"code","24437e2e":"code","5c9f82fb":"code","5fefc143":"code","d96b4861":"code","3be8c57c":"code","54b8418f":"code","a496febf":"code","4fa0045d":"code","326b8833":"code","86301e29":"code","5c8cc0c4":"code","1a0abd5f":"code","8c85d5f9":"code","4901d5ca":"code","dd7ad673":"code","1c80f0f1":"code","b98e497c":"code","0b3439a8":"code","c20a40de":"code","cd1fdb2b":"code","36dc4e13":"code","ab0fc14c":"code","64c43d0b":"code","f09e502a":"code","0dfbacb2":"code","3ae3e499":"code","5223f6ae":"code","e0bf2b1c":"code","0230face":"code","c5c27c29":"code","691ab829":"code","8941bde2":"code","5f78d367":"code","db690635":"code","a9e26760":"code","4a5f6669":"markdown","7246b502":"markdown","abf9f69b":"markdown","ebecd862":"markdown","3a750317":"markdown","9d6866b5":"markdown","e97b9a27":"markdown","58f44321":"markdown","d18c1cd1":"markdown","2163d2ae":"markdown","f77914a5":"markdown","7b5077b4":"markdown","13194c78":"markdown"},"source":{"d04f98ef":"# Import all the tools we need\n\n# Regular EDA (exploratory data analysis) and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we want our plots to appear inside the notebook\n%matplotlib inline \n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score ,accuracy_score\nfrom sklearn.metrics import plot_roc_curve","2cae0b2c":"#Importing data \ndf = pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndf.head()","ab0612c4":"df.shape","819e57f5":"df.info()","7a621292":"df.groupby('degree_t')['status'].value_counts()","5219db8a":"df\n","bbfdcdf9":"df.workex.value_counts().Yes","efdb0901":"df.groupby('workex')[\"status\"].value_counts().unstack()","8fc328ad":"## This Function code is to display values in charts\ndef show_values(axs, orient=\"v\", space=.01):\n    def _single(ax):\n        if orient == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n                value = '{:.2f}'.format(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif orient == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n                value = '{:.2f}'.format(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _single(ax)\n    else:\n        _single(axs)","24437e2e":"sns.set_palette(\"pastel\") # chossing colour for charts (optional)\nshow_values(sns.barplot(x= \"degree_t\" , y ='salary' , data=df))","5c9f82fb":"##Analysing Various Labels Using countplot\n\n\nfig, ax = plt.subplots(3,2,figsize=(14,18)) # creating multi plot (3*2) with figure size of (10,14)\nfig.subplots_adjust(hspace=0.4, wspace=0.25) #Adjusting  space between charts\n\nshow_values(sns.countplot(x='ssc_b',data=df,ax=ax[0,0]))\nax[0,0].set(xlabel = \"Secondary Board of education \")\n\nshow_values(sns.countplot(x='hsc_b',data=df,ax=ax[0,1]))\nax[0,1].set(xlabel = \"Higher secondary Board of education\")\n\nshow_values(sns.countplot(x='degree_t',data=df,ax=ax[1,0]))\nax[1,0].set(xlabel = \"Degree_type\")\n\nshow_values(sns.countplot(x='gender',data =df , ax= ax[1,1]))\n\nshow_values(sns.countplot(x = 'status' ,data = df , ax = ax[2,0]))\nax[2,0].set(xlabel = \"Status of students\")\n\nshow_values(sns.countplot(x = 'workex' ,data = df , ax = ax[2,1]))\nax[2,1].set(xlabel = \"Work Experience among students\")","5fefc143":"##ANALYSING VARIOUS DATA USING HISTPLOT\n\n\nfig , ax =plt.subplots(2,2,figsize=(10,7) )\nfig.subplots_adjust(hspace= 0.4, wspace = 0.3)\n\nsns.histplot( x ='ssc_p' , data = df , ax=ax[0,0] )\nax[0,0].set(xlabel = \"% in secondary exam\")\n\nsns.histplot( x = 'hsc_p', data = df , ax = ax[0,1])\nax[0,1].set(xlabel = \" % in higher secondary exam\")\n\nsns.histplot( x = 'degree_p', data = df , ax = ax[1,0])\nax[1,0].set(xlabel = \" % in UG DEGREE\")\n\nsns.histplot( x = 'mba_p', data = df , ax = ax[1,1])\nax[1,1].set(xlabel = \" % in MBA \")","d96b4861":"sns.histplot( x = 'etest_p', data = df  )\n","3be8c57c":"sns.pairplot(df , hue = 'degree_t', palette=\"Set2\",height = 2.5)","54b8418f":"fig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(df.corr(), annot = True, cmap = 'Blues')","a496febf":"#converting 'placed'= 1 , \"Not placed\" = 0\ndf['status'] = df['status'].replace(['Placed'],1)\ndf['status'] = df['status'].replace(['Not Placed'],0)","4fa0045d":"df.head()","326b8833":"#converting object into int\n\ndf1= pd.get_dummies(df[[\"sl_no\", \"gender\", \"ssc_p\",\"ssc_b\",\"hsc_p\",\"hsc_b\",\"hsc_s\",\"degree_p\",\"degree_t\",\"workex\",\"etest_p\",\"specialisation\",\"mba_p\",'status',\"salary\"]])\ndf1","86301e29":"# Splitting data into X and y\n\nX = df1.drop([\"sl_no\",\"status\",\"salary\"], axis=1)\ny = df1[\"status\"]","5c8cc0c4":"X","1a0abd5f":"# Split data into train and test sets\n\nnp.random.seed(42)\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2)","8c85d5f9":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(solver='liblinear'),\n          \"KNN\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier(),\n          \"gnb\" : GaussianNB(),\n          \"decision_tree\" : DecisionTreeClassifier(random_state=0, max_depth=2)}\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of differetn Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","4901d5ca":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\n\nmodel_scores","dd7ad673":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nshow_values(model_compare.T.plot.bar());","1c80f0f1":"np.random.seed(42)\ncv_acc_log = cross_val_score(LogisticRegression(solver='liblinear'), X, y, cv=20, scoring=\"accuracy\").mean()\ncv_acc_gnb =cross_val_score(GaussianNB(), X, y, cv=20, scoring=\"accuracy\").mean()\ncv_acc_rnd =cross_val_score(RandomForestClassifier(), X, y, cv=20, scoring=\"accuracy\").mean()","b98e497c":"cross_val = { \"Logisticregression\" : cv_acc_log ,\n              \"Gaussiannb\" :cv_acc_gnb,\n               \"RandomForestclssifier \" : cv_acc_rnd }\n\ncross_val","0b3439a8":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\nnp.random.seed(42)\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=50,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","c20a40de":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(X_test, y_test)","cd1fdb2b":"# Plot ROC curve and calculate and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test)","36dc4e13":"y_preds = gs_log_reg.predict(X_test)\ny_preds","ab0fc14c":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True,\n                     cbar=False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n    \nplot_conf_mat(y_test, y_preds)","64c43d0b":"print(classification_report(y_test, y_preds))","f09e502a":"def evaluate_preds(y_true, y_preds):\n    \"\"\"\n    Performs evaluation comparison on y_true labels vs. y_pred labels\n    on a classification.\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_preds)\n    precision = precision_score(y_true, y_preds)\n    recall = recall_score(y_true, y_preds)\n    f1 = f1_score(y_true, y_preds)\n    metric_dict = {\"accuracy\": round(accuracy, 2),\n                   \"precision\": round(precision, 2),\n                   \"recall\": round(recall, 2),\n                   \"f1\": round(f1, 2)}\n    print(f\"Acc: {accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 score: {f1:.2f}\")\n    \n    return metric_dict","0dfbacb2":"baseline_metrics = evaluate_preds(y_test, y_preds)\nbaseline_metrics","3ae3e499":"cv_accuracy = cross_val_score(LogisticRegression(solver='liblinear'), X, y, cv=10, scoring=\"accuracy\").mean()\ncv_precision = cross_val_score(LogisticRegression(solver='liblinear'), X, y, cv=10, scoring=\"precision\").mean()\ncv_recall = cross_val_score(LogisticRegression(solver='liblinear'), X, y, cv=10, scoring=\"recall\").mean()\ncv_f1 = cross_val_score(LogisticRegression(solver='liblinear'), X, y, cv=10, scoring=\"f1\").mean()","5223f6ae":"cross_for_eva_mat = {\"cross_val_accuracy\" :cv_accuracy,\n                     \"cross_val_precision\" :cv_precision,\n                     \"cross_val_recall\" : cv_recall,\n                      \"cross_val_f1 \" : cv_f1}","e0bf2b1c":"cross_for_eva_mat","0230face":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")\n\nclf.fit(X_train, y_train);","c5c27c29":"# Check coef_\nclf.coef_","691ab829":"# Match coef's of features to columns\nfeature_dict = dict(zip(X.columns, list(clf.coef_[0])))\nfeature_dict","8941bde2":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\", legend=False);","5f78d367":"df.groupby('workex')[\"status\"].value_counts().unstack()","db690635":"plt.figure(figsize=(10, 6))\n\n# Scatter with postivie examples\nplt.scatter(df.ssc_p[df.status==1],\n            df.hsc_p[df.status==1],\n            c=\"salmon\")\n\n# Scatter with negative examples\nplt.scatter(df.ssc_p[df.status==0],\n            df.hsc_p[df.status==0],\n            c=\"lightblue\")\n\n\nplt.title(\"Comparison of ssc % and hsc %  with status \")\nplt.xlabel(\"secondary exam  score in % (ssc_p)\")\nplt.ylabel(\"higher secondary score in % (hsc_p)\")\nplt.legend([\"Placed\", \"Not Placed\"]);","a9e26760":"plt.figure(figsize=(10, 6))\n\n# Scatter with postivie examples\nplt.scatter(df.ssc_p[df.status==1],\n            df.degree_p[df.status==1],\n            c=\"salmon\")\n\n# Scatter with negative examples\nplt.scatter(df.ssc_p[df.status==0],\n            df.degree_p[df.status==0],\n            c=\"lightblue\")\n\n\nplt.title(\"Comparison of ssc % and degree %  with status \")\nplt.xlabel(\"secondary exam  score in % (ssc_p)\")\nplt.ylabel(\"Degree score in % (degree_p)\")\nplt.legend([\"Placed\", \"Not Placed\"]);","4a5f6669":"# Evaluation","7246b502":"# Basic data visualization","abf9f69b":"# confusion Matrix","ebecd862":"# Correlation Map","3a750317":"# Roc_curve","9d6866b5":"# Data modelling","e97b9a27":"# Data dictionary\n\n1.sl_no : Serial Number,\n\n2.ssc_p : Secondary Education percentage- 10th Grade\n\n3.ssc_b : Board of Education- Central\/ Others\n\n4.hsc_p : Higher Secondary Education percentage- 12th Grade\n\n5.hsc_b : Board of Education- Central\/ Others\n\n6.hsc_s : Specialization in Higher Secondary Education\n\n7.degree_p : Degree Percentage\n\n8.degree_t : Under Graduation(Degree type)- Field of degree education\n\n9.workex : Work Experience\n\n10.etest_p : Employability test percentage ( conducted by college)\n\n11.specialisation : Post Graduation(MBA)- Specialization\n\n12.mba_p : MBA percentage\n\n13.status : Status of placement- Placed\/Not placed","58f44321":"# Classification_Report","d18c1cd1":"# Feature Importance","2163d2ae":"From above data we can say that people with work experience have 86% ( 64 placed out of 74)  of getting placed\n","f77914a5":"# Cross_valuation_score for different models","7b5077b4":"From the above two scatter plots we can see that student who have more than 73% in their secondary exam have\nmore chance of getting placed","13194c78":"# Hyperparameter tuning using GridSearchCV"}}