{"cell_type":{"dc560dde":"code","9fe82d75":"code","9f5cf476":"code","b2483c57":"code","3ffabb66":"code","ad874d8e":"code","da32b0fb":"code","bd0fe31c":"code","69dff989":"code","111d80f8":"code","911c6b5a":"code","1d7f6c4b":"code","b0a795c6":"code","0049835a":"code","5952bcb1":"code","f6310b81":"code","d8f2c037":"code","0e69478e":"code","73c083ff":"code","a3655a48":"code","6904c8db":"code","f8df106e":"code","b1457eb7":"code","56bc5014":"code","5a8a343d":"code","d17ad4dd":"code","b3f58d51":"code","da0981ad":"code","75d499ba":"code","5716cf10":"code","e692011c":"code","eb8da99b":"code","b9d7ff0f":"code","9e475a5b":"code","fd964e5e":"code","a1d45df7":"code","22bdaede":"code","9d9b458c":"code","82803dff":"code","1a03a852":"code","da74c353":"code","d73c8d20":"code","1dc92a5d":"code","2bb1f6b9":"code","e0b21224":"code","1970f7cd":"code","545d8239":"code","5729d099":"code","3f3c25a5":"code","c4cc0c5b":"code","762ece5a":"code","dfd61570":"code","f42669ba":"code","770d8d95":"code","1aa21d20":"code","791b9cfb":"code","257666f1":"code","3178de48":"code","54a2aaa8":"code","1159a163":"code","0e4c29b8":"code","07c31823":"code","60587a19":"code","75a29918":"code","26183aa3":"code","62958b42":"code","6e5511b0":"code","c0a26c4b":"code","65ccbb2f":"code","de2f27bb":"code","ded9f08b":"code","770a37d8":"code","570da890":"markdown","0cbd4662":"markdown","ad356c3c":"markdown","3f673ec9":"markdown","2d54a19e":"markdown","0e00d4f3":"markdown","13f94e71":"markdown","ccce4df8":"markdown","4508b68f":"markdown","54d11e23":"markdown","6f58f02c":"markdown","909ab766":"markdown","a77f498a":"markdown","b82b8fa7":"markdown","a6be493a":"markdown","36fdc2c6":"markdown","9a9b1bd0":"markdown","a4db978f":"markdown","518a95f5":"markdown","619d01b3":"markdown","f5b755a7":"markdown","703a8466":"markdown","cebb9831":"markdown","25bdae9c":"markdown","5708159b":"markdown","230ff123":"markdown","51921d31":"markdown","5374dd37":"markdown","4df7203d":"markdown","183ba957":"markdown","0aa0ec73":"markdown","19d9fff1":"markdown","a77a8e60":"markdown","98c58828":"markdown","f9759761":"markdown","029c55f2":"markdown","006ef24d":"markdown","9f8bb336":"markdown","ea6d5c09":"markdown","603eebb9":"markdown","ad2cb710":"markdown","42e6576f":"markdown","84e67dbd":"markdown","930a2065":"markdown","218b90d5":"markdown","d1298978":"markdown","f7865d65":"markdown","5e96d360":"markdown","6fa3b578":"markdown","65b0af50":"markdown","a1817b29":"markdown","099efa64":"markdown","cd2a165b":"markdown"},"source":{"dc560dde":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n%matplotlib inline\nsns.set()\n\n#import pydot\nfrom IPython.display import Image\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# To release RAM import gc collector\nimport gc\ngc.enable()","9fe82d75":"# Reduce memory function was taken from the kaggle following kernel:\n# https:\/\/www.kaggle.com\/ashishpatel26\/lightgbm-gbdt-dart-baysian-ridge-reg-lb-3-61\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","9f5cf476":"# Read CSV-Fikes\n\ntest_data = pd.read_csv('..\/input\/test.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv' )\n","b2483c57":"# Print the head of the data \nprint(test_data.head())\nprint('\\n\\n')\nprint(sample_submission.head(10))","3ffabb66":"# delete test data and the submission file for now\ndel test_data\ndel sample_submission\ngc.collect","ad874d8e":"# Declaring a dictionary with column name and column type, \n# so that we can save memory while reading the csv-file\n# the dictionary is based on the result from the memory reduce function\n\ndtypesDict_tr = {\n'id'                            :         'int32',\n'target'                        :         'float16',\n'severe_toxicity'               :         'float16',\n'obscene'                       :         'float16',\n'identity_attack'               :         'float16',\n'insult'                        :         'float16',\n'threat'                        :         'float16',\n'asian'                         :         'float16',\n'atheist'                       :         'float16',\n'bisexual'                      :         'float16',\n'black'                         :         'float16',\n'buddhist'                      :         'float16',\n'christian'                     :         'float16',\n'female'                        :         'float16',\n'heterosexual'                  :         'float16',\n'hindu'                         :         'float16',\n'homosexual_gay_or_lesbian'     :         'float16',\n'intellectual_or_learning_disability':    'float16',\n'jewish'                        :         'float16',\n'latino'                        :         'float16',\n'male'                          :         'float16',\n'muslim'                        :         'float16',\n'other_disability'              :         'float16',\n'other_gender'                  :         'float16',\n'other_race_or_ethnicity'       :         'float16',\n'other_religion'                :         'float16',\n'other_sexual_orientation'      :         'float16',\n'physical_disability'           :         'float16',\n'psychiatric_or_mental_illness' :         'float16',\n'transgender'                   :         'float16',\n'white'                         :         'float16',\n'publication_id'                :         'int8',\n'parent_id'                     :         'float32',\n'article_id'                    :         'int32',\n'funny'                         :         'int8',\n'wow'                           :         'int8',\n'sad'                           :         'int8',\n'likes'                         :         'int16',\n'disagree'                      :         'int16',\n'sexual_explicit'               :         'float16',\n'identity_annotator_count'      :         'int16',\n'toxicity_annotator_count'      :         'int16'\n}","da32b0fb":"# Read file to CSV  \ntrain_data = pd.read_csv('..\/input\/train.csv',dtype=dtypesDict_tr,parse_dates=['created_date'])  # nrows=10000000\ntrain_data['created_date'] = pd.to_datetime(train_data['created_date']).values.astype('datetime64[M]')\n\ngc.collect()","bd0fe31c":"# Use the methos to well import the data\n# reduce_mem_usage(train_data)\ntrain_data.info()","69dff989":"# Look at the top of the dataset\ntrain_data.head(3)","111d80f8":"# Inspect the statistical summary of the dataset\ntrain_data.describe()","911c6b5a":"pd.options.display.max_colwidth=300\n# Print the most severe comments (with target value greater than 0.8) \n# to get a feeling for the data but also on the data structure\nfor n, v in enumerate(train_data.loc[train_data.target>0.8, 'comment_text']):\n    print(n, ': ', v)\n    if n == 10:\n        break","1d7f6c4b":"pd.options.display.max_colwidth=300\n# Print the non-toxic comments \nfor n, v in enumerate(train_data.loc[train_data.target==0.0, 'comment_text']):\n    print(n, ': ', v)\n    if n == 10:\n        break","b0a795c6":"# Plot the number of comments over time in the training dataset.\ncnt_srs = train_data['created_date'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='blue')\nplt.xticks(rotation='vertical')\nplt.xlabel('Creation Date', fontsize=12)\nplt.ylabel('Number of comments', fontsize=12)\nplt.title(\"Number of comments over time in the train set\")\nplt.show()","0049835a":"# Plot the number of toxic comments over time in the training dataset.\ncnt_srs = train_data.loc[train_data['target']>=0.5,'created_date'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('Creation Date', fontsize=12)\nplt.ylabel('Number of comments', fontsize=12)\nplt.title(\"Number of comments over time in train set\")\nplt.show()","5952bcb1":"# Plot the average toxicity over time.\n\ntoxicity_gb_time = train_data[train_data['target']>0.5]['target'].groupby(train_data['created_date']).count()\/train_data['target'].groupby(train_data['created_date']).count()\n#print(toxicity_gb_time)\ntoxicity_gb_time = toxicity_gb_time.fillna(0)\n\ntoxicity_gb_time = toxicity_gb_time.sort_index()\n\nplt.figure(figsize=(14,6))\n#sns.lineplot(x=toxicity_gb_time.index, y=toxicity_gb_time.values, label='target')\nplt.plot(toxicity_gb_time.index, toxicity_gb_time.values, marker='o', linestyle='-', linewidth=2, markersize=0)\n\nplt.xticks(rotation=45)\nplt.xlabel('Creation Date', fontsize=12)\nplt.ylabel('Ratio of Toxic Comments', fontsize=12)\nplt.title(\"Ratio of Toxic Comments over Time\")\nplt.show()","f6310b81":"#Plot distribution of the target variable\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\nax1.hist(train_data.target, bins=2)\nax1.set_title('Distribution of the target variable')\nax1.set_ylabel('count')\n\nsns.distplot(train_data.target, ax=ax2)\n#ax2.hist(train_data.target)\nax2.set_title('Distribution of the target variabl')\nax2.set_ylabel('count')\n\nplt.show()","d8f2c037":"# Analyse the partition of toxicity classification against the full dataset\n# When we interpret the accuracy we need to consider the fact that the number\n# of toxic vs. non-toxic comments is not equally balanced\n\ncolumns = ['target','severe_toxicity', 'obscene',\n       'identity_attack', 'insult', 'threat']\n# Create series with the number of comments that only contain toxicity values greater than 0.5\ntoxic_crit = train_data.loc[:, columns]\ntoxic_crit = toxic_crit>0.5\ntoxic_crit = toxic_crit.sum()\ntoxic_crit = toxic_crit.sort_values(ascending = False)\ngc.collect()","0e69478e":"# Plot the distribution of the comments with toxicity values greater than 0.5\nplt.figure(figsize=(14,6))\nsns.barplot(toxic_crit.index, toxic_crit.values, alpha=0.8, color='green')\nplt.xticks(rotation=45)\nplt.xlabel('Criterias', fontsize=12)\nplt.ylabel('Number of occurences', fontsize=12)\nplt.title(\"Toxicity annotation greater 0.5\")\nplt.show()","73c083ff":"# Plot the distribution of the toxicity and identity annotator count\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\nsns.boxplot(train_data.toxicity_annotator_count, ax=ax1)\nax1.set_title('Distribution of the toxicity_annotator_count')\nax1.set_ylabel('count')\nsns.boxplot(train_data.identity_annotator_count, ax=ax2)\n#ax2.hist(train_data.identity_annotator_count)\nax2.set_title('Distribution of the identity_annotator_count')\nax2.set_ylabel('count')\nplt.show()","a3655a48":"import matplotlib.gridspec as gridspec\n\n# Create 3x2 sub plots\ngs = gridspec.GridSpec(3, 2)\n\nfig = plt.figure(figsize=(14, 18))\nax1 = fig.add_subplot(gs[0, 0]) # row 0, col 0\nax2 = fig.add_subplot(gs[0, 1]) # row 0, col 1\nax3 = fig.add_subplot(gs[1, 0]) # row 1, col 0\nax4 = fig.add_subplot(gs[1, 1]) # row 1, col 1\nax5 = fig.add_subplot(gs[2, 0]) # row 2, col 0\n\nsns.distplot(train_data['severe_toxicity'],kde=False, hist=True, bins=30, label='severe_toxicity', ax=ax1)\nax1.set_title('Dist. of the severe_toxicity')\nsns.distplot(train_data['obscene'],kde=False, hist=True, bins=30, label='obscene', ax=ax2)\nax2.set_title('Dist. of the obscene')\nsns.distplot(train_data['identity_attack'],kde=False, hist=True, bins=30, label='identity_attack', ax=ax3)\nax3.set_title('Dist. of the identity_attack')\nsns.distplot(train_data['insult'],kde=False, hist=True, bins=30, label='insult', ax=ax4)\nax4.set_title('Dist. of the insult')\nsns.distplot(train_data['threat'],kde=False, hist=True, bins=30, label='threat', ax=ax5)\nax5.set_title('Dist. of the threat')\n\nplt.show()","6904c8db":"id_attr = ['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian','bisexual', 'other_sexual_orientation', 'christian','jewish','muslim','hindu','buddhist',\n   'atheist','other_religion','black','white', 'asian','latino','other_race_or_ethnicity','physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness','other_disability']","f8df106e":"# Analyse the partition of toxicity classification against the full dataset\n# When we interpret the accuracy we need to consider the fact that the number\n# of toxic vs. non-toxic comments is not equally balanced\n\nother_attr = train_data.loc[:, id_attr]\nother_attr = other_attr>0.5\nother_attr = other_attr.sum()\nother_attr = other_attr.sort_values(ascending = False)","b1457eb7":"#cnt_srs = test_df['first_active_month'].dt.date.value_counts()\n#cnt_srs = cnt_srs.sort_index()\n\n# Plot the distribution of the comments with identity values greater than 0.5\nplt.figure(figsize=(14,6))\nsns.barplot(other_attr.index, other_attr.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('Criterias', fontsize=12)\nplt.ylabel('Number of occurences', fontsize=12)\nplt.title(\"Distribution of other attribute annotation greater 0.5\")\nplt.show()","56bc5014":"# How many entries (comments) have identity features that are all 0 or NaN?\nperson_cat = train_data\nperson_cat_nan = person_cat[(np.isnan(person_cat['asian']) | (person_cat['asian'] == 0.0))\n                            & (np.isnan(person_cat['atheist']) | (person_cat['atheist'] == 0.0)) \n                            & (np.isnan(person_cat['bisexual']) | (person_cat['bisexual'] == 0.0)) \n                            & (np.isnan(person_cat['black']) | (person_cat['black'] == 0.0)) \n                            & (np.isnan(person_cat['buddhist']) | (person_cat['buddhist'] == 0.0)) \n                            & (np.isnan(person_cat['christian']) | (person_cat['christian'] == 0.0)) \n                            & (np.isnan(person_cat['female']) | (person_cat['female'] == 0.0)) \n                            & (np.isnan(person_cat['heterosexual']) | (person_cat['heterosexual'] == 0.0)) \n                            & (np.isnan(person_cat['hindu']) | (person_cat['hindu'] == 0.0)) \n                            & (np.isnan(person_cat['homosexual_gay_or_lesbian']) | (person_cat['homosexual_gay_or_lesbian'] == 0.0)) \n                            & (np.isnan(person_cat['intellectual_or_learning_disability']) | (person_cat['intellectual_or_learning_disability'] == 0.0)) \n                            & (np.isnan(person_cat['jewish']) | (person_cat['jewish'] == 0.0)) \n                            & (np.isnan(person_cat['latino']) | (person_cat['latino'] == 0.0)) \n                            & (np.isnan(person_cat['male']) | (person_cat['male'] == 0.0)) \n                            & (np.isnan(person_cat['muslim']) | (person_cat['muslim'] == 0.0)) \n                            & (np.isnan(person_cat['other_disability']) | (person_cat['other_disability'] == 0.0)) \n                            & (np.isnan(person_cat['other_gender']) | (person_cat['other_gender'] == 0.0)) \n                            & (np.isnan(person_cat['other_race_or_ethnicity']) | (person_cat['other_race_or_ethnicity'] == 0.0)) \n                            & (np.isnan(person_cat['other_religion']) | (person_cat['other_religion'] == 0.0)) \n                            & (np.isnan(person_cat['other_sexual_orientation']) | (person_cat['other_sexual_orientation'] == 0.0)) \n                            & (np.isnan(person_cat['physical_disability']) | (person_cat['physical_disability'] == 0.0))\n                            & (np.isnan(person_cat['psychiatric_or_mental_illness']) | (person_cat['psychiatric_or_mental_illness'] == 0.0)) \n                            & (np.isnan(person_cat['transgender']) | (person_cat['transgender'] == 0.0)) \n                            & (np.isnan(person_cat['white']) | (person_cat['white'] == 0.0))]\n\nprint(\"Apparently only \" + str((person_cat_nan.shape[0] \/ train_data.shape[0])*100) + \" % of the comments do not have a value greater than 0 in any of the identity columns\")","5a8a343d":"# How many of the comments that do not have a value greater than 0 in any of the identity columns,\n# have a value greater than 0.5 for toxicity? \nprint(\"\" + str((len(person_cat_nan[person_cat_nan['target']>0.5]['target'])\/person_cat_nan.shape[0])*100) + \" % of the comments that do not have a value greater than 0 in any of the identity columns, are toxic.\")","d17ad4dd":"# How many of the comments that do not have a value greater than 0 in any of the identity columns,\n# have a value greater than 0.5 for identity_attack? \nlen(person_cat_nan[person_cat_nan['identity_attack']>0.5]['identity_attack'])\n\nprint(\"Only \" + str((len(person_cat_nan[person_cat_nan['identity_attack']>0.5]['identity_attack'])\/person_cat_nan.shape[0])*100) + \" % of the comments that do not have a value greater than 0 in any of the identity columns,\\nhave a value greater than 0.5 for identity_attack.\")","b3f58d51":"del person_cat_nan\ngc.collect()","da0981ad":"person_cat = person_cat[((person_cat['asian'] > 0.0))# & person_cat[person_cat.asian.notnull()])\n                        | ((person_cat['atheist'] > 0.0))\n                        | (person_cat['bisexual'] > 0.0) \n                        | (person_cat['black'] > 0.0)\n                        | (person_cat['buddhist'] > 0.0)\n                        | (person_cat['christian'] > 0.0)\n                        | (person_cat['female'] > 0.0)\n                        | (person_cat['heterosexual'] > 0.0)\n                        | (person_cat['hindu'] > 0.0)\n                        | (person_cat['homosexual_gay_or_lesbian'] > 0.0)\n                        | (person_cat['intellectual_or_learning_disability'] > 0.0)\n                        | (person_cat['jewish'] > 0.0)\n                        | (person_cat['latino'] > 0.0)\n                        | (person_cat['male'] > 0.0)\n                        | (person_cat['muslim'] > 0.0)\n                        | (person_cat['other_disability'] > 0.0)\n                        | (person_cat['other_gender'] > 0.0)\n                        | (person_cat['other_race_or_ethnicity'] > 0.0)\n                        | (person_cat['other_religion'] > 0.0)\n                        | (person_cat['other_sexual_orientation'] > 0.0)\n                        | (person_cat['physical_disability'] > 0.0)\n                        | (person_cat['psychiatric_or_mental_illness'] > 0.0)\n                        | (person_cat['transgender'] > 0.0)\n                        | (person_cat['white'] > 0.0)]\n\nperson_cat.shape\nprint(\"\" + str((person_cat.shape[0] \/ train_data.shape[0])*100) + \" % of the comments have a value greater than 0 in at least one of the identity columns\")","75d499ba":"print(\"\" + str(((len(person_cat[person_cat['target']>0.5]['target']) \/ person_cat.shape[0])*100)) + \" % of the comments that have a value greater than 0 in any of the identity columns, are toxic.\")","5716cf10":"# Explanation\n# categories = ['target']+list(train_data)[slice(8,32)]\n# SUM(x*y)\/COUNT(identity_col > 0)\n# categories.iloc[:, 1:] multiply all the identity columns  \n# categories.iloc[:, 0]  with the target column\n# categories.iloc[:, 1:].multiply(categories.iloc[:, 0], axis=\"index\").sum()  create the sum\n# categories.iloc[:, 1:][categories.iloc[:, 1:]>0].count()    devide by all the number of the identity values that are bigger than 0\n\n# Percent of toxic comments related to different identities, \n# using target and popolation amount of each identity as weights:\n\ncategories = train_data.loc[:, ['target']+list(train_data)[slice(8,32)]].dropna() # take the column target and all the categorizing columns\n# categories.iloc[:, 0] --> target column\n# categories.iloc[:, 1] --> all identity columns\nweighted_toxic = categories.iloc[:, 1:].multiply(categories.iloc[:, 0], axis=\"index\").sum()\/categories.iloc[:, 1:][categories.iloc[:, 1:]>0].count()\nweighted_toxic = weighted_toxic.sort_values(ascending=False)\nplt.figure(figsize=(10,8))\nsns.set(font_scale=1)\nax = sns.barplot(x = weighted_toxic.values, y = weighted_toxic.index, saturation=0.5, alpha=0.99)\nplt.ylabel('Categories')\nplt.xlabel('Weighted Toxic')\nplt.show()","e692011c":"# Percent of identity_attack related to different identities, \n# using target and popolation amount of each identity as weights:\n\ncategories = train_data.loc[:, ['identity_attack']+list(train_data)[slice(8,32)]] #.dropna() # take the column target and all the categorizing columns\n# categories.iloc[:, 0] --> identity_attack column\n# categories.iloc[:, 1] --> all identity columns\nweighted_identity_attack = categories.iloc[:, 1:].multiply(categories.iloc[:, 0], axis=\"index\").sum()\/categories.iloc[:, 1:][categories.iloc[:, 1:]>0].count()\nweighted_identity_attack = weighted_identity_attack.sort_values(ascending=False)\nplt.figure(figsize=(10,8))\nsns.set(font_scale=1)\nax = sns.barplot(x = weighted_identity_attack.values, y = weighted_identity_attack.index, saturation=0.5, alpha=0.99)\nplt.ylabel('categories')\nplt.xlabel('weighted identity_attack')\nplt.show()","eb8da99b":"#Let's see if there are correlations between the identity columns in train_data and the target:\n\ncorrs = np.abs((person_cat.loc[:, ['target']+list(person_cat)[slice(8,32)]]).corr())\nordered_cols = (corrs).sum().sort_values().index\nnp.fill_diagonal(corrs.values, 0)\nplt.figure(figsize=[8,8])\nplt.imshow(corrs.loc[ordered_cols, ordered_cols], cmap='plasma', vmin=0, vmax=1)\nplt.colorbar(shrink=0.7)\nplt.xticks(range(corrs.shape[0]), list(ordered_cols), size=16, rotation=90)\nplt.yticks(range(corrs.shape[0]), list(ordered_cols), size=16)\nplt.title('Heat map of coefficients of correlation between identity categories and target', fontsize=17)\nplt.show()\n","b9d7ff0f":"# Distribution of race and ethnicity\n\n# Create 3x2 sub plots\nimport matplotlib.gridspec as gridspec\n\ngs = gridspec.GridSpec(3, 2)\n\nfig = plt.figure(figsize=(14, 18))\nax1 = fig.add_subplot(gs[0, 0]) # row 0, col 0\nax2 = fig.add_subplot(gs[0, 1]) # row 0, col 1\nax3 = fig.add_subplot(gs[1, 0]) # row 1, col 0\nax4 = fig.add_subplot(gs[1, 1]) # row 1, col 1\nax5 = fig.add_subplot(gs[2, 0]) # row 2, col 0\nax6 = fig.add_subplot(gs[2, 1]) # row 2, col 1\n\nsns.distplot(person_cat['asian'],kde=False, hist=True, bins=30, label='asian', ax=ax1)\nax1.set_title('Dist. of the asian')\nsns.distplot(person_cat['black'],kde=False, hist=True, bins=30, label='black', ax=ax2)\nax2.set_title('Dist. of the black')\nsns.distplot(person_cat['jewish'],kde=False, hist=True, bins=30, label='jewish', ax=ax3)\nax3.set_title('Dist. of the jewish')\nsns.distplot(person_cat['latino'],kde=False, hist=True, bins=30, label='latino', ax=ax4)\nax4.set_title('Dist. of the atino')\nsns.distplot(person_cat['other_race_or_ethnicity'],kde=False, hist=True, bins=30, label='other_race_or_ethnicity', ax=ax5)\nax5.set_title('Dist. of the other_race_or_ethnicity')\nsns.distplot(person_cat['other_race_or_ethnicity'],kde=False, hist=True, bins=30, label='other_race_or_ethnicity', ax=ax6)\nax6.set_title('Dist. of the other_race_or_ethnicity')\n\nplt.show()","9e475a5b":"# Plot ethnicity features over time.\n\n# Extract month and year from created_date and aggregate\n\n# create dataframe with all the identity columns, target and creation date\nwithdate = train_data.loc[:, ['created_date', 'target']+list(train_data)[slice(8,32)]].dropna() \n# weight the identity scores, dividing each value by the sum of the whole column\nraceweighted = withdate.iloc[:, 2:]\/withdate.iloc[:, 2:].sum()  \n# Multipy the raceweight columns witht the target\nrace_target_weighted = raceweighted.multiply(withdate.iloc[:, 1], axis=\"index\")\n# Create created_date column\nrace_target_weighted['created_date'] = pd.to_datetime(withdate['created_date']).values.astype('datetime64[M]')\n# Group by the creation date\nweighted_demo = race_target_weighted.groupby(['created_date']).sum().sort_index()","fd964e5e":"plt.figure(figsize=(14,6))\n#sns.lineplot(x=toxicity_gb_time.index, y=toxicity_gb_time.values, label='target')\nplt.plot(weighted_demo.index, weighted_demo['white'], marker='o', linestyle='-', linewidth=2, markersize=0)\nplt.plot(weighted_demo.index, weighted_demo['asian'], marker='o', linestyle='-', linewidth=2, markersize=0)\nplt.plot(weighted_demo.index, weighted_demo['black'], marker='o', linestyle='-', linewidth=2, markersize=0)\nplt.plot(weighted_demo.index, weighted_demo['jewish'], marker='o', linestyle='-', linewidth=2, markersize=0)\nplt.plot(weighted_demo.index, weighted_demo['latino'], marker='o', linestyle='-', linewidth=2, markersize=0)\nplt.plot(weighted_demo.index, weighted_demo['other_race_or_ethnicity'], marker='o', linestyle='-', linewidth=2, markersize=0)\n\nplt.xticks(rotation=45)\nplt.xlabel('Creation Date', fontsize=12)\n#plt.ylabel('Ratio of Toxic Comments', fontsize=12)\nplt.title(\"Time Series Toxicity & Race\")\nplt.legend()\nplt.show()","a1d45df7":"del toxic_crit, fig,ax1, ax2, other_attr, person_cat\ndel weighted_identity_attack\ndel weighted_toxic\ndel race_target_weighted\ndel raceweighted\ngc.collect","22bdaede":"# Print out all the variables in use\n# %whos","9d9b458c":"train_ids = train_data[['created_date','id', 'publication_id', 'parent_id', 'article_id', 'target']]\ntrain_ids.head(10)","82803dff":"# Check missing values\n# Only parent_id seems to have missing values\ntrain_ids.isnull().sum()","1a03a852":"# HOW MANY BAD COMMENTS DIVIDED BY ALL COMMENTS PER PUBLICATION_ID (RATIO)\nplt.figure(figsize=(14,6))\n# Group by publication_id and count the number of comments with toxcicty (over 0.5) per publication_id group.\ny = train_data[train_data['target']>0.5]['target'].groupby(train_data['publication_id']).count()\/train_data['target'].groupby(train_data['publication_id']).count()\n# Plot the values in descending order \nsns.barplot(y.index, y.sort_values(ascending =False), color='green')\nplt.ylabel('Number of Toxic Comments', fontsize=12)\nplt.title(\"Distribution of Toxic Comments over publication_id\")","da74c353":"# Convert the data type of 'created date' column\n#train_data['created_date'] = pd.to_datetime(train_data['created_date']).values.astype('datetime64[M]')\n#train_data['created_date'].head()","d73c8d20":"# Plot the distribution of the ID-s over time \nplt.figure(figsize=(16, 6))\nsns.lineplot(x='created_date', y='id', label='IDs', data=train_data)\nplt.title('Distribution of IDs over years')","1dc92a5d":"train_eda = train_data.loc[:, [\"target\", \"sad\", \"wow\", \"funny\", \"likes\", \"disagree\"]]\ntrain_eda[train_eda['target']>0].head()","2bb1f6b9":"train_eda.describe()","e0b21224":"train_eda.isnull().sum()","1970f7cd":"colormap = plt.cm.RdBu\nplt.figure(figsize=(12,12))\nsns.heatmap(train_eda.corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","545d8239":"eda_wow = train_eda[train_eda[\"wow\"] >= 1]\neda_wow.drop([\"sad\", \"disagree\", \"funny\", \"likes\"], axis=1).describe()","5729d099":"eda_funny = train_eda[train_eda[\"funny\"] >= 1]\neda_funny.drop([\"sad\", \"wow\", \"disagree\", \"likes\"], axis=1).describe()","3f3c25a5":"eda_sad = train_eda[train_eda[\"sad\"] >= 1]\neda_sad.drop([\"disagree\", \"wow\", \"funny\", \"likes\"], axis=1).describe()","c4cc0c5b":"eda_likes = train_eda[train_eda[\"likes\"] >= 1]\neda_likes.drop([\"sad\", \"wow\", \"funny\", \"disagree\"], axis=1).describe()","762ece5a":"eda_disagree = train_eda[train_eda[\"disagree\"] >= 1]\neda_disagree.drop([\"sad\", \"wow\", \"funny\", \"likes\"], axis=1).describe()","dfd61570":"del columns, corrs, eda_likes, eda_sad, eda_wow, eda_funny\ndel eda_disagree\ndel train_eda\ngc.collect","f42669ba":"!pip install nltk\nimport nltk\nimport re\n\n# Import the English language class\nfrom spacy.lang.en import English\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Import Counter\nfrom collections import Counter\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n\n!pip install gensim\nfrom nltk.corpus import stopwords  \nimport gensim \nfrom gensim.utils import simple_preprocess \nfrom gensim.parsing.preprocessing import STOPWORDS \nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer \nfrom nltk.tokenize import word_tokenize","770d8d95":"# Tokenize the comment text of toxic comments\ntok_comments = [word_tokenize(com) for com in train_data[train_data['target']>0.5]['comment_text']]","1aa21d20":"# Remove stopwords\ntokens = [[w for w in s if (w not in stop_words) & (len(w)>2)] for s in tok_comments]","791b9cfb":"from nltk.probability import FreqDist","257666f1":"#plot the most frequent words\ntokens = np.array([np.array(s) for s in tokens])\nfdist = FreqDist(np.concatenate(tokens))\nfdist.plot(30,cumulative=False)\nplt.show()","3178de48":"#plot the most frequent word pairs\nfrom nltk import bigrams, ngrams\nbigrams_tokens = bigrams(np.concatenate(tokens))\nfdist_bigrams = FreqDist(list(bigrams_tokens))\nfdist_bigrams.plot(30,cumulative=False)\nplt.show()","54a2aaa8":"def print_top_20_words(comment_text):\n  \n  # delete the numbers from the string\n  #comment_text = re.sub(r'\\d+', '', comment_text)\n\n  # Create the nlp object\n  nlp = English()\n\n  # Process a text\n  doc = nlp(comment_text)\n\n  # Print the document text\n  # print(doc.text)\n\n  # Tokenize the article: tokens\n  tokens = word_tokenize(doc.text)\n\n  # Convert the tokens into lowercase: lower_tokens\n  lower_tokens = [t.lower() for t in tokens]\n\n  # Remove stopwords\n  \n  lower_tokens = [word for word in lower_tokens if word not in stopwords.words('english')]\n  lower_tokens = [word.lower() for word in lower_tokens if word.isalpha()]\n\n\n  # Create a Counter with the lowercase tokens: bow_simple\n  bow_simple = Counter(lower_tokens)\n  \n  # Print the 20 most common tokens\n  # print(bow_simple.most_common(10))\n  d = dict()\n  for elem in bow_simple.most_common(20):\n    print(elem[1], elem[0])\n    d[elem[0]] = elem[1]\n    \n  return d","1159a163":"downsampled_train_data = pd.read_csv('..\/input\/train.csv',dtype=dtypesDict_tr,parse_dates=['created_date'], nrows=200000)","0e4c29b8":"# Turn the whole comment_text column into list of strings (text)\ncomment_text = downsampled_train_data[downsampled_train_data['target'] >0.5]['comment_text'].to_string()\n\n# List of top 20 most used words in comments with toxicity (target) higher than 0.7\n# Work in chunks because nlp cannot process more than 1 Million characters\nfirst_chunk = print_top_20_words(comment_text[:(len(comment_text)\/\/4)])  # \/\/ devision returns a natural number without the rest\nprint('\\n\\n')\nsecond_chunk = print_top_20_words(comment_text[(len(comment_text)\/\/4+1):(2*len(comment_text)\/\/4)])\nprint('\\n\\n')\nthird_chunk = print_top_20_words(comment_text[(2*len(comment_text)\/\/4+1):(3*len(comment_text)\/\/4)])\nprint('\\n\\n')\nfourth_chunk = print_top_20_words(comment_text[(3*len(comment_text)\/\/4+1):])","07c31823":"# Build an average out of the 2 dictionaries\n\nnewd1 = {}\nfor key in first_chunk.keys():\n    for key2 in second_chunk.keys():\n        if key in second_chunk.keys():\n            newd1[key] = int(first_chunk.get(key)) + int(second_chunk.get(key))\n        else:\n            newd1[key] = first_chunk.get(key)\n        if key2 not in first_chunk.keys():\n            newd1[key2] = second_chunk.get(key2)\n\nnewd2 = {}   \nfor key in third_chunk.keys():\n    for key2 in fourth_chunk.keys():\n        if key in fourth_chunk.keys():\n            newd2[key] = int(third_chunk.get(key)) + int(fourth_chunk.get(key))\n        else:\n            newd2[key] = third_chunk.get(key)\n        if key2 not in third_chunk.keys():\n            newd2[key2] = fourth_chunk.get(key2)\n\nnewd = {}\nfor key in newd1.keys():\n    for key2 in newd2.keys():\n        if key in newd2.keys():\n            newd[key] = int(newd1.get(key)) + int(newd2.get(key))\n        else:\n            newd[key] = newd1.get(key)\n        if key2 not in newd1.keys():\n            newd[key2] = newd2.get(key2)\n\n\n# Sort dictionary numerically descending\nprint(\"Top 20 most used words in comments with toxicity (target) higher than 0.5:\")\ni = 0\nfor key, value in sorted(newd.items(), key=lambda item: item[1], reverse=True):\n    if i < 20:\n        print(\"%s: %s\" % (key, value))\n    i = i + 1","60587a19":"# List of top 20 most used words in comments with identity attack higher than 0.5\ncomment_text = downsampled_train_data[downsampled_train_data['identity_attack'] >0.5]['comment_text'].to_string()\nprint(\"Top 20 most used words in comments with identity attack higher than 0.5:\")\ndict_identity_attack = print_top_20_words(comment_text)","75a29918":"from wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image","26183aa3":"def toxicwordcloud(dict1, subset=train_data[train_data.target>0.5], title = \"Words Frequented\"):\n    stopword=set(STOPWORDS)\n    wc= WordCloud(background_color=\"black\",max_words=4000,stopwords=stopword)\n    wc.generate(\" \".join(list(dict1.keys())))\n    plt.figure(figsize=(8,8))\n    plt.xticks([])\n    plt.yticks([])\n    plt.axis('off')\n    plt.title(title, fontsize=20)\n    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)","62958b42":"toxicwordcloud(newd)","6e5511b0":"toxicwordcloud(dict_identity_attack)","c0a26c4b":"del downsampled_train_data\ngc.collect()","65ccbb2f":"words = train_data[\"comment_text\"].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\n\ntrain_data['words'] = words\nwords_toxic = train_data.loc[(train_data['words']<200)&(train_data['target'] >0.7)]['words']\nwords_identity_attack = train_data.loc[(train_data['words']<200)&(train_data['target'] >0.7)&(train_data['identity_attack'] >0.7)]['words']\nwords_threat = train_data.loc[(train_data['words']<200)&(train_data['target'] >0.7)&(train_data['threat'] >0.7)]['words']\nwords_nontoxic = train_data.loc[(train_data['words']<200)&(train_data['target'] <0.3)]['words']\n\nsns.set()\nplt.figure(figsize=(12,6))\nplt.title(\"Comment Length (words)\")\nsns.distplot(words_toxic,kde=True,hist=False, bins=120, label='toxic')\nsns.distplot(words_identity_attack,kde=True,hist=False, bins=120, label='identity_attack')\nsns.distplot(words_threat,kde=True,hist=False, bins=120, label='threat')\nsns.distplot(words_nontoxic,kde=True,hist=False, bins=120, label='nontoxic')\nplt.legend(); plt.show()","de2f27bb":"# Create new features\ntrain_data['total_length'] = train_data['comment_text'].apply(len)\ntrain_data['capitals'] = train_data['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\ntrain_data['num_exclamation_marks'] = train_data['comment_text'].apply(lambda comment: comment.count('!'))\ntrain_data['num_question_marks'] = train_data['comment_text'].apply(lambda comment: comment.count('?'))\ntrain_data['num_symbols'] = train_data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\ntrain_data['num_words'] = train_data['comment_text'].apply(lambda comment: len(comment.split()))\ntrain_data['num_smilies'] = train_data['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))","ded9f08b":"# Correlations between new features and the toxicity features\nfeatures = ('total_length', 'capitals', 'num_exclamation_marks','num_question_marks', 'num_symbols', 'num_words', 'num_smilies')\ncolumns = ('target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat')\nrows = [{c:train_data[f].corr(train_data[c]) for c in columns} for f in features]\ntrain_correlations = pd.DataFrame(rows, index=features)","770a37d8":"train_correlations","570da890":"<a id=\"10\"><\/a> <br>\n## Distribution of Additional Columns","0cbd4662":"<a id=\"6\"><\/a> <br>\n## Quick check on the testset and submission file","ad356c3c":"As we can see not so many of the entries have values in the indentity columns. These are about 12 % of all the entries and almost 10 % of these entries are toxic. Out of the columns that do not have values greater than 0 in any of the identity columns, only 5 % of them are toxic and 0.05 % have are market as an identity attack (value > 0.5). Even though these ratios seem small, they are meaningful numbers considering that the amount of toxic comments compared to non-toxic ones is small.  ","3f673ec9":"It is quite hard to work without interruptions when you have very large amounts of data and around 12 GB of RAM memory under your disposal. The kernels on Kaggle die or run much slower due to this. In this part we want to show you some small tricks on how to use less memory, so that the kernel does not die as often and the committing takes a little less time.\n\n1.  **The garbage collector** \n\n   The garbage collector (Python module: gc) attempts to collect  garbage, or to free memory occupied by objects that are no longer in use by the program\/script.\n    What you can do is to call gc.collect() after everytime you delete unnecessary data or new variables are being assigned.\n    The collector has many more methods to offer. Probably the most important one is gc.enable(), which enables the automatic garbage collection.\n    \n2. **The %whos command** \n\n     Make use of the %whos command in the python notebook to see all the current variables that you are not using anymore and manually delete them with the del command. Thus has been done several times throughout this kernel. \n\n3. **The reduce memory function** (Source : [LIghtGBM (goss + dart) + Parameter Tuning](https:\/\/www.kaggle.com\/ashishpatel26\/lightgbm-gbdt-dart-baysian-ridge-reg-lb-3-61))\n\n      The reduce memory function **reduce_mem_usage** works similar to the lean importing of the files, it changes the datatype of columns to an appropriate one. You could use this function when you read a new csv-file.\n      \n4. **Lean importing of files** (Source: [3 Simple Ways to Handle Large Data with Pandas](https:\/\/towardsdatascience.com\/3-simple-ways-to-handle-large-data-with-pandas-d9164a3c02c1))\n\n      Define columns and data-types with a dictionary. Python imports the data usually at highest resolution (int64, float64) which may not be always necessary. Make use of the reduce-memory-function after importing the data and check to which data type the columns are transformed (e.g. df.info()). This is the basis for the dictionary for future data imports of your CSV when rerunning the notebook.\n      For example, if we downloaded a dataset for predicting stock prices, our prices might be saved as 64 bit floating point. Since we might not really need such a long float number, we can import these columns as a float16 instead and save space in your RAM.\n\n5. **Delete unnecessary columns of the dataframe**\n\n      Once you're done with your exploratory data analysis, you know which features are of use to you. At this point you can drop all the columns that you think will not improve your result in the end. Since we're talking about millions of lines in the dataframe, it is worth doing and will make a difference when it comes to using less memory.","2d54a19e":"A heatmap is created to take a look at the pairwise correlations. The strongest relations exists between \"sad\" and \"disagree\" which can be explained by the fact that the content of comments that people disagree with probably make them sad as well. \nLooking at the correlation to the target variable the correlation is very weak. As the data is not coming from Facebook, Instagram or similar platforms where up- and downvoting is popular. This can be an explanation for a) the low number of votes and b) the weak correlation. \n\nOverall,any correlation is below 0.3 and therefore it is not really relevant for our further analysis\/seems to not really help for the classification of toxicity.","0e00d4f3":"Inspecting the head() of the first comments that also comments considered as toxic got some 'likes' but no 'dislikes', which is surprising in a way. Le's inspect the correlation.","13f94e71":"As we can see on the heatmap, there is no significant correlation between the columns. There might nonetheless be a very small correlations between some identity column.","ccce4df8":"## Notebook  Content\n* [Intro and Agenda](#1)\n* [Import Packages](#2)\n* [How to use less memory while programming?](#3)\n* [EDA](#4)\n    * [References](#5)\n    * [Quick check on the testset and submission file](#6)\n    * [Training Dataset](#7)\n    * [Examples of toxic and non-toxic comments](#8)\n    * [Distribution of the Target Variable](#9)\n    * [Distribution of Additional Columns](#10)\n    * [Identity Columns](#11)\n        * [Information About the Additional Columns](#12)\n    * [Exploration of the ID-Columns](#13)\n    * [Reaction Columns](#14)\n    * [Conclusion on Additional Columns](#15)\n    * [Comment Columns](#16)\n        * [Visualize Most Common Words](#17)","4508b68f":"The vast majority of the comments have a value a little greater than 0 in any of the toxicity features.\n\nWhat we can notice is that insult has more values above 0 than any of the other toxicity features. Maybe this is just because it is easier to insult without intent, whereas threats, identity attackts and obscenity tend to always have an intent behind.","54d11e23":"<a id=\"17\"><\/a> <br>\n### Visualize Most Common Words","6f58f02c":"**Comment length**","909ab766":"<a id=\"4\"><\/a> <br>\n# EDA","a77f498a":"The comment length has a pretty big variety of values, starting from very short ones to much longer ones (around 200 characters).","b82b8fa7":"Here we will explore the training data and see if it can tell us something more about what makes a comment toxic.","a6be493a":"After tokenizing the text of toxic comments, we see that many of the most used words, also refer to ethnic groups, religions, sex, sexual orientation, political views and so on. Hence, the toxic comments seem like an insult to different identity groups.","36fdc2c6":"The graph shows the identity scores of the comments over time of the ethnicitiy groups. White and Black seem to be the groups with the higher scores in these columns. ","9a9b1bd0":"As we can see the percent of toxic comments related to different identities is higher for  the identities: black, homosexual_gay_or_lesbian, white, muslim, jewish, atheist and so on. Maybe this is due to the fact that these identities are more common in society and as they are getting more media attention. ","a4db978f":"# Jigsaw Kaggle Competition - WIP ","518a95f5":"<a id=\"16\"><\/a> <br>\n## Comment Column","619d01b3":"The major amount of comments is clearly non toxic (~95%) according to the training data. When exploring the distribution of the target variable results, it looks sort of categorical. The reason for that is the method how toxicity was evaluated. A single comment was evaluated by up to 5 voters saying it is toxic (1) or non-toxic (0). Calculating the average out of these votes explains the distribution of the toxicity variable. Considering this nature of distribution, both trees and neural networks seem to be a more resonable choice.  Also Multinomial regression can be tried.  Logistic regression and only work with a binary input variable, creating a propability distribution, which can be used to decide on the binary classification.","f5b755a7":"<a id=\"2\"><\/a> <br>\n## Import Packages","703a8466":"Looking at the distribution of the share of toxic versus non-toxic comments, we can see that there are some publications where the share of toxic comments is considered to be much higher than for ofther publications. This information may be a helpful feature for toxic vs. non toxic calssification.","cebb9831":"<a id=\"13\"><\/a> <br>\n## Exploration of the ID-Columns","25bdae9c":"As we can see in the year there is a boost of ID-s at certain points in time. This is probably due to political\/social events and is also related to the number of comments since each comment has a unique id.","5708159b":"In this kernel we aim to explore the Jigsaw data, in order to get some insights on what makes a toxic comment and what doesn't. We also want to find out whether some of the given information in the training data is relevant to our predicitons or not. A problem we hope to solve with the exploratory data analysis is the prediction of many non-toxic comments as toxic, only because they contain words that are often part of toxic comments. Maybe a deeper look into the provided data, will help us find a way to eliminate such prediction mistakes.\n\nNext to that we want to share our techniques to import and processing the data consuming minimal RAM.\n\n**Outline:**\n \n- check submission file\n- check test file\n- deep dive into the training set\n - visualize additional parameters\n - give some examples\n - check columns separately (toxicity, identity, id-s, reaction and comment)\n    ","230ff123":"\n\nThere are no missing values in these columns.\n","51921d31":"As we can see the test data only includes the comment id and the comment text. The submission file contains the id of the comment and the perdiction, which is the target (the toxicity value). As we do not need these dataframes for any further exploratory data analysis, we will delete them.","5374dd37":"The above graphs visualize the most common words and word-groups in the toxic comments in an unfiltered way (words such as \"the\", are also included). As we can see some of the words, such as Trump, United States, country and so on hint at political disagreements.","4df7203d":"The time frames that havea ratio of toxic comments of 0, can be explained by the very low number of comments during these periods.This can also be seen on the graph that shows the number of comments over time.","183ba957":"* [Simple EDA Text Preprocessing - Jigsaw ](https:\/\/www.kaggle.com\/nz0722\/simple-eda-text-preprocessing-jigsaw)\n* [Jigsaw EDA](https:\/\/www.kaggle.com\/gpreda\/jigsaw-eda)\n* [Jigsaw Competition : EDA and Modeling](https:\/\/www.kaggle.com\/tarunpaparaju\/jigsaw-competition-eda-and-modeling)\n* [EDA - Toxicity of Identities (Updated 29\/4)](https:\/\/www.kaggle.com\/chewzy\/eda-toxicity-of-identities-updated-29-4)\n","0aa0ec73":"<a id=\"14\"><\/a> <br>\n## Reaction Columns","19d9fff1":"As we can see there is no significant correlation between the new features and the toxicity features. Maybe if we take the number of emojis as well, these values will show something different. ","a77a8e60":"<a id=\"3\"><\/a> <br>\n## How to use less memory while programming?","98c58828":"The identity features with the highest occurrencies are: female, male, black, christian, muslim and heterosexual_gay_or_lesbian. Maybe because these are attributes most commonly discussed in the media. ","f9759761":"Overall, it seems that the additional data, coming along with the target variable and the comment, seems not really contributing to predict the toxicity. We thus decide on excluding the information for the following preprocessing and modeling. This does not mean that for other datasets from other platforms will lead to the same conclusions, here we can well imagine that the reaction fields as well as publication ids may help to predict. For sure considering publication id in a model must be well considered as it may raise troubles regarding neutrality.","029c55f2":"<a id=\"9\"><\/a> <br>\n## Distribution of the Target Variable","006ef24d":"Even in this case where the comments have a value of 0 or a little higher than 0 in at least one of the identity columns, the majority of the comments have a value a little greater than 0 in the ethnicity features. It should be considered which value is high enough to influence our results.","9f8bb336":"<a id=\"15\"><\/a> <br>\n## Conclusion on Additional Columns","ea6d5c09":"<a id=\"11\"><\/a> <br>\n## Identity Columns","603eebb9":"<a id=\"8\"><\/a> <br>\n## Examples of toxic and non-toxic comments","ad2cb710":"<a id=\"5\"><\/a> <br>\n## References","42e6576f":"<a id=\"1\"><\/a> <br>\n## Intro and Agenda","84e67dbd":"<a id=\"12\"><\/a> <br>\n### Information About the Additional Columns","930a2065":"Overalll the id columns are well maintained, however the parent ID has quite some missing values. We will thus exclude it from our EDA.","218b90d5":"**Reduce Memory Function**","d1298978":"<a id=\"7\"><\/a> <br>\n## Training Dataset","f7865d65":"Another way to visualize the observations made above is a pairplot. Again the same observations can be made.","5e96d360":"From the column summary we can already derive that most of the classification columns range between 0 and 1.  The IDs should for sure rather be seen as categorical values.  The indentity annotator and the toxicity annotator seems to have strong outliers comparing the maximum values to the 75% percentile.","6fa3b578":"\n\nIn the following the data frame is described. As the table contains rather large values there can't be drawn clear conclusions from this.\n","65b0af50":"**Correlations between new features and the toxicity features**","a1817b29":"In addition to the labels described above, the dataset also provides metadata from Jigsaw's annotation: \ntoxicity_annotator_count and identity_annotator_count and metadata from Civil Comments: \n\ncreated_date, publication_id, parent_id, article_id, rating, funny, wow, sad, likes, disagree. \n\nCivil Comments' label rating is the civility rating Civil Comments users gave the comment.","099efa64":"The training data contains additional information around the comment, which may help to explain toxicity. The testset however does not include this data. We will thus decide based on the EDA if we further investigate methods to include the data into prediction, or if we remove the additional columns for additional model creation.","cd2a165b":"In the following some exploratory data analysis will be performed. This part only looks at the following columns \"target\", \"wow\", \"funny\", \"sad\" and \"like\". Therefore the data frame is filtered to only contain the mentioned columns."}}