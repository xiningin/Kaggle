{"cell_type":{"9da9a5fa":"code","3ca3ae7b":"code","f0bfe4dc":"code","73ade9b3":"code","a6c9f9c3":"code","766d928e":"code","c9039f30":"code","e9ff6289":"code","492d316b":"code","f68573b0":"code","e0aaf956":"code","77661da7":"code","5a36337a":"code","ef103ae7":"code","453c6f84":"code","66d09060":"code","ae3c4e4f":"code","2f354698":"code","7b4b2025":"code","9eec335c":"code","92a4c139":"code","bd7d989d":"code","e828f0e1":"code","de0a2fd0":"code","cfd84e7e":"code","4ca19cb8":"code","ea98eccd":"code","0efd467e":"code","ae81360f":"code","1aa5742e":"code","204405d8":"code","8bd33994":"code","d0ebbd41":"code","dff9ff6f":"code","6755872c":"code","8c19126e":"code","2e216da9":"code","84879546":"code","7e8e4534":"code","39b2bdd8":"code","a760cb99":"code","1b266ea2":"code","355fb8ea":"code","f04a147b":"code","22f7d8cc":"code","8bcf93f1":"code","cc763788":"code","1282a97b":"code","179f378c":"code","978d69d8":"code","e2e2da50":"code","5acf2589":"code","ac780ea6":"code","f9c1aa10":"code","9310f090":"code","2f1a0099":"code","31ae618d":"code","e4dfbfd2":"code","4a547b96":"code","6669ea90":"code","61e5cdbb":"code","7e85b5c2":"code","02cc061d":"markdown","5f1e534d":"markdown","0d62491e":"markdown","9268eb33":"markdown","59254e0d":"markdown","c83779a9":"markdown","995a8f08":"markdown","417f1251":"markdown","1aae1fbf":"markdown","67f0ab8f":"markdown","df6e2b4b":"markdown","9703573b":"markdown","c4610da2":"markdown","d21ea641":"markdown","900e3dfb":"markdown","ec13f4cd":"markdown","70bdef03":"markdown"},"source":{"9da9a5fa":"import numpy as np\nimport pandas as pd\nimport time, os\nimport nltk\nfrom functools import reduce\n\nfrom matplotlib import pyplot as plt\n\nfrom keras.layers import Embedding\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split","3ca3ae7b":"embeddings_index = {}\n\nf = open(os.path.join(\"..\/input\/glove6b100dtxt\/glove.6B.100d.txt\"))\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","f0bfe4dc":"embeddings_index[\".\"]","73ade9b3":"embeddings_index[\"PAD\"] = np.zeros(100, dtype='float32')\nembeddings_index[\"UNK\"] = np.zeros(100, dtype='float32')","a6c9f9c3":"embeddings_index[\"UNK\"]","766d928e":"word2idx = {}\nidx2word = {}\n\ncounter = 0\nfor word, _ in embeddings_index.items():\n    word2idx[word] = counter\n    idx2word[counter] = word\n    \n    counter += 1","c9039f30":"idx2word","e9ff6289":"word2idx[\"the\"]","492d316b":"idx2word[0]","f68573b0":"embeddings_index[\"the\"]","e0aaf956":"embeddings_index[idx2word[0]]","77661da7":"word2idx[\"PAD\"], word2idx[\"UNK\"]","5a36337a":"idx2word[400000], idx2word[400001]","ef103ae7":"embeddings_index[\"PAD\"]","453c6f84":"embeddings_index[idx2word[400000]]","66d09060":"embeddings_index[\"UNK\"]","ae3c4e4f":"embeddings_index[idx2word[400001]]","2f354698":"EMBEDDING_DIM = 100","7b4b2025":"embedding_matrix = np.zeros((len(embeddings_index), EMBEDDING_DIM))","9eec335c":"embedding_matrix.shape","92a4c139":"for word, index in word2idx.items():\n    \n    embedding_vector = embeddings_index.get(word)\n    \n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[index] = embedding_vector","bd7d989d":"embedding_matrix[0]","e828f0e1":"embedding_matrix[474]","de0a2fd0":"print(\"Already exists\") if os.path.isdir(\"data\/\") else os.mkdir(\"data\/\")\n\nfolder_path = \"..\/input\/sentiment1\/data\/\"","cfd84e7e":"print(\"Already exists\") if os.path.isdir(folder_path + \"train\/\") else os.mkdir(folder_path + \"train\/\")\nprint(\"Already exists\") if os.path.isdir(folder_path + \"test\/\") else os.mkdir(folder_path + \"test\/\")","4ca19cb8":"# Collect all positive review files\npos_files = os.listdir(folder_path + \"train\/pos\/\")","ea98eccd":"print(pos_files[:10])","0efd467e":"# Collect all negative review files\nneg_files = os.listdir(folder_path + \"train\/neg\/\")","ae81360f":"print(neg_files[:10])","1aa5742e":"# Total number of files positive & negative i.e. equals to total number of reviews\nprint(len(pos_files), len(neg_files))","204405d8":"# Put all text data inside X & label data inside Y\nX = []\nY = [] # positive = 1 & negative = 0","8bd33994":"# Download tokenizer 'punket' of nltk library\nnltk.download('punkt')","d0ebbd41":"# Read all positive reviews & put inside X & it's respective label inside Y\nfor file in pos_files:\n    with open(folder_path + \"train\/pos\/\" + file, \"r\") as f:\n        sentence = f.readline().lower()\n        tokens = nltk.word_tokenize(sentence)\n        X.append(tokens)\n        Y.append(1)","dff9ff6f":"# Length X, Y\nlen(X), len(Y)","6755872c":"print(X[:2])","8c19126e":"# Read all negative reviews & put inside X & it's respective label inside Y\nfor file in neg_files:\n    with open(folder_path + \"train\/neg\/\" + file, \"r\") as f:\n        sentence = f.readline().lower()\n        tokens = nltk.word_tokenize(sentence)\n        X.append(tokens)\n        Y.append(0)","2e216da9":"# Length X, Y\nlen(X), len(Y)","84879546":"# Put data inside pandas dataframe for some analysis\ndataset = pd.DataFrame({\"Sentence\": X, \"Label\": Y})","7e8e4534":"# print top 10 sentences\ndataset.head(10)","39b2bdd8":"# print bottom 10 sentences\ndataset.tail(10)","a760cb99":"# Is there any class imbalance?\ndataset[\"Label\"].value_counts().plot(kind=\"barh\")","1b266ea2":"# Calculate all sentences individual lengths\nsentences_len = [len(sent) for sent in X]","355fb8ea":"# Maximum size of any particular sentence\nmax_sentence_size = max(sentences_len)\nmax_sentence_size","f04a147b":"# Minimum size of any particular sentence\nmin_sentence_size = min(sentences_len)\nmin_sentence_size","22f7d8cc":"# Average size of any particular sentence\navg_sentence_size = reduce((lambda x, y: x + y), sentences_len) \/\/ len(X)\navg_sentence_size","8bcf93f1":"# Let's try to see how length of sentences varies\nplt.plot(list(range(len(sentences_len))), sentences_len)","cc763788":"# This function convert sentences array into number array\ndef num_sentence(sentences_tokens):\n    x = [[word2idx.get(word, word2idx.get(\"UNK\")) for word in sent] for sent in sentences_tokens]\n    \n    return x","1282a97b":"X = num_sentence(X)","179f378c":"len(X)","978d69d8":"print(X[:2])","e2e2da50":"# Perform padding for sentences less then 750\nX = pad_sequences(X, maxlen = 750, padding = \"pre\", truncating = \"pre\", value = word2idx.get(\"PAD\"))","5acf2589":"plt.plot(list(range(len(X))), [len(sent) for sent in X])","ac780ea6":"print(X[:2])","f9c1aa10":"# let's check our data is in proper shape\nX.shape","9310f090":"# let's check our data is in proper shape for softmax activation function\nY = np.array(Y)\nY.shape","2f1a0099":"# Time to convert Y values into one hot encoding\nY = to_categorical(Y)\nY.shape","31ae618d":"print(Y[:2])","e4dfbfd2":"# Now divide data into training & validations data\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, Y, test_size = 0.10, random_state = 10)","4a547b96":"# Model input data dimensions\nword_tokens = embedding_matrix.shape[0]\ntimesteps = 750\ndata_dim = 100\noutput_len = 2","6669ea90":"# Neural network architecture\nmodel = Sequential()\n\nmodel.add(Embedding(word_tokens, data_dim, input_shape = (timesteps,), weights = [embedding_matrix], trainable = False))\n\nmodel.add(LSTM(50, return_sequences = True))\n# model.add(Dropout(0.2))\n\nmodel.add(LSTM(100, return_sequences = False))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(output_len))\nmodel.add(Activation(\"softmax\"))\n\nstart = time.time()\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = \"rmsprop\", metrics = ['accuracy'])\nprint ('compilation time : ', time.time() - start)","61e5cdbb":"model.summary()","7e85b5c2":"# Now data is in proper format let's fit it\nmodel.fit(train_x, train_y, batch_size = 128, epochs = 1, validation_split = 0.05, verbose = 1)","02cc061d":"!mv kaggle.json \/root\/.kaggle\/","5f1e534d":"!kaggle competitions download sentiment-classification-on-large-movie-review -f train.zip","0d62491e":"# Download dataset following Kaggle competition \nhttps:\/\/www.kaggle.com\/c\/sentiment-classification-on-large-movie-review\/\n    \n# Create following folder structure\nsentiment_analysis.ipynb\ndata\/\n    train\/\n        pos\/\n            *.txt\n        neg\/\n            *.txt\n    test\/","9268eb33":"# Now after downloading dataset from kaggle move train data into train folder & do it same for test folder & sample.csv into data folder","59254e0d":"!chmod 600 \/root\/.kaggle\/kaggle.json","c83779a9":"!ls","995a8f08":"!unzip -q \/content\/train.zip","417f1251":"# Following is offical kaggle package. Check this link for API crendientials & upload it on google colabs \n# https:\/\/github.com\/Kaggle\/kaggle-api\n!pip install kaggle","1aae1fbf":"https:\/\/www.kaggle.com\/rtatman\/glove-global-vectors-for-word-representation?select=glove.6B.100d.txt","67f0ab8f":"## If you are using google colabs use following code for downloading dataset from kaggle","df6e2b4b":"## Download glove embeddings & use it to create embedding layer matrix","9703573b":"!pwd","c4610da2":"# Use this only in case of google colabs\nfrom google.colab import files\nfiles.upload()","d21ea641":"folder_path = \"\/content\/train\/\"","900e3dfb":"!mkdir \/root\/.kaggle\/","ec13f4cd":"## Now time to split dataset into train & test data","70bdef03":"## If you are using local machine follow following steps"}}