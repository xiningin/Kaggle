{"cell_type":{"07c06667":"code","bf025c8e":"code","17c5a92b":"code","ff5745f1":"code","00f9f961":"code","119c52cd":"code","b8793850":"code","83fac62e":"code","5718b2ce":"code","2997399b":"code","ae9f19b0":"code","af8cacb8":"code","3143ba62":"code","3ef79720":"code","04027617":"code","ea123a20":"code","3d7adfe4":"code","4d8856be":"code","2a6ec555":"code","6c91ad23":"code","8042d61a":"code","13d0396a":"code","f654ad4b":"code","bbdc0031":"code","62238fe9":"code","33810394":"code","648f160d":"code","e826d90d":"code","a359e814":"code","4fb4130c":"code","583b29df":"code","bd99091b":"code","3ddf065f":"code","402940fb":"code","00b9024d":"code","f5bb780e":"code","4a4ac973":"code","5dcf1a20":"code","a0d0fd1b":"code","e9cdc1b2":"code","2d5fefc9":"code","f35b50ef":"code","c5d7b9de":"code","800e68d8":"code","cedcfb20":"code","5ddfd69e":"code","c96ff145":"code","5e692bb5":"code","3bf8b0fc":"code","520e955a":"code","7e31fedb":"code","708b4573":"code","a15b97ac":"code","6667ae63":"code","1e3460ba":"code","ce9b626b":"code","9c2f49d5":"code","77d631d3":"code","f8aa5909":"code","b522f6d3":"code","190c5651":"code","9cb8541a":"code","525b4e94":"code","4e17c77c":"code","d5eeadc6":"markdown","eff13277":"markdown","6ca93463":"markdown","5a98b1cd":"markdown","d76e2240":"markdown","0414809d":"markdown","4f12f8af":"markdown","f767c866":"markdown","2b27eaf6":"markdown","5ba50368":"markdown","63f17926":"markdown","4e7ea032":"markdown","a4445f4e":"markdown","4eb57012":"markdown","7277e20f":"markdown","f5dd9ace":"markdown","7c7da47f":"markdown","34dd4635":"markdown","7f49c086":"markdown","047e54e6":"markdown","d5a8d743":"markdown","5c46c4de":"markdown"},"source":{"07c06667":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf025c8e":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nprint('Using Tensorflow version:', tf.__version__)","17c5a92b":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    \n    return np.array(enc_di['input_ids'])","ff5745f1":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","00f9f961":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 2    \nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","119c52cd":"train_df = pd.read_csv('..\/input\/sentiment-data\/train (add leak).csv')\noriginal_df = train_df.copy()\nadded_df = pd.read_csv('..\/input\/shopee-reviews\/shopee_reviews.csv')\ntest_df = pd.read_csv('..\/input\/sentiment-data\/test.csv')\n\ntrain_df.drop('review_id', axis=1, inplace=True)\n\nprint('Train shape:', train_df.shape)\nprint('Test shape:', test_df.shape)","b8793850":"review_train = train_df['review'].tolist()\nreview_test = test_df['review'].tolist()\n\nprint(len(set(review_train).intersection(set(review_test))))\n\nsame_data_list = list(set(review_train).intersection(set(review_test)))\nsame_data_list[0:5]","83fac62e":"added_df = added_df.rename(columns={'label': 'rating','text':'review'})","5718b2ce":"added_df.iloc[1431262]","2997399b":"# Drop the trash data\nadded_df = added_df.drop(1431262)","ae9f19b0":"train_df = train_df.append(added_df,ignore_index = True)  ## Use this to use both shopee data and scraped data","af8cacb8":"train_df['rating'] = train_df['rating'].astype(int)","3143ba62":"train_df['rating'].value_counts()","3ef79720":"review_train = added_df['review'].tolist()\nreview_test = test_df['review'].tolist()\n#Inspect data leak (after adding scraped data)\nmatched_reviews = set(review_train).intersection(set(review_test))\nprint('Matched reviews from scraped data and the test set:', len(matched_reviews))","04027617":"import emoji\ndef emoji_cleaning(text):\n    \n    # Change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n    # Delete repeated emoji\n    tokenizer = text.split()\n    repeated_list = []\n    \n    for word in tokenizer:\n        if word not in repeated_list:\n            repeated_list.append(word)\n    \n    text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    return text","ea123a20":"have_emoji_train_idx = []\nhave_emoji_test_idx = []\n\nfor idx, review in enumerate(train_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_train_idx.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_test_idx.append(idx)","3d7adfe4":"train_emoji_percentage = round(len(have_emoji_train_idx) \/ train_df.shape[0] * 100, 2)\nprint(f'Train data has {len(have_emoji_train_idx)} rows that used emoji, that means {train_emoji_percentage} percent of the total')\n\ntest_emoji_percentage = round(len(have_emoji_test_idx) \/ test_df.shape[0] * 100, 2)\nprint(f'Test data has {len(have_emoji_test_idx)} rows that used emoji, that means {test_emoji_percentage} percent of the total')","4d8856be":"train_df_original = train_df.copy()\ntest_df_original = test_df.copy()\n\n# emoji_cleaning\ntrain_df.loc[have_emoji_train_idx, 'review'] = train_df.loc[have_emoji_train_idx, 'review'].apply(emoji_cleaning)\ntest_df.loc[have_emoji_test_idx, 'review'] = test_df.loc[have_emoji_test_idx, 'review'].apply(emoji_cleaning)","2a6ec555":"# before cleaning\ntrain_df_original.loc[have_emoji_train_idx, 'review'].tail()","6c91ad23":"# after cleaning\ntrain_df.loc[have_emoji_train_idx, 'review'].tail()","8042d61a":"import string\nstring.punctuation","13d0396a":"# Prints the distribution of the train set\noriginal_df['rating'].value_counts(normalize = True)","f654ad4b":"for punc in string.punctuation:\n    print(punc)\n    print(original_df[original_df['review'].str.contains(punc,regex=False)].rating.value_counts(normalize = True))\n    print('------------------------------------------------------------')","bbdc0031":"import re\ndef review_cleaning(text):\n    \n    # delete lowercase and newline\n    text = text.lower()\n    text = re.sub(r'\\n', '', text)\n    text = re.sub('([.,!?()])', r' \\1 ', text)\n    text = re.sub('\\s{2,}', ' ', text)\n    \n    # change emoticon to text\n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n    # We decide to include punctuation in the model so we comment this line out!\n#     text = re.sub('[^a-z0-9! ]', ' ', text)\n    \n    tokenizer = text.split()\n    \n    return ' '.join([text for text in tokenizer])","62238fe9":"train_df['review'] = train_df['review'].apply(review_cleaning)\ntest_df['review'] = test_df['review'].apply(review_cleaning)","33810394":"repeated_rows_train = []\nrepeated_rows_test = []\n\nfor idx, review in enumerate(train_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_train.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_test.append(idx)","648f160d":"def delete_repeated_char(text):\n    \n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    \n    return text","e826d90d":"train_df.loc[repeated_rows_train, 'review'] = train_df.loc[repeated_rows_train, 'review'].apply(delete_repeated_char)\ntest_df.loc[repeated_rows_test, 'review'] = test_df.loc[repeated_rows_test, 'review'].apply(delete_repeated_char)","a359e814":"print('Before: ', train_df_original.loc[92129, 'review'])\nprint('After: ', train_df.loc[92129, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[56938, 'review'])\nprint('After: ', train_df.loc[56938, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[72677, 'review'])\nprint('After: ', train_df.loc[72677, 'review'])\n\nprint('\\nBefore: ', train_df_original.loc[36558, 'review'])\nprint('After: ', train_df.loc[36558, 'review'])","4fb4130c":"def recover_shortened_words(text):\n    \n    # put \\b (boundary) for avoid the characters in the word to be replaced\n    # I only make a few examples here, you can add if you're interested :)\n    \n    text = re.sub(r'\\bapaa\\b', 'apa', text)\n    \n    text = re.sub(r'\\bbsk\\b', 'besok', text)\n    text = re.sub(r'\\bbrngnya\\b', 'barangnya', text)\n    text = re.sub(r'\\bbrp\\b', 'berapa', text)\n    text = re.sub(r'\\bbgt\\b', 'banget', text)\n    text = re.sub(r'\\bbngt\\b', 'banget', text)\n    text = re.sub(r'\\bgini\\b', 'begini', text)\n    text = re.sub(r'\\bbrg\\b', 'barang', text)\n    \n    text = re.sub(r'\\bdtg\\b', 'datang', text)\n    text = re.sub(r'\\bd\\b', 'di', text)\n    text = re.sub(r'\\bsdh\\b', 'sudah', text)\n    text = re.sub(r'\\bdri\\b', 'dari', text)\n    text = re.sub(r'\\bdsni\\b', 'disini', text)\n    \n    text = re.sub(r'\\bgk\\b', 'gak', text)\n    \n    text = re.sub(r'\\bhrs\\b', 'harus', text)\n    \n    text = re.sub(r'\\bjd\\b', 'jadi', text)\n    text = re.sub(r'\\bjg\\b', 'juga', text)\n    text = re.sub(r'\\bjgn\\b', 'jangan', text)\n    \n    text = re.sub(r'\\blg\\b', 'lagi', text)\n    text = re.sub(r'\\blgi\\b', 'lagi', text)\n    text = re.sub(r'\\blbh\\b', 'lebih', text)\n    text = re.sub(r'\\blbih\\b', 'lebih', text)\n    \n    text = re.sub(r'\\bmksh\\b', 'makasih', text)\n    text = re.sub(r'\\bmna\\b', 'mana', text)\n    \n    text = re.sub(r'\\borg\\b', 'orang', text)\n    \n    text = re.sub(r'\\bpjg\\b', 'panjang', text)\n    \n    text = re.sub(r'\\bka\\b', 'kakak', text)\n    text = re.sub(r'\\bkk\\b', 'kakak', text)\n    text = re.sub(r'\\bklo\\b', 'kalau', text)\n    text = re.sub(r'\\bkmrn\\b', 'kemarin', text)\n    text = re.sub(r'\\bkmrin\\b', 'kemarin', text)\n    text = re.sub(r'\\bknp\\b', 'kenapa', text)\n    text = re.sub(r'\\bkcil\\b', 'kecil', text)\n    \n    text = re.sub(r'\\bgmn\\b', 'gimana', text)\n    text = re.sub(r'\\bgmna\\b', 'gimana', text)\n    \n    text = re.sub(r'\\btp\\b', 'tapi', text)\n    text = re.sub(r'\\btq\\b', 'thanks', text)\n    text = re.sub(r'\\btks\\b', 'thanks', text)\n    text = re.sub(r'\\btlg\\b', 'tolong', text)\n    text = re.sub(r'\\bgk\\b', 'tidak', text)\n    text = re.sub(r'\\bgak\\b', 'tidak', text)\n    text = re.sub(r'\\bgpp\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bgapapa\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bga\\b', 'tidak', text)\n    text = re.sub(r'\\btgl\\b', 'tanggal', text)\n    text = re.sub(r'\\btggl\\b', 'tanggal', text)\n    text = re.sub(r'\\bgamau\\b', 'tidak mau', text)\n    \n    text = re.sub(r'\\bsy\\b', 'saya', text)\n    text = re.sub(r'\\bsis\\b', 'sister', text)\n    text = re.sub(r'\\bsdgkan\\b', 'sedangkan', text)\n    text = re.sub(r'\\bmdh2n\\b', 'semoga', text)\n    text = re.sub(r'\\bsmoga\\b', 'semoga', text)\n    text = re.sub(r'\\bsmpai\\b', 'sampai', text)\n    text = re.sub(r'\\bnympe\\b', 'sampai', text)\n    text = re.sub(r'\\bdah\\b', 'sudah', text)\n    \n    text = re.sub(r'\\bberkali2\\b', 'repeated', text)\n    \n    text = re.sub(r'\\byg\\b', 'yang', text)\n    \n    return text","583b29df":"%%time\ntrain_df['review'] = train_df['review'].apply(recover_shortened_words)","bd99091b":"rating_mapper_encode = {1: 0,\n                        2: 1,\n                        3: 2,\n                        4: 3,\n                        5: 4}\n\n# convert back to original rating after prediction later(dont forget!!)\nrating_mapper_decode = {0: 1,\n                        1: 2,\n                        2: 3,\n                        3: 4,\n                        4: 5}\n\ntrain_df['rating'] = train_df['rating'].map(rating_mapper_encode)","3ddf065f":"train_df['rating'].value_counts()","402940fb":"from sklearn.utils import resample\ndf_majority = train_df[train_df.rating==4]\ndf_other = train_df[train_df.rating!=4]\n\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=500000,     # to match minority class\n                                 random_state=123) # reproducible results\n\ntrain_df = pd.concat([df_majority_downsampled, df_other])\n","00b9024d":"# zero,one,two,three,four = np.bincount(train_df['rating'])\n# total = zero + one + two + three + four\n\n\n# weight_for_0 = (1 \/ zero)*(total)\/5 \n# weight_for_1 = (1 \/ one)*(total)\/5\n# weight_for_2 = (1 \/ two)*(total)\/5\n# weight_for_3 = (1 \/ three)*(total)\/5\n# weight_for_4 = (1 \/ four)*(total)\/5\n\n# class_weight = {0: weight_for_0, 1: weight_for_1, 2:weight_for_2,3:weight_for_3,4:weight_for_4}\n# class_weight","f5bb780e":"train_df = train_df.drop_duplicates(subset =\"review\")","4a4ac973":"from tensorflow.keras.utils import to_categorical\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(train_df['rating'], num_classes=5)","5dcf1a20":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_df['review'],\n                                                  train_labels,\n                                                  stratify=train_labels,\n                                                  test_size=0.1,\n                                                  random_state=2020)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","a0d0fd1b":"MODEL = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFAutoModel.from_pretrained('bert-base-uncased')","e9cdc1b2":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.distplot(train_df['review'].str.len())\nMAX_LEN = 320\nplt.show()","2d5fefc9":"X_train_encode = regular_encode(X_train.values, tokenizer, maxlen=MAX_LEN)\nX_val_encode = regular_encode(X_val.values, tokenizer, maxlen=MAX_LEN)\nX_test_encode = regular_encode(test_df['review'].values, tokenizer, maxlen=MAX_LEN)","f35b50ef":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_encode, y_train))\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val_encode, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test_encode)\n    .batch(BATCH_SIZE)\n)","c5d7b9de":"def build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    sequence_output = tf.keras.layers.Dropout(0.2)(sequence_output)   \n    cls_token = sequence_output[:, 0, :]\n    out = Dense(5, activation='softmax')(cls_token) # 5 ratings to predict\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","800e68d8":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","cedcfb20":"n_steps = X_train.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","5ddfd69e":"import matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\n\n# Get training and test loss histories\ntraining_loss = train_history.history['loss']\ntest_loss = train_history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","c96ff145":"pred = model.predict(test_dataset, verbose=1)","5e692bb5":"# Check if this works\n\nimport pickle\nwith open('pred_bert.pkl','wb') as f:\n    pickle.dump(pred, f)","3bf8b0fc":"pred_sentiment = np.argmax(pred,axis = 1)","520e955a":"import pickle","7e31fedb":"with open('..\/input\/lastday\/bert-cased-500k.pkl','rb') as f:\n    pred_bert500_cased = pickle.load(f)\n    print(pred_bert500_cased.shape)","708b4573":"with open('..\/input\/lastday\/bert-uncased-500k.pkl','rb') as f:\n    pred_bert500_uncased = pickle.load(f)\n    print(pred_bert500_uncased.shape)","a15b97ac":"with open('..\/input\/lastday\/bert-based-500k-320length.pkl','rb') as f:\n    pred_bert500_cased_len320 = pickle.load(f)\n    print(pred_bert500_cased_len320.shape)","6667ae63":"pred_bert = (pred_bert500_cased + pred_bert500_uncased + pred_bert500_cased_len320)\/3","1e3460ba":"with open('..\/input\/lastday\/gpt2_pred_500k_len250.pkl','rb') as f:\n    pred_gpt500 = pickle.load(f)\n    print(pred_gpt500.shape)","ce9b626b":"with open('..\/input\/add-scrape\/gpt2_pred_4epoch_400.pkl','rb') as f:\n    pred_gpt = pickle.load(f)\n    print(pred_gpt.shape)","9c2f49d5":"pred_gpt = (pred_gpt500 + pred_gpt)\/2","77d631d3":"with open('..\/input\/add-scrape\/ROBERTA_pred_4epoch_subscore66.pkl','rb') as f:\n    pred_roberta = pickle.load(f)\n    print(pred_roberta.shape)","f8aa5909":"pred = (pred_bert + pred_gpt + pred_roberta ) \/ 3","b522f6d3":"final_pred = []\nconfident_3_index = []\nfor idx,p in enumerate(pred):\n    if np.argmax(p) == 2 and p[2]>0.438: \n#         print(idx)\n        confident_3_index.append(idx)\n        final_pred.append(2)  #because it's 0-4\n    else:\n        p[2] = 0\n        final_pred.append(np.argmax(p))\nfinal_pred\nsubmission = pd.DataFrame({'review_id':[i+1 for i in range(60427)],'rating':final_pred})\nsubmission['rating'].value_counts(normalize = True)\n","190c5651":"final_pred = []\nconfident_4_index = []\nfor idx,p in enumerate(pred):\n    if np.argmax(p) == 3 and p[3]>0.34: \n#         print(idx)\n        confident_3_index.append(idx)\n        final_pred.append(3)  #because it's 0-4\n    else:\n        p[3] = 0\n        final_pred.append(np.argmax(p))\nfinal_pred\nsubmission = pd.DataFrame({'review_id':[i+1 for i in range(60427)],'rating':final_pred})\nsubmission['rating'].value_counts(normalize = True)\n# [0.11388, 0.02350, 0.06051, 0.39692, 0.40519]","9cb8541a":"final_pred = []\nfor idx,p in enumerate(pred):\n    if np.argmax(p) == 4 and p[4]>0.2: \n#         print(idx)\n        final_pred.append(4)  #because it's 0-4\n    else:\n        p[4] = 0\n        if p[0] > 0.14:\n            final_pred.append(0)\n        else:\n            final_pred.append(np.argmax(p))\nfinal_pred\nsubmission = pd.DataFrame({'review_id':[i+1 for i in range(60427)],'rating':final_pred})\nsubmission['rating'].value_counts(normalize = True)\n# [0.11388, 0.02350, 0.06051, 0.39692, 0.40519]","525b4e94":"rating_mapper_decode = {0: 1,\n                        1: 2,\n                        2: 3,\n                        3: 4,\n                        4: 5}\n\nsubmission['rating'] = submission['rating'].map(rating_mapper_decode)\n\nsubmission.to_csv('3model_ensemble.csv', index=False)","4e17c77c":"# Public test set distribution  : [0.11388, 0.02350, 0.06051, 0.39692, 0.40519]\nsubmission.rating.value_counts(normalize = True)","d5eeadc6":"# Hi! Congratulations to all teams and the 1st place https:\/\/www.kaggle.com\/garyongguanjie who has taken rank 1 in the last hour!!\n\n### This is my first time writing a notebook and also my first coding competition. It has been a very fun ride and I'd really appreciate any comments on things I can improve.\n\n### Thanks to Indra Lin for writing cool notebook in which my notebook is mostly based on with some slight adjustments : https:\/\/www.kaggle.com\/indralin\/text-processing-augmentation-tpu-baseline-0-4544 \/\n\n### The Final score is obtained by ensembling Bert-base, GPT-2, and ROBERTa with some distribution adjustments.","eff13277":"# BERT","6ca93463":"# Ensembling and Distribution Adjustments\n\n* Our method of distribution adjustments is to keep confident predictions and throw out inconfident predictions to other class by adjusting probabilities threshold.\n\n* Note that a more optimal method can be found in these 2 notebooks\n    * garyong : (https:\/\/www.kaggle.com\/garyongguanjie\/lb-dist-hacking-final)   \n    * Team Servants of the Joy : (https:\/\/www.kaggle.com\/huikang\/week6-process-gpu-tpu-output)\n* But this is my first time and I'm happy with my results haha.\n\n* Note that the public distribution is [0.11388, 0.02350, 0.06051, 0.39692, 0.40519]","5a98b1cd":"### The cleaning below is mostly based on Indra Lin's Notebook with some adjustments","d76e2240":"### Adding a dropout layer have improved our performance ~2%","0414809d":"## Model Loading\n### We use the code above to create multiple BERT, GPT-2, and ROBERTA models and the final result is a simple average of the models (GPT-2,BERT, and ROBERTA)","4f12f8af":"### We tried to include class weights in our model but it seems it doesnt help with our predictions. (~2% worse)","f767c866":"## Training time! (Thanks to Indra Lin again!)","2b27eaf6":"## The training data used is the combination of the Shopee given training data, the data previous testset data and the scraped data Thanks to Tony Ng : https:\/\/www.kaggle.com\/c\/student-shopee-code-league-sentiment-analysis\/discussion\/170953","5ba50368":"## Review some train data that matches the test set","63f17926":"### It seems that most of the reviews are about 300words. I have experiemented MAX_LEN = 512 but the run time takes a little too long so we go with 320 just to be safe.","4e7ea032":"## Based on our experiment number of epochs trained for best validation score are as follows:\n\n### BERT: 2 Epochs\n### GPT-2 : 4 Epochs\n### ROBERTA: 4 Epochs","a4445f4e":"## Append original train with the scraped training data","4eb57012":"# ROBERTA","7277e20f":"### Let's drop some duplicates","f5dd9ace":"## Inspecting Data leak\n\n### The data scraped may match the test set reviews so I wanted to verify that the most of the scraped data doesn't match the reviews in the test set (I do not want to feel guilty about using too much leaked data.)","7c7da47f":"# GPT-2","34dd4635":"### Since the scraped data mostly contains rating = 5 (4 in a 0-4 scale). Sadly, we did not have much time to try to choose what to include so we have to downsampled to about ~500k of rating = 5 to account for class imbalance and reduce training time.","7f49c086":"# Thank you all for reading through my notebook till the end. I've gained alot of experience learning from you all (especially from public notebooks) and I hope that I'll be able to learn more. See you all again soon! :)","047e54e6":"### Seems like there're some leakage but most of them are general review like \"Awesome product\", \"Fast respond\" etc...","d5a8d743":"### Seems like reviews with different punctuation has different target distribution which may help our model predict better!\n### Fortunately, pre-trained models has ids representing most punctuation so this is good for us!","5c46c4de":"### EDA some effects of punctuation"}}