{"cell_type":{"a7c9f656":"code","a333bd02":"code","84646d7a":"code","76a1224b":"code","bbcefafb":"code","4d422440":"code","fc832636":"code","8fdca44e":"code","aef0a8f2":"code","a9f9fba4":"code","c5574f4a":"code","d6b71783":"code","aca99fbb":"code","0b2a7318":"code","0bf550bf":"code","2dd714f3":"code","48cfdbed":"markdown","4ab59015":"markdown","b939f0ac":"markdown","94de2e5f":"markdown","fd690b59":"markdown","b18a5ce9":"markdown","22a7d164":"markdown","4ab52d44":"markdown","29b6067d":"markdown","020167df":"markdown","2e49bf60":"markdown","a9f05aef":"markdown","79d20640":"markdown","0be8be0a":"markdown","a3344aff":"markdown","642b5923":"markdown","0426be59":"markdown","d766eea8":"markdown","250b5372":"markdown"},"source":{"a7c9f656":"# Helper libraries\nimport os\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport json\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\n# TensorFlow and Keras\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Input\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization","a333bd02":"path = '..\/input\/noisy-and-rotated-scanned-documents\/scan_doc_rotation\/'","84646d7a":"# Load and open images\nnames = [ file for file in os.listdir(path+'images\/') ]\nnames = sorted(names)\nN = len(names)\n\nprint(names[:10])\n\n# Load two images to check\nimages = [Image.open(path+'images\/'+names[i]) for i in range(2)]\n\nplt.figure(figsize=(14,8))\nplt.subplot(121),plt.imshow(images[0], cmap = 'gray')\nplt.subplot(122),plt.imshow(images[1], cmap = 'gray')\n\nprint('Total number of images: %d'%N)","76a1224b":"eg_img = path+'\/images\/'+names[18]\nimg = cv2.imread(eg_img)\nimg = img[:,:,0] #zeroth component is the red from RGB channel ordering\nf = cv2.dft(np.float32(img))\nfshift = np.fft.fftshift(f)\nf_abs = np.abs(fshift) + 1.0 #shift to ensure no zeroes are present in image array\nf_img = 20 * np.log(f_abs)\n\nplt.figure(figsize=(14,8))\n\nplt.subplot(121),plt.imshow(img, 'gray')\nplt.title('Original scan')\n\nplt.subplot(122),plt.imshow(f_img, 'gray')\nplt.title('FFT of scan')\nplt.colorbar() \nplt.show()","bbcefafb":"from skimage.restoration import denoise_tv_chambolle\n\nimg = denoise_tv_chambolle(img, weight=1.0, multichannel=0)\n                               \nf = cv2.dft(np.float32(img))\nfshift = np.fft.fftshift(f)\nf_abs = np.abs(fshift) + 1.0\nf_img = 20 * np.log(f_abs)\n\nplt.figure(figsize=(14,8))\nplt.subplot(121),plt.imshow(img, 'gray')\nplt.subplot(122),plt.imshow(f_img, 'gray')\nos.system('mkdir -p .\/processed')\ncv2.imwrite(path+'\/processed\/'+names[18], f_img)\nplt.colorbar() \nplt.show()","4d422440":"fft_images = []\nfor i in range(N):\n    img = cv2.imread(path+'\/images\/'+names[i])\n    img = img[:,:,0]\n    img = denoise_tv_chambolle(img, weight=1.0, multichannel=0)\n    f = cv2.dft(np.float32(img))\n    fshift = np.fft.fftshift(f)\n    f_abs = np.abs(fshift) + 1.0 # shift to avoid np.log(0) = nan\n    f_img = 20 * np.log(f_abs)\n    fft_images.append( f_img )\n    cv2.imwrite(path+'\/processed\/'+names[i], f_img)","fc832636":"# Load and open labels\nlabel_names = [ file for file in os.listdir(path+'.\/labels') ]\nlabel_names = sorted(label_names)\nM = len(label_names)\n\nprint(label_names[:10])\n\nlabels = [ np.loadtxt(path+'.\/labels\/'+label_names[j])\nfor j in range(M) ]\nlabels = [ round(float(labels[j])) for j in range(M) ]\n\n# Load first 10 labels\n[print(labels[i]) for i in range(10)]\nprint('Total number of labels %d'%len(labels))","8fdca44e":"# Deserialize JSON data lists for training and test sets\nwith open(path+'train_list.json') as train_data:\n    train = json.load(train_data)\n    \ntrain_size = len(train)\nprint('Training set size: %d'%train_size)\n\nwith open(path+'test_list.json') as test_data:\n    test = json.load(test_data)\n    \ntest_size = len(test)\nprint('Test set size: %d'%test_size)","aef0a8f2":"# Get images into tensor form\nimage_arr = [ tf.keras.preprocessing.image.img_to_array(fft_images[i]) \nfor i in range(N) ]\n\n# get pixel dimensions of image\nimg_height = image_arr[0].shape[0]\nimg_width = image_arr[0].shape[1]\n\n# Training and test image stacks\nX_train = tf.stack(image_arr[:train_size], axis=0, name='train_set')\nX_test = tf.stack(image_arr[-test_size:], axis=0, name='test_set')\n\npixel_count = img_height * img_width\n\n# Reshape to 3D Tensor\nX_train = np.array(X_train).reshape(train_size, pixel_count)\nX_test = np.array(X_test).reshape(test_size, pixel_count)\n\n# Normalise pixel values\nX_train \/= 255\nX_test \/= 255\n\n# Training labels\nY_train_ = np.array(labels).reshape(len(labels))\n\n# Check shape of each tensor\nprint(X_train.shape, Y_train_.shape)\nprint(X_test.shape)","a9f9fba4":"# Show number of unique labels\nclasses = np.unique(Y_train_)\nprint(classes)\nn_classes = len(np.unique(Y_train_))\n\n# Create classes from unique labels\nY_train = to_categorical(Y_train_, n_classes)\nprint(Y_train.shape)","c5574f4a":"model = Sequential()\n\nmodel.add(Dense(512, activation='relu', \n                 input_shape=(pixel_count,)))\n\nmodel.add(Dense(512, activation='relu'))\n\nmodel.add(Dense(512, activation='relu'))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(n_classes, activation = 'softmax'))","d6b71783":"model.compile(optimizer = 'adam', \n             loss = 'categorical_crossentropy', \n             metrics = ['accuracy'] )","aca99fbb":"history = model.fit(X_train, Y_train, batch_size=32, epochs=25, verbose=2)","0b2a7318":"predictions = model.predict_classes(X_test)\nprint(predictions)","0bf550bf":"df = pd.DataFrame({'Image name': names[-test_size:], 'Rotated angle (deg)': classes[predictions]})\ndf.head(10)","2dd714f3":"fig = plt.figure()\nplt.subplot(2,1,1)\nplt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\n\nplt.subplot(2,1,2)\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\n\nplt.tight_layout()","48cfdbed":"The scanned documents are loaded as images via the using the Python Imaging Library (PIL). Two examples are shown below.   \n","4ab59015":"The images are shaped in the standard TensorFlow way for inputs: <br> (batch_size, img_height, img_width). The training images and their corresponding labels are loaded as X_train and Y_train, respectively. <br>\nThe goal is to feed these into the input layer of the CNN and train the model. If trained well, the model should make accurate predictions for the scanned angles of unlabelled document scans. <br>\nI have borrowed the architecture of the hand-written digits CNN classifier for this problem. Below the model is instantiated and four convolutional layers between the input and output layers. There are 512 nodes in the input and hidden layers. In the output layer, 9 nodes represent each of the 9 classes for the angle prediction. ","b939f0ac":"There are 500 labels in total for the 500 training set images. That works out since this is supervised learning. \nThe list of training and test set images have been provided in JSON data objects. ","94de2e5f":"The numbers represent the indices of the classes labelled from $-5$ to $5^\\circ.$ The first 10 predictions are shown in the dataframe below. ","fd690b59":"Import the necessary libraries for the workflow. ","b18a5ce9":"# Noisy and Rotated Scanned Documents\n\n<img src=\"https:\/\/i.stack.imgur.com\/KAuwx.png\"  width=\"600\" >\n\n## Problem Description\nAn insurance company is working on a system that can perform Optical Character\nRecognition (OCR) on documents from their archives. Unfortunately, their documents have\nbeen scanned at angles ranging from -5\u00b0 to 5\u00b0 from the horizontal. To increase OCR accuracy,\nthey need a preprocessing step that determines the angle of a given page so this distortion can\nbe corrected.","22a7d164":"From the image and label data, it is clear that 500 images have known rotations and 100 do not which makes sense in light of the sizes of the training and test sets read from the JSON data files. ","4ab52d44":"### 3. Reporting","29b6067d":"There appears to be a bit of angular detail implicit in the FFT image of the original scanned document in this step. I will go ahead and process the rest of the images in this way. There may be better denoising methods for this case and, ideally, I would spend some more time finding the optimal denoising method that would provide the clearest angle definition in the FFT. ","020167df":"The FFT does not provide clear orientation detail and this may be due to the noise - the little black dots scattered across the page. We can attempt to denoise the scanned page with a filter from the scikit image library and get a FFT of the denoised image to find its orientation. ","2e49bf60":"The images load correctly and there are 600 of them in total. Through trial and error, I realised that feeding these raw images into a CNN to train a model to find rotation angles of similar documents will be difficult because of difference between the texts contained within each document. The images need to be preprocessed to a simpler format which gives away only their rotation angle. Much credit is due to a user of Quora who provided <a href = \"https:\/\/www.quora.com\/How-do-I-detect-a-rotated-image-and-fix-it-back-to-its-proper-position-using-Python-OpenCV\">this hint to perform a Fast Fourier Transform on the scanned image first.<\/a> This is one existing method for denoising an image and extracting frequency or amplitude from it which reveal the angular orientation [1]. An example of the FFT of one scanned document from the batch is shown below. ","a9f05aef":"## Approach\n\nThe input of learning algorithm will be the scanned document images and the labels which are the different possible scanning angles. <br>\n\nJSON data files provide a list of the scanned images which form the training and test sets separately. The algorithm will learn from the training set scanned document images and be validated on the test set images. The output of the learning algorithm will be the images labelled by their angles which have been predicted by the learning method. <br>\n\nLabels have been provided, hence it is best to treat this a classification problem where the scanned pages are images that are labelled according to their scanned angle. With a training set consisting of images and their associated labels, a convolutional neural network (CNN) can be trained to recognise the angular rotation of new scanned images fed to it. This will form the preprocessing step needed to recognise the misalignment in scanned images in order to straighten them. <br>\n\nThe machine learning workflow designed for this is split into the three recommended stages: <br> 1. Ingestion <br> 2. Algorithm Design <br> 3. Reporting ","79d20640":"We load the labels as integers to simplify the classification. This means that the angles, $\\theta,$ will be classed as $\\theta \\in [-5,5]^{\\circ}$ for $\\theta \\in \\mathbb{Z}$ and that will give is 9 classes in total with which the CNN can classify new images.<br>\nNote: I have set the negative angles to denote angular rotations below the horizontal (text tilted to the left) and positive angles above the horizontal (text titled to the right). ","0be8be0a":"In the interest of quantifying performance, the accuracy and loss are shown as a function of training iterations. ","a3344aff":"The following is done in the next steps:<br>\n$\\bullet$ Converted the FFT images into arrays <br>\n$\\bullet$ Stack them into a batch of test and training data <br>\n$\\bullet$ Reshape the images to tensors <br>","642b5923":"### 1. Data Ingestion","0426be59":"This kernel was loads of fun to write and I am happy with the result. \nLooking forward to seeing what other interesting things can be done with this dataset. \n\n### References:\n[1] https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras <br>\n[2] https:\/\/github.com\/d4nst\/RotNet <br>\n[3] https:\/\/timdettmers.com\/2015\/03\/26\/convolution-deep-learning\/#  <br>\n[4] https:\/\/machinelearningmastery.com\/display-deep-learning-model-training-history-in-keras\/\n","d766eea8":"### 2. Algorithm Selection\nTo recap, given that the training set data is labelled, this learning method will behave as a classifier and will train on the scanned images that have labelled angular rotations. The aim is to use the labelled data to select train an algorithm to correctly predict the angular rotations of 100 images in the test set as well as other scanned document images of a similar nature. ","250b5372":"The model is trained and can make predictions on the angular rotation of scanned documents that it is given. I begin, of course, with the 100 scanned images in the test set. "}}