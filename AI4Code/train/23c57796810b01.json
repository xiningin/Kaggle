{"cell_type":{"9c72ac5a":"code","8a4a62a0":"code","ef1a098f":"code","e979245b":"code","e08efcb1":"code","ae063fed":"code","f35602d7":"code","1bfc1ce6":"code","e7a3f574":"code","09adca72":"code","09459cb9":"code","5f505e1e":"code","1519e6e7":"code","7981b250":"code","8de8ff35":"code","0ce7269f":"code","5b1e1ea6":"code","99e30097":"code","8338c575":"code","5c44dfe2":"code","7bcf23b2":"code","0b0bc0db":"code","34524c22":"code","577fc91b":"markdown","9d36e349":"markdown","291b44c6":"markdown","6d2f4bc5":"markdown","4efc36ad":"markdown","37e8c053":"markdown","3d47617b":"markdown","4b5353ab":"markdown","af1e41b5":"markdown","f95ff476":"markdown","a15c7952":"markdown","7323e0ae":"markdown","1ef4d55d":"markdown","69f1f5e7":"markdown","b6c4e684":"markdown","b559fde1":"markdown","2acf25ac":"markdown","e7762434":"markdown","ed656745":"markdown","8b8360c2":"markdown","28803a59":"markdown","22b7bf6f":"markdown","05d8b4e1":"markdown","0f797012":"markdown","2a84b8a2":"markdown"},"source":{"9c72ac5a":"from IPython.display import Image\nurl = 'http:\/\/ep60qmdjq8-flywheel.netdna-ssl.com\/wp-content\/uploads\/2009\/08\/commercial-real-estate.jpg'\nImage(url=url,width=800, height=600)","8a4a62a0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom xgboost import XGBRegressor\nimport warnings\n\n# Turn off the nagging warnings from sklearn and seaborn\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\n# Identify numeric columns\ndef numeric_cols(data_frame):\n    numeric_cols = [cname for cname in data_frame.columns if \n                                data_frame[cname].dtype in ['int64', 'float64']]\n    return(numeric_cols)\n\n# Identify categorical columns with low cardinality (a small number of distinct values)\ndef low_cardinality_cols(data_frame):\n    low_cardinality_cols = [cname for cname in data_frame.columns if \n                                data_frame[cname].nunique() < 50 and\n                                data_frame[cname].dtype == \"object\"]\n    return(low_cardinality_cols)\n\n# Identify columns with missing data\ndef cols_with_missing(data_frame):\n    cols_with_missing = [cname for cname in data_frame.columns \n                                 if data_frame[cname].isnull().any()]\n    return(cols_with_missing)","ef1a098f":"# Read core training and test data\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\n\nprint(\"Train set size:\", train_data.shape)\nprint(\"Test set size:\", test_data.shape)","e979245b":"print(\"Columns in training data but not in testing data\")\nprint([x for x in train_data.columns if x not in test_data.columns])\nprint(\"Columns in testing data but not in training data\")\nprint([x for x in test_data.columns if x not in train_data.columns])","e08efcb1":"# view the data and get the statistical properties\ntrain_data.describe(include = 'all')\n#train_data.info()","ae063fed":"if train_data['Id'].nunique() == train_data['Id'].size:\n    train_data.drop(['Id'],axis=1,inplace=True)","f35602d7":"corrmat = train_data.corr()\nplt.subplots(figsize=(12,9))\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corrmat, mask=mask, vmax=0.9, cmap=\"YlGnBu\",\n            square=True, cbar_kws={\"shrink\": .5})","1bfc1ce6":"# Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# pull all data data into target (y) and predictors (X)\ntrain_y = train_data.SalePrice\n\n# only keep data that is either numeric or has a low cardinality for categorical data\n# in this case it keeps most all of the columns\ntargeted_train_X_cols = low_cardinality_cols(train_data) + numeric_cols(train_data)\ntrain_X = train_data[targeted_train_X_cols]\ntrain_X = train_X.drop(['SalePrice'], axis=1)\ntrain_X = pd.get_dummies(train_X)\n\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntargeted_test_X_cols = low_cardinality_cols(test_data) + numeric_cols(test_data)\ntest_X = test_data[targeted_test_X_cols]\ntest_X = pd.get_dummies(test_X)\n\n# inner join the data to ensure the exact columns included are aligned\ntrain_X, test_X = train_X.align(test_X,\n                                join='inner', \n                                axis=1)","e7a3f574":"# Get columns that are most correlated to SalePrice to scatter plot them\ntesting_columns = corrmat.loc[(corrmat.index != 'SalePrice'), (corrmat.SalePrice > 0)].index\n\n# Count subplots, total columns of plot, and total rows of plots\nsubplot_count = int(len(testing_columns))\ncol_count = 2\nrow_count = int((subplot_count\/col_count)+1)\n\n# Initiate the subplots\nfig, axes = plt.subplots(nrows=row_count, \n                         ncols=col_count, \n                         sharey=True, \n                         figsize=(12,row_count*2), \n                         squeeze=False)\naxes_list = [item for sublist in axes for item in sublist] \nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=1.5, wspace=0.4)\niter_col_count = 0\niter_row_count = 0\n\n# Loop through all columns to create a subplot for each\nfor i in testing_columns:\n    ax = axes_list.pop(0)\n    ax = sns.regplot(train_X[i],\n                     train_y,\n                     ax=axes[iter_row_count][iter_col_count],\n                     color='blue'\n                    )\n    if iter_col_count == col_count - 1:\n        iter_col_count = 0\n        iter_row_count = iter_row_count + 1\n    else:\n        iter_col_count = iter_col_count + 1\n    ax.set_title(i)\n    ax.tick_params(which='both', bottom='off', left='off',\n                   right='off', top='off'\n                  )\n    ax.spines['left'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n# delete anything we didn't use\nfor ax in axes_list:\n    ax.remove()","09adca72":"# Check out \nexplore_data = train_X.copy()\nexplore_data['SalePrice'] = train_data.SalePrice\n\nsns.lmplot(x='GrLivArea', y='SalePrice', hue='OverallQual',\n           #markers=['o', 'x', '*'], \n           data=explore_data, fit_reg=False)\n\nsns.lmplot(x='GrLivArea', y='SalePrice', hue='OverallQual',\n           #markers=['o', 'x', '*'], \n           data=explore_data.loc[explore_data['OverallQual'] == 10], fit_reg=False)","09459cb9":"# detect outliers\noutlier_detect = explore_data.loc[explore_data['OverallQual'] == 10]\noutlier_detect = outlier_detect[['SalePrice','GrLivArea']]\noutlier_detect['outlier'] = LocalOutlierFactor(n_neighbors=20).fit_predict(outlier_detect)\nprint(outlier_detect)","5f505e1e":"# remove outliers from primary training data sets\noutliers = outlier_detect.loc[outlier_detect['outlier']==-1].index.values\nprint('Shape of data prior to removal')\nprint(train_X.shape)\nprint(train_y.shape)\ntrain_X.drop(outliers, inplace=True)\ntrain_y.drop(outliers, inplace=True)\nprint('Shape of data after removal')\nprint(train_X.shape)\nprint(train_y.shape)","1519e6e7":"plt.subplots(figsize=(12,9))\nplt.subplot(1, 2, 1)\nsns.distplot(train_y)\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log1p(train_y))\nplt.xlabel('Log SalePrice')","7981b250":"# perform log transformation to reduce skew\n#train_y = np.log1p(train_y)","8de8ff35":"# Add Total SF\nfeatures_to_sum = ['TotalBsmtSF','1stFlrSF','2ndFlrSF']\ntrain_X['newTotalSF'] = train_X[features_to_sum].sum(axis=1)\ntest_X['newTotalSF'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Finished SF\n#features_to_sum = ['BsmtFinSF1','BsmtFinSF2','1stFlrSF','2ndFlrSF']\n#train_X['newFinTotalSF'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newFinTotalSF'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Bathrooms\n#features_to_sum = ['FullBath','HalfBath','BsmtFullBath','BsmtHalfBath']\n#train_X['newTotalBath'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newTotalBath'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Porch SF\n#features_to_sum = ['OpenPorchSF','3SsnPorch','EnclosedPorch','ScreenPorch','WoodDeckSF']\n#train_X['newTotalPorchSF'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newTotalPorchSF'] = test_X[features_to_sum].sum(axis=1)","0ce7269f":"# create pipeline\nmy_pipeline = make_pipeline(Imputer(),\n                            XGBRegressor(n_estimators=1000, \n                                         learning_rate=0.05)\n                           )","5b1e1ea6":"# Scoring - Root Mean Squared Error\ndef rmse_score(model,X,y):\n    return np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n\n# Scoring - Mean Absolute Error\ndef mae_score(model,X,y):\n    return -cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\", cv=5)\n\n# score the pipeline\n#mae = mae_score(my_pipeline,train_X,train_y)\n#print('Mean Absolute Error: {}'.format(mae.mean()))\n#rmse = rmse_score(my_pipeline,train_X,train_y)\n#print('Root Mean Squared Error: {}'.format(rmse.mean()))\nrmse_log = rmse_score(my_pipeline,train_X,np.log1p(train_y))\nprint('Root Mean Squared Error with log tranform: {}'.format(rmse_log.mean()))","99e30097":"# train the model\nmodel = my_pipeline.fit(train_X, train_y)\n\n# Use the model to make predictions\nfinal_predicted_prices = model.predict(test_X)\n\n# transform log values for sale price back into regular sale price\n#final_predicted_prices = np.expm1(final_predicted_prices)\n\n# round final prices\nfinal_predicted_prices = [round(x,2) for x in final_predicted_prices]\n\n# look at the predicted prices to ensure we have something sensible.\nprint(final_predicted_prices[:5])","8338c575":"#TO DO - if I want this to work it seems I'll need to concentrate on only numeric values\n\n#import eli5\n#from eli5.sklearn import PermutationImportance\n\n# I'm using training data here for now since I didn't take the time to split out validation data \n#perm = PermutationImportance(model, random_state=1).fit(train_X, train_y)\n\n#eli5.show_weights(perm, feature_names = train_X.columns.tolist())","5c44dfe2":"from pdpbox import pdp, get_dataset, info_plots\n\ncompare_cols = [cname for cname in test_X.columns]\n\nfeatures_to_review = ['OverallQual','YearBuilt','MoSold','Fireplaces']\n\nfor feat_name in features_to_review:\n    pdp_dist = pdp.pdp_isolate(model=model, dataset=test_X, model_features=compare_cols, feature=feat_name, num_grid_points=100)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()\n\n#help(pdp)","7bcf23b2":"features_to_review2 = ['OverallQual','YearBuilt']\n\npdp_inter1 = pdp.pdp_interact(model=model, dataset=test_X, model_features=compare_cols, features=features_to_review2)\npdp.pdp_interact_plot(pdp_interact_out=pdp_inter1, feature_names=features_to_review2, plot_type='contour')\nplt.show()    ","0b0bc0db":"import shap  # package used to calculate Shap values\n\ndef prediction_factors(model, predict_data, core_data):\n    # Had to use KernelExplainer instead of TreeExplainer due to pipeline\n    explainer = shap.KernelExplainer(model, core_data)\n    shap_values = explainer.shap_values(predict_data)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], predict_data)\n\nsample_data_for_prediction = test_X.iloc[0]\nprediction_factors(my_pipeline, sample_data_for_prediction, test_X)","34524c22":"# submit results\nmy_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': final_predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)","577fc91b":"**Create pipeline with models**","9d36e349":"**Sum multiple features together to get overall totals**","291b44c6":"# 5. Submit results","6d2f4bc5":"**Drop the Id column since it isn't worth keeping**\n\nBy determining that all values of this column are unique I feel comfortable dropping it","4efc36ad":"**Run outlier detection on Overall Quality of 10**\n\nFrom the chart above we can clearly see that there are a few outliers for quality 10 so we'll run the Local Outlier Factor to detect these outliers. (anything with a -1 is an outlier)","37e8c053":"# 3. Feature engineering","3d47617b":"**Check for skew in the sales price vs transform with log**","4b5353ab":"This is the basis of my submission to the [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition. This notebook was started based on the lessons on machine learning curated on [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/machine-learning) then refined further to achieve more accuracy and additional visualization.\n\n<br\/>\nI also was able to learn some new techniques from the following kernels:\n* [Laurenstc's kernel](https:\/\/www.kaggle.com\/laurenstc\/top-2-of-leaderboard-advanced-fe)\n* [Aleksandrs Gehsbargs's kernel](https:\/\/www.kaggle.com\/agehsbarg\/top-10-0-10943-stacking-mice-and-brutal-force)\n* [Serigne's kernel](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n* [Massquantity's kernel](https:\/\/www.kaggle.com\/massquantity\/all-you-need-is-pca-lb-0-11421-top-4)\n* [Fkstepz's kernel](https:\/\/www.kaggle.com\/fkstepz\/step-by-step-house-prices-prediction)\n\n<br\/>\nThe notebook is arranged into the following sections:<br\/>\n1. Prep<br\/>\n2. Exploratory Visualization and Data Cleansing<br\/>\n3. Feature Engineering<br\/>\n4. Modeling<br\/>\n5. Submit results","af1e41b5":"**Score pipeline**","f95ff476":"**Remove the outliers from our data set**","a15c7952":"**View statistical properties of the data**","7323e0ae":"**Get the data into  better working form**","1ef4d55d":"**Plot all significant features along with a regression line**","69f1f5e7":"**Load the data**","b6c4e684":"**SHAP Values for one house**","b559fde1":"**Train the model**","2acf25ac":"# 2. Exploratory Visualization and Data Cleansing","e7762434":"# 4. Modeling\nBuild best model and predict final results","ed656745":"**View correlation of data**\n\nView the correlation map of data with clean colors and elimination of the duplicate display on the upper right hand corner of the chart that comes with it by default.","8b8360c2":"**Permutation Importance - Importance of various features**","28803a59":"**Plot sales price against Living Area sliced by Overall Quality**","22b7bf6f":"**Partial Dependence Plot (PDP)**","05d8b4e1":"# 1. Prep\nImport all libraries, define functions, and load data.","0f797012":"**Import all libraries and define functions**","2a84b8a2":"**Confirm column overlap between train and test**\n\nWhile it can be assumed that all of the columns match in these data sets, I still wanted to get the code in here to confirm that SalePrice is the only column difference between the two."}}