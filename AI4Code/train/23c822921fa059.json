{"cell_type":{"95c514ce":"code","14e06ea8":"code","39e32912":"code","fd81204d":"code","40ec696d":"code","f8cff4bf":"code","6ff1b435":"code","00a1d024":"code","14aa8844":"code","04e46231":"code","8bc653c5":"markdown","25e5bf1e":"markdown","86bbeb98":"markdown","0b904387":"markdown","84228169":"markdown","d4943a09":"markdown","bffa93a7":"markdown","a6bf3dfb":"markdown"},"source":{"95c514ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14e06ea8":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Input, Dense, Activation, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom matplotlib import pyplot\n","39e32912":"train_dir = '..\/input\/digit-recognizer\/train.csv'\n\ndata = pd.read_csv(train_dir)\n# extract x,y training data\ny_train = data['label']\ny_train = to_categorical(y_train.values, num_classes=10)\nprint(y_train.shape)\nx_train = data.drop(labels = ['label'], axis = 1)\nx_train = x_train.to_numpy()\n# normalise values into [0,1] range to speed up training\nx_train = x_train\/255\n# resize to represent 28x28 image\n# x_train shape changes from (42000,784) to (42000, 28, 28, 1) \nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nprint(x_train.shape)","fd81204d":"for i in range(9):\n    pyplot.subplot(330 + 1 + i)\n    pyplot.imshow(x_train[i][:,:,0])","40ec696d":"test_dir = '..\/input\/digit-recognizer\/test.csv'\ntest = pd.read_csv(test_dir)\ntest = test.to_numpy()\ntest = test\/255\ntest = test.reshape(test.shape[0], 28, 28, 1)","f8cff4bf":"simple_cnn = Sequential()\nsimple_cnn.add(Conv2D(filters=20, kernel_size=(2, 2), activation='relu', input_shape=(28,28,1)))\nsimple_cnn.add(MaxPool2D())\nsimple_cnn.add(Conv2D(filters=20, kernel_size=(2, 2), activation='relu'))\nsimple_cnn.add(MaxPool2D())\nsimple_cnn.add(Flatten())\nsimple_cnn.add(Dense(units = 120, activation = 'relu'))\nsimple_cnn.add(Dense(units = 10, activation = 'softmax'))\n\nsimple_cnn.compile('adam', 'categorical_crossentropy',metrics = ['acc'])\nsimple_cnn.summary()","6ff1b435":"# Early stopping dependent on validation accuracy\nes = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n\nhistory = simple_cnn.fit(x_train, y_train, validation_split = 0.2, epochs = 500, callbacks = [es])","00a1d024":"pyplot.plot(history.history['acc'], label='train')\npyplot.plot(history.history['val_acc'], label='val')\npyplot.legend()\npyplot.show()","14aa8844":"preds = simple_cnn.predict(test)\npreds = np.argmax(preds,axis = 1)\npreds","04e46231":"results = pd.DataFrame()\nresults['ImageId'] = np.arange(len(preds)) + 1\nresults['Label'] = pd.Series(preds)\nresults\n# index false so we don't write row names\nresults.to_csv('submission.csv', index = False)","8bc653c5":"Let's now visualise some of the images in the training set. Here we display the first 9 images in a 3x3 grid. You can see the variation between 1's and 0's which will need to be learnt by the classifier. ","25e5bf1e":"The training data is in the form of a csv file where each digit image is represented by 784 columns. Hence, need to reformat this data (to 28x28) to allow a CNN to operate on it as it would with an image.","86bbeb98":"Early stopping means that the model stops training if the validation accuracy hasn't increased in n training epochs (here n = 10). It's included as a callback in model.fit() so it is called on every training cycle.","0b904387":"Compile the results in the specified format (as given in sample_submission.csv)","84228169":"Define the CNN architecture. Due to the small input images, I have chosen a relatively shallow CNN with small kernel sizes (2,2). Categorical cross entropy is chosen as the loss function as it works well for multi-class classification problems. Dropout is used to prevent overfitting.","d4943a09":"Plot the results so underfitting\/overfitting problems can be easily diagnosed.","bffa93a7":"Predict the values on the test dataset using the newly trained model.","a6bf3dfb":"This notebook contains a simple CNN architecture which will be trained to perform digit recognition. The following parts will be considered:\n* Converting CSV file data to an image representation (square matrix) using Numpy \n* Visualising the dataset using MatPlotLib\n* Defining a CNN architecture using Keras\n* Using early stopping as part of a Keras callback to stop training early when accuracy stops increasing\n* Plotting curves of accuracy and validation accuracy\n* Predicting the classifications of a test dataset\n* Compile the results into a specified .csv format\n\nThese steps can be applied to a variety of data science problems."}}