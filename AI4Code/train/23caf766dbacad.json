{"cell_type":{"10bc2779":"code","e95c182a":"code","aeddde6e":"code","3a5f6072":"code","40e87f15":"code","6507ce5b":"code","ee35d58b":"code","4b790dc9":"code","59da49a0":"code","837fb1af":"code","8b7a691d":"code","74345ab6":"code","ca7b9587":"code","ad249e01":"code","af3111a8":"code","1c6c8ca0":"code","21dab3e6":"code","e3c00cea":"code","5fd68a60":"code","ff5f33bd":"code","2463c56a":"code","2f868bbf":"code","4f81b80d":"code","847350db":"code","03d43663":"code","1bf7bb99":"code","f5b12357":"code","edf05a23":"code","610ce32e":"code","63d79321":"code","594aa893":"code","68285aa8":"code","ad1cf7ef":"code","332e6bff":"code","4d508a12":"code","fe2b8a46":"code","180d5fd1":"code","ea5e162a":"code","651d4480":"code","cedb7cfb":"markdown","48e7e4ba":"markdown","2d185f3f":"markdown","45036cd3":"markdown","596b2e9e":"markdown","24ea43a1":"markdown","81494ebc":"markdown","6154ef1d":"markdown","d3e40c23":"markdown","42b514a0":"markdown","061bfe13":"markdown","5ec626a4":"markdown","8650a3e6":"markdown","a3e50b81":"markdown","8516010d":"markdown","6bafd3a6":"markdown","056441ea":"markdown","72ddb873":"markdown","90f022c4":"markdown","9b442428":"markdown","0c56a679":"markdown","c0be0302":"markdown","426e1bb1":"markdown","44cbf418":"markdown","d068a5f7":"markdown","b5fe8e99":"markdown","884470f8":"markdown","82922b87":"markdown","c116bacb":"markdown","9b4b4d85":"markdown","23ed1c3e":"markdown","8580dc23":"markdown","94f1c8bc":"markdown","97376bec":"markdown","010b42a8":"markdown","35148baa":"markdown","1119dc99":"markdown","69524072":"markdown"},"source":{"10bc2779":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n%matplotlib inline","e95c182a":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")\n\nprint(\"Train dataset has {} samples and {} attributes\".format(*train.shape))\nprint(\"Test dataset has {} samples and {} attributes\".format(*test.shape))","aeddde6e":"train.head()","3a5f6072":"n=len(train)\nsurv_0=len(train[train['Survived']==0])\nsurv_1=len(train[train['Survived']==1])\n\nprint(\"% of passanger survived in train dataset: \",surv_1*100\/n)\nprint(\"% of passanger not survived in train dataset: \",surv_0*100\/n)","40e87f15":"cat=['Pclass','Sex','Embarked']\nnum=['Age','SibSp','Parch','Fare']","6507ce5b":"corr_df=train[num]  #New dataframe to calculate correlation between numeric features\ncor= corr_df.corr(method='pearson')\nprint(cor)","ee35d58b":"fig, ax =plt.subplots(figsize=(8, 6))\nplt.title(\"Correlation Plot\")\nsns.heatmap(cor, mask=np.zeros_like(cor, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","4b790dc9":"csq=chi2_contingency(pd.crosstab(train['Survived'], train['Sex']))\nprint(\"P-value: \",csq[1])","59da49a0":"csq2=chi2_contingency(pd.crosstab(train['Survived'], train['Embarked']))\nprint(\"P-value: \",csq2[1])","837fb1af":"csq3=chi2_contingency(pd.crosstab(train['Survived'], train['Pclass']))\nprint(\"P-value: \",csq3[1])","8b7a691d":"print(train.isnull().sum())","74345ab6":"print(test.isnull().sum())","ca7b9587":"train['Age'].describe()","ad249e01":"med=np.nanmedian(train['Age'])\ntrain['Age']=train['Age'].fillna(med)\ntest['Age']=test['Age'].fillna(med)","af3111a8":"train['Cabin'].value_counts()","1c6c8ca0":"train['Cabin']=train['Cabin'].fillna(0)\ntest['Cabin']=test['Cabin'].fillna(0)","21dab3e6":"train['Embarked'].value_counts()","e3c00cea":"train['Cabin']=train['Cabin'].fillna(\"S\")","5fd68a60":"train['Fare'].describe()","ff5f33bd":"med=np.nanmedian(train['Fare'])\ntest['Fare']=test['Fare'].fillna(med)","2463c56a":"train['hasCabin']=train['Cabin'].apply(lambda x: 0 if x==0 else 1)\ntest['hasCabin']=test['Cabin'].apply(lambda x: 0 if x==0 else 1)","2f868bbf":"train['FamilyMem']=train.apply(lambda x: x['SibSp']+x['Parch'], axis=1)\ntest['FamilyMem']=test.apply(lambda x: x['SibSp']+x['Parch'], axis=1)","4f81b80d":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"","847350db":"train['title']=train['Name'].apply(get_title)\ntest['title']=test['Name'].apply(get_title)","03d43663":"title_lev1=list(train['title'].value_counts().reset_index()['index'])\ntitle_lev2=list(test['title'].value_counts().reset_index()['index'])","1bf7bb99":"title_lev=list(set().union(title_lev1, title_lev2))\nprint(title_lev)","f5b12357":"train['title']=pd.Categorical(train['title'], categories=title_lev)\ntest['title']=pd.Categorical(test['title'], categories=title_lev)","edf05a23":"cols=['Pclass','Sex','Embarked','hasCabin','title']\nfcol=['Pclass','Sex','Embarked','hasCabin','title','Age','FamilyMem','Fare']","610ce32e":"for c in cols:\n    train[c]=train[c].astype('category')\n    test[c]=test[c].astype('category')","63d79321":"train_df=train[fcol]\ntest_df=test[fcol]","594aa893":"train_df=pd.get_dummies(train_df, columns=cols, drop_first=True)\ntest_df=pd.get_dummies(test_df, columns=cols, drop_first=True)","68285aa8":"y=train['Survived']","ad1cf7ef":"x_train, x_test, y_train, y_test = train_test_split(train_df, y, test_size=0.3, random_state=42)","332e6bff":"prams_DTree = {\n    'min_samples_split' : range(10,500,20),\n    'max_depth': range(1,20,2)\n}\n\nfrom sklearn.tree import DecisionTreeClassifier\nclf_DTree = DecisionTreeClassifier()\nclf_DTree = GridSearchCV(clf_DTree, prams_DTree)\nclf_DTree.fit(x_train, y_train)\nprint(\"Best: %f using %s\" % (clf_DTree.best_score_, clf_DTree.best_params_))\nclf_Dtree_preds = clf_DTree.predict(x_test)\nprint(\"0. Accuracy for Random Forest on CV data: \",accuracy_score(y_test,clf_Dtree_preds))","4d508a12":"params_rf = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nrfc=RandomForestClassifier(random_state=42)\nclf_rf = GridSearchCV(estimator=rfc, param_grid=params_rf, cv= 5)\nclf_rf.fit(x_train, y_train)\nprint(\"Best: %f using %s\" % (clf_rf.best_score_, clf_rf.best_params_))\nclf_rf_preds = clf_rf.predict(x_test)\nprint(\"1. Accuracy for Random Forest on CV data: \",accuracy_score(y_test,clf_rf_preds))","fe2b8a46":"from sklearn import svm\n\nparams_svm = {\n    'kernel':('linear', 'rbf'),\n    'C':(1,0.25,0.5,0.75),\n    'gamma': (1,2,3,'auto'),\n    'decision_function_shape':('ovo','ovr'),\n    'shrinking':(True,False)\n}\n\nsvc = svm.SVC(gamma=\"scale\")\nclf_svm = GridSearchCV(svc, params_svm, cv=5)\n\n%time clf_svm.fit(x_train, y_train)\nprint(\"Best: %f using %s\" % (clf_svm.best_score_, clf_svm.best_params_))\n\n%time clf_svm_preds = clf_svm.predict(x_test)\nprint(\"2. Accuracy for SVM on CV data: \",accuracy_score(y_test,clf_svm_preds))","180d5fd1":"from sklearn.linear_model import LogisticRegression\nsvm_parameters = {\n    'dual': [True,False],\n    'max_iter': [100,110,120,130,140],\n    'C': [1.0,1.5,2.0,2.5]\n}\n\nlr = LogisticRegression(penalty='l2')\nclf_lr = GridSearchCV(lr, svm_parameters, cv = 5)\n\n%time clf_lr.fit(x_train, y_train)\n# print(\"Best: %f using %s\" % (clf_lr.best_score_, clf_lr.best_params_))\n\n%time clf_lr_preds = clf_lr.predict(x_test)\n# print(\"3. Accuracy for LogisticRegression on CV data: \",accuracy_score(y_test,clf_lr_preds))","ea5e162a":"from sklearn.naive_bayes import GaussianNB\nclf_nb = GaussianNB()\n%time clf_nb.fit(x_train, y_train)\n%time clf_nb_preds = clf_nb.predict(x_test)\nprint(\"4. Accuracy for Naive bayas on CV data: \",accuracy_score(y_test, clf_nb_preds))","651d4480":"DTC = DecisionTreeClassifier(random_state = 11,\n#                              max_features = \"auto\",\n#                              class_weight = \"balanced\",\n#                              max_depth = None\n                            )\n\nfrom sklearn.ensemble import AdaBoostClassifier\nparams_abc = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"n_estimators\": [1, 2]\n             }\n\nABC = AdaBoostClassifier(base_estimator = DTC)\n\nclf_abc = GridSearchCV(ABC, param_grid=params_abc)\nclf_abc.fit(x_train, y_train)\n\n%time clf_abc_preds = clf_abc.predict(x_test)\nprint(\"5. Accuracy for Ada boost on CV data: \",accuracy_score(y_test, clf_abc_preds))","cedb7cfb":"P values for features Sex, Embarked and Pclass are very low. So we can reject our Null Hypothesis which is these features are independent and have no relationship with target variable","48e7e4ba":"Let's replace NaN by 0","2d185f3f":"#### Cabin","45036cd3":"## Data seperation test and train","596b2e9e":"###  Let's create dummy variables","24ea43a1":"We have 11 feature columns and target variable **Survived** which is binary.","81494ebc":"#### Embarked****","6154ef1d":"### Assigning datatypes","d3e40c23":"* Let's combine SibSp and Parch features to create new one **FamilyMem**","42b514a0":"* Let's check which features contain missing values","061bfe13":"## Visualization ","5ec626a4":"from **cabin** let's create a new feature **hasCabin** ","8650a3e6":"### Let's use chi-square test to understand relationship between categorical variables and target variable","a3e50b81":"Pclass, Sex and Embarked are **Categorical** Features while Age, SibSp, Parch and Fare are **continuous** variables.","8516010d":"## EDA","6bafd3a6":"## 5. AdaBoostClassifier","056441ea":"Only 4 features have missing values","72ddb873":"### Let's find correlation between Numeric Variable","90f022c4":"## 4. Naive Bayes \nNo need to grid search due to there are no parameters to tune","9b442428":"And even if we do nothing we would get approximately 61% accuracy by simple marking all passangers as not survived(**Accuracy Paradox**). So our aim should be to get accuracy higher than this.","0c56a679":"We will use Name, Ticket and Cabin variable in Feature Engineering","c0be0302":" I would like to keep all the features as there is no strong evidence of data redundancy.","426e1bb1":"# Start Creating Model","44cbf418":"#### Fare","d068a5f7":"# Titanic Survival Predicton","b5fe8e99":"Passanger not survived has edge over survived passanger.  ","884470f8":"### 2. Support vector machine - SVM","82922b87":"So these features contribute by providing some information.","c116bacb":"## Feature Engineering","9b4b4d85":"There's no strong correlation between any two variables. The strongest correlation is between **SibSp** and **Parch** features (0.414).","23ed1c3e":"#### Age","8580dc23":"The objective is to predict if passanger has survived or not.","94f1c8bc":"## 0. Decision Tree","97376bec":"****Let's replace missing values by median of Age.","010b42a8":"Let's use prefixes in the name to Create a new column **Title** ","35148baa":" Let's replace the NaN by mode","1119dc99":"## 3. LogisticRegression","69524072":"## 1. Random Forest"}}