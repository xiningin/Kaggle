{"cell_type":{"aea848d5":"code","c139a065":"code","5c12c84f":"code","6ede4751":"code","277eb4de":"code","d1f2a053":"code","cdd6f237":"code","f1e27bb2":"code","df37a4d7":"code","69d022c9":"code","9750c009":"code","0779557a":"code","3ee91122":"code","4193d581":"code","5bd42035":"code","3fabcdf3":"code","8e38f2f8":"code","fe3bb7f5":"markdown","212e3430":"markdown","c757a78f":"markdown","2aca3d39":"markdown","252f7904":"markdown","ed8be1b2":"markdown","5dac6fc8":"markdown","a0647a39":"markdown","7b696dad":"markdown","5feb15b0":"markdown","ea2d8880":"markdown","1ddd239a":"markdown","d3c58f54":"markdown","12935e0e":"markdown","ab4a3051":"markdown","0ddc42ad":"markdown","ae18330e":"markdown"},"source":{"aea848d5":"!pip install --upgrade wandb  # Machine learning developer platform for building better models, faster with experiment tracking, dataset versioning, and model management","c139a065":"import os  # operating system library\nimport gc  # Garbage Collector - module provides the ability to disable the collector, tune the collection frequency, and set debugging options\nimport copy  # The assignment operation does not copy the object, it only creates a reference to the object. \n# For mutable collections, or for collections containing mutable items, a copy is often needed so that it can be modified without changing the original. \n# This module provides general (shallow and deep) copy operations.\n\nimport time  # time library\nimport random  # library for working with random values\n\nimport string  # common string operations\n\n# For data manipulation\nimport pandas as pd  # data analysis library\nimport numpy as np  # library linear algebra, Fourier transform and random numbers\n\n# Pytorch Imports\nimport torch  #  a Tensor library like NumPy, with strong GPU support\nimport torch.nn as nn  # a neural networks library deeply integrated with autograd designed for maximum flexibility\nimport torch.optim as optim  # Pytorch also has a package with various optimization algorithms.\n# We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter.\n# This will let us replace our previous manually coded optimization step\n\nfrom torch.optim import lr_scheduler  # provides several methods to adjust the learning rate based on the number of epochs.\nfrom torch.utils.data import Dataset, DataLoader  # DataLoader and other utility functions for convenience\n# DataLoader is responsible for managing batches. You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches. \n# PyTorch has an abstract Dataset class. A Dataset can be anything that has a __len__ function (called by Python\u2019s standard len function) and a __getitem__ function \n# as a way of indexing into it\n\n\n# Utils\nfrom tqdm import tqdm  # tqdm derives from the Arabic word taqaddum  which can mean \"progress,\" and is an abbreviation for \"I love you so much\" in Spanish \n# (te quiero demasiado).  this library show a smart progress meter - just wrap any iterable with tqdm(iterable)\nfrom collections import defaultdict  # Usually, a Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary. \n# The defaultdict in contrast will simply create any items that you try to access (provided of course they do not exist yet). \n\n\n# Sklearn Imports\n# sklearn - \u0430 set of python modules for machine learning and data mining\nfrom sklearn.metrics import mean_squared_error  # Mean squared error regression loss\nfrom sklearn.model_selection import StratifiedKFold, KFold  # Stratified K-Folds cross-validator.\n# K-Folds cross-validator. Provides train\/test indices to split data in train\/test sets. Split dataset into k consecutive folds (without shuffling by default).\n# Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AdamW  \n#  In many cases, the architecture you want to use can be guessed from the name or the path of the \n# pretrained model you are supplying to the from_pretrained() method. AutoClasses are here to do this job for you so that you automatically retrieve the \n# relevant model given the name\/path to the pretrained weights\/config\/vocabulary.\n# Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\n# ANSI escape character sequences have long been used to produce colored terminal text and cursor positioning on Unix and Macs. \n# Colorama makes this work on Windows, too, by wrapping stdout, stripping ANSI sequences it finds (which would appear as gobbledygook in the output), \n# and converting them into the appropriate win32 calls to modify the state of the terminal. On other platforms, Colorama does nothing.\n\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings  # Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program\n\nwarnings.filterwarnings(\"ignore\")  # This is the base class of all warning category classes. It is a subclass of Exception. \n# The warnings filter controls whether warnings are ignored, displayed, or turned into errors (raising an exception) \n# \"ignore\" - never print matching warnings\n# For descriptive error messages\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n# The beautiful thing of PyTorch's immediate execution model is that you can actually debug your programs. Sometimes, however, the asynchronous nature \n# of CUDA execution makes it hard. Here is a little trick to debug your programs. When you run a PyTorch program using CUDA operations, \n# the program usually doesn't wait until the computation finishes but continues to throw instructions at the GPU until it actually needs \n# a result (e.g. to evaluate using .item() or .cpu() or printing).\n# While this behaviour is key to the blazing performance of PyTorch programs, there is a downside: When a cuda operation fails, your program \n# has long gone on to do other stuff. The usual symptom is that you get a very non-descript error at a more or less random place somewhere \n# after the instruction that triggered the error.\n# One option in debugging is to move things to CPU. But often, we use libraries or have complex things where that isn't an option. So what now? If we could only get a good traceback, we should find the problem in no time.\n# This is how to get a good traceback:You can launch the program with the environment variable CUDA_LAUNCH_BLOCKING set to 1.\n\n\nfrom transformers import logging  # lib for logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()","5c12c84f":"import wandb  # Machine learning experiment tracking, dataset versioning, and model evaluation Weights & Biases is the machine learning platform \n# for developers to build better models faster.\n\n\nwandb.init(project=\"my-test-project\", entity=\"doom84\")  # Enter your data here (your username and project name)\n# When you run this code, you will be prompted to enter API key\n\n# Weights & Biases connection error handler\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\n Get your W&B access token from here: https:\/\/wandb.ai\/authorize')","6ede4751":"def id_generator(size=12, chars=string.ascii_lowercase + string.digits):  # this function Takes random choices from 12 ascii etters and digits\n    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))  # returns the resulting 12 character string\n \n# In Python, string ascii_lowercase will give the lowercase letters \u2018abcdefghijklmnopqrstuvwxyz\u2019.\nHASH_NAME = id_generator(size=12)  # will create a test variable and check how our generation function works\nprint(HASH_NAME)  # display the result","277eb4de":"# define the configuration of our model\nCONFIG = {\"seed\": 42, # 2021->42\n          \"epochs\": 3, # 3->5->10->7->3\n          \"model_name\": \"roberta-base\",\n          \"train_batch_size\": 32,\n          \"valid_batch_size\": 64,\n          \"max_length\": 128,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-6,\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 25, #5->10->5->10\n          \"n_accumulate\": 1,\n          \"num_classes\": 1,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"hash_name\": HASH_NAME\n          }\n\n# 10folds * 3epochs * 13 minuts = 390 minuts = 6,5 hours  on GPU kaggle\n# if Using GPU: Tesla K80 google colab it will turn out many times faster = 1,5 \u0447\u0430\u0441\u0430\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\nCONFIG['group'] = f'{HASH_NAME}-Baseline'","d1f2a053":"# Sets the seed of the entire notebook so results are the same every time we run.\n# This is for REPRODUCIBILITY\ndef set_seed(seed=42): \n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","cdd6f237":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")  # create a variable dataframe containing data from the original competition data file  \nprint(df.shape)  # display statistics for in this file\ndf.head()  # display the first 5 rows of the dataframe table","f1e27bb2":"skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])  # set the parameters for splitting our dataframe into data for training and testing\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):  # dataframe splitting\n    df.loc[val_ , \"kfold\"] = int(fold)\n    \ndf[\"kfold\"] = df[\"kfold\"].astype(int)  # add one more column of folder number to the original dataframe\ndf.head()  # display the first 5 rows of the dataframe table","df37a4d7":"class JigsawDataset(Dataset):  # create a JigsawDataset class\n    def __init__(self, df, tokenizer, max_length):  # initialization of the class at the input of the dataframe, tokenizer, max_length\n        # set the class attributes\n        # the __init__ function is run once when instantiating the Dataset object.\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        \n    def __len__(self):  # the __len__ function returns the number of samples in our dataset.\n        return len(self.df)\n    \n    def __getitem__(self, index):  # __getitem__ function loads and returns a sample from the dataset at the given index index. Based on the index, \n        # it identifies the data location on disk, encodes the data, and sets the target = 1. Finally it returns tensors of the ids, mask, target. \n        # Remember that we are using the roberta model here which does not have token_type_ids. \n        \n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        # Since no Target variable is present in the dataset (remember we explicitly specified Target = 1 in the dataset class), we use the Margin Ranking Loss\n        # It creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensors, and a label 1D mini-batch tensor y (containing 1 or -1).\n        # If y = 1, then it assumed the first input should be ranked higher (should have a larger value) than the second input, and vice-versa for y = -1.\n        # For the same reason, we set Target = 1 earlier.\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {  # returns the obtained values\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }","69d022c9":"class JigsawModel(nn.Module):  # create a JigsawModel class\n    def __init__(self, model_name):  # initialization of the class at the input of the dataframe, tokenizer, max_length\n        # set the class attributes\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        \n        return outputs  # returns the obtained values","9750c009":"def criterion(outputs1, outputs2, targets):  # Creates a criterion that measures the loss\n    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)","0779557a":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):  # one epoch training function\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    \n    for step, data in bar:\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        loss = loss \/ CONFIG['n_accumulate']\n        loss.backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            optimizer.step()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss  # returns the result of the training function for one epoch","3ee91122":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):  # one epoch check function\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    gc.collect()\n    \n    return epoch_loss  # returns the result of the check function for one epoch","4193d581":"def run_training(model, optimizer, scheduler, device, num_epochs, fold):  # general training function\n    # To automatically log gradients\n    wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG['device'], epoch=epoch)\n        \n        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n                                         epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        \n        # Log the metrics\n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Valid Loss\": val_epoch_loss})\n        \n        # deep copy the model\n        if val_epoch_loss <= best_epoch_loss:\n            print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n            run.summary[\"Best Loss\"] = best_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"Loss-Fold-{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","5bd42035":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=2, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","3fabcdf3":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","8e38f2f8":"for fold in range(0, CONFIG['n_fold']):\n    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n    run = wandb.init(project='Jigsaw', \n                     config=CONFIG,\n                     job_type='Train',\n                     group=CONFIG['group'],\n                     tags=['roberta-base', f'{HASH_NAME}', 'margin-loss'],\n                     name=f'{HASH_NAME}-fold-{fold}',\n                     anonymous='must')\n    \n    # Create Dataloaders\n    train_loader, valid_loader = prepare_loaders(fold=fold)\n    \n    model = JigsawModel(CONFIG['model_name'])\n    model.to(CONFIG['device'])\n    \n    # Define Optimizer and Scheduler\n    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n    scheduler = fetch_scheduler(optimizer)\n    \n    model, history = run_training(model, optimizer, scheduler,\n                                  device=CONFIG['device'],\n                                  num_epochs=CONFIG['epochs'],\n                                  fold=fold)\n    \n    run.finish()\n    \n    del model, history, train_loader, valid_loader\n    _ = gc.collect()\n    print()\nprint('\u0421ompleted training and saving models!')","fe3bb7f5":"### Read the Data ","212e3430":"### Dataset Class","c757a78f":"### Run Training","2aca3d39":"### Start Training \nWith the data, model and hardware prepared we are now ready to train the model. We use N Fold Training as a Cross-Validation Strategy here.","252f7904":"### Install Required Libraries","ed8be1b2":"### Create Model","5dac6fc8":"### Training Configuration","a0647a39":"### Weights & Biases\nMachine learning experiment tracking, dataset versioning, and model evaluation\nWeights & Biases is the machine learning platform for developers to build better models faster. Use W&B's lightweight, interoperable tools to quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, visualize results and spot regressions, and share findings with colleagues.\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.","7b696dad":"### Loss Function\n\n![](https:\/\/i.imgur.com\/qYwVt8V.jpg)\n","5feb15b0":"### Jigsaw Pytorch Starter\nModel used: Roberta-base\n#### RoBERTa\nRoBERTa stands for Robustly Optimized Bidirectional Encoder Representations from Transformers.\n\nRoBERTa builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n\nRoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.\n\nRoBERTa doesn\u2019t have token_type_ids, you don\u2019t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token.\n\nRoBERTa is an extension of BERT with changes to the pretraining procedure. The modifications include:\n\n- Training the model longer, with bigger batches, over more data\n- Removing the next sentence prediction objective\n- Training on longer sequences\n- Dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset of comparable size to other privately used datasets, to better control for training set size effects.\n\nIt was proposed in 2019 by Yinhan Liu Et. al. in RoBERTa: A Robustly Optimized BERT Pretraining Approach\n\n### Introduction\nIn this article, we will walk through a baseline model for the Jigsaw Rate Severity of Toxic Comments Competition on Kaggle. The goal of the competition is to rank relative ratings of toxicity between comments.","ea2d8880":"### Create Folds","1ddd239a":"### loging in Weights & Biases (W&B) ","d3c58f54":"### Import Required Libraries ","12935e0e":"### Validation Function","ab4a3051":"### Set Seed for Reproducibility","0ddc42ad":"It is always a better idea to have a separate configuration file for your project. In this case, we declare a dictionary named CONFIG, which includes all the required configs.\n\nFine-tuning Transformer models tend to exhibit training instability. Even with the same hyperparameter values (learning rate, batch size, etc.), distinct random seeds can lead to substantially different results. The issue is even more apparent, especially when using the large variants of Transformers on small datasets.\n\n#### Problem\nThe instability of the Transformer fine-tuning process has been known since the introduction of BERT, and from then various methods have been proposed to address it.\n\n#### Solution\nThere are many recently proposed methods to increase few-sample fine-tuning stability and they show a significant performance improvement over simple finetuning methods.\n- Debiasing Omission In BertADAM\n- Re-Initializing Transformer Layers\n- Utilizing Intermediate Layers\n- Layer-wise Learning Rate Decay (LLRD)\n- Mixout Regularization\n- Pre-trained Weight Decay\n- Stochastic Weight Averaging\n\n\nNote 1: These methods are independent and it's not recommended to use all of them at once. Even though mixing two or more techniques might result in improvement but this may not be always true.","ae18330e":"### Training Function "}}