{"cell_type":{"93997e38":"code","15a8fb94":"code","6f97d983":"code","ca65526c":"code","bc26c1c2":"code","f274d20f":"code","0626b157":"code","b4a973ef":"code","b65eec16":"code","651b3294":"code","56f59db0":"code","ead19aba":"code","813b7fd9":"code","12f339aa":"code","d7318b7a":"code","b1aa99c0":"code","796f15ff":"code","c7c05919":"code","ff55c69a":"code","4199828a":"code","44540c34":"code","357f7c0e":"code","73964c5b":"code","c952f296":"code","245436b2":"code","de1f8c79":"code","18faf930":"code","9dcc5304":"code","437941ab":"code","6f584ae2":"code","fcca4425":"code","c4f21122":"code","ca6bf79c":"code","c4bb138b":"code","44cd7059":"code","53fdf549":"code","8838ec09":"code","178bd4b9":"code","0bd71b0b":"code","c4fe853a":"code","ca9a8468":"code","2ea1aeb3":"code","9ea23fcc":"code","f5b157ef":"code","aeb01d9f":"code","1f168564":"code","66578420":"code","04988e4c":"code","bfd69df2":"code","e8aec723":"code","284010d1":"code","0849b910":"markdown","ef6be15d":"markdown","ed461455":"markdown","5bf6428c":"markdown","62a6669d":"markdown","6ece9a4b":"markdown","b53ba401":"markdown","0d38e7df":"markdown","ea148d59":"markdown","7f9fa8eb":"markdown","50d36300":"markdown","159aa970":"markdown","db559e0b":"markdown","79c4ab19":"markdown","b9878356":"markdown","a88c0d17":"markdown","f7420918":"markdown","22112d71":"markdown","9fa33a82":"markdown","b0425b33":"markdown","b48228de":"markdown","74f42d77":"markdown","0dc1d576":"markdown","f0692c67":"markdown","1a35bd91":"markdown","edc9cbcc":"markdown","c44e99ba":"markdown","ca3d66dd":"markdown","a9cc6a4b":"markdown","9db1d0bb":"markdown","f5a856a4":"markdown","31c4cc3d":"markdown","ffd94cbb":"markdown","1f0fd62e":"markdown","b62e811e":"markdown","5d527c9f":"markdown","7a547ed7":"markdown"},"source":{"93997e38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","15a8fb94":"data = pd.read_csv('..\/input\/train.csv')","6f97d983":"data.tail(5)","ca65526c":"data.drop('Cabin',axis=1,inplace=True)\ndata.tail(5)","bc26c1c2":"from sklearn.impute import SimpleImputer\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_mean.fit(data['Age'].values.reshape(-1,1))\ndata['Age'] = imp_mean.transform(data['Age'].values.reshape(-1,1))\ndata.tail(10)","f274d20f":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler() \nscaled_values = scaler.fit_transform(data['Age'].values.reshape(-1, 1)) \ndata['Age'] = scaled_values","0626b157":"scaler = MinMaxScaler() \nscaled_values = scaler.fit_transform(data['Fare'].values.reshape(-1, 1)) \ndata['Fare'] = scaled_values","b4a973ef":"data.isnull().sum()","b65eec16":"data.dropna(how='any', inplace=True)\ndata.reset_index(drop=True, inplace=True)","651b3294":"data.info()","56f59db0":"data.drop('PassengerId',axis=1,inplace=True)\ndata.head()","ead19aba":"#data['Level'] = data.apply(lambda x : str(x)[0])\n#data.head(5)\n#data['Level'] = data['Cabin'].apply(lambda x : x[0])\n#data.head(3)","813b7fd9":"'''\nfrom sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\n#df[['Ticket', 'Fare']]\ndict_values = data[['Level']].T.to_dict().values()\none_hot_arr = vec.fit_transform(dict_values).toarray()\none_hot_df = pd.DataFrame(one_hot_arr, columns=vec.get_feature_names())\none_hot_df\n\nlevel_features = one_hot_df.columns\n'''","12f339aa":"#data[one_hot_df.columns] = one_hot_df\n#data","d7318b7a":"from sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\n#df[['Ticket', 'Fare']]\ndict_values = data[['Sex']].T.to_dict().values()\none_hot_arr = vec.fit_transform(dict_values).toarray()\none_hot_df = pd.DataFrame(one_hot_arr, columns=vec.get_feature_names())\none_hot_df.head()","b1aa99c0":"#one_hot_df.set_index(data.index)\ndata[one_hot_df.columns] = one_hot_df\ndata.head()","796f15ff":"from sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\n#df[['Ticket', 'Fare']]\ndict_values = data[['Embarked']].T.to_dict().values()\none_hot_arr = vec.fit_transform(dict_values).toarray()\none_hot_df = pd.DataFrame(one_hot_arr, columns=vec.get_feature_names())\none_hot_df.head()","c7c05919":"#one_hot_df.set_index(data.index)\ndata[one_hot_df.columns] = one_hot_df\ndata.head()","ff55c69a":"data.describe()","4199828a":"import matplotlib.pyplot as plt\nfig = data[data.Survived==0].plot(kind='scatter',x='Age',y='Fare',color='orange', label='Dead')\ndata[data.Survived==1].plot(kind='scatter',x='Age',y='Fare',color='blue', label='Survived',ax=fig)\nfig.set_xlabel(\"Age\")\nfig.set_ylabel(\"Fare\")\nfig.set_title(\"Age vs Fare\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","44540c34":"data.hist(edgecolor='black', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()","357f7c0e":"import seaborn as sns\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='Survived',y='Age',data=data)\nplt.subplot(2,2,2)\nsns.violinplot(x='Survived',y='Fare',data=data)\nplt.subplot(2,2,3)\nsns.violinplot(x='Survived',y='SibSp',data=data)\nplt.subplot(2,2,4)\nsns.violinplot(x='Survived',y='Pclass',data=data)","73964c5b":"plt.figure(figsize=(15,15)) \nsns.heatmap(data.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","c952f296":"from sklearn.model_selection import train_test_split #to split the dataset for training and testing\ntrain, test = train_test_split(data, test_size = 0.3)# in this our main data is split into train and test","245436b2":"data.columns","de1f8c79":"features = ['Fare', 'Sex=female', 'Sex=male', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Pclass', 'Age', 'SibSp']\n#features.extend(level_features)\npd.DataFrame(data[features], index=data.index).head(2)","18faf930":"sum(n < 0 for n in X.values.flatten())","9dcc5304":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nX = data[features]  #independent columns\ny = data['Survived']    #target column i.e price range\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","437941ab":"features = featureScores[featureScores.Score > .2]['Specs'].tolist()\nfeatures","6f584ae2":"train_X = train[features]\ntrain_y = train['Survived']\ntest_X = test[features]\ntest_y = test['Survived']","fcca4425":"from sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\n\nmodel = svm.SVC() #select the algorithm\nmodel.fit(train_X,train_y) # we train the algorithm with the training data and the training output\nprediction=model.predict(test_X) #now we pass the testing data to the trained algorithm\nprint('The accuracy of the SVM is:',metrics.accuracy_score(prediction,test_y))#now we check the accuracy of the algorithm. \n#we pass the predicted output by the model and the actual output","c4f21122":"from sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\n\nmodel = LogisticRegression()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,test_y))","ca6bf79c":"from sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\n\nmodel=DecisionTreeClassifier()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,test_y))\n","c4bb138b":"from sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\n\nmodel=KNeighborsClassifier(n_neighbors=10) #this examines 3 neighbours for putting the new data into a class\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction,test_y))","44cd7059":"a_index=list(range(1,11))\na=pd.Series()\nx=[1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_y)))\nplt.plot(a_index, a)\nplt.xticks(x)","53fdf549":"from tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv1D, Dropout\n","8838ec09":"'''\ntrain_X = train_X.values.reshape(train_X.shape[0], len(features),1)\ntest_X = test_X.values.reshape(test_X.shape[0], len(features), 1)\n\nmodel.add(Conv1D(12, kernel_size=1, activation=\"relu\", input_shape=(len(features), 1)))\nmodel.add(Conv1D(20, activation='relu', kernel_size=1, strides=2))\nmodel.add(Conv1D(20, activation='relu', kernel_size=1))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n'''","178bd4b9":"train_X = train_X.values\ntest_X = test_X.values\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=len(features), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","0bd71b0b":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","c4fe853a":"callbacks_list = [keras.callbacks.EarlyStopping(monitor='acc', patience=100)]\nhistory = model.fit(train_X, train_y, batch_size=32, epochs = 500, validation_split = 0.2, callbacks = callbacks_list, verbose=1)","ca9a8468":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","2ea1aeb3":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","9ea23fcc":"print(model.metrics_names)\nmodel.evaluate(test_X, test_y, verbose=1)","f5b157ef":"pred_y = pd.DataFrame(model.predict_classes(test_X, verbose=1))\npred_y.head(5)","aeb01d9f":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(test_y, pred_y)","1f168564":"labels = [0,1]\ncm = confusion_matrix(test_y, pred_y, labels)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","66578420":"test_y = test_y.values\npred_y = pred_y.iloc[:,0].values\n(test_y != pred_y).sum()\/len(test_y)","04988e4c":"data = pd.read_csv('..\/input\/test.csv',sep=',',skipinitialspace=True,quotechar='\"',engine='python')","bfd69df2":"\nfrom sklearn.impute import SimpleImputer\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_mean.fit(data['Age'].values.reshape(-1,1))\ndata['Age'] = imp_mean.transform(data['Age'].values.reshape(-1,1))\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler() \nscaled_values = scaler.fit_transform(data['Age'].values.reshape(-1, 1)) \ndata['Age'] = scaled_values\n\nscaler = MinMaxScaler() \nscaled_values = scaler.fit_transform(data['Fare'].values.reshape(-1, 1)) \ndata['Fare'] = scaled_values\n\nfrom sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\ndict_values = data[['Sex']].T.to_dict().values()\none_hot_df = vec.fit_transform(dict_values).toarray()\none_hot_df = pd.DataFrame(one_hot_df, columns=vec.get_feature_names())\none_hot_df.head(5)\ndata[one_hot_df.columns] = one_hot_df\n\nvec = DictVectorizer()\ndict_values = data[['Embarked']].T.to_dict().values()\none_hot_df = vec.fit_transform(dict_values).toarray()\none_hot_df = pd.DataFrame(one_hot_df, columns=vec.get_feature_names())\none_hot_df.head(5)\ndata[one_hot_df.columns] = one_hot_df\n\ntest_X = data[features]\n#test_X = test_X.values.reshape(test_X.shape[0], len(features), 1)\ntest_X = test_X.values\n\npred_y = model.predict_classes(test_X, verbose=1)\noutput_df = data[['PassengerId']]\noutput_df['Survived'] = pd.DataFrame(pred_y)\noutput_df.head(10)","e8aec723":"output_df.PassengerId.size","284010d1":"output_df.to_csv('output.csv', index=False)","0849b910":"Normalize Age","ef6be15d":"Read test data","ed461455":"Grab the best features","5bf6428c":"Histogram columns","62a6669d":"Remove any rows that have NaN","6ece9a4b":"Try different n_neighbor values for K neighbors ","b53ba401":"Write submission csv","0d38e7df":"Create a one hot for gender","ea148d59":"Create sequential model","7f9fa8eb":"Check if there are any other NaN values","50d36300":"Drop cabin for now. Not sure how to apply it.","159aa970":"Try logistic regression","db559e0b":"Violin plot some values","79c4ab19":"Normalize Fare","b9878356":"Predict with model","a88c0d17":"Plot confusion matrix","f7420918":"Compile with binary crossentropy","22112d71":"Create a one hot for embarked","9fa33a82":"Fit with earliy stopping callback","b0425b33":"Split training and verification data","b48228de":"Evaluate model","74f42d77":"Remove passenger ID, redundant index","0dc1d576":"Generate predictions for test data","f0692c67":"Plot loss","1a35bd91":"Try SVC","edc9cbcc":"Plot accuracy","c44e99ba":"Add it to the data frame","ca3d66dd":"Add it to the data frame","a9cc6a4b":"Check feature correlation with a heatmap","9db1d0bb":"Get error","f5a856a4":"Scatter plot age vs fare","31c4cc3d":"Fill in missing values for age","ffd94cbb":"Read training data","1f0fd62e":"Try K neighbors","b62e811e":"Select the features we can work with","5d527c9f":"Try decision tree classifier","7a547ed7":"Grade the features"}}