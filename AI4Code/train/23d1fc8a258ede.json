{"cell_type":{"6b3e5db1":"code","92dfc514":"code","1c304053":"code","aa78d869":"code","c89ea317":"code","81bb1f41":"code","26f558b2":"code","613ea99f":"code","9b3c15fb":"code","4f81edee":"code","655b05f2":"code","ae164374":"code","309e67f2":"code","97fdf181":"code","c3c4fb28":"code","1fca7cfb":"code","99361278":"code","fc5658a2":"code","86165df7":"code","080be226":"code","8f06ceba":"code","68a8188b":"code","be1cb230":"code","2adc1550":"code","aa57ae81":"code","6aa64466":"code","fe653d64":"code","752ba996":"code","1490bdd3":"code","c7638685":"code","194622f0":"code","0c86c9b8":"code","f79b0b2d":"code","a26d4eef":"code","a0fecb41":"code","207feb96":"code","c0219b5f":"code","5e8d49be":"code","6c5ee8f0":"code","ac5d4056":"code","b92c9725":"code","47a6dfec":"code","35ac42eb":"code","669f20a2":"code","a7e22446":"code","b98e789e":"code","2e5916a8":"code","04e76b03":"markdown","e947eb0b":"markdown","8266c196":"markdown","e7b7786c":"markdown","30f2f0ca":"markdown","94b70dea":"markdown","4c36d2bf":"markdown","e1118c25":"markdown"},"source":{"6b3e5db1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","92dfc514":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","1c304053":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","aa78d869":"print(f'The length of the training dataset is: {len(train)}')\nprint(f'The length of the testing dataset is: {len(test)}')","c89ea317":"print(f'Number of columns in train dataset: {len(train.columns)}')  #checking for the number of columns.\nprint(f'Number of columns in test dataset: {len(test.columns)}') #Survived column is not in testing dataset.","81bb1f41":"pd.set_option('precision', 4) #setting decimal place values\npd.set_option('max_columns', 12) #maximum column to appear","26f558b2":"train.columns","613ea99f":"train.head()","9b3c15fb":"sns.countplot(x= train['Survived']) #Survival rate","4f81edee":"#comparing male to female ratio of those that survived \nsns.countplot(x='Sex', hue= 'Survived', data=train, palette = 'bright') ","655b05f2":"train.hist('Age')","ae164374":"train['Fare'].plot.hist(bins=20, figsize=(10,5))","309e67f2":"sns.countplot(y='SibSp', hue = \"Survived\", data=train)","97fdf181":"sns.countplot(x='Parch', hue = \"Survived\", data=train)\nplt.legend(loc = 'upper right')","c3c4fb28":"sns.boxplot(x='Pclass', y='Age', data=train)","1fca7cfb":"target = train['Survived']","99361278":"train = train.drop('Survived', axis=1) #dropping survived column ","fc5658a2":"data = train.append(test) #concatenating both train and test data for pre processing\ndata.head()","86165df7":"data.info()","080be226":"target = pd.DataFrame(target)\ntarget.head()","8f06ceba":"data.isnull().sum()","68a8188b":"#dropping columns with high number of missing values\ndata.drop(['Age', 'Cabin'], axis = 1, inplace = True)","be1cb230":"data.head()","2adc1550":"data.isnull().sum()","aa57ae81":"#filling missing fare value with mean value\ndata['Fare'] = data['Fare'].fillna(data['Fare'].mean())","6aa64466":"#using forward-fill to fill the missing value in the Embarked column\ndata['Embarked'] = data['Embarked'].fillna(method = 'ffill')","fe653d64":"data.isnull().sum() #all the missing values have been catered for through data munging","752ba996":"data.info() #all the missings value have been catered for","1490bdd3":"#preprocessing and dropping of some columns with low correlation\ndata.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace = True) ","c7638685":"data['Embarked'].unique() ","194622f0":"data['Sex'].unique()","0c86c9b8":"#transforming columns with one_hot_encoding\nembarked_dummies = pd.get_dummies(data.Embarked)\ndata = pd.concat([data,embarked_dummies], axis=1)\ndata = data.drop(\"Embarked\", axis=1)\n\nsex_dummies= pd.get_dummies(data.Sex)\ndata = pd.concat([data,sex_dummies], axis=1)\ndata = data.drop(\"Sex\", axis=1)","f79b0b2d":"data.head()","a26d4eef":"(data.shape) #train and train data shape","a0fecb41":"new_train_data = data.iloc[:891,]\nnew_train_data","207feb96":"new_test_data = data.iloc[891:,]\nnew_test_data","c0219b5f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","5e8d49be":"estimators = {\n    'Logisitic_Regression': LogisticRegression(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SVC': SVC(gamma = 'auto'),\n    'GaussianNB': GaussianNB(),\n    'discriminant_analysis': LinearDiscriminantAnalysis()\n}","6c5ee8f0":"#using Kfold and cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report","ac5d4056":"for name, code in estimators.items():\n    kfold = StratifiedKFold(n_splits=10, random_state=11 , shuffle=True)\n    cv_result = cross_val_score(code, X = new_train_data, y = target,\n                            cv = kfold, scoring= 'accuracy')\n    print(f'{name:>20}: ' + \n          f'mean accuracy={cv_result.mean():.2%}; ' +\n          f'standard deviation={cv_result.std():.2%}')","b92c9725":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(new_train_data, target, random_state=11,test_size = 0.3)\n","47a6dfec":"display(X_train.shape)\ndisplay(X_test.shape)\ndisplay(y_train.shape)\ndisplay(y_test.shape)","35ac42eb":"def get_score(X_train, X_test, y_train, y_test):\n    model = DecisionTreeClassifier(random_state=11)\n    model.fit(X_train, y_train)\n    preds_val = model.predict(X_test)\n    acc_score = accuracy_score(y_test, preds_val)\n    return(acc_score)","669f20a2":"my_score = get_score(X_train, X_test, y_train, y_test)\nprint(f'The accuracy score of the model is: {(my_score*100):.2f}%.')","a7e22446":"# XGBoost for result improvement\nfrom xgboost import XGBClassifier\n\nmy_model1 = XGBClassifier(random_state = 11)\nmy_model1.fit(X_train, y_train)\n\npreds = my_model1.predict(X_test)\nacc = (accuracy_score(y_test, preds))* 100\nprint(f'Accuracy Score: {acc:.2f}%')","b98e789e":"cm = confusion_matrix(y_test, preds)\nprint(f'Confusion Matrix: \\n{cm}')\n\nsns.heatmap(cm, annot=True, cmap='nipy_spectral_r')","2e5916a8":"crpt = classification_report(y_test, preds)\nprint(f'Classification Report: \\n\\n{crpt}')","04e76b03":"# Data Wrangling","e947eb0b":"- Decision Tree gives the highest accuracy score (80.81%)","8266c196":"- EDA and Feature Engineering.","e7b7786c":"# Titanic Data Analysis and Prediction","30f2f0ca":"# Modelling \n- hyperparameter tuning and using of various classification model tools to pick the best model.","94b70dea":"- KFold Cross validation","4c36d2bf":"# Analysing data\n- Checking target distribution where 1 is for Survived and 0 is for Not Survived","e1118c25":"- Gradient Boosting"}}