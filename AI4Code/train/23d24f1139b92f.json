{"cell_type":{"204b1132":"code","b3050fea":"code","3b53c647":"code","fcb25018":"code","42138ae4":"code","de369a0e":"code","2d67e914":"code","df220ab9":"code","4b0d55fc":"code","4b6959ab":"code","f16cf1c1":"code","548591d5":"code","dc51e73a":"code","89a18656":"code","109e93cc":"code","85a2bc65":"code","17dd8ac8":"code","abe2ba22":"code","f6d62d2c":"code","81b3e18b":"code","e4207314":"code","b5a26ecb":"code","5d3bf519":"code","3ab6a95d":"code","4df96949":"code","01f03c03":"markdown","b4469b45":"markdown","fcd1b81e":"markdown","254b52c6":"markdown","52d4defb":"markdown","c3991778":"markdown","c565fab1":"markdown","22335603":"markdown","ca8d11f0":"markdown","304fbdc5":"markdown","915155bc":"markdown","2511ceb5":"markdown","be829528":"markdown","bfab95be":"markdown","b6192b17":"markdown","72d9fba3":"markdown","e21c178b":"markdown","c613fe59":"markdown","e4b7b36c":"markdown","9a3ef347":"markdown","5474acab":"markdown","fa915aaa":"markdown","f7d157ca":"markdown","c1bccc2e":"markdown","dbb1be9b":"markdown","f1cc4c63":"markdown","53816c92":"markdown","a9430942":"markdown"},"source":{"204b1132":"import albumentations as A\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport torch\nimport importlib\nimport cv2 \nimport pandas as pd\nimport numpy as np\n\nimport ast\nimport shutil\nimport sys\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nfrom PIL import Image\nfrom IPython.display import display","b3050fea":"# Modified from https:\/\/www.kaggle.com\/remekkinas\/yolox-inference-on-kaggle-for-cots-lb-0-507\n# Additions: \n#     allows customized box color (BGR)\n\ndef draw_yolox_predictions(img, bboxes, scores, bbclasses, classes_dict, boxcolor = (0,0,255)):\n    outimg = img.copy()\n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        cls_id = int(bbclasses[i])\n        score = scores[i]\n        x0 = int(box[0])\n        y0 = int(box[1])\n        x1 = x0 + int(box[2])\n        y1 = y0 + int(box[3])\n\n        cv2.rectangle(outimg, (x0, y0), (x1, y1), boxcolor, 2)\n        cv2.putText(outimg, '{}:{:.1f}%'.format(classes_dict[cls_id], score * 100), (x0, y0 - 3), cv2.FONT_HERSHEY_PLAIN, 0.8, boxcolor, thickness = 1)\n    return outimg","3b53c647":"%cd \/kaggle\/working\n\nfrom sklearn.model_selection import GroupKFold\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_path(row):\n    row['image_path'] = f'{ROOT_DIR}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row\n\nROOT_DIR  = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\n\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\n\n# Don't filter for annotated frames. Include frames with no bboxes as well!\ndf[\"num_bbox\"] = df['annotations'].apply(lambda x: str.count(x, 'x'))\ndf_train = df\n\n# Annotations \ndf_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\n\ndf_train = df_train.progress_apply(get_path, axis=1)\n\nkf = GroupKFold(n_splits = 5) \ndf_train = df_train.reset_index(drop=True)\ndf_train['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y = df_train.video_id.tolist(), groups=df_train.sequence)):\n    df_train.loc[val_idx, 'fold'] = fold\n\ndf_test = df_train[df_train.fold == 4]","fcb25018":"image_paths = df_test.image_path.tolist()\ngt = df_test.bboxes.tolist()","42138ae4":"i = 1380\n\nimage_path = image_paths[i]\nImg = Image.open(image_path)\ndisplay(Img)","de369a0e":"img_np = np.array(Img)[:360,:640]\nout_image = draw_yolox_predictions(img_np, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\ndisplay(Image.fromarray(out_image))","2d67e914":"def show_augmentation(img, augmentation):\n    \"\"\"\n        img: a numpy array of the image\n        augmentation: a function from the Albumentations library\n        see https:\/\/albumentations.ai\/docs\/getting_started\/transforms_and_targets\/\n        \n        returns: a numpy array of the augmented image\n    \"\"\"\n    transform = A.Compose([augmentation])\n    img_aug = transform(image=img)['image']\n    return(img_aug)","df220ab9":"AUGMENTATION = A.Blur(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","4b0d55fc":"AUGMENTATION = A.GaussianBlur(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","4b6959ab":"AUGMENTATION = A.MedianBlur(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","f16cf1c1":"AUGMENTATION = A.Downscale(scale_min=0.5, scale_max=0.5, p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","548591d5":"AUGMENTATION = A.ImageCompression(quality_lower=20, quality_upper=40, p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","dc51e73a":"AUGMENTATION = A.Sharpen(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","89a18656":"AUGMENTATION = A.CLAHE(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","109e93cc":"AUGMENTATION = A.IAAAdditiveGaussianNoise(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","85a2bc65":"AUGMENTATION = A.Emboss(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","17dd8ac8":"AUGMENTATION = A.HueSaturationValue(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","abe2ba22":"AUGMENTATION = A.RGBShift(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","f6d62d2c":"AUGMENTATION = A.RandomGamma(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","81b3e18b":"AUGMENTATION = A.RandomContrast(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","e4207314":"AUGMENTATION = A.RandomBrightness(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","b5a26ecb":"AUGMENTATION = A.ChannelShuffle(p = 1.0)\n\nfor q in range(1):\n    img_aug = show_augmentation(img_np, AUGMENTATION)\n    out_image = draw_yolox_predictions(img_aug, gt[i], [1.0] * len(gt[i]), [0] * len(gt[i]), ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","5d3bf519":"def show_compound_augmentation(img, bboxes, labels, augmentation_list):\n    \"\"\"\n        img: a numpy array of the image\n        bboxes: COCO-format bounding boxes\n        labels: a list of labels\n        augmentation_list: a list of functions from the Albumentations library\n        see https:\/\/albumentations.ai\/docs\/getting_started\/transforms_and_targets\/\n        \n        returns: a numpy array of the augmented image\n    \"\"\"\n    transform = A.Compose(augmentation_list,\n        bbox_params=A.BboxParams(\n        format='coco',\n        label_fields=['class_labels']\n    ))\n    transformed = transform(image=img, bboxes=bboxes, class_labels = labels)\n    img_aug = transformed['image']\n    boxes = np.array([list(b) for b in transformed['bboxes']])\n    labels = np.array(transformed['class_labels'])\n    return img_aug, boxes, labels","3ab6a95d":"# Example taken from https:\/\/analyticsindiamag.com\/hands-on-guide-to-albumentation\/\n\nAUGMENTATION_LIST = [\n        A.RandomRotate90(),\n        A.Flip(),\n        A.Transpose(),\n        A.OneOf([\n            A.MotionBlur(p=.2),\n            A.MedianBlur(blur_limit=3, p=0.3),\n            A.Blur(blur_limit=3, p=0.1),\n        ], p=0.2),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n        A.OneOf([\n            A.CLAHE(clip_limit=2),\n            A.RandomBrightnessContrast(),            \n        ], p=0.3),\n        A.HueSaturationValue(p=0.3),\n    ]\n\nfor q in range(5):\n    img_aug, bboxes, labels = show_compound_augmentation(img_np, gt[i], [0] * len(gt[i]), AUGMENTATION_LIST)\n    out_image = draw_yolox_predictions(img_aug, bboxes, [1.0] * len(bboxes), labels, ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","4df96949":"AUGMENTATION_LIST = [A.Affine(scale = (0.8, 1.2), translate_percent = 0.1, rotate = (-45,45), shear = (-5, 5), cval = (114,114,114), p = 1.0)]\n\nfor q in range(5):\n    img_aug, bboxes, labels = show_compound_augmentation(img_np, gt[i], [0] * len(gt[i]), AUGMENTATION_LIST)\n    out_image = draw_yolox_predictions(img_aug, bboxes, [1.0] * len(bboxes), labels, ['COTS'], (0,255,0))\n    display(Image.fromarray(out_image))","01f03c03":"### MedianBlur","b4469b45":"### RGBShift","fcd1b81e":"## Clarity Augmentations","254b52c6":"## Utility Functions","52d4defb":"### RandomBrightness","c3991778":"### Downscale","c565fab1":"# COTS Augmentation Gallery using the Albumentations Library\n\n* This notebook is a set of quick examples of augmentation techniques using the Albumentations Library\n\n* It also briefly shows (at the end) how to move your bounding boxes with albumentations\n\nFor the full set of augmentations and their documentation, visit: https:\/\/albumentations.ai\/docs\/getting_started\/transforms_and_targets\/","22335603":"This is my favourite image. Because all the COTS are in the top left hand corner","ca8d11f0":"### ImageCompression","304fbdc5":"### Blur","915155bc":"# The Albumentations","2511ceb5":"### IAAAdditiveGaussianNoise","be829528":"### Sharpen","bfab95be":"### RandomContrast","b6192b17":"### RandomGamma","72d9fba3":"## Pick Your Example Image Here","e21c178b":"### HSV Augmentation","c613fe59":"* One can include a conga list of their favourite augmentations\n* This example also shows how to move bounding boxes when the augmentation moves the image","e4b7b36c":"### ChannelShuffle","9a3ef347":"### CLAHE","5474acab":"* Includes support for spatial augmentations (i.e. those that alter object location, therefore the need to move the bboxes)","fa915aaa":"# Compound Augmentations","f7d157ca":"# A cropped image for simplicity, and with boxes showing you where the COTS are","c1bccc2e":"## Color Augmentations","dbb1be9b":"### Emboss","f1cc4c63":"### GaussianBlur","53816c92":"# Affine Transformation with BBoxes","a9430942":"## Blurring Augmentations"}}