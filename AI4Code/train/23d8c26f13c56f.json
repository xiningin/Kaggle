{"cell_type":{"53d72b01":"code","69e58093":"code","0cbe3dc4":"code","861a1470":"code","a465f0bc":"code","faf49c69":"code","e19a23ab":"code","1edbaa1f":"code","23c135ba":"code","c08ff945":"code","a656b48b":"code","c23e0677":"code","a35c33ba":"code","4499af0e":"code","0a29dcc1":"code","48c54d21":"code","f3b6cc41":"code","764caa36":"code","06bab66e":"code","e7151bc1":"code","6e6a54f1":"code","e3f7b0fa":"code","d0c90520":"code","25c1c11c":"code","106c4ade":"code","d5215c56":"code","93200bfd":"code","4fbb5b87":"code","76973007":"code","c5013452":"code","aa93f4cc":"code","054e7a7d":"code","e9f7e543":"code","4598c672":"code","f571b929":"code","21d831ee":"code","3b820859":"code","381983dc":"code","61501f0b":"markdown","40c20381":"markdown","1df07c73":"markdown","1afce1e8":"markdown","ab2f189d":"markdown","994fc070":"markdown","2f58a908":"markdown","0aedf6eb":"markdown","088aaea9":"markdown","50963fd6":"markdown","6216c516":"markdown","229aa86d":"markdown"},"source":{"53d72b01":"# import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to see all the comands result in a single kernal \nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# to increase no. of rows and column visibility in outputs\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","69e58093":"# Model_01\n# just found 1 date leak and get result as 92.47 with rank of 7 on public leader board\n# i have seen that around 89% of date IDs are repeated to merge test and train data sort them with ids and \n# given next outcome value to current one and fill missing value with D\n# Model_02\n# make an model and \n# if the same id is not present in test data set then fill its value by predicted value ","0cbe3dc4":"# Import data\ntrain = pd.read_csv(r'..\/input\/customer\/Train.csv')\ntest = pd.read_csv(r'..\/input\/customer\/Test.csv')\nsample_submmission = pd.read_csv(r'..\/input\/customer\/sample_submission.csv')","861a1470":"# Having a look at data\ntrain.head()\ntest.head()\nsample_submmission.head()\ntrain.shape ,test.shape, sample_submmission.shape","a465f0bc":"# to check type of variables and missing columns in dataset\ntrain.info()\ntest.info()","faf49c69":"# checking missing values \n(100*train.isna().sum().sort_values(ascending=False)\/len(train)).round(2)\n(100*test.isna().sum().sort_values(ascending=False)\/len(test)).round(2)","e19a23ab":"# Checking Distribution of Target variable\nsns.countplot(train['Segmentation'])","1edbaa1f":"train.columns","23c135ba":"train['ID'].nunique()\ntest['ID'].nunique()\ndf['ID'].nunique()","c08ff945":"#Merging train and test data\ntrain['is_train']=1\ntest['is_train']=0\ndf=pd.concat([train,test],axis=0)\ndf.shape","a656b48b":"train.sort_values(by=['ID'],inplace=True)\ntest.sort_values(by=['ID'],inplace=True)\ndf.sort_values(by=['ID','is_train'],inplace=True)","c23e0677":" df['Segmentation_output'] = df['Segmentation'].shift(-1)","a35c33ba":"Encoding = df.groupby('ID').size()\ndf['ID_Freq_encoded']= df.ID.map(Encoding)","4499af0e":"out=df[df['is_train']==0]","0a29dcc1":"out['Segmentation_output']=out['Segmentation_output'].fillna('D')","48c54d21":"col=['ID','ID_Freq_encoded','Segmentation_output']\nout_1=out[col]","f3b6cc41":"df['Ever_Married']=df['Ever_Married'].replace({'Yes':1,'No':0})\ndf['Gender']=df['Gender'].replace({'Male':1,'Female':0})\ndf['Graduated']=df['Graduated'].replace({'Yes':1,'No':0})\ndf['Spending_Score']=df['Spending_Score'].replace({'Low':0,'Average':1,'High':2})\ndf['Var_1']=df['Var_1'].str[-1:]","764caa36":"train.head()\ntrain.nunique()\ntrain['Segmentation'].value_counts()","06bab66e":"train.describe().T\ntest.describe().T","e7151bc1":"# Missing values\ntrain.isna().sum()\/len(train)\ntest.isna().sum()\/len(test)","6e6a54f1":"train['Ever_Married']=train['Ever_Married'].replace({'Yes':1,'No':0})\ntrain['Gender']=train['Gender'].replace({'Male':1,'Female':0})\ntrain['Graduated']=train['Graduated'].replace({'Yes':1,'No':0})\ntrain['Spending_Score']=train['Spending_Score'].replace({'Low':0,'Average':1,'High':2})","e3f7b0fa":"Encoding = df.groupby('Profession').size()\ndf['Profession_Freq_encoded']= df.Profession.map(Encoding)\nEncoding = df.groupby('Var_1').size()\ndf['Var_1_Freq_encoded']= df.Var_1.map(Encoding)","d0c90520":"df_var=pd.get_dummies(df['Var_1'])\ndf_Profession=pd.get_dummies(df['Profession'])\ndf_Segmentation=pd.get_dummies(df['Segmentation'])","25c1c11c":"df_var.shape, df_Profession.shape , df.shape","106c4ade":"df_1=pd.concat([df,df_var,df_Profession,df_Segmentation],axis=1)","d5215c56":"df_1.head()\ndf_1.shape","93200bfd":"df_1=df_1.drop(['Profession','Var_1'],axis=1)","4fbb5b87":"train_1=df_1[df_1['is_train']==1]\ntest_1=df_1[df_1['is_train']==0]\ntest_id=test_1['ID']","76973007":"train_1=train_1.drop('ID',axis=1)","c5013452":"train_1=train_1.drop(['is_train','Segmentation_output','ID_Freq_encoded','A','B','C','D'],axis=1)\ntest_1=test_1.drop(['Segmentation','is_train','Segmentation_output','ID_Freq_encoded','A','B','C','D'],axis=1)","aa93f4cc":"Y=train_1['Segmentation']\ntrain_1=train_1.drop('Segmentation',axis=1)","054e7a7d":"from sklearn.model_selection import train_test_split\nX_t, X_tt, y_t, y_tt = train_test_split(train_1, Y, test_size=.25, random_state=2)","e9f7e543":"from lightgbm import LGBMClassifier\nlgbcl = LGBMClassifier(n_estimators=60)\nlgbcl= lgbcl.fit(X_t, y_t,eval_metric='multi_error',eval_set=(X_tt , y_tt),verbose=10)\ny_predict = lgbcl.predict(X_tt)\nprint(lgbcl.score(X_t , y_t))\nprint(lgbcl.score(X_tt , y_tt))","4598c672":"feat_importances = pd.Series(lgbcl.feature_importances_, index=X_t.columns)\n#feat_importances.nlargest(30).plot(kind='barh')\nfeat_importances.nsmallest(100).plot(kind='barh')\nplt.show()","f571b929":"test_ID=test_1['ID']\ntest_1=test_1.drop('ID',axis=1)\nlgbcl= lgbcl.fit(train_1, Y)\ny_predict = lgbcl.predict(test_1)","21d831ee":"out_2 = pd.DataFrame({ 'ID':test_ID,'Segmentation_1':y_predict}) \n#submission.to_csv(\"1st_try.csv\", index = False)\nout=pd.merge(out_1,out_2,how='inner', on='ID')","3b820859":"out['Segmentation']=out['Segmentation_output'][out['ID_Freq_encoded']==2]\nout['Segmentation']=out['Segmentation'].fillna(out['Segmentation_1'])","381983dc":"col=['ID','Segmentation']\noutput=out[col]\noutput.to_csv(\"sol.csv\", index = False)","61501f0b":" Apart from target column there are 6 other columns which are object type and there are also missing values in data","40c20381":"# Model","1df07c73":"### Basic Visualization","1afce1e8":"It's a balanced distribution ","ab2f189d":"6 columns has missing values and Work_Experience has around 10% values mising","994fc070":"# Its an Analytics vidhya Competition \n### To get an idea about competition and use data set for your own Notebooks  \n* Dataset Link- https:\/\/www.kaggle.com\/vetrirah\/customer\n* Competition Link - https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-customer-segmentation\/#ProblemStatement","2f58a908":"I personally Achieced Rank 8 on Public leaderboard and 25 on Private leaderboard,But some of my other solutions can give rank 3 and 6 on private leader board which i missed to select,One more important think about this competition there is small amount of data so making a right validation set is also important( rank 2 on public leader board recieved rank 52 on private leader board in this competition)","0aedf6eb":"#In testset and train set all ids are unique as their no. of rows equal to no. of unique values of Id \n#but some of Train set Id's get repeated in Test as no. of unique id in df not equal to sum of test train unique ids\n#exact no.approx 89 %(2332 observations)\n#assuming outcome remain same if ID. is same\n#These below article may help in understanding this \n#Approach- https:\/\/www.kaggle.com\/questions-and-answers\/171939 \n#Approach in Detail in this competition- https:\/\/www.kaggle.com\/questions-and-answers\/171953","088aaea9":"# If you find thsi notebook helpful don't forget to upvote. It will Enourage me to make some more Notebooks like this.","50963fd6":"# Model 1","6216c516":"# Visulization","229aa86d":"# 2nd Method"}}