{"cell_type":{"4f8bce9e":"code","43ac94c4":"code","59ca07d4":"code","8a02ca20":"code","9412cb1e":"code","53b879bc":"code","da943144":"code","428e90a4":"code","9c79dcec":"code","9707daa2":"code","789e74a9":"code","3a60b764":"code","fafad590":"code","cc76657e":"code","c65f17e1":"code","352212bc":"code","c18773f2":"code","4b352e9a":"markdown","4a17e7b6":"markdown","65e4edff":"markdown","97839784":"markdown","02120153":"markdown"},"source":{"4f8bce9e":"# Install vaex & pythran from pip\n!pip install vaex\n!pip install pythran","43ac94c4":"# Import packages\nimport vaex\nimport vaex.ml\n\nimport numpy as np\n\nimport pylab as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\n\nimport time","59ca07d4":"df = vaex.ml.datasets.load_iris_1e8()","8a02ca20":"# Get a preview of the data\ndf","9412cb1e":"def benchmark(func, reps=5):\n    times = []\n    for i in tqdm(range(reps), leave=False, desc='Benchmark in progress...'):\n        start_time = time.time()\n        res = func()\n        times.append(time.time() - start_time)\n    return np.mean(times), res","53b879bc":"def some_heavy_function(x1, x2, x3, x4):\n    a = x1**2 + np.sin(x2\/x1) + (np.tan(x2**2) - x4**(2\/3))\n    b = (x1\/x3)**(0.3) + np.cos(x1) - np.sqrt(x2) - x4**3\n    return a**(2\/3) \/ np.tan(b)","da943144":"# Numpy\ndf['func_numpy'] = some_heavy_function(df.sepal_length, df.sepal_width, \n                                       df.petal_length, df.petal_width)\n\n# Numba\ndf['func_numba'] = df.func_numpy.jit_numba()\n\n# Pythran\ndf['func_pythran'] = df.func_numpy.jit_pythran()\n\n# CUDA\ndf['func_cuda'] = df.func_numpy.jit_cuda()","428e90a4":"# Calculation of the sum of the virtual columns - this forces their evaluation\nduration_numpy, res_numpy =  benchmark(df.func_numpy.sum)\nduration_numba, res_numba =  benchmark(df.func_numba.sum)\nduration_pythran, res_pythran =  benchmark(df.func_pythran.sum)\nduration_cuda, res_cuda =  benchmark(df.func_cuda.sum)","9c79dcec":"print(f'Result from the numpy sum {res_numpy:.5f}')\nprint(f'Result from the numba sum {res_numba:.5f}')\nprint(f'Result from the pythran sum {res_pythran:.5f}')\nprint(f'Result from the cuda sum {res_cuda:.5f}')","9707daa2":"# Calculate the speed-up compared to the (base) numpy computation\ndurations = np.array([duration_numpy, duration_numba, duration_pythran, duration_cuda])\nspeed_up = duration_numpy \/ durations\n\n# Compute\ncompute = ['numpy', 'numba', 'pythran', 'cuda']","789e74a9":"# Let's visualise it\n\nplt.figure(figsize=(16, 6))\n\nplt.subplot(121)\nplt.bar(compute, speed_up)\nplt.tick_params(labelsize=14)\n\nfor i, (comp, speed) in enumerate(zip(compute, speed_up)):\n    plt.annotate(s=f'x {speed:.1f}', xy=(i-0.1, speed+0.3), fontsize=14)\nplt.annotate(s='(higher is better)', xy=(0, speed+2), fontsize=16)\n\nplt.title('Evaluation of complex virtual columns with Vaex', fontsize=16)\nplt.xlabel('Accelerators', fontsize=14)\nplt.ylabel('Speed-up wrt numpy', fontsize=14)\nplt.ylim(0, speed_up[-1]+5)\n\nplt.subplot(122)\nplt.bar(compute, durations)\nplt.tick_params(labelsize=14)\n\nfor i, (comp, duration) in enumerate(zip(compute, durations)):\n    plt.annotate(s=f'{duration:.1f}s', xy=(i-0.1, duration+0.3), fontsize=14)\nplt.annotate(s='(lower is better)', xy=(2, durations[0]+3), fontsize=16)\n\nplt.title('Evaluation of complex virtual columns with Vaex', fontsize=16)\nplt.xlabel('Accelerators', fontsize=14)\nplt.ylabel('Duration [s]', fontsize=14)\nplt.ylim(0, durations[0]+5)\n\n\nplt.tight_layout()\nplt.show()","3a60b764":"def arc_distance(theta_1, phi_1, theta_2, phi_2):\n    temp = (np.sin((theta_2-theta_1)\/2*np.pi\/180)**2\n           + np.cos(theta_1*np.pi\/180)*np.cos(theta_2*np.pi\/180) * np.sin((phi_2-phi_1)\/2*np.pi\/180)**2)\n    distance = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n    return distance * 3958.8","fafad590":"# Numpy\ndf['arc_distance_numpy'] = arc_distance(df.sepal_length, df.sepal_width, \n                                       df.petal_length, df.petal_width)\n\n# Numba\ndf['arc_distance_numba'] = df.arc_distance_numpy.jit_numba()\n\n# Pythran\ndf['arc_distance_pythran'] = df.arc_distance_numpy.jit_pythran()\n\n# CUDA\ndf['arc_distance_cuda'] = df.arc_distance_numpy.jit_cuda()","cc76657e":"# Calculation of the sum of the virtual columns - this forces their evaluation\nduration_numpy, res_numpy =  benchmark(df.arc_distance_numpy.sum)\nduration_numba, res_numba =  benchmark(df.arc_distance_numba.sum)\nduration_pythran, res_pythran =  benchmark(df.arc_distance_pythran.sum)\nduration_cuda, res_cuda =  benchmark(df.arc_distance_cuda.sum)","c65f17e1":"print(f'Result from the numpy sum {res_numpy:.5f}')\nprint(f'Result from the numba sum {res_numba:.5f}')\nprint(f'Result from the pythran sum {res_pythran:.5f}')\nprint(f'Result from the cuda sum {res_cuda:.5f}')","352212bc":"# Calculate the speed-up compared to the (base) numpy computation\ndurations = np.array([duration_numpy, duration_numba, duration_pythran, duration_cuda])\nspeed_up = duration_numpy \/ durations","c18773f2":"# Let's visualise it\n\nplt.figure(figsize=(16, 6))\n\nplt.subplot(121)\nplt.bar(compute, speed_up)\nplt.tick_params(labelsize=14)\n\nfor i, (comp, speed) in enumerate(zip(compute, speed_up)):\n    plt.annotate(s=f'x {speed:.1f}', xy=(i-0.1, speed+0.3), fontsize=14)\nplt.annotate(s='(higher is better)', xy=(0, speed+2), fontsize=16)\n\nplt.title('Evaluation of complex virtual columns with Vaex', fontsize=16)\nplt.xlabel('Accelerators', fontsize=14)\nplt.ylabel('Speed-up wrt numpy', fontsize=14)\nplt.ylim(0, speed_up[-1]+5)\n\nplt.subplot(122)\nplt.bar(compute, durations)\nplt.tick_params(labelsize=14)\n\nfor i, (comp, duration) in enumerate(zip(compute, durations)):\n    plt.annotate(s=f'{duration:.1f}s', xy=(i-0.1, duration+0.3), fontsize=14)\nplt.annotate(s='(lower is better)', xy=(2, durations[0]+3), fontsize=16)\n\nplt.title('Evaluation of complex virtual columns with Vaex', fontsize=16)\nplt.xlabel('Accelerators', fontsize=14)\nplt.ylabel('Duration [s]', fontsize=14)\nplt.ylim(0, durations[0]+5)\n\n\nplt.tight_layout()\nplt.show()","4b352e9a":"### Vaex performance test (GPU instance)\n\nThe purpose of this Notebook is to evaluate the performance of the [vaex](https:\/\/github.com\/vaexio\/vaex) DataFrame library on a Kaggle instance with an active GPU.\n\nThe process is simple:\n- Install vaex\n- Generate a \"big-ish\" dataset (100_000_000 rows) by replicating the Iris flower dataset\n- Create a couple of computationally expensive virtual columns\n- Evaluate them on the fly via `numpy`, `jit_numba` and `jit_cuda` and compare performance.","4a17e7b6":"The following method replicates the Iris flower dataset many times, and creates a hdf5 file on disk comprising ~1e8 samples. The purpose is to create \"big-ish\" data for various types of performance testing. ","65e4edff":"Let us define a simple function that will measure the execution time of other functions (vaex methods).","97839784":"Now let's do some performance testing. I have defined some function, just on top of my head that is a bit computationally challenging to be calculated on the fly. The idea is to see how fast vaex performs when the computations are done with numpy, numba, pythran and cuda. ","02120153":"Let us try another involved function, this time one calculating the arc-distance between two points on a sphere. We don't have such data here, but let's use this anyway in order to test the speed of the computations."}}