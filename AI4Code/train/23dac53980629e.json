{"cell_type":{"67aac73a":"code","3d89457d":"code","7f144651":"code","88b80771":"code","335e9e71":"code","e3d3180e":"code","47b2893a":"code","20446178":"code","354a5d3a":"code","eb65d049":"code","958f3110":"code","a7150168":"code","c6de602c":"code","20ad25c2":"code","b575b479":"code","5a951999":"code","6c3a7a67":"code","f702d3f9":"code","870b97c9":"code","2f3af34f":"code","bb662438":"code","582c6fcb":"code","c648a173":"code","58c7f765":"code","c88686fa":"code","38a83bd6":"code","393e76a6":"code","f91e5d89":"code","6c7d047d":"code","05a75c67":"code","ecfc55e7":"code","8c535cef":"code","10588557":"code","9838c6b3":"code","b4bd8929":"code","f939b874":"code","3eb4609c":"code","dc90f1a0":"code","effc41f9":"markdown","71e26dea":"markdown","6717ed0e":"markdown","da7be14b":"markdown","805bb6a3":"markdown","f5e4eab1":"markdown","543044d0":"markdown","a28087e7":"markdown","ef82c828":"markdown","5a7e17d1":"markdown","8a98af71":"markdown","a30a2250":"markdown","11dff2a2":"markdown","ea7e0f82":"markdown","07b4d0b3":"markdown","801374cd":"markdown"},"source":{"67aac73a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport nltk\n\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant","3d89457d":"train = pd.read_csv(r'..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv(r'..\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv(r'..\/input\/nlp-getting-started\/sample_submission.csv')","7f144651":"pd.set_option('display.max_colwidth',200)","88b80771":"train.head()","335e9e71":"train.shape, test.shape, sub.shape","e3d3180e":"train.isnull().sum()","47b2893a":"test.isnull().sum()","20446178":"train.drop(['id','keyword','location'],axis=1,inplace=True)\ntest.drop(['id','keyword','location'],axis=1,inplace=True)","354a5d3a":"train[train['target']==1].head()","eb65d049":"train[train['target']==0].head()","958f3110":"train['target'].value_counts()","a7150168":"length_train = train['text'].str.len()\nlength_test = test['text'].str.len()\nplt.hist(length_train,bins=20,label = 'train_text')\nplt.hist(length_test,bins=20,label='test_text')\nplt.legend()\nplt.show()","c6de602c":"def remove_pattern(input_txt,pattern):\n    r=re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i,'',input_txt)\n    return input_txt","20ad25c2":"train['text'] = np.vectorize(remove_pattern)(train['text'],'@[\/w]*')\ntest['text'] = np.vectorize(remove_pattern)(test['text'],'@[\/w]*')","b575b479":"train['text'] = train['text'].str.replace('[^a-zA-Z#]',' ')\ntest['text'] = test['text'].str.replace('[^a-zA-Z#]',' ')","5a951999":"train['text'] = train['text'].apply(lambda x: ' '.join([w for w in x.split()\n                                                                 if len(w)>2]))\ntest['text'] = test['text'].apply(lambda x: ' '.join([w for w in x.split()\n                                                                 if len(w)>2]))\ntrain.head()","6c3a7a67":"all_words = ' '.join([text for text in train['text']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","f702d3f9":"normal_words = ' '.join([text for text in train['text'][train['target']==0]])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","870b97c9":"disaster_related_words = ' '.join([text for text in train['text'][train['target']==1]])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(disaster_related_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","2f3af34f":"# function to collect hashtags\ndef hashtag_extract(x):   \n            hashtags = []    \n\n            # Loop over the words in the tweet    \n            for i in x:        \n                ht = re.findall(r\"#(\\w+)\", i)        \n                hashtags.append(ht)     \n            return hashtags\n\n# extracting hashtags from non racist\/sexist tweets \nHT_regular = hashtag_extract(train['text'][train['target'] == 0]) \n\n# extracting hashtags from racist\/sexist tweets \nHT_negative = hashtag_extract(train['text'][train['target'] == 1]) \n\n# unnesting list \nHT_regular = sum(HT_regular,[])\nHT_negative = sum(HT_negative,[])","bb662438":"a = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                 'Count': list(a.values())})\n\n#selecting top 20 most frequent hashtags\nd = d.nlargest(columns = 'Count',n=20)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data = d, x='Hashtag', y='Count')\nax.set(ylabel = 'Count')\nplt.show()","582c6fcb":"b = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(b.keys()),\n                 'Count': list(b.values())})\n\n#selecting top 20 most frequent hashtags\ne = e.nlargest(columns = 'Count',n=20)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data = e, x='Hashtag', y='Count')\nax.set(ylabel = 'Count')\nplt.show()","c648a173":"data = pd.concat([train,test])\ndata.shape","58c7f765":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","c88686fa":"corpus = create_corpus_new(data)","38a83bd6":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","393e76a6":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","f91e5d89":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","6c7d047d":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec    ","05a75c67":"tweet_pad[0][0:]","ecfc55e7":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\nmodel.summary()","8c535cef":"train_new = tweet_pad[:train.shape[0]]\ntest_new = tweet_pad[train.shape[0]:]","10588557":"X_train,X_test,y_train,y_test=train_test_split(train_new,train['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","9838c6b3":"# Recomended 10-20 epochs\nhistory=model.fit(X_train,y_train,batch_size=128,epochs=10,validation_data=(X_test,y_test),verbose=2)","b4bd8929":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","f939b874":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","3eb4609c":"test_pred= model.predict(test_new)\ntest_pred_int = test_pred.round().astype('int')\nsub['target'] = test_pred_int\nsub.to_csv('pred.csv',index=False)","dc90f1a0":"sub","effc41f9":"Let\u2019s check out a few normal text.","71e26dea":"Let\u2019s have a glimpse at target-distribution in the train dataset.","6717ed0e":"Given below is a user-defined function to remove unwanted text patterns from the text.","da7be14b":"**Understanding the impact of Hashtags on texts sentiment**\n\nHashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value in this task.","805bb6a3":"Now we will check the distribution of length of the tweets, in terms of words, in both train and test data.","f5e4eab1":"**Understanding the common words used in the texts: WordCloud**\n\nNow I want to see how well the given sentiments are distributed across the train dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds.\n\nA wordcloud is a visualization where in the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n\nLet\u2019s visualize all the words our data using the wordcloud plot.","543044d0":"**Removing Short Words**\n\nWe have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 2 or less. For example, terms like \u201chmm\u201d, \u201coh\u201d are of very little use. It is better to get rid of them.","a28087e7":"Let\u2019s check out a few disaster related text .","ef82c828":"### Model Building","5a7e17d1":"### GloVe\n\nI will use GloVe pretrained corpus model to represent our words.","8a98af71":"There are quite a many words and characters which are not really required. So, we will try to keep only those words which are important and add value.","a30a2250":"Here we will replace everything except characters and hashtags with spaces. The regular expression \u201c[^a-zA-Z#]\u201d means anything except alphabets and \u2018#\u2019.","11dff2a2":"**Removing Twitter handles (@user)** ","ea7e0f82":"**Removing Punctuations, Numbers, and Special Characters**","07b4d0b3":"**Data Cleaning**\n\nIn any natural language processing task, cleaning raw text data is an important step. It helps in getting rid of the unwanted words and characters which helps in obtaining better features. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don\u2019t carry much weightage in context to the text.","801374cd":"### Prediction"}}