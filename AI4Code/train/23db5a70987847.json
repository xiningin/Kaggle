{"cell_type":{"5a6acc31":"code","137edd00":"code","be6301db":"code","33c54764":"code","f9e9139d":"code","410a2955":"code","9befdd42":"code","d57d885a":"code","ba94870b":"code","01c2b165":"markdown","7e9d6305":"markdown","8cf97878":"markdown","81fbded3":"markdown","8def6867":"markdown","6c10e303":"markdown","5e18f893":"markdown","384eecd3":"markdown","a88f0e21":"markdown","2feec23b":"markdown","13b860ad":"markdown","b8f55af1":"markdown","251c772e":"markdown","cfc5d091":"markdown","48db9349":"markdown","92314441":"markdown"},"source":{"5a6acc31":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","137edd00":"# Declaring the path to load efficientNet models.\nimport sys\nsys.path.append('..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master')","be6301db":"#IMPORTS\nimport os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom tqdm.auto import tqdm\nfrom typing import Dict\nimport matplotlib.pyplot as plt\nfrom tempfile import gettempdir\nfrom prettytable import PrettyTable\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n\n# TORCH\nimport torch\nfrom torch import nn, optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\n\n# TORCH XLA\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\n\n# L5Kit\nfrom l5kit.configs import load_config_data\nfrom l5kit.geometry import transform_points\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\nfrom l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\nfrom l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n\n# EfficientNet\nfrom efficientnet_pytorch import model as enet\n\n# Catalyst Module\nfrom catalyst import dl\nfrom catalyst.utils import metrics\nfrom catalyst.dl import utils\nfrom catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n\n# Filter warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","33c54764":"# CONFIGRATIONS\n\nclass Config:\n    WEIGHT_FILE = None # Model state_dict path of previously trained model\n    \n    MODEL_NAME = \"efficientnet-b0\" # b0-b7 could be the different choices.\n    \n    IMG_SIZE = 222\n    \n    PIXEL_SIZE = 0.45\n        \n    EPOCHS = 3 # Epochs to train the model for.\n    \n    l_rate = 0.001 # Learning rate\n\n    scheduler_params = dict(\n        mode='max',\n        factor=0.7,\n        patience=0,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau # Scheduler for learning rate.\n    \n    criterion = torch.nn.MSELoss(reduction=\"none\") # Loss function.\n     \n    verbose_steps = 50 # Steps to print model's training status after.\n    \nconfig = Config()","f9e9139d":"# L5KIT'S CONFIGRATIONS\n\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\ndm = LocalDataManager()\ncfg = {\n          'model_params': {'model_architecture': 'efficientnet-b6',\n          'history_num_frames': 0,\n          'history_step_size': 1,\n          'history_delta_time': 0.1,\n          'future_num_frames': 50,\n          'future_step_size': 1,\n          'future_delta_time': 0.1},\n\n        'raster_params': {'raster_size': [config.IMG_SIZE, config.IMG_SIZE],\n          'pixel_size': [config.PIXEL_SIZE, config.PIXEL_SIZE],\n          'ego_center': [0.25, 0.5],\n          'map_type': 'py_semantic',\n          'satellite_map_key': 'aerial_map\/aerial_map.png',\n          'semantic_map_key': 'semantic_map\/semantic_map.pb',\n          'dataset_meta_key': 'meta.json',\n          'filter_agents_threshold': 0.5},\n\n        'train_data_loader': {'key': 'scenes\/train.zarr',\n          'batch_size': 12,\n          'shuffle': True,\n          'num_workers': 8},\n\n        \"valid_data_loader\":{\"key\": \"scenes\/validation.zarr\",\n                            \"batch_size\": 8,\n                            \"shuffle\": False,\n                            \"num_workers\": 5},\n    \n        }","410a2955":"class TPUFitter:\n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n\n        # Following some lines are for setting up the AdamW optimizer.\n        # See below explanation for details.\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n             'weight_decay': 0.001},\n            \n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n             'weight_decay': 0.0}\n        ]\n\n        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.l_rate*xm.xrt_world_size())\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        \n        # Following function is used for printing to output efficiently. \n        xm.master_print(f'Model Loaded on {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        \"\"\"Function to fit the model.\"\"\"\n        \n        # Loop over the Epochs\n        for e in range(config.EPOCHS):\n            \n            t = time.time() # Get a measurement of time.\n            para_loader = pl.ParallelLoader(train_loader, [self.device]) # Distributed loading of the model.\n            loss = self.forward(para_loader.per_device_loader(self.device))\n            xm.master_print(\n                            f'[RESULT]: Train. Epoch: {e+1}, ' + \\\n                            f'loss: {loss:.5f}, '+ \\\n                            f'time: {(time.time() - t):.5f}'\n                           )\n            xm.master_print(\"\\n\")\n\n            t = time.time()\n            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n            loss = self.validation(para_loader.per_device_loader(self.device))\n            xm.master_print(\n                     f'[RESULT]: Validation. Epoch: {e+1}, ' + \\\n                     f'loss: {loss:.5f}, ' + \\\n                     f'time: {(time.time() - t):.5f}'\n                    )\n            xm.master_print(\"\\n\")\n\n    def validation(self, val_loader):\n        \"\"\"Function to validate the model's predictions.\"\"\"\n        \n        # Setting model to evaluation mode.\n        self.model.eval()\n        t = time.time()\n        \n        for step, data in enumerate(val_loader):\n            with torch.no_grad():\n                inputs = data[\"image\"].to(self.device)\n                targets = data[\"target_positions\"].to(self.device)\n                target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(self.device)\n                \n                outputs = self.model(inputs)\n                outputs = outputs.reshape(targets.shape)\n                loss = config.criterion(outputs, targets)\n                loss = loss * target_availabilities\n                loss = loss.mean()\n\n                if step % config.verbose_steps == 0:\n                    xm.master_print(\n                        f'Valid Step {step}, loss: ' + \\\n                        f'{loss:.4f}' + \\\n                        f'time: {(time.time() - t):.5f}'\n                    )                \n        return loss\n         \n    def forward(self, train_loader):\n        \"\"\"Function to perform custom forward propagation.\"\"\"\n        \n        # Setting model to training mode.\n        self.model.train()\n        \n        t = time.time()\n        for step, data in enumerate(train_loader):\n            inputs = data[\"image\"].to(self.device)\n            target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(self.device)\n            targets = data[\"target_positions\"].to(self.device)\n    \n            outputs = self.model(inputs)\n            outputs = outputs.reshape(targets.shape)\n            loss = config.criterion(outputs, targets)\n    \n            loss = loss * target_availabilities\n            loss = loss.mean()\n\n            if step % config.verbose_steps == 0:\n                xm.master_print(\n                    f'Train Step {step}, loss: ' + \\\n                    f'{loss:.4f}' + \\\n                    f'time: {(time.time() - t):.5f}'\n                )\n            self.optimizer.zero_grad()\n        \n            loss.backward()\n            xm.optimizer_step(self.optimizer)\n        \n        self.model.eval()\n        self.save('last-checkpoint.bin')\n        return loss\n\n    def save(self, path):\n        \"\"\"Function to save the model's current state.\"\"\"\n        xm.save(self.model.state_dict(), path)","9befdd42":"# Implementation of class to load the particular EfficientNet model.\nclass LyftModel(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.backbone = enet.EfficientNet.from_name(config.MODEL_NAME)\n        \n        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n        num_in_channels = 3 + num_history_channels\n        num_targets = 2*cfg[\"model_params\"][\"future_num_frames\"]\n    \n        self.backbone._conv_stem = nn.Conv2d(\n            num_in_channels,\n            self.backbone._conv_stem.out_channels,\n            kernel_size=self.backbone._conv_stem.kernel_size,\n            stride=self.backbone._conv_stem.stride,\n            padding=self.backbone._conv_stem.padding,\n            bias=False\n        )\n    \n        self.backbone._fc = nn.Linear(in_features=self.backbone._fc.in_features, out_features=num_targets)\n    \n    def forward(self, x):\n        \"\"\"Function to perform forward propagation.\"\"\"\n        x = self.backbone(x)\n        return x","d57d885a":"def get_dataloader(config, zarr_data, subset_len, map_type=\"py_semantic\"):\n    \"\"\"Creates DataLoader instance for the given dataset.\"\"\"\n    \n    cfg[\"raster_params\"][\"map_type\"] = map_type\n    rasterizer = build_rasterizer(cfg, dm)\n    chunk_data = ChunkedDataset(zarr_data).open()\n    agent_data = AgentDataset(cfg, chunk_data, rasterizer)\n    \n    # Getting Subset of the dataset.\n    subset_data = torch.utils.data.Subset(agent_data, range(0, subset_len))\n    \n    dataloader = DataLoader(subset_data, \n                            batch_size=config[\"batch_size\"],\n                            num_workers=config[\"num_workers\"],\n                            shuffle=config[\"shuffle\"]\n                           )\n    return dataloader\n\ndef train():\n    device = xm.xla_device()\n    model = LyftModel(cfg).to(device)\n    fitter = TPUFitter(model, device)\n    \n    xm.master_print(\"Preparing the dataloader..\")\n    train_dataloader = get_dataloader(cfg[\"train_data_loader\"], dm.require(\"scenes\/train.zarr\"), 5000)\n    val_dataloader   = get_dataloader(cfg[\"valid_data_loader\"], dm.require(\"scenes\/validate.zarr\"), 300)\n    \n    xm.master_print(\"Training the model..\")\n    fitter.fit(train_dataloader, val_dataloader)\n    \n    return fitter","ba94870b":"model = train()","01c2b165":"<body>\n<p style=\"color:#260000\"><b>Basically Pytorch uses <a href=\"https:\/\/pytorch.org\/xla\/release\/1.6\/index.html\">torch_xla<\/a> package to connect to do computations on TPU. Not only we can train on a single TPU core but we can also perform computations utilizing all the available TPU cores. The following cell loads the torch_xla package and setup dependencies for it.<\/b><\/p>\n    <p style=\"color:#de7171\"><b>The next cell is inspired from the <a href=\"https:\/\/www.kaggle.com\/abhishek\/bert-multi-lingual-tpu-training\">notebook<\/a> of one of my favorite mentors- Abhishek Thakur.<\/b><\/p>\n    <\/body>","7e9d6305":"# Utility Functions\n\n<p style=\"color:green\">At the end I would try to explain some of the parts of the code that I think should be discussed with concepts.<\/p>\n\n<p style=\"color:black\"><b>To keep things simpler I did not add the code for logging. You could, if you want to or I would be anyways adding it in the next version of this notebook.<\/b><\/p>","8cf97878":"# Setup Pytorch for TPU.","81fbded3":"<body>\n    <p style=\"color:magenta; font-size:20px; font-family:sanserif;\">Imports<\/p>\n<\/body>","8def6867":"# Train the Model","6c10e303":"<body>\n<p style=\"color:#260000\"><b>Here I only used a subset of the original dataset to train faster as this is an illustration based notebook. However you must use a larger subset or the complete dataset for training of the model.<\/b><\/p>","5e18f893":"**Following are a set of main parameters and configrations that I would be going to use in this version of the notebook-**\n\n1. Model family: EfficientNet (other architectures such as resnet or inception could also prove beneficial).\n2. Pixel size: 0.45\n3. Raster Size: 222\n4. Training batch size: 12\n5. Validation batch size: 8\n6. Epochs: 3 (should be atleast 5 or the subset of dataset must be larger)\n7. Train Subset length: 5000\n8. Validation Subset length: 300\n\nFor more information on why I chose this specific combination of `pixel_size` and `raster_size`, please have a look at this [discussion thread](https:\/\/www.kaggle.com\/c\/lyft-motion-prediction-autonomous-vehicles\/discussion\/178323) and this [notebook](https:\/\/www.kaggle.com\/forwet\/lyft-efficientnet-model-train).","384eecd3":"<p style=\"color: #1bab4e\"><b>Till now I find that only AdamW optimizer should be explained and the rest of the code is easily understandable. However you could ask for any explanations in the comments or reach me out through mail and if I would be knowing that, I would be delighted to help you. Thanks!<\/b><\/p>","a88f0e21":"# Explanations\n\n<p style=\"color:#806f04\">In the following notebook we would be looking at explanations of some important code areas which should be into your knowledge both for your experience as well as to completely understand this notebook.<\/p>","2feec23b":"<p style=\"color:red;font-size:18px\"><b>Hope You Guys Liked my notebook.<\/b><\/p>\n<p style=\"color:magenta;font-size:18px\"><b>Please upvote if you did to keep me motivated.\ud83d\ude01\ud83d\ude00<\/b><\/p>","13b860ad":"<p style=\"color:#b900bf; font-size:18px\"><b>Future Scope:<\/b><\/p>\n\n1. LabelSmoothing\n2. Sampler (if necessary)\n3. Logging Functionality\n\nFollowing code cell is inspired from this [kernel](https:\/\/www.kaggle.com\/shonenkov\/tpu-training-super-fast-xlmroberta).","b8f55af1":"<body>\n    <p style=\"color:#f21616;\"> Caution: Run the first three cells in the sequential order or you will end up getting errors in imports.<\/p>\n<\/body>","251c772e":"# THE TPU POWER\nAs we all know that **TPUs** enhance the computation power and also results in good performance of the model. Hence in this notebook we would be going to look up at using the power of TPUs to perform motion prediction on the Lyft Dataset. Specifically we would be training a model using Pytorch on TPU(XLA).\n\n![image.png](attachment:image.png)\n   >Image source: kaggle\n   \nIf we trained the same model six months back we could have had issues in running a Pytorch model on TPU as is evident from this [discussion thread](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/138265).  But now things are pretty much sorted and we can train a model with minimal issues.\ud83d\ude0d\ud83d\ude09\n\nOne more thing to praise TPUs is that they are highly memory efficient. They are well designed to reuse memory space and does not store garbage data. Try it on model running with and without TPU and see the results. Share them also.\ud83d\ude09","cfc5d091":"Just a note that you have saved your progress when you were iterating through the above code so now you could simply load the model to further train it. Please also note the TPU requirements for this competition.\ud83d\ude0f\ud83d\ude44 ","48db9349":"# Configurations","92314441":"## AdamW Optimizer\n\nAssuming you're having a good knowledge about [Adam](https:\/\/arxiv.org\/abs\/1412.6980) optimizer, we would head directly towards understanding of the [AdamW](https:\/\/www.fast.ai\/2018\/07\/02\/adam-weight-decay\/) optimizer.\n\nSo, `AdamW` optimizer is equipped with an extra **weight decay** parameter (a hyperparameter) that it uses along with the general implementation of `Adam` algorithm. Note that apart from the case of vanilla SGD, L2 regularization (which Adam implements generally) and weight decay(which AdamW implements) are two different things.\n\nThe formula for L2 regularization would be-\n\n![L2.PNG](attachment:L2.PNG)\n\nAnd formula for weight decay would be-\n\n![image.png](attachment:image.png)\n\nSo a basic difference could be understood as that we add a penalizing term to the loss in the `L2-regularization` whereas we subtract an additional fraction of the weight from itself when updating it. Here `wd` is simply a hyperparameter which measures how larger decay in the weight you want at each update. \n\nThat's why we added a `weight_decay` parameter here (from above code)-\n    ![weight_decay.PNG](attachment:weight_decay.PNG)\n    \nHere we are simply specifying a `weight_decay` value of 0 for the parameter for which we don't want the weights to decay additionally. Finally we pass this configuration to `AdamW` optimizer. With this we just combined the power of `Adam` optimization with weight_decay so that weights are learned and updated much more efficiently.\n\n**Read more about AdamW from [here](https:\/\/www.fast.ai\/2018\/07\/02\/adam-weight-decay\/).**"}}