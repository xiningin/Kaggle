{"cell_type":{"c728cb74":"code","1229c5e3":"code","bd9109dc":"code","58fd4190":"code","2c992ba5":"code","f9c8aeed":"code","71df8ef3":"code","73954c15":"code","097a7e90":"code","0e3e9525":"code","3f8bb64b":"code","2bd15146":"code","aba56228":"code","21712e30":"code","70af285f":"code","3c64f1e2":"code","dcd8a03d":"code","4c8e120f":"code","364a870b":"code","7e80332b":"code","3a5a3cd9":"code","4af11cb1":"code","8152c88d":"code","481596e0":"code","5f3c5386":"code","89e172ef":"code","a5d0324a":"code","380ad4eb":"code","cb3be454":"code","c345e413":"code","e4a5635f":"code","bc3be7af":"code","4bd3a3b6":"code","90fc186e":"code","2f4752ac":"code","e40228f8":"code","1686fdda":"code","38e7a1f8":"code","0a997fd3":"code","d41fbcf8":"code","59d68787":"code","d62ba2fd":"code","eebf738e":"code","67c329da":"code","ea536f84":"code","2a653f40":"code","4ae430dd":"code","8e72bc7b":"code","17a3e5d4":"code","0d0ed4dd":"code","b35c7b75":"code","abed538e":"code","9d0deac5":"code","70a86560":"code","1cebb504":"code","836a051f":"code","324c7a79":"code","98feb8d2":"code","e5cf75c2":"code","79cc5909":"code","4b503d36":"code","5d5e8f17":"code","543a57c6":"code","abbfb43c":"code","b31fe4d5":"code","a4b39073":"code","df050d44":"code","9e47539a":"code","5ac6f846":"code","14178899":"code","e24b4040":"code","4e41f103":"code","062f5438":"code","29dfffa3":"code","6438f46f":"code","ced7ab41":"code","454dc7f2":"code","1df281dd":"code","dcafc46c":"code","eba698e1":"code","94e79dad":"code","5ba2841f":"code","b43ca7b9":"code","445c5036":"code","c895a1f7":"code","774054df":"code","92490f6f":"code","d0b13c6f":"code","d76ee61c":"code","eeb54cda":"code","edcf5bf2":"code","0be380da":"markdown","0b738a83":"markdown","6e02441f":"markdown","00599dce":"markdown","c75d4f2c":"markdown","44063072":"markdown","c3141e90":"markdown","4b42a34b":"markdown","1975a65b":"markdown","5e09fb09":"markdown","b9a663f0":"markdown","7f838fe1":"markdown","613cde23":"markdown","c25a2fbe":"markdown","73326c26":"markdown","038fb2b0":"markdown","250bfb11":"markdown","2ff662bd":"markdown","7b9e4511":"markdown","8bc0d6a2":"markdown","1c74f4cd":"markdown","ed047b1e":"markdown","cd577915":"markdown","419a4d85":"markdown","66a3065d":"markdown","82e6affd":"markdown"},"source":{"c728cb74":"#Importing all the libaries and models that we will be using throughout this Notebook\nimport numpy as np # linear algebra\nimport pandas as pd # data processing and reading csv files\nimport matplotlib.pyplot as plt # visualising libary to visualise data\nimport seaborn as sns # A libary to visualise data\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n#Importing all the models we going to use in this Notebook\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn import metrics\nimport re","1229c5e3":"#Importing the train and test csv into dataframes \ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","bd9109dc":"#train information\ntrain.info()","58fd4190":"#All the stats related information \ntrain.describe()","2c992ba5":"#Counting the dtype \ntrain.get_dtype_counts()","f9c8aeed":"#Checking the variables that are 0.3 and above correlated with the Independant variable which is Saleprice\ncorrmat = train.corr()\ntop_f = corrmat.index[abs(corrmat['SalePrice'])>0.3]\ntop_f\n","71df8ef3":"train_id = train['Id'] #Making a variable called train_id which will containg the ID numbers of train\ntest_id = test['Id'] #Making a variable called test_id which will containg the ID numbers of test\ntrain = train.drop('Id', axis = 1) #Dropping the train ID in our orginal DataFrame\ntest = test.drop('Id', axis = 1) #Dropping the test ID in our orginal DataFrame","73954c15":"#We are going to use this to seperate the combined train and test dataframe closer to the end \nntrain = train.shape[0]\nntest = test.shape[0]","097a7e90":"#Taking outliers out of the equation to produce better results\nplt.scatter(x=train['GrLivArea'],y=train['SalePrice'])\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\nplt.scatter(x=train['GrLivArea'],y=train['SalePrice'])","0e3e9525":"#Checking the amount of data that is missing through visual representation\ntrain_na = (train.isnull().sum() \/ len(train))*100\ntrain_na = train_na.drop(train_na[train_na==0].index).sort_values(ascending=False)\nplt.xticks(rotation='90')\nplt.bar(train_na.index,train_na)","3f8bb64b":"#Code for checking the percentage of the missing data in a dataFrame\nmissing_data = pd.DataFrame(train_na, columns = ['NAN VALUES in perc'])","2bd15146":"#Missing data in percentage\nmissing_data","aba56228":"train[\"PoolQC\"] = train[\"PoolQC\"].fillna(\"None\") # We will assume that for the NAN values in PoolQC, it indicates that theres no pool\ntrain[\"MiscFeature\"] = train[\"MiscFeature\"].fillna(\"None\")# Assuming theres no MiscFeatures\ntrain[\"Alley\"] = train[\"Alley\"].fillna(\"None\")#Assuming that the house doesnt have an Alley \ntrain[\"FireplaceQu\"] = train[\"FireplaceQu\"].fillna(\"None\")#Assumuing theres no firplace when the values are NAN\ntrain[\"Fence\"] = train[\"Fence\"].fillna(\"None\")#Assuming theres no fence\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train[col] = train[col].fillna('None')#Assuming theres no Garage None\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    train[col] = train[col].fillna(0)#Assuming that theres no Garage by using 0 \ntrain['LotFrontage'] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x:x.fillna(x.median()))\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    train[col] = train[col].fillna(0)#Assuming that theres no \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train[col] = train[col].fillna('None')\ntrain[\"MasVnrType\"] = train[\"MasVnrType\"].fillna(\"None\")\ntrain[\"MasVnrArea\"] = train[\"MasVnrArea\"].fillna(0)\ntrain['MSZoning'] = train['MSZoning'].fillna(train['MSZoning'].mode()[0])\ntrain.drop('Utilities',inplace=True,axis=1)\ntrain['Functional'] = train['Functional'].fillna('Typ')\nmode_col = ['Electrical','KitchenQual', 'Exterior1st', 'SaleType']\nfor col in mode_col:\n    train[col] = train[col].fillna(train[col].mode()[0])\ntrain[\"Exterior2nd\"] = train[\"Exterior2nd\"].fillna(\"None\")","21712e30":"def checking_nan(df):\n    for column in df:\n            if (df[column].isnull().sum() > 0):\n                print(column)","70af285f":"checking_nan(train)","3c64f1e2":"train.shape","dcd8a03d":"sns.catplot(\"Fireplaces\",\"SalePrice\",data=train,hue=\"FireplaceQu\", kind = 'point')","4c8e120f":"sns.violinplot(train[\"GarageCars\"],train[\"SalePrice\"])\nplt.title(\"Garage Cars Vs SalePrice \")\nplt.ylabel(\"SalePrice\")\nplt.xlabel(\"Number of Garage cars\")\nplt.show()","364a870b":"sns.stripplot(x=\"HeatingQC\", y=\"SalePrice\",data=train,hue='CentralAir',jitter=True, dodge = True)\nplt.title(\"Sale Price vs Heating Quality\")\nplt.show()","7e80332b":"sns.catplot(\"KitchenAbvGr\",\"SalePrice\",data=train,hue=\"KitchenQual\", kind = 'point')\nplt.title(\"Sale Price vs Kitchen\");","3a5a3cd9":"plt.figure(figsize = (10,10))\nplt.barh(train[\"Neighborhood\"],train[\"SalePrice\"])\nplt.title(\"Sale Price vs Neighborhood\");","4af11cb1":"sns.barplot(train['OverallQual'], train['SalePrice'])\nplt.title('Sale Price vs Overall Qual')\nplt.show()","8152c88d":"#Creating copies of the original variables to manually encode it \ntrain['WoodDeckSF_Bool']  = train['WoodDeckSF'].copy()\ntrain['OpenPorchSF_Bool']  = train['OpenPorchSF'].copy()\ntrain['MasVnrArea_Bool']  = train['MasVnrArea'].copy()\ntrain['GarageArea_Bool'] = train['GarageArea'].copy()\nnew_bool = 'WoodDeckSF_Bool', 'OpenPorchSF_Bool', 'MasVnrArea_Bool', 'GarageArea_Bool'","481596e0":"#Manual encoding for these variables. Esentially it indicates if there is a presence for that variable by using 1's and 0's\ndef changing_to_bool1(column):\n    for rows in train[column]:\n        if rows > 0:\n            train[column] = train[column].replace(rows, 1)","5f3c5386":"#Iterating through new_bool to apply the function \nfor col in new_bool:\n    changing_to_bool1(col)","89e172ef":"#Manual encoding for these variables. Esentially it indicates if there is a presence for that variable by using 1's and 0's\ndef changing_to_bool2(column):\n    for rows in train[column]:\n        if rows == 'Ex':\n            train[column] = train[column].replace(rows, 5)\n        elif rows == 'Gd':\n            train[column] = train[column].replace(rows,4)\n        elif rows == 'TA':\n            train[column] = train[column].replace(rows,3)\n        elif rows == 'Fa':\n            train[column] = train[column].replace(rows,2)\n        elif rows == 'Po':\n            train[column] = train[column].replace(rows,1)\n        elif rows == 'None':\n            train[column] = train[column].replace(rows,0)","a5d0324a":"bool_values = ('FireplaceQu', 'ExterQual', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual')","380ad4eb":"#Iterating through bool_values to apply the function\nfor col in bool_values:\n    changing_to_bool2(col)","cb3be454":"#Manual encoding for these variables. Esentially it indicates if there is a presence for that variable by using 1's and 0's\ndef changing_to_bool3(column):\n    for rows in train[column]:\n        if rows == 'Y':\n            train[column] = train[column].replace(rows,1)\n        else:\n            train[column] = train[column].replace(rows, 0)","c345e413":"changing_to_bool3('CentralAir')","e4a5635f":"#Manual encoding for these variables. Esentially it indicates if there is a presence for that variable by using 1's and 0's\ndef changing_to_bool4(column):\n    for rows in train[column]:\n        if rows == 'GLQ':\n            train[column] = train[column].replace(rows,6)\n        elif rows == 'ALQ':\n            train[column] = train[column].replace(rows,5)\n        elif rows == 'BLQ':\n            train[column] = train[column].replace(rows,4)\n        elif rows == 'Rec':\n            train[column] = train[column].replace(rows,3)\n        elif rows == 'LwQ':\n            train[column] = train[column].replace(rows,2)\n        elif rows == 'Unf':\n            train[column] = train[column].replace(rows,1)\n        elif rows == 'None':\n            train[column] = train[column].replace(rows,0)","bc3be7af":"changing_to_bool4('BsmtFinType1')","4bd3a3b6":"#This a combination of variables to create new ones \ntrain['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\ntrain['TotalBath'] = train['BsmtFullBath'] + (train['BsmtHalfBath'] \/ 0.5) + train['FullBath'] + (train['HalfBath'] \/ 0.5)\ntrain['Age'] = train['YrSold'] -train['YearBuilt']\ntrain['TotalFinSF'] = (train['GrLivArea'] + train['BsmtFinSF1'] + train['BsmtFinSF2']) - train['LowQualFinSF'] \ntrain['OtherAreas'] = train['TotalBsmtSF'] + train['GarageArea'] + train['OpenPorchSF'] + train['WoodDeckSF'] + train['3SsnPorch'] + train['OpenPorchSF'] + train['MasVnrArea']\ntrain['AreaPerRoom'] = train['GrLivArea'] \/ train['TotRmsAbvGrd']\ntrain['Porches_SF'] = train['WoodDeckSF'] + train['OpenPorchSF'] + train['ScreenPorch']","90fc186e":"sns.jointplot(x='Porches_SF', y='SalePrice', data=train, kind='reg', height=10, color= 'green')\n#plt.title('Sale Price vs Total Porches SF')\nplt.show()","2f4752ac":"sns.jointplot(x='AreaPerRoom', y='SalePrice', data=train, kind='reg', height=10);\nplt.show();","e40228f8":"sns.jointplot(x='OtherAreas', y='SalePrice', data=train, kind='reg', height=10, color = 'green');\nplt.show();","1686fdda":"sns.jointplot(x='TotalFinSF', y='SalePrice', data=train, kind='reg', height=10);\nplt.show();","38e7a1f8":"sns.jointplot(x='Age', y='SalePrice', data=train, kind='reg', height=10, color = 'green');\nplt.show();","0a997fd3":"sns.jointplot(x='TotalBath', y='SalePrice', data=train, kind='reg', height=10);\nplt.show();","d41fbcf8":"sns.jointplot(x='TotalSF', y='SalePrice', data=train, kind='reg', height=10, color = 'green');\nplt.show();","59d68787":"#Correlation between the all the variables versus Sale Price that we manually encoded on top \nplt.figure(figsize = (9,9))\nsns.heatmap(train[['FireplaceQu', 'ExterQual', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual','SalePrice']].corr(),annot =True, cmap=\"YlGnBu\")","d62ba2fd":"#Highest variables that correlates with SalePrice \ncorrmat = train.corr()\ntop_f = corrmat.index[abs(corrmat['SalePrice'])>0.6]\ntop_f\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[top_f].corr(),annot=True)","eebf738e":"#Importing the train and test csv into dataframes \ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","67c329da":"train_id = train['Id'] #Making a variable called train_id which will containg the ID numbers of train\ntest_id = test['Id'] #Making a variable called test_id which will containg the ID numbers of test\ntrain = train.drop('Id', axis = 1) #Dropping the train ID in our orginal DataFrame\ntest = test.drop('Id', axis = 1) #Dropping the test ID in our orginal DataFrame","ea536f84":"#Taking outliers out of the equation to produce better results\nplt.scatter(x=train['GrLivArea'],y=train['SalePrice'])\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","2a653f40":"#This function will check the skewness of the data\ndef checkskew(col):\n    sns.distplot(train[col],fit=norm)\n    (mu, sigma) = norm.fit(train[col])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\ncheckskew('SalePrice')","4ae430dd":"#We will then use np.log1p to make the SalePrice data evenly distributed \ntrain['SalePrice'] = np.log1p(train['SalePrice'])\ncheckskew('SalePrice')","8e72bc7b":"#We will create a new variable called y_train, we will use this as the independant variable \ny_train = train.SalePrice.values\ntrain.drop('SalePrice',axis=1,inplace=True)\ny_train.shape","17a3e5d4":"#We are going to use this to seperate the combined train and test dataframe closer to the end \nntrain = train.shape[0]\nntest = test.shape[0]","0d0ed4dd":"train.shape","b35c7b75":"y_train.shape","abed538e":"train[\"PoolQC\"] = train[\"PoolQC\"].fillna(\"None\") # We will assume that for the NAN values in PoolQC, it indicates that theres no pool\ntrain[\"MiscFeature\"] = train[\"MiscFeature\"].fillna(\"None\")# Asuuming theres no MiscFeatures\ntrain[\"Alley\"] = train[\"Alley\"].fillna(\"None\")#Assuming that the house doesnt have an Alley \ntrain[\"FireplaceQu\"] = train[\"FireplaceQu\"].fillna(\"None\")#Assumuing theres no firplace when the values are NAN\ntrain[\"Fence\"] = train[\"Fence\"].fillna(\"None\")#Assuming theres no fence\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train[col] = train[col].fillna('None')#Assuming theres no Garage None\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    train[col] = train[col].fillna(0)#Assuming that theres no Garage by using 0 \ntrain['LotFrontage'] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x:x.fillna(x.median()))\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    train[col] = train[col].fillna(0)#Assuming that theres no basement by using 0\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train[col] = train[col].fillna('None')\ntrain[\"MasVnrType\"] = train[\"MasVnrType\"].fillna(\"None\")\ntrain[\"MasVnrArea\"] = train[\"MasVnrArea\"].fillna(0)\ntrain['MSZoning'] = train['MSZoning'].fillna(train['MSZoning'].mode()[0])\ntrain.drop('Utilities',inplace=True,axis=1)\ntrain['Functional'] = train['Functional'].fillna('Typ')\nmode_col = ['Electrical','KitchenQual', 'Exterior1st', 'SaleType']\nfor col in mode_col:\n    train[col] = train[col].fillna(train[col].mode()[0])\ntrain[\"Exterior2nd\"] = train[\"Exterior2nd\"].fillna(\"None\")","9d0deac5":"#Function to check the NAN values\ndef checking_nan(df):\n    for column in df:\n            if (df[column].isnull().sum() > 0):\n                print(column)","70a86560":"checking_nan(train)","1cebb504":"test_na = (test.isnull().sum() \/ len(train))*100\ntest_na = test_na.drop(test_na[test_na==0].index).sort_values(ascending=False)\nplt.xticks(rotation='90')\nplt.bar(test_na.index,test_na)","836a051f":"#Doing the same here for what i did to the train dataset \ntest[\"PoolQC\"] = test[\"PoolQC\"].fillna(\"None\")\ntest[\"MiscFeature\"] = test[\"MiscFeature\"].fillna(\"None\")\ntest[\"Alley\"] = test[\"Alley\"].fillna(\"None\")\ntest[\"FireplaceQu\"] = test[\"FireplaceQu\"].fillna(\"None\")\ntest[\"Fence\"] = test[\"Fence\"].fillna(\"None\")\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    test[col] = test[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    test[col] = test[col].fillna(0)\ntest['LotFrontage'] = test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x:x.fillna(x.median()))\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    test[col] = test[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    test[col] = test[col].fillna('None')\ntest[\"MasVnrType\"] = test[\"MasVnrType\"].fillna(\"None\")\ntest[\"MasVnrArea\"] = test[\"MasVnrArea\"].fillna(0)\ntest['MSZoning'] = test['MSZoning'].fillna(test['MSZoning'].mode()[0])\ntest.drop('Utilities',inplace=True,axis=1)\ntest['Functional'] = test['Functional'].fillna('Typ')\nmode_col = ['Electrical','KitchenQual', 'Exterior1st', 'SaleType']\nfor col in mode_col:\n    test[col] = test[col].fillna(test[col].mode()[0])\ntest[\"Exterior2nd\"] = test[\"Exterior2nd\"].fillna(\"None\")","324c7a79":"#Function to check the NaN values \ndef checking_nan(df):\n    for column in df:\n            if (df[column].isnull().sum() > 0):\n                print(column)","98feb8d2":"checking_nan(test)","e5cf75c2":"#Joining the test and train dataset to make data (new dataframe)\ndata = pd.concat((train, test), sort = False).reset_index(drop=True)","79cc5909":"data.shape","4b503d36":"#This a combination of variables to create new ones \ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['TotalBath'] = data['BsmtFullBath'] + (data['BsmtHalfBath'] \/ 0.5) + data['FullBath'] + (data['HalfBath'] \/ 0.5)\ndata['Age'] = data['YrSold'] - data['YearBuilt']\ndata['TotalFinSF'] = (data['GrLivArea'] + data['BsmtFinSF1'] + data['BsmtFinSF2']) - data['LowQualFinSF'] \ndata['OtherAreas'] = data['TotalBsmtSF'] + data['GarageArea'] + data['OpenPorchSF'] + data['WoodDeckSF'] + data['3SsnPorch'] + data['OpenPorchSF'] + data['MasVnrArea']\ndata['AreaPerRoom'] = data['GrLivArea'] \/ data['TotRmsAbvGrd']\ndata['Porches_SF'] = data['WoodDeckSF'] + data['OpenPorchSF'] + data['ScreenPorch']","5d5e8f17":"def changing_to_bool2(column):\n    for rows in data[column]:\n        if rows == 'Ex':\n            data[column] = data[column].replace(rows, 5)\n        elif rows == 'Gd':\n            data[column] = data[column].replace(rows,4)\n        elif rows == 'TA':\n            data[column] = data[column].replace(rows,3)\n        elif rows == 'Fa':\n            data[column] = data[column].replace(rows,2)\n        elif rows == 'Po':\n            data[column] = data[column].replace(rows,1)\n        elif rows == 'None':\n            data[column] = data[column].replace(rows,0)","543a57c6":"bool_values = ('FireplaceQu', 'ExterQual', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual')","abbfb43c":"data = data.drop([ 'MiscVal', 'MiscFeature'], axis = 1 )","b31fe4d5":"set1 = data.dtypes[data.dtypes != 'object'].index\nskew_f = data[set1].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewed = pd.DataFrame({'Skew':skew_f})\nskewed","a4b39073":"skewed = skewed[abs(skewed) > 0.75]\nfrom scipy.special import boxcox1p\nskewed_features = skewed.index\nlam = 0.15\nfor feat in skewed_features:\n    data[feat] = boxcox1p(data[feat], lam)","df050d44":"mylist = list(data.select_dtypes(include=['object']).columns)","9e47539a":"dummies = pd.get_dummies(data[mylist], prefix= mylist)","5ac6f846":"data.drop(mylist, axis=1, inplace = True)","14178899":"data = pd.concat([data,dummies], axis =1 )","e24b4040":"train = data[:ntrain]\ntest = data[ntrain:]\ntrain.shape","4e41f103":"test.shape","062f5438":"train.shape","29dfffa3":"y_train.shape","6438f46f":"#Creating variables for the Independent and dependent variables \nX = train\ny = y_train","ced7ab41":"#Splitting the train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","454dc7f2":"#This cell will be refered to as \"the iterator\" if discussed in comments.\n#In this cell, various regression models are prepared to take turns being active. Three models are chosen for each run of \n#the function. During a full run, the iterator will fit and measure the mse of 99 iterations of three models. The model's \n#parameters can be set to adjust according to the iteation count, this allows a user to focus in and find the ideal setting for\n#a model or combination of models. Outputs are stored in lists and are available for other functions later.\n#You can go boil the kettle because this should take almost ten minutes.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nimport lightgbm as lgb\n\nlislas = []\noutput=[]\ntree=[]\nsubmission_options = []\naggregate = []\nfor g in range(10):\n    for l in range(10):\n        w = str(g) + str(l)\n        val = float('0.000' + w)\n        def KR():\n            KRR = KernelRidge(alpha=float('0'+'.'+str(g)+w), degree=3, coef0=2.9, kernel='polynomial')\n            KRR.fit(X_train, y_train)\n            KR = KRR.predict(X_test)\n            return (KR, KRR)\n            \n        def GBO():\n            GBoost = GradientBoostingRegressor(n_estimators=1030, learning_rate=float('0.014'+w)+0.005,\n                                   max_depth=3, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=float('0.142'),#+float('0.0'+w), \n                                   loss='huber', random_state =5)\n            GBoost.fit(X_train, y_train)\n            GBO = GBoost.predict(X_test)\n            return(GBO, GBoost)\n            \n        def LIG():\n            light = lgb.LGBMRegressor(objective='regression',num_leaves=3, \n                                      learning_rate=0.05, n_estimators=550 +int(w),\n                                      max_bin = 55, bagging_fraction = 0.8,\n                                      bagging_freq = 5, feature_fraction =0.2172,\n                                      feature_fraction_seed=9,bagging_seed=9, min_data_in_leaf =3,\n                                      min_sum_hessian_in_leaf = 11)\n            light.fit(X_train, y_train)\n            Lig = light.predict(X_test)\n            return (Lig, light)\n            \n        def ENet():\n            ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=float('0.000'+w), l1_ratio=2.222, random_state=3, warm_start=True))\n            ENet.fit(X_train, y_train)\n            EN = ENet.predict(X_test)\n            return (EN, ENet)\n        \n        def FR():\n            rf = RandomForestRegressor(n_estimators=150, criterion='mae', max_features=float('0.0'+w)+0.001, n_jobs=-1)\n            rf.fit(X_train,y_train)\n            rfr = rf.predict(X_test)\n            return (rfr, rf)\n        \n        def las():\n            las = make_pipeline(RobustScaler(), Lasso(alpha =float('0.000' + '49'), random_state=True))\n            las.fit(X_train,y_train)\n            la = las.predict(X_test)\n            return (la, las)\n\n        model_dict = {\n        'las':las(),\n        'KRR':KR(),\n        'ENet':ENet(),\n        'GB':GBO(),\n        'Lig':LIG(),\n        'FR': FR()}\n        \n        #Assign variables to chosen regression models for use in the following functions.\n        model1 = 'las'\n        reg1, md1 = model_dict[model1]\n        model2 = 'KRR'\n        reg2, md2 = model_dict[model2]\n        model3 = 'GB'\n        reg3, md3 = model_dict[model3]\n        \n        combined = []\n        for e in range(len(reg1)):\n            trio = (reg1[e] + reg2[e] + reg3[e])\/3\n            combined.append(trio) \n        \n        #Calculate individual MSEs and also MSE of the average of all three model's predictions.\n        mse_reg1 = metrics.mean_squared_error(y_test, reg1)\n        mse_reg2 = metrics.mean_squared_error(y_test, reg2)\n        mse_reg3 = metrics.mean_squared_error(y_test, reg3)\n        mse_trio = metrics.mean_squared_error(y_test, combined)\n        \n        #Details from each iteration and fitting of models is stored in a list 'lislas'\n        lislas.append((int(str(g) + str(l)), mse_reg1, mse_reg2, mse_reg3, mse_trio))\n        \n        #a seperate ist is made containing the average of all three model's prdictions for each iteration. This data can be used\n        #for analysis and tweaking of outputs later on.\n        aggregate.append(combined)\n        \n        #A tuple containing each model's prediction is crated and stored in a list with the iteration number as a form of ID\n        trioOut = ((reg1, reg2, reg3))\n        submission_options.append(trioOut)\n        \n        #Results are printe while function runs in order to provide live feedback.\n        print(w)\n        print('reg1 mse:', mse_reg1)\n        print('reg2 mse:', mse_reg2)\n        print('reg3 mse:', mse_reg3)\n        print('trio mse:', mse_trio)","1df281dd":"#BEST BLEND\n#Locates and returns the iteration where the three models stacked equally performed best.\ndef bestBlend():\n    ave_val = 99999\n    ave_pos = 0\n    for t, b, z, h, f in lislas:\n        if f < ave_val:\n            ave_val = f\n            \n            ave_pos = t\n    return(lislas[t])\n","dcafc46c":"#Here bestBlend function is used to return the best iteration for all three models stacked with eqal weighting.\nprint(bestBlend()[-1], bestBlend()[0])","eba698e1":"#BEST reg1\n#Here you can find the best prediction by reg1. This allows the user to note the iteration number and use that number to\n#fine tune any variable that might have been set to be adjusted by the iteration count ('val' in the iterator ).\ndef reg1():\n    temp = 500\n    alpha = 0\n    for t, b, z, h, f in lislas:\n        if b < temp:\n            temp = b\n            alpha = t\n    return (temp, alpha)       \nreg1()","94e79dad":"#BEST Reg2\n#Here you can find the best prediction by reg2. This allows the user to note the iteration number and use that number to\n#fine tune any variable that might have been set to be adjusted by the iteration count ('val' in the iterator ).\ndef reg2():\n    low = 99999\n    bow = 0\n    for t, b, z, h, f in lislas:\n        if z < low:\n            low = z\n            bow = t\n    return (low, bow)\nprint(reg2())","5ba2841f":"#BEST reg3\n#Here you can find the best prediction by reg1. This allows the user to note the iteration number and use that number to\n#fine tune any variable that might have been set to be adjusted by the iteration count ('val' in the iterator ).\ndef reg3():\n    tr = 100\n    tr_pos = 0\n    for t, b, z, h, f in lislas:\n        if h < tr:\n            tr = h\n            tr_pos = t\n    return (tr, tr_pos)          \nprint(reg3())    ","b43ca7b9":"def run(lis, loc, g, t, first, second):\n    #This function receives the output list from the iterator. It then uses the the parameters 'g' and 't' to\n    #create a new list that is the product of their weights according to paramters 'first' and 'second'. The parameters\n    #'first' states what percentage to take from the  list 'g' and 'second' is the percentage taken from list 't'.\n    #This function will be called three hundred times (Three permutations of picking teo lists from the three stored\n    #in 'solution_options', and this is done for each percent).\n    temp=[]\n    ng = lis[loc][g]\n    nt = lis[loc][t]\n    ng = ng * first\n    nt = nt * second\n    for i in range(len(ng)):\n        ng[i] = ng[i] + nt[i]\n    temp = [str(g) + ' AND ' + str(t),  ng, str(first) + ' x ' + str(g), str(second) + ' x ' + str(t), first, second]\n    return temp","445c5036":"def run2(lis, loc, g, t, first, second):\n    #This function receives the output list from the iterator. It then uses the the parameters 'g' and 't' to\n    #create a new list that is the product of their weights according to paramters 'first' and 'second'. The parameters\n    #'first' states what percentage to take from the  list 'g' and 'second' is the percentage taken from list 't'.\n    #This function will be called three hundred times (Three permutations of picking teo lists from the three stored\n    #in 'solution_options', and this is done for each percent).\n    a = 3-(g+t)\n    temp=[]\n    ng = lis[loc][g]\n    nt = lis[loc][t]\n    na = lis[loc][a]\n    ng = ng * first\n    nt = nt * second\n    na = na * first\n    \n    for w in range(len(nt)):\n        nt[w] = nt[w] + ng[w]\n    no = nt * second    \n\n    for i in range(len(na)):\n        na[i] = na[i] + no[i]\n\n    temp = [str(g) + ' AND ' + str(t),  na, str(first) + ' x ' + str(g), str(second) + ' x ' + str(t), first, second]\n    return temp","c895a1f7":"#This function receives the output list from the iterator. It then \ndef weight(inflow, loc, first, second, flag):\n    #This function divides the three lists in 'possible_solutions' into three permutations where each list is ppartnered with \n    #both other lists once. The function then passes 'possible_solutions' to the function 'run' with instructions on which two \n    #lists to process along with the percentage weightings for each list.\n    j=[]\n    g=[[],[],[]]\n    a = [[0, 1], [1,2], [2,0]]\n    if flag ==1:\n        for u, d in enumerate(a):\n            a,b,c,d,e,f = run(inflow, loc, d[0], d[1], first, second)\n            g[u] = [a,b,c,d,e,f]\n    if flag == 2:\n        for u, d in enumerate(a):\n            a,b,c,d,e,f = run2(inflow, loc, d[0], d[1], first, second)\n            g[u] = [a,b,c,d,e,f]\n        \n    return(g)     \n\ndef evaluateWeight(df, loc, flag):\n    #This function iterates through percentages (one to a hundred) and provides the function 'weight' with two complimentry \n    #percentages that will be used to combine the predictions in 'possible_solutions' lists in one percent increments.\n    theThree=[]\n    for q in range(0, 101):\n        q = q\/100\n        k = 1 - q\n        theThree.append(weight(df, loc, q, k, flag))\n    return (theThree)   ","774054df":"def seeker():\n    #This function goes through all three hundred combinations and weightings of predictions to find the combination that\n    #produced the best MSE. Once it has compared all entries, it outputs the iteration ID information along with some other \n    #information. The ID is used to calculate the settings that were used by 'the iterator' to achieve this MSE. These settings\n    #will later be used to fit the models in order to predict the y values for the test set. \n    \n    retainer = []\n    check1 = [bestBlend()[-1]]\n    check2 = [bestBlend()[-1]]\n    temp_mse = 500\n\n    for p in range(len(submission_options)):\n        options = evaluateWeight(submission_options, p, 1)\n        options2 = evaluateWeight(submission_options, p, 2)\n        \n        #second index: list of combination permutations with varying weights\/blends in a list.\n        for s in range(len(options)):\n            for yi in range(3):\n                cross_v = (metrics.mean_squared_error(y_test, options[s][yi][1]),1)\n      #         print(options[s][yi][1][200])\n                cross_vi = (metrics.mean_squared_error(y_test, options2[s][yi][1]),2)\n      #          print(options2[s][yi][1][200], 'vi', 'p is:', p, 'yi is', yi)\n                \n                if cross_v[0] < temp_mse:\n                    temp_mse = cross_v[0]\n                    check1.append(cross_v[0])\n                    retainer.append((options[s][yi][0], options[s][yi][2],options[s][yi][3], cross_v[0],  'ID:', p, 'Yi:', yi, cross_v[1], options[s][yi][4],options[s][yi][5]))\n                if cross_vi[0] < temp_mse:\n                    temp_mse = cross_vi[0]\n                    check2.append(cross_vi[0])\n                    retainer.append((options2[s][yi][0], options2[s][yi][2],options2[s][yi][3], cross_vi[0],  'ID:', p, 'Yi:', yi, cross_vi[1],options[s][yi][4], options[s][yi][5]))\n\n    \n    if len(retainer) == 0:\n        return bestBlend()\n    print(check1[-1], check2[-1])\n    return retainer[-1]\n\nseeker = seeker()    \n\n#A printout below is read and information used in callibrating the models used ofr the final prediction.\n#The information in the output is as follows.\n#Lists used in combination (predictions by 'reg1', 'reg2' and 'reg3' in 'the iterator')\nseeker","92490f6f":"#Parameters for refitting models to train set with new weights are assigned to variables.\nfirs = seeker[9]\nsec = seeker[10]\npermutations = [[0, 1], [1,2], [2,0]]\nmodel_dicti = {\n    1:md1,\n    2:md2,\n    3:md3}\n\n#This option is initiated if the best mse was achieved with a blend of outputs directly from the models.\nif seeker[8] == 1:\n    modelDetails = seeker()\n    models = modelDetails[0]\n    modelApercent = modelDetails[1]\n    modelBpercent = modelDetails[2]\n    \n    modelA = int(models[0])+1\n    modelB = int(models[-1])+1\n    print(modelB, 'modelB')\n    selected_A = model_dicti[modelA] \n    selected_B = model_dicti[modelB]\n    \n    #Extract percentage Float from string output provided by 'seeker' function\n    modelApe = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", modelApercent)\n    modelApercent = round(float(modelApe[0]), 3)\n    modelBpe = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", modelBpercent)\n    modelBpercent = round(float(modelBpe[0]), 3)\n\n#This option is activated if the best mse was achieved through a combination of blends of outputs from the models - still under construction.\nif seeker[8] == 2:\n    submission_options = np.array(submission_options)\n    print(type(submission_options))\n    modelB = int(models[0])+1\n    modelC = int(models[-1])+1\n    modelA = 6-(modelB+modelC)\n    if modelA == 0:\n        one = np.array(submission_options[0] * fir * sec)\n        two = np.array(submission_options[1] * sec * sec)\n        three = np.array(submission_options[2] * sec * sec)\n    if modelA == 1:\n        one = np.array(submission_options[1] * fir * sec)\n        two = np.array(submission_options[2] * sec * sec)\n        three = np.array(submission_options[2] * fir * sec)\n        four = np.array(submission_options[0] * sec * sec)\n    if modelA == 2:\n        one = np.array(submission_options[2] * fir * sec)\n        two = np.array(submission_options[0] * sec * sec)\n        three = np.array(submission_options[0] * fir * sec)\n        four = np.array(submission_options[1] * sec * sec)\n        \n    \n    modelApercent = modelDetails[1]\n    modelBpercent = modelDetails[2]\n    modelCpercent = modelDetails[1]\n    \n    #Extract percentage Float from string output provided by 'seeker' function\n    modelApe = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", modelApercent)\n    modelApercent = round(float(modelApe[0]), 3)\n    modelBpe = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", modelBpercent)\n    modelBpercent = round(float(modelBpe[0]), 3)\n    modelCpe = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", modelCpercent)\n    modelCpercent = round(float(modelBpe[0]), 3)\n    \n    \n    selected_A = model_dicti[modelA]\n    selected_B = model_dicti[permutations]\n    selected_C = model_dicti[modelC]\n    print('selected_A', selected_A, modelApercent)\n    print('selected_B', selected_B, modelBpercent)\n    print('selected_C', selected_C, modelCpercent)\n    \n","d0b13c6f":"#Fit models on new settings\nif seeker[-1] == 1:\n    model_A = selected_A.fit(X, y)\n    model_B = selected_B.fit(X, y)\n    A_toAdjust = model_A.predict(test)\n    B_toAdjust = model_B.predict(test)\n    \n    #Apply weights to each model's output and add them together to make a full complement\n    A_out = np.expm1(A_toAdjust) * 0.88\n    B_out = np.expm1(B_toAdjust) * 0.12\n    final_out = A_out + B_out\n    \nif seeker[-1] == 2:\n    print(modelApercent)\n    print(modelBpercent)\n    print(modelCpercent)\n    model_A = selected_A.fit(X, y)\n    model_B = selected_B.fit(X, y)\n    model_C = selected_C.fir(X, y)\n    A_toAdjust = model_A.predict(test)\n    B_toAdjust = model_B.predict(test)\n    C_toAdjust = model_C.predict(test)\n    \n    #Apply weights to each model's output and add them together to make a full complement\n    A_out = np.expm1(A_toAdjust) * modelApercent\n    B_out = np.expm1(B_toAdjust) * modelBpercent\n    C_out = np.expm1(C_toAdjust) * modelCpercent\n    final_out = A_out + B_out\n","d76ee61c":"pd.DataFrame({'Id': test_id, 'SalePrice': final_out}).to_csv('Yhat.csv', index =False)","eeb54cda":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download Test file\", filename = \"Yhat.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\ncreate_download_link(filename='Yhat.csv')","edcf5bf2":"pd.read_csv('Yhat.csv')","0be380da":"We can see that if theres 1 kitchen that is in Excellent condition it will increase the Sale Price more than anything else","0b738a83":"# Imputing missing data for Train ","6e02441f":"# Split test and train and implement models ","00599dce":"# Standardising all the columns in the combined dataset ","c75d4f2c":"# Getting dummies for combined dataset of test and train","44063072":"We can see here by having 2 Fire places that Ex Quality can have a massive impact on your Sale Price ","c3141e90":"# Feature engineering for the combined dataset of test and train","4b42a34b":"# Dropping columns ","1975a65b":"We can see here that the age negatively correlates with Sale Price, this means that the more older the house the less the Price will be ","5e09fb09":"# Outliers ","b9a663f0":"TotalBath is the addition of all the bathrooms in the house. It looks to be correlated to the Sale Price ","7f838fe1":"# Feature Engineering for Train data ","613cde23":"There is a correlation between TotalSF and Sale Price. TotalFinSF is a combination of TotalBsmtSF,1stFlrSF and 2ndFlrSF","c25a2fbe":"There is a correlation between TotalFinSF and Sale Price. TotalFinSF is the addition of all the areas that has a finshed Square feet and subtracting the unfinished square feet with the total finished Square feet of the entire house","73326c26":"# Time to start building our model for the predictions \n\n1. We will need to impute the missing data in both train and test\n2. We are going to take out our dependent variable\n3. Join test and train together\n3. Featuring engineering \n4. Scaling data\n5. See what results we get by exploring different models ","038fb2b0":"We can see that the more the quality of the house is the more Sale Price goes up","250bfb11":"# Data imputation for Test Data","2ff662bd":"We can see there is a correlation between OtherAreas and Saleprice. This is basically all the other areas of the house combined(not incl bathroom, bedroom and kitchen)","7b9e4511":"# EDA through graphs for the imputed train data ","8bc0d6a2":"In this Graph it shows if you have 3 car garage it will increase the Sale Price. There were probably more expensive houses that had 3 garage and there were houses that were cheaper that had 4 garages","1c74f4cd":"# Graphs showing the Sale Price Correlation with the new variables we created through Featuring Engineering","ed047b1e":"# Data imputation for train data ","cd577915":"# Introduction \n\nWe as a team have been trying to get the best results, by implementing all different things to the model that we learned over the course of this sprint. We were determined to get a better result before the due date came and we did, i guess thats just a great attribute for a data scientist to have, work hard and get better results, it wont be immediatly but with time and patience you could get somewhere.\n\nThere were alot of videos and articles we read about improving the model here are a few ways we discovered to improve our model:\n\n--The more quality data you put into your model the better your results will be \n\n--You will have to gain real insight from your data, by doing data exploration and by adding new variables derived from your indepedent variables (This is called Featuring Enginnering)\n\n-- Clean your data properly \n\n-- Blend your models to get the best results \n\nBut remeber Garbage in Garbage out. This is the rule of thumb when doing EDA and Featuring Enginnering.\n\nWe are going to predict the SalePrice for a house based on independent variables.\n\nThroughout this Notebook we will be doing statistical and visual analysis just to get an insight into the data, and to get a better understanding about the data.\n\nThis Notebook Consists out of the following:\n\n--Imputing missing data \n\n--Feature Engineering\n\n--EDA through scatter and distribution plots \n\n--Transforming Categorical variables to Numerical (Manual Encoding)\n\n--Box Cox Transformation - This will allow all our data to be evenly distributes for each varible\n\n--Dummy Variables \n\n\nIn this special case for predicing housing prices. We looked at the factors that could reduce and increase house prices and comapred it to the data we had available to us through the test and train dataframes \n\nWe realised that there was alot of outliers in this dataset, we didnt want to take all the outliers out because that would mean that we taking valuable information away. We used Regression models such as ","419a4d85":"We can assume that the Neighborhood that has the highest priced houses are NoRidge and we can also assume that BrDale has the least Priced houses  ","66a3065d":"We can see that Heating Quality with Central Air increases the houses price","82e6affd":"We can see on the graph on top that PoolQC, MiscFeature, Alley and Fence is the most data missing "}}