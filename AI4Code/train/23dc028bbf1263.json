{"cell_type":{"34b3a018":"code","db8c1d9e":"code","1f8a5584":"code","cb3a3c38":"code","00412af4":"code","f80a31da":"code","7a20b5d9":"code","e07ce18b":"code","15ad54e5":"code","18f9e9e6":"code","e5019216":"code","5ddcc953":"code","bc614b20":"code","5f9b156c":"code","4f684eae":"code","1feed65d":"code","087a1a07":"code","9b43fc76":"code","5d2ad3b2":"code","9661f956":"code","5061f732":"code","17d123ce":"code","05596242":"code","79ed8e31":"code","225775c2":"code","95ebe649":"code","29ba00af":"code","c5dfaa3e":"code","caa500cd":"code","a9577645":"code","bc3bb77b":"code","a38c9e25":"code","adbc10e5":"code","171a744b":"code","bafc1613":"code","8f6fe321":"code","bfb03b99":"code","6429846e":"code","5da4cf2e":"code","e11b065d":"code","4c423116":"code","dde1d035":"code","404d61ab":"code","c8e83d92":"code","dd4ee0e8":"code","c3cb2e4f":"code","e37cadb3":"code","c383b998":"code","e99cfd9a":"code","4921d660":"code","5f0e8399":"code","a03d633b":"code","336733ff":"code","6c863fed":"code","43e4f753":"code","7bfee7c5":"code","a1869059":"code","d79b36b7":"code","39e2467b":"code","3ba4fe01":"code","eb7fbb3b":"code","6863fac7":"code","b78b3f97":"code","4f5adc14":"code","d9da650b":"code","121b4135":"code","bdca2a51":"code","f5293b43":"code","f5b24f02":"code","e36f6b30":"code","a064c192":"code","3f320e82":"code","547b26c1":"code","93925094":"code","99f3bbb5":"code","47fc4a92":"code","376f398c":"markdown","ea0decda":"markdown","88a917be":"markdown","4c56bd42":"markdown","416cdb98":"markdown","5ded0cb3":"markdown","a5d14428":"markdown","336c4388":"markdown","bffb2a6d":"markdown","f2ac217e":"markdown","23ea732d":"markdown","57f461d3":"markdown","65b6b160":"markdown","53ac1ed4":"markdown","d949de14":"markdown","762d15b1":"markdown","31e0c9d4":"markdown","0e02fcbe":"markdown","6e03d9e5":"markdown","cdbd1f49":"markdown","55ebbb18":"markdown","2eb34548":"markdown","bd3371a1":"markdown","92c91a41":"markdown","da2c8dc4":"markdown","5cf3d8e4":"markdown","01bd7a67":"markdown","85591b28":"markdown","78ff8b28":"markdown","c0ea07c1":"markdown","7073d9c9":"markdown","e0ca18f2":"markdown","9e2b0cac":"markdown","405dd2f3":"markdown","8957bf4b":"markdown","f181c778":"markdown","bb931d92":"markdown","5abdf2bb":"markdown","635963ae":"markdown","28f19fd8":"markdown","98aab414":"markdown","107934c5":"markdown","150c0bed":"markdown","cf133d3b":"markdown","1f4d8ad8":"markdown","be26cd5a":"markdown","5814cc01":"markdown","5f2827d3":"markdown","dabe1df6":"markdown","0370931b":"markdown","e15b7de0":"markdown","befbfa78":"markdown","e57cdc28":"markdown","9ddb0214":"markdown","f0d4b6df":"markdown","dd59cbb7":"markdown","b3d7118b":"markdown","6fef97e7":"markdown","a01319ce":"markdown","5081ff91":"markdown","64f16e95":"markdown","76deb755":"markdown","fe52d955":"markdown","f3287294":"markdown","6e084a3e":"markdown","85871fad":"markdown","c64a4813":"markdown","74719027":"markdown","88217e97":"markdown","5f1711ed":"markdown","38efdb68":"markdown","b922bdbc":"markdown","74284b0e":"markdown","6e5a7987":"markdown","cd6d4e4b":"markdown","e02d3fc0":"markdown","8d46c65a":"markdown","39e16ca5":"markdown","d26001bb":"markdown","cde9fde7":"markdown","ca760ee8":"markdown","b68f3fb8":"markdown","ce71ff3b":"markdown","2896ba6b":"markdown","6a7178c0":"markdown","ccf1ac69":"markdown","76cd5667":"markdown","5bbfb910":"markdown","99ea75c8":"markdown","8dcdff93":"markdown","bd408e41":"markdown","2143c907":"markdown","cf3dc079":"markdown","64031bc8":"markdown","4bf88b3e":"markdown","566fc3fc":"markdown","2abc1e8a":"markdown","3f1492fd":"markdown","f469b7db":"markdown","4ea3d1e3":"markdown","471f97dc":"markdown","fc09109a":"markdown","61acab9c":"markdown","6fcc6d33":"markdown","431bc9b6":"markdown","12868e26":"markdown"},"source":{"34b3a018":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nsns.set(font_scale=2)","db8c1d9e":"data = pd.read_csv('..\/input\/titanic\/train.csv')","1f8a5584":"data.head()","cb3a3c38":"data.isnull().sum()","00412af4":"data.head()","f80a31da":"\n\nf, ax = plt.subplots(1,2,figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data,ax=ax[1])\nax[1].set_title('Survived')\n\nplt.show()","7a20b5d9":"data.groupby(['Sex','Survived'])['Survived'].count()","e07ce18b":"f,ax = plt.subplots(1,2,figsize=(18,8))\ndata[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","15ad54e5":"data[['Sex','Survived']].groupby(['Sex']).mean()","18f9e9e6":"pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r')","e5019216":"f , ax = plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_title('Count')\nsns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Pclass: Survived vs Dead')\nplt.show()","5ddcc953":"pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='summer_r')","bc614b20":"sns.factorplot('Pclass','Survived',hue='Sex',data=data)\n\nplt.show()","5f9b156c":"print('Oldest Passenger was of :',data['Age'].max(),'Years')\nprint('Youngest Passenger was of :',data['Age'].min(),'Years')\nprint('average Passenger was of :',data['Age'].mean(),'Years')","4f684eae":"f, ax = plt.subplots(1,2,figsize=(18,8))\nsns.violinplot('Pclass','Age',hie='Survived',data=data,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot('Sex','Age',hue='Survived',data=data,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","1feed65d":"data['initial']=0\nfor i in data:\n    data['initial']=data.Name.str.extract('([A-Za-z]+)\\.')","087a1a07":"pd.crosstab(data.initial,data.Sex).T.style.background_gradient(cmap='summer')","9b43fc76":"data['initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","5d2ad3b2":"data.groupby('initial')['Age'].mean()","9661f956":"data.loc[(data.Age.isnull())&(data.initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.initial=='Other'),'Age']=46\n","5061f732":"data.Age.isnull().any() # So no null values left finally","17d123ce":"f,ax =plt.subplots(1,2,figsize=(20,10))\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived =0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived=1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","05596242":"sns.factorplot('Pclass','Survived',col='initial',data=data)\nplt.show()","79ed8e31":"pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')\n","225775c2":"sns.factorplot('Embarked','Survived',data=data)\nfig = plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","95ebe649":"f, ax = plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=data,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","29ba00af":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)\nplt.show()","c5dfaa3e":"data['Embarked'].fillna('S',inplace=True)","caa500cd":"data.Embarked.isnull().any()","a9577645":"pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')","bc3bb77b":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('SibSp','Survived',data=data,ax=ax[0])\nax[0].set_title('SibSp vs Survived')\n\nsns.pointplot('SibSp','Survived',data=data,ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","a38c9e25":"pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r')","adbc10e5":"pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')","171a744b":"f,ax = plt.subplots(1,2,figsize=(20,8))\nsns.barplot('Parch','Survived',data=data,ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.pointplot('Parch','Survived',data=data,ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","bafc1613":"print('Heighest Fare was:',data['Fare'].max())\nprint('Lowest Fare was:',data['Fare'].min())\nprint('Average Fare was:',data['Fare'].mean())","8f6fe321":"f, ax = plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass= 2')\nsns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass= 3')","bfb03b99":"sns.set(font_scale=1.2)\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2)\nfig = plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","6429846e":"data['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[(data['Age']>64),'Age_band']=4\ndata['Age_band'].value_counts()","5da4cf2e":"\ndata['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer_r')","e11b065d":"sns.factorplot('Age_band','Survived',data=data,col='Pclass')\nplt.show()","4c423116":"data['Family_size']=0\ndata['Family_size']=data['Parch']+data['SibSp']\ndata['Alone']=0\ndata.loc[data.Family_size==0,'Alone']=1\n\nf,ax = plt.subplots(1,2,figsize=(18,6))\nsns.pointplot('Family_size','Survived',data=data,ax=ax[0])\nax[0].set_title('Family_size vs Survived')\nsns.pointplot('Alone' , 'Survived',data=data,ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.show()","dde1d035":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","404d61ab":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91,'Fare_cat']=0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat']=2\ndata.loc[(data['Fare']>31.0)&(data['Fare']<=513),'Fare_cat']=3\n","c8e83d92":"sns.factorplot('Fare_cat','Survived',data=data,hue='Sex')\nplt.show()","dd4ee0e8":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","c3cb2e4f":"data.columns","e37cadb3":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)","c383b998":"data.columns","e99cfd9a":"sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidth=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","4921d660":"#importing all the required packages.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","5f0e8399":"#data.columns[:1]","a03d633b":"train,test= train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","336733ff":"model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction1=model.predict(test_X)\nprint('Accuracy for rbf SVM is:',metrics.accuracy_score(prediction1,test_Y))","6c863fed":"model = svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))","43e4f753":"model = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction3=model.predict(test_X)\nprint('The accuracy of the Logstic Regression is:',metrics.accuracy_score(prediction3,test_Y))","7bfee7c5":"model = DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is:',metrics.accuracy_score(prediction4,test_Y))","a1869059":"from sklearn.neighbors import KNeighborsClassifier","d79b36b7":"model = KNeighborsClassifier()\nmodel.fit(train_X,train_Y)\nprediction5 = model.predict(test_X)\nprint('The accuracy of the K-Nearest Neighbours is',metrics.accuracy_score(prediction5,test_Y))","39e2467b":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i)\n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    a = a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\nplt.plot(a_index,a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracy for different values of n are:',a.values,'with the max value as',a.values.max())","3ba4fe01":"model = GaussianNB()\nmodel.fit(train_X,train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))","eb7fbb3b":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is:',metrics.accuracy_score(prediction7,test_Y))","6863fac7":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nkfold = KFold(n_splits=10, random_state=22,shuffle=True)\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear SVM','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),\n       DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\n\nfor i in models:\n    model = i\n    cv_result=cross_val_score(model,X,Y,cv=kfold,scoring='accuracy')\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)\n\nnew_models_dataframe2","b78b3f97":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()","4f5adc14":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","d9da650b":"f,ax =plt.subplots(3,3,figsize=(12,10))\ny_pred=cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\n\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\n\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\n\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\n\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\n\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\n\ny_pred= cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","121b4135":"from sklearn.model_selection import GridSearchCV\nC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","bdca2a51":"n_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","f5293b43":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                             \n                                            ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ],\n                                 voting='soft').fit(train_X,train_Y)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\ncross=cross_val_score(ensemble_lin_rbf,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score is',cross.mean())","f5b24f02":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\nresult = cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","e36f6b30":"model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',result.mean())","a064c192":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult = cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","3f320e82":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","547b26c1":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The crooss validated score for XGBoost is:',result.mean())","93925094":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","99f3bbb5":"ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nresult=cross_val_predict(ada,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","47fc4a92":"f,ax =plt.subplots(2,2,figsize=(15,12))\nmodel = RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\n\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature importance in Random Forests')\nmodel= AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,Y)\n\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\n\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\n\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feautre importance in XGBoost')\nplt.show()","376f398c":"## Logstic Regression","ea0decda":"* First let us undeerstand the difference types of features","88a917be":"\nSo what **qcut** does is it splits or arranges the values according the number of bins we have passed. \n<br\/>So if we pass for 5bins, it will arrange the values equally spaced into 5 seperate bins or value ranges.","4c56bd42":"The chances for survival for Port C is highest around 0.55 while it is lower for S","416cdb98":"The machine learning models are like a Black-Box There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is knows as Hyper-Parameter Tuning","5ded0cb3":"Do you think we should use both of them as **One of them is redundant** while making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.","a5d14428":"* **Continus Feautre:**\n","336c4388":"## **Fare -> Continuous Feature**","bffb2a6d":"## SVM","f2ac217e":"### **Changes for Survival by Port Of Embarkation**","23ea732d":"## Feature Importance","57f461d3":"### **Age_band**","65b6b160":"* For Pclass 1 %Survived is around 63% while for Pclass 2 is around **48%**. So money and status matters. Such a materialstic World.","53ac1ed4":"* we use **Factorplot** in this case , because they make separation of categorical values easy.\n* Looking at the **Crosstab** and the Factorplot, we can easily infer that survival for **Women from Pclass** is about **95-96**%, as only 3 out of 94 Women from Pclass 1died.\n* It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass 1 have a very low survival rate\n* Looks like Pclass is also an important feature. Lets analyse other feature","d949de14":"**Observations:**\n\n1) The Toddlers(age<5) were savved in large numbers(The Women and Child First Policy).\n\n2) The oldest Passengers was saved (80years).\n\n3) Maximum number of deaths were in the age group of 30-40.","762d15b1":"* Let's check for other intersting observations. Lets check survival rate with **Secx and Pclass** Together","31e0c9d4":"### **Observations**\nThe barplot and factorplot shows that if passengers is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decrease if the number of siblings increase. This make sense. That is, if i habe a family on board, I will try to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is **0%**. The reason may be Pclass??\n\nThe reason is **Pclass**. The crosstab shows that Person with SibSp>3 were all in Pclass3. It is imminent that all the large families in Pclass3(>3) died.","0e02fcbe":"### **Parch**","6e03d9e5":"## Voting Classifier\nIt is simplest way of combining predictions from many different simple machine learning models.\nIt gives an average prediction result bassd on the prediction of all the submodels. The submodels or the basemodels are all of different types.","cdbd1f49":"# Age -> Continous Feature","55ebbb18":"## Types Of Features","2eb34548":"\n- An ordinal variable is similar to categorical values, but the difference between them is that we can have relatvie ordering or sorting between the values.","bd3371a1":"**Dropping UnNeeded Feature**\n**Name** -> We don't need name feature as it cannot be converted into any categorical value.\n\n**Age** -> We have the Age_band feature, so no need of this.\n\n**Ticket** -> It is any random string that cannot be categorised.\n\n**Fare** -> same as the Age.\n\n**Cabin** -> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\n**Fare_range** -> we have the fare_cat feature.\n\n**PassengerId** -> Cannot be categorised.","92c91a41":"The looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. As this also continuous, we can convert into discrete values by using bining.","da2c8dc4":"## Random Forests","5cf3d8e4":"* It is evident that not many passengers survived the accident","01bd7a67":"## Gaussian Naive Bayes","85591b28":"- If we have feature like height which values tall, medium,short,them hieght is a ordinal variable.","78ff8b28":"* \uacf5\uc9dc\ub85c \ud0c4 \uc0ac\ub78c\uc774 \uc788\ub2e4..!(The lowest fare is **0!** )","c0ea07c1":"### **Filling Embarked NaN**\nAs we saw that maximum passengers boarded from Port S, we replace Nan with S.","7073d9c9":"The accuracy of a model is not the only that determines the robustness of the classifer.\n<br\/>Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%\n<br\/>\n<br\/>\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that com over??. The answer is **No**, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increases of decreases. This is knows as **model variance**.\n<br\/>\nTo overcome this and get a generalized model, we use **Cross Validation**.","e0ca18f2":"## Bagging\nBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging, there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.","9e2b0cac":"Since fare is also a continuous feature, we need to convert it into ordinal value. For this we will use **Pandas.qcut.**","405dd2f3":"* The **Age,Cabin,Embarked** have null values, I will try to fix them.","8957bf4b":"## K-Nearest Neighbours(KNN)","f181c778":"## Radial Support Vector Machine(rbf-SVM)","bb931d92":"## **Hyper-Parameters Tuning**","5abdf2bb":"* **Family_size**=0 means the passenger is alone. Clearly if you are alone or family_size=0, then<br\/> chances for survival is very low. For family size >4, the chances decrease too. This also looks to be an important feature for the model.","635963ae":"## Bagged DecisionTree\n","28f19fd8":"* \uc694\uae08\uc744 \ub9ce\uc774 \ub0bc\uc218\ub85d \uc0dd\uc874\ub960\uc774 \uc62c\ub77c\uac00\uace0 \uc788\uc74c\uc744 \ud655\uc778 \ud560 \uc218 \uc788\ub2e4.","98aab414":"\n### Hyper-Parameter Tuning for AdaBoost","107934c5":"* Ordinal features","150c0bed":"**Age is continuous feature**, there is a problem with Continuous Variables in Machine Learning Models.\n\n**Eg:** if i say to group or arrange Sports Person by **Sex**,we can easily segregate them by Malde and Female.\n\nNow if i say to group them by their **Age**, then how would you do it? If there are 30Persons, there may be 30 age values, Now this is problatic.\n\nwe need to convert these **Continuous values into categorical values** by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.\n\nOkay, so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins.","cf133d3b":"### **SibSip -> Discrete Feature**\nThis feature represents whether a person is alone or with his family members.\n\nSibling -> brother,sister,stepbrother,stepsister\n\nSpouse = husband,wife\n**","1f4d8ad8":"The left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong predicions made. Lets consider the first plot for rbf-SVM:\n\n1)The no. of correct predictions are **491(for dead) + 247(for survived)** with the mean CV accuracy being **(491+247)\/891 = 82.8%** which we did get earlier.\n\n2)**Errors** -> Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived.","be26cd5a":"## **Pclass -> Ordienal Feature**","5814cc01":"### **Famliy_size and Alone**","5f2827d3":"## **Interpreting Confusion Matrix**","dabe1df6":"## Random Forests","0370931b":"## Linear Support Vector Machine(linear-SVM)","e15b7de0":"* The looks interesting. The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twich the number of males saved. The survival rates for a women on the ship is around 75% while that for men is around 18-19%","befbfa78":"Now from the above heatmap, we can see that the features are not much correlated. The highest correlation is between **SibSp and Parch 0.41** So we can carry on with all features.","e57cdc28":"* **Categorical Features**\n* **A categorical variable is one that has two or more categiries and each value in taht feature can be categorized by them.**","9ddb0214":"* THE best score for Rbf-Svm is **82.82% with C=0.4 and gamma=0.3** For RandomForest,score is abt **81.9% with n_estimators=300**.","f0d4b6df":"## Interesting The Heatmap\nTHe first thing to note is only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.","dd59cbb7":"At this point , we can create a new feature called **Family_size** and \"Alone\" and analyse it.  This <br\/>\nfeature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not","b3d7118b":"### AdaBoost(Adaptive Boosting)\nThe weak learner or estimator in this case is a Decision Tree. But we can change the default base_estimator to any algorithm of our choice.","6fef97e7":"* A feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column","a01319ce":"### Confusion Matrix for the Best Model","5081ff91":"## Part 3: Predictive Modeling\n\nwe have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a paasenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classiification Algorithms. Follwing are the algorithms I will use to make the model:\n\n1)Logistic Regression\n\n2)Support Vector Machines(Linear and radial)\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n\n7)Logistic Regression","64f16e95":"* \uc21c\uc11c\uac00 \uc5c6\ub294 cateogical data\ub294 \ub77c\ubca8 \uc778\ucf54\ub529\uc744 \ud55c \ud6c4 \n<br\/>one hot encoding,frequency encoding, mean encoding \uc911 \ud558\ub098\ub97c \uace0\ub978\ub2e4.\n\n* \uc21c\uc11c\uac00 \uc788\ub2e4\uba74 \ub77c\ubca8\uc778\ucf54\ub529 \uc2dc\uc791","76deb755":"### **How many Survived?**","fe52d955":"### **Embarked -> Categorical Value**","f3287294":"* We can see the important features for various classifiers like RandomForests, AdaBoost.etc.","6e084a3e":"## Bagged KNN\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of **n_neighbors**, as small value of n_neighbors.","85871fad":"As we had seen earlier, the Age feature has **177** null values. To replace these NAN values, we can assign them the mean age of the dataset.\n\nBut the problem is, there were many people with many different ages. We just cant assign a 4year kid with the mean age that is 29years. Is there any way to find out what age-band does the passenger lie?","c64a4813":"\uac00\uc7a5 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ubcf4\uc778 AdaBoost\ub294 **82.93% with n_estimaotrs=100 and learning_rate=0.1**","74719027":"## Boosting\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model. Boosting works as follows:\n\nA model is first trained on the complete dataset. Now the model will get some instances right while some worng.\nNow in the next iteration, the learner will focus more on the wrongly predicted\ninstances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continuous, and new classifiers are added to the model until the limit is reached on the accuracy.","88217e97":"I hope all of you did gain some insights to Machine Learning. Some other great notebooks for Machine Learning:are \n\n1) For R: Divide and Conquer by Oscar Takeshita\n\n2) For Python:Pytanic by Heads and Tails\n\n3) For Python:Introduction to Ensembling\/Stacking by Anisotropic\n\n","5f1711ed":"The classification accuracy can be sometimes misleading due to imbalence. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.","38efdb68":"## **Observations**\n\n**\uc131\ubcc4**: \uc5ec\uc790\uac00 \ub0a8\uc790\ub791 \ube44\uad50 \ud588\uc744 \ub54c \uc0dd\uc874\ub960\uc774 \ub354 \ub192\uc558\uc74c\n\n**\ud0d1\uc2b9\ub4f1\uae09** : 1st \ud074\ub798\uc2a4 \uc2b9\uac1d\ub4e4\uc774 \uc0dd\uc874\ub960\uc774 \ub354 \ub178\ud320\uace0 3st \ud074\ub798\uc2a4 \uc2b9\uac1d\ub4e4\uc740 \ub0ae\uc558\uc74c. \uc5ec\uc131\uc758 \uacbd\uc6b0 1st \ud074\ub798\uc2a4\uc758 \uc0dd\uc874\ub960\uc774 \uac70\uc758 1\uc5d0 \uac00\uae4c\uc6b0\uba70 2st \ud074\ub798\uc2a4\uc758 \uc5ec\uc131 \uc5ed\uc2dc \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc600\ub2e4.\n\n**\ub098\uc774**: 5-10\uc0b4\uc758 \uc544\uc774\ub4e4\uc774 \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc784. 15~35\uc0b4\uc758 \uc0ac\ub78c\ub4e4\uc774 \ub9ce\uc774 \uc0ac\ub9dd\ud588\uc74c.\n\n**\ud0d1\uc2b9\ud56d\uad6c** : 1st\uc758 \uace0\uac1c\ub4e4\uc774 \ub300\ubd80\ubd84 S\uc5d0 \uc788\uc5c8\uc9c0\ub9cc , C\uac00 \uc0dd\uc874\ub960\uc774 \uac00\uc7a5 \ub192\uc740\uac83\uc744 \uc54c \uc218 \uc788\uc74c Q \ud56d\uad6c\uc758 \uc2b9\uac1d\uc740 3st\uc758 \uc2b9\uac1d\ub4e4\uc784.\n\n**\ud615\uc81c,\uc790\ub9e4** : 1-2 \ud639\uc740 1-3\uba85\uc758 \ud615\uc81c,\uc790\ub9e4 \uadf8\ub9ac\uace0 \ubc30\uc6b0\uc790\ub85c \uc774\ub8e8\uc5b4\uc9c4 \uc2b9\uac1d\ub4e4\uc774 \ud63c\uc790\uc774\uac70\ub098 \ub300\uac00\uc871\ub4e4\ubcf4\ub2e4 \ub192\uc740 \uc0dd\uc874\ub960\uc744 \ubcf4\uc600\uc74c.","b922bdbc":"* True that.. the survival rate decreases as the age increases irrsepctive of the Pclass","74284b0e":"* \ud06c\ub85c\uc2a4\ud0ed\uc744 \ubcf4\uba74 \ub9ce\uc740 \uc218\uc758 \uac00\uc871\ub4e4\uc774 3\ub4f1\uc11d\uc5d0 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4.","6e5a7987":"* Now we cannot sort or give any ordering to such variables.","cd6d4e4b":"### **Observations:**\n\nHere too the results are quite similar. Passengers with their parents onboard have greater chance of survival.\nIt however reduces as the number goes up.\n\nThe chances of survival is godd for somebody who has 1-3 parents on the ship. Being alon also proves to be fatal and the chances for survival decreases when somebody has>4 parents on the ship.","e02d3fc0":"* We will try to check the survival rate by using the difference features of the dataset. Some of the features being Sex, Port Of Embarcation, Age.etc","8d46c65a":"* AdaBoost\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ubcf4\uc600\ub530. \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\ub3c4 \uc2dc\ub3c4\ud574\ubcf4\uaca0\ub2e4.","39e16ca5":"1) Some of the common important features are initial,Fare_cat,Pclass,Family_Size.\n\n2) The Sex feature doesn't seem to give any importance, which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating facotr. Sex looks to be important only in RandomForests.\n\nHowever, we can see the feature Initial, which is at the top in many classifiers. We had already seen the positive correlation between Sex and Initial, so they both refer to the gender.\n\n3) Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone,Parch and SibSp.","d26001bb":"## Cross Validation","cde9fde7":"## **Converting String Valeus into Numeric**\nSince we cannot pass strings to a machine learning model, we need to convert features loke Sex, Embarked, etc into numeric values.","ca760ee8":"* For example , gender is a categorical variable having two categories (male and female)","b68f3fb8":"### **Observations:**\n1) The survival chances are almost 1 for women Pclass1 and Pclass2 irrseppective of the Pclass\n\n2) Port S looks to be very unlucky for Pclass3 Passengers as the survival rate for both men and women is very low **(Money Matters)**\n\n3) Port Q looks to b e unlukiest for Men , as almost all were from Pclass3","ce71ff3b":"* Clearly, as the Fare_cat increases, the survival chances increases, This feature may become an important feature during modeling along with the Sex.\n","2896ba6b":"## Decision Tree","6a7178c0":"* Pclass \uac19\uc740 \uacbd\uc6b0\ub294 \uc21c\uc11c\uac00 \uc788\ub294 Ordinal \ub370\uc774\ud130\uc774\ub2e4 \uadf8\ub7ec\ubbc0\ub85c \uc6d0\ud56b\uc778\ucf54\ub529\ubcf4\ub2e4\ub294 \ub77c\ubca8\uc778\ucf54\ub529\uc744 \ud574\uc11c \uc21c\uc11c\ub77c\ub294 \uc815\ubcf4\uac00 \uc0b4\uc544\ub098\uac8c \ud55c\ub2e4.","ccf1ac69":"* \ube48 null age \ub370\uc774\ud130\ub97c \uc704 \ub098\uc774\ub85c replace","76cd5667":"* Now the above correlation plot , we can see some positively related features, Some of them being **SibSp and Family_size** and **Parch and Family_size** and some negative ones like **Alone and Family_size**","5bbfb910":"We will tune the hyper-parameters for the 2best classifiers i.e the SVM and RandomForests.","99ea75c8":"## Ensembling\nEnsembling is a good way to increase the accuracy of performance of a model In simple words, it is the combination of various simple moldes to create a single powerful model.\n\nLets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is **Ensembling**. which improves the stability of the model. Ensembling can b e done in ways like:\n\n1)Voting Classifier\n\n2)Bagging\n\n3)Boosting.","8dcdff93":"**Observations:**\n\n1) Maxmimum Passnegers boarded from S. Majority of them being from Pclass3.\n\n2) The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.\n\n3) The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around **81%** didn't survive.\n\n4) Port Q had almost 95% of the passengers were from Pclass3","bd408e41":"### Stohastic Gradient Boosting\nHere too the weak learner is a Decision Tree","2143c907":"Many a times, the data is imbalenced, i.e there may be a high number of class 1 instances but less number of other class instacnes. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset.\n\n<br\/>\n1)The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\n\n2)Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.\n\n3)We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\n\nThis is called K-Fold Validation.\n\n4)An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.","cf3dc079":"## Confusion Matrix for the Best Model","64031bc8":"### **Part2:Feature Engineering and Data Cleaning**\n\nWhenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\n\nAn example would be getting the Initials feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will transform the existing relevant features to suitable form for Predictive Modeling","4bf88b3e":"**Observation**\n\n1)The number of children increases with Pclass and the survival rate for passengers below Age 10 looks to be good irrespective of the Pclass\n\n2)Survival Chanes for Passengers aged 20-50 from Pclass1 is high and is even better for Women\n\n3)For males, the survival chances decreases with an increase in age.","566fc3fc":"Now the accuracy for the KNN model changes as we change the values for **n_neighbors** attribute. The default values is **5**. Lets check the accracies over various values of n_neighbors.","2abc1e8a":"* A\uc870 : \ud55c\uad6d \ub3c5\uc77c \uc2a4\uc6e8\ub374 \uba55\uc2dc\ucf54\n* B\uc870 : \uc2a4\ud398\uc778 \ube0c\ub77c\uc9c8 \ub374\ub9c8\ud06c \uc789\uae00\ub79c\ub4dc\n* C\uc870 : \uc77c\ubcf8 \uc774\ub780 \ub124\ub35c\ub780\ub4dc \ud504\ub791\uc2a4","3f1492fd":"* Even though the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around 25%","f469b7db":"### XGBoost","4ea3d1e3":"## **Correlation Between The Features**","471f97dc":"**The Women and Child first policy thus holds true irrespective of the class**","fc09109a":"## Part1 : Exploratory Data Analysis(EDA)\n","61acab9c":"* People say **Money Can't Buy EveryThing** But we can clearly see that Passengers Of Pclass 1 were given a very gigh priority while rescue.","6fcc6d33":"## Observations","431bc9b6":"* Name Feature Check","12868e26":"* Out of 891 passsengers in training set, only around 350 survived i.e Only **38.4%** of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't"}}