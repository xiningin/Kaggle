{"cell_type":{"2e69e241":"code","bb268070":"code","2ae1d853":"code","80868bb0":"code","479ea67b":"code","50e3e986":"code","b9ba6555":"code","2d552d00":"code","c32abbc6":"code","c1f30075":"code","6861b8cd":"code","77f147e1":"code","4be20932":"code","fcfb783f":"code","e72a40ea":"code","d107f9f5":"code","1491a43e":"code","e49601eb":"code","a4dc9dba":"code","0d3d5206":"code","2839a1e3":"markdown"},"source":{"2e69e241":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb268070":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","2ae1d853":"train.head()","80868bb0":"train.columns","479ea67b":"test.columns","50e3e986":"y = train['Survived']\nX = train.drop(['Survived'], axis=1)","b9ba6555":"eda = pd.concat([X, test])\nlen(eda)","2d552d00":"eda.nunique(axis=0, dropna=True)","c32abbc6":"for column in eda.columns:\n    print('{} has {} nulls'.format(column, eda[column].isna().sum()))","c1f30075":"X = X.drop(['Name', 'Cabin'], axis=1)","6861b8cd":"X['Sex'] = LabelEncoder().fit_transform(X['Sex'])\nX['Embarked'] = X['Embarked'].fillna('S')\nX['Embarked'] = LabelEncoder().fit_transform(X['Embarked'])\nX['Ticket'] = LabelEncoder().fit_transform(X['Ticket'])\nX.fillna(0, inplace=True)","77f147e1":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8)","4be20932":"rfc = RandomForestClassifier()\nparams = {'n_estimators':[10, 25, 50, 75, 100], 'max_depth':[4, 5, 6, 7, 8]}\nmodel1 = RandomizedSearchCV(rfc, params).fit(X_train, y_train)\nmodel1.best_params_, model1.score(X_valid, y_valid)","fcfb783f":"rbf = SVC()\nparams = {'C':[0.01, 0.1, 10, 100, 1000], 'gamma':[0.01, 0.1, 10, 100, 1000]}\nmodel2 = RandomizedSearchCV(rbf, params).fit(X_train, y_train)\nmodel2.best_params_, model2.score(X_valid, y_valid)","e72a40ea":"xgb = XGBClassifier(n_jobs=-1)\nparams = {'learning_rate':[0.1, 0.2, 0.08, 0.05], 'max_depth':[3, 4, 5, 6, 7], 'n_estimators':[100, 125, 150, 175]}\nmodel3 = RandomizedSearchCV(xgb, params).fit(X_train, y_train)\nmodel3.best_params_, model3.score(X_valid, y_valid)","d107f9f5":"test = test.drop(['Name', 'Cabin'], axis=1)\ntest['Sex'] = LabelEncoder().fit_transform(test['Sex'])\ntest['Embarked'] = X['Embarked'].fillna('S')\ntest['Embarked'] = LabelEncoder().fit_transform(test['Embarked'])\ntest['Ticket'] = LabelEncoder().fit_transform(test['Ticket'])\ntest.fillna(0, inplace=True)","1491a43e":"test","e49601eb":"prediction = model1.predict(test)","a4dc9dba":"len(prediction), len(test)","0d3d5206":"df = pd.DataFrame()\ndf['PassengerId'] = test['PassengerId']\ndf['Survived'] = prediction\ndf.to_csv('submit.csv', index=False)","2839a1e3":"Perform some exploratory data analysis by concatenating the train and test data."}}