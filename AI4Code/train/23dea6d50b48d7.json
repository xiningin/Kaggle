{"cell_type":{"bb097c0e":"code","e17b772a":"code","bf9189c9":"code","8ae69224":"code","a08976a6":"code","9e32d7fb":"code","73d0468a":"code","38bea6c0":"code","fe61438c":"code","221e949e":"code","3791c0f0":"code","f2db9787":"code","d2dafb38":"code","f8763e78":"code","93828540":"code","e2bdbd61":"code","5093a761":"code","7f4afdca":"code","3537abad":"code","8bc77a8d":"code","634a9950":"code","fd4b0d90":"code","f8e2c77a":"code","08c3a7bd":"code","0ddb035c":"code","fbf47d43":"code","98b711e8":"code","580f4db5":"code","21fb5940":"code","c89f393e":"code","26f740e0":"code","e5f05f6c":"code","c0827f01":"code","137cad32":"code","4246cb17":"code","f0e4e09b":"code","60a3388e":"code","561b331a":"code","4fb6ee21":"code","2b07e243":"code","5effe393":"code","ca626063":"code","3175d8c3":"code","5446476b":"code","5ddd546c":"code","36dbd1bf":"code","f1f0a631":"code","22b275d4":"code","36fc8568":"code","ec2e0754":"code","092292f7":"code","50e06b6a":"code","a0546801":"code","c96ebe71":"code","3f7dfe48":"code","09f6f777":"code","f67a6614":"code","02c5bb8b":"code","00ceb337":"code","cde83b8c":"code","5bca8708":"code","47233dd2":"code","a208ae77":"code","df00cdac":"code","282ae6fb":"code","8823b332":"code","99312e08":"code","d026cd91":"code","20d1f1eb":"code","b99e4d52":"code","0ee6411f":"code","ff631cf2":"code","a72e5fa6":"code","b869727f":"code","95cabbb4":"code","37a5f301":"code","5e33b237":"code","88e45d36":"code","16ab1948":"code","254932db":"code","d2b35e0d":"code","1221164d":"code","f29587b7":"code","e53782c6":"code","693aea3b":"code","a64c83e7":"code","b781558f":"code","a53ae14c":"code","778ccab4":"code","ddcfa7fa":"code","9d745063":"code","41a714a9":"code","c602ef75":"code","3c3ed72b":"code","a141c996":"code","2e5c0365":"code","a2abf3ee":"code","e7eac7fc":"code","f3594e07":"code","4215165b":"code","5bb40971":"code","7b8519ca":"code","99c83369":"code","6d8258a3":"code","3ab6802c":"code","3b74dddd":"code","c9233aa7":"code","a63e9ccb":"code","beafc131":"code","3a127484":"code","1acbd371":"code","daec9670":"code","15dc79f1":"code","beefa6c1":"code","0d92dee2":"code","da6f309c":"code","ab0da26b":"code","21f89761":"code","450fc7f5":"code","a23f4ce9":"code","790cb3f1":"code","97bca267":"code","b2817eba":"code","58149f46":"code","6bc2c777":"code","2478eeff":"code","bc9f7c49":"code","9bc5d7cd":"code","b4f1efa0":"code","639801d1":"code","5744879f":"code","deee4919":"code","9fe107f7":"code","cd271ba6":"code","7df74863":"code","0a240ca7":"code","055bd774":"code","fa2adf43":"code","92fe0f02":"code","e0622207":"code","3acf82cd":"markdown","60aebfa3":"markdown","77181f6a":"markdown","4ae3c1b9":"markdown","b0536c42":"markdown","c5ca2ea9":"markdown","08b61d60":"markdown","b55d8477":"markdown","448a092f":"markdown","83e614e7":"markdown","28e96219":"markdown","3876f871":"markdown","b98f9444":"markdown","b00b4e60":"markdown","63fb4909":"markdown"},"source":{"bb097c0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e17b772a":"from sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import norm\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(15,12)})\nimport matplotlib.pyplot as plt\n%matplotlib inline","bf9189c9":"import numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport pandas_profiling\nimport scipy.stats as stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rc(\"font\", size=14)\nplt.rcParams['axes.grid'] = True\nplt.figure(figsize=(6,3))\nplt.gray()\n\nfrom matplotlib.backends.backend_pdf import PdfPages\n\nimport statsmodels.formula.api as sm\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrices","8ae69224":"import logging\nfrom numbers import Integral\n\nimport numpy as np\n\nfrom matplotlib import rcParams\nfrom matplotlib import backends, docstring, projections\nfrom matplotlib import __version__ as _mpl_version\nfrom matplotlib import get_backend\n\nimport matplotlib.artist as martist\nfrom matplotlib.artist import Artist, allow_rasterization\nfrom matplotlib.backend_bases import FigureCanvasBase\nimport matplotlib.cbook as cbook\nimport matplotlib.colorbar as cbar\nimport matplotlib.image as mimage\n\nfrom matplotlib.axes import Axes, SubplotBase, subplot_class_factory\nfrom matplotlib.blocking_input import BlockingMouseInput, BlockingKeyMouseInput\nfrom matplotlib.gridspec import GridSpec\nimport matplotlib.legend as mlegend\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.projections import (get_projection_names,\n                                    process_projection_requirements)\nfrom matplotlib.text import Text, TextWithDash\nfrom matplotlib.transforms import (Affine2D, Bbox, BboxTransformTo,\n                                   TransformedBbox)\nimport matplotlib._layoutbox as layoutbox\nfrom matplotlib.backend_bases import NonGuiException","a08976a6":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns","9e32d7fb":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # visualization\nfrom scipy import stats\nfrom scipy.stats import norm \nimport warnings \nwarnings.filterwarnings('ignore') #ignore warnings\n%matplotlib inline\nimport gc\nimport datetime as dt","73d0468a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport sklearn.metrics as metrics\nimport statsmodels.formula.api as sm","38bea6c0":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport os","fe61438c":"titanic_train = pd.read_csv('..\/input\/titanic-dataset-csv\/titanic_train.csv')\ntitanic_train.head()","221e949e":"titanic_train = titanic_train.drop(['name', 'ticket', 'cabin', 'home.dest'],axis = 1)\ntitanic_train","3791c0f0":"titanic_test = pd.read_csv('..\/input\/titanic-dataset-csv\/titanic_test.csv')\ntitanic_test.head()","f2db9787":"titanic_train['survived'] = titanic_train.survived.astype(str)","d2dafb38":"titanic_train.head()","f8763e78":"titanic_train.tail()","93828540":"titanic_test.head()","e2bdbd61":"titanic_test.tail()","5093a761":"titanic_train.describe()","7f4afdca":"titanic_train.info()","3537abad":"titanic_test.describe()","8bc77a8d":"titanic_test.info()","634a9950":"titanic_train.columns","fd4b0d90":"len(titanic_train.columns)","f8e2c77a":"titanic_train.dtypes","08c3a7bd":"titanic_train.isnull()","0ddb035c":"titanic_train.shape","fbf47d43":"# Plotting correlation between all important features\ncorr = titanic_train.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr, annot=True)\nplt.plot()","98b711e8":"titanic_train['sex'] = pd.factorize(titanic_train.sex)[0]\ntitanic_train","580f4db5":"titanic_train['embarked'] = pd.factorize(titanic_train.embarked)[0]\ntitanic_train","21fb5940":"numeric_var_names=[key for key in dict(titanic_train.dtypes) if dict(titanic_train.dtypes)[key] in ['float64', 'int64', 'float32', 'int32','uint8']]\ncat_var_names=[key for key in dict(titanic_train.dtypes) if dict(titanic_train.dtypes)[key] in ['object']]\nprint(numeric_var_names)\nprint(cat_var_names)","c89f393e":"numeric_var_names","26f740e0":"cat_var_names","e5f05f6c":"titanic_train_num = titanic_train[numeric_var_names]\ntitanic_train_num","c0827f01":"titanic_train_cat = titanic_train[cat_var_names]\ntitanic_train_cat","137cad32":"# Use a general function that returns multiple values\ndef var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n\nnum_summary = titanic_train_num.apply(lambda x: var_summary(x)).T","4246cb17":"num_summary","f0e4e09b":"import numpy as np\nfor col in titanic_train_num.columns:\n    percentiles = titanic_train_num[col].quantile([0.01,0.99]).values\n    titanic_train_num[col] = np.clip(titanic_train_num[col], percentiles[0], percentiles[1])","60a3388e":" titanic_train_num","561b331a":"def Missing_imputation(x):\n    x = x.fillna(x.median())\n    return x","4fb6ee21":"titanic_train_num = titanic_train_num.apply(lambda x: Missing_imputation(x))\ntitanic_train_num","2b07e243":"def Cat_Missing_imputation(x):\n    x = x.fillna(x.mode())\n    return x","5effe393":"titanic_train_cat = titanic_train_cat.apply(lambda x:  Cat_Missing_imputation(x))\ntitanic_train_cat","ca626063":"titanic_train_new = pd.concat([titanic_train_num, titanic_train_cat], axis=1)\ntitanic_train_new","3175d8c3":"pandas_profiling.ProfileReport(titanic_train_new)","5446476b":"titanic_train_new.head()","5ddd546c":"titanic_train_new.tail()","36dbd1bf":"titanic_train_new.describe().T.head(10)","f1f0a631":"titanic_train_num.fare.hist()","22b275d4":"titanic_train_num.sex.hist()","36fc8568":"titanic_train_num.embarked.hist()","ec2e0754":"titanic_train_num.age.hist()","092292f7":"titanic_train_num.body.hist()","50e06b6a":"titanic_train_num.pclass.hist()","a0546801":"# An utility function to create dummy variable\ndef create_dummies( titanic_train_new, colname ):\n    col_dummies = pd.get_dummies(titanic_train_new[colname], prefix=colname, drop_first=True)\n    #col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n    titanic_train_new = pd.concat([titanic_train_new, col_dummies], axis=1)\n    titanic_train_new.drop( colname, axis = 1, inplace = True )\n    return titanic_train_new","c96ebe71":"cat_var_names","3f7dfe48":"#for c_feature in categorical_features\ntitanic_train_cat_new = titanic_train_cat\nfor c_feature in cat_var_names:\n    titanic_train_cat_new[c_feature] = titanic_train_cat_new[c_feature].astype('category')\n    titanic_train_cat_new = create_dummies(titanic_train_cat_new , c_feature )","09f6f777":"titanic_train_cat_new","f67a6614":"titanic_train_new = pd.concat([titanic_train_num, titanic_train_cat_new], axis=1)\ntitanic_train_new","02c5bb8b":"titanic_train_new.head()","00ceb337":"titanic_train_new.tail()","cde83b8c":"titanic_train_new.columns","5bca8708":"bp = PdfPages('WOE Plots.pdf')\n\nfor num_variable in titanic_train_new.columns.difference(['survived_1']):\n    binned = pd.cut(titanic_train_new[num_variable], bins=10, labels=list(range(1,11)))\n    #binned = binned.dropna()\n    odds = titanic_train_new.groupby(binned)['survived_1'].sum() \/ (titanic_train_new.groupby(binned)['survived_1'].count()-titanic_train_new.groupby(binned)['survived_1'].sum())\n    log_odds = np.log(odds)\n    fig,axes = plt.subplots(figsize=(10,4))\n    sns.barplot(x=log_odds.index,y=log_odds)\n    plt.ylabel('Log Odds Ratio')\n    plt.title(str('Logit Plot for identifying if the bucketing is required or not for variable ') + str(num_variable))\n    bp.savefig(fig)\n\nbp.close()","47233dd2":"titanic_train_new.columns","a208ae77":"import statsmodels.formula.api as sm","df00cdac":"logreg_model = sm.logit('survived_1~passenger_id',data = titanic_train_new).fit()","282ae6fb":"logreg_model = sm.logit('survived_1~pclass',data = titanic_train_new).fit()","8823b332":"logreg_model = sm.logit('survived_1~sex',data = titanic_train_new).fit()","99312e08":"logreg_model = sm.logit('survived_1~age',data = titanic_train_new).fit()","d026cd91":"logreg_model = sm.logit('survived_1~sibsp',data = titanic_train_new).fit()","20d1f1eb":"logreg_model = sm.logit('survived_1~parch',data = titanic_train_new).fit()","b99e4d52":"logreg_model = sm.logit('survived_1~fare',data = titanic_train_new).fit()","0ee6411f":"logreg_model = sm.logit('survived_1~embarked',data = titanic_train_new).fit()","ff631cf2":"logreg_model = sm.logit('survived_1~body',data = titanic_train_new).fit()","a72e5fa6":"logreg_model = sm.logit('survived_1~boat_10',data = titanic_train_new).fit()","b869727f":"logreg_model = sm.logit('survived_1~boat_11',data = titanic_train_new).fit()","95cabbb4":"logreg_model = sm.logit('survived_1~boat_12',data = titanic_train_new).fit()","37a5f301":"logreg_model = sm.logit('survived_1~boat_14',data = titanic_train_new).fit()","5e33b237":"logreg_model = sm.logit('survived_1~boat_15',data = titanic_train_new).fit()","88e45d36":"logreg_model = sm.logit('survived_1~boat_16',data = titanic_train_new).fit()","16ab1948":"logreg_model = sm.logit('survived_1~boat_2',data = titanic_train_new).fit()","254932db":"logreg_model = sm.logit('survived_1~boat_3',data = titanic_train_new).fit()","d2b35e0d":"logreg_model = sm.logit('survived_1~boat_4',data = titanic_train_new).fit()","1221164d":"logreg_model = sm.logit('survived_1~boat_5',data = titanic_train_new).fit()","f29587b7":"logreg_model = sm.logit('survived_1~boat_6',data = titanic_train_new).fit()","e53782c6":"logreg_model = sm.logit('survived_1~boat_7',data = titanic_train_new).fit()","693aea3b":"logreg_model = sm.logit('survived_1~boat_8',data = titanic_train_new).fit()","a64c83e7":"logreg_model = sm.logit('survived_1~boat_9',data = titanic_train_new).fit()","b781558f":"logreg_model = sm.logit('survived_1~boat_A',data = titanic_train_new).fit()","a53ae14c":"logreg_model = sm.logit('survived_1~boat_B',data = titanic_train_new).fit()","778ccab4":"logreg_model = sm.logit('survived_1~boat_C',data = titanic_train_new).fit()","ddcfa7fa":"logreg_model = sm.logit('survived_1~boat_D',data = titanic_train_new).fit()","9d745063":"p = logreg_model.predict(titanic_train_new)\np","41a714a9":"metrics.roc_auc_score(titanic_train_new['survived_1'],p)","c602ef75":"2*metrics.roc_auc_score(titanic_train_new['survived_1'],p)-1","3c3ed72b":"titanic_train_new1 = titanic_train_new.loc[:,[\"passenger_id\", \"pclass\", \"sex\", \"age\",\"sibsp\", \"parch\", \"fare\",\"embarked\", \"body\", \"boat_10\", \"boat_11\",\"boat_12\", \"boat_14\", \"boat_15\", \"boat_16\", \"boat_2\", \"boat_3\", \"boat_4\", \"boat_5\", \"boat_6\", \"boat_7\", \"boat_8\", \"boat_9\", \"boat_A\", \"boat_B\",\"boat_C\", \"boat_D\", \"survived_1\"]]\ntitanic_train_new1","a141c996":"somersd_df = pd.DataFrame()\nfor num_variable in titanic_train_new1.columns.difference(['survived_1']):\n    logreg_model = sm.logit(formula = str('survived_1 ~ ')+str(num_variable), data=titanic_train_new1)\n    result = logreg_model.fit()\n    #result = logit.fit(method='bfgs')\n    y1_score = pd.DataFrame(result.predict())\n    y1_score.columns = ['Score']\n    somers_d = 2*metrics.roc_auc_score(titanic_train_new1['survived_1'],y1_score) - 1\n    temp = pd.DataFrame([num_variable,somers_d]).T\n    temp.columns = ['Variable Name', 'SomersD']\n    somersd_df = pd.concat([somersd_df, temp], axis=0)\nsomersd_df","2e5c0365":"somersd_df.sort_values('SomersD', ascending=False, inplace=True)","a2abf3ee":"somersd_df","e7eac7fc":"titanic_train_new2 = titanic_train_new.loc[:,[\"passenger_id\", \"pclass\", \"sex\", \"age\",\"sibsp\", \"parch\", \"fare\",\"embarked\", \"body\", \"boat_10\", \"boat_11\",\"boat_12\", \"boat_14\", \"boat_15\", \"boat_16\", \"boat_2\", \"boat_3\", \"boat_4\", \"boat_5\", \"boat_6\", \"boat_7\", \"boat_8\", \"boat_9\", \"boat_A\", \"boat_B\",\"boat_C\", \"boat_D\", \"survived_1\"]]\ntitanic_train_new2\n","f3594e07":"from sklearn import datasets\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nX = titanic_train_new2[titanic_train_new2.columns.difference(['survived_1'])]\nlogreg = LogisticRegression()\nrfe = RFE(logreg, 15)\nrfe = rfe.fit(X, titanic_train_new2[['survived_1']] )\n\nprint(rfe.support_)\nprint(rfe.ranking_)","4215165b":"X.columns","5bb40971":"# summarize the selection of the attributes\nimport itertools\nfeature_map = [(i, v) for i, v in itertools.zip_longest(X.columns, rfe.get_support())]\n\nfeature_map\n\n#Alternative of capturing the important variables\nRFE_features=X.columns[rfe.get_support()]\n\nselected_features_from_rfe = X[RFE_features]","7b8519ca":"RFE_features","99c83369":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif","6d8258a3":"X = titanic_train_new2[titanic_train_new2.columns.difference(['survived_1'])]\nX_new = SelectKBest(f_classif, k=15).fit(X, titanic_train_new2[['survived_1']] )","3ab6802c":"X_new.get_support()","3b74dddd":"X_new.scores_","c9233aa7":"# summarize the selection of the attributes\nimport itertools\nfeature_map = [(i, v) for i, v in itertools.zip_longest(X.columns, X_new.get_support())]\n\nfeature_map\n\n#Alternative of capturing the important variables\nKBest_features=X.columns[X_new.get_support()]\n\nselected_features_from_KBest = X[KBest_features]","a63e9ccb":"KBest_features","beafc131":"X = pd.concat([titanic_train_new2[titanic_train_new2.columns.difference(['survived_1'])],titanic_train_new2['survived_1']], axis=1)\nfeatures = \"+\".join(titanic_train_new2.columns.difference(['survived_1']))\nX.head()","3a127484":"features","1acbd371":"a,b = dmatrices(formula_like='survived_1 ~ '+ 'age+boat_10+boat_11+boat_12+boat_14+boat_15+boat_16+boat_2+boat_3+boat_4+boat_5+boat_6+boat_7+boat_8+boat_9+boat_A+boat_B+boat_C+boat_D+body+embarked+fare+parch+pclass+sex+sibsp', data = titanic_train_new2, return_type='dataframe')\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(b.values, i) for i in range(b.shape[1])]\nvif[\"features\"] = b.columns\n\nprint(vif)","daec9670":"#for logistic regression using statsmodels\ntrain1, test1 = train_test_split(titanic_train_new2, test_size=0.3, random_state=0)","15dc79f1":"train1","beefa6c1":"import statsmodels.formula.api as sm\nimport sklearn.metrics as metrics","0d92dee2":"logreg = sm.logit(formula='survived_1 ~ age+boat_10+boat_11+boat_12+boat_14+boat_15+boat_16+boat_2+boat_3+boat_4+boat_5+boat_6+boat_7+boat_8+boat_9+boat_A+boat_B+boat_C+body+embarked+fare+parch+pclass+sex', data = train1)\nresult = logreg.fit()","da6f309c":"print(result.summary())","ab0da26b":"train_gini = 2*metrics.roc_auc_score(train1['survived_1'], result.predict(train1)) - 1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test1['survived_1'], result.predict(test1)) - 1\nprint(\"The Gini Index for the model built on the Test Data is : \", test_gini)\n\ntrain_auc = metrics.roc_auc_score(train1['survived_1'], result.predict(train1))\ntest_auc = metrics.roc_auc_score(test1['survived_1'], result.predict(test1))\n\nprint(\"The AUC for the model built on the Train Data is : \", train_auc)\nprint(\"The AUC for the model built on the Test Data is : \", test_auc)\n                                 ","21f89761":"## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's\ntrain_predicted_prob = pd.DataFrame(result.predict(train1))\ntrain_predicted_prob.columns = ['prob']\ntrain_actual = train1['survived_1']\n# making a DataFrame with actual and prob columns\ntrain_predict = pd.concat([train_actual, train_predicted_prob], axis=1)\ntrain_predict.columns = ['actual','prob']\ntrain_predict.head()","450fc7f5":"## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's\ntest_predicted_prob = pd.DataFrame(result.predict(test1))\ntest_predicted_prob.columns = ['prob']\ntest_actual = test1['survived_1']\n# making a DataFrame with actual and prob columns\ntest_predict = pd.concat([test_actual, test_predicted_prob], axis=1)\ntest_predict.columns = ['actual','prob']\ntest_predict.head()","a23f4ce9":"## Intuition behind ROC curve - confusion matrix for each different cut-off shows trade off in sensitivity and specificity\nroc_like_df = pd.DataFrame()\ntrain_temp = train_predict.copy()\n\nfor cut_off in np.linspace(0,1,50):\n    train_temp['cut_off'] = cut_off\n    train_temp['predicted'] = train_temp['prob'].apply(lambda x: 0.0 if x < cut_off else 1.0)\n    train_temp['tp'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['fp'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['tn'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==0 else 0.0, axis=1)\n    train_temp['fn'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==0 else 0.0, axis=1)\n    sensitivity = train_temp['tp'].sum() \/ (train_temp['tp'].sum() + train_temp['fn'].sum())\n    specificity = train_temp['tn'].sum() \/ (train_temp['tn'].sum() + train_temp['fp'].sum())\n    accuracy = (train_temp['tp'].sum()  + train_temp['tn'].sum() ) \/ (train_temp['tp'].sum() + train_temp['fn'].sum() + train_temp['tn'].sum() + train_temp['fp'].sum())\n    roc_like_table = pd.DataFrame([cut_off, sensitivity, specificity, accuracy]).T\n    roc_like_table.columns = ['cutoff', 'sensitivity', 'specificity', 'accuracy']\n    roc_like_df = pd.concat([roc_like_df, roc_like_table], axis=0)","790cb3f1":"roc_like_df","97bca267":"## Finding ideal cut-off for checking if this remains same in OOS validation\nroc_like_df['total'] = roc_like_df['sensitivity'] + roc_like_df['specificity']","b2817eba":"roc_like_df.head()","58149f46":"#Cut-off based on highest sum(sensitivity+specicity)   - common way of identifying cut-off\nroc1=roc_like_df[roc_like_df['total']==roc_like_df['total'].max()]\nroc1","6bc2c777":"#Cut-off based on highest accuracy   - some teams use this as methodology to decide the cut-off\nroc2=roc_like_df[roc_like_df['accuracy']==roc_like_df['accuracy'].max()]\nroc2","2478eeff":"#Cut-off based on highest sensitivity\nroc3=roc_like_df[roc_like_df['sensitivity']==roc_like_df['sensitivity'].max()]\nroc3","bc9f7c49":"#Choosen Best Cut-off is 0.367347 based on highest (sensitivity+specicity)\n\ntest_predict['predicted'] = test_predict['prob'].apply(lambda x: 1 if x > 0.367347 else 0)\ntrain_predict['predicted'] = train_predict['prob'].apply(lambda x: 1 if x > 0.367347 else 0)","9bc5d7cd":"test_predict.head()","b4f1efa0":"train_predict.head()","639801d1":"sns.heatmap(pd.crosstab(train_predict['actual'], train_predict['predicted']), annot=True, fmt='.0f')\nplt.title('Train Data Confusion Matrix')\nplt.show()\nsns.heatmap(pd.crosstab(test_predict['actual'], test_predict['predicted']), annot=True, fmt='.0f')\nplt.title('Test Data Confusion Matrix')\nplt.show()","5744879f":"print(\"The overall accuracy score for the Train Data is : \", metrics.accuracy_score(train_predict.actual, train_predict.predicted))\nprint(\"The overall accuracy score for the Test Data  is : \", metrics.accuracy_score(test_predict.actual, test_predict.predicted))","deee4919":"print(metrics.classification_report(train_predict.actual, train_predict.predicted))","9fe107f7":"print(metrics.classification_report(test_predict.actual, test_predict.predicted))","cd271ba6":"train_predict['Deciles']=pd.qcut(train_predict['prob'],10, labels=False)\n\ntrain_predict.head()","7df74863":"test_predict['Deciles']=pd.qcut(test_predict['prob'],10, labels=False)\n\ntest_predict.head()","0a240ca7":"# Decile Analysis for train data\n\nno_1s = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).sum().sort_index(ascending=False)['actual']\nno_total = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).count().sort_index(ascending=False)['actual']\nmax_prob = train_predict[['Deciles','prob']].groupby(train_predict.Deciles).max().sort_index(ascending=False)['prob']\nmin_prob = train_predict[['Deciles','prob']].groupby(train_predict.Deciles).min().sort_index(ascending=False)['prob']","055bd774":"Decile_analysis_train1 = pd.concat([max_prob, min_prob, no_1s, no_total-no_1s, no_total], axis=1)\n\nDecile_analysis_train1.columns = ['max_prob','min_prob','#1','#0','total']","fa2adf43":"Decile_analysis_train1","92fe0f02":"# Decile Analysis for train data\n\nno_1s = test_predict[['Deciles','actual']].groupby(test_predict.Deciles).sum().sort_index(ascending=False)['actual']\nno_total = test_predict[['Deciles','actual']].groupby(test_predict.Deciles).count().sort_index(ascending=False)['actual']\nmax_prob = test_predict[['Deciles','prob']].groupby(test_predict.Deciles).max().sort_index(ascending=False)['prob']\nmin_prob = test_predict[['Deciles','prob']].groupby(test_predict.Deciles).min().sort_index(ascending=False)['prob']\n\nDecile_analysis_test1 = pd.concat([max_prob, min_prob, no_1s, no_total-no_1s, no_total], axis=1)\n\nDecile_analysis_test1.columns = ['max_prob','min_prob','#1','#0','total']","e0622207":"Decile_analysis_test1","3acf82cd":"Model Building","60aebfa3":"Handling missings - Method2","77181f6a":"Variable reduction using Select K-Best technique","4ae3c1b9":"IMPORTING DATASET IN CSV FILES\u00b6","b0536c42":"IMPORTING IMPORTANT LIBRARIES","c5ca2ea9":"Variance Inflation Factor assessment","08b61d60":"Handling missings - Method2","b55d8477":"Accuracy Metrics","448a092f":"Variable Reduction using univariate Regression (short list based on Somer's D values)","83e614e7":"Handling Outliers - Method2","28e96219":"Thanks for reading the notebook\n\n**More to come. Stay tuned!**","3876f871":"Variable reduction using WOE or log(odds)\n\n\nIdentify important variables using WOE or log(odds) comparing with Y Variable Transformation: (i) Bucketing if the variables are not having linear relationship with log(odds)","b98f9444":"Variable Reduction using Recursive Feature Elimination\n\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.\n\n","b00b4e60":"**Please upvote if you like this notebook**","63fb4909":"creating data auditing report"}}