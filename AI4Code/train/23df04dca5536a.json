{"cell_type":{"e5f7f168":"code","393d96ed":"code","bcd8cb40":"code","e2dd365e":"code","270a5621":"code","5a46bf73":"code","b578d523":"code","3018b74a":"code","a1b03683":"code","374fd381":"code","0b7e3771":"code","d0f9b03f":"code","519b5575":"markdown","3ce80a5e":"markdown","2bde8acb":"markdown","1406da5f":"markdown","27e97d0b":"markdown","a64f2a62":"markdown","94a541b3":"markdown","ab207534":"markdown","1115a40b":"markdown","7c23391a":"markdown","b75f7ffc":"markdown"},"source":{"e5f7f168":"import json\nimport math\nimport os\nfrom pprint import pprint\n\nimport numpy as np\nimport tensorflow as tf\nprint(tf.version.VERSION)","393d96ed":"N_POINTS = 100\nX = tf.constant(range(N_POINTS), dtype=tf.float32)\nY = 2 * X + 100","bcd8cb40":"def create_dataset(X,Y,epochs,batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((X,Y))\n    dataset = dataset.repeat(epochs).batch(batch_size,drop_remainder=True)\n    return dataset","e2dd365e":"BATCH=32\nEPOCH = 10\n\ndataset = create_dataset(X,Y,EPOCH,BATCH)\nfor i, (x, y) in enumerate(dataset):\n    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n    assert len(x) == BATCH\n    assert len(y) == BATCH\nassert  EPOCH","270a5621":"\ndef loss_mse(X, Y, w0, w1):\n    Y_hat = w0 * X + w1\n    errors = (Y_hat - Y)**2\n    return tf.reduce_mean(errors)\n\n\ndef compute_gradients(X, Y, w0, w1):\n    with tf.GradientTape() as tape:\n        loss = loss_mse(X, Y, w0, w1)\n    return tape.gradient(loss, [w0, w1])","5a46bf73":"EPOCHS = 250\nBATCH_SIZE = 32\nLEARNING_RATE = .02\nscale =100\n\nMSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n\nw0 = tf.Variable(0.0)\nw1 = tf.Variable(0.0)\n\ndataset = create_dataset(X\/scale, Y\/scale, epochs=EPOCHS, batch_size=BATCH_SIZE)\n\nfor step, (X_batch, Y_batch) in enumerate(dataset):\n\n    dw0, dw1 = compute_gradients(X_batch, Y_batch, w0, w1)\n    w0.assign_sub(dw0 * LEARNING_RATE)\n    w1.assign_sub(dw1 * LEARNING_RATE)\n\n    if step % 100 == 0:\n        loss = loss_mse(X_batch, Y_batch, w0, w1)\n        print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n        \nassert loss < 1\nassert abs(w0 - 2) < 1\nassert abs(w1 - 10) < 10","b578d523":"!ls -l ..\/data\/taxi*.csv","3018b74a":"CSV_COLUMNS = [\n    'fare_amount',\n    'pickup_datetime',\n    'pickup_longitude',\n    'pickup_latitude',\n    'dropoff_longitude',\n    'dropoff_latitude',\n    'passenger_count',\n    'key'\n]\nLABEL_COLUMN = 'fare_amount'\nDEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]","a1b03683":"def create_dataset(pattern):\n    return tf.data.experimental.make_csv_dataset(\n        pattern, 1, CSV_COLUMNS, DEFAULTS)\n\n\ntempds = create_dataset('..\/data\/taxi-train*')\nprint(tempds)","374fd381":"\nfor data in tempds.take(2):\n    pprint({k: v.numpy() for k, v in data.items()})\n    print(\"\\n\")","0b7e3771":"UNWANTED_COLS = ['pickup_datetime', 'key']\n\n# TODO 4a\ndef features_and_labels(row_data):\n    label = row_data.pop(LABEL_COLUMN)\n    features = row_data\n    \n    for unwanted_col in UNWANTED_COLS:\n        features.pop(unwanted_col)\n\n    return features, label","d0f9b03f":"for row_data in tempds.take(2):\n    features, label = features_and_labels(row_data)\n    pprint(features)\n    print(label, \"\\n\")\n    assert UNWANTED_COLS[0] not in features.keys()\n    assert UNWANTED_COLS[1] not in features.keys()\n    assert label.shape == [1]","519b5575":"Let's iterate over the two first element of this dataset using dataset.take(2) and let's convert them ordinary Python dictionary with numpy array as values for more readability:","3ce80a5e":"In this notebook, we will start by refactoring the linear regression we implemented in the previous lab so that it takes data from atf.data.Dataset, and we will learn how to implement stochastic gradient descent with it. In this case, the original dataset will be synthetic and read by the tf.data API directly from memory.","2bde8acb":"# VTraining loop\nThe main difference now is that now, in the traning loop, we will iterate directly on the tf.data.Dataset generated by our create_dataset function.\n\nWe will configure the dataset so that it iterates 250 times over our synthetic dataset in batches of 32.","1406da5f":"# Transforming the features\nWhat we really need is a dictionary of features + a label. So, we have to do two things to the above dictionary:\n\nRemove the unwanted column \"key\"\nKeep the label separate from the features\nLet's first implement a function that takes as input a row (represented as an OrderedDict in our tf.data.Dataset as above) and then returns a tuple with two elements:\n\nThe first element being the same OrderedDict with the label dropped\nThe second element being the label itself (fare_amount)\nNote that we will need to also remove the key and pickup_datetime column, which we won't use.","27e97d0b":"{'dropoff_latitude': array([40.787454], dtype=float32),\n'dropoff_longitude': array([-73.95557], dtype=float32),\n'fare_amount': array([9.], dtype=float32),\n'key': array([b'2536'], dtype=object),\n'passenger_count': array([1.], dtype=float32),\n'pickup_datetime': array([b'2012-09-20 18:29:35 UTC'], dtype=object),\n'pickup_latitude': array([40.762016], dtype=float32),\n'pickup_longitude': array([-73.97203], dtype=float32)}\n\n{'dropoff_latitude': array([40.777454], dtype=float32),\n'dropoff_longitude': array([-73.95057], dtype=float32),\n'fare_amount': array([9.], dtype=float32),\n'key': array([b'2536'], dtype=object),\n'passenger_count': array([1.], dtype=float32),\n'pickup_datetime': array([b'2012-09-20 18:29:35 UTC'], dtype=object),\n'pickup_latitude': array([40.762016], dtype=float32),\n'pickup_longitude': array([-73.97203], dtype=float32)}","a64f2a62":"# TensorFlow Dataset API\n### Learning Objectives\n\n* Learn how to use tf.data to read data from memory\n* Learn how to use tf.data in a training loop\n* Learn how to use tf.data to read data from disk\n* Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)","94a541b3":"We begin with implementing a function that takes as input\n\n* our $X$ and $Y$ vectors of synthetic data generated by the linear function $y= 2x + 100$\n* the number of passes over the dataset we want to train on (epochs)\n* the size of the batches the dataset (batch_size)","ab207534":"# Loading data from disk\nLocating the CSV files\nWe will start with the taxifare dataset CSV files that we wrote out in a previous lab.\n\nThe taxifare dataset files have been saved into ..\/data.\n\nCheck that it is the case in the cell below, and, if not, regenerate the taxifare","1115a40b":"## Loss function and gradients\nThe loss function and the function that computes the gradients are the same as before:","7c23391a":"# Loading data from memory\n## Creating the dataset","b75f7ffc":"## Use tf.data to read the CSV files\n## The tf.data API can easily read csv files using the helper function tf.data.experimental.make_csv_dataset\n\nIf you have TFRecords (which is recommended), you may use tf.data.experimental.make_batched_features_dataset\n\nThe first step is to define\n\nthe feature names into a list CSV_COLUMNS\ntheir default values into a list DEFAULTS"}}