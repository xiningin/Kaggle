{"cell_type":{"3e79479c":"code","da7c15f6":"code","42ae5362":"code","db4dd68f":"code","7618e27a":"code","88a651af":"code","429759a5":"code","39eb0c2e":"code","3f45ebad":"code","e1bb5d37":"code","f088a80a":"code","fd3aa4c4":"code","7532be83":"code","41cc778f":"code","60a66143":"code","3544f8d6":"code","4ad4b0b3":"markdown","5d7d749c":"markdown","48524983":"markdown","879f7342":"markdown","2626e880":"markdown","44025476":"markdown","f21d84b5":"markdown","a28191a2":"markdown","f3f7e283":"markdown"},"source":{"3e79479c":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder\n\nfrom tqdm import tqdm\n\nsns.set_style('whitegrid')","da7c15f6":"train_df = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntargets_df = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n\ncat_features = [x for x in list(train_df.columns) if 'cp_' in x]\ngene_features = [x for x in list(train_df.columns) if 'g-' in x]\ncell_features = [x for x in list(train_df.columns) if 'c-' in x]\n\nfeatures = cat_features + gene_features + cell_features ","42ae5362":"train_df = train_df[train_df['cp_type'] != 0]\ntargets_df = targets_df.loc[train_df.index]\n\ntrain_df = train_df.reset_index(drop=True)\ntargets_df = targets_df.reset_index(drop=True)","db4dd68f":"means = train_df[gene_features + cell_features].mean(axis=0)\n\n\nplt.figure(figsize=(7, 5))\nsns.kdeplot(means, shade=True)\n\nplt.title('Distribution of column means', fontsize=20, fontweight=\"bold\");","7618e27a":"for col in tqdm(gene_features + cell_features):\n    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n    \n    vec_len = len(train_df[col])\n    \n    raw_vec = train_df[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]","88a651af":"means = train_df[gene_features + cell_features].mean(axis=0)\n\n\nplt.figure(figsize=(7, 5))\nsns.kdeplot(means, shade=True)\n\nplt.title('Distribution of column means', fontsize=20, fontweight=\"bold\");","429759a5":"for feat in cat_features:\n    le = LabelEncoder()\n    train_df[feat] = le.fit_transform(train_df[feat])","39eb0c2e":"# HELPER FUNCTIONS FOR CROSS_VALIDATION\n\n#######################################\n############# IGNORE ##################\n#######################################\n\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport os, re, random, gc\nfrom tqdm import tqdm\nfrom glob import glob\n\nfrom datetime import datetime\nimport time\n\nfrom sklearn.metrics import log_loss\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#######################################\n############# CONFIG ##################\n#######################################\n\nclass config:\n    \n    ###############\n    # Training\n    ###############\n    \n    num_folds = 5\n    \n    num_workers = 8\n    batch_size = 128\n    num_epochs = 30\n    \n    ###############\n    # LR scheduling\n    ###############\n    \n    step_scheduler = True\n    lr = 1e-4\n    \n    ###############\n    # Miscellaneous\n    ###############\n    \n    seed = 2020\n    verbose = False\n    verbose_step = 5\n        \n#######################################\n############# UTILS ###################\n#######################################\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(config.seed)\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        \n#######################################\n############# DATASET #################\n#######################################\n        \nclass MoADataset(Dataset):\n    def __init__(self, features, targets=None, train=True):\n        super().__init__()\n        self.features = features\n        self.train = train\n        \n        if self.train:\n            self.targets = targets\n                \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):\n        feats = self.features[item, :].astype(np.float32)\n        \n        if self.train: \n            \n            targets = self.targets[item, :].astype(np.float32) \n            \n            return {\n                'features': torch.tensor(feats, dtype=torch.float),\n                'targets': torch.tensor(targets, dtype=torch.float),\n            }\n        else: \n            return {'features': torch.tensor(feats, dtype=torch.float)}\n\n#######################################\n############# MODEL ###################\n#######################################\n\nclass BaselineModel(nn.Module):\n    def __init__(self, num_features):\n        super(BaselineModel, self).__init__()\n        \n        self.num_features = num_features\n        \n        self.block1 = nn.Sequential(\n            nn.BatchNorm1d(self.num_features),\n            nn.Dropout(0.2),\n            nn.utils.weight_norm(nn.Linear(self.num_features, 2048)),\n            nn.ReLU(),\n        )\n        \n        self.block2 = nn.Sequential(\n            nn.BatchNorm1d(2048),\n            nn.Dropout(0.5),\n            nn.utils.weight_norm(nn.Linear(2048, 1024)),\n            nn.ReLU(),\n        )\n        \n        self.block3 = nn.Sequential(\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.5),\n            nn.utils.weight_norm(nn.Linear(1024, 206)),\n        )\n    \n    def forward(self,\n                inputs):\n        \n        \n        x = self.block1(inputs)\n        x = self.block2(x)\n            \n        return self.block3(x)\n\n#######################################\n############# FITTER ##################\n#######################################\n    \nclass Fitter:\n    def __init__(self, model, seed, fold, device, config):\n        self.config = config\n        self.model = model\n        self.seed = seed\n        self.device = device\n        self.fold = fold\n                        \n        self.epoch = 0\n        \n        self.history = {\n            'train_history_loss': [],\n            'val_history_loss': [],\n        }\n        \n        self.base_dir = '.\/'\n        self.log_path = f'{self.base_dir}\/log.txt'\n        \n        self.best_loss = float('inf')\n        \n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            weight_decay=1e-5\n        )\n        \n        self.scheduler = lr_scheduler.ReduceLROnPlateau(\n            self.optimizer,\n            mode='min',\n            factor=0.1,\n            patience=3,\n            eps=1e-4,\n            verbose=False\n        )\n        \n        self.criterion = nn.BCEWithLogitsLoss().to(self.device)\n        self.log(f'Fitter prepared. Training on {self.device}')\n    \n    def fit(self, train_loader, valid_loader):\n        \n        for epoch in range(self.config.num_epochs):\n            \n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}\\n')\n            \n            t = time.time()\n            train_loss = self.train_one_epoch(train_loader)\n            self.history['train_history_loss'].append(train_loss.avg)\n            \n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, ' + \\\n                     f'loss: {train_loss.avg:.5f}, ' + \\\n                     f'time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}\/last-checkpoint.bin')\n            \n            t = time.time()\n            val_loss, y_oof = self.validation_one_epoch(valid_loader)\n            self.history['val_history_loss'].append(val_loss.avg)\n            \n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, ' + \\\n                     f'val_loss: {val_loss.avg:.5f}, ' + \\\n                     f'time: {(time.time() - t):.5f}')\n            \n            self.scheduler.step(val_loss.avg)\n            \n            if val_loss.avg < self.best_loss:\n                self.best_loss = val_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}\/best-loss-fold-{str(self.fold)}-seed-{str(self.seed)}.bin')\n            \n            self.epoch += 1 \n        \n        return y_oof\n    \n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        \n        loss_score = AverageMeter()\n        \n        t = time.time()\n        \n        for step, data in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}\/{len(train_loader)}, ' + \\\n                        f'loss: {loss_score.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            features = data['features']\n            targets = data['targets']\n            \n            features = features.to(self.device)\n            targets = targets.to(self.device).float()\n                \n            batch_size = features.shape[0]\n            \n            for p in self.model.parameters(): p.grad = None\n                \n            outputs = self.model(\n                features\n            )\n                \n            loss = self.criterion(outputs, targets)\n            loss.backward()\n                \n            loss_score.update(\n                loss.detach().item(), \n                batch_size\n            )\n                \n            self.optimizer.step()\n        \n        return loss_score\n\n    def validation_one_epoch(self, valid_loader):\n        self.model.eval()\n        \n        preds = []\n        \n        loss_score = AverageMeter()\n        \n        t = time.time()\n        \n        for step, data in enumerate(valid_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}\/{len(valid_loader)}, ' + \\\n                        f'loss: {loss_score.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            features = data['features']\n            targets = data['targets']\n            \n            features = features.to(self.device)\n            targets = targets.to(self.device).float()\n            \n            batch_size = features.shape[0]\n            \n            with torch.no_grad():\n                outputs = self.model(\n                    features\n                )\n                loss = self.criterion(outputs, targets)\n                loss_score.update(loss.detach().item(), batch_size)\n                \n                preds.append(\n                    torch.sigmoid(outputs).detach().cpu().numpy()\n                )\n        \n        return loss_score, np.concatenate(preds)\n    \n    def save(self, path):\n        self.model.eval()\n        \n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_loss': self.best_loss,\n            'epoch': self.epoch,\n            'history': self.history,\n        }, path)\n    \n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        self.history = checkpoint['history']\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')\n                \n    def print_history(self):\n        plt.figure(figsize=(15,5))\n        \n        plt.plot(\n            np.arange(self.config.num_epochs),\n            self.history['train_history_loss'],\n            '-o',\n            label='Train loss',\n            color='#ff7f0e'\n        )\n        \n        plt.plot(\n            np.arange(self.config.num_epochs),\n            self.history['val_history_loss'],\n            '-o',\n            label='Val loss',\n            color='#1f77b4'\n        )\n        \n        x = np.argmin(self.history['val_history_loss'])\n        y = np.min(self.history['val_history_loss'])\n        \n        plt.ylim(0, 0.03)\n        \n        xdist = plt.xlim()[1] - plt.xlim()[0]\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        \n        plt.scatter(x, y, s=200, color='#1f77b4')\n        \n        plt.text(\n            x-0.03*xdist,\n            y-0.13*ydist,\n            'min loss\\n%.5f'%y,\n            size=14\n        )\n        \n        plt.ylabel('Loss', size=14)\n        plt.xlabel('Epoch', size=14)\n        \n        plt.legend(loc=2)\n        \n        plt.title(f'FOLD {self.fold + 1}',size=18)\n        \n        plt.legend(loc=3)\n        plt.show()  \n\n#######################################\n############# ENGINE ##################\n#######################################\n\ndef cross_validate_strategy(X, y):\n\n    oof_preds = np.zeros((len(train_df), 206))\n\n    device = torch.device(\n        'cuda' if torch.cuda.is_available() else 'cpu'\n    )\n\n    kfold = MultilabelStratifiedKFold(config.num_folds, shuffle=True, random_state=config.seed)\n\n    for fold, (trn_, val_) in tqdm(enumerate(kfold.split(X, y)), total=config.num_folds):\n\n        # Model\n        model = BaselineModel(X.shape[1]).to(device)\n\n        # Data\n        X_train = X[trn_, :]\n        X_valid = X[val_, :]\n\n        y_train = y[trn_, :]\n        y_valid = y[val_, :]\n\n        # Dataset\n        train_dataset = MoADataset(X_train, y_train)\n        valid_dataset = MoADataset(X_valid, y_valid)\n\n        # Dataloader\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=config.batch_size,\n            pin_memory=True,\n            drop_last=True,\n            shuffle=True,\n            num_workers=config.num_workers\n        )\n\n        valid_loader = torch.utils.data.DataLoader(\n            valid_dataset,\n            batch_size=config.batch_size,\n            num_workers=config.num_workers,\n            shuffle=False,\n            pin_memory=True,\n            drop_last=False,\n        )\n\n        # Fitter\n        fitter = Fitter(model, config.seed, fold, device, config)\n\n        y_oof = fitter.fit(train_loader, valid_loader) \n        oof_preds[val_, :] = y_oof\n    \n    target_cols = list(targets_df.columns)\n    target_cols.remove('sig_id')\n    \n    oof_score = 0\n    y_true = targets_df[target_cols].values\n\n    for i in range(oof_preds.shape[1]):\n        _score = log_loss(y_true[:,i], oof_preds[:,i])\n        oof_score += _score \/ y_true.shape[1]\n    \n    return oof_score","3f45ebad":"%%time\n\nX_gene_cell = train_df[gene_features + cell_features].values\ny = targets_df.iloc[:, 1:].values\n\npca = PCA(n_components=436)\nX_pca = pca.fit_transform(X_gene_cell)\n\nX = np.concatenate((train_df[cat_features], X_pca), axis=1)","e1bb5d37":"plt.figure(figsize=(7, 5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.title('Explained variance ratio', fontsize=20, fontweight=\"bold\");","f088a80a":"scores = []\nn_comps = [x for x in range(100, 450, 50)]\n\nfor n_comp in n_comps:\n    pca = PCA(n_components=n_comp)\n    \n    X_pca = pca.fit_transform(X_gene_cell)\n    X = np.concatenate((train_df[cat_features], X_pca), axis=1)\n    \n    score = cross_validate_strategy(X, y)\n    scores.append(score)","fd3aa4c4":"fig = plt.figure(figsize=(10, 6))\nplt.plot(n_comps, scores)\nplt.xlabel('Number of principal components')\nplt.ylabel('Cross validation score')\nplt.title('CV score vs n_components', fontsize=20, fontweight=\"bold\");","7532be83":"%%time\n\nX_gene = train_df[gene_features].values\nX_cell = train_df[cell_features].values\ny = targets_df.iloc[:, 1:].values\n\npca1 = PCA(n_components=580)\nX_gene_pca = pca1.fit_transform(X_gene)\n\npca2 = PCA(n_components=75)\nX_cell_pca = pca2.fit_transform(X_cell)","41cc778f":"fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\nax[0].plot(np.cumsum(pca1.explained_variance_ratio_))\nax[0].set_xlabel('Number of components')\nax[0].set_ylabel('Cumulative explained variance')\nax[0].set_title('Gene', fontsize=14)\n\nax[1].plot(np.cumsum(pca2.explained_variance_ratio_))\nax[1].set_xlabel('Number of components')\nax[1].set_ylabel('Cumulative explained variance')\nax[1].set_title('Cell', fontsize=14)\n\nfig.suptitle('Explained variance ratio', fontsize=20, fontweight=\"bold\");","60a66143":"scores = []\nn_comps1 = [100, 150, 200, 250, 300, 350]\nn_comps2 = [20, 25, 30, 40, 50, 60]\n\nfor n_comp1, n_comp2 in zip(n_comps1, n_comps2):\n    pca1 = PCA(n_components=n_comp1)\n    pca2 = PCA(n_components=n_comp2)\n    \n    X_gene_pca = pca1.fit_transform(X_gene)\n    X_cell_pca = pca2.fit_transform(X_cell)\n    \n    X = np.concatenate((train_df[cat_features], X_gene_pca, X_cell_pca), axis=1)\n    \n    score = cross_validate_strategy(X, y)\n    scores.append(score)","3544f8d6":"fig = plt.figure(figsize=(10, 6))\nplt.plot(n_comps1, scores)\nplt.xlabel('Number of principal components (Gene, Cell)')\nplt.ylabel('Cross validation score')\nplt.title('CV score vs n_components', fontsize=20, fontweight=\"bold\");","4ad4b0b3":"Much better... Now the data has been centered.\nLet's finally encode the categorical features, and we will be done with preprocessing.","5d7d749c":"The dataset is composed of 3 categorical features, 100 cell features and 772 gene features. From the EDA performed in a previous notebook, we have inferred that some features have strong linear relationships, we want to get rid of some of them with PCA to reduce computational time and variance of our model. We will try two strategies:\n\n1. Applying PCA to both cell features and gene features\n2. Applying PCA to cell features and gene features separately\n\nSince the categorical features are distinct (categorical) and each one brings additional and unrelated information, I won't apply PCA on them.\n\n\n### Applying PCA to the entire dataset\n\nWe'll apply PCA on the whole gene and cell features.","48524983":"## Applying PCA to gene and cell features separately","879f7342":"For this experiment, I'll remove control groups since we know they don't trigger any mechanisms.","2626e880":"Since we want to make sure the columns are centered, we apply some form of pre-processing. It could be **MinMaxScaler**, **StandardScaler**. Here I will go with **QuantileTransformer** since at the time of writing it has been shown that QuantileTransformer is particularly useful for this competition. In addition to centering our data, it will make them Gaussian.","44025476":"# Mastering PCA - An essential tool for data science\n\nHello everyone, in this notebook we shall see how to apply PCA for dimensionality reduction purposes. It is a key tool for visualization and statistical modeling. \nFirst, we'll see what is PCA and the maths behind it, then we'll use **Mechanism of Action** competition to practice PCA.\n\nI assume that you have some foundations in algebra and statistics. \n\nSome resources to help you: \n- [Gilbert Strang, Linear Algebra course, MIT](https:\/\/ocw.mit.edu\/courses\/mathematics\/18-06-linear-algebra-spring-2010\/)\n- [An introductory course to Statistics, MIT](https:\/\/www.youtube.com\/watch?v=VPZD_aij8H0&ab_channel=MITOpenCourseWare\/)\n\n## What is PCA?\n\nPCA is a dimensionality reduction technique. It consists in finding a low-dimensional representation that captures the statistical properties of high-dimensional data. Indeed, with PCA we are interested in analyzing the latent structure of the data, by removing as much noise as possible while retaining the maximum variance. PCA projects the data into a low-dimensional manifold. Let's recall that a manifold is a topological structure (a set of points connected to each other) embedded in a higher dimensional space. \n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_lle_digits_003.png\">\n\nFor engineers, you might also know this technique from the **Karhunen-Lo\u00e8ve transform** in signal processing.\n\n**How does PCA do that?**\n\nPCA projects high-dimensional data into a lower dimensional space spanned by orthogonal variables called **principal components**.\n\nFor an interesting read (a bit mathy) on how PCA is actually computed with QR decomposition, [check this paper](https:\/\/arxiv.org\/abs\/0909.4061). Thanks @cdeotte for the link! Most scientific software when computing PCA use a stochastic description described in the paper above, thus leading to slightly different principal components each time PCA is computed!\n\n\n## A bit of maths\n\nThere are two ways to approach PCA: \n1. PCA can be defined as the orthogonal projection that minimizes the average projection  cost, usually measured using the mean squared distance between the data points and their projections. \n2. PCA can be defined as the orthogonal projection of the data onto a lower dimensional linear space known as the principal subspace, such that the variance of the projected data is maximized.\n\nLet $(X_i)_{1\\leqslant i\\leqslant n}$ be i.i.d. random variables in $\\mathbb{R}^d$ and consider the matrix $X\\in\\mathbb{R}^{n\\times d}$ such that the $i$-th row of $X$ is the observation $X'_i$. Let $\\Sigma_n$ be the empirical covariance matrix:\n$$\n\\Sigma_n = \\frac{1}{n}\\sum_{i=1}^n X_i X'_i\\,.\n$$\n\n### Minimizing the reconstruction error\n\nPrincipal Component Analysis  aims at reducing the dimensionality of the observations $(X_i)_{1\\leqslant i \\leqslant n}$ using a \"compression\" matrix $W\\in \\mathbb{R}^{p\\times d}$ with $p\\leqslant d$ so that for each $1\\leqslant i \\leqslant n$, $WX_i$ ia a low dimensional representation of $X_i$. The original observation may then be partially recovered using another matrix $U\\in \\mathbb{R}^{d\\times p}$. Let's note that since \\\\(U\\\\) is not surjective (not full rank), this recovery is necessarily incomplete. Principal Component Analysis computes $U$ and $W$ using the least squares approach:\n$$\n\\hspace{-0.5cm}\\underset{(U,W)\\in \\mathbb{R}^{d\\times p}\\times \\mathbb{R}^{p\\times d}}{\\mathrm{argmin}} \\;\\sum_{i=1}^n\\|X_i - UWX_i\\|^2\\,, \n$$\n\nLet $\\{\\vartheta_1,\\ldots,\\vartheta_d\\}$ be orthonormal eigenvectors associated with the eigenvalues $\\lambda_1\\geqslant \\ldots \\geqslant \\lambda_d$ of $\\Sigma_n$. Then a solution is given by the matrix $U_{\\star}$ with columns $\\{\\vartheta_1,\\ldots,\\vartheta_p\\}$ and $W_{\\star} = U_{\\star}'$.\n\n### Maximizing the variance of the projection\n\nFor any dimension $1\\leqslant p \\leqslant  d$, let $\\mathcal{F}_d^p$ be the set of all vector suspaces of $\\mathbb{R}^d$ with dimension $p$. Principal Component Analysis computes a linear span $V_d$ such as\n$$\nV_p \\in \\underset{V\\in \\mathcal{F}_d^p}{\\mathrm{argmin}} \\;\\sum_{i=1}^n\\|X_i - \\pi_V(X_i)\\|^2\\,, \n$$\nwhere $\\pi_V$ is the orthogonal projection onto the linear span $V$. Consequently, $V_1$ is a solution if and only if $v_1$ is solution to:\n$$\nv_1 \\in \\underset{v \\in \\mathbb{R}^d\\,;\\, \\|v\\|=1}{\\mathrm{argmax}} \\sum_{i=1}^n   \\langle X_i, v \\rangle^2\\,.\n$$\nFor all $2\\leqslant p \\leqslant d$, following the same steps, it can be proved that  a solution is given by $V_p = \\mathrm{span}\\{v_1, \\ldots, v_p\\}$ where\n$$\nv_1 \\in \\underset{v\\in \\mathbb{R}^d\\,;\\,\\|v\\|=1}{\\mathrm{argmax}} \\sum_{i=1}^n\\langle X_i,v\\rangle^2 \\quad\\mbox{and for all}\\;\\; 2\\leqslant k \\leqslant p\\;,\\;\\; v_k \\in \\underset{\\substack{v\\in \\mathbb{R}^d\\,;\\,\\|v\\|=1\\,;\\\\ v\\perp v_1,\\ldots,v\\perp v_{k-1}}}{\\mathrm{argmax}}\\sum_{i=1}^n\\langle X_i,v\\rangle^2\\,. \n$$\n\n\n## PCA in practice\n\nAs mentioned above, the data is assumed to be centered (the expectation of the columns is 0). Hence, beforehand, we need to preprocess our data by centering the feature columns.\n\n1. Compute the covariance matrix of the data \\\\(X\\\\): \\\\(XX^T\\\\)\n2. Compute the corresponding eigenvalues and eigenvectors of the matrix \n3. Normalize each of the eigenvectors to turn them into unit vectors.\n4. Once done, each eigenvector can be interpreted as an axis of the ellipsoid fitted to the data.\n\nObviously, in practice, everything is handled for you by Scikit-Learn, but it is also good to know how things work in the background.\n\n## PCA and SVD\n\nSome of you might have heard that SVD is closely related to [singular value decomposition](https:\/\/en.wikipedia.org\/wiki\/Singular_value_decomposition). I won't enter into too much details here since this notebook is about applying PCA. However, it is good to remember that when applying PCA to a dataset, instead of computing the eigenvectors as stated above, the software was usually compute the singular value decomposition of the design matrix \\\\(X\\\\). Using the spectral theorem, it can be shown that computing the eigenvalues of the covariance matrix and computing the SVD of the design matrix is equivalent and solve the optimization problems of PCA.\n\nNow let's apply PCA on the **MoA** dataset.\n\n## Applying PCA to MoA\n\nFirst of all, if you seek a comprehensive EDA: https:\/\/www.kaggle.com\/rftexas\/moa-in-depth-eda-start-here\n\nI hope you like the notebook! Don't hesitate to upvote it ;)","f21d84b5":"Obviously we see that adding more dimensions we'll explain more variance. For your theoretical understanding, explained variance is:\n$$\\frac{\\sum_{i=1}^d \\lambda_i}{\\sum_{j=1}^p \\lambda_j}$$\n\nwhere \\\\((\\lambda_1, ..., \\lambda_k)\\\\) are eigenvalues of the covariance matrix. Here \\\\(d\\\\) is the number of retained principal components, while \\\\(p\\\\) is the number of features.","a28191a2":"Let's plot the distribution of the means of the columns.","f3f7e283":"Here we see that we have a clear winner, with 300 components. The log_loss is minimal with 300 components."}}