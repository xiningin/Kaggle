{"cell_type":{"97bf04ae":"code","fa14e029":"code","573cf9cd":"code","a7361a11":"code","9902bccf":"code","c53e8af3":"code","6b2d62c9":"code","082f75a4":"code","a173934d":"code","d43c1e3f":"code","2b48ddc6":"code","eaff8a11":"code","5c0a607b":"code","9d27ce0f":"code","763d92c3":"code","283548f7":"code","0a292025":"code","fe04fd4f":"code","70121d0b":"code","862f767d":"code","18543d53":"code","a68ee1a2":"code","8b7eeb8c":"code","119d550e":"code","4e5cdfa3":"code","54160a3a":"code","125133b3":"code","8890ca8e":"code","eec5575c":"code","dfa44c6d":"code","f55aa606":"code","fb63b936":"code","e2958a97":"code","e485f39f":"code","155ca5bc":"code","5d58c3e5":"code","d64bc88e":"code","669e5fd7":"markdown","d5a0363f":"markdown","5c2eaae3":"markdown","89a30430":"markdown","a0f0022a":"markdown","09bb7a5e":"markdown","b57a232c":"markdown","06179597":"markdown","fd709b89":"markdown","f53b970f":"markdown","51c3eae0":"markdown","82e97f69":"markdown"},"source":{"97bf04ae":"!pip install BorutaShap","fa14e029":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture\n\nfrom BorutaShap import BorutaShap\nfrom boruta import BorutaPy\nimport tqdm\nimport ipywidgets\nfrom scipy.special import boxcox, boxcox1p\n\n# Functions\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n\ndef quick_relplot(df, features):\n\n    sns.relplot(\n        x=\"value\", y=\"target\", col=\"variable\", \n        data=df.melt(id_vars='target', value_vars=features), facet_kws=dict(sharex=False), height = 5,\n    aspect = 1, col_wrap=3);\n    \n    \ndef quick_catplot(df, features, kind):\n    \n    \"\"\"Create a multiple catplot quickly\"\"\"\n\n    melted_df = df.melt(id_vars=['target'], value_vars=features, var_name='features')\n    sns.catplot(x='value', y='target', col='features', data=melted_df, palette='rainbow', col_wrap=4, height=4, aspect=1.5, kind=kind);","573cf9cd":"X_full = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", index_col=\"id\")\nX_test = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", index_col=\"id\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")","a7361a11":"print(X_full.shape)\nX_full.info()","9902bccf":"categorical = X_full.select_dtypes('object').columns # To retrieve the names\ncategorical","c53e8af3":"numerical = X_full.drop(['target'], axis=1).select_dtypes('float64').columns # To retrieve the names\nnumerical","6b2d62c9":"y = X_full.target\nX_full = X_full.drop(['target'], axis=1)","082f75a4":"ord_enc = OrdinalEncoder()\nX_full[categorical] = ord_enc.fit_transform(X_full[categorical])\nord_enc.categories_","a173934d":"X_test[categorical] = ord_enc.transform(X_test[categorical])","d43c1e3f":"len(X_full.columns)","2b48ddc6":"# stratifying for CV\nkm = KMeans(n_clusters=20, random_state=42)\npca = PCA(n_components=24, random_state=42)\npca.fit(X_full)\nkm.fit(pca.transform(X_full))","eaff8a11":"km.labels_","5c0a607b":"y_strat = km.labels_","9d27ce0f":"pca.components_\nprint(pca.explained_variance_)\npca.explained_variance_ratio_","763d92c3":"params = dict([('colsample_bytree', 0.1),\n               ('learning_rate', 0.03),\n               ('max_depth', 3),\n               ('n_estimators', 1_000),\n               ('reg_alpha', 20),\n               ('reg_lambda', 10.0),\n               ('subsample', 1.0)])\nmodel = XGBRegressor(random_state=0, objective='reg:squarederror',\n                         **params, tree_method='gpu_hist')","283548f7":"folds = 3\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=42)","0a292025":"selected_columns = list()\n    \nfor k, (train_idx, val_idx) in enumerate(skf.split(X_full, y_strat)):\n    \n    print(f\"FOLD {k+1}\/{folds}\")\n    \n    \n    Feature_Selector = BorutaShap(model=model,\n                                  importance_measure='shap', \n                                  classification=False)\n\n    Feature_Selector.fit(X=X_full.iloc[train_idx, :], \n                         y=y.iloc[train_idx], \n                         n_trials=50, random_state=0)\n    \n    Feature_Selector.plot(which_features='all', figsize=(24,12))\n    \n    selected_columns.append(sorted(Feature_Selector.Subset().columns))\n    \n    print(f\"Selected features at fold {k+1} are: {selected_columns[-1]}\")","fe04fd4f":"sel_feats = ['cat1', 'cat3', 'cat5', 'cat8', 'cont0', 'cont1', 'cont10', \n 'cont11', 'cont12', 'cont13', 'cont2', 'cont3', 'cont4', 'cont5',\n 'cont6', 'cont7', 'cont8', 'cont9']","70121d0b":"sel_feats = selected_columns[2]","862f767d":"skewness = X_full[numerical].skew().sort_values(ascending=False)\nkurtosis = X_full[numerical].kurt().sort_values(ascending=False)\n\ndf_norm = pd.concat([skewness,kurtosis],axis=1,keys=[\"Skewness\",\"Kurtosis\"])\n\nprint(df_norm)\n\nhigh_skew = skewness[abs(skewness) > 0.5].sort_values(ascending=False)\n## Normalization of independant variables\nfor feat in high_skew.index:\n    X_full[feat] = boxcox1p(X_full[feat], stats.boxcox_normmax(X_full[feat] + 1))","18543d53":"X_train, X_valid, y_train, y_valid = train_test_split(X_full[sel_feats], y, train_size=0.8, test_size=0.2, random_state=0)\n","a68ee1a2":"X_test_2 = X_test[sel_feats]","8b7eeb8c":"X_test_2","119d550e":"X_train","4e5cdfa3":"y_train","54160a3a":"space ={\n        'max_depth': hp.choice('max_depth', np.arange(3, 6, 1)),\n        'min_child_weight': hp.choice ('min_child_weight', np.arange(15, 40, 1)),\n        'subsample': hp.choice('subsample', np.arange(0.6, 1, 0.05)),\n        'n_estimators' : hp.choice('n_estimators', np.arange(5000, 10000, 250 )),\n        'learning_rate' : hp.choice('learning_rate', np.arange(0.01, 0.6, 0.03)),\n        'gamma' : hp.choice('gamma', np.arange(0.5, 3, 0.05)),\n        'colsample_bytree' : hp.choice('colsample_bytree', np.arange(0.1, 1, 0.05)),\n        'reg_lambda': hp.choice('reg_lambda', np.arange(20, 40, 1)),\n        'alpha': hp.choice('alpha', np.arange(0, 10, 1)),\n\n    }","125133b3":"def gb_mse_cv(params, random_state=42, cv=3, X=X_train, y=y_train): \n    # the function gets a set of variable parameters in \"param\"\n    params = {\"n_estimators\" : params['n_estimators'],\n                           \"max_depth\" : params['max_depth'],\n                           \"min_child_weight\" : params['min_child_weight'],\n                           \"subsample\" : params['subsample'],\n                           \"learning_rate\" : params['learning_rate'],\n                           \"gamma\" : params['gamma'],\n                           \"colsample_bytree\" : params['colsample_bytree'],\n                           \"reg_lambda\" : params['reg_lambda'],\n                            \"alpha\" : params['alpha']\n             }\n    \n    # XGBOOSTregressor\n    model = xgb.XGBRegressor(**params, random_state=random_state,\n#                             objective='reg:squarederror',\n                                tree_method='gpu_hist')\n    \n    # and then conduct the cross validation with the same folds as before\n    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n    print(f\"MSE: {score}\")\n    \n    return score","8890ca8e":"# # Hyperparameter tunning\n\n# space ={\n#         'max_depth': hp.choice('max_depth', np.arange(3, 6, 1)),\n#         'min_child_weight': hp.choice ('min_child_weight', np.arange(4, 30, 1)),\n#         'subsample': hp.choice('subsample', np.arange(0.5, 1, 0.05)),\n#         'n_estimators' : hp.choice('n_estimators', np.arange(7000, 10000, 1 )),\n#         'learning_rate' : hp.choice('learning_rate', np.arange(0.1, 0.5, 0.05)),\n#         'gamma' : hp.choice('gamma', np.arange(0, 0.7, 0.05)),\n#         'colsample_bytree' : hp.choice('colsample_bytree', np.arange(0.5, 1, 0.05)),\n#         'reg_lambda': hp.choice('reg_lambda', np.arange(20, 40, 1)),\n#         'alpha': hp.choice('alpha', np.arange(0, 15, 1)),\n\n#     }\n\n# def objective(space):\n\n#     clf = xgb.XGBRegressor(n_estimators = space['n_estimators'],\n#                            max_depth = space['max_depth'],\n#                            min_child_weight = space['min_child_weight'],\n#                            subsample = space['subsample'],\n#                            learning_rate = space['learning_rate'],\n#                            gamma = space['gamma'],\n#                            colsample_bytree = space['colsample_bytree'],\n#                            objective='reg:squarederror',\n#                            tree_method='gpu_hist',\n#                            random_state=42\n#                            )\n    \n#     evaluation = [(X_valid, y_valid)]\n    \n#     clf.fit(X_train, y_train,\n#             eval_set=evaluation, eval_metric=\"rmse\",\n#             early_stopping_rounds=50,verbose=False)\n    \n\n#     pred = clf.predict(X_valid)    \n#     rmse = mean_squared_error(pred,y_valid, squared=False)\n#     print(f\"MSE: {mean_squared_error(pred,y_valid)} \\t RMSE: {rmse}\")\n    \n#     return{'loss':rmse, 'status': STATUS_OK }\n\n   ","eec5575c":"# Searching for the best hyperparameters\n\ntrials = Trials()\n\nbest_hyperparams = fmin(fn = gb_mse_cv,\n                        space = space,\n                        algo = tpe.suggest, # optimization algorithm\n                        max_evals = 200,\n                        trials = trials, # logging, \n                       rstate=np.random.RandomState(42) # fixing random state for the reproducibility\n                       )","dfa44c6d":"best_hyperparams","f55aa606":"hyperparams = {}","fb63b936":"hyperparams['max_depth'] = np.arange(3, 6, 1)[best_hyperparams['max_depth']]\nhyperparams['min_child_weight'] = np.arange(15, 40, 1)[best_hyperparams['min_child_weight']]\nhyperparams['subsample'] = np.arange(0.6, 1, 0.05)[best_hyperparams['subsample']]\nhyperparams['colsample_bytree'] = np.arange(0.1, 1, 0.05)[best_hyperparams['colsample_bytree']]\nhyperparams['reg_lambda'] = np.arange(20, 40, 1)[best_hyperparams['reg_lambda']]\nhyperparams['alpha'] = np.arange(0, 10, 1)[best_hyperparams['alpha']]\nhyperparams['gamma'] = np.arange(0.5, 3, 0.05)[best_hyperparams['gamma']]\nhyperparams['learning_rate'] = np.arange(0.01, 0.6, 0.03)[best_hyperparams['learning_rate']]\nhyperparams['n_estimators'] = np.arange(5000, 10000, 250)[best_hyperparams['n_estimators']]","e2958a97":"hyperparams","e485f39f":"model = XGBRegressor(**hyperparams, random_state=42)\nmodel.fit(X_train, y_train, \n         #eval_set = [(X_valid, y_valid)],\n         #eval_metric = 'rmse',\n#          early_stopping_rounds = 10,\n          verbose=0)","155ca5bc":"pred = model.predict(X_valid)\nmean_squared_error(pred,y_valid, squared=True)","5d58c3e5":"pred1 = model.predict(X_test_2)\nresults = pd.DataFrame({'Id': X_test.index,\n                       'target': pred1})\nresults.to_csv('submission_search.csv', index=False)\nresults\n","d64bc88e":"tpe_results=np.array([[x['result']['loss'],\n                      x['misc']['vals']['learning_rate'][0],\n                      x['misc']['vals']['max_depth'][0],\n                      x['misc']['vals']['n_estimators'][0]] for x in trials.trials])\n\ntpe_results_df=pd.DataFrame(tpe_results,\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\ntpe_results_df.plot(subplots=True,figsize=(10, 10))","669e5fd7":"First, we are going to read all datsets (train, test and submission).","d5a0363f":"# Boruta algorithm","5c2eaae3":"We are going to import all libraries and functions to carry out our analysis.","89a30430":"It requires to study deeply all datasets to gain a higher insigt about them.\nWe are going to observe each variable and know their type in python.","a0f0022a":"## ordinal encoding","09bb7a5e":"# 30 Days of Machine Learning Competition","b57a232c":"## Overview data","06179597":"## Load data","fd709b89":"## XBOOST with selected features and hyperparameter search","f53b970f":"We have two types of variables, objects anf float64. We are goin to keep the index for future analysis. ","51c3eae0":"## Libraries and functions","82e97f69":"We can inspect sklearn fit\/transform methods to retrieve information:"}}