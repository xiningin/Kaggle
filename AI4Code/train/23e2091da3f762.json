{"cell_type":{"b0c30670":"code","a6186439":"code","bd9c0771":"code","6ff74648":"code","d1e537ce":"code","19599b74":"code","6a058349":"code","105810c9":"code","9a83d134":"code","0c755b9b":"code","55cf7dfe":"code","ff26c745":"code","bbcfb6f7":"code","d75713a9":"code","474d0932":"code","88bbe134":"code","525d9b58":"code","2682c1cc":"code","cdedaf9a":"code","0e213fb3":"code","6cff738d":"code","0ff7a758":"code","758ad54a":"code","d78b02ac":"code","c64a76eb":"code","9798ea9a":"code","651253c9":"code","93d46254":"code","ba265b8c":"code","31f39268":"code","75dc033d":"code","dec88f85":"code","74a22aee":"code","70d3d674":"code","eed7a3b8":"code","ba5bddbd":"code","414a2aa8":"code","ea2b547b":"code","d6b255e2":"code","29f19440":"code","3c2e1fb0":"code","1a98bb2b":"code","98a37021":"code","929ad1ad":"code","76fe3c42":"code","c5de85be":"code","530451c0":"code","94e69339":"code","c4298385":"code","12cf99a5":"code","f6f2b60e":"code","7d257a67":"code","42329087":"code","8c84c7f7":"code","45afa235":"code","876c18e8":"code","5e495441":"code","c1133094":"code","d4b7b7ed":"code","b5cf3476":"code","5e1b5f29":"code","96aca582":"code","0dbf5275":"code","40f1e211":"code","6640064f":"code","78fdabf7":"code","f39058e6":"markdown","7fefc301":"markdown","cc293ebf":"markdown","9713a645":"markdown","9b414047":"markdown","99250100":"markdown","4c0b2133":"markdown","cee83236":"markdown","b70653f5":"markdown","43e95bb6":"markdown","5d3fc11f":"markdown","b79f2e5e":"markdown","fb4d689c":"markdown","67dd3418":"markdown","32c0f2f3":"markdown","ee00b7a1":"markdown","f8baef44":"markdown","dafac029":"markdown","f18babb0":"markdown","39569c4c":"markdown","6ca1d8db":"markdown","5fa7f78b":"markdown","0a48b81f":"markdown","76d50ab9":"markdown","0997fdbe":"markdown"},"source":{"b0c30670":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","a6186439":"import pandas as pd","bd9c0771":"import numpy as np","6ff74648":"import matplotlib.pyplot as plt","d1e537ce":"import seaborn as sns","19599b74":"import statsmodels.api as sm","6a058349":"import statsmodels.formula.api as smf","105810c9":"pip install skompiler","9a83d134":"from skompiler import skompile","0c755b9b":"from sklearn.linear_model import LinearRegression","55cf7dfe":"from sklearn.metrics import mean_squared_error,r2_score","ff26c745":"from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV","bbcfb6f7":"from sklearn.decomposition import PCA","d75713a9":"from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier","474d0932":"from sklearn.preprocessing import scale","88bbe134":"from sklearn import model_selection","525d9b58":"from sklearn.neighbors import KNeighborsRegressor","2682c1cc":"from sklearn.ensemble import BaggingRegressor,RandomForestRegressor,BaseEnsemble,GradientBoostingRegressor","cdedaf9a":"pip install astor","0e213fb3":"import astor","6cff738d":"import time","0ff7a758":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","758ad54a":"hts.dropna(inplace=True)","d78b02ac":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","c64a76eb":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","9798ea9a":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","651253c9":"y = hts.Salary # Target-dependent variable","93d46254":"hts.shape","ba265b8c":"#Independent Variables\nX.shape","31f39268":"#Dependent Variables\ny.shape","75dc033d":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","dec88f85":"X_train = pd.DataFrame(X_train.Hits)\nX_test = pd.DataFrame(X_test.Hits)","74a22aee":"cart_model = DecisionTreeRegressor(max_leaf_nodes=15).fit(X_train,y_train)","70d3d674":"cart_model","eed7a3b8":"plt.figure(figsize=(20,5))\nx_grid = np.arange(min(np.array(X_train)),max(np.array(X_train)),0.01)\nx_grid = x_grid.reshape(len(x_grid),1)\nplt.scatter(X_train,y_train,color=\"blue\")\nplt.plot(x_grid,cart_model.predict(x_grid),color=\"red\")\nplt.title(\"Cart Regression Tree\")\nplt.xlabel(\"Hits\")\nplt.ylabel(\"Salary\")\nplt.show()\n","ba5bddbd":"print(skompile(cart_model.predict).to(\"python\/code\"))","414a2aa8":"x= [140]","ea2b547b":"((2127.333 if x[0] <= 14.0 else 219.1551724137931 if x[0] <= 54.5 else \n    1300.0 if x[0] <= 55.5 else 369.7562688172043) if x[0] <= 115.5 else ((\n    (631.6666666666666 if x[0] <= 122.5 else 1468.5236666666667 if x[0] <= \n    125.5 else 826.8055) if x[0] <= 129.5 else 589.34995) if x[0] <= 143.0 else\n    (935.646 if x[0] <= 150.5 else 2460.0) if x[0] <= 151.5 else \n    540.8333333333334 if x[0] <= 158.5 else (910.2526086956523 if x[0] <= \n    191.0 else 1279.1666666666667) if x[0] <= 211.5 else 365.0) if x[0] <= \n    230.5 else 1975.0)","d6b255e2":"cart_model","29f19440":"y_pred=cart_model.predict(X_train)","3c2e1fb0":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","1a98bb2b":"r2_score(y_train,y_pred)","98a37021":"y_pred=cart_model.predict(X_test)","929ad1ad":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","76fe3c42":"r2_score(y_test,y_pred)","c5de85be":"hts = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhts.head()","530451c0":"hts.dropna(inplace=True)","94e69339":"one_hot_encoded = pd.get_dummies(hts[[\"League\",\"Division\",\"NewLeague\"]])\none_hot_encoded.head()","c4298385":"new_hts = hts.drop([\"League\",\"Division\",\"NewLeague\",\"Salary\"],axis=1).astype(\"float64\")","12cf99a5":"X = pd.concat([new_hts,one_hot_encoded[[\"League_N\",\"Division_W\",\"NewLeague_N\"]]],axis=1)\nX.head()","f6f2b60e":"y = hts.Salary # Target-dependent variable","7d257a67":"hts.shape","42329087":"#Independent Variables\nX.shape","8c84c7f7":"#Dependent Variables\ny.shape","45afa235":"X_train = X.iloc[:210]\nX_test = X.iloc[210:]\ny_train = y[:210]\ny_test = y[210:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","876c18e8":"cart_model = DecisionTreeRegressor().fit(X_train,y_train)","5e495441":"cart_params = {\"max_leaf_nodes\": [2,5,7,10,15],\n               \"min_samples_split\" : [5,9,11,15,19,25,30,45],\n               \"min_samples_leaf\": [2,5,7,10,15],\n               \"max_depth\": [3,5,8,11,15]\n              }","c1133094":"cart_cv_model= GridSearchCV(cart_model,cart_params,cv=15).fit(X_train,y_train)","d4b7b7ed":"cart_cv_model.best_params_","b5cf3476":"tuned_cart_model = DecisionTreeRegressor(max_leaf_nodes=cart_cv_model.best_params_['max_leaf_nodes'],\n                                         min_samples_leaf=cart_cv_model.best_params_['min_samples_leaf'],\n                                         min_samples_split=cart_cv_model.best_params_['min_samples_split'],\n                                         max_depth=cart_cv_model.best_params_['max_depth']).fit(X_train,y_train)","5e1b5f29":"y_pred=tuned_cart_model.predict(X_train)","96aca582":"#Train Error\nnp.sqrt(mean_squared_error(y_train,y_pred))","0dbf5275":"r2_score(y_train,y_pred)","40f1e211":"y_pred=tuned_cart_model.predict(X_test)","6640064f":"#Test Error\nnp.sqrt(mean_squared_error(y_test,y_pred))","78fdabf7":"r2_score(y_test,y_pred)","f39058e6":"**Lockdown Example**\n\nFor example, I tried to generate a guide for citizens in the big cities about\ngoing-out permissions during Covid-19 Pandemic. For the sake of simplicity, the users are adult\ncitizens who are older than 20 years.\n\nThe guide first checks the age of the citizen. If the citizen is 65+, then it checks if it is an Out-Day-\nFor65+. A 65+ user is allowed to go out only if it is an Out-Day-For-65+, otherwise he\/she is\nnot allowed to go out.\n\nIf the age of the citizen is not 65+, then the guide checks if that day is a Lockdown-Day. If it is not a\nlockdown day, this younger citizen is allowed to go out. However, if it is a lockdown day, the\nyounger citizens can go out only if he\/she accompanies a 65+ person. Otherwise he is not allowed to go out.\n\nIn this example, *Age of Citizen* is **root node** at the beginning of a tree. It represents entire population being analyzed. From the root node, the population is divided according to age.\n\n*Allowed to go out* and *Not allowed to go out* is **Leaf(Terminal) Node** that does not split anything.Splitting is a process of dividing a node into two or more sub-nodes.\n\n**Parent and Child Node** is a node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node.","7fefc301":"### Model","cc293ebf":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","9713a645":"For a real world example, we will work with **Hitters** dataset.\n\nIt can be downloaded here: https:\/\/www.kaggle.com\/floser\/hitters","9b414047":"Now we will split our dataset as train and test set.","99250100":"Now we will remove NA values.","4c0b2133":"The Regression Tree (CART) algorithm provides a foundation for important algorithms like bagged decision trees, random forest and boosted decision trees.\n","cee83236":"\n- Regression(Decision) Trees (CART) (Theory - Model- Tuning)","b70653f5":"Now we will split our dataset as train and test set.","43e95bb6":"**Check out My Github for other Regression Models**\n\nGithub Repository Including:\n\nK - Nearest Neighbors(KNN) (Theory - Model- Tuning)\n\nEnsemble Learning - Bagged Trees(Bagging) (Theory - Model- Tuning)\n\nEnsemble Learning - Random Forests (Theory - Model- Tuning)\n\nGradient Boosting Machines(GBM) (Theory - Model- Tuning)\n\nLight Gradient Boosting Machines(LGBM) (Theory - Model- Tuning)\n\nXGBoost(Extreme Gradient Boosting) (Theory - Model- Tuning)\n\nCatboost (Theory - Model- Tuning)\n\nCheck it out: https:\/\/github.com\/berkayalan\/Data-Science-Tutorials\/blob\/master\/Non-Linear%20Models%20-%20Regression.ipynb\n\nFor more Tutorial: https:\/\/github.com\/berkayalan","5d3fc11f":"Decision Trees are an important type of algorithm for predictive modeling machine learning. Regression Trees are a type of Decision Trees.\n\nThe representation for the CART model is a binary tree. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n\nThe leaf nodes of the tree contain an output variable (y) which is used to make a prediction.","b79f2e5e":"Let's build a decision rule to see better.","fb4d689c":"## Content\n","67dd3418":"### Theory","32c0f2f3":"We will do **One Hot Encoding** to categorical columns.","ee00b7a1":"## Regression(Decision) Trees (CART)","f8baef44":"### Model Tuning","dafac029":"### Prediction","f18babb0":"Now we will remove NA values.","39569c4c":"![Screen%20Shot%202021-07-24%20at%2012.56.03.png](attachment:Screen%20Shot%202021-07-24%20at%2012.56.03.png)","6ca1d8db":"We will do **One Hot Encoding** to categorical columns.","5fa7f78b":"## Importing Libraries","0a48b81f":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n\n- [**Understanding Confusion Matrix**](https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62)\n\n- [**Classification And Regression Trees for Machine Learning**](https:\/\/machinelearningmastery.com\/classification-and-regression-trees-for-machine-learning\/)\n\n- [**Regression Trees by Statquest**](https:\/\/www.youtube.com\/watch?v=g9c66TUylZ4&ab_channel=StatQuestwithJoshStarmer)\n\n- [**Decision Tree Algorithm, Explained**](https:\/\/www.kdnuggets.com\/2020\/01\/decision-tree-algorithm-explained.html)","76d50ab9":"**Created by Berkay Alan**\n\n**Non-Linear Models - Regression | Decision Trees (CART)**\n\n**3 August 2021**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan","0997fdbe":"## Resources"}}