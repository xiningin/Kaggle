{"cell_type":{"2690ac0a":"code","627ffa43":"code","39213cca":"code","f08926d5":"code","bccc71b8":"code","5f6cac53":"code","4cabcb2c":"code","b5f85fbf":"code","b13e0418":"code","be4cd6f0":"code","4eaa17b6":"code","a95042f4":"code","c41aa360":"code","e9d2af26":"code","621b6544":"code","dcecd4e9":"code","ebb07eeb":"code","438796c7":"code","8aee977b":"code","4b76d187":"code","d4519653":"code","8fe3a3cf":"code","af116223":"code","bdc1ddf9":"code","633692ed":"code","cd384f45":"code","956c7da4":"code","43fda47a":"code","0adfde7b":"code","311ee45d":"code","f1b8ef1d":"code","33936b9d":"code","e589d88b":"code","b6b58e2e":"code","2422e67c":"code","16b22d75":"code","015cd876":"code","126f0b94":"code","d67c4f8f":"code","42b340f2":"code","37fec322":"code","c1af7eca":"code","4069b0b6":"code","203c5d0a":"code","6dafd85c":"code","8c363cde":"code","73bdb4cc":"code","1e17499d":"code","b01bbb90":"code","3ea53791":"code","bae11b33":"code","218b1411":"code","e4597542":"code","51d234e0":"code","092f45b8":"code","12740bce":"code","55b75ca2":"code","051c4b70":"code","090f1286":"code","608f2a27":"code","fe62c7ca":"code","2328cbc0":"code","0ebd22a6":"code","6cb12f20":"code","34f7bdb0":"code","cfffe652":"code","de5c905f":"code","17506470":"code","20a19332":"code","ff01a941":"code","278d34c4":"code","0282fd05":"code","2839b1c8":"code","d8832a32":"code","d07d8e9a":"code","5e8e5a5e":"code","10a78a2d":"code","24721541":"code","6705c1e2":"code","dccc54ff":"code","8aa52d78":"code","06c9f30d":"code","22a787d9":"code","e742d03d":"code","4b4e85ba":"code","cca017b4":"code","1f543b58":"code","1e3b684d":"code","fbe3796f":"code","82e09e17":"code","14463e5c":"markdown","02427f69":"markdown","bc9e85c3":"markdown","6635db16":"markdown","e230daff":"markdown","29520180":"markdown","8663c712":"markdown","3466c481":"markdown","b8578c77":"markdown","162de5b2":"markdown","fa48c194":"markdown","a5c4974b":"markdown","4aa93b0a":"markdown","b3228a60":"markdown","a4f95d0f":"markdown","b278eb27":"markdown","d6867dea":"markdown","55d6b8ca":"markdown","9010c8fc":"markdown","29eb7a24":"markdown","c68d7acf":"markdown","124a9c34":"markdown","2d763d57":"markdown","50152e8c":"markdown","ee5989dc":"markdown","82668090":"markdown","35020063":"markdown","b54815fb":"markdown","b80c1206":"markdown","daddb59c":"markdown","ccdb13e1":"markdown","f10d1a1d":"markdown","c1afc58d":"markdown","a09863e2":"markdown","58f3c0b9":"markdown","fa11619e":"markdown","a4ec41ce":"markdown","655d8e5f":"markdown","f49b6ac3":"markdown","2d9eb2a1":"markdown","a10cbfd3":"markdown","3747ed50":"markdown","8a7ec38f":"markdown","b8d1597f":"markdown","e48dc21d":"markdown","fd927b19":"markdown","af7625c4":"markdown","29409ff3":"markdown","a54453f3":"markdown","24889fc7":"markdown","790e36c5":"markdown","76963612":"markdown","ac5f1f71":"markdown"},"source":{"2690ac0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","627ffa43":"import cv2\nimport matplotlib.pyplot as plt","39213cca":"\nfull = cv2.imread(\"..\/input\/original-dog\/sammy.jpg\")\nfull = cv2.cvtColor(full,cv2.COLOR_BGR2RGB)\nplt.imshow(full) #This dog is the larger pattern we have ","f08926d5":"face = cv2.imread(\"..\/input\/dog-face\/sammy_face.jpg\")\nface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\nplt.imshow(face) #This is the aprt of dog we look for.\n","bccc71b8":"print(full.shape) #This is the original big image of a dog\nprint(face.shape) #This is the a part of the image that only shows dog\u00a8s face, thus the shape is smaller than previous one","5f6cac53":"height, width,channels = face.shape\nprint(height)\nprint(width)\nprint(channels)","4cabcb2c":"# All the 6 methods for comparison in a list\n# Note how we are using strings, later on we'll use the eval() function to convert to function\nmethods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR','cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']","b5f85fbf":"method1 = eval('cv2.TM_CCOEFF')\nresult = cv2.matchTemplate(full,face,method1)\nplt.imshow(result) #This shows where the method detected the image","b13e0418":"method2 = eval('cv2.TM_CCOEFF_NORMED')\nresult = cv2.matchTemplate(full,face,method2)\nplt.imshow(result,cmap=\"jet\") #This shows where the method detected the image","be4cd6f0":"for m in methods:\n    full_copy = full.copy()\n    method = eval(m)\n    result = cv2.matchTemplate(full,face,method)\n    # The next step is to find maximum and minimum values of this heatmap and their location\n    # and then use them to draw a rectange that shows the detected face in the full image\n    min_value, max_value, min_location, max_location = cv2.minMaxLoc(result)\n    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n        top_left = min_location\n    else:\n        top_left = max_location\n    height, width, channels = face.shape\n    bottom_right = (top_left[0]+width,top_left[1]+height)\n    cv2.rectangle(full_copy, top_left,bottom_right,(255,0,0), 10)\n    #plot and show the images:\n    plt.subplot(121)\n    plt.imshow(result)\n    plt.title(\"Heatmap of the Template Matching\")\n    \n    plt.subplot(122)\n    plt.imshow(full_copy)\n    plt.title(\"Detection of Template\")\n    \n    plt.suptitle(m)\n    plt.show()\n    print(\"\\n\")\n    print(\"\\n\")\n    ","4eaa17b6":"chess= cv2.imread(\"..\/input\/chessboard\/flat_chessboard.png\")\nchess = cv2.cvtColor(chess, cv2.COLOR_BGR2RGB)\nplt.imshow(chess)","a95042f4":"gray_chess = cv2.cvtColor(chess, cv2.COLOR_RGB2GRAY)\nplt.imshow(gray_chess,cmap=\"gray\")","c41aa360":"real_chess = cv2.imread(\"..\/input\/real-chess\/real_chessboard.jpg\")\nreal_chess = cv2.cvtColor(real_chess, cv2.COLOR_BGR2RGB)\nplt.imshow(real_chess)","e9d2af26":"gray_real_chess = cv2.cvtColor(real_chess, cv2.COLOR_RGB2GRAY)\nplt.imshow(gray_real_chess,cmap=\"gray\")","621b6544":"new_chess = np.float32(gray_chess)\nnew_chess # now the numbers are floats","dcecd4e9":"destination = cv2.cornerHarris(src=new_chess, blockSize=2, ksize=3,k=0.04)\ndestination = cv2.dilate(destination,None)\n","ebb07eeb":"#now ge incall the original image and assign those values\nchess[destination > 0.01*destination.max()] = [255,0,0] \n# here say that whereever the value is greater that one procent of the destination's maximum value, it should be corner\n#>Then we assign red color channel into these values\nplt.imshow(chess)","438796c7":"# this image is very flat, so lets do the same operation with the real complicated chess board\nnew_real_chess = np.float32(gray_real_chess)\nnew_real_chess","8aee977b":"dst = cv2.cornerHarris(src=new_real_chess,blockSize=2, ksize=3,k=0.04 )\ndst = cv2.dilate(dst,None)\n","4b76d187":"real_chess[dst > 0.01*dst.max()] = [255,0,0]\nplt.imshow(real_chess) \n# This is not as clear as the previous flat example because the corners are difficult to detect in this real image","d4519653":"dst2= cv2.cornerHarris(src=new_real_chess, blockSize=2, ksize=5, k=0.9)\ndst2 = cv2.dilate(dst2, None)\n","8fe3a3cf":"real_chess[dst2 > 0.01*dst2.max()] = [255,0,0]\nplt.imshow(real_chess)","af116223":"#Lets get the original images again\nflat_chess= cv2.imread(\"..\/input\/chessboard\/flat_chessboard.png\")\nflat_chess= cv2.cvtColor(flat_chess, cv2.COLOR_BGR2RGB)\nreal_chess= cv2.imread(\"..\/input\/real-chess\/real_chessboard.jpg\")\nreal_chess= cv2.cvtColor(real_chess, cv2.COLOR_BGR2RGB)\ngray_flat_chess= cv2.cvtColor(flat_chess, cv2.COLOR_RGB2GRAY)\ngray_real_chess= cv2.cvtColor(real_chess, cv2.COLOR_RGB2GRAY)","bdc1ddf9":"corners=cv2.goodFeaturesToTrack(image=gray_flat_chess,maxCorners=5,qualityLevel=0.01,minDistance=10)\ncorners = np.int0(corners)\ncorners","633692ed":"for i in corners:\n    x,y = i.ravel()\n    cv2.circle(flat_chess,(x,y),radius=3,color=(255,0,0),thickness=-1)","cd384f45":"plt.imshow(flat_chess) # it detects 5 corners and we can increas the number of corners","956c7da4":"corners=cv2.goodFeaturesToTrack(image=gray_flat_chess,maxCorners=64,qualityLevel=0.01,minDistance=10)\n#Now we increas ethe number of corners into 64 and it detects all the corners in the flat chess board\ncorners = np.int0(corners)\nfor i in corners:\n    x,y = i.ravel()\n    cv2.circle(flat_chess,(x,y),radius=3,color=(255,0,0),thickness=-1)\nplt.imshow(flat_chess)","43fda47a":"corners = cv2.goodFeaturesToTrack(image=gray_real_chess, maxCorners=100,qualityLevel=0.01, minDistance=10)\ncorners = np.int0(corners) # we need to transform into integers because the good features function returns floats\nfor i in corners:\n    x,y= i.ravel()\n    cv2.circle(real_chess,(x,y),radius=3, color=(0,255,0),thickness=-1)\nplt.imshow(real_chess)","0adfde7b":"import cv2\nimport matplotlib.pyplot as plt\nface= cv2.imread(\"..\/input\/dog-face\/sammy_face.jpg\")\nface= cv2.cvtColor(face,cv2.COLOR_BGR2RGB)\nplt.imshow(face)","311ee45d":"edges= cv2.Canny(face,threshold1=127, threshold2=127 )\nplt.imshow(edges,cmap=\"gray\")","f1b8ef1d":"edges= cv2.Canny(face,threshold1=127, threshold2=255 )\nplt.imshow(edges,cmap=\"gray\")","33936b9d":"blurred_img = cv2.blur(face, (5,5))\nedges= cv2.Canny(blurred_img,threshold1=127, threshold2=255 )\nplt.imshow(edges,cmap=\"gray\") # we need to lower the thresholds","e589d88b":"median=np.median(face)\nblurred_img = cv2.blur(face, (5,5))\nedges= cv2.Canny(blurred_img,threshold1=median, threshold2=median+120 )\nplt.imshow(edges,cmap=\"gray\") # now we get better edge detection when we use blurred version of the image","b6b58e2e":"chessboard= cv2.imread(\"..\/input\/chessboard\/flat_chessboard.png\")\nchessboard = cv2.cvtColor(chessboard,cv2.COLOR_BGR2RGB)\nplt.imshow(chessboard)","2422e67c":"found, corners = cv2.findChessboardCorners(chessboard,(7,7))\nfound #The first one shows whether it find the corners or not","16b22d75":"corners #This shows the corners","015cd876":"cv2.drawChessboardCorners(chessboard, (7,7),corners, found)\nplt.imshow(chessboard)","126f0b94":"dotgrid= cv2.imread(\"..\/input\/dotgrid\/dot_grid.png\")\ndotgrid= cv2.cvtColor(dotgrid, cv2.COLOR_BGR2RGB)\nplt.imshow(dotgrid)","d67c4f8f":"found, corners = cv2.findCirclesGrid(dotgrid,(10,10),cv2.CALIB_CB_ASYMMETRIC_GRID)\nfound","42b340f2":"cv2.drawChessboardCorners(dotgrid,(10,10),corners,found)\nplt.imshow(dotgrid)","37fec322":"image= cv2.imread(\"..\/input\/contours\/internal_external.png\",0)\nplt.imshow(img,cmap=\"gray\")","c1af7eca":"image.shape","4069b0b6":"image, contours = cv2.findContours(img, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n","203c5d0a":"print(type(img))\nprint(type(contours))","6dafd85c":"contours","8c363cde":"external_contours= np.zeros(len(image))\nexternal_contours","73bdb4cc":"import cv2\nimport matplotlib.pyplot as plt\nreeses = cv2.imread(\"..\/input\/reeses\/reeses_puffs.png\",0)\nplt.figure(figsize=(12,8))\nplt.imshow(reeses,cmap=\"gray\")","1e17499d":"cereals = cv2.imread(\"..\/input\/cereals\/many_cereals.jpg\",0)\nplt.figure(figsize=(12,8))\nplt.imshow(cereals,cmap=\"gray\")","b01bbb90":"orb = cv2.ORB_create()\nkey_points1, descriptors1 = orb.detectAndCompute(image=reeses,mask=None)\nkey_points2, descriptors2 = orb.detectAndCompute(image=cereals, mask=None)","3ea53791":"bruteforce= cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck = True)","bae11b33":"matches = bruteforce.match(descriptors1,descriptors2) # now we have matches","218b1411":"single_match = matches[0]\nsingle_match.distance # the higher distance means the worse match and lower distance means better match","e4597542":"#The next step is to sort them according to their distances\nmatches = sorted(matches, key = lambda x : x.distance)\n","51d234e0":"reeses_matches = cv2.drawMatches(reeses,key_points1,cereals,key_points2,\n                                matches[:25],None, 2)\nplt.figure(figsize=(15,10))\nplt.imshow(reeses_matches)","092f45b8":"import cv2\nimg1 = cv2.imread(\"..\/input\/reeses\/reeses_puffs.png\",0)\nimg2 = cv2.imread(\"..\/input\/cereals\/many_cereals.jpg\",0)\n# Initiate SIFT detector\nsift = cv2.SIFT_create()\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\n# BFMatcher with default params\nbf = cv2.BFMatcher()\nmatches = bf.knnMatch(des1,des2,k=2)\n# Apply ratio test\ngood = []\nfor m,n in matches:\n    if m.distance < 0.75*n.distance:\n        good.append([m])\n# cv.drawMatchesKnn expects list of lists as matches.\nimg3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","12740bce":"plt.figure(figsize=(12,10))\nplt.imshow(img3) # Now this sift descriptor detect very good","55b75ca2":"# Initiate SIFT detector\nsift = cv2.SIFT_create()\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\n# FLANN parameters\nFLANN_INDEX_KDTREE = 1\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks=50)   # or pass empty dictionary\nflann = cv2.FlannBasedMatcher(index_params,search_params)\nmatches = flann.knnMatch(des1,des2,k=2)\n# Need to draw only good matches, so create a mask\nmatchesMask = [[0,0] for i in range(len(matches))]\n# ratio test as per Lowe's paper\nfor i,(m,n) in enumerate(matches):\n    if m.distance < 0.7*n.distance:\n        matchesMask[i]=[1,0]\ndraw_params = dict(matchColor = (0,255,0),\n                   singlePointColor = (255,0,0),\n                   matchesMask = matchesMask,\n                   flags = cv2.DrawMatchesFlags_DEFAULT)\nimg3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)","051c4b70":"plt.figure(figsize=(12,10))\nplt.imshow(img3) ","090f1286":"import cv2 \nimport matplotlib.pyplot as plt","608f2a27":"pennies= cv2.imread(\"..\/input\/pennies\/pennies.jpg\")\nplt.figure(figsize=(12,10))\nplt.imshow(pennies)","fe62c7ca":"blurring= cv2.medianBlur(pennies,25)\nplt.figure(figsize=(12,10))\nplt.imshow(blurring)","2328cbc0":"#Lets get gray version of the image\ngray_blurring= cv2.cvtColor(blurring, cv2.COLOR_BGR2GRAY)\nplt.figure(figsize=(12,10))\nplt.imshow(gray_blurring,cmap=\"gray\")","0ebd22a6":"ret, sep_thresh = cv2.threshold(gray_blurring,160,255,cv2.THRESH_BINARY_INV)\nplt.figure(figsize=(12,10))\nplt.imshow(sep_thresh,cmap=\"gray\")","6cb12f20":"pennies= cv2.imread(\"..\/input\/pennies\/pennies.jpg\")\ngray_pennies= cv2.cvtColor(pennies, cv2.COLOR_BGR2GRAY)\nplt.figure(figsize=(12,10))\nplt.imshow(gray_pennies,cmap=\"gray\")","34f7bdb0":"blurred = cv2.medianBlur(gray_pennies,35)\nplt.figure(figsize=(12,10))\nplt.imshow(blurred,cmap=\"gray\")","cfffe652":"ret, thresh = cv2.threshold(blurred,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\nplt.figure(figsize=(12,10))\nplt.imshow(thresh,cmap=\"gray\")","de5c905f":"import numpy as np\nkernel= np.ones((10,10), np.uint8)\nnoise = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)\nplt.figure(figsize=(12,10))\nplt.imshow(noise,cmap=\"gray\")","17506470":"# sure background area\nsure_bg = cv2.dilate(noise,kernel,iterations=3)\nplt.figure(figsize=(12,10))\nplt.imshow(sure_bg,cmap=\"gray\")","20a19332":"dist_transform = cv2.distanceTransform(noise,cv2.DIST_L2,5)\nplt.figure(figsize=(12,10))\nplt.imshow(dist_transform,cmap=\"gray\")","ff01a941":"ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\nplt.figure(figsize=(12,10))\nplt.imshow(sure_fg,cmap=\"gray\")","278d34c4":"# Finding unknown region\nsure_fg = np.uint8(sure_fg)\nunknown = cv2.subtract(sure_bg,sure_fg) # we substract sure foreground from sure background\nplt.figure(figsize=(12,10))\nplt.imshow(unknown,cmap=\"gray\")","0282fd05":"# Marker labelling\nret, markers = cv2.connectedComponents(sure_fg)\n# Add one to all labels so that sure background is not 0, but 1\nmarkers = markers+1\n# Now, mark the region of unknown with zero\nmarkers[unknown==255] = 0\nplt.figure(figsize=(12,10))\nplt.imshow(markers,cmap=\"gray\")","2839b1c8":"markers = cv2.watershed(pennies,markers)\nplt.figure(figsize=(12,10))\nplt.imshow(markers)","d8832a32":"road = cv2.imread('..\/input\/roadimage\/road_image.jpg')\nroad_copy = np.copy(road)\nplt.imshow(road)","d07d8e9a":"road.shape","5e8e5a5e":"road.shape[:2]","10a78a2d":"marker_image = np.zeros(road.shape[:2],dtype=np.int32)","24721541":"segments = np.zeros(road.shape,dtype=np.uint8)","6705c1e2":"segments.shape","dccc54ff":"from matplotlib import cm\ncm.tab10(0)\n","8aa52d78":"np.array(cm.tab10(0))","06c9f30d":"np.array(cm.tab10(0))[:3]","22a787d9":"np.array(cm.tab10(0))[:3]*255","e742d03d":"x = np.array(cm.tab10(0))[:3]*255\nx.astype(int)","4b4e85ba":"tuple(x.astype(int))","cca017b4":"def create_rgb(i):\n    x = np.array(cm.tab10(i))[:3]*255\n    return tuple(x)","1f543b58":"colors = []\n# One color for each single digit\nfor i in range(10):\n    colors.append(create_rgb(i))","1e3b684d":"colors","fbe3796f":"# Numbers 0-9\nn_markers = 10\n# Default settings\ncurrent_marker = 1\nmarks_updated = False\ndef mouse_callback(event, x, y, flags, param):\n    global marks_updated \n\n    if event == cv2.EVENT_LBUTTONDOWN:\n        \n        # TRACKING FOR MARKERS\n        cv2.circle(marker_image, (x, y), 10, (current_marker), -1)\n        \n        # DISPLAY ON USER IMAGE\n        cv2.circle(road_copy, (x, y), 10, colors[current_marker], -1)\n        marks_updated = True","82e09e17":"cv2.namedWindow('Road Image')\ncv2.setMouseCallback('Road Image', mouse_callback)\n\nwhile True:\n    \n    # SHow the 2 windows\n    cv2.imshow('WaterShed Segments', segments)\n    cv2.imshow('Road Image', road_copy)\n        \n        \n    # Close everything if Esc is pressed\n    k = cv2.waitKey(1)\n\n    if k == 27:\n        break\n        \n    # Clear all colors and start over if 'c' is pressed\n    elif k == ord('c'):\n        road_copy = road.copy()\n        marker_image = np.zeros(road.shape[0:2], dtype=np.int32)\n        segments = np.zeros(road.shape,dtype=np.uint8)\n        \n    # If a number 0-9 is chosen index the color\n    elif k > 0 and chr(k).isdigit():\n        # chr converts to printable digit\n        \n        current_marker  = int(chr(k))\n        \n        # CODE TO CHECK INCASE USER IS CARELESS\n#         n = int(chr(k))\n#         if 1 <= n <= n_markers:\n#             current_marker = n\n    \n    # If we clicked somewhere, call the watershed algorithm on our chosen markers\n    if marks_updated:\n        \n        marker_image_copy = marker_image.copy()\n        cv2.watershed(road, marker_image_copy)\n        \n        segments = np.zeros(road.shape,dtype=np.uint8)\n        \n        for color_ind in range(n_markers):\n            segments[marker_image_copy == (color_ind)] = colors[color_ind]\n        \n        marks_updated = False\n        \ncv2.destroyAllWindows()","14463e5c":"We have too much detail in this image, including light, the face edges on the coins, and too much detail in the background. Let's use Median Blur Filtering to blur the image a bit, which will be useful later on when we threshold.","02427f69":"## 4. Grid Detection:","bc9e85c3":"The watershed transformation treats the image it operates upon like a topographic map, with the brightness of each point representing its height, and finds the lines that run along the tops of ridges.","6635db16":"Step 10: Apply Watershed Algorithm to find Markers","e230daff":"Step 2: Apply Blur","29520180":"7.2. Using the WaterShed Algorithm:","8663c712":"There are 3 types of feature matching:","3466c481":"detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n    .   Detects keypoints and computes the descriptors","b8578c77":"\n\nfindContours\n\nfunction will return back contours in an image, and based on the RETR method called, you can get back external, internal, or both:\n\ncv2.RETR_EXTERNAL:Only extracts external contours\ncv2.RETR_CCOMP: Extracts both internal and external contours organized in a two-level hierarchy\ncv2.RETR_TREE: Extracts both internal and external contours organized in a tree graph\ncv2.RETR_LIST: Extracts all contours without any internal\/external relationship","162de5b2":"7.1.1. Apply Median Blurring:\n","fa48c194":"Step 8: Find Unknown Region","a5c4974b":"7.1. Lets Try Object Detection with Previous Methods:","4aa93b0a":"THis method does not work for this kind of complicated situations","b3228a60":"## 2. Corner Detection:","a4f95d0f":"Lets do the same operations for the real chess board","b278eb27":"## 3. Edge Detection:","d6867dea":"Optional Step 4: Noise Removal","55d6b8ca":"The final code needs a video capturing, thus is not working inside kaggle notebook.","9010c8fc":"The Process of Canny edge detection algorithm can be broken down to 5 different steps:\n\n1.Apply Gaussian filter to smooth the image in order to remove the noise\n\n2.Find the intensity gradients of the image\n\n3.Apply gradient magnitude thresholding or lower bound cut-off suppression to get rid of spurious response to edge detection\n\n4.Apply double threshold to determine potential edges\n\n5.Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.","29eb7a24":"<font color=\"blue\">\nIt is a simple form of object detection\n\nTemplate matching[1] is a technique in digital image processing for finding small parts of an image which match a template image. It can be used in manufacturing as a part of quality control,[2] a way to navigate a mobile robot,[3] or as a way to detect edges in images.","c68d7acf":"Discern Background from Foreground:\nStep 5: Grab Background that you are sure of","124a9c34":"Contours are defined a curve joining all the continuous points along the boundary, having the same color or density.\n\n","2d763d57":"## 6. Feature Matching:","50152e8c":"goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]]) -> corners\n    .   @brief Determines strong corners on an image.\n    .   \n    .   The function finds the most prominent corners in the image or in the specified image region\n    \n    @param image Input 8-bit or floating-point 32-bit, single-channel image.\n    .   @param corners Output vector of detected corners.\n    .   @param maxCorners Maximum number of corners to return. If there are more corners than are found,\n    .   the strongest of them is returned. `maxCorners <= 0` implies that no limit on the maximum is set\n    .   and all detected corners are returned.\n    .   @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The\n    .   parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue\n    .   (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the\n    .   quality measure less than the product are rejected. For example, if the best corner has the\n    .   quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure\n    .   less than 15 are rejected.\n    .   @param minDistance Minimum possible Euclidean distance between the returned corners.\n    .   @param mask Optional region of interest. If the image is not empty (it needs to have the type\n    .   CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.\n    .   @param blockSize Size of an average block for computing a derivative covariation matrix over each\n    .   pixel neighborhood. See cornerEigenValsAndVecs .\n    .   @param useHarrisDetector Parameter indicating whether to use a Harris detector (see #cornerHarris)\n    .   or #cornerMinEigenVal.\n    .   @param k Free parameter of the Harris detector.","ee5989dc":"Lets compare the performance of corner detection of both algorithms in two different images\n\nThe first thing is to transform the image data from integer into floats","82668090":"dilate(...)\n    dilate(src, kernel[, dst[, anchor[, iterations[, borderType[, borderValue]]]]]) -> dst\n    .   @brief Dilates an image by using a specific structuring element.","35020063":"Step 7: Find Sure Foreground","b54815fb":"Step 9: Label Markers of Sure Foreground","b80c1206":"drawMatches(img1, keypoints1, img2, keypoints2, matches1to2, outImg[, matchColor[, singlePointColor[, matchesMask[, flags]]]]) -> outImg\n    .   @brief Draws the found matches of keypoints from two images.\n    .   \n    .   @param img1 First source image.\n    .   @param keypoints1 Keypoints from the first source image.\n    .   @param img2 Second source image.\n    .   @param keypoints2 Keypoints from the second source image.\n    .   @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]\n    .   has a corresponding point in keypoints2[matches[i]] .\n    .   @param outImg Output image. Its content depends on the flags value defining what is drawn in the\n    .   output image. See possible flags bit values below.\n    .   @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1)\n    .   , the color is generated randomly.\n    .   @param singlePointColor Color of single keypoints (circles), which means that keypoints do not\n    .   have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.\n    .   @param matchesMask Mask determining which matches are drawn. If the mask is empty, all matches are\n    .   drawn.\n    .   @param flags Flags setting drawing features. Possible flags bit values are defined by\n    .   DrawMatchesFlags.","daddb59c":"6.3. FLANN based Matcher","ccdb13e1":"6.2.Brute-Force Matching with SIFT Descriptors :","f10d1a1d":"1. Harris Corner Detection Algorithm:","c1afc58d":"## 1. Template Matching:","a09863e2":"## 7. Waterched Algorithm:","58f3c0b9":"7.2. Custom Seeds with Watershed Algorrithm:","fa11619e":"Step 1: Read Image","a4ec41ce":"Step 11: Find Contours on Markers","655d8e5f":"#2. Lets bblur the image:","f49b6ac3":"A corner is the junction of two edges where an edge is a sudden change in image brightness.\n    Mots used Algorthms:\n    \n            1. Harris Corner Detection: It says that corners can be detected by looking for significant change in all directions.\n            2. Shi_Thomasi Corner Detection: This makes some changes in Harris Corner Algorithm and it changes scoring function selection criteria that Harris uses for corner detection.","2d9eb2a1":"2. Shi-Thomasi Corner Detection Algorithm:","a10cbfd3":"The next step is to create matching object","3747ed50":"findContours(image, mode, method[, contours[, hierarchy[, offset]]]) -> contours, hierarchy\n    .   @brief Finds contours in a binary image.\n    .   \n    .   The function retrieves contours from the binary image using the algorithm @cite Suzuki85 . The contours\n    .   are a useful tool for shape analysis and object detection and recognition. \n    \n    @param image Source, an 8-bit single-channel image\n    \n    @param contours Detected contours\n    \n    @param hierarchy Optional output vector (e.g. std::vector<cv::Vec4i>), containing information about the image topology. It has as many elements as the number of contours.\n    \n    @param mode Contour retrieval mode, see #RetrievalModes\n    \n    @param method Contour approximation method, see #ContourApproximationModes\n    \n    @param offset Optional offset by which every contour point is shifted. This is useful if the\n    .   contours are extracted from the image ROI and then they should be analyzed in the whole image\n    .   context.","8a7ec38f":"6.1.Brute Force Detection with ORB Descriptors: ","b8d1597f":"Step 3: Apply Threshold (Inverse Binary with OTSU as well)","e48dc21d":"<font color=\"blue\">\ncv2.matchTemplate(image, templ, method[, result[, mask]]) -> result\n\n@param image Image where the search is running. It must be 8-bit or 32-bit floating-point.\n    .   @param templ Searched template. It must be not greater than the source image and have the same\n    .   data type.\n    .   @param result Map of comparison results. It must be single-channel 32-bit floating-point. If image\n    .   is \\f$W \\times H\\f$ and templ is \\f$w \\times h\\f$ , then result is \\f$(W-w+1) \\times (H-h+1)\\f$ .\n    .   @param method Parameter specifying the comparison method, see #TemplateMatchModes\n    .   @param mask Optional mask. \n\n","fd927b19":"Step 6: Using Distance Transform","af7625c4":"knnMatch(queryDescriptors, trainDescriptors, k[, mask[, compactResult]]) -> matches\n    .   @brief Finds the k best matches for each descriptor from a query set.","29409ff3":"cornerHarris(src, blockSize, ksize, k[, dst[, borderType]]) -> dst\n    .   @brief Harris corner detector.\n    .   \n    .   The function runs the Harris corner detector on the image. Similarly to cornerMinEigenVal and\n    .   cornerEigenValsAndVecs , for each pixel \\f$(x, y)\\f$ it calculates a \\f$2\\times2\\f$ gradient covariance\n    .   matrix \\f$M^{(x,y)}\\f$ over a \\f$\\texttt{blockSize} \\times \\texttt{blockSize}\\f$ neighborhood. Then, it\n    .   computes the following characteristic:\n    \n    Corners in the image can be found as the local maxima of this response map.\n    .   \n    .   @param src Input single-channel 8-bit or floating-point image.\n    .   @param dst Image to store the Harris detector responses. It has the type CV_32FC1 and the same\n    .   size as src .\n    .   @param blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).\n    .   @param ksize Aperture parameter for the Sobel operator.\n    .   @param k Harris detector free parameter. See the formula above.\n    .   @param borderType Pixel extrapolation method. ","a54453f3":"Canny(image, threshold1, threshold2[, edges[, apertureSize[, L2gradient]]]) -> edges\n    .   @brief Finds edges in an image using the Canny algorithm @cite Canny86 .\n    .   \n    .   The function finds edges in the input image and marks them in the output map edges using the\n    .   Canny algorithm.\n    \n     @param image 8-bit input image.\n    .   @param edges output edge map; single channels 8-bit image, which has the same size as image .\n    .   @param threshold1 first threshold for the hysteresis procedure.\n    .   @param threshold2 second threshold for the hysteresis procedure.\n    .   @param apertureSize aperture size for the Sobel operator.","24889fc7":"Distance Transform makes the center of the image brighet while the image becomes fade away to the edges","790e36c5":"7.1.2. Binary Threshold:","76963612":"## 5. Contour Detection:","ac5f1f71":"#There are a lot of noise in the picture. In order to ged rid of noise;\n\n1. We can blur the image before we apply the Canny algorith\n\n2. Or we can play with the parameters"}}