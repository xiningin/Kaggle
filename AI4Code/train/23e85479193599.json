{"cell_type":{"f87a3e9f":"code","7e29d793":"code","69e499d1":"code","57e369de":"code","5009a069":"code","02a5f9ea":"code","a56fbb0b":"code","c13fcd99":"code","9783473f":"code","c31d3fb0":"code","44cd0290":"code","d14e4cf7":"code","94054dc0":"code","fffdb64c":"code","8f40ae32":"code","226d7860":"code","1838b646":"code","8ead5377":"code","556330e6":"code","33a52ae9":"code","a85e7cce":"code","fed3b5d0":"code","7adec7fb":"code","932e2940":"code","fc72cbf6":"code","b4625f54":"code","5b1d51bb":"code","dd1d4c71":"code","2c1dd697":"code","d1508224":"code","76624644":"code","35709344":"code","5e532b11":"code","106b611e":"code","ca78edfb":"markdown","b2056787":"markdown","b12050a7":"markdown","20e2a5f2":"markdown","ed54e06f":"markdown"},"source":{"f87a3e9f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport nltk \nimport string\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","7e29d793":"data = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\")\ndata = data.iloc[:,2:]\ndata.head()","69e499d1":"data.isnull().sum()","57e369de":"filter1 = data['Rating'] >= 4\ndf1 = data[filter1]\n\nfilter2 = data['Rating'] < 4\ndf2 = data[filter2]","5009a069":"df1","02a5f9ea":"text1 = df1['Review Text'].str.lower().str.strip().str.cat(sep = \" \")\ntext1 = text1.translate(str.maketrans(\"\",\"\", string.punctuation))\n\ntext2 = df2['Review Text'].str.lower().str.strip().str.cat(sep = \" \")\ntext2 = text2.translate(str.maketrans(\"\",\"\", string.punctuation))","a56fbb0b":"words_rating_above4 = nltk.word_tokenize(text1)\nwords_rating_below4 = nltk.word_tokenize(text2)\nstopwords = nltk.corpus.stopwords.words(\"english\")\n\nli1 = []\nli2 = []\n\nword_net_lemmatizer = WordNetLemmatizer()\n\nfor word in words_rating_above4: \n    if(word in stopwords):\n        continue\n    li1.append(word_net_lemmatizer.lemmatize(word, pos='v'))\n    \nfor word in words_rating_below4: \n    if(word in stopwords):\n        continue\n    li2.append(word_net_lemmatizer.lemmatize(word, pos='v'))","c13fcd99":"freq1 = nltk.FreqDist(li1)\n\nfreq2 = nltk.FreqDist(li2)","9783473f":"\nfig = plt.figure(figsize = (20,10))\nplt.gcf().subplots_adjust() # to avoid x-ticks cut-off\nfreq1.plot(100, cumulative=False)\nplt.show()\n","c31d3fb0":"fig = plt.figure(figsize = (20,10))\nplt.gcf().subplots_adjust() # to avoid x-ticks cut-off\nfreq2.plot(100, cumulative=False)\nplt.show()\n","44cd0290":"df1=df1.fillna(\"\")\ndf2=df2.fillna(\"\")","d14e4cf7":"vectorizer = CountVectorizer(stop_words= stopwords)\nmatrix = vectorizer.fit_transform(df1['Review Text'])\nfeature_names = np.array(vectorizer.get_feature_names())","94054dc0":"lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\",\n                                max_iter=200, random_state=0) \ndocument_topics = lda.fit_transform(matrix)","fffdb64c":"# Set n to your desired number of tokens \nn = 8\n# Find top n tokens\ntopics = dict()\nfor idx, component in enumerate(lda.components_): \n    top_n_indices = component.argsort()[:-(n + 1): -1] \n    topic_tokens = [feature_names[i] for i in top_n_indices] \n    topics[idx] = topic_tokens\n\ntopics","8f40ae32":"vectorizer2 = CountVectorizer(stop_words= stopwords)\nmatrix2 = vectorizer2.fit_transform(df2['Review Text'])\nfeature_names2 = np.array(vectorizer2.get_feature_names())","226d7860":"lda2 = LatentDirichletAllocation(n_components=10, learning_method=\"batch\",\n                                max_iter=200, random_state=0) \ndocument_topics2 = lda2.fit_transform(matrix2)","1838b646":"# Set n to your desired number of tokens \nn = 8\n# Find top n tokens\ntopics2 = dict()\nfor idx, component in enumerate(lda2.components_): \n    top_n_indices2 = component.argsort()[:-(n + 1): -1] \n    topic_tokens2 = [feature_names2[i] for i in top_n_indices2] \n    topics2[idx] = topic_tokens2\n\ntopics2","8ead5377":"data.head()","556330e6":"data1 = data.drop(['Age', 'Positive Feedback Count', 'Division Name', 'Department Name'], axis = 1)\n\ndata1.head()","33a52ae9":"data1['Rating'] = data1['Rating'].apply(lambda x: 1 if(x>=4) else 0)\n","a85e7cce":"X = data1.drop(['Rating'], axis = 1)\ny = data1['Rating']","fed3b5d0":"\nX['Title'] = X['Title'].apply(lambda x: str(x))\nX['Review Text'] = X['Review Text'].apply(lambda x: str(x))\nX.head()","7adec7fb":"X['Title_Review'] = X['Title'] + \" \" + X['Review Text']\nX['Title_Review'] = X['Title_Review'].apply(lambda x: x.replace(\"nan\",\"\").strip())\nX = X.drop(['Review Text', \"Title\"], axis = 1)\nX.head()","932e2940":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom textblob import TextBlob\n\nword_net_lemmatizer = WordNetLemmatizer()\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\nstopwords1 = STOP_WORDS\nst2 = string.digits+string.punctuation\n\ndef lemmatize_text(text):\n    lis=[]\n    words = text.split()\n    for word in words:\n        if(word in stopwords1):\n            continue\n        lis.append(word_net_lemmatizer.lemmatize(word, pos='v'))\n    return lis\n\n\nX['Title_Review'] = X['Title_Review'].apply(lambda x: \" \".join(lemmatize_text(x)))\nX['Title_Review'] = X['Title_Review'].apply(lambda x: x.translate(str.maketrans(\"\",\"\", st2)))\nX['Title_Review'] = X['Title_Review'].apply(lambda x: x.lower())\n","fc72cbf6":"from textblob import TextBlob\n\nX['Sentiment_Polarity'] = X['Title_Review'].apply(lambda x: TextBlob(x).sentiment[0])\nX['Sentiment_Subjectivity'] = X['Title_Review'].apply(lambda x: TextBlob(x).sentiment[1])\n","b4625f54":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer(stop_words='english')\ndf_text1 = vect.fit_transform(X['Title_Review'])\ndf_text1 = pd.DataFrame(df_text1.toarray(), columns= vect.get_feature_names())\n","5b1d51bb":"X1 = X.drop(['Title_Review'], axis = 1)\nX1 = pd.concat([X1, df_text1], axis = 1)\n\nX1 = pd.get_dummies(columns = ['Class Name'], data = X1)\nX1 = X1.iloc[:, :-1]\nX1","dd1d4c71":"X_train, X_test, y_train, y_test = train_test_split(X1, y, random_state = 0, test_size = 0.3)\n","2c1dd697":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nrf = RandomForestClassifier()\n\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint(\"accuracy score is: {}\".format(accuracy_score(y_test, y_pred)))","d1508224":"print(\"Confusion matrix is:\")\nprint((confusion_matrix(y_test, y_pred)))\nprint(\"Classification report is:\")\nprint((classification_report(y_test, y_pred)))","76624644":"X2 = X1.copy()\nX2['Rating'] = data['Rating']\ny2 = data['Recommended IND']\nX2= X2.drop(['Recommended IND'], axis = 1)\nX2","35709344":"X_train, X_test, y_train, y_test = train_test_split(X2, y2, random_state = 0, test_size = 0.3)\n","5e532b11":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nrf = RandomForestClassifier()\n\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint(\"accuracy score is: {}\".format(accuracy_score(y_test, y_pred)))","106b611e":"print(\"Confusion matrix is:\")\nprint((confusion_matrix(y_test, y_pred)))\nprint(\"Classification report is:\")\nprint((classification_report(y_test, y_pred)))","ca78edfb":"# Topics with rating above 4","b2056787":"# Topic modeling with rating below 4","b12050a7":"# 1st model with ratings as target variable","20e2a5f2":"# Creating a Classification model","ed54e06f":"# 2nd model with Recommended as target variable"}}