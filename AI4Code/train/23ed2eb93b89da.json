{"cell_type":{"0fcb88d9":"code","7e07e887":"code","147a1ed2":"code","11523785":"code","5bcee854":"code","f8c10da4":"code","b6dd4f51":"code","b7f1c6f8":"code","15494b88":"code","f3952e5f":"code","475d1dd1":"code","9cdaa14b":"code","def74f1e":"code","e80afbea":"code","d6436d7b":"code","0679927e":"code","20b806bc":"code","632569ef":"code","68b10142":"code","0348134b":"code","13fdd7b2":"code","44a18ff1":"code","6c3f8d41":"code","8ef89664":"code","0ed92346":"code","2bff9065":"code","b97fa342":"code","57a8a2be":"code","da2f0530":"code","8afd3709":"code","d3a8ae6f":"code","c5a64655":"code","593afaa3":"code","6649623f":"code","575c16b5":"code","cee7328e":"code","313d23dd":"code","5c66c08e":"code","a147cc29":"code","10938eb6":"code","e92b1975":"code","4a4f1dc4":"code","b70c3961":"code","1228fe83":"code","c838630e":"code","9cd73dad":"code","2b7b86a5":"code","813d3083":"code","f5bab67f":"code","d0582fff":"code","5b37f22d":"code","76566dd2":"code","db8597b6":"code","b5aec59d":"code","16de9648":"code","a76e5bd1":"code","ff5b4797":"code","a09e7110":"code","11dbe34f":"code","6c1c0195":"code","672d39d0":"code","0bb3c284":"code","cce45861":"markdown","483e9d92":"markdown","7e8c41e1":"markdown"},"source":{"0fcb88d9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nfrom keras. utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Flatten\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import *\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import classification_report, confusion_matrix\n","7e07e887":"pd.set_option('display.max_colwidth', -1)","147a1ed2":"df_train = pd.read_csv(\n    '\/kaggle\/input\/nlp-getting-started\/train.csv', \n    usecols=['text', 'target'], \n    dtype={'text': str, 'target': np.int64}\n)\n\nlen(df_train)","11523785":"df_train.head()","5bcee854":"df_test = pd.read_csv(\n    '\/kaggle\/input\/nlp-getting-started\/test.csv', \n    usecols=['text', 'id'], \n    dtype={'text': str, 'id': str}\n)\ndf_test.head()","f8c10da4":"indices = [4415, 4400, 4399,4403,4397,4396, 4394,4414, 4393,4392,4404,4407,4420,4412,4408,4391,4405]\ndf_train.loc[indices]","b6dd4f51":"df_train.loc[indices, 'target'] = 0","b7f1c6f8":"indices = [6840,6834,6837,6841,6816,6828,6831]\ndf_train.loc[indices]","15494b88":"df_train.loc[indices, 'target'] = 0","f3952e5f":"indices = [601,576,584,608,606,603,592,604,591, 587]\ndf_train.loc[indices]","475d1dd1":"df_train.loc[indices, 'target'] = 1","9cdaa14b":"indices = [3913,3914,3936,3921,3941,3937,3938,3136,3133,3930,3933,3924,3917]\ndf_train.loc[indices]","def74f1e":"df_train.loc[indices, 'target'] = 0","e80afbea":"indices = [246,270,266,259,253,251,250,271]\ndf_train.loc[indices]","d6436d7b":"df_train.loc[indices, 'target'] = 0","0679927e":"indices = [6119,6122,6123,6131,6160,6166,6167,6172,6212,6221,6230,6091,6108]\ndf_train.loc[indices]","20b806bc":"df_train.loc[indices, 'target'] = 0","632569ef":"indices = [7435,7460,7464,7466,7469,7475,7489,7495,7500,7525,7552,7572,7591,7599]\ndf_train.loc[indices]","68b10142":"df_train.loc[indices, 'target'] = 0","0348134b":"df_train.shape","13fdd7b2":"sns.countplot(df_train['target'])\nplt.title('Counts of Target')\nplt.show()","44a18ff1":"df_train['len_text'] = df_train['text'].str.split().apply(lambda x: len(x))","6c3f8d41":"df_train.head()","8ef89664":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n\nax1.hist(df_train[df_train['target']==1]['len_text'],color = 'red')\nax1.set_title('Disaster Tweet')\nax2.hist(df_train[df_train['target']==0]['len_text'])\nax2.set_title('Not Disaster Tweet')\nplt.show()","0ed92346":"#Preprocessing\n\n","2bff9065":"ps = PorterStemmer()\ndef preprocess_data(data):\n    review = re.sub(r'https?:\/\/\\S+|www\\.\\S+|http?:\/\/\\S+',' ',data) # remove URL\n    review = re.sub(r'<.*>',' ',review) # remove HTML tags\n    review = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',review)\n    review = re.sub('[^a-zA-Z]',' ',review) # filtering out miscellaneous text.\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(words) for words in review if words not in stopwords.words('english') and words.isalpha()]\n    review = ' '.join(review)\n    return review","b97fa342":"df_train['new_text']= df_train['text'].apply(preprocess_data)","57a8a2be":"df_test['new_text'] = df_test['text'].apply(preprocess_data)","da2f0530":"# Now we have cleaned our data ","8afd3709":"#Lets create WordCloud with common words for Disaster and Not Disaster texts","d3a8ae6f":"wc = WordCloud(background_color = 'white')\nwc.generate(' '.join(df_train[df_train['target']==1]['new_text']))\nplt.imshow(wc,interpolation = 'bilinear')\nplt.title('Real Disaster')\nplt.axis('off')\nplt.show()","c5a64655":"wc1= WordCloud(background_color = 'white')\nwc1.generate(' '.join(df_train[df_train['target']==0]['new_text']))\nplt.imshow(wc1,interpolation = 'bilinear')\nplt.title('Not Disaster')\nplt.axis('off')\nplt.show()","593afaa3":"#Lets analyse top 50 words of Disaster and Not Disaster Text","6649623f":"disaster_tweet = df_train[df_train['target']==1]['new_text']\nnotdisaster_tweet = df_train[df_train['target']==0]['new_text']","575c16b5":"series_disaster = pd.Series(' '.join([i for i in disaster_tweet]).split())\nseries_disaster_top = series_disaster.value_counts().head(50)","cee7328e":"series_disaster_top.plot(kind = 'bar',figsize = (20,20))\nplt.title('Disaster Tweet')\nplt.show()","313d23dd":"series_not_disaster_top = pd.Series(' '.join([i for i in notdisaster_tweet ]).split())\nseries_not_disaster_top = series_not_disaster_top.value_counts().head(50)","5c66c08e":"series_not_disaster_top.plot(kind = 'bar',figsize = (20,20))\nplt.title('Not Disaster')\nplt.show()","a147cc29":"common_words = set(series_disaster_top.index).intersection(set(series_not_disaster_top.index))","10938eb6":"def text_cleaning(data):\n    review = ' '.join([i for i in data.split() if i not in common_words])\n    return review","e92b1975":"df_train['new_text'] = df_train['new_text'].apply(text_cleaning)\ndf_test['new_text'] = df_test['new_text'].apply(text_cleaning)","4a4f1dc4":"df_train['target'] = df_train['target'].astype('category')\ndf_train['target'] = df_train['target'].cat.codes\ndf_train_target = to_categorical([df_train['target']])","b70c3961":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train['new_text'])\ndf_train_text = tokenizer.texts_to_sequences(df_train['new_text'])\ndf_train_text = pad_sequences(df_train_text, maxlen = 120)","1228fe83":"tokenizer1 = Tokenizer()\ntokenizer.fit_on_texts(df_train['new_text'])\ndf_test_text = tokenizer.texts_to_sequences(df_test['new_text'])\ndf_test_text = pad_sequences(df_test_text,maxlen = 120)","c838630e":"df_train_text_train = df_train_text[:6500]\ndf_train_text_test = df_train_text[6500:]\ndf_train_target_train = df_train_target[:6500]\ndf_train_target_test = df_train_target[6500:]","9cd73dad":"df_train_target_train = df_train_target[0][:6500]\n","2b7b86a5":"df_train_target_test = df_train_target[0][6500:]","813d3083":"#Start to establish Deep learning model\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim = len(tokenizer.word_index) + 1,input_length = 120,output_dim =120))","f5bab67f":"model.add(Dropout(0.35))\nmodel.add(LSTM(120))\nmodel.add(Dropout(0.35))\nmodel.add(Dense(32,activation='relu'))\n\nmodel.add(Dense(2,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy'])\nprint(model.summary())","d0582fff":"early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, \n                           mode='min', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='min')","5b37f22d":"# Training the model\n\nhistory = model.fit(x = df_train_text_train,y = df_train_target_train,validation_data=(df_train_text_test,df_train_target_test),callbacks=[early_stop,reduce_lr],epochs=30,batch_size= 64)","76566dd2":"pred = model.predict(df_test_text)\npred","db8597b6":"pred = [np.argmax(i) for i in pred]","b5aec59d":"df_test['target'] = pred\ndf_test","16de9648":"df = pd.DataFrame(history.history)\ndf","a76e5bd1":"plt.plot(history.history['accuracy'] )\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ff5b4797":"# We see that epoch 4 is the best for prevent overfitting","a09e7110":"print('The accuracy of our model is {}'.format(model.evaluate(df_train_text_test,df_train_target_test)[1]))","11dbe34f":"train_pred = model.predict(df_train_text_train)\ntrain_pred","6c1c0195":"train_pred =  [np.argmax(i)for i in train_pred]\n","672d39d0":"confusion_matrix(df_train.target[:6500],train_pred)","0bb3c284":"submission = df_test[['id','target']]\nsubmission.to_csv(\"Submission.csv\",index=False)\nsubmission","cce45861":"## Load Data\n\nNow I'll load the training dataset. ","483e9d92":"Recently I have been learning about RNNs (Recurrent Neural Networks) and NLP (Natural Language Processing) through Andrew Ngs excellent \"Sequence Models\" course on Coursera ([link](https:\/\/www.coursera.org\/learn\/nlp-sequence-models)). I wanted to have a go implementing a language model using this knowledge and Tensorflow v2.\n\nI picked the \"Real or Not? NLP with Disaster Tweets\" ([link](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview)) getting started competition for its straight forward task (label tweets as either reporting a disaster or not reporting disaster) and the size of the dataset (large enough to contain enough information for the model but not so much that there will be a lot of processing).\n\nFirst things first then, let's load the libraries.","7e8c41e1":"Mislabelled examples\nThere are a number of examples in the training dataset that are mislabelled. The keyword can be used to find these.\n\nThanks to Dmitri Kalyaevs whose notebook is where I found to do this: https:\/\/www.kaggle.com\/dmitri9149\/transformer-svm-semantically-identical-tweets"}}