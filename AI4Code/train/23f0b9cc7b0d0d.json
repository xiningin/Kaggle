{"cell_type":{"192d1ef8":"code","14d5103d":"code","8bfa87eb":"code","d50cbdc5":"code","de42dd77":"code","9609f1b4":"code","539d25d5":"code","f1f6722f":"code","f185f8a0":"code","57f12d70":"code","2f8d3f7a":"code","33348519":"code","a04081db":"code","e69a6227":"code","c36faeee":"code","04b41660":"code","e5544fc1":"code","f463bde7":"code","97230015":"code","fb6b1c5c":"code","c5be8820":"code","d0764d8b":"code","f5648396":"code","9685b8cf":"code","7f3778f3":"code","ddd9e807":"code","8f910af6":"code","bd47f6bc":"code","e502e39e":"code","1ecdb9a1":"code","e174f876":"code","1a6718d3":"code","459f142d":"code","b6079860":"code","dc0790dd":"code","6811dd5c":"code","bad4d5be":"code","efb67aad":"code","4c14fb7b":"code","e8787863":"code","2df32181":"code","408851c3":"code","2afddf2e":"code","ef50237e":"code","33365a14":"code","04a6761a":"code","a4611868":"code","e7c982bf":"code","154d7788":"code","34fa38f6":"code","7504ac52":"code","fb8be265":"code","7ec6628c":"code","6ccb2d55":"code","6359599a":"code","30fcb6c7":"code","b57cdc78":"code","19d5778f":"code","bac341b5":"code","863f6f62":"code","57295701":"code","38e6e805":"code","e661e3f2":"code","974051af":"code","7e05ee88":"code","04806fcb":"code","c59e4e75":"code","72671f30":"code","0bf89476":"code","65517b77":"code","4253ea73":"code","13dde13e":"code","62005791":"markdown","28ab0427":"markdown","3345bd9c":"markdown","09849507":"markdown","c64dafe4":"markdown","7f23820b":"markdown","d5744a8d":"markdown","c2e3ce68":"markdown","413089b3":"markdown","a6d76dc7":"markdown","0415feed":"markdown","af2c359d":"markdown","7aef4f0c":"markdown","00eb1040":"markdown","15b4bc68":"markdown","5af2380f":"markdown","b0266b90":"markdown","28b1044f":"markdown","5bf1f78c":"markdown","a0cdfb02":"markdown","65d0e305":"markdown","17bfc201":"markdown","86148421":"markdown","6872c352":"markdown","b6b55775":"markdown","810d6a68":"markdown","a7eccadf":"markdown","54a08934":"markdown","f34469cf":"markdown","a5360999":"markdown","3b349eaf":"markdown","474a1915":"markdown","784dc6ac":"markdown","8731bc56":"markdown","1c016743":"markdown","d32a39e9":"markdown","f8f800f2":"markdown","abb2b70c":"markdown","206e3175":"markdown","d85a5f3c":"markdown","18dcf595":"markdown","4902221e":"markdown","471b4ac5":"markdown","d23fee67":"markdown","74dbd753":"markdown","3b742967":"markdown","3dd5da1a":"markdown","8b49fd4d":"markdown","9a915711":"markdown","69468330":"markdown","5bbdb82f":"markdown","2d735ae2":"markdown","eb7f8454":"markdown"},"source":{"192d1ef8":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 90)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nPALETTE = ['#dd4124','#009473', '#b4b4b4', '#336b87']\nBACKCOLOR = '#f6f5f5'\nsns.set_palette(PALETTE)\n\nfrom scipy.stats import norm, probplot, skew\nfrom scipy.special import boxcox1p\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold\nfrom sklearn.model_selection import RepeatedKFold, train_test_split, RandomizedSearchCV\nfrom sklearn.neighbors import  KNeighborsClassifier as knn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix\n\nfrom IPython.core.display import HTML\n\nimport matplotlib as mpl\nmpl.rcParams['figure.dpi'] = 120\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","14d5103d":"# Import training and test data.\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# When exploring a dataset, it is recommended to use the entire data.\nall_data = pd.concat((train, test)).reset_index(drop=True)","8bfa87eb":"all_data.head(10)","d50cbdc5":"all_data.info()","de42dd77":"def multi_table(table_list):\n    return HTML(\n        f\"<table><tr> {''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list])} <\/tr><\/table>\")","9609f1b4":"multi_table([pd.DataFrame(all_data[i].value_counts()) for i in all_data.columns])","539d25d5":"numerical_vars = ['Age', 'SibSp', 'Parch', 'Fare']\nordinal_vars = ['Pclass']\nnominal_vars = ['Survived', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']","f1f6722f":"train0 = train[train.Survived == 0]\ntrain1 = train[train.Survived == 1]\ncnt = 0\ndetail_desc = []\nfor c in train.columns:\n    if c == 'PassengerId':\n        continue\n    if train[c].dtypes != 'object':\n        desc = pd.DataFrame(columns=['feature', 'data', 'type', 'count', 'mean', 'median', 'std', 'min', 'max', 'skew', 'null'])\n        desc.loc[0] = [c, 'Train', train[c].dtype.name, train[c].count(), train[c].mean(), train[c].median(), train[c].std(), train[c].min(), train[c].max(), train[c].skew(), train[c].isnull().sum()]\n        desc.loc[1] = [c, 'All', train[c].dtype.name, all_data[c].count(), all_data[c].mean(), all_data[c].median(), all_data[c].std(), all_data[c].min(), all_data[c].max(), all_data[c].skew(), all_data[c].isnull().sum()]\n        desc.loc[2] = [c, 'Target=0', train0[c].dtype.name, train0[c].count(), train0[c].mean(), train0[c].median(), train0[c].std(), train0[c].min(), train0[c].max(), train0[c].skew(), train0[c].isnull().sum()]      \n        desc.loc[3] = [c, 'Target=1', train1[c].dtype.name, train1[c].count(), train1[c].mean(), train1[c].median(), train1[c].std(), train1[c].min(), train1[c].max(), train1[c].skew(), train1[c].isnull().sum()]\n        desc = desc.set_index(['feature', 'data'],drop=True)\n        detail_desc.append(desc.style.background_gradient())","f185f8a0":"train0 = train[train.Survived == 0]\ntrain1 = train[train.Survived == 1]\ncnt = 0\ndetail_desc = []\nfor c in train.columns:\n    if c == 'PassengerId':\n        continue\n    if train[c].dtypes == 'object':\n        desc = pd.DataFrame(columns=['feature', 'data', 'type', 'count', 'null', 'mode', 'value_count'])\n        desc.loc[0] = [c, 'Train', train[c].dtype.name, train[c].count(), train[c].isnull().sum(), train[c].mode(), train[c].value_counts()]\n        #desc.loc[1] = [c, 'All', train[c].dtype.name, all_data[c].count(), all_data[c].mean(), all_data[c].median(), all_data[c].std(), all_data[c].min(), all_data[c].max(), all_data[c].skew(), all_data[c].isnull().sum()]\n        #desc.loc[2] = [c, 'Target=0', train0[c].dtype.name, train0[c].count(), train0[c].mean(), train0[c].median(), train0[c].std(), train0[c].min(), train0[c].max(), train0[c].skew(), train0[c].isnull().sum()]      \n        #desc.loc[3] = [c, 'Target=1', train1[c].dtype.name, train1[c].count(), train1[c].mean(), train1[c].median(), train1[c].std(), train1[c].min(), train1[c].max(), train1[c].skew(), train1[c].isnull().sum()]\n        desc = desc.set_index(['feature', 'data'],drop=True)\n        detail_desc.append(desc.style.background_gradient())","57f12d70":"multi_table(detail_desc)","2f8d3f7a":"total_cnt = train.Survived.count()\nf, ax = plt.subplots(1, 1, figsize=(10, 5))\nsns.countplot(train.Survived, edgecolor='black', linewidth=4, ax=ax)\nax.set_xlabel('Survived', weight='bold', size=13)\nax.set_ylabel('Count', weight='bold', size=13)\nax.set_facecolor(BACKCOLOR)\nax.spines[['top', 'right']].set_visible(False)\nax.set_title(f\"Survived's distribution\", size=15, weight='bold')\nfor patch in ax.patches:\n    x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n    ax.text(x + width \/ 2, height + 5, f'{height} \/ {height \/ total_cnt * 100:2.2f}%', va='center', ha='center', size=15, bbox={'facecolor': 'white', 'boxstyle': 'round'})\n\nplt.show()","33348519":"f, ax = plt.subplots(1,1,figsize=(15, 5))\nax.pie(train.Survived.value_counts(),\n       explode=[.01,.01],\n       labels=['NonSurvived', 'Survived'],\n       autopct='%1.1f%%',\n       \n)\nplt.show()","a04081db":"def cat_dist(data, var, hue, msg_show=True):\n    total_cnt = data[var].count()\n    f, ax = plt.subplots(1, 2, figsize=(25, 8))\n    hues = [None, hue]\n    titles = [f\"{var}'s distribution\", f\"{var}'s distribution by {hue}\"]\n\n    for i in range(2):\n        sns.countplot(data[var], edgecolor='black', hue=hues[i], linewidth=4, ax=ax[i], data=data)\n        ax[i].set_xlabel(var, weight='bold', size=13)\n        ax[i].set_ylabel('Count', weight='bold', size=13)\n        ax[i].set_facecolor(BACKCOLOR)\n        ax[i].spines[['top', 'right']].set_visible(False)\n        ax[i].set_title(titles[i], size=15, weight='bold')\n        for patch in ax[i].patches:\n            x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n            if msg_show:\n                ax[i].text(x + width \/ 2, height + 3, f'{height} \\n({height \/ total_cnt * 100:2.2f}%)', va='center', ha='center', size=12, bbox={'facecolor': 'white', 'boxstyle': 'round'})\n    plt.show()","e69a6227":"cat_dist(train, var='Pclass', hue='Survived')\ntrain.pivot_table(index=\"Pclass\", values=\"Survived\", aggfunc=['count', 'sum', 'mean']).style.background_gradient(vmin=0)","c36faeee":"cat_dist(train, var='Sex', hue='Survived')\ntrain.pivot_table(index='Sex', values='Survived', aggfunc=['count', 'sum', 'mean']).style.background_gradient(vmin=0)","04b41660":"# Heatmap can visualize continuous values (or binary variables) in categories and categories.\nplt.subplots(figsize=(10, 5))\ng = sns.heatmap(train.pivot_table(index='Pclass', columns='Sex', values='Survived'), annot=True, cmap=\"YlGnBu\")\ng.set_title('Survived ratio by Pclass and Sex', weight='bold', size=15)\ng.set_xlabel('Sex', weight='bold', size=13)\ng.set_ylabel('Pclass', weight='bold', size=13)\nplt.show()\n\npd.crosstab([train.Sex, train.Survived], train.Pclass,margins=True).style.background_gradient()","e5544fc1":"def continuous_dist(data, x, y):\n    f, ax = plt.subplots(1, 3, figsize=(35, 10))\n    sns.violinplot(x=data[x], y=data[y], ax=ax[0], edgecolor='black', linewidth=5)\n    sns.boxplot(x=data[x], y=data[y], ax=ax[1])\n    sns.stripplot(x=data[x], y=data[y], ax=ax[2])\n    for i in range(3):\n        ax[i].spines[['top','right']].set_visible(False)\n        ax[i].set_xlabel(x, weight='bold', size=20)\n        ax[i].set_ylabel(y, weight='bold', size=20)\n        ax[i].set_facecolor(BACKCOLOR)\n    f.suptitle(f\"{y}'s distribution by {x}\", weight='bold', size=25)\n    plt.show()","f463bde7":"f, ax = plt.subplots(1, 2, figsize=(25, 5))\nsns.distplot(train.Age, ax=ax[0])\n# sns.distplot(train.loc[train.Survived == 0, 'Age'], ax=ax[1])\n# sns.distplot(train.loc[train.Survived == 1, 'Age'], ax=ax[1])\nsns.histplot(data=train, x='Age', hue='Survived', ax=ax[1], element='step')\n\nfor i in range(2):\n    ax[i].spines[['top','right']].set_visible(False)\n    ax[i].set_xlabel('Age', weight='bold', size=15)\n    ax[i].set_ylabel('Density', weight='bold', size=15)\n    ax[i].set_facecolor(BACKCOLOR)\nf.suptitle(\"Age' distribution\", weight='bold', size=20)\nplt.show()\n\ncontinuous_dist(train, x='Survived', y='Age')","97230015":"import copy\n\ntmp_train = copy.deepcopy(train)\ntmp_train['AgeBin'] = 6\nfor i in range(6):\n    tmp_train.loc[(tmp_train.Age >= 10*i) & (tmp_train.Age < 10*(i + 1)), 'AgeBin'] = i\ntmp_train.head(3)","fb6b1c5c":"t0 = pd.pivot_table(index='AgeBin', values='Survived', data=tmp_train).style.background_gradient()\nt1 = pd.pivot_table(index='Pclass', columns='AgeBin', values='Survived', data=tmp_train).style.background_gradient()\nt2 = pd.crosstab([tmp_train.AgeBin, tmp_train.Pclass], [tmp_train.Sex, tmp_train.Survived],margins=True).style.background_gradient(vmax=100)\nt3 = pd.pivot_table(index='Sex', columns='AgeBin', values='Survived', data=tmp_train).style.background_gradient()\nmulti_table([t2, t0, t1, t3])","c5be8820":"cat_dist(tmp_train, var='AgeBin', hue='Survived', msg_show=False)","d0764d8b":"all_data.corr().Age","f5648396":"continuous_dist(train, x='Pclass', y='Age')","9685b8cf":"f, ax = plt.subplots(2, 2, figsize=(25, 10))\nsns.distplot(train.SibSp, ax=ax[0][0])\nsns.histplot(data=train, x='SibSp', hue='Survived', ax=ax[0][1], element='step')\n\nsns.distplot(train.Parch, ax=ax[1][0])\nsns.histplot(data=train, x='Parch', hue='Survived', ax=ax[1][1], element='step')\n\nfor i in range(4):\n    ax[i\/\/2][i%2].spines[['top','right']].set_visible(False)\n    if i < 2:\n        ax[i\/\/2][i%2].set_xlabel('SibSp', weight='bold', size=10)\n    else:\n        ax[i\/\/2][i%2].set_xlabel('Parch', weight='bold', size=10)\n    ax[i\/\/2][i%2].set_ylabel('Density', weight='bold', size=10)\n    ax[i\/\/2][i%2].set_facecolor(BACKCOLOR)\nf.suptitle(\"SibSp and Parch' distribution\", weight='bold', size=20)\nplt.show()","7f3778f3":"continuous_dist(train, x='Survived', y='SibSp')","ddd9e807":"continuous_dist(train, x='Survived', y='Parch')","8f910af6":"t0 = pd.pivot_table(index='SibSp', values='Survived', data=train).style.bar()\nt1 = pd.pivot_table(index='Parch', values='Survived', data=train).style.bar()\nt2 = pd.pivot_table(index='SibSp', columns='Parch', values='Survived', data=train).style.bar()\nmulti_table([t0, t1, t2])","bd47f6bc":"f, ax = plt.subplots(1, 3, figsize=(25, 5))\nsns.distplot(train.Fare, ax=ax[0])\nsns.histplot(data=train, x='Fare', hue='Survived', ax=ax[1], element='step')\nsns.histplot(data=train[train.Fare < 300], x='Fare', hue='Survived', ax=ax[2], element='step')\n\nfor i in range(3):\n    ax[i].spines[['top','right']].set_visible(False)\n    ax[i].set_xlabel('Fare', weight='bold', size=15)\n    ax[i].set_ylabel('Density', weight='bold', size=15)\n    ax[i].set_facecolor(BACKCOLOR)\nf.suptitle(\"Fare' distribution\", weight='bold', size=20)\nplt.show()","e502e39e":"continuous_dist(train, x='Survived', y='Fare')","1ecdb9a1":"tmp_train = copy.deepcopy(train)\ntmp_train['FareBin'] = pd.cut(tmp_train.Fare, 10)\ntmp_train['FareBin'] = LabelEncoder().fit_transform(tmp_train.FareBin)\ntmp_train.head(3)","e174f876":"cat_dist(tmp_train, var='FareBin', hue='Survived', msg_show=False)","1a6718d3":"plt.subplots(figsize=(15, 6))\ng = sns.countplot('FareBin', hue='Pclass', data=tmp_train)\ng.set_title('Count by FareBin and Pclass', weight='bold', size=20)\ng.spines[['top','right']].set_visible(False)\ng.set_xlabel('FareBin', weight='bold', size=15)\ng.set_ylabel('Pclass', weight='bold', size=15)\ng.set_facecolor(BACKCOLOR)\nplt.show()","459f142d":"pd.pivot_table(index='FareBin', columns='Pclass', values='Survived', data=tmp_train).style.bar()","b6079860":"cat_dist(train, var='Embarked', hue='Survived')","dc0790dd":"pd.pivot_table(data=train, index='Embarked', values='Survived', aggfunc=['count', 'sum', 'mean']).style.background_gradient()","6811dd5c":"tmp_all_data = copy.deepcopy(all_data)\nt0 = pd.DataFrame(tmp_all_data.Name)\nt1 = pd.DataFrame(tmp_all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip()).value_counts())\nmulti_table([t0, t1])","bad4d5be":"tmp_all_data['Title'] = tmp_all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#tmp_all_data['Title'] = tmp_all_data.Title.apply(lambda x: 'Others' if x in list(tmp_all_data.Title.value_counts()[tmp_all_data.Title.value_counts() < 8].index) else x)","efb67aad":"continuous_dist(tmp_all_data, x='Title', y='Age')","4c14fb7b":"tmp_train.Cabin.value_counts()","e8787863":"tmp_train['CabinCnt'] = tmp_train.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ntmp_train['CabinClass'] = tmp_train.Cabin.apply(lambda x: str(x)[0])","2df32181":"t0 = pd.DataFrame(tmp_train.CabinCnt.value_counts())\nt1 = pd.DataFrame(tmp_train.CabinClass.value_counts())\nmulti_table([t0, t1])","408851c3":"cat_dist(tmp_train, var='CabinCnt', hue='Survived', msg_show=False)","2afddf2e":"cat_dist(tmp_train, var='CabinClass', hue='Survived', msg_show=False)","ef50237e":"tmp_train.Ticket","33365a14":"tmp_train['IsNumericTicket'] = tmp_train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntmp_train['TicketType'] = tmp_train.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)","04a6761a":"cat_dist(tmp_train, var='IsNumericTicket', hue='Survived')","a4611868":"pd.pivot_table(data=tmp_train, index='TicketType', values='Survived').T.style.background_gradient(axis=1)","e7c982bf":"# missing values\nall_data['Age'] = all_data.Age.fillna(train.Age.median())\nall_data['Fare'] = all_data.Fare.fillna(train.Fare.median())\nall_data.dropna(subset=['Embarked'], inplace=True)\ncabins = all_data.Cabin\nall_data.drop(['Cabin'], axis=1, inplace=True)","154d7788":"# derivative features\nall_data['CabinCnt'] = cabins.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['CabinClass'] = cabins.apply(lambda x: str(x)[0])\nall_data['IsNumericTicket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['TicketType'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)\nall_data['Title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\nall_data['Family'] = all_data.SibSp + all_data.Parch","34fa38f6":"# feature transform\nnumeric_vars = ['Age', 'SibSp', 'Parch', 'Fare', 'CabinCnt', 'Family']\nordinal_vars = ['Pclass']\nnominal_vars = ['Name', 'Sex', 'Ticket', 'Embarked', 'CabinClass', 'IsNumericTicket', 'TicketType', 'Title']\nall_data[nominal_vars] = all_data[nominal_vars].astype('str')\n\nfor feature in numeric_vars:\n    all_data[feature] = np.log1p(all_data[feature])\n\nscaler = StandardScaler()\nnumeric_vars = all_data.columns[(all_data.dtypes != 'object') & (all_data.columns != 'PassengerId') & (all_data.columns != 'Survived') & (all_data.columns != 'IsTrain')]\nall_data[numeric_vars] = scaler.fit_transform(all_data[numeric_vars])","7504ac52":"# split data\nall_data.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\ndata_dummies = pd.get_dummies(all_data)\nX_train = data_dummies[data_dummies.Survived.notnull()].drop(['Survived'], axis=1)\ny_train = data_dummies[data_dummies.Survived.notnull()].Survived\nX_test = data_dummies[data_dummies.Survived.isnull()].drop(['Survived'], axis=1)","fb8be265":"X_train.shape, y_train.shape, X_test.shape","7ec6628c":"selector = RandomForestClassifier().fit(X_train, y_train)\n\nimps = pd.DataFrame(selector.feature_importances_, X_train.columns, columns=['Importance'])\nimps = pd.DataFrame(imps.Importance.sort_values(ascending=False))\n\nplt.subplots(figsize=(20, 10))\ng = sns.barplot(x=imps.index, y=imps.Importance)\ng.set_xticklabels(g.get_xticklabels(),rotation = 90)\nplt.show()","6ccb2d55":"all_data.Title = all_data.Title.apply(lambda x: 'Others' if x in list(all_data.Title.value_counts()[all_data.Title.value_counts() < 8].index) else x)\nall_data.TicketType = all_data.TicketType.apply(lambda x: 'Others' if x in list(all_data.TicketType.value_counts()[all_data.TicketType.value_counts() < 10].index) else x)","6359599a":"# split data2\ndata_dummies = pd.get_dummies(all_data)\nX_train = data_dummies[data_dummies.Survived.notnull()].drop(['Survived'], axis=1)\nX_test = data_dummies[data_dummies.Survived.isnull()].drop(['Survived'], axis=1)","30fcb6c7":"selector = RandomForestClassifier().fit(X_train, y_train)\n\nimps = pd.DataFrame(selector.feature_importances_, X_train.columns, columns=['Importance'])\nimps = pd.DataFrame(imps.Importance.sort_values(ascending=False))\n\nplt.subplots(figsize=(20, 10))\ng = sns.barplot(x=imps.index, y=imps.Importance)\ng.set_xticklabels(g.get_xticklabels(),rotation = 50)\nplt.show()","b57cdc78":"# Hyperparameter tuning takes a lot of time. If this variable is False, the tuning process will be omitted and the learning will proceed \n# with the hyperparameters already obtained. If this variable is true, you can proceed with the tuning process directly.\nallow_tuning = False","19d5778f":"# This function is a function created by myself to eliminate repeated code generated by tuning XGBoost.\n# params_grid_xgb: Combines fixed parameters for grid search in xgboost.\n# features: Target features to be tuned using this function\n# values: Search parameters for each feature\n# X,y: Datasets.\n# last: If this value is false, change each value of the GridSearchCV object's best_params to a list for immediate use in the next adjustment.\ndef xgb_gridsearch(params_grid_xgb, features, values, X, y, last=False):\n    x_train, x_test = train_test_split(X, test_size=.2, random_state=42)\n    y_train_tmp, y_test_tmp = train_test_split(y, test_size=.2, random_state=42)\n\n    cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 42)\n\n    model_xgb = XGBClassifier(use_label_encoder = False, \n                              objective = 'binary:logistic')\n    \n    for i in range(len(features)):\n        params_grid_xgb[features[i]] = values[i]\n    search_xgb = GridSearchCV(model_xgb, params_grid_xgb, verbose = 0,\n                              scoring = 'neg_log_loss', cv = cv).fit(x_train, y_train_tmp, early_stopping_rounds = 15, \n                                  eval_set = [[x_test, y_test_tmp]], \n                                  eval_metric = 'logloss', verbose = False)\n    for i in range(len(features)):\n        print(f\"{features[i]}: {search_xgb.best_params_[features[i]]}\")\n    if not last:\n        for k, v in search_xgb.best_params_.items():\n            search_xgb.best_params_[k] = [v]\n    return search_xgb, search_xgb.best_params_","bac341b5":"if allow_tuning:\n    params_knn = {\n        'n_neighbors' : range(1, 10),\n        'weights' : ['uniform', 'distance'],\n        'algorithm' : ['auto', 'ball_tree','kd_tree'],\n        'p' : [1,2]\n    }\n    model_knn = knn()\n    search_knn = GridSearchCV(model_knn, params_knn, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, y_train)\n    print(search_knn.best_params_)","863f6f62":"if allow_tuning:\n    params_logistic = {\n        'max_iter': [2000],\n        'penalty': ['l1', 'l2'],\n        'C': np.logspace(-4, 4, 20),\n        'solver': ['liblinear']\n    }\n    model_logistic = LogisticRegression()\n    search_logistic = GridSearchCV(model_logistic, params_logistic, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, y_train)\n    print(search_logistic.best_params_)","57295701":"if allow_tuning:\n    params_svc = [{'kernel': ['rbf'], 'gamma': [.01, .1, .5, 1, 2, 5, 10], 'C': [.1, 1, 10, 100, 1000], 'probability': [True]},\n                  {'kernel': ['poly'], 'degree' : [2, 3, 4, 5], 'C': [.01, .1, 1, 10, 100, 1000], 'probability': [True]}]\n    model_svc = SVC()\n    search_svc = GridSearchCV(model_svc, params_svc, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, y_train)\n    print(search_svc.best_params_)","38e6e805":"if allow_tuning:\n    params_svc = {'kernel': ['rbf'], 'gamma': [i\/10000 for i in range(90, 110)], 'C': range(50, 80, 10), 'probability': [True]}\n    model_svc = SVC()\n    search_svc = GridSearchCV(model_svc, params_svc, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, y_train)\n    print(search_svc.best_params_)","e661e3f2":"# if allow_tuning:\n#     params_rf = {\n#         'n_estimators': range(100, 2000, 200),\n#         'criterion':['gini','entropy'],\n#         'bootstrap': [True, False],\n#         'max_depth': list(range(5, 100, 5)) + [None],\n#         'max_features': ['auto','sqrt', 5, 10],\n#         'min_samples_leaf': range(2, 11, 2),\n#         'min_samples_split': range(2, 11, 2)}\n#     model_rf = RandomForestClassifier()\n#     search_rf = RandomizedSearchCV(model_rf, params_rf, cv=5,\n#                                    scoring='accuracy', n_jobs=-1, verbose=1,\n#                                    n_iter=100).fit(X_train, y_train)\n#     print(search_rf.best_params_)","974051af":"if allow_tuning:\n    params_rf = {\n        'n_estimators': [95, 100, 105],\n        'criterion':['entropy'],\n        'bootstrap': [True, False],\n        'max_depth': [40, 45, 50],\n        'max_features': [4, 5, 6],\n        'min_samples_leaf': [1, 2, 3],\n        'min_samples_split': [9, 10, 11],\n        'random_state': [734]}\n    model_rf = RandomForestClassifier()\n    search_rf = GridSearchCV(model_rf, params_rf, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, y_train)\n    search_rf.best_params_['random_state']=242\n    search_rf.best_estimator_.random_state=242\n    print(search_rf.best_params_)","7e05ee88":"if allow_tuning:\n    # Initial params.\n    params_xgb = {'n_estimators': [1000],\n                  'learning_rate': [0.1],\n                  'max_depth': [5],\n                  'min_child_weight': [1],\n                  'gamma': [0],\n                  'subsample': [0.8],\n                  'colsample_bytree': [0.8],\n                  'n_jobs': [-1],\n                  'objective': ['binary:logistic'],\n                  'use_label_encoder': [False],\n                  'eval_metric': ['logloss'],\n                  'scale_pos_weight': [1]}\n    \n    # learning rate tuning.\n    search_xgb, params_xgb = xgb_gridsearch(params_xgb, \n                                            ['learning_rate'], \n                                            [[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2]],\n                                            X_train, y_train)\n    # max_depth, min_child_weight tuning.\n    search_xgb, params_xgb = xgb_gridsearch(params_xgb,\n                                            ['max_depth', 'min_child_weight'],\n                                            [range(3, 10), range(1, 6)],\n                                            X_train, y_train)\n    \n    # gamma tuning.\n    search_xgb, params_xgb = xgb_gridsearch(params_xgb,\n                                            ['gamma'],\n                                            [[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2]],\n                                            X_train, y_train)\n    search_xgb, params_xgb = xgb_gridsearch(params_xgb,\n                                            ['subsample', 'colsample_bytree'],\n                                            [[i\/100.0 for i in range(75,90,5)], [i\/100.0 for i in range(75,90,5)]],\n                                            X_train, y_train)\n    \n    # reg_alpha tuning.\n    search_xgb, params_xgb = xgb_gridsearch(params_xgb,\n                                            ['reg_alpha'], \n                                            [[1e-5, 1e-2, 0.1, 1, 100]], \n                                            X_train, y_train)\n    \n    # learning rate re tuning.\n    params_xgb['n_estimators'] = [5000]\n    search_xgb, params_xgb = xgb_gridsearch(params_xgb,\n                                            ['learning_rate'],\n                                            [[0.001, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2]],\n                                            X_train, y_train, last=True)\n\n    x_train, x_test = train_test_split(X_train, test_size=.2, random_state=42)\n    y_train_tmp, y_test_tmp = train_test_split(y_train, test_size=.2, random_state=42)\n    model_xgb = XGBClassifier(**params_xgb)\n    \n    # n_estimators tuning.\n    model_xgb = model_xgb.fit(x_train, y_train_tmp, eval_set=[(x_test, y_test_tmp)], eval_metric=['logloss'], early_stopping_rounds=15, verbose=0)\n    search_xgb.best_estimator_.n_estimators = model_xgb.best_iteration","04806fcb":"if allow_tuning:\n    model_knn = search_knn.best_estimator_\n    model_logistic = search_logistic.best_estimator_\n    model_svc = search_svc.best_estimator_\n    model_rf = search_rf.best_estimator_\n    model_xgb = search_xgb.best_estimator_\nelse:\n    model_knn = knn(algorithm='auto', \n                    n_neighbors=9,\n                    p=1, \n                    weights='uniform')\n    \n    model_logistic = LogisticRegression(C=0.08858667904100823,\n                                        max_iter=2000, \n                                        penalty='l2', \n                                        solver='liblinear')\n    model_svc = SVC(C=70,\n                    gamma=0.0106,\n                    kernel='rbf',\n                    probability=True)\n    \n    model_rf = RandomForestClassifier(bootstrap=True,\n                                      criterion='entropy',\n                                      max_depth=50, max_features=6, \n                                      min_samples_leaf=1, \n                                      min_samples_split=10, \n                                      n_estimators=100,\n                                      random_state=734)\n    \n    model_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                              colsample_bynode=1, colsample_bytree=0.8,\n                              enable_categorical=False, eval_metric='logloss', gamma=0.8,gpu_id=-1, importance_type=None, interaction_constraints='',\n                              learning_rate=0.15, max_delta_step=0, max_depth=5,\n                              min_child_weight=1, missing=np.nan, monotone_constraints='()',\n                              n_estimators=15, n_jobs=-1, num_parallel_tree=1, predictor='auto',\n                              random_state=0, reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1,\n                              subsample=0.8, tree_method='exact', use_label_encoder=False,\n                              validate_parameters=1, verbosity=0)\n\nmodels = {\n    'knn': model_knn,\n    'logistic': model_logistic,\n    'svc': model_svc,\n    'rf': model_rf,\n    'xgb': model_xgb\n}","c59e4e75":"import copy\n\n# goal: The number of models to combine.\n# estimaors: empty list.\n# voting: voting method.\ndef select_models(start, cnt, goal, estimators, voting):\n    if cnt == goal:\n        estimators_copy = copy.deepcopy(estimators)\n        voting_name = f'{voting}_' + '_'.join([i[0] for i in list(estimators_copy)])\n        models[voting_name] = VotingClassifier(estimators=estimators_copy, voting=voting)\n        return\n    for i in range(start, 5):\n        estimators.append(list(models.items())[i])\n        select_models(i + 1, cnt + 1, goal, estimators, voting)\n        estimators.pop()","72671f30":"# create voting models\nselect_models(0, 0, 2, [], 'hard')\nselect_models(0, 0, 3, [], 'hard')\nselect_models(0, 0, 4, [], 'hard')\nselect_models(0, 0, 5, [], 'hard')\n\nselect_models(0, 0, 2, [], 'soft')\nselect_models(0, 0, 3, [], 'soft')\nselect_models(0, 0, 4, [], 'soft')\nselect_models(0, 0, 5, [], 'soft')","0bf89476":"# Dictionary for storing results for each model.\nresult_by_model = pd.DataFrame({'model name': models.keys(), 'model': models.values(), 'score': 0})","65517b77":"# Cross-validation progresses for all models.\nfor name, model in models.items():\n    result_by_model.loc[result_by_model['model name'] == name, 'score'] = cross_val_score(model, X_train,y_train,cv=5).mean()","4253ea73":"# Cross validation scores of all models.\nresult_by_model.sort_values('score', ascending=False).reset_index(drop=True)","13dde13e":"model_name = 'rf'\nmodels[model_name].fit(X_train, y_train)\ny_pred = models[model_name].predict(X_test).astype('int')\n\nsubmission = pd.DataFrame({'PassengerId': test.PassengerId, \n                              'Survived': y_pred})\n\nsubmission.to_csv('submission.csv', index = False)","62005791":"# Read Data","28ab0427":"## Age","3345bd9c":"## SVC","09849507":"Cabin is rarely recorded and is subject to deletion. However, some observations may have multiple cabins based on blanks. Therefore, we can come up with a new variable called Cabin count. I can replace the missing value of the Cabin count with a certain value, but I easily processed it to 0.\n\nAlso, the first letter of Cabin starts with an alphabet. This value may indicate the type of Cabin. This value can be processed by CabinClass.","c64dafe4":"The process is similar to that of exploring Age earlier. You can see the following:\n\n1) Fare has a certain degree of normality, but very low levels account for a certain percentage.  \n2) Groups with very low Fare have lower survival rates, and the higher the Fare, the higher the survival rate.  \n\n=> Like Age, Fare can be grouped, which is a variable that clearly affects survival.","7f23820b":"Start Feature Engineering based on the contents organized through EDA. Some processes have been simplified (ex: Age Missing Value Processing). You can proceed with this course in many ways as you like.","d5744a8d":"## Sex","c2e3ce68":"## Survived (Dependent, Nominal)","413089b3":"## Voting model\n\nI also tested the voting model suitable for the classification problem. I created a recursive function that allows users to combine as many as they want in a total of five models.","a6d76dc7":"## XGBoost","0415feed":"After visualizing the survival rates of CabinCnt and CabinClass, we can see that these are the variables that can be used.","af2c359d":"## Logistic Regression","7aef4f0c":"I tested all of the above models and found that Random Forest had the highest test score, unlike the cross validation results.","00eb1040":" Reading the meaning of variables has many advantages.    \n  \nYou can see the actual data types of all variables. All variables are separated as numerical, categorical data, and categorical data are separated back to ordinal \/ nominal. However, the variables in the program are divided into (int and float) \/ object. We cannot determine that all int and float variables mean actual countinuous numbers. Reading the data description table and then dividing it directly is a reliable way to distinguish between data types. For example, Pclass is an int type. However, it is an ordinal variable with keys 1st, 2nd, and 3rd. This variable may later be label encoded and scaled. Embarked is an object variable. You don't know if it's ordinal or nominal. If you look at the table, you can be sure that it is a nominal variable with the keys Cherbourg, Queenstown, and Southampton.","15b4bc68":"We can be sure of some facts.\n  \n1) Continuous variables: Age, SibSp, Parch, Fare  \n2) Ordinal variables: Pclass  \n3) Nominal variables: PassengerId, Survived, Name, Sex, Ticket, Cabin, Embarked  \n4) Pclass is already encoded in numbers. Therefore, leave it as it is.  \n5) Name and Cabin are too many items to use.  ","5af2380f":"Through the above process, we can learn the following facts.  \n1) There are 1309 points in total.    \n2) There are a total of 11 variables.  \n3) Age, Fare and Cabin have missing values.  \n4) Numerical variables(int, float): PassengerId, Pclass, Age, SibSp, Parch, and Fare.  \n5) Category variables(object): Name, Sex, Ticket, Cabin, Embarked.","b0266b90":"Create each model based on hyperparameter tuning results. If allow_tuning is False, create a model based on the results tuned in advance.","28b1044f":"## KNN","5bf1f78c":"Through this process, you can learn several facts.\n\n1) The survival rate of single-person passengers without family members is low (34-35%).  \n2) The survival rate of passengers with brothers and sisters above 3 will decrease.  \n3) In the case of passengers who are not with parents or children, the survival rate increases with the number of brothers and sisters, but the survival rate of passengers who are with two parents or children gradually decreases.\n\n=> SibSp and Parch are expected to affect survival rates. Since both variables are family-related variables, it is considered possible to combine them.","a0cdfb02":"Fare's level and Pclass are considered to be correlated, so multivariate searches using two variables were conducted.","65d0e305":"Survived:  \nIt is important to first determine the distribution of dependent variables. You need to know beforehand whether there are biased and unbalanced problems in both classes.\nSurvived is a binary variable consisting of 0 and 1. As a result of drawing the countplot, 0 represents 62% and 1 represents 38%. 0 is more than 1, but one of them is not too biased, so it can be analyzed.","17bfc201":"# Variables explore","86148421":"## Detail Describe (numeric)","6872c352":"I looked more closely at the numerical variables. Basic statistics and missing values of each variable were checked in the training dataset and the entire dataset. And I checked the difference by comparing the statistics when Survived was 0 and 1.","b6b55775":"Creating a good model is as important as creating a good variable. A good model refers to a model with the best generalization performance using hyperparameters optimized for a given dataset. To solve this problem, I used Logistic Regression, Knn, Support Vector Machine, Radnom Forest, XGBoost, and Voting Model. Because each model is suitable for binary classification tasks and has different algorithms and characteristics, learning the same dataset can perform differently. In addition, because each model has a different categorization point for key people, it may be better to combine them.\n\nI used Grid Search to find this model's HYPER PARAMTER. Random Forest hyperparameter tuning is too wide and difficult. It is easier to adjust by narrowing the range using Random Search first. XGBoost has many parameters and a wide range, so tuning is more difficult than other models. Therefore, tuning all hyperparameters at once takes a very long time.\n\nXGBoost has many hyperparameters, but there are parameters that have a relatively significant impact. This includes learning_rate and n_estimators. I took advantage of early_stopping to get the best learning_rate, and then tuned the hyperparameters such as max_depth, min_child_weight, gamma, subsample, etc. After tuning the core parameters, the optimal parameters are derived by final tuning the learning_rate and n_estimators again.","810d6a68":"## Name","a7eccadf":"Hello. Nice to meet you. I'm a student who's been fascinated by the charm of Kaggle lately and studying hard. I studied the good notes of many Kagglers through Titanic examples and tried to organize them in my own way. In particular, I focused on visualization, preprocessing, and model tuning methods that are useful for binary classification. After a lot of trial and error, I got the top 3% test score and decided to write a note for other Kagglers, especially beginners like me.\n  \nThrough this note, you can learn the following.\n  \n  1. Random Forest, XGB Hyperparameter Tuning: You can optimize your model through Random Search, Grid Search, and sequential tuning.\n  2. Ensemble: Voting: Ensemble: Voting: You can use the Voting algorithm to improve the performance of your taxonomic model. I built and verified a total of 52 types of Voting models.\n  3. Visualization for EDA: You can effectively visualize binary sorting problems.\n  4. Basic preprocessing: To solve binary sorting problems, you can learn preprocessing methods such as pruning, generating derivative variables, transforming variables, and selecting variables.\n  \nIf you follow this note and submit the results, you will get a high score (0.806). Hyper parameter tuning takes a lot of time, so I defined a variable called allow_tuning so that users can choose this process. If you want to perform hyperparameter tuning yourself, change allow_tuning to True (it takes a lot of time on the Kagle kernel, so it is recommended that you do-it-yourself environment).","54a08934":"We can see some facts from the above picture.\n\n1) 65% of all passengers are male and 35% are female.  \n2) The survival rate of men (18%) is significantly lower than that of women (74%).  \n\n=> Sex is a key variable in this problem because there are clear differences in survival rates depending on gender.","f34469cf":"|Variable|Definition|Key|\n|------|---|---|\n|Survived|Survival|0 = No, 1= Yes|\n|Pclass|Ticket class|1=1st, 2=2nd, 3=3rd|\n|Sex|Sex||\n|Age|Age in years||\n|SibSp|number of siblings \/ spouses aboard the Titanic||\n|Parch|number of parents \/ children aboard the Titanic||\n|Ticket|Ticket number||\n|Fare|Passenger fare||\n|Cabin|Cabin number||\n|Embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|","a5360999":"## Embarked","3b349eaf":"We can be sure that the survival rate varies from age to age. There are missing values at the current age. Age is a continuous variable, so we can process missing values with central tendency values such as mean and median. However, I don't think this method is a good way because age is a variable that has a lot to do with survival. After I found other variables that seemed to be more relevant to age, I decided to treat unrecorded passengers as the average age of other passengers who had similar characteristics to me.","474a1915":"# Import Modules","784dc6ac":"Women in Pclass 1 are said to have the highest survival rate. On the other hand, men in Pclass 3 have the lowest survival rate.","8731bc56":"I created a simple function using the countplot I used earlier. You can apply any number of seaborn functions.","1c016743":"You can see some facts from the diagram above.\n\nPassengers in their 20s and 40s are the most common.\n2) Age distribution is similar regardless of survival.  \n3) Most passengers in their 50s and older died (violin, boxplot), but the oldest survived (strippplot).  \n4) The survival rate of children is higher than that of those in their 20s and 30s.(You can tell by looking at violin)  \n\n=> If you look at the histogram, the age distribution of survival is similar. I observed more closely through violin plots, box plots, and strip plots, and I was convinced that age would affect survival. In particular, the survival rate of infants is high, and the survival rate of people in their 20s and 30s is low. I felt it was necessary to categorize age variables into sections and see the survival rate by age group more clearly.","d32a39e9":"Check the distribution of Pclasses with relatively high correlation coefficients with Age. The average age is the highest in Pclass1 and the lowest in Pclass3. Through this result, you will be able to process age missing values as average values by Pclass. We haven't generated a derivative variable yet, so we can do this again with the variables that will be added later.","f8f800f2":"## Fare","abb2b70c":"## Pclass","206e3175":"## FareBin and Pclass","d85a5f3c":"## SibSp and Parch","18dcf595":"We can see some facts from the above picture.\n\n1) Embarked accounts for the largest proportion in the order of S, C, and Q.  \n2) S accounts for the largest percentage, but has the lowest survival rate.  \n3) Embarked C has the highest survival rate.  \n\n=> Embarked is also a good variable because the survival rate varies significantly depending on the value.","4902221e":"## Cabin","471b4ac5":"First, use Random Search to narrow the search range, and then proceed with Grid Search. Random search results vary from time to time. Increasing n_iter results in more consistent results.","d23fee67":"# Feature engineering","74dbd753":"## Random Forest","3b742967":"## Ticket","3dd5da1a":"You can extract common keywords (Mr, Miss, etc.) by name.","8b49fd4d":"# Understand each features","9a915711":"Tickets also contain several common keywords. Therefore, new variables can be generated after parsing according to specific criteria.","69468330":"We can see some facts from the above picture.\n\n1) In Pclass, 3 accounts for the largest percentage (55%).  \n2) For Pclass, 2 is the smallest (21%).  \n3) Pclass1 has a larger percentage of survivors.  \n4. Pclass 2 has a higher death rate.  \n5) Pclass3 has an overwhelming death rate.  \n\n=> Most passengers belong to Pclass 3 and most are dead. The percentage of survivors increases from 3 to 1. This variable is the core variable in the classification because the difference in survival rates is obvious depending on the Pclass.","5bbdb82f":"We have determined that we can get hints about age in Title, so we have confirmed the distribution of Age in Title. There seems to be some correlation. It can be used to handle Age missing values.","2d735ae2":"# Modeling","eb7f8454":"## Pclass and Sex"}}