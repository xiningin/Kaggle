{"cell_type":{"9cd35b33":"code","cf708da8":"code","874399a1":"code","e8c6087f":"code","b501a6d1":"code","e0b3bee9":"markdown"},"source":{"9cd35b33":"# Import Required Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\nfrom scipy.stats import mode","cf708da8":"# Read 5 Fold Train, Test and Sample Submission Files\ndf_train = pd.read_csv(\"..\/input\/tps-dec21-5-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\ndf_submission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","874399a1":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","e8c6087f":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","b501a6d1":"useful_features = [c for c in df_train.columns if c not in (\"Id\", \"Cover_Type\", \"kfold\")]\n#cont_cols = [col for col in useful_features if 'Soil_Type' not in col]\n\ndf_train = df_train[df_train.Cover_Type!=5]\n\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\n\nscores = []\n\nfor fold in range(5):\n    xtrain =  df_train[df_train.kfold != fold].reset_index(drop=True)\n    xvalid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    xtest = df_test.copy()\n    \n    # Store IDs of validation Dataset\n    valid_ids = xvalid.Id.values.tolist()\n    \n    #Label encoding Y\n    le = preprocessing.LabelEncoder().fit(xtrain.Cover_Type)\n    \n    ytrain = le.transform(xtrain.Cover_Type)\n    yvalid = le.transform(xvalid.Cover_Type)\n    \n    #Save a copy of yvalid\n    true_valid = xvalid.Cover_Type\n    \n    n_class = len(xtrain.Cover_Type.unique())\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    params = {'learning_rate': 0.03811822061503613, \n              'reg_lambda': 17.136779266696237, \n              'reg_alpha': 1.196532346754796e-05, \n              'subsample': 0.16103284130404089, \n              'colsample_bytree': 0.9165052246716364, \n              'max_depth': 10,\n              'grow_policy': 'depthwise'}\n    \n    model = XGBClassifier(\n        \n        random_state = 42,\n        tree_method='gpu_hist',\n        objective = 'multi:softmax',\n        sampling_method = 'gradient_based',\n        n_estimators=10000,\n        n_jobs=-1,\n        num_class = n_class,\n        use_label_encoder=False,\n        eval_metric = 'mlogloss',\n        **params\n    )\n    model.fit(xtrain, ytrain,early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = le.inverse_transform(model.predict(xvalid))\n    \n    test_preds = le.inverse_transform(model.predict(xtest))\n    \n    final_test_predictions.append(test_preds)\n    \n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    \n    acc_scr = accuracy_score(true_valid, preds_valid)\n    \n    print(fold, acc_scr)\n    \n    scores.append(acc_scr)\n\n\n#final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n#final_valid_predictions.columns = [\"Id\", \"Cover_Type\"]    \n    \ndf_submission.Cover_Type = mode(np.column_stack(final_test_predictions), axis=1)[0]\ndf_submission.columns = [\"Id\", \"Cover_Type\"]\ndf_submission.to_csv(\"submission.csv\", index=False)","e0b3bee9":"### TPS Dec 2021 - Baseline Model\n\n- For modeling, i am using 5 Folds [data](https:\/\/www.kaggle.com\/nitishraj\/tps-dec21-5-folds) created by [Tps-dec-2021-5-folds](https:\/\/www.kaggle.com\/nitishraj\/tps-dec-2021-5-folds)"}}