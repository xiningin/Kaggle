{"cell_type":{"7de04e8f":"code","6522c598":"code","4fd4d8d6":"code","1c3e5c66":"code","cc67cf63":"code","5db78bf8":"code","85201959":"code","5c4c95ed":"code","27a23df2":"code","f52aa1f8":"code","a989c27e":"code","b3f76d34":"code","4e14969e":"code","8d7aca18":"code","40430d1f":"code","6a497b8b":"code","ebebfb57":"code","2cd6409f":"code","33c8ac06":"code","869ec07f":"code","079ca4f6":"code","f17fd415":"code","eeb6b694":"code","18842037":"code","f8e118a3":"code","72ff0031":"code","e04155ba":"code","fb5bf6a1":"code","d057f93e":"code","a2124fc5":"code","03f23f72":"code","274faa20":"code","26af6dfe":"code","4683bd4c":"code","ab227c07":"code","ae86493f":"code","33445bd7":"code","194027ff":"code","426d3999":"code","4390268a":"code","0dd3e803":"code","a5536bab":"code","3d884b94":"code","53f8e04e":"code","d30c1d73":"code","7922a900":"code","c813be78":"code","b5257f61":"code","ba452d1a":"code","3f3f010e":"code","0a96d04d":"code","f23d7b43":"markdown","80953527":"markdown","00179ecc":"markdown","4f76a07a":"markdown","46ca9ba8":"markdown","c3e665a0":"markdown","334257c2":"markdown","7902bd5a":"markdown","fa2fcdfe":"markdown","bbb3f57f":"markdown","aaa5cbd7":"markdown","97f9bfe4":"markdown","f04de46e":"markdown","c792ef84":"markdown","75836038":"markdown","cec1877a":"markdown","4ff8cd4b":"markdown","f2437053":"markdown","38f86f7c":"markdown","a2d7c540":"markdown","df05cf84":"markdown"},"source":{"7de04e8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6522c598":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom scipy.stats import uniform as sp_uniform","4fd4d8d6":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntrain.head()","1c3e5c66":"train_copy = train.copy()","cc67cf63":"test = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest.head()","5db78bf8":"train.drop('Survived', axis=1, inplace = True)","85201959":"data = pd.concat([train,test], ignore_index=True)\ndata.head()","5c4c95ed":"data.shape","27a23df2":"data.isnull().sum()","f52aa1f8":"mean_age = data['Age'].mean()\nmean_age","a989c27e":"data['Age'] = data['Age'].fillna(mean_age)","b3f76d34":"mean_fare = data['Fare'].mean()\nmean_fare","4e14969e":"data['Fare'] = data['Fare'].fillna(mean_fare)","8d7aca18":"data['Ticket'] = data['Ticket'].fillna('X')","40430d1f":"data['Cabin'] = data['Cabin'].fillna('X')","6a497b8b":"mode_embarked = data['Embarked'].mode()[0]\nmode_embarked","ebebfb57":"data['Embarked'] = data['Embarked'].fillna(mode_embarked)","2cd6409f":"data[['First Name','Last Name']] = data.Name.str.split(\",\", expand=True,)","33c8ac06":"data = data.drop(['Name','First Name'], axis=1)","869ec07f":"data.head()","079ca4f6":"data.drop('PassengerId', axis=1,inplace = True)","f17fd415":"data.head()","eeb6b694":"data['Pclass'] = data['Pclass'].astype('object')","18842037":"ordinal = OrdinalEncoder()\nlabel = LabelEncoder()","f8e118a3":"data['Pclass'] = ordinal.fit_transform(data[['Pclass']])","72ff0031":"data['Cabin'] = label.fit_transform(data['Cabin'])","e04155ba":"data['Last Name'] = label.fit_transform(data['Last Name'])","fb5bf6a1":"data['Ticket'] = label.fit_transform(data['Ticket'])","d057f93e":"data_dummies = pd.get_dummies(data[['Sex','Embarked']])","a2124fc5":"data = pd.concat([data,data_dummies], axis=1)\ndata.head()","03f23f72":"data = data.drop(['Sex','Embarked'], axis=1)","274faa20":"train_data = data[0:100000]\ntest_data = data[100000:]","26af6dfe":"train_data = pd.concat([train_data, train_copy['Survived']], axis=1)\ntrain_data.head()","4683bd4c":"X = train_data.iloc[:,0:13]\ny = train_data.iloc[:,13:]","ab227c07":"X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state = 42)","ae86493f":"pipe_log = Pipeline([('scalar', StandardScaler()), ('log', LogisticRegression())])","33445bd7":"pipe_log.fit(X_train,y_train)","194027ff":"score_log = pipe_log.score(X_test,y_test)","426d3999":"pipe_ada = Pipeline([('scalar', StandardScaler()), ('ada', AdaBoostClassifier())])","4390268a":"pipe_ada.fit(X_train,y_train)","0dd3e803":"score_ada = pipe_ada.score(X_test,y_test)","a5536bab":"pipe_xgb = Pipeline([('scalar', StandardScaler()), ('xgb', XGBClassifier())])","3d884b94":"pipe_xgb.fit(X_train,y_train)","53f8e04e":"score_xgb = pipe_xgb.score(X_test,y_test)","d30c1d73":"pipe_lgm = Pipeline([('scalar', StandardScaler()), ('lgm', LGBMClassifier())])","7922a900":"pipe_lgm.fit(X_train,y_train)","c813be78":"score_lgm = pipe_lgm.score(X_test,y_test)","b5257f61":"score_df = pd.DataFrame({\n    'Score': [score_log, score_ada, score_xgb, score_lgm]\n})\nscore_df","ba452d1a":"prediction_lgm = pipe_lgm.predict(test_data)","3f3f010e":"prediction_lgm = pd.DataFrame(prediction_lgm)","0a96d04d":"prediction_lgm.to_csv('prediction_lgm.csv')","f23d7b43":"## Importing Libraries","80953527":"## Combining train and test daatset","00179ecc":"## Dropping Passenger ID feature","4f76a07a":"## Checking the shape of the dataset","46ca9ba8":"## Imputing values in features (Age, Ticket, Fare, Cabin and Embarked)","c3e665a0":"## Checking the null values from the dataset","334257c2":"## Dropping the First Name and Name columns","7902bd5a":"## Splitting the data into train and test from training dataset","fa2fcdfe":"## Reading Training and Testing data","bbb3f57f":"## Changing Pclass feature type from integer to object","aaa5cbd7":"## Creating a pipeline for XGBoost Classifier","97f9bfe4":"## Creating a pipeline for Logistic regression ","f04de46e":"## Creating a pipeline for Adaboost Classifier","c792ef84":"## Appending Survived feature to training dataset","75836038":"## Transforming the features","cec1877a":"## Separating Independent and dependent feature","4ff8cd4b":"## Splitting the Name column into two with First Name and Last Name","f2437053":"## Making a copy of training dataset","38f86f7c":"## Creating a pipeline for LGBMBooster Classifier","a2d7c540":"## Separating training and testing dataset","df05cf84":"## Dropping Survived feature from train dataset"}}