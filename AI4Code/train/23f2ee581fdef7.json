{"cell_type":{"1d46799c":"code","fec86b5f":"code","ec40843c":"code","1710949f":"code","9a3101fe":"code","3369cd2d":"code","7daad7cd":"code","30baef28":"code","496ebadf":"code","b5a56a06":"code","d2256190":"code","c30ee236":"code","a5b83419":"code","8c80b757":"code","68e655a1":"code","be114b6a":"code","8b02a6a3":"code","19b596a6":"code","5b067998":"code","5359178a":"code","ffff3094":"code","02ab653c":"code","19c75320":"code","2c224f04":"code","3858a356":"code","87d0ee00":"code","ffd41c35":"code","732ae532":"code","04116923":"code","dbbe595a":"code","514266a4":"code","26bb1c82":"code","8eebc646":"code","0cab94dc":"code","6f6e2c28":"code","3f72c968":"code","2b5a11a1":"code","98450f3d":"code","e5a0caef":"code","31a752f3":"code","5e076768":"code","29a8fd81":"code","cb4881af":"code","9e96b365":"code","e7cfd801":"code","469d286a":"code","4a95b21b":"code","0a6b3d60":"code","11a188fc":"code","397a28a7":"code","98f1b899":"code","547cad78":"code","144c8d7c":"code","266c98c6":"code","a8649ebd":"code","79a8ef21":"code","1bd4ca23":"code","2cdcac59":"code","54410854":"code","c3fbd1a8":"code","f4d2373b":"code","4edb211a":"code","d070a40c":"code","fd30f71f":"code","e5991852":"code","df4bf803":"code","01a6ef3e":"code","10808f2c":"code","57984871":"code","d104ce07":"code","8ac114cc":"code","bf2ce018":"code","b28b960f":"code","31b0a5e6":"code","cd96db76":"code","fccc556a":"code","d0c21392":"code","99dad49c":"code","78b85f92":"code","443f341b":"code","0eded5a9":"code","bb01b59a":"code","8855c3b7":"code","be638812":"code","fffe55b7":"code","f7ffd072":"code","ece93113":"code","031e844b":"code","d5ed8e8e":"code","0e94e1cc":"code","12791b45":"code","40e163f3":"code","0b986a11":"code","779aee67":"code","0855eb37":"code","3aab0ce2":"code","90480d99":"markdown","8e77f030":"markdown","36cc3996":"markdown","8bab6e26":"markdown","6add4ee2":"markdown","d65fc053":"markdown","484feb0d":"markdown","dbaa8634":"markdown","c37e2820":"markdown","2539c534":"markdown","1c277845":"markdown","75b03136":"markdown","69b77983":"markdown","58d695cb":"markdown","193f2648":"markdown","4a0bff57":"markdown","b059da0f":"markdown","ff7155d2":"markdown","155ef3ca":"markdown","a7057050":"markdown","4187b870":"markdown","513ac934":"markdown","937d7666":"markdown","310fd824":"markdown","884364f6":"markdown","1fcb2f8d":"markdown"},"source":{"1d46799c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import figure\n\nimport math as mt\nimport missingno as msno\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.metrics import mean_squared_error, r2_score \nfrom sklearn.model_selection import train_test_split, cross_val_score,  cross_validate, GridSearchCV , validation_curve, RandomizedSearchCV\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV \n\nfrom sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor, GradientBoostingRegressor, VotingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nimport warnings\nfrom warnings import filterwarnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning) \n\nfilterwarnings(\"ignore\",category=DeprecationWarning)\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.6f' % x)\npd.set_option('display.max_rows', 500)","fec86b5f":"df_ = pd.read_csv(\"..\/input\/hitters-baseball-data\/hitters.csv\")\ndf  = df_.copy()\n\ndf.head()","ec40843c":"def upper_col_name(dataframe):\n    upper_cols = [col.upper() for col in dataframe.columns]\n    dataframe.columns = upper_cols\n    return dataframe\n\ndf = upper_col_name(df)\n\ndf.head()","1710949f":"# Genel Exploration for Dataset\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n    \ncheck_df(df)   ","9a3101fe":"\n# Selection of Categorical and Numerical Variables:\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\" and \"ID\" not in col]\n\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","3369cd2d":"print(cat_cols)","7daad7cd":"print(num_cols)","30baef28":"# General Exploration for Categorical Variables:\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\n\nfor col in cat_cols:\n    cat_summary(df, col, plot=True)        ","496ebadf":"# General Exploration for Numerical Variables:\n\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n\nfor col in num_cols:\n    num_summary(df, col, plot=True)         ","b5a56a06":"# Other Visualizations:\n\n# Salary distribution:\nsns.distplot(df[\"SALARY\"]);","d2256190":"# Salary by League & New League: \n\ng = sns.catplot(x=\"LEAGUE\", y=\"SALARY\",\n                hue=\"NEWLEAGUE\", \n                data=df, kind=\"bar\",\n                height=4, aspect=1,palette=\"deep\");","c30ee236":"# Salary by Division: \n\nsns.barplot(x=\"DIVISION\",y = \"SALARY\", data=df, hue=\"NEWLEAGUE\", palette=\"deep\");","a5b83419":"# Missing Values :  Check the features containing NaN values\n\ndef missing_values_df(dataframe, na_name=False):\n    na_column = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0] # missing de\u011fer iceren kolon ad\u0131\n    n_miss = dataframe[na_column].isnull().sum().sort_values(ascending=False) # bo\u015f g\u00f6zlem say\u0131s\u0131\n    ratio = (dataframe[na_column].isnull().sum() * 100\/ dataframe.shape[0]).sort_values(ascending=False)\n    missing_df = pd.DataFrame({\"n_miss\":n_miss, \"n_miss_ratio\":ratio})\n    # missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_column\n    \nmissing_values_df(df)","8c80b757":"# Firstly, let's create a dataframe which  does not contain NA values:\n\nhitters_df = df.copy()\nhitters_df.dropna(inplace=True)\n\n# One Hot encoding:\ndms = pd.get_dummies(hitters_df[cat_cols], drop_first=True, dtype=\"int64\")\ndf_ = hitters_df.drop(columns=cat_cols, axis=1)\nhitters_df_ = pd.concat([df_, dms],axis=1)\n\n# Standart Scaler\nnum_cols = [col for col in df.columns if df[col].dtype in (\"int64\",\"float64\") and  \"SALARY\" not in col]\n\n# Base Model:\n\nX = hitters_df_.drop([\"SALARY\"], axis=1)\ny = hitters_df_[\"SALARY\"]\n\nlgb = LGBMRegressor()\nlgb_model = lgb.fit(X, y)\ny_pred = lgb_model.predict(X)   \n\n# r2 Score:\nr2_scr= r2_score(y,y_pred)\nprint(\"R2 Score:\",r2_scr) \n\n#  5-Fold Cross-Validation Score?\nprint(\"5 Fold CV Score:\", np.mean(np.sqrt(-cross_val_score(lgb_model, X, y, cv=5, scoring=\"neg_mean_squared_error\"))))\n\n","68e655a1":"# Feature Selection:\n\ndef plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(6, 6))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.title(f\"Features for {type(model).__name__}\")\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(lgb_model, X, num=10)","be114b6a":"# Let's fill in the \"salary\" according to the most important top 5 variables that represents it:\n\n# CRBI_Cat\n\nhitters_df['CRBI_CAT'] = pd.qcut(x=hitters_df['CRBI'], q=3 ,labels = [\"low\", \"medium\", \"high\"]) \nhitters_df.groupby(\"CRBI_CAT\").agg({\"CRBI_CAT\": [\"count\", \"min\", \"max\"],\n                                     \"SALARY\": [\"mean\",\"median\"] })","8b02a6a3":" # Walks_Cat\n\nhitters_df['WALKS_CAT'] = pd.qcut(x=hitters_df['WALKS'], q=3 ,labels = [\"low\", \"medium\", \"high\"]) \nhitters_df.groupby(\"WALKS_CAT\").agg({\"WALKS_CAT\": [\"count\", \"min\", \"max\"],\n                                     \"SALARY\": [\"mean\",\"median\"] })","19b596a6":"# CWalks_Cat\n \nhitters_df['CWALKS_CAT'] = pd.qcut(x=hitters_df['CWALKS'], q=3 ,labels = [\"low\", \"medium\", \"high\"]) \nhitters_df.groupby(\"CWALKS_CAT\").agg({\"CWALKS_CAT\": [\"count\", \"min\", \"max\"],\n                                     \"SALARY\": [\"mean\",\"median\"] })","5b067998":"# PutOuts_Cat\n \nhitters_df['PUTOUTS_CAT'] = pd.qcut(x=hitters_df['PUTOUTS'], q=2, labels = [\"low\", \"high\"]) \nhitters_df.groupby('PUTOUTS_CAT').agg({\"PUTOUTS\": [\"count\", \"min\", \"max\"],\n                                     \"SALARY\": [\"mean\",\"median\"] })","5359178a":"# Years_Cat\nyears_bins = [0, 3, 6, 12,  int(hitters_df[\"YEARS\"].max())]\nhitters_df[\"YEARS_CAT\"] = pd.cut(hitters_df[\"YEARS\"], years_bins, \n                                 labels= [\"new\",\"low_experienced\",  \"medium_experienced\",  \"high_experienced\"])\n\nhitters_df.groupby(\"YEARS_CAT\").agg({\"YEARS\": [\"count\", \"min\", \"max\"],\n                                     \"SALARY\": \"mean\"})","ffff3094":"# Now let's create the new categorical variables based on the above 5 features on the original dataframe (df):\n\n\n# CRBI_Cat \ndf['CRBI_CAT'] = pd.qcut(x=df['CRBI'], q=3 ,labels = [\"low\", \"medium\", \"high\"]) \n\n# Walks_Cat\ndf['WALKS_CAT'] = pd.qcut(x=df['WALKS'], q=3 ,labels = [\"low\", \"medium\", \"high\"]) \n\n# CWalks_Cat\ndf['CWALKS_CAT'] = pd.qcut(x=df['CWALKS'], q=3, labels = [\"low\", \"medium\", \"high\"]) \n\n# PutOuts_Cat\ndf['PUTOUTS_CAT'] = pd.qcut(x=df['PUTOUTS'], q=2, labels = [\"low\", \"high\"]) \n\n# Years_Cat\nyears_bins = [0, 3, 5, 10,  int(df[\"YEARS\"].max())]\ndf[\"YEARS_CAT\"] = pd.cut(df[\"YEARS\"], years_bins, \n                                 labels= [\"new\", \"medium_experienced\", \"experienced\", \"high_experienced\"])","02ab653c":"# Fill NA Salary values:\n\ndf[\"SALARY\"] = df[\"SALARY\"].fillna(df.groupby([\"YEARS_CAT\",\"PUTOUTS_CAT\", \"CWALKS_CAT\", \"WALKS_CAT\", \"CRBI_CAT\"])[\"SALARY\"].transform(\"median\"))","19c75320":"df.isnull().sum().sum()","2c224f04":"# Salary values of these observations are NaN, so we can delete them:\n\ndf[df[[\"SALARY\"]].isnull().any(axis=1)][[\"YEARS_CAT\",\"PUTOUTS_CAT\", \"CWALKS_CAT\", \"WALKS_CAT\", \"CRBI_CAT\",\"SALARY\"]]","3858a356":"df.dropna(inplace=True)","87d0ee00":"df[\"NEW_YEARS_SCALED\"] = MinMaxScaler(feature_range=(0,1)).fit(df[[\"YEARS\"]]).transform(df[[\"YEARS\"]])\n\ndf['NEW_HITS_PER_GAME'] = df[\"HITS\"] \/ 9\ndf['NEW_CHITS_PER_GAME'] = df[\"CHITS\"] \/ 9\ndf[\"NEW_CWALKS_PER_GAME\"] = df[\"CWALKS\"] \/ 9\ndf[\"NEW_WALKS_PER_GAME\"] = df[\"WALKS\"] \/ 9\ndf[\"NEW_CHMRUN_PER_GAME\"] = df[\"CHMRUN\"] \/ 9\ndf[\"NEW_HMRUN_PER_GAME\"] = df[\"HMRUN\"] \/ 9\n\n#  Earned run average\ndf[\"NEW_ERA\"] = df[\"RUNS\"] \/9\n\n# Hits Rate:\ndf[\"NEW_HITS_RATE\"] = (df[\"HITS\"] \/ df[\"ATBAT\"]) \n\n# Number of missed hits :\ndf[\"NEW_NUM_OF_MISSING\"] =  (df[\"ATBAT\"]) - ( df[\"HITS\"] )\ndf[\"NEW_NUM_OF_MISSING_RATE\"] = df[\"NEW_NUM_OF_MISSING\"] \/ df[\"ATBAT\"] \ndf[\"NEW_HITS_NUM_OF_MISSING_RATE\"] = (df[\"HITS\"] \/ df[\"NEW_NUM_OF_MISSING\"] )\n\n# WALKS and HITS per inning pitched:\ndf[\"NEW_WHIP\"] = df[\"WALKS\"] + df[\"WALKS\"]\n \n# Hits rate missed hits and rate over career:  \ndf[\"NEW_HITS_RATE_CAREER\"] = (df[\"CHITS\"] \/ df[\"CATBAT\"]) \ndf[\"NEW_NUM_OF_MISSING_CAREER\"] = df[\"CATBAT\"] - df[\"CHITS\"] \ndf[\"NEW_NUM_OF_MISSING_RATE_CAREER\"] = df[\"CHITS\"] \/ df[\"CATBAT\"] \ndf[\"NEW_HITS_NUM_OF_MISSING_CAREER_RATE\"] = (df[\"CHITS\"] \/ df[\"NEW_NUM_OF_MISSING_CAREER\"] )\n\ndf[\"NEW_NUM_OF_MISSING_RATE\"] = df[\"HITS\"] \/ df[\"ATBAT\"] \n\n# HITS_Missing_Rate_Delta\ndf[\"NEW_HITS_MISSING_RATE_DELTA\"] = (df[\"NEW_HITS_NUM_OF_MISSING_CAREER_RATE\"] \/ df[\"NEW_HITS_NUM_OF_MISSING_CAREER_RATE\"] )\n\ndf[\"NEW_HITS\/CHITS\"] = (df[\"HITS\"] \/ df[\"CHITS\"]) \ndf[\"NEW_HMRUN\/ATBAT\"] = (df[\"HMRUN\"] \/ df[\"ATBAT\"]) \n\ndf[\"NEW_HMRUN\/CHMRUN\"] = (df[\"HMRUN\"] \/ df[\"CHMRUN\"])\n\ndf[\"NEW_RBI\/ATBAT\"] = (df[\"RBI\"] \/ df[\"ATBAT\"]) \ndf[\"NEW_RUNS\/ATBAT\"] = (df[\"RUNS\"] \/ df[\"ATBAT\"]) \ndf[\"NEW_CHMRUN\/ATBAT\"] = (df[\"CHMRUN\"] \/ df[\"ATBAT\"]) \n\ndf[\"NEW_CHMRUN\/CATBAT\"] = (df[\"CHMRUN\"] \/ df[\"CATBAT\"] )\n\ndf[\"NEW_ATBAT\/CATBAT\"] = (df[\"ATBAT\"] \/ df[\"CATBAT\"]) \n\ndf[\"NEW_HMRUN\/HITS\"] = (df[\"HMRUN\"] \/ df[\"HITS\"]) \ndf[\"NEW_HITS\/CHITS\"] = (df[\"HITS\"] \/ df[\"CHITS\"]) \n\ndf[\"NEW_CHITS\/CRBI\"] = (df[\"CHITS\"] \/ df[\"CRBI\"])\n\ndf[\"NEW_CHMRUN\/CRUNS\"] = (df[\"CHMRUN\"] \/ df[\"CRUNS\"])\ndf[\"NEW_RUNS\/CRUNS\"] = (df[\"RUNS\"] \/ df[\"CRUNS\"])\ndf['NEW_HMRUN\/RUNS'] =  (df['HMRUN'] \/ df['RUNS']) \ndf['NEW_CHITS\/CATBAT'] = (df['CHITS'] \/ df['CATBAT'])\n\ndf[\"NEW_WALKS\/RBI\"] = (df[\"WALKS\"] \/ df[\"RBI\"])\ndf[\"NEW_WALKS\/CWALKS\"] = (df[\"WALKS\"] \/ df[\"CWALKS\"])\n\ndf[\"NEW_RBI\/CRBI\"] = (df[\"RBI\"] \/ df[\"CRBI\"])\ndf[\"NEW_CRBI\/RBI\"] = (df[\"CRBI\"] \/ df[\"RBI\"]) \ndf[\"NEW_Total_RBI\"] = df[\"RBI\"] * df[\"WALKS\"]  \n\ndf[\"NEW_CRUNS\/CHITS\"] = (df[\"CRUNS\"] \/ df[\"CHITS\"])\ndf[\"NEW_CRUNS\/CATBAT\"] = df[\"CRUNS\"]\/df[\"CATBAT\"]\n\ndf[\"NEW_CRBI\/RBI\"] = (df[\"CRBI\"] \/ df[\"RBI\"])\n\ndf[\"NEW_PUTOUTS\/ATBAT\"] = (df[\"PUTOUTS\"] \/ df[\"ATBAT\"]) \ndf[\"NEW_ASSISTS\/PUTOUTS\"] = (df[\"ASSISTS\"] \/ df[\"PUTOUTS\"])\ndf[\"NEW_ERRORS\/ASSISTS\"] = (df[\"ERRORS\"] \/ df[\"ASSISTS\"] ) \ndf[\"NEW_ERRORS\/CWALKS\"] = (df[\"ERRORS\"] \/ df[\"CWALKS\"] ) \ndf[\"NEW_ASSISTS\/ERRORS\"] = (df[\"ASSISTS\"] \/ df[\"ERRORS\"] )\n\ndf[\"NEW_CATBAT_PER_YEAR\"] = (df[\"CATBAT\"] \/ df[\"YEARS\"] )  \ndf[\"NEW_CHITS_PER_YEAR\"] = (df[\"CHITS\"] \/ df[\"YEARS\"] )  \ndf[\"NEW_CRUNS_PER_YEAR\"] = (df[\"CRUNS\"] \/ df[\"YEARS\"] )  \ndf[\"NEW_CHMRUN_PER_YEAR\"] = (df[\"CHMRUN\"] \/ df[\"YEARS\"] ) \ndf[\"NEW_WALKS_PER_YEAR\"] = (df[\"WALKS\"] \/ df[\"YEARS\"] ) \ndf[\"NEW_CWALKS_PER_YEAR\"] = (df[\"CWALKS\"] \/ df[\"YEARS\"] ) \ndf[\"NEW_PUTOUTS_PER_YEAR\"] = (df[\"PUTOUTS\"] \/ df[\"YEARS\"] ) \ndf[\"NEW_CRBI_PER_YEAR\"] = (df[\"CRBI\"] \/ df[\"YEARS\"])\n\ndf[\"NEW_SCORE\"]= df[\"RUNS\"]*df[\"HMRUN\"]\ndf[\"NEW_TOTAL_SCORE\"] =  (df[\"CRUNS\"] * df[\"RUNS\"] )\/  ( df[\"NEW_HITS_NUM_OF_MISSING_RATE\"] \/ df[\"NEW_HITS_MISSING_RATE_DELTA\"]  * df[\"NEW_YEARS_SCALED\"] )\ndf[\"NEW_EQA\"] = (df[\"ATBAT\"] + df[\"HITS\"] +  (1.5 * df[\"WALKS\"]) ) \/ (df[\"ATBAT\"] + df[\"WALKS\"] )\n\ndf['NEW_HITS_RATE_CAT'] = pd.qcut(x=(df['NEW_HITS_RATE']), q=3, labels= [\"low\", \"medium\", \"high\"] ) \n\ndf['NEW_NUM_OF_MISSING_RATE_CAT'] = pd.qcut(x=(df['NEW_NUM_OF_MISSING_RATE']), q=3,  labels= [\"low\", \"medium\", \"high\"] ) \n","ffd41c35":"df[\"NEW_LEAGUE_BEST_PLAYER\"] = np.where( ( (df[\"LEAGUE\"] ==\"A\") & (df[\"NEWLEAGUE\"] ==\"N\") &  (df[\"DIVISION\"] ==\"E\")), 1 , 0)\ndf[\"NEW_LEAGUE_BEST_PLAYER\"] = np.where(((df[\"LEAGUE\"] ==\"N\") & (df[\"NEWLEAGUE\"] ==\"N\") & (df[\"DIVISION\"] ==\"E\") ), 1, df[\"NEW_LEAGUE_BEST_PLAYER\"] )\n\ndf.groupby([\"LEAGUE\",\"NEWLEAGUE\",\"NEW_LEAGUE_BEST_PLAYER\"]).agg({\"SALARY\" :[ \"mean\", \"count\"]})","732ae532":"# Salary mean differences according to League and New league Classes:\n\nprint(\"New League A: \", df[df[\"NEWLEAGUE\"]==\"A\"].agg({\"SALARY\":\"mean\"}))\nprint(\"New League N: \", df[df[\"NEWLEAGUE\"]==\"N\"].agg({\"SALARY\":\"mean\"}))\n\n# Division'lar aras\u0131nda maa\u015f ortalamas\u0131 ve median'lar\u0131 aras\u0131nda fark var m\u0131?\n\nprint(\"Division E: \", df[df[\"DIVISION\"]==\"E\"].agg({\"SALARY\":\"mean\"}))\nprint(\"Division W: \", df[df[\"DIVISION\"]==\"W\"].agg({\"SALARY\":\"mean\"}))","04116923":"# If Hits_Missing_Rate_Delta is greater than 1, this may represent that the player has been playing more successfully lately:\n\ndf[\"NEW_PLAYER_SUCCESS_INCREASED\"] =  np.where(df['NEW_HITS_MISSING_RATE_DELTA'] >1  , \"YES\", \"NO\")\n\n\ndf[\"NEW_IS_YOUNG_TALENTED\"] =  np.where(( (df['YEARS'] <= 5) & (df[\"NEW_HITS_RATE\"] >=0.30)),\"YES\", \"NO\")\ndf.groupby(\"NEW_IS_YOUNG_TALENTED\").agg({\"SALARY\": [\"mean\",\"median\",\"count\"]})\n","dbbe595a":"df[\"NEW_HITS_RATE\/CHITS_RATE\"] = (df['NEW_HITS_RATE'] \/ df['NEW_HITS_RATE_CAREER'] )\n\ndf[\"NEW_HITS_RATE\"].sort_values().head()","514266a4":"df[\"NEW_IS_PLAYER_UPGRADED\"] =  np.where(( (df['YEARS'] >=5) & (df[\"NEW_HITS_RATE\/CHITS_RATE\"] >1.1 )) , \"YES\", \"NO\")\n\ndf.groupby(\"NEW_IS_PLAYER_UPGRADED\").agg({\"SALARY\": [\"mean\",\"median\",\"count\"]})","26bb1c82":"df.head()","8eebc646":"# Let's check for missing values again because we have derived new variables: \n\nmissing_values_df(df)","0cab94dc":"df[[\"HMRUN\", \"RUNS\", \"NEW_HMRUN\/CHMRUN\"]][df[[\"HMRUN\", \"RUNS\", \"NEW_HMRUN\/CHMRUN\"]].isnull().any(axis=1)].head(3)","6f6e2c28":"df[[\"HMRUN\", \"RUNS\", \"NEW_HMRUN\/RUNS\"]][df[[\"HMRUN\", \"RUNS\", \"NEW_HMRUN\/RUNS\"]].isnull().any(axis=1)].head(3)","3f72c968":"df[[\"CWALKS\", \"WALKS\", \"NEW_WALKS\/CWALKS\"]][df[[\"CWALKS\", \"WALKS\", \"NEW_WALKS\/CWALKS\"]].isnull().any(axis=1)].head(3)","2b5a11a1":"df[[\"RBI\", \"WALKS\", \"NEW_WALKS\/RBI\"]][df[[\"RBI\", \"CRBI\", \"NEW_WALKS\/RBI\"]].isnull().any(axis=1)].head(3)","98450f3d":"df[[\"RBI\", \"WALKS\", \"NEW_WALKS\/RBI\"]][df[[\"RBI\", \"CRBI\", \"NEW_WALKS\/RBI\"]].isnull().any(axis=1)].head(3)","e5a0caef":"df[[\"RBI\", \"CRBI\", \"NEW_RBI\/CRBI\"]][df[[\"RBI\", \"CRBI\", \"NEW_CRBI\/RBI\"]].isnull().any(axis=1)].head(3)","31a752f3":"df[[\"RBI\", \"CRBI\", \"NEW_RBI\/CRBI\"]][df[[\"RBI\", \"CRBI\", \"NEW_RBI\/CRBI\"]].isnull().any(axis=1)].head(3)","5e076768":"df[[\"ASSISTS\", \"PUTOUTS\", \"NEW_ASSISTS\/PUTOUTS\"]][df[[\"ASSISTS\", \"ERRORS\",  \"NEW_ASSISTS\/PUTOUTS\"]].isnull().any(axis=1)].head(3)","29a8fd81":"df[[\"ASSISTS\", \"ERRORS\", \"NEW_ASSISTS\/ERRORS\"]][df[[\"ASSISTS\", \"ERRORS\", \"NEW_ASSISTS\/ERRORS\"]].isnull().any(axis=1)].head(3)","cb4881af":"df[[\"ASSISTS\", \"ERRORS\", \"NEW_ASSISTS\/ERRORS\"]][df[[\"ASSISTS\", \"ERRORS\", \"NEW_ERRORS\/ASSISTS\"]].isnull().any(axis=1)].head(3)","9e96b365":"# These variables are NA because the numerator is zero; We can fill these with 0: \n\nna_cols = [var for var in df.columns if df[var].isnull().sum() > 0]\ndf[na_cols] = df[na_cols].fillna(0)","e7cfd801":"# Let's check \"inf, -inf\" values:\n\ndf[df.isin([np.nan, np.inf, -np.inf]).any(axis=1)].shape[0]","469d286a":"# Replace [inf, -inf] values with zero:\n\ndf.replace([np.inf, -np.inf], 0, inplace=True)","4a95b21b":"#  Let's describe num_cols again due to creating new features at the feature extaractions process:\n\nnum_cols = grab_col_names(df)[1]\nnum_cols","0a6b3d60":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    q1 = dataframe[col_name].quantile(q1)  # 1.\u00c7eyrek\n    q3 = dataframe[col_name].quantile(q3)  # 3.\u00c7eyrek\n    interquantile_range = q3 - q1  # range'i hesaplayal\u0131m\n    low_limit = q1 - 1.5 * interquantile_range # low & up limit:\n    up_limit = q3 + 1.5 * interquantile_range\n    return low_limit, up_limit","11a188fc":"# Let's check if features include outliers :\n\ndef check_outlier(dataframe, col_name, q1=0.10, q3=0.90):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n    \n    \nfor col in num_cols:\n    print(col, \":\", check_outlier(df, col))\n","397a28a7":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\ndf_ = df[num_cols]\n\nclf = LocalOutlierFactor(n_neighbors=10)\nclf.fit_predict(df_)\n\ndf_scores = clf.negative_outlier_factor_\n","98f1b899":"# LOF Visualization: \nscores = pd.DataFrame(np.sort(df_scores))\nscores.plot(stacked=True, xlim=[0, 10], style='.-')\nplt.show()","547cad78":"np.sort(df_scores)[0:10]  ","144c8d7c":"th = np.sort(df_scores)[0:5][3]\nth","266c98c6":"df[df_scores < th]","a8649ebd":"df.drop(df[df_scores < th].index, inplace=True)","79a8ef21":"df.shape","1bd4ca23":"# Let's define categorical variables again:\n\ncat_cols = grab_col_names(df)[0]","2cdcac59":"# Rare Analysing:\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\nrare_analyser(df, \"SALARY\", cat_cols)","54410854":"useless_cols = [col for col in df.columns if ( ( df[col].nunique() == 2 \n                                            and (df[col].value_counts() \/ len(df) < 0.01).any(axis=None))\n                                            | df[col].nunique() == 1 )]\nprint(useless_cols)","c3fbd1a8":"df.drop( useless_cols, axis = 1,inplace=True)","f4d2373b":"# Rare Encoding:\n\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n\n    rare_columns = [col for col in cat_cols if (dataframe[col].value_counts() \/ len(dataframe) <= rare_perc).sum() > 1]\n\n    for col in rare_columns:\n        tmp = dataframe[col].value_counts() \/ len(dataframe) # th alt\u0131nda kalan s\u0131n\u0131f\u0131 olan de\u011fi\u015fkenlerin s\u0131n\u0131f frekanslar\u0131ndan olusan df yarat\n        rare_labels = tmp[tmp <= rare_perc].index # s\u0131n\u0131f frekans\u0131 < th olanlar\u0131n indexlerini bul\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col]) # th alt\u0131nda kalan de\u011ferleri Rare olarak grupla\n\n    return dataframe","4edb211a":"cat_cols = grab_col_names(df)[0]\ndf = rare_encoder(df, 0.01, cat_cols)\n\nrare_analyser(df, \"SALARY\", cat_cols)","d070a40c":"#  One-Hot Encoding:\n\ncat_cols = grab_col_names(df)[0]\ndms = pd.get_dummies(df[cat_cols], drop_first=True, dtype=\"int64\")\ndf_ = df.drop(columns=cat_cols, axis=1)\ndf = pd.concat([df_, dms],axis=1)","fd30f71f":"num_cols = grab_col_names(df)[1]\nnum_cols = [col for col in num_cols if \"SALARY\" not in col]\nnum_cols","e5991852":"df.head()","df4bf803":"\ndef StandartScaling(dataframe, col_name):\n    ss = StandardScaler()\n    dataframe[col_name] = ss.fit_transform(dataframe[col_name])\n    return dataframe\n\ndef MinMaxScaling(dataframe, col_name):\n    mms = MinMaxScaler()\n    dataframe[col_name] = mms.fit_transform(dataframe[col_name])\n    return dataframe\n\ndef RobustScaling(dataframe, col_name):\n    rs = RobustScaler()\n    dataframe[col_name] = rs.fit_transform(dataframe[col_name])\n    return dataframe\n\n\ndef Scaling(dataframe, method):\n    numerical_cols = grab_col_names(dataframe)[1]\n    if method == \"StandartScaling\":\n        StandartScaling(dataframe, numerical_cols)\n    elif method == \"MinMaxScaling\":\n        MinMaxScaling(dataframe, numerical_cols)\n    else:\n        RobustScaling(dataframe, numerical_cols)\n    return dataframe\n","01a6ef3e":"df[num_cols] = (Scaling(df[num_cols], \"RobustScaling\"))","10808f2c":"df.head()","57984871":"# Correlation matrix\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.95):\n    num_cols = grab_col_names(df)[1]\n    corr = dataframe[num_cols].corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\")\n        plt.show()\n    return drop_list","d104ce07":"high_correlated_cols(df, plot=True, corr_th=0.95)","8ac114cc":"high_correlated_col_df =  high_correlated_cols(df, corr_th=0.95)\ndf.drop(columns=high_correlated_col_df, axis=1, inplace=True)","bf2ce018":"#Let's look at the data before modelling:\n\ndf.describe().T","b28b960f":"# Defining dependent and independent variables:\n\nX = df.drop(\"SALARY\", axis=1)\ny = df[[\"SALARY\"]]","31b0a5e6":"# Base Models:\n\nmodels = [('Lasso', Lasso()),\n          ('KNN', KNeighborsRegressor()),\n          ('CRT',DecisionTreeRegressor()),\n          ('RF' , RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('ExtraTrees', ExtraTreesRegressor()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGB\", XGBRegressor(objective='reg:squarederror')),\n          (\"LGBM\",LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\nfor name,  regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    \n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","cd96db76":"# Fit Model:\n\nLasso_model = Lasso().fit(X,y)   ","fccc556a":"print(Lasso_model.coef_[0:10])","d0c21392":"print(Lasso_model.intercept_) ","99dad49c":"# Lasso Regression - Hyperparameter Optimization:\n\ncoefs = []   \nalphas= 10**np.linspace(10,-2,100)*0.5\n\nlasso_cv = LassoCV(alphas = alphas,cv = 10,max_iter=100000)\nlasso_cv_model = lasso_cv.fit(X,y)\n\n#Optimum alpha:\n\nlasso_cv_model.alpha_","78b85f92":"# Lasso Regression - Tuned Model:\n  \nls = Lasso(lasso_cv_model.alpha_)\nlasso_tuned_model = ls.fit(X,y)\n\ny_pred = lasso_tuned_model.predict(X)\nprint(\"RMSE:\" , np.sqrt(mean_squared_error(y, y_pred)))\nprint(\"r2 Score:\", r2_score(y,y_pred))\n\n# 10-Fold Cross-Validation:\n\nprint(\"10 - Fold CV Score:\", np.mean(np.sqrt(-cross_val_score(lasso_tuned_model, X, y, cv=10, scoring=\"neg_mean_squared_error\"))))","443f341b":"# Lasso Regression Coefficients:\nImportance = pd.DataFrame({\"Feature\": X.columns, \n                           \"Coefs\" : lasso_tuned_model.coef_ })\n\nImportance.sort_values(\"Coefs\").sort_values(\"Coefs\",ascending=True).head(10)","0eded5a9":"# Drop colums if coefficient is zero:\n\ndrop_list = list(Importance[Importance[\"Coefs\"] == 0][\"Feature\"])\ndf.drop(columns=drop_list, axis=1, inplace=True)","bb01b59a":"rf_params = {\"max_depth\": [5, 7, 8],\n             \"max_features\": [\"sqrt\", \"auto\"],\n             \"min_samples_split\": [8, 15, 20],\n             \"n_estimators\": [5000, 2500, 1000]}  \n\n\ngbm_params= {\"learning_rate\": [0.01,0.001],\n            \"max_depth\": [5,None],\n            \"max_features\": [\"auto\",\"sqrt\"],\n            \"n_estimators\": [1000, 2000,3000]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1],\n                  \"n_estimators\": [500, 2500, 4000],\n                  \"colsample_bytree\": [0.3, 0.7]}\n\nextratrees_params= {\"min_samples_split\": [8, 10, 12],\n                  \"n_estimators\": [3000, 2500, 1000],\n                  \"max_features\": [\"auto\", \"sqrt\"],\n                  \"max_depth\": [5,7,8]}\n\nregressors = [(\"RF\", RandomForestRegressor(random_state=110), rf_params),\n              (\"GBM\", GradientBoostingRegressor(random_state=110), gbm_params),\n              ('LightGBM', LGBMRegressor(n_jobs=-1,random_state=110), lightgbm_params),\n              ('ExtraTrees', ExtraTreesRegressor(n_jobs=-1,random_state=110),extratrees_params )]","8855c3b7":"# Model Performance with Hyperparameter Optimizastion:\n\nbest_models = {}\nparams = {}\n\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    # Parametrelerin tune edilmesi\n    gs_best = GridSearchCV(regressor, params, cv=5, n_jobs=-1, verbose=False).fit(X, y.values.ravel())\n\n    # Best after modelin valide edilip, rmse hesaplanmas\u0131\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse_final = np.mean(np.sqrt(-cross_val_score(final_model,X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n   \n    print(f\"RMSE (After): {round(rmse_final, 4)} ({name}) \")\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n \n    best_models[name] = final_model ","be638812":"# Feature Importances of 4 Models (Random Forest, LGBM, GBM, ExtraTrees)\nfeature_imp_all = pd.DataFrame({'Model': np.NaN, 'CV_Score': np.NaN,'Feature': np.NaN, 'Value': np.NaN, 'Weighted_Score': np.NaN}, index=[0])\n   \nfor model, reg in best_models.items():\n \n    final_model = reg.fit(X, y.values.ravel())\n    rmse_final = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n\n    feature_imp = pd.DataFrame({ 'Model' : model,\n                                 'CV_Score': rmse_final,\n                                 'Feature': X.columns,\n                                 'Value': final_model.feature_importances_, \n                                 'Weighted_Score': rmse_final * final_model.feature_importances_ })\n    \n    feature_imp_all = pd.concat([feature_imp_all, feature_imp], axis = 0)\n    feature_imp_all.dropna(inplace=True)\n    feature_imp_all.reset_index(inplace=True)\n    feature_imp_all.drop(columns=\"index\", axis=1, inplace=True)    ","fffe55b7":"print(feature_imp_all.head(10))","f7ffd072":"feature_imp_all = feature_imp_all.pivot_table(values = \"Weighted_Score\", \n                                             columns=\"Model\", \n                                             index=['Feature'],\n                                             aggfunc=np.mean).reset_index()","ece93113":"# Let's scale CV Score values for all models:\n\nscaled_cols = [col for col in feature_imp_all.columns if feature_imp_all[col].dtypes == \"float64\"]\n\nscaler = MinMaxScaler()\nfeature_imp_all[scaled_cols] = scaler.fit_transform(feature_imp_all[scaled_cols])","031e844b":"# Average Feature Importance based on Selected Models:\n\nfeature_imp_all[\"Avg_Importance_Value\"] = (feature_imp_all[\"ExtraTrees\"] + \n                                           feature_imp_all[\"GBM\"] +\n                                           feature_imp_all[\"LightGBM\"] +\n                                           feature_imp_all[\"RF\"] ) \/4","d5ed8e8e":"feature_imp_all.head()","0e94e1cc":"# Let's show the hybrid feature importance values with the plot:\n\nplt.figure(figsize=(10, 10))\nsns.set(font_scale=1)\nsns.barplot(x=\"Avg_Importance_Value\", y=\"Feature\", data=feature_imp_all.sort_values(by=\"Avg_Importance_Value\", ascending=False)[0:30])\nplt.title('Hybrid Features Importance')\nplt.tight_layout()\nplt.show();","12791b45":"len(X.columns)","40e163f3":"# We have 63 features, now fit the hybrid model by using top 30 features which have the highest Importance:\n\nselected_cols = list(feature_imp_all.sort_values(\"Avg_Importance_Value\", ascending=False).head(30)[\"Feature\"].values)","0b986a11":"model_results_all = pd.DataFrame({'Model': np.NaN, \n                                  'CV_Score': np.NaN}, index=[0])\n   \nfor model, reg in best_models.items():\n    \n    final_model = reg.fit(X[selected_cols], y.values.ravel())\n    rmse_final = np.mean(np.sqrt(-cross_val_score(final_model, X[selected_cols], y, cv=5, scoring=\"neg_mean_squared_error\")))\n\n    model_results = pd.DataFrame({ 'Model' : model,\n                                   'CV_Score': rmse_final} , index=[0])\n    model_results_all = pd.concat([model_results_all, model_results], axis = 0)\n    model_results_all.dropna(inplace=True)\n    model_results_all.reset_index(inplace=True)\n    model_results_all.drop(columns=\"index\", axis=1, inplace=True)    ","779aee67":"model_results_all","0855eb37":"#  Ensemble Modelling - 1 : GBM & Light GBM\n\nvoting_reg_model = VotingRegressor(estimators= [('GBM',best_models['GBM']),\n                                                ('LGBM',best_models['LightGBM'])])\n\n# Model Fit:\nvoting_reg_model.fit(X[selected_cols], y)\n\n# Prediction: \ny_pred = voting_reg_model.predict(X[selected_cols])\n\n\n# Model Performance:\nr2_scr = r2_score(y,y_pred)\nprint(\"R2 :\",r2_scr) \nprint(\"5-Fold CV Score:\", np.mean(np.sqrt(-cross_val_score(voting_reg_model, X[selected_cols], y, cv=5, scoring=\"neg_mean_squared_error\"))))","3aab0ce2":"#  Ensemble Modelling - 2 : GBM & Random Forest\nvoting_reg_model = VotingRegressor(estimators= [('GBM',best_models['GBM']),\n                                                ('RF',best_models['RF'])])\n\n# Model Fit:\nvoting_reg_model.fit(X[selected_cols], y)\n\n# Prediction: \ny_pred = voting_reg_model.predict(X[selected_cols])\n\n\n# Model Performance:\nr2_scr = r2_score(y,y_pred)\nprint(\"R2 :\",r2_scr) \nprint(\"5-Fold CV Score:\", np.mean(np.sqrt(-cross_val_score(voting_reg_model, X[selected_cols], y, cv=5, scoring=\"neg_mean_squared_error\"))))","90480d99":"**2. Outliers:**","8e77f030":"<a id=\"section-one\"><\/a>\n# **Data Understanding**","36cc3996":"**Let's choose four best models for hyperparameter optimization:**\n\n* GBM\n* CatBoost\n* LGBM\n* ExtraTrees","8bab6e26":"Now, firstly we'll use Lasso Regression for feature selection.\n\nThe LASSO method is mainly used to achieve simultaneous parameter estimation and model selection in regression analysis (Muthukrishnan & Rohini, 2016). This algorithm shows that features with zero weights have with low explanatory power.\nBecause Lasso regression brings the coefficients of features with low effect closer to 0.","6add4ee2":"***We will be performing the other data preprocessing steps after deriving the new variables.***","d65fc053":"<a id=\"section-six\"><\/a>\n# **Ensemble Learning**\n","484feb0d":"<a id=\"section-seven\"><\/a>\n# **Summary**\n","dbaa8634":"***1. Missing Values***","c37e2820":"***4. Feature Scaling:***","2539c534":"**Goal:** \n\nThe purpose of this project is to predict salaries of players by using machine learning algorithms.","1c277845":"<a id=\"section-three\"><\/a>\n# **Modelling**\n","75b03136":"<a id=\"section-five\"><\/a>\n# **Hybrid Feature Selection**","69b77983":"* \"SALARY\" is our target variable and since about 20% of the dataset is blank, we should fill it with the most accurate method. Based on the observations that do not contain NA values, we can fill in the other variables that best explain this variable.*\n\n* For these purposes, by using the Light GBM model, we will try to determine the variables that best explain the salary variable and fill it according to these variables.*","58d695cb":"<a id=\"section-two\"><\/a>\n# **Data Preprocessing & Feature Engineering**\n\nThis part consists of 4 steps which are below:\n\n1. Missing Values  \n2. Outliers \n3. Rare Encoding, Label Encoding, One-Hot Encoding \n4. Feature Scaling  ","193f2648":"**Let's observe the 5-Fold CV Scores of each models by using selected features which have been obtained with hybrid features selection method:**","4a0bff57":"**Import Libraries & Setting Configurations**","b059da0f":"***FEATURE ENGINEERING:***","ff7155d2":"**Import Data**","155ef3ca":"**1. Dataset was read.**\n\n**2. Exploratory Data Analysis:**\n\n    * It was observed that there were 322 observations and 20 variables in the data set. \n    \n    * Data exploration stage has been completed by examining descriptive statistics and seperating categorical and numeric columns.\n\n**3. Data PreProcessing & Feature Engineering:**\n\n* It was observed that only target variable \"Salary\" has 59 missing observations. However, since the dataset size is quite small,  dropping them would be a bad idea. Considering the observations that the \"salary\" variable is full, the Light GBM model was improved and the  5 variables that best explained the salary variable were determined by using feature importance. NA values of dataset were filled with  median value of Salary  by grouping according to these 5 variables.\n\n* Outliers were dropped by usinf LOF method.\n\n* Class distributions of variables were analyzed. Useless features which do not carry parser information were dropped.\n\n* Dummy variables were created by using One Hot Encoding.\n\n* The X variables were normalized.\n\n**4. Model Building:** \n\n* X and y variables were determined.\n\n* Base models were improved (Lasso, KNN, C&R Tree, Random Forest, SVR, Extra Trees, GBM, Light GBM, XGBoost, CatBoost)\n\n* Using Lasso Regression for determining feature importance of variables, useless features were dropped. \n\n* Observing the performance of the base models, 4 models were chosen for Hyperparameter Optimization. \n\n* Hybrid feature selection methodology  which each 4 models performance would affect the feature importance value were improved , and ensemble learning was carried out with 30 variables instead of 63 variables.\n\n\nThank you for your comments and votes:)","a7057050":"***LOF(Local Outliers Factor) method:***","4187b870":"* We'll handle the **\"LOF (Local Outlier Factor)\"** method for the solution of outliers: ","513ac934":"**Methodology:**\n\n* [Data Understanding](#section-one)\n* [Data Preprocessing & Feature Engineering](#section-two)\n* [Modelling](#section-three)\n* [Automated Hyperparameter Optimization](#section-four)\n* [Hybrid Feature Selection](#section-five)\n* [Ensemble Modelling](#section-six)\n* [Summary](#section-seven)","937d7666":"# **Salary Prediction with Hitters Data Set**\n\n\n![image.png](attachment:b99fc245-3746-4c37-8e6a-871fe966ce64.png)","310fd824":"***3. Rare Encoding , Label & One-Hot Encoding:***","884364f6":"<a id=\"section-four\"><\/a>\n# **Automated Hyperparameter Optimization**\n\n","1fcb2f8d":" # **Dataset Story**\n\n* The Hitters is a data set which contains certain statistics and salaries of Major league baseball players for the years 1986\u201387. \n\n* The dataset is part of the data used in the 1988 ASA Graphics Section Poster Session. \n\n* Salary data originally taken from Sports Illustrated, April 20, 1987. \n\n* 1986 and career statistics are from the 1987 Baseball Encyclopedia Update, published by Collier Books, Macmillan Publishing Company, New York. \n\n# **Features**\n\n***Dataset consist of 20 variables and 322 observations, only the salary variable has missing observations. The definitions of the variables of the dataset are as follows:***\n\n* AtBat: Number of shots made with a baseball bat during the 1986\u20131987 season\n* Hits: Number of hits made in the 1986\u20131987 season\n* HmRun: Most valuable hits in the 1986\u20131987 season\n* Runs: The points he earned for his team in the 1986\u20131987 season\n* RBI: Number of players a batsman had jogged when he hit in the season\n* Walks: Number of mistakes made by the opposing player\n* Years: Player\u2019s playing time in major league (in year)\n* CAtBat: Number of shots made with a baseball bat in career\n* CHits: Number of hits made in the career\n* CHmRun: Most valuable hits in the career\n* CRuns: The points he earned for his team in his career\n* CRBI: Number of players a batsman had jogged when he hit in the career\n* CWalks: Number of mistakes made by the opposing player in career\n* League: A factor with A and N levels showing the league in which the player played until the end of the season\n* Division: A factor with levels E and W indicating the position played by the player at the end of 1986\n* PutOuts: Helping your teammate in-game\n* Assists: Number of assists made by the player in the 1986\u20131987 season\n* Errors: Player\u2019s errors in the 1986\u20131987 season\n* Salary: The salary of the player in the 1986\u20131987 season (in thousand)\n* NewLeague: A factor with A and N levels showing the player\u2019s league at the start of the 1987 season"}}