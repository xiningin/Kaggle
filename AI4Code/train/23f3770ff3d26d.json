{"cell_type":{"db229cc8":"code","4f264b65":"code","7e63be70":"code","9d056444":"code","7d081eab":"code","855ecd15":"code","a748c90f":"code","ab5df34d":"code","39fdc5d9":"code","16fa48fb":"code","be8dc5a7":"code","71ebe221":"code","469274ed":"code","bd0640b6":"code","d4013601":"code","a1581dc9":"code","0ae26ead":"code","3e12bfe2":"code","21ce9044":"code","09e5d301":"code","b9d48a62":"markdown","e9faf1f7":"markdown","eec119e7":"markdown","4c3696eb":"markdown","6bdbb1ec":"markdown","48953d32":"markdown","871a5888":"markdown","3cd2d4f1":"markdown","b9279b04":"markdown","32d9c0a4":"markdown","d82a1dd7":"markdown","cc83f34b":"markdown","58ef1032":"markdown"},"source":{"db229cc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Styles\nplt.style.use('ggplot')\nsns.set_style('whitegrid')\n\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 10\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 10\nplt.rcParams['figure.titlesize'] = 12\nplt.rcParams['patch.force_edgecolor'] = True\n\n# Text Preprocessing\nimport nltk\n# nltk.download(\"all\")\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.tokenize import word_tokenize\n\nimport spacy\n\nnlp = spacy.load(\"en\")","4f264b65":"spam_folder = '\/kaggle\/input\/ham-and-spam-dataset\/spam'\nham_folder = '\/kaggle\/input\/ham-and-spam-dataset\/ham'\n\nham_filenames = [name for name in sorted(os.listdir(ham_folder)) if len(name) > 20]\nspam_filenames = [name for name in sorted(os.listdir(spam_folder)) if len(name) > 20]\n\nprint('Number of non-spam samples:', len(ham_filenames))\nprint('Number of spam samples:', len(spam_filenames))\nprint('Ratio of non-spam to spam samples:', len(ham_filenames)\/len(spam_filenames))","7e63be70":"import email\nimport email.policy\n\ndef load_email(is_spam, filename):\n    directory = spam_folder if is_spam else ham_folder\n    \n    with open(os.path.join(directory, filename), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n    \nham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]","9d056444":"from collections import Counter\n\ndef get_email_structure(email):\n    if isinstance(email, str):\n        return email\n    payload = email.get_payload()\n    if isinstance(payload, list):\n        return \"multipart({})\".format(\", \".join([\n            get_email_structure(sub_email)\n            for sub_email in payload\n        ]))\n    else:\n        return email.get_content_type()\n\ndef structures_counter(emails):\n    structures = Counter()\n    for email in emails:\n        structure = get_email_structure(email)\n        structures[structure] += 1\n    return structures\n\nham_structure = structures_counter(ham_emails)\nspam_structure = structures_counter(spam_emails)","7d081eab":"def html_to_plain(email):\n    try:\n        soup = BeautifulSoup(email.get_content(), 'html.parser')\n        return soup.text.replace('\\n\\n','')\n    except:\n        return \"empty\"","855ecd15":"def email_to_plain(email):\n    struct = get_email_structure(email)\n    for part in email.walk():\n        partContentType = part.get_content_type()\n        if partContentType not in ['text\/plain','text\/html']:\n            continue\n        try:\n            partContent = part.get_content()\n        except: # in case of encoding issues\n            partContent = str(part.get_payload())\n        if partContentType == 'text\/plain':\n            return partContent\n        else:\n            return html_to_plain(part)\n        \n#print(email_to_plain(ham_emails[42]))\nprint(email_to_plain(spam_emails[45]))","a748c90f":"# structure all emails into plain text\nham_emails_plain = [email_to_plain(email) for email in ham_emails if len(ham_emails) > 100]\nspam_emails_plain = [email_to_plain(email) for email in spam_emails if len(spam_emails) > 100]\n\n# ham_structure = structures_counter(ham_emails_plain)\n\n# ham_structure.most_common()","ab5df34d":"# some data conversion to get it into pandas\nham_dic = {}\nspam_dic = {}\nham_dic['text'] = ham_emails_plain\nspam_dic['text'] = spam_emails_plain\n\nham_df = pd.DataFrame(ham_dic, columns = ['text', 'category'])\nspam_df = pd.DataFrame(spam_dic, columns = ['text', 'category'])\n\n# setting labels\nham_df['category'] = 0\nspam_df['category'] = 1\n\nframes = [ham_df, spam_df]\n\n# dataframe of messages with proper labels for spam and non-spam\nmessages = pd.concat(frames).reset_index(drop=True)","39fdc5d9":"# Dropping rows with NA values\nmessages.dropna(inplace=True)\n\nmessages[\"category\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\nplt.ylabel(\"Spam vs Ham\")\nplt.legend([\"Ham\", \"Spam\"])\nplt.show()","16fa48fb":"spam_messages = messages[messages[\"category\"] == 1][\"text\"]\nham_messages = messages[messages[\"category\"] == 0][\"text\"]\n\n\nspam_words = []\nham_words = []\n\n# # Since this is just classifying the message as spam or ham, we can use isalpha(). \n# # This will also remove the not word in something like can't etc. \n# # In a sentiment analysis setting, its better to use \n# # sentence.translate(string.maketrans(\"\", \"\", ), chars_to_remove)\n\ndef extractSpamWords(spamMessages):\n    global spam_words, spam_exception_count\n    spam_exception_count = 0\n    try:\n        word_tokenized = word_tokenize(spamMessages)\n        words = [word.lower() for word in word_tokenized if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n        spam_words = spam_words + words\n    except:\n        spam_exception_count += 1\n    \n    \n    \ndef extractHamWords(hamMessages):\n    global ham_words, ham_exception_count\n    ham_exception_count = 0\n    try:\n        word_tokenized = word_tokenize(hamMessages)\n        words = [word.lower() for word in  word_tokenized if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n        ham_words = ham_words + words\n    except:\n        ham_exception_count += 1\n        \n    \n# Checking tokenization errors. At some point I had to troubleshoot the code\nspam_messages.apply(extractSpamWords)\nprint('spam exception count: ', spam_exception_count)\nham_messages.apply(extractHamWords)\nprint('ham exception count: ', ham_exception_count)\n","be8dc5a7":"from wordcloud import WordCloud","71ebe221":"#Ham word cloud\n\nham_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(ham_words))\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","469274ed":"#Spam Word cloud\n\nspam_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(spam_words))\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()\n","bd0640b6":"# Top 10 spam words\n\nspam_words = np.array(spam_words)\nprint(\"Top 10 Spam words are :\\n\")\nprint(pd.Series(spam_words).value_counts().head(n = 10))\n\nham_words = np.array(ham_words)\nprint(\"\\nTop 10 Ham words are :\\n\")\nprint(pd.Series(ham_words).value_counts().head(n = 10))\n","d4013601":"messages[\"messageLength\"] = messages[\"text\"].apply(len)\nmessages[\"messageLength\"].describe()","a1581dc9":"f, ax = plt.subplots(1, 2, figsize = (20, 6))\n\nsns.distplot(messages[messages[\"category\"] == 1][\"messageLength\"], bins = 20, ax = ax[0])\nax[0].set_xlabel(\"Spam Message Word Length\")\n\nsns.distplot(messages[messages[\"category\"] == 0][\"messageLength\"], bins = 20, ax = ax[1])\nax[0].set_xlabel(\"Ham Message Word Length\")\n\nplt.show()\n","0ae26ead":"from nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\n\ndef cleanText(message):\n    \n    try:\n        message = message.translate(str.maketrans('', '', string.punctuation))\n        words = [stemmer.stem(word) for word in message.split() if word.lower() not in stopwords.words(\"english\")]\n        return \" \".join(words)\n    except:\n        print(message)\n        \n    \nmessages[\"text\"] = messages[\"text\"].apply(cleanText)\nmessages.head(n = 10)    \n","3e12bfe2":"from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", stop_words = \"english\")\nfeatures = vec.fit_transform(messages[\"text\"])\nprint(features.shape)\n","21ce9044":"from sklearn.model_selection import train_test_split\nprint(features.shape)\nprint(messages[\"category\"].shape)\nX_train, X_test, y_train, y_test = train_test_split(features, messages[\"category\"], stratify = messages[\"category\"], test_size = 0.2)\n","09e5d301":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\n\nnames = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n         \"Naive Bayes\", \"SVM Linear\"]\n\nclassifiers = [\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    LogisticRegression(),\n    SGDClassifier(max_iter = 100),\n    MultinomialNB(),\n    SVC(kernel = 'linear')\n]\n\nmodels = zip(names, classifiers)\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(\"\\n\" + name + \":\")\n    print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n    print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n    print(\"Confusion Matrix:\\n\") \n    confusion_m = confusion_matrix(y_test, y_pred)\n    print(confusion_m)\n","b9d48a62":"## Importing and Initializing plotting and feature extraction libraries","e9faf1f7":"## top words","eec119e7":"# Tokenization of Spam and Ham into words","4c3696eb":"## Converting different email types to normal text","6bdbb1ec":"## Word cloud for spam and ham","48953d32":"## Vectorization","871a5888":"## Here I tried to check if message length is a good additional feature but it seems not","3cd2d4f1":"## Distributions are too similar for Ham and Spam","b9279b04":"## Setting the input folder and reading the filenames","32d9c0a4":"## Getting data into Panda dataframe","d82a1dd7":"## Cleaning and stemming","cc83f34b":"## Loading data","58ef1032":"## Train and test data split"}}