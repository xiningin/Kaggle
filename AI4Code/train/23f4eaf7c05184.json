{"cell_type":{"6e47c55e":"code","4985dea2":"code","357b607d":"code","17af1bee":"code","0f9debdc":"code","0c51281d":"code","8bf2d5a3":"code","5a76d04a":"code","c7d3221e":"code","4453dfaa":"code","ff1f65d8":"code","15956121":"code","f279afe5":"code","90ff697d":"code","5468d9b9":"code","c3205828":"code","bafcdf4d":"code","26fb5462":"code","b20c358c":"code","3efe8ad5":"code","3cb799c2":"code","2aee3d52":"code","b73cf070":"code","c96f24ea":"code","aa2c1a32":"code","bcfbf6ed":"code","713edb08":"code","3b6ed1bb":"code","21f1edee":"code","9c55ec4c":"code","5403e36a":"code","3d7c677f":"code","104e37f8":"code","59843840":"code","dbff205d":"code","d05dc200":"code","bf181840":"code","be223143":"code","bd1758f2":"code","0720b112":"code","47610692":"code","beb6abe4":"code","1d9054f5":"code","f52c0148":"code","fe009669":"code","d4f319f6":"code","b7f5263f":"code","7905b85c":"code","d414f1f6":"code","f7122079":"code","18f1e6b9":"code","19522f8c":"code","f3385640":"code","6c3861f3":"code","be758e4f":"code","1a94969f":"code","d8e919c3":"code","f5b22a89":"code","182017f9":"code","d292d60e":"code","f71ec008":"code","7e400276":"code","c4448411":"code","e6fb76e1":"code","e8881cc9":"code","48d925cc":"code","beb91706":"code","a38ef5fb":"code","ed837ad7":"code","7384aa40":"code","cab5f71f":"code","c0069e89":"code","8b6d8768":"code","e025ed03":"code","8c5b87cb":"code","219500ab":"code","2e47d12c":"code","a0eb0e22":"code","d5281592":"code","98e8231b":"code","e2d73ac3":"code","27ff36bd":"code","664f6cf8":"code","6245bcb2":"code","2f54c2e9":"code","e70eb2d6":"code","126d0c62":"code","881a78ca":"code","4ec9c187":"code","d21a8d2c":"code","5dc4a442":"code","15471970":"code","943414a1":"code","39df8d5e":"code","c1886232":"code","55582551":"code","5b7364ec":"code","e4c8f6c3":"code","41995c6b":"code","59e4f908":"code","21cbc7ad":"code","02f61a98":"code","4ae5d3b2":"code","eac18758":"code","6a4a7ea8":"code","a8925132":"code","def218d2":"code","b7d8a798":"code","49aa47d5":"code","550bd2eb":"code","b7e998c3":"code","8110e36a":"code","57e6a7c2":"code","fb96e9e4":"code","5570306b":"code","bda09c47":"code","4bc037bf":"code","fb4049d6":"code","a81c6234":"code","7d1b0dd6":"code","4c0fa811":"code","d77d2b48":"code","50e4c779":"code","9b526bae":"code","858b47b3":"code","2b31238d":"code","13c40c52":"code","df822486":"code","d4bab30c":"code","ea43226f":"code","4ed96dc4":"code","fe409cd6":"code","fc3560f1":"code","4cba4526":"code","1b2b05ce":"code","0ff7c650":"code","d627e29d":"code","64a21228":"code","93fcf52c":"code","d1841a80":"code","b64dd8e6":"markdown","76c751f0":"markdown","3c363d52":"markdown","44a5cbdd":"markdown","33aabf05":"markdown","94a0cdfc":"markdown","fa4ff40e":"markdown","8125caa6":"markdown","3b75e432":"markdown","81daee63":"markdown","7f6470a3":"markdown","69ddb1ce":"markdown","c1929f7d":"markdown","42b0565a":"markdown","94049cfc":"markdown","823e9cfb":"markdown","36719e4d":"markdown","356ee185":"markdown","65cd5fec":"markdown","1a8d434c":"markdown","0a4b8b44":"markdown","d4d9f478":"markdown","27d59d35":"markdown"},"source":{"6e47c55e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4985dea2":"# import libraries\n# Importamos Librerias.\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random \n\n# visualization\nimport seaborn as sns\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import preprocessing\n## scikit modeling libraries\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n                             GradientBoostingClassifier, ExtraTreesClassifier,\n                             VotingClassifier)\n\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score, cross_val_predict,\n                                     StratifiedKFold, learning_curve)\n\n## Load metrics for predictive modeling\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n## Warnings and other tools\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")","357b607d":"# Load dataset train \ndf= pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')","17af1bee":"# delete enrolle_id features.\ndf.sort_values([\"enrollee_id\"])","0f9debdc":"df.drop([\"enrollee_id\"], axis = 'columns', inplace=True)","0c51281d":"# visualize features\ndf.columns","8bf2d5a3":"# Check dataframe basic info\nprint(\"Dataset has {0} Rows\".format(df.shape[0]))\nprint(\"Dataset has {0} Cols\".format(df.shape[1]))","5a76d04a":" # Check unique values \nprint (\"\\nUnique values :  \\n\",df.nunique())","c7d3221e":"# visualize target\nsns.countplot(x=df['target']);\nprint(df.target.value_counts(normalize=True))","4453dfaa":"# Missings values\n# Detecci\u00f3n de los valores nulos por variables.\n\nnull_count = df.isnull().sum()\nnull_percentage = round((df.isnull().sum()\/df.shape[0])*100, 2)\nnull_df = pd.DataFrame({'column_name' : df.columns,'null_count' : null_count,'null_percentage': null_percentage})\nnull_df.reset_index(drop = True, inplace = True)\nnull_df.sort_values(by = 'null_percentage', ascending = False)","ff1f65d8":"# check duplicates values and delete.\ndf.duplicated().sum()\ndf.drop_duplicates()","15956121":"# Target Objetivo en el dataset \nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\nplt.subplots_adjust(hspace = 0.3)\n\n# Outliers con boxplot\nsns.boxplot(df['city_development_index'], ax=axes[0, 0])\naxes[0, 0].set_title(\"Outliers con boxplot\")\n\n# Feature susceptible de transformaci\u00f3n\nsns.boxplot(df['city_development_index'], ax=axes[0, 1])\naxes[0, 1].set_title(\"Feature susceptible de transformaci\u00f3n\")\n\n# Transformacion logaritmica\nsns.boxplot(np.log(df['city_development_index']), ax=axes[0, 2])\naxes[0, 2].set_title(\"Transformacion logaritmica\")\n\n# Posibles outliers mediante FDP\nsns.kdeplot(df['city_development_index'], ax=axes[1, 0])\naxes[1, 0].set_title(\"Posibles outliers en FDP\")\n\n# Outliers mediante bivariante\nsns.scatterplot(data=df, x='city_development_index', y='target', ax=axes[1, 1])\naxes[1, 1].set_title(\"Outliers mediante bivariante\")\n\n# Transformacion de FDP\nsns.kdeplot(np.log(df[\"city_development_index\"]), ax=axes[1, 2])\naxes[1, 2].set_title(\"Transformacion \");","f279afe5":"# Target Objetivo en el dataset \nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\nplt.subplots_adjust(hspace = 0.3)\n\n# Outliers con boxplot\nsns.boxplot(df['training_hours'], ax=axes[0, 0])\naxes[0, 0].set_title(\"Outliers con boxplot\")\n\n# Feature susceptible de transformaci\u00f3n\nsns.boxplot(df['training_hours'], ax=axes[0, 1])\naxes[0, 1].set_title(\"Feature susceptible de transformaci\u00f3n\")\n\n# Transformacion logaritmica\nsns.boxplot(np.log(df['training_hours']), ax=axes[0, 2])\naxes[0, 2].set_title(\"Transformacion logaritmica\")\n\n# Posibles outliers mediante FDP\nsns.kdeplot(df['training_hours'], ax=axes[1, 0])\naxes[1, 0].set_title(\"Posibles outliers en FDP\")\n\n# Outliers mediante bivariante\nsns.scatterplot(data=df, x='training_hours', y='target', ax=axes[1, 1])\naxes[1, 1].set_title(\"Outliers mediante bivariante\")\n\n# Transformacion de FDP\nsns.kdeplot(np.log(df[\"training_hours\"]), ax=axes[1, 2])\naxes[1, 2].set_title(\"Transformacion \");","90ff697d":"from collections import Counter\n\n#drop the label \noutlier_list = ['city_development_index', 'training_hours'] \n\ndef detect_outlier(df,feature):\n    \n    outlier_indices = []\n    \n    for f in feature:\n        \n        #lower quartile\n        q1 = np.percentile(df[f],25)\n        \n        #upper quartile\n        q3 = np.percentile(df[f],75)\n        \n        #interquartile range\n        iqr = q3-q1\n        \n        #with coefficient\n        with_coef = 1.5*iqr\n        \n        #detect outlier(s)\n        lower_rule = q1-with_coef\n        upper_rule = q3+with_coef\n        \n        outlier_variable = df[(df[f]<lower_rule) | (df[f]>upper_rule)].index\n        outlier_indices.extend(outlier_variable)\n    \n    #converting to amount\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i,v in outlier_indices.items() if v>2)\n    \n    return multiple_outliers\n\n \n#drop outliers\ndf = df.drop(detect_outlier(df,outlier_list),axis=0).reset_index(drop=True)","5468d9b9":"# Check values\ndf['city'].unique().tolist()","c3205828":"# count values\nlen(df['city'].unique().tolist())","bafcdf4d":"# delete this features\ndf.drop(['city'], axis=1,inplace=True)","26fb5462":"# check values\ndf['city_development_index'].unique().tolist()","b20c358c":"# visualize features\nsns.kdeplot(data=df, x=\"city_development_index\")\n","3efe8ad5":"# compare this feature with target\nsns.kdeplot(data=df, x=\"city_development_index\", hue=\"target\")\n","3cb799c2":"plt.figure(figsize=(12, 5))\nsns.boxplot(x='target',y='city_development_index',data=df)\nxlabel='Looking for job'\nylabel = 'city_development_index'","2aee3d52":"# check values of this features\ndf['gender'].value_counts()","b73cf070":"# Visualize gender\n# create a figure and axis \nfig, ax = plt.subplots() \n# count the occurrence of each class \ndata = df['gender'].value_counts() \n# get x and y data \npoints = data.index \nfrequency = data.values \n# create bar chart \nax.bar(points, frequency) \n\n# set title and labels \nax.set_title('Compare Gender') \nax.set_xlabel('Gender') \nax.set_ylabel('Number')","c96f24ea":"# missings values \ndf['gender'].isna().sum()","aa2c1a32":"# filling missings values with random imputation, 50% female, 50% male\nfrom random import choice\n\nmask = df[\"gender\"].isna()\n\ndf.loc[mask,[\"gender\"]] = df.loc[mask,[\"gender\"]].applymap(lambda _: choice([\"Male\",\"Female\"]))","bcfbf6ed":"# value \"Other\", in gender  is replace with female values.\ndf['gender'].replace({\"Other\": \"Female\"}, inplace=True)","713edb08":"# visualize this features with target\nsns.countplot(x=df.gender, hue=df.target, palette='light:#5A9')","3b6ed1bb":"sns.barplot(x=\"gender\", y=\"target\", data=df)\nplt.show()","21f1edee":"# Binarize male y female feature.\ndf['Sex_bin'] = df['gender']\ndf = df.replace({'Sex_bin': {'Male': 0, 'Female': 1}})\ndf.head(4)","9c55ec4c":"# delete this columns\ndf.drop(['gender'], axis=1,inplace=True)\n","5403e36a":"# check values\nsns.countplot(x=df.relevent_experience)\n","3d7c677f":"# compare with target\nsns.countplot(x=df.relevent_experience, hue=df.target, palette='flare', dodge=False) ","104e37f8":"# compare with gender\npd.crosstab(df['relevent_experience'], df['Sex_bin'], margins=False).apply(lambda r:(r\/r.sum())*100, axis=0)\n","59843840":"# Binarize Relevent_ experience\ndf['relev_exp_bin'] = df['relevent_experience']\ndf = df.replace({'relev_exp_bin': {'Has relevent experience': 1, 'No relevent experience': 0}})","dbff205d":"# delete this features\ndf.drop(['relevent_experience'], axis=1,inplace=True)\n","d05dc200":"# Missin values\ndf['education_level'].isna().sum()\n","bf181840":"# filling missing values\ndf['education_level'].fillna('High School', inplace=True)","be223143":"# visualize data\nlabels = list(df[\"education_level\"].value_counts().index)\nvalues = list(df[\"education_level\"].value_counts().values)\n\nplt.figure(figsize=(8,8))\nplt.pie(values, labels=labels, autopct=\"%1.1f%%\",\n        colors=[\"#7fcc00\", \"#ff3a33\", \"#f7ff66\", \"#3339ff\", \"#99ffa9\"],\n        shadow=True, explode = [0.03, 0.03, 0.03, 0.03, 0.03],\n        startangle=270)\nplt.title(\"Education Level\");","bd1758f2":"# compare this feature with target\nsns.catplot(x=\"education_level\", y=\"target\", data=df, kind=\"bar\", ci=None)","0720b112":"# check values\nsns.countplot(y= df['enrolled_university'], palette=\"Blues\")\n","47610692":"df.drop(['enrolled_university'],axis=1,inplace=True) \n# delete this columns because is not useful","beb6abe4":"# check values\nsns.countplot(x=df.major_discipline, palette=\"Set3\")","1d9054f5":"# compare this features with target\npd.crosstab(df['major_discipline'], df['target'], margins=False).apply(lambda r:(r\/r.sum())*100, axis=0)","f52c0148":"# visualize this feature with target.\nplt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'major_discipline',hue='target',data=df)","fe009669":"# delete this feature, is note very helpful\ndf.drop(['major_discipline'], axis=1,inplace=True)\n","d4f319f6":"# missing values\ndf['experience'].isna().sum()","b7f5263f":"df['experience'].fillna(\"0\", inplace=True)","7905b85c":"df[\"experience\"].replace({\">20\": \"21\",\"<1\":\"0\"}, inplace=True)","d414f1f6":"df[\"experience\"] = pd.to_numeric(df[\"experience\"])","f7122079":"df[\"experience\"] = pd.to_numeric(df[\"experience\"])","18f1e6b9":"df[\"experience\"].dtypes","19522f8c":"# visuealize with target\nplt.figure(figsize= [15.0, 7.0])\nsns.barplot(x=\"experience\", y=\"target\", data=df)","f3385640":"# Imputation\nbins = [0, 5, 10, 15, 20, np.inf]\nlabels = ['0-5', '5-10', '10-15', '15-20', '+20']\nfor score in df:\n    df['years_experience'] = pd.cut(df[\"experience\"], bins=bins, labels=labels)","6c3861f3":"plt.figure(figsize= [15.0, 7.0])\nsns.barplot(x=\"years_experience\", y=\"target\", data=df)","be758e4f":"df.drop(['experience'], axis=1,inplace=True)\n","1a94969f":"sns.countplot(x=df.company_size, palette=\"Set3\")\n","d8e919c3":"# Missing values.\ndf['company_size'].isna().sum()","f5b22a89":"df['company_size'].fillna(method='ffill', limit=1, inplace=True)","182017f9":"df['company_size'].fillna(method='bfill', limit=1, inplace=True)","d292d60e":"# Imputation, reduce values\ndf['company_size'] = df.company_size.replace({'50-99':\"Medium\",\n\"<10\":\"Small\",\n\"10000+\":\"Big\",\n\"5000-9999\": \"Big\",\n\"1000-4999\":\"Big\",\n\"10\/49\":\"Small\",\n\"100-500\":\"Medium\",\n\"500-999\":\"Big\"})","f71ec008":"# compare with target\nplt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'company_size',hue='target',data=df)","7e400276":"# Missing values.\ndf['company_size'].isna().sum()","c4448411":"df['company_size'] = df.company_size.fillna(\"Small\")\n","e6fb76e1":"sns.countplot(y= df['company_type'], palette=\"Blues\") \n","e8881cc9":"# compare with target\nplt.figure(figsize=(10, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'company_type',hue='target',data=df,palette='winter_r')","48d925cc":"# missing values\ndf['company_type'].isnull().sum()\/len(df)*100","beb91706":"#Too much missing values, so we delete this features\ndf.drop(['company_type'], axis=1,inplace=True)\n","a38ef5fb":"# missing values\ndf['last_new_job'].fillna('never', inplace=True)","ed837ad7":"# replace values\ndf[\"last_new_job\"].replace({\">4\": \"5\",\"never\":\"0\"}, inplace=True)","7384aa40":"df[\"last_new_job\"] = pd.to_numeric(df[\"last_new_job\"])","cab5f71f":"sns.countplot(x=df.last_new_job, palette=\"Reds\")\n","c0069e89":"plt.figure(figsize= [15.0, 7.0])\nsns.barplot(x=\"last_new_job\", y=\"target\", data=df)","8b6d8768":"sns.distplot(df.training_hours);\nplt.figure(figsize=(12, 5))\nsns.set_style('whitegrid')\nsns.kdeplot(data=df, x=\"training_hours\", hue=\"target\",cut = 0,palette='Set1')\n","e025ed03":"sns.heatmap(df.isnull(),yticklabels=False,cmap=\"viridis\")","8c5b87cb":"# delete missing values\ndf.dropna(inplace=True)","219500ab":"# Standarize features\ncols_to_standarize = ['training_hours','last_new_job']\nfor col in cols_to_standarize:\n  df[col] = preprocessing.scale(df[col], axis=0, copy=False)","2e47d12c":"# get dummies, object features\ncategorical_feature = (df.dtypes == \"category\") | (df.dtypes == object)\ncategorical_cols = df.columns[categorical_feature].tolist()\ndf = pd.get_dummies(df, columns=categorical_cols)","a0eb0e22":"# correlation \ncorrelation_matrix = df.corr()","d5281592":"plt.figure(figsize=(20.0,12.0))\nplt.title('Pearson Correlation of Features')\nsns.heatmap(correlation_matrix, annot=True)","98e8231b":"# show up the most features correlate\ncorr = df.corr()\nprint(corr['target'].sort_values(ascending=False)[:6], '\\n')\nprint(corr['target'].sort_values(ascending=False)[-6:])","e2d73ac3":"# copy for later, for an undersampling\ndf1=df","27ff36bd":"X = df.drop(['target'], axis = 1)\ny = df['target']","664f6cf8":"# Get the most importance features with SelectKBest method\nselector = SelectKBest(score_func=f_classif)\nX_new = selector.fit_transform(X, y)\n\nscores = sorted(zip(map(lambda x: round(x, 4), selector.scores_), X.columns), reverse=True)\npd.DataFrame(scores, columns=['Score', 'target']).head(6)","6245bcb2":"X=df[['city_development_index','years_experience_0-5','relev_exp_bin','years_experience_+20','education_level_Graduate','last_new_job']]","2f54c2e9":"y = df['target']","e70eb2d6":"X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.2, random_state = 1000)","126d0c62":"# Algoritmo\nmodels, names = list(), list() \n\nmodels.append(LogisticRegression()) \nnames.append('LR') \nmodels.append(KNeighborsClassifier()) \nnames.append('KNN') \nmodels.append( DecisionTreeClassifier()) \nnames.append('DT') \nmodels.append(GaussianNB()) \nnames.append('NB') \nmodels.append(SVC(gamma='auto')) \nnames.append('SVC') \nmodels.append(RandomForestClassifier())\nnames.append('RF')\nmodels.append(GradientBoostingClassifier())\nnames.append('GB')\n\n\nresults = list() \n\n# cross validation score,\nfor model, name in zip(models, names) :\n\tresult = cross_val_score(model, X_train, y_train, cv=10, scoring=\"accuracy\")\n\tresults.append(result)\n","881a78ca":"for result, name in zip(results, names):\n\tprint(\"Accuracy of {}: {}\".format(name, result.mean()))\n","4ec9c187":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)","d21a8d2c":"print('Accuracy :       ', accuracy_score(y_test, y_predict))\nprint('F-Measure :      ', f1_score(y_test, y_predict, average = 'weighted'))\nprint('ROC Score :      ', roc_auc_score(y_test, y_predict))","5dc4a442":"# GBC Classifier tuning - \nmodel_2 = GradientBoostingClassifier()\n\n# Ajuste de par\u00e1metros\ngb_param_grid = {\"loss\": [\"deviance\"], \n                          \"learning_rate\": [0.05, 0.1, 0.2, 0.4, 0.5], \n                          \n                          \"n_estimators\": [20,50,100,200], \n                          \n                          \"max_depth\": [1,2,3,4,5], \n                          \n                          \n                          \"max_features\": [\"sqrt\", 3, 4], \n                          }\n\n#  gridSearchCV\ngsGBC = GridSearchCV(model_2, param_grid=gb_param_grid, cv=10,\n                     scoring='accuracy', n_jobs=4, verbose=1)\n\n# Train Model\ngsGBC.fit(X_train, y_train)\n\ny_predict = gsGBC.predict(X_test)","15471970":"print('Accuracy :       ', accuracy_score(y_test, y_predict))\nprint('F-Measure :      ', f1_score(y_test, y_predict, average = 'weighted'))\nprint('ROC Score :      ', roc_auc_score(y_test, y_predict))","943414a1":"# Create a confusion matrix\nmatrix = confusion_matrix(y_test, y_predict)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","39df8d5e":"# Create a performance_auc dict\nperformance_auc = {}","c1886232":"# Visualize results by ROC graph\nfpr, tpr, thresholds = roc_curve(y_test, y_predict)\nroc_auc = auc(fpr, tpr)\nperformance_auc['GB'] = roc_auc\n\n# Plotting\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","55582551":"X=df1[['city_development_index','years_experience_0-5','relev_exp_bin','years_experience_+20','education_level_Graduate','last_new_job']]","5b7364ec":"y = df1['target']","e4c8f6c3":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1, stratify=df['target'])","41995c6b":"y_train.value_counts()","59e4f908":"rus = RandomUnderSampler(random_state=42)\nX_train_under, y_train_under = rus.fit_resample(X_train, y_train)","21cbc7ad":"y_train_under.value_counts()","02f61a98":"X_train_under, X_test, y_train_under, y_test = train_test_split (X, y, test_size = 0.2, random_state = 1000)","4ae5d3b2":"# Models\nmodels, names = list(), list() \n\nmodels.append(LogisticRegression()) \nnames.append('LR') \nmodels.append(KNeighborsClassifier()) \nnames.append('KNN') \nmodels.append( DecisionTreeClassifier()) \nnames.append('DT') \nmodels.append(GaussianNB()) \nnames.append('NB') \nmodels.append(SVC(gamma='auto')) \nnames.append('SVC') \nmodels.append(RandomForestClassifier())\nnames.append('RF')\nmodels.append(GradientBoostingClassifier())\nnames.append('GB')\n\n\nresults = list() \n\n# cross validation score,\nfor model, name in zip(models, names) :\n\tresult = cross_val_score(model, X_train_under, y_train_under, cv=10, scoring=\"accuracy\")\n\tresults.append(result)\n","eac18758":"for result, name in zip(results, names):\n\tprint(\"Accuracy of {}: {}\".format(name, result.mean()))\n","6a4a7ea8":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)","a8925132":"print('Accuracy :       ', accuracy_score(y_test, y_predict))\nprint('F-Measure :      ', f1_score(y_test, y_predict, average = 'weighted'))\nprint('ROC Score :      ', roc_auc_score(y_test, y_predict))","def218d2":"\n# GBC Classifier tuning -\nmodel_2 = GradientBoostingClassifier()\n\n\ngb_param_grid = {\"loss\": [\"deviance\"], \n                          \"learning_rate\": [0.05, 0.1, 0.2],  \n                          \n                          \"n_estimators\": [20,50,100], \n                                                           \n                          \n                          \"max_depth\": [1,3,5], \n                                                    \n                          \n                          \n                          \"max_features\": [\"sqrt\", 3, 4], \n                          }\n\n# gridSearchCV\ngsGBC = GridSearchCV(model_2, param_grid=gb_param_grid, cv=10,\n                     scoring='accuracy', n_jobs=4, verbose=1)\n\n# Train Model\ngsGBC.fit(X_train, y_train)\n\ny_predict = gsGBC.predict(X_test)","b7d8a798":"print('Accuracy :       ', accuracy_score(y_test, y_predict))\nprint('F-Measure :      ', f1_score(y_test, y_predict, average = 'weighted'))\nprint('ROC Score :      ', roc_auc_score(y_test, y_predict))","49aa47d5":"# Cross validate model with Kfold stratified cross val\nK_fold = StratifiedKFold(n_splits=10)\n\n\n# Modeling step Test differents algorithms \nrandom_state = 17\n\nmodels = [] # append all models or predictive models \ncv_results = [] # cross validation result\ncv_means = [] # cross validation mean value\ncv_std = [] # cross validation standard deviation\n\nmodels.append(KNeighborsClassifier())\nmodels.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nmodels.append(DecisionTreeClassifier(random_state=random_state))\nmodels.append(RandomForestClassifier(random_state=random_state))\nmodels.append(ExtraTreesClassifier(random_state=random_state))\nmodels.append(SVC(random_state=random_state))\nmodels.append(GradientBoostingClassifier(random_state=random_state))\nmodels.append(LogisticRegression(random_state = random_state))\nmodels.append(LinearDiscriminantAnalysis())\nmodels.append(MLPClassifier(random_state=random_state))\n\n\nfor model in models :\n    cv_results.append(cross_val_score(model, X_train, y_train, \n                                      scoring = \"accuracy\", cv = K_fold, n_jobs=4))\n\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_frame = pd.DataFrame(\n    {\n        \"CrossValMeans\":cv_means,\n        \"CrossValErrors\": cv_std,\n        \"Algorithms\":[\n                     \"KNeighboors\",\n                     \"AdaBoost\", \n                     \"DecisionTree\",   \n                     \"RandomForest\",\n                     \"ExtraTrees\",\n                     \"SVC\",\n                     \"GradientBoosting\",                      \n                     \"LogisticRegression\",\n                     \"LinearDiscriminantAnalysis\",\n                     \"MultipleLayerPerceptron\"]\n    })\n\ncv_plot = sns.barplot(\"CrossValMeans\",\"Algorithms\", data = cv_frame,\n                palette=\"husl\", orient = \"h\", **{'xerr':cv_std})\n\ncv_plot.set_xlabel(\"Mean Accuracy\")\ncv_plot = cv_plot.set_title(\"CV Scores\")","550bd2eb":"# GBC Classifier tuning - ajuste de Hyperpar\u00e1metros\nGBC = GradientBoostingClassifier()\n\n# Ajuste de par\u00e1metros\ngb_param_grid = {\n    'loss': ['deviance', 'exponential'],\n    'n_estimators' : [25, 50],\n    'learning_rate': [ 0.05, 1],\n    'max_depth': [3, 7],\n    'min_samples_leaf': [25, 100],\n    'max_features': [ 0.3]\n\n}\n\n# Creamos el gridSearchCV\ngsGBC = GridSearchCV(GBC, param_grid=gb_param_grid, cv=K_fold,\n                     scoring='accuracy', n_jobs=4, verbose=1)\n\n# Ajustamos al modelo\ngsGBC.fit(X_train, y_train)\n\n# Extrae el mejor resultado\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","b7e998c3":"VotingPredictor = VotingClassifier(estimators =\n                            [('gbc', GBC_best)],\n                           voting='soft', n_jobs = 4)\n\n# The best result for or training\nVotingPredictor = VotingPredictor.fit(X_train, y_train)","8110e36a":"model=GradientBoostingClassifier()","57e6a7c2":"df_test= pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","fb96e9e4":"df_test.drop(['company_type', 'company_size','gender','enrolled_university','city','training_hours','major_discipline'], axis = 'columns', inplace=True)","5570306b":"df_test.info()","bda09c47":"# missing values in Test dataset\nnull_count = df_test.isnull().sum()\nnull_percentage = round((df_test.isnull().sum()\/df_test.shape[0])*100, 2)\nnull_df = pd.DataFrame({'column_name' : df_test.columns,'null_count' : null_count,'null_percentage': null_percentage})\nnull_df.reset_index(drop = True, inplace = True)\nnull_df.sort_values(by = 'null_percentage', ascending = False)","4bc037bf":"df_test['education_level'].fillna('High School', inplace=True)","fb4049d6":"df_test['last_new_job'].fillna('never', inplace=True)","a81c6234":"df_test[\"last_new_job\"].replace({\">4\": \"5\",\"never\":\"0\"}, inplace=True)","7d1b0dd6":"df_test[\"last_new_job\"] = pd.to_numeric(df[\"last_new_job\"])","4c0fa811":"df_test[\"last_new_job\"].isna().sum()","d77d2b48":"df_test['experience'].fillna(\"0\", inplace=True)","50e4c779":"df_test[\"experience\"].replace({\">20\": \"21\",\"<1\":\"0\"}, inplace=True)","9b526bae":"df_test[\"experience\"] = pd.to_numeric(df_test[\"experience\"])","858b47b3":"df_test[\"experience\"].dtypes","2b31238d":"bins = [0, 5, 10, 15, 20, np.inf]\nlabels = ['0-5', '5-10', '10-15', '15-20', '+20']\nfor score in df:\n    df_test['years_experience'] = pd.cut(df_test[\"experience\"], bins=bins, labels=labels)","13c40c52":"df_test.drop(['experience'], axis=1,inplace=True)","df822486":"df_test['relev_exp_bin'] = df_test['relevent_experience']\ndf_test = df_test.replace({'relev_exp_bin': {'Has relevent experience': 1, 'No relevent experience': 0}})","d4bab30c":"df_test.dropna(inplace=True)","ea43226f":"# get dummies\ncategorical_feature = (df_test.dtypes == \"category\") | (df_test.dtypes == object)\ncategorical_cols = df_test.columns[categorical_feature].tolist()\ndf_test = pd.get_dummies(df_test, columns=categorical_cols)","4ed96dc4":"# data estandarizacion\ncols_to_standarize = ['last_new_job']\nfor col in cols_to_standarize:\n  df_test[col] = preprocessing.scale(df_test[col], axis=0, copy=False)","fe409cd6":"df_test.info()","fc3560f1":"df_test = df_test.drop(columns = ['education_level_High School','education_level_Masters','education_level_Phd','education_level_Primary School'])","4cba4526":"validations_cols = X_train.columns.values.tolist()\nvalidation = df_test[validations_cols]","1b2b05ce":"df_test.shape","0ff7c650":"validation.shape","d627e29d":"ids = df_test['enrollee_id']\ndf_test.drop('enrollee_id', axis=1, inplace=True)","64a21228":"# Realizando la predicci\u00f3n al conjunto de datos de validaci\u00f3n\ndata_val = VotingPredictor.predict(validation)","93fcf52c":"submission = pd.DataFrame({\n    'Worker_ID' :ids,\n    'Jobseeker': data_val\n})","d1841a80":"submission.to_csv(\"mysubmission.csv\", index=False)","b64dd8e6":"### last_new_job","76c751f0":"### experience","3c363d52":"### Machine leaning with Undersampling","44a5cbdd":"### education_level","33aabf05":"### Missing values","94a0cdfc":"### Company_type\n","fa4ff40e":"## Outliers","8125caa6":"Conclusion: the best accuracy and models is without undersampling and without Tuning\n","3b75e432":"### company_size","81daee63":"### Relevent_experience","7f6470a3":"### major_discipline","69ddb1ce":"### City","c1929f7d":"### enrolled_university","42b0565a":"### last_new_job","94049cfc":"### City_development_index","823e9cfb":"# Prediction","36719e4d":"### education_level","356ee185":"### training_hours","65cd5fec":"## Analyse and Cleaning Data","1a8d434c":"### Machine Learning without Undersampling\n","0a4b8b44":"# Machine Learning","d4d9f478":"### gender","27d59d35":"### Experience"}}