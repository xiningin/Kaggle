{"cell_type":{"02a9c5a2":"code","a3ecf496":"code","9ce9a4cb":"code","fff48a59":"code","f5933da2":"code","c84305e6":"code","778f1842":"code","522eb4b2":"code","ed49ccc1":"code","a688e37a":"code","ba4f5ff4":"code","fba1d544":"code","06e0e6a9":"code","886bc03a":"code","f2048623":"code","c32bf497":"code","8ddc1691":"code","018f0f4f":"code","951af68c":"code","a749ef82":"code","548190db":"code","9720f61c":"code","ba7bd417":"code","a1370051":"code","435d7839":"code","f2b08a87":"code","340db015":"code","3a225ff6":"code","541e91cc":"code","8da4ca88":"code","b6caefb0":"code","7a6993da":"code","29bcb71e":"code","a4bbff65":"code","f0492cfd":"code","1519b635":"code","ee3b372a":"code","328df806":"code","1fa95255":"code","169be192":"code","5988fbdd":"code","9ac5b25c":"code","09e5cb62":"code","e1ddc9be":"code","584dbb41":"code","17b8e882":"code","cd1ec51d":"code","f5657c95":"code","195b6d05":"code","51d1d99a":"code","674e7a7d":"code","01a9e4af":"code","48e8b373":"code","346cdf0d":"code","50fa6dbe":"code","bbd1ebff":"code","1075a78b":"code","bba3a724":"markdown","76911d57":"markdown","dcc85075":"markdown","caa4365a":"markdown","d5e332f1":"markdown","7f7b82ed":"markdown","af5d003f":"markdown","b15fcab6":"markdown","fe22d042":"markdown","2715e46c":"markdown","1d6dcf2a":"markdown","1c6e99c3":"markdown","7d4024b3":"markdown","486d5d1e":"markdown","ce49c512":"markdown","a01c189a":"markdown","5b0a6221":"markdown","c2825236":"markdown","f3520f44":"markdown","94943083":"markdown","1c8f3faa":"markdown","9e8af25d":"markdown","860657bf":"markdown","a5a7f422":"markdown","840756a6":"markdown","7e0b041b":"markdown","38cdf38f":"markdown","c531df60":"markdown","4b4ca36b":"markdown","12d3349e":"markdown","a105292f":"markdown","a5b036fe":"markdown","a065b80e":"markdown","32145ce4":"markdown","55d1e3a3":"markdown","4df65ae2":"markdown","815a8a0f":"markdown","03f80610":"markdown","36fe9eee":"markdown","2ed6c6f7":"markdown","9459f6de":"markdown","6d10862b":"markdown","0e2bd34e":"markdown","b394f4b9":"markdown","bf7da8de":"markdown","30928f6b":"markdown","24971dd0":"markdown","3682805a":"markdown","1ce1cd0c":"markdown","e4398b38":"markdown","a84d4991":"markdown","fa95fe29":"markdown","67dadb24":"markdown","c9d5d88a":"markdown"},"source":{"02a9c5a2":"import numpy as np \nimport pandas as pd \n\ndb = pd.read_csv('\/kaggle\/input\/ecommerce-users-of-a-french-c2c-fashion-store\/6M-0K-99K.users.dataset.public.csv')","a3ecf496":"repeat_columns = []\n# unused and repeated metadata are dropped\nrepeat_columns += ['identifierHash', 'type','country','gender','civilityTitle']\ndb1=db.drop(repeat_columns,axis=1)\ndb1.head()","9ce9a4cb":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\n\nstring_columns = ['language','countryCode','hasAnyApp','hasAndroidApp','hasIosApp','hasProfilePicture']\n\nfor var in string_columns:\n    var_cat = db[[var]] #use double brakets to make sure i'm taking a dataframe \n    var_cat_encoded = ordinal_encoder.fit_transform(var_cat)\n    var_cat_df = pd.DataFrame(var_cat_encoded)\n    var_cat_df.columns = [var + '_encoded'] \n    db1 = db1.merge(var_cat_df, how = 'inner', left_index = True, right_index = True)\n\ndb2 = db1.drop(string_columns, axis = 1)\ndb2.head()\ndb2.info()","fff48a59":"#remove variables with no correlations\nno_columns=['seniority','seniorityAsMonths','seniorityAsYears']\n#week_columns=['language_encoded']\n#unused_columns=no_columns+week_columns\ndb3 = db2.drop(no_columns, axis = 1)","f5933da2":"def models(X_train,y_train,X_test,y_test):\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.linear_model import LinearRegression\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error as mae\n    from sklearn.metrics import mean_squared_error as mse\n    from sklearn.metrics import r2_score as r2\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import GradientBoostingRegressor as gb\n    from sklearn.linear_model import Ridge\n    from sklearn.linear_model import Lasso\n    from sklearn.svm import SVR\n    from sklearn.model_selection import cross_val_score\n    import warnings\n    warnings.simplefilter('ignore')\n    from sklearn.metrics import confusion_matrix , classification_report\n    from sklearn.metrics import accuracy_score\n    from sklearn.linear_model import LogisticRegression \n    from sklearn import svm\n    import xgboost as xgb\n    print('Select 1 : Naive Bayes, 2: Support Vector Machines, 3: Logistic Regression, 4: Decision Tree, 5: RandomForestClassifier,6:Xtreme gradient boosting')\n    mo = int(input())\n    list=[1,2,3,4,5,6]\n\n    if mo == 1 :\n        model = GaussianNB()\n    elif mo == 2 :\n        model = svm.SVC()\n    elif mo == 3 :\n        model = LogisticRegression()\n    elif mo == 4 :\n        model = DecisionTreeClassifier()\n    elif mo == 5 :\n        model = RandomForestClassifier(random_state=15325)\n    elif mo == 6 :\n        model = xgb.XGBClassifier()\n    else :\n        print('Invalid Entry')\n    model.fit(X_train,Y_train)\n    predict= model.predict(X_test)\n    print(\"testing set accuracy score: \",accuracy_score(Y_test,predict))\n    accuracies = cross_val_score(estimator = model , X= X_train , y=Y_train , cv =10)\n    print(\"testing set accuracy mean: \", accuracies.mean())\n    print(classification_report(Y_test,predict))\n    print(\"confusion matrix: \")\n    print(confusion_matrix(Y_test,predict))\n    ","c84305e6":"#define confusion matrix visualization function\nimport matplotlib.pyplot as plt\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    import itertools\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","778f1842":"#define accuracy function\ndef predAcc(x_train,x_test,y_train,y_test,model):\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import confusion_matrix , classification_report\n    model.fit(x_train,y_train)\n    predictTR= model.predict(x_train)\n    predictTT= model.predict(x_test)\n    print('Accuracy on train set: {:.3f}'.format(accuracy_score(y_train,predictTR)))\n    print('Accuracy on test set: {:.3f}'.format(accuracy_score(y_test,predictTT)))\n    print('Model Evaluation:')\n    print(\"classification report of train set: \")\n    print(classification_report(y_train,predictTR))\n    print(\"classification report of test set: \")\n    print(classification_report(y_test,predictTT))\n    \n    \n    print(\"Confusion matrix of test set: \")\n    cnf_matrix = confusion_matrix(y_test, predictTT)\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, \n                      #classes=['Inactive buyers','Occational buyers','Frequent buyers'],\n                      classes=['Inactive sellers','Occational sellers','Frequent sellers'],  \n                      title='Confusion matrix  accumulate')\n    \n    #Y_pred_prob=logreg.predict_proba(X_test)\n    #if (func in ['logreg','logregf','rlogreg','rlogregf']):\n        #print(f\"Coefficient: {func.coef_} \")\n        #print(f\"intercept: {logreg.intercept_} \")","522eb4b2":"#define feature selection function\ndef feaSelect(x_train,y_train,func,x):\n    import warnings\n    warnings.simplefilter('ignore')\n    from sklearn.feature_selection import RFE\n    predictors=x_train\n    selector=RFE(func,n_features_to_select=1)\n    selector=selector.fit(predictors,y_train)\n    order=selector.ranking_\n    order\n\n    feature_ranks=[]\n    for i in order:\n        feature_ranks.append(f\"{i-1}.{x.columns[i-1]}\")\n    \n    print(feature_ranks)","ed49ccc1":"def org(x,y):\n    #split the dataset\n    X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.33, random_state=42)\n    #rebuild model\n    model.fit(X_train,Y_train)\n    print(\"Original Model Evaluation: \")\n    predAcc(X_train, X_test, Y_train, Y_test, model)\n    print(\"Original Model's features selection: \")\n    feaSelect(X_train,Y_train,model,x)","a688e37a":"def filt(xf,yf):\n    #re-split the filtered dataset\n    Xf_train,Xf_test,Yf_train,Yf_test=train_test_split(xf,yf,test_size=0.33, random_state=42)\n    #rebuild model\n    model.fit(Xf_train,Yf_train)\n    print(\"Filtered Model Evaluation: \")\n    predAcc(Xf_train, Xf_test, Yf_train, Yf_test, model)\n    print(\"Filtered Model's Feature Importance Ranking: \")\n    feaSelect(Xf_train,Yf_train,model,xf)","ba4f5ff4":"def balanced(x_ros,y_ros):\n    #re-split the balanced dataset\n    X_ros_train,X_ros_test,Y_ros_train,Y_ros_test=train_test_split(x_ros,y_ros,test_size=0.33, random_state=42)\n    #rebuild model\n    model.fit(X_ros_train,Y_ros_train)\n    print(\"Balanced Model Evaluation: \")\n    predAcc(X_ros_train, X_ros_test, Y_ros_train, Y_ros_test, model)\n    print(\"Balanced Model's feature Selections: \")\n    feaSelect(X_ros_train,Y_ros_train,model,x_ros)\n    print('Resample training dataset shape', Y_ros_train.shape[0])\n    print('Resample testing dataset shape', Y_ros_test.shape[0])","fba1d544":"def balancedFilt(xf_ros,yf_ros):\n    #re-split the balanced filtered dataset\n    Xf_ros_train,Xf_ros_test,Yf_ros_train,Yf_ros_test=train_test_split(xf_ros,yf_ros,test_size=0.33, random_state=42)\n    #Rebuild model\n    model.fit(Xf_ros_train,Yf_ros_train)\n    print(\"Balanced Filtered Model Evaluation: \")\n    predAcc(Xf_ros_train, Xf_ros_test, Yf_ros_train, Yf_ros_test, model)\n    print(\"Balanced Filtered Model's Feature Importance Ranking: \")\n    feaSelect(Xf_ros_train,Yf_ros_train,model,xf_ros)","06e0e6a9":"from sklearn.model_selection import train_test_split\nX=db3[['socialNbFollowers','socialNbFollows','socialProductsLiked','productsListed','productsSold','productsPassRate',\n          'productsWished','civilityGenderId','daysSinceLastLogin','language_encoded','countryCode_encoded','hasAnyApp_encoded',\n          'hasAndroidApp_encoded','hasIosApp_encoded','hasProfilePicture_encoded']]\n#Y=db3['HasBought'] = db3['productsBought'].apply(lambda x: '1' if x >0 else '0')\nY=db3['HasBought'] = db3['productsBought'].apply(lambda x: '2' if x >=3 else('1' if x <3 and x>0 else '0'))","886bc03a":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.33, random_state=42)\nmodels(X_train,Y_train,X_test,Y_test)","f2048623":"#define logistic regression function\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nmodel= LogisticRegression(multi_class='multinomial',solver='newton-cg') #other solvers not converge","c32bf497":"org(X,Y)","8ddc1691":"#according to the correlation heatmap, pairplot and feature selection, keep top 7\n \nd_columns=['productsSold', 'hasAnyApp_encoded', 'productsPassRate', 'hasProfilePicture_encoded', \n           'productsListed', 'socialNbFollows', 'socialProductsLiked', 'socialNbFollowers']\n\n#socialNBFollows & socialProductsLiked has no relationship with a user's willingness to buy a product\nXf = X.drop(d_columns, axis = 1)\nYf = db3['HasBought']\nXy=db3[['productsBought']]\nXf_pp = pd.concat([Xf,Xy], join = 'outer', axis = 1) ","018f0f4f":"import seaborn as sns\nsns.pairplot(Xf_pp, x_vars=Xf.columns, y_vars='productsBought', height=7, aspect=0.7, kind='reg')","951af68c":"filt(Xf,Yf)","a749ef82":"c=Xf\nc.dataframeName = \"Formean\"\nc.describe()","548190db":"#try to predict when a user using mean\nnewdata=[[2,2,581,2,94,0,0]]\nY_pred1 = model.predict(newdata)\nprint(\"prediction with mean features: \",Y_pred1)\n#socialNbFollowers affect the prediction the most\n\n#try to predict when a user using max\nnewdata1=[[2635,3,709,4,198,1,1]]\nY_pred2=model.predict(newdata1)\nprint(\"prediction with max features: \", Y_pred2)","9720f61c":"print(db3['HasBought'].value_counts())\n#very imbalanced sample","ba7bd417":"from collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\nros = RandomOverSampler(random_state=42)\n\n # fit predictor and target variable\nX_ros, Y_ros = ros.fit_resample(X, Y)\nprint('Original dataset shape', Counter(Y))\nprint('Resample dataset shape', Counter(Y_ros))\n\nX_ros=pd.DataFrame(X_ros,columns=['socialNbFollowers', 'socialNbFollows', 'socialProductsLiked',\n       'productsListed', 'productsSold', 'productsPassRate', 'productsWished',\n       'civilityGenderId', 'daysSinceLastLogin', 'language_encoded',\n       'countryCode_encoded', 'hasAnyApp_encoded', 'hasAndroidApp_encoded',\n       'hasIosApp_encoded', 'hasProfilePicture_encoded'])\nY_ros.to_frame() \n#Y_ros=pd.DataFrame(Y_ros,columns=['HasBought'])","a1370051":"balanced(X_ros,Y_ros)","435d7839":"#according to the correlation heatmap, pairplot and feature selection, keep top 7\nd_columns=[ 'productsPassRate', 'hasAndroidApp_encoded', 'daysSinceLastLogin', 'hasProfilePicture_encoded', \n           'productsListed', 'socialProductsLiked', 'socialNbFollows', 'socialNbFollowers']\n#socialNBFollows & socialProductsLiked has no relationship with a user's willingness to buy a product\nXf_ros = X_ros.drop(d_columns, axis = 1)\nYf_ros = Y_ros","f2b08a87":"balancedFilt(Xf_ros,Yf_ros)","340db015":"c=Xf_ros\nc.dataframeName = \"Formean\"\nc.describe()","3a225ff6":"#try to predict when a user using mean\nnewdata=[[1,22,2,2,90,0,0]]\nY_pred1 = model.predict(newdata)\nprint(\"prediction with mean features: \",Y_pred1)\n#socialNbFollowers affect the prediction the most\n\n#try to predict when a user using max\nnewdata1=[[174,2635,3,4,198,1,1]]\nY_pred2=model.predict(newdata1)\nprint(\"prediction with max features: \", Y_pred2)","541e91cc":"import xgboost as xgb\nmodel= xgb.XGBClassifier()","8da4ca88":"org(X,Y)","b6caefb0":"#according to the correlation heatmap, pairplot and feature selection, keep top 7\n\nd_columns=[ 'productsSold', 'socialNbFollowers', 'language_encoded', 'hasAndroidApp_encoded', 'productsListed', \n           'hasAnyApp_encoded', 'daysSinceLastLogin', 'hasProfilePicture_encoded']\n\n#socialNBFollows & socialProductsLiked has no relationship with a user's willingness to buy a product\nXf = X.drop(d_columns, axis = 1)\nYf = db3['HasBought']\nXy=db3[['productsBought']]\nXf_pp = pd.concat([Xf,Xy], join = 'outer', axis = 1) ","7a6993da":"import seaborn as sns\nsns.pairplot(Xf_pp, x_vars=Xf.columns, y_vars='productsBought', height=7, aspect=0.7, kind='reg')","29bcb71e":"filt(Xf,Yf)","a4bbff65":"c=Xf\nc.dataframeName = \"Formean\"\nc.describe()","f0492cfd":"\n\n#try to predict when a user using mean and max\nnUser = {'socialNbFollows': [8,13764],\n        'socialProductsLiked': [4,51671],\n        'productsPassRate':[1,100],\n        'productsWished':[2,2635],\n        'civilityGenderId':[2,3],\n        'countryCode_encoded':[94,198],\n        'hasIosApp_encoded' :[0,1]  \n        }\n\ndf2 = pd.DataFrame(nUser, columns = ['socialNbFollows','socialProductsLiked','productsPassRate','productsWished','civilityGenderId',\n                'countryCode_encoded','hasIosApp_encoded'])\n\nY_pred2 = model.predict(df2)\nY_pred2\n","1519b635":"print(db3['HasBought'].value_counts())\n#very imbalanced sample","ee3b372a":"balanced(X_ros,Y_ros)","328df806":"#according to the correlation heatmap, pairplot and feature selection, keep top 7\nd_columns=[ 'productsPassRate', 'socialNbFollowers', 'countryCode_encoded', 'daysSinceLastLogin', \n           'productsListed', 'hasAnyApp_encoded', 'productsWished', 'language_encoded']\nXf_ros = X_ros.drop(d_columns, axis = 1)\nYf_ros = Y_ros","1fa95255":"balancedFilt(Xf_ros,Yf_ros)","169be192":"c=Xf_ros\nc.dataframeName = \"Formean\"\nc.describe()","5988fbdd":"#try to predict when a user using mean and max\nnUser = {'socialNbFollows': [10,13764],\n        'socialProductsLiked': [47,51671],\n        'productsSold':[1,174],\n        'civilityGenderId':[2,3],\n        'hasAndroidApp_encoded':[0,1],\n        'hasIosApp_encoded':[0,1],\n        'hasProfilePicture_encoded' :[1,1]  \n        }\n\ndf2 = pd.DataFrame(nUser, columns = ['socialNbFollows','socialProductsLiked','productsSold','civilityGenderId',\n                                     'hasAndroidApp_encoded','hasIosApp_encoded', 'hasProfilePicture_encoded'])\n\nY_pred2 = model.predict(df2)\nY_pred2\n\n","9ac5b25c":"from sklearn.model_selection import train_test_split\nX=db3[['socialNbFollowers','socialNbFollows','socialProductsLiked','productsListed','productsBought','productsPassRate',\n          'productsWished','civilityGenderId','daysSinceLastLogin','language_encoded','countryCode_encoded','hasAnyApp_encoded',\n          'hasAndroidApp_encoded','hasIosApp_encoded','hasProfilePicture_encoded']]\nY=db3['HasSold'] = db3['productsSold'].apply(lambda x: '2' if x >=6 else('1' if x <6 and x>0 else '0'))","09e5cb62":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.33, random_state=42)\nmodels(X_train,Y_train,X_test,Y_test)","e1ddc9be":"import xgboost as xgb\nmodel= xgb.XGBClassifier()","584dbb41":"org(X,Y)","17b8e882":"#according to the correlation heatmap, pairplot and feature selection, keep top 7\nd_columns=[ 'productsBought', 'productsListed', 'productsPassRate', 'civilityGenderId', 'language_encoded',\n           'hasAndroidApp_encoded', 'hasProfilePicture_encoded', 'productsWished']\n\n#socialNBFollows & socialProductsLiked has no relationship with a user's willingness to buy a product\nXf = X.drop(d_columns, axis = 1)\nYf = db3['HasSold']\nXy=db3[['productsSold']]\nXf_pp = pd.concat([Xf,Xy], join = 'outer', axis = 1)","cd1ec51d":"import seaborn as sns\nsns.pairplot(Xf_pp, x_vars=Xf.columns, y_vars='productsSold', height=7, aspect=0.7, kind='reg')","f5657c95":"filt(Xf,Yf)","195b6d05":"c=Xf\nc.dataframeName = \"Formean\"\nc.describe()","51d1d99a":"#try to predict when a user using mean and max\nnUser = {'socialNbFollowers': [3,744],\n        'socialNbFollows': [8,13764],\n        'socialProductsLiked':[4,51671],\n        'daysSinceLastLogin':[581,709],\n        'countryCode_encoded':[94,198],\n        'hasAnyApp_encoded':[0,1],\n        'hasIosApp_encoded' :[0,1]  \n        }\n\ndf2 = pd.DataFrame(nUser, columns = ['socialNbFollowers','socialNbFollows','socialProductsLiked','daysSinceLastLogin',\n                                     'countryCode_encoded','hasAnyApp_encoded','hasIosApp_encoded'])\n\nY_pred2 = model.predict(df2)\nY_pred2\n","674e7a7d":"from collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\nros = RandomOverSampler(random_state=42)\n\n # fit predictor and target variable\nX_ros, Y_ros = ros.fit_resample(X, Y)\nprint('Original dataset shape', Counter(Y))\nprint('Resample dataset shape', Counter(Y_ros))\n\nX_ros=pd.DataFrame(X_ros,columns=['socialNbFollowers', 'socialNbFollows', 'socialProductsLiked',\n       'productsListed', 'productsBought', 'productsPassRate', 'productsWished',\n       'civilityGenderId', 'daysSinceLastLogin', 'language_encoded',\n       'countryCode_encoded', 'hasAnyApp_encoded', 'hasAndroidApp_encoded',\n       'hasIosApp_encoded', 'hasProfilePicture_encoded'])\nY_ros.to_frame()\n#Y_ros=pd.DataFrame(Y_ros,columns=['HasSold'])","01a9e4af":"balanced(X_ros,Y_ros)","48e8b373":"#according to the correlation heatmap, pairplot and feature selection, keep top 7\n\nd_columns=[ 'productsWished', 'productsListed', 'productsPassRate', 'daysSinceLastLogin',\n           'hasIosApp_encoded', 'hasProfilePicture_encoded', 'hasAndroidApp_encoded', 'productsBought']\n\nXf_ros = X_ros.drop(d_columns, axis = 1)\nYf_ros = Y_ros","346cdf0d":"balancedFilt(Xf_ros,Yf_ros)","50fa6dbe":"c=Xf_ros\nc.dataframeName = \"Formean\"\nc.describe()","bbd1ebff":"#try to predict when a user using mean and max\nnUser = {'socialNbFollowers': [13,744],\n        'socialNbFollows': [31,13764],\n        'socialProductsLiked':[78,51671],\n        'civilityGenderId':[2,3],\n        'language_encoded':[2,4],\n        'countryCode_encoded':[86,198],\n        'hasAnyApp_encoded' :[1,1]  \n        }\n\ndf2 = pd.DataFrame(nUser, columns = ['socialNbFollowers','socialNbFollows','socialProductsLiked','civilityGenderId',\n                                     'language_encoded','countryCode_encoded','hasAnyApp_encoded'])\n\nY_pred2 = model.predict(df2)\nY_pred2\n","1075a78b":"For filtered dataset(both imbalanced and balanced):\nThey both had variables socialNbFollwers, countryCode_encoded, socialNbFollows, socialProductsLiked, hasAnyApp_encoded\nImbalanced dataset used variables daysSinceLastLogin, hasIosApp_encoded\nBalanced dataset used language_encoded, civilityGenderId\n\nThey chosed almost the same varibles for model building\nBut balanced dataset prediction only has slightly lower f1socre on HasBought=1","bba3a724":"New User's Prediction","76911d57":"f1-score on HasSold=1 dropped significantly on filtered dataset","dcc85075":"Model Built","caa4365a":"Re-split the filtered dataset","d5e332f1":"Re-split the balanced dataset","7f7b82ed":"# Improve two models with the best performance","af5d003f":"New User's Prediction","b15fcab6":"F1-score of both train and test set is extremely low","fe22d042":"## Bootstrap to resample the imbalanced data","2715e46c":"overall accuracy and f1-score on HasSold=1 decreased on filtered dataset for both balanced and imbalanced cases.\nEspecially the f1-score on HasSold=1 decreased significantly on imbalanced cases.","1d6dcf2a":"# HasSold As Predictor","1c6e99c3":"Model Re-evaluation with the Filtered Dataset","7d4024b3":"Define X and Y ","486d5d1e":"Re-split the filtered dataset","ce49c512":"Balanced Model Re-evaluation with Filtered Dataset","a01c189a":"Balanced Model Evaluation","5b0a6221":"Applied two classification models(Naive Bayes, Support Vector Machines), two regression models(Logistic Regression, Decision Tree), and two advanced supervised models(RandomForestClassifier, Xtreme gradient boosting) for model performance.","c2825236":"They both used countryCode_encoded, language_encoded, hasIosApp_encoded, productsWished, civilityGenderId\nImbalanced used daysSinceLast Login, hasAndroidApp_encoded\nBalanced used productsSold\n\nBalanced dataset prediction has significantly higher f1socre on HasBought=1   ","f3520f44":"New users' prediction","94943083":"Model Built","1c8f3faa":"# Improve model with the best performance\nsince all have almost the same accuracy on the test set","9e8af25d":"## 2. Encode variables","860657bf":"Model Re-evaluation with the Filtered Dataset","a5a7f422":"New users' prediction","840756a6":"# Logistic Regression","7e0b041b":"Model Evaluation","38cdf38f":"Balanced Model Re-evaluation with the Filtered Dataset","c531df60":"Visualize the relationship between individual variable and dependent variable","4b4ca36b":"We could clearly observe more balanced f1-score that the f1-score of case 'HasBought'=1 increased significantly although 0's dropped slightly.","12d3349e":"Model Re-evaluation with the Filtered Dataset","a105292f":"F1-score of both train and test set increased significantly comparing to LR's","a5b036fe":"Visualize the relationship between individual variable and dependent variable","a065b80e":"# HasBought As Predictor","32145ce4":"Define X and Y","55d1e3a3":"No strong linear relationship, only week linear relationship on productsWished and productsBought","4df65ae2":"Model Evaluation","815a8a0f":"Visualize the relationship between individual variable and dependent variable","03f80610":"Updated New users\u2019 prediction","36fe9eee":"# Predictor Analysis","2ed6c6f7":"Balanced Model Evaluation","9459f6de":"Balanced Model Evaluation","6d10862b":"Applied two classification models(Naive Bayes, Support Vector Machines), two regression models(Logistic Regression, Decision Tree), and two advanced supervised models(RandomForestClassifier, Xtreme gradient boosting) for model performance.","0e2bd34e":"# IMPORT DATA","b394f4b9":"Re-split the filtered dataset","bf7da8de":"# Extreme Gradient Boosting","30928f6b":"Model Evaluation","24971dd0":" In logistic regression, if the intercept is below 1 implies a reduction in the probability that the event happens. To sum up:\nb) logit negative value = logistic < 1 = decrease in the probability of the event when you have a positive change in the independent variables","3682805a":"Balanced Model Re-evaluation with the Filtered Dataset","1ce1cd0c":"## 1. Delete unused variables ","e4398b38":"Model Built","a84d4991":"For filtered dataset(both imbalanced and balanced):\nThe both used productsWished, civilityGenderId, countryCode_encoded, hasIosApp_encoded\nImbalanced used socialProductsLiked, socialNbFollows, productsPassRate\nBalanced used productsSold, language_encoded, hasAnyApp_encoded\n\nBalanced dataset prediction has significantly higher f1socre on HasBought=1","fa95fe29":"## Bootstrap to resample the imbalanced data","67dadb24":"## Bootstrap to resample the imbalanced data","c9d5d88a":"New users' prediction"}}