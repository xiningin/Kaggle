{"cell_type":{"77ee8677":"code","8bd960e4":"code","29ec429f":"code","bc25b3eb":"code","7f4a6752":"code","e9b8ef3b":"code","216eefa3":"code","356546fa":"code","55d9d595":"code","c1014f58":"code","73306b98":"code","322f7269":"code","af478adb":"code","a6936e94":"code","e019868e":"code","e7e01ef4":"code","3f452414":"code","33faa58f":"code","e73d41b1":"code","079dfdb2":"code","6490808b":"code","6039f3b2":"code","f2661c8c":"code","a7f2ebbc":"code","a5faaea0":"code","bd4e87ba":"code","96d76c56":"code","93be04ea":"code","1b6b78eb":"code","9abc5151":"markdown","af9b20a8":"markdown","dc4119b8":"markdown","85c96b43":"markdown","64cc7250":"markdown","b03cea49":"markdown","78aaf58a":"markdown","b27f6e64":"markdown","9636dd12":"markdown","5d16aed9":"markdown","306b0a19":"markdown","2d0206ab":"markdown","3bd3d5fc":"markdown","eeafb1c5":"markdown","7cfe0c98":"markdown","3987148a":"markdown","c0ca41f0":"markdown","cf5967ed":"markdown","10354126":"markdown","263cfd6c":"markdown","47a1c852":"markdown"},"source":{"77ee8677":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8bd960e4":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nprint(data.shape)\ndata.head()","29ec429f":"data.dtypes","bc25b3eb":"# Class count\nprint(data['Class'].value_counts())\nsns.countplot(data=data, x='Class')","7f4a6752":"# Distribution of Amount\nplt.figure(figsize=(20,6))\nax1 = plt.subplot(1,3,1)\nax2 = plt.subplot(1,3,2)\nax3 = plt.subplot(1,3,3)\nsns.distplot(data['Amount'], ax=ax1)\nsns.distplot((data[data['Class']==0])['Amount'], ax=ax2)\nsns.distplot((data[data['Class']==1])['Amount'], ax=ax3)","e9b8ef3b":"# Investigating time\nprint(data['Time'].value_counts())\nplt.figure(figsize=(20,6))\nax1 = plt.subplot(1,3,1)\nax2 = plt.subplot(1,3,2)\nax3 = plt.subplot(1,3,3)\nsns.distplot(data['Time'],ax=ax1)\nsns.distplot(data[data['Class']==1]['Time'],ax=ax2)\nsns.distplot(data[data['Class']==0]['Time'],ax=ax3)","216eefa3":"# Other variables are normalized so let's plot correlation matrix\nplt.figure(figsize=(10,10))\nsns.heatmap(data.corr())","356546fa":"def choseRandom(d, c):\n    \"\"\"d is the original dataframe and c is the column name denoting variable to be predicted(class)\"\"\"\n    values = d[c].unique()\n    lst =[]\n    for i in values:\n        lst.append(d[d[c]==i][c].count())\n    min_count = min(lst)\n    df = pd.DataFrame()\n    for i in values:\n        df = df.append(d[d[c]==i].sample(n=min_count))\n        \n    return df","55d9d595":"sampled_random = choseRandom(data,'Class')\nprint(sampled_random.shape)\nsampled_random.head()","c1014f58":"plt.figure(figsize=(6,4))\nsns.set(style='darkgrid')\nsns.countplot(sampled_random['Class'])\nplt.xlabel(\"Class Distribution\")\nplt.title('Class Distribution of resampled data')\nplt.show()","73306b98":"# Plotting distributions of 'Amount'\nplt.figure(figsize=(20,6))\nax1 = plt.subplot(1,3,1)\nax2 = plt.subplot(1,3,2)\nax3 = plt.subplot(1,3,3)\nsns.set(style='darkgrid')\nsns.distplot(sampled_random['Amount'], ax=ax1)\nsns.distplot((sampled_random[sampled_random['Class']==0])['Amount'], ax=ax2)\nsns.distplot((sampled_random[sampled_random['Class']==1])['Amount'], ax=ax3)","322f7269":"# Heatmap on new sampled data\nplt.figure(figsize=(8,6))\nsns.heatmap(sampled_random.corr())\nplt.show()","af478adb":"from sklearn.model_selection import train_test_split\ndef splitData(d, test_size=0.2):\n    train, test = train_test_split(d, stratify = d['Class'], test_size=test_size)\n    return train,test\n\ntrain, test = splitData(data)","a6936e94":"# Let us plot some box plot for Class-vs-Amount\nsns.set(style='darkgrid')\nsns.boxplot(x = 'Class', y = 'Amount', data = sampled_random, hue='Class')","e019868e":"# boxplots for variables V1-V28\nplt.figure(figsize=(20,10))\nax1 = plt.subplot(2,4,1)\nax2 = plt.subplot(2,4,2)\nax3 = plt.subplot(2,4,3)\nax4 = plt.subplot(2,4,4)\nax5 = plt.subplot(2,4,5)\nax6 = plt.subplot(2,4,6)\nax7 = plt.subplot(2,4,7)\nax8 = plt.subplot(2,4,8)\nsns.set(style='darkgrid')\nsns.boxplot(x = 'Class', y = 'V1', data = sampled_random, hue='Class', ax=ax1)\nsns.boxplot(x = 'Class', y = 'V2', data = sampled_random, hue='Class', ax=ax2)\nsns.boxplot(x = 'Class', y = 'V28', data = sampled_random, hue='Class', ax=ax3)\nsns.boxplot(x = 'Class', y = 'V26', data = sampled_random, hue='Class', ax=ax4)\nsns.boxplot(x = 'Class', y = 'V20', data = sampled_random, hue='Class', ax=ax5)\nsns.boxplot(x = 'Class', y = 'V18', data = sampled_random, hue='Class', ax=ax6)\nsns.boxplot(x = 'Class', y = 'V12', data = sampled_random, hue='Class', ax=ax7)\nsns.boxplot(x = 'Class', y = 'V8', data = sampled_random, hue='Class', ax=ax8)","e7e01ef4":"# Cleaning up the outliers \ndef cleanOutliers(d, q1, q3, r):\n    \"\"\"d is the data frame, q1 and q3 are quantiles and r is the range above of below quantiles, \n    Generally, q1=0.25, q3=0.75, r=1.5\"\"\"\n    cols = d.columns\n    for i in cols:\n        Q1 = d[i].quantile(q1)\n        Q3 = d[i].quantile(q3)\n        IQR = Q3-Q1\n        low = Q1 - r*IQR\n        high = Q3 + r*IQR\n        d = d[(d[i]>=low) & (d[i]<=high)]\n    return d","3f452414":"temp = cleanOutliers(train, 0.25, 0.75, 1.5)\nprint(\"Class count for 0.25-0.75 and 1.5 IQR fields\",temp['Class'].value_counts())\ntemp = cleanOutliers(train, 0.2, 0.8, 2.0)\nprint(\"Class count for 0.2-0.8 and 2.0 IQR fields\",temp['Class'].value_counts())\ntemp = cleanOutliers(train, 0.1, 0.9, 2.5)\nprint(\"Class count for 0.1-0.9 and 2.5 IQR fields\",temp['Class'].value_counts())\ntemp = cleanOutliers(train, 0, 1, 2.5)\nprint(\"Class count including all samples\",temp['Class'].value_counts())","33faa58f":"def logTransform(d,col):\n    d[col] = np.log1p(d[col])\n    return d\ndef invLog(d, col):\n    d[col] = np.expm1(d[col])\n    return d[col]","e73d41b1":"train = logTransform(train, 'Amount')\ntest = logTransform(test, 'Amount')\nplt.figure(figsize=(20,8))\nax1 = plt.subplot(1,2,1)\nax2 = plt.subplot(1,2,2)\nsns.distplot(train['Amount'], ax=ax1)\nsns.distplot(test['Amount'],ax=ax2)\nplt.show()","079dfdb2":"from sklearn.preprocessing import StandardScaler\ndef getScaleDataParams(d, col):\n    trans = StandardScaler().fit(d[[col]])\n    return trans\ndef scaleData(d, col, param):\n    d[[col]] = param.transform(d[[col]])\n    return d\ndef scaleDF(d):\n    \"\"\"Pass a dataFrame to scale all columns, provided that all the columns are numeric\"\"\"\n    cols = d.columns\n    for i in cols:\n        params = getScaleDataParams(d,i)\n        d = scaleData(d, i, params)\n    return d","6490808b":"train.iloc[:,:-1] = scaleDF(train.iloc[:,:-1])\ntest.iloc[:,:-1] = scaleDF(test.iloc[:,:-1])\ntrain.head()","6039f3b2":"# Undersampling on training data\nrandom_train = choseRandom(train,'Class')\nprint(random_train.shape)\nrandom_train.head()","f2661c8c":"# PCA\nfrom sklearn.decomposition import PCA\ndef getFullPCAData(d):\n    pca = PCA()\n    pca_data = pca.fit_transform(d)\n    return pca_data, pca.explained_variance_ratio_*100\n\ndef getPCAData(d, n_components=2):\n    pca = PCA(n_components=n_components)\n    pca_data = pca.fit_transform(d)\n    return pca_data, pca.explained_variance_ratio_*100","a7f2ebbc":"def plotPCAVariations(pca_data,var):\n    per_var = np.round(var, decimals=1)\n    labels = [\"PC\"+str(x) for x in range(1,len(per_var)+1)]\n    plt.figure(figsize=(20,6))\n    plt.bar(x = range(1,len(per_var)+1), height = per_var, tick_label = labels)\n    plt.show()","a5faaea0":"pca_data, var = getFullPCAData(random_train.iloc[:,:-1])\nplotPCAVariations(pca_data,var)","bd4e87ba":"def plotPCAData(d, mask):\n    \"\"\"Plots PCA Data considering n_components = 2\"\"\"\n    pca_2c, var = getPCAData(d,2)\n    pca_df = pd.DataFrame(pca_2c, columns=['PC1','PC2'])\n    pca_df['Class'] = mask.values\n    plt.figure(figsize=(8,6))\n    sns.scatterplot(x='PC1', y='PC2',hue='Class',data = pca_df)\n    plt.show()\n    return pca_df\n    \nmask = random_train.iloc[:,-1]\npca_df = plotPCAData(random_train.iloc[:,:-1], mask)","96d76c56":"from sklearn.manifold import TSNE\ndef getTSNE(d):\n    tsne = TSNE()\n    d_emb = tsne.fit_transform(d)\n    return d_emb","93be04ea":"d_emb = getTSNE(random_train.iloc[:,:-1])","1b6b78eb":"def plotTSNEData(d, mask):\n    \"\"\"Plots PCA Data considering n_components = 2\"\"\"\n    d_emb = getTSNE(d)\n    tsne_df = pd.DataFrame(d_emb, columns=['C1','C2'])\n    tsne_df['Class'] = mask.values\n    plt.figure(figsize=(8,6))\n    sns.scatterplot(x='C1', y='C2',hue='Class',data = tsne_df)\n    plt.show()\n    return tsne_df\n    \nmask = random_train.iloc[:,-1]\ntsne_df = plotTSNEData(random_train.iloc[:,:-1], mask)","9abc5151":"### Undersampling","af9b20a8":"**Inferences from t-SNE:**\n* t-SNE provided much better results than PCA\n* t-SNE shows that data is rather well classified even for 2 components.\n* Classifiers can work well when applied properly.","dc4119b8":"<a id=\"splitting\"><\/a>\n### Splitting into Train and Test data","85c96b43":"<a id=\"tsne\"><\/a>\n### t-SNE","64cc7250":"# Credit Card Fraud Detection\n##### Credit Card Fraud Detection is a classic example of class imabalace. Such datasets have a relatively low number of instances of main class of interest and therefore pose a challenge for data analysis and modelling. \n##### If not dealt properly, results on such datasets can be misleading often prone to overfitting and wrong correlations. There are certain techniques and trick to be used while dealing with class imbalanced data. \n##### In this notebook, we shall analyze the dataset and train machine learning models so that we can get accurate results on such a dataset.\n\n## Table of Content\n* [Data Exploration](#exploration)\n* [Class Imbalanced Data, Oversampling and Undersampling](#cib)\n* [Splitting](#splitting)\n* [Outlier Removal](#outlier)\n* [Normalization and Tranformation](#nt)\n* [PCA and t-SNE](#pcts)\n    * [PCA](#pca)\n    * [t-SNE](#tsne)\n\n\n<br><br>\n**Please hit upvote if you find the notebook useful.**\n<br><br>\n**Note:- I have completed the analysis part, modelling part will be implemented soon.**\n","b03cea49":"**Inferences from so far analysis**\n* Class distribution is highly imbalanced.\n* 0 - Non Fraud, 1 - Fraud (only 492 instances)\n* Amount distributions are highly skewed and most transaction amounts are low.\n* Time does not convey much but 100000 is particularly low\n* Correlation heatmap on full dataset conveys less but still v2-Amount are negatively correlated while v7-Amount and v20-Amount show some positive correlation.","78aaf58a":"To analyse the data, I shall take out random tuples equal to number of minority tuples so that class distribution becomes balanced.","b27f6e64":"<a id=\"cib\"><\/a>\n### Class Imbalanced Data, Oversampling and Undersampling\n* When the distribution of classes is such that a class or classes is found in smaller number as compared to other class(es), data is said to be class imbalanced data. The rare class is often the class of importance.\n* Class imbalanced data therefore needs to be dealt carefully taking careful measures to handle imbalance.\n* Accuracy as a measure of performance can not be used to access the classifier in case of class imbalanced data.\n\n#### Oversampling\nTo deal with class imbalance, the tuples of minority classes can be duplicated or synthetically generated tuples can be added so that the distribution of classes becomes similar. This is called oversampling.<br>\nThere are a number of ways to perform oversampling, e.g. random duplication of minority tuples, SMOTE, etc.\n\n#### Undersampling\nUndersampling refers to deletion of minority tuples to obtain the balanced class distribution. There are a number of ways to perform undersampling, e.g. Random undersampling, NearMiss, etc.\n<br><br>\nOversampling and undersampling can also be combined.","9636dd12":"**Inferences**\n* Amount has to be normalized\n* There are certian correlations among attributes which were not apparant.\n<br><br>\n**To Do:** \n* Outlier Detection\n* Normalize\n* PCA, t-SNE\n\nBefore further proceeding let's split the test and train sets. This is because futher steps include outlier detection and other steps which include changing the data. We do not want to change the test data.","5d16aed9":"<a id=\"pca\"><\/a>\n### PCA","306b0a19":"**Inference:**\nThe above analysis shows that frauds transactions are actually present as outliers which is justified.<br>\nSo we can not remove outliers from our data.\n<br>\nConsidering all features to remove outliers is not a good decision as analysis shows all the positive class (Class 1) tuples are removed.\n<br>\nAlso, since fraud transactions may actually be outliers, we will **not remove outliers** for now.","2d0206ab":"First thing to look upon is **class distribution** because Fraud Data is mostly imbalanced which supports the fact that Fraud Transaction are rare. Next, Amount should be investigated then we shall look upon Time.","3bd3d5fc":"##### Inferences from PCA -\n* Most of the information is contained in first Principal Component\n* Taking PC1-PC5 prevent much loss of data, however, some information is still lost.\n* Taking 2 PCs and plotting a scatter plot, we see that data is mixed, there is not a clear boundary. Thus linear algorithms on 2 PCs won't work well.","eeafb1c5":"On randomly assessing each of the variables from V1-V28 and Amount, it would be obvious to remove outliers outside $1.5*IQR$ and $-1.5*IQR$. \n<br>\n$IQR$ stands for Interquartile Range. To know more refer to [here](https:\/\/www.kaggle.com\/mohtashimnawaz\/types-of-data-statistics-and-proximity-measures).","7cfe0c98":"<a id=\"nt\"><\/a>\n### Normalizations and Transformations\nAs the 'Amount' is highly skewed, we will apply log transform on it to get the normal distribution.\n<br><br>\nSo we would have to normalize the 'Amount' attribute. For now, I shall use MinMaxScaler which transform data to a given range. Although, Robust Scaler is robust to outliers, it produces negative quantities too, which for Amount of transactions is not desirable as it can not be negative. ","3987148a":"## Models Using Random Undersampling\nNext, we shall try models. There are two ways to go, either undersampling or oversampling. I shall try both starting with random undersampling.<br><br>\n**Note:-** While using CrossValidation it is important to note that data should be sampled inside cross-validation otherwise it will cause data leak.\n<br><br>\nI will implement the modelling part soon.","c0ca41f0":"**Inferences after first look on data**<br>\n* **V1-V28** - Unknown normalized attributes. (Numeric, Continuous)\n* **Time** - Does not represent actual date. (Ordinal)\n* **Amount** - Amount of transaction performed. (Numeric, Continuous)\n* **Class** - Represents Fraud\/Non-Fraud. (Categorical\/Nominal)","cf5967ed":"<a id=\"exploration\"><\/a>\n### Data Exploration","10354126":"<a id=\"outlier\"><\/a>\n### Outlier Removal","263cfd6c":"<a id=\"pcts\"><\/a>\n### PCA and t-SNE\n* **PCA** stands for Principal Component Analysis and is widely used for dimensionality reduction and to visualize high dimensional data by reducing it into lower dimensions.\n* **t-SNE** is a non-linear probabilistic dimensionality reduction algorithm. It cluters the similar points together and dissimilar points farther in lowe dimensional space. \n* t-SNE is more computationally expensive than PCA.","47a1c852":"We have got a dataframe with equal class distribution now. For analyses we can now use this DataFrame.\n<br>\nLet's begin by class distribution itself."}}