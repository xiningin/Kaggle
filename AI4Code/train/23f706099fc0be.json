{"cell_type":{"4dc3f0df":"code","685ad100":"code","45143d30":"code","4828e33d":"code","57f18f41":"code","5a8573c3":"code","2add1456":"code","67ecf1ec":"code","ed91da24":"code","e96e6182":"code","ec787ab1":"code","29e37ea9":"code","3bd89f25":"code","edc05060":"markdown","966bb266":"markdown","7c46ace2":"markdown","e59fd3f9":"markdown"},"source":{"4dc3f0df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","685ad100":"from collections import OrderedDict\nimport spacy\nfrom spacy import displacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load('en_core_web_sm')\n\n","45143d30":"def custom_retokenizer(doc):\n    matcher = Matcher(nlp.vocab)\n    patterns = [[{\"LOWER\": \"el\"}, {\"LOWER\": \"paso\"}]]\n    matcher.add(\"TO_MERGE\", None, *patterns)\n    matches = matcher(doc)\n    with doc.retokenize() as retokenizer:\n        for match_id, start, end in matches:\n            span = doc[start:end]\n            retokenizer.merge(span)\n    return doc","4828e33d":"nlp.add_pipe(custom_retokenizer, before=\"tagger\")","57f18f41":"class TextRankForKeyword():\n    \"\"\"Extract keywords from text\"\"\"\n    \n    def __init__(self):\n        self.d = 0.85 # damping coefficient, usually is .85\n        self.min_diff = 1e-5 # convergence threshold\n        self.steps = 10 # iteration steps\n        self.node_weight = None # save keywords and its weight\n\n    \n    def set_stopwords(self, stopwords):  \n        \"\"\"Set stop words\"\"\"\n        for word in STOP_WORDS.union(set(stopwords)):\n            lexeme = nlp.vocab[word]\n            lexeme.is_stop = True\n    \n    def sentence_segment(self, doc, candidate_pos, lower):\n        \"\"\"Store those words only in cadidate_pos\"\"\"\n        sentences = []\n        for sent in doc.sents:\n            selected_words = []\n            for token in sent:\n                # Store words only with cadidate POS tag\n                if token.pos_ in candidate_pos and token.is_stop is False:\n                    if lower is True:\n                        selected_words.append(token.text.lower())\n                    else:\n                        selected_words.append(token.text)\n            sentences.append(selected_words)\n        return sentences\n        \n    def get_vocab(self, sentences):\n        \"\"\"Get all tokens\"\"\"\n        vocab = OrderedDict()\n        i = 0\n        for sentence in sentences:\n            for word in sentence:\n                if word not in vocab:\n                    vocab[word] = i\n                    i += 1\n        return vocab\n    \n    def get_token_pairs(self, window_size, sentences):\n        \"\"\"Build token_pairs from windows in sentences\"\"\"\n        token_pairs = list()\n        for sentence in sentences:\n            for i, word in enumerate(sentence):\n                for j in range(i+1, i+window_size):\n                    if j >= len(sentence):\n                        break\n                    pair = (word, sentence[j])\n                    if pair not in token_pairs:\n                        token_pairs.append(pair)\n        return token_pairs\n        \n    def symmetrize(self, a):\n        return a + a.T - np.diag(a.diagonal())\n    \n    def get_matrix(self, vocab, token_pairs):\n        \"\"\"Get normalized matrix\"\"\"\n        # Build matrix\n        vocab_size = len(vocab)\n        g = np.zeros((vocab_size, vocab_size), dtype='float')\n        for word1, word2 in token_pairs:\n            i, j = vocab[word1], vocab[word2]\n            g[i][j] = 1\n            \n        # Get Symmeric matrix\n        g = self.symmetrize(g)\n        \n        # Normalize matrix by column\n        norm = np.sum(g, axis=0)\n        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n        \n        return g_norm\n\n    \n    def get_keywords(self, number=10):\n        \"\"\"Print top number keywords\"\"\"\n        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n        for i, (key, value) in enumerate(node_weight.items()):\n            print(key + ' - ' + str(value))\n            if i > number:\n                break\n        \n        \n    def analyze(self, text, \n                candidate_pos=['NOUN', 'PROPN'], \n                window_size=4, lower=False, stopwords=list()):\n        \"\"\"Main function to analyze text\"\"\"\n        \n        # Set stop words\n        self.set_stopwords(stopwords)\n        \n        # Pare text by spaCy\n        doc = nlp(text)\n        \n        for sent in doc.sents:\n            for token in sent:\n                print(token.text)\n        for sent in doc.sents:\n            displacy.render(sent, style=\"dep\")    \n        for sent in doc.sents:\n            displacy.render(sent, style=\"ent\")\n        \n        # Filter sentences\n        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n        \n        # Build vocabulary\n        vocab = self.get_vocab(sentences)\n        \n        # Get token_pairs from windows\n        token_pairs = self.get_token_pairs(window_size, sentences)\n        print(token_pairs)\n        # Get normalized matrix\n        g = self.get_matrix(vocab, token_pairs)\n        \n        # Initialization for weight(pagerank value)\n        pr = np.array([1] * len(vocab))\n        \n        # Iteration\n        previous_pr = 0\n        for epoch in range(self.steps):\n            pr = (1-self.d) + self.d * np.dot(g, pr)\n            if abs(previous_pr - sum(pr))  < self.min_diff:\n                break\n            else:\n                previous_pr = sum(pr)\n\n        # Get weight for each node\n        node_weight = dict()\n        for word, index in vocab.items():\n            node_weight[word] = pr[index]\n        \n        self.node_weight = node_weight","5a8573c3":"import warnings\nwarnings.filterwarnings('ignore')","2add1456":"text = '''\nA woman in a wedding dress, the Bride, lies wounded in a chapel in El Paso, Texas, having been attacked by the Deadly Viper Assassination Squad. She tells their leader, Bill, that she is pregnant with his baby. He shoots her in the head.\nFour years later, having survived the attack, the Bride goes to the home of Vernita Green, planning to kill her.\n'''\n\ntr4w = TextRankForKeyword()\ntr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\ntr4w.get_keywords(10)","67ecf1ec":"\"\"\"Example of training spaCy's named entity recognizer, starting off with an\nexisting model or a blank model.\n\nFor more details, see the documentation:\n* Training: https:\/\/spacy.io\/usage\/training\n* NER: https:\/\/spacy.io\/usage\/linguistic-features#named-entities\n\nCompatible with: spaCy v2.0.0+\nLast tested with: v2.1.0\n\"\"\"\nfrom __future__ import unicode_literals, print_function\n\nimport plac\nimport random\nfrom pathlib import Path\nfrom spacy.util import minibatch, compounding\n\n\n# training data\nTRAIN_DATA = [\n    (\"Where is El Paso?\", {\"entities\": [(10, 17, \"GPE\")]}),\n    (\"I like London and El Paso.\", {\"entities\": [(7, 13, \"GPE\"), (18, 25, \"GPE\")]}),\n    (\"I like to go to the place called El Paso.\", {\"entities\": [(33, 40, \"GPE\")]}),\n    (\"El Paso is in Texas.\", {\"entities\": [(0, 7, \"GPE\"), (14, 29, \"GPE\")]}),\n    (\"Krid was born in El Paso, Texas.\", {\"entities\": [(0, 4, \"PERSON\"), (17, 24, \"GPE\"), (26, 31, \"GPE\")]}),\n    (\"In 1680, the small village of El Paso became the temporary base for Spanish governance\", {\"entities\": [(3, 7, \"DATE\"), (30, 37, \"GPE\"), (68, 75, \"NORP\")]})\n]","ed91da24":"@plac.annotations(\n    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n)\ndef retrain(model=\"en\", output_dir=None, n_iter=100):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    print(\"Other pipes present: \", other_pipes)\n    \n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # reset and initialize the weights randomly \u2013 but only if we're\n        # training a new model\n        if model is None:\n            nlp.begin_training()\n        for itn in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n            print(\"Losses\", losses)\n\n    # test the trained model\n    for text, _ in TRAIN_DATA:\n        doc = nlp(text)\n        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        # test the saved model\n        print(\"Loading from\", output_dir)\n        nlp2 = spacy.load(output_dir)\n        for text, _ in TRAIN_DATA:\n            doc = nlp2(text)\n            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n","e96e6182":"# retrain(model=\"en_core_web_sm\", output_dir=\"\/kaggle\/working\/custom_model\", n_iter=100)","ec787ab1":"# nlp = spacy.load('\/kaggle\/working\/custom_model') #loading the custom model into the same variable","29e37ea9":"# Test again\n# tr4w = TextRankForKeyword()\n# tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n# tr4w.get_keywords(10)","3bd89f25":"#TODO: Code against Catastrophic Forgetting: https:\/\/explosion.ai\/blog\/pseudo-rehearsal-catastrophic-forgetting","edc05060":"# this cell is used for testing alone hence has been disabled by changing it into raw\n# click on a cell, press esc and then r. That converts it to a \"raw\" cell. Similar thing can be done to convert it back, esc + y\n\nnlp = English()\ndoc = nlp(\"I live in El Paso\")\nfor token in doc:\n    print('\"' + token.text + '\"')\n    \n\n\nnlp = English()\nmatcher = Matcher(nlp.vocab)\npatterns = [[{\"LOWER\": \"el\"}, {\"LOWER\": \"paso\"}]]\nmatcher.add(\"TO_MERGE\", None, *patterns)\n\ndoc = nlp(\"I live in El Paso\")\nmatches = matcher(doc)\nprint(matches)\nwith doc.retokenize() as retokenizer:\n    for match_id, start, end in matches:\n        span = doc[start:end]\n        retokenizer.merge(span)\n\nfor token in doc:\n    print('\"' + token.text + '\"')","966bb266":"### Following is custom retokenization\n\nThere are 2 approaches as described here: \nhttps:\/\/stackoverflow.com\/questions\/56289487\/is-there-a-way-to-turn-off-specific-built-in-tokenization-rules-in-spacy\n\nI used the second approach.","7c46ace2":"### Model re-train - Updating pre-trained model with new examples\n\nhttps:\/\/spacy.io\/usage\/training#ner\n\nhttps:\/\/spacy.io\/api\/annotation#named-entities\n\nSuppose El Paso is not identified as GPE then this is how it can be retrained on the existing model","e59fd3f9":"Based on inputs from https:\/\/towardsdatascience.com\/textrank-for-keyword-extraction-by-python-c0bae21bcec0"}}