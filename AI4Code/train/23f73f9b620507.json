{"cell_type":{"48082fde":"code","2eb05c12":"code","040c9131":"code","cf6966ac":"code","524b9fad":"code","8360c123":"code","61e113d9":"code","4263c7a0":"code","ec6c4df3":"code","7dd1618e":"code","133eb0a9":"code","8f3428cc":"code","a15530ee":"code","d1979b6b":"code","fdd1216f":"code","38ed5469":"code","0c47ac1e":"code","fd861c1c":"code","02b66551":"code","669ca2fe":"code","557e96af":"code","ba229283":"code","fc60ebd3":"code","703953a7":"code","5fa2943a":"code","07728af4":"code","544768e3":"code","f15b7045":"code","0bec25f9":"code","e89c7d2d":"code","0eae26e0":"markdown","dbfb2441":"markdown","15cb7f49":"markdown","4b56a88f":"markdown","37ec2381":"markdown","c29988ff":"markdown","4eb70866":"markdown","09ff3354":"markdown","8cb3af11":"markdown","40c32005":"markdown","8a3e5725":"markdown"},"source":{"48082fde":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport time","2eb05c12":"import sys\nsys.path.append('..\/input\/dtw-barycenter-averaging')\n!pip install tslearn","040c9131":"import DBA_multivariate as DBA\nfrom tslearn.metrics import dtw, dtw_path, cdist_dtw\nfrom sklearn.cluster import AgglomerativeClustering\nfrom tslearn.clustering import KShape\nfrom sklearn.metrics import silhouette_score","cf6966ac":"df = pd.read_csv('..\/input\/gobal-terrorism-database-time-series\/Ter_Time_Series.csv')\ndf.sort_values(['country_txt', 'iyear'], inplace = True)\ndf.set_index(['country_txt'], inplace = True, drop = True)","524b9fad":"to_keep = []\nfor country in df.index.unique():\n    if 1980 in df.loc[country]['iyear'].values:\n        to_keep.append(country)\ndf1 = df.loc[to_keep]    \n\nfeatures = ['suicide', 'attacktype1_txt_Armed Assault',\n       'attacktype1_txt_Assassination', 'attacktype1_txt_Bombing\/Explosion',\n       'attacktype1_txt_Facility\/Infrastructure Attack',\n       'attacktype1_txt_Hijacking',\n       'attacktype1_txt_Hostage Taking (Barricade Incident)',\n       'attacktype1_txt_Hostage Taking (Kidnapping)',\n       'attacktype1_txt_Unarmed Assault', 'attacktype1_txt_Unknown',\n       'weaptype1_txt_Biological', 'weaptype1_txt_Chemical',\n       'weaptype1_txt_Explosives', 'weaptype1_txt_Fake Weapons',\n       'weaptype1_txt_Firearms', 'weaptype1_txt_Incendiary',\n       'weaptype1_txt_Melee', 'weaptype1_txt_Other',\n       'weaptype1_txt_Radiological', 'weaptype1_txt_Sabotage Equipment',\n       'weaptype1_txt_Unknown',\n       'weaptype1_txt_Vehicle (not to include vehicle-borne explosives, i.e., car or truck bombs)',\n       'targtype1_txt_Abortion Related', 'targtype1_txt_Airports & Aircraft',\n       'targtype1_txt_Business', 'targtype1_txt_Educational Institution',\n       'targtype1_txt_Food or Water Supply',\n       'targtype1_txt_Government (Diplomatic)',\n       'targtype1_txt_Government (General)',\n       'targtype1_txt_Journalists & Media', 'targtype1_txt_Maritime',\n       'targtype1_txt_Military', 'targtype1_txt_NGO', 'targtype1_txt_Other',\n       'targtype1_txt_Police', 'targtype1_txt_Private Citizens & Property',\n       'targtype1_txt_Religious Figures\/Institutions',\n       'targtype1_txt_Telecommunication',\n       'targtype1_txt_Terrorists\/Non-State Militia', 'targtype1_txt_Tourists',\n       'targtype1_txt_Transportation', 'targtype1_txt_Unknown',\n       'targtype1_txt_Utilities', 'targtype1_txt_Violent Political Party']#,'success']\n\ndf1 = df1.loc[df1.iyear >=1980][['iyear']+features]\n","8360c123":"#First Clustering technique we will use will be Agglomerative Clustering","61e113d9":"#format for distance matrix\narr = np.array(df1[features]).reshape(108,39,44)\ndtw_mat = cdist_dtw(arr)","4263c7a0":"AC = AgglomerativeClustering(affinity = 'precomputed', linkage = 'average',distance_threshold = 0,  n_clusters =None)\nAC.fit(dtw_mat )\nAC.labels_","ec6c4df3":"from scipy.cluster.hierarchy import dendrogram\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)","7dd1618e":"plot_dendrogram(AC, truncate_mode = 'level', p = 8)","133eb0a9":"#One challenge here is which method to use to find the centroids of the time series. First will try mean and median\n#One of the drawbacks of mean and median are that can only compare series of the same length ","8f3428cc":"def centroid_mean(cluster, data):\n    cluster_sum =0\n    for k in cluster:\n        cluster_sum += data.loc[k,features].values\n    return [m\/len(cluster) for m in cluster_sum]","a15530ee":"def centroid_median(cluster, data):\n    cluster_series = [data.loc[k, features].values for k in cluster]\n    \n    \n    return np.median(cluster_series, axis = 0)\n","d1979b6b":"#DBA function requires time series to be in format (channels, time) so will have to swap axes twice\ndef centroid_DBA(cluster, data):\n    cluster_series = np.array([data.loc[k, features].values for k in cluster])\n    cluster_series = cluster_series.swapaxes(1,2)\n    centroid = DBA.performDBA(cluster_series)\n    return list(centroid.swapaxes(0,1))\n\n","fdd1216f":"def medoid(cluster, data):\n    cluster_series = [data.loc[k, features].values for k in cluster]\n    distance_mat = cdist_dtw(cluster_series)\n    distance_sums = np.sum(distance_mat, axis = 0)\n    medoid = np.argmin(distance_sums)\n    return cluster_series[medoid]","38ed5469":"def array_of_labels(data,cluster_dic):\n    res = np.zeros((len(data.index.unique())), dtype = 'int')\n    for key in cluster_dic.keys():\n        for country in cluster_dic[key]:\n            ind = list(data.index.unique()).index(country)\n            res[ind] = key\n            \n    return res","0c47ac1e":"def kmeans(data, features,  n_clusters, centroid_func, steps):\n    centroids_countries = random.sample(list(data.index.unique()), n_clusters)\n    centroids = [data.loc[country][features].values for country in centroids_countries]\n    current_step = 0\n    \n    for step in range(steps):\n        current_step +=1\n        clusters = {}\n        \n        for country in data.index.unique():\n            min_dist = float('inf')\n            closest_cluster = None\n            \n            for j, centroid in enumerate(centroids):\n                dist = dtw(data.loc[country][features], centroid)\n                if dist < min_dist:\n                    min_dist = dist\n                    closest_cluster = j\n            \n            if closest_cluster in clusters:\n                clusters[closest_cluster].append(country)\n            else:\n                clusters[closest_cluster] = []\n                clusters[closest_cluster].append(country)   \n        \n        for key in clusters.keys():\n            centroids[key] = centroid_func(clusters[key],data)\n        \n        \n        \n    return centroids, clusters\n                    ","fd861c1c":"\ndef within_cluster_distances(data, clusters,centroids, n_clusters, features=features, distance = dtw):\n    total = 0\n    for cluster in clusters.values():\n        for i in range(len(cluster)):\n            for j in range(i+1,len(cluster)):\n                total+= distance(data.loc[cluster[i],features],data.loc[cluster[j],features] )\n    return total","02b66551":"def between_cluster_distances(data, clusters, centroids, n_clusters, features = features, distance = dtw):\n    total =0\n    for i in range(len(centroids)):\n        for j in range(i+1,len(centroids)):\n            total+=distance(centroids[i], centroids[j])\n    return total\/n_clusters","669ca2fe":"def Calinski_Harabasz(data, clusters, centroids, n_clusters, features = features, distance = dtw):\n    N = sum([len(i) for i in clusters.values()])\n    k = n_clusters\n    return ((N-k)\/(k-1))*(between_cluster_distances(data,clusters, centroids, n_clusters)\/within_cluster_distances(data,clusters, centroids, n_clusters))","557e96af":"def Silhouette(data,clusters, centroids, n_clusters, features = features, distance = dtw):\n    dist_mat = cdist_dtw(np.array(data[features]).reshape(108,39,len(features)))\n    score = silhouette_score(dist_mat, array_of_labels(data,clusters))\n    return score","ba229283":"def evaluate_clustering_technique(data, features, clustering_technique, eval_metrics):\n    med_evals = [[] for _ in range(len(eval_metrics))]\n    for n_cluster in range(2, 11):\n        evaluations = [[] for _ in range(len(eval_metrics))]\n        i = 0\n        while i <= 7:                                                                       \n            centroids, clusters=  kmeans(data,features, n_cluster,clustering_technique,5)\n            for j, metric in enumerate(eval_metrics):\n                evaluations[j].append(metric(data, clusters,centroids, n_cluster))\n        \n            i +=1\n            \n        for j in range(len(evaluations)):\n            med_evals[j].append(np.median(evaluations[j]))\n    return med_evals","fc60ebd3":"\ntechniques = ['Mean', 'Median', 'DBA', 'Medoid']\nmetrics = ['WCD', 'BCD', 'CH', 'Silhouette']\ncolours = ['b', 'g', 'r', 'y']\nfig, axs = plt.subplots(4,4, figsize = (24,20))\nfinal_evals = {}\nfor i, technique in enumerate([centroid_mean, centroid_median, centroid_DBA, medoid]):\n    evals = []\n    ts = time.time()\n    print('Starting ' + techniques[i])\n    evals = evaluate_clustering_technique(df1, features, technique, [within_cluster_distances, between_cluster_distances, Calinski_Harabasz, Silhouette])\n    for j in range(len(evals)):    \n        axs[i,j].plot(list(range(2,11)), evals[j], colours[j])\n        axs[i,j].set_title(techniques[i]+' '+metrics[j])\n    final_evals[techniques[i]] = evals\n    print(techniques[i]+' Took {} seconds to complete'.format(time.time()-ts))","703953a7":"def best_kmeans(data, technique,metric,iterations):    \n    num_clusters = 5\n    i=0\n    max_score = 0\n    while i < 3:\n        print(i)\n        centroids, clusters=  kmeans(data,features, num_clusters,technique, iterations)\n        score = metric(data, clusters,centroids, num_clusters)\n        if score > max_score:\n            max_score = score\n            best_cluster = clusters\n            best_centroids = centroids\n        i +=1\n        \n    print(max_score)\n    return best_cluster, best_centroids","5fa2943a":"DBA_clusters, DBA_centroids = best_kmeans(df1, centroid_DBA, Calinski_Harabasz, 5)\nmean_clusters, mean_centroids = best_kmeans(df1,centroid_mean, Calinski_Harabasz,5)\nmedian_clusters, median_centroids = best_kmeans(df1,centroid_mean, Calinski_Harabasz,5)\nmedoid_clusters, medoids = best_kmeans(df1, medoid, Calinski_Harabasz,5)","07728af4":"DBA_clusters","544768e3":"mean_clusters","f15b7045":"#Want to visualise the centroids of the clusters for different features. Want to find out if the clustering was actually able to separate countries based on the type of attacks\n#rather than just the frequency of the attacks, so will look at the categorical features as a proportion of the total successful attacks.\n\ndef plot_centroid_features(feat):\n\n    centroids = [mean_centroids,median_centroids,DBA_centroids, medoids]\n\n    fig, axs = plt.subplots(4,len(feat), figsize = (10*len(feat),24))\n\n    for i, technique in enumerate(centroids):\n        for j, feature in enumerate(feat):\n            for cluster_centroid in technique:\n                axs[i,j].plot(list(range(1980,2019)),[year[features.index(feature)]\/sum(year[1:10]) for year in cluster_centroid])\n\n            axs[i,j].legend(list(range(5)))\n            axs[i,j].legend(list(range(5)))\n            axs[i,j].set_title(techniques[i] +' '+ feature)\n            plt.xlabel('year')\n            plt.ylabel('successful attacks per million')","0bec25f9":"plot_centroid_features(['attacktype1_txt_Bombing\/Explosion','attacktype1_txt_Armed Assault'])","e89c7d2d":"plot_centroid_features(['targtype1_txt_Business','targtype1_txt_Private Citizens & Property','targtype1_txt_Government (General)'])","0eae26e0":"Observations  \nDBA and Euclidean Mean centroid methods seem to perform better across the board than medoid and median centroids. \nAll methods WCD levels off sharply at 5 clusters, whereas BCD continues to rise for most methods until 9 clusters, with mean and medoid continuting to rise at 10. \nMean Silhouette score decreases smoothly for DBA, and Mean levels off a bit at 6 clusters, before falling sharply. Silhouette scores for median are negative, indicating countries are being poorly assigned to their clusters. Medoid varies quite a lot, should probably run more iterations to see if it smooths out. \n\n\n\nFor Within Cluster Differences, Mean seems to find lower values for smaller numbers of clusters. Medoid starts off the worst then catches up\nFor Between Cluster Difference, Mean Centroid seems to be the largest, however this could be due to some of the issues with using arithmetic mean to calculate time-series centroids\n(see https:\/\/github.com\/fpetitjean\/DBA for a nice demonstration)\n\nC-H statistic pretty similar at low cluster numbers for Mean and DBA, then they diverge due to the difference in their BCD values. \n\nSeems like 5 clusters makes sense.\n\nWill now look at the clusters and try to learn a bit about how the algorithms have divided the dataset. \n","dbfb2441":"First thing that is noticeable is that for Euclidean mean and median, there is a large number of crossings of the centroids. Lots of co-movement\n\nDBA centroids seem more dispersed, less co-movement?\n\nMedoids seem to be missing a lot of values for cluster 4 - need to investigate\n\n\n","15cb7f49":"The agglomerative method seems to result in one cluster representing a very high proportion of the dataset, with a handful of outliers ending up in other   \nLinkage method doesn't seem to change much, generally end up with El Salvador on its own, Iraq on its own, Afghanistan, Somalia together and roughly 100 other countries together","4b56a88f":"Can see that Calinski_Harabasz and Silhouette take a LONG time to complete. Complexity is O(nN) and O(nN^2) respectively, and with a distance metric as expensive as DTW, computing the distance matrix for the entire dataset (as is the case for Silhouette) makes it pretty inefficient.\nDefinitely will look for improvements on the efficiency front. Using LB_Keogh an obvious place to start. ","37ec2381":"Clustering Evaluation Metrics","c29988ff":"Using a combination of the H-C stat and the Silhouette Score, can say that DBA and euclidean Mean probably provides the best way to use K-Means clustering on this multivariate time-series dataset. \nWill also visualise the centroids to help gain more of an idea of how the clustering techniques differed. \nWould love in the future to compare some more techniques such as K-Shape, however could not find a Python implementation of it which works for multi-dimensional time-series.","4eb70866":"Implemented this from scratch as didn't know about tslearn's TimeSeriesKmeans func","09ff3354":"To Do:\n\nProper hyperparameter tuning on num_iterations\n\nIf have time, go back and look at feature engineering stage. Is standardising features definitely a bad idea? etc \n\nTry to link maths of centroid calculation technique to shapes of centroids.\n\nCompare to clustering based only on geographical location. \n\nMore efficient distance metrics. ","8cb3af11":"DBA centroids for target type are clearly more spaced out than Mean and Median. Could show that DBA is doing a better job of separating the clusters based on this categorical variable, but could also just mean that the centroids of the clusters are more meaningful as opposed to the clusters themselves.\n\nCould maybe use DBA to find centroids of final Euclidean Mean clusters and then compare them. ","40c32005":"Next Method will be Kmeans. Have decided to keep distance method constant as dtw, however will try different methods of finding centroids of the clusters.","8a3e5725":"Mean and Median raise issues when using a non-Euclidean distance metric, so I will be comparing how they do with a more time-series specific method proposed by Francois Petijohn, called DTW Bary-center Averaging. I will also compare these with k-medoids."}}