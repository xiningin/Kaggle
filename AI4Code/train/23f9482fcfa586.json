{"cell_type":{"3170573d":"code","a4b49834":"code","e82161b5":"code","2f7b51d3":"code","b1e2e125":"code","71f18cfe":"code","bb6cfa7d":"code","e4695e99":"code","b2af940f":"code","1b99c724":"code","680183f5":"code","83209c74":"code","5676f3a4":"code","3262c552":"code","e23c736c":"code","81160c8f":"code","ffff4831":"code","51c2a311":"code","4efe6e54":"code","3712f540":"code","6eb0941f":"code","7348f27c":"code","5132a12f":"code","69fde1e7":"code","3b7a7f16":"code","32276d3d":"code","8b63c163":"code","3aa524dc":"markdown","806cb7f9":"markdown","97af98fa":"markdown","dc7c6a1d":"markdown","718b3a8b":"markdown","3192bf45":"markdown","a5909930":"markdown","104720f3":"markdown","fe214ae6":"markdown","465c4e48":"markdown","74ddf9d9":"markdown","5fa9d1cf":"markdown","1511c7fe":"markdown","9bd37513":"markdown","7404d672":"markdown","dbb0a394":"markdown","c46688e6":"markdown","92f5263f":"markdown","79008569":"markdown","ac261793":"markdown"},"source":{"3170573d":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom torchvision.utils import make_grid\n\nfrom PIL import Image\nfrom IPython.display import display\nimport cv2\n\n\nimport glob\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport pickle as pkl\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","a4b49834":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(42)","e82161b5":"# Let's define the path\ndata_dir = '..\/input\/celeba-dataset\/img_align_celeba\/'\n","2f7b51d3":"# defining the image transform rules\n'''\nThe augmentions are as follows, \n1. Resizing the image to 32x32 as the smaller the image the faster we can train \n2. Cropping from center with 32x32\n3. Chagning type to tensor\nwe won't be using normalize here as we will have to do that manually later for tanh activation function\n'''\n\ntransform_img = transforms.Compose([\n        transforms.Resize(32),  \n        transforms.CenterCrop(32),\n        transforms.ToTensor()\n    ])","b1e2e125":"def get_dataloader(batch_size, data_dir, transforms):\n    \"\"\"\n    Batch the neural network data using DataLoader\n    :param batch_size: The size of each batch; the number of images in a batch\n    :param data_dir: Directory where image data is located\n    :param transforms: data augmentations\n    :return: DataLoader with batched data\n    \"\"\"\n    \n    ds = datasets.ImageFolder(data_dir, transform_img)\n    data_loader = torch.utils.data.DataLoader(ds, batch_size=batch_size,num_workers= 4, shuffle=True)\n\n    return data_loader","71f18cfe":"batch_size = 32 # Instead of individual samples, the data loader produces batched samples of given number\ntrain_loader = get_dataloader(batch_size,data_dir, transform_img)\n","bb6cfa7d":"print(len(train_loader.dataset))","e4695e99":"# Let's see if the train loader is working and sending us iterable images :) \n# Also note that we converted images to tensor above with ToTensor(). Now we need to convert back to numpy to plot them\n\n# obtain one batch of training images which means 32 images\ndataiter = iter(train_loader)\nimg, _ = dataiter.next() # _ for labels, Dataloader sends labels automatically as defined in Dataloader class.\n#However we don't need labels neither we assigned one :p so we put _ \n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(30, 7))\nplot_size=30 # gonna plot 30 images only\nfor idx in np.arange(plot_size):\n    ax = fig.add_subplot(3, plot_size\/3, idx+1, xticks=[], yticks=[])\n    npimg = img[idx].numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","b2af940f":"def scale_images(x, max = 1.00 , min = -1.00):\n    x = x * (max - min) + min\n    return x","1b99c724":"#let's check the scaling\nimg = img[5]\nprint('Before scaling min: ', img.min())\nprint('Before scaling max: ', img.max())\n\nscaled_img = scale_images(img)\n\nprint('After Scaling Min: ', scaled_img.min())\nprint('After Scaling Max: ', scaled_img.max())","680183f5":"'''\nThe inputs to the discriminator are 32x32x3 tensor images\nThe output would be a single value that will indicate whether a given image is real or fake\n'''\ndef conv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True, bias = False):\n    layers = []\n    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                           kernel_size=kernel_size, stride=stride, padding=padding)\n    #appending convolutional layer\n    layers.append(conv_layer)\n    #appending batch norm layer\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n        \n    return nn.Sequential(*layers)","83209c74":"class Discriminator(nn.Module):\n\n    def __init__(self, conv_dim):\n        \"\"\"\n        Initializing the Discriminator Module\n        :param conv_dim: The depth of the first convolutional layer based on which we will create the  next ones where next  layer depth = 2 * previous layer depth\n        \"\"\"\n        super(Discriminator, self).__init__()\n\n        # complete init function\n        self.conv_dim = conv_dim\n        \n        \n        self.conv1 = conv(3, conv_dim, batch_norm=False)  \n        self.conv2 = conv(conv_dim, conv_dim*2)           \n        self.conv3 = conv(conv_dim*2, conv_dim*4)\n        self.conv4 = conv(conv_dim*4, conv_dim*8)\n        self.fc = nn.Linear(conv_dim*4*4*2, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Forward propagation of the neural network\n        :param x: The input to the neural network     \n        :return: Discriminator logits; the output of the neural network\n        \"\"\"\n        # define feedforward behavior\n        x = F.leaky_relu(self.conv1(x), 0.2)\n        x = F.leaky_relu(self.conv2(x), 0.2)\n        x = F.leaky_relu(self.conv3(x), 0.2)\n        x = F.leaky_relu(self.conv4(x), 0.2)\n        \n        x = x.view(-1, self.conv_dim*4*2*4)\n        \n        x = self.fc(x)\n        \n        \n        return x\n","5676f3a4":"def deconv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True, bias = False):\n    layers = []\n    \n    # append transpose conv layer -- we are not using bias terms in conv layers\n    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding))\n    \n    # optional batch norm layer\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n        \n    return nn.Sequential(*layers)","3262c552":"class Generator(nn.Module):\n    \n    def __init__(self, z_size, conv_dim):\n        \"\"\"\n        Initialize the Generator Module\n        :param z_size: The length of the input latent vector, z\n        :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer\n        \"\"\"\n        super(Generator, self).__init__()\n        self.conv_dim = conv_dim\n        \n        self.fc = nn.Linear(z_size, conv_dim*4*4*4)\n        # complete init function\n        \n        self.de_conv1 = deconv(conv_dim*4, conv_dim*2)\n        self.de_conv2 = deconv(conv_dim*2, conv_dim)\n        self.de_conv3 = deconv(conv_dim, 3, 4, batch_norm=False )\n        \n        self.dropout = nn.Dropout(0.3)\n        \n        \n    def forward(self, x):\n        \"\"\"\n        Forward propagation of the neural network\n        :param x: The input to the neural network     \n        :return: A 32x32x3 Tensor image as output\n        \"\"\"\n        # define feedforward behavior\n        x = self.fc(x)\n        x = self.dropout(x)\n        \n        x = x.view(-1, self.conv_dim*4, 4, 4)\n        \n        x = F.relu(self.de_conv1(x))\n        x = F.relu(self.de_conv2(x))\n        x = self.de_conv3(x)\n        x = F.tanh(x)\n        \n        \n        return x","e23c736c":"#Initializing the weights to a normal distribution, centered around 0, with a standard deviation of 0.02.\n\ndef weights_init_normal(m):\n    \"\"\"\n    :param m: A module or layer in a network    \n    \"\"\"\n    # like `Conv`, `BatchNorm2d`, `Linear`, etc.\n    classname = m.__class__.__name__\n    \n    #  initial weights to convolutional and linear layers\n    if (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n        nn.init.normal(m.weight.data, 0.0, 0.2)\n        \n    if hasattr(m, 'bias') and m.bias is not None:\n        nn.init.constant(m.bias.data, 0.0)","81160c8f":"# model hyperparameters\nd_conv_dim = 64\ng_conv_dim = 128\nz_size = 100\n# building discriminator and generator from the classes defined above\ndiscriminator = Discriminator(d_conv_dim)\ngenerator = Generator(z_size=z_size, conv_dim=g_conv_dim)\n\n# initialize model weights\ndiscriminator.apply(weights_init_normal)\ngenerator.apply(weights_init_normal)\nprint(\"done\")","ffff4831":"# let's look at our discriminator model\nprint(discriminator)","51c2a311":"# let's look at our generator model\nprint(generator)","4efe6e54":"use_gpu = torch.cuda.is_available()\n","3712f540":"lr = 0.0002 #learning rate\nbeta1=0.5\nbeta2=0.999\n\n# optimizers for the discriminator D and generator G\ndiscriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr, (beta1, beta2)) # for discriminator\ngenerator_optimizer = torch.optim.Adam(generator.parameters(), lr, (beta1, beta2)) # for generator","6eb0941f":"def real_loss(D_out, smooth = False):\n    '''Calculates how close discriminator outputs are to being real.\n       param, D_out: discriminator logits\n       return: real loss'''\n    batch_size = D_out.size(0)\n    \n    if smooth:\n        labels = torch.ones(batch_size)*0.9\n    else:\n        labels = torch.ones(batch_size) \n    \n    if use_gpu:\n        labels = labels.cuda()\n    \n    criterion = nn.BCEWithLogitsLoss()\n    loss = criterion(D_out.squeeze(), labels)\n    \n    return loss\n\ndef fake_loss(D_out):\n    '''Calculates how close discriminator outputs are to being fake.\n       param, D_out: discriminator logits\n       return: fake loss'''\n    batch_size = D_out.size(0)\n    labels = torch.zeros(batch_size)\n    \n    if use_gpu:\n        labels = labels.cuda()\n        \n    criterion = nn.BCEWithLogitsLoss()\n    loss = criterion(D_out.squeeze(), labels)\n    \n    return loss","7348f27c":"def train(D, G, n_epochs, train_on_gpu, print_every=50):\n    '''Trains adversarial networks for some number of epochs\n       param, D: the discriminator network\n       param, G: the generator network\n       param, n_epochs: number of epochs to train for\n       param, print_every: when to print and record the models' losses\n       return: D and G losses'''\n    \n    # move models to GPU\n    if train_on_gpu:\n        D.cuda()\n        G.cuda()\n\n    # keep track of loss and generated, \"fake\" samples\n    samples = []\n    losses = []\n\n    # Get some fixed data for sampling. These are images that are held\n    # constant throughout training, and allow us to inspect the model's performance\n    sample_size=16\n    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n    fixed_z = torch.from_numpy(fixed_z).float()\n    # move z to GPU if available\n    if train_on_gpu:\n        fixed_z = fixed_z.cuda()\n\n    # epoch training loop\n    for epoch in range(n_epochs):\n\n        # batch training loop\n        for batch_i, (real_images, _) in enumerate(train_loader):\n\n            batch_size = real_images.size(0)\n            real_images = scale_images(real_images)\n\n            # Train the discriminator on real and fake images\n            discriminator_optimizer.zero_grad()\n            \n            if train_on_gpu:\n                real_images = real_images.cuda()\n                \n            D_real = D(real_images)\n            d_real_loss = real_loss(D_real)\n\n            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n            z = torch.from_numpy(z).float()\n\n            if train_on_gpu:\n                z = z.cuda()\n                \n            fake_images = G(z)\n            \n            D_fake = D(fake_images)\n            d_fake_loss = fake_loss(D_fake)\n            \n            \n            d_loss = d_real_loss + d_fake_loss\n            d_loss.backward()\n            discriminator_optimizer.step()     \n\n            # 2. Train the generator with an adversarial loss\n            generator_optimizer.zero_grad()\n            \n            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n            z = torch.from_numpy(z).float()\n            \n            if train_on_gpu:\n                z = z.cuda()\n            \n            fake_images = G(z)\n            \n            D_fake = D(fake_images)\n            \n            g_loss = real_loss(D_fake)\n        \n            g_loss.backward()\n            generator_optimizer.step()\n            \n            # Print some loss stats\n            if batch_i % print_every == 0:\n                # append discriminator loss and generator loss\n                losses.append((d_loss.item(), g_loss.item()))\n                # print discriminator and generator loss\n                print('Epoch [{:5d}\/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))\n\n\n        ## AFTER EACH EPOCH##    \n        # this code assumes your generator is named G, feel free to change the name\n        # generate and save sample, fake images\n        G.eval() # for generating samples\n        samples_z = G(fixed_z)\n        samples.append(samples_z)\n        G.train() # back to training mode\n\n    # Save training generator samples\n    with open('train_samples.pkl', 'wb') as f:\n        pkl.dump(samples, f)\n    \n    # finally return losses\n    return losses","5132a12f":"n_epochs = 10\nlosses = train(discriminator, generator, n_epochs=n_epochs, train_on_gpu = use_gpu)","69fde1e7":"fig, ax = plt.subplots()\nlosses = np.array(losses)\nplt.plot(losses.T[0], label='Discriminator', alpha=0.5)\nplt.plot(losses.T[1], label='Generator', alpha=0.5)\nplt.title(\"Training Losses\")\nplt.legend()","3b7a7f16":"def view_samples(epoch, samples):\n    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n    for ax, img in zip(axes.flatten(), samples[epoch]):\n        img = img.detach().cpu().numpy()\n        img = np.transpose(img, (1, 2, 0))\n        img = ((img + 1)*255 \/ (2)).astype(np.uint8)\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        im = ax.imshow(img.reshape((32,32,3)))","32276d3d":"with open('train_samples.pkl', 'rb') as f:\n    samples = pkl.load(f)","8b63c163":"_ = view_samples(-1, samples)","3aa524dc":"The project's main target is to define and train a DCGAN on a dataset of faces. The goal is to get a generator network to generate new images of faces that look as realistic as possible!","806cb7f9":"At first we need to import the libraries. It is considered as standard imports.\n\n","97af98fa":"## Generator \n\nThe name,itself is self explanatory. The generator component of a GAN learns to generate fake data by incorporating discriminator feedback. It learns to manipulate the discriminator so that its output is classified as real. The generator should upsample an input and create a new image with the same dimensions as our training data (28x28x3). This should mostly consist of transpose convolutional layers with normalzed output. ","dc7c6a1d":"Dataloader provides an iterable over the specified dataset by combining a dataset with a sampler. ","718b3a8b":"# Defining Model","3192bf45":"# Visualize the generated images","a5909930":"first, let's define essentials to train the model. ","104720f3":"Now we need to scale our images as the output of a tanh activated generator will contain pixel values in a range from -1 to 1. So, we need to rescale our training images to a range of -1 to 1. ","fe214ae6":"# Import Libraries","465c4e48":"This train function was provided in the udacity deep learning nanodegree course. It's great. ","74ddf9d9":"Training the discriminator by alternating on real and fake images.\nThen the generator, which tries to trick the discriminator and should have an opposing loss function.","5fa9d1cf":"To help the models converge, we should initialize the weights of the convolutional and linear layers in your model. From the original [DCGAN paper](https:\/\/arxiv.org\/pdf\/1511.06434.pdf), they say:\n> All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02.\n\n","1511c7fe":"## Discriminator\nThe discriminator is a classifier that detects if the input samples are genuine or fabricated. As a result, the discriminator in a GAN is essentially a classifier. It attempts to discern between genuine data and data generated by the generator. It could utilize any network architecture suitable for the type of data it's classifying. Here I will be using convolutional classifier, only without any maxpooling layers. It is recommended to employ a deep network with normalization to deal with this kind of difficult data.","9bd37513":"# Training the model","7404d672":"# Dataloader and Visualization","dbb0a394":"We will be using this function mostly everywhere to run our experiments deterministically. Random functions of Numpy and Pandas will behave deterministically after this. To learn more about Deterministic Neural Networks please check out [this notebook](https:\/\/www.kaggle.com\/bminixhofer\/deterministic-neural-networks-using-pytorch)","c46688e6":"### Training loss","92f5263f":"### Loss functions for generator\nThe generator's goal is to get the discriminator to think its generated images are real.","79008569":"now our min and max are in range of [-1,1]","ac261793":"### Optimizers\nTo learn about adam optimizer please check [this blog](https:\/\/towardsdatascience.com\/adam-latest-trends-in-deep-learning-optimization-6be9a291375c). I found it helpful for me. :) "}}