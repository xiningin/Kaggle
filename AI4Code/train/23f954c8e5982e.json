{"cell_type":{"7f8a3fa5":"code","d728d95d":"code","036aeb66":"code","baa57f45":"code","02f31572":"code","83087fc6":"code","e73a73e7":"code","26dbc44a":"code","2fbd5ea1":"code","b17dbce2":"code","db71d3d4":"code","8b04dbae":"code","9445d7f8":"code","5571a44d":"code","e89fa438":"code","d1512da6":"code","0398451f":"code","f9f80573":"code","469da040":"code","a8ec8305":"code","21d90390":"code","bb6109d6":"code","fb033550":"code","111385a9":"code","bb8441c1":"code","659beb3b":"code","fe68c775":"code","e461fb19":"code","871bd423":"code","87db30c5":"code","7fa906ff":"code","7f2585d9":"code","8a58f1ba":"code","6f0263e8":"code","0f2d8e32":"code","ad1dbc4d":"code","5410692e":"code","e746b294":"code","aef72f61":"code","f74e9e48":"code","5f139320":"code","10c2f088":"code","c78b8418":"code","7045017f":"code","2fc34642":"code","2807472b":"code","08c8aed2":"code","7f621fb4":"code","b3ce181c":"code","d594534f":"code","d4bdc788":"code","6c07014a":"code","f67566e1":"code","a9ef910e":"code","52f2d80b":"code","12a8e7e9":"code","9bafd7fd":"markdown","c6d4fe0f":"markdown","2d0ae0e9":"markdown","a2103578":"markdown","aff73e1e":"markdown","eb2cf447":"markdown","6a60c9cc":"markdown","975e7f1b":"markdown","cb2373a1":"markdown","c7a96127":"markdown","d134448a":"markdown","83b0fe77":"markdown","54669e0f":"markdown","ba7cf4d3":"markdown","35a5e8d0":"markdown","8bc2c2a0":"markdown","0c1addd8":"markdown"},"source":{"7f8a3fa5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport pandas_profiling\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set(rc = {'figure.figsize':(15, 10)})\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom IPython.core.interactiveshell import InteractiveShell \nInteractiveShell.ast_node_interactivity = \"all\"","d728d95d":"# Carregando os dados\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","036aeb66":"pandas_profiling.ProfileReport(train)","baa57f45":"# Avaliando o shape do dataset de treino e teste\ntrain.shape, test.shape","02f31572":"# Avaliando os t\u00edpos das vari\u00e1veis\ntrain.info()","83087fc6":"# Aval\ntrain.head().T","e73a73e7":"#Analisando a quantidade de inadimplencias atrav\u00e9s da vari\u00e1vel target LOAN_DEFAULT\ntrain['LOAN_DEFAULT'].value_counts(normalize=True)\ntrain['LOAN_DEFAULT'].value_counts(normalize=True).plot.bar()","26dbc44a":"# Cria\u00e7\u00e3o de c\u00f3pia do dataframe de treino\ntrain_copy = train.copy()","2fbd5ea1":"train.shape, train_copy.shape","b17dbce2":"# Os dataframes de teste e treino ser\u00e3o unidos afim de facilitar o processo de transforma\u00e7\u00e3o e de feature engineering\ntrain = train.append(test)","db71d3d4":"train.shape","8b04dbae":"#Verificando como est\u00e3o os dados da coluna AVERAGE_ACCT_AGE\ntrain['AVERAGE_ACCT_AGE'].value_counts(normalize=True)\n\n","9445d7f8":"# Vamos criar nova coluna (AVERAGE_ACCT_AGE_EM_MESES), que vai contabilizar a quantidade de meses que est\u00e3o dispostas na coluna AVERAGE_ACCT_AGE\ntrain['AVERAGE_ACCT_AGE_EM_MESES'] = train['AVERAGE_ACCT_AGE'].str.split('y').str.get(0).astype(int)*12 + train['AVERAGE_ACCT_AGE'].str.split(' ').str.get(1).str.split('m').str.get(0).astype(int)","5571a44d":"train.head().T","e89fa438":"#Verificando como est\u00e3o os dados da coluna CREDIT_HISTORY_LENGTH\ntrain['CREDIT_HISTORY_LENGTH'].value_counts(normalize=True)","d1512da6":"# Vamos criar nova coluna (CREDIT_HISTORY_LENGTH_EM_MESES), que vai contabilizar a quantidade de meses que est\u00e3o dispostas na coluna CREDIT_HISTORY_LENGTH\ntrain['CREDIT_HISTORY_LENGTH_EM_MESES'] = train['CREDIT_HISTORY_LENGTH'].str.split('y').str.get(0).astype(int)*12 + train['CREDIT_HISTORY_LENGTH'].str.split(' ').str.get(1).str.split('m').str.get(0).astype(int)\n","0398451f":"train.head().T","f9f80573":"# Verifica-se que a data de nascimento est\u00e1 na coluna DATE_OF_BIRTH\n# Iremos transformar a data de nascimento em idade\ntrain['DATE_OF_BIRTH'] = train['DATE_OF_BIRTH'].astype('datetime64[ns]')\nnow = pd.Timestamp('now')\ntrain['DATE_OF_BIRTH'] = pd.to_datetime(train['DATE_OF_BIRTH'], format='%m%d%y')\ntrain['DATE_OF_BIRTH'] = train['DATE_OF_BIRTH'].where(train['DATE_OF_BIRTH'] < now, train['DATE_OF_BIRTH'] -  np.timedelta64(100, 'Y'))\ntrain['Idade'] = (now - train['DATE_OF_BIRTH']).astype('<m8[Y]').astype(int)","469da040":"# Verificando a nova coluna Idade\ntrain['Idade']","a8ec8305":"# DISBURSAL_DATE - data do desembolso\n# Esta coluna ser\u00e1 tranformada em dias afim de verificar se o tempo decorrido a partir da concess\u00e3o do cr\u00e9dito \u00e9\n# determinante do inadimplemento dos clientes\n\ntrain['DISBURSAL_DATE'] = train['DISBURSAL_DATE'].astype('datetime64[ns]')\nnow = pd.Timestamp('now')\ntrain['DISBURSAL_DATE'] = pd.to_datetime(train['DISBURSAL_DATE'], format='%m%d%y')\ntrain['DISBURSAL_DATE'] = train['DISBURSAL_DATE'].where(train['DISBURSAL_DATE'] < now, train['DISBURSAL_DATE'] -  np.timedelta64(100, 'Y'))\ntrain['DIAS_DESEMBOLSO'] = (now - train['DISBURSAL_DATE']).dt.days","21d90390":"# verificando a quantidade de dias de desembolso\ntrain['DIAS_DESEMBOLSO']","bb6109d6":"#Verificando como est\u00e3o os dados da coluna EMPLOYMENT_TYPE\ntrain['EMPLOYMENT_TYPE'].value_counts()","fb033550":"# Categorizando os dados, inclusive os valores nulos\nle_employment_type = LabelEncoder()\ntrain['EMPLOYMENT_TYPE'] = le_employment_type.fit_transform(list(train['EMPLOYMENT_TYPE']))\ntrain['EMPLOYMENT_TYPE'].value_counts(normalize=True)","111385a9":"# Verificando a quantidade de valores nulos em todo a base\ntrain.isnull().sum()","bb8441c1":"#Verificando como est\u00e3o os dados da coluna PERFORM_CNS_SCORE_DESCRIPTION\ntrain['PERFORM_CNS_SCORE_DESCRIPTION'].value_counts()","659beb3b":"#transformando em dummies\ntrain = pd.get_dummies(train, columns=['PERFORM_CNS_SCORE_DESCRIPTION'])","fe68c775":"train.info()","e461fb19":"#dividindo os data sets\ntest = train[train['LOAN_DEFAULT'].isnull()]\ntrain = train[~train['LOAN_DEFAULT'].isnull()]","871bd423":"# Criando uma base de valida\u00e7\u00e3o\ntrain, validation = train_test_split(train, test_size = 0.30, random_state = 42)","87db30c5":"# Verificando as dimens\u00f5es dos dataset\ntrain.shape, validation.shape, test.shape","7fa906ff":"# selecionar as colunas para uso no treinamento e valida\u00e7\u00e3o\n\n# lista das colunas n\u00e3o usadas\nremoved_cols = ['LOAN_DEFAULT','UNIQUEID', 'AVERAGE_ACCT_AGE','CREDIT_HISTORY_LENGTH','DISBURSAL_DATE','DATE_OF_BIRTH']\n\n# lista das features (colunas usadas no modelo)\nfeats = [c for c in train.columns if c not in removed_cols]","7f2585d9":"# Defini\u00e7\u00e3o dos hiperpar\u00e2metros dos modelos\nn_arvores = 300\noob_score = True\nn_jobs = -1\nn_cortes = 5\nmax_profundidade = 7\nrandom_state = 42\nlearning_rate = 0.1","8a58f1ba":"# Defini\u00e7\u00e3o do modelo de arvor\u00e9\nrf = RandomForestClassifier(n_estimators = n_arvores,\n                            oob_score = oob_score,\n                            n_jobs = n_jobs,\n                            min_samples_split = n_cortes,\n                            max_depth = max_profundidade,\n                            random_state = random_state)","6f0263e8":"# Realiza\u00e7\u00e3o do treino do modelo\nrf.fit(train[feats], train['LOAN_DEFAULT'])","0f2d8e32":"# Exibindo o score do out-of-bag do modelo\nrf.oob_score_","ad1dbc4d":"# Realizando as predi\u00e7\u00f5es no modelo criado a partir do dataset de valida\u00e7\u00e3o\npredictions = rf.predict(validation[feats])","5410692e":"# Exibindo o score da valida\u00e7\u00e3o\naccuracy_score(validation['LOAN_DEFAULT'], predictions)","e746b294":"# M\u00e9todo para gerar um gr\u00e1fico da matrix de confus\u00e3o\n# Fonte: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\ndef plot_confusion_matrix(y_true, y_pred, \n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           #xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","aef72f61":"# Plotando a matriz de confus\u00e3o com valores absolutos para \nplot_confusion_matrix(validation['LOAN_DEFAULT'], predictions, \n                      title='Matriz de Confus\u00e3o [Valores Absolutos], Modelo RF')\n\n# Plotando a matriz de confus\u00e3o com valores normalizados \nplot_confusion_matrix(validation['LOAN_DEFAULT'], predictions, normalize=True,\n                      title='Matriz de Confus\u00e3o [Valores Normalizados], Modelo RF')","f74e9e48":"# Defini\u00e7\u00e3o do modelo GCM\ngbm = GradientBoostingClassifier(n_estimators = n_arvores, learning_rate = learning_rate, max_depth = max_profundidade, random_state = random_state)","5f139320":"# Realiza\u00e7\u00e3o do treino do modelo\ngbm.fit(train[feats], train['LOAN_DEFAULT'])","10c2f088":"# Realizando as predi\u00e7\u00f5es no modelo criado a partir do dataset de valida\u00e7\u00e3o\npredictions = gbm.predict(validation[feats])","c78b8418":"# Exibindo o score da valida\u00e7\u00e3o\naccuracy_score(validation['LOAN_DEFAULT'], predictions)","7045017f":"# Plotando a matriz de confus\u00e3o com valores absolutos para \nplot_confusion_matrix(validation['LOAN_DEFAULT'], predictions, \n                      title='Matriz de Confus\u00e3o [Valores Absolutos], Modelo GBM')\n\n# Plotando a matriz de confus\u00e3o com valores normalizados \nplot_confusion_matrix(validation['LOAN_DEFAULT'], predictions, normalize=True,\n                      title='Matriz de Confus\u00e3o [Valores Normalizados], Modelo GBM')","2fc34642":"# Defini\u00e7\u00e3o do modelo XGBoost\nxgb = XGBClassifier(n_estimators = n_arvores, learning_rate = learning_rate, random_state = random_state)","2807472b":"# Realiza\u00e7\u00e3o do treino do modelo\nxgb.fit(train[feats], train['LOAN_DEFAULT'])","08c8aed2":"# Realizando as predi\u00e7\u00f5es no modelo criado a partir do dataset de valida\u00e7\u00e3o\npredictions = xgb.predict(validation[feats])","7f621fb4":"# Exibindo o score da valida\u00e7\u00e3o\naccuracy_score(validation['LOAN_DEFAULT'], predictions)","b3ce181c":"# Plotando a matriz de confus\u00e3o com valores absolutos para \nplot_confusion_matrix(validation['LOAN_DEFAULT'], predictions, \n                      title='Matriz de Confus\u00e3o [Valores Absolutos], Modelo XGB')\n\n# Plotando a matriz de confus\u00e3o com valores normalizados \nplot_confusion_matrix(validation['LOAN_DEFAULT'], predictions, normalize=True,\n                      title='Matriz de Confus\u00e3o [Valores Normalizados], Modelo XGB')","d594534f":"# Fun\u00e7\u00e3o para cria\u00e7\u00e3o de um DF de features importantes para o modelo\ndef create_df_feature_importance(column_names, importances):\n    df = pd.DataFrame({'feature': column_names,\n                       'feature_importance': importances}) \\\n           .sort_values('feature_importance', ascending = False) \\\n           .reset_index(drop = True)\n    return df\n\n# Fun\u00e7\u00e3o para plotar o features importance do modelo\ndef plot_feature_importance(imp_df, title):\n    imp_df.columns = ['feature', 'feature_importance']\n    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n       .set_title(title, fontsize = 20)","d4bdc788":"# Criando o DF das principais features do modelo de RandomFlorest\nbase_imp = create_df_feature_importance(feats, rf.feature_importances_)\n\n# Imprimindo as principais features utilizadas no modelo RandonFlorest\nplot_feature_importance(base_imp, 'Feature Importances')","6c07014a":"# Criando o DF das principais features do modelo GBM\nbase_imp = create_df_feature_importance(feats, gbm.feature_importances_)\n\n# Imprimindo as principais features utilizadas no modelo GBM\nplot_feature_importance(base_imp, 'Feature Importances')","f67566e1":"# Criando o DF das principais features do modelo XGB\nbase_imp = create_df_feature_importance(feats, xgb.feature_importances_)\n\n# Imprimindo as principais features utilizadas no modelo XGB\nplot_feature_importance(base_imp, 'Feature Importances')","a9ef910e":"train['LTV'].mean()\ntrain['LTV'].groupby(train['LOAN_DEFAULT']).mean()","52f2d80b":"train['DISBURSED_AMOUNT'].mean()\ntrain['DISBURSED_AMOUNT'].groupby(train['LOAN_DEFAULT']).mean()","12a8e7e9":"riscoA = train_copy[train_copy['PERFORM_CNS_SCORE_DESCRIPTION'] == 'A-Very Low Risk']\nriscoA['LOAN_DEFAULT'].value_counts(normalize=True)\n\nriscoE = train_copy[train_copy['PERFORM_CNS_SCORE_DESCRIPTION'] == 'E-Low Risk']\nriscoE['LOAN_DEFAULT'].value_counts(normalize=True)\n\nriscoH = train_copy[train_copy['PERFORM_CNS_SCORE_DESCRIPTION'] == 'H-Medium Risk']\nriscoH['LOAN_DEFAULT'].value_counts(normalize=True)\n\nriscoL = train_copy[train_copy['PERFORM_CNS_SCORE_DESCRIPTION'] == 'L-Very High Risk']\nriscoL['LOAN_DEFAULT'].value_counts(normalize=True)","9bafd7fd":"Nota-se que os modelos t\u00eam acur\u00e1cia muito pr\u00f3ximas, embora o modelo ??? tenha tido a maior acur\u00e1cia dentre os tr\u00eas. Outra informa\u00e7\u00e3o importante est\u00e1 relacionada aos falsos positivos gerados a partir do modelo. Acredita-se que a principal causa dessa anomalida seja a propor\u00e7\u00e3o desbalanceada dos valores presentes na vari\u00e1vel target. Como h\u00e1 um percentual maior de inadimplentes, logo o modelo tende a taxar a maioria dos novos valores como inadimplentes tamb\u00e9m. H\u00e1 diversas solu\u00e7\u00f5es para esse problema, a principal delas \u00e9 normalizar a base de dados de modo que a propor\u00e7\u00e3o entre as classes seja o mais igualit\u00e1rio poss\u00edvel.","c6d4fe0f":"# Importa\u00e7\u00e3o dos dados","2d0ae0e9":"De forma preliminar ao treinamento, os dados j\u00e1 tratados ser\u00e3o dividos novamente em treino e teste. Al\u00e9m disso, uma base de valida\u00e7\u00e3o, para avaliar o modelo, tamb\u00e9m ser\u00e1 gerada. ","a2103578":"As institui\u00e7\u00f5es financeiras est\u00e3o sujeitas \u00e0 perdas significativas em decorr\u00eancia de inadimpl\u00eancia nas opera\u00e7\u00f5es de financiamento de ve\u00edculos. A eleva\u00e7\u00e3o dos \u00edndices de inadimplemento resulta em maior taxa de rejei\u00e7\u00e3o de pedidos de financiamento e no aumento das taxas de juros - que encarece o produto para consumidores finais e dessestimula muitos compradores.\n\nDiante deste cen\u00e1rio, torna-se cada vez mais relevante a aplica\u00e7\u00e3o de modelos estat\u00edsticos adequados para prever os riscos envolvidos nos contratos celebrados e permitir a concess\u00e3o para clientes com menor risco de cr\u00e9dito.\n\nO presente estudo procura estimar as caracter\u00edsticas determinantes para inadimpl\u00eancia em contratos de ve\u00edculos. Para isto ser\u00e3o utilizadas caracter\u00edsticas hist\u00f3ricas de clientes e empr\u00e9stimos realizados por uma institui\u00e7\u00e3o financeira.\n\n","aff73e1e":"LTV (Loan to value of the asset) \n* \u00e9 um indicador usado para definir qual o percentual m\u00e1ximo do valor do bem que pode ser emprestado para o cliente. Conforme abaixo, vemos que em m\u00e9dia at\u00e9 74% do valor do bem \u00e9 concedido para empr\u00e9stimos para os clientes. Quando se observa os clientes com inadimpl\u00eancia, este percentual chega pr\u00f3ximo de 77%, desta forma \u00e9 recomend\u00e1vel que os empr\u00e9stimos sejam menores em rela\u00e7\u00e3o ao valor do ve\u00edculo financiado\n","eb2cf447":"RISCO DE CR\u00c9DITO\n* Este \u00e9 o risco atribu\u00eddo pela institui\u00e7\u00e3o no momento do cr\u00e9dito do ve\u00edculo\n* P\u00f3ss\u00edvel perceber inadipl\u00eancia maior em clientes com maior risco, isso sugere que a institui\u00e7\u00e3o financeira j\u00e1 det\u00e9m expertise ao atribuir risco para os clientes que pode ser melhorada atrav\u00e9s deste modelo","6a60c9cc":"Nesta fase do trabalho os dados ser\u00e3o tratados de forma a permitir a aplica\u00e7\u00e3o de algoritmos de machine learning. \n* ser\u00e3o criadas novas colunas no dataframe a partir de colunas j\u00e1 existentes (feature engineering);\n* dummeriza\u00e7\u00e3o de vari\u00e1veis; ","975e7f1b":"# Introdu\u00e7\u00e3o","cb2373a1":"A partir da observa\u00e7\u00e3o dos tipos de cada vari\u00e1vel, algumas decis\u00f5es ser\u00e3o executadas:\n- Transformar as vari\u00e1veis do tipo object em tipo inteiro ou dummy\n- Transformar as datas em tipo datetime e criar colunar para contabilizar a idade da observa\u00e7\u00e3o (em ano ou dia)\n- Popular os dados faltantes com um valor \"desconhecido\" pela coluna como, por exemplo, o um valor \"-1\" em uma coluna de inteiros n\u00e3o-negativos\n\n\nNeste trabalho, a vari\u00e1vel dependente (*target*) ser\u00e1 o campo **LOAN_DEFAULT**.","c7a96127":"# Proposta de aplica\u00e7\u00e3o do problema ao neg\u00f3cio","d134448a":"# An\u00e1lise dos dados","83b0fe77":"DISBURSED AMOUNT\n* \u00c9 o valor concedido de empr\u00e9stimo. Aqui podemos observar que os empr\u00e9stimos com inadipl\u00eancia possuem, em m\u00e9dia, valor mais altos de desembolso.","54669e0f":"Observa-se que a vari\u00e1vel target possui um volume de valores 0 (\"Inadimplente\") muito maior que valores 1 (\"Adimplente\"). Essa ser\u00e1 uma oberva\u00e7\u00e3o importante para a an\u00e1lise do modelo.","ba7cf4d3":"De acordo com o relat\u00f3rio apresentado pela fun\u00e7\u00e3o *ProfileReport()*, h\u00e1 233.154 linhas e 41 vari\u00e1veis no dataset, sendo elas 25 n\u00famericas, 6 categ\u00f3ricas, 6 do tipo boolean e 4 do tipo constante (*rejected*).\n\nObserva-se que h\u00e1 alguns *warning* registrados para algumas vari\u00e1veis do dataset que apontam alguns potenciais problemas para o modelo. S\u00e3o eles: alta cardinalidade, grande quantidade de valores zero na coluna, valores faltantes e um desequil\u00edbrio na distribui\u00e7\u00e3o dos valores da vari\u00e1veis (*y1*).\n\nEssas observa\u00e7\u00f5es ser\u00e3o tratadas ao longo da an\u00e1lise.","35a5e8d0":"# Tratamento dos dados","8bc2c2a0":"Em rela\u00e7\u00e3o aos dados obtidos pela aplica\u00e7\u00e3o do modelo de Random Forest, foi poss\u00edvel perceber que os principais fatores s\u00e3o:","0c1addd8":"# Iniciando o treinamento do modelo"}}