{"cell_type":{"d6543388":"code","e46d65df":"code","0f1d30c0":"code","5044be67":"code","f6c31eaf":"code","701bad4b":"code","c010b383":"code","ffb24c84":"code","0d75eb6a":"code","03e5a882":"code","b81cd9e3":"code","ec6384b5":"code","f76457fe":"code","4f8328fe":"code","07d4b5c1":"code","d499e003":"code","2260ea37":"code","ac30cd8f":"code","e5ef580a":"code","822bd360":"code","a4bffe40":"code","844ab813":"code","a5541ae5":"code","baf4186b":"code","22186e6b":"code","5fd11b6b":"code","01ed03a0":"code","42693615":"code","fcdde8e4":"markdown","72b4641f":"markdown","832e38d4":"markdown","956dea5e":"markdown","8f425d49":"markdown","4031f410":"markdown","01819f6d":"markdown","5c27bcb0":"markdown","eac50e25":"markdown","f5c7b8a6":"markdown","8a4f1764":"markdown","645457ad":"markdown"},"source":{"d6543388":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e46d65df":"# Import datasets\ntrain = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv', index_col='id')","0f1d30c0":"train.describe()","5044be67":"# Missing values count\nmissing_train  = train.isna().sum().sum() + train.isnull().sum().sum()\nmissing_test = test.isna().sum().sum() + test.isnull().sum().sum()\n\nprint('Missing values - train data: {}'.format(missing_train))\nprint('Missing values - test data: {}'.format(missing_test))","f6c31eaf":"# Split data into X and y\nfrom sklearn.model_selection import train_test_split\n\ny = train['target']\nX = train.drop(['target'], axis = 1)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","701bad4b":"from sklearn import preprocessing\n# Numerical columns\nnumeric_features = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n\nscaler = preprocessing.StandardScaler()\n\nscaled_X_train = scaler.fit_transform(X_train[numeric_features])\nscaled_X_valid = scaler.transform(X_valid[numeric_features])\nscaled_X_test = scaler.transform(test[numeric_features])\n\nscaled_X_train = pd.DataFrame(scaled_X_train, columns = X_train[numeric_features].columns, index = X_train.index)\nscaled_X_valid = pd.DataFrame(scaled_X_valid, columns = X_train[numeric_features].columns, index = X_valid.index)\nscaled_X_test = pd.DataFrame(scaled_X_test, columns = X_train[numeric_features].columns, index = test.index)","c010b383":"# Identify object type columns\nobject_cols = [col for col in X.columns if X[col].dtype == 'object']\n\n# Check if all the values of categorical columns of the validation set are present in the training set\n# All the categorical columns\ngood_label_cols = [col for col in object_cols if set(X_valid[col]).issubset(set(X_train[col]))]\n\n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\nif good_label_cols == object_cols:\n    print('All object columns are good!')\nelse:\n    print('These are the bad object columns: ', bad_label_cols)","ffb24c84":"# Ordinal Encoding of categorical variables\nfrom sklearn.preprocessing import OrdinalEncoder\nenc = OrdinalEncoder()\n\n# Drop categorical columns that will not be encoded\nOE_X_train = X_train.drop(bad_label_cols + numeric_features, axis=1)\nOE_X_valid = X_valid.drop(bad_label_cols + numeric_features, axis=1)\n\nOE_X_columns = OE_X_train.columns\n\n# Apply ordinal encoder \nOE_X_train = enc.fit_transform(X_train[good_label_cols])\nOE_X_valid[good_label_cols] = enc.transform(X_valid[good_label_cols])\n\n\nOE_X_train = pd.DataFrame(OE_X_train, columns = OE_X_columns, index = X_train.index)\nOE_X_valid = pd.DataFrame(OE_X_valid, columns = OE_X_columns, index = X_valid.index)","0d75eb6a":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","03e5a882":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse = False)\n\nOH_X_train = X_train[low_cardinality_cols]\nOH_X_valid = X_valid[low_cardinality_cols]\nOH_X_test = test[low_cardinality_cols]\n\nOH_X_train = enc.fit_transform(X_train[low_cardinality_cols]) \nOH_X_valid = enc.transform(X_valid[low_cardinality_cols])\nOH_X_test = enc.transform(test[low_cardinality_cols])\n\nOH_X_train = pd.DataFrame(OH_X_train, index = X_train.index)\nOH_X_valid = pd.DataFrame(OH_X_valid, index = X_valid.index)\nOH_X_test = pd.DataFrame(OH_X_test, index = test.index)","b81cd9e3":"final_OE_X_train_scaled = pd.concat([scaled_X_train, OE_X_train], axis = 1) \nfinal_OE_X_valid_scaled = pd.concat([scaled_X_valid, OE_X_valid], axis = 1) \n\nfinal_OH_X_train_scaled = pd.concat([scaled_X_train, OH_X_train], axis = 1)\nfinal_OH_X_valid_scaled = pd.concat([scaled_X_valid, OH_X_valid], axis = 1)\nfinal_OH_X_test_scaled = pd.concat([scaled_X_test, OH_X_test], axis = 1)","ec6384b5":"final_OE_X_train_noscale = pd.concat([X_train[numeric_features], OE_X_train], axis = 1) \nfinal_OE_X_valid_noscale = pd.concat([X_valid[numeric_features], OE_X_valid], axis = 1) \n\nfinal_OH_X_train_noscale = pd.concat([X_train[numeric_features], OH_X_train], axis = 1)\nfinal_OH_X_valid_noscale = pd.concat([X_valid[numeric_features], OH_X_valid], axis = 1)","f76457fe":"final_OH_X_train_scaled.head()","4f8328fe":"# Random Forest initial model\nfrom sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(n_estimators = 100, random_state = 1)\n\n# XGB initial model\nfrom xgboost import XGBRegressor\nxgb_model = XGBRegressor(random_state = 0)\n\n# SGD regressor\nfrom sklearn.linear_model import SGDRegressor\nsgd_model = SGDRegressor(loss = 'squared_loss', max_iter = 10000)\n\n# Elastic net\nfrom sklearn.linear_model import ElasticNet\nenet_model = ElasticNet()","07d4b5c1":"# Train Random Forest models\nfrom sklearn.metrics import mean_squared_error\n\n# Function to choose ordinal or one-hot encoding\n# Scaled vs non-scaled numerical features\n# Train, predict and calculate RMSE\ndef rf_function(cat_encoding, scaled):\n    if cat_encoding == 'ordinal' and scaled == True:\n        rf_model.fit(final_OE_X_train_scaled, y_train)\n        y_pred_OE_rf = rf_model.predict(final_OE_X_valid_scaled)\n        return mean_squared_error(y_valid, y_pred_OE_rf, squared = False)\n    elif cat_encoding == 'ordinal' and scaled == False:\n        rf_model.fit(final_OE_X_train_noscale, y_train)\n        y_pred_OE_rf = rf_model.predict(final_OE_X_valid_noscale)\n        return mean_squared_error(y_valid, y_pred_OE_rf, squared = False)\n    elif cat_encoding == 'onehot'and scaled == True:\n        rf_model.fit(final_OH_X_train_scaled, y_train)\n        y_pred_OH_rf = rf_model.predict(final_OH_X_valid_scaled)\n        return mean_squared_error(y_valid, y_pred_OH_rf, squared = False)\n    elif cat_encoding == 'onehot'and scaled == False:\n        rf_model.fit(final_OH_X_train_noscale, y_train)\n        y_pred_OH_rf = rf_model.predict(final_OH_X_valid_noscale)\n        return mean_squared_error(y_valid, y_pred_OH_rf, squared = False)\n\n# Train XGB models    \ndef xgb_function(cat_encoding, scaled):\n    if cat_encoding == 'ordinal' and scaled == True:\n        xgb_model.fit(final_OE_X_train_scaled, y_train)\n        y_pred_OE_rf = xgb_model.predict(final_OE_X_valid_scaled)\n        return mean_squared_error(y_valid, y_pred_OE_rf, squared = False)\n    if cat_encoding == 'ordinal' and scaled == False:\n        xgb_model.fit(final_OE_X_train_noscale, y_train)\n        y_pred_OE_rf = xgb_model.predict(final_OE_X_valid_noscale)\n        return mean_squared_error(y_valid, y_pred_OE_rf, squared = False)\n    elif cat_encoding == 'onehot' and scaled == True:\n        xgb_model.fit(final_OH_X_train_scaled, y_train)\n        y_pred_OH_rf = xgb_model.predict(final_OH_X_valid_scaled)\n        return mean_squared_error(y_valid, y_pred_OH_rf, squared = False)\n    elif cat_encoding == 'onehot' and scaled == False:\n        xgb_model.fit(final_OH_X_train_noscale, y_train)\n        y_pred_OH_rf = xgb_model.predict(final_OH_X_valid_noscale)\n        return mean_squared_error(y_valid, y_pred_OH_rf, squared = False)","d499e003":"# RMSE random forest ordinal encoding\n\nOE_rf_rmse = rf_function('ordinal', scaled = True)\nprint('MSE for Random Forest numerical scaled features and ordinal encoding is: {}'.format(OE_rf_rmse))\n\nOE_rf_rmse = rf_function('ordinal', scaled = False)\nprint('MSE for Random Forest numerical non-scaled features and ordinal encoding is: {}'.format(OE_rf_rmse))","2260ea37":"# RMSE random forest one-hot encoding\n\nOH_rf_rmse = rf_function('onehot', scaled = True)\nprint('MSE for Random Forest numerical scaled one-hot encoding is: {}'.format(OH_rf_rmse))\n\nOH_rf_rmse = rf_function('onehot', scaled = False)\nprint('MSE for Random Forest numerical non-scaled one-hot encoding is: {}'.format(OH_rf_rmse))","ac30cd8f":"# RMSE XGB ordinal encoding\n\nOE_xgb_rmse = xgb_function('ordinal', scaled = True)\nprint('MSE for XGB scaled ordinal encoding is: {}'.format(OE_xgb_rmse))\n\nOE_xgb_rmse = xgb_function('ordinal', scaled = False)\nprint('MSE for XGB non-scaled ordinal encoding is: {}'.format(OE_xgb_rmse))","e5ef580a":"# RMSE XGB one-hot encoding\n\nOH_xgb_mse = xgb_function('onehot', scaled = True)\nprint('MSE for XGB scaled one-hot encoding is: {}'.format(OH_xgb_mse))\n\nOH_xgb_mse = xgb_function('onehot', scaled = False)\nprint('MSE for XGB non-scaled one-hot encoding is: {}'.format(OH_xgb_mse))","822bd360":"# Cross validation of RMSE XGB model\nfrom sklearn.model_selection import cross_val_score\n\nxgb_model.fit(final_OH_X_train_scaled, y_train)\nscores = -1 * cross_val_score(xgb_model, final_OH_X_train_scaled, y_train,\n                              cv=5,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"Average RMSE score:\", scores.mean())","a4bffe40":"# Cross validation of SGD model\nsgd_model.fit(final_OH_X_train_scaled, y_train)\nscores = -1 * cross_val_score(sgd_model, final_OH_X_train_scaled, y_train,\n                              cv=5,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"Average RMSE score:\", scores.mean())","844ab813":"# Cross validation of RMSE ElasticNet model\nenet_model.fit(final_OH_X_train_scaled, y_train)\nscores = -1 * cross_val_score(enet_model, final_OH_X_train_scaled, y_train,\n                              cv=5,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"Average RMSE score:\", scores.mean())","a5541ae5":"# Let's try to improve the result of XGB with OH encoding and scaled numeric features\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # trying learning rates from 0.01 to 0.2\n    # and max depth from 4 to 10\n    {'eta': [0.01, 0.05, 0.1, 0.2], 'max_depth': [4, 6, 8, 10]}   \n  ]\n\nxgb_model = XGBRegressor(random_state = 0)\n\ngrid_search = GridSearchCV(xgb_model, param_grid, cv=5,\n                           scoring='neg_root_mean_squared_error',\n                           return_train_score = True)\n\ngrid_search.fit(final_OH_X_valid_scaled, y_valid)","baf4186b":"# Best set of parameters found with grid search\ngrid_search.best_params_","22186e6b":"# Train final model with best parameters\nfinal_model = XGBRegressor(learning_rate = 0.2, max_depth = 4)\nfinal_model.fit(final_OH_X_train_scaled, y_train)\n\nscores = -1 * cross_val_score(final_model, final_OH_X_train_scaled, y_train,\n                              cv=5,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"Average RMSE score:\", scores.mean())","5fd11b6b":"# Check RMSE in validation data\nscores = -1 * cross_val_score(final_model, final_OH_X_valid_scaled, y_valid,\n                              cv=5,\n                              scoring='neg_root_mean_squared_error')\n\nprint(\"Average RMSE score:\", scores.mean())","01ed03a0":"predictions = final_model.predict(final_OH_X_test_scaled)\npredictions","42693615":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","fcdde8e4":"**One Hot Encoding**","72b4641f":"**Training XGB model with best parameters**","832e38d4":"**Ordinal Encoding**","956dea5e":"Scaled numeric features","8f425d49":"**Make predictions on test dataset**","4031f410":"**Scaling numerical features**","01819f6d":"**Cross validation**","5c27bcb0":"Non-scaled numeric features","eac50e25":"**Models**","f5c7b8a6":"**Grid search for parameter tuning**","8a4f1764":"**Concatenate datasets**","645457ad":"**Categorical features**"}}