{"cell_type":{"23751cd9":"code","ae1bf920":"code","fda453af":"code","fcb34174":"code","afc62aa3":"code","c10f36f1":"code","b12c6131":"code","4be7f12d":"code","db1e537f":"code","e084cb6f":"code","32fce1bf":"code","32598779":"code","9436c4b7":"code","06a7b271":"code","a1eda8e2":"code","dd54fe37":"code","83c21cfd":"code","eadeb360":"code","47cee5f3":"code","c537b66b":"code","8af955d2":"code","7a8fcb1c":"code","832ed5bb":"code","f31a4c85":"code","31b8ac0d":"markdown","84056958":"markdown","a95385d4":"markdown","fb6a4e12":"markdown"},"source":{"23751cd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom plotnine import *\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import is_string_dtype\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","ae1bf920":"training_data = '..\/input\/adult-training.csv'\ntest_data = '..\/input\/adult-test.csv'\ncolumns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\\\n           'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n\ndf_train_set = pd.read_csv(training_data, names = columns)\ndf_test_set = pd.read_csv(test_data, names = columns)\n\n#fnlwgt\u5217\u65e0\u5b9e\u9645\u610f\u4e49\uff0c\u4e0d\u91cd\u8981\u53ef\u53bb\u9664\ndf_train_set.drop('fnlwgt', axis = 1, inplace=True)\nprint('Training data shape: ', df_train_set.shape)\nprint('Tesing data shape: ', df_test_set.shape)\ndf_train_set.head()\nall_data = [df_train_set, df_test_set]\n","fda453af":"#\u67e5\u770b\u7f3a\u5931\u503c\ndef missing_values_table(df):\n    # Total missing values\n        mis_val = df.isnull().sum()\n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\nprint(\"df_train_set\")\nmissing_values_table(df_train_set)\nprint(\"df_test_set\")\nmissing_values_table(df_test_set)\n","fcb34174":"df_test_set","afc62aa3":"#\u901a\u8fc7\u67e5\u770b\u6570\u636e\uff0c\u53d1\u73b0\u5b58\u5728\u5f02\u5e38\u6570\u636e\u5982\uff1f,\u5c06?\u66ff\u6362\u4e3aNan\u6216Unknown\ndf_test_set.drop(df_test_set.index[0])\ndf_train_set.replace(\" ?\", np.nan, inplace=True)\ndf_test_set.replace(\" ?\", np.nan, inplace=True)\ndf_train_set.dropna(inplace=True)\ndf_test_set.dropna(inplace=True)\n","c10f36f1":"df_train_set.income_bracket.value_counts()","b12c6131":"df_test_set.income_bracket.value_counts()","4be7f12d":"all_data = [df_train_set, df_test_set]\nfor data in all_data:\n    data['target']=data['income_bracket'].apply(lambda x: x.replace('.', ''))\n    data['target']=data['target'].apply(lambda x: x.strip())\n    data['target'] = data['target'].apply(lambda x: 1 if x=='>50K' else 0)\n    data.drop(['income_bracket'], axis=1, inplace=True)\ndf_train_set.target.value_counts()","db1e537f":"df_train_set.dtypes.value_counts()","e084cb6f":"df_train_set.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","32fce1bf":"df_train_set.drop('native_country', axis=1, inplace=True)\ndf_test_set.drop('native_country', axis=1, inplace=True)\ndf_train_set.drop('education', axis=1, inplace=True)\ndf_test_set.drop('education', axis=1, inplace=True)","32598779":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in df_train_set:\n    if df_train_set[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(df_train_set[col].unique())) <= 2:\n            print(col + \" were label encoded\")\n            # Train on the training data\n            le.fit(df_train_set[col])\n            # Transform both training and testing data\n            df_train_set[col] = le.transform(df_train_set[col])\n            df_test_set[col] = le.transform(df_test_set[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","9436c4b7":"# one-hot encoding of categorical variables\ndf_train_set = pd.get_dummies(df_train_set)\ndf_test_set = pd.get_dummies(df_test_set)\nprint('Training Features shape: ', df_train_set.shape)\nprint('Testing Features shape: ', df_test_set.shape)","06a7b271":"df_train_set, df_test_set = df_train_set.align(df_test_set, join = 'inner', axis = 1)\nprint('Training Features shape: ', df_train_set.shape)\nprint('Testing Features shape: ', df_test_set.shape)","a1eda8e2":"df_train_set.columns","dd54fe37":"cols = list(df_train_set.columns)\ncols.remove(\"target\")\n\nx_train, y_train = df_train_set[cols].values, df_train_set[\"target\"].values\nx_test, y_test = df_test_set[cols].values, df_test_set[\"target\"].values\n","83c21cfd":"# \u91c7\u7528\u51b3\u7b56\u6811\u7b97\u6cd5\ntreeClassifier = DecisionTreeClassifier()\ntreeClassifier.fit(x_train, y_train)\ntreeClassifier.score(x_test, y_test)","eadeb360":"import itertools\nfrom sklearn.metrics import confusion_matrix\n# \u6df7\u6dc6\u77e9\u9635\ndef plot_confusion_matrix(cm, classes, normalize=False):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    cmap = plt.cm.Blues\n    title = \"Confusion Matrix\" \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=3)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","47cee5f3":"# \u51b3\u7b56\u6811\u7b97\u6cd5\u8bc4\u4f30\ny_pred = treeClassifier.predict(x_test)\ncfm = confusion_matrix(y_test, y_pred, labels=[0, 1])\nplt.figure(figsize=(10,6))\nplot_confusion_matrix(cfm, classes=[\"<=50K\", \">50K\"], normalize=True)","c537b66b":"# \u91c7\u7528\u968f\u673a\u68ee\u6797\u7b97\u6cd5\nrclf = RandomForestClassifier(n_estimators=500)\nrclf.fit(x_train, y_train)\nrclf.score(x_test, y_test)","8af955d2":"# \u968f\u673a\u68ee\u6797\u7b97\u6cd5\u8bc4\u4f30\ny_pred = rclf.predict(x_test)\ncfm = confusion_matrix(y_test, y_pred, labels=[0, 1])\nplt.figure(figsize=(10,6))\nplot_confusion_matrix(cfm, classes=[\"<=50K\", \">50K\"], normalize=True)","7a8fcb1c":"# \u7279\u5f81\u91cd\u8981\u6027\nimportances = rclf.feature_importances_\nindices = np.argsort(importances)\ncols = [cols[x] for x in indices]\nplt.figure(figsize=(10,20))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), cols)\nplt.xlabel('Relative Importance')","832ed5bb":"\nparameters = {\n    'n_estimators':(100, 500, 1000),\n    'max_depth':(None, 24, 16),\n    'min_samples_split': (2, 4, 8),\n    'min_samples_leaf': (16, 4, 12)\n}\n\nclf = GridSearchCV(RandomForestClassifier(), parameters, cv=5, n_jobs=8)\nclf.fit(x_train, y_train)\nclf.best_score_, clf.best_params_","f31a4c85":"rclf2 = RandomForestClassifier(n_estimators=1000,max_depth=24,min_samples_leaf=4,min_samples_split=8)\nrclf2.fit(x_train, y_train)\n\ny_pred = rclf2.predict(x_test)\ncfm = confusion_matrix(y_test, y_pred, labels=[0, 1])\nplt.figure(figsize=(10,6))\nplot_confusion_matrix(cfm, classes=[\"<=50K\", \">50K\"], normalize=True)","31b8ac0d":"# \u7b97\u6cd5\u6a21\u578b","84056958":"# \u6570\u636e\u9884\u5904\u7406","a95385d4":"# \u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8fdb\u884c\u4f18\u5316","fb6a4e12":"# \u8bfb\u53d6\u6570\u636e"}}