{"cell_type":{"2f04d23a":"code","c8fbbaf7":"code","d06c9d15":"code","f0521967":"code","d15503a2":"code","47bcea89":"code","23d110ea":"code","f530866a":"code","d43e40de":"code","6f091b8e":"code","f8c480b8":"code","de8a58af":"code","70a9ab00":"code","0d3b5e2e":"code","51f89176":"code","be1b7611":"code","d1ff09e5":"code","41eb4272":"code","e7a2ff3d":"code","8ee5b041":"code","4207b61c":"code","e0afae5e":"code","5fe90554":"code","80816760":"code","b7be5e98":"code","59eb8fe3":"code","e6da883b":"code","f9f8368c":"code","61398b99":"code","90cacf9e":"code","7fe2e59b":"code","34831730":"code","ff5d5e3c":"markdown","1b32d5d0":"markdown","476a2d71":"markdown","b4887627":"markdown","d2755c04":"markdown","ad760309":"markdown","5c3bc625":"markdown","4feeee39":"markdown","dc80130f":"markdown","f68f96d3":"markdown","d52760e9":"markdown","cbe62f13":"markdown","c384366e":"markdown","b67163fc":"markdown","53e12101":"markdown","7229afbd":"markdown"},"source":{"2f04d23a":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c8fbbaf7":"df = pd.read_csv('..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv',delimiter=',',encoding='latin-1')\ndf = df[['Category','Message']]\ndf = df[pd.notnull(df['Message'])]\ndf.rename(columns = {'Message':'Message'}, inplace = True)\ndf.head()","d06c9d15":"df.shape","f0521967":"df.index = range(5572)\ndf['Message'].apply(lambda x: len(x.split(' '))).sum()","d15503a2":"cnt_pro = df['Category'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","47bcea89":"def print_message(index):\n    example = df[df.index == index][['Message', 'Category']].values[0]\n    if len(example) > 0:\n        print(example[0])\n        print('Message:', example[1])\nprint_message(12)","23d110ea":"print_message(0)","f530866a":"from bs4 import BeautifulSoup\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\ndf['Message'] = df['Message'].apply(cleanText)","d43e40de":"df['Message'] = df['Message'].apply(cleanText)\ntrain, test = train_test_split(df, test_size=0.000001 , random_state=42)\nimport nltk\nfrom nltk.corpus import stopwords\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            #if len(word) < 0:\n            if len(word) <= 0:\n                continue\n            tokens.append(word.lower())\n    return tokens\ntrain_tagged = train.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.Category]), axis=1)\ntest_tagged = test.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.Category]), axis=1)\n\n# The maximum number of words to be used. (most frequent)\nmax_fatures = 500000\n\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 50\n\n#tokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['Message'].values)\nX = tokenizer.texts_to_sequences(df['Message'].values)\nX = pad_sequences(X)\nprint('Found %s unique tokens.' % len(X))","6f091b8e":"X = tokenizer.texts_to_sequences(df['Message'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","f8c480b8":"#train_tagged.values[2173]\ntrain_tagged.values","de8a58af":"d2v_model = Doc2Vec(dm=1, dm_mean=1, size=20, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\nd2v_model.build_vocab([x for x in tqdm(train_tagged.values)])\n","70a9ab00":"%%time\nfor epoch in range(30):\n    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    d2v_model.alpha -= 0.002\n    d2v_model.min_alpha = d2v_model.alpha","0d3b5e2e":"print(d2v_model)\n","51f89176":"len(d2v_model.wv.vocab)\n","be1b7611":"# save the vectors in a new matrix\nembedding_matrix = np.zeros((len(d2v_model.wv.vocab)+ 1, 20))\n\nfor i, vec in enumerate(d2v_model.docvecs.vectors_docs):\n    while i in vec <= 1000:\n    #print(i)\n    #print(model.docvecs)\n          embedding_matrix[i]=vec\n    #print(vec)\n    #print(vec[i])","d1ff09e5":"d2v_model.wv.most_similar(positive=['urgent'], topn=10)\n","41eb4272":"d2v_model.wv.most_similar(positive=['cherish'], topn=10)\n","e7a2ff3d":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding\n\n\n# init layer\nmodel = Sequential()\n\n# emmbed word vectors\nmodel.add(Embedding(len(d2v_model.wv.vocab)+1,20,input_length=X.shape[1],weights=[embedding_matrix],trainable=True))\n\n# learn the correlations\ndef split_input(sequence):\n     return sequence[:-1], tf.reshape(sequence[1:], (-1,1))\nmodel.add(LSTM(50,return_sequences=False))\nmodel.add(Dense(2,activation=\"softmax\"))\n\n# output model skeleton\nmodel.summary()\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])","8ee5b041":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","4207b61c":"Y = pd.get_dummies(df['Category']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","e0afae5e":"batch_size = 32\nhistory=model.fit(X_train, Y_train, epochs =50, batch_size=batch_size, verbose = 2)","5fe90554":"plt.plot(history.history['acc'])\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_accuracy.png')\n\n# summarize history for loss\nplt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_loss.png')","80816760":"# evaluate the model\n_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\nprint('Train: %.3f, Test: %.4f' % (train_acc, test_acc))","b7be5e98":"# predict probabilities for test set\nyhat_probs = model.predict(X_test, verbose=0)\nprint(yhat_probs)\n# predict crisp classes for test set\nyhat_classes = model.predict_classes(X_test, verbose=0)\nprint(yhat_classes)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\n#yhat_classes = yhat_classes[:, 1","59eb8fe3":"import numpy as np\nrounded_labels=np.argmax(Y_test, axis=1)\nrounded_labels","e6da883b":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(rounded_labels, yhat_classes)\ncm","f9f8368c":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlstm_val = confusion_matrix(rounded_labels, yhat_classes)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(lstm_val, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('LSTM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","61398b99":"validation_size = 200\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","90cacf9e":"\nmodel.save('Mymodel.h5')","7fe2e59b":"message = ['Congratulations! you have won a $1,000 Walmart gift card. Go to http:\/\/bit.ly\/123456 to claim now.']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['ham','spam']\nprint(pred, labels[np.argmax(pred)])","34831730":"message = ['thanks for accepting my request to connect']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['ham','spam']\nprint(pred, labels[np.argmax(pred)])","ff5d5e3c":"VISUALIZING THE DATA","1b32d5d0":"![https:\/\/miro.medium.com\/max\/9000\/1*h0mO4PdZaQKtbwWJW40FKQ.jpeg](https:\/\/miro.medium.com\/max\/9000\/1*h0mO4PdZaQKtbwWJW40FKQ.jpeg)","476a2d71":"# This Work\n\nThis work tries to detect spam or ham using LSTM\n\n\n# What is LSTM ?\n\nLong Short Term Memory (LSTM) is a special kind of Recurrent Neural Network (RNN), capable of learning long-term dependencies. These long-term dependencies have a great influence on the meaning and overall polarity of a document. Long short-term memory networks (LSTM) address this long-term dependency problem by introducing a memory into the network. It was first introduced by **Hochreiter & Schmidhuber**. The LSTM architecture has a range of repeated modules for each time step as in a standard RNN. At each time step, the output of the module is controlled by a set of gates, as a function of the old hidden state \u210e\ud835\udc61\u22121 and the input at the current time step \ud835\udc65\ud835\udc61 : the forget gate \ud835\udc53\ud835\udc61, the input gate \ud835\udc56\ud835\udc61 , and the output gate \ud835\udc42\ud835\udc61 . These gates collectively decide how to update the current memory cell \ud835\udc36\ud835\udc61 and the current hidden state \u210e\ud835\udc61 . The LSTM transition functions are defined as follows:\n\n\ud835\udc56\ud835\udc61=(\ud835\udc4a\ud835\udc56[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc56) \n\n\ud835\udc36\u00b4\ud835\udc61=\ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udc4a\ud835\udc50[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc36) \n\n\ud835\udc53\ud835\udc61=(\ud835\udc4a\ud835\udc53[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc53) \n\n\ud835\udc42\ud835\udc61=(\ud835\udc4a\ud835\udc5c[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc5c) \n\n\ud835\udc36\ud835\udc61= \ud835\udc53\ud835\udc61\u2217\ud835\udc36\ud835\udc61\u22121+\ud835\udc56\ud835\udc61\u2217\ud835\udc36\u00b4\ud835\udc61\n\nHere \ud835\udf0e is logistic sigmoid function that has an output in \ud835\udc5b [0,1] ,tanh denotes the hyperbolic tangent function that has an output \u210e \ud835\udc56\ud835\udc5b [\u22121,1], and \u2217 denotes the pointwise multiplication.\n\n\nAs I mentioned before to deal with Deep learning we have to map sentence to vector of number. In this work inspired by [3] Doc2vec as the embedding used for extracting information context. The Doc2vec is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts such as sentences, paragraphs, and documents.\n","b4887627":"# NLP Techniques\n\nThe earliest approach for solving NLP task involves rule-based approachers, which dominated the indusytry for decades. Examples of techniques using rule based approaches include Regular Expressions (RegExs) and Context Free Grammars (CFGs). RegExs are sometimes used in order to remove HTML tags from text that has been scraped from a web page or uwanted special characters from a document.\n\n\nThe second approach involved training a machine learning model with some data that is based on some user defined features. This technique requires a considerable amount of features engineering ( a nontrivial task), and includes analyzing the text to remove undersired and superfluous content(including stop words), as well as transforming the word (e.g., converting upercase to lowercase).\n\n\nThe most recent approach involves deep learning, whereby a neural network learns the features instead of relying on human to perform feature engineering. One of the key ideas involves mapping words to numbers, which enables us to map sentence to vector of number. After transforming documents to vector, we can perform a myriad of operations on those vector.  For example we use the notion of vector space to define vector space model, where the distance between two vector can be measured  by the angle between them (related to cosine similarity). If two vector are closed to each other, then it's likelier that the coresponding sentence are similar meaning. Their similarity is based on the distributional hypothesis, which asserts that words in the same contexts tent to have similar meaning.The NLP models that use deep learning can comprise CNNs, RNNs, LSTMs, and bidirectional LSTMs.\n","d2755c04":"# Example Cases : Text Classification ","ad760309":"Text Preprocessing Below we define a function to convert text to lower-case and strip punctuation\/symbols from words and so on.","5c3bc625":"**Test with new and different data set aside from the data to build the model.**","4feeee39":"Model DM = 1\n\nThis work use DM=1 (it preserve word order)","dc80130f":"# NLP\n\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n\nNLP is currently the focus of significant interest in the machine learning community. Some of the use cases for NLP are listed here:\n* Chatbots\n* Search(text and Audio)\n* Text Classification\n* Sentiment Analysis\n* Recomendation System\n* Quesstion Answering\n* Speech recognation\n* NLU (Natural Language Understanding)\n* NLG ( Natural Language Generation)\n\nYou encounter many of these use cases in everyday life : when you visit web pages or perform an online search for books, or recommendation regarding movies.\n\n\n","f68f96d3":"# Measuring distance between two vectors (related to cosine similarity)","d52760e9":"**Save Model**\n","cbe62f13":"References\n* [1] Artificial Inteligence, Machine Learning and Deep Learning by Mercury Learning and Informarion (Book)\n* [2] Source Information of NLP :https:\/\/monkeylearn.com\/what-is-text-classification\/\n* [3] Parameter doc2vec and  LSTM inspired  by : https:\/\/dl.acm.org\/doi\/10.1145\/3406601.3406624","c384366e":"# Practical Example","b67163fc":"Read Dataset","53e12101":"# Create the LSTM Model\n\nThe parameter used here inspired by [3].\n\n","7229afbd":"**What is Text classification ?**\n\nText classification is the process of categorizing text into organized groups. By using Natural Language Processing (NLP), text classifiers can automatically analyze text and then assign a set of pre-defined tags or categories based on its content. Text classifiers with NLP have proven to be a great alternative to structure textual data in a fast, cost-effective, and scalable way.Text classification is becoming an increasingly important part of businesses as it allows to easily get insights from data and automate business processes. Some of the most common examples and use cases for automatic text classification include the following:\n\n* Sentiment Analysis: the process of understanding if a given text is talking positively or negatively about a given subject (e.g. for brand monitoring purposes).\n\n* Spam Detection :  detect unsolicited, unwanted, and virus-infested email (called spam) and stop it from getting into email inboxes. Internet Service Providers (ISPs) use spam filters to make sure they aren't distributing spam.\n\n* Topic Detection: the task of identifying the theme or topic of a piece of text (e.g. know if a product review is about Ease of Use, Customer Support, or Pricing when analyzing customer feedback).\n\n* Language Detection: the procedure of detecting the language of a given text (e.g. know if an incoming support ticket is written in English or Spanish for automatically routing tickets to the appropriate team).\n\n\n"}}