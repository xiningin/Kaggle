{"cell_type":{"8baf4107":"code","892c6870":"code","7755826b":"code","42d0c0a2":"code","56734462":"code","167beb1b":"code","33e37a64":"code","813cddbd":"code","e8fe5b9f":"code","1bf90fa2":"code","804834ee":"code","8843db53":"code","5e3bb423":"code","7beab0fc":"code","45942998":"code","9b81730c":"code","3533cc41":"code","56f1399d":"code","034162e8":"code","cf0dd3d8":"code","6ea88bdb":"code","9e8f0194":"code","4932b467":"code","7128bab0":"code","204052de":"code","447546e9":"code","e8735e06":"code","7c5e0bde":"code","c80fa8f1":"code","b54996fa":"code","10b8dd29":"code","d20fb3fc":"code","8d61c028":"code","d3f37b92":"code","5e554979":"code","c60805e7":"code","5e83f7e6":"code","c777df25":"code","79c9a34c":"code","4875eea7":"code","573b757b":"code","0857013c":"code","0fa1e566":"code","ae2158a6":"code","c6d7f00a":"code","7499b715":"code","604c52b6":"code","3a084cf6":"code","eb1ac8a3":"code","bdbf2bd1":"code","97b851f3":"code","15fae104":"code","502124ea":"code","c29c9d4e":"code","68d62494":"code","9669b7a0":"code","adab84ba":"code","2b8d3765":"code","3cb4aced":"code","9734c757":"code","000aa1cf":"code","083b294d":"code","ef540aa3":"code","8f6c98ca":"code","37a8c67b":"code","143639c4":"code","9945da1e":"code","9e0164d7":"code","665bc01d":"code","254a47fa":"code","5a4b449b":"code","4ca66b5c":"code","6c84c138":"code","2dd11ba9":"code","2cf70ba7":"code","5bc41a14":"code","9023fec3":"code","c849136b":"code","6a6c074d":"code","1ab5228c":"code","3717a470":"code","9e82e411":"code","25d0a6d4":"code","2fd8af72":"code","30a366b9":"code","c0fad90c":"code","f6ce2077":"code","0749d2b4":"code","279eaa71":"code","a1c2c4c4":"code","0da287ac":"code","7609f01e":"code","a12ff9b9":"markdown","1a2a46c7":"markdown","810be323":"markdown","0f98fa4c":"markdown","92226a7d":"markdown","59f2eabb":"markdown","161a1bb7":"markdown","4d98eba1":"markdown","38cea4ae":"markdown","73227505":"markdown","7b2b6a0d":"markdown","f8e39d94":"markdown","61d4f765":"markdown","b2876435":"markdown","2a09a1b4":"markdown","ce8ebe73":"markdown","d4817a12":"markdown","49007e6a":"markdown","23351c8c":"markdown","844fe49a":"markdown","33730b03":"markdown","c3597b0c":"markdown","02b29d36":"markdown","3533a096":"markdown","865fdbf4":"markdown","06802784":"markdown","aa1662d8":"markdown","326490ff":"markdown","253ec73a":"markdown","4194926a":"markdown","2ae97bdd":"markdown"},"source":{"8baf4107":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","892c6870":"from sklearn.datasets import load_iris\niris=load_iris()","7755826b":"print(iris.feature_names)","42d0c0a2":"print(iris.target_names)","56734462":"X=iris.data","167beb1b":"y=iris.target","33e37a64":"print(y)","813cddbd":"y_df=pd.DataFrame(y)\ny_df.head()","e8fe5b9f":"y_df.columns=['species_index']","1bf90fa2":"y_df","804834ee":"X.shape","8843db53":"y.shape","5e3bb423":"data=pd.DataFrame(X,columns=iris.feature_names)\nn_data=pd.DataFrame(X,columns=['sepal_len','sepal_wid','petal_len','petal_wid'])","7beab0fc":"n_data.head()","45942998":"y_df.loc[:,:]","9b81730c":"n_data['species_index']=y_df.loc[:,:]","3533cc41":"n_data.head()","56f1399d":"data.head()","034162e8":"data.columns=data.columns.str.replace(' ','_')","cf0dd3d8":"data.head()","6ea88bdb":"data.shape","9e8f0194":"sns.pairplot?","4932b467":"sns.pairplot(n_data,size=3.5,vars=['sepal_len','sepal_wid','petal_len','petal_wid'],hue='species_index')","7128bab0":"sns.pairplot(n_data,x_vars=['sepal_len','sepal_wid'],y_vars=['petal_len','petal_wid'],hue='species_index',size=4)","204052de":"from sklearn.model_selection import train_test_split","447546e9":"data_train,data_test,y_train,y_test=train_test_split(data,y,test_size=0.25,random_state=5)","e8735e06":"print(data_train.shape)\nprint(data_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7c5e0bde":"from sklearn.neighbors import KNeighborsClassifier","c80fa8f1":"knn=KNeighborsClassifier(n_neighbors=5)","b54996fa":"knn.fit(data_train,y_train)","10b8dd29":"knn.predict([[5.1,4.8,2.4,3.5]])","d20fb3fc":"knn.predict_proba([[5.1,4.8,2.4,3.5]])","8d61c028":"#predicting on x_test \ny_pred=knn.predict(data_test)","d3f37b92":"print(y_pred)\nprint(y_test)","5e554979":"from sklearn import metrics","c60805e7":"metrics.accuracy_score(y_test,y_pred)","5e83f7e6":"#choosing a different n_neighbors and again evaluating the accuracy\nknn=KNeighborsClassifier(n_neighbors=6)\nknn.fit(data_train,y_train)\ny_pred=knn.predict(data_test)\nmetrics.accuracy_score(y_test,y_pred)","c777df25":"#choosing the best value of n_neighbors\na=[]\nfor i in range(1,30):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(data_train,y_train)\n    y_pred=knn.predict(data_test)\n    a.append(metrics.accuracy_score(y_test,y_pred))\nprint(a)    ","79c9a34c":"plt.plot(a,'b--')\nplt.grid()\nplt.xlabel('neighbors')\nplt.ylabel('accuracy_score')\nplt.title('graph')","4875eea7":"print(a[15])","573b757b":"#using n_neighbors=16 as it gives maximum accuracy\ndata_train,data_test,y_train,y_test=train_test_split(data,y,random_state=5,test_size=0.25)\nknn=KNeighborsClassifier(n_neighbors=16)\nknn.fit(data_train,y_train)\ny_pred=knn.predict(data_test)\nmetrics.accuracy_score(y_test,y_pred)","0857013c":"#now changing the random state let's see what effect it has on accuracy_score\ndata_train,data_test,y_train,y_test=train_test_split(data,y,random_state=1,test_size=0.25)\nknn=KNeighborsClassifier(n_neighbors=16)\nknn.fit(data_train,y_train)\ny_pred=knn.predict(data_test)\nmetrics.accuracy_score(y_test,y_pred)","0fa1e566":"from sklearn.model_selection import cross_val_score","ae2158a6":"knn=KNeighborsClassifier(n_neighbors=5)\nscores=cross_val_score(knn,data,y,cv=10,scoring='accuracy')\nprint(scores)","c6d7f00a":"#selecting the best n_neighbor for knn using cross_val_score\ni_range=list(range(1,31))\na=[]\nfor i in i_range:\n    knn=KNeighborsClassifier(n_neighbors=i)\n    scores=cross_val_score(knn,data,y,cv=10,scoring='accuracy')\n    a.append(scores.mean())\nprint(a)    ","7499b715":"plt.plot(i_range,a,'b')\nplt.xlabel('n_neighnors')\nplt.ylabel('accuracy_score')\nplt.grid()\nplt.title('accuracy_graph')","604c52b6":"max_acc=max(a)\nprint(max_acc)","3a084cf6":"a.index(max_acc)","eb1ac8a3":"print(a[16])","bdbf2bd1":"knn=KNeighborsClassifier(n_neighbors=13)\nscores=cross_val_score(knn,data,y,cv=10,scoring='accuracy')\nprint(scores.mean())","97b851f3":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nscores=cross_val_score(logreg,data,y,cv=10,scoring='accuracy')\nprint(scores)","15fae104":"print(scores.mean())","502124ea":"from sklearn.svm import SVC","c29c9d4e":"clf=SVC(kernel='linear')\nclf.fit(data_train,y_train)\ny_pred=clf.predict(data_test)\nprint(metrics.accuracy_score(y_test,y_pred))","68d62494":"clf=SVC(kernel='rbf',C=1)\nclf.fit(data_train,y_train)\ny_pred=clf.predict(data_test)\nprint(metrics.accuracy_score(y_test,y_pred))","9669b7a0":"clf=SVC(kernel='linear')\nscores=cross_val_score(clf,data,y,cv=10,scoring='accuracy')\nprint(scores)","adab84ba":"print(scores.mean())","2b8d3765":"#by changing the kernel to 'rbf'\nclf=SVC(kernel='rbf')\nscores=cross_val_score(clf,data,y,cv=10,scoring='accuracy')\nprint(scores.mean())","3cb4aced":"from sklearn.ensemble import RandomForestClassifier","9734c757":"clf=RandomForestClassifier(n_estimators=10)\nclf.fit(data_train,y_train)\ny_pred=clf.predict(data_test)\nprint(metrics.accuracy_score(y_test,y_pred).mean())","000aa1cf":"clf=RandomForestClassifier(n_estimators=1000)\nclf.fit(data_train,y_train)\ny_pred=clf.predict(data_test)\nprint(metrics.accuracy_score(y_test,y_pred).mean())","083b294d":"a=[]\nclf_range=list(range(1,100))\nfor i in clf_range:\n    clf=RandomForestClassifier(n_estimators=i)\n    scores=cross_val_score(clf,data,y,cv=10,scoring='accuracy')\n    a.append(scores.mean())\nplt.plot(clf_range,a)","ef540aa3":"data.head()","8f6c98ca":"sns.pairplot(data)","37a8c67b":"sns.pairplot(data,x_vars='petal_length_(cm)',y_vars='petal_width_(cm)',size=5)","143639c4":"new_data=data[['sepal_length_(cm)','sepal_width_(cm)']]","9945da1e":"new_data.head(10)","9e0164d7":"y\n","665bc01d":"knn=KNeighborsClassifier(n_neighbors=5)\nscores=cross_val_score(knn,new_data,y,cv=10,scoring='accuracy')\nprint(scores.mean())","254a47fa":"a=[]\nnew_range=list(range(1,31))\nfor i in new_range:\n    knn=KNeighborsClassifier(n_neighbors=i)\n    scores=cross_val_score(knn,new_data,y,cv=10,scoring='accuracy')\n    a.append(scores.mean())\nprint(a)    ","5a4b449b":"plt.plot(new_range,a,'r')\nplt.grid()\nplt.xlabel('n_neighbors')\nplt.ylabel('accuracy')\nplt.title('graph')","4ca66b5c":"print(max(a))","6c84c138":"from sklearn.model_selection import GridSearchCV ","2dd11ba9":"# finding the best_value of n_neighbors for knn model using grid_search\n# creating param_grid\nk_range=list(range(2,31))\nprint(k_range)","2cf70ba7":"param_grid=dict(n_neighbors=k_range)\nprint(param_grid)","5bc41a14":"grid=GridSearchCV(knn,param_grid,cv=10,scoring='accuracy',return_train_score=True)","9023fec3":"grid.fit(data,y)","c849136b":"#getting the best estimator as found by grid_search.\ngrid.best_estimator_","6a6c074d":"grid.best_params_","1ab5228c":"#let's see the best score which can be acheived by training our model(knn in this case) with the best parameters.\ngrid.best_score_","3717a470":"df=pd.DataFrame(grid.cv_results_)","9e82e411":"df.head()","25d0a6d4":"df_sub=df.loc[:,['mean_train_score','std_train_score','params']]","2fd8af72":"df_sub.head()","30a366b9":"scores=grid.cv_results_['mean_test_score']\nprint(scores)","c0fad90c":"plt.plot(k_range,scores,'r',linewidth=2)\nplt.grid()\nplt.xlabel('n_neighbors')\nplt.ylabel('scores')\nplt.title('graph')","f6ce2077":"from sklearn.model_selection import RandomizedSearchCV","0749d2b4":"param_dist=dict(n_neighbors=k_range)\nprint(param_dist)","279eaa71":"grid_2=RandomizedSearchCV(knn,param_dist,cv=10,scoring='accuracy')","a1c2c4c4":"\ngrid_2.fit(data,y)","0da287ac":"grid_2.best_estimator_","7609f01e":"grid_2.best_score_","a12ff9b9":">**grid searchCV** is basically a way to automate the process of choosing the best parameters,which was initially done by k_fold cross validation by trying every feature and then computing the accuracy score by using **k_fold CrossValidation**.\n\n>**grid searchCV** allows to pass in a dictionary of all features  mapped with a bunch of different possible values.\nAnd what **grid searchCV** does is ,it compute accuracy score by trying each possible combination of these features and then calculate the accuracy by using **k_fold CrossValidation**.\n\n>The process becomes computationally infeasible if the size of dataset is large,because all combinations of featuresalong with k_fold validation kindof becomes too much. \n\n>**grid searchCV** takes a parameter *param_grid*.\n\n>**param_grid**(as written in documentation):- Dictionary with parameters names (string) as keys and lists of\n    parameter settings to try as values, or a list of such\n    dictionaries, in which case the grids spanned by each dictionary\n    in the list are explored. This enables searching over any sequence\n    of parameter settings.","1a2a46c7":"Got exactly the same graph as we got by cross_validation.","810be323":"we can see that for n_neighbors=16 the accuracy predicted by train_test_split was 100% but in reality after using n_neighbors=16 with cross_validation the score comes out to be lesser than before.","0f98fa4c":">The first column is basically giving the accuracy_score ,the second one is giving the varience of a particular estimator.\nIf the varience is high then it might not be a good estimator.","92226a7d":"## A beginner guide for implementing machine learning model using scikit learn and pandas.\n## Introducing various concepts such as train_test_split,grid_searchCV,randomised_searchCV and many more .\n## Iris dataset available in scikitlearn is used for this purpose.","59f2eabb":"The maximum accuracy obtained by onlu selecting two features is 80% ,which is not doing any good.we can switch back to original datafrane or try some different combinations.\n\nTo perform the task of trying different combinations efficiently we can use **grid search cv** ","161a1bb7":"---\n----------------------","4d98eba1":"By tuning the kernel to 'linear' we are getting 100% accuracy.But we are using train_test_split so, just evaluate the accuracy again by k_fold cross validation.\nBut first let's tune the kernel to 'rbf'.","38cea4ae":"            By using cross validation thre accuracy is pretty much less than what we got using train_test_split","73227505":"we will use knn with n_neighbors=13 because it is the better than logistic regression.now we will se if accuracy can be further incresed by selecting good features ","7b2b6a0d":"petal_length and petal_width seems to be a good choice because it is able to seperate the species in a better wey relative to other features.","f8e39d94":"we can clearly see that *petal_len* vs *petal_wid* proves a more promising feature to seperate the three sepecies from each other,more presisely the *blue ones* are seems to be seperated in quite a good way from each other.  ","61d4f765":">we can see that the value of best n_neighbor for knn comes out to be exectly the same i.e 13 as previously found by writting loop and calculating accuracy for every neighbors.GridSearch does this job autometically.\n    \n we can even feed multiple dictionary to param_grid and it will select the best estimator from each dictionary.   ","b2876435":">what **randomised searchCV** does is,it selects random combinations of given parameters and does fit, predict on the given data using **k_fold CrossValidation** and calculates the accuracy score.\n\n>It takes much less time than **grid searchCV** as it only operates on a fixed number of times using random combinations of given parameters.\n\n>**randomised searchCV** may produce less accurate result in comparison to **grid search** but it is very close to the best reult, it also saves a lot of computation time.","2a09a1b4":"Evaluation by logistic regression","ce8ebe73":"## let's try Random Forest","d4817a12":"## feature selection by crossvalidation","49007e6a":"we can see that logistic regression has accuracy lower than knn so by using crossvalidation we can select best model for tha data.","23351c8c":" Random Forest is giving the accuracy of approx 97% thus fro this we conclude that the best fit model on our data is **support vector classifier** and **knn with n_neighbors=13** ","844fe49a":"# we can see that train_test_split gives high varience estimate and changing the training and testing set greately changes the accuracy_score.\n# To solve this problem we can use k_fold_crossvalidation","33730b03":"## Trying combinations with grid search cv","c3597b0c":"now we can conclude that for knn the choice of best n_neighbors=13 or 21 and the accuracy is approx 98%","02b29d36":"## Evaluating different models by cross_validation","3533a096":"      rbf performed better than linear kernel.","865fdbf4":"### plotting graphs gives a kind of insight to the data and thus we have a better understanding of our dataset.\n### It is always advisable to kind of look inside the data and plotting is the best way to acheive this.","06802784":"graph shows a more reliable accuracy score and thus helps in finding a better value of n_neighbors","aa1662d8":" we can further try to improve the accuracy by tuning the parametrs using **grid searchCV**.","326490ff":"### we can train our models by using the best parameters which we got by grid searchCV.","253ec73a":">To decrese the computational expense of **grid searchCV**  we can use a cousin of this called **randomised searchCV** .\n## let's now see randomised searchCV","4194926a":"***\nlet's try **Support Vector Machine(SVM)** ","2ae97bdd":"let's try a different value of n_estimator."}}