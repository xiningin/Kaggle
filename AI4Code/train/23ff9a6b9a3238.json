{"cell_type":{"d8924b1f":"code","583f77aa":"code","9e944198":"code","923d1ec0":"code","af70f050":"code","1cc9fa0e":"code","20da9143":"code","f6a6c7d6":"code","8427edd1":"code","9a54c0a0":"code","4b9289c3":"code","f5c2278a":"code","14ad64c0":"code","d89c6a58":"code","2d84199c":"code","39bc5afd":"code","ae9fd61b":"code","d79492e2":"code","8f68cfab":"code","cb079c07":"code","cc72094d":"code","3a18af2d":"code","b1874f2b":"code","21998dda":"code","0cd89ed7":"code","c41c494d":"code","11dc7344":"code","22247822":"code","78fdebcd":"code","fc183000":"code","d577adc4":"code","50feda7e":"code","4ea59da5":"code","142997d7":"code","c5b6f84a":"code","521df579":"code","7ed062a5":"markdown","d00fe327":"markdown","1e61446e":"markdown","ff4763ee":"markdown","ca8f5fdf":"markdown","6c2fffdf":"markdown","322633a0":"markdown","707142bc":"markdown","ae84a202":"markdown","35edb5b8":"markdown","8f584cf4":"markdown","5961ff5e":"markdown","7d1a4a7b":"markdown","15b7967b":"markdown","fec92a22":"markdown","91402016":"markdown","5cb682fd":"markdown","fa532f6c":"markdown","26cab6b4":"markdown","120d6401":"markdown","c0e2f585":"markdown"},"source":{"d8924b1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","583f77aa":"missing_values = [\"n\/a\", \"na\", \"NaN\",\"nan\"] #these values will be considered as missing values\nbiden = pd.read_csv('\/kaggle\/input\/us-election-2020-tweets\/hashtag_joebiden.csv',na_values = missing_values)","9e944198":"trump = pd.read_csv('\/kaggle\/input\/us-election-2020-tweets\/hashtag_donaldtrump.csv',na_values = missing_values, engine=\"python\")","923d1ec0":"biden.head()","af70f050":"trump.head()","1cc9fa0e":"biden = biden.drop(columns = ['created_at', 'tweet_id','source','user_id','user_name','user_screen_name','user_description','user_location','lat','long','city','state_code','collected_at']) \n","20da9143":"trump = trump.drop(columns = ['created_at', 'tweet_id','source','user_id','user_name','user_screen_name','user_description','user_location','lat','long','city','state_code','collected_at']) \n","f6a6c7d6":"biden = biden.dropna(axis = 0, how = 'any') #it will remove all rows containing 'any' value as null(at least one)\nprint(len(biden)) #size after removing rows\nprint(biden.isnull().sum()) #count of null values in each column","8427edd1":"biden.head()","9a54c0a0":"trump = trump.dropna(axis = 0, how = 'any')\nprint(len(trump))\nprint(trump.isnull().sum())","4b9289c3":"#retwwet_count and user_followers_count columns conatins values as object\/string\n#so it'll be better to convert into float\nbiden = biden.astype({\"retweet_count\": float, \"user_followers_count\":float})\ntrump = trump.astype({\"retweet_count\": float, \"user_followers_count\":float})","f5c2278a":"\"\"\" import random\ndef fill_missing(column):\n    col = column.value_counts().to_dict()\n    prob = random.choices(list(col.keys()), weights = list(col.values()), k=100)\n    null_val = column.isnull()\n    for i in range(len(null_val)):\n        if null_val[i]== True:\n            column[i] = prob[i%100]\n    return column\n\"\"\"","14ad64c0":"def make_dictionary(dataframe,column):\n    dictt = {}\n    for i in list(dataframe[column]):\n        if i in dictt:\n            dictt[i] = dictt.get(i) + 1\n        else:\n            dictt[i] = 1\n    return dictt\n    ","d89c6a58":"country_biden = make_dictionary(biden,'country')\nstate_biden = make_dictionary(biden,'state')\ncontinent_biden = make_dictionary(biden,'continent')\ncontinent_biden","2d84199c":"country_trump = make_dictionary(trump,'country')\nstate_trump = make_dictionary(trump,'state')\ncontinent_trump = make_dictionary(trump,'continent')\ncontinent_trump","39bc5afd":"#all keys with values less than 1000 will be deleted\nfor i in list(country_biden):\n    if country_biden.get(i)<1000:\n        del country_biden[i]\n#now arranging keys according to values in asc order\ncountry_biden  = {k: v for k, v in sorted(country_biden.items(), key=lambda item: item[1])}\ncountry_biden\n","ae9fd61b":"for i in list(country_trump):\n    if country_trump.get(i)<1000:\n        del country_trump[i]\ncountry_trump  = {k: v for k, v in sorted(country_trump.items(), key=lambda item: item[1])}\ncountry_trump\n","d79492e2":"import matplotlib.pyplot as plt\np1 = plt.bar(np.arange(len(country_biden)),list(country_biden.values()),0.9)\nplt.ylabel('tweets')\nplt.title('tweets per country for biden')\nplt.xticks(np.arange(len(country_biden)), country_biden.keys(),rotation='vertical')\nplt.show()","8f68cfab":"p11 = plt.bar(np.arange(len(country_trump)),list(country_trump.values()),0.9)\nplt.ylabel('tweets')\nplt.title('tweets per country for trump')\nplt.xticks(np.arange(len(country_trump)), country_trump.keys(),rotation='vertical')\nplt.show()","cb079c07":"state_biden","cc72094d":"#1 for winning, 0 for losing in that state for biden\nbiden_won_states = {'Georgia':1, 'Arizona':1,'Florida':0,'Iowa':0,'Michigan':1,'Nevada':1,'New Hampshire':1,\n                   'North Carolina':0,'Ohio':0,'Pennsylvania':1,'Texas':0,'Wisconsin':1,'Washington':1,\n                   'Idaho':0,'Oregon':1,'Utah':0,'New Mexico':1,'Oklahoma':0,'Nebraska':0,\n                   'Wyoming':0,'Montana':0,'North Dakota':0,'South Dakota':0,'Minnesota':1,'Illinois':1,\n                   'Indiana':0,'West Virginia':0,'Virginia':1,'Maine':1,'Massachusetts':1,'Alabama':0,\n                   'Mississippi':0,'Lousiana':0,'Michigan':1,'South Carolina':0,'Maryland':1,\n                    'District Of Columbia':1,'Delaware':1,'New Jersey':1,'Connecticut':1,'Rhode Island':1,\n                   'Vermont':1,'Arkansas':0,'Kansas':0,'Missouri':0,'Tennessee':0,'Kentucky':0}\nst_x = [] #stores number of tweets from each state\nst_y = [] #stores 1 for winning, 0 for losing in each state\nfor i in state_biden.keys():\n    if i in biden_won_states.keys():\n        st_x.append(state_biden.get(i))\n        st_y.append(biden_won_states.get(i))\nprint(st_x)\nprint(st_y)\ncorr = np.corrcoef(st_x,st_y) #correlation b\/w tweets and winning in particular state\nprint(corr)","3a18af2d":"#vote percent of biden in each state\nbiden_won_states_percent = {'Georgia':49.5, 'Arizona':49.4,'Florida':47.9,'Iowa':45.0,'Michigan':50.6,'Nevada':50.1,'New Hampshire':52.8,\n                   'North Carolina':48.7,'Ohio':45.2,'Pennsylvania':50.0,'Texas':46.5,'Wisconsin':49.6,'Washington':58.4,\n                   'Idaho':33.1,'Oregon':56.9,'Utah':37.7,'New Mexico':54.3,'Oklahoma':32.3,'Nebraska':39.4,\n                   'Wyoming':26.7,'Montana':40.5,'North Dakota':31.9,'South Dakota':35.6,'Minnesota':52.6,'Illinois':57.4,\n                   'Indiana':41.0,'West Virginia':29.7,'Virginia':54.3,'Maine':52.9,'Massachusetts':65.6,'Alabama':36.6,\n                   'Mississippi':40.5,'Lousiana':39.9,'Michigan':50.6,'South Carolina':43.4,'Maryland':65.4,\n                    'District Of Columbia':92.9,'Delaware':58.8,'New Jersey':57.2,'Connecticut':59.3,'Rhode Island':59.6,\n                   'Vermont':66.4,'Arkansas':34.8,'Kansas':41.3,'Missouri':41.3,'Tennessee':37.4,'Kentucky':36.2}\nst_x_per = [] #stores tweets count of each state\nst_y_per = [] #store vote percent of each state for biden\nfor i in state_biden.keys():\n    if i in biden_won_states_percent.keys():\n        st_x_per.append(state_biden.get(i))\n        st_y_per.append(biden_won_states_percent.get(i))\nprint(st_x_per)\nprint(st_y_per)\ncorr1 = np.corrcoef(st_x_per,st_y_per)\nprint(corr1)","b1874f2b":"#deleting some keys with less values or state which are not in USA\nfor i in list(state_biden):\n    if state_biden.get(i)<2000:\n        del state_biden[i]\ndel state_biden['Istanbul']\ndel state_biden['Berlin']\ndel state_biden['Maharashtra']\ndel state_biden['Delhi']\ndel state_biden['Ontario']\ndel state_biden['Ile-de-France']\ndel state_biden['England']\n\nstate_biden  = {k: v for k, v in sorted(state_biden.items(), key=lambda item: item[1])} #sorting dict\nstate_biden","21998dda":"for i in list(state_trump):\n    if state_trump.get(i)<2500:\n        del state_trump[i]\ndel state_trump['Istanbul']\ndel state_trump['Berlin']\ndel state_trump['Maharashtra']\ndel state_trump['Delhi']\ndel state_trump['Ontario']\ndel state_trump['Ile-de-France']\ndel state_trump['England']\ndel state_trump['North Rhine-Westphalia']\ndel state_trump['British Columbia']\ndel state_trump['Scotland']\nstate_trump  = {k: v for k, v in sorted(state_trump.items(), key=lambda item: item[1])}\nstate_trump","0cd89ed7":"p2 = plt.bar(np.arange(len(state_biden)),list(state_biden.values()),0.9)\nplt.ylabel('tweets')\nplt.title('tweets per state for biden')\nplt.xticks(np.arange(len(state_biden)), state_biden.keys(),rotation='vertical')\nplt.show()","c41c494d":"p3 = plt.bar(np.arange(len(state_trump)),list(state_trump.values()),0.9)\nplt.ylabel('tweets')\nplt.title('tweets per state for trump')\nplt.xticks(np.arange(len(state_trump)), state_trump.keys(),rotation='vertical')\nplt.show()","11dc7344":"for i in list(continent_biden):\n    if continent_biden.get(i)<100:\n        del continent_biden[i]\ncontinent_biden  = {k: v for k, v in sorted(continent_biden.items(), key=lambda item: item[1])}\ncontinent_biden","22247822":"for i in list(continent_trump):\n    if continent_trump.get(i)<100:\n        del continent_trump[i]\ncontinent_trump  = {k: v for k, v in sorted(continent_trump.items(), key=lambda item: item[1])}\ncontinent_trump","78fdebcd":"#for biden\nplt.pie([v for v in continent_biden.values()],labels = [k for k in continent_biden.keys()],autopct='%1.1f%%')\nplt.show()","fc183000":"#for trump\nplt.pie([v for v in continent_trump.values()],labels = [k for k in continent_trump.keys()],autopct='%1.1f%%')\nplt.show()","d577adc4":"text_biden = \" \".join(biden.tweet)\nlen(text_biden)","50feda7e":"n = 10000\ntext_list_biden = [text_biden[i:i+n] for i in range(0, len(text_biden), n)]\n","4ea59da5":"len(text_list_biden) #total lists each containing 10000 chars","142997d7":"text_trump = \" \".join(trump.tweet)\nn = 10000\ntext_list_trump = [text_trump[i:i+n] for i in range(0, len(text_trump), n)]\nlen(text_list_trump)","c5b6f84a":"# first, we import the relevant modules from the NLTK library\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n# next, we initialize VADER so we can use it within our Python script\nsid = SentimentIntensityAnalyzer()\nscores_biden = {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}\n#compound for overall sentiment\nfor i in range(1000): #we're analysing only 1000 list for saving time instead of ~5000\n    score = sid.polarity_scores(text_list_biden[0]) #sentiment score of each list\n    #adding this score to total score\n    scores_biden['compound'] = scores_biden['compound'] + score['compound']\n    scores_biden['neg'] = scores_biden['neg'] + score['neg']\n    scores_biden['neu'] = scores_biden['neu'] + score['neu']\n    scores_biden['pos'] = scores_biden['pos'] + score['pos']\n#since we've added scores of 1000 lists \n#divide each score by 1000 to get mean score\nscores_biden['compound'] = scores_biden['compound']\/1000\nscores_biden['neg'] = scores_biden['neg'] \/1000\nscores_biden['neu'] = scores_biden['neu'] \/1000\nscores_biden['pos'] = scores_biden['pos'] \/1000\nscores_biden","521df579":"scores_trump = {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}\nfor i in range(1000):\n    score = sid.polarity_scores(text_list_trump[0])\n    scores_trump['compound'] = scores_trump['compound'] + score['compound']\n    scores_trump['neg'] = scores_trump['neg'] + score['neg']\n    scores_trump['neu'] = scores_trump['neu'] + score['neu']\n    scores_trump['pos'] = scores_trump['pos'] + score['pos']\nscores_trump['compound'] = scores_trump['compound'] \/1000\nscores_trump['neg'] = scores_trump['neg'] \/1000\nscores_trump['neu'] = scores_trump['neu'] \/1000\nscores_trump['pos'] = scores_trump['pos'] \/1000\nscores_trump","7ed062a5":"# EDA","d00fe327":"**finding correlation between winning in particular state with no. of tweets from that state**","1e61446e":"**we can see we got slightly more correlation using vote percent.**","ff4763ee":"**Since there're so many keys in each dictionary, we can delete some keys with values less than certain threshold number**","ca8f5fdf":"**As you can see we got very less value for corr b\/w tweets and winning. Now we'll try using vote percent instead of 0\/1 and see the corr b\/w tweets and vote percent**","6c2fffdf":"**Same procedure for trump dataset**","322633a0":"**we got overall sentiment for trump as -0.99 which is more negative than biden(-0.97) which concludes that people are saying more negative things for trump.\nThere may be some tweets for #biden in which negative words are meant for trump and vice-versa. \nSo we can get more accurate results by considering only those tweets which are only for either biden or trump.\nBut I think still there'll be more negative tweets for trump ;)**","707142bc":"**Some columns are not so useful for our work so we'll remove them**","ae84a202":"quick look at trump dataset","35edb5b8":"Combining all text from tweets into single string","8f584cf4":"# Sentiment analysis","5961ff5e":"quick look at biden dataset","7d1a4a7b":"**Some rows contains null values. Since dataset is quite large we can manage to remove these rows**","15b7967b":"**After removing unuseful columns and rows**","fec92a22":"**we got overall sentiment as -0.97 which indicates that mostly tweets are negative rather than positive**","91402016":"**Plotting percentage of tweets from diff continents**","5cb682fd":"**dividing large text into lists each containing 10000 chars**","fa532f6c":"**deleting unuseful keys of continent dict**","26cab6b4":"**Below function will fill null values according to percentage of non-null values in that column**","120d6401":"Making dictionaries for certain columns","c0e2f585":"**Below function will make dictionary for certain column. Keys will be its elements and values will be count of elements in that columns**"}}