{"cell_type":{"cf9c5e52":"code","a53dac86":"code","8c4e4d3f":"code","10e1dd19":"code","9153faa9":"code","60f36cbd":"code","b70b36b4":"code","70fcf3ea":"code","548d8974":"code","7ab3dff9":"code","7e51f937":"code","14d7de71":"code","ac353064":"code","2fc21475":"code","d7d12d60":"code","74257f09":"code","4bac7a7b":"code","5cce121d":"code","427c1646":"code","c5e8e30d":"code","4bc78524":"code","28a1a0a5":"code","e32c0e36":"code","7131da75":"code","d0a1a1e9":"code","95354da1":"code","ac9d4905":"code","752e8cb2":"code","06a69259":"code","8ceade4f":"code","0efd2171":"code","2c52f424":"code","e83a7d7f":"code","7fe9abff":"code","7630e3ff":"code","88df44d5":"code","9ba33093":"code","bd46bcbc":"code","a67283e1":"code","533c56d0":"code","96d6e77e":"code","c0be7035":"code","8f413d41":"markdown","36aea290":"markdown","c3255e16":"markdown","d6e67181":"markdown","efb8cf8e":"markdown","705ef967":"markdown","2f67c303":"markdown","df3b090e":"markdown","b5a7b2c2":"markdown","6192e553":"markdown","f2129ae7":"markdown","603f3841":"markdown","ba76d5f5":"markdown","fa639f8d":"markdown","cd8daa93":"markdown","396caf4d":"markdown","6a731085":"markdown","19432e44":"markdown","c1407281":"markdown","bc04b10d":"markdown","a1d1b00e":"markdown","dee22b79":"markdown","f6c5f73e":"markdown","9bc19598":"markdown","a9670bff":"markdown","2c7db625":"markdown","c60c7ac0":"markdown","796b9306":"markdown","dd6a3318":"markdown","002e360c":"markdown","b37b6718":"markdown","8a5bc0c2":"markdown","7e312cd9":"markdown","6d5d5649":"markdown","51d7edd9":"markdown","2644fc12":"markdown","7dd55bd6":"markdown","5a3c9b29":"markdown"},"source":{"cf9c5e52":"#Import Libraries:\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import  train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import roc_auc_score, classification_report\n","a53dac86":"#Read in data:\ndata = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata.head()","8c4e4d3f":"#The target has been wrongly classified in this particular dataset so we must first switch the labels around:\ndata['target'] = data['target'].replace({0:1,1:0})","10e1dd19":"data.info()","9153faa9":"#Rename columns:\ndata = data.rename(columns={'cp':'chest_pain','trestbps':'resting_bp','chol':'cholesterol','fbs':'fasting_bs',\n                           'restecg':'resting_ecg','thalach':'max_heart_rate','exang':'ex_angina',\n                           'oldpeak':'ST_depression','ca':'vessels','thal':'blood_flow'})","60f36cbd":"#Reclassify categorical variables as objects:\nfor col in ['sex','chest_pain','fasting_bs','resting_ecg', 'ex_angina','slope','blood_flow']:\n    data[col] = data[col].astype('object')\n\n#Replace categorical factor labels:\ndata['sex'] = data['sex'].replace({0:'female', 1:'male'})\ndata['chest_pain'] = data['chest_pain'].replace({0:'typical angina', 1:'atypical angina', 2:'non-anginal pain', 3:'asymptomatic'})\ndata['fasting_bs'] = data['fasting_bs'].replace({0:'under 120 mg\/dl', 1:'over 120 mg\/dl'})\ndata['resting_ecg'] = data['resting_ecg'].replace({0:'normal', 1:'wave abnormality', 2:'vent. hypertrophy'})\ndata['slope'] = data['slope'].replace({0:'upsloping', 1:'flat', 2:'downsloping'})\ndata['blood_flow'] = data['blood_flow'].replace({0:'NULL', 1:'normal flow', 2:'fixed defect', 3:'reversible defect'})","b70b36b4":"#Separate features and target:\ndata_y = data['target'].copy()\ndata_X = data.drop('target', axis=1).copy()\n\n#Split data into training and test sets:\nX, X_test, y, y_test = train_test_split(data_X,data_y,test_size=0.2,random_state=0)","70fcf3ea":"#Create lists of numeric and categorical variables:\n\nnum_vars = [col for col in X.columns if X[col].dtype in ['float64','int64']]\ncat_vars = [col for col in X.columns if X[col].dtype=='object']\n\nprint(\"num_vars:\\n\", num_vars)\nprint(\"cat_vars:\\n\", cat_vars)","548d8974":"X[num_vars].describe()","7ab3dff9":"#Histograms of numeric variables:\nplt.subplots(2,3,figsize=(10,6))\nplt.tight_layout()\nj=1\nfor i in num_vars:\n    skew = X[i].skew()\n    plt.subplot(2,3,j)\n    sns.histplot(X[i], bins=30)\n    plt.title(i + \", skew=\"+ str(round(skew, 2)), fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","7e51f937":"#Create lists of continuous and discrete numeric variables:\ncont_num = [col for col in num_vars if X[col].nunique()>5]\ndisc_num = [col for col in num_vars if col not in cont_num]","14d7de71":"#Boxplots of continuous features by CHD status:\nnrows=2\nncols=3\nplt.subplots(nrows,ncols,figsize=(9,7))\nplt.tight_layout()\n\nj=1\nfor i in cont_num:\n    plt.subplot(nrows,ncols,j)\n    sns.boxplot(x=y,y=X[i], palette={'lightgrey','lightcoral'})\n    plt.title(i)\n    j+=1","ac353064":"#Calculate CHD proportions by no of vessel:\ntable = pd.crosstab(X['vessels'], data_y, normalize='index')\ntable[[1,0]].plot.bar(stacked=True, color=['lightcoral','lightgrey'])\nplt.title(\"CHD Proportion by Number of Vessels\", fontsize=15)\ntable","2fc21475":"#Plot correlation matrix of numeric variables:\ncorrmat = X[num_vars].corr(method='spearman')\n\n#Mask from Seaborn tutorial:\nmask = np.zeros_like(corrmat,dtype=np.bool)\nmask[np.triu_indices_from(mask)]=True\n\nplt.figure(figsize=(6,5))\nsns.heatmap(corrmat,annot=True,mask=mask,cmap=sns.diverging_palette(240,10,as_cmap=True),vmin=-1,vmax=1)\nplt.title(\"Feature Correlation Matrix\",fontsize=20)\nplt.show()","d7d12d60":"#Pairplots of features:\nall_training = pd.concat([X[num_vars],y,],axis=1)\nsns.pairplot(all_training, hue='target', height=1.5)\nplt.show()","74257f09":"#Countplots of categorical variables:\nplt.subplots(3,3,figsize=(11,11))\nplt.tight_layout()\n\nj=1\nfor i in cat_vars:\n    plt.subplot(3,3,j)\n    sns.countplot(x=X[i], palette='viridis')\n    plt.title(i,fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","4bac7a7b":"#Plot CHD by categorical variables:\nfor i in cat_vars:\n    table = pd.crosstab(X[i],y)\n    table[[1,0]].div(table.sum(1),axis=0).plot.bar(stacked=True,color=['lightcoral','lightgrey'])\n    plt.title(\"CHD by \" +i,fontsize=15)\n    plt.ylabel(\"Proportion\")","5cce121d":"#Remove observations:\nprint(\"X shape\",X.shape)\nall_X = pd.concat([X,y],axis=1)\nall_X = all_X[(all_X['blood_flow']!='NULL') & (all_X['cholesterol']<500)]\nprint(\"all_X shape\",all_X.shape)\nX = all_X.drop('target', axis=1).copy()\ny = all_X['target'].copy()\nprint(\"X shape\",X.shape,\"\\ny shape\",y.shape)","427c1646":"#Re-plot cont_num variables:\n\nplt.subplots(2,3,figsize=(10,6))\nplt.tight_layout()\nj=1\nfor i in cont_num:\n    plt.subplot(2,3,j)\n    sns.histplot(X[i],bins=30, kde=False)\n    plt.title(i, fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","c5e8e30d":"#Plot log transformed cont_num variables:\n\nplt.subplots(2,3,figsize=(10,6))\nplt.tight_layout()\nj=1\nfor i in cont_num:\n    plt.subplot(2,3,j)\n    sns.histplot(np.log1p(X[i]),bins=30, kde=False)\n    plt.title(i, fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","4bc78524":"#Log transform log_cols in X and X_test:\nlog_cols = ['resting_bp','cholesterol','ST_depression']\nX_test = X_test.copy()\n\nfor i in log_cols:\n    X[i] = np.log1p(X[i])\n    X_test[i] = np.log1p(X_test[i])","28a1a0a5":"#Create list of cat vars to encode:\nencode_vars = ['chest_pain', 'resting_ecg', 'slope', 'blood_flow']    \n\n#Join train and test together:\nX_both = pd.concat([X,X_test],axis=0)\n\ndummies = pd.get_dummies(X_both[encode_vars],prefix = ['chest_pain', 'resting_ecg', 'slope', 'blood_flow'],drop_first=True)\nX_both.drop(encode_vars, axis=1, inplace=True)\n\nX_both = pd.concat([X_both,dummies],axis=1)\n\n\n#Remove labels from sex and fasting_bs:\nX_both['sex'] = X_both['sex'].replace({'female':0, 'male':1})\nX_both['fasting_bs'] = X_both['fasting_bs'].replace({'under 120 mg\/dl':0, 'over 120 mg\/dl':1})\n\n#Recode ex_angina as an int64:\nX_both['ex_angina'] = X_both['ex_angina'].astype('int64')\n\n#Split X and X_test apart again:\nX = X_both.iloc[:len(X),:].copy()\nX_test = X_both.iloc[len(X):,:].copy()\n\n\n\nX.shape,X_test.shape","e32c0e36":"X.info()","7131da75":"#Sample proportion of CHD:\n#Calculate proportions:\nprop = 100*y.value_counts()\/len(y)\n\n#Plot doughnut chart:\n#labels = ['No CHD','CHD']\nlabels = ['No CHD '+ str(round(prop[0],1)) + \"%\",'CHD '+ str(round(prop[1],1)) + \"%\"]\ncolormap = {'lightgrey','lightcoral'}\ny.value_counts().plot.pie(startangle=90, colors=colormap, labels=labels)\nplt.title(\"Overall CHD Proportion\", fontsize=15)\nplt.ylabel('')\ncircle = plt.Circle((0,0),0.7,color=\"white\")\np = plt.gcf()\np.gca().add_artist(circle)\nplt.show()","d0a1a1e9":"#Tune an L1 Log Reg model:\n#Pipeline with scaling:\nclf_L1 = LogisticRegression(penalty='l1', solver = 'liblinear', max_iter=10000)\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_L1)])\n\nhparams = {'model__C':[0.01,0.03,0.1,0.3,1,3,10]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X,y)\nprint(\"Best Params\", grid.best_params_)\nprint(\"L1 Logistic Regression best CV AUC score:\",grid.best_score_)","95354da1":"#Get selected features direct from gridsearchCV best estimator:\n\ngrid_coefs = pd.Series(grid.best_estimator_.named_steps['model'].coef_[0],X.columns)\ngrid_sel_feats = grid_coefs[grid_coefs!=0]\nselected = list(grid_sel_feats.index)\n#Plot coefs of selected:\ngrid_sel_feats.sort_values().plot.barh()\nplt.title(\"L1 Regularization kept Feature Coefficients\", fontsize=15);\n\ndropped = [col for col in X.columns if col not in grid_sel_feats.index]\nprint(len(dropped),\"dropped features:\", dropped, \"\\n\")\nprint(len(selected),\"selected features:\", selected, \"\\n\\n\")","ac9d4905":"#Update selected num_vars:\nsel_num_vars = [col for col in num_vars if col in selected]","752e8cb2":"#Create a table for model evaluation:\nmodel_table = pd.DataFrame(columns = ['Model','CV AUC Score'])","06a69259":"#Fit L1 logistic regression to selected features:\nclf_L1 = LogisticRegression(penalty='l1', solver = 'liblinear', max_iter=10000)\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_L1)])\n\nhparams = {'model__C':[0.01,0.03,0.1,0.3,1,3,10]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X[selected],y)\n\nprint(\"Best Params\", grid.best_params_)\nprint(\"L1 Logistic Regression best CV AUC score:\",grid.best_score_)\n\nmodel_table = model_table.append({'Model': clf_L1,'CV AUC Score':grid.best_score_}, ignore_index=True)","8ceade4f":"#Fit L2 logistic regression to selected features:\nclf_L2 = LogisticRegression(max_iter=10000)\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_L2)])\n\n\nhparams = {'model__C':[0.01,0.03,0.1,0.3,1,3,10]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X[selected],y)\nprint(\"Best Params\", grid.best_params_)\nprint(\"L2 Logistic Regression best CV score\",grid.best_score_)\n\nmodel_table = model_table.append({'Model': clf_L2,'CV AUC Score':grid.best_score_}, ignore_index=True)","0efd2171":"#Fit KNN to selected features:\nclf_knn = KNeighborsClassifier()\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_knn)])\n\nhparams = {'model__n_neighbors':[3,5,7,9]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X[selected],y)\n\nprint(\"Best Params\", grid.best_params_)\nprint(\"KNN best CV score\",grid.best_score_)\n\nmodel_table = model_table.append({'Model': clf_knn,'CV AUC Score':grid.best_score_}, ignore_index=True)","2c52f424":"#Fit LinearSVC to selected:\nclf_lsvc = SVC(kernel='linear')\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_lsvc)])\n\nhparams = {'model__C':[0.01,0.03,0.1,0.3,1,3,10]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X[selected],y)\n\nprint(\"Best Params\", grid.best_params_)\nprint(\"LinearSVC best CV score\",grid.best_score_)\n\nmodel_table = model_table.append({'Model': clf_lsvc,'CV AUC Score':grid.best_score_}, ignore_index=True)","e83a7d7f":"#Fit RBF SVC to selected features:\nclf_rbf = SVC(kernel='rbf')\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_rbf)])\n\nhparams = {'model__C':[0.001,0.01,0.1,1,10],\n          'model__gamma':[0.0001,0.001,0.01,1,10]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X[selected],y)\n\nprint(\"Best Params\", grid.best_params_)\nprint(\"RbfSVC best CV score\",grid.best_score_)\n\nmodel_table = model_table.append({'Model': clf_rbf,'CV AUC Score':grid.best_score_}, ignore_index=True)","7fe9abff":"#Fit Decision tree to selected:\nclf_DT = DecisionTreeClassifier()\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_DT)])\n\nhparams = {'model__max_depth':[2,3,4,6],\n          'model__max_leaf_nodes':[6,8,10,20]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X[selected],y)\n\nprint(\"Best Params\", grid.best_params_)\nprint(\"Decision Tree best CV score\",grid.best_score_)\n\nmodel_table = model_table.append({'Model': clf_DT,'CV AUC Score':grid.best_score_}, ignore_index=True)","7630e3ff":"#Fit Random Rofest to selected:\nclf_RF = RandomForestClassifier(random_state=0)\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_RF)])\n\nhparams = {'model__max_depth':[2,3,4,5,6],\n          'model__n_estimators':[20,30,40,50,60,80]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X[selected],y)\n\nprint(\"Best Params\", grid.best_params_)\nprint(\"Random Forest best CV score\",grid.best_score_)\n\nmodel_table = model_table.append({'Model': clf_RF,'CV AUC Score':grid.best_score_}, ignore_index=True)","88df44d5":"#Fit Gradient Boosting to selected:\nclf_GB = GradientBoostingClassifier()\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_GB)])\n\nhparams = {'model__n_estimators':[20, 40, 60, 80],\n          'model__learning_rate':[0.01,0.03,0.1],\n          'model__max_depth':[3,4,5]}\n\ngrid = GridSearchCV(pipe, param_grid = hparams, cv=5,n_jobs=3,scoring = 'roc_auc',verbose=1 )\ngrid.fit(X[selected],y)\nprint(\"Best Params\", grid.best_params_)\nprint(\"GradientBoosting best CV score\",grid.best_score_)\n\nmodel_table = model_table.append({'Model': clf_GB,'CV AUC Score':grid.best_score_}, ignore_index=True)","9ba33093":"model_table.sort_values(by='CV AUC Score',ascending=False)","bd46bcbc":"#Refit the tuned Random Forest Model:\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.2, random_state=0)\n\nclf_RF = RandomForestClassifier(max_depth= 3, n_estimators= 40, random_state=0)\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_RF)])\n\npipe.fit(X_train[selected],y_train)","a67283e1":"#Get Feature Importances:\n#Get Feature Importances from the final tuned RF model, index is selected:\nfeat_imp = pd.Series(pipe.named_steps['model'].feature_importances_, index=selected)\nfeat_imp = feat_imp.sort_values(ascending=False)\nsns.barplot(x=feat_imp, y=feat_imp.index)\nplt.title(\"Feature Importances of Random Forest Model\", fontsize=15)","533c56d0":"#Refit the tuned Random Forest Model:\n\nclf_RF = RandomForestClassifier(max_depth= 3, n_estimators= 40, random_state=0)\n\npreprocessor = ColumnTransformer(transformers =[('num', MinMaxScaler(),sel_num_vars)], remainder='passthrough')\n\npipe = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('model', clf_RF)])\n\npipe.fit(X[selected],y)\ny_preds = pipe.predict(X_test[selected])\ny_probs = pipe.predict_proba(X_test[selected])[:,1]\nauc_score = roc_auc_score(y_test, y_probs)\nprint(\"Final AUC Evaluation with Test Set:\", auc_score)","96d6e77e":"#Create Confusion Matrix:\ncon_mat = pd.crosstab(y_test,y_preds,rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(con_mat,annot=True, cmap='Blues')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.show()","c0be7035":"print(classification_report(y_test, y_preds))","8f413d41":"Subjects with CHD are older with a lower max_heart_rate and higher ST_depression score. There are several outliers with one particularly extreme cholesterol value of over 500.","36aea290":"## Refit Final Model to Test Set","c3255e16":"Resting_bp, cholesterol and ST_depression are improved by the transformation, although ST_depression is still not normally distributed. We will transform these 3 features and rely on the central limit theorem for the others.","d6e67181":"## L2 Logistic Regression","efb8cf8e":"There is considerable difference in CHD rates across the categorical varable levels. The rate of CHD is higher in male subjects, those with typical anginal, left ventrical hypertrophy, exercise induced angina, flat slope and either a normal flow or a reversible defect.","705ef967":"We will consider and apply the following classification models:\n- Logistic Regression with l1 regularisation,\n- Logistic Regression with L2 regularisation,\n- Naive Bayes,\n- KNN,\n- LinearSVC,\n- KernelSVC,\n- Decision Tree,\n- Random Forest,\n- Gradient Boosting.","2f67c303":"## Gradient Boosting","df3b090e":"## KernelSVC","b5a7b2c2":"## Balance","6192e553":"Our data is quite evenly balanced.","f2129ae7":"# Summary","603f3841":"### Encode Categorical Variables\nOf our 7 categorical variables, sex, fasting_bs and ex_angina are already 2 level dummy variables. We will create dummy variables for the remaining 4 categorical variables.","ba76d5f5":"The incidence of CHD rises as the number of clear vessels rises before dropping back again at 4 vessels. However, there are only 4 subjects in this group. This outcome seems to cotradict the definition of CHD. It is likely that these labels are misclassified so caution must be used when interpreting these results.","fa639f8d":"### Numeric Variables Relationship with Outcome and Each Other","cd8daa93":"Max_heart_rate was the most important risk factor for CHD in both the Logistic Regression and final Random Forest models. Our final predictive model gave a recall of 85% and micro and macro accuracies of 87%. However, this is an old dataset with much uncertainty around the feature labelling so this analysis should be taken as a demonstration of techniques rather than one of accurate insights.","396caf4d":"### Table of Results","6a731085":"## KNN\nWe can use KNN for this binary classification problem. Non-parametric in nature, it can handle mixed data types with no assumptions about their distributions, as long as they are scaled. ","19432e44":"# Categorical Variables","c1407281":"The dataset consists of 303 observations with 13 features - all coded as integers or floats - and a binary target of the presence or absence of CHD. There are no missing values. There are 6 numeric features and 7 categorical features, which we will reclassify as object types. Some of the feature names are a little opaque, so we will rename them. We will also add in the categorical level labels.","bc04b10d":"There is little correlation between the numeric features. The strongest correlation of -0.42 is between ST_depression and max_heart_rate, giving no cause for concern.","a1d1b00e":"The tuned Random Forest model gives the highest AUC score. ","dee22b79":"# Risk Factors and Prediction of Coronary Heart Disease","f6c5f73e":"### Feature Importance","9bc19598":"## Random Forest","a9670bff":"## Naive Bayes\nThis dataset contains mixed data types of continuous and binary variables. It was, therefore, felt not to be appropriate to use any of the Naive Bayes models.","2c7db625":"**Context**\n\nCoronary heart disease (CHD) is a [major cause of death](https:\/\/www.nhs.uk\/conditions\/coronary-heart-disease\/) in the UK and worldwide.\nCHD is the term that describes what happens when the heart's blood supply is blocked or interrupted by a build-up of fatty substances in the coronary arteries. This build-up process, known as atherosclerosis, can be caused by lifestyle factors, such as smoking and regularly drinking excessive amounts of alcohol. The risk of atherosclerosis increases with conditions like high cholesterol, high blood pressure (hypertension) or diabetes.\n\nThe main symptom of coronary heart disease is chest pain (angina). CHD is diagnosed from a combination of risk assessment (medical, family history and lifestyle) and testing. A number of different tests are used to diagnose heart related problems, including:\n    \n    blood tests\n    electrocardiogram (ECG)\n    exercise stress tests (eg, a treadmill test)\n    coronary angiography ( using dye and X-ray to detect coronary artery blockages)\n    \nOur aim in this analysis is to determine which of the given features, if any, are risk factors for CHD and to fit a predictive model to determine CHD status.\n\n**The Dataset**\n\nThe UCI Heart Disease dataset contains records for 303 subjects, with the recruitment process being unknown. The original dataset contained 76 attributes, but all published experiments refer to using the below subset of 14 of them.  There are several descriptions of these variables and their values online. The description below was settled upon after cross referencing with the variables in the dataset. The \"target\" field refers to the presence or absence of heart disease in the patient. \n\n**Attribute Information:**\n        \n        age: (age of subject)\n        sex: ( 1=male, 0=female)\n        cp: chest pain type\n            0: typical angina\n            1: atypical angina\n            2: non-anginal pain\n            3: asymptomatic\n        trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n        chol: serum cholesterol in mg\/dl \n        fbs: fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n        restecg: resting electrocardiographic results (\u2018ST\u2019 relates to positions on the ECG plot)\n            0: normal\n            1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n            2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n            \n        thalach: maximum heart rate achieved\n        exang: exercise induced angina (1 = yes; 0 = no) \n        oldpeak: ST depression induced by exercise relative to rest (\u2018ST\u2019 relates to positions on the ECG plot.)\n        slope: the slope of the peak exercise ST segment\n            1: upsloping\n            2: flat\n            3: downsloping\n        ca: number of major vessels (0-3) colored by dye in angiography (i.e.number of clear vessels)\n        thal: Results of the blood flow observed via the radioactive dye\n            0: NULL (dropped from the dataset previously)\n            1: normal blood flow\n            2: fixed defect (no blood flow in some part of the heart)\n            3: reversible defect (a blood flow is observed but it is not normal)\n        target: diagnosis of heart disease (angiographic disease status) in any major vessel\n            0: < 50% diameter narrowing\n            1: > 50% diameter narrowing\n\n\n**Acknowledgements**\n\nCreators:\n\n    Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n    University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n    University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n    V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.","c60c7ac0":"Notable is the cholesterol outlier visible in several plots.","796b9306":"## Feature Selection with L1 Regularisation","dd6a3318":"Age is approximately normally distributed with a mean of 54.9 years (std = 8.9). Max_heart_rate and resting_bp are both moderately skewed with medians of 153 bpm (IQR=31.5) and 130mmHg (IQR=20) respectively. [Note the rounding of blood pressure values to units of ten] . Cholesterol is very skewed, but this is influenced by the outlier value over 500. Without this value the skew for cholesterol is a more moderate 0.5. Neither ST_depression, median = 0.8 (IQR=1.75), or vessels, median = 1 ( IQR=1), are normally distributed, with vessels being a discrete count of the number of clear vessels.","002e360c":"## LinearSVC","b37b6718":"## Numeric Variables","8a5bc0c2":"# Feature Engineering\nWe will:\n- Remove the 2 NULL blood flow entries,\n- Remove the cholesterol outlier,\n- Consider transforming the continuous variables,\n- Encode categorical variables,\n- Examine balance.","7e312cd9":"# Machine Learning","6d5d5649":"# EDA","51d7edd9":"All categorical variables have more than 1 value, but there are only 4 observations in the resting_ecg left ventricle hypertrophy group. There are 2 NULL blood_flow values included which, according to the variable information, should have been removed beforehand. We will remove them later.","2644fc12":"### Transformations","7dd55bd6":"## L1 Logistic Regression","5a3c9b29":"## Decision Tree"}}