{"cell_type":{"6f327251":"code","48e50584":"code","63b3a96a":"code","11a6726c":"code","453d3fc9":"code","1605a94b":"code","74c76818":"code","9155e419":"code","8f293882":"code","40ddbadd":"code","43679e09":"code","5dc6c46b":"code","1e7c88da":"code","586f9ba0":"code","7b447529":"code","1fed598b":"code","5d849d3c":"code","856bd5b8":"code","4d49e378":"code","98bddaa7":"code","700c04e8":"code","1d62c2c5":"code","f43be2d0":"code","00687135":"code","320ff72d":"code","e5ad5cdc":"code","8f52858a":"code","f5669342":"code","543bc0a4":"code","b8d5e8c8":"code","1db16295":"code","da298c40":"code","a909d87b":"code","f2789179":"code","ad1941f7":"code","890af860":"code","02d51028":"code","b5e37128":"code","436ca2dc":"code","27d5a4b5":"code","9e3bcb9c":"code","23dad6da":"code","ab395fc4":"code","4ac0ba4f":"code","d71d6cc7":"code","023fa0e2":"code","8df12af1":"code","1d86e269":"code","483c253e":"code","6a0f6db7":"code","7a278ec5":"code","4d63d71c":"code","e8b7c641":"code","775a44d6":"code","a1049129":"code","0cd3e643":"code","c6eb1d73":"code","30c159c9":"code","be05e336":"code","853e0afc":"code","c93b6be5":"code","74edd5e9":"code","c17da9bd":"code","a693b383":"code","c1fab83e":"code","ad1cc2a5":"code","83b24f20":"code","c1e0fc29":"code","2cb150e8":"code","629795a2":"code","15df4675":"code","8645befb":"code","31cd395f":"code","b2d2c3a7":"code","fbfecb22":"code","e36f03e6":"markdown","e37cf998":"markdown","754b85e2":"markdown","c39126ed":"markdown","c71cdd9b":"markdown","34f59736":"markdown","25d5ada4":"markdown","47705c84":"markdown","c1f91e6e":"markdown","34e31350":"markdown","37c9d696":"markdown","a5d9b600":"markdown","f81b4eaf":"markdown","da97f3b7":"markdown","5341cc41":"markdown","59cca364":"markdown","ecd65f11":"markdown","7af01efa":"markdown","e1b6b9bd":"markdown","5ead1ba7":"markdown","2782f693":"markdown","634573c7":"markdown","3cd521f1":"markdown","01f1f560":"markdown","42fcbfa5":"markdown","aff5e11a":"markdown","3bff299f":"markdown","d6f6cab5":"markdown","6c68e064":"markdown","086344a9":"markdown","bded4b61":"markdown","92ffd61e":"markdown","92f5ec45":"markdown","cbbd69d8":"markdown","6adf17b3":"markdown","bef7d601":"markdown","5d5317c2":"markdown","e9348dbc":"markdown","0d3e3971":"markdown","1d7ffff9":"markdown","044fb529":"markdown","ff70c70c":"markdown","b4303e5e":"markdown","110f14f7":"markdown","28199998":"markdown","6033191b":"markdown","8000937d":"markdown","cb1711b7":"markdown","e8e885bd":"markdown","40484b3c":"markdown","69d6cb3e":"markdown","dc88036c":"markdown","d2aa2619":"markdown","10c923e5":"markdown","55901c9e":"markdown","01d76b05":"markdown","8ab77f92":"markdown","21c72c44":"markdown","b9a57316":"markdown","4e8ab5d6":"markdown"},"source":{"6f327251":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48e50584":"# Importing python libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.colors = True\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report","63b3a96a":"typeh = pd.read_csv('\/kaggle\/input\/healthcare-insurance\/Part1 - Type_H.csv')\nnorm = pd.read_csv('\/kaggle\/input\/healthcare-insurance\/Part1 - Normal.csv')\ntypes = pd.read_csv('\/kaggle\/input\/healthcare-insurance\/Part1 - Type_S.csv')","11a6726c":"print('Shape of the Normal dataset is',norm.shape,'and size is',norm.size)\nprint('Shape of the Type H dataset is',typeh.shape,'and size is',typeh.size)\nprint('Shape of the Type S dataset is',types.shape,'and size is',types.size)","453d3fc9":"norm.head()","1605a94b":"typeh.head()","74c76818":"types.head()","9155e419":"data = pd.concat([norm,typeh,types])","8f293882":"print('Shape of the final dataset is',data.shape,'and size is',data.size)","40ddbadd":"data.info()","43679e09":"data.head()","5dc6c46b":"data.describe(include='all')","1e7c88da":"data.Class.unique()","586f9ba0":"data['Class'] = data.Class.replace({'Nrmal':'Normal','type_h':'Type_H','tp_s':'Type_S'})","7b447529":"data.isnull().sum()","1fed598b":"data.dropna().shape","5d849d3c":"data['Class']=data['Class'].astype('category')","856bd5b8":"data.info()","4d49e378":"data.Class.value_counts()","98bddaa7":"data.groupby('Class').count()","700c04e8":"data.describe(include = 'all')","1d62c2c5":"#Looking at mean distributions across classes wrt each feature","f43be2d0":"data.groupby('Class')['P_incidence'].mean()","00687135":"data.groupby('Class')['P_tilt'].mean()","320ff72d":"data.groupby('Class')['L_angle'].mean()","e5ad5cdc":"data.groupby('Class')['S_slope'].mean()","8f52858a":"data.groupby('Class')['P_radius'].mean()","f5669342":"# Means are similar wrt P_radius","543bc0a4":"data.groupby('Class')['S_Degree'].mean()","b8d5e8c8":"#One-way ANOVA test\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nmod = ols(\"P_radius~Class\",data = data[['P_radius','Class']]).fit()\naov_table = sm.stats.anova_lm(mod,typ = 2)\naov_table","1db16295":"sns.distplot(data.P_incidence)","da298c40":"sns.boxplot(data.Class,data.P_incidence);","a909d87b":"upper_fence = plt.boxplot(data['P_incidence'])['caps'][1].get_data()[1][1]\nprint('Upper fence is at',upper_fence)\nprint('Number of outliers:',len(data[data['P_incidence']>upper_fence]))","f2789179":"data.drop(data[data['P_incidence']>upper_fence].index, inplace = True)","ad1941f7":"data.shape","890af860":"sns.distplot(data.P_tilt)","02d51028":"sns.boxplot(data.Class,data.P_tilt)","b5e37128":"sns.distplot(data.L_angle)","436ca2dc":"sns.boxplot(data.Class,data.L_angle)","27d5a4b5":"sns.distplot(data.S_slope)","9e3bcb9c":"sns.boxplot(data.Class,data.S_slope)","23dad6da":"sns.distplot(data.P_radius)","ab395fc4":"sns.boxplot(data.Class,data.P_radius)","4ac0ba4f":"sns.distplot(data.S_Degree)","d71d6cc7":"sns.boxplot(data.Class,data.S_Degree)","023fa0e2":"upper_fence = plt.boxplot(data['S_Degree'])['caps'][1].get_data()[1][1]\nprint('Upper fence is at',upper_fence)\nprint('Number of outliers:',len(data[data['S_Degree']>upper_fence]))","8df12af1":"data.drop(data[data['S_Degree']>upper_fence].index, inplace = True)","1d86e269":"data.shape","483c253e":"sns.heatmap(data.corr(),annot=True, mask = np.triu(data.corr()))","6a0f6db7":"#How does P_incidence vary with S_slope\nsns.scatterplot(data.P_incidence, data.S_slope, hue = data.Class)","7a278ec5":"#How does L_angle vary with P_radius\nsns.scatterplot(data.L_angle, data.P_radius, hue = data.Class)","4d63d71c":"#How does L_angle vary with P_radius\nsns.scatterplot(data.L_angle, data.S_Degree, hue = data.Class)","e8b7c641":"#Encoding the Class variable before proceeding with segregation.\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndata['Class'] = le.fit_transform(data['Class'])","775a44d6":"X = data.drop(labels = 'Class', axis=1)\ny = data['Class'] #target variables","a1049129":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)","0cd3e643":"#Since data could be in different units, performing normalization\n\n#Xscaled = X_train.apply(zscore)\n\n#Standardizing data ahead of bulidin the model\nscaler = preprocessing.MinMaxScaler()\nX_train = X_train.values[:,:]\nX_train = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train)\nX_train.hist()","c6eb1d73":"X_test = scaler.transform(X_test.values[:,:])\nX_test = pd.DataFrame(X_test)\nX_test.hist()","30c159c9":"pd.value_counts(data['Class']).plot(kind='bar')","be05e336":"c = pd.value_counts(data['Class'])\nc\/c.sum()","853e0afc":"#Trying a basic classifier with neighbours = 3\nNNH = KNeighborsClassifier(n_neighbors = 3)","c93b6be5":"NNH.fit(X_train,y_train)","74edd5e9":"#predict the response\ny_train_pred = NNH.predict(X_train)\n#evaluate accuracy\naccuracy_score(y_train,y_train_pred)","c17da9bd":"#predict the response\ny_pred = NNH.predict(X_test)\n#evaluate accuracy\naccuracy_score(y_test,y_pred)","a693b383":"from sklearn import metrics\ncm =metrics.confusion_matrix(y_test, y_pred, labels = [0,1,2])\ndf_cm = pd.DataFrame(cm, index = [i for i in [0,1,2]], \n                    columns = [i for i in [\"Pred0\",\"Pred1\",\"Pred2\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","c1fab83e":"#Classification report\nprint(classification_report(y_test,y_pred))","ad1cc2a5":"#import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,10)\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test) ","83b24f20":"#Generate plot\nplt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","c1e0fc29":"# grid searching key hyperparametres for KNeighborsClassifier by trying different k,\n# weights and metric hyperparameters\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# define models and parameters\nmodel = KNeighborsClassifier()\nn_neighbors = range(1, 11, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","2cb150e8":"#Trying the classifier with neighbours = 9 and weights = 'distance' and metric = 'eucledian'\nKNN_2 = KNeighborsClassifier(n_neighbors = 9, weights = 'distance', metric = 'euclidean')","629795a2":"KNN_2.fit(X_train,y_train)","15df4675":"#Checking training data accuracy\n#predict the response\ny_train_pred2 = KNN_2.predict(X_train)\n#evaluate accuracy\naccuracy_score(y_train,y_train_pred2)","8645befb":"#Checking test data accuracy\n#predict the response\ny_pred2 = KNN_2.predict(X_test)\n#evaluate accuracy\naccuracy_score(y_test,y_pred2)","31cd395f":"#Confusion matrix\nfrom sklearn import metrics\ncm =metrics.confusion_matrix(y_test, y_pred2, labels = [0,1,2])\ndf_cm = pd.DataFrame(cm, index = [i for i in [0,1,2]], \n                    columns = [i for i in [\"Pred0\",\"Pred1\",\"Pred2\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","b2d2c3a7":"#Classification report\nprint(classification_report(y_test,y_pred2))","fbfecb22":"#Classification report of NNH classifier for comparison\nprint(classification_report(y_test,y_pred))","e36f03e6":"### Automate the task of finding best values of K for KNN.","e37cf998":"## Data cleansing","754b85e2":"### Reading the .csv files","c39126ed":"## Conclusion and Improvisation:","c71cdd9b":"Obervations:\n- Distribution of S_Degree is right skewed. Since we plan to do KNN, distribution does not matter and we will not do transformation\n- Presence of outliers is detected; will need outlier treatment\n- Median S_Degree is almost similar across Normal and Type H classes but much higher for Type_S","34f59736":"### Display and explain the classification report in detail.","25d5ada4":"## Data analysis & visualization","47705c84":"##### S_Degree","c1f91e6e":"Obervations:\n\n- Median P_radius is almost similar across 3 classes (maybe slightly higher for Normal)","34e31350":"##### S_Slope","37c9d696":"## Data pre-processing:","a5d9b600":"##### L_angle","f81b4eaf":"Observations:\n    \n    - Precision:\n        - Of all predicted Class 0's 72% were actual Class 0\n        - Of all predicted Class 1's 75% were actual Class 1\n        - Of all predicted Class 2's 95% were actual Class 2\n        \n    - Recall:\n        - Of all actual Class 0s 82% were correctly labeled as Class 0\n        - Of all actual Class 1s 53% were correctly labeled as Class 1\n        - Of all actual Class 2s 98% were correctly labeled as Class 2\n        \n    - F1 score is the harmonic mean of Precision and Recall and therefore considers False positives and False negatives. It is not influenced by class imbalance and is a better evaluator of model performance. Here F1 score of 0.78 (macro avg) and 0.83 (weighted avg) is satisfactory.\n    \n    - One thing to note however is that recall and fi-scores for class 1 is not very high. We could go back to business and ask for more data with respect to class 1. Perhaps it will help in better prediction.","da97f3b7":"Observation:\n    We see that accuracy for test data is same for KNN_2 classifier as for NNH classifier.","5341cc41":"- Null Hypothesis: The means of P_radius is same across all three classes\n- Alternative Hypothesis: The means of P_radius is different across two or all three classes\n- Let alpha = 0.05","59cca364":"Observation: As L_angle increases, it is likelier to indicate presence of CLass Type_S","ecd65f11":"Obervations:n\n- Median P_tilt is almost similar for all 3 classes","7af01efa":"### Merging all datasets into one and exploring final shape and size","e1b6b9bd":"## Import and warehouse data","5ead1ba7":"### Perform Univariate, Bi-variate, multivariate analyses","2782f693":"### 4.2 Perform train-test split.","634573c7":"Displaying accuracy for test data","3cd521f1":"### Explore and if required correct datatype of each attribute","01f1f560":"#Since number of outliers are very few, we will delete these observations.","42fcbfa5":"### Display the classification accuracies for train and test data.","aff5e11a":"Observations:\n\n- P_incidence and S_slope are highly correlated with r = 0.81 \n- P_incidence and L_angle are correlated with r = 0.72\n- P_incidence and S_Degree are mildly correlated with r = 0.64\n- P_incidence and P_tilt are mildly correlated with r = 0.63\n- S_slope and L_angle are mildly correlated with r = 0.6","3bff299f":"Observation:\n\n- No null values showing. \n- Except for Class feature all other features are reflecting correct DataType\n- Class is a string object and has to be converted to categorical variable","d6f6cab5":"### Design and train a KNN classifier","6c68e064":"Since there is slight class imbalance accuracy is not the best measure of model performance. We can instead rely on F1 score","086344a9":"Observation:\n    We see that accuracy for train data is more for KNN_2 classifier than for NNH classifier. Since it is 1, it suggests model is overfit on train data","bded4b61":"##### P_incidence","92ffd61e":"### 4.3 Perform normalization or scaling if required","92f5ec45":"##### Correlation Matrix","cbbd69d8":"#### Statistical Analysis","6adf17b3":"### Explore and if required correct datatype of each attribute","bef7d601":"Comments: Splitting into train test before scaling since it is good practice to avoid information leakage","5d5317c2":"Comments:\n1. Quality - Data quality is good since not a lot of outliers were present\n2. Quantity - We could use more data to improve recall and F1 score further especially for class 1. Also we could use other classification models that are tree based\n3. Variety -  Features were a very few - only 6 or so. Hence, not sure if this model will accurately predict on future data. We could also have other associated diagnoses or diseases to see if they may have a high correlation with the class types.\n4. Velocity - With real time data, this model can help classify the right diagnosis and prove to be effective in medical field\n5. Veracity - Data veracity can be maintained by using actual masked data provided by healthcare functions (hospitals, clinics, governments etc.)","e9348dbc":"Observation: There are 6 unique class instead of 3. Checking for the same.","0d3e3971":"For higher values of S_Degree, it is likelier to be Tyep_S","1d7ffff9":"### 4.1 Segregate predictor vs target variables","044fb529":"Observation:\n- A few outliers are present in P_incidence\n- Median P_incidence for Type_S is much higher than for Normal, Type_H","ff70c70c":"Observation: \n- Accuracy for train and test is highest with k = 3","b4303e5e":"Displaying accuracy for train data","110f14f7":"## Model training, testing and tuning:","28199998":"Obervations:\n- Median S_slope is higher for Type_S","6033191b":"Conclusion:\n    \n- We saw the presence of a few outliers and treated them by removal since they constituted a very small percentage of the data\n- As seen from multi-variate analysis, there is indication of separability of data\n- Hence KNN classification is a good method to predict classes, since there are multiple classes involved\n- KNN being a non-parametric method does not make assumptions about the distribution of the data, hence we did not need transformations\n- KNN however is sensitive to scale, and hence we ensured to use z-scores (standardized data)\n- The 3 classes were not equally balanced ; since there is imbalance, could have addressed by artifically increased class size\n- However, refrained from SMOTE technique, since imbalance is not extreme. Instead we ensure to stratify data by y variable.\n- We built a KNN model by training on 70 percent of dataset; we tested on remaining 30 percent\n- We looked at several evaluation metrics: Accuracy, Recall, Precision, F1-score\n- Accuracy is not a good measure of model performance since it is sensitive to imbalance\n- Hence we focus on Recall and F1 scores\n- We initially used a simple KNN classifier with k = 3. \n- However through tuning, we see a slightly better performing model when k = 9 with Euclidean distance and distance weights is used","8000937d":"#### Comments","cb1711b7":"##### P_tilt","e8e885bd":"- Type_S comprises 48.4% of the data while Normal and Type_H comprises 32.2% and 19.4% respectively.\n- This shows the dataset is slightly imbalanced especially with respect to Type_H\n- Imbalance could be addressed through SMOTE technique to synthetically create more Type_H data\n- For current analysis, will ensure to stratify correctly when splitting into train-test.","40484b3c":"#### Multivariate analysis","69d6cb3e":"Observation:\n    - As P_incidence and S_slope increases, Class is more likely to be Type_S","dc88036c":"### Apply all the possible tuning techniques to train the best model for the given data. Select the final best trained model with your comments for selecting this model.","d2aa2619":"### 4.4 Check for target balancing. Add your comments.","10c923e5":"## Importing datasets","55901c9e":"Observation:\n\n    - Although it looks like KNN_2 classifier is better performing, there is a suggestion of overfit since accuracy of train data is 1. Moreover, accuracy is not best measure when there is class imbalance.\n  \n    - Macro avg Precision is slighly improved with KNN_2 (0.80 to 0.81) and so has Recall\n    \n    - Recall is an important measure for us since we want to maximize the number of correctly classified 1s.\n    \n    - F1 score is not influenced by class imbalance and is a better measure of model performance. We see that KNN_2 model has slightly better F1 scores\n      \n    - Hence we would prefer the second classifier KNN_2.","01d76b05":"Replacing with correct labels","8ab77f92":"##### P_radius","21c72c44":"Since p value is extremely small (smaller than 0.05) we reject null hypothesis. \nHence there is difference among the 3 classes wrt P_radius.","b9a57316":"#### Perform Univariate and Bivariate","4e8ab5d6":"Obervations:\n- Median L_angle is higher for Type_S"}}