{"cell_type":{"4312c051":"code","0fb6caff":"code","9d200d24":"code","5fc2419b":"code","d1d7fb16":"code","b21b4a2c":"code","29c7ded5":"code","57de4fa8":"code","e32870a7":"code","b8f97f8c":"code","6cfe5dc7":"code","6eafc1ef":"code","bcdb4dbb":"code","3a4873b3":"code","d655eef9":"code","fbd7c208":"code","c27070ee":"code","60df6987":"code","a605c38a":"code","37a97006":"code","6ac4efdf":"code","c52f86a6":"code","7ae1bed5":"code","416e5db9":"code","9e2fbcc1":"code","1d241eb0":"code","d0599281":"code","b7ac9877":"code","5f6cfdb8":"code","83170159":"code","c29a74ac":"code","4c3e4776":"code","a0b99aa2":"code","a6282b2e":"code","efd41c33":"code","b1b0e954":"code","fceff757":"code","b15b8688":"code","585656c7":"code","7329e788":"markdown"},"source":{"4312c051":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0fb6caff":"import tensorflow as tf","9d200d24":"tf.constant([[1., 2., 3.], [4., 5., 6.]])","5fc2419b":"tf.constant(42)","d1d7fb16":"t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\nt.shape","b21b4a2c":"t.dtype","29c7ded5":"t[:, 1:]","57de4fa8":"t[..., 1, tf.newaxis]","e32870a7":"t+10","b8f97f8c":"tf.square(t)","6cfe5dc7":"t @ tf.transpose(t)","6eafc1ef":"from tensorflow import keras\nK = keras.backend\nK.square(K.transpose(t)) + 10","bcdb4dbb":"a = np.array([2.,4.,5.])\ntf.constant(a)","3a4873b3":"t.numpy()","d655eef9":"tf.square(a)","fbd7c208":"np.square(t)","c27070ee":"tf.constant(2) + tf.constant(40)","60df6987":"t2 = tf.constant(40., dtype=tf.float64)","a605c38a":"tf.constant(2.0) + tf.cast(t2, tf.float32)","37a97006":"v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\nv","6ac4efdf":"v.assign(2 * v)           \nv[0, 1].assign(42)        \nv[:, 2].assign([0., 1.])\nv.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])","c52f86a6":"def huber_fn(y_true, y_pred):\n    error = y_true - y_pred\n    is_small_error = tf.abs(error) < 1\n    squared_loss = tf.square(error) \/ 2\n    linear_loss  = tf.abs(error) - 0.5\n    return tf.where(is_small_error, squared_loss, linear_loss)","7ae1bed5":"def my_softplus(z): # return value is just tf.nn.softplus(z)\n    return tf.math.log(tf.exp(z) + 1.0)\n\ndef my_glorot_initializer(shape, dtype=tf.float32):\n    stddev = tf.sqrt(2. \/ (shape[0] + shape[1]))\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n\ndef my_l1_regularizer(weights):\n    return tf.reduce_sum(tf.abs(0.01 * weights))\n\ndef my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n    return tf.where(weights < 0., tf.zeros_like(weights), weights)","416e5db9":"layer = keras.layers.Dense(30, activation=my_softplus,\n                           kernel_initializer=my_glorot_initializer,\n                           kernel_regularizer=my_l1_regularizer,\n                           kernel_constraint=my_positive_weights)","9e2fbcc1":"class MyL1Regularizer(keras.regularizers.Regularizer):\n    def __init__(self, factor):\n        self.factor = factor\n    def __call__(self, weights):\n        return tf.reduce_sum(tf.abs(self.factor * weights))\n    def get_config(self):\n        return {\"factor\": self.factor}","1d241eb0":"exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))","d0599281":"class MyDense(keras.layers.Layer):\n    def __init__(self, units, activation=None, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = keras.activations.get(activation)\n\n    def build(self, batch_input_shape):\n        self.kernel = self.add_weight(\n            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n            initializer=\"glorot_normal\")\n        self.bias = self.add_weight(\n            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n        super().build(batch_input_shape) # must be at the end\n\n    def call(self, X):\n        return self.activation(X @ self.kernel + self.bias)\n\n    def compute_output_shape(self, batch_input_shape):\n        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"units\": self.units,\n                \"activation\": keras.activations.serialize(self.activation)}","b7ac9877":"class MyMultiLayer(keras.layers.Layer):\n    def call(self, X):\n        X1, X2 = X\n        return [X1 + X2, X1 * X2, X1 \/ X2]\n\n    def compute_output_shape(self, batch_input_shape):\n        b1, b2 = batch_input_shape\n        return [b1, b1, b1] # should probably handle broadcasting rules","5f6cfdb8":"class MyGaussianNoise(keras.layers.Layer):\n    def __init__(self, stddev, **kwargs):\n        super().__init__(**kwargs)\n        self.stddev = stddev\n\n    def call(self, X, training=None):\n        if training:\n            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n            return X + noise\n        else:\n            return X\n\n    def compute_output_shape(self, batch_input_shape):\n        return batch_input_shape","83170159":"class ResidualBlock(keras.layers.Layer):\n    def __init__(self, n_layers, n_neurons, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n                                          kernel_initializer=\"he_normal\")\n                       for _ in range(n_layers)]\n\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        return inputs + Z","c29a74ac":"class ResidualRegressor(keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n                                          kernel_initializer=\"he_normal\")\n        self.block1 = ResidualBlock(2, 30)\n        self.block2 = ResidualBlock(2, 30)\n        self.out = keras.layers.Dense(output_dim)\n\n    def call(self, inputs):\n        Z = self.hidden1(inputs)\n        for _ in range(1 + 3):\n            Z = self.block1(Z)\n        Z = self.block2(Z)\n        return self.out(Z)","4c3e4776":"class ReconstructingRegressor(keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n                                          kernel_initializer=\"lecun_normal\")\n                       for _ in range(5)]\n        self.out = keras.layers.Dense(output_dim)\n\n    def build(self, batch_input_shape):\n        n_inputs = batch_input_shape[-1]\n        self.reconstruct = keras.layers.Dense(n_inputs)\n        super().build(batch_input_shape)\n\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        reconstruction = self.reconstruct(Z)\n        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n        self.add_loss(0.05 * recon_loss)\n        return self.out(Z)","a0b99aa2":"def f(w1, w2):\n    return 3 * w1 ** 2 + 2 * w1 * w2","a6282b2e":"w1, w2 = 5, 3\neps = 1e-6","efd41c33":"(f(w1 + eps, w2) - f(w1, w2)) \/ eps","b1b0e954":"(f(w1, w2+eps) - f(w1,w2))\/eps","fceff757":"w1, w2 = tf.Variable(5.), tf.Variable(3.)\nwith tf.GradientTape() as tape:\n    z = f(w1, w2)\n    \n    gradients = tape.gradient(z, [w1, w2])","b15b8688":"gradients","585656c7":"@tf.custom_gradient\ndef my_better_softplus(z):\n    exp = tf.exp(z)\n    def my_softplus_gradients(grad):\n        return grad \/ (1 + 1 \/ exp)\n    return tf.math.log(exp + 1), my_softplus_gradients","7329e788":"THIS NOTEBOOK FOLLOWS CHAPTER 12 OF HANDS-ON MACHINE LEARNIGN WITH SCIKIT-LEARN, KERAS, AND TENSORFLOW, 2ND EDITION"}}