{"cell_type":{"b0e02e0f":"code","6f78b057":"code","5811d378":"code","a05058f0":"code","56e10445":"code","4bdd0593":"code","5d514728":"code","3cf26de3":"code","4f932eb7":"code","b0ac4aab":"code","692e828f":"code","6fe0d58c":"code","7da3c4ba":"code","5c5b7dac":"code","745f8399":"code","33dfc730":"code","576074cd":"code","83f144b1":"markdown","f2a5ae42":"markdown","3fa9684a":"markdown","8779c8d2":"markdown","7ece3e14":"markdown","96f1770c":"markdown","9680732b":"markdown","65e5da2c":"markdown","c5a5093b":"markdown","9ceb3fd2":"markdown","5e285f1e":"markdown","6345ac84":"markdown","3459bebf":"markdown","73bd5bcc":"markdown","9993ffe8":"markdown","9ea1c164":"markdown","a5d72010":"markdown"},"source":{"b0e02e0f":"import os,cv2\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n \nfrom sklearn.model_selection import train_test_split\n \n\nimport keras\n \nfrom keras import backend as K\n \nfrom keras.models import Sequential, load_model\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD,RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Reshape, Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, UpSampling2D, LeakyReLU\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\n \n# To handle image loading problem\nfrom PIL import Image, ImageFile\n \n%matplotlib inline","6f78b057":"os.environ['KAGGLE_CONFIG_DIR'] = '\/content'\n \n!kaggle datasets download -d ashishpatel26\/indian-movie-face-database-imfdb","5811d378":"# unzip the data\n!unzip \\*.zip && rm *.zip","a05058f0":"# make the needed folders\n% mkdir -p dataset\/{train\/{YOUNG,MIDDLE,OLD},validation\/{YOUNG,MIDDLE,OLD}}\n# % rm -r dataset\n# read the data from the csv file\ntrain_csv = pd.read_csv(\"train.csv\")\ntrain_csv[\"Class\"].unique()\ntrain_csv.head()","56e10445":"# split the data into training and validation\nX = np.array(train_csv['ID'])\ny = np.array(train_csv['Class'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","4bdd0593":"# organize the data\nfor x, y in zip(X_train, y_train):\n  shutil.copy2(\"train\/Train\/\"+x,\"dataset\/train\/\"+y)\nfor x, y in zip(X_test, y_test):\n  shutil.copy2(\"train\/Train\/\"+x,\"dataset\/validation\/\"+y)","5d514728":"batch_size=64\ntarget_size = (32, 32)\nclasses = [\"YOUNG\", \"MIDDLE\", \"OLD\"]\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ntrain_datagen = ImageDataGenerator( rescale = 1.\/255,\n                                    rotation_range=20, \n                                    width_shift_range=0.1,\n                                    height_shift_range=0.1,\n                                    shear_range=0.2,\n                                    zoom_range=0.2,\n                                    horizontal_flip=True,\n                                    fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator( rescale=1.\/255 )\n\nprint('training set:')\ntraining_set = train_datagen.flow_from_directory('dataset\/train',\n                                   target_size = target_size,\n                                   batch_size = batch_size,\n                                   class_mode = 'categorical',\n                                   classes=classes)\n\nprint('validation set:')\nvalidation_set = test_datagen.flow_from_directory('dataset\/validation',\n                                   target_size = target_size,\n                                   batch_size = batch_size,\n                                   class_mode = 'categorical',\n                                   classes=classes)\n\nprint(training_set.class_indices)","3cf26de3":"model = Sequential()\n\n# Convolutional and pooling layers\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = \"same\", activation ='relu', input_shape = (32,32,3)))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Conv2D(filters = 64, kernel_size= (3,3),padding = \"same\", activation = \"relu\"))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\",activation=\"relu\"))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\",activation=\"relu\"))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Fully Connected Layers\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(84, activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\n# softmax classifier\nmodel.add(Dense(3,activation=\"softmax\"))\n\nmodel.summary()","4f932eb7":"model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.0001),\tmetrics=[\"accuracy\"])\n\nhistory = model.fit(training_set,\n                    steps_per_epoch = len(training_set),\n                    validation_data = validation_set,\n                    validation_steps = len(validation_set),\n                    epochs = 70)","b0ac4aab":"history = model.history\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n \nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \n# plot the accuracy curves \nplt.plot(acc)\nplt.plot(val_acc)\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n \n# plot the loss curves \nplt.figure() \nplt.plot(loss)\nplt.plot(val_loss)\nplt.title('model error')\nplt.ylabel('categorical_crossentropy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","692e828f":"score = model.evaluate(validation_set, verbose=1)\nprint(\"Validation score:\", score[0])\nprint('Validation accuracy:', score[1])","6fe0d58c":"def predict(images, model):\n  prediction = model.predict(images)\n  prediction = prediction.argmax(axis = 1)\n  prediction_labels = [classes[prediction_index] for prediction_index in prediction]\n  return prediction_labels","7da3c4ba":"import random\nimg_path = 'Train\/Train\/' + random.choice(X_test)\nimage=cv2.imread(img_path)\nplt.imshow(image)\nimage=cv2.resize(image , (32,32))\nimage = [image]\nimage = np.array(image, dtype=\"float\") \/ 255.\npredict(np.array(image), model)","5c5b7dac":"model.save('my_model.h5')","745f8399":"np.save('my_model_history', history.history)","33dfc730":"old_model = load_model('my_model.h5')","576074cd":"old_history = np.load('my_model_history.npy', allow_pickle='TRUE')","83f144b1":"  load a model","f2a5ae42":"# Data segregation","3fa9684a":"  evaluating the model on the validation set","8779c8d2":"  load a history","7ece3e14":"1.   Split the training set into validation set and training set (80\/20) \n2.   Since there are 3 classes, each folder (Train\/Validation) will have three sub-folders (Young\/Middle\/Old)\n3.   These 3 sub-folders will contain the relevant images based on the image-label mapping file (train.csv)\n","96f1770c":"# Save and load a model","9680732b":"  save the model","65e5da2c":"  Training and validation curves","c5a5093b":"# Test the model","9ceb3fd2":"# Model building and training","5e285f1e":"  predict the age class of a random image","6345ac84":"# download the data","3459bebf":"# Data preprocessing and augmentation\n","73bd5bcc":"  save the history","9993ffe8":"\n1.   resize both training and validation images to the size (32, 32, 3)\n2.   normalize both training and validation images (by dividing over 255)\n3.   augment the training data by applying some random transformation to the images","9ea1c164":"# Model evaluation","a5d72010":"# Importing Essential Libraries"}}