{"cell_type":{"807880f8":"code","f2a1ec1f":"code","609233d8":"code","282f78fa":"code","d62da23e":"code","92538d95":"code","976bb6ac":"code","8013fcaf":"code","cd7b218e":"code","41dc6428":"code","072cf01c":"code","d4eb3ec8":"code","9346084d":"code","56da2395":"code","372cf2e9":"code","f45555a4":"code","e3e66588":"code","9ddb6842":"code","a685723e":"code","b60a2ee9":"code","35ec0f23":"code","cdb77568":"markdown","81bf75b7":"markdown","a7358719":"markdown","f37639eb":"markdown","b99e69a1":"markdown","3278fd75":"markdown","63eaf9d6":"markdown","c60ae986":"markdown","4533507d":"markdown"},"source":{"807880f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2a1ec1f":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets, neighbors, linear_model\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import plot_importance\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import make_column_transformer\nfrom imblearn.under_sampling import RandomUnderSampler \nfrom imblearn.over_sampling import SMOTE \n\nimport warnings  \nwarnings.filterwarnings('ignore')","609233d8":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","282f78fa":"df.info()","d62da23e":"df.describe()","92538d95":"df_ex = df.copy()\ndf_ex = df_ex[df_ex.gender != 'Other']\n\nfp = 100*df_ex[df_ex.stroke==1].shape[0]\/df_ex.shape[0]\nprint(\"The data has %f%% positive sample or a total of %d positive cases out of %d total cases\" \n      % (fp, fp*df_ex.shape[0]\/100, df_ex.shape[0]))","976bb6ac":"def splt1(ft, data, bins, lsc):\n    rows = ['Male','Female']\n    opt = list(map(lambda x: list(zip([0,1], [x]*2)), rows))\n    \n    opt = [item for lst in opt for item in lst]\n    \n    color_code = ['green', 'red', 'green', 'red']\n    \n    fig, axs = plt.subplots(2,2, sharey = True, figsize = (8,8))\n    for i in range(len(opt)):\n        splt = axs[i\/\/2,i%2]\n        sns.histplot(data = data,\n                     x = data[(data['gender'] == opt[i][1])\n                            & (data['stroke'] == opt[i][0])][ft], \n                     bins = bins,\n                     stat = 'probability',\n                     kde = True,\n                     log_scale = lsc,\n                     line_kws={\"color\": color_code[i], \"lw\": 3},\n                     color = color_code[i],\n                     label = 'stroke = ' + str(opt[i][0]),\n                     ax = axs[i\/\/2,i%2])\n        \n        splt.tick_params(axis ='x', which ='major', \n                   labelsize = 14, pad = 10)\n        splt.set_yticks([])\n        splt.legend(prop = {'size': 12})\n        \n    \n        if i > 1:\n            splt.set_xlabel(ft.title(), fontsize = 14)\n        else:\n            splt.set_xlabel(None)\n            splt.set_xticks([])\n            \n        if ((i == 0) or (i == 2)):\n            splt.set_ylabel(rows[i\/\/2].title(), fontsize = 14)\n        else:\n            splt.set_ylabel(None)\n\n    plt.tight_layout()\n    plt.show()","8013fcaf":"splt1('age', df_ex, 30, False)","cd7b218e":"splt1('bmi', df_ex, 30, False)","41dc6428":"splt1('avg_glucose_level', df_ex, 30, False)","072cf01c":"fig = plt.figure(figsize = (7,7))\nsns.heatmap((df_ex.iloc[:,1:]).corr(), annot=True)\nplt.show()","d4eb3ec8":"cat_cols = df_ex.select_dtypes(\n    include = ['object']).columns.values\nprint(\"The categorical variables are\\n%s\" %cat_cols)","9346084d":"df_ex['smoking_status'] = df_ex['smoking_status'].replace(['formerly smoked'], 'smokes')\ndf_ex['bmi'] = df_ex['bmi'].fillna(df_ex['bmi'].median())","56da2395":"X = df_ex.drop(['id', 'stroke'], axis = 1)\nY = df_ex['stroke']\n\nlb_enc = LabelEncoder()\nfor col in cat_cols:\n    X[col] = lb_enc.fit_transform(X[col])","372cf2e9":"r_state = 0\nrand_us = RandomUnderSampler(random_state = r_state)\nX_urs, Y_urs = rand_us.fit_resample(X, Y)\nprint(Y_urs.value_counts())","f45555a4":"rand_smote = SMOTE(random_state = r_state)\nX_ors, Y_ors = rand_smote.fit_resample(X, Y)\nprint(Y_ors.value_counts())","e3e66588":"X_train_urs, X_test_urs, Y_train_urs, Y_test_urs = train_test_split(\n    X_urs, Y_urs, test_size = 0.25, random_state = r_state)\n\nX_train_ors, X_test_ors, Y_train_ors, Y_test_ors = train_test_split(\n    X_ors, Y_ors, test_size = 0.25, random_state = r_state)\n\ndef m_run(model, _name, X_train, Y_train, X_test, Y_test):\n    m_train = model.fit(X_train, Y_train)\n    m_pred = model.predict(X_test)\n    m_acc = accuracy_score(Y_test, m_pred)\n    m_prc = precision_score(Y_test, m_pred, average = 'binary')\n    m_rec = recall_score(Y_test, m_pred, average = 'binary')\n    m_f1s = f1_score(Y_test, m_pred, average = 'binary')\n    print(\"Classifier Type: %s\" % _name)\n    print(\"\\n\\\n    F1 score: %f\\n\\\n    Precision score: %f\\n\\\n    Recall score: %f\\n\\\n    Accuracy score: %f\" % (m_f1s, m_prc, m_rec, m_acc))\n    plot_confusion_matrix(xgb, X_test, Y_test)","9ddb6842":"xgb = XGBClassifier(learning_rate = np.random.randint(10,50)\/100,\n                    # using a random learning rate to ensure reaching global minima\n                    n_estimators = 200, \n                    max_depth = 10,\n                    objective='binary:logistic', eval_metric = 'auc',\n                    random_state = r_state)","a685723e":"m_run(xgb, \"XGBoost with random under sampling\", X_train_urs, Y_train_urs, X_test_urs, Y_test_urs)","b60a2ee9":"m_run(xgb, \"XGBoost with random over sampling\", X_train_ors, Y_train_ors, X_test_ors, Y_test_ors)","35ec0f23":"_train = xgb.fit(X_train_ors, Y_train_ors)\nplot_importance(xgb)\nplt.show()","cdb77568":"Let us mark the categorical variables","81bf75b7":"The distribution of data in the two gender groups seems similar.  \nNote: The bottom row of the subplots should print ylabel as \"Female\", it's working when I am running it on my laptop! ","a7358719":"We change the smoking_status of \"formerly smoked\" to \"smokes\" and replace the missing 'bmi' values with the median","f37639eb":"We will evaluate the models using the f1-score, while keeping track of precision & recall scores and the confusion matrix. Let us split the data into train & test sets, and define the model evaluation","b99e69a1":"Next we define the feature matrix, X (dropping 'id' and 'stroke') and the target, Y ('stroke'). The categorical variable values are transformed to numerical by applying LabelEncoder ","3278fd75":"This is a case of *imbalanced classification*, hence we need to use either random under \/ over sampling to make sure that both the train and test set have a balanced distribution of the target (stroke = 0 \/ 1). We will compare both under sampling and over sampling using **RandomUnderSampler** and **SMOTE**","63eaf9d6":"## Conclusion:\nThe XGBoost classifier coupled with random over sampling by SMOTE provides a F1-score of 0.96, predicting the occurrence of stroke with ~97% accuracy and the negative cases with 95% accuracy. The key features affecting the prediction are (1) avg_glucose_level, (2) age, and (3) bmi. \n\nFrom exploratory analysis, it seemed that the data distribution for both the genders are quite similar. However, it might be useful to segment the data by age groups and run the classifier on each groups. First, predicting the vulnerable cases in a younger age group (say, < 50) will improve the usefulness of the model, because people in that age group may not go for regular heath check up. Second, it might increase the accuracy of the model when applied to each of the age groups.     ","c60ae986":"Lastly we would like to find out what are the key features used in our model ","4533507d":"Let us classify and plot the numerical variables to ensure that the two groups behave similarly "}}