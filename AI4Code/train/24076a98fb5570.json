{"cell_type":{"d153537e":"code","eff29b64":"code","9c8a1e23":"code","aa89b0ba":"code","0d60945b":"code","5526a874":"code","00fc9c9b":"code","fff38fd8":"code","0b447aa4":"code","f080575a":"code","def5a970":"code","095fd4bb":"code","05ed0ec4":"code","5557c097":"code","5ba5d39a":"code","5e29a7b6":"code","1f2e01e1":"code","f26d351b":"code","0de2d7b5":"code","23c7c196":"code","adb1ceb0":"code","a12107fb":"code","405d8092":"code","66491790":"code","e919a025":"code","ecc46303":"code","eaf5a5ca":"code","e78de893":"code","0df5fc55":"code","6d71b488":"code","e3940068":"code","64f388ab":"code","4e153196":"code","d88e9e81":"code","a3790936":"code","aa362b03":"code","6c4a8599":"code","85415155":"code","068ab23f":"code","095c9bba":"code","7dca9e77":"code","27faed66":"code","4da7af2d":"code","a410afa3":"code","7e75865e":"code","9649414e":"code","97f582c6":"code","d7cb2a0c":"code","9690f6f4":"code","1d004ce5":"code","868923de":"code","dfb5835b":"code","37173de9":"markdown","2d41bf91":"markdown","cd552d66":"markdown","9e755bc4":"markdown","2a2e6f70":"markdown","af15a728":"markdown","07c61935":"markdown","173224d2":"markdown","bc9af9b9":"markdown","ec4758b3":"markdown","68dc165b":"markdown","8d5a183f":"markdown","17f860aa":"markdown","b98a777e":"markdown"},"source":{"d153537e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eff29b64":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nprint(train_data.shape)\nprint(train_data.columns)\nprint(test_data.shape)\ntrain_data.head()","9c8a1e23":"train_data = train_data.drop('id',axis = 1)\ntest_data = test_data.drop('id',axis = 1)","aa89b0ba":"test_data = test_data.fillna('')\ntrain_data = train_data.fillna('')","0d60945b":"#keywords = list(train_data['keyword'].unique())\n#print(keywords)","5526a874":"import re\ndef keyword_correction(x):\n    try:\n        x = x.split(\"%20\")\n        x = ' '.join(x)\n        return x\n    except:\n        return x","00fc9c9b":"train_data['keyword'] = train_data['keyword'].apply(lambda x: keyword_correction(x))\ntest_data['keyword'] = test_data['keyword'].apply(lambda x: keyword_correction(x))","fff38fd8":"#train_data['keyword'].unique()","0b447aa4":"#list(train_data['location'].unique())","f080575a":"from nltk.corpus import stopwords\nimport string\nfrom bs4 import BeautifulSoup\ndef text_cleaning(text):\n    forbidden_words = set(stopwords.words('english'))\n    if text:\n        text = ' '.join(text.split('.'))\n        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z0-9]', ' ', text.strip().lower())).strip()\n        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n        text = [word for word in text.split() if word not in forbidden_words]\n        return text\n    return []\n#clean data\n#this following cleaning is taken from https:\/\/www.kaggle.com\/nxhong93\/tweet-predict1\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"}\n\npuncts = puncts + list(string.punctuation)\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('\\d+', ' ', x)\n    return x\n\n\ndef replace_typical_misspell(text):\n    mispellings_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n\n    def replace(match):\n        return mispell_dict[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef remove_space(string):\n    string = BeautifulSoup(string).text.strip().lower()\n    string = re.sub(r'((http)\\S+)', 'http', string)\n    string = re.sub(r'\\s+', ' ', string)\n    return string\n\n\ndef clean_data(df, columns: list):\n    \n    for col in columns:\n        df[col] = df[col].apply(lambda x: remove_space(x).lower())        \n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n        df[col] = df[col].apply(lambda x: clean_text(x))\n        \n    return df","def5a970":"for col in ['location','text']:\n    train_data[col] = train_data[col].apply(lambda x: ' '.join(text_cleaning(x)))\n    test_data[col] = test_data[col].apply(lambda x: ' '.join(text_cleaning(x)))\ntrain_data = clean_data(train_data,['keyword','text'])\ntest_data = clean_data(test_data,['keyword','text'])","095fd4bb":"import spacy\nnlp = spacy.load('en_core_web_lg')","05ed0ec4":"def location_detection(text):\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        entities.append(ent)\n    if len(entities)>0:\n        return 1\n    return 0","5557c097":"train_data['original_locations'] = train_data['location'].apply(lambda x: location_detection(x))\ntest_data['original_locations'] = test_data['location'].apply(lambda x: location_detection(x))","5ba5d39a":"#list(train_data['location'].unique())","5e29a7b6":"spam_locations = ['place','room','home','somewhere','nowhere','everywhere','location',\n                  'dope','kidding','moon','wherever','dimension','world','fvck','fuck','beside']\ndef is_location_spammy(text):\n    for word in spam_locations:\n        if word in text:\n            return 1\n    return 0","1f2e01e1":"train_data['Is_location_spam'] = train_data['location'].apply(lambda x: is_location_spammy(x))\ntest_data['Is_location_spam'] = test_data['location'].apply(lambda x: is_location_spammy(x))","f26d351b":"def digit_counter(text):\n    \"\"\"detects any digit in any token and counts\n       once par token.\"\"\"\n    sum_number = 0\n    doc = nlp(text)\n    for token in doc:\n        sum_number += bool(re.search(r'\\d', token.text))*1\n    return sum_number    ","0de2d7b5":"train_data['digit_count_location'] = train_data['location'].apply(lambda x: digit_counter(x))\ntest_data['digit_count_location'] = test_data['location'].apply(lambda x: digit_counter(x))","23c7c196":"disaster_tweets =' '.join(train_data[train_data['target'] == 1]['text'].tolist())\nnon_disaster_tweets = ' '.join(train_data[train_data['target'] == 0]['text'].tolist())","adb1ceb0":"import nltk\ndef return_top_words(text,words = 10):\n    allWords = nltk.tokenize.word_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')\n    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in allWords if w not in stopwords)    \n    mostCommontuples= allWordExceptStopDist.most_common(words)\n    mostCommon = [tupl[0] for tupl in mostCommontuples]\n    return mostCommon","a12107fb":"top_50_disaster_words = return_top_words(disaster_tweets,50)\ntop_50_nondisaster_words = return_top_words(non_disaster_tweets,50)","405d8092":"#top_50_disaster_words","66491790":"#top_50_nondisaster_words","e919a025":"top_200_disaster_words = return_top_words(disaster_tweets,400)\ntop_200_nondisaster_words = return_top_words(non_disaster_tweets,400)\ntop_disaster_exclusive = list(set(top_200_disaster_words).difference(set(top_200_nondisaster_words)))\ntop_nondisaster_exclusive = list(set(top_200_nondisaster_words).difference(set(top_200_disaster_words)))","ecc46303":"#top_disaster_exclusive","eaf5a5ca":"#top_nondisaster_exclusive","e78de893":"total_vocab = top_disaster_exclusive + top_nondisaster_exclusive","0df5fc55":"for word in total_vocab:\n    train_data['Is_'+word+'_present'] = train_data['text'].apply(lambda x: (word in x)*1)\n    test_data['Is_'+word+'_present'] = test_data['text'].apply(lambda x: (word in x)*1)","6d71b488":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf_idf = TfidfVectorizer(ngram_range=(1, 3),\n                         binary=True,\n                         max_features = 5000,\n                         smooth_idf=False)\nX_train_tfidf = tf_idf.fit_transform(train_data['text'])\nX_test_tfidf = tf_idf.transform(test_data['text'])\ntf_kw = TfidfVectorizer(ngram_range = (1,2),\n                        binary = True,\n                        max_features = 1500,\n                        smooth_idf = False)\nkw_train_tfidf = tf_kw.fit_transform(train_data['keyword'])\nkw_test_tfidf = tf_kw.transform(test_data['keyword'])\ntf_location = TfidfVectorizer(ngram_range = (1,2),\n                              binary = True,\n                              max_features = 1500,\n                              smooth_idf = False)\nlocation_train_tfidf = tf_location.fit_transform(train_data['location'])\nlocation_test_tfidf = tf_location.transform(test_data['location'])","e3940068":"train_data = pd.concat([train_data,pd.DataFrame(X_train_tfidf.toarray(),\n                                                columns = ['text_contains_'+ str(text) for text in tf_idf.get_feature_names()]),\n                        pd.DataFrame(kw_train_tfidf.toarray(),\n                                     columns = ['keyword_contains_'+str(text) for text in tf_kw.get_feature_names()]),\n                        pd.DataFrame(location_train_tfidf.toarray(),\n                                     columns = ['location_contains_'+str(text) for text in tf_location.get_feature_names()])],axis = 1)\ntest_data = pd.concat([test_data,pd.DataFrame(X_test_tfidf.toarray(),\n                                              columns = ['text_contains_'+ str(text) for text in tf_idf.get_feature_names()]),\n                       pd.DataFrame(kw_test_tfidf.toarray(),\n                                    columns = ['keyword_contains_'+str(text) for text in tf_kw.get_feature_names()]),\n                       pd.DataFrame(location_test_tfidf.toarray(),\n                                    columns = ['location_contains_'+str(text) for text in tf_location.get_feature_names()])],axis = 1)","64f388ab":"#for col in train_data.columns:\n#    if col == 'text':\n#        print(train_data[col].describe())","4e153196":"def create_vec(dataframe):\n    texts = dataframe['text'].tolist()\n    vectors = []\n    for doc in nlp.pipe(texts):\n        vectors.append(list(doc.vector))\n    df = pd.DataFrame(vectors,columns = ['vec_'+str(i) for i in range(300)])\n    return df\nvec_train = create_vec(train_data)\nvec_test = create_vec(test_data)\ntrain_data = pd.concat([train_data,vec_train],axis = 1)\ntest_data = pd.concat([test_data,vec_test],axis = 1)","d88e9e81":"train_data = train_data.drop(['keyword','location','text'],axis = 1)\ntest_data = test_data.drop(['keyword','location','text'],axis = 1)","a3790936":"X_train = train_data.drop('target',axis = 1)\nY_train = train_data['target']\nprint('target' in test_data.columns)","aa362b03":"len(train_data.columns)","6c4a8599":"from sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.metrics import classification_report\nforest = rfc(n_estimators = 128,max_depth = 8,min_samples_split = 15,\n             class_weight = {0:1,1:1.6},oob_score = True)\nforest.fit(X_train,Y_train)\nprint(forest.oob_score_)\nY_pred_train = forest.predict(X_train)\nprint(classification_report(Y_pred_train,Y_train))","85415155":"#features = list(X_train.columns)\n#feature_importances = forest.feature_importances_\n#data = pd.DataFrame()\n#data['features'] = features\n#data['feature_importances'] = feature_importances\n#data = data.sort_values(by = 'feature_importances',ascending = False)\n#print(data)","068ab23f":"#bad_features = data[data['feature_importances']<0.001]['features'].tolist()","095c9bba":"#X_train_reduced = X_train.drop(bad_features,axis = 1)\n#test_data_reduced = test_data.drop(bad_features,axis = 1)","7dca9e77":"#X_train_reduced.shape","27faed66":"#forest = rfc(n_estimators = 128,max_depth = 5,min_samples_split = 15,\n#             class_weight = {0:1,1:1.53},\n#             oob_score = True)\n#forest.fit(X_train_reduced,Y_train)\n#print(forest.oob_score_)\n#Y_pred_train = forest.predict(X_train_reduced)\n#print(classification_report(Y_pred_train,Y_train))","4da7af2d":"#taken from https:\/\/www.kaggle.com\/vishalsiram50\/fine-tuning-bert-88-accuracy\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\ndef get_auc_CV(model,X_train,Y_train):\n    \"\"\"\n    Return the average AUC score from cross-validation.\n    \"\"\"\n    # Set KFold to shuffle data before the split\n    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n\n    # Get AUC scores\n    auc = cross_val_score(\n        model, X_train, Y_train, scoring=\"roc_auc\", cv=kf)\n\n    return auc.mean()","a410afa3":"#from sklearn.naive_bayes import MultinomialNB as MNB\n#from sklearn.metrics import classification_report\n#for alpha in [0.001,0.1,1]:\n#    print(alpha)\n#    clf = MNB(alpha = alpha)\n#    auc = get_auc_CV(clf,X_train,Y_train)\n#    print(auc)\n#    clf.fit(X_train,Y_train)\n#    Y_pred_train = clf.predict(X_train)\n#    print(classification_report(Y_train,Y_pred_train))","7e75865e":"#clf = MNB(alpha = 0.1)\n#clf.fit(X_train,Y_train)\n#Y_pred_train = clf.predict(X_train)\n#print(classification_report(Y_train,Y_pred_train))","9649414e":"#code taken from https:\/\/www.kaggle.com\/lucidlenn\/data-analysis-and-classification-using-xgboost\nimport time\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nxgb = XGBClassifier(n_estimators=200,learning_rate = 0.2,max_depth = 8)\ntraining_start = time.perf_counter()\nxgb.fit(X_train, Y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npred_final = xgb.predict(test_data)\npred_train = xgb.predict(X_train)\nprint(classification_report(Y_train,pred_train))\nprediction_end = time.perf_counter()\n#acc_xgb = (preds == y_test).sum().astype(float) \/ len(preds)*100\nxgb_train_time = training_end-training_start\nxgb_prediction_time = prediction_end-prediction_start\n#print(\"XGBoost's prediction accuracy is: %3.2f\" % (acc_xgb))\nprint(\"Time consumed for training: %4.3f\" % (xgb_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (xgb_prediction_time))","97f582c6":"get_auc_CV(xgb,X_train,Y_train)","d7cb2a0c":"#test_data.isna().sum().sum()","9690f6f4":"#test_data.shape","1d004ce5":"#test_prediction = clf.predict(test_data)","868923de":"sample_submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nprint(sample_submission.columns)","dfb5835b":"dataframe = pd.DataFrame()\ndataframe['id'] = sample_submission['id']\ndataframe['target'] = pred_final\ndataframe.to_csv(\"final_submission.csv\",index = False)","37173de9":"## <a id='section5'>vector embedding<\/a>\nIn this section we create 300 dimensional vector embedding from the text feature; and include these as 300 features to the original dataset. Reason we include vector embedding is to give a sense of original meaning; and to gather any semantic structure in the disaster tweets if present. Here we are using spacy's en_core_web_lg model's embeddings. For learning more about vector embeddings using spacy, read [this post](https:\/\/shyambhu20.blogspot.com\/2020\/10\/calculate-word-similarity-spacy-nlp.html).<br\/>","2d41bf91":"We have added the vector embedding with negative values as features. That's why we can't use naive bayes anymore; as that only allows non-negative values of features. You can use maxabsscaler as we suggested above and try out this part. Also tune the naivebayes for alpha value. ","cd552d66":"## <a id = 'section3'>feature-generation<\/a>\nAs we have seen and explored the location feature, we will generate a few features out of that to capture information.<br\/>\nSo location seems to have legit locations, as well as garbage words. So we will detect if there is entities in the location, as legit location should give more credibility to the tweets.","9e755bc4":"## <a id = 'section4'>NLTK frequency analysis and TF-IDF feature generation<\/a>\nNow that we have sort of exhausted the location feature; let's check the original tweets. We will first check the different frequent words occuring in disaster tweets vs non-disaster tweets. We will use NLTK's freqdist() function to do this. Then we will generate the tfidf features using sklearn's tfidf vectorizer.","2a2e6f70":"So we can approach a 77% accuracy using these features. Let's check the feature importances for the features and drop the low performing features.","af15a728":"so 0.1 has the highest auc, so we will go with alpha = 0.1. Let's train this model and create submission.","07c61935":"there seems to be web addresses available as locations as well. We will create a feature to capture if there is any web address in the location text. On reading the locations manually, www, amazon, youtube, twitch,gmail are some associated words to website addresses.<br\/>\nAlso, most spammy locations contain these words: <br\/>\nplace, room, home, somewhere, dope,nowhere,location,kidding,moon,searching bae,gotham city,wherever,5th dimension,anywhere,idn,spying thoughts,beside,happily married 2 kids,'c h c g','playa','visit youtube channel',fvck,fuck,world.<br\/>\nSome of these are too specific, but other words we will use as bag of words for signalling spam location.<br\/>\nsocial media locations are also mentioned:<br\/>\nusually the locations are mentioned with instagram, snapchat.<br\/>\nAlso, another thing is that, if location is full numeric, then it maybe very well be latitude and longitudes.<br\/>","173224d2":"So there are some more negative words in the disaster text, and words in non-disaster text are less negative. We will now check what are the top words not occurring in the other categories. For that we will use set differences with top 400 words. The name of the variable is top_200 as initially I created using top 200 words; but to increase accuracy I included 400. You will have to reset the values according to your vocabulary size for the problem in hand.","bc9af9b9":"Let's train an Xgboost model with this data too.","ec4758b3":"## <a id='section6'>Extensive Modeling zone<\/a>\nIn this portion, we will try out random forest classifier model, which achieves around 77% accuracy with fine-tuning.<br\/>\nThen we have the code for naive bayes but it is commented out, as with vector embeddings having negative values; naive bayes can't run. If you do want to run and experiment it, use maxabscaler() to scale all the features as to non-negative values and then you can run it. <br\/>\nFinally, we will run the xgboost code. We have not done much xgboost fine-tuning, but you can experiment on that to try and get a higher score in it.<br\/>","68dc165b":"## <a id ='section2'>Extensive Text cleaning<\/a>\nIn this part, we will follow some extensive text cleaning, part of which we have taken from [this awesome notebook](https:\/\/www.kaggle.com\/nxhong93\/tweet-predict1). ","8d5a183f":"## <a id = section1>Basic data exploration<\/a>\nIn this part, we will read keyword, location and texts; do some preliminary cleaning. For example, on printing keyword in the above code block you can see that in keyword, gaps are filled with %20 sign. So in this next function we are cleaning the keywords by replacing that with space.","17f860aa":"Also, as we noted if all the words in the location are numbers, then it can be a latitude longitude point. As there are different version of that, we will count the number of digit tokens, as well as we will count if the whole text is just digits.","b98a777e":"## NLP with disaster tweets:\nNLP with disaster tweets is the beginner's dataset for learning and applying NLP techniques. In this notebook, we will explore several nlp applications and libraries. Here is a list of things we have covered in this notebook:<br\/>\n### sections:\n(1) [basic data exploration](#section1)<br\/>\nWe go over the different features present in the dataset. Some of the codes are commented out currently; which you can uncomment and explore. In this part, we normally explore the dataset and do some basic transformations.<br\/>\n(2) [extensive text cleaning](#section2)<br\/>\nIn this part, we import NLTK( a text cleaning library), beautifulsoup, regex( text string manipulation library) and then perform extensive text cleaning. We have taken part of the cleaning code from another high score(0.84) notebook. If you are just starting out in NLP, this part will give you a good lesson on text cleaning as well as you can reuse some of this code in normal NLP also.<br\/>\n(3)[feature-generation](#section3)<br\/>\nIn this part, we explore the location feature and generate several features out of it to capture extra information about the tweet's location.<br\/>\n(4)[NLTK frequency analysis and tf-idf](#section4)<br\/>\nIn this part, we have used the NLTK library to find out the top n words appearing in each of the classes; i.e. disaster tweets and non-disaster tweets. Top frequency words make a very good feature in identifying text classes; as their presence denotes high signal for the class in which they occur. Check this part's code to understand in details.<br\/>\nWe have also sklearn's tfidf vectorizer to generate tfidf features from keyword, location and main text. check the code to understand how we have done it.<br\/>\n(5)[vector embedding creation using spacy](#section5)<br\/>\nIn this part, we use spacy's large english model to create 300 dimensional vector embedding for the twitter texts; and add these as feature to the dataset.<br\/>\n(6)[Modeling with random forest, naive bayes and xgboost](#section6)<br\/>\nIn this part, we have done modeling and rough fine tuning with random forest; added commented code and experimentation guidance for naive-bayes and finally trained and created submission file using xgboost model.<br\/>\n### Resources:\n(1) [Deep understanding of tfidf vectorizer](https:\/\/medium.com\/@cmukesh8688\/tf-idf-vectorizer-scikit-learn-dbc0244a911a)<br\/>\n(2) [introduction to spacy](https:\/\/shyambhu20.blogspot.com\/2020\/09\/introduction-to-spacy-basic-nlp-usage.html)<br\/>\n(3)[xgboost modeling documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/model.html)<br\/>\n(4)[sklearn naive bayes documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html)<br\/>\n(5)[why use naive bayes for text classification](https:\/\/monkeylearn.com\/text-classification-naive-bayes\/)<br\/>\n(6) [how to create inbound links in kaggle notebook](https:\/\/sebastianraschka.com\/Articles\/2014_ipython_internal_links.html#top)"}}