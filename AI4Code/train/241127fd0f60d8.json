{"cell_type":{"5c9ab526":"code","80fd33d7":"code","ab487f37":"code","bcd24ae0":"code","549849d1":"code","90624906":"code","1eb65122":"code","81e492ab":"code","8a61f043":"code","ade12b1f":"code","bd9cd115":"code","9e8ce36a":"code","d284c141":"code","b8497ec5":"code","6562cd64":"code","ddeb5bb9":"code","d6fa275b":"code","d2a883bc":"code","51e96e24":"code","f55e4aef":"code","34d1fdff":"code","80ed6bb1":"code","3505bd15":"code","f1ed7948":"code","c8a6ed04":"code","0aeb33b4":"code","c448e5ef":"code","eee5ceac":"code","5cde78ea":"code","fc2263d6":"code","febc960d":"code","bc714f3d":"code","e26061ac":"code","41b6d43b":"code","a3742750":"code","90fb413b":"code","73af71e7":"code","c22e3c73":"code","7d69e1d6":"code","a7f24842":"code","c02856d1":"code","0ca7a00a":"code","c77cc116":"code","3bdcecf0":"code","01dff41b":"code","c877fcd2":"markdown","bc7c7bea":"markdown","0e5cdc38":"markdown"},"source":{"5c9ab526":"#Importing Necessary Libraries.\nfrom PIL import Image\nimport numpy as np\nimport os\nimport cv2\nimport keras\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout","80fd33d7":"print(os.listdir('..\/input\/cell_images\/cell_images'))","ab487f37":"infectados = os.listdir('..\/input\/cell_images\/cell_images\/Parasitized\/')\nsaudaveis = os.listdir('..\/input\/cell_images\/cell_images\/Uninfected\/')","bcd24ae0":"data = []\nlabels = []\n\n#we'll save 3 images from each image, one rotated 30\u00ba and another rotated 60\u00b0 from the original\n\nfor i in infectados:\n    try:\n    \n        image = cv2.imread(\"..\/input\/cell_images\/cell_images\/Parasitized\/\"+i)\n        image_array = Image.fromarray(image , 'RGB')\n        resize_img = image_array.resize((50 , 50))\n        rotated30 = resize_img.rotate(30)\n        rotated60 = resize_img.rotate(60)\n        blur = cv2.blur(np.array(resize_img) ,(10,10))\n        data.append(np.array(resize_img))\n        data.append(np.array(rotated30))\n        data.append(np.array(rotated60))\n        data.append(np.array(blur))\n        labels.append(1)\n        labels.append(1)\n        labels.append(1)\n        labels.append(1)\n        \n    except AttributeError:\n        print('')\n    \nfor s in saudaveis:\n    try:\n        \n        image = cv2.imread(\"..\/input\/cell_images\/cell_images\/Uninfected\/\"+s)\n        image_array = Image.fromarray(image , 'RGB')\n        resize_img = image_array.resize((50 , 50))\n        rotated30 = resize_img.rotate(30)\n        rotated60 = resize_img.rotate(60)\n        data.append(np.array(resize_img))\n        data.append(np.array(rotated30))\n        data.append(np.array(rotated60))\n        labels.append(0)\n        labels.append(0)\n        labels.append(0)\n        \n    except AttributeError:\n        print('')","549849d1":"celulas = np.array(data)\nclasses = np.array(labels)\n","90624906":"import matplotlib.pyplot as plt\nplt.figure(1 , figsize = (15 , 9))\nn = 0 \nfor i in range(49):\n    n += 1 \n    r = np.random.randint(0 , celulas.shape[0] , 1)\n    plt.subplot(7 , 7 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    plt.imshow(celulas[r[0]])\n    plt.title('{} : {}'.format('Infectados' if classes[r[0]] == 1 else 'Saudaveis' ,\n                               classes[r[0]]) )\n    plt.xticks([]) , plt.yticks([])\n    \nplt.show()","1eb65122":"plt.figure(1, figsize = (15 , 7))\nplt.subplot(1 , 2 , 1)\nplt.imshow(celulas[0])\nplt.title('Infectada')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(1 , 2 , 2)\nplt.imshow(celulas[60000])\nplt.title('Saudavel')\nplt.xticks([]) , plt.yticks([])\n\nplt.show()","81e492ab":"np.random.seed(0)\n\nn = np.arange(celulas.shape[0])\nnp.random.shuffle(n)\ncells = celulas[n]\nlabels = classes[n]","8a61f043":"cells = cells.astype(np.float32)\nlabels = labels.astype(np.int32)\ncells = cells\/255","ade12b1f":"from sklearn.model_selection import train_test_split\n\nX , x_test , y , y_test = train_test_split(cells , labels , \n                                            test_size = 0.2 ,\n                                            random_state = 111)\n\nX_tr , X_val , y_tr , y_val = train_test_split(X , y , \n                                                    test_size = 0.5 , \n                                                    random_state = 111)","bd9cd115":"num_classes=len(np.unique(labels))\nlen_data=len(cells)\n\ny_tr = keras.utils.to_categorical(y_tr,num_classes)\ny_val = keras.utils.to_categorical(y_val,num_classes)\ny_test = keras.utils.to_categorical(y_test,num_classes)","9e8ce36a":"print(y_tr.shape, y_val.shape, X_tr.shape, X_val.shape)","d284c141":"#creating the architecture of our neural network \n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters=128,kernel_size=2,padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\n\nmodel.add(Dense(500,activation=\"relu\"))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(2,activation=\"sigmoid\")) # we're using 0 to healthy cells and 1 to infected cells \nmodel.summary()","b8497ec5":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","6562cd64":"history = model.fit(X_tr,y_tr,validation_data=(X_val, y_val),batch_size=50,epochs=10,verbose=1)","ddeb5bb9":"score = model.evaluate(x_test, y_test, verbose=1)","d6fa275b":"pred = model.predict(x_test, verbose=1)","d2a883bc":"from keras.models import load_model\nmodel.save('cells.h5')","51e96e24":"# Plot training & validation accuracy values\nplt.figure(figsize=(10,10))\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","f55e4aef":"# Plot training & validation loss values\nplt.figure(figsize=(10,10))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","34d1fdff":"model.summary()","80ed6bb1":"from keras.models import Model\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(X_tr[20].reshape(1,50,50,3))\n \ndef display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n            activation_index += 1","3505bd15":"plt.imshow(X_tr[20][:,:,:]);","f1ed7948":"display_activation(activations, 4, 4, 0)","c8a6ed04":"display_activation(activations, 4, 4, 1)","0aeb33b4":"display_activation(activations, 4, 4, 2)","c448e5ef":"display_activation(activations, 4, 4, 3)","eee5ceac":"display_activation(activations, 4, 4, 4)","5cde78ea":"from sklearn import metrics\nconf = metrics.confusion_matrix(y_test[:,0], np.around(pred[:,0]))\nconf_norm = conf\/conf.sum(axis=1)\nprint(conf_norm)","fc2263d6":"from keras.applications.mobilenet import MobileNet\n","febc960d":"mobile_model = MobileNet(input_shape=(50,50,3), alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=False, weights=None, input_tensor=None, pooling=None, classes=2)","bc714f3d":"x = mobile_model.output\nx = Flatten()(x)\npreds = Dense(2,activation='softmax')(x) #final layer with softmax activation\nmobile_model=Model(inputs=mobile_model.input,outputs=preds)\nmobile_model.summary()","e26061ac":"mobile_model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\nhistory_mobile = mobile_model.fit(X_tr,y_tr,validation_data=(X_val, y_val),batch_size=128,epochs=10,verbose=1)","41b6d43b":"score_mobile = mobile_model.predict(x_test, verbose=1)","a3742750":"conf_mob = metrics.confusion_matrix(y_test[:,0], np.around(score_mobile[:,0]))\nconf_mob_norm = conf_mob\/conf_mob.sum(axis=1)\nprint(conf_mob_norm)","90fb413b":"layer_outputs = [layer.output for layer in mobile_model.layers[1:]]\nactivation_model = Model(inputs=mobile_model.input, outputs=layer_outputs)\nactivations_mob = activation_model.predict(X_tr[20].reshape(1,50,50,3))\nplt.imshow(X_tr[20][:,:,:]);\n","73af71e7":"display_activation(activations_mob, 8, 4, 1)","c22e3c73":"display_activation(activations_mob, 8, 4, 2)","7d69e1d6":"display_activation(activations_mob, 8, 4, 3)","a7f24842":"display_activation(activations_mob, 8, 8, 10)","c02856d1":"display_activation(activations_mob, 8, 8, 20)","0ca7a00a":"# Plot training & validation accuracy values\nplt.figure(figsize=(10,10))\nplt.plot(history_mobile.history['acc'])\nplt.plot(history_mobile.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","c77cc116":"# Plot training & validation loss values\nplt.figure(figsize=(10,10))\nplt.plot(history_mobile.history['loss'])\nplt.plot(history_mobile.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","3bdcecf0":"model.evaluate(x_test, y_test, verbose=1)","01dff41b":"mobile_model.evaluate(x_test, y_test, verbose=1)","c877fcd2":"In this kernel, I wanto to compare a model created by myself to solve this problem and compare with another famous model, MobileNet. I've done this work for a school work at *State University of Campinas - UNICAMP*. At my local host, I've tried to compare another architechtures, like VGG16, VGG19 and Xception, but they didn't converged. I tried to do some transfer learning too, but this dataset have images with less pixels than the architechtuce could operate.  ","bc7c7bea":"Now, importing MobileNet, we'll compare the performance of this famous model in this *dataset*","0e5cdc38":"Importing the libraries and the data\n"}}