{"cell_type":{"6b6efb80":"code","36eb98a0":"code","115df6d3":"code","8f0673e5":"code","fee36ec3":"code","7703ff91":"code","79eb4e80":"code","2067c67b":"code","b14c2992":"code","6f2dc558":"code","9171c70e":"code","778106ab":"code","fef7c5c6":"code","064f6e5b":"code","883c5804":"code","5546ccbb":"code","435f35df":"code","decab2a6":"code","731d8a68":"code","8893d621":"code","24ed3db5":"code","1c6c3f3c":"code","a1b2871c":"code","c372cb85":"code","36c466b5":"markdown","0625ab9f":"markdown","5e521859":"markdown","ab023f44":"markdown","66faa1ab":"markdown","49bd58d0":"markdown","6cb93d6c":"markdown","b59ec20e":"markdown"},"source":{"6b6efb80":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import interp, stats\nfrom itertools import cycle\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, auc\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom pylab import rcParams\n\n%matplotlib inline\nsns.set(style='whitegrid', palette='muted', font_scale=1.5)","36eb98a0":"# load the data\ndata = pd.read_csv(\"..\/input\/creditcard.csv\")","115df6d3":"# get column names\ncolNames = data.columns.values\ncolNames","8f0673e5":"# get dataframe dimensions\nprint (\"Dimension of dataset:\", data.shape)","fee36ec3":"# get attribute summaries\nprint(data.describe())","7703ff91":"# get class distribution\nprint (\"Normal transaction:\", data['Class'][data['Class']==0].count()) #class = 0\nprint (\"Fraudulent transaction:\", data['Class'][data['Class']==1].count()) #class = 1","79eb4e80":"# separate classes into different datasets\nnormal_class = data.query('Class == 0')\nfraudulent_class = data.query('Class == 1')\n\n# randomize the datasets\nnormal_class = normal_class.sample(frac=1,random_state=69)\nfraudulent_class = fraudulent_class.sample(frac=1,random_state=69)","2067c67b":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,9))\nf.suptitle('Time of transaction vs Amount by class')\n\nax1.scatter(fraudulent_class.Time, fraudulent_class.Amount)\nax1.set_title('Fraud')\n\nax2.scatter(normal_class.Time, normal_class.Amount)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","b14c2992":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,9))\nf.suptitle('Amount per transaction by class')\n\nbins = 50\n\nax1.hist(fraudulent_class.Amount, bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(normal_class.Amount, bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","6f2dc558":"data = data.drop(['Time'], axis=1)\ndata['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))","9171c70e":"# separate classes into different datasets\nnormal_class = data.query('Class == 0')\nfraudulent_class = data.query('Class == 1')\n\n# randomize the datasets\nnormal_class = normal_class.sample(frac=1,random_state=69)\nfraudulent_class = fraudulent_class.sample(frac=1,random_state=69)","778106ab":"X = data.drop(['Class'], axis = 1)\n\ny = data['Class']","fef7c5c6":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    plt.figure(figsize=(12, 9), dpi=80)\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(X[y==l, 0], X[y==l, 1], c=c, label=l, marker=m)\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","064f6e5b":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority', random_state=69)\nX_sm, y_sm = smote.fit_sample(X, y)\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')","883c5804":"X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, random_state=69)","5546ccbb":"# See category counts for test data\ncategory, records = np.unique(y_test, return_counts= True)\ncat_counts = dict(zip(category,records))\n\nprint(cat_counts)","435f35df":"rf_model = RandomForestClassifier(n_estimators=100)","decab2a6":"rf_model.fit(X_train,y_train)","731d8a68":"pred_rf = rf_model.predict(X_test)","8893d621":"print(confusion_matrix(y_test,pred_rf))\nprint()\nprint(classification_report(y_test,pred_rf))","24ed3db5":"print(\"Cohen's Kappa Score:\\t\",round(cohen_kappa_score(y_test,pred_rf),4)*100)\nprint()\nprint(\"R-Squared Score:\\t\",round(r2_score(y_test,pred_rf),4)*100)\nprint()\nprint(\"Area Under ROC Curve:\\t\",round(roc_auc_score(y_test,pred_rf),4)*100)","1c6c3f3c":"'''\n# Checking 10-fold Cross-Validation Score for this model\n\nkfold = StratifiedKFold(n_splits=5, random_state=69)\n\n# use area under the precision-recall curve to show classification accuracy\nscoring = 'roc_auc'\nresults = cross_val_score(rf_model, X_sm, y_sm, cv=kfold, scoring = scoring)\nprint( \"AUC: %.3f (%.3f)\" % (results.mean(), results.std()) )\n'''","a1b2871c":"'''\n# change size of Matplotlib plot\nfig_size = plt.rcParams[\"figure.figsize\"] # Get current size\n\nold_fig_params = fig_size\n# new figure parameters\nfig_size[0] = 15\nfig_size[1] = 10\n   \nplt.rcParams[\"figure.figsize\"] = fig_size # set new size\n'''","c372cb85":"'''\n# plot roc-curve\n# code adapted from http:\/\/scikit-learn.org\n\nmean_tpr = 0.0\nmean_fpr = np.linspace(0, 1, 100)\n\ncolors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])\nlw = 2\n\ni = 0\nfor (train, test), color in zip(kfold.split(X_sm, y_sm), colors):\n    probas_ = rf_model.fit(X_sm[train], y_sm[train]).predict_proba(X_sm[test])\n    # Compute ROC curve and area under the curve\n    fpr, tpr, thresholds = roc_curve(y_sm[test], probas_[:, 1])\n    mean_tpr += interp(mean_fpr, fpr, tpr)\n    mean_tpr[0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=lw, color=color,\n             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n\n    i += 1\nplt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',\n         label='Luck')\n\nmean_tpr \/= kfold.get_n_splits(X_sm, y_sm)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',\n         label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n'''","36c466b5":"# Exploratory Data Analysis","0625ab9f":"### The above graph shows that most of the fraudulent transactions are of very low amount","5e521859":"# Oversampling to deal with class imbalance\n\nThe examples of the majority class, in this case the normal transactions, drastically outnumber the \nincidences of fraudulent transactions in our dataset. One of the strategies employed in the data science community is \nto generate synthetic data points for under-represented class to improve the learning function.","ab023f44":"# Importing required Libraries","66faa1ab":"### Random Forest Classifier","49bd58d0":"# Context\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n### Content\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n### Acknowledgements\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http:\/\/mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http:\/\/mlg.ulb.ac.be\/BruFence and http:\/\/mlg.ulb.ac.be\/ARTML\n\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015","6cb93d6c":"# Time to train and test the performance of various models","b59ec20e":"### The above graph shows that **Time** is irrelevent for detecting fraudulent transactions"}}