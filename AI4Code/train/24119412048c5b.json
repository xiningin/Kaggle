{"cell_type":{"2a8a7caa":"code","74be9c83":"code","572366a4":"code","2ece4135":"code","0654bf31":"code","37551182":"code","27cd817f":"code","15be2c14":"code","5e03b262":"code","ffa8b81e":"code","f8331db5":"code","c86f0588":"code","910477e9":"code","66387a43":"code","3a943c37":"code","ddaa21bc":"code","ed74db0a":"code","fdcffb31":"code","1266cdbd":"code","dfad71e8":"code","1ca584cd":"code","3f27969a":"code","dedd1fa1":"code","9ffa1f38":"code","e3cabe4e":"code","c37820b1":"code","dcaebbf6":"code","dd4e0199":"code","1e9ca4f4":"code","487570e5":"code","5c69c9da":"code","066af7ff":"code","92916e94":"code","d1410983":"code","b42c4926":"code","ef1f4a03":"code","3b50f05a":"code","ed58aee2":"code","0e1403a0":"code","f0e5c6b8":"code","aea4ca3c":"markdown","fa218970":"markdown","83931e09":"markdown","cee7fbbf":"markdown","8dd2338c":"markdown","b1e7cc17":"markdown","1353207c":"markdown","6c5a9f32":"markdown","9e26e9fe":"markdown","f8c8cf04":"markdown","1ffb21b3":"markdown"},"source":{"2a8a7caa":"# After running\n! git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\n\n#You can run this oneliner which will build and compile LightGBM with GPU enabled in colab:\n! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ..\/..\/LightGBM && make -j4 && cd ..\/python-package && python3 setup.py install --precompile --gpu;    ","74be9c83":"URL_calendar = \"https:\/\/slavadatasets.s3.us-east-2.amazonaws.com\/calendar.csv\"\nURL_sales_train ='https:\/\/slavadatasets.s3.us-east-2.amazonaws.com\/sales_train_validation.csv'\nURL_prices = 'https:\/\/slavadatasets.s3.us-east-2.amazonaws.com\/sell_prices.csv' ","572366a4":"from  datetime import datetime, timedelta\nimport numpy as np, pandas as pd\nimport gc\nimport io\nimport dask.dataframe as dd\nimport lightgbm as lgb","2ece4135":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","0654bf31":"pd.options.display.max_columns = 50","37551182":"def create_dt(is_train = True, nrows = None, first_day = 1200):\n     \n    prices = dd.read_csv(URL_prices,dtype = PRICE_DTYPES).compute()\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n        \n    cal = dd.read_csv(URL_calendar,dtype = CAL_DTYPES).compute()\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = dd.read_csv(URL_sales_train, \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype).compute()\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    return dt","27cd817f":"def create_fea(dt):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")","15be2c14":"h = 28 \nmax_lags = 57\ntr_last = 1913\nfday = datetime(2016,4, 25) \nfday","5e03b262":"# If you want to load all the data set it to '1' -->  Great  memory overflow  risk ! default= 350\nFIRST_DAY = 1300","ffa8b81e":"df = create_dt(is_train=True, first_day= FIRST_DAY)\ndf.shape","f8331db5":"create_fea(df)\ndf.shape","c86f0588":"df.dropna(inplace = True)\ndf.shape","910477e9":"df.info()","66387a43":"df.to_csv(\"df_F5.gzip\",index=False,compression='gzip')\nfrom google.colab import files\nfiles.download('df_F5.gzip')","3a943c37":"URL_df_F5_gzip = \"https:\/\/slavadatasets.s3.us-east-2.amazonaws.com\/df_F5.gzip\"","ddaa21bc":"%%time\ndf =dd.read_csv(URL_df_F5_gzip ,compression='gzip' ).compute()","ed74db0a":"useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\ntrain_cols = df.columns[~df.columns.isin(useless_cols)]\nX = df[train_cols]\ny = df[\"sales\"]","fdcffb31":"train_cols","1266cdbd":"del df; gc.collect()","dfad71e8":"X.info()","1ca584cd":"np.random.seed(777)\n\nfake_valid_inds = np.random.choice(X.index.values, 2_000_000, replace = False)","3f27969a":"categorical_feature = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI',  'week','quarter', 'mday']","dedd1fa1":"train_inds = np.setdiff1d(X.index.values, fake_valid_inds)\nX_train, y_train = X.loc[train_inds] , y.loc[train_inds]\n\nX_valid, y_valid = X.loc[fake_valid_inds], y.loc[fake_valid_inds],\n                             \n# This is a random sample, we're not gonna apply any time series train-test-split tricks here!","9ffa1f38":"del X, y, fake_valid_inds, train_inds ; gc.collect()","e3cabe4e":"params = {\n        \"objective\" : \"poisson\",\n        \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        \"learning_rate\" : 0.075,\n#         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n#         \"nthread\" : 4\n    \n    'device' : 'gpu',\n    'verbosity': 1,\n    #'num_iterations' : 1200,\n    'num_leaves': 128,\n    \"min_data_in_leaf\": 100,\n}","c37820b1":"model = lgb.LGBMRegressor(**params)\nmodel","dcaebbf6":"%%time\n\nm_lgb = model.fit(X_train, y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='rmse',\n        early_stopping_rounds=5) ","dd4e0199":"model.booster_.save_model('mode.txt') ","1e9ca4f4":"from google.colab import files\nfiles.download('mode.txt')","487570e5":"URL_F5_A_model_LGBM_00 = \"https:\/\/slavadatasets.s3.us-east-2.amazonaws.com\/F5_A_model_LGBM_00.txt\"","5c69c9da":"#load from model:\n\nm_lgb = lgb.Booster(model_file=URL_F5_A_model_LGBM_00)","066af7ff":"print('Starting predicting...')\n# predict\ny_pred = m_lgb.predict(X_test, num_iteration=gbm.best_iteration_)\n# eval\nprint('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n\n# feature importances\nprint('Feature importances:', list(m_lgb.feature_importances_))","92916e94":"# feature importances\nprint('Feature importances:', list(m_lgb.feature_importances_))","d1410983":"%%time\n\nalphas = [1.028, 1.023, 1.018]\nweights = [1\/len(alphas)]*len(alphas)\nsub = 0.\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n    te = create_dt(False)\n    cols = [f\"F{i}\" for i in range(1,29)]\n\n    for tdelta in range(0, 28):\n        day = fday + timedelta(days=tdelta)\n        print(tdelta, day)\n        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n        create_fea(tst)\n        tst = tst.loc[tst.date == day , train_cols]\n        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n\n\n    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace = True)\n    te_sub.sort_values(\"id\", inplace = True)\n    te_sub.reset_index(drop=True, inplace = True)\n    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n    if icount == 0 :\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    print(icount, alpha, weight)\n\nsub2 = sub.copy()\nsub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)    ","b42c4926":"sub.head(10)","ef1f4a03":"sub.id.nunique(), sub[\"id\"].str.contains(\"validation$\").sum()","3b50f05a":"sub.shape","ed58aee2":"#sub.to_csv(\"submission_LGBM_GPU.csv\",index=False)","0e1403a0":"sub.to_csv(\"submission_LGBM_GPU.gzip\",index=False,compression='gzip')","f0e5c6b8":"from google.colab import files\nfiles.download('submission_LGBM_GPU.gzip')","aea4ca3c":"========================================================","fa218970":"================================================================","83931e09":"All steps of data upload+preprocessing+model+training take only couple minites\nAs well you may skip any stape at all and go forward to make submision file with \"magic multiplier\"","cee7fbbf":"# Preprocessing (you may skip this chapter)","8dd2338c":"- <b>Change Runtime type to GPU<\/b> \n- <b>Install proper bild for use GPU<\/b> \n- <b>Restart runtime<\/b>","b1e7cc17":"The code based on [\"M5 First Public Notebook Under 0.50\"](https:\/\/www.kaggle.com\/kneroma\/m5-first-public-notebook-under-0-50)","1353207c":"# <b>LightGBM GPU baseline model for Google Colab in minutes!<\/b>","6c5a9f32":"# Use model to make submission with \"magic multiplier\"","9e26e9fe":"download <b>notebook<\/b> file https:\/\/www.dropbox.com\/s\/xib9g4peo6j9832\/F5_Baseline_05_LightGBM_GPU.ipynb?dl=0","f8c8cf04":"# Upload Dataset + GPU model training","1ffb21b3":"# Dataset preprocessing(functions):"}}