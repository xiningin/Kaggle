{"cell_type":{"63031fc6":"code","9b0216f2":"code","6770285a":"code","f8cf2dd2":"code","d42668bd":"code","bb0f8fad":"code","35916898":"code","00ed1f7f":"code","5dc98bce":"code","2d48107a":"code","d04316fd":"code","054b6102":"code","5c6227f8":"code","dfa0883a":"markdown","82da35b9":"markdown","e18ea0ef":"markdown","3d5fcd34":"markdown","3740036d":"markdown","419b1d25":"markdown","fa147910":"markdown"},"source":{"63031fc6":"!pip install transformers\n\nfrom datetime import datetime\nimport string\n\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('all')\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering","9b0216f2":"import os\n__print__ = print\ndef print(string):\n    os.system(f'echo \\\"{string}\\\"')\n    __print__(string)","6770285a":"def preprocess():\n    \"\"\" This method reads article metadata to a Pandas DataFrame.\n    \n    \"\"\"\n    root_path = '\/kaggle\/input\/CORD-19-research-challenge'\n    metadata_path = f'{root_path}\/metadata.csv'\n    \n    meta_df = pd.read_csv(\n        metadata_path, \n        dtype={'pubmed_id': str, 'Microsoft Academic Paper ID': str, 'doi': str}, \n        low_memory=False)\n    \n    df = meta_df[['sha', 'title', 'abstract', 'license', 'publish_time', 'journal', 'url']]\n    df.dropna(subset=['sha', 'abstract'], inplace=True)\n    \n    df.set_index('sha', inplace=True)\n    df.index.name = 'paper_id'\n    \n    return df","f8cf2dd2":"def identify_relevant_abstracts(key_word, df):\n    \"\"\" This method uses GloVe 100d word embeddings to identify the relevant abstracts, which \n        we will explore in more detail to answer questions.\n    \n    \"\"\"\n\n    # This container will hold a list of `paper_ids`.\n    relevant_abstracts = []\n\n    # retrieve GloVe embeddings    \n    glove_file = datapath('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt')\n    word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n    glove2word2vec(glove_file, word2vec_glove_file)\n\n    model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n            \n    for i, row in df.iterrows():\n        abstract = df.loc[i][\"abstract\"]\n        current_index = i\n\n        most_similar_words = model.most_similar(key_word)\n        for similar_word in most_similar_words:\n            if (similar_word[0] in abstract) and (current_index not in relevant_abstracts):\n                relevant_abstracts.append(current_index)\n\n    print(\"Identified relevant abstracts: %s\" % len(relevant_abstracts))\n    print(relevant_abstracts)\n    return relevant_abstracts","d42668bd":"study_types = [\n    \"Regression\",\n    \"Simulation\",\n    \"Meta-Regression\",\n    \"Systemic\",\n    \"Time-series\",\n    \"Retrospective\",\n    \"Eco-epidemiological\",\n    \"Ecological\",\n    \"Modelling\"\n]","bb0f8fad":"factors_questions = [\n    \"Factors of COVID-19?\",\n    \"What causes COVID-19 to decline?\",\n    \"What causes COVID-19 to fall?\",\n    \"What caused COVID-19 to decrease?\",\n    \"What causes COVID-19 spread?\",\n    \"What associations with COVID-19?\",\n    \"What associations COVID-19?\",\n    \"What interventions COVID-19?\",\n    \"What impacts COVID-19?\",\n    \"What correlates with COVID-19?\"\n]","35916898":"evidence_questions = [\n    \"Number of cases?\",\n    \"Number of locations?\",\n    \"Number of countries?\",\n    \"Which countries?\"\n]","00ed1f7f":"def answer_question_with_model(tokenizer, model, question, text):\n    \"\"\" This method uses a QA model to answer a question.  Reference:\n        https:\/\/huggingface.co\/transformers\/model_doc\/albert.html\n        \n    \"\"\"\n    \n    input_dict = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n    input_ids = input_dict[\"input_ids\"].tolist()\n\n    # ALBERT model can support 512 or fewer tokens.\n    if len(input_ids[0]) > 512:\n        return False\n\n    start_scores, end_scores = model(**input_dict)\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    answer = ''.join(all_tokens[torch.argmax(start_scores):torch.argmax(end_scores)+1]).replace('\u2581', ' ').strip()\n\n    return answer","5dc98bce":"def answer_parts_of_speech(answer):\n    \"\"\" This method breaks an answer into parts of speech, to allow us to \n        identify factors, which most often show up as nouns.\n\n    \"\"\"\n\n    answer_tokens = nltk.word_tokenize(answer)\n    answer_parts_of_speech = nltk.pos_tag(answer_tokens)\n            \n    factors = []\n    factor_to_join = []\n    \n    for word, part_of_speech in answer_parts_of_speech:\n        if part_of_speech in [\"NN\", \"NNS\"]:\n            if len(factor_to_join) == 0:\n                factors.append(word)\n            else:\n                factor_to_join.append(word)\n                factors.append(' '.join(factor_to_join))\n                factor_to_join = []\n        elif part_of_speech in [\"JJ\"]:\n            factor_to_join.append(word)\n\n    return factors","2d48107a":"def identify_study_type(title, abstract):\n    \"\"\" This method identifies the study type of an abstract by pattern matching.\n    \n    \"\"\"\n    \n    study_type = \"\"\n    \n    for selection in study_types:\n        if selection.lower() in title.lower():\n            if selection == \"Regression\":\n                study_type = \"Ecological Regression\"\n                break\n            elif selection == \"Systemic\":\n                study_type = \"Systemic review\"\n                break\n            elif selection == \"Time-series\":\n                study_type = \"Time-series analysis\"\n                break\n            elif selection == \"Retrospective\":\n                study_type = \"Retrospective Study\"\n                break\n            elif selection == \"Eco-epidemiological\":\n                study_type = \"Eco-epidemiological Study\"\n                break\n            elif selection == \"Ecological\":\n                study_type = \"Ecological Study\"\n                break\n            elif selection == \"Modelling\":\n                study_type = \"Modelling Study\"\n                break\n\n    if study_type == \"\":            \n        for selection in study_types:\n            if selection.lower() in abstract.lower():\n                if selection == \"Regression\":\n                    study_type = \"Ecological Regression\"\n                    break\n                elif selection == \"Systemic\":\n                    study_type = \"Systemic review\"\n                    break\n                elif selection == \"Time-series\":\n                    study_type = \"Time-series analysis\"\n                    break\n                elif selection == \"Retrospective\":\n                    study_type = \"Retrospective Study\"\n                    break\n                elif selection == \"Eco-epidemiological\":\n                    study_type = \"Eco-epidemiological Study\"\n                    break\n                elif selection == \"Ecological\":\n                    study_type = \"Ecological Study\"\n                    break\n                elif selection == \"Modelling\":\n                    study_type = \"Modelling Study\"\n                    break\n\n    if study_type == \"\":\n        study_type = \"Retrospective Study\"  \n        \n    return study_type","d04316fd":"def generate_summary_table_csv(relevant_abstracts, question, df):\n    \"\"\" This method generates summary tables corresponding with a question.\n    \n    \"\"\"\n\n    # we instantiate the pd.DataFrame which will hold our summary table\n    columns = [\"Date\", \"Study\", \"Study Link\", \"Journal\", \"Study Type\", \"Factors\",\n                \"Influential\", \"Excerpt\", \"Measure of Evidence\", \"Added on\"]\n    rows = []\n    summary_table_df = pd.DataFrame(columns=columns)\n\n    # we instantiate the tokenizer and model for our albert-xlarge-v2\n    tokenizer = AutoTokenizer.from_pretrained(\"ktrapeznikov\/albert-xlarge-v2-squad-v2\")\n    model = AutoModelForQuestionAnswering.from_pretrained(\"ktrapeznikov\/albert-xlarge-v2-squad-v2\")\n\n    for paper_id in relevant_abstracts:\n        print(\"Iterating `paper_id` %s\" % paper_id)\n        abstract = df.loc[paper_id][\"abstract\"]\n        \n        # prepare abstract for the model\n        abstract = abstract.translate(str.maketrans('', '', string.punctuation))\n        abstract = abstract.replace(\"SARS-CoV-2\", \"COVID-19\")\n        abstract = abstract.replace(\"Covid-19\", \"COVID-19\")\n\n        # identify factors\n        factors_retrieved = False\n        for fq in factors_questions:\n            if factors_retrieved == True:\n                continue\n\n            factors_answer = answer_question_with_model(tokenizer, model, fq, abstract)\n            \n            if (factors_answer == False) or (factors_answer == \"[CLS]\"):\n                continue\n            \n            factors = answer_parts_of_speech(factors_answer)\n\n            if factors == []:\n                continue\n                \n            factors_retrieved = True\n\n            for factor in factors:\n                # first, fill in metadata columns\n                publish_time = datetime.strptime(df.loc[paper_id][\"publish_time\"], '%Y-%m-%d')\n                date = publish_time.strftime('%-m\/%d\/%y')\n                title = df.loc[paper_id][\"title\"]\n                study_link = df.loc[paper_id][\"url\"]\n                journal = df.loc[paper_id][\"license\"]\n                added_on = datetime.today().strftime('%-m\/%d\/%y')                \n\n                # now, we ask whether the factor is influential.\n                excerpt_question = \"\"\"How does %s influence COVID-19?\"\"\" % factor\n                excerpt = answer_question_with_model(tokenizer, model, excerpt_question, abstract)\n\n                if (excerpt == False) or (excerpt == \"[CLS]\"):\n                    excerpt = \"-\"\n\n                # Then, if an excerpt can be found, conclude that the factor was influential.\n                if (excerpt is not None) and (excerpt != \"-\"):\n                    influential = \"Y\"\n                else:\n                    influential = \"N\"\n\n                # Then, we identify the study type of the abstract.\n                study_type = identify_study_type(title, abstract)\n                \n                # Finally, we identify the evidence if any exists.\n                evidence = \"\"\n                for eq in evidence_questions:\n                    evidence_answer = answer_question_with_model(tokenizer, model, eq, abstract)                        \n\n                    if (evidence_answer != False) and (evidence_answer != \"[CLS]\"):\n                        if eq == \"Number of cases?\":\n                            evidence = \"cases: \" + evidence_answer\n                        elif eq == \"Number of locations?\":\n                            evidence = \"locations: \" + evidence_answer\n                        elif eq == \"Number of countries?\":\n                            evidence = \"countries: \" + evidence_answer\n                        elif eq == \"Which countries?\":\n                            evidence = \"countries: \" + evidence_answer\n                            \n                if evidence == \"\":\n                    evidence = \"-\"\n\n                df_row = {\"Date\": date,\n                          \"Study\": title,\n                          \"Study Link\": study_link,\n                          \"Journal\": journal,\n                          \"Study Type\": study_type,\n                          \"Factors\": factor,\n                          \"Influential\": influential,\n                          \"Excerpt\": excerpt,\n                          \"Measure of Evidence\": evidence,\n                          \"Added on\": added_on\n                         }\n    \n                rows.append(df_row)\n\n    summary_table_df = pd.DataFrame(rows, columns=columns)\n    print(summary_table_df)\n    print(\"Finalizing summary table csv %s\" % question)\n    summary_table_df.to_csv(\"%s-v1.csv\" % question, index=False)","054b6102":"def main():\n    df = preprocess()\n\n    #relevant_abstracts = identify_relevant_abstracts(key_word, df)\n\n    question_dict = {\n        \"Seasonality of transmission\": [\"a789d41d9bafdf73dab3e1a6c90f46c1ce963ff9\", \"e116dfb0acbbf969bf78e312780ae45e65ac638e\",\"0d11705a07ab7028753be9f85fc714007e2ee841\",\"78b825a616f8756c05ba9af7f8c87572c58ee731\",\"31405dd697c54599864408c6cae1725043d5acd8\",\"9082bff2bab68c199d1ce43d6cfdfc4abe8179fb\",\"1979adc54a27e3dee0ffbf2b08b583bfb9900bb4\",\"84af9c7197860f0aeef586622f26f2fd13d5fbfd\",\"6c0620455fe27bceb7d411f31f7fa05be84bf50c\",\"888c4a8022d2ce985b917103d649420f72bdb349\"],\n        \"How does temperature and humidity affect the transmission of 2019-nCoV\": [\"e116dfb0acbbf969bf78e312780ae45e65ac638e\",\"0d11705a07ab7028753be9f85fc714007e2ee841\",\"78b825a616f8756c05ba9af7f8c87572c58ee731\",\"9082bff2bab68c199d1ce43d6cfdfc4abe8179fb\",\"31405dd697c54599864408c6cae1725043d5acd8\",\"84af9c7197860f0aeef586622f26f2fd13d5fbfd\",\"6c0620455fe27bceb7d411f31f7fa05be84bf50c\",\"c3c0a8ba2dc4e9f7ca6e4152f3266a1616e1a63f; f8d6e0978748ee23eeaa1eb9c50dc22bed31ea7b\",\"cc7a5fcd4ce8ced4b5005d4ea8d09da2fcdf9f0a; 72565d63479f6c7a483ebfa2ac7b7ef10b021628\"],\n        \"Effectiveness of workplace distancing to prevent secondary transmission\": [\"b72e843b66eeb54b85568d509994443b5dac047e\",\"11ca9a2c809a5ff5401bbd5e16a2742b5d4d9bd8\",\"9c33486a49de4aea64ce61c0a2c21a88c316b6a8\",\"2913d91f13fd59c698f68ba63008d8e0550c0607\"],\n        \"Effectiveness of school distancing\": [\"76a1a3f4055df0fd3d7041316d7d8ba48ac98b12\",\"7e65f55efd6ab86bfcbdaf22146c652e47e6f235\",\"33807c0c3367aebc5ed29500a4a9cfba882cce16\",\"11ca9a2c809a5ff5401bbd5e16a2742b5d4d9bd8\",\"a3ec2c34f77f54f03fdc1e60db040ede8a93a03a\",\"78c92c6c7176ea5ca38ddc44462279df3325c4d6\"],\n        \"Effectiveness of inter_inner travel restriction\": [\"f7ed51444c210f58c010f7d6a8e8ff454520a796\",\"7197c20da00b41eef947e8d0d821a41ad1638f7c\",\"a6bfd3583719947b0790e282d33772593e202011\",\"9f1421f795084d05cda18dcd08dc9bec99fac178\",\"d644cced28a5b2246b394cb5204087c857196e01\",\"0cbe23280cccea688ea36bc5314f3af18148d4ae\",\"abd6288b4399dd34f431fef5ad539a99ddb7ffeb\",\"6e218868d9a3bf4057ccf0be71cd2ac6828a9c76\",\"3d847478e2ff0104ed05c49db2c2e37f75ceece6; 60672300dca1b56b2be5cb96875ef2994dcf4965\",\"7662e461bdac4972293ba461b73f4b7be24cb387\"],\n        \"Effectiveness of community contact reduction\": [\"a60e5f418229143cbfd6bf2b3f0c53a2ec9d09ae\",\"b301c06e1c036a4b8f2803b8ada254ba227912e6\",\"5262a0c9e3150f9b3e3d33a45a91b9e9cca7da86\",\"a6bfd3583719947b0790e282d33772593e202011\",\"9f1421f795084d05cda18dcd08dc9bec99fac178\",\"0cbe23280cccea688ea36bc5314f3af18148d4ae\",\"3b597f1ae76cfae9a60ca5a13a6353511063956f\",\"abd6288b4399dd34f431fef5ad539a99ddb7ffeb\",\"9f3c081d9cef02ea81a9666a2077639725b65ac8\",\"ab782d7af76c72ab3f5559ebbce93766d799bedf\"],\n        \"Effectiveness of case isolation_isolation of exposed individuals to prevent secondary transmission\": [\"a60e5f418229143cbfd6bf2b3f0c53a2ec9d09ae\",\"92ea1980ea8bd1105a53e6b8cc132d2448199864\",\"27c7020d5ee9f6d7b2cf92a1990cd56072cc1bc0\",\"6e80241c8b6547c944ca073b224e2bf05064f75d\",\"0cbe23280cccea688ea36bc5314f3af18148d4ae\",\"76a1a3f4055df0fd3d7041316d7d8ba48ac98b12\",\"3b597f1ae76cfae9a60ca5a13a6353511063956f\",\"8f8d59261474f6961ad9b59f0bef8e67b6fc6734\",\"abd6288b4399dd34f431fef5ad539a99ddb7ffeb\"],\n        \"Effectiveness of a multifactorial strategy to prevent secondary transmission\": [\"b301c06e1c036a4b8f2803b8ada254ba227912e6\",\"f7ed51444c210f58c010f7d6a8e8ff454520a796\",\"a6bfd3583719947b0790e282d33772593e202011\",\"9f1421f795084d05cda18dcd08dc9bec99fac178\",\"6e80241c8b6547c944ca073b224e2bf05064f75d\",\"76a1a3f4055df0fd3d7041316d7d8ba48ac98b12\",\"5a51bc6bfc087af2ae924c899952fc7474a0c4ce\",\"abd6288b4399dd34f431fef5ad539a99ddb7ffeb\",\"7cdd84cbbaa193437e5665afb32b365d75a6077f\",\"6e218868d9a3bf4057ccf0be71cd2ac6828a9c76\"]\n    }\n\n    for question in question_dict:\n        generate_summary_table_csv(question_dict[question], question, df)","5c6227f8":"main()","dfa0883a":"Next, we will process the metadata to retrieve abstracts.","82da35b9":"Now that we have identified the abstracts which contain potential answers to a question, we iterate through this list and ask sub-questions using an `ALBERT-xlarge-v2` model.  An example of a sub-question is \"What factors cause COVID-19?\"\n\nWe instantiate some lists and other variables first, which will be useful later.","e18ea0ef":"To begin, install and import the required libraries.","3d5fcd34":"We will identify the abstracts which are relevant to the question being asked.  An example of such a question might be, \"What does the literature tell us about seasonality of transmission?\"\n\nTo do so, we will take as input a key word corresponding with a question.  Next, we use GloVe's 100-dimensional word embeddings to recognize the most-similar words (by cosine similarity) to this key word.  Finally, we search the abstract for any such similar words; if any such words are identified in the abstract, then we claim it is likely to correspond with the question.","3740036d":"# COBERT: A Novel Approach for Question Answering\n\nCOBERT is a novel approach to answering questions about COVID-19.\n\n**For the purposes of the challenge, we have included our output files from a run of COBERT under the \"Input\" directory.  This output may also be seen in Version 33 of this kernel.**\n\n# Introduction\nQuestion answering (QA) is a challenging task for machines, since it requires two things: 1) the ability to comprehend natural, human language and 2) knowledge about the world we live in.  It is a leading problem in natural language processing (NLP) and artificial intelligence (AI) research, mainly because QA is a suitable benchmark by which machines' ability to understand complex human reasoning is measured.  In the past, the bulk of QA consisted of text retrieval methods, which relied largely on matching patterns.  Today, the field has progressed to include deep-learning architectures, and the use cases of QA have become both broader and deeper.\n\nOur proposed solution, nicknamed COBERT (a health-care application of BERT), concerns the body of research around COVID-19.  It uses a mixture of pattern matching and deep-learning architectures to generate insightful tables of information from tens of thousands of scientific abstracts.\n\n# Related Work\nBERT [1], a deep Transformer architecture, was introduced in 2018 and achieved SOTA results on SQuAD 2.0 [5].  ALBERT [2] uses two parameter reduction techniques to achieve better results than BERT while also having fewer parameters (namely, \"A Lite BERT\").  These techniques are \"factorized embedding parameterization\" and \"cross-layer parameter sharing,\" which allows ALBERT to share all parameters across layers.\n\n# Approach\n\n## Original Work\nOur solution is based on a combination of techniques; specifically, pattern matching, occasionally with the help of GloVe embeddings, alongside a high-performing QA model, ALBERT.  We found ALBERT suitable for this use case given the resource allotment of the environment.\n\nFirst, we process the data to include abstracts which are relevant to the question asked.  There are exactly 10 questions which comprise this task.  To achieve this, we identify a key word from the text of the question.  Then, we use Stanford's GloVe 100d word embeddings to generate a list of words which have a strong cosine similarity with the key word.  Next, we iterate through the abstracts and match those similar words to text of the abstracts.  We hypothesize that if an abstract contains a word in this list, then there is a strong likelihood that abstract will be relevant to our question.\n\nThen, we use an ALBERT-xlarge model to answer questions about each abstract.  These questions include: \"What are factors of COVID-19?\", \"How does <factor> influence COVID-19?\", and \"How many countries?\".  We use the model as a question-answering engine which has specific knowledge about COVID-19.\n\n# Conclusions\nUsing the above approach, we were able to generate summary tables which correspond to the target tables in the challenge.  We would like to highlight a few discoveries:\n\n## Insightful QA\nThis proposal derives meaning from the text, allowing us to retrieve words and phrases which closely match the descriptions in the challenge.  As a result, we found we did not need to retrieve large excerpts and passages, which gives the output a more human readable quality somewhat matching the target tables.\n\n## Customizable Questions\nOur approach is customizable in that it is possible to add to the lists of questions which get passed to ALBERT-xlarge: `factors_questions` and `evidence_questions`.  Adding more questions will take additional computing time, but probably lead to more accurate information.\n\n## Extensible, Easy-To-Understand Code\nWe believe our solution is generalizable to other tasks.  We have tried to keep it as humanly understandable as possible, as well as free from hard-coded values.\n\n\n## Computational Resources\nWe found this approach, while compelling, to be computationally expensive.  Using a QA model, like ALBERT, to answer questions allowed us to derive insights, but at the cost of greater time to compute the growing volume of scientific papers.  Running COBERT on even one question across the full list of relevant abstracts took more than 3 hours.  As a result, we have provided a hard-coded list of abstracts taken from the target tables to demonstrate that COBERT can re-create the target tables (as much as possible).  We predict that additional computational resources, architectures, and time would allow our approach to scale appropriately with a full run.\n\n## Improvements\nWe can fine-tune our model to better understand the medical data in these abstracts on a data set like MedQuAD: https:\/\/github.com\/abachaa\/MedQuAD.  Additionally, our model, and any BERT-based model, will derive knowledge from the context itself and return a span containing the answer to a question.  It is possible to improve performance with a repository of related knowledge, instead of just the information found in the text.\n\n# Acknowledgements\nWe relied heavily on this ALBERT-xlarge-v2 model trained on SQuAD v2: https:\/\/huggingface.co\/ktrapeznikov\/albert-xlarge-v2-squad-v2 with credit to Kirill Trapeznikov.\n\n# References\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre- training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805v2, 2018. https:\/\/arxiv.org\/abs\/1810.04805.\n\n[2] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In International Conference on Learning Representations, 2020. https:\/\/arxiv.org\/abs\/ 1909.11942.\n\n[3] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2014. https: \/\/www.aclweb.org\/anthology\/D14-1162.\n\n[4] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv:1606.05250, 2016. https:\/\/arxiv. org\/abs\/1606.05250.\n\n[5] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know What You Don\u2019t Know: Unanswerable Questions for SQuAD. arXiv:1806.03822, 2018. https:\/\/arxiv.org\/abs\/1806.03822.","419b1d25":"We can add to these lists of `factors_questions` and `evidence_questions` to make the comprehension power of the ALBERT model even more robust.","fa147910":"This method, `answer_question_with_model`, is mostly borrowed from the PyTorch documentation and encapsulated here in a reusable function."}}