{"cell_type":{"8561e01f":"code","d8f85c34":"code","93936405":"code","3c6c6186":"code","84b0b802":"code","9a2737a3":"code","397ffe08":"code","14db5d95":"code","800c7d3b":"code","c797e8ff":"code","c0f359e7":"code","3d28e1cd":"markdown","b1be6162":"markdown","6101fecb":"markdown","7e1cbc8d":"markdown","2c63d3e8":"markdown","cb1e52e6":"markdown","85b7a283":"markdown","b85ea4f6":"markdown"},"source":{"8561e01f":"import numpy as np\nimport torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.utils as tvutils\nimport torch.utils.data as datautils\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n# use gpu if available\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","d8f85c34":"# return iterable over dataset that will load minibatches (size = 128) for training\n# preprocessing: images are resized to 64x64, each channel is normalized with mean = 0.5, s = 0.5\ndef get_loader(datapath):\n    dataset = datasets.ImageFolder(root=datapath,transform=transforms.Compose([\n                               transforms.Resize(64),\n                               transforms.CenterCrop(64),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n    loader = datautils.DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)\n    return loader","93936405":"# view training data sample images as 8x8 grid\ndef view_images(loader):\n    sample = next(iter(dataloader))\n    plt.figure(figsize=(12,12))\n    plt.axis('off')\n    plt.title('Dataset Sample Images')\n    plt.imshow(np.transpose(tvutils.make_grid(sample[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))","3c6c6186":"class Generator(nn.Module):\n    def __init__(self, z_size=100, num_channels=3, gfm_size=64):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            \n            # input layer (latent vector z taken as input)\n            nn.ConvTranspose2d(z_size, gfm_size * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(gfm_size * 8),\n            nn.ReLU(True),\n            \n            # state size: (gfm_size * 8) x 4 x 4\n            nn.ConvTranspose2d(gfm_size * 8, gfm_size * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(gfm_size * 4),\n            nn.ReLU(True),\n            \n            # state size: (gfm_size * 4) x 8 x 8\n            nn.ConvTranspose2d( gfm_size * 4, gfm_size * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(gfm_size * 2),\n            nn.ReLU(True),\n            \n            # state size: (gfm_size * 2) x 16 x 16\n            nn.ConvTranspose2d( gfm_size * 2, gfm_size, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(gfm_size),\n            nn.ReLU(True),\n            \n            # state size: (gfm_size) x 32 x 32\n            nn.ConvTranspose2d(gfm_size, num_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # output: num_channels x 64 x 64\n        )\n        \n    def forward(self, input):\n        return self.main(input)","84b0b802":"class Discriminator(nn.Module):\n    def __init__(self, num_channels=3, dfm_size=64):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            \n            # input: num_channels x 64 x 64 \n            nn.Conv2d(num_channels, dfm_size, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # state size: dfm_size x 32 x 32\n            nn.Conv2d(dfm_size, dfm_size * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(dfm_size * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # state size: (dfm_size * 2) x 16 x 16\n            nn.Conv2d(dfm_size * 2, dfm_size * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(dfm_size * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # state size: (dfm_size * 4) x 8 x 8\n            nn.Conv2d(dfm_size * 4, dfm_size * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(dfm_size * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # state size: (dfm_size * 8) x 4 x 4\n            nn.Conv2d(dfm_size * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        # view(-1) flattens the output from 2D to 1D \n        return self.main(input).view(-1)","9a2737a3":"# initialize all model weights\ndef init_weights(model):\n    classname = model.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(model.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(model.weight.data, 1.0, 0.02)\n        nn.init.constant_(model.bias.data, 0)","397ffe08":"path = '\/kaggle\/input\/complete-pokemon-image-dataset\/pokemon\/'\ndataloader = get_loader(path)\nview_images(dataloader)","14db5d95":"num_epochs = 50\nz_size = 100\nreal_label = 1\nfake_label = 0\n\n# creating generator and discriminator networks\ngen_net = Generator().to(device)\ndisc_net = Discriminator().to(device)\n\n# init_weights is applied to self and every submodule recursively\ngen_net.apply(init_weights)\ndisc_net.apply(init_weights)\n\nprint(gen_net, disc_net)\n\n# create criterion to measure error (binary cross entropy between target and output)\nbce_loss = nn.BCELoss()\n\n# create Adam SGD optimizers with learning rate 0.0002 and beta1 = 0.5\ngen_opt = optim.Adam(gen_net.parameters(), lr=0.0002, betas=(0.5, 0.999))\ndisc_opt = optim.Adam(disc_net.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n# noise used for visualizing generator's learning curve \ntest_noise = torch.randn(64, z_size, 1, 1, device=device)\n\nimage_lst = []\ngen_losses = []\ndisc_losses = []\nnum_iter = 0","800c7d3b":"for epoch in range(num_epochs):\n    for idx,data in enumerate(dataloader):\n        \n        # DISCRIMINATOR UPDATE\n\n        # set gradients of all model parameters to zero\n        disc_net.zero_grad()\n        \n        # get batch of all real images\n        real_batch = data[0].to(device)\n        batch_size = real_batch.size(0)\n        labels = torch.full((batch_size,), real_label, device=device)\n        \n        # forward pass real batch through discriminator\n        output = disc_net(real_batch)\n        disc_loss_real = bce_loss(output, labels)\n        \n        # calculate gradients for discriminator in backward pass\n        disc_loss_real.backward()\n        Dx_value = output.mean().item()\n        \n        # get batch of all fake images\n        latent_z = torch.randn(batch_size, z_size, 1, 1, device=device)\n        fake_batch = gen_net(latent_z)\n        labels.fill_(fake_label)\n        \n        # classify fake images using discriminator\n        output = disc_net(fake_batch.detach())\n        disc_loss_fake = bce_loss(output, labels)\n        \n        # calculate gradients for discriminator in backward pass\n        disc_loss_fake.backward()\n        DGz_value_1 = output.mean().item()\n        \n        # add gradients from both batches, save total loss\n        disc_loss = disc_loss_real + disc_loss_fake\n        disc_losses.append(disc_loss.item())\n        \n        # update discriminator\n        disc_opt.step()\n        \n        # GENERATOR UPDATE\n        \n        # set gradients of all model parameters to zero\n        gen_net.zero_grad()\n        \n        # fake labels are \"real\" in terms of generator cost\n        labels.fill_(real_label)\n        \n        # after updating discriminator, perform another forward pass of fake batch\n        output = disc_net(fake_batch)\n        \n        # calculate and save generator loss on discriminator's new output \n        gen_loss = bce_loss(output, labels)\n        gen_losses.append(gen_loss.item())\n        \n        # calculate gradients for generator in backward pass\n        gen_loss.backward()\n        DGz_value_2 = output.mean().item()\n        \n        # update generator\n        gen_opt.step()\n        \n        # print update every 100th batch\n        if idx % 100 == 0:\n            print('Epoch: %d\/%d Idx: %d\/%d\\nLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f \/ %.4f'\n                % (epoch, num_epochs, idx, len(dataloader),\n                disc_loss.item(), gen_loss.item(), Dx_value, DGz_value_1, DGz_value_2))\n        \n        # periodically test and save generator performance on test_noise\n        # do not calculate gradients\n        if (num_iter % 500 == 0) or ((epoch == num_epochs-1) and (idx == len(dataloader)-1)):           \n            with torch.no_grad():\n                fake_image = gen_net(test_noise).detach().cpu()\n            image_lst.append(tvutils.make_grid(fake_image, padding=2, normalize=True))\n        \n        num_iter += 1","c797e8ff":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(gen_losses,label=\"Generator\")\nplt.plot(disc_losses,label=\"Discriminator\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","c0f359e7":"# generate 12x12\" figure, axes off, showing progression of generated images\nfig = plt.figure(figsize=(12,12))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)))] for i in image_lst]","3d28e1cd":"# Generator and Discriminator Models","b1be6162":"# Generator and Discriminator Loss Graph","6101fecb":"Final set of images.","7e1cbc8d":"# Training Loop\nNote: In order to obtain images from the generator that looked recognizably Pokemon-like, the number of epochs was increased from 5 (as described in the GAN paper) to 50.","2c63d3e8":"# Generated Images","cb1e52e6":"# Generative Adversarial Network for Pokemon Generation\nThis is a simple implementation of the DCGAN (Deep Convolutional Generative Adversarial Network) architecture, as described in [Generative Adversarial Nets](https:\/\/arxiv.org\/pdf\/1406.2661.pdf) by Ian Goodfellow, et. al.\n<br><br>\nThe GAN was applied to the Complete Pokemon Image Dataset (24,647 .jpg images of size 160x160px, comprising all existing Pokemon designs) in order to generate new designs.","85b7a283":"# Importing Data","b85ea4f6":"# Data Utility Functions"}}