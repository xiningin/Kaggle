{"cell_type":{"3d5f6e6f":"code","2e7e24ac":"code","804ab183":"code","df28401a":"code","842d68be":"code","ce69422b":"code","1be650ac":"code","5f1c5f05":"code","533d950f":"code","987a0f2b":"code","2c9187df":"code","1c1eee60":"markdown","b89942be":"markdown","e5869e4b":"markdown","7f993dd2":"markdown","5286315c":"markdown","1dacb667":"markdown"},"source":{"3d5f6e6f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2e7e24ac":"from tensorflow.python.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.python.keras.models import Sequential, Model\n#from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\nfrom tensorflow.python.keras.layers import Dense\nimport scipy\nimport cv2     \nimport matplotlib.pyplot as plt\nimport ast","804ab183":"def get_base_model():\n    resnet_weights_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n    base_model = ResNet50( weights=resnet_weights_path)\n    #print(base_model.summary())\n    return base_model","df28401a":"def get_new_model( base_model ):\n    #get_weights returns a list where the 0th index are the weights and 1st index is the bias\n    # we do not care for the bias henc ewe only get weights present at the 0th index\n    weights_last_classification_layer = base_model.layers[-1].get_weights()[0] # dim: 2048 * 1000\n   \n    # create a new model with the output that we want. Since we want to get activation map of the  layer just before Global Average Pooling layer, we add its output as the output \n    #of our new model\n    new_model = Model(inputs=base_model.input, \n        outputs=(base_model.layers[-4].output, base_model.layers[-1].output)) \n    \n    return new_model, weights_last_classification_layer","842d68be":"from keras.preprocessing import image   \n\n# the image that is originally in 3D width*height* chanels. If we look at our model summary we see that the Input is required to be a 4D tensor\n# Hence we converting our input image in 3D format ( width*height*channel) to 4D tensor and apply pre-processing that Resnet base model applied during training\n\ndef prepare_image_for_prediction( img_path):\n    # loads the input image as PIL ( Python Image Libraray ) image\n    img = image.load_img(img_path, target_size=(224, 224))\n    # the below function returns a 3D numpy array (width * height *channel )\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    # The below function inserts an additional dimension at the axis position provided\n    x = np.expand_dims(x, axis=0)\n    # perform pre-processing that was done when resnet model was trained.\n    return preprocess_input(x)","ce69422b":"def ResNet_CAM(img_path, model, weights_last_classification_layer):\n    # get filtered images from convolutional output + model prediction vector\n    last_conv_output, pred_vec = model.predict(prepare_image_for_prediction(img_path))\n    # Refer to the model summary. The output of the last convolution layer is 4D. Convert it to 3D \n    # change dimensions of last convolutional output to 7 x 7 x 2048. The squeeze function drops the dimensions with value 1 by default\n    # We now get 2048 activation maps of size 7 X 7 \n    last_conv_output = np.squeeze(last_conv_output) \n    \n    # get model's prediction (number between 0 and 999, inclusive)\n    pred_class = np.argmax(pred_vec)\n    # bilinear upsampling to resize each filtered image to size of original image \n    # The original image is 224 X 244 in width and height. Activation maps are of size 7 X 7. Zoom factor = 224\/7 = 32. The factor remains 1 for  the channel dimension\n    feature_activation_maps = scipy.ndimage.zoom(last_conv_output, (32, 32, 1), order=1) # dim: 224 x 224 x 2048\n    # get weights from the last classification layer for the class predicted for the input image\n    predicted_class_weights = weights_last_classification_layer[:, pred_class] # dim: (2048,) \n    # get class activation map for object class that is predicted to be in the image\n    final_output = np.dot(feature_activation_maps.reshape((224*224, 2048)), predicted_class_weights).reshape(224,224) # dim: 224 x 224\n    # return class activation map\n    return final_output, pred_class","1be650ac":"def plot_ResNet_CAM(orig_image_path,CAM,imagenet100_clsid_to_text_path,pred_class ):\n  \n    im = image.load_img(orig_image_path, target_size=(224, 224))\n    # plot image\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n    fig.set_size_inches(10, 20)\n    ax[0].imshow(im)\n    ax[0].set_title('Original Image')\n    ax[0].axis('off')\n    \n    ax[1].imshow(im, alpha=0.5)\n    ax[1].imshow(CAM, cmap='jet', alpha=0.5)\n    ax[1].axis('off')\n   \n    # get the category name\n    with open(imagenet100_clsid_to_text_path) as imagenet_classes_file:\n        imagenet_classes_dict = ast.literal_eval(imagenet_classes_file.read())\n    ax[1].set_title(imagenet_classes_dict[pred_class].split(',')[0])    \n    plt.show()","5f1c5f05":"base_model = get_base_model()\nnew_model, weights_last_classification_layer = get_new_model(base_model)\nimagenet100_clsid_to_text_path = '..\/input\/imagenetcategorytext\/imagenet1000_clsid_to_human.txt'\n","533d950f":"img_path='..\/input\/stl10\/test_images\/test_image_png_1024.png'\nCAM, pred_class = ResNet_CAM(img_path, new_model, weights_last_classification_layer )\nplot_ResNet_CAM(img_path, CAM, imagenet100_clsid_to_text_path, pred_class )","987a0f2b":"img_path='..\/input\/stl10\/test_images\/test_image_png_1026.png'\nCAM, pred_class = ResNet_CAM(img_path, new_model, weights_last_classification_layer )\nplot_ResNet_CAM(img_path, CAM, imagenet100_clsid_to_text_path, pred_class )","2c9187df":"img_path='..\/input\/stl10\/test_images\/test_image_png_1030.png'\nCAM, pred_class = ResNet_CAM(img_path, new_model, weights_last_classification_layer )\nplot_ResNet_CAM(img_path, CAM, imagenet100_clsid_to_text_path, pred_class )","1c1eee60":"## Let us attempt classification and object localization on some images:","b89942be":"## 1.  Introduction\nDeeplearning computer vision tasks are primarily concerned with Image Classification, Object Localisation and Object Detection.\n\n**Image classification :** The task of image classification is to  classify the image into one of the predefined categories  \n**Image Segmentation :** The task of image segmentation is to  classify each pixels of image into one of the predefined categories  \n**Object Localization:**  Image localization is concerned with identifying location of a single object in an image  \n**Object Detection:** Object detection involves  identifying multiple objects within a single image\n\nObject localization is implemented by drawing a bounding box around the localized object. This requires training the model to output position of the bounding box around a locoalized object. However, there is an alternate approach that can be used to localize objects by using model trained to classify images. The guiding principle behind the philosopy is that the model implictly learns the location of the object in the process of classifying an image by learning which area in the image it gives weigtage to while deciding on the image class. This information is however lost when the model layers are flattened to a fully-connected layer output layer. This approach uses the Global Average Pooling Layer in the model to extract localization information out of a model trained for image classification. The localization is expressed as a heat map (referred to as a class activation map), where the color-coding scheme identifies regions that are relatively important for the GAP-CNN to perform the object identification task.\n\nThe work in this tutorial is heavily based on work  by [Alexis B Cook]( https:\/\/alexisbcook.github.io\/2017\/global-average-pooling-layers-for-object-localization\/  )  who implements techniques in this [paper]( http:\/\/cnnlocalization.csail.mit.edu\/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf  ). The intention of the author is to implement the algorithm on her own to gain deeper insight and clarity into it.  \n\n## 2. Pre-requisites\nThis tutorial assumes basic understanding of Keras and CNN concepts.","e5869e4b":"What this implies is that the model is capable of classifying **1000  image categories** and basis this classication on **2048  patterns** it detects in the images. The contribution of the each of the 2048 pattern is ditermined by the weight matrix of last FC layer. The weight matrix of last FC layer ditermines the contribution of each GAP layer node to an image category. This  weight  can be interpreted as the  average contribution of the pattern ( called the Activation Map). The activation map is however not of the same size as the original image, so bilinear upsampling needs to be applied for comparison\n\n![WeightsMapping.png](attachment:WeightsMapping.png)","7f993dd2":"## References\n\n[1] https:\/\/alexisbcook.github.io\/2017\/global-average-pooling-layers-for-object-localization\/  \n[2] http:\/\/cnnlocalization.csail.mit.edu\/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf  ","5286315c":"## 3. Approach\n\nWe now explain the basic approach of using Global Address Pooling layer for obtaining image localization details.  Let us first understand what Global Address Pooling ( GAP) layer  does. It converts a **h x w x d** layer into **1 x 1 x d** layer by taking average of  **hw** activations in each dimension.\n\nLet's see how the Renet50 model uses GAP . Below you see the summary output of the last 4 layers of the Resnet50 model. The GAP Layer collapses **7 x 7 x 2048** layer to **1 x 1 x 2048** layer, flattens it into a  1- dimensional **(2048, )** array and then passes it to a fully connected dense layer of shape **(1000,)** which forms the last layer for image prediction.\n\n![image.png](attachment:image.png)\n\n\n","1dacb667":"## 4. Implementation Details\n\n1. Load pre-requisite packages  \n2. Import pre-trained Resnet50 model  \n3. Get the weights Wc of the last fully connected layer ( dim: 2048 x 1000 )\n4. Create a new model from the original ResNet50 model that is same as ResetNet50 model but also outputs the Input to the GAP layer i.e. the output of the 4th last layer.\n5.  Load the single image that needs to be predicted and the object  localized and set its size 224 x 224.\n6.  Run predict() function for the loaded image. It shall returns the outputs of the last and fourth last layer of the model.\n7. The Class Activation Map ( CAM ) is output the fourth last layer of dimension ( 1 x 7 x 7 x 2028 ) which is collapsed to 7 x 7 x 2048\n8.  Zoom it to the size of original image  i.e. of dimmension 224 x 224 x2048\n9.  Unroll each Activation Map to a 1-D vector such that we have 2048 1-D representation of Activation Maps  = ( dim: (224 *224) x 2048 ) )\n10. Get weight of the predicted class  wc = Wc[ : , Pc ]  dim: (2048 x 1)\n11. Get the Class Activation Map for the predicted image = Dot Product ( Activation Map , Wc ) dim: (224*224, )\n12. Reshape to 224 x 224 image szie\n12. Superipose the heat map of the CAM over Original image, each with with 0.5 transparancy "}}