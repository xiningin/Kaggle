{"cell_type":{"705ce5dc":"code","09ce669c":"code","004f4a05":"code","3b310141":"code","41dec7f1":"code","d7e6e076":"code","c3935758":"code","c43611f9":"code","7bd9eef0":"code","c53af756":"code","3ffa70ec":"code","86cc61cb":"code","8ed8be94":"code","023b2d6d":"code","2c69444d":"code","5510e441":"code","cc4aea16":"code","4aeaeb7e":"code","4a3d01f2":"code","a03647c6":"code","8fce0bc1":"code","869ac6ba":"code","95831cd0":"code","d70d5694":"code","a879fda3":"code","2055ea96":"code","c73cab71":"code","d8b95845":"code","05273636":"markdown","99ecc3c3":"markdown","8a2be61a":"markdown","4a3fc342":"markdown","9b3d3e3a":"markdown","b0c4954f":"markdown","6d503ad3":"markdown","9f589e85":"markdown"},"source":{"705ce5dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09ce669c":"import cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\nimport zipfile\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D","004f4a05":"root_path = '..\/input\/facial-keypoints-detection\/'\ncsv_data = pd.read_csv(root_path + 'IdLookupTable.csv')\ntrain_zip_dir = 'training.zip'\ntest_zip_dir = 'test.zip'","3b310141":"\n#Will unzip the files so that you can see them..\n\nwith zipfile.ZipFile(\"..\/input\/facial-keypoints-detection\/\"+train_zip_dir,\"r\") as z:\n    z.extractall(\".\")\n\nwith zipfile.ZipFile(\"..\/input\/facial-keypoints-detection\/\"+test_zip_dir,\"r\") as z2:\n    z2.extractall(\".\")","41dec7f1":"train_data = pd.read_csv('.\/training.csv')\ncopy_train_data = train_data.copy()","d7e6e076":"train_data.isnull().any()\n## so there are 3 columns which have no vacant values. so we will replace the missing values with median value of that particular column","c3935758":"train_input = train_data.iloc[:,-1:]\ntrain_output = train_data.iloc[:,:-1]","c43611f9":"train_output.fillna(train_output.median(), inplace= True)","7bd9eef0":"imag = []\nfor i in range(0,len(train_input)):\n    img = train_input['Image'][i].split(' ')\n    img = [0 if x == '' else int(x) for x in img]\n    imag.append(img)","c53af756":"imag = np.array(imag)","3ffa70ec":"x_train = imag.reshape(-1,96,96,1)\nplt.imshow(x_train[0],cmap='gray')","86cc61cb":"y_train = []\nfor i in range(0,len(train_output)):\n    y = train_output.iloc[i,:]\n    y_train.append(y)\ny_train = np.array(y_train,dtype = 'float')","8ed8be94":"x_train = x_train\/255","023b2d6d":"input_shape = x_train.shape","2c69444d":"input_shape","5510e441":"input_shape","cc4aea16":"model = Sequential()\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=input_shape[1:]))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n# model.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","4aeaeb7e":"model.compile(optimizer='adam', \n              loss='mean_squared_error',\n              metrics=['mae'])","4a3d01f2":"model.fit(x_train,y_train,epochs = 50,batch_size = 128,validation_split = 0.2)","a03647c6":"loss = model.history.history['loss']\nmae = model.history.history['mae']\nplt.plot(loss)\n","8fce0bc1":"plt.plot(mae)","869ac6ba":"test_input = pd.read_csv('.\/test.csv')","95831cd0":"test_imag = []\nfor i in range(0,len(test_input)):\n    img = test_input['Image'][i].split(' ')\n    img = [0 if x == '' else int(x) for x in img]\n    test_imag.append(img)","d70d5694":"x_test = np.array(test_imag)\nx_test = x_test.reshape(-1,96,96,1)\ncopy_x_test = x_test.copy()\nx_test = x_test\/255","a879fda3":"pred = model.predict(x_test)","2055ea96":"plt.imshow(copy_x_test[0], cmap='gray')","c73cab71":"image = x_test[0]\nfeatures_loc = np.array(pred[0], dtype = np.int)\n\nfor i in range(0,len(features_loc),2):\n    x = features_loc[i]\n    y = features_loc[i+1]\n    \n    cv2.circle(image, (x,y), radius=1, color=(0, 0, 255), thickness=-1)\n    \n    \n    ","d8b95845":"plt.imshow(image,cmap='gray')","05273636":"## Data Unzipping","99ecc3c3":"### Now split the input and ouput where image is input and rest are the output","8a2be61a":"### Replacing Nan with median for each column","4a3fc342":"we can see that model is working perfectly","9b3d3e3a":"### Now decide the model architecture","b0c4954f":"### Testing preparations","6d503ad3":"## Import all the libraries","9f589e85":"## Data Analysing"}}