{"cell_type":{"5545b22b":"code","066b9210":"code","9a1bc8ef":"code","a134c3c0":"code","f74f06c9":"code","398c0430":"code","bb0cc541":"code","e7c6948c":"code","24feb7bc":"code","a23a92d0":"code","89d762da":"code","e8a0645d":"code","c579666a":"code","89ae344e":"code","12bd87da":"code","92b0b4a0":"code","5900dd95":"code","8fb82cc5":"code","a14b84bb":"code","30bee7b7":"code","6cda5f62":"code","af5e555f":"code","06395a2f":"code","a94e8fc6":"code","990e7358":"code","0c621d7a":"code","54e2796a":"code","8804f49a":"code","1b2cdc3b":"code","451ecc63":"code","2cc89486":"code","b41646c6":"code","3d76766a":"code","bc724517":"code","4b85580f":"code","c94f1445":"code","b3241d67":"code","0d749801":"code","42b600ac":"markdown","10ca3ce1":"markdown","7f5bb1be":"markdown","69674428":"markdown","6a67005b":"markdown","2b7b2039":"markdown","52224e09":"markdown","046cb35e":"markdown","1bf99c63":"markdown","1ea6f4fe":"markdown","db8502ba":"markdown","29dea03f":"markdown","670443de":"markdown","bffc58a4":"markdown","74b568c1":"markdown","6085963b":"markdown","302d8337":"markdown","2ab7b22f":"markdown","9bc9c66a":"markdown","8c48557e":"markdown","61cfc896":"markdown","a6281f3b":"markdown","c9fb6f0e":"markdown","cd244824":"markdown","fd33376e":"markdown","70e0304b":"markdown","e3f7abf3":"markdown","5443588f":"markdown","b22941a5":"markdown","2b6ad910":"markdown","e390661f":"markdown","0d025ea1":"markdown","dd3d2a19":"markdown","d7b12079":"markdown","ab6f827a":"markdown","4be47ce1":"markdown","ace1a55e":"markdown","87ea185e":"markdown","ec65bcf7":"markdown"},"source":{"5545b22b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nimport warnings\n\n# ignore certain warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# set seaborn defaults\nsns.set()\n\n%config InlineBackend.figure_format = 'png' #set 'png' here when working in notebook\n%matplotlib inline\n\n# identify data sets\ntrainData = '..\/input\/train.csv'\ntestData = '..\/input\/test.csv'\n\n# import data sets\ntrain = pd.read_csv(trainData, header=0)\ntest = pd.read_csv(testData, header=0)\n\n# combine all data (ignoring Id and SalePrice features)\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'], test.loc[:,'MSSubClass':'SaleCondition']))","066b9210":"# view training data\ntrain.head()","9a1bc8ef":"# view testing data\ntest.head()","a134c3c0":"# view combined data\nall_data.head()","f74f06c9":"rcParams['figure.figsize'] = (6.0, 6.0) # define size of figure\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train)\nplt.show()","398c0430":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<200000)].index).reset_index(drop=True)\n\n# reset combined data set with new training set\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'], test.loc[:,'MSSubClass':'SaleCondition']))","bb0cc541":"all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","e7c6948c":"from sklearn.preprocessing import LabelEncoder\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))","24feb7bc":"from scipy.stats import skew\n\n# plot histogram of \"SalePrice\"\nrcParams['figure.figsize'] = (12.0, 6.0) # define size of figure\ng = sns.distplot(train[\"SalePrice\"], label=\"Skewness: %.2f\"%(train[\"SalePrice\"].skew()))\ng = g.legend(loc=\"best\")\nplt.show()","a23a92d0":"normalizedSalePrice = np.log1p(train[\"SalePrice\"])\n\n# plot histogram of log transformed \"SalePrice\"\nrcParams['figure.figsize'] = (12.0, 6.0) # define size of figure\ng = sns.distplot(normalizedSalePrice, label=\"Skewness: %.2f\"%(normalizedSalePrice.skew()))\ng = g.legend(loc=\"best\")\nplt.show()","89d762da":"# apply log transform to target\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","e8a0645d":"# determine features that are heavily skewed\ndef get_skewed_features():\n    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())) # computes \"skewness\"\n    skewed_feats = skewed_feats[abs(skewed_feats) > 0.75]\n    return skewed_feats.index","c579666a":"from sklearn.preprocessing import power_transform\n\n# find heavily skewed numerical features\nskewed_feats = get_skewed_features()\nprint(\"{} heavily skewed features.\".format(len(skewed_feats)))\n\n# apply power transform to all heavily skewed numeric features\nall_data[skewed_feats] = power_transform(all_data[skewed_feats], method='yeo-johnson')\nprint(\"Applied power transform.\")","89ae344e":"# create dummy variables\nall_data = pd.get_dummies(all_data)\nall_data.shape # we now have 219 features columns compared to original 79","12bd87da":"# check for any missing values\nall_data.isnull().any().any()","92b0b4a0":"# replace NA's with the mean of the feature\nall_data = all_data.fillna(all_data.mean())\n\n# check again for any missing values\nall_data.isnull().any().any()","5900dd95":"# create matrices for sklearn\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice","8fb82cc5":"from sklearn.model_selection import cross_val_score\n\n# determine average root mean square error (RMSE) using k-fold cross validation\ndef rmse_cv(model, cv=5):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = cv))\n    return rmse","a14b84bb":"from sklearn.linear_model import LinearRegression\n\n# estimate RMSE for linear regression model\nlinearModel = LinearRegression()\nrmse = rmse_cv(linearModel)\nprint(\"RMSE estimate: {}, std: {}\".format(rmse.mean(), rmse.std()))","30bee7b7":"# fit linear model\nlinearModel.fit(X_train, y)\n\n# get largest magnitude coefficients\ncoef = pd.Series(linearModel.coef_, index = X_train.columns)\nimp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\n\nrcParams['figure.figsize'] = (8.0, 10.0) # define size of figure\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Most Important Coefficients Selected by Ridge\")\nplt.show()","6cda5f62":"from sklearn.linear_model import Ridge\n\n# determine RMSE for ridge regression model with alpha = 0.1\nridgeModel = Ridge(alpha = 0.1)\nrmse = rmse_cv(ridgeModel)\nprint(\"RMSE estimate: {}, std: {}\".format(rmse.mean(), rmse.std()))","af5e555f":"rcParams['figure.figsize'] = (12.0, 6.0) # define size of figure\n\n# calculate RMSE over several alphas\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]\ncv_ridge = pd.Series(cv_ridge, index = alphas)\n\n# plot RMSE vs alpha\ncv_ridge.plot(title = \"RMSE of Ridge Regression as Alpha Scales\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.show()","06395a2f":"rcParams['figure.figsize'] = (12.0, 6.0) # define size of figure\n\n# calculate RMSE over several alphas\nalphas = np.linspace(9.8, 15.2, 541)\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]\ncv_ridge = pd.Series(cv_ridge, index = alphas)\n\n# plot RMSE vs alpha\ncv_ridge.plot(title = \"RMSE of Ridge Regression as Alpha Scales\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.show()","a94e8fc6":"optimalRidgeAlpha = cv_ridge[cv_ridge == cv_ridge.min()].index.values[0]\nprint(\"Optimal ridge alpha: {}\".format(optimalRidgeAlpha))","990e7358":"# determine RMSE for ridge regression model with optimal alpha\nridgeModel = Ridge(alpha = optimalRidgeAlpha)\nrmse = rmse_cv(ridgeModel)\nprint(\"RMSE estimate: {}, std: {}\".format(rmse.mean(), rmse.std()))","0c621d7a":"# fit ridge model\nridgeModel.fit(X_train, y)\n\n# get largest magnitude coefficients\nridge_coef = pd.Series(ridgeModel.coef_, index = X_train.columns)\nridge_imp_coef = pd.concat([ridge_coef.sort_values().head(10), ridge_coef.sort_values().tail(10)])\n\nrcParams['figure.figsize'] = (8.0, 10.0) # define size of figure\ndf = pd.DataFrame({ \"RidgeRegression\" : ridge_imp_coef, \"LinearRegression\" : imp_coef })\ndf.plot(kind = \"barh\")\nplt.title(\"Most Important Coefficients Selected by Ridge\")\nplt.show()","54e2796a":"from sklearn.linear_model import Lasso\n\n# determine RMSE for lasso regression model with alpha = 0.1\nlassoModel = Lasso(alpha = 0.1)\nrmse = rmse_cv(lassoModel)\nprint(\"RMSE estimate: {}, std: {}\".format(rmse.mean(), rmse.std()))","8804f49a":"from sklearn.linear_model import Lasso\n\nrcParams['figure.figsize'] = (12.0, 6.0) # define size of figure\n\n# calculate RMSE over several alphas\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_lasso = [rmse_cv(Lasso(alpha = alpha)).mean() for alpha in alphas]\ncv_lasso = pd.Series(cv_lasso, index = alphas)\n\n# plot RMSE vs alpha\ncv_lasso.plot(title = \"RMSE of Lasso Regression as Alpha Scales\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.show()","1b2cdc3b":"from sklearn.linear_model import LassoCV\n\n# use built in LassoCV function to select best model for data\nlassoModel = LassoCV(alphas = np.linspace(0.0002, 0.0022, 21), cv = 5).fit(X_train, y)\nlassoModel.alpha_\n\noptimalLassoAlpha = lassoModel.alpha_\nprint(\"Optimal lasso alpha: {}\".format(optimalLassoAlpha))","451ecc63":"lassoModel = Lasso(alpha = optimalLassoAlpha)\nrmse = rmse_cv(lassoModel)\nprint(\"RMSE estimate: {}, std: {}\".format(rmse.mean(), rmse.std()))","2cc89486":"# fit lasso model\nlassoModel.fit(X_train, y)\n\n# get largest magnitude coefficients\nlasso_coef = pd.Series(lassoModel.coef_, index = X_train.columns)\nlasso_imp_coef = pd.concat([lasso_coef.sort_values().head(10), lasso_coef.sort_values().tail(10)])\n\nrcParams['figure.figsize'] = (8.0, 10.0) # define size of figure\ndf = pd.DataFrame({ \"LassoRegression\" : lasso_imp_coef, \"LinearRegression\" : imp_coef })\ndf.plot(kind = \"barh\")\nplt.title(\"Most Important Coefficients Selected by Lasso\")\nplt.show()","b41646c6":"lasso_coef = pd.Series(lassoModel.coef_, index = X_train.columns)\nprint(sum(lasso_coef != 0))\nprint(sum(lasso_coef == 0))","3d76766a":"# scale alpha\nalphas = np.linspace(0.0002, 0.4002, 2001)\nnonZeros = []\n\n# for each alpha, fit model to training data\nfor alpha in alphas:\n    lassoModel = Lasso(alpha = alpha).fit(X_train, y)\n    coef = pd.Series(lassoModel.coef_, index = X_train.columns)\n    # append the number of non-zero coefficients\n    nonZeros = np.append(nonZeros, sum(coef != 0))\n\n# plot number of non-zeros (L0-Norm) vs alpha\nrcParams['figure.figsize'] = (12.0, 6.0) # define size of figure\nlzeroNorm = pd.Series(nonZeros, index = alphas)\nlzeroNorm.plot(title = \"L0-Norm of Lasso Regression Model as Alpha Scales\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"number of non-zeros\")\nplt.show()","bc724517":"lzeroNorm.max()","4b85580f":"lzeroNorm.min()","c94f1445":"linearModel = LinearRegression().fit(X_train, y)\nlr_submission = pd.DataFrame()\nlr_submission['Id'] = test['Id']\nlr_submission['SalePrice'] = np.expm1(linearModel.predict(X_test))\nlr_submission.to_csv('linear-regression.csv', index=False)","b3241d67":"ridgeModel = Ridge(alpha = optimalRidgeAlpha).fit(X_train, y)\nridge_submission = pd.DataFrame()\nridge_submission['Id'] = test['Id']\nridge_submission['SalePrice'] = np.expm1(ridgeModel.predict(X_test))\nridge_submission.to_csv('ridge.csv', index=False)","0d749801":"lassoModel = Lasso(alpha = optimalLassoAlpha).fit(X_train, y)\nlasso_submission = pd.DataFrame()\nlasso_submission['Id'] = test['Id']\nlasso_submission['SalePrice'] = np.expm1(lassoModel.predict(X_test))\nlasso_submission.to_csv('lasso.csv', index=False)","42b600ac":"Alpha = 0.0004. This looks to be close enough to optimal for our purposes, so let's find our updated RMSE estimate with this newly found optimal alpha.","10ca3ce1":"### Numerical to categorical conversions\n\n`MSSubClass`, `OverallCond`, `YrSold`, and `MoSold` while numerical, are really categorical type features, thus we will convert them to strings so that we can encode them next.","7f5bb1be":"By plotting the distribution of our target feature, we quickly notice that the distribution appears to be righlty skewed. (Note that we use the skew function from scipy.stats to determine the \"skewness\" of the feature.)","69674428":"It appears this Lasso model selected 107 of the features in this instance, the most important of which are included in the plot above, while zeroing out the other 112. We won't go into any more detail with regards to the specifc features at this time, but know that the selected features are not always the \"correct\" features, and should be considered, especially when multicollinearity exists within the feature set.\n\n## L0-Norm\n\nLastly, to get an idea of how the number of features chosen is impacted by the strength of alpha, let's plot the number of non-zero coefficients that lasso produces as you vary the strength of the regularization parameter alpha. (This is also called the L0-Norm of the coefficients.)","6a67005b":"### Indicator variables\n\nNext, we need to create dummy\/indicator variables for all of the categorical features so that they can be reasonably used in our regression models.","2b7b2039":"### Model matrices\n\nLastly, let's setup the matrices needed for sklearn, and then we can begin with the regular ordinary least squares linear regression model. This wraps up our preprocessing steps.","52224e09":"Hmm, okay so perhaps maybe not the plot we were expecting. The optimum alpha seems to be quite small, but we know it has to be greater than 0, so let's use sklearn's builtin LassoCV function which will use cross validation to select the best alpha from a list of potential options for the fit. (Note there also exists a RidgeCV function that works in a similar way which we could have used for the Ridge model earlier.)","046cb35e":"# Credits\n\nPlease note that some of the ideas and code in this notebook come from, or are at least inspired by, the work of:\n* Alexandru Papiu: https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n\nIf you use parts of this notebook in your own scripts, please give some sort of credit (for example a link back to this or the above original notebook) and upvote. Thanks in advance!\n\n## **Thanks for reading. Please feel free to comment, and remember to upvote if you found this notebook helpful or interesting!**","1bf99c63":"Here we see that a lasso regression model with alpha = 0.1 actually made the least accurate model yet when evaluating by RMSE. Before we give up on lasso regression though, let's use cross validation to tune alpha. Perhaps our 0.1 value was way off.\n\nLet's try to do this the same way we did so for ridge regression.","1ea6f4fe":"We'll now create our submission predictions. Remember we log transformed the target values so we will need to exponentiate our predictions.","db8502ba":"**Q. Why are we \"normalizing\" the numerical features?**\n\nIn general, standardized or normally distributed data is nice to have, and provides various benefits in different situations. All the specific benefits and situations goes beyond the scope of this notebook, but typically normalizing your data is a good idea in the absence of any other information against the case. In our situation where we plan to use regularization methods, the more extreme observation values in the highly skewed features create a bias that can cause different explanatory variables to be treated not so equally by the regularization penalty term. By normalizing these skewed distributions, it is believed the regularization penalty will then treat different explanatory variables on a more equal footing. Ideally, we want all observations and variables to be treated perfectly equally by our models.","29dea03f":"### Outliers\n\nWe must be careful when choosing to drop outliers of the risk of losing valuable information, but we see here in the plot below 2 clear outliers toward the bottom right of the plot representing \"bad\" deals for sellers (low price for large area).","670443de":"RMSE appears to be minimal around alpha = 10.62. This looks to be good enough for our purposes, so let's find our updated RMSE estimate using this newly found optimal alpha value.","bffc58a4":"### Missing values\n\nLet's now check for missing values, and replace them with the mean of the corresponding feature.","74b568c1":"Again, we have improved our RMSE. We now have RMSE = 0.11320 for a ridge regression model with an optimal alpha of about 10.62, about a 7.04 % improvement on that of the linear regression model. This looks to be about the best RMSE we can hope to get using this training data and a single ridge regression without anymore advanced preprocessing or feature engineering.\n\nPrior to moving on to Lasso Regression, let's again revisit the largest magnitudes of the selected coefficients and compare them against those selected by the linear regression model.","6085963b":"Cool, so we got a number to compare future models against, RMSE = 0.12178.\n\nIf we now fit this model, we can also look at the largest magnitude coefficient values produced. We'll later compare these against those produced by our regularization models.","302d8337":"Typically, our regression models will perform best with normally distributed data. Thus for best results, let's attempt to normalize the feature with a log transform. (For rightly skewed data, a log transform has the effect of shifting the distribution to appear more \"normal\", while for leftly skewed data, a log transform will only make the distribution even more leftly skewed.)","2ab7b22f":"Again as expected, the values seem to have been compressed toward 0 when compared to those chosen by the original linear regression model.\n\nAs noted above, the Lasso method will actually perform feature selection, so let's now look at the number of coefficients equal and not equal to zero.\n\n**This is an important difference to take note of between ridge regression and lasso regression. While ridge regression punishes high coefficient values, it will not get rid of irrelevant features by enforcing their coefficients to zero. It will only try to minimize their impact. Lasso regression on the other hand will both punish high coefficient values, and get rid of irrelevant features by setting their coefficiants to zero. Thus, when training data sets with many irrelevant features, the lasso model can be useful in feature selection.** ","9bc9c66a":"Let's now estimate the RMSE produced by a default linear regression model with the given training data.","8c48557e":"Ah ha, we already see an improvement upon the ordinary least squares linear regression model. We now have RMSE = 0.12046 for ridge regression with alpha = 0.1. Remember though we chose 0.1 randomly, and thus most likely isn't the optimal value. Hence, we can probably improve our RMSE even further by tuning alpha. \n\nLet's plot the RMSE as alpha scales to get an idea of how RMSE is affected by the value of alpha.","61cfc896":"# Getting started\n\nTo help build our understanding of these methods, we will be working with the [House Prices](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) data set, and will first create a regular ordinary least squares linear regression model with no regularization. We will then attempt to improve upon this model by adding a regularization penalty by means of the Ridge and Lasso Regression models.\n\nTo start, we of course need to first import all our data and some needed libraries.","a6281f3b":"# An Exploration of L1 and L2 Regularization\n\nAuthor: Bryson J. Banks (@bjbanks, GitHub)","c9fb6f0e":"Notice the U shape. We see from the plot the minimum RMSE occurs somewhere around when alpha is in the 10-15 range. Just to be a little more precise, we'll zoom in with values for alpha closer around this range.","cd244824":"Cool, so we see at an optimal alpha of about 0.0004, the lasso regression model seems to perform even better than the optimal ridge regression model for this data set. We now have RMSE = 0.11182, which is about a 8.17 % improvement from that of our linear regression model. This looks to be about the best RMSE we can hope to get using this training data and a single lasso regression without anymore advanced preprocessing or feature engineering.\n\nLet's now briefly take a look at the features the lasso regression model deems important. Note that the Lasso method will actually do feature selection for you - setting coefficients of features it deems unimportant to zero.","fd33376e":"### Normalize","70e0304b":"# Predictions","e3f7abf3":"# Ridge Regression (L2-Regularization) \n\nBoth L1 and L2 regularization aims to optimize the residual sum of squares (RSS) plus a regularization term. For ridge regression (L2), this regularization term is the **sum of the squared coefficients** times a non-negative scaling factor lambda (or alpha in our sklearn model). \n\nAs we did for the typcial linear regression model, we will again estimate this model's average RMSE in the same way for comparison. First, we will do this for alpha = 0.1, and then we will use cross validation to estimate the optimal alpha that produces the minimum RMSE. Note that 0.1 was chosen at random here, with no particular motivation.","5443588f":"# Data Preprocessing\n\nOur initial data preprocessing steps are adapted from [Alexandru](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) with a few additional steps. We'll choose simplicity here over more complicated steps as our focus is on the models themselves, and not advanced preprocessing techniques. We'll do just enough to be able to use and get reliable results from our regression models. Our steps will include:\n\n1. Drop clear outliers\n2. Convert numerical features to strings for features that should really be categorical (e.g. years, months, etc.)\n3. Encode all categorical labels with value between 0 and n_classes-1\n4. Normalize heavily skewed numerical features\n5. Create dummy\/indicator features from categorical features\n6. Replace missing values with feature means\n7. Set up matrices for regression models","b22941a5":"As expected, the regularization method has noticeably constrained the largest magnitude coefficient values toward zero when compared to those of the orginal linear regression model.","2b6ad910":"From the plot above, we see as the strength of the regularization parameter alpha grows, the number of selected features drops extremely quick from a max of 134 before eventually leveling off at 4 features when alpha is slightly greater than 0.25. It seems that in general, the higher the strength of alpha, the more restrictive the lasso model becomes with regards to the number of selected features. Keep this in mind when dealing with data sets that contain a large number of irrelevant features.","e390661f":"# Lasso Regression (L1-Regularization)\n\nBoth L1 and L2 regularization aims to optimize the residual sum of squares (RSS) plus a regularization term. For lasso regression (L2), this regularization term is the **sum of the squared coefficients** times a non-negative scaling factor lambda (or alpha in our sklearn model).\n\nAs we did for the ridge regression model, we will again estimate this model's average RMSE in the same way for comparison. First, we will do this for alpha = 0.1 just as before, and then we will use k-fold cross validation to estimate the optimal alpha that produces the minimum RMSE.","0d025ea1":"Cool, we see our log transform did surprisingly well, and had the intended effect - the new distribution looks much more \"normal\". Let's go ahead and apply this log transformation of \"SalePrice\" to our training data.","dd3d2a19":"As we'll see, several of the non-target numerical features are also heavily skewed, both rightly and leftly. For each of these, this time we'll choose to use a blanket \"yeo-johnson\" power transform to attempt to \"normalize\" each of them, since this tranform \"normalizes\" both righlty and leftly skewed data. (Here we consider all features with a \"skewness\" magnitude above 0.75 as \"heavily\" skewed.)","d7b12079":"Note we don't see any really high coefficient values chosen here because we did a fairly good job preprocessing our data. Had we not removed outliers and normalized the skewed numerical features for example, there would have been higher variance and a high chance of the model picking some noticably high coefficient values in comparison to these. Even with these values though, we'll still be able to see them get compressed toward zero by the regularization models.","ab6f827a":"### Encode Categorical Labels\n\nWe'll now encode all categorical feature labels with values between 0 and n_classes-1.","4be47ce1":"As these two observations clearly don't align with the rest of the data, we choose here to drop these as we don't want these clearly \"bad\" deals introducing extra bias in our prediction models.","ace1a55e":"# What is Regularization?\n\n[Regularization](https:\/\/towardsdatascience.com\/regularization-in-machine-learning-76441ddcf99a) is a modified form of regression with the purpose to minimize the risk of overfitting, particularly when [multicollinearity](https:\/\/en.wikipedia.org\/wiki\/Multicollinearity) exists within the feature set of the data. High levels of multicollinearity wihtin the feature set leads to increased variance of the coefficient estimates in a typical linear regression model, resulting in estimates that can be very sensitive to minor changes in the model.\n\nBy constraining, shrinking, or \"regularizing\" the regression coefficient estimates toward zero, this technique discourages our model from taking a more complex or flexible fit, in favor of a more stable fit with less variance in the coefficient estimates. In terms of a typical linear regression model using ordinary least squares, this is done by modifying our typcial loss function ([Residual Sum of Squares, RSS](https:\/\/en.wikipedia.org\/wiki\/Residual_sum_of_squares)) by adding a penalty for higher magnitude coefficient values.\n\nAs with any model, there are tradeoffs to consider when using regularization. We must carefully balance bias vs. variance, by tuning the hyper parameter that scales the magnitude of the added regularization penalty. The more we \"regularize\" the data, the more we will reduce variance, but only at the expense of introducing more bias. In this notebook we will be focusing on linear regularization practices, specifically looking at the two most common linear regularization methods, Lasso (L1) and Ridge (L2), how they compare to ordinary least squares linear regression, and methods for tuning each of their inherent hyper parameters using cross validation.\n\nFor a more detailed and mathematical explanation of regularization, I found this article particularly informative and easy to follow: http:\/\/www.chioka.in\/differences-between-l1-and-l2-as-loss-function-and-regularization\/","87ea185e":"## A quick look at the data\n\nAfter importing both the training and testing data sets, let's first check to see that the data was imported properly, and get a feel for what the data looks like. In particular, notice the combination of both numerical and categorical features, as well as the missing values.","ec65bcf7":"# Linear Regression\n\nThe typcial Ordinary Least Squares Linear Regression model aims to optimize the residual sum of squares (RSS), which is defined as:\n\n![Residual Sum of Squares](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/2f6526aa487b4dc460792bf1eeee79b2bba77709)\n\nTo analyze how well this model performs for this data set, we will fit the model using the training data, and then estimate the model's average root mean square error (RMSE) using k-fold cross validation. Note that we choose RMSE here to analyze the model's accuracy since RMSE is used by the [House Prices](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition for evaluation and scoring. There are many other metrics that could have been used instead.\n\nFirst, we define a function adapted from [Alexandru](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) to calculate average RMSE using k-fold cross validation and our training data, so that we can reliably estimate the RMSE produced by each of our models."}}