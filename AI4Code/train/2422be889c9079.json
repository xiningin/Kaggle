{"cell_type":{"f758403f":"code","8c8a3a83":"code","d31cc2f2":"code","10fd03e0":"code","88e8f201":"code","f70f6e8b":"code","3ad22364":"code","09cdd643":"code","801a90a8":"code","b7c3f6c3":"code","c99188a4":"code","441e1a5b":"code","3d25793d":"code","bcfc1aa7":"code","742d8f9c":"code","5a91a302":"code","b23f3482":"markdown","2a370c83":"markdown","ada63bb1":"markdown","f9c32126":"markdown","3d72629e":"markdown","62fd2f46":"markdown","7029d159":"markdown","07097532":"markdown","90055ea8":"markdown","3e35d9a3":"markdown","067699cc":"markdown"},"source":{"f758403f":"from functools import partial\nfrom concurrent.futures import ProcessPoolExecutor\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom tqdm import tnrange","8c8a3a83":"train_df = pd.read_csv(\n    '..\/input\/santander-value-prediction-challenge\/train.csv', index_col='ID')","d31cc2f2":"row_value_counts = [\n    {'id': row_id, 'value_counts': row_s[row_s != 0].value_counts()}\n    for row_id, row_s in train_df.iterrows()]","10fd03e0":"row_value_counts[0]","88e8f201":"def get_jaccard_index(row_value_count, row_value_count2):\n    intersection = (pd.concat(\n                        (row_value_count['value_counts'], row_value_count2['value_counts']),\n                        axis=1, join='inner')\n                    .min(1).sum())\n    union = (row_value_count['value_counts'].sum()\n             + row_value_count2['value_counts'].sum()\n             - intersection)\n    return intersection \/ union\n\ntry:\n    # The processs runs too long, so let's use the result I generated previously.\n    jaccard_index_df = pd.read_hdf('..\/input\/svpc-additional-data\/jaccard_index.h5')\nexcept IOError:\n    jaccard_index = []\n    with ProcessPoolExecutor() as executor:\n        for i in tnrange(len(row_value_counts) - 1):\n            result = executor.map(partial(get_jaccard_index, row_value_counts[i]),\n                                  row_value_counts[i+1:],\n                                  chunksize=8)\n            jaccard_index.extend(result)\n    index = pd.MultiIndex.from_tuples((i, j)\n                                      for i in range(len(row_value_counts) - 1)\n                                      for j in range(i+1, len(row_value_counts)))\n    jaccard_index_df = pd.DataFrame({'jaccard_index': jaccard_index}, index=index)\n    jaccard_index_df.to_hdf('jaccard_index.h5', 'df')","f70f6e8b":"jaccard_index_df","3ad22364":"jaccard_index_df.describe()","09cdd643":"jaccard_index_df['jaccard_index'].hist(bins=20)","801a90a8":"threshold = 0.95\npairs = jaccard_index_df.index[jaccard_index_df['jaccard_index'] > threshold].tolist()\nprint(\"number of pairs:\", len(pairs))\ng = nx.Graph()\ng.add_edges_from(pairs)\nprint(\"number of rows:\", len(g))\nconnected_components = list(nx.connected_components(g))\nprint(\"number of groups:\", len(connected_components))","b7c3f6c3":"biggest_component = max(connected_components, key=len)\n# biggest_component = connected_components[2]\nnx.draw_networkx(g.subgraph(biggest_component))","c99188a4":"rows = [2276, 1327, 2803, 1366, 3901, 2536, 2779, 4309]\nsame_user_df = train_df.iloc[rows]\nsame_user_df","441e1a5b":"def find_feature_pairs(assumed_future: np.ndarray, cols_to_match: np.ndarray):\n    is_matched = np.isclose(assumed_future, cols_to_match).all(0)\n    return np.where(is_matched)[0]\n            \n# remove all zero columns\nno_all_zeros_same_user_df = (same_user_df.loc[:, ~(same_user_df == 0).all()]\n                             .drop(columns='target'))\nlag_data = no_all_zeros_same_user_df.iloc[:-1].values\nfuture_data = no_all_zeros_same_user_df.iloc[1:].values\ncolumn_pairs = []\nfor i in range(lag_data.shape[1]):\n    matched_idx = find_feature_pairs(lag_data[:, [i]], future_data)\n    col_i = no_all_zeros_same_user_df.columns[i]\n    column_pairs.extend((col_i, no_all_zeros_same_user_df.columns[idx])\n                        for idx in matched_idx)","3d25793d":"print(\"number of pairs:\", len(column_pairs))\nfeature_g = nx.DiGraph()\nfeature_g.add_edges_from(column_pairs)\nprint(\"number of matched features:\", len(feature_g))\nprint(\"number of groups:\", nx.number_weakly_connected_components(feature_g))","bcfc1aa7":"# remove the in\/out edges of the nodes that have multiple in\/out edges\nfor node in list(feature_g.nodes):\n    out_edges = list(feature_g.out_edges(node))\n    if len(out_edges) > 1:\n        feature_g.remove_edges_from(out_edges)\n    in_edges = list(feature_g.in_edges(node))\n    if len(in_edges) > 1:\n        feature_g.remove_edges_from(in_edges)\n# remove isolated nodes\nfeature_g.remove_nodes_from(list(nx.isolates(feature_g)))\n\nprint(\"number of matched features:\", len(feature_g))\ncomponents = list(nx.weakly_connected_components(feature_g))\nprint(\"number of groups:\", len(components))\ncomponents.sort(key=len, reverse=True)\nCounter(len(c) for c in components)","742d8f9c":"time_series_features = list(nx.topological_sort(feature_g.subgraph(components[0])))\nprint(time_series_features)\nsame_user_df[['target'] + time_series_features]","5a91a302":"time_series_features = list(nx.topological_sort(feature_g.subgraph(components[1])))\nprint(time_series_features)\nsame_user_df[['target'] + time_series_features]","b23f3482":"- there is an obvious order\n- we actually need more analysis to make sure they are really close, but we simply trust it now","2a370c83":"## Build a directed graph","ada63bb1":"- this is obviously the label-related series","f9c32126":"## Calculate Jaccard Index","3d72629e":"## Calculate row value counts","62fd2f46":"## Use Jaccard Index to find row time series","7029d159":"## Find column time series based on the row time series we have found","07097532":"## Let's see the 2 time series that have length 40","90055ea8":"I saw a lot of people asking how to find the data leak from scratch. Here is the method I used at the very begining. I was also curious about this after Giba disclosed his findings. Therefore, I tried to do this from scratch without Giba's rows and columns. Though I trust Giba, I think it is still a little dangerous to use other people's findings directly.\n\nMy solution is based on [Jaccard index](https:\/\/en.wikipedia.org\/wiki\/Jaccard_index) and some graph techniques. I found 30 sets of columns using this simple algorithm, and 2 of them have length 40.","3e35d9a3":"## Prune suspicious edges","067699cc":"- this is another time series that have a lot of non-zero values"}}