{"cell_type":{"1ba3cf52":"code","f20d0e65":"code","3a15dbb8":"code","79e6a2b0":"code","65f0727f":"code","d2825189":"code","4608e519":"code","93c5b236":"code","34e7feb4":"code","8ca18d9d":"code","5569a0f4":"code","d1b09dfd":"code","fd851871":"code","d8701966":"code","03a27245":"code","3417ebb1":"code","16e9ffc3":"code","f826fadd":"code","a934bf33":"code","785e0ace":"code","7e8d6541":"code","363676cd":"code","905aa9f5":"code","991b7b12":"code","653773ff":"code","6608b04c":"code","184603af":"code","7196014f":"code","4edcf9af":"code","21d99fc1":"code","4468aabd":"code","9921bad1":"code","494fede9":"code","d39fa4c0":"code","8bebd3ef":"code","4ffce8c6":"code","5611cdd5":"code","d4e0c67d":"code","1c00a615":"code","43c06cb1":"code","f5044a8c":"code","4fb9aaf9":"code","f2ffe5eb":"code","6e8df6cb":"code","adb1a623":"code","339731e4":"code","e9ddcf20":"code","02460f11":"code","219a5df1":"code","b37888f4":"code","7b3aadae":"code","9c8ca0f7":"code","dd74588f":"code","5dfef037":"code","e8fb6db5":"code","4d528d75":"code","536d9ee8":"code","69611220":"code","4edcbaf5":"code","8fd84175":"code","5314e054":"code","8ae504e7":"markdown","e05a3d7d":"markdown"},"source":{"1ba3cf52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f20d0e65":"import os\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport json\nfrom tqdm import tqdm_notebook\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import mean_absolute_error","3a15dbb8":"from nltk.corpus import stopwords\nimport gensim\nfrom scipy import sparse\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nimport seaborn as sns\nimport pyLDAvis.gensim","79e6a2b0":"from html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()","65f0727f":"def read_json_line(line=None):\n    result = None\n    try:        \n        result = json.loads(line)\n    except Exception as e:      \n        # Find the offending character index:\n        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n        # Remove the offending character:\n        new_line = list(line)\n        new_line[idx_to_replace] = ' '\n        new_line = ''.join(new_line)     \n        return read_json_line(line=new_line)\n    return result","d2825189":"def preprocess(path_to_inp_json_file):\n    output_list = []\n    with open(path_to_inp_json_file, encoding='utf-8') as inp_file:\n        for line in tqdm_notebook(inp_file):\n            json_data = read_json_line(line)\n            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n            content_no_html_tags = strip_tags(content)\n            output_list.append(content_no_html_tags)\n    return output_list","4608e519":"%%time\ntrain_raw_content = preprocess(path_to_inp_json_file=os.path.join('..\/input', \n                                                                  'train.json'),)","93c5b236":"train_raw_content[0]","34e7feb4":"%%time\ntest_raw_content = preprocess(path_to_inp_json_file=os.path.join('..\/input', \n                                                                  'test.json'),)","8ca18d9d":"# custom stop_words for job search \n# custom_stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\",\n#                      'aa','aaa','bb','bbb','c','ccc','d','ddd','e','eee','f','fff','ummm','hmmm','xiii','xxiii','http','https','sooo','orc','mmm',\n#                      'ets'\n           \n#                     ]","5569a0f4":"# from nltk import word_tokenize          \n# from nltk.stem import WordNetLemmatizer \n# class LemmaTokenizer(object):\n#     def __init__(self):\n#         self.wnl = WordNetLemmatizer()\n#     def __call__(self, articles):\n#         return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]","d1b09dfd":"# cv = CountVectorizer(max_features=80000,stop_words=custom_stop_words, analyzer='word',min_df=1,max_df=0.7,token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n#                      )","fd851871":"# %%time\n# X_train = cv.fit_transform(train_raw_content)","d8701966":"# cv.vocabulary_.items()= ","03a27245":"#cv.get_feature_names()","3417ebb1":"# %%time\n# X_test = cv.transform(test_raw_content)","16e9ffc3":"# print(X_train[2,:])","f826fadd":"# X_train.shape, X_test.shape","a934bf33":"# train_target = pd.read_csv(os.path.join('..\/input', 'train_log1p_recommends.csv'), \n#                            index_col='id')","785e0ace":"# train_target.shape","7e8d6541":"# y_train = train_target['log_recommends'].values","363676cd":"# train_part_size = int(0.7 * train_target.shape[0])\n# X_train_part = X_train[:train_part_size, :]\n# y_train_part = y_train[:train_part_size]\n# X_valid =  X_train[train_part_size:, :]\n# y_valid = y_train[train_part_size:]","905aa9f5":"# from sklearn.linear_model import Ridge","991b7b12":"# ridge = Ridge(random_state=17,alpha=0.21)","653773ff":"# %%time\n# ridge.fit(X_train_part, y_train_part);","6608b04c":"# ridge_pred = ridge.predict(X_valid)","184603af":"# plt.hist(y_valid, bins=30, alpha=.5, color='red', label='true', range=(0,10));\n# plt.hist(ridge_pred, bins=30, alpha=.5, color='green', label='pred', range=(0,10));\n# plt.legend();","7196014f":"# valid_mae = mean_absolute_error(y_valid, ridge_pred)\n# valid_mae, np.expm1(valid_mae)","4edcf9af":"# %%time\n# ridge.fit(X_train, y_train);","21d99fc1":"# %%time\n# ridge_test_pred = ridge.predict(X_test)","4468aabd":"# def write_submission_file(prediction, filename,\n#     path_to_sample=os.path.join('..\/input', 'sample_submission.csv')):\n#     submission = pd.read_csv(path_to_sample, index_col='id')\n    \n#     submission['log_recommends'] = prediction\n#     submission.to_csv(filename)","9921bad1":"# write_submission_file(prediction=ridge_test_pred, \n#                       filename='first_ridge4.csv')","494fede9":"# full_sparse_data =  sparse.vstack([X_train, X_test])","d39fa4c0":"# #Transform our sparse_data to corpus for gensim\n# corpus_data_gensim = gensim.matutils.Sparse2Corpus(full_sparse_data, documents_columns=False)","8bebd3ef":"# #Create dictionary for LDA model\n# vocabulary_gensim = {}\n# for key, val in cv.vocabulary_.items():\n#     vocabulary_gensim[val] = key\n    \n# dict = Dictionary()\n# dict.merge_with(vocabulary_gensim)","4ffce8c6":"# lda = LdaModel(corpus_data_gensim, num_topics = 30 )","5611cdd5":"# data_ =  pyLDAvis.gensim.prepare(lda, corpus_data_gensim, dict)\n","d4e0c67d":"# pyLDAvis.display(data_)","1c00a615":"# def document_to_lda_features(lda_model, document):\n#     topic_importances = lda.get_document_topics(document, minimum_probability=0)\n#     topic_importances = np.array(topic_importances)\n#     return topic_importances[:,1]\n\n# lda_features = list(map(lambda doc:document_to_lda_features(lda, doc),corpus_data_gensim))","43c06cb1":"# data_pd_lda_features = pd.DataFrame(lda_features)\n# data_pd_lda_features.head()","f5044a8c":"# data_pd_lda_features_train = data_pd_lda_features.iloc[:y_train.shape[0]]\n# data_pd_lda_features_train['target'] = y_train\n\n# fig, ax = plt.subplots()\n# # the size of A4 paper\n# fig.set_size_inches(20.7, 8.27)\n# sns.heatmap(data_pd_lda_features_train.corr(method = 'spearman'), cmap=\"RdYlGn\", ax = ax)","4fb9aaf9":"# X_tr = sparse.hstack([X_train, data_pd_lda_features_train.drop('target', axis = 1)]).tocsr()","f2ffe5eb":"# X_test1 = sparse.hstack([X_test, data_pd_lda_features.iloc[y_train.shape[0]:]]).tocsr()","6e8df6cb":"# ridge = Ridge(random_state=17)\n# ridge.fit(X_tr,y_train)","adb1a623":"# ridge_test_pred1 = ridge.predict(X_test1)","339731e4":"# plt.hist(ridge_test_pred1, bins=30, alpha=.5, color='green', label='pred', range=(0,10));\n# plt.legend();","e9ddcf20":"# write_submission_file(prediction=ridge_test_pred1, \n#                       filename='first_ridge_lda.csv')","02460f11":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re","219a5df1":"# train_raw_content.","b37888f4":"max_fatures = 30000\ntokenizer = Tokenizer(nb_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(train_raw_content)\nX1 = tokenizer.texts_to_sequences(train_raw_content)\n","7b3aadae":"tokenizer.fit_on_texts(test_raw_content)\nX2_test = tokenizer.texts_to_sequences(test_raw_content)","9c8ca0f7":"X1 = pad_sequences(X1,maxlen=900)","dd74588f":"X2_test = pad_sequences(X2_test,maxlen=900)","5dfef037":"train_target = pd.read_csv(os.path.join('..\/input', 'train_log1p_recommends.csv'), \n                           index_col='id')\nY1 = train_target['log_recommends'].values","e8fb6db5":"X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, random_state = 42)\nprint(X1_train.shape,Y1_train.shape)\nprint(X1_test.shape,Y1_test.shape)","4d528d75":"embed_dim = 150\nlstm_out = 200\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X1.shape[1], dropout=0.2))\nmodel.add(LSTM(lstm_out, dropout_U=0.2,dropout_W=0.2))\nmodel.add(Dense(1,kernel_initializer='normal'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam')\nprint(model.summary())","536d9ee8":"batch_size = 80\nmodel.fit(X1_train, Y1_train, nb_epoch = 5, batch_size=batch_size, verbose = 2)","69611220":"lstm_test_pred = model.predict(X2_test)","4edcbaf5":" def write_submission_file(prediction, filename,\n    path_to_sample=os.path.join('..\/input', 'sample_submission.csv')):\n    submission = pd.read_csv(path_to_sample, index_col='id')\n    \n    submission['log_recommends'] = prediction\n    submission.to_csv(filename)","8fd84175":"lstm_test_pred.shape","5314e054":"write_submission_file(prediction=lstm_test_pred, \n                      filename='first_lstm.csv')","8ae504e7":"#### Applying LDA ","e05a3d7d":"### LSTM"}}