{"cell_type":{"18ee50e1":"code","80ed7875":"code","84b10f60":"code","86d8671e":"code","1b971e13":"code","9f94ac24":"code","ea9d4162":"code","8246bd2b":"code","6b992616":"code","fe818028":"code","5153b8f6":"code","1e0e3ecd":"code","05e6037b":"code","48b2acb0":"code","b794e67e":"code","f081327f":"code","2399d260":"code","3da1d5d0":"code","70444642":"code","dfd404eb":"code","7562b801":"code","045acb5a":"code","b572ad41":"code","a0c7130d":"code","87ef7a4c":"code","db9c68b4":"code","485021c3":"code","8f4a8b1d":"code","d2ebcca4":"code","f42f52cd":"code","bca5f0f4":"code","25af7a2d":"code","58a69c40":"code","830e27c0":"code","cea52113":"code","5d65118c":"code","66a0359f":"code","4c4ecc28":"code","ff57e049":"code","9920e71e":"code","80ed73e8":"code","6b3a8dff":"code","9faf6ccd":"code","596a4d29":"code","7dbc1828":"code","78aa4702":"code","15fe5a72":"code","e06f8d0b":"code","691ec113":"code","9f2e053f":"code","74b6ffe5":"code","f322c491":"code","757583d6":"code","ac1bc8db":"code","b5a90312":"code","036524be":"code","648142a7":"code","92557d75":"code","4ebe90e0":"code","8630d5f3":"code","763843cc":"code","e1ab77ab":"code","ea30048e":"code","27073cf9":"code","0438b4a4":"code","d8f2eae4":"code","1a09d107":"code","fc5d903e":"code","aa48b46b":"code","b68fb15e":"code","dc7a8f55":"code","920b2132":"code","a3a42332":"code","101e5a3f":"code","faf6a1d8":"code","74d35a2b":"code","3f0758d3":"code","2464efb6":"code","aeacf9ea":"code","8673e93e":"code","2d2324b1":"code","52dfbb2c":"code","273d147d":"code","4cd16b66":"code","4ac27be8":"code","18f7f3aa":"code","bace5672":"code","00f6c0e6":"code","54ef7776":"code","b2f611d9":"code","9a7f0cdb":"code","26d7661a":"code","ace94e52":"code","45ed67e1":"code","b764298b":"code","e3cd444a":"code","77b73eba":"code","25375341":"code","85748636":"code","bdb9818d":"code","8137e10b":"code","a26140b1":"code","049bf3d2":"code","df4bfe44":"code","3d89ccba":"code","6f1d48b0":"code","380e247c":"code","8d0f39aa":"code","f89f470d":"code","de0ec0ae":"code","e0fe9170":"code","cf8aa95a":"code","44c3a46a":"code","5cbe5bf5":"code","61f55408":"code","1107d614":"code","9b367941":"code","d7c8481d":"code","363bc01c":"code","54fb4c8d":"code","3b1a327f":"code","8d4c2ca9":"code","d9ec3672":"code","162a7393":"code","137e64cb":"code","a95235f3":"code","a8c08263":"code","2cb14539":"code","77540f5f":"code","b2c43065":"code","6a9c01fe":"code","812411b5":"code","346917ef":"code","f12c0976":"code","9217338a":"markdown","7a812294":"markdown","7d69b0e0":"markdown","edb85cf4":"markdown","8fe1287a":"markdown","b5808740":"markdown","59f57ad2":"markdown","c21255e6":"markdown","b51a5c0a":"markdown","3ca77d9f":"markdown","18f6147c":"markdown","4e91a731":"markdown","3b56fc63":"markdown","90fca87d":"markdown","f21595b3":"markdown","4b4be134":"markdown","4bdb9382":"markdown","ccc0f4b6":"markdown","f8e315ed":"markdown","051f1cac":"markdown","d921da8d":"markdown","686b5385":"markdown","4905e45a":"markdown","8434c013":"markdown","e800848a":"markdown","c1eb606c":"markdown","9e10e7b7":"markdown","c8f5899b":"markdown","dd612005":"markdown","6492082a":"markdown","9dc6d292":"markdown","971c86c1":"markdown","59edcef2":"markdown","92200a54":"markdown","7b713983":"markdown","8c5d2b1a":"markdown","d32665c9":"markdown","31b20f65":"markdown","b46dc383":"markdown","e1eb3195":"markdown","4d8f2668":"markdown","acf16bbf":"markdown","a6b14e12":"markdown","e881247f":"markdown","e58d581f":"markdown","c858f218":"markdown","2675b0c9":"markdown","dbab090a":"markdown","8a3dcbfa":"markdown","d170cc47":"markdown","0facfeb3":"markdown","fdea29ab":"markdown","fd932f1c":"markdown","bf36cefa":"markdown","2c995bb8":"markdown","a8b5bec1":"markdown","b9e7d547":"markdown","124bfd55":"markdown","a1e23a3b":"markdown","e1bc9cf0":"markdown","50cc766b":"markdown","dad6c954":"markdown","e97bea93":"markdown","90e42232":"markdown","349852a5":"markdown","6e449028":"markdown","da7568d1":"markdown","32ad913a":"markdown"},"source":{"18ee50e1":"#linear algebra\nimport numpy as np\n\n#dataframe\nimport pandas as pd\n\n#data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#regex\nimport re\n\n#machine learning\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import classification_report,f1_score,accuracy_score\nfrom xgboost import XGBClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom vecstack import stacking\n\n#neural network\nimport tensorflow as tf\nfrom tensorflow import keras","80ed7875":"df_flight = pd.read_csv(\"\/kaggle\/input\/datavidia2019\/flight.csv\")\ndf_hotel = pd.read_csv(\"\/kaggle\/input\/datavidia2019\/hotel.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/datavidia2019\/test.csv\")","84b10f60":"df_flight.sample(3)","86d8671e":"df_flight.info()","1b971e13":"df_hotel.head()","9f94ac24":"df_hotel.info()","ea9d4162":"df_test.head(3)","8246bd2b":"df_test.info()","6b992616":"df_flight.drop([\"order_id\"],axis=1,inplace=True)\ndf_test.drop([\"order_id\"],axis=1,inplace=True)","fe818028":"# function for fix list in form of string into a python accessable list\ndef stringoflist_to_list(text,as_float=False):\n    result = re.sub(\"[\\'\\,\\[\\]]\",\"\",text)\n    result = re.split(\"\\s\",result)\n    if(as_float):  \n        result = [float(x) for x in result]  \n    return result","5153b8f6":"#fix the df_flight dataframe\nvisited_city_idx = df_flight.columns.get_loc(\"visited_city\")\nlog_transaction_idx = df_flight.columns.get_loc(\"log_transaction\")\nfor index, row in df_flight.iterrows():\n    df_flight.iat[index, visited_city_idx] = stringoflist_to_list(row[\"visited_city\"],False)\n    df_flight.iat[index, log_transaction_idx] = stringoflist_to_list(row[\"log_transaction\"],True)","1e0e3ecd":"#fix the df_test dataframe\nvisited_city_idx = df_test.columns.get_loc(\"visited_city\")\nlog_transaction_idx = df_test.columns.get_loc(\"log_transaction\")\nfor index, row in df_test.iterrows():\n    df_test.iat[index, visited_city_idx] = stringoflist_to_list(row[\"visited_city\"],False)\n    df_test.iat[index, log_transaction_idx] = stringoflist_to_list(row[\"log_transaction\"],True)","05e6037b":"df_flight.sample(3)","48b2acb0":"df_test.sample(3)","b794e67e":"df_test[\"member_duration_days\"] = df_test[\"member_duration_days\"].astype(\"float64\")\ndf_test[\"no_of_seats\"] = df_test[\"no_of_seats\"].astype(\"float64\")","f081327f":"df_flight[\"member_duration_days\"].describe()","2399d260":"df_test[\"member_duration_days\"].describe()","3da1d5d0":"df_flight[\"account_id\"].value_counts(sort=True)","70444642":"df_flight[\"gender\"].value_counts()","dfd404eb":"df_test[\"gender\"].value_counts()","7562b801":"df_flight[\"trip\"].value_counts()","045acb5a":"df_test[\"trip\"].value_counts()","b572ad41":"df_flight[\"trip\"] = np.where(df_flight[\"trip\"]==\"trip\",\"trip\",\"roundtrip\")\ndf_test[\"trip\"] = np.where(df_test[\"trip\"]==\"trip\",\"trip\",\"roundtrip\")","a0c7130d":"df_flight[\"trip\"].value_counts()","87ef7a4c":"df_test[\"trip\"].value_counts()","db9c68b4":"df_flight[\"service_class\"].value_counts()","485021c3":"df_test[\"service_class\"].value_counts()","8f4a8b1d":"df_flight[\"price\"].describe()","d2ebcca4":"df_test[\"price\"].describe()","f42f52cd":"df_flight[\"is_tx_promo\"].value_counts()","bca5f0f4":"df_test[\"is_tx_promo\"].value_counts()","25af7a2d":"df_flight[\"no_of_seats\"].describe()","58a69c40":"df_test[\"no_of_seats\"].describe()","830e27c0":"df_flight[\"airlines_name\"].value_counts()","cea52113":"df_test[\"airlines_name\"].value_counts()","5d65118c":"df_flight[\"route\"].value_counts()","66a0359f":"df_test[\"route\"].value_counts()","4c4ecc28":"df_flight.drop([\"route\"],axis=1,inplace=True)\ndf_test.drop([\"route\"],axis=1,inplace=True)","ff57e049":"df_flight[\"hotel_id\"].value_counts()","9920e71e":"df_flight[\"is_cross_sell\"] = np.where(df_flight[\"hotel_id\"]==\"None\",0,1)","80ed73e8":"df_flight[\"is_cross_sell\"].value_counts()","6b3a8dff":"df_flight.drop([\"hotel_id\"],axis=1,inplace=True)","9faf6ccd":"df_flight.sample(3)","596a4d29":"Var_Corr = df_flight.corr()\nsns.heatmap(Var_Corr, xticklabels=Var_Corr.columns, yticklabels=Var_Corr.columns, annot=True)\nplt.title(\"Correlation heatmap\")\nplt.show()","7dbc1828":"sns.countplot(x=\"is_cross_sell\", data=df_flight)\nplt.title(\"is_cross_sell class count\")\nplt.show()","78aa4702":"sns.distplot(df_flight[\"member_duration_days\"])\nplt.title(\"member_duration_days data distribution\")\nplt.show()","15fe5a72":"sns.boxplot(x=\"is_cross_sell\", y=\"member_duration_days\", data=df_flight)\nplt.title(\"is_cross_sell vs member_duration_days\")\nplt.show()","e06f8d0b":"sns.countplot(x=\"gender\", hue=\"is_cross_sell\", data=df_flight)\nplt.title(\"gender count\")\nplt.show()","691ec113":"sns.countplot(x=\"trip\", hue=\"is_cross_sell\", data=df_flight)\nplt.title(\"trip count\")\nplt.show()","9f2e053f":"sns.distplot(df_flight[\"price\"])\nplt.title(\"price data distribution\")\nplt.show()","74b6ffe5":"sns.boxplot(x=\"is_cross_sell\", y=\"price\", data=df_flight)\nplt.title(\"is_cross_sell vs price\")\nplt.show()","f322c491":"sns.countplot(x=\"is_tx_promo\", hue=\"is_cross_sell\", data=df_flight)\nplt.title(\"tx_with_promo count\")\nplt.show()","757583d6":"sns.boxplot(x=\"is_cross_sell\", y=\"no_of_seats\", data=df_flight)\nplt.title(\"is_cross_sell vs num_of_seats\")\nplt.show()","ac1bc8db":"sns.countplot(x=\"airlines_name\", hue=\"is_cross_sell\", data=df_flight)\nplt.title(\"airlines_name count\")\nplt.show()","b5a90312":"## df_flight\n\n#initial value for the column\ndf_flight[\"number_city_visited\"] = 0.0\n\n#get the index of the column number_city_visited\nthe_idx = df_flight.columns.get_loc(\"number_city_visited\")\n\n\n#assign the value for every row\nfor index, row in df_flight.iterrows():\n    df_flight.iat[index, the_idx] = len(row[\"visited_city\"])\n    \n#drop the original column as we already got what we need\ndf_flight.drop([\"visited_city\"],axis=1,inplace=True)","036524be":"##same process as the above cell but for df_test\ndf_test[\"number_city_visited\"] = 0.0\nthe_idx = df_test.columns.get_loc(\"number_city_visited\")\nfor index, row in df_test.iterrows():\n    df_test.iat[index, the_idx] = len(row[\"visited_city\"])\ndf_test.drop([\"visited_city\"],axis=1,inplace=True)","648142a7":"##df_flight\n#initial column value\ndf_flight[\"amount_spent\"] = 0.0\ndf_flight[\"min_spend\"] = 0.0\ndf_flight[\"max_spend\"] = 0.0\ndf_flight[\"average_spend\"] = 0.0\ndf_flight[\"number_of_transaction\"] = 0.0\n\n#assign value for each row\nfor index, row in df_flight.iterrows():\n    df_flight.iat[index, -5] = sum(row[\"log_transaction\"])\n    df_flight.iat[index, -4] = min(row[\"log_transaction\"])\n    df_flight.iat[index, -3] = max(row[\"log_transaction\"])\n    df_flight.iat[index, -2] = np.average(row[\"log_transaction\"])\n    df_flight.iat[index, -1] = len(row[\"log_transaction\"])\n    \n# drop the original column as we already got what we need\ndf_flight.drop([\"log_transaction\"],axis=1,inplace=True)","92557d75":"#same process for the above cell but fr df_test\ndf_test[\"amount_spent\"] = 0.0\ndf_test[\"min_spend\"] = 0.0\ndf_test[\"max_spend\"] = 0.0\ndf_test[\"average_spend\"] = 0.0\ndf_test[\"number_of_transaction\"] = 0.0\nfor index, row in df_test.iterrows():\n    df_test.iat[index, -5] = sum(row[\"log_transaction\"])\n    df_test.iat[index, -4] = min(row[\"log_transaction\"])\n    df_test.iat[index, -3] = max(row[\"log_transaction\"])\n    df_test.iat[index, -2] = np.average(row[\"log_transaction\"])\n    df_test.iat[index, -1] = len(row[\"log_transaction\"])\ndf_test.drop([\"log_transaction\"],axis=1,inplace=True)","4ebe90e0":"df_flight.sample(3)","8630d5f3":"df_test.sample(3)","763843cc":"ics_history = df_flight.groupby([\"account_id\"]).mean()[\"is_cross_sell\"]","e1ab77ab":"df_flight = pd.merge(df_flight, ics_history, on='account_id', how=\"left\")\nold_column_name = df_flight.columns[-1]\ndf_flight = df_flight.rename(columns = {\"is_cross_sell_x\" : \"is_cross_sell\", old_column_name : \"ics_probability\"})","ea30048e":"df_test = pd.merge(df_test, ics_history, on='account_id', how=\"left\")\nold_column_name = df_test.columns[-1]\ndf_test[old_column_name] = df_test[old_column_name].fillna(0)\ndf_test = df_test.rename(columns = {\"is_cross_sell_x\" : \"is_cross_sell\", old_column_name : \"ics_probability\"})","27073cf9":"df_flight.drop([\"account_id\"],axis=1,inplace=True)\ndf_test.drop([\"account_id\"],axis=1,inplace=True)","0438b4a4":"df_flight.sample(3)","d8f2eae4":"df_test.sample(3)","1a09d107":"#trip\nle = LabelEncoder()\ndf_flight['trip'] = le.fit_transform(df_flight['trip'])\n\n#service_class\nle = LabelEncoder()\ndf_flight['service_class'] = le.fit_transform(df_flight['service_class'])\n\n#is_tx_promo\nle = LabelEncoder()\ndf_flight['is_tx_promo'] = le.fit_transform(df_flight['is_tx_promo'])","fc5d903e":"#trip\nle = LabelEncoder()\ndf_test['trip'] = le.fit_transform(df_test['trip'])\n\n#service_class\nle = LabelEncoder()\ndf_test['service_class'] = le.fit_transform(df_test['service_class'])\n\n#is_tx_promo\nle = LabelEncoder()\ndf_test['is_tx_promo'] = le.fit_transform(df_test['is_tx_promo'])","aa48b46b":"# make sure to concat the training and testing so both of the data get equal data dummies\ndf = pd.concat([df_flight[df_flight.columns[:-1]], df_test],sort = False)\nairlines_bin_df = pd.get_dummies(df[\"airlines_name\"], prefix=\"an\")\ngender_bin_df = pd.get_dummies(df[\"gender\"], prefix=\"g\")","b68fb15e":"df_flight = pd.concat([df_flight, gender_bin_df.iloc[:len(df_flight)], airlines_bin_df.iloc[:len(df_flight)]], axis = 1)\ndf_flight.drop([\"gender\"],axis=1,inplace=True)\ndf_test.drop([\"gender\"],axis=1,inplace=True)\n\ndf_test = pd.concat([df_test, gender_bin_df.iloc[len(df_flight):], airlines_bin_df.iloc[len(df_flight):]], axis = 1)\ndf_flight.drop([\"airlines_name\"],axis=1,inplace=True)\ndf_test.drop([\"airlines_name\"],axis=1,inplace=True)","dc7a8f55":"df_flight.sample(3)","920b2132":"df_test.sample(3)","a3a42332":"feature_to_scale = [\"member_duration_days\",\"price\",\"no_of_seats\",\"number_city_visited\",\"amount_spent\",\"min_spend\",\"max_spend\",\"average_spend\",\"number_of_transaction\"]","101e5a3f":"minmax_scaler = MinMaxScaler()\ndf_temp = pd.concat([df_flight.drop([\"is_cross_sell\"],axis=1), df_test],sort=False)\nminmax_scaler.fit(df_temp[feature_to_scale],)","faf6a1d8":"df_flight[feature_to_scale] = minmax_scaler.transform(df_flight[feature_to_scale])\ndf_test[feature_to_scale] = minmax_scaler.transform(df_test[feature_to_scale])","74d35a2b":"df_flight.sample(3)","3f0758d3":"df_test.sample(3)","2464efb6":"df_flight.sample(3)","aeacf9ea":"df_test.sample(3)","8673e93e":"X = df_flight.drop([\"is_cross_sell\"],axis=1)\ny = df_flight[\"is_cross_sell\"]","2d2324b1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","52dfbb2c":"# the hyperparameter is tuned on other kernel\nrf_clf = RandomForestClassifier(\n    bootstrap=False,\n    max_depth=50,\n    max_features = \"auto\",\n    min_samples_leaf=1,\n    min_samples_split=2,\n    n_estimators=1000,\n    n_jobs=-1\n)","273d147d":"rf_clf.fit(X_train,y_train)","4cd16b66":"rf_clf.score(X_test,y_test)","4ac27be8":"f1_score(rf_clf.predict(X_test),y_test)","18f7f3aa":"knn_clf = KNeighborsClassifier(n_neighbors=5)","bace5672":"knn_clf.fit(X_train,y_train)","00f6c0e6":"knn_clf.score(X_test,y_test)","54ef7776":"f1_score(knn_clf.predict(X_test),y_test)","b2f611d9":"lr_clf = LogisticRegression(max_iter=1000)","9a7f0cdb":"lr_clf.fit(X_train,y_train)","26d7661a":"lr_clf.score(X_test,y_test)","ace94e52":"f1_score(lr_clf.predict(X_test),y_test)","45ed67e1":"n_cols = X_train.shape[1] \nnn_clf = keras.Sequential()\nnn_clf.add(keras.layers.Dense(20, activation='relu', input_shape=(n_cols,)))\nnn_clf.add(keras.layers.Dropout(rate=0.2))\nnn_clf.add(keras.layers.Dense(20, activation='relu'))\nnn_clf.add(keras.layers.Dropout(rate=0.2))\nnn_clf.add(keras.layers.Dense(1, activation='sigmoid'))","b764298b":"nn_clf.compile( \n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","e3cd444a":"nn_clf.fit(X_train, y_train, epochs=10, verbose=2)","77b73eba":"nn_clf.evaluate(X_test,y_test)","25375341":"f1_score(np.where(nn_clf.predict(X_test)> 0.5, 1, 0),y_test)","85748636":"xgb_clf = XGBClassifier(random_state=0)","bdb9818d":"xgb_clf.fit(X_train,y_train)","8137e10b":"xgb_clf.score(X_test,y_test)","a26140b1":"f1_score(xgb_clf.predict(X_test),y_test)","049bf3d2":"rf_clf = RandomForestClassifier(\n    bootstrap=False,\n    max_depth=50,\n    max_features = \"auto\",\n    min_samples_leaf=1,\n    min_samples_split=2,\n    n_estimators=1000,\n    n_jobs=-1\n)","df4bfe44":"knn_clf = KNeighborsClassifier(n_neighbors=5)","3d89ccba":"lr_clf = LogisticRegression(max_iter=1000)","6f1d48b0":"models = [\n    rf_clf,\n    knn_clf,\n    lr_clf,\n]","380e247c":"S_train, S_test = stacking(\n    models,                   \n    X_train, y_train, X_test,   \n    regression=False, \n    mode='oof_pred_bag', \n    needs_proba=False,\n    save_dir=None, \n    metric=accuracy_score, \n    n_folds=4, \n    stratified=True,\n    shuffle=True,  \n    random_state=0,    \n    verbose=2\n)","8d0f39aa":"S_train = pd.DataFrame(S_train, columns = [\"rf\",\"knn\",\"lr\"])\nS_test = pd.DataFrame(S_test, columns = [\"rf\",\"knn\",\"lr\"])","f89f470d":"X_train.reset_index(inplace=True)\nX_test.reset_index(inplace=True)","de0ec0ae":"df_stack_train = pd.concat([X_train,S_train],axis=1)\ndf_stack_test = pd.concat([X_test,S_test],axis=1)","e0fe9170":"df_stack_train.drop([\"index\"],axis=1,inplace=True)\ndf_stack_train.head()","cf8aa95a":"df_stack_test.drop([\"index\"],axis=1,inplace=True)\ndf_stack_test.head()","44c3a46a":"n_cols = df_stack_train.shape[1]","5cbe5bf5":"nn_clf = keras.Sequential()\nnn_clf.add(keras.layers.Dense(16, activation='relu', input_shape=(n_cols,)))\nnn_clf.add(keras.layers.Dropout(rate=0.2))\nnn_clf.add(keras.layers.Dense(1, activation='sigmoid'))","61f55408":"nn_clf.compile( \n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","1107d614":"xgb_clf = XGBClassifier(random_state=0)","9b367941":"nn_clf.fit(df_stack_train, y_train, epochs=10, verbose=2)","d7c8481d":"y_pred_nn = np.where(nn_clf.predict(df_stack_test)> 0.5, 1, 0)","363bc01c":"f1_score(y_pred_nn,y_test)","54fb4c8d":"xgb_clf.fit(df_stack_train,y_train)","3b1a327f":"y_pred_xgb = xgb_clf.predict(df_stack_test)","8d4c2ca9":"xgb_clf.score(df_stack_test,y_test)","d9ec3672":"f1_score(y_pred_xgb,y_test)","162a7393":"rf_clf = RandomForestClassifier(\n    bootstrap=False,\n    max_depth=50,\n    max_features = \"auto\",\n    min_samples_leaf=1,\n    min_samples_split=2,\n    n_estimators=1000,\n    n_jobs=-1\n)","137e64cb":"knn_clf = KNeighborsClassifier(n_neighbors=5)","a95235f3":"lr_clf = LogisticRegression(max_iter=1000)","a8c08263":"models = [\n    rf_clf,\n    knn_clf,\n    lr_clf,\n]","2cb14539":"S_train, S_test = stacking(\n    models,                   \n    X, y, df_test,   \n    regression=False, \n    mode='oof_pred_bag', \n    needs_proba=False,\n    save_dir=None, \n    metric=accuracy_score, \n    n_folds=4, \n    stratified=True,\n    shuffle=True,  \n    random_state=0,    \n    verbose=2\n)","77540f5f":"S_train = pd.DataFrame(S_train, columns = [\"rf\",\"knn\",\"lr\"])\nS_test = pd.DataFrame(S_test, columns = [\"rf\",\"knn\",\"lr\"])","b2c43065":"df_stack_train = pd.concat([X,S_train],axis=1)\ndf_stack_test = pd.concat([df_test,S_test],axis=1)","6a9c01fe":"xgb_clf = XGBClassifier(random_state=0)","812411b5":"xgb_clf.fit(df_stack_train,y)","346917ef":"y_pred_xgb = xgb_clf.predict(df_stack_test)","f12c0976":"temp = pd.read_csv(\"\/kaggle\/input\/datavidia2019\/test.csv\")\ntemp = temp[\"order_id\"]\n\nd = {'order_id': temp, 'is_cross_sell': y_pred_xgb}\noutput = pd.DataFrame(data = d)\n\noutput[\"is_cross_sell\"] = np.where(output[\"is_cross_sell\"]==0,\"no\",\"yes\")\noutput.to_csv(\"stacking-meta-xgb.csv\",index=False)","9217338a":"<b>Logistic Regression<\/b>","7a812294":"<b>2.2. member duration days<\/b>","7d69b0e0":"In the previous section, spotted that there are slight difference in <i>member_duration_days<\/i> and <i>no_of_seats<\/i> features data type. In cell below we fix the consistency of the feature","edb85cf4":"<b>Logistic Regression<\/b>","8fe1287a":"only CGK-DPS route available. So there's no point of using this feature. We remove the <i>route<\/i> feature in cell below","b5808740":"No problem for <i>is_tx_promo<\/i> feature","59f57ad2":"<b>5.2. Meta Classifier<\/b>\n<p>We will build the meta classifier. In this chance, we try to use neural network and XGBoost Classifier<\/p>","c21255e6":"<b>4.2. Stacking Model<\/b>\n<p>The idea of this model is to use the output of ML algorithm (layer 0) as a new feature for another ML algorithm (layer 1) <\/p>","b51a5c0a":"Now we can drop the <i>hotel_id<\/i> feature ","3ca77d9f":"We can see that using neural network as meta classifier not giving any improvement while using XGBoost giving a slight improvement which is worth to try. So for now we are using this method which is our best accuracy obtained","18f6147c":"<b>3.1. Data labelling<\/b>\n<p>Next, we're going to transform any attribute which the type is text into a number<\/p>\n\n1. binary categorical data will be transform to 0\/1 with scikit-learn LabelEncoder class.\n2. multinomial categorical data which is in this data is gender and airlines name will be transform into one hot encoding with pandas get_dummies method.","4e91a731":"<b>2.3. Gender<\/b>","3b56fc63":"From the data_dictionary.csv, told that there are only 2 kind of trip available which is <i>trip<\/i> and <i>roundtrip<\/i>. By this fact,<b> we assume that <i>round<\/i> and <i>roundtrip<\/i> are the same type of trip <\/b>. Than we fix the dataset in cell below","90fca87d":"No problem for <i>service_class<\/i> feature\n<br>","f21595b3":"<b> 1.2. Data Wrangling<\/b>\n<p> \nIn this section we will modify data into another format with the intent of making it more appropriate and valuable\n<\/p>","4b4be134":"We can see for the <i>account_id<\/i> feature, several customer is booking multiple times in a year. This is very <b>important<\/b> fact that we may check further more in the feature engineering section","4bdb9382":"<br>\n<h4> <b>\n4. Modeling\n<\/b> <\/h4>\n\n<p> \nBuild ML Model\n<\/p>","ccc0f4b6":"<b>2.8. Airlines Name<\/b>","f8e315ed":"<b>2.6. Transaction with promo<\/b>","051f1cac":"<b>2.7. Number of Seats<\/b>","d921da8d":"<p>Next, we're going to fix the visited_city and log_transaction feature as it is seems to be a list but still formatted as string so we need to change it a little bit so we can take advantage of it<\/p>","686b5385":"<b> 0.2. Data import <\/b>\n<p> \nIn the cell below we import all the data given by the competition for performing the prediction\n<\/p>","4905e45a":"we can see from the plot above, this classification problem is purely imbalance","8434c013":"<p>Firstly on the cell below, <i>order_id<\/i> features will be removed as those are computer generated value for customer data encryption. So it is not really going to help us reach our goal<\/p>","e800848a":"<b>K-Nearest Neighbor<\/b>","c1eb606c":"<br>\n<b>Takeaways from this EDA section<\/b>:\n\n1. This is an imbalance classification problem as the number of actual positive cross selling is far more smaller than the actual negative cross selling\n2. From the correlation heatmap we can see that our available feature is not correlated with each other. This is a good sign showing that there are no redundant data\n3. We haven't dig more information for visited_city and log_transaction feature as both of it still in form of python list. These feature need to be fixed so we can get some information from it","9e10e7b7":"<p>First, as we already know from the EDA section, <i>visited_city<\/i> feature contain list(array) type data which not really helpful. We have to extract the data so the feature may be more useful. We're extracting how many city does the customer already visit. We see later if this approach can help us to predict to cross selling <\/p>","c8f5899b":"<h1> <b>Customer Hotel Cross Seling Prediction on Flights Booking <\/b><\/h1>\n<p>By <b>qw3rty<\/b> <\/p>\n<ul>\n<li>DP. Nala Krisnanda<\/li>\n<li>M. Alwi Sukra<\/li>\n<li>Bimo Arief Wicaksana<\/li>\n<\/ul>\n","dd612005":"<b>Random Forest Classifier<\/b>","6492082a":"As we can see there is an extra gender <b>(None)<\/b> in the flight dataset which mean some action need to be taken to fix this. This data oddity can be handled in many ways. In this case , we will leave it alone as the third gender.","9dc6d292":"We can see from the chart, where Gender 2 which is female is showing greater is_cross_sell ratio than male or None","971c86c1":"<b> Kernel Outline <\/b>\n 0. Library and data import \n 1. Data preprocessing\n 2. Exploratory Data Analysis\n 3. Feature Engineering \n 4. Modelling \n 5. Validation","59edcef2":"<b>XGBoost<\/b>","92200a54":"<b>4.2.1. Base Classifier<\/b>\n<p>layer 0 also called as base classifier usually using different kind of algorithm. In this case, for the base classifier we use:<\/p>\n\n1. Distance (neighbor) base algorithm (KNN)\n2. Ensamble algorithm (RF)\n3. Linear algorithm (LR)","7b713983":"<p>Next, <i>log_transaction<\/i> is same as <i>visited_city<\/i> feature which contain list(array) type data. The difference is the data inside the list is in form of continous data. So in the cell below we will extract the descriptive statistics which is number of transaction, min, max, average, and total as a new dataframe column<\/p>","8c5d2b1a":"<b>Logistic Regression<\/b>","d32665c9":"So here is the final dataframe after this section finished","31b20f65":"<br>\n<h4> <b>\n1. Data Preprocessing\n<\/b> <\/h4>","b46dc383":"<br>\n<h4> <b>\n3. Feature Engineering\n<\/b> <\/h4>\n\n<p> \n\n<\/p>","e1eb3195":"<b>Random Forest Classifier<\/b>","4d8f2668":"<b>3.2 Data Scaling<\/b>\n<p>The scaling method used here is the minmax scaling. We scale only the continous data not the categorical data<\/p>","acf16bbf":"<b>2.1. is_cross_selling<\/b>\n<p> Firstly, we want to see how is the target variable doing in the dataset<\/p>","a6b14e12":"<br>\n<h4> <b>\n2. Exploratory Data Analysis (EDA)\n<\/b> <\/h4>\n\n<p> \nTaking insights from the data that may helping to build accurate model\n<\/p>","e881247f":"No Problem for <i>airlines_name<\/i> feature","e58d581f":"<b>4.2.2. Meta Classifier<\/b>\n<p>We will build the meta classifier. In this chance, we try to use neural network and XGBoost Classifier<\/p>","c858f218":"No problem for <i>price<\/i> feature","2675b0c9":"Now we look inside every features, checking is it contain any anomaly that might make our predictor model biased","dbab090a":"We can see that there are slight difference between customer who did cross selling and not. Customer who did cross selling tend to be joined to the platform for longer period of time\n<br>","8a3dcbfa":"<b> 0.1. Library import <\/b>\n<p> \nThe cell below is for importing useful library for <i>exploratory data analysis<\/i> (EDA), data visualization, and of course data modeling\n<\/p>","d170cc47":"<b>Random Forest Classifier<\/b>","0facfeb3":"Seems there is no problem for the <i>member_duration_days<\/i> feature\n<br>","fdea29ab":"Now the <i> trip<\/i> feature is good to go\n<br>","fd932f1c":"<b>K-Nearest Neighbor<\/b>","bf36cefa":"<b>4.1. Single Model<\/b>\n<p>We try to see the performance of difference ML algorithms in this data<\/p>","2c995bb8":"<p>As we already know from the data wrangling section, some customers are doing bookings more than once in a year. We want to see if previous cross selling will trigger another cross selling on their next flight booking. In the cells below, we try to make use of feature account_id to find the is_cross_sell probability of each account <\/p>","a8b5bec1":"<b>2.5. Price<\/b>","b9e7d547":"<b>Neural Network<\/b>","124bfd55":"<b> 5.1 Base Classifier<\/b>","a1e23a3b":"<b> 1.1. Data Quick Look <\/b>\n<p> \nIn this section we will take a look at the basic information of the data we've been import in the previous section\n<\/p>","e1bc9cf0":"<b>2.4. Trip<\/b>","50cc766b":"<p>Luckily, on every datasets we don't face any NaN value and all the features seems pretty consistent on their data type.<\/p>\n\n<p>From the quick look above here some informations that we got:\n<ul>\n<li> <mark>flight.csv<\/mark> contain the customer flight booking data <\/li>\n<li> <mark>hotel.csv<\/mark> contain the hotel detail information <\/li>\n<li> <mark>test.csv<\/mark> is data for testing purpose which has same features with <mark>flight.csv<\/mark> but without the <i>hotel_id<\/i> feature<\/li>\n<\/ul>\n<\/p>\n\n <p>We jump to the next section!<\/p>","dad6c954":"The number of trip is far more greater than the roundtrip. the cross selling activities seems to be linearly correlated to the number of the type of trip","e97bea93":"<br>\n<h4> <b>\n0. Library and Data Import\n<\/b> <\/h4>","90e42232":"<b>K-Nearest Neighbor<\/b>","349852a5":"From the attempt did above, we can see the logistic regression somehow outperform the other algorithm. It reach 0.86 F1 score. The score followed by the XGBoost and the random forest classifier\n<p>But the difference between all algorithm is not significance<\/p>","6e449028":"<br>\n<h4> <b>\n5. Validation\n<\/b> <\/h4>\n\n<p> \nPreparing model for validation in Kaggle platform\n<\/p>","da7568d1":"No Problem for <i>no_of_seats<\/i> feature","32ad913a":"we creating a new feature in df_flight named <i>is_cross_sell<\/i> to check wheter the <i>hotel_id<\/i> is None or not"}}