{"cell_type":{"d19d30ff":"code","f3fbee35":"code","b939d852":"code","863e0f50":"code","f3b3a1ae":"code","97cc1ed3":"code","5e51f19d":"code","9d42b819":"code","7b2ae847":"code","ed7766c1":"code","b2e5be1f":"code","f5b73815":"code","2270c961":"code","115e2160":"code","01e56073":"code","14fa266f":"code","d557f7c6":"code","2383787f":"code","c462cfe7":"code","58449081":"code","bf6bc062":"code","6467df9d":"code","4d4ede70":"code","161fc03a":"code","8389180f":"code","dc5e5ea0":"code","bb464f05":"code","a24787db":"code","a5fd2a11":"code","1425065d":"code","e1af3a2d":"code","ace8538c":"code","119cf3cf":"code","21fb7823":"code","7fd11ec5":"code","edf04e18":"code","36b5460d":"code","ee0203d0":"code","f1cc1b45":"code","78668862":"code","41c068e4":"code","291d14d5":"code","f3bf8fc7":"code","1bc34497":"code","727ebe05":"code","b6b1df76":"code","f537eeae":"code","c54feaeb":"markdown","ce254991":"markdown","ee642079":"markdown","325fd3f0":"markdown","4e02cb61":"markdown","81b303a9":"markdown","6b778c9d":"markdown","e18d6609":"markdown","5efe79f2":"markdown","2bc800fe":"markdown"},"source":{"d19d30ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3fbee35":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder","b939d852":"train_df=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df=pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission_df=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","863e0f50":"train_df.head()","f3b3a1ae":"train_df.info()\n#age, cabin, and embarked have null values","97cc1ed3":"test_df.info()\n# Age cabin and fare have null values","5e51f19d":"#merging data for easy processing\nall_df=pd.concat([train_df, test_df], ignore_index= True)\nall_df.tail()","9d42b819":"all_df.info()","7b2ae847":"all_df.shape","ed7766c1":"#Filling missing values\nall_df","b2e5be1f":"# Ticket no. 3701 has null fare in test table. He embarked from station S in pclass 3. Since fare prices are different for different pclass. For now i will fill this value with median of fare values for pclass 3\nMed_val=all_df[all_df['Pclass']==3]['Fare'].median()\nall_df.loc[all_df['Ticket']=='3701',['Fare']]=Med_val","f5b73815":"#Embarked has two missing values, both are females who are travelling on same ticket no. 113572 in pclass 1, sharing same cabin.\n# It means both of them embarked from same port. Out of those in pclass 1, 141 embarked from C and 177 embarked from S. Only 3 passengers embarked from Q.\n# So i'll just fill both values with S, it will not effect the outcome much though\nall_df.loc[all_df['Ticket']=='113572',['Embarked']]='S'","2270c961":"all_df.head()","115e2160":"all_df['Age'].isnull().sum()","01e56073":"#Age has 263 null values. Salutation for missing values is Dr, Master, Miss, Mr, Mrs, Ms\n#Lets first spilt the name variable in three columns each for salutation, family name and first name\nall_df['Family_Name']=all_df['Name'].apply(lambda x: x.split(',')[0])\nall_df['Name']=all_df['Name'].apply(lambda x: x.split(',')[1])","14fa266f":"all_df['Salut']=all_df['Name'].apply(lambda x: x.split('.')[0])\nall_df['Name']=all_df['Name'].apply(lambda x: x.split('.')[1])","d557f7c6":"all_df['Salut']=all_df['Salut'].apply(lambda x: x.strip())","2383787f":"#Age can be an important factor for determining outcome so for filling null values. I'll categorise them on the basis of age and sex\nfor i in range(len(all_df)):\n    if ((all_df.iloc[i,all_df.columns.get_loc('Sex')]=='male') and (all_df.iloc[i,all_df.columns.get_loc('Age')]<=14)):\n        all_df.iloc[i, all_df.columns.get_loc('Salut')] = 1\n    if ((all_df.iloc[i,all_df.columns.get_loc('Sex')]=='female') and (all_df.iloc[i,all_df.columns.get_loc('Age')]<=14)):\n        all_df.iloc[i, all_df.columns.get_loc('Salut')] = 2\n    if ((all_df.iloc[i,all_df.columns.get_loc('Sex')]=='male') and (all_df.iloc[i,all_df.columns.get_loc('Age')]>14)):\n        all_df.iloc[i, all_df.columns.get_loc('Salut')] = 3\n    if ((all_df.iloc[i,all_df.columns.get_loc('Sex')]=='female') and (all_df.iloc[i,all_df.columns.get_loc('Age')]>14)):\n        all_df.iloc[i, all_df.columns.get_loc('Salut')] = 4","c462cfe7":"all_df[all_df['Age'].isnull()]","58449081":"all_df['Age']=all_df.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\nall_df.head()","bf6bc062":"# Master=all_df[all_df['Salut']==1]['Age'].mean()\n# Miss=all_df[all_df['Salut']==2]['Age'].mean()\n# Mr=all_df[all_df['Salut']==3]['Age'].mean()\n# Mrs=all_df[all_df['Salut']==4]['Age'].mean()\n# print(Master)\n# print(Miss)\n# print(Mr)\n# print(Mrs)\n# # There is not much difference in values of male and female, so for simplicity i'll fill 6 and 32.\n\n","6467df9d":"\nfor i in range(len(all_df)):\n#     if all_df.iloc[i,all_df.columns.get_loc('Salut')] in ['Dr','Capt','Col','Don','Dona','Jonkheer','Lady','Major','Miss','Mlle','Mme','Mr','Mrs','Ms','Rev','Sir','the Countess']:\n#         all_df.iloc[i,all_df.columns.get_loc('Age')]=32\n#     if all_df.iloc[i,all_df.columns.get_loc('Salut')] in ['Master']:\n#         all_df.iloc[i,all_df.columns.get_loc('Age')]=6\n    if all_df.iloc[i,all_df.columns.get_loc('Salut')] in ['Dr','Capt','Col','Don','Jonkheer','Major','Mr','Rev','Sir']:\n        all_df.iloc[i,all_df.columns.get_loc('Salut')]=3\n    if all_df.iloc[i,all_df.columns.get_loc('Salut')] in ['Dona','Lady','Miss','Mlle','Mme','Mrs','Ms']:\n        all_df.iloc[i,all_df.columns.get_loc('Salut')]=4\n    if all_df.iloc[i,all_df.columns.get_loc('Salut')] in ['Master']:\n        all_df.iloc[i,all_df.columns.get_loc('Salut')]=1\n","4d4ede70":"all_df.info()\n#only cabin is left with null values but it has more than 1000 null values so we will remove this column\n# we well also remove the columns which are not helpful in predicting the survival such as ticket no., Name, Family_name.","161fc03a":"# all_df['Cabin']=all_df['Cabin'].fillna('U')\n# all_df['Cabin']","8389180f":"# deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n# all_df['Deck'] = all_df['Cabin'].astype(str).str[0] \n# all_df['Deck'] = all_df['Deck'].str.capitalize()\n# all_df['Deck'] = all_df['Deck'].map(deck)\n# all_df['Deck'] = all_df['Deck'].fillna(0)\n# all_df['Deck'] = all_df['Deck'].astype(int) \n# all_df['Deck']","dc5e5ea0":"all_df.drop(columns=['Cabin','Name','Family_Name','Ticket','Pclass'], inplace=True)\nall_df.info()","bb464f05":"#now we will map the data to make it categorical\ncleanup_maps = {\"Sex\":     {\"male\": 1, \"female\": 2},\n                \"Embarked\": {\"S\": 1, \"Q\": 2, \"C\": 3}}\nall_df.replace(cleanup_maps, inplace=True)\nall_df.head()","a24787db":"corr=all_df.corr()\nsns.heatmap(corr)","a5fd2a11":"# all_df['family_size']=all_df['SibSp']+all_df['Parch']+1\n# all_df.head()","1425065d":"# all_df['is_alone']=0\n# for i in range(len(all_df)):\n#     if all_df.iloc[i,all_df.columns.get_loc('family_size')]>1:\n#         all_df.iloc[i,all_df.columns.get_loc('is_alone')]=1\n        \n# all_df.head()","e1af3a2d":"# all_df.drop(columns=['SibSp','Parch'], inplace=True)\n# all_df.head()","ace8538c":"all_df['Age'] = pd.qcut(all_df['Age'], 10)\nall_df.head()","119cf3cf":"all_df['Fare'] = pd.qcut(all_df['Fare'], 13)\nall_df.head()","21fb7823":"non_numeric_features=['Age', 'Fare']\nfor feature in non_numeric_features:        \n        all_df[feature] = LabelEncoder().fit_transform(all_df[feature])\n        \nall_df.head()","7fd11ec5":"train_df=all_df.iloc[0:891,:]\ntest_df=all_df.iloc[891: ,:]\nprint(train_df.shape)\nprint(test_df.shape)","edf04e18":"train_passengerId=train_df['PassengerId']\ntest_passengerId=test_df['PassengerId']\ntarget=train_df['Survived']\ntrain_df.drop(columns=['PassengerId','Survived'],inplace=True)\ntest_df.drop(columns=['PassengerId','Survived'],inplace=True)","36b5460d":"test_df.info()","ee0203d0":"decision_tree=DecisionTreeClassifier(random_state=0, max_depth=6)\naccuracy_decision_tree = cross_val_score(decision_tree, train_df, target, scoring='accuracy', cv = 10)\nprint(\"The accuracy from decision tree model is {0}\".format(accuracy_decision_tree.mean()))\n\n# # logistic_regression= LogisticRegression()\n# # accuracy_logistic_regression = cross_val_score(logistic_regression, train_df, target, scoring='accuracy', cv = 10)\n# # print(\"The accuracy from logistic regression model is {0}\".format(accuracy_logistic_regression.mean()))\n\n# # clf_svm= svm.SVC()\n# # accuracy_svm = cross_val_score(clf_svm, train_df, target, scoring='accuracy', cv = 10)\n# # print(\"The accuracy from svm model is {0}\".format(accuracy_svm.mean()))\n\n# # clf_rf = RandomForestClassifier(max_depth=6, random_state=0)\n# # accuracy_random_forest = cross_val_score(clf_rf, train_df, target, scoring='accuracy', cv = 10)\n# # print(\"The accuracy from random forest model is {0}\".format(accuracy_random_forest.mean()))\n\n# clf_gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n# accuracy_gradiet_boost = cross_val_score(clf_gbc, train_df, target, scoring='accuracy', cv = 10)\n# print(\"The accuracy from gradient boost model is {0}\".format(accuracy_gradiet_boost.mean()))\n\n# clf_bagging = BaggingClassifier()\n# accuracy_bagging = cross_val_score(clf_bagging, train_df, target, scoring='accuracy', cv = 10)\n# print(\"The accuracy from bagging model is {0}\".format(accuracy_bagging.mean()))\n\n# clf_gnb = GaussianNB()\n# accuracy_gaussian = cross_val_score(clf_gnb, train_df, target, scoring='accuracy', cv = 10)\n# print(\"The accuracy from Gaussian model is {0}\".format(accuracy_gaussian.mean()))\n\n# clf_xgb = XGBClassifier()\n# accuracy_xgb = cross_val_score(clf_xgb, train_df, target, scoring='accuracy', cv = 10)\n# print(\"The accuracy from xgb model is {0}\".format(accuracy_xgb.mean()))\n\n","f1cc1b45":"decision_tree = DecisionTreeClassifier(random_state=0, max_depth=6)\ndecision_tree = decision_tree.fit(train_df, target)\npred=decision_tree.predict(test_df)\n","78668862":"# print('BEFORE DT Parameters: ', decision_tree.get_params())","41c068e4":"# param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n#               #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n#               'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n#               #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n#               #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n#               #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n#               'random_state': [0] #seed or control random number generator: https:\/\/www.quora.com\/What-is-seed-in-random-number-generation\n#              }","291d14d5":"# tune_model = model_selection.GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = 10)\n# tune_model.fit(train_df, target)","f3bf8fc7":"# print('AFTER DT Parameters: ', tune_model.best_params_)","1bc34497":"# pred=tune_model.predict(test_df)","727ebe05":"\n# accuracy_tune_model = cross_val_score(tune_model, train_df, target, scoring='accuracy', cv = 10)\n# print(\"The accuracy from tune_model is {0}\".format(accuracy_tune_model.mean()))","b6b1df76":"data = {'PassengerId':test_passengerId, 'Survived':pred}\nsubmission=pd.DataFrame(data)\nsubmission['Survived']=submission['Survived'].apply(lambda x: int(x))\nsubmission.head(25)","f537eeae":"submission.to_csv('submission.csv', index=False)","c54feaeb":"<a id='4'><\/a>\n# Cleanin the data","ce254991":"<a id='Prediction'><\/a>\n# Prediction","ee642079":"### Some Interesting Findings:\n* Pclass feature has no impact on final outcome on leaderboard so i removed it later\n* Filling Cabin feature missing values, creating family size and is_alone features or hypertuning reduces the accuracy","325fd3f0":"<a id='1'><\/a>\n# IMPORTING DATA AND LIBRARIES","4e02cb61":"# Index\n<a href = '#1'> Important data and libraries<\/a><br>\n<a href = '#2'> Data Overview<\/a><br>\n<a href = '#3'> Filling missing values<\/a><br>\n<a href = '#4'> Cleaning the data<\/a><br>\n<a href = '#5'> Feature Generation<\/a><br>\n<a href = '#Prediction'>Prediction<\/a><br>\n<a href = '#6'> HyperParameter tuning<\/a><br>\n<a href = '#7'> Submission<\/a><br>\n\n","81b303a9":"<a id='7'><\/a>\n# Submission","6b778c9d":"<a id='3'><\/a>\n# Filling missing values","e18d6609":"<a id='2'><\/a>\n# DATA OVERVIEW","5efe79f2":"<a id='5'><\/a>\n# Feature Generation","2bc800fe":"<a id='6'><\/a>\n# Hyper Parameter Tuning"}}