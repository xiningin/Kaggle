{"cell_type":{"8443f97e":"code","381783d8":"code","f8fec496":"code","61848aa6":"code","8f88a015":"code","a562a578":"code","6e477bf4":"code","a802d2b5":"code","118545dd":"code","f70f83fb":"code","c2391a92":"code","b563b693":"code","cf65f5ed":"code","35d79f31":"code","11e63f04":"code","76a78208":"code","e02559c2":"code","25552c08":"code","4a38bd23":"code","b6e3032a":"code","27437c1f":"code","3233dbe4":"code","0bb6797c":"code","c2c6ed7e":"code","a6eb52f9":"code","ced058c3":"code","7946623e":"code","69946abb":"code","bf3118fe":"code","d4b024b1":"code","394a8c0b":"code","3ee82504":"code","09d2d744":"code","ab904f50":"code","b5bb77d6":"code","79986a56":"code","c62c446a":"code","2a520a60":"code","cdfe78b0":"code","a1b10663":"code","069c2d4e":"code","380693f4":"code","e954032d":"code","9f9bb38c":"code","a5a9d4fc":"code","679f86fd":"code","3fc0dd57":"code","ff3bf21b":"code","8a302684":"code","e4b41344":"code","4b6762a9":"code","ed1e3308":"code","7e54408e":"code","cd8cfe07":"code","028ee6ce":"code","5985b297":"code","f96acfb1":"code","8b8e595c":"code","3d229dec":"code","a71acd3f":"code","28076215":"code","9219556d":"code","10bc9f37":"code","c2d0708e":"code","e0530d89":"code","95f4d3d5":"code","7ce82b4f":"code","73a200de":"code","4bf57611":"code","e124ab5d":"code","5ee8432e":"code","e6469211":"code","09441d0f":"code","ba959c64":"code","db6ad0f9":"code","76fc98ea":"code","12801fdb":"markdown","fc50ec12":"markdown","984448fe":"markdown","f5ad58f5":"markdown","0206ca71":"markdown","8a5836c6":"markdown","c5601c31":"markdown","a317067c":"markdown"},"source":{"8443f97e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","381783d8":"# importing the libraries\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\", 100)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f8fec496":"#Reading Required Data\n\ndata_train = pd.read_csv(\"\/kaggle\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv\",encoding=\"utf-8\", delimiter=',')\ndata_train.head()","61848aa6":"data_train.head()","8f88a015":"## Removing customerID and Row Number\n\ndata_train2 = data_train.drop(['RowNumber','CustomerId','Surname'],axis=1)\ndata_train2.shape","a562a578":"data_train2.info()","6e477bf4":"data_train2.head()","a802d2b5":"data_train2.describe()","118545dd":"## Lets findout the distribution of Churn: Yes and Nos First\n\n# Good Practice: Always check if data set is balance or imbalance.\nsns.set_style('whitegrid')\nsns.countplot(x='Exited',data=data_train2,palette='RdBu_r')","f70f83fb":"## There are some outliers in Balance and Estimated Salry. 1. Balance (min to 25%) is 0; while Estimated salary is \n## 11.58 rupees as minimum","c2391a92":"## Lets see this through dist plot\n\nplt.hist(data_train2.EstimatedSalary, bins=5\n         , rwidth=0.8)\nplt.xlabel('EstimatedSalary')\nplt.ylabel('Count')\nplt.show()","b563b693":"plt.hist(data_train2.EstimatedSalary, bins=7\n         , rwidth=0.8)\nplt.xlabel('EstimatedSalary')\nplt.ylabel('Count')\nplt.yscale('log')\nplt.show()","cf65f5ed":"# We will use percentiles technique to detect and remove outliers\n\n\nMaxThershold = data_train2['EstimatedSalary'].quantile(0.999)\nMaxThershold","35d79f31":"MinThershold = data_train2['EstimatedSalary'].quantile(0.015)\nMinThershold","11e63f04":"data_train3 = data_train2[(data_train2.EstimatedSalary < MaxThershold) & (data_train2.EstimatedSalary > MinThershold)]\ndata_train3.head()","76a78208":"data_train3.shape","e02559c2":"10000-9540","25552c08":"## Detecting outlier in Balance\n\n## Lets see this through dist plot\n\nplt.hist(data_train3.Balance, bins=5\n         , rwidth=0.8)\nplt.xlabel('Balance')\nplt.ylabel('Count')\nplt.show()","4a38bd23":"plt.hist(data_train3.Balance, bins=7\n         , rwidth=0.8)\nplt.xlabel('Balance')\nplt.ylabel('Count')\nplt.yscale('log')\nplt.show()","b6e3032a":"# We will use percentiles technique to detect and remove outliers\n\n\nMaxThershold = data_train3.Balance.quantile(0.999)\nMaxThershold","27437c1f":"MinThershold = data_train3.Balance.quantile(0.370)\nMinThershold","3233dbe4":"data_train4 = data_train3[(data_train3.Balance < MaxThershold) & (data_train3.Balance > MinThershold)]\ndata_train4.head()","0bb6797c":"data_train4.shape","c2c6ed7e":"data_train4.dtypes","a6eb52f9":"#Lets seperate all as numerical\/Categorical\n# Findout Missing Value %age\n\nstatistics_of_data = []\nfor col in data_train4.columns:\n  statistics_of_data.append((col,\n                             data_train4[col].isnull().sum()*100\/data_train4.shape[0],\n                             data_train4[col].dtype\n                             ))\nstats_df = pd.DataFrame(statistics_of_data, columns=['Feature', 'missing_val', 'type'])","ced058c3":"stats_df.sort_values('missing_val', ascending=False)","7946623e":"##No missing Values Found","69946abb":"## Seperate out Numerical\/Int Variables.\nnumerical_features = [feature for feature in data_train4.columns if data_train4[feature].dtypes != 'O' ]\nprint(len(numerical_features))\ndata_train4[numerical_features].head()","bf3118fe":"## Numerical variables are usually of 2 type\n## 1. Continous variable and Discrete Variables\n\ndiscrete_feature = [feature for feature in numerical_features if len(data_train4[feature].unique())<25]\nprint(len(discrete_feature))\ndiscrete_feature","d4b024b1":"## Continous Features\n\n\ncontinous_feature = [feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continous_feature)))\ndata_train4[continous_feature].head()","394a8c0b":"def print_unique_col_values(df):\n    i=1\n    for column in df:\n        str = \"{i}. {a} column have {b} unique values\"\n        print(str.format(i=i,a=column,b=df[column].unique()))\n        i=i+1","3ee82504":"print_unique_col_values(data_train4[discrete_feature])","09d2d744":"# Now lets create visuals for comparison of continous\/discrete and target vairable\n##Exited: 0 -> No,1 ->Yes\ntenure_churn_no = data_train4[data_train4.Exited==0].Tenure\ntenure_churn_yes = data_train4[data_train4.Exited==1].Tenure\n\nplt.xlabel(\"tenure\")\nplt.ylabel(\"Number Of Customers\")\nplt.title(\"Customer Churn Prediction Visualiztion\")\n\nplt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['red','green'],label=['Churn=1','Churn=0'])\nplt.legend()","ab904f50":"# Now lets create visuals for comparison of continous\/discrete and target vairable\n##Exited: 0 -> No,1 ->Yes\nAge_churn_no = data_train4[data_train4.Exited==0].Age\nAge_churn_yes = data_train4[data_train4.Exited==1].Age\n\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number Of Customers\")\nplt.title(\"Customer Churn Prediction Visualiztion\")\n\nplt.hist([Age_churn_yes, Age_churn_no], rwidth=0.95, color=['red','green'],label=['Churn=1','Churn=0'])\nplt.legend()","b5bb77d6":"##Exited: 0 -> No,1 ->Yes\nBalance_churn_no = data_train4[data_train4.Exited==0].Balance\nBalance_churn_yes = data_train4[data_train4.Exited==1].Balance\n\nplt.xlabel(\"Balance\")\nplt.ylabel(\"Number Of Customers\")\nplt.title(\"Customer Churn Prediction Visualiztion\")\n\nplt.hist([Balance_churn_yes, Balance_churn_no], rwidth=0.95, color=['red','green'],label=['Churn=1','Churn=0'])\nplt.legend()","79986a56":"#EstimatedSalary\n##Exited: 0 -> No,1 ->Yes\nEstimatedSalary_churn_no = data_train4[data_train4.Exited==0].EstimatedSalary\nEstimatedSalary_churn_yes = data_train4[data_train4.Exited==1].EstimatedSalary\n\nplt.xlabel(\"EstimatedSalary\")\nplt.ylabel(\"Number Of Customers\")\nplt.title(\"Customer Churn Prediction Visualiztion\")\n\nplt.hist([EstimatedSalary_churn_yes, EstimatedSalary_churn_no], rwidth=0.95, color=['red','green'],label=['Churn=1','Churn=0'])\nplt.legend()","c62c446a":"#Now lets seperate our categorical features so that we can add other transformation uopn them\ncategoricalVariable = [feature for feature in data_train4.columns if data_train3[feature].dtype == 'O' ]\nlen(categoricalVariable)","2a520a60":"data_train4[categoricalVariable].head()","cdfe78b0":"#Male->1, Female 0\ndata_train4['Gender'].replace({'Female':1,'Male':0},inplace=True)","a1b10663":"#Lets apply One hot encoding for categorical column Geography\n\ndata_train5 = pd.get_dummies(data=data_train4, columns=['Geography'])\ndata_train5.columns","069c2d4e":"data_train5.shape","380693f4":"data_train5.sample(5)","e954032d":"#Data Scaling of continous data\n\ncontinous_feature","9f9bb38c":"### Data Scaling -->>Continous data only\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_train5[continous_feature] = scaler.fit_transform(data_train5[continous_feature])","a5a9d4fc":"ij=1\nfor col in data_train5:\n    str = \"{ij}. {a} column have {b} unique values\"\n    print(str.format(ij = ij,a=col,b=data_train5[col].unique()))\n    ij=ij+1","679f86fd":"### Train Test Split:::\n\nX = data_train5.drop('Exited',axis='columns')\ny = data_train5['Exited']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)","3fc0dd57":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","ff3bf21b":"len(X_train.columns)","8a302684":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n","e4b41344":"from tensorflow_addons import losses\nfrom sklearn.metrics import confusion_matrix , classification_report","4b6762a9":"def ArtificalNeuralNets(X_train, y_train, X_test, y_test, loss, weights):\n    model_ChurnPred = keras.Sequential([\n        keras.layers.Dense(12, input_shape=(12,), activation='relu'),\n        keras.layers.Dense(15, activation='relu'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model_ChurnPred.compile(optimizer='adam',\n                            loss='binary_crossentropy',\n                            metrics=['accuracy'])\n    if weights == -1:\n        model_ChurnPred.fit(X_train, y_train, epochs=100)\n    else:\n        model_ChurnPred.fit(X_train, y_train, epochs=100, class_weight = weights)\n        \n    print(model_ChurnPred.evaluate(X_test, y_test))\n    y_preds = model_ChurnPred.predict(X_test)\n    y_preds = np.round(y_preds)\n    print(\"Classification Report: \\n\", classification_report(y_test, y_preds))\n    return y_preds\n    \n    ","ed1e3308":"y_preds = ArtificalNeuralNets(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)","7e54408e":"# Class count\ncount_class_0, count_class_1 = data_train5.Exited.value_counts()\n\nprint(count_class_1)\ncount_class_0","cd8cfe07":"# Divide by class\ndf_class_0 = data_train5[data_train5['Exited'] == 0]\ndf_class_1 = data_train5[data_train5['Exited'] == 1]","028ee6ce":"# Undersample 0-class and concat the DataFrames of both class\ndf_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)","5985b297":"print('Random under-sampling:')\nprint(df_test_under.Exited.value_counts())","f96acfb1":"X = df_test_under.drop('Exited',axis='columns')\ny = df_test_under['Exited']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)","8b8e595c":"# Number of classes in training Data\ny_train.value_counts()\n","3d229dec":"y_preds = ArtificalNeuralNets(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)","a71acd3f":"df_class_1 = data_train5[data_train5['Exited'] == 1]","28076215":"# Oversample 1-class and concat the DataFrames of both classes\ndf_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_class_1_over","9219556d":"df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.Exited.value_counts())","10bc9f37":"X = df_test_over.drop('Exited',axis='columns')\ny = df_test_over['Exited']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)","c2d0708e":"loss = keras.losses.BinaryCrossentropy()\nweights = -1\ny_preds = ArtificalNeuralNets(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)","e0530d89":"X = data_train5.drop('Exited',axis='columns')\ny = data_train5['Exited']","95f4d3d5":"from imblearn.over_sampling import SMOTE\n","7ce82b4f":"import sklearn\nsklearn.__version__","73a200de":"import imblearn\nimblearn.__version__","4bf57611":"from imblearn.over_sampling import SMOTE","e124ab5d":"smote = SMOTE(sampling_strategy='minority')","5ee8432e":"#X_sm, y_sm = smote.fit_sample(X, y)","e6469211":"#y_sm.value_counts()","09441d0f":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=15, stratify=y_sm)","ba959c64":"# Number of classes in training Data\n#y_train.value_counts()","db6ad0f9":"#y_preds = ArtificalNeuralNets(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)","76fc98ea":"####################### ********** THE END ********************** ############################################","12801fdb":"## Observations:::\nWe are having 0.90 and 0.58 F1 scores. For value 0 it is good that we have 0.90 score but for 1 we have pretty low \nrate that is 0.58; lets start with \"under sampling\" and visualize our results\n","fc50ec12":"### Method3: SMOTE","984448fe":"\n\n### Now we are Builiding a Deep Learning Model (ANN) On keras\/Tensorflow\n\n### Now we are Builiding a Deep Learning Model (ANN) On keras\/Tensorflow With Hyperparameter Optimization through keras tuner\n","f5ad58f5":"## Bank Turnover Dataset\n#### Can you predict if bank customers will turnover next cycle ?","0206ca71":"#### Observations: \n\n- This is imbalanced dataset, There are different ways to cater it, But for the sake of learning deep learning, We can ignore it for now. ","8a5836c6":"Check classification report above. f1-score for minority class 1 improved from 0.57 to 0.73. Score for class 0 reduced to 0.77 from 0.85 but that's ok. We have more generalized classifier which classifies both classes with similar prediction score","c5601c31":"### Method2: Oversampling","a317067c":"Check classification report above. f1-score for minority class 1 improved from 0.58 to 0.73. Score for class 0 reduced to 0.76 from 0.90 but that's ok. We have more generalized classifier which classifies both classes with similar prediction score"}}