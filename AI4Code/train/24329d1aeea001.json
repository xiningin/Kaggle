{"cell_type":{"1a1fcc9c":"code","deda3bb3":"code","dbddc49d":"code","c7472498":"code","65e2af95":"code","ca27b521":"code","61a6b5f5":"code","c30459ab":"code","0d3f7f75":"code","63298808":"code","11734a1e":"code","262320ea":"code","88186e49":"code","73ff7533":"code","38930ee8":"code","03304fc2":"code","842ebbba":"code","d5fad328":"code","73a8b18f":"code","e9c85f17":"code","b598904c":"code","5be2a7b1":"code","056c7596":"code","7ae011ae":"code","b583f0be":"code","2fec2c0b":"markdown","b51eb700":"markdown","a5f44f81":"markdown","ece8fbb5":"markdown","f48c6874":"markdown","b302e9e3":"markdown","72e5e37d":"markdown","39f001f2":"markdown","9e12dde8":"markdown","4a44f7ae":"markdown","adbac996":"markdown","50bf5495":"markdown","0cb1d594":"markdown","a1f1c601":"markdown","d7c8ee5a":"markdown","7a498771":"markdown","8f3b8511":"markdown","711d478a":"markdown","f00dd3eb":"markdown"},"source":{"1a1fcc9c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))","deda3bb3":"train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","dbddc49d":"train.head()","c7472498":"test.head()","65e2af95":"#  Saving id columns\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain_ID.head()","ca27b521":"#Id as always useless for predictions\ntrain.drop('Id',axis=1,inplace=True)\ntest.drop('Id',axis=1,inplace=True)\ntrain.head()","61a6b5f5":"#Here is  what i am going to do is check whichs columns haves the most correlation and then\n# I am going to visualize the outliers, and if i think they are a little too far fetched they outy\ntrain.describe().T","c30459ab":"#So i dont have too much work, i am going to you know check which columns have the most corr to sales price. SO i dont have to actually see each and every single columns\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)\n\n#From what i could see, GarageArea GarageCars, GrLiveArea, OverAllQual. Are the most correlated so i am going to do a little bit of outliers removal on those specific columns","0d3f7f75":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","63298808":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice'] < 300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","11734a1e":"fig, ax = plt.subplots()\nax.scatter(x = train['GarageArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\nplt.show()","262320ea":"train = train.drop(train[(train['GarageArea']>1300) & (train['SalePrice'] < 300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(x = train['GarageArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\nplt.show()","88186e49":"fig, ax = plt.subplots()\nax.scatter(x = train['GarageCars'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageCars', fontsize=13)\nplt.show()","73ff7533":"fig, ax = plt.subplots()\nax.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.show()","38930ee8":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data=pd.concat((train,test)).reset_index(drop=True)\nall_data.drop(['SalePrice'],axis=1,inplace=True)\nprint(f'all_data size = {(all_data.shape)}')\n#Just getting all data value","03304fc2":"#NOW WE ARE GOING TO CHECK THE WHOLE NULL VALUES ON EACH EVERY SINGLE COLUMN\nall_data_na=(all_data.isnull().sum()\/ len(all_data)) * 100\n#This is droping the columns that dont have null values or nan values, also limiting how much we get bcs u know just having one isnt as muchs as an issue\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","842ebbba":"#Now pretty graph available\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","d5fad328":"#Well pool has a big percentage, but mostly because houses dont have pool it is not very common so yeah it is fine we fill it up with None\nall_data['PoolQC']=all_data['PoolQC'].fillna('None')\n#The second values is MiscFeature, which also make sense because some houses dont have those\nall_data['MiscFeature']=all_data['MiscFeature'].fillna('None')\n#Some goes with alleys\nall_data['Alley']=all_data['Alley'].fillna('None')\n#Same with fence\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n#Same with fireplace\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n#Now this one is a little special, bcs it doesnt have any description so lets just substitute by the median hmm kay\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n#Lambda is just the dirty function to da that\n#Houses sometimes dont have garages\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n#And since u dont have a garage u dont have a car inside ur garage\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n#Sometimes houses dont have basements, and since they dont have basements their area related values is going to be 0.\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n#Same thing here sometimes houses dont have basements\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n#Sometimes they dont have mansory veeners\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n#And those veeners have an areas of 0 since you know they dont exist\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n#Basically filling ms with the most comoon value, the mode function helps us with that\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n#Useless bcs it only contains the same value over and over again\nall_data = all_data.drop(['Utilities'], axis=1)\n#Na is the most typical value on this one\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n#Same thing as we did in MsZoning\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n#Same here\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n#Filling up with none                                                       \nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","73a8b18f":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","e9c85f17":"all_data['MoSold'].dtype","b598904c":"#Making sure categorical data is as a string\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# This serves the purpose of making kinda of a like a groupy by of the categories making sure they are \n#defined as a category in  the column they are in, \n#and them changing then into numerical values\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n    \nprint(f'Shape:{all_data.shape}')","5be2a7b1":"#Adding that envolves the total basemente area which seens to be lacking\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","056c7596":"# This is selecting all datasets, that are numerical\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","7ae011ae":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n\n    all_data[feat] = boxcox1p(all_data[feat], lam)","b583f0be":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","2fec2c0b":"## Step 2 - Outliers removal","b51eb700":"### GarageArea","a5f44f81":"## Step 3- Na handling","ece8fbb5":"* This one is a toughie to judge solely so i am not going to take anything off\n\n###  OverallQual","f48c6874":"**Making a good column for fitting later**","b302e9e3":"## Step 1- Trying to process the data also,  defining training data and so on.\n","72e5e37d":"### GrLiveArea","39f001f2":"**To handle that it there is something called box cox which makes sure that the skewing of the feature, be diminished at least a little**\n* I aint gonna lie i stole that from someones elses \n* Note to self watch theoretical class","9e12dde8":"*  GrLiveArea, GarageArea GarageCars, OverAllQual","4a44f7ae":"**Handling dtypes and doing label encoding**","adbac996":"* This one makes a lot of sense actually so y3eah no removal here","50bf5495":"### Inputing missing values to the data\n","0cb1d594":"### Outliers removal process","a1f1c601":"* A little bit of checking","d7c8ee5a":"**Finally we get the dummies**","7a498771":"###  GarageCars\n","8f3b8511":"## Introduction to this project\n\n* Heyo name is yappy you can call me Francisco or whatever\n* Just a data handling notebook for myself where i keep a few ways of making your data more precise, without losing the touch with reality\n* Cant believe this actually got upvotes, love u guys.\n* Thank you for attention signed Yappy yo","711d478a":"**It seens this dataset has a skewed classes issue**","f00dd3eb":"## Step4- Checking dtypes"}}