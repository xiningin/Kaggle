{"cell_type":{"968cc61f":"code","3dbaa3dd":"code","81d3cc49":"code","6ead0fdc":"code","95522446":"code","5b63ae5e":"code","08fdb0e4":"code","ac1e6181":"code","f2da4f05":"code","b4bf59d2":"code","1ea8536f":"code","83679e69":"code","ee8bb99d":"code","7b4992ad":"code","06d75e8c":"code","35f305a0":"code","5b0a4ac6":"code","4eeca529":"code","cdfe9e0a":"code","96b1d2b7":"code","1d230c95":"code","c392bfaf":"code","0cea5115":"code","01778339":"code","2155a8e5":"code","1a7ec17f":"markdown","f1c77dc9":"markdown","bdfb2087":"markdown","d2b95ec4":"markdown","3c9bedaf":"markdown","6b0c74b1":"markdown","069b8d4a":"markdown","dbfef44c":"markdown","969fb3f8":"markdown","083e4945":"markdown","a750616a":"markdown"},"source":{"968cc61f":"\"\"\"Importing libraries and stuff\"\"\"\n# Author: Fernando-Lopez-Velasco\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import Imputer\nimport category_encoders as ce\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier","3dbaa3dd":"\"\"\"Loading files as a pandas dataframe\"\"\"\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","81d3cc49":"\"\"\"Splitting data\"\"\"\n\nY = train['Survived'].copy() # We extract the target vector\nXtrain = train.drop(['Survived','PassengerId', 'Name'], axis=1) # Drop some columns which are not useful\nXtest = test.drop(['PassengerId','Name'], axis=1)","6ead0fdc":"Xtest.head()","95522446":"\"\"\"First we split data in categorical and no categorical values\"\"\"\n\ntrain_category = Xtrain.select_dtypes(include=['object']).copy()\ntest_category = Xtest.select_dtypes(include=['object']).copy()\ntrain_float = Xtrain.select_dtypes(exclude=['object']).copy()\ntest_float = Xtest.select_dtypes(exclude=['object']).copy()","5b63ae5e":"imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimp.fit(train_float)","08fdb0e4":"Xtrain_float= imp.transform(train_float)\nXtest_float = imp.transform(test_float)","ac1e6181":"\"\"\"Declaring the object of BackwardDifferenceEncoder and fitting\"\"\"\n\nencoder = ce.BackwardDifferenceEncoder(cols=['Sex', 'Ticket','Cabin','Embarked'])\nencoder.fit(train_category)","f2da4f05":"\"\"\"Transforming data\"\"\"\n\nXtrain_category = encoder.transform(train_category)\nXtest_category = encoder.transform(test_category)","b4bf59d2":"\"\"\"We need to drop some columns, this is because the transformation have generated extra columns\"\"\"\n\ntrain_cols = Xtrain_category.columns\ntest_cols = Xtest_category.columns","1ea8536f":"flag = 0\ncols_to_drop = []\nfor i in train_cols:\n    for j in test_cols:\n        if i == j:\n            flag = 1\n    if flag == 0:\n        cols_to_drop.append(i)\n    else:\n        flag = 0","83679e69":"\"\"\"Dropping columns\"\"\"\n\nXtrain_category = Xtrain_category.drop(cols_to_drop, axis=1)","ee8bb99d":"print(Xtrain_category.shape)\nprint(Xtest_category.shape)","7b4992ad":"\"\"\"Intialize the object imputer\"\"\"\n\nimp.fit(Xtrain_category)","06d75e8c":"\"\"\"Transforming data\"\"\"\n\nXtrain_category = pd.DataFrame(imp.transform(Xtrain_category), columns = Xtrain_category.columns)\nXtest_category = pd.DataFrame(imp.transform(Xtest_category), columns = Xtest_category.columns)","35f305a0":"\"\"\"Initializing and fiting\"\"\"\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nmin_max_scaler.fit(Xtrain_float)","5b0a4ac6":"\"\"\"Scaling\"\"\"\n\nXtrain_float = pd.DataFrame(min_max_scaler.transform(Xtrain_float), columns = train_float.columns)\nXtest_float = pd.DataFrame(min_max_scaler.transform(Xtest_float), columns = test_float.columns)","4eeca529":"Xtest_float.head()","cdfe9e0a":"Xtest_category.head()","96b1d2b7":"\"\"\"As we have two kinds of datasets which are categorical and not categorical data, we need to concatenate both\"\"\"\n\nXtrain = pd.concat([Xtrain_float,Xtrain_category], axis=1)\nXtest = pd.concat([Xtest_float,Xtest_category], axis=1)","1d230c95":"\"\"\"Initializing the XBoost classifier\"\"\"\n\nmodel = xgb.XGBClassifier(n_estimators=2000, max_depth=5, learning_rate=0.1)","c392bfaf":"\"\"\"Fitting\"\"\"\n\nmodel.fit(Xtrain, Y)","0cea5115":"\"\"\"Making a prediction\"\"\"\n\nYpred = model.predict(Xtest)","01778339":"\"\"\"Saving data\"\"\"\nYpred = pd.DataFrame({'Survived':Ypred})\nprediction = pd.concat([test['PassengerId'], Ypred], axis=1)\nprediction.to_csv('predictions_xboost.csv', sep=',', index=False)","2155a8e5":"prediction.head()","1a7ec17f":"# 3. Handling null values\n\nIn this section we will to solve the problem with missing or null values","f1c77dc9":"# 6. XBoost Classifier\n\nTo solve this classification problem we will to apply the XBoost classifier.","bdfb2087":"## 3.1 Null values in not categorical data\n\nFirst we need to implement some method to adress this problem, in this case we will use the Imputer method provided by scikit-learn.","d2b95ec4":"# 1. Introduction\n\nThe idea of this work is to show in a simple and easy way, a solution for the classification problem in the Titanic disaster. In this notebook we use the XGBoost classifier.","3c9bedaf":"# Machine Learning for Disaster","6b0c74b1":"# 4. Scaling data\n\nTo scale data, we will use the function MinMaxScaler provided by scikit-learn.","069b8d4a":"## 3.3 Null values in categorical data\n\nTo solve the problem with null values in categorical data we will implement the Imputer function provided by scikit-learn.","dbfef44c":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","969fb3f8":"# 5. Concatenating categorical and numerical data","083e4945":"## 3.2 Transformation of categorical data into numerical format\nNow that we have solved the problem of null values in categorical data, we need to transform continuos values into discrete format. To do this we will use the technique \"Backward Difference Encoder\".","a750616a":"# 2. Let's Start"}}