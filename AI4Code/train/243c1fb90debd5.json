{"cell_type":{"4cfb4655":"code","c0271b8c":"code","8b07640f":"code","4a732b46":"code","19f37329":"code","e1e12c7c":"code","b4d03823":"code","5e62276f":"code","0fc1f4de":"code","2c4b5990":"code","2ac9f150":"code","98f9c6db":"code","165d564e":"code","f9b6dcd4":"code","b46062cb":"code","ac3f4f45":"code","b693d4f5":"code","b3f0fa71":"code","4be417ab":"code","b31b64a5":"code","30299a77":"code","320928c1":"code","8a750441":"code","65e4e6bb":"code","ce583650":"markdown","231beed9":"markdown","e7e7b490":"markdown","b7b06c67":"markdown","609180c9":"markdown","65902c9c":"markdown","6d31b861":"markdown","e7163d20":"markdown","6cc53ac1":"markdown"},"source":{"4cfb4655":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0271b8c":"!pip install pyforest   # it automatically imports few important libraries including numpy, pandas and matplotlib\nimport pyforest\nimport matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\n# matplotlib.rcParams['figure.figsize'] = (13.0, 7.0)","8b07640f":"df=pd.read_csv(\"..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","4a732b46":"df.head()","19f37329":"plt.figure(figsize=(13,5))\nsns.heatmap(df.isna())","e1e12c7c":"plt.figure(figsize=(15,5))\ndf.Attrition.value_counts().plot(kind='bar')","b4d03823":"fig, axes =plt.subplots(3,3, figsize=(15,15))\naxes = axes.flatten()\nobject_bol = df.dtypes == 'object'\nfor ax, catplot in zip(axes, df.dtypes[object_bol].index):\n    sns.countplot(hue=catplot,x=df.Attrition ,data=df, ax=ax,)\n\nplt.tight_layout()  \nplt.show()","5e62276f":"fig, axes = plt.subplots(5,5, figsize=(15,15))\naxes = axes.flatten()\nobject_bol = df.dtypes == 'int'\nfor ax, catplot in zip(axes, df.dtypes[object_bol].index):\n    sns.boxplot(y=df[catplot],x= df[\"Attrition\"],ax=ax)\n\nplt.tight_layout()  \nplt.show()","0fc1f4de":"df1=df[['Attrition','Age','BusinessTravel', 'DistanceFromHome', 'MonthlyIncome','PercentSalaryHike','TotalWorkingYears']]\nsns.pairplot(df1,hue='Attrition',)","2c4b5990":"plt.figure(figsize=(15,5))\nsns.distplot(df.Age,bins=20, kde=False)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Counts\")\nplt.title(\"Age Counts\")\nplt.show()","2ac9f150":"s = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","98f9c6db":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_data = df.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_data[col] = label_encoder.fit_transform(df[col])","165d564e":"label_data.head()","f9b6dcd4":"rf = RandomForestClassifier(class_weight=\"balanced\", n_estimators=500) \nrf.fit(label_data.drop(['Attrition'],axis=1), hr.Attrition)\nimportances = rf.feature_importances_\nnames = label_data.columns\nimportances, names = zip(*sorted(zip(importances, names)))\n\n# Lets plot this\nplt.figure(figsize=(15,8))\nplt.barh(range(len(names)), importances, align = 'center')\nplt.yticks(range(len(names)), names)\nplt.xlabel('Importance of features')\nplt.ylabel('Features')\nplt.title('Importance of each feature')\nplt.show()","b46062cb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(label_data.drop(['Attrition'],axis=1), label_data.Attrition,\n                                                    test_size=0.25, random_state=42)","ac3f4f45":"def kfold_and_confusion_matrix(model, t_model):\n    kfold = KFold(n_splits=5)\n    model_kfold = model\n    results_kfold = model_selection.cross_val_score(model_kfold, X_train, y_train,  cv=kfold)\n    print(\"K Fold Accuracy: %.2f%%\" % (results_kfold.mean()*100.0)) \n    \n    plt.figure(figsize=(15,5))\n    y_pred=t_model.predict(X_test)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')","b693d4f5":"from sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\nprint(\"Normal Accuracy:\",model.score(X_test, y_test))\nkfold_and_confusion_matrix(LogisticRegression(), model)","b3f0fa71":"# SMOTE for imbalanced data\noversampler=SMOTE(random_state=0)\nX, y = oversampler.fit_sample(label_data.drop(['Attrition'],axis=1), label_data.Attrition,)   #label_data[list(names)[14:], using less features are reducing the accuracy\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.25)","4be417ab":"model=LogisticRegression()\nmodel.fit(X_train, y_train, )\nprint(\"Normal Accuracy: %.2f%%\" % ( model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(LogisticRegression(), model)\n","b31b64a5":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nlogregpipe = Pipeline([('scale', StandardScaler()),\n                   ('logreg',LogisticRegression())])\n\n# Gridsearch to determine the value of C\nparam_grid = {'logreg__C':np.arange(0.01,100,10)}\nlogreg_cv = GridSearchCV(logregpipe,param_grid,cv=5,return_train_score=True)\nlogreg_cv.fit(X_train,y_train)\nprint(logreg_cv.best_params_)\n\n\nbestlogreg = logreg_cv.best_estimator_\nbestlogreg.fit(X_train,y_train)\nbestlogreg.coef_ = bestlogreg.named_steps['logreg'].coef_\nprint(\"Normal Accuracy: %.2f%%\" % (bestlogreg.score(X_train,y_train)*100))\n\nkfold_and_confusion_matrix(logreg_cv.best_estimator_, bestlogreg)\n","30299a77":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(class_weight=\"balanced\", n_estimators=500)\nmodel.fit(X_train, y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(RandomForestClassifier(), model)","320928c1":"from sklearn.ensemble import  GradientBoostingClassifier\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(GradientBoostingClassifier(), model)","8a750441":"import xgboost as xgb\nfrom sklearn import metrics\n# Parameter Tuning\nmodel = xgb.XGBClassifier()\nparam_dist = {\"max_depth\": [10,30,50],\n              \"min_child_weight\" : [1,3,6],\n              \"n_estimators\": [200],\n              \"learning_rate\": [0.05, 0.1,0.16],}\ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nmodel=grid_search.best_estimator_\n# model = xgb.XGBClassifier(max_depth=50, min_child_weight=1,  n_estimators=200,\\\n#                           n_jobs=-1 , verbose=1,learning_rate=0.16)\nmodel.fit(X_train,y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(grid_search.best_estimator_, model)","65e4e6bb":"import lightgbm as lgb\nfrom sklearn import metrics\n\n\nlg = lgb.LGBMClassifier(silent=False)\nparam_dist = {\"max_depth\": [25,50, 75],\n              \"learning_rate\" : [0.01,0.05,0.1],\n              \"num_leaves\": [300,900,1200],\n              \"n_estimators\": [200]\n             }\ngrid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\ngrid_search.fit(X_train,y_train)\ngrid_search.best_estimator_\nmodel= grid_search.best_estimator_\nmodel.fit(X_train,y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(grid_search.best_estimator_, model)","ce583650":"**Lets look at our dataset**","231beed9":"**Logistic Regression with SMOTEd data and default parameters**","e7e7b490":"**Creating a function for KFold Cross-Validation and Confusion Matrix(using heatmap)**","b7b06c67":"**Build a forest to predict attrition and compute the feature importances**","609180c9":"**Spliting the data for training and testing**","65902c9c":"**We will use Oversampling technique SMOTE to match the number of target in both classes**","6d31b861":"**Logistic Regression with SMOTEd data and GridSearch**","e7163d20":"**Now we will see if data is balanced or not?**","6cc53ac1":"**Trying Logistic Regression with default parameters**"}}