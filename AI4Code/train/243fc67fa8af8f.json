{"cell_type":{"dae895ed":"code","1a3275dd":"code","a8bde46c":"code","f89a251e":"code","c3df3157":"code","804da19e":"code","47776480":"code","5a566f0c":"code","055ec6d0":"code","868d8d1a":"code","9f0eee9d":"code","885b8830":"code","37da4872":"code","f4e03b95":"code","fe3e7e80":"code","a6902613":"code","71fdc7ed":"code","4635cc77":"code","0726c1e2":"code","97c7b324":"code","6f80da6f":"code","b4a1379a":"code","c50d6bef":"code","72a2d776":"code","48e6b70e":"code","ad2337ab":"code","77464e18":"code","2b757ae3":"code","931cd30c":"code","cecd1a60":"code","c1cae8df":"code","689c2660":"code","95a2f6e8":"code","612d4773":"code","90c37026":"code","7b310b4c":"code","a0c6e087":"code","a95bb416":"code","9bf0a81e":"code","03b355ea":"code","d3a8baf5":"code","483a8308":"code","f19342ff":"code","bca587f7":"code","ceb1863a":"code","d0e49fb2":"code","b132f658":"code","33de11ee":"code","9212a6f4":"code","6b3c1446":"code","12ebe80c":"code","d0fecc9e":"code","22a8b2e0":"code","562850ac":"code","7890400d":"code","04ff8559":"code","12316d14":"code","d731564c":"code","e63d6dea":"code","9a57664a":"code","0b0b0e3e":"code","66021d73":"code","e3c2254e":"code","319a55ba":"code","d8d33331":"code","a7f23858":"code","3507b609":"code","2a2efccd":"code","4a1ac30f":"code","3e8a5229":"code","a6c9093b":"code","69385784":"code","ad7c8d44":"code","6b10df34":"code","e354dc7e":"code","fe891b36":"code","a9e7ea72":"code","5ab1d234":"markdown","b0f1bb54":"markdown","bf30b313":"markdown","d044753d":"markdown","f41cf8f7":"markdown","72461a46":"markdown","1b17da13":"markdown","24f64e76":"markdown","ba471aaf":"markdown","85799b3b":"markdown","fb70c7d4":"markdown","b0294bab":"markdown","bc364f15":"markdown","aa9667b7":"markdown","841f2737":"markdown","39996fe8":"markdown","10cf0e7a":"markdown","831ab47a":"markdown","c9f820c9":"markdown","53415c03":"markdown","206b040d":"markdown","70388280":"markdown","f70c8657":"markdown","ec6f0005":"markdown","684be69b":"markdown","c88bd522":"markdown","f1ca7906":"markdown","be589be6":"markdown","03f452f5":"markdown","f9468a91":"markdown","577bca34":"markdown","094403ee":"markdown","2d5cbfc5":"markdown","999beeae":"markdown","8f897232":"markdown","e8dc63ec":"markdown","183c3aea":"markdown","79c69307":"markdown","e9741095":"markdown","f02588df":"markdown"},"source":{"dae895ed":"# Data and plotting imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\n\n# Statistical Libraries\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nfrom scipy import stats\n\n\n# Plotly imports\nfrom plotly import tools\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# Maintain the Ids for submission\ntrain_id = train['Id']\ntest_id = test['Id']","1a3275dd":"train['SalePrice'].describe()","a8bde46c":"# It seems we have nulls so we will use the imputer strategy later on.\nMissing = pd.concat([train.isnull().sum(), test.isnull().sum()], axis=1, keys=['train', 'test'])\nMissing[Missing.sum(axis=1) > 0]","f89a251e":"# We have several columns that contains null values we should replace them with the median or mean those null values.\ntrain.info()","c3df3157":"train.describe()","804da19e":"corr = train.corr()\nplt.figure(figsize=(14,8))\nplt.title('Overall Correlation of House Prices', fontsize=18)\nsns.heatmap(corr,annot=False,cmap='RdYlBu',linewidths=0.2,annot_kws={'size':20})\nplt.show()","47776480":"# Create the categories\noutsidesurr_df = train[['Id', 'MSZoning', 'LotFrontage', 'LotArea', 'Neighborhood', 'Condition1', 'Condition2', 'PavedDrive', \n                    'Street', 'Alley', 'LandContour', 'LandSlope', 'LotConfig', 'MoSold', 'YrSold', 'SaleType', 'LotShape', \n                     'SaleCondition', 'SalePrice']]\n\nbuilding_df = train[['Id', 'MSSubClass', 'BldgType', 'HouseStyle', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', \n                    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'Foundation', 'Functional', \n                    'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'MoSold', 'YrSold', 'SaleType',\n                    'SaleCondition', 'SalePrice']]\n\nutilities_df = train[['Id', 'Utilities', 'Heating', 'CentralAir', 'Electrical', 'Fireplaces', 'PoolArea', 'MiscVal', 'MoSold',\n                     'YrSold', 'SaleType', 'SaleCondition', 'SalePrice']]\n\nratings_df = train[['Id', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                   'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature',\n                   'GarageCond', 'GarageQual', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice']]\n\nrooms_df = train[['Id', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BsmtFinSF1', 'BsmtFinSF2',\n                 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF','TotRmsAbvGrd', \n                 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MoSold', 'YrSold', 'SaleType',\n                 'SaleCondition', 'SalePrice']]\n\n\n\n\n# Set Id as index of the dataframe.\noutsidesurr_df = outsidesurr_df.set_index('Id')\nbuilding_df = building_df.set_index('Id')\nutilities_df = utilities_df.set_index('Id')\nratings_df = ratings_df.set_index('Id')\nrooms_df = rooms_df.set_index('Id')\n\n# Move SalePrice to the first column (Our Label)\nsp0 = outsidesurr_df['SalePrice']\noutsidesurr_df.drop(labels=['SalePrice'], axis=1, inplace=True)\noutsidesurr_df.insert(0, 'SalePrice', sp0)\n\nsp1 = building_df['SalePrice']\nbuilding_df.drop(labels=['SalePrice'], axis=1, inplace=True)\nbuilding_df.insert(0, 'SalePrice', sp1)\n\nsp2 = utilities_df['SalePrice']\nutilities_df.drop(labels=['SalePrice'], axis=1, inplace=True)\nutilities_df.insert(0, 'SalePrice', sp2)\n\nsp3 = ratings_df['SalePrice']\nratings_df.drop(labels=['SalePrice'], axis=1, inplace=True)\nratings_df.insert(0, 'SalePrice', sp3)\n\nsp4 = rooms_df['SalePrice']\nrooms_df.drop(labels=['SalePrice'], axis=1, inplace=True)\nrooms_df.insert(0, 'SalePrice', sp4)","5a566f0c":"import seaborn as sns\nsns.set_style('white')\n\nf, axes = plt.subplots(ncols=4, figsize=(16,4))\n\n# Lot Area: In Square Feet\nsns.distplot(train['LotArea'], kde=False, color=\"#DF3A01\", ax=axes[0]).set_title(\"Distribution of LotArea\")\naxes[0].set_ylabel(\"Square Ft\")\naxes[0].set_xlabel(\"Amount of Houses\")\n\n# MoSold: Year of the Month sold\nsns.distplot(train['MoSold'], kde=False, color=\"#045FB4\", ax=axes[1]).set_title(\"Monthly Sales Distribution\")\naxes[1].set_ylabel(\"Amount of Houses Sold\")\naxes[1].set_xlabel(\"Month of the Year\")\n\n# House Value\nsns.distplot(train['SalePrice'], kde=False, color=\"#088A4B\", ax=axes[2]).set_title(\"Monthly Sales Distribution\")\naxes[2].set_ylabel(\"Number of Houses \")\naxes[2].set_xlabel(\"Price of the House\")\n\n# YrSold: Year the house was sold.\nsns.distplot(train['YrSold'], kde=False, color=\"#FE2E64\", ax=axes[3]).set_title(\"Year Sold\")\naxes[3].set_ylabel(\"Number of Houses \")\naxes[3].set_xlabel(\"Year Sold\")\n\nplt.show()","055ec6d0":"# Maybe we can try this with plotly.\nplt.figure(figsize=(12,8))\nsns.distplot(train['SalePrice'], color='r')\nplt.title('Distribution of Sales Price', fontsize=18)\n\nplt.show()","868d8d1a":"# People tend to move during the summer\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(12,8))\nsns.countplot(y=\"MoSold\", hue=\"YrSold\", data=train)\nplt.show()","9f0eee9d":"plt.figure(figsize=(12,8))\nsns.boxplot(x='YrSold', y='SalePrice', data=train)\nplt.xlabel('Year Sold', fontsize=14)\nplt.ylabel('Price sold', fontsize=14)\nplt.title('Houses Sold per Year', fontsize=16)","885b8830":"plt.figure(figsize=(14,8))\nplt.style.use('seaborn-white')\nsns.stripplot(x='YrSold', y='YearBuilt', data=train, jitter=True, palette=\"Set2\", linewidth=1)\nplt.title('Economic Activity Analysis', fontsize=18)\nplt.xlabel('Year the house was sold', fontsize=14)\nplt.ylabel('Year the house was built', rotation=90, fontsize=14)\nplt.show()","37da4872":"outsidesurr_df.describe()","f4e03b95":"outsidesurr_df.columns","fe3e7e80":"# Lot Area and Lot Frontage influenced hugely on the price. \n# However, YrSold does not have that much of a negative correlation with SalePrice as we previously thought.\n# Meaning the state of IOWA was not affected as other states.\nplt.style.use('seaborn-white')\ncorr = outsidesurr_df.corr()\n\nsns.heatmap(corr,annot=True,cmap='YlOrRd',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(14,10)\nplt.title(\"Outside Surroundings Correlation\", fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","a6902613":"# We already know which neighborhoods were the most sold but which neighborhoods gave the most revenue. \n# This might indicate higher demand toward certain neighborhoods.\nplt.style.use('seaborn-white')\nzoning_value = train.groupby(by=['MSZoning'], as_index=False)['SalePrice'].sum()\nzoning = zoning_value['MSZoning'].values.tolist()\n\n\n# Let's create a pie chart.\nlabels = ['C: Commercial', 'FV: Floating Village Res.', 'RH: Res. High Density', 'RL: Res. Low Density', \n          'RM: Res. Medium Density']\ntotal_sales = zoning_value['SalePrice'].values.tolist()\nexplode = (0, 0, 0, 0.1, 0)\n\nfig, ax1 = plt.subplots(figsize=(12,8))\ntexts = ax1.pie(total_sales, explode=explode, autopct='%.1f%%', shadow=True, startangle=90, pctdistance=0.8,\n       radius=0.5)\n\n\nax1.axis('equal')\nplt.title('Sales Groupby Zones', fontsize=16)\nplt.tight_layout()\nplt.legend(labels, loc='best')\nplt.show()","71fdc7ed":"plt.style.use('seaborn-white')\nSalesbyZone = train.groupby(['YrSold','MSZoning']).SalePrice.count()\nSalesbyZone.unstack().plot(kind='bar',stacked=True, colormap= 'gnuplot',  \n                           grid=False,  figsize=(12,8))\nplt.title('Building Sales (2006 - 2010) by Zoning', fontsize=18)\nplt.ylabel('Sale Price', fontsize=14)\nplt.xlabel('Sales per Year', fontsize=14)\nplt.show()","4635cc77":"fig, ax = plt.subplots(figsize=(12,8))\nsns.countplot(x=\"Neighborhood\", data=train, palette=\"Set2\")\nax.set_title(\"Types of Neighborhoods\", fontsize=20)\nax.set_xlabel(\"Neighborhoods\", fontsize=16)\nax.set_ylabel(\"Number of Houses Sold\", fontsize=16)\nax.set_xticklabels(labels=train['Neighborhood'] ,rotation=90)\nplt.show()","0726c1e2":"# Sawyer and SawyerW tend to be the most expensive neighberhoods. Nevertheless, what makes them the most expensive\n# Is it the LotArea or LotFrontage? Let's find out!\nfig, ax = plt.subplots(figsize=(12,8))\nax = sns.boxplot(x=\"Neighborhood\", y=\"SalePrice\", data=train)\nax.set_title(\"Range Value of the Neighborhoods\", fontsize=18)\nax.set_ylabel('Price Sold', fontsize=16)\nax.set_xlabel('Neighborhood', fontsize=16)\nax.set_xticklabels(labels=train['Neighborhood'] , rotation=90)\nplt.show()","97c7b324":"sns.jointplot(x='GrLivArea',y='SalePrice',data=train,\n              kind='hex', cmap= 'CMRmap', size=8, color='#F84403')\n\nplt.show()","6f80da6f":"sns.jointplot(x='GarageArea',y='SalePrice',data=train,\n              kind='hex', cmap= 'CMRmap', size=8, color='#F84403')\n\nplt.show()","b4a1379a":"sns.jointplot(x='TotalBsmtSF',y='SalePrice',data=train,\n              kind='hex', cmap= 'CMRmap', size=8, color='#F84403')\n\nplt.show()","c50d6bef":"plt.figure(figsize=(16,6))\nplt.subplot(121)\nax = sns.regplot(x=\"LotFrontage\", y=\"SalePrice\", data=train)\nax.set_title(\"Lot Frontage vs Sale Price\", fontsize=16)\n\nplt.subplot(122)\nax1 = sns.regplot(x=\"LotArea\", y=\"SalePrice\", data=train, color='#FE642E')\nax1.set_title(\"Lot Area vs Sale Price\", fontsize=16)\n\nplt.show()","72a2d776":"building_df.head()","48e6b70e":"corr = building_df.corr()\n\ng = sns.heatmap(corr,annot=True,cmap='coolwarm',linewidths=0.2,annot_kws={'size':20})\ng.set_xticklabels(g.get_xticklabels(), rotation = 90, fontsize = 8)\nfig=plt.gcf()\nfig.set_size_inches(14,10)\nplt.title(\"Building Characteristics Correlation\", fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","ad2337ab":"# To understand better our data I will create a category column for SalePrice.\ntrain['Price_Range'] = np.nan\nlst = [train]\n\n# Create a categorical variable for SalePrice\n# I am doing this for further visualizations.\nfor column in lst:\n    column.loc[column['SalePrice'] < 150000, 'Price_Range'] = 'Low'\n    column.loc[(column['SalePrice'] >= 150000) & (column['SalePrice'] <= 300000), 'Price_Range'] = 'Medium'\n    column.loc[column['SalePrice'] > 300000, 'Price_Range'] = 'High'\n    \ntrain.head()","77464e18":"import matplotlib.pyplot as plt\npalette = [\"#9b59b6\", \"#BDBDBD\", \"#FF8000\"]\nsns.lmplot('GarageYrBlt', 'GarageArea', data=train, hue='Price_Range', fit_reg=False, size=7, palette=palette,\n          markers=[\"o\", \"s\", \"^\"])\nplt.title('Garage by Price Range', fontsize=18)\nplt.annotate('High Price \\nCategory Garages \\n are not that old', xy=(1997, 1100), xytext=(1950, 1200), \n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.show()","2b757ae3":"plt.style.use('seaborn-white')\ntypes_foundations = train.groupby(['Price_Range', 'PavedDrive']).size()\ntypes_foundations.unstack().plot(kind='bar', stacked=True, colormap='Set1', figsize=(13,11), grid=False)\nplt.ylabel('Number of Streets', fontsize=16)\nplt.xlabel('Price Category', fontsize=16)\nplt.xticks(rotation=45, fontsize=12)\nplt.title('Condition of the Street by Price Category', fontsize=18)\nplt.show()","931cd30c":"# We can see that CentralAir impacts until some extent the price of the house.\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(14,10))\nplt.suptitle('Relationship between Saleprice \\n and Categorical Utilities', fontsize=18)\nsns.pointplot(x='CentralAir', y='SalePrice', hue='Price_Range', data=train, ax=ax1)\nsns.pointplot(x='Heating', y='SalePrice', hue='Price_Range', data=train, ax=ax2)\nsns.pointplot(x='Fireplaces', y='SalePrice', hue='Price_Range', data=train, ax=ax3)\nsns.pointplot(x='Electrical', y='SalePrice', hue='Price_Range', data=train, ax=ax4)\n\nplt.legend(loc='best')\nplt.show()","cecd1a60":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\n\nfig, ax = plt.subplots(figsize=(14,8))\npalette = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\", \"#FF8000\", \"#AEB404\", \"#FE2EF7\", \"#64FE2E\"]\n\nsns.swarmplot(x=\"OverallQual\", y=\"SalePrice\", data=train, ax=ax, palette=palette, linewidth=1)\nplt.title('Correlation between OverallQual and SalePrice', fontsize=18)\nplt.ylabel('Sale Price', fontsize=14)\nplt.show()","c1cae8df":"with sns.plotting_context(\"notebook\",font_scale=2.8):\n    g = sns.pairplot(train, vars=[\"OverallCond\", \"OverallQual\", \"YearRemodAdd\", \"SalePrice\"],\n                hue=\"Price_Range\", palette=\"Dark2\", size=6)\n\n\ng.set(xticklabels=[]);\n\nplt.show()","689c2660":"# What type of material is considered to have a positive effect on the quality of the house?\n# Let's start with the roof material\n\nwith sns.plotting_context(\"notebook\",font_scale=1):\n    g = sns.factorplot(x=\"SalePrice\", y=\"RoofStyle\", hue=\"Price_Range\",\n                   col=\"YrSold\", data=train, kind=\"box\", size=5, aspect=.75, sharex=False, col_wrap=3, orient=\"h\",\n                      palette='Set1');\n    for ax in g.axes.flatten(): \n        for tick in ax.get_xticklabels(): \n            tick.set(rotation=20)\n\nplt.show()","95a2f6e8":"with sns.plotting_context(\"notebook\",font_scale=1):\n    g = sns.factorplot(x=\"MasVnrType\", y=\"SalePrice\", hue=\"Price_Range\",\n                   col=\"YrSold\", data=train, kind=\"bar\", size=5, aspect=.75, sharex=False, col_wrap=3,\n                      palette=\"YlOrRd\");\n    \nplt.show()","612d4773":"plt.style.use('seaborn-white')\ntypes_foundations = train.groupby(['Neighborhood', 'OverallQual']).size()\ntypes_foundations.unstack().plot(kind='bar', stacked=True, colormap='RdYlBu', figsize=(13,11), grid=False)\nplt.ylabel('Overall Price of the House', fontsize=16)\nplt.xlabel('Neighborhood', fontsize=16)\nplt.xticks(rotation=90, fontsize=12)\nplt.title('Overall Quality of the Neighborhoods', fontsize=18)\nplt.show()","90c37026":"# Which houses neighborhoods remodeled the most.\n# price_categories = ['Low', 'Medium', 'High']\n# remod = train['YearRemodAdd'].groupby(train['Price_Range']).mean()\n\nfig, ax = plt.subplots(ncols=2, figsize=(16,4))\nplt.subplot(121)\nsns.pointplot(x=\"Price_Range\",  y=\"YearRemodAdd\", data=train, order=[\"Low\", \"Medium\", \"High\"], color=\"#0099ff\")\nplt.title(\"Average Remodeling by Price Category\", fontsize=16)\nplt.xlabel('Price Category', fontsize=14)\nplt.ylabel('Average Remodeling Year', fontsize=14)\nplt.xticks(rotation=90, fontsize=12)\n\nplt.subplot(122)\nsns.pointplot(x=\"Neighborhood\",  y=\"YearRemodAdd\", data=train, color=\"#ff9933\")\nplt.title(\"Average Remodeling by Neighborhood\", fontsize=16)\nplt.xlabel('Neighborhood', fontsize=14)\nplt.ylabel('')\nplt.xticks(rotation=90, fontsize=12)\nplt.show()","7b310b4c":"numeric_features = train.dtypes[train.dtypes != \"object\"].index\n\n# Top 5 most skewed features\nskewed_features = train[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_features})\nskewness.head(5)","a0c6e087":"from scipy.stats import norm\n\n# norm = a normal continous variable.\n\nlog_style = np.log(train['SalePrice'])  # log of salesprice\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(14,10))\nplt.suptitle('Probability Plots', fontsize=18)\nax1 = sns.distplot(train['SalePrice'], color=\"#FA5858\", ax=ax1, fit=norm)\nax1.set_title(\"Distribution of Sales Price with Positive Skewness\", fontsize=14)\nax2 = sns.distplot(log_style, color=\"#58FA82\",ax=ax2, fit=norm)\nax2.set_title(\"Normal Distibution with Log Transformations\", fontsize=14)\nax3 = stats.probplot(train['SalePrice'], plot=ax3)\nax4 = stats.probplot(log_style, plot=ax4)\n\nplt.show()","a95bb416":"print('Skewness for Normal D.: %f'% train['SalePrice'].skew())\nprint('Skewness for Log D.: %f'% log_style.skew())\nprint('Kurtosis for Normal D.: %f' % train['SalePrice'].kurt())\nprint('Kurtosis for Log D.: %f' % log_style.kurt())","9bf0a81e":"# Most outliers are in the high price category nevertheless, in the year of 2007 saleprice of two houses look extremely high!\n\nfig = plt.figure(figsize=(12,8))\nax = sns.boxplot(x=\"YrSold\", y=\"SalePrice\", hue='Price_Range', data=train)\nplt.title('Detecting outliers', fontsize=16)\nplt.xlabel('Year the House was Sold', fontsize=14)\nplt.ylabel('Price of the house', fontsize=14)\nplt.show()","03b355ea":"corr = train.corr()\ncorr['SalePrice'].sort_values(ascending=False)[:11]","d3a8baf5":"fig, ((ax1, ax2), (ax3, ax4))= plt.subplots(nrows=2, ncols=2, figsize=(14,8))\nvar1 = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var1]], axis=1)\nsns.regplot(x=var1, y='SalePrice', data=data, fit_reg=True, ax=ax1)\n\n\nvar2 = 'GarageArea'\ndata = pd.concat([train['SalePrice'], train[var2]], axis=1)\nsns.regplot(x=var2, y='SalePrice', data=data, fit_reg=True, ax=ax2, marker='s')\n\nvar3 = 'TotalBsmtSF'\ndata = pd.concat([train['SalePrice'], train[var3]], axis=1)\nsns.regplot(x=var3, y='SalePrice', data=data, fit_reg=True, ax=ax3, marker='^')\n\nvar4 = '1stFlrSF'\ndata = pd.concat([train['SalePrice'], train[var4]], axis=1)\nsns.regplot(x=var4, y='SalePrice', data=data, fit_reg=True, ax=ax4, marker='+')\n\nplt.show()","483a8308":"y_train = train['SalePrice'].values\n# We will concatenate but we will split further on.\nrtrain = train.shape[0]\nntest = test.shape[0]\ntrain.drop(['SalePrice', 'Price_Range', 'Id'], axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","f19342ff":"complete_data = pd.concat([train, test])\ncomplete_data.shape","bca587f7":"total_nas = complete_data.isnull().sum().sort_values(ascending=False)\npercent_missing = (complete_data.isnull().sum()\/complete_data.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total_nas, percent_missing], axis=1, keys=['Total_M', 'Percentage'])\n\n\n# missing.head(9) # We have 19 columns with NAs","ceb1863a":"complete_data[\"PoolQC\"] = complete_data[\"PoolQC\"].fillna(\"None\")\ncomplete_data[\"MiscFeature\"] = complete_data[\"MiscFeature\"].fillna(\"None\")\ncomplete_data[\"Alley\"] = complete_data[\"Alley\"].fillna(\"None\")\ncomplete_data[\"Fence\"] = complete_data[\"Fence\"].fillna(\"None\")\ncomplete_data[\"FireplaceQu\"] = complete_data[\"FireplaceQu\"].fillna(\"None\")\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    complete_data[col] = complete_data[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    complete_data[col] = complete_data[col].fillna('None')\ncomplete_data['MSZoning'] = complete_data['MSZoning'].fillna(complete_data['MSZoning'].mode()[0])\ncomplete_data[\"MasVnrType\"] = complete_data[\"MasVnrType\"].fillna(\"None\")\ncomplete_data[\"Functional\"] = complete_data[\"Functional\"].fillna(\"Typ\")\ncomplete_data['Electrical'] = complete_data['Electrical'].fillna(complete_data['Electrical'].mode()[0])\ncomplete_data['KitchenQual'] = complete_data['KitchenQual'].fillna(complete_data['KitchenQual'].mode()[0])\ncomplete_data['Exterior1st'] = complete_data['Exterior1st'].fillna(complete_data['Exterior1st'].mode()[0])\ncomplete_data['Exterior2nd'] = complete_data['Exterior2nd'].fillna(complete_data['Exterior2nd'].mode()[0])\ncomplete_data['SaleType'] = complete_data['SaleType'].fillna(complete_data['SaleType'].mode()[0])\ncomplete_data['MSSubClass'] = complete_data['MSSubClass'].fillna(\"None\")","d0e49fb2":"# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ncomplete_data[\"LotFrontage\"] = complete_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    complete_data[col] = complete_data[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    complete_data[col] = complete_data[col].fillna(0)\n    \ncomplete_data[\"MasVnrArea\"] = complete_data[\"MasVnrArea\"].fillna(0)","b132f658":"# Drop\ncomplete_data = complete_data.drop(['Utilities'], axis=1)","33de11ee":"# Adding total sqfootage feature \ncomplete_data['TotalSF'] = complete_data['TotalBsmtSF'] + complete_data['1stFlrSF'] + complete_data['2ndFlrSF']","9212a6f4":"complete_data.head()","6b3c1446":"# splitting categorical variables with numerical variables for encoding.\ncategorical = complete_data.select_dtypes(['object'])\nnumerical = complete_data.select_dtypes(exclude=['object'])\n\nprint(categorical.shape)\nprint(numerical.shape)","12ebe80c":"\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists\/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","d0fecc9e":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# class combination attribute.\n# First we need to know the index possition of the other cloumns that make the attribute.\nnumerical.columns.get_loc(\"TotalBsmtSF\") # Index Number 37\nnumerical.columns.get_loc(\"1stFlrSF\") # Index NUmber 42\nnumerical.columns.get_loc(\"2ndFlrSF\") # Index NUmber 43\n\nix_total, ix_first, ix_second = 9, 10, 11\n# complete_data['TotalSF'] = complete_data['TotalBsmtSF'] + complete_data['1stFlrSF'] + complete_data['2ndFlrSF']\n\nclass CombineAttributes(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, total_area=True): # No args or kargs\n        self.total_area = total_area\n        \n    def fit(self, X, y=None):\n        return self \n    \n    def transform(self, X, y=None):\n        total_sf = X[:,ix_total] + X[:,ix_first] + X[:,ix_second]\n        if self.total_area:\n            return np.c_[X, total_sf]\n        else: \n            return np.c_[X]\n\nattr_adder = CombineAttributes(total_area=True)\nextra_attribs = attr_adder.transform(complete_data.values)","22a8b2e0":"# Scikit-Learn does not handle dataframes in pipeline so we will create our own class.\n# Reference: Hands-On Machine Learning\nfrom sklearn.base import BaseEstimator, TransformerMixin\n# Create a class to select numerical or cateogrical columns.\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit (self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","562850ac":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nlst_numerical = list(numerical)\n\nnumeric_pipeline = Pipeline([\n    ('selector', DataFrameSelector(lst_numerical)),\n    ('extra attributes', CombineAttributes()),\n    ('std_scaler', StandardScaler()),\n])\n\ncategorical_pipeline = Pipeline([\n    ('selector', DataFrameSelector(['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', \n                                    'Neighborhood', 'Condition1', 'Condition2','BldgType', 'HouseStyle', 'RoofStyle',\n                                    'RoofMatl', 'Exterior1st',  'Exterior2nd','ExterQual','ExterCond', 'Foundation',\n                                    'Heating','HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n                                    'PavedDrive', 'SaleType', 'SaleCondition'])),\n    ('encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n])","7890400d":"# Combine our pipelines!\nfrom sklearn.pipeline import FeatureUnion\n\nmain_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', numeric_pipeline),\n    ('cat_pipeline', categorical_pipeline)\n])\n\ndata_prepared = main_pipeline.fit_transform(complete_data)\ndata_prepared","04ff8559":"features = data_prepared\nlabels = np.log1p(y_train) # Scaling the Saleprice column.\n\ntrain_scaled = features[:rtrain] \ntest_scaled = features[rtrain:]","12316d14":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nfrom sklearn.linear_model import Ridge\nfrom yellowbrick.regressor import PredictionError, ResidualsPlot","d731564c":"# This is data that comes from the training test.\nX_train, X_val, y_train, y_val = train_test_split(train_scaled, labels, test_size=0.25, random_state=42)\n","e63d6dea":"# Our validation set tends to perform better. Less Residuals.\nridge = Ridge()\nvisualizer = ResidualsPlot(ridge, train_color='#045FB4', test_color='r', line_color='#424242')\nvisualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_val, y_val)\ng = visualizer.poof(outpath=\"residual_plot\")","9a57664a":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model, features, labels):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(features) # Shuffle the data.\n    rmse= np.sqrt(-cross_val_score(model, features, labels, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse.mean())","0b0b0e3e":"rid_reg = Ridge()\nrid_reg.fit(X_train, y_train)\ny_pred = rid_reg.predict(X_val)\nrmsle_cv(rid_reg, X_val, y_val)","66021d73":"from sklearn.model_selection import GridSearchCV\n\nparams = {'n_estimators': list(range(50, 200, 25)), 'max_features': ['auto', 'sqrt', 'log2'], \n         'min_samples_leaf': list(range(50, 200, 50))}\n\ngrid_search_cv = GridSearchCV(RandomForestRegressor(random_state=42), params, n_jobs=-1)\ngrid_search_cv.fit(X_train, y_train)","e3c2254e":"grid_search_cv.best_estimator_","319a55ba":"# Show best parameters.\ngrid_search_cv.best_params_","d8d33331":"# You can check the results with this functionof grid search.\n# RandomSearchCV takes just a sample not all possible combinations like GridSearchCV.\n# Mean test score is equivalent to 0.2677\ngrid_search_cv.cv_results_\ndf_results = pd.DataFrame(grid_search_cv.cv_results_)\ndf_results.sort_values(by='mean_test_score', ascending=True).head(2)","a7f23858":"rand_model = grid_search_cv.best_estimator_\n\nrand_model.fit(X_train, y_train)","3507b609":"# Final root mean squared error.\ny_pred = rand_model.predict(X_val)\nrand_mse = mean_squared_error(y_val, y_pred)\nrand_rmse = np.sqrt(rand_mse)\nrand_rmse","2a2efccd":"# It was overfitting a bit.\nscore = rmsle_cv(rand_model, X_val, y_val)\nprint(\"Random Forest score: {:.4f}\\n\".format(score))","4a1ac30f":"# Display scores next to attribute names.\n# Reference Hands-On Machine Learning with Scikit Learn and Tensorflow\nattributes = X_train\nrand_results = rand_model.feature_importances_\ncat_encoder = categorical_pipeline.named_steps[\"encoder\"]\ncat_features = list(cat_encoder.categories_[0])\ntotal_features = lst_numerical + cat_features\nfeature_importance = sorted(zip(rand_results, total_features), reverse=True)\nfeature_arr = np.array(feature_importance)\n# Top 10 features.\nfeature_scores = feature_arr[:,0][:10].astype(float)\nfeature_names = feature_arr[:,1][:10].astype(str)\n\n\nd = {'feature_names': feature_names, 'feature_scores': feature_scores}\nresult_df = pd.DataFrame(data=d)\n\nfig, ax = plt.subplots(figsize=(12,8))\nax = sns.barplot(x='feature_names', y='feature_scores', data=result_df, palette=\"coolwarm\")\nplt.title('RandomForestRegressor Feature Importances', fontsize=16)\nplt.xlabel('Names of the Features', fontsize=14)\nplt.ylabel('Feature Scores', fontsize=14)","3e8a5229":"params = {'learning_rate': [0.05], 'loss': ['huber'], 'max_depth': [2], 'max_features': ['log2'], 'min_samples_leaf': [14], \n          'min_samples_split': [10], 'n_estimators': [3000]}\n\n\ngrad_boost = GradientBoostingRegressor(learning_rate=0.05, loss='huber', max_depth=2, \n                                       max_features='log2', min_samples_leaf=14, min_samples_split=10, n_estimators=3000,\n                                       random_state=42)\n\n\ngrad_boost.fit(X_train, y_train)","a6c9093b":"y_pred = grad_boost.predict(X_val)\ngboost_mse = mean_squared_error(y_val, y_pred)\ngboost_rmse = np.sqrt(gboost_mse)\ngboost_rmse","69385784":"# Gradient Boosting was considerable better than RandomForest Regressor.\n# scale salesprice.\n# y_val = np.log(y_val)\nscore = rmsle_cv(grad_boost, X_val, y_val)\nprint(\"Gradient Boosting score: {:.4f}\\n\".format(score))","ad7c8d44":"# Define the models\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Lasso, Ridge\n\n# Parameters for Ridge\n# params = {\"alpha\": [0.5, 1, 10, 30, 50, 75, 125, 150, 225, 250, 500]}\n# grid_ridge = GridSearchCV(Ridge(random_state=42), params)\n# grid_ridge.fit(X_train, y_train)\n\n# Parameters for DecisionTreeRegressor\n# params = {\"criterion\": [\"mse\", \"friedman_mse\"], \"max_depth\": [None, 2, 3], \"min_samples_split\": [2,3,4]}\n\n# grid_tree_reg = GridSearchCV(DecisionTreeRegressor(), params)\n# grid_tree_reg.fit(X_train, y_train)\n\n\n\n# Parameters for SVR\n# params = {\"kernel\": [\"rbf\", \"linear\", \"poly\"], \"C\": [0.3, 0.5, 0.7, 0.7, 1], \"degree\": [2,3]}\n# grid_svr = GridSearchCV(SVR(), params)\n# grid_svr.fit(X_train, y_train)\n\n\n\n# Tune Parameters for elasticnet\n# params = {\"alpha\": [0.5, 1, 5, 10, 15, 30], \"l1_ratio\": [0.3, 0.5, 0.7, 0.9, 1], \"max_iter\": [3000, 5000]}\n# grid_elanet = GridSearchCV(ElasticNet(random_state=42), params)\n\n# Predictive Models\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.9, max_iter=3000)\nsvr = SVR(C=1, kernel='linear')\ntree_reg = DecisionTreeRegressor(criterion='friedman_mse', max_depth=None, min_samples_split=3)\nridge_reg = Ridge(alpha=10)\n\n# grid_elanet.fit(X_train, y_train)","6b10df34":"from mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\n# Try tomorrow with svr_rbf = SVR(kernel='rbf')\n# Check this website!\n# Consider adding two more models if the score does not improve.\nlin_reg = LinearRegression()\n\nensemble_model = StackingRegressor(regressors=[elastic_net, svr, rand_model, grad_boost], meta_regressor=SVR(kernel=\"rbf\"))\n\nensemble_model.fit(X_train, y_train)\n\n\nscore = rmsle_cv(ensemble_model, X_val, y_val)\nprint(\"Stacking Regressor score: {:.4f}\\n\".format(score))","e354dc7e":"# We go for the stacking regressor model\n# although sometimes gradientboosting might show to have a better performance.\nfinal_pred = ensemble_model.predict(test_scaled)","fe891b36":"# # Dataframe\nfinal = pd.DataFrame()\n\n# Id and Predictions\nfinal['Id'] = test_id\nfinal['SalePrice'] = np.expm1(final_pred)\n\n# CSV file\nfinal.to_csv('submission.csv', index=False) # Create Submission File\nprint('The File has been Submitted!')","a9e7ea72":"# import tensorflow as tf\n\n# import keras\n# from keras import backend as K\n# from keras.models import Sequential\n# from keras.layers import Activation\n# from keras.layers.core import Dense\n# from keras.optimizers import Adam\n# from keras.initializers import VarianceScaling\n\n\n\n# # Reset the graph looks crazy\n# def reset_graph(seed=42):\n#     tf.reset_default_graph()\n#     tf.set_random_seed(seed)\n#     np.random.seed(seed)\n    \n    \n# reset_graph()\n\n\n# m, n = X_train.shape\n\n# # Look at the preprocess data of the video and see the reshape part and apply it to X_train!\n# # he_init = keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_in\", distribution='normal', seed=None)\n\n\n\n\n# # Create a model (Add layers)\n# model = Sequential([\n#     Dense(n, input_shape=(n,), kernel_initializer='random_uniform', activation='relu'), # Start with the inputs\n#     Dense(50, input_shape=(1,), kernel_initializer='random_uniform', activation='relu'), # Number of Layers\n#     Dense(1, kernel_initializer='random_uniform')\n# ]) \n\n\n# model.summary()","5ab1d234":"# Outliers Analysis:\n<a id=\"analysis_outliers\"><\/a>\n**Analysis**:\n<ul>\n<li> The year of <b>2007<\/b> had the highest outliers (peak of the housing market before collapse). <\/li>\n<li>  The highest outliers are located in the <b> High category <\/b> of the Price_Range column.<\/li>\n<\/ul>","b0f1bb54":"### RandomForestRegressor:\n<a id=\"random_forest\"><\/a> \n<img src=\"https:\/\/techblog.expedia.com\/wp-content\/uploads\/2017\/06\/BoostedTreeExample.jpg\">\n**RandomForestRegressor** gives us more randomness, insead of searching through impurity the best feature, RandomForest picks features in a randomly manner to reduce variance at the expense of a higher bias. Nevertheless, this helps us find what the trend is. After all the trees have predicted the outcome for a specific instance, the average from all the DecisionTree models is taken and that will be the prediction for a specific instance.","bf30b313":"# Implementing Advanced Regression  Techniques for Prediction:\n\nThere are several factors that impact the overall price of the house, some of those factors are more **tangible** as the quality of the house or the overall size (area) of the house and other factors are more **intrinsic** such as the performance of the economy. Coming with an accurate model that predicts with such precision the actual value is an arduous task since there are both internal and external factors that will affect the price of a single house. Nevertheless, what we can do is **detect** those features that carry a heavier weight on the overall output (Price of the house). <br><br>\n\nBefore the housing crisis that occurred during the years of (2007-2008), most people believed that the prices of houses tended to go up throughout the years and that people that invested into properties were certain that they will get a return. This was not the case since banks were basically approving loans to people that were not able to afford to pay a house, there were even financial institutions who were approving loans to ordinary individuals at a variable interest rate (meaning rate will change depending on the current market rate) and when the crisis occurred lots of those ordinary individuals were not able to afford to pay back their mortgages. Of course, there were other reasons that caused the financial crisis in the first place such as the introduction of complex financial instruments (*derivatives are still not widely understood*), hedging financial instruments (credit default swaps), and the deregulation of the financial industry as a whole. While we can argue about the factors that caused the financial crisis, the main objective of this post is to determine what possible features could have a real impact on the overall value of a house. We will try to answer questions such as to what extent did the recession impacted the value house prices? What materials were most commonly used with houses that had a high price range? (Rooftop, walls etc.) Which neighborhoods were the most exclusive? <br><br>\n\nI believe that in order to perform an extensive analysis of this data we should explore our data, by this I mean getting a sense of what is the **story behind the data**. Most of the time I tend to reject the idea of just building a model that have a good accuracy score for predicting values instead, I analyze my data carefully (determining distributions, missing values, visualizations) in order to have a better understanding of what is going on. ONly after my extensive analysis I proceed to developing the predictive model, in this case we will use **regression models.** The downside of this to many of you who will see this post, is that it will be somewhat long, so if you think you should **skip** all the sections and start from the regression model step, please feel free to do so! I will create an outline so it will help you find the section you wish to start with. <br><br>\n\n**I'd rather have a full house at a medium price than a half-full at a high price. - George Shinn**\n***","d044753d":"## Outline: \n***\nI. **Understanding our Data**<br>\na) [Splitting into Different Categories](#splitting)<br>\nb) [Gathering Basic Insight](#insight) <br><br>\n\nII. **Economic Activity**<br><br>\nIII. [Outside Surroundings](#outside)<br>\na) [Type of Zoning](#zoning)<br>\nb) [Neighborhoods](#neighborhoods) <br><br>\n\nIV. **Areas of the House** <br>\na) [The Impact of Space towards Price](#space)<br><br>\n\nV. **Building Characteristics**<br>\na) [Correlations with SalePrice](#correlation)<br>\nb) [What garages tell about House Prices?](#garage)<br><br>\n\nVI. **Miscellaneous and Utilities**<br>\na) [What determines the quality of the house?](#quality)<br>\nb) [Intersting insights](#interesting)<br>\nc) [Which Material Combination increased the Price of Houses?](#material)<br><br>\n\nVII. [Quality of Neighborhoods](#quality_neighborhoods)<br><br>\n\nVIII. **The Purpose of using Log Transformations** <br>\na)[Log Transformations](#log_transformations)<br>\nb) [Skewedness and Kurtosis](#skew_kurt)<br>\nc) [Outliers Analysis](#analysis_outliers)<br>\nd) [Bivariate Analysis](#bivariate) <br><br>\n\nIX. **Feature Engineering** <br>\na) [Dealing with Missing Values](#missing_values)<br>\nb) [Transforming Values](#transforming_values)<br>\nc) [Combining Attributes](#combining_atributes) <br>\nd) [Dealing with numerical and categorical values](#num_cat_val) <br><br>\n\nX. **Scaling** <br>\na) [Categorical Encoding Class](#categorical_class)<br>\nb) [Combine Attribute Class](#combining)<br>\nc) [Pipelines](#combining)<br><br>\n\nXI. **Predictive Models** <br>\na) [Residual Plot](#residual_plot) <br>\nb) [RandomForests Regressor](#random_forest) <br>\nc) [GradientBoosting Regressor](#gradient_boosting)<br>\nd) [Stacking Regressor](#stacking_regressor)","f41cf8f7":"<h1 align=\"center\"> Quality of Neighborhoods <\/h1>\n<a id=\"quality_neighborhoods\"><\/a>\n<img src=\"http:\/\/www.unitedwaydenver.org\/sites\/default\/files\/UN_neighborhood.jpg\">\n\n## Which Neighborhoods had the best Quality houses?\n<a id=\"which_neighborhoods\"><\/a>","72461a46":"## Combine Attribute Class:\n<a id=\"combining\"><\/a>\nThis class will help us to include the total area variable into our pipeline for further scaling.","1b17da13":"## Dealing with Numerical and Categorical Values:\n<a id=\"num_cat_val\"><\/a>","24f64e76":"<h1 align=\"center\"> Feature Engineering <\/h1>\n<a id=\"feature_engineering\"><\/a>\n## Dealing with Missing Values:\n<a id=\"missing_values\"><\/a>","ba471aaf":"## Conclusion:\nI got a 0.13 score approximately, in the future I aim to fix some issues with regards to the tuning of hyperparameters and implement other concepts of feature engineering that will help algorithms make a more concise prediction. Nevertheless, this project helped me understand more complex models that could be implemented in practical situations. Hope you enjoyed our in-depth analysis of this project and the predictive models used to come with close to accurate predictions. Open to constructive criticisms!","85799b3b":"# Miscellaneous and Utilities:\n<a id=\"utilities\"><\/a>","fb70c7d4":"## StackingRegressor:\n<img src=\"https:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingRegressor_files\/stackingregression_overview.png\">\n<a id=\"stacking_regressor\"><\/a>\nIn stacking regressor we combine different models and use the predicted values in the training set to mae further predictions. In case you want to go deeper into parameter <b>\"tuning\"<\/b> I left you the code above the different models so you can perform your own GridSearchCV and find even more efficient parameters! <br>\n<ul>\n<li> ElasticNet <\/li>\n<li> DecisionTreeRegressor <\/li>\n<li> MLPRegressor (Later I will include it after learning more about neural networks) <\/li>\n<li> SVR <\/li>\n<\/ul>","b0294bab":"## Neighborhoods: \n<a id=\"neighborhoods\">\n","bc364f15":"# Splitting the Variables into Different Categories:\n<a id=\"splitting\"><\/a>\n## Data Analysis:\nFor data analysis purposes I am going to separate the different features into different categories in order to segment our analysis. These are the steps we are going to take in our analysis: Nevertheless, I will split the categories so you can analyse thoroughly the different categories.<br>\n1) Separate into different categories in order to make our analysis easier. <br>\n2) All of our categories will contain sales price in order to see if there is a significant pattern.<br>\n3) After that we will create our linear regression model in order to make accurate predictions as to what will the price of the houses will be.<br><br>\n4) For all the categories we have id, salesprice, MoSold, YearSold, SaleType and SaleCondition.\n\n**Note:** At least for me, it is extremely important to make a data analysis of our data, in order to have a grasp of what the data is telling us, what might move salesprice higher or lower. Instead of just running a model and just predict prices, we must make a thorough analysis of our data. Also, using these different categories is completely optional in case you want to make a more in-depth analysis of the different features.","aa9667b7":"# High Correlated Variables with SalePrice:\n<a id=\"correlation\"><\/a>\n1) YearBuilt - The Date the building was built. <br>\n2) YearRemodAdd - The last time there wasa building remodeling. <br>\n3) MasVnArea - Masonry veneer area in square feet. <br>\n4) GarageYrBlt - Year garage was built. <br>\n5) GarageCars - Size of garage in car capacity. <br>\n6) GarageArea - Size of garage in square feet. <br>","841f2737":"**Kurtosis**:\n<ul>\n<li><b>Kourtosis<\/b> is a measure of how extreme observations are in a dataset.<\/li>\n<li> The <b> greater the kurtosis coefficient <\/b>, the more peaked the distribution around the mean is. <\/li>\n<li><b>Greater coefficient<\/b> also means fatter tails, which means there is an increase in tail risk (extreme results) <\/li>\n<\/ul>\n\n**Reference**:\nInvestopedia: https:\/\/www.investopedia.com\/terms\/m\/mesokurtic.asp\n","39996fe8":"<img src=\"http:\/\/tibmadesignbuild.com\/images\/female-hands-framing-custom-kitchen-design.jpg\">\n\n## Interesting insights:\n<a id=\"interesting\"><\/a>\n1) **Overall Condition**: of the house or building, meaning that further remodelations are likely to happen in the future, either for reselling or to accumulate value in their real-estate.. <br>\n2) **Overall Quality**: The quality of the house is one of the factors that mostly impacts SalePrice. It seems that the overall material that is used for construction and the finish of the house has a great impact on SalePrice. <br>\n3) **Year Remodelation**: Houses in the **high** price range remodelled their houses sooner. The sooner the remodelation the higher the value of the house. <br>\n","10cf0e7a":"## Categorical Encoding Class:\n<a id=\"categorical_class\"><\/a>\nThis is a way to encode our features in a way that it avoids the assumption that two nearby values are more similar than two distant values. This is the reason we should avoid using LabelEncoder to scale features (inputs) in our dataset and in addition the word **LabelEncoder** is used for scaling labels (outputs). This could be used more often in **binary classification problems** were no *association* exists between the outputs.","831ab47a":"## Keras and TensorFlow:\nAlthough the accuracy of our neural network  is still not as accurate as our ensemble and boosting model, I wanted to share two main aspects of tensorflow.\n<ul>\n<li> Implementing a Neural Network with a real life <b>regression scenario<\/b>. <\/li>\n<li>Show the structure of Neural Networks through <b>tensorboard<\/b> (we will do this with ipython display.) <\/li>\n<\/ul>\n<br><br>\n\n(Reference: Hands On Machine Learning and TensorFlow by Aur\u00e9lien G\u00e9ron)","c9f820c9":"## Bivariate Analysis (Detecting outliers through visualizations):\n<a id=\"bivariate\"><\/a>\n**There are some outliers in some of this columns but there might be a reason behind this, it is possible that these outliers in which the area is high but the price of the house is not that high, might be due to the reason that these houses are located in agricultural zones.**","53415c03":"## Skewedness and Kurtosis:\n<a id=\"skew_kurt\"><\/a>\n**Skewedness**: <br>\n<ul>\n<li> A skewness of <b>zero<\/b> or near zero indicates a <b>symmetric distribution<\/b>.<\/li>\n<li> A <b>negative value<\/b> for the skewness indicate a <b>left skewness<\/b> (tail to the left) <\/li>\n<li> A <b>positive value<\/b> for te skewness indicate a <b> right skewness <\/b> (tail to the right) <\/li>\n<ul>","206b040d":"## Combining Attributes\n<a id=\"combining_atributes\"><\/a>","70388280":"## What Garages tells us about each Price Category:\n<a id=\"garage\"><\/a>\n<img src=\"https:\/\/www.incimages.com\/uploaded_files\/image\/970x450\/garage-office-970_24019.jpg\">","f70c8657":"### References: \n1) <a href=\"https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\">Stacked Regressions : Top 4% on LeaderBoard<\/a> by Serigne.\n- Good if you are looking for stacking models and to gather an in-depth analysis for feature engineering. <br><br>\n\n2) <a href=\"https:\/\/www.kaggle.com\/vhrique\/simple-house-price-prediction-stacking\"> Simple House Price Prediction Stacking <\/a> by Victor Henrique Alves Ribeiro.  \n- Gave me an idea of which algorithms to implement in my ensemble methods. <br>\n- Also Victor is really open to answer any doubts with regards to this project. <br><br>\n\n3) <a href=\"https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\"> Comprehensive data exploration with Python <\/a> by Pedro Marcelino.\n- Help me understand more in depth the different linear regularization methods and its parameters. <br><br>\n\n4) <b> Hands on Machine Learning with Scikit-Learn & TensorFlow by Aur\u00e9lien G\u00e9ron (O'Reilly). CopyRight 2017 Aur\u00e9lien G\u00e9ron   <\/b><br>\n- Good reference for understanding how Pipelines work. <br>\n- Good for understanding ensemble methods such as RandomForests and GradientBoosting. <br>\n- This book is a must have for people starting in the area of machine learning.<br><br>\n\n\n5) <a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/a-comprehensive-guide-for-linear-ridge-and-lasso-regression\/\"> A comprehensive beginners guide for Linear, Ridge and Lasso Regression <\/a> by Shubham Jain at Analytics Vidhya.\n- Helped me implement a residual plot.. <br>\n- Better understanding of Ridge, Lasso and ElasticNet (Good for Beginners).","ec6f0005":"## Goal of this Project:\n***\n### Achieving our goal is split into two phases: <br>\n1) **Exploratory Data Analysis (EVA)**: In this phase our main aim is to have a better understanding of the features involved in our data. It might be possible that some are left behind but I will be focusing on the features that have the highest correlation towards SalePrice. <br><br>\n\n2) **Advanced Regression**: We will implement Regression model to predict a possible SalePrice (label) of the house.","684be69b":"<h1 align=\"center\"> Implementing Predictive Models <\/h1>\n\n<img src=\"http:\/\/precisionanalytica.com\/blog\/wp-content\/uploads\/2014\/09\/Predictive-Modeling.jpg\">\n\n## Residual Plot:\n<a id=\"residual_plot\"><\/a>\n<ul>\n<li><b>Residual plots<\/b> will give us more or less the actual prediction errors our models are making. In this example, I will use <b>yellowbrick library<\/b> (statistical visualizations for machine learning) and a simple linear regression model.  In our <b>legends<\/b> of the residual plot it says training and test data but in this scenario instead of the test set it is the <b>validation set<\/b> we are using. [If there is a possibility to change the name of the legend to validation I will make the update whenever possible.<\/li>\n<li> Create a validation set within the training set to actually predict values. (Remember the test set does not have the training price, and also when testing data it should be done during the last instance of the project.) <\/li>\n\n<\/ul>","c88bd522":"**Note:** Interestingly, the Masonry Veneer type of stone became popular after 2007 for the houses that belong to the **high** Price Range category. I wonder why? <br>\n**For some reason during the year of 2007, the Saleprice of houses within the high range made of stone dropped drastically! \n\n","f1ca7906":"## Which Material Combination increased the Price of Houses?\n<a id=\"material\"><\/a>\n<ul>\n<li> <b>Roof Material<\/b>: <b>Hip<\/b> and <b>Gable<\/b> was the most expensive since people who bought <b>high value<\/b> houses tended to buy this material bor he rooftop.<\/li>\n<li> <b>House Material<\/b>: Houses made up of <b>stone<\/b> tend to influence positively the price of the house. (Except in 2007 for <b>High Price House Values. <\/b>)  <\/li>\n<\/ul>\n","be589be6":"## The Purpose of Log Transformations:\n<a id=\"log_transformations\"><\/a>\nThe main reason why we use log transformation is to reduce **skewness** in our data. However, there are other reasons why we log transform  our data: <br>\n<ul>\n<li> Easier to interpret patterns of our data. <\/li>\n<li> For possible statistical analysis that require the data to be normalized.<\/li>\n<\/ul>","03f452f5":"<h1 align=\"center\">The Impact of Space towards Price:<\/h1>\n<a id=\"space\"><\/a>\n<img src=\"http:\/\/www.archiii.com\/wp-content\/uploads\/2013\/06\/Office-Orchard-House-Interior-Design-by-Arch11-Architecture-Interior.jpg\" width=700 height=300>\n<br><br>\n\n## The Influence of Space:\nHow much influence does space have towards the price of the house. Intuitively, we might think the bigger the house the higher the price but let's take a look in order to see ifit actually has a positive correlation towards **SalePrice**.\n\n## Summary:\n<ul>\n<li><b>GrlivingArea:<\/b> The living area square feet is positively correlated with the price of the house.<\/li>\n<li> <b> GarageArea:<\/b> Apparently, the space of the garage is an important factor that contributes to the price of the house. <\/li>\n<li>  <b>TotalBsmft:<\/b> The square feet of the basement contributes positively to the value of the house. <\/li>\n<li> <b>LotArea and LotFrontage:<\/b> I would say from all the area features these are the two that influencess the less on the price of the house.  <\/li>\n<\/ul>","f9468a91":"<h1 align=\"center\"> Economic Activity: <\/h1>\n<a id=\"economy\"><\/a>\n<img src=\"http:\/\/vietsea.net\/upload\/news\/2016\/12\/1\/11220161528342876747224.jpg\">\nWe will visualize how the housing market in **Ames, IOWA** performed during the years 2006 - 2010 and how bad it was hit by the economic recession during the years of 2007-2008.  \n\n## Level of Supply and Demand (Summary):\n<ul>\n<li><b>June<\/b> and <b>July<\/b> were the montnths in which most houses were sold. <\/li>\n<li> The <b> median house price <\/b> was at its peak in 2007 (167k) and it was at its lowest point during the year of 2010 (155k) a difference of 12k. This might be a consequence of the economic recession. <\/li>\n<li> Less houses were <b>sold<\/b> and <b>built<\/b> during the year of 2010 compared to the other years. <\/li>\n<\/ul>\n\n","577bca34":"<h1 align=\"center\"> Outside Surroundings of the House: <\/h1>\n<a id=\"outside\"><\/a>\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/bc\/Lot_map.PNG\">\n## Features from Outside: \nIn this section we will create an in-depth analysis of how the outside surroundings affect the price. Which variables have the highest weight on price. You can use the **train** dataframe or the **outsidesurr_df** to simplify the amount of features and have a closer look as to how they behave towards **\"SalePrice\"**. For the correlation matrix I will be using outsidesurr_df so you can have a better look as to which variables from the **outside surroundings category** impact the most on the price of a house. <br><br>\n\n## Summary:\n<ul>\n<li> The <b>mean price<\/b> of the house of is 180,921, this will explain why the data is right skewed. <\/li>\n<li> <b>Standard deviation<\/b> is pretty high at 79442.50 meaning the data deviates a lot from the mean (many outliers) <\/li>\n<li> <b>LotArea<\/b> and <b>LotFrontage<\/b> had the highest correlation with the price of a house from the <b> outside surroundings category <\/b>. <\/li>\n<li> Most of the houses that were sold were from a <b> Residential Low Density Zone <\/b>.<\/li>\n<li> The most exclusive Neighborhoods are <b>Crawfor<\/b>, <b>Sawyer<\/b> and <b>SawyerW<\/b><\/li>\n<\/ul>","094403ee":"# Gathering a Basic Insight of our Data:\n<a id=\"insight\"><\/a>\n<br><br>\n<img src=\"http:\/\/blog.algoscale.com\/wp-content\/uploads\/2017\/06\/algoscale_data_analytics4.jpg\">\n<br><br>\n\n## Summary:\n<ul>\n<li> The distribution of <b> house prices <\/b> is right skewed.<\/li>\n<li> There is a <b>drop<\/b> in the number of houses sold during the year of 2010. <\/li>\n<\/ul>","2d5cbfc5":"## Right-Skewed Distribution Summary:\nIn a right skew or positive skew the mean is most of the times to the right of the median. There is a higher frequency of occurence to the left of the distribution plot leading to more exceptions (outliers to the right). Nevertheless, there is a way to transform this histogram into a normal distributions by using <b>log transformations<\/b> which will be discussed further below.","999beeae":"## Type of Zoning:\n<a id=\"zoning\"><\/a>","8f897232":"## Pipelines:\n<a id=\"pipelines\"><\/a> \n\nCreate our numerical and cateogircal pipelines to scale our features.","e8dc63ec":"## GradientBoostingRegressor:\n<img src=\"https:\/\/image.slidesharecdn.com\/slides-140224130205-phpapp02\/95\/gradient-boosted-regression-trees-in-scikitlearn-21-638.jpg?cb=1393247097\">\n<a id=\"gradient_boosting\"><\/a>\nThe Gradient Boosting Regressor class trains the models over the residuals (prediction errors) leading to smaller variances and higher accuracy.  ","183c3aea":"<h1 align=\"center\"> What determines the quality of the House? <\/h1>\n<a id=\"quality\"><\/a>\n\nRemember quality is the most important factor that contributes to the SalePrice of the house. <br>\n**Correlations with OverallQual:**<br>\n1) YearBuilt <br>\n2) TotalBsmtSF <br>\n3) GrLivArea <br>\n4) FullBath <br>\n5) GarageYrBuilt <br>\n6) GarageCars <br>\n7) GarageArea <br><br>","79c69307":"<h1 align=\"center\"> Building Characteristics: <\/h1>\n<a id=\"building_characteristics\"><\/a>\n","e9741095":"## Transforming our Data:\n<ul>\n<li> Separate the <b> features <\/b> and <b> labels <\/b> from the training dataset. <\/li>\n<li> Separate <b> numeric <\/b> and <b> categorical <\/b> variables for the purpose of running them in separate pipelines and scaling them with their respective scalers. <\/li>\n\n<\/ul>","f02588df":"## Transforming Missing Values:\n<a id=\"transforming_values\"><\/a>\n"}}