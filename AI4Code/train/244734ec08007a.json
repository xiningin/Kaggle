{"cell_type":{"190c47f8":"code","a2a242ee":"code","44776bf9":"code","585fb539":"code","c9e0207d":"code","0534356e":"code","2e719ace":"code","63938cbf":"code","e1898a23":"code","b17ffa85":"code","50d9dcfd":"code","3defd8ed":"code","c1d8d6ec":"code","49352964":"code","14d09615":"code","ba2342c1":"code","45311212":"code","2e4bf2e6":"code","77cd8833":"code","bf90b73b":"code","f4f4b0cf":"code","200ba5dd":"code","8435e94b":"code","1df48f2b":"code","2665cf72":"markdown","26006176":"markdown"},"source":{"190c47f8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.python.data import Dataset\n\nimport os\nprint(os.listdir(\"..\/input\"))","a2a242ee":"#loading the data\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format\n\ncovtype = pd.read_csv(\"..\/input\/covtype.csv\", sep=\",\")\ncovtype = covtype.reindex(\n    np.random.permutation(covtype.index)) # randomize the position of records","44776bf9":"covtype.head() #checking the data index was radomized","585fb539":"covtype.shape #number of records","c9e0207d":" #since data is ready with all dummy variables, we don't need to transform it\ncovtype['Cover_Type'].unique()","0534356e":"covtype['Cover_Type']=covtype['Cover_Type'].astype('category') # convert to categorical datatype for multiple output prediction","2e719ace":"#target is categorical data, we convert it into dummies\ndummies= pd.get_dummies(covtype, columns=['Cover_Type'])","63938cbf":"dummies.shape","e1898a23":"#dummies['new']=covtype['Cover_Type']","b17ffa85":"one_hot_code=dummies # assign dataset to one_hot_code\n\n#one_hot_code=covtype\none_hot_code.head()","50d9dcfd":"one_hot_code.columns #easier to copy column names for next code","3defd8ed":"one_hot_code","c1d8d6ec":"mean_tr=1\nstd_tr=1","49352964":"\ndef preprocess_features(one_hot_code, batch_norm_required):\n    selected_features = one_hot_code[ # features that will be used for training data\n    [ 'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n       'Soil_Type39', 'Soil_Type40']]\n    batch_norm = selected_features[['Aspect','Elevation',  'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points']] \n    if batch_norm_required:\n        global mean_tr, std_tr\n        mean_tr=batch_norm.mean()\n        std_tr=batch_norm.std()\n     \n    batch_norm=(batch_norm-mean_tr)\/std_tr\n    selected_features.update(batch_norm)\n\n        \n    # Add noise\n    processed_features = selected_features.copy()  \n    \n    return selected_features\n\ndef preprocess_targets(one_hot_code): # targets that are used for adjusting the model\n    output_targets = one_hot_code[\n        [ \n        'Cover_Type_1' , 'Cover_Type_2',\n       'Cover_Type_3', 'Cover_Type_4', 'Cover_Type_5', 'Cover_Type_6',\n       'Cover_Type_7'\n        ]]\n    return output_targets","14d09615":"#(one_hot_code.head(464808)[\"Horizontal_Distance_To_Hydrology\"]-one_hot_code.head(464808)[\"Horizontal_Distance_To_Hydrology\"].mean())\/(one_hot_code.head(464808)[\"Horizontal_Distance_To_Hydrology\"]).std()","ba2342c1":"# Train and test sets\n# Choose examples for training.\ntraining_examples = preprocess_features(one_hot_code.head(464808), 1)\ntraining_targets = preprocess_targets(one_hot_code.head(464808))\n\n# Choose examples for validation.\nvalidation_examples = preprocess_features(one_hot_code.tail(116203), 0)\nvalidation_targets = preprocess_targets(one_hot_code.tail(116203))\n\n# Double-check that we've done the right thing.\nprint(\"Training examples summary:\")\ndisplay.display(training_examples.describe())\nprint(\"Validation examples summary:\")\ndisplay.display(validation_examples.describe())\n\nprint(\"Training targets summary:\")\ndisplay.display(training_targets.describe())\nprint(\"Validation targets summary:\")\ndisplay.display(validation_targets.describe())","45311212":"training_examples.shape # neurons should be two times the amount of features, usually","2e4bf2e6":"# model construction: forming the network layers using keras\n\nmodel = keras.Sequential([\n keras.layers.Dense(1024, activation=tf.nn.relu, # 108 neurons with relu activation, first layer with input\n input_shape=(training_examples.shape[1],)), \n #keras.layers.Dropout(0.5), # dropout for reducing the overfitting problem\n keras.layers.Dense(512, activation=tf.nn.softplus), # second layer\n #keras.layers.Dropout(0.5),\n keras.layers.Dense(256, activation=tf.nn.relu),\n\n keras.layers.Dense(7, activation=tf.nn.softmax) #  output layer with 7 categories\n ])\n\noptimizer = tf.train.AdamOptimizer()\n\nmodel.compile(loss='categorical_crossentropy', #this loss method is useful for multiple categories, otherwise our model does not work\n optimizer=optimizer,\n metrics=['accuracy'])\n#model.summary()","77cd8833":"class PrintDot(keras.callbacks.Callback):\n def on_epoch_end(self, epoch, logs):\n  if epoch % 100 == 0: print('')\n  print('.', end='')\nEPOCHS = 5  \n# Store training stats\nb_history = model.fit(training_examples, training_targets, epochs=EPOCHS,batch_size=1024, #divisible by training size\n                    validation_data= (validation_examples, validation_targets), verbose=0,\n                    callbacks=[PrintDot()])\n#l2_history = l2_model.fit(training_examples, training_targets, epochs=EPOCHS,batch_size=2568,\n#                    validation_data= (validation_examples, validation_targets), verbose=0,\n#                    callbacks=[PrintDot()])","bf90b73b":"#b_history.history","f4f4b0cf":"import matplotlib.pyplot as plt\ndef plot_history(b_history):\n plt.figure()\n plt.xlabel('Epoch')\n plt.ylabel('accuracy')\n plt.plot(b_history.epoch, np.array(b_history.history['acc']),\n label='Train acc')\n plt.plot(b_history.epoch, np.array(b_history.history['val_acc']),\n label = 'Val acc')\n plt.legend()\n plt.ylim([0, 1])\nplot_history(b_history)","200ba5dd":"#plot_history(l2_history)\nprint('training acc.:',b_history.history['acc'][-1],'\\n','test acc.:', (b_history.history['val_acc'])[-1])","8435e94b":"#tr_loss, tr_acc = model.evaluate(training_examples, training_targets)\n#test_loss, test_acc = model.evaluate(validation_examples, validation_targets)\n#print('Training accuracy:', tr_acc, '\\n Test accuracy:', test_acc) # baseline model accuracy","1df48f2b":"#tr_loss, tr_acc = l2_model.evaluate(training_examples, training_targets)\n#test_loss, test_acc = l2_model.evaluate(validation_examples, validation_targets)\n#print('Training accuracy:', tr_acc, '\\n Test accuracy:', test_acc) # l2 regularized model","2665cf72":"# Baseline model with dropouts and L2 regularized models are both showing the problem of overfitting. More tuning of the models is required. One way to reduce overfitting is dimension reduction where we remove some features.","26006176":"## Predicting the cover type of forest for our data set is a very interesting problem. Although we have more than 580 thousand observations, our neural network model is suffering from the problem of overfitting. Training accuracy is around 80% whereas test accuracy is hovering around 62%. After doing batch normalization on data, we are getting little improvement for test accuracy . "}}