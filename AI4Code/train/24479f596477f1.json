{"cell_type":{"c7d3474b":"code","049401b7":"code","a0c41868":"code","53a7c0e4":"code","36e5fc02":"code","828fba32":"code","d9c6d249":"code","f639e277":"code","00f24fb5":"code","6c4c223a":"code","1d00f1e7":"code","7c4002b6":"code","62d715a6":"code","52cd1cbf":"code","9354a601":"code","0dc3287a":"code","7fc13817":"code","0d459f50":"code","13187350":"code","590d6b53":"code","5b6bbe3f":"code","c537320b":"code","e8a571fb":"code","5727cbda":"code","9dbc4886":"code","3cc36a7d":"code","e442b479":"code","17ac974f":"code","7bf49215":"code","bbfe9f7b":"code","bdff2d52":"code","68b56cf4":"code","bdfba67c":"code","dd12e4fb":"code","241b7de8":"code","46b64e09":"code","b82024aa":"code","3d246470":"code","abdb5c01":"code","1db76caf":"code","da5b7866":"code","43b9c014":"markdown","7885a751":"markdown","d7c23af3":"markdown","17aaa282":"markdown","28c64d92":"markdown","2845ae42":"markdown","31782c94":"markdown","69904e6a":"markdown","43fd9aa7":"markdown","711be49c":"markdown","fd444f37":"markdown","a31ea5d3":"markdown","e0380932":"markdown","e82fe1a3":"markdown","2d089881":"markdown","c7bf50e0":"markdown","d393d081":"markdown","66c0adca":"markdown","1955cf27":"markdown","dc19f61f":"markdown","043b42e2":"markdown","1232541c":"markdown","de923861":"markdown","16c667a5":"markdown","c7af2050":"markdown","9f38ca85":"markdown","ec471fb2":"markdown","579d7e96":"markdown","fc095c12":"markdown","2274f537":"markdown","94f1a294":"markdown","294eee26":"markdown","b010a9c5":"markdown","6fcb9053":"markdown","f590de97":"markdown","9a893efb":"markdown","ba673f2c":"markdown","e47793b3":"markdown","2924620d":"markdown","01035745":"markdown","3bf542e4":"markdown"},"source":{"c7d3474b":"# For data manipulation\nimport pandas as pd\nimport numpy as np\n\n# For data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n%matplotlib inline\n\n# For nice displaying output in this notebook\nfrom IPython.display import display, HTML\n\n# For data modeling & prediction\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor","049401b7":"# code snippet from https:\/\/stackoverflow.com\/questions\/38783027\/jupyter-notebook-display-two-pandas-tables-side-by-side\nfrom IPython.core.display import display, HTML\n\ndef display_side_by_side(dfs:list, captions:list):\n    \"\"\"Display tables side by side to save vertical space\n    Input:\n        dfs: list of pandas.DataFrame\n        captions: list of table captions\n    \"\"\"\n    output = \"\"\n    combined = dict(zip(captions, dfs))\n    for caption, df in combined.items():\n        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n        output += \"\\xa0\\xa0\\xa0\"\n    display(HTML(output))","a0c41868":"filepath = '..\/input\/walmart-recruiting-store-sales-forecasting\/'\ndatasets = dict(\n    stores = pd.read_csv(f'{filepath}stores.csv'),\n    train_data = pd.read_csv(f'{filepath}train.csv.zip', parse_dates=['Date'], compression='zip'),\n    test_data = pd.read_csv(f'{filepath}test.csv.zip', parse_dates=['Date'], compression='zip'),\n    features = pd.read_csv(f'{filepath}features.csv.zip', parse_dates=['Date'], compression='zip')\n)","53a7c0e4":"display_side_by_side([datasets[k].head() for k in datasets], [k for k in datasets])","36e5fc02":"# Set label for identifying dataset\ndatasets['train_data']['Set'] = 'Train'\ndatasets['test_data']['Set'] = 'Test'\n\n# Set missing values for Weekly_sales in test_data (we'll predict these values)\ndatasets['test_data']['Weekly_Sales'] = np.nan\n\n# Concatenate train & test datasets into a single dataset\nfull = pd.concat([datasets['train_data'], datasets['test_data']], sort=False)\n\n# Merge train\/test dataset with stores & features\nfull = datasets['stores'].merge(full)\nfull = full.merge(datasets['features'])\n\n# Order by set\nfull = (full.sort_values(['Set', 'Date', 'Store', 'Dept'], ascending=[False, True, True, True])\n        .reset_index(drop=True))\n\n# Display first lines and dimensions\ndisplay(full.head())\nprint('{dim[0]} rows and {dim[1]} columns.'.format(dim = full.shape))","828fba32":"# Split into new columns after the 'Date' column\nfull.insert(full.columns.tolist().index('Date')+1, 'Week', full.Date.dt.week)\nfull.insert(full.columns.tolist().index('Date')+2, 'Month', full.Date.dt.month)\nfull.insert(full.columns.tolist().index('Date')+3, 'Year', full.Date.dt.year)","d9c6d249":"# Copy the holidays specs from the competition data description\nholidays = {\n    'Super Bowl': pd.to_datetime(['12-Feb-10', '11-Feb-11', '10-Feb-12', '8-Feb-13']),\n    'Labor Day': pd.to_datetime(['10-Sep-10', '9-Sep-11', '7-Sep-12', '6-Sep-13']),\n    'Thanksgiving': pd.to_datetime(['26-Nov-10', '25-Nov-11', '23-Nov-12', '29-Nov-13']),\n    'Christmas': pd.to_datetime(['31-Dec-10', '30-Dec-11', '28-Dec-12', '27-Dec-13'])\n}\n\n# Make a new column for specifying the holiday (after the IsHoliday column)\nfull.insert(full.columns.tolist().index('IsHoliday')+1, 'Holiday', np.nan)\n# Label the holidays in this column\nfor holiday in holidays:\n    full.loc[full['Date'].isin(holidays[holiday]), 'Holiday'] = holiday","f639e277":"full.info()","00f24fb5":"category_cols = ['Store', 'Type', 'Dept', 'Week', 'Month', 'Year', 'Holiday', 'Set']\nfor col in category_cols:\n    if col in ['Week', 'Month', 'Year']:\n        full[col] = full[col].astype('category').cat.as_ordered()\n    else:\n        full[col] = full[col].astype('category')","6c4c223a":"full.info()","1d00f1e7":"full.describe()","7c4002b6":"pd.set_option('display.max_rows', None) # set an option to show all rows \ndisplay(full.groupby('Set').describe().T) # show all rows grouping by set\npd.reset_option('display.max_rows') # reset that option","62d715a6":"# Set negative values to 0\ncols = full.filter(regex='^Weekly|^Mark').columns\nfull[cols] = full[cols].apply(lambda x: np.where(x<0, 0, x))","52cd1cbf":"full.describe(include=['datetime', 'category'])","9354a601":"full.groupby('Set').describe(include=['datetime', 'category']).T","0dc3287a":"dummies = pd.get_dummies(full, columns=['Type', 'Holiday']).filter(regex=\"^[Type|Holiday].+_\")\nfull = pd.concat([full, dummies], axis=1)","7fc13817":"# Set categorial variables to numeric in order to appear in the heatmap\nfull[['Dept', 'Store', 'Week','Month', 'Year']] = full[['Dept', 'Store', 'Week', 'Month', 'Year']].astype('int')\n\n# Plot Heatmap from variable's correlation\nplt.figure(figsize=(16,10))\nm = full.corr()\nnp.fill_diagonal(m.values, np.nan)\nsns.heatmap(m, cmap='seismic', annot=True, fmt='.2f', annot_kws={\"size\": 9})","0d459f50":"# Set categorial variables back as a category\nfull[['Store', 'Dept', 'Week', 'Month', 'Year']] = full[['Store', 'Dept', 'Week', 'Month', 'Year']].astype('category')\n# Separate numerical variables for plotting\ny_target = 'Weekly_Sales'\nx_num_vars = full.select_dtypes(['float', 'int']).columns\nx_num_vars = x_num_vars[x_num_vars!=y_target]\nmarkdown_cols = full.filter(regex='Mark*').columns\nx_num_vars = x_num_vars[~x_num_vars.isin(markdown_cols)]","13187350":"sns.pairplot(full.query('Set==\"Train\"'), x_vars=x_num_vars, y_vars=y_target, hue='IsHoliday', height=4, aspect=.8)","590d6b53":"sns.pairplot(full.query('Set==\"Train\"'), x_vars=markdown_cols, y_vars=y_target, hue='IsHoliday', height=3.2, aspect=.75 )","5b6bbe3f":"mkdn = full[full.Set=='Train'].melt('IsHoliday', [f'MarkDown{i}' for i in [1,4,5]])\nf = sns.pointplot(data=mkdn, x='IsHoliday', y='value', hue='variable', estimator=np.mean)\nf.set_ylabel('Weekly Sales mean')","c537320b":"cat_cols = full.select_dtypes(['category']).columns[:-2]","e8a571fb":"def plt_category(category):\n    plt.figure(figsize=(12,6))\n    plt_order=full.groupby(category)['Weekly_Sales'].mean().sort_values().index\n    fig = sns.pointplot(data=full, x=category, y=y_target, hue='IsHoliday', order=plt_order)\n    return fig","5727cbda":"list(map(plt_category, cat_cols));","9dbc4886":"fig, axes = plt.subplots(1, 5, figsize=(20,5))\n\nfor i in range(5):\n    mkdn = f'MarkDown{i+1}'\n    plt_order=full.groupby('Type')[mkdn].mean().sort_values().index\n    sns.pointplot(data=full, x='Type', y=mkdn, hue='IsHoliday', order=plt_order, ax = axes[i])\n    axes[i].set_title(mkdn)\n    axes[i].set_ylabel('')","3cc36a7d":"plt.figure(figsize=(12,8))\nl = full[full.Set=='Train'].tail(1).index[0]\nf = sns.heatmap(full.isnull(), cbar=False, yticklabels='')\nf.axhline(l, ls='--', color='r'); # Separate the last occurrence from the train dataset","e442b479":"# Get percentage of NAs for each variable by dataset\nnas_perc = full.groupby('Set').apply(lambda x: (x.isna().sum() \/ full.shape[0])*100).T.sort_values('Train', ascending=False)\nnas_perc = nas_perc[nas_perc.apply(lambda x: sum(x)>0, axis=1)]\n\nnas_perc = (\n    nas_perc.reset_index()\n    .melt(id_vars='index', value_vars=['Test',\"Train\"])\n    .rename({'index':'Variable', 'value': 'NA (%)'}, axis=1)\n    .iloc[:, [1,0,2]]\n)","17ac974f":"plt.figure(figsize=(8,6))\nf = sns.barplot(data=nas_perc.sort_values(['Set','NA (%)'], ascending=False),\n            y='Variable', x='NA (%)', hue='Set')\nf.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f'{x:.0f}%'))\nf.set_xlabel('NAs');\nplt.grid()","7bf49215":"# Split the data into training and testing datasets\ntrain = full[full.Set=='Train'].reset_index(drop=True).copy()\ntest = full[full.Set=='Test'].reset_index(drop=True).copy()","bbfe9f7b":"## Filling missing values as described above ##\n## Train dataset\n# For each Markdown column\nfor col in [\"MarkDown\" + str(i) for i in range(1,6)]:\n    # Get mean by Type and Holiday columns\n    grouped = train.groupby(['Type', 'Holiday']).median()\n    # For each group\n    for name in grouped.index:\n        # Update missing values by group median\n        idx = (train[col].isna()) & (train.Type==name[0]) & (train.Holiday==name[1])\n        train.loc[idx, col] = grouped.loc[name, col]\n        # Update remaining missing values by general median\n        train.loc[train[col].isna(), col] = train[col].median()\n## Test dataset\n# For each Markdown column\nfor col in [\"MarkDown\" + str(i) for i in range(1,6)]:\n    # Get mean by Type and Holiday columns\n    grouped = test.groupby(['Type', 'Holiday']).median()\n    # For each group\n    for name in grouped.index:\n        # Update missing values by group median\n        idx = (test[col].isna()) & (test.Type==name[0]) & (test.Holiday==name[1])\n        test.loc[idx, col] = grouped.loc[name, col]\n        # Update remaining missing values by general median\n        test.loc[test[col].isna(), col] = test[col].median()\n# For Unemployment & CPI columns\nfor col in ['Unemployment', 'CPI']:\n    # Get mean by Store Type\n    grouped = test.groupby('Type').median()\n    # For each group\n    for name in grouped.index:\n        # Update missing values by group median\n        idx = (test[col].isna()) & (test.Type==name)\n        test.loc[idx, col] = grouped.loc[name, col]","bdff2d52":"# Drop columns like object type\nobj_like_cols = ['Type', 'Holiday', 'Set']\ntrain = train.drop(obj_like_cols, axis = 1)\ntest = test.drop(obj_like_cols, axis = 1)\n# Transform categorical cols into numeric\ncat_cols = test.select_dtypes('category').columns\ntrain[cat_cols] = train[cat_cols].astype('int64')\ntest[cat_cols] = test[cat_cols].astype('int64')\n# Set all integers to int64\ntrain[train.select_dtypes('uint8').columns] = train[train.select_dtypes('uint8').columns].astype('int')\ntest[test.select_dtypes('uint8').columns] = test[test.select_dtypes('uint8').columns].astype('int')\n# Scale variables\ncols_to_scale = train.select_dtypes(['int', 'float']).drop(['Weekly_Sales'], axis=1).columns\nscaler = preprocessing.StandardScaler().fit(train[cols_to_scale])\ntrain[cols_to_scale] = scaler.transform(train[cols_to_scale])","68b56cf4":"def WMAE(y, y_pred, isholiday):\n    W = np.ones(y_hat.shape)\n    W[isholiday == 1] = 5\n    metric = (1\/np.sum(W))*np.sum(W*np.abs(y-y_hat))\n\n    return metric","bdfba67c":"# Get predictor variables\nX = train[['Store', 'Size', 'Dept', 'Week', 'Month', 'Year', 'IsHoliday', 'Temperature', 'Fuel_Price', \n          'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', \n          'Type_A', 'Type_B', 'Type_C', 'Holiday_Christmas', 'Holiday_Labor Day',\n          'Holiday_Super Bowl', 'Holiday_Thanksgiving']]\n# Target variable\ny = train[y_target]\n# Split trainning data into train and test (validating) data\nX_train, X_test, y_train, y_test = train_test_split(X, y)","dd12e4fb":"# # Select best parameters and get mean score by cross validation\n# gsc = GridSearchCV (\n#         estimator=RandomForestRegressor(),\n#         param_grid={\n#             'max_depth': range(3,7),\n#             'n_estimators': (10, 50, 100, 150, 200),\n#         },\n#         cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n\n# grid_result = gsc.fit(X, y)\n# best_params = grid_result.best_params_\n# rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"])\n# scores = cross_val_score(rfr, X, y, cv=10, scoring='r2')\n# np.mean(scores)","241b7de8":"# Set the models\nmodels = {\n    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=1),\n    'extraTreesRegressor': ExtraTreesRegressor(n_estimators=100, max_features='auto', random_state=1),\n    'RandomForestRegressor': RandomForestRegressor(n_estimators=100, random_state=1)\n}\n\n# Dict for storing model and metrics\nres = {}\n\n# Fit the models and performing the prediction\nfor model_name, model in models.items():\n    model.fit(X_train, y_train)\n    y_hat = model.predict(X_test)\n    wmae = WMAE(X_test.IsHoliday, y_test, y_hat)\n    r2 = r2_score(y_test, y_hat)\n    res[model_name] = (model, wmae, r2)\n    print(f'{model_name}:')\n    print(f'WMAE = {wmae} and R-square = {r2}')","46b64e09":"(pd.DataFrame(res['RandomForestRegressor'][0].feature_importances_, index=X.columns, columns=['Importance'])\n.sort_values('Importance', ascending=False))","b82024aa":"# Running again without the columns: Type_C, Year, Holiday_Labor_Day and Holiday_Super_Bowl.\nX = train[['Store', 'Size', 'Dept', 'Month', 'IsHoliday', 'Temperature', 'Fuel_Price', \n          'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', \n          'Type_A', 'Type_B', 'Holiday_Christmas', 'Holiday_Thanksgiving']]\ny = train[y_target]\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n# Set the models\nmodels = {\n    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=1),\n    'extraTreesRegressor': ExtraTreesRegressor(n_estimators=100, max_features='auto', random_state=1),\n    'RandomForestRegressor': RandomForestRegressor(n_estimators=100, random_state=1)\n}\n\n# Dict for storing model and metrics\nres = {}\n\n# Fit the models and performing the prediction\nfor model_name, model in models.items():\n    model.fit(X_train, y_train)\n    y_hat = model.predict(X_test)\n    wmae = WMAE(X_test.IsHoliday, y_test, y_hat)\n    r2 = r2_score(y_test, y_hat)\n    res[model_name] = (model, wmae, r2)\n    print(f'{model_name}:')\n    print(f'WMAE = {wmae} and R-square = {r2}')","3d246470":"# Order values by store, department and date\ntest = test.sort_values(['Store','Dept','Date']).reset_index(drop=True)\n\n# Get Id as requested by the competition\ntest['Id'] = (test[['Store','Dept','Date']]\n              .astype('str')\n              .apply('_'.join, axis=1)\n)","abdb5c01":"# Select variables\nX = train[['Store', 'Size', 'Dept', 'Week', 'Month', 'Year', 'IsHoliday', 'Temperature', 'Fuel_Price', \n          'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', \n          'Type_A', 'Type_B', 'Type_C', 'Holiday_Christmas', 'Holiday_Labor Day',\n          'Holiday_Super Bowl', 'Holiday_Thanksgiving']]\ny = train[y_target]\n\n# Full Model\nrf = RandomForestRegressor(n_estimators=100, random_state=1)\nrf.fit(X, y)\n# Predict testing data\ny_hat = rf.predict(test[X.columns])","1db76caf":"# Load sample submission\nsample_submission = pd.read_csv(f'{filepath}sampleSubmission.csv.zip', compression='zip')\n\n# Create own submission dataframe\nsubmission = pd.DataFrame({'Id': test['Id'], 'Weekly_Sales': y_hat})\n\n# Compare column names and Id row order\nprint(all(sample_submission.columns == submission.columns)) # column names\nprint(all(sample_submission.Id == submission.Id)) # Id names","da5b7866":"# Writing submission file\nsubmission.to_csv('submission.csv', index=False)","43b9c014":"By looking at the explanatory analysis above, it seems clear there's no linear relationship between the target (weekly sales) and the predictor variables. Therefore, linear regression must not be useful for this data. So I'll try a set of methods that can be used both to classification and regression based on [Decision Trees (DTs)](https:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree).","7885a751":"<a id=\"ch0\"><\/a>\n# Settings & libraries\n\nFirst, let's load some libraries to analyze the data.","d7c23af3":"We can see they've been successfully loaded by using the head() function by printing the first lines: ","17aaa282":"These graphics allow us to draw some preliminary conclusions:\n- Holidays do not influence much the weekly sales according to the store, department or type;\n- The holiday in November (Thanksgiving) has more influence than others, especially the one from December (Christmas), that showed a negative correlation;\n- Still, for each year, there were more weekly sales on holidays.","28c64d92":"<a id=\"ch2_1\"><\/a>\n### Variable's correlation\n\nWe can split the 'Type' and 'Holiday' variables so that we can use them for showing for plotting, as well as in the models. By default, Pandas set the new dummy variables as numerical.","2845ae42":"Check some basic information of the dataset, like the data type for each column:","31782c94":"Now we are ready for starting the exploratory analysis.","69904e6a":"Indeed, at least for the MarkDown 1 and 5, it seems to have a different mean depending if it's holiday or not (which statistical significance could be assessed confirmed by ANOVA).","43fd9aa7":"We can also look at grouped data by set (train and test dataset):","711be49c":"Now, let's split the date into new columns, and create another column to label the holidays.  \nIt could help us to get better insight from the data.","fd444f37":"<a id=\"ch1\"><\/a>\n# 1. Getting data\n\nData is provided as [CSV files](https:\/\/en.wikipedia.org\/wiki\/Comma-separated_values), so we can read them into a dictionary using [Pandas](https:\/\/pandas.pydata.org\/), parsing the date column as a date type.","a31ea5d3":"We can see that the most frequent store is the same for both the train and test dataset, which is good if it provides useful information for modeling. The dates are different for an obvious reason (they correspond to distinct time range).","e0380932":"Visualizing the estimated importance for each feature from the random forest model:","e82fe1a3":"Then we run the models and get the error metrics.","2d089881":"Before going further on exploring the data, let's get them together (train_data, test_data, stores, and features).","c7bf50e0":"It seems like the Store and their respective departments, type, and size, are valuable predictor to the weekly sales.","d393d081":"![](http:\/\/)In this notebook, I will be using the [Python programming language](https:\/\/www.python.org\/) to solve the problems provided in this competition. I've also worked in a solution using [R programming language](https:\/\/www.r-project.org\/), which you can access [through this link](https:\/\/www.kaggle.com\/cmcoutosilva\/zedelivery-r), although it's incomplete.\n\nThis notebook is organized in topics for facilitating understanding. It's worth to note that the \"feature engineering\" doesn't have a particular topic because I made it across different topics.\n\nYou can easily go right to the topic you are interested in by clicking in the links in summary below:","66c0adca":"I've decided to deal with missing values in the MarkDown columns by replacing them by the median per group of Type and Holiday since there is a correlation for these variables. For missing values outside these groups, I'll replace them by the overall median of the respective variable.\n\nI'll take a similar approach for filling missing values to Unemployment and CPI variables, where I'll replace the NAs by the median per Store Type. These modifications will be done on each set separately:","1955cf27":"# SUMMARY\n\n* [Settings & libraries](#ch0)\n* 1. [Getting data](#ch1)\n* 2. [Exploring data](#ch2)  \n    \\- [Variable's correlation](#ch2_1)  \n    \\- [Visualize numerical data](#ch2_2)  \n    \\- [Visualize categorical data](#ch2_3)\n* 3. [Dealing with missing data](#ch3)\n* 4. [Prediction](#ch4)\n* 5. [Submission](#ch5)\n","dc19f61f":"Let's rerun it, excluding some variables with low importance.","043b42e2":"<a id=\"ch2_2\"><\/a>\n### Visualizing numerical data\n\nDistribution plots can provide a good general idea of how the data is structured.  \nLet's take a look into the data:","1232541c":"And define a function for displaying nicely multiple dataframes.","de923861":"It's also interesting to see the relationship of the MarkDown columns with the Store Type and holidays:","16c667a5":"With this, we can take some important notes:\n- Both Weekly_Sales and Markdown columns have negative values. \n- There are columns with distinct total counts, which means **missing values**.\n\nFirst, let's update these negatives values to zero. We'll deal with the missing values later.","c7af2050":"<a id=\"ch5\"><\/a>\n# Submission","9f38ca85":"There's no much difference, so let's keep the first one, which performed a bit better. All three models showed similar results. I'll keep the RandomForest Regressor due to the reasonable tradeoff.\n\nNow, let's fit the chosen model with the full training data.","ec471fb2":"<a id=\"ch3\"><\/a>\n## Dealing with missing data","579d7e96":"<a id=\"ch2\"><\/a>\n# 2. Exploring the data\n\nLet's first take a look at the numeric variables.","fc095c12":"Since this competition was evaluated on the weighted mean absolute error (WMAE), I'll define a function for calculating this metric (although I'll analyze the R-square as well):","2274f537":"Now let's see some statistics for DateTime and categorical variables.","94f1a294":"It seems that for Markdown 1, 4, and 5, there are more weekly sales when it's no holiday.  \nWe can further examine that with the plot below.","294eee26":"Let's see how NAs are distributed on our data:","b010a9c5":"# Overview\n\n---","6fcb9053":"Now we see how the variables relate to each other:","f590de97":"Here we see an evident pattern of correlation of Markdown with Store Type (other than C) and with holidays, which we've already seen in the correlation plot before.","9a893efb":"<a id=\"ch4\"><\/a>\n# Machine Learning","ba673f2c":", then see the changes successfully applied:","e47793b3":"There are columns classified as integer or object when they could be classified as a category. Let's fix that:","2924620d":"Let's first select the target columns (target and predictor variables), drop the unwanted ones, and scale the values to get better performance of the algorithms we're running.","01035745":"The red line delimitates the training and test dataset (train dataset, the biggest one, is above the line). We can see there's no information about Weekly sales for the test dataset (which makes sense, since we're going to predict it). There are a lot of missing values in the Holiday column, which it's also expected due to the manner I made it.\n\nInterestingly, there's a lot of missing values for the Markdown columns in the training dataset. It's according to the data description: \"MarkDown data is only available after Nov 2011, and is not available for all stores all the time\". We have to deal with it, but let's first look at how many NAs are for each variable.","3bf542e4":"<a id=\"ch2_3\"><\/a>\n### Visualizing categorical data"}}