{"cell_type":{"81bdbe19":"code","a686e9fa":"code","baa703a4":"code","b386c71a":"code","4f9394d2":"code","895cbd30":"code","b111a846":"code","8668c33b":"code","4db01722":"code","f33d07da":"code","3d00a17f":"code","6426f24a":"code","f24e283f":"code","65921d1f":"code","3925a074":"code","09004ea2":"code","de354467":"code","3b449da9":"code","71e734a4":"code","d0a95b99":"code","c7a4931b":"code","687420cc":"code","675c38be":"code","f6376c81":"code","e3587a8c":"code","ce34e043":"code","25dec903":"code","b67aa0d1":"code","0e61e1a8":"code","4132b3e9":"code","627a082a":"code","04bb5407":"markdown","9b745e2a":"markdown","8c48ff67":"markdown","3cb2cafa":"markdown","e721ab01":"markdown","ebbd6511":"markdown","db2d16b1":"markdown","4b541096":"markdown","260f3ed3":"markdown","226cb87d":"markdown"},"source":{"81bdbe19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a686e9fa":"df = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = 'Latin-1', names=('target','id','date','flag','user','text'))","baa703a4":"df.head()","b386c71a":"df.info()","4f9394d2":"df.describe()","895cbd30":"import seaborn as sns","b111a846":"##lets look at the occurence of each target value\nsns.countplot(x='target',data=df)\n##no neutral (target=2) tweets","8668c33b":"sns.countplot(x='flag',data=df)","4db01722":"#check for null values\ndf.isnull().sum()\n#no null values","f33d07da":"#converting the 4 in target column to 1 to denote the value 'positive'\ndf['target'] = df['target'].apply(lambda x: 1 if x==4 else x)","3d00a17f":"df['target'].unique()","6426f24a":"#drop the irrelevant columns that will not be used in sentiment analysis\ndf.drop(['date','flag','user'], axis=1, inplace=True)\ndf.drop('id', axis=1, inplace=True)\n","f24e283f":"df.head()","65921d1f":"import nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize","3925a074":"import re\nurlPattern = r'(http[^ ]*|www\\.[^ ]*)'\nuserPattern = '@[^\\s]+'","09004ea2":"#creating instance of WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()","de354467":"\ndef text_process(mess):\n    \"\"\"\n    Function takes in a string of text, then performs the following:\n    1. Remove all URL & users\n    2. Remove all punctuation\n    3. Join characters again to form words & full sentence\n    4. Tokenize to split sentence into words\n    5. Remove all stopwords\n    6. Lemmatization to convert words to its base form\n    7. Returns a list of the cleaned text\n    \"\"\"\n    # remove url & users\n    cleaned = re.sub(urlPattern,'',mess)\n    \n    cleaned = re.sub(userPattern,'',cleaned)\n    \n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in cleaned if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    #Tokenize: Split the sentence into words\n    token = word_tokenize(nopunc)\n    \n    # Now just remove any stopwords\n    words= [word for word in token if word.lower() not in stopwords.words('english')]\n\n    #Lemmatization\n    final_words = [lemmatizer.lemmatize(w) for w in words]\n    \n    return final_words","3b449da9":"df['text'].head(30)","71e734a4":"df['text'].head(30).apply(text_process)","d0a95b99":"from sklearn.feature_extraction.text import CountVectorizer","c7a4931b":"bow_transformer = CountVectorizer(analyzer=text_process).fit(df['text'])","687420cc":"text_bow = bow_transformer.transform(df['text'])","675c38be":"from sklearn.feature_extraction.text import TfidfTransformer","f6376c81":"tfidf_transformer = TfidfTransformer().fit(text_bow)\ntext_tfidf = tfidf_transformer.transform(text_bow)","e3587a8c":"from sklearn.model_selection import train_test_split\nX=text_tfidf\ny=df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)","ce34e043":"from sklearn.naive_bayes import ComplementNB","25dec903":"naive_model = ComplementNB().fit(X_train,y_train)","b67aa0d1":"predictions = naive_model.predict(X_test)","0e61e1a8":"from sklearn.metrics import confusion_matrix,classification_report","4132b3e9":"print(confusion_matrix(predictions,y_test))","627a082a":"print(classification_report(predictions,y_test))","04bb5407":"WEIGHTING AND NORMALIZATION USING TF-IDF","9b745e2a":"Creating a function to remove stopwords, punctuations, URLs and user mentions","8c48ff67":"MODEL EVALUATION","3cb2cafa":"TRAINING A MODEL USING NAIVE BAYES","e721ab01":"VECTORIZATION","ebbd6511":"PREPROCESSING","db2d16b1":"DATA EXPLORATION","4b541096":"Let's do a quick check that the function does what it is supposed to do","260f3ed3":"TRAIN\/TEST SPLIT","226cb87d":"We will first use SciKit Learn's CountVectorizer. This model will convert a collection of text documents to a matrix of token counts. There are a lot of arguments and parameters that can be passed to the CountVectorizer. In this case we will just specify the analyzer to be our own previously defined function: text_process"}}