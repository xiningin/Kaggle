{"cell_type":{"8e126123":"code","8d6ec995":"code","8094ffed":"code","8628ff30":"code","38602b6a":"code","50f8cd00":"code","5d51847c":"code","2a56730d":"code","dbaa342e":"code","e7e15fce":"code","82366f69":"code","6751f091":"code","92fd8128":"code","03f7e6b4":"code","d8f02d16":"code","b7595432":"code","f1b7fe7d":"markdown","2ea32c7c":"markdown","1c6016da":"markdown","ea529c17":"markdown","8e5054eb":"markdown","7e6a8352":"markdown","1b8edb09":"markdown","5a791fac":"markdown","ee0c0cd4":"markdown","fc8ff7cd":"markdown"},"source":{"8e126123":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport seaborn as sns\n%matplotlib inline\n\n# - Sklearn\nfrom sklearn.model_selection import train_test_split\n\n# - Keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.utils import np_utils\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom keras.optimizers import Adam, RMSprop\n\n\nimport glob\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8d6ec995":"# def read_images(img_paths, img_height=image_size, img_width=image_size):\n#     imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]\n#     img_array = np.array([img_to_array(img) for img in imgs])\n#     return(img_array)\n\ndef read_pix(jpg_dir):\n    filenames = glob.glob(os.path.join(jpg_dir, '*.jpg'))\n    img_array = np.zeros((len(filenames), 32, 32, 3), dtype=int)  # images as np.array\n    img_index = []  # image filenames in correct order\n    for idx, filename in enumerate(filenames):\n        im_tmp = matplotlib.image.imread(filename)\n        img_array[idx, :, :, :] = np.array(im_tmp, dtype=int)\n        img_index.append(os.path.basename(filename))\n    return img_array, img_index\n\ndef prepare_data(img_array, img_index, train_response, test_size=0.25):\n    \n    # - Response\n    y_train_series, y_test_series = train_test_split(train_response, \n                                                     shuffle=True,\n                                                     random_state=12,\n                                                     test_size=test_size)\n    y_train = y_train_series.values[:, np.newaxis]\n    y_test = y_test_series.values[:, np.newaxis]\n\n    # - Predictors\n    train_index = [img_index.index(idx) for idx in y_train_series.index]\n    test_index = [img_index.index(idx) for idx in y_test_series.index]\n    x_train = img_array[train_index, :, :, :]\n    x_test = img_array[test_index, :, :, :]\n\n    return x_train, y_train, x_test, y_test, y_train_series, y_test_series\n\ndef simple_oversample(x, y, oversample_class, oversample_factor=3):\n    new_x = x.copy()\n    new_y = y.copy()\n    oversample_mask = np.ravel(y == oversample_class)\n    oversample_x = x[oversample_mask, :, :, :]\n    oversample_y = y[oversample_mask, :]\n    for i in range(oversample_factor):\n        new_x = np.concatenate([new_x, oversample_x], axis=0)\n        new_y = np.concatenate([new_y, oversample_y], axis=0)\n    return new_x, new_y","8094ffed":"train_labels = pd.read_csv('..\/input\/train.csv')  # image labels in training data\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')  # example submission\n\n# - Read training data (images as np.arrays, associated labels as list)\ntrain_img_array, train_img_index = read_pix('..\/input\/train\/train')\ntest_img_array, test_img_index = read_pix('..\/input\/test\/test')\n    \n# - Put labels into pd.Series object\ntrain_response = train_labels['has_cactus']\ntrain_response.index = train_labels['id']\ntrain_response = train_response.loc[train_img_index]","8628ff30":"# - Image labels\nprint('Image Labels')\nprint(train_labels.head(2))\nprint('\\n')\n\n# - Image labels\nprint('Image Labels (Series)')\nprint(train_response.head(2))\nprint('\\n')\n\n# - Submission format\nprint('Submission Example')\nprint(sample_submission.head(2))","38602b6a":"print('Some examples')\n\nnp.random.seed(27)\ninspect = np.random.randint(low=0, high=train_img_array.shape[0], size=9)\n# inspect = np.arange(9)\nfor i in range(9):\n    pic_id = inspect[i]\n    plt.subplot(330 + 1 + i)\n    plt.imshow(train_img_array[pic_id].astype(int))\nplt.show()\n\nprint('Image Labels')\ntrain_response.loc[np.array(train_img_index)[inspect]]","50f8cd00":"count_classes = pd.crosstab(train_response, columns='count')\ncount_classes.index = ['no cactus', 'cactus']\ncount_classes.plot(kind='bar', legend=False)\nplt.xticks(rotation=0)\nplt.title('Number of instances in training data')\nplt.show()\nratio = count_classes.loc['cactus', 'count'] \/ count_classes.loc['no cactus', 'count']\nprint('Ratio (cactus vs. no cactus): {:.2f}'.format(ratio))","5d51847c":"# - Split into training and validation set (25% validation)\nx_train, y_train, x_test, y_test, y_train_series, y_test_series = prepare_data(train_img_array, train_img_index, train_response)\n\ncount_classes_train = pd.crosstab(y_train_series, columns='count')\ncount_classes_train.index = ['no cactus', 'cactus']\ncount_classes_train.plot(kind='bar', legend=None)\nplt.xticks(rotation=0)\nplt.title('Number of instances in train data')\nplt.show()\nratio = count_classes_train.loc['cactus', 'count'] \/ count_classes_train.loc['no cactus', 'count']\nprint('Ratio (cactus vs. no cactus): {:.2f}'.format(ratio))","2a56730d":"print('Some images from the training set')\n\nnp.random.seed(26)\ninspect = np.random.randint(low=0, high=y_train.shape[0], size=9)\n# inspect = np.arange(9)\nfor i in range(9):\n    pic_id = inspect[i]\n    plt.subplot(330 + 1 + i)\n    plt.imshow(x_train[pic_id].astype(int))\nplt.show()\n\nprint('Accompanying labels')\n\ny_train[inspect]","dbaa342e":"# - Parameters\nbatch_size = 128\nnum_classes = 2\n\n# Generators\ndatagen = ImageDataGenerator(\n    rotation_range=90,\n    width_shift_range=.1,\n    height_shift_range=.1,\n    shear_range=.2,\n    zoom_range=.1,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest',\n)\n\n# - Prepare and oversample data\nx_train, y_train, x_test, y_test, y_train_series, y_test_series = prepare_data(train_img_array, train_img_index, train_response, test_size=0.2)\nx_train, y_train = simple_oversample(x_train, y_train, oversample_class=0, oversample_factor=2)\nx_test, y_test = simple_oversample(x_test, y_test, oversample_class=0, oversample_factor=2)\n\nprint(\"Number of 'no cactus' samples in y_train: {}\".format((y_train == 0).sum()))\nprint(\"Number of 'cactus' samples in y_train: {}\".format((y_train == 1).sum()))","e7e15fce":"print('Some images from the oversampled training set')\n\nnp.random.seed(26)\ninspect = np.random.randint(low=0, high=y_train.shape[0], size=9)\n# inspect = np.arange(9)\nfor i in range(9):\n    pic_id = inspect[i]\n    plt.subplot(330 + 1 + i)\n    plt.imshow(x_train[pic_id].astype(int))\nplt.show()\n\nprint('Accompanying labels')\n\ny_train[inspect]","82366f69":"# Normalize inputs\nx_train = x_train \/ 255\nx_test = x_test \/ 255\n\n# # One hot encoding of response\n# y_train = np_utils.to_categorical(y_train, num_classes=num_classes)\n# y_test = np_utils.to_categorical(y_test, num_classes=num_classes)\n\ntrain_generator = datagen.flow(x_train, y_train)\ntest_datagen = ImageDataGenerator()\nvalidation_generator = test_datagen.flow(x_test, y_test)","6751f091":"def convnet_model():\n    # create model\n    model = Sequential()\n    model.add(Conv2D(32, (5, 5), input_shape=(32, 32, 3), activation='relu'))\n    model.add(Conv2D(32, (5, 5), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(64, (5, 5), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile model\n#     model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['accuracy'])\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-4, decay=1e-6), metrics=['accuracy'])\n    return model","92fd8128":"# build model\nmodel = convnet_model()\n# fit model\n# hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=200, verbose=2)\nhist = model.fit_generator(train_generator,\n                           steps_per_epoch=np.ceil(x_train.shape[0] \/ 32),\n                           epochs=120,\n                           validation_data=validation_generator,\n                           validation_steps=np.ceil(x_test.shape[0] \/ 32)\n                   )\n# final evaluation\nscores = model.evaluate(x_test, y_test, verbose=0)\nprint('CNN error: {:.2f}'.format(100-scores[1]*100))","03f7e6b4":"# summarize history for accuracy\nplt.plot(hist.history['acc'])\nplt.plot(hist.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","d8f02d16":"probability = model.predict_proba(test_img_array\/255)\nres = pd.DataFrame({\n    'id': test_img_index,\n    'has_cactus': probability.ravel(),\n})\nres.to_csv('submission.csv', index=False)","b7595432":"res.head()","f1b7fe7d":"### Description\n\nThis dataset contains a large number of 32 x 32 thumbnail images containing aerial photos of a columnar cactus (Neobuxbaumia tetetzo). Kaggle has resized the images from the original dataset to make them uniform in size. The file name of an image corresponds to its id.\n\nYou must create a classifier capable of predicting whether an images contains a cactus.\nFiles\n\n    train\/ - the training set images\n    test\/ - the test set images (you must predict the labels of these)\n    train.csv - the training set labels, indicates whether the image has a cactus (has_cactus = 1)\n    sample_submission.csv - a sample submission file in the correct format","2ea32c7c":"# Modeling","1c6016da":"### Read Data","ea529c17":"# Functions","8e5054eb":"## Prepare data","7e6a8352":"# Prepare submission","1b8edb09":"### Exploratory Data Analysis","5a791fac":"### Inspect data","ee0c0cd4":"**Imbalanced dataset:** 3 times more often 'cactus' than 'no cactus'\n\n**Requires augmentation\/oversampling of 'no cactus' class**","fc8ff7cd":"## ConvNet"}}