{"cell_type":{"7f3dcd6a":"code","d279ce07":"code","9ad05eb2":"code","59536333":"code","2c69aa32":"code","f6801fd5":"code","b7d473ca":"code","4806f1c0":"code","27f8970e":"code","9acc5524":"code","c23dd777":"code","39a64d96":"code","a866d084":"code","71148af4":"code","752e6e00":"code","750b69e1":"code","05b3be93":"code","7d13ee6a":"code","df3f9c14":"code","7cb8c49f":"code","0d06ce70":"code","30008cd5":"code","04af7fa6":"code","ed2d339b":"code","f63faf31":"code","17fa95d9":"code","a5d5abe5":"code","e2803e14":"code","3925479c":"code","9ea5a93b":"code","8851d52c":"code","41719b00":"code","3eb9db64":"code","eca73005":"code","b846ff51":"code","7f748be7":"code","c7a68499":"code","5db4580f":"code","ac537835":"code","d7cc929b":"code","3832d8fa":"code","16cbc9ee":"code","4b4fecc6":"code","adb5e68b":"code","0e220740":"code","38106fd0":"code","6020c3d7":"code","031ebbb2":"code","25945c1b":"code","0a2a06ce":"code","53fa9120":"code","e9e28058":"code","e2619110":"code","d08b5fd4":"code","743432eb":"code","f86d2be7":"code","1a1aadb6":"code","71924d36":"code","ec68de4b":"code","0a91dfee":"code","f942bd9e":"code","0f9f6e7d":"code","9ed684e8":"code","69bd20d6":"code","c4561611":"code","5add9384":"code","fe2c9075":"code","9455d55f":"code","11b7c84a":"markdown","34465bdb":"markdown","b58c7da1":"markdown","4b3fecd2":"markdown","14df1f63":"markdown","d85ea9da":"markdown","4dd5eae9":"markdown","da480b9e":"markdown","fc39bea6":"markdown","8f429f2b":"markdown","a8f9e5dc":"markdown","9a9b3e0d":"markdown","689a99fe":"markdown","1bbb2072":"markdown","f23844b3":"markdown","0f7a511a":"markdown","b8caf4f2":"markdown","6a7eee55":"markdown","ff72f891":"markdown","b3976080":"markdown","80b95cfd":"markdown","939471c3":"markdown","15e96560":"markdown","fe430e51":"markdown","9497f714":"markdown","9146f665":"markdown","8825391a":"markdown","b34152e1":"markdown","545bef40":"markdown","09344cb3":"markdown"},"source":{"7f3dcd6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d279ce07":"test = pd.read_csv('\/kaggle\/input\/testdata\/Test_BNBR.csv')\ntrain = pd.read_csv('\/kaggle\/input\/encoded-train\/encoded_train.csv')","9ad05eb2":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nimport gc\ngc.collect()","59536333":"!pip install pretrainedmodels\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n!pip install fastai==1.0.52\nimport fastai\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\n\nfrom torchvision.models import *\nimport pretrainedmodels\n\nfrom utils import *\nimport sys\n\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback","2c69aa32":"%%bash\npip install pytorch-pretrained-bert","f6801fd5":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    \"bert-base-uncased\",\n)","b7d473ca":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","4806f1c0":"from sklearn.model_selection import train_test_split","27f8970e":"DATA_ROOT = Path(\"..\") \/ \"input\"\n\ntrain, test = [pd.read_csv(DATA_ROOT \/ fname) for fname in [\"train\", \"test\"]]\ntrain, val = train_test_split(train, shuffle=True, test_size=0.2, random_state=42)","9acc5524":"train.head()","c23dd777":"test.head()","39a64d96":"val.head()","a866d084":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","71148af4":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=256), pre_rules=[], post_rules=[])","752e6e00":"label_cols = [\"Depression\", \"Alcohol\", \"Suicide\", \"Drugs\"]\n\ndatabunch_1 = TextDataBunch.from_df(\".\", train, val, \n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=\"text\",\n                  label_cols=label_cols,\n                  bs=32,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","750b69e1":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos\/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]","05b3be93":"class BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)","7d13ee6a":"# this will produce a virtually identical databunch to the code above\ndatabunch_2 = BertDataBunch.from_df(\".\", train_df=train, valid_df=val,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  text_cols=\"text\",\n                  label_cols=label_cols,\n                  bs=32,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","df3f9c14":"path=Path('..\/input\/')","7cb8c49f":"databunch_2.show_batch()","0d06ce70":"databunch_1.show_batch()","30008cd5":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM\nbert_model_class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)","04af7fa6":"loss_func = nn.BCEWithLogitsLoss()","ed2d339b":"acc_02 = partial(accuracy_thresh, thresh=0.5)","f63faf31":"model = bert_model_class","17fa95d9":"from fastai.callbacks import *\n\nlearner = Learner(\n    databunch_1, model,\n    loss_func=loss_func, model_dir='\/temp\/model', metrics=acc_02,\n)","a5d5abe5":"def bert_clas_split(self) -> List[nn.Module]:\n    \n    bert = model.bert\n    embedder = bert.embeddings\n    pooler = bert.pooler\n    encoder = bert.encoder\n    classifier = [model.dropout, model.classifier]\n    n = len(encoder.layer)\/\/3\n    print(n)\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","e2803e14":"x = bert_clas_split(model)","3925479c":"learner.split([x[0], x[1], x[2], x[3]])  #  , x[5]","9ea5a93b":"learner.lr_find()","8851d52c":"learner.recorder.plot()","41719b00":"learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-5, 1e-4, 1e-3, 1e-2))","3eb9db64":"learner.save('head')\nlearner.load('head')","eca73005":"learner.freeze_to(-2)\nlearner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-5, 1e-4, 1e-3, 1e-2))","b846ff51":"learner.save('head-2')\nlearner.load('head-2')","7f748be7":"learner.unfreeze()\nlearner.lr_find()\nlearner.recorder.plot(suggestion=True)","c7a68499":"learner.fit_one_cycle(2, slice(5e-6, 5e-5), moms=(0.8,0.7), pct_start=0.2, wd =(1e-5, 1e-4, 1e-3, 1e-2))","5db4580f":"text = 'I feel alone and unwanted by people around me'\nlearner.predict(text)","ac537835":"text = 'Lonely and happiness'\nlearner.predict(text)","d7cc929b":"text = 'I feel sad and lost'\nlearner.predict(text)","3832d8fa":"text = 'Effects of alcohol on my body health'\nlearner.predict(text)","16cbc9ee":"src_lm = ItemLists(path, TextList.from_df(train, path=\".\", cols = \"text\"), \n                   TextList.from_df(val, path=\".\", cols = 'text'))","4b4fecc6":"data_lm = src_lm.label_for_lm().databunch(bs=32)","adb5e68b":"data_lm.show_batch()","0e220740":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, model_dir=\"\/temp\/model\")","38106fd0":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","6020c3d7":"learn.fit_one_cycle(1, max_lr=slice(5e-4, 5e-3), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-5, 1e-4, 1e-3))","031ebbb2":"learn.save('fit_head')\nlearn.load('fit_head')","25945c1b":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","0a2a06ce":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4,  1e-2))","53fa9120":"learn.save('fine-tuned')\nlearn.load('fine-tuned')\nlearn.save_encoder('fine-tuned')","e9e28058":"TEXT = \"He is a piece of\"\nN_WORDS = 10\nN_SENTENCES = 2","e2619110":"print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","d08b5fd4":"src_clas = ItemLists(path, TextList.from_df( train, path=\".\", cols=\"comment_text\", vocab = data_lm.vocab),\n                    TextList.from_df( val, path=\".\", cols=\"comment_text\", vocab = data_lm.vocab))","743432eb":"data_clas = src_clas.label_from_df(cols=label_cols).databunch(bs=32)","f86d2be7":"data_clas.show_batch()","1a1aadb6":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, model_dir='\/temp\/model', metrics=acc_02, loss_func=loss_func)\nlearn.load_encoder('fine-tuned')","71924d36":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","ec68de4b":"learn.fit_one_cycle(2, max_lr=slice(1e-3, 1e-2), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","0a91dfee":"learn.save('first-head')\nlearn.load('first-head')","f942bd9e":"learn.freeze_to(-2)\nlearn.fit_one_cycle(2, slice(5e-2\/(2.6**4),5e-2), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","0f9f6e7d":"learn.save('second')\nlearn.load('second')","9ed684e8":"learn.freeze_to(-3)\nlearn.fit_one_cycle(2, slice(5e-2\/(2.6**4),5e-2), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","69bd20d6":"learn.save('third')\nlearn.load('third')","c4561611":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","5add9384":"learn.fit_one_cycle(2, slice(1e-4\/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","fe2c9075":"learn.predict('she is so sweet')","9455d55f":"learn.predict('you are pathetic piece of shit')","11b7c84a":"Before we move further, lets have a look at the Data on which we have to work.\n\nWe will split the train data into two parts: Train, Validation. However, for the purpose of this project, we will not be using Test Data","34465bdb":"Loss function to be used is Binary Cross Entropy with Logistic Losses","b58c7da1":"Now, we can create our Databunch. Important thing to note here is to use BERT Tokenizer, BERT Vocab. And to and put include_bos and include_eos as False as Fastai puts some default values for these","4b3fecd2":"# Classification Model","14df1f63":"# BERT Model","d85ea9da":"Now, lets create learner function","4dd5eae9":"# Fastai - ULMFiT","da480b9e":"This is awesome!\n\nWith few number of epochs, we are able to get the accuracy of around 98% on this multi-label classification task.\n\nNow, lets see how does Fastai ULMFiT fare on this task","fc39bea6":"In this project, we will use BasicNeedsBasicRights Comments dataset which has categorized each text item into 4 classes: \n\n1. Depression\n2. Alcohol\n3. Suicide\n4. Drugs\n\nThis is a **multi-label text classification challenge**.","8f429f2b":"In this section, we will import Fastai libraries and few other important libraries for our task","a8f9e5dc":"In this notebook, I want to use two state of the art Natural Language Processing (NLP) techniques which have sort of revolutionalized the area of NLP in Deep Learning.\n\nThese techniques are as follows:\n\n1. BERT (Deep Bidirectional Transformers for Language Understanding)\n2. Fastai ULMFiT (Universal Language Model Fine-tuning for Text Classification)\n\nBoth these techniques are very advanced and very recent NLP techniques (BERT was introduced by Google in 2018). Both of them incorporate the methods of Transfer Learning which is quite cool and are pre-trained on large corpuses of Wikipedia articles. I wanted to compare the overall performance of these two techniques.\n\nI really like using Fastai for my deep learning projects and can't thank enough for this amazing community and our mentors - Jeremy & Rachael for creating few wonderful courses on the matters pertaining to Deep Learning. Therefore one of my aims to work on this project was to **integrate BERT with Fastai**. This means power of BERT combined with the simplicity of Fastai. It was not an easy task especially implementing Discriminative Learning Rate technique of Fastai in BERT modelling. \n\nIn my project, below article helped me in understanding few of these integration techniques and I would like to extend my gratidue to the writer of this article:\n\n[https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/](http:\/\/)\n\n","9a9b3e0d":"## Data","689a99fe":"## Overall objective","1bbb2072":"Considering this is a multi-label classification problem, we cant use simple accuracy as metrics here. Instead, we will use accuracy_thresh with threshold of 25% as our metric here.","f23844b3":"# Importing Libraries & Data Preparation","0f7a511a":"Important thing to remember in the Language Model is that we train it without label. The basic objective by training language model is to predict the next sentence \/ words in a sequence of text.","b8caf4f2":"This will have two parts:\n\n1. Training the Language Model\n2. Training the Classifier Model","6a7eee55":"BERT has several flavours when it comes to Tokenization. For our modelling purposes, we will use the most common and standard method named as \"bert-case-uncased\".\n\nWe will name this as bert_tok","ff72f891":"Below code will help us in splitting the model into desirable parts which will be helpful for us in Discriminative Learning i.e. setting up different learning rates and weight decays for different parts of the model.","b3976080":"# Project Description","80b95cfd":"Now, we will unfreeze last two last layers and train the model again","939471c3":"## Language Model\n","15e96560":"We will now see our model's prediction power","fe430e51":"As mentioned in the article in first section, we will change the tokenizer of Fastai to incorporate BertTokenizer. One important thing to note here is to change the start and end of each token with [CLS] and [SEP] which is a requirement of BERT.","9497f714":"Let's split the model now in 6 parts","9146f665":"Both Databunch_1 and Databunch_2 can be used for modelling purposes. In this project, we will be using Databunch_1 which is easier to create and use.","8825391a":"Let's import Huggingface's \"pytorch-pretrained-bert\" model (this is now renamed as pytorch-transformers)\n\n[https:\/\/github.com\/huggingface\/pytorch-transformers](http:\/\/)\n\nThis is a brilliant repository of few of amazing NLP techniques and already pre-trained.","b34152e1":"Alternatively, we can pass our own list of Preprocessors to the databunch (this is effectively what is happening behind the scenes)","545bef40":"In following code snippets, we need to wrap BERT vocab and BERT tokenizer with Fastai modules","09344cb3":"We will now unfreeze the entire model and train it"}}