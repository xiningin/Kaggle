{"cell_type":{"34cb0fb7":"code","9583a6a0":"code","cca8109d":"code","9313f7f4":"code","de344ee7":"code","639b81a1":"code","274470fe":"code","33e4c177":"code","c5a139ea":"code","14e9b0dd":"code","990117c7":"code","73d40b19":"code","f0796a93":"code","db39fe9b":"code","9ef489f4":"code","b56b7a56":"code","5a7f91cc":"code","65b5c01e":"code","70c635bb":"code","09cc6c10":"code","8a938d9f":"code","bc680a70":"code","c4ddca2c":"code","29d072df":"code","2dbe76d7":"code","da62e739":"code","53c8ce38":"code","04485bb1":"markdown","ef8e332c":"markdown","c12db11f":"markdown","f5a143a3":"markdown","1c62a63c":"markdown","d067c1f3":"markdown","78fe9f39":"markdown","c6f6ff70":"markdown","643fb312":"markdown"},"source":{"34cb0fb7":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","9583a6a0":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cca8109d":"from PIL import Image\nfrom numpy import asarray\nimport numpy as np\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy.random import randn\n# from numpy.random import randin\n\nfrom keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model,Sequential, load_model\nfrom keras.layers import Input, MaxPooling2D, GlobalAveragePooling2D,UpSampling2D,Conv2DTranspose,LeakyReLU,Dropout,Activation,Dense,Flatten,Reshape,Conv2D, BatchNormalization\n\nimport matplotlib.pyplot as plt","9313f7f4":"len(filenames)","de344ee7":"dataset_orig = np.empty((len(filenames), 64, 64,3))","639b81a1":"ind=0\nfor i in filenames:\n    image = Image.open(os.path.join(dirname, i))\n    data = asarray(image)\n    dataset_orig[ind]=data\n    ind+=1","274470fe":"dataset_orig.shape","33e4c177":"width, height, channel = 64, 64, 3\nnp.random.shuffle(dataset_orig)\nX=dataset_orig","c5a139ea":"X = (X - 127.5) \/ 127.5","14e9b0dd":"X.shape","990117c7":"def show_data(X, title=\"\"):\n    plt.figure(figsize=(11,11))\n    \n    i = 1\n    for img in X:\n        plt.subplot(10, 10, i)\n        plt.imshow(img.reshape((height, width,channel)))\n        plt.axis('off')\n        i+=1\n        if i>100: break\n\n    plt.suptitle(title, fontsize = 25)\n    plt.show()\n    \nshow_data(X, title=\"Original Dataset\")","73d40b19":"gen_optimizer = Adam(0.0002, 0.5)\ndisc_optimizer = Adam(0.0002, 0.5)\nnoise_dim = 100","f0796a93":"def buildGenerator():\n    model = Sequential()\n\n    model.add(Dense(8*8*256, use_bias=False, input_shape=(100,)))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n\n    model.add(Reshape((8, 8, 256)))\n    assert model.output_shape == (None, 8, 8, 256) # Note: None is the batch size\n\n    model.add(Conv2DTranspose(256, (4,4), strides=(2, 2), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n    \n    model.add(Conv2DTranspose(256, (4,4), strides=(2, 2), padding='same', use_bias=False))\n#     assert model.output_shape == (None, 32, 32, 64)\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n    \n    model.add(Conv2DTranspose(256, (4,4), strides=(2, 2), padding='same', use_bias=False))\n#     assert model.output_shape == (None, 32, 32, 64)\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n\n\n    model.add(Conv2DTranspose(3, (3,3), strides=(1,1), padding='same', use_bias=False, activation='sigmoid'))\n#     assert model.output_shape == (None, 64, 64, 3)\n    \n    return model","db39fe9b":"generator = buildGenerator()\ngenerator.summary()\n# plot the model\n# plot_model(generator, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)","9ef489f4":"def buildDiscriminator():\n    model = Sequential()\n    \n    model.add(Conv2D(256, (4,4), strides=(2, 2), padding='same', input_shape=(width, height, channel)))\n    model.add(LeakyReLU())\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(128, (4,4), strides=(2, 2), padding='same'))\n    model.add(LeakyReLU())\n    model.add(Dropout(0.3))\n    \n    model.add(Conv2D(64, (4,4), strides=(2, 2), padding='same'))\n    model.add(LeakyReLU())\n    model.add(Dropout(0.3))\n    \n    model.add(Conv2D(32, (4,4), strides=(2, 2), padding='same'))\n    model.add(LeakyReLU())\n    model.add(Dropout(0.3))\n    \n    model.add(Conv2D(16, (4,4), strides=(2, 2), padding='same'))\n    model.add(LeakyReLU())\n    model.add(Dropout(0.3))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer=disc_optimizer)\n    return model","b56b7a56":"discriminator = buildDiscriminator()\ndiscriminator.summary()\n# plot the model\n# plot_model(discriminator, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)","5a7f91cc":"noise = Input(shape=(noise_dim,))\nfake_data = generator(noise)\ndiscriminator.trainable = False\noutput = discriminator(fake_data)\ngan = Model(noise, output)\ngan.compile(loss='binary_crossentropy', optimizer=gen_optimizer)","65b5c01e":"gan.summary()\n# plot the model\nplot_model(gan, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)","70c635bb":"fixed_noise = np.random.normal(0, 1, size=(100, noise_dim))","09cc6c10":"def show_generated_fabric(title, epoch):\n    imgs = generator.predict(fixed_noise)\n    imgs = 0.5 * imgs + 0.5\n    plt.figure(figsize=(11,11))\n    \n    i = 1\n    for img in imgs:\n        plt.subplot(10, 10, i)\n        plt.imshow(img.reshape((height,width,channel)))\n        plt.axis('off')\n        i+=1\n    plt.suptitle(title, fontsize = 25)\n    plt.savefig(\"img_\"+str(epoch+1)+\".png\", transparent=True)\n    plt.show()","8a938d9f":"epochs = 3000\nbatch_size = 128\nsteps_per_epoch = len(X)\/\/batch_size","bc680a70":"for epoch in range(epochs):\n    for batch in range(steps_per_epoch):\n        input_gen = np.random.normal(0, 1, size=(batch_size, noise_dim))\n        fake_data = generator.predict(input_gen)\n        \n        real_data = X[np.random.randint(0, X.shape[0], size=batch_size)]\n        real_data = real_data.reshape((batch_size, width, height, channel))\n        \n        input_disc = np.concatenate((real_data, fake_data))\n\n        label_disc = np.zeros(2*batch_size)\n        label_disc[:batch_size] = 0.9\n        label_disc[batch_size:] = 0.1\n        loss_disc = discriminator.train_on_batch(input_disc, label_disc)\n\n        label_gen = np.ones(batch_size)\n        loss_gen = gan.train_on_batch(input_gen, label_gen)\n\n    print(\"epoch: \", epoch)\n    print(\"discriminator loss: \", loss_disc)\n    print(\"generator loss: \", loss_gen)\n    print(\"-\"*80)\n    \n    if (epoch+1) % 20 == 0:\n        show_generated_fabric(\"Generated Fabric\", epoch)\n        filename = 'generator_model_%03d.h5' % (epoch+1)\n        generator.save(filename)","c4ddca2c":"!zip generated_pics.zip *.png","29d072df":"!zip models.zip *.h5","2dbe76d7":"model = load_model('generator_model_2000.h5')\n\nfixed_noise = np.random.normal(0, 1, size=(100, noise_dim))\nimgs = model.predict(fixed_noise)\nimgs = 0.5 * imgs + 0.5\nplt.figure(figsize=(11,11))\ni = 1\nfor img in imgs:\n    plt.subplot(10, 10, i)\n    plt.imshow(img.reshape((height,width,channel)))\n    plt.axis('off')\n    i+=1\nplt.show()","da62e739":"model = load_model('generator_model_3000.h5')","53c8ce38":"fixed_noise = np.random.normal(0, 1, size=(100, noise_dim))\nimgs = model.predict(fixed_noise)\nimgs = 0.5 * imgs + 0.5\nplt.figure(figsize=(11,11))\ni = 1\nfor img in imgs:\n    plt.subplot(10, 10, i)\n    plt.imshow(img.reshape((height,width,channel)))\n    plt.axis('off')\n    i+=1\nplt.show()","04485bb1":"For now I forked https:\/\/www.kaggle.com\/rafi03\/africanfabricgan and changed architectures of Generator and Dicriminator using the one from this article: https:\/\/towardsdatascience.com\/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b and modified it a bit.","ef8e332c":"# **Visualizing the dataset.**","c12db11f":"As for now, one conclusion - more layers don't mean better results","f5a143a3":"# Special Thanks to this github project.\n**I took several idea from here**\nhttps:\/\/github.com\/Suji04\/NormalizedNerd\/tree\/master\/Alphabet%20GAN\n","1c62a63c":"Instead of 440th I printed the last one.","d067c1f3":"# Training the model","78fe9f39":"**Making the range between [-1.1]**","c6f6ff70":"# Processing the images","643fb312":"# Defining the model"}}