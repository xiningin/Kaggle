{"cell_type":{"d7c6f202":"code","8bfe1d10":"code","8eb8a971":"code","7620f9f1":"code","97e3b983":"code","72358dfa":"code","22cecdbe":"code","d477bdb5":"code","54b96cdf":"code","369935a8":"code","ad64e4ee":"code","6e20c404":"code","de64cecf":"code","32b7587f":"code","62a590e6":"code","995c5920":"code","aa8aecd1":"code","d8fe6c4c":"code","e6a70dd2":"code","395bfc80":"code","6374fe20":"code","7a2a2672":"code","cad15ebe":"code","8a646e2b":"code","9a5fad6d":"code","4ceea493":"code","9bf4d674":"code","b58b6ba1":"code","fe69dcc7":"code","f90c528a":"code","3de996d8":"code","053305b5":"code","9633d118":"code","58181d20":"code","c93ba616":"code","a856973e":"code","7f08cb68":"code","09412bb2":"code","62eb8169":"code","a3711991":"code","b6bc3c82":"code","84eb0875":"code","81af72dd":"code","65155f77":"code","a9aac973":"code","290fcf6d":"code","1ba147b5":"code","377181a0":"code","ff25ccbf":"code","3748bbdb":"code","01899f7d":"code","685eace6":"code","da85ea41":"code","c9f8a147":"code","83019a04":"code","a22aed50":"code","2802be21":"code","037e394c":"code","1dee27fe":"code","083807eb":"code","1980b266":"code","58a0f167":"code","18551cd5":"code","3ac8f34f":"code","dce18afa":"code","be935d15":"code","a390ef11":"code","4032b234":"code","5a2202c0":"code","140a1b8c":"code","21b99870":"code","0bfdffcf":"code","054a4056":"code","45c533ed":"code","1cddae49":"code","7ce633af":"code","f2bedfe6":"code","e0da09a5":"code","fae46a9e":"markdown","778ae96e":"markdown","92ac2899":"markdown","d6941cae":"markdown","26c97b09":"markdown","ee48f3fd":"markdown","a5023405":"markdown","39244158":"markdown","16ca8896":"markdown","20c27880":"markdown","72942d44":"markdown","32b28dcb":"markdown","87ed2155":"markdown","1c8eb83b":"markdown","f0a8930f":"markdown","72b336a2":"markdown","f039ccb7":"markdown","bad7b01a":"markdown","6f2d3f8e":"markdown","8194b3e6":"markdown","a89bc7ff":"markdown","f86863a2":"markdown","78b7c811":"markdown","469627dc":"markdown","3f7d08dc":"markdown","f08dc44c":"markdown","35f3ea9f":"markdown","8c97d395":"markdown","f5ba2b17":"markdown","314f1e56":"markdown","4af38e72":"markdown","c9002c88":"markdown","458fa703":"markdown","6bc2e9e0":"markdown","a43e1753":"markdown","652ddc82":"markdown","c5f25906":"markdown","a809e661":"markdown","1ca1bc64":"markdown","db4c31ad":"markdown","1a7aef7e":"markdown"},"source":{"d7c6f202":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8bfe1d10":"# Loading Data\ntrain= pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n\n# Creating a Split for Passenger Id\ntest_ids = test['PassengerId']","8eb8a971":"\ntest = test.merge(gender_submission, on='PassengerId', how='left')","7620f9f1":"train.head()","97e3b983":"train.columns, train.shape","72358dfa":"train.info()","22cecdbe":"# train[['Survived','Pclass','Sex','SibSp','Embarked']] = train[['Survived','Pclass','Sex','SibSp','Embarked']].astype('category')\n# train","d477bdb5":"train[['Age','Fare']].describe().T","54b96cdf":"print(train['Survived'].value_counts() \/ len(train) * 100)","369935a8":"print(train['Pclass'].value_counts() \/ len(train) * 100)","ad64e4ee":"print(train['Sex'].value_counts() \/ len(train) * 100)","6e20c404":"print(train['Embarked'].value_counts() \/ len(train) * 100)","de64cecf":"train['Age'].isna().sum(), test['Age'].isna().sum()","32b7587f":"train_miss = train.isna().sum() \n\ntrain_miss[train_miss>0]","62a590e6":"(train.isna().sum()) \/ len(train) * 100","995c5920":"def missing(df):\n    na = df.isna().sum()\/ len(df)\n    na = na[na >0].sort_values()\n    return na\n    ","aa8aecd1":"missing_dat_train = missing(train)","d8fe6c4c":"plt.figure(figsize=(12,6))\nsns.barplot(x=missing_dat_train.index, y=missing_dat_train)\nplt.xticks(rotation=90)","e6a70dd2":"train[train['Embarked'].isnull()]","395bfc80":"train= train.dropna(axis=0, subset=['Embarked'])\nmissing_dat_train = missing(train)","6374fe20":"train.isnull().sum()","7a2a2672":"plt.figure(figsize=(12,6))\nsns.histplot(x='Age',data=train,kde=True)\nplt.show()","cad15ebe":"train['Age']= train.loc[:,['Age']].fillna(train.loc[:,['Age']].median())\n","8a646e2b":"train = train.drop('Cabin',axis=1)","9a5fad6d":"train.isna().sum()","4ceea493":"test_miss = test.isna().sum()\n\ntest_miss[test_miss >0]","9bf4d674":"test_prop = test.isna().sum()\/ len(test)\n\ntest_prop[test_prop>0]","b58b6ba1":"missing_dat = missing(test)","fe69dcc7":"test[test['Fare'].isnull()]","f90c528a":"test['Age'] = test['Age'].fillna(test['Age'].median())","3de996d8":"test['Fare'] = test['Fare'].fillna(test['Fare'].median())","053305b5":"test = test.drop('Cabin',axis=1)","9633d118":"test.isnull().sum()","58181d20":"train['Survived'] = train['Survived'].astype('int64')","c93ba616":"plt.figure(figsize=(12,6))\ncorr = train.iloc[:,1:].corr()\n\nsns.heatmap(corr,annot=True)\nplt.show()","a856973e":"train.corr()['Survived']","7f08cb68":"plt.figure(figsize=(12,6))\nsns.histplot(x=train.Fare ,data=train, binwidth=30,kde=True)\nplt.show()","09412bb2":"plt.figure(figsize=(12,6))\nsns.histplot(x='Age',data=train,kde=True,hue='Survived')\nplt.show()","62eb8169":"plt.figure(figsize=(12,6))\nsns.catplot(x='Survived',y='Age' ,data=train,kind='bar',ci=False)\nplt.show()","a3711991":"plt.figure(figsize=(12,6))\nsns.catplot(x='Embarked',y='Survived',data=train,kind='bar',ci=False)\nplt.show()","b6bc3c82":"plt.figure(figsize=(15,10))\nsns.catplot(x='Sex',y='Survived' ,data=train,kind='violin')\nplt.show()","84eb0875":"train_ml = train\ntest_ml = test\ntrain_ml.head()","81af72dd":"from sklearn.preprocessing import LabelEncoder\n\n# Instantiating Label Encoder\nencoder = LabelEncoder()\n\n# Instantiating Label Encoder\ncols= ['Sex','Embarked']\n\n# Fitting Label Encoder For Each Column\nfor col in cols:\n    train_ml[col]= encoder.fit_transform(train_ml[col])\n    test_ml[col]= encoder.fit_transform(test_ml[col])\n    print(encoder.classes_)","65155f77":"# Subset Dataset\n\ntrain_ml = train_ml.loc[:,['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Survived']]\ntest_ml = test_ml.loc[:,['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Survived']]\n\n# Preparing X_train, y_train\n\nX_train = train_ml.drop('Survived',axis=1)\ny_train = train_ml['Survived']\n\n# Preparing X_test, y_test\n\n\nX_test = test_ml.drop('Survived',axis=1)\ny_test = test_ml['Survived']","a9aac973":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBRFClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score,recall_score, precision_score, cohen_kappa_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n","290fcf6d":"scaler = StandardScaler()\n\nX_train.iloc[:,[2,5]] = scaler.fit_transform(X_train.iloc[:,[2,5]])\nX_test.iloc[:,[2,5]] = scaler.fit_transform(X_test.iloc[:,[2,5]])\n","1ba147b5":"# Shuffle Dataset to Ensure Random Ordering & Prevent Model From learning Pattersn related to Ordering\n# X_train, y_train= shuffle(X_train,y_train)","377181a0":"%%time \nlr = LogisticRegression(C= 0.1, penalty = 'l2', solver= 'newton-cg',random_state=12)\n\nlr.fit(X_train,y_train)","ff25ccbf":"lr_predictions= lr.predict(X_test)","3748bbdb":"Survival = ['Did Not Survive','Survived']\n# Initialise class names \nclasses = np.unique(Survival)\n\n# Display Confusion Matrix using Matplotlib\n\ncm = confusion_matrix(lr_predictions, y_test)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\",xticklabels='',yticklabels=classes\n       ,title=\"Logistic Regression Confusion Matrix\")\nplt.yticks(rotation=0)\n\nplt.show()","01899f7d":"print(classification_report(lr_predictions,y_test))","685eace6":"print(f\"The accuracy for the logistic regression model is: {accuracy_score(lr_predictions,y_test):.3f}\")\nprint(f\"The f1 score for the logistic regression model is: {f1_score(lr_predictions,y_test):.3f}\")\nprint(f\"The recall score for the logistic regression model is: {recall_score(lr_predictions,y_test):.3f}\")\nprint(f\"The precision score for the logistic regression model is: {precision_score(lr_predictions,y_test):.3f}\")\nprint(f\"The Kappa score for the logistic regression model is: {cohen_kappa_score(lr_predictions,y_test):.3f}\")","da85ea41":"from sklearn.metrics import roc_curve, auc\nfpr_3, tpr_3, _ = roc_curve(lr_predictions,y_test)\nroc_auc_3 = auc(fpr_3, tpr_3)\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr_3, tpr_3, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc_3)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","c9f8a147":"%%time\n\ndt = DecisionTreeClassifier(random_state=0)\n\ndt.fit(X_train,y_train)","83019a04":"\ndt_predictions = dt.predict(X_test)","a22aed50":"print(f\"The accuracy for the decision tree classifier model is: {accuracy_score(dt_predictions,y_test):.3f}\")\nprint(f\"The f1 score for the decision tree classifier model is: {f1_score(dt_predictions,y_test):.3f}\")\nprint(f\"The recall score for the decision tree classifier model is: {recall_score(dt_predictions,y_test):.3f}\")\nprint(f\"The precision score for decision tree classifier model is: {precision_score(dt_predictions,y_test):.3f}\")\nprint(f\"The Kappa score decision tree classifier model is: {cohen_kappa_score(dt_predictions,y_test):.3f}\")\n","2802be21":"print(classification_report(dt_predictions,y_test))","037e394c":"Survival = ['Did Not Survive','Survived']\n# Initialise class names \nclasses = np.unique(Survival)\n\n# Display Confusion Matrix using Matplotlib\n\ncm = confusion_matrix(dt_predictions, y_test)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\",xticklabels='',yticklabels=classes\n       ,title=\"Confusion Matrix\")\nplt.yticks(rotation=0)\n\nplt.show()","1dee27fe":"from sklearn.metrics import roc_curve, auc\nfpr_2, tpr_2, _ = roc_curve(dt_predictions,y_test)\nroc_auc_2 = auc(fpr_2, tpr_2)\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr_2, tpr_2, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc_2)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","083807eb":"%%time\n\nrf = RandomForestClassifier(random_state=0,n_estimators=2500)\n\nrf.fit(X_train,y_train)","1980b266":"\nrf_predictions = rf.predict(X_test)\n","58a0f167":"print(f\"The accuracy for the Random Forest classifier model is: {accuracy_score(rf_predictions,y_test):.3f}\")\nprint(f\"The f1 score for the Random Forest classifier model is: {f1_score(rf_predictions,y_test):.3f}\")\nprint(f\"The recall score for the Random Forest classifier model is: {recall_score(rf_predictions,y_test):.3f}\")\nprint(f\"The precision score for Random Forest classifier model is: {precision_score(rf_predictions,y_test):.3f}\")\nprint(f\"The Kappa score Random Forest model is: {cohen_kappa_score(rf_predictions,y_test):.3f}\")","18551cd5":"print(classification_report(rf_predictions,y_test))","3ac8f34f":"Survival = ['Did Not Survive','Survived']\n# Initialise class names \nclasses = np.unique(Survival)\n\n# Display Confusion Matrix using Matplotlib\n\ncm = confusion_matrix(rf_predictions, y_test)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\",xticklabels='',yticklabels=classes\n       ,title=\"Confusion Matrix\")\nplt.yticks(rotation=0)\n\nplt.show()","dce18afa":"from sklearn.metrics import roc_curve, auc\nfp1, tpr_1, _ = roc_curve(rf_predictions,y_test)\nroc_auc_1 = auc(fp1, tpr_1)\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fp1, tpr_1, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc_1)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","be935d15":"%%time \n\nsvc = SVC(random_state=0)\n\nsvc.fit(X_train,y_train)","a390ef11":"svc_predictions = svc.predict(X_test)","4032b234":"print(f\"The accuracy for the support vector classifier model is: {accuracy_score(svc_predictions,y_test):.4f}\")\nprint(f\"The f1 score for the support vector classifier model is: {f1_score(svc_predictions,y_test):.4f}\")\nprint(f\"The recall score for the support vector classifier model is: {recall_score(svc_predictions,y_test):.3f}\")\nprint(f\"The precision score for support vector classifier model is: {precision_score(svc_predictions,y_test):.3f}\")\nprint(f\"The Kappa score support vector classifier model is: {cohen_kappa_score(svc_predictions,y_test):.3f}\")\n","5a2202c0":"print(classification_report(svc_predictions,y_test))","140a1b8c":"Survival = ['Did Not Survive','Survived']\n# Initialise class names \nclasses = np.unique(Survival)\n\n# Display Confusion Matrix using Matplotlib\n\ncm = confusion_matrix(svc_predictions, y_test)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\",xticklabels='',yticklabels=classes\n       ,title=\"Confusion Matrix\")\nplt.yticks(rotation=0)\n\nplt.show()","21b99870":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(svc_predictions,y_test)\nroc_auc = auc(fpr, tpr)\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', label='SVM ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","0bfdffcf":"\nXGB_class = Pipeline(steps=[('XGB_RF',XGBRFClassifier(n_estimators=200,random_state=0))])","054a4056":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(svc_predictions,y_test)\nroc_auc = auc(fpr, tpr)\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', label='SVM curve (area = %0.3f)' % roc_auc)\nplt.plot(fp1, tpr_1, color='blue', label='LR curve (area = %0.3f)' % roc_auc_1)\nplt.plot(fpr_2, tpr_2, color='red', label='DT curve (area = %0.3f)' % roc_auc_2)\nplt.plot(fpr_3, tpr_3, color='green', label='RF (area = %0.3f)' % roc_auc_3)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","45c533ed":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Hyperparameter tuning for Logistic Regreesion\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\nmax_iter = [500,1000,2000]\n\n# Create a dictionary containing the grid of parameters to scan\ngrid = dict(solver=solvers,penalty=penalty,C=c_values,max_iter=max_iter)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=lr, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n# Summarise results \nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nprint('\\n')\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","1cddae49":"# Hyperparameter tuning for Support Vector Machine\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = ['scale']\n\n# Create a dictionary containing the grid of parameters to scan\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=svc, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","7ce633af":"# Hyperparameter tuning for Random Forest Classifier\nmodel = RandomForestClassifier()\nn_estimators = [10, 100, 1000]\nmax_features = ['sqrt', 'log2']\n\n# Create a dictionary and grid of parameters to scan\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=rf, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","f2bedfe6":"submission = pd.DataFrame({'PassengerId':test_ids,'Survived':svc_predictions})","e0da09a5":"submission.to_csv(\".\/submission.csv\", index=False)","fae46a9e":"# Getting Kaggle Environment Set Up","778ae96e":"### Imputing Missing Data Ages of Passengers","92ac2899":"> Drop Two Rows of Embarked Data","d6941cae":"### Decision Tree","26c97b09":"## Removing Missing Rows From Training and Test Sets\n\n**Data Quality Issues**\n\n* Return for Imputation\n* Get Histogram to estimate distribution of Missing Ages \n* Thinking about missingness in terms of whether the data is missing at random\n* Deal with missingness \n* Converting Categories into Numerical Data \n\n\n","ee48f3fd":"# Model Comparison","a5023405":"# Data Cleaning & Preparation","39244158":"**Saving Submission**","16ca8896":"# Exploring Data Analysis of Survival ","20c27880":"## Logistic Regression Hyperparameter Search","72942d44":"### Verifying How Much Missing Data is Present in Training Set","32b28dcb":"### Checking Value Counts for Passenger Class","87ed2155":"### Data Type Conversion","1c8eb83b":"### Getting Columns and Shapes","f0a8930f":"### Joining Dataset ","72b336a2":"**Creating Input and Output Variables**","f039ccb7":"### Visualising Missing Data","bad7b01a":"### Describe Numerical Features","6f2d3f8e":"### Checking Value Counts for For Place of Embarkment","8194b3e6":"### Importing Data","a89bc7ff":"### XGBoost","f86863a2":"# Data Preparation Before Modelling","78b7c811":"### Getting Information about Datatypes","469627dc":"### SVM Classifier","3f7d08dc":"### Checking Value Counts for Passengers Gender","f08dc44c":"# Data Importing & Inspection","35f3ea9f":"### Logistic Regression","8c97d395":"### Proportion of Missing Data","f5ba2b17":"## Submission","314f1e56":"> Drop Cabin Variable to reduce its impact on overall outcome","4af38e72":"### Checking Value Counts for Passengers That Survived","c9002c88":"It is clear to see that the SVM has the edge with a ``` 95.00% ``` overall model Accuracy and the worst performer was the decision tree with``` 63.00% ```. All models were run using the default hyperparameters. With further tuning ","458fa703":"# Machine Learning","6bc2e9e0":"> All missing values have been removed","a43e1753":"### Dropping Values for Missing Cabins","652ddc82":"# Hyperparameter Tuning Round","c5f25906":"## Support Vector Machine Hyperparameter Tuning ","a809e661":"### Inspecting Dataset","1ca1bc64":"### Random Forest Classifier","db4c31ad":"# Random Forest Hyperparameter Tuning","1a7aef7e":"### Getting Information about Missing Data Types "}}