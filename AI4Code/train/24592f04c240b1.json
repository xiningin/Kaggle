{"cell_type":{"5ec6c037":"code","4d480a57":"code","408d709e":"code","9cb458e8":"code","652a7b50":"code","8e54804f":"code","8645b1db":"code","f41b5674":"code","0c74167a":"code","7bf35536":"code","7adac6b6":"code","306b559d":"code","32e0bee3":"code","9c2c52c1":"code","2096f8b7":"code","f9ab1507":"code","96525cc1":"code","fffaa5c5":"code","11c9216b":"code","9be4982d":"code","b49372d3":"code","4c488aef":"code","57438b08":"code","883fcda8":"code","c4f6eafa":"code","931d3acc":"code","4a052bb6":"code","01124833":"code","ed622d3b":"code","c60e308e":"code","6a59b952":"code","d0eac334":"code","ba69312c":"code","fabaec7f":"code","4f83bd1f":"code","50c839f7":"code","dfafad24":"code","ee640a57":"code","676b9b22":"code","a754b6d0":"code","b30276db":"code","9427bbb9":"code","7b9bced5":"code","03d9c6a5":"code","8e137ef7":"code","1537a7c9":"code","1680c92b":"code","d8dcae1b":"markdown","a350b49e":"markdown","3721082b":"markdown","b24abdac":"markdown","068f6477":"markdown","3c30ff93":"markdown","5aa2b1a7":"markdown","3190cffc":"markdown","60271e55":"markdown","e0f5967c":"markdown","d9940a27":"markdown","5467361a":"markdown","d96a1bad":"markdown","24114f95":"markdown","b38e63dc":"markdown","d6454e94":"markdown","f7d7984b":"markdown","f3ff7300":"markdown","7f2d8684":"markdown","4264be55":"markdown","8c83dc04":"markdown","a53175b4":"markdown","88427c84":"markdown","b219a15d":"markdown","a9c0a0ab":"markdown","c4238ae2":"markdown","4e13adb5":"markdown","094f4776":"markdown","2c030d2e":"markdown","617bb4ba":"markdown","4a7edd77":"markdown","b453f18b":"markdown","fd044975":"markdown","401dbec7":"markdown","51a71cfc":"markdown","01bb8a13":"markdown","ac24583d":"markdown","5373a26b":"markdown","2b1b0073":"markdown","450fb19f":"markdown"},"source":{"5ec6c037":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom itertools import product\nimport sklearn\nimport scipy.sparse \nimport lightgbm \nimport gc\n\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4d480a57":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","408d709e":"sales_train = pd.read_csv('..\/input\/sales_train.csv')\nprint('sales_train')\ndisplay(sales_train.head())\n\ntest = pd.read_csv('..\/input\/test.csv')\nprint('test')\ndisplay(test.head())\n\nitems = pd.read_csv('..\/input\/items.csv')\nprint('items')\ndisplay(items.head())\n\nitem_categories = pd.read_csv('..\/input\/item_categories.csv')\nprint('item_categories')\ndisplay(item_categories.head())\n\nshops = pd.read_csv('..\/input\/shops.csv')\nprint('shops')\ndisplay(shops.head())\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nprint('sample_submission')\ndisplay(sample_submission.head())","9cb458e8":"print('sales_train')\ndisplay(sales_train.describe(include='all').T)\n\nprint('test')\ndisplay(test.describe(include='all').T)\n\nprint('items')\ndisplay(items.describe(include='all').T)\n\nprint('item_categories')\ndisplay(item_categories.describe(include='all').T)\n\nprint('shops')\ndisplay(shops.describe(include='all').T)","652a7b50":"for col in ['item_price','item_cnt_day']:\n    plt.figure()\n    plt.title(col)\n    sns.boxplot(x=sales_train[col]);\n","8e54804f":"shape0 = sales_train.shape[0] # train size before dropping values\nfor col in ['item_price','item_cnt_day']:\n    max_val = sales_train[col].quantile(.99) # get 99th percentile value\n    sales_train = sales_train[sales_train[col]<max_val] # drop outliers\n    print(f'{shape0-sales_train.shape[0]} {col} values over {max_val} removed')\n\nprint(f'new training set has {sales_train.shape[0]} records')","8645b1db":"sales_train[sales_train['item_price']<=0]","f41b5674":"sales_train=sales_train[sales_train['item_price']>0]","0c74167a":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train.loc[sales_train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_train.loc[sales_train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)","7bf35536":"print(grid.shape)\ngrid.head()","7adac6b6":"# latest month\ngrid['date_block_num'].max()","306b559d":"# append next month\ntest['date_block_num'] = 34\n# add to grid\ngrid = pd.concat([grid, test[grid.columns]], ignore_index=True, sort=False)\nprint('grid shape: ',grid.shape)\nprint('missing values:',grid.isna().sum())\n","32e0bee3":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_train.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum','trips':'size'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = sales_train.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum','trips_shop':'size'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales_train.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum','trips_item':'size'}})\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\nall_data.head()","9c2c52c1":"# median item monthly price (using median to avoid outliers)\ngb = sales_train.groupby(['date_block_num','item_id'],as_index=False).agg({'item_price':{'median_item_price':'median'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','item_id'])\n\n# make sure no na values\nprint('na median_item_price:',all_data['median_item_price'].isna().sum())\n","2096f8b7":"gb = all_data.groupby(['item_id'],as_index=False).agg({'date_block_num':{'item_first_month':'min'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(all_data, gb, how='left', on=['item_id'])","f9ab1507":"all_data['new_item'] = (all_data['date_block_num']==all_data['item_first_month']).astype(int)\nall_data['new_item'].value_counts()","96525cc1":"# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();\nall_data.head()","fffaa5c5":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols+['item_first_month'])) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\nall_data = downcast_dtypes(all_data)\ngc.collect();","11c9216b":"all_data.head(5)","9be4982d":"item_target_enc_na = .3343 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id')['target'].cumcount()\n\nall_data['item_target_enc'] = cumsum\/cumcnt\nall_data['item_target_enc'].fillna(item_target_enc_na,inplace=True)\ncorr = np.corrcoef(all_data['target'].values, all_data['item_target_enc'])[0][1]\nprint(corr)","b49372d3":"item_target_enc_na = .3343 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('shop_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('shop_id')['target'].cumcount()\n\nall_data['shop_id_enc'] = cumsum\/cumcnt\nall_data['shop_id_enc'].fillna(item_target_enc_na,inplace=True)\ncorr = np.corrcoef(all_data['target'].values, all_data['shop_id_enc'])[0][1]\nprint(corr)","4c488aef":"all_data.drop(columns='shop_id_enc',inplace=True)\nall_data.columns","57438b08":"# figure out difference between month and month\nsales_train[['date','date_block_num']].sample(5)","883fcda8":"# rule seems to be date_block_num%12+1\nsales_train['month'] = sales_train['date_block_num']%12+1\nsales_train[['date','month','date_block_num']].sample(5)\n","c4f6eafa":"# add this to all_data\nall_data['month'] = all_data['date_block_num']%12+1","931d3acc":"items.groupby('item_category_id',as_index=False)['item_id'].count().rename(columns={'item_id':'total_items'}).describe()","4a052bb6":"all_data['item_category_id'] = all_data['item_id'].map(items.set_index('item_id')['item_category_id'])","01124833":"all_data['item_category_id'].isna().sum() # no missing categories","ed622d3b":"item_target_enc_na = 0 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('item_category_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_category_id')['target'].cumcount()\n\nitem_category_id_enc = cumsum\/cumcnt\nitem_category_id_enc.fillna(item_target_enc_na,inplace=True)\nall_data['item_category_id_enc'] = item_category_id_enc\ncorr = np.corrcoef(all_data['target'].values, item_category_id_enc)[0][1]\nprint(corr)","c60e308e":"words = ' '.join(shops['shop_name']).split(' ')\nfrom collections import Counter\nc = Counter(words)\nc.most_common(10)","6a59b952":"shop_by_store = sales_train.groupby('shop_id',as_index=False)['item_cnt_day'].sum()\nshop_by_store = shop_by_store.merge(shops, on='shop_id')\nprint(shop_by_store['shop_name'].isna().sum())\nshop_by_store.head()","d0eac334":"shop_by_store['name_array'] = shop_by_store['shop_name'].str.split(' ')\ntop_words = [x for x,y in c.most_common(6)] # common words in shop name\nfor w in top_words:\n    shop_by_store[w] = shop_by_store['shop_name'].map(lambda x: 1 if w in x else 0)\n#     shop_by_store[w] = w in shop_by_store['shop_name'].str.split(' ')","ba69312c":"for w in top_words:\n    print(shop_by_store.groupby(by=w)['item_cnt_day'].mean())","fabaec7f":"top_words = top_words[0:4] # important shop features\nshops['name_array'] = shops['shop_name'].str.split(' ')\nfor w in top_words:\n    shops[w] = shops['shop_name'].map(lambda x: 1 if w in x else 0)\n\nall_data = pd.merge(all_data,shops[['shop_id']+top_words],on='shop_id',how='left') # merge\n\nprint(all_data[top_words].isna().sum()) # make sure no nulls\nall_data.head()","4f83bd1f":"leaking_columns = ['median_item_price','date_block_num','target','target_shop','target_item','trips','trips_shop','trips_item']\n\nX_train = all_data.loc[all_data['date_block_num'] < 33].drop(leaking_columns, axis=1)\nX_val = all_data.loc[all_data['date_block_num'] == 33].drop(leaking_columns, axis=1)\nX_test = all_data.loc[all_data['date_block_num'] == 34].drop(leaking_columns, axis=1)\n\ny_train = all_data.loc[all_data['date_block_num'] < 33,'target'].values\ny_val = all_data.loc[all_data['date_block_num'] == 33,'target'].values\n\n# save all_data\n# all_data.to_csv('all_data.csv',index=False)\ndel all_data\ngc.collect();\n\n","50c839f7":"X_train.tail()","dfafad24":"def validation_function(y_pred,y_true):\n    print(f'rmse before [0,20] clipping: {mean_squared_error(y_true, y_pred)}')\n    y_pred = y_pred.clip(0,20)\n    y_true = y_true.clip(0,20)\n    print(f'rmse after [0,20] clipping: {mean_squared_error(y_true, y_pred)}')\n    return \n    ","ee640a57":"# linear regression\n\nlr = LinearRegression()\nlr.fit(X_train.values, y_train)\n# (due to memory issues we train on half the data)\n# lr.fit(X_train[round(X_train.shape[0]\/2):-1].values, y_train[round(X_train.shape[0]\/2):-1])\npred_lr = lr.predict(X_val.values)\n\nprint('Test R-squared for linreg is %f' % r2_score(y_val, pred_lr))\nvalidation_function(y_val,pred_lr)","676b9b22":"# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n# specify your initial configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'root_mean_squared_error'},\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 10\n}\n","a754b6d0":"gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=15,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                early_stopping_rounds=5)\n\nprint(gbm.pandas_categorical)\nlgb.plot_importance(gbm,figsize=(10,10));","b30276db":"categorical_features = ['shop_id','item_id','item_category_id','new_item']+top_words\n\nlgb_train = lgb.Dataset(X_train, y_train,categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,categorical_feature=categorical_features)\n\ngbm2 = lgb.train(params,\n                lgb_train,\n                num_boost_round=15,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                categorical_feature = categorical_features,\n                early_stopping_rounds=5)\n\nprint(gbm2.pandas_categorical)\nlgb.plot_importance(gbm2,figsize=(10,10));\n","9427bbb9":"lgb_train = lgb.Dataset(X_train, y_train,categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,categorical_feature=categorical_features)\n\nlgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':10 \n              }\n\nmodel = lgb.train(lgb_params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                categorical_feature = categorical_features,\n                early_stopping_rounds=5)\n\n\nprint(model.pandas_categorical)\nlgb.plot_importance(model,figsize=(10,10));\n\n","7b9bced5":"pred_lgb = model.predict(X_val)\n\nprint('Test R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb))\nvalidation_function(y_val,pred_lgb)","03d9c6a5":"# train\nX_train_level2 = np.c_[model.predict(X_train), lr.predict(X_train.values)] \nlr2 = LinearRegression()\nlr2.fit(X_train_level2, y_train)\n\n# predict\nX_val_level2 = np.c_[model.predict(X_val), lr.predict(X_val.values)] \npred_lr2 = lr2.predict(X_val_level2)\n\nvalidation_function(y_val,pred_lr2)\n","8e137ef7":"# lightgbm and lr predicitons\npred_lgb = model.predict(X_test).clip(0,20)\npred_lr = lr.predict(X_test.values).clip(0,20)\n\n# ensamble predicitons\nX_test_level2 = np.c_[pred_lgb, pred_lr] \npred_ensamble = lr2.predict(X_test_level2)","1537a7c9":"# make sure results are in the same order as the original test set\n(test[['shop_id','item_id']].values == X_test[['shop_id','item_id']].values).all()","1680c92b":"pd.DataFrame(data={'ID':test['ID'],'item_cnt_month':pred_lgb}).to_csv('lgbm_predictions.csv',index=False)\npd.DataFrame(data={'ID':test['ID'],'item_cnt_month':pred_ensamble}).to_csv('ensamble_predictions.csv',index=False)\n","d8dcae1b":"assuming month of year plays a big role in number of items sold (seasonality).\nlet's add month","a350b49e":"# Train\/Validation\/Test Split\nTest set is last `date_block_num` (34).\nWe will validate on the month before that (33) and train on everything else\n* be sure to drop leaking columns such as target_shop,etc...","3721082b":"any useful words in shop names?","b24abdac":"## *sales_train* EDA","068f6477":"## we will try lightgbm with different parameters","3c30ff93":"before rmse looks better than lightgbm, but after clipping, we get pretty much the same results","5aa2b1a7":"# EDA","3190cffc":"# Read and Display Files","60271e55":"`item_price` features\n(getting lots of nulls)","e0f5967c":"according to google translate top values are\n* Shopping Center\n* Moscow\n* Mega\n* Dispenser\n* TC\n\nLet's see if more people buy there","d9940a27":"`item_cnt_day` features\n* total items\n* total trips ","5467361a":"first four seem to be good features.\nlet's add them to the full table.  ","d96a1bad":"one transaction with pric<=0.\nIt's safe to remove that item.","24114f95":"# Mean Encoding for *item_id*","b38e63dc":"lots of items in each category (median of 43).\nwe can use it as a categorical feature as well as encoded feature","d6454e94":"*item_price* and *item_cnt_day* from **sales_train** seem to have weird min\/max values","f7d7984b":"# Overview\nFirs of all,\nGo check out the course - https:\/\/www.coursera.org\/learn\/competitive-data-science.  \nThis is a great course which combines theory with hands on exercises (like this competition).  \nDefinitely worth the price.\n\nNow,\nThe notebook gives us predictions for future sales on the test set and the sections are as follows:  \n\n## EDA\ndata analysis is done on the different available datasets and includes outlier removal  \n\n## Feature Matrix Creation\nWe have sales of different items from different stores on each month.  \nFeature Matrix will be a FULL table of all possible item\/store pairs in each month.  \nWe will include the test set in this matrix for later features creation.  \n\n## Feature Creation\n* sales count and price features with lags on past months\n* converting month index to month of year (assuming seasonality, it will be usefull to know the month)\n* mean encoding - `item_id` for example is not a useful feature, encoding that id using sales of the item is.\n* `shop_name` text analysis - can we find keywords in a shop name that correlates with high sales and use it as features?  \n\n## Train\/Validation\/Test Split\nAs we will be predicting future sales, we have to make sure our validation schema does not cause us to have training data from dates beyond the validation and test sets.  \nFull data has N months.  \nWe use Month N for test. Month N-1 for validation. And the rest for train.  \n\n## Training Different Models and Validating on RMSE\nWe tested different models\n* Linear Regression\n* LightGBM (multiple models tested with different hyperparameters)\n* Ensamble of both  \n\n## Results Submission","f3ff7300":"# append test data to matrix with next month's date_block_num\ntest file predicts next month.  ","7f2d8684":"# Train and Validate Different Models\n* linear regression\n* lightgbm","4264be55":"let's remove outliers over the 99th percentile","8c83dc04":"# Update parameters\n* model did not converge: we can increase boosting rounds\n* try model after explicitly specifying which columns are categorical","a53175b4":"# Mean Encoding for *shop_id*","88427c84":"## Create monthly features from *sales_train*\nas predictions are per month, we need features aggregated to a monthly level","b219a15d":"# Helper Functions","a9c0a0ab":"# Explore Items Dataset","c4238ae2":"# Submit Results on Test Set\nwe will save lightgbm and ensamble resuts on test set","4e13adb5":"# Add Month Feature","094f4776":"# Try Ensamble\nwe will build a simple lr on top of the train predictions to see if they improve validation","2c030d2e":"# Features Creation","617bb4ba":"not much correlation heare. we can drop that column","4a7edd77":"let's checck which items have item_price<=0","b453f18b":"# Create Validation Function\nvalidation is rmse on True target values are clipped into [0,20] range.  \nLet's create such validation","fd044975":"check if encoding item category is beneficial ","401dbec7":"lag features from [1,2,3,4,5,12] months ago","51a71cfc":"# Feature Matrix Creation\nGet all shop\/id pairs for each month","01bb8a13":"test one more set of parameters","ac24583d":"first item appearance feature","5373a26b":"# Explore Shop Names","2b1b0073":"# Using best lgbm - *model*","450fb19f":"use `item_first_month` to create `new_item` feature"}}