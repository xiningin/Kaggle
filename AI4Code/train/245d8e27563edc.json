{"cell_type":{"e682066a":"code","6ce222a2":"code","0e29d87a":"code","437e50c7":"code","8f6c742a":"code","770c5b6f":"code","caed988d":"code","a08158dc":"code","a1c37e18":"code","d888dd20":"code","0b6f7656":"code","9e1ca0a7":"code","fd1f23c2":"code","888ee244":"code","7ebd1597":"code","3b67c38a":"code","f114e16a":"markdown","f88d4471":"markdown","117a2e33":"markdown","2836c410":"markdown","9d94bff5":"markdown","c913e50a":"markdown"},"source":{"e682066a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\n","6ce222a2":"testW=pd.read_csv(\"\/kaggle\/input\/qa-download\/test_document_text.csv\")\ntrainW=pd.read_csv(\"\/kaggle\/input\/qa-download\/train_document_text.csv\")\ntestQ=pd.read_csv(\"\/kaggle\/input\/qa-download\/test_question_text.csv\")\ntrainQ=pd.read_csv(\"\/kaggle\/input\/qa-download\/train_question_text.csv\")\ntrainW.columns=['id','start','stop','short','wiki']\ntestW.columns=['id','wiki']\ntrainQ.columns=['id','start','stop','short','question']\ntestQ.columns=['id','question']\ntrainW\n","0e29d87a":"trainW.info()","437e50c7":"' '.join(trainW.iloc[1]['wiki'].split(' ')[212:310])  #paragraph in wiki","8f6c742a":"' '.join(trainW.iloc[1]['wiki'].split(' ')[213:215])  #short answer","770c5b6f":"#  find the start stop  token position of a sentence \ndef find_start_end_token(txt,sent):\n    pos=txt.find(np.str(sent) )\n    start=len(txt[:pos-1].split(' '))\n    end=start+len(np.str(sent).split(' '))\n    return start,end\n\nsentence='How I Met Your Mother'\nfind_start_end_token(trainW.iloc[1]['wiki'],sentence),trainW.iloc[1]['wiki'].split(' ')[210:310],find_start_end_token(trainW.iloc[1]['wiki'],' '.join(trainW.iloc[1]['wiki'].split()[210:217]))\n","caed988d":"trainW","a08158dc":"from bs4 import BeautifulSoup\n\ndef wiki_tagsplit(html,wi):\n    temp=[]\n    #html = trainW.iloc[wi]['wiki'] \n    for hi in html:\n        \n        soup = BeautifulSoup(hi, 'html.parser')\n        for ti in ['h1','h2','p','table','tr']:  #splitting tags extracting this features\n            allep=soup.find_all(ti) #p paragraph\n            for pi in allep:\n                start,stop=find_start_end_token(hi,pi.get_text())\n                if start>1:\n                    line=[wi,ti,start,stop,pi.get_text()]\n                    temp.append(line)\n                    \n        wi=wi+1\n    return pd.DataFrame(temp,columns=['id','tag','start','stop','txt'])\nwiki_tagsplit(trainW.iloc[1:2].wiki,1)","a1c37e18":"import numpy as np\nfrom scipy.sparse import csr_matrix\n!pip install sparse_dot_topn\nimport sparse_dot_topn.sparse_dot_topn as ct\n\ndef awesome_cossim_top(A, B, ntop, lower_bound=0):\n    # force A and B as a CSR matrix.\n    # If they have already been CSR, there is no overhead\n    A = A.tocsr()\n    B = B.tocsr()\n    M, _ = A.shape\n    _, N = B.shape\n \n    idx_dtype = np.int32\n \n    nnz_max = M*ntop\n \n    indptr = np.zeros(M+1, dtype=idx_dtype)\n    indices = np.zeros(nnz_max, dtype=idx_dtype)\n    data = np.zeros(nnz_max, dtype=A.dtype)\n\n    ct.sparse_dot_topn(\n        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n        np.asarray(A.indices, dtype=idx_dtype),\n        A.data,\n        np.asarray(B.indptr, dtype=idx_dtype),\n        np.asarray(B.indices, dtype=idx_dtype),\n        B.data,\n        ntop,\n        lower_bound,\n        indptr, indices, data)\n\n    return csr_matrix((data,indices,indptr),shape=(M,N))\n\n\ndef get_matches_df(sparse_matrix, name_vectorQ,name_vectorR, top=100):\n    non_zeros = sparse_matrix.nonzero()\n    \n    sparserows = non_zeros[0]\n    sparsecols = non_zeros[1]\n    \n    if top<sparsecols.size:\n        nr_matches = top\n    else:\n        nr_matchesQ = sparserows.size\n        nr_matchesR =sparsecols.size\n    \n    left_row = np.empty([nr_matchesQ], dtype=object)\n    left_side = np.empty([nr_matchesQ], dtype=object)\n    right_row =np.empty([nr_matchesQ], dtype=object)\n    right_side = np.empty([nr_matchesQ], dtype=object)\n    similairity = np.zeros(nr_matchesQ)\n    \n    for index in range(0, nr_matchesQ):\n        #print(index,sparserows[index],name_vector[sparserows[index]])\n        left_row[index] =sparserows[index]\n        right_row[index] =sparsecols[index]\n        left_side[index] = name_vectorQ[sparserows[index]]\n        right_side[index] = name_vectorR[sparsecols[index]]\n        similairity[index] = sparse_matrix.data[index]\n    \n    return pd.DataFrame({'left_nr':left_row,\n                        'left_side': left_side,\n                         'right_nr':right_row,\n                          'right_side': right_side,\n                           'similarity': similairity})","d888dd20":"%%time\nvect = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\nvect.fit( trainQ.question.values )","0b6f7656":"\nfor qi in range(0,1):\n    questions=trainQ[qi:qi+5].question.values\n    wikis=trainW.iloc[qi:qi+5].wiki.values\n    wiki_split=wiki_tagsplit(wikis,qi)\n    print(questions)\n    Qtfidf_tot = vect.transform( questions ) #.append( (testQ.question))) )\n    Wtfidf_tot = vect.transform(wiki_split.txt) #.append((testQ.question)) ) )\n    \n    matches = awesome_cossim_top(Qtfidf_tot, Wtfidf_tot.T, 5,0.357)\n    matches_df = get_matches_df(matches, questions,wiki_split.txt.values ,top=20000)\n    matches_df = matches_df[matches_df['similarity'] < 1.] \n\nmatches_df.merge(wiki_split, left_on='right_side', right_on='txt')","9e1ca0a7":"    Qtfidf_tot = vect.transform(trainQ[:1000].question.values ) #.append( (testQ.question))) )\n    wikis=trainW.iloc[:1000].wiki.values\n    wiki_split=wiki_tagsplit(wikis,qi)\n    \n    Wtfidf_tot = vect.transform(wiki_split.txt) #.append((testQ.question)) ) )\n","fd1f23c2":"matches = awesome_cossim_top(Qtfidf_tot, Wtfidf_tot.T, 1,0.357)\nmatches","888ee244":"leng=1000\nmatches = awesome_cossim_top(Qtfidf_tot[:leng], Wtfidf_tot[:leng].T, 10,0.37)\nmatches_df = get_matches_df(matches, trainQ[:leng].question.values,wiki_split.txt.values ,top=20000)\nmatches_df = matches_df[matches_df['similarity'] < 1.99999] \nprint(len(matches_df))\nmatches_df","7ebd1597":"for qi in range(100,110,1):\n    print(trainQ.iloc[qi])\n    Qtfidf_tot = vect.transform(trainQ.iloc[qi:qi+1].question ) #.append( (testQ.question))) )\n    wikis=trainW.iloc[qi:qi+1].wiki\n    wiki_split=wiki_tagsplit(wikis,qi)\n    \n    Wtfidf_tot = vect.transform(wiki_split.txt) #.append((testQ.question)) ) )\n    \n    wiki_split['pro']=Wtfidf_tot.dot(Qtfidf_tot.T.todense())\n    print(wiki_split[wiki_split.pro>0][['start','stop','txt','pro']].sort_values('pro')[-3:])\n","3b67c38a":"for qi in range(100,110,1):\n    print(trainQ.iloc[qi])\n    Qtfidf_tot = vect.transform(trainQ.iloc[qi:qi+1].question ) #.append( (testQ.question))) )\n    wikis=trainW.iloc[qi:qi+1].wiki\n    wiki_split=wiki_tagsplit(wikis,qi)\n    \n    Wtfidf_tot = vect.transform(wiki_split.txt) #.append((testQ.question)) ) )\n    \n    wiki_split['pro']=Wtfidf_tot.dot(Qtfidf_tot.T.todense())\n    print(wiki_split[wiki_split.pro>0][['start','stop','txt','pro']].sort_values('pro')[-1:])\n    headerwords=wiki_split[wiki_split.pro>0][['start','stop','txt','pro']].sort_values('pro')[-1:].txt\n    headerwords=[str.lower(wi) for wi in headerwords.values[0].split(' ') ]\n    remainQ=[wi for wi in trainQ.iloc[qi].question.split(' ') if wi not in headerwords]\n    remainQ=(' ').join(remainQ)\n    print(remainQ)\n    Qtfidf_tot = vect.transform([remainQ,''])\n    wiki_split['pro']=Wtfidf_tot.dot(Qtfidf_tot.T.todense())\n    print(wiki_split[wiki_split.pro>0][['start','stop','txt','pro']].sort_values('pro')[-1:])\n    \n","f114e16a":"**i exported the data in a subdirectory**\n\n\n* with a previous script\n* i use wiki[:5000] characters versus question\n* since the wiki can be lengthy, i don't advise to cap off the text\n","f88d4471":"## Most simple method to match the wiki content with the question","117a2e33":"top=pd.DataFrame(np.dot(W_tot,Q_tot[4])).sort_values(0)[-40:]\ntop.reset_index().sort_values('index')","2836c410":"## **Trying to find a solution**\n\nsearching a system to find the closest match with train","9d94bff5":"## **testing on top 1000 matching**\n\nas you can see without training you can match text with questions\nbut this is more a solution to find a needle in the haystack or to find the text that matches the question","c913e50a":"## writing a splitter, that splits the wiki in paragraphs guided by the 'tags'\n\ni so a difference of 1 position between the database and my script... I don't know why, but i find my positions corrector. Anyway we will need to adopt those script to make collide the positions\nsee the difference between train1, and the train data\n\n"}}