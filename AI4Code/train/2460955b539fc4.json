{"cell_type":{"b7b6b859":"code","5bfb3540":"code","ef9acb63":"code","73eaf425":"code","9f7620bc":"code","5183a6f0":"code","2f081e56":"code","cadc6c1c":"code","ea659e84":"code","ef7bfd6f":"code","abc7cce6":"code","fcdf2727":"code","5d1b48ec":"code","510c1ec8":"code","48682bbd":"code","b9ea46e9":"code","688ab26c":"code","64910323":"code","537cac6a":"code","f4c7a7d0":"code","117b94d8":"code","a629dab8":"code","d2bcd57c":"code","e7d95417":"code","9e97733d":"code","19b0af6c":"code","677c5f1d":"code","f23af985":"code","abbec120":"code","5b6b2534":"code","aa675ade":"code","58333352":"markdown","723aee3a":"markdown","38833ec3":"markdown","5042ef3b":"markdown","720595b5":"markdown","8bcb6ff0":"markdown","c91d593c":"markdown","c7d0c1c2":"markdown","8bb1f78f":"markdown","5d94d7b9":"markdown","7891b035":"markdown","2b8d1690":"markdown","0d6ca978":"markdown","81306170":"markdown","3845be8e":"markdown","2a07960c":"markdown","5552a9b8":"markdown"},"source":{"b7b6b859":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5bfb3540":"#importing data set\ndf_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_train.head()","ef9acb63":"#Removing the passengerId and Name column since they are unique to every passenger and hence does not help the model\ndf_train.drop([\"PassengerId\", \"Name\"], axis = 1, inplace = True)\ndf_test.drop([\"Name\"], axis = 1, inplace = True) #Since passengerID is needed while submission","73eaf425":"#Getting list of categorical and numerical columns\ncat_columns = list(df_train.select_dtypes(include=['object']).columns)\nnum_columns = list(df_train.select_dtypes(exclude=['object']).columns)\nnum_columns.remove(\"Survived\") #Since its the dependent variable\n\nprint(\"Categorical columns: \",cat_columns)\nprint(\"Numerical columns: \",num_columns)","9f7620bc":"#Checking for missing data in train set\ndf_train.info()","5183a6f0":"#Removing Cabin from train and test set\ndf_train.drop([\"Cabin\"], axis = 1, inplace = True)\ndf_test.drop([\"Cabin\"], axis = 1, inplace = True)\n\n#Using simple imputer to fill in missing data points in age with mean value\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nage_values = np.array(df_train['Age'].values)\nage_values = np.reshape(age_values, (age_values.shape[0], 1))\nimputer.fit(age_values)\ndf_train['Age'] = imputer.transform(age_values)\n\n#Removing 2 rows from dataset where Embarked data is missing\ndf_train.dropna(axis = 0, inplace = True, how = 'any')\n\ndf_train.info()","2f081e56":"#checking for duplicates\ndf_train.loc[df_train.duplicated()]\nduplicate_contribution = df_train.duplicated(keep='first').value_counts(normalize = True) * 100\nduplicate_contribution = 100.0 - duplicate_contribution[0] \nprint(duplicate_contribution)","cadc6c1c":"df_train.drop_duplicates(keep = \"first\", inplace = True)\ndf_train.info()","ea659e84":"#Checking frequency of categorical variables\ncat_columns.remove(\"Cabin\") #Since we removed this column\nfor category in cat_columns:\n    print(df_train[category].value_counts())","ef7bfd6f":"#Counting unique values in column \"Ticket\"\nprint(len(df_train.Ticket.unique()))","abc7cce6":"df_train.drop([\"Ticket\"], axis = 1, inplace = True)\ndf_test.drop([\"Ticket\"], axis = 1, inplace = True)\ncat_columns.remove(\"Ticket\")\ndf_train.info()","fcdf2727":"#Visualizing frequency distribution of Survived class\nax = sns.countplot(x = \"Survived\", data = df_train)\nax.set_title(\"Frequency distribution of Survived class\")\nplt.show()","5d1b48ec":"#Survived with respect to sex\nax = sns.countplot(x = \"Survived\", hue = \"Sex\", data = df_train)\nax.set_title(\"Frequency distribution of Survived wrt Sex\")\nplt.show()","510c1ec8":"#Survived vs Embarked\nax = sns.countplot(x = \"Survived\", hue = \"Embarked\", data = df_train)\nax.set_title(\"Frequency distribution of Survived wrt Embarked\")\nplt.show()","48682bbd":"#Visualizing frequency distribution of Pclass\nf, ax = plt.subplots(1, 2, figsize = (16,6))\nax[0] = sns.countplot(x = \"Pclass\", data = df_train, ax=ax[0])\nax[0].set_title(\"Frequency distribution of Pclass\")\n\n#Survived vs Pclass\nax[1] = sns.countplot(x = \"Survived\", hue = \"Pclass\", data = df_train)\nax[1].set_title(\"Frequency distribution of Survived wrt Pclass\")\nplt.show()","b9ea46e9":"#Age - Distribution and relationship with Survived\nf, ax = plt.subplots(1, 2, figsize = (16,6))\nax[0] = sns.histplot(x = df_train[\"Age\"], ax=ax[0])\nax[0].set_title(\"Distribution of age variable\")\n\nax[1] = sns.boxplot(x = \"Survived\", y = \"Age\", data = df_train)\nax[1].set_title(\"Visualize Survived wrt Age variable\")\nplt.show()","688ab26c":"#Grouping age in bracket of 20s and finding largest group\n#0-20\nfirst_bracket = df_train[(df_train.Age > 0) & (df_train.Age < 20)].count()\n#20-40\nsecond_bracket = df_train[(df_train.Age >= 20) & (df_train.Age < 40)].count()\n#40-60\nthird_bracket = df_train[(df_train.Age >= 40) & (df_train.Age < 60)].count()\n#60 and above\nfourth_bracket = df_train[(df_train.Age >= 60)].count()\n\nprint(\"Age distribution according to brackets: \")\nprint(\"0-20 : \", first_bracket[0])\nprint(\"20-40 : \", second_bracket[0])\nprint(\"40-60 : \", third_bracket[0])\nprint(\">=60 : \", fourth_bracket[0])","64910323":"#SibSp - Distribution and relationship with Survived\nf, ax = plt.subplots(1, 2, figsize = (16,6))\nax[0] = sns.histplot(x = df_train[\"SibSp\"], ax=ax[0])\nax[0].set_title(\"Distribution of SibSp variable\")\n\nax[1] = sns.boxplot(x = \"Survived\", y = \"SibSp\", data = df_train)\nax[1].set_title(\"Visualize Survived wrt SibSp variable\")\nplt.show()","537cac6a":"#Parch - Distribution and relationship with Survived\nf, ax = plt.subplots(1, 2, figsize = (16,6))\nax[0] = sns.histplot(x = df_train[\"Parch\"], ax=ax[0])\nax[0].set_title(\"Distribution of Parch variable\")\n\nax[1] = sns.boxplot(x = \"Survived\", y = \"Parch\", data = df_train)\nax[1].set_title(\"Visualize Survived wrt Parch variable\")\nplt.show()","f4c7a7d0":"#Fare - Distribution and relationship with Survived\nf, ax = plt.subplots(1, 2, figsize = (16,6))\nax[0] = sns.histplot(x = df_train[\"Fare\"], ax=ax[0])\nax[0].set_title(\"Distribution of Fare variable\")\n\nax[1] = sns.boxplot(x = \"Survived\", y = \"Fare\", data = df_train)\nax[1].set_title(\"Visualize Survived wrt Fare variable\")\nplt.show()","117b94d8":"#With boxplots we also saw the presence of outliers. Lets remove them from all numerical columns\nfrom scipy import stats\nz = np.abs(stats.zscore(df_train[num_columns]))\nthreshold = 3\n\n#Removing outliers\ndf_new = df_train[(z < 3).all(axis=1)]\ndf_new.info()","a629dab8":"#Plotting correlation matrix\nplt.figure(figsize=(8,6))\nsns.heatmap(df_new.corr(),annot = True,cmap = 'inferno',mask = np.triu(df_new.corr(),k = 1))\nplt.show()","d2bcd57c":"#Encoding categorical variables\n#We will use dummy variable encoder\ndummies = pd.get_dummies(df_new[cat_columns], drop_first = True)\n\n#Add the results to the original dataframe\ndf_new = pd.concat([df_new, dummies], axis = 1)\n\n#Drop the original cat variables as dummies are already created\ndf_new.drop(cat_columns, axis = 1, inplace = True)\ndf_new.info()","e7d95417":"df_new.head()","9e97733d":"#Preparing the test set\ndf_test.info()","19b0af6c":"#Using simple imputer to fill in missing data points in age with mean value\nimputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nage_values = np.array(df_test['Age'].values)\nage_values = np.reshape(age_values, (age_values.shape[0], 1))\nimputer.fit(age_values)\ndf_test['Age'] = imputer.transform(age_values)\n\n#Imputing missing value in Fare with mean fare\nfare_values = np.array(df_test['Fare'].values)\nfare_values = np.reshape(fare_values, (fare_values.shape[0], 1))\nimputer.fit(fare_values)\ndf_test['Fare'] = imputer.transform(fare_values)\n\ndf_test.info()","677c5f1d":"#Encoding categorical variables\n#We will use dummy variable encoder\ndummies = pd.get_dummies(df_test[cat_columns], drop_first = True)\n\n#Add the results to the original dataframe\ndf_test = pd.concat([df_test, dummies], axis = 1)\n\n#Drop the original cat variables as dummies are already created\ndf_test.drop(cat_columns, axis = 1, inplace = True)\ndf_test.info()","f23af985":"#Preparing train and test set\nX_train = df_new.iloc[:,1:].values\ny_train = df_new.iloc[:,0].values\nX_test = df_test.iloc[:,1:].values\n\nprint(\"X train: \", X_train.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"X test: \", X_test.shape)","abbec120":"#Resampling to solve imbalance in Survived class\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\n# define pipeline\nover = SMOTE()\nunder = RandomUnderSampler()\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n\n# transform the dataset\nX_train, y_train = pipeline.fit_resample(X_train, y_train)","5b6b2534":"#Preparing models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport catboost as cb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Logistic regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nscore = round(logreg.score(X_train, y_train) * 100, 2)\nprint(\"Logistic regression: \", score)\n\n#SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\nscore = round(svc.score(X_train, y_train) * 100, 2)\nprint(\"SVC: \", score)\n\n#Decision Tree\ndectree = DecisionTreeClassifier()\ndectree.fit(X_train, y_train)\nscore = round(dectree.score(X_train, y_train) * 100, 2)\nprint(\"Decision Tree Classifier: \", score)\n\n#Random Forest\nrandforest = RandomForestClassifier()\nrandforest.fit(X_train, y_train)\nscore = round(randforest.score(X_train, y_train) * 100, 2)\nprint(\"Random Forest Classifier: \", score)\n\n#KNN\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nscore = round(knn.score(X_train, y_train) * 100, 2)\nprint(\"K Nearest Classifier: \", score)\n\n#Gaussian NB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nscore = round(nb.score(X_train, y_train) * 100, 2)\nprint(\"Naive Bayes Classifier: \", score)\n\n#Catboost\ncb_classifier = cb.CatBoostClassifier(logging_level=\"Silent\")\ncb_classifier.fit(X_train, y_train)\nscore = round(cb_classifier.score(X_train, y_train) * 100, 2)\nprint(\"Cat Boost Classifier: \", score)","aa675ade":"\n#Submission\nY_pred = cb_classifier.predict(X_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('.\/submission.csv', index=False)\n","58333352":"**PassengerId:** Unique ID given to each passenger\n\n**Survived:** 1 - passenger survived and 0 - passenger did not survive\n\n**Pclass:** Ticket class 1 - Upper, 2 - Middle and 3 - Lower\n\n**Name:** Name of passenger\n\n**Sex:** Gender of passengers on board\n\n**Age:** Age of passengers\n\n**SibSp:** Number of siblings\/spouses on board\n\n**Parch:** Number of parents\/children on board\n\n**Ticket:** Ticket number\n\n**Fare:** Passenger fare\n\n**Cabin:** Cabin number\n\n**Embarked:** Port of embarkation - C = Cherbourg, Q = Queenstown, S = Southampton","723aee3a":"Fare and Parch seem to have a stronger impact on the survival of passengers.","38833ec3":"There seems to be no relationship between survived and sibsp meaning people wiht siblings or spouses were not given and higher priority.","5042ef3b":"Lets take care of missing values and encoding","720595b5":"We have some missing data in the following columns:\n\n**Age** - 177 missing data points (Numerical variable)\n\n**Cabin** - 687 missing data points (Categorical variable)\n\n**Embarked** - 2 missing data points (Categorical variable)\n\nThe **\"Cabin\"** variable has too much missing data and so it would be better to remove it from our data set. As for **\"Age\"** we will use simple imputer to fill in the data with the mean age. **\"Embarked\"** has only 2 missing data points and so we will just drop these rows.","8bcb6ff0":"It seems that a lot more female passengers survived which makes sense since the Titanic follow the Birkenhead Protocol.It is defined as the act of allowing women and children to escape first when a ship is in distress, came about during the sinking of HMS Birkenhead, after it run aground on a sandbank off the coast of South Africa, in 1852.\n\nThe Birkenhead Drill has no basis in maritime law and has actually only ever been officially implemented on two occasions. On the aforementioned HMS Birkenhead in 1852 and on the ill fated RMS Titanic in 1912.","c91d593c":"It seems that higher priority was given to people with higher Pclass or the privelaged class. Even though the majority of people were from the low class (pclass = 3), the number of them that survived is lower than the upper class (pclass = 1). ","c7d0c1c2":"We have only about 1.68% of duplicate data and so we will remove it.","8bb1f78f":"Now let us check for any duplicate data","5d94d7b9":"Given that the maority of people were in the age group 20 - 40 it makes sense that majority of people that survived are from the same age group. There is a slight indication that more younger people survived.","7891b035":"Given that the number of passengers boarding from each port is:\n\nS = 633, C = 165, Q = 76\n\nWhich we found through our frequency calculation of categorical variables, there seems to be no significant relationship between the embarking port and survival rate. From the graph it seems that the chance of people surving was S > C > Q but so were the number of people from each of these ports.","2b8d1690":"There seems to be 680 unique values in the Ticket column, suggesting less significance overall. So we will proceed to remove it.","0d6ca978":"People that paid a higher fare had an higher chance of survival which we have also seen with the relationship between Pclass and Survived, considering that higher pclass would have a higher fare.","81306170":"**Titanic competition**","3845be8e":"**Sex_male:** 1 indicates male and 0 indicates female\n\n**Embarked_Q:** 1 indicates Q as point of embarkment 0 indicates Q is not point of embarkment\n\n**Embarked_S:** 1 indicates S as point of embarkment 0 indicates S is not point of embarkment\n\nWhen **Embarked_Q** and **Embarked_S** are 0 indicates C as point of embarkment","2a07960c":"Decision tree and Random forest give us the same and highest accuracies. But we will choose catboost as they seem to have overfitted the training set","5552a9b8":"People with parents\/children had a better chance of survival"}}