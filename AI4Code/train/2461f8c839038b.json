{"cell_type":{"ced2b981":"code","446b0ea3":"code","3d526b7c":"code","a067e32f":"code","74bc523c":"code","6758d50b":"code","8d289527":"code","62bf30ea":"code","d9d7a8dd":"code","93702ebc":"code","1ea4b3ff":"code","216c6419":"code","90a12a32":"code","2bf5bbd1":"code","b6712bbe":"code","5ebdda8c":"code","9b58d767":"code","296caa72":"code","419dbe52":"code","e6054bac":"code","9d027b41":"code","63084655":"code","3f402170":"code","1cd1ca54":"code","3db3464a":"code","ad9aa8ef":"code","4e5cf8b3":"code","4f3373bd":"code","a0cc4ad2":"code","9ee2b331":"code","4706268c":"code","7df01c86":"code","07f1ecf1":"code","032239f3":"code","694719d4":"code","990920d5":"code","57031c35":"code","e3eff56c":"code","5f246ba9":"code","e086d8a5":"code","dac5fd25":"markdown","3af1df78":"markdown","4741606f":"markdown","b416255d":"markdown","deef61fe":"markdown","9fe8ffd4":"markdown","23c3a37c":"markdown","1b791869":"markdown","39eb5a5e":"markdown","aa4a3af5":"markdown","cc7501be":"markdown","ddff0005":"markdown","e62b031d":"markdown","a8946ba0":"markdown","86518601":"markdown","8ebb6ade":"markdown","d902f954":"markdown","70140e37":"markdown","24d54529":"markdown","6aa2a012":"markdown","8133cef1":"markdown","139f4888":"markdown","c49de48f":"markdown","cfa048a7":"markdown","e657ee8e":"markdown","aa9649f4":"markdown","a8d00603":"markdown","02920142":"markdown","ac8378f7":"markdown","867149f7":"markdown","8f93c707":"markdown","b782e6ea":"markdown","0dbc1cb3":"markdown","a73c4ca8":"markdown","c7921f5a":"markdown","dba8d633":"markdown","fff93587":"markdown","58896daf":"markdown","f1a02cbc":"markdown","0c81b4e4":"markdown","2f938d43":"markdown","686c81b3":"markdown","4046b5f4":"markdown","ee1464c6":"markdown","ade3163d":"markdown","b7a6327d":"markdown","5abd60b9":"markdown"},"source":{"ced2b981":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams[\"figure.figsize\"] = [8,8]","446b0ea3":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","3d526b7c":"plt.plot(train_data['GrLivArea'], train_data['SalePrice'], 'ro')\nplt.show()","a067e32f":"train_data = train_data[train_data['GrLivArea']<4000]","74bc523c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n                                        train_data.drop('SalePrice',axis=1),\n                                        train_data['SalePrice'],\n                                        test_size=0.3,\n                                        random_state=0\n                                    )","6758d50b":"pd.DataFrame(X_train.dtypes.values, columns=['dtype']).reset_index().groupby('dtype').count().plot(kind='bar')","8d289527":"#I Can't pip install a library in kernel, hence embedding source of the package\n#This can be assumed to be lib code\nimport numpy as np\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn_pandas import DataFrameMapper\n\nclass CustomImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean', filler='NA'):\n        self.strategy = strategy\n        self.fill = filler\n\n    def fit(self, X, y=None):\n        if self.strategy in ['mean', 'median']:\n            if not all([dtype in [np.number, np.int] for dtype in X.dtypes]):\n                raise ValueError('dtypes mismatch np.number dtype is required for ' + self.strategy)\n        if self.strategy == 'mean':\n            self.fill = X.mean()\n        elif self.strategy == 'median':\n            self.fill = X.median()\n        elif self.strategy == 'mode':\n            self.fill = X.mode().iloc[0]\n        elif self.strategy == 'fill':\n            if type(self.fill) is list and type(X) is pd.DataFrame:\n                self.fill = dict([(cname, v) for cname, v in zip(X.columns, self.fill)])\n        return self\n\n    def transform(self, X, y=None):\n        if self.fill is None:\n            self.fill = 'NA'\n        return X.fillna(self.fill)\n    \ndef CustomMapper(result_column='mapped_col', value_map={}, default=np.nan):\n    def mapper(X, result_column, value_map, default):\n        def colmapper(col):\n            return col.apply(lambda x: value_map.get(x, default))\n        mapped_col = X.apply(colmapper).values\n        mapped_col_names = [result_column + '_' + str(i) for i in range(mapped_col.shape[1])]\n        return pd.DataFrame(mapped_col, columns=[mapped_col_names])\n    return FunctionTransformer(\n        mapper,\n        validate=False,\n        kw_args={'result_column': result_column, 'value_map': value_map, 'default': default}\n    )","62bf30ea":"#numerical features\nX_train.select_dtypes([int, float]).columns","d9d7a8dd":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.pipeline import FeatureUnion, make_union\nfrom sklearn_pandas import DataFrameMapper, gen_features\n#import sklearn_pipeline_utils as skutils\n\n# Using dataFrameMapper to map the chosen imputer to the columns\n# we are imputing the columns that doesn't have missing values also, this is generally \n# a good practice because we are not making any assumptions on the test data\n\nnumerical_data_pipeline = DataFrameMapper(\n        [\n            (['LotFrontage',\n              'LotArea',\n              'OverallQual',\n              'OverallCond', \n              'YearBuilt',\n              'YearRemodAdd'],CustomImputer(strategy='median'), {'alias': 'num_data1'}\n            ),\n            (['BsmtFinSF1',\n              'BsmtFinSF2',\n              'BsmtUnfSF',\n              'GrLivArea',\n              '1stFlrSF',\n              '2ndFlrSF',\n              'BedroomAbvGr',\n              'TotRmsAbvGrd',\n              'Fireplaces',\n              'GarageCars',\n              'GarageArea',\n              'WoodDeckSF'], CustomImputer(strategy='fill', filler=0), {'alias': 'num_data2'}\n            )\n        ],input_df=True ,df_out=True)","93702ebc":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaled_numerical_pipeline = make_pipeline(\n    numerical_data_pipeline,\n    StandardScaler(),\n    MinMaxScaler()\n)","1ea4b3ff":"numerical_data_pipeline.fit_transform(X_train).head()","216c6419":"scaled_numerical_pipeline.fit_transform(X_train)[0:2]","90a12a32":"train_data.select_dtypes('object').columns","2bf5bbd1":"from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\nfrom sklearn_pandas import gen_features\n\nimpute_mode_cols = gen_features(\n    columns=['MSSubClass', 'MSZoning', 'LotShape', 'LandContour',\n             'LotConfig', 'LandSlope', 'Foundation', 'Condition1',\n             'Condition2', 'BldgType', 'HouseStyle'],\n    classes=[\n        {'class':CustomImputer,'strategy':'mode'},\n        {'class':LabelBinarizer}\n    ]\n)\n\nimpute_NA_cols = gen_features(\n    columns=['Neighborhood', 'SaleType', 'SaleCondition', 'RoofStyle', 'GarageType'],\n    classes=[\n        {'class':CustomImputer, 'strategy':'fill', 'filler':'NA'},\n        {'class':LabelBinarizer}\n    ]\n)\n\ncategorical_data_pipeline = make_union(\n    DataFrameMapper(impute_mode_cols, input_df=True, df_out=True),\n    DataFrameMapper(impute_NA_cols, input_df=True, df_out=True)\n)","b6712bbe":"# we wrote this manually in case of numerical data pipeline, we have used gen_features to genrate this here\n# printing the first two, similar transformers mapping is generated for all columns\nimpute_mode_cols[0:2]","5ebdda8c":"categorical_data_pipeline.fit_transform(X_train).shape","9b58d767":"score_map = {\n    'Ex' : 5.0, 'Gd' : 4.0,\n    'TA' : 3.0,'Av' : 3.0,\n    'Fa' : 2.0, 'Po' : 1.0,\n    'NA' : 0, 'No' : 1.0,\n    'GLQ': 6,'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA':0,\n    'Fin' : 3, 'RFn' : 2,\n    'Typ' : 6 ,'Min2': 5,\n    'Min1': 4, 'Mod' : 3,\n    'Maj1': 2, 'Maj2': 1,\n    'Sev' : 0, 'Mn' : 2.0,\n}\n\nscore_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n             'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n             'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond',\n             'FireplaceQu', 'GarageFinish', 'Functional'\n            ]\n\nscore_data_pipeline = DataFrameMapper([\n    (score_cols, [CustomImputer(strategy='fill', filler='NA'),\n                  CustomMapper(value_map=score_map, default=0)], {'alias': 'score_col'})\n], input_df=True, df_out=True)","296caa72":"score_data_pipeline.fit_transform(X_train).head()","419dbe52":"tdf = train_data.copy()","e6054bac":"tdf['remod'] = tdf['YearBuilt']!=tdf['YearRemodAdd']\ntdf.boxplot(column=['SalePrice'], by=['remod'])","9d027b41":"tdf['recent_remod'] = tdf['YrSold'] == tdf['YearRemodAdd']\ntdf.boxplot(column=['SalePrice'], by=['recent_remod'])","63084655":"tdf['garage_remod'] = tdf['YearBuilt'] != tdf['GarageYrBlt']\ntdf.boxplot(column=['SalePrice'], by=['garage_remod'])","3f402170":"### How are recently built house treated in the market\n\ntdf['recentbuilt'] = tdf['YrSold']==tdf['YearBuilt']\ntdf.boxplot(column=['SalePrice'], by=['recentbuilt'])","1cd1ca54":"from sklearn.preprocessing import FunctionTransformer\n\ndef ColumnsEqualityChecker(result_column='equality_col', inverse=False):\n    def equalityChecker(X, result_column, inverse=False):\n        def roweq(row):\n            eq = all(row.values == row.values[0])\n            return eq\n        eq = X.apply(roweq, axis=1)\n        if inverse:\n            eq = eq.apply(np.invert)\n        return pd.DataFrame(eq.values.astype(int), columns=[result_column])\n    return FunctionTransformer(\n        equalityChecker,\n        validate=False,\n        kw_args={'result_column': result_column, 'inverse': inverse}\n    )","3db3464a":"engineered_feature_pipeline = DataFrameMapper([\n    (['YearBuilt','YearRemodAdd'], ColumnsEqualityChecker(inverse=True)),\n    (['YearRemodAdd', 'YrSold'], ColumnsEqualityChecker()),\n    (['YearBuilt', 'GarageYrBlt'], ColumnsEqualityChecker(inverse=True)),\n    (['YearBuilt', 'YrSold'], ColumnsEqualityChecker(inverse=True)),\n], input_df=True, df_out=True)","ad9aa8ef":"engineered_feature_pipeline.fit_transform(X_train).head()","4e5cf8b3":"features_pipeline = make_union(scaled_numerical_pipeline, \n                      categorical_data_pipeline, \n                      score_data_pipeline,\n                      engineered_feature_pipeline)","4f3373bd":"features_pipeline.fit_transform(X_train).shape","a0cc4ad2":"import xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import ExtraTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor \n\nregressors = [\n    SVR(),\n    SGDRegressor(),\n    KNeighborsRegressor(),\n    DecisionTreeRegressor(),\n    ExtraTreeRegressor(),\n    GradientBoostingRegressor(),\n    AdaBoostRegressor(),\n    xgb.XGBRegressor()\n]","9ee2b331":"Regression_pipeline = Pipeline([\n    ('features', features_pipeline),\n    ('regressor', regressors[0])\n])","4706268c":"from sklearn.model_selection import cross_validate\nfrom pprint import pprint\n\nfor reg in regressors:\n    Regression_pipeline.set_params(regressor=reg)\n    scores = cross_validate(Regression_pipeline, X_train, y_train, scoring='neg_mean_squared_log_error', cv=10)\n    print('----------------------')\n    print(str(reg))\n    print('----------------------')\n    pprint('Leaderboard score - mean log rmse train '+str((-scores['train_score'].mean())**0.5))\n    pprint('Leaderboard score - mean log rmse test '+str((-scores['test_score'].mean())**0.5))","7df01c86":"Regression_pipeline.fit(X_train, y_train)","07f1ecf1":"y_validation_predict = Regression_pipeline.predict(X_test)","032239f3":"from sklearn.metrics import mean_squared_log_error\n\nscore = (mean_squared_log_error(y_validation_predict, y_test))**0.5","694719d4":"print('Validation Score '+str(score))","990920d5":"X = train_data.drop('SalePrice', axis=1)\ny = train_data['SalePrice']","57031c35":"Regression_pipeline.fit(X,y)\n","e3eff56c":"result = Regression_pipeline.predict(test_data)","5f246ba9":"def generate_submission(filename, y_predict):\n    df = pd.DataFrame({'Id': range(1461,2920), 'SalePrice': y_predict})\n    df.to_csv(filename, index=False)","e086d8a5":"#generate_submission('improved_pipe2.csv', result)","dac5fd25":"## Let's examine the attributes","3af1df78":"### How hot is a recently remodelled house?","4741606f":"## Numerical Features pipeline","b416255d":"## Let's construct pipelines\n\nGiven below is the overview of our pipeline, We'll have a seperate pipelines to handle numerical data, categorical data, data that can transformed to scores and a seperate one to handle any engineered feature, we might come up with. ","deef61fe":"## Libraries required\n\nThe base library of our pipelines is [sklearn.pipeline](http:\/\/scikit-learn.org\/stable\/modules\/pipeline.html), in addition to that we'll be requiring a few helpers to make life easier.\n\n### [sklearn_pandas](https:\/\/github.com\/scikit-learn-contrib\/sklearn-pandas)\n\n#### Installation \n\n`pip install sklearn_pandas`\n\nsklearn pipelines natively have poor support for pandas dataframe, sklearn_pandas is an effort to bridge the gap between pandas and sklearn. They have an amazingly easy documentation to their api, it took me just an hour to adapt it in my flow, I would highly encourage everyone to give it a try.\n    \n   * **sklearn_pandas.DataFrameMapper** - A Pipeline util that can be used to apply specific set of transformations to column\/columns of a DataFrame\n    \n   * **sklearn_pandas.gen_features** - An utility which can be used to map the same transformer steps to multiple columns in the DataFrame\n   \n### [sklearn_pipeline_utils](https:\/\/github.com\/gautham20\/sklearn_pipeline_utils)\n\n#### Installation\n\n`pip install sklearn_pipeline_utils`\n\nsklearn_pipeline_utils is a library I hacked together. It's set of pipeline transformer that I felt will be reusable across all the pipelines I'll be building in the future. It has transformers that are lacking in the sklearn.preprocessing libraries\n\n\n\n   * **sklearn_pipeline_utils.CustomImputer** - sklearn.preprocessing.Imputer does not work on categorical data, so this is a wrapper to handle any data formats \n   \n   * **sklearn_pipeline_utils.CustomMapper** - This is a transformer to map the values of a column based on the dictionary we provide. \n\n","9fe8ffd4":"**ColumnsEqualityChecker** returns a FunctionTransformer that wraps the function **equalityChecker**. Any arguments to equalityChecker can be passed in kw_args of FunctionTransformer","23c3a37c":"## And there you go!! A Practical solution using pipelines from end to end as promised  :)    \n\nThis scratches the surface on what's possible with pipelines, fork the notebook and try to add new features, try writing custom transformations. If you end up writing a transformer that might be useful for the community let's curate that in [sklearn_pipeline_utils](https:\/\/github.com\/gautham20\/sklearn_pipeline_utils), feel free to issue a pull request.\n\nI'd love to hear your comments on pipelines and any ways to improve this is very much welcome. \n\nIf you've learned and enjoyed this kernel, **support me with an upvote**\n\n**Thank You**","1b791869":"## Features Pipeline","39eb5a5e":"We are at the final step of applying Regressors to predict the SalePrice with our pipeline. Let's spot check regressors to pick the best one.","aa4a3af5":"### Filter out the outliers","cc7501be":"I'm not making any further changes to features for simplicity, but other Feature selection and Decompositions steps can take place at this point.","ddff0005":"We now train the pipeline with 70% of train_data, and predict the prices of validation dataset-30% of the train data we set aside","e62b031d":"## Categorical Data Pipeline","a8946ba0":"## Pipelines prevents data leakage\n\nLet's take this code,\n\n``train_data['GrlivArea'].fillna(train_data['GrlivArea'].mean())``\n\nAnd train_data is used is 3 fold cross validation, this is a classic example of **Data Leakage**. \n\n**why?** - because the while training your model with 2 folds in CV, `GrLivArea` has it's missing values imputed with `mean()` of all GrlivArea values, including values in the 3rd hold out set. Essentially data leaked from your test set to train set.\n\n**This doesn't happen in pipelines** because all transformations happen as a part of the pipeline. And while CV it gets only the 2 folds data to train, with no knowledge of the test_data.","86518601":"I've explained every step along the way, So it should be easy to follow. \n\n**What can you expect in this notebook**\n\n**Beginner** - I've never heard of sklearn pipelines - You are in for a treat. You I'll be introduced to a new\/clean\/easy coding model to build predictive models\n\n**Intermediate** - I've dabbled with pipes, haven't used them end to end - I'll introduce you to new techniques that can help you use pipelines better.\n\n**Advanced** - I've wrote complex pipelines in production - I've made some cool tweaks and wrote a helper library with common transformation which you can evaluate. Let's dicuss best practices","8ebb6ade":"### As expected, we are using this feature in our model","d902f954":"## Applying pipeline to Validation data","70140e37":"Bonus - Significant parts of these helper libraries are currently under development to be included in sklearn library. For the curious minds - [Heterogenous Feature Union](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/2034), [Categorical Encoder](https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/9151), [ColumnTransformer](https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/3886). Hoping that these make it to the scikit-learn0.20 stable release, things are going to get a lot better for pipelines","24d54529":"![pflow](https:\/\/i.imgur.com\/fnBNz6O.jpg)","6aa2a012":"#### This is unexpected, why does building a garage after the house is built reduce it's price, I know that this assumption is a stretch, but let me know if there a reason behind it.","8133cef1":"#### Notice how easy it was to apply all the transformations to a new dataset? :D\n\nThe benifits we get by using pipelines are,\n\n\n* The original dataset X_train and X_test remains unchanged\n* No intermediate dataframes, I don't have to keep track of multiple dataframes, each with it's own transformations\n* All the transformer objects that has been fitted with the train data, all applied to my test data too. No place for confusions.","139f4888":"* ### How did we generate scores?\n\n* **score_data_pipeline** also uses the DataFrameMapper, to all the score_cols we have identified, we are appling **skutils.CustomMapper**, with the **score_map** as values. Let's review our score_data_pipeline output","c49de48f":"* the ouput of both impute_mode_cols and impute_NA_cols and concatenated by using **make_union**, this is helper function to the [**FeatureUnion**]() in sklearn, which can be used to concatenate outputs from multiple pipelines\/transformers. Let's check out the shape of categorical_data_pipeline","cfa048a7":"All our feature transformations steps now live inside pipelines, namely **scaled_numerical_pipeline**, **categorical_data_pipeline**, **score_data_pipeline** and **engineered_feature_pipeline**. Combining all the features using a FeatureUnion will give the data our model will train on","e657ee8e":"### Splitting our train data into 70% train dataset and 30% test dataset","aa9649f4":"#### there isn't much difference in the lower ranges, I can see that remodelled house have little edge in upper ranges, so I'll take it","a8d00603":"# Building End to End predictive models with sklearn pipelines\n\nSklearn pipelines have been around for a while now. I always thought pipelines as a nifty feature that can take out the back forth scrolling in kernel notebooks between data cleaning, feature engineering and prediction steps, and have a single object\/pipe that can take in data and give out predictions. I also found that this was a promiseland, it was not a easy task to create a practical\/useful pipeline, that took in a DataFrame and gave out predictions.\n\nBut now, after a revisit to the land of pipelines, I could bet you that pipelines are back in action. No, I'm serious, I bet that it's possible to acheive the most organized and concise code for a problem by adapting pipelines. I'm willing to bet my upvote on it ;)","02920142":"### Feeling a little lost?\n\nLet's look back on what we've done, We've imputed all our numerical features in the **numerical_data_pipeline**\nand reused the same in **scaled_numerical_pipeline** to apply StandardScaler and MinMaxScaler. \n\n### Step by Step review.\n\n* **numerical_data_pipeline** - A DataFrameMapper was used to apply imputation based on median to a set of columns and impute zero to missing values in some columns, the result of which can be observed below","ac8378f7":"#### Significantly hot :)","867149f7":"* This numerical_data_pipeline is the first step in our **scaled_numerical_pipeline**, this means that given the train_data to scaled_numerical_data, the transformations in numberical_data_pipeline is applied to train_data, and result is passed to the next steps of the pipeline, which take care of scaling. the result can be observed below","8f93c707":"## Predicting test data","b782e6ea":"Now we train the pipeline with entire test set and predict the result for the test dataset. Since we can apply all transformations to test dataset with a single line of code, this step becomes a piece of cake :)","0dbc1cb3":"## Let's engineer new features","a73c4ca8":"### Does building a garage after few years make the house more valuable?","c7921f5a":"## Now to the problem at hand.\n\nThere are pretty awesome kernels out there that does an amazing job at feature engineering. So without going in depth on that, let's do some basic analysis of our data.","dba8d633":"## Let's assign scores to our categorical data","fff93587":"## Regression Pipeline and Cross Validation","58896daf":"#### I'm skipping tuning hyper parameter to keep it consise. That being said hyper parameter tuning is the secret sauce, fork kernel and grid search for the best xgboost params.  ","f1a02cbc":"### Engineered Data Pipeline","0c81b4e4":"All our engineered features involves comparing if two columns equal or unequal to each other. A single transformer that does this is all we need to engineer these features to our pipeline.\n\nLet's see how to develop a custom transformer to be used in pipeline.\n\nA Transformer by definition should support the **fit()\/transform()** interface. There are two ways to achieve this,\n\n* creating a class which inherits from **sklean.base.TransformerMixin** and override fit() and transform()\n* **sklearn.preprocessing.FunctionTransformer** can be used to transform any functions to a transformer\n\nWe I'll be using the **FunctionTransformer** as it's simpler and adequete for our needs. You can check out the other method [here](https:\/\/github.com\/gautham20\/sklearn_pipeline_utils\/blob\/master\/sklearn_pipeline_utils\/skutils.py) in defenition of **CustomImputer**","2f938d43":"according to the [data](https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock\/datadocumentation.txt) here, most of these attibutes can be converted to scores. The other attributes can be transformed into one hot vectors.","686c81b3":"## Building Custom Transformer for Pipelines","4046b5f4":"### Our data set has a mix of numerical and categorical data. our pipeline should handle both","ee1464c6":"## Let's Start afresh...what are pipelines.\n\nOn the outlook solution to any supervised learning problem, is to learn a set of transformations to apply to the training data so that it matches the results closely, and applying the same transformation to the future test data, to get closely matching result. This entire process is the premise of pipelines. Pipelines are objects that help to record a chain of transformer steps applied to training data, and apply the same to the test data.\n\n\n![Imgur](https:\/\/i.imgur.com\/tDMLxup.png)\n\n### What are the benefits of this approach.\n\n- You are in complete control of the transformations that goes in to your final model. \n- The whole model right from data preprocessing to regression\/classification can be pickled into a single object. No more is the trouble of remembering what features went into my best model.\n- When used right, reduces the lines of code drastically, this is more apparent when you start developing reusable transformers. (Exactly what I've done here)\n- It's data leakage proof. Here is an [interesting read](https:\/\/machinelearningmastery.com\/data-leakage-machine-learning\/) on this topic. This is illustrated in detail in the coming sections.","ade3163d":"### what happened?\n\n* using **gen_features** we mapped a unique CustomImputer and LabelBinarizer to each column in the columns feed, this can be fed into DataFrameMapper, the output of gen_features can be seen below.","b7a6327d":"### Is a remodelled house more valuable than a house that has never been remodelled?","5abd60b9":"#### Let's have a look the result"}}