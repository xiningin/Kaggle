{"cell_type":{"e47ec7a4":"code","e309a585":"code","5bd8844b":"code","883ce00c":"code","719cbaa5":"code","556c43a5":"code","7662f897":"code","500a35f6":"code","4b1ed4b4":"code","080bf697":"code","fa0eb409":"code","00e7c81b":"code","9a6e9e32":"code","e1cd7d9b":"code","98333b35":"code","310ad6ec":"code","83eacfc8":"code","1c20e84e":"code","b528d12b":"markdown","b37cc86f":"markdown","acffb2dd":"markdown","35b4ad8e":"markdown","e9751132":"markdown"},"source":{"e47ec7a4":"from sklearn.cluster import DBSCAN \nfrom sklearn.preprocessing import StandardScaler\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\nimport sklearn.metrics as sk\nfrom sklearn.cluster import KMeans\nfrom math import sqrt\nfrom sklearn import preprocessing\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os\n","e309a585":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","5bd8844b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","883ce00c":"df=pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","719cbaa5":"df.head()","556c43a5":"#correltaion matrix\/heatmap\ndf.corr()\nf,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr())\nplt.title('heat map')","7662f897":"x=df.drop(['Outcome'],axis=1)","500a35f6":"#standardising data\nx_scale = StandardScaler().fit_transform(x)","4b1ed4b4":"#elbow method to find n clusters\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(x_scale)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","080bf697":"#check silhouette score\n# Instantiate a scikit-learn K-Means model\nmodel = KMeans(random_state=0)\n\n# Instantiate the KElbowVisualizer with the number of clusters and the metric \nvisualizer = KElbowVisualizer(model, k=(2,10), metric='silhouette', timings=False)\n\n# Fit the data and visualize\nvisualizer.fit(x_scale)    \nvisualizer.poof()   ","fa0eb409":"#applying kmeans algorith\nkmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_y = kmeans.fit_predict(x_scale)\nplt.scatter(x_scale[:,0],x_scale[:,1],c=pred_y,cmap='viridis')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\nplt.show()\n#calculating davies bouldin score\nsklearn.metrics.davies_bouldin_score(x_scale,pred_y)","00e7c81b":"#comparisons\nx_kmeans=x.copy()\nx_kmeans['labels']=pred_y\nx_kmeans.groupby('labels').mean()\ndf.groupby('Outcome').mean()","9a6e9e32":"#hierarchical clustering-plotting dendrogram\ndendrogram = sch.dendrogram(sch.linkage(x_scale, method='ward'))","e1cd7d9b":"#applying agglomerative clustering algorithm\nmodel = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\nmodel.fit_predict(x_scale)\nlabels = model.labels_\n#plotting clusters on scatter plot\nplt.figure(figsize=(10, 7))\nplt.scatter(x_scale[labels==0, 0], x_scale[labels==0, 1], s=50, marker='o', color='red')\nplt.scatter(x_scale[labels==1, 0], x_scale[labels==1, 1], s=50, marker='o', color='blue')\nsklearn.metrics.davies_bouldin_score(x_scale,labels)","98333b35":"x_hrcl=x.copy()\nx_hrcl['labels']=labels\nx_hrcl.groupby('labels').mean()\ndf.groupby('Outcome').mean()\n","310ad6ec":"#density based clustering\ndb = DBSCAN(eps=3, min_samples=10).fit(x_scale) \ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool) \ncore_samples_mask[db.core_sample_indices_] = True\nlabels_db = db.labels_ \n  \n# Number of clusters in labels, ignoring noise if present. \nn_clusters_ = len(set(labels_db)) - (1 if -1 in labels_db else 0) \n  \nprint(labels_db) \n  \n# Plot result \n  \n# Black removed and is used for noise instead. \nunique_labels = set(labels_db) \ncolors = ['y', 'b', 'g', 'r'] \nprint(colors) \nfor k, col in zip(unique_labels, colors): \n    if k == -1: \n        # Black used for noise. \n        col = 'k'\n  \n    class_member_mask = (labels_db == k) \n  \n    xy = x_scale[class_member_mask & core_samples_mask] \n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, \n                                      markeredgecolor='k',  \n                                      markersize=6) \n  \n    xy = x_scale[class_member_mask & ~core_samples_mask] \n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, \n                                      markeredgecolor='k', \n                                      markersize=6) \n  \nplt.title('number of clusters: %d' %n_clusters_) \nplt.show() \n\nsklearn.metrics.davies_bouldin_score(x_scale,labels_db )","83eacfc8":"x_db=x.copy()\nx_db['labels']=labels_db\nx_db.groupby('labels').mean()\ndf.groupby('Outcome').mean()\n","1c20e84e":"db=pd.DataFrame()\ndb['model']=['K-Means','Agglomerative Clustering','DBSCAN']\ndb['davies bouldin index']=[1.22,1.22,1.78]\ndb","b528d12b":"# Agglomerative Clustering","b37cc86f":"(please upvote if you like)","acffb2dd":"# Silhouette method","35b4ad8e":"# Kmeans Algorithm","e9751132":"**Diabets problemsloved using unsupervised learning ******"}}