{"cell_type":{"dc8d5bd6":"code","2d076487":"code","fd465a07":"code","4579266a":"code","7722a898":"code","2243b0d6":"code","115ec7e3":"code","fea72bbf":"code","6cb066c0":"code","d5b4149f":"code","bbf59e20":"code","1f18b0cc":"code","5174bed6":"code","02bb9b3b":"markdown","1bb0524b":"markdown","3425055e":"markdown","9d81d215":"markdown","be032b20":"markdown","279e2451":"markdown","8d807471":"markdown","7a6f6994":"markdown","9a68a3af":"markdown","893e2a6c":"markdown","bd79c446":"markdown","48661d58":"markdown","2909da37":"markdown","22e57146":"markdown","bad32e87":"markdown","71eefd63":"markdown","7c239305":"markdown","7fab5123":"markdown"},"source":{"dc8d5bd6":"import numpy as np\nimport pandas as pd","2d076487":"!ls ..\/input\/","fd465a07":"train_org = pd.read_csv(\"..\/input\/train.csv\")\nprint(\"train shape:\", train_org.shape)\ntrain_org.head()","4579266a":"def clean_text_slow(x, maxlen=None):\n    puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n    '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n    '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n    '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n    '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n    x = x.lower()\n    for punct in puncts[:maxlen]:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_text_fast(x, maxlen=None):\n    puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n    '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n    '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n    '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n    '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n    x = x.lower()\n    for punct in puncts[:maxlen]:\n        if punct in x:  # add this line\n            x = x.replace(punct, f' {punct} ')\n    return x","7722a898":"%%time\n_ = train_org.question_text.apply(lambda x: clean_text_slow(x, maxlen=None))","2243b0d6":"%%time\n_ = train_org.question_text.apply(lambda x: clean_text_fast(x, maxlen=None))","115ec7e3":"%%time\n_ = train_org.question_text.apply(lambda x: clean_text_slow(x, maxlen=65))","fea72bbf":"%%time\n_ = train_org.question_text.apply(lambda x: clean_text_fast(x, maxlen=65))","6cb066c0":"def load_glove_slow(word_index, max_words=200000, embed_size=300):\n    EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_words: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n\ndef load_glove_fast(word_index, max_words=200000, embed_size=300):\n    EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n    emb_mean, emb_std = -0.005838499, 0.48782197\n\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words, embed_size))\n    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\") as f:\n        for line in f:\n            word, vec = line.split(' ', 1)\n            if word not in word_index:\n                continue\n            i = word_index[word]\n            if i >= max_words:\n                continue\n            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n            if len(embedding_vector) == 300:\n                embedding_matrix[i] = embedding_vector\n    return embedding_matrix","d5b4149f":"from keras.preprocessing.text import Tokenizer","bbf59e20":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_org.question_text.values)","1f18b0cc":"%%time\n_ = load_glove_slow(tokenizer.word_index, len(tokenizer.word_index) + 1)","5174bed6":"%%time\n_ = load_glove_fast(tokenizer.word_index, len(tokenizer.word_index) + 1)","02bb9b3b":"As our `puncts` grows longer and longer... I think i dont need to say anymore.","1bb0524b":"use hard code to speed up.","3425055e":"Let create the word_index:","9d81d215":"In the `load_glove_slow`, we calculate the `emb_mean` and  `emb_std` on every loading.\n\nIn the `load_glove_fast`, we write the `emb_mean` and  `emb_std` in the code, and avoid create `dict`","be032b20":"## 1. replace text in question_text","279e2451":"the `puncts` contains 130 words or characters\n\nthe `fast` function only add one line: `if punct in x:`\n\nLet's look at the run time first.","8d807471":"In this section, i will use `clean_text_fast` to speed up. The original replace function is `clean_text_slow`.\n\nThe two functions are defined as blow:","7a6f6994":"the `fast` function use less **32.5** seconds the `slow`\n\n**And in my code, the slow code runs totally 5mins while the `fast` runs only 40~50 seconds.**","9a68a3af":"| No | Category                 | Type   | DictLength   | Run time (total)  | Run time(s\/w) |\n|-----|-----------------------|--------|----------------|---------------------| ----------------- |\n|1      |     replace text         |  slow  |          130      |                  43.4 s  |     0.3338          |\n|2     |     replace text         |   fast   |          130      |                    8.8 s  |     0.0677          |\n|3     |     replace text         |   slow  |          65       |                  23.9 s  |     0.3677          |\n|4     |     replace text         |   fast   |          65       |                  6.05 s  |     0.0931          |\n|5     |     load embedding |   slow  |          ---       |                   51.6 s  |             ---         |\n|6     |     load embedding |   fast   |          ---       |                    19.1 s  |             ---         |\n","893e2a6c":"> **do not create a `new string object` if you can use `in operation`** in python.","bd79c446":"run the functions:","48661d58":"In this kernel, i will help you speed up your preprocessing at:\n\n1. replace text in `question_text`:\n\n    replace the text in the dict, e.g.: x = x.replace(\"?\", \" ? \")\n\n2. load embeddings","2909da37":"As we can see. The `slow` function runs **double** while the `puncts` become **double**\n\nThe `fast` only use extra **1\/3** seconds while the `puncts` become  **double**.","22e57146":"> Try use hard code if it is possible**?**","bad32e87":"This is because the `in` operation is more fast than `create a new str object` in python.\n\nIn the `slow` function, we create a new str in every iteration.\n\nNext, let's use a half `puncts` length to calculate the run time.","71eefd63":"## 2. load embeddings.","7c239305":"First, let's import the packages and load the datas.","7fab5123":"There is a quick report about the kernel. If you want to see the details, follow the codes :)"}}