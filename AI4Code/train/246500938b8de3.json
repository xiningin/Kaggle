{"cell_type":{"fd6c940b":"code","846cf201":"code","f52b1c0e":"code","700a74c1":"code","46457f26":"code","459a10e9":"code","56fef493":"code","af60d2ff":"code","f2b1d222":"code","760db448":"code","2f348357":"code","3df93fdf":"code","558c58b3":"code","2c881866":"code","f0ffbd4d":"code","41ccea74":"code","3b6b3e15":"code","742645f7":"code","18970b47":"code","146e74d6":"code","0026c205":"code","a63ec6d0":"code","85635b6d":"code","128117d5":"code","0c3494dc":"code","947c4718":"markdown","c6da5186":"markdown","b5f1f48a":"markdown","b15b0f3b":"markdown","eb2970f0":"markdown","bbbe746b":"markdown","05db5194":"markdown","7ffd0bc2":"markdown","da5803e6":"markdown"},"source":{"fd6c940b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom os.path import join as pjoin\n\ndata_root = '..\/input\/make-data-ready'\nprint(os.listdir(data_root))\n\n# Any results you write to the current directory are saved as output.","846cf201":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor as RFF\nimport xgboost as xgb\n\nfrom pprint import pprint\nimport math\n\nfrom scipy.stats import kurtosis, skew\n","f52b1c0e":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport shap\nplt.rcParams['figure.figsize'] = (12,6)","700a74c1":"def load_data(data='train',n=2):\n    df = pd.DataFrame()\n    for i in range(n) :\n        if data=='train':\n            if i > 8 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'train_{i}.pkl'))\n        elif data=='test':\n            if i > 2 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'test_{i}.pkl'))\n        df = pd.concat([df,dfpart])\n        del dfpart\n    return df\n        ","46457f26":"df_train = load_data(n=9)\ndf_test = load_data('test',n=4)","459a10e9":"# df_test.date.min(),df_test.date.max()","56fef493":"# start ,end = datetime.strptime('2017-05-01','%Y-%m-%d'),datetime.strptime('2017-10-15','%Y-%m-%d')","af60d2ff":"# df_train = df_train[(df_train['date']>start) & (df_train['date'] <end)]","f2b1d222":"# df_train.shape","760db448":"print(f'# of columns has na value: {(df_test.isnull().sum().sort_values(ascending=False) > 0).sum()}')","2f348357":"def rmse(y_true, y_pred):\n    return round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)\n\ndef split_data(df=df_train,rate=.8):\n    # sort the date first\n    df = df.sort_values('date').copy()\n    \n    df.drop(['fullVisitorId','visitId','visitStartTime'],axis=1,inplace=True)\n    df['Revenue'] = np.log1p(df['Revenue'])\n    \n    global X_train,X_valid,y_train,y_valid\n    \n    n_train = int(len(df)*rate)\n    X_train = df.drop(['Revenue','date'],axis=1).iloc[:n_train]\n    X_valid = df.drop(['Revenue','date'],axis=1).iloc[n_train:]\n    \n    y_train = df['Revenue'].iloc[:n_train]\n    y_valid = df['Revenue'].iloc[n_train:]\n    \n    print(X_train.shape,X_valid.shape)\n    \n    \n\ndef encode_data(verbose=False):\n    global df_train_encoded,df_test_encoded\n    df_train_encoded = df_train.copy()\n    df_test_encoded = df_test.copy()\n    for col in df_train.columns:\n        if df_train_encoded[col].dtype == 'object' and col not in ['fullVisitorId','visitId','visitStartTime','date']:\n            if verbose:\n                print(col)\n            lb = LabelEncoder()\n            lb.fit( list(df_train_encoded[col].unique()) + list(df_test_encoded[col].unique()))\n            df_train_encoded[col] = lb.transform(df_train_encoded[col])\n            df_test_encoded[col] = lb.transform(df_test_encoded[col])\n        \ndef run_xgb():\n   \n    params = {\n        'objective':'reg:linear',\n        'eval_metric':'rmse',\n        'learning_rate':.01,\n        'eta': 0.15, # Step size shrinkage used in update to prevents overfitting\n#         'max_depth': 10, # V3 : 1.0471 on LB\n#         'max_depth':5, # V5 : 0.9331 on LB\n        'subsample': 0.6, # sample of rows\n        'colsample_bytree': 0.6, # sample of features\n#         'alpha':0.001, \n        'lambda':1, # l2 regu\n        'random_state': 42,\n        'silent':True\n        \n    }\n    \n    \n    # got params from https:\/\/www.kaggle.com\/kailex\/group-xgb-for-gstore-v2\n    params['n_thread'] = -1\n    params['max_depth'] = 8\n    params['min_child_weight'] = 100\n    params['gamma'] = 5\n    params['subsample'] = 1\n    params['colsample_bytree'] = .95\n    params['colsample_bylevel'] = 0.35\n    params['alpha'] = 25\n    params['lambda'] = 25\n    \n    xgb_train_data = xgb.DMatrix(X_train, y_train)\n    xgb_val_data = xgb.DMatrix(X_valid, y_valid)\n    \n    model = xgb.train(params, xgb_train_data,\n#           num_boost_round=1000, # V3 : 1.0471 on LB\n#           num_boost_round=200, # 1.0471 on LB\n          num_boost_round = 200,\n          evals= [(xgb_train_data, 'train'), (xgb_val_data, 'valid')],\n#           early_stopping_rounds=10, # V11 0.9301 on LB\n          early_stopping_rounds=50, \n          verbose_eval=20\n         )\n    return model\n\ndef submit():\n    test_matrix = xgb.DMatrix(X_test)\n    y_pred = clf.predict(test_matrix,ntree_limit=clf.best_ntree_limit)\n    df_test['PredictedLogRevenue'] = y_pred\n    engineer_prediction\n    print('rmse after engineer prediction')\n    print(rmse(y_pred,df_test['PredictedLogRevenue']))\n    submit = df_test[['PredictedLogRevenue','fullVisitorId']].groupby('fullVisitorId').PredictedLogRevenue.sum().reset_index()\n    submit.to_csv('submit.csv',index=False)\n    \n    test(y_pred)\n    \n    \ndef engineer_prediction(df_test):\n    df_test[df_test['totals_hits'] == 1].PredictedLogRevenue = 0\n    df_test[df_test['totals_timeOnSite'] == 0].PredictedLogRevenue = 0\n    df_test[df_test['totals_bouces'] == 1].PredictedLogRevenue = 0\n    return dftest\n    \n    \ndef test(predict):\n    y_test = np.log1p(df_test['totals_transactionRevenue'])\n    print(rmse(y_test,predict))\n","3df93fdf":"def prepare_data(df_train,df_test,\n                 del_col=['fullVisitorId','visitId','visitStartTime','date'],to_log=None):\n    df_train = df_train.sort_values('date').copy()\n    \n    df_train = df_train.drop(del_col,axis=1).copy()\n    df_test = df_test.drop(del_col,axis=1).copy()\n    \n    # Log some column\n    if to_log is not None:\n        df_train[to_log] = np.log1p(df_train[to_log])\n        df_test[to_log] = np.log1p(df_test[to_log])\n    \n    # totals_transactionRevenue\n    df_train['totals_transactionRevenue'] = np.log1p(df_train['totals_transactionRevenue'])\n    df_test['totals_transactionRevenue'] = np.log1p(df_test['totals_transactionRevenue'])\n    \n    global X_train,X_valid,y_train,y_valid,X_test,y_test\n    # 80\/20 : train\/valid\n    n_train = int(len(df_train)*.8)\n    \n    # split\n    X_train = df_train.drop(['totals_transactionRevenue'],axis=1).iloc[:n_train]\n    X_valid = df_train.drop(['totals_transactionRevenue'],axis=1).iloc[n_train:]\n    \n    y_train = df_train['totals_transactionRevenue'].iloc[:n_train]\n    y_valid = df_train['totals_transactionRevenue'].iloc[n_train:]\n    \n    X_test = df_test.drop(['totals_transactionRevenue'],axis=1)\n    y_test = df_test['totals_transactionRevenue']\n    \n    ","558c58b3":"def feature_engineering(df):\n    df = df.copy()\n    # Copy from : https:\/\/www.kaggle.com\/qnkhuat\/base-model-v2-with-with-full-features\/edit\n    \n    # time based\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    df['browser_category'] = df['device_browser'] + '_' + df['device_deviceCategory']\n    df['browser_operatingSystem'] = df['device_browser'] + '_' + df['device_operatingSystem']\n\n    df['month_unique_user_count'] = df.groupby('month')['fullVisitorId'].transform('nunique')\n    df['day_unique_user_count'] = df.groupby('day')['fullVisitorId'].transform('nunique')\n    df['weekday_unique_user_count'] = df.groupby('weekday')['fullVisitorId'].transform('nunique')\n    df['weekofyear_unique_user_count'] = df.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n    \n    \n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean')\n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean')\n    \n    df['mean_hits_per_day'] = df.groupby(['day'])['totals_hits'].transform('mean')\n    df['sum_hits_per_day'] = df.groupby(['day'])['totals_hits'].transform('sum')\n    \n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean')\n\n    df['sum_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('sum')\n    df['count_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('count')\n    df['mean_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('mean')\n    \n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean')\n\n    df['sum_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('sum')\n    df['count_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('count')\n    df['mean_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('mean')\n\n    df['sum_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('sum')\n    df['count_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('count')\n    df['mean_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('mean')\n    \n    df['user_pageviews_sum'] = df.groupby('fullVisitorId')['totals_pageviews'].transform('sum')\n    df['user_hits_sum'] = df.groupby('fullVisitorId')['totals_hits'].transform('sum')\n    \n    df['user_pageviews_count'] = df.groupby('fullVisitorId')['totals_pageviews'].transform('count')\n    df['user_hits_count'] = df.groupby('fullVisitorId')['totals_hits'].transform('count')\n\n    \n    df['user_pageviews_sum_to_mean'] = df['user_pageviews_sum'] \/ df['user_pageviews_sum'].mean()\n    df['user_hits_sum_to_mean'] = df['user_hits_sum'] \/ df['user_hits_sum'].mean()\n\n    df['user_pageviews_to_region'] = df['user_pageviews_sum'] \/ df['mean_pageviews_per_region']\n    df['user_hits_to_region'] = df['user_hits_sum'] \/ df['mean_hits_per_region']\n    \n    return df","2c881866":"df_train = feature_engineering(df_train)\ndf_test = feature_engineering(df_test)","f0ffbd4d":"encode_data(verbose=True)\nprepare_data(df_train_encoded,df_test_encoded,del_col=['fullVisitorId','visitId',\n            'visitStartTime','date','totals_transactions','totals_totalTransactionRevenue'])\n","41ccea74":"# clf = run_xgb()","3b6b3e15":"# try to find a good validation set\n# Why our score so different with the leader board?\n# check with the target in dataset first","742645f7":"gs_params = {\n    'max_depth':[3,5,7,10,15,20],\n    'learning_rate':[.01,.1,.5],\n    'n_estimators':[10,50,100,150,200],\n    'n_jobs':[-1],\n    'gamma':[0,5],\n    'min_child_weight':[1,5,7],\n    'subsample': [0.6,1], # sample of rows\n    'colsample_bytree': [0.5,1], # Subsample ratio of columns when constructing each tree.\n    'colsample_bylevel':[0.35,.5,.7],# Subsample ratio of columns for each split, in each level.\n    'reg_alpha':[1,5,10,25],\n    'reg_lambda':[1,5,10,25],\n    'objective':['reg:linear'],\n}\n","18970b47":"fit_params = {\n    'num_boost_round':200,\n    'early_stopping_rounds':50, \n    'verbose_eval':20,\n    \n    \n}\nmodel = xgb.XGBRegressor()\nmodel_cv = model_selection.GridSearchCV(model,param_grid=gs_params,fit_params= fit_params,scoring = 'neg_mean_squared_error',cv=5)\n","146e74d6":"%%time\nbest = model_cv.fit(X_train,y_train,eval_metric='rmse')","0026c205":"best.best_params_","a63ec6d0":"# xgb.plot_importance(clf,importance_type='gain',max_num_features=20)\n# plt.title('Gain Feature important')","85635b6d":"# xgb.plot_importance(clf,importance_type='cover',max_num_features=20)\n# plt.title('Cover Feature important')","128117d5":"# xgb.plot_importance(clf,importance_type='weight',max_num_features=20)\n# plt.title('Weight Feature important')","0c3494dc":"# submit()","947c4718":"# Base model","c6da5186":"# Feature important","b5f1f48a":"\"weight\" is the number of times a feature appears in a tree","b15b0f3b":"# Import and load data","eb2970f0":"\"gain\" is the average gain of splits which use the feature","bbbe746b":"\"cover\" is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split","05db5194":"# GRID SEARCH","7ffd0bc2":"Data are generated from this script : https:\/\/www.kaggle.com\/qnkhuat\/make-data-ready","da5803e6":"# Submit"}}