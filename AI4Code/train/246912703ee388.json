{"cell_type":{"704b4874":"code","e5721057":"code","90e06235":"code","fae94f08":"code","abf2532c":"code","df4601b2":"code","0645aca0":"code","b412a288":"code","59867420":"code","db357a06":"code","8011e157":"code","6f7da7a8":"code","3563c6af":"code","afd89542":"code","c86951dd":"code","105a80dd":"code","f90f329d":"code","ef9d7c06":"code","55e79679":"code","28c7ffeb":"markdown","29e3981c":"markdown","3974937d":"markdown","a625f78c":"markdown","5983a9ad":"markdown","da0ee353":"markdown","78ef2a66":"markdown","c8ecef4f":"markdown","539f2d2f":"markdown","87e4c224":"markdown","34db5deb":"markdown"},"source":{"704b4874":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5721057":"df = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\nprint(df.shape)\ndf.head()","90e06235":"df.describe()","fae94f08":"df.isna().sum()","abf2532c":"df['MINIMUM_PAYMENTS'].fillna(df['MINIMUM_PAYMENTS'].mean(skipna=True), inplace=True)\ndf['CREDIT_LIMIT'].fillna(df['CREDIT_LIMIT'].mean(skipna=True), inplace=True)","df4601b2":"df.isna().sum()","0645aca0":"columns = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT','PAYMENTS', 'MINIMUM_PAYMENTS'] # All features with outlandish values\n\nfor c in columns:    \n    Range = c+'_RANGE'\n    df[Range]=0        \n    df.loc[((df[c]>0)&(df[c]<=500)),Range]=1\n    df.loc[((df[c]>500)&(df[c]<=1000)),Range]=2\n    df.loc[((df[c]>1000)&(df[c]<=3000)),Range]=3\n    df.loc[((df[c]>3000)&(df[c]<=5000)),Range]=4\n    df.loc[((df[c]>5000)&(df[c]<=10000)),Range]=5\n    df.loc[((df[c]>10000)),Range]=6","b412a288":"columns=['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'PRC_FULL_PAYMENT']\n\nfor c in columns: \n    Range=c+'_RANGE'\n    df[Range]=0\n    df.loc[((df[c]>0)&(df[c]<=0.1)),Range]=1\n    df.loc[((df[c]>0.1)&(df[c]<=0.2)),Range]=2\n    df.loc[((df[c]>0.2)&(df[c]<=0.3)),Range]=3\n    df.loc[((df[c]>0.3)&(df[c]<=0.4)),Range]=4\n    df.loc[((df[c]>0.4)&(df[c]<=0.5)),Range]=5\n    df.loc[((df[c]>0.5)&(df[c]<=0.6)),Range]=6\n    df.loc[((df[c]>0.6)&(df[c]<=0.7)),Range]=7\n    df.loc[((df[c]>0.7)&(df[c]<=0.8)),Range]=8\n    df.loc[((df[c]>0.8)&(df[c]<=0.9)),Range]=9\n    df.loc[((df[c]>0.9)&(df[c]<=1.0)),Range]=10","59867420":"columns=['PURCHASES_TRX', 'CASH_ADVANCE_TRX']  \n\nfor c in columns:\n    \n    Range=c+'_RANGE'\n    df[Range]=0\n    df.loc[((df[c]>0)&(df[c]<=5)),Range]=1\n    df.loc[((df[c]>5)&(df[c]<=10)),Range]=2\n    df.loc[((df[c]>10)&(df[c]<=15)),Range]=3\n    df.loc[((df[c]>15)&(df[c]<=20)),Range]=4\n    df.loc[((df[c]>20)&(df[c]<=30)),Range]=5\n    df.loc[((df[c]>30)&(df[c]<=50)),Range]=6\n    df.loc[((df[c]>50)&(df[c]<=100)),Range]=7\n    df.loc[((df[c]>100)),Range]=8","db357a06":"df.drop(['CUST_ID', 'BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_FREQUENCY',  'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT' ], axis=1, inplace=True)\n\nX= np.asarray(df)\ndf.head()","8011e157":"columns = df.columns\nfor c in columns:\n    if c == 'TENURE':\n        continue\n    large_values = dict(df[c].value_counts())\n    lists = sorted(large_values.items())\n    x, y = zip(*lists)\n    plt.title(c)\n    plt.plot(x, y)\n    plt.show()","6f7da7a8":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX.shape","3563c6af":"clusters = 25\ncost = []\nfor i in range(1,clusters):\n    kmeans = KMeans(i)\n    kmeans.fit(X)\n    cost.append(kmeans.inertia_)","afd89542":"plt.plot(cost, 'ro-')","c86951dd":"kmeans = KMeans(6)\nkmeans.fit(X)\nlabels = kmeans.labels_","105a80dd":"clusters = pd.concat([df, pd.DataFrame({'cluster':labels})], axis=1)\nclusters","f90f329d":"pca = PCA(2)\nprincipalComponents = pca.fit_transform(X)\nx, y = principalComponents[:, 0], principalComponents[:, 1]\nprint(principalComponents.shape)\n\ncolors = {0: 'red', 1: 'blue', 2: 'green', 3: 'yellow', 4: 'orange', 5:'purple'}","ef9d7c06":"final_df = pd.DataFrame({'x': x, 'y':y, 'label':labels}) \ngroups = final_df.groupby(labels)","55e79679":"fig, ax = plt.subplots(figsize=(15, 10)) \n\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5, color=colors[name], mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n    ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n    \nax.set_title(\"Customers Segmentation based on their Credit Card usage bhaviour.\")\nplt.show()","28c7ffeb":"Preprocessing the data first.","29e3981c":"The above graphs show that the frequencies for lower values are high since most values in the data are small. This is evident from the **minimum, 1st quartile, median 3rd quartile and maximum values** from the data distribution we obtained by *df.describe()*. This process however took a certain number of trials but didn't consume much time. Now we normalize all the values to adjust them in the range of 0-1.","3974937d":"The output of this step is a 'cluster' variable, which contains the cluster number for each record\/row of the dataset. Let us add this variable at the end of the dataset.","a625f78c":"In order to visualize the clusters created and see if they're well-defined, we need to reduce the dimensionality of the data since it's difficult to visualize n-dimensional data in 2 dimensional space. However, while reducing the dimensionality of the data,we want to make sure that we capture as many features of the original dataset as possible. For this, we use Principal Component Analysis (PCA), which helps us to achieve the objective mentioned above.","5983a9ad":"Only 2 columns have null values. The missing values are small fraction of the entire dataset (1\/8950) and (313\/8950) and hence can be easily imputed. We'll impute CREDIT_LIMIT with mean value and since MINIMUM_PAYMENTS is a continuous variable skewed towards the lower side, we can impute it with either the mean or median. It shouldn't make much of a difference since this the fraction of missing values is quite small. We'll go with imputing with mean values.","da0ee353":"Since we have modified all the exisitng feature names, we can delete the existing feature names.","78ef2a66":"We seem to reach an inflection point when K = 6. After this value of K, the cost decreases very slowly.","c8ecef4f":"For all the columns\/features, we see that the standard deviation is quite high and the min and max values are too far apart with the distribution being skewed towards lower values as can be seen from the **75% mark, mean and max**. This means that the given dataset has a lot of outliers and these outliers need to be dealt with. Simply ignoring the outliers will result in quite a lot of data loss","539f2d2f":"Since there are too many outliers, let's convert the entire dataset's values to categorical values. Since we are interested in finding similarities through clusters, it is a good idea to group values in a particular range and assigning them a category. Later we'll normalize these category values as well to make sure that no large value in any column dominates\/skews the clustering result.","87e4c224":"Finally, we plot all the clusters as various subplots inside a single plot.","34db5deb":"Now that we have converted data from continuous to discrete values and brought it down to a particular range, we've made sure that we give equal importance to all the features. Now, we can go ahead and apply K-means. Let's use **Elbow Method** to choose an optimal value of K."}}