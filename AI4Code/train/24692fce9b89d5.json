{"cell_type":{"669a77c3":"code","c417ba50":"code","99459c9f":"code","37193c5e":"code","c7e918eb":"code","e1e1ff6b":"code","c80f727f":"code","00f61171":"code","ca057010":"code","56a5d609":"code","f803901f":"code","be0c26c7":"code","a68b916d":"code","85d79f4c":"code","fc0e8664":"code","d1746e93":"code","4a958f0c":"code","c0da10b5":"code","6a5a20b3":"code","ec3f4b68":"code","b7ca8b4e":"code","17b2621a":"code","5b7d20e4":"code","76d3f92b":"code","1e9bce14":"code","c7e9bc28":"code","70fb2158":"code","3de7a17d":"code","51f84c5d":"code","0e5c3388":"code","91da5628":"code","7b276ebe":"code","439d580c":"code","c6cae029":"code","06f6e8fb":"code","1346795c":"markdown","60320a90":"markdown","ac6b8f64":"markdown","bb5d20b7":"markdown","7f3b09be":"markdown","67d21cfc":"markdown","1a6ee9dc":"markdown","366fff6b":"markdown","6095c7fb":"markdown","76f5179c":"markdown","693830cf":"markdown","1dd504db":"markdown","9dc5d8a0":"markdown","34cca6cd":"markdown","b58dd9b2":"markdown","198d43ad":"markdown","5ea5ebd5":"markdown","bb002132":"markdown","a54bb96b":"markdown","07377f5e":"markdown","a3a514cf":"markdown","f7a19476":"markdown","b7fe5980":"markdown","8cc7616a":"markdown","efa285da":"markdown","fd0bcbb6":"markdown","e8d5fac8":"markdown","2b4b366e":"markdown","83f2f154":"markdown","bd6b898c":"markdown","ae8997b4":"markdown","d4aa54cc":"markdown","8d76df3d":"markdown"},"source":{"669a77c3":"pip install spacy","c417ba50":"pip install --pre gql[all]","99459c9f":"#Python\nimport os\nimport sys\nfrom collections import defaultdict\nimport json\n\n#String manipulation\nimport string\nimport re\n\n#GraphQl\nfrom gql import gql, Client\nfrom gql.dsl import DSLQuery, DSLSchema, dsl_gql\nfrom gql.transport.requests import RequestsHTTPTransport\n\n#API Key\nfrom yelp_api_config import Yelp\n\n#Data\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\n#Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import chain\n\n# NLP\nimport nltk\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\n\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\npyLDAvis.enable_notebook()\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim import matutils, models, corpora\nfrom gensim.models import word2vec, LsiModel\nfrom gensim.models.nmf import Nmf\n\n#Spacy\nimport spacy\n\n\n#Configuration\nnltk.download('stopwords', quiet=True)\npd.set_option.display_max_columns = 200\n\n#We retrieve the first 1000000 lines contained\npath_to_file = '..\/input\/yelp-dataset\/yelp_academic_dataset_review.json'\ndf = pd.read_json(path_to_file, lines=True, nrows = 1000000)","37193c5e":"#Remove warnings for pyLDAvis\nimport warnings\n\nwarnings.filterwarnings('ignore')","c7e918eb":"#Convert list of list to list.\ndef list_list_to_list(x):\n    return [item for sublist in x for item in sublist]","e1e1ff6b":"#Histplot\ndef plot_hist_stats(df, feature, title=None, verticale=False):\n    \"\"\"Affiche l'histogramme d'une caract\u00e9ristique\"\"\"\n    tmp = df[feature].value_counts()\n    df_tmp = pd.DataFrame({ \n                            feature: tmp.index,\n                            'Quantity': tmp.values\n                          })\n\n    fig, ax = plt.subplots(figsize=(15,6))\n    sns.set_color_codes(\"pastel\")\n\n    s = sns.barplot(ax = ax,\n                    x = feature,\n                    y = 'Quantity',\n                    data = df_tmp)\n   \n    if title != None:\n        plt.title(title)\n\n    if verticale == True:\n        plt.xticks(rotation=90)\n\n    plt.tick_params(axis='both', which='major', labelsize=10)\n\n    plt.show();\n    \ndef plot_coherence_values(df_coherence):\n        \"\"\"\n        plot c_v coherence for various number of topics\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(12,6))\n        sns.set_color_codes(\"pastel\")\n    \n        s = sns.lineplot(ax = ax, x = 'nb_of_topics', y = 'coherence_value', data = df_coherence)\n        plt.title('Coherence score evolution with number of topics')\n        plt.show()","c80f727f":"class NLP:\n    \n    def __init__(self, df, type_of_reviews=0):\n        \"\"\" path_to_file = Path of your file.csv\n            type_of_reviews = [0, 1] by default : 0 keep bad reviews only, else good reviews\n        \"\"\"\n        if type_of_reviews not in [0, 1]:\n            print('[Syntax ERROR] : type_of_reviews need to be set to 0 or 1', file=sys.stderr)\n            sys.exit(-1)\n        \n        self.path_to_file = path_to_file \n        self.type_of_reviews = type_of_reviews\n        self.models_best_coherence_score = {}\n        self.df_sample = df\n        \n        #Filtered reviews\n        self.df_reviews_filtered = self.filter_review_type(self.df_sample, self.type_of_reviews)\n        \n        #Preprocessing\n        self.cleaning()\n        \n        #Tokenize\n        self.tokenize()\n        \n        #Final Preparation \n        self.df['data_ready_for_model'] = self.process_words(self.df['clean_text'].to_list())\n\n    def clean_data_rdy(self, data_ready_for_model):\n        dictionary = gensim.corpora.Dictionary(data_ready_for_model)\n        count = 0\n        \n        for k, v in dictionary.iteritems():\n            count += 1\n            if count > 10:\n                break\n                \n        dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n        return dictionary\n    \n    def LDA_TF_IDF(self, start, limit):\n        # Create Dictionary\n        id2word = self.clean_data_rdy(self.df['data_ready_for_model'])\n\n        # Create Corpus: Term Document Frequency\n        corpus = [id2word.doc2bow(text) for text in self.df['data_ready_for_model']]\n        \n        tfidf = models.TfidfModel(corpus)\n        corpus_tfidf = tfidf[corpus]\n        \n        coherence = []\n        for k in list(range(start, limit, 1)):         \n            ldamodel_tf_idf = gensim.models.LdaMulticore(corpus_tfidf,\n                                                         num_topics=k,\n                                                         id2word = id2word,\n                                                         iterations=200,\n                                                         passes=2,\n                                                         workers=4)\n            \n            cm = gensim.models.coherencemodel.CoherenceModel(model = ldamodel_tf_idf, \n                                                             texts = self.df['data_ready_for_model'],\n                                                             dictionary = id2word,\n                                                             coherence='c_v')\n\n            coherence.append((k,cm.get_coherence()))\n        \n        #Create dataframe with coherence value for each number of topic\n        df_coherence = pd.DataFrame(coherence, columns=['nb_of_topics', 'coherence_value'])\n        \n        #Select best number of topic\n        best_num_topic = df_coherence['nb_of_topics'][df_coherence['coherence_value'].idxmax()]\n        self.models_best_coherence_score['LDA_TF_IDF'] = df_coherence['coherence_value'].max()\n        \n        #Plot\n        plot_coherence_values(df_coherence)\n        \n        lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                                     num_topics=best_num_topic,\n                                                     id2word=id2word,\n                                                     iterations=200,\n                                                     passes=2,\n                                                     workers=4)\n        #visualization\n        visualisation = gensimvis.prepare(lda_model_tfidf, corpus, id2word)\n        \n        return lda_model_tfidf, visualisation\n    \n        \n    def LDA(self, start, limit):\n        \"\"\"\n        Compute c_v coherence for various number of topics\n        \"\"\"        \n        # Create Dictionary\n        id2word = self.clean_data_rdy(self.df['data_ready_for_model'])\n\n        # Create Corpus: Term Document Frequency\n        corpus = [id2word.doc2bow(text) for text in self.df['data_ready_for_model']]\n        \n        coherence = []\n        for k in list(range(start, limit, 1)):\n            Lda = gensim.models.ldamodel.LdaModel\n            \n            ldamodel = Lda( corpus,\n                            num_topics=k,\n                            id2word = id2word,\n                            iterations=200)\n            \n            cm = gensim.models.coherencemodel.CoherenceModel(model = ldamodel, \n                                                             texts = self.df['data_ready_for_model'],\n                                                             dictionary = id2word,\n                                                             coherence='c_v')\n\n            coherence.append((k,cm.get_coherence()))\n        \n        #Create dataframe with coherence value for each number of topic\n        df_coherence = pd.DataFrame(coherence, columns=['nb_of_topics', 'coherence_value'])\n        \n        #Select best number of topic\n        best_num_topic = df_coherence['nb_of_topics'][df_coherence['coherence_value'].idxmax()]\n        self.models_best_coherence_score['LDA'] = df_coherence['coherence_value'].max()\n        \n        #Plot\n        plot_coherence_values(df_coherence)\n        \n        \n        #Build LDA model\n        lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n                                                    id2word = id2word,\n                                                    num_topics = best_num_topic,\n                                                    iterations=200)\n        \n        #visualization\n        visualisation = gensimvis.prepare(lda_model, corpus, id2word)\n        \n        return lda_model, visualisation\n    \n    def LSI(self, start, limit):\n        \"\"\"\n        Compute c_v coherence for various number of topics\n        \"\"\"        \n        # Create Dictionary\n        id2word = self.clean_data_rdy(self.df['data_ready_for_model'])\n\n        # Create Corpus: Term Document Frequency\n        corpus = [id2word.doc2bow(text) for text in self.df['data_ready_for_model']]\n        \n        coherence = []\n        for k in list(range(start, limit, 1)):\n            lsimodel = LsiModel( corpus,\n                            num_topics=k,\n                            id2word = id2word,\n                            decay=0.5)\n            \n            cm = gensim.models.coherencemodel.CoherenceModel(model = lsimodel, \n                                                             texts = self.df['data_ready_for_model'],\n                                                             dictionary = id2word,\n                                                             coherence='c_v')\n\n            coherence.append((k,cm.get_coherence()))\n        \n        #Create dataframe with coherence value for each number of topic\n        df_coherence = pd.DataFrame(coherence, columns=['nb_of_topics', 'coherence_value'])\n        \n        #Select best number of topic\n        best_num_topic = df_coherence['nb_of_topics'][df_coherence['coherence_value'].idxmax()]\n        self.models_best_coherence_score['LSI'] = df_coherence['coherence_value'].max()\n        \n        #Plot\n        plot_coherence_values(df_coherence)\n        \n        \n        #Build LSI model\n        lsi_model = LsiModel(corpus = corpus,\n                            id2word = id2word,\n                            num_topics = best_num_topic,                                    \n                            decay = 0.5)\n        \n        return lsimodel\n\n    def NMF(self, start, limit):\n        \"\"\"\n        Compute c_v coherence for various number of topics\n        \"\"\"        \n        # Create Dictionary\n        id2word = self.clean_data_rdy(self.df['data_ready_for_model'])\n\n        # Create Corpus: Term Document Frequency\n        corpus = [id2word.doc2bow(text) for text in self.df['data_ready_for_model']]\n        \n        coherence = []\n        for k in list(range(start, limit, 1)):\n            nmf_model = Nmf(corpus = corpus,\n                            id2word = id2word,\n                            num_topics=k)\n            \n            cm = gensim.models.coherencemodel.CoherenceModel(model = nmf_model, \n                                                             texts = self.df['data_ready_for_model'],\n                                                             dictionary = id2word,\n                                                             coherence='c_v')\n\n            coherence.append((k,cm.get_coherence()))\n        \n        #Create dataframe with coherence value for each number of topic\n        df_coherence = pd.DataFrame(coherence, columns=['nb_of_topics', 'coherence_value'])\n        \n        #Select best number of topic\n        best_num_topic = df_coherence['nb_of_topics'][df_coherence['coherence_value'].idxmax()]\n        self.models_best_coherence_score['NMF'] = df_coherence['coherence_value'].max()\n        \n        #Plot\n        plot_coherence_values(df_coherence)\n        \n        \n        #Build nmf model\n        nmf_model = Nmf(corpus = corpus,\n                        id2word = id2word,\n                        num_topics = best_num_topic)\n        \n        return nmf_model\n    \n    def cleaning(self):\n        \"\"\" This function is the preprocessing function for our reviews.\n        \n        It remove all empty reviews,\n        convert all our text in lowercase, remove punctuation, finally\n        remove newlines, remove stop_words and return a Dataframe with all\n        the modifications\n        \"\"\"\n        \n        #remove empty reviews\n        not_empty = [x for x in self.df_reviews_filtered[\"text\"].tolist() if len(x) != 0]\n        \n        #Lowercase\n        lower = [x.lower() for x in not_empty]\n        \n        #Suppress Digits\n        remove_digit = str.maketrans('', '', string.digits)\n        no_digit = [x.translate(remove_digit) for x in lower]\n        \n        #Suppress punctuation\n        translator = str.maketrans('', '', string.punctuation)\n        punctuation = [x.translate(translator) for x in no_digit]\n        \n        #Suppress newlines\n        reviews_prepro = [x.replace('\\n', '') for x in punctuation]\n            \n        #Create a new DF for our class with all the modification\n        self.df = pd.DataFrame({\n            'text': not_empty,\n            'clean_text' : reviews_prepro\n        })\n    \n    def filter_review_type(self, df, val):\n        if val == 0:\n            return df[df['stars'] < 3]\n        return df[df['stars'] > 3]\n        \n    #Tokenization\n    def tokenize(self):\n        tokenizer = nltk.RegexpTokenizer(r'\\w+')\n        self.df['text_tokenize'] = [tokenizer.tokenize(x) for x in self.df['clean_text'].to_list()]\n\n        \n    #Plots\n    def dist_plot_nb_characters_and_words(self):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n        sns.set_style('darkgrid')\n        \n        list_nb_characters = [len(x) for x in self.df['clean_text']]\n        list_nb_of_words = [len(x.split()) for x in self.df['clean_text']]\n        \n        ax1.set_title('number of characters per clean review distribution')\n        sns.histplot(list_nb_characters, ax = ax1)\n        \n        ax2.set_title('number of words per clean review distribution')\n        sns.histplot(list_nb_of_words, ax = ax2)\n        \n        plt.show()\n        \n    def plot_freqDist(self):\n        ## Creating FreqDist,keeping the 30 most common tokens\n        fdist_sw = FreqDist(list_list_to_list(self.df['text_tokenize'].to_list())).most_common(30)\n        fdist_wout_sw = FreqDist(list_list_to_list(self.df['data_ready_for_model'].to_list())).most_common(30)\n        \n        \n        ## Conversion to Pandas series via Python Dictionary for easier plotting\n        df_fdist_sw = pd.Series(dict(fdist_sw))\n        df_fdist_wout_sw = pd.Series(dict(fdist_wout_sw))\n\n        ## Setting figure, ax into variables\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n\n        ## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n        ax1.set_title('Most commons words with stop words')\n        sns.barplot(x=df_fdist_sw.values, y=df_fdist_sw.index, ax=ax1)\n        \n        ax2.set_title('Most commons words for our model ready text')\n        sns.barplot(x=df_fdist_wout_sw.values, y=df_fdist_wout_sw.index, ax=ax2)\n        \n        plt.show()\n        \n    \n    def process_words(self, texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n        #Vars Initialisation\n        stop_words = self.remove_stop_words()\n        lemma = WordNetLemmatizer()\n        \n        bigram = gensim.models.Phrases(texts, min_count=5, threshold=100) # higher threshold fewer phrases.\n        trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n        bigram_mod = gensim.models.phrases.Phraser(bigram)\n        trigram_mod = gensim.models.phrases.Phraser(trigram)\n        \n        \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n        texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n        texts = [bigram_mod[doc] for doc in texts]\n        texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n        \n        texts_out = []\n        nlp = spacy.load('en', disable=['parser', 'ner'])\n        for sent in texts:\n            doc = nlp(\" \".join(sent)) \n            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n\n        # remove stopwords once more after lemmatization\n        result = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]  \n        return result\n\n    def remove_stop_words(self):\n        sw = stopwords.words(\"english\")\n        \n        #Extended list of stopwords\n        sw.extend(['yelp', 'star', 'review',\n                   'friend', 'husband', 'feel',\n                   'good', 'great', 'love',\n                   'live', 'let', 'try', \n                   'disappointed', 'horrible', 'bad',\n                   'terrible','extremely', 'awful',\n                   'nice', 'usually','know', \n                   'want', 'think', 'decide', \n                   'actually','come', 'finally', \n                   'need', 'start', 'end',\n                   'year', 'month',\n                   'day', 'minute', 'hour',\n                   'tell', 'ask', 'thing'])\n        \n        #Combine nltk stopwords list with gensim\n        all_stopwords = STOPWORDS.union(set(sw))\n    \n        return all_stopwords","00f61171":"sample_reviews = df.sample(n = 500000)\nsample_reviews.head(3)","ca057010":"plot_hist_stats(sample_reviews, 'stars')","56a5d609":"sample_reviews = sample_reviews.loc[sample_reviews.stars != 3]","f803901f":"sample_reviews['positive'] = np.where(sample_reviews['stars'] > 3, True, False)\nsample_reviews['positive'].value_counts()","be0c26c7":"nlp = NLP(sample_reviews, 0)","a68b916d":"nlp.df.head(5)","85d79f4c":"print(nlp.df.text.iloc[0])","fc0e8664":"print(nlp.df.clean_text.iloc[0])","d1746e93":"print(' '.join(nlp.df.data_ready_for_model.iloc[0]))","4a958f0c":"nlp.dist_plot_nb_characters_and_words()","c0da10b5":"nlp.plot_freqDist()","6a5a20b3":"LDA_model, lda_visu = nlp.LDA(4,11)\n\nfor idx, topic in LDA_model.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","ec3f4b68":"lda_visu","b7ca8b4e":"TF_IDF_LDA_model, lda_tf_idf_visu = nlp.LDA_TF_IDF(4, 11)\n\nfor idx, topic in TF_IDF_LDA_model.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","17b2621a":"lda_tf_idf_visu","5b7d20e4":"LSI_model = nlp.LSI(4, 11)\n\nfor idx, topic in LSI_model.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","76d3f92b":"nmf_model = nlp.NMF(4, 11)\n\nfor idx, topic in nmf_model.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","1e9bce14":"pd.DataFrame.from_dict(nlp.models_best_coherence_score,\n                       orient='index',\n                       columns=['Best_coherence_score']).sort_values(by=['Best_coherence_score'])","c7e9bc28":"header = {\n    'Authorization': 'bearer {}'.format(Yelp.config('api_key')),\n    'Content-Type':\"application\/json\"\n}\n\ntransport = RequestsHTTPTransport(url = Yelp.config('url'),\n                                  headers = header,\n                                  use_json = True)\n\nclient = Client(transport = transport, fetch_schema_from_transport = True)\n\ncities = ['Paris', 'Bordeaux', \n           'Lyon', 'Vannes',\n           'Marseille', 'Nice',\n           'Lille', 'Grenoble',\n           'Cannes', 'Limoges',\n           'Versailles', 'Nice',\n           'Nantes']\n\nresults = []\nfor city in cities :\n    query = gql(\"\"\"\n            query RequestForEachCity($city: String!) {\n                search(term : \"restaurant\", location : $city, limit : 50) {\n                    business{\n                        name\n                        id\n                        reviews {\n                            id\n                            text\n                            rating\n                        }\n                    }\n                }\n            }\"\"\")\n    \n    params = {'city': city}\n    results.append(client.execute(query, variable_values=params))","70fb2158":"dict_reviews = []\n\nfor res in results:\n    for val in res['search']['business']:\n        for reviews in val['reviews']:\n            dict_reviews.append({\n                'review_id' : reviews['id'],\n                'text' : reviews['text'],\n                'stars' : reviews['rating']\n            })","3de7a17d":"df_yelp = pd.DataFrame.from_dict(dict_reviews)\ndf_yelp = df_yelp.dropna()","51f84c5d":"df_yelp.head(5)","0e5c3388":"nlp_yelp = NLP(df_yelp, 0)","91da5628":"nlp_yelp.df.head(5)","7b276ebe":"def topics_prediction(model, text):\n    id2word = nlp.clean_data_rdy(nlp.df['data_ready_for_model'])\n    \n    labels = ['Coffee',\n              'Delivery',\n              'Hotel',\n              'Beauty Salon',\n              'Car services',\n              'Restaurant',\n              'Noddles Restaurant',\n              'Fast Food']\n    \n    corpus = id2word.doc2bow(text)\n    prob_topics = model.get_document_topics(corpus)\n    max_index = max(prob_topics, key=lambda x: x[1])\n\n    return labels[max_index[0]]","439d580c":"print('Review : ', nlp_yelp.df['text'].iloc[55])\nres = topics_prediction(TF_IDF_LDA_model, nlp_yelp.df['data_ready_for_model'].iloc[55])\nprint ('Prediction :', res)","c6cae029":"print('Review : ', nlp_yelp.df['text'].iloc[13])\nres = topics_prediction(TF_IDF_LDA_model, nlp_yelp.df['data_ready_for_model'].iloc[13])\nprint ('Prediction :', res)","06f6e8fb":"df_yelp.to_csv(r'reviews_yelp.csv', index = False)","1346795c":"# 2. **Exploratory Analysis**\n## 2.1 **Sample**","60320a90":"### 2.3.2 **Distribution**","ac6b8f64":"# 1. **Utilities**\n## 1.1 **Functions**","bb5d20b7":"### 2.3.1.3 **Lemmatization**","7f3b09be":"## 2.3 **NLP Class Initialisation**","67d21cfc":"## 3.5 **Training Results**","1a6ee9dc":"## 4.2 **Transform Reviews into Dataframe**","366fff6b":"# PIP\n*****************","6095c7fb":"# Imports & Configuration\n****************","76f5179c":"We observe that about **~25% of the reviews are negatives**.","693830cf":"# 3. **Models**\n## 3.1 **Latent Dirichlet Allocation**","1dd504db":"### 1.1.2 **Natural language processing Class**","9dc5d8a0":"LDA_TF_IDF got the best score with : 0.559040","34cca6cd":"### 2.3.1.2 **Review preprocessed**","b58dd9b2":"As we can see, we have more positives reviews than negatives one. Let's assume that **1 and 2 stars are reviews are associate with negative sentiment, 3 stars neutral reviews, 4 and 5 star reviews are associate with a positive sentiment**.\nFor simplicity purposes let's remove neutral reviews...","198d43ad":"## 3.4 **Non-Negative Matrix Factorization**","5ea5ebd5":"## 3.2 **Latent Dirichlet Allocation TF-IDF**","bb002132":"### 4.2.3 **Preprocess and Explore reviews**","a54bb96b":"## 3.5.2 **Selection of our model based on coherence score**","07377f5e":"# 4. **YELP API**\n## 4.1 **Request**","a3a514cf":"First, we collect 500 000 individuals in our sample of 1 000 000","f7a19476":"### 4.2.5 **Extract reviews to .csv**","b7fe5980":"let's see what our reviews looks like for each steps","8cc7616a":"let's add a new column in our dataframe : 'positive'.\nIf the review is associate with a positive sentiment it's **TRUE** else it will be **FALSE**","efa285da":"## 2.2 **Distributions**","fd0bcbb6":"## 3.3 **Latent Semantic Indexation**","e8d5fac8":"### 2.3.1.1 **Basic Review**","2b4b366e":"## 3.5.2 **Topics of bad reviews for our best model**\n![Topics](https:\/\/i.ibb.co\/s3Q7MQB\/Untitled-Diagram.png)","83f2f154":"### 4.2.1 **Creation of our Dataframe**","bd6b898c":"### 4.2.2 **Drop empty reviews**","ae8997b4":"### 1.1.1 **Plots**","d4aa54cc":"### 2.3.1 **Preprocessing and Parse review**\n\nDuring the initialisation nlp class clean all reviews in this order :\n1. **Filter Reviews**\n2. **Remove empty reviews**\n3. **Lowercase**\n4. **Remove punctuation**\n5. **Remove digits**","8d76df3d":"### 4.2.4 **Predictions**"}}