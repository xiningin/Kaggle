{"cell_type":{"eef546be":"code","72978d8e":"code","2362d8fc":"code","c0aa3a94":"code","8a0aa8c3":"code","e3949f22":"code","3f16468d":"code","62752009":"code","9475db6e":"code","f8945c7c":"code","bbebc768":"code","53860621":"code","1923db9c":"code","da0ca9db":"code","95d57ea4":"code","87b4337b":"code","c8e200cf":"code","1e3d2196":"code","0d993e2a":"code","5082b176":"code","b67d8183":"code","e96612bc":"markdown","19446f8c":"markdown","8573cc09":"markdown","f49cad65":"markdown","0b717064":"markdown"},"source":{"eef546be":"from IPython.display import Image\nImage(\"..\/input\/diagram\/diagram.png\")","72978d8e":"import os\nimport glob\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms, utils\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport imgaug as ia\nimport imgaug.augmenters as iaa\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2362d8fc":"import sys\nsys.path.append('..\/input\/efficientnet-pytorch')\nsys.path.append('..\/input\/efficientnet\/EfficientNet-PyTorch-master\/')","c0aa3a94":"path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\ntrain_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\nprint('Num of train samples:', len(train_data))","8a0aa8c3":"train_data.head()","e3949f22":"sometimes = lambda aug: iaa.Sometimes(0.1, aug)\n\nseq = iaa.Sequential(\n    [\n        # apply the following augmenters to most images\n        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n        iaa.Flipud(0.5), # vertically flip 20% of all images\n        # crop images by -5% to 10% of their height\/width\n        sometimes(iaa.CropAndPad(\n            percent=(-0.05, 0.05),\n            pad_mode=ia.ALL,\n            pad_cval=(0, 255)\n        )),\n        sometimes(iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n            rotate=(-45, 45), # rotate by -45 to +45 degrees\n            shear=(-16, 16), # shear by -16 to +16 degrees\n            order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n            cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n        )),\n        # execute 0 to 5 of the following (less important) augmenters per image\n        # don't execute all of them, as that would often be way too strong\n        iaa.SomeOf((0, 5),\n            [\n                sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n                iaa.OneOf([\n                    iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n                    iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n                    iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n                ]),\n                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n                # search either for all edges or for directed edges,\n                # blend the result with the original image using a blobby mask\n                iaa.SimplexNoiseAlpha(iaa.OneOf([\n                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n                    iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n                ])),\n                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n                iaa.OneOf([\n                    iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n                    iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n                ]),\n                iaa.Invert(0.05, per_channel=True), # invert color channels\n                iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n                \n                # either change the brightness of the whole image (sometimes\n                # per channel) or change the brightness of subareas\n                iaa.OneOf([\n                    iaa.Multiply((0.5, 1.5), per_channel=0.5),\n                    iaa.FrequencyNoiseAlpha(\n                        exponent=(-4, 0),\n                        first=iaa.Multiply((0.5, 1.5), per_channel=True),\n                        second=iaa.LinearContrast((0.5, 2.0))\n                    )\n                ]),\n                iaa.LinearContrast((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n                sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n            ],\n            random_order=True\n        )\n    ],\n    random_order=True\n)","3f16468d":"def dicom2array(paths, voi_lut=True, fix_monochrome=True, remove_black_boundary=True, aug = False):\n    \n    for path in paths:\n        dicom = pydicom.read_file(path)\n        # VOI LUT (if available by DICOM device) is used to\n        # transform raw DICOM data to \"human-friendly\" view\n        if voi_lut:\n            data = apply_voi_lut(dicom.pixel_array, dicom)\n        else:\n            data = dicom.pixel_array\n        if data.max() > 0.0: # avoiding black images (if possible)\n            break\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    if remove_black_boundary: # we get slightly more details\n        (x, y) = np.where(data > 0)\n        if len(x) > 0 and len(y) > 0:\n            x_mn = np.min(x)\n            x_mx = np.max(x)\n            y_mn = np.min(y)\n            y_mx = np.max(y)\n            if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n                data = data[:,np.min(y):np.max(y)]\n    data = cv2.resize(data, (512, 512))\n    if aug and random.randint(0,1) == 1: # augmenting only 50% of the time\n        data = seq(images=data)\n    return data\n\ndef load_rand_dicom_images(scan_id, split = \"train\", aug = False):\n    \"\"\"\n    send 4 random slices of each modality\n    \"\"\"\n    if split != \"train\" or split != \"test\":\n        split = \"train\"\n    flair = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/FLAIR\/*.dcm\"))\n    flair_img = dicom2array(random.sample(flair, max(len(flair)\/\/2, 1)), aug = aug)\n    t1w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1w\/*.dcm\"))\n    t1w_img = dicom2array(random.sample(t1w, max(len(t1w)\/\/2, 1)), aug = aug)\n    t1wce = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1wCE\/*.dcm\"))\n    t1wce_img = dicom2array(random.sample(t1wce, max(len(t1wce)\/\/2, 1)), aug = aug)\n    t2w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T2w\/*.dcm\"))\n    t2w_img = dicom2array(random.sample(t2w, max(len(t2w)\/\/2, 1)), aug = aug)\n    \n    return np.array((flair_img, t1w_img, t1wce_img, t2w_img)).T","62752009":"load_rand_dicom_images(\"00000\", aug = True).shape","9475db6e":"def plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(512,512)):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i in range(4):\n        img = imgs[:,:,i]\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","f8945c7c":"slices = load_rand_dicom_images(\"00000\")\nplot_imgs(slices)\naug_slices = load_rand_dicom_images(\"00000\", aug = True)\nplot_imgs(aug_slices)","bbebc768":"# let's write a simple pytorch dataloader\n\n\nclass BrainTumor(Dataset):\n    def __init__(self, path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification', split = \"train\", validation_split = 0.2):\n        # labels\n        train_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n        self.labels = {}\n        brats = list(train_data[\"BraTS21ID\"])\n        mgmt = list(train_data[\"MGMT_value\"])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        if split == \"valid\":\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/train\/\" + \"\/*\"))]\n            self.ids = self.ids[:int(len(self.ids)* validation_split)] # first 20% as validation\n        elif split == \"train\":\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            self.ids = self.ids[int(len(self.ids)* validation_split):] # last 80% as train\n        else:\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            \n    \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        imgs = load_rand_dicom_images(self.ids[idx], self.split, aug = True)\n        \n        transform = transforms.Compose([transforms.ToTensor()]) # transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5))\n        imgs = transform(imgs)\n        \n        imgs = imgs - imgs.min()\n        imgs = (imgs + 1e-5) \/ (imgs.max() - imgs.min() + 1e-5)\n        \n        if self.split != \"test\":\n            label = self.labels[self.ids[idx]]\n            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n        else:\n            return torch.tensor(imgs, dtype = torch.float32)","53860621":"# testing the dataloader\n# testing the dataloader\ntrain_bs = 10\nval_bs = 2\n\ntrain_dataset = BrainTumor()\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True, num_workers = 8)\n\nval_dataset = BrainTumor(split=\"valid\")\nval_loader = DataLoader(val_dataset, batch_size=val_bs, shuffle=True, num_workers = 8)","1923db9c":"for img, label in train_loader:\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break\n\nfor img, label in val_loader:\n    print(img.shape)\n    print(label.shape)\n    break","da0ca9db":"# let's write our simplest cnn\n\nclass SimpleCNN(nn.Module): \n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=9) # 4 incoming channels\n        self.conv2 = nn.Conv2d(64, 32, kernel_size=7)\n        self.conv2_drop = nn.Dropout2d()\n        self.conv3 = nn.Conv2d(32, 16, kernel_size=5)\n        self.conv3_drop = nn.Dropout2d()\n        self.conv4 = nn.Conv2d(16, 8, kernel_size=3)\n        self.conv4_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(200, 256)\n        self.fc2 = nn.Linear(256, 2)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 4))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 4))\n        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n        x = F.relu(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n        x = x.view(x.shape[0],-1)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x","95d57ea4":"from efficientnet_pytorch import EfficientNet\nfrom efficientnet_pytorch.utils import Conv2dStaticSamePadding\n\nPATH = \"..\/input\/efficientnet-pytorch\/efficientnet-b1-dbc7070a.pth\"\nmodel = EfficientNet.from_name('efficientnet-b1')\nmodel.load_state_dict(torch.load(PATH))\n\n# augment model with 4 channels\n\nmodel._conv_stem = Conv2dStaticSamePadding(4, 32, kernel_size = (3,3), stride = (2,2), \n                                                             bias = False, image_size = 512)\n\nmodel._fc = torch.nn.Linear(in_features=1280, out_features=2, bias=True)\n\n# https:\/\/github.com\/zabir-nabil\/Fibro-CoSANet    # will update later        ","87b4337b":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(),lr = 0.001, weight_decay=0.02)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\nn_epochs = 2","c8e200cf":"print(model)\nmodel(torch.randn(1, 4, 512, 512))","1e3d2196":"# helper\ndef one_hot(arr):\n    return [[1, 0] if a_i == 0 else [0, 1] for a_i in arr]","0d993e2a":"# let's train\ngpu = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(gpu)\n\ntrain_loss = []\nval_loss = []\ntrain_roc = []\nval_roc = []\nbest_roc = 0.0\n\nfor epoch in range(n_epochs):  # loop over the dataset multiple times\n    y_all = []\n    outputs_all = []\n    running_loss = 0.0\n    roc = 0.0\n    \n    model.train()\n    for i, data in tqdm(enumerate(train_loader, 0)):\n        x, y = data\n        \n        # x = torch.unsqueeze(x, dim = 1)\n        x = x.to(gpu)\n        y = y.to(gpu)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(x)\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        y_all.extend(y.tolist())\n        outputs_all.extend(outputs.tolist())\n    \n    roc += roc_auc_score(one_hot(y_all), outputs_all) \/ train_bs\n    print(f\"epoch {epoch+1} train loss: {running_loss} train roc: {roc}\")\n    \n    train_loss.append(running_loss)\n    train_roc.append(roc)\n\n    y_all = []\n    outputs_all = []\n    running_loss = 0.0\n    roc = 0.0 \n    \n    model.eval()\n    for i, data in tqdm(enumerate(val_loader, 0)):\n\n        x, y = data\n        \n        # x = torch.unsqueeze(x, dim = 1)\n        x = x.to(gpu)\n        y = y.to(gpu)\n\n        # forward\n        outputs = model(x)\n        loss = criterion(outputs, y)\n\n        # print statistics\n        running_loss += loss.item()\n        y_all.extend(y.tolist())\n        outputs_all.extend(outputs.tolist())\n    \n    roc += roc_auc_score(one_hot(y_all), outputs_all) \/ val_bs\n    scheduler.step(running_loss)\n        \n    print(f\"epoch {epoch+1} val loss: {running_loss} val roc: {roc}\")\n    \n    val_loss.append(running_loss)\n    val_roc.append(roc)\n    \n    if roc > best_roc:\n        best_roc = roc\n        torch.save(model.state_dict(), f'best_roc_{round(roc, 2)}_loss_{round(running_loss, 2)}.pt')","5082b176":"plt.plot(train_loss, label = 'train loss')\nplt.plot(val_loss, label = 'val loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['train loss', 'val loss'])\nplt.show()\n\nplt.plot(train_roc, label = 'train roc')\nplt.plot(val_roc, label = 'val roc')\nplt.xlabel('epochs')\nplt.ylabel('roc auc')\nplt.legend(['train roc', 'val roc'])\nplt.show()","b67d8183":"submission = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","e96612bc":"## **Training**","19446f8c":"## **Augmentation**","8573cc09":"## **Problem Description**:\n\nThere are structural multi-parametric MRI (mpMRI) scans for different subjects, in DICOM format. The exact mpMRI scans included are:\n\n* Fluid Attenuated Inversion Recovery (FLAIR)\n* T1-weighted pre-contrast (T1w)\n* T1-weighted post-contrast (T1Gd)\n* T2-weighted (T2)\n\n`train_labels.csv` - file contains the target MGMT_value for each subject in the training data **(e.g. the presence of MGMT promoter methylation)**.\n\nSo, it's a binary classification problem.\n\n## **A simple solution**:\n\n* For each patient, we consider 4 sequences (FLAIR, T1w, T1Gd, T2), and for each of those sequences take a slice randomly. Idea from [https:\/\/github.com\/zabir-nabil\/Fibro-CoSANet](https:\/\/github.com\/zabir-nabil\/Fibro-CoSANet)\n\n* Construct a 4-channel image out of these 4 sequences.\n\n* Design a 4 channel pytorch model.\n\n* Perform binary classification.\n\n### **Updates** v3:\n\n* Added augmentation. Check out the augmentation notebook: https:\/\/www.kaggle.com\/furcifer\/mri-data-augmentation-pipeline\n* Added few heuristics to avoid black\/empty scans.\n* Added full training script with weights saving.\n* Added modified efficient-net (4 channels) as model.\n\n### **Updates** v4:\n\n* Experimenting with first training without any augmentation, and after few epochs adding augmentation.\n* Training longer\n\n## **Check out my other kernels**\n\n### \u26a1 **Training kernel:** https:\/\/www.kaggle.com\/furcifer\/torch-efficientnet3d-for-mri-no-train\/\n\n### \u26a1 **Inference kernel:** https:\/\/www.kaggle.com\/furcifer\/torch-effnet3d-for-mri-no-inference\/\n\n","f49cad65":"## **CNN (dummy) + EfficientNet**","0b717064":"## **CNN with 4 channels**"}}