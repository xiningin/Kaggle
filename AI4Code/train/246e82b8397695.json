{"cell_type":{"16e1993e":"code","f83227f9":"code","5b78d060":"code","8f51e975":"code","d3af47f1":"code","8baeaf82":"code","431266c8":"code","f353af00":"code","e194a59b":"code","ab50c451":"markdown","ac1d0cd8":"markdown","ed447a2a":"markdown","81b631ac":"markdown","99eb88fb":"markdown","eb11cca7":"markdown","9878303d":"markdown","d80b398b":"markdown","3a167b72":"markdown","d69e3762":"markdown"},"source":{"16e1993e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))","f83227f9":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\nprint(df.info())\nprint(df.describe().to_string())\nprint(df.columns)\n","5b78d060":"def Admissible(value):\n    mean = df['Chance of Admit '].mean()\n    if value >= mean:\n        return(1)\n    else:\n        return(0)\n\ndf[\"AdmitOrNot\"] = [Admissible(x) for x in df['Chance of Admit '].values]    ","8f51e975":"df.hist()\nplt.show()","d3af47f1":"correlations = df.corr()\n\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nnames = df.columns\nticks = np.arange(0,len(names),1)\n\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nplt.xticks(rotation=90)\nax.set_yticklabels(names)\nplt.show()\n","8baeaf82":"factors = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA']\nX = df[factors].values\ny = df[\"AdmitOrNot\"]\n\nprint(X.shape)\nprint(y.shape)\n","431266c8":"from sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\nscaling = {\"Normalizer\" : Normalizer(norm=\"l2\"),\n           \"Standard Scaler\" : StandardScaler(),\n           \"MinMaxScaler\" : MinMaxScaler(feature_range=(0,1)),\n           \"PCA\" : PCA(n_components=3, random_state=0, whiten=False)\n            }\n","f353af00":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nclassifiers = {\"K-Nearest Neighbors\" : KNeighborsClassifier(n_neighbors=3),\n               \"Random Forest\" : RandomForestClassifier(n_estimators=10, random_state=0),\n               #\"Support Vector Clf\" : LinearSVC(penalty=\"l2\", random_state=0), THIS ONE CAUSES ERRORS IN KAGGLE\n               \"Logistic Regression\" : LogisticRegression(penalty=\"l2\", random_state=0),\n               \"Perceptron\" : Perceptron(penalty=\"l2\", random_state=0),\n               \"Decision Tree\" : DecisionTreeClassifier(random_state=0),\n               \"Naive Bayes\" : GaussianNB()\n              }\n","e194a59b":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# Let's get learning\nfold_splits = 5\nrounding_prec = 4 # digits after the decimal\n#results_dict = {\"Clf Name\" : {\"Scaling Method\":\"Accuracy\"}}\nresults_dict = {}\nfor clfy_name, clfy in classifiers.items(): \n\n    acc_dict = {}\n    acc_dict[\"Non-scaled\"] = round(cross_val_score(clfy, X, y, cv=fold_splits\n                                  ).mean(), rounding_prec) \n\n    for scl_name, sclr in scaling.items(): \n        pipln = []\n        pipln.append((scl_name, sclr)) \n        pipln.append((\"clf\", clfy))\n\n        pip = Pipeline(pipln)\n        acc_dict[scl_name] = round(cross_val_score(pip, X, y, cv=fold_splits\n                                                  ).mean(), rounding_prec)\n    results_dict[clfy_name] = acc_dict\n\n# MAKE DF\ndf_ML_acc = pd.DataFrame(results_dict)\ndf_ML_acc.name = \"Machine Learning Models Accuracy Scores\"\nprint(df_ML_acc.to_string())\nprint('---*---Best Results:')\nprint(df_ML_acc.max())","ab50c451":"**Hello, and welcome** to my Kernel which will use Graduate Admissions data and various machine learning models to predict admissions decisions. \n\nThis kernel is mostly to showcase different skills I have been devloping regarding the implementation of pipelines and scaling techinques. \n\nI must point out now that I am going to approach this problem from a categorical viewpoint rather than a continuous one. In the dataset as is stands now, we are attempting to predict a proability of admission. My approach will be to categorize an indivudal as either being \"admissable\" or not, by assigning a cutoff at the mean admission proability. \n\nLet's start off by loading up the modules we need, and some basic inspection. \n","ac1d0cd8":"With this dataset, it is clear we are attempting to predict the probablity of admission. I'm going to take a different route. Instead of using regression to determine the chance of admission, anything above a 72.4% chance of admission (the mean value of Chance of Admit) is an \"admit\", and anything below is not. \n\nSome of the column names have spaces at the end, so be careful when trying to reproduce the results. \n\nLet's make that admissible column now:","ed447a2a":"It seems there is some correlation among the \"Research\" category, so I will exclude it from these algorithms, as many of them require or assume linear independance. \n\nNow, we will set up the factors and target.","81b631ac":"There are many paths to take going forward. Each model can be tuned, and different assumptions can be made when scaling data. \n\n**Thank you! Please consider leaving a comment. **\n\n","99eb88fb":"All of these distributions appear to have good distributions, but since the relative scales are different, this dataset may be suited for various scaling techniques. All of the values are positive, but some are more categorical in nature such as \"Research\". \n\nLet's check if any are intercorrelated.\n\n","eb11cca7":"From the results, we can see that different models behave differently under different scaling. Overall, the algorithm with the highest accuracy was Logistic Regression with MinMaxScaler scaling procedures with 85% accuracy.  ","9878303d":"Now we can define the classifiyers to use. I will use a suite of classifiers based on the size of the dataset and the categorical targets we are predicting. ","d80b398b":"I will perform a 5-fold cross validation on each model, and each model will have each type of scaling methodology. I will later store this information in a new dataframe, so I will append the results to a dictionary first. \n\nMy procedure is as such:\n1. Instantiate the empty dictionary\n2. For every classifyer...\n3. ...Instantiate a new results dictionary\n4. ...Perform a non-scaled train\/test\/predict\/cross-val and append results to dictionary\n5. ---For every scaling method---\n6. ---Instantiate an empty list\n7. ---Append a scaler to a pipeline, then append the clasifyer to the pipeline\n8. ---Perform a scaled train\/test\/predict\/cross-val and append results to dictionary\n\n\n","3a167b72":"Since the data in the factors vary in their range and scale, a variety of scaling factors will be used to pre-process the data for our ML algorithms. \nThe four scalers I'll use are: \n* Normalizer - Ccales each row to have unit norm. That is, the sum of sqaures of every element  is one.\n* StandardScaler - Divides each element by a factor such that the mean of elements is zero with unit standard deviation.\n* MinMaxScaler - Similar to Standard Scaler, but divides each element by a factor such that the range of values is specified\n* PCA - Creates a specified number of new factors which are linear combinations of original factors. ","d69e3762":"Let's take a moment to inspect the distributions of factors:"}}