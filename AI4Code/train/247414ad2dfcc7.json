{"cell_type":{"59b6c1e1":"code","80063cd1":"code","b4fb17fd":"code","9b67431b":"code","153f78dc":"code","85a4b60a":"code","39e89793":"code","bdf15c90":"code","53f7c2f8":"code","4f6d0546":"code","161f39f3":"code","c1ee1cf3":"code","5745dd98":"code","de7533ec":"code","738def52":"code","3cea1542":"code","6eae0466":"code","2f69e13b":"code","3b8cea9c":"code","10b583d6":"code","2c9d644e":"code","87a3852a":"code","45293d9d":"code","71ae842c":"code","9f993262":"code","4813cc06":"code","4a9c5ec0":"code","32e5f587":"markdown","0e1a73d3":"markdown","c3e8f37e":"markdown","c705a152":"markdown","e8e16318":"markdown","5d4873af":"markdown","4fdadcfd":"markdown","3014953d":"markdown","a58669d4":"markdown"},"source":{"59b6c1e1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline","80063cd1":"import os\npath = os.listdir(\"..\/input\")\nprint(path)","b4fb17fd":"# Read the data\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv(\"..\/input\/test.csv\")","9b67431b":"# Set up the data\ny_train = train_data['label'].values\nX_train = train_data.drop(columns=['label']).values\/255\nX_test = test_data.values\/255","153f78dc":"X_train.shape","85a4b60a":"X_train.shape[0]","39e89793":"# Split the training data set into training and cross-validation\nfrom sklearn.model_selection import train_test_split\nX_train_training, X_cross_validation, y_train_training, y_cross_validation = train_test_split(X_train, y_train, test_size=0.3, random_state=0)","bdf15c90":"X_train_training.shape","53f7c2f8":"X_train[0].shape","4f6d0546":"X_train.shape","161f39f3":"y_train_training.shape","c1ee1cf3":"fig, axes = plt.subplots(2,5, figsize=(12,5))\naxes = axes.flatten()\nidx = np.random.randint(0,42000,size=10)\nfor i in range(10):\n    axes[i].imshow(X_train[idx[i],:].reshape(28,28), cmap='gray')\n    axes[i].axis('off') # hide the axes ticks\n    axes[i].set_title(str(int(y_train[idx[i]])), color= 'black', fontsize=25)\nplt.show()","5745dd98":"# relu activation function\n# THE fastest vectorized implementation for ReLU\ndef relu(x):\n    x[x<0]=0\n    return x","de7533ec":"def h(X,W,b):\n    '''\n    Hypothesis function(model): simple FNN with 2 hidden layers\n    Layer 1: input\n    Layer 2: hidden layer, with a size implied by the arguments W[0], b[0]\n    Layer 3: hidden layer, with a size implied by the arguments W[1], b[1]\n    Layer 4: output layer, with a size implied by the arguments W[2]\n    '''\n    # layer 1 = input layer\n    a1 = X\n    # layer 1 (input layer) -> layer 2 (hidden layer)\n    z1 = np.matmul(X, W[0]) + b[0]\n    \n    # add one more layer\n    \n    # layer 2 activation\n    a2 = relu(z1)\n    \n    # layer 2 -> layer 3 (hidden layer)\n    z2 = np.matmul(a2,W[1]) + b[1]\n    # layer 3 activation\n    a3 = relu(z2)\n    \n    # layer 3 (hidden layer) -> layer 4 (output layer)\n    z3 = np.matmul(a3, W[2])\n    s = np.exp(z3)\n    total = np.sum(s, axis=1).reshape(-1,1)\n    sigma = s\/total\n    # the output is a probability for each sample\n    return sigma","738def52":"def softmax(X_in,weights):\n    '''\n    Un-used cell for demo\n    activation function for the last FC layer: softmax function \n    Output: K probabilities represent an estimate of P(y=k|X_in;weights) for k=1,...,K\n    the weights has shape (n, K)\n    n: the number of features X_in has\n    n = X_in.shape[1]\n    K: the number of classes\n    K = 10\n    '''\n    \n    s = np.exp(np.matmul(X_in,weights))\n    total = np.sum(s, axis=1).reshape(-1,1)\n    return s \/ total","3cea1542":"def loss(y_pred,y_true):\n    '''\n    Loss function: cross entropy with an L^2 regularization\n    y_true: ground truth, of shape (N, )\n    y_pred: prediction made by the model, of shape (N, K) \n    N: number of samples in the batch\n    K: global variable, number of classes\n    '''\n    global K \n    K = 10\n    N = len(y_true)\n    # loss_sample stores the cross entropy for each sample in X\n    # convert y_true from labels to one-hot-vector encoding\n    y_true_one_hot_vec = (y_true[:,np.newaxis] == np.arange(K))\n    loss_sample = (np.log(y_pred) * y_true_one_hot_vec).sum(axis=1)\n    # loss_sample is a dimension (N,) array\n    # for the final loss, we need take the average\n    return -np.mean(loss_sample)","6eae0466":"X_train[0].shape","2f69e13b":"X_train.shape[0]","3b8cea9c":"def backprop(W,b,X,y,alpha):\n    '''\n    Step 1: explicit forward pass h(X;W,b)\n    Step 2: backpropagation for dW and db\n    '''\n    K = 10\n    N = X.shape[0]\n    \n    ### Step 1:\n    # layer 1 = input layer\n    a1 = X\n    # layer 1 (input layer) -> layer 2 (hidden layer)\n    z1 = np.matmul(X, W[0]) + b[0]\n    # layer 2 activation\n    a2 = relu(z1)\n    \n    # one more layer\n    \n    # layer 2 (hidden layer) -> layer 3 (hidden layer)\n    z2 = np.matmul(a2, W[1]) + b[1]\n    # layer3 (hidden layer) activation\n    a3 = relu(z2)\n    \n    # layer 3 (hidden layer -> layer 4 (output layer)\n    z3 = np.matmul(a3, W[2])\n    s = np.exp(z3)\n    total = np.sum(s, axis=1).reshape(-1,1)\n    sigma = s\/total\n    \n    ### Step 2:\n    \n    # layer 3->layer 4 weights' derivative\n    # delta3 is \\partial L\/partial z3, of shape (N,K)\n    y_one_hot_vec = (y[:,np.newaxis] == np.arange(K))\n    delta3 = (sigma - y_one_hot_vec)\n    grad_W2 = np.matmul(a3.T, delta3)\n    \n    # layer 2 -> layer 3 weights' derivative\n    # delta2 is \\partial L\/partial z2\n    delta2 = np.matmul(delta3, W[2].T)*(z2>0)\n    grad_W1 = np.matmul(a2.T, delta2)\n    \n    # layer 1->layer 2 weights' derivative\n    # delta1 is \\partial L\/partial z1\n    # layer 2 activation's (weak) derivative is 1*(z1>0)\n    delta1 = np.matmul(delta2, W[1].T)*(z1>0)\n    grad_W0 = np.matmul(X.T, delta1)\n    \n    # Student project: extra layer of derivative\n    \n    # no derivative for layer 1\n    \n    # the alpha part is the derivative for the regularization\n    # regularization = 0.5*alpha*(np.sum(W[1]**2) + np.sum(W[0]**2))\n    \n    \n    dW = [grad_W0\/N + alpha*W[0],grad_W1\/N + alpha*W[1], grad_W2\/N + alpha*W[2]]\n    db = [np.mean(delta1, axis=0),np.mean(delta2,axis=0)]\n    # dW[0] is W[0]'s derivative, and dW[1] is W[1]'s derivative; similar for db\n    return dW, db","10b583d6":"eta = 5e-1\nalpha = 1e-4 # regularization: our alpha is larger than previous 1e-6 to avoid overfitting, and I found that if I keep making\n# aplha larger than 1e-4, the prediction accuracy just dropped dramatically, so 1e-4 might be a good aplha that can\n# avoid overfitting and give us an acceptable prediction accuracy.\ngamma = 0.99 # RMSprop\neps = 1e-3 # RMSprop\nnum_iter = 2000 # number of iterations of gradient descent\nn_H1 = 256 # number of neurons in the hidden layer 1\nn_H2 = 128 # number of neurons in the hidden layer 2\nn = X_train.shape[1] # number of pixels in an image\nK = 10","2c9d644e":"# initialization\nnp.random.seed(1127)\nW = [1e-1*np.random.randn(n, n_H1), 1e-1*np.random.randn(n_H1,n_H2), 1e-1* np.random.randn(n_H2,K)]\nb = [np.random.randn(n_H1),np.random.randn(n_H2)]","87a3852a":"%%time\ngW0 = gW1 = gW2 = gb0 = gb1 = 1\n\nfor i in range(num_iter):\n    \n    # Mini-batch SGD\n    # sampling size is 256 \n    idx = np.random.choice(29400,256,replace=True)\n    X_train_sample = X_train_training[idx] \n    y_train_sample = y_train_training[idx]\n    \n    dW, db = backprop(W,b,X_train_sample,y_train_sample,alpha)\n    \n    gW0 = gamma*gW0 + (1-gamma)*np.sum(dW[0]**2)\n    etaW0 = eta\/np.sqrt(gW0 + eps)\n    W[0] -= etaW0 * dW[0]\n    \n    gW1 = gamma*gW1 + (1-gamma)*np.sum(dW[1]**2)\n    etaW1 = eta\/np.sqrt(gW1 + eps)\n    W[1] -= etaW1 * dW[1]\n    \n    gW2 = gamma*gW2 + (1-gamma)*np.sum(dW[2]**2)\n    etaW2 = eta\/np.sqrt(gW2 + eps)\n    W[2] -= etaW2 * dW[2]\n    \n    gb0 = gamma*gb0 + (1-gamma)*np.sum(db[0]**2)\n    etab0 = eta\/np.sqrt(gb0 + eps)\n    b[0] -= etab0 * db[0]\n    \n    gb1 = gamma*gb1 + (1-gamma) * np.sum(db[1]**2)\n    etab1 = eta\/ np.sqrt(gb1 + eps)\n    b[1] -= etab1 * db[1]\n    \n    if i % 500 == 0:\n        # sanity check 1\n        y_pred = h(X_train_training,W,b)\n        print(\"Cross-entropy loss after\", i+1, \"iterations is {:.8}\".format(\n              loss(y_pred,y_train_training)))\n        print(\"Training accuracy after\", i+1, \"iterations is {:.4%}\".format( \n              np.mean(np.argmax(y_pred, axis=1)== y_train_training)))\n        \n        # sanity check 2\n        print(\"gW0={:.4f} gW1={:.4f} gW2={:.4f} gb0={:.4f} gb1={:.4f}\\netaW0={:.4f} etaW1={:.4f} etaW2 = {:.4f} etab0={:.4f} etab1{:.4f}\"\n              .format(gW0, gW1, gW2, gb0, gb1, etaW0, etaW1, etaW2, etab0, etab1 ))\n        \n        # sanity check 3\n        print(\"|dW0|={:.5f} |dW1|={:.5f} |dW2| = {:.5f} |db0|={:.5f} |db1|={:.5f}\"\n             .format(np.linalg.norm(dW[0]), np.linalg.norm(dW[1]), np.linalg.norm(dW[2]),np.linalg.norm(db[0]),np.linalg.norm(db[1])), \"\\n\")\n        \n        # reset RMSprop\n        gW0 = gW1 = gb0 = gW2 = gb1 = 1\n\ny_pred_final = h(X_train_training,W,b)\nprint(\"Final cross-entropy loss is {:.8}\".format(loss(y_pred_final,y_train_training)))\nprint(\"Final training accuracy is {:.4%}\".format(np.mean(np.argmax(y_pred_final, axis=1)== y_train_training)))","45293d9d":"# Mean square Error\ny_test_pred = np.argmax(h(X_cross_validation,W,b),axis=1)\nMSE = np.mean((y_cross_validation - y_test_pred)**2)\nprint(MSE) ","71ae842c":"# prediction accuracy to our cross-validation set\nnp.mean(y_test_pred==y_cross_validation)","9f993262":"Rsquared = 1 - \\\nnp.sum((y_test_pred - y_cross_validation)**2)\/np.sum((y_cross_validation - np.mean(y_cross_validation))**2)\nprint(Rsquared)","4813cc06":"# predictions\ny_pred_test = np.argmax(h(X_test,W,b), axis=1)","4a9c5ec0":"# Generating submission using pandas for grading\nsubmission = pd.DataFrame({'ImageId': range(1,len(X_test)+1) ,'Label': y_pred_test })\nsubmission.to_csv(\"simplemnist_result3.csv\",index=False)","32e5f587":"# Network structures\n\n<img src=\"https:\/\/faculty.sites.uci.edu\/shuhaocao\/files\/2019\/06\/nn-3layers.png\" alt=\"drawing\" width=\"700\"\/>\n\nThe figure above is a simplication of the neural network used in this example. The circles labeled \"+1\" are the bias units. Layer 1 is the input layer, and Layer 3 is the output layer. The middle layer, Layer 2, is the hidden layer.\n\nThe neural network in the figure above has 2 input units (not counting the bias unit), 3 hidden units, and 1 output unit. In this actual computation below, the input layer has 784 units, the hidden layer has 256 units, and the output layers has 10 units ($K =10$ classes).\n\nThe weight matrix $W^{(0)}$ mapping input $\\mathbf{x}$ from the input layer (Layer 1) to the hidden layer (Layer 2) is of shape `(784,256)` together with a `(256,)` bias. Then $\\mathbf{a}$ is the activation from the hidden layer (Layer 2) can be written as:\n$$\n\\mathbf{a} = \\mathrm{ReLU}\\big((W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b}\\big),\n$$\nwhere the ReLU activation function is $\\mathrm{ReLU}(z) = \\max(z,0)$ and can be implemented in a vectorized fashion as follows.","0e1a73d3":"# Predictions for testing data\nThe prediction labels are generated by $(\\ast)$.","c3e8f37e":"# Gradient Descent: training of the network\n\nIn the training, we use a GD-variant of the RMSprop: for $\\mathbf{w}$ which stands for the parameter vector in our model\n> Choose $\\mathbf{w}_0$, $\\eta$, $\\gamma$, $\\epsilon$, and let $g_{-1} = 1$ <br><br>\n>    For $k=0,1,2, \\cdots, M$<br><br>\n>    &nbsp;&nbsp;&nbsp;&nbsp;  $g_{k} = \\gamma g_{k-1} + (1 - \\gamma)\\, \\left|\\partial_{\\mathbf{w}} L (\\mathbf{w}_k)\\right|^2$<br><br>\n>    &nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{k+1} =  \\mathbf{w}_k -  \\frac{\\eta} {\\sqrt{g_{k}+ \\epsilon}} \\partial_{\\mathbf{w}} L(\\mathbf{w}_k)$  \n\n### Remark: \nThe training takes a while since we use the gradient descent for all samples.","c705a152":"## Softmax activation, prediction, and the loss function\n\nFrom the hidden layer (Layer 2) to the output layer (layer 3), the weight matrix $W^{(1)}$ is of shape `(256,10)`, the form of which is as follows:\n$$\nW^{(1)} =\n\\begin{pmatrix}\n| & | & | & | \\\\\n\\boldsymbol{\\theta}_1 & \\boldsymbol{\\theta}_2 & \\cdots & \\boldsymbol{\\theta}_K \\\\\n| & | & | & |\n\\end{pmatrix},\n$$\nwhich maps the activation from Layer 2 to Layer 3 (output layer), and there is no bias because a constant can be freely added to the activation without changing the final output. \n\nAt the last layer, a softmax activation is used, which can be written as follows combining the weights matrix $W^{(1)}$ that maps the activation $\\mathbf{a}$ from the hidden layer to output layer:\n$$\nP\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big) = \\sigma_k(\\mathbf{a}; W^{(1)}) := \\frac{\\exp\\big(\\boldsymbol{\\theta}^{\\top}_k \\mathbf{a} \\big)}\n{\\sum_{j=1}^K \\exp\\big(\\boldsymbol{\\theta}^{\\top}_j \\mathbf{a} \\big)}.\n$$\n$\\{P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)\\}_{k=1}^K$ is the probability distribution of our model, which estimates the probability of the input $\\mathbf{x}$'s label $y$ is of class $k$. We denote this distribution by a vector \n$$\\boldsymbol{\\sigma}:= (\\sigma_1,\\dots, \\sigma_K)^{\\top}.$$\nWe hope that this estimate is as close as possible to the true probability: $1_{\\{y=k\\}}$, that is $1$ if the sample $\\mathbf{x}$ is in the $k$-th class and 0 otherwise. \n\nLastly, our prediction $\\hat{y}$ for sample $\\mathbf{x}$ can be made by choosing the class with the highest probability:\n$$\n\\hat{y} = \\operatorname{argmax}_{k=1,\\dots,K}  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big). \\tag{$\\ast$}\n$$\n\nDenote the label of the $i$-th input as $y^{(i)}$, and then the sample-wise loss function is the cross entropy measuring the difference of the distribution of this model function above with the true one $1_{\\{y^{(i)}=k\\}}$: denote $W = (W^{(0)}, W^{(1)})$, $b = (\\mathbf{b})$, let $\\mathbf{a}^{(i)}$ be the activation for the $i$-th sample in the hidden layer (Layer 2),\n$$\nJ_i:= J(W,b;\\mathbf{x}^{(i)},y^{(i)}) := - \\sum_{k=1}^{K} \\left\\{  1_{\\left\\{y^{(i)} = k\\right\\} }\n\\log P\\big(y^{(i)} = k \\;| \\;\\mathbf{a}^{(i)}; W^{(1)}\\big)\\right\\}. \\tag{1}\n$$\n\nDenote the data sample matrix $X := (\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)})^{\\top}$, its label vector as $\\mathbf{y} := (y^{(1)}, \\dots, y^{(N)})$, and then the final loss has an extra $L^2$-regularization term for the weight matrices (not for bias): \n$$\nL(W,b; X, \\mathbf{y}) := \\frac{1}{N}\\sum_{i=1}^{N} J_i  + \\frac{\\alpha}{2} \\Big(\\|W^{(0)}\\|^2 + \\|W^{(1)}\\|^2\\Big),\n\\tag{2}\n$$\nwhere $\\alpha>0$ is a hyper-parameter determining the strength of the regularization, the bigger the $\\alpha$ is, the smaller the magnitudes of the weights will be after training.","e8e16318":"## Data input","5d4873af":"## Some examples of the input data\nWe randomly choose 10 samples from the training set and visualize it.","4fdadcfd":"# Backpropagation (Chain rule)\n\nThe derivative of the cross entropy $J$ in (1), for a single sample and its label $(\\mathbf{x}, y)$ , with respect to the weights and the bias is computed using the following procedure:\n> **Step 1**: Forward pass: computing the activations $\\mathbf{a} = (a_1,\\dots, a_{n_2})$ from the hidden layer (Layer 2), and $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots, \\sigma_K)$ from the output layer (Layer 3). \n>\n> **Step 2**: Derivatives for $W^{(1)}$: recall that $W^{(1)} = (\\boldsymbol{\\theta}_1 ,\\cdots,  \\boldsymbol{\\theta}_K)$ and denote \n$$\\mathbf{z}^{(2)} = \\big(z^{(2)}_1, \\dots, z^{(2)}_K\\big)  = (W^{(1)})^{\\top}\\mathbf{a} =\n\\big(\\boldsymbol{\\theta}^{\\top}_1 \\mathbf{a} ,\\cdots,  \\boldsymbol{\\theta}^{\\top}_K \\mathbf{a}\\big),$$ \nfor the $k$-th output unit in the output layer (Layer 3), then\n$$\n\\delta^{(2)}_k\n:= \\frac{\\partial J}{\\partial z_k^{(2)}} = \\Big\\{  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)- 1_{\\{ y = k\\}} \\Big\\} = \\sigma_k - 1_{\\{ y = k\\}}\n$$\nand \n$$\n\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}_k}= \\frac{\\partial J}{\\partial z_k^{(2)}}\\frac{\\partial z_k^{(2)}}{\\partial \\boldsymbol{\\theta}_k} = \\delta^{(2)}_k \\mathbf{a}.\n$$\n>\n> **Step 3**: Derivatives for $W^{(0)}$, $\\mathbf{b}$: recall that $W^{(0)} = (\\boldsymbol{w}_1 ,\\cdots,  \\boldsymbol{w}_{n_2})$, $\\mathbf{b} = (b_1,\\dots, b_{n_2})$, where $n_2$ is the number of units in the hidden layer (Layer 2), and denote \n$$\\mathbf{z}^{(1)} = (z_1^{(1)}, \\dots, z_{n_2}^{(1)})  = (W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b} =\n\\big(\\mathbf{w}^{\\top}_1 \\mathbf{x} +b_1 ,\\cdots,  \\mathbf{w}^{\\top}_{n_2} \\mathbf{x} + b_{n_2}\\big),$$ \nfor each node $i$ in the hidden layer (Layer $2$), $i=1,\\dots, n_2$, then\n$$\\delta^{(1)}_i : = \\frac{\\partial J}{\\partial z^{(1)}_i}  =\n\\frac{\\partial J}{\\partial a_i} \n\\frac{\\partial a_i}{\\partial z^{(1)}_i}=\n\\frac{\\partial J}{\\partial \\mathbf{z}^{(2)}}\n\\cdot\\left(\\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a_i} \n\\frac{\\partial a_i}{\\partial z^{(1)}_i}\\right)\n\\\\\n=\\left( \\sum_{k=1}^{K} \\frac{\\partial J}{\\partial {z}^{(2)}_k}\n\\frac{\\partial {z}^{(2)}_k}{\\partial a_i}  \\right) f'(z^{(1)}_i) = \\left( \\sum_{k=1}^{K} w_{ki} \\delta^{(2)}_k \\right) 1_{\\{z^{(1)}_i\\; > 0\\}},\n$$\nwhere $1_{\\{z^{(1)}_i\\; > 0\\}}$ is ReLU activation $f$'s (weak) derivative, and the partial derivative of the $k$-th component (before activated by the softmax) in the output layer ${z}^{(2)}_k$ with respect to the $i$-th activation $a_i$ from the hidden layer is the weight $w^{(1)}_{ki}$. Thus\n>\n$$\n\\frac{\\partial J}{\\partial w_{ji}}  = x_j \\delta_i^{(1)} ,\\;\n\\frac{\\partial J}{\\partial b_{i}} = \\delta_i^{(1)}, \\;\\text{ and }\\;\n\\frac{\\partial J}{\\partial \\mathbf{w}_{i}}  = \\delta_i^{(1)}\\mathbf{x} ,\\;\n\\frac{\\partial J}{\\partial \\mathbf{b}} = \\boldsymbol{\\delta}^{(1)}.\n$$","3014953d":"## Hyper-parameters and network initialization","a58669d4":"# Summary\n\nThis note is an MNIST digit recognizer implemented in numpy from scratch.\n\nThis is a simple demonstration mainly for pedagogical purposes, which shows the basic workflow of a machine learning algorithm using a simple feedforward neural network. The derivative at the backpropagation stage is computed explicitly through the chain rule.\n\n* The model is a 3-layer feedforward neural network (FNN), in which the input layer has 784 units, and the 256-unit hidden layer is activated by ReLU, while the output layer is activated by softmax function to produce a discrete probability distribution for each input. \n* The loss function, model hypothesis function, and the gradient of the loss function are all implemented from ground-up in numpy in a highly vectorized fashion (no FOR loops).\n* The training is through a standard gradient descent with step size adaptively changing by Root Mean Square prop (RMSprop), and there is no cross-validation set reserved nor model averaging for simplicity.\n\nThe code is vectorized and is adapted from the Softmax regression and Neural Network lectures used in [UCI Math 10](https:\/\/github.com\/scaomath\/UCI-Math10). \n\nCaveat lector: fluency in linear algebra and multivariate calculus.\n\n\n#### References:\n* [Stanford Deep Learning tutorial in MATLAB](http:\/\/ufldl.stanford.edu\/tutorial\/supervised\/MultiLayerNeuralNetworks\/).\n* [Learning PyTorch with examples](https:\/\/pytorch.org\/tutorials\/beginner\/pytorch_with_examples.html).\n* [An overview of gradient descent optimization algorithms](http:\/\/ruder.io\/optimizing-gradient-descent\/)."}}