{"cell_type":{"b824e299":"code","2771e48c":"code","aefa9c88":"code","a329f095":"code","c067a87c":"code","f4ab27a7":"code","7bb45082":"code","4baade4d":"code","7d915e50":"code","42b19b99":"code","73e71690":"code","65835b3d":"code","cf826a39":"code","038733a3":"code","dbf82567":"code","4c9b39c7":"code","69bad03a":"code","1ca65d83":"code","cca163c7":"code","e82123f3":"code","59b0e42f":"code","f594e09d":"code","5eaa3ca1":"code","b7f5f7fa":"code","c175925a":"code","8d8a3834":"code","8aea0648":"code","1a42988f":"code","9b1ae508":"code","aad10b09":"code","b1166a12":"code","25eb0ce4":"code","8af4eaef":"code","d49be322":"code","6e6f7470":"code","0eaa1940":"code","d01975ab":"markdown","edf29d7c":"markdown","ef064371":"markdown","f8d551e0":"markdown","703d04ec":"markdown","536b8a0e":"markdown","08abba01":"markdown","a9c99e07":"markdown","8389fae9":"markdown","952e79e7":"markdown","0938c270":"markdown","712ec07a":"markdown","021477bb":"markdown","da67dcfb":"markdown","8fc051c5":"markdown","973be320":"markdown","e32523ad":"markdown","1a77ace0":"markdown","60ed56e7":"markdown","90f5eced":"markdown"},"source":{"b824e299":"# import libraries\n# Essentials\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom category_encoders import MEstimateEncoder\n\n# models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n\n# stats\nfrom scipy.stats import norm\nfrom scipy import stats","2771e48c":"# import training data\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n# drop id column\ntrain_df = train_df.drop(columns= 'Id')\nprint(f\"Size of training set: {train_df.shape}\")\ntrain_df.head()","aefa9c88":"# import test data\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_id = test_df['Id']\n# drop id column\ntest_df = test_df.drop(columns='Id')\nprint(f\"Size of test set: {test_df.shape}\")\ntest_df.head()","a329f095":"cols = train_df.columns\nquantitatives = train_df.select_dtypes(exclude={'object'}).columns\nqualitatives = train_df.select_dtypes(include={'object'}).columns\nprint(f\"Number of quantitative variables: {len(quantitatives)}\")\nprint(quantitatives)\nprint(f\"Number of qualitative variables: {len(qualitatives)}\")\nprint(qualitatives)","c067a87c":"train_df.describe()","f4ab27a7":"sns.distplot(train_df['SalePrice'], fit= norm)\nplt.title(\"Sale Price distribution\")\nplt.xlabel(\"Sale Price\")\nplt.ylabel(\"Frequency\")\n\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot= plt)","7bb45082":"# skewness and kurtosis\nprint(f\"Skewness: {train_df['SalePrice'].skew()}\")\nprint(f\"Kurtosis: {train_df['SalePrice'].kurt()}\")","4baade4d":"corr_matrix = train_df.corr()\n\nfig = plt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix)\nplt.title(\"Correlation heatmap\")","7d915e50":"top10_corr = corr_matrix.sort_values('SalePrice', ascending= False)[0:10]\ntop10_corr = top10_corr.loc[:, top10_corr.index]\n\nfig = plt.figure(figsize= (6, 6))\nsns.heatmap(top10_corr, annot= True)\nplt.title(\"Zoom in heatmap\")","42b19b99":"n_rows = 12\nn_cols = 3\n\nfig, axs = plt.subplots(n_rows, n_cols, figsize= (4*n_cols, 3*n_rows))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        i = row * n_cols + col\n        if i > len(quantitatives) - 2:\n            break\n        variable = quantitatives[i]\n        sns.scatterplot(x= train_df[variable], y= train_df['SalePrice'], ax= axs[row, col])\n\nplt.tight_layout()\nplt.show()","73e71690":"# outliers in GrLivArea\ntrain_df = train_df.drop(train_df.loc[(train_df['GrLivArea'] > 4000) & (train_df['SalePrice'] < 200000)].index)\nsns.scatterplot(x= train_df['GrLivArea'], y= train_df['SalePrice'])","65835b3d":"# outliers in TotalBsmtSF\ntrain_df = train_df.drop(train_df[train_df['TotalBsmtSF'] > 6000].index)\nsns.scatterplot(x= train_df['TotalBsmtSF'], y= train_df['SalePrice'])","cf826a39":"n_rows = 15\nn_cols = 3\n\nfig, axs = plt.subplots(n_rows, n_cols, figsize= (4*n_cols, 3*n_rows))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        i = row * n_cols + col\n        if i >= len(qualitatives):\n            break\n        variable = qualitatives[i]\n        sns.boxplot(x= train_df[variable], y= train_df['SalePrice'], ax= axs[row, col])\n        \nplt.tight_layout()\nplt.show()","038733a3":"train_missing_value = train_df.isnull().sum().sort_values(ascending= False)\ntrain_missing_value = train_missing_value[train_missing_value > 0]\ntrain_missing_value","dbf82567":"test_missing_value = test_df.isnull().sum().sort_values(ascending= False)\ntest_missing_value = test_missing_value[test_missing_value > 0]\ntest_missing_value","4c9b39c7":"# cut saleprice\nsale_price = train_df['SalePrice']\ntrain_df = train_df.drop(columns={'SalePrice'})","69bad03a":"# combine test and training data\nall_data = pd.concat([train_df, test_df], ignore_index= True)\nall_data.shape","1ca65d83":"def knn_regressor_fill(col, df):\n    df = df.copy()\n    numeric_non_na_cols = df.select_dtypes(exclude= {'object'}).loc[:, df.isnull().sum() == 0].columns\n\n    train = df.loc[df[col].isna() == False][numeric_non_na_cols]\n    labels = df.loc[df[col].isna() == False][col]\n    test = df.loc[df[col].isna() == True][numeric_non_na_cols]\n\n    knn = KNeighborsRegressor()\n    knn.fit(train, labels)\n    preds = knn.predict(test)\n    \n    df.loc[df[col].isna() == True, col] = preds\n    return df[col]","cca163c7":"def knn_clf_fill(col, df):\n    df = df.copy()\n    numeric_non_na_cols = df.select_dtypes(exclude= {'object'}).loc[:, df.isnull().sum() == 0].columns\n\n    train = df.loc[df[col].isna() == False][numeric_non_na_cols]\n    labels = df.loc[df[col].isna() == False][col]\n    test = df.loc[df[col].isna() == True][numeric_non_na_cols]\n\n    knn = KNeighborsClassifier()\n    knn.fit(train, labels)\n    preds = knn.predict(test)\n    \n    df.loc[df[col].isna() == True, col] = preds\n    return df[col]","e82123f3":"# Missing Categorical variables can be consider as None or NA\nfill_na = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageType',\n          'GarageFinish', 'GarageQual', 'BsmtExposure', 'BsmtFinType2', 'BsmtCond', 'BsmtQual', 'BsmtFinType1']\nfill_cat = ['Electrical', 'MSZoning', 'Functional', 'Utilities', 'KitchenQual', 'SaleType', 'Exterior2nd', 'Exterior1st', 'GarageCars']\nfill_num = ['LotFrontage', 'BsmtFinSF1', 'BsmtFinSF2', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt']\nfill_0 = ['MasVnrArea', 'BsmtHalfBath', 'BsmtFullBath', 'BsmtUnfSF']\n\nfor col in fill_na:\n    all_data[col].fillna('NA', inplace= True)\n\nfor col in fill_cat:\n    #val = train_df[col].mode()[0] \n    #all_data[col].fillna(val, inplace= True)\n    # fill with knn maybe ??\n    all_data[col] = knn_clf_fill(col, all_data)\n\nfor col in fill_num:\n    #val =train_df[col].mean()\n    #all_data[col].fillna(val, inplace= True)\n    # fill with knn maybe ??\n    all_data[col] = knn_regressor_fill(col, all_data)\n    \nfor col in fill_0:\n    all_data[col].fillna(0, inplace= True)\n    \nall_data['MasVnrType'].fillna('None', inplace= True)\n\nprint(f\"Number of missing data: {all_data.isnull().sum().sum()}\")","59b0e42f":"# creating features\nall_data['TotBathRms'] = all_data['FullBath'] + all_data['BsmtFullBath'] + 0.5*(all_data['HalfBath'] + all_data['BsmtHalfBath'])\nall_data['TotOutsideSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + \\\n                            all_data['ScreenPorch'] + all_data['WoodDeckSF']\nall_data[\"LivLotRatio\"] = all_data['GrLivArea'] \/ all_data['LotArea']\nall_data[\"Spaciousness\"] = (all_data['1stFlrSF'] + all_data['2ndFlrSF']) \/ all_data['TotRmsAbvGrd']\nall_data['hasPool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else x)\nall_data['hasGarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else x)\nall_data['hasBsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else x) ","f594e09d":"target_encoding_cols = train_df.select_dtypes(include= {'object'}).nunique()\ntarget_encoding_cols = target_encoding_cols[target_encoding_cols > 10].index\ntarget_encoding_cols","5eaa3ca1":"encoder = MEstimateEncoder(cols= target_encoding_cols.values.tolist(), m= 2)\nencoder.fit(all_data[:len(train_df)], sale_price)\nall_data = encoder.transform(all_data)","b7f5f7fa":"sale_price = np.log1p(sale_price)\n\n# plot the SalePrice again to check normality\nsns.distplot(sale_price, fit= norm)\nplt.title(\"Sale Price distribution\")\nplt.xlabel(\"Sale Price\")\nplt.ylabel(\"Frequency\")\n\nfig = plt.figure()\nres = stats.probplot(sale_price, plot= plt)","c175925a":"skewness_df = pd.DataFrame(train_df.skew(), columns=['Skewness'], \\\n                           index= train_df.select_dtypes(exclude=['object']).columns)\nskewness_df['Skewed'] = skewness_df['Skewness'].apply(lambda x: True if abs(x) > 0.5 else False)\nskewness_df","8d8a3834":"log_transform_cols = skewness_df[skewness_df['Skewed'] == True].index.to_list()\nfor col in log_transform_cols:\n    all_data.loc[all_data[col] > 0][col+'_log'] = np.log1p(all_data.loc[all_data[col] > 0][col])","8aea0648":"all_data = pd.get_dummies(all_data)\nall_data.shape","1a42988f":"X_train = all_data[:len(train_df)]\ny_train = sale_price\nX_test = all_data[len(train_df):]\n\nprint(f\"Number of training observations: {X_train.shape[0]}\")","9b1ae508":"kf = KFold(n_splits= 5, random_state= 7, shuffle= True)","aad10b09":"rf_params = {\n    'n_estimators': 1000,\n    'max_depth' : 10,\n    'random_state': 5\n}\n\nGBR_params = {\n    'learning_rate': 0.0511,\n    'n_estimators': 5000,\n    'max_depth' : 5,\n    'n_iter_no_change' : 5,\n    'random_state' : 5\n}\n\nxgboost_params = {\n    'learning_rate' : 0.115,\n    'max_depth' : 6,\n    'n_estimators' : 500,\n    'random_state' : 5,\n    'subsample' : 0.8,\n    'gamma' : 0.05,\n    'random_state' : 5\n}\n\nridge_params = {\n    'alphas' : np.array([1, 1.511, 0.95, 0.93, 0.01, 0.05]),\n    'cv' : kf\n}","b1166a12":"models = {\n    \"Ridge\" : RidgeCV(**ridge_params),\n    \"RandomForest\": RandomForestRegressor(**rf_params),\n    \"GradientBoosting\": GradientBoostingRegressor(**GBR_params),\n    \"XGBoost\": XGBRegressor(**xgboost_params)\n}","25eb0ce4":"for name, model in models.items():\n    model.fit(X_train, y_train)\n    print(name + \" trained\")","8af4eaef":"for name, model in models.items():\n    scores = -cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', n_jobs= -1, cv=kf)\n    print(f\"{name}: {scores} -- Average: {scores.mean()}\")\n","d49be322":"final_preds = 0.1 * np.expm1(models['RandomForest'].predict(X_test)) \\\n            + 0.2 * np.expm1(models['GradientBoosting'].predict(X_test)) \\\n            + 0.35 * np.expm1(models['XGBoost'].predict(X_test)) \\\n            + 0.35 * np.expm1(models['Ridge'].predict(X_test))\n\nfinal_preds","6e6f7470":"submission = pd.concat([test_id, pd.Series(final_preds, name='SalePrice')], axis=1)\nsubmission","0eaa1940":"submission.to_csv('.\/submission.csv', index=False, header=True)","d01975ab":"## 4.2. Set up model and metric","edf29d7c":"Now, we log transform skewed numerical variables.","ef064371":"From those charts, highly correlated variables mentioned above have sort of linear relationship with 'SalePrice'. Those charts also imply some exponential relationships, so we can log transform some features to get a better model.\n\nCandidates for log transformation: 'LotFrontage', 'LotArea', 'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea'.\n\nWe can see that some variables are rather categorical variables, and some outliers need to be eliminate.\n\n**Outliers**","f8d551e0":"# 2. Explainatory Data Analysis","703d04ec":"## 4.3. Train Model and Evaluate","536b8a0e":"## 3.2. Features Engineering\n","08abba01":"# 1. Overview\n\n**Data**\n\nThe data set is about house price in Ames, Iowa. It contains 79 explainatory variables describing some aspects of residential homes.\n\n**Goal**\n\nThe goal is to predict the sale price for each house based on given 79 explainatory variables.\n\n**Metric**\n\nPredictions are evaluated using RMSE between the logarithm of the predicted value and the logarithm of the observed sales price.","a9c99e07":"## 2.3. Categorical Variables\n\n**Relation of categorical variables to 'SalePrice'**","8389fae9":"The SalePrice is positive skewed, and show peakedness.","952e79e7":"Target Encoding","0938c270":"Highly related variables:\n- 'TotalBsmtSF' and '1stFlrSF'\n- 'GarageCars' and 'GarageArea'\n- 'GrLivArea' and 'TotRmsAbvGrd'\n- 'YearBuilt' and 'GarageYrBlt'\n\nVariables that correlated with 'SalePrice': 'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'YearBuilt'. '1stFlrSF', 'GarageCars', 'TotRmsAbvGrd' are excluded because they are related to one of those 4. 'FullBath' is considered not so important.\n\n\n\n**Relation of numerical variables to 'SalePrice'**","712ec07a":"Add more features","021477bb":"## 3.3. Data Transformation\nFirst, we fix skewness in 'SalePrice'","da67dcfb":"# 4. Train Model\n## 4.1. Split Data\n","8fc051c5":"## 2.2. Numerical Variables\n\n**Heatmap**","973be320":"## 3.4. Encode Categorical Variables","e32523ad":"## 2.1. Examine Target Variable (Sale Price)\nFirst, we examine the distribution of SalePrice","1a77ace0":"## 4.4. Prediction and Submission","60ed56e7":"From the plots:\n- Variables having good disparity with respect to 'SalePrice' are: \u2018MSZoning\u2019, \u2019Neighborhood\u2019, \u2018Condition1\u2019, \u2018Condition2\u2019, \u2018RoofMatl\u2019, \u2018MatVnrType\u2019, \u2019ExterQual\u2019, \u2018BsmtQual\u2019, \u2018BsmtCond\u2019, \u2018CentralAir\u2019, \u2019KitchenQual\u2019, \u2018SaleType\u2019, \u2018SaleCondition\u2019.\n- House that have excellent Pool Quality tends to have higher sale price.\n- Partial sale condition tends to have higher sale price.\n\n# 3. Data Preprocessing\n## 3.1. Missing value\n\n\n","90f5eced":"<font size=\"6\"><b>House price - Advanced Regression Technique<\/b><\/font>\n\nShout out to these notebooks that helped me a lot in my first notebook on Kaggle:\n- <a href=\"https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\">Comprehensive data exploration with Python - PEDRO MARCELINO<\/a>\n- <a href=\"https:\/\/www.kaggle.com\/dgawlik\/house-prices-eda\">House Prices EDA - DOMINIK GAWLIK<\/a>\n- <a href=\"https:\/\/www.kaggle.com\/gcdatkin\/top-10-house-price-regression-competition-nb\">House Price Regression Competition NB - GABRIEL ATKIN<\/a>"}}