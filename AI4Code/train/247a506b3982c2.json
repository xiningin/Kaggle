{"cell_type":{"8bfc95c5":"code","bd36fd93":"code","343ffb4e":"code","ad098143":"code","167bc11c":"code","bc1a2c00":"code","8d60cebd":"code","478b8844":"code","5d4a83cc":"code","62f9cc6d":"code","31fd5b35":"code","795aad53":"code","9fef3dbb":"code","deff8210":"code","0ef3810a":"code","caff712a":"code","7909ef61":"code","af016f88":"code","170f6b3c":"code","cea12d7f":"code","8bfb0c6f":"code","af902e4f":"code","4ea9acc9":"code","b62766cf":"code","587b3b32":"code","05b338f3":"code","966ef2ae":"code","6d32d9fb":"code","f8b55a2a":"code","2ca26c80":"code","3c63ce6d":"code","d3afb55a":"code","e40e5cf7":"code","b00e68d9":"code","a84d2404":"code","a251254c":"code","c8856878":"code","2bc4b9ad":"code","55e4833a":"code","d932db26":"code","89365223":"code","cea43e30":"code","6f074ca8":"code","1409b7ed":"code","d079c8c4":"code","abcbee38":"code","d465ea7a":"code","0cbe6288":"code","029ab90e":"markdown","9b3bb892":"markdown","70d54e62":"markdown","9a9a2cb6":"markdown","3125f807":"markdown","ed2dba65":"markdown","21864268":"markdown","694edb73":"markdown","a84f647e":"markdown","48383d06":"markdown","513e59ea":"markdown","b3afcabf":"markdown","c2f6f8e2":"markdown","e2b1cfac":"markdown","3f641ebd":"markdown","17bfad90":"markdown","264c5ec3":"markdown","96df2c0b":"markdown","650f70b7":"markdown","cc24e89b":"markdown","d3bb4f73":"markdown","e3a14616":"markdown","211db77b":"markdown","d44bcdbe":"markdown","62a00efb":"markdown","577bce1c":"markdown","38ab856f":"markdown","7ca7cccd":"markdown","f77a0a0e":"markdown","ee811adb":"markdown","ce9bc1ea":"markdown","40ec89ca":"markdown","745849ad":"markdown","9555efcf":"markdown","80d128f5":"markdown","b0885504":"markdown","3e5a145b":"markdown","eccf98c7":"markdown","3b6e8677":"markdown","d677e8b6":"markdown","3c6b2a2d":"markdown","9e8f9f89":"markdown","f8596b1d":"markdown"},"source":{"8bfc95c5":"# importing the required library \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n                    # Input data files are available in the \"..\/input\/\" directory.\nimport os\nimport matplotlib.pyplot as plt #visualization\nfrom PIL import  Image\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns #visualization\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport io\nimport plotly.offline as py #visualization\npy.init_notebook_mode(connected=True) #visualization\nimport plotly.graph_objs as go #visualization\nimport plotly.tools as tls #visualization\nimport plotly.figure_factory as ff#visualization\n\n\n","bd36fd93":"# loading the dataset and reading the data frame\ndataset = pd.read_csv('..\/input\/Churn_Modelling.csv', delimiter=',')\n# viewing first 10 rows\ndataset.head(10)\n","343ffb4e":"print('Dataset at glance')\nprint('-----------------')\nprint (\"Rows     : \" ,dataset.shape[0])\nprint (\"Columns  : \" ,dataset.shape[1])\nprint (\"\\nFeatures : \\n\" ,dataset.columns.tolist())\nprint (\"\\nMissing values :  \", dataset.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",dataset.nunique())","ad098143":"#Age Distribution \nplt.subplots(figsize=(12,5))\nsns.set(color_codes = True)\nax = sns.distplot(dataset.Age, kde = False,rug = True);\nax.set_title('Age Distribution')\nax.set_ylabel('Counts')","167bc11c":"# Converting Age column into categorical column\ndef age_to_categ(dataset) :\n    if dataset[\"Age\"] <= 25 :\n        return \"Age_0-25\"\n    elif (dataset[\"Age\"] > 25) & (dataset[\"Age\"] <= 35 ):\n        return \"Age_25-35\"\n    elif (dataset[\"Age\"] > 35) & (dataset[\"Age\"] <= 45) :\n        return \"Age_35-45\"\n    elif (dataset[\"Age\"] > 45) & (dataset[\"Age\"] <= 55) :\n        return \"Age_45-55\"\n    elif (dataset[\"Age\"] > 55) & (dataset[\"Age\"] <= 65) :\n        return \"Age_55-65\"\n    elif dataset[\"Age\"] > 65 :\n        return \"Age_gt_65\"\n# Append the Age_Categ column to the dataset   \ndataset[\"Age_Categ\"] = dataset.apply(lambda dataset:age_to_categ(dataset),\n                                      axis = 1)","bc1a2c00":"# Tenure Distribution \nplt.subplots(figsize=(12,5))\nsns.set(color_codes = True)\nax = sns.distplot(dataset.Tenure,kde = False, rug = True );\nax.set_title('Tenure Distribution') \nax.set_ylabel('Counts')","8d60cebd":"#Tenure to categorical column\ndef tenure_categ(dataset) :\n    if dataset[\"Tenure\"] < 1 :\n        return \"Tenure_blw-1\"\n    elif (dataset[\"Tenure\"] >= 1) & (dataset[\"Tenure\"] < 4 ):\n        return \"Tenure_1-4\"\n    elif (dataset[\"Tenure\"] >= 4) & (dataset[\"Tenure\"] < 6) :\n        return \"Tenure_4-6\"\n    elif (dataset[\"Tenure\"] >= 6) & (dataset[\"Tenure\"] < 9) :\n        return \"Tenure_6-9\"\n    elif dataset[\"Tenure\"] >= 9 :\n        return \"Tenure_gt_9\"\n    \ndataset[\"Tenure_Categ\"] = dataset.apply(lambda dataset:tenure_categ(dataset),\n                                      axis = 1)\n","478b8844":"#Separating exited and non exited customers\nexited     = dataset[dataset[\"Exited\"] == 1]\nnot_exited = dataset[dataset[\"Exited\"] == 0]","5d4a83cc":"#Separating catagorical and numerical columns\nId_col     = ['CustomerID']\ntarget_col = [\"Exited\"]\ncat_cols   = dataset.nunique()[dataset.nunique() <= 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\nnum_cols   = [x for x in dataset.columns if x not in cat_cols + target_col + Id_col]","62f9cc6d":"print(Id_col)\nprint(target_col)\nprint(cat_cols)\nprint(num_cols)","31fd5b35":"#labels\nlab = [\"Retained\",\"Exited\"]\n\n#values\nval = dataset[\"Exited\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  ['lime','red'],\n                             line = dict(color = \"white\",\n                                         width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Proportion of customers who exited and retained\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\ndata = [trace]\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","795aad53":"#function  for pie plot for customer churn types \n\ndef plot_pie(column) :\n    trace1 = go.Pie(values  = exited[column].value_counts().values.tolist(),\n                    labels  = exited[column].value_counts().keys().tolist(),\n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [0,.48]),\n                    name    = \"Exited Customers\",\n                    marker  = dict(line = dict(width = 2,\n                                               color = \"rgb(243,243,243)\")\n                                  ),\n                    hole    = .6\n                   )\n    trace2 = go.Pie(values  = not_exited[column].value_counts().values.tolist(),\n                    labels  = not_exited[column].value_counts().keys().tolist(),\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(line = dict(width = 2,\n                                               color = \"rgb(243,243,243)\")\n                                  ),\n                    domain  = dict(x = [.52,1]),\n                    hole    = .6,\n                    name    = \"Retained Customers\" \n                   )\n\n\n    layout = go.Layout(dict(title = column + \" Distribution in Customer Churn \",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            annotations = [dict(text = \"Exited customers\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .15, y = .5),\n                                           dict(text = \"Retained customers\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .88,y = .5\n                                               )\n                                          ]\n                           )\n                      )\n    data = [trace1,trace2]\n    fig  = go.Figure(data = data,layout = layout)\n    py.iplot(fig)\n\n\n#function  for histogram for customer churn types\ndef histogram(column) :\n    trace1 = go.Histogram(x  = exited[column],\n                          histnorm= \"percent\",\n                          name = \"Exited Customers\",\n                          marker = dict(line = dict(width = .5,\n                                                    color = \"black\"\n                                                    )\n                                        ),\n                         opacity = .9 \n                         ) \n    \n    trace2 = go.Histogram(x  = not_exited[column],\n                          histnorm = \"percent\",\n                          name = \"Retained customers\",\n                          marker = dict(line = dict(width = .5,\n                                              color = \"black\"\n                                             )\n                                 ),\n                          opacity = .9\n                         )\n    \n    data = [trace1,trace2]\n    layout = go.Layout(dict(title =column + \" Distribution in Customer Churn \",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = column,\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = \"percent\",\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                           )\n                      )\n    fig  = go.Figure(data=data,layout=layout)\n    py.iplot(fig)\n    \n    \n    \n#function  for scatter plot matrix  for numerical columns in data\ndef scatter_matrix(dataframe)  :\n    \n    dataframe  = dataframe.sort_values(by = \"Exited\" ,ascending = True)\n    classes = dataframe[\"Exited\"].unique().tolist()\n    classes\n    \n    class_code  = {classes[k] : k for k in range(2)}\n    class_code\n\n    color_vals = [class_code[cl] for cl in dataframe[\"Exited\"]]\n    color_vals\n\n    pl_colorscale = \"Portland\"\n\n    pl_colorscale\n\n    text = [dataframe.loc[k,\"Exited\"] for k in range(len(dataframe))]\n    text\n\n    trace = go.Splom(dimensions = [\n                                  dict(label  = 'CreditScore',\n                                       values = dataframe['CreditScore']),\n                                  dict(label  = 'Balance',\n                                       values = dataframe['Balance']),\n                                  dict(label  = 'EstimatedSalary',\n                                       values = dataframe['EstimatedSalary'])\n                                  ],\n                     text = text,\n                     marker = dict(color = color_vals,\n                                   colorscale = pl_colorscale,\n                                   size = 3,\n                                   showscale = False,\n                                   line = dict(width = .1,\n                                               color='rgb(230,230,230)'\n                                              )\n                                  )\n                    )\n    axis = dict(showline  = True,\n                zeroline  = False,\n                gridcolor = \"#fff\",\n                ticklen   = 4\n               )\n    \n    layout = go.Layout(dict(title  = \n                            \"Scatter plot matrix for Numerical columns for customer attrition\",\n                            autosize = False,\n                            height = 800,\n                            width  = 800,\n                            dragmode = \"select\",\n                            hovermode = \"closest\",\n                            plot_bgcolor  = 'rgba(240,240,240, 0.95)',\n                            xaxis1 = dict(axis),\n                            yaxis1 = dict(axis),\n                            xaxis2 = dict(axis),\n                            yaxis2 = dict(axis),\n                            xaxis3 = dict(axis),\n                            yaxis3 = dict(axis),\n                           )\n                      )\n    data   = [trace]\n    fig = go.Figure(data = data,layout = layout )\n    py.iplot(fig)\n\n","9fef3dbb":"#for all categorical columns plot pie\nfor i in cat_cols :\n    plot_pie(i)\n\n","deff8210":"#for all non-categorical columns plot histogram    \nfor i in num_cols :\n    histogram(i)\n\n","0ef3810a":"#scatter plot matrix\nscatter_matrix(dataset)","caff712a":"#customer churn in tenure categories\ntg_ch  =  exited[\"Tenure_Categ\"].value_counts().reset_index()\ntg_ch.columns  = [\"Tenure_Categ\",\"count\"]\ntg_nch =  not_exited[\"Tenure_Categ\"].value_counts().reset_index()\ntg_nch.columns = [\"Tenure_Categ\",\"count\"]\n\n#bar - churn\ntrace1 = go.Bar(x = tg_ch[\"Tenure_Categ\"]  , y = tg_ch[\"count\"],\n                name = \"Exited Customers\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                opacity = .9)\n\n#bar - not churn\ntrace2 = go.Bar(x = tg_nch[\"Tenure_Categ\"] , y = tg_nch[\"count\"],\n                name = \"Retained Customers\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                opacity = .9)\n\nlayout = go.Layout(dict(title = \"Customer Churn in Tenure Categories\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"Tenure Categories\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"count\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                       )\n                  )\ndata = [trace1,trace2]\nfig  = go.Figure(data=data,layout=layout)\npy.iplot(fig)","7909ef61":"\ndataset[['CreditScore', 'Balance','Tenure',\"Tenure_Categ\"]]\n\n#scatter plot monthly charges & total charges by tenure group\n\ndef plot_tenure_scatter(tenure_categ,color) :\n    tracer = go.Scatter(x = dataset[dataset[\"Tenure_Categ\"] == tenure_categ][\"CreditScore\"],\n                        y = dataset[dataset[\"Tenure_Categ\"] == tenure_categ][\"Balance\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = tenure_categ,\n                        opacity = .9\n                       )\n    return tracer\n\n#scatter plot monthly charges & total charges by churn group\ndef plot_churncharges_scatter(exited,color) :\n    tracer = go.Scatter(x = dataset[dataset[\"Exited\"] == exited][\"CreditScore\"],\n                        y = dataset[dataset[\"Exited\"] == exited][\"Balance\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = \"Exited - \" + str(exited),\n                        opacity = .9\n                       )\n    return tracer\n\n\ntrace1 = plot_tenure_scatter(\"Tenure_blw-1\",\"#FF3300\")\ntrace2 = plot_tenure_scatter(\"Tenure_1-4\",\"#FF3300\")\ntrace3 = plot_tenure_scatter(\"Tenure_4-6\",\"#6666FF\")\ntrace4 = plot_tenure_scatter(\"Tenure_6-9\",\"#99FF00\")\ntrace5 = plot_tenure_scatter(\"Tenure_gt_9\",\"grey\")\ntrace6 = plot_churncharges_scatter(1,\"red\")\ntrace7 = plot_churncharges_scatter(0,\"blue\")\n\ndata1   = [trace1,trace2,trace3,trace4,trace5] \ndata2   = [trace7,trace6]\n\n#layout\ndef layout_title(title) :\n    layout = go.Layout(dict(title = title,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Credit Score\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Balance\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            height = 600\n                           )\n                      )\n    return layout\n\nlayout1  = layout_title(\"Credit Score & Balance by Tenure Categories\")\nlayout2  = layout_title(\"Credit Score & Balance by Churn group\")\nfig1 = go.Figure(data = data1,layout = layout1)\nfig2 = go.Figure(data = data2,layout = layout2)\npy.iplot(fig1)\npy.iplot(fig2)","af016f88":"\ndataset[['EstimatedSalary', 'Balance','Tenure',\"Tenure_Categ\"]]\n\n#scatter plot monthly charges & total charges by tenure group\n\ndef plot_tenure_scatter(tenure_categ,color) :\n    tracer = go.Scatter(x = dataset[dataset[\"Tenure_Categ\"] == tenure_categ][\"EstimatedSalary\"],\n                        y = dataset[dataset[\"Tenure_Categ\"] == tenure_categ][\"Balance\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = tenure_categ,\n                        opacity = .9\n                       )\n    return tracer\n\n#scatter plot monthly charges & total charges by churn group\ndef plot_churncharges_scatter(exited,color) :\n    tracer = go.Scatter(x = dataset[dataset[\"Exited\"] == exited][\"EstimatedSalary\"],\n                        y = dataset[dataset[\"Exited\"] == exited][\"Balance\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = \"Exited - \" + str(exited),\n                        opacity = .9\n                       )\n    return tracer\n\n\ntrace1 = plot_tenure_scatter(\"Tenure_blw-1\",\"#FF3300\")\ntrace2 = plot_tenure_scatter(\"Tenure_1-4\",\"#FF3300\")\ntrace3 = plot_tenure_scatter(\"Tenure_4-6\",\"#6666FF\")\ntrace4 = plot_tenure_scatter(\"Tenure_6-9\",\"#99FF00\")\ntrace5 = plot_tenure_scatter(\"Tenure_gt_9\",\"grey\")\ntrace6 = plot_churncharges_scatter(1,\"red\")\ntrace7 = plot_churncharges_scatter(0,\"blue\")\n\ndata1   = [trace1,trace2,trace3,trace4,trace5] \ndata2   = [trace7,trace6]\n\n#layout\ndef layout_title(title) :\n    layout = go.Layout(dict(title = title,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Estimated Salary\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Balance\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            height = 600\n                           )\n                      )\n    return layout\n\nlayout1  = layout_title(\"Estimated Salary & Balance by Tenure Categories\")\nlayout2  = layout_title(\"Estimated Salary & Balance by Churn group\")\nfig1 = go.Figure(data = data1,layout = layout1)\nfig2 = go.Figure(data = data2,layout = layout2)\npy.iplot(fig1)\npy.iplot(fig2)","170f6b3c":"\ndataset[['EstimatedSalary', 'CreditScore','Tenure',\"Tenure_Categ\"]]\n\n#scatter plot monthly charges & total charges by tenure group\n\ndef plot_tenure_scatter(tenure_categ,color) :\n    tracer = go.Scatter(x = dataset[dataset[\"Tenure_Categ\"] == tenure_categ][\"EstimatedSalary\"],\n                        y = dataset[dataset[\"Tenure_Categ\"] == tenure_categ][\"CreditScore\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = tenure_categ,\n                        opacity = .9\n                       )\n    return tracer\n\n#scatter plot monthly charges & total charges by churn group\ndef plot_churncharges_scatter(exited,color) :\n    tracer = go.Scatter(x = dataset[dataset[\"Exited\"] == exited][\"EstimatedSalary\"],\n                        y = dataset[dataset[\"Exited\"] == exited][\"CreditScore\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = \"Exited - \" + str(exited),\n                        opacity = .9\n                       )\n    return tracer\n\n\ntrace1 = plot_tenure_scatter(\"Tenure_blw-1\",\"#FF3300\")\ntrace2 = plot_tenure_scatter(\"Tenure_1-4\",\"#FF3300\")\ntrace3 = plot_tenure_scatter(\"Tenure_4-6\",\"#6666FF\")\ntrace4 = plot_tenure_scatter(\"Tenure_6-9\",\"#99FF00\")\ntrace5 = plot_tenure_scatter(\"Tenure_gt_9\",\"grey\")\ntrace6 = plot_churncharges_scatter(1,\"red\")\ntrace7 = plot_churncharges_scatter(0,\"blue\")\n\ndata1   = [trace1,trace2,trace3,trace4,trace5] \ndata2   = [trace7,trace6]\n\n#layout\ndef layout_title(title) :\n    layout = go.Layout(dict(title = title,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Estimated Salary\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"CreditScore\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            height = 600\n                           )\n                      )\n    return layout\n\nlayout1  = layout_title(\"Estimated Salary & CreditScore by Tenure Categories\")\nlayout2  = layout_title(\"Estimated Salary & CreditScore by Churn group\")\nfig1 = go.Figure(data = data1,layout = layout1)\nfig2 = go.Figure(data = data2,layout = layout2)\npy.iplot(fig1)\npy.iplot(fig2)","cea12d7f":"\n\navg_tgc = dataset.groupby([\"Tenure_Categ\",\"Exited\"])[[\"Balance\",\n                                                    \"EstimatedSalary\"]].mean().reset_index()\n\n#function for tracing \ndef mean_charges(column,aggregate) :\n    tracer = go.Bar(x = avg_tgc[avg_tgc[\"Exited\"] == aggregate][\"Tenure_Categ\"],\n                    y = avg_tgc[avg_tgc[\"Exited\"] == aggregate][column],\n                    name = aggregate,marker = dict(line = dict(width = 1)),\n                    text = \"Exited\"\n                   )\n    return tracer\n\n#function for layout\ndef layout_plot(title,xaxis_lab,yaxis_lab) :\n    layout = go.Layout(dict(title = title,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',title = xaxis_lab,\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',title = yaxis_lab,\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                           )\n                      )\n    return layout\n    \n\n#plot1 - mean monthly charges by tenure groups\ntrace1  = mean_charges(\"Balance\",1)\ntrace2  = mean_charges(\"Balance\",0)\nlayout1 = layout_plot(\"Average Balance by Tenure Categories\",\n                      \"Tenure_Categ\",\"Balance\")\ndata1   = [trace1,trace2]\nfig1    = go.Figure(data=data1,layout=layout1)\n\n#plot2 - mean total charges by tenure groups\ntrace3  = mean_charges(\"EstimatedSalary\",1)\ntrace4  = mean_charges(\"EstimatedSalary\",0)\nlayout2 = layout_plot(\"Average Estimated Salary by Tenure Categories\",\n                      \"Tenure_Categ\",\"Estimated Salary\")\ndata2   = [trace3,trace4]\nfig2    = go.Figure(data=data2,layout=layout2)\n\npy.iplot(fig1)\npy.iplot(fig2)","8bfb0c6f":"##copy data\nbank_df = dataset.copy()\n#Drop Tenure_Categ column\n#dataset = dataset.drop(columns = \"Tenure_Categ\",axis = 1)\n\ntrace1 = go.Scatter3d(x = exited[\"EstimatedSalary\"],\n                      y = exited[\"Balance\"],\n                      z = exited[\"Tenure\"],\n                      mode = \"markers\",\n                      name = \"Exited customers\",\n                      text = \"Id : \" + str(exited[\"CustomerId\"]),\n                      marker = dict(size = 1,color = \"red\")\n                     )\ntrace2 = go.Scatter3d(x = not_exited[\"EstimatedSalary\"],\n                      y = not_exited[\"Balance\"],\n                      z = not_exited[\"Tenure\"],\n                      name = \"Retained customers\",\n                      text = \"Id : \" + str(not_exited[\"CustomerId\"]),\n                      mode = \"markers\",\n                      marker = dict(size = 1,color= \"green\")\n                     )\n\n\n\nlayout = go.Layout(dict(title = \"Estimated Salary,Balance & Tenure in customer churn\",\n                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n                                                   center=dict(x=0, y=0, z=0),\n                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n                                     xaxis  = dict(title = \"Estimated Salary\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'),\n                                     yaxis  = dict(title = \"Balance\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'\n                                                  ),\n                                     zaxis  = dict(title = \"Tenure\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'\n                                                  )\n                                    ),\n                        height = 700,\n                       )\n                  )\n                  \n\ndata = [trace1,trace2]\nfig  = go.Figure(data = data,layout = layout)\npy.iplot(fig)\n","af902e4f":"##copy data\nbank_df = dataset.copy()\n#Drop Tenure_Categ column\n#dataset = dataset.drop(columns = \"Tenure_Categ\",axis = 1)\n\ntrace1 = go.Scatter3d(x = exited[\"EstimatedSalary\"],\n                      y = exited[\"CreditScore\"],\n                      z = exited[\"Tenure\"],\n                      mode = \"markers\",\n                      name = \"Exited customers\",\n                      text = \"Id : \" + str(exited[\"CustomerId\"]),\n                      marker = dict(size = 1,color = \"red\")\n                     )\ntrace2 = go.Scatter3d(x = not_exited[\"EstimatedSalary\"],\n                      y = not_exited[\"CreditScore\"],\n                      z = not_exited[\"Tenure\"],\n                      name = \"Retained customers\",\n                      text = \"Id : \" + str(not_exited[\"CustomerId\"]),\n                      mode = \"markers\",\n                      marker = dict(size = 1,color= \"green\")\n                     )\n\n\n\nlayout = go.Layout(dict(title = \"Estimated Salary,CreditScore & Tenure in customer churn\",\n                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n                                                   center=dict(x=0, y=0, z=0),\n                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n                                     xaxis  = dict(title = \"Estimated Salary\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'),\n                                     yaxis  = dict(title = \"CreditScore\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'\n                                                  ),\n                                     zaxis  = dict(title = \"Tenure\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'\n                                                  )\n                                    ),\n                        height = 700,\n                       )\n                  )\n                  \n\ndata = [trace1,trace2]\nfig  = go.Figure(data = data,layout = layout)\npy.iplot(fig)\n","4ea9acc9":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\n# dropping RowNumber and Surname features\ndataset = dataset.drop(['RowNumber','Surname'], axis=1)  \n\n#customer id col\nId_col     = ['CustomerId']\n#Target columns\ntarget_col = [\"Exited\"]\n#categorical columns\ncat_cols   = dataset.nunique()[dataset.nunique() <= 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\n#numerical columns\nnum_cols   = [x for x in dataset.columns if x not in cat_cols + target_col + Id_col]\n#Binary columns with 2 values\nbin_cols   = dataset.nunique()[dataset.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    dataset[i] = le.fit_transform(dataset[i])\n    \n#Duplicating columns for multi value columns\ndataset = pd.get_dummies(data = dataset,columns = multi_cols )\ndataset.head(20)\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(dataset[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_dataset_og = dataset.copy()\ndataset = dataset.drop(columns = num_cols,axis = 1)\ndataset = dataset.merge(scaled,left_index=True,right_index=True,how = \"left\")\nprint('Preprocessing is successful completed...')","b62766cf":"summary = (df_dataset_og[[i for i in df_dataset_og.columns if i not in Id_col]].\n           describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\n\nval_lst = [summary['feature'], summary['count'],\n           summary['mean'],summary['std'],\n           summary['min'], summary['25%'],\n           summary['50%'], summary['75%'], summary['max']]\n\ntrace  = go.Table(header = dict(values = summary.columns.tolist(),\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = ['#119DFF']),\n                               ),\n                  cells  = dict(values = val_lst,\n                                line = dict(color = ['#506784']),\n                                fill = dict(color = [\"lightgrey\",'#F5F8FF'])\n                               ),\n                  columnwidth = [200,60,100,100,60,60,80,80,80])\nlayout = go.Layout(dict(title = \"Variable Summary\"))\nfigure = go.Figure(data=[trace],layout=layout)\npy.iplot(figure)","587b3b32":"#correlation\ncorrelation = dataset.corr()\n#tick labels\nmatrix_cols = correlation.columns.tolist()\n#convert to array\ncorr_array  = np.array(correlation)\n\n#Plotting\ntrace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   colorscale = \"Viridis\",\n                   colorbar   = dict(title = \"Pearson Correlation coefficient\",\n                                     titleside = \"right\"\n                                    ) ,\n                  )\n\nlayout = go.Layout(dict(title = \"Correlation Matrix for variables\",\n                        autosize = False,\n                        height  = 720,\n                        width   = 800,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                      ),\n                        yaxis   = dict(tickfont = dict(size = 9)),\n                        xaxis   = dict(tickfont = dict(size = 9))\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","05b338f3":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\n\nX = dataset[[i for i in dataset.columns if i not in Id_col + target_col]]\n\nY = dataset[target_col + Id_col]\n\nprincipal_components = pca.fit_transform(X)\npca_data = pd.DataFrame(principal_components,columns = [\"PC1\",\"PC2\"])\npca_data = pca_data.merge(Y,left_index=True,right_index=True,how=\"left\")\n\ndef pca_scatter(target,color) :\n    tracer = go.Scatter(x = pca_data[pca_data[\"Exited\"] == target][\"PC1\"] ,\n                        y = pca_data[pca_data[\"Exited\"] == target][\"PC2\"],\n                        name = target,mode = \"markers\",\n                        marker = dict(color = color,\n                                      line = dict(width = .5),\n                                      symbol =  \"diamond-open\"),\n                       \n                        text = (\"Customer Id : \" + str(\n                                pca_data[pca_data[\"Exited\"] == target]['CustomerId'])\n                                )\n                       )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Visualising data with principal components\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"principal component 1\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"principal component 2\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        height = 600\n                       )\n                  )\ntrace1 = pca_scatter(1,'red')\ntrace2 = pca_scatter(0,'royalblue')\ndata = [trace2,trace1]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","966ef2ae":"#separating binary columns\nbi_cs = dataset.nunique()[dataset.nunique() == 2].keys()\ndat_rad = dataset[bi_cs]\n\n#plotting radar chart for churn and non churn customers(binary variables)\ndef plot_radar(df,aggregate,title) :\n    data_frame = df[df[\"Exited\"] == aggregate] \n    data_frame_x = data_frame[bi_cs].sum().reset_index()\n    data_frame_x.columns  = [\"feature\",\"yes\"]\n    data_frame_x[\"no\"]    = data_frame.shape[0]  - data_frame_x[\"yes\"]\n    data_frame_x  = data_frame_x[data_frame_x[\"feature\"] != \"Exited\"]\n    \n    #count of 1's(yes)\n    trace1 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(),\n                             theta = data_frame_x[\"feature\"].tolist(),\n                             fill  = \"toself\",name = \"count of 1's\",\n                             mode = \"markers+lines\",\n                             marker = dict(size = 5)\n                            )\n    #count of 0's(No)\n    trace2 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n                             theta = data_frame_x[\"feature\"].tolist(),\n                             fill  = \"toself\",name = \"count of 0's\",\n                             mode = \"markers+lines\",\n                             marker = dict(size = 5)\n                            ) \n    layout = go.Layout(dict(polar = dict(radialaxis = dict(visible = True,\n                                                           side = \"counterclockwise\",\n                                                           showline = True,\n                                                           linewidth = 2,\n                                                           tickwidth = 2,\n                                                           gridcolor = \"white\",\n                                                           gridwidth = 2),\n                                         angularaxis = dict(tickfont = dict(size = 10),\n                                                            layer = \"below traces\"\n                                                           ),\n                                         bgcolor  = \"rgb(243,243,243)\",\n                                        ),\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            title = title,height = 700))\n    \n    data = [trace2,trace1]\n    fig = go.Figure(data=data,layout=layout)\n    py.iplot(fig)\n\n#plot\nplot_radar(dat_rad,1,\"Exited -  Customers\")\nplot_radar(dat_rad,0,\"Retained - Customers\")","6d32d9fb":"#Required libraries for fitting model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nimport statsmodels.api as sm\nfrom sklearn.metrics import precision_score,recall_score\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n\n#randomly splitting train and test data. training data is 80% while testing data is 20% \ntrain,test = train_test_split(dataset,test_size = .20 ,random_state = 111)\n    \n##seperating dependent and independent variables\ncols    = [i for i in dataset.columns if i not in Id_col + target_col]\ntrain_X = train[cols]\ntrain_Y = train[target_col]\ntest_X  = test[cols]\ntest_Y  = test[target_col]\n\n\n    \ndef bank_churn_prediction_baseline(algorithm,training_x,testing_x,\n                             training_y,testing_y,cols,cf,threshold_plot) :\n           #Function attributes\n           #dataframe     - processed dataframe\n           #Algorithm     - Algorithm used \n           #training_x    - predictor variables dataframe(training)\n           #testing_x     - predictor variables dataframe(testing)\n           #training_y    - target variable(training)\n           #training_y    - target variable(testing)\n           #cf - [\"coefficients\",\"features\"](cooefficients for logistic \n                                 #regression,features for tree based models)\n\n           #threshold_plot - if True returns threshold plot for model\n    \n    #model\n    algorithm.fit(training_x,training_y)\n    predictions   = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n    #coeffs\n    if   cf == \"coefficients\" :\n        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n    elif cf == \"features\" :\n        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n        \n    column_df     = pd.DataFrame(cols)\n    coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n                              right_index= True, how = \"left\"))\n    coef_sumry.columns = [\"coefficients\",\"features\"]\n    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n    \n    print (algorithm)\n    print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n    print (\"Accuracy   Score : \",accuracy_score(testing_y,predictions))\n    \n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y,predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y,predictions) \n    print (\"Area under curve : \",model_roc_auc,\"\\n\")\n    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n    \n    #plot confusion matrix\n    trace1 = go.Heatmap(z = conf_matrix ,\n                        x = [\"Retained\",\"Exited\"],\n                        y = [\"Retained\",\"Exited\"],\n                        showscale  = False,colorscale = \"Picnic\",\n                        name = \"matrix\")\n    \n    #plot roc curve\n    trace2 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n    trace3 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    \n    #plot coeffs\n    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                    name = \"coefficients\",\n                    marker = dict(color = coef_sumry[\"coefficients\"],\n                                  colorscale = \"Picnic\",\n                                  line = dict(width = .6,color = \"black\")))\n    \n    #subplots\n    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                            subplot_titles=('Confusion Matrix',\n                                            'Receiver operating characteristic',\n                                            'Feature Importances'))\n    \n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,1,2)\n    fig.append_trace(trace4,2,1)\n    \n    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n                         autosize = False,height = 900,width = 800,\n                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                         margin = dict(b = 195))\n    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n                                        tickangle = 90))\n    py.iplot(fig)\n    \n    if threshold_plot == True : \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()\n        \nlogit  = LogisticRegression(C = 1.0, class_weight = None, dual = False, fit_intercept = True,\n          intercept_scaling = 1, max_iter = 100, multi_class = 'ovr', n_jobs = 2,\n          penalty = 'l2', random_state = None, solver = 'liblinear', tol = 0.0001,\n          verbose = 0, warm_start = False)\n\nbank_churn_prediction_baseline(logit,train_X,test_X,train_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = True)","f8b55a2a":"from imblearn.over_sampling import SMOTE\n\ncols    = [i for i in dataset.columns if i not in Id_col+target_col]\n\nsmote_X = dataset[cols]\nsmote_Y = dataset[target_col]\n\n#Split train and test data\nsmote_train_X,smote_test_X,smote_train_Y,smote_test_Y = train_test_split(smote_X,smote_Y,\n                                                                         test_size = .20 ,\n                                                                         random_state = 111)\n\n#oversampling minority class using smote\nos = SMOTE(random_state = 0)\nos_smote_X,os_smote_Y = os.fit_sample(smote_train_X,smote_train_Y)\nos_smote_X = pd.DataFrame(data = os_smote_X,columns=cols)\nos_smote_Y = pd.DataFrame(data = os_smote_Y,columns=target_col)\n###\n\n\n\nlogit_smote = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\nbank_churn_prediction_baseline(logit_smote,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = True)","2ca26c80":"from sklearn.feature_selection import RFE\n\nlogit = LogisticRegression()\n\nrfe = RFE(logit,10)\nrfe = rfe.fit(os_smote_X,os_smote_Y.values.ravel())\n\nrfe.support_\nrfe.ranking_\n\n#identified columns Recursive Feature Elimination\nidc_rfe = pd.DataFrame({\"rfe_support\" :rfe.support_,\n                       \"columns\" : [i for i in dataset.columns if i not in Id_col + target_col],\n                       \"ranking\" : rfe.ranking_,\n                      })\ncols = idc_rfe[idc_rfe[\"rfe_support\"] == True][\"columns\"].tolist()\n\n\n#separating train and test data\ntrain_rf_X = os_smote_X[cols]\ntrain_rf_Y = os_smote_Y\ntest_rf_X  = test[cols]\ntest_rf_Y  = test[target_col]\n\nlogit_rfe = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n#applying model\nbank_churn_prediction_baseline(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n                         cols,\"coefficients\",threshold_plot = True)\n\ntab_rk = ff.create_table(idc_rfe)\npy.iplot(tab_rk)","3c63ce6d":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\n\n#select columns\ncols = [i for i in dataset.columns if i not in Id_col + target_col ]\n\n#dataframe with non negative values\ndf_x = df_dataset_og[cols]\ndf_y = df_dataset_og[target_col]\n\n#fit model with k= 3\nselect = SelectKBest(score_func = chi2,k = 3)\nfit    = select.fit(df_x,df_y)\n\n#Summerize scores\nprint (\"scores\")\nprint (fit.scores_)\nprint (\"P - Values\")\nprint (fit.pvalues_)\n\n#create dataframe\nscore = pd.DataFrame({\"features\":cols,\"scores\":fit.scores_,\"p_values\":fit.pvalues_ })\nscore = score.sort_values(by = \"scores\" ,ascending =False)\n\n\n#createing new label for categorical and numerical columns\nscore[\"feature_type\"] = np.where(score[\"features\"].isin(num_cols),\"Numerical\",\"Categorical\")\n\n#plot\ntrace  = go.Scatter(x = score[score[\"feature_type\"] == \"Categorical\"][\"features\"],\n                    y = score[score[\"feature_type\"] == \"Categorical\"][\"scores\"],\n                    name = \"Categorial\",mode = \"lines+markers\",\n                    marker = dict(color = \"red\",\n                                  line = dict(width =1))\n                   )\n\ntrace1 = go.Bar(x = score[score[\"feature_type\"] == \"Numerical\"][\"features\"],\n                y = score[score[\"feature_type\"] == \"Numerical\"][\"scores\"],name = \"Numerical\",\n                marker = dict(color = \"royalblue\",\n                              line = dict(width =1)),\n                xaxis = \"x2\",yaxis = \"y2\"\n               )\nlayout = go.Layout(dict(title = \"Scores for Categorical & Numerical features\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     tickfont = dict(size =10),\n                                     domain=[0, 0.7],\n                                     tickangle = 90,zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"scores\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(b=200),\n                        xaxis2=dict(domain=[0.8, 1],tickangle = 90,\n                                    gridcolor = 'rgb(255, 255, 255)'),\n                        yaxis2=dict(anchor='x2',gridcolor = 'rgb(255, 255, 255)')\n                        )\n                  )\n\ndata=[trace,trace1]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","d3afb55a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nfrom graphviz import Source\nfrom IPython.display import SVG,display\n\n#top 3 categorical features\nfeatures_cat  = score[score[\"feature_type\"] == \"Categorical\"][\"features\"][:3].tolist()\n\n#top 3 numerical features\nfeatures_num  = score[score[\"feature_type\"] == \"Numerical\"][\"features\"][:3].tolist()\n\n\n#Function attributes\n#columns        - selected columns\n#maximum_depth  - depth of tree\n#criterion_type - [\"gini\" or \"entropy\"]\n#split_type     - [\"best\" or \"random\"]\n#Model Performance - True (gives model output)\n\ndef plot_decision_tree(columns,maximum_depth,criterion_type,\n                       split_type,model_performance = None) :\n    \n    #separating dependent and in dependent variables\n    dtc_x = df_x[columns]\n    dtc_y = df_y[target_col]\n    \n    #model\n    dt_classifier = DecisionTreeClassifier(max_depth = maximum_depth,\n                                           splitter  = split_type,\n                                           criterion = criterion_type,\n                                          )\n    dt_classifier.fit(dtc_x,dtc_y)\n    \n    #plot decision tree\n    graph = Source(tree.export_graphviz(dt_classifier,out_file=None,\n                                        rounded=True,proportion = False,\n                                        feature_names = columns, \n                                        precision  = 2,\n                                        class_names=[\"not_exited\",\"exited\"],\n                                        filled = True                         \n                                       )\n                  )\n    \n    #model performance\n    if model_performance == True :\n        bank_churn_prediction_baseline(dt_classifier,\n                                 dtc_x,test_X[columns],\n                                 dtc_y,test_Y,\n                                 columns,\"features\",threshold_plot = True)\n    display(graph)\n    \nplot_decision_tree(features_num,3,\"gini\",\"best\")","e40e5cf7":"plot_decision_tree(features_cat,3,\"entropy\",\"best\",\n                   model_performance = True ,)","b00e68d9":"#using contract,tenure and paperless billing variables\ncolumns = ['Age', 'Tenure', 'EstimatedSalary']\nplot_decision_tree(columns,3,\"gini\",\"best\",model_performance= True)\n","a84d2404":"def knn_bank_churn_prediction_alg(algorithm,training_x,testing_x,\n                                 training_y,testing_y,threshold_plot = True) :\n    \n    #model\n    algorithm.fit(training_x,training_y)\n    predictions   = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n    \n    print (algorithm)\n    print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n    print (\"Accuracy Score   : \",accuracy_score(testing_y,predictions))\n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y,predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y,predictions) \n    print (\"Area under curve : \",model_roc_auc)\n    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n     \n    #plot roc curve\n    trace1 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2),\n                       )\n    trace2 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    \n    #plot confusion matrix\n    trace3 = go.Heatmap(z = conf_matrix ,x = [\"not_exited\",\"exited\"],\n                        y = [\"not_exited\",\"exited\"],\n                        showscale  = False,colorscale = \"Blues\",name = \"matrix\",\n                        xaxis = \"x2\",yaxis = \"y2\"\n                       )\n    \n    layout = go.Layout(dict(title=\"Model performance\" ,\n                            autosize = False,height = 500,width = 800,\n                            showlegend = False,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(title = \"false positive rate\",\n                                         gridcolor = 'rgb(255, 255, 255)',\n                                         domain=[0, 0.6],\n                                         ticklen=5,gridwidth=2),\n                            yaxis = dict(title = \"true positive rate\",\n                                         gridcolor = 'rgb(255, 255, 255)',\n                                         zerolinewidth=1,\n                                         ticklen=5,gridwidth=2),\n                            margin = dict(b=200),\n                            xaxis2=dict(domain=[0.7, 1],tickangle = 90,\n                                        gridcolor = 'rgb(255, 255, 255)'),\n                            yaxis2=dict(anchor='x2',gridcolor = 'rgb(255, 255, 255)')\n                           )\n                  )\n    data = [trace1,trace2,trace3]\n    fig = go.Figure(data=data,layout=layout)\n    \n    py.iplot(fig)\n    \n    if threshold_plot == True : \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()\n\n    \nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=40, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n           weights='uniform')\nknn_bank_churn_prediction_alg(knn,os_smote_X,test_X,\n                             os_smote_Y,test_Y,threshold_plot = True)","a251254c":"from sklearn.ensemble import RandomForestClassifier\n\n#function attributes\n#columns  - column used\n#nf_estimators   - The number of trees in the forest.\n#estimated_tree  - tree number to be displayed\n#maximum_depth   - depth of the tree\n#criterion_type  - split criterion type [\"gini\" or \"entropy\"]\n#Model performance - prints performance of model\n\ndef plot_tree_randomforest(columns,nf_estimators,\n                           estimated_tree,maximum_depth,\n                           criterion_type,model_performance = None) :\n    \n    dataframe = df_dataset_og[columns + target_col].copy()\n    \n    #train and test datasets\n    rf_x     = dataframe[[i for i in columns if i not in target_col]]\n    rf_y     = dataframe[target_col]\n    \n    #random forest classifier\n    rfc   = RandomForestClassifier(n_estimators = nf_estimators,\n                                   max_depth = maximum_depth,\n                                   criterion = criterion_type,\n                                  )\n    rfc.fit(rf_x,rf_y)\n    \n    estimated_tree = rfc.estimators_[estimated_tree]\n    \n    graph = Source(tree.export_graphviz(estimated_tree,out_file=None,\n                                        rounded=True,proportion = False,\n                            feature_names = columns, \n                            precision  = 2,\n                            class_names=[\"not_exited\",\"exited\"],\n                            filled = True))\n    display(graph)\n    \n    #model performance\n    if model_performance == True :\n        bank_churn_prediction_baseline(rfc,\n                                 rf_x,test_X[columns],\n                                 rf_y,test_Y,\n                                 columns,\"features\",threshold_plot = True)\n        \n\ncols1 = [ i for i in train_X.columns if i not in target_col + Id_col] \nplot_tree_randomforest(cols1,100,99,3,\"entropy\",True)","c8856878":"#making 10 trees with random forest.\nn = np.arange(0,10).tolist()\ncols1 = [ i for i in train_X.columns if i not in target_col + Id_col] \nfor i in n :\n    plot_tree_randomforest(cols1,10,i,3,\"entropy\",model_performance=False)","2bc4b9ad":"#making 10 trees with random forest for columns \n#selected from recursive feature elimination\n\nn = np.arange(0,10).tolist()\ncols = idc_rfe[idc_rfe[\"rfe_support\"] == True][\"columns\"].tolist() \nfor i in n :\n    plot_tree_randomforest(cols,10,i,3,\"gini\",model_performance=False)","55e4833a":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB(priors=None)\n\nknn_bank_churn_prediction_alg(gnb,os_smote_X,test_X,os_smote_Y,test_Y)","d932db26":"from sklearn.svm import SVC\n\n#Support vector classifier\n#using linear hyper plane\nsvc_lin  = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n               decision_function_shape='ovr', degree=3, gamma=1.0, kernel='linear',\n               max_iter=-1, probability=True, random_state=None, shrinking=True,\n               tol=0.001, verbose=False)\n\ncols = [i for i in dataset.columns if i not in Id_col + target_col]\nbank_churn_prediction_baseline(svc_lin,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = False)","89365223":"#tuning parameters\n#Support vector classifier\n#using non-linear hyper plane(\"rbf\")\n\nsvc_rbf  = SVC(C=1.0, kernel='rbf', \n               degree= 3, gamma=1.0, \n               coef0=0.0, shrinking=True,\n               probability=True,tol=0.001,\n               cache_size=200, class_weight=None,\n               verbose=False,max_iter= -1,\n               random_state=None)\n\nknn_bank_churn_prediction_alg(svc_rbf,os_smote_X,test_X,os_smote_Y,test_Y,threshold_plot = False)","cea43e30":"from lightgbm import LGBMClassifier\n\nlgbm_c = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                        learning_rate=0.5, max_depth=7, min_child_samples=20,\n                        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n                        n_jobs=-1, num_leaves=500, objective='binary', random_state=None,\n                        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n                        subsample_for_bin=200000, subsample_freq=0)\n\ncols = [i for i in dataset.columns if i not in Id_col + target_col]\nbank_churn_prediction_baseline(lgbm_c,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"features\",threshold_plot = True)","6f074ca8":"from xgboost import XGBClassifier\n\nxgc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bytree=1, gamma=0, learning_rate=0.95, max_delta_step=0,\n                    max_depth = 7, min_child_weight=1, missing=None, n_estimators=100,\n                    n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=True, subsample=1)\n\n\nbank_churn_prediction_baseline(xgc,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"features\",threshold_plot = True)","1409b7ed":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\n\n#gives model report in dataframe\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    roc_auc      = roc_auc_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    kappa_metric = cohen_kappa_score(testing_y,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df\n\n#outputs for every model\nmodel1 = model_report(logit,train_X,test_X,train_Y,test_Y,\n                      \"Logistic Regression(Baseline_model)\")\nmodel2 = model_report(logit_smote,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"Logistic Regression(SMOTE)\")\nmodel3 = model_report(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n                      \"Logistic Regression(RFE)\")\ndecision_tree = DecisionTreeClassifier(max_depth = 9,\n                                       random_state = 123,\n                                       splitter  = \"best\",\n                                       criterion = \"gini\",\n                                      )\nmodel4 = model_report(decision_tree,train_X,test_X,train_Y,test_Y,\n                      \"Decision Tree\")\nmodel5 = model_report(knn,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"KNN Classifier\")\nrfc = RandomForestClassifier(n_estimators = 1000,\n                             random_state = 123,\n                             max_depth = 9,\n                             criterion = \"gini\")\nmodel6 = model_report(rfc,train_X,test_X,train_Y,test_Y,\n                      \"Random Forest Classifier\")\nmodel7 = model_report(gnb,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"Naive Bayes\")\nmodel8 = model_report(svc_lin,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"SVM Classifier Linear\")\nmodel9 = model_report(svc_rbf,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"SVM Classifier RBF\")\nmodel10 = model_report(lgbm_c,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"LGBM Classifier\")\nmodel11 = model_report(xgc,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"XGBoost Classifier\")\n\n#concat all models\nmodel_performances = pd.concat([model1,model2,model3,\n                                model4,model5,model6,\n                                model7,model8,model9,\n                                model10,model11],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","d079c8c4":"model_performances\ndef output_tracer(metric,color) :\n    tracer = go.Bar(y = model_performances[\"Model\"] ,\n                    x = model_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"#6699FF\")\ntrace2  = output_tracer('Recall_score',\"red\")\ntrace3  = output_tracer('Precision',\"#33CC99\")\ntrace4  = output_tracer('f1_score',\"lightgrey\")\ntrace5  = output_tracer('Kappa_metric',\"#FFCC99\")\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","abcbee38":"lst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,15))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    plt.subplot(4,3,j+1)\n    predictions = i.predict(test_X)\n    conf_matrix = confusion_matrix(predictions,test_Y)\n    sns.heatmap(conf_matrix,annot=True,fmt = \"d\",square = True,\n                xticklabels=[\"not churn\",\"churn\"],\n                yticklabels=[\"not churn\",\"churn\"],\n                linewidths = 2,linecolor = \"w\",cmap = \"Set1\")\n    plt.title(k,color = \"b\")\n    plt.subplots_adjust(wspace = .3,hspace = .3)","d465ea7a":"lst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nplt.style.use(\"dark_background\")\nfig = plt.figure(figsize=(12,16))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    fpr,tpr,thresholds = roc_curve(test_Y,probabilities[:,1])\n    plt.plot(fpr,tpr,linestyle = \"dotted\",\n             color = \"royalblue\",linewidth = 2,\n             label = \"AUC = \" + str(np.around(roc_auc_score(test_Y,predictions),3)))\n    plt.plot([0,1],[0,1],linestyle = \"dashed\",\n             color = \"orangered\",linewidth = 1.5)\n    plt.fill_between(fpr,tpr,alpha = .4)\n    plt.fill_between([0,1],[0,1],color = \"k\")\n    plt.legend(loc = \"lower right\",\n               prop = {\"size\" : 12})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xticks(np.arange(0,1,.3))\n    plt.yticks(np.arange(0,1,.3))","0cbe6288":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n\nlst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,17))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    \n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    recall,precision,thresholds = precision_recall_curve(test_Y,probabilities[:,1])\n    plt.plot(recall,precision,linewidth = 1.5,\n             label = (\"avg_pcn : \" + \n                      str(np.around(average_precision_score(test_Y,predictions),3))))\n    plt.plot([0,1],[0,0],linestyle = \"dashed\")\n    plt.fill_between(recall,precision,alpha = .2)\n    plt.legend(loc = \"lower left\",\n               prop = {\"size\" : 10})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xlabel(\"recall\",fontsize =7)\n    plt.ylabel(\"precision\",fontsize =7)\n    plt.xlim([0.25,1])\n    plt.yticks(np.arange(0,1,.3))","029ab90e":"**2.2. Varibles distribution in customer churn**","9b3bb892":"** - Using top three categorical features**","70d54e62":"**4.4. Univariate Selection**\n\nFeature Extraction with Univariate Statistical Tests (Chi-squared for classification)\nuses the chi squared (chi^2) statistical test for non-negative features to select the best features","9a9a2cb6":"using Age, Tenure,  EstimatedSalary variables","3125f807":"**5.2. Compare model metrics**","ed2dba65":"**2.5. EstimatedSalary and CreditScore by Tenure and Churn groups**","21864268":"** 2.7. Estimated Salary, Balance  and Tenure in Customer Churn**","694edb73":"**2.5. EstimatedSalary and Balance by Tenure and Churn groups**","a84f647e":"**4.7. Visualising a decision tree from random forest classifier**","48383d06":"**5.5. Precision recall curves**","513e59ea":"**4. Fitting a Model**","b3afcabf":"**4.1. Baseline Model**","c2f6f8e2":"**1.2 Data Manipulation**","e2b1cfac":"**1. 1 Data Overview**","3f641ebd":"**4.3. Recursive Feature Elimination**\n\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.","17bfad90":"**INTRODUCTION **\n\nThis Kernel addresses the churn prediction challenge at BankCo. Every month BankCo loses thousands of customers to it\u2019s competitors.  My work is to help BankCo to predict customers may churn in future  so that they can take steps to incentivise those customers to stay. \n\nMy work consists of training a classification model that learn from a ten thousands observations and 14 columns data set. \n\nIt begins with Data overview and understanding step by which data dataset is loaded, looked at, and a lit a bit manipulated for the purpose of understanding the features. \nThe second step is Exploratory Data Analysis which dig dipper into the  visual statistics of features. It involves scatter pie, histograms, plots, bar plots, radars and much more visualization techniques for the purpose of getting relationship, distributions and patterns in data and understanding the predictive power. The third step   is data pre-processing which involves further feature engineering and transformation. New categorical features will be created. I evaluate correlation, bias and variance and I perform Principal Component Analysis to remove the correlation along dimensions of features.  Just before modelling process, Data is split into training and testing dataset followed the famous splitting rule of 80% training and 20% test data. \n\nTo start the model training process which is the forth step, I would like to put on the quote by a great statistician George Box, \u201cAll models are wrong but some are useful\u201d. Box repeated the aphorism twice more in his 1987 book, Empirical Model-Building and Response Surfaces . The first repetition is on p. 74: \"Remember that\u00a0all models are wrong; the practical question is how wrong do they have to be to not be useful.\" \n\nAs many models as possible considering the time and computing power constraints are trained. Those models include Logistic Regression(Baseline_model) , Logistic Regression(SMOTE), Decision Tree, KNN Classifier, Random Forest Classifier , Naive Bayes, SVM Classifier Linear , SVM Classifier RBF, LGBM Classifier, XGBoost Classifier.\n\nModels are evaluated using different evaluations techniques and metrics. Different models have different advantages and disadvantages looking on those metrics. Metrics are  accuracy score, recall_score, precision, f1 score, Area under the curve (AuC) and Kappa metric. \n\n","264c5ec3":"**3. Data preprocessing**","96df2c0b":"**5.4. ROC - Curves comparison**","650f70b7":"** 2.6 Average CreditScore and Balance by tenure groups**","cc24e89b":"**3.1 Data framing**","d3bb4f73":"**3.3. Correlation Matrix**","e3a14616":"**4.9. Gaussian Naive Bayes.**","211db77b":"**4.10. Support Vector Machine**\n\n\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space .where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes","d44bcdbe":"**2.4. CreditScore and Balance by Tenure and Churn groups**","62a00efb":"**4.11. Tuning parameters for support vector machine**","577bce1c":"**3.5  Binary variables distribution in customer Churn(Radar Chart)**","38ab856f":"**1. DATA OVERVIEW AND UNDERSTANDING DATA**","7ca7cccd":"**4.8. A random forest classifier.**\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement .\nBelow are the trees produced by random forest model with 10 estimated trees with maximum depth of three for each tree. Each tree produced is slightly different from other.","f77a0a0e":"** 2.9. Estimated Salary, CreditScore  and Tenure in Customer Churn**","ee811adb":"6. Conclusion ","ce9bc1ea":"**3.2. Variable Summary**","40ec89ca":"**4.2. Synthetic Minority Oversampling TEchnique (SMOTE)**\n\n* Randomly pick a point from the minority class.\n* Compute the k-nearest neighbors (for some pre-specified k) for this point.\n* Add k new points somewhere between the chosen point and each of its neighbors","745849ad":"**2. Exploratory Data Analysis**","9555efcf":"**5.3. Confusion matrices comparisons**","80d128f5":"**2.3. Customer Churn in Tenure_Categ**","b0885504":"**5. Model Performances**\n\n**5.1. model performance metrics**","3e5a145b":"**3.4. Visualising data with principal components**","eccf98c7":"**4.5. Decision Tree Visualization**\n\n**- Using top three numerical features**","3b6e8677":"**4.13. XGBoost Classifier**","d677e8b6":"**4.6. KNN Classifier** \n\nApplying KNN algorithm to smote oversampled data.","3c6b2a2d":"**2. 1. Exited Customer in data**","9e8f9f89":"Predictive predictive power of model depends on business data is it feed and the algorithms used to create  the mode. Having a good algorithm requires mathematical skills, computer science skills and mostly business knowledge skills and require sometime a team. However after going through a comprehensive data analysis, I fitted different model stating from very commonly used logistic regression. This model served as baseline model and achieved 84,38 % of accuracy. No other model achieved such accuracy using different feature selections techniques. Logistic regression is a suitable model to predict customer churn at BankCo. Whoever, with more data accessible other predictive techniques like forecasting would help to improve accuracy of the classification model.","f8596b1d":"**4.12. LightGBMClassifier**"}}