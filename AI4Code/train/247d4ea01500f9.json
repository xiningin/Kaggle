{"cell_type":{"4f7f2515":"code","a4ee6e01":"code","3fd2b98e":"code","0ee16a54":"code","4fc72994":"code","0b773798":"code","eaeb3685":"code","16960f3d":"code","1bb94c76":"code","d8465928":"code","5c31c2b6":"code","4c5ebaaf":"code","f46b1ba7":"code","0d896e29":"code","ab76c22d":"code","a54dbc67":"code","cdbe054c":"code","268150d4":"code","7a698c75":"code","6e5c6cdf":"code","89037acc":"code","a85fed55":"code","cffc26f2":"code","c20da1a8":"code","12534d65":"code","e58456ef":"markdown","a3a4fb3c":"markdown","7f2cc126":"markdown","aacc19ff":"markdown","9c3da17d":"markdown","0dc98947":"markdown","06d22360":"markdown","c1b394a0":"markdown","63814386":"markdown","6866313d":"markdown","c3ee71fd":"markdown","6d13964a":"markdown","fe6d591c":"markdown","d09664e8":"markdown","cc2ae423":"markdown","ff97fc63":"markdown"},"source":{"4f7f2515":"import numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow\nfrom tensorflow.keras.models import load_model\nimport gc\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport dlib\nfrom IPython.display import clear_output","a4ee6e01":"!cd \/kaggle\/working\/\n!mkdir frames\n!mkdir results\n!mkdir transformed\n!mkdir final\n!ls \/kaggle\/working\/","3fd2b98e":"input_path = '\/kaggle\/input\/presidentsdataset\/presidents\/trump1.mp4'\noutput_path = '\/kaggle\/working\/frames\/'\n\ndef extract_frames(input_path,output_path):\n    videocapture = cv2.VideoCapture(input_path)\n    success,image = videocapture.read()\n    count = 0\n    while success:\n        cv2.imwrite(output_path+\"%d.jpg\" % count, image)     \n        success,image = videocapture.read()\n        count += 1\n    print('Frames extraction has ended')\n    return count\n\nframes = extract_frames(input_path,output_path)","0ee16a54":"%matplotlib inline\nplt.figure()\nimage = cv2.imread('\/kaggle\/working\/frames\/120.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\nplt.show()","4fc72994":"!pip install mtcnn","0b773798":"from mtcnn import MTCNN\nimport cv2\n \ndetector = MTCNN()\nimage = cv2.imread('\/kaggle\/working\/frames\/120.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ndetections = detector.detect_faces(image)\nx, y, width, height = detections[0]['box']\nx1,y1,x2,y2 = x-10,y+10,x-10 +width + 20,y+10+height\nface = image[y1:y2, x1:x2]\n#face = cv2.resize(face, (170, 170), interpolation=cv2.INTER_AREA) #if shape is > 120x120\nface = cv2.resize(face, (120, 120), interpolation=cv2.INTER_LINEAR)\nplt.imshow(face)\nplt.show()","eaeb3685":"def extract_faces(source,destination,detector):\n    counter = 0\n    for dirname, _, filenames in os.walk(source):\n        for filename in filenames:\n            try:\n                image = cv2.imread(os.path.join(dirname, filename))\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                detections = detector.detect_faces(image)\n                x, y, width, height = detections[0]['box']\n                x1,y1,x2,y2 = x-10,y+10,x-10 +width + 20,y+10+height\n                face = image[y1:y2, x1:x2]\n                face = cv2.resize(face, (120, 120), interpolation=cv2.INTER_LINEAR)\n                plt.imsave(os.path.join(destination,filename),face)\n                clear_output(wait=True)\n                print(\"Extraction progress: \"+str(counter)+\"\/\"+str(len(filenames)-1))\n            except:\n                pass\n            counter += 1","16960f3d":"detector = MTCNN()\nextract_faces('\/kaggle\/working\/frames\/', '\/kaggle\/working\/results\/',detector)","1bb94c76":"%matplotlib inline\nplt.figure()\nimage = cv2.imread('\/kaggle\/working\/results\/120.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\nplt.show()","d8465928":"autoencoder_a = load_model(\"\/kaggle\/input\/deepfakes-model-training\/autoencoder_a.hdf5\")\nautoencoder_b = load_model(\"\/kaggle\/input\/deepfakes-model-training\/autoencoder_b.hdf5\")","5c31c2b6":"# LOADING THE ENCODER A\nencoder_a = keras.Model(autoencoder_a.layers[1].input, autoencoder_a.layers[1].output)\n# LOADING THE DECODER B\ndecoder_b = keras.Model(autoencoder_b.layers[2].input, autoencoder_b.layers[2].output)","4c5ebaaf":"def face_transform(source,destination,encoder,decoder):\n    counter = 0\n    for dirname, _, filenames in os.walk(source):\n        for filename in filenames:\n            # load the image\n            try:\n                image = cv2.imread(os.path.join(source, filename))\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = image.astype('float32')\n                image \/= 255.0\n                image = encoder.predict(np.array([image]))\n                image = decoder.predict(image)\n                image = cv2.normalize(image, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n                image = image.astype(np.uint8)\n                plt.imsave(os.path.join(destination,filename),image[0])\n                counter += 1\n                clear_output(wait=True)\n                print(\"Transformation progress: \"+str(counter)+\"\/\"+str(len(filenames)))\n            except:\n                print('exception')\n                pass","f46b1ba7":"face_transform('\/kaggle\/working\/results\/','\/kaggle\/working\/transformed',encoder_a,decoder_b)","0d896e29":"%matplotlib inline\nplt.figure()\nimage = cv2.imread('\/kaggle\/working\/transformed\/120.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\nplt.show()","ab76c22d":"!git clone https:\/\/github.com\/matthewearl\/faceswap.git","a54dbc67":"!ls \/kaggle\/working\/faceswap","cdbe054c":"%%writefile .\/faceswap\/faceswap.py\n# %load .\/faceswap\/faceswap.py\n#!\/usr\/bin\/python\n\n# Copyright (c) 2015 Matthew Earl\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n#     The above copyright notice and this permission notice shall be included\n#     in all copies or substantial portions of the Software.\n# \n#     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n#     OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n#     MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n#     NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n#     DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n#     OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n#     USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\"\"\"\nThis is the code behind the Switching Eds blog post:\n\n    http:\/\/matthewearl.github.io\/2015\/07\/28\/switching-eds-with-python\/\n\nSee the above for an explanation of the code below.\n\nTo run the script you'll need to install dlib (http:\/\/dlib.net) including its\nPython bindings, and OpenCV. You'll also need to obtain the trained model from\nsourceforge:\n\n    http:\/\/sourceforge.net\/projects\/dclib\/files\/dlib\/v18.10\/shape_predictor_68_face_landmarks.dat.bz2\n\nUnzip with `bunzip2` and change `PREDICTOR_PATH` to refer to this file. The\nscript is run like so:\n\n    .\/faceswap.py <head image> <face image>\n\nIf successful, a file `output.jpg` will be produced with the facial features\nfrom `<head image>` replaced with the facial features from `<face image>`.\n\n\"\"\"\n\nimport cv2\nimport dlib\nimport numpy\n\nimport sys\n\nPREDICTOR_PATH = '..\/input\/shape-predictor-68-face-landmarksdat\/shape_predictor_68_face_landmarks.dat' #Modifying path to right one\nSCALE_FACTOR = 1 \nFEATHER_AMOUNT = 11\n\nFACE_POINTS = list(range(17, 68))\nMOUTH_POINTS = list(range(48, 61))\nRIGHT_BROW_POINTS = list(range(17, 22))\nLEFT_BROW_POINTS = list(range(22, 27))\nRIGHT_EYE_POINTS = list(range(36, 42))\nLEFT_EYE_POINTS = list(range(42, 48))\nNOSE_POINTS = list(range(27, 35))\nJAW_POINTS = list(range(0, 17))\n\n# Points used to line up the images.\nALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS +\n                               RIGHT_BROW_POINTS + NOSE_POINTS + MOUTH_POINTS)\n\n# Points from the second image to overlay on the first. The convex hull of each\n# element will be overlaid.\nOVERLAY_POINTS = [\n    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n    NOSE_POINTS + MOUTH_POINTS,\n]\n\n# Amount of blur to use during colour correction, as a fraction of the\n# pupillary distance.\nCOLOUR_CORRECT_BLUR_FRAC = 0.6\n\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(PREDICTOR_PATH)\n\nclass TooManyFaces(Exception):\n    pass\n\nclass NoFaces(Exception):\n    pass\n\ndef get_landmarks(im):\n    rects = detector(im, 1)\n    \n    if len(rects) > 1:\n        raise TooManyFaces\n    if len(rects) == 0:\n        raise NoFaces\n\n    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n\ndef annotate_landmarks(im, landmarks):\n    im = im.copy()\n    for idx, point in enumerate(landmarks):\n        pos = (point[0, 0], point[0, 1])\n        cv2.putText(im, str(idx), pos,\n                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n                    fontScale=0.4,\n                    color=(0, 0, 255))\n        cv2.circle(im, pos, 3, color=(0, 255, 255))\n    return im\n\ndef draw_convex_hull(im, points, color):\n    points = cv2.convexHull(points)\n    cv2.fillConvexPoly(im, points, color=color)\n\ndef get_face_mask(im, landmarks):\n    im = numpy.zeros(im.shape[:2], dtype=numpy.float64)\n\n    for group in OVERLAY_POINTS:\n        draw_convex_hull(im,\n                         landmarks[group],\n                         color=1)\n\n    im = numpy.array([im, im, im]).transpose((1, 2, 0))\n\n    im = (cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0) > 0) * 1.0\n    im = cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0)\n\n    return im\n    \ndef transformation_from_points(points1, points2):\n    \"\"\"\n    Return an affine transformation [s * R | T] such that:\n\n        sum ||s*R*p1,i + T - p2,i||^2\n\n    is minimized.\n\n    \"\"\"\n    # Solve the procrustes problem by subtracting centroids, scaling by the\n    # standard deviation, and then using the SVD to calculate the rotation. See\n    # the following for more details:\n    #   https:\/\/en.wikipedia.org\/wiki\/Orthogonal_Procrustes_problem\n\n    points1 = points1.astype(numpy.float64)\n    points2 = points2.astype(numpy.float64)\n\n    c1 = numpy.mean(points1, axis=0)\n    c2 = numpy.mean(points2, axis=0)\n    points1 -= c1\n    points2 -= c2\n\n    s1 = numpy.std(points1)\n    s2 = numpy.std(points2)\n    points1 \/= s1\n    points2 \/= s2\n\n    U, S, Vt = numpy.linalg.svd(points1.T * points2)\n\n    # The R we seek is in fact the transpose of the one given by U * Vt. This\n    # is because the above formulation assumes the matrix goes on the right\n    # (with row vectors) where as our solution requires the matrix to be on the\n    # left (with column vectors).\n    R = (U * Vt).T\n\n    return numpy.vstack([numpy.hstack(((s2 \/ s1) * R,\n                                       c2.T - (s2 \/ s1) * R * c1.T)),\n                         numpy.matrix([0., 0., 1.])])\n\ndef read_im_and_landmarks(fname):\n    im = cv2.imread(fname, cv2.IMREAD_COLOR)\n    im = cv2.resize(im, (im.shape[1] * SCALE_FACTOR,\n                         im.shape[0] * SCALE_FACTOR))\n    s = get_landmarks(im)\n\n    return im, s\n\ndef warp_im(im, M, dshape):\n    output_im = numpy.zeros(dshape, dtype=im.dtype)\n    cv2.warpAffine(im,\n                   M[:2],\n                   (dshape[1], dshape[0]),\n                   dst=output_im,\n                   borderMode=cv2.BORDER_TRANSPARENT,\n                   flags=cv2.WARP_INVERSE_MAP)\n    return output_im\n\ndef correct_colours(im1, im2, landmarks1):\n    blur_amount = COLOUR_CORRECT_BLUR_FRAC * numpy.linalg.norm(\n                              numpy.mean(landmarks1[LEFT_EYE_POINTS], axis=0) -\n                              numpy.mean(landmarks1[RIGHT_EYE_POINTS], axis=0))\n    blur_amount = int(blur_amount)\n    if blur_amount % 2 == 0:\n        blur_amount += 1\n    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n\n    # Avoid divide-by-zero errors.\n    im2_blur += (128 * (im2_blur <= 1.0)).astype(im2_blur.dtype)\n\n    return (im2.astype(numpy.float64) * im1_blur.astype(numpy.float64) \/\n                                                im2_blur.astype(numpy.float64))\n\nim1, landmarks1 = read_im_and_landmarks(sys.argv[1])\nim2, landmarks2 = read_im_and_landmarks(sys.argv[2])\n\nM = transformation_from_points(landmarks1[ALIGN_POINTS],\n                               landmarks2[ALIGN_POINTS])\n\nmask = get_face_mask(im2, landmarks2)\nwarped_mask = warp_im(mask, M, im1.shape)\ncombined_mask = numpy.max([get_face_mask(im1, landmarks1), warped_mask],\n                          axis=0)\n\nwarped_im2 = warp_im(im2, M, im1.shape)\nwarped_corrected_im2 = correct_colours(im1, warped_im2, landmarks1)\n\noutput_im = im1 * (1.0 - combined_mask) + warped_corrected_im2 * combined_mask\n\n#cv2.imwrite('output.jpg', output_im) #Modifying this line to save images with different names\ncv2.imwrite(sys.argv[3], output_im)","268150d4":"def massive_face_swap(source,destination,output):\n    counter = 0\n    for dirname, _, filenames in os.walk(source):\n        for filename in filenames:\n            current_src = os.path.join(dirname, filename)\n            current_dst = os.path.join(destination, filename)\n            current_out = os.path.join(output, filename)\n            !python \/kaggle\/working\/faceswap\/faceswap.py {current_dst} {current_src} {current_out}\n            clear_output(wait=True)\n            print(\"Swap progress: \"+str(counter)+\"\/\"+str(len(filenames)-1))\n            counter += 1","7a698c75":"massive_face_swap('.\/transformed','.\/frames','.\/final')","6e5c6cdf":"%matplotlib inline\nplt.figure()\nimage = cv2.imread('\/kaggle\/working\/final\/150.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\nplt.show()","89037acc":"%matplotlib inline\nplt.figure()\nimage = cv2.imread('\/kaggle\/working\/frames\/150.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\nplt.show()","a85fed55":"frames = []\nfor i in range(1701):\n    image = image = cv2.imread(os.path.join('\/kaggle\/working\/final', str(i)+'.jpg'))\n    height, width, layers = image.shape\n    frames.append(image)\nframes = np.array(frames)\nvideomaker = cv2.VideoWriter('\/kaggle\/working\/deepfake.avi', cv2.VideoWriter_fourcc(*'DIVX'), 25, (width,height))\n \nfor frame in frames:\n    videomaker.write(frame)\n\nvideomaker.release()","cffc26f2":"!ls \/kaggle\/working\/","c20da1a8":"!zip -r \/kaggle\/working\/output.zip \/kaggle\/working\/","12534d65":"!rm -rf  \/kaggle\/working\/*.jpg","e58456ef":"# Faces transformation","a3a4fb3c":"# Imports","7f2cc126":"## Cloning the script to swap faces","aacc19ff":"# Face swapping","9c3da17d":"In case the final output doesn't download, use this link -> \n<a href=\".\/output.zip\"> Download File <\/a>","0dc98947":"# Plotting the original frame","06d22360":"# Faces extraction","c1b394a0":"## Swapping faces massively","63814386":"# Video rebuilding","6866313d":"# Directories creation","c3ee71fd":"# Deep Fakes (3\/3) - Face Swapping stage","6d13964a":"This notebook is part of a group of notebooks that together belong to a Deep Fakes article. I try to teach all of you how to build Autoencoders for Deep Fakes from scratch.\n\nIn this particular notebook, I'll go through several steps to extract frames from a video, extract faces from them, use encoders and decoders obtained in [a previous notebook](https:\/\/www.kaggle.com\/sergiovirahonda\/deepfakes-model-training) in order to transform those faces into new ones with their particular features but with the facial expressions from the source faces. At the end, I'll swap faces in the source images with the respective transformed ones and rebuild the video.","fe6d591c":"# Frames extraction","d09664e8":"## Making some modifications to make the script runnable in the Notebook","cc2ae423":"## Plotting a sample","ff97fc63":"I hope you've enjoyed this project! See you around :)"}}