{"cell_type":{"b04ac8be":"code","ae691da7":"code","3d9c2cfb":"code","6a306e17":"code","ca98b500":"code","a79ce161":"code","9b8c1ff2":"code","153a5905":"code","9148b282":"code","04a48730":"code","b05e7589":"code","c921d7f5":"code","efdcca24":"code","72407516":"code","9a9e0d09":"code","5d01d8e7":"code","c8fd0459":"code","997d3590":"code","7e270443":"code","81f6cf44":"code","14712d5d":"code","2bada2a2":"code","a34ca4fa":"code","88fbc417":"code","00a032bf":"code","55ce86b5":"code","1517bdb5":"code","c5399770":"code","19f9bfd0":"code","b54dfe51":"code","08a1bce2":"code","14402c46":"code","4c58176e":"code","8393e5b3":"code","adfa152a":"code","5df5b710":"code","d14c122a":"code","2ba0986d":"code","bb671b47":"code","5b839129":"code","14467423":"code","3ce2a558":"code","450e83ed":"code","5330d504":"code","c208f5b1":"code","feff4d03":"code","bf0015b6":"code","3fe21e30":"code","17744888":"code","6cda7ea2":"code","705f7830":"code","2789e6be":"code","a51b8284":"code","0ff17966":"code","dde46965":"code","f5193a45":"code","5c445283":"code","fd69b4ab":"code","3981ac45":"code","012a0e18":"code","1b0a5de1":"code","30ee8c6f":"code","5e8b63c5":"code","ef5e7d6f":"code","204deb29":"code","22d92011":"code","db166f92":"code","52fa5ff3":"code","2d356add":"code","53ab2c8b":"code","32df7230":"code","d370916f":"code","9fbcc8f2":"code","1d9338a0":"code","61216a2e":"code","61673b31":"code","cb831893":"code","b6fe5490":"code","f7a3cd14":"code","9ace41d3":"code","861bd305":"code","5595dc58":"code","928405e3":"code","0b1ced95":"code","07f614a5":"code","5a5e7aed":"code","ad1f4a3c":"code","9255c318":"code","6f27d974":"code","db42e267":"code","973fd268":"code","9ced3c68":"code","0036d83e":"code","6986bdc7":"markdown","11fbd415":"markdown","28f2d1f7":"markdown","28600009":"markdown","f8688952":"markdown","eb4465cf":"markdown","5fcce3d3":"markdown","78d40b39":"markdown","418b148f":"markdown","e34a2512":"markdown","22e67bad":"markdown","125cf99f":"markdown","67ffdd65":"markdown","04d1c89b":"markdown","fdb4c319":"markdown","32ac61a1":"markdown","21974853":"markdown","78e4c7d1":"markdown","70a223eb":"markdown","659c8add":"markdown","c5a9a9f2":"markdown","c696bce9":"markdown","39c6431b":"markdown","f18b0eaa":"markdown","648deb2d":"markdown","51e962ac":"markdown","75ffa220":"markdown","50e0434f":"markdown","c7111b53":"markdown","c9628636":"markdown","ff3441df":"markdown","4c78fc72":"markdown","4e9a57bb":"markdown","f3300813":"markdown","28f829b6":"markdown","68d0a004":"markdown","01c3fed4":"markdown","0f0bf6db":"markdown","5f4db58d":"markdown","60292492":"markdown","d61c688e":"markdown","2483955d":"markdown","c7b53b64":"markdown","837595a0":"markdown","9c2fb537":"markdown","2f273a71":"markdown","8265cbf7":"markdown","2dd83fe2":"markdown","23a5b61a":"markdown","d121db00":"markdown","69e186b2":"markdown","e3080041":"markdown","f5c57faa":"markdown","bbc3c464":"markdown","16c8ae5f":"markdown","4dc3b050":"markdown","a4be5e61":"markdown","984595e3":"markdown","e959623a":"markdown"},"source":{"b04ac8be":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import hstack\nimport seaborn as sns\nsns.set()\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier as rgb\nfrom xgboost import XGBRFClassifier as xgb\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import log_loss, confusion_matrix, accuracy_score\n\nfrom sklearn.ensemble import StackingClassifier\nfrom prettytable import PrettyTable, MSWORD_FRIENDLY, DEFAULT","ae691da7":"stroke = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","3d9c2cfb":"stroke.head()","6a306e17":"stroke.info()","ca98b500":"stroke.describe()","a79ce161":"stroke.shape","9b8c1ff2":"stroke.isnull().sum()","153a5905":"stroke.bmi.replace(to_replace=np.nan, value=stroke.bmi.mean(), inplace=True)","9148b282":"stroke.isnull().sum()","04a48730":"stroke.nunique()","b05e7589":"stroke.stroke.value_counts()","c921d7f5":"stroke.gender.unique()","efdcca24":"stroke.hypertension.unique()","72407516":"stroke.heart_disease.unique()","9a9e0d09":"stroke.ever_married.unique()","5d01d8e7":"stroke.work_type.unique()","c8fd0459":"stroke.Residence_type.unique()","997d3590":"stroke.smoking_status.unique()","7e270443":"sns.boxplot(data=stroke, y='bmi')\nplt.title('Boxplot of bmi')\nplt.show()","81f6cf44":"for i in range(0, 110, 10):\n    print(f'The {i}th percentile of BMI is: {np.percentile(stroke.bmi, i)}')","14712d5d":"for i in range(90, 101, 1):\n    print(f'The {i}th percentile of BMI is: {np.percentile(stroke.bmi, i)}')","2bada2a2":"for i in np.arange(0, 1.1, 0.1):\n    print(f'The {99+i}th percentile of BMI is: {np.percentile(stroke.bmi, 99+i)}')","a34ca4fa":"sns.boxplot(data=stroke, y='avg_glucose_level')\nplt.title('Boxplot of avg_glucose_level')\nplt.show()","88fbc417":"for i in range(0, 110, 10):\n    print(f'The {i}th percentile of Average Glucose Level is: {np.percentile(stroke.avg_glucose_level, i)}')","00a032bf":"for i in range(90, 101):\n    print(f'The {i}th percentile of Average Glucose Level is: {np.percentile(stroke.avg_glucose_level, i)}')","55ce86b5":"for i in np.arange(0, 1.1, 0.1):\n    print(f'The {99+i}th percentile of Average Glucose Level is: {np.percentile(stroke.avg_glucose_level, 99+i)}')","1517bdb5":"sns.boxplot(data=stroke, y='age')\nplt.title('Boxplot of age')\nplt.show()","c5399770":"plots = sns.countplot(x='hypertension', hue='stroke', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\n    \nplt.title('Effect of Hypertension on Stroke')","19f9bfd0":"plots = sns.countplot(x='gender', hue='stroke', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\nplt.title('Stroke based on Gender')","b54dfe51":"plots = sns.countplot(x='heart_disease', hue='stroke', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\nplt.title('Effect of Heart Disease on Stroke')","08a1bce2":"plots = sns.countplot(x='ever_married', hue='stroke', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\nplt.title('Effect of Marital Status on Stroke')","14402c46":"plots = sns.countplot(x='Residence_type', hue='stroke', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\nplt.title('Effect of Residence Type on Stroke')","4c58176e":"plots = sns.countplot(x='work_type', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\nplt.title('Work Type of People')","8393e5b3":"plots = sns.countplot(x='work_type', hue='stroke', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\nplt.title('Effect of Work Type on Stroke')","adfa152a":"plots = sns.countplot(x='smoking_status', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\nplt.title('Number of Smokers\/Non Smokers')","5df5b710":"plots = sns.countplot(x='smoking_status', hue='stroke', data=stroke)\n\nfor bar in plots.patches:\n    plots.annotate(f'{round(bar.get_height()\/len(stroke)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n\nplt.title('Effect of Smoking on Stroke')","d14c122a":"correlation = stroke.corr()","2ba0986d":"plt.figure(figsize=(10,10))\nsns.heatmap(correlation, annot=True)","bb671b47":"stroke.drop(labels='id', axis=1, inplace=True)","5b839129":"stroke.head()","14467423":"stroke.drop(stroke[stroke.bmi>65].index, inplace=True)\nstroke.shape","3ce2a558":"stroke.head()","450e83ed":"x = stroke.iloc[:,:-1]\ny = stroke.iloc[:,-1]","5330d504":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y)\nx_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train)","c208f5b1":"x_train.shape","feff4d03":"x_test.shape","bf0015b6":"y_train.value_counts()","3fe21e30":"rom = RandomOverSampler(random_state=42)\nx_train, y_train = rom.fit_resample(x_train, y_train)","17744888":"x_train.shape","6cda7ea2":"y_train.value_counts()","705f7830":"ohe = ColumnTransformer([('ohe', OneHotEncoder(handle_unknown='ignore'), ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'])], remainder='passthrough')\nohe.fit(x_train)\nprint(x_train.shape, y_train.shape)\n\nx_train_ohe = ohe.transform(x_train)\nx_cv_ohe = ohe.transform(x_cv)\nx_test_ohe = ohe.transform(x_test)\n\nprint('After Vectorization......')\nprint(x_train_ohe.shape, y_train.shape)\nprint(x_cv_ohe.shape, y_cv.shape)\nprint(x_test_ohe.shape, y_test.shape)","2789e6be":"x_train_ohe = x_train_ohe[:, :16]\nx_cv_ohe = x_cv_ohe[:, :16]\nx_test_ohe = x_test_ohe[:, :16]\n\nprint('After Vectorization......')\nprint(x_train_ohe.shape, y_train.shape)\nprint(x_cv_ohe.shape, y_cv.shape)\nprint(x_test_ohe.shape, y_test.shape)","a51b8284":"features = ohe.get_feature_names()\nfeatures","0ff17966":"x_train_hyp = np.array(x_train['hypertension']).reshape((-1,1))\nx_cv_hyp = np.array(x_cv['hypertension']).reshape((-1,1))\nx_test_hyp = np.array(x_test['hypertension']).reshape((-1,1))\n\nprint('After Vectorization......')\nprint(x_train_hyp.shape, y_train.shape)\nprint(x_cv_hyp.shape, y_cv.shape)\nprint(x_test_hyp.shape, y_test.shape)","dde46965":"x_train_hd = np.array(x_train['heart_disease']).reshape((-1,1))\nx_cv_hd = np.array(x_cv['heart_disease']).reshape((-1,1))\nx_test_hd = np.array(x_test['heart_disease']).reshape((-1,1))\n\nprint('After Vectorization......')\nprint(x_train_hd.shape, y_train.shape)\nprint(x_cv_hd.shape, y_cv.shape)\nprint(x_test_hd.shape, y_test.shape)","f5193a45":"std = ColumnTransformer([('norm', MinMaxScaler(), ['age', 'avg_glucose_level', 'bmi'])], remainder='drop')\nstd.fit(x_train)\nprint(x_train.shape, y_train.shape)\n\nx_train_std = std.transform(x_train)\nx_cv_std = std.transform(x_cv)\nx_test_std = std.transform(x_test)\n\nprint('After Vectorization......')\nprint(x_train_std.shape, y_train.shape)\nprint(x_cv_std.shape, y_cv.shape)\nprint(x_test_std.shape, y_test.shape)","5c445283":"x_tr = np.hstack((x_train_ohe.astype(np.float), x_train_hyp.astype(np.float), x_train_hd.astype(np.float), x_train_std.astype(np.float)))\nx_cv = np.hstack((x_cv_ohe.astype(np.float), x_cv_hyp.astype(np.float), x_cv_hd.astype(np.float), x_cv_std.astype(np.float)))\nx_te = np.hstack((x_test_ohe.astype(np.float), x_test_hyp.astype(np.float), x_test_hd.astype(np.float), x_test_std.astype(np.float)))\n\nprint(\"Final Data Matrix Shape is........\")\nprint(x_tr.shape,y_train.shape)\nprint(x_cv.shape,y_cv.shape)\nprint(x_te.shape,y_test.shape)","fd69b4ab":"def cnf_matrix(true_y, pred_y):\n\n    cf_matrix = confusion_matrix(y_test, predicted_y)\n    print('-'*40, 'Confusion Matrix', '-'*40)\n    group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\n    labels = [f'{v1}\\n{v2}\\n' for v1, v2 in zip(group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=\"YlGnBu\")\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    # Precision Matrix\n    pc_matrix =(cf_matrix\/cf_matrix.sum(axis=0))\n    print(\"-\"*40, \"Precision matrix (Columm Sum=1)\", \"-\"*40)\n    sns.heatmap(pc_matrix, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=[0,1], yticklabels=[0,1])\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    # Recall Matrix\n    rl_matrix =(((cf_matrix.T)\/(cf_matrix.sum(axis=1))).T)\n    print(\"-\"*40, \"Recall matrix (Row sum=1)\", \"-\"*40)\n    sns.heatmap(rl_matrix, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=[0,1], yticklabels=[0,1])\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')","3981ac45":"test_data_len = x_test.shape[0]\ncv_data_len = x_cv.shape[0]\n\n# we create a output array that has exactly same size as the CV data\ncv_predicted_y = np.zeros((cv_data_len,2))\nfor i in range(cv_data_len):\n    rand_probs = np.random.rand(1,2)\n    cv_predicted_y[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\n    \n# Test-Set error.\n# We create a output array that has exactly same as the test data\ntest_predicted_y = np.zeros((test_data_len,2))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,2)\n    test_predicted_y[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\n\npredicted_y = np.argmax(test_predicted_y, axis=1)\npredicted_cv = np.argmax(cv_predicted_y, axis=1)\n\n\nll_rm_cv = log_loss(y_cv,cv_predicted_y, eps=1e-15)\nac_rm_cv = accuracy_score(y_cv, predicted_cv)\nll_rm_te = log_loss(y_test,test_predicted_y, eps=1e-15)\nac_rm_te = accuracy_score(y_test, predicted_y)\n\nprint(\"Log loss on Cross Validation Data using Random Model\",ll_rm_cv)\nprint(\"Log loss on Test Data using Random Model\",ll_rm_te)\nprint('Accuracy on Cross Validation using Random Model', ac_rm_cv)\nprint('Accuracy on Test Data using Random Model', ac_rm_te)\n\ncnf_matrix(y_test, predicted_y)","012a0e18":"alpha = [10 ** x for x in range(-6, 4)]\nparams = {'alpha':alpha}\nclf1 = SGDClassifier(loss='log', n_jobs=-1, random_state=42)\nr_search = RandomizedSearchCV(clf1, param_distributions=params, return_train_score=True, random_state=42)\nr_search.fit(x_tr, y_train)","1b0a5de1":"print(f'The best hyperparameter values is {r_search.best_params_} at which the score is {r_search.best_score_}')","30ee8c6f":"clf1 = SGDClassifier(loss='log', n_jobs=-1, random_state=42, **r_search.best_params_)\nclf1.fit(x_tr, y_train)\ncal_clf1 = CalibratedClassifierCV(clf1, cv='prefit')\ncal_clf1.fit(x_tr, y_train)","5e8b63c5":"y_pred_cv = cal_clf1.predict(x_cv)\ny_prob_cv = cal_clf1.predict_proba(x_cv)\ny_pred = cal_clf1.predict(x_te)\ny_prob = cal_clf1.predict_proba(x_te)","ef5e7d6f":"ll_lg_cv = log_loss(y_cv, y_prob_cv, eps=1e-15)\nac_lg_cv = accuracy_score(y_cv, y_pred_cv)\nll_lg_te = log_loss(y_test, y_prob, eps=1e-15)\nac_lg_te = accuracy_score(y_test, y_pred)\n\n\nprint(\"Log loss on Cross Validation Data using Logistic Regression\",ll_lg_cv)\nprint(\"Log loss on Test Data using Logistic Regression\",ll_lg_te)\nprint('Accuracy on Cross Validation using Logistic Regression', ac_lg_cv)\nprint('Accuracy on Test Data using Logistic Regression', ac_lg_te)\n\ncnf_matrix(y_test, y_pred)","204deb29":"importance = clf1.coef_\n# summarize feature importance\nfor i,v in enumerate(importance[0]):\n    print(f'Feature: {i}, Score: {v}')\n\nplt.figure(figsize=(20,7))    \nsns.barplot(x=[x for x in range(importance.shape[1])], y=importance[0]).set_xticklabels(features, rotation=90)\nplt.show()","22d92011":"alpha = [10 ** x for x in range(-6, 4)]\nparams = {'alpha':alpha}\nclf2 = SGDClassifier(loss='hinge', n_jobs=-1, random_state=42)\nr_search = RandomizedSearchCV(clf2, param_distributions=params, return_train_score=True, random_state=42)\nr_search.fit(x_tr, y_train)","db166f92":"print(f'The best hyperparameter values is {r_search.best_params_} at which the score is {r_search.best_score_}')","52fa5ff3":"clf2 = SGDClassifier(loss='hinge', n_jobs=-1, random_state=42, **r_search.best_params_)\nclf2.fit(x_tr, y_train)\ncal_clf2 = CalibratedClassifierCV(clf2, cv='prefit')\ncal_clf2.fit(x_tr, y_train)","2d356add":"y_pred_cv = cal_clf2.predict(x_cv)\ny_prob_cv = cal_clf2.predict_proba(x_cv)\ny_pred = cal_clf2.predict(x_te)\ny_prob = cal_clf2.predict_proba(x_te)","53ab2c8b":"ll_svm_cv = log_loss(y_cv, y_prob_cv, eps=1e-15)\nac_svm_cv = accuracy_score(y_cv, y_pred_cv)\nll_svm_te = log_loss(y_test, y_prob, eps=1e-15)\nac_svm_te = accuracy_score(y_test, y_pred)\n\n\nprint(\"Log loss on Cross Validation Data using SVM\",ll_svm_cv)\nprint(\"Log loss on Test Data using SVM\",ll_svm_te)\nprint('Accuracy on Cross Validation using SVM', ac_svm_cv)\nprint('Accuracy on Test Data using SVM', ac_svm_te)\n\ncnf_matrix(y_test, y_pred)","32df7230":"importance = clf2.coef_\n# summarize feature importance\nfor i,v in enumerate(importance[0]):\n    print(f'Feature: {i}, Score: {v}')\n\nplt.figure(figsize=(20,7))    \nsns.barplot(x=[x for x in range(importance.shape[1])], y=importance[0]).set_xticklabels(features, rotation=90)\nplt.show()","d370916f":"alpha = [0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,50,100]\nparams = {'alpha':alpha}\nclf3 = MultinomialNB()\nr_search = RandomizedSearchCV(clf3, param_distributions=params, return_train_score=True, random_state=42)\nr_search.fit(x_tr, y_train)","9fbcc8f2":"print(f'The best hyperparameter values is {r_search.best_params_} at which the score is {r_search.best_score_}')","1d9338a0":"clf3 = MultinomialNB(**r_search.best_params_)\nclf3.fit(x_tr, y_train)\ncal_clf3 = CalibratedClassifierCV(clf3, cv='prefit')\ncal_clf3.fit(x_tr, y_train)","61216a2e":"y_pred_cv = cal_clf3.predict(x_cv)\ny_prob_cv = cal_clf3.predict_proba(x_cv)\ny_pred = cal_clf3.predict(x_te)\ny_prob = cal_clf3.predict_proba(x_te)","61673b31":"ll_nb_cv = log_loss(y_cv, y_prob_cv, eps=1e-15)\nac_nb_cv = accuracy_score(y_cv, y_pred_cv)\nll_nb_te = log_loss(y_test, y_prob, eps=1e-15)\nac_nb_te = accuracy_score(y_test, y_pred)\n\n\nprint(\"Log loss on Cross Validation Data using Naive Bayes\",ll_nb_cv)\nprint(\"Log loss on Test Data using Naive Bayes\",ll_nb_te)\nprint('Accuracy on Cross Validation using Naive Bayes', ac_nb_cv)\nprint('Accuracy on Test Data using Naive Bayes', ac_nb_te)\n\ncnf_matrix(y_test, y_pred)","cb831893":"# Maximum number of levels in tree\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'log2', None]\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 3, 4, 5]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nparams = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nclf4 = rgb(n_jobs=-1, random_state=42)\nr_search = RandomizedSearchCV(clf4, param_distributions=params, return_train_score=True, random_state=42)\nr_search.fit(x_tr, y_train)","b6fe5490":"print(f'The best hyperparameter values are {r_search.best_params_} at which the score is {r_search.best_score_}')","f7a3cd14":"clf4 = rgb(**r_search.best_params_, n_jobs=-1, random_state=42)\nclf4.fit(x_tr, y_train)\ncal_clf4 = CalibratedClassifierCV(clf4, cv='prefit')\ncal_clf4.fit(x_tr, y_train)","9ace41d3":"y_pred_cv = cal_clf4.predict(x_cv)\ny_prob_cv = cal_clf4.predict_proba(x_cv)\ny_pred = cal_clf4.predict(x_te)\ny_prob = cal_clf4.predict_proba(x_te)","861bd305":"ll_rf_cv = log_loss(y_cv, y_prob_cv, eps=1e-15)\nac_rf_cv = accuracy_score(y_cv, y_pred_cv)\nll_rf_te = log_loss(y_test, y_prob, eps=1e-15)\nac_rf_te = accuracy_score(y_test, y_pred)\n\n\nprint(\"Log loss on Cross Validation Data using RF\",ll_rf_cv)\nprint(\"Log loss on Test Data using RF\",ll_rf_te)\nprint('Accuracy on Cross Validation using RF', ac_rf_cv)\nprint('Accuracy on Test Data using RF', ac_rf_te)\n\ncnf_matrix(y_test, y_pred)","5595dc58":"importance = clf4.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print(f'Feature: {i}, Score: {v}')\n\nplt.figure(figsize=(20,7))    \nsns.barplot(x=[x for x in range(len(importance))], y=importance).set_xticklabels(features, rotation=90)\nplt.show()","928405e3":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n\nlearning_rate = [x for x in np.linspace(start=0.01, stop=0.2, num=10)]\n\nmin_child_weight = [1, 3, 5, 7]\n\nmax_depth = [3, 5, 7, 9]\n\nsubsample = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n\ncolsample_bytree = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n\n# Create the random grid\nparams = {'n_estimators': n_estimators, \n          'learning_rate':learning_rate,\n          'min_child_weight': min_child_weight,\n          'max_depth': max_depth,\n          'subsample': subsample,\n          'colsample_bytree': colsample_bytree}\nclf5 = xgb(n_jobs=-1, random_state=42)\nr_search = RandomizedSearchCV(clf5, param_distributions=params, return_train_score=True, random_state=42)\nr_search.fit(x_tr, y_train)","0b1ced95":"print(f'The best hyperparameter values are {r_search.best_params_} at which the score is {r_search.best_score_}')","07f614a5":"clf5 = xgb(**r_search.best_params_, n_jobs=-1, random_state=42)\nclf5.fit(x_tr, y_train)\ncal_clf5 = CalibratedClassifierCV(clf5, cv='prefit')\ncal_clf5.fit(x_tr, y_train)","5a5e7aed":"y_pred_cv = cal_clf5.predict(x_cv)\ny_prob_cv = cal_clf5.predict_proba(x_cv)\ny_pred = cal_clf5.predict(x_te)\ny_prob = cal_clf5.predict_proba(x_te)","ad1f4a3c":"ll_xg_cv = log_loss(y_cv, y_prob_cv, eps=1e-15)\nac_xg_cv = accuracy_score(y_cv, y_pred_cv)\nll_xg_te = log_loss(y_test, y_prob, eps=1e-15)\nac_xg_te = accuracy_score(y_test, y_pred)\n\n\nprint(\"Log loss on Cross Validation Data using RF\",ll_xg_cv)\nprint(\"Log loss on Test Data using RF\",ll_xg_te)\nprint('Accuracy on Cross Validation using RF', ac_xg_cv)\nprint('Accuracy on Test Data using RF', ac_xg_te)\n\ncnf_matrix(y_test, y_pred)","9255c318":"importance = clf5.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print(f'Feature: {i}, Score: {v}')\n\nplt.figure(figsize=(20,7))    \nsns.barplot(x=[x for x in range(len(importance))], y=importance).set_xticklabels(features, rotation=90)\nplt.show()","6f27d974":"estimators = [('svc', clf2), ('nb', clf3), ('rf',  clf4)]\n\nscl = StackingClassifier(estimators=estimators, final_estimator=clf1, n_jobs=-1)\nscl.fit(x_tr, y_train)\ncal_clf = CalibratedClassifierCV(scl, cv='prefit')\ncal_clf.fit(x_tr, y_train)","db42e267":"y_pred_cv = cal_clf.predict(x_cv)\ny_prob_cv = cal_clf.predict_proba(x_cv)\ny_pred = cal_clf.predict(x_te)\ny_prob = cal_clf.predict_proba(x_te)","973fd268":"ll_sc_cv = log_loss(y_cv, y_prob_cv, eps=1e-15)\nac_sc_cv = accuracy_score(y_cv, y_pred_cv)\nll_sc_te = log_loss(y_test, y_prob, eps=1e-15)\nac_sc_te = accuracy_score(y_test, y_pred)\n\n\nprint(\"Log loss on Cross Validation Data using Stacking Classifier\",ll_sc_cv)\nprint(\"Log loss on Test Data using Stacking Classifier\",ll_sc_te)\nprint('Accuracy on Cross Validation using Stacking Classifier', ac_sc_cv)\nprint('Accuracy on Test Data using Stacking Classifier', ac_sc_te)\n\ncnf_matrix(y_test, y_pred)","9ced3c68":"table = PrettyTable()\ntable.field_names = ['Model', 'CV Log Loss', 'Test Log Loss', 'CV Accuracy', 'Test Accuracy']\ntable.add_rows([['Random Model', round(ll_rm_cv,3), round(ll_rm_te,4), round(ac_rm_cv,3), round(ac_rm_te,3)],\n                ['Logistic Regression', round(ll_lg_cv,3), round(ll_lg_te,3), round(ac_lg_cv,3), round(ac_lg_te,3)],\n                ['Naive Bayes', round(ll_nb_cv,3), round(ll_nb_te,3), round(ac_nb_cv,3), round(ac_nb_te,3)],\n                ['SVM', round(ll_svm_cv,3), round(ll_svm_te,3), round(ac_svm_cv,3), round(ac_svm_te,3)],\n                ['Random Forest(Scikit)', round(ll_rf_cv,3), round(ll_rf_te,3), round(ac_rf_cv,3), round(ac_rf_te,3)],\n                ['Random Forest(Xgboost)', round(ll_xg_cv,3), round(ll_xg_te,3), round(ac_xg_cv,3), round(ac_xg_te,3)],\n                ['Stacking Classifier', round(ll_sc_cv,3), round(ll_sc_te,3), round(ac_sc_cv,3), round(ac_sc_te,3)]])","0036d83e":"table.set_style(DEFAULT)\nprint(table)","6986bdc7":"### Hyperparameter Tuning","11fbd415":"# Introduction","28f2d1f7":"## Splitting The Dataset","28600009":"<p style=\"font-size:32px;text-align:center\"> <b>Stroke Prediction<\/b> <\/p>","f8688952":"### Performance of the model","eb4465cf":"### Performance of the model","5fcce3d3":"### Hyperparameter Tuning","78d40b39":"### Performance of the model","418b148f":"# Removing Outliers and Redundant Columns","e34a2512":"### Performance of the model","22e67bad":"### Feature Importance","125cf99f":"### Hyperparameter Tuning","67ffdd65":"## Univariate  Analysis","04d1c89b":"### Marital Status vs Stroke","fdb4c319":"## Support Vector Machines","32ac61a1":"### Work Type vs Stroke","21974853":"### Training the model","78e4c7d1":"### Boxplot of Average Glucose Level","70a223eb":"### Hyperparameter Tuning","659c8add":"### Training the model","c5a9a9f2":"### Heart Disease vs Stroke","c696bce9":"**Our data is imbalanced.**","39c6431b":"- A stroke occurs when a blood vessel in the brain ruptures and bleeds, or when there\u2019s a blockage in the blood    supply to the brain. The rupture or blockage prevents blood and oxygen from reaching the brain\u2019s tissues.\n\n- Risk factors for stroke\n\nCertain risk factors make you more susceptible to stroke. According to the National Heart, Lung, and Blood InstituteTrusted Source, the more risk factors you have, the more likely you are to have a stroke. Risk factors for stroke include:\n\n**Diet**\n\nAn unhealthy diet that increases your risk of stroke is one that\u2019s high in:\n\n    salt\n    saturated fats\n    trans fats\n    cholesterol\n\n**Inactivity**\n\nInactivity, or lack of exercise, can also raise your risk for stroke.\n\nRegular exercise has a number of health benefits. The CDC recommends that adults get at least 2.5 hoursTrusted Source of aerobic exercise every week. This can mean simply a brisk walk a few times a week.\n\n**Alcohol consumption**\n\nYour risk for stroke also increases if you drink too much alcohol. Alcohol consumption should be done in moderation. This means no more than one drink per day for women, and no more than two for men. More than that may raise blood pressure levels as well as triglyceride levels, which can cause atherosclerosis.\n\n**Tobacco use**\n\nUsing tobacco in any form also raises your risk for stroke, since it can damage your blood vessels and heart. This is further increased when smoking, because your blood pressure rises when you use nicotine.\nPersonal background\n\nThere are certain personal risk factors for stroke that you can\u2019t control. Stroke risk can be linked to your:\n\n    Family history: Stroke risk is higher in some families because of genetic health issues, such as high blood pressure.\n    Sex: While both women and men can have strokes, they\u2019re more common in women than in men in all age groups.\n    Age: The older you are, the more likely you are to have a stroke.\n    Race and ethnicity: Caucasians, Asian Americans, and Hispanics are less likely to have a stroke than African-Americans, Alaska Natives, and American Indians.\n\n**Health history**\n\nCertain medical conditions are linked to stroke risk. These include:\n\n    a previous stroke or TIA\n    high blood pressure\n    high cholesterol\n    heart disorders, such as coronary artery disease\n    heart valve defects\n    enlarged heart chambers and irregular heartbeats\n    sickle cell disease\n    diabetes\n    \n    \n    \n![stroke-image](https:\/\/www.mayoclinic.org\/-\/media\/kcms\/gbs\/patient-consumer\/images\/2013\/11\/15\/17\/44\/ds00150_ds01030_my00077_im00074_r7_ischemicstrokethu_jpg.jpg)","f18b0eaa":"### Performance of the model","648deb2d":"### Combining all encoded columns","51e962ac":"# Exploratory Data Analysis","75ffa220":"### Hypertension vs Stroke","50e0434f":"### Hyperparameter Tuning","c7111b53":"# Machine Learning Models","c9628636":"## Naive Bayes","ff3441df":"### Training the model","4c78fc72":"# Importing Libraries and Data","4e9a57bb":"### Feature Importance","f3300813":"## Random Forest using Xgboost","28f829b6":"### Feature Importance","68d0a004":"So our best model is Random Forest using sklearn with **an accuracy of 94.6% and log loss of 0.322**","01c3fed4":"Splitting the dataset into train and test in 80:20.","0f0bf6db":"## Random Forest using Sklearn","5f4db58d":"## Bivariate Analysis","60292492":"1. id: unique identifier\n\n2. gender: \"Male\", \"Female\" or \"Other\"\n\n3. age: age of the patient\n\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6. ever_married: \"No\" or \"Yes\"\n\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8. Residence_type: \"Rural\" or \"Urban\"\n\n9. avg_glucose_level: average glucose level in blood\n\n10. bmi: body mass index\n\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12. stroke: 1 if the patient had a stroke or 0 if not\n\n**Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient**","d61c688e":"### Boxplot of bmi","2483955d":"# Summary of Performance","c7b53b64":"## Correlation Heatmap","837595a0":"### Boxplot of age","9c2fb537":"## Logistic Regression","2f273a71":"## Random Model and its Performance","8265cbf7":"## One-Hot Encoding of Categorical Data","2dd83fe2":"### Feature Importance","23a5b61a":"### Gender vs Stroke","d121db00":"## Stacking Classifier","69e186b2":"**From the above calculations we can see that 99.9% of people have BMI less than 65.**","e3080041":"### Training the model","f5c57faa":"# Cleaning of Data","bbc3c464":"### Residence Type vs Stroke","16c8ae5f":"### Smoking vs Stroke","4dc3b050":"### Training the model","a4be5e61":"# Data ","984595e3":"### Performance of the model","e959623a":"![](http:\/\/)"}}