{"cell_type":{"d5423708":"code","cc9fd436":"code","73c6a0f4":"code","ce45000d":"code","05e79ac8":"code","33eb3102":"code","f2f8bbe6":"code","623fe68a":"code","d633097b":"code","4f4b19b2":"code","2606d98a":"code","83c76486":"code","af123b5f":"code","1be37b66":"code","121dc21b":"code","3b43b1b9":"code","64754e6e":"code","7d244cea":"markdown","ae30f9c0":"markdown","8ae204b0":"markdown","895ad947":"markdown","0720c9e2":"markdown","c67728ab":"markdown","8e376732":"markdown"},"source":{"d5423708":"import os\nimport gc\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\nfrom scipy import stats","cc9fd436":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeature_columns = ['investment_id', 'time_id'] + features\ntrain = pd.read_pickle('..\/input\/ubiquant-market-prediction-half-precision-pickle\/train.pkl')\ntrain.head()","73c6a0f4":"# from itertools import combinations\n\n# cols_interaction = sorted(\n#     ['f_271', 'f_298', 'f_184', 'f_253', 'f_250', 'f_231', 'f_269', 'f_78', 'f_209', 'f_19', 'f_241', 'f_43', 'f_182'],\n#     key=lambda x: int(x[2:])\n# )\n# for x1, x2 in list(combinations(cols_interaction, r=2)):\n#     train[f'{x1}_{x2}'] = train[x1] * train[x2]\n    \n# train","ce45000d":"train.info()","05e79ac8":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","33eb3102":"_ = train.pop(\"time_id\")","f2f8bbe6":"y = train.pop(\"target\")\ny.head()","623fe68a":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer.adapt(investment_id)","d633097b":"def preprocess(X, y):\n    print(X)\n    print(y)\n    return X, y\n\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","4f4b19b2":"USE_PCA = True\n\nif USE_PCA:\n    n_components = 80\nelse:\n    n_components = train.shape[1]\n\n    \ndef get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((n_components, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.4)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.4)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.4)(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    \n    output = layers.Dense(1)(x)\n    \n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    \n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=[rmse])\n    return model","2606d98a":"model = get_model()\nmodel.summary()","83c76486":"keras.utils.plot_model(model, show_shapes=True)","af123b5f":"del model\ngc.collect()","1be37b66":"%%time\n\n\nrandom.seed(10)\nnp.random.seed(20)\ntf.random.set_seed(30)\n\nSAMPLE_FRAC = 0.5\n\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\nmodels = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    X_train = train.iloc[train_indices].sample(frac=SAMPLE_FRAC, random_state=2)\n    investment_id_train = investment_id[train_indices].sample(frac=SAMPLE_FRAC, random_state=2)\n    y_train = y.iloc[train_indices].sample(frac=SAMPLE_FRAC, random_state=2)\n    \n    X_val = train.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    y_val = y.iloc[valid_indices]\n    \n    # PCA\n    components = [f'x_{i}' for i in range(n_components)]\n    pca = PCA(n_components=n_components, random_state=2)\n    X_train = pca.fit_transform(X_train)\n    X_val = pca.transform(X_val)\n    print('Explained variance ratio:', np.cumsum(pca.explained_variance_ratio_))\n    \n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    \n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}.tf\", save_best_only=True, save_weights_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=5)\n    history = model.fit(train_ds, epochs=20, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    models.append(model)\n    \n    # predict\n    score = stats.pearsonr(y_val.values, model.predict(valid_ds).ravel())[0]\n    print('Pearson:', score)\n    print()\n    \n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    \n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    \n    gc.collect()\n    \n    break","121dc21b":"del train\ndel investment_id\ndel y\ngc.collect()","3b43b1b9":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","64754e6e":"import ubiquant\n\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(pca.transform(test_df[features]), test_df['investment_id'])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","7d244cea":"## Make Tensorflow dataset","ae30f9c0":"## Import dataset","8ae204b0":"#  Ubiquant Market Prediction with DNN\n\nPlease upvote the original kernel from where I modified this one:              \nhttps:\/\/www.kaggle.com\/lonnieqin\/ubiquant-market-prediction-with-dnn\n\nI just added some minor changes:\n\n- PCA (80 components)   \n- Lot of Dropout layers with 0.4 dropout ratio   \n\nI think that's it.   ","895ad947":"## Modeling","0720c9e2":"## Adding some interactions","c67728ab":"## Submission","8e376732":"## Create a IntegerLookup layer for investment_id input"}}