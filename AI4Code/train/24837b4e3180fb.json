{"cell_type":{"926b26e3":"code","f5957977":"code","1e622d51":"code","4ab9c5c9":"code","fb8369da":"code","ecd539b4":"code","0eb44e8a":"code","43f82e8a":"code","efea6fcb":"code","ce149abd":"code","ba13c38b":"code","88d31801":"code","8868d2e5":"code","193a5510":"code","e1df85b9":"code","c4a7d9ed":"code","b3191f72":"code","221b8955":"code","8ce2e14b":"code","84938765":"code","5e982b89":"code","4c045bfd":"markdown","a50dfdd2":"markdown","ef0eaef3":"markdown","cace0690":"markdown","84e59f3c":"markdown","b63a8901":"markdown","cdcd9305":"markdown","b8ef17a5":"markdown","83a8803c":"markdown","6d010062":"markdown","7d6b0949":"markdown","ba3e60d0":"markdown","c2147afa":"markdown","226e122f":"markdown","124fe3e9":"markdown","95767807":"markdown","89463cc9":"markdown","a2ddc223":"markdown"},"source":{"926b26e3":"# Python program to create \n# Image Classifier using CNN \n# Importing the required libraries \nimport cv2\nimport os \nimport numpy as np \nfrom random import shuffle \nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image\nimport math \nimport datetime\nimport time\nimport random\nimport gc   #Gabage collector for cleaning deleted data from memory\n","f5957977":"TRAIN_DIR = '\/kaggle\/input\/training-set\/trainData'\nTEST_DIR = '\/kaggle\/input\/testingset\/testData'\nIMG_SIZE = 50\nLR = 1e-3\n\n#Getting Train Data - Cats and Dogs\ntrain_dogs = ['\/kaggle\/input\/training-set\/trainData\/{}'.format(i) \n              for i in os.listdir(TRAIN_DIR) if 'dog' in i]  #get dogs images\ntrain_cats = ['\/kaggle\/input\/training-set\/trainData\/{}'.format(i) \n              for i in os.listdir(TRAIN_DIR) if 'cat' in i]  #get cat images\n\n\n#Getting Test Data -\ntest_imgs = ['\/kaggle\/input\/testingset\/testData\/{}'.format(i) \n             for i in os.listdir(TEST_DIR)] #get test images\n\n\ntrain_imgs = train_dogs[:2000] + train_cats[:2000]  # slice the dataset and use 2000 in each class\nrandom.shuffle(train_imgs)  # shuffle it randomly\n\n\n","1e622d51":"#Lets declare our image dimensions\n#we are using coloured images. \nnrows = 150\nncolumns = 150\nchannels = 3  #change to 1 if you want to use grayscale image\n\n\n#A function to read and process the images to an acceptable format for our model\ndef read_and_process_image(list_of_images):\n    \"\"\"\n    Returns two arrays: \n        X is an array of resized images\n        y is an array of labels\n    \"\"\"\n    \n    X = [] # images\n    y = [] # labels\n    \n    for image in tqdm(list_of_images):\n        X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), \n                            (nrows,ncolumns), interpolation=cv2.INTER_CUBIC))  #Read the image\n        #get the labels\n        if 'dog' in image:\n            y.append(1)\n        elif 'cat' in image:\n            y.append(0)\n    \n    return X, y","4ab9c5c9":"#get the train and label data\n\nX, y = read_and_process_image(train_imgs)\n","fb8369da":"#Lets view some of the pics\nplt.figure(figsize=(20,10))\ncolumns = 5\nfor i in tqdm(range(columns)):\n    plt.subplot(5 \/ columns + 1, columns, i + 1)\n    plt.imshow(X[i])","ecd539b4":"import seaborn as sns\ndel train_imgs\ngc.collect()\n\n#Convert list to numpy array\nX = np.array(X)\ny = np.array(y)\n\n\n#Lets plot the label to be sure we just have two class\nsns.countplot(y)\nplt.title('Labels for Cats and Dogs')","0eb44e8a":"#Lets split the data into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=2)\n\nprint(\"Shape of train images is:\", X_train.shape)\nprint(\"Shape of validation images is:\", X_val.shape)\nprint(\"Shape of labels is:\", y_train.shape)\nprint(\"Shape of labels is:\", y_val.shape)","43f82e8a":"#We can see that our image is a tensor of rank 4, or \n#we could say a 4 dimensional array with dimensions 4000 x 150 x 150 x 3 \n#which correspond to the batch size, height, width and channels respectively.\n\nprint(\"Shape of train images is:\", X.shape)\nprint(\"Shape of labels is:\", y.shape)\n\n","efea6fcb":"#clear memory\ndel X\ndel y\ngc.collect()\n\n#get the length of the train and validation data\nntrain = len(X_train)\nnval = len(X_val)\nprint(ntrain)\nprint(nval)\n\n#We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***\nbatch_size = 32 ","ce149abd":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout , Activation, Flatten,Conv2D,MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import applications \n#from tensorflow.keras.utils import to_categorical \n","ba13c38b":"\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))  #Dropout for regularization\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))  #Sigmoid function at the end because we have just two classes\n\n","88d31801":"\n#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])","8868d2e5":"#Lets create the augmentation configuration\n#This helps prevent overfitting, since we are using a small dataset\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,   #Scale the image between 0 and 1\n                                    rotation_range=40,\n                                    width_shift_range=0.2,\n                                    height_shift_range=0.2,\n                                    shear_range=0.2,\n                                    zoom_range=0.2,\n                                    horizontal_flip=True,)\n\n#added\ntrain_datagen = ImageDataGenerator(rescale=1.\/255   #Scale the image between 0 and 1\n                                  )\n\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)  #We do not augment validation data. we only perform rescale","193a5510":"#Create the image generators\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\nval_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)\n","e1df85b9":"#The training part\n#We train for 64 epochs with about 100 steps per epoch\n\n#A total of 3200 images divided by the batch size of 32 will give us 100 steps. \n#This means we going to make a total of 100 gradient update to our model in one pass through the entire training set.\n\nhistory = model.fit_generator(train_generator,\n                              steps_per_epoch=ntrain \/\/ batch_size,\n                              epochs=80,\n                              validation_data=val_generator,\n                              validation_steps=nval \/\/ batch_size)","c4a7d9ed":"#lets plot the train and val curve\n#get the details form the history object\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n#Train and validation accuracy\nplt.plot(epochs, acc, 'b', label='Training accurarcy')\nplt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\nplt.title('Training and Validation accurarcy')\nplt.legend()\n\n\n\nplt.figure()\n#Train and validation loss\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\n\nplt.show()","b3191f72":"#Now lets predict on the first 10 Images of the test set\nX_test, y_test = read_and_process_image(test_imgs[0:40]) #Y_test in this case will be empty.\nx = np.array(X_test)\ntest_datagen = ImageDataGenerator(rescale=1.\/255) \n","221b8955":"out = model.evaluate_generator(val_generator)\nprint(out)\n","8ce2e14b":"#need to verify \n\nY_pred = model.predict(X_val)\nprint(Y_pred.shape)\n#y_pred = np.argmax(Y_pred, axis=1) - used for multiclass\ny_pred = (Y_pred > 0.5) * 1.0\ny_pred = y_pred.reshape(y_val.shape)\ny_pred.sum()\n\n\n","84938765":"i = 0\ntext_labels = []\nplt.figure(figsize=(30,20))\n\nfor batch in test_datagen.flow(x, batch_size=1):\n    pred = model.predict(batch)\n    if pred > 0.5:\n        text_labels.append('dog')\n    else:\n        text_labels.append('cat')\n    plt.subplot(5 \/ columns + 1, columns, i + 1)\n    plt.title('This is a ' + text_labels[i])\n    #print(batch[0])\n    imgplot = plt.imshow(batch[0])\n    i += 1\n    if i % 10 == 0:\n        break\nplt.show()","5e982b89":"from sklearn.metrics import confusion_matrix,classification_report\n# demonstration of calculating metrics for a neural network model using sklearn\nfrom sklearn.datasets import make_circles\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\n# Predict the values from the validation dataset\n\n\n\nprint('Confusion Matrix')\nprint(confusion_matrix(y_val, y_pred))\n\n\nprint('Classification Report')\ntarget_names = ['Cats', 'Dogs']\nprint(classification_report(y_val, y_pred,target_names=target_names))\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_val, y_pred)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_val, y_pred)\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_val, y_pred)\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_val, y_pred)\nprint('F1 score: %f' % f1)\n \n# kappa\n#kappa = cohen_kappa_score(y_val, y_pred)\n\n#print('Cohens kappa: %f' % kappa)\n# ROC AUC\n#auc = roc_auc_score(y_val, yhat_probs)\n#print('ROC AUC: %f' % auc)\n# confusion matrix\n#matrix = confusion_matrix(y_val, y_pred)\n#print(matrix)\n","4c045bfd":"1. * Create a list to hold the labels we are going to generate.\n1. We set the figure size of the images we\u2019re going to plot.\n1. Here we make a prediction on that particular image provided by the ImageDataGenerator by calling the .predict( ) method on our trained model.\n1. The pred variable is a probability of how sure the model is that the current image is a dog.\n1. Since we gave dogs a label of 1, a high probability \u2014 at least greater than average 0.5 \u2014 means our model is very confident that the image is a Dog, otherwise it is a cat.\n1. So we simply create an if -else statement that appends the string \u2018Dog\u2019 if the probability is greater than 0.5 otherwise it appends \u2018cat\u2019 to the text_label.\n1. We do this so we can add a title to the image when we plot it.\n1. Here we add a subplot so we can plot multiple images.\n1. Here we add the predicted class as a title to the image plot.\n1. We finally plot the image.","a50dfdd2":"150 by 150 for height and width and 3 channels.\n\nA colored Image is made up of 3 channels, i.e 3 arrays of red, green and blue pixel values. We could use 1 channel which would read our images in gray-scale format (black and white).\n\nCreate a new variable X which will hold the new training set and y which will hold our training labels. (1 if the image is a dog and 0 if it is a cat)\nwe read our images one after the other and resize them with the cv2 commands.\nWe append 1 to y if the image is a dog and 0 if it is a cat.","ef0eaef3":"**First let\u2019s import the neccessary keras modules we are going to use**\n\n1. Here we import keras layers module which contains different types of layers used in deep learning such as:\n** Convolutional layer (Mostly used in computer vision)\n** Pooling layer (also used in computer vision)\n** Recurrent layer (Mostly used in sequential and time series modelling)\n** Embedding layers (Mostly used in Natural Language processing)\n** Normalization layers\n** and many more\n\n\n1. Here we import keras models which contains two types:\n**Sequential model which we\u2019ll be using in this tutorial and\n**The model with the Functional API\n\n1. Here we import keras optimizer, a module that contains different types of back propagation algorithm for training our model. Some of these optimizers are:\n**sgd (stochastic gradient descent)\n**rmsprop (root mean square propagation)\n**Adams\n**Adagrad\n**Adadelta\n\n\n\n1. Here we import one of the most important function (ImageDataGenerator) used when working with a small data set\n\n1. Now lets create our Network architecture. We are going to follow a popular, effective and simple architecture called the VGGnet","cace0690":"![VGGNET](http:\/\/\/\/miro.medium.com\/max\/1276\/1*130UnEWfZF3D2aW1yC-OrQ.jpeg)","84e59f3c":"1. We pass three parameters to the model.compile() command\n1. Loss [\u2018binary_crossentropy\u2019]: We specify a loss function that our optimizer will minimize. In this case, since we\u2019re working with a two-class problem, we use binary cross-entropy loss.\n1. Remember the optimizers we defined earlier? we\u2019re going to use one of them called the rmsprop. This is not a fixed choice, it is part of a process called hyperparameter tuning which may be the difference between a world-class model and a naive one.","b63a8901":"1. We call the .flow( ) method on the data generators we created above passing in the data and label set.\n1. X_train and y_train for training then X_val and y_val for validation.\n1. The batch size tells the data generator to only take the specified batch(32 in our case) of Images at a time.","cdcd9305":"#ImageDataGenerator() lets us quickly set-up python generators that automatically turn image files \n#into preprocessed tensors that can be fed directly into models during training. \n#It performs the following functions for us easily:\n#1. Decode the JPEG content to RGB grids of pixels.\n#2. Convert these into floating-point tensors.\n#3. Rescale the pixel values (between 0 and 255) to the [0, 1] interval ( neural networks perform better with normalize data).\n#4. It helps us easily augment images. (An important feature we\u2019ll be using since we\u2019re training on a small data set).","b8ef17a5":"1. We can see that our image is a tensor of rank 4, or we could say a 4 dimensional array with dimensions 4000 x 150 x 150 x 3 which correspond to the batch size, height, width and channels respectively.\n\n1. Now that our data is ready (X,y) we could start training, but first we have to do something that is very important, which is to split our data into train and validation set. This is one of the most important things to do before you start training your model.\n1. For splitting, we\u2019re going to use a handy function from a popular machine learning package in python called sklearn.\n\n1. Imports train_test_split from sklearn\n1. We tell the function we want 20% of the data to be assigned to the validation set and the other 80% to the train set.\n1. Here we print the shape of the new train and validation set","83a8803c":"1. We pass the rescale option to the ImageDataGenerator object. The rescale=1.\/255 option is a very IMPORTANT parameter. It normalizes the image pixel values to have zero mean and standard deviation of 1. It helps your model to generally learn and update its parameters efficiently.\n1. The second set of options is Image augmentation. They tell the ImageDataGenerator to randomly apply some transformation to the Image. This will help to augment our data-set and improve generalization.\n1. Here we also create an ImageDataGenerator object for our validation set. Note: we don\u2019t do data augmentation here. We only perform rescale.","6d010062":"1. X and y are currently of type list (list of python array), we will convert these to numpy array so we can feed it into our model.\n1. Plot a colorful diagram to confirm the number of classes in our y label variable","7d6b0949":"> The train data set contains a total of 8,000 images, but since we are experimenting working with a small data set and we obviously have access to little computational power, we\u2019re going to extract only 2000 images from both classes.\n** 2000 dog images and 2000 cat images, making a training data set of 4000 images.\nSo we grab the first 2000 images from the train_dogs and train_cats, then concatenate them into one train set called train_imgs.","ba3e60d0":"1. We are going to use a small vggnet, but you can see below that our filter size increases as we go down layers.\n\n# 32 \u2192 64 \u2192128 \u2192512 \u2014 and final layer is 1\n\n\n1. Here we create a sequential model. This tells keras to stack all layers sequentially.\n1. Here we create the first layer by calling the .add() function on the model we created and pass the type of layer we want \u2014 a Conv2D layer. This first layer is called the input layer and has some important parameters we need to set.\n* ** filter size [32]: This is the size of the output dimension (i.e. the number of output filters in the convolution)\n* ** kernel_size [3,3]: This specifies the height and width of the 2D convolution window.\n* ** activation [\u2018relu\u2019]: We select an activation function also called non-linearity to be used by our neural network. ReLU (Rectified Linear Unit) is the most common activation function used today, other variations are leaky ReLU and eLU.\n* ** input shape [150,150,3]: Remember the dimensions we resized our images to? 150 by 150 right? we pass that here including the channel of 3.\n\n1. Here we add a MaxPool2D layer. Its function is to reduce the spatial size of the incoming features and therefore helps reduce the number of parameters and computation in the network, thereby helping to reduce overfitting.\n1. Overfitting happens when our model memorizes the training data. The model will perform excellently at training time but fail at test time.\n\n1. Here we add a Flatten layer. A conv2D layers extract and learn spatial features which are then passed to a dense layer after it has been flattened. This is the work of the flatten layer.\n1. Here we add a Dropout layer with value 0.5. Dropout randomly drops some layers in a neural networks and then learns with the reduced network. This way, the network learns to be independent and not reliable on a single layer. Bottom-line is that it helps in overfitting.\n1. 0.5 means to randomly drop half of the layers.\n\n1. The last layer has an output size of 1 and a different activation function called sigmoid. This is because we\u2019re trying to detect if an image is a dog or a cat. i.e we want the model to output a probability of how sure an image is a dog and not a cat, that means we want a probability score where higher values means the classifier believes the image is a dog and lower values means it is a cat.\n1. The sigmoid is perfect for this because it takes in a set of numbers and returns a probability distribution in the range of 0 to 1.","c2147afa":"1. After training a Keras model, it always calculates and saves the metric we specified when we compiled our model in a variable called history. We can extract these values and plot them.\n1. Note: The history object contains all the updates that happened during training.\n1. Here we simply get the size of our epoch from the number of values in the \u2018acc\u2019 list.\n1. Here we plot the accuracy against the epoch size.\n1. Here we plot the loss against the epoch size.\n\n\n**So what can we take away from this plot?**\n* The first thing to note is that we\u2019re not overfitting as the train and validation accuracy are pretty close and following each other.\n* We can also notice that the accuracy keeps increasing as the epoch increases, giving us the intuition that increasing the epoch size will likely give us a higher accuracy.","226e122f":"Remember we have a total of 4000 images (2000 dogs and 2000 cats), therefore our label list y should contain 2000 of 1s and 2000 of 0s. Let\u2019s plot this and confirm.\n","124fe3e9":"# Image Classification using Keras - CNN \n\n**About Libraries**\n\n1. cv2 also called OpenCV, is an image and video processing library available in Python and many other high level programming languages. It is used for all sorts of image and video analysis, like facial recognition and detection, license plate reading, photo editing, advanced robotic vision, optical character recognition,\n\n2. NumPy is the most popular mathematical library in Python. It makes working and computing large, multi-dimensional arrays and matrices super easy and fast. It has a large collection of high-level mathematical functions to operate on these arrays.\n\n3. Pandas pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\n\n4. Matplotlib is a plotting library for the Python. It can be used for plotting lines, bar-chart, graphs, histograms and even displaying Images.\n\n5. %matplotlib inline is a command that makes our plots appear in the notebook.\n6. os is an inbuilt python package for accessing your computer and file system. It can be used to display content in directories, create new folders and even delete folders.\n\n7. random will help us create random numbers which will be used when we split or shuffle our data set.\n8. gc short for garbage collector is an important tool for manually cleaning and deleting unnecessary variables. We\u2019ll actively use this on Kaggle kernels because the free memory allocated to us may get full since we are working on Image datasets.","95767807":"1. Now we train our network by calling .fit( ) method on the model and passing some parameters. The first parameter is the training set ImageDataGenerator object [train_generator].\n1. Here we specify the number of steps per epoch. This tells our model how many images we want to process before making a gradient update to our loss function.\n1. A total of 3200 images divided by the batch size of 32 will give us 100 steps. This means we going to make a total of 100 gradient update to our model in one pass through the entire training set.\n1. An epoch is a full-cycle or pass through the entire training set. In our case, an epoch is reached when we make 100 gradient updates as specified by our steps_per_epoch parameter.\n1. Epochs = 64, means we want to go over our training data 64 times and each time we will make gradient updates 100 times.\n1. We pass in our validation data generator.","89463cc9":"1. We perform the same pre-processing we did on the train and validation set.\n1. We read and convert the first 10 images in our test set to a list of array.\n1. Note: y_test will be empty because the test set has no label.\n1. We convert the list of array to one big numpy array.\n1. We create a test ImageDataGenerator and perform normalization only.\n1. Note: We do not augment the test set .","a2ddc223":"X is now an array of image pixel values and y is a list of labels."}}