{"cell_type":{"b9577ea1":"code","cfecd9f9":"code","b0ea635f":"code","b64e6940":"code","b3eaf046":"code","253e604a":"code","9c9f46bd":"code","b57caebf":"code","4c7a8fe7":"code","caed9928":"code","a2115b2e":"code","30166dfe":"code","2e49970f":"code","6526f616":"code","a21fdb52":"code","c0e25238":"code","1b0cab86":"code","5442a996":"code","54e91d40":"code","43e86333":"code","99172992":"code","202edae0":"code","f4dbb8ca":"code","7ccfdcb0":"code","8e1282fb":"code","bc92a82a":"code","daf61f98":"code","fab6b0f1":"code","e395df70":"code","19f114c4":"code","0314a7ab":"code","2800c646":"code","0c576263":"code","9a22873c":"code","a8e035c6":"code","ef2af8da":"code","182d98e0":"code","fc6329b8":"code","86ff5f27":"code","b788820e":"code","5cc8f62e":"code","0abafc00":"code","f2087aae":"code","1c7a55ce":"code","c934ea7b":"code","4310e518":"code","745ea666":"code","38c8e343":"code","ba609cb0":"code","7fef4e5a":"code","bf2da486":"code","583a9986":"code","f9a44625":"code","c8a966a8":"code","880a6292":"code","62985ec7":"code","bbcefe7c":"code","225f3f05":"code","63d7cce2":"code","8e70aa23":"code","39f97d75":"markdown","f08f01e6":"markdown","f6bf9e88":"markdown","fd0dc383":"markdown","bf859040":"markdown","c2969082":"markdown","08b8ca3f":"markdown","fd3f7c2c":"markdown","ce840e47":"markdown","a822f519":"markdown","34f2b3bf":"markdown","3ba30629":"markdown","ea229e15":"markdown","a7eea8b4":"markdown","bb03ccd3":"markdown","3afe1ae7":"markdown","0fd304aa":"markdown","f8b67033":"markdown","fc9e3b31":"markdown","f52478a9":"markdown","b78aa598":"markdown","aa2d7b98":"markdown","b673370d":"markdown","9b7bb92b":"markdown","e8264b35":"markdown","09e75227":"markdown","0b0f1adb":"markdown","9c610447":"markdown","339d6bc3":"markdown","d9f3e0e3":"markdown","333d2ac5":"markdown","3a46d84d":"markdown","f7449762":"markdown","54af21e1":"markdown","9d493622":"markdown","d3e47ea4":"markdown","262547d7":"markdown","b097fd8d":"markdown","49ee0a90":"markdown","9ad2f347":"markdown","19fd9a38":"markdown","5763615d":"markdown","76470cd0":"markdown","6a5f7bcd":"markdown","51c002df":"markdown","9001daab":"markdown","361da861":"markdown","1a87b0f1":"markdown","dbf02426":"markdown","fc8d8a6c":"markdown","83c390b2":"markdown","1c34d6cb":"markdown","992c737a":"markdown","6e9b4a88":"markdown","79fef7ba":"markdown","1cf58c72":"markdown","d25f57fe":"markdown","e711c68f":"markdown","afc9142e":"markdown","a44b6151":"markdown","b018190a":"markdown","465e1e18":"markdown","526104ab":"markdown","5c7a307e":"markdown","05dfd91b":"markdown"},"source":{"b9577ea1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='whitegrid', font='Arial')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n!pip install jcopml\nfrom jcopml.pipeline import num_pipe, cat_pipe\nfrom jcopml.utils import save_model, load_model\nfrom jcopml.feature_importance import mean_score_decrease\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","cfecd9f9":"df_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_train.head()","b0ea635f":"df_train.shape","b64e6940":"df_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf_test.head()","b3eaf046":"df_test.shape","253e604a":"df = pd.concat((df_train, df_test)).reset_index(drop=True)","9c9f46bd":"df.head()","b57caebf":"#save ID\ntrain_id = df_train['Id']\ntest_id = df_test['Id']\n\n#drop ID\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\ndf.drop(\"Id\", axis = 1, inplace = True)","4c7a8fe7":"df.shape","caed9928":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data = missing_data[missing_data['Percent'] > 0]\nmissing_data = missing_data.drop(['SalePrice'], axis=0)\nmissing_data","a2115b2e":"f, ax = plt.subplots(figsize=(10, 7))\nplt.xticks(rotation='90')\nsns.barplot(y=missing_data.loc['PoolQC':'BsmtHalfBath'].index, \n            x=missing_data.loc['PoolQC':'BsmtHalfBath']['Percent'], color='b');\nplt.ylabel('Fitur', fontsize=15,);\nplt.xlabel('Persen', fontsize=15);\nplt.title('20 Fitur dengan missing value terbanyak', fontsize=15);","30166dfe":"df_train['SalePrice'].describe()","2e49970f":"sns.distplot(df_train['SalePrice'], fit=norm);","6526f616":"corrmat = df_train.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(corrmat , square=True, cmap='RdYlBu');","a21fdb52":"f, ax = plt.subplots(figsize=(10, 10))\nk = 11 #Top k variabel yang berkorelasi dengan SalePrice\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, fmt='.2f', square=True, annot_kws={'size': 12}, \n                 yticklabels=cols.values, xticklabels=cols.values, cmap='RdYlBu')\nplt.show()","c0e25238":"sns.set(style='whitegrid')\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","1b0cab86":"sns.set(style='whitegrid')\nfig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\nsns.scatterplot(x=df_train['GrLivArea'], y=df_train['SalePrice']);\n\naxes2 = fig.add_axes([1.1, 0.1, 0.8, 0.8])\nsns.scatterplot(x=df_train['TotalBsmtSF'], y=df_train['SalePrice']);","5442a996":"sns.set(style='whitegrid')\nfig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\nsns.scatterplot(x=df_train['GrLivArea'], y=df_train['SalePrice']);\n\naxes2 = fig.add_axes([1.1, 0.1, 0.8, 0.8])\nsns.scatterplot(x=df_train['TotalBsmtSF'], y=df_train['SalePrice']);","54e91d40":"df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\ndf_train.drop(df_train.index[1298], inplace=True)\ndf_train.drop(df_train.index[523], inplace=True)\n\ndf_train.sort_values(by = 'TotalBsmtSF', ascending = False)[:1]\ndf_train.drop(df_train.index[1298], inplace=True)","43e86333":"sns.set(style='whitegrid')\nfig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\nsns.scatterplot(x=df_train['GrLivArea'], y=df_train['SalePrice'], color='g');\n\naxes2 = fig.add_axes([1.1, 0.1, 0.8, 0.8])\nsns.scatterplot(x=df_train['TotalBsmtSF'], y=df_train['SalePrice'], color='g');","99172992":"def imputer(df):\n    col_name = df.columns\n    for col_name in df:\n        df[\"PoolQC\"] = df[\"PoolQC\"].fillna(\"None\")\n        df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(\"None\")\n        df[\"Alley\"] = df[\"Alley\"].fillna(\"None\")\n        df[\"Fence\"] = df[\"Fence\"].fillna(\"None\")\n        df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(\"None\")\n\n        df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].median())\n\n        for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n                        df[col] = df[col].fillna('None')\n\n        for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n                        df[col] = df[col].fillna(0)\n\n        for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n            df[col] = df[col].fillna(0)\n\n        for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n            df[col] = df[col].fillna('None')\n\n        df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(\"None\")\n        df[\"MasVnrArea\"] = df[\"MasVnrArea\"].fillna(0)\n\n        df['MSZoning'] = df['MSZoning'].fillna('RL')\n\n        df[\"Functional\"] = df[\"Functional\"].fillna('Typ')\n\n        df['Electrical'] = df['Electrical'].fillna('SBrkr')\n\n        df['KitchenQual'] = df['KitchenQual'].fillna('TA')\n\n        df['Exterior1st'] = df['Exterior1st'].fillna('VinylSd')\n        df['Exterior2nd'] = df['Exterior2nd'].fillna('VinylSd')\n\n        df['SaleType'] = df['SaleType'].fillna('WD')\n\n        df['MSSubClass'] = df['MSSubClass'].fillna(\"None\")\n\n        df.drop(columns=['Utilities','1stFlrSF','GarageYrBlt','TotRmsAbvGrd','GarageArea'],inplace=True)\n        return df","202edae0":"df_train = imputer(df_train)","f4dbb8ca":"print('df_train shape = {}'.format(df_train.shape))\ndf_train.head()","7ccfdcb0":"df_test = imputer(df_test)","8e1282fb":"print('df_test shape = {}'.format(df_test.shape))\ndf_test.head()","bc92a82a":"df_train.isnull().any().value_counts()","daf61f98":"df_test.isnull().any().value_counts()","fab6b0f1":"sns.distplot(df_train['SalePrice'], fit=norm);","e395df70":"target_transformer = ColumnTransformer([('target', num_pipe(scaling='standard',transform='box-cox'),['SalePrice'])])\ny_target = target_transformer.fit_transform(df_train)\ny_target = pd.Series(y_target.flatten())","19f114c4":"sns.distplot(y_target, fit=norm);","0314a7ab":"num = df_train.select_dtypes(exclude=['object'])\ncat = df_train.select_dtypes(include=['object'])","2800c646":"num.columns","0c576263":"cat.columns","9a22873c":"cat.head()","a8e035c6":"# Ordinal\ncat_or = cat[['Street', 'Alley', 'LandContour', 'LandSlope', 'ExterQual', 'ExterCond',\n            'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n            'BsmtFinType2', 'HeatingQC','CentralAir', 'KitchenQual', 'Functional',\n            'FireplaceQu','GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive',\n            'PoolQC', 'Fence', 'SaleCondition' ]]\n\n# Nominal\ncat_nom = cat[['MSZoning', 'LotShape', 'LotConfig', 'Neighborhood','Condition1', 'Condition2',\n             'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n             'MasVnrType', 'Heating', 'Electrical', 'GarageType', 'MiscFeature', 'SaleType']]","ef2af8da":"X = df_train.drop(columns='SalePrice')\ny = y_target #target yang sudah di scaling dan transform\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","182d98e0":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, OrdinalEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer","fc6329b8":"num_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n    ('transformer', PowerTransformer(method='yeo-johnson'))   \n])\n\ncat_ord_pipe = Pipeline([\n    ('encoder', OrdinalEncoder())\n])\n\ncat_nom_pipe = Pipeline([\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])","86ff5f27":"preprocessor = ColumnTransformer([\n    ('numeric1', num_pipe,  [\n       'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n       'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n       'KitchenAbvGr', 'Fireplaces', 'GarageCars', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold']),\n    \n    ('categoric1', cat_ord_pipe , [\n       'Street', 'Alley', 'LandContour', 'LandSlope', 'ExterQual', 'BsmtQual', \n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','CentralAir', 'KitchenQual',\n       'FireplaceQu','GarageFinish', 'GarageQual', 'PavedDrive', 'Fence', 'SaleCondition' ]),\n    \n    ('categoric2', cat_nom_pipe, [\n       'MSZoning', 'LotShape', 'LotConfig', 'Neighborhood','Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n       'MasVnrType', 'Heating', 'Electrical', 'GarageType', 'MiscFeature', 'SaleType',\n       'Foundation', 'HeatingQC','Functional','ExterCond','BsmtCond','GarageCond', 'PoolQC'])    \n])","b788820e":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV","5cc8f62e":"pipeline = Pipeline([\n    ('prep', preprocessor),\n    ('algo', RandomForestRegressor(n_jobs=-1, random_state=42))\n])\n\nparameter = {'algo__n_estimators': np.arange(100,200),\n 'algo__max_depth': np.arange(20,80),\n 'algo__max_features': np.arange(0.1,1),\n 'algo__min_samples_leaf': np.arange(1,20)}\n\nmodel = RandomizedSearchCV(pipeline, parameter, cv=3, n_iter=50, n_jobs=-1, verbose=1, random_state=42)\nmodel.fit(X_train, y_train)\n\nprint(model.best_params_)\nprint(model.score(X_train, y_train), model.best_score_, model.score(X_test, y_test))","0abafc00":"X = df_train.drop(columns='SalePrice')\ny = df_train['SalePrice'] #target original\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","f2087aae":"model.fit(X_train, y_train)\n\nprint(model.best_params_)\nprint(model.score(X_train, y_train), model.best_score_, model.score(X_test, y_test))","1c7a55ce":"from jcopml.automl import AutoRegressor","c934ea7b":"X = df_train.drop(columns=[\"SalePrice\"])\ny = df_train['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","4310e518":"model = AutoRegressor([\n    'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n    'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n    'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n    'KitchenAbvGr', 'Fireplaces', 'GarageCars', 'WoodDeckSF', 'OpenPorchSF',\n    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n    'MoSold', 'YrSold'], \n                      \n    ['Street', 'Alley', 'LandContour', 'LandSlope', 'ExterQual', 'BsmtQual', \n     'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','CentralAir', 'KitchenQual',\n     'FireplaceQu','GarageFinish', 'GarageQual', 'PavedDrive', 'Fence', \n     'SaleCondition', 'MSZoning', 'LotShape', 'LotConfig', 'Neighborhood',\n     'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', \n     'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Heating', 'Electrical', \n     'GarageType', 'MiscFeature', 'SaleType', 'Foundation', 'HeatingQC','Functional',\n     'ExterCond','BsmtCond','GarageCond', 'PoolQC'] )","745ea666":"model.fit(X, y)","38c8e343":"model.plot_results()","ba609cb0":"model.fit(X,y, algo='rf')","7fef4e5a":"X = df_train.drop(columns=[\"SalePrice\"])\ny = df_train['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","bf2da486":"from sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import RobustScaler","583a9986":"num_pipe = Pipeline([\n    ('imputer', KNNImputer(n_neighbors=5,add_indicator=True)),\n    ('scaler', RobustScaler()),\n    ('transformer', PowerTransformer(method='yeo-johnson'))   \n])\n\ncat_ord_pipe = Pipeline([\n    ('encoder', OrdinalEncoder())\n])\n\ncat_nom_pipe = Pipeline([\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])","f9a44625":"preprocessor = ColumnTransformer([\n    ('numeric1', num_pipe,  [\n       'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n       'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n       'KitchenAbvGr', 'Fireplaces', 'GarageCars', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold']),\n    \n    ('categoric1', cat_ord_pipe , [\n       'Street', 'Alley', 'LandContour', 'LandSlope', 'ExterQual', 'BsmtQual', \n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','CentralAir', 'KitchenQual',\n       'FireplaceQu','GarageFinish', 'GarageQual', 'PavedDrive', 'Fence', 'SaleCondition' ]),\n    \n    ('categoric2', cat_nom_pipe, [\n       'MSZoning', 'LotShape', 'LotConfig', 'Neighborhood','Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n       'MasVnrType', 'Heating', 'Electrical', 'GarageType', 'MiscFeature', 'SaleType',\n       'Foundation', 'HeatingQC','Functional','ExterCond','BsmtCond','GarageCond', 'PoolQC'])    \n])","c8a966a8":"pipeline = Pipeline([\n    ('prep', preprocessor),\n    ('algo', RandomForestRegressor(n_jobs=-1, random_state=42))\n])\n\nparameter = {'algo__n_estimators': [123],\n 'algo__max_depth': [43],\n 'algo__max_features': [0.4640573144099711],\n 'algo__min_samples_leaf': [1]}\n\nmodel = RandomizedSearchCV(pipeline, parameter, cv=3, n_iter=50, n_jobs=-1, verbose=1, random_state=42)\nmodel.fit(X_train, y_train)\n\nprint(model.best_params_)\nprint(model.score(X_train, y_train), model.best_score_, model.score(X_test, y_test))","880a6292":"from jcopml.utils import save_model","62985ec7":"save_model(model, \"House Price - Advanced Regression Techniques.pkl\")","bbcefe7c":"df_test.head()","225f3f05":"predicted_prices = model.predict(df_test)","63d7cce2":"my_submission = pd.DataFrame({'Id': test_id, 'SalePrice': predicted_prices})\n\nmy_submission.to_csv('submission.csv', index=False)","8e70aa23":"my_submission.head()","39f97d75":"Seperti yang sudah saya jelaskan di bagian intro, saya akan menggunakan algoritma Random Forest Regressor, lalu pakai Randomized Search sebagai Hyper Parameter.","f08f01e6":"Ini adalah data submission kita:","f6bf9e88":"Target kita memiliki skew positif, dan kita ingin mengubah target menjadi berdistribusi normal.\n\nCaranya, kita akan pakai transformer 'box-cox', lalu kita scaling menggunakan standard scaler.","fd0dc383":"### 3. Correlation Matrix\n\nCorrelation Matrix digunakan untuk menunjukkan korelasi koefisien antar variabel. \n\nMendekati 1, artinya korelasi positif. Mendekati -1, artinya korelasi Negatif. mendekati 0, artinya tidak memiliki korelasi apapun.\n\nMetode yang digunakan dalam menghitung korelasinya adalah Pearson Coefficient Correlation.\n\nUntuk menghindari Multicollinearity, kita akan drop variabel yang memiliki tingkat korelasi yang tinggi.","bf859040":"## AutoML","c2969082":"Terlihat di fitur GrLivArea kedua titik dikanan bawah adalah Outliers, kita akan membuangnya.\n\nBegitu juga dengan 'TotalBsmtSF', dikanan bawah terlihat satu titik yang merupakan outlier, dan kita juga akan membuang point tersebut.","08b8ca3f":"### 1. Missing Value\n\nPlot Missing Value beserta Persentasenya. \n\nData yang akan kita gunakan adalah data gabungan, kita ingin memastikan bahwa data testnya juga bebas dari missing value.","fd3f7c2c":"Ternyata target menunjukkan sedikit Skew Positive. Nantinya kita akan transform target menjadi distribusi mormal menggunakan Box-Cox. \n\nKita transform karena distribusi normal memudahkan model untuk menemukan global minimum (dasar dari loss plane).","ce840e47":"# Modeling","a822f519":"Ternyata ada banyak sekali fitur yang memiliki missing value, haruskah kita drop fitur-fitur tersebut? atau impute? Untuk menjawabnya, kita harus lakukan analisa terlebih dahulu.\n\nPertama, kita analisa fitur yang memiliki missing value diatas 15%, fitur-fitur tersebut adalah:\n\n1. PoolQC: Pool quality. NA = No Pool.\n2. MiscFeature: Miscellaneous feature not covered in other categories\n3. Alley: Type of alley access. NA = No alley access\n4. Fence: Fence quality. NA = None. (tidak memiliki fitur seperti elevator, lapangan tennis, dll)\n5. FireplaceQu: Fireplace quality. NA = No Fireplace\n\n*note: Untuk keterangan yang lebih jelas kalian bisa membacanya di data deskripsi.* \n\nPenyebab absennya data terbilang cukup masuk akal, karena pada umumnya rumah memang tidak disediakan atribut ekslusif seperti itu. \n\nJadi Untuk kelima fitur diatas (dan fitur lain yang memiliki sifat yang sama), kita akan impute dengan 'NONE', atau 0 untuk numeric. **'NONE' dan 0 menandakan tidak adanya atribut eksklusif yang ada dirumah itu.**\n\nDan sisanya kita akan aplikasikan seperti ini:\n\n1. Fitur **LotFrontage**, karena bersifat numeric: impute dengan **median**.\n\n2. Fitur **GarageType**, GarageFinish, GarageQual dan GarageCond: impute dengan **'NONE'**.\n\n3. Fitur **GarageYrBlt**, GarageArea dan GarageCars: impute dengan **0**.\n\n4. Fitur **BsmtFinSF1**, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath dan BsmtHalfBath: impute dengan **0**.\n\n5. Fitur **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 dan BsmtFinType2**: impute dengan **'NONE'**\n\n6. Fitur **MasVnrArea dan MasVnrType**: impute **0** untuk Area, dan **'NONE'** untuk yang Type.\n\n7. Fitur **MSZoning**: impute dengan **'RL'** (most frequent)\n\n8. Fitur **Utilities** bisa kita **Drop** karena kurangmya variansi. (hanya satu yang memiliki 'NoSeWa')\n\n9. Fitur **Funtional**: impute dengan **most frequent**.\n\n10. Fitur **Electrical**: impute dengan **most frequent**.\n\n11. Fitur **KitchenQual**: impute dengan **most frequent**.\n\n12. Fitur **Exterior1st dan Exterior2nd**: impute dengan **'NONE'**\n\n13. Fitur **GarageCars**: impute dengan **most frequent**.\n\n14. Fitur **GarageArea**: impute dengan **mean**.\n\n15. Fitur **TotalBsmtSF**: impute dengan **most Frquent**.","34f2b3bf":"Bisa dilihat, algoritma Random Forest kita tidak kalah jauh kok, mesin merekomendasikan XGBRegressor karena tidak terlalu overfit jika dibandingkan dua algoritma setelahnya. Namun perlu diingat, Automl ini tidak bisa dijadikan acuan pasti ya, karena ini hanya base model saja, tidak menutup kemungkinan ElasticNet dan RF bisa jadi lebih bagus jika dituning.\n\nSekarang kita coba automl khusus algoritma Random Forest, kira-kira parameter seperti apa yang akan direkomendasikan? seberapa jauh perbedaannya dengan XGBRegressor? kita coba ya.","3ba30629":"Done, outliers sudah hilang.","ea229e15":"### 1. Hapus Outliers","a7eea8b4":"### Gabungan Data Train dan Test","bb03ccd3":"Hasilnya lebih bagus ternyata, jadi kita pakai target yang original saja ya.\n\nKira-kira apalagi yang mau dituning? fiturnya? modelnya? sebelum tuning lebih jauh, kita bisa memanfaatkan automl untuk mencari base model, dari situ kita akan tahu algoritma apa yang kira-kira cocok dengan data ini, yuk kita coba.","3afe1ae7":"## Submit","0fd304aa":"# Import Packages","f8b67033":"Dan hasilnya mesin merekomendasikan XGBRegressor. Pertanyaannya, jika dibandingkan dengan algoritma lain, seberapa jauh perbedaannya? yuk kita coba plot perbandingannya.","fc9e3b31":"- Outliers 'GrLivArea': dua point kanan bawah.\n- Outliers 'TotalBsmtSF': satu pon kanan bawah.","f52478a9":"Cek apakah masih ada missing value.","b78aa598":"### Data Train","aa2d7b98":"### by: [Al Fath Terry](https:\/\/www.instagram.com\/al.fath.terry\/)\n\nHalo teman-teman, bagaimana kabar kalian semua? saya harap kalian semua baik-baik saja ya. \n \nDikesempatan kali ini saya ingin mencoba untuk melakukan modeling kasus regresi, dengan goal utamanya adalah memprediksikan harga rumah.\n\nAlgoritma yang akan saya gunakan adalah Random Forest Regression (RF).\n\nSedikit penjelasan mengenai RF, kalau kalian familiar dengan algoritma Decision Tree (DT), pada dasarnya RF hanyalah algorima DT yang di Bootstrap Aggregating (Bagging), kita melakukan Bagging untuk melemahkan algoritma RF, karena RF memiliki sifat cenderung overfit.\n\n- RF memiliki 4 parameter yaitu:\n    1. N Estimators\/Trees : Jumlah DT yang ingin digunakan.\n    1. Max Depth : Kedalaman percabangan DT. \n    1. Minimal Samples Leaf: standar minimal yang digunakan untuk mencegah DT melakukan percabangan.\n    1. Max Features : Mau berapa persen Fitur yang dipakai ditiap percabangan.\n    \nSaya tidak akan menjelaskan secara detil mengenai algoritmanya karena bukan itu fokus utama saya, yang ingin saya tekankan adalah gambaran workflow cara kerjanya mulai dari import data sampai submit.\n\n- Workflow yang akan kita lakukan:\n    1. Import packages dan data\n    1. EDA\n    1. Features Engineering\n    1. Modeling\n    1. Evaluasi\n    1. Save model\n    1. Submit\n\nYuk mari langsung ke pembahasannya.","b673370d":"Hasil: ","9b7bb92b":"### 4. Pisahkan data Numeric dan Categoric","e8264b35":"Hasilnya masih belum memuaskan ya, bagaimana kalau kita pakai target yang original, yuk kita coba.","09e75227":"Kita buat function untuk melakukan impute sekaligus membuang fitur penyebab Multicollinearity.","0b0f1adb":"Gunakan target yang sudah di Scaling dan Transform.","9c610447":"### Simpan kolom 'Id'\n\nSimpan kolom 'Id', lalu drop kolom tersebut. \n\nKolom 'Id' tidak akan kita pakai saat modeling, maka dari itu kita drop. Namun kolom 'Id' akan dipakai saat submit nanti, jadi kolom 'Id' harus disimpan kedalam variabel terlebih dahulu.","339d6bc3":"## **Always start with simple model**\n\nDikutip dari pernyataan Emmanuel Ameisen, Head of AI at Insight Data Science\n\n>    **\"The exact same approach of starting with a very simple model can be applied to machine learning engineering, and it usually proves very valuable. In fact, after seeing hundreds of projects go from ideation to finished products at Insight, we found that starting with a simple model as a baseline consistently led to a better end product.\"**\n   \nTapi kenapa harus always start with simple model? alasannya:\n\n1. simple model dibuat 10 kali lebih cepat, dan sudah menghasilkan 90% hasil yang kita ingikan.\n1. Kedua, model yang kompleks tidak menjamin kenaikan tingkat score, padahal effort yang dikeluarkan bisa 10 kali lebih lama dari simple model.\n1. Ketiga, bisa dijadikan sebagai baseline\/benchmark.\n1. keempat, mini EDA, simple model bisa menjadi sarana proses pengenalan data, sebelum masuk menyelami lebih jauh kondisi data.","d9f3e0e3":"### Dataset Splitting","333d2ac5":"### 3. Transform SalePrice","3a46d84d":"Kita bisa melakukan sorting berdasarkan data typenya, namun kita harus berhati-hati, tidak semua yang terlihat numeric adalah data numeric, bisa saja itu adalah data numeric yang berbentuk categoric, kita harus melakukan sorting manual lagi nantinya.","f7449762":"Data Numeric.","54af21e1":"Oh ya, seharusnya algoritma tree base seperti RF ini tidak perlu discaling ya, karena scaling tidak membantu RF, scaling akan berguna untuk algoritma non tree base seperti SVM, Linear Regression, dll.\n\nkita coba bandingkan base model dengan hasil automl.\n\n- base model:\n    - train: 0.9825567617534368\n    - val:   0.8691723628403284\n    - test:  0.858116250180361\n    \n    \n\n- Automl RF:\n    - train: 0.9845873568062422\n    - val:    0.8819071038468227\n    - test:   0.9102346150696684\n    \n    \n- Automl XGBRegressor:\n    - Train: 0.9817541030994398\n    - Valid: 0.9030264447822179\n    - Test : 0.9191114621057876\n\nParameter yang diberikan automl terbukti berhasil meningkatkan akurasi data val dan test ya, good job mesin!.\n\nNext kita bisa lakukan tuning algoritma RF nya lagi, feature engineering (binning, tambah fitur, dll), atau tuning hyper parameter memakai grid search atau bayesian search. Ada banyak hal yang bisa kita coba, dan itulah tugas Data SCientist, eksperimen!\n\nUntuk sementara, saya akan biarkan seperti ini ya.\n\nOhya, jika kalian mau tuning algoritma RF, berikut saya berikan sense-nya:\n\n- increase n_estimators => more averaging => var turun => mengurangi overfit\n- increase max_depth => more decision => var meningkat => menambah overfit\n- increase min_samples_split => prevent splitting => less depth => mengurangi overfit\n- increase max_features => get better decision => less bias => menambah overfit\n- increase min_impurity_decrease => prevent splitting => less depth => mengurangi overfit\n- increase min_samples_leaf => prevent splitting => less depth => mengurangi overfit","9d493622":"###  4. Correlation Matrix 'SalePrice'","d3e47ea4":"# EDA (Exploratory Data Analysis)","262547d7":"### 2. Impute Missing Values","b097fd8d":"Impute data train dan test.","49ee0a90":"Kita memiliki 79 fitur, dan 1 target(SalePrice).","9ad2f347":"# House Price Prediction\n\n![house](https:\/\/www.listenmoneymatters.com\/wp-content\/uploads\/2018\/04\/LMM-Cover-Images-2.jpg)","19fd9a38":"Lalu fit, auto ml akan mengeluarkan rekomendasi parameter yang kira-kira bisa menghasilkan model yang bagus.","5763615d":"Mungkin itu saja dari saya, jika ada pertanyaan atau masukan, bisa kalian tulis di kolom komentar atau boleh japri di ig saya:\n\n## [Al Fath Terry](https:\/\/www.instagram.com\/al.fath.terry\/)\n\nDan mohon di upvote jika dirasa bermanfaat.\n\nTerimakasih :)","76470cd0":"Disini kita mendapati beberapa fitur yang berkorelasi tinggi. \n\n1. Yang pertama adalah korelasi antara 'TotalBsmtSF' dengan '1stFlrSF'.\n    - TotalBsmtSF: Total square feet of basement area.\n    - 1stFlrSF: First Floor square feet.\n\nKemungkinan besar, korelasi ini terjadi karena total luas ruangan basement dan first floor itu relatif sama dikebanyakan rumah, jadi mereka berbagi sifat yang sama, yaitu luas ruangan. Untuk kasus seperti ini, kita akan drop salah satu fitur yang memiliki korelasi lebih rendah dengan SalePrice untuk menghindari Multicollinearity.\n\nBeberapa kasus yang serupa:\n\n2. 'GarageYrBlt' dengan 'YearBuilt'\n    - GarageYrBlt: Year garage was built.\n    - YearBuilt: Original construction date.\n    \n\n3. 'TotRmsAbvGrd' dengan 'GrLivArea'\n    - TotRmsAbvGrd: Total rooms above grade (does not include bathrooms).\n    - GrLivArea: Above grade (ground) living area square feet.\n    \n\n4. 'GaraceCars' dengan 'GarageArea'\n    - GarageCars: Size of garage in car capacity\n    - GarageArea: Size of garage in square feet","6a5f7bcd":"Masukkan ke pipa Preprocessor.\n\n- Untuk data Numeric, kita akan melakukan scaling dengan 'StandardScaler', transform 'yeo-johnson, dan impute pakai strategi mean.\n- Untuk data Categoric Nominal, encode pakai 'one-hot'. \n- Untuk data categoric ordinal, encode pakai 'ordinal encoder'.\n\nKenapa pakai tuning seperti itu? alasannya karena think simple aja dulu, lakukan apa yang kira-kira bisa membuat model kita bagus, setelah itu baru kita lakukan evaluasi.","51c002df":"### 2. Analisa SalePrice (Target Variabel)","9001daab":"Hasilnya memang overfit, tapi ingat, ini hanya base model saja, lagipula perbedaannya hanya 1-2% saja di tingkat akurasi data validation dan test. \n \nIdealnya untuk mencari algoritma terbaik kita harus bekerja sama, bentuk tim, lalu tiap individu diberi tugas untuk mengoptimalkan masing-masing algoritma, namun karena saya solo player (lol), jadi saya akan teruskan memakai Random Forest.\n\nSekarang kita coba untuk aplikasikan rekomendasi parameter dari automl\n\n======================================================================================================================\n\nBest Model Info\n\n- algo                      | RandomForestRegressor\n- algo__max_depth           | 43\n- algo__max_features        | 0.4640573144099711\n- algo__min_samples_leaf    | 1\n\n- algo__n_estimators        | 123\n\n\nBest Preprocessor Info \n\n- categorical_imputer       | SimpleImputer(add_indicator=False, strategy='most_frequent')\n- categorical_encoder       | OneHotEncoder\n- numerical_imputer         | KNNImputer(add_indicator=True, n_neighbors=5)\n\n- numerical_scaler          | RobustScaler","361da861":"10 Fitur diatas adalah fitur yang paling berkorelasi dengan SalePrice.\n\nSekarang saatnya kita seleksi fitur.\n\n1. 'TotalBsmtSF' vs '1stFlrSF': keduanya memiliki korelasi yang sama dengan SalePrice, jadi kita akan pilih salah satunya dan yang akan kita drop adalah: '1stFlrSF'\n\n2. 'GarageYrBlt' vs 'YearBuilt': Kita akan buang 'GarageYrBlt', karena fitur itu memiliki korelasi yang lebih rendah dengan SalePrice. (bahkan tidak masuk top 10).\n\n3. 'TotRmsAbvGrd' vs 'GrLivArea': Kita akan buang 'TotRmsAbvGrd'\n\n4. 'GaraceCars' vs 'GarageArea': Kita akan buang 'GarageArea'","1a87b0f1":"Hapus outliers.","dbf02426":"### Data Test","fc8d8a6c":"Kita gunakan automl yang ada di jcopml, gunakan AutoRegressor untuk kasus regresi.","83c390b2":"Automl yang ada di Jcopml hanya perlu memisahkan data numeric dan categoric.","1c34d6cb":"### 5. Pisahkan data Categoric Ordinal dengan Nominal","992c737a":"Kita awali dengan melihat gambaran deskriptif target.","6e9b4a88":"Kita menemukan Outliers di:\n\n- Saleprice\/GrLivArea\n- SalePrice\/TotalBsmtSF","79fef7ba":"Sofar.. semuanya terlihat normal. Coba kita buat Histogram.","1cf58c72":"Kita manfaatkan paiplot untuk memberikan gambaran visual mengenai kemungkinan adanya outliers.\n\n***Note: cara 'koboi' seperti ini tidak dianjurkan, sebaiknya gunakan IQR atau metode lainnya untuk mendeteksi outliers***","d25f57fe":"# Import Data","e711c68f":"# Features Engineering","afc9142e":"## Save Model","a44b6151":"Done. Data sudah bersih ya, tidak ada lagi missing value.","b018190a":"Untuk seleksi fitur seperti ini, kita tidak bisa melakukannya secara otomatis dengan mesin, karena fitur-fitur seperti ini butuh sense dari manusia untuk memisahkannya.","465e1e18":"### 5. Pair Plot (deteksi outliers)","526104ab":"Done, sekarang sudah berdistribusi normal targetnya.","5c7a307e":"### Preprocessor","05dfd91b":"Data Categoric."}}