{"cell_type":{"4225b534":"code","88a6cda1":"code","0eed5089":"code","537b6f0a":"code","3432d177":"code","57ec5f9b":"code","3e2389df":"code","61c19fc5":"code","b6540ce0":"code","f9019cd4":"code","e91d0246":"code","b02478db":"code","6ea7278b":"code","1938ada3":"code","a588c7a6":"code","d28d622d":"code","720c5822":"markdown","567ad42f":"markdown","f6aff172":"markdown","22b1a409":"markdown","48df3a43":"markdown","232427b4":"markdown","d6938895":"markdown","cb486792":"markdown","18de7786":"markdown","148d460f":"markdown","a36cd22e":"markdown","e27e06f1":"markdown","098aba6a":"markdown"},"source":{"4225b534":"#\n# Setting for obtaining reproducible results\n#\n\nimport numpy as np\nimport tensorflow as tf\nimport random as rn\n\n# The below is necessary in Python 3.2.3 onwards to\n# have reproducible behavior for certain hash-based operations.\n# See these references for further details:\n# https:\/\/docs.python.org\/3.4\/using\/cmdline.html#envvar-PYTHONHASHSEED\n# https:\/\/github.com\/keras-team\/keras\/issues\/2280#issuecomment-306959926\n\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\n\n# The below is necessary for starting Numpy generated random numbers\n# in a well-defined initial state.\n\nnp.random.seed(1234)\n\n# The below is necessary for starting core Python generated random numbers\n# in a well-defined state.\n\n#rn.seed(12345)\n\n# Force TensorFlow to use single thread.\n# Multiple threads are a potential source of\n# non-reproducible results.\n\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, \n                              inter_op_parallelism_threads=1)\n\nfrom keras import backend as K\n\n# The below tf.set_random_seed() will make random number generation\n# in the TensorFlow backend have a well-defined initial state.\n# For further details, \n# see: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/set_random_seed\n\ntf.set_random_seed(1234)\n\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","88a6cda1":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import *","0eed5089":"df_train = pd.read_csv('..\/input\/X_train.csv', encoding='cp949')\ndf_test = pd.read_csv('..\/input\/X_test.csv', encoding='cp949')\ny_train = pd.read_csv('..\/input\/y_train.csv').gender\nIDtest = df_test.cust_id.unique()\n\ndf_train.head(10)","537b6f0a":"df_train.goods_id.nunique(), df_train.gds_grp_nm.nunique(), df_train.gds_grp_mclas_nm.nunique()\n#df_train.store_nm.astype('category').cat.codes\n#pd.to_datetime(df_train.tran_date).dt.weekday","3432d177":"max_features = 100000\nmax_len = 100\nemb_dim = 128 ","57ec5f9b":"# Converts a \"goods_id\" to a sequence of indexes in a fixed-size hashing space\ndf_train.goods_id = df_train.goods_id.apply(lambda x: str(x))\ndf_test.goods_id = df_test.goods_id.apply(lambda x: str(x))\nX_train = df_train.groupby('cust_id')['goods_id'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\nX_test = df_test.groupby('cust_id')['goods_id'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\n\n# Pads sequences to the same length\nX_train_low = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_low = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_low.shape, X_test_low.shape","3e2389df":"# Converts a \"gds_grp_nm\" to a sequence of indexes in a fixed-size hashing space\nX_train = df_train.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(i, max_features\/\/10)[0] for i in x]).values\nX_test = df_test.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(i, max_features\/\/10)[0] for i in x]).values\n\n# Pads sequences to the same length\nX_train_mid = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_mid = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_mid.shape, X_test_mid.shape","61c19fc5":"# Converts a \"gds_grp_mclas_nm\" to a sequence of indexes in a fixed-size hashing space\nX_train = df_train.groupby('cust_id')['gds_grp_mclas_nm'].apply(lambda x: [one_hot(i, max_features\/\/100)[0] for i in x]).values\nX_test = df_test.groupby('cust_id')['gds_grp_mclas_nm'].apply(lambda x: [one_hot(i, max_features\/\/100)[0] for i in x]).values\n\n# Pads sequences to the same length\nX_train_high = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_high = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_high.shape, X_test_high.shape","b6540ce0":"# Converts a \"store_nm\" to a sequence of indexes in a fixed-size hashing space\nX_train = df_train.groupby('cust_id')['store_nm'].apply(lambda x: [one_hot(i, max_features\/\/100)[0] for i in x]).values\nX_test = df_test.groupby('cust_id')['store_nm'].apply(lambda x: [one_hot(i, max_features\/\/100)[0] for i in x]).values\n\n# Pads sequences to the same length\nX_train_str = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_str = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_str.shape, X_test_str.shape","f9019cd4":"df_train_new = df_train[['cust_id','tran_date','amount']].copy()\ndf_test_new = df_train[['cust_id','tran_date','amount']].copy()\ndef func(row):\n    row= pd.to_datetime(row['tran_date']).month\n    if 3 <= row  <=5:\n        return 'Spring'\n    elif  6 <= row <=8:\n        return 'Summer'\n    elif  9 <= row <=11:\n        return 'Fall'\n    else:\n        return 'Winter'\ndf_train_new['season'] =  df_train_new.apply(func, axis=1)\ndf_test_new['season'] =  df_test_new.apply(func, axis=1)\ndf_train_new\nX_train = df_train_new.groupby('cust_id')['season'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\nX_test = df_test_new.groupby('cust_id')['season'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\n\n\n# Pads sequences to the same length\nX_train_season = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_season = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_season.shape, X_test_season.shape\n","e91d0246":"# Converts a \"tran_date\" to a sequence of indexes in a fixed-size hashing space\ndf_train.tran_date = pd.to_datetime(df_train.tran_date).dt.weekday\ndf_test.tran_date = pd.to_datetime(df_test.tran_date).dt.weekday\n\nX_train = df_train.groupby('cust_id')['tran_date'].apply(lambda x: list(x)).values\nX_test = df_test.groupby('cust_id')['tran_date'].apply(lambda x: list(x)).values\n\n# Pads sequences to the same length\nX_train_day = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_day = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_day.shape, X_test_day.shape\n","b02478db":"\nX_train = df_train.groupby('cust_id')['amount'].agg([('totamount','sum')]).values\nX_test = df_test.groupby('cust_id')['amount'].agg([('totamount','sum')]).values\n\n# Pads sequences to the same length\nX_train_amount = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_amount = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_amount.shape, X_test_amount.shape","6ea7278b":"#\uc8fc\uc911\ud615\/\uc8fc\ub9d0\ud615\ndef func(row):\n    Dweek = row.tran_date\n    Dweek = Dweek.value_counts().to_dict()\n    a = Dweek.get(5, 0) + Dweek.get(6,0)\n    b = Dweek.get(0, 0) + Dweek.get(1,0) +Dweek.get(2, 0) + Dweek.get(3,0) +Dweek.get(4, 0)\n    if a >= b:\n        x = 1 \n    else: x = 0\n    return pd.Series({'\uc694\uc77c\uad6c\ub9e4\ud328\ud134':x})\n    \nX_train = df_train.groupby('cust_id').apply(func).values\nX_test = df_test.groupby('cust_id').apply(func).values    \nX_train_weekend = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_weekend = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_weekend.shape, X_test_weekend.shape","1938ada3":"#\ud658\ubd88\uad6c\ub9e4\uc561\n\ndf_train_new = df_train[['cust_id','amount']].copy()\ndf_test_new = df_train[['cust_id','amount']].copy()\n\ndf_train_new['return'] =np.where(df_train_new['amount']<0,-df_train_new['amount'],0) \ndf_test_new['return'] =np.where(df_test_new['amount']<0,-df_test_new['amount'],0) \n\n\nX_train = df_train_new.groupby('cust_id')['return'].agg([('\ucd1d\ud658\ubd88\uae08\uc561','sum')]).values\nX_test = df_test_new.groupby('cust_id')['return'].agg([('\ucd1d\ud658\ubd88\uae08\uc561','sum')]).values\n\n# Pads sequences to the same length\nX_train_returnsum= sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_returnsum = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_returnsum.shape, X_test_returnsum.shape\n","a588c7a6":"%%time\n\nfrom keras.models import Model\nfrom keras import Input\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nfrom keras.constraints import max_norm\nfrom keras.callbacks import EarlyStopping\n\n# Define the Model & its Architecture\nin_low = Input(shape=(max_len,), dtype='int32', name='low')\nx = layers.Embedding(max_features, emb_dim)(in_low)\nx = layers.Conv1D(32, 5, activation='elu')(x)\nx = layers.MaxPooling1D(5)(x)\nx = layers.Conv1D(32, 5, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_low = layers.Dropout(0.5)(x)\n\nin_mid = Input(shape=(max_len,), dtype='int32', name='mid')\nx = layers.Embedding(max_features\/\/10, emb_dim)(in_mid)\nx = layers.Conv1D(32, 3, activation='elu')(x)\nx = layers.MaxPooling1D(3)(x)\nx = layers.Conv1D(32, 3, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_mid = layers.Dropout(0.5)(x)\n\nin_high = Input(shape=(max_len,), dtype='int32', name='high')\nx = layers.Embedding(max_features\/\/100, emb_dim)(in_high)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.MaxPooling1D(1)(x)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_high = layers.Dropout(0.5)(x)\n\nin_str = Input(shape=(max_len,), dtype='int32', name='store')\nx = layers.Embedding(max_features\/\/100, emb_dim)(in_str)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.MaxPooling1D(1)(x)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_str = layers.Dropout(0.5)(x)\n\nin_day = Input(shape=(max_len,), dtype='int32', name='day')\nx = layers.Embedding(7, emb_dim)(in_day)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.MaxPooling1D(1)(x)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_day = layers.Dropout(0.5)(x)\n\n\nin_amount = Input(shape=(max_len,), dtype='int32', name='low')\nx = layers.Embedding(max_features, emb_dim)(in_low)\nx = layers.Conv1D(32, 5, activation='elu')(x)\nx = layers.MaxPooling1D(5)(x)\nx = layers.Conv1D(32, 5, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_amount = layers.Dropout(0.5)(x)\n\nin_weekend = Input(shape=(max_len,), dtype='int32', name='weekend')\nx = layers.Embedding(max_features\/\/100, emb_dim)(in_weekend)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.MaxPooling1D(1)(x)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_weekend = layers.Dropout(0.5)(x)\n\nin_season = Input(shape=(max_len,), dtype='int32', name='season')\nx = layers.Embedding(7, emb_dim)(in_season)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.MaxPooling1D(1)(x)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_season = layers.Dropout(0.5)(x)\n\n\nin_returnsum = Input(shape=(max_len,), dtype='int32', name='returnsum')\nx = layers.Embedding(7, emb_dim)(in_returnsum)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.MaxPooling1D(1)(x)\nx = layers.Conv1D(32, 1, activation='elu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nout_returnsum = layers.Dropout(0.5)(x)\n\n\nx = layers.add([out_low, out_mid, out_high, out_str, out_day, out_amount, out_weekend, out_season,  out_returnsum])\nout = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = Model([in_low, in_mid, in_high, in_str, in_day, in_amount, in_weekend, in_season, in_returnsum], out)\nmodel.summary()\n\n# Choose the Optimizer and the Cost function\nmodel.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n\n# Train the Model\nhistory = model.fit([X_train_low, X_train_mid, X_train_high, X_train_str, X_train_day, X_train_amount, X_train_weekend, X_train_season,  X_train_returnsum], y_train, epochs=50, batch_size=16, \n                    validation_split=0.2, callbacks=[EarlyStopping(patience=7)])\n\nplt.plot(history.history[\"loss\"], label=\"train loss\")\nplt.plot(history.history[\"val_loss\"], label=\"validation loss\")\nplt.legend()\nplt.title(\"Loss\")\nplt.show()","d28d622d":"pred = model.predict([X_test_low, X_test_mid, X_test_high, X_test_str, X_test_day, X_test_amount, X_test_weekend, X_test_season, X_test_returnsum])[:,0]\nfname = 'submissions.csv'\nsubmissions = pd.concat([pd.Series(IDtest, name=\"cust_id\"), pd.Series(pred, name=\"gender\")] ,axis=1)\nsubmissions.to_csv(fname, index=False)\nprint(\"'{}' is ready to submit.\" .format(fname))","720c5822":"## End","567ad42f":"##### weekday","f6aff172":"##### store","22b1a409":"### Transform Data","48df3a43":"### Make Submissions","232427b4":"##### low level: goods_id","d6938895":"##### high level: gds_grp_mclas_nm","cb486792":"## This model has the following characteristics:\n* No feature engineering\n* Applying multiple Conv1Ds to raw transactions","18de7786":"##### middle level: gds_grp_nm","148d460f":"\n### Read Data","a36cd22e":"### Build Models","e27e06f1":"amount","098aba6a":"#time difference\ndf_train_new = df_train[['cust_id','tran_date']].copy()\ndf_test_new = df_train[['cust_id','tran_date']].copy()\ndf_train_new.tran_date = pd.to_datetime(df_train.tran_date)\ndf_test_new.tran_date = pd.to_datetime(df_test.tran_date)\n\ndf_train_new['diffs'] = df_train_new.groupby('cust_id')['tran_date'].diff()\ndf_test_new['diffs'] = df_test_new.groupby('cust_id')['tran_date'].diff()\ndf_train_new.diffs = df_train_new.diffs.dt.days\ndf_test_new.diffs = df_test_new.diffs.dt.days\ndf_train_new.fillna(0,inplace=True)\ndf_test_new.fillna(0, inplace = True)\n\nX_train = df_train_new.groupby('cust_id')['diffs'].apply(lambda x: list(x)).values\nX_test = df_test_new.groupby('cust_id')['diffs'].apply(lambda x: list(x)).values\n\n# Pads sequences to the same length\nX_train_diff = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test_diff = sequence.pad_sequences(X_test, maxlen=max_len)\n\nX_train_diff.shape, X_test_diff.shape"}}