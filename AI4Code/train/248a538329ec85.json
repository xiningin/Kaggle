{"cell_type":{"276e7629":"code","96500327":"code","53e06b8f":"code","288cbc1d":"code","ead99542":"code","67dd2dee":"code","1b1c58fe":"code","aace9cf4":"code","70dcbe31":"code","e1df9520":"code","ca87872c":"code","e3b45e58":"code","74ccbdb4":"code","6e777393":"code","533ec25a":"code","e9cc382c":"code","cf2e85e7":"code","464a8997":"code","b1f07e2a":"code","397be3dc":"code","184c337a":"code","6226b0bb":"code","e0296992":"code","db345db0":"code","a04db0b6":"code","cf1499e3":"code","127afac7":"code","7f32d7cd":"code","42b83372":"code","8944ba46":"code","eef22c71":"code","9bfe93ec":"code","2cb016e8":"code","f87f0b0c":"code","b56da7ea":"code","97957911":"code","d3398306":"code","61f498f1":"code","c4219909":"code","5b036798":"code","cb0b4a32":"code","3feb31f2":"markdown","57fd1722":"markdown","1d123aab":"markdown","8f20616c":"markdown","8b40e02f":"markdown","7cd1f9b9":"markdown","9bf80b9f":"markdown","98fb24cd":"markdown","0667c5d7":"markdown","a01cf294":"markdown","251854fe":"markdown","63c12473":"markdown","612a739e":"markdown","a1f7c6c2":"markdown","88031a82":"markdown","54293c3b":"markdown","4bb3aab4":"markdown","217a1c7b":"markdown","4703f9c4":"markdown","5faab070":"markdown","052efd19":"markdown","41b287b7":"markdown"},"source":{"276e7629":"!pip install fastai2 --quiet","96500327":"from fastai2.vision.all import *\n\npath = Path('..\/input\/global-wheat-detection')","53e06b8f":"path.ls()","288cbc1d":"df = pd.read_csv(path\/'train.csv')","ead99542":"df.head()","67dd2dee":"df['bbox'].isna().sum()","1b1c58fe":"imgs = get_image_files(path\/'train')","aace9cf4":"len(imgs) == df['image_id'].nunique()","70dcbe31":"len(imgs) - df['image_id'].nunique()","e1df9520":"im_df = df['image_id'].unique()","ca87872c":"im_df = [fn + '.jpg' for fn in im_df]","e3b45e58":"im_df[:5]","74ccbdb4":"fns = [Path(str(path\/'train') + f'\/{fn}') for fn in im_df]","6e777393":"fns[0]","533ec25a":"fns[0].name[:-4]","e9cc382c":"def get_items(noop): return fns","cf2e85e7":"df['label'] = 'wheat'","464a8997":"df_np = df.to_numpy()","b1f07e2a":"coco_source = untar_data(URLs.COCO_TINY)\nimages, lbl_bbox = get_annotations(coco_source\/'train.json')\nimg2bbox = dict(zip(images, lbl_bbox))","397be3dc":"fn = images[0]; fn","184c337a":"img2bbox[fn][0][0]","6226b0bb":"def get_tmp_bbox(fn):\n    \"Grab bounding boxes from `DataFrame`\"\n    rows = np.where((df_np[:, 0] == fn.name[:-4]))\n    bboxs = df_np[rows][:,3]\n    bboxs = [b.replace('[', '').replace(']', '') for b in bboxs]\n    return np.array([np.fromstring(b, sep=',') for b in bboxs])","e0296992":"def get_tmp_lbl(fn):\n    \"Grab label from `DataFrame`\"\n    rows = np.where((df_np[:, 0] == fn.name[:-4]))\n    return df_np[rows][:,5]","db345db0":"fnames = df['image_id'].unique(); fnames[:3]","a04db0b6":"bboxs = get_tmp_bbox(fns[0])\nlbls = get_tmp_lbl(fns[0])\narr = np.array([fns[0].name[:-4], bboxs, lbls])","cf1499e3":"arr","127afac7":"for fname in fns[1:]:\n    bbox = get_tmp_bbox(fname)\n    lbl = get_tmp_lbl(fname)\n    arr2 = np.array([fname.name[:-4], bbox, lbl])\n    arr = np.vstack((arr, arr2))","7f32d7cd":"arr[:,1][0][0][0] + arr[:,1][0][0][2]","42b83372":"arr[:,1][0][1]","8944ba46":"for i, im in enumerate(arr[:,1]):\n    for j, box in enumerate(im):\n        arr[:,1][i][j][2] = box[0]+box[2]\n        arr[:,1][i][j][3] = box[1]+box[3]","eef22c71":"arr[0][1][0]","9bfe93ec":"np.save('data.npy', arr)","2cb016e8":"def get_bbox(fn):\n    \"Gets bounding box from `fn`\"\n    idx = np.where((arr[:,0] == fn.name[:-4]))\n    return arr[idx][0][1]","f87f0b0c":"def get_lbl(fn):\n    \"Get's label from `fn`\"\n    idx = np.where((arr[:,0] == fn.name[:-4]))\n    return arr[idx][0][2]","b56da7ea":"%%timeit\n_ = get_bbox(fns[0])","97957911":"%%timeit\n_ = get_lbl(fns[0])","d3398306":"wheat = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n                 get_items=get_items,\n                 splitter=RandomSplitter(),\n                 get_y=[get_bbox, get_lbl],\n                 item_tfms=Resize(256, method=ResizeMethod.Pad),\n                 n_inp=1)","61f498f1":"dls = wheat.dataloaders(path,bs=32)","c4219909":"dls.show_batch(max_n=1, figsize=(12,12))","5b036798":"batch = dls.one_batch()","cb0b4a32":"batch[1].shape","3feb31f2":"`get_y` needs to return the coordinates then the label. Let's look at an example quickly","57fd1722":"Now let's start building our ground truth data. We'll want an initial array to add to","1d123aab":"And now we can add the rest of the data","8f20616c":"And now we can build our `DataLoaders` and you're done!","8b40e02f":"*Much* more efficent to use `NumPy` here.\n\n# DataLoaders","7cd1f9b9":"And now let's grab the `vision` library and set up our `Path`","9bf80b9f":"For a true test of speed, to get the first value with pandas it takes ~6.4-6.6 milliseconds for each function. Let's see how ours does:","98fb24cd":"Our labels are inside of the `train.csv`, let's take alook","0667c5d7":"That looks much better!","a01cf294":"# `fastai2` Starter Kernel\n\nThis kernel will walk you through how to set up the `DataBlock` for this competition!","251854fe":"For our `DataLoaders`, we're going to want to use a `ImageBlock` for our input, and the `BBoxBlock` and `BBoxLblBlock` for our outputs, our custom `get_items`, along with some `get_y`'s. I chose some very simple transforms for us to use here. Finally we need to specify the number of inputs to simply be 1, telling `fastai` we have two outputs","63c12473":"So we have 49 images that *aren't* labelled. Let's build on this now by simply making a custom set of `Paths` that contain our working images, and build a `get_items` for it","612a739e":"We don't require as much for the labels, as all of them are simply a string of \"wheat\"","a1f7c6c2":"First let's grab `fastai2` (make sure your internet is turned on!)","88031a82":"Now that we know the format, let's work with our `DataFrame` to return something `fastai2` wants. First let's convert our bounding boxes into something we can use. NumPy has a nice `np.fromstring`, but it wants just numbers whereas our `DataFrame` will give us: `\"[0,0,0,0]\"`, which we don't want! So we'll replace both of the brackets first. This is just temporary for now though, as we want to make everything run on NumPy for efficiency","54293c3b":"Let's make our true `get_bbox` and `get_lbl`. We'll want to first search our NumPy array for a matching filename, then grab the second or third index for the bounding box or the label respectively ","4bb3aab4":"Now we have our actual data array, we need to make some adjustments. Currently our coordinates are x,y,w,h and we want x1,y1,x2,y2. So let's look at converting those!","217a1c7b":"We'll make a `get_items` that simply returns our *good* images","4703f9c4":"For a time comparison, the Pandas method took ~17 seconds to build the dataloaders and 1.3 seconds per batch. Using NumPy we reduce this to 90ms and 838ms per batch (with most of that time taken up by shuffling the data)","5faab070":"So we can see that we get an `image_id` and one label per row. We'll need to remember that in a little!","052efd19":"## DataFrame Format:\n\n`image_id` is the same, `bbox` contains bounding boxes, all labels are `wheat`\n\nKeeping everything in the `DataFrame` is super inefficient, so we'll move everything to a NumPy array to load it faster.","41b287b7":"To convert it, we need to add our width and height to the respective x and y. We can do this like so:"}}