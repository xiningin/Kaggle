{"cell_type":{"0059f9cd":"code","9f2c459a":"code","c82ac057":"code","8828eaba":"code","6be7636a":"code","a439c52a":"code","f534553b":"code","39a23e88":"code","01f433b6":"code","62daaedf":"code","bd9cb71a":"code","68222c54":"code","e210c0a5":"code","0e74be45":"code","fc472130":"code","d5f92d54":"code","f7890706":"code","46316f2a":"code","d742b645":"code","c33f5ef3":"code","e6c634df":"code","b789a899":"code","829d5e48":"code","d6ae2e67":"code","12756c4d":"markdown","4d9cd753":"markdown","0b2a3ed9":"markdown","b44c0a03":"markdown","a2a2aa6f":"markdown","22a551ab":"markdown","64c7a4ae":"markdown","43053531":"markdown","9d2bebe6":"markdown","c4b070a6":"markdown"},"source":{"0059f9cd":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport lightgbm\nimport xgboost\nimport catboost\nfrom tensorflow import keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndirectory = '\/kaggle\/input\/tabular-playground-series-may-2021\/'","9f2c459a":"train = pd.read_csv(directory + 'train.csv')\ntest = pd.read_csv(directory + 'test.csv')\nsubmission = pd.read_csv(directory + 'sample_submission.csv')\nsubmission = submission.set_index('id')\ntrain.head()","c82ac057":"train_features = train.drop(['target', 'id'], axis=1).values\ntest_features = test.drop('id', axis=1).values\ntarget = train['target'].values\ntrain_features","8828eaba":"max_nums = []\nfor col in train_features.T:\n    max_nums.append(max(col))\nprint(max_nums.index(max(max_nums)))\nmax(max_nums)","6be7636a":"sns.histplot(target)\npd.DataFrame(train_features).describe(include='all')","a439c52a":"def class_to_num(classes):\n    return [int(word[-1]) for word in classes]\n\ndef num_to_class(nums):\n    return ['Class_' + str(num) for num in nums]\n\nlabels = np.array(class_to_num(target))\nlabels","f534553b":"X_train, X_valid = train_test_split(train_features, test_size=0.3, shuffle=True, random_state=2021)\ny_train, y_valid = train_test_split(labels, test_size=0.3, shuffle=True, random_state=2021)","39a23e88":"lgb = lightgbm.LGBMClassifier()\nlgb.fit(X_train, y_train)\nlgb_pred = lgb.predict_proba(X_valid)\nlog_loss(y_valid, lgb_pred)","01f433b6":"ctb = catboost.CatBoostClassifier(verbose=False)\nctb.fit(X_train, y_train)\nctb_pred = ctb.predict_proba(X_valid)\nlog_loss(y_valid, ctb_pred)","62daaedf":"xgb = xgboost.XGBClassifier(verbose=False)\nxgb.fit(X_train, y_train)\nxgb_pred = xgb.predict_proba(X_valid)\nlog_loss(y_valid, xgb_pred)","bd9cb71a":"def objective(trial):\n    params = {\n        'n_estimators' : 20000,\n        'max_depth' : trial.suggest_int('max_depth', 5, 10),\n        'learning_rate' : trial.suggest_float('learning_rate', 0.005, 0.05),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 30),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 30),        \n        'subsample':trial.suggest_float('subsample', 0.6, 1),\n        'colsample_bytree':trial.suggest_float('colsample_bytree', 0.1, 0.3),\n        'colsample_bynode':trial.suggest_float('colsample_bynode', 0.1,1), \n        'colsample_bylevel':trial.suggest_float('colsample_bylevel', 0.1, 1),   \n\n        \n    }\n    \n    xgb = xgboost.XGBClassifier(verbose=0, **params)\n    xgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=150, verbose=False)\n    xgb_pred = xgb.predict_proba(X_valid)\n    return log_loss(y_valid, xgb_pred)","68222c54":"def objective(trial):\n    params = {\n            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 0.5),\n            'n_estimators' : 20000,\n            'max_bin' : int(trial.suggest_int('max_bin', 10, 5000)),\n            'num_leaves' : int(trial.suggest_int('num_leaves', 10, 200)),\n            'max_depth' : int(trial.suggest_int('max_depth', 5, 200)),\n            'reg_alpha' : trial.suggest_float('reg_alpha', 0, 10),\n            'reg_lambda' : trial.suggest_float('reg_lambda', 0, 10),\n            'subsample' : trial.suggest_float('subsample', 0.5, 1),\n        }\n    \n    lgb = lightgbm.LGBMClassifier(verbose=0, **params)\n    lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=150, verbose=False)\n    lgb_pred = lgb.predict_proba(X_valid)\n    return log_loss(y_valid, lgb_pred)","e210c0a5":"def objective(trial):\n    params = {\n        'n_estimators' : (trial.suggest_int('n_estimators', 8000, 13000)),\n        'depth' : (trial.suggest_int('depth', 2, 8)),\n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.008, 0.8),\n        'colsample_bylevel' : trial.suggest_float('colsample_bylevel',0.5, 1),\n        'bagging_temperature' : trial.suggest_float('bagging_temperature',0.3, 0.7),\n        'l2_leaf_reg' : trial.suggest_float('l2_leaf_reg', 0, 15),\n    }\n    \n    ctb = catboost.CatBoostClassifier(verbose=0, **params)\n    ctb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=150, verbose=False)\n    ctb_pred = ctb.predict_proba(X_valid)\n    return log_loss(y_valid, ctb_pred)","0e74be45":"lgb_params = {'learning_rate': 0.045955784574255566, 'n_estimators': 20000, 'max_bin': 94,\n              'num_leaves': 10, 'max_depth': 27, 'reg_alpha': 8.457214771314742, \n              'reg_lambda': 6.853524481506691, 'subsample': 0.7497817286847477}","fc472130":"xgb_params = {'n_estimators': 20000, 'max_depth': 6, 'learning_rate': 0.020120141936692624, 'reg_lambda': 29.32699373396152, \n              'subsample': 0.818335257624409, 'colsample_bytree': 0.23592240474190632, \n              'colsample_bynode': 0.8200588520341814, 'colsample_bylevel': 0.45383415964985685}","d5f92d54":"ctb_params = {'n_estimators': 20000, 'depth': 4, 'learning_rate': 0.023629454134134822, \n              'colsample_bylevel': 0.6550855840039158, 'bagging_temperature': 0.9219975014443456, \n              'l2_leaf_reg': 10.133650161121691}","f7890706":"folds = 10\nlgb_train_preds = []\nlgb_test_preds = []\ntrain_features = pd.DataFrame(train_features)\nlabels = pd.DataFrame(labels)\nNfold = StratifiedKFold(folds, shuffle=True, random_state=2021)\nfor fold, (train_index, test_index) in enumerate(Nfold.split(train_features, labels)):\n    \n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    \n    X_train, X_valid = train_features.iloc[train_index], train_features.iloc[test_index]\n    y_train, y_valid = np.array(labels.iloc[train_index]).ravel(), np.array(labels.iloc[test_index]).ravel()\n    \n    lgb = lightgbm.LGBMClassifier(**lgb_params)\n    lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50, verbose=False)\n    lgb_pred = lgb.predict_proba(X_valid)\n    lgb_train_preds.append(lgb_pred)\n    lgb_test_preds.append(lgb.predict_proba(test_features))\n    print(f'Log Loss for Lightgbm Fold {fold+1}: {log_loss(y_valid, lgb_pred)}')","46316f2a":"ctb_train_preds = []\nctb_test_preds = []\n\nfor fold, (train_index, test_index) in enumerate(Nfold.split(train_features, labels)):\n    \n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    \n    X_train, X_valid = train_features.iloc[train_index], train_features.iloc[test_index]\n    y_train, y_valid = np.array(labels.iloc[train_index]).ravel(), np.array(labels.iloc[test_index]).ravel()\n    \n    ctb = catboost.CatBoostClassifier(**ctb_params)\n    ctb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=150, verbose=False)\n    ctb_pred = ctb.predict_proba(X_valid)    \n    ctb_train_preds.append(ctb_pred)\n    ctb_test_preds.append(ctb.predict_proba(test_features))\n    print(f'Log Loss for Catboost Fold {fold+1}: {log_loss(y_valid, ctb_pred)}')","d742b645":"import warnings\nwarnings.filterwarnings(\"ignore\")\nxgb_train_preds = []\nxgb_test_preds = []\n\nfor fold, (train_index, test_index) in enumerate(Nfold.split(train_features, labels)):\n    \n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    \n    X_train, X_valid = train_features.iloc[train_index], train_features.iloc[test_index]\n    y_train, y_valid = np.array(labels.iloc[train_index]).ravel(), np.array(labels.iloc[test_index]).ravel()\n    \n    xgb = xgboost.XGBClassifier(**xgb_params)\n    xgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=150, verbose=False, eval_metric='mlogloss')\n    xgb_pred = xgb.predict_proba(X_valid)    \n    xgb_train_preds.append(xgb_pred)\n    xgb_test_preds.append(xgb.predict_proba(test_features))\n    print(f'Log Loss for XGBoost Fold {fold+1}: {log_loss(y_valid, xgb_pred)}')","c33f5ef3":"y_valids = []\nfor fold, (train_index, test_index) in enumerate(Nfold.split(train_features, labels)):\n        \n    y_valid = labels.iloc[test_index]\n    y_valids.append(y_valid)","e6c634df":"from sklearn.linear_model import RidgeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nrd = CalibratedClassifierCV(RidgeClassifier())\nblend_probas = []\nfor i, (lgb_pred, xgb_pred, ctb_pred, lgb_test, xgb_test, ctb_test) in enumerate(\n                                                                      zip(lgb_train_preds, xgb_train_preds, ctb_train_preds,\n                                                                      lgb_test_preds, xgb_test_preds, ctb_test_preds)):\n    blend_train = np.c_[lgb_pred, xgb_pred, ctb_pred]\n    blend_test = np.c_[lgb_test, xgb_test, ctb_test]\n    rd.fit(blend_train, y_valids[i][0].values)\n    print(log_loss(y_valids[i][0].values, rd.predict_proba(blend_train)))\n    blend_probas.append(rd.predict_proba(blend_test))","b789a899":"from scipy.optimize import minimize\nscores = []\nweights = []\nfor y, lgb_pred, xgb_pred, ctb_pred in zip(y_valids, lgb_train_preds, xgb_train_preds, ctb_train_preds):\n    preds = []\n    preds.append(lgb_pred)\n    preds.append(xgb_pred)\n    preds.append(ctb_pred)\n    def log_weight_loss(weights):\n        weighted_pred = (weights[0]*preds[0]) + (weights[1]*preds[1]) + (weights[2]*preds[2])\n        return log_loss(y, weighted_pred)\n    starting_values = [0.3]*len(preds) \n    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n    bounds = [(0,1)]*len(preds) \n    res = minimize(log_weight_loss, starting_values, method='Nelder-Mead', bounds=bounds, constraints=cons)\n    \n    weights.append(res['x'])\n    print(res['fun'])\n    scores.append(res['fun'])","829d5e48":"final_weights = sum(weights)\/folds\nweighted_preds = np.array((final_weights[0] * sum(lgb_test_preds)\/folds)+(final_weights[1] * sum(xgb_test_preds)\/folds)+(final_weights[2] * sum(ctb_test_preds)\/folds))","d6ae2e67":"submission[['Class_1', 'Class_2', 'Class_3', 'Class_4']] = ((sum(blend_probas)\/folds)+weighted_preds)\/2\nsubmission.to_csv('submission.csv')","12756c4d":"Finally, we combine the two predictions in a simple non-weighted average to generate the final predictions:","4d9cd753":"The following simply will generate the validation targets for the corresponding validation sets.","0b2a3ed9":"The following histogram shows a large imabalance between the classes, with class2 being significantly more frequent than the others. The highest number in the training features is 66 and is in column 38:","b44c0a03":"The following are the optuna functions used to optimize the parameters for each of the models. Better parameters can probably be found as I did not run too many trials for each one. ","a2a2aa6f":"Now that the best parameters are found, we can generate 10 sets of predictions for each model using cv. The following three cells take roughly 45-50 minutes, but could vary based on the device you are using.","22a551ab":"The following cell uses scipy to find the optimal weights in a weighted average between the predictions of lightgbm, xgboost, and catboost.","64c7a4ae":"The cell underneath will train a stacked model where the meta-estimator is a Calibrated Ridge Classifier, and is trained on each fold of the booster predictions. ","43053531":"The following parameters are the best ones that were recovered by Optuna. ","9d2bebe6":"The three models I use are lightgbm, xgboost, and catboost Classifiers which seem to be the most powerful in the tps. These are the baseline scores, and we can see how to improve upon these scores.","c4b070a6":"The following functions simply map the given targets to numbers by taking the last digit of the target and vice versa:"}}