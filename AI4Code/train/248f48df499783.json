{"cell_type":{"81ec850b":"code","92fae949":"code","558afea2":"code","705fb412":"code","f6db07ce":"code","9b4bbdde":"code","99787425":"code","78157d38":"code","83cb2dc2":"code","725caa8f":"code","d9b908b0":"code","11ba2982":"code","c53bcbf9":"code","262a9095":"code","aaa3fae1":"code","3ddc4593":"code","5a0b666f":"code","ed5c234a":"code","02613bb1":"code","13e6d883":"code","dbbb8259":"code","cf0708e4":"code","5529951f":"code","10da928d":"code","e51d5964":"code","022d32ad":"code","0620d972":"code","9c22d746":"code","d20cb705":"code","397b0830":"code","6e2cc07f":"code","450bd48f":"code","a1f7cd75":"code","80b2b1f3":"code","9ab5f42d":"code","7f970e2a":"code","ba768763":"code","90f7ac02":"code","21f1ec76":"code","6afe453a":"code","d4815b7a":"code","76730d94":"code","38113706":"code","311be578":"code","010c8c17":"code","cd4fe649":"code","0e9286a9":"code","9ca16c32":"code","522a323d":"code","20704aec":"code","61d49768":"code","05fb2e1d":"code","f8e86c2a":"code","816fd410":"code","0081098a":"code","2ab5a3b8":"code","a1616862":"code","8b5aef01":"code","c1359d0b":"code","6e107188":"code","34131365":"code","4aafc19a":"code","a6d8a202":"code","af67c5e4":"code","6822db28":"code","cd182664":"code","eb6f2292":"code","636b0e10":"code","d1483864":"code","5c564d2c":"code","74b58f49":"code","57b83d62":"code","d1c97d3a":"code","ab5c02c7":"code","84edc9be":"code","b6298ec6":"code","e17a887c":"code","691a2f62":"code","2e7399ce":"code","14075f84":"code","4eba01d0":"code","b1cde84a":"code","aced0e6c":"code","bb2978da":"code","62e302ae":"code","269c6f67":"code","94d3e968":"code","eb4ae535":"code","5e6ad4c4":"code","075556bd":"code","9c43cfbf":"code","7328be5a":"code","842b2bcf":"code","443dde12":"code","7bdd0396":"code","522edc0b":"code","d2730c35":"code","89968c22":"code","280406ad":"code","1f7066cb":"code","11412209":"code","fd57c403":"code","2d9e4eab":"code","55aacb12":"code","63544b66":"code","f8ac7142":"markdown","872e0336":"markdown","7a1bcfbc":"markdown","61ae21cb":"markdown","c5eb98b5":"markdown","4d046f9b":"markdown","8beed7cd":"markdown","4f38c342":"markdown","f309c686":"markdown","9569ff21":"markdown","7019c69b":"markdown","6cef4a83":"markdown","ce6d445e":"markdown","39b587ed":"markdown","d8dd32a4":"markdown","e1ff1e3c":"markdown","db2d0218":"markdown","d7692267":"markdown","bbc2c92a":"markdown","c1ce6ac2":"markdown","1796d97f":"markdown","653d460f":"markdown","a58ce492":"markdown","5a3554f4":"markdown","2a0b6c92":"markdown","88b7f3e2":"markdown","9dd98c13":"markdown","3cc47bc2":"markdown","9b30481f":"markdown","5aafa4a1":"markdown","87d68ceb":"markdown","daa81a23":"markdown","25ef05a7":"markdown","5f6d4756":"markdown","990c61a0":"markdown","cf0bfda9":"markdown","00887057":"markdown","badc19e4":"markdown","c56b7aba":"markdown","a243d09b":"markdown","279f6550":"markdown","9a08fe90":"markdown","014467fa":"markdown","e981faed":"markdown","5c554196":"markdown","fe93369c":"markdown","4c534125":"markdown","05e1b412":"markdown","8d54e390":"markdown","61e3926a":"markdown","a8740f9a":"markdown","c45368d7":"markdown","5424c83d":"markdown","013f7cb3":"markdown","133a416c":"markdown","6595f66b":"markdown","b7431e95":"markdown","acee2be5":"markdown","14c1c9b3":"markdown","dacd7d05":"markdown","2be204e2":"markdown","f3e7ecca":"markdown"},"source":{"81ec850b":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.stats import linregress\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nimport math\nsns.set()\n# plt.style.use(\"dark_background\")\nwarnings.filterwarnings('ignore')\n%matplotlib inline","92fae949":"train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","558afea2":"test_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","705fb412":"ids = test_df[\"Id\"]","f6db07ce":"# Helper Functions -- Jump to code cell below\ndef get_scatterplot(df, var, col = 10, row = 7):\n    plt.figure(figsize=(col, row))\n    data = pd.concat([df[var], df[target]], axis = 1)\n    sns.scatterplot(data = data, x = var, y = target)\n\ndef get_boxplot(df, x, y = None, col = 10, row = 7):\n    plt.figure(figsize=(col, row))\n    data = pd.concat([df[x], df[target]], axis = 1)\n    if y == None:\n        sns.boxplot(data = data, x = x)\n    else:\n        sns.boxplot(data = data, x = x, y = y)\n\ndef get_heatmap(correlations, theme = \"bwr\", col = 15, row = 15, round_off = 1, annot = False):\n    plt.figure(figsize=(col, row))\n    round_off = str(round_off)\n    sns.heatmap(correlations, square = True, cmap = theme, fmt = \".\" + round_off + \"f\", annot = annot, center = 0, linecolor =\"white\")\n\ndef get_correlated(df, k = 10, plot = True, theme = \"bwr\", col = 15, row = 15, round_off = 1, annot = False):\n    cols = df.corr().nlargest(k, target)[target]\n    if plot == True:\n        get_heatmap(df[cols.index].corr(), theme, col, row, round_off, annot)\n    return cols\n\ndef get_pairplot(df, vars, col = 15, row = 15, size = 2.5):\n    plt.figure(figsize=(col, row))\n    sns.pairplot(df[vars], size = size)\n\n# Set kde = False to disable line.\ndef get_distplot(df, var, kde = True, fit = None):\n    sns.distplot(df[var], kde = kde, fit = fit)\n    return {\"Skewness\" : df[var].skew(), \"Kurtosis\" : df[var].kurt()}\n\n# Returns categorical cols, numeric cols, and labelled dataframe consisting of both.\ndef identify_columns(df):\n    categorical = [col for col in df.columns if df[col].dtype == 'O']\n    numeric = [col for col in df.columns if df[col].dtype != 'O']\n    return categorical, numeric","9b4bbdde":"train_df.shape, test_df.shape","99787425":"categorical, numeric = identify_columns(train_df)\nprint(\"{} Numeric features : {} \\n{} Categorical features : {}\".format(len(numeric), numeric,len(categorical), categorical))","78157d38":"target = \"SalePrice\"\ntrain_df[target].describe()","83cb2dc2":"plt.figure(figsize = (12, 5))\nsns.distplot(train_df[target], kde = True, color = 'Purple')","725caa8f":"#GrLivArea\nsns.jointplot(x = train_df[\"GrLivArea\"], y = train_df[\"SalePrice\"], hue = train_df[\"OverallQual\"])","d9b908b0":"outlier_idx = train_df[\"GrLivArea\"].sort_values(ascending=False)[:2].index\ntrain_df.drop(index = outlier_idx, inplace = True)","11ba2982":"# TotalBsmtSF\nsns.jointplot(x = train_df[\"TotalBsmtSF\"], y = train_df[\"SalePrice\"], hue = train_df[\"OverallQual\"])","c53bcbf9":"outlier_idx = train_df[\"TotalBsmtSF\"].sort_values(ascending=False)[:1].index\ntrain_df.drop(index = outlier_idx, inplace = True)","262a9095":"plt.figure(figsize = (18, 9))\nax = sns.boxplot(x='OverallQual', y='SalePrice', data=train_df, palette = \"RdPu\")\nx = plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")","aaa3fae1":"plt.figure(figsize = (18, 9))\nax = sns.boxplot(x='OverallCond', y='SalePrice', data=train_df, palette = \"RdPu\")\nx = plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")","3ddc4593":"plt.figure(figsize=(18, 9))\nsns.scatterplot(x = \"YearBuilt\", y = \"SalePrice\", data = train_df, hue = \"OverallQual\")","5a0b666f":"plt.figure(figsize = (19 , 19))\ncorr = train_df.corr()\nsns.heatmap(corr[(corr >= 0.3) | (corr <= -0.3)], vmax=1.0, vmin=-1.0, linewidths=0,\n            annot=True, square=True, fmt = \".1f\", cmap = \"RdPu_r\");\nplt.title(\"Heatmap\\n\", fontsize = 32)","ed5c234a":"features = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'GarageArea','YearBuilt','YearRemodAdd', 'GarageYrBlt', 'MasVnrArea']\nplt.figure(figsize = (19, 12))\nplot_no = 1\nfor ft in features:\n\tplt.subplot(4, 3, plot_no)\n\tsns.regplot(x = ft, y = \"SalePrice\", data = train_df,  scatter_kws={\"color\" : \"#B900B9\", \"s\": 40}, line_kws={\"color\": \"red\"}, x_bins=50)\n\tplot_no += 1\n\nplt.tight_layout()","02613bb1":"to_analyse = ['HeatingQC', 'KitchenQual', 'Exterior2nd', 'SaleType', 'FireplaceQu', 'ExterQual', 'CentralAir', 'Exterior1st', 'Neighborhood', 'LotShape', 'GarageFinish', 'SaleCondition', 'Foundation', 'BsmtFinType1', 'MSZoning', 'BsmtQual', 'GarageCond', 'GarageType', 'BsmtExposure', 'MasVnrType', 'GarageQual', 'BldgType', 'HouseStyle', 'RoofStyle']\nplt.figure(figsize = (18, 27))\nplot_no = 1\nfor feature in to_analyse:\n\tplt.subplot(6, 4, plot_no)\n\tsns.countplot(x = feature, data = train_df, palette=\"RdPu_r\")\n\tplt.xticks(rotation = 60, fontsize = 10)\n\tplot_no += 1\nplt.suptitle(\"Distribution of Categorical Features\\n\", fontsize =36, style = \"oblique\")\nplt.tight_layout()","13e6d883":"to_analyse = categorical\nplt.figure(figsize = (18, 54))\nplot_no = 1\nfor feature in to_analyse:\n\tplt.subplot(11, 4, plot_no)\n\torder = train_df.groupby([feature] ,as_index=False).agg({'SalePrice':'mean'}).sort_values(\"SalePrice\")[feature].tolist()\n\tsns.boxplot(x = feature, y = \"SalePrice\", data = train_df, palette=\"RdPu_r\",order = order)\n\tplt.xticks(rotation = 60, fontsize = 10)\n\tplot_no += 1\nplt.suptitle(\"Categorical Features v SalePrice\\n\\n\", fontsize =36, style = \"oblique\")\nplt.tight_layout()","dbbb8259":"discrete_high_corr = get_correlated(train_df, annot=True, round_off=2, k=15, plot = False)\nplt.figure(figsize = (18, 18))\nsns.heatmap(train_df[discrete_high_corr.index].corr(), cmap = \"RdPu\", vmax=1, vmin=-1, annot = True, fmt = \".2f\", square=True)\nplt.title(\"Top 15 correlated features w\/ SalePrice\\n\", fontsize = 28)","cf0708e4":"print(\"Highly Correlated Variables :\\n {}\".format(discrete_high_corr.index.tolist()))","5529951f":"discrete_high_corr.drop(index=[\"TotRmsAbvGrd\", \"GarageYrBlt\"], inplace = True)","10da928d":"sns.pairplot(data = train_df[['SalePrice', 'GrLivArea', '1stFlrSF', 'GarageArea','YearBuilt', \"OverallQual\"]], hue=\"OverallQual\", palette=\"RdPu\")","e51d5964":"discrete_features = discrete_high_corr.index.tolist()\ndiscrete_features.remove(\"SalePrice\")\ncategorical_features = ['BsmtFinType1', 'GarageCond', 'BsmtExposure', 'BsmtQual', 'KitchenQual', 'GarageFinish', 'Neighborhood', 'MSZoning', 'ExterQual', 'GarageType', 'LotShape', 'HeatingQC', 'SaleCondition', 'SaleType', 'GarageQual', 'Exterior2nd', 'MasVnrType', 'CentralAir', 'FireplaceQu', 'Foundation', 'Exterior1st']\ntrain_df = train_df[categorical_features + discrete_features + [\"SalePrice\" , \"2ndFlrSF\"]]\ntest_df = test_df [categorical_features + discrete_features + ['2ndFlrSF']]","022d32ad":"# Combining Both the dataframes, as any transformation we apply (on non-target columns)\n# must be applied to both train and test.\nntrain = train_df.shape[0]\nntest = test_df.shape[0]\ny_train = train_df[\"SalePrice\"]\ntrain_df.drop(columns = \"SalePrice\", inplace = True)\ndf = pd.concat([train_df, test_df])\nprint(\"Combined Dimensions are {}\".format(df.shape))\nprint(\"y_train dimensions are {}\".format(y_train.shape))\ndf['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']","0620d972":"# Let's check how bad it is...\ntotal = df.isnull().sum().sort_values(ascending=False)\npercentage = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False) * 100\n\nmissing_data = pd.concat([total, percentage], axis=1, keys=[\"Total\", \"% Missing\"])\nmissing_data = missing_data[missing_data[\"Total\"] != 0]\nplt.figure(figsize = (18, 9))\nax = sns.barplot(x = missing_data.index, y = missing_data[\"% Missing\"], palette=\"RdPu_r\")\nax.set_yscale(\"log\")\ntemp = plt.xticks(rotation = 60)","9c22d746":"# replacing na with appropriate values where na doesn't mean \"data not available\"\ndf[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(\"None\") # na means no fireplace","d20cb705":"# replacing values for homes without garages.\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df[col] = df[col].fillna('None')\nfor col in ('GarageArea', 'GarageCars'):\n    df[col] = df[col].fillna(0)","397b0830":"# replacing values for homes without basements.\nfor col in ('BsmtQual','BsmtExposure', 'BsmtFinType1'):\n    df[col] = df[col].fillna('None')\nfor col in ('BsmtFinSF1', 'TotalBsmtSF'):\n    df[col] = df[col].fillna(0)","6e2cc07f":"df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(\"None\")\ndf[\"MasVnrArea\"] = df[\"MasVnrArea\"].fillna(0)","450bd48f":"missing_data = [var for var in df.columns if df[var].isnull().sum() > 0]\nfor var in missing_data:\n    df[var] = df[var].fillna(df[var].mode()[0])","a1f7cd75":"print(\"Missing cells in dataframe : {}\".format(df.isnull().sum().max()))","80b2b1f3":"not_discrete = []\nfor feature in df.columns:\n    if len(df[feature].unique()) < 20 and df[feature].dtype != \"object\":\n        df[feature] = df[feature].astype(str)\n        not_discrete.append(feature)\ndf[\"YearRemodAdd\"] = df[\"YearRemodAdd\"].astype(str)\ndf[\"YearBuilt\"] = df[\"YearBuilt\"].astype(str)","9ab5f42d":"not_discrete = not_discrete + [\"YearRemodAdd\", \"YearBuilt\"]\nfrom sklearn.preprocessing import LabelEncoder\n\ncols = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',\n        'ExterQual', 'ExterCond', 'HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',\n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',\n        'YrSold', 'MoSold']\ncols = tuple(set(cols + not_discrete))\nprint(cols)\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    try:\n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values))\n        df[c] = lbl.transform(list(df[c].values))\n    except:\n        pass\n\n# shape\nprint('Shape all_data: {}'.format(df.shape))","7f970e2a":"sns.distplot(y_train, kde = True, fit = norm, color = \"purple\")","ba768763":"from sklearn import preprocessing\ntargetScaler = preprocessing.StandardScaler()\ny_train = targetScaler.fit_transform(y_train.to_numpy().reshape(-1, 1))\nsns.distplot(y_train, kde = True, fit = norm, color = \"purple\")","90f7ac02":"# Let's find the skew for all the numerical features.\nnumeric = [var for var in df.columns if df[var].dtype != \"object\"]\nskewed = [df[var].skew() for var in numeric]\nskewed = pd.Series(skewed, index=numeric, name=\"Skew\").sort_values(ascending=False)","21f1ec76":"# Filtering out variables whose skew is greater than 0.75 (abs)\nfor (skew, index) in zip(skewed, skewed.index):\n    if abs(skew) < 0.75:\n        skewed.drop(index = index, inplace = True)\nskewed","6afe453a":"# Visualizing distribution.\nplot_no = 0\nplt.figure(figsize=(16, 12))\nfor feature in skewed.index:\n    plt.subplot(3, 4, plot_no + 1)\n    sns.distplot(df[feature], kde = True, fit=norm, color = \"purple\")\n    plot_no += 1\nplt.tight_layout()","d4815b7a":"# applying log transformation to 1 + x.\ndf_temp = df.copy()\nfrom scipy.special import boxcox1p\nlam = 0.15\nfor feat in skewed.index:\n    df_temp[feat] = boxcox1p(df_temp[feat], lam)","76730d94":"sc = StandardScaler()\nsc.fit(df_temp.iloc[:ntrain][numeric])","38113706":"df_temp[numeric] = sc.transform(df_temp[numeric])","311be578":"plot_no = 0\nplt.figure(figsize=(18, 60))\nfor feature in skewed.index:\n    plt.subplot(12, 4, plot_no + 1)\n    sns.distplot(df[feature], kde = True, fit=norm, color = \"purple\")\n    plt.title(\"Before\", fontsize = 20)\n    plt.subplot(12, 4, plot_no + 2)\n    sns.distplot(df_temp[feature], kde = True, fit=norm, color = \"green\")\n    plt.title(\"After\", fontsize = 20)\n    plot_no += 2\n\nplt.tight_layout()","010c8c17":"df = df_temp","cd4fe649":"df = pd.get_dummies(df)","0e9286a9":"df.shape","9ca16c32":"train = df[:ntrain]\ntest = df[ntrain:]\ny = y_train","522a323d":"train.shape","20704aec":"from sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest\nselector = SelectKBest(mutual_info_regression, k=40)\ntrain_new = selector.fit_transform(train, y)\nmask = selector.get_support()\nnew_features = train.columns[mask]\nprint(new_features)","61d49768":"categorical_high_corr = []\nfor ftr in new_features.tolist():\n    if ftr not in discrete_high_corr.index.tolist():\n        categorical_high_corr.append((ftr.split(\"_\")[0]))\nprint(set(categorical_high_corr), len(categorical_high_corr))","05fb2e1d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state=97)","f8e86c2a":"y_test = y_test.reshape(len(y_test))\ny_test = targetScaler.inverse_transform(y_test)","816fd410":"# Calculating R2 and Adjusted R2\ndef rsquared(x, y):\n    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n    return r_value**2\n\ndef adj_rsquared(x, y, p):\n    r = rsquared(x, y)\n    n = len(x)\n    return 1 - (((1 - r) * (n - 1))\/(n - p - 1))\n# Calculating RMSE\ndef mse(x, y):\n    mse_score = 0\n    for actual, pred, in zip(x, y):\n        mse_score += (actual - pred) ** 2\n    return mse_score \/ len(x)\n\n# Calculating MAE\ndef mae(x, y):\n    mae_score = 0\n    for actual, pred in zip(x, y):\n        mae_score += abs(actual - pred)\n    return mae_score \/ len(x)\n\n# Calculating root means squared of log of errors.\ndef rmsle(x, y):\n    rmsle_score = 0\n    for actual, pred in zip(x, y):\n        rmsle_score += (np.log1p(actual) - np.log1p(pred)) ** 2\n    return math.sqrt(rmsle_score \/ len(x))\n","0081098a":"model_results = pd.DataFrame(columns=[\"R2\", \"Adj-R2\", \"RMSE\", \"RMSLE\", \"MAE\"])","2ab5a3b8":"# automatically fit, get predictions and return results.\n# Uses X_train, X_test, y_train, y_test\ndef fit_predict(model, name):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    y_pred = targetScaler.inverse_transform(y_pred).reshape(len(y_pred))\n    predict_data = pd.DataFrame({\"Actual Values\" : y_test, \"Predicted\" : y_pred})\n\n\n    fig = plt.figure(figsize=(9, 9))\n    sns.scatterplot(data = predict_data, x = \"Actual Values\", y = \"Predicted\", color=\"purple\")\n    sns.kdeplot(x=\"Actual Values\", y=\"Predicted\", data = predict_data, levels=5, color=\"purple\", linewidths=1)\n    sns.lineplot(x = [0, 700000], y = [0, 700000], color = \"#0000ff\", linewidth = 3)\n    plt.ylim([0, 700000])\n    plt.xlim([0, 700000])\n    plt.title(name + \"\\n\", fontsize = 20)\n    fig.text(.5, .05, \"Points closer to the blue line are more accurate.\", ha = \"center\")\n\n    model_results.loc[len(model_results)] = [rsquared(y_pred, y_test), adj_rsquared(y_pred, y_test, X_train.shape[1]), math.sqrt(mse(y_pred, y_test)), rmsle(y_pred, y_test), mae(y_pred, y_test)]\n    indexes = model_results.index.tolist()\n    indexes[len(model_results) - 1] = name\n    model_results.index = indexes\n    # return y_pred\n","a1616862":"# Fitting Random Forest Regression to the dataset\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 500, random_state = 0)","8b5aef01":"fit_predict(regressor, \"Random Forest\")","c1359d0b":"model_results.tail(1)","6e107188":"from sklearn import linear_model\nlin_reg = linear_model.LinearRegression()","34131365":"pred = fit_predict(lin_reg, \"Multiple Linear\")","4aafc19a":"model_results.tail(1)","a6d8a202":"# first we have to raise the matrix of features to different powers.\nfrom sklearn import preprocessing\npoly_reg = preprocessing.PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X_train)\nlin_reg2 = linear_model.LinearRegression()\nlin_reg2.fit(X_poly, y_train)","af67c5e4":"y_pred = lin_reg2.predict(poly_reg.fit_transform(X_test))\ny_pred = targetScaler.inverse_transform(y_pred.reshape(len(y_pred)))","6822db28":"predict_data = pd.DataFrame({\"Actual Values\" : y_test, \"Predicted\" : y_pred})","cd182664":"sns.set()\nfig = plt.figure(figsize=(9, 9))\nsns.scatterplot(data = predict_data, x = \"Actual Values\", y = \"Predicted\", color=\"purple\")\nsns.kdeplot(x=\"Actual Values\", y=\"Predicted\", data = predict_data, levels=5, color=\"purple\", linewidths=1)\nsns.lineplot(x = [0, 700000], y = [0, 700000], color = \"#0000ff\", linewidth = 3)\nplt.title('Polynomial Regression' + \"\\n\", fontsize = 20)\nfig.text(.5, .05, \"Points closer to the blue line are more accurate.\", ha = \"center\")\n","eb6f2292":"X_poly.shape","636b0e10":"from sklearn.svm import SVR\nsvr_reg = SVR(kernel='rbf')","d1483864":"fit_predict(svr_reg, \"Support Vector\")","5c564d2c":"model_results.tail(1)","74b58f49":"# Let's introduce cross-validation.\nfrom sklearn.model_selection import KFold, cross_val_score\ndef rmse_cv(model, k_fold):\n    kf = KFold(k_fold, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return (rmse)\n\ndef rmsle_cv(model, k_fold):\n    kf = KFold(k_fold, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmsle= np.sqrt(-cross_val_score(model, train.values, y, scoring=\"neg_mean_squared_log_error\", cv = kf))\n    return (rmsle)","57b83d62":"from sklearn.linear_model import Lasso\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import preprocessing","d1c97d3a":"lasso_reg = make_pipeline(preprocessing.RobustScaler(), Lasso(alpha=0.01, random_state=0))","ab5c02c7":"fit_predict(lasso_reg, \"Lasso\")","84edc9be":"model_results.tail(1)","b6298ec6":"from sklearn.linear_model import ElasticNet\nENet = make_pipeline(preprocessing.RobustScaler(), ElasticNet(alpha=0.01, l1_ratio=.9, random_state=3))","e17a887c":"fit_predict(ENet, \"Elastic Net\")","691a2f62":"model_results.tail(1)","2e7399ce":"from sklearn.ensemble import GradientBoostingRegressor\ngboost_reg = GradientBoostingRegressor(random_state=0)","14075f84":"fit_predict(gboost_reg, \"Gradient Boost\")","4eba01d0":"model_results.tail(1)","b1cde84a":"import xgboost as xgb\nxgb_reg = xgb.XGBRegressor(random_state=0)","aced0e6c":"fit_predict(xgb_reg, \"XGBoost\")","bb2978da":"model_results.tail(1)","62e302ae":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(random_state=0)","269c6f67":"fit_predict(model_lgb, \"LightGBM\")","94d3e968":"model_results.tail(1)","eb4ae535":"from sklearn.linear_model import RANSACRegressor\nransac_reg = RANSACRegressor()","5e6ad4c4":"fit_predict(ransac_reg, \"RANSAC\")","075556bd":"model_results.tail(1)","9c43cfbf":"model_results.sort_values(\"RMSLE\", ascending = True)","7328be5a":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n\n    def predict(self, X):\n        predictions = np.column_stack([\n            targetScaler.inverse_transform(model.predict(X)) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","842b2bcf":"xgb_reg = xgb.XGBRegressor(random_state = 0)\ngboost_reg = GradientBoostingRegressor(random_state = 0)\nrfr_reg = RandomForestRegressor(n_estimators=1000, random_state=0)\nlgb_reg = lgb.LGBMRegressor(random_state = 0)\n\nbagged_model = AveragingModels(models = (rfr_reg, xgb_reg, gboost_reg, lgb_reg))","443dde12":"bagged_model.fit(X_train, y_train)\ny_pred = bagged_model.predict(X_test)","7bdd0396":"predict_data = pd.DataFrame({\"Actual Values\" : y_test, \"Predicted\" : y_pred})\n\nfig = plt.figure(figsize=(9, 9))\nsns.scatterplot(data = predict_data, x = \"Actual Values\", y = \"Predicted\", color=\"purple\")\n# sns.histplot(x = \"Actual Values\", y = \"Predicted\", data  = predict_data , cmap = \"RdPu\", thresh= 0, pthresh= 0, cbar = True)\nsns.kdeplot(x=\"Actual Values\", y=\"Predicted\", data = predict_data, levels=5, color=\"purple\", linewidths=1)\nsns.lineplot(x = [0, 700000], y = [0, 700000], color = \"#0000ff\", linewidth = 3)\nplt.ylim([0, 700000])\nplt.xlim([0, 700000])\nplt.title(\"Average Ensemble Results\\n\", fontsize = 20)\nfig.text(.5, .05, \"Points closer to the blue line are more accurate.\", ha = \"center\")","522edc0b":"model_results.loc[len(model_results)] = [rsquared(y_pred, y_test), adj_rsquared(y_pred, y_test, X_train.shape[1]), math.sqrt(mse(y_pred, y_test)), rmsle(y_pred, y_test), mae(y_pred, y_test)]\nindexes = model_results.index.tolist()\nindexes[len(model_results) - 1] = \"Average Ensemble\"\nmodel_results.index = indexes\nmodel_results.tail(1)","d2730c35":"model_results.drop(index = \"RANSAC\", inplace = True)","89968c22":"model_results.sort_values(\"MAE\", ascending=True)","280406ad":"plt.figure(figsize=(16, 16))\nplt.subplot(2, 2, 1)\nplt.title(\"Root Mean Square Error\", fontsize=18)\nrmseplot = sns.barplot(y=model_results[\"RMSE\"].sort_values(ascending=True),\n                       x=model_results[\"RMSE\"].sort_values(ascending=True).index, palette = \"RdPu_r\",  errcolor=\".2\", edgecolor=\".2\")\nplt.xticks(rotation=75)\nplt.subplot(2, 2, 2)\nplt.title(\"Mean Square Error\", fontsize=18)\nmaeplot = sns.barplot(y=model_results[\"MAE\"].sort_values(ascending=True),\n                      x=model_results[\"MAE\"].sort_values(ascending=True).index, palette = \"RdPu_r\",  errcolor=\".2\", edgecolor=\".2\")\nplt.xticks(rotation=75)\nplt.subplot(2, 2, 3)\nplt.title(\"R Squared\", fontsize=18)\nmaeplot = sns.barplot(y=model_results[\"R2\"].sort_values(ascending=False),\n                      x=model_results[\"R2\"].sort_values(ascending=False).index, palette = \"RdPu_r\",  errcolor=\".2\", edgecolor=\".2\")\nplt.xticks(rotation=75)\nplt.subplot(2, 2, 4)\nplt.title(\"Root Mean Square of Log Error\", fontsize=18)\nmaeplot = sns.barplot(y=model_results[\"RMSLE\"].sort_values(ascending=True),\n                      x=model_results[\"RMSLE\"].sort_values(ascending=True).index, palette = \"RdPu_r\",  errcolor=\".2\", edgecolor=\".2\")\nplt.xticks(rotation=75)\nplt.tight_layout()\nplt.show()","1f7066cb":"xgb_reg = xgb.XGBRegressor(random_state = 0)\ngboost_reg = GradientBoostingRegressor(random_state = 0)\nrfr_reg = RandomForestRegressor(n_estimators=1000, random_state=0)\nlgb_reg = lgb.LGBMRegressor(random_state = 0)\n\nbagged_model = AveragingModels(models = (rfr_reg, xgb_reg, gboost_reg, lgb_reg))","11412209":"bagged_model.fit(train, y)","fd57c403":"test_prediction = bagged_model.predict(test)","2d9e4eab":"solution = pd.concat([ids, pd.Series(test_prediction)], axis=1, ignore_index=True)","55aacb12":"solution.columns = [\"Id\", \"SalePrice\"]","63544b66":"solution.to_csv(\"solution.csv\", index=False)","f8ac7142":"### 4. **C \u2192 T** (Categorical vs Discrete Analysis)","872e0336":"### 4. Support Vector Regression","7a1bcfbc":"### Initial Dataset Analysis","61ae21cb":"## B) Advanced Modelling\n","c5eb98b5":"# IV. Missing Data\n<hr>","4d046f9b":"### 3. Polynomial Regression","8beed7cd":"# I. Getting Started\n<hr>","4f38c342":"### 9. LightGBM Regression","f309c686":"<hr>\n<h1><font size = \"+6\"> <b>House Price Predictor<\/b><\/font><\/h1>\n<hr>","9569ff21":"# II. Exploratory Data Analysis\n<hr>","7019c69b":"### 7. Gradient Boost","6cef4a83":"# VII. Modelling\n<hr>\nLet's start with basic models like Linear Regression, SVR etc, then move on to their respective improvements like Lasso Regression and finally to Ensemble Learning.\n","ce6d445e":"<font size=\"+2\"> <i> Basic regression techinques are resulting in large RMSLE and adj. R-squared errors.\nLet's move on to ensemble learning. <\/i><\/font>\n<hr>\n<font size=\"+1\"> <b>Conclusion<\/b> : Best accuracy achieved with Support Vector Regression and Random Forest Regression. <\/font>","39b587ed":"# III. Feature Selection\n<hr>","d8dd32a4":"### 1. Analyzing Sales Price - target variable","e1ff1e3c":"### 8. XGBoost Regression","db2d0218":"Now that we have all the skewed features, I chose to apply log1p transformation.\nlog1p adds 1 to all values and then applies log. This is to prevent log(0) errors.","d7692267":"## A) Basic Modelling","bbc2c92a":"### 2. Categorical Features\nI'm not finding these features right now. Since Mutual Information needs all the categorical features to be OneHotEncoded, I have calculated these values later in down in the notebook, after dealing with Missing Values. After doing that I am removing thet columns here.","c1ce6ac2":"- **Step 2** : Dealing with all garage features.","1796d97f":"#### Distribution of Categorical Features","653d460f":"- **Step 3** : Dealing with all basement features.","a58ce492":"<h3> 5. Lasso Regression <\/h3>","5a3554f4":"### 2. Examining Relationships with SalePrice","2a0b6c92":"### Heatmap Analysis\n- GrLivArea and TotRmsAbvGrd have a strong +ve correlation of 0.83. We can safely drop TotRmsAbvGrd.\n- GarageYrBlt and YearBuilt have a strong +ve correlation of 0.83. We can safely drop GarageYrBlt.","88b7f3e2":"A distinct upward trend in SalePrice is observed as expected.","9dd98c13":"It looks like apart from the strong +ve correlations with SalePrice, there are no other relations. <br> This is awesome!","3cc47bc2":"A positive correlation can is visible, but there are **too** many outliers everywhere. Using this can be risky...","9b30481f":"2 Outliers present!","5aafa4a1":"### Bagging (Bootstrap Aggregation)\nReturns mean prediction of all given models.","87d68ceb":"#### Correlation with SalePrice","daa81a23":"### 2. Multiple Linear Regression","25ef05a7":"<font size=\"+1\"> Lasso Regression behaves exactly like Linear Regression because alpha = 0. <\/font>","5f6d4756":"- Lasso Regression is an improvement over Multiple Linear Regression.\n- It tried to reduce overfitting and increase generalization.\n- The value of alpha determines the level of generalization. At alpha = 0, Lasso Regression behaves exactly like Multiple Linear Regression.","990c61a0":"- **Step 4** : Replacing discrete values with median.<br> **Note** : By replacing with median of the entire dataset (test + train), data leakage occurs. But that is ultimately better for accuracy in competition.","cf0bfda9":"#### Pre-defined functions for calculating metrics easily.\nIgnore below code cell.","00887057":"#### Combining Test and Train Dataset.\nThis will help deal with missing data efficiently, in both test and train data.","badc19e4":"#### Distribution of top correlated features","c56b7aba":"- **Step 1** : Data description says that with these columns \"na\" actually means that the given feature is not present in the house. <br> This means that the data is not missing, the feature is.","a243d09b":"Let's start with SalePrice","279f6550":"# V. Normalizing and Standardizing Variables\n<hr>","9a08fe90":"# VI. Final Changes\n<hr>","014467fa":"### 10. RANSAC Regression","e981faed":"Let's define cross validation functions as it will be necessary from here on.","5c554196":"<font size = \"+1\"> Results are best at alpha = 0.01. Used Binary Search principle to arrive at this value <\/font>","fe93369c":"# VIII. Results\n<hr>","4c534125":"These are actually categorical classes but disguised as discrete numbers.<br> Let's first convert them into categorical values, and then use LabelEncoder on them to prevent an inconsistencies.","05e1b412":"## C) Ensemble Learning\nFirst, let's take a look at all the performance of all the models tested.","8d54e390":"Top 3 models are Gradient Boost, Random Forest, and Lasso Regression.","61e3926a":"### Exporting Predictions","a8740f9a":"### 3. **D \u2192 Target** (Discrete vs Target analysis)","c45368d7":"### 6. Elastic-Net Regression\nCombines both ridge and lasso regression.","5424c83d":"<font size=\"+1\"> Polynomial Regression is not suited for datasets with such high column count.\nI'm actually not sure what happened here, because the number of columns in X_poly can be calculated by :\n<\/font><br>\n<center> features_poly = degree ** features_linear <\/center>\n<font size=\"+1\"> <br>\nwhich in this case = 2 ** (200 +) which is basically infinity. <br>\nNot even worth finding accuracy metrics.\n<\/font>","013f7cb3":"**Function to rapidly train model and get results**","133a416c":"These are actually categorical classes but disguised as discrete numbers. Let\u2019s first convert them into categorical values, and then use LabelEncoder on them to prevent an inconsistencies.","6595f66b":"### 1. Random Forest Regressor","b7431e95":"### 1. Discrete Features","acee2be5":"##### Pre-defined function template\nIgnore below code cell !","14c1c9b3":"1 Outlier present.","dacd7d05":"#### Creating Dummy Variables","2be204e2":"Some predictions are very inaccurate. ","f3e7ecca":"### Feature Selection with Categorical Variables"}}