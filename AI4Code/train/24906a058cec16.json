{"cell_type":{"161405ff":"code","6922c45c":"markdown","e38c0d94":"markdown","3f82df59":"markdown","0546c3f5":"markdown","b3e0e958":"markdown","722f3d93":"markdown","be2302c8":"markdown","0bde51db":"markdown","e68e3917":"markdown","5c32f29a":"markdown","f8436b20":"markdown","7cf7f7ce":"markdown","38aa8f40":"markdown","c8da43c5":"markdown","c7bd13bf":"markdown","af9f0a0d":"markdown","cf8cf55a":"markdown","8f874333":"markdown","aac80156":"markdown","e55d676d":"markdown"},"source":{"161405ff":"##Categorizing the season column using Dummy Vars : the reason is because there is no Hierarchy..\n#meaning that, \"Fall IS NOT Higher or Better than Summer\"\n\ndef data_prep(df_clean):\n    \n    def parse_time(x):\n        DD=datetime.strptime(x,\"%m\/%d\/%y %H:%M\")\n        time=DD.hour \n        day=DD.day\n        month=DD.month\n        year=DD.year\n        mins=DD.minute\n        return time,day,month,year,mins\n    \n    \n    \n    parsed = np.array([parse_time(x) for x in df_clean.Dates])\n    \n    df_clean['Dates'] = pd.to_datetime(df_clean['Dates'])\n    df_clean['WeekOfYear'] = df_clean['Dates'].dt.weekofyear\n    #df_clean['n_days'] = (df_clean['Dates'] - df_clean['Dates'].min()).apply(lambda x: x.days)\n    df_clean['HOUR'] = parsed[:,[0]]\n    df_clean['day'] = parsed[:,[1]]\n    df_clean['month'] = parsed[:,[2]]\n    df_clean['year'] = parsed[:,[3]]\n    df_clean['mins'] = parsed[:,[4]]\n    \n    \n    #adding season variable\n    def get_season(x):\n        if x in [5, 6, 7]:\n            r = 'summer'\n        elif x in [8, 9, 10]:\n            r = 'fall'\n        elif x in [11, 12, 1]:\n            r = 'winter'\n        elif x in [2, 3, 4]:\n            r = 'spring'\n        return r\n    \n    df_clean['season'] = [get_season(i) for i in df_clean.month] \n    \n    \n    df_clean['Block'] = df_clean['Address'].str.contains('block', case=False)\n    df_clean['Block'] = df_clean['Block'].map(lambda x: 1 if  x == True else 0)\n    \n    #creating dummy variables\n    df_clean_onehot = pd.get_dummies(df_clean, columns=['season'], prefix = [''])\n    s = (len(list(df_clean_onehot.columns))-len(df_clean.season.value_counts()))\n    df_clean = pd.concat([df_clean,df_clean_onehot.iloc[:,s:]], axis=1)\n\n    ##Categorizing the DayOFWeek column using Dummy Vars \n    df_clean_onehot = pd.get_dummies(df_clean, columns=['DayOfWeek'], prefix = [''])\n    \n    l = (len(list(df_clean_onehot.columns))-len(df_clean.DayOfWeek.value_counts()))\n    df_clean = pd.concat([df_clean,df_clean_onehot.iloc[:,l:]],axis=1)\n\n    ##Categorizing the MONTH column using Dummy Vars : the reason is because there is no Hierarchy..\n    #meaning that, \"FEB IS NOT Higher or Better than JAN\"\n    #This insight was shown from the EDA result (forecasting data with trend might be a different case)\n\n    df_clean_onehot = pd.get_dummies(df_clean, columns=['month'], prefix = ['month'])\n    n = (len(list(df_clean_onehot.columns))-len(df_clean.month.value_counts()))\n    df_clean = pd.concat([df_clean,df_clean_onehot.iloc[:,n:]],axis=1)\n\n    ##Categorizing the District column using Dummy Vars \n    df_clean_onehot = pd.get_dummies(df_clean, columns=['PdDistrict'], prefix = [''])\n    o = (len(list(df_clean_onehot.columns))-len(df_clean.PdDistrict.value_counts()))\n    df_clean = pd.concat([df_clean,df_clean_onehot.iloc[:,o:]],axis=1)\n    \n    df_clean['IsInterection']=df_clean['Address'].apply(lambda x: 1 if \"\/\" in x else 0)\n    df_clean['Awake']=df_clean['HOUR'].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n    \n    ##changing the Output Variables to integer\n    labels = df_clean['Category'].astype('category').cat.categories.tolist()\n    replace_with_int = {'Category' : {k: v for k,v in zip(labels,list(range(0,len(labels))))}}\n    df_clean.replace(replace_with_int, inplace=True)\n    \n    #Normalizing the columns\n    def norm_func(i):\n        r = (i-min(i))\/(max(i)-min(i))\n        return(r)\n\n    df_clean['normHOUR']=norm_func(df_clean.HOUR)\n    df_clean['normmins']=norm_func(df_clean.mins)\n    df_clean['normdate_day']=norm_func(df_clean.day)\n    df_clean['normLat']=norm_func(df_clean.X)\n    df_clean['normLong']=norm_func(df_clean.Y)\n    df_clean['normmonth']=norm_func(df_clean.month)\n    df_clean['normyear']=norm_func(df_clean.year)\n    df_clean['normWeekOfYear']=norm_func(df_clean.WeekOfYear)\n    #df_clean['normNDAYS']=norm_func(df_clean.n_days)\n    \n\n\n    ##removing the unused columns\n    df_clean.drop(columns = ['Dates','season','HOUR','day','X','Y'\n                             ,'DayOfWeek','Address','PdDistrict','mins','month','year','WeekOfYear','Resolution'], axis = 1,inplace=True)\n                             #'Count_rec_x','Count_rec_y'], axis = 1,inplace=True)\n    return(df_clean)","6922c45c":"![image.png](attachment:image.png)","e38c0d94":"<h1>Predicting Crime in San Francisco Using ANN and Keras<\/h1>\n=========\n<br>\n<h2> 1. Introduction<\/h2>\n---------------------------------\n<p>In this project, I predicted the category of crimes that happened in San Fransisco based on different variables such as the latitude, longitude, date and time. This resulted in 2.51353 kaggle score (or top 33%).<\/p>\n\n<p>Pandas is used for data manipulation. Numpy is the fundamental package for scientific computation in Python. \nNeural Networks is the classification algorithm used to make the final predictions. Seaborn is a nice tool for data visualisation built on top of matplotlib. \nThe import code is as follows:<\/p>\n\n\n<h3>Libraries I used :<\/h3>\n<p>import pandas as pd<br>\nimport numpy as np<br>\nimport matplotlib.pyplot as plt<br>\nimport tensorflow as tf<br>\nfrom tensorflow import keras<br>\nfrom datetime import datetime<br>\nfrom sklearn.model_selection import train_test_split<br>\nfrom sklearn.model_selection import GridSearchCV<br>\nfrom sklearn.tree import DecisionTreeClassifier<br>\nfrom sklearn import preprocessing<br>\nfrom sklearn.metrics import log_loss<br>\nfrom sklearn.metrics import make_scorer<br>\nfrom sklearn.model_selection import StratifiedShuffleSplit<br>\nfrom matplotlib.colors import LogNorm<br>\nfrom sklearn.decomposition import PCA<br>\nfrom keras.layers.advanced_activations import PReLU<br>\nfrom keras.layers.core import Dense, Dropout, Activation<br>\nfrom keras.layers.normalization import BatchNormalization<br>\nfrom keras.models import Sequential<br>\nfrom keras.utils import np_utils<br>\nfrom copy import deepcopy<br>\n%matplotlib inline<p>\n\n","3f82df59":"![image.png](attachment:image.png)","0546c3f5":"![image.png](attachment:image.png)","b3e0e958":"![image.png](attachment:image.png)","722f3d93":"predictiondata.to_csv('...\/SFprediction_dataSF.csv'\n                      ,encoding='utf-8', index=True)","be2302c8":"## Step 2 : Data Cleaning\n\nI decided to combine both the train and test files together so that it is easier when I create the one-hot dummy variables. <br>\nI created a function to parse the datetime variables, create dummy variables for categorical data, and normalize all the continuous values. <br>\nOnce all the data have been cleaned and organized in the same format, I splitted both the train and test data.","0bde51db":"## Checking Test Data","e68e3917":"train_clean.head()","5c32f29a":"![image.png](attachment:image.png)","f8436b20":"![image.png](attachment:image.png)![](http:\/\/)","7cf7f7ce":"combined = data_prep(combined)\n\ntrain_clean = combined[combined.Train == 1] <br>\ntrain_clean.drop(['Train','Test'], axis=1,inplace = True)","38aa8f40":"![image.png](attachment:image.png)","c8da43c5":"![image.png](attachment:image.png)","c7bd13bf":"![image.png](attachment:image.png)","af9f0a0d":"## Step 1 : EDA\nDuring the EDA stage, I plotted the dataset to identify any trends\/correlations and then identify outliers. \nI used the learnings from EDA to decide how I will treat my independent variables, clean the data, and to build some feature engineering and put them in my model.<br>\n\nBased on the EDA, below are the key takeaways: <br>\n    1. There was no ordinal trend between month and number of reported crime. \n       Therefore, I will treat the month variable as a categorical variable and create dummy variable.\n    2. There was no ordinal trend between day of week and number of reported crime as well. Therefore, I will create create dummy variable.\n    3. There were outlier Longitude and I will be removing that from my train dataset.\n    4. There were a lot of crimes happening in the same address at the same district.","cf8cf55a":"## Data Cleaning and Feature Engineering","8f874333":"predictiondata.head()","aac80156":"## Checking Train Data","e55d676d":"## Training the neural network model"}}