{"cell_type":{"84b3c2a5":"code","808938ad":"code","e5b6d391":"code","3f4afb67":"code","6c3622cb":"code","aebbbb79":"code","2655fcb8":"code","4c18aaaa":"code","27a4053a":"code","48ec711d":"code","f51d8e5b":"code","a6985d24":"code","be8a8eb1":"code","995e2360":"code","341e3018":"code","a6aac806":"code","fbcd419d":"code","4ff96a91":"code","4e30f071":"markdown","5b6dbec6":"markdown","8654bfc1":"markdown","1a868628":"markdown","6a8711d4":"markdown","96985038":"markdown","19d844fa":"markdown","7fa5a533":"markdown","94e7ac6c":"markdown","c27b6552":"markdown","b480582b":"markdown","99e1e8b0":"markdown","afb0d667":"markdown","214a7b05":"markdown","78fd2161":"markdown","5f43b711":"markdown","271ba920":"markdown"},"source":{"84b3c2a5":"\nimport numpy as np # linear algebra\nimport numpy.polynomial.polynomial as npPol\n\nimport pandas as pd # \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n","808938ad":"#1. load the data as a pandas dataframe\nGDP_BB = pd.read_csv('..\/input\/gdp-per-capita-and-internet-usage-in-the-world\/correlation-between-internet-users-as-a-share-of-the-population-and-gdp-per-capita.csv')\n\n#2. select year 2014, \nGDP_BB_2014 = GDP_BB[ GDP_BB['Year']==2014 ]\n\n#3. Create a dataframe with no missing continent entries, and set index to 'Entity' to facilitate the combine operation. Drop all columns but 'Continent',\ncolumn_names = list(GDP_BB_2014.columns)\ncol_to_drop = [ x  for x in column_names if x not in ['Continent', 'Entity'] ]\n\ncontinent_df = GDP_BB.dropna( subset = ['Continent'] ).drop(columns=col_to_drop).set_index('Entity')\n\n#4. Combine the dataframes to fill missing Continent values. Then, drop missing rows of data with NaN entries in the columns of interest\nGDP_BB_2014 = GDP_BB_2014.set_index('Entity').combine_first( continent_df).reset_index().dropna( subset=['Percentage of Individuals using the Internet (ICT) (2015)', 'GDP per capita, PPP (constant 2011 international $)'] )\n\n#4.1. Set continent as categorical. Helps with faceting in seaborn and plotly\nGDP_BB_2014['Continent']=GDP_BB_2014['Continent'].astype('category')\n\n#5. Rename columns to something more manageable\nnew_col_names={'Entity': 'COUNTRY','Code': 'CODE', 'Continent': 'CONTINENT','GDP per capita, PPP (constant 2011 international $)':'GDP_PC',\\\n               'Percentage of Individuals using the Internet (ICT) (2015)': 'PCT_INTERNET_USERS','Total population (Gapminder, HYDE & UN)': 'POP', 'Year':'YEAR'}\nGDP_BB_2014.rename(columns=new_col_names, inplace=True)\n\n#6. view results of all operations\ndisplay(GDP_BB_2014.head(6))\nprint(GDP_BB_2014.info())\n","e5b6d391":"#use describe() to view the basic statistics of the data. We are mostly interested in GDP PC spread\ndisplay(GDP_BB_2014.describe())","3f4afb67":"#use plotly express for a quick boxplot\nbox_fig = px.box(data_frame=GDP_BB_2014, y='GDP_PC', points='all', hover_data=['COUNTRY'],title=\"Box plot of the GDP per-capita observations\")\nbox_fig.add_annotation(x=0, y=100000,\n            text=\"Outliers are data points above the upper fence line\",\n            showarrow=False,\n            yshift=10)\nbox_fig.show()","6c3622cb":"# Add the two transformed columns\nGDP_BB_2014['LOG_GDP'] = np.log10(GDP_BB_2014['GDP_PC'])\nGDP_BB_2014['LOG_PCT_USERS'] = np.log10(GDP_BB_2014['PCT_INTERNET_USERS'])\n\n#compute the correlation coefficient for the original and transformed data columns\ncorr_matrix = GDP_BB_2014[['GDP_PC','PCT_INTERNET_USERS','LOG_GDP', 'LOG_PCT_USERS']].corr()\nfig=go.Figure(go.Heatmap(x=corr_matrix.columns, y=corr_matrix.columns,z=corr_matrix.values.tolist(),colorscale='rdylgn',zmin=0,zmax=1))\nfig.update_layout(height=300, width=800, title_text='GDP per-capita and percentage of internet users: correlation heatmap')\nfig.show()\ncorr_matrix.style.background_gradient()\n\n","aebbbb79":"#Prepare to plot the figures with plotly. We add extra data fields to use as part of the text displayed on hover over each data point.\nhover_text = []\nbubble_size = []\n\nfor index, row in GDP_BB_2014.iterrows():\n    hover_text.append(('Country: {country}<br>'+\n                      'GDP per capita: {gdp}K<br>'+\n                      'Population: {pop}').format(country=row['COUNTRY'],\n                                            gdp=np.round(row['GDP_PC']\/1000,1),\n                                            pop=str(np.round(row['POP']\/1E6,1)) + ' M'))\n    bubble_size.append(np.sqrt(row['POP']))\n\n#GDP_BB_2014['TEXT'] = hover_text\nGDP_BB_2014['SIZE'] = bubble_size\nsizeref = 2.*max(GDP_BB_2014['SIZE'])\/(100**2)","2655fcb8":"#Use plotly to display the relationship between our two variables via scatter plots. The size of the markes is based on the country population, but using a square root transformation\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Scatter(x=GDP_BB_2014['PCT_INTERNET_USERS'], y=GDP_BB_2014['GDP_PC'],mode=\"markers\",marker_size=GDP_BB_2014['SIZE'],name='GDP per capita', text=hover_text),\n    row=1, col=1)\n\nfig.add_trace(\n    go.Scatter(x=GDP_BB_2014['PCT_INTERNET_USERS'], y=GDP_BB_2014['LOG_GDP'],mode=\"markers\",marker_size=GDP_BB_2014['SIZE'],name='Log of GDP per capita',text=hover_text),\n    row=1, col=2)\n\n#customize no the two traces created above.\nfig.update_traces( marker=dict(sizemode='area',sizeref=sizeref, line_width=2))\nfig.update_layout(height=800, width=1600, title_text='GDP per-capita versus percentage of internet users')\n# Update xaxis properties\nfig.update_xaxes(title_text=\"Percentage of Internet users\", row=1, col=1)\nfig.update_xaxes(title_text=\"Percentage of Internet users\", row=1, col=2)\n# Update yaxis properties\nfig.update_yaxes(title_text=\"GDP per capita\", row=1, col=1)\nfig.update_yaxes(title_text=\"Log of GDP per capita\", row=1, col=2)\nfig.show()\n\n","4c18aaaa":"#Uncomment this code for a seaborn approach to the scatter figures\n\n#f, ax = plt.subplots(1, 2, figsize=(15, 7.5))\n#f.tight_layout(pad=3.5)\n\n#sns.set_style('whitegrid')\n#sns.set_context('talk')\n\n#sns.regplot(x='PCT_INTERNET_USERS', y='GDP_PC', data=GDP_BB_2014, ax=ax[0])\n#sns.regplot(x='PCT_INTERNET_USERS', y='LOG_GDP', data=GDP_BB_2014,ax=ax[1],color='r')\n\n#plt.show()\n\n#Uncomment this code for a plotly express set of figures\n\n\n#px_scatter_1=px.scatter(data_frame=GDP_BB_2014, title=\"GDP per-capita versus percentage of internet users\",\n#    x='PCT_INTERNET_USERS', y='GDP_PC',size=\"POP\", trendline='ols',width=600, height=600,)\n#px_scatter_1.show()\n#px_scatter_2=px.scatter(data_frame=GDP_BB_2014, title=\"GDP per-capita versus percentage of internet users\",\n#    x='PCT_INTERNET_USERS', y='LOG_GDP',size=\"POP\", trendline='ols',width=600, height=600,)\n#px_scatter_2.show()","27a4053a":"#Filter out outliers in the GDP_PC column using the IQR threshold\nQ_1 = GDP_BB_2014['GDP_PC'].quantile(0.25)\nQ_3 = GDP_BB_2014['GDP_PC'].quantile(0.75)\nIQR = Q_3 - Q_1\n\nGDP_BB_2014_outlier_removed = GDP_BB_2014.query( '(@Q_1 - 1.5 * @IQR) <= GDP_PC <= (@Q_3 + 1.5 * @IQR)' )\n\n#use plotly express for a quick boxplot\nbox_fig2 = px.box(data_frame=GDP_BB_2014_outlier_removed, y='GDP_PC', points='all', hover_data=['COUNTRY'],title=\"Box plot of the GDP per-capita, without outliers\")\nbox_fig2.show()","48ec711d":"# obtain the slope and intercept coefficientes using numpy: method 1\n\nslope, intercept = npPol.polyfit(GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'], GDP_BB_2014_outlier_removed['LOG_GDP'],1)\n\nprint(f'The slope of the line is {np.round(slope,2)}, and the intercept is {np.round(intercept,2)}')","f51d8e5b":"predicted_y = npPol.polyval( 70,[slope, intercept],)\nprint(f'The predicted value of log(GDP per-capita) is {np.round(predicted_y,2)} and the predicted GPD per-capita is {np.round(10**(predicted_y),2)}')","a6985d24":"from scipy.optimize import curve_fit\nfrom scipy.stats import linregress\n\n#case 1: use optimize.curvefit\ndef model_to_optimize(x, a0, a1):\n    return a0 + (x*a1)\n\nparam_opt, _ = curve_fit(model_to_optimize, GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'], GDP_BB_2014_outlier_removed['LOG_GDP'])\n\ncurvefit_intercept = param_opt[0] # a0 is the intercept in y = a0 + a1*x\ncurvefit_slope = param_opt[1] # a1 is the slope in y = a0 + a1*x\n\nprint(f'CURVEFIT: The slope of the line is {np.round(curvefit_slope,2)}, and the intercept is {np.round(curvefit_intercept,2)}')\n\n#case 2: use linregress\nresults = linregress(GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'], GDP_BB_2014_outlier_removed['LOG_GDP'])\n\nlinregress_intercept = results.intercept\nlinregress_slope = results.slope\n\nprint(f'LINREGRESS: The slope of the line is {np.round(linregress_slope,2)}, and the intercept is {np.round(linregress_intercept,2)}')\n","be8a8eb1":"print(f'The predicted value of log(GDP per-capita) is {np.round(model_to_optimize(70, curvefit_intercept, curvefit_slope),2)} \\\n        and the predicted GPD per-capita is {np.round(10**(model_to_optimize(70, curvefit_intercept, curvefit_slope)),2)}')","995e2360":"#compute RSS for the model.\nRSS =  np.sum(np.square(model_to_optimize(GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'], curvefit_intercept, curvefit_slope) - GDP_BB_2014_outlier_removed['LOG_GDP']))\n\nprint(f'The sum of the squares of the residuals is {np.round(RSS,2)} and the coefficient of correlation for the SCIPY.STATS regression is {np.round(results.rvalue,2)}')\n\n","341e3018":"from sklearn.linear_model import LinearRegression\n\n#1. instantiate the model\nreg = LinearRegression()\n\nold_x_shape=GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'].values.shape\n\n#2. reshape the data to adapt it to what LinearRegression requires, a single column vector\nX_data=GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'].values.reshape(-1,1)\ny_data=GDP_BB_2014_outlier_removed['LOG_GDP'].values.reshape(-1,1)\nprint(f'Shape of X_data {X_data.shape} (while the shape of the series was {old_x_shape}), and of y_data {y_data.shape}')\n\n#3. fit a model\nreg.fit(X_data,y_data)\n\n#now view the model's coefficients\nprint(f'Coefficients: slope {np.float(np.round(reg.coef_,2))} and intercept { np.float(np.round(reg.intercept_,2))} ')\nprint(f'The coefficient of determination (R2) of the model, is {np.round(reg.score(X_data,y_data),2)}')\n\n#4. Predict using the model\nx_wanted = np.array([70]).reshape(-1,1)\ny_wanted=reg.predict(x_wanted)\nprint(f'The predicted value of Log(GPD p-c) for x={float(x_wanted)} is {np.round(float(y_wanted),2)}, for a GDP value of {np.round(10**(float(y_wanted)))}')\n","a6aac806":"#Make a prediction for the extremes of the data set,\nX_min=GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'].values.min()\nX_max=GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'].values.max()\n\ny_min = 3.32 + 0.02*X_min\ny_max = 3.32 + 0.02*X_max\n\n#predict using the second model, using 70% of the data\ny_min_2 = 3.36+ 0.01* X_min\ny_max_2 = 3.36+ 0.01* X_max\n\n#Display the scatter and regression lines together, on each plot.\nfig_2 = make_subplots(rows=1, cols=1)\n\nfig_2.add_trace(\n    go.Scatter(x=GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'], y=GDP_BB_2014_outlier_removed['LOG_GDP'],mode=\"markers\",marker_size=GDP_BB_2014_outlier_removed['SIZE'],name='Log of GDP per capita',text=hover_text),\n    row=1, col=1)\n\nfig_2.add_trace(\n    go.Scatter(x=np.array([X_min,X_max]), y=np.array([y_min,y_max]),mode=\"lines+markers\",name='model 1 (log units)'),row=1, col=1)\n\nfig_2.add_trace(\n    go.Scatter(x=np.array([X_min,X_max]), y=np.array([y_min_2,y_max_2]),mode=\"lines+markers\",name='model 2 (log units)', line={'dash': 'dash'}),row=1, col=1)\n\n\n#customize the traces created above.\nfig_2.update_traces( marker=dict(sizemode='area',sizeref=sizeref, line_width=2))\nfig_2.update_layout(height=800, width=1600, title_text='GDP per-capita versus percentage of internet users')\n# Update xaxis properties\nfig_2.update_xaxes(title_text=\"Percentage of Internet users\", row=1, col=1)\n# Update yaxis properties\nfig_2.update_yaxes(title_text=\"Log of GDP per capita\", row=1, col=1)\nfig_2.show()","fbcd419d":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\n#1. instantiate the model\nreg_2 = LinearRegression()\n\n#2. reshape the data to adapt it to what LinearRegression requires, a single column vector\nX_data=GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'].values.reshape(-1,1)\ny_data=GDP_BB_2014_outlier_removed['LOG_GDP'].values.reshape(-1,1)\n\n#3 split the data in to test, train sets: 70% train, 30% test\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3, random_state=12)\n\n#3. fit a model\nreg_2.fit(X_train,y_train)\n\n#now view the model's coefficients\nprint(f'The coefficients determined from the test set are: slope {np.float(np.round(reg_2.coef_,2))} and intercept { np.float(np.round(reg_2.intercept_,2))} ')\nprint(f'A difference in slope of {np.float(reg_2.coef_)-np.float(reg.coef_)}, and in intercept of {np.float(reg_2.coef_)-np.float(reg.coef_)}')\n\n#Score it on the test data\nprint(f'The coefficient of determination (R2) of the model, on the test data, is {np.round(reg_2.score(X_test,y_test),2)}')\n\n#4. Predict using the model\nX_wanted = np.array([70]).reshape(-1,1)\ny_wanted=reg_2.predict(X_wanted)\n\nprint(f'The predicted value of Log(GPD p-c) for x={float(X_wanted)} is {np.round(float(y_wanted),2)}, for a GDP value of {np.round(10**(float(y_wanted)))}')","4ff96a91":"#Prepare a plot using seaborn\nf, ax = plt.subplots(1, 1, figsize=(15, 7.5))\nf.tight_layout(pad=3.5)\n\nsns.set_style('whitegrid')\nsns.set_context('talk')\n\nsns.regplot(x='PCT_INTERNET_USERS', y='LOG_GDP', data=GDP_BB_2014_outlier_removed,ax=ax,color='r')\nax.grid(True)\n\nfgrid = sns.lmplot(x='PCT_INTERNET_USERS', y='LOG_GDP', data=GDP_BB_2014_outlier_removed, fit_reg=True, height=7, aspect=2)\n\nplt.show()","4e30f071":"\n### 5.2 Building a linear model in Python : Scipy Optimize and STATS\nThe SCIPY package includes routines to optimize a function, by minimizing the value of an error metric.\nThe [optimize][1] class contains methods for curve fitting that will determine the model parameters by a least-squares approach, that is, minimizing the sum of squares of the residuals.\nOptimize.curve_fit will return the optimal parameters in the least-squares sense, of the model we pass to it. We thus have to define a function to optimize.\n\n    def model_to_optimize(x, a0, a1):\n        return a0+(x*a1)\n\nThe call to curve fit is then :\n\n    curve_fit(model_to_optimize, X_data, y_data )\n\n\n**SCIPY STATS**\n\nAdditionally, SCIPY's STATS module contains a method for linear regression, **[linregress][2]**. Linregress by definition returns the optimal parameters of a linear model, so there is no need to feed it a model function.\nthe call is\n    \n    model_output = linregress(X_data, y_data)\n    \nLet's try both approaches in the cell below.\n\n[1]: https:\/\/docs.scipy.org\/doc\/scipy\/reference\/optimize.html?highlight=optimize#module-scipy.optimize\n[2]: https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.linregress.html#scipy.stats.linregress\n","5b6dbec6":"## Some observations on linear regression\n\nSo far we have just discussed how to use certain Python packages to perform a linear regression, but I have not discussed some key aspects of the modelling process and that could result in a model that either overfits the data or is simply inadequate to make predictions outside of the observed domain.\n\nIn some cases, it will be necessary to transform the data to either center it, or to adjust its dynamic range so that the X and Y values are comparable in magnitude. We have not done that in our exercise, but the packages we have reviewed (like scipy and scikit-learn) also contain methods to normalize and center data, to standardize it and to perform other more complex transformations that may be needed to make the data easier to model. See for example this blog post in [Towards Data Science][1]\n\n\n\n\n**Overfitting**\nAnother important aspect to consider is the risk of overfitting. With overfitting we produce a model that fits very well the observed data, but fails to predict outside of the domain of observation. In some circumstances, where you do not intend to predict outside te domain of observation or do not intend to predict at all, it is ok to use the full set of observations to obtain the model. \nIn other circumstances, it is more advisable to hold out some of your observations for testing, and build the model using a subset of it. In scikit-learn, this is done using the train_test_split method, but you can also achieve this by taking a random subset of the observations out of the data\n\nIn the figure above, the dashed line corresponds to a second model developed using a split set of observations. The difference is immediately visible. Let's follow the process of regression using train\/test split, and compare the results of the models obtained.\n\n[1]: https:\/\/towardsdatascience.com\/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf#:~:text=Normalization%20typically%20means%20rescales%20the,of%201%20(unit%20variance)\n[2]: https:\/\/www.kaggle.com\/luisemiliani\/exploring-the-ucs-satellite-database\n[3]: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.RANSACRegressor.html","8654bfc1":"## Concluding\n\nThe main objective of this notebook was to provide examples of linear regressions using various python packages and modules, without elaborating much on the why of each parameter and the appraoch of each module. Altough I did cover some additional preparatory matters before jumping into a regression, there are also more things I did not cover. Books on Econometrics and Statistics cover a lot of foundational material to understand linear models, and of course there are plenty of resources in youtube and elsewhere on the web. There are also other ways to perform linear regressions such as using neural networks or regression trees. For examples of those appraoches, head on to the scikit learn documentation.\n\nThat said, I hope the examples shown are useful.\n\n","1a868628":"![OLR_python.png](attachment:f64f1361-da04-4427-8378-2f9d6c9f50e0.png)) \n# An overview of linear regression using Python: A case study using GDP and Internet users data.\n\n<div style=\"text-align: right\"> L. Emiliani. v1.1 Sept.,2021 <\/div>\n\nOne of the best things about Python is the large amount of libraries and modules available to support your analysis needs.\nYou are certain to find useful libraries for data analysis and statistics, but this also means that there will be many paths to reach the same destination. Many methods to deliver the same result.\nLinear Regressions are one example: there are many different libraries that help you execute linear regression, each with their own strengths and advantages.\nIn this tutorial I compile a few of the methods for carrying out a linear regression that I have come across since starting my journey into Python.\nWe will explore the link between Contry Gross Domestic Product (GDP) and the percentage of the population using Internet.\n\n## 1. What is linear regression ?\n\n[Linear regression][1] is a linear approach to model the relationship between two variables, that is, the two variables will be related by a straight line.\n\n<p style=\"text-align: center;\">y =$\\alpha_{0}$+$\\alpha_{1}$x<\/p>\n\nwhere ***x*** is the *independent* variable, ***y*** is the *dependent* variable, $\\alpha_{1}$ is the slope, and $\\alpha_{0}$ is the intercept. To obtain both coefficients $\\alpha_{0}$ and $\\alpha_{1}$, we can use several packages and functions.\n\nI will explore how to perform linear regression using the following libraries:\n\n - numpy,\n - scipy,\n - statmodels\n - scikitlearn\n \n ----\n \n## 2. First things first, set up the environment\n \n First, we set up the environment. This will take care of the background packages needed to display the data. We will be using both Seaborn and Plotly throughout this notebook.\n The actual packages needed to perform the linear regression will be loaded as required, as we go through each section of the notebook.\n \n [1]: https:\/\/en.wikipedia.org\/wiki\/Linear_regression","6a8711d4":"### 4.1 Observing the relationship between variables.\n\n\nNow that the data has been extracted and somewhat cleaned, we can observe the relationship between the two variables we intend to model: **GDP per-capita** and **percentage of Internet users**\n\nLet's explore the correlation between the variables, and introduce two new columns corresponding to a transformation ( Log() ) of GPD_PC and PCT_INTERNET_USERS","96985038":"## 5. Performing linear regressions\n\nSo, as we can see from the correlation matrix results, and from the scatter plots above, the variables are positively correlated. It appears that a linear model may be adequate to represent the relationship, at least to a first order of magnitude. We must note that the presence of a few data **outliers** (see Macao, Qatar, Equatorial Guinea, Brunei) will cause issues with our modelling, as we will see later when we view the results of the linear prediction.\n\n\n**Outliers** are observations that are *out of the norm*, those are probably valid data points in our case, but they deviate from the usual values. A traditional, ordinary regression will fail to produce a good model in the presence of outliers. We would need to use a different approach to regression, a **robust** approach, to return a better model. One such approach is called RANSAC regression, and I use it in another notebook, [here][2], to perform a robust regression to a dataset on satellite mass and DC power capability. RANSAC regression is covered with examples in the SCIKIT-LEARN documentation, [here][3]\n\nTo remove outliers we can establish a threshold based on the standard deviation of the sample or based on the inter-quartile range (IQR). \nFor example, using the IQR\n\n    Q_1 = GDP_BB_2014['LOG_GDP'].quantile(0.25)\n    Q_3 = GDP_BB_2014['LOG_GDP'].quantile(0.75)\n    IQR = Q_3 - Q_1\n    \n    GDP_BB_2014_outlier_removed = GDP_BB_2014.query( '(@Q_1 - 1.5 * @IQR) <= LOG_GDP <= (@Q_3 + 1.5 * @IQR)' )\n\nthe new dataframe, **GDP_BB_2014_outlier_removed**, should be used for modelling.\n\nAlternatively, using a criteria based on say, 5 standard deviations,\n\n    y_max = 5*GDP_BB_2014['LOG_GDP'].std()\n    GDP_BB_2014_outlier_removed = GDP_BB_2014.query( 'LOG_GDP <= @y_max' )\n\n[2]:(https:\/\/www.kaggle.com\/luisemiliani\/exploring-the-ucs-satellite-database)\n[3]:(https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_ransac.html)","19d844fa":"\n## 4. Explore visually the data\n\nAn important step before starting any modelling activity is to execute a visual exploration of the data, including some basic statistic properties. It would be useful to understand how the data is distributed, and especially determine if there will be any problematic data points, like outliers, that will affect the quality of the modelling.\n\nAfter processing the data, we end up with **181** valid data points. Let's explore some statistics of the data, such as the min, max and std. deviation, by using *.describe()*\nFurther, we can explore the data distribution visually using a [Box plot][1].\nIt is clear that there are some outliers that will affect the quality of our model.\n\n\n[1]: https:\/\/en.wikipedia.org\/wiki\/Box_plot","7fa5a533":"Unsurprisingly, the values are identical as the ones obtained using the numpy *polyfit* approach. This is a mathematically simple problem that has a closed solution, therefore the packages will not return different values.\nWe can explore the results of each fit by observing, for instance,  the sum of squares of the residuals (RSS) or the resulting correlation coefficient. \n    \n    RSS = np.sum(np.square(predicted_y - y))\n\nNote that the correlation coefficient will be ideally the same as that we computed in a previous section using pandas .corr() method. LINREGRESS returns the correlation coefficient as part of its results, as the property *.rvalue*\n","94e7ac6c":"The next section of the notebook will focus on building the linear model using different techniques available to us in Python.\n\n### 5.1 Building a linear model in Python : Numpy Polyfit \nThe first techniques that we will explore are those included in Numpy. Numpy is a python package focused on providing tools for mathematical and scientific computing. Lots of information about numpy can be found of course in the main numpy repository, https:\/\/numpy.org\/doc\/stable\/contents.html\nNumpy provides at least two ways to obtain the parameters for a linear model\n\n#### 5.1.1 Using POLYFIT\nThe [numpy.polynomial.polynomial.polyfit][1] method performs a least-squares fit of a polynomial to data and returns the coefficients of a polynomial of degree (*deg*) that fits the points (x,y), minimizing the square error.\n\n    polynomial.polynomial.polyfit(x, y, deg)\n    \nIn our case, the degree of the polynomial is 1, as a linear model has the form\n\n<p style=\"text-align: center;\">y =$\\alpha_{0}$+$\\alpha_{1}$x<\/p>\n\nwhere $\\alpha_{1}$ is the slope (coefficient that accompanies the independent variable) and $\\alpha_{0}$ is the intercept. What are first degree polynomials you say? First degree polynomials are the mathematical expression of a line.\n\nTo obtain the coefficients we will use numpy's polyfit function as follows:\n\n    poly_coeffs = polynomial.polynomial.polyfit(GDP_BB_2014_outlier_removed['PCT_INTERNET_USERS'], GDP_BB_2014_outlier_removed['LOG_GDP'],1)\n\nwhere the independent variable is the *Percentage of internet users*, and the dependent variable is the *log of per-capita GDP for the year 2015*.\n\n[1]: https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.polynomial.polynomial.polyfit.html#numpy.polynomial.polynomial.polyfit","c27b6552":"From the above correlation numbers we can see that:\n - there is strong positive correlation between the variables (PCT_INTERNET_USERS, GDP_PC) : 0.7\n - the correlation is higher with the transformed variables, LOG_PCT_USERS and LOG_GDP.\n \nLet's visualize the relationships. I wil use plotly to benefit from the added interactivity it provides. Hover over the data points to see other bits of contry-level information.\n\nIt is also possible to use seaborn's REGPLOT visualization, which in itself is a tool that can present a linear model over the data. REGPLOT enables you to create the model, but it is not in itself a modelling tool, and it is not straightforward to retrieve the model parameters. For more info see the [documentation](https:\/\/seaborn.pydata.org\/tutorial\/regression.html)","b480582b":"### 5.2 Building a linear model in Python : ScikitLearn Linear_Model LinearRegression\n\n[SCIKIT LEARN][1] is a very popular package for Machine learning, data science and modelling in Python. Scikit Learn provides a variety of algorithms including of course, linear models and [linear regressions][2].\n\nThe process for modelling in scikit learn usually follows three steps:\n\n1. instantiate the model,\n2. fit a model,\n3. predict using the model\n\nand we need to ensure the input data is in the corect shape for scikitLearn, a column vector for independent (features) and target (dependent) variables.\nThe quality of the model can be evaluated using the RSS as shown before, or the [coefficient of determination][3].\n\n[1]: https:\/\/scikit-learn.org\/stable\/index.html\n[2]: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html\n[3]: https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination","99e1e8b0":"Again, no huge surprises. The parameters are the same as those obtained before, but the mechanics are different. This is closer to SCIPY in the there is a flow for fitting: **instantiate, fit predict**.\n\n# Observing the model and data\nNow that we have obtained the parameters of the model, we can finally write the equation of the line that best fits our observations:\n\n<p style=\"text-align: center;\">y = 3.32+0.02x<\/p>\n\nand we can make a set of predictions to help us overlay the model over the observed data.\n\n","afb0d667":"So, even though  the slope and intercept values are very close, we obtained the following results\n\nFor a value of X of 70,\n\n- the model developed using the full set of observations resulted in a predicted GDP of 23,667 USD\n- the model developed using 70% of the data as training set, returned a prediction of 23,399 USD\n\nWith the data we have at hand, and based on the observations, we could argue that the second model is a better choice, as it is not as severly affected by outliers as the model using the full data set. Clearly, the first lesson to learn is that outliers should be removed from the data before modelling. \n\nMost generally, it may be that only after using a model to predict unobserved data and reviewing the performance results, will we be certain of which model is performing better. Thus, modelling is a continuous effort, as we obtain new data and improve our assumptions and develop better models.\n","214a7b05":"## 3. Loading and cleaning the data\n\n\nWe now load the data to use in this exercise. The data, available from [our world in data][1],  illustrates the link between Internet usage and country per-capita gross domestic product (GDP)\n\nThis is a known link, which underpins the efforts to bring communications and internet connectivity to communities. The positive correlation between GPD and communications was first noted in the 80s, when the [Maitland Report][2] was published. \n\nWe will look at a subset of this data set, for the year 2014. \n\nTo load the data into a pandas dataframe, we use read_csv()\n    \n    GDP_BB = pd.read_csv('..\/input\/gdp-per-capita-and-internet-usage-in-the-world\/correlation-between-internet-users-as-a-share-of-the-population-and-gdp-per-capita.csv')\n    \nThe dataset contains country-level information on GDP per capita and percentage of internet users per country, for a series of years.\nWe will focus only on the data for 2014 in this exercise. \n   \n    GDP_BB_2014 = GDP_BB[ GDP_BB['Year']==2014 ]\n\n### 3.1 Cleaning the data : missing continent names\n\n\nTo complete the missing entries in \"continent\", I will create a new \"continent\" dataframe that contains no missing continent entries, using as source the data we have in our dataframe for previous years, and then execute an inner join with the GDP_BB_2014 dataframe. The resulting dataframe will have all continent entries filled, for every country for which data points exist. The full code is available as STEP 4 in the code cell below.\n\n\n### 3.2 Cleaning the data: Missing pct. internet users data points.\n\nAfter exploring the data we see that there are a few missing data points in the column \"Percentage of Individuals using the Internet (ICT) (2015)\", which contains the variable we want to study.\nThere are no simple ways to impute this variable. In this case, it is preferable to evaluate the set after dropping empty values, provided we have enough left for a meaningful analysis.\nI will chain a .dropna() method to drop rows without data (NaN) in the \"Percentage of Individuals using the Internet (ICT) (2015)\" column.\n\n\n### 3.3 Dataframe organization: change to more manageable column names.\n\nFinally, since the original column names from the CSV file are rather long, I will rename the columns with shorter names, to make handling the dataframe easier.\nSee step 5 in the cell below.\n\n\n[1]:(https:\/\/ourworldindata.org\/grapher\/correlation-between-internet-users-as-a-share-of-the-population-and-gdp-per-capita?minPopulationFilter=1000000&time=latest)\n[2]:(http:\/\/www.itu.int\/osg\/spuold\/sfo\/missinglink\/index.html)","78fd2161":"To predict, we can use again POLYVAL, passing the coefficients, or execute a direct prediction with the model_func function we used to optimise :\n\n    predicted_y = model_to_optimize(desired_x, intercept, slope)","5f43b711":"#### 5.1.2 Using POLYVAL to perform predictions\nNow that we have a set of coeficients for our linear model, we can make predictions. One quick way to predict is using POLYVAL, which evaluates a polynomial using as input the values of the coefficients.\n    \n    numpy.polynomial.polynomial.polyval(x, c)\n    \n    npPol.polyval(desired_x, [slope, intercept])\n   \nwhere [slope, intercept]is a list with the model coefficients, and predict_x is the new value of X for which we want to predict a value of Y.\nFor example, let's predict the value of Y (log of GDP per-capita) for a country that has 70% internet users.","271ba920":"## Seaborn: REGPLOT and LMPLOT\nThe seaborn package for data visualization provides plot options to display polynomial models over the observed data. in the plot below, I use REGPLOT to display a linear model over the LOG_GDP and GSP observations. The plots include by default and overlay of confidence intervals. The model parameters are not provided as part of the figure output parameters, so it is mainly for visualization purposes.\n\n[LMPLOT][1] is similar to REGPLOT, but it returns a [facet][2]. The arguments **fit_reg** and **order** are used to fit a model. **order** is used with numpy's **polyfit** to fit the model.\n\n[1]: https:\/\/seaborn.pydata.org\/generated\/seaborn.lmplot.html\n[2]: https:\/\/seaborn.pydata.org\/generated\/seaborn.FacetGrid.html#seaborn.FacetGrid"}}