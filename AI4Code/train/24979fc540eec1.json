{"cell_type":{"63067641":"code","8118f76f":"code","44bc2676":"code","daa4f674":"code","9b3e344b":"code","1d38dd9e":"code","b2448465":"code","cef04c26":"code","f1edda09":"code","f6cca497":"code","292d2de3":"code","51b3ea39":"code","608aaa95":"code","3a72dbc7":"code","dfde6b32":"code","b1136c53":"code","aa82072d":"code","e814b372":"code","89cbe9f2":"code","357d9e30":"code","64333b41":"code","12613751":"code","db405251":"code","87c247be":"code","aa3a1bba":"code","d6ee8f65":"code","9fe113ef":"code","9a1a2a9f":"code","6ca66d4d":"code","6f1c2cfa":"code","de5aa015":"code","ac561f49":"markdown","a515c528":"markdown","f7a6eb14":"markdown","86899575":"markdown","deb646c8":"markdown","9020a4dd":"markdown","b2c16a87":"markdown","a990921b":"markdown","b334bac6":"markdown","0e1db6db":"markdown","bd46ad4e":"markdown","29b338a4":"markdown","20479330":"markdown","e22cd1d3":"markdown","2fb904b5":"markdown","85bbd870":"markdown","417bcd87":"markdown","16e1938f":"markdown","c4b5ca97":"markdown","33730a7d":"markdown","e402d239":"markdown","5cd59002":"markdown","c67e3ac6":"markdown","9f9a434f":"markdown","224a50a7":"markdown","faf16f81":"markdown","a698350f":"markdown","a3d8e153":"markdown","7ca89c11":"markdown","56047829":"markdown","6d02857a":"markdown","e7ee9290":"markdown","7a41af4e":"markdown","957cf361":"markdown","1e473986":"markdown","a8cb96de":"markdown","966764cf":"markdown"},"source":{"63067641":"###Import Libraries \n### Data handling imports\nimport pandas as pd\nimport numpy as np\n\n### Plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n%matplotlib inline\n\nfrom pandas.core.reshape.pivot import pivot_table\n\n# Advanced plotting... Plotly\nfrom plotly import tools\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n# Statistics imports\nimport scipy, scipy.stats\nimport statsmodels.api as sm\nfrom statsmodels.stats import diagnostic as diag\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n%matplotlib inline\n\n# df.head() displays all the columns without truncating\npd.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings('ignore')","8118f76f":"# Plot bar graphs\ndef pretty_bar(data, ax, xlabel=None, ylabel=None, title=None, int_text=False):\n    \n    # Plots the data\n    fig = sns.barplot(data.values, data.index, ax=ax)\n    \n    # Places text for each value in data\n    for i, v in enumerate(data.values):\n        \n        # Decides whether the text should be rounded or left as floats\n        if int_text:\n            ax.text(0, i, int(v), color='k', fontsize=12)\n        else:\n            ax.text(0, i, round(v, 3), color='k', fontsize=12)\n     \n    ### Labels plot\n    ylabel != None and fig.set(ylabel=ylabel)\n    xlabel != None and fig.set(xlabel=xlabel)\n    title != None and fig.set(title=title)\n\n    \n### Style Python print statements\nclass color:\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[3m'\n    END = '\\033[0m'","44bc2676":"###Load the Data into Pandas\nusa_census = pd.read_csv(\"..\/input\/2017-us-census-ed\/acs2017_bystates_data_ProjectITU.csv\")\nusa_census.head()","daa4f674":"usa_census.tail(2)","9b3e344b":"usa_census = usa_census.dropna()\nusa_census.shape","1d38dd9e":"usa_census = usa_census.drop(columns=['TractId', 'VotingAgeCitizen'])","b2448465":"usa_census = usa_census.dropna()\nprint(\"Shape of dataset\", usa_census.shape)\nprint(\"Columns\", usa_census.columns)\nusa_census.head()","cef04c26":"### Show unique datas\nusa_census[\"State\"].unique()","f1edda09":"# get column name and data types to make sure the types are right, results looking good.  \nusa_census.dtypes","f6cca497":"#Lets see how the population looks like on the graph by states \nusastates= usa_census.groupby('State', as_index=False).sum()\nusastates= usastates.sort_values('TotalPop', ascending = 0)\n\n#Total population by country looks like below; \n\nfig, ax = plt.subplots(figsize=(16,6))\nfig = sns.barplot(x = usastates['State'], y = usastates['TotalPop'], data=usastates)\nfig.axis(ymin=0, ymax=40000000)\nplt.xticks(rotation=90)\nplt.show()","292d2de3":" # Create a new dataset which contains data of states with top 20 total population\n\nca = usa_census.groupby('State').get_group('California')\ntx = usa_census.groupby('State').get_group('Texas')\nfl = usa_census.groupby('State').get_group('Florida')\nny = usa_census.groupby('State').get_group('New York')\nil = usa_census.groupby('State').get_group('Illinois')\npn = usa_census.groupby('State').get_group('Pennsylvania')\noh = usa_census.groupby('State').get_group('Ohio')\ngo = usa_census.groupby('State').get_group('Georgia')\nnc = usa_census.groupby('State').get_group('North Carolina')\nmg = usa_census.groupby('State').get_group('Michigan')\nnj = usa_census.groupby('State').get_group('New Jersey')\nvn = usa_census.groupby('State').get_group('Virginia')\nwg = usa_census.groupby('State').get_group('Washington')\nmt = usa_census.groupby('State').get_group('Massachusetts')\nar = usa_census.groupby('State').get_group('Arizona')\nia = usa_census.groupby('State').get_group('Indiana')\ntn = usa_census.groupby('State').get_group('Tennessee')\nms = usa_census.groupby('State').get_group('Missouri')\nmy = usa_census.groupby('State').get_group('Maryland')\nwc = usa_census.groupby('State').get_group('Wisconsin')\n\nusatop20states = pd.concat([ca, tx, fl, ny, il, pn, oh, go, nc, mg, nj, vn, wg, mt, ar, ia, tn, ms, my, wc])\nusatop20states.head()","51b3ea39":"#Lets see the correlations between the columns we have those're meaningful to correlate \nusatop20states = usatop20states[['Men', 'Women', 'Hispanic', 'White','Black', 'Native', 'Asian', 'Pacific', 'Professional', 'Service', 'Office', 'Construction', 'Production', 'Income','IncomePerCap', 'Poverty', 'Unemployment']]\nusatop20states.head(5)","608aaa95":"# correlation\nplt.figure(figsize=(12,12))\nsns.heatmap(usatop20states.corr(), annot=True, cmap=\"coolwarm\", square=True)","3a72dbc7":"fig, axes = plt.subplots(nrows=2, ncols=3)\nfig.set_figheight(5)\nfig.set_figwidth(20)\n\nusatop20states.plot(ax=axes[0,0],x='Hispanic', y='IncomePerCap', kind = 'scatter', title = 'Hispanic Density vs. IncomePerCap')\nusatop20states.plot(ax=axes[0,1],x='Black', y='IncomePerCap', kind = 'scatter', title = 'Black Density vs. IncomePerCap')\nusatop20states.plot(ax=axes[0,2],x='White', y='IncomePerCap', kind = 'scatter', title = 'White Density vs. IncomePerCap')\nusatop20states.plot(ax=axes[1,0],x='Professional', y='Income', kind = 'scatter', title = 'Professional vs. Income')\nusatop20states.plot(ax=axes[1,1], x='Professional', y='IncomePerCap', kind = 'scatter', title = 'Professional vs. IncomePerCap')\nusatop20states.plot(ax=axes[1,2], x='Professional', y='Poverty', kind = 'scatter', title = 'Professional vs. Poverty')\n\nc=np.array([0.5, 0.5, 0.5]).reshape(1,-1)\n\nplt.tight_layout()","dfde6b32":"# get the summary\ndesc_df = usatop20states.describe()\n\n# add the standard deviation metric\ndesc_df.loc['+3_std'] = desc_df.loc['mean'] + (desc_df.loc['std'] * 3)\ndesc_df.loc['-3_std'] = desc_df.loc['mean'] - (desc_df.loc['std'] * 3)\n\n# display it\ndesc_df","b1136c53":"# define our input variable (X) & output variable\nX = usatop20states[['White', 'Professional']]\nY = usatop20states['IncomePerCap']","aa82072d":"#let's split the data into a training and testing set; a healthy ratio is 20% testing and 80% training but a 30% 70% split is also ok.\n\n#create test and trainning datasets\nfrom sklearn.model_selection import train_test_split\n\n# Split X and y into X_\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\n# create a Linear Regression model object\nregression_model = LinearRegression()\n\n# pass through the X_train & y_train data set\nregression_model.fit(X_train, y_train)","e814b372":"# Get multiple predictions\ny_predict = regression_model.predict(X_test)\n\n# Show the first 5 predictions\ny_predict[:5]","89cbe9f2":"# define our intput\nX2 = sm.add_constant(X)\n\n# create a OLS model\nmodel = sm.OLS(Y, X2)\n\n# fit the data\nest = model.fit()","357d9e30":"#Evaluating the Model: StatsModel1 (heteroscedasticity)\n\n# Run the White's test\n\n_, pval, __, f_pval = diag.het_white(est.resid, est.model.exog)\nprint(pval, f_pval)\nprint('-'*100)\n\n# print the results of the test\nif pval > 0.05:\n    print(\"For the White's Test\")\n    print(\"The p-value was {:.4}\".format(pval))\n    print(\"We fail to reject the null hypthoesis, so there is no heterosecdasticity. \\n\")\n    \nelse:\n    print(\"For the White's Test\")\n    print(\"The p-value was {:.4}\".format(pval))\n    print(\"We reject the null hypthoesis, so there is heterosecdasticity. \\n\")\n\n# Run the Breusch-Pagan test\n_, pval, __, f_pval = diag.het_breuschpagan(est.resid, est.model.exog)\nprint(pval, f_pval)\nprint('-'*100)\n\n# print the results of the test\nif pval > 0.005:\n    print(\"For the Breusch-Pagan's Test\")\n    print(\"The p-value was {:.4}\".format(pval))\n    print(\"We fail to reject the null hypthoesis, so there is no heterosecdasticity.\")\n\nelse:\n    print(\"For the Breusch-Pagan's Test\")\n    print(\"The p-value was {:.4}\".format(pval))\n    print(\"We reject the null hypthoesis, so there is heterosecdasticity.\")","64333b41":"# test for autocorrelation\nfrom statsmodels.stats.stattools import durbin_watson\n\n# calculate the lag, optional\nlag = min(40, (len(X)\/\/5))\nprint('The number of lags will be {}'.format(lag))\nprint('-'*100)\n\n# run the Ljung-Box test for no autocorrelation of residuals\n# test_results = diag.acorr_breusch_godfrey(est, nlags = lag, store = True)\ntest_results = diag.acorr_ljungbox(est.resid, lags = lag)\n\n# grab the p-values and the test statistics\nibvalue, p_val = test_results\n\n# print the results of the test\nif min(p_val) > 0.05:\n    print(\"The lowest p-value found was {:.4}\".format(min(p_val)))\n    print(\"We fail to reject the null hypthoesis, so there is no autocorrelation.\")\n    print('-'*100)\nelse:\n    print(\"The lowest p-value found was {:.4}\".format(min(p_val)))\n    print(\"We reject the null hypthoesis, so there is autocorrelation.\")\n    print('-'*100)\n\n# plot autocorrelation\nsm.graphics.tsa.plot_acf(est.resid)\nplt.show()","12613751":"import pylab\n\n# check for the normality of the residuals\nsm.qqplot(est.resid, line='s')\npylab.show()\n\n# also check that the mean of the residuals is approx. 0.\nmean_residuals = sum(est.resid)\/ len(est.resid)\nprint(\"The mean of the residuals is {:.4}\".format(mean_residuals))","db405251":"import math\n# calculate the mean squared error\nmodel_mse = mean_squared_error(y_test, y_predict)\n\n# calculate the mean absolute error\nmodel_mae = mean_absolute_error(y_test, y_predict)\n\n# calulcate the root mean squared error\nmodel_rmse =  math.sqrt(model_mse)\n\n# display the output\nprint(\"MSE {:.3}\".format(model_mse))\nprint(\"MAE {:.3}\".format(model_mae))\nprint(\"RMSE {:.3}\".format(model_rmse))","87c247be":"#The R-Squared metric provides us a way to measure the goodness of fit or, in other words, how well our data fits the model.\n\nmodel_r2 = r2_score(y_test, y_predict)\nprint(\"R2: {:.2}\".format(model_r2))","aa3a1bba":"# print out a summary\nprint(est.summary())","d6ee8f65":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\nX = usatop20states[['White', 'Professional']]\nY = usatop20states['IncomePerCap']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=10)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nmean_squared_error(y_test, y_pred), mean_absolute_error(y_test, y_pred), r2_score(y_test, y_pred)","9fe113ef":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\n\ndef create_polynomial_regression_model(degree):\n poly_features = PolynomialFeatures(degree=degree)\n X_poly = poly_features.fit_transform(X)\n poly = LinearRegression()\n return np.mean(cross_val_score(poly, X_poly, Y, cv=5))\npoly_cv = []\nfor i in range(1,4):\n poly_cv.append(create_polynomial_regression_model(i))\nplt.scatter(range(1,4),poly_cv)","9a1a2a9f":"from sklearn.linear_model import LassoCV\nlasso = LassoCV(cv=5).fit(X, Y)\nlasso.score(X, Y)","6ca66d4d":"from sklearn.linear_model import RidgeCV\nridge = RidgeCV(cv=5).fit(X, Y)\nridge.score(X, Y)","6f1c2cfa":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\nnp.mean(cross_val_score(dt, X, Y, cv=5))","de5aa015":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nnp.mean(cross_val_score(rf, X, Y, cv=5))","ac561f49":"### Getting Multiple Projections\n\nWe can also now make predictions with our newly trained model. The process is simple; we call the predict method and then pass through some values. In this case, we have some values predefined with the x_test variable so we will pass that through. Once we do that, we can select the predictions by slicing the array.","a515c528":"### Polynomial Regression","f7a6eb14":"We got almost the same result as the model before.","86899575":"## Section 6: Trying with Different Models\n\nWe will try pipeline, which make a compination between StandardScaler, polynomial and linear regression.","deb646c8":"## Section 2: Load the Data\n\n### Background about the Dataset Used and Analysed\nEvery 10 years, the US governemt conducts a survey of the entire nation to understand the current distribution of the population. Every citizen in the States receives a questionaire (see questionaire below). The idea of a Census has been since the county's founding with the first Census taken in 1790 under Secretary of State, Thomas Jefferson.\n\nThe data comes from Kaggle user Muon Neutrino who took this data from the DP03 and DP05 tables of the 2017 American Community Survey 5-year estimates. I recommend just using his data as I found the American Fact Finder website slow and a bit hard to navigate.\n\nThe two tables have essentially the same information. The data is collected in tracts which are subsections of a county while the county data is an accumulation of all the tract data.\n\nLink: data.census.gov\n\n\"US Census Demographic Data - 2017\" obtained by states and county ","9020a4dd":"#### Checking For Normally Distributed Residuals","b2c16a87":"### Random Forest","a990921b":"### Lasso Regression","b334bac6":"#### Description of Data\n\nAdditional detail may be found in the data set page.\n\n- Location: State\/County are distinct and define a census region\n- Sex:  TotalPop=Men+Women \n- Race: % of TotalPop.  Hispanic+White+Black+Native+Asian+Pacific<=100% \n- Citizen: Number of people who are citizens.  Citizen<=TotalPop \n- Income: Median household income\n- IncomeErr: Median household income error\n- IncomePerCap: Income per capita (Total Income \/ Total population in that area)\n- Poverty: % of TotalPop under poverty level\n- ChildPoverty: % of children under poverty level\n- Work Class: % of Employed.  Professional+Service+Office+Construction+Production<=100% \n- Transportation: % of Employed?  Drive+Carpool+Transit+Walk+OtherTransp+WorkAtHome<=100% \n- MeanCommute: Average commute time (minutes) of Employed\n- Employed: Number of people employed (16+)\n- Work Sector: % of Employed.  PrivateWork+PublicWork+SelfEmployed+FamilyWork<=100% \n- Unemployment: % of TotalPop who have filed for unemployement","0e1db6db":"Looking at the heatmap along with the correlation matrix we can identify a few highly correlated variables. For example, if you look at the correlation between job grup of 'professional'and 'income' and 'incomepercap'; they end up at almost .82 and .75. There is also another correlation between  race of 'white'and 'income' and 'incomepercap'; they end up at almost .29 and .38. \n\n#### Lets see the relation on the plots below;  ","bd46ad4e":"##### We will analyse the variable of job group of \"Professional' and race of 'White' and prediction of 'IncomePerCap.   ","29b338a4":"We also got 0.64 very close to 0.68.","20479330":"## Table of contents\n\n#### Section 1: Import our Libraries\n#### Section 2: Load the Data\n#### Section 3: Clean and Explor the Data\n#### Section 4: Build the Model\n#### Section 5: Create a Summary of the Model Output\n#### Section 6: Trying with Different Models\n#### Section 7: Conclusion ","e22cd1d3":"### Explore Population By State","2fb904b5":"#### Checking for Heteroscedasticity\n\nOur goal is to fail to reject the null hypothesis, have a high p-value because that means we have no heteroscedasticity.\n","85bbd870":"As we can see from this plot, there is a strong negative correlation between the fraction of hispani and black people, and postive correlation between white people in a state and the IncomePercap. Conversely, there is a strong positive correlation between the fraction of Hispanic and African.\n\nAs we can see from this plot, there is a strong positive correlation between the fraction of Professional job group in a state and the Income and IncomePercap. Conversely, there is a strong negative correlation between the fraction of Professional job group and Poverty.\n\nWe will analyse the variable of job group of \"Professional' and race of 'White' and prediction of 'IncomePerCap.   ","417bcd87":"After splitting the data, we will create an instance of the linear regression model and pass through the X_train and y_train variables using the fit() function.","16e1938f":"## Section 3: Clean and Explore the Data\nCleaning the data is an essential part of your data organization. Once inspecting your data, you will notice that there will be some data that you don't necessarily need or data that could influenze your numbers and impact your analysis.\n\nFor purposes of this tutorial, I will drop all rows that contain null values","c4b5ca97":"Different regression models were evaluated based on the CV scores and it\u2019s observed that ridge\/lasso regression best fits the data compared to all the other methods. We can also observe that a perfect linear model is not a good approximation for the given data set. The reason behind the performance of respective models for the given data can be further understood by looking at the relationship between the observed and target variables.","33730a7d":"#### Checking for Autocorrelation\nAutocorrelation is a characteristic of data in which the correlation between the values of the same variables is based on related objects. It violates the assumption of instance independence, which underlies most of conventional models.","e402d239":"#### Final Dataset\n\nWe create a new dataset which contains data of states with Top 20 total population for consodering the homegenity of the data. Our analysis will be to be able to pick more homegenius data.","5cd59002":"You can see that we have dropped ~1.5K rows.","c67e3ac6":"We got almost the same result as the model before.\n\n### Ridge Model ","9f9a434f":"   ----- End-----","224a50a7":"# R-Squared\nThe R-Squared metric provides us a way to measure the goodness of fit or, in other words, how well our data fits the model. The higher the R-Squared metric, the better the data fit our model. However, one limitation is that R-Square increases as the number of features increase in our model, so if I keep adding variables even if they're poor choices R-Squared will still go up! A more popular metric is the adjusted R-Square which penalizes more complex models, or in other words models with more exploratory variables. In the example below, I calculate the regular R-Squared value, however, the statsmodel summary will calculate the Adjusted R-Squared below.","faf16f81":"## Section 4: Build the Model\n\n### Assumptions of the Model\n\nIt's essential to understand the assumptions of the model before we start building and coding. Each assumption if violated means we may have to take extra steps to improve our model or in some cases dump the model altogether. Here is a list of the assumptions of the model:\n\n - Regression residuals must be normally distributed.\n - A linear relationship is assumed between the dependent variable and the independent variables.\n - The residuals are homoscedastic and approximately rectangular-shaped.\n - No Autocorrelation of the residuals.\n\nI will be explaining these assumptions in more detail as we arrive at each of them in the tutorial. At this point, however, we need to have an idea of what they are.\n\n\nWe've loaded, cleaned, and explored the data we can proceed to the next part, building the model. The first thing we need to do is, define our exploratory variables and our explanatory variable. From here, let's split the data into a training and testing set; a healthy ratio is 20% testing and 80% training but sometime a 30% 70% split is also ok.","a698350f":"#### Correlation Heatmap\n\nLets see how the categories for 'Men', 'Women', 'Hispanic', 'White',' Black', 'Native', 'Asian', 'Pacific', 'Income','IncomePerCap', 'Poverty', 'Unemployment' are related.\n","a3d8e153":"Decision tree model gave 0.40 results, lower than our model but acceptable.","7ca89c11":"Before we get to an in-depth exploration of the data or even building the model, we should explore the data a little more and see how the data is distributed and if there are any outliers. I will be adding a few more metrics to the summary data frame, sp that it now includes a metric for three standard deviations below and above the mean.\n\nI'll store my information in a new variable called desc_df.","56047829":"### Decision Tree","6d02857a":"## Section 1: Import our Libraries\n\nThe first thing we need to do is import the libraries we will be using in this tutorial. To visualize our data, we will be using matplotlib and seaborn to create heatmaps and a scatter matrix. To build our model, we will be using the sklearn library, and the evaluation will be taking place with the statsmodels library. I've also added a few additional modules to help calculate certain metrics.","e7ee9290":"\nThe first thing we notice is that the p-values from up above are now easier to read and we can now determine that the coefficients that have a p-value greater than 0.05 can be removed.\n\nThe other metric that stands out is our Adjusted R-Squared value which is .677, lower than our R-Squared value of 68%. This makes sense as we were probably docked for the complexity of our model. However, an R-Squared over .878 is still very strong.\n\nThe only additional metrics we will describe here is the t-value which is the coefficient divided by the standard error. The higher the t-value, the more evidence we have to reject the null hypothesis. Also the standard error, the standard error is the approximate standard deviation of a statistical sample population.","7a41af4e":"## Section 5: Create a Summary of the Model Output\nLet's create a summary of some of our keep metrics, some of the metrics might differ slightly, but they generally should be the same.","957cf361":"Our R-Suared result of **accuracy rate of 68%** seems good. Our regression analysis shows a moderate effect (0.5 < R^2 < 0.75) of variables of 'Professional' and 'White' and the IncomePerCap in each state. ","1e473986":"#### Design Settings\n\nDesign some python settings ","a8cb96de":"CV score for a 2nd degree polynomial is between 0.71 and 0.72","966764cf":"## Step 7: Conclusion\nThe best R^2 we get is 0.68 which is very good estimation for Income. The model is poly(2) with linear regression. and we found that IncomePerCap depend mostly on the job group of 'Professional' and race of 'White' and can be projected with %68 accuracy.  \n\nThe purpose of this study was to examine the relationship between the job group of 'Professional' and race of 'White' with 'IncomePerCap' as income per capital in each state information collected by or derived from the 2017 United States Census using a limited dataset. "}}