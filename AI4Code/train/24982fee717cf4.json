{"cell_type":{"79c52d7c":"code","4e7cd6a9":"code","7a6edc4d":"code","4adb5839":"code","9de7c984":"code","0b6482d6":"code","bcb2b495":"code","32513f3a":"code","58737c34":"code","4a65a8a1":"code","ad2cff0f":"code","396f9b87":"code","0f65fce1":"code","b5c65c9c":"code","99c0f8fc":"code","d72e95e2":"code","57beaeac":"code","13b1dd63":"code","8a51df5c":"code","417388ab":"markdown","3c37fd33":"markdown","4e6c6b1c":"markdown","e4069779":"markdown","d6ab9ea9":"markdown","5494027c":"markdown","021a9e2d":"markdown","57e77b59":"markdown"},"source":{"79c52d7c":"#Load libraries\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n\nplt.style.use('fivethirtyeight')\nplt.rcParams[\"figure.figsize\"] = (12, 8) # (w, h)","4e7cd6a9":"#Store the data set\ndf = pd.read_csv('..\/input\/datasets_228_482_diabetes.csv')\n#Look at first 7 rows of data\ndf.head(7)\n","7a6edc4d":"# Prepare the dataset\n\n#Convert the data into an array\ndataset = df.values\ndataset\n\n# Get all of the rows from the first eight columns of the dataset\nX = dataset[:,0:8] \n# Get all of the rows from the last column\ny = dataset[:,8]\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)\nX_scale\n\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.2, random_state = 33)\n","4adb5839":"acc_scores = []\n\nfor i in range(5):\n    model = Sequential()\n    n = 6\n    model.add(Dense(n, activation='relu', input_shape=( 8 ,)))\n    \n    for j in range(i):\n        n = 6\n        model.add(Dense(n, activation='relu'))\n    \n    model.add(Dense(1, activation='sigmoid'))\n\n\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    hist = model.fit(X_train, y_train,\n              batch_size=32, epochs=32, validation_split=0.2)\n    \n    print(model.summary())\n    pred = model.predict(X_test)\n    print(pred[0:3])\n    pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n    print(pred[0:3])\n    print(classification_report(y_test ,pred ))\n    print(accuracy_score(y_test,pred)*100)\n    acc_scores.append(accuracy_score(y_test,pred)*100)\n    \n    \n    model.reset_metrics()\n","9de7c984":"plt.plot(list(range(1,6)), acc_scores)\nplt.xlabel(\"Hidden Layer Number\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Layer Number Optimization - Neuron numbers are 6\")\nplt.show()","0b6482d6":"acc_scores = []\nneuron_numbers = [28, 24,20, 16, 12]\n\nfor i in range(len(neuron_numbers)):\n    model = Sequential()\n    n = neuron_numbers[i]\n    model.add(Dense(n, activation='relu', input_shape=(8,)))\n    model.add(Dense(n, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    hist = model.fit(X_train, y_train,\n              batch_size=32, epochs=32, validation_split=0.2)\n    \n    print(model.summary())\n    pred = model.predict(X_test)\n    print(pred[0:3])\n    pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n    print(pred[0:3])\n    print(classification_report(y_test ,pred ))\n    print(accuracy_score(y_test,pred)*100)\n    acc_scores.append(accuracy_score(y_test,pred)*100)\n    \n    \n    model.reset_metrics()","bcb2b495":"plt.title(\"Neuron Number Optimization\")\nplt.ylabel(\"Accuracy (%)\")\nplt.xlabel(\"Neuron Numbers\")\nplt.plot([\"28\", \"24\", \"20\", \"16\", \"12\"], acc_scores)","32513f3a":"acc_scores = []\nact_funcs = ['relu', 'sigmoid', 'softmax', 'selu']\n\nfor i in range(len(act_funcs)):\n    model = Sequential()\n    for j in range(len(act_funcs)):\n        n = 28\n        model.add(Dense(n, activation=act_funcs[i], input_shape=(8,))) #Fonksiyonlar\u0131n kombinasyonlar\u0131 deneniyor\n        model.add(Dense(n, activation=act_funcs[j]))\n        model.add(Dense(1, activation='sigmoid'))\n\n        model.compile(optimizer='adam',\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\n        hist = model.fit(X_train, y_train,\n                  batch_size=32, epochs=32, validation_split=0.2)\n\n        print(model.summary())\n        pred = model.predict(X_test)\n        print(pred[0:3])\n        pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n        print(pred[0:3])\n        print(classification_report(y_test ,pred ))\n        print(accuracy_score(y_test,pred)*100)\n        acc_scores.append(accuracy_score(y_test,pred)*100)\n\n\n        model.reset_metrics()","58737c34":"act_func_combs = []\nact_funcs_sh = ['re', 'sg', 'sm', 'sl']\nfor i in range(len(act_funcs)):\n    for j in range(len(act_funcs)):\n        act_func_combs.append(act_funcs_sh[i] + \"-\" + act_funcs_sh[j])\n        \nplt.rcParams[\"figure.figsize\"] = (18, 9) # (w, h)\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Activation Function Optimization\")\n\nplt.plot(act_func_combs,acc_scores)\n\nplt.rcParams[\"figure.figsize\"] = (12, 8) # (w, h)","4a65a8a1":"acc_scores = []\noptimizers = ['SGD', 'Adam', 'RMSProp', 'Adagrad', 'Adadelta', 'Adamax']\n\nfor i in range(len(optimizers)):\n    model = Sequential()\n    n = 28\n    model.add(Dense(n, activation='selu', input_shape=(8,))) #Fonksiyonlar\u0131n kombinasyonlar\u0131 deneniyor\n    model.add(Dense(n, activation='selu')) #Fonksiyonlar\u0131n kombinasyonlar\u0131 deneniyor\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer=optimizers[i],\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    hist = model.fit(X_train, y_train,\n              batch_size=32, epochs=32, validation_split=0.2)\n\n    print(model.summary())\n    pred = model.predict(X_test)\n    print(pred[0:3])\n    pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n    print(pred[0:3])\n    print(classification_report(y_test ,pred ))\n    print(accuracy_score(y_test,pred)*100)\n    acc_scores.append(accuracy_score(y_test,pred)*100)\n\n\n    model.reset_metrics()","ad2cff0f":"plt.plot(optimizers, acc_scores)","396f9b87":"acc_scores = []\nlearning_rates = [0.0001, 0.001, 0.01, 0.03]\n\nfor i in range(len(learning_rates)):\n    model = Sequential()\n    n = 28\n    model.add(Dense(n, activation='selu', input_shape=(8,)))\n    model.add(Dense(n, activation='selu'))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    optimizer = optimizers.Adam(learning_rate=learning_rates[i])\n    \n    model.compile(optimizer=optimizer,\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    hist = model.fit(X_train, y_train,\n              batch_size=32, epochs=256, validation_split=0.2)\n    \n    print(model.summary())\n    pred = model.predict(X_test)\n    print(pred[0:3])\n    pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n    print(pred[0:3])\n    print(classification_report(y_test ,pred ))\n    print(accuracy_score(y_test,pred)*100)\n    acc_scores.append(accuracy_score(y_test,pred)*100)\n    \n    \n    model.reset_metrics()","0f65fce1":"plt.title(\"Learning rate optimization\")\nplt.ylabel(\"Accuracy (%)\")\nplt.xlabel(\"Learning rates\")\nplt.plot([\"0.0001\", \"0.001\", \"0.01\", \"0.03\"], acc_scores)\nplt.show()\n","b5c65c9c":"    model = Sequential()\n    n = 28\n    model.add(Dense(n, activation='selu', input_shape=(8,)))\n    model.add(Dense(n, activation='selu'))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    optimizer = optimizers.Adam(learning_rate=0.001)\n    \n    model.compile(optimizer=optimizer,\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    hist = model.fit(X_train, y_train,\n              batch_size=32, epochs=256, validation_split=0.2)\n    \n    print(model.summary())\n    pred = model.predict(X_test)\n    print(pred[0:3])\n    pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n    print(pred[0:3])\n    print(classification_report(y_test ,pred ))\n    print(accuracy_score(y_test,pred)*100)\n    acc_scores.append(accuracy_score(y_test,pred)*100)\n    \n    \n    model.reset_metrics()","99c0f8fc":"#visualize the training loss and the validation loss to see if the model is overfitting\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()\n\n#visualize the training accuracy and the validation accuracy to see if the model is overfitting\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","d72e95e2":"X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.2, random_state = 5)\npred = model.predict(X_test)\nprint(pred[0:3])\npred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\nprint(pred[0:3])\nprint(classification_report(y_test ,pred ))\nprint(accuracy_score(y_test,pred)*100)","57beaeac":"pred = model.predict(X_train)\npred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\nprint(classification_report(y_train ,pred ))\nprint('Confusion Matrix: \\n',confusion_matrix(y_train,pred))\nprint()\nprint('Accuracy: ', accuracy_score(y_train,pred))\nprint()","13b1dd63":"from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\npred = model.predict(X_test)\npred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\nprint(classification_report(y_test ,pred ))\nprint('Confusion Matrix: \\n',confusion_matrix(y_test,pred))\nprint()\nprint('Accuracy: ', accuracy_score(y_test,pred))\nprint()","8a51df5c":"model.evaluate(X_test, y_test)[1]\n","417388ab":"# Prepare Dataset","3c37fd33":"**2. Neuron Number Optimization**\n\n* Neuron numbers to test are 28 24 20 16 12  and will decrease 1\/3 each layer \n* Others are same with above\n\n![image.png](attachment:image.png)","4e6c6b1c":"**3. Activation Function Optimization**\n\n\nFunctions to test are:\n\n* relu function\n* sigmoid function\n* softmax function\n* tanh function\n\n\n\nLimits:\n* 2 Hidden layers\n* Layers decrease 1\/3 from 30\n* First function is relu, last function is sigmoid\n* Other conditions are same","e4069779":"**1. Layer Optimization 1**\n\n> There is currently no theoretical reason to use neural networks with any more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer.\n\nConstants:\n\n* Max Layer count: 5\n* Neuron counts will are same : 6\n* Last layer will use sigmoid function others are relu\n* epoch=32, batch_size=32\n* optimizer=adam, loss=binary_crossentropy ","d6ab9ea9":"# Define Models To Test These\n\n(s\u0131ralama de\u011fi\u015fecek)\n\n1. Layer Optimization\n2. Neuron Number Optimization\n3. Activation Func\n4. Optimizer Optimization\n\n\n\n","5494027c":"**4. Optimizer Optimization**","021a9e2d":"**5. Learning Rate optimization**","57e77b59":"# Final Version\n\n\n**Neler \u00f6\u011frendik?**\n\n* Layer say\u0131lar\u0131n\u0131\n* N\u00f6ron say\u0131s\u0131 belirleme\n..."}}