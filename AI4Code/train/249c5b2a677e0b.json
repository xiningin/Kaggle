{"cell_type":{"da1f22d9":"code","ca9fcfff":"code","c7d87110":"code","4d136625":"code","df30e52d":"code","9c2e6a03":"code","39274d77":"code","32106616":"code","09334025":"code","5efeb429":"code","8a5dbabe":"code","bb58a07f":"code","2237cd95":"code","8a421b5d":"code","cd4f3ff4":"code","5bcf1d5e":"code","2905d125":"code","d19a88e8":"code","0ab052f0":"code","d3b9af20":"code","9b6aee1b":"code","839bdb9a":"code","ad8c4451":"code","2298f205":"code","205a6048":"code","3c5bdf00":"code","cdbade25":"code","28efc8b8":"code","1e820cfa":"code","e8272979":"code","4bf92eb1":"code","0a54ff66":"code","588b7a4d":"code","13817968":"code","08e86720":"code","0f082b82":"markdown","cc33114c":"markdown","61ccc3e6":"markdown","06d31039":"markdown","cd8988f2":"markdown","4ad147de":"markdown","41ea6bc4":"markdown","242cf35c":"markdown","bead80a5":"markdown","2137f580":"markdown","c121872b":"markdown","70b01bfe":"markdown","2cc250fa":"markdown","8da6415b":"markdown","5480f177":"markdown","a3bcf566":"markdown","23fedc43":"markdown","98e7cf25":"markdown","a2b188a8":"markdown"},"source":{"da1f22d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ca9fcfff":"# Import numpy, pandas, matpltlib.pyplot, sklearn modules and seaborn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\nplt.style.use('ggplot')\n\n# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc","c7d87110":"# Import the data\ndf = pd.read_csv('\/kaggle\/input\/us-accidents\/US_Accidents_May19.csv')\ndf.info()","4d136625":"# Convert Start_Time and End_Time to datetypes\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\ndf['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n\n# Extract year, month, day, hour and weekday\ndf['Year']=df['Start_Time'].dt.year\ndf['Month']=df['Start_Time'].dt.strftime('%b')\ndf['Day']=df['Start_Time'].dt.day\ndf['Hour']=df['Start_Time'].dt.hour\ndf['Weekday']=df['Start_Time'].dt.strftime('%a')\n\n# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\ntd='Time_Duration(min)'\ndf[td]=round((df['End_Time']-df['Start_Time'])\/np.timedelta64(1,'m'))\ndf.info()","df30e52d":"# Check if there is any negative time_duration values\ndf[td][df[td]<=0]","9c2e6a03":"# Drop the rows with td<0\n\nneg_outliers=df[td]<=0\n\n# Set outliers to NAN\ndf[neg_outliers] = np.nan\n\n# Drop rows with negative td\ndf.dropna(subset=[td],axis=0,inplace=True)\ndf.info()","39274d77":"# Double check to make sure no more negative td\ndf[td][df[td]<=0]","32106616":"# Remove outliers for Time_Duration(min): n * standard_deviation (n=3), backfill with median\n\nn=3\n\nmedian = df[td].median()\nstd = df[td].std()\noutliers = (df[td] - median).abs() > std*n\n\n# Set outliers to NAN\ndf[outliers] = np.nan\n\n# Fill NAN with median\ndf[td].fillna(median, inplace=True)\n\ndf.info()","09334025":"# Print time_duration information\nprint('Max time to clear an accident: {} minutes or {} hours or {} days; Min to clear an accident td: {} minutes.'.format(df[td].max(),round(df[td].max()\/60), round(df[td].max()\/60\/24), df[td].min()))","5efeb429":"# Export the data\n# df.to_csv('.\/US_Accidents_May19_clean.csv',index=False)","8a5dbabe":"# Set the list of features to include in Machine Learning\nfeature_lst=['Source','TMC','Severity','Start_Lng','Start_Lat','Distance(mi)','Side','City','County','State','Timezone','Temperature(F)','Humidity(%)','Pressure(in)', 'Visibility(mi)', 'Wind_Direction','Weather_Condition','Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop','Sunrise_Sunset','Hour','Weekday', 'Time_Duration(min)']","bb58a07f":"# Select the dataset to include only the selected features\ndf_sel=df[feature_lst].copy()\ndf_sel.info()","2237cd95":"# Export the data with selected features\n# df_sel.to_csv('.\/US_Accidents_May19_clean_sel.csv',index=False)","8a421b5d":"# Check missing values\ndf_sel.isnull().mean()","cd4f3ff4":"df_sel.dropna(subset=df_sel.columns[df_sel.isnull().mean()!=0], how='any', axis=0, inplace=True)\ndf_sel.shape","5bcf1d5e":"# Export the data with selected features\n# df_sel.to_csv('.\/US_Accidents_May19_clean_sel_dropna.csv',index=False)","2905d125":"# Import data if it was already exported based on previous work\n# df_sel=pd.read_csv('.\/US_Accidents_May19_clean_sel_dropna.csv')","d19a88e8":"# Set state\nstate='PA'\n\n# Select the state of Pennsylvania\ndf_state=df_sel.loc[df_sel.State==state].copy()\ndf_state.drop('State',axis=1, inplace=True)\ndf_state.info()","0ab052f0":"# Map of accidents, color code by county\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=df_state, hue='County', legend=False, s=20)\nplt.show()","d3b9af20":"# Set county\ncounty='Montgomery'\n\n# Select the state of Pennsylvania\ndf_county=df_state.loc[df_state.County==county].copy()\ndf_county.drop('County',axis=1, inplace=True)\ndf_county.info()","9b6aee1b":"# Map of accidents, color code by city\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=df_county, hue='City', legend=False, s=20)\nplt.show()","839bdb9a":"# Generate dummies for categorical data\ndf_county_dummy = pd.get_dummies(df_county,drop_first=True)\n\n# Export data\n# df_county_dummy.to_csv('.\/US_Accidents_May19_{}_dummy.csv'.format(state),index=False)\n\ndf_county_dummy.info()","ad8c4451":"# Assign the data\ndf=df_county_dummy\n\n# Set the target for the prediction\ntarget='Severity'\n\n\n# Create arrays for the features and the response variable\n\n# set X and y\ny = df[target]\nX = df.drop(target, axis=1)\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)","2298f205":"# List of classification algorithms\nalgo_lst=['Logistic Regression',' K-Nearest Neighbors','Decision Trees','Random Forest']\n\n# Initialize an empty list for the accuracy for each algorithm\naccuracy_lst=[]","205a6048":"# Logistic regression\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))","3c5bdf00":"# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint('[K-Nearest Neighbors (KNN)] knn.score: {:.3f}.'.format(knn.score(X_test, y_test)))\nprint('[K-Nearest Neighbors (KNN)] accuracy_score: {:.3f}.'.format(acc))","cdbade25":"# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, n_neighbor in enumerate(neighbors):\n    \n    # Setup a k-NN Classifier with n_neighbor\n    knn = KNeighborsClassifier(n_neighbors=n_neighbor)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n","28efc8b8":"# Decision tree algorithm\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_test, y_pred)\n\n\n# Print accuracy_entropy\nprint('[Decision Tree -- entropy] accuracy_score: {:.3f}.'.format(accuracy_entropy))\n\n\n\n# Instantiate dt_gini, set 'gini' as the information criterion\ndt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_gini.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_gini.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_gini = accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\nacc=accuracy_gini\naccuracy_lst.append(acc)\n\n# Print accuracy_gini\nprint('[Decision Tree -- gini] accuracy_score: {:.3f}.'.format(accuracy_gini))","1e820cfa":"# Random Forest algorithm\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Randon forest algorithm] accuracy_score: {:.3f}.\".format(acc))\n","e8272979":"feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n\n# Creating a bar plot, displaying only the top k features\nk=10\nsns.barplot(x=feature_imp[:10], y=feature_imp.index[:k])\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","4bf92eb1":"# List top k important features\nk=20\nfeature_imp.sort_values(ascending=False)[:k]","0a54ff66":"# Create a selector object that will use the random forest classifier to identify\n# features that have an importance of more than 0.03\nsfm = SelectFromModel(clf, threshold=0.03)\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\nfeat_labels=X.columns\n\n# Print the names of the most important features\nfor feature_list_index in sfm.get_support(indices=True):\n    print(feat_labels[feature_list_index])","588b7a4d":"# Transform the data to create a new dataset containing only the most important features\n# Note: We have to apply the transform to both the training X and test X data.\nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)\n\n# Create a new random forest classifier for the most important features\nclf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n\n# Train the new classifier on the new dataset containing the most important features\nclf_important.fit(X_important_train, y_train)","13817968":"# Apply The Full Featured Classifier To The Test Data\ny_pred = clf.predict(X_test)\n\n# View The Accuracy Of Our Full Feature Model\nprint('[Randon forest algorithm -- Full feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))\n\n# Apply The Full Featured Classifier To The Test Data\ny_important_pred = clf_important.predict(X_important_test)\n\n# View The Accuracy Of Our Limited Feature Model\nprint('[Randon forest algorithm -- Limited feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_important_pred)))","08e86720":"# Make a plot of the accuracy scores for different algorithms\n\n# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_acc=pd.DataFrame(list(zip(algo_lst, accuracy_lst)), columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'],ascending = True)\n\n# Export to a file\ndf_acc.to_csv('.\/Accuracy_scores_algorithms_{}.csv'.format(state),index=False)\n\n# Make a plot\nax=df_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='0.5')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.1)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)\nplt.title('[{}-{}] Which algorithm is better?'.format(state, county))\n\nplt.show()","0f082b82":"### Step 4. Deal with outliers\n\n#### B. Fill outliers with median values","cc33114c":"### Step 5. Select a list of features for machine learning algorithms\n\n Only select relavant columns without overwhelming the computer","61ccc3e6":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n#### Data preparation: train_test_split","06d31039":"### Step 1. Import libraries","cd8988f2":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm B. The K-Nearest Neighbors (KNN) algorithm\n   ##### KNN with 6 neighors","4ad147de":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm B. The K-Nearest Neighbors (KNN) algorithm\n   ##### Optmize the number of neighors: plot the accuracy versus number of neighbors","41ea6bc4":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm C. Decision Tree                 ","242cf35c":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n#### Plot the accuracy score versus algorithm","bead80a5":"# How You Can Avoid Car Accident in 2020\nsee post on medium.com:\nhttps:\/\/medium.com\/@RonghuiZhou\/how-you-can-avoid-car-accident-in-2020-c9626c9b6f68\n\n\n### Use LR\/KNN\/Decision Tree\/Random Forest classification algorithms from sklearn to predict the accident severity\n\nDue to the limit of computer capacity, I am focusing on Montgomery County in the State of Pennsylvania. I will only select a few features I believe are more relavant to severity. Categorical data will be treated with Pandas get_dummies method. Rows with missing values will be dropped.\n\n### Data source\nhttps:\/\/www.kaggle.com\/sobhanmoosavi\/us-accidents\n\n\n### Acknowledgements\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. \u201cA Countrywide Traffic Accident Dataset.\u201d, 2019.\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.\n\n\n####  Author\nRonghui Zhou, zhou.uf@gmail.com\nhttps:\/\/github.com\/RonghuiZhou","2137f580":"### Step 8. Deal with categorical data: pd.get_dummies()\n","c121872b":"### Step 7. Select the state of interest: PA; and County of interest: Montgomery\n   \n\nDue to the limitation of personal laptop, the whole US dataset is too big to handle","70b01bfe":"### Step 4. Deal with outliers\n\n#### A. Drop rows with negative time_duration","2cc250fa":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm D. Random Forest   \n   ##### Select the top important features, set the threshold      ","8da6415b":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm D. Random Forest   \n   ##### Visualize important features      ","5480f177":"### Step 6. Drop rows with missing values","a3bcf566":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm A. Logistic regression              ","23fedc43":"### Step 2. Import the dataset","98e7cf25":"### Step 3. Extract year, month, day, hour, weekday, and time to clear accidents","a2b188a8":"### Step 9. Predict the accident severity with various supervised machine learning algorithms\n\n   #### Algorithm D. Random Forest   \n   ##### n_estimators=100                 "}}