{"cell_type":{"78898cbb":"code","d33a79fe":"code","ff6b45c4":"code","4e0da440":"code","3e1b59a5":"code","423aa691":"code","25606719":"code","9cb66554":"code","d96a04e1":"code","3bd5e03f":"code","211f6943":"code","e922de34":"code","f118b5e1":"code","653f2251":"code","0b838e18":"markdown","fa05f924":"markdown","8aa6e8d9":"markdown","7322cab8":"markdown","57424ab8":"markdown","f08e4c19":"markdown","e6bcd8f3":"markdown","7472bbb0":"markdown","d27e2062":"markdown","14a63815":"markdown","68896999":"markdown","44b89c7e":"markdown","07694d8b":"markdown"},"source":{"78898cbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d33a79fe":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","ff6b45c4":"train_data.describe()","4e0da440":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef visualize_null_data(df,Title):\n    miss = df.isnull().sum()\/len(df)\n    miss = miss[miss> 0]*100\n    miss.sort_values(inplace=True)\n    print(miss)\n\n    #visualising missing values\n    miss = miss.to_frame()\n    miss.columns = ['count']\n    miss.index.names = ['Name']\n    miss['Name'] = miss.index\n\n    #plot the missing value count\n    sns.set(style=\"whitegrid\", color_codes=True)\n    sns.barplot(x = 'Name', y = 'count', data=miss)\n    plt.xticks(rotation = 90)\n    plt.title(Title)\n    plt.show()","3e1b59a5":"visualize_null_data(train_data,\"Titanic Training Data - Null Value %\")","423aa691":"#train_data[train_data.Sex=='male']['Age'].fillna(train_data[train_data.Sex=='male']['Age'].median())\ntrain_data.loc[(train_data.Sex=='male') & (train_data['Age'].isnull()),'Age']=train_data[train_data.Sex=='male']['Age'].median()\ntrain_data.loc[(train_data.Sex=='female') & (train_data['Age'].isnull()),'Age']=train_data[train_data.Sex=='female']['Age'].median()\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0],inplace=True)","25606719":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","9cb66554":"visualize_null_data(test_data,\"Titanic Test Data Null Values %\")","d96a04e1":"test_data.loc[(test_data.Sex=='male') & (test_data['Age'].isnull()),'Age']=test_data[test_data.Sex=='male']['Age'].median()\ntest_data.loc[(test_data.Sex=='female') & (test_data['Age'].isnull()),'Age']=test_data[test_data.Sex=='female']['Age'].median()\ntest_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].mean())","3bd5e03f":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)","211f6943":"#Exploring explainable\nimport lime\nfrom lime import lime_tabular\n\ninterpretor=lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X),\n    feature_names=X.columns,\n    mode='classification'\n)","e922de34":"ForExplainableData=X.copy()\nForExplainableData.insert(0,'Survived',y)\nprint(ForExplainableData)","f118b5e1":"exp=interpretor.explain_instance(\n    data_row=ForExplainableData.iloc[0][1:],\n    predict_fn=model.predict_proba\n)\nexp.show_in_notebook(show_table=True)","653f2251":"exp=interpretor.explain_instance(\n    data_row=ForExplainableData.iloc[1][1:],\n    predict_fn=model.predict_proba\n)\nexp.show_in_notebook(show_table=True)","0b838e18":"# Now we have reached the core section.\n\nLIME stands for Local Interpretable Model-agnostic Explanations. It was developed by Marco Ribeiro in 2016.  It helps in explaining predictions of Machine Learning models.\n\nLet us pass our input data (X) and the feature names to LimeTabularExplainer method. ","fa05f924":"Let us read the training and test data.  *I will give minimum write up for the standard part* i.e. up to model building. **Will try to do more discussion for the Explainable AI part** ","8aa6e8d9":"Let us look into the above data.  We can take 1st and 2nd rows. 1st passenger could not survive and 2nd passenger survived.\n\nNow we can use explain_instance method to interpret and analyze further.  Let us pass the first row to the explain_instance method and view the outcome using the show_in_notebook method.  Please note, this method takes the RFC ML Model that we have built earlier and predicts the output based on that model.","7322cab8":"Let us replace null values in both Fare & Age columns. For Fare column let us impute it using Mean.","57424ab8":"Now let us analyze for the second record, where passenger has survived\n\nAs per the above:\n\n**1. Survival rate is high:**\n* If the passenger is Female\n* If the passenger has travelled in 1st or 2nd class (pclass)\n* If passenger has travelled with one sibling \/ spouse (SibSp factor)\n\n\n**2. Survival rate is low:**\n* If the # of parents \/ children aboard the Titanic for the passenger is <= 0 (Parch factor)\n\nNow for our second record as it is indicating above, this passenger was a Female passenger and travelled in 1st class, travelled with one sibling \/ spouse. Hence above method computed that 97% this passenger would have survived. \n\nOnly one factor that does  not influence the survival is Parch.  # of parents \/ children aboard for this passenger was 0.  Hence only 3% that this passenger could not have survived.\n\n**This very clearly explains what exactly happens behind the scene in Random Forest Classifer to do the classification.**","f08e4c19":"Let us take the copy of the input data in another df.  Add the target column (i.e. Survived column in it).","e6bcd8f3":"Hope you liked this Code Snippet and the brief explanations.\n\n**I found this useful as it helped me to visualize how the RFC is doing the classification.  Hope you too find it useful.**\n\n## If you like it please upvote!!!!\n\nThis is continued in Part 2. Please click [here](https:\/\/www.kaggle.com\/rajamykaggle\/titanic-explainable-ml-code-part2) to go to Part 2.","7472bbb0":"Let us visualize the columns that has Null Values in the Training and Test Data.  Let us write a method to do the same.","d27e2062":"Let us read the Test data and populate missing values.","14a63815":"# **Explainable Machine Learning**\n\nDear All\n\nThis is my first code that I am sharing in this forum.  If you like this, ***kindly upvote that motivates*** me to continue writing and sharing.\n\nOne of the **key challenges in the Machine Learning** ***is the explainability of the ML model***. When we show the outcome of the model, how customer will understand what are the factors that are influencing the final outcome (i.e. prediction done by ML Model).\n\nCustomer could see this as a black box.  For example, let us say our ML model is predicting the end customer who will subscribe for a particular banking product.  When we show our ML Model result to our customer, their question could be, out of the given factors (dependent variables), which are the influencing factors that will help us to decide whether a particular end customer will subscribe to a product or not.\n\nAs a data scientist how do we answer to this question?\n\n**Explainable AI helps to answer above question.**\n\nRecently I have learned about this & thought to share that knowledge with all of you.\n\nWe will experiment the same in this Titanic Data.\n\nLet us start.","68896999":"It is very nicely shown in the above which are the influencing factors useful for prediction.\n\nAs per the above:\n\n**1. Survival rate is low:**\n* If the passenger is Male\n* If the passenger has travelled in 3rd class (pclass)\n* If the # of parents \/ children aboard the Titanic for the passenger is <= 0 (Parch factor)\n\n\n**2. Survival rate is high:**\n* If passenger has travelled with one sibling \/ spouse (SibSp factor)\n\nNow for our first record as it is indicating above, this passenger was a Male passenger and travelled in 3rd class, # of parents \/ children aboard the Titanic was zero. Hence above method computed that 86% this passenger would not have survived. \n\nOnly one factor that influences the survival is SibSp.  # of siblings \/ spouses aboard the Titanic for this passenger was 1.  Hence Survival % is 14% for this customer.\n\n**This very clearly explains what exactly happens behind the scene in Random Forest Classifer to predict the classification.**","44b89c7e":"Based on the above chart we can see that Embarked, Age and Cabin columns are having null values.  We can look into the imputation for Cabin data later as it is having huge amount of Null Values.\n\n* For Embarked column, let us impute using Mode.\n* For Age column, let us impute it using Median for Male and Female Separately.","07694d8b":"Let us build the **Random Forest Classifier (RFC)** using the \"Pclass\", \"Sex\", \"SibSp\", \"Parch\" features."}}