{"cell_type":{"d475d522":"code","be4afafb":"code","b51f2f26":"code","1bc09c6f":"code","93f82e14":"code","79351f8c":"code","3a0d4c85":"code","53cdfc20":"code","240da477":"code","d688bc24":"code","070911f3":"code","275502e3":"code","c22fd5da":"code","ec6a403c":"code","ecabc12b":"code","8279899c":"code","53248213":"code","fa3f9129":"code","fce0ae08":"code","c6b66ded":"code","cefebe1d":"code","a6abd2e5":"code","a6070c4d":"code","a6699c53":"code","a79ca473":"code","2b510adc":"code","f82785f2":"code","a2f7b12b":"code","fd953343":"code","acc0d6ea":"code","36363ba4":"code","6304394e":"code","29c57e58":"code","6118c732":"code","e57bc177":"code","cdb0508a":"code","097c9eb4":"code","274a12ad":"code","359fe055":"code","73e8c3e9":"code","e080c175":"code","8e6fc74b":"code","ca284f5f":"code","9d528b10":"code","1b388496":"code","250edac2":"code","e5dba65f":"code","a8add06b":"code","06b9c85d":"code","6cc9af1a":"code","7fc6041b":"code","11eb2190":"code","5d05b992":"code","1100281e":"code","fcc3fd5d":"code","412f6b3f":"code","9cdd5b59":"code","5cb545a1":"code","c8cdc373":"code","efe9e779":"code","101b5ed8":"code","ba7b2ed9":"code","4ea7c71b":"code","373cf11f":"code","1f06f4e0":"code","34901dfb":"code","7c49a575":"code","1e453fa0":"code","74b42890":"code","0ea979ee":"code","ad1ffa21":"code","962368e2":"code","dfc4415e":"code","81bd485a":"code","c68ad1af":"code","76f44ce8":"code","a66410ed":"code","8b22d017":"code","5ae904c7":"code","0bfc7e1c":"code","9e25773f":"code","fb661634":"code","a0973bf7":"code","cdf95977":"code","1c7fb78e":"code","0b4a5348":"code","3359fc09":"code","cc8535a0":"code","07d60b91":"code","6b0d8cc4":"code","fb63805b":"code","a525df1d":"code","f40979ed":"code","8ca7703a":"code","83fd1d9c":"code","eb877d3d":"markdown","0d176fa6":"markdown","1a0c6f67":"markdown","5dc6ec14":"markdown","51eec584":"markdown","11fcc0ae":"markdown","20f26038":"markdown","a4c76a29":"markdown","eec5b530":"markdown","76b001ab":"markdown","477eaeb1":"markdown","e6625571":"markdown","1087fe34":"markdown","c763b8c6":"markdown","6308e33a":"markdown","8b2240a1":"markdown","e0c66ef1":"markdown","3a7f9960":"markdown","d032ff37":"markdown","0b4b5c3c":"markdown","e60e0fcc":"markdown","f9b0e206":"markdown","0f0e3a27":"markdown","4ff5e38e":"markdown","320b5586":"markdown"},"source":{"d475d522":"import numpy as np\nimport re\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom transformers import BertModel, BertTokenizer\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import DistilBertTokenizer, DistilBertModel\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport time\nfrom sklearn.metrics import silhouette_score\nimport pickle","be4afafb":"!nvidia-smi","b51f2f26":"!pip install kmeanstf --quiet","1bc09c6f":"# reading files\ncategories = list()\nlabels_all = list()\nscores = list()\n\nwith open('..\/input\/hyppr-images-mapping\/categories.txt', 'r') as file:\n    for line in file.readlines():\n        categories.append(line.lower().rstrip().split(','))\n        \nwith open('..\/input\/hyppr-images-mapping\/labels.txt', 'r') as file:\n    for line in file.readlines():\n        labels_all.append(line.rstrip(',\\n').split(','))\n\nwith open('..\/input\/hyppr-images-mapping\/scores.txt', 'r') as file:\n    for line in file.readlines():\n        scores.append(line.rstrip(',\\n').split(','))\n        \n\nwith open(\"..\/input\/hyppr-images-mapping\/all_ids.p\", \"rb\") as f:\n    all_ids = pickle.load(f)","93f82e14":"labels = [lab for label in labels_all for lab in label]\nlabels = list(set(labels))\ncats = [cat[0] for cat in categories]","79351f8c":"labels_n = [\" \".join(my_list) for my_list in labels_all.copy()]\nlabels_n = [\" \".join(re.split(' |-', my_list)) for my_list in labels_n]\n\ncategories_n = [\" \".join(my_list) for my_list in categories.copy()]","3a0d4c85":"# FEATURE OF HUGGINSFACE FOR FAST EMBEDDINGS BUILDING. NOT USING IT NOW\n\n# from transformers import pipeline\n\n# nlp_features = pipeline('feature-extraction')\n# out = nlp_features(labels_n)\n\n# out = np.array(out)\n\n# out = out[:, 8, :]\n\n# a, b = 123, 36\n# print(labels[a], labels[b])\n# cosine_similarity([out[a]], [out[b]])","53cdfc20":"model = BertModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nr_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nr_model = RobertaModel.from_pretrained('roberta-base')","240da477":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained('distilbert-base-uncased')","d688bc24":"tokenized_categories = tokenizer(categories_n, padding = True, truncation = True, return_tensors = 'pt')\n\nids = tokenized_categories['input_ids']\nmask = tokenized_categories['attention_mask']   \n\nmodel.cpu()\n\noutput = model(ids, mask)\nfinal_layer = output.last_hidden_state\n\ncat_embeddings = final_layer.detach().numpy()\n\ncat_embeddings = cat_embeddings[:, 7, :]","070911f3":"tokenized_labels = tokenizer(labels_n, padding = True, truncation = True, return_tensors = 'pt')\n\ninput_ids = tokenized_labels['input_ids']\nmasks = tokenized_labels['attention_mask']","275502e3":"# since there is a lot of sentences we need to batch it \nbatch_size = 16\n\ntrain_data = TensorDataset(input_ids, masks)\n#train_sampler = RandomSampler(train_data)\ntrain_dl = DataLoader(train_data, batch_size=batch_size)\n\nmodel.cuda()\nout_list = []\nmodel.eval()\nwith torch.no_grad():\n    for d in train_dl:\n        d = [i.cuda() for i in d[:3]]\n        out_list.append(model(*d)[0][:, 7, :].cpu()) \n    del d\n    \nlabels_embeddings = torch.cat(out_list)\nlabels_embeddings = labels_embeddings.numpy() ","c22fd5da":"tokenized_categories = r_tokenizer(categories_n, padding = True, truncation = True, return_tensors = 'pt')\n\nids = tokenized_categories['input_ids']\nmask = tokenized_categories['attention_mask']   \n\nr_model.cpu()\noutput = r_model(ids, mask)\nfinal_layer = output.last_hidden_state\n\ncat_embeddings = final_layer.detach().numpy()\n\ncat_embeddings = cat_embeddings[:, 8, :]","ec6a403c":"tokenized_labels = r_tokenizer(labels_n, return_tensors=\"pt\", padding = True)\n\ninput_ids = tokenized_labels['input_ids']\nmasks = tokenized_labels['attention_mask']\n\n# since there is a lot of sentences we need to batch it \nbatch_size = 32\n\ntrain_data = TensorDataset(input_ids, masks)\ntrain_dl = DataLoader(train_data, batch_size=batch_size)\n\nr_model.cuda()\nout_list = []\nr_model.eval()\nwith torch.no_grad():\n    for d in train_dl:\n        d = [i.cuda() for i in d[:3]]\n        out_list.append(r_model(*d)[0][:, 8, :].cpu()) \n    del d\n    \nlabels_embeddings = torch.cat(out_list)\nlabels_embeddings = labels_embeddings.numpy() \nlabels_embeddings.shape","ecabc12b":"from sklearn.preprocessing import Normalizer\nimport umap\nimport matplotlib.pyplot as plt\n\ntr_l_e = Normalizer().fit_transform(labels_embeddings)\ntr_c_e = Normalizer().fit_transform(cat_embeddings)\n\nreducer = umap.UMAP()\n%time lab_emb_2d = reducer.fit_transform(tr_l_e)\n\nplt.scatter(\n    lab_emb_2d[:, 0],\n    lab_emb_2d[:, 1])","8279899c":"labels_embeddings = tr_l_e.copy()\ncat_embeddings = tr_c_e.copy()","53248213":"cosine_similarity([labels_embeddings[0]], [labels_embeddings[12]])","fa3f9129":"cosine_similarity(cat_embeddings)","fce0ae08":"dict_embeddings = {}\nfor i in range(len(labels)):\n    dict_embeddings.update({labels[i]:labels_embeddings[i]})","c6b66ded":"n = list(dict_embeddings.values())[0].shape[0]\n\nimages_embeddings = [] # we will find embeddings for every image\nouter_elements = 0\nfor image_ind in range(len(labels_all)): # going on list by lists, where last \"lists\" are images\n    result_embedding = np.zeros(n)\n    norm = np.zeros(n)\n    for im_label in range(len(labels_all[image_ind])):\n        try:\n            emb_dot_score = np.dot(dict_embeddings[labels_all[image_ind][im_label]], float(scores[image_ind][im_label])) # vector on scalar -> vector\n            norm = np.add(norm, dict_embeddings[labels_all[image_ind][im_label]] )#normalizing each component: find sum of vectors what is vector, than divide inner by norm\n        except: # list index out of range\n            outer_elements += 1\n            \n        result_embedding = np.add(result_embedding,emb_dot_score)\n    result_embedding = np.divide(result_embedding, norm)\n    images_embeddings.append(result_embedding)\nimages_embeddings = np.array(images_embeddings)\n\nprint('Number of list index out of range elements: {}'.format(outer_elements))","cefebe1d":"labels_embeddings = images_embeddings\nlabels_embeddings","a6abd2e5":"from sklearn.decomposition import PCA\npca = PCA(n_components = 150)\n\n%time pca_labels = pca.fit_transform(labels_embeddings)","a6070c4d":"import matplotlib.pyplot as plt\n\npca_2d = PCA(n_components = 2)\n%time lab_emb_2d = pca_2d.fit_transform(labels_embeddings)\n\nplt.scatter(\n    lab_emb_2d[:, 0],\n    lab_emb_2d[:, 1])","a6699c53":"import umap\nimport matplotlib.pyplot as plt\n\nreducer = umap.UMAP()\n%time lab_emb_2d = reducer.fit_transform(labels_embeddings)\n\nlab_emb_2d.shape\n\nplt.scatter(\n    lab_emb_2d[:, 0],\n    lab_emb_2d[:, 1])","a79ca473":"def draw_umap(data = labels_embeddings, n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine', title=''):\n    fit = umap.UMAP(\n        n_neighbors=n_neighbors,\n        min_dist=min_dist,\n        n_components=n_components,\n        metric=metric\n    )\n    \n    start_time = time.time()\n    u = fit.fit_transform(data)\n    end_time = time.time()\n    print('UMAP execution time in seconds: {}'.format(end_time - start_time))\n    \n    fig = plt.figure()\n    if n_components == 1:\n        ax = fig.add_subplot(111)\n        ax.scatter(u[:,0], range(len(u)), c=data)\n    if n_components == 2:\n        ax = fig.add_subplot(111)\n        ax.scatter(u[:,0], u[:,1])\n    if n_components == 3:\n        ax = fig.add_subplot(111, projection='3d')\n        ax.scatter(u[:,0], u[:,1], u[:,2], c=data, s=100)\n    plt.title(title, fontsize=18)","2b510adc":"for n in (5,10, 20, 35, 50):\n    draw_umap(n_neighbors=n, title='n_neighbors = {}'.format(n))","f82785f2":"for d in (0.0, 0.1, 0.25, 0.5, 0.8, 0.99):\n    draw_umap(min_dist=d, title='min_dist = {}'.format(d))","a2f7b12b":"draw_umap(n_neighbors=5, min_dist = 0.05, title='Final result')","fd953343":"reducer = umap.UMAP(n_components = 5, n_neighbors = 15,min_dist = 0.0, metric = 'cosine')\nlab_emb_150d = reducer.fit_transform(labels_embeddings)","acc0d6ea":"cosine_similarity([lab_emb_150d[0]], [lab_emb_150d[0]])","36363ba4":"cosine_similarity([labels_embeddings[0]], [labels_embeddings[4]])","6304394e":"dict_embeddings = dict()\nfor i in range(len(all_ids)):\n    dict_embeddings[all_ids[i]] = lab_emb_150d[i]","29c57e58":"with open(\"..\/input\/hyppr-images-mapping\/objid_postid.p\", \"rb\") as f:\n    objid_postid = pickle.load(f)\n# reverse:\npostid_objid = dict()\nfor objid, postid in objid_postid.items():\n    set_key(postid_objid, postid, objid)","6118c732":"with open('..\/input\/hyppr-images-mapping\/category_to_posts_vision.p', 'rb') as f:   # opening given model\n    category_to_posts = pickle.load(f)\ncategory_to_posts = dict(category_to_posts)\n\nposts_to_category = dict()\nfor cat, posts in category_to_posts.items():\n    for post in posts:\n        if post not in posts_to_category.keys():\n            set_key(posts_to_category, post, cat)","e57bc177":"import tensorflow as tf\nfrom kmeanstf import KMeansTF\nfrom sklearn.metrics import silhouette_score","cdb0508a":"silhouettes_umap_f = []\nsilhouettes_pca_f = []\n\ndistortions_f = [] # making this for using elbow method in next cells\n\nK = range(2,30,2)\nfor k in K:\n    start_time = time.time()\n    kmeanstf = KMeansTF(n_clusters = k, random_state = 21)\n    \n    labels_embeddings_tf = tf.convert_to_tensor(lab_emb_150d)  # learning on umap first\n    kmeanstf.fit(labels_embeddings_tf)\n    silhouettes_umap_f.append(silhouette_score(lab_emb_150d, kmeanstf.labels_))\n    \n    distortions_f.append(kmeanstf.inertia_)\n    \n    labels_embeddings_tf = tf.convert_to_tensor(pca_labels)  # learning on pca\n    kmeanstf.fit(labels_embeddings_tf)\n    silhouettes_pca_f.append(silhouette_score(pca_labels, kmeanstf.labels_))\n    \n    end_time = time.time()\n    print('KmeansTF execution time in seconds: {}'.format(end_time - start_time))","097c9eb4":"fig, ax = plt.subplots(1, 2, figsize = (16, 8), sharey = True)\nax[0].plot(K, silhouettes_umap_f, 'yx-')\nax[0].set(xlabel = 'k', ylabel = 'Silhouette score', title = 'Silhouette for UMAP, k = 2 - 30, n_neighbors = 5, min_dist = 0.1')\n# plt.yscale('log')\nax[1].plot(K, silhouettes_pca_f, 'cx-')\nax[1].set(xlabel = 'k', ylabel = 'Silhouette score', title = 'Silhouette for PCA, k = 2 - 30')\nplt.savefig('silhouettes_comparison.pdf', bbox_inches='tight')","274a12ad":"silhouettes_umap_s = []\nsilhouettes_pca_s = []\n\ndistortions_s = []\n\nK = range(30,150,3)\nfor k in K:\n    start_time = time.time()\n    kmeanstf = KMeansTF(n_clusters = k, random_state = 21)\n    \n    labels_embeddings_tf = tf.convert_to_tensor(lab_emb_150d)  # learning umap first\n    kmeanstf.fit(labels_embeddings_tf)\n    silhouettes_umap_s.append(silhouette_score(lab_emb_150d, kmeanstf.labels_))\n    \n    distortions_s.append(kmeanstf.inertia_)\n    \n    labels_embeddings_tf = tf.convert_to_tensor(pca_labels)  # learning pca\n    kmeanstf.fit(labels_embeddings_tf)\n    silhouettes_pca_s.append(silhouette_score(pca_labels, kmeanstf.labels_))\n    \n    end_time = time.time()\n    print('KmeansTF execution time in seconds: {}'.format(end_time - start_time))","359fe055":"fig, ax = plt.subplots(1, 2, figsize = (16, 8), sharey = True)\nax[0].plot(K, silhouettes_umap_s, 'yx-')\nax[0].set(xlabel = 'k', ylabel = 'Silhouette score', title = 'Silhouette for UMAP, k = 30 - 150, n_neighbors = 5, min_dist = 0.1')\n# plt.yscale('log')\nax[1].plot(K, silhouettes_pca_s, 'cx-')\nax[1].set(xlabel = 'k', ylabel = 'Silhouette score', title = 'Silhouette for PCA, k = 30 - 150.')\nplt.savefig('silhouettes_comparison2.pdf', bbox_inches='tight')","73e8c3e9":"#lab_emb_150d = labels_embeddings.copy() # using PCA finally # now we wont do that since PCA works bad\n#cat_emb_150d = cat_embeddings.copy()","e080c175":"fig, ax = plt.subplots(1, 2, figsize = (16, 8))\nax[0].plot(range(2,30,2), distortions_f, 'bx-')\nax[0].set(xlabel = 'k', ylabel = 'Distortion', title = 'The Elbow Method. k = ')\nax[0].set_yscale(\"log\")\nax[1].plot(range(2,30,2), silhouettes_umap_f, 'rx-')\nax[1].set(xlabel = 'k', ylabel = 'Silhouette score', title = 'The Average Silhouette Method')\nplt.savefig('elbowSilhouette1.pdf', bbox_inches='tight')","8e6fc74b":"max_silh_id = np.argmax(silhouettes_umap_f)\nmax_silh = silhouettes_umap_f[max_silh_id]","ca284f5f":"fig, ax = plt.subplots(1, 2, figsize = (16, 8))\nax[0].plot(K, distortions_s, 'bx-')\nax[0].set(xlabel = 'k', ylabel = 'Distortion', title = 'The Elbow Method')\nax[0].set_yscale('log')\nax[1].plot(K, silhouettes_umap_s, 'rx-')\nax[1].set(xlabel = 'k', ylabel = 'Silhouette score', title = 'The Average Silhouette Method')\nplt.savefig('elbowSilhouette2.pdf', bbox_inches='tight')","9d528b10":"silhouettes = silhouettes_umap_f + silhouettes_umap_s \n# on UMAP there is monotone graph, so around n_clusters = 15 is about right\nif silhouettes[np.argmax(silhouettes)] > max_silh:\n    max_silh_id = np.argmax(silhouettes)\nn_themes = (max_silh_id - len(silhouettes_umap_f)) * 3 + 30\nn_themes","1b388496":"import hdbscan\nfrom sklearn.metrics import pairwise_distances\n\ndistance = pairwise_distances(lab_emb_150d, metric='cosine')\nclusterer = hdbscan.HDBSCAN(min_cluster_size=15, metric='precomputed')\nclusterer.fit(distance.astype('float64'))\nclusterer.labels_","250edac2":"lab_emb_150d = labels_embeddings.copy()\ncat_emb_150d = cat_embeddings.copy()","e5dba65f":"n_themes = 200\nkmeanstf = KMeansTF(n_clusters = n_themes, random_state = 21)\nlabels_embeddings_tf = tf.convert_to_tensor(labels_embeddings)\nkmeanstf.fit(labels_embeddings_tf)","a8add06b":"def set_key(dictionary, key, value):\n    if key not in dictionary:\n        if type(value) == list:\n            dictionary[key] = [value]\n        else:\n            dictionary[key] = value\n    elif type(dictionary[key]) == list:\n         dictionary[key].append(value)\n    else:\n         dictionary[key] = [dictionary[key], value]","06b9c85d":"print(\"Silhouette metric score: {}\".format(silhouette_score(lab_emb_150d, kmeanstf.labels_)))\n\ncluster_centers = kmeanstf.cluster_centers_\ncluster_centers = cluster_centers.numpy()\n\ncluster_labels = kmeanstf.labels_.numpy() # labels of each point\ncluster_to_emb = dict()                        # making dictionary containing label of each cluster \ncluster_to_im = dict()                         # for the keys() and embeddings as elements of each cluster\nobjid_cluster = dict()\n\n# we are going to make two dictionaries: cluster - embeddings of images; cluster - images distributions in cluster\nfor ID in range(cluster_labels.shape[0]):\n    set_key(cluster_to_emb, cluster_labels[ID], labels_embeddings[ID].copy())\n    set_key(cluster_to_im, cluster_labels[ID], labels_all[ID].copy())\n    set_key(objid_cluster, all_ids[ID], cluster_labels[ID].copy())\n    if type(labels_all[ID]) != list:\n        print('f')\n        print(labels_all[ID])","6cc9af1a":"# cluster_centers = dict()\n# for key, val in cluster_to_emb.items():\n#     center = np.zeros(labels_embeddings.shape[1])\n#     for emb in val:\n#         center = np.add(center, emb)\n#     center = np.divide(center, len(val))\n#     set_key(cluster_centers, key, center)","7fc6041b":"# for idcl_lab in range(len(cluster_labels)):\n#     if cluster_labels[idcl_lab] == 0:\n#         print (idcl_lab)","11eb2190":"with open('..\/input\/hyppr-images-mapping\/category_to_posts_vision.p', 'rb') as f:   # opening given model\n    category_to_posts = pickle.load(f)\ncategory_to_posts = dict(category_to_posts)\n\nposts_to_category = dict()\nfor cat, posts in category_to_posts.items():\n    for post in posts:\n        if post not in posts_to_category.keys():\n            set_key(posts_to_category, post, cat)","5d05b992":"len(cluster_to_im[7])","1100281e":"np.unique(cluster_labels)\n# starting from 0","fcc3fd5d":"cl_cent = []\nfor i in range(n_themes):\n    cl_cent.append(cluster_centers[i])","412f6b3f":"cl_cent = cluster_centers.copy()","9cdd5b59":"most_similars = cosine_similarity(cat_embeddings, cl_cent)\nmost_similars.shape","5cb545a1":"most_similars","c8cdc373":"#print(np.round(most_similars, 8)[0])\nnearest_id = np.argmax(most_similars[1])\n\nnearest_id","efe9e779":"cluster_to_im[35]","101b5ed8":"cats","ba7b2ed9":"cluster_to_im[10]","4ea7c71b":"most_similars_indices = np.argmax(most_similars, axis = 1)\ncurr_set_indices = np.arange(most_similars.shape[1]) # constructing a set for eleminating viewed parts","373cf11f":"curr_set_indices = np.arange(most_similars.shape[1])\nsim_indices = []\nfor i in range(min(most_similars.shape[0], most_similars.shape[1] ) ):   # we're bounded by n_clusters defined in KMeans. So max we can take n_clusters sim_indices. \n    # make a threshold, make something else\n    sim_ind_index_old_arr = np.argmax(most_similars[i][curr_set_indices]) # taking every row and finding non-visited max in this row\n    sim_ind = curr_set_indices[sim_ind_index_old_arr]\n    curr_set_indices = np.delete(curr_set_indices, sim_ind_index_old_arr)\n    sim_indices.append(sim_ind)\n\nsim_indices            # in this vec we have indices of most similar vectors of clusters for categories we were given","1f06f4e0":"from sklearn.metrics.pairwise import cosine_similarity as cos\ncos([lab_emb_150d[0]], [lab_emb_150d[4]])","34901dfb":"cos_matrix = cos(cat_emb_150d, cluster_centers)\ncos_matrix","7c49a575":"def indices_of_similarity(sim_matrix, metric):\n    curr_set_indices = np.arange(max(sim_matrix.shape))\n    \n    sim_indices = []\n    for i in range(min(sim_matrix.shape)):   # we're bounded by n_clusters defined in KMeans. So max we can take n_clusters sim_indices. \n        # make a threshold, make something else\n        if metric == 'euclidean':\n            sim_ind_index_old_arr = np.argmin(sim_matrix[i][curr_set_indices]) # taking every row and finding non-visited max in this row\n        elif metric == 'cosine':\n            sim_ind_index_old_arr = np.argmax(sim_matrix[i][curr_set_indices])\n        sim_ind = curr_set_indices[sim_ind_index_old_arr]\n        curr_set_indices = np.delete(curr_set_indices, sim_ind_index_old_arr)\n        sim_indices.append(sim_ind)\n\n    return sim_indices  ","1e453fa0":"cos_sim = indices_of_similarity(most_similars, 'cosine')\nmost_similar = cos_sim","74b42890":"most_similar","0ea979ee":"objid_category = dict()   # resulting dictionary\nfor objid, clusterid in objid_cluster.items():\n    sim_category_id = 0\n    if clusterid in most_similar:\n        sim_category_id = most_similar.index(clusterid)\n                                          # we can interpretate it next way: \"most similar TO cluster with index <index of cluster>\"\n        category_similar = categories[sim_category_id][0]    # we have a list with categories distributions.  \\ \n                                                                # We take only first element --- true similar string of category\n        set_key(objid_category, objid, category_similar)\n        \ncategory_objid = dict()\nfor objid, cat in objid_category.items():\n    set_key(category_objid, cat, objid)","ad1ffa21":"with open('..\/input\/hyppr-images-mapping\/category_to_posts_vision.p', 'rb') as f:   # opening given model\n    category_to_posts = pickle.load(f)\ncategory_to_posts = dict(category_to_posts)\nwith open('..\/input\/hyppr-images-mapping\/objid_postid.p', 'rb') as f:   # opening for making category_postid dictionary\n    objid_postid = pickle.load(f)","962368e2":"len(objid_postid)","dfc4415e":"category_postid = dict()\nno_obj = 0\nfor cat, objid in category_objid.items():\n    for obj in objid:\n        try:\n            set_key(category_postid, cat, objid_postid[obj])\n        except KeyError as e:\n            #print(KeyError)\n            no_obj += 1\nprint(no_obj)","81bd485a":"num_els = 0\nfor cat, idlist in category_postid.items():\n    num_els += len(idlist)\nnum_els","c68ad1af":"num_els = 0\nfor cat, idlist in category_to_posts.items():\n    num_els += len(idlist)\nnum_els","76f44ce8":"right = 0\nfor cat, idlist in category_to_posts.items():\n    for idpost in idlist:\n        if idpost in category_postid[cat]:\n            right += 1   \nright","a66410ed":"with open(\".\/objid_category.p\", \"wb\") as f:\n    pickle.dump(objid_category, f)","8b22d017":"posts_to_category = dict()\npostid_category = dict()\n\nfor cat, posts in category_to_posts.items():\n    for post in posts:\n        set_key(posts_to_category, post, cat)\n    \nfor cat, posts in category_postid.items():\n    for post in posts:\n        set_key(postid_category, post, cat)","5ae904c7":"#visible_obj = [cat for cat, posts in category_to_posts.items()]   # len of category_to_posts is 10 so we need to truncate \n                                                                    # len of BERT model result\nyDef = list()\nyBert = list()\n\nfor post, cat in posts_to_category.items():\n    if post in postid_category.keys():\n        el = postid_category[post]\n        if type(el) is list:\n            yBert.append(el[0])\n        else:\n            yBert.append(el)\n    else:\n        yBert.append('none')\n        \n    if type(cat) is list:\n        yDef.append(cat[0])\n    else:\n        yDef.append(cat)\n# for post, cat in postid_category.items():\n#     if post in posts_to_category.keys():\n#         pass\n#     else:\n#         yBert.append(cat)\n#         yDef.append('none')","0bfc7e1c":"nodata = ['dance', 'entertainment', 'tech']  # we don't have data on cite for this categories so we can't consider data communicated with it\ncats = [cat[0] for cat in categories if cat[0] not in nodata]   # so we just throw it away\ncats.append('none')","9e25773f":"from sklearn.metrics import confusion_matrix\n\nconf_matr = confusion_matrix(yDef, yBert, labels = cats)","fb661634":"import seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_cm = pd.DataFrame(conf_matr, index = [i for i in range(len(cats))],\n                  columns = [i for i in range(len(cats))])\nplt.figure(figsize = (10,7))\nsn.heatmap(df_cm,cmap=\"YlGnBu\",linewidths=1, annot=True, fmt = 'd')","a0973bf7":"!pip install hdbscan --no-build-isolation --no-binary :all:  --quiet # extremely insane to built it this way. Without it nothing works\n        \n!pip install bertopic --quiet","cdf95977":"from bertopic import BERTopic","1c7fb78e":"topic_model = BERTopic(nr_topics = int(n_themes), calculate_probabilities=True)\n%time topics, probabilities = topic_model.fit_transform(labels_n)","0b4a5348":"topic_model.get_topic_freq().head()","3359fc09":"topic_model.get_topic(17)","cc8535a0":"topic_model.visualize_topics()","07d60b91":"topic_model.visualize_distribution(probabilities[0])","6b0d8cc4":"similar_topics, similarity = topic_model.find_topics(\"fashion\", top_n=5)\n\ntopic_model.get_topic(similar_topics[0])","fb63805b":"similarity","a525df1d":"topic_model = BERTopic(nr_topics=\"auto\")\ntopics, _ = topic_model.fit_transform(labels_n)\n\ntopic_model.get_topic_freq().head()","f40979ed":"len(labels_n)","8ca7703a":"topic_model.visualize_topics()","83fd1d9c":"similar_topics, similarity = topic_model.find_topics(\"fashion\", top_n=5)\n\ntopic_model.get_topic(similar_topics[1])","eb877d3d":"## ONLY FOR OLD MODEL DICTIONARY:\n","0d176fa6":"### Now we need to find closest vector to each of the given categories. Then we can map it.\n### We'll use ```cosine_similarity``` function from ```sklearn.metrics.pairwise``` for cosine similarity metric for it","1a0c6f67":"So by graphs we're taking **min_dist** = 0.1 and **n_neighbors** = 10","5dc6ec14":"### Let's print it on 2D","51eec584":"# Bringing PCA on","11fcc0ae":"# Weighted arithmetic mean aggregation: \n \\begin{equation*} \\LARGE\n \t\t\t\\text{result}_j = \\frac{\\sum_i^{m_l} v_{ij} \\cdot s_i}{\\sum_i^{m_l} v_{ij}}, \\; \\forall j \\in 1, ..., n.\n \\end{equation*}","20f26038":"# Bringing UMAP on","a4c76a29":"# Try with RoBERTa","eec5b530":"## Categories mapping","76b001ab":"## Now we need to find out optimal number of clusters.\n### Since Elbow Method works not always fine, we need to compare it with Average Silhouette Method and find out which is better","477eaeb1":"## Doing reverse dictionaries:","e6625571":"# Dict Embeddings:","1087fe34":"# COMPARISON UMAP AND PCA GRAPHICS:","c763b8c6":"# QUALITY COMPARISON","6308e33a":"# Let see with first category and nearest element:","8b2240a1":"## Labels mapping","e0c66ef1":"# Models loading","3a7f9960":"### Find out where Silhouette is bigger:","d032ff37":"# COMPARISON PCA AND UMAP\n\n## 1.  UMAP is potentially better since it causes to connect nearest neighbours making pre-clustering\n## 2.  Also UMAP is better since it finds non-linear connections between data ","0b4b5c3c":"# WARNING WARNING","e60e0fcc":"# Clusterization","f9b0e206":"# BERTopic approach","0f0e3a27":"# TODO: make a threshold","4ff5e38e":"# Comparison of Elbow Method and Average Silhouette","320b5586":"# Normalizing the data"}}