{"cell_type":{"f545435c":"code","fa693bb1":"code","5f271390":"code","9ba658af":"code","9f36b994":"code","7de4b129":"code","317b6c67":"code","ab6168a1":"code","25f4f80d":"code","eb0ed213":"code","412c5730":"code","d56e31d9":"code","9d7a4a96":"code","915734b5":"code","93fe3af7":"code","b1dc48d5":"code","30de7db5":"code","9bba39f3":"code","fa5e5bb6":"code","9190c01e":"code","67904780":"code","d48de8b6":"code","11ee331a":"code","47f6658f":"code","ad39f9a5":"code","13041ceb":"code","64bb3d98":"code","56c64930":"code","dcfdde3a":"code","5a1743f1":"code","3f5abb86":"code","7182bd54":"code","b1ccfeb5":"code","ccdb63c3":"code","05ed2437":"code","c1dba5e4":"code","d3ed11e1":"code","70f24cf8":"code","6b8ba844":"code","92c7a6c5":"code","960f2ca6":"code","bf28c9a3":"markdown","ceb1a8a5":"markdown"},"source":{"f545435c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom sklearn.preprocessing import QuantileTransformer\n\n\nfrom sklearn import preprocessing\nimport torch.optim as optim","fa693bb1":"import torch.nn.functional as F","5f271390":"import optuna\nimport plotly as pl","9ba658af":"from sklearn.model_selection import StratifiedKFold","9f36b994":"import lightgbm as lgb","7de4b129":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'","317b6c67":"test_fea = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_fea = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_tar_nonsco = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_tar_sco = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nsubmission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","ab6168a1":"def seed_everything(seed=1062):\n    np.random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=1062)","25f4f80d":"class Sparsemax(nn.Module):\n    def __init__(self, dim=None):\n        super(Sparsemax, self).__init__()\n        self.dim = -1 if dim is None else dim\n\n    def forward(self, input):\n        input = input.transpose(0, self.dim)\n        original_size = input.size()\n        input = input.reshape(input.size(0), -1)\n        input = input.transpose(0, 1)\n        dim = 1\n\n        number_of_logits = input.size(dim)\n        \n        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n        range = torch.arange(start=1, end=number_of_logits + 1, device=device,step=1, dtype=input.dtype).view(1, -1)\n        range = range.expand_as(zs)\n\n        bound = 1 + range * zs\n        cumulative_sum_zs = torch.cumsum(zs, dim)\n        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n        zs_sparse = is_gt * zs\n        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) \/ k\n        taus = taus.expand_as(input)\n        self.output = torch.max(torch.zeros_like(input), input - taus)\n        output = self.output\n        output = output.transpose(0, 1)\n        output = output.reshape(original_size)\n        output = output.transpose(0, self.dim)\n        return output\n    def backward(self, grad_output):\n        dim = 1\n        nonzeros = torch.ne(self.output, 0)\n        sum = torch.sum(grad_output * nonzeros, dim=dim) \/ torch.sum(nonzeros, dim=dim)\n        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n        return self.grad_input","eb0ed213":"def initialize_non_glu(module,inp_dim,out_dim):\n    gain = np.sqrt((inp_dim+out_dim)\/np.sqrt(4*inp_dim))\n    torch.nn.init.xavier_normal_(module.weight, gain=gain)\n    \nclass GBN(nn.Module):\n    def __init__(self,inp,vbs=128,momentum=0.01):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(inp,momentum=momentum)\n        self.vbs = vbs\n    def forward(self,x):\n        chunk = torch.chunk(x,max(1,x.size(0)\/\/self.vbs),0)\n        res = [self.bn(y) for y in chunk ]\n        return torch.cat(res,0)\n\nclass GLU(nn.Module):\n    def __init__(self,inp_dim,out_dim,fc=None,vbs=128):\n        super().__init__()\n        if fc:\n            self.fc = fc\n        else:\n            self.fc = nn.Linear(inp_dim,out_dim*2)\n        self.bn = GBN(out_dim*2,vbs=vbs) \n        self.od = out_dim\n    def forward(self,x):\n        x = self.bn(self.fc(x))\n        return x[:,:self.od]*torch.sigmoid(x[:,self.od:])\n    \n\nclass FeatureTransformer(nn.Module):\n    def __init__(self,inp_dim,out_dim,shared,n_ind,vbs=128):\n        super().__init__()\n        first = True\n        self.shared = nn.ModuleList()\n        if shared:\n            self.shared.append(GLU(inp_dim,out_dim,shared[0],vbs=vbs))\n            first= False    \n            for fc in shared[1:]:\n                self.shared.append(GLU(out_dim,out_dim,fc,vbs=vbs))\n        else:\n            self.shared = None\n        self.independ = nn.ModuleList()\n        if first:\n            self.independ.append(GLU(inp,out_dim,vbs=vbs))\n        for x in range(first, n_ind):\n            self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n        self.scale = torch.sqrt(torch.tensor([.5],device=device))\n    def forward(self,x):\n        if self.shared:\n            x = self.shared[0](x)\n            for glu in self.shared[1:]:\n                x = torch.add(x, glu(x))\n                x = x*self.scale\n        for glu in self.independ:\n            x = torch.add(x, glu(x))\n            x = x*self.scale\n        return x\nclass AttentionTransformer(nn.Module):\n    def __init__(self,inp_dim,out_dim,relax,vbs=128):\n        super().__init__()\n        self.fc = nn.Linear(inp_dim,out_dim)\n        self.bn = GBN(out_dim,vbs=vbs)\n#         self.smax = Sparsemax()\n        self.r = torch.tensor([relax],device=device)\n    def forward(self,a,priors):\n        a = self.bn(self.fc(a))\n        mask = torch.sigmoid(a*priors)\n        priors =priors*(self.r-mask)\n        return mask\n\nclass DecisionStep(nn.Module):\n    def __init__(self,inp_dim,n_d,n_a,shared,n_ind,relax,vbs=128):\n        super().__init__()\n        self.fea_tran = FeatureTransformer(inp_dim,n_d+n_a,shared,n_ind,vbs)\n        self.atten_tran = AttentionTransformer(n_a,inp_dim,relax,vbs)\n    def forward(self,x,a,priors):\n        mask = self.atten_tran(a,priors)\n        loss = ((-1)*mask*torch.log(mask+1e-10)).mean()\n        x = self.fea_tran(x*mask)\n        return x,loss\n\nclass TabNet(nn.Module):\n    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,n_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n        super().__init__()\n        if n_shared>0:\n            self.shared = nn.ModuleList()\n            self.shared.append(nn.Linear(inp_dim,2*(n_d+n_a)))\n            for x in range(n_shared-1):\n                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n        else:\n            self.shared=None\n        self.first_step = FeatureTransformer(inp_dim,n_d+n_a,self.shared,n_ind) \n        self.steps = nn.ModuleList()\n        for x in range(n_steps-1):\n            self.steps.append(DecisionStep(inp_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n        self.fc = nn.Linear(n_d,final_out_dim)\n        self.bn = nn.BatchNorm1d(inp_dim)\n        self.n_d = n_d\n    def forward(self,x):\n        x = self.bn(x)\n        x_a = self.first_step(x)[:,self.n_d:]\n        loss = torch.zeros(1).to(x.device)\n        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n        priors = torch.ones(x.shape).to(x.device)\n        for step in self.steps:\n            x_te,l = step(x,x_a,priors)\n            out += F.relu(x_te[:,:self.n_d])\n            x_a = x_te[:,self.n_d:]\n            loss += l\n        return self.fc(out),loss","412c5730":"class TabNetWithEmbed(nn.Module):\n    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,n_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n        super().__init__()\n        self.tabnet = TabNet(inp_dim,final_out_dim,n_d,n_a,n_shared,n_ind,n_steps,relax,vbs)\n        self.cat_embed = []\n        self.emb1 = nn.Embedding(2,1)\n        self.emb3 = nn.Embedding(3,1)\n        self.cat_embed.append(self.emb1)\n        self.cat_embed.append(self.emb3)\n        \n    def forward(self,x):\n#         catv = catv.to(device)\n#         contv = contv.to(device)\n#         embeddings = [embed(catv[:,idx]) for embed,idx in zip(self.cat_embed,range(catv.size(1)))]\n#         catv = torch.cat(embeddings,1)\n#         x = torch.cat((catv,contv),1).contiguous()\n        x,l = self.tabnet(x)\n        return torch.sigmoid(x),l","d56e31d9":"class DrugData(Dataset):\n    \n    def __init__(self,df,out,train=True):\n        self.df = df\n        self.out = out\n        self.train=train\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        if self.train:\n            tar = np.where(self.out[idx].reshape(-1)==0,0.00001,0.99999)\n            return torch.from_numpy(self.df[idx,:]).float(),torch.tensor(tar).float()\n        else:\n            return torch.from_numpy(self.df[idx,:]).float(),torch.tensor(self.out[idx].reshape(-1)).float()","9d7a4a96":"train_fea['cp_dose'].replace({'D1':0,'D2':1},inplace=True)\ntest_fea['cp_dose'].replace({'D1':0,'D2':1},inplace=True)\n\ntrain_fea['cp_time'].replace({24:0,72:1,48:2},inplace=True)\ntest_fea['cp_time'].replace({24:0,72:1,48:2},inplace=True)\n\ntrain_fea['cp_type'].replace({'trt_cp':0,'ctl_vehicle':1},inplace=True)\ntest_fea['cp_type'].replace({'trt_cp':0,'ctl_vehicle':1},inplace=True)\n\ntrain_fea = train_fea.drop(columns='sig_id') \ntest_fea = test_fea.drop(columns='sig_id')","915734b5":"cat_col = [0,2]\nnum_col = len(train_fea.columns)","93fe3af7":"train_tar = train_tar_sco.drop(columns='sig_id').values","b1dc48d5":"from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA","30de7db5":"data = pd.concat([train_fea,test_fea],ignore_index=True)\ng = [*(x for x in data.columns if 'g' in x)]\nc = [*(x for x in data.columns if 'c-' in x)]","9bba39f3":"for col in g:\n    sel = QuantileTransformer(n_quantiles=100,random_state=0,output_distribution='normal')\n    sel.fit(data[col].to_numpy().reshape(-1,1))\n    data[col] = sel.transform(data[col].to_numpy().reshape(-1,1))\nfor col in c:\n    sel = QuantileTransformer(n_quantiles=100,random_state=0,output_distribution='normal')\n    sel.fit(data[col].to_numpy().reshape(-1,1))\n    data[col] = sel.transform(data[col].to_numpy().reshape(-1,1))","fa5e5bb6":"pca_c = PCA(n_components=20)\nextra_c = pd.DataFrame(pca_c.fit_transform(data[c]))\npca_g = PCA(n_components=208)\nextra_g = pd.DataFrame(pca_g.fit_transform(data[g]))\ndata = pd.concat((data,extra_c,extra_g),axis=1)","9190c01e":"# from umap import UMAP","67904780":"# umap_c = UMAP(random_state=256,n_components=15)\n# extra_c = pd.DataFrame(umap_c.fit_transform(data[c]))\n# umap_g = UMAP(random_state=256,n_components=50)\n# extra_g = pd.DataFrame(umap_g.fit_transform(data[g]))\n# data = pd.concat((data,extra_c,extra_g),axis=1)","d48de8b6":"from sklearn.feature_selection import VarianceThreshold","11ee331a":"vt = VarianceThreshold(0.65)\ndat = data.iloc[:,:3]\ndata = vt.fit_transform(data.iloc[:,3:])\ndata = pd.concat((dat,pd.DataFrame(data)),axis=1)","47f6658f":"train_df = data.iloc[:len(train_fea),:].values\ntest_df= data.iloc[len(train_fea):,:].values","ad39f9a5":"kfold = StratifiedKFold(n_splits=5)","13041ceb":"loss_func = nn.BCELoss()","64bb3d98":"# ra = np.arange(3,train_df.shape[1])\n# np.random.shuffle(ra)\n# train_df[:,3:] = train_df[:,ra]\n# test_df[:,3:] = test_df[:,ra]","56c64930":"# Trial 97 finished with value: 0.015035743787155454 and parameters: {'batch_size': 8, 'nd': 8, 'na': 4, 'shared': 1, 'indep': 2, 'steps': 5, 'relax': 2.5, 'vbs': 7, 'lr': 0.008107344577671696}. Best is trial 97 with value: 0.015035743787155454.\n# 0.015003892647780077 and parameters: {'batch_size': 9, 'nd': 8, 'na': 8, 'shared': 1, 'indep': 2, 'steps': 2, 'relax': 1, 'vbs': 7, 'lr': 0.009373664964478735}. Best is trial 95 with value: 0.015003892647780077.","dcfdde3a":"# divisor=0","5a1743f1":"# seed_everything(1006)\n# submission.iloc[:,1:]=0\n# for train,test in kfold.split(train_df,np.zeros(len(train_df))):\n#     batch_size=512\n#     sparse_constant = 0\n#     model = TabNetWithEmbed(train_df.shape[1]-3,train_tar.shape[1],n_d=256,n_a=256,n_shared=1,n_ind=2,n_steps=2,relax=1,vbs=128)\n#     model.to(device)\n#     torch.cuda.empty_cache()\n#     optimizer = optim.Adam(model.parameters(),lr=0.009373664964478735,weight_decay=0.00001)\n#     sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1,patience=1,verbose=True)\n#     train_dataset = DrugData(train_df[train],train_tar[train])\n#     valid_dataset = DrugData(train_df[test],train_tar[test],False)\n#     train_loader = DataLoader(train_dataset,batch_size=batch_size,num_workers=4,shuffle=True)\n#     valid_loader = DataLoader(valid_dataset,batch_size=batch_size,num_workers=4,shuffle=True)\n#     losses=[]\n#     norm = []\n#     t = time.time()\n#     for x in range(22):\n#         train_loss=0.\n#         grad_norm_sum = 0.\n#         for inp,tar in train_loader:\n#             model.zero_grad()\n#             out,l = model(inp[:,3:].to(device))\n#             loss = loss_func(out,tar.to(device))#+l*sparse_constant\n#             loss.backward()\n#             optimizer.step()\n# #             sched.step()\n#             train_loss+=loss.item()*tar.size(0)\n#         valid_loss=0.\n#         v=0\n#         for inp,tar in valid_loader:\n#             v+=1\n#             out,_ = model(inp[:,3:].to(device))\n#             loss = loss_func(out,tar.to(device))\n\n#             valid_loss+= loss.item()*tar.size(0)\n#         losses.append(valid_loss\/len(valid_dataset))\n#         print('%d epoch, %.8f valid_loss, %.8f training_loss %fsec time'% (x+1,losses[-1],train_loss\/len(train_dataset),(time.time()-t)))\n#         sched.step(losses[-1])\n#         t = time.time()\n#     print(\"completed training one fold -------------- \")\n#     model.eval()\n#     divisor += 1\/losses[-1]\n#     submission.iloc[:,1:] += model(torch.from_numpy(test_df[:,3:]).float())[0].data.cpu().numpy()\/losses[-1]","3f5abb86":"seed_everything(1006)\nmsk = np.random.rand(len(train_df))<0.85\ntrain_inp = train_df[msk]\nvalid_inp = train_df[~msk]\ntrain_targ = train_tar[msk]\nvalid_targ = train_tar[~msk]","7182bd54":"import gc","b1ccfeb5":"def objective(trail):\n    model=None\n    seed_everything(1006)\n    try:\n#         sparse_constant = trail.suggest_loguniform('sparse_const',0.000001,0.001)\n        batch_size= 2**trail.suggest_int('batch_size',8,12)\n        train_dataset = DrugData(train_inp,train_targ)\n        valid_dataset = DrugData(valid_inp,valid_targ,False)\n        train_loader = DataLoader(train_dataset,batch_size=batch_size,num_workers=4,shuffle=True)\n        valid_loader = DataLoader(valid_dataset,batch_size=batch_size,num_workers=4,shuffle=True)\n        model = TabNetWithEmbed(train_df.shape[1]-3,train_tar.shape[1],n_d=2**trail.suggest_int('nd',4,9),\n                                n_a=2**trail.suggest_int('na',4,9),n_shared=trail.suggest_int('shared',1,6),\n                                n_ind=trail.suggest_int('indep',2,4),n_steps=trail.suggest_int('steps',2,7),\n                                relax=trail.suggest_categorical('relax',[1,1.2,1.5,2,2.3,2.5]),vbs=2**trail.suggest_int('vbs',4,7))\n        model.to(device)\n        torch.cuda.empty_cache()\n        optimizer = optim.Adam(model.parameters(),lr=trail.suggest_loguniform('lr',3e-4,0.01),weight_decay=trail.suggest_loguniform('wd',0.000005,0.00005))\n        sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1,patience=3,verbose=True)\n        losses=[]\n        norm = []\n        t = time.time()\n        for x in range(35):\n            train_loss=0.\n            grad_norm_sum = 0.\n            for inp,tar in train_loader:\n                model.zero_grad()\n                out,l = model(inp[:,3:].to(device))\n                loss = loss_func(out,tar.to(device))#+l*sparse_constant\n                loss.backward()\n                optimizer.step()\n\n                train_loss+=loss.item()\n            valid_loss=0.\n            for inp,tar in valid_loader:\n                out,_ = model(inp[:,3:].to(device))\n                loss = loss_func(out,tar.to(device))\n\n                valid_loss+= loss.item()*tar.size(0)\n            losses.append(valid_loss\/len(valid_dataset))\n#             print('%d epoch, %.8f valid_loss, %.8f training_loss %fsec time'% (x+1,losses[-1],train_loss*batch_size\/len(train_dataset),(time.time()-t)))\n            sched.step(losses[-1])\n            t = time.time()\n        return min(losses)\n    except:\n        model=None\n        gc.collect()\n        torch.cuda.empty_cache()\n        return 99.99","ccdb63c3":"# import joblib","05ed2437":"# !cp ..\/input\/drug-prediction\/optuna.db .\/optuna.dbff","c1dba5e4":"study = optuna.create_study(direction='minimize',storage='sqlite:\/\/\/.\/optuna.dbff',load_if_exists=True)","d3ed11e1":"study.optimize(objective,200) ","70f24cf8":"# for col in range(train_tar.shape[1]):\n#     gc.collect()\n#     t = time.time()\n# #     for train,test in kfold.split(train_df,train_tar[:,col]):\n# #         model  = mutual_info_classif()\n# # #         tdset = lgb.Dataset(train_df[:,3:][train],train_tar[:,col][train])\n# # #         vdset = lgb.Dataset(train_df[:,3:][test],train_tar[:,col][test])\n# #         model.fit(train_df[:,3:][train],train_tar[:,col][train])\n# #         submission.iloc[:,col+1] += model.predict(test_df[:,3:])\n# #         features.iloc[col,:] += mutual.feature_importance()\n# #           features.iloc\n#     features.iloc[col,:] = mutual_info_classif(train_df[:,3:],train_tar[:,col])\n#     print('Trained %d label in time: %t'%(col,(time.time()-t)\/60))","6b8ba844":"# submission.iloc[:,1:] = submission.iloc[:,1:]\/divisor","92c7a6c5":"# submission = submission.copy()","960f2ca6":"# submission.to_csv('submission.csv',index=False)","bf28c9a3":"[I 2020-11-08 14:37:58,675] Trial 3 finished with value: 0.015910927270087877 and parameters: {'batch_size': 10, 'nd': 8, 'na': 6, 'shared': 1, 'indep': 3, 'steps': 7, 'relax': 3.5, 'vbs': 6, 'lr': 0.000844953649205203}. Best is trial 3 with value: 0.015910927270087877.\n[I 2020-11-08 14:59:41,919] Trial 13 finished with value: 0.015747105775683584 and parameters: {'batch_size': 11, 'nd': 8, 'na': 5, 'shared': 2, 'indep': 3, 'steps': 3, 'relax': 3.5, 'vbs': 6, 'lr': 0.009918007325458078}. Best is trial 13 with value: 0.015747105775683584.\n[I 2020-11-08 15:08:50,521] Trial 21 finished with value: 0.015424192366516902 and parameters: {'batch_size': 10, 'nd': 7, 'na': 4, 'shared': 1, 'indep': 4, 'steps': 3, 'relax': 1.5, 'vbs': 6, 'lr': 0.009473372341704749}. Best is trial 21 with value: 0.015424192366516902.\n[I 2020-11-08 15:12:08,382] Trial 23 finished with value: 0.015248623840681005 and parameters: {'batch_size': 9, 'nd': 7, 'na': 4, 'shared': 1, 'indep': 4, 'steps': 3, 'relax': 1.5, 'vbs': 6, 'lr': 0.0078097190001649875}. Best is trial 23 with value: 0.015248623840681005.","ceb1a8a5":" Trial 8 finished with value: 0.01601797854527831 and parameters: {'sparse_const': 5.569518821520305e-06, 'batch_size': 10, 'nd': 8, 'na': 8, 'shared': 4, 'indep': 2, 'steps': 6, 'relax': 1.5, 'vbs': 6}. Best is trial 8 with value: 0.01601797854527831.\n Trial 24 finished with value: 0.015906228683888912 and parameters: {'sparse_const': 2.0155530946123628e-05, 'batch_size': 11, 'nd': 9, 'na': 8, 'shared': 3, 'indep': 3, 'steps': 7, 'relax': 2.5, 'vbs': 7}. Best is trial 24 with value: 0.015906228683888912\n Trial 2 finished with value: 0.01621854634040915 and parameters: {'sparse_const': 0.00013392320643434687, 'batch_size': 11, 'nd': 9, 'na': 8, 'shared': 5, 'indep': 2, 'steps': 7, 'relax': 1.5, 'vbs': 5}. Best is trial 2 with value: 0.01621854634040915.\n Trial 64 finished with value: 0.015986942599541212 and parameters: {'sparse_const': 7.667690166509668e-05, 'batch_size': 11, 'nd': 9, 'na': 9, 'shared': 4, 'indep': 3, 'steps': 4, 'relax': 3, 'vbs': 5}. Best is trial 64 with value: 0.015986942599541212.\n Trial 80 finished with value: 0.01593467401459108 and parameters: {'sparse_const': 6.937792007865797e-05, 'batch_size': 11, 'nd': 9, 'na': 9, 'shared': 4, 'indep': 2, 'steps': 4, 'relax': 2, 'vbs': 5}. Best is trial 80 with value: 0.01593467401459108."}}