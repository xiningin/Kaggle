{"cell_type":{"4adfe126":"code","9bb1e9f2":"code","d59187a3":"code","a43834af":"code","91b9f039":"code","2a29f563":"code","210e8eea":"code","a57fded4":"code","5966f4db":"code","4e041759":"code","ab52a606":"code","a00df32b":"code","8cd31105":"code","7d1f639e":"code","df6a12fe":"code","bd35077c":"code","7a34dc98":"code","194ec0d9":"code","a70058a4":"code","523a48cb":"code","abc35efb":"code","afd6c49c":"code","3013408d":"code","8d267843":"code","e4655dfd":"code","b331fafd":"code","e68cff5e":"code","d5d99b77":"code","079c2cc8":"code","1118e6a2":"code","8e514339":"code","48b0ff3e":"code","6769c8b8":"code","021a5f35":"code","4e3a22d1":"code","f17b6074":"code","fe0ae0ff":"code","2f427f93":"code","f210031b":"code","14b253c1":"code","5f64194c":"code","14509e9c":"code","eb5e86ff":"code","310ed8ee":"code","3a0a798b":"code","dbf72b4e":"code","8e7b7518":"markdown","77c03e26":"markdown","d358d945":"markdown","60249f72":"markdown","cea3b711":"markdown","1bb98cea":"markdown","1d781e8c":"markdown","358261b8":"markdown","1e7400db":"markdown","b4e4f5e0":"markdown","d0fefb1c":"markdown","9b74872d":"markdown","f2825a81":"markdown","da9d09d4":"markdown","263a0f84":"markdown","efa8cdef":"markdown","f98d2556":"markdown","da29980b":"markdown"},"source":{"4adfe126":"import os\nimport cv2\nimport platform\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nos.environ['PYTHONHASHSEED'] = '73'\n\nseed = 73\nrandom.seed(seed)\nnp.random.seed(seed)\n\nprint(platform.platform())\n%matplotlib inline","9bb1e9f2":"!nvidia-smi","d59187a3":"!ls \/kaggle\/input\/","a43834af":"MyDrive = '\/kaggle\/working'\nclear_output()\n\nDataDir = '..\/input\/covidx-cxr2'\nPneumoniaDir = '..\/input\/chest-xray-pneumonia\/chest_xray'\n\nprint('> Covid 19 dir:', os.listdir(DataDir))\nprint('> Pneumonia dir:', os.listdir(PneumoniaDir))","91b9f039":"from tqdm import tqdm\n\ntrain_image_dir = PneumoniaDir + '\/train'\ntest_image_dir = PneumoniaDir + '\/test'\nval_image_dir = PneumoniaDir + '\/val'\n\nimg_map = []\n\ndef prepareData(Dir, strat):\n    cats = [\"NORMAL\",\"PNEUMONIA\"]\n    for category in cats:\n        path = os.path.join(Dir,category)\n        class_num = cats.index(category)\n        \n        for img in tqdm(os.listdir(path)):\n            img_path = os.path.join(path,img)\n            img_map.append({'path': img_path, 'label': category})\n\nprepareData(train_image_dir,'train')\nprepareData(test_image_dir,'test')\nprepareData(val_image_dir, 'val')\n\nimg_map = pd.DataFrame(img_map).sample(frac = 1, random_state=seed)","2a29f563":"#ricord, rsna, cohen, actmed, sirm, \ndef getClass(label):\n    if label == 'negative':\n        return 'NORMAL'\n    if label == 'positive':\n        return 'COVID'\n\ndef get_image_map(txt_path, strat):\n    train_txt = open(txt_path, 'r')\n    Lines = train_txt.readlines()\n    paths = []\n    \n    img_formats = ['jpg', 'jpeg', 'png']\n    \n    for n, line in enumerate(Lines):\n        querywords = line.split()\n\n        if len(querywords) == 4:\n            image_id = querywords[0]\n            image_path = DataDir + '\/' + strat + '\/'+ querywords[1]\n            label = querywords[2]\n\n        if len(querywords) == 5:\n            image_id = querywords[0]\n            image_path = DataDir + '\/' + strat + '\/'+ querywords[2]\n            label = querywords[3]\n            \n        for img_type in img_formats:\n            if img_type in line:\n                obj_ = {'path': image_path, 'label': getClass(label)}\n                if (('positive' in line) | ('negative' in line)):\n                    paths.append(obj_)\n\n    paths_df = pd.DataFrame(paths)\n    return paths_df","210e8eea":"train_map = get_image_map(DataDir + '\/train.txt', \n                          strat='train').sample(frac = 1, random_state=73)\n\ntest_map = get_image_map(DataDir + '\/test.txt',\n                         strat='test').sample(frac = 1, random_state=73)","a57fded4":"img_path_map = pd.concat([img_map, train_map, test_map], axis=0).sample(frac = 1, random_state=73)\nimg_path_map.head()","5966f4db":"import matplotlib.pyplot as plt\n\ndef print_images(samples): \n    images = samples[\"path\"].to_numpy()\n    labels = samples['label'].to_numpy()\n    \n    fig=plt.figure(figsize=(20, 8))\n    columns = 4\n    rows = 1\n    \n    for i, image_path in enumerate(images):\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        \n        fig.add_subplot(rows,columns,i + 1)\n        title = '{}'.format(labels[i])\n        \n        Sample_image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_CUBIC)\n        \n        plt.imshow(Sample_image, cmap='gray')\n        plt.title(title)\n        \n    plt.show()\n        \nprint_images(img_path_map[img_path_map['label']==\"NORMAL\"].iloc[0:4])\nprint_images(img_path_map[img_path_map['label']==\"PNEUMONIA\"].iloc[0:4])\nprint_images(img_path_map[img_path_map['label']==\"COVID\"].iloc[0:4])\n\n%matplotlib inline","4e041759":"def getLabelCount(frame):\n    label_count = pd.Series(frame['label'].values.ravel()).value_counts()\n    n_classes = (label_count)\n    return label_count\n\nlabel_count = getLabelCount(img_path_map)\nprint(label_count)","ab52a606":"from sklearn.model_selection import StratifiedShuffleSplit\n\nfeatures = img_path_map['path'].to_numpy()\nlabels = img_path_map['label'].to_numpy()\n\nstratified_sample = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=73)","a00df32b":"for train_index, test_index in stratified_sample.split(features, labels):\n    X_train, test_X = features[train_index], features[test_index]\n    y_train, test_y = labels[train_index], labels[test_index]\n    \nhalf_size = np.int(len(test_X) \/ 2)\nX_test, y_test = test_X[0:half_size], test_y[0:half_size]\nX_val, y_val = test_X[half_size:], test_y[half_size:]","8cd31105":"train_map = pd.DataFrame()\ntrain_map['path'], train_map['label'] = X_train, y_train","7d1f639e":"test_map = pd.DataFrame()\ntest_map['path'], test_map['label'] = X_test, y_test","df6a12fe":"val_map = pd.DataFrame()\nval_map['path'], val_map['label'] = X_val, y_val","bd35077c":"# data summary\nprint('> {} train size'.format(X_train.shape[0]))\nprint('> {} test size'.format(X_test.shape[0]))\nprint('> {} val size'.format(X_val.shape[0]))","7a34dc98":"import cv2\nimport time\nimport imageio\nimport imgaug.augmenters as iaa\nimport imgaug as ia\nia.seed(73)\n\nColorCh = 3\nIMG_SIZE = 224\ninput_shape=(IMG_SIZE, IMG_SIZE, ColorCh)\n\nclasses = (\"COVID\", \"NORMAL\",\"PNEUMONIA\")\nCATEGORIES = sorted(classes)\n\nprint('> Classes:',CATEGORIES)","194ec0d9":"from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n\ndatagen = ImageDataGenerator(rescale = 1.\/255, \n                             horizontal_flip=True,\n                             brightness_range=[1.0,1.3],\n                             rotation_range=15,\n                             #zoom_range=0.2\n                            )","a70058a4":"batch_size = 64\n\ndef get_generator(frame_):\n    generator = datagen.flow_from_dataframe(\n                          dataframe=frame_,\n                          x_col=\"path\",\n                          y_col=\"label\",\n                          batch_size=batch_size,\n                          seed=seed,\n                          shuffle=False,\n                          class_mode=\"sparse\",\n                          color_mode=\"rgb\",\n                          save_format=\"jpeg\",\n                          target_size=(IMG_SIZE,IMG_SIZE)             \n             )\n    \n    return generator","523a48cb":"train_df = train_map.sample(frac=1, random_state=seed)\ntrain_generator = get_generator(train_df)\n\nprint('> label count for train set')\ngetLabelCount(train_df)","abc35efb":"test_df = test_map.sample(frac=1, random_state=seed)\ntest_generator = get_generator(test_df)\n\nprint('> label count for test set')\ngetLabelCount(test_df)","afd6c49c":"val_df = val_map.sample(frac=1, random_state=seed)\nval_generator = get_generator(val_df)\n\nprint('> label count for val set')\ngetLabelCount(val_df)","3013408d":"print('> input shape:', input_shape)","8d267843":"import keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Add, add\nfrom tensorflow.keras.layers import InputLayer, Input, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, Activation, MaxPool2D, ZeroPadding2D, SeparableConv2D\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.models import Model, Sequential\nfrom keras import regularizers\n\nkernel_regularizer = regularizers.l2(0.0001)\n\nfinal_activation = 'softmax'\nentropy = 'sparse_categorical_crossentropy'\nn_classes = len(CATEGORIES)\nprint('> {} classes'.format(n_classes))","e4655dfd":"def FCLayers(baseModel):\n    baseModel.trainable = True\n    headModel = baseModel.output\n    headModel = Dropout(0.5, seed=73)(headModel)\n    headModel = Dense(n_classes, activation=final_activation)(headModel)\n    model = Model(inputs = baseModel.input, outputs = headModel)\n    5\n    return model","b331fafd":"from keras.layers.merge import concatenate\n\ndef Inception_block(input_layer, f1, f2, f3, f4):    \n    \n    path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n    \n    path2 = Conv2D(filters = f2[0], kernel_size = (1,1), \n                   padding = 'same', activation = 'relu')(input_layer)\n    \n    path2 = Conv2D(filters = f2[1], kernel_size = (3,3), \n                   padding = 'same', activation = 'relu')(path2)\n\n    path3 = Conv2D(filters = f3[0], kernel_size = (1,1), \n                   padding = 'same', activation = 'relu')(input_layer)\n    \n    path3 = Conv2D(filters = f3[1], kernel_size = (5,5), \n                   padding = 'same', activation = 'relu')(path3)\n\n    path4 = MaxPooling2D((3,3), strides= (1,1), \n                         padding = 'same')(input_layer)\n    \n    path4 = Conv2D(filters = f4, kernel_size = (1,1), \n                   padding = 'same', activation = 'relu')(path4)\n    \n    output_layer = concatenate([path1, path2, path3, path4], axis = -1)\n\n    return output_layer","e68cff5e":"# auxiliary_classifiers\ndef Extra_network_2(X):\n    X2 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n    X2 = Conv2D(filters = 128, kernel_size = (1,1), \n                padding = 'same', activation = 'relu')(X2)\n    \n    X2 = Flatten()(X2)\n    X2 = Dense(1024, activation = 'relu')(X2)\n    X2 = Dropout(0.5)(X2)\n    X2 = Dense(n_classes, activation = final_activation, name=\"output2\")(X2)\n    return X2\n\n\ndef Extra_network_1(X):\n    X1 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n    X1 = Conv2D(filters = 128, kernel_size = (1,1), \n                padding = 'same', activation = 'relu')(X1)\n    \n    X1 = Flatten()(X1)\n    X1 = Dense(1024, activation = 'relu')(X1)\n    X1 = Dropout(0.5)(X1)\n    X1 = Dense(n_classes, activation = final_activation, name=\"output1\")(X1)\n    return X1","d5d99b77":"def layer_4(X):\n    X = Inception_block(X, 192, (96, 208) , (16, 48), 64)\n    \n    X1 = Extra_network_1(X)\n    \n    X = Inception_block(X, 160, (112, 224), (24, 64), 64)\n    X = Inception_block(X, 128, (128, 256), (24, 64), 64)\n    X = Inception_block(X, 112, (144, 288), (32, 64), 64)\n    \n    X2 = Extra_network_2(X)\n    \n    X = Inception_block(X, 256, (160, 320), (32, 128), 128)\n    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n    \n    return X, X1, X2\n\ndef layer_3(X):\n    X = Inception_block(X, 64, (96, 128), (16, 32), 32)\n    X = Inception_block(X, 128, (128, 192), (32, 96), 64)\n    X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n    \n    return X\n\ndef layer_2(X):\n    X = Conv2D(filters = 64, \n               kernel_size = 1, \n               strides = 1, \n               padding = 'same', \n               activation = 'relu')(X)\n    \n    X = Conv2D(filters = 192, \n               kernel_size = 3, \n               padding = 'same', \n               activation = 'relu')(X)\n    \n    X = MaxPooling2D(pool_size= 3, strides = 2)(X)\n    \n    return X","079c2cc8":"def load_GoogLeNet():\n    input_layer = Input(shape = input_shape)\n    \n    X = Conv2D(64, kernel_size = 7, strides = 2, \n               padding = 'valid', activation = 'relu')(input_layer)\n    \n    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n    \n    X = layer_2(X)\n    X = layer_3(X)\n    X, X1, X2 = layer_4(X)\n\n    X = Inception_block(X, 256, (160, 320), (32, 128), 128)\n    X = Inception_block(X, 384, (192, 384), (48, 128), 128)\n\n    X = GlobalAveragePooling2D()(X)\n    X = Dropout(0.6)(X)\n    \n    X = Dense(n_classes, activation = final_activation, name=\"output3\")(X)\n  \n    model = Model(input_layer, [X, X1, X2], name = 'GoogLeNet')\n\n    return model\n\nload_GoogLeNet().summary()","1118e6a2":"from keras.applications import DenseNet121\n\ndef load_DenseNet121():\n    input_tensor = Input(shape=input_shape)\n    baseModel = DenseNet121(pooling='avg',\n                            include_top=False, \n                            input_tensor=input_tensor)\n    \n    model = FCLayers(baseModel)\n    return model\n\nload_DenseNet121().summary()","8e514339":"def getMetrics(name, type_):\n    if name == 'GoogLeNet':\n        if type_ == 'accuracy':\n            return 'output3_accuracy'\n        if type_ == 'loss':\n            return 'output3_loss'\n        if type_ == 'val_accuracy':\n            return 'val_output3_accuracy'\n        if type_ == 'val_loss':\n            return 'val_output3_loss'\n        \n    else:\n        if type_ == 'accuracy':\n            return 'accuracy'\n        if type_ == 'loss':\n            return 'loss'\n        if type_ == 'val_accuracy':\n            return 'val_accuracy'\n        if type_ == 'val_loss':\n            return 'val_loss'","48b0ff3e":"from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n          \nEPOCHS = 120\npatience = 3\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\n\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n        \ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)\/rampup_epochs * epoch + start_lr\n    elif epoch < rampup_epochs + sustain_epochs:\n        return max_lr\n    else:\n        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n        \ndef getCallbacks(name):\n    class myCallback(Callback):\n        def on_epoch_end(self, epoch, logs={}):\n            if ((logs.get(getMetrics(name,'accuracy'))>=0.999)):\n                print(\"\\nLimits Reached cancelling training!\")\n                self.model.stop_training = True\n\n            \n    end_callback = myCallback()\n\n    lr_plat = ReduceLROnPlateau(patience = 2, mode = 'min')\n\n    lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=False)\n\n    early_stopping = EarlyStopping(patience = patience, monitor=getMetrics(name, 'val_loss'),\n                                 mode='min', restore_best_weights=True, \n                                 verbose = 1, min_delta = .00075)\n\n\n    checkpoint_filepath = name + '_Weights.h5'\n\n    model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n                                        save_weights_only=True,\n                                        monitor=getMetrics(name, 'val_loss'),\n                                        mode='min',\n                                        verbose = 1,\n                                        save_best_only=True)\n\n    import datetime\n    log_dir=\"logs\/fit\/\" + '_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  \n    tensorboard_callback = TensorBoard(log_dir = log_dir, write_graph=True, histogram_freq=1)\n\n    return [end_callback, \n             lr_callback, \n             model_checkpoints,\n             early_stopping,\n             #tensorboard_callback,\n             lr_plat\n            ]\n\nGoogLeNet_callbacks = getCallbacks('GoogLeNet')\ncallbacks = getCallbacks('DenseNet121')","6769c8b8":"def CompileModel(name, model):\n    if name == 'GoogLeNet':\n        model.compile(optimizer='adam', loss=entropy, metrics={\"output1\":\"accuracy\", \"output2\":\"accuracy\", \"output3\":\"accuracy\"})\n    else:\n        model.compile(optimizer='adam', loss=entropy, metrics=[\"accuracy\"])\n    return model\n\ndef FitModel(model, name):\n    callbacks_ = callbacks\n    if name == 'GoogLeNet':\n        callbacks_ = GoogLeNet_callbacks\n    history = model.fit(train_generator, \n                        epochs=EPOCHS,\n                        callbacks=callbacks_,\n                        validation_data = val_generator,\n                        steps_per_epoch=(len(train_generator.labels) \/ 80),\n                        validation_steps=(len(val_generator.labels) \/ 80),\n                       )\n    \n    model.load_weights(name + '_Weights.h5')\n\n    final_accuracy_avg = np.mean(history.history[getMetrics(name, \"val_accuracy\")][-5:])\n\n    final_loss = history.history[getMetrics(name, \"val_loss\")][-1]\n  \n    group = {history: 'history', name: 'name', model: 'model', final_accuracy_avg:'acc', final_loss: 'loss'}\n\n    print('\\n')\n    print('---'*15)\n    print(name,' Model')\n    print('Total Epochs :', len(history.history[getMetrics(name, 'loss')]))    \n    print('Restoring best Weights')\n    \n    index = (len(history.history[getMetrics(name, 'loss')]) - (patience + 1))\n    print('---'*15)\n    print('Best Epoch :', index)\n    print('---'*15)\n    \n    train_accuracy = history.history[getMetrics(name, 'accuracy')][index]\n    train_loss = history.history[getMetrics(name, 'loss')][index]\n    \n    val_accuracy = history.history[getMetrics(name, 'val_accuracy')][index]\n    val_loss = history.history[getMetrics(name, 'val_loss')][index]\n\n    print('Accuracy on train:', train_accuracy,\n          '\\tLoss on train:', train_loss)\n    \n    print('Accuracy on val:', val_accuracy ,\n          '\\tLoss on val:', val_loss)\n    print('---'*15)\n\n    return model, history","021a5f35":"def BuildModel(name):\n    if name == 'GoogLeNet':\n        prepared_model = load_GoogLeNet() \n    if name == 'DenseNet121':\n        prepared_model = load_DenseNet121()\n        \n    compiled_model = CompileModel(name, prepared_model)\n    return compiled_model","4e3a22d1":"g_compiled_model = BuildModel('GoogLeNet')\ng_model, g_history = FitModel(g_compiled_model, 'GoogLeNet')","f17b6074":"d_compiled_model = BuildModel('DenseNet121')\nd_model, d_history = FitModel(d_compiled_model, 'DenseNet121')","fe0ae0ff":"%matplotlib inline\ndef print_graph(item, index, history):\n    plt.figure()\n    train_values = history.history[item][0:index]\n    plt.plot(train_values)\n    test_values = history.history['val_' + item][0:index]\n    plt.plot(test_values)\n    plt.legend(['training','validation'])\n    plt.title('Training and validation '+ item)\n    plt.xlabel('epoch')\n    plt.show()\n    plot = '{}.png'.format(item)\n    plt.savefig(plot)","2f427f93":"import seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, roc_auc_score, plot_roc_curve, accuracy_score, classification_report, confusion_matrix\n\ndef test_set_results(pred_value, n=1):    \n    y_test = test_generator.labels\n    X_test, _ = test_generator.next()\n    \n    corr_pred = metrics.confusion_matrix(y_test, pred_value)\n    fig=plt.figure(figsize=(10, 8))\n    ax = plt.axes()\n    \n    sns.heatmap(corr_pred,annot=True, fmt=\"d\",cmap=\"Purples\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n    ax.set_title('Dense Output {}'.format(n))\n    plt.show()\n    \n    n_correct = np.int(corr_pred[0][0] + corr_pred[1][1] + corr_pred[2][2])\n    print('...'*15)\n\n    print('> Correct Predictions:', n_correct)\n    \n    n_wrongs = len(y_test) - n_correct\n    print('> Wrong Predictions:', n_wrongs)\n    print('...'*15)\n    \n    print(classification_report(test_generator.labels, pred_value, target_names=CATEGORIES))","f210031b":"def printResults(name, model):\n    predictions = model.predict(test_generator, verbose=1)\n    preds = np.argmax(predictions, axis=1)\n    test_set_results(preds)","14b253c1":"def model_summary(model, history, name):\n    index = (len(history.history[getMetrics(name, 'loss')]) - (patience + 1))\n    print('Best Epochs: ', index)\n    \n    if name == 'GoogLeNet':\n        results = model.evaluate(test_generator, verbose=1)\n        loss, output3_loss, output1_loss, output2_loss, output3_accuracy, output1_accuracy, output2_accuracy = results\n        \n        for i in range(3):\n            n = i + 1\n            out_layer = 'Output Layer {}'.format(n)\n            \n            if n == 1:\n                test_accuracy = output1_accuracy\n                test_loss = output1_loss\n\n            if n == 2:\n                test_accuracy = output2_accuracy\n                test_loss = output2_loss\n                \n            if n == 3:\n                test_accuracy = output3_accuracy\n                test_loss = output3_loss\n                \n                \n            output_name = 'output{}_'.format(n)\n            train_accuracy, train_loss = history.history[output_name + 'accuracy'][index], history.history[output_name + 'loss'][index]\n            \n  \n            print_graph(output_name + 'loss', index, history)\n            print_graph(output_name + 'accuracy', index, history)\n        \n            print('---'*15)  \n            print('GoogLeNet Dense output {}:'.format(n))\n            \n            print('> Accuracy on train :'.format(out_layer), train_accuracy, \n                  '\\tLoss on train:',train_loss)\n        \n            print('> Accuracy on test :'.format(out_layer), test_accuracy,\n                  '\\tLoss on test:',test_loss)\n            \n            print('---'*15)\n            print('> predicting test')\n            print('---'*15)\n            \n            predictions = model.predict(test_generator, verbose=1)\n            preds = np.argmax(predictions[i], axis=1)\n            test_set_results(preds, n)\n                \n    else:\n        test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n        \n        train_accuracy = history.history['accuracy'][index]\n        train_loss = history.history['loss'][index]\n\n        print_graph('loss', index, history)\n        print_graph('accuracy', index, history)\n        \n        print('---'*15) \n        print(name)\n        print('> Accuracy on train:',train_accuracy, \n              '\\tLoss on train:',train_loss)\n        \n        print('> Accuracy on test:',test_accuracy,\n              '\\tLoss on test:',test_loss)\n        \n        print('---'*15)\n        print('> predicting test')\n        print('---'*15)\n        \n        printResults(name, model)","5f64194c":"model_summary(g_model, g_history, 'GoogLeNet')","14509e9c":"model_summary(d_model, d_history, 'DenseNet121')","eb5e86ff":"from IPython.display import FileLink","310ed8ee":"g_model.save('GoogLeNet_model.h5')\nFileLink(r'.\/GoogLeNet_model.h5')","3a0a798b":"d_model.save('DenseNet121_model.h5')\nFileLink(r'.\/DenseNet121_model.h5')","dbf72b4e":"from IPython.display import IFrame\nIFrame(src='https:\/\/model-tester.web.app\/covid_19', width='100%', height=1000)","8e7b7518":"## **Call Backs**","77c03e26":"## **DenseNet121**","d358d945":"## **Data Preparation**","60249f72":"## **Virus Classification**\n\n+ **Datasets**: chest-xray-pneumonia + covidx-cxr2\n\n+ **Classes**: Normal, Pneumonia, Covid_19\n\n+ **Models**: GoogLeNet, DenseNet121","cea3b711":"## **Training DenseNet121**","1bb98cea":"## **Deployed model**\n\n+ **Models**: DenseNet121\n+ **Size**: 85.9 MB\n+ **Build With**: React Native\n+ **Supported Versions**: ANDROID, IOS, WEB","1d781e8c":"### **Getting image path and labels from *.txt files**","358261b8":"![](https:\/\/miro.medium.com\/max\/664\/1*4nb4lVJnaKJZAu6Lthuz2Q.png)","1e7400db":"## **Building Models**","b4e4f5e0":"### **Visualization**","d0fefb1c":"## **GoogLeNet Results**","9b74872d":"## **Compile** and **Fit Model**","f2825a81":"## **DenseNet121 Results**","da9d09d4":"## **Model Evaluation on the TestSet**","263a0f84":"### **Inception Block**\n\n![](https:\/\/miro.medium.com\/max\/2400\/1*zIcot5nm9q_TC8zqcGQ7Dg.png)","efa8cdef":"## **GoogLenet**\n\n**Blog Reference**: https:\/\/medium.com\/mlearning-ai\/implementation-of-googlenet-on-keras-d9873aeed83c","f98d2556":"## **Saving Models**","da29980b":"## **Training GoogLeNet**"}}