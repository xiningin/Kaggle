{"cell_type":{"75d7e948":"code","ce86abb3":"code","1b79d765":"code","09270167":"code","55d95dad":"code","b4cb75c9":"code","7e73cae7":"code","5ca12635":"code","0499f51b":"markdown","7c684275":"markdown"},"source":{"75d7e948":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport optuna\nfrom pickle import dump, load\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_log_error\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess","ce86abb3":"path = '\/kaggle\/input\/store-sales-time-series-forecasting\/'\ntrain = pd.read_csv(\n                f'{path}train.csv',\n                usecols=['store_nbr', 'family', 'date', 'sales'],\n                dtype={\n                    'store_nbr': 'category',\n                    'family': 'category',\n                    'sales': 'float32',\n                },\n                parse_dates=['date'],\n                infer_datetime_format=True,\n)\n\ntrain['date'] = train.date.dt.to_period('D')\ntrain = train.set_index(['store_nbr', 'family', 'date']).sort_index()\n\ny = train.unstack(['store_nbr', 'family']).loc[\"2017\"]\ny.head()","1b79d765":"test = pd.read_csv(f'{path}\/test.csv',\n                    dtype={\n                        'store_nbr': 'category',\n                        'family': 'category',\n                        'onpromotion': 'uint32',\n                    },\n                    parse_dates=['date'],\n                    infer_datetime_format=True,\n)\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['store_nbr', 'family', 'date']).sort_index()\n\ntest.head()","09270167":"# Create training data\nfourier = CalendarFourier(freq='M', order=4)\ndp = DeterministicProcess(\n    index=y.index,\n    order=1,\n    seasonal=True,\n    additional_terms=[fourier],\n    drop=True,\n)\nX = dp.in_sample()\nX['NewYear'] = (X.index.dayofyear == 1)\nX.head()","55d95dad":"def optimize_model(trial):\n    \n    params = {\n        'random_state' : 42,\n        'n_estimators' : trial.suggest_int('n_estimators', 50, 300, log=True)\n    }\n    \n    model = MultiOutputRegressor(RandomForestRegressor(**params), n_jobs=-1)\n    \n    cv_metrics = np.zeros(3)\n    tscv = TimeSeriesSplit(n_splits=3, test_size=16)\n    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n    \n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_valid)\n        y_pred[y_pred<0] = 0 #Fix undershooting\n        \n        cv_metrics[i] = mean_squared_log_error(y_valid, y_pred)\n        \n        trial.report(np.mean(cv_metrics[:i+1]), i)\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n        \n    return np.mean(cv_metrics)","b4cb75c9":"def optimize_model(n_trials=10):\n    \n    #Define Optuna Sampler and Pruner\n    sampler = optuna.samplers.TPESampler()\n    pruner  = optuna.pruners.MedianPruner(n_startup_trials=1, \n                                      n_warmup_steps=0, \n                                      interval_steps=1)\n    \n    study = optuna.create_study(sampler=sampler, pruner=pruner,\n                                storage='sqlite:\/\/\/results\/lightgbm_hyperparams.db', \n                                load_if_exists=True, direction=\"minimize\", \n                                study_name='Time_series_forecasting')\n    \n    study.optimize(optimize_model, n_trials=n_trials)\n    params = {\n        'random_state' : 42\n    }    \n    params.update(study.best_params)\n    model = MultiOutputRegressor(LGBMRegressor(**params))\n    model.fit(X, y)\n    dump(model, open(f'models\/model.pkl', 'wb'))\n    \n    return model","7e73cae7":"#model = optimize_model(n_trials=10) Optimize the model!\n\n#Params from the Optuna optimization process\nparams = {\n    'random_state' : 42,\n    'n_estimators': 112\n}\nmodel = MultiOutputRegressor(RandomForestRegressor(**params), n_jobs=-1)\nmodel.fit(X, y)","5ca12635":"X_test = dp.out_of_sample(steps=16) #Next 16 steps\nX_test.index.name = 'date'\nX_test['NewYear'] = (X_test.index.dayofyear == 1)\n\ny_pred = model.predict(X_test)\ny_pred[y_pred<0] = 0 #Fix undershooting\n\ny_submit = pd.DataFrame(y_pred, index=X_test.index, columns=y.columns)\ny_submit = y_submit.stack(['store_nbr', 'family'])\ny_submit = y_submit.join(test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('.\/submission.csv', index=False)","0499f51b":"# Starting Notebook for Time Series Forecasting\n\nThe time series of this competitions is similar to the M5 competition (https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy) held on kaggle few months ago. This notebook aims at introducing a relatively simple baseline solution using a deterministic process along with LightGBM (with Optuna for hyperparameters tuning). \n\nNote that this notebook will only use a deterministic process to infer the amount of sales. This means that we do not care about oil prices\/promotions\/holydays (except for NYE). Additionally, we will train a multioutput regressor, so we will neglect the interactions between various products in the same store. Again, this aims at being a baseline solutions, for a more accurate prediction I suggest to follow the https:\/\/github.com\/Mcompetitions\/M5-methods solutions, that fully exploit the properties of this dataset.  ","7c684275":"## Load the train and test dataset\n\nWe will only use the train and test dataset. Since this is a baseline solution, we will just need the deterministic process to make predictions"}}