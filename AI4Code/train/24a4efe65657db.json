{"cell_type":{"a54d20aa":"code","80128dff":"code","a95b6e5f":"code","94dd0001":"code","0560401e":"code","092c360d":"code","e874e38b":"code","ee469b1a":"code","f425cf4a":"code","54fe937b":"code","c8843507":"code","6e5d75e6":"code","b1bd5557":"code","8f5564ad":"code","aa2872fc":"code","b66d2d70":"code","6004cdb6":"code","d3c0f893":"code","58f82153":"code","af5c6a12":"code","31f5aba7":"code","40949ab6":"code","b7e6e96d":"code","f54ac72e":"code","82ecb977":"code","67fcf9b4":"code","742d6f03":"code","efc268d5":"code","5070883b":"code","dbca3f49":"code","2da8acd6":"code","b9887678":"code","b232eb83":"code","adb7d623":"code","4119631f":"code","2cd8ca17":"code","b7a13aea":"code","66b7dfa4":"code","a4067002":"code","cf6dbac9":"code","83d0e9d8":"code","251c855d":"code","cd12bdc9":"code","e98205f4":"code","acdc92aa":"code","4080e675":"code","1f164820":"code","8c0d2f35":"markdown","90dc64b9":"markdown","28be3f87":"markdown","fe82b682":"markdown","3a2056a6":"markdown","f87fd355":"markdown","526ec057":"markdown","28c1fb36":"markdown","41eafed9":"markdown","014a8a6e":"markdown","2807f8ce":"markdown","ff29047b":"markdown","35aa70bb":"markdown","8c47d360":"markdown","c3d07c95":"markdown","2d044000":"markdown","924f2ec1":"markdown","a96b9321":"markdown","34fb3ad8":"markdown","48ed64e8":"markdown","639cff19":"markdown","c5d8a25b":"markdown","36618786":"markdown","eb15dd4d":"markdown","5be7eca0":"markdown","9ac2e35f":"markdown","3cb78ce6":"markdown","f0c5adae":"markdown","03d0ef6e":"markdown","3ba741f8":"markdown","2c4de1cc":"markdown","ae1b6123":"markdown","73b0fdd2":"markdown","65819304":"markdown","0c6b2ab8":"markdown","6330c1b3":"markdown","296d06a8":"markdown","61691225":"markdown","972f32a5":"markdown","25038485":"markdown","9e6949f6":"markdown","d7184188":"markdown","09d6c9b8":"markdown","9f93fcc0":"markdown","24b0ab35":"markdown","d517fefd":"markdown","9d2060ca":"markdown","bf92e38c":"markdown","1798d7d3":"markdown","011d0d6a":"markdown","147cda7c":"markdown","ce223b36":"markdown","0ff2a94b":"markdown","d10e8fbc":"markdown","dd99b02b":"markdown","ac5484d7":"markdown","ef61678a":"markdown","b63137d7":"markdown","c97c213a":"markdown","bbb6a221":"markdown","c188f9df":"markdown","1668e585":"markdown","616bd300":"markdown","4ea8d005":"markdown","aadec8e4":"markdown","282a739e":"markdown","98d433dd":"markdown"},"source":{"a54d20aa":"# Import basic toolkits for the analysis\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport datetime\n\n# Import Stats Tool\nfrom scipy import stats\n\n# Import Zscore & IQR Tools\nfrom mlxtend.preprocessing import minmax_scaling \n\n# Import ML Tools\nimport matplotlib.dates as md\nfrom mpl_toolkits.axes_grid1 import host_subplot\nimport mpl_toolkits.axisartist as AA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Remove warnings for aesthetic\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Library for map drawing & converting longitude and latitude into Points & Polygon\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\ncrs = {'init':'epsg:4326'}  #specify the EPSG:4326 CRS\nimport contextily as cx","80128dff":"# Import Datasets\nmeta = pd.read_csv('..\/input\/buildingdatagenomeproject2\/metadata.csv')\ndf_electricity = pd.read_csv(\"..\/input\/buildingdatagenomeproject2\/electricity.csv\", parse_dates=True)","a95b6e5f":"# Filter buildings that have electricity data\nmeta_electricity = meta.loc[meta[\"electricity\"] == \"Yes\"]\n\n# Drop the unused data coloumns, our study focuses on \"US\/Eastern timezone Education College\/University Buidlings\"\nmeta_electricity = meta_electricity.drop(columns = [\"hotwater\",\"chilledwater\",\"steam\",\"water\",\"irrigation\",\"solar\",\"gas\",\"heatingtype\"])\n\n# Filter the timezones - US\/Eastern\uff0c primaryspaceusage - Education, industry - Education\nmeta_electricity = meta_electricity.loc[meta_electricity[\"timezone\"] == \"US\/Eastern\"]\nmeta_electricity = meta_electricity.loc[meta_electricity[\"primaryspaceusage\"] == \"Education\"]\nmeta_electricity = meta_electricity.loc[meta_electricity[\"industry\"] == \"Education\"]\nmeta_electricity.info()","94dd0001":"# Map drawing ref: https:\/\/towardsdatascience.com\/geopandas-101-plot-any-data-with-a-latitude-and-longitude-on-a-map-98e01944b972\n# import base map (source:https:\/\/www.kaggle.com\/ykai5229\/basemap2)\nUSAmap = gpd.read_file(\"..\/input\/basemap2\/USA.shp\")\n\n# create geometry point from long and lat\ngeometry = [Point(xy) for xy in zip(meta_electricity[\"lng\"],meta_electricity[\"lat\"])]\n\n# creating the geo_dataframe for meta_electricity\ngeo_meta_electricity = gpd.GeoDataFrame (meta_electricity, #specify data\n                                        crs =crs, # specify coordinate reference system\n                                        geometry = geometry) # specify the geometry list\n\n# Locate the location of Cockatoo, Peacock, Moose, Crow on the US map\nfig,ax = plt.subplots(figsize=(10,10))\nUSAmap.plot(ax = ax, alpha = 0.4, color = \"grey\")\ngeo_meta_electricity[geo_meta_electricity[\"site_id\"] == \"Cockatoo\"].plot(ax=ax, color =\"blue\", label = \"Cockatoo\")\ngeo_meta_electricity[geo_meta_electricity[\"site_id\"] == \"Peacock\"].plot(ax=ax, color =\"red\", label = \"Peacock\")\ngeo_meta_electricity[geo_meta_electricity[\"site_id\"] == \"Moose\"].plot(ax=ax, color =\"green\", label = \"Moose\")\ngeo_meta_electricity[geo_meta_electricity[\"site_id\"] == \"Crow\"].plot(ax=ax, color =\"yellow\", alpha = 0.4, label = \"Crow\", marker ='^')\nax.set_axis_off()\n\nplt.legend(prop={'size':10})","0560401e":"# Number of buildings under each sub Education usage\nmeta_electricity[\"sub_primaryspaceusage\"].value_counts(ascending=False)","092c360d":"# Number of buildings at each site\nmeta_electricity[\"site_id\"].value_counts(ascending=False)","e874e38b":"# Convert timestamp column into datetime format\ndf_electricity['timestamp'] = df_electricity['timestamp'].apply(pd.to_datetime) \n\n# Extract building_id of the  selected 78 building id into a separate list\nbuilding_list = meta_electricity[\"building_id\"].tolist()\n\n# Filter out all electricity information for the selected buildings from df_electricity\ndf_electricity_building = pd.concat([df_electricity[building_list],df_electricity.timestamp],axis=1)\ndf_electricity_building","ee469b1a":"# Visualize the timeseries plot can help to have quick overview on all the buildings' electricity data status\ndf_electricity_building.set_index(\"timestamp\").plot(figsize=(30,40), subplots=True)\nplt.xlabel(\"Timestamp by hourly frequency\")\nplt.ylabel(\"Electricity Consumption\")\n# Reference: https:\/\/www.kaggle.com\/claytonmiller\/bdg2-pandas-times-series-data-analysis-example?scriptVersionId=74064984&cellId=46","f425cf4a":"df_electricity_building.set_index(\"timestamp\").resample(\"M\").sum().plot(figsize=(30,40), subplots=True)\nplt.xlabel(\"Timestamp by monthly frequency\")\nplt.ylabel(\"Electricity Consumption\")\n# Reference: https:\/\/www.kaggle.com\/claytonmiller\/bdg2-pandas-times-series-data-analysis-example?scriptVersionId=74064984&cellId=47","54fe937b":"#  The selected 78 education building\ndf_building = df_electricity[building_list]\n\n# Check & Visualize the number of null value from df_electricity data with additional information from meta_electricity data\nmissing_count = df_building.isnull().sum().sort_values(ascending=False)\nmissing_count = missing_count.to_frame(name=\"missing_count\")\nmissing_count.index.names = [\"building_id\"]\nmissing_count.reset_index()\nget_meta_site_id = meta_electricity.iloc[:,[0,1,5,22]]\nmissing_count_visual = pd.merge(missing_count, get_meta_site_id, on=\"building_id\")\nmissing_count_visual.info()","c8843507":"sns.set(rc={'figure.figsize':(100,100)},font_scale=5,style='whitegrid')\nsns.barplot(y =\"building_id\",x = \"missing_count\",data = missing_count_visual, hue=\"site_id\",palette=\"Set2\")","6e5d75e6":"# Total count to frame\ntotal_count = df_building.count()\ntotal_count = total_count.to_frame(name=\"total_count\")\n\n# Create Missing Data Analysis Table\nmissing_data_analysis_table = pd.concat([missing_count,total_count],axis=1)\nmissing_data_analysis_table [\"missing_rate\"] = (missing_data_analysis_table.missing_count\/len(df_building))*100\n\n# Define function to grade the data completness \ndef Data_Completness_Grade(n):\n    if n == 0:\n        return \"Perfect\"\n    if 80 < n < 100:\n        return \"Bad\"\n    if 50 < n < 80:\n        return \"Poor\"\n    if 30 < n < 50:\n        return \"Fine\"\n    if 0 < n < 30:\n        return \"Good\"\n    if n == 100:\n        return \"No Reading\"\n    \nmissing_data_analysis_table [\"DataCompletness_Grade\"] = missing_data_analysis_table.missing_rate.map(Data_Completness_Grade)\nmissing_data_analysis_table = missing_data_analysis_table.reset_index()\nmissing_data_analysis_table = missing_data_analysis_table.rename(columns = {\"index\" : \"building_id\"})\n\n# Explore number of buildings under different data completeness grade and generate the chart to visualise\nmissing_data_analysis_table_groupbycompletness = missing_data_analysis_table.groupby([\"DataCompletness_Grade\"]).count().drop(columns=[\"missing_count\",\"total_count\",\"building_id\"])\nmissing_data_analysis_table_groupbycompletness = missing_data_analysis_table_groupbycompletness.rename(columns={\"missing_rate\":\"building_count\"})\nmissing_data_analysis_table_groupbycompletness\nsns.set(rc={'figure.figsize':(10,10)},font_scale=2,style='whitegrid')\nsns.heatmap(data=missing_data_analysis_table_groupbycompletness, cmap=\"PiYG\",annot=True)","b1bd5557":"# Identify Buildings with Perfect Electricity Reading Completness \nBuildings_PerfectData = missing_data_analysis_table.loc[missing_data_analysis_table['DataCompletness_Grade'] == \"Perfect\"]\nBuildings_PerfectData\n# Identify Buildings with Good Electricity Reading Completness\nBuildings_GoodData = missing_data_analysis_table.loc[missing_data_analysis_table['DataCompletness_Grade'] == \"Good\"]\nBuildings_GoodData\n# Identify Buildings with Poor Electricity Reading Completness\nBuildings_PoorData = missing_data_analysis_table.loc[missing_data_analysis_table['DataCompletness_Grade'] == \"Poor\"]\nBuildings_PoorData\n# Show poor data output as example","8f5564ad":"df_building_0count = (df_building == 0).sum(axis=0).to_frame(name=\"df_building_0count\") \n\ndf_building_0count \n\ndf_building_0count=df_building_0count.reset_index() \n\ndf_building_0count[df_building_0count[\"index\"].isin([\"Moose_education_Ricardo\",\"Moose_education_Abbie\",\"Moose_education_Sasha\"])] ","aa2872fc":"# Code Reference: https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/feature-engineering-how-to-detect-and-remove-outliers-with-python-code\/\n\n# Set dataframe index as \"timestamp\" prior to outlier detection and removal\ndf_electricity_building.set_index('timestamp', inplace=True)\n\n# Filter selected buildings and copy to new dataframe df_building IQR \ndf_buildingIQR = df_electricity_building.loc[:,['Moose_education_Ricardo','Moose_education_Abbie','Moose_education_Sasha']] \ndf_buildingIQR","b66d2d70":"# Copy dataframe of 3 selected buildings to new 3 separate dataframe for IQR processing \ndf_buildingIQR['building1'] = df_buildingIQR['Moose_education_Ricardo']   \ndf_buildingIQR['building2'] = df_buildingIQR['Moose_education_Abbie'] \ndf_buildingIQR['building3'] = df_buildingIQR['Moose_education_Sasha'] \ndf =  df_buildingIQR\n\n# Define first and third quartile of data as normal distribution range \npercentile25a = df['building1'].quantile(0.25)   \npercentile75a = df['building1'].quantile(0.75)   \niqr1 = percentile75a - percentile25a \n\npercentile25b = df['building2'].quantile(0.25)\npercentile75b = df['building2'].quantile(0.75)   \niqr2 = percentile75b - percentile25b\n\npercentile25c = df['building3'].quantile(0.25)\npercentile75c = df['building3'].quantile(0.75)   \niqr3 = percentile75c - percentile25c\n\n# Define upper and lower limit for data distribution    \nupper_limit1 = percentile75a + 1.5 * iqr1   \nlower_limit1 = percentile25a - 1.5 * iqr1   \ndf[df['building1'] > upper_limit1]   \ndf[df['building1'] < lower_limit1]  \n\nupper_limit2 = percentile75b + 1.5 * iqr2   \nlower_limit2 = percentile25b - 1.5 * iqr2   \ndf[df['building2'] > upper_limit2]   \ndf[df['building2'] < lower_limit2]  \n\nupper_limit3 = percentile75c + 1.5 * iqr3   \nlower_limit3 = percentile25c - 1.5 * iqr3   \ndf[df['building3'] > upper_limit3]   \ndf[df['building3'] < lower_limit3] \n\n# Removal of outliers - data distribution below set limit  \nIQR1_df = df[df['building1'] < upper_limit1]  \nIQR2_df = df[df['building2'] < upper_limit2]\nIQR3_df = df[df['building3'] < upper_limit3]","6004cdb6":"# Display outlier data removed using IQR for 3 selected buildings \n\nprint(\"Moose_education_Ricardo outliers removed by IQR method =\",df_electricity['Moose_education_Ricardo'].count() - IQR1_df['building1'].count()) \n\nprint(\"Moose_education_Abbie outliers removed by IQR method =\",df_electricity['Moose_education_Abbie'].count() - IQR2_df['building2'].count()) \n\nprint(\"Moose_education_Sasha outliers removed by IQR method =\",df_electricity['Moose_education_Sasha'].count() - IQR3_df['building3'].count()) ","d3c0f893":"# Comparison plots for before and after outliers removal using IQR method\nplt.figure(figsize=(16,32))  \nsns.set_context(\"paper\", font_scale=0.9)\nplt.subplot(6,2,1)   \nsns.distplot(df_buildingIQR['Moose_education_Ricardo'],color='green')   \nplt.title(\"Dist Plot before IQR method outlier removal\",fontsize=12)   \nplt.subplot(6,2,2)   \nsns.boxplot(df_buildingIQR['Moose_education_Ricardo'],color='green')   \nplt.title(\"Box Plot before IQR method outlier removal\",fontsize=12)  \nplt.subplot(6,2,3)   \nsns.distplot(IQR1_df['Moose_education_Ricardo'],color='green')   \nplt.title(\"Dist Plot After IQR method outlier removal\",fontsize=12)   \nplt.subplot(6,2,4)   \nsns.boxplot(IQR1_df['Moose_education_Ricardo'],color='green')   \nplt.title(\"Box Plot After IQR method outlier removal\",fontsize=12)   \n\nplt.subplot(6,2,5)   \nsns.distplot(df_buildingIQR['Moose_education_Abbie'],color='magenta')   \nplt.title(\"Dist Plot before IQR method outlier removal\")   \nplt.subplot(6,2,6)   \nsns.boxplot(df_buildingIQR['Moose_education_Abbie'],color='magenta')   \nplt.title(\"Box Plot before IQR method outlier removal\")  \nplt.subplot(6,2,7)   \nsns.distplot(IQR2_df['Moose_education_Abbie'],color='magenta')   \nplt.title(\"Dist Plot After IQR method outlier removal\")   \nplt.subplot(6,2,8)   \nsns.boxplot(IQR2_df['Moose_education_Abbie'],color='magenta')   \nplt.title(\"Box Plot After IQR method outlier removal\")  \n\nplt.subplot(6,2,9)   \nsns.distplot(df_buildingIQR['Moose_education_Sasha'],color='pink')  \nplt.title(\"Dist Plot before IQR method outlier removal\")   \nplt.subplot(6,2,10)   \nsns.boxplot(df_buildingIQR['Moose_education_Sasha'],color='pink')   \nplt.title(\"Box Plot before IQR method outlier removal\")  \nplt.subplot(6,2,11)   \nsns.distplot(IQR3_df['Moose_education_Sasha'],color='pink')   \nplt.title(\"Dist Plot After IQR method outlier removal\")   \nplt.subplot(6,2,12)   \nsns.boxplot(IQR3_df['Moose_education_Sasha'],color='pink')   \nplt.title(\"Box Plot After IQR method outlier removal\") \n\n# Specify plots spacing   \nplt.tight_layout(pad=3.3)   \nplt.show()","58f82153":"# Timeseries data comparison plots for selected buildings electrical data before \/ after IQR outlier removal. \nplt.figure(figsize=(16,21))\nplt.subplot(6,2,1)   \nsns.lineplot(data=df_buildingIQR.loc[:,['Moose_education_Ricardo']])   \nplt.title(\"line Plot before IQR method outlier removal\") \nplt.subplot(6,2,2)   \nsns.lineplot(data=IQR1_df.loc[:,['building1']]) \nplt.title(\"line Plot after IQR method outlier removal\")\nplt.subplot(6,2,3)   \nsns.lineplot(data=df_buildingIQR.loc[:,['Moose_education_Abbie']])  \nplt.title(\"line Plot before IQR method outlier removal\") \nplt.subplot(6,2,4)   \nsns.lineplot(data=IQR2_df.loc[:,['building2']])\nplt.title(\"line Plot after IQR method outlier removal\")\nplt.subplot(6,2,5)   \nsns.lineplot(data=df_buildingIQR.loc[:,['Moose_education_Sasha']])\nplt.title(\"line Plot before IQR method outlier removal\")\nplt.subplot(6,2,6)   \nsns.lineplot(data=IQR3_df.loc[:,['building3']])\nplt.title(\"line Plot after IQR method outlier removal\")\nplt.tight_layout(pad=3.3)   \nplt.show() ","af5c6a12":"# list of outlier for building 3 - Moose_education_Sasha \n\nIQR3 = df[df['building3'] > upper_limit3] \nIQR_Outlier3 = IQR3.loc[:,['Moose_education_Sasha']] \nIQR_Outlier3 ","31f5aba7":"# Filter selected buildings and copy to new dataframe df_buildingzscore \ndf_buildingzscore = df_electricity_building.loc[:,['Moose_education_Ricardo','Moose_education_Abbie','Moose_education_Sasha']] \ndf_buildingzscore","40949ab6":"# Copy dataframe of 3 selected buildings to new 3 separate dataframe for Z-score processing \ndf_buildingzscore['building1'] = df_electricity_building['Moose_education_Ricardo'] \ndf_buildingzscore['building2'] = df_electricity_building['Moose_education_Abbie']\ndf_buildingzscore['building3'] = df_electricity_building['Moose_education_Sasha']\n\n# Define Z-score outlier identification and trimming of outliers\nHighest1 = df['building1'].mean() + 3*df['building1'].std()\nLowest1 = df['building1'].mean() - 3*df['building1'].std() \ndf[(df['building1'] > Highest1) | (df['building1'] < Lowest1)]   \nzscore1_df = df[(df['building1'] < Highest1) & (df['building1'] > Lowest1)]\n\nHighest2 = df['building2'].mean() + 3*df['building2'].std()\nLowest2 = df['building2'].mean() - 3*df['building2'].std() \ndf[(df['building2'] > Highest2) | (df['building2'] < Lowest2)]   \nzscore2_df = df[(df['building2'] < Highest2) & (df['building2'] > Lowest2)]\n\nHighest3 = df['building3'].mean() + 3*df['building3'].std()\nLowest3 = df['building3'].mean() - 3*df['building3'].std() \ndf[(df['building3'] > Highest3) | (df['building3'] < Lowest3)]   \nzscore3_df = df[(df['building3'] < Highest3) & (df['building3'] > Lowest3)] ","b7e6e96d":"#Display summary of outliers removed by Z-Score method \n\nprint(\"Moose_education_Ricardo outliers removed by Z-Score method =\",df_electricity['Moose_education_Ricardo'].count() - zscore1_df['building1'].count()) \n\nprint(\"Moose_education_Abbie outliers removed by Z-Score method =\",df_electricity['Moose_education_Abbie'].count() - zscore2_df['building2'].count()) \n\nprint(\"Moose_education_Sasha outliers removed by Z-Score method =\",df_electricity['Moose_education_Sasha'].count() - zscore3_df['building3'].count()) ","f54ac72e":"# Comparison plots for before and after outliers removal using Z-score method  \nplt.figure(figsize=(16,24)) \nplt.subplot(6,2,1)   \nsns.distplot(df_buildingzscore['Moose_education_Ricardo'],color='green')   \nplt.title(\"Dist Plot before Z-Score method outlier removal\")   \nplt.subplot(6,2,2)   \nsns.boxplot(df_buildingzscore['Moose_education_Ricardo'],color='green')   \nplt.title(\"Box Plot before Z-Score method outlier removal\")  \nplt.subplot(6,2,3)   \nsns.distplot(zscore1_df['building1'],color='green')   \nplt.title(\"Dist Plot After Z-Score method outlier removal\")   \nplt.subplot(6,2,4)   \nsns.boxplot(zscore1_df['building1'],color='green')   \nplt.title(\"Box Plot After Z-Score method outlier removal\")   \n\nplt.subplot(6,2,5)   \nsns.distplot(df_buildingzscore['Moose_education_Abbie'],color='magenta')   \nplt.title(\"Dist Plot before Z-Score method outlier removal\")   \nplt.subplot(6,2,6)   \nsns.boxplot(df_buildingzscore['Moose_education_Abbie'],color='magenta')   \nplt.title(\"Box Plot before Z-Score method outlier removal\")  \nplt.subplot(6,2,7)   \nsns.distplot(zscore2_df['building2'],color='magenta')   \nplt.title(\"Dist Plot After Z-Score method outlier removal\")   \nplt.subplot(6,2,8)   \nsns.boxplot(zscore2_df['building2'],color='magenta')   \nplt.title(\"Box Plot After Z-Score method outlier removal\")  \n\nplt.subplot(6,2,9)   \nsns.distplot(df_buildingzscore['Moose_education_Sasha'],color='pink')   \nplt.title(\"Dist Plot before Z-score method outlier removal\")   \nplt.subplot(6,2,10)   \nsns.boxplot(df_buildingzscore['Moose_education_Sasha'],color='pink')   \nplt.title(\"Box Plot before Z-Score method outlier removal\")  \nplt.subplot(6,2,11)   \nsns.distplot(zscore3_df['building3'],color='pink')   \nplt.title(\"Dist Plot After Z-Score method outlier removal\")   \nplt.subplot(6,2,12)   \nsns.boxplot(zscore3_df['building3'],color='pink')   \nplt.title(\"Box Plot After Z-Score method outlier removal\")  \n\n#Specify plots spacing   \nplt.tight_layout(pad=3.3)   \nplt.show() ","82ecb977":"plt.figure(figsize=(16,21))\nplt.subplot(6,2,1)   \nsns.lineplot(data=df_buildingzscore.loc[:,['Moose_education_Ricardo']])   \nplt.title(\"Dist Plot before Z-score method outlier removal\") \nplt.subplot(6,2,2)   \nsns.lineplot(data=zscore1_df.loc[:,['building1']]) \nplt.title(\"Dist Plot after Z-score method outlier removal\")\nplt.subplot(6,2,3)   \nsns.lineplot(data=df_buildingzscore.loc[:,['Moose_education_Abbie']])\nplt.title(\"Dist Plot before Z-score method outlier removal\")\nplt.subplot(6,2,4)   \nsns.lineplot(data=zscore2_df.loc[:,['building2']])\nplt.title(\"Dist Plot after Z-score method outlier removal\")\nplt.subplot(6,2,5)   \nsns.lineplot(data=df_buildingzscore.loc[:,['Moose_education_Sasha']])\nplt.title(\"Dist Plot before Z-score method outlier removal\")\nplt.subplot(6,2,6)   \nsns.lineplot(data=zscore3_df.loc[:,['building3']])\nplt.title(\"Dist Plot after Z-score method outlier removal\")\nplt.tight_layout(pad=3.3)   \nplt.show() ","67fcf9b4":"# List of Moose_education_Abbie Outliers \n\nzscore2 = df[(df_buildingzscore['Moose_education_Abbie'] > Highest2) + (df_buildingzscore['Moose_education_Abbie'] < Lowest2)].drop(columns=[\"Moose_education_Ricardo\",\"Moose_education_Sasha\",\"building1\",\"building2\",\"building3\"] )  \n\nzscore2 ","742d6f03":"#List of Moose_education_Sasha Outliers \n\nzscore3 = df[df_electricity_building['Moose_education_Sasha'] > Highest3].drop(columns=[\"Moose_education_Ricardo\",\"Moose_education_Abbie\",\"building1\",\"building2\",\"building3\"] )  \n\nzscore3 ","efc268d5":"# Preparing a separate dataframe for the 3 buildings\ndf = df_electricity_building.reset_index() \ndf_3Building = df[[\"Moose_education_Ricardo\",\"Moose_education_Abbie\", \"Moose_education_Sasha\",\"timestamp\"]]\n\nMoose_education_Ricardo = df_3Building.iloc[:,[0,-1]].rename(columns={'Moose_education_Ricardo':'reading'})\nMoose_education_Abbie  = df_3Building.iloc[:,[1,-1]].rename(columns={'Moose_education_Abbie':'reading'})\nMoose_education_Sasha = df_3Building.iloc[:,[2,-1]].rename(columns={'Moose_education_Sasha':'reading'})\n\nMoose_education_Ricardo[\"building_id\"] = \"Moose_education_Ricardo\" \nMoose_education_Abbie[\"building_id\"] = \"Moose_education_Abbie\" \nMoose_education_Sasha[\"building_id\"] = \"Moose_education_Sasha\"\n\ndf_ML = pd.concat([Moose_education_Ricardo,Moose_education_Abbie,Moose_education_Sasha])\n\n# Change the datetime format\ndf_ML['timestamp'] = df_ML['timestamp'].apply(pd.to_datetime)\n\n# Drop NaN Value; missing data is analyzed in section 2, hence, we will drop the empty data cells to ensure we feed the actual performance data in ML.\ndf_ML = df_ML.dropna()\n\n# As a check to ensure no negative values\ndf_ML.loc[df_ML['reading'] < 0]","5070883b":"df_ML['Year'] = df_ML['timestamp'].dt.year\ndf_ML['Month'] = df_ML['timestamp'].dt.month\ndf_ML['Day'] = df_ML['timestamp'].dt.day\ndf_ML['Hour'] = df_ML['timestamp'].dt.hour\ndf_ML['Minute'] = df_ML['timestamp'].dt.minute\ndf_ML[\"IsWeekend\"] = df_ML[\"timestamp\"].dt.dayofweek > 4","dbca3f49":"# Define function to classify timeframe\ndef Timeframe(n):\n    if -1 < n < 7:\n        return 1\n    if 6 < n < 11:\n        return 2\n    if 10 < n < 18:\n        return 3\n    if 17 < n < 24:\n        return 4\n\n# Add in time\ndf_ML [\"Timeframe\"] = df_ML.Hour.map(Timeframe)","2da8acd6":"# Dataframe for respective building to carry out machine learning outliers detection\nMoose_education_Ricardo_ML = df_ML.loc[df_ML['building_id'] == \"Moose_education_Ricardo\"] \nMoose_education_Sasha_ML = df_ML.loc[df_ML['building_id'] == \"Moose_education_Sasha\"] \nMoose_education_Abbie_ML = df_ML.loc[df_ML['building_id'] == \"Moose_education_Abbie\"]","b9887678":"data_onesvm_1 = Moose_education_Ricardo_ML[[\"reading\",\"Year\",\"Month\",\"Hour\",\"Timeframe\",\"IsWeekend\"]]\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(data_onesvm_1)\ndata_onesvm_1_1 = pd.DataFrame (np_scaled)\n\noutlier_fraction = 0.01\n\n#train oneclass SVM\nmodel = OneClassSVM(nu = outlier_fraction,kernel = \"rbf\", gamma = 0.01)\nmodel.fit(data_onesvm_1_1)\nMoose_education_Ricardo_ML[\"OneSVM Anomaly\"]=pd.Series(model.predict(data_onesvm_1_1))\n\nfig,ax = plt.subplots(figsize = (10,6))\na = Moose_education_Ricardo_ML.loc[Moose_education_Ricardo_ML[\"OneSVM Anomaly\"] == -1, [\"timestamp\",\"reading\"]]\nplt.title(\"Moose_education_Ricardo_Anomalies_OneClassSVM\") \nax.plot(Moose_education_Ricardo_ML[\"timestamp\"],Moose_education_Ricardo_ML[\"reading\"], color = \"blue\", label = \"Normal\")\nax.scatter(a[\"timestamp\"],a[\"reading\"],color = \"red\", label = \"Anomaly\")\nplt.legend()\nplt.show()","b232eb83":"# View the result \nMoose_education_Ricardo_ML_OneSVM = Moose_education_Ricardo_ML.loc[Moose_education_Ricardo_ML[\"OneSVM Anomaly\"] == -1] \nMoose_education_Ricardo_ML_OneSVM.info() ","adb7d623":"data_onesvm_2 = Moose_education_Abbie_ML[[\"reading\",\"Year\",\"Month\",\"Hour\",\"Timeframe\",\"IsWeekend\"]]\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(data_onesvm_2)\ndata_onesvm_2_1 = pd.DataFrame (np_scaled)\n\noutlier_fraction = 0.01\n\n#train oneclass SVM\nmodel = OneClassSVM(nu = outlier_fraction,kernel = \"rbf\", gamma = 0.01)\nmodel.fit(data_onesvm_2_1)\nMoose_education_Abbie_ML[\"OneSVM Anomaly\"]=pd.Series(model.predict(data_onesvm_2_1))\n\nfig,ax = plt.subplots(figsize = (10,6))\na = Moose_education_Abbie_ML.loc[Moose_education_Abbie_ML[\"OneSVM Anomaly\"] == -1, [\"timestamp\",\"reading\"]]\nplt.title(\"Moose_education_Abbie_Anomalies_OneClassSVM\") \nax.plot(Moose_education_Abbie_ML[\"timestamp\"],Moose_education_Abbie_ML[\"reading\"], color = \"blue\", label = \"Normal\")\nax.scatter(a[\"timestamp\"],a[\"reading\"],color = \"red\", label = \"Anomaly\")\nplt.legend()\nplt.show()","4119631f":"# View the result \nMoose_education_Abbie_ML_OneSVM = Moose_education_Abbie_ML.loc[Moose_education_Abbie_ML[\"OneSVM Anomaly\"] == -1] \nMoose_education_Abbie_ML_OneSVM.info() ","2cd8ca17":"data_onesvm_3 = Moose_education_Sasha_ML[[\"reading\",\"Year\",\"Month\",\"Hour\",\"Timeframe\",\"IsWeekend\"]]\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(data_onesvm_3)\ndata_onesvm_3_1 = pd.DataFrame (np_scaled)\n\noutlier_fraction = 0.01\n\n#train oneclass SVM\nmodel = OneClassSVM(nu = outlier_fraction,kernel = \"rbf\", gamma = 0.01)\nmodel.fit(data_onesvm_3_1)\nMoose_education_Sasha_ML[\"OneSVM Anomaly\"]=pd.Series(model.predict(data_onesvm_3_1))\n\nfig,ax = plt.subplots(figsize = (10,6))\na = Moose_education_Sasha_ML.loc[Moose_education_Sasha_ML[\"OneSVM Anomaly\"] == -1, [\"timestamp\",\"reading\"]]\nplt.title(\"Moose_education_Sasha_Anomalies_OneClassSVM\") \nax.plot(Moose_education_Sasha_ML[\"timestamp\"],Moose_education_Sasha_ML[\"reading\"], color = \"blue\", label = \"Normal\")\nax.scatter(a[\"timestamp\"],a[\"reading\"],color = \"red\", label = \"Anomaly\")\nplt.legend()\nplt.show()","b7a13aea":"# View the result \nMoose_education_Sasha_ML_OneSVM = Moose_education_Sasha_ML.loc[Moose_education_Sasha_ML[\"OneSVM Anomaly\"] == -1] \nMoose_education_Sasha_ML_OneSVM.info() ","66b7dfa4":"data1 = Moose_education_Ricardo_ML[[\"reading\",\"Year\",\"Month\",\"Hour\",\"Timeframe\",\"IsWeekend\"]]\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(data1)\ndata2 = pd.DataFrame (np_scaled)\n\noutlier_fraction = 0.01\n\n#train isolation forest\nmodel = IsolationForest (contamination = outlier_fraction)\nmodel.fit(data2)\nMoose_education_Ricardo_ML [\"anomaly_IsolationForest\"] = pd.Series (model.predict(data2))\n\n#visualization\nfig,ax = plt.subplots(figsize = (10,6))\na = Moose_education_Ricardo_ML.loc[Moose_education_Ricardo_ML[\"anomaly_IsolationForest\"] == -1, [\"timestamp\",\"reading\"]]\nplt.title(\"Moose_education_Ricardo_Anomalies_OneClassSVM\") \nax.plot(Moose_education_Ricardo_ML[\"timestamp\"],Moose_education_Ricardo_ML[\"reading\"], color = \"blue\", label = \"Normal\")\nax.scatter(a[\"timestamp\"],a[\"reading\"],color = \"red\", label = \"Anomaly\")\nplt.legend()\nplt.show()","a4067002":"# View the result \nMoose_education_Ricardo_ML_Iso = Moose_education_Ricardo_ML.loc[Moose_education_Ricardo_ML[\"anomaly_IsolationForest\"] == -1] \nMoose_education_Ricardo_ML_Iso.info() ","cf6dbac9":"data3 = Moose_education_Abbie_ML[[\"reading\",\"Year\",\"Month\",\"Hour\",\"Timeframe\",\"IsWeekend\"]]\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(data3)\ndata4 = pd.DataFrame (np_scaled)\n\noutlier_fraction = 0.01\n\n#train isolation forest\nmodel = IsolationForest (contamination = outlier_fraction)\nmodel.fit(data4)\nMoose_education_Abbie_ML [\"anomaly_IsolationForest\"] = pd.Series (model.predict(data4))\n\n#visualization\nfig,ax = plt.subplots(figsize = (10,6))\na = Moose_education_Abbie_ML.loc[Moose_education_Abbie_ML[\"anomaly_IsolationForest\"] == -1, [\"timestamp\",\"reading\"]]\nplt.title(\"Moose_education_Abbie_Anomalies_OneClassSVM\") \nax.plot(Moose_education_Abbie_ML[\"timestamp\"],Moose_education_Abbie_ML[\"reading\"], color = \"blue\", label = \"Normal\")\nax.scatter(a[\"timestamp\"],a[\"reading\"],color = \"red\", label = \"Anomaly\")\nplt.legend()\nplt.show()","83d0e9d8":"# View the result \nMoose_education_Abbie_ML_Iso = Moose_education_Abbie_ML.loc[Moose_education_Abbie_ML[\"anomaly_IsolationForest\"] == -1] \nMoose_education_Abbie_ML_Iso.info() ","251c855d":"data5 = Moose_education_Sasha_ML[[\"reading\",\"Year\",\"Month\",\"Hour\",\"Timeframe\",\"IsWeekend\"]]\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(data5)\ndata6 = pd.DataFrame (np_scaled)\n\noutlier_fraction = 0.01\n\n#train isolation forest\nmodel = IsolationForest (contamination = outlier_fraction)\nmodel.fit(data6)\nMoose_education_Sasha_ML [\"anomaly_IsolationForest\"] = pd.Series (model.predict(data6))\n\n#visualization\nfig,ax = plt.subplots(figsize = (10,6))\na = Moose_education_Sasha_ML.loc[Moose_education_Sasha_ML[\"anomaly_IsolationForest\"] == -1, [\"timestamp\",\"reading\"]]\nplt.title(\"Moose_education_Sasha_Anomalies_OneClassSVM\") \nax.plot(Moose_education_Sasha_ML[\"timestamp\"],Moose_education_Sasha_ML[\"reading\"], color = \"blue\", label = \"Normal\")\nax.scatter(a[\"timestamp\"],a[\"reading\"],color = \"red\", label = \"Anomaly\")\nplt.legend()\nplt.show()","cd12bdc9":"#View the result \nMoose_education_Sasha_ML_Iso = Moose_education_Sasha_ML.loc[Moose_education_Sasha_ML[\"anomaly_IsolationForest\"] == -1] \nMoose_education_Sasha_ML_Iso.info() ","e98205f4":"# Creating new dataframe\nimport re \nimport sys \n \nfrom pandas.plotting import register_matplotlib_converters \nregister_matplotlib_converters() \n\ndata_e_A = df_electricity.loc[:, ['timestamp','Moose_education_Abbie']].set_index('timestamp') \ndata_e_R = df_electricity.loc[:, ['timestamp','Moose_education_Ricardo']].set_index('timestamp') \ndata_e_S = df_electricity.loc[:, ['timestamp','Moose_education_Sasha']].head(8784).set_index('timestamp')","acdc92aa":"# Additional processing from previous outlier detection techniques for comparison analysis \n\n# IQR outlier for Moose_education_Sasha  \ni3_anomaly = df_buildingIQR  \ni3_anomaly['Moose_education_Sasha_IQR_anomaly'] = i3_anomaly['Moose_education_Sasha']  \ni3_anomaly['Moose_education_Sasha_IQR_anomaly']= np.where((df_buildingIQR['building3'] > upper_limit3) ,0,i3_anomaly['Moose_education_Sasha_IQR_anomaly'])  \ni3_anomaly = i3_anomaly.drop(columns=[\"Moose_education_Ricardo\",\"Moose_education_Abbie\",\"building1\",\"building2\",\"building3\"] )  \nMoose_education_Sasha_IQR = i3_anomaly  \n \n# Zscore outlier for Moose_education_Abbie  \nz2_anomaly = df_buildingzscore  \nz2_anomaly['Moose_education_Abbie_anomaly'] = z2_anomaly['Moose_education_Abbie']  \nz2_anomaly['Moose_education_Abbie_anomaly']= np.where((df_buildingzscore['Moose_education_Abbie'] >Highest2)+ (df_buildingzscore['Moose_education_Abbie'] < Lowest2) ,0,z2_anomaly['Moose_education_Abbie_anomaly'])  \nz2_anomaly = z2_anomaly.drop(columns=[\"Moose_education_Ricardo\",\"Moose_education_Sasha\",\"building1\",\"building2\",\"building3\"] )   \nMoose_education_Abbie_zscore = z2_anomaly  \n \n# Zscore outlier for Moose_education_Sasha  \nz3_anomaly = df_buildingzscore   \nz3_anomaly['Moose_education_Sasha_anomaly'] = z3_anomaly['Moose_education_Sasha']  \nz3_anomaly['Moose_education_Sasha_anomaly']= np.where((df_buildingzscore['Moose_education_Sasha'] > Highest3) ,0,z3_anomaly['Moose_education_Sasha_anomaly'])  \nz3_anomaly_ = z3_anomaly.drop(columns=[\"Moose_education_Ricardo\",\"Moose_education_Abbie\",\"building1\",\"building2\",\"building3\"] )  \nMoose_education_Sasha_zscore = z3_anomaly ","4080e675":"df_zscore = Moose_education_Sasha_zscore.loc[:,['Moose_education_Sasha','Moose_education_Sasha_anomaly']].head(8784) \ndf_IQR = Moose_education_Sasha_IQR.head(8784) \ndf_Abbie = Moose_education_Abbie_zscore   \n\n# Format processing \ndef dfTonp(data): \n    return np.array(data).flatten() \n \ndef listTofloat(l): \n    list = [] \n    for i in l: \n        item = float(i) \n        list.append(item) \n    return list  \n\n# Define function to locate the highlight points \ndef creathighlight(data): \n    list_scat = [] \n    for i in range(0, len(data)): \n        sasha = data.iloc[i, 0] \n        sasha_anomaly = data.iloc[i, 1] \n        if sasha == sasha_anomaly: \n            list_scat.append(0) \n        else: \n            list_scat.append(sasha) \n    return list_scat \n \n\n# Define function to draw  \ndef draw(data1,data2, time): \n    plt.figure(figsize=(36, 24)) \n    index = data1.index \n    y_plot = dfTonp(data1.iloc[:, 0]) \n    plt.plot(index, y_plot, label='normal', c='#7a7e83', zorder=1) \n    if not data1 is None: \n        y_scat1 = creathighlight(data1) \n        plt.scatter(index, y_scat1, label='zscore', c='#ff6633', zorder=2, s=64) \n    if not data2 is None: \n        y_scat2 = creathighlight(data2) \n        plt.scatter(index, y_scat2, label='IQR', c='b', zorder=3, s=16) \n\n    plt.xticks(fontsize=20) \n    plt.yticks(fontsize=20) \n    plt.legend(fontsize=20) \n    plt.grid(True) \n\n    if time is not None and data2 is not None: \n        plt.title('sasha'+time,fontsize=20) \n    elif time is None and data2 is not None: \n        plt.title('all_sasha',fontsize=20) \n    elif time is None and data2 is None: \n        plt.title('all_abbie',fontsize=20) \n    else: \n        plt.title('abbie'+time,fontsize=20) \n    plt.show() \n    plt.close() \n\n# split the year into months for Sasha \nY_M = ['2016-1', '2016-2', '2016-3', '2016-4', '2016-5', '2016-6', '2016-7', '2016-8', '2016-9', '2016-10', '2016-11', \n       '2016-12'] \n\n# split the year into months \nY_M_plus = ['2016-1', '2016-2', '2016-3', '2016-4', '2016-5', '2016-6', '2016-7', '2016-8', '2016-9', '2016-10', '2016-11', \n'2016-12', \n'2017-1', '2017-2', '2017-3', '2017-4', '2017-5', '2017-6', '2017-7', '2017-8', '2017-9', '2017-10', '2017-11', \n'2017-12'] \n\n\n# Define function to comeout the comparision diagram \ndef run(name): \n    if name == 'all': \n        draw(df_zscore,df_IQR,None)                     # for Sasha \n        draw(df_Abbie,None,None)                        # for Abbie \n    elif name == 'abbie': \n        for i in Y_M_plus: \n            draw(df_Abbie[i],None,i) \n    else: \n        for i in Y_M: \n            draw(df_zscore[i],df_IQR[i],i) \n\nall = 'all' \nrun(all)      \nrun('abbie') \nrun('sasha') ","1f164820":"df_A_ONESVM = Moose_education_Abbie_ML_OneSVM.loc[:, ['timestamp','reading']].set_index('timestamp') \ndf_A_ONESVM.index = pd.to_datetime(df_A_ONESVM.index, format='%Y-%m-%d %H:%M:%S') \ndf_A_Iso = Moose_education_Abbie_ML_Iso.loc[:, ['timestamp','reading']].set_index('timestamp') \n\ndf_R_ONESVM = Moose_education_Ricardo_ML_OneSVM.loc[:, ['reading']] \ndf_R_ONESVM.index = pd.to_datetime(df_R_ONESVM.index, format='%Y-%m-%d %H:%M:%S') \ndf_R_Iso =Moose_education_Ricardo_ML_Iso.loc[:, ['reading']] \ndf_R_Iso.index = pd.to_datetime(df_R_Iso.index, format='%Y-%m-%d %H:%M:%S') \n \ndf_S_ONESVM = Moose_education_Sasha_ML_OneSVM.loc[:, ['reading']] \ndf_S_ONESVM.index = pd.to_datetime(df_S_ONESVM.index, format='%Y-%m-%d %H:%M:%S') \ndf_S_Iso = Moose_education_Sasha_ML_Iso.loc[:, ['reading']] \ndf_S_Iso.index = pd.to_datetime(df_S_Iso.index, format='%Y-%m-%d %H:%M:%S')  \n\ndef scatOneSVM(data, data_method): \n    a = data.index \n    list = [] \n    b = data_method.index \n    for i in a: \n        if i in b: \n            list.append(data_method.loc[i, 'reading']) \n        elif i not in data_method.index: \n            list.append(0) \n        else: \n            print('there is an error in list filling\\n') \n    return list \n\n# Define function to draw monthly  \ndef draw1(city, data, df_OneSVM, df_Iso, time): \n    ax = plt.figure(figsize=(36, 24)) \n    x_index = data.index \n    y_OneSVM = scatOneSVM(data, data_method=df_OneSVM) \n    y_OneSVM = listTofloat(y_OneSVM) \n    y_Iso = scatOneSVM(data, data_method=df_Iso) \n    y_Iso = listTofloat(y_Iso) \n    y_Sum = dfTonp(data) \n    \n    plt.plot(x_index, y_Sum, label='sum', c='#BDBAA7', zorder=1) \n    plt.scatter(x_index, y_OneSVM, label='OneSVM', c='#F9294F', zorder=2, s=256.0) \n    plt.scatter(x_index, y_Iso, label='Iso', c='#294FF9', zorder=3, s=256.0) \n    \n    plt.xticks(fontsize=30) \n    plt.yticks(fontsize=50) \n    plt.legend(fontsize=50) \n    plt.show() \n    plt.close() \n\n\n# Define function to draw yearly \ndef draw1all(city,data,df_OneSVM,df_Iso): \n    ax = plt.figure(figsize=(36, 24)) \n    x_index = data.index \n    y_OneSVM = scatOneSVM(data, data_method=df_OneSVM) \n    y_OneSVM = listTofloat(y_OneSVM) \n    y_Iso = scatOneSVM(data, data_method=df_Iso) \n    y_Iso = listTofloat(y_Iso) \n    y_Sum = dfTonp(data) \n    \n    plt.plot(x_index, y_Sum, label='sum', c='#BDBAA7', zorder=1) \n    plt.scatter(x_index, y_OneSVM, label='OneSVM', c='#F9294F', zorder=2, s=256.0) \n    plt.scatter(x_index, y_Iso, label='Iso', c='#294FF9', zorder=3, s=256.0) \n    \n    plt.xticks(fontsize=30) \n    plt.yticks(fontsize=50) \n    plt.legend(fontsize=50) \n    plt.show() \n    plt.close() \n\n# Define function to comeout the comparision diagram \ndef run1(city, time=Y_M_plus): \n    if city == 'Abbie': \n        for i in time: \n            if i in data_e_A.index and i in df_A_ONESVM.index and i in df_A_Iso.index: \n                draw1(city, data_e_A[i], df_OneSVM=df_A_ONESVM[i], df_Iso=df_A_Iso[i], time=i) \n    elif city == 'Ricardo': \n        for i in time: \n            if i in data_e_R.index and i in df_R_ONESVM.index and i in df_R_Iso.index: \n                draw1(city, data_e_R[i], df_OneSVM=df_R_ONESVM[i], df_Iso=df_R_Iso[i], time=i) \n    elif city == 'Sasha': \n        for i in time: \n            if i in data_e_S.index and i in df_S_ONESVM.index and i in df_S_Iso.index: \n                draw1(city, data_e_S[i], df_OneSVM=df_S_ONESVM[i], df_Iso=df_S_Iso[i], time=i) \n    elif city == 'all': \n        draw1all('Abbie',data_e_A,df_A_ONESVM,df_A_Iso) \n        draw1all('Ricardo',data_e_R,df_R_ONESVM,df_R_Iso) \n        draw1all('Sasha',data_e_S,df_S_ONESVM,df_S_Iso) \n    else: \n        print('there is an error in run1\\n') \n\nrun1('Abbie') \nrun1('Ricardo') \nrun1('Sasha') \nrun1('all') ","8c0d2f35":"Begin the data processing by loading the targeted data set to kaggle. The data sets selected for the analysis are electricity meter data from 3 buildings that were chosen from the result of Data Completeness Test in section 2: \n\n- Moose_education_Ricardo (Perfect) \n\n- Moose_education_Abbie (Good) \n\n- Moose_education_Sasha (Poor) ","90dc64b9":"The following diagrams will display the outliers identified for respective building using One Class SVM","28be3f87":"#### Monthly Electricity Consumption","fe82b682":"### One Class SVM - Moose_education_ Sasha","3a2056a6":"The table above shows the 5 buildings under the poor category in terms of data completeness. \n\nTo demonstrate the abnormal analysis, we will choose one building from each category. These 3 buildings will be within the same site to control the weather differences: \n\n* Moose_education_Ricardo (Perfect)\n* Moose_education_Abbie (Good)\n* Moose_education_Sasha (Poor)","f87fd355":"### Isolation Forest - Moose_education_Sasha ","526ec057":"## 3.1  Interquartile Range (IQR) data outlier detection and removal \n\nIQR is also known as standard deviation and boxplot method which was developed by Tukey [4], it is one of the simpler ways to detect data outlier. This method is reasonable when the data distribution is symmetric and mount-shaped such as the normal distribution. This helps to estimate the likelihood of having extreme values in the data. The lower quartile (q1) is the 25th percentile, and the upper quartile (q3) is the 75th percentile of the data. The inter-quartile range (IQR) is defined as the interval between q1 and q3. The data above q1-(1.5iqr) and below q3+(1.5iqr) are defined as \u201cinner fences\u201d, data more below q1-(1.5iqr) and above q3+(1.5iqr) are \u201couter fences\u201d, the observations between an inner fence and its nearby outer fence considered as \u201coutside\u201d, and anything beyond outer fences are \u201cfar out\u201d, which will be identified as the outliers. This method is quite effective, especially when working with large continuous data sets that are not highly skewed.[5] \n\n![image.png](attachment:ad4e8d47-a697-4411-a822-ac11bf068ef9.png)    \nFigure showing IQR [5]","28c1fb36":"## 2.2  Create Missing Data Analysis Table\nWith the missing data count derived and arranged for the 78 buildings, we then create a function to categorize the buildings into different categories depending on their data availability. ","41eafed9":"# Introduction:  \n***\n\n  \n\nThe higher education landscape is undergoing digital shift with many schools adopting Internet of Things (IoT) to create smart campus. [1][2][3]   \n\n  \n\nThe IoT system produces large amounts of raw data in the form of timeseries logs that have great potential to be further analyzed to realize hyper efficiency building operation management and predictive maintenance.  \n\n  \n\nData cleaning and anomalies detection are crucial to make raw data generated from IoT useful. However, such processes are usually time consuming and abnormal data could easily be overlooked. Therefore, we need to explore efficient ways to detect these outliers. \n\n \n\n**The Scenario** \n\nIn this project, we will act as an analytic consultant in the building industry with the main market focusing on the US Eastern region. We will be exploring the data from the education institute, a rising segment of the market due to the uptake of IoT infrastructures. We wish to explore the best statistical methods to identify abnormal observations under different data completeness situations. We will be testing out using 4 anomaly detection techniques of different complexity:  \n\n- _simple univariate statistics: **standard deviation z score** and **interquartile range**_\n\n- _automatic outlier detection: **isolation forest** and **one-class SVM**_","014a8a6e":"Z-score and IQR are elementary anomaly detection methods compared to Isolation Forest and One Class SVM. Thus, the analysis process is divided into two parts, one for Z-score and IQR, and the other for Isolation Forest and One Class SVM. ","2807f8ce":"Z-score method managed to identify 5 outliers and 124 outliers from Moose_education_Abbie and Moose_education_Sasha respectively, however there was no outlier detected for Moose_education_Ricardo.  \n\nBelow codes will generate comparison distribution plots and box plots for 3 building data. ","ff29047b":"IQR method only managed to remove 146 outliers from Moose_education_Sasha building, which has the data set rated as \u201cpoor\u201d in section 2 data processing. \n\nThe following codes will let us plot two types of comparison graphs - distribution plots and box plots to show the original data and after outlier removal data using IQR method. The plots are customized to 3 different colors for 3 different buildings better identification: \u201dMoose_education_Ricardo\u201d are plotted in green; \u201dMoose_education_Abbie\u201d are plotted in magenta and \u201dMoose_education_Sasha\u201d are plotted in pink. ","35aa70bb":"### **5.1.2 Outlier Comparison Plot**\n\nSince only Sasha has anomalies detected for both method, its anomalies depicted in plot form is selected:\n\n![image.png](attachment:b2cd9f97-0777-42bc-8989-9bbb4106a12c.png)\nSasha 2016 \n\nBoth IQR and Z scores will do the classification in one single stage to the whole data set without further delving into details. For example, in our case, IQR define the outliers as data exceeded the upper_limit range, which is percentile 75 + 1.5 * iqr, while for Z-scores, if data point exceeds 3 in absolute value, they will be considered as outliers. There are no further subsequent anomalies detection to identify unusual patterns within the bigger chunk in between. Therefore, even though there may be presences of anomalies in the main area (marked in green box), both methods will ignore them. ","8c47d360":"### One Class SVM - Moose_education_ Abbie ","c3d07c95":"Display list of outliers for Moose_education_Sasha","2d044000":"# Section 4: Machine Learning Outliers Detection\n<a id=\"4\"><\/a>\n***","924f2ec1":"The following codes will identify and remove z-score outliers:","a96b9321":"Likewise the plots are only showing difference for Moose_education_Sasha building, which all the outliers > value 6 are removed and the data is distributed from 0 \u2013 6 and box plot  showing majority of data concentrated at approx 0.8 to 3.1.  \n\nThe following codes will produce time-series data comparison lineplots for the 3 selected building electricity data, before or after outlier removal using IQR method. ","34fb3ad8":"## 1.1 Meta  Data File \n\nThe metadata frame gives an overview of the data availability for respective buildings. ","48ed64e8":"# Section 6: Conclusion\n<a id=\"6\"><\/a>\n***","639cff19":"We will further examine the list of outliers removed using IQR method for Moose_education_Sasha, which is the only building with data removed by IQR method. ","c5d8a25b":"Below are the charts which overlays the results of the 2 machine learning techniques: Isolation Forest and One Class SVM","36618786":"Codes for identification of 25 percentile, 75 percentile of loaded electricity data, then obtain the IQR which is between 25 to 75 percentile. The outliers are defined as data exceeded the \u201cupper_limit\u201d range, which is percentile 75 + 1.5 * iqr. The 3 buildings\u2019 IQR outliers are identified and removed altogether in the following section.  ","eb15dd4d":"# Section 0: Notebook Setting\n<a id=\"0\"><\/a>\n***","5be7eca0":"The following codes will display number of outliers removed by IQR method for each building ","9ac2e35f":"## 5.1 Z-score and IQR analysis\n\n### **5.1.1 Outlier Comparison Table**\nFor the perfect data, Ricardo, both methods have no anomaly. But for Abbie, Z-core does detect few anomalies while IQR still thinks there is no anomaly. For Sasha, they both agree there are some abnormal values and IQR thinks the situation is worse. \n\n![image.png](attachment:e41aebf3-094a-4adb-b1ef-0edd00f88d1c.png)\n\nRicardo which has the \u2018perfect\u2019 grading in data completeness have no anomalies detected in both methods.. But for Abbie, Z-core does detect few anomalies while IQR did not pick up any. For Sasha, both methods detected anomalies and IQR managed to capture more.","3cb78ce6":"\nAs stated in the assumption, we hope to understand the number of absolute \u201c0\u201d in the dataset and do a quick check on the three selected buildings. \n\nFrom the result, there is no \u201c0\u201d value presence in the dataset, therefore, it is good to proceed with outlier analysis. ","f0c5adae":" From the chart, the 78 buildings can be split into 3 categories: Perfect, Good and Poor. ","03d0ef6e":"In conclusion, with the increasing demand for data driven building operation and high challenge of detecting anomalies, the problem of finding the right anomalies detection method is faced by more building owners in sectors such as education institutes. It is essential to understand whether the anomaly detection technique chosen fits the data model and is scalable to accommodate ever-growing data size. The anomalies detection method can vary from data to data with the goal of letting the dataset undergo proper data engineering to achieve good data quality that is fit for future analysis. \n\nThis notebook provides an overview of the different techniques to detect anomalies in data of various completeness. We find that the automatic outlier detection tools such as Isolation Forest and One Class SVM tend to identify more anomalies than the simple univariate statistics such as IQR and Z-score. However, there are a few caveats that need to be highlighted. \n\nFirst, a higher number of anomalies detected does not mean a better detection method because we are uncertain if all of them are true anomalies. This can be seen in our case where some are considered as anomalies by one method but not by the other. \n\nSecondly, different methods work better under different conditions, such as how dense the data is and the complexity of the process. It is therefore advisable to apply more than one and identify the overlapped points across the methods to increase accuracy. \n\nThirdly, due to limited data available in Eastern region of the US on Education buildings, we only managed to compare 3 buildings in the same site under different categories of data completeness. Thus, more trials will be needed in the future in other areas with more buildings with different levels of missing data. ","3ba741f8":"## 4.1 One Class SVM \n\nOne Class SVM is very sensitive to outliers especially for high dimensional distributed data. It is a special case of Support Vector Machine (SVM) and is considered as unsupervised learning. The estimator is suitable for novelty detection when the training set is not contaminated by the outliers [9]. One Class SVM does not require training set but creates a hypersphere that contains the center and the radius. The center of the hypersphere is a result of the linear combination of support vectors where the minimization formula is indicated in the picture below [10]. The outliers are determined if the examined point is falling outside of the hypersphere.\n\n![image.png](attachment:4b938d09-bae6-4c38-bf2a-869ded66ae54.png)!\n\nWhen setting up the One Class SVM model, the nu = outliers_fraction indicates the upper bound on the fraction of training errors as well as the lower bound of the fraction of support vectors. This value falls between 0 and 1. In short, this value describes the proportion of outliers in the studied data. By adjusting this value to a higher value close to 1, the scenario of overfitting may occur. Hence, it is subject to the user to define the proportion of outliers. In this study, 0.01 is used. \n\n- **\u201crbf\u201d kernel** is specified to enable SVM to apply a non-linear function for the plotting of hyperspace to higher dimension.  \n\n- **gamma** refers to a parameter in of the \u201crbf\u201d kernel which controls the impact of individual training samples and smooth the model.  \n\n- **predict(data)** implement classification on the dataset, the return of this is +1 or \u20131, which \u20131 is anomaly and +1 is normal data. ","2c4de1cc":"# Section 5: Outlier Results Analysis\n<a id=\"5\"><\/a>\n***","ae1b6123":"The following diagrams will display the outliers identified for respective building using Isolation Forest","73b0fdd2":"In the following, we first show the number of outliers detected by respective method in table form in each month before visualizing using the chart. \n\nAs mentioned, we will divide into 2 parts, first on Z-score and IQR, the elementary anomaly detection, then the more complex Isolation Forest and One Class SVM ","65819304":"## 5.2 Isolation Forest and One Class SVM\n\n","0c6b2ab8":"The 78 education buildings are mainly college classrooms and laboratory, and the site of Cockatoo has most of them as indicated in the list below: ","6330c1b3":"The following figures provides general timeseries visualisation on their electricity consumption in 2016 and 2017. ","296d06a8":"# Section 7: References\n<a id=\"7\"><\/a>\n***\n[1]Jadoul, M. (2021) How Industry 4.0 is transforming higher education. Retrieved from: https:\/\/edtechnology.co.uk\/comments\/how-industry-4-0-transforming-higher-education\/ \n\n[2] NUS (2021) NUS and Johnson Controls embark on S$5 million research to co-create people-centric smart building systems. Retrieved from:  https:\/\/news.nus.edu.sg\/nus-and-johnson-controls-embark-on-s5-million-research-to-co-create-people-centric-smart-building-systems\/ \n\n[3] Birmingham University Press (2021) University Birmingham partners Siemens create smartest university campus in the world. Retrieved from: https:\/\/press.siemens.com\/global\/en\/pressrelease\/university-birmingham-partners-siemens-create-smartest-university-campus-world \n\n[4] Seo, S. . (2006). A Review and Comparison of Methods for Detecting Outliers in Univariate Data Sets. \n\n[5] Oracle\u00ae Cloud. (2014, 2021).Working with Planning. Retrieved from: https:\/\/docs.oracle.com\/en\/cloud\/saas\/planning-budgeting-cloud\/pfusu\/insights_metrics_IQR.html \n\n[6] Ronald E., S. (2012). Maximum Z Score and Outliers. In The American Statistician (pp. 79-80). doi:http:\/\/dx.doi.org\/10.1080\/00031305.1988.104755 \n\n[7] Oracle\u00ae Cloud. (2014, 2021).Working with Planning. Retrieved from: https:\/\/docs.oracle.com\/en\/cloud\/saas\/planning-budgeting-cloud\/pfusu\/insights_metrics_Z-Score.html \n\n[8] Nawaz, M., & Mahejabeen, P.-M. (2018). Building An Anomaly Detection Engine (ADE) For IoT Smart Applications. Procedia Computer Science, 10-17. doi:https:\/\/doi.org\/10.1016\/j.procs.2018.07.138. \n\n[9] Comparing anomaly detection algorithms for outlier detection on toy datasets. (n.d.). Retrieved from scikit-learn: https:\/\/scikit-learn.org\/stable\/auto_examples\/miscellaneous\/plot_anomaly_comparison.html?highlight=outlier%20detection \n\n[10] Yugesh, V. (2021). How to use Support Vector Machines for One-Class Classification? Retrieved from: https:\/\/analyticsindiamag.com\/how-to-use-support-vector-machines-for-one-class-classification\/ \n\n[11] Fei Tony, L., Kai Ming, T., & Zhi-Hua, Z. (2012). Isolation Forest. Retrieved from https:\/\/cs.nju.edu.cn\/zhouzh\/zhouzh.files\/publication\/icdm08b.pdf \n\n[12] Xiaoli Li, Quanbo Liu, Kang Wang, Fuqiang Wang, Guimei Cui, Yang Li, \"Multimodel Anomaly Identification and Control in Wet Limestone-Gypsum Flue Gas Desulphurization System\", Complexity, vol. 2020, Article ID 6046729, 17 pages, 2020. https:\/\/doi.org\/10.1155\/2020\/6046729\n\n[13] Susan, L. (2019, January 24). Time Series of Price Anomaly Detection. Retrieved from towardsdatascience: https:\/\/medium.com\/m\/global-identity?redirectUrl=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-of-price-anomaly-detection-13586cd5ff46 \n\n\n\n","61691225":"# Group 5 Anomalies Detection Project\n### Group Members: Chen Xiayim, Liu Xianzhou, Tsen Siong Kee, Wang Huayue, Zhang Yikai\n### **Task:** _What are good ways to find anomalous behaviour in building meter data?_\nhttps:\/\/www.kaggle.com\/claytonmiller\/buildingdatagenomeproject2\/tasks?taskId=2544","972f32a5":"## 3.2 Z-Score data outlier detection and removal \nAnother method that can be used to screen data for outliers is the Z-Score, using the mean and standard deviation (sd) of data. The basic idea of this rule is: if X follows a normal distribution, then Z follows a standard normal distribution, N (0, 1), and Z-scores that exceed 3 in absolute value are generally considered as outliers. This method is simple and it is the same formula as the 3 times standard deviation method when the criterion of an outlier is an absolute value of a Z-score of at least 3. It presents a reasonable criterion for identification of the outlier when data follow the normal distribution. A possible maximum Z-score is dependent on sample size, and it is computed as (n \u22121)\/ n. [6] \n\n\n![image.png](attachment:49c50e2a-b908-4326-8e9b-4422ac034a64.png)\n\nData distribution range derived by Z-Score [7] ","25038485":"Sasha which belongs to \u201cPoor\u201d data completeness category, this means that there are few data clustered at certain points while the rest will cluster at area with higher density. As illustrated in the plot below, we could find more anomalies point in red identified by One Class SVM which appears in area with denser amount of data. On the other hand, the blue anomalies points are found in the protruding region where there are lesser data points around. This could be explained by how One Class SVM and Isolation Forest works. \n\n \n![image.png](attachment:42a7f629-d6c8-4033-832d-48305fa499e0.png) Sasha 2016\n \n\nIn On Class SVM is a max-margin method to find a hypersphere to encompass the instances and minimize the hypersphere, i.e., maximize the margin. Ample data is needed to form the hypersphere and go further down to identify the outer and inner. Thus, it is hard for the algorithm to go into work in the sparse protruding part while the dense lower part has provided a great working environment for the algorithm. \n\nOne the other hand, Isolation Forest defines the anomaly as \"more likely to be separated\" [12]. It can be understood as a point with sparse distribution and far away from high-density groups. Thus, those points in the sparse protruding area away from the dense lower part can be more easily defined by Isolation Forest.  ","9e6949f6":"## 1.2 Electricity Consumption Data File\nFrom the electricity consumption dataset, we further extract out relevant comsumption information of the 78 education buildings identified previously.","d7184188":"The graph below illustrates the missing count by \u201cbuilding_id\u201d with the bar color scheme defined by the \u201csite_id\u201d. ","09d6c9b8":"# Section 3 IQR & Z-score Outliers Detections \n<a id=\"3\"><\/a>\n***","9f93fcc0":"We will look at the results of outliers identified by using z-score method. ","24b0ab35":"## Data","d517fefd":"There are two main branches of machine learning study, supervised and unsupervised. The supervised learning is used when the pattern of anomaly is known while unsupervised learning applies to the situation where the detection is done by inference or featuring [8]. In the context of this study, the pattern of anomaly is unknown, hence the machine learning methodology selected are two prevalent methods namely, One Class SVM and Isolation Forest in unsupervised learning domain. \n\nWe will examine the outliers in the selected buildings\u2019 datasets using the respective modules from sklearn, namely `sklearn.svm.OneClassSVM`  and `sklearn.ensemble.IsolationForest`. The selected datasets are required to be preprocessed to enhance the quality of data, the standards preprocessing includes clean Null data, standardize format, organize raw data into extraction-friendly format and then enrich the datasets with essential features. Machine learning can only take in numerical values which the encoding step is required to translate the features to numeric or Boolean value. In this study, all the features are in either numeric or Boolean format, hence no special encoding process is presented. Furthermore, there is a standard preprocessing tool use `sklearn.preprocessing.StandardScaler` to remove the mean and scaling to unit variance using z score for all the machine learning analysis. The `outlier_fraction` used in the study is 0.01. ","9d2060ca":"#### Hourly Electricity Consumption","bf92e38c":"### **5.2.2 Outlier Comparison Plot**\n\nIn the charts below, anomalies detected by One Class SVM are highlighted in red and those by Isolation Forest are highlighted in blue. \n\nIn those plots, the data points are clustered in similar density and we find that the red points are concentrated in the upper part and the blue points are concentrated in the lower part \n\nThis implies that One Class SVM tends to detect on the anomalies in upper range while Isolation Forest tends to detect on the anomalies in lower range values. \n\n![image.png](attachment:8408faa1-6431-47b0-916d-fb708744d0fa.png)\nAbbie, 2016 & 2017 \n\n\n![image.png](attachment:5d6005e0-ca68-4eb3-95cd-01b566085595.png)\nRicard, 2016 & 2017 \n\n![image.png](attachment:31eebb24-b6fa-483c-a347-df162886be86.png)\nRicardo, 2016.12 \n\n![image.png](attachment:5c4611cd-5cf7-4adb-b11e-4842bc112f20.png)\nRicardo, 2017.1 \n\n![image.png](attachment:1a9a3f26-0c30-4cfe-8285-d8bd17fd77a6.png)\nRicardo, 2017.12 \n\n![image.png](attachment:fef203c3-f7ff-4fed-9285-3ef99f5fcd0d.png)\nAbbie, 2016.1 \n\n\n","1798d7d3":"### One Class SVM - Moose_education_Ricardo ","011d0d6a":"\nThis left us with buildings within 4 sites located in New York and New Jersey at the Eastern Time Zone of the USA as shown in the map below ","147cda7c":"## 2.1  Understand Null Data Count by building_id\n\nTo visualize the missing information, we create a new dataframe,missing_count_visual, which has building id, missing count, site id, sub primary space usage, LEED level information in addition to the electricity consumption.","ce223b36":"From the barplot with site_id as \"hue\" setting, we can conclude that \n1. Buildings in Cockatoo have the most missing data with many appear in the top rank of the missing count\n1. Top 4 buildings with highest missing values count are all in Peacock\n1. The education buildings in Crow and Moose have fewer missing values as compared to Peacock and Cockatoo \n","0ff2a94b":"The following codes will produce time-series data comparison line plots for the 3 selected building electricity data, before and after outlier removal using Z-score method. ","d10e8fbc":"As a result of data preprocessing, the electricity datasets used in this study will consist of \"reading\", \"Year\", \"Month\", \"Hour\", \"Timeframe\", \"IsWeekend\" columns. The \u201creadings\u201d column is the dependent variables whereas \u201cYear\", \"Month\", \"Hour\", \"Timeframe\", \"IsWeekend\" are the timestamp related features to the \u201creadings\u201d. ","dd99b02b":"# Section 2: Data Completeness Test \n<a id=\"2\"><\/a>\nHigh quality dataset is essential for energy data analytics. The raw data from energy meters may contain missing values. From the electricity consumption time series plot in Section 1, there are many missing values shown in the raw datasets. Therefore, we hope to dive into the missing data statistics in the selected datasets and categorize the studied buildings into levels of data completeness. This will enable us to  a have quick insights into the data completeness level over our portfolio. \n\n> **Assumption made:** The studied buildings are running 24\/7 with normal operation in the morning and essential services at night. Energy meters should not have absolute 0 reading and missing data should be highlighted as one of the performance indicators for energy meters. ","ac5484d7":"We will define a new dataframe for Z-score outlier detection - df_buildingzscore. ","ef61678a":"### **5.2.1 Outlier Comparison Table**\n\nCompared to the number of anomalies detected by elementary anomaly detection, both Isolation Forest (Iso) and One Class SVM generally can detect more in each month and accumulatively across whole year for all 3 data completeness categories. \n\n ![image.png](attachment:2f23de33-92d7-4969-b75c-32a9f930ceaf.png)\n\nComparing both machine learning methods: \n\n- For the \u201cPoor\u201d data category, One Class SVM detected a greater number of outliers accumulatively in total in 2016 compared to Isolation Forest (Iso).  \n\n- For the \u201cGood\u201d data category, One Class SVM detected a greater number of outliers accumulatively in total in 2016 compared to Isolation Forest but lesser in 2017. \n\n- For the \u201cPerfect\u201d data category, One Class SVM detected a lesser number of outliers accumulatively in total in 2016 compared to Isolation Forest but greater in 2017. \n\n \n\nHence, it is difficult to decide which is a better method in outlier detection if we consider the yearly accumulative number. \n\nIn addition, yearly accumulated number of anomalies may not reflect the full story. Take the Ricardo in 2016 as an example. One -Class SVM identifies more anomalies in more months than Isolation Forest in 2016., but the accumulated number in terms of the entire year is lesser. The reason is due to a higher number of outliers detected by Isolation Forest in the month of Dec. \n\nTherefore, it is difficult to define a method a superior by the number of anomalies detected . ","b63137d7":"The following code is to overlay the results from the 4 statistical methods to plot out various charts at different time length (yearly and monthly) for all 3 buildings. Thereafter, we will identify a few charts for further analysis. \n\nFirst, we overlay the results for Z-score with IQR","c97c213a":"# Section 1 : Data Overview\n<a id=\"1\"><\/a>\n***\nThis section is to provide an overview of the data used and how we filter out the target information. ","bbb6a221":"The following are dataframes for the 3 respective building to carry out machine learning outliers detection","c188f9df":"## Library","1668e585":"## 4.2 Isolation Forest \n\nIsolation Forest algorithm is built based on decision trees which is an unsupervised model that performs well in multi-dimensional data [11]. The Isolation Forest algorithm refers to the process of isolating the observations via a randomly selected feature and then splitting the datasets between the extreme values of the selected feature. A tree structure is used to represent the recursive partitioning where the number of splitting to the sample is equivalent to the path length from the root node to the terminating node [9]. The pathway, which is normalized over a forest of random trees depicts the measure of normality and the decision function. Hence, random partitioning will result in significant shorter paths for anomalies.  \n\nUsing the diagram below to illustrate the idea of Isolation Forest when handling two-dimensional dataset. The Isolation Forest algorithm recreckonon that the characteristics of the outliers are \u201cfew and different\u201d [11]. The steps of Isolation are as follows, \n\n1. When a dataset is given, a random sub-sample of the data is chosen and assigned to a binary tree.  \n\n1. Tree branches are developed by selecting a random feature and then a random threshold. If the datapoint value is less than the defined threshold, it goes to the left and else goes to the right. \n\n1. Step 2 will repeat until each datapoint is completely isolated. \n\n![image.png](attachment:6bf179b7-5854-4335-9f25-a827f56b1170.png)\n\nWhen we implementing the Isolation Forest, the `contamination = outliers_fraction` is set to illustrate the proportion of outliers in the selected dataset. In this study, 0.01 is used. The `fit` and `predict (data)` performs the main outlier detection on the dataset and return +1 for normal data and \u20131 for anomaly. ","616bd300":"Next, we add adding Timeframe Classification for dataset\n* 00:00\u201306:00 = latenight = 1\n* 07:00\u201310:00 = rise time = 2\n* 11:00\u201317:00 = daytime = 3\n* 18:00\u201323:00 = evening = 4","4ea8d005":"### Isolation Forest - Moose_education_Ricardo ","aadec8e4":"# Table of contents\n***\n- [Section 0: Notebook Setting](#0)\n\n- [Section 1: Data Overview](#1)\n\n- [Section 2: Data Completeness Test ](#2)\n\n- [Section 3: IQR & Z-score Outliers Detections ](#3)\n\n- [Section 4: Machine Learning Outliers Detection](#4)\n\n- [Section 5: Outlier Results Analysis](#5)\n\n- [Section 6: Conclusion](#6)\n\n- [Section 7: References](#7)\n","282a739e":"Display list of outliers for Moose_education_Abbie ","98d433dd":"### Isolation Forest - Moose_education_Abbie "}}