{"cell_type":{"73b6b630":"code","13d0ba79":"code","dde88d96":"code","f41c40a0":"code","d61ee06c":"code","7dbcf116":"code","c2856b28":"code","31c54942":"code","8d69c8cd":"code","d422a546":"code","bb22c9f4":"code","ea360937":"code","49766482":"code","d5bb081b":"code","0c509344":"code","220c6793":"code","83cf169f":"code","cfcaf393":"code","d15869f2":"code","e70ecead":"code","06bd4b80":"code","72a4a8b9":"code","45e86bf9":"code","bd711782":"code","19ae2e82":"code","bc71ca99":"code","c1f075a3":"code","a092cf68":"code","0856c97e":"code","b16c83cb":"code","d55ae0c9":"code","349dec41":"code","b5899279":"code","56a3c70f":"code","220e5f78":"code","98aef161":"code","c2374309":"code","5daa24e5":"code","6fcb42a4":"code","b6d2e6ae":"code","13d06a91":"markdown","717137b5":"markdown","35ee8ca8":"markdown","5e0aaeb1":"markdown","78fc9a2f":"markdown","c1d226de":"markdown","0711fc37":"markdown","8b9b632c":"markdown","172dcb0e":"markdown","dbc69a34":"markdown","bd22522c":"markdown","e7bee666":"markdown"},"source":{"73b6b630":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, normalize, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import StratifiedKFold\n\npd.options.display.max_rows = None\nplt.style.use('dark_background')","13d0ba79":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dde88d96":"# Loading train & test files\ntrainDf = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntestDf = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","f41c40a0":"trainDf = trainDf.set_index('id')\nprint(trainDf.duplicated().sum())\ntrainDf.drop_duplicates(inplace=True)","d61ee06c":"sns.countplot(y=trainDf.target)","7dbcf116":"trainDf.isnull().sum()","c2856b28":"vctrain =  trainDf.keyword.value_counts()\nplt.figure(figsize=(16,9))\nsns.countplot(y=trainDf.keyword, order=vctrain.iloc[:10].index)","31c54942":"plt.figure(figsize=(16,9))\nsns.countplot(y=trainDf.keyword, order=vctrain.iloc[:10].index)\nplt.subplot(121)\nsns.countplot(y=trainDf.keyword[trainDf.target == 0], order= trainDf.keyword[trainDf.target == 0].value_counts().iloc[:10].index, color='r')\nplt.title('Top Keyword for non-Disaster')\nplt.subplot(122)\nsns.countplot(y=trainDf.keyword[trainDf.target == 1], order= trainDf.keyword[trainDf.target == 1].value_counts().iloc[:10].index, color='g')\nplt.title('Top Keyword for Disaster')","8d69c8cd":"vcltrain =  trainDf.location.value_counts()\nplt.figure(figsize=(16,9))\nsns.countplot(y=trainDf.location, order=vcltrain.iloc[:10].index)","d422a546":"def tweet_cleaning(text):\n    stopword = stopwords.words('english')\n    lemmentizer = WordNetLemmatizer()\n    text = text.lower()\n    text =  re.sub(r\"https?:\/\/\\S+\",'',text)\n    text = re.sub(r\"@\\w+\", '',text)\n    text = re.sub(r\"#\", '',text)\n    text = re.sub(r'\\n',' ', text)\n    text = re.sub('\\s+', ' ', text).strip()\n    text = word_tokenize(text)\n    text = [word for word in text if word.isalnum()]\n    text = [word for word in text if word not in stopword]\n    text = [lemmentizer.lemmatize(word) for word in text]\n    text = \" \".join(word for word in text)\n    \n    return text","bb22c9f4":"def mentions(text):\n    text = re.findall(r\"@\\w+\",text)\n    return [txt.strip('@') for txt in text]","ea360937":"def hashtags(text):\n    text = re.findall(r\"#\\w+\",text)\n    return [txt.strip('#') for txt in text]","49766482":"trainDf['clean_tweets'] = trainDf.text.apply(tweet_cleaning)\ntrainDf['mentions'] = trainDf.text.apply(mentions)\ntrainDf['hashtags'] = trainDf.text.apply(hashtags)","d5bb081b":"testDf['clean_tweets'] = testDf.text.apply(tweet_cleaning)\ntestDf['mentions'] = testDf.text.apply(mentions)\ntestDf['hashtags'] = testDf.text.apply(hashtags)","0c509344":"vdata = pd.concat([trainDf.drop('target', axis=1), testDf], axis=0)\nvdata.head()","220c6793":"vectorizer = TfidfVectorizer()\nvectorizer.fit(vdata.clean_tweets)\nvectorize_tweets_train = vectorizer.transform(trainDf.clean_tweets)\nvectorize_tweets_test = vectorizer.transform(testDf.clean_tweets)\ntrain = pd.DataFrame(vectorize_tweets_train.toarray(),index=trainDf.index)\ntest = pd.DataFrame(vectorize_tweets_test.toarray(), index=testDf.index)","83cf169f":"train = pd.concat([train, trainDf.target],axis=1)","cfcaf393":"X = train.iloc[:,:-1].values\ny = train.target.values","d15869f2":"Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.3)","e70ecead":"def train_model(clf, Xtrain=Xtrain, ytrain=ytrain, Xtest=Xtest, ytest=ytest,nsplit=3):\n    skf = StratifiedKFold(n_splits=nsplit)\n    \n    for trainIndex, testIndex in skf.split(Xtrain,ytrain):\n        clf.fit(Xtrain[trainIndex],ytrain[trainIndex])\n        pred = clf.predict(Xtrain[testIndex])\n\n        print(\"Confusion Matrix :\\n\", confusion_matrix(ytrain[testIndex], pred))\n        print(\"Classification Report :\\n\", classification_report(ytrain[testIndex], pred))\n        print(\"Accuracy :\\n\", accuracy_score(ytrain[testIndex], pred))\n        \n    pred = clf.predict(Xtest)\n    print(\"\\033[1mConfusion Matrix :\\033[0m\\n\", confusion_matrix(ytest, pred))\n    print(\"\\033[1mClassification Report :\\033[0m\\n\", classification_report(ytest, pred))\n    print(\"\\033[1mAccuracy :\\033[0m\\n\", accuracy_score(ytest, pred))\n    \n    return clf","06bd4b80":"mnb = MultinomialNB(alpha=1.5)","72a4a8b9":"mnb = train_model(mnb)","45e86bf9":"gnb = GaussianNB(var_smoothing=1e-01)","bd711782":"gnb = train_model(gnb)","19ae2e82":"rgc = RidgeClassifier(alpha=2)","bc71ca99":"rgc = train_model(rgc)","c1f075a3":"dtc = DecisionTreeClassifier()","a092cf68":"dtc = train_model(dtc)","0856c97e":"rfc = RandomForestClassifier(n_estimators=200)","b16c83cb":"rfc = train_model(rfc)","d55ae0c9":"xgc = XGBClassifier(n_jobs= -1, warm_start = True)","349dec41":"xgc = train_model(xgc)","b5899279":"svc = SVC(C=0.5)","56a3c70f":"svc = train_model(svc)","220e5f78":"prediction = rgc.predict(test)\nresult = pd.DataFrame(prediction, index=testDf.id, columns=['target'])","98aef161":"result = result.reset_index()","c2374309":"result.columns = [['id', 'target']]","5daa24e5":"result.index = range(1, 3264, 1)","6fcb42a4":"result.to_csv('nlp_sub1.csv',index=False)","b6d2e6ae":"result","13d06a91":"## Multinomial Naive Bayes ","717137b5":"## XGBOOST","35ee8ca8":"## Support Vector","5e0aaeb1":"## Desicion Tree","78fc9a2f":"## Ridge Classifier","c1d226de":"# Model","0711fc37":"## Random Forest","8b9b632c":"# Prediction","172dcb0e":"## **Location**","dbc69a34":"# Text","bd22522c":"# **Keyword**","e7bee666":"## Gaussian Naive Bayes "}}