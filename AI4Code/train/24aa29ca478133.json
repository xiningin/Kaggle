{"cell_type":{"71865ace":"code","c77cb778":"code","05f4aa78":"code","3f10acf8":"code","ac2c6b76":"code","b5326d27":"code","9df739f0":"code","a17dd5e4":"code","f6541838":"code","cdc6bf55":"code","a3c309fb":"code","283020e0":"code","bc326bf6":"code","c26aa468":"code","ed0b7406":"code","2e0ea0ff":"code","e0c5bab7":"code","f719f9d4":"code","0d392433":"code","05eb2016":"code","d85cd9a9":"code","6b7cbb18":"code","d7b7382b":"code","8ab1e6e7":"code","61c8ada7":"code","0440f5cb":"code","91455826":"code","ce7b8cc3":"code","c207aa02":"code","92d88343":"code","4716a6a3":"code","5fef6bbc":"code","9cb85dc1":"code","a9a19e5d":"code","09181509":"code","bb49a1cc":"code","f5eafe1d":"code","2f4bdd06":"code","f4e0372e":"code","593e8fc2":"code","e693d1cd":"code","90fa11b7":"code","d67e19e2":"code","819ff9fa":"code","3af8e246":"code","ea9cda15":"code","8d69b711":"code","f329b172":"code","b8c21dcd":"code","40667c55":"code","6344e0cc":"code","aa2004be":"code","55551a75":"code","6e81e1bf":"code","b4fa9b5a":"code","b915b131":"code","8863a870":"code","b5f5a18f":"code","636e003e":"code","bc364a23":"code","31cd101f":"code","b48c77af":"code","2cd2d5b9":"code","40eb815c":"code","63267bf7":"code","f2cd0540":"code","0707b1cc":"code","3e866e8f":"code","d74b99c6":"code","e4ab2a43":"code","6391137d":"code","26b57054":"code","71393801":"code","33452c59":"code","874b5184":"code","17805c06":"code","d28ff5bf":"code","9ee1b02d":"code","4af5d7b9":"code","7afa0e51":"code","00948186":"code","c2aaab5c":"code","f5228e12":"code","b2a3797f":"code","4238669c":"code","896c522e":"code","452b52e2":"code","dad9e678":"code","e22f11a3":"code","7dd12c66":"code","d7d89450":"code","b8834765":"code","6d05b29c":"code","323e6df1":"code","75fe0376":"code","83a3d792":"code","a118e6f5":"code","386ee554":"code","9995004d":"code","bfdda98f":"code","502984df":"code","f3c20b94":"code","1e6297aa":"code","4b409ad9":"code","5254a602":"code","4ad3171b":"code","6b941eea":"code","64e07f10":"markdown","3a343460":"markdown","0982a9b4":"markdown","1e96823c":"markdown","d1923691":"markdown","9ddc6e76":"markdown","1dcdc301":"markdown","bdb91370":"markdown","89168c4e":"markdown","c8b7bc96":"markdown","b9b22e67":"markdown","0a664c6a":"markdown","fd724123":"markdown","4fb855f7":"markdown","1a1f08e0":"markdown","8d323408":"markdown","7ba5b0dc":"markdown","209c3d35":"markdown","77acea9e":"markdown","133e152b":"markdown","89ed37d0":"markdown","aa0cce33":"markdown","47d3683d":"markdown","00958287":"markdown","3f773417":"markdown","16e32f40":"markdown","5a8f4ed7":"markdown","19626204":"markdown","a2985397":"markdown","fef00585":"markdown","e5e38eae":"markdown","f534bb2f":"markdown","c3c9373f":"markdown","c5bce767":"markdown","1eee45ac":"markdown","c848342b":"markdown","2c252562":"markdown","294d4925":"markdown","600248be":"markdown","5b420520":"markdown","22ae4936":"markdown","384f1fb0":"markdown","b981eb13":"markdown","e842cb63":"markdown"},"source":{"71865ace":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","c77cb778":"Fire_Dataset_Path = Path(\"..\/input\/fire-dataset\/fire_dataset\")","05f4aa78":"PNG_Path = list(Fire_Dataset_Path.glob(r\"*\/*.png\"))","3f10acf8":"PNG_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],PNG_Path))","ac2c6b76":"print(\"FIRE: \", PNG_Labels.count(\"fire_images\"))\nprint(\"NO_FIRE: \", PNG_Labels.count(\"non_fire_images\"))","b5326d27":"PNG_Path_Series = pd.Series(PNG_Path,name=\"PNG\").astype(str)\nPNG_Labels_Series = pd.Series(PNG_Labels,name=\"CATEGORY\")","9df739f0":"print(PNG_Path_Series)","a17dd5e4":"print(PNG_Labels_Series)","f6541838":"PNG_Labels_Series.replace({\"non_fire_images\":\"NO_FIRE\",\"fire_images\":\"FIRE\"},inplace=True)","cdc6bf55":"print(PNG_Labels_Series)","a3c309fb":"Main_Train_Data = pd.concat([PNG_Path_Series,PNG_Labels_Series],axis=1)","283020e0":"print(Main_Train_Data.head(-1))","bc326bf6":"Main_Train_Data = Main_Train_Data.sample(frac=1).reset_index(drop=True)","c26aa468":"print(Main_Train_Data.head(-1))","ed0b7406":"print(Main_Train_Data[\"PNG\"][2])\nprint(Main_Train_Data[\"CATEGORY\"][2])\nprint(Main_Train_Data[\"PNG\"][200])\nprint(Main_Train_Data[\"CATEGORY\"][200])\nprint(Main_Train_Data[\"PNG\"][45])\nprint(Main_Train_Data[\"CATEGORY\"][45])\nprint(Main_Train_Data[\"PNG\"][852])\nprint(Main_Train_Data[\"CATEGORY\"][852])","2e0ea0ff":"remove_PNG = '..\/input\/fire-dataset\/fire_dataset\/non_fire_images\/non_fire.189.png'\nMain_Train_Data = Main_Train_Data.loc[~(Main_Train_Data.loc[:,'PNG'] == remove_PNG),:]","e0c5bab7":"print(Main_Train_Data.loc[Main_Train_Data.loc[:,'PNG'] == remove_PNG,:])","f719f9d4":"print(Main_Train_Data.head(-1))","0d392433":"plt.style.use(\"dark_background\")","05eb2016":"sns.countplot(Main_Train_Data[\"CATEGORY\"])\nplt.show()","d85cd9a9":"Main_Train_Data['CATEGORY'].value_counts().plot.pie(figsize=(5,5))\nplt.show()","6b7cbb18":"figure = plt.figure(figsize=(10,10))\nx = cv2.imread(Main_Train_Data[\"PNG\"][0])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(Main_Train_Data[\"CATEGORY\"][0])","d7b7382b":"figure = plt.figure(figsize=(10,10))\nx = cv2.imread(Main_Train_Data[\"PNG\"][993])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(Main_Train_Data[\"CATEGORY\"][993])","8ab1e6e7":"figure = plt.figure(figsize=(10,10))\nx = cv2.imread(Main_Train_Data[\"PNG\"][20])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(Main_Train_Data[\"CATEGORY\"][20])","61c8ada7":"figure = plt.figure(figsize=(10,10))\nx = cv2.imread(Main_Train_Data[\"PNG\"][48])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(Main_Train_Data[\"CATEGORY\"][48])","0440f5cb":"fig, axes = plt.subplots(nrows=5,\n                        ncols=5,\n                        figsize=(10,10),\n                        subplot_kw={\"xticks\":[],\"yticks\":[]})\n\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(cv2.imread(Main_Train_Data[\"PNG\"][i]))\n    ax.set_title(Main_Train_Data[\"CATEGORY\"][i])\nplt.tight_layout()\nplt.show()","91455826":"fig, axes = plt.subplots(nrows=5,\n                        ncols=5,\n                        figsize=(10,10),\n                        subplot_kw={\"xticks\":[],\"yticks\":[]})\n\nfor i,ax in enumerate(axes.flat):\n    x = cv2.imread(Main_Train_Data[\"PNG\"][i])\n    x = cv2.cvtColor(x,cv2.COLOR_RGB2BGR)\n    ax.imshow(x)\n    ax.set_title(Main_Train_Data[\"CATEGORY\"][i])\nplt.tight_layout()\nplt.show()","ce7b8cc3":"Train_Generator = ImageDataGenerator(rescale=1.\/255,\n                                    shear_range=0.3,\n                                    zoom_range=0.2,\n                                    brightness_range=[0.2,0.9],\n                                    rotation_range=30,\n                                    horizontal_flip=True,\n                                    vertical_flip=True,\n                                    fill_mode=\"nearest\",\n                                    validation_split=0.1)","c207aa02":"Test_Generator = ImageDataGenerator(rescale=1.\/255)","92d88343":"Train_Data,Test_Data = train_test_split(Main_Train_Data,train_size=0.9,random_state=42,shuffle=True)","4716a6a3":"print(\"TRAIN SHAPE: \",Train_Data.shape)\nprint(\"TEST SHAPE: \",Test_Data.shape)","5fef6bbc":"print(Train_Data.head(-1))\nprint(\"----\"*20)\nprint(Test_Data.head(-1))","9cb85dc1":"print(Test_Data[\"CATEGORY\"].value_counts())","a9a19e5d":"encode = LabelEncoder()","09181509":"For_Prediction_Class = encode.fit_transform(Test_Data[\"CATEGORY\"])","bb49a1cc":"example_Image = Train_Data[\"PNG\"][99]\nLoad_Image = image.load_img(example_Image,target_size=(200,200))\nArray_Image = image.img_to_array(Load_Image)\nArray_Image = Array_Image.reshape((1,) + Array_Image.shape)\n\ni = 0\nfor batch in Train_Generator.flow(Array_Image,batch_size=1):\n    plt.figure(i)\n    IMG = plt.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\nplt.show()","f5eafe1d":"Train_IMG_Set = Train_Generator.flow_from_dataframe(dataframe=Train_Data,\n                                                   x_col=\"PNG\",\n                                                   y_col=\"CATEGORY\",\n                                                   color_mode=\"rgb\",\n                                                   class_mode=\"categorical\",\n                                                   batch_size=32,\n                                                   subset=\"training\")","2f4bdd06":"Validation_IMG_Set = Train_Generator.flow_from_dataframe(dataframe=Train_Data,\n                                                   x_col=\"PNG\",\n                                                   y_col=\"CATEGORY\",\n                                                   color_mode=\"rgb\",\n                                                   class_mode=\"categorical\",\n                                                   batch_size=32,\n                                                   subset=\"validation\")","f4e0372e":"Test_IMG_Set = Test_Generator.flow_from_dataframe(dataframe=Test_Data,\n                                                 x_col=\"PNG\",\n                                                 y_col=\"CATEGORY\",\n                                                 color_mode=\"rgb\",\n                                                 class_mode=\"categorical\",\n                                                 batch_size=32)","593e8fc2":"for data_batch,label_batch in Train_IMG_Set:\n    print(\"DATA SHAPE: \",data_batch.shape)\n    print(\"LABEL SHAPE: \",label_batch.shape)\n    break","e693d1cd":"for data_batch,label_batch in Validation_IMG_Set:\n    print(\"DATA SHAPE: \",data_batch.shape)\n    print(\"LABEL SHAPE: \",label_batch.shape)\n    break","90fa11b7":"for data_batch,label_batch in Test_IMG_Set:\n    print(\"DATA SHAPE: \",data_batch.shape)\n    print(\"LABEL SHAPE: \",label_batch.shape)\n    break","d67e19e2":"print(\"TRAIN: \")\nprint(Train_IMG_Set.class_indices)\nprint(Train_IMG_Set.classes[0:5])\nprint(Train_IMG_Set.image_shape)\nprint(\"---\"*20)\nprint(\"VALIDATION: \")\nprint(Validation_IMG_Set.class_indices)\nprint(Validation_IMG_Set.classes[0:5])\nprint(Validation_IMG_Set.image_shape)\nprint(\"---\"*20)\nprint(\"TEST: \")\nprint(Test_IMG_Set.class_indices)\nprint(Test_IMG_Set.classes[0:5])\nprint(Test_IMG_Set.image_shape)","819ff9fa":"Model = Sequential()\n\nModel.add(Conv2D(32,(3,3),activation=\"relu\",\n                 input_shape=(256,256,3)))\nModel.add(BatchNormalization())\nModel.add(MaxPooling2D((2,2)))\n\n#\nModel.add(Conv2D(64,(3,3),\n                 activation=\"relu\",padding=\"same\"))\nModel.add(Dropout(0.2))\nModel.add(MaxPooling2D((2,2)))\n\n#\nModel.add(Conv2D(128,(3,3),\n                 activation=\"relu\",padding=\"same\"))\nModel.add(Dropout(0.5))\nModel.add(MaxPooling2D((2,2)))\n\n\n#\nModel.add(Flatten())\nModel.add(Dense(256,activation=\"relu\"))\nModel.add(Dropout(0.5))\nModel.add(Dense(1,activation=\"sigmoid\"))","3af8e246":"Call_Back = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=5,mode=\"min\")","ea9cda15":"Model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","8d69b711":"CNN_Model = Model.fit(Train_IMG_Set,\n                      validation_data=Validation_IMG_Set,\n                            callbacks=Call_Back,\n                      epochs=50)","f329b172":"print(Model.summary())","b8c21dcd":"plot_model(Model,to_file=\"Model_One.png\",show_layer_names=True,show_dtype=True,show_shapes=True)","40667c55":"Model_Results = Model.evaluate(Test_IMG_Set)\nprint(\"LOSS:  \" + \"%.4f\" % Model_Results[0])\nprint(\"ACCURACY:  \" + \"%.2f\" % Model_Results[1])","6344e0cc":"plt.plot(CNN_Model.history[\"accuracy\"])\nplt.plot(CNN_Model.history[\"val_accuracy\"])\nplt.ylabel(\"ACCURACY\")\nplt.legend()\nplt.show()","aa2004be":"plt.plot(CNN_Model.history[\"loss\"])\nplt.plot(CNN_Model.history[\"val_loss\"])\nplt.ylabel(\"LOSS\")\nplt.legend()\nplt.show()","55551a75":"plt.plot(CNN_Model.history[\"val_accuracy\"])\nplt.plot(CNN_Model.history[\"val_loss\"])\nplt.ylabel(\"ACCURACY-LOSS\")\nplt.legend()\nplt.show()","6e81e1bf":"Dict_Summary_One = pd.DataFrame(CNN_Model.history)\nDict_Summary_One.plot()","b4fa9b5a":"Prediction_One = Model.predict(Test_IMG_Set)\nPrediction_One = Prediction_One.argmax(axis=-1)","b915b131":"print(Prediction_One)","8863a870":"Predict_Class = Model.predict_classes(Test_IMG_Set)","b5f5a18f":"print(Predict_Class)","636e003e":"fig, axes = plt.subplots(nrows=8,\n                         ncols=8,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(cv2.imread(Test_Data[\"PNG\"].iloc[i]))\n    ax.set_title(f\"TEST:{Test_Data.CATEGORY.iloc[i]}\\n PREDICTION:{Predict_Class[i]}\")\nplt.tight_layout()\nplt.show()","bc364a23":"print(confusion_matrix(For_Prediction_Class,Predict_Class))","31cd101f":"print(classification_report(For_Prediction_Class,Predict_Class))","b48c77af":"image_path = \"..\/input\/test-dataset\/Fire-Detection\/1\/12.jpg\"\nimg = image.load_img(image_path,target_size=(256,256))\nx = image.img_to_array(img)\nx = np.expand_dims(x,axis=0)","2cd2d5b9":"Diff_Pred = Model.predict(x)\nDiff_Pred = Diff_Pred.argmax(axis=-1)\nprint(Diff_Pred)","40eb815c":"image_path_Two = \"..\/input\/test-dataset\/Fire-Detection\/0\/12.jpg\"\nimg_Two = image.load_img(image_path_Two,target_size=(256,256))\nx_Two = image.img_to_array(img_Two)\nx_Two = np.expand_dims(x_Two,axis=0)","63267bf7":"Diff_Pred_Two = Model.predict(x_Two)\nDiff_Pred_Two = Diff_Pred_Two.argmax(axis=-1)\nprint(Diff_Pred_Two)","f2cd0540":"Model_Two = tf.keras.models.Sequential([\n  # inputs \n  tf.keras.layers.experimental.preprocessing.Rescaling(1.\/255),\n  tf.keras.layers.Flatten(input_shape=(256,)),\n  # hiddens layers\n  tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  # output layer\n  tf.keras.layers.Dense(2,activation=\"softmax\")\n])","0707b1cc":"Model_Two.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])","3e866e8f":"ANN_Model = Model_Two.fit(Train_IMG_Set,\n                          validation_data=Validation_IMG_Set,\n                          callbacks=Call_Back,\n                      epochs=100)","d74b99c6":"print(Model_Two.summary())","e4ab2a43":"plot_model(Model_Two,to_file=\"Model_Two.png\",show_layer_names=True,show_dtype=True,show_shapes=True)","6391137d":"Model_Results_Two = Model_Two.evaluate(Test_IMG_Set)\nprint(\"LOSS:  \" + \"%.4f\" % Model_Results_Two[0])\nprint(\"ACCURACY:  \" + \"%.2f\" % Model_Results_Two[1])","26b57054":"plt.plot(ANN_Model.history[\"accuracy\"])\nplt.plot(ANN_Model.history[\"val_accuracy\"])\nplt.ylabel(\"ACCURACY\")\nplt.legend()\nplt.show()","71393801":"plt.plot(ANN_Model.history[\"loss\"])\nplt.plot(ANN_Model.history[\"val_loss\"])\nplt.ylabel(\"LOSS\")\nplt.legend()\nplt.show()","33452c59":"plt.plot(ANN_Model.history[\"val_accuracy\"])\nplt.plot(ANN_Model.history[\"val_loss\"])\nplt.ylabel(\"ACCURACY-LOSS\")\nplt.legend()\nplt.show()","874b5184":"Dict_Summary_Two = pd.DataFrame(ANN_Model.history)\nDict_Summary_Two.plot()","17805c06":"Prediction_Two = Model_Two.predict(Test_IMG_Set)\nPrediction_Two = Prediction_Two.argmax(axis=-1)","d28ff5bf":"print(Prediction_Two)","9ee1b02d":"Prediction_Class_Two = Model_Two.predict_classes(Test_IMG_Set)","4af5d7b9":"print(Prediction_Class_Two)","7afa0e51":"fig, axes = plt.subplots(nrows=8,\n                         ncols=8,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(cv2.imread(Test_Data[\"PNG\"].iloc[i]))\n    ax.set_title(f\"TEST:{Test_Data.CATEGORY.iloc[i]}\\n PREDICTION:{Prediction_Two[i]}\")\nplt.tight_layout()\nplt.show()","00948186":"print(confusion_matrix(For_Prediction_Class,Prediction_Class_Two))","c2aaab5c":"print(classification_report(For_Prediction_Class,Prediction_Class_Two))","f5228e12":"image_path = \"..\/input\/test-dataset\/Fire-Detection\/1\/12.jpg\"\nimg = image.load_img(image_path,target_size=(256,256))\nx = image.img_to_array(img)\nx = np.expand_dims(x,axis=0)","b2a3797f":"Diff_Pred = Model_Two.predict(x)\nDiff_Pred = Diff_Pred.argmax(axis=-1)\nprint(Diff_Pred)","4238669c":"image_path_Two = \"..\/input\/test-dataset\/Fire-Detection\/0\/12.jpg\"\nimg_Two = image.load_img(image_path_Two,target_size=(256,256))\nx_Two = image.img_to_array(img_Two)\nx_Two = np.expand_dims(x_Two,axis=0)","896c522e":"Diff_Pred_Two = Model_Two.predict(x_Two)\nDiff_Pred_Two = Diff_Pred_Two.argmax(axis=-1)\nprint(Diff_Pred_Two)","452b52e2":"Model_Three = Sequential()\n\nModel_Three.add(Conv2D(12,(3,3),activation=\"relu\",\n                 input_shape=(256,256,3)))\nModel_Three.add(BatchNormalization())\nModel_Three.add(MaxPooling2D((2,2)))\n\n#\nModel_Three.add(Conv2D(24,(3,3),\n                 activation=\"relu\"))\nModel_Three.add(Dropout(0.2))\nModel_Three.add(MaxPooling2D((2,2)))\n\n\n#\nModel_Three.add(TimeDistributed(Flatten()))\nModel_Three.add(Bidirectional(LSTM(32,\n                                  return_sequences=True,\n                                  dropout=0.5,\n                                  recurrent_dropout=0.5)))\nModel_Three.add(Bidirectional(GRU(32,\n                                  return_sequences=True,\n                                  dropout=0.5,\n                                  recurrent_dropout=0.5)))\n\n#\nModel_Three.add(Flatten())\n\nModel_Three.add(Dense(256,activation=\"relu\"))\nModel_Three.add(Dropout(0.5))\nModel_Three.add(Dense(2,activation=\"softmax\"))","dad9e678":"Model_Three.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])","e22f11a3":"RCNN_Model = Model_Three.fit(Train_IMG_Set,\n                          validation_data=Validation_IMG_Set,\n                          callbacks=Call_Back,\n                      epochs=100)","7dd12c66":"print(Model_Three.summary())","d7d89450":"plot_model(Model_Three,to_file=\"Model_Three.png\",show_layer_names=True,show_dtype=True,show_shapes=True)","b8834765":"Model_Results_Three = Model_Three.evaluate(Test_IMG_Set)\nprint(\"LOSS:  \" + \"%.4f\" % Model_Results_Three[0])\nprint(\"ACCURACY:  \" + \"%.2f\" % Model_Results_Three[1])","6d05b29c":"plt.plot(RCNN_Model.history[\"accuracy\"])\nplt.plot(RCNN_Model.history[\"val_accuracy\"])\nplt.ylabel(\"ACCURACY\")\nplt.legend()\nplt.show()","323e6df1":"plt.plot(RCNN_Model.history[\"loss\"])\nplt.plot(RCNN_Model.history[\"val_loss\"])\nplt.ylabel(\"LOSS\")\nplt.legend()\nplt.show()","75fe0376":"plt.plot(RCNN_Model.history[\"val_accuracy\"])\nplt.plot(RCNN_Model.history[\"val_loss\"])\nplt.ylabel(\"ACCURACY-LOSS\")\nplt.legend()\nplt.show()","83a3d792":"Dict_Summary_Three = pd.DataFrame(RCNN_Model.history)\nDict_Summary_Three.plot()","a118e6f5":"Prediction_Three = Model_Three.predict(Test_IMG_Set)\nPrediction_Three = Prediction_Three.argmax(axis=-1)","386ee554":"print(Prediction_Three)","9995004d":"Prediction_Class_Three = Model_Three.predict_classes(Test_IMG_Set)","bfdda98f":"print(Prediction_Class_Three)","502984df":"fig, axes = plt.subplots(nrows=8,\n                         ncols=8,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(cv2.imread(Test_Data[\"PNG\"].iloc[i]))\n    ax.set_title(f\"TEST:{Test_Data.CATEGORY.iloc[i]}\\n PREDICTION:{Prediction_Three[i]}\")\nplt.tight_layout()\nplt.show()","f3c20b94":"print(classification_report(For_Prediction_Class,Prediction_Three))","1e6297aa":"print(confusion_matrix(For_Prediction_Class,Prediction_Three))","4b409ad9":"image_path = \"..\/input\/test-dataset\/Fire-Detection\/1\/12.jpg\"\nimg = image.load_img(image_path,target_size=(256,256))\nx = image.img_to_array(img)\nx = np.expand_dims(x,axis=0)","5254a602":"Diff_Pred = Model_Three.predict(x)\nDiff_Pred = Diff_Pred.argmax(axis=-1)\nprint(Diff_Pred)","4ad3171b":"image_path_Two = \"..\/input\/test-dataset\/Fire-Detection\/0\/12.jpg\"\nimg_Two = image.load_img(image_path_Two,target_size=(256,256))\nx_Two = image.img_to_array(img_Two)\nx_Two = np.expand_dims(x_Two,axis=0)","6b941eea":"Diff_Pred_Three = Model_Three.predict(x)\nDiff_Pred_Three = Diff_Pred_Three.argmax(axis=-1)\nprint(Diff_Pred_Three)","64e07f10":"#### CHECKING","3a343460":"#### IMAGE GENERATOR","0982a9b4":"#### IMAGES","1e96823c":"# FULLY-CONNECTED","d1923691":"[1]","9ddc6e76":"# PACKAGES AND LIBRARIES","1dcdc301":"# TRANSFORMATION TO DATAFRAME STRUCTURE","bdb91370":"#### CHECKING","89168c4e":"# PATH & LABEL PROCESS","c8b7bc96":"# HISTORY","b9b22e67":"* FIRE","0a664c6a":"#### CHECKING","fd724123":"#### MAIN PATH","4fb855f7":"# DETERMINATION TRAIN AND TEST DATA","1a1f08e0":"#### SPLITTING TRAIN AND TEST","8d323408":"# TRANSFORMATION TO SERIES STRUCTURE","7ba5b0dc":"# CNN-RCNN","209c3d35":"#### PREDICTION","77acea9e":"* NOT FIRE","133e152b":"* NOT FIRE","89ed37d0":"* {'FIRE': 0, 'NO_FIRE': 1}","aa0cce33":"* {'FIRE': 0, 'NO_FIRE': 1}","47d3683d":"#### GENERAL","00958287":"* {'FIRE': 0, 'NO_FIRE': 1}","3f773417":"#### PREDICTION FOR DIFFERENT SOURCE","16e32f40":"#### PREDICTION FOR ANOTHER SOURCE","5a8f4ed7":"* We need to remove Non_Fire_189, this PNG is broken","19626204":"#### CLASSIFICATION & CONFUSION REPORT","a2985397":"#### How Generator Applied Image Look Like","fef00585":"#### CLASSIFICATION & CONFUSION REPORT","e5e38eae":"#### LABEL PROCESS","f534bb2f":"# VISUALIZATION","c3c9373f":"* NOT FIRE","c5bce767":"# CNN","1eee45ac":"* FIRE","c848342b":"#### SHUFFLING","2c252562":"#### APPLYING GENERATOR AND TRANSFORMATION TO TENSOR","294d4925":"#### Context\n* The dataset was created by my team during the NASA Space Apps Challenge in 2018, the goal was using the dataset to develop a model that can recognize the images with fire.\n\n#### Content\n* Data was collected to train a model to distinguish between the images that contain fire (fire images) and regular images (non-fire images), so the whole problem was binary classification.\n\n* Data is divided into 2 folders, fireimages folder contains 755 outdoor-fire images some of them contains heavy smoke, the other one is non-fireimages which contain 244 nature images (eg: forest, tree, grass, river, people, foggy forest, lake, animal, road, and waterfall).\n\n* Hint: Data is skewed, which means the 2 classes(folders) doesn't have an equal number of samples, so make sure that you have a validation set with an equally-sized number of images per class (eg: 40 images of both fire and non-fire classes).","600248be":"#### CLASSIFICATION & CONFUSION REPORT","5b420520":"* FIRE","22ae4936":"##### PREDICTION FOR DIFFERENT SOURCE","384f1fb0":"[0]","b981eb13":"#### PATH PROCESS","e842cb63":"#### PREDICTION"}}