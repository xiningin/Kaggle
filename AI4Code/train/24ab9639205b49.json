{"cell_type":{"886f708c":"code","7188fc59":"code","2ef66e12":"code","8ad44d5e":"code","d6fe7474":"code","a934d1be":"code","298f846e":"code","0f58643c":"code","91f8d514":"code","15cabfa8":"code","d1e03a71":"code","c3a969af":"markdown","fceb0e89":"markdown","5ca03f28":"markdown"},"source":{"886f708c":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport optuna\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso, Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n%matplotlib inline","7188fc59":"##Collect information of dataframe:\n#df_train = pd.read_csv('train.csv')\n#df_test = pd.read_csv('test.csv')\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\nprint(\"###train.csv###\")\nprint(\"elements:\\n\", df_train.columns, \"\\n\")\nprint(\"df shape:\", df_train.shape, \"\\n\")\nprint(\"head of df_train:\")\ndisplay(df_train.head())\n\nprint(\"SalePrice:\\n\", df_train['SalePrice'].describe(), \"\\n\")\nsns.displot(df_train['SalePrice'])\n\nprint(\"###test.csv###\")\nprint(\"elements:\\n\", df_test.columns, \"\\n\")\nprint(\"df shape:\", df_test.shape, \"\\n\")\nprint(\"head of df_test:\")\ndisplay(df_test.head())","2ef66e12":"##Calc correlation and show heatmap\ncorrelation = df_train.corr()\nax = plt.subplots(figsize=(12, 9))\nsns.heatmap(correlation, vmax=.8, square=True)","8ad44d5e":"##extract 10 element which has largeest correlation.\nelement_num = 10 #number of variables for heatmap\ncols_partial = correlation.nlargest(element_num, 'SalePrice')['SalePrice'].index\n#cols_partial = cols_partial.to_list()\nprint(\"10 element which has largeest correlation:\\n\", cols_partial)\ncoeff_matrix_partial = np.corrcoef(df_train[cols_partial].values.T)\nsns.set(font_scale=1.25)\nheatmap_partial = sns.heatmap(coeff_matrix_partial, \n                              cbar=True, \n                              annot=True, \n                              square=True, \n                              fmt='.2f', \n                              annot_kws={'size': 10}, \n                              yticklabels=cols_partial.values, xticklabels=cols_partial.values)\n\nplt.show()\n\n##pairplot by seaborn\nsns.pairplot(df_train[cols_partial], height = 2.5)\nplt.show()","d6fe7474":"##df_train\ndf_train_partial = df_train[cols_partial]\ndf_train_partial = df_train_partial.dropna()\nprint(\"shape of df_train_partial:\", df_train_partial.shape)\ndisplay(df_train_partial.head())\n\n##df_test\ncols_partial_test = [\"Id\"]\ncols_temp = [dummy for dummy in cols_partial if dummy != \"SalePrice\"]\ncols_partial_test = cols_partial_test + cols_temp\n\ndf_test_partial = df_test[cols_partial_test]\ndf_test_partial = df_test_partial.fillna(df_test_partial.mean())\nprint(\"shape of df_test_partial:\", df_test_partial.shape)\ndisplay(df_test_partial.head())\n\n##split train data >> learn data and check data.\ndf_learn, df_check = train_test_split(df_train_partial, test_size=0.3, random_state=0)\nprint(\"df_learn\")\ndisplay(df_learn.head())\nprint(\"df_check\")\ndisplay(df_check.head())\n\nsns.distplot(df_learn['SalePrice'])\nsns.distplot(df_check['SalePrice'])\n\n##x and y\ny_learn = df_learn.iloc[:, 0]\nx_learn = df_learn.iloc[:, 1:]\ny_check = df_check.iloc[:, 0]\nx_check = df_check.iloc[:, 1:]\nx_test = df_test_partial.iloc[:, 1:]\n\ny_learn_log10 = np.log10(y_learn)\ny_check_log10 = np.log10(y_check)\n\n#display(Y_train.head())","a934d1be":"sns.distplot(np.log10(df_learn['SalePrice']))\nsns.distplot(np.log10(df_check['SalePrice']))","298f846e":"##Linear regression and Results. (using 10 element which has largeest correlation.)\nlr_model = LinearRegression().fit(x_learn, y_learn)\nr2score_lr = lr_model.score(x_learn, y_learn)\ny_learn_pred_bylr = lr_model.predict(x_learn)\ny_check_pred_bylr = lr_model.predict(x_check)\n\nprint(\"###lr reults###\")\nprint(\"r2score:\", r2score_lr)  \nprint(\"coeff:\", lr_model.coef_)\nprint(\"intercept:\", lr_model.intercept_)\nprint(\"params:\", lr_model.get_params()) \n\n##graph setting\nplt.rcParams['font.family'] =\"Times New Roman\"\nplt.rcParams['font.size'] = 12\nplt.rcParams['xtick.direction'] = 'in'\nplt.rcParams['ytick.direction'] = 'in'\n\n##yy plot\nfig = plt.figure(figsize=(4,4))\nplt.xlabel(\"Y_actual\")\nplt.ylabel(\"Y_predicted\")\n#plt.xlim(0, 100)\n#plt.ylim(0.0, 20.0) \nplt.scatter(y_learn, y_learn_pred_bylr, color='black')\nplt.scatter(y_check, y_check_pred_bylr, color='blue')\nplt.plot(y_learn, y_learn, color='red')\nplt.show()\n\n##logarithm\nlr_model_log = LinearRegression().fit(x_learn, y_learn_log10)\nr2score_lrlog = lr_model_log.score(x_learn, y_learn_log10)\ny_learn_pred_bylrlog = lr_model_log.predict(x_learn)\ny_check_pred_bylrlog = lr_model_log.predict(x_check)\n\nprint(\"###lr log reults###\")\nprint(\"r2score:\", r2score_lrlog) \n\n##yy plot\nfig = plt.figure(figsize=(4,4))\nplt.xlabel(\"Y_actual\")\nplt.ylabel(\"Y_predicted\")\n#plt.xlim(0, 100)\n#plt.ylim(0.0, 20.0) \nplt.scatter(y_learn, 10 ** y_learn_pred_bylrlog, color='black')\nplt.scatter(y_check, 10 ** y_check_pred_bylrlog, color='blue')\nplt.plot(y_learn, y_learn, color='red')\nplt.show()","0f58643c":"##Lasso regression and Results. (using 10 element which has largeest correlation.)\nlasso_model = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(x_learn, y_learn)\nr2score_laso = lasso_model.score(x_learn, y_learn)\ny_learn_pred_bylasso = lasso_model.predict(x_learn)\ny_check_pred_bylasso = lasso_model.predict(x_check)\n\nprint(\"###lasso reults###\")\nprint(\"r2score:\", r2score_laso)  \nprint(\"coeff:\", lasso_model.coef_)\nprint(\"intercept:\", lasso_model.intercept_)\nprint(\"params:\", lasso_model.get_params()) \n\n##yy plot\nfig = plt.figure(figsize=(4,4))\nplt.xlabel(\"Y_actual\")\nplt.ylabel(\"Y_predicted\")\n#plt.xlim(0, 100)\n#plt.ylim(0.0, 20.0) \nplt.scatter(y_learn, y_learn_pred_bylasso, color='black')\nplt.scatter(y_check, y_check_pred_bylasso, color='blue')\nplt.plot(y_learn, y_learn, color='red')\nplt.show()\n\n##logarithm\nlasso_model_log = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(x_learn, y_learn_log10)\nr2score_lasolog = lasso_model_log.score(x_learn, y_learn_log10)\ny_learn_pred_bylassolog = lasso_model_log.predict(x_learn)\ny_check_pred_bylassolog = lasso_model_log.predict(x_check)\n\nprint(\"###lasso reults###\")\nprint(\"r2score:\", r2score_lasolog)   \n\n##yy plot\nfig = plt.figure(figsize=(4,4))\nplt.xlabel(\"Y_actual\")\nplt.ylabel(\"Y_predicted\")\n#plt.xlim(0, 100)\n#plt.ylim(0.0, 20.0) \nplt.scatter(y_learn, 10 ** y_learn_pred_bylassolog, color='black')\nplt.scatter(y_check, 10 ** y_check_pred_bylassolog, color='blue')\nplt.plot(y_learn, y_learn, color='red')\nplt.show()","91f8d514":"##Optuna and xgb\nx_learn = x_learn.values\nx_check = x_check.values\n\n\ndef objective(trial):\n    eta =  trial.suggest_loguniform('eta', 1e-8, 1.0)\n    gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n    max_depth = trial.suggest_int('max_depth', 1, 20)\n    min_child_weight = trial.suggest_loguniform('min_child_weight', 1e-8, 1.0)\n    max_delta_step = trial.suggest_loguniform('max_delta_step', 1e-8, 1.0)\n    subsample = trial.suggest_uniform('subsample', 0.0, 1.0)\n    reg_lambda = trial.suggest_uniform('reg_lambda', 0.0, 1000.0)\n    reg_alpha = trial.suggest_uniform('reg_alpha', 0.0, 1000.0)\n    \n    xgb_regr =xgb.XGBRegressor(eta = eta, gamma = gamma, max_depth = max_depth,\n                           min_child_weight = min_child_weight, max_delta_step = max_delta_step,\n                           subsample = subsample,reg_lambda = reg_lambda,reg_alpha = reg_alpha)\n\n    #score = cross_val_score(regr, X_train, y_train, cv=5, scoring=\"r2\")\n    xgb_regr.fit(x_learn, y_learn_log10)\n    y_learn_log10_pred_bytuna = xgb_regr.predict(x_learn)\n    #residual = r2_score(x_learn, y_learn_log10_pred_bytuna)\n    residual = xgb_regr.score(x_learn, y_learn_log10)\n    return residual\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=200)\noptimised_rf = xgb.XGBRegressor(eta = study.best_params['eta'],gamma = study.best_params['gamma'],\n                                max_depth = study.best_params['max_depth'],min_child_weight = study.best_params['min_child_weight'],\n                                max_delta_step = study.best_params['max_delta_step'],subsample = study.best_params['subsample'],\n                                reg_lambda = study.best_params['reg_lambda'],reg_alpha = study.best_params['reg_alpha'])\n\noptimised_rf.fit(x_learn ,y_learn_log10)\n","15cabfa8":"y_learn_log10_pred_opt = optimised_rf.predict(x_learn)\ny_check_log10_pred_opt = optimised_rf.predict(x_check)\n##\nr2score_xgb_tuna = optimised_rf.score(x_learn, y_learn_log10)\nr2score_xgb_tuna_check = optimised_rf.score(x_check, y_check_log10)\n\nprint(\"best_params:\", study.best_params)\nprint(\"r2score(learning, check):\", r2score_xgb_tuna, r2score_xgb_tuna_check)\n\n##yy plot\nfig = plt.figure(figsize=(4,4))\nplt.xlabel(\"Y_actual\")\nplt.ylabel(\"Y_predicted\")\n#plt.xlim(0, 100)\n#plt.ylim(0.0, 20.0) \nplt.scatter(y_learn, 10 ** y_learn_log10_pred_opt, color='black')\nplt.scatter(y_check, 10 ** y_check_log10_pred_opt, color='blue')\nplt.plot(y_learn, y_learn, color='red')\nplt.show()","d1e03a71":"###Output using xgb model.\ndf_test_dropna =  df_test.dropna()\ny_test_pred_log = optimised_rf.predict(x_test.values)\ny_test_pred = 10 ** y_test_pred_log\n\ndf_solution = pd.DataFrame({\"id\":df_test_partial[\"Id\"], \"SalePrice\":y_test_pred})\ndf_solution.to_csv(\"my_submission.csv\", index = False)\n\n##check output\ndisplay(df_solution.head())\nprint(\"SalePrice:\\n\", df_solution['SalePrice'].describe(), \"\\n\")\nsns.distplot(df_solution['SalePrice'])\nsns.distplot(df_learn['SalePrice'])","c3a969af":"# \"House Prices - Advanced Regression Techniques\"\n(Uno, 2021)\n\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\n\n## Comments\nTo provide a machine learning model with high learning efficiency, \nWe know that it is important to avoid overlearning train data.\nFirst of all, the train-test split procedure(scikit learn) is applied for \"train.csv\", I tried to construct the model which satisfied with a high learning rate for train data and test data. As a result, I got a XGBoost machine learning model with an over 94% learning rate by using optuna. The distribution of \"SalePrice\" predicted by test data agreed with \"SalePrice\" of train data.\n\n\n\u6a5f\u68b0\u5b66\u7fd2\u3067\u3088\u3044\u5b66\u7fd2\u7cbe\u5ea6\u3092\u5f97\u308b\u4e0a\u3067\u306f\u904e\u5b66\u7fd2\u3092\u9632\u3050\u3053\u3068\u304c\u91cd\u8981\u3067\u3042\u308b\u3068\u77e5\u3089\u308c\u3066\u308b\u3002\n\u305d\u3053\u3067\u79c1\u306f\u3001scikit learn\u306etrain-test split\u306e\u6a5f\u80fd\u3092\u6d3b\u7528\u3059\u308b\u3053\u3068\u3067\u3001\"train.csv\"\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8a13\u7df4\u7528\u3068\u30c6\u30b9\u30c8\u7528\u306b\u5206\u3051\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u4e88\u6e2c\u8aa4\u5dee\u53cc\u65b9\u3092\u6e80\u8db3\u3059\u308b\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u3092\u8a66\u307f\u305f\u3002\n\u7d50\u679c\u3068\u3057\u3066\u3001optuna\u3092\u7528\u3044\u305fXGBoost\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u306b\u306694%\u4ee5\u4e0a\u306e\u5b66\u7fd2\u7387\u3092\u5f97\u305f\u3002\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u304b\u3089\u4e88\u6e2c\u3055\u308c\u308b\u4f4f\u5b85\u4fa1\u683c\u306e\u5206\u5e03\u306f\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3068\u5408\u81f4\u3057\u3066\u3044\u305f\u3002\n\n\n## Refferene: \n\"COMPREHENSIVE DATA EXPLORATION WITH PYTHON\"\n\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n\n\"Regularized Linear Models\"\n\nhttps:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n\n\"Optuna\u3092\u4f7f\u3063\u305fxgboost\u306e\u8a2d\u5b9a\u65b9\u6cd5\"\n\nhttps:\/\/qiita.com\/motoyuki1963\/items\/70343a93899f42382765\n\n\"Optuna\u3092\u4f7f\u3063\u3066XGBoost\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3092\u3084\u3063\u3066\u307f\u308b\"\n\nhttp:\/\/www.algo-fx-blog.com\/xgboost-optuna-hyperparameter-tuning\/\n","fceb0e89":"There are correlation between \"Overall Qual\" and \"Saleprices\". \"GrLivArea\" and \"Saleprice\" are also.","5ca03f28":"Input 79; Output: \"SalePrice\".\n\nThe histogram of saleprices is unimodal."}}