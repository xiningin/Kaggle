{"cell_type":{"71372409":"code","38fc0822":"code","52a091ba":"code","7267ff7d":"code","e84c853d":"code","c1421150":"code","555cd8d2":"code","e3b64236":"code","84de99a9":"code","9fb261ca":"code","3cb40d3a":"code","c4d4572a":"code","ee3d3bda":"code","d04bbd63":"code","73b6e2ac":"code","0875e5a4":"code","12e3f66d":"code","d84500e3":"code","7897cefe":"code","64de61ee":"code","84129f8d":"code","8f709672":"code","8e22fef2":"code","365815d4":"code","905ccf98":"code","90e341ac":"code","e96f090b":"code","6fe4f691":"markdown","3911484d":"markdown","9e93508b":"markdown"},"source":{"71372409":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38fc0822":"import os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\n#import numpy as np\n#import pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom datetime import datetime\n\nfrom scipy.stats import skew, norm  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom xgboost import XGBRegressor\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","52a091ba":"def load_data():\n    # Read data\n    data_dir = Path(\"..\/input\/30-days-of-ml\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","7267ff7d":"def clean(df):\n    return df\ndef impute(df):\n    return df\ndef encode(df):\n    cat_cols = [col for col in df.columns if 'cat' in col]\n    enc = OrdinalEncoder()\n    df[cat_cols] = enc.fit_transform(df[cat_cols])\n    df.head()\n    return df\n\n#def encode(df):\n#    for colname in df.select_dtypes(\"object\"):\n#        df[colname], _ = df[colname].factorize()\n#   df.head()\n#    return df","e84c853d":"df_train, df_test = load_data()\ndf = pd.concat([df_train, df_test])","c1421150":"cont_features = [f for f in df_train.columns.tolist() if f.startswith('cont')]\ncat_features = [f for f in df_train.columns.tolist() if f.startswith('cat')]\nfeatures = cat_features + cont_features","555cd8d2":"data = df_train[features]\ntarget = df_train['target']\nall_data = pd.concat([data, df_test])","e3b64236":"fig, ax = plt.subplots(5, 3, figsize=(14, 24))\nfor i, feature in enumerate(cont_features):\n    plt.subplot(5, 3, i+1)\n    sns.histplot(all_data[feature][::100], \n                 color=\"blue\", \n                 kde=True, \n                 bins=100)\n    plt.xlabel(feature, fontsize=9)\nplt.show()","84de99a9":"from sklearn.mixture import GaussianMixture\ninits = [[0.3, 0.5, 0.7, 0.9], \n         [0.039, 0.093, 0.24, 0.29, 0.35, 0.42, 0.49, 0.56, 0.62, 0.66, 0.76],\n         [0.176, 0.322, 0.416, 0.495, 0.548, 0.618, 0.707, 0.937],\n         [0.2, 0.35, 0.44, 0.59, 0.75, 0.83],\n         [0.28, 0.31, 0.42, 0.5, 0.74, 0.85],\n         [0.25, 0.38, 0.43, 0.58, 0.75, 0.9],\n         [0.34, 0.48, 0.7, 0.88],\n         [0.25, 0.29, 0.35, 0.48, 0.61, 0.68, 0.78, 0.9],\n         [0.11, 0.2, 0.3, 0.35, 0.45, 0.6, 0.76, 0.9],\n         [0.22, 0.32, 0.38, 0.44, 0.53, 0.63, 0.71, 0.81, 0.87],\n         [0.19, 0.27, 0.37, 0.46, 0.56, 0.61, 0.71, 0.86],\n         [0.23, 0.35, 0.52, 0.7, 0.84],\n         [0.27, 0.32, 0.35, 0.49, 0.63, 0.7, 0.79, 0.88],\n         [0.22, 0.29, 0.35, 0.4, 0.47, 0.58, 0.68, 0.72, 0.8]]\ngmms = []\nfor feature, init in zip(cont_features, inits):\n    X_ = np.array(all_data[feature].tolist()).reshape(-1, 1)\n    means_init = np.array(init)[:,None]\n    gmm_ = GaussianMixture(n_components=len(init),\n                           means_init=means_init,\n                           random_state=0).fit(X_)\n    gmms.append(gmm_)\n    preds = gmm_.predict(X_)\n    all_data[f'{feature}_gmm'] = preds\n    df_train[f'{feature}_gmm'] = preds[:len(df_train)]\n    df_test[f'{feature}_gmm'] = preds[len(df_train):]","9fb261ca":"fig, ax = plt.subplots(5, 3, figsize=(24, 30))\nfor i, feature in enumerate(cont_features):\n    plt.subplot(5, 3, i+1)\n    sns.scatterplot(x=feature, \n                    y=\"target\", \n                    data=df_train[::150], \n                    hue=f'{feature}_gmm', \n                    palette='muted')\n    plt.xlabel(feature, fontsize=9)\nplt.show()","3cb40d3a":"fig, ax = plt.subplots(5, 3, figsize=(24, 30))\nfor i, feature in enumerate(cont_features):\n    plt.subplot(5, 3, i+1)\n    sns.histplot(x=feature, \n                 data=df_train[::150], \n                 hue=f'{feature}_gmm', \n                 kde=True, \n                 bins=100, \n                 palette='muted')\n    plt.xlabel(feature, fontsize=9)\nplt.show()","c4d4572a":"# Peek at the values\ndisplay(df_test)\n\n# Display information about dtypes and missing values\ndisplay(df_test.info())","ee3d3bda":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = df_train.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.set(font_scale=1.6)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .9})\nplt.show()","d04bbd63":"# Peek at the values\ndisplay(df_train)\ndisplay(df_test)\n\n# Display information about dtypes and missing values\ndisplay(df_train.info())\ndisplay(df_test.info())","73b6e2ac":"X = df_train.copy()\ny = X.pop(\"target\")\nX.pop(\"cat2\")\nX.pop(\"cat6\")\nX.pop(\"cat4\")\nX.pop(\"cat3\")\n\n#baseline_score = score_dataset(X, y)\n#print(f\"Baseline score: {baseline_score:.5f} RMSLE\")","0875e5a4":"print(X.shape)","12e3f66d":"print('START ML', datetime.now(), )\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)","d84500e3":"# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds,))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, random_state=42, l1_ratio=e_l1ratio))\n                                        \nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n                                   \n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\n                                       \n# changed objective from reg:linear to reg:squarederror\n#xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n#                                     max_depth=3, min_child_weight=0,\n#                                     gamma=0, subsample=0.7,\n#                                     colsample_bytree=0.7,\n#                                     objective='reg:squarederror', nthread=-1,\n#                                     scale_pos_weight=1, seed=27,\n#                                     reg_alpha=0.00006, random_state=42)\n\nxgb_params = {\n    'n_estimators': 5000,\n    'learning_rate': 0.12,\n    'subsample': 0.96,\n    'colsample_bytree': 0.12,\n    'max_depth': 2,\n    'booster': 'gbtree', \n    'reg_lambda': 66.1,\n    'reg_alpha': 15.9,\n    'random_state':40\n}\n\nxgboost = XGBRegressor(**xgb_params)\n\n# stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","7897cefe":"print(datetime.now(), 'xgbBoosting')\nxgboost_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'xgbBoosting end')","64de61ee":"def xgboost_models_predict(X):\n    return ((1 * xgboost_model_full_data.predict(X)))\n\ny_gbr_pred = xgboost_models_predict(X)\nprint('RMSLE score on train data:')\nprint(rmsle(y, y_gbr_pred))","84129f8d":"print(datetime.now(), 'lightgbm')\nlightgbm_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'lightgbm end')","8f709672":"def lightgbm_models_predict(X):\n    return ((1 * lightgbm_model_full_data.predict(X)))\n\ny_lightgbm_pred = lightgbm_models_predict(X)\nprint('RMSLE score on train data - lightgbm:')\nprint(rmsle(y, y_lightgbm_pred))","8e22fef2":"X_test = df_test.copy()\ny_test = X_test.pop(\"target\")\nX_test.pop(\"cat2\")\nX_test.pop(\"cat6\")\nX_test.pop(\"cat4\")\nX_test.pop(\"cat3\")\n\nprint(X_test.shape, X.shape)","365815d4":"print('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission.iloc[:,1] = xgboost_models_predict(X_test)\nsubmission.head()","905ccf98":"submission.to_csv(\"30days_ML_Submission_v18_xgb_original_param.csv\", index=False)\nprint('Save submission', datetime.now(),)","90e341ac":"print('Predict submission', datetime.now(),)\nsubmission1 = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission1.iloc[:,1] = lightgbm_models_predict(X_test)\nsubmission1.head()","e96f090b":"submission1.to_csv(\"30days_ML_Submission_v18_lightgbm_gmm.csv\", index=False)\nprint('Save submission', datetime.now(),)","6fe4f691":"# Correlation Matrix\n\nThe correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.\n","3911484d":"# Introduction\n\nThis notebook users Gaussian Mixture Models to create new feature for predictions with reference to the following publshed notebooks:\n\nReference Notebook : 3rd-place solution: Ensembling GBDTs from Ken\n\nReference: https:\/\/scikit-learn.org\/stable\/modules\/mixture.html#gmm","9e93508b":"## Feature Engineering with Gaussian Mixture Models\n"}}