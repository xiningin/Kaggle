{"cell_type":{"68d4e7cb":"code","1b512579":"code","9c24bd58":"code","1190c1d6":"code","6171bd5b":"code","52fb08e7":"code","8a78ad47":"code","06c775ba":"code","243c2d2f":"code","486bf766":"code","4fe7560f":"code","f6e361b0":"code","711165a4":"code","c973fe59":"code","ffa7d98d":"code","c90598da":"code","dae79144":"code","6a505749":"code","b39681ab":"code","9c0b2b6a":"code","7c5bacba":"markdown","66c7090b":"markdown"},"source":{"68d4e7cb":"import nltk\nimport pandas as pd","1b512579":"\nData=pd.read_csv('..\/input\/imdb-review-stemming\/IMDB Review Dataset.csv')\n\nData.head(10)\n\n","9c24bd58":"\nData['review']","1190c1d6":"from nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n\nps=PorterStemmer()\nimport re ","6171bd5b":"\nfor i in range(len(Data)):\n    #print(Data['review'][i])\n    #print(i)\n    text=re.sub(r'[^a-zA-Z]',' ',Data['review'][i])\n    text=text.lower()\n    text=text.split(' ')\n    text=[ps.stem(word) for word in text if word not in set(stopwords.words('english'))]\n    \n    Data['review'][i]=' '.join(text)\n    #print(Data['review'][i])\n    #break\n    \n    \nData.to_csv('..\/input\/imdb-review-stemming\/PreProcessedStemming.csv')    ","52fb08e7":"# Pre-Proccessed review by removing the following.\n#  1. Remove special character\n#  2. remove numbers\n#  3. Removing Stopwords\n#  4. stemming of words\n\nData=pd.read_csv('..\/input\/imdb-review-stemming\/PreProcessedStemming.csv')\n\n\nData.head()","8a78ad47":"Data.shape","06c775ba":"corpus=[]\nimport re\n\n\nfor i in range(0,len(Data)):\n    \n    review=re.sub('[^a-zA-Z]',' ', Data['review'][i])\n    review.lower()\n    review=review.split(' ')\n    #review=[ps.stem(word) for word in reviews if  not word in set(stopwords.words('english'))]\n    \n    \n    review=' '.join(review)\n    corpus.append(review)","243c2d2f":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ncv=CountVectorizer(max_features=10000,ngram_range=(1,3))\nX=cv.fit_transform(corpus).toarray()\nX.shape\n\ncorpus=[]\n\n#y=Data['sentiment']\n","486bf766":"# label encoding of sentiment\n\n# 0 - Positive\n# 1 - Negative\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\nData['sentiment'] = lb_make.fit_transform(Data['sentiment'])\n\n\ny=Data['sentiment']\n\ny","4fe7560f":"\n## Train Test Split\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train , X_test, y_train , y_test=train_test_split(X,y,test_size=0.3,random_state=2)","f6e361b0":"# Model Selection Naive Bayes\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodel=MultinomialNB().fit(X_train,y_train)","711165a4":"y_pred=model.predict(X_test)","c973fe59":"from sklearn.metrics import confusion_matrix\n\nconfusionM=confusion_matrix(y_test,y_pred)\nconfusionM","ffa7d98d":"TP=confusionM[0][0]\nFP=confusionM[0][1]\nFN=confusionM[1][0]\nTN=confusionM[1][1]\n\n\n\naccuracy=((TP + TN)\/(TP+FP+FN+TN)) * 100\nprint(accuracy)\n\n#Precision tells us how many of the correctly predicted cases actually turned out to be positive.\nprecision=(TP\/(TP +FP) ) * 100\nprint(precision)\n\n#Recall tells us how many of the actual positive cases we were able to predict correctly with our model.\nrecall= (TP \/ (TP + FN)) * 100\nprint(recall)","c90598da":"#Or we can use direct library avaiable to get accuracy of the prediction.\nfrom sklearn.metrics import accuracy_score\n\naccuracyscore=accuracy_score(y_test,y_pred)\naccuracyscore\n","dae79144":"#apply LogisticRegression classfier\nfrom sklearn.linear_model import LogisticRegression\nlg = LogisticRegression(class_weight='balanced').fit(X_train, y_train)\nprint (lg.coef_)\nprint('training set score obtained Logistic Regression: {:.2f}'.format(lg.score(X_train, y_train)))\nprint('test set score obtained Logistic Regression: {:.2f}'.format(lg.score(X_test, y_test)))","6a505749":"y_pred = lg.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","b39681ab":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","9c0b2b6a":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\nlogit_roc_auc = roc_auc_score(y_test, lg.predict_proba(X_test)[: ,1])\nfpr, tpr, thresholds = roc_curve(y_test, lg.predict_proba(X_test)[:,1])\n# print(thresholds)\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","7c5bacba":"# Logistic Regression","66c7090b":"import pickle \n  \n# Save the trained model as a pickle string. \nsaved_model = pickle.dumps(model) \n\nfilename='MultinomialNB.pkl'\n\npickle.dump(model, open(filename, 'wb'))\n#print(saved_model)\n#joblib.dump(model, 'MultinomialNB.pkl') "}}