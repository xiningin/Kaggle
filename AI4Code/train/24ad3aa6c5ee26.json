{"cell_type":{"0baef4d7":"code","cc73bef5":"code","3ea38ac7":"code","9f099446":"code","ccf74f9c":"code","bd516ea0":"code","ed729a70":"code","5411e8fd":"code","bb2cf4fd":"code","f2307416":"code","078e8507":"code","52831abd":"code","35a37a64":"code","a755f2f9":"code","13a3d6f2":"code","30267c96":"code","d64b0da3":"code","183cd496":"code","6062bfa9":"code","7f038a45":"code","a54952a8":"code","15d355e8":"code","c727d8a7":"code","dc24cf38":"code","d6e128b8":"code","7ebfb32e":"code","fbaa09d0":"code","c723cbf5":"code","ac613c92":"code","4a0bfc9c":"code","51464666":"code","f65dc178":"code","bb659f3c":"code","e182c040":"code","20143fc3":"code","c1b7634b":"code","3364f775":"code","91924473":"code","9b6b0842":"code","787e75b4":"code","55028598":"code","22ffd50f":"code","27a26a69":"code","c8be275a":"code","6885c011":"code","317cc79e":"code","a6aacc18":"code","9e491eec":"code","56ac4c6d":"code","a7d78665":"markdown","3bd0f3bc":"markdown","f6eca567":"markdown"},"source":{"0baef4d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc73bef5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\n","3ea38ac7":"data_path = '..\/input\/pima-indians-diabetes-database\/diabetes.csv'\ndata = pd.read_csv(data_path)\ndata.head()\n","9f099446":"data.isnull().sum()","ccf74f9c":"sns.pairplot(data, hue='Outcome')","bd516ea0":"X= data.drop('Outcome', axis = 1).values\ny= data['Outcome'].values\nX_train,X_test, y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=0)","ed729a70":"## Creating tensors\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\n# Float Tensor converts the X to a float values -> Independent Variables\n\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)","5411e8fd":"## Creating Model with PyTorch\n\nclass ANN_Model(nn.Module):\n    def __init__(self,input_features = 8, hidden1=20, hidden2=20, output=2):\n        super().__init__()\n        self.f_connected1= nn.Linear(input_features,hidden1)\n        self.f_connected2 = nn.Linear(hidden1,hidden2)\n        self.out= nn.Linear(hidden2,output)\n        \n    def forward(self, x):\n        x = f.relu(self.f_connected1(x))\n        x = f.relu(self.f_connected2(x))\n        x = self.out(x)\n        return x    ","bb2cf4fd":"## Instantiating ANN_model\ntorch.manual_seed(20)\nmodel = ANN_Model()","f2307416":"model.parameters","078e8507":"## Backward Propagation -> Loss Function and Optimizers\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.01)","52831abd":"epochs = 500\nfinal_losses = []\nfor i in range(epochs):\n    i = i+1\n    y_pred = model.forward(X_train)\n    loss = loss_function(y_pred,y_train)\n    final_losses.append(loss)\n    if i%10==1:\n        print(\"Epochs number: {} and the loss: {} \".format(i,loss.item()))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    ","35a37a64":"## plot the Loss Function \nplt.plot(range(epochs),final_losses)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')","a755f2f9":"## Prediction with X_test\npredictions = []\nwith torch.no_grad():\n    for i,data in enumerate(X_test):\n        y_pred = model(data)\n        predictions.append(y_pred.argmax().item())\n        print(y_pred.argmax().item()) #  argmax() gives the categorical data\n    ","13a3d6f2":"cm = confusion_matrix(y_test,predictions)\ncm","30267c96":"plt.figure(figsize=(10,6))\nsns.heatmap(cm,annot=True)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted values')\n","d64b0da3":"score = accuracy_score(y_test,predictions)","183cd496":"## Saving the model\ntorch.save(model,'diabetes.pt')\n","6062bfa9":"# loading the saved model\nmodel=torch.load('.\/diabetes.pt')","7f038a45":"model.eval()","a54952a8":"list1 = [6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0]\n\nnew_data = torch.tensor(list1)\n\nwith torch.no_grad():\n    print(model(new_data))\n    print(model(new_data).argmax().item())\n","15d355e8":"import datetime\n\nfrom sklearn.preprocessing import LabelEncoder","c727d8a7":"col_list1 = ['SalePrice','MSSubClass','MSZoning','LotFrontage','LotArea','Street','YearBuilt','LotShape','1stFlrSF','2ndFlrSF']\ncol_list2 = ['MSSubClass','MSZoning','LotFrontage','LotArea','Street','YearBuilt','LotShape','1stFlrSF','2ndFlrSF']\npricing_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',usecols=col_list1).dropna()\npricing_test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',usecols=col_list2).dropna()\n","dc24cf38":"pricing_data.info()","d6e128b8":"for i in pricing_data.columns:\n    print(\"Column name {} has {} unique values\".format(i,len(pricing_data[i].unique())))","7ebfb32e":"pricing_data['Total Years'] = datetime.datetime.now().year - pricing_data['YearBuilt']\npricing_data.drop('YearBuilt',axis=1,inplace=True)","fbaa09d0":"cat_features = ['MSSubClass','MSZoning','Street','LotShape']\nlabel_encoder ={}\n\nfor feature in cat_features:\n    label_encoder[feature]=LabelEncoder()\n    pricing_data[feature]= label_encoder[feature].fit_transform(pricing_data[feature])\n\npricing_data.head()","c723cbf5":"cat_features = np.stack([pricing_data['MSSubClass'],pricing_data['MSZoning'],pricing_data['Street'],pricing_data['LotShape']],1)\ncat_features","ac613c92":"## Converting numpy to tensors\ncat_features = torch.tensor(cat_features,dtype=torch.int64)\ncat_features","4a0bfc9c":"## Creating Continous Variables\ncont_features =[]\nfor i in pricing_data.columns:\n    if i in ['MSSubClass','MSZoning','Street','LotShape','SalePrice']:\n        pass\n    else:\n        cont_features.append(i)\n\ncont_values = np.stack([pricing_data[i].values for i in cont_features],axis = 1)\ncont_values = torch.tensor(cont_values,dtype=torch.float)\nlen(cont_features)","51464666":"## Dependent Feature\ny = torch.tensor(pricing_data['SalePrice'].values,dtype=torch.float).reshape(-1,1)\ny","f65dc178":"cat_features.shape,cont_values.shape,y.shape","bb659f3c":"cat_dims = [len(pricing_data[col].unique()) for col in ['MSSubClass','MSZoning','Street','LotShape']]\n## output dimensions should be set based on the input dimensions (min)\nembedding_dims = [(x, min(50,(x+1)\/\/2)) for x in cat_dims]\nembedding_dims","e182c040":"embed_representation = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dims])\nembed_representation","20143fc3":"embedding_val=[]\nfor i,e in enumerate(embed_representation):\n    embedding_val.append(e(cat_features[:,i]))\nembedding_val","c1b7634b":"z = torch.cat(embedding_val,1)\nz","3364f775":"# Dropout layer -> Regularization method\ndropout=nn.Dropout(0.4)\nfinal_embed = dropout(z)\nfinal_embed","91924473":"## Creating the neural network\nclass FeedForwardNN(nn.Module):\n    def __init__(self,embedding_dim,n_cont,out_sz,layers,p=0.5):\n        super().__init__()\n        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        \n        layerlist = []\n        n_emb = sum((out for inp,out in embedding_dim))\n        n_in = n_emb + n_cont\n        \n        for i in layers:\n            layerlist.append(nn.Linear(n_in,i))\n            layerlist.append(nn.ReLU(inplace=True))\n            layerlist.append(nn.BatchNorm1d(i))\n            layerlist.append(nn.Dropout(p))\n            n_in =i\n        layerlist.append(nn.Linear(layers[-1],out_sz))\n        \n        self.layers = nn.Sequential(*layerlist)\n    \n    def forward(self,x_cat,x_cont):\n        embeddings = []\n        for i,e in enumerate(self.embeds):\n            embeddings.append(e(x_cat[:,i]))\n        x = torch.cat(embeddings,1)\n        x = self.emb_drop(x)\n        \n        x_cont = self.bn_cont(x_cont)\n        x = torch.cat([x,x_cont],1)\n        x = self.layers(x)\n        return x","9b6b0842":"len(cont_features)","787e75b4":"torch.manual_seed(100)\nprice_model = FeedForwardNN(embedding_dims,len(cont_features),1,[100,50],p=0.4)\nprice_model","55028598":"loss_ftn = nn.MSELoss()\noptimizer = torch.optim.Adam(price_model.parameters(),lr = 0.01)\n","22ffd50f":"pricing_data.shape","27a26a69":"batch_size = 1200\ntest_size = int(batch_size*0.15)\ntrain_categorical = cat_features[:batch_size - test_size]\ntest_categorical = cat_features[batch_size - test_size: batch_size]\ntrain_cont = cont_values[:batch_size - test_size]\ntest_cont = cont_values[batch_size - test_size: batch_size]\n\ny_train = y[:batch_size - test_size]\ny_test = y[batch_size - test_size:batch_size]","c8be275a":"len(train_categorical),len(test_categorical),len(train_cont),len(test_cont),len(y_train),len(y_test)","6885c011":"epochs = 5000\nfinal_losses = []\nfor i in range(epochs):\n    i = i+1\n    y_pred = price_model(train_categorical,train_cont)\n    loss = torch.sqrt(loss_ftn(y_pred,y_train)) ## RMSE\n    final_losses.append(loss)\n    if i%10 == 1:\n        print(\"Epoch number: {} and the loss: {}\".format(i, loss.item()))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n","317cc79e":"plt.plot(range(epochs),final_losses)\nplt.ylabel('RMSE Loss')\nplt.xlabel('epoch')","a6aacc18":"# Validation of the test data\ny_pred = \"\"\nwith torch.no_grad():\n    y_pred = price_model(test_categorical,test_cont)\n    loss = torch.sqrt(loss_ftn(y_pred,y_test))\nprint('RMSE: {}'.format(loss))","9e491eec":"data_verify = pd.DataFrame(y_test.tolist(),columns=['Test'])\ndata_predicted = pd.DataFrame(y_pred.tolist(),columns=['Prediction'])\ndata_predicted","56ac4c6d":"torch.save(price_model,'houseprice.pt')","a7d78665":"## Embedding Categorical Features into the Neural Network\n*  While embedding the categorical features, we need to know the exact input and output dimensions of the features\n* Here 'MSSubClass','MSZoning','Street','LotShape' has output dimensions as [15,5,2,4] respectively","3bd0f3bc":"### Define Loss and Optimizer","f6eca567":"## Pytorch"}}