{"cell_type":{"8a6c12d1":"code","a3a56c1e":"code","49a10e7c":"code","8bc3d1d8":"code","807bec4c":"code","d5674340":"code","c8ccf0b7":"code","16783524":"code","a6b76f32":"code","8818c0e0":"code","e5f99be4":"code","728f1035":"code","05432504":"code","360e0ebd":"code","71771e3b":"code","371ea905":"code","78ccdbec":"code","60e3d23f":"code","c5e0d1f4":"code","43c0d36f":"code","d44c0cf1":"code","015b315c":"code","00292f0a":"code","1c83aeff":"code","5dad1896":"code","d766e5ec":"code","9e080ec2":"code","da6a602d":"code","e0fd5e2c":"code","da2acb0f":"code","3267ba7f":"code","5aa900c7":"code","20b3fd86":"code","49ba4f18":"code","52203f38":"code","ef5b2089":"code","d9bc4e3f":"code","b5e1b4ac":"code","247031f3":"code","761ca8b1":"code","40cb6441":"code","9defaf98":"code","95f10911":"code","6356ab3b":"code","87651f4a":"code","23e2d179":"code","ca4ff382":"code","f68fc0d3":"code","99ae066b":"code","2e6ea063":"code","e0f5d7e8":"code","1d9c8f35":"code","91fb606c":"code","c0f3dbd8":"code","c1dfe273":"code","193ec0e9":"code","7c577376":"code","5c0ae08f":"markdown","ce144951":"markdown","59316a8b":"markdown","8b703ace":"markdown","894d5f7a":"markdown","76fe5c23":"markdown","37f41b90":"markdown","544d79f1":"markdown","6570fa03":"markdown","ef261b27":"markdown","67efe36f":"markdown","d31dc85f":"markdown","70b52f30":"markdown","5600e705":"markdown","1ef4b72d":"markdown","739be5fd":"markdown","0f44ea3b":"markdown","1e8fca1f":"markdown","a8f54473":"markdown","352220cf":"markdown","0ca67bf9":"markdown","5ae06cf2":"markdown","21798f41":"markdown","c831b29f":"markdown","b7c5f3f1":"markdown","2ed77a6a":"markdown","a2a0df24":"markdown","22718272":"markdown","2b2e98d2":"markdown"},"source":{"8a6c12d1":"!pip install sweetviz","a3a56c1e":"# Importing Libraries(Technologies)\nfrom math import sqrt\nfrom pandas_datareader import data \nimport matplotlib.pyplot as plt\nimport scipy\nimport pandas as pd\nimport datetime as dt\nfrom datetime import date\nimport urllib.request,json\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport sweetviz\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM","49a10e7c":"df=pd.read_csv('https:\/\/github.com\/ajtheb\/Machine-Learning-Challenges-and-Hackathons\/raw\/master\/Lowest%20price%20hackerearth\/Dataset\/Train.csv')\ndftest=pd.read_csv('https:\/\/github.com\/ajtheb\/Machine-Learning-Challenges-and-Hackathons\/raw\/master\/Lowest%20price%20hackerearth\/Dataset\/Test.csv')","8bc3d1d8":"my_report = sweetviz.analyze([dftest, \"Test\"])\nmy_report.show_html('Report.html')","807bec4c":"df.head()","d5674340":"dftest.head()","c8ccf0b7":"df.describe()","16783524":"dftest.describe()","a6b76f32":"df.isnull().sum()","8818c0e0":"dftest.isnull().sum()","e5f99be4":"df.boxplot(rot=90,grid=False)","728f1035":"dftest.boxplot(rot=90,grid=False)","05432504":"import matplotlib.pyplot as plt\n\nx = df['Demand']\ny = dftest['Demand']\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(len(x)),x, s=10, c='b', marker=\"s\", label='train')\nax1.scatter(range(len(y)),y, s=10, c='r', marker=\"o\", label='test')\nplt.legend(loc='upper left');\nplt.show()","360e0ebd":"import matplotlib.pyplot as plt\n\nx = df['High_Cap_Price']\ny = dftest['High_Cap_Price']\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(len(x)),x, s=10, c='b', marker=\"s\", label='train')\nax1.scatter(range(len(y)),y, s=10, c='r', marker=\"o\", label='test')\nplt.legend(loc='upper left');\nplt.show()","71771e3b":"import matplotlib.pyplot as plt\n\nx = df['Product_Category']\ny = dftest['Product_Category']\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(len(x)),x, s=10, c='b', marker=\"s\", label='train')\nax1.scatter(range(len(y)),y, s=10, c='r', marker=\"o\", label='test')\nplt.legend(loc='upper left');\nplt.show()","371ea905":"import matplotlib.pyplot as plt\n\nx = df['Market_Category']\ny = dftest['Market_Category']\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(len(x)),x, s=10, c='b', marker=\"s\", label='train')\nax1.scatter(range(len(y)),y, s=10, c='r', marker=\"o\", label='test')\nplt.legend(loc='upper left');\nplt.show()","78ccdbec":"df['Demand'].sort_values(ascending=False).head(6)","60e3d23f":"df=df.drop(labels=[1682],axis=0)","c5e0d1f4":"plt.scatter(df['Demand'],df['Low_Cap_Price'])","43c0d36f":"plt.scatter(df['High_Cap_Price'],df['Low_Cap_Price'])","d44c0cf1":"df.groupby('Date')['Low_Cap_Price'].mean().plot(kind='bar')","015b315c":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(12,12))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\",square=True)","00292f0a":"df.groupby(['State_of_Country','Product_Category'])['Low_Cap_Price'].mean().plot(kind='bar')","1c83aeff":"df['Date']=pd.to_datetime(df['Date'])\ndftest['Date']=pd.to_datetime(dftest['Date'])\n\ndf['month']=pd.DatetimeIndex(df['Date']).month\ndf['day']=pd.DatetimeIndex(df['Date']).day\ndf['year']=pd.DatetimeIndex(df['Date']).year\ndf['weekday']=pd.DatetimeIndex(df['Date']).weekday\ndf['day_m_y']=df['day']*df['month']*df['year']\ndf['quarter'] = df['Date'].dt.quarter\ndf['weekofyear'] = df['Date'].dt.weekofyear\ndf['dayofyear'] = df['Date'].dt.dayofyear\n\ndftest['month']=pd.DatetimeIndex(dftest['Date']).month\ndftest['day']=pd.DatetimeIndex(dftest['Date']).day\ndftest['year']=pd.DatetimeIndex(dftest['Date']).year\ndftest['weekday']=pd.DatetimeIndex(dftest['Date']).weekday\ndftest['day_m_y']=dftest['day']*dftest['month']*dftest['year']\ndftest['quarter'] = dftest['Date'].dt.quarter\ndftest['weekofyear'] = df['Date'].dt.weekofyear\ndftest['dayofyear'] = df['Date'].dt.dayofyear","5dad1896":"df.head()","d766e5ec":"df.groupby('Grade')['Low_Cap_Price'].mean().sort_values().plot(kind='bar')","9e080ec2":"pre={\n      2:0,\n      3:1,\n      0:2,\n      1:3\n}\ndf['grade_coded']=df['Grade'].map(pre)\ndftest['grade_coded']=dftest['Grade'].map(pre)","da6a602d":"df['Demand*grade']=((df['Demand']-df['Demand'].min())\/(df['Demand'].max()-df['Demand'].min()))*((df['grade_coded']-df['grade_coded'].min())\/(df['grade_coded'].max()-df['grade_coded'].min()))\ndftest['Demand*grade']=((dftest['Demand']-dftest['Demand'].min())\/(dftest['Demand'].max()-dftest['Demand'].min()))*((dftest['grade_coded']-dftest['grade_coded'].min())\/(dftest['grade_coded'].max()-dftest['grade_coded'].min()))","e0fd5e2c":"ordinal=['State_of_Country','Market_Category','Product_Category']","da2acb0f":"for f in ordinal:\n  s=df.groupby(f)['Low_Cap_Price'].mean().sort_values()\n  pre={}\n  x=0\n  for rownum,(indx,val) in enumerate(s.iteritems()):\n    pre[indx]=x\n    x+=1\n  df[f+'coded']=df[f].map(pre)\n  dftest[f+'coded']=dftest[f].map(pre)\n  df=df.drop(columns=[f],axis=1)\n  dftest=dftest.drop(columns=[f],axis=1)\n","3267ba7f":"#ddf['prod_quarter']=df['Product_Categorycoded']*df['quarter']\ndf['prod_quar']=((df['Product_Categorycoded']-df['Product_Categorycoded'].min())\/(df['Product_Categorycoded'].max()-df['Product_Categorycoded'].min()))*((df['quarter']-df['quarter'].min())\/(df['quarter'].max()-df['quarter'].min()))\ndftest['prod_quar']=((dftest['Product_Categorycoded']-dftest['Product_Categorycoded'].min())\/(dftest['Product_Categorycoded'].max()-dftest['Product_Categorycoded'].min()))*((dftest['quarter']-dftest['quarter'].min())\/(dftest['quarter'].max()-dftest['quarter'].min()))","5aa900c7":"df.head()","20b3fd86":"#df['Market_product']=df['Market_Category']*df['Product_Category']\ndf['Market_product']=((df['Product_Categorycoded']-df['Product_Categorycoded'].min())\/(df['Product_Categorycoded'].max()-df['Product_Categorycoded'].min()))*((df['Market_Categorycoded']-df['Market_Categorycoded'].min())\/(df['Market_Categorycoded'].max()-df['Market_Categorycoded'].min()))\ndftest['Market_product']=((dftest['Product_Categorycoded']-dftest['Product_Categorycoded'].min())\/(dftest['Product_Categorycoded'].max()-dftest['Product_Categorycoded'].min()))*((dftest['Market_Categorycoded']-dftest['Market_Categorycoded'].min())\/(dftest['Market_Categorycoded'].max()-dftest['Market_Categorycoded'].min()))","49ba4f18":"plt.scatter(df['Demand*grade'],df['Low_Cap_Price'])","52203f38":"df['State_Prod']=df['State_of_Countrycoded']-df['Product_Categorycoded']\ndftest['State_Prod']=dftest['State_of_Countrycoded']-dftest['Product_Categorycoded']","ef5b2089":"pearsoncorr = df.corr(method='pearson')\nabs(pearsoncorr['Low_Cap_Price']).sort_values(ascending=False)","d9bc4e3f":"X=df.drop(columns=['Low_Cap_Price','Item_Id','Date','day','month','weekday','grade_coded','day_m_y','prod_quar','Grade','year'],axis=1)\ny=df['Low_Cap_Price'].values","b5e1b4ac":"Datatest=dftest.drop(columns=['Item_Id','Date','day','month','weekday','grade_coded','day_m_y','prod_quar','Grade','year'],axis=1)","247031f3":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)","761ca8b1":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error,r2_score\nxgbr=xgb.XGBRegressor()\nxgbr.fit(X_train,y_train)\ny=xgbr.predict(X_test)\ny=abs(y)\nfrom sklearn.metrics import mean_squared_log_error\nprint('RMSLE:', 100-mean_squared_log_error(y_test, y))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, y)))\nprint('RMSE:', r2_score(y_test, y))","40cb6441":"from xgboost import plot_importance\nplot_importance(xgbr)","9defaf98":"import lightgbm as lgb\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test)\n\nparam = {'objective': 'regression',\n         'num_leaves':20,\n         'boosting': 'gbdt',  \n         'metric': 'mae',\n         'learning_rate': 0.2, \n         'num_iterations': 1000,\n         'num_leaves': 80,\n         'max_depth': 6,\n         'min_data_in_leaf': 11,\n         'bagging_fraction': 0.80,\n         'bagging_freq': 1,\n         'bagging_seed': 142,\n         'feature_fraction': 0.80,\n         'feature_fraction_seed': 2,\n         'early_stopping_round': 200,\n         'max_bin': 250\n         }\n\nlgbm = lgb.train(params=param, verbose_eval=100, train_set=train_data, valid_sets=[test_data])\n\ny_pred_lgbm = lgbm.predict(X_test)\ny_pred_lgbm=abs(y_pred_lgbm)\n#print('RMSLE:', sqrt(mean_absolute_error(np.expm1(y_cv), np.expm1(y_pred_lgbm))))\nfrom sklearn.metrics import mean_squared_log_error\nprint('RMSLE:', 100-mean_squared_log_error(y_test, y_pred_lgbm))\n","95f10911":"import seaborn as sns\nfeature_imp = pd.DataFrame(sorted(zip(lgbm.feature_importance(), X.columns), reverse=True)[:50], \n                           columns=['Value','Feature'])\nplt.figure(figsize=(12, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","6356ab3b":"# Hyper Parameter Optimization\n\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20] ,\n \"max_depth\"        : [ 3,4,5,6,8,10,13,15],\n \"min_child_weight\" : [  3,5, 6,9 ,11,14,19],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ,0.5],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7,0.9 ],\n \"num_boost_round\": [10, 25, 50,80,100,150],\n  \"n_estimators\" :[50,100,200,300,500] \n}","87651f4a":"xg_reg = xgb.XGBRegressor()","23e2d179":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","ca4ff382":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nrandom_search=RandomizedSearchCV(xg_reg,param_distributions=params,n_iter=5,n_jobs=1,cv=4,verbose=3)","f68fc0d3":"from datetime import datetime\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X_train,y_train)\ntimer(start_time) # timing ends here for \"start_time\" variable","99ae066b":"random_search.best_params_","2e6ea063":"random_search.best_estimator_","e0f5d7e8":"random_search.best_score_","1d9c8f35":"xgbr=xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.7, gamma=0.1, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.05, max_delta_step=0, max_depth=13,\n             min_child_weight=19,  monotone_constraints='()',\n             n_estimators=500, n_jobs=0, num_boost_round=50,\n             num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n             scale_pos_weight=1, subsample=1, tree_method='exact',\n             validate_parameters=1, verbosity=None)\nxgbr.fit(X_train,y_train)\ny=xgbr.predict(X_test)\ny=abs(y)\nprint('RMSLE:', 100-mean_squared_log_error(y_test, y))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, y)))\nprint('r2 score:', r2_score(y_test, y))","91fb606c":"from xgboost import plot_importance\nplot_importance(xgbr)","c0f3dbd8":"from matplotlib.pyplot import figure\nfigure(figsize=(14,14))\nplt.scatter(range(len(y)),y,label='predicted',c='r')\nplt.scatter(range(len(y_test)),y_test,label='Actual',c='g')","c1dfe273":"out=xgbr.predict(Datatest)\nout=abs(out)","193ec0e9":"dataf=pd.DataFrame({\n    'Item_Id':dftest['Item_Id'],\n    'Low_Cap_Price':out\n})","7c577376":"dataf.to_csv('lowest_price_sub.csv',index=False)","5c0ae08f":"Quite the same","ce144951":"Demand for a particular grade of product can be a good feature.","59316a8b":"# Feature Generation","8b703ace":"Lets check for any outliers in the data.\nWe can see Demand and High Cap have very high range of values,and some of them are very higher than mean of the column data.","894d5f7a":"Light GBM","76fe5c23":"XG Boost","37f41b90":"It can be improved by using LightGBM.\n\nI scored 99.854 on private leaderboard.","544d79f1":"Features from Date could be a good feature.","6570fa03":"if u find it useful, please upvote!!!","ef261b27":"Check for null values","67efe36f":"# HyperParameter Tuning","d31dc85f":"# Training And Testing","70b52f30":"![](https:\/\/github.com\/ajtheb\/Machine-Learning-Challenges-and-Hackathons\/raw\/master\/Lowest%20price%20hackerearth\/Capture.PNG)","5600e705":"Removing Outlier","1ef4b72d":"High Cap price and Demand are two most important features in the model.","739be5fd":"That is a mess, cant say.","0f44ea3b":"Prediction on testing Dataset","1e8fca1f":"[Competetion Link](https:\/\/www.hackerearth.com\/challenges\/competitive\/hackerearth-machine-learning-challenge-pet-adoption\/?utm_source=challenges-modern&utm_campaign=participated-challenges&utm_medium=right-panel)","a8f54473":"Plot between Actual and Predicted values of Low cap price.","352220cf":"![](https:\/\/github.com\/ajtheb\/Machine-Learning-Challenges-and-Hackathons\/raw\/master\/Lowest%20price%20hackerearth\/sweetviz.PNG)","0ca67bf9":"Good corelation u can see between High_Cap_Price and Low_Cap_Price as highest price of product could tell how much it can be decreased so that people would be interested in buying it.","5ae06cf2":"# Data Loading ","21798f41":"Target Encoding for Grade,priority according to mean Low Cap Price for each Demand as considering it a ordinal categorical feature.","c831b29f":"No Null values in train and test data,that's good, we dont have to spend time in finding the best technique for imputing null values.","b7c5f3f1":"In this graph ,you can see some difference in the distribution of Demand in training and testing data.","2ed77a6a":"Target Encoding,priority according to mean Low Cap Price for each of these features as considering them as ordinal categorical feature.","a2a0df24":"XGBOOST","22718272":"Some Products are seasonal,so may be in more demand in a particular quarter of year.","2b2e98d2":"As discussed above , corelation of 0.72 between High_Cap_Price and Low_Cap_Price.\n\nCorelation of 0.23 between State_of_Country and Low_Cap_Price as State may have regulations on lowest price of products\n\nCorelation of 0.2 between Market_Category and Low_Cap_Price as market can play role in the price of product, for example you can find expensive things for cheap in a theif market.\n\nCorelation of 0.18 between Product_Category and Low_Cap_Price,watch u may get for 50 Rs but a Refrigarator you wont get for 50 rs(Dont talk crazy man!)\n\nAs such,Grade and Demand doesnt show much corelation with Low_Cap_Price.\n"}}