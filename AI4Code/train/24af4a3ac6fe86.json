{"cell_type":{"8c2980f3":"code","4e8c4ae5":"code","0972f17d":"code","07f29336":"code","a5269d80":"code","d08f98ae":"code","81abb24b":"code","f8f74642":"code","c958bf6f":"code","850bf1d8":"code","199a73d1":"code","a3eee53f":"code","e26c6495":"code","d9c54590":"code","19b163d0":"code","cebb8299":"code","c4cb86af":"code","fa9e17e9":"code","a70cd55c":"code","8d08574a":"markdown","d5d91291":"markdown","66269041":"markdown","09348537":"markdown","aef91873":"markdown","91d9a960":"markdown"},"source":{"8c2980f3":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport os\nimport gc\nimport pickle\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom tqdm.notebook import tqdm\nfrom time import time","4e8c4ae5":"def create_folds(num_starts, num_splits):\n    \n    folds = []\n    \n    # LOAD FILES\n    train_feats = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n    scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    drug = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\n    scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n    drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on = 'sig_id', how = 'left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n    \n    for seed in range(num_starts):\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}; dct2 = {}\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        scored['fold'] = scored.drug_id.map(dct1)\n        scored.loc[scored.fold.isna(),'fold'] =\\\n            scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n        scored.fold = scored.fold.astype('int8')\n        folds.append(scored.fold.values)\n        \n        del scored['fold']\n        \n    return np.stack(folds)","0972f17d":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss0 = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ncols = [c for c in ss.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","07f29336":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    return - np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip))\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']","a5269d80":"from sklearn.preprocessing import QuantileTransformer\n\nqt = QuantileTransformer(n_quantiles = 100, output_distribution = 'normal', random_state = 42)\nqt.fit(pd.concat([pd.DataFrame(train[GENES + CELLS]), pd.DataFrame(test[GENES + CELLS])]))\ntrain[GENES + CELLS] = qt.transform(train[GENES + CELLS])\ntest[GENES + CELLS] = qt.transform(test[GENES + CELLS])","d08f98ae":"from sklearn.decomposition import PCA\n\n# GENES\nn_comp_genes = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])])\npca_genes = PCA(n_components=n_comp_genes, random_state = 42)\ndata2 = pca_genes.fit_transform(data[GENES])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)\n\n#CELLS\nn_comp_cells = 10  #<--Update\n\ndata = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\npca_cells = PCA(n_components=n_comp_cells, random_state = 42)\ndata2 = pca_cells.fit_transform(data[CELLS])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","81abb24b":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train.append(test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 3:])\n\ntrain_transformed = data_transformed[ : train.shape[0]]\ntest_transformed = data_transformed[-test.shape[0] : ]\n\ntrain = pd.DataFrame(train[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntrain = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n\ntest = pd.DataFrame(test[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntest = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)\n\nprint(train.shape)\nprint(test.shape)","f8f74642":"train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain_targets_nonscored = train_targets_nonscored.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)\n\nprint(train.shape)","c958bf6f":"top_feats = np.arange(1, train.shape[1])\nprint(len(top_feats))","850bf1d8":"train.head()","199a73d1":"param = {'objective': 'binary:logistic',\n         'eval_metric': 'logloss', \n         'tree_method': 'gpu_hist', \n#          'tree_method': 'auto',\n#          'nthread': 4,\n         'verbosity': 0, \n         'colsample_bytree': 0.1818593017814899, \n         'eta': 0.012887963193108452, \n         'gamma': 6.576022976359221, \n         'max_depth': 8, \n         'min_child_weight': 8.876744371188476, \n         'subsample': 0.7813380253086911, \n        }","a3eee53f":"N_STARTS = 1\nN_SPLITS = 5\nLBS = 0.001\n\nfolds = create_folds(N_STARTS, N_SPLITS)\nprint(folds)","e26c6495":"ss0.loc[:, train_targets.columns] = 0\nss = ss0.copy()\n\nres0 = train_targets.copy()\nres0.loc[:, train_targets.columns] = 0\nres = res0.copy()\n\nfor nums, seed in enumerate(range(N_STARTS)):\n    start_time_seed = time()\n    mean_score = 0\n    param['random_state'] = seed\n    for n, foldno in enumerate(set(folds[nums])):\n        start_time_fold = time()\n        tr = folds[nums] != foldno\n        te = folds[nums] == foldno\n        \n        start_time_fold = time()\n        x_tr, x_val = train.values[tr][:, top_feats], train.values[te][:, top_feats]\n        y_tr, y_val = train_targets.values[tr], train_targets.values[te]\n        x_tt = test.values[:, top_feats]\n        \n        # Label Smoothing\n        y_tr = y_tr * (1 - LBS) + 0.5 * LBS\n            \n        val_predict = np.zeros(train_targets.shape)\n        \n        for tar in tqdm(range(train_targets.shape[1])):\n            \n            y_tr_tar, y_val_tar = y_tr[:, tar], y_val[:, tar]\n            \n            xgb_tr = xgb.DMatrix(x_tr, label = y_tr_tar, nthread = -1)\n            xgb_val = xgb.DMatrix(x_val, label = y_val_tar, nthread = -1)\n            xgb_tt = xgb.DMatrix(x_tt, nthread = -1)\n            \n            model = xgb.train(param, xgb_tr, 1000, [(xgb_val, 'eval')], early_stopping_rounds = 25, verbose_eval = 0)\n                        \n            test_predict = model.predict(xgb_tt, ntree_limit = model.best_ntree_limit)\n            val_predict[te, tar] = model.predict(xgb_val, ntree_limit = model.best_ntree_limit)\n            \n            ss0.loc[:, train_targets.columns[tar]] += test_predict \/ (N_SPLITS * N_STARTS)\n            res0.loc[te, train_targets.columns[tar]] += val_predict[te, tar] \/ N_STARTS\n            \n        fold_score = log_loss_metric(y_val, val_predict[te])\n        mean_score += fold_score \/ N_SPLITS\n        \n        print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] First Stage Seed {seed}, Fold {n}:', fold_score)\n        \n    print(f'[{str(datetime.timedelta(seconds = time() - start_time_seed))[0:7]}] First Stage Seed {seed} Mean Score:', mean_score)","d9c54590":"print(f'First Stage OOF Metric: {log_loss_metric(train_targets.values, res0.values)}')","19b163d0":"ss0.loc[test['cp_type'] == 1, train_targets.columns] = 0\nnp.save(f'GroupCV_First_Stage_oof.npy', res0[cols].values)\nnp.save(f'GroupCV_First_Stage_sub.npy', ss0[cols].values)\nss0.to_csv(f'submission_First_Stage_XGB.csv', index = False)","cebb8299":"ss0.loc[:, train_targets.columns] = 0\nss = ss0.copy()\n\nres0 = train_targets.copy()\nres0.loc[:, train_targets.columns] = 0\nres = res0.copy()\n\nres0.loc[:, train_targets.columns] = np.load('.\/GroupCV_First_Stage_oof.npy')\nss0.loc[:, train_targets.columns] = np.load('.\/GroupCV_First_Stage_sub.npy')","c4cb86af":"for nums, seed in enumerate(range(N_STARTS)):\n    start_time_seed = time()\n    mean_score = 0\n    param['random_state'] = seed\n    for n, foldno in enumerate(set(folds[nums])):\n        start_time_fold = time()\n        tr = folds[nums] != foldno\n        te = folds[nums] == foldno\n        x_tr, x_val = train.values[tr][:, top_feats], train.values[te][:, top_feats]\n        y_tr, y_val = train_targets.values[tr], train_targets.values[te]\n        x_tt = test.values[:, top_feats]\n        \n        # Label Smoothing\n        y_tr = y_tr * (1 - LBS) + 0.5 * LBS\n        \n        # Feature Aggregation\n        x_tr = np.concatenate([x_tr, res0.loc[tr, train_targets.columns].values], axis = 1)\n        x_val = np.concatenate([x_val, res0.loc[te, train_targets.columns].values], axis = 1)\n        x_tt = np.concatenate([x_tt, ss0.loc[:, train_targets.columns].values], axis = 1)\n            \n        val_predict = np.zeros(train_targets.shape)\n        \n        for tar in tqdm(range(train_targets.shape[1])):\n            \n            y_tr_tar, y_val_tar = y_tr[:, tar], y_val[:, tar]\n            \n            xgb_tr = xgb.DMatrix(x_tr, label = y_tr_tar, nthread = -1)\n            xgb_val = xgb.DMatrix(x_val, label = y_val_tar, nthread = -1)\n            xgb_tt = xgb.DMatrix(x_tt, nthread = -1)\n            \n            model = xgb.train(param, xgb_tr, 1000, [(xgb_val, 'eval')], early_stopping_rounds = 25, verbose_eval = 0)\n                        \n            test_predict = model.predict(xgb_tt, ntree_limit = model.best_ntree_limit)\n            val_predict[te, tar] = model.predict(xgb_val, ntree_limit = model.best_ntree_limit)\n            \n            ss.loc[:, train_targets.columns[tar]] += test_predict \/ (N_SPLITS * N_STARTS)\n            res.loc[te, train_targets.columns[tar]] += val_predict[te, tar] \/ N_STARTS\n            \n        fold_score = log_loss_metric(y_val, val_predict[te])\n        mean_score += fold_score \/ N_SPLITS\n        \n        print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] Second Stage Seed {seed}, Fold {n}:', fold_score)\n        \n    print(f'[{str(datetime.timedelta(seconds = time() - start_time_seed))[0:7]}] Second Stage Seed {seed} Mean Score:', mean_score)","fa9e17e9":"print(f'Second Stage OOF Metric: {log_loss_metric(train_targets.values, res.values)}')","a70cd55c":"ss.loc[test['cp_type'] == 1, train_targets.columns] = 0\nnp.save(f'GroupCV_Second_Stage_oof.npy', res[cols].values)\nnp.save(f'GroupCV_Second_Stage_sub.npy', ss[cols].values)\nss.to_csv(f'submission.csv', index = False)","8d08574a":"# First Stage","d5d91291":"# XGB","66269041":"# Self-Stacking GroupCV XGBoost\n\nIn this notebook, I introduce a self-stacking XGBoost (XGB) pipeline. \n\nXGB model does not support multi-label learning so it cannot fully learn the label correlation as well as neural networks (NNs). Then I come up with an idea of self-stacking learning for XGB. The purpose is to enhance the label correlation learning by taking the first-stage predictions as additional features for the second-stage learning.\n\n![image.png](attachment:image.png)","09348537":"# Data Preparation","aef91873":"# Group CV","91d9a960":"# Second Stage (Self-Stacking)"}}