{"cell_type":{"7e559bdf":"code","a5ea79ea":"code","58cc171d":"code","d2fb4d75":"code","af85e44c":"code","a7854ee6":"code","df052987":"code","6f699376":"code","b817ed30":"code","838613fd":"code","e785bded":"code","a0038f7e":"code","51d6c8a8":"code","b9db5e29":"code","f27f234a":"code","3f222124":"code","b25506cf":"code","0bc9d70a":"code","891fa5b4":"code","d8d60995":"code","8dc9cc59":"code","ee48ff62":"code","64f35d2a":"code","3f323bc9":"code","a9bfaa06":"code","7a17a94f":"code","f973a1ae":"code","0a99f224":"code","e0cee202":"code","328c85e1":"code","a096d76e":"markdown","5033e46c":"markdown","f695b862":"markdown","c24ffb9e":"markdown","b9a0ffdd":"markdown","619f8207":"markdown","9bf286d7":"markdown","a3592684":"markdown","05e32422":"markdown","f3d3332e":"markdown","8226b38c":"markdown","cbd060eb":"markdown","750b60d7":"markdown","997adca5":"markdown","24fec5b2":"markdown","281db905":"markdown","71c1379a":"markdown","77303ecf":"markdown","9357450e":"markdown","010c79a2":"markdown","67c7f5eb":"markdown","d0fab4f8":"markdown","67773685":"markdown","57f10a38":"markdown","5ab7d785":"markdown","af24fb87":"markdown","712a6608":"markdown","054af457":"markdown","6995da8a":"markdown","eb164a26":"markdown","91bc284e":"markdown","014e6d06":"markdown","288ac53c":"markdown","e657df7e":"markdown","3001da28":"markdown","a93e8f7c":"markdown","297b994f":"markdown","5fcc3878":"markdown","62c9f8ff":"markdown"},"source":{"7e559bdf":"from PIL import ImageTk, Image  \nimage1 = Image.open(\"..\/input\/fgsgsgfrfxg\/heart_the.png\")\nimage1","a5ea79ea":"import pandas as pd # Importing pandas for data manipulation and analysis\nimport numpy as np # Importing python linear algebra library to do work with arrays\nfrom sklearn.model_selection import train_test_split # to split the data into train and test sets\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pylab import *\nfrom sklearn.preprocessing import MinMaxScaler#Importing the MinMaxSclaer from sklearn.preprocessing\nfrom sklearn.preprocessing import StandardScaler#Importing the MinMaxSclaer from sklearn.preprocessing\nfrom sklearn.model_selection import GridSearchCV\nimport warnings # to import warnings\nwarnings.filterwarnings('ignore') # to import warnings as 'ignore'\nfrom sklearn.metrics import accuracy_score","58cc171d":"df=pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\") # to import csv file as data frame","d2fb4d75":"df","af85e44c":"#Checking For Null values in our datasets and then removing the same.\npd.DataFrame(df.isna().sum()) #This will give the snapshot if me have any null values in our dataset.","a7854ee6":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_value_df = pd.DataFrame({'column_name': df.columns,\n                                 'percent_missing': percent_missing})\nmissing_value_df.sort_values('percent_missing', inplace=True)\nmissing_value_df","df052987":"df.info()","6f699376":"df.describe()","b817ed30":"# visualizing numeric variables using seaborn\nsns.set(font_scale=1.5)\nsns.set_style(style='darkgrid')\nf, axes = plt.subplots(2,3,figsize=(25,25))\nsns.distplot( df[\"age\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2), color=\"#0000ff\", ax=axes[0, 0])\nsns.distplot( df[\"oldpeak\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2), color=\"#00cc00\", ax=axes[0, 1])\nsns.distplot( df[\"trestbps\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2), color=\"#e68a00\", ax=axes[0, 2])\nsns.distplot( df[\"chol\"] , hist_kws=dict(edgecolor=\"k\", linewidth=2),color=\"#992600\", ax=axes[1, 0])\nsns.distplot( df[\"thalach\"] ,hist_kws=dict(edgecolor=\"k\", linewidth=2), color=\"#e600ac\", ax=axes[1, 1])","838613fd":"import matplotlib as mpl\nmpl.rcParams['font.size'] =25\nf, axes = plt.subplots(3,3,figsize=(29,29))\ndf['sex'].value_counts().plot.pie(autopct='%1.1f%%', pctdistance=0.5,colors=['red','green'], shadow=True, counterclock=True, startangle=90, wedgeprops={'linewidth':2, 'linestyle': 'solid', 'antialiased': True,'edgecolor':\"k\"},ax=axes[0, 0])\ndf['cp'].value_counts().plot.pie(autopct='%1.1f%%',pctdistance=0.6,colors=['blue','orange','red','olive'], shadow=True, counterclock=False, startangle=90, wedgeprops={'edgecolor':\"k\",'linewidth': 2, 'linestyle': 'solid', 'antialiased': True},ax=axes[0, 1])\ndf['fbs'].value_counts().plot.pie(autopct='%1.1f%%', pctdistance=0.6,colors=['blue','skyblue'], shadow=True, counterclock=False, startangle=90, wedgeprops={'edgecolor':\"k\",'linewidth': 2, 'linestyle': 'solid', 'antialiased': True},ax=axes[0, 2])\ndf['restecg'].value_counts().plot.bar(color=['olive','skyblue','red'],title='restecg',ax=axes[1, 0])\ndf['exang'].value_counts().plot.pie(autopct='%1.1f%%', pctdistance=0.6,colors=['olive','orange'], shadow=True, counterclock=False, startangle=90, wedgeprops={'edgecolor':\"k\",'linewidth': 2, 'linestyle': 'solid', 'antialiased': True},ax=axes[1, 1])\ndf['slope'].value_counts().plot.pie(autopct='%1.1f%%', pctdistance=0.6,colors=['olive','orange','red'], shadow=True, counterclock=False, startangle=90, wedgeprops={'edgecolor':\"k\",'linewidth': 2, 'linestyle': 'solid', 'antialiased': True},ax=axes[1, 2])\ndf['ca'].value_counts().plot.bar(color=['olive','orange','red','blue','skyblue'],title='ca',ax=axes[2, 0])\ndf['thal'].value_counts().plot.bar(color=['olive','orange','red','blue','skyblue'],title='thal',ax=axes[2, 1])\ndf['target'].value_counts().plot.pie(autopct='%1.1f%%', pctdistance=0.7,colors=['olive','orange','red','blue','skyblue'], shadow=True, counterclock=False, startangle=180, wedgeprops={'edgecolor':\"k\",'linewidth': 2, 'linestyle': 'solid', 'antialiased': True},ax=axes[2, 2])","e785bded":"corrMatrix = round(df.corr(),1)\nplt.figure(figsize=(16,11))\nsns.heatmap(corrMatrix,annot=True,cmap='seismic',linewidths=2,linecolor='black')\nplt.title(\"Heatmap Correlation of Heart Failure Prediction\", fontsize = 23)\nplt.show()","a0038f7e":"X = df.drop('target', axis=1).values #Feature datasets for the purpose of calculation.\ny = df['target'].values #Target data sets for the purpose of calculations.\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=42)#Splitting the data into train and test sets.\npd.DataFrame(X_train) # to have a look at trained dataset","51d6c8a8":"#creating an object of Scaler\nscaler = StandardScaler()\n\n#Fitting the training features\nscaler.fit(X_train)\n\n#transforming the train features\nX_train_scaled = scaler.transform(X_train)\n\n#transforming the test features\nX_test_scaled = scaler.transform(X_test)","b9db5e29":"train_accuracies=[]\ntest_accuracies=[]","f27f234a":"# Part 1 -: SELECT THE BEST HYPERPARMETERS WITH HELP OF GRID SEARCH\n\nfrom sklearn.neighbors import KNeighborsClassifier\n#List Hyperparameters that we want to tune.\nHyper_parameters = {'n_neighbors' : list(range(1,56)),'leaf_size' : list(range(1,50))}\n# apply grid search to find best parameters for the model\nGridSearch_knc = GridSearchCV(estimator = KNeighborsClassifier(), \n                           param_grid = Hyper_parameters,\n                           cv = 10,\n                           n_jobs = -1)\nGridSearch_knc.fit(X_train_scaled, y_train)\n\nprint(\"Best hyperparameters for model:\"+str(GridSearch_knc.best_params_))\nprint(\"Best estimator for model:\"+str(GridSearch_knc.best_estimator_))","3f222124":"#Part 2 -: Build a model with the help of best parameters\nknc = KNeighborsClassifier(n_neighbors=16,leaf_size=1)\nknc.fit(X_train_scaled, y_train)\n# to predict the target values\npred_dt_test = knc.predict(X_test_scaled)\npred_dt_train = knc.predict(X_train_scaled)\ntrain_accuracy_knn=accuracy_score(y_train,pred_dt_train)*100\ntest_accuracy_knn=accuracy_score(y_test,pred_dt_test)*100\ntrain_accuracies.append(train_accuracy_knn)\ntest_accuracies.append(test_accuracy_knn)\n\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_dt_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_dt_test)*100) )","b25506cf":"# Part 1 -: SELECT THE BEST HYPERPARMETERS WITH HELP OF GRID SEARCH\nfrom sklearn.svm import SVC \n# defining parameter range \nHyper_parameters = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \nGridSearch_svc = GridSearchCV(estimator = SVC(),\n                               param_grid = Hyper_parameters,\n                               cv = 15,\n                               n_jobs = -1)\nGridSearch_svc.fit(X_train_scaled, y_train)\n\nprint(\"Best hyperparameters for model:\"+str(GridSearch_svc.best_params_))\nprint(\"Best estimator for model:\"+str(GridSearch_svc.best_estimator_))","0bc9d70a":"#Part 2 -: Build a model with the help of best parameters\nsvc = SVC(C=10, gamma=0.01,kernel='rbf',probability=True)\nsvc.fit(X_train_scaled, y_train)\n# to predict the target values\npred_svc_test = svc.predict(X_test_scaled)\npred_svc_train = svc.predict(X_train_scaled)\ntrain_accuracy_svc=accuracy_score(y_train,pred_svc_train)*100\ntest_accuracy_svc=accuracy_score(y_test,pred_svc_test)*100\ntrain_accuracies.append(train_accuracy_svc)\ntest_accuracies.append(test_accuracy_svc)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_svc_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_svc_test)*100) )","891fa5b4":"# Part 1 -: Hypertuning of parameters\nfrom sklearn.ensemble import RandomForestClassifier\n#The structure that Scikit-learn needs to run Grid search\nparam_grid={'max_depth':[3,4,5],\n           'max_leaf_nodes':[10,15,20],\n            'min_samples_leaf':[10,15,20,25]}\nfrom sklearn.model_selection import GridSearchCV\n#applying GridSearch on a Decisiontree classifier with a 3 different parameters:\ngrid_search = GridSearchCV(RandomForestClassifier(n_estimators=11,random_state=573),param_grid,cv=10,return_train_score=True)\ngrid_search.fit(X_train_scaled,y_train)\nprint(\"Best parameters:\"+str(grid_search.best_params_))\nprint(\"Best estimator:\"+str(grid_search.best_estimator_))","d8d60995":"#Part 2 -: Build a model with the help of best parameters\nrf = RandomForestClassifier(max_depth=5, max_leaf_nodes=15, min_samples_leaf=10,\n                       n_estimators=11, random_state=573)\nrf.fit(X_train_scaled, y_train)\n# to predict the target values\npred_rf_test = rf.predict(X_test_scaled)\npred_rf_train = rf.predict(X_train_scaled)\ntrain_accuracy_rf=accuracy_score(y_train,pred_rf_train)*100\ntest_accuracy_rf=accuracy_score(y_test,pred_rf_test)*100\ntrain_accuracies.append(train_accuracy_rf)\ntest_accuracies.append(test_accuracy_rf)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_rf_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_rf_test)*100) )","8dc9cc59":"# Part 1 -: Hypertuning of parameters\n#XGBoost\nfrom xgboost import XGBClassifier\nxg=XGBClassifier(random_state=573)\n\n#List Hyperparameters that we want to tune.\n\nparameter_grid_xg={'learning_rate':[0.05, 0.10, 0.15, 0.20],'max_depth':[3,4,5],'gamma':[ 0.0, 0.1, 0.2 , 0.3]}\ngridsearch_xg = GridSearchCV(xg, parameter_grid_xg,cv=10)\ngridsearch_xg.fit(X_train_scaled, y_train);\n\n#Get best hyperparameters\ngridsearch_xg.best_params_","ee48ff62":"#Part 2 -: Build a model with the help of best parameters\nxg =XGBClassifier(gamma=0.1,learning_rate=0.05,max_depth=3,random_state=573)\nxg.fit(X_train_scaled, y_train)\n# to predict the target values\npred_xg_test = xg.predict(X_test_scaled)\npred_xg_train = xg.predict(X_train_scaled)\ntrain_accuracy_xg=accuracy_score(y_train,pred_xg_train)*100\ntest_accuracy_xg=accuracy_score(y_test,pred_xg_test)*100\ntrain_accuracies.append(train_accuracy_xg)\ntest_accuracies.append(test_accuracy_xg)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_xg_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_xg_test)*100) )","64f35d2a":"# Part 1 -: Hypertuning of parameters\nfrom sklearn.neural_network import MLPClassifier\n\n# defining parameter range \nHyper_parameters = {'solver':['lbfgs'], 'alpha':[1e-1,1e-5,1e-10],\n                    'hidden_layer_sizes':[(5, 2)], 'random_state':[1,6]}\nGridSearch_mlp = GridSearchCV(estimator = MLPClassifier(),\n                               param_grid = Hyper_parameters,cv=10,n_jobs=-1)\nGridSearch_mlp.fit(X_train_scaled, y_train)\n\nprint(\"Best hyperparameters for model:\"+str(GridSearch_mlp.best_params_))\nprint(\"Best estimator for model:\"+str(GridSearch_mlp.best_estimator_))","3f323bc9":"#Part 2 -: Build a model with the help of best parameters\nmlp = MLPClassifier(alpha=0.1, hidden_layer_sizes=(5, 2), random_state=6,\n              solver='lbfgs')\nmlp.fit(X_train_scaled, y_train)\n# to find the accuracy of the model on training and testing data\n# to predict the target values\npred_mlp_test = mlp.predict(X_test_scaled)\npred_mlp_train = mlp.predict(X_train_scaled)\ntrain_accuracy_mlp=accuracy_score(y_train,pred_mlp_train)*100\ntest_accuracy_mlp=accuracy_score(y_test,pred_mlp_test)*100\ntrain_accuracies.append(train_accuracy_mlp)\ntest_accuracies.append(test_accuracy_mlp)\n# to find the accuracy of the model on training and testing data\nprint(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_mlp_train)*100) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_mlp_test)*100) )","a9bfaa06":"label = ['KNN','SVC','Random Forest','XG Boost','MLP']\nprint(label)\n\n#checking the train and test accuracies for all the parameter values\ntrain_accurac = [round(num, 2) for num in train_accuracies]\nprint(\"Train Accuracies \"+str(train_accurac))\n\ntest_accurac = [round(num, 2) for num in test_accuracies]\nprint(\"\\nTest Accuracies \"+str(test_accurac))","7a17a94f":"#Accuracy dataframe\nAcc_df = pd.DataFrame({'Model':label,'Train Accuracy(%)': train_accurac,'Test Accuracy(%)': test_accurac})\nAcc_df.style.background_gradient(cmap='Reds')","f973a1ae":"# to plot roc curve for this model\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# generate a no skill prediction (majority class)\nns_probs = [0 for _ in range(len(y_test))]\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = xg.predict_proba(X_test_scaled)[:,1]\n# Compute and print AUC score\nauc_xg=roc_auc_score(y_test, y_pred_prob)*100\nprint(\" ----AUC:----- {}\".format(roc_auc_score(y_test, y_pred_prob)*100))\n#Plot ROC Curve\nfpr_dt, tpr_dt, _ = roc_curve(y_test,y_pred_prob)# to plot random chances\nns_fpr,ns_tpr, _ = roc_curve(y_test, ns_probs)# to plot roc curve for the model\n# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Chances',color='Blue') # plot the random chances for the model\nplt.plot(fpr_dt, tpr_dt, marker='.', label='ROC Curve',color='Red')# plot the roc curve for the model\n# axis labels\nplt.xlabel('False Positive Rate',fontsize=20)\nplt.ylabel('True Positive Rate',fontsize=20)\nplt.title('Receiver Operating Characteristics',fontsize=20,color='green')\nplt.legend()\n# show the legend\n# show the grid\nplt.grid()\n# show the plot\nprint('**************************************************************************************************')","0a99f224":"# to plot roc curve for this model\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# generate a no skill prediction (majority class)\nns_probs = [0 for _ in range(len(y_test))]\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = mlp.predict_proba(X_test_scaled)[:,1]\n# Compute and print AUC score\nauc_mlp=roc_auc_score(y_test, y_pred_prob)*100\nprint(\" ----AUC:----- {}\".format(roc_auc_score(y_test, y_pred_prob)))\n#Plot ROC Curve\nfpr_dt, tpr_dt, _ = roc_curve(y_test,y_pred_prob)# to plot random chances\nns_fpr,ns_tpr, _ = roc_curve(y_test, ns_probs)# to plot roc curve for the model\n# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Chances',color='Blue') # plot the random chances for the model\nplt.plot(fpr_dt, tpr_dt, marker='.', label='ROC Curve',color='Red')# plot the roc curve for the model\n# axis labels\nplt.xlabel('False Positive Rate',fontsize=20)\nplt.ylabel('True Positive Rate',fontsize=20)\nplt.title('Receiver Operating Characteristics',fontsize=20,color='green')\nplt.legend()\n# show the legend\n# show the grid\nplt.grid()\n# show the plot\nprint('**************************************************************************************************')","e0cee202":"label = ['XG Boost','MLP']\ntrain_acc = [round(train_accuracy_xg,2),round(train_accuracy_mlp,2)]\n\ntest_acc = [round(test_accuracy_xg,2),round(test_accuracy_mlp,2)]\nauc_values=[round(auc_xg,2),round(auc_mlp,2)]","328c85e1":"x = np.arange(len(label))  \nwidth = 0.1\n# Figure Size \nfig, ax = plt.subplots(figsize=(15, 11))\n\n# creating the bar plot\nbar1 = ax.bar(x - width\/2, train_acc, width, label='Train',color='olive')\nbar2 = ax.bar(x + width\/5, test_acc, width, label='Test',color='orange')\nbar3 = ax.bar(x + width, auc_values, width, label='AUC Score',color='red')\n\n\n# put labels\nax.set_ylabel('Score',fontsize=10,fontweight='bold')\nax.set_xlabel('Models',fontsize=10,fontweight='bold')\nax.set_title('Comparison of models',fontsize=21,fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(label)\nax.set_ylim([0, 110])\nax.legend(loc='center right', bbox_to_anchor=(1.19, 0.9))\n\n# to annotate\ndef autolabel(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3),\n                    textcoords=\"offset points\",fontsize=10,fontweight='bold')\nautolabel(bar1)\nautolabel(bar2)\nautolabel(bar3)\nplt.grid(b = True, color ='black', linestyle ='-', linewidth = 0.7, alpha = 1.0)\n\nplt.show()","a096d76e":"### `Analysis of the output` -:Here, I will consider 'MLP' and 'XGBoost' as my top 2 models as there train and test accuracies are having good percentage values. I am not taking random forest in top 2 models because it seems like random forest model is overfitting the data as its value of test accuracy is more than train accuracy, which means that this model is outperforming for unseen data.","5033e46c":"### v) Multilayer Perceptron <a id=\"7.5\"><\/a>","f695b862":"### `Analysis of the output` -: As we can see from the above output that the dataset has no missing value, which is obviously a good thing.","c24ffb9e":"### `Analysis of the output` -:According to the output it seems like MLP is the best model as its train and test accuracies are way more better than XG boost. Moreover, its AUC value is 85.25% which means MLP model is 85% capable of predicting 1 as 1 and 0 as 0.<br> However, the dataset is small in size, therefore, the model might not met with today's industrial standards.","b9a0ffdd":"### i) Datatype of each feature <a id=\"5.1\"><\/a>","619f8207":"### ii) MLP <a id=\"7.2\"><\/a>","9bf286d7":"### i) Splitting the training and testing sets <a id=\"6.1\"><\/a>","a3592684":"### ii) Its Attributes <a id=\"2\"><\/a><a id=\"2.2\"><\/a>\n  -   age -: age in years\n  -   sex -: (1 = male; 0 = female)\n  -   cp  -: chest pain type (4 values)\n      - Value 0: typical angina\n      - Value 1: atypical angina\n      - Value 2: non-anginal pain\n      - Value 3: asymptomatic\n  -   trestbps -: resting blood pressure\n  -   chol -:serum cholestoral in mg\/dl\n  -   fbs -: fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n  -   restecg -: resting electrocardiographic results (values 0,1,2)\n      - Value 0: normal\n      - Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n      - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n  -   thalach -: maximum heart rate achieved\n  -   exang -: exercise induced angina (1 = yes; 0 = no)\n  -   oldpeak -: ST depression induced by exercise relative to rest\n  -   slope -: the slope of the peak exercise ST segment\n      - Value 0: upsloping\n      - Value 1: flat\n      - Value 2: downsloping\n  -   ca -: number of major vessels (0-3) colored by flourosopy\n  -   thal -: 3 = normal; 6 = fixed defect; 7 = reversable defect\n  -   target -: 0 = No heart disease, 1 = Heart disease is present","05e32422":"#  <font color='#660000'>Models <a id=\"7\"><\/a>","f3d3332e":"#  <font color='#660000'>Data Preprocessing <a id=\"4\"><\/a>","8226b38c":"### i) KNN Classifier <a id=\"7.1\"><\/a>","cbd060eb":"#  <font color='#660000'>Dataset <a id=\"2\"><\/a>","750b60d7":"#  <font color='#660000'>Heart Failure Analysis","997adca5":"### iii) Calculating percentage of missing values in the dataset <a id=\"4.3\"><\/a>","24fec5b2":"### iv) XG Boost <a id=\"7.4\"><\/a>","281db905":"The dataset is taken from UCI Machine Learning Repository.The link to the dataset is given below:<br>\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease","71c1379a":"### iv) Univariant Analysis of Categorical features <a id=\"5.3\"><\/a>","77303ecf":"### i) Importing Libraries <a id=\"2\"><\/a><a id=\"3.1\"><\/a>","9357450e":"#  <font color='#660000'>Exploratory Data Analysis <a id=\"5\"><\/a>","010c79a2":"### i) XG Boost<a id=\"7.1\"><\/a>","67c7f5eb":"### `Analysis of the output` -: a) sex -: According the pie chart, the proportion of male patients are nearly twice than female patients, which indicates that heart disease is more common in man than in woman.<br><br>b) cp -: The most common chest pain type in patients comes out to be typical angina which denoted by value 0.<br><br>c)fbs -:In most of the patients fasting blood sugar is less than 120 mg\/dl which is depicted by the highest percentage of 0 value in pie chart. <br><br>d)restecg-:The highest number of patients have ST-T wave abnormality. However, according to the bar chart, still many of the patients have normal ecg results.<br><br>e)exang -: In most of the patients execise induced angina is not present.<br><br>f) slope -:In large number of patients, the slope of the peak exercise ST segment is either flat or downsloping.<br><br>g) ca -:Majority of patients have zero number of major vessels (0-3) colored by flourosopy.<br><br>h) thal-: Most of the heart disease patients are having thal stress value as 2, which indicates the fixed defect.<br><br>i) target -: There are higher chances, that patients who are admitted to hospital have heart disease.","d0fab4f8":"#  <font color='#660000'>Compare MLP and XG Boost <a id=\"9\"><\/a>","67773685":"### Multivariant Analysis of all the features with help of correlation matrix <a id=\"5.4\"><\/a>","57f10a38":"### ii) Support Vector Machine <a id=\"7.2\"><\/a>","5ab7d785":"#  <font color='#660000'>Contents\n\n* [Indroduction to dataset](#2)\n* [Importing Libraries and dataset](#3)\n* [Data Preprocessing](#4)\n* [Exploratory Data Analysis](#5)\n* [Splitting and Scaling the data](#6)\n* [Models](#7)\n    * [KNN](#7.1)\n    * [SVM](#7.2)\n    * [Random Forest](#7.3)\n    * [XG Boost](#7.4)\n    * [Multilayer Perceptron](#7.5)\n    * [compare the above models](#7.5)\n* [Calculate AUC and plot ROC curve for two best models](#8)\n* [Compare top 2 models](#9)","af24fb87":"### ii) Scaling the data <a id=\"6.2\"><\/a>","712a6608":"### iii) Univariant Analysis of Numerical feature <a id=\"5.2\"><\/a>","054af457":"### `Analysis of the output` -: a) Age -: According to age histogram,the majority of the patients belonged to the age range between 50-65, and very less num of patients are under 35 years and even above 70 years.<br><br>b)Oldpeak -:As per the definition, the normal ST depression in ECG curve occurs during physical exercise and depression appears in the curve is generally less than 1mm, which will get fixed rapidly after the exercise is stopped. However,as per the histogram of old peak, we can see that the highest ST depression induced by exercise relative to rest shows highest frequency when it is below 1 mm, which means that most of the ST depression which appeared in patients ECG curve is normal,which get vanished as soon as patients stop doing physical exercise.<br><br>c) trestbps -: The highest value of resting blood pressure of patients who had been admitted to hospital is within the range of 120 mm Hg to 140 mm Hg. However, 120 mm Hg is considered as the normal blood pressure in humans.<br><br>d) chol -:The large number of patients are having cholestrol in between the range of 200 mg\/dl to 300 mg\/dl.We consider cholestrol level less than 200 mg\/dl as normal. Therefore, according to the histogram, we can say most of the patients are having \"borderline high\" cholestrol level which raises the risk of heart disease.<br><br>e) thalach -: The maximum heart rate achieved by patients is in between 130 to 175. Generally,a normal resting heart rate of adults is within the range of 60 to 100 beats per minute.","6995da8a":"### ii) Checking missing values in the dataset <a id=\"4.2\"><\/a>","eb164a26":"### iii) Random Forest <a id=\"7.3\"><\/a>","91bc284e":"### i) Source of dataset <a id=\"2\"><\/a><a id=\"2.1\"><\/a>","014e6d06":"### `Analysis of the output` -:- As per the heat map,the features that are extremely negatievly correlated are age & thalach,exang & cp, thalach & oldpeak, exang & thalach, exang & target, exang & cp, slop & oldpeak, oldpeak & target, ca & target.","288ac53c":"#  <font color='#660000'>Calculate AUC values for selected model and Plot their ROC curve <a id=\"8\"><\/a>","e657df7e":"### i) Reading the dataset <a id=\"4.1\"><\/a>","3001da28":"### ii) Statistical description of each feature <a id=\"5.2\"><\/a>","a93e8f7c":"### ii) Importing Dataset <a id=\"2\"><\/a><a id=\"3.2\"><\/a>","297b994f":"#  <font color='#660000'>Splitting and Scaling <a id=\"6\"><\/a>","5fcc3878":"#  <font color='#660000'>Importing <a id=\"3\"><\/a>","62c9f8ff":"### vi) Comparing the test and train accuracies of each model  <a id=\"7.6\"><\/a>"}}