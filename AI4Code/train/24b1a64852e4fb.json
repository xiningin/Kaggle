{"cell_type":{"177ada97":"code","8d17880e":"code","551d3732":"code","504fe8b3":"code","8e5ca0bc":"code","66c47536":"code","ed067fba":"code","8038aa2f":"code","83a81850":"code","b8ad9f1c":"code","2849e792":"code","8191e98c":"code","7e6ba5b6":"code","7efd5810":"code","71fe8c20":"code","74bf6d3b":"code","541f6ee7":"code","1cde336b":"code","0959ecab":"code","b3fbae95":"code","910a1fa6":"code","548d03c8":"code","44d65717":"code","5e103f13":"code","30c6c9b9":"code","557cf585":"code","11f6b553":"code","7b06939e":"code","6680098b":"code","ed3a53ce":"code","931df8d4":"code","4a375da6":"code","32965d12":"code","4e0d6f46":"code","16e12902":"code","85967b48":"code","2f0bdd17":"code","7e46271a":"markdown","cd20b1c9":"markdown","7bfd90af":"markdown","c64cba06":"markdown","39891ce9":"markdown","8e948a61":"markdown","c7aa01b3":"markdown","86c58f32":"markdown","202da3e2":"markdown","584e13f1":"markdown","ec8484a5":"markdown","3b7cdc40":"markdown"},"source":{"177ada97":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom datetime import datetime\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.float_format', lambda x: '%.3f' % x)","8d17880e":"train_data = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest_data = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nsample_submission = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","551d3732":"train_data.head()","504fe8b3":"train_data.info()","8e5ca0bc":"test_data.info()","66c47536":"cat_cols = [col for col in train_data.select_dtypes(include = ['object'])]\nfor col in train_data[cat_cols]:\n    print(f\"{col}-{train_data[col].nunique()}\")","ed067fba":"cat_cols = [col for col in test_data.select_dtypes(include = ['object'])]\nfor col in test_data[cat_cols]:\n    print(f\"{col}-{test_data[col].nunique()}\")","8038aa2f":"num_cols = [col for col in train_data.select_dtypes(include = ['int64', 'float64'])]#\nfor col in train_data[num_cols]:\n    plt.figure(figsize=(10,4))\n    sns.kdeplot(data = train_data[col])\n    plt.title(f\"distribution of {col} column in the train set\")","83a81850":"num_cols = [col for col in test_data.select_dtypes(include = ['int64', 'float64'])]#\nfor col in test_data[num_cols]:\n    plt.figure(figsize=(10,4))\n    sns.kdeplot(data = test_data[col], color = 'orange')\n    plt.title(f\"distribution of {col} column in the test set\")","b8ad9f1c":"train_data['duration_seconds'].describe()","2849e792":"test_data['duration_seconds'].describe()","8191e98c":"# plot\ntrain_data['categoryId'].value_counts(normalize = True).sort_values().plot(kind='bar', figsize=(10,4), rot=0, color = 'orange')\n\nplt.xlabel(\"categoryId\", labelpad=10, fontsize=20)\nplt.ylabel(\"Percent of data\", labelpad=10, fontsize=20)\nplt.xticks(size = 15)\nplt.yticks(size = 15)\nplt.grid()\nplt.title(\"CategoryId in the train set\", y=1.02, fontsize=20)","7e6ba5b6":"# plot\ntest_data['categoryId'].value_counts(normalize = True).sort_values().plot(kind='bar', figsize=(10,4), rot=0, color = 'turquoise')\n\nplt.xlabel(\"categoryId\", labelpad=10, fontsize=20)\nplt.ylabel(\"Percent of data\", labelpad=10, fontsize=20)\nplt.xticks(size = 15)\nplt.yticks(size = 15)\nplt.grid()\nplt.title(\"CategoryId in the test set\", y=1.02, fontsize=20)","7efd5810":"plt.figure(figsize = (15,6))\nsns.boxplot(x = train_data['categoryId'], y= train_data['target'])\nplt.xlabel(\"categoryId\", labelpad=10, fontsize=20)\nplt.ylabel(\"like\/view_count ratio\", labelpad=10, fontsize=20)\nplt.xticks(size = 15)\nplt.yticks(size = 15)\nplt.grid()\nplt.title(\"categoryId wise target stats\", y=1.02, fontsize=20)","71fe8c20":"train_data.groupby('categoryId')['target'].describe()","74bf6d3b":"train_data['publishedAt'] = pd.to_datetime(train_data['publishedAt'])\ntest_data['publishedAt'] = pd.to_datetime(test_data['publishedAt'])\n\ntrain_data['trending_date'] = pd.to_datetime(train_data['trending_date'])\ntest_data['trending_date'] = pd.to_datetime(test_data['trending_date'])","541f6ee7":"train_data['publishedAt'].describe(datetime_is_numeric=True)","1cde336b":"test_data['publishedAt'].describe(datetime_is_numeric=True)","0959ecab":"train_data['trending_date'].describe(datetime_is_numeric=True)","b3fbae95":"test_data['trending_date'].describe(datetime_is_numeric=True)","910a1fa6":"train_data['description'][0]","548d03c8":"train_data['description'][0]","44d65717":"train_data['description'][21]","5e103f13":"train_data['tags'][0]","30c6c9b9":"train_data['title'][1]","557cf585":"test_data.head()","11f6b553":"X = train_data.drop(columns = {'target'})\ny = train_data['target']\n\nprint(X.shape, y.shape)","7b06939e":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=1)\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)","6680098b":"CAT_COLS = ['video_id', 'channelId', 'channelTitle', 'categoryId', 'comments_disabled', 'ratings_disabled', \\\n            'published_day', 'published_month', 'published_hour', 'published_dayofweek', 'trending_day', \\\n            'trending_month', 'trending_dayofweek']\n\nFEATURE_COLS = ['video_id', 'channelId', 'channelTitle', 'categoryId', 'comments_disabled', 'ratings_disabled', \\\n            'published_day', 'published_month', 'published_hour', 'published_dayofweek', 'trending_day', \\\n            'trending_month', 'trending_dayofweek', 'duration_seconds', 'id']\n\n\n# very basic feature engineering without using the tags\/ description column\ndef feature_transform(df: pd.DataFrame):\n    \n    # date feature engineering\n    df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc = True)\n    df['published_day'] = df['publishedAt'].dt.day\n    df['published_month'] = df['publishedAt'].dt.month\n    df['published_hour'] = df['publishedAt'].dt.hour\n    df['published_dayofweek'] = df['publishedAt'].dt.dayofweek\n    \n    df['trending_date'] = pd.to_datetime(df['trending_date'], utc = True)\n    df['trending_day'] = df['trending_date'].dt.day\n    df['trending_month'] = df['trending_date'].dt.month\n    df['trending_dayofweek'] = df['trending_date'].dt.dayofweek\n    \n    \n    # duration_seconds\n    \n    df['duration_seconds'] = df['duration_seconds'].fillna(df.groupby('categoryId')['duration_seconds'].transform('mean'))\n    \n    \n    # categorical_transform():\n    for col in CAT_COLS:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n        \n        \n    return df\n    ","ed3a53ce":"train = feature_transform(X_train)\nX_train = train[FEATURE_COLS].set_index('id')\n\nval = feature_transform(X_val)\nX_val = val[FEATURE_COLS].set_index('id')","931df8d4":"# transform for using all data for training at the end\n\nX_train_all = feature_transform(X)\nX_train_all = X_train_all[FEATURE_COLS].set_index('id')\n\ny_train_all = y\n\nprint(X_train_all.shape, y_train_all.shape)","4a375da6":"test = feature_transform(test_data)\ntest_df = test[FEATURE_COLS].set_index('id')","32965d12":"test_df.info()","4e0d6f46":"# prepare a list of base models\ndef get_models():\n    models = list()\n    models.append(('rfr', RandomForestRegressor()))\n    models.append(('xgb', xgb.XGBRegressor()))\n    models.append(('lgbm', LGBMRegressor()))\n    return models\n\n\n# evaluate each base model\ndef evaluate_models(models, X_train, X_val, y_train, y_val):\n    # fit and evaluate the models\n    scores = list()\n    for name, model in models:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n        scores.append(-mae)\n    return scores\n\n\nmodels = get_models()\nscores = evaluate_models(models, X_train, X_val, y_train, y_val)\nprint(scores)\nfor i in range(len(models)):\n    print('%s: %.3f' % (models[i][0], scores[i]))","16e12902":"ensemble = VotingRegressor(estimators=models)\nensemble.fit(X_train_all, y_train_all)\ntest_df['target'] = ensemble.predict(test_df)","85967b48":"submission = test_df['target'].reset_index()\nsubmission","2f0bdd17":"submission.to_csv('submission.csv',index=False)","7e46271a":"#### Inspecting the CategoryId column:\n\nUpon checking the categoryId column from the test and train data sets below, we found that the categories 15 and 29 are not present in the test data set. categoryId 23 has the highest ratio of likes\/view count(our target)","cd20b1c9":"#### Inspect some of the comments columns like, description, tags etc.\n\nSome descriptions contain the #hashtag, and some donot have that.","7bfd90af":"### Feature Engineering:","c64cba06":"The duration_seconds does look interesting here, because the maximum value in the train data set is 485620 seconds much more than the test data's max duration. However, the 50th percentile(median) is in the same range. ","39891ce9":"#### Inspecting Date Columns:\n\nWe have two date columns that says the published date and trending date of a video. We can derive some features from these dates in further steps. The published videos in the train set are between 2020-08-03 to 2021-11-29. The published dates in the test data are between 2021-11-15 to 2021-12-29","8e948a61":"#### Check the distriution of numeric columns in test set:\nLooks like we don't have few columns present in the test set, like view_count, likes, dislikes, and comment count. Our target variable is the **ratio of like to view count**, so obiviously these two coloumns will not be there in the test set. \n","c7aa01b3":"#### Making few baseline models with minimal feature engineering:\n\nUsing the Voting regressor technique from this article from machinelearningmastery https:\/\/machinelearningmastery.com\/weighted-average-ensemble-with-python\/ . Please go through it. ","86c58f32":"### Next steps: robust feature engineering and hyperparameter tuning","202da3e2":"##### Feature Engineering: \n\nHere I am just using the datetime columns to derive few basic features to make baseline model.","584e13f1":"### Exploratory Analysis:","ec8484a5":"### **About the analysis \ud83d\udcca:**\n\nAs mentioned in the competition description:\n\nThis is the first Pog Champs challenge, where members of the kaggle and twitch community combine to create awesome models and share amazing notebooks!!\n\nThe task of this competition is to predict the like to view_count ratio of youtube videos based on the title, description, thumbnail and additional metadata. \n\nWork in Progress- This is a super late EDA as the competition is nearing its end and here I am first trying to know the data better and intend to build some interesting ML models in the future and gain tons of knowledge on the go. ","3b7cdc40":"#### Check the distriution of numeric columns in train set:\n\nThe categoryId is a categorical feature instead of a continuous one. "}}