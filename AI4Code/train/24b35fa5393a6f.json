{"cell_type":{"ccdb0dc9":"code","ac8db458":"code","1012f4ee":"code","bce738fe":"code","7bd8f90b":"code","02a0054b":"code","c74ce5da":"code","36780991":"code","a87a7a32":"markdown","47dd52f7":"markdown","b97c3787":"markdown","d82f9cce":"markdown","38c7dbe8":"markdown","0c6c83f2":"markdown","f741b6ff":"markdown","62d3874b":"markdown","c3b8edd6":"markdown","c177586e":"markdown","01343868":"markdown","1781769f":"markdown","c26df9e2":"markdown","c4344fec":"markdown","6e643564":"markdown","aba59017":"markdown"},"source":{"ccdb0dc9":"# display some images for every different expression\n\nimport numpy as np\nimport seaborn as sns\nfrom keras.preprocessing.image import load_img, img_to_array\nimport matplotlib.pyplot as plt\nimport os\n\n# size of the image: 48*48 pixels\npic_size = 48\n\n# input path for the images\nbase_path = \"..\/input\/images\/images\/\"\n\nplt.figure(0, figsize=(12,20))\ncpt = 0\n\nfor expression in os.listdir(base_path + \"train\/\"):\n    for i in range(1,6):\n        cpt = cpt + 1\n        plt.subplot(7,5,cpt)\n        img = load_img(base_path + \"train\/\" + expression + \"\/\" +os.listdir(base_path + \"train\/\" + expression)[i], target_size=(pic_size, pic_size))\n        plt.imshow(img, cmap=\"gray\")\n\nplt.tight_layout()\nplt.show()","ac8db458":"# count number of train images for each expression\n\nfor expression in os.listdir(base_path + \"train\"):\n    print(str(len(os.listdir(base_path + \"train\/\" + expression))) + \" \" + expression + \" images\")","1012f4ee":"from keras.preprocessing.image import ImageDataGenerator\n\n# number of images to feed into the NN for every batch\nbatch_size = 128\n\ndatagen_train = ImageDataGenerator()\ndatagen_validation = ImageDataGenerator()\n\ntrain_generator = datagen_train.flow_from_directory(base_path + \"train\",\n                                                    target_size=(pic_size,pic_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True)\n\nvalidation_generator = datagen_validation.flow_from_directory(base_path + \"validation\",\n                                                    target_size=(pic_size,pic_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False)","bce738fe":"from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\n\n# number of possible label values\nnb_classes = 7\n\n# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(nb_classes, activation='softmax'))\n\nopt = Adam(lr=0.0001)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","7bd8f90b":"%%time\n\n# number of epochs to train the NN\nepochs = 50\n\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nhistory = model.fit_generator(generator=train_generator,\n                                steps_per_epoch=train_generator.n\/\/train_generator.batch_size,\n                                epochs=epochs,\n                                validation_data = validation_generator,\n                                validation_steps = validation_generator.n\/\/validation_generator.batch_size,\n                                callbacks=callbacks_list\n                                )","02a0054b":"# serialize model structure to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","c74ce5da":"# plot the evolution of Loss and Acuracy on the train and validation sets\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,10))\nplt.subplot(1, 2, 1)\nplt.suptitle('Optimizer : Adam', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(history.history['acc'], label='Training Accuracy')\nplt.plot(history.history['val_acc'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.show()","36780991":"# show the confusion matrix of our predictions\n\n# compute predictions\npredictions = model.predict_generator(generator=validation_generator)\ny_pred = [np.argmax(probas) for probas in predictions]\ny_test = validation_generator.classes\nclass_names = validation_generator.class_indices.keys()\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n# compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\nplt.show()","a87a7a32":"We got outputs at each step of the training phase. All those outputs were saved into the 'history' variable. We can use it to plot the evolution of the loss and accuracy on both the train and validation datasets:","47dd52f7":"# Train the model","b97c3787":"Everything is set up, let's train our model now!","d82f9cce":"Can you guess which images are related to which expressions? \n\nThis task is quite easy for a human, but it may be a bit challenging for a predictive algorithm because:\n- the images have a low resolution\n- the faces are not in the same position\n- some images have text written on them\n- some people hide part of their faces with their hands\n\nHowever all this diversity of images will contribute to make a more generalizable model.","38c7dbe8":"# Analyze the results","0c6c83f2":"# Setup our Convolutional Neural Network (CNN)\n\nWe chose to use a Convolutional Neural Network in order to tackle this face recognition problem. Indeed this type of Neural Network (NN) is good for extracting the features of images and is widely used for image analysis subjects like image classification.\n\n**Quick reminder of what a NN is:**\n\nA Neural Network is a learning framework that consists in multiple layers of artificial neurons (nodes). Each node gets weighted input data, passes it into an activation function and outputs the result of the function:\n\n![](https:\/\/skymind.ai\/images\/wiki\/perceptron_node.png)\n\nA NN is composed of several layers of nodes:\n\n![](https:\/\/www.researchgate.net\/profile\/Martin_Musiol\/publication\/308414212\/figure\/fig1\/AS:409040078295040@1474534162122\/A-general-model-of-a-deep-neural-network-It-consists-of-an-input-layer-some-here-two.png)\n\n- An input layer that will get the data. The size of the input layer depends on the size of the input data.\n- Some hidden layers that will allow the NN to learn complex interactions within the data. A Neural Network with a lot of hidden layers is called a Deep Neural Network.\n- An output layer that will give the final result, for instance a class prediction. The size of this layer depends on the type of output we want to produce (e.g. how many classes do we want to predict?)\n\nClassic NNs are usually composed of several fully connected layers. This means that every neuron of one layer is connected to every neurons of the next layer. \n\nConvolutional Neural Networks also have Convolutional layers that apply sliding functions to group of pixels that are next to each other. Therefore those structures have a better understanding of patterns that we can observe in images. We will explain this in more details after.\n\nNow let's define the architecture of our CNN:","f741b6ff":"We define our CNN with the following global architecture:\n- 4 convolutional layers\n- 2 fully connected layers\n\nThe convolutional layers will extract relevant features from the images and the fully connected layers will focus on using these features to classify well our images. This architecture was inspired by the following work on the subject: https:\/\/github.com\/jrishabh96\/Facial-Expression-Recognition\n\nLet's focus on how our convolution layers work. Each of them contain the following operations:\n- A convolution operator: extracts features from the input image using sliding matrices to preserve the spatial relations between the pixels. The following image summarizes how it works:\n![](http:\/\/deeplearning.stanford.edu\/wiki\/images\/6\/6c\/Convolution_schematic.gif)\n\nThe green matrix corresponds to the raw image values. The orange sliding matrix is called a 'filter' or 'kernel'. This filter slides over the image by one pixel at each step (stride). During each step, we multiply the filter with the corresponding elements of the base matrix. There are different types of filters and each one will be able to retrieve different image features:\n![](https:\/\/ujwlkarn.files.wordpress.com\/2016\/08\/screen-shot-2016-08-05-at-11-03-00-pm.png?w=342&h=562)\n\n- We apply the ReLU function to introduce non linearity in our CNN. Other functions like tanh or sigmoid could also be used, but ReLU has been found to perform better in most situations.\n- Pooling is used to reduce the dimensionality of each features while retaining the most important information. Like for the convolutional step, we apply a sliding function on our data. Different functions can be applied: max, sum, mean... The max function usually performs better.\n![](http:\/\/cs231n.github.io\/assets\/cnn\/maxpool.jpeg)\n\nWe also use some common techniques for each layer:\n- Batch normalization: improves the performance and stability of NNs by providing inputs with zero mean and unit variance.\n- Dropout: reduces overfitting by randomly not updating the weights of some nodes. This helps prevent the NN from relying on one node in the layer too much.\n\nWe chose softmax as our last activation function as it is commonly used for multi-label classification.\n\nNow that our CNN is defined, we can compile it with a few more parameters. We chose the Adam optimizer as it is one of the most computationally effective. We chose the categorical cross-entropy as our loss function as it is quite relevant for classification tasks. Our metric will be the accuracy, which is also quite informative for classification tasks on balanced datasets.","62d3874b":"The image expressions in our training dataset are pretty balanced, except for the 'disgust' category.","c3b8edd6":"The validation accuracy starts to stabilize at the end of the 50 epochs between 60% and 65% accuracy.\n\nThe training loss is slightly higher than the validation loss for the first epochs which can be surprising. Indeed we are more used to see higher validation losses than training losses in machine learning. Here this is simply due to the presence of dropout, which is only applied during the training phase and not during the validation phase.\n\nWe can see that the training loss is becoming much smaller than the validation loss after the 20th epochs. This means that our model starts to overfit our training dataset after too much iterations. That is why the validation loss does not decrease a lot after. One solution consists in early-stopping the training of the model. \n\nWe could also use some different dropout values and performing data augmentation. Those methods were tested on this dataset, but they did not significantly increase the validation accuracy although they reduced the overfitting effect. Using them slightly increased the training duration of the model.\n\nFinally we can plot the confusion matrix in order to see how our model classified the images:","c177586e":"Our best model managed to obtain a validation accuracy of approximately 65%, which is quite good given the fact that our target class has 7 possible values!\n\nAt each epoch, Keras checks if our model performed better than during the previous epochs. If it is the case, the new best model weights are saved into a file. This will allow us to load directly the weights of our model without having to re-train it if we want to use it.\n\nWe also have to save the structure of our CNN (layers etc.) into a file:","01343868":"# Quick data visualization","1781769f":"# Introduction\n\nThe data comes from the past Kaggle challenge \"Challenges in Representation Learning: Facial Expression Recognition Challenge\":\n\nhttps:\/\/www.kaggle.com\/c\/challenges-in-representation-learning-facial-expression-recognition-challenge\n\nThe data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. Each image corresponds to a facial expression in one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The dataset contains approximately 36K images.\n\nThe original data consisted in arrays with a greyscale value for each pixel. We converted this data into raw images and splitted them in multiple folders: \n\nimages\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;train\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;angry\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disgust\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fear\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;happy\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;neutral\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sad\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;surprise\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;validation\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;angry\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disgust\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fear\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;happy\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;neutral\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sad\/<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;surprise\/<br>\n\n80% of our images are contained inside the train folder, and the last 20% are inside the validation folder.","c26df9e2":"First let's see how our images look like:","c4344fec":"# Setup the data generators","6e643564":"Deep learning models are trained by being fed with batches of data. Keras has a very useful class to automatically feed data from a directory: ImageDataGenerator. \n\nIt can also perform data augmentation while getting the images (randomly rotating the image, zooming, etc.). This method is often used as a way to artificially get more data when the dataset has a small size.\n\nThe function flow_from_directory() specifies how the generator should import the images (path, image size, colors, etc.).\n","aba59017":"Our model is very good for predicting happy and surprised faces. However it predicts quite poorly feared faces because it confuses them with sad faces.\n\nWith more research and more resources this model could certainly be improved, but the goal of this study was primarily to focus on obtaining a fairly good model compared to what has been done in this field. \n\nNow it's time to try our model in a real situation! We will use flask to serve our model in order to perform real-time predictions with a webcam input."}}