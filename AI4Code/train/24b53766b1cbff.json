{"cell_type":{"eab3cdce":"code","f06b42a3":"code","94a6d90f":"code","09e90ec5":"code","2bc0b83c":"code","aa19e25a":"code","649812c4":"code","1972d782":"code","7928de35":"code","83c71774":"code","d34e1888":"code","6a65dda0":"code","75deff68":"code","289c7ad3":"code","5c9dd20f":"code","2c2e6350":"code","b1f15f6c":"code","2fc04d22":"code","dc3b5ef5":"code","eba9eab3":"code","010706f3":"code","5c26f633":"markdown","1bcc4bd2":"markdown","7be67fc7":"markdown","b1aed540":"markdown","758591e4":"markdown","0681a65a":"markdown","7318639c":"markdown"},"source":{"eab3cdce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f06b42a3":"from keras.layers import Input,Lambda,Dense,Flatten\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16,preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n","94a6d90f":"IMAGE_SIZE=[224,224]\ndata_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\n\n# Path to train directory (Fancy pathlib...no more os.path!!)\ntrain_dir = data_dir + 'train'\n\n# Path to validation directory\nval_dir = data_dir + 'val'\n\n# Path to test directory\ntest_dir = data_dir + 'test'","09e90ec5":"vgg=VGG16(input_shape=IMAGE_SIZE+[3],weights='imagenet',include_top=False)","2bc0b83c":"for layer in vgg.layers:\n    layer.trainable = False","aa19e25a":"x=Flatten()(vgg.output)","649812c4":"folders=glob(train_dir+'\/*')","1972d782":"len(folders)","7928de35":"prediction=Dense(len(folders),activation='softmax')(x)\nmodel=Model(inputs=vgg.input,outputs=prediction)","83c71774":"model.summary()","d34e1888":"from sklearn.metrics import recall_score\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","6a65dda0":"from keras.preprocessing.image import ImageDataGenerator\ntrain_datagen=ImageDataGenerator(rescale=1.\/255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\nvalid_datagen=ImageDataGenerator(rescale=1.\/255)\ntest_datagen=ImageDataGenerator(rescale=1.\/255)\n\n","75deff68":"training_set=train_datagen.flow_from_directory('..\/input\/chest-xray-pneumonia\/chest_xray\/train',target_size=(224,224),batch_size=32,class_mode='categorical')","289c7ad3":"val_set=valid_datagen.flow_from_directory('..\/input\/chest-xray-pneumonia\/chest_xray\/val',target_size=(224,224),batch_size=32,class_mode='categorical')","5c9dd20f":"test_set=test_datagen.flow_from_directory('..\/input\/chest-xray-pneumonia\/chest_xray\/test',target_size=(224,224),batch_size=32,class_mode='categorical')","2c2e6350":"r=model.fit_generator(training_set,validation_data=test_set,epochs=2,steps_per_epoch=len(training_set),validation_steps=len(test_set))","b1f15f6c":"r.history.keys()","2fc04d22":"feature=model.predict(test_set)","dc3b5ef5":"feature","eba9eab3":"feature.shape","010706f3":"preds = np.argmax(feature, axis=-1)","5c26f633":"**we don't have to train the layers ...they are already trained we ponly have to utilize them ...thatiswhy trainable is false**","1bcc4bd2":"**defining vgg 16 model and taking weight from imagenet**","7be67fc7":"**as changed above we cann se the val_accuracy from above**","b1aed540":"**applying softmax activation fpor prediction layer than attaching it into last of our neural net**","758591e4":"**flattening the out put layer of our model**","0681a65a":"**we can plot confusion matrix between preds and test data for simply* TO FIND ACCURACY...... AS OUR VALIDATION DATA OMLY HAD 16 IMAGES WE CAN CHANGE THAT IN OUR MODEL.FIT_GENERATOR TO TEST DATA ANF SIMPLY CXAN SEE ARRCURACY FROM THERE.\n\n\nCHANGIMG THE SAME FROM ABOVE***","7318639c":"**our model look like this**"}}