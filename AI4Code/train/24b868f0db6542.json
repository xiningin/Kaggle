{"cell_type":{"6fe7f9af":"code","b1346b7a":"code","da5f9742":"code","28155fab":"code","4188a42d":"code","50cc943e":"code","ae15e5e6":"code","ce1035a4":"code","8c9f352e":"code","ba6253af":"code","d74ebd0f":"code","b047baba":"code","eb18c646":"code","aee4299c":"code","7579fc5c":"code","68ced16b":"code","a63ab392":"code","b248e982":"code","fb0800c7":"code","56dfc4e2":"code","afaf3ba0":"code","ca40f799":"code","eb59964c":"code","42ca6f6f":"code","a88c161f":"code","97a538c5":"code","070c4d9b":"code","c2d2c48f":"code","574b6ee4":"code","19a61cb8":"code","b45f4237":"code","d1277ea9":"code","2fb7cb49":"code","850cb18f":"code","cb49700c":"code","755328d8":"code","dbf1126f":"code","245031d3":"code","1bfb48c0":"code","ceebcadc":"code","0efe9b73":"markdown","4cbe2231":"markdown","996baaa8":"markdown","e029dea1":"markdown","739df21b":"markdown","dfd00031":"markdown","ac057209":"markdown","8fb9ba15":"markdown","b5acc49d":"markdown","c4b12e40":"markdown","d58a7b70":"markdown","8d52d401":"markdown","00a6cce3":"markdown","ec4bea54":"markdown","7ec2d2d3":"markdown","42830f41":"markdown","5aa8ed0c":"markdown","6a4dc7be":"markdown","b6296add":"markdown"},"source":{"6fe7f9af":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os,gc\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom math import sqrt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","b1346b7a":"train = pd.read_csv('..\/input\/restaurant-revenue-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/restaurant-revenue-prediction\/test.csv')\nsample = pd.read_csv('..\/input\/restaurant-revenue-prediction\/sampleSubmission.csv')","da5f9742":"print(train.shape, test.shape)","28155fab":"train.head(n=10)","4188a42d":"train.info()","50cc943e":"train.describe()","ae15e5e6":"plt.subplots(figsize=(6,6))\nsns.distplot(train['revenue'], kde=True, bins=20)\nplt.title('Number of Restaurants vs Revenue')\nplt.xlabel('Revenue')\nplt.ylabel('Number of Restaurants')","ce1035a4":"train['City'].nunique()","8c9f352e":"plt.subplots(figsize=(8,4))\ntrain['City'].value_counts().plot(kind='bar')\nplt.title('No of restaurants vs City')\nplt.xlabel('City')\nplt.ylabel('No of restaurants')","ba6253af":"train[['City','revenue']].groupby('City').mean().plot(kind='bar')\nplt.title('Mean Revenue Generated vs City')\nplt.xlabel('City')\nplt.ylabel('Mean Revenue Generated')","d74ebd0f":"mean_revenue_per_city = train[['City', 'revenue']].groupby('City', as_index=False).mean()\nmean_revenue_per_city['revenue'] = mean_revenue_per_city['revenue'].apply(lambda x: int(x\/1e6)) \nmean_revenue_per_city\n\nmean_dict = dict(zip(mean_revenue_per_city.City, mean_revenue_per_city.revenue))\nmean_dict","b047baba":"train.replace({\"City\":mean_dict}, inplace=True)\ntest.replace({\"City\":mean_dict}, inplace=True)","eb18c646":"test['City'] = test['City'].apply(lambda x: 6 if isinstance(x,str) else x)","aee4299c":"train['City Group'].unique()","7579fc5c":"sns.countplot(train['City Group'])\nplt.ylabel('No. of Restaurants')\nplt.title('No of Restaurants vs City Group')","68ced16b":"train[['City Group', 'revenue']].groupby('City Group').mean().plot(kind='bar')\nplt.ylabel('Mean Revenue Generated')\nplt.title('Mean Revenue Generated vs City Group')","a63ab392":"lr = LabelEncoder()\ntrain['City Group'] = lr.fit_transform(train['City Group'])\ntest['City Group'] = lr.transform(test['City Group'])","b248e982":"train['Type'].unique()","fb0800c7":"sns.countplot(train['Type'])","56dfc4e2":"train[['Type', 'revenue']].groupby('Type').mean().plot(kind='bar')\nplt.title('Mean Revenue per Type')","afaf3ba0":"test['Type'] = lr.fit_transform(test['Type'])\ntrain['Type'] = lr.transform(train['Type'])","ca40f799":"train.info()","eb59964c":"train_correlations = train.drop([\"revenue\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","42ca6f6f":"plt.figure(figsize=(20,20))\nsns.heatmap(train.corr(), annot=True)","a88c161f":"X = train.drop(['revenue', 'Id', 'Open Date'],axis=1)\ny = train['revenue']","97a538c5":"X.head()","070c4d9b":"model = LinearRegression(normalize=True)\nmodel.fit(X,y)","c2d2c48f":"perm = PermutationImportance(model, random_state=1).fit(X,y)\neli5.show_weights(perm, feature_names = X.columns.to_list())","574b6ee4":"important_features = ['P26', 'P9', 'P16', 'P36', 'P8', 'P18']\n\nf, axes = plt.subplots(3,2, figsize=(12,12), sharex=True)\nf.suptitle('Distribution Plots of Important Features')\n\nfor ax,feature in zip(axes.flatten(), important_features):\n    sns.distplot(X[feature], ax=ax)","19a61cb8":"sns.pairplot(train[important_features])","b45f4237":"important_features","d1277ea9":"train['P26_to_City_mean'] = train.groupby('City')['P26'].transform('mean')\ntrain['P9_to_City_mean'] = train.groupby('City')['P9'].transform('mean')\ntrain['P16_to_City_mean'] = train.groupby('City')['P16'].transform('mean')\ntrain['P36_to_City_mean'] = train.groupby('City')['P36'].transform('mean')\ntrain['P8_to_City_mean'] = train.groupby('City')['P8'].transform('mean')\ntrain['P18_to_City_mean'] = train.groupby('City')['P18'].transform('mean')\n\ntest['P26_to_City_mean'] = test.groupby('City')['P26'].transform('mean')\ntest['P9_to_City_mean'] = test.groupby('City')['P9'].transform('mean')\ntest['P16_to_City_mean'] = test.groupby('City')['P16'].transform('mean')\ntest['P36_to_City_mean'] = test.groupby('City')['P36'].transform('mean')\ntest['P8_to_City_mean'] = test.groupby('City')['P8'].transform('mean')\ntest['P18_to_City_mean'] = test.groupby('City')['P18'].transform('mean')","2fb7cb49":"train['P26_to_City_group_mean'] = train.groupby('City Group')['P26'].transform('mean')\ntrain['P9_to_City_group_mean'] = train.groupby('City Group')['P9'].transform('mean')\ntrain['P16_to_City_group_mean'] = train.groupby('City Group')['P16'].transform('mean')\ntrain['P36_to_City_group_mean'] = train.groupby('City Group')['P36'].transform('mean')\ntrain['P8_to_City_group_mean'] = train.groupby('City Group')['P8'].transform('mean')\ntrain['P18_to_City_group_mean'] = train.groupby('City Group')['P18'].transform('mean')\n\ntest['P26_to_City_group_mean'] = test.groupby('City Group')['P26'].transform('mean')\ntest['P9_to_City_group_mean'] = test.groupby('City Group')['P9'].transform('mean')\ntest['P16_to_City_group_mean'] = test.groupby('City Group')['P16'].transform('mean')\ntest['P36_to_City_group_mean'] = test.groupby('City Group')['P36'].transform('mean')\ntest['P8_to_City_group_mean'] = test.groupby('City Group')['P8'].transform('mean')\ntest['P18_to_City_group_mean'] = test.groupby('City Group')['P18'].transform('mean')","850cb18f":"X = train.drop(['revenue', 'Id', 'Open Date'],axis=1)\ny = train['revenue']","cb49700c":"X.head()","755328d8":"cv = KFold(n_splits=10, shuffle=True, random_state=108)\nmodel = LGBMRegressor(n_estimators=200, learning_rate=0.01, subsample=0.7, colsample_bytree=0.8)\n\nscores = []\nfor train_idx, test_idx in cv.split(X):\n    X_train = X.iloc[train_idx]\n    X_val = X.iloc[test_idx]\n    y_train = y.iloc[train_idx]\n    y_val = y.iloc[test_idx]\n    \n    model.fit(X_train,y_train)\n    preds = model.predict(X_val)\n    \n    rmse = sqrt(mean_squared_error(y_val, preds))\n    print(rmse)\n    scores.append(rmse)\n\nprint(\"\\nMean score %d\"%np.mean(scores))","dbf1126f":"test.head()","245031d3":"predictions = model.predict(test.drop(['Id', 'Open Date'], axis=1))\nsample['Prediction'] = predictions","1bfb48c0":"sns.distplot(predictions, bins=20)","ceebcadc":"sample.to_csv('submission.csv', index=False)","0efe9b73":"Features in train dataset are highly correlated as compared to the test set. Let's create a baseline and check the most important features using Permutation Importance.","4cbe2231":"Using KFold cross-validation, because the size of training data is very small.","996baaa8":"Here,\n1. We have three types of restaurants, but in the test set another type 'MB' is present. We'll have to fit the label encoder on the test data.","e029dea1":"1. Istanbul has the maximum number of restaurants.\n2. Second is Ankara and then Izimir.\n3. Rest of the cities has less than 10 restaurants.","739df21b":"## Preprocessing and EDA","dfd00031":"Mean revenue generated by restaurants in 'Big Cities' is close to 5M whereas in 'Other' cities it is close to 4M. We can use label encoding on this column.","ac057209":"Now, let's see the 'City Group' column.","8fb9ba15":"Here,\n1. There are no missing values.\n2. We have 4 categorical columns.","b5acc49d":"1. We have just 137 rows to train the model.\n2. The test data is pretty huge compared to the train data.","c4b12e40":"## Feature Engineering","d58a7b70":"## Baseline Submission","8d52d401":"The number of restaurants located in Big Cities is more.","00a6cce3":"Brute force feature engineering.","ec4bea54":"The values towards the top are the most important features, and those towards the bottom matter least. P26, P9, P16, P36, P8, P18 and City are important features. Now, let's plot their graphs.","7ec2d2d3":"Now the 'Type' column.","42830f41":"Most restaurant generate revenue between 0.25e7 to 0.5e7. Now, let's see how the city affects the restaurant's revenue,","5aa8ed0c":"Here, \n1. Mean Revenue Generated is over 5M for a few cities.\n2. MRG is between 2M to 4M for most cities.\n3. It is less than 2M for just 2 cities.\n\nWe can't use label encoding on this column,it will mislead the model.We can bin the cities based on Mean Revenue Generated.","6a4dc7be":"Now, only 'Open Date' categorical column is left. We'll ignore it for now.","b6296add":"1. The target column is 'revenue'.\n2. Dataset is anonymised.\n3. This is a regression problem."}}