{"cell_type":{"8e9aebab":"code","94c71665":"code","039837b2":"code","239daeb5":"code","67264a69":"code","a3f4dbc7":"code","b27ccd83":"code","3aec5ead":"code","875cd476":"code","3b361262":"code","01728b13":"code","4d16de2b":"code","a81095ef":"code","7ad224d1":"code","fad489e3":"code","a1190fcb":"code","2fbacf90":"code","143d9c75":"code","fe94fac0":"code","19411673":"markdown","a344ab93":"markdown","655561d0":"markdown"},"source":{"8e9aebab":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import  accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nimport seaborn as sns","94c71665":"df = pd.read_csv('\/kaggle\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\ntime = 10 #Mins\ndf.info()","039837b2":"df.head()","239daeb5":"plt.figure()\ndf['blueTotalGold'].hist()\ndf['redTotalGold'].hist()\nplt.legend(['blue', 'red'])","67264a69":"plt.figure()\ndf['blueGoldDiff'].hist()\ndf['redGoldDiff'].hist()\nplt.legend(['blue', 'red'])","a3f4dbc7":"df_before_preproc = df\ny = df['blueWins']\ndf = df.drop('blueWins', axis = 1)","b27ccd83":"df.drop('gameId', axis = 1, inplace = True)\nGoldDiff = np.sum(df['blueTotalGold'] - df['redTotalGold'])\nExpDiff = np.sum(df['blueExperienceDiff'] - df['redExperienceDiff'])\nif (np.sum(df['blueGoldDiff'])  - GoldDiff) == 0:\n    df.drop('blueGoldDiff', axis = 1, inplace = True)\n    df.drop('redGoldDiff', axis = 1, inplace = True)","3aec5ead":"if (2 * np.sum(df['blueExperienceDiff'])  - ExpDiff) == 0:\n    df.drop('blueExperienceDiff', axis = 1, inplace = True)\n    df.drop('redExperienceDiff', axis = 1, inplace = True)","875cd476":"GoldPerMin = np.sum((time * df['blueGoldPerMin']) - df['blueTotalGold'])\nif GoldPerMin == 0:\n    df.drop('blueGoldPerMin', axis = 1, inplace = True)\n    df.drop('redGoldPerMin', axis = 1, inplace = True) ","3b361262":"CSPerMin = (np.sum(df['blueCSPerMin'])*time) == np.sum(df['blueTotalMinionsKilled'])\nif CSPerMin == True:\n    df.drop('blueCSPerMin', axis = 1, inplace = True)\n    df.drop('redCSPerMin', axis = 1, inplace = True)\n\ndf.drop('blueDeaths', axis = 1, inplace = True)\ndf.drop('redDeaths', axis = 1, inplace = True)\ndf.drop('blueTotalExperience', axis = 1, inplace = True)\ndf.drop('redTotalExperience', axis = 1, inplace = True)","01728b13":"plt.figure(figsize = (10,10))\nsns.heatmap(df_before_preproc.corr())","4d16de2b":"plt.figure(figsize = (10,10))\nsns.heatmap(df.corr())","a81095ef":"scaler = MinMaxScaler()\nX = df\nScaled_X = scaler.fit_transform(X)","7ad224d1":"X_train,X_test,y_train,y_test = train_test_split(Scaled_X, y, test_size = 0.3, shuffle = True, random_state = 4)","fad489e3":"Estimator = LogisticRegression()\ncv = 3\nparam_grid = {'C' : [0.0005, 0.005, 0.05, 0.5, 1], 'penalty' : ['l1','l2']}\nOptimizer = GridSearchCV(Estimator, param_grid = param_grid, cv = cv)","a1190fcb":"Optimizer.fit(X_train, y_train)","2fbacf90":"predsTest = Optimizer.predict(X_test)\nacc_test = accuracy_score(y_test, predsTest)","143d9c75":"param_grid = {'max_depth' : [None, 1, 2, 3, 4], 'min_samples_leaf' : [ 5, 10, 20, 100], }\nEstimator = DecisionTreeClassifier()\nOptimizer = GridSearchCV(Estimator, param_grid = param_grid, cv = cv)\nOptimizer.fit(X_train,y_train)","fe94fac0":"predsTestT = Optimizer.predict(X_test)\nacc_testT = accuracy_score(y_test, predsTestT)\n\n'Best accuracy scores for LogReg and DecTree with GridSearch: {} and {}'.format(round(100*acc_test,2), round(100*acc_testT,2))","19411673":"**In League of Legends, the winner of a game is often determined by two main factors: advantage in gold, and ownership of objects. This implies multiple dependencies in the parameters of experience, and the difference in gold. Using this dataset, let's try to predict the winner in the first 10 minutes of the game. However, it is also required to carry out preliminary processing of the data, since most likely it contains many parameters dependent on each other.**","a344ab93":"# As we can see, the result of the data processing has paid off. Now the data is less correlated with each other, and you can start building the initial model.","655561d0":"# **As we can see, in this dataset, some variables are very similar to each other in terms of distribution and values. Let's carry out preprocessing in order to reduce the number of such features and compare the resulting result with the initial one.**"}}