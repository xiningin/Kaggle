{"cell_type":{"8151a74b":"code","ef9a7976":"code","55a7fc3c":"code","573e42d1":"code","783beeb9":"code","b931cceb":"code","469da7b4":"code","7ec565f4":"code","b62d35dc":"code","98e505e1":"code","f2829991":"code","960c6478":"code","53b6aedf":"code","dc037c0f":"code","41858633":"code","0cf9e2d0":"code","b3c704ee":"code","4d7e7581":"code","f459852f":"code","4b7067d0":"code","5d323039":"code","abd1a946":"code","78727308":"code","4037c8ea":"code","e1a513a9":"code","cabecc3b":"code","428ac99d":"code","e4010c93":"code","8f5dc3c3":"markdown","878b4bc4":"markdown","cb9ef788":"markdown","52e96aad":"markdown","4bc97d98":"markdown","31b1d5a1":"markdown","7cc302be":"markdown","6a82778e":"markdown","1bd42616":"markdown","4141cd5e":"markdown","8008cd6f":"markdown","2b184237":"markdown","482e0126":"markdown","46c3984a":"markdown","7d6cf8a0":"markdown","a45760ae":"markdown","7e001328":"markdown","a5e3731b":"markdown","ad6caa53":"markdown","14765e50":"markdown","467513bc":"markdown","8a148cab":"markdown","840c3ccc":"markdown","59681a8c":"markdown","08d0f2bb":"markdown","4dd942e3":"markdown","06041a9e":"markdown","d1ebbd26":"markdown","1687e87b":"markdown","e8bd955a":"markdown","b158f7de":"markdown","db07d517":"markdown","65ed52c0":"markdown","13ec407f":"markdown","43ff34d2":"markdown","79f132c3":"markdown","591ff03c":"markdown","ebf89a0c":"markdown","f3a0e6af":"markdown","a191cd81":"markdown","bb6944bb":"markdown","f091970d":"markdown","b0d5223c":"markdown","79d91e0a":"markdown","2c585c01":"markdown","91e9af2d":"markdown","54e8312e":"markdown","6d74a57f":"markdown","b8e01f26":"markdown","1f473b01":"markdown","7e9acf6b":"markdown","538dceff":"markdown","af420e28":"markdown","078044ff":"markdown","9b65a7e6":"markdown"},"source":{"8151a74b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\n\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n\nfrom matplotlib import ticker\nimport time\nimport warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n\nRANDOM_STATE = 12 \nFOLDS = 5","ef9a7976":"train = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/sample_submission.csv\")","55a7fc3c":"train.head()","573e42d1":"print(f'\\033[92mNumber of rows in train data: {train.shape[0]}')\nprint(f'\\033[94mNumber of columns in train data: {train.shape[1]}')\nprint(f'\\033[91mNumber of values in train data: {train.count().sum()}')\nprint(f'\\033[91mNumber missing values in train data: {sum(train.isna().sum())}')","783beeb9":"train.describe()","b931cceb":"test.head()","469da7b4":"print(f'\\033[92mNumber of rows in test data: {test.shape[0]}')\nprint(f'\\033[94mNumber of columns in test data: {test.shape[1]}')\nprint(f'\\033[91mNumber of values in train data: {test.count().sum()}')\nprint(f'\\033[91mNo of rows with missing values  in test data: {sum(test.isna().sum())}')","7ec565f4":"test.describe()","b62d35dc":"submission.head()","98e505e1":"train.drop([\"row_id\"] , axis = 1 , inplace = True)\ntest.drop([\"row_id\"] , axis = 1 , inplace = True)\nTARGET = 'target'\nFEATURES = [col for col in train.columns if col not in ['row_id', TARGET]]\nRANDOM_STATE = 12 ","f2829991":"train.iloc[:, :-1].describe().T.sort_values(by='std' , ascending = False)\\\n                     .style.background_gradient(cmap='GnBu')\\\n                     .bar(subset=[\"max\"], color='#F8766D')\\\n                     .bar(subset=[\"mean\",], color='#00BFC4')","960c6478":"df = pd.concat([train[FEATURES], test[FEATURES]], axis=0)\n\ncat_features = [col for col in FEATURES if df[col].nunique() < 25]\ncont_features = [col for col in FEATURES if df[col].nunique() >= 25]\n\ndel df\nprint(f'Total number of features: {len(FEATURES)}')\nprint(f'\\033[92mNumber of categorical (<25 Unique Values) features: {len(cat_features)}')\nprint(f'\\033[96mNumber of continuos features: {len(cont_features)}')\n\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical(<25 Unique Values)', 'Continuos'],\n        colors=['#F8766D', '#00BFC4'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","53b6aedf":"ncols = 5\nnrows = 20\nn_features = cont_features[:100]\nfig, axes = plt.subplots(nrows, ncols, figsize=(25, 15*4))\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = n_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#F8766D', label='Train data' , fill =True )\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#00BFC4', label='Test data', fill =True)\n        axes[r,c].legend()\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8)\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(6)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","dc037c0f":"ncols = 5\nnrows = 20\nn_features = cont_features[100:200]\nfig, axes = plt.subplots(nrows, ncols, figsize=(25, 60))\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = n_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#F8766D', label='Train data' , fill =True )\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#00BFC4', label='Test data', fill =True)\n        axes[r,c].legend()\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8)\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(6)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","41858633":"ncols = 5\nnrows = 15\nn_features = cont_features[200:]\nfig, axes = plt.subplots(nrows, ncols, figsize=(25, 45))\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = n_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#F8766D', label='Train data' , fill =True )\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#00BFC4', label='Test data', fill =True)\n        axes[r,c].legend()\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8)\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(6)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","0cf9e2d0":"print(f'\\033[92mNo Categorical features.')\nprint(f'\\033[92mAll feature distribution with less than 25 unique values plotted above with continous feature distributions')\nprint(f'\\033[94mContinous Features with their unique value count:')\nfor cat in cat_features:\n    print(str(cat) + \" -   \" + str(train[cat].nunique()))","b3c704ee":"target_df = pd.DataFrame(train[TARGET].value_counts()).reset_index()\ntarget_df.columns = [TARGET, 'count']\nfig = px.bar(data_frame =target_df, \n             x = TARGET,\n             y = 'count' , \n             color = \"count\",\n             color_continuous_scale=\"Emrld\") \nfig.update_layout(template = \"plotly_white\")\nfor idx,target in enumerate(target_df[\"target\"]):\n    print(\"\\033[94mPercentage of \" + str(target) + \" category  : {:.2f} %\".format(target_df[\"count\"][idx] *100 \/ train.shape[0]))\nfig.show()","4d7e7581":"train[\"mean\"] = train[FEATURES].mean(axis=1)\ntrain[\"std\"] = train[FEATURES].std(axis=1)\ntrain[\"min\"] = train[FEATURES].min(axis=1)\ntrain[\"max\"] = train[FEATURES].max(axis=1)\n\ntest[\"mean\"] = test[FEATURES].mean(axis=1)\ntest[\"std\"] = test[FEATURES].std(axis=1)\ntest[\"min\"] = test[FEATURES].min(axis=1)\ntest[\"max\"] = test[FEATURES].max(axis=1)\n\nFEATURES.extend(['mean', 'std', 'min', 'max'])","f459852f":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntrain[TARGET] = encoder.fit_transform(train[TARGET])","4b7067d0":"lgb_params = {\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'device' : 'gpu',\n}\n\n\nlgb_predictions = []\nlgb_scores = []\nlgb_fimp = []\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[FEATURES], train[TARGET])):\n    \n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    \n    X_train, X_valid = train.iloc[train_idx][FEATURES], train.iloc[valid_idx][FEATURES]\n    y_train , y_valid = train[TARGET].iloc[train_idx] , train[TARGET].iloc[valid_idx]\n    \n    model = LGBMClassifier(**lgb_params)\n    model.fit(X_train, y_train,verbose=0)\n    \n    preds_valid = model.predict(X_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    lgb_scores.append(acc)\n    run_time = time.time() - start_time\n    \n    print(f\"Fold={fold+1}, Accuracy: {acc:.2f}, Run Time: {run_time:.2f}s\")\n    fim = pd.DataFrame(index=FEATURES,\n                 data=model.feature_importances_,\n                 columns=[f'{fold}_importance'])\n    lgb_fimp.append(fim)\n    test_preds = model.predict(test[FEATURES])\n    lgb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy :\", np.mean(lgb_scores))","5d323039":"lgbm_fis_df = pd.concat(lgb_fimp, axis=1).head(15)\nlgbm_fis_df.sort_values('1_importance').plot(kind='barh', figsize=(15, 10),\n                                       title='Feature Importance Across Folds')\nplt.show()","abd1a946":"catb_params = {\n    \"objective\": \"MultiClass\",\n    \"task_type\": \"GPU\",\n}\n\ncatb_predictions = []\ncatb_scores = []\ncatb_fimp = []\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[FEATURES], train[TARGET])):\n    \n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    \n    X_train, X_valid = train.iloc[train_idx][FEATURES], train.iloc[valid_idx][FEATURES]\n    y_train , y_valid = train[TARGET].iloc[train_idx] , train[TARGET].iloc[valid_idx]\n    \n    model = CatBoostClassifier(**catb_params)\n    model.fit(X_train, y_train,verbose=0)\n    \n    preds_valid = model.predict(X_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    catb_scores.append(acc)\n    run_time = time.time() - start_time\n    \n    print(f\"Fold={fold+1}, Accuracy: {acc:.2f}, Run Time: {run_time:.2f}s\")\n    fim = pd.DataFrame(index=FEATURES,\n                 data=model.feature_importances_,\n                 columns=[f'{fold}_importance'])\n    catb_fimp.append(fim)\n    test_preds = model.predict(test[FEATURES])\n    catb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy :\", np.mean(catb_scores))","78727308":"catb_fis_df = pd.concat(catb_fimp, axis=1).head(15)\ncatb_fis_df.sort_values('1_importance').plot(kind='barh', figsize=(15, 10),\n                                       title='Feature Importance Across Folds')\nplt.show()","4037c8ea":"xgb_params = {\n    'objective': 'multi:softmax',\n    'eval_metric': 'mlogloss',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    }\n\n\nxgb_predictions = []\nxgb_scores = []\nxgb_fimp = []\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[FEATURES], train[TARGET])):\n    \n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    \n    X_train, X_valid = train.iloc[train_idx][FEATURES], train.iloc[valid_idx][FEATURES]\n    y_train , y_valid = train[TARGET].iloc[train_idx] , train[TARGET].iloc[valid_idx]\n    \n    model = XGBClassifier(**xgb_params)\n    model.fit(X_train, y_train,verbose=0)\n    \n    preds_valid = model.predict(X_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    xgb_scores.append(acc)\n    run_time = time.time() - start_time\n    \n    print(f\"Fold={fold+1}, Accuracy: {acc:.2f}, Run Time: {run_time:.2f}s\")\n    test_preds = model.predict(test[FEATURES])\n    fim = pd.DataFrame(index=FEATURES,\n                 data=model.feature_importances_,\n                 columns=[f'{fold}_importance'])\n    xgb_fimp.append(fim)\n    xgb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy :\", np.mean(xgb_scores))","e1a513a9":"xgb_fis_df = pd.concat(xgb_fimp, axis=1).head(15)\nxgb_fis_df.sort_values('1_importance').plot(kind='barh', figsize=(15, 10),\n                                       title='Feature Importance Across Folds')\nplt.show()","cabecc3b":"lgb_submission = submission.copy()\nlgb_submission[\"target\"] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(lgb_predictions),axis = 1)[0]).astype('int'))\nlgb_submission.to_csv(\"lgb-subs.csv\",index=False)\nlgb_submission.head()","428ac99d":"catb_submission = submission.copy()\ncatb_submission[\"target\"] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(catb_predictions),axis = 1)[0]).astype('int'))\ncatb_submission.to_csv(\"catb-subs.csv\",index=False)\ncatb_submission.head()","e4010c93":"xgb_submission = submission.copy()\nxgb_submission[\"target\"] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(xgb_predictions),axis = 1)[0]).astype('int'))\nxgb_submission.to_csv(\"xgb-subs.csv\",index=False)\nxgb_submission.head()","8f5dc3c3":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","878b4bc4":"<a id=\"3.3\"><\/a>\n## Submission File","cb9ef788":"### Catboost Classifier Submission","52e96aad":"# <center> [TPS-FEB-22] \ud83d\udccaEDA + Modelling\ud83d\udcc8 <\/center>\n## <center>If you find this notebook useful, support with an upvote\ud83d\udc4d<\/center>","4bc97d98":"### Basic statistics of test data","31b1d5a1":"<a id=\"4.1\"><\/a>\n## Overview of Data","7cc302be":"### LGBM Classifier Submission","6a82778e":"<a id=\"4.7\"><\/a>\n## Null Value Distribution ","1bd42616":"### Quick view of Train Data","4141cd5e":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \n    \n### <center>Thank you for reading\ud83d\ude42<\/center>\n### <center>If you have any feedback or find anything wrong, please let me know!<\/center>\n","8008cd6f":"<a id=\"6\"><\/a>\n#  Submission","2b184237":"### Feature Importance for XGBoost Classifier (Top 15 Features)","482e0126":"### Feature Distribution of 101-200 Features","46c3984a":"Below is the first 5 rows of train dataset:","7d6cf8a0":"#### <i><u>(NOTE : THE ABOVE DISCUSSED CATEGORICAL FEATURES ARE INCLUDED IN THE FOLLOWING CONTINUOS FEATURE DISTRIBUTION PLOTS)<\/u><\/i>\n### Feature Distribution of first 100 Features","a45760ae":"# Table of Contents\n<a id=\"toc\"><\/a>\n- [1. Introduction](#1)\n- [2. Imports](#2)\n- [3. Data Loading and Preperation](#3)\n    - [3.1 Exploring Train Data](#3.1)\n    - [3.2 Exploring Test Data](#3.2)\n    - [3.3 Submission File](#3.3)\n- [4. EDA](#4)\n    - [4.1 Overview of Data](#4.1)\n    - [4.2 Null Value Distribution](#4.7)\n    - [4.3 Continuos and Categorical Data Distribution](#4.2)\n    - [4.4 Feature Distribution of Continous Features](#4.3)\n    - [4.5 Feature Distribution of Categorical Features](#4.4)\n    - [4.6 Target Distribution ](#4.5)\n- [5. Feature Engineering](#5)   \n- [6. Modelling](#6)\n    - [6.1 LGBM Classifier](#6.1)\n    - [6.2 Catboost Classifier](#6.2)\n    - [6.3 XGBoost Classifier](#6.3)\n- [7. Submission](#7)   ","7e001328":"**The task of this compeition is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. The dataset used for this compeition is derived from this [paper](https:\/\/www.frontiersin.org\/articles\/10.3389\/fmicb.2020.00257\/full).**\n\n**Submissions are evaluated based on their categorization accuracy..**","a5e3731b":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","ad6caa53":"<a id=\"3.2\"><\/a>\n## Exploring Test Data","14765e50":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Train Data:<\/u><\/b><br>\n\n* <i>There are total of <b><u>288<\/u><\/b> columns : <b><u>278<\/u><\/b> continous , <b><u>8<\/u><\/b> categorical <b><u>1<\/u><\/b> row_id and <b><u>1<\/u><\/b> target column<\/i><br>\n* <i> There are total of <b><u>200000<\/u><\/b> rows in train dataset.<\/i><br>\n* <i> <b><u>target<\/u><\/b> is the target variable which is only available in the <b><u>train<\/u><\/b> dataset..<\/i><br>\n* <i> Train dataset contain <b><u>57600000<\/u><\/b> observation with <b><u>0<\/u><\/b>  missing \/ null values.<\/i><br>\n* <i> No <b><u>NULL<\/u><\/b> Values \ud83d\ude42 <\/i><br>\n    \n<\/div>","467513bc":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","8a148cab":"### Quick view of Submission File","840c3ccc":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","59681a8c":"### Feature Importance for LGBM Classifier (Top 15 Features)","08d0f2bb":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Target Modelling :<\/u><\/b><br>\n    \n* <i> <u><b>LGBMClassifier<\/u><\/b> , <u><b>CatBoostClassifier<\/u><\/b> and <u><b>XGBClassifier<\/u><\/b> used in modelling on 5-fold validation.<\/i><br>\n* <i> Further Hyperparameter tuning can imporve the results.<\/i><br>\n    \n<\/div>","4dd942e3":"**Created by Sanskar Hasija**\n\n**[TPS-FEB-22] \ud83d\udccaEDA + Modelling\ud83d\udcc8**\n\n**01 February 2022**\n","06041a9e":"<a id=\"3.1\"><\/a>\n## Exploring Train Data","d1ebbd26":"<a id=\"1\"><\/a>\n# Introduction","1687e87b":"Below is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","e8bd955a":"<a id=\"4.3\"><\/a>\n## Feature Distribution of Continous Features","b158f7de":"###  Basic Feature Engineering","db07d517":"<a id=\"6.1\"><\/a>\n## LGBM Classifier","65ed52c0":"<a id=\"4.4\"><\/a>\n## Feature Distribution of Categorical Features","13ec407f":"<a id=\"2\"><\/a>\n# Imports","43ff34d2":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Null Value Distribution :<\/u><\/b><br>\n\n* <i> No Null values. <\/i><br>\n<\/div>","79f132c3":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Data Distribution :<\/u><\/b><br>\n    \n* <i>Out of 286 features <b><u>278<\/u><\/b> features are continous <\/i><br>\n* <i>The reamining <b><u>8<\/u><\/b> features are categorical. <b><u>(can be considered as categorical,since they have less than 25 unique values)<\/u><\/b><\/i><br>\n    \n<\/div>\n","591ff03c":"<a id=\"4.5\"><\/a>\n## Target Distribution","ebf89a0c":"### Feature Importance for Catboost Classifier (Top 15 Features)","f3a0e6af":"### Feature Distribution of 201-275 Features","a191cd81":"<a id=\"6.3\"><\/a>\n## XGBoost Classifier","bb6944bb":"<a id=\"4.2\"><\/a>\n## Continuos and Categorical Data Distribution","f091970d":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","b0d5223c":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Target Distribution :<\/u><\/b><br>\n\n* <i>There are <b><u>10<\/u><\/b> different target values<\/i><br>\n* <i>All target values are equally distributed approx - <b><u>10%<\/u><\/b> of total observations for each target.<\/i><br>\n    \n<\/div>","79d91e0a":"<a id=\"6.2\"><\/a>\n## Catboost Classifier","2c585c01":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","91e9af2d":"<a id=\"3\"><\/a>\n# Data Loading and Preperation","54e8312e":"Below is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","6d74a57f":"### Quick view of Test Data","b8e01f26":"<a id=\"5\"><\/a>\n#  Feature Engineering","1f473b01":"### XGBoost Classifier Submission","7e9acf6b":"<a id=\"4\"><\/a>\n# EDA","538dceff":"### Basic statistics of training data","af420e28":"<a id=\"6\"><\/a>\n#  Modelling","078044ff":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b><u>Observations in Test Data:<\/u><\/b><br>\n\n* <i> There are total of <b><u>287<\/u><\/b> columns : <b><u>278<\/u><\/b> continous , <b><u>8<\/u><\/b> categorical <b><u>1<\/u><\/b> row_id in <b><u>test<\/u><\/b> dataset.<\/i><br>\n* <i> There are total of <b><u>100000<\/u><\/b> rows in test dataset.<\/i><br>\n* <i> Test dataset contain <b><u>28700000<\/u><\/b> observation with <b><u>0<\/u><\/b>  missing values.<\/i><br>\n* <i> No <b><u>NULL<\/u><\/b> Values again. \ud83d\ude42<\/i><br>\n    \n<\/div>","9b65a7e6":"<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>"}}