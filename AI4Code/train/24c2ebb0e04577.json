{"cell_type":{"19f5e332":"code","10326359":"code","4443ad18":"code","cc2692f0":"code","e542baac":"code","68f8a813":"code","a69ac8a5":"code","4cc4b8a0":"code","52db6dcc":"code","60ae2aab":"code","8073d7a4":"code","48d09394":"code","71f8b48a":"code","a85b8e51":"code","32ff1482":"code","17345d98":"code","540d991d":"code","77fec871":"code","5563715a":"code","c7e789a5":"code","c4f3bb31":"code","8a772560":"code","ab5102f9":"code","64eded88":"code","02a1b596":"markdown","4adb7610":"markdown","665f02c1":"markdown","cda1ee04":"markdown","a1c4ac36":"markdown","b7b6c9bd":"markdown","6234a74a":"markdown","bcdc70d2":"markdown","189f8037":"markdown","cee9382b":"markdown","e021cee1":"markdown","b05e1ae2":"markdown","c3df0ab2":"markdown","c73a11c7":"markdown","c9ed5f02":"markdown","1fd134e9":"markdown","272f5b53":"markdown"},"source":{"19f5e332":"p = \"\/kaggle\/input\/kermany2018\/OCT2017 \/train\"\np1 = \"\/kaggle\/input\/ai-in-healthcare-hackathon\/DataVerse_Dataset\"","10326359":"import os\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport seaborn as sns\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\n\nfrom keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,MaxPooling2D,AveragePooling2D, BatchNormalization, PReLU\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications import ResNet50, InceptionResNetV2\nfrom keras.preprocessing.image import load_img,img_to_array\nfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\n\nimport matplotlib.cm as cm\nfrom IPython.display import Image, display\n\nimport itertools\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report","4443ad18":"train = defaultdict(list)\ntrain_class = defaultdict(list)\n\nfor i in ['DME', 'CNV', 'NORMAL']:\n    path = os.path.join(p, i)\n    for j in tqdm(os.listdir(path)):\n        im = plt.imread(os.path.join(path, j))\n        if im.shape == (496, 768):\n            train[str(i)].append(os.path.join(path, j))\n            train_class[str(i)].append(i)","cc2692f0":"for i in ['DME', 'CNV', 'NORMAL']:\n    print(i,len(train[i]))","e542baac":"data_path = []\ndata_label = []\nfor i,j in zip(['DME', 'CNV', 'NORMAL'],[816, 900, 800]):\n    temp  = list(zip(train[i], train_class[i]))\n    random.shuffle(temp)\n    res1, res2 = zip(*temp)\n    data_path.extend(res1[0:j])\n    data_label.extend(res2[0:j])","68f8a813":"def labeller(j):\n    if j.startswith(\"AMRD\"): return 'CNV'\n    elif j.startswith(\"DR\"): return 'DME'\n    else: return 'NORMAL'","a69ac8a5":"for i in ['Macular Degeneration', 'Diabetic_Retinopathy','Normal_retina']:\n    full_path = os.path.join(p1,i)\n    img_dir = os.listdir(full_path)\n    for j in img_dir:\n        data_path.append(os.path.join(full_path, j))\n        data_label.append(labeller(j))","4cc4b8a0":"X_train, X_test, y_train, y_test = train_test_split(data_path, data_label, test_size=0.2, random_state=42)\ndf_train = pd.DataFrame(list(zip(X_train, y_train)),columns =['image_path', 'label'])\ndf_test = pd.DataFrame(list(zip(X_test, y_test)),columns =['image_path', 'label'])","52db6dcc":"plt.rcParams[\"figure.figsize\"] = (15,4)\ndf = df_train\nlab = df['label']\ndist = lab.value_counts()\nsns.countplot(x = lab)","60ae2aab":"plt.rcParams[\"figure.figsize\"] = (15,4)\ndf = df_test\nlab = df['label']\ndist = lab.value_counts()\nsns.countplot(x = lab)","8073d7a4":"train_aug = ImageDataGenerator(\n    rotation_range=0.2,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    shear_range=0.05,\n    zoom_range=0.05,\n    rescale = 1.\/255)\n\ntest_aug = ImageDataGenerator(\n    rescale = 1.\/255)\n\ntrain_generator= train_aug.flow_from_dataframe(\ndataframe=df_train,\nx_col=\"image_path\",\ny_col=\"label\",\nbatch_size=32,\ncolor_mode=\"rgb\",\nshuffle = True,\ntarget_size = (224, 224),\nclass_mode=\"categorical\")\n\ntest_generator= test_aug.flow_from_dataframe(\ndataframe=df_test,\nx_col=\"image_path\",\ny_col=\"label\",\ncolor_mode=\"rgb\",\nbatch_size=32,\nshuffle = False, \ntarget_size = (224, 224),\nclass_mode=\"categorical\")","48d09394":"#Model Defining\ndef generate_model(pretrained_model = 'vgg16', num_classes =3):\n    if pretrained_model == 'inceptionv3':\n        weight_path = '..\/input\/keras-pretrained-models\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        base_model = InceptionV3(weights = weight_path, include_top=False, input_shape=(224, 224, 3))\n    elif pretrained_model == 'inceptionresnet':\n        base_model = InceptionResNetV2(weights = 'imagenet', include_top=False, input_shape=(224, 224, 3))\n    else:\n        weight_path = '..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        base_model = VGG16(weights = weight_path, include_top=False, input_shape=(224, 224, 3)) # Topless\n    \n    # Add top layer\n    \n    x = base_model.output\n    #x = Conv2D(256, kernel_size = (3,3), padding = 'valid')(x)\n    x = Flatten()(x)\n    x = Dropout(0.4)(x)\n    x = Flatten()(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    #Freezing Convolutional Base\n    \n    for layer in base_model.layers:\n        layer.trainable = False  \n    return model","71f8b48a":"def train_model(model, train_generator, test_generator, num_epochs, optimizer):\n    model.compile(loss='categorical_crossentropy', \n                  optimizer=optimizer, \n                  metrics=['accuracy'])\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",patience=6, verbose=1)\n    rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=4)\n    print(model.summary())\n    \n    # Train using .fit\n    \n    history = model.fit(train_generator, epochs=num_epochs, \n                        validation_data=test_generator, verbose=1,\n                        callbacks = [rlr, early_stop])\n    \n    return model, history","a85b8e51":"def plot_loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \ndef plot_acc(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n# It prints & plots the confusion matrix, normalization can be applied by setting normalize=True.\n    \ndef plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n\n    plt.figure(figsize = (5,5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndef plot_roc_curves(y_true, y_pred, num_classes, class_labels):\n    \n    lb = LabelBinarizer()\n    lb.fit(y_true)\n    y_test = lb.transform(y_true)\n\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(num_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Plot all ROC curves\n    for i in range(num_classes):\n        fig, c_ax = plt.subplots(1,1, figsize = (6, 4))\n        c_ax.plot(fpr[i], tpr[i],\n                 label='ROC curve of class {0} (area = {1:0.4f})'\n                 ''.format(class_labels[i], roc_auc[i]))\n        c_ax.set_xlabel('False Positive Rate')\n        c_ax.set_ylabel('True Positive Rate')\n        c_ax.set_title('ROC curve of class {0}'.format(class_labels[i]))\n        c_ax.legend(loc=\"lower right\")\n        plt.show()\n    return roc_auc_score(y_test, y_pred)","32ff1482":"def evaluate_model(model, history, test_generator):\n    # Evaluate model\n    score = model.evaluate(test_generator, verbose=0)\n    print('\\nTest set accuracy:', score[1], '\\n')\n    \n    y_true = np.array(test_generator.labels)\n    y_pred = model.predict(test_generator, verbose = 1)\n    y_pred_classes = np.argmax(y_pred,axis = 1)\n    class_labels = list(test_generator.class_indices.keys())   \n    \n    print('\\n', sklearn.metrics.classification_report(y_true, y_pred_classes, target_names=class_labels), sep='')\n    confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n    plot_acc(history)\n    plt.show()\n    plot_loss(history)\n    plt.show()\n    plot_confusion_matrix(confusion_mtx, classes = class_labels)\n    plt.show()\n    print(\"ROS AUC score:\", plot_roc_curves(y_true, y_pred,3, class_labels))","17345d98":"vgg_model = generate_model('vgg16', 3)","540d991d":"vgg_model, vgg_history = train_model(vgg_model, train_generator, test_generator, 12, 'adam')","77fec871":"evaluate_model(vgg_model, vgg_history, test_generator)","5563715a":"model_builder = tf.keras.applications.vgg16.VGG16\n\nlast_conv_layer_name =\"block5_conv3\"\nimage_path=df_test['image_path'][5]\n\nimg = load_img(image_path, target_size=(224,224,3)) # stores image in PIL format\nimage_array=img_to_array(img)\n","c7e789a5":"display(Image(image_path))","c4f3bb31":"def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n\n    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","8a772560":"# Make model\nmodel = model_builder(weights=\"imagenet\")\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None\n\nimg_array=np.expand_dims(image_array, axis=0)\n# Prepare particular image\nimg_array = preprocess_input(img_array)    \n\n# Print what the top predicted class is\npreds = model.predict(img_array)  \n\n# Generate class activation heatmap\nheatmap= make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()","ab5102f9":"def save_and_display_gradcam(img_path, heatmap, cam_path, alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n    display(Image(cam_path))\n","64eded88":"save_and_display_gradcam(image_path, heatmap,cam_path=\"\/kaggle\/working\/GradCamTest.jpg\")","02a1b596":"# Data Pre Processing & Visualization","4adb7610":"Test Data","665f02c1":"# Importing Data & Libraries","cda1ee04":"#### In this notebook, we used data from 2 datasets containing OCT images, \n 1.  #### Kermany (Retinal_OCT2017)\n 2.  #### Dataverse Data Set \n\n####  CNV:   AMD (55 - dataverse) + CNV(900-kermany)\n####  DME:   DR (107 - dataverse) + DME(816-kermany)\n#### Normal: Normal(208 - dataverse) + Normal(800-kermany)","a1c4ac36":"Create Heat Map","b7b6c9bd":"# Transfer Learning using VGG16","6234a74a":"Training Data","bcdc70d2":"Training","189f8037":"# Model Evaluation\n## Plotting Curves - Loss, Accuracy, ROC + Confusion Matrix","cee9382b":"## Data Augmentation","e021cee1":"# MODEL\n## Compiling & Training","b05e1ae2":"# Model Interpretability using Grad CAM\n\nOne way to ensure the model is performing correctly is to debug your model and visually validate that it is \u201clooking\u201d and \u201cactivating\u201d at the correct locations in an image.\n\nSelvaraju et al. published a novel paper entitled, Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization (https:\/\/arxiv.org\/abs\/1610.02391). \n\nGRAD-CAM works by (1) finding the final convolutional layer in the network and then (2) examining the gradient information flowing into that layer.\n\nThe output of Grad-CAM is a heatmap visualization for a given class label (either the top, predicted label or an arbitrary label we select for debugging). We can use this heatmap to visually verify where in the image the CNN is looking.","c3df0ab2":"Compilation","c73a11c7":"Evaluation","c9ed5f02":"Display Original Image","1fd134e9":"GRAD CAM Algorithm","272f5b53":" Create a superimposed visualization"}}