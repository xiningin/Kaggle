{"cell_type":{"cdd6a35f":"code","59c4d21b":"code","8dba6352":"code","6d3345fe":"code","853da572":"code","53d59005":"code","29cb1c59":"code","423bf4d9":"markdown","36197087":"markdown","169b41b5":"markdown","85453cda":"markdown","dce806bc":"markdown"},"source":{"cdd6a35f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59c4d21b":"data = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndata.head()","8dba6352":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\ndata['Species']= label_encoder.fit_transform(data['Species'])\n \ndata['Species'].unique()","6d3345fe":"import random\nimport math\nfrom collections import Counter\n\n\n# Calculate Entropy\ndef entropy(y):\n    hist = np.bincount(y)\n    ps = hist\/len(y)\n    return - np.sum([p * np.log2(p) for p in ps if p > 0])\n\n\n# Create Node\nclass Node:\n    \n    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n    \n    def is_leaf_node(self):\n        return self.value is not None\n        \n\n#Decision Tree\nclass DecissionTree:\n    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None, max_features='auto'):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.root = None\n        self.max_features = max_features\n    \n    def fit(self, X, y):\n        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n        self.cols = list(X.columns)\n        self.root = self.grow_tree(X, y)\n        \n    def grow_tree(self, X, y, depth=0):\n        \n        df = X.copy()\n        df['target'] = y\n        \n        n_samples, n_features = X.shape\n        n_labels = len(np.unique(y))\n        \n        # stopping criteria\n        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n            leaf_value = self.most_common_label(y)\n            return Node(value=leaf_value)\n        \n        # array of random columns in Dataset\n        \n        data = self.feature_sampling(X, self.max_features)\n               \n        feats_idxs = list(data.columns)\n                        \n        best_feat, best_thresh = self.best_criteria(X, y.tolist(), feats_idxs)\n        \n        left_df, right_df = df[df[best_feat]<=best_thresh].copy(), df[df[best_feat]>best_thresh].copy() \n      \n        left = self.grow_tree(left_df.drop('target', axis=1), left_df['target'].values, depth+1)\n        right = self.grow_tree(right_df.drop('target', axis=1), right_df['target'].values, depth+1)\n        \n        return Node(best_feat, best_thresh, left, right)\n        \n    def best_criteria(self, X, y, feats_idxs):\n        best_gain = -1\n        split_idx, split_tresh = None, None\n        \n        X = X.to_numpy()\n        \n        for feats_idx in feats_idxs:\n            \n            index = int(self.cols.index(feats_idx))\n            \n            df = pd.DataFrame(X[:, index], columns=['X_col'])\n            df['y'] = y\n            df = df.sort_values(by=['X_col'], ascending=True)\n            \n            X_col_2 = df.X_col\n            y_2 = df.y\n                        \n            X_col_2 = X_col_2.to_numpy()\n            y_2 = y_2.to_numpy()\n            \n            for val in X_col_2:\n                gain = self.information_gain(y_2, X_col_2, val)\n                if gain > best_gain:\n                    best_gain = gain\n                    split_idx = feats_idx\n                    split_tresh = val\n        \n        return split_idx, split_tresh\n    \n    def information_gain(self, y, X_col, thresh):\n        parent_entropy = entropy(y)\n        \n        left, right = self.split(X_col, thresh)\n\n        if len(left) == 0 or len(right) == 0:\n            return 0\n        \n        n = len(y)\n        n_l, n_r = len(left), len(right)\n        e_l, e_r = entropy(y[left]), entropy(y[right])\n        \n        child_entropy = (n_l \/ n) * e_l + (n_r \/ n) * e_r\n        \n        ig = parent_entropy - child_entropy\n        return ig\n    \n    def split(self, X_col, split_tresh):\n        \n        left_idxs = np.argwhere(X_col <= split_tresh).flatten()\n        right_idxs = np.argwhere(X_col > split_tresh).flatten()\n\n        return left_idxs, right_idxs\n    \n    def most_common_label(self, y):\n        counter = Counter(y)\n        most_common = counter.most_common(1)[0][0]\n        return most_common\n    \n    def predict(self, X):\n        X = X.to_numpy().tolist()\n        return np.array([self.traverse_tree(x, self.root) for x in X])\n    \n    def traverse_tree(self, x, node):\n        if node.is_leaf_node():\n            return node.value\n        \n        index = int(self.cols.index(node.feature))\n\n        if x[index] <= node.threshold:\n            return self.traverse_tree(x, node.left)\n        \n        return self.traverse_tree(x, node.right)\n    \n    def feature_sampling(self, data, val):\n        if type(val) == int:\n            col = random.sample(data.columns.tolist()[:], val)\n            new_df = data[col]\n            return new_df\n        elif type(val) == float:\n            col = random.sample(data.columns.tolist()[:], int(val * data.shape[1]))\n            new_df = data[col]\n            return new_df\n        elif val == 'auto' or val == 'sqrt':\n            col = random.sample(data.columns.tolist()[:], int(math.sqrt(data.shape[1])))\n            new_df = data[col]\n            return new_df\n        elif val == 'log2':\n            col = random.sample(data.columns.tolist()[:], int(math.log2(data.shape[1])))\n            new_df = data[col]\n            return new_df\n        else:\n            return data\n\n\nclass randomforestclassifier:\n    def __init__(self, n_estimators=100, criterion='entropy', max_depth=None, min_samples_split=2, bootstrap=True, max_samples=None,\n                 max_features='auto', oob_score=False):\n        self.n_estimators = n_estimators\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.bootstrap = bootstrap\n        self.max_samples = max_samples\n        self.max_features = max_features\n        self.oob_score = oob_score\n    \n    def fit(self, X_train, y_train):\n        dummy_data = X_train.copy()\n        dummy_data['target'] = y_train\n        \n        self.tree_list = []\n        \n        for i in range(self.n_estimators):\n            \n            if self.bootstrap == True:\n                df = self.row_sampling(dummy_data, self.max_samples)\n            else:\n                df = dummy_data.copy()\n            \n            tree = DecissionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, max_features=self.max_features)\n            \n            tree.fit(df.drop('target', axis=1), df.target)\n\n            self.tree_list.append(tree)\n            \n    \n    def predict(self, X_test):\n        y_preds = np.empty((X_test.shape[0], len(self.tree_list)))\n        # Let each tree make a prediction on the data\n        for i, tree in enumerate(self.tree_list):\n            # Indices of the features that the tree has trained on\n            # idx = tree.feature_indices\n            # Make a prediction based on those features\n            prediction = tree.predict(X_test)\n            y_preds[:, i] = prediction\n        \n        y_pred = []\n        # For each sample\n        for sample_predictions in y_preds:\n            # Select the most common class prediction\n            y_pred.append(np.bincount(sample_predictions.astype('int')).argmax())\n        return y_pred\n    \n    def score(self, y_true=None, y_pred=None):\n        acc = np.sum(y_true == y_pred)\/len(y_true)\n        return acc\n    \n    def row_sampling(self, data, val):\n        if type(val) == float:\n            return data.sample(int(val * data.shape[0]), replace=True)\n        if type(val) == int:\n            return data.sample(val, replace=True)\n        if val == None:\n            return data","853da572":"if __name__ == '__main__':\n    x = data.drop('Species', axis=1)\n    y = data.Species\n    \n    from sklearn.model_selection import train_test_split\n    \n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n            \n    re = randomforestclassifier(n_estimators=10, max_depth=5, min_samples_split=5, max_samples=50, max_features=3)\n    \n    re.fit(X_train, y_train)","53d59005":"y_pred = re.predict(X_test)","29cb1c59":"sc = re.score(y_test, y_pred)\nprint('Accuracy :', sc)","423bf4d9":"# Load Data","36197087":"# Main Function","169b41b5":"# RandomForest Classifier","85453cda":"# Find Accuracy","dce806bc":"# Encode Species Column"}}