{"cell_type":{"593a1514":"code","0dc295b6":"code","b3b288ce":"code","30ff9683":"code","9b7571c0":"code","e9bcdf36":"code","0e0cfa6e":"code","58620c2c":"code","7798eccb":"code","e3fb2ae6":"code","21315947":"code","88e9b1ea":"code","44decc73":"code","36fcb13c":"code","3043894f":"code","c3b411c3":"code","e363e9d1":"code","d1d66117":"code","ef8fc009":"code","f6074fc1":"code","fb020697":"code","9a1c2184":"markdown","dcd06a6b":"markdown","2beba989":"markdown","0f550b87":"markdown","7e97bcbf":"markdown"},"source":{"593a1514":"!pip install vaderSentiment","0dc295b6":"import pandas as pd\nimport numpy as np\nfrom scipy.sparse import hstack,csr_matrix\nfrom itertools import combinations,chain\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom nltk import pos_tag\nfrom nltk.tokenize import RegexpTokenizer\ntoker = RegexpTokenizer(r'\\w+')\nfrom nltk.stem import snowball\nst = snowball.SnowballStemmer('english')\nwnl =  nltk.wordnet.WordNetLemmatizer()\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\nfrom nltk.corpus import stopwords\nstopWords = set(stopwords.words('english'))\nangram_range = (1,1)\n#token_pattern=r'\\w+'\ntf_vectorizer = CountVectorizer( ngram_range=angram_range ,strip_accents='unicode',stop_words=stopWords)\ntfidf_vectorizer = TfidfVectorizer(ngram_range=angram_range,strip_accents='unicode',stop_words=stopWords)\nfrom scipy.stats import ttest_ind,mstats\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix, roc_curve,roc_auc_score,auc,precision_recall_curve,f1_score\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier,LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nimport matplotlib.pyplot as plt \nimport seaborn as sns","b3b288ce":"df_Train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_Test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","30ff9683":"df_Train.head()","9b7571c0":"df_Test.head()","e9bcdf36":"print('number of tweets')\nprint(len(df_Train))\nprint('number of disaster tweets')\nprint(df_Train['target'].sum())","0e0cfa6e":"#unqiue counts\nprint('unqiue count id')\nprint(len(pd.unique(df_Train['id'])))\nprint('unqiue count keyword')\nprint(len(pd.unique(df_Train['keyword'])))\nprint('unqiue count location')\nprint(len(pd.unique(df_Train['location'])))\nprint('unqiue count target')\nprint(len(pd.unique(df_Train['target'])))","58620c2c":"#nan counts\nprint('nan count id')\nprint(sum(pd.isna(df_Train['id'])))\nprint('nan count keyword')\nprint(sum(pd.isna(df_Train['keyword'])))\nprint('nan count location')\nprint(sum(pd.isna(df_Train['location'])))\nprint('nan count target')\nprint(sum(pd.isna(df_Train['target'])))","7798eccb":"print(pd.unique(df_Train['keyword']))","e3fb2ae6":"#replace '%20' with spaces\ndf_Train.loc[:,'keyword'] = df_Train['keyword'].apply(lambda x: x.replace('%20',' ') if type(x)==str else x)\ndf_Test.loc[:,'keyword'] = df_Test['keyword'].apply(lambda x: x.replace('%20',' ') if type(x)==str else x)","21315947":"#create hashtag indictor\ndf_Train['Hashtag_indicator']=df_Train['text'].apply(lambda x: 0  if str(x).find('#')==-1 else 1 )\ndf_Test['Hashtag_indicator']=df_Test['text'].apply(lambda x: 0  if str(x).find('#')==-1 else 1 )","88e9b1ea":"print('number of tweets with hashtag')\nprint(df_Train['Hashtag_indicator'].sum())","44decc73":"#sentiment- neg sentiment could help model prediction\ndf_Train['sentiment'] = df_Train['text'].apply(lambda x: sid.polarity_scores(str(x))['compound'])\ndf_Test['sentiment'] = df_Test['text'].apply(lambda x: sid.polarity_scores(str(x))['compound'])","36fcb13c":"#tfidf vectorize the acutal text\ntfidf_fit = tfidf_vectorizer.fit(df_Train['text'])\n\nX2 = tfidf_fit.transform(df_Train['text'])\nX4 = csr_matrix(df_Train['sentiment'].to_list()).transpose()\nX6 = csr_matrix(df_Train['Hashtag_indicator'].to_list()).transpose()\n\nX2_test = tfidf_fit.transform(df_Test['text'])\nX4_test = csr_matrix(df_Test['sentiment'].to_list()).transpose()\nX6_test =csr_matrix(df_Test['Hashtag_indicator'].to_list()).transpose()\n\n#combine all features into one matrix\nX=hstack([X2,X4,X6])\nX_test=hstack([X2_test,X4_test,X6_test])\nX=csr_matrix(X)\nX_test =csr_matrix(X_test)\ny = df_Train['target'].to_list()\ny=np.array(y)","3043894f":"print('top 20 text words - avg tfidf score')\npd.DataFrame(X2.mean(axis=0),columns=tfidf_fit.get_feature_names()).T.sort_values(0,ascending=False)[:20].plot.bar()","c3b411c3":"df_Train_target1 =df_Train[df_Train['target']==1]\ndf_Train_target0 =df_Train[df_Train['target']==0]\nL_target1_Pos = list(df_Train_target1.index)\nL_target0_Pos = list(df_Train_target0.index)\nX_target1 = X[L_target1_Pos,:]\nX_target0 = X[L_target0_Pos,:]","e363e9d1":"print('avg sentiment for disaster:')\nprint(df_Train_target1['sentiment'].mean())\nprint('avg sentiment for non-disaster:')\nprint(df_Train_target0['sentiment'].mean())","d1d66117":"print('avg Hashtag_indicator for disaster:')\nprint(df_Train_target1['Hashtag_indicator'].mean())\nprint('avg Hashtag_indicator for non-disaster:')\nprint(df_Train_target0['Hashtag_indicator'].mean())","ef8fc009":"\n#filter cols with t-test, might take some time\nLtTest = [col for col in range(X.shape[1]) if ttest_ind(X_target0[:,col].toarray(), X_target1[:,col].toarray(), axis=0, equal_var=False, nan_policy='omit')[1]<=0.3]\nX = X[:,LtTest]\nX_test = X_test[:,LtTest]","f6074fc1":"#train models\nclassifier_tree = XGBClassifier(n_estimators=1200,n_jobs=-1,random_state=123)\nclassifier_svm = SVC(probability=True,gamma='scale',kernel='rbf')\nclassifier_linear = BaggingClassifier(LogisticRegression(solver='lbfgs'),n_estimators=10,random_state=123)\n\nLclassifiers = [classifier_linear,classifier_tree ]#\ncv = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=2)\n##find best theshold from f1 score\n#Threshold_pred = 0.5\nfor train, test in cv.split(X, y):\n    X_train_cv  = X[train]\n    X_test_cv = X[test]\n    y_train_cv = y[train]\n    y_test_cv = y[test]\n    #X_trainT_cv = csr_matrix.transpose(X_train_cv)#X_train_cv.T\n    \n    print('over sample with smote')\n    intKn = 5\n    smoter = SMOTE(k_neighbors=intKn)\n    X_train_cv, y_train_cv = smoter.fit_resample(X_train_cv, y_train_cv)\n    \n    X_train_cv=csr_matrix(X_train_cv)\n    #train models in list\n    print('fit model')\n    Lfit_var_cv = [model.fit(X_train_cv, y_train_cv) for model in Lclassifiers]\n    print('predict model')\n    Lcv_probas_ = [predcitive_fit.predict_proba(X_test_cv)[:,1] for predcitive_fit in Lfit_var_cv]\n    \n    cv_probas_ = np.mean(Lcv_probas_,axis = 0)\n    \n    print('compute the ROC curve')\n    fpr, tpr, thresholds = roc_curve(y_test_cv, cv_probas_)#[:, 1])##\n    roc_auc = auc(fpr, tpr)\n    \n    print('compute the precision recall curve')\n    precision, recall, thresholds_PR = precision_recall_curve(y_test_cv, cv_probas_)\n    PR_auc =  auc(recall, precision)\n    \n    #make sure 50% is in tested list\n    thresholds_PR=list(thresholds_PR)\n    thresholds_PR.append(0.5)\n    print('find % threshold that gives the highest f1 score')\n    df_f1_PR = pd.DataFrame([[thresholds_,f1_score(y_test_cv,np.where(cv_probas_>thresholds_,1,0))]  for thresholds_ in thresholds_PR])\n    df_f1_PR_max = df_f1_PR[df_f1_PR[1] == max(df_f1_PR[1])]\n    #Threshold_pred = df_f1_PR_max[0].values[0]\n    Threshold_pred=0.5\n    cv_pred_var =  np.where(cv_probas_>Threshold_pred,1,0)\n    Lcm = list(confusion_matrix(y_test_cv,cv_pred_var).ravel())\n    print('__________________')\n    df_f1_PR.columns = ['Thresholds','f1_score']\n    df_f1_PR=df_f1_PR.sort_values('f1_score',ascending=False)\n    print('top 5 thresholds and f1 score')\n    print(df_f1_PR.head())\n    print('bottom 5 thresholds and f1 score')\n    print(df_f1_PR.tail())\n    print('__________________')\n    print('Threshold for predictions:')\n    print(Threshold_pred)\n    print('__________________')\n    print('vailidation metrics:')\n    print('__________________')\n    print('ROC AUC')\n    print(roc_auc)\n    print('PR AUC')\n    print(PR_auc)\n    print('CM [tn,fp,fn,tp]:')\n    print(Lcm)\n    print('Accuracy:')\n    print((Lcm[0]+Lcm[3])\/sum(Lcm))\n    print('Precision:')\n    print(Lcm[3]\/(Lcm[3]+Lcm[1]))\n    print('Recall sensitivity:')\n    print(Lcm[3]\/(Lcm[3]+Lcm[2]))\n    print('Specificity')\n    print(Lcm[0]\/(Lcm[0]+Lcm[1]))\n    print('F1 Score ')\n    print((2*Lcm[3])\/((2*Lcm[3])+Lcm[1]+Lcm[2]))\n    plt.close()\n    mid_line = np.divide(list(range(0,101,1)),100)\n    mid_line2=list(mid_line)\n    mid_line2.reverse()\n    roc_ax = sns.scatterplot(fpr, tpr)\n    roc_ax.set(xlabel='fpr', ylabel='tpr', title='ROC AUC:'+str(roc_auc))\n    plt.plot(mid_line, mid_line, color='r')\n    plt.show()\n    plt.close()\n    PR_ax = sns.scatterplot(precision, recall)\n    PR_ax.set(xlabel='precision', ylabel='recall', title='PR AUC:'+str(PR_auc))\n    plt.plot(mid_line, mid_line2, color='r')\n    plt.show()\n    plt.close()\n    df_his = pd.DataFrame([cv_probas_,y_test_cv]).T\n    x_1 = df_his[df_his[1]==1.0][0].values\n    x_0 = df_his[df_his[1]==0.0][0].values\n    plt.clf()\n    plt.hist(x_1, density=True, bins=30,color='r',label='target1',alpha = 0.5)#normed\n    plt.hist(x_0, density=True, bins=30,color='b',label='target0',alpha = 0.5)#normed\n    plt.axvline(x=Threshold_pred)\n    plt.xlabel('Probability of prediction')\n    plt.legend()","fb020697":"smoter = SMOTE(k_neighbors=intKn)\nX_oversampled, y_oversampled = smoter.fit_resample(X, y)\nLfit_var = [model.fit(X_oversampled, y_oversampled) for model in Lclassifiers]\nLprobas = [predcitive_fit.predict_proba(X_test)[:,1] for predcitive_fit in Lfit_var]\nprobas_ = np.mean(Lprobas,axis = 0)\npred_var =  np.where(probas_>Threshold_pred,1,0)\ndf_Test['target'] = pred_var\ndf_Test[['id','target']].to_csv(\"submission.csv\",index=False, header=True)\nprint('finshed')","9a1c2184":"# final model - on train on all data","dcd06a6b":"# vailidation model","2beba989":"# import libs","0f550b87":"# read in data files","7e97bcbf":"# preprocessing+profiling"}}