{"cell_type":{"4b9c6e84":"code","c9ddb9f7":"code","4fd533da":"code","05fff80a":"code","af27ae30":"code","ea3b5183":"code","92ddf8fd":"code","2953d1fe":"code","46260e95":"code","35d2477a":"code","5232e317":"code","123b6b86":"code","060f852b":"code","82978bb1":"code","6369b0fa":"code","42898206":"code","bc53f01d":"code","c87865a5":"code","fd45af7d":"code","2b5de705":"code","7ef4100c":"code","fccab634":"code","09e380ae":"code","fb5e2465":"code","812793c6":"code","ccf17774":"code","de312277":"code","96f2511f":"code","df387a8e":"code","68d9bd84":"code","2a29fabc":"code","e569a795":"code","6624121b":"code","5c23d43c":"code","a302d763":"code","4775695b":"code","a2ba31e7":"code","76d21907":"code","664bb4a3":"code","e85ce5ea":"markdown","18356844":"markdown","a293d423":"markdown","15d01585":"markdown","56fa13d5":"markdown","a47d6ee0":"markdown","7ec715d2":"markdown","a86e2bfe":"markdown","b7a58247":"markdown","ec3a017a":"markdown","91047c37":"markdown","4a921caf":"markdown","5ab85e59":"markdown","e9b3a9ec":"markdown","be1ee5b8":"markdown","cda90fd1":"markdown","8ccb0728":"markdown","393577e5":"markdown","1ecb90da":"markdown","d45536be":"markdown","9e3c396f":"markdown","5de30df6":"markdown","9f6f576b":"markdown","a1bf174a":"markdown","09ea8041":"markdown","4ac8f3f3":"markdown","6afb014e":"markdown","0f2d1b42":"markdown","82f09346":"markdown","8b673001":"markdown","52dfb8d9":"markdown"},"source":{"4b9c6e84":"!pip install missingpy","c9ddb9f7":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import Image as show_gif\nfrom sklearn.cluster import KMeans\nfrom skimage.transform import resize\nimport scipy.ndimage as ndimage\nfrom plotly.tools import FigureFactory as FF\nfrom skimage import measure, morphology, segmentation\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom missingpy import MissForest\nimport lightgbm as lgb\nfrom scipy import stats\nimport copy\nimport pydicom\nimport glob\nimport re\nimport os\nimport scipy","4fd533da":"train = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv')\ntest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')","05fff80a":"train","af27ae30":"train.describe()","ea3b5183":" train[train.duplicated(subset=['Patient','Weeks'], keep=False)]","92ddf8fd":"train = train.groupby(['Patient', 'Weeks']).agg({ \n    'FVC': 'mean', \n    'Percent': 'mean', \n    'Age': 'first',\n    'Sex': 'first',\n    'SmokingStatus': 'first'\n}).reset_index()","2953d1fe":"print('Number of duplicates: %s'%len(train[train.duplicated(subset=['Patient','Weeks'], keep=False)]))","46260e95":"train.isna().any()","35d2477a":"print(\"Number of unique patients: %s\"%(train.Patient.nunique()))","5232e317":"num_obs_per_patient = train.groupby('Patient').count()['Weeks'].sort_values()\n\nfig, axs = plt.subplots(2, 1, figsize=(15, 10))\ndense = sns.kdeplot(num_obs_per_patient, bw=.5, ax=axs[0])\ndense.get_legend().remove()\ndense.set_xlabel(\"Number of Observations\")\nbar = sns.barplot(x=list(range(len(num_obs_per_patient))), y=num_obs_per_patient, ax=axs[1])\nbar.axes.get_xaxis().set_visible(False);\nbar.set_ylabel(\"Oberservations\");","123b6b86":"fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n\naxs[0,0].set_title('Weeks', fontsize=18)\nsns.kdeplot(train.Weeks, shade=True, ax=axs[0,0])\naxs[0,0].get_legend().remove()\n\naxs[0,1].set_title('FVC', fontsize=18)\nsns.kdeplot(train.FVC, shade=True, ax=axs[0,1])\naxs[0,1].get_legend().remove()\n\naxs[1,0].set_title('Percent', fontsize=18)\nsns.kdeplot(train.Percent, shade=True, ax=axs[1,0])\naxs[1,0].get_legend().remove()\n\naxs[1,1].set_title('Age', fontsize=18)\nsns.kdeplot(train.Age, shade=True, ax=axs[1,1])\naxs[1,1].get_legend().remove()","060f852b":"def corrfunc(x, y, **kws):\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.1, .9), xycoords=ax.transAxes)\n\ng = sns.PairGrid(train, palette=[\"red\"])\ng.map_upper(plt.scatter, s=10)\ng.map_diag(sns.distplot, kde=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_lower(corrfunc)","82978bb1":"id_patients_most_weeks = train.groupby('Patient').Weeks.count().sort_values(ascending=False).iloc[:5].index\ntrain_patients = train[train.Patient.isin(id_patients_most_weeks)].sort_values('Weeks')\n\nfig, ax = plt.subplots(figsize=(16,6))\n\nfor name, group in train_patients.groupby('Patient'):\n    color = next(ax._get_lines.prop_cycler)['color']\n    group.sort_values('Weeks').plot(x='Weeks', y='FVC', ax=ax, label=name, color=color)\n    reg = LinearRegression().fit(np.array(group.Weeks).reshape(-1, 1), np.array(group.FVC))\n    ax.plot(group.Weeks,reg.predict(np.array(group.Weeks).reshape(-1, 1)),'--', color=color)\n    \nax.set_title('Progress of patients with the most FVC measurements', fontsize=18)\nax.set_xlabel('Weeks', fontsize=12)\nax.set_ylabel('FVC', fontsize=12);","6369b0fa":"example_patient_file_path = \"..\/input\/osic-pulmonary-fibrosis-progression\/train\/ID00014637202177757139317\/\"","42898206":"num_scans = {}\nfor patient_id in train.Patient.unique():\n    files = glob.glob(\"..\/input\/osic-pulmonary-fibrosis-progression\/train\/%s\/*.dcm\"%patient_id)\n    num_scans[patient_id] = len(files)\ndf_scans = pd.DataFrame.from_dict(num_scans, orient='index', columns=['num_scans'])\ndf_scans.index = df_scans.index.rename('Patient_id')\ndf_scans = df_scans.sort_values('num_scans')\n\nfig, axs = plt.subplots(2, 1, figsize=(15, 10))\nfig.suptitle('Number of CT-Scans for every Patient id', fontsize=16)\nbar = sns.barplot(x=df_scans.index, y=df_scans.num_scans, ax=axs[0])\nbar.axes.get_xaxis().set_visible(False)\nbar.set_ylabel(\"Number of CT-Scan Files\")\n\ndense = sns.kdeplot(df_scans.num_scans, bw=50, ax=axs[1])\ndense.get_legend().remove()\ndense.set_xlabel('Numober of CT-Scan Files');","bc53f01d":"df_scans.num_scans.describe()","c87865a5":"#https:\/\/github.com\/pydicom\/pydicom\/issues\/319\ndef dictify(ds):\n    \"\"\"Turn a pydicom Dataset into a dict with keys derived from the Element tags.\n\n    Parameters\n    ----------\n    ds : pydicom.dataset.Dataset\n        The Dataset to dictify\n\n    Returns\n    -------\n    output : dict\n    \"\"\"\n    output = dict()\n    for elem in ds:\n        # skip the image data\n        if elem.name=='Pixel Data':\n            continue\n        if elem.VR != 'SQ':\n            output[elem.name] = elem.value\n        else:\n            output[elem.name] = [dictify(item) for item in elem]\n    return output","fd45af7d":"ds1 = dictify(pydicom.filereader.dcmread(example_patient_file_path+\"1.dcm\"))\nds2 = dictify(pydicom.filereader.dcmread(example_patient_file_path+\"2.dcm\"))\nds1","2b5de705":"def get_diff(ds1, ds2):\n    diff_keys = []\n    for key in ds1.keys():\n        if ds1[key]!=ds2[key]:\n            diff_keys.append(key)\n    return diff_keys\n\nchanging_keys = get_diff(ds1, ds2)\nstable_keys = ds1.keys()-changing_keys","7ef4100c":"stable_keys","fccab634":"additional_metadata = []\nfor patient_id in tqdm(train.Patient.unique()):\n    file = glob.glob(\"..\/input\/osic-pulmonary-fibrosis-progression\/train\/%s\/*.dcm\"%patient_id)[0]\n    ds_dict = dictify(pydicom.filereader.dcmread(file))\n    metadata = {k:ds_dict[k] for k in stable_keys if k in ds_dict}\n    additional_metadata.append(metadata)\n    \ndf_add_meta = pd.DataFrame(additional_metadata)","09e380ae":"for column in df_add_meta.columns:\n    if df_add_meta[column].dtypes!='float64':\n        try:\n            print(column, \":\", df_add_meta[column].unique()[:5], \"-\", df_add_meta[column].dtypes)\n        except:\n            print(column, \":\", df_add_meta[column].iloc[:10].values, \"-\", df_add_meta[column].dtypes)\n        print(\"-\"*100)","fb5e2465":"df_add_meta['Convolution Kernel'].values","812793c6":"column = 'Convolution Kernel'\n# strange way to get the single faulty value but it works \ndf_add_meta[column][(pd.isna(df_add_meta[column].str.contains('I50f'))).values] ='I50f'","ccf17774":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncolumns_to_keep = [\"Patient Position\", \"Patient ID\", \"Manufacturer\", \"Convolution Kernel\"]\ncolumns_to_keep = list(df_add_meta.select_dtypes(include=numerics).columns.values) + columns_to_keep\ndf_add_meta = df_add_meta[columns_to_keep]","de312277":"perc_missing_cols = (df_add_meta.isna().sum()\/len(df_add_meta)).sort_values(ascending=False)\nprint(\"Percentage of missing values in each column:\")\nprint(\"-\"*50)\nprint(perc_missing_cols[perc_missing_cols!=0])","96f2511f":"perc_missing_cols = perc_missing_cols[perc_missing_cols<0.1]\ndf_add_meta = df_add_meta[df_add_meta.columns.intersection(list(perc_missing_cols.index))]","df387a8e":"df_train_join = pd.merge(train, df_add_meta, left_on='Patient', right_on=\"Patient ID\", how='inner')","68d9bd84":"perc_missing_cols = (df_train_join.isna().sum()\/len(df_train_join)).sort_values(ascending=False)\nprint(\"Percentage of missing values in each column:\")\nprint(\"-\"*50)\nprint(perc_missing_cols[perc_missing_cols!=0])","2a29fabc":"imputer = MissForest()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n# create a new dataframe containing only numeric values which can be imputed\ndf_train_join_numeric = df_train_join.select_dtypes(include=numerics)\nimputed_matrix = imputer.fit_transform(df_train_join_numeric)\n# replace imputed columns in the df_add_meta dataframe\ndf_train_join_numeric = pd.DataFrame(imputed_matrix, index=df_train_join_numeric.index, columns=df_train_join_numeric.columns)\nfor column in df_train_join_numeric.columns:\n    df_train_join[column] = df_train_join_numeric[column]","e569a795":"for column in df_train_join.select_dtypes(include=numerics).columns:\n    if(df_train_join[column].apply(float.is_integer).all()):\n        df_train_join[column] = df_train_join[column].astype(int)","6624121b":"def evaluate_features(X_train, y_train, X_test, y_test, params, metrices):\n    \"\"\"\n    Trains a simple gradient boosting model and evaluates its feature importances (if multiple columns provided).\n    Furthermore the trained model is evaluated with the provided metric(es).\n    :param X_train:\n    :param y_train:\n    :param X_test:\n    :param y_test:\n    :param params:\n    :param metrices:\n    :return:\n    \"\"\"\n\n    for col in X_train.select_dtypes(include='object').columns:\n        le = LabelEncoder()\n        le.fit(list(X_train[col].astype(str).values) + list(X_test[col].astype(str).values))\n        X_train[col] = le.transform(list(X_train[col].astype(str).values))\n        X_test[col] = le.transform(list(X_test[col].astype(str).values))\n\n\n    clf = lgb.LGBMRegressor(**params)\n    clf.fit(X_train.values, y_train)\n\n    importances = clf.feature_importances_\n    indices = np.argsort(importances)[::-1]\n\n    features_to_show = len(X_train.columns)\n\n    plt.figure(figsize=(15,10))\n    plt.title(\"Feature importances\")\n    plt.bar(range(features_to_show), importances[indices][:features_to_show],\n            color=\"r\", align=\"center\")\n    feature_names = [X_train.columns[indices[f]] for f in range(features_to_show)]\n    plt.xticks(range(features_to_show), feature_names, rotation='vertical')\n    plt.xlim([-1, features_to_show])\n    plt.show()\n\n    scores = get_model_scores(clf, X_train, y_train, X_test, y_test, metrices, True)\n\n    df_feature_importance = pd.DataFrame({'column':X_train.columns[indices], 'importance':importances[indices]})\n    return (df_feature_importance, scores)\n\ndef get_model_scores(model, x_train, y_train, x_test, y_test, metrices, print_values=True):\n    scores = {}\n    for metric in metrices:\n        try:\n            score_train = metric(model.predict(x_train), y_train)\n            score_test = metric(model.predict(x_test), y_test)\n            if print_values:\n                print(metric.__name__, \"(train):\", score_train)\n                print(metric.__name__, \"(test):\", score_test)\n                print(\"------------------------------------------------------------\")\n            scores[metric.__name__] = [score_train, score_test]\n        except:\n            print(\"Could not calculate score\", metric.__name__)\n            print(\"------------------------------------------------------------\")\n            scores[metric.__name__] = [None, None]\n    return scores","5c23d43c":"X_train, X_test, y_train, y_test = train_test_split(df_train_join.drop(columns=['FVC']), df_train_join.FVC)","a302d763":"evaluate_features(X_train, y_train, X_test, y_test, {}, [mean_squared_error])","4775695b":"df_train_join.to_csv('train_merged_and_cleaned.csv', index=False)","a2ba31e7":"def center_crop(img, new_width=512, new_height=512):        \n\n    width = img.shape[1]\n    height = img.shape[0]\n\n    if new_width is None:\n        new_width = min(width, height)\n\n    if new_height is None:\n        new_height = min(width, height)\n\n    left = int(np.ceil((width - new_width) \/ 2))\n    right = width - int(np.floor((width - new_width) \/ 2))\n\n    top = int(np.ceil((height - new_height) \/ 2))\n    bottom = height - int(np.floor((height - new_height) \/ 2))\n\n    if len(img.shape) == 2:\n        center_cropped_img = img[top:bottom, left:right]\n    else:\n        center_cropped_img = img[top:bottom, left:right, ...]\n\n    return center_cropped_img\n\ndef slices_as_gif(slices, fps=10):\n    fig = plt.figure()\n    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n    plt.axis('off')\n    slices = [[plt.imshow(img, cmap='gray')] for img in slices]\n    ani = animation.ArtistAnimation(fig, slices, interval=200, repeat_delay=0)\n    ani.save('test_anim.gif', writer='imagemagick', fps=fps)\n    plt.close()\n    return 'test_anim.gif'\n    \ndef read_dicoms(file, with_mask=False):\n    img = pydicom.filereader.dcmread(file).pixel_array\n    img = center_crop(img)\n    if with_mask:\n        img = make_lungmask(img)\n    return img","76d21907":"files = glob.glob(example_patient_file_path+\"*.dcm\")\nfiles = sorted(files, key=lambda x:float(re.findall(\"(\\d+)\",x)[-1]))\nfig=plt.figure(figsize=(18, 6))\ncolumns = 10\nrows = 3\nfor i in range(30):\n    ds = pydicom.filereader.dcmread(files[i])\n    fig.add_subplot(rows, columns, i+1)\n    plt.imshow(center_crop(ds.pixel_array), cmap='gray')\n    plt.title(os.path.basename(files[i]))\n    plt.axis('off')\n    plt.grid(b=None)","664bb4a3":"slices = [read_dicoms(file) for file in files]\nshow_gif(filename=slices_as_gif(slices), format='png', width=512, height=512)","e85ce5ea":"## I. Duplicates","18356844":"Most patients have 9 oberservations in the dataset providing FVC values over time. ","a293d423":"### I. Extract ","15d01585":"## II. Metadata","56fa13d5":"## I. General <a id='dicom_general'><\/a>","a47d6ee0":"Next we need to check if those columns contain new missing values. This can happen when there is no value(s) for a patient. ","7ec715d2":"Looks like the dataset contains duplicate rows with different FVC and Percent values. We remove those rows by calculating the average for FVC and Percent","a86e2bfe":"# \ud83e\ude7a EDA \ud83d\udcca","b7a58247":"So none of the columns contain any missing values! ","ec3a017a":"# III. Investigate the progress of some patients <a id='progress'><\/a>","91047c37":"Now that we have a \"cleaned\" dataset from the additional metadata we can join them directly over the patient id.","4a921caf":"In this Notebook we get to know the data we are working with in this competition. The main steps are:\n\nI. [Clean tabular data](#clean_data)\n\nII. [Information about patients](#patients)\n\nIII. [Investigate the progress of some patients](#progress)\n\nIV. [Data from DICOM files](#dicom)","5ab85e59":"Based on the feature importance of the Regression Tree many additional metadata attributes seem to hold a certain level of information to predict the <font size=\"4\">**current**<\/font> FVC value. ","e9b3a9ec":"# II. Information about patients <a id='patients'><\/a>","be1ee5b8":"Lets have a first look at the data.","cda90fd1":"Many colums seem to have too many missing values as they would contain any relevant information. Therefore all columns are dropped that contain more than 10% missing values.","8ccb0728":"# I. Clean tabular data <a id='clean_data'><\/a>","393577e5":"# IV. Data from DICOM files <a id='dicom'><\/a>","1ecb90da":"## II. Missing data","d45536be":"In each DICOM-File a lot of metadata can be found. The basic idea of the next steps is to determine attributes that don't change over DICOM-Files of the same patient and extract additional features that can be merged with the train.csv","9e3c396f":"Impute missing values using the MissForest-Algorithm","5de30df6":"As seen in the correlation plot a certain correlation between FVC and Percent can be observed (0.67).","9f6f576b":"## III. Image data","a1bf174a":"Next we look at the provided image data from the DICOM-Files.","09ea8041":"We can see a steady decline in all patient lungs over time, as expected.","4ac8f3f3":"There are **49** attributes that (propably) don't change over image files from the same patient. Now we extract all of those attributes by reading the first file of each patient. The attributes are stored in a data frame which will later get merged with the provided train.csv file","6afb014e":"**Now that we have an overview of the data we can proceed with preprocessing the data and the images!**\n\n**Click [here](https:\/\/www.kaggle.com\/foodaholic\/image-preprocessing-tfrecords-with-3d-scan-2-3\/edit) for my following notebook**","0f2d1b42":"After a closer look of non numeric column some of them seem to have strange values. Take \"Convolution Kernel\" as an example:\n\nPandas reports that no unique values can be extracted because the values are of type 'MultiValue'. But if we show all values we can see that there is only one faulty entry","82f09346":"The last step is now to reduce the memory size of the dataset by changing columns that are of type float to int if the column contains only integer values","8b673001":"### Check for missing values","52dfb8d9":"One entry contains an array with an additional value. We can fix this by only using the first element of the array as value."}}