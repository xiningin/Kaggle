{"cell_type":{"3deff225":"code","63c9730c":"code","6bce2bf3":"code","ee051867":"code","1205e1fc":"code","267fc750":"code","e04e2d26":"code","d4fde356":"code","c9bae2b2":"code","41105f8d":"code","a9028f8a":"code","dc8f778f":"code","83957dc7":"code","0c6da15f":"code","adaee525":"code","c56bee85":"code","e5ddefa2":"code","b0394d9e":"code","f5d12720":"code","3ab0fdb6":"code","d643d1e3":"code","305b80a2":"code","2e1fa674":"code","c3169989":"code","33b76dee":"code","99fcd6a3":"code","d3ef363a":"code","433d97b6":"code","08cffde2":"code","4aebf14d":"code","3f2bbedc":"code","5f01cd3c":"markdown","55b4bc0e":"markdown","a7e0ae8e":"markdown","03a8f77a":"markdown","da45ea73":"markdown","e4c2d997":"markdown","1daf8c34":"markdown","f17f00f0":"markdown","82fc27f9":"markdown","dc37f250":"markdown","1ceaab61":"markdown","b3d6f299":"markdown","d6e6ab2f":"markdown","e0207450":"markdown","13ffb0ac":"markdown","eb5d3a16":"markdown","8025e4c5":"markdown"},"source":{"3deff225":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\n\n\nfrom wordcloud import WordCloud\n\n\nfrom collections import Counter\nimport os\nimport numpy as np\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.util import ngrams    \n\nimport html\nimport unicodedata\n\nstop_words = stopwords.words('english')\n%config InlineBackend.figure_format = 'retina'\n","63c9730c":"def wordcloud(text,ngram=1):\n    wordcloud = WordCloud(width=1400, \n                            height=800,\n                            random_state=2021,\n                            background_color='black',\n                            )\n    if ngram ==1:\n        wordc = wordcloud.generate(' '.join(text))\n    else:\n        wordc = wordcloud.generate_from_frequencies(text)\n    plt.figure(figsize=(12,6), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n      \n\ndef get_n_grans_count(text, n_grams, min_freq):\n    output = {}\n    tokens = nltk.word_tokenize(text)\n\n    #Create the n_gram\n    if n_grams == 2:\n        gs = nltk.bigrams(tokens)\n        \n    elif n_grams == 3:\n        gs = nltk.trigrams(tokens)\n\n    else:\n        return 'Only 2_grams and 3_grams are supported'\n    \n    # compute frequency distribution for all the bigrams in the text by threshold with min_freq\n    fdist = nltk.FreqDist(gs)\n    for k,v in fdist.items():\n        if v > min_freq:\n            index = ' '.join(k)\n            output[index] = v\n    \n    return output\n    \ndef remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text, and by defult lemmatize nouns\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem or lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","6bce2bf3":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n\ntrain_data.head()","ee051867":"train_data.describe()","1205e1fc":"# showing if any feature has at least one null value\ntrain_data.isnull().any()","267fc750":"# count null values\ntrain_data.isnull().sum()","e04e2d26":"fig, ax = plt.subplots(1,2,figsize=(12,7))\nsns.histplot(train_data['target'], kde= True, ax=ax[0])\nsns.histplot(train_data['standard_error'], kde= True, ax=ax[1])\nax[0].set_title(\"Target Distribution\")\nax[1].set_title(\"Standard Error Distribution\")\nplt.show();","d4fde356":"train_data['license'].value_counts()","c9bae2b2":"plt.figure(figsize=(12, 6))\nsns.countplot(data= train_data, y= 'license')\nplt.title('License Distribution')\nplt.show();","41105f8d":"# showing the shaper of url's\nurls = train_data['url_legal'].dropna()\nurls = [url for url in urls]\nurls[:5]","a9028f8a":"# Extract all url's\nurl_list = train_data['url_legal'].dropna().apply(lambda x : re.findall('https?:\/\/([A-Za-z_0-9.-]+).*',x)[0])\nurl_list = [url for url in url_list]\nurl_list[:10]","dc8f778f":"# count url's and sort them descending order \nurls_counts = Counter(url_list)\nurls_counts_sorted = sorted(urls_counts.items(), key=lambda pair: pair[1], reverse=True)\nurls_counts_df = pd.DataFrame(urls_counts_sorted, columns=['sites', 'counts'])\nurls_counts_df","83957dc7":"plt.figure(figsize=(12, 6))\nsns.barplot(data= urls_counts_df, x= 'counts', y= 'sites')\nplt.title('Unique Sites count')\nplt.show();","0c6da15f":"train_data['excerpt'][0]","adaee525":"normalize_text(train_data['excerpt'][0])","c56bee85":"train_data['clean_text'] = [normalize_text(sent) for sent in train_data['excerpt']]\ntrain_data.head()","e5ddefa2":"# Also we should make text preprocessing on text data\ntest_data['excerpt'] = [normalize_text(sent) for sent in test_data['excerpt']]","b0394d9e":"# make all clear sentence as a huge text, then tokenize it\nwords_list = text2words(''.join(sents for sents in train_data['clean_text']))\nwords_list[:10]\n","f5d12720":"# Number of words we have\nlen(words_list)","3ab0fdb6":"# frequent of the most 30 words\nwords_list_freq = Counter(words_list)\nwords_list_freq_sorted = sorted(words_list_freq.items(), key=lambda pair: pair[1], reverse=True)\n\nwords_list_freq_sorted_df = pd.DataFrame(words_list_freq_sorted, columns=['words', 'counts'])[:30]\nwords_list_freq_sorted_df.head()","d643d1e3":"plt.figure(figsize=(12, 6))\nsns.barplot(data= words_list_freq_sorted_df, y= 'words', x= 'counts')\nplt.title('Top 30 frequent words')\nplt.show();","305b80a2":"wordcloud(train_data['excerpt'])","2e1fa674":"text= ' '.join(setns for setns in train_data['clean_text'])","c3169989":"two_grams = get_n_grans_count(text, n_grams=2, min_freq=10)\ntwo_grams_df = pd.DataFrame(two_grams.items(), columns= ['two_grams', 'counts']).sort_values(by='counts',ascending=False)\ntwo_grams_df.head()","33b76dee":"plt.figure(figsize=(12, 6))\nsns.barplot(data= two_grams_df[:30], y= 'two_grams', x= 'counts')\nplt.title('Top 30 frequent bigram')\nplt.show();","99fcd6a3":"two_grams_wordcloud = {w.replace(' ','_'): c for w,c in two_grams.items()}\nwordcloud(two_grams_wordcloud,ngram=2)","d3ef363a":"X = train_data['clean_text']\ny = train_data['target']\nX_train, X_valid, y_train, y_valid =  train_test_split(X, y, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","433d97b6":"print (X_train.shape)\nprint (X_valid.shape)","08cffde2":"# Make an Sklearn pipeline for this Ridge Regression\nridge = Ridge(fit_intercept=True, normalize=False)\n\nridge_pipline = make_pipeline(\n    TfidfVectorizer(binary= True, min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n    , ridge)\n\n# training\nridge_pipline.fit(X_train, y_train)\n\n# Evaluation\ny_pred = ridge_pipline.predict(X_valid)\nmse_loss = mean_squared_error(y_pred, y_valid)\n\nprint(f\"MSE Loss using Ridge and TfIdfVectorizer: {mse_loss}\")","4aebf14d":"# Make an Sklearn pipeline for this xgboost Regression\nxgboost = xgb.XGBRegressor(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n\nxgboost_pipline = make_pipeline(\n    TfidfVectorizer(binary= True, min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n    , xgboost)\n\n# training\nxgboost_pipline.fit(X_train, y_train)\n\n# Evaluation\ny_pred = xgboost_pipline.predict(X_valid)\nmse_loss = mean_squared_error(y_pred, y_valid)\n\nprint(f\"MSE Loss using xgboost and TfIdfVectorizer: {mse_loss}\")","3f2bbedc":"test_text = test_data['excerpt']\ntest_pred = ridge_pipline.predict(test_text)\n\nsubmission = pd.DataFrame()\nsubmission['id'] = test_data['id']\nsubmission['target'] = test_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","5f01cd3c":"## Url legal","55b4bc0e":"**Frequent words**","a7e0ae8e":"### Submission","03a8f77a":"**The original text**","da45ea73":"**Bigrams**","e4c2d997":"**Word Cloud for all words**","1daf8c34":"**Seems like no luck with XGBoost!**","f17f00f0":"**Goal:building algorithms to rate the complexity of reading passages for grade 3-12 classroom use**","82fc27f9":"Before going further it is important that we split the data into training and validation sets. We can do it using train_test_split from the model_selection module of scikit-learn.","dc37f250":"## EDA","1ceaab61":"### Modeling","b3d6f299":"## Target and Standard Error Distributions","d6e6ab2f":"**The cleaned text**","e0207450":"**Adding cleat text in the data frame**","13ffb0ac":"## excerpt","eb5d3a16":"**Our first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.**","8025e4c5":"## license"}}