{"cell_type":{"af5138b1":"code","2126e1e4":"code","1ce3edcc":"code","f77b6ab2":"code","60fab40f":"code","eac9277a":"code","56ce6a7f":"code","f71a33d7":"code","883fb13d":"code","db22347a":"code","c37bcaf8":"code","e067306b":"code","59dab185":"code","b1b9ec73":"code","e1232d47":"code","b9926390":"code","5d135649":"markdown","54fcef2b":"markdown","5c0b8492":"markdown","ce41270a":"markdown","432c8f9b":"markdown","3ab761ea":"markdown"},"source":{"af5138b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2126e1e4":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/train.csv')\ndata.head()","1ce3edcc":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28*28, 400)\n        self.fc2 = nn.Linear(400, 400)\n        self.fc3 = nn.Linear(400, 10)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=1)\n\n\nnet = Net()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnet.cuda(device=device)","f77b6ab2":"import pycuda.driver as cuda\ncuda.init()","60fab40f":"import torch.optim as optim\n\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n# create a loss function\ncriterion = nn.NLLLoss()","eac9277a":"data_train = data.head(40000)\ndata_test = data.tail(4000)\n\nX_train = data_train.drop('label', axis=1)\nX_test = data_test.drop('label', axis=1)\n\ny_train = data_train['label']\ny_test = data_test['label']","56ce6a7f":"import torch\nepochs = 1000\n\n\ndata, target = torch.tensor(X_train.values, dtype=torch.float).cuda()\/255, torch.tensor(y_train.values).cuda()\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    net_out = net(data)\n    loss = criterion(net_out, target)\n    loss.backward()\n    optimizer.step()","f71a33d7":"net_out = net(data)\nloss = criterion(net_out, target)\nloss.data","883fb13d":"data, target = torch.tensor(X_test.values, dtype=torch.float).cuda()\/255, torch.tensor(y_test.values).cuda()\nnet_out = net(data)\ntest_loss = criterion(net_out, target).data\npred = net_out.data.max(1)[1]  # get the index of the max log-probability\ncorrect = pred.eq(target.data).sum()\n\nprint('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n    test_loss, correct, len(data),\n    100. * correct \/ len(data)))","db22347a":"test = pd.read_csv('..\/input\/test.csv')\ntest.head()","c37bcaf8":"test_tensor = torch.tensor(test.values, dtype=torch.float).cuda()\/255\nnet_out = net(test_tensor)\nnet_out","e067306b":"labels = net_out.data.max(1)[1]","59dab185":"labels = labels.cpu().numpy()","b1b9ec73":"test.index.values","e1232d47":"submission = pd.DataFrame({'ImageId': pd.Series(range(1,28001)), 'Label': labels})\nsubmission.head()","b9926390":"submission.to_csv('submission.csv', index=False)","5d135649":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 test.csv \u0432 test \u0438 \u0432\u044b\u0432\u043e\u0434 \u043f\u0435\u0440\u0432\u044b\u0445 5 \u0434\u0430\u043d\u043d\u044b\u0445","54fcef2b":"# \u041b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u043d\u0430\u044f \u0440\u0430\u0431\u043e\u0442\u0430 2","5c0b8492":"# \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u043d\u0430 \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0445 \u0438\u0437 train \u0434\u0430\u043d\u043d\u044b\u0445; 1000 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439","ce41270a":"# \u0418\u043c\u043f\u043e\u0440\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0434\u043b\u044f \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u0443\u0441\u0442\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438","432c8f9b":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 train.csv \u0432 data \u0438 \u0432\u044b\u0432\u043e\u0434 \u043f\u0435\u0440\u0432\u044b\u0445 5 \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","3ab761ea":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438"}}