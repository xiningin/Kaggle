{"cell_type":{"b9151d6c":"code","eae3ba9f":"code","8910a684":"code","cd914c7a":"code","a6169e22":"code","3e242079":"code","efb1e0fd":"code","3f3578ca":"code","e9ad7055":"code","5c0bfc79":"code","6f3ad89e":"code","23f94b0b":"code","09fcd719":"code","ca80aacc":"code","b7e6b8df":"code","adb912b4":"code","31250643":"code","886235b3":"code","9fa3eeac":"code","8f9a6f59":"code","4bad92f3":"code","76ec2e44":"code","20fec504":"code","f049651b":"code","e99e30b3":"code","74bed74d":"code","26c93e77":"code","e4c71b74":"code","9367240a":"code","8a9afdf4":"code","21f53465":"markdown","2306b693":"markdown","3fddf48b":"markdown","50685448":"markdown","8b69437e":"markdown","11c354ce":"markdown","150bf0b2":"markdown","7c36daad":"markdown","9490c19a":"markdown","6711638a":"markdown","ba82976b":"markdown","9e241ac7":"markdown","7e28e847":"markdown","8073b32f":"markdown","1fc178bf":"markdown","05f7f7f5":"markdown","55c0670f":"markdown","960b1b03":"markdown","d8803266":"markdown","14ec5efc":"markdown","dfd67ea3":"markdown","186a62b8":"markdown","a787479c":"markdown"},"source":{"b9151d6c":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n#!pip install -U pandas_profiling\nimport pandas_profiling\nimport warnings\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.ensemble import StackingRegressor, RandomForestRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR \nimport seaborn as sns\nfrom scipy.stats import boxcox, skew\nfrom pyearth import Earth\nfrom catboost import CatBoostRegressor","eae3ba9f":"train_file_path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\ntest_file_path = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ndf_train = pd.read_csv(train_file_path).drop(columns = ['Id'])\ndf_test = pd.read_csv(test_file_path)\ntest_Id = df_test['Id'] #for submission\ndf_test = df_test.drop(columns = ['Id'])","8910a684":"outliers = [30, 88, 328, 410, 462, 495, 523, 588, 632, 688, 710, 728, 874, 916, 968, 970, 1298, 1324, 1432, 1453]\n#outliers = [30, 88, 462, 523, 632, 1298, 1324]\ndf_train = df_train.drop(outliers).reset_index(drop=True)","cd914c7a":"frac = 0.8\nmissing_val_features = [(feature, df_train[feature].isnull().sum()\/df_train.shape[0]) for feature in df_train.columns if df_train[feature].isnull().sum() >= frac*df_train.shape[0]]\n#print(missing_val_features)\nmissing_val_features = [f for (f,i) in missing_val_features]\ndf_train = df_train.drop(missing_val_features,axis = 1)\ndf_test = df_test.drop(missing_val_features,axis = 1)\n\nfrac = 0.95\nzero_val_features = [(feature, (df_train[feature] == 0).sum()\/df_train.shape[0]) for feature in df_train.columns if (df_train[feature] == 0).sum() >= frac*df_train.shape[0]]\n#print(zero_val_features)\nzero_val_features = [f for (f,i) in zero_val_features]\n\ndf_train = df_train.drop(zero_val_features, axis = 1)\ndf_test = df_test.drop(zero_val_features, axis = 1)","a6169e22":"#Replace year with how much time has passed since\n\nyear_features = [feature for feature in df_train.columns if \"Yr\" in feature or \"Year\" in feature]\nprint(year_features)\nnew_names = ['Age', 'RemodAge','GarageAge', 'YrsSinceSell']\nfor i in range(4):\n    feature, new_feature = year_features[i], new_names[i]\n    \n    df_train[feature].replace(0,np.nan)\n    df_train.insert(0, new_feature, df_train[feature].max() - df_train[feature])\n    df_train = df_train.drop(feature,axis=1)\n    \n    df_test[feature].replace(0,np.nan)\n    df_test.insert(0, new_feature, df_test[feature].max() - df_test[feature])\n    df_test = df_test.drop(feature,axis=1)\n    \nfor df in [df_train, df_test]:\n    df['Total_SF'] = (df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'])\n    df['Age(House+Remod)'] = df['Age'] + df['RemodAge']\n    #df['Total_SF'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'])\n    df['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n    df['Total_porch_SF'] = (df['OpenPorchSF'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])","3e242079":"num_features = list(df_train.select_dtypes(include = 'number').columns)\nthr = 10\ncat_candidates = [feature for feature in num_features if df_train[feature].nunique() < thr]\nprint(cat_candidates)\nhist = df_train[cat_candidates].hist(figsize = (12,10))","efb1e0fd":"def group_values(df, bounds):\n    \"\"\" Replaces the values of df by 0,...,len(bounds)\n    so that values below bounds[0] will be 0, etc\"\"\"\n    return df.apply(lambda x: sum(x >= np.array(bounds)))\n\nfor df in [df_train, df_test]:\n    df['BsmtFullBath'] = group_values(df['BsmtFullBath'],[1])\n    df['BsmtFullBath'] = df['BsmtFullBath'].astype(str)\n    \n    df['BsmtHalfBath'] = group_values(df['BsmtHalfBath'],[1])\n    df['BsmtHalfBath'] = df['BsmtHalfBath'].astype(str)\n    \n    df['Fireplaces'] = group_values(df['Fireplaces'],[1,2])\n    df['Fireplaces'] = df['Fireplaces'].astype(str)\n    \n    df['FullBath'] = group_values(df['FullBath'],[1,2,3])\n    df['FullBath'] = df['FullBath'].astype(str)\n    \n    df['GarageCars'] = group_values(df['GarageCars'],[1,2,3])\n    df['GarageCars'] = df['GarageCars'].astype(str)\n    \n    df['HalfBath'] = group_values(df['HalfBath'],[1])\n    df['HalfBath'] = df['HalfBath'].astype(str)\n    \n    df['KitchenAbvGr'] = group_values(df['KitchenAbvGr'],[1])\n    df['KitchenAbvGr'] = df['KitchenAbvGr'].astype(str)\n    \n    df['YrsSinceSell'] = group_values(df['YrsSinceSell'],[1,2,3,4])\n    df['YrsSinceSell'] = df['YrsSinceSell'].astype(str)\n    \n    df['BedroomAbvGr'] = group_values(df['BedroomAbvGr'],[2,3,4,5])\n    df['BedroomAbvGr'] = df['BedroomAbvGr'].astype(str)","3f3578ca":"num_features = list(df_train.select_dtypes(include = 'number').columns)\nnum_features.remove('SalePrice')\ncat_features = list(df_train.select_dtypes(include = 'object').columns)\nfeatures = list(df_train.columns)\nfeatures.remove('SalePrice')\n\nprint('There are', len(num_features), 'numerical features')\nprint('There are', len(cat_features), 'categorical features')\nprint('There are', len(features), 'features')\n\nnum_imputer = SimpleImputer(strategy='median')\nnum_imputer.fit(df_train[num_features])\ndf_train[num_features] = num_imputer.transform(df_train[num_features])\ndf_test[num_features] = num_imputer.transform(df_test[num_features])\ndf_train[cat_features] = df_train[cat_features].fillna('None')\ndf_test[cat_features] = df_test[cat_features].fillna('None')","e9ad7055":"for feature in num_features:\n    if skew(df_train[feature]) >= 0.5:\n        df_train[feature], best_lambda = boxcox(1 + df_train[feature])\n        df_test[feature] = boxcox(1 + df_test[feature], best_lambda)","5c0bfc79":"n_train = df_train.shape[0]\ny = np.log(df_train['SalePrice'].values)\ndf_train = df_train.drop(['SalePrice'], axis=1)\ndf_all = pd.concat([df_train, df_test], axis=0)\ndf_all = pd.get_dummies(df_all)\ndf_train_dummies, df_test_dummies = df_all[:n_train], df_all[n_train:]\n\nfrac = 0.997\nfor feature in df_train_dummies.columns:\n    zero_frac = (df_train_dummies[feature] == 0).sum()\/n_train\n    if (zero_frac >= frac):\n        df_train_dummies = df_train_dummies.drop(feature, axis = 1)\n        df_test_dummies = df_test_dummies.drop(feature, axis = 1)\n        \nX = df_train_dummies.values\nnum_cols = list(range(len(num_features)))\ncat_cols = list(range(len(num_features),len(features)))","6f3ad89e":"def rmse(y1, y2):\n    return np.sqrt(mean_squared_error(y1, y2))\n\ndef rmse_model(model, X, y):\n    y_pred = model.predict(X)\n    return rmse(y,y_pred)\n\ndef examine_model(model, X, y, pipeline=False, features = [], coeffs=True):\n    \n    #X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3, shuffle=True)\n    rmse_val = np.sqrt(-np.mean(cross_val_score(model, X, y, scoring = 'neg_mean_squared_error', cv=5)))\n    \n    reg = model.fit(X, y)\n    rmse_train = rmse_model(reg, X, y)\n    print(\"Train RMSE: \", rmse_train, \"Validation RMSE: \", rmse_val)\n    \n    if pipeline: reg = reg[-1]\n    if features:\n        coeffs = list(zip(features, reg.coef_))\n        coeffs.sort(key = lambda tup: np.abs(tup[1]), reverse=True)\n        print(\"Important Features: \", coeffs[:5])\n        \n    if coeffs:\n        print(sum(1 for coeff in reg.coef_ if coeff==0), '\/', len(reg.coef_), ' features are 0')\n\n    return None","23f94b0b":"linear_model = LinearRegression()\nexamine_model(linear_model, X, y)","09fcd719":"alphas = [10, 30, 60, 100, 140, 190]\nridge_cv = make_pipeline(StandardScaler(), RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=5))\nridge_cv.fit(X,y)\nbest_alpha = ridge_cv[-1].alpha_\nprint('Best alpha: ', best_alpha)\n\nridge = make_pipeline(StandardScaler(), Ridge(alpha=best_alpha))\nexamine_model(ridge, X, y, pipeline=True)","ca80aacc":"#ridge.fit(X,y)\n#predictions = ridge.predict(X)\n#pred_error = y-predictions\n#plt.boxplot(pred_error)\n#thr = np.std(pred_error)*4.2\n#mu = np.mean(pred_error)\n#outliers = [i for i in range(pred_error.size) if np.abs(pred_error[i])-mu > thr]\n#print(outliers)","b7e6b8df":"lasso_cv = make_pipeline(StandardScaler(), LassoCV(n_alphas=100, cv=5))\nlasso_cv.fit(X,y)\nbest_alpha = lasso_cv[-1].alpha_\nprint('Best alpha: ', best_alpha)\n\nlasso = make_pipeline(StandardScaler(), Lasso(alpha=best_alpha))\nexamine_model(lasso, X, y, pipeline=True)","adb912b4":"l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\nnet_cv = make_pipeline(StandardScaler(), ElasticNetCV(l1_ratio=l1_ratios, n_alphas=100, cv=5, max_iter=3000))\nnet_cv.fit(X,y)\nbest_alpha_net = net_cv[-1].alpha_\nprint('Best alpha: ', best_alpha_net)\nbest_l1_ratio_net = net_cv[-1].l1_ratio_\nprint('Best l1_ratio: ', best_l1_ratio_net)\n\nnet = make_pipeline(StandardScaler(), ElasticNet(l1_ratio=best_l1_ratio_net, alpha=best_alpha_net, max_iter=6000))\nexamine_model(net, X, y, pipeline=True)","31250643":"kernel_ridge = make_pipeline(StandardScaler(), KernelRidge(kernel='poly', alpha=1))\nexamine_model(kernel_ridge, X, y, pipeline=True, coeffs=False)","886235b3":"svr = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=10, epsilon=0.01, gamma=0.00025))\nexamine_model(svr, X, y, pipeline=True, coeffs=False)","9fa3eeac":"class BasisExpander(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms the data by adding extra features of the form h(feature)\n    for each basis expansion fucntion `h` and numerical feature `feature`\n    \"\"\"\n    def __init__(self, numerical_columns, basis_exps):\n        self.numerical_columns = numerical_columns\n        self.basis_exps = basis_exps\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        nr_basis_exps = len(self.basis_exps)\n        X_num = X[:, self.numerical_columns]\n        n, m = X_num.shape\n        X_be = np.zeros((n, nr_basis_exps*m))\n\n        for i,f in enumerate(self.basis_exps):\n            X_be[:, i*m:(i+1)*m] = f(X_num)\n        \n        return np.concatenate([X, X_be], axis=1)","8f9a6f59":"basis_exps = [lambda x: x**2, lambda x: x**3]\nBE = BasisExpander(num_cols, basis_exps)\nl1_ratios = [0.3, 0.5, 0.7, 0.9, 0.99]\nnet_cv = make_pipeline(BE, StandardScaler(), ElasticNetCV(l1_ratio=l1_ratios, n_alphas=100, cv=5, max_iter=3000))\nnet_cv.fit(X,y)\nbest_alpha_be = net_cv[-1].alpha_\nprint('Best alpha: ', best_alpha_be)\nbest_l1_ratio_be = net_cv[-1].l1_ratio_\nprint('Best l1_ratio: ', best_l1_ratio_be)\n\nnet_be = make_pipeline(BE, StandardScaler(), ElasticNet(l1_ratio=best_l1_ratio_be, alpha=best_alpha_be, max_iter=6000))\nexamine_model(net_be, X, y, pipeline=True)","4bad92f3":"class PiecewiseBasisExpander(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms the data by performing the necessary basis expansions for piecewise linear functions\n    \"\"\"\n    def __init__(self, numerical_columns, nr_knots, model='pw-linear'):\n        self.numerical_columns = numerical_columns\n        self.nr_knots = nr_knots\n        self.knots = None\n        self.model = model\n        \n    def fit(self, X, y=None):\n        k = self.nr_knots - 1\n        X_num = X[:, self.numerical_columns]\n        n, m = X_num.shape\n\n        knots = np.zeros((self.nr_knots, m))\n        knots[0,:] = np.min(X_num, axis=0)\n        knots[-1,:] = np.max(X_num, axis=0)\n        for i in range(1, k):\n            knots[i,:] = knots[0,:] + (i\/k) * (knots[-1,:] - knots[0,:])\n        \n        self.knots = knots\n        return self\n    \n    def transform(self, X):\n        k = self.nr_knots - 1\n        X_num = X[:, self.numerical_columns]\n        n, m = X_num.shape\n        knots = self.knots\n        \n        if self.model == 'pw-linear':\n            X_be = np.zeros((n, (k-1)*m))    \n            for i in range(k-1):\n                X_be[:, i*m:(i+1)*m] = np.maximum(X_num - knots[i+1,:], 0)\n                \n        if self.model == 'cubic-spline':\n            X_be = np.zeros((n, (k+1)*m))\n            X_be[:, :m] = X_num**2\n            X_be[:, m:2*m] = X_num**3                         \n            for i in range(1, k):\n                X_be[:, (i+1)*m:(i+2)*m] = np.maximum(X_num - knots[i,:], 0)**3\n                \n        if self.model == 'natural-cubic-spline':\n            X_be = np.zeros((n, (k+1)*m))\n\n            d_last = (np.maximum(X_num-knots[k-1,:],0)**3 - np.maximum(X_num-knots[k,:],0)**3) \/ (knots[k,:] - knots[k-1,:])\n            for i in range(1, k-1):\n                d = (np.maximum(X_num-knots[i,:],0)**3 - np.maximum(X_num-knots[k,:],0)**3) \/ (knots[k,:] - knots[i,:])\n                X_be[:, (i-1)*m:i*m] = d - d_last\n                \n        return np.concatenate([X, X_be], axis=1)","76ec2e44":"BE_pwl = PiecewiseBasisExpander(num_cols, nr_knots=8)\nl1_ratios = [0.3, 0.5, 0.7, 0.9, 0.99]\nnet_cv = make_pipeline(BE_pwl, StandardScaler(), ElasticNetCV(l1_ratio=l1_ratios, n_alphas=100, cv=5, max_iter=3000))\nnet_cv.fit(X,y)\nbest_alpha_pwl = net_cv[-1].alpha_\nprint('Best alpha: ', best_alpha_pwl)\nbest_l1_ratio_pwl = net_cv[-1].l1_ratio_\nprint('Best l1_ratio: ', best_l1_ratio_pwl)\n\nnet_pwl = make_pipeline(BE_pwl, StandardScaler(), ElasticNet(l1_ratio=best_l1_ratio_pwl, alpha=best_alpha_pwl, max_iter=6000))\nexamine_model(net_pwl, X, y, pipeline=True)","20fec504":"BE_cs = PiecewiseBasisExpander(num_cols, nr_knots=6, model='cubic-spline')\nl1_ratios = [0.3, 0.5, 0.7, 0.9, 0.99]\nnet_cv = make_pipeline(BE_cs, StandardScaler(), ElasticNetCV(l1_ratio=l1_ratios, n_alphas=100, cv=5, max_iter=3000))\nnet_cv.fit(X,y)\nbest_alpha_cs = net_cv[-1].alpha_\nprint('Best alpha: ', best_alpha_cs)\nbest_l1_ratio_cs = net_cv[-1].l1_ratio_\nprint('Best l1_ratio: ', best_l1_ratio_cs)\n\nnet_cs = make_pipeline(BE_cs, StandardScaler(), ElasticNet(l1_ratio=best_l1_ratio_cs, alpha=best_alpha_cs, max_iter=6000))\nexamine_model(net_cs, X, y, pipeline=True)","f049651b":"BE_ncs = PiecewiseBasisExpander(num_cols, nr_knots=4, model='natural-cubic-spline')\nl1_ratios = [0.3, 0.5, 0.7, 0.9, 0.99]\nnet_cv = make_pipeline(BE_ncs, StandardScaler(), ElasticNetCV(l1_ratio=l1_ratios, n_alphas=100, cv=5, max_iter=3000))\nnet_cv.fit(X,y)\nbest_alpha_ncs = net_cv[-1].alpha_\nprint('Best alpha: ', best_alpha_ncs)\nbest_l1_ratio_ncs = net_cv[-1].l1_ratio_\nprint('Best l1_ratio: ', best_l1_ratio_ncs)\n\nnet_ncs = make_pipeline(BE_ncs, StandardScaler(), ElasticNet(l1_ratio=best_l1_ratio_ncs, alpha=best_alpha_ncs, max_iter=6000))\nexamine_model(net_ncs, X, y, pipeline=True)","e99e30b3":"mars = make_pipeline(StandardScaler(), Earth(max_degree=1))\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    examine_model(mars, X, y, coeffs=False, pipeline=True)\n#mars = mars.fit(X,y)","74bed74d":"catboost = CatBoostRegressor(verbose=0, allow_writing_files=False)\nexamine_model(catboost, X, y, coeffs=False)","26c93e77":"rf = RandomForestRegressor()\nexamine_model(rf, X, y, coeffs=False)","e4c71b74":"estimators = [('net', net), ('svr', svr), ('net_pwl', net_pwl), ('net_ncs', net_ncs), ('catboost', catboost)]\nstacked_model = StackingRegressor(estimators=estimators, final_estimator=Lasso(alpha=0, positive=True, max_iter=2000))\n#examine_model(stacked_model, X, y, coeffs=False)","9367240a":"stacked_model = stacked_model.fit(X,y)\nX_test = df_test_dummies.values\npredictions = np.exp(stacked_model.predict(X_test))\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': predictions})\nmy_submission.to_csv('submission.csv', index=False)","8a9afdf4":"my_submission","21f53465":"## Ridge Regression","2306b693":"## Elastic Net","3fddf48b":"## Multivariate Adaptive Regression Splines (MARS)\n\nThe Mars algorithm fits a linear combination of (in this case 1d but higher orders are possible as well) splines automatically selecting features and knots. I am surprised that it doesn't perform better. My best bet is that it struggles with the dummies which we ignored for higher order terms above.","50685448":"**Cubic Spline**","8b69437e":"# Non-linear Regression Models\n\nIn this kernel, I want to explore non-linear models, mostly arising from linear models after basis expansions (polynomial, piecewice linear, splines, support vector regressions), but also tree methods.\n\nI only perform basic feature engineering and the focus is on the models. There are many great kernels out there with very nice data exploration feature engineering, e.g. [this kernel](https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing#Feature-engineering).","11c354ce":"### Remove features without enough data","150bf0b2":"### Splitting numerical and categorical features and filling in missing values","7c36daad":"## Basis Expansion\n\nWe will now introduce non-linearity by adding new features which are non-linear functions of the old features.\n\nSklearn has two such models implemented kernel_ridge and Support Vector Machines for Regression. Both do not perform very well on this data (they are more suitable for more complex functions I suppose), so we will not go in detail.","9490c19a":"## Ordinary Least Squares Linear Regression","6711638a":"## Outlier Detection\n\nWe use this model to find outliers. We fit the data on the whole training set, and remove examples which are predicted particularly bad. This idea is from here [this kernel](https:\/\/www.kaggle.com\/jack89roberts\/top-7-using-elasticnet-with-interactions).","ba82976b":"**Piecewise Linear**","9e241ac7":"## Feature Engineering\n\n### Outliers\nWe define outliers as those rows where a basic ridge model makes particularly bad predictions (see below after the ridge model, this is from [this kernel](https:\/\/www.kaggle.com\/jack89roberts\/top-7-using-elasticnet-with-interactions)).","7e28e847":"## Submission","8073b32f":"### Transforming and adding some features","1fc178bf":"### Variable type\nWe know split our features in categorical and numerical variables.\nSome of the numerical features have very few unique values and should really be categorical instead.","05f7f7f5":"## Stacking\nIn the last step we will use a stacking regressor to find a linear combination of our models with the goal to reduce variance. In order to prevent overfitting we will not allow negative weights (that is models having a negative impact to our final estimator). The choice of Lasso as final estimator is simply because unlike ridge, it has this parameter.\n\nThis step is very slow, since both stacked model and our function examine_model perform a 5-fold crossvalidation, so every model is trained 25 times.","55c0670f":"**Natural Cubic Spline**","960b1b03":"## Tree methods\n\nBoth gradient boosting and random forests are not extremely good on this data, but help with blending later.","d8803266":"We will instead add new features, which are non-linear functions of the original features. We follow [Elements of Statistics, Chapter 5].\n\nWe begin with adding squares and cubes, thus fitting a cubic model. Unsurprisingly, we observe lower bias and higher variance (lower training error, but bigger difference between training and validation errror)","14ec5efc":"### Fixing skewed features using boxcox","dfd67ea3":"### Creating dummies and removing features corresponding to rare values\n\nIn most models we need dummy variables for the categorical features. We will write this as an estimator so we can add it to the pipeline only when needed.","186a62b8":"## Lasso Regression","a787479c":"### Piecewise Models\n\nWe will now use basis expansions to allow for functions which are piecewise linear\/cubic in each dimension. Note that such functions are more general and therefore we expect lower bias and higher variance. We follow [Elements of statistics, Chapter 5]. \n\n**Piecewise linear functions:**\nAssume (for simplicity) that $X$ is one dimensional and let  $\\min(X) = \\xi_0, \\ldots, \\xi_k = \\max(X)$ be a set of knots.\n\nLet $h_1 = I(X<\\xi_1)$, $h_i = I(\\xi_{i-1} \\leq X < \\xi_i)$ for $i = 2, \\ldots, k-1$, and $h_{k} = I(\\xi_{k-1} \\leq X)$, where $I$ denotes the indicator function which is $1$ if the condition is satisfied and $0$ otherwise. (Note that $h_1$ and $h_{k}$ look a little different so that we can make guesses for new data which exceeds the range of $X$). We then let $h_i'(X) = h_i(X) \\cdot X$ for every $i = 0, \\ldots, k$. It is easy to see that $\\{h_0,h_0', \\ldots, h_k,h_k'\\}$ is a basis for all piecewise linear functions.\n\n**Piecewise linear continuous functions:**\nPiecewise linear functions can have quite big jumps and we would like to avoid this to reduce variance. Thus we need the extra conditions $f(\\xi_i^-) = f(\\xi_i^+)$ for all $i = 1, \\ldots, k-1$ and each linear combination $f$ of $h_0, \\ldots, h_k$. This would be annoying to deal with in our models. Luckily, there is the following equivalent way to achieve this:\nLet $h_0 = 1$, $h_1(X) = X$, $h_{i+1}(X) = \\max(X-\\xi_i, 0)$ for all $i = 2, \\ldots, k-1$. Then, $\\{h_0, \\ldots, h_{k-1}\\}$ form a basis of all piecewise linear continuous functions. Note that the bias term in linear regression already gives us $h_0$, and $h_1$ is the original data, so we will not use those.\n\n**Cubic Splines:**\nA *cubic spline* is a continuous piecewise cubic polynomial with continuous with continuos first and second derivatives. It can be seen that  $h_0 = 1$, $h_1(X) = X$, $h_2(X) = X^2$, $h_3(X) = X^3$, and $h_{i+2}(X) = (\\max(X-\\xi_i, 0))^3$ for all $i = 2, \\ldots, k-1$, define a basis for cubic splines. Again, we do not need $h_0$ due to the bias term.\n\n**Natural Cubic Splines:**\nA cubic spline can behave quite poorly outside the boundary. We can fix this by requiring linearity outside the boundary. This is called a *natural cubic spline*. For more details and the basis, see 'Elements of Statistics'. "}}