{"cell_type":{"6f3b77d0":"code","b0e8be98":"code","b7aeffc3":"code","eff5a51f":"code","5f67d905":"code","4964a754":"code","40c1c7e2":"code","0ee59603":"code","3c4466cf":"code","ff0c44f5":"code","c9d50ee8":"code","48cf6bcd":"code","f1d539b0":"code","85ab0001":"code","80f90ef7":"code","878d8070":"code","603d3f6e":"code","c67826af":"code","b89544c5":"code","60c5fc1c":"code","65266686":"code","a4a0c506":"code","ea569ea5":"code","e34d2fb6":"code","7cce72d6":"code","56b6b37d":"code","d2d830ee":"code","c2a67ce4":"code","51f70527":"code","a5040915":"code","a68b46de":"code","40fd17c2":"code","036bdccb":"code","1de3be43":"code","523bfbcd":"code","da3c9ebb":"code","1d09a5a7":"code","9fab0824":"code","c476caee":"code","95a1de61":"code","55cafa38":"code","b3fc53c9":"code","66e26a53":"code","24704a03":"code","6fbf7157":"markdown","d3e26898":"markdown","c5a01b8a":"markdown","10e066e3":"markdown","8e3c0c8e":"markdown","a26e9c48":"markdown","bb7f5d95":"markdown","c3f79d18":"markdown","93b137b8":"markdown","18dbf514":"markdown","c83d3f45":"markdown","fd94d396":"markdown","6d53aea6":"markdown","8e59f5eb":"markdown","f564a034":"markdown","930d9fc9":"markdown","2ce08ed6":"markdown","d681f9f7":"markdown","7d96de6b":"markdown","575400b7":"markdown","945be69a":"markdown","53aa3b72":"markdown","7a4bd763":"markdown","7da29bb8":"markdown","974172ac":"markdown","6fb7bf99":"markdown","270ee296":"markdown","95a42e43":"markdown","c32bbbd9":"markdown","36351a24":"markdown","e7264433":"markdown","6210ba69":"markdown","b219d573":"markdown","edff47f6":"markdown","6218851e":"markdown","3dad3c7a":"markdown","d52ad1ce":"markdown","713b645a":"markdown","6b2c0080":"markdown","78f8589d":"markdown","74851198":"markdown","67f6a0ce":"markdown","9e71e9b4":"markdown","c6ad59df":"markdown","a5bfba72":"markdown","4816e293":"markdown","cdae5622":"markdown","91f3c3af":"markdown","42da31bb":"markdown","3fe14be6":"markdown","42186a41":"markdown","50664b46":"markdown","382b5dac":"markdown","9bef5f47":"markdown","1ad089cf":"markdown","e2de9175":"markdown","d693a257":"markdown","42ac37e7":"markdown","feb3d868":"markdown","85ae0415":"markdown","50175eed":"markdown","21af5153":"markdown","0851a3ca":"markdown","fbfba142":"markdown","95f8167a":"markdown","21f17e0d":"markdown","f51392fa":"markdown","be3679ab":"markdown","daa8ee35":"markdown","8fa18e9b":"markdown","33c4ccca":"markdown","2be690bf":"markdown","ce3da6dd":"markdown","4b4ee09a":"markdown","54fa9f61":"markdown","467b23fe":"markdown","3a5e1c72":"markdown","f8e64715":"markdown","f2623a33":"markdown","27de4ef4":"markdown","e635b52a":"markdown","d633740e":"markdown"},"source":{"6f3b77d0":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport matplotlib.cm as cm\nimport seaborn as sns\n\nimport pandas as pd\nimport pandas_profiling\nimport numpy as np\nfrom numpy import percentile\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\n\nimport os, sys\nimport re\nfrom tabulate import tabulate\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom umap import UMAP\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rc('font', size=18)        \nplt.rc('axes', titlesize=22)      \nplt.rc('axes', labelsize=18)      \nplt.rc('xtick', labelsize=12)     \nplt.rc('ytick', labelsize=12)     \nplt.rc('legend', fontsize=12)   \n\nplt.rcParams['font.sans-serif'] = ['Verdana']\n\npd.options.mode.chained_assignment = None\npd.options.display.max_seq_items = 500\npd.options.display.max_rows = 500\npd.set_option('display.float_format', lambda x: '%.5f' % x)","b0e8be98":"df = pd.read_csv(\"..\/input\/autompg-dataset\/auto-mpg.csv\")\ndf.head()","b7aeffc3":"df.info(verbose=True, null_counts=True)","eff5a51f":"# just for good measure: reduce the memory footprint \n# by setting floats and ints to 32bit\n\nfor col in df.columns:\n    if df[col].dtype =='float64': df[col] = df[col].astype('float32')\n    if df[col].dtype =='int64': df[col] = df[col].astype('int32')\n        \n# remove spaces from column names for `car name` and `model year`\ndf.rename({\"car name\" : \"name\",\n          \"model year\" : \"model_year\"}, axis=1, inplace=True)\n\n# convert `origin` back to actual names for now\ncar_origin = {1 : \"usa\", 2 : \"europe\", 3 : \"japan\"}\ndf[\"origin\"] = df.origin.map(car_origin)","5f67d905":"print(df.shape)\nprint(df.drop_duplicates().shape)","4964a754":"missing = [(c, df[c].isna().mean()*100) for c in df]\nmissing = pd.DataFrame(missing, columns=[\"feature\", \"percentage\"])\nmissing = missing[missing.percentage > 0]\ndisplay(missing.sort_values(\"percentage\", ascending=False))","40c1c7e2":"df.horsepower.unique()","0ee59603":"df.horsepower = df.horsepower.apply(lambda x: np.nan if x is \"?\" else x)\ndf.horsepower = df.horsepower.astype(\"float32\")\ndf.horsepower.fillna(df.horsepower.mean(), inplace=True)\ndf.info()","3c4466cf":"# split() with expand=True yields one column per list element\n# we only split on the first space by setting n=1\ndf[[\"manufacturer\", \"model\"]] = df[\"name\"].str.split(\" \", n=1, expand=True)\ndf.drop(\"name\", axis=1, inplace=True)\ndf.head(1).T","ff0c44f5":"print(sorted(df.manufacturer.unique()))","c9d50ee8":"errors = {\n         \"vokswagen\" : \"volkswagen\", \n         \"vw\" : \"volkswagen\", \n         \"toyouta\" : \"toyota\", \n         \"mercedes-benz\" : \"mercedes\", \n         \"chevroelt\": \"chevrolet\",\n         \"chevy\" : \"chevrolet\", \n         \"maxda\" : \"mazda\"\n         }\n\ndf.manufacturer = df.manufacturer.map(errors).fillna(df.manufacturer)","48cf6bcd":"def alphanumeric(x):\n    return re.sub('[^A-Za-z0-9]+', '', (str(x)))\n\ndf[\"model\"] = df.model.apply(lambda x: alphanumeric(x))","f1d539b0":"print(\"Origin\")\nprint(tabulate(pd.DataFrame(df.origin.value_counts())))\n\nplt.figure(figsize=(16,5));\ndf.groupby(\"origin\")[\"origin\"].count().sort_values(ascending=False).plot(kind=\"bar\")\nplt.title(\"Origin\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Country\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Cars from {df.manufacturer.nunique()} manufacturers (Top10)\")\nprint(tabulate(pd.DataFrame(df.manufacturer.value_counts()[:10])))\n\nplt.figure(figsize=(16,5));\ndf.groupby(\"manufacturer\")[\"manufacturer\"].count().sort_values(ascending=False).plot(kind=\"bar\")\nplt.title(\"Manufacturers\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Manufacturer\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nprint(f\"{df.model.nunique()} car models (Top 10)\")\nprint(tabulate(pd.DataFrame(df.model.value_counts()[:10])))\n\nplt.figure(figsize=(16,5));\ndf.groupby(\"model\")[\"model\"].count().sort_values(ascending=False)[:20].plot(kind=\"bar\")\nplt.title(\"Car models (Top 20 plotted)\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Car models\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,5));\ndf.groupby(\"model_year\")[\"model_year\"].count().sort_index().plot(kind=\"bar\")\nplt.title(\"Car models per year\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Year\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","85ab0001":"df[df.model == \"pinto\"]","80f90ef7":"stats_ = df.describe().T.drop([\"count\", \"25%\", \"75%\"], axis=1)\nstats_ = pd.concat([stats_, df.skew()], axis=1)\nstats_.columns = [\"mean\", \"std\", \"min\", \"median\", \"max\", \"skew\" ]\ncols = [\"mean\", \"median\", \"std\", \"skew\", \"min\", \"max\"]\nstats_ = stats_[cols]\nprint(tabulate(stats_, headers=\"keys\", floatfmt=\".1f\"))","878d8070":"for feature in df.select_dtypes(\"number\").columns:\n    if feature == \"model_year\":\n        continue\n    plt.figure(figsize=(16,5))\n    #sns.distplot(df[feature], hist_kws={\"rwidth\": 0.9})\n    #plt.xlim(df[feature].min(), df[feature].max())\n    df[feature].plot(kind=\"hist\", rwidth=0.9, bins=50)\n    plt.title(f\"{feature.capitalize()}\")\n    plt.tight_layout()\n    plt.show()","603d3f6e":"# calculate normal and extreme upper and lower cut off\nfor feature in df.select_dtypes(\"number\").columns:\n\n    cut_off = df[feature].std() * 3\n    lower   = df[feature].mean() - cut_off \n    upper   = df[feature].mean() + cut_off\n    df_lower = df[df[feature] < lower]\n    df_upper = df[df[feature] > upper]\n    if df_lower.shape[0] != 0 or df_upper.shape[0] != 0:\n        print(f\"{feature}\")\n        print(f\"lower bound: {lower:.2f}\\nupper bound: {upper:.2f}\")\n        if df_lower.shape[0] != 0:\n            display(df[df[feature] < lower].sort_values(feature))\n        if df_upper.shape[0] != 0:\n            display(df[df[feature] > upper].sort_values(feature))","c67826af":"pca = PCA(n_components=2, random_state=1)\n# fit on all numerical features and reduce dimensionality to two dimensions\ncols = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']\npipe = make_pipeline(StandardScaler())\ndf_std = pipe.fit_transform(df[cols])\ndata_pca = pca.fit_transform(df_std)\n\n# get percentage of retained variance after dimensionality reduction\nex_variance = pca.explained_variance_ratio_.sum()*100\nprint(f\"The reduced 2-dimensional data still contains {ex_variance:.1f}% of the variance of the original data.\")\n\n# we colorize the plot with the two features `cylinders` and `origin`\ndata_pca = pd.DataFrame(data_pca, columns=[\"x\", \"y\"])\ndata_pca = data_pca.join(df)\nplt.figure(figsize=(16,7))\nsns.scatterplot(x=\"x\", y=\"y\", data=data_pca, hue=\"cylinders\", linewidth=0.2, alpha=0.9)\nplt.title(f\"PCA of numerical features\")\nplt.tight_layout()\nplt.show()","b89544c5":"tsne = TSNE(n_components=2, random_state=1)\ndata_tsne = tsne.fit_transform(df_std)\n\nfor feature in [\"cylinders\", \"origin\"]:\n    data_tsne = pd.DataFrame(data_tsne, columns=[\"x\", \"y\"])\n    data_tsne = data_tsne.join(df)\n    plt.figure(figsize=(16,7))\n    sns.scatterplot(x=\"x\", y=\"y\", data=data_tsne, hue=feature, linewidth=0.2, alpha=0.9)\n    plt.title(f\"TSNE of numerical features\")\n    plt.tight_layout()\n    plt.show()","60c5fc1c":"plt.figure(figsize=(16,7))\nsns.swarmplot(x=\"origin\", y=\"cylinders\", hue=\"origin\", data=df)\nplt.title(f\"Cylinders vs origin\")\nplt.tight_layout()\nplt.show()","65266686":"# get correlation among all numerical features with pandas .corr() function\ncorr = df.corr()\n# filter correlations less than 0.5\ncorr = corr[(corr > 0.5) | (corr < -0.5)]\n\nplt.subplots(figsize=(16,16));\nsns.heatmap(corr, cmap=\"RdBu\", square=True, cbar_kws={\"shrink\": .7}, )\nplt.title(\"Correlation matrix of numerical features\")\nplt.tight_layout()\nplt.show()","a4a0c506":"plt.figure(figsize=(16,5))\ncorr[\"mpg\"].sort_values(ascending=True)[:-1].plot(kind=\"barh\")\nplt.title(\"Correlation of numerical features to mpg\")\nplt.xlabel(\"Correlation to \u00abmpg\u00bb\")\nplt.tight_layout()\nplt.show()","ea569ea5":"plt.figure(figsize=(16,5))\ndf.mpg.plot(kind=\"hist\", bins=100, rwidth=0.9)\nplt.title(\"Miles per Gallon: value distribution\")\nplt.xlabel(\"Miles per Gallon (mpg)\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,5))\ndf.mpg.plot(kind=\"box\", vert=False)\nplt.title(\"Miles per Gallon: value distribution\")\nplt.xlabel(\"Miles per Gallon (mpg)\")\nplt.yticks([0], [''])\nplt.ylabel(\"Miles per gallon\\n\", rotation=90)\nplt.tight_layout()\nplt.show()","e34d2fb6":"print(f\"The skewness of mpg is: {df.mpg.skew():.2f}\")\n\nplt.figure(figsize=(16,7))\n_ = stats.probplot(df['mpg'], plot=plt)\nplt.title(\"Probability plot: mpg\")\nplt.show()","7cce72d6":"print(f\"The skewness of mpg is                   :  {df.mpg.skew():.2f}\")\nprint(f\"The skewness of mpg log transformed is   : {np.log1p(df.mpg).skew():.2f}\")\nprint(f\"The skewness of mpg boxcox transformed is: {boxcox1p(df.mpg, 0.15).skew():.2f}\")\n\nplt.figure(figsize=(16,7))\n_ = stats.probplot(np.log1p(df.mpg), plot=plt)\nplt.title(\"Probability plot: mpg log transformed\")\nplt.show()\n\nplt.figure(figsize=(16,7))\n_ = stats.probplot(boxcox1p(df.mpg, 0.15), plot=plt)\nplt.title(\"Probability plot: mpg boxcox transformed\")\nplt.show()","56b6b37d":"plt.figure(figsize=(16,7))\norder = df.groupby(\"origin\")[\"mpg\"].median().sort_values(ascending=True).index\nsns.boxplot(x=\"origin\", y=\"mpg\", data=df, order=order, width=0.5)\nplt.title(\"Distribution of miles per gallon in relation to origin\")\nplt.ylabel(\"Miles per gallon (in 1000)\")\nplt.tight_layout()\nplt.show()","d2d830ee":"plt.figure(figsize=(16,7))\ntop20 = df.manufacturer.value_counts().index[:20]\ntop20 = df[df.manufacturer.isin(top20)]\norder = top20.groupby(\"manufacturer\")[\"mpg\"].median().sort_values(ascending=True).index\nsns.boxplot(x=\"manufacturer\", y=\"mpg\", hue=\"origin\", dodge=False, data=top20, order=order)\nplt.title(\"MPG per manufacturer (top20 plotted)\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","c2a67ce4":"for color, country in zip(sns.color_palette()[:3], df.origin.unique()):\n    plt.figure(figsize=(16,7))\n    sns.boxplot(x=\"model_year\", y=\"mpg\", color=color, data=df[df.origin==country])\n    plt.ylim(8, 48)\n    plt.title(f\"Miles per gallon per model_year ({country.upper()})\")\n    plt.tight_layout()\n    plt.show()","51f70527":"for feature in ['displacement', 'horsepower', 'weight', 'acceleration']:\n    plt.figure(figsize=(16,5));\n    sns.scatterplot(x=feature, y=\"mpg\", data=df, linewidth=0.2, alpha=0.7, hue=\"origin\")\n    plt.title(f\"{feature} vs. mpg\")\n    plt.legend(bbox_to_anchor=(1, 1), loc=2)\n    plt.tight_layout()\n    plt.show()","a5040915":"plt.figure(figsize=(16,5))\nsns.distplot(df.mpg)\nplt.title(\"mpg \u2013 unscaled\")\nplt.show()\n\nscalers = [StandardScaler(), \n           MinMaxScaler(), \n           RobustScaler(), \n           PowerTransformer(method=\"box-cox\"),\n           QuantileTransformer(output_distribution=\"normal\")]\n\nfor scaler in scalers:\n    pipe = make_pipeline(scaler)\n    standardized = pipe.fit_transform(df.mpg.values.reshape(-1, 1))\n    plt.figure(figsize=(16,5))\n    sns.distplot(pd.DataFrame(standardized))\n    plt.title(f\"mpg \u2013 {str(scaler.__class__).split('.')[-1][:-2]}\")\n    plt.show()","a68b46de":"def prepare_data(data, scaler):\n    # remove feature \u00abmodel\u00bb from dataset\n    data.pop(\"model\")\n    df_numeric = data.select_dtypes(exclude=['object'])\n    df_obj = data.select_dtypes(include=['object']).copy()\n\n    cols = []\n    for c in df_obj:\n        dummies = pd.get_dummies(df_obj[c])\n        dummies.columns = [c + \"_\" + str(x) for x in dummies.columns]\n        cols.append(dummies)\n    df_obj = pd.concat(cols, axis=1)\n    \n    pipe = make_pipeline(scaler)\n    scaled = pipe.fit_transform(df_numeric)\n    df_numeric = pd.DataFrame(scaled, columns=df_numeric.columns)\n    \n    data = pd.concat([df_numeric, df_obj], axis=1)\n    data.reset_index(inplace=True, drop=True)\n    return data\n\nclean_df = prepare_data(df.copy(), StandardScaler())\nclean_df.head()","40fd17c2":"import sklearn\nsorted(sklearn.metrics.SCORERS.keys())","036bdccb":"metrics = [\"r2\",\n           \"explained_variance\",\n           \"max_error\", \n           \"neg_mean_absolute_error\", \n           \"neg_mean_squared_error\", \n           \"neg_median_absolute_error\",\n]\n\nclean_df = prepare_data(df.copy(), StandardScaler())\nX = clean_df.drop(\"mpg\", axis=1)\ny = clean_df.mpg\n\nclf = make_pipeline(LinearRegression())\nresults = cross_validate(clf, X, y, scoring=metrics)\n\nfor k, v in results.items():\n    if k in [\"fit_time\", \"score_time\"]:\n        continue\n    print(f\"{v.mean(): .4f} {k.replace('test_', '')}\")","1de3be43":"classifiers = [\n               DummyRegressor(),\n               LinearRegression(), \n               Ridge(random_state=1), \n               Lasso(random_state=1), \n               ElasticNet(random_state=1),\n               KernelRidge(),\n               SVR(kernel=\"linear\"),\n               RandomForestRegressor(n_jobs=-1, random_state=1),\n               GradientBoostingRegressor(random_state=1),\n               lgb.LGBMRegressor(n_jobs=-1, random_state=1),\n               xgb.XGBRegressor(objective=\"reg:squarederror\", n_jobs=-1, random_state=1),\n]\n\nclf_names = [\n            \"dummy       \", \n            \"linear      \", \n            \"ridge       \", \n            \"lasso       \",\n            \"elasticnet  \",\n            \"kernlrdg    \",\n            \"svr         \",\n            \"randomforest\", \n            \"gbm         \", \n            \"lgbm        \", \n            \"xgboost     \",\n]\n\n\nscalers = [\n           StandardScaler(), \n           MinMaxScaler(), \n           RobustScaler(), \n           PowerTransformer(method=\"box-cox\"),\n           QuantileTransformer(output_distribution=\"normal\"),\n          ]\n\nscalers_names = [\n                \"Standard\", \n                \"MinMax\", \n                \"Robust\", \n                \"PowerTransform\",\n                \"QuantileTransform\",\n]","523bfbcd":"def score_models(data, metric):\n    \n    frames = []\n    \n    for scaler_name, scaler in zip(scalers_names, scalers):\n        \n        clean_df = prepare_data(data.copy(), scaler)\n        X = clean_df.drop(\"mpg\", axis=1)\n        y = clean_df.mpg\n\n        scores = []\n\n        for clf_name, clf in zip(clf_names, classifiers):\n            score = cross_val_score(clf, X, y, cv=10, scoring=metric).mean()\n            scores.append(score)\n\n        frames.append(pd.DataFrame(scores, columns=[scaler_name]))\n\n    score_df = pd.concat(frames, axis=1)\n    score_df[\"clf\"] = clf_names\n    score_df.set_index(\"clf\", inplace=True)\n\n    score_df.sort_values(\"Standard\", ascending=False, inplace=True)\n    return score_df\n\nscore_models(df, \"r2\")","da3c9ebb":"score_models(df, \"explained_variance\")","1d09a5a7":"score_models(df, \"neg_mean_squared_error\")","9fab0824":"clean_df = prepare_data(df.copy(), StandardScaler())\nX = clean_df.drop(\"mpg\", axis=1)\ny = clean_df.mpg\nmetric = 'neg_mean_squared_error'\n\nscores = []\n\n# remove the dummy regressor and linear regression from set of estimators\nfor clf_name, clf in zip(clf_names[2:], classifiers[2:]):\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    scores.append([clf_name, np.sqrt(-cross_val_score(clf, X, y, cv=kfold, scoring=metric)).mean()])\n\npd.DataFrame(scores, columns=[\"clf\", \"rmse\"]).sort_values(\"rmse\")","c476caee":"clf = lgb.LGBMRegressor(n_jobs=-1, random_state=1)\ncoeffs = clf.fit(X, y).feature_importances_\ndf_co = pd.DataFrame(coeffs, columns=[\"importance_\"])\ndf_co.index = X.columns\ndf_co.sort_values(\"importance_\", ascending=True, inplace=True)\n\nplt.figure(figsize=(16,16))\ndf_co.importance_.plot(kind=\"barh\")\nplt.title(f\"LightGBM feature importance\")\nplt.show()","95a1de61":"clean_df = prepare_data(df.copy(), StandardScaler())\nX = clean_df.drop(\"mpg\", axis=1)\ny = clean_df.mpg\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nmetric = 'neg_mean_squared_error'\n\nclf = lgb.LGBMRegressor(n_jobs=-1, random_state=1)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)","55cafa38":"plt.figure(figsize=(10, 10))\nsns.scatterplot(x=y_test, y=predictions)\nplt.plot(np.arange(-1.5, 2.6), np.arange(-1.5, 2.6))\nfor x_, y_, t_ in zip(y_test, predictions, X_test.index):\n    plt.text(x_, y_, t_, fontsize=10)\nplt.title(\"Residuals: predicted vs actual values\")\nplt.xlabel(\"actual values\")\nplt.ylabel(\"predicted values\")\nplt.show()","b3fc53c9":"print(\"Predictions too high\")\noutliers_too_high = [366, 332, 303, 346]\ndisplay(df.loc[outliers_too_high])\n#display(X_test.loc[outliers_too_high])\n\nprint(\"Predictions too low\")\noutliers_too_low = [298, 309, 327]\ndisplay(df.loc[outliers_too_low])\n#display(X_test.loc[outliers_too_low])\n\n# print basic stats for comparison\ndisplay(df.median())","66e26a53":"plt.figure(figsize=(16,7))\nsns.distplot(y_test)\nsns.distplot(predictions)\nplt.legend([\"mpg: actual values\", \"mpg: predicted\"])\nplt.title(\"Distributions of trained and predicted values\")\nplt.tight_layout()\nplt.show()","24704a03":"clean_df = prepare_data(df.copy(), StandardScaler())\nX = clean_df.drop(\"mpg\", axis=1)\ny = clean_df.mpg\n\nclf = lgb.LGBMRegressor(n_jobs=-1, random_state=1)\n# increase steps in np.linspace for more granular search\nparam_grid = {\n     'n_estimators' : np.linspace(100, 500, 3, dtype=\"int\"),\n     'learning_rate' : np.linspace(0.01, 0.1, 3),\n}\n\nmetric = 'r2'\n# increase cv for more precision but longer processing time\nsearch = GridSearchCV(clf, param_grid, cv=3, scoring=metric, n_jobs=-1, verbose=True)\nsearch.fit(X, y)\n\nprint(f\"{search.best_params_}\")\nprint(f\"{search.best_score_:.4}\")","6fbf7157":"Again a quick summary:\n\n- The range of our target `mpg` is between 9 and 46.6.\n- Cars seem to have a wide range of technical specs.\n- All features are positively skewed. \n- `horsepower` seems significantly more skewed than the other features. \n\nSo let's plot all distributions in comparison.","d3e26898":"## **Examine and plot categorical features**\n\nLet's plot bar graphs for the categoricals to gain more insight.","c5a01b8a":"## **How much sense do make predictions from our model?**","10e066e3":"So after the first exploration \u2013 **our data in a nutshell:**\n\n- 398 different cars from 1970 to 1982\n- 296 distinct models from 30 manufacturers, a few of them dominating like Ford, Chevrolet.\n- Cars mainly from USA (249), much less from Japan (79) and Europe (70)\n- 1973 and 1978 seem a little bit stronger years with more samples.\n","8e3c0c8e":">**<font color=\"darkgreen\">RobustScaling<\/font>** rescales the data based on percentiles and is therefore **not influenced by a few number of large marginal outliers**. The RobustScaler keeps the outliers so they are still present in the transformed data. If a outlier clipping is desirable, a non-linear transformation like with a PowerTransformer() is required.","a26e9c48":"Let's get more **detailed stats about the distribution**.","bb7f5d95":"> There are questions marks (\"?\") amongst the values. This likely represents a missing value. Let's fix this by filling in the mean.","c3f79d18":"[Jake VanderPlas's Data Science Handbook](https:\/\/nbviewer.jupyter.org\/github\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/00.00-Preface.ipynb)\n\n**My other kernels:**\n\nhttps:\/\/www.kaggle.com\/chmaxx\/sklearn-pipeline-playground-for-12-classifiers<br>\nhttps:\/\/www.kaggle.com\/chmaxx\/extensive-data-exploration-modelling-python<br>\nhttps:\/\/www.kaggle.com\/chmaxx\/slim-data-cleaning-modelling-weighted-ensemble<br>\n\nhttps:\/\/www.kaggle.com\/chmaxx\/train-12-classifiers-with-one-line-of-code<br>\nhttps:\/\/www.kaggle.com\/chmaxx\/train-12-regressors-with-just-one-line-of-code<br>\n\n**Utility scripts:**\n\nhttps:\/\/www.kaggle.com\/chmaxx\/quick-regression<br>\nhttps:\/\/www.kaggle.com\/chmaxx\/quick-classification<br>","93b137b8":"> Job done! We found suitable hyperparameters and now could train our classifier on the full data set and e.g. submit to a competition.","18dbf514":"- According to explained_variance too LightGBM performs best with all scalers but PowerTransform. \n- PowerTransform gives a slight edge to the Ridge and KernelRidge estimators, likely due to fixing the skewed feature and target distributions.\n\nLet's now try the **mean squared error** as a metric.","c83d3f45":"## **Correlation among features**\nHow are the features correlated to each other?","fd94d396":"- We make a general distinction between **error metrics** and **scoring metrics**. \n- We want **errors to be as small as possible** and **scores, like accuracy, to be as high as possible.**\n- scikit-learn provides **three different options to evaluate a model**: \n   - **score methods of estimators**\n   - the **scoring parameter of crossvalidation tools** (e.g. cross_val_score()) \n   - and **metric functions**.\n\n\n- To streamline all scikit APIs in a consistent way all scorer objects follow the convention that **higher return values are better than lower return values**. [More info here](https:\/\/stackoverflow.com\/questions\/43081251\/sklearn-metrics-log-loss-is-positive-vs-scoring-neg-log-loss-is-negative) too.\n- Metrics which measure the distance between the model and the data, like `metrics.mean_squared_error`, are therefore available as their negative counterpart, e.g. `neg_mean_squared_error`, which **returns the negated value of the metric**. This way all functions work in the same numerical direction. \n- Available scorers can be listed with `sklearn.metrics.SCORERS.keys()`.\n- We can **use many metrics at once** with `GridSearchCV`, `RandomizedSearchCV` and `cross_validate()`.\n- **Dummy estimators** are useful to get a baseline of those metrics for random predictions.","6d53aea6":"## **References**","8e59f5eb":"## **Examining our target variable**\n\nWe now plot the feature correlation more specifically in relation to our target variable.","f564a034":"We observe three clusters that correspond to cylinders and even the origin of the cars. Probably US American cars are the only ones with 8 cylinders in the dataset. The group with 4 cylinders is a more heterogenuous group. ","930d9fc9":"Observations:\n\n- LightGBM performs best with all scalers and yields the highest score with standard scaling.\n- LinearRegression doesn't work at all. This is very likely due to exploding coefficients and it's lack of regularization.\n\nWe now compare to **explained_variance**.","2ce08ed6":"## **First look into the data**\n","d681f9f7":"Some model names also seem redundant or wrong. \n\nHand checking would be way too time-consuming so we only improve this features brute-force by removing all special characters. ","7d96de6b":">**<font color=\"darkgreen\">Standardization<\/font>** is done by subtracting the mean and dividing by standard deviation. So a value which lies 1 SD above the mean \u2013 say 150 \u2013 will have a value of 1 after standardization. However, outliers have an influence on the calculation of the mean and standard deviation. So the spread of the transformed data on each feature can still be very different after standardization. **`StandardScaler()` doesn't guarantee balanced feature scales in the presence of outliers.**","575400b7":"Quick reexamination: Why do we have just 296 models but 398 samples?\n\nEasy answer: Cars have different technical properties depending on the year they were built. E.g. the Ford Pinto has six different flavours... ","945be69a":"---\n>*If you use parts of this notebook in your own scripts or kernels, please give credit (for example link back to this, upvote or send flowers). Thanks! \nAnd I very much appreciate your feedback or comments! Thanks for that too. \ud83d\udc4d*\n\n---","53aa3b72":"And indeed: `horsepower` appears more skewed than the other variables. `acceleration` in turn almost looks normally distributed.","7a4bd763":"## **Machine Learning Methodology \u2013 step by step**","7da29bb8":">**<font color=\"darkgreen\">QuantileTransformer<\/font>** transforms the features to follow a uniform or a normal distribution. For a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers. It is therefore a robust preprocessing scheme. Feature values of new\/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. **This transform is non-linear.** It may **distort linear correlations between variables measured at the same scale** but renders variables measured at different scales more directly comparable.","974172ac":"Not surprisingly the feature describing the engine are strongly correlated to another. Many cylinders equals more displacement equals more horsepower which in turn is appropiate for heavier cars.","6fb7bf99":"Let's start with quick baselines. First we have to choose our error \/ scoring metric(s).","270ee296":"## **Tune hyperparameters**\n\nAs an example we grid search the optimal settings for LightGBM, our most promising classifier.","95a42e43":"We are asked to **predict miles per gallon (\u00abmpg\u00bb) of various car models** from describing features. We have a dataset with 398 samples.\n\n><span style=\"color:darkgreen\">**Our goal is to use the Auto MPG data to build a machine learning model that can predict the fuel consumption of a given car.**\n\nThe data in the training set includes the fuel consumption, which makes this **a supervised regression machine learning task**:\n\n>**Supervised**: We have access to both the features and the target and our goal is to train a model that can learn a mapping between the two.\n>**Regression**: The fuel consumpteion is a continuous variable.","c32bbbd9":"There are several errors in the manufacturers names. E.g. \u00abvokswagen\u00bb, \u00abmaxda\u00bb etc. We fix these by replacing the wrong entries. We also fix some synomyms like \u00abvw\u00bb.","36351a24":"## **Check for outliers**\n\nIn the next step we check for outliers that might negatively influence our regression algorithms. **But what exactly is an outlier?** And **can we isolate the outliers with statistical methods?**","e7264433":"We will follow these steps:\n\n1. Exploratory data analysis (EDA)\n2. Data cleaning and formatting\n3. Try and compare various machine learning models on a performance metric\n4. Perform hyperparameter tuning for the most promising model\n\nDuring **data cleaning and formatting** will especially take care of:\n\n- Missing values\n- Wrong values\n- Wrong datatypes\n- Outliers\n- Skewed distributions\n\n","6210ba69":"## **Examine and plot numerical features**\n\nWe now have a look at the basic stats of our numerical features.","b219d573":"# **2. Exploratory data analysis**","edff47f6":"From this plot we too can confirm that there is a noticable deviation on the right end with higher values for `mpg` \u2013 see the big blue bar. ","6218851e":">**<font color=\"darkgreen\">PowerTransformer<\/font>** applies a power transformation to each feature **to make the data more Gaussian-like**. scikit-learn's `PowerTransformer()` implements the Yeo-Johnson and Box-Cox transforms. **The power transform finds the optimal scaling factor to stabilize variance and mimimize skewness**. By default, PowerTransformer also applies zero-mean, unit variance normalization to the transformed output. **Box-Cox can only be applied to positive data**. If negative values are present the Yeo-Johnson transformed is to be preferred.","3dad3c7a":"### **A quick [refresher on scaling](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)**:\n\n- **Different scales and outliers degrade the performance of many machine learning algorithms.** \n- Unscaled data can also **slow down or even prevent the convergence of many gradient-based estimators.**\n- Many **estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales.** Exception: Decision tree-based estimators that are robust to arbitrary scaling of the data.\n- sklearn's **`Scalers` are linear transformers** and differ in the way to estimate the parameters used to shift and scale each feature.\n- sklearn's **`QuantileTransformer` provides non-linear transformations** in which distances between marginal outliers and inliers are shrunk. \n- sklearn's **`PowerTransformer` provides non-linear transformations** in which data is mapped to a normal distribution to stabilize variance and minimize skewness.\n- In addition to scikit's transformers we can apply any mathematical transformation directly, like log transform or box-cox.\n- **Normalization** refers to a per sample transformation instead of a per feature transformation.\n\n<font color=\"darkred\">**Caveat:** \n- We always have to be **careful not to introduce a so called data leakage** by fitting a scaler on the whole dataset (test and train). \n- We **always fit just on the training data** and then just **transform on the training and test data!**\n<\/font>","d52ad1ce":"Do we have duplicates? No, we don't.","713b645a":"# Exploration and Regression on the Auto MPG Data Set\nKernel by [chmaxx](https:\/\/www.kaggle.com\/chmaxx) \u2013 Oktober 2019\n\nMy personal goal with this kernel is to use this simple dataset to learn more about: \n- different **scaling methods** (e.g. standard scaling, power transforms, box-cox)\n- the various **error and scoring metrics** and **how scikit-learn implements these**\n- **dimensionality reduction and clustering with PCA and t-SNE** to get hidden insights about the data","6b2c0080":"We have found 5 outliers in the feature values of `horsepower` and 2 outliers in `acceleration`. We might consider dropping these samples during training. I think though, that they still lie quite close to the normal value range of +\/- 3 standard deviations and can safely be used for modelling.","78f8589d":"- I tried to **follow a systematic way of examining, preparing and modelling data.**\n- I especially tried to dig deeper into the **various ways we can scale data**, **un-skew distributions that aren't normal** and learn more about **error and scoring metrics.**","74851198":">**<font color=\"darkgreen\">MinMaxScaling<\/font>** rescales the data such that all feature values are in the range [0, 1]. Like `StandardScaler()`, the **`MinMaxScaler()` is very sensitive to the presence of outliers.**","67f6a0ce":"There seem to be three distinct groups in the data that correspond to the number of cylinders (4, 6 or 8). \n\nPCA is flexible, fast, and easily interpretable. Yet it **does not perform well when there are nonlinear relationships within the data.** To address this we use so called manifold learning methods. These are unsupervised estimators that seek to describe datasets as low-dimensional manifolds embedded in high-dimensional spaces. These algorithms basically search for locally connected structures and try to project these into less dimensions so that we can plot these.\n\nWe start to visualize our data with [t-SNE](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE).","9e71e9b4":"This confirms our previous insight: \n\n- American manufacturers have a much lower median fuel efficiency.\n- European and japanese manufacturers build significantly more efficient cars.","c6ad59df":"Both transforms improve the linearity of `mpg` significantly. **Boxcox seems to almost totally bring the values back to a normal distribution.** \n\nWe keep that in mind for later, when we actually model the data.","a5bfba72":"> The data is skewed on the lower end and a little bit on the higher end of values. Can we improve this by e.g. log transforming or boxcox transforming `mpg`?","4816e293":"[Taken from machinelearningmastery.com:](https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/)\n\n>**An outlier is an observation that is unlike the other observations.** It is rare, or distinct, or does not fit in some way. \n\n>_Outliers can have many causes, such as: Measurement or input error, data corruption or true outlier observation (e.g. Michael Jordan in basketball).\n**There is no precise way to define and identify outliers in general because of the specifics of each dataset.** Instead, you, or a domain expert, must interpret the raw observations and decide whether a value is an outlier or not.\nNevertheless, we can use statistical methods to identify observations that appear to be rare or unlikely given the available data._ \n\nOutliers will very likely decrease our models accuracy \u2013 since they make no sense and do not follow any regularities that can be learned by the algorithm. They are wrong and possibly have to be excluded from the training data. \n\nA sound practical approach for _normally distributed data_ is to filter values that lie beyond 3 standard deviations from the mean. ","cdae5622":"According to [the dataset source...](https:\/\/archive.ics.uci.edu\/ml\/datasets\/auto+mpg) \n\n>the **\u00abdataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The dataset was used in the 1983 American Statistical Association Exposition.\u00bb**\n\nThe dataset is a slightly modified version where 8 samples with missing values for the \"mpg\" feature have been removed.\n\nAccording to data scientist Ross Quinlan the data **\u00abconcerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\"**","91f3c3af":"In general our model predicts most of the values closely to actual values with some deviations and minor outliers. However, the higher `mpg` is the less precise the model seems to be. We can now more closely examine some of the outliers.","42da31bb":"Are the categorical entries correct? No they aren't...","3fe14be6":"We now setup up all available regression metrics as a list and feed that as an argument into our crossvalidation.\n\nA **regressor can be evaluated using many different metrics** e.g. the following:\n- **Mean absolute error** (MAE): The average of absolute errors of all predicted data points. It uses the same scale as the target variable and gives an idea of how close predictions are to the actual values.\n- **Median absolute error**: The median of all the errors in the given dataset. The main advantage of this metric is that it's robust to outliers. A single bad point in the test dataset wouldn't skew the entire error metric, as opposed to a mean error metric.\n- **Mean squared error** (MSE): The average of the squares of the errors of all predicted data points and a very common metric. We can take the square root on top of the MSE in order to convert the values back into the original scale of the target variable being estimated. This yields the **root mean squared error (RMSE).**\n- **Explained variance score**: This score measures how well our model can account for the variation in our dataset. A score of 1.0 indicates that our model is perfectly able to replicate our target distribution.\n- **R2 score**: indicates the goodness of the fit of a regression model. It ranges from 0 to 1 (but can be negative too), meaning from no fit to perfect prediction.\n\n`mean_squared_log_error` can only be used for strictly positive target values so we leave it out for now.","42186a41":"- **A high powered engine and a heavier car are negatively correlated to `mpg`.** \n- **Newer cars** and **high acceleration** have a **positive correlation**.\n\nSince several regression algorithms rely on a linear relationship between features and target we examine the distribution of `mpg` more thoroughly.","50664b46":"Now I'd like to do something, that I feel I should do way more often \u2013 look at the predictions in a systematic way.\n\nAgain we train a classifier, make predictions with a test set and examine the predictions in more detail.","382b5dac":"## **Try out 10 of the most common classifiers**\n\nWe now setup a couple of regressors to try out in combination with the scalers mentioned above.","9bef5f47":"- We have 9 columns \/ features. \n- `mpg` is the target variable that we want to predict. \n- So we have **8 features to predict from**.","1ad089cf":"## **Data cleaning and preprocessing**\nIn a first step we do some preprocessing and cleaning in order to be able to understand the dataset.","e2de9175":"According to the dataset description we have:\n\n- 5 **continuous features**: `mpt` (the target), `displacement`, `horsepower`, `weight`, `acceleration`\n- 4 **categorical features**: `cylinders` and `model_year` (ordinal), `origin` and `name`","d693a257":"We are now **examining the relation of our target variable to the other correlated features**.","42ac37e7":"## **Problem Definition**","feb3d868":"In this step we will:\n    \n- **scale** our numerical features\n- **un-skew numerical features** and especially `mpg` to be more normally distributed\n- **one hot encode** true categoricals\n- remove `model` as potentially misleading feature since it is (almost) unique to a specific car. We very likely won't see the same model names as categories in unseen data (although we saw in EDA that some models have several makings throughout the years).","85ae0415":"- Throughout the years cars in all three country get more efficient. \n- Interestingly the US shows a slow and steady progress whereas Japan and Europe have ups and downs. This could be due to the small size of the dataset \u2013 we have no idea, how much car models actually where on the market during these 12 years. At the same time japanese and european manufacturers might have reacted much faster and more sensitively to market changes and maybe oil prices. \n- [1973 and 1979 were years of oil crisis](https:\/\/en.wikipedia.org\/wiki\/Oil_crisis) and this might have affected car development in later years. We see a huge jump in efficiency for the US from 79 to the next years. We notice a jump too in Japan from 1973 in relation to the next years.","50175eed":"## **Which features do the classifiers pick up?**\n\nWhat features are relevant for the classifiers? We'll get the feature importance of LightGBM as an example.","21af5153":"# 1. Import libraries and set globals","0851a3ca":"The correlations are clearly noticable. And again: We see clear groups in regard to origin.","fbfba142":"Observations:\n\n- LightGBM again yields the best results overall.\n- **On first sight we get the lowest errors with MinMax** and might **be tempted to use this scaling** rather than the others. **However, MinMax compresses all values to a range between 0 and 1 and thus to a much smaller scale than the other scalers. It therefore yields much lower errors regardless of the models quality!**\n\nLet's stick with standard scaling and let's setup RMSE.","95f8167a":">**PCA is a statistical procedure that converts samples of possibly correlated features into linearly uncorrelated features called principal components.** \n\nThe first principal component has the largest possible variance and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated set of features orthogonal to each other. \n\nWe **define the number of target dimensions when instantiating the PCA model**. We fit the instance to our data and PCA will try to retain as much variance as possible under this constraint. \n\nWe can use PCA as a **tool for visualization** (reducing many feature dimensions to 3d or 2d for plotting), for **noise filtering** (remove unneeded signal that doesn't meaningfully contribute to variance), for **feature extraction and engineering** and much more. \n\nFeatures influence the result in accordance to their absolute values. So PCA is sensitive to the scaling of the data and [results improve very much by standard scaling before fitting PCA.](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html) The PCA implementation of scitkit-learn centers the data by default but doesn't standardize, so we add this step in form of a quick pipeline.","21f17e0d":"## **Examine the data with PCA and t-SNE**\n\nBy applying dimensionality reduction methods like PCA and manifold learning algorithms like t-SNE we aim to gain more hidden insights.","f51392fa":"As expected \u2013 top correlated features like weight, model_year etc. are very informative to the estimator. ","be3679ab":"On to preparing the categorical features. \n\n- `model_year` is ordinal. So we simply stick to dtype `int`.\n- `origin` is a true categorical. We need to one hot encode it later.\n- `name` actually contains two useful bits of information: the cars manufacturer and the model. We split that into two new columns.","daa8ee35":">**<font color=\"darkgreen\">Normalization<\/font>** scales samples **row-wise** to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one. Scaling inputs to unit norms is a **common operation for text classification or clustering**.","8fa18e9b":"## **For what was the dataset created?**","33c4ccca":"Now checking for missing values in more detail.","2be690bf":"## **Setup our data preparation function**\n\nWe\u00b4re ready now to actually model our data and make predictions. I setup a simple data preparation function that allows to use any of the above scikit-learn scalers. ","ce3da6dd":"Let's plot all these transforms on `mpg` (apart from normalization which doesn't make sense here).","4b4ee09a":"# **4. Try out and tune classifiers**","54fa9f61":"# **Conclusion**","467b23fe":"# **3. Data preparation**","3a5e1c72":"[**Skewness**](https:\/\/en.wikipedia.org\/wiki\/Skewness) is the measure how skewed (aka deviant from a normal distribution) our data is. \n\nA normal distribution has a skewness of `0`. A positive value of skewness means that the tail is on the right, and vice versa with negative values. \n\nPandas provides a convenient function. We add a probability plot to visualize the skewness of the sale prices.","f8e64715":"We don't seem to have missing values. However, let's look more closely at `horsepower`.","f2623a33":"We observe that:\n- **US American cars have the least efficient consumption**.\n- **Japanese cars do get the most out of their fuel** and **European cars rank 2nd**.","27de4ef4":"### **A quick [refresher on model evaluation and scoring](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html)**...\n\nHow exactly to do choose an appropiate metric for evaluating our models?","e635b52a":"[](http:\/\/)Ad hoc nothing looks particularly special about these samples. It only looks like that younger car models yield worse predictions. ","d633740e":"As expected \u2013 **only PowerTransformer and QuantileTransformer scale to a more normal distribution.** "}}