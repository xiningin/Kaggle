{"cell_type":{"16a04e96":"code","d7d67831":"code","e718145d":"code","f5390adc":"code","32a3f65c":"code","3a4b02bb":"code","f8e7ce5a":"code","64f8e0d8":"code","1cf46b0a":"code","acbe9cab":"code","7f16ff29":"code","fe0f7d80":"code","5d1f5d22":"code","b10a73aa":"code","a0ad1ae6":"code","b5a4a460":"code","e8fd17fe":"code","40bfc750":"code","a673d119":"code","ec851e1f":"code","7a78187e":"code","2a487b39":"code","d3030a7e":"code","7389d92b":"code","6fcdaa8a":"code","017f116b":"code","d348d27a":"code","4992c1b6":"code","4fd07b36":"code","047e125c":"code","fdc92da4":"code","33efc8a1":"code","dc62d6ae":"code","5315664f":"code","0f08f22d":"code","379cf286":"code","1aff8564":"code","350ebf0b":"code","5ebc89e8":"code","92256c22":"code","65f008fb":"code","8fc8254f":"code","9701cf00":"code","67e9e762":"code","ae39a49b":"markdown","04f7351c":"markdown","70089d67":"markdown","59e1abe0":"markdown","a57a1c24":"markdown","6050cad1":"markdown","7905c7c1":"markdown","203e7bef":"markdown","fa357ea9":"markdown","6ade0d75":"markdown","70e5015b":"markdown","36ce3a28":"markdown","49560be3":"markdown","a36d6850":"markdown","be051508":"markdown","7154d14b":"markdown","4dbe9711":"markdown","c0be38b2":"markdown","951d8172":"markdown","21d6838b":"markdown","32a8381f":"markdown","69619828":"markdown","8e7437cf":"markdown","51da0a88":"markdown","05531c5a":"markdown","e12406de":"markdown","95ce050a":"markdown","c5239fc5":"markdown","84d4781d":"markdown","b8ea1892":"markdown","e4ebc963":"markdown","9a0c7371":"markdown","cf504110":"markdown","4587b702":"markdown","cd8e832c":"markdown","6b1308e5":"markdown","8a513871":"markdown","00b72609":"markdown","d36cf184":"markdown","2254a9ac":"markdown"},"source":{"16a04e96":"import tensorflow as tf\ntf.__version__","d7d67831":"import pandas as pd\n\n# For local:\noriginal_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\noriginal_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\noriginal_test","e718145d":"# remove not-useful columns\ntrain = original_train.drop(columns=[\"Name\",\"Ticket\",\"Cabin\"])\ntest = original_test.drop(columns=[\"Name\",\"Ticket\",\"Cabin\"])\ntrain","f5390adc":"# One hot encode the categorical data\ntrain_onehot = pd.get_dummies(train)\ntest_onehot = pd.get_dummies(test)\ntrain_onehot","32a3f65c":"# Create features and labels\nX_train, y_train = train_onehot.drop(columns=[\"Survived\"]), train_onehot[\"Survived\"]\nX_train","3a4b02bb":"# Let's check the shapes\nX_train.shape, y_train.shape","f8e7ce5a":"# 1. Build the model\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")  \n])\n\n# 2. Compile the model\nmodel_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n                metrics=[\"accuracy\"])\n\n# 3. Fit the model\nmodel_1.fit(X_train, \n            y_train, \n            epochs=100, \n            batch_size=32,\n            verbose=1)","64f8e0d8":"# check in which columns nan values are occuring\nX_train.isna().sum()","1cf46b0a":"X_train = X_train.drop(columns=[\"Age\"])\ntest_onehot = test_onehot.drop(columns=[\"Age\"])\nX_train","acbe9cab":"# Set the random seed\ntf.random.set_seed(6)\n\n# 1. Build the model\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")  \n])\n\n# 2. Compile the model\nmodel_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                metrics=[\"accuracy\"])\n\n# 3. Fit the model\nmodel_1.fit(X_train, \n            y_train, \n            epochs=100, \n            batch_size=32,\n            verbose=1)","7f16ff29":"# Set the random seed\ntf.random.set_seed(6)\n\n# 1. Build the model\nmodel_2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")  \n])\n\n# 2. Compile the model\nmodel_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                metrics=[\"accuracy\"])\n\n# 3. Fit the model\nmodel_2.fit(X_train, \n            y_train, \n            epochs=100, \n            batch_size=32,\n            verbose=1)","fe0f7d80":"# Let's view the data again\nX_train","5d1f5d22":"# # Drop the embarked columns (they won't affect survival)\nX_train = X_train.drop(columns=[\"Embarked_C\", \"Embarked_Q\", \"Embarked_S\"])\ntest_onehot = test_onehot.drop(columns=[\"Embarked_C\", \"Embarked_Q\", \"Embarked_S\"])","b10a73aa":"# Combine the parch (parents and children aboard) and sibsp (siblings and spouses aboard) columns\n# so that our model can view a total family_count\nX_train[\"family_count\"] = X_train[\"SibSp\"] + X_train[\"Parch\"]\n# X_train = X_train.drop(columns=[\"SibSp\",\"Parch\"])\ntest_onehot[\"family_count\"] = test_onehot[\"SibSp\"] + test_onehot[\"Parch\"]\n# test_onehot = test_onehot.drop(columns=[\"SibSp\",\"Parch\"])\nX_train","a0ad1ae6":"# Bring back the name column\nX_train[\"Name\"] = original_train[\"Name\"]\ntest_onehot[\"Name\"] = original_test[\"Name\"]\nX_train","b5a4a460":"# Get the different titles of people from the name column (Dr., Sir. Countess...)\ndef get_titles_from_names(df):\n    title=\"\"\n    titles = []\n    for name in df[\"Name\"]:\n        if name.split(\",\")[1].split(\" \")[1]==\"the\":\n            title = \"Countess.\"\n        else:\n            title = name.split(\",\")[1].split(\" \")[1]\n        titles.append(title)\n    df[\"titles\"] = titles\n    print(set(titles))\nget_titles_from_names(X_train)\nget_titles_from_names(test_onehot)\nX_train = X_train.drop(columns=[\"Name\"])\ntest_onehot = test_onehot.drop(columns=[\"Name\"])","e8fd17fe":"# Visualize our X_train and y_train together\npd.concat([X_train, y_train], axis=1)","40bfc750":"# assign each title to a group\nMr_synonyms = ['Mr.','Dr.','Rev.','Major.','Col.','Capt.','Sir.','Jonkheer.','Don.']\nMrs_synonyms = ['Mrs.','Countess.','Mme.','Lady.']\nMiss_synonyms = ['Miss.','Ms.','Mlle.']\nMaster_synonyms = ['Master.']\ndef set_group_based_on_title(titles):\n    group = []\n    for title in titles:\n        if title in Mr_synonyms:\n            group.append(0)\n        elif title in Master_synonyms:\n            group.append(1)\n        elif title in Miss_synonyms:\n            group.append(2)\n        elif title in Mrs_synonyms:\n            group.append(3)\n        else:\n            group.append(0)\n    return group\nX_train[\"title_priority\"] = set_group_based_on_title(X_train[\"titles\"])\ntest_onehot[\"title_priority\"] = set_group_based_on_title(test_onehot[\"titles\"])\nX_train = X_train.drop(columns=[\"titles\"])\ntest_onehot = test_onehot.drop(columns=[\"titles\"])","a673d119":"# Calculate the fare per person\nX_train['Fare_Per_Person'] = X_train['Fare']\/(X_train['family_count']+1)\ntest_onehot['Fare_Per_Person'] = test_onehot['Fare']\/(test_onehot['family_count']+1)\nX_train = X_train.drop(columns=[\"Fare\"])\ntest_onehot = test_onehot.drop(columns=[\"Fare\"])","ec851e1f":"# View our feature engineered data\nX_train","7a78187e":"# View test_onehot\ntest_onehot","2a487b39":"# Set the random seed\ntf.random.set_seed(6)\n\n# 1. Build the model\nmodel_3 = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")  \n])\n\n# 2. Compile the model\nmodel_3.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                metrics=[\"accuracy\"])\n\n# 3. Fit the model\nmodel_3.fit(X_train, \n            y_train, \n            epochs=200,\n            verbose=1)","d3030a7e":"# Check out its accuracy on the training data\nmodel_3.evaluate(X_train, y_train)","7389d92b":"import numpy as np\n\ndef generate_predictions(model, test_data):\n    submission=pd.DataFrame()\n    submission[\"PassengerId\"] = test[\"PassengerId\"]\n    submission[\"Survived\"] = np.nan_to_num(model.predict(test_data).round().squeeze()).astype(int)\n    print(submission)\n    submission.to_csv(\"submission.csv\", index=False)","6fcdaa8a":"generate_predictions(model_3, test_onehot)","017f116b":"from sklearn.ensemble import RandomForestClassifier\n\n# 1. Build the model\nrf_1 = RandomForestClassifier(n_estimators=100)\n\n# 2. Fit the model\nrf_1.fit(X_train, y_train)","d348d27a":"# Create our predictions\ngenerate_predictions(rf_1, test_onehot.fillna(0))","4992c1b6":"# Import the library\nimport xgboost\n\n# 1. Build the model\nxgbc_1 = xgboost.XGBClassifier()\n\n# 2. Fit the model\nxgbc_1.fit(X_train, y_train)","4fd07b36":"generate_predictions(xgbc_1, test_onehot)","047e125c":"# View the original dataset\noriginal_train","fdc92da4":"# Plot features importance for xgbc_1\nfrom xgboost.plotting import plot_importance\nplot_importance(xgbc_1);","33efc8a1":"# PassengerID shouldn't be that important\nX_train = X_train.drop(columns=[\"PassengerId\"])\ntest_onehot = test_onehot.drop(columns=[\"PassengerId\"])","dc62d6ae":"isAlone = []\nfor family in X_train[\"family_count\"]:\n    if family==0:\n        isAlone.append(1)\n    else:\n        isAlone.append(0)\nX_train[\"isAlone\"] = isAlone\nX_train","5315664f":"\nisAlone = []\nfor family in test_onehot[\"family_count\"]:\n    if family==0:\n        isAlone.append(1)\n    else:\n        isAlone.append(0)\ntest_onehot[\"isAlone\"] = isAlone\ntest_onehot","0f08f22d":"# Bring back the age column\nAge = original_train[\"Age\"].fillna(original_train[\"Age\"].median())\nX_train[\"Age\"] = Age\nAge = original_test[\"Age\"].fillna(original_train[\"Age\"].median())\ntest_onehot[\"Age\"] = Age","379cf286":"# Bring back the cabin column\nCabin = original_train[\"Cabin\"].fillna(\"U\")\ncabins = []\nfor cabin in Cabin:\n    cabins.append(cabin[0])\nX_train[\"Cabin\"] = cabins\nX_train = pd.get_dummies(X_train)\n\nCabin = original_test[\"Cabin\"].fillna(\"U\")\ncabins = []\nfor cabin in Cabin:\n    cabins.append(cabin[0])\ntest_onehot[\"Cabin\"] = cabins\ntest_onehot = pd.get_dummies(test_onehot)\n\nX_train","1aff8564":"test_onehot[\"Cabin_T\"] = 0\nCabin_U = test_onehot[\"Cabin_U\"]\ntest_onehot = test_onehot.drop(columns=[\"Cabin_U\"])\ntest_onehot[\"Cabin_U\"] = Cabin_U\ntest_onehot","350ebf0b":"X_val = X_train[int(X_train.shape[0]*.8):]\nX_train_new = X_train[:int(X_train.shape[0]*.8)]\ny_val = y_train[int(y_train.shape[0]*.8):]\ny_train_new  = y_train[:int(y_train.shape[0]*.8)]\nX_val","5ebc89e8":"# Import the library\nimport xgboost\n\n# 1. Build the model\nxgbc_2 = xgboost.XGBClassifier(\n    learning_rate = 0.03,\n    n_estimators = 200,\n    max_depth = 1,\n    subsample = 0.8,\n    colsample_bytree = 0.5,\n    gamma = 1,  \n)\n\neval_set = [(X_train_new, y_train_new), (X_val, y_val)]\neval_metric = [\"logloss\", \"auc\"]\n# 2. Fit the model\nhistory = xgbc_2.fit(X_train_new, y_train_new, eval_metric=eval_metric, eval_set=eval_set, verbose=False)","92256c22":"import matplotlib.pyplot as plt\n\nresults = xgbc_2.evals_result()\nplt.plot(results['validation_0']['logloss'], label='train_logloss')\nplt.plot(results['validation_1']['logloss'], label='validation_logloss')\nplt.legend();","65f008fb":"plt.plot(results['validation_0']['auc'], label='train_auc')\nplt.plot(results['validation_1']['auc'], label='validation_auc')\nplt.legend();","8fc8254f":"xgbc_2.score(X_val, y_val)","9701cf00":"generate_predictions(xgbc_2, test_onehot)","67e9e762":"from xgboost.plotting import plot_importance\n\nplot_importance(xgbc_2)","ae39a49b":"### Create predictions","04f7351c":"### 78% Accurate ","70089d67":"### **77.5%** accurate","59e1abe0":"### Recreate model_1\n\nNow that we have removed the `nan` values, our model should start to work.","a57a1c24":"We have succesfully feature engineered our data. Now, let's see how our model will perform.\n\n---\n\n","6050cad1":"## model_3","7905c7c1":"Submitting this to Kaggle won't give us a better score so we will have to do some more feature engineering.","203e7bef":"## xgbc_1","fa357ea9":"### Create the model","6ade0d75":"# Random Forests\n\nAs mentioned in this [post](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/295548#1622634), DL requires large amounts of data, and the titanic training set has only about 900 rows of data.\n\nA better approach that we could use is [Random Forests](https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2). \n\nA Random Forest is basically a group of independent decision trees. Each decision tree outputs it's own class prediction for a randomly chosen subset of the data. Then, the decision tree with the most class votes becomes the model's predictions. \n\nThis cooperative functionality allows Decision Trees to be less prone to overfitting, and a good option for Binary Classification.","70e5015b":"# XGBoost\n\nUptil now, our most accurate model was a `GradientBoostedTreesModel`. Let's try a variant of this called **XGBoost**.\n\nInstead of minimizing a BINOMIAL_LOG_LIKELIHOOD loss function like `gbm_1`, [XGBoost](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-HowItWorks.html) minimizes a regularized L1 or L2 loss function.\n\nA loss function without any regularization computes the difference between `y_true` and `y_pred`. \n\nA **L1 loss function** adds the sum of the absolute values of the weights to the loss function.\n\nA **L2 Loss function** adds the sum of the squares of the weights to the loss function.\n\nBy implementing these regularizations to the typical Gradient Boost algorithm, XGBoost is able to acheive better results than a normal gbm.\n","36ce3a28":"# Summary\n\nIn this notebook we expored different binary classification techniques and got 78% Accuracy using popular Gradient Boosting algorithms like XGBoost. We learned how to use Feature Engineering to make the data easier to interpret for the model. \n\nI hope you found this kernel helpful in you journey through the Titanic competition. See if you can get a better accuracy than me and post it in the comments. Upvote this notebook if you liked it.","49560be3":"Increasing the hidden layers doesn't seem to be much of a help. Let's try to apply **feature engineering** on our data to further improve our model's performance.","a36d6850":"### Create validation data","be051508":"## Preproccess the data","7154d14b":"## rf_1\n","4dbe9711":"# Neural Network Approach","c0be38b2":"After running the previous cell, you should see the `.csv` in your project dir. After submitting this on Kaggle, we get an accuracy of `75.5%`. This is good, but we can do better.","951d8172":"### Create the model","21d6838b":"After submitting it to a [Kaggle](https:\/\/www.kaggle.com\/c\/titanic\/leaderboard), we get an improved accuracy of `77.5%`. ","32a8381f":"The Age column seems to have all of the `nan` values. Let's drop it out of our dataframe.","69619828":"### Create the model\n\nTo start, we'll create a simple model with 1 hidden layer.","8e7437cf":"### Create the model","51da0a88":"### Check out it's accuracy on the training set","05531c5a":"## xgbc_2","e12406de":"## model_1","95ce050a":"Hmmm... Seems like our loss is `nan`. \n\nThis tells us that there might be some `nan` values in our data that are confusing our model. Let's check this out.","c5239fc5":"### Check out its accuracy on the validation data","84d4781d":"We have got our training accuracy to about 80%. Let's see how it performs on [Kaggle](https:\/\/www.kaggle.com\/c\/titanic\/leaderboard).","b8ea1892":"## Read in the data","e4ebc963":"### Make the predictions\n\nWe will now generate our `submission.csv` using `model_3`.","9a0c7371":"Yes! We now have our baseline model with an accuracy of `0.8002` and a loss of `0.4721`. \n\nOur model is still fairly simple. Let's increase the complexity by adding more hidden layers.","cf504110":"### **75.5%** Accurate","4587b702":"## model_2 (adding more layers)","cd8e832c":"# Predict survival on the Titanic\n\nGiven information about a passenger, our job is to predict if the person survived on the Titanic.\n\nWe are going to use the [Kaggle Titanic Dataset](https:\/\/www.kaggle.com\/c\/titanic\/data).","6b1308e5":"### Create the model","8a513871":"### Create predictions","00b72609":"### Create submission.csv","d36cf184":"## Feature Engineering\n\nFeature engineering is basically messing around with the data so it easier for our model to find patterns in it.\n\nWe already applied this concept when we removed the Age column filled with `nan` values. Now, let's see how we can mess around with the data some more to improve our results.","2254a9ac":"## Feature Engineering"}}