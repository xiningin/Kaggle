{"cell_type":{"1a0ee065":"code","be649c10":"code","f6a8e988":"code","28d91f16":"code","ec82a853":"code","4f72a964":"code","a81238a7":"code","748e6ef2":"code","4c32aa4d":"code","1a19f604":"code","4fc9daf7":"code","d71c9ef0":"code","145fd486":"code","5a813853":"code","4d446bd0":"code","fd75d0f4":"code","70c14b31":"code","70da92ec":"code","2bf66b1f":"code","9d5526a3":"code","d6ffd8b6":"code","9da64b84":"code","ef8632d0":"code","71e352be":"code","c51832a2":"code","52b007e9":"code","39fc7a2f":"code","b0afdb56":"code","935c8970":"markdown","2454dec9":"markdown","4ede9815":"markdown","0f28ba6a":"markdown","f51dcf2b":"markdown","b1d9c49b":"markdown","4ae1f321":"markdown","e75a512c":"markdown","2d5101fe":"markdown","05089ee1":"markdown","31426725":"markdown","a8554835":"markdown","b59244ef":"markdown","9df59e59":"markdown","67357a3c":"markdown","96576c8b":"markdown","33cee04e":"markdown","dbc16f18":"markdown","2de62611":"markdown","a13fcb63":"markdown"},"source":{"1a0ee065":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import Ridge\nfrom scipy.stats import pearsonr\n\nfrom kaggle_secrets import UserSecretsClient\n\nimport wandb\n# always wanted to try it out - now it's time to do so! :-) \n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n#sns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","be649c10":"use_wandb=False\nif use_wandb:\n    user_secrets = UserSecretsClient()\n    secret_value_wb = user_secrets.get_secret(\"wandb\")\n    ! wandb login $secret_value_wb\n    wandb.init(project=\"ubiquant\", name=\"starter\")","f6a8e988":"train = pd.read_parquet('..\/input\/ubiquant-parquet\/train_low_mem.parquet')\ntrain.head()","28d91f16":"test = pd.read_parquet('..\/input\/ubiquant-parquet\/example_test.parquet')\ntest.head()","ec82a853":"train.shape[0]","4f72a964":"train.investment_id.nunique()","a81238a7":"train.groupby(\"investment_id\").time_id.max().value_counts()","748e6ef2":"train.groupby(\"investment_id\").time_id.max().value_counts().index.max()","4c32aa4d":"train.groupby(\"investment_id\").time_id.max().value_counts().index.min()","1a19f604":"selection = train.groupby(\"investment_id\").time_id.max()\noutlier_inv_ids = selection[selection != 1219].index.values\nlen(outlier_inv_ids)","4fc9daf7":"plt.figure(figsize=(20,10))\nfor n in range(50):\n    plt.plot(train[train.investment_id == outlier_inv_ids[n]].time_id,\n               train[train.investment_id == outlier_inv_ids[n]].target.cumsum(), '.', alpha=0.5)\n    plt.xlim([0,1220])\n    plt.title(\"Return\/target cumsum for outlier investments\")\n    plt.xlabel(\"time_id\")\n    plt.ylabel(\"cumsum return\");","d71c9ef0":"selection = train.groupby(\"investment_id\").time_id.max()\nnot_outlier_inv_ids = selection[selection == 1219].index.values\nlen(not_outlier_inv_ids)","145fd486":"plt.figure(figsize=(20,10))\nfor n in range(100):\n    plt.plot(train[train.investment_id == not_outlier_inv_ids[n]].time_id,\n               train[train.investment_id == not_outlier_inv_ids[n]].target.cumsum(), '.', alpha=0.5)\n    plt.xlim([0,1220])\n    plt.title(\"Return\/target for non-outlier investments\")\n    plt.xlabel(\"time_id\")\n    plt.ylabel(\"return\");","5a813853":"num_investments_per_time_id = train.groupby(\"time_id\").investment_id.nunique()\n\nplt.figure(figsize=(20,5))\nplt.plot(num_investments_per_time_id.index, num_investments_per_time_id.values, 'o', color=\"black\")\nplt.xlabel(\"time_id\")\nplt.ylabel(\"count\")\nplt.title(\"Number of unique investment_ids given time_id\");","4d446bd0":"fig, ax = plt.subplots(2,2,figsize=(20,10))\nsns.distplot(train.target.values, color=\"red\", ax=ax[0,0])\nax[0,0].set_title(\"Target distribution\")\nn = 0\nselected_id = train[train.target > 8].investment_id.values[n]\nselection = train[train.investment_id == selected_id][[\"time_id\", \"target\"]]\nax[0,1].set_title(\"Target distribution of investment id {}\".format(selected_id))\nsns.distplot(selection.target.values, ax=ax[0,1], color=\"seagreen\")\nax[1,0].plot(selection.time_id.values, selection.target.values)\nax[1,0].set_title(\"Target timeseries of investment id {}\".format(selected_id))\nax[1,1].plot(selection.time_id.values, selection.target.cumsum().values)\nax[1,1].set_title(\"Target timeseries cumsum of investment id {}\".format(selected_id))","fd75d0f4":"my_feature = \"f_0\"\ntrain.loc[:, my_feature+\"_diff\"] = train.groupby(\"investment_id\")[my_feature].diff()\nselection = train.loc[:, [\"time_id\", \"investment_id\", my_feature, my_feature+\"_diff\", \"target\"]].dropna()","70c14b31":"import matplotlib.animation as animation\nfrom matplotlib import rc\nrc('animation', html='html5')\n\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\nclass ShowFeatureOverTimeID:\n\n    def __init__(self, df, feature):\n        self.df = df\n        self.max_iter = df.time_id.max() - 1\n        self.feature = feature\n        \n        data = self.df[self.df.time_id==1][self.feature]\n        self.fig, self.ax = plt.subplots()\n        _, _, self.bar_container = self.ax.hist(data, 70, lw=0.1, fc=\"blue\", alpha=0.6)\n        self.ax.set_xlim([self.df[feature].min(),self.df[feature].max()])\n        self.ax.set_title(\"How does the feature distribution change over time_id?\")\n        self.ax.set_ylabel(\"count\")\n        self.ax.set_xlabel(feature)\n        \n    def show(self):\n        self.ani = animation.FuncAnimation(self.fig, self.prepare_animation(self.bar_container),\n                                           frames=self.max_iter,\n                                           blit=False,\n                                           repeat=False)\n        #Writer = animation.writers['ffmpeg']\n        #writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n        #self.ani.save('example.mp4', writer=writer)\n        plt.close()\n        return self.ani\n    \n    def prepare_animation(self,bar_container):\n        def animate(i):\n            \n            data = self.df[self.df.time_id==i+1][self.feature]\n            n, _ = np.histogram(data, 100)\n            for count, rect in zip(n, bar_container.patches):\n                rect.set_height(count)\n            return bar_container.patches\n        return animate","70da92ec":"ani = ShowFeatureOverTimeID(selection, \"target\")\nani.show()","2bf66b1f":"plt.figure(figsize=(20,5))\nplt.plot(selection.groupby(\"time_id\")[\"f0_diff\"].mean())\nplt.title(\"How does the mean of f_0 changes behave over time?\");\nplt.xlabel(\"time_id\")\nplt.ylabel(\"f0_diff mean over all investment ids\");","9d5526a3":"train.head()","d6ffd8b6":"selection = None","9da64b84":"selection = [\"time_id\", \"investment_id\"]\nfeature_names = train.drop([\"row_id\", \"time_id\", \"investment_id\", \"target\"], axis=1).columns.values\nN = np.random.choice(np.arange(0, len(feature_names)), replace=False, size=100)\nmy_features = [\"f_{}\".format(n) for n in N]\nselection.extend(my_features)","ef8632d0":"X = train[selection].copy()\nY = train.target\nfor f in my_features:\n    X[f + \"_shift1\"] = X.groupby(\"investment_id\")[f].shift(1).fillna(0)\n\n    #X[f + \"_diff\"] = X[f] - X[f+\"_shift1\"]\n    #X[f + \"_diff\"] = X[f+\"_diff\"].fillna(0)","71e352be":"X.shape","c51832a2":"features = [\"f_{}_shift1\".format(n) for n in N]\nfeatures.extend([\"f_{}\".format(n) for n in N])\nprint(len(features))","52b007e9":"V = 0.9\nx_train, x_dev = X[0:int(V*X.shape[0])][features], X[int(V*X.shape[0])::][features]\ny_train, y_dev = Y[0:int(V*X.shape[0])], Y[int(V*X.shape[0])::]","39fc7a2f":"lm = Ridge()\nlm.fit(x_train.values, y_train.values)\ny_pred = lm.predict(x_dev)\npearsonr(y_dev, y_pred)","b0afdb56":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n\nprevious_test_df = train[train.time_id == train.time_id.max()].iloc[0:int(V*train.shape[0])]\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    for f in my_features:\n        test_df.loc[:, f+\"_shift1\"] = 0\n        already_known = previous_test_df[previous_test_df.investment_id.isin(test_df.investment_id)]\n        test_df.loc[test_df.investment_id.isin(already_known.investment_id), f+\"_shift1\"] = already_known[f]\n        test_df.loc[:, f+\"_shift1\"] = test_df.loc[:, f+\"_shift1\"].fillna(0)\n        \n        #test_df.loc[:, f+\"_diff\"] = test_df[f] - test_df[f+\"_shift1\"]\n        #test_df.loc[:, f+\"_diff\"] = test_df.loc[:, f+\"_diff\"].fillna(0)\n    \n    pred = lm.predict(test_df[features])\n    sample_prediction_df['target'] = pred  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions\n    previous_test_df = test_df.copy()","935c8970":"# Ubiquant market prediction - EDA\n\n<img src=\"https:\/\/cdn.pixabay.com\/photo\/2017\/08\/30\/07\/56\/clock-2696234_1280.jpg\" width=\"800px\">","2454dec9":"### How is the target distributed?","4ede9815":"The loop seems to work. Next topic - automatic feature generation and a few more experiments on the feature per time_id distributions.","0f28ba6a":"What's the maximum time_id per investment id?","f51dcf2b":"### Insights\n\n* The number of unique investment ids seems to be roughly constant in the beginning but has an increasind trend after the crazy id 400. \n* We can see that the number of investments given the time id varies especially around the id 400.\n\nWhat does that mean...? The description says...\n\n>  The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\nHmm... what?!\n\nDoes it mean given a single investment the time id is in order but the time between ids can vary (also for this single investment)? If so, do we have different \"temporal spaces\" per investment? Then time_id 12 for example would be a different time for each investment? Or does it mean that this id 12 is the same real time for all investment ids? ","b1d9c49b":"### How many investments do we have per time id?","4ae1f321":"### Insights\n\n* Looks like Student's t-distribution (like normal with heavy tails).\n* Browsing through different outlier series the heavy tails belong to steep changes given our temporal interval. Looking at the cumsum we can see that it does not necessarily mean that this is a strange behaviour as we could also have strong changes over a small number of time_id steps. ","e75a512c":"### Insights\n\n* We can clearly see that some investments miss parts of their timeseries or end earlier.\n* Looking back into the competition description, we find: \"The ID code for an investment. **Not all investment have data in all time IDs**.\"\n* A lot of investment ids have a **break** close to time_id 400 (roughly **380**) and\/or stop at roughly **1050**.  ","2d5101fe":"# How are features (or the target) distributed over time?","05089ee1":"# Model structure <a class=\"anchor\" id=\"model\"><\/a>\n\nOk, this problem has some difficulties:\n\n* We have investment_ids that can be present in train & test or only in train or only in test. \n* We can have some kind of measurement gaps where values are missing.\n* Timeseries can have different lengths in total. \n* The time inverals can vary.\n\n","31426725":"There are no ids with an id > 1219 or < 62. We should better look at a few examples. By the way - how many outlier samples do we have?","a8554835":"# Motivation <a class=\"anchor\" id=\"motivation\"><\/a>\n\nOur goal is to predict a metric (not known by us but related to the return rate) that should help traders to make a trading decision. To solve this task we are given:\n\n* 300 anonymized features\n* different investments. They are not the same all the time, there can be different investment ids in the test data than in train [(look here)](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/301693).\n* time_ids per investment\n* (and row ids)\n\nThe features in the training set were derived using real historic data. Furthermore the description says that: \n\n**the final private leaderboard will be determined using data gathered after the training period closes, which means that the public and private leaderboards will have zero overlap**\n\nWhat exactly does this mean? Is the test data gathered in temporal order right after the training period? Is it subsequent? Or could it also mean that the test data was gathered later after some time has passed by? For me this is not really clear...","b59244ef":"# Initial explorations <a class=\"anchor\" id=\"initial_eda\"><\/a>","9df59e59":"# Table of contents\n\n- [Motivation](#motivation)\n- [Loading packages and data](#packages)\n- [Initial explorations (to be continued)](#initial_eda)\n- [Feature generation (to be continued)](#new_features)\n    - [Operations to perform](#operations)\n    - [The feature selector](#selector)\n    - [Model structure](#model)\n    - [Go, honey, go!](#search)","67357a3c":"# Loading packages and data <a class=\"anchor\" id=\"packages\"><\/a>","96576c8b":"How much samples can be found?","33cee04e":"Before adding more and more content about feature engineering, I like to build a loop that will allow to add previous features.","dbc16f18":"That's interesting! Most of the ids have a max time_id of 1219. But there are a few ids that differ from them. ","2de62611":"How many investments are present?","a13fcb63":"# Exploratory Analysis <a class=\"anchor\" id=\"exploration\"><\/a>"}}