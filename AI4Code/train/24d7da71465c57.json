{"cell_type":{"29799bd2":"code","f2cc5fcc":"code","05781a16":"code","5bada8a7":"code","0213110b":"code","9c56f23d":"code","15764abe":"code","bfcd590b":"code","e2497b60":"code","ce1bd1f6":"code","60164535":"code","cd01880b":"code","ad47b113":"code","4471dcab":"code","22e25279":"code","fe763414":"code","79fc0f2e":"code","ad71f52d":"code","b7f27075":"code","cea3d5a3":"code","30cae399":"code","465b81a5":"code","938d3d26":"code","61f63622":"code","3a970cc9":"code","80438a8f":"code","f016c566":"code","a9add08b":"code","0e7ebdab":"code","e58c242e":"code","e7afa666":"code","d7b0ab04":"code","b37fa11c":"code","e6697ab1":"code","030c8619":"code","efaa3688":"markdown"},"source":{"29799bd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f2cc5fcc":"import pandas as pd\nfrom tqdm import tqdm_notebook","05781a16":"train = pd.read_csv(\"..\/input\/train.csv\")","5bada8a7":"train","0213110b":"y = train[\"label\"]\nX = train.drop(labels = [\"label\"],axis = 1)","9c56f23d":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt","15764abe":"y =  np.array(y)\nX = np.array(X)","bfcd590b":"index = 100\ns= np.reshape(X[index],(28,28))\nplt.imshow(s)\nprint(y[index])","e2497b60":"tf.reset_default_graph()\nimg_placeholder = tf.placeholder(dtype=tf.float32,shape=(None,28,28,1),name=\"image\")\nlabel_placeholder = tf.placeholder(dtype=tf.float32,shape=(None,10),name=\"label\")","ce1bd1f6":"def model(img_placeholder):\n    w1=tf.get_variable(name=\"w1\",shape=[3,3,1,64],initializer=tf.contrib.layers.xavier_initializer())\n    conv1=tf.nn.conv2d(input=img_placeholder,strides=[1,1,1,1],filter=w1,padding=\"SAME\",name=\"conv1\")\n    bias1 = tf.get_variable(name=\"b1\",shape=[64],initializer=tf.contrib.layers.xavier_initializer())\n    add = tf.nn.bias_add(conv1,bias1)\n    act1=tf.nn.leaky_relu(add,name=\"activation1\")\n    print(act1)\n    w2=tf.get_variable(name=\"w2\",shape=[3,3,64,128],initializer=tf.contrib.layers.xavier_initializer())\n    conv2=tf.nn.conv2d(input=act1,strides=[1,1,1,1],filter=w2,padding=\"SAME\",name=\"conv2\")\n    bias2 = tf.get_variable(name=\"b2\",shape=[128],initializer=tf.contrib.layers.xavier_initializer())\n    add2 = tf.nn.bias_add(conv2,bias2)\n    act2=tf.nn.leaky_relu(add2,name=\"activation2\")\n    print(act2)\n    w3=tf.get_variable(name=\"w3\",shape=[3,3,128,256],initializer=tf.contrib.layers.xavier_initializer())\n    conv3=tf.nn.conv2d(input=act2,strides=[1,1,1,1],filter=w3,padding=\"SAME\",name=\"conv3\")\n    bias3 = tf.get_variable(name=\"b3\",shape=[256],initializer=tf.contrib.layers.xavier_initializer())\n    add3 = tf.nn.bias_add(conv3,bias3)\n    act3=tf.nn.leaky_relu(add3,name=\"activation3\")\n    print(act3)\n    mpool1 = tf.nn.max_pool(value=act3,ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"VALID\",name=\"maxpooling1\")\n    print(mpool1)\n    w4=tf.get_variable(name=\"w4\",shape=[3,3,256,512],initializer=tf.contrib.layers.xavier_initializer())\n    conv4=tf.nn.conv2d(input=mpool1,strides=[1,1,1,1],filter=w4,padding=\"SAME\",name=\"conv4\")\n    bias4 = tf.get_variable(name=\"b4\",shape=[512],initializer=tf.contrib.layers.xavier_initializer())\n    add4 = tf.nn.bias_add(conv4,bias4)\n    act4=tf.nn.leaky_relu(add4,name=\"activation4\")\n    mpool2 = tf.nn.max_pool(value=act4,ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"VALID\",name=\"maxpooling2\")\n    print(mpool2)\n    w5=tf.get_variable(name=\"w5\",shape=[3,3,512,1024],initializer=tf.contrib.layers.xavier_initializer())\n    conv5=tf.nn.conv2d(input=mpool2,strides=[1,1,1,1],filter=w5,padding=\"SAME\",name=\"conv5\")\n    bias5 = tf.get_variable(name=\"b5\",shape=[1024],initializer=tf.contrib.layers.xavier_initializer())\n    add5 = tf.nn.bias_add(conv5,bias5)\n    act5=tf.nn.leaky_relu(add5,name=\"activation5\")\n    print(act5)\n    flatten = tf.layers.flatten(act5,name=\"flatten\")\n    print(flatten)\n    dense1 = tf.layers.dense(flatten,4096,activation=tf.nn.leaky_relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n    print(dense1)\n    dense2 = tf.layers.dense(dense1,4096,activation=tf.nn.leaky_relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n    print(dense2)\n    dense3 = tf.layers.dense(dense2,2048,activation=tf.nn.leaky_relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n    print(dense3)\n    dense4 = tf.layers.dense(dense3,1024,activation=tf.nn.leaky_relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n    print(dense4)\n    dense5 = tf.layers.dense(dense4,512,activation=tf.nn.leaky_relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n    print(dense5)\n    dense6 = tf.layers.dense(dense5,512,activation=tf.nn.leaky_relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n    print(dense6)\n    dense6 =tf.layers.dense(dense6,10,activation=tf.nn.leaky_relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n    print(dense6)\n    softmax_out = tf.nn.softmax(logits=dense6)\n    return dense6,softmax_out","60164535":"logits,soft=model(img_placeholder)\nprint(soft)\nloss=tf.reduce_mean(input_tensor=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=label_placeholder),name=\"loss\")\noptimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\naccur = 100*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(soft, 1), tf.argmax(label_placeholder, 1))))\nout_label = tf.argmax(soft, axis=1)\nvalloss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=label_placeholder)","cd01880b":"from sklearn.model_selection import train_test_split","ad47b113":"from sklearn.preprocessing import LabelBinarizer\nbinencoder = LabelBinarizer()\nY = binencoder.fit_transform(y)","4471dcab":"y","22e25279":"X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.1,shuffle=True)","fe763414":"X_train = X_train\/784\nX_test = X_test\/784","79fc0f2e":"X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28 , 1)","ad71f52d":"X_train.shape","b7f27075":"X_test.shape","cea3d5a3":"epochs = 20\nbatchsize = 100\nval_batchsize = 500","30cae399":"modelpath = \"..\/savedmodel\"\nif not os.path.exists(modelpath):\n    os.mkdir(modelpath)\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    batch_count = X_train.shape[0]\/\/batchsize\n    batch_count_v = X_test.shape[0]\/\/val_batchsize\n    for epoch in range(epochs):\n        for batch in tqdm_notebook(range(batch_count)):\n            image_batch = X_train[(batch*batchsize):(batch*batchsize)+batchsize]\n            label_batch = y_train[(batch*batchsize):(batch*batchsize)+batchsize]\n            _,train_loss, train_accur=sess.run(fetches=[optimizer,loss,accur],feed_dict={img_placeholder:image_batch.astype(np.float32),label_placeholder:label_batch.astype(np.float32)})\n        for tbatch in tqdm_notebook(range(batch_count_v)):\n            timage_batch = X_test[(tbatch*val_batchsize):(tbatch*val_batchsize)+val_batchsize]\n            tlabel_batch = y_test[(tbatch*val_batchsize):(tbatch*val_batchsize)+val_batchsize]\n            test_loss,test_accur=sess.run(fetches=[loss,accur],feed_dict={img_placeholder:timage_batch.astype(np.float32),label_placeholder:tlabel_batch.astype(np.float32)})\n        print(\"::epoch number-> \",epoch,\" :::loss--> \",train_loss,\" ::testloss \",test_loss,\" ::trainaccuracy-> \",train_accur,\" ::test accuracy--> \",test_accur)\n    save_path = saver.save(sess,os.path.join(modelpath,\"model.ckpt\"))\n    print(\"model saved \", save_path)\n#         if epoch == 3:\n#             predcition1 = sess.run(fetches=[out_label], feed_dict={img_placeholder:np.reshape(X_test[0],(1,28,28,1)).astype(np.float32)})\n#             print(\"+----- \", predcition1)","465b81a5":"test = pd.read_csv(\"..\/input\/test.csv\")","938d3d26":"test","61f63622":"testdata = np.array(test)","3a970cc9":"testdata.shape","80438a8f":"os.listdir(modelpath)","f016c566":"# tf.reset_default_graph()\n# saver = tf.train.Saver()","a9add08b":"with tf.Session() as sess:\n    saver.restore(sess,os.path.join(modelpath,\"model.ckpt\"))\n    print(\"Model restored.\")\n    predcition1 = sess.run(fetches=[out_label], feed_dict={img_placeholder:np.reshape(testdata[8],(1,28,28,1)).astype(np.float32)})\n    print(\"+----- \", predcition1[0][0])\n    ","0e7ebdab":"sess = tf.Session()\nsaver.restore(sess,os.path.join(modelpath,\"model.ckpt\"))\nprint(\"Model restored.\")","e58c242e":"outputfile = pd.read_csv(\"..\/input\/sample_submission.csv\")","e7afa666":"outputfile[\"ImageId\"][0]","d7b0ab04":"for e,i in tqdm_notebook(enumerate(range(testdata.shape[0]))):\n    predcition = sess.run(fetches=[out_label], feed_dict={img_placeholder:np.reshape(testdata[i],(1,28,28,1)).astype(np.float32)})\n#     print(\"+----- \", predcition[0][0])\n    outputfile[\"ImageId\"][e] = e+1\n    outputfile[\"Label\"][e] = predcition[0][0]","b37fa11c":"# index = 100\ns= np.reshape(testdata[2],(28,28))\nplt.imshow(s)\n# print(y[index])","e6697ab1":"outputfile.to_csv(\"submission.csv\",index=False)","030c8619":"outputfile","efaa3688":"<a href=\".\/submission.csv\"> Download File <\/a>\n"}}