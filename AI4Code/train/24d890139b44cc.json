{"cell_type":{"9dbb2cd4":"code","aab8b061":"code","f28ba84d":"code","2220eb67":"code","ea599f48":"code","fdd42cd6":"code","a6aeac8d":"code","53755f99":"code","3e1d7e81":"code","f336ba8e":"code","0756a202":"code","f36639f9":"code","80efacde":"code","01597fe6":"code","bdabce35":"code","d93fab96":"code","91a781f0":"code","b82a6760":"code","772d596b":"code","ecc332b2":"code","7a33f9f0":"code","3ca4fa2f":"code","b8802944":"code","d779f869":"code","5289252f":"code","9c45af23":"code","32b25190":"code","99745296":"code","11206d28":"code","33f976e8":"code","8e0159a2":"code","03a7afa7":"code","e68e911d":"code","fb9f7c5b":"code","ea4518f2":"code","26261650":"code","622e3ed3":"code","bba1d1ec":"code","5eac7ec2":"code","a76033c2":"code","bb05cc51":"code","35201854":"code","b99e24cc":"code","0080402c":"code","789c18f2":"code","c8937c1a":"code","7c34b6b8":"code","2def860a":"code","038263d5":"code","4c506d84":"code","b4368e62":"code","945e6a63":"code","299f0637":"code","c2a44a92":"markdown","7d16ed41":"markdown","92bbb4e7":"markdown","71c19134":"markdown","df16157b":"markdown","64bdac5e":"markdown","3316a56d":"markdown","ca6ad505":"markdown","a45ac42b":"markdown","250ccb08":"markdown","4d7786fd":"markdown","6e3b26eb":"markdown","3aa225ca":"markdown","b522d0cb":"markdown","4af543cf":"markdown","c6afaef6":"markdown","feac2899":"markdown","f2ef3afa":"markdown","0f092204":"markdown","66b09a78":"markdown","93637cc9":"markdown","5948952a":"markdown","f254a435":"markdown","9d279b8f":"markdown","8a16d7b0":"markdown","c860704b":"markdown","39e23edd":"markdown","701de2de":"markdown","f8749633":"markdown"},"source":{"9dbb2cd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aab8b061":"#importing the holy trinity of data science packages\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Other visualization packages\nimport seaborn as sns\n\n#Importing NLP plugins\nfrom nltk.corpus import stopwords \nstop_words = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer \nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#Importing our Sklearn Plugins\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n#importing our models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n#Model Evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score","f28ba84d":"df = pd.read_csv(\"..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\")\ndf.shape","2220eb67":"df.head(3)","ea599f48":"#Checking our Data Types\ndf.info()","fdd42cd6":"#Check percentage of data missing for each feature\/column\ndf.isna().sum()\/len(df)","a6aeac8d":"#Checking for unique elements for each column\ndf.nunique()","53755f99":"# Always good practice to make a copy of your dataframe ever so often,\n# so you can roll back your mistakes much easier than rerunning your whole kernal again.\ndf_2 = df.copy()\ndf_2 = df_2.drop(labels = ['job_id','salary_range',\n                    'department','benefits',\n                    'company_profile'], axis = 1) #axis = 1 to refer droping columns","3e1d7e81":"df_2.tail(3)","f336ba8e":"df_2['employment_type'] = df_2['employment_type'].bfill(axis=0)\ndf_2['required_experience'] = df_2['required_experience'].bfill(axis = 0)\ndf_2['required_education'] = df_2['required_education'].bfill(axis = 0)\ndf_2['industry'] = df_2['industry'].bfill(axis = 0)\ndf_2['function'] = df_2['function'].bfill(axis = 0)","0756a202":"# Make Dataframe copy\ndf_3 = df_2.copy()\n\n# Keeping non NaN rows in my new dataframe\ndf_3 = df_3[df_3['description'].notna()]\n\n# Replacing NaNs with an empty string.\n#df_3 = df_3.replace(np.nan, '', regex = True)","f36639f9":"# For good measure let's drop any other Nans \ndf_3 = df_3.dropna(axis = 0, how = 'any')","80efacde":"print(f'We currenlty have {len(df_3)} rows. However, let\\'s drop duplicates and compare.')","01597fe6":"# drop duplicates\ndf_3 = df_3.drop_duplicates(keep = 'first')","bdabce35":"df_3.isna().sum()\/len(df)","d93fab96":"print(f'After dropping duplicates we have {len(df_3)} rows left. It seems there were 178 duplicate rows.')","91a781f0":"# Make copy\ndf_4 = df_3.copy()\n\n#concatenating our description and requirments columns\ndf_4['description'] = df_4['description'] + ' ' + df_4['requirements']\ndel df_4['requirements']","b82a6760":"#Clean DataFrame\ndf_clean = df_4.copy()\n\ndisplay(df_clean.head(7))\nprint(df_clean.shape)","772d596b":"#Ploting the Target variable\nplt.figure(figsize = (10,5))\nsns.countplot(x = df.fraudulent, data = df,palette=\"Set3\")\nplt.title('Fradulent (Target Variable) Count')\nplt.show()","ecc332b2":"#Stylistic Set\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize = (14,11))\n\n#fig 1\nplt.subplot(2,2,1)\nsns.countplot(y = df.employment_type, data = df,palette=\"Set3\", \n              order = df.employment_type.value_counts().index)\nplt.title(\"Employment Type Count\")\nplt.ylabel(\"\")\n\n#fig2\nplt.subplot(2,2,2)\n#matplotlib version\n#df.required_experience.value_counts().plot(kind='barh')\n#sns version\nsns.countplot(y = df.required_experience, data = df,palette=\"Set3\",\n             order = df.required_experience.value_counts().index)\nplt.title(\"Required Experience Count\")\nplt.ylabel(\"\")\n\n#fig 3\nplt.subplot(2,2,3)\nsns.countplot(y = df.required_education, data = df,palette=\"Set3\",\n             order = df.required_education.value_counts().index)\nplt.title(\"Required Education Count\")\nplt.ylabel(\"\")\n\nplt.tight_layout()\nplt.show()","7a33f9f0":"industry = df.industry.value_counts()[:10]\nfunction = df.function.value_counts()[:10]\n\nplt.figure(figsize = (12,12))\n\nplt.subplot(2,1,1)\nindustry.plot(kind = 'barh')\nplt.title('Top 10 Industries Represented in this Dataset.')\nplt.xlabel('Count')\n\nplt.subplot(2,1,2)\nfunction.plot(kind = 'barh')\nplt.title('Top 10 Business Functions Represented in this Dataset.')\nplt.xlabel('Count')\n\nplt.tight_layout()\nplt.show()","3ca4fa2f":"#Make Copy\ndf_5 = df_clean.copy()\n\n# One Hot Encoding using Pandas get dummies function\ncolumns_to_1_hot = ['employment_type','required_experience','required_education',\n                   'industry', 'function']\n\nfor column in columns_to_1_hot:\n    encoded = pd.get_dummies(df_5[column])\n    df_5 = pd.concat([df_5, encoded], axis = 1)\n","b8802944":"columns_to_1_hot += ['title', 'location']\n    \n#droping the original columns that we just one hot encoded from\ndf_5 = df_5.drop(columns_to_1_hot, axis = 1)","d779f869":"df_5.head()","5289252f":"def tokenizer(text):\n    \n    #All characters in this string will be converted to lowercase\n    text = text.lower()\n    \n    #Removing sentence punctuations\n    for punctuation_mark in string.punctuation:\n        text = text.replace(punctuation_mark,'')\n    \n    #Creating our list of tokens\n    list_of_tokens = text.split(' ')\n    #Creating our cleaned tokens list \n    cleaned_tokens = []\n    #Intatiating our Lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    \n    #Removing Stop Words in our list of tokens and any tokens that happens to be empty strings\n    for token in list_of_tokens:\n        if (not token in stop_words) and (token != ''):\n            #lemmatizing our token\n            token_lemmatized = lemmatizer.lemmatize(token)\n            #appending our finalized cleaned token\n            cleaned_tokens.append(token_lemmatized)\n    \n    return cleaned_tokens","9c45af23":"df_6 = df_5.copy()\n\n#Instatiating our tfidf vectorizer\ntfidf = TfidfVectorizer(tokenizer = tokenizer, min_df = 0.05, ngram_range=(1,3))\n#Fit_transform our description \ntfidf_features = tfidf.fit_transform(df_6['description']) #this will create a sparse matrix","32b25190":"#I want to append this sparse matrix to the original pandas Dataframe\ntfidf_vect_df = pd.DataFrame(tfidf_features.todense(), columns = tfidf.get_feature_names())\n\ndf_tfidf = pd.concat([df_6, tfidf_vect_df], axis = 1)\n\n#Minor Cleaning steps after appending our tfidf results to our Dataframe, we will need to drop the description column. \ndf_tfidf = df_tfidf.drop(['description'], axis = 1)\ndf_tfidf = df_tfidf.dropna()","99745296":"df_tfidf.head(3)","11206d28":"#Instatiating our CountVectorizer\ncount_vect = CountVectorizer(tokenizer = tokenizer, min_df = 0.05, ngram_range=(1,3))\n#Fit_transform our description \ncount_vect_features = count_vect.fit_transform(df_6['description']) #this will create a sparse matrix\n\ncount_vect_df = pd.DataFrame(count_vect_features.todense(), columns = count_vect.get_feature_names())\n\ndf_count_vect = pd.concat([df_6, count_vect_df], axis = 1)\ndf_count_vect = df_count_vect.drop(['description'], axis = 1)\ndf_count_vect = df_count_vect.dropna()","33f976e8":"df_count_vect.head(3)","8e0159a2":"target = df_tfidf.fraudulent\nfeatures = df_tfidf.drop(['fraudulent'], axis = 1)\n\n#Spliting our Data into train and holdout sets to test our models\nX_train, X_hold, y_train, y_hold = train_test_split(features, target, test_size = 0.1,\n                                                    stratify = target, random_state = 42)","03a7afa7":"#Intatiating our Logistic Regression Model\nlog_reg = LogisticRegression()\n#I want to optimze the C-Value and penalty\nc_values = [.00001, .0001, .001, .1, 1, 10, 100, 1000, 10000]\npenalty_options = ['l1','l2']\n\nparam_grid = dict(C = c_values, penalty = penalty_options)","e68e911d":"grid_tfidf = GridSearchCV(log_reg, param_grid= param_grid, cv = 10, scoring = 'roc_auc', n_jobs = -1)","fb9f7c5b":"grid_tfidf.fit(X_train, y_train)","ea4518f2":"print(grid_tfidf.best_score_)\nprint(grid_tfidf.best_params_)","26261650":"log_reg_tfidf_pred = grid_tfidf.predict(X_hold)\nprint(roc_auc_score(y_hold, log_reg_tfidf_pred))\nprint(classification_report(y_hold, log_reg_tfidf_pred))","622e3ed3":"target_2 = df_count_vect.fraudulent\nfeatures_2 = df_count_vect.drop(['fraudulent'], axis = 1)\n\n#Spliting our Data into train and holdout sets to test our models\nX_train_2, X_hold_2, y_train_2, y_hold_2 = train_test_split(features_2, target_2, test_size = 0.1,\n                                                    stratify = target_2, random_state = 42)\n\n#Intiatiating our previous logistic regression model, using the count vectorizer dataset\ngrid_count_vect = GridSearchCV(log_reg, param_grid= param_grid, cv = 10, scoring = 'roc_auc', n_jobs = -1)","bba1d1ec":"grid_count_vect.fit(X_train_2, y_train_2)\nprint(grid_count_vect.best_score_)\nprint(grid_count_vect.best_params_)","5eac7ec2":"log_reg_pred_2 = grid_count_vect.predict(X_hold_2)\nprint(roc_auc_score(y_hold_2, log_reg_pred_2))\nprint(classification_report(y_hold_2, log_reg_pred_2))","a76033c2":"# Model - KNearestNeighbors\nknn = KNeighborsClassifier()\n\n#The parameters we would like to optimize for\nk_range = list(np.arange(2,23,2))\nparam_grid_knn = dict(n_neighbors=k_range)\nprint(param_grid_knn)","bb05cc51":"#Intatiate our knn gridsearch\ngrid_knn = GridSearchCV(knn, param_grid_knn, cv=10, scoring='roc_auc',\n                        n_jobs = -1)\n\n#Fit our grid_knn\ngrid_knn.fit(X_train, y_train)\nprint(grid_knn.best_score_)\nprint(grid_knn.best_params_)","35201854":"#predicting on our holdout data\nknn_pred = grid_knn.predict(X_hold)\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, knn_pred))\nprint(classification_report(y_hold, knn_pred))","b99e24cc":"#Intatiating our SVM model\nsvc = SVC(kernel = 'linear', gamma = 'auto' )\n\n# I wont use a gridsearch because SVMs usually take a long looong time. I will just use a simple SVC\n# and see how it plays out\nsvc.fit(X_train, y_train)","0080402c":"#predicting our holdout data\nsvc_pred = svc.predict(X_hold)\n\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, svc_pred))\nprint(classification_report(y_hold, svc_pred))","789c18f2":"#Instatiating our random forest\n\nrf = RandomForestClassifier()\n\n#The parameters we want to tune with our random forest\nn_estimators_range = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n\nparam_grid_rf = dict(n_estimators=n_estimators_range)\n\ngrid_rf = GridSearchCV(rf, param_grid_rf, cv=10, scoring='roc_auc',\n                        n_jobs = -1)","c8937c1a":"grid_rf.fit(X_train, y_train)\nprint(grid_rf.best_score_)\nprint(grid_rf.best_params_)","7c34b6b8":"rf_pred = grid_rf.predict(X_hold)\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, rf_pred))\nprint(classification_report(y_hold, rf_pred))","2def860a":"#Instatiatie our MLPClassifier\nmlp = MLPClassifier(solver='lbfgs', \n                    activation = 'relu',\n                   hidden_layer_sizes = (100,50,30), \n                    max_iter = 1000)","038263d5":"mlp.fit(X_train, y_train)","4c506d84":"mlp_pred = mlp.predict(X_hold)\n\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, mlp_pred))\nprint(classification_report(y_hold, mlp_pred))","b4368e62":"#Instatiatie our MLPClassifier\nmlp = MLPClassifier(solver='adam', \n                    activation = 'relu',\n                   hidden_layer_sizes = (100,50,30), \n                    max_iter = 1000)","945e6a63":"mlp.fit(X_train, y_train)","299f0637":"mlp_pred = mlp.predict(X_hold)\n\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, mlp_pred))\nprint(classification_report(y_hold, mlp_pred))","c2a44a92":"We need to do some feature engineering. I would like to one hot encode my categorical data, as well as fit a TFIDF Vectorizer to my text data column. Might do a Count Vectorizer as well, and see if that changes anything to my model. In addition, I probably want to fit a PCA to reduce computational time. \n\n**Next Steps:**\n\n1. One Hot Encode Cateogrical Data\n2. Fit in a TFIDF Vectorizer\n3. Fit in a Count Vectorizer\n4. Determine if using a PCA would help. ","7d16ed41":"Interesting, using our holdout data our logistic regression with the tfidf data, had an AUC score of 0.58. Which is okay, that will be our baseline model. ","92bbb4e7":"# Part 3 - Feature Engineering & Modeling","71c19134":"# Model 4 - Support Vector Classification","df16157b":"Dissapointed.","64bdac5e":"It is clear that our data is highly imbalanced. This may cause some difficulties when modeling with highly imbalanced data. ","3316a56d":"# Model 2 - Logistic Regression w\/ Count Vectorizer","ca6ad505":"Very Dissapointed that the SVC didn't do better either. Very dissapointed.","a45ac42b":"# Part 2 - Exploratory Data Analysis","250ccb08":"# Model 1 - Logistic Regresion w\/ Tfidf","4d7786fd":"# Model 7 - Neural Nets - MLPClassifier w\/ solver = 'adam'","6e3b26eb":"Using the adam solver made our model perform even better! **an AUC score of 0.72! **","3aa225ca":"I'm a little bit dissapointed that the knn pediction on the holdout data was around the same too the original logistic regression.","b522d0cb":"Breath of fresh air! Finally a model that shows a **significant improvement from our baseline model.**","4af543cf":"## Count Vectorizer\nNow let's do a similar procedure with a Count Vectorizer, so we can compare the two vectorizers in performance later on.","c6afaef6":"The Count Vectorizer did not really improve from my previous model, it did worse by 3 percentage points. The AUC score on our holdout data was 0.55. Thus, I will stick using the tfidf data.","feac2899":"# Model 6 - Neural Nets - MLPClassifier w\/ solver = 'lbfgs' ","f2ef3afa":"# Model 3 - KNearestNeighbors","0f092204":"Welcome! In this notebook, I will be going through a complete data science workflow starting with data cleaning, EDA, and various model interations! Enjoy!\n\n**Model Results**\n\n|Model|AUC Score|\n|---|---|\n|Baseline - Logistic Regression using TFIDF data| 0.58|\n|Logistic Regression using Count Vectorizer Data| 0.55|\n|KNN| 0.58|\n|SVC| 0.53|\n|Random Forest| 0.52|\n|Neural Network - MLPClassifier w\/ 'lbfgs'| 0.69|\n|**Neural Network - MLPClassifer w\/ 'adam'**|**0.727**|","66b09a78":"Next step is to append the *description* column and *requirments* column together into one column. However, before I do this, **I want to avoid the NaN values in both of these columns.** In order to do so since there is a small number of missing rows in the description column, I will drop those rows first. From there, I will fill in all NaN values in the *requirments* column with \" \" aka. blank string. \n\nIn addition I will drop duplicated description columns as well, prior to the great concatenation. ","93637cc9":"Just from a quick glance my data, it seems that there is quite a few features that have a lot of missing rows. As such, **I will delete the various columns:**\n\n1. job_id because my DataFrame already has a built in index. \n2. salary_range because around 84% of the data is missing\n3. department because around 65% of the data is missing\n4. benefits because 40% of the data is missing\n5. company_profile because I want to combine the description + requirements columns to one features, in order to perform a tfidf vectorizer on it later on. \n\nThe rest of the columns will be filled out in a methedolical order. ","5948952a":"Filling missing values for **employment_type**, **required_experience**, **required_education**, **industry**, **function** using the pandas bfill function. I did this because these features had the fewest unique elements for a non-binary feature.\n\n> *Pandas bfill is a function that is used with the fillna function to back fill the values in a dataframe. Thus, if there is a NaN cell then bfill will replace that NaN value with the next row or column based on the axis equaling to 0 or 1.*","f254a435":"Great, we now have two different dataframes with two different vectorizers preprocessing our description data. I will hold out on the PCA to see if I need it. I will only do it if the modelimg takes too long. \n\n**I will conduct the following steps:**\n1. Logistic Regression w\/ Tfidf\n2. Logistic Regression w\/ Count Vectorizer\n3. I will evaluate both models and determine which is better, and for simplicity stake pick the superior vectorizer for the other models I would like to run.","9d279b8f":"# Part 1: Data Cleaning","8a16d7b0":"# Model 5 - Random Forest","c860704b":"## TfidfVectorizer\n\nI will need to run a tfidf vectorizer on our description data, and append the results to our DataFrame. ","39e23edd":"### Handling the description column \n\nFirst of all we need to clean up our text data a little bit. Now let us creat some helper funcitons.","701de2de":"## Data Dicitonary\nThere are 17880 rows with 18 features.\n\n|Column\/Feature|Discription|\n|---|---|\n|job_id|Unique Job ID|\n|title|The title of the job ad entry.|\n|location|Geographical location of the job ad.|\n|department|Corporate department (e.g. sales).|\n|salary_range|Indicative salary range (e.g. $50,000-$60,000)|\n|company_profile|A brief company description.|\n|description|The details description of the job ad.|\n|requirment|Enlisted requirements for the job opening.|\n|benefits|Enlisted offered benefits by the employer.|\n|telecommuting|True for telecommuting positions.|\n|has_company_logo|True if company logo is present.|\n|has_questions|True if screening questions are present.|\n|employment_type|Full-type, Part-time, Contract, etc.|\n|required_experience|Executive, Entry level, Intern, etc.|\n|required_education|Doctorate, Master\u2019s Degree, Bachelor, etc.|\n|industry|Automotive, IT, Health care, Real estate, etc.|\n|function|Consulting, Engineering, Research, Sales etc.|\n|fradulent|target - Classification attribute.|\n\n**Target Variable** = fradulent (1 or 0) with 1 being fradulent","f8749633":"### EDA Insights\n\n* Most job offers were Full-time, followed by Contract work.\n\n* Most jobs required an experience of mid-senior level, followed closely by Entry Level and Associate Level. Which is similar. \n\n* Most education experience required is a Bachelor\u2019s Degree, with very few requiring Master\u2019s Degree. Which signals that **work experience matters more** than education experience, and that the bachelor degree is a piece paper that proves you\u2019ve done something. \n\n* In this dataset, The top 3 Industries were all tech related.\n\n* The top 3 business functions were Information Technology, Sales, and Engineering.\n\n\n#### Future Plots\n1. A couple of word cloud images, people for some reason love world clouds.\n2. Plot of a map, showing the counts of jobs for each country. etc."}}