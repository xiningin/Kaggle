{"cell_type":{"c0290e48":"code","1d833681":"code","25906776":"code","619e0a84":"code","fb1d21db":"code","1668282b":"code","e921a154":"code","84f9fee7":"code","a53d8331":"code","5c78da5a":"code","d09c886f":"code","0167467e":"code","ee521151":"code","593ed8e8":"code","5901e436":"code","1ddacf8e":"code","f15a76d3":"code","960bedfe":"code","9df7b231":"code","31c6648a":"code","c8889d6b":"code","e0ca8899":"code","e909da62":"code","748e655c":"code","7f884b32":"code","69c5fa37":"code","dc03b9d1":"code","43a06663":"code","2c7a2cf8":"code","5c669085":"code","4c4c9f72":"code","067d1dfb":"code","1826328f":"code","7666988f":"code","cb7ea526":"code","bbba9f5e":"code","2a2e6f90":"markdown","bdc16863":"markdown","4cd9008e":"markdown","b8477413":"markdown","a93bc488":"markdown","2bcae949":"markdown","d7baaf10":"markdown","aadc5650":"markdown","278e7371":"markdown","9cacf234":"markdown","fb75ac8a":"markdown","1d77a492":"markdown","b256c7c7":"markdown","17d9a801":"markdown","1a186567":"markdown","11558e36":"markdown"},"source":{"c0290e48":"import numpy as np\nimport pandas as pd\n\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.metrics import accuracy_score","1d833681":"df = pd.read_csv('..\/input\/gmail-spam-detection-dataset\/spam1.csv', encoding = 'windows-1252')","25906776":"df.head()","619e0a84":"# Adding one more column with the name spam.\n# Here if a mail is spam it will print 1 else 0.\ndf['spam'] = df['type'].map({'spam': 1, 'ham': 0}).astype(int)","fb1d21db":"df.head()","1668282b":"df.shape","e921a154":"df['spam'].value_counts()","84f9fee7":"df.info()","a53d8331":"df.isnull().sum()","5c78da5a":"df['text'][1]","d09c886f":"def tokenizer(text):\n    return text.split()","0167467e":"df['text'] = df['text'].apply(tokenizer)","ee521151":"df['text'][1]","593ed8e8":"porter = SnowballStemmer(\"english\", ignore_stopwords=False)","5901e436":"def stem_it(text):\n    return [porter.stem(word) for word in text]","1ddacf8e":"df['text'] = df['text'].apply(stem_it)","f15a76d3":"df['text'][1]","960bedfe":"df['text'][153]","9df7b231":"lemmitizer = WordNetLemmatizer()","31c6648a":"def lemmit_it(text):\n    return [lemmitizer.lemmatize(word, pos = 'a') for word in text]","c8889d6b":"df['text'] = df['text'].apply(lemmit_it)","e0ca8899":"df['text'][153]","e909da62":"stop_words = stopwords.words('english')","748e655c":"def stop_it(text):\n    review = [word for word in text if not word in stop_words]\n    return review","7f884b32":"df['text'] = df['text'].apply(stop_it)","69c5fa37":"df.head()","dc03b9d1":"df['text'] = df['text'].apply(' '.join)","43a06663":"df.head()","2c7a2cf8":"tfidf = TfidfVectorizer()\ny = df.spam.values\nx = tfidf.fit_transform(df['text'])","5c669085":"x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 0, test_size = 0.2, shuffle = False)","4c4c9f72":"df.head()","067d1dfb":"lr = LogisticRegression()\nlr.fit(x_train, y_train)\ny_pred  = lr.predict(x_test)","1826328f":"acc_log = accuracy_score(y_pred, y_test)*100\nprint(\"Accuracy\", acc_log)","7666988f":"svc = LinearSVC(random_state=0)\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)","cb7ea526":"acc_svc = accuracy_score(y_pred, y_test)*100\nprint(\"Accuracy\", acc_svc)","bbba9f5e":"#input_mail = input(\"Enter the mail text: \")\ninput_mail = 'Your free ringtone is waiting to be collected. Simply text the password \\MIX\\\" to 85069 to verify. Get Usher and Britney. FML'\ninput_mail = [input_mail]\ntransformed_data = tfidf.transform(input_mail)\n\nprediction = svc.predict(transformed_data)\n\nif (prediction == 1):\n    print(\"\\nSpam mail\")\nelse:\n    print(\"\\nHam mail\")","2a2e6f90":"# Predictive Model\nSince, the accuracy of LinearSVC is slightly better than Logistic Regression, I will be using LinearSVC to make the predictive model.","bdc16863":"# Introduction\nThe major issues faced by all the email users are spam mails which contain unwanted information and data and some fake data to spoil the life of the people and also some mails which cause harmful effects. To reduce this risk and to save the people from this danger of spam mails, we are building this Gmail Spam Detection Model.","4cd9008e":"# Loading Dataset","b8477413":"# Tokenization \nTokenization stands for splitting up of data into tokens, that is comma seperated values.","a93bc488":"# Vectorization\nIt is the method to convert textual data into numeric format. Since computers are unable to understand textual data, hence we need to convert text into numerical format.\n\nI will be using TfidfVectorizer for the same, that is Term Frequency-Inverse Document Frequency.","2bcae949":"# Logistic Regression","d7baaf10":"# Importing Libraries","aadc5650":"# StopWord Remmoval\nIt is used to remove common words such as is, an, the, etc. The search engine is programmed to ignore such words.","278e7371":"# LinearSVC Accuracy","9cacf234":"# Lemmitization\nIt is the process of finding lemma of a word depending on their meaning. It aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as lemma. For example, converting is, am, was, are to the lemma word be. \n\nDifference between Stemming and Lemmitization is that stemming can often create non-existent words, whereas lemmas are actual words. ","fb75ac8a":"We can see that our dataset contains 5572 rows, in which 4825 are ham and 747 mails are spam mails.\nMoreover, our dataset does not contain any null or 0 values.","1d77a492":"# Aim\nAim of this project is the detection of spam email using the dataset and natural language processing. We will also be using machine learning algorithms such as logistic regression.","b256c7c7":"# Problem Statement\nThe objective of this project is to build a prediction model to predict whether a mail is spam or not.","17d9a801":"# Stemming\nStemming is the process of removing of suffix to convert the word into core values. For example, converting waits, waiting, waited to the core word wait.\n\nThere are different stemmers in the package such as snowball, porter, lancaster, etc. I will be using Snowball.","1a186567":"# Dataset\nthe dataset consists of two fields as follows - \n1. Type\n2. Text\n\nThe type field consists of whether the text of a given mail is a spam or a ham. Hence Type acts as an target variable while text acts as an input variable.","11558e36":"# Natural Language Processing \nNatural Language Processing (NLP) is the study of making computers understand how humans naturally speak, write and communicate.\n\nI will be using NLTK (Natural Language Toolkit) for doing natural language processing in English Language. The NLTK is a a collection of python libraries designed specially for identifying and tag parts of speech found in text of natural language like English."}}