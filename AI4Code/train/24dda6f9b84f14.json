{"cell_type":{"449884ae":"code","1656c1d9":"code","9df20629":"code","07afdb58":"code","0692a9e8":"code","ac23cca7":"code","96353d6d":"code","819ad03c":"code","5013c231":"code","482e009f":"code","1f0b70e4":"code","cef63794":"code","489e9425":"code","d737a318":"code","7941326a":"code","ee2ef7f0":"code","870e2321":"code","d6bab7fb":"code","90242af4":"code","c552de18":"markdown","c5ed5535":"markdown","5fe6bb60":"markdown","7a1c531e":"markdown","9f7c9344":"markdown","c9190f22":"markdown","a3389ba1":"markdown","e8a252ba":"markdown","e1b434ec":"markdown","c3eef329":"markdown","5380f89d":"markdown","c5aeb61f":"markdown","1ffa6596":"markdown","831e3e1a":"markdown","f3796273":"markdown","d68e4e5a":"markdown","8fd36b9f":"markdown","9ecc24e9":"markdown","f675d2e3":"markdown","0909ae7c":"markdown","4df8ef55":"markdown","ebd8e2be":"markdown","48f81a16":"markdown"},"source":{"449884ae":"import numpy as np\nimport pandas as pd\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\nimport gc\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPooling1D, Concatenate, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.initializers import Constant\nimport spacy\nimport pickle\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")","1656c1d9":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain.head()","9df20629":"train['keyword'].value_counts().sort_values(ascending=False)","07afdb58":"pd.crosstab(train['keyword'].isnull(), train['target'])","0692a9e8":"train['location'].value_counts().sort_values(ascending=False)[0:20]","ac23cca7":"train['location'].isnull().sum() \/ train.shape[0]","96353d6d":"pd.crosstab(train['location'].isnull(), train['target'])","819ad03c":"train['target'].value_counts()","5013c231":"def count_regex(pattern, tweet):\n    return len(re.findall(pattern, tweet))\n    \nfor df in [train, test]:\n    df['words_count'] = df['text'].apply(lambda x: count_regex(r'\\w+', x))\n    df['unique_words_count'] = df['text'].apply(lambda x: len(set(str(x).split())))\n    df['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    df['chars_count'] = df['text'].apply(lambda x: len(str(x)))\n    df['mentions_count'] = df['text'].apply(lambda x: count_regex(r'@\\w+', x))\n    df['hashtags_count'] = df['text'].apply(lambda x: count_regex(r'#\\w+', x))\n    df['capital_words_count'] = df['text'].apply(lambda x: count_regex(r'\\b[A-Z]{2,}\\b', x))\n    df['excl_quest_marks_count'] = df['text'].apply(lambda x: count_regex(r'!|\\?', x))\n    df['urls_count'] = df['text'].apply(lambda x: count_regex(r'http.?:\/\/[^\\s]+[\\s]?', x))","482e009f":"new_features = ['words_count', 'unique_words_count', 'mean_word_length', 'chars_count', 'mentions_count', \n                'hashtags_count', 'capital_words_count', 'excl_quest_marks_count', 'urls_count']\ndisaster_tweets_idx = train['target'] == 1\nfig, axes = plt.subplots(ncols=2, nrows=len(new_features), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(new_features):\n    sns.distplot(train.loc[~disaster_tweets_idx][feature], label='Not Disaster', ax=axes[i][0], color='green')\n    sns.distplot(train.loc[disaster_tweets_idx][feature], label='Disaster', ax=axes[i][0], color='red')\n\n    sns.distplot(train[feature], label='Train', ax=axes[i][1])\n    sns.distplot(test[feature], label='Test', ax=axes[i][1])\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=12)\n        axes[i][j].tick_params(axis='y', labelsize=12)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","1f0b70e4":"train_text = train['text']\ntest_text = test['text']\ntarget = train['target'].values\n\n# combine the text from the training dataset and the test dataset\ntext_list = pd.concat([train_text, test_text])\n\n# number of training samples\nnum_train_data = target.shape[0]","cef63794":"text_list.iloc[171]","489e9425":"text_list = text_list.apply(lambda x: re.sub('&amp;', ' and ', x))\ntext_list = text_list.apply(lambda x: re.sub('w\/', 'with', x))","d737a318":"# https:\/\/www.kaggle.com\/wowfattie\/3rd-place\nnlp = spacy.load('en_core_web_lg')\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\ndocs = nlp.pipe(text_list, n_threads = 2)\n\n# convert words to integers and save the results in word_sequences\nword_sequences = []\n\n# store the mapping in word_dict\nword_dict = {}\nlemma_dict = {}\n\n# store the frequence of each word\nword_freq = {}\n\nword_index = 1\nfor doc in docs:\n    word_seq = []\n    for word in doc:\n        try:\n            word_freq[word.text] += 1\n        except KeyError:\n            word_freq[word.text] = 1\n        if (word.text not in word_dict) and (word.pos_ is not 'PUNCT'):\n            word_dict[word.text] = word_index\n            word_index += 1\n            lemma_dict[word.text] = word.lemma_\n        # do not include punctuations in word_dict\n        # this essentially removes hashtags and mentions\n        if word.pos_ is not 'PUNCT':\n            word_seq.append(word_dict[word.text])\n    word_sequences.append(word_seq)\ndel docs\ngc.collect()\n\n# maximum number of words per tweet in the dataset\nmax_length = max([len(s) for s in word_sequences])\n\n# number of unique words\n# add 1 because 0 is reserved for padding\nvocab_size = len(word_dict) + 1\n\ntrain_word_sequences = word_sequences[:num_train_data]\ntest_word_sequences = word_sequences[num_train_data:]\n\n# add zeros at the end of each word sequence so that their lengths are fixed at max_len\ntrain_word_sequences = pad_sequences(train_word_sequences, maxlen=max_length, padding='post')\ntest_word_sequences = pad_sequences(test_word_sequences, maxlen=max_length, padding='post')","7941326a":"def load_embeddings(embeddings_index, word_dict, lemma_dict):\n    embed_size = 300\n    vocab_size = len(word_dict) + 1\n    embedding_matrix = np.zeros((vocab_size, embed_size), dtype=np.float32)\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n    for key in word_dict:\n        word = key\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.lower()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.upper()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.capitalize()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lemma_dict[key]\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        embedding_matrix[word_dict[key]] = unknown_vector                    \n    return embedding_matrix","ee2ef7f0":"def load_pickle_file(path):\n    with open(path, 'rb') as f:\n        file = pickle.load(f)\n    return file\n\n# the asterisk below allows the function to accept an arbitrary number of inputs\ndef get_coefs(word,*arr): \n    \"\"\" convert the embedding file into a Python dictionary \"\"\" \n    return word, np.asarray(arr, dtype='float32')\n                                                  \npath_glove = '\/kaggle\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'\npath_paragram = '\/kaggle\/input\/paragram-300-sl999\/paragram_300_sl999.txt'\n\nembeddings_index_glove = load_pickle_file(path_glove)\n# the asterisks below unpacks the list\nembeddings_index_paragram = dict(get_coefs(*o.split(\" \")) for o in open(path_paragram, encoding=\"utf8\", errors='ignore') if len(o) > 100)\n\nembedding_matrix_glove = load_embeddings(embeddings_index_glove, word_dict, lemma_dict)\nembedding_matrix_paragram = load_embeddings(embeddings_index_paragram, word_dict, lemma_dict)\n\n# stack the two pre-trained embedding matrices\nembedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_paragram), axis=1)","870e2321":"embedding_size = 600\nlearning_rate = 0.001\nbatch_size = 32\nnum_epoch = 5\n\ndef build_model(embedding_matrix, vocab_size, max_length, embedding_size=300):\n    model = Sequential([\n        Embedding(vocab_size, embedding_size, embeddings_initializer=Constant(embedding_matrix), \n                  input_length=max_length, trainable=False),\n        SpatialDropout1D(0.3),\n        Bidirectional(LSTM(128, return_sequences=True)),\n        Conv1D(64, kernel_size=2), \n        GlobalMaxPooling1D(),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","d6bab7fb":"reps = 5\npred_prob = np.zeros((len(test_word_sequences),), dtype=np.float32)\nfor r in range(reps):\n    model = build_model(embedding_matrix, vocab_size, max_length, embedding_size)\n    model.fit(train_word_sequences, target, batch_size=batch_size, epochs=num_epoch, verbose=2)\n    pred_prob += np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2) \/ reps)","90242af4":"submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission['target'] = pred_prob.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","c552de18":"Now, let's take a look at each variable more closely.","c5ed5535":"Now, build an index that maps the words to their vector representation. If the pre-trained model doesn't recognize a word, we will try to use a modified version of the word, e.g., lowercase, uppercase, stem, lemma and so on.","5fe6bb60":"## Text\nBelow is a modification of the code from \n- [Sentiment Analysis with Text Mining](https:\/\/towardsdatascience.com\/sentiment-analysis-with-text-mining-13dd2b33de27)\n- [NLP with Disaster Tweets - Read Before Start EDA](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-read-before-start-eda)","7a1c531e":"## Model Training","9f7c9344":"The missing percentage for class 0 is 33.6% and the missing percentage for class 1 is 32.9%. It doesn't seem like they are related.","c9190f22":"# Intoduction\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies). In this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified.","a3389ba1":"I looked it up online and found that it is a character reference for `&` in HTML. I will just replace it with the word `and`. I will also replace `w\/` with its long form `with`. We will take care of other data cleaning issues in the next section.","e8a252ba":"Let's look at the dataset. Each sample in the training set has the following information:\n\n- `id` - a unique identifier for each tweet\n- `text` - the text of the tweet\n- `location` - the location the tweet was sent from (may be blank)\n- `keyword` - a particular keyword from the tweet (may be blank)\n- `target` - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","e1b434ec":"## Target\nLet's look at the frequency of each class.","c3eef329":"# Exploratory Data Analysis","5380f89d":"# Model building\nIt seems like the keyword is already included in the tweet and I wasn't sure how to use the `location` variable, so I didn't use them in my model.","c5aeb61f":"- All the features have a similar distribution in the training set and the test set.\n- Some features seem to have different distributions for disaster and non-disaster tweets: `word_count`, `unique_words_count`, `mean_word_length`, `chars_count` and `excl_quest_marks_count`. I tried to include them in my model, but for some reason I got a lower score, so I decided to leave them out eventually.","1ffa6596":"## Location","831e3e1a":"Check whether the missingness is associated with the target variable.","f3796273":"## Keyword","d68e4e5a":"Make a submission.","8fd36b9f":"The missing percentage for class 0 and class 1 is 0.4% and 1% respectively. It doesn't seem like they are related.","9ecc24e9":"It's a fairly balanced dataset.","f675d2e3":"Check whether the missingness is associated with the target variable.","0909ae7c":"It seems like most reported locations are from the US.\n\nThe percentage of missing values is","4df8ef55":"## Embeddings\nWe will use two pre-trained word embeddings models. But first, we will have to tokenize our text. We will also remove the punctuations in the process.","ebd8e2be":"I saw quite a few tweets contain this word `&amp;`. For example,","48f81a16":"The model I use is similar to the one posted here https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/80568. I ran the model 5 times and then took the average."}}