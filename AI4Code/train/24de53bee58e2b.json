{"cell_type":{"3a99a048":"code","60953985":"code","5364d90b":"code","4fe3727d":"code","bd33a211":"code","72d8d411":"code","f5a6ee04":"code","d25ddc63":"code","f69cd589":"code","33cc9588":"code","080617ea":"code","34aee49c":"code","46b3c64c":"code","510cb2a4":"code","a63d20ef":"code","0abcb8d9":"code","ff339cc7":"code","5cff87b1":"code","9fd72ac1":"code","740f4a8c":"code","9d5d4650":"code","31c04c4b":"code","c2a8813d":"code","dd1380fa":"code","bbaadfff":"code","5b9cfa81":"code","1e8e6fc3":"markdown","12fb1559":"markdown","9665a236":"markdown","3b80cc76":"markdown"},"source":{"3a99a048":"import numpy as np \nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score, train_test_split, RepeatedKFold, StratifiedKFold\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n","60953985":"train_data = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\nvalid_data = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")","5364d90b":"train_data.drop('id', axis=1, inplace=True)\n#test_data.drop('id', axis=1, inplace=True)","4fe3727d":"X = train_data.drop('loss', axis=1)\ny = train_data['loss']","bd33a211":"ss = StandardScaler()\nX = ss.fit_transform(X)","72d8d411":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42)","f5a6ee04":"valid_ids = valid_data['id']\nvalid_data = valid_data.drop('id',axis=1)","d25ddc63":"valid_data = ss.fit_transform(valid_data)","f69cd589":"def objective(trial,data=X,target=y):\n    \n    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25,random_state=42)\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n             'loss_function':'RMSE',\n              'task_type':\"GPU\",\n              'eval_metric':'RMSE',\n              'leaf_estimation_method':'Newton',\n              'bootstrap_type': 'Bernoulli',\n              'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n              'subsample': trial.suggest_uniform('subsample',0,1),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n    model = CatBoostRegressor(**params)  \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n        \n    y_preds = model.predict(X_test)\n    loss = np.sqrt(mean_squared_error(y_test, y_preds))\n    \n    return loss","33cc9588":"OPTUNA_OPTIMIZATION = True\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","080617ea":"cat_params = study.best_trial.params\ncat_params['loss_function'] = 'RMSE'\ncat_params['eval_metric'] = 'RMSE'\ncat_params['bootstrap_type']= 'Bernoulli'\ncat_params['leaf_estimation_method'] = 'Newton'\ncat_params['random_state'] = 42\ncat_params['task_type']='GPU'","34aee49c":"kf = StratifiedKFold(n_splits = 10 , shuffle = True , random_state = 42)","46b3c64c":"test_preds=None\n\nfor fold, (tr_index , val_index) in enumerate(kf.split(X , y)):\n    \n    print(\"\u2059\" * 10)\n    print(f\"Fold {fold + 1}\")\n    \n    x_train,x_val = X[tr_index] , X[val_index]\n    y_train,y_val = y[tr_index] , y[val_index]\n        \n    eval_set = [(x_val, y_val)]\n    \n    model =CatBoostRegressor(**cat_params)\n    model.fit(x_train, y_train, eval_set = eval_set, verbose = False)\n    \n    train_preds = model.predict(x_train)    \n    val_preds = model.predict(x_val)\n    \n    print(np.sqrt(mean_squared_error(y_val, val_preds)))\n    \n    if test_preds is None:\n        test_preds = model.predict(valid_data)\n    else:\n        test_preds += model.predict(valid_data)","510cb2a4":"test_preds \/= 10","a63d20ef":"test_preds","0abcb8d9":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","ff339cc7":"submission['loss']=test_preds","5cff87b1":"submission.to_csv('catboost.csv',index=False)","9fd72ac1":"model.predict(valid_data)","740f4a8c":"def objective_xgb(trial,data=X,target=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(data, target, stratify=target, test_size=0.15)\n\n    param_grid = {\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, 100),\n        'eta': 0.01,\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 50),\n        'reg_lambda': trial.suggest_int('reg_lambda', 5, 100),\n        'min_child_weight': trial.suggest_int('min_child_weight', 5, 20),\n    }\n\n    reg = XGBRegressor(tree_method='gpu_hist', **param_grid)\n    # TODO: PRUNING\n    # pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'validation-error')\n    reg.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)], eval_metric='rmse',\n            verbose=False)\n\n#     return np.sqrt(-cross_val_score(reg, X_valid, y_valid, scoring='neg_mean_squared_error').mean())\n    return mean_squared_error(y_valid, reg.predict(X_valid), squared=False)","9d5d4650":"OPTUNA_OPTIMIZATION = True\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective_xgb, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","31c04c4b":"from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold","c2a8813d":"\nxgb_params = study.best_trial.params\nxgb_params['eta'] = 0.01\nxgb_params['tree_method'] = 'gpu_hist'","dd1380fa":"n_splits = 10\ntest_preds = None\nkf_rmse = []\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X, y)):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_metric='rmse', verbose=False)\n       \n    valid_pred = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_pred, squared=False)\n    print(f'Fold {fold+1}\/{n_splits} RMSE: {rmse:.4f}')\n    kf_rmse.append(rmse)\n    \n    if test_preds is None:\n        test_preds = model.predict(valid_data)\n    else:\n        test_preds += model.predict(valid_data)\n\ntest_preds \/= n_splits\nprint(f'Average KFold RMSE: {np.mean(np.array(kf_rmse)):.5f}')","bbaadfff":"sample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","5b9cfa81":"sample_submission['loss'] = test_preds\nsample_submission.to_csv('submission.csv', index=False)","1e8e6fc3":"## Training Models and doing Hyperparameter Tuning","12fb1559":"## Importing Libraries","9665a236":"# Boosting Algorithms for Tabular Regression\n\nUsing CatBoost and XGB for regression and tuning the models with Optuna for improved accuracy.","3b80cc76":"## Loading, Exploring and Preprocessing Data"}}