{"cell_type":{"547f5aec":"code","60e18c04":"code","e0f76bb7":"code","a63b15b6":"code","c8684c79":"code","02d6779d":"code","143de387":"code","6d8c77c2":"code","7819954f":"code","55b98488":"code","d1208bde":"code","bba073f3":"code","fe26a0f3":"code","fe8f6124":"code","9956cb1b":"code","eab9c3c1":"code","3f0a1658":"code","ab8d326e":"code","c5d4cb61":"code","b8e24dff":"code","7818cb0c":"code","af5e376c":"code","15a559f0":"code","01321463":"code","7d0a248b":"code","8d636f53":"code","28ef6bd2":"code","13c9811a":"code","b8c63b53":"code","a4ae4503":"code","18be9cd1":"code","66b100ec":"markdown","21df8eac":"markdown","65a0f316":"markdown","3dea65b6":"markdown","1369a670":"markdown","1ac419c6":"markdown","1e6b3aaf":"markdown","481ac062":"markdown","fb5af59f":"markdown","72169dea":"markdown","4b9ca97b":"markdown","c4189f58":"markdown","a5710c69":"markdown","c8e36a58":"markdown","fe02b674":"markdown","1bf9866e":"markdown","1e770877":"markdown","2c9bd39a":"markdown","dd52675a":"markdown","5562aeaf":"markdown","f4bf0a0f":"markdown","f3e70b78":"markdown","d22d14fb":"markdown","3dc92f55":"markdown","8446bab1":"markdown","2dfcde93":"markdown"},"source":{"547f5aec":"import numpy as np # linear algebra\nimport os\n\n# ensure consistency across runs\nfrom numpy.random import seed\nseed(1)\n\n# Imports to view data\nimport cv2\nfrom glob import glob","60e18c04":"train_data_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\"\nval_data_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/val\"\ntest_data_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/test\"\n\ntarget_size = (128,128)   \ntarget_dims = (128, 128, 3) # add channel for RGB\nn_batch_size = 16 ","e0f76bb7":"from keras.preprocessing.image import ImageDataGenerator","a63b15b6":"data_augmentor = ImageDataGenerator(samplewise_center=True, rescale=1.\/255, shear_range=0.2,zoom_range = 0.2,samplewise_std_normalization=True)\n\ntrain_generator = data_augmentor.flow_from_directory(train_data_dir,  target_size=target_size, batch_size= n_batch_size,class_mode='binary')\nval_generator = data_augmentor.flow_from_directory(val_data_dir, batch_size= n_batch_size,target_size=target_size, class_mode='binary')\ntest_generator = data_augmentor.flow_from_directory(test_data_dir, target_size=target_size,batch_size= 1,class_mode=None, shuffle=False)","c8684c79":"train_generator.class_indices","02d6779d":"data_augmentor = ImageDataGenerator(rescale=1.\/255, shear_range=0.2,zoom_range = 0.2,validation_split=0.2) \ntest_imgs = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = data_augmentor.flow_from_directory(train_data_dir, shuffle=True, target_size=target_size, subset='training',batch_size= n_batch_size,class_mode='binary')\nval_generator = data_augmentor.flow_from_directory(train_data_dir, target_size=target_size,subset='validation',class_mode='binary')\ntest_generator = test_imgs.flow_from_directory(test_data_dir, target_size=target_size,batch_size= 1,class_mode=None, shuffle=False)","143de387":"from matplotlib import pyplot as plt","6d8c77c2":"plt.figure(figsize=(40, 40))\nfor images, labels in train_generator:\n    for i in range(8):        \n        ax = plt.subplot(1, 10, i + 1)\n        plt.imshow(images[i])        \n        if int(labels[i]) == 1:\n            plt.title(\"PNEUMONIA\")\n        else:\n            plt.title(\"NORMAL\")                \n        plt.axis(\"off\")\n    break","7819954f":"from collections import Counter\n\ncounter = Counter(train_generator.classes)                          \ncounter","55b98488":"from collections import Counter\n\ncounter = Counter(train_generator.classes)                          \nmax_val = float(max(counter.values()))       \n\nclass_weights = {class_id : max_val\/num_images for class_id, num_images in counter.items()}                     \n\nclass_weights","d1208bde":"from tensorflow.keras import layers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential","bba073f3":"def initialize_model():    \n    model = Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=target_dims, padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(2, 2)))\n    model.add(Dropout(0.1))\n    \n    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(2, 2)))\n    model.add(Dropout(0.1))\n        \n    model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding='same'))\n    model.add(layers.MaxPool2D(pool_size=(3, 3)))\n    model.add(Dropout(0.1))\n    \n    model.add(layers.Flatten())    \n    model.add(layers.Dense(64, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(layers.Dense(1, activation='sigmoid'))\n\n    return model","fe26a0f3":"model = initialize_model()\nmodel.summary()","fe8f6124":"model.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=\"binary_accuracy\")","9956cb1b":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2","eab9c3c1":"callback = [EarlyStopping(patience=5, monitor='val_accuracy', restore_best_weights=True),\n            ReduceLROnPlateau(monitor = 'val_loss', patience = 2, factor=0.5, verbose=1),\n            ModelCheckpoint(\"xray_model_v2.h5\",save_best_only=True)]","3f0a1658":"history = model.fit(train_generator,\n                                      batch_size=n_batch_size,\n                                      epochs=20,\n                                      validation_data=val_generator,shuffle=True,\n                                      callbacks=callback) #,class_weight=class_weights)","ab8d326e":"scores = model.evaluate(val_generator)\nscores","c5d4cb61":"print(history.history.keys())","b8e24dff":"acc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']","7818cb0c":"EPOCHS = 20\n\nplt.figure(figsize=(8, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(EPOCHS), acc, label='Training Accuracy')\nplt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(EPOCHS), loss, label='Training Loss')\nplt.plot(range(EPOCHS), val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","af5e376c":"from sklearn.metrics import confusion_matrix ","15a559f0":"predictions = (model.predict(test_generator) > 0.5).astype(\"int32\")\ncm = confusion_matrix(y_true=test_generator.classes, y_pred=predictions) #np.argmax(predictions, axis=-1)) in case of Multiclass classification","01321463":"import itertools    \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","7d0a248b":"test_generator.class_indices","8d636f53":"cm_plot_labels = ['NORMAL','PNEUMONIA']\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","28ef6bd2":"tn, fp, fn, tp = cm.ravel()\n\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\n\nprint(\"Precision of the model is {:.2f}\".format(precision))\nprint(\"Recall of the model is {:.2f}\".format(recall))","13c9811a":"f1 = 2 * (precision*recall) \/ (precision + recall)\nprint(\"F1 score of the model is {:.2f}\".format(f1))","b8c63b53":"class_names = list(train_generator.class_indices.keys())\nclass_names","a4ae4503":"import random\n\nplt.figure(figsize=(40, 40))\n\nfor j in range(10):\n    ax = plt.subplot(1, 10, j + 1)   \n    \n    index = random.randint(1,len(test_generator))                \n    plt.imshow(test_generator[index][0])  \n    \n        \n    batch_prediction_main = model.predict(test_generator[index])\n    batch_prediction = (batch_prediction_main > 0.5).astype(\"int32\")   \n          \n    actual_class = class_names[test_generator.classes[index]]\n    predicted_class = class_names[batch_prediction[0][0]]\n    \n    plt.title(f\"Actual: {actual_class}\\n Predicted: {predicted_class}\")\n    plt.axis(\"off\")","18be9cd1":"model.save(\"ChestXrayClassificationv11.h5\")","66b100ec":"#### Resize, Normalize and Scale data \nKeras `ImageDataGenerator` class allows image rescaling, resizing options. here, I have applied image augmentation and also converted every image into binary (0 and 1). After initializing generators we can define the flow of our data. Resizing and scaling is applied on images to save time in training. Data augmenting helps to avoid overfitting. By augmenting data, we will be making slight variations to our data so that we have more data, without losing semantic meaning. The augmentation occurs in the parameters of the ImageDataGenerator method. For more details, check [this link](http:\/\/https:\/\/keras.io\/api\/preprocessing\/image\/).","21df8eac":"Following plot_confusion_matrix() is copied from scikit-learn. ","65a0f316":"Our x-ray classification model predicts the probability that each patient's x-ray belongs to one class 'NORMAL' or 'PNEUMONIA'. It is important to evaluate the performance of the classifications model in order to use these models in production for solving real world problems. Performance measures in machine learning classification models are used to assess how well machine learning classification algorithms perform in a given context. These performance metrics include accuracy, precision, recall and F1-score. Because it helps us understand the strengths and limitations of these models when making predictions in new situations, model performance is essential for machine learning. Let's explore how to generate these metrics.\n\n**Precision** is Correct positive predictions relative to total positive predictions. Precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions.\n       \n       Precision = (True positive) \/ (True Positive + False Positive)\n\n**Recall** is Correct positive predictions relative to total actual positives. It is the set of all positive predictions out of the total number of  positive instances. Recall helps us identify the mis-classified positive predictions.\n       \n       Recall = (True positive) \/ (True Positive + False Negative)","3dea65b6":"The confusion matrix shows the model's prediction in terms of:\n\n\nTN (True negative): This is the count of outcomes that were originally negative and were predicted negative.\n\nFP (False positive): This is the count of outcomes that were originally negative but were predicted positive. This error is also called a type 1 error","1369a670":"Our model has 98% recall and 92% of precision rate. Both recall and precision are important metrics, however there's a tradeoff between precision and recall. Depending on how the business requirements are defined, we may want o focus on optimizing one over the other. Using two values, we may not be able to determine if one algorithm is superior to another. For example, if one algorithm has higher precision but lower recall than other, we cannot tell which algorithm is better? On the other side, If we have a specific goal in your mind like 'Precision is the king. I don't care much about recall', then there's no problem. Higher precision is better. \n\n#### The F1-Score for imbalance Dataset\n\nIf we don't have a strong goal, and the dataset is imbalanced we need a combined metric. There comes the F1-score. The F1-score incorporates both precision and recall into a single metric. It is  used in the case where we have skewed classes i.e one type of class examples more than the other type class examples. In our case we have number of Pnuemonia xray images 2x more than normal xray images.\n\nFormula to compute F1 Score is: \n\n       F1 Score = 2 * (Precision * Recall) \/ (Precision + Recall)","1ac419c6":"We need to check class_indices for the labels so that we know in which order to pass them to our confusion matrix. This can be done by checking class_indices value of the test_generator","1e6b3aaf":"There aren't enough files in the validation folder. To generate enough validation samples, we can try following things:\n- Split training data into train and validation (80:20) sets. To perform this specify `validation_split` parameter in `ImageDataGenerator` function. Set directory path to training directory so that the Generator can take data from training directory.\n- Test generator Shuffle is set to `FALSE`. This is because, after predictions we want to plot our predictions to a Confusion Matrix and we want to be able to have one-one direct mapping of unshuffled samples.","481ac062":"Notice in following cell, I have commented `Class_Weight` parameter. When it was included, the model gave between 92-92% of accuracy. Removing it from training increased accuracy to 96%. The model penalizes mistakes in samples of class with its class_weight. If the class has higher class-weight it means the model wants you to put more emphasis on a class. How removing class_weight from training is ultimately taken care is discussed later in this notebook.","fb5af59f":"### 5. Train the model\nFitting the model to (i.e. using the .fit() method on) the training data is the process of finding coefficients for the equation specified via the algorithm being used.","72169dea":"### 1. Introduction\n\nOne of the main Machine Learning applications in healthcare is the identification and diagnosis of diseases. This kernal explains the complete pipeline from loading and preparing data to predicting results adjusting weights, applying regularizations and checking difference in model accuracy. The dataset consists pneumonia samples for two of four classes\/type of Pneumonia. Pneumonia is an infection that inflames patient's lungs' air sacs (alveoli). At the end of this notebook, our trained model will be able to predict if the patient's lungs are infected with pneumonia or not.\n\nLet's try to build the model that can predict pneumonia from given x-ray image.","4b9ca97b":"#### Check for Bias in data","c4189f58":"### 4. Regularization\n\nOverfitting makes the model relevant to its data set only, and irrelevant to any other data sets. In the context of machine learning, `regularization` is the process which regularizes or shrinks the coefficients towards zero. It discourages learning a more complex or flexible model, to prevent overfitting.\n\n- Add the `Earlystopping` parameter to stop training process when the model starts becoming stagnant, or even worse, when the model starts overfitting after 5 epochs and set the parameter <\/b>restore_best_weights to True so that the weights of best score on monitored metric - here val_accuracy (accuracy on test set) - are restored when training stops. This way the model has the best weights (i.e. low loss and high accuracy) possible on unseen data.\n\n- Add the `ReduceLROnPlateau` parameter to set on val_loss with a patience parameter set on 1 and a factor parameter set on 0.5 so that the learning rate is reduced by 2 whenever the val_loss parameter starts increasing\n\n- Add the `ModelCheckpoint` parameter to save the best weights of the model, so next time we want to use the model, we do not have to spend time training it. ","a5710c69":"### 3. Prepare Model","c8e36a58":"### Finally, Save the model to use it later for predictions!","fe02b674":"\n### 2. Load the data\nLet's prepare the data for which we'll be training the CNN. The Chest X-ray data is given into three saperate folders `train`, `val`, and `test`. Run following cell to set dataset path and other few variables which are used by `ImageDataGenerator` in next step. ","1bf9866e":"#### 8. Calculate Precision and Recall & F1-Score","1e770877":"### 9. Run inference on few test images","2c9bd39a":"- Class 0 'NORMAL' has 1073 samples\n- Class 1 'PNEUMONIA' has 3100 sample\n\nThe count shows major difference (imbalance) in our data. It can be taken care by adding `weight` values for each `class` while fitting the model. However, my observation was, when i added class_weight while model training, the model's accuracy was 93%. Section 8 in this kernel describes more about what measures are useful while working with imbalanced data.\n","dd52675a":"Check model score on unseen data (validation data)","5562aeaf":"Karas provides a rich pool of inbuilt metrices to evaluate performance. \n\nAdam has been most popular optimizer so far. Other than optimizers,other factor seem to have much more influence on the final model performance are adjusting the learning rate during training can be very effective. Also saving the weights for the lowest validation loss and loading the model with those weights to make predictions works very well. Keras provides two callbacks that help you achieve this which we will cover in cell [38].\n\nAn accuracy metric is used to measure the algorithm's performance in an interpretable way. For classification problems - binary_accuracy, categorical_accuracy or sparse_categorical_accuracy are used whereas for Regression problems - Mean Squared Error, Mean Absolute Error or and Mean Absolute Percentage Error are used. Keras also allow s to define our own custom metrics. For our dataset where we need to classify x-ray images into one of two categories, we will be using `binary_accuracy` as a metric.\n\nLoss value implies how poorly or well a model behaves after each iteration of optimization. We will use `binary_crossentropy` since it computes the cross-entropy loss between true labels and predicted labels for binary (0 or 1) classification.","f4bf0a0f":"##### Import `matplotlib` to plot training sample images","f3e70b78":"I hope you found this kernel useful. Please leave comments if you think I missed any important details or if you have any other questions or feedback about this topic.\nHappy Kaggling!!","d22d14fb":"### 7. Confusion matrix \nA confusion matrix describes the performance of the classification model. In other words, confusion matrix is a way to summarize classifier performance. ","3dc92f55":"Below is the basic CNN model that contains 3 convolutional layers, and 2 fully connected layers. Here, in the first layer, we put 32 nodes with mask size 3X3 and activation function as relu, which is applied per pixel and replaces all negative pixel values in the feature map by zero. and drop out of 20% which means every time 80% of nodes are selected randomly for the operation, it will increase randomness in the model which will reduce bias in output.\nSimilarly, the 2nd layer contains 64 nodes having activation function as relu and 3rd layer with 128 nodes and the 2 fully connected layers.\n\nThe last layer has only 1 unit. So the output (y_pred) will be a single floating point as the true (actual) label (y_true).\n\nFor the last layer, the activation function can be: None,sigmoid,softmax (In our case, we will setup last layer activation function as sigmoid since this is a binary classification problem)","8446bab1":" High F1 score is a sign that our model is performing well, even in situations where you might have imbalanced classes :)","2dfcde93":"### 6. Evaluate the model Performance \n\nPlot Accuracy and Loss. \nThe fit() returns a History object that can be used to visualize the training history. It contains a dictionary with loss and metric values at each epoch calculated both for training and validation datasets.\nFor example, lets extract the `validation accuracy` and `loss` metric and plot it."}}