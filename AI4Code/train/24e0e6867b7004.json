{"cell_type":{"095bdb0f":"code","03ee95fa":"code","623ae2fa":"code","d010c687":"code","d640900e":"code","36d992ba":"code","256cb434":"code","291db356":"code","b383ae30":"code","78b9711d":"code","305d40e3":"code","7ce9595e":"code","922e4b74":"code","7e460bf4":"code","6884c982":"code","3b67bcd6":"code","1676b292":"code","69132e62":"code","901f8a30":"code","26be826f":"code","ae7bfaa8":"markdown","ef0e1274":"markdown","24aac2dc":"markdown","2c60bfa1":"markdown","980291a1":"markdown","0811922a":"markdown"},"source":{"095bdb0f":"import os\nimport sys\nimport yaml","03ee95fa":"# !git clone https:\/\/github.com\/kumar-shubham-ml\/HandyRL.git\n!git clone https:\/\/github.com\/Dena\/HandyRL.git\n!cp -r HandyRL\/. .","623ae2fa":"%%writefile config.yaml\n\nenv_args:\n    env: 'HungryGeese'\n    source: 'handyrl.envs.kaggle.hungry_geese'\n\n\ntrain_args:\n    turn_based_training: False  # always False for Hungry Geese\n    observation: False\n    gamma: 0.8\n    forward_steps: 32\n    compress_steps: 4\n    entropy_regularization: 2.0e-3\n    entropy_regularization_decay: 0.3\n    update_episodes: 500\n    batch_size: 400  # GPU memory 10GB\n    minimum_episodes: 100\n    maximum_episodes: 5000  # RAM 64GB\n    eval_rate: 0.1\n    epochs: 120\n    num_batchers: 7\n    worker:\n        num_parallel: 6\n    lambda: 0.7\n    policy_target: 'TD'\n    value_target: 'TD'\n    seed: 0\n    restart_epoch: 0\n\n\nworker_args:\n    server_address: ''\n    num_parallel: 8","d010c687":"!ls","d640900e":"!pip3 install -r requirements.txt","36d992ba":"# !pip3 install -r handyrl\/envs\/kaggle\/requirements.txt","256cb434":"from handyrl.train import train_main \nfrom handyrl.train import train_server_main\nfrom handyrl.worker import worker_main\nfrom handyrl.evaluation import eval_main\nfrom handyrl.evaluation import eval_server_main\nfrom handyrl.evaluation import eval_client_main","291db356":"with open('config.yaml') as f:\n    args = yaml.safe_load(f)\nprint(args)","b383ae30":"train_main(args)","78b9711d":"!ls","305d40e3":"!ls models\/","7ce9595e":"! du -h","922e4b74":"!cp models\/latest.pth \/tmp\/\n!rm -r *\n!cp \/tmp\/latest.pth .\n!ls","7e460bf4":"import pickle\nimport bz2\nimport base64\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nmodel_path = 'latest.pth'\n\nweights = torch.load(model_path)\n\nPARAM = base64.b64encode(bz2.compress(pickle.dumps(weights)))\nstate_dict = pickle.loads(bz2.decompress(base64.b64decode(PARAM)))","6884c982":"type(state_dict)","3b67bcd6":"\n%%writefile submission.py\n\n# This is a lightweight ML agent trained by self-play.\n# After sharing this notebook,\n# we will add Hungry Geese environment in our HandyRL library.\n# https:\/\/github.com\/DeNA\/HandyRL\n# We hope you enjoy reinforcement learning!\n\n\nimport pickle\nimport bz2\nimport base64\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# Neural Network for Hungry Geese\n\nclass TorusConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.edge_size = (kernel_size[0] \/\/ 2, kernel_size[1] \/\/ 2)\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size=kernel_size)\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = torch.cat([x[:,:,:,-self.edge_size[1]:], x, x[:,:,:,:self.edge_size[1]]], dim=3)\n        h = torch.cat([h[:,:,-self.edge_size[0]:], h, h[:,:,:self.edge_size[0]]], dim=2)\n        h = self.conv(h)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\n\nclass GeeseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers, filters = 12, 32\n        self.conv0 = TorusConv2d(17, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([TorusConv2d(filters, filters, (3, 3), True) for _ in range(layers)])\n        self.head_p = nn.Linear(filters, 4, bias=False)\n        self.head_v = nn.Linear(filters * 2, 1, bias=False)\n\n    def forward(self, x):\n        h = F.relu_(self.conv0(x))\n        for block in self.blocks:\n            h = F.relu_(h + block(h))\n        h_head = (h * x[:,:1]).view(h.size(0), h.size(1), -1).sum(-1)\n        h_avg = h.view(h.size(0), h.size(1), -1).mean(-1)\n        p = self.head_p(h_head)\n        v = torch.tanh(self.head_v(torch.cat([h_head, h_avg], 1)))\n\n        return {'policy': p, 'value': v}\n\n\n# Input for Neural Network\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n\n    return b.reshape(-1, 7, 11)\n\n\n# Load PyTorch Model\n\nPARAM = b'XXXXXXXXXX'\n\nstate_dict = pickle.loads(bz2.decompress(base64.b64decode(PARAM)))\nmodel = GeeseNet()\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\n\n# Main Function of Agent\n\nobses = []\n\ndef agent(obs, _):\n    obses.append(obs)\n    x = make_input(obses)\n    with torch.no_grad():\n        xt = torch.from_numpy(x).unsqueeze(0)\n        o = model(xt)\n    p = o['policy'].squeeze(0).detach().numpy()\n\n    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    return actions[np.argmax(p)]","1676b292":"# Read in the submission file\nwith open('submission.py',) as file :\n    filedata = file.read()\n\n# Replace the target string\nfiledata = filedata.replace('XXXXXXXXXX', PARAM.decode(\"utf-8\") )\n\n# Write the file out again\nwith open('submission.py','w') as file:\n    file.write(filedata)","69132e62":"with open('submission.py',) as file :\n    filedata = file.read()","901f8a30":"# print(filedata)","26be826f":"from kaggle_environments import make\nenv = make(\"hungry_geese\", debug=True)\n\nenv.reset()\nenv.run(['submission.py', 'submission.py', 'submission.py', 'submission.py'])\nenv.render(mode=\"ipython\", width=800, height=700)","ae7bfaa8":"# Training","ef0e1274":"### This notebook is based on pytorch implementation of handyrl (https:\/\/github.com\/DeNA\/HandyRL) with minor changes\n\nThe inference part is taken from : https:\/\/www.kaggle.com\/yuricat\/smart-geese-trained-by-reinforcement-learning. As mentioned here https:\/\/www.kaggle.com\/c\/hungry-geese\/discussion\/218190, this model was trained for around 1,500 epochs with 800,000 games.","24aac2dc":"## Install Necessary Dependencies","2c60bfa1":"# Preparing Submission File","980291a1":"## Prepare Config","0811922a":"# Clone handyrl (keep your own fork for flexibility)"}}