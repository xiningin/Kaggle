{"cell_type":{"25a95b26":"code","3ec10550":"code","134c4d48":"code","4715ff16":"code","0c4b57ec":"code","9f932d4e":"code","215b63e8":"code","047f2cc3":"code","7f6b9b30":"code","5649f8c6":"code","c4d68c15":"code","550719ff":"code","87b96ad5":"code","723f420b":"code","e4f43289":"code","f5a6e083":"code","dc281902":"code","eb4d31a2":"code","293b3321":"code","770e7b9b":"code","c89a3ee5":"code","3af3649b":"code","ee793b1b":"code","2a645878":"code","7a87474b":"code","383dc0a1":"code","7c1e372a":"code","69b87adf":"code","98b3dbc8":"code","b7c098ce":"code","5d0daf99":"code","627064a3":"code","cb97dd56":"code","ae3c82f5":"code","4de5df9a":"code","9dcb28b1":"code","2d845178":"code","7c33e788":"code","bdc31f52":"code","88b677a7":"code","c49d2c25":"code","d3c40fb2":"code","3f4664b3":"code","2f19d33e":"code","aa98be5a":"code","0d148260":"code","c02fbd10":"code","fe4c6ea1":"code","22c52bc1":"code","c8455395":"code","5299de3f":"code","7afaae99":"code","dfc8f864":"code","65593172":"code","8a468b60":"code","8e392f45":"code","58afbcb7":"code","7b6358bd":"code","a21a46b3":"code","523a80b7":"code","b6ba6c5e":"markdown","d9a33998":"markdown","3403e8d4":"markdown","45d12d7e":"markdown","8a127413":"markdown","db60784e":"markdown","078d4904":"markdown","8761a2a2":"markdown","fbce28ef":"markdown","ed9fa59c":"markdown","f1f76cb5":"markdown","e9c300c2":"markdown","5a822680":"markdown","d578d04c":"markdown","bc96ab83":"markdown","d8ac314b":"markdown","6df61c9f":"markdown","e8d3ea0c":"markdown","ceb34e94":"markdown","26a89a94":"markdown","ed4f79ef":"markdown","9716d835":"markdown","a441438e":"markdown","98c25ca2":"markdown","cafe0e71":"markdown","6b49f133":"markdown","04db116e":"markdown","83587b93":"markdown","72b33c3a":"markdown","c97389a2":"markdown","61530344":"markdown","3a1fca94":"markdown","61bbe1ef":"markdown","820bfee0":"markdown","bb6966e3":"markdown","0ec7ae92":"markdown","fb05f3bc":"markdown","de4d095b":"markdown","c5f2f8c7":"markdown","6d848e03":"markdown","212aa213":"markdown","20132b78":"markdown","30cb02d3":"markdown","67c0417e":"markdown","78c15d1d":"markdown","3cb819ba":"markdown","5055f0b3":"markdown","c4c9044f":"markdown","dd761894":"markdown"},"source":{"25a95b26":"import io\nimport pickle\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,plot_confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, KFold\nfrom scipy.stats import sem\nfrom sklearn.metrics import precision_recall_curve,plot_precision_recall_curve,plot_roc_curve\nfrom sklearn.base import BaseEstimator, RegressorMixin, clone, is_regressor, is_classifier\nfrom sklearn.utils.validation import check_is_fitted, check_X_y, check_array\nfrom sklearn.exceptions import NotFittedError\n%matplotlib inline\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns',None)","3ec10550":"pd.read_parquet('..\/input\/multiparamss\/multisimmers.parquet').to_csv('TELECOME.csv')","134c4d48":"dataframe = pd.read_csv('.\/TELECOME.csv')","4715ff16":"dataframe.head()","0c4b57ec":"def analysis(read):\n    \n    if len(read) > 0:\n        print(\"PROCESS HAS BEEN STARTED\\n\")\n\n        print(\"DATA SHAPE\")\n        print(\"Observation:\", read.shape[0], \"Column:\", read.shape[1], \"\\n\")\n\n        print(\"EXPLORE MORE ABOUT THE DATA\")\n        if len(read.select_dtypes(\"object\").columns) > 0:\n            print(\"Object Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"object\").columns), \"\\n\", \n                  read.select_dtypes(\"object\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"integer\").columns) > 0:\n            print(\"Integer Variables:\", \"\\n\", \"VVariables:\", \n                  len(read.select_dtypes(\"integer\").columns), \"\\n\", \n                  read.select_dtypes(\"integer\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"float\").columns) > 0:\n            print(\"Float Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"float\").columns), \"\\n\", \n                  read.select_dtypes(\"float\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"bool\").columns) > 0:\n            print(\"Bool Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"bool\").columns), \"\\n\", \n                  read.select_dtypes(\"bool\").columns.tolist(), \"\\n\")\n\n        print(\"IS THERE ANY MISSING VALUE\")\n        print(\" \\n \", np.where(read.isnull().values.any() == False,\"No missing value!\", \"Data includes missing value!\"), \"\\n\")\n\n        buf = io.StringIO()\n        read.info(buf=buf)\n        check = True\n        check = buf.getvalue().split('\\n')[-2].split(\":\")[1].strip()\n        print(\"MEMORY \\n\", check)\n\n    else:\n        print(\"ERROR!\")\n\n    return read","9f932d4e":"dataframe.drop('Unnamed: 0',axis=1,inplace=True)\ndata = analysis(dataframe)\ndata.head()","215b63e8":"def data_cleaning(df):\n\n    print(\"*********{} *********\".format('Inspecting missing values'))\n    \n    data = df.isna().sum().reset_index().sort_values(by=0, ascending=False)\n    clean_data = data[data[0] != 0].shape[0]\n    columns = df.shape[1]\n    rows = df.shape[0]\n    data.columns = [\"name\", \"missing appearences\"]\n    data[\"%missing from total\"] = data[data[\"missing appearences\"]!=0][\"missing appearences\"]\/rows\n    mis_data = data[data[\"%missing from total\"] > 0.5].shape[0]\n    drop_data = np.array(data[data[\"%missing from total\"] > 0.5][\"name\"])\n    \n    print(\"{}\/{} total missing data in terms of column shape.\".format(clean_data, columns))\n    print(\"{}\/{} columns  will be dropped. name of the drop column is {}\".format(mis_data, columns,drop_data))\n    \n    return data, drop_data","047f2cc3":"missing_data, to_drop = data_cleaning(dataframe)","7f6b9b30":"missing_data","5649f8c6":"data.drop(to_drop,axis=1,inplace=True)","c4d68c15":"missing_data = missing_data[missing_data['missing appearences']!=0]\nmissing_data","550719ff":"less_missing_columns = ['is_featurephone','is_smartphone','is_dualsim','var_74','tenure',\n                       'device_os_name','dev_man','age','simcard_type']\n\nhigh_missing_columns = ['var_76','var_133']","87b96ad5":"data.dropna(subset=less_missing_columns,axis=0,inplace=True)","723f420b":"data[high_missing_columns].describe().transpose()","e4f43289":"data[high_missing_columns].mode()","f5a6e083":"data[high_missing_columns].median()","dc281902":"data.var_76.fillna(data.var_76.mean(),inplace=True)\ndata.var_133.fillna(data.var_133.mean(),inplace=True)","eb4d31a2":"data_cleaning(data)","293b3321":"# I would like to run this but it takes long \n#sns.pairplot(data,hue='target',corner=True)","770e7b9b":"# This also took lots of time to run unfortunaltely \n# plt.figure(figsize=(10,8),dpi=100)\n# sns.heatmap(data.corr(),cmap=\"viridis\",annot=True,linewidth=0.5)","c89a3ee5":"def network_corr(data,corr_):\n    \n    \n    corr_interval=[-0.8, 0.8]\n    corr = corr_\n    corr = pd.melt(corr.reset_index(), id_vars='index')\n    corr.columns = ['x', 'y', 'value']\n    high_corr = corr[((corr['value'] <= corr_interval[0]) | (corr['value'] >= corr_interval[1]))]\n    high_corr = high_corr[(high_corr['value'] != 1)].reset_index(drop=True)\n    sources = list(high_corr.x.unique())\n    targets = list(high_corr.y.unique())\n    plt.figure(figsize=(20, 15))\n    g = nx.from_pandas_edgelist(high_corr, source='x', target='y') \n    layout = nx.spring_layout(g, iterations=50, k=0.6, seed=123)\n    target_size = [g.degree(t) * 80 for t in targets]\n    nx.draw_networkx_nodes(g, \n                           layout, \n                           nodelist=targets, \n                           node_size=target_size)\n    # Draw every connection\n    nx.draw_networkx_nodes(g, layout, nodelist=sources, node_size=800, alpha=0.5)\n\n    nx.draw_networkx_edges(g, layout, width=1)\n\n    target_dict = dict(zip(targets, targets))\n    nx.draw_networkx_labels(g, layout, labels=target_dict)\n\n    plt.axis('off')\n    plt.title(\" Correlations Network\", fontsize=25)\n    plt.show()","3af3649b":"correlations = data.corr()","ee793b1b":"network_corr(data,correlations)","2a645878":"def hist(x,title):\n    plt.figure(figsize=(10,8))\n    ax = sns.distplot(x, kde=False,bins=30)\n    values = np.array([rec.get_height() for rec in ax.patches])\n    norm = plt.Normalize(values.min(), values.max())\n    colors = plt.cm.jet(norm(values))\n    for rec, col in zip(ax.patches,colors):\n        rec.set_color(col)\n    plt.title(title, size=20, color='black')","7a87474b":"age = data[data.age<75]","383dc0a1":"hist(age.age,'Age Distributions') ","7c1e372a":"hist(data.tenure,'Tenure Distributions') ","69b87adf":"plt.figure(figsize=(12,6))\nsns.boxplot(x='gndr',y='tenure',data=data,hue='device_os_name')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","98b3dbc8":"# Explore Age vs Target\ngrid = sns.FacetGrid(data, col='target')\ngrid = grid.map(sns.distplot, \"age\")","b7c098ce":"print(data.gndr.value_counts())","5d0daf99":"print(data.dev_man.value_counts())","627064a3":"print(data.device_os_name.value_counts())","cb97dd56":"print(data.simcard_type.value_counts())","ae3c82f5":"# Drop done\ndata.drop('dev_man',axis=1,inplace=True)","4de5df9a":"def get_encoder_inst(feature_col):\n  \n    assert isinstance(feature_col, pd.Series)\n    feature_vec = feature_col.sort_values().values.reshape(-1, 1)\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(feature_vec) \n  \n    filename = '.pickle'\n    pickle.dump(enc, open(filename, 'wb'))\n    return enc\n\ndef get_one_hot_enc(feature_col, enc,cols):\n  \n    assert isinstance(feature_col, pd.Series)\n    assert isinstance(enc, OneHotEncoder)\n    unseen_vec = feature_col.values.reshape(-1, 1)\n    encoded_vec = enc.transform(unseen_vec).toarray()\n    column_name = enc.get_feature_names([cols])\n    encoded_df = pd.DataFrame(encoded_vec, columns= column_name)\n    return encoded_df","9dcb28b1":"cat_list = ['device_os_name','gndr']\ncat_data = data[cat_list]\ndata.drop(cat_list,axis=1,inplace=True)","2d845178":"data_list = []\nfor cols in cat_data.columns:\n    encoder = get_encoder_inst(cat_data[cols])\n    one = get_one_hot_enc(cat_data[cols],encoder,cols)\n    data_list.append(one)\n    \nfinal_ohe = pd.concat(data_list,axis=1)\ndata.reset_index(drop=True, inplace=True)\nfinal_ohe.reset_index(drop=True, inplace=True)\nfor cols in final_ohe.columns:\n    final_ohe[cols] = final_ohe[cols].astype('int')","7c33e788":"final_ohe","bdc31f52":"final_data = pd.concat([data,final_ohe],axis=1)","88b677a7":"le = LabelEncoder()\nfinal_data.simcard_type = le.fit_transform(final_data.simcard_type)","c49d2c25":"final_data.head(2)","d3c40fb2":"def clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep].astype(np.float64)","3f4664b3":"final_data = clean_dataset(final_data)","2f19d33e":"columns = []\nfor col in final_data.columns:\n    temp = final_data[col].value_counts()\n    temp = pd.DataFrame(temp.values,columns=['Value'])\n    temp['index_'] = temp.index\n    for ind , values in zip(temp.index_,temp.Value):\n        if int(ind) == 0:\n            if values>20000:\n                columns.append(col)","aa98be5a":"len(columns)","0d148260":"final_data.drop('subscriber_id',axis=1,inplace=True)","c02fbd10":"sns.countplot(data=data,x='target')\nplt.xticks(rotation=90)","fe4c6ea1":"input_data = final_data.drop('target',axis=1)\noutput_data = final_data[['target']]","22c52bc1":"rus = RandomUnderSampler(random_state=0)","c8455395":"input_rus, output_rus = rus.fit_resample(input_data, output_data)","5299de3f":"sns.countplot(data=output_rus,x='target')\nplt.xticks(rotation=90)","7afaae99":"X_train, X_test, y_train, y_test = train_test_split(input_rus, output_rus, test_size=0.3, random_state=101)","dfc8f864":"scaler = MinMaxScaler()\nscaled_train = scaler.fit_transform(X_train)\nscaled_test = scaler.transform(X_test)","65593172":"class ZeroInflatedRegressor(BaseEstimator, RegressorMixin):\n\n    def __init__(self, classifier, regressor) -> None:\n        self.classifier = classifier\n        self.regressor = regressor\n\n    def fit(self, X, y, sample_weight=None):\n        \n        X, y = check_X_y(X, y)\n        self._check_n_features(X, reset=True)\n        if not is_classifier(self.classifier):\n            raise ValueError(\n                f\"`classifier` has to be a classifier. Received instance of {type(self.classifier)} instead.\")\n        if not is_regressor(self.regressor):\n            raise ValueError(f\"`regressor` has to be a regressor. Received instance of {type(self.regressor)} instead.\")\n\n        try:\n            check_is_fitted(self.classifier)\n            self.classifier_ = self.classifier\n        except NotFittedError:\n            self.classifier_ = clone(self.classifier)\n            self.classifier_.fit(X, y != 0, sample_weight=sample_weight)\n\n        non_zero_indices = np.where(self.classifier_.predict(X) == 1)[0]\n\n        if non_zero_indices.size > 0:\n            try:\n                check_is_fitted(self.regressor)\n                self.regressor_ = self.regressor\n            except NotFittedError:\n                self.regressor_ = clone(self.regressor)\n                self.regressor_.fit(\n                    X[non_zero_indices],\n                    y[non_zero_indices],\n                    sample_weight=sample_weight[non_zero_indices] if sample_weight is not None else None\n                )\n        else:\n            raise ValueError(\n                \"The predicted training labels are all zero, making the regressor obsolete. Change the classifier or use a plain regressor instead.\")\n\n        return self\n\n    def predict(self, X):\n\n        check_is_fitted(self)\n        X = check_array(X)\n        self._check_n_features(X, reset=False)\n\n        output = np.zeros(len(X))\n        non_zero_indices = np.where(self.classifier_.predict(X))[0]\n\n        if non_zero_indices.size > 0:\n            output[non_zero_indices] = self.regressor_.predict(X[non_zero_indices])\n\n        return output","8a468b60":"zir = ZeroInflatedRegressor(\n    classifier=SVC(),\n    regressor=LinearRegression()\n)\n\nzir.fit(scaled_train, y_train)","8e392f45":"predictions  = zir.predict(scaled_test)","58afbcb7":"trashold = .5\npreds =[]\nfor  val in predictions:\n    if val>trashold:\n        preds.append(1)\n    else:\n        preds.append(0)\n        ","7b6358bd":"accuracy_score(y_test,preds)","a21a46b3":"print(classification_report(y_test,preds))","523a80b7":"confusion_matrix(y_test,preds)","b6ba6c5e":"#### After analyzing our data ve decinde to drop column `var_75` due to the including to much missing value","d9a33998":"# Overall","3403e8d4":"# One-Hot Encoding","45d12d7e":"### We need to see distributions for this perpose lets write helper function","8a127413":"### Define RandomUnderSampling class","db60784e":"### I would prefer to define helper function for this aim. It could be safe way.","078d4904":"### Way to encode Categorical data ","8761a2a2":"### `So we have different version of data. We are going to convert it csv format`","fbce28ef":"## Unfortunately there is undersampling ","ed9fa59c":"### Our data almost ready but there is a little problem zero values have high frequency. It would be problem for train.\n\n> Let's define function for adjust mostly zero columns","f1f76cb5":"# \ud83c\udfa8VISUALIZATIONS\ud83d\uddbc\ufe0f","e9c300c2":"### First let's start with easy one to drop rows","5a822680":"\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/ichi.pro\/assets\/images\/max\/724\/0*C-cPP9D2MIyeexAT.gif\" alt=\"Heat beating\" style=\"height:700px;margin-top:3rem;\"> <\/div>","d578d04c":"# Result is not good enugh due to the several reasons. I would Tune the params but it will take lots of time\n\n\n\n### Thanks for reading. I hope you enjoyed\ud83d\ude0a\n> <img src=\"https:\/\/i.pinimg.com\/originals\/f8\/89\/1e\/f8891ef65e086abc67e5b448acb8bc12.gif\">","bc96ab83":"# Libraries\ud83d\udcda","d8ac314b":"##### Lets start dealing with missing values at the very begining","6df61c9f":"# So, we need to more information about data before we start to analyze. For this purposes lets define helper function  ","e8d3ea0c":"# EDA \u2714\ufe0f","ceb34e94":"#### What is zero inflated? \n#### Zero Inflate is we fit zeros seperately from numeric data. We use 2 different type of machine learning. One is classification other is regression\n#### In this case I will use simple ML algorithms due to the make shorter time to train and test.","26a89a94":"# We have columns object type  `gndr - dev_man - device_os_name - simcard_type`","ed4f79ef":"# UnderSampling","9716d835":"### As I mentioned  before our data includes zeros mostly. we cant fit it easly. So we should use zero inflated method. \n","a441438e":"### Now time to analyze 2 columns ","98c25ca2":"### There are 123 columns have mostly zero value. That is `zero inflated` case  ","cafe0e71":"### Lets create categoric data set","6b49f133":"#### Train Test Split \n\nour `train` set is 70% and `test` set 30% of total data","04db116e":"**we are done with helper function lets start to clean our data**\u2728","83587b93":"# Target \n","72b33c3a":"> OUR Data\ud83d\udcca Telecome data analysis-------------\n\nHeader | Definition\n---|---------\n`subscirber_id`| id of costumer\n`trf` | Label encoded colum of subscriber tariff\n`gndr` |Gender of subscriber \n`age` |age of subscriber\n`is_smartphone` | 0 or 1 \n`operation system` |Andriod, IOS, ect\n`sim_card_type` | technology type of simcard\n`divce` | Samsung,APPLE,Mi ect\n`target` | 0 - subscriber have more than one sim 1 - one sim\n\n>    Other columns are transformed different name","c97389a2":"## Data set too large and I decided to write function for showing correlation network\ud83d\udcd3 ","61530344":"##### `We encode our columns`","3a1fca94":"#  \ud83d\udd14Telecom Costumer Analysis\ud83d\udd14\n\n<div style=\"color:black;\n           display:fill;\n           border-radius:3px;\n           background-color:#b5e48c;\n           font-size:110%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\"> In this case, we are ging to analyze random telecom campany`s costumer and find which costumers are using more than one companies services. For this purpose `subscribe` columns encoded. There are lot of columns in our dataset so some operations are hard to run. So make a short run time I cut those steps and make It short and readable.\n<\/p>\n    \n <p style='padding:10px;\n           color:white;'>\n     In terms of model we will use double machine learning algorithm because our data include zeros\n    <\/p>","61bbe1ef":"# Lets fill null values with mean\u26fd","820bfee0":"# \u274c`Missing Values`\u274c","bb6966e3":"# Scale","0ec7ae92":"##  The mean is greater than the median, the distribution is positively skewed.","fb05f3bc":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/miro.medium.com\/max\/1838\/1*P93SeDGPGw0MhwvCcvVcXA.png\" alt=\"Heat beating\" style=\"height:300px;margin-top:3rem;\"> <\/div>","de4d095b":"#### Lets Retest Our data check if there are missing value or not","c5f2f8c7":"### In our missing value table there are 11 columns that include less missing value. Remind that `var_75` has already dropped. \n#### We are going to use two different approach\n\n> Drop null rows for missing value percentages is very low\n\n> Fillna for high missing","6d848e03":"# Read Data\ud83d\udcd6","212aa213":"##### `This is how our dataframe looks like`","20132b78":"### `So we have much information about our dataset. There are problems we have:`\n>`Missing values`\n\n>`Object type columns`","30cb02d3":"## Lets define helper function for One hot encoding","67c0417e":"### Let's go","78c15d1d":"# Model\ud83d\udd25","3cb819ba":"## Now we dropped column that has most missing value. We are going to clean other columns which has missing value too\u270d\ufe0f\n\nLets filter our `missing_data` table","5055f0b3":"# Label Encoding","c4c9044f":"## Lastly `subscriber_id` column is useless so we can drop it","dd761894":"`gender` columns has 3 different values so onehot encoding will be the best.\n\n`divice operation system` columns has 8 different values one hot encoding well be the best\n\n`sim card type` has only 2 different value so we can use label encoding\n\n`device man` has lots of different valeus so it is not necessery if we use one-hot encode there will be lots of zero value. Also , lable encoding will lead model to overfitting due to the vary of column values. So Approach is drop that column"}}