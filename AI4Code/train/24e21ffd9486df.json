{"cell_type":{"b87799d5":"code","28020eae":"code","fa802b3a":"code","2686242e":"code","4a8cdfea":"code","ef13be4d":"code","92f01c08":"code","93cc155d":"code","1a5a5e70":"code","8abf9cd1":"code","3e759b0e":"code","ee693ed4":"code","dd38ed7d":"code","88cdff8a":"code","974d15eb":"code","a862296d":"markdown","7674c5a6":"markdown","be9f3adb":"markdown","69bcde99":"markdown","8c8395d9":"markdown","48422333":"markdown","7d2db797":"markdown","681e3e5a":"markdown"},"source":{"b87799d5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.options.mode.chained_assignment = None  # default='warn'\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","28020eae":"pd.set_option('display.max_rows', None)\n\nX_full = pd.read_csv('\/kaggle\/input\/hourly-gbpusd-w-technical-indicators-20002019\/USDGBP_Hourly.csv', usecols=['date','close'])\nX = X_full[-62:]\n\n# remove the weekends\nX['date'] = pd.to_datetime(X['date'])\nX = X[X['date'].dt.weekday < 5]\nX.drop(axis=1, columns=['date'], inplace=True)\n\n# locate and drop duplicate values\nX = X[X.shift() != X]\nX.dropna(inplace=True)\n\n# reset the index\nX = X.reset_index(drop=True)","fa802b3a":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.xticks(rotation=70)\nsns.lineplot(data=X, dashes=False)","2686242e":"from statsmodels.tsa.stattools import adfuller\n\n# Use the dickey-fuller method to check stationality of the data\nprint('Dickey Fuller Test:')\ndftest = adfuller(X['close'], autolag='AIC')\n\n# Learn what these statistics mean\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n    \nprint(dfoutput)\n\n# For a time series to be stationary, its ADCF test should have:\n# a low p-value\n# the citical values at 1,5,10% should be as close as possible to the test statistic\n# Currently the time series is not stationary, max p-value=1.","4a8cdfea":"# Start by logging the data to try and make it more stationary\nX_log = np.log1p(X['close'])\n\n# take the rolling average and standard deviation over 12 hours (or 12 intervals)\nmovingAverage = X_log.rolling(window=12).mean()\nmovingSTD = X_log.rolling(window=12).std()\n\nplt.plot(X_log)\nplt.plot(movingAverage, color='red')\n# plt.xticks(rotation=70)\n\nX_log_minus_avg = X_log - movingAverage\n\n#Remove NAN values from rolling average starting after 12 intervals\nX_log_minus_avg.dropna(inplace=True)","ef13be4d":"from statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\n\ndef test_stationarity(timeseries):\n    \n    # Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    # Plot rolling statistics\n    orig = plt.plot(timeseries, color='blue', label='Original')\n    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')\n    std = plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    # Perform Dickey\u2013Fuller test:\n    print('Results of Dickey Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","92f01c08":"test_stationarity(X_log_minus_avg)","93cc155d":"# Take the exponential weighted mean of the data\nX_log_EWM = X_log.ewm(halflife=12, min_periods=0, adjust=True).mean()\nplt.plot(X_log)\nplt.plot(X_log_EWM, color='red')\nplt.xticks(rotation=70)","1a5a5e70":"X_log_minus_EWM = X_log - X_log_EWM\ntest_stationarity(X_log_minus_EWM)","8abf9cd1":"X_log_TS = X_log - X_log.shift()\nplt.plot(X_log_TS)","3e759b0e":"X_log_TS.dropna(inplace=True)\ntest_stationarity(X_log_TS)","ee693ed4":"decomposition = seasonal_decompose(X_log, period=24) \n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nfig, axis = plt.subplots(4,1, figsize=(8, 8))\n\nsns.lineplot(data=X_log, label='Original', ax=axis[0])\n\n# The actual trend of the data\nsns.lineplot(data=trend, label='Trend', ax=axis[1])\n\n# The seasonality \/ cyclic trend (maybe this is the algorithms in the background)\nsns.lineplot(data=seasonal, label='Seasonality', ax=axis[2])\n\n# The residual noise in the data\nsns.lineplot(data=residual, label='Residuals', ax=axis[3])","dd38ed7d":"#there can be cases where an observation only has trend & seasonality. In that case, there won't be \n#any residual component & that would be a null or NaN. so also remove those cases.\n\nX_log_residual = residual\nX_log_residual.dropna(inplace=True)\n\nX_log_seasonal = seasonal\nX_log_trend = trend\n\n# reset the index\nX_log_residual.reset_index(drop=True, inplace=True)\n\n# chck if the residual is stationary\ntest_stationarity(X_log_residual)","88cdff8a":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Use the stationary element of the trend\nlag_acf = acf(X_log_TS, nlags=24)\nlag_pacf = pacf(X_log_TS, nlags=24, method='ols')\n\n# From ACF(at y=0), get Q\n# From PACF(at y=0), get P\n# Find the first -ve value\n# Q = list(lag_acf).index([val for val in lag_acf if val < 0][0])\n# P = list(lag_pacf).index([val for val in lag_pacf if val < 0][0]) \n\nfig, axis = plt.subplots(1,2, figsize=(8,4))\n\n# ACF (autocorrellation graph)\nx = plot_acf(X_log_TS, ax=axis[0])\n\n# PACF (partial autocorrellation graph)\ny = plot_pacf(X, ax=axis[1])","974d15eb":"from statsmodels.tsa.arima.model import ARIMA\nfrom itertools import product\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Fit the stationary time-shifted log values\nmodel = ARIMA(X_log_TS, order=(8,2,12))\nresults_ARIMA = model.fit()\nplt.plot(X_log_TS[-100:])\nplt.plot(results_ARIMA.fittedvalues[-100:], color='red')\n\nRSS = sum((results_ARIMA.fittedvalues - X_log_TS)**2)\n\nprint(\"RSS: {}\".format(RSS))\n\nresults = results_ARIMA.forecast(steps=24)\n\nplt.plot(results)","a862296d":"# ACF & PACF","7674c5a6":"Next steps are to inverse the time differencing, inverse the exponential log, and check the forecast against actual USD:GBP pricing.\n\nThanks for checking out the notebook!","be9f3adb":"# Time Shift Transformation","69bcde99":"# Breaking down log transformation into seasonality","8c8395d9":"# Predicting USD:GBP FOREX price using ARIMA\n\nIn this notebook I'm exploring time series anlaysis, stationality of data and the auto regressive integrate moving average (ARIMA) prediction algorithm. \n\nAs you can see I made a prediction, but it looked like there wasn't really a statistically significant auto correllation or partial auto correlation to previous lags (hourly close prices) for USD:GBP.\n\nAlthough unsucessful, it made some sort of inaccurate prediction and I learnt a lot about time series forecasting, stationality, PACF, ACF and brushed up on some statistics. As a follow up to this I'd like to try a similar prediction using a LSTM RNN.","48422333":"# Log Transformation","7d2db797":"# Exponential Decay Transformation","681e3e5a":"There could be a partial auto correllation with the 8th lag, and auto correllation with the 12th \/ 2nd lag (although not statistically significant)."}}