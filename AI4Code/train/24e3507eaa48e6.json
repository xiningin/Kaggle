{"cell_type":{"6d37b253":"code","df559df3":"code","68d3b4ea":"code","81731147":"code","96929e8c":"code","84b6d830":"code","effe5b99":"code","e6158128":"code","9e5bba53":"code","b92193f9":"code","d8a05121":"code","f844636d":"code","bd0b8036":"code","3bcd54d8":"code","ebfde815":"code","d6f118a4":"code","bc26d467":"code","315034ff":"code","575072fa":"code","25e81e2b":"code","353f673c":"code","151f45ed":"code","cd1c8e2c":"code","96fb70a7":"code","143a6d32":"code","45acb29e":"code","602b4766":"code","32ae3f4b":"code","f6b7d32f":"markdown","2bbe03ce":"markdown","38c6fca1":"markdown","559d2deb":"markdown","2b20744e":"markdown","4c36adce":"markdown","249f6d3a":"markdown","e32d1d97":"markdown","7f22303e":"markdown","7d187b2f":"markdown","968e9b6b":"markdown","0f7e35e1":"markdown","db49c311":"markdown","a8b956a4":"markdown"},"source":{"6d37b253":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# machine learning\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df559df3":"df = pd.read_csv(\"..\/input\/used-car-dataset-ford-and-mercedes\/merc.csv\")","68d3b4ea":"df.isnull().sum()\ndf.info()","81731147":"sns.pairplot(df)","96929e8c":"fig, ax = plt.subplots(figsize = (10,5))\nsns.countplot(y = 'model', data = df, order = df['model'].value_counts().index)\nplt.ylabel('Car model')\nplt.title('Model distribution')","84b6d830":"ohe = pd.get_dummies(df)\nohe.head()","effe5b99":"X = ohe.drop(['price'], axis=1)\ny = ohe['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","e6158128":"pipe = Pipeline([('scaler', StandardScaler()), ('LinReg', LinearRegression())])","9e5bba53":"pipe.fit(X_train, y_train)","b92193f9":"pipe.score(X_train, y_train)","d8a05121":"y_pred = pipe.predict(X_test)","f844636d":"from sklearn.metrics import r2_score\nr2_lin = r2_score(y_test, y_pred)","bd0b8036":"from sklearn.metrics import mean_squared_error\n\ndef plot_learning_curves (model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n    train_errors, val_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_pred = model.predict(X_train[:m])\n        y_val_pred = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train_pred, y_train[:m]))\n        val_errors.append(mean_squared_error(y_val_pred, y_val))\n        plt.plot(np.sqrt(train_errors), 'r--', linewidth=2, label='train')\n        plt.plot(np.sqrt(val_errors), 'b--', linewidth=2, label='val')\n        plt.ylabel('RMSE')\n        plt.xlabel('Number of samples')\n\nlinReg = LinearRegression()\nplot_learning_curves(linReg, X[:200], y[:200])","3bcd54d8":"#from sklearn.preprocessing import PolynomialFeatures\n#poly_features = PolynomialFeatures(degree=2)\n#X_poly_train = poly_features.fit_transform(X_train)\n#X_poly_test = poly_features.fit_transform(X_test)\n\n#pipe_poly = Pipeline([('poly', poly_features), ('linreg', LinearRegression())])\n#pipe_poly.fit(X_poly_train, y_train)\n#y_pred_poly = pipe_poly.predict(X_poly_test)","ebfde815":"#pipe_poly.score(X_poly_train, y_train)","d6f118a4":"#r2_poly = pipe_poly.score(X_poly_test, y_pred_poly)","bc26d467":"#r2_score(y_test, y_pred_poly)","315034ff":"#from sklearn.model_selection import cross_val_score\n\n\n#print(cross_val_score(pipe_poly, X, y, cv=3))","575072fa":"from sklearn.linear_model import Ridge\n\npipe_ridge = Pipeline([('scaler', StandardScaler()), ('ridge', Ridge(alpha=100, solver='sag', max_iter=2000))])","25e81e2b":"pipe_ridge.fit(X_train, y_train)\npipe_ridge.score(X_train, y_train)","353f673c":"y_pred_ridge = pipe_ridge.predict(X_test)\nr2_ridge = r2_score(y_test, y_pred_ridge)","151f45ed":"from sklearn.model_selection import cross_val_score\nprint(cross_val_score(pipe_ridge, X, y, cv=3))","cd1c8e2c":"from sklearn.linear_model import Lasso\n\npipe_lasso = Pipeline([('scaler', StandardScaler()), ('ridge', Lasso(alpha=0.1, max_iter=100000, warm_start=True))])\npipe_lasso.fit(X_train, y_train)\npipe_lasso.score(X_train, y_train)","96fb70a7":"y_pred_lasso = pipe_lasso.predict(X_test)\nr2_lasso = r2_score(y_test, y_pred_lasso)","143a6d32":"from sklearn.linear_model import ElasticNet\n\npipe_en = Pipeline([('scaler', StandardScaler()), \n                    ('ridge', ElasticNet(alpha=0.01, l1_ratio=0.5))])\npipe_en.fit(X_train, y_train)\npipe_en.score(X_train, y_train)","45acb29e":"y_pred_en = pipe_en.predict(X_test)\nr2_en = r2_score(y_test, y_pred)","602b4766":"r2_scores = sorted([r2_lin, r2_ridge, r2_lasso, r2_en])\nnames = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']","32ae3f4b":"import plotly.express as px\n\ng = px.bar(x=names, y=r2_scores, log_y=True)\ng.show()","f6b7d32f":"**Let's plot learning curves (depending on the number of samples in the set). The RMSE should tend to converge closer to the max limit of samples.**","2bbe03ce":"# Exploratory Data Analysis (EDA)","38c6fca1":"**So, learning curves tell us that the RMSE stabilizes as long as the number of samples grow in volume.**","559d2deb":"**Let's check the price distribution among the models represented. Hypothesis: price is very correlated with the model.**","2b20744e":"**One-hot encoding of categorical features:**\n\n**We do this to avoid the misinterpretation of feature correlations by the ML algorithm.**","4c36adce":"**So, even though the R2-score among these four models vary insignificantly, the best model to predict Mercedes-car prices is ElasticNet.**","249f6d3a":"<img src =\"https:\/\/images.hgmsites.net\/hug\/mercedes-benz-historical-logos_100711609_h.jpg\" width=\"200\" height=\"200\">","e32d1d97":"**To sum up,**\n\nMercedes car price correlates with the features in the dataset. ElasticNet regression proves to perform best out of the ones presented in the kernel.","7f22303e":"**Apparently, a more complex model with polynomial features is too much for this model. Let's try Ridge regression.**","7d187b2f":"# Mercedes price prediction","968e9b6b":"**And finally, Lasso regression.**","0f7e35e1":"**Dataset used:** https:\/\/www.kaggle.com\/adityadesai13\/used-car-dataset-ford-and-mercedes\n\n**Goal:** predict the car price depending on the features of the car (e.g. mileage, engine type, transmission etc.) - both categorical and continuous.\n\n**ML task type:** regression.","db49c311":"**Let's try a polynomial regression (maybe there are some hidden non-linear correlation between the features).**\n\n**+ CrossValScore + feature importances**","a8b956a4":"*This part of code is commented cause I ran out of memory in the kernel :(*"}}