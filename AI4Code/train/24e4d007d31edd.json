{"cell_type":{"c6a13d70":"code","c23c79b0":"code","f081c650":"code","652fafae":"code","a9d2a694":"code","4913bf6c":"code","a447487c":"code","f8bfcdf2":"code","3598ca2b":"code","e0ce5660":"code","c7d9d308":"code","f9ad2d18":"code","e3653a11":"code","32763ed5":"code","0de9183d":"code","f8aa589b":"code","013f248c":"code","79a1e7e4":"code","03baa595":"code","9d7b6ce9":"code","3b855517":"code","36cf643e":"code","ef61ad36":"code","26d646b4":"code","6965fe19":"code","6bac915b":"code","3efc6151":"markdown","3ea1298d":"markdown","171d70b3":"markdown","e737e894":"markdown","c6a529af":"markdown","c4a505e7":"markdown","8e81739e":"markdown","86419043":"markdown","2e6702ab":"markdown"},"source":{"c6a13d70":"from fastai.core import *\nPath.read_csv = lambda o: pd.read_csv(o)\ninput_path = Path(\"\/kaggle\/input\/data-science-bowl-2019\")\npd.options.display.max_columns=200\npd.options.display.max_rows=200\ninput_path.ls()","c23c79b0":"train_with_features_part1 = pd.read_csv(\"..\/input\/dsbowl2019-feng-part1\/train_with_features_part1.csv\")","f081c650":"sample_subdf = (input_path\/'sample_submission.csv').read_csv()\n# specs_df = (input_path\/\"specs.csv\").read_csv()\n# train_labels_df = (input_path\/\"train_labels.csv\").read_csv()\n# train_df = (input_path\/\"train.csv\").read_csv()\ntest_df = (input_path\/\"test.csv\").read_csv()","652fafae":"sample_subdf.shape, test_df.shape, train_with_features_part1.shape","a9d2a694":"from fastai.tabular import *\nimport types\n\nstats = [\"median\",\"mean\",\"sum\",\"min\",\"max\"]\nUNIQUE_COL_VALS = pickle.load(open(\"..\/input\/dsbowl2019-feng-part1\/UNIQUE_COL_VALS.pkl\", \"rb\"))","4913bf6c":"for k in UNIQUE_COL_VALS.__dict__.keys():\n    print(k, len(UNIQUE_COL_VALS.__dict__[k]))","a447487c":"def array_output(f):\n    def inner(*args, **kwargs): return array(listify(f(*args, **kwargs))).flatten()\n    return inner\n\nfeature_funcs = []\n\n@array_output\ndef time_elapsed_since_hist_begin(df):\n    \"total time passed until assessment begin\"\n    return df['timestampElapsed'].max() - df['timestampElapsed'].min()\n\nfeature_funcs.append(time_elapsed_since_hist_begin)\n\n@array_output\ndef time_elapsed_since_each(df, types, dfcol):\n    \"time since last occurence of each types, if type not seen then time since history begin\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    last_elapsed = df['timestampElapsed'].max()\n    _d = dict(df.iloc[:-1].groupby(dfcol)['timestampElapsed'].max())\n    return [last_elapsed - _d[t] if t in _d else time_elapsed_since_hist_begin(df)[0] for t in types]\n\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(time_elapsed_since_each, types=\"event_codes\", dfcol=\"event_code\"))\n\n@array_output\ndef countfreqhist(df, types, dfcol, freq=False):\n    \"count or freq of types until assessment begin\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _d = dict(df[dfcol].value_counts(normalize=(True if freq else False)))\n    return [_d[t] if t in _d else 0 for t in types]\n\nfeature_funcs.append(partial(countfreqhist, types=\"media_types\", dfcol=\"type\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"media_types\", dfcol=\"type\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"titles\", dfcol=\"title\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"titles\", dfcol=\"title\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"event_ids\", dfcol=\"event_id\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"event_ids\", dfcol=\"event_id\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"worlds\", dfcol=\"world\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"worlds\", dfcol=\"world\", freq=True))\n\nfeature_funcs.append(partial(countfreqhist, types=\"event_codes\", dfcol=\"event_code\", freq=False))\nfeature_funcs.append(partial(countfreqhist, types=\"event_codes\", dfcol=\"event_code\", freq=True))\n\n@array_output\ndef overall_event_count_stats(df):\n    \"overall event count stats until assessment begin\"\n    return df['event_count'].agg(stats)\nfeature_funcs.append(overall_event_count_stats)\n\n@array_output\ndef event_count_stats_each(df, types, dfcol):\n    \"event count stats per media types until assessment begin, all zeros if media type missing for user\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _stats_df = df.groupby(dfcol)['event_count'].agg(stats)\n    _d = dict(zip(_stats_df.reset_index()[dfcol].values, _stats_df.values))\n    return [_d[t] if t in _d else np.zeros(len(stats)) for t in types]\nfeature_funcs.append(partial(event_count_stats_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(event_count_stats_each, types=\"event_codes\", dfcol=\"event_code\"))\n\n@array_output\ndef overall_session_game_time_stats(df):\n    \"overall session game time stats until assessment begin\"\n    return df['game_time'].agg(stats)\nfeature_funcs.append(overall_session_game_time_stats)\n\n@array_output\ndef session_game_time_stats_each(df, types, dfcol):\n    \"session game time stats per media types until assessment begin, all zeros if missing for user\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _stats_df = df.groupby(dfcol)['game_time'].agg(stats)\n    _d = dict(zip(_stats_df.reset_index()[dfcol].values, _stats_df.values))\n    return [_d[t] if t in _d else np.zeros(len(stats)) for t in types]\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"media_types\", dfcol=\"type\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"titles\", dfcol=\"title\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"event_ids\", dfcol=\"event_id\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"worlds\", dfcol=\"world\"))\nfeature_funcs.append(partial(session_game_time_stats_each, types=\"event_codes\", dfcol=\"event_code\"))\n\nlen(feature_funcs)","f8bfcdf2":"def get_sorted_user_df(df, ins_id):\n    \"extract sorted data for a given installation id and add datetime features\"\n    _df = df[df.installation_id == ins_id].sort_values(\"timestamp\").reset_index(drop=True)\n    add_datepart(_df, \"timestamp\", time=True)\n    return _df\n\ndef get_test_assessment_start_idxs(df): \n    return list(df.sort_values(\"timestamp\")\n                  .query(\"type == 'Assessment' & event_code == 2000\")\n                  .groupby(\"installation_id\").tail(1).index)\n\ndef get_test_feats_row(idx, i):\n    \"get all faeatures by an installation start idx\"\n    ins_id = test_df.loc[idx, \"installation_id\"]\n    _df = get_sorted_user_df(test_df, ins_id)\n    assessment_row = _df.iloc[-1]\n    row_feats = np.concatenate([f(_df) for f in feature_funcs])\n    feat_row = pd.Series(row_feats, index=[f\"static_feat{i}\"for i in range(len(row_feats))])\n    row = pd.concat([assessment_row, feat_row])\n    return row","3598ca2b":"# Feature Engineering part 1\nstart_idxs = get_test_assessment_start_idxs(test_df)\nres = parallel(get_test_feats_row, start_idxs)\ntest_with_features_df = pd.concat(res,1).T","e0ce5660":"test_with_features_part1 = test_with_features_df","c7d9d308":"# check to see train and test have same features\nnum_test_feats = [c for c in test_with_features_df.columns if c.startswith(\"static\")]\nnum_train_feats = [c for c in train_with_features_part1.columns if c.startswith(\"static\")]\nassert num_train_feats == num_test_feats","f9ad2d18":"from fastai.tabular import *","e3653a11":"train_with_features_part1.shape, test_with_features_part1.shape","32763ed5":"# create validation set - split by installation_id\nnp.random.seed(42)\nvalid_ids = (np.random.choice(train_with_features_part1.installation_id.unique(),\n                              int(len(train_with_features_part1)*0.05)))\nvalid_idx = (train_with_features_part1[train_with_features_part1.installation_id.isin(valid_ids)].index); valid_idx","0de9183d":"# get data\ncat_names = ['title','world']\ncont_names = [c for c in train_with_features_part1.columns if c.startswith(\"static_\")]\n\nprocs = [FillMissing, Categorify, Normalize]\ndata = TabularDataBunch.from_df(path=\".\", df=train_with_features_part1, dep_var=\"accuracy\", \n                                valid_idx=valid_idx, procs=procs, cat_names=cat_names, cont_names=cont_names)\n\ndata.add_test(TabularList.from_df(test_with_features_part1, cat_names=cat_names, cont_names=cont_names));","f8aa589b":"# fit\nlearner = tabular_learner(data, [256,256], y_range=(0.,1.), ps=0.6)\nlearner.fit_one_cycle(10, 3e-3)","013f248c":"from sklearn.metrics import cohen_kappa_score","79a1e7e4":"coefs=array([0.25,0.50,0.75])\ndef soft2hard(o):\n    if o < coefs[0]: return 0\n    elif o < coefs[1]: return 1\n    elif o < coefs[2]: return 2\n    else: return 3","03baa595":"# get valid preds\npreds, targs = learner.get_preds()","9d7b6ce9":"# get accuracy_group for preds and targs\n_preds = array([soft2hard(o.item()) for o in preds])\n_targs = array(train_with_features_part1.iloc[valid_idx]['accuracy_group'].values)","3b855517":"# see validation score\ncohen_kappa_score(_targs, _preds, weights=\"quadratic\")","36cf643e":"# get test preds\npreds,targs=learner.get_preds(DatasetType.Test)\n_preds = array([soft2hard(o.item()) for o in preds])","ef61ad36":"Counter(_preds)","26d646b4":"# get installation ids for test set\ntest_ids = test_with_features_part1['installation_id'].values; len(test_ids)","6965fe19":"# generate installation_id : pred dict\ntest_preds_dict = dict(zip(test_ids, _preds)); len(test_preds_dict)","6bac915b":"# create submission\nsample_subdf['accuracy_group'] = sample_subdf.installation_id.map(test_preds_dict)\nsample_subdf['accuracy_group'] = sample_subdf['accuracy_group'].fillna(3)\nsample_subdf.to_csv(\"submission.csv\", index=False)","3efc6151":"### Read Data","3ea1298d":"### Features (part1)\n\nBasically here we redefine the feature generation code for test.","171d70b3":"### Imports","e737e894":"### Test Feature Engineering\n\nTest set in LB and Private LB is different than what is publicly shared. So feature engineering and inference for test set should be done online.","c6a529af":"### TabularLearner Model\n\nHere we use a single validation but in later stages once we finalize features we should use cross-validation. We don't over optimize the model or do any hyperparameter search since the whole purpose is to get a baseline and build on top of it in upcoming parts.","c4a505e7":"### Submit","8e81739e":"### end","86419043":"### TabularLearner with part 1\n\nIn this notebook data generated from this [kernel](https:\/\/www.kaggle.com\/keremt\/fastai-feature-engineering-part1-6160-feats\/) is used during modeling. Data can also be found as a Kaggle [dataset](https:\/\/www.kaggle.com\/keremt\/dsbowl2019-feng-part1). This notebook is part 1 of series of notebooks that will model data from corresponding feature engineering kernels as we keep adding hopefully some creative features :)\n\n**Important Note:** Feature generation for test data will happen online since private test set is not publicly available for precomputation!\n\nThis notebook will give a LB around: 0.506 (score can vary but it's solely for baseline purposes)\n\n**To see how features are generated in more detail:** [Feature Engineering Part 1 Notebook](https:\/\/www.kaggle.com\/keremt\/fastai-feature-engineering-part1-6160-feats\/)","2e6702ab":"### Check Validation Score\n\nAgain, we don't search for optimal coefficients since main purpose is to create a baseline."}}