{"cell_type":{"7728ed83":"code","20bd83af":"code","eb4618b2":"code","a95bd812":"code","d1bca91b":"code","ae88de94":"code","87e5290b":"code","0cd74f28":"code","105d3639":"code","4d78d541":"code","eb407d61":"code","88d1e505":"code","deea470d":"code","54c80cef":"code","367305f4":"code","02cf954a":"code","3dd6d41e":"code","cb2b030e":"code","e6ef4b03":"code","eadd5bc3":"code","6b752494":"code","02fac828":"code","6d7d0813":"code","f901d7f7":"code","ce88c139":"code","ccf8d300":"code","bd0f22c0":"code","66e0752b":"code","6bd864a3":"code","0a04c3ae":"markdown","197dd1de":"markdown","8519b249":"markdown","d854fada":"markdown","a7d60c51":"markdown","4613e883":"markdown","fc41e975":"markdown","01c9801c":"markdown","df6dc479":"markdown","1156940a":"markdown"},"source":{"7728ed83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","20bd83af":"# importing libraries\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, GRU, Bidirectional\nfrom keras.optimizers import SGD\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","eb4618b2":"# function which plots ibm stock prices: real and predicted both\n\ndef plot_predictions(test, predicted):\n    plt.plot(test, color=\"red\", label=\"real IBM stock price\")\n    plt.plot(predicted, color=\"blue\", label=\"predicted stock price\")\n    plt.title(\"IBM stock price prediction\")\n    plt.xlabel(\"time\")\n    plt.ylabel(\"IBM stock price\")\n    plt.legend()\n    plt.show()","a95bd812":"# function which calculates root mean squared error\n\ndef return_rmse(test, predicted):\n    rmse = math.sqrt(mean_squared_error(test, predicted))\n    print(\"the root mean squared error is : {}.\".format(rmse))","d1bca91b":"data = pd.read_csv(\"..\/input\/IBM_2006-01-01_to_2018-01-01.csv\", index_col='Date', parse_dates=[\"Date\"])\n\ndata.shape","ae88de94":"data.head(5)","87e5290b":"train = data[:'2016'].iloc[:,1:2].values\ntest = data['2017':].iloc[:,1:2].values","0cd74f28":"# visualization of \"High\" attribute of the dataset\n\ndata[\"High\"][:'2016'].plot(figsize=(16,4), legend=True)\ndata[\"High\"][\"2017\":].plot(figsize=(16,4), legend=True)\nplt.legend([\"Training set (before 2017)\", \"Test set (from 2017)\"])\nplt.title(\"IBM stock prices\")\nplt.show()","105d3639":"# scaling the training set\n\nsc = MinMaxScaler(feature_range=(0,1))\ntrain_scaled = sc.fit_transform(train)","4d78d541":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements\n\nx_train = []\ny_train = []\n\nfor i in range(60,2769):\n    x_train.append(train_scaled[i-60:i, 0])\n    y_train.append(train_scaled[i,0])\n\nx_train, y_train = np.array(x_train), np.array(y_train)","eb407d61":"x_train[0]","88d1e505":"y_train[0]","deea470d":"len(x_train)","54c80cef":"len(y_train)","367305f4":"x_train.shape","02cf954a":"y_train.shape","3dd6d41e":"# reshaping x_train for efficient modelling\n\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))","cb2b030e":"x_train.shape","e6ef4b03":"# LSTM architecture\n\nregressor = Sequential()\n\n# add first layer with dropout\n\nregressor.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n\n# add second layer\n\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n\n# add third layer\n\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n\n# add fourth layer\n\nregressor.add(LSTM(units=50))\nregressor.add(Dropout(0.2))\n\n# the output layer\n\nregressor.add(Dense(units=1))","eadd5bc3":"# compiling the LSTM RNN network\n\nregressor.compile(optimizer='rmsprop', loss='mean_squared_error')\n\n# fit to the training set\n\nregressor.fit(x_train, y_train, epochs=5, batch_size=32)","6b752494":"# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole 'High' attribute data for processing\n\ndataset_total = pd.concat((data['High'][:'2016'], data['High']['2017':]), axis=0)\nprint(dataset_total.shape)\n\ninputs = dataset_total[len(dataset_total)-len(test)-60 : ].values\nprint(inputs.shape)\ninputs = inputs.reshape(-1,1)\nprint(inputs.shape)\ninputs = sc.transform(inputs)\nprint(inputs.shape)","02fac828":"# preparing x_test\n\nx_test = []\nfor i in range(60,311):\n    x_test.append(inputs[i-60:i, 0])\n    \nx_test = np.array(x_test)\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))","6d7d0813":"# predicting the stock prices for test set\n\npredicted = regressor.predict(x_test)\npredicted = sc.inverse_transform(predicted)","f901d7f7":"# visualizing the results: predicted vs test\n\nplot_predictions(test, predicted)","ce88c139":"# evaluating the model\n\nreturn_rmse(test, predicted)","ccf8d300":"# The GRU architecture\nregressorGRU = Sequential()\n# First GRU layer with Dropout regularisation\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(x_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Second GRU layer\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(x_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Third GRU layer\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(x_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# Fourth GRU layer\nregressorGRU.add(GRU(units=50, activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n# The output layer\nregressorGRU.add(Dense(units=1))","bd0f22c0":"# compiling the model\n\nregressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False), loss='mean_squared_error')\n\n# fitting the model\n\nregressorGRU.fit(x_train, y_train, epochs=5, batch_size=150)","66e0752b":"# predicting the stock prices for test set and visualization\n\npredicted_with_gru = regressorGRU.predict(x_test)\npredicted_with_gru = sc.inverse_transform(predicted_with_gru)\n\nplot_predictions(test, predicted_with_gru)","6bd864a3":"# evaluating the model performance\n\nreturn_rmse(test, predicted_with_gru)","0a04c3ae":"Vanishing Gradient problem:\n\nVanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.","197dd1de":"I will use LSTMs for predicting the price of stocks of IBM for the year 2017","8519b249":"Gated Recurrent Units\n\nIn simple words, the GRU unit does not have to use a memory unit to control the flow of information like the LSTM unit. It can directly makes use of the all hidden states without any control. GRUs have fewer parameters and thus may train a bit faster or need less data to generalize. But, with large data, the LSTMs with higher expressiveness may lead to better results.\n\nThey are almost similar to LSTMs except that they have two gates: reset gate and update gate. Reset gate determines how to combine new input to previous memory and update gate determines how much of the previous state to keep. Update gate in GRU is what input gate and forget gate were in LSTM. We don't have the second non linearity in GRU before calculating the output, .neither they have the output gate.","d854fada":"Components of LSTMs:\n\nForget Gate \u201cf\u201d ( a neural network with sigmoid)\n\nCandidate layer \u201cC\"(a NN with Tanh)\n\nInput Gate \u201cI\u201d ( a NN with sigmoid )\n\nOutput Gate \u201cO\u201d( a NN with sigmoid)\n\nHidden state \u201cH\u201d ( a vector )\n\nMemory state \u201cC\u201d ( a vector)\n\nInputs to the LSTM cell at any step are Xt (current input) , Ht-1 (previous hidden state ) and Ct-1 (previous memory state).\n\nOutputs from the LSTM cell are Ht (current hidden state ) and Ct (current memory state)","a7d60c51":"In a recurrent neural network we store the output activations from one or more of the layers of the network. Often these are hidden later activations. Then, the next time we feed an input example to the network, we include the previously-stored outputs as additional inputs. You can think of the additional inputs as being concatenated to the end of the \u201cnormal\u201d inputs to the previous layer. For example, if a hidden layer had 10 regular input nodes and 128 hidden nodes in the layer, then it would actually have 138 total inputs (assuming you are feeding the layer\u2019s outputs into itself \u00e0 la Elman) rather than into another layer). Of course, the very first time you try to compute the output of the network you\u2019ll need to fill in those extra 128 inputs with 0s or something.","4613e883":"Thanks if you find it useful! :)","fc41e975":"The expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs.","01c9801c":"Long Short Term Memory(LSTM)\n\nLong short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.","df6dc479":"Now, even though RNNs are quite powerful, they suffer from Vanishing gradient problem which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: Long Short Term Networks(LSTM).","1156940a":"Reference and complete code adopted by:\n\nhttps:\/\/www.kaggle.com\/thebrownviking20\/intro-to-recurrent-neural-networks-lstm-gru\/notebook"}}