{"cell_type":{"999ed553":"code","cca749ea":"code","c0734698":"code","1758b426":"code","0612c8ce":"code","fb4eec27":"code","bcfcf12a":"code","297e41d4":"code","adbe04f1":"code","89619c60":"code","72000627":"code","aa4aa2f3":"code","b28e48a9":"code","759d4612":"code","a498202a":"code","b2139148":"code","1b97174e":"code","e3020e8b":"code","b5ead201":"code","60698811":"code","d0805bd2":"code","a108699e":"code","57c3fb0d":"code","f0f44463":"code","5e81c8e8":"code","d895f2c8":"code","1fbf8d18":"code","8bda1764":"code","fc542e4e":"code","64ab76e2":"code","a5f4227d":"code","459c4575":"code","1dd1c99a":"code","0fcab1fc":"code","5e40af73":"code","00a0f28e":"code","c688fbbd":"code","e795c100":"code","62e17000":"code","627f0d5c":"code","0c3ca1ff":"code","88502d94":"code","59508a79":"code","fd67ad61":"code","3c58a020":"code","9a6fcc12":"code","6011f181":"code","54a18584":"code","982e4e77":"code","5e0ee9ac":"code","55debaf5":"code","610862e7":"code","759b11d6":"code","e757a719":"code","8556bef4":"code","e133e13e":"code","a89bde05":"code","b16e3da5":"code","21466cd6":"code","1cd45106":"code","4bc0b8a4":"code","6ea60922":"code","653a15cc":"code","6bbbff85":"code","38001fe8":"code","59af5186":"code","74a5fafc":"code","d274ea5d":"code","d26074cc":"code","c536691a":"code","cbce91ac":"code","c2ac814e":"code","0ae314f4":"code","fd200133":"code","0021130e":"code","082f7580":"code","e070e3a6":"code","472c7492":"code","50589a9c":"code","ff2c6f8b":"code","af1bcbc3":"code","4e520a43":"code","c7faeb9a":"code","e953634b":"code","b0cc8272":"code","7d7725e5":"code","c911a461":"code","7d7621ba":"code","66cf1fd2":"code","512bf49b":"code","1ba59872":"code","1b8a93ec":"code","dd1a373b":"code","5222e4e0":"code","8d701a38":"code","90c7a362":"code","fe6dc4be":"code","c9152368":"code","64fbed82":"code","726c9d1c":"code","615440f4":"code","04115d11":"code","c4982061":"code","aa402e89":"code","79ae94b0":"code","7e05326f":"code","871ff417":"code","d0544536":"code","221ddadc":"code","527fe06b":"code","faee3fde":"code","32f7c928":"code","08158899":"code","e4681901":"code","176bc7f8":"code","49ba5463":"code","e1c119e8":"code","bd5d9304":"code","ccc4b88c":"code","bb6fea2e":"code","a240339c":"code","3cee9838":"code","05d51706":"code","32f0772f":"code","0465655d":"code","ecd0fc3f":"code","02b657e6":"code","3d782d3f":"code","542f54f5":"code","f6439b7b":"code","bc0a61ec":"code","87c406b6":"code","f909bddf":"code","e23f8684":"code","c2bc84e8":"code","8f1fe4f4":"code","83676aaf":"code","1fb92186":"code","389d78cd":"code","93120bb0":"code","f1e867ea":"code","82dd2ea6":"code","eac71244":"code","c3ffd775":"code","b7423f5a":"code","4c69f2ec":"code","d864a099":"code","3c26859c":"code","c3c473e8":"code","fbd5823d":"code","43637081":"code","882e35b4":"code","b9b513da":"code","291ea2b3":"code","dfe0789f":"code","cf2ba24b":"code","c86a1bda":"code","fe8f0d1c":"code","5eebf451":"code","11330281":"code","3ebd4d9a":"code","70c4207a":"code","54c3bfe1":"code","74acaabd":"code","4e0f90c2":"code","5482bdd7":"code","18a5c646":"code","26641a9e":"code","0ca7d8bc":"code","bbf42335":"code","b2ae8fd5":"code","049df3f1":"code","529ea0e6":"code","2e93c786":"code","3dfd1c74":"code","83587a88":"code","1ffbf497":"code","d07bd538":"code","d20553a9":"code","285522e1":"code","c795913d":"code","774df037":"code","9dc3fe50":"code","4a75ff88":"code","c5e22498":"markdown","97ece00d":"markdown","e2f47242":"markdown","fb0280f1":"markdown","acfbe092":"markdown","ef2b9495":"markdown","ceb3c8b1":"markdown","b24f5763":"markdown","251b59fc":"markdown","e89e025e":"markdown","eb4d0458":"markdown","7a4838d7":"markdown","4e0ae6fb":"markdown","6434cb02":"markdown","7d4f1ecc":"markdown","672f9e4f":"markdown","1ad9571b":"markdown","b557d4b0":"markdown","52b7c1b4":"markdown","96c6aad1":"markdown","db358cc9":"markdown","f1181212":"markdown","059b3702":"markdown","726ef7f6":"markdown","95d0465a":"markdown","4f99b496":"markdown","ea5603cd":"markdown","31dfcdd7":"markdown","d929ed23":"markdown","d7789cff":"markdown","83db0993":"markdown","3b103f4f":"markdown","7b4dfd43":"markdown","72c4cf43":"markdown","268dfe1a":"markdown","eb6c912b":"markdown","b4f53807":"markdown","7484dc0f":"markdown","12cfed6c":"markdown","72c4ef06":"markdown","08d5e5f9":"markdown","cffbe914":"markdown","9af62f2e":"markdown","248a50d5":"markdown","b044fb5a":"markdown","eb4edd4c":"markdown","204f69ff":"markdown","e48423cb":"markdown","8f3e0ed6":"markdown","193b2a90":"markdown","b0fac1af":"markdown","c3031adf":"markdown","9b2ec290":"markdown","a3409d8b":"markdown","3ed35dc2":"markdown","c4cba5de":"markdown","5558c864":"markdown","fbb0d829":"markdown","466dc60a":"markdown","ea366d86":"markdown","8afbabdc":"markdown","f2098db6":"markdown","4459907f":"markdown","98262f5f":"markdown","1f56c792":"markdown","1fe2c013":"markdown","4e5de0ca":"markdown","b81c73c7":"markdown","3b7bd7b7":"markdown","1eea1366":"markdown","7c1d5dcd":"markdown","c5a40253":"markdown","366c00bb":"markdown","a48c2f8b":"markdown","46d20999":"markdown","980849a1":"markdown","a74e3a78":"markdown","c9aa127b":"markdown","d13b309e":"markdown","8842cfa8":"markdown","e1b4326d":"markdown","b214a975":"markdown","aa02199e":"markdown","fd2a6778":"markdown","5afcb96d":"markdown","77a196d3":"markdown","8527ec8f":"markdown","d1517e6d":"markdown","61945b46":"markdown","ced17098":"markdown","ef34f12d":"markdown","7906e294":"markdown","0f6b62db":"markdown","3d0c72e0":"markdown","cbfed628":"markdown","c41fb952":"markdown","2171160a":"markdown","f4319790":"markdown","bd971b75":"markdown","8bbd303b":"markdown","ef4fd1ac":"markdown","cc2c2ca8":"markdown","9eca1556":"markdown","6032c012":"markdown","264f6d98":"markdown","0438f4ed":"markdown","efa69e9f":"markdown","3dd57418":"markdown","66508f4a":"markdown","fbdef0bf":"markdown","3bfbd1bc":"markdown","4daa5376":"markdown","4c8f5a94":"markdown","029fec5e":"markdown","074abb0f":"markdown","f766f70c":"markdown","3018317b":"markdown","e33e75f2":"markdown","138e940d":"markdown","fec733cf":"markdown","9c9e33a7":"markdown","df09ff46":"markdown","cbd919c5":"markdown","34d49ee0":"markdown","00b00352":"markdown","450ef5cc":"markdown","b652fc29":"markdown","6161efd1":"markdown","2aa6d6b1":"markdown","ced5d2b2":"markdown","74849de3":"markdown","214918b8":"markdown","6298c8a0":"markdown","d5d32e49":"markdown","dd62bf54":"markdown","99a22182":"markdown","73bea1bf":"markdown","1259f288":"markdown","b331b358":"markdown","de18c9ca":"markdown","5a1381c8":"markdown","03b15e91":"markdown","a4d515ca":"markdown","e6a4acfa":"markdown","2bcb9d6c":"markdown","6293d4cd":"markdown","14324137":"markdown","29e24e44":"markdown","df189c13":"markdown","b8c9e185":"markdown","f401a38d":"markdown","1d7e7f12":"markdown","ca2805ae":"markdown","f228e299":"markdown","047e6cab":"markdown","79e96cc8":"markdown","a6998415":"markdown","be9aaab6":"markdown","7a2e1841":"markdown","8b38072b":"markdown","e6204a57":"markdown","da855992":"markdown","5fc1fc57":"markdown","5de86b39":"markdown","edb1bd74":"markdown","c811e6c4":"markdown","9883581c":"markdown","f5d7ed34":"markdown","dfb30ab5":"markdown","1300cd45":"markdown","9fcd2bce":"markdown","b5afba26":"markdown","2a8a1eb1":"markdown","ffb87643":"markdown","ea43a357":"markdown","85933e2f":"markdown","4c110429":"markdown","b1d5273e":"markdown","bbbe667e":"markdown","1338812e":"markdown","73d36e5a":"markdown","9de1ab88":"markdown","23f9d9ce":"markdown","e70ea220":"markdown","715938f1":"markdown","150e5792":"markdown"},"source":{"999ed553":"%%capture\n!pip install --upgrade -q comet_ml preprocessing chart_studio plotly nltk surprise ipython-autotime","cca749ea":"# import comet_ml in the top of your file\n# from comet_ml import Experiment","c0734698":"# Basic packages\nimport pickle\nimport re\nimport numpy as np\nimport pandas as pd\n\n# Additional packages\nimport warnings\nfrom collections import OrderedDict\nfrom datetime import date\n\n# NTLK packages\nimport nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# Visualisation packages\nimport chart_studio.plotly as py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot, plot\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\n\n# Scikit-learn packages\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom scipy.sparse.linalg import svds\n\n# Surprise packages\nfrom surprise import (\n    NMF,\n    SVD,\n    BaselineOnly,\n    CoClustering,\n    Dataset,\n    KNNBasic,\n    NormalPredictor,\n    Reader,\n    SlopeOne,\n    SVDpp,\n)\nfrom surprise import accuracy\nfrom surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n\nnltk.download('punkt', quiet=True)\n\nwarnings.filterwarnings('ignore')","1758b426":"# Import Data\n\n# Kaggle base path\nbase_path = \"..\/input\/edsa-recommender-system-predict\/\"\n\n# # Local base path\n# base_path = \"..\/..\/edsa-recommender-predict\/\"\n\nratings_df = pd.read_csv(base_path + \"train.csv\")\nmovies_df = pd.read_csv(base_path + \"movies.csv\")\nimdb_data_df = pd.read_csv(base_path + \"imdb_data.csv\")\n\ngenome_scores_df = pd.read_csv(base_path + \"genome_scores.csv\")\ngenome_tags_df = pd.read_csv(base_path + \"genome_tags.csv\")\nlinks_df = pd.read_csv(base_path + \"links.csv\")\ntags_df = pd.read_csv(base_path + \"tags.csv\")\n\ntest_df = pd.read_csv(base_path + \"test.csv\")\nsample_submission = pd.read_csv(base_path + \"sample_submission.csv\")","0612c8ce":"# Display top 5 rows of dataframe\nratings_df.head()","fb4eec27":"# Gather information about the dataframe\nratings_df.info()","bcfcf12a":"# Check if dataframe as any null values\nratings_df.isnull().sum()","297e41d4":"# Check if dataframe has any empty strings\nnp.where(ratings_df.applymap(lambda x: x == \"\"))","adbe04f1":"# Determining number of rows for each rating value\nrows_rating = ratings_df[\"rating\"].value_counts()\nrows_rating_df = pd.DataFrame({\"rating\": rows_rating.index, \"Rows\": rows_rating.values})\n\n# Determining percentage of rows for each rating value\npercentage_rating = round(ratings_df[\"rating\"].value_counts(normalize=True) * 100, 2)\npercentage_rating_df = pd.DataFrame(\n    {\"rating\": percentage_rating.index, \"Percentage\": percentage_rating.values}\n)\n\n# Joining row and percentage information\nratings_distribution_df = pd.merge(\n    rows_rating_df, percentage_rating_df, on=\"rating\", how=\"outer\"\n)\nratings_distribution_df.set_index(\"rating\", inplace=True)\nratings_distribution_df.sort_index(axis=0)","89619c60":"pyo.init_notebook_mode()\ninit_notebook_mode(connected=True)\ndata = ratings_df[\"rating\"].value_counts().sort_index(ascending=False)\n\n# Plot data\ntrace = go.Bar(\n    x=data.index,\n    text=[\"{:.1f} %\".format(val) for val in (data.values \/ ratings_df.shape[0] * 100)],\n    textposition=\"auto\",\n    textfont=dict(color=\"#000000\"),\n    y=data.values,\n)\n\n# Create layout\nlayout = dict(\n    title=\"Distribution Of {} ratings\".format(ratings_df.shape[0]),\n    xaxis=dict(title=\"Rating\"),\n    yaxis=dict(title=\"Count\"),\n)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\npyo.iplot(fig)","72000627":"# Create list of date objects\nrating_date_list = [\n    date.fromtimestamp(timestamp) for timestamp in list(ratings_df[\"timestamp\"])\n]\n\n# Create year column\nratings_df[\"review_year\"] = [date_item.year for date_item in rating_date_list]\nyears_with_half_scores = ratings_df[\n    ratings_df[\"rating\"].isin([0.5, 1.5, 2.5, 3.5, 4.5])\n][\"review_year\"]\nunique_years_with_half_scores = set(years_with_half_scores)\nprint(\n    \"There are {} years with half scores. \\nThey are: {}.\".format(\n        len(unique_years_with_half_scores), sorted(list(unique_years_with_half_scores))\n    )\n)","aa4aa2f3":"all_scores_after_2003 = len(ratings_df[\"rating\"])\nnumber_of_years_with_half_scores = len(years_with_half_scores)\nprint(\n    \"The percentage of reviews with half scores in the data from 2003 onwards is {:.2%}\".format(\n        number_of_years_with_half_scores \/ all_scores_after_2003\n    )\n)","b28e48a9":"# Find the total number of users and movies and the number of unique users \n# and movies\nunique_users = ratings_df[\"userId\"].nunique()\ntotal_users = len(ratings_df[\"userId\"])\nunique_movies = ratings_df[\"movieId\"].nunique()\ntotal_movies = len(ratings_df[\"movieId\"])\n\n# Display these values\nprint(\n    \"Total number of unique users: \\t{} \\n\"\n    \"Total number of unique movies: \\t{}\\n\"\n    \"Percentage of unique users: \\t{:.2%}\\n\"\n    \"Percentage of unique movies: \\t{:.2%}\".format(\n        unique_users,\n        unique_movies,\n        unique_users \/ total_users,\n        unique_movies \/ total_movies,\n    )\n)","759d4612":"# Display top 5 rows of dataframe\nmovies_df.head()","a498202a":"# Gather information about the dataframe\nmovies_df.info()","b2139148":"# Check if dataframe has any empty strings\nnp.where(movies_df.applymap(lambda x: x == \"\"))","1b97174e":"# Information regarding number of unique values in each column:\nprint(\n    \"Total number of unique movie IDs: \\t{}\\n\"\n    \"Total number of unique movie titles: \\t{}\\n\"\n    \"Total number of unique movie genres: \\t{}\\n\".format(\n        movies_df[\"movieId\"].nunique(),\n        movies_df[\"title\"].nunique(),\n        movies_df[\"genres\"].nunique(),\n    )\n)\nprint(\n    \"There are {} movies with the same name.\".format(\n        movies_df[\"movieId\"].nunique() - movies_df[\"title\"].nunique()\n    )\n)","e3020e8b":"# Top 20 genres by volume:\ngenres_df = movies_df[\"genres\"].value_counts()\ngenres_df.head(10)","b5ead201":"# Bottom 20 genres by volume:\ngenres_df = movies_df[\"genres\"].value_counts()\ngenres_df.tail(20)","60698811":"# Display top 5 rows of dataframe\nimdb_data_df.head()","d0805bd2":"# Gather information about the dataframe\nimdb_data_df.info()","a108699e":"# Find percentage of missing values in each column\ncolumns = imdb_data_df.columns\npercent_missing_values = imdb_data_df.isnull().sum() \/ len(imdb_data_df.index) * 100\nmissing_value_df = pd.DataFrame(\n    {\"column_name\": columns, \"percent_missing\": percent_missing_values}\n)\nmissing_value_df","57c3fb0d":"# Check if dataframe has any empty strings\nnp.where(imdb_data_df.applymap(lambda x: x == \"\"))","f0f44463":"# Information regarding number of unique values in certain column:\nprint(\n    \"Total number of unique movie IDs: \\t{}\\n\"\n    \"Total number of unique title casts: \\t{}\\n\"\n    \"Total number of unique directors: \\t{}\\n\"\n    \"Total number of unique plot keywords: \\t{}\".format(\n        imdb_data_df[\"movieId\"].nunique(),\n        imdb_data_df[\"title_cast\"].nunique(),\n        imdb_data_df[\"director\"].nunique(),\n        imdb_data_df[\"plot_keywords\"].nunique(),\n    )\n)","5e81c8e8":"imdb_data_df[imdb_data_df[\"movieId\"].duplicated()]","d895f2c8":"# Top 5 title cast members by volume:\ncast_df = imdb_data_df[\"title_cast\"].value_counts()\ncast_df.head()","1fbf8d18":"# Top 5 directors by volume:\ndirectors_df = imdb_data_df[\"director\"].value_counts()\ndirectors_df.head()","8bda1764":"# Top 5 plot keywords by volume:\nkeywords_df = imdb_data_df[\"plot_keywords\"].value_counts()\nkeywords_df.head()","fc542e4e":"# Display top 5 rows of dataframe\ngenome_scores_df.head()","64ab76e2":"# Gather information about the dataframe\ngenome_scores_df.info()","a5f4227d":"# Information regarding number of unique values in each column:\nprint(\n    \"Total number of unique movie IDs: \\t\" + str(genome_scores_df[\"movieId\"].nunique())\n)\nprint(\"Total number of unique tag IDs: \\t\" + str(genome_scores_df[\"tagId\"].nunique()))","459c4575":"# Display top 5 rows of dataframe\ngenome_tags_df.head()","1dd1c99a":"# Gather information about the dataframe\ngenome_tags_df.info()","0fcab1fc":"# Information regarding number of unique values in each column:\nprint(\"Total number of unique Tags: \\t\\t\" + str(genome_tags_df[\"tag\"].nunique()))\nprint(\"Total number of unique Tag IDs: \\t\" + str(genome_tags_df[\"tagId\"].nunique()))","5e40af73":"# Display top 5 rows of dataframe\nlinks_df.head()","00a0f28e":"# Gather information about the dataframe\nlinks_df.info()","c688fbbd":"print(\"Total number of unique Movie Ids: \\t\" + str(links_df[\"movieId\"].nunique()))\nprint(\"Total number of unique IMDB Ids: \\t\" + str(links_df[\"imdbId\"].nunique()))\nprint(\"Total number of unique TMDB Ids: \\t\" + str(links_df[\"tmdbId\"].nunique()))","e795c100":"links_df.isnull().sum()","62e17000":"# Check how many tmdb IDs are duplicated:\ntmdb_df = links_df[links_df[\"tmdbId\"].duplicated()]\ntmdb_total = tmdb_df[\"tmdbId\"].value_counts().sum()\nprint(\"Total number of tmdb Ids duplicated: \\t\" + str(tmdb_total))","627f0d5c":"# Display top 5 rows of dataframe\ntest_df.head()","0c3ca1ff":"# Gather information about the dataframe\ntest_df.info()","88502d94":"# Display top 5 rows of dataframe\nsample_submission.head()","59508a79":"# Gather information about the dataframe\nsample_submission.info()","fd67ad61":"sample_submission[sample_submission[\"Id\"].duplicated()]","3c58a020":"# Join Ratings and Movies Data Sets\ncombined_df = pd.merge(ratings_df, movies_df, on=\"movieId\", how=\"left\")\n\n# Join IMDB Data Set as well\ncombined_df = pd.merge(combined_df, imdb_data_df, on=\"movieId\", how=\"left\")\n\n# Display top 5 rows of new combined dataframe\ncombined_df.head(5)","9a6fcc12":"# Information about the combined dataframe\ncombined_df.info()","6011f181":"# Determine how many null values there are in every column\ncombined_df.isnull().sum()","54a18584":"# Generate summary statistics\nsummary_statistics = combined_df[[\"rating\"]].describe().round(2)\nsummary_statistics","982e4e77":"aggregated_df = (\n    combined_df[[\"userId\", \"rating\"]].groupby(\"userId\").agg([\"count\", \"mean\"])\n)\naggregated_df.head(5)","5e0ee9ac":"# Get average\naverage_rating = summary_statistics.loc[\"mean\"][\"rating\"]\n\n# Create figure\nfig = px.scatter(x=aggregated_df[\"rating\"][\"count\"], y=aggregated_df[\"rating\"][\"mean\"])\n\n# Add a title and labels to the axes\nfig.update_layout(\n    title=\"Average Rating By User's Total Rating Submissions\",\n    xaxis_title=\"Count of individual user submissions\",\n    yaxis_title=\"Average rating\",\n)\n\n# Add a line for the average\nfig.add_trace(\n    go.Scatter(\n        x=[0, 15000],\n        y=[average_rating, average_rating],\n        mode=\"lines\",\n        name=\"Average rating\",\n    )\n)\nfig.show()","55debaf5":"# Filter out the one point with over twelve thousand ratings\nremoved_outliers_agg_df = aggregated_df[(aggregated_df[\"rating\"][\"count\"] <= 10000)]\n\n# Create figure\nfig = px.scatter(\n    x=removed_outliers_agg_df[\"rating\"][\"count\"],\n    y=removed_outliers_agg_df[\"rating\"][\"mean\"],\n)\n\n# Add a title and labels to the axes\nfig.update_layout(\n    title=\"Average Rating By User's Total Rating Submissions\",\n    xaxis_title=\"Count of individual user submissions\",\n    yaxis_title=\"Average rating\",\n)\n\n# Add a line for the average\nfig.add_trace(\n    go.Scatter(\n        x=[0, 4000],\n        y=[average_rating, average_rating],\n        mode=\"lines\",\n        name=\"Average rating\",\n    )\n)\nfig.show()","610862e7":"# Get user ID by filtering the aggregated dataframe for the count (2184) attached \n# to the user ID\nuser_id = aggregated_df[aggregated_df[\"rating\"][\"count\"] == 2184].index.values[0]\n\n# Filter ratings dataframe for this user\nanomaly_df = combined_df[combined_df[\"userId\"] == user_id]\nanomaly_df.head(5)","759b11d6":"anomaly_df[\"month\"] = anomaly_df[\"timestamp\"].apply(\n    lambda x: date.fromtimestamp(x).month\n)\nanomaly_df[\"day\"] = anomaly_df[\"timestamp\"].apply(lambda x: date.fromtimestamp(x).day)\n\n# Print out the unique number of ratings and years\nprint(\n    \"Number of reviews: {} \\n \\\n      Unique rating values: {} \\n \\\n      Unique years: {} \\n \\\n      Number of distinct movies watched: {}\\n \\\n      Number of distinct months: {}\\n \\\n      Number of distinct days: {}\".format(\n        len(anomaly_df[\"rating\"]),\n        set(anomaly_df[\"rating\"]),\n        set(anomaly_df[\"review_year\"]),\n        len(set(anomaly_df[\"movieId\"])),\n        len(set(anomaly_df[\"month\"])),\n        len(set(anomaly_df[\"day\"])),\n    )\n)","e757a719":"# Filter dataframe by users with over 1000 reviews\nuser_ids = list(set(aggregated_df[(aggregated_df[\"rating\"][\"count\"] > 1000)].index))\nfiltered_df = combined_df[combined_df[\"userId\"].isin(user_ids)]\n\n# Create dictionary with user ids\nusers_years = {}\nfor user_id in user_ids:\n    list_of_years = sorted(\n        list(set(filtered_df[filtered_df[\"userId\"] == user_id][\"review_year\"]))\n    )\n    users_years[user_id] = {\n        year: list_of_years.index(year) + 1 for year in list_of_years\n    }\n\n# Create new column in dataframe using the dictionary\nfiltered_df[\"year_label\"] = [\n    users_years[int(filtered_df.iloc[row][\"userId\"])][\n        int(filtered_df.iloc[row][\"review_year\"])\n    ]\n    for row in range(len(list(filtered_df.index)))\n]\n\nfiltered_df.head(5)","8556bef4":"year_on_year_summary = (\n    filtered_df[[\"year_label\", \"rating\"]].groupby(\"year_label\").agg([\"mean\", \"count\"])\n)\nyear_on_year_summary.head(5)","e133e13e":"# Create figure\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Bar(\n        x=year_on_year_summary.index,\n        y=year_on_year_summary[\"rating\"][\"count\"],\n        name=\"Number of Reviews\",\n    ),\n    secondary_y=False,\n)\nfig.add_trace(\n    go.Line(\n        x=year_on_year_summary.index,\n        y=year_on_year_summary[\"rating\"][\"mean\"],\n        name=\"Average Rating\",\n    ),\n    secondary_y=True,\n)\n\n# Add a line for the average\nfig.add_trace(\n    go.Scatter(\n        x=[0, 24],\n        y=[average_rating, average_rating],\n        mode=\"lines\",\n        name=\"Average rating (entire dataset)\",\n    ),\n    secondary_y=True,\n)\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Average Rating\", secondary_y=True)\nfig.update_yaxes(title_text=\"Number of Reviews\", secondary_y=False)\n\n# Add a title and an x-axis label\nfig.update_layout(\n    title=\"Average Rating By Year From First Review\",\n    xaxis_title=\"Year Since First Review\",\n)\nfig.show()","a89bde05":"# Create list of date objects\nrating_date_list = [\n    date.fromtimestamp(timestamp) for timestamp in list(combined_df[\"timestamp\"])\n]\n\n# Create a \"day of the week\" dictionary\ndays_of_the_week = {\n    0: \"Monday\",\n    1: \"Tuesday\",\n    2: \"Wednesday\",\n    3: \"Thursday\",\n    4: \"Friday\",\n    5: \"Saturday\",\n    6: \"Sunday\",\n}\n\n# Create day of week and weekday_or_weekend columns\ncombined_df[\"day_of_week\"] = [date_item.weekday() for date_item in rating_date_list]\ncombined_df[\"name_of_day\"] = [\n    days_of_the_week[date_item.weekday()] for date_item in rating_date_list\n]\ncombined_df[\"weekday_or_weekend\"] = [\n    \"Weekday\" if weekday < 5 else \"Weekend\"\n    for weekday in list(combined_df[\"day_of_week\"])\n]\ncombined_df.head(5)","b16e3da5":"# Get mean rating by year\nmean_by_year = combined_df.groupby(\"review_year\").mean()[[\"rating\"]]\n\n# Plot data\nfig = px.line(x=mean_by_year.index, y=mean_by_year[\"rating\"])\nfig.update_layout(\n    title=\"Average Rating Over Time\", xaxis_title=\"Year\", yaxis_title=\"Average rating\"\n)\nfig.show()","21466cd6":"combined_df[combined_df[\"review_year\"] == 1995]","1cd45106":"# Get mean rating by year and weekend\/weekday split\nmean_by_year = (\n    combined_df.groupby([\"review_year\", \"weekday_or_weekend\"])\n    .mean()[[\"rating\"]]\n    .reset_index(level=[1])\n)\n\n# Plot data\nfig = px.line(\n    x=mean_by_year.index,\n    y=mean_by_year[\"rating\"],\n    color=mean_by_year.weekday_or_weekend,\n)\nfig.update_layout(\n    title=\"Average Rating Over Time, Coloured By Day Type\",\n    xaxis_title=\"Year\",\n    yaxis_title=\"Average rating\",\n    legend_title=\"Type of day\",\n)\nfig.show()","4bc0b8a4":"# Exclude the one review from 1995\nmean_by_year = mean_by_year[mean_by_year.index != 1995]\n\n# Plot data\nfig = px.line(\n    x=mean_by_year.index,\n    y=mean_by_year[\"rating\"],\n    color=mean_by_year.weekday_or_weekend,\n)\nfig.update_layout(\n    title=\"Average Rating Over Time, Coloured By Day Type\",\n    xaxis_title=\"Year\",\n    yaxis_title=\"Average rating\",\n    legend_title=\"Type of day\",\n)\nfig.show()","6ea60922":"# Get mean rating by year and weekend\/weekday split\nmean_by_year = (\n    combined_df.groupby([\"review_year\", \"name_of_day\"])\n    .mean()[[\"rating\"]]\n    .reset_index(level=[1])\n)\n\n# Exclude the one review from 1995\nmean_by_year = mean_by_year[mean_by_year.index != 1995]\n\n# Plot data\nfig = px.line(\n    x=mean_by_year.index, \n    y=mean_by_year[\"rating\"], \n    color=mean_by_year[\"name_of_day\"]\n)\nfig.update_layout(\n    title=\"Average Rating Over Time, Coloured By Day\",\n    xaxis_title=\"Year\",\n    yaxis_title=\"Average rating\",\n    legend_title=\"Day\",\n)\nfig.show()","653a15cc":"def extract_year(text):\n    \"\"\"Extracts a year from round brackets within text\"\"\"\n    year = re.findall(\"\\((\\d{4})\\)\", text)\n    if year:\n        return year[-1]\n    else:\n        return None","6bbbff85":"# Use the extract_year function to extract the year the movie was released\ncombined_df[\"movie_release_year\"] = combined_df[\"title\"].apply(\n    lambda x: extract_year(x)\n)\n\n# Check for null values\ncombined_df[combined_df[\"movie_release_year\"].isnull()].info()","38001fe8":"# Extract the number of movies without release year data\nnull_release_year = combined_df[combined_df[\"movie_release_year\"].isnull()]\n\n# Examine results\nprint(\n    \"There are {} movies in the dataset that do not have a release year in their title.\\n\"\n    \"These movies make up {} entries, which is {:.3%} of the total.\".format(\n        null_release_year[\"movieId\"].nunique(),\n        len(null_release_year[\"movieId\"]),\n        null_release_year[\"movieId\"].nunique() \/ len(combined_df[\"movieId\"]),\n    )\n)","59af5186":"# Drop rows with null values for movie_release_year\ncombined_df.dropna(subset=[\"movie_release_year\"], axis=0, inplace=True)","74a5fafc":"# Drop duplicate movie IDs:\nmovies_released_year = combined_df[[\"movieId\", \"movie_release_year\"]].drop_duplicates(\n    subset=[\"movieId\"], keep=False\n)\nmovies_released_year = movies_released_year.groupby(\"movie_release_year\").count()\n\n# Create figure\nfig = px.line(x=movies_released_year.index, y=movies_released_year[\"movieId\"])\nfig.update_layout(\n    title=\"Number of Movies Released Each Year\",\n    xaxis_title=\"Release Year\",\n    yaxis_title=\"Number of Movies\",\n)\nfig.show()","d274ea5d":"review_count_by_release_year = (\n    combined_df[[\"movieId\", \"movie_release_year\"]].groupby(\"movie_release_year\").count()\n)\n\n# Create figure\nfig = px.line(\n    x=review_count_by_release_year.index, y=review_count_by_release_year[\"movieId\"]\n)\nfig.update_layout(\n    title=\"Number of Reviews For Every Release Year In The Dataset\",\n    xaxis_title=\"Release Year\",\n    yaxis_title=\"Number of Reviews\",\n)\nfig.show()","d26074cc":"# Create figure\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Line(\n        x=review_count_by_release_year.index,\n        y=review_count_by_release_year[\"movieId\"],\n        name=\"Number of Reviews Per Release Year\",\n    ),\n    secondary_y=False,\n)\nfig.add_trace(\n    go.Line(\n        x=movies_released_year.index,\n        y=movies_released_year[\"movieId\"],\n        name=\"Number of Movies Released\",\n    ),\n    secondary_y=True,\n)\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Number of Reviews Per Release Year\", secondary_y=False)\nfig.update_yaxes(title_text=\"Number of Movies Released\", secondary_y=True)\n\n# Add a title and an x-axis label\nfig.update_layout(\n    title=\"Number of Movies Released Per Year vs The Number of Reviews For Each Release Year\",\n    xaxis_title=\"Year\",\n)\nfig.show()","c536691a":"combined_df[\"age_of_movie_when_reviewed\"] = combined_df[\"review_year\"] - combined_df[\n    \"movie_release_year\"\n].astype(int)\ncombined_df.head(3)","cbce91ac":"# Check that all movies were above 0 years old when reviewed\ncombined_df[combined_df[\"age_of_movie_when_reviewed\"] < 0].head(3)","c2ac814e":"movies_with_incorrect_age = combined_df[combined_df[\"age_of_movie_when_reviewed\"] < 0][\n    \"title\"\n]\nprint(\n    \"There are {} movies with an age at review below zero and a review \"\n    \"timestamp\/movieId that is therefore possibly incorrect\\n\"\n    \"These entries form {:.3%} of the data.\\n\"\n    \"The first 10 examples of these movies are:\\n{}\".format(\n        movies_with_incorrect_age.nunique(),\n        len(movies_with_incorrect_age) \/ len(combined_df[\"age_of_movie_when_reviewed\"]),\n        list(movies_with_incorrect_age)[:10],\n    )\n)","0ae314f4":"# Drop rows that have an age below zero\ncombined_df.drop(\n    combined_df[combined_df[\"age_of_movie_when_reviewed\"] < 0].index, inplace=True\n)\n\n# Create aggregated dataframe\naverage_rating_by_movie_age = (\n    combined_df[[\"age_of_movie_when_reviewed\", \"rating\"]]\n    .groupby(\"age_of_movie_when_reviewed\")\n    .agg([\"mean\", \"count\"])\n)","fd200133":"# Create figure object\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Line(\n        x=average_rating_by_movie_age.index,\n        y=average_rating_by_movie_age[\"rating\"][\"mean\"],\n        name=\"Average Rating\",\n    ),\n    secondary_y=False,\n)\nfig.add_trace(\n    go.Line(\n        x=average_rating_by_movie_age.index,\n        y=average_rating_by_movie_age[\"rating\"][\"count\"],\n        name=\"Number of Movies Released\",\n    ),\n    secondary_y=True,\n)\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Average Rating\", secondary_y=False)\nfig.update_yaxes(title_text=\"Number of Reviews Submitted\", secondary_y=True)\n\n# Add a title and an x-axis label\nfig.update_layout(\n    title=\"Average Rating and Number of Reviews Submitted Over Movie Lifetime\",\n    xaxis_title=\"Age of Movie\",\n)\nfig.show()","0021130e":"# Exclude rows with no budget values\nbudget_df = combined_df[\n    (~combined_df[\"budget\"].isnull())\n    & (combined_df[\"movie_release_year\"].astype(int) >= 2015)\n]\n\n# Convert budget to integer\nbudget_df[\"budget\"] = [\n    int(re.sub(\"[^0-9]\", \"\", budget)) for budget in budget_df[\"budget\"]\n]\n\n# Check how many years there are in the dataframe\nbudget_df[\"movie_release_year\"].unique()","082f7580":"print(\n    \"There are {} distinct genres in the dataset.\\nThe first 10 \"\n    \"are as follows:\\n{}\".format(\n        len(set(combined_df[\"genres\"])), \"\\n\".join(list(combined_df[\"genres\"])[:10])\n    )\n)","e070e3a6":"# Aggregate data to get the mean rating value for all genres\ngenre_ratings = (\n    combined_df[[\"genres\", \"rating\"]]\n    .groupby([\"genres\"])\n    .agg([\"mean\"])\n    .sort_values(by=[(\"rating\", \"mean\")], ascending=False)\n)\ngenre_ratings.head(10)","472c7492":"# Find the top 10 most and least popular genres\nprint(\n    \"The top 10 highest rated genres are:\\n\\n{}\\n\\nand the 10 lowest rated \"\n    \"genres are:\\n\\n{}\".format(\n        \"\\n\".join(list(genre_ratings.head(10).index)),\n        \"\\n\".join(list(genre_ratings.tail(10).index)),\n    )\n)","50589a9c":"# Aggregate data to get total number of reviews for each genre\ngenre_count = (\n    combined_df[[\"genres\", \"rating\"]]\n    .groupby([\"genres\"])\n    .agg([\"count\"])\n    .sort_values(by=[(\"rating\", \"count\")])\n)\n\n# Filter result\ngenre_count[genre_count[\"rating\"][\"count\"] > 1000]\nprint(\"Rows in genre_count = {}\".format(len(genre_count.index)))\ngenre_count.head(5)","ff2c6f8b":"# Extract list of top 10 genres by volume, excluding \"(no genres listed)\"\nall_genres = list(genres_df.index)\nall_genres.remove(\"(no genres listed)\")\ntop_10_genres = all_genres[:10]\ntop_10_genres","af1bcbc3":"# Filter combined df to extract only the performance of the top 10 genres for \n# the first 20 years\ntop_10_genres_df = combined_df[\n    (combined_df[\"genres\"].isin(top_10_genres))\n    & (combined_df[\"age_of_movie_when_reviewed\"] <= 20)\n][[\"genres\", \"rating\", \"age_of_movie_when_reviewed\"]]\ntop_10_genres_df.head(10)","4e520a43":"# Aggregate data by genre and age of movie when reviewed\ntop_10_genres_df = top_10_genres_df.groupby(\n    [\"genres\", \"age_of_movie_when_reviewed\"]\n).mean()\n\n# Plot data\nfig = px.line(\n    x=top_10_genres_df.index.get_level_values(1),\n    y=top_10_genres_df[\"rating\"],\n    color=top_10_genres_df.index.get_level_values(0),\n)\nfig.update_layout(\n    title=\"Average Rating Over Movie Lifetime, Coloured By Genre\",\n    xaxis_title=\"Movie Lifetime\",\n    yaxis_title=\"Average rating\",\n    legend_title=\"Genre\",\n)\nfig.show()","c7faeb9a":"# Get a dataframe with unique rows for each movie ID and its genre\nunique_movies = (\n    combined_df[[\"movieId\", \"genres\"]]\n    .drop_duplicates([\"movieId\", \"genres\"])\n    .reset_index(drop=True)\n)\nunique_movies.head(5)","e953634b":"def examine_genre(genre_name):\n    \"\"\"\n    Returns a plotly chart with ratings over time for movies in the specified \n    genre and all other movies\n    \n    \"\"\"\n\n    column_name = str(genre_name + \"_or_not\")\n    df_to_merge = combined_df[[\"movieId\", \"rating\", \"age_of_movie_when_reviewed\"]]\n\n    # Create a new column to classify each movie as either being in the genre or \n    # not in the genre\n    unique_movies[column_name] = [\n        genre_name\n        if genre_name in unique_movies.iloc[row][\"genres\"]\n        else \"Not \" + genre_name\n        for row in list(unique_movies.index)\n    ]\n\n    # Merge the unique movies dataframe with the dataframe containing all the \n    # reviews for each movie\n    merged_df = pd.merge(df_to_merge, unique_movies, on=\"movieId\", how=\"left\")\n\n    # Aggregate the newly merged dataframe to obtain the mean for each year for \n    # both the 'genre' and 'not genre' categories\n    aggregated_reviews_df = (\n        merged_df[[\"rating\", \"age_of_movie_when_reviewed\", column_name]]\n        .groupby([column_name, \"age_of_movie_when_reviewed\"])\n        .agg([\"mean\"])\n    )\n\n    # Exclude ratings from 10 or more years ago\n    aggregated_reviews_df = aggregated_reviews_df[\n        aggregated_reviews_df.index.get_level_values(1) <= 20\n    ]\n\n    # Plot data\n    fig = px.line(\n        y=aggregated_reviews_df[\"rating\"][\"mean\"],\n        x=aggregated_reviews_df.index.get_level_values(1),\n        color=aggregated_reviews_df.index.get_level_values(0),\n    )\n    fig.update_layout(\n        title=\"Average Rating Over Time For \" + genre_name + \" Movies\",\n        xaxis_title=\"Movie Lifetime\",\n        yaxis_title=\"Average rating\",\n        legend_title=genre_name + \"\/ Not \" + genre_name,\n    )\n    return fig","b0cc8272":"# Create a list of every subgenre in the dataset\nall_genres = sorted(list(set(\"|\".join(list(unique_movies[\"genres\"])).split(\"|\"))))\nall_genres","7d7725e5":"# Iterate over the all_genres list and create a ratings over time chart for each \n# subgenre in the list.\nfor genre in all_genres:\n    examine_genre(genre).show()","c911a461":"# Find top 10 highest rated movies\ncombined_df.groupby(\"title\")[\"rating\"].mean().sort_values(ascending=False).head(10)","7d7621ba":"# Find the total number of ratings for a movie and display the top ten most rated movies\ncombined_df.groupby(\"title\")[\"rating\"].count().sort_values(ascending=False).head(10)","66cf1fd2":"# New dataframe with average rating per movie\nmean_rating_df = pd.DataFrame(combined_df.groupby(\"title\")[\"rating\"].mean())\n\n# Add the number of ratings per movie to new dataframe\nmean_rating_df[\"total_count\"] = pd.DataFrame(\n    combined_df.groupby(\"title\")[\"rating\"].count()\n)\n\n# Order dataframe by total ratings received\nmean_rating_df = mean_rating_df.sort_values([\"total_count\"], ascending=[False])\nmean_rating_df.head(10)","512bf49b":"mean_rating_df.describe()","1ba59872":"fig = plt.figure(figsize=(18, 8))\nax = fig.gca()\nmean_rating_df.hist(ax=ax, bins=20)\nplt.show();","1b8a93ec":"plt.figure(figsize=(8, 6))\nsns.jointplot(x=\"rating\", y=\"total_count\", data=mean_rating_df, alpha=0.6);","dd1a373b":"# Dataframe displaying all movies rated by a user\nuser_by_movie = combined_df.groupby([\"userId\", \"title\"])[\"rating\"].min().to_frame()\nuser_by_movie.reset_index(level=[\"userId\", \"title\"]).head(5)","5222e4e0":"%load_ext autotime","8d701a38":"# Select sample size of 500 000 to test base models\nmodel_testing_df = ratings_df.sample(n=500000)\n\nreader = Reader(rating_scale=(0.5, 5))\ndata = Dataset.load_from_df(model_testing_df[[\"userId\", \"movieId\", \"rating\"]], reader)","90c7a362":"benchmark = []\n\n# Iterate over all algorithms\nfor algorithm in [\n    SVD(),\n    NMF(),\n    NormalPredictor(),\n    BaselineOnly(),\n    SlopeOne(),\n    CoClustering(),\n]:\n\n    # Perform cross validation\n    results = cross_validate(algorithm, data, cv=5)\n\n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(\n        pd.Series([str(algorithm).split(\" \")[0].split(\".\")[-1]], index=[\"Algorithm\"])\n    )\n    benchmark.append(tmp)","fe6dc4be":"# Show summary dataframe\nsummary_results = (\n    pd.DataFrame(benchmark).set_index(\"Algorithm\").sort_values(\"test_rmse\")\n)\nsummary_results","c9152368":"benchmark_2 = []\n\n# Iterate over all algorithms\nfor algorithm_2 in [SVDpp()]:\n\n    # Perform cross validation\n    results_2 = cross_validate(algorithm_2, data, cv=5)\n\n    # Get results & append algorithm name\n    tmp_2 = pd.DataFrame.from_dict(results_2).mean(axis=0)\n    tmp_2 = tmp_2.append(\n        pd.Series([str(algorithm_2).split(\" \")[0].split(\".\")[-1]], index=[\"Algorithm\"])\n    )\n    benchmark_2.append(tmp_2)","64fbed82":"# Show summary dataframe\nsummary_results_2 = (\n    pd.DataFrame(benchmark_2).set_index(\"Algorithm\").sort_values(\"test_rmse\")\n)\nsummary_results_2","726c9d1c":"## The Kaggle kernel times-out with the KNNBasic model.\n## Uncomment for use in a jupyter kernel.\n\n# model_testing_df_3 = ratings_df.sample(n=100000)\n# data_3 = Dataset.load_from_df(\n#     model_testing_df_3[[\"userId\", \"movieId\", \"rating\"]], reader\n# )\n# benchmark_3 = []\n\n# # Iterate over all algorithms\n# for algorithm_3 in [KNNBasic()]:\n\n#     # Perform cross validation\n#     results_3 = cross_validate(algorithm_3, data_3, cv=3)\n\n#     # Get results & append algorithm name\n#     tmp_3 = pd.DataFrame.from_dict(results_3).mean(axis=0)\n#     tmp_3 = tmp_3.append(\n#         pd.Series([str(algorithm_3).split(\" \")[0].split(\".\")[-1]], index=[\"Algorithm\"])\n#     )\n#     benchmark_3.append(tmp_3)","615440f4":"# # Show summary dataframe\n# summary_results_3 = (\n#     pd.DataFrame(benchmark_3).set_index(\"Algorithm\").sort_values(\"test_rmse\")\n# )\n# summary_results_3","04115d11":"# Use all rows in Ratings dataframe\ndata_4 = Dataset.load_from_df(ratings_df[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data_4, test_size=0.25)","c4982061":"BaselineOnly_1 = BaselineOnly()\n\n# Train the algorithm on the train set, and predict ratings for the test set\nBaselineOnly_1.fit(trainset)\npred_Baseline = BaselineOnly_1.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_Baseline)","aa402e89":"SVD_1 = SVD()\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nSVD_1.fit(trainset)\npred_SVD_1 = SVD_1.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_SVD_1)","79ae94b0":"## Please uncomment for running in a kernel such as jupyter\n\n# # API key to run experiment in Comet\n# experiment = Experiment(\n#     api_key=\"h9aq14TfOuTPJxNhr12fk20kk\",\n#     project_name=\"cinematic-psychic\",\n#     workspace=\"maddy-muir\",\n# )\n\n# reader = Reader(rating_scale=(0.5, 5))\n# model_testing_df_3 = ratings_df.sample(n=10000)\n# data_3 = Dataset.load_from_df(\n#     model_testing_df_3[[\"userId\", \"movieId\", \"rating\"]], reader\n# )\n# param_grid = {\n#     \"n_factors\": [10, 20, 50, 100, 150, 200],\n#     \"n_epochs\": [15, 20, 25, 50, 75, 100],\n#     \"lr_all\": [0.005, 0.008, 0.001],\n#     \"reg_all\": [0.1, 0.3, 0.5],\n# }\n\n# gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n# gs.fit(data_3)\n# algo = gs.best_estimator[\"rmse\"]\n\n# # best RMSE score\n# print(gs.best_score[\"rmse\"])\n\n# # combination of parameters that gave the best RMSE score\n# print(gs.best_params[\"rmse\"])\n\n# experiment.log_dataset_hash(data_3)\n# experiment.log_parameters({\"model_type\": \"SVD\", \"param_grid\": param_grid})\n# experiment.log_metrics({\"RMSE\": gs.best_score[\"rmse\"]})\n# experiment.end()","7e05326f":"# RMSE test whole dataset 1st set of tuned parameters - logged in Comet_ml\nSVD_2 = SVD(n_factors=20, n_epochs=25, lr_all=0.008, reg_all=0.1, random_state=27)\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nSVD_2.fit(trainset)\npred_SVD_2 = SVD_2.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_SVD_2)","871ff417":"# RMSE test whole dataset 1st set of tuned parameters - logged in Comet_ml\nSVD_3 = SVD(n_factors=50, n_epochs=50, lr_all=0.005, reg_all=0.1, random_state=27)\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nSVD_3.fit(trainset)\npred_SVD_3 = SVD_3.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_SVD_3)","d0544536":"## Uncomment code for running in another kernel like Jupyter\n\n# # API key to run experiment in Comet\n# from surprise import BaselineOnly\n\n# experiment = Experiment(\n#     api_key=\"h9aq14TfOuTPJxNhr12fk20kk\",\n#     project_name=\"cinematic-psychic\",\n#     workspace=\"maddy-muir\",\n# )\n\n# param_grid = {\n#     \"bsl_options\": {\n#         \"method\": [\"sgd\"],\n#         \"learning_rate\": [0.004, 0.006, 0.008, 0.010],  # gamma\n#         \"reg\": [0.015, 0.020, 0.025],  # lambda 1 and 5\n#     }\n# }\n# gs_baseline = GridSearchCV(\n#     BaselineOnly,\n#     param_grid,\n#     measures=[\"rmse\"],\n#     cv=3,\n#     return_train_measures=True,\n#     n_jobs=1,\n# )\n# gs_baseline.fit(data_3)\n\n# algo_baseline = gs_baseline.best_estimator[\"rmse\"]\n\n# # best RMSE score\n# print(gs_baseline.best_score[\"rmse\"])\n\n# # combination of parameters that gave the best RMSE score\n# print(gs_baseline.best_params[\"rmse\"])\n\n# experiment.log_dataset_hash(data_3)\n# experiment.log_parameters({\"model_type\": \"BaselineOnly\", \"param_grid\": param_grid})\n# experiment.log_metrics({\"RMSE\": gs_baseline.best_score[\"rmse\"]})\n\n# experiment.end()","221ddadc":"# Testing RMSE on full data set with parameters from first gridsearch\nBaselineOnly_2 = BaselineOnly(\n    bsl_options={\"method\": \"sgd\", \"learning_rate\": 0.01, \"reg\": 0.025}\n)\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nBaselineOnly_2.fit(trainset)\npred_Baseline = BaselineOnly_2.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_Baseline)","527fe06b":"# Testing RMSE on full data set with parameters from first gridsearch\nBaselineOnly_3 = BaselineOnly(bsl_options={\"method\": \"als\", \"reg_i\": 8, \"reg_u\": 4})\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nBaselineOnly_3.fit(trainset)\npred_Baseline_3 = BaselineOnly_3.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_Baseline_3)","faee3fde":"# Number of ratings per user\ndata = ratings_df.groupby(\"userId\")[\"rating\"].count().clip(upper=100)\n\n# Create trace\ntrace = go.Histogram(\n    x=data.values, name=\"Ratings\", xbins=dict(start=0, end=100, size=2)\n)\n# Create layout\nlayout = go.Layout(\n    title=\"Distribution Of Number of Ratings Per User (Clipped at 100)\",\n    xaxis=dict(title=\"Ratings Per User\"),\n    yaxis=dict(title=\"Count\"),\n    bargap=0.2,\n)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","32f7c928":"# Group by user ID and a count of their ratings\nratings_df.groupby(\"userId\")[\"rating\"].count().reset_index().sort_values(\n    \"rating\", ascending=False\n)[:10]","08158899":"# Removing data for movies with less than 25 ratings\nmin_ratings = 25\nfilter_movies = ratings_df[\"movieId\"].value_counts() > min_ratings\nfilter_movies = filter_movies[filter_movies].index.tolist()\n\n# Removing data for users with less than 10 ratings\nmin_user_ratings = 10\nfilter_users = ratings_df[\"userId\"].value_counts() > min_user_ratings\nfilter_users = filter_users[filter_users].index.tolist()\n\nratings_new = ratings_df[\n    (ratings_df[\"movieId\"].isin(filter_movies))\n    & (ratings_df[\"userId\"].isin(filter_users))\n]\nprint(\"The original data frame shape:\\t{}\".format(ratings_df.shape))\nprint(\"The new data frame shape:\\t{}\".format(ratings_new.shape))","e4681901":"# Use all rows in Ratings dataframe\ndata_5 = Dataset.load_from_df(ratings_new[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data_5, test_size=0.25)","176bc7f8":"SVD_4 = SVD(random_state=27)\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nSVD_4.fit(trainset)\npred_SVD_4 = SVD_4.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_SVD_4)","49ba5463":"BaselineOnly_4 = BaselineOnly(bsl_options={\"method\": \"als\", \"reg_i\": 8, \"reg_u\": 4})\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nBaselineOnly_4.fit(trainset)\npred_Baseline_4 = BaselineOnly_4.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_Baseline_4)","e1c119e8":"remove_user_df = ratings_new.copy()\n\n# Drop rows for userID 75309 from data\nremove_user_df.drop(\n    remove_user_df[remove_user_df[\"userId\"] == 75309].index, inplace=True\n)\nremove_user_df.shape","bd5d9304":"reader = Reader(rating_scale=(0.5, 5))\n\n# Use all rows in Ratings dataframe\ndata_6 = Dataset.load_from_df(remove_user_df[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data_6, test_size=0.25)","ccc4b88c":"SVD_5 = SVD(random_state=27)\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nSVD_5.fit(trainset)\npred_SVD_5 = SVD_5.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_SVD_5)","bb6fea2e":"BaselineOnly_5 = BaselineOnly(bsl_options={\"method\": \"als\", \"reg_i\": 8, \"reg_u\": 4})\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nBaselineOnly_5.fit(trainset)\npred_Baseline_5 = BaselineOnly_5.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_Baseline_5)","a240339c":"# Copy dataframe containg ratings and year extracted\ndf_2010 = combined_df.copy()\n\n# Filter by year from 2010\ndf_2010 = df_2010[df_2010[\"review_year\"] >= 2010]","3cee9838":"reader = Reader(rating_scale=(0.5, 5))\n\n# Use all rows in Ratings dataframe\ndata_7 = Dataset.load_from_df(df_2010[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data_7, test_size=0.25)","05d51706":"SVD_6 = SVD()\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nSVD_6.fit(trainset)\npred_SVD_6 = SVD_6.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_SVD_6)","32f0772f":"BaselineOnly_6 = BaselineOnly(bsl_options={\"method\": \"als\", \"reg_i\": 8, \"reg_u\": 4})\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nBaselineOnly_6.fit(trainset)\npred_Baseline_6 = BaselineOnly_6.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(pred_Baseline_6)","0465655d":"# Use all rows in Ratings dataframe\ndata = Dataset.load_from_df(ratings_df[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data_4, test_size=0.25)\n\n# Final Model Building\nSVD_model = SVD(random_state=27)\ntrainset = data.build_full_trainset()\nSVD_model.fit(trainset)","ecd0fc3f":"test_df[\"rating\"] = test_df.apply(\n    lambda x: SVD_model.predict(x[\"userId\"], x[\"movieId\"]).est, axis=1\n)\ntest_df[\"Id\"] = test_df.apply(lambda x: f\"{x['userId']:.0f}_{x['movieId']:.0f}\", axis=1)\nsubmission = test_df[[\"Id\", \"rating\"]]","02b657e6":"submission.to_csv(\"SVD_model.csv\", index=False)","3d782d3f":"# Uncomment to pickle and download the final model\n# pickle.dump(SVD_model, open(\".\/SVD_model.pkl\",'wb'))","542f54f5":"# Select a sample set of 10'000 rows to work with\nsample_df = ratings_df.sample(n=10000)\n\n# Count number of unique movies and users in sample set\nnum_users = sample_df.userId.unique().shape[0]\nnum_movies = sample_df.movieId.unique().shape[0]\nprint(\n    \"Number of users = \" + str(num_users) + \" | Number of movies = \" + str(num_movies)\n)","f6439b7b":"# Pivot the sample set for rating so the columns are the movie ratings per user\npivot_ratings = sample_df.pivot(\n    index=\"userId\", columns=\"movieId\", values=\"rating\"\n).fillna(0)\npivot_ratings.head()","bc0a61ec":"# Format and normalize ratings matrix\nR = pivot_ratings.to_numpy()\nuser_ratings_mean = np.mean(R, axis=1)\nratings_demeaned = R - user_ratings_mean.reshape(-1, 1)","87c406b6":"# Check sparsity level\nsparsity = round(1.0 - len(sample_df) \/ float(num_users * num_movies), 3)\nprint(\"The sparsity level of the dataset is \" + str(sparsity * 100) + \"%\")","f909bddf":"# Use the SVD function to make predictions\nU, sigma, Vt = svds(ratings_demeaned, k=50)\nsigma = np.diag(sigma)\nall_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(\n    -1, 1\n)\npredictions = pd.DataFrame(all_user_predicted_ratings, columns=pivot_ratings.columns)\npredictions.head()","e23f8684":"# Function to provide recommended movies\ndef recommend_movies(pred, userID, movies, original_ratings, num_recommendations):\n\n    # Get and sort the user's predictions\n    user_row_number = userID - 1  # User ID starts at 1, not 0\n    sorted_user_predictions = pred.iloc[user_row_number].sort_values(\n        ascending=False\n    )  # User ID starts at 1\n\n    # Get the user's data and merge in the movie information.\n    user_data = original_ratings[original_ratings.userId == (userID)]\n    user_full = user_data.merge(\n        movies, how=\"left\", left_on=\"movieId\", right_on=\"movieId\"\n    ).sort_values([\"rating\"], ascending=False)\n\n    print(\"User {0} has already rated {1} movies.\".format(userID, user_full.shape[0]))\n    print(\n        \"Recommending highest {0} predicted ratings movies not already rated.\".format(\n            num_recommendations\n        )\n    )\n\n    # Recommend the highest predicted rating movies that the user hasn't seen yet.\n    recommendations = (\n        movies[~movies[\"movieId\"].isin(user_full[\"movieId\"])]\n        .merge(\n            pd.DataFrame(sorted_user_predictions).reset_index(),\n            how=\"left\",\n            left_on=\"movieId\",\n            right_on=\"movieId\",\n        )\n        .rename(columns={user_row_number: \"Predictions\"})\n        .sort_values(\"Predictions\", ascending=False)\n        .iloc[:num_recommendations, :-1]\n    )\n\n    return user_full, recommendations","c2bc84e8":"# Give predictions for user 3 for top 10 movies\nalready_rated, predicted = recommend_movies(predictions, 3, movies_df, ratings_df, 10)","8f1fe4f4":"already_rated.head(10)","83676aaf":"predicted.head(10)","1fb92186":"already_rated, predicted = recommend_movies(predictions, 5, movies_df, ratings_df, 10)","389d78cd":"already_rated.head(10)","93120bb0":"predicted.head(10)","f1e867ea":"# Filter ratings with users that rated more than 60 times to be able to pivot \n# the table\nfiltered_ratings = ratings_df.groupby(\"userId\").filter(lambda x: len(x) >= 60)\n\n# List the movie titles after filtering\nmovie_list_rating = filtered_ratings.movieId.unique().tolist()\n\n# View shape of filtered ratings\nprint(filtered_ratings.shape)","82dd2ea6":"# Calculate percentage of movies and users in filtered dataframe\nunique_movies_f = (\n    len(filtered_ratings.movieId.unique()) \/ len(ratings_df.movieId.unique()) * 100\n)\nunique_users_f = (\n    len(filtered_ratings.userId.unique()) \/ len(ratings_df.userId.unique()) * 100\n)\n\nprint(\n    round(unique_movies_f, 2), \"% of original movie titles in the filtered dataframe.\"\n)\nprint(round(unique_users_f, 2), \"% of original users in the filtered dataframe.\")","eac71244":"# Filter the movies dataframe with the movie titles from the filtered list\nmovies_df = movies_df[movies_df.movieId.isin(movie_list_rating)]","c3ffd775":"# Replace | in genres with a space and make lowercase to use as metadata later\nmovies_df[\"genres\"] = [item.replace(\"|\", \" \").lower() for item in movies_df[\"genres\"]]\nmovies_df.head()","b7423f5a":"# Create movie dictionary to map title to id\nmovie_dict = dict(zip(movies_df.title.tolist(), movies_df.movieId.tolist()))\n\n# Drop timestamp column from filtered dataframe\nfiltered_ratings.drop([\"timestamp\"], axis=1, inplace=True)\nfiltered_ratings.head()","4c69f2ec":"# Create combined dataframe with genres, titles and tags with relevance above 0.7\ncombined = pd.merge(movies_df, genome_scores_df, on=\"movieId\", how=\"left\").merge(\n    genome_tags_df, on=\"tagId\", how=\"left\"\n)\n\nfilter_combined = combined[combined[\"relevance\"] > 0.7]\nfilter_combined.drop([\"tagId\", \"relevance\"], axis=1, inplace=True)\n\n# Replace NaN with empty string\nfilter_combined.fillna(\"\", inplace=True)\nfilter_combined.head()","d864a099":"# Create metadata column from movie tags and genres\nfilter_combined = pd.DataFrame(\n    filter_combined.groupby(\"movieId\")[\"tag\"].apply(lambda x: \" \".join(x))\n)\n\nmovies_meta = pd.merge(movies_df, filter_combined, on=\"movieId\", how=\"left\").fillna(\"\")\nmovies_meta[\"metadata\"] = movies_meta[\"tag\"] + \" \" + movies_meta[\"genres\"]\n\nmovies_meta[[\"movieId\", \"title\", \"metadata\"]].head()","3c26859c":"# Create latent matrix from movie metadata using TF-IDF\ntfidf = TfidfVectorizer(stop_words=\"english\")\n\ntfidf_matrix = tfidf.fit_transform(movies_meta[\"metadata\"])\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=movies_meta.index.tolist())\n\nprint(tfidf_df.shape)","c3c473e8":"# Select top features with Truncated SVD\nsvd = TruncatedSVD(n_components=200)\nlatent_matrix = svd.fit_transform(tfidf_df)\n\n# Plot variance expalained to see what dimensions to use\nexplained = svd.explained_variance_ratio_.cumsum()\nplt.plot(explained, \".-\", ms=10, color=\"blue\")\nplt.xlabel(\"Singular value components\", fontsize=12)\nplt.ylabel(\"Cumulative percent of variance\", fontsize=12)\nplt.show()\n\n# Print percentage of variance explained\nprint(\n    \"200 components explains\",\n    round(svd.explained_variance_ratio_.sum(), 2) * 100,\n    \"% of the variance\",\n)","fbd5823d":"# Number of dimensions to keep\nn = 200\nlatent_matrix_1_df = pd.DataFrame(\n    latent_matrix[:, 0:n], index=movies_meta.title.tolist()\n)\n\n# Content latent matrix shape\nlatent_matrix.shape","43637081":"# View filtered ratings dataset\nfiltered_ratings.head()","882e35b4":"# Merge with movies table with filtered ratings table\nfiltered_ratings_1 = pd.merge(\n    movies_df[[\"movieId\"]], filtered_ratings, on=\"movieId\", how=\"right\"\n)","b9b513da":"# Pivot table\nfiltered_ratings_2 = filtered_ratings_1.pivot(\n    index=\"movieId\", columns=\"userId\", values=\"rating\"\n).fillna(0)","291ea2b3":"# Select top features with Truncated SVD\nsvd = TruncatedSVD(n_components=2000)\nlatent_matrix_2 = svd.fit_transform(filtered_ratings_2)\nlatent_matrix_2_df = pd.DataFrame(latent_matrix_2, index=movies_meta.title.tolist())","dfe0789f":"# Plot variance expalained to see what dimensions to use\nexplained = svd.explained_variance_ratio_.cumsum()\nplt.plot(explained, \".-\", ms=10, color=\"blue\")\nplt.xlabel(\"Singular value components\", fontsize=12)\nplt.ylabel(\"Cumulative percent of variance\", fontsize=12)\nplt.show()\n\n# Print percentage of variance explained\nprint(\n    \"2000 components explains\",\n    round(svd.explained_variance_ratio_.sum(), 2) * 100,\n    \"% of the variance\",\n)","cf2ba24b":"# Get latent vectors for selected movie from content and collaborative matrixes\na_1 = np.array(latent_matrix_1_df.loc[\"Toy Story (1995)\"]).reshape(1, -1)\na_2 = np.array(latent_matrix_2_df.loc[\"Toy Story (1995)\"]).reshape(1, -1)\n\n# Calculate the similartity of selected movie with the other movies\nscore_1 = cosine_similarity(latent_matrix_1_df, a_1).reshape(-1)\nscore_2 = cosine_similarity(latent_matrix_2_df, a_2).reshape(-1)\n\n# Hybrid - an average measure of both content and collaborative\nhybrid = (score_1 + score_2) \/ 2.0\n\n# Create dataframe of similar movies\ndictDf = {\"content\": score_1, \"collaborative\": score_2, \"hybrid\": hybrid}\nsimilar = pd.DataFrame(dictDf, index=latent_matrix_1_df.index)\n\n# Values can be sorted by: content, collaborative or hybrid score\n# Content based sorting selected here\nsimilar.sort_values(\"content\", ascending=False, inplace=True)\n\n# Display top 10 movies\nsimilar[1:].head(11)","c86a1bda":"# Load data from filtered ratings\ndata = Dataset.load_from_df(filtered_ratings[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Train SVD model on 75% of known rates\ntrainset, testset = train_test_split(data, test_size=0.25)\n\n# Fit svd model\nalgorithm = SVD(random_state=27)\nalgorithm.fit(trainset)\n\n# Make predictions on testset\npredictions = algorithm.test(testset)\n\n# Accuracy score using root mean square error\naccuracy.rmse(predictions)","fe8f0d1c":"# Function to return top 10 movies with user ID input\ndef pred_user_rating(ui):\n    if ui in filtered_ratings.userId.unique():\n        ui_list = filtered_ratings[filtered_ratings.userId == ui].movieId.tolist()\n        d = {k: v for k, v in movie_dict.items() if not v in ui_list}\n        predictedL = []\n        for i, j in d.items():\n            predicted = algorithm.predict(ui, j)\n            predictedL.append((i, predicted[3]))\n        pdf = pd.DataFrame(predictedL, columns=[\"movies\", \"ratings\"])\n        pdf.sort_values(\"ratings\", ascending=False, inplace=True)\n        pdf.set_index(\"movies\", inplace=True)\n        return pdf.head(10)\n    else:\n        print(\"User Id does not exist in the list!\")\n        return None","5eebf451":"# Select random user for top 10 movie suggestions\nuser_id = 162478\npred_user_rating(user_id)","11330281":"# List the movie titles of unique title\nmovie_list_rating = ratings_df.movieId.unique().tolist()\nprint(len(movie_list_rating))","3ebd4d9a":"# Filter the movies dataframe with the movie titles from the filtered list\nstreamlit_movies = movies_df[movies_df.movieId.isin(movie_list_rating)]","70c4207a":"# Create a copy of ratings df and drop timesatmp column\nstreamlit_combined = pd.merge(\n    streamlit_movies, genome_scores_df, on=\"movieId\", how=\"left\"\n).merge(genome_tags_df, on=\"tagId\", how=\"left\")\n\n# Keep the tags that have a relevance higher than 0.95\nstreamlit_combined = streamlit_combined[streamlit_combined[\"relevance\"] > 0.95]\n\n# Drop tafId and relevance columns, we are done using them\nstreamlit_combined.drop([\"tagId\", \"relevance\"], axis=1, inplace=True)\n\n# Replace NaN with empty string\nstreamlit_combined.fillna(\"\", inplace=True)\n\nstreamlit_combined.head(3)","54c3bfe1":"# Group by movie Id and create tag column listing all the tags of the movie\nstreamlit_combined = pd.DataFrame(\n    streamlit_combined.groupby(\"movieId\")[\"tag\"].apply(lambda x: \" \".join(x))\n)\n\n# Merge movies df with combined df containing the tags\nstreamlit_movies_meta = pd.merge(\n    streamlit_movies, streamlit_combined, on=\"movieId\", how=\"left\"\n).fillna(\"\")\n\n# Merge movies df with imdb df to get director and title_cast information\nstreamlit_movies_meta = pd.merge(\n    streamlit_movies_meta,\n    imdb_data_df[[\"movieId\", \"director\", \"title_cast\"]],\n    on=\"movieId\",\n    how=\"left\",\n).fillna(\"\")\n\n# Replace '(no genres listed)' with empty string\nstreamlit_movies_meta[\"genres\"] = streamlit_movies_meta[\"genres\"].replace(\n    \"(no genres listed)\", \"\"\n)\n\n# Make director column lowercase for consitency with metadata column we are about \n# to create\nstreamlit_movies_meta[\"director\"] = streamlit_movies_meta[\"director\"].apply(\n    lambda x: x.lower()\n)\n\n# Replace | in genres with a space and make lowercase to use as metadata later\nstreamlit_movies_meta[\"genres\"] = [\n    item.replace(\"|\", \" \").lower() for item in streamlit_movies_meta[\"genres\"]\n]\n\nstreamlit_movies_meta.head(3)","74acaabd":"# Function to extract the year from the movie title\ndef extract_year(text):\n    text = re.findall(\"\\((\\d{4})\\)\", text)\n    for item in text:\n        text = int(item)\n    return text","4e0f90c2":"# Create new 'year' column by extracting year from title\nstreamlit_movies_meta[\"year\"] = streamlit_movies_meta[\"title\"].apply(extract_year)\n\n# Create new top_cast column with first 3 actors of title_cast, seperated on the '|''\nstreamlit_movies_meta[\"top_cast\"] = streamlit_movies_meta[\"title_cast\"].apply(\n    lambda x: \" \".join(x.lower().split(\"|\")[:3])\n)\n\n# Review dataframe\nstreamlit_movies_meta.head(3)","5482bdd7":"# Create metadata column by combining the year, director, top_cast, tags and \n# genre columns\nstreamlit_movies_meta[\"metadata\"] = (\n    streamlit_movies_meta[\"year\"].astype(str)\n    + \" \"\n    + streamlit_movies_meta[\"director\"]\n    + \" \"\n    + streamlit_movies_meta[\"top_cast\"]\n    + \" \"\n    + streamlit_movies_meta[\"tag\"]\n    + \" \"\n    + streamlit_movies_meta[\"genres\"]\n)\n\n# Review dataframe\nstreamlit_movies_meta[[\"movieId\", \"title\", \"metadata\"]].head(3)","18a5c646":"# Creating tokens for the metadata\nstreamlit_movies_meta[\"metadata\"] = streamlit_movies_meta[\"metadata\"].apply(\n    lambda x: word_tokenize(x)\n)\n\n# Review dataframe\nstreamlit_movies_meta[[\"movieId\", \"title\", \"metadata\"]].head(3)","26641a9e":"# Tranform words to their base form using lemmatization\nlemmatizing = WordNetLemmatizer()\n\nstreamlit_movies_meta[\"metadata\"] = streamlit_movies_meta[\"metadata\"].apply(\n    lambda x: \" \".join([lemmatizing.lemmatize(i) for i in x])\n)\n\n# Review dataframe\nstreamlit_movies_meta[[\"movieId\", \"title\", \"metadata\"]].head(3)","0ca7d8bc":"# Remove duplicated words\nstreamlit_movies_meta[\"metadata\"] = (\n    streamlit_movies_meta[\"metadata\"]\n    .str.split()\n    .apply(lambda x: \" \".join(OrderedDict.fromkeys(x).keys()))\n)\n\n# Review dataframe\nstreamlit_movies_meta[[\"movieId\", \"title\", \"metadata\"]].head(3)","bbf42335":"# Create latent matrix from movie metadata using TF-IDF\nvectorizer = TfidfVectorizer(\n    stop_words=\"english\", ngram_range=(1, 2), max_features=1500\n)\n\ntfidf_matrix = vectorizer.fit_transform(streamlit_movies_meta[\"metadata\"])\n\ntfidf_df = pd.DataFrame(\n    tfidf_matrix.toarray(), index=streamlit_movies_meta.movieId.tolist()\n)\n\nprint(tfidf_df.shape)","b2ae8fd5":"# View small subset of top features\nvectorizer.get_feature_names()[600:610]","049df3f1":"# View tfidf_matrix dataframe\ntfidf_df.head(3)","529ea0e6":"# Save tfidf_df as csv file to reference in Streamlit app\n\n# tfidf_df.to_csv('tfidf_df.csv',index=False)","2e93c786":"# Function to find the movie Id for each movie title:\ndef get_movie_id_from_title(title):\n    return movies_df.loc[movies_df[\"title\"] == title, \"movieId\"].iloc[0]","3dfd1c74":"# Function to extract user id's\ndef get_top_rated_users_for_movie(movie_id):\n    \"\"\"Map a given favourite movie to users within the\n       MovieLens dataset with the same preference.\n\n    Parameters\n    ----------\n    movie_id : int\n        A MovieLens Movie ID.\n\n    Returns\n    -------\n    list\n        User IDs of users with similar high ratings for the given movie.\n\n    \"\"\"\n    user_id_df = ratings_df.loc[ratings_df[\"movieId\"] == movie_id].sort_values(\n        \"rating\", axis=0, ascending=False\n    )\n\n    # Return the 5 users who rated this movie the highest\n    return list(user_id_df[\"userId\"])[:5]","83587a88":"# Function to calculate predicted rating of users in list paired with each movie\ndef get_top_predictions_for_users(user_list):\n\n    list_movieId = list(movies_df.movieId.unique())\n\n    predicted_ratings_list = []\n\n    for userID in user_list:\n        for movieID in list_movieId:\n            prediction_value = SVD_model.predict(userID, movieID)\n            predicted_ratings_list.append(\n                {\n                    \"userId\": userID,\n                    \"movieId\": movieID,\n                    \"predicted_rating\": prediction_value.est,\n                }\n            )\n\n    prediction_data = pd.DataFrame(predicted_ratings_list)\n    prediction_data = prediction_data.sort_values(\"predicted_rating\", ascending=False)\n\n    return prediction_data[[\"movieId\", \"predicted_rating\"]].iloc[:50]","1ffbf497":"def collab_model(movie_list, top_n=10):\n    \"\"\"Performs Collaborative filtering based upon a list of movies supplied\n       by the app user.\n\n    Parameters\n    ----------\n    movie_list : list (str)\n        Favorite movies chosen by the app user.\n    top_n : type\n        Number of top recommendations to return to the user.\n\n    Returns\n    -------\n    list (str)\n        Titles of the top-n movie recommendations to the user.\n\n    \"\"\"\n    # Finding movie ID's for input\n    movie_ids = []\n    user_ids = []\n\n    for movie in movie_list:\n        movie_id = get_movie_id_from_title(movie)\n        user_id = get_top_rated_users_for_movie(movie_id)\n\n        if user_id:\n            user_ids = user_ids + user_id\n\n    if len(user_ids) < 1:\n        raise ValueError(\"Unable to find matching users\")\n\n    filtered_predictions = get_top_predictions_for_users(user_ids)\n\n    prediction_output = pd.merge(\n        filtered_predictions, movies_df, on=\"movieId\", how=\"left\"\n    )\n\n    return list(prediction_output[\"title\"].iloc[:top_n])","d07bd538":"list_movies = [\n    \"Grown Ups (2010)\",\n    \"Deadpool (2016)\",\n    \"Grand Budapest Hotel, The (2014)\",\n]","d20553a9":"collab_model(list_movies, top_n=10)","285522e1":"## Uncommnet for running in another kernel such as Jupyter\n\n# #SVD Gridsearch\n\n# #API key to run experiment in Comet\n# experiment = Experiment(api_key=\"x6HjyHbvjLjz7Nlk960OnEMX4\",\n#                         project_name=\"movie-recommender\", \n#                         workspace=\"karinlouw\")\n\n\n# param_grid = {'n_factors': [200],\n#               'n_epochs' : [20,30],\n#               'lr_all': [0.005],\n#               'reg_all': [0.02, 0.03, 0.04],\n#               'init_std_dev': [0.02, 0.03]}\n\n# gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n# gs.fit(sample_data)\n\n# algo = gs.best_estimator['rmse']\n\n# # best RMSE score\n# print(gs.best_score['rmse'])\n\n# # combination of parameters that gave the best RMSE score\n# print(gs.best_params['rmse'])\n\n# experiment.log_dataset_hash(data)\n# experiment.log_parameters({\"model_type\": \"SVD\", \"param_grid\": param_grid})\n# experiment.log_metrics({'RMSE': gs.best_score['rmse']})\n\n# experiment.end()","c795913d":"full_trainset = data_4.build_full_trainset()","774df037":"svd_algo = SVD(n_factors = 200, n_epochs = 30, lr_all = 0.005, reg_all = 0.02, init_std_dev=0.02)\nsvd_algo.fit(full_trainset)","9dc3fe50":"test_df[\"rating\"] = test_df.apply(\n    lambda x: svd_algo.predict(x[\"userId\"], x[\"movieId\"]).est, axis=1\n)\ntest_df[\"Id\"] = test_df.apply(lambda x: f\"{x['userId']:.0f}_{x['movieId']:.0f}\", axis=1)\nsubmission = test_df[[\"Id\", \"rating\"]]","4a75ff88":"submission.to_csv(\"kaggle_submission.csv\", index=False)","c5e22498":"### Links DataFrame\nHere we explore the links dataframe. From Kaggle, this data serves as a link between a MovieLens ID and the IMDB and TMDB IDs associated with it.","97ece00d":"#### Summary of the basic analysis of the Ratings DataFrame\n\nThe ratings dataframe consists of 10'000'038 rows and 4 columns (userId, movieID, rating and timestamp).\n\nRatings are from 0.5 to 5 in increments of 0.5. The majority of ratings were a 4, comprising of close to 27% of all the given data. \n\nThe data contains 162'541 unique users and 48'213 movies were rated.\n\nInteger scores appear favoured over half scores, which were only introduced in 2003.","e2f47242":"This is interesting - horror is in the top ten genres by volume, yet this genre has the lowest average rating.\n\nNext we move on to step 2 to examine the trends by subgenres.","fb0280f1":"The following code shows the approach we took for creating a content based recommender system for the Streamlit app. We adapted the existing function, but the main part was creating a dataframe with the appropriate metadata to make movie recommendations. We create a metadata column with movie tags that have a relevance score of more than, genres, release year, director and first 3 cast members of the film.","acfbe092":"We see that the average is 3.53, which seems sensible for movie reviews with a maximum score of 5 and a minimum score of 0.5. We now want to examine how user reviews change over time and how user reviews change with the user as they become more seasoned critics. To do this, we need to aggregate user data by user IDs to get the average ratings.","ef2b9495":"## Exploring the overall dataset rating trends over time","ceb3c8b1":"## End of Code for Kaggle Competition Submission","b24f5763":"We test this function below by looking at user 3 and 5:","251b59fc":"# Kaggle Competition Submission\n\nThe following code is for Kaggle submission purposes only and is not part of the rest of the notebook.","e89e025e":"Interestingly, we can see that the top genres by volume only have one or two genre types, whereas the bottom genres consist of multiple genres. This is probably because these movies are a lot more niche and uncommon, resulting in a lower volume in the dataset.\n\n#### Summary of the basic analysis of the Movies DataFrame\n\nThe movies dataframe contains 62'423 rows and 3 columns (movieId, title and genres). 98 of the rows have duplicate titles.\n1'639 unique genres are listed, which includes combination genres. 5062 movies do not have a genre listed and 3 most popular genres are: Drama, Comedy and Documentary.\n\nNext we move to the IMBD dataset.\n\n### IMDB Data\nHere we explore the IMBD data to learn more about the content of the movies and the people who worked on them.\n\nWe begin by examining the dataframe.","eb4d0458":"## Python Package Installation\n\nTo run this notebook, install the following root packages on your local machine:\n\n- matplotlib==3.2.2\n- nltk==3.5\n- numpy==1.18.5\n- pandas==1.0.5\n- plotly==4.9.0\n- scikit-learn==0.23.1\n- seaborn==0.10.1\n- surprise==0.1","7a4838d7":"Here we see that there are only 27277 movies in this dataframe, which is less than the 48213 movies in the ratings dataframe. \n\nNext we check for missing data:","4e0ae6fb":"<center><img src=\"https:\/\/3.bp.blogspot.com\/-tRH4a36gEOc\/VlJcXFoY9bI\/AAAAAAAAADo\/fRu2BNRW7W4\/s1600\/Film%2BReel.jpg\" width=\"90%\" \/><\/center>","6434cb02":"# Model Building<a name=\"models\"><\/a>\n[Return to top](#top) <br><br>\n\nHere we build models to test with the test data from Kaggle. We begin this process by using the entire dataset to train our model.","7d4f1ecc":"### Join Data Sets\nHere we join all the datasets together into one combined dataframe and perform a brief basic EDA on the combined data.","672f9e4f":"We do the same with the our fifth BaselineOnly model:","1ad9571b":"These 10 movies have an average rating of 5.0 but they might not be the true top 10 movies - this is because the average rating can be 5.0 even if the movie was only rated once. To determine if they are truly the top movies, we need to look at the number of times a movie was rated and use that to determine a more truthful average rating.","b557d4b0":"Now that we have additional data related to the time at which the review was submitted, we will create dataframes to summarise and plot this data to reveal the following insights:\n1. How the average rating has changed over time\n2. How the average rating has changed over time for weekdays and days on the weekend\n3. How the average rating has changed over time for the day of the week.\n\nWe start by examining hwow the average rating has changed over time:","52b7c1b4":"## Content based system for Streamlit","96c6aad1":"Next, we perform a more granular analysis of individual movie performance. ","db358cc9":"## Fit Model to Whole Dataset\nKnowing that models perform best when trained on a lot of data, we now switch to training our model on the entire train dataset.","f1181212":"# Introduction","059b3702":"From the above, we see that there are movies for which reviews are logged before they were released. We'll briefly examine the movies affected by this:","726ef7f6":"Both the SVD and BaselineOnly model showed an improved RMSE. The next step would be to make a submission on Kaggle with the test data predictions in order to see if it is truly a better dataset to do predictions with.","95d0465a":"From this printout, we can sensibly conclude that this data is neither realistic nor valid. It is not possible for one person to watch that many movies in one day and we can therefore conclude that this user is either a bot or someone unlikely to represent other users.\n\nThe scatter plot from earlier above leaves us with a new set of questions: do people become harsher critics as they review more movies? Or are there more bad movies than good ones, leading to a lower average score by diluting the effect the few good movies have on the user's score? From the EDA earlier in this notebook, we know the most common review score is 4.0 and the average score is 3.53, which indicates perhaps it is not the prevelance of 'bad' movies as much as a change in the reviewer's behaviour as they review more movies. We will examine this trend next.\n\nTo examine this trend, we will filter our dataframe to exclude users with less than 1000 reviews. Once this is done, we create a dictionary with an item for each user_id that points to another dictionary. This inner dictionary has the year and corresponding label for each user (e.g. user 1 started reviewing movies in 1997 therefore the dictionary will look like this: user 1: {1997: Year 1} while user 2 might look like this user 2: {2005: Year 1} because they only started reviewing movies in 2005.","4f99b496":"These movies form only 0.004% of the data and we therefore should exclude them before proceeding. Once this is done, we'll examine how average ratings change over a movie's lifetime.","ea5603cd":"Next we'll examine this data by plotting it in an interactive scatter plot alongside the value for the average rating of the dataset that we calculated earlier in this EDA.","31dfcdd7":"From the display above, we can see that the percentage of unique users and movies are both low.\n\nWe now move on to exploring the ratings dataframe.","d929ed23":"### SVD Model","d7789cff":"## Exploring Average Rating per Movie\nHere we aim to explore data for individual movies by examining both averages and weighted averages for individual movies.","83db0993":"The BaselineOnly model RMSE did not improve with these parameters:  bsl_options={'method': 'sgd', 'learning_rate': 0.01, 'reg': 0.025}","3b103f4f":"Now that we understand the content of this dataframe, we proceed to filter the movies dataframe to exlude items not in the movie_list_rating and process the genres column:","7b4dfd43":"## Pickle Model for use in Streamlit","72c4cf43":"We then filter this dataframe and then create a dataframe with appropriate metadata:","268dfe1a":"The BaselineOnly model has an improved RMSE with the additional data trimming applied for user 75309 but this is not the case with the SVD Model.","eb6c912b":"Now that we have successfully removed this potentially disruptive data, we will move on to testing these models below:","b4f53807":"We then run our fifth SVD model on the data excluding the potential bot:","7484dc0f":"Next we move on to model testing. In order to properly evaluate suitability for real-world use, we import a timer to monitor how long our models take to run.","12cfed6c":"Next we add tags with a high relevance score (above 0.7)","72c4ef06":"# Basic Data Analysis: Conclusion \nFrom this topline EDA, we have found that the combined dataframe has many missing values as a result of the IMDB dataset missing data in all of its columns except movie ID.\n\nNext we perform a more in-depth analysis using the combined dataframe and the insights we gleaned about out data and its limitations in the section above. ","08d5e5f9":"### Model-based collaborative filtering","cffbe914":"We then create and call a function to return the top 10 movies for any user ID and we execute this function with a randonly selected user ID.","9af62f2e":"Testing the two top models, SVD and BaselineOnly model, on the whole dataset with train-test-split the SVD Model has a much lower RMSE of 0.8380 compared to the BaselineOnly RMSE of 0.8683. Gridsearch will now be performed on the SVD Model to do hyperparamenter tuning and hopefully improve the model.","248a50d5":"From the figure above, we can see movies that were released in 1995 are have more reviews in the dataset than any other release year. We understand that this prevelance will possibly make our models favour older movies from this era.\n\nWe then plot these charts together to develop a better understanding of our dataset:","b044fb5a":"Next we find the number of movies released per year in the dataset:","eb4edd4c":"Finally, we can examine the relevant features:","204f69ff":"In today\u2019s technology driven world, recommender systems are socially and economically critical for ensuring that individuals can make appropriate choices surrounding the content they engage with on a daily basis. One application where this is especially true surrounds movie content recommendations, where intelligent algorithms can help viewers find great titles from tens of thousands of options.   \n\nThis notebook follows the step-by-step process to construct a recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences. \n\nProviding an accurate and robust solution to this challenge has immense economic potential, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity. \n","e48423cb":"# Collaborative & Content Based Models<a name=\"model\"><\/a>\n[Return to top](#top) <br><br>\n\n<center><img src=\"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/07\/data-science-movie-recommendation-project.jpg\" width=\"70%\" \/><\/center>\n","8f3e0ed6":"#### Testing Models on Trimmed Dataset - I\n\nWe start by creating a new dataframe and spliting the data into a train, test split:","193b2a90":"### BaselineOnly Model\nWe repeat this process for the BaselineOnly model by running another GridSearchCV.","b0fac1af":"Lastly, we create the tf-idf matrix and add that to a dataframe that is exported as a CSV.","c3031adf":"### Memory-based collaborative filtering","9b2ec290":"### Removing data for movies < 25 ratings & users <10 ratings","a3409d8b":"The KNNBasic model was only tested on a sample size of 100 000 rows and a cv of 3 but took longer than the SVDpp model with testing on 500 000 rows and a cv of 5. The KNNBasic model is not a suitable model to pursue for further testing due to the testing time and a high RMSE.\n\nNext we perform split testing.","3ed35dc2":"We can see that before 2003, movies were not rated with half scores, which helps to explain why half scores are less popular than integer scores. Before moving on, we'll check the percentage of half scores of the total for the ratings from 2003 onwards:","c4cba5de":"*Users who liked this item also liked these other items.* This is a very familiar message from recommender systems all over the internet, whether it's on an online shopping platform or Netflix. With collaborative filtering, our recommender system can recommend movies that are similar to any given movie based on user ratings. This means that we make recommendations based on movies that received a similar rating by other similar users. The dataset we are working with here contains user id's, movies id's, user ratings and a timestamp to indicate when the rating was made. For our purposes, we won't be making use of the timestamp column.\n\nWe start the process by pivoting the table having movie id's as rows, user id's as columns and the ratings that each user made for a movie as the values. There are many missing ratings by users as not all users have rated all movies. This is known as a sparse matrix. We replace these missing ratings with 0's representing missing ratings. Such a large sparse matrix can be complex and computationally expensive to work with. We make use of scikit-learn's Truncated SVD once again to reduce the number of features to a chosen value that explains a percentage of the data.\ncosine\nThe next step is to use a similarity measure to fint the top N most similar movies to any given movie. Cosine similarity is the similarity measure that we will be using here. Mathematically, Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction.\n<br><br>Cosine similarity reference: https:\/\/www.sciencedirect.com\/topics\/computer-science\/cosine-similarity#:~:text=Cosine%20similarity%20measures%20the%20similarity,document%20similarity%20in%20text%20analysis\n\nWe start by first exmaining the dataframe and then merging and filtering the dataframe.","5558c864":"From the summary above the genre_count, we can see there are 555 rows left, which is still too many to condense into a graph. Instead, we will follow two analysis routes: \n1. Examining the trends for the top ten genres (by volume) only, and then \n2. Examinng the trends over time for each subgenre listed in the genre column (e.g. for a movie classigiced as a \"Romantic Comedy\", the subgenres will be \"Romantic\" and \"Comedy\". To do this, we first need to get each movie ID with its genre so that we can iterate over this data for each genre and classify the movie as either being part of the subgenre of not part of the subgenre. We then need to create a function to generate a plot for each genre, as well as a loop to call the function while iterating over a list of the genres. We perform all these steps below, starting with examining trends for the top 10 genres:","fbb0d829":"With a dataset of 500 000 rows the BaselineOnly model outperforms the SVD Model. The fit time of the BaselineOnly model is much better than the SVD model. Gridsearch will be performed on these two models.\nTwo other models, SVDpp and KNNBasic, will also be tested but seperately as the computation time might be much longer. SVDpp will be tested on the same sample size of 500 000 rows and cv of 5 but the KNNBasic will be tested on a much smaller sample set, 100 000 rows and a cv of 3 instead of 5 as we suspect that the computation time will be much longer.","466dc60a":"# Exploratory Data Analysis: Introduction\n<a name=\"eda\"><\/a>\n[Return to top](#top) <br><br>\n\nIn this section we perform an in-depth analysis of the data and seek to find trends, anomalies and additional features to provide rich insights.","ea366d86":"We then graph this summarised data in an attempt to answer our question posed earlier regarding changes in reviewer behaviour as they become more exerienced reviewers. ","8afbabdc":"Reference: https:\/\/blog.codecentric.de\/en\/2019\/07\/recommender-system-movie-lens-dataset\/","f2098db6":"The rating values are incremented at values of 0.5, which have taller bars than the floating values since most of the users assign ratings as the following values (0.5, 1, 1.5, 2 up to 5). \nThe mean value around 3 and there are a few outliers in the data.\n\n\nMost movies received less than 1250 ratings.","4459907f":"### Ratings DataFrame\nWe begin this basic data analysis by examining the ratings dataframe.","98262f5f":"Here we create our streamlit_combined dataframe by merging and processing the streamlit_movies dataframe created above and merging that with the genome_scores_df dataframe.","1f56c792":"The current structure of the dataframe is not in an ideal form, so we pivot it to work around this:","1fe2c013":"### Movie predictions","4e5de0ca":"# Data Trimming<a name=\"trim\"><\/a>\n[Return to top](#top) <br><br> \n<centre><img src=\"https:\/\/library.kissclipart.com\/20180906\/erw\/kissclipart-film-editing-clipart-film-editing-video-editing-cl-29c98066179edee5.jpg\" width=\"25%\" \/><\/centre>\n\nHere we discuss additional methods used in an effort to improve model performance. We begin by experimenting with the removal of users and movies with few ratings.","b81c73c7":"Now that we understand what each column contains and we are certain there are no null values or empty strings, we proceed to examine the number of unique values in the dataset:","3b7bd7b7":"### Genome Tags DataFrame\nHere we explore the tag data. These tags are assigned by a user.","1eea1366":"We then run our fourth SVD model on the trimmed data:","7c1d5dcd":"# Exploring rating trends by genre over time\n\n\n\n<center><img src=\"https:\/\/d2gg9evh47fn9z.cloudfront.net\/800px_COLOURBOX23926136.jpg\" width=\"50%\" \/><\/center>\n\nIn this section we aim to develop an understanding of how the ratings for the various genres change over time. This analysis is challenging as most movies have more than one genre, such as \"Romantic comedies\" or \"Dramic thrillers\". In the below analysis we refer to 'genre' as the entire classification in the dataframe and 'subgenre' as the descriptions separated by pipes (|) in the genre column.\n\nWe will begin by examining the number of unique genres listed in the dataframe.","c5a40253":"From the graph above we can see there is one outlier with close to thirteen thousand reviews. This causes the rest of the data to be squashed near the y-axis, reducing our ability to see any trends. This interactive graph allows users to zoom in to examine the trends currently not visible with the outlier. For the sake of easy reading, to work around the impact of this outlier, we will drop it from the graphing data and replot the data.","366c00bb":"The RMSE have not improved with the application of hyper parameters found via GridSearch.\n\nOf the three models, we can see that the default parameters gives the best RMSE.","a48c2f8b":"These movies make up a small percentage of the total and we can therefore drop them before continuing.","46d20999":"This is the final function in this process, it will call of the functions above to effectively perform the 5 steps.","980849a1":"In the following steps, the similarity of an input movie can be calculated by making use of both the content and collaborative latent matrices. We also added a hybrid system which is an average measure of similarity from both the content and filtering matrices. Researchers argue that the hybrid system predicts the most reasonable similar movie titles compared to predictions made from a single system.","a74e3a78":"Uncomment the code below to install additional packages.","c9aa127b":"We start by installing and importing required packages\/libraries.","d13b309e":"Next we check the dataframe for duplicated movies:","8842cfa8":"From this chart, we can see that the average score for the frequent reviewers consistently falls below the average score for the entire dataset. We can see that even in their first year of submitting reviews, the average was lower than the dataset average. Perhaps the people who regularly review movies are more passionate about them and are therefore less tolerant of a low quality movie and\/or more likely to find fault with movies than someone who rarely reviews movies. \n\nNext , we will move away from the user data and explore the average ratings over time for the entire dataset and then for individual movies. ","e1b4326d":"We then perform the same train and test for the third SVD model below:","b214a975":"Next we examine the percentage prevelance of unique movies in the filtered_ratings dataframe that was created above:","aa02199e":"We then create create a ratings matrix:","fd2a6778":"We then examine the most common cast members, directors, and plot keywords by volume in the dataset.","5afcb96d":"#### Test Models on Trimmed Dataset - II\nWe now train and test additional models on this modified data:","77a196d3":"We create our dataset and split our data below:","8527ec8f":"# EDSA Movie Recommendation Challenge","d1517e6d":"We then select the top features:","61945b46":"We can see that, even after their introduction, half scores are still not as popular as integer scores. \n\nNow that we have visualized the distrubution of ratings in the dataset and have a better understanding of the ratings distribution, we examine the user data:","ced17098":"## Download CSV for Kaggle Competition","ef34f12d":"## Cross-Validation Testing\nHere we perform cross-validation testing on five algorithms: SVD, NormalPredictor, BaseLineOnly, SlopeOne and CoClustering.","7906e294":"The chart above shows that in only 8 years out of 23, users rated movies lower on weekends than what they did during the week. However, these averages do appear to follow similar year on year movements. Next, we'll break down this data further by examining the trend by weekday.","0f6b62db":"From the output above we can see that there are fewer unique movie titles than unique IDs. We know that the difference between these two numbers is equal to the number of movies with the exact same name, which we see is 98. We will keep this in mind while building the recommender system. \n\nNext we explore the genres column, first by finding the top 20 and lowest 20 genres by volume:","3d0c72e0":"#### Summary of the test and sample submission Data\n\nBoth dataframes have 5'000'019 and no IDs are ducplicated.","cbfed628":"#### [Environment Setup](#environment)\n\n1.1 Python Package Setup\n\n1.2 Comet Initialization\n\n1.3 Package Imports\n\n#### [Data](#data)\n\n2.1 Download of dataset\n\n2.2 Basic Data Analysis\n\n#### [Exploratory Data Analysis](#eda)\n\n3.1 Exploring user data\n\n3.2 Exploring overall rating trends over time\n\n3.3 Exploring rating trends over time per movie\n\n3.4 Exploring ratings per genre over time\n\n3.5 Exploring Average Rating per Movie\n\n#### [Base Model Testing](#recommender)\n\n4.1 Cross-Validation Testing \n\n4.2 Train-Test-Split\n\n4.3 Grid Search\n\n#### [Data Trimming](#trim)\n\n5.1 Removing Data with users <10 ratings and movies <25 ratings\n\n5.2 Removing User\/s with excessive reviews in a year (rating all = 5)\n\n5.3 Removing Data for ratings pre-2010\n\n#### [Model Building](#models)\n\n6.1 Fit model to whole dataset \n\n6.2 Download CSV for Kaggle Competition\n\n6.3 Pickle model for use in Streamlit \n\n\n#### [Collaborative & Content Based Models](#model)\n\n7.1 Collaborative Filtering - Approach I\n\n7.2 Content and Collaborative Based - Approach II\n\n7.3 Content Based System for Streamlit\n\n7.4 Collaborative Based System for Streamlit\n\n#### [Conclusion](#conclusion)\n\n","c41fb952":"## GridSearch\nNext we attempt to improve our model's performance by conducting a grid search on both the SVD and BaselineOnly models.","2171160a":"We then tokenize then contents of this column for later use with the if_idf vectorizer:","f4319790":"In the dataframe above we can see the review year and timestamp look similar for the first five columns. We will extract additional data from the timestamp, specifically month and the day of the month, to see how varied the user's contributions are.","bd971b75":"The graph shows that, in general, movies with higher average ratings received more ratings, compared with movies that have lower average ratings.","8bbd303b":"Sparsity and density are terms used to describe the percentage of cells in a database table that are not populated and populated, respectively. From this, we understand that the sum of the sparsity and density should equal 100%. We check the sparsity level below:","ef4fd1ac":"## Basic Data Analysis\nIn this section we perform a basic analysis of the data in the various CSVs to develop an understanding of the data we're able to work with. We conclude this basic data analysis by combining all the data into one dataframe and then continuing into a more in-depth analysis.","cc2c2ca8":"Let's examine this trend without the anomaly in the year 1995:","9eca1556":"# Data imports<a name=\"data\"><\/a>\n[Return to top](#top) <br><br>","6032c012":"We see the average rating become a lot more volatile as the number of reviews submitted becomes smaller, which is expected because there's less smoothing. We see the number of reviews peak when movies are between 0 and 1 years old, which is also where we see an average rating very close to the dataset average. \n\nWe also note a dip in average rating when the movies are between 5 - 12 years old, after which the average rating increases again.\n\n### Change in budgets over time (past 5 years)\n\nLastly, we examine how movie budgets have changed in the past five years. We only examine the past 5 years to work around having to adjust for inflation when comparing monetary values over larger periods. ","264f6d98":"With the Surprise library, the following algorithms will be used. RMSE is used as the accuracy metric for the predictions.\n\n### NormalPredictor\nA basic algorithm that does not do much work but that is still useful for comparing accuracies. This algorithm predicts a random rating based on the distribution of the training set, which is assumed to be normal.\nThe prediction $\\hat{r}_{ui}$ is generated from a normal distribution $\\mathcal{N}(\\hat{\\mu},\\hat{\\sigma}^{2})$ where $\\hat{\\mu}$ and $\\hat{\\sigma}$ are estimated from the training data using Maximum Likelihood Estimation:\n$$\\hat{\\mu}=\\frac{1}{\\mid{R_{train}}\\mid}\\sum_{r_{ui}\\in{R_{train}}} r_{ui}$$\n$$\\hat{\\sigma}=\\sqrt{\\sum_{r_{ui}\\in{R_{train}}} \\frac{(r_{ui}-\\hat{\\mu})^2}{\\mid{R_{train}}\\mid}}$$\n\n### BaselineOnly\nA basic algorithm that does not do much work but that is still useful for comparing accuracies. This algorithm predicts the baseline estimate for a given user and item. \n$$\\hat{r}_{ui}=b_{ui}=\\mu+b_u+b_i$$\nIf user $u$ is unknown, then the bias $b_u$ is assumed to be zero. The same applies for item $i$ with $b_i$.\n\n### KNNBasic\nThis is an algorithms that is directly derived from a basic nearest neighbors approach. The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n\nThe actual number of neighbors that are aggregated to compute an estimation is necessarily less than or equal to $k$. First, there might just not exist enough neighbors and second, the sets $N^{k}_{i}(u)$ and $N^{k}_{u}(i)$ only include neighbors for which the similarity measure is **positive**. It would make no sense to aggregate ratings from users (or items) that are negatively correlated. For a given prediction, the actual number of neighbors can be retrieved in the `actual_k` field of the `details` dictionary of the `prediction`.\nThe prediction $\\hat{r}_{ui}$ is set as:\n<img src=\"https:\/\/github.com\/hesterstofberg\/flipcards\/blob\/master\/KKNbasic.png?raw=true\"\/>\ndepending on the `user_based` field of the `sim_options` parameter.\n\n\n### SVD\nThe famous SVD algorithm, as popularized by [Simon Funk](https:\/\/sifter.org\/~simon\/journal\/20061211.html) during the Netflix Prize.\n\nThe prediction $\\hat{r}_{ui}$ is set as: $\\hat{r}_{ui}=\\mu+b_u+b_i+q^{T}_{i}p_u$\n\nIf user $u$ is unknown, then the bias $b_u$ and the factors $p_u$ are assumed to be zero. The same applies for item $i$ with $b_i$ and $q_i$.\n\nTo estimate all the unknown, we minimize the following regularized squared error:\n$$\\sum_{r_{ui}\\in{R_{train}}}(r_{ui}-\\hat{r}_{ui})^{2}+\u03bb(b^{2}_{i}+b^{2}_{u}+\\parallel{q_{i}}\\parallel^{2}+\\parallel{p_{u}}\\parallel^{2})$$ \nThe minimization is performed by a very straightforward stochastic gradient descent:\n$$b_u\\gets{b_{u}+\u03b3(e_{ui}\u2212\u03bbb_{u})}$$\n$$b_i\\gets{b_{i}+\u03b3(e_{ui}\u2212\u03bbb_{i})}$$\n$$p_u\\gets{p_{u}+\u03b3(e_{ui}.q_i\u2212\u03bbp_{u})}$$\n$$q_i\\gets{q_{i}+\u03b3(e_{ui}.p_u\u2212\u03bbq_{i})}$$\n\nwhere $e_{ui}=r_{ui}\u2212\\hat{r}_{ui}$. These steps are performed over all the ratings of the trainset and repeated `n_epochs` times. Baselines are initialized to `0`. User and item factors are randomly initialized according to a normal distribution, which can be tuned using the `init_mean` and `init_std_dev` parameters.\n\nYou also have control over the learning rate \u03b3 and the regularization term \u03bb. Both can be different for each kind of parameter (see below). By default, learning rates are set to `0.005` and regularization terms are set to `0.02`.\n\n\n### SVDpp\nThe SVD++ algorithm, an extension of `SVD`, takes into account implicit ratings.\nThe prediction $\\hat{r}_{ui}$ is set as:\n$$\\hat{r}_{ui}=\u03bc+b_u+b_i+q^{T}_{i}(p_u+|I_u|^{\u22121\/2}\\sum_{j\\in{I_u}}y_j)$$\n\nWhere the $y_j$ terms are a new set of item factors that capture implicit ratings. Here, an implicit rating describes the fact that a user $u$ rated an item $j$, regardless of the rating value.\n\nIf user $u$ is unknown, then the bias $b_u$ and the factors $p_u$ are assumed to be zero. The same applies for item $i$ with $b_i$, $q_i$ and $y_i$.\n\nJust as for `SVD`, the parameters are learned using a SGD on the regularized squared error objective.\n\nBaselines are initialized to `0`. User and item factors are randomly initialized according to a normal distribution, which can be tuned using the `init_mean` and `init_std_dev` parameters.\n\nYou have control over the learning rate \u03b3and the regularization term \u03bb. Both can be different for each kind of parameter. By default, learning rates are set to `0.005` and regularization terms are set to `0.02`.\n\n\n### Nonnegative Matrix Factorization (NMF)\nNMF is a matrix factorization method where we constrain the matrices to be nonnegative. It is very similar with SVD.\n\nSuppose we factorize a matrix $X$ into two matrices $W$ and $H$ so that $X$ $\\approx$ $WH$. There is no guarantee that we can recover the original matrix, so we will approximate it as best as we can. Now, suppose that $X$ is composed of m rows $x_1$, $x_2$, ... $x_m$ , $W$ is composed of k rows $w_1$, $w_2$, ... $w_k$,  $H$ is composed of m rows $h_1$, $h_2$, ... $h_m$ . Each row in $X$ can be considered a data point. For instance, in the case of decomposing images, each row in $X$ is a single image, and each column represents some feature.\n\n<img src=\"https:\/\/github.com\/hesterstofberg\/flipcards\/blob\/master\/NMF.png?raw=true\"\/>\n\nBasically, we can interpret $x_i$ to be a weighted sum of some components (or bases if you are more familiar with linear algebra), where each row in $H$ is a component, and each row in $W$ contains the weights of each component.\n\n<img src=\"https:\/\/github.com\/hesterstofberg\/flipcards\/blob\/master\/NMF1.png?raw=true\"\/>\n\nIn practice, we introduce various conditions on the components, so that they can be interpreted in a meaningful manner. In the case of NMF, we constrict the underlying components and weights to be non-negative. Essentially, NMF decomposes each data point into an overlay of certain components.\n\n### SlopeOne\nSlope One algorithm uses simple linear regression model.The prediction $\\hat{r}_{ui}$ is set as:\n$$\\hat{r}_{ui}=\\mu_{u}+\\frac{1}{\\mid{R_{i}(u)}\\mid}\\sum_{j\\in{R_{i}(u)}}dev(i,j)$$\nwhere $R_{i}(u)$ is the set of relevant items, i.e. the set of items $j$ rated by $u$ that also have at least one common user with $i$. $dev(i,j)$ is defined as the average difference between the ratings of $i$ and those of $j$:\n$$dev(i,j)=\\frac{1}{\\mid{U_{ij}}\\mid}\\sum_{u\\in{U_{ij}}}r_{ui}-r_{uj}$$\n\n### CoClustering\nBasically, users and items are assigned some clusters $C_u, C_i$, and some co-clusters $C_{ui}$. The prediction $\\hat{r}_{ui}$ is set as:\n$$\\hat{r}_{ui}=\\bar{C_{ui}}+(\u03bc_u\u2212\\bar{C_u})+(\u03bc_i\u2212\\bar{C_i})$$\nwhere $\\bar{C_{ui}}$ is the average rating of co-cluster $C_{ui}$, $C_u$ is the average rating of $u$\u2019s cluster, and $\\bar{C_i}$ is the average rating of $i$\u2019s cluster. \n\nIf the user is unknown, the prediction is $\\hat{r}_{ui}=\u03bc_i$. If the item is unknown, the prediction is $\\hat{r}_{ui}=\u03bc_u$. If both the user and the item are unknown, the prediction is $\\hat{r}_{ui}=\u03bc$.","0438f4ed":"We then process the genres column to remove the pipe symbol:","efa69e9f":"## Collaborative Based System for Streamlit","3dd57418":"### BaselineOnly Model\nTest the BaseLineOnly model.","66508f4a":"We then create a dataframe to display the count and percentage of each rating value in the dataset.","fbdef0bf":"### Removing Data for all Ratings before 2010","3bfbd1bc":"Once this is done, we're able to create a new metadata column in the dataframe by concatenating the tag with the genres.","4daa5376":"Deploying the collaborative filtering model on our Streamlit presented quite challenge as the app input is three movies titles instead of a user ID or user name, which is a change from our current recommendation systems. Without having a user ID it is difficult for the model discussed earlier in the notebook to offer a list of predictions. \n\nWe devised a solution to work around this problem. The following steps were taken in order provide movie recommendations on the collaborative system:\n\n    1) Find the movie ID for each movie title provided\n    2) Use the movie ID to find 5 user profiles (per movie) that gave these movies the highest rating \n       by referencing the Ratings Dataframe\n    3) From this, we will now have a list of 15 user IDs\n    4) Pair all these user IDs with all the movie IDs in the movies dataframe and use the SVD model to give these \n       combos a predicted rating.\n    5) Use the output from step 4 and place it in a dataframe ordered in descending order by the predicted rating.\n    6) Select the top n movies as recommendation for the app user.\n    \nRefrence code below for implementation. \n\nAs an example the following 3 movies were selected for implementation and testing:\n\nGrown Ups (2010)\n\nDeadpool (2016)\n\nGrand Budapest Hotel, The (2014)\n\nWe start by defining all the functions that are required to perform the 5 steps stated above:","4c8f5a94":"### Test and sample submission Data\nHere we briefly explore the data provided by Kaggle that users use for submissions.","029fec5e":"We then use this matric we created as input to generate predictions with the SVD function:","074abb0f":"In the chart above we see that the number of movies released in 1995 is not much different from the number of movies released in years in the same decade, yet those movies have been reviewed more than movies released before or after this year. This might be because the internet was just taking off around this time and submitting movie reviews was a new, novel process that many people took part in initially.\n\n### Change in movie ratings over time\n\nNext we explore how a movie's rating changes with the movies age.","f766f70c":"### Model Testing with Hyper Parameters Tuned - BaselineOnly\nHere we ","3018317b":"We then generate a random list of movies:","e33e75f2":"By removing users with less than 10 ratings and movies with less than 25 ratings the RMSE for the BaselineOnly model remained the same but the SVD Model improved slightly. Next we will remove a certain user and test the RMSE for BaselineOnly and SVD again.","138e940d":"## Train-Test-Split Testing of Top 2 Models\nHere we perform a train-test-split test on the BaselineOnly model and SVD model.","fec733cf":"In order to examine the trend over time, we first need to convert the timestamp to a date object and retrieve the year attribute from this object. In addition, we also retrieve additional data from the timestamp such as day of week and a 'weekday_or_weekend' column. Once complete, we will then pick an arbitrary cutoff 'number of reviews per user' of 1500 reviews and exclude all users with less than 1500 reviews from the data.","9c9e33a7":"### Matrix Factorisation","df09ff46":"We then perform trimming on the dataframe by removing the movies with less than 25 ratings and users with less than 10 ratings:","cbd919c5":"Our data is ready for feature reduction! We do this using truncated SVD to select features to use.","34d49ee0":"Finally, we will be making use of the SVD model of the Surprise package to make movie recommendations. With this model, we make a function which takes in a user ID and recommends 10 movies based on the data we have of that user.\n\nThe Singular Value Decomposition (SVD) is a method from linear algebra and is generally used as a dimensionality reduction technique to reduce the number of features of a dataset. We applied this method earlier when we reduced the number of features in our content and collaborative matrices. In the context of the recommender system, the SVD uses the same matrix structure as the collaborative based where each row represents a user, and each column represents an item. The values are ratings that are given to items by users. \n\nThe factorisation of this matrix is done by the singular value decomposition. It finds factors of matrices from the factorisation of a high-level (user-item-rating) matrix. The singular value decomposition is a method of decomposing a matrix into three other matrices as given below:\n![alt text](https:\/\/mk0analyticsindf35n9.kinstacdn.com\/wp-content\/uploads\/2020\/03\/eq1.png)\nWhere A is a *m x n* utility matrix, *U* is a *m x r* orthogonal left singular matrix, which represents the relationship between users and latent factors, *S* is a *r x r* diagonal matrix, which describes the strength of each latent factor and *V* is a *r x n* diagonal right singular matrix, which indicates the similarity between items and latent factors. \n<br><br>\nSVD reference: https:\/\/analyticsindiamag.com\/singular-value-decomposition-svd-application-recommender-system\/#:~:text=In%20the%20context%20of%20the,given%20to%20items%20by%20users.\n\nWe start this process by splitting our data, fitting an SVD model to the training data and testing it.","00b00352":"The recommender was quite accurate in picking up on the taste of the user. The user watched \"Harry Potter and the Chambers of Secret\" and gave it a rating of 5 and the system recommended \"Harry Potter and the Deathly Hallows: Part I\"","450ef5cc":"# Base Model Testing<a name=\"recommender\"><\/a>\n[Return to top](#top) <br><br>","b652fc29":"We now want to construct the tf-idf matrix as mentioned above","6161efd1":"Collaborative Filtering (CF) is the most popular approach to build Recommendation System and has been successfully employed in many applications. In CF, past user behavior are analyzed in order to establish connections between users and items to recommend an item to a user based on opinions of other users. Those customers, who had similar likings in the past, will have similar likings in the future. CF faces issues with sparsity of rating matrix and growing nature of data. These challenges are well taken care of by Matrix Factorization (MF). \n\nMatrix decompositions are methods that reduce a matrix into constituent parts that make it easier to calculate more complex matrix operations. Matrix decomposition methods, also called matrix factorization methods, are a foundation of linear algebra in computers, even for basic operations such as solving systems of linear equations, calculating the inverse, and calculating the determinant of a matrix.\n\n\nFor more information regarding this topic the following source material can be referenced:\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S1877050915007462\n\nhttps:\/\/machinelearningmastery.com\/introduction-to-matrix-decompositions-for-machine-learning\/","2aa6d6b1":"# Environment Setup<a name=\"environment\"><\/a>\n[Return to top](#top) <br><br>","ced5d2b2":"As previously seen in the EDA user 75309 has seen an unrealistic amount of movies within one a year time period and rated all movies a 5. Below is a summary of this user's rating information:\n\n       Number of reviews: 2184 \n       Unique rating values: {5.0} \n       Unique years: {2019} \n       Number of distinct movies watched: 2184\n\nThe data for this user will now be removed and the RMSE will be evaluated.","74849de3":"## Package Imports","214918b8":"In the dataframe above we can see that 4.0 is the most commonly score, with 26.53% of the movies in the dataframe assigned that score.\n\nWe visualize the data below:","6298c8a0":"#### Summary of the basic analysis of the IMDB DataFrame\n\nThe IMDB Dataframe has 27'278 rows and 6 columns (movieId, title_cast, director, runtime, budget and plot_keywords). MovieId is the only column that doesn't have any null values. All the other columns have at least 36% of missing values, with the budget column having the hightest percentage of null values at 71%. No movieId's are duplicated. \n\nLuc Besson, Woody Allen and Stephen King are the 3 directors that appear most often in this dataset. \n\nThe most popular plot keywords are \"documentary\", \"action\", and \"f rated\".","d5d32e49":"In this section we aim to explore the data specific to the users who contributed ratings. Specifically, we would like to examine how ratings vary by the total number of contributions per user (i.e. if the number of ratings the user submits impacts the rating value).\n\nWe start this EDA by generating summary statistics for the rating values:","dd62bf54":"### Model Testing with Hyper Parameters Tuned - SVD\nHere we use the first set of tuned hyperparameters to train and test our second SVD model.","99a22182":"### Removing Data for user 75309","73bea1bf":"We continue to extract and engineer features here by adding information about the movie release year, director, cast members, tags and genres to a single column in the dataframe. We examine the results of this effort below: ","1259f288":"This data is noisy and does not highlight a distinct relationship between the day of the week and the average rating assigned. Instead, the year in which the rating was submitted appears to have a more important effect on the size of the average rating. To examine the influence of the year further, we would probably need to combine this data with data about the economy, mental health and the political climate - for example, the stock market crash in September 2008 may have had an impact on movie budgets and mental health, causing a drop in the average rating from 2008 to 2009. The influence of external factors is beyond the scope of this EDA and will not be discussed further.\n\nNext we will examine the ratings by movie over time, with a specific focus on movies whose reviews have changed dramatically over time (i.e. have either 'not aged well' or become cult classics). \n\n# Exploring individual movie trends & rating trends over time\n\nIn this section we aim to uncover insights about how the reception of a movie changes over time. Here we will focus on how a movie's average score changes over time and whether or not this change is meaningful. As with the user ratings, we will group by year and examine both the volume and average ratings the movies received over the time since it was released. \n\n### Movie releases by year\nHere we explore the movie releases per year. We'll start by extracting the year the movie was released from the movie title.","b331b358":"Here we seek to improve our RMSE score by excluding movies with very few reviews, the logic behind this process is that by excluding movies with few reviews we protentially exclude volatility. By doing this we hope to train our models on trustworthy data representative of the population data. A similar approach will be taken by excluding users that have given less than a certain number of reviews.\n\nFirst we will look at the number of ratings given by a user.","de18c9ca":"## Collaborative Filtering - Approach 1","5a1381c8":"We know that opions and humour have changed quite a bit since 1995, the first year of ratings. Because of this we want to determine if predictions will improve if we potentially remove the 'outdated' ratings. In order to test this we will only work with the data that was rated from 2010 onwards.","03b15e91":"We see the RMSE is 0.8321, which is not our best score. We continue testing this trimming method by training and testing our fourth BaselineOnly model: ","a4d515ca":"Interestingly, we see half scores (0.5, 1.5, 2.5, 3.5 and 4.5) are less commonly used than integer score values. We don't know if this is because users prefer to rate movies with integer values or if it's because half scores were introduced after the original scoring system was already in use, leading to a decreased volume in a dataset with ratings from 1995. We quickly attempt to understand this further by investigating which years recorded half-score ratings:","e6a4acfa":"Instead of working with all the data provided, we will select a sample to work with as one does not need all the movies and users to get a good approximation.\n\nWe start by selecting the sample and examining the number of unique movies and users in this sample we generated:","2bcb9d6c":"We continue to process this dataframe by dropping the timestamp column and mappign the movie title to its ID.","6293d4cd":"This system works well and generates a varied selection of options for the user.\n\n# Conclusion<a name=\"conclusion\"><\/a>\n[Return to top](#top) <br><br>","14324137":"Resource: https:\/\/github.com\/khanhnamle1994\/movielens\/blob\/master\/SVD_Model.ipynb","29e24e44":"## Content & Collaborative Based - Approach II","df189c13":"Next we need to check if the dataframe has any empty strings as empty strings are not noticed when checking the dataframe for null values.","b8c9e185":"### SVD Model\nTest the SVD model.","f401a38d":"We continue to process and adjust the streamlit_combined dataframe below:","1d7e7f12":"### Genome Scores DataFrame\nHere we explore the genome scores data. This dataset contains scores that measure the relevance of a tag to a movie.","ca2805ae":"We then create a summary dataframe to find the average rating as well as the total number of movies reviewed by year label (i.e. how many movies in total were reviewed in reviewers' first year of reviewing movies):","f228e299":"Here we see many columns are missing data. At 36.91%, the title cast column is missing the least amount of data - this is not ideal. We will keep this data set's limitation in mind further on in the EDA. \n\nNext, we again check the dataframe for empty strings:","047e6cab":"Next we examine the number of unique items in each column, keeping in mind that all columns except for the movie ID column is missing data:","79e96cc8":"This is interesting! From 1813 reviews onwards, only two of the reviews are above the dataset average while the rest are below the average rating. One of these two has an average of 5.0, which is suspiscious - we do not expect an average of 2184 scores to yield 5.00. This user is investigated below:","a6998415":"In this method, we don't have a usual model that learns from data to make a prediction. Instead, we use a pre-computed matrix of similarities that can be used for predictions. We collect features from each movie by selecting the genre, release year, the most relevant tags given to each movie by several users, the director and the top cast members of each movie to form a dataframe with the metadata. We use scikit-learn's TF-IDF vectorizer to transform this metadata text to vectors of features. This can be a very large vector with more than 100 000 features! We don't need that many, and not all of them will add much value to the data. We reduce the number of features with scikit-learn's Truncated SVD and we only select the features that explain most of the data. We refer to this matrix as a latent-matrix.\n\nAside from the metadata we have of each movie, we have another very valuable source of information available, the user data. Later in the notebook, we will be making use of this user data to make predictions when given a user ID, the collaborative based recommender system. Finally, these two systems can be used together to recommend more accurate predictions by calculating the average score of each model, this is known as a hybrid recommender system.\n\nWe start this process by filtering the user and movie data by users that rated more than 60 movies. We are losing some information in this process, but it is necessary to create a user-item matrix for the collaborative based system later in this notebook.","be9aaab6":"**Please run the cell below in order for the formulas to display properly.**\n\n**The formulas will be correctly displayed in text editors like Jupyter Notebook\/Kaggle, displaying these formulas on Github may result in some issues.**","7a2e1841":"### Movies DataFrame\nHere we perform a basic analysis of the movies dataframe. We begin this analysis by generating the head of the dataframe below:","8b38072b":"Next we examine data related to the variance explained:","e6204a57":"We train another SVD model on this smaller, more up to date data:","da855992":"Next we want to see how the popularity of each of these genres has changed over time. However, because there are 1558 unique genres, we will first create a new dataframe with a count of each unique genre and exclude the genres with less than 1000 reviews. ","5fc1fc57":"In a continued effort to prepare out metadata column for langauge processing, we lemmatize the words in this now tokenized column to transform each word to its base form.","5de86b39":"We continue the processing of the metadata cell by removing repeated words:","edb1bd74":"## Comet Initialization\nDue to the way Comet ties into other Maching Learning packages automatically to track certain features, it is required to be one of the first packages imported at the top of the notebook.","c811e6c4":"These 10 movies have the highest number of reviews and this list is vastly different to the list printed above. In order to work around the issue of a small number of reviews distorting the true average, we create a new dataframe below:","9883581c":"We can see that the number of movies released each year shows a general increasing trend. \n\nThis is in line with our expectations as movie production technology has improved over the years, allowing for faster production. \n\n### Reviews per movie release year\n\nNext we examine how many many reviews exist for every release year:","f5d7ed34":"We have run six SVD models and6 BaseLineOnly models. Below is a table summary of the various models we have tested:\n\n|Model |RMSE |Parameters |Description |\n|--- |--- |--- |--- |\n|SVD_1 |0.8380 |n_factors=100,n_epochs=20,lr_all=0.005,reg_all=0.02|Default |\n|SVD_2 |0.8539 |n_factors=20,n_epochs=25,lr_all=0.008,reg_all=0.1 |GridSearh |\n|SVD_3 |0.8458 |n_factors=50,n_epochs=50,lr_all=0.005,reg_all=0.1 |GridSearh |\n|SVD_4 |0.8321 |Default |Trimmed Data(A): movies <25 Ratings & Users <10 Ratings |\n|SVD_5 |0.8332 |Default |Trimmed Data(A) & User 75309 Removed |\n|SVD_6 |0.8283 |Default |Trimmed Data (B) - Remove ratings pre-2010 |\n|BaselineOnly_1 |0.8683 |bsl_options={} |Default |\n|BaselineOnly_2|0.8719 |bsl_options={'method':'als','reg_i':8,'reg_u':4} |GridSearh |\n|BaselineOnly_3|0.8653 |bsl_options={'method':'sgd','learning_rate':0.01,'reg':0.025}) |GridSearh |\n|BaselineOnly_4|0.8653 |bsl_options={'method':'als','reg_i':8,'reg_u':4} |Trimmed Data: movies <25 Ratings & Users <10 Ratings |\n|BaselineOnly_5|0.8610 |bsl_options={'method':'als','reg_i':8,'reg_u':4} |Trimmed Data: & User 75309 Removed |\n|BaselineOnly_6 |0.8528 |bsl_options={'method':'als','reg_i':8,'reg_u':4} |Trimmed Data (B) - Remove ratings pre-2010 |\n\n*RMSE - these values might be slightly different on rerunning the notebook\n\nBased on the summary it is clear that the SVD model gives a lower RMSE value than the BaselineOnly model. With Hyper Parameter tuning the SVD model RMSE did not improve, whereas the BaselineOnly model did have an improvement on the RMSE. \n\nTaking the approach of the data trimming is definitely an avenue that should be explored further as the SVD Model RMSE did improve (#4 & #6). The BaselineOnly model RMSE improved every time the Data Trimming was applied.\n\nAdditionally, we know from our EDA that a disproportionate number of reviews were submitted for movies released in the year 1995, which means that the models we've generated are trained on slightly outdated data and could presumably be improved by additional training on a dataset with reviews for more current movies. In addition, we would like to repeat this process on larger datasets that would allow us to omit incomplete rows.","dfb30ab5":"We do the same with a BaselineOnly model:","1300cd45":"#### Summary of the basic analysis of the Genome Scores DataFrame\n\nThe Genome Scores Dataframe has 15'584'448 rows and 3 columns (movieId, tagId and relevance). There are 13'816 unique movie ids and 1'128 unique tag ids. ","9fcd2bce":"We then create a function to generate a dataframe of recommended movies, which is then used for a randomly selected user to test this system.","b5afba26":"Next we'll find the 10 most popular and 10 least popular genres using the average of the rating value.","2a8a1eb1":"There was a small improvement in the RMSE score for the BaselineOnly model with these hyper parameters: \n\nbsl_options={'method':'als', 'reg_i':8, 'reg_u':4}","ffb87643":"Although the SVDpp has a lower RMSE score, the fitting time is almost triple the amount of time it takes for the SVD model and is therefore not a suitable model to continue testing.","ea43a357":"We then test this system using a selection of the top ten recommended movies:","85933e2f":"We can see that the year 1995 only logged one review for the year, which explains why the average rating is 5.0. Because the year only has one rating value, the average statistic here is not as meaningful as the same statistic for the other years.\n\nNext, we examine the average rating over time broken out by whether the review was submitted during the week or over the weekend. The expectation is that people are more stressed during the week and are possibly more likely to leave a bad review as what they would be if they had to review the same movie over the weekend (in a more relaxed state). We examine the trend below:","4c110429":"In the chart above we have another anomaly: the year 1995 has an average of 5.0. This is possibly due to a very small sample size or human error. We'll examine our combined_df dataframe to see what could be causing this: ","b1d5273e":"We then extract additional data from the raw data by creating a function to extract the year from the movie title and extract the top cast members from the title cast column in the dataframe.","bbbe667e":"#### Summary of the Links DataFrame\n\nThe links dataframe has 62'423 rows and 3 columns (movieId, imdbId and tmdbId). There are 62'423 unique movie and imdb IDs, which correspond with the number of rows in the dataframe. 107 of the tmdbId's are null values and 35 values are also dupclicates.","1338812e":"We can see in the above that there are only 4752 rows in the entire datset that are missing a release year. We briefly determine the number of unique movies this number represents:","73d36e5a":"We then calculate the RMSEs for the five algorithms and display the scores in a dataframe.","9de1ab88":"From the above we see that movies from 2016 onwards do not have any budget-related data. This is a serious limitation of the data and no further budget analysis can be conducted (without needing to first adjust for inflation).\n\nWe now move on to examining genre trends over time.","23f9d9ce":"## Exploring user data","e70ea220":"# Table of Contents\n<a id = \"top\"><\/a>","715938f1":"#### Summary of the basic analysis of the Genome Tags DataFrame\n\nThe Genome Tags Dataframe has 1'128 rows and 2 columns (tagId and tag). All values are unique.","150e5792":"### Memory based content filtering"}}