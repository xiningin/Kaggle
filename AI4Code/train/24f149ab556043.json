{"cell_type":{"9cdf3f5f":"code","545a7ca5":"code","f6bb15fb":"code","19dd95b1":"code","d86915b6":"code","65a05631":"code","b7058874":"code","98bc79ce":"code","db93332e":"code","f9790cb3":"code","f8fdddb6":"code","8d28f9eb":"code","79a45be4":"code","6cccaebb":"code","fbab266e":"code","403a0906":"code","ad8085e8":"code","e66f69a3":"code","d36b60a9":"code","442f3521":"code","d5fca936":"code","de25d35c":"code","b89974c5":"code","7e919f61":"code","845035ba":"code","dca59e25":"code","855af5ab":"code","1b8fc779":"code","181b2f89":"code","8ed6fd58":"code","d63bd09e":"code","72b19bc1":"code","12b3a125":"code","26912f4a":"code","2543e887":"code","1c883a33":"markdown","fc9fabbf":"markdown","9f503770":"markdown","2ed565de":"markdown","91157288":"markdown","514c0142":"markdown","440d00a5":"markdown","518b3cee":"markdown","f7dafd61":"markdown","7426337a":"markdown","f6face6b":"markdown","3b231ec2":"markdown","ba11fc08":"markdown","1f1b88b8":"markdown","69f589b3":"markdown","51ad55b5":"markdown","bc4cd1b2":"markdown","dc830f40":"markdown","f687dda5":"markdown","0bf4188f":"markdown","91bc1f85":"markdown","7343e2b1":"markdown"},"source":{"9cdf3f5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # visualization\n# machine learning\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms, datasets, models\n\n!pip3 install progressbar\nimport progressbar\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","545a7ca5":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","f6bb15fb":"widgets = [\n    ' [', progressbar.Timer(), '] ',\n    progressbar.Percentage(), ' ',\n    progressbar.Bar(),\n    ' (', progressbar.ETA(), ') ',\n]","19dd95b1":"# import mnist dataset\nBATCH_SIZE = 500\n\ntrain_loader = torch.utils.data.DataLoader(\n    torchvision.datasets.MNIST('\/kaggle\/working',\n                               train=True,\n                               download=True,\n                               transform=torchvision.transforms.ToTensor()),\n    batch_size=BATCH_SIZE,shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    torchvision.datasets.MNIST('\/kaggle\/working',\n                               train=False,\n                               download=True,\n                               transform=torchvision.transforms.ToTensor()),\n    batch_size=BATCH_SIZE,shuffle=True)","d86915b6":"class BasicClassifier(nn.Module):\n    def __init__(self,num_classes) -> None:\n        self.num_classes = num_classes\n        self.metric = Metrics()\n        super(BasicClassifier, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1,64,kernel_size=7),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64,128,kernel_size=11),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128,182,kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(182,256,kernel_size=5),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096,2048),\n            nn.ReLU(inplace=True),\n            nn.Linear(2048,1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024,num_classes),\n            nn.Softmax(),            \n        )\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x","65a05631":"def train(model,train_loader,test_loader,datafunc,proportion,optimizer,criterion,NUM_CLASSSES=10,NUM_EPOCHS=25):\n    epoch_metrics = {}\n    epoch_test_metrics = {}\n    bar = progressbar.ProgressBar(NUM_EPOCHS*len(train_loader),widgets=widgets).start()\n    for epoch in range(NUM_EPOCHS):\n        model.metric.reset_confusion_matrix(NUM_CLASSSES)\n        for i, (inputs, labels) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(datafunc(inputs,proportion).to(device))\n            loss = criterion(outputs, labels.to(device))\n            loss.backward()\n            optimizer.step()\n            # statistics\n            model.metric.update_confusion_matrix(outputs.to('cpu'),labels)\n            # progressbar\n            bar.update(epoch*len(train_loader)+i)\n        epoch_metrics[epoch] = model.metric.classification_metrics()\n        \n        with torch.no_grad():\n            model.metric.reset_confusion_matrix(NUM_CLASSSES)\n            for (inputs, labels) in test_loader:\n                outputs = model(datafunc(inputs,proportion).to(device))\n                # statistics\n                model.metric.update_confusion_matrix(outputs.to('cpu'),labels)\n                # progressbar\n            epoch_test_metrics[epoch] = model.metric.classification_metrics()\n            \n    return (epoch_metrics,epoch_test_metrics)\n\ndef train_simple(model,train_loader,test_loader,datafunc,optimizer,criterion,NUM_CLASSSES=10,NUM_EPOCHS=25):\n    epoch_metrics = {}\n    epoch_test_metrics = {}\n    bar = progressbar.ProgressBar(NUM_EPOCHS*len(train_loader),widgets=widgets).start()\n    for epoch in range(NUM_EPOCHS):\n        model.metric.reset_confusion_matrix(NUM_CLASSSES)\n        for i, (inputs, labels) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(datafunc(inputs).to(device))\n            loss = criterion(outputs, labels.to(device))\n            loss.backward()\n            optimizer.step()\n            # statistics\n            model.metric.update_confusion_matrix(outputs.to('cpu'),labels)\n            # progressbar\n            bar.update(epoch*len(train_loader)+i)\n        epoch_metrics[epoch] = model.metric.classification_metrics()\n        \n        with torch.no_grad():\n            model.metric.reset_confusion_matrix(NUM_CLASSSES)\n            for (inputs, labels) in test_loader:\n                outputs = model(datafunc(inputs).to(device))\n                # statistics\n                model.metric.update_confusion_matrix(outputs.to('cpu'),labels)\n                # progressbar\n            epoch_test_metrics[epoch] = model.metric.classification_metrics()\n    return (epoch_metrics,epoch_test_metrics)\n    \ndef display_training_metrics(name,epoch_metrics):\n    sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,1]) # precision\n    sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,2]) # recall\n    sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,0],) # accuracy\n    plt.xlabel('epoch')\n    plt.title(name)\n    \ndef display_testing_metrics(name,epoch_metrics):\n    sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,1]) # precision\n    sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,2]) # recall\n    sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,0],) # accuracy\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.xlabel('% loss')\n    plt.title(name)\n    \ndef display_testing_metrics_hist(name,metrics):\n    sns.barplot(x=['accuracy','precision','recall'],y=metrics[0:3])\n    plt.ylim([0, 1])\n    plt.title(name)","b7058874":"class Metrics:\n    def __init__(self):\n        pass\n    \n    def update_confusion_matrix(self, outputs: torch.Tensor, labels: torch.Tensor) -> None:       \n        for (guess,label) in zip(outputs.argmax(1),labels):\n            self.confusion_matrix[guess,label] += 1\n            \n    def get_confusion_matrix(self,norm=False) -> np.array:\n        if norm:\n            return self.confusion_matrix \/ self.confusion_matrix.sum()\n        return self.confusion_matrix\n    \n    def reset_confusion_matrix(self,num_classes: int) -> None:\n        self.confusion_matrix = np.zeros((num_classes,num_classes))\n    \n    def classification_metrics(self,print_=False) -> tuple:\n        stats = [self.accuracy(),self.precision(),self.recall(),self.f1_score(),self.confusion_matrix.sum()]\n        s = \"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nf1-score: {}\\nSupport: {}\".format(stats[0],stats[1],stats[2],stats[3],stats[4])\n        if print_:\n            print(s)\n        return stats\n    \n    def accuracy(self) -> float:\n        stat = np.diag(self.confusion_matrix).sum() \/ self.confusion_matrix.sum()\n        if np.isnan(stat) or np.isinf(stat):\n            return 0.0\n        else:\n            return stat\n        \n    def precision(self) -> float:\n        stat = np.nanmean(np.diag(self.confusion_matrix) \/ np.sum(self.confusion_matrix, axis = 0))\n        if np.isnan(stat) or np.isinf(stat):\n            return 0.0\n        else:\n            return stat\n        return stat\n        \n    def recall(self) -> float:\n        stat = np.nanmean(np.diag(self.confusion_matrix) \/ np.sum(self.confusion_matrix, axis = 1))\n        if np.isnan(stat) or np.isinf(stat):\n            return 0.0\n        return stat\n    \n    def f1_score(self) -> float:\n        p = self.precision()\n        r = self.recall()\n        if p+r < 1:\n            return 0.0\n        stat = (2.0*p*r)\/(p+r)\n        if np.isnan(stat):\n            return 0.0\n        else:\n            return stat\n    \n    def feature_map(self,inputs: torch.Tensor,model,print_=False):\n        no_of_layers=0\n        conv_layers=[]\n\n        model_children=list(model.children())\n\n        for child in model_children:\n            if type(child)==nn.Conv2d:\n                conv_layers.append(child)\n            elif type(child) == nn.Sequential:\n                for layer in child.children():\n                    if type(layer) == nn.Conv2d:\n                        conv_layers.append(layer)\n#         (inputs,labels) = next(iter(test_loader))\n        results = [conv_layers[0](inputs.to(device))]\n        for i in range(1, len(conv_layers)):\n            results.append(conv_layers[i](results[-1]))\n        outputs = results\n        if print_:\n            plt.imshow(inputs[0,0].to('cpu'),cmap='Greys')\n            plt.show()\n            for num_layer in range(len(outputs)):\n                plt.figure(figsize=(50, 10))\n                layer_viz = outputs[num_layer][0, :, :, :]\n                layer_viz = layer_viz.data\n#                 print(\"Layer \",num_layer+1)\n                for i, filter in enumerate(layer_viz.to('cpu')):\n                    if i == 8: \n                        break\n                    plt.subplot(2, 8, i + 1)\n                    plt.imshow(filter, cmap='gray')\n                    plt.axis(\"off\")\n                plt.show()\n                plt.close()\n        return results\n    \n    def feature_map_diff(map1,map1_loss) -> np.array:\n        pass\nmetric = Metrics()","98bc79ce":"class DataLoss:\n    def __init__(self):\n        pass\n    def unaltered(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        return inputs\n    \n    def random_per_pixel(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        # ppp: proportion per pixel\n        lossyinputs = torch.clone(inputs)\n        mask = torch.Tensor(np.random.rand(inputs.shape[0],inputs.shape[1],inputs.shape[2],inputs.shape[3]))\n        lossyinputs = lossyinputs * (mask > ppp)\n        return lossyinputs.type(torch.FloatTensor)\n    \n    def random_per_img(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        if int(inputs.shape[2]*inputs.shape[3]*ppp) < 1:\n            return inputs\n        if ppp > 1.0:\n            ppp = 1.0\n        num_loss = int(inputs.shape[2]*inputs.shape[3]*ppp)\n        lossyinputs = torch.clone(inputs)\n        mask = np.concatenate((np.zeros((inputs.shape[0],num_loss)),np.ones((inputs.shape[0],int(inputs.shape[2]*inputs.shape[3] - num_loss)))),axis=1)\n        for i in range(inputs.shape[0]):\n            np.random.shuffle(mask[i])\n        mask = torch.Tensor(mask.reshape((inputs.shape[0],inputs.shape[1],inputs.shape[2],inputs.shape[3])))\n        lossyinputs *= mask\n        return lossyinputs.type(torch.FloatTensor)\n    \n    def columns_per_img(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        num_col = int(inputs.shape[3]*ppp)\n        to_rem = np.random.choice([x for x in range(inputs.shape[2])],size=num_col,replace=False)\n        mask = np.ones((inputs.shape[0],inputs.shape[1],inputs.shape[2],inputs.shape[3]))\n        for col in to_rem:\n            mask[:,:,:,col] = 0\n        lossyinputs = torch.clone(inputs)*mask\n        return lossyinputs.type(torch.FloatTensor)\n    \n    def rows_per_img(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        num_row = int(inputs.shape[2]*ppp)\n        to_rem = np.random.choice([x for x in range(inputs.shape[2])],size=num_row,replace=False)\n        mask = np.ones((inputs.shape[0],inputs.shape[1],inputs.shape[2],inputs.shape[3]))\n        for row in to_rem:\n            mask[:,:,row,:] = 0\n        lossyinputs = torch.clone(inputs)*mask\n        return lossyinputs.type(torch.FloatTensor)\n    \n    def rowcol_per_img(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        lossyinputs = torch.clone(inputs)\n        lossyinputs = self.rows_per_img(lossyinputs,ppp*.95)\n        lossyinputs = self.columns_per_img(lossyinputs,ppp*.95)\n        return lossyinputs.type(torch.FloatTensor)\n    \n    def pattern_column(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        lossyinputs = torch.clone(inputs)\n        for i in range(0,lossyinputs.shape[3],2):\n            lossyinputs[:,:,:,i] = 0\n        return lossyinputs\n    \n    def pattern_row(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        lossyinputs = torch.clone(inputs)\n        for i in range(0,lossyinputs.shape[2],2):\n            lossyinputs[:,:,i,:] = 0\n        return lossyinputs\n    \n    def pattern_checkerboard(self,inputs: torch.Tensor,ppp: float=0.0) -> torch.Tensor:\n        lossyinputs = torch.clone(inputs)\n        mask = np.zeros(inputs.shape)\n        for i in range(inputs.shape[2]):\n            for j in range(inputs.shape[3]):\n                if (i*inputs.shape[3] + j + i%2) % 2 == 0:\n                    mask[:,:,i,j] = 1\n        lossyinputs = torch.Tensor(mask)*lossyinputs\n        return lossyinputs\n    \ndloss = DataLoss()","db93332e":"inputs = next(iter(train_loader))[0]\nplt.imshow(inputs[0][0],cmap='Greys')\nplt.title('raw')\nplt.show()\nplt.imshow(dloss.random_per_pixel(inputs,0.3)[0][0],cmap='Greys')\nplt.title('random_per_pixel')\nplt.show()\nplt.imshow(dloss.random_per_img(inputs,0.3)[0][0],cmap='Greys')\nplt.title('random_per_img')\nplt.show()\nplt.imshow(dloss.columns_per_img(inputs,0.3)[0][0],cmap='Greys')\nplt.title('columns_per_img')\nplt.show()\nplt.imshow(dloss.rows_per_img(inputs,0.3)[0][0],cmap='Greys')\nplt.title('rows_per_img')\nplt.show()\nplt.imshow(dloss.rowcol_per_img(inputs,0.3)[0][0],cmap='Greys')\nplt.title('rowcol_per_img')\nplt.show()\n\nplt.imshow(dloss.pattern_row(inputs,0.3)[0][0],cmap='Greys')\nplt.title('pattern_row')\nplt.show()\nplt.imshow(dloss.pattern_column(inputs,0.3)[0][0],cmap='Greys')\nplt.title('pattern_column')\nplt.show()\nplt.imshow(dloss.pattern_checkerboard(inputs,0.3)[0][0],cmap='Greys')\nplt.title('pattern_checkerboard')\nplt.show()","f9790cb3":"# model_unaltered = BasicClassifier(10)\n# model_unaltered.to(device)\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model_unaltered.parameters(),lr=3e-5)","f8fdddb6":"# training\n# find best number of epochs\n# NUM_EPOCHS = 20\n# epoch_metrics = {}\n# bar = progressbar.ProgressBar(NUM_EPOCHS*len(train_loader),widgets=widgets).start()\n# for epoch in range(NUM_EPOCHS):\n#     metric.reset_confusion_matrix(10)\n#     for i, (inputs, labels) in enumerate(train_loader):\n#         optimizer.zero_grad()\n#         outputs = model_unaltered(inputs.to(device))\n#         loss = criterion(outputs, labels.to(device))\n#         loss.backward()\n#         optimizer.step()\n#         # statistics\n#         metric.update_confusion_matrix(outputs.to('cpu'),labels)\n#         # progressbar\n#         bar.update(epoch*len(train_loader)+i)\n#     epoch_metrics[epoch] = metric.classification_metrics()","8d28f9eb":"# leg = ['accuracy','precision','recall']\n# sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,0],) # accuracy\n# sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,1]) # precision\n# sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,2]) # recall\n# plt.legend(leg)\n# plt.xlabel('epoch')\n# plt.title('Training Unaltered Model on Unaltered Data')\n# plt.show()","79a45be4":"# testing\n# metric.reset_confusion_matrix(10)\n# bar = progressbar.ProgressBar(len(test_loader),widgets=widgets).start()\n# with torch.no_grad():\n#     running_loss = 0.0\n#     for i, (inputs, labels) in enumerate(test_loader):\n#         outputs = model_unaltered(inputs.to(device))\n#         # statistics\n#         metric.update_confusion_matrix(outputs.to('cpu'),labels)\n#         # progressbar\n#         bar.update(i)\n# m = metric.get_confusion_matrix(norm=True)\n# sns.heatmap(m,square=True,cmap='Greys')\n# plt.title('Testing Unaltered Model on Unaltered Data')\n# plt.show()\n# metric.classification_metrics(print_=True)","6cccaebb":"# loss_stats = {}\n# percents = [x\/10.0 for x in range(0,10)] + [0.95] # input parameters to DataLoss::random_per_img()\n# bar = progressbar.ProgressBar(len(percents),widgets=widgets).start()\n# for cc, ppp in enumerate(percents):\n#     metric.reset_confusion_matrix(10)\n#     with torch.no_grad():\n#         running_loss = 0.0\n#         for i, (inputs, labels) in enumerate(test_loader):\n#             outputs = model_unaltered(dloss.random_per_img(inputs,ppp).to(device)) # apply data loss\n#             # statistics\n#             metric.update_confusion_matrix(outputs.to('cpu'),labels)\n#             # progressbar\n#     bar.update(cc)\n#     loss_stats[ppp] = metric.classification_metrics()\n\n# leg = ['accuracy','precision','recall']\n# sns.lineplot(x=list(loss_stats.keys()),y=np.array(list(loss_stats.values()),dtype=float)[:,0],) # accuracy\n# sns.lineplot(x=list(loss_stats.keys()),y=np.array(list(loss_stats.values()),dtype=float)[:,1]) # precision\n# sns.lineplot(x=list(loss_stats.keys()),y=np.array(list(loss_stats.values()),dtype=float)[:,2]) # recall\n# plt.legend(leg)\n# plt.xlabel('% loss')\n# plt.title('Testing Unaltered Model on Lossy Data(Proportion of Pixels per Image)')\n# plt.show()","fbab266e":"# (inputs,labels) = next(iter(test_loader))\n# res_full = metric.feature_map(inputs[0:1],model,print_=True)\n# print(model(inputs[0:1].to(device)).to('cpu').argmax())","403a0906":"# (inputs,labels) = next(iter(test_loader))\n# loss_inputs = dloss.random_per_img(inputs[0:1],.75)\n# res_dloss = metric.feature_map(loss_inputs,model,print_=True)\n# print(model(loss_inputs.to(device)).to('cpu').argmax())","ad8085e8":"# model_lossy_1 = BasicClassifier(10)\n# model_lossy_1.to(device)\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model_lossy_1.parameters(),lr=3e-5)","e66f69a3":"# training\n# find best number of epochs\n# NUM_EPOCHS = 20\n# epoch_metrics = {}\n# ppp = 0.5 # could also make ppp a function of the epoch number\n# bar = progressbar.ProgressBar(NUM_EPOCHS*len(train_loader),widgets=widgets).start()\n# for epoch in range(NUM_EPOCHS):\n#     running_loss = 0.0\n#     for i, (inputs, labels) in enumerate(train_loader):\n#         optimizer.zero_grad()\n#         outputs = model_lossy_1(dloss.random_per_img(inputs,ppp).to(device))\n#         loss = criterion(outputs, labels.to(device))\n#         loss.backward()\n#         optimizer.step()\n#         # statistics\n#         metric.update_confusion_matrix(outputs.to('cpu'),labels)\n#         # progressbar\n#         bar.update(epoch*len(train_loader)+i)\n#     epoch_metrics[epoch] = metric.classification_metrics()","d36b60a9":"# leg = ['accuracy','precision','recall']\n# sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,0],) # accuracy\n# sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,1]) # precision\n# sns.lineplot(x=list(epoch_metrics.keys()),y=np.array(list(epoch_metrics.values()),dtype=float)[:,2]) # recall\n# plt.legend(leg)\n# plt.xlabel('epoch')\n# plt.title('Training Lossy Model on Lossy Data(Proportion of Pixels per Image)')\n# plt.show()","442f3521":"# metric.reset_confusion_matrix(10)\n# bar = progressbar.ProgressBar(len(test_loader),widgets=widgets).start()\n# with torch.no_grad():\n#     for i, (inputs, labels) in enumerate(test_loader):\n#         outputs = model_lossy_1(inputs.to(device))\n#         # statistics\n#         metric.update_confusion_matrix(outputs.to('cpu'),labels)\n#         # progressbar\n#         bar.update(i)\n# m = metric.get_confusion_matrix(norm=True)\n# sns.heatmap(m,square=True,cmap='Greys')\n# plt.title('Testing Lossy Model on Unaltered Data')\n# plt.show()\n# metric.classification_metrics(print_=True)","d5fca936":"# loss_stats = {}\n# percents = [x\/10.0 for x in range(10)] + [0.95,0.99] # input parameters to DataLoss::random_ppp()\n# bar = progressbar.ProgressBar(len(percents),widgets=widgets).start()\n# for cc, ppp in enumerate(percents):\n#     metric.reset_confusion_matrix(10)\n#     with torch.no_grad():\n#         running_loss = 0.0\n#         for i, (inputs, labels) in enumerate(test_loader):\n#             outputs = model_lossy_1(dloss.random_per_img(inputs,ppp).to(device)) # apply data loss\n#             # statistics\n#             metric.update_confusion_matrix(outputs.to('cpu'),labels)\n#             # progressbar\n#     bar.update(cc)\n#     loss_stats[ppp] = metric.classification_metrics()\n\n# leg = ['accuracy','precision','recall']\n# sns.lineplot(x=list(loss_stats.keys()),y=np.array(list(loss_stats.values()),dtype=float)[:,0],) # accuracy\n# sns.lineplot(x=list(loss_stats.keys()),y=np.array(list(loss_stats.values()),dtype=float)[:,1]) # precision\n# sns.lineplot(x=list(loss_stats.keys()),y=np.array(list(loss_stats.values()),dtype=float)[:,2]) # recall\n# plt.legend(leg)\n# plt.xlabel('% loss')\n# plt.title('Testing Lossy Model on Lossy Data(Proportion of Pixels per Image)')\n# plt.show()","de25d35c":"# class_frequency = {x:0 for x in range(10)}\n# freq_pop = 10000\n# inputs = torch.Tensor(np.random.rand(freq_pop,1,28,28)).to(device)\n# outputs = model_unaltered(inputs)\n# print(\"mean:\",outputs.argmax(1).to('cpu').numpy().mean())\n# print(\"std:\",outputs.argmax(1).to('cpu').numpy().std())\n# print(\"var:\",outputs.argmax(1).to('cpu').numpy().var())","b89974c5":"# class_frequency = {x:0 for x in range(10)}\n# freq_pop = 10000\n# inputs = torch.Tensor(np.random.rand(freq_pop,1,28,28)).to(device)\n# outputs = model_lossy_1(inputs)\n# print(\"mean:\",outputs.argmax(1).to('cpu').numpy().mean())\n# print(\"std:\",outputs.argmax(1).to('cpu').numpy().std())\n# print(\"var:\",outputs.argmax(1).to('cpu').numpy().var())","7e919f61":"model_metrics = []\nmodel_test_metrics = []\nmodel_list = []\nmodel_unaltered = BasicClassifier(10)\nmodel_unaltered.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_unaltered.parameters(),lr=3e-5)\n(train_metrics,test_metrics) = train(model_unaltered,train_loader,test_loader,getattr(dloss, 'unaltered'),0.0,optimizer,criterion)\nmodel_metrics.append(('Unaltered',train_metrics))\nmodel_test_metrics.append(('Unaltered',test_metrics))\nmodel_list.append(('Unaltered',model_unaltered))","845035ba":"func_list = [('rPixel',getattr(dloss, 'random_per_img')),('rRow',getattr(dloss, 'rows_per_img')),('rColumn',getattr(dloss, 'columns_per_img')),('rRow and rColumn',getattr(dloss, 'rowcol_per_img'))]","dca59e25":"proportion = 0.6\nbar = progressbar.ProgressBar(len(func_list),widgets=widgets).start()\nfor cc, (name,func) in enumerate(func_list):\n    model = BasicClassifier(10)\n    model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(),lr=3e-5)\n    (train_metrics,test_metrics) = train(model,train_loader,test_loader,func,proportion,optimizer,criterion)\n    model_metrics.append((name,train_metrics))\n    model_test_metrics.append((name,test_metrics))\n    model_list.append((name,model))\n    bar.update(cc)","855af5ab":"fig, axarr = plt.subplots(1,len(model_metrics),figsize=(30,5))\nfor i, (name,metrics) in enumerate(model_metrics):\n        plt.sca(axarr[i])\n        display_training_metrics(name,metrics)\nleg = ['precision','recall','accuracy']\nplt.legend(leg)\nplt.suptitle('Training')\nplt.show()\nfig, axarr = plt.subplots(1,len(model_metrics),figsize=(30,5))\nfor i, (name,metrics) in enumerate(model_test_metrics):\n        plt.sca(axarr[i])\n        display_training_metrics(name,metrics)\nleg = ['precision','recall','accuracy']\nplt.legend(leg)\nplt.suptitle('Testing during Training')\nplt.show()","1b8fc779":"# model_list = [('Unaltered',model_1),('Random Per Image',model_2)]#,('Checkered',None),('Column',None),('Row',None)] # list of all the trained models\n# func_list = [('Unaltered',getattr(dloss, 'unaltered')),('Random Per Image',getattr(dloss, 'random_per_img'))]\npercents = [x\/10.0 for x in range(11)] # input parameters to DataLoss\ncrosstab = {name:{func_name:{percent:[] for percent in percents} for (func_name,_) in func_list} for (name,_) in model_list} # each model has a metric for each \n\n# testing loop\n\nbar = progressbar.ProgressBar(len(func_list)*len(percents),widgets=widgets).start()\nfor cc,(func_name,func) in enumerate(func_list): # number of data loss functions including unaltered\n    for pl, ppp in enumerate(percents):\n\n        for (name,model) in model_list:\n            model.metric.reset_confusion_matrix(10)\n\n        with torch.no_grad():\n            for i, (inputs, labels) in enumerate(test_loader):\n                inputs = func(inputs=inputs,ppp=ppp)\n                for (model_name,model) in model_list:\n                    outputs = model(inputs.to(device))\n                    # statistics\n                    model.metric.update_confusion_matrix(outputs.to('cpu'),labels)\n                    # progressbar\n            for (model_name,model) in model_list:\n                crosstab[model_name][func_name][ppp] = model.metric.classification_metrics()\n            bar.update(cc*len(percents) + pl)","181b2f89":"fig, axarr = plt.subplots(len(crosstab.keys()), len(crosstab[list(crosstab.keys())[0]].keys()),figsize=(30,25))\nfor i, model in enumerate(crosstab.keys()):\n    for j, func in enumerate(crosstab[model].keys()):\n        plt.sca(axarr[i, j])#\n        display_testing_metrics(\"{} on set {}\".format(model,func),crosstab[model][func])\nleg = ['precision','recall','accuracy']\nplt.legend(leg)\n#     plt.setp(axarr[0, i], xlabel=model)\n#     plt.setp(axarr[i, 0], ylabel=model)\n","8ed6fd58":"func_list_1d = [('pCheckerboard',getattr(dloss,'pattern_checkerboard')),('pColumn',getattr(dloss,'pattern_column')),('pRow',getattr(dloss,'pattern_row'))]","d63bd09e":"model_list_1d = []\nmodel_metrics_1d = []\nmodel_metrics_1d_test = []\nbar = progressbar.ProgressBar(len(func_list_1d),widgets=widgets).start()\nfor cc, (name,func) in enumerate(func_list_1d):\n    model = BasicClassifier(10)\n    model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(),lr=3e-5)\n    (train_metrics,test_metrics) = train_simple(model,train_loader,test_loader,func,optimizer,criterion)\n    model_metrics_1d.append((name,train_metrics))\n    model_metrics_1d_test.append((name,test_metrics))\n    model_list_1d.append((name,model))\n    bar.update(cc)","72b19bc1":"fig, axarr = plt.subplots(1,len(model_metrics_1d),figsize=(30,5))\nfor i, (name,metrics) in enumerate(model_metrics_1d):\n        plt.sca(axarr[i])\n        display_training_metrics(name,metrics)\nleg = ['precision','recall','accuracy']\nplt.legend(leg)\nplt.suptitle('Training')\nplt.show()\nfig, axarr = plt.subplots(1,len(model_metrics_1d_test),figsize=(30,5))\nfor i, (name,metrics) in enumerate(model_metrics_1d_test):\n        plt.sca(axarr[i])\n        display_training_metrics(name,metrics)\nleg = ['precision','recall','accuracy']\nplt.legend(leg)\nplt.suptitle('Testing during Training')\nplt.show()","12b3a125":"complete_func_list = [('Unaltered',getattr(dloss,'unaltered'))] + func_list + func_list_1d\ncomplete_model_list = model_list + model_list_1d\ncrosstab2 = {mname:{fname:{} for (fname,func) in complete_func_list} for (mname,model) in complete_model_list}\n\nproportion = 0.60 # increase data loss on test to see limitations better\n\nbar = progressbar.ProgressBar(len(complete_model_list)*len(complete_func_list),widgets=widgets).start()\nfor cc, (model_name,model) in enumerate(complete_model_list):\n    for dd, (func_name,func) in enumerate(complete_func_list):\n        \n        for (name,model) in complete_model_list:\n            model.metric.reset_confusion_matrix(10)\n\n        with torch.no_grad():\n            for i, (inputs, labels) in enumerate(test_loader):\n                inputs = func(inputs=inputs,ppp=proportion) # by default same as training\n                for (model_name,model) in complete_model_list:\n                    outputs = model(inputs.to(device))\n                    # statistics\n                    model.metric.update_confusion_matrix(outputs.to('cpu'),labels)\n                    # progressbar\n            for (model_name,model) in complete_model_list:\n                crosstab2[model_name][func_name] = model.metric.classification_metrics() # we just want the metrics for a single proportion\n        bar.update(cc*len(complete_func_list) + dd)","26912f4a":"fig, axarr = plt.subplots(len(crosstab2.keys()), len(crosstab2[list(crosstab2.keys())[0]].keys()),figsize=(30,40))\nfor i, model in enumerate(crosstab2.keys()):\n    for j, func in enumerate(crosstab2[model].keys()):\n        plt.sca(axarr[i, j]) \n        display_testing_metrics_hist(\"{} on set {}\".format(model,func),crosstab2[model][func])\n# plt.suptitle('Crosstab Testing w 50% loss')\nplt.show()","2543e887":"for (name,model) in complete_model_list:\n    torch.save(model.state_dict(),'.\/{}.h5'.format(name))","1c883a33":"# Model trained on unaltered data","fc9fabbf":"# Model Trainer","9f503770":"# Creating Model for Each Function","2ed565de":"## Setup Environment","91157288":"## Load MNIST Dataset","514c0142":"# Image Data Loss Exploratory Analysis","440d00a5":"## Train model_lossy_1 on Lossy Data : loss = 50%","518b3cee":"# Feature Map\nhttps:\/\/androidkt.com\/how-to-visualize-feature-maps-in-convolutional-neural-networks-using-pytorch\/","f7dafd61":"## Test model_lossy_1 on Lossy Data","7426337a":"## Feature Map without DataLoss","f6face6b":"# Model Trained on Lossy Data","3b231ec2":"## Train model_unaltered on Unaltered Data","ba11fc08":"## Test model_unaltered on Unaltered Data","1f1b88b8":"## Metrics Class","69f589b3":"## Test model_lossy_1 on Unaltered Data","51ad55b5":"## Feature Map with DataLoss","bc4cd1b2":"### DataLoss::random_per_img()","dc830f40":"## Test model_unaltered on Lossy Data","f687dda5":"# Cross Table Of Data Loss Models Tested on each others Data sets","0bf4188f":"# Data Loss Class\n\n\n","91bc1f85":"## MNIST Classifier \/ Baseline Model","7343e2b1":"# Model Biases"}}