{"cell_type":{"0e7ae0c8":"code","295212e4":"code","f9d7a7ae":"code","45c368ab":"code","9c3c6f4e":"code","d5a1a41c":"code","d3c486d5":"code","e5749974":"code","aa2c8670":"code","349fd467":"code","11368da0":"code","0873f5c0":"code","317f8647":"code","c3c93e3d":"code","64918781":"code","75074913":"code","b45ec4f7":"code","9ba122a0":"code","54904709":"code","9eaaddac":"code","0bd6b8bf":"code","84fe3e33":"code","6a04982d":"code","2b32d537":"code","36f83180":"code","32497646":"code","9742c89d":"code","86b4d218":"code","dc5d8a00":"code","6190bf46":"code","a5ccd9ab":"code","16cfbf01":"code","e16eac08":"code","dc0621cd":"code","b340705c":"code","944e3f19":"code","3e0469bf":"code","30f85793":"code","ac22d82c":"code","cf0aa558":"code","cfbc75b7":"code","8f08ff95":"code","a62b9ac9":"code","122c56f8":"code","62839561":"code","dc855c95":"markdown","ed80f19f":"markdown","721fd3b8":"markdown","559ff609":"markdown","d03c2b18":"markdown","227c4907":"markdown","7e04217d":"markdown","1671dbea":"markdown","3509b317":"markdown","4baa10d9":"markdown","1074099f":"markdown","9107a345":"markdown","5cc46a12":"markdown"},"source":{"0e7ae0c8":"## Load libraries\n\nimport re\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom textblob import TextBlob\n\nfrom wordcloud import WordCloud\nfrom PIL import Image # To mask the wordcloud, I have to import PIL - Pillow library","295212e4":"## Set Default Settings\n\nsns.set_style('darkgrid')\nnlp = spacy.load('en_core_web_sm')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)","f9d7a7ae":"## Load CSV file\ndf_tweets = pd.read_csv('..\/input\/Trump_Obama_Tweets.csv')\n\n# Display the first few rows to understand your data\ndf_tweets.head()","45c368ab":"# Features types\ndf_tweets.dtypes","9c3c6f4e":"# Check null values\ndf_tweets.isnull().sum()","d5a1a41c":"# To avoid displaying scientific numbers like '9.500000e+01'\npd.options.display.float_format = '{:20,.2f}'.format","d3c486d5":"# Default method will display numeric features only\ndf_tweets.describe().round(2).T","e5749974":"# Using include=['O'] to display categorical features\ndf_tweets.describe(include=['O']).T","aa2c8670":"# Tweets shares\ndf_tweets.user.value_counts(normalize = True).round(3) * 100 ","349fd467":"# Remove URLs from the tweets\ndef re_remove_url(x):\n    return re.sub(r'http\\S+', '', x)\n\n#Extracts hashtags from tweets\ndef extract_hashtags(x):\n    try:\n        hashtags = re.findall(r\"#(\\w+)\", x) # Extract hashtags\n        if not hashtags:\n            return np.nan\n        elif isinstance(hashtags, list): # Check if it's a list object\n            return ', '.join(hashtags) # Convert the list to str object\n        else:\n            return hashtags\n    except:\n        return np.nan\n     \n#Extracts mentions from tweets\ndef extract_mentions(x):\n    try:\n        mentions = re.findall(r\"@(\\w+)\", x) # Extract mentions\n        if not mentions:\n            return np.nan\n        if isinstance(mentions, list):\n            return ', '.join(mentions)\n        else:\n            return mentions\n    except:\n        return np.nan\n\n# Add exctracted data in new columns\ndf_tweets['tweets']   = df_tweets.text.apply(lambda x: re_remove_url(x))\ndf_tweets['hashtags'] = df_tweets.text.apply(lambda x: extract_hashtags(x))\ndf_tweets['mentions'] = df_tweets.text.apply(lambda x: extract_mentions(x))\n\n# Drop unwanted columns\ndf_tweets.drop(['text'], axis = 1, inplace = True)","11368da0":"# Extract additional information about the tweets\ntweets_length_list           = []\ntweets_spaces_list           = []\ntweets_uppercase_list        = []\ntweets_punctuations_list     = []\ntweets_questionmark_list     = []\ntweets_exclamation_mark_list = []\n\ndef extract_text_details(x):\n    tweets_length_list.append(len(x))                                                 # Length of the tweet\n    tweets_spaces_list.append(sum([1 for l in x if l.isspace()]))                     # Total number of spaces exists in the tweet\n    tweets_uppercase_list.append(sum([1 for l in x if l.isupper()]))                  # Total number of uppercases used in the tweet\n    tweets_punctuations_list.append(sum([1 for l in x if l in string.punctuation]))   # Total number of punctuation exists in the tweet\n    tweets_questionmark_list.append(x.count('?'))                                     # Total number of question marks in tweet \n    tweets_exclamation_mark_list.append(x.count('!'))                                 # Total number of exclamation marks in tweet\n\n_ = df_tweets.tweets.apply(lambda x: extract_text_details(x)) # Since the function doesnt return values, it returns 'None' by default. Instead of displaying them, I stored them in temp object '_'\ndel _ # Delete _ object\n\ndf_tweets['tweets_length']           = tweets_length_list\ndf_tweets['tweets_spaces']           = tweets_spaces_list\ndf_tweets['tweets_uppercase']        = tweets_uppercase_list\ndf_tweets['tweets_punctuations']     = tweets_punctuations_list\ndf_tweets['tweets_questionmark']     = tweets_questionmark_list\ndf_tweets['tweets_exclamation_mark'] = tweets_exclamation_mark_list","0873f5c0":"# Extract polarity and subjectivity of the tweets \npolarity_list     = []\nsubjectivity_list = []\n\ndef polarity_subjectivity(x):\n    analysis = TextBlob(x)\n    polarity_list.append(round(analysis.polarity, 2))\n    subjectivity_list.append(round(analysis.subjectivity, 2))\n    \n_ = df_tweets.tweets.apply(lambda x: polarity_subjectivity(x))\ndel _\n\ndf_tweets['polarity']     = polarity_list\ndf_tweets['subjectivity'] = subjectivity_list","317f8647":"# Very Positive \/ Positive \/ Very Negative \/ Negative \/ Neutral\ndef polarity_status(x):\n    if x == 0:\n        return 'Neutral'\n    elif x > 0.00 and x < 0.50:\n        return 'Positive'\n    elif x >= 0.50:\n        return 'Very Positive'\n    elif x < 0.00 and x > -0.50:\n        return 'Negative'\n    elif x <= -0.50:\n        return 'Very Negative'\n    else:\n        return 'Unknown'\n\n# Very Positive \/ Positive \/ Very Negative \/ Negative \/ Neutral\ndef subjectivity_status(x):\n    if x == 0:\n        return 'Very Objective'\n    elif x > 0.00 and x < 0.40:\n        return 'Objective'\n    elif x >= 0.40 and x < 0.70:\n        return 'Subjective'\n    elif x >= 0.70:\n        return 'Very Subjective'\n\n# Extract \/ Classify polarity and subjectivity\ndf_tweets['polarity_status'] = df_tweets.polarity.apply(lambda x: polarity_status(x))\ndf_tweets['subjectivity_status'] = df_tweets.subjectivity.apply(lambda x: subjectivity_status(x))","c3c93e3d":"# Positive \/ Negative \/ Neutral numeric\n# Very Positive and Positive are going to be ['is_positive']\nneutral_list  = []\npositive_list = []\nnegative_list = []\n\ndef polarity_status(x):\n    if x == 0:\n        neutral_list.append(1)\n        positive_list.append(0)\n        negative_list.append(0)\n    elif x > 0.00:\n        neutral_list.append(0)\n        positive_list.append(1)\n        negative_list.append(0)\n    elif x < 0.00:\n        neutral_list.append(0)\n        positive_list.append(0)\n        negative_list.append(1)\n    \n_ = df_tweets.polarity.apply(lambda x: polarity_status(x))\ndel _\n\ndf_tweets['is_neutral']  = neutral_list\ndf_tweets['is_positive'] = positive_list\ndf_tweets['is_negative'] = negative_list","64918781":"# Convert [date] feature type to datetime type inorder to manipulate dates and times \ndf_tweets.date = pd.to_datetime(df_tweets.date)","75074913":"# Extract tweeting times [early, morning, noon, evening, midnight]\nearly_list    = []\nmorning_list  = []\nnoon_list     = []\nevening_list  = []\nmidnight_list = []\n\ndef part_of_the_day(x):\n    try:\n        if x >= 5: \n            early_list.append(1)\n            morning_list.append(0)\n            noon_list.append(0)\n            evening_list.append(0)\n            midnight_list.append(0)\n            return 'Early Morning'\n\n        elif x >= 8: \n            early_list.append(0)\n            morning_list.append(1)\n            noon_list.append(0)\n            evening_list.append(0)\n            midnight_list.append(0)\n            return 'Morning'\n\n        elif x >= 12: \n            early_list.append(0)\n            morning_list.append(0)\n            noon_list.append(1)\n            evening_list.append(0)\n            midnight_list.append(0)\n            return 'Afternoon'\n\n        elif x >= 18: \n            early_list.append(0)\n            morning_list.append(0)\n            noon_list.append(0)\n            evening_list.append(1)\n            midnight_list.append(0)\n            return 'Evening'\n\n        elif x >= 0 and x < 5:\n            early_list.append(0)\n            morning_list.append(0)\n            noon_list.append(0)\n            evening_list.append(0)\n            midnight_list.append(1)\n            return 'Mid Night'\n    except:\n        early_list.append(np.nan)\n        morning_list.append(np.nan)\n        noon_list.append(np.nan)\n        evening_list.append(np.nan)\n        midnight_list.append(np.nan)\n        return np.nan\n    \ndf_tweets['part_of_day'] = df_tweets.date.dt.hour.apply(lambda x: part_of_the_day(x))\n\ndf_tweets['is_early']    = early_list\ndf_tweets['is_morning']  = morning_list\ndf_tweets['is_noon']     = noon_list\ndf_tweets['is_evening']  = evening_list\ndf_tweets['is_midnight'] = midnight_list ","b45ec4f7":"is_norp_list    = []  # Nationalities or religious or political groups.\nis_time_list    = []\nis_org_list     = []  # Companies, agencies, institutions, etc.\nis_gpe_list     = []  # Countries, cities, states.\nis_loc_list     = []  # Non-GPE locations, mountain ranges, bodies of water.\nis_product_list = []    \nis_workart_list = []  # Titles of books, songs, etc.\nis_fac_list     = []  # Buildings, airports, highways, bridges, etc.\n\nis_noun_list    = []  # girl, cat, tree, air, beauty\nis_pron_list    = []  # I, you, he, she, myself, themselves, somebody\nis_adv_list     = []  # very, tomorrow, down, where, there\nis_propn_list   = []  # Mary, John, London, NATO, HBO\nis_verb_list    = []   \nis_intj_list    = []  # psst, ouch, bravo, hello\n\ndef extract_tweet_style(x):\n    \n    doc = nlp(x)\n    \n    is_norp_list.append(sum([1 for i in doc.ents if i.label_ == 'NORP']))\n    is_time_list.append(sum([1 for i in doc.ents if i.label_ == 'TIME']))\n    is_org_list.append(sum([1 for i in doc.ents if i.label_ == 'ORG']))\n    is_gpe_list.append(sum([1 for i in doc.ents if i.label_ == 'GPE']))\n    is_loc_list.append(sum([1 for i in doc.ents if i.label_ == 'LOC']))\n    is_product_list.append(sum([1 for i in doc.ents if i.label_ == 'PRODUCT']))\n    is_workart_list.append(sum([1 for i in doc.ents if i.label_ == 'WORK_OF_ART']))\n    is_fac_list.append(sum([1 for i in doc.ents if i.label_ == 'FAC']))\n\n    is_noun_list.append((sum([1 for i in doc if i.pos_ == 'NOUN'])))\n    is_pron_list.append((sum([1 for i in doc if i.pos_ == 'PRON'])))\n    is_adv_list.append((sum([1 for i in doc if i.pos_ == 'ADV'])))\n    is_propn_list.append((sum([1 for i in doc if i.pos_ == 'PROPN'])))\n    is_verb_list.append((sum([1 for i in doc if i.pos_ == 'VERB'])))\n    is_intj_list.append((sum([1 for i in doc if i.pos_ == 'INTJ'])))\n\n\n_ = df_tweets.tweets.apply(lambda x: extract_tweet_style(x))\ndel _\n                   \ndf_tweets['is_norp']     = is_norp_list\ndf_tweets['is_time']     = is_time_list\ndf_tweets['is_org']      = is_org_list\ndf_tweets['is_gpe']      = is_gpe_list\ndf_tweets['is_loc']      = is_loc_list\ndf_tweets['is_product']  = is_product_list\ndf_tweets['is_workart']  = is_workart_list\ndf_tweets['is_fac']      = is_fac_list\n\ndf_tweets['is_noun']     = is_noun_list\ndf_tweets['is_pron']     = is_pron_list\ndf_tweets['is_adv']      = is_adv_list\ndf_tweets['is_propn']    = is_propn_list\ndf_tweets['is_verb']     = is_verb_list\ndf_tweets['is_intj']     = is_intj_list","9ba122a0":"df_tweets.groupby(['user']).agg({'is_norp': 'sum',\n                                 'is_time': 'sum',\n                                 'is_org': 'sum',\n                                 'is_gpe': 'sum',\n                                 'is_loc': 'sum',\n                                 'is_product': 'sum',\n                                 'is_workart': 'sum',\n                                 'is_fac': 'sum',\n                                 'is_noun': 'sum',\n                                 'is_pron': 'sum',\n                                 'is_adv': 'sum',\n                                 'is_propn': 'sum',\n                                 'is_verb': 'sum',\n                                 'is_intj': 'sum'})","54904709":"# Extract months, week days, days \ndf_tweets['month'] = df_tweets.date.dt.month\ndf_tweets['day'] = df_tweets.date.dt.day\ndf_tweets['week_day'] = df_tweets.date.dt.weekday # Weekday as number\ndf_tweets['week_day_name'] = df_tweets.date.dt.weekday_name # Weekday as text\ndf_tweets['hour'] = df_tweets.date.dt.hour","9eaaddac":"# Create Dictionaries\nround1_cols      = ['user', 'week_day_name', 'part_of_day', 'polarity_status', 'subjectivity_status']\nround1_titles    = ['Twitter Profile', 'Week Day', 'Part of The Day', 'Polarity', 'Subjectivity']\nround1_cols_dict = dict(zip(range(0, len(round1_cols)), round1_cols))\n\n# Create Pie Plot\nfig, ax = plt.subplots(ncols = 5, figsize= (40,8))\n\nfor indx, col in round1_cols_dict.items():\n    \n    legend_list = df_tweets[col].value_counts(normalize = True).sort_index(ascending=True).keys().tolist()\n    x = df_tweets[col].value_counts(normalize = True).sort_index(ascending=True)\n    \n    ax[indx].pie(data = df_tweets, x = x, autopct='%1.1f%%', textprops = {'fontsize': 11, 'color': 'w', 'weight': 'bold'})\n    ax[indx].add_patch(plt.Circle((0,0), 0.35, fc = 'white'))\n    \n    ax[indx].legend(legend_list, loc = 2)\n    ax[indx].set_title(round1_titles[indx], size = 15)\n\nplt.show()","0bd6b8bf":"grouped = df_tweets.groupby([df_tweets.week_day_name, df_tweets.user]).size().reset_index().rename(columns = {0: 'counts'})\ngrouped","84fe3e33":"# Annotation function\ndef annotate_perct(ax_plot, total, add_height, rot):\n    '''\n    Definition - \n    \n    Parameters - \n        1. ax_plot: is the graph object\n        2. total: is the length of the dataframe or the sum of specific column, the use of this parameter depends on the objective of the graph\n        3. add_height: the additional hight added to the actual hight in order to display the annotation on top of the bar. \n        4. rot: whether to display annotation with angles by passing [i.e. 75 \/ 85\/ 90] or horizontal as it is (the default, which is 0) \n        \n    Additional Explaination - \n        Once the hight of each bar is extracted, first I check if it's null (incase theres no values for specific cases) to assign 0 for the hight otherwise, I just add the extra hight provided (if any)        \n    '''\n    for p in ax_plot.patches:\n        if np.isnan(p.get_height()): \n            height = 0\n            ax_plot.text(p.get_x() + p.get_width()\/2., height, '', ha=\"center\", va='center', fontsize=10, rotation = rot)  \n        else:\n            height = p.get_height()\n            ax_plot.text(p.get_x() + p.get_width()\/2., height + add_height, '{}  ( {}% )'.format(int(height), round((round(height \/ total, 3) * 100), 1)), ha=\"center\", va='center', fontsize=10, rotation = rot)","6a04982d":"temp = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\nsns.set_style('whitegrid')\n\nplt.figure(figsize = (20,9))\n\n_ = sns.barplot(data = grouped, x = 'week_day_name', y = 'counts', hue = 'user', order = temp)\nannotate_perct(ax_plot = _, add_height = 0.7, total = grouped.counts.sum(), rot= 0)\n\n_.set_title('', pad = 40, weight= 'bold', size = 15)\n_.set_xlabel('', weight= 'bold')\n_.set_xticklabels(temp, rotation = 0,  weight= 'bold', fontsize = 15)\n_.set_ylabel('Total Tweets', fontsize = 15, weight= 'bold')\n_.margins(0)\n\nplt.show()","2b32d537":"# Define donut_plot function\ndef donut_plot(data_1, data_2, target, plot_title):\n\n    fig, ax = plt.subplots(ncols = 2, figsize= (15,7))\n    \n    # Data preproccessing\n    x = pd.DataFrame((data_1[target].value_counts(normalize=True).sort_index() * 100).round(1).reset_index().rename(columns = {'index': 'variable', 'gender': 'churn_yes'}))\n    x['trump'] = (data_2[target].value_counts(normalize=True).sort_index() * 100).round(1).values\n\n    cols_list = x.columns[1:3]\n    hue_list = x.variable.unique()\n\n    profile_list = ['Obama', 'Trump']\n\n    for indx in range(0,2):\n        # Technically its pie chart :) but I'm drawing a white circle to make them donuts!\n        ax[indx].pie(data = x, x = cols_list[indx], autopct='%1.1f%%', textprops = {'fontsize': 11, 'color': 'w', 'weight': 'bold'}) \n        ax[indx].add_patch(plt.Circle((0,0), 0.35, fc = 'white'))\n        label = ax[indx].annotate('{}'.format(profile_list[indx]), xy = (0, 0), fontsize = 13, ha = \"center\")  # weight = 'bold'\n        ax[indx].legend(hue_list, loc = 2)\n        ax[indx].set_title(plot_title, size = 16)\n    plt.show()\n\n# Seperate dataframe into obama and trump's dataframes\nobama_df = df_tweets[df_tweets.user == 'Barak Obama']\ntrump_df = df_tweets[df_tweets.user == 'Donald Trump']\n\n# Call donut plot function for each column\nfor indx, col in round1_cols_dict.items():\n    if indx != 0: # Avoid Twitter profile comparison\n        donut_plot( data_1 = obama_df, \n                    data_2 = trump_df, \n                    target = round1_cols_dict[indx], \n                    plot_title = '{}'.format(round1_titles[indx].title()) ) # .title() to capitilize the first character","36f83180":"## Detailed Language Observation\n\n# Aggregate dataframe \nsummarized_df = df_tweets.groupby(['user']).agg({  'favorite_counts': 'sum',       # Tweets overview\n                                                   'retweet_counts': 'sum',\n                                                   'is_positive': 'sum',\n                                                   'is_negative': 'sum',\n\n                                                   'tweets_length': 'sum',         # Tweets writing style\n                                                   'tweets_uppercase': 'sum',\n                                                   'tweets_punctuations': 'sum',\n                                                   'tweets_questionmark': 'sum',\n\n                                                   'is_norp': 'sum',               # Tweets detailed writing style\n                                                   'is_time': 'sum',\n                                                   'is_org': 'sum',\n                                                   'is_gpe': 'sum',\n                                                   'is_loc': 'sum',\n                                                   'is_product': 'sum',\n                                                   'is_workart': 'sum',\n                                                   'is_fac': 'sum',\n                                                   'is_noun': 'sum',\n                                                   'is_pron': 'sum',\n                                                   'is_adv': 'sum',\n                                                   'is_propn': 'sum',\n                                                   'is_verb': 'sum',\n                                                   'is_intj': 'sum'  }).reset_index()\n\n# Seperate dataframe \nobama_summarized_df = summarized_df[summarized_df.user == 'Barak Obama'].drop('user', axis = 1).copy()\ntrump_summarized_df = summarized_df[summarized_df.user == 'Donald Trump'].drop('user', axis = 1).copy()\n\n# Get columns\nsummarized_cols      = obama_summarized_df.columns\n\n# Create features for each round in list format\nround_1_cols = summarized_cols[:5]\nround_2_cols = summarized_cols[5:10]\nround_3_cols = summarized_cols[10:15]\nround_4_cols = summarized_cols[15:20]\nround_5_cols = summarized_cols[20:22]\n\n# Combine all lists into one list\ntemp_list = [round_1_cols, round_2_cols, round_3_cols, round_4_cols, round_5_cols]\n\n# Dictionary to rename the titles\nrep_title_dict = {  'favorite_counts': 'Tweets Likes',\n                    'retweet_counts': 'Re-Tweets',\n                    'is_positive': 'Positivity',\n                    'is_negative': 'Negativity',\n                    'tweets_length': 'Length of Tweets',\n                    'tweets_uppercase': 'Uppercase Characters Used',\n                    'tweets_punctuations': 'Punctuations Used',\n                    'tweets_questionmark': 'Questionmark Used',\n                    'is_norp': 'Nationalities | Religious | Political Groups',\n                    'is_time': 'Mentioned Time Related',\n                    'is_org': 'Corporate | Governmental',\n                    'is_gpe': 'Countries | Cities | States',\n                    'is_loc': 'Location Mentioned',\n                    'is_product': 'Objects | Vehicles | Foods',\n                    'is_workart': 'Books | Songs',\n                    'is_fac': 'Buildings | Airports | Highways',\n                    'is_noun': 'Noun Used',\n                    'is_pron': 'Pronoun Used',\n                    'is_adv': 'Adverb Used',\n                    'is_propn': 'Propn (like Apple, UK, US)',\n                    'is_verb': 'Verb Used',\n                    'is_intj': 'Bravo | Hello | Ouch' }\n\n# Create function to plot summarized details\ndef summarized_donut_plot(data_1, data_2, indx_list):#, plot_title):\n    \n    if indx_list == 4: \n        fig, ax = plt.subplots(ncols = 2, figsize= (13,6))\n    else:\n        fig, ax = plt.subplots(ncols = 5, figsize= (35,6))\n        \n    for indx, target in enumerate(temp_list[indx_list]):\n\n        total = df_tweets[target].sum()\n    \n        # Data preproccessing\n        x  = round(float(data_1[target] \/ total * 100), 1)\n        y  = round(float(data_2[target] \/ total * 100), 1)\n\n        #print(total, x, y)\n        \n        user_list = ['Obama', 'Trump']\n        results_list = [x, y]\n\n        x = pd.DataFrame(data = results_list, index = user_list)\n\n        ax[indx].pie(x, autopct='%1.1f%%', textprops = {'fontsize': 11, 'color': 'w', 'weight': 'bold'})\n        ax[indx].add_patch(plt.Circle((0,0), 0.35, fc = 'white'))\n\n        ax[indx].legend(user_list, loc = 2)\n        ax[indx].set_title(rep_title_dict[target], size = 13)\n\n    plt.show()\n\n# Call pie plot function for each column\nfor indx, sublist in enumerate(temp_list):\n    #print(indx, sublist)\n    summarized_donut_plot( data_1 = obama_summarized_df, data_2 = trump_summarized_df, indx_list = indx ) ","32497646":"## Second round of cleaning before we use spaCy\n\n# Stopwords external list\nunwanted_text_list = ['\u201c','\u201d', 'lol', 'lmao', 'tell', 'twitter', 'list', 'whatever', 'yes', 'like', \n                      'im', 'know', 'just', 'dont', 'thats', 'right', 'youre', 'got', 'gonna','think',\n                      'said', 'amp', 'omg', 'say', 'boy', 'lot', 'sir', 'office']","9742c89d":"# Function to clean tweets using spacy from punctuations, stopwords and lemmatize them\ndef cleaning_tweets(x):\n    # Spacy pipeline\n    tweet = nlp(x)\n    # Extract lemmatized words in lower case format if not digits, not punctuation, not stopword, and lenght not less than 2 \n    tweet = ' '.join([token.lemma_.lower() for token in tweet if not token.is_stop and not token.is_punct and not token.text.isdigit() and len(token.text) > 2])\n    tweet = ' '.join([token for token in tweet.split() if token not in unwanted_text_list])\n    return tweet\n\n# Store clean tweets\ndf_tweets['clean_tweets'] = df_tweets.tweets.apply(lambda x: cleaning_tweets(x))","86b4d218":"# Check\ndf_tweets.loc[:, ['tweets', 'clean_tweets']][:5]","dc5d8a00":"## Words count\n\n# Initiate lists\nobama_words_list = []\ntrump_words_list  = []\n\n# Function to append word by word to their specific list\ndef collect_words(x, user_list):\n    words = nlp(x.lower())\n    [user_list.append(token.text) for token in words if not token.is_stop and not token.is_punct and not token.is_space]\n\n# Send tweets to the function\n_ = df_tweets.loc[df_tweets.user == 'Barak Obama', 'clean_tweets'].apply(lambda x: collect_words(x, obama_words_list))\n_ = df_tweets.loc[df_tweets.user == 'Donald Trump', 'clean_tweets'].apply(lambda x: collect_words(x, trump_words_list))\n\n# Apply counter function to count words\nobama_freq_words = Counter(obama_words_list)\ntrump_freq_words = Counter(trump_words_list)\n\n# Store the top 100 words\nobama_top100_words = obama_freq_words.most_common(100)\ntrump_top100_words = trump_freq_words.most_common(100)\n\n# Print top 5 words from each list\nprint(obama_top100_words[:5])\nprint(trump_top100_words[:5])","6190bf46":"## WordCloud\n\n# Combine all tweets into one for each user\nobama_full_words = ' '.join([sent for sent in df_tweets.loc[df_tweets.user == 'Barak Obama', 'clean_tweets']])\ntrump_full_words = ' '.join([sent for sent in df_tweets.loc[df_tweets.user == 'Donald Trump', 'clean_tweets']])\n\n# Initiate the WorldCloud\nwc_obama = WordCloud(background_color=\"white\", colormap=\"Blues\", max_font_size=200, random_state=42) # blue color\nwc_trump = WordCloud(background_color=\"white\", colormap=\"Reds\", max_font_size=200, random_state=42) # red color\n\n# Function to display most common words\ndef plot_wordcloud(lists, titles, wc):\n    \n    fig, ax = plt.subplots(ncols = 2, figsize= (40,12))\n    \n    for indx, sents in enumerate(lists):\n        wc[indx].generate(sents)\n        ax[indx].imshow(wc[indx], interpolation = 'bilinear')\n        ax[indx].axis('off')\n        ax[indx].set_title(titles[indx], pad = 14, weight = 'bold')\n    plt.show()\n\n# Send user's tweets to the function\nplot_wordcloud( lists  = [obama_full_words, trump_full_words],\n                titles = [\"Obama's Top 100 Words\" , \"Trump's Top 100 Words\"],\n                wc     = [wc_obama, wc_trump] )","a5ccd9ab":"# Drop unwanted columns and convert user profile into numeric reps\ndf_tweets['profile'] = df_tweets.user.replace({'Donald Trump': 0, 'Barak Obama': 1})\ndf_tweets.drop(['id', 'user', 'tweets'], axis = 1, inplace = True)","16cfbf01":"from sklearn.utils import shuffle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score, roc_curve\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nimport xgboost as xgb\nimport lightgbm as lgb","e16eac08":"def process_data(data, feature, target):\n    \n    shuf_df = shuffle(data)                             # Shuffle DataFrame\n    shuf_df.reset_index(drop = True, inplace = True)    # Reset DataFrame index\n    \n    X = data[feature]\n    y = data[target]\n    \n    return X, y\n\nX, y = process_data( data    = df_tweets, \n                     feature = ['clean_tweets'], \n                     target  = ['profile'] )","dc0621cd":"# Split train, test dataframe\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = 42)\n\n# Check shapes\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\nprint(X_train.values.shape, X_test.values.shape, y_train.values.ravel().shape, y_test.values.ravel().shape)","b340705c":"# Initiate TFIDF Vect\ntfidf_vect = TfidfVectorizer(ngram_range = (1,3), stop_words = 'english', sublinear_tf = True)\n\n# Fit & transform the X_train\ntfidfVect_train    = tfidf_vect.fit_transform(X_train.clean_tweets)\ntfidfVect_train_df = pd.DataFrame(tfidfVect_train.toarray(), columns = tfidf_vect.get_feature_names())\n# Transform the X_test\ntfidfVect_test     = tfidf_vect.transform(X_test.clean_tweets)\ntfidfVect_test_df  = pd.DataFrame(tfidfVect_test.toarray(), columns = tfidf_vect.get_feature_names())","944e3f19":"# Initiate lists \nmodel_name_list = []\naccuracyScore_list = []\nrecallScore_list = []\nprecisionScore_list = []\nrocAucScore_list = []\nf1Score_list = []\n\ndef evaluate_classifier(model, Xtrain, Xtest, ytrain, ytest):\n    # Initiate the Naive Bayes 'MultinomialNB' classifier\n    clf = model[1]\n\n    # Fit classifier with the X_train_tfidf, y_train\n    clf.fit(Xtrain, ytrain)\n\n    # Store predicted values ub y_pred\n    y_pred = clf.predict(Xtest)\n    \n    accuracyScore  = accuracy_score(ytest, y_pred)\n    recallScore    = recall_score(ytest, y_pred)\n    precisionScore = precision_score(ytest, y_pred)\n    rocAucScore    = roc_auc_score(ytest, y_pred)\n    f1Score        = f1_score(ytest, y_pred)\n\n    model_name_list.append(model[0])\n    accuracyScore_list.append(accuracyScore)\n    recallScore_list.append(recallScore)\n    precisionScore_list.append(precisionScore)\n    rocAucScore_list.append(rocAucScore)\n    f1Score_list.append(f1Score)\n    \n    print('Accuracy Score: {}\\n\\n'.format(round(accuracyScore * 100),1))\n\n    print('Confusion Matrix:')\n    sns.heatmap(confusion_matrix(ytest, y_pred), annot = True, xticklabels = [\"Trump\",\"Obama\"], yticklabels = [\"Obama\", \"Trump\"])\n    plt.show()\n\n    print('\\n\\nClassification Report: \\n{}\\n'.format(classification_report(ytest, y_pred)))","3e0469bf":"# Evaluate Naive Bayes 'MultinomialNB' classifier\nevaluate_classifier( model  = ['MultinomialNB', MultinomialNB(alpha= 0.01)], \n                     Xtrain = tfidfVect_train_df.values, \n                     Xtest  = tfidfVect_test_df.values, \n                     ytrain = y_train.values.ravel(), \n                     ytest  = y_test.values.ravel())","30f85793":"# Evaluate XGBoost classifier\nevaluate_classifier( model  = ['XGBoost', xgb.XGBClassifier(learning_rate = 0.5)], \n                     Xtrain = tfidfVect_train_df.values, \n                     Xtest  = tfidfVect_test_df.values, \n                     ytrain = y_train.values.ravel(), \n                     ytest  = y_test.values.ravel())","ac22d82c":"# Evaluate LogisticRegression classifier\nevaluate_classifier( model  = ['LogisticRegression', LogisticRegression()], \n                     Xtrain = tfidfVect_train_df.values, \n                     Xtest  = tfidfVect_test_df.values, \n                     ytrain = y_train.values.ravel(), \n                     ytest  = y_test.values.ravel())","cf0aa558":"# Assemble scores from lists to dataframe\ndef assemble_scores():\n\n    results_dict = { 'Model': model_name_list,\n                     'Accuracy_Score': accuracyScore_list,\n                     'Recall_Score': recallScore_list,\n                     'Precision_Score': precisionScore_list,\n                     'ROC_AUC_Score': rocAucScore_list,\n                     'F1_Score': f1Score_list }\n\n    results_df = pd.DataFrame(results_dict)\n    \n    return results_df\n\n# Call assemble_scoes function to assemble scores into dataframe\nresults_df = assemble_scores()\n\n# Display results\nresults_df","cfbc75b7":"# Change the shape of the dataframe using pd.melt\nresults_melted_df = pd.melt(frame = results_df, id_vars = ['Model'], value_vars = ['Accuracy_Score', 'Recall_Score', 'Precision_Score', 'ROC_AUC_Score', 'F1_Score'], var_name = 'Score_Type', value_name = 'Score')\n\n# Plot scores againt models\nplt.figure(figsize= (20,8))\n\n_ = sns.barplot( data = results_melted_df, x = 'Score', y = 'Model', hue = 'Score_Type', palette= \"Paired\")\n\n_.set_title('Models Metrics \/ Scores', pad = 10, weight= 'bold')\n_.set_xlabel('Performance Score', weight= 'bold')\n_.set_ylabel('Model', weight= 'bold')\n\nplt.show()","8f08ff95":"## Final text transformer model\n\n# Initiate TFIDF Vect\ntfidf_vectorize = TfidfVectorizer(ngram_range = (1,3), stop_words = 'english', sublinear_tf = True)\n\n# Fit & transform the \ntfidfVect_X    = tfidf_vectorize.fit_transform(X.clean_tweets)\ntfidfVect_X_df = pd.DataFrame(tfidfVect_X.toarray(), columns = tfidf_vectorize.get_feature_names())","a62b9ac9":"## Final classifier\n\n# GridSearchCV\ncls_nb = MultinomialNB()\n\npara_grid = { 'alpha': [0.001, 0.01, 0.1, 0.5, 1],\n              'fit_prior' : [False, True] }\n\ncls_nb_gscv = GridSearchCV( iid = False, estimator = cls_nb, param_grid = para_grid, cv = 10, return_train_score = True, n_jobs = -1 )\ncls_nb_gscv_fit = cls_nb_gscv.fit(tfidfVect_X_df.values, y.values.ravel())\n\ncls_nb_results_df = pd.DataFrame(cls_nb_gscv_fit.cv_results_)\ncls_nb_results_df.sort_values('mean_test_score', ascending = False)[:5]","122c56f8":"print('Best Score: {}\\nBest Parameters: {}'.format(round(cls_nb_gscv_fit.best_score_ *100, 1) , cls_nb_gscv_fit.best_params_))","62839561":"T1 = 'Nancy just said she \u201cjust doesn\u2019t understand why?\u201d Very simply, without a Wall it all doesn\u2019t work. Our Country has a chance to greatly reduce Crime, Human Trafficking, Gangs and Drugs. Should have been done for decades. We will not Cave!'\nT2 = 'Without a Wall there cannot be safety and security at the Border or for the U.S.A. BUILD THE WALL AND CRIME WILL FALL!'\nT3 = 'The Fake News Media loves saying \u201cso little happened at my first summit with Kim Jong Un.\u201d Wrong! After 40 years of doing nothing with North Korea but being taken to the cleaners, & with a major war ready to start, in a short 15 months, relationships built, hostages & remains....'\n\nO1 = 'I\u2019ve always drawn inspiration from what Dr. King called life\u2019s most persistent and urgent question: \"What are you doing for others?\" Let\u2019s honor his legacy by standing up for what is right in our communities and taking steps to make a positive impact on the world.'\nO2 = 'In 2018 people stepped up and showed up like never before. Keep it up in 2019. We\u2019ve got a lot of work to do, and I\u2019ll be right there with you. Happy New Year, everybody!'\nO3 = 'I hope you find inspiration in the stories of Dejah, Moussa, Sandor, Hong and Jonny. Their journeys began with a decision to build the better future they wanted to see. The same is true for you. What matters isn\u2019t the size of the step you take; what matters is that you take it.'\n\nQ3 = 'Nadler just said that I \u201cpressured Ukraine to interfere in our 2020 Election.\u201d Ridiculous, and he knows that is not true. Both the President & Foreign Minister of Ukraine said, many times, that there \u201cWAS NO PRESSURE.\u201d Nadler and the Dems know this, but refuse to acknowledge!'\n\ntweets_list = [T1, O1, T2, O2, T3, O3, Q3]\n\nfor indx, tweet in enumerate(tweets_list):\n    result = cls_nb_gscv_fit.predict(tfidf_vectorize.transform([tweet])).item()\n    print(indx, 'Donald Trump' if result == 0 else 'Barak Obama')","dc855c95":"\n### **Quick Observation**\n\n- We have 531 entries\n- Donald Trump has more share than Obama with 52.5% (192 tweets)\n- Feature [date] is not recognised as datetime format\n- No null values\n- The most liked tweet has 4,515,657 likes and the most retweeted tweet has 1,666,772 retweets\n- High varience comparing the max\/ min values for both likes and retweets counts. More details in EDA section\n- Tweets [text] still raw. Needs to be cleaned up and extract insights\/ usefull information out of it.","ed80f19f":"### **Sentiment Analysis**\n\n- Get polarity in percentage (emotions expressed in tweets weather it's positive or negative). Range is [-1.0, 1.0]\n- Get subjectivity in percentage (opinions expressed in tweets like personal feelings, views, or beliefs). Range is [0, 1.0]\n- Classify tweets [Very Negative, Negative, Neutral, Positive, Very Positive]\n- Classify tweets as subjective or objective","721fd3b8":"### **Observations:**\n1. Obama prefers to tweet begining of each week unlike Trump as he prefers to tweet end of each week.\n2. Trump's tweets contain negativity much more than Obama's\n3. Trump's tweets reflects mostly his personal opinion unlike Obama as his tweets are fact-based and also personal opinion.\n    ","559ff609":"### **Steps**\n- **TfidfVectorizer** for text vectorizing with **ngram(1,2)**\n- Multiple classifiers, evaluate them, select model and tune its parameters\n- Final model, train on all dataset and predict new tweets","d03c2b18":"### **Language Analysis**\n- I will use spaCy for this part of analysis\n- Extract additional information from the text","227c4907":"### **Predict New Tweets Samples**","7e04217d":"### **General Insights**","1671dbea":"### **Initial Cleaning Steps**\n\n1. Remove URLs from tweets\n2. Extract 'hashtags' and 'mentions' from tweets\n3. Extract additional information from tweets (i.e. number of spaces or ! or ? used.. etc)\n4. Remove unwated features","3509b317":"### **Evaluate Models**\n1. Evaluate models\n2. Display results\n3. Select best model based on **Accuracy**, **Precision** and **F1 Score**\n4. Tune model's parameter using GridSearchCV\n5. Train model on all dataset\n6. Predict new tweets samples","4baa10d9":"## **Machine Learning - Classifier**","1074099f":"### **Observations**\n1. Obama's tweets have the most number of likes and retweets surpassing Trump by 80%\n2. Obama and Trump's general positivity score are almost the same but interms of negativity, Trumps dominate the negativity.\n3. Trumps surpass Obama in the length of his tweets and in using uppercase characters, punctuations and questionmarks which reflects Trump's frustration and that explains the negativity score.\n4. Trump's also likes to use or name nationalities, corporates, countries, religious, political groups, airports and products in his tweets\n5. However, it seems Obama cares more about food and vehicles\n6. On the other hand, time and dates are important to Obama and that importance reflects in his tweets  \n7. Surprisingly, it shows that Trump usually mention book's titles and songs in his tweets (you can do additional step of analysis to see whether it's used in a negative way or positive way)","9107a345":"#### Initiate Twitter Connection\n\n#### Import Tweepy Library\nfrom tweepy import OAuthHandler, API, Stream, StreamListener\n\n#### Consumer API keys\nAPI_key        = 'GET YOUR KEYS'\nAPI_secret_key = 'GET YOUR KEYS'\n\n#### Access token & access token secret\nAccess_token        = 'GET YOUR KEYS'\nAccess_token_secret = 'GET YOUR KEYS'\n\nauth = OAuthHandler(API_key, API_secret_key)\nauth.set_access_token(Access_token, Access_token_secret)\napi = API(auth)\n\n\n#### Load Obama and Trump Tweets\ndt_tweets = api.user_timeline(screen_name= \"realDonaldTrump\", count= 1000, include_rts= False, tweet_mode= 'extended')\nprint(\"Number of tweets extracted: {}.\\n\".format(len(dt_tweets)))\n\n#### Stream Barak Obama tweets\nbo_tweets = api.user_timeline(screen_name= \"BarackObama\", count= 1000, include_rts= False, tweet_mode= 'extended')\nprint(\"Number of tweets extracted: {}.\\n\".format(len(bo_tweets)))\n\n#### Convert Raw Data To Readable DataFrame\n#### Extracting id, created_at, full_text, favorite_count, retweet_count details from Donald Trump and Obama's Tweets\n\n#### Initiate lists to hole the extracted information\ntw_id_list = []\ntw_user_list = []\ntw_created_date_list = []\ntw_tweet_txt_list = []\ntw_fav_count_list = []\ntw_retweet_count_list = []\n\n#### Function to extract the information\ndef fillin_tweets(tweets_obj, user):\n    try:\n        for tw in tweets_obj:\n            tw_id_list.append(tw.id)\n            tw_user_list.append(user)\n            tw_created_date_list.append(tw.created_at)\n            tw_tweet_txt_list.append(tw.full_text)\n            tw_fav_count_list.append(tw.favorite_count)\n            tw_retweet_count_list.append(tw.retweet_count)\n    except:\n        tw_id_list.append(np.nan)\n        tw_user_list.append(user)\n        tw_created_date_list.append(np.nan)\n        tw_tweet_txt_list.append(np.nan)\n        tw_fav_count_list.append(np.nan)\n        tw_retweet_count_list.append(np.nan)\n\n#### Extract the information\nfillin_tweets(tweets_obj= dt_tweets, user= 'Donald Trump')\nfillin_tweets(tweets_obj= bo_tweets, user= 'Barak Obama')\n\n#### Create dictionary of the extracted information\ndict_dataframe = {  'id': tw_id_list,\n                    'user': tw_user_list, \n                    'date': tw_created_date_list,\n                    'text': tw_tweet_txt_list,\n                    'favorite_counts': tw_fav_count_list,\n                    'retweet_counts': tw_retweet_count_list  }\n\n#### Create dataframe from the created dictionary\ndf_obama_trump = pd.DataFrame(dict_dataframe)\n\n#### Save dataframe into CSV file\ndf_obama_trump.to_csv('Trump_Obama_Tweets.csv')","5cc46a12":"### **Additional Steps You Can Take**\n-To enhance the result, evaluate the extracted features and combine them with the transformed text <br>\n-To build an app, export TFIDF text transformer and classifier as .SAV file \n\n        import joblib\n        joblib.dump(cls_nb_gscv_fit, 'ot_classifier.sav')\n        joblib.dump(tfidf_vectorize, 'tfidf_transformer.sav')\n\n"}}