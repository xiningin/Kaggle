{"cell_type":{"af07fcc6":"code","ebacf2bd":"code","b1900c63":"code","8d66c420":"code","eba3c5fb":"code","cbc34f12":"code","7acd8a21":"code","7a953681":"code","f1cd1ca8":"code","d3df2f25":"code","912c3648":"code","7945f048":"code","0cc727f0":"code","5f23218d":"code","bebe2768":"code","39eb0e5c":"code","bc25c179":"code","8605022c":"code","74b4b147":"code","2df88583":"code","7393200f":"code","50d6666c":"code","296c9190":"code","5b2a8aa4":"code","0e6b11d7":"code","b0197572":"code","f35b76d8":"code","965b5de8":"code","de42165b":"code","5beab2bf":"code","11285a54":"code","53c0b3c5":"code","0ecb3646":"code","f76d1d94":"code","646760db":"code","6eb287f8":"code","38c3f669":"code","9b14f05b":"code","e2a6b43b":"code","87902ce2":"code","7ea08a9e":"code","7e1aec06":"code","2428e66a":"code","3c15e59c":"code","77ec0f6c":"code","6a9a9e54":"code","dfcba9ac":"code","43743b51":"code","66c38b43":"code","a24cf8ce":"code","89085598":"code","b5d2ffec":"code","f68a8647":"code","ba94a622":"code","07ff3981":"code","f6a734fc":"code","d9f08831":"code","bb1f2e1f":"code","dad2ef2a":"code","bcf6fa63":"code","4f0d3d0c":"code","4b8bd61e":"code","b20b0b26":"code","2fb708b5":"code","3e70dce1":"code","1921ac3d":"code","1e15a074":"code","c19b6a03":"code","3ad8c879":"code","d7e9393b":"code","1dd4275c":"code","0a4c3f64":"code","d523410f":"code","3ccc3d14":"code","240233ae":"markdown","7eea728a":"markdown","0d4e0313":"markdown","5c506d0a":"markdown","4ba2a78e":"markdown","b3a61313":"markdown","83d168ce":"markdown","a82cf151":"markdown","009424c2":"markdown","857ed599":"markdown","71c05e4d":"markdown","dbb25c9e":"markdown","87563315":"markdown","b2b6ca42":"markdown","26ca842d":"markdown","98f4d721":"markdown"},"source":{"af07fcc6":"#!pip -q install --upgrade --ignore-installed numpy pandas scipy sklearn","ebacf2bd":"#!pip -q install catboost\n#!pip -q install lightgbm","b1900c63":"#!pip -q install \"dask[complete]\"\n#!pip -q install \"dask-ml[complete]\"","8d66c420":"## https:\/\/stackoverflow.com\/questions\/49853303\/how-to-install-pydot-graphviz-on-google-colab?rq=1\n#!pip -q install graphviz \n#!apt-get install graphviz -qq\n#!pip -q install pydot","eba3c5fb":"# After this restart your kernel using Ctrl+M+. to reset all loaded libraries to latest ones, that we just installed.","cbc34f12":"import pandas as pd\nimport numpy as np\nfrom multiprocessing import Pool\nfrom pandas.io.json import json_normalize\nfrom sklearn.pipeline import make_pipeline, Pipeline\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom threading import Thread as trd\nimport queue\nimport json\nimport gc\ngc.enable()","7acd8a21":"PATH = \"..\/input\/ga-customer-revenue-prediction\/\"","7a953681":"part = pd.read_csv(PATH+\"train_v2.csv\",dtype={\"fullVisitorId\":\"str\", \"visitId\":\"str\"}, nrows=10)\npart.shape","f1cd1ca8":"columns = part.columns\npart.head(2)","d3df2f25":"# From above output you will have a basic understanding of type of columns. Divide them so you\n# can use same function on similar columns (for exploration etc).\nid_columns = [\"fullVisitorId\", \"visitId\", \"visitStartTime\"]\nnum_columns = [\"visitNumber\"]\nobj_columns = [\"channelGrouping\", \"socialEngagementType\", \"date\"]\ndict_columns = [ \"device\", \"geoNetwork\", \"totals\", \"trafficSource\"]\ncomplex_columns = [\"customDimensions\", \"hits\"]","912c3648":"col = \"device\"\ndf = pd.read_csv(PATH+\"train_v2.csv\", usecols = [3], converters={col: json.loads})\n\ncolumn_as_df = json_normalize(df[col])\ncolumn_as_df.head()","7945f048":"# Explore this column. Check if you want to keep it or not.\n# If you want to scale this column or LableEncode this column, \n# do it and maintain a dictionary with all columns as keys and\n# corresponding Scalar or LabelEncoder or both as values.","0cc727f0":"# Calculate Standard Deviation or check number of uniques, check if they are constant...\ncolumn_as_df.nunique()","5f23218d":"drop_cols = [col for col in column_as_df.columns if col not in ['browser', 'deviceCategory', 'isMobile', 'operatingSystem']]\ncolumn_as_df.drop(drop_cols, axis=1, inplace=True)","bebe2768":"column_as_df.head()","39eb0e5c":"final_selected_columns = {\n    \"device\": [\"browser\", \"deviceCategory\", \"isMobile\", \"operatingSystem\"],\n    \"geoNetwork\":[\"city\", \"continent\", \"country\", \"metro\", \"networkDomain\", \"region\", \"subContinent\"],\n    \"totals\":[\"transactions\", \"sessionQualityDim\", \"hits\", \"pageviews\", \"totalTransactionRevenue\", \"timeOnSite\", \"transactionRevenue\"],\n    \"trafficSource\":[\"adContent\", \"adwordsClickInfo.adNetworkType\", \"adwordsClickInfo.page\", \"adwordsClickInfo.slot\",\n                     \"campaign\", \"keyword\",\t\"medium\", \"referralPath\", \"source\"],\n    \"customDimensions\": [\"customDimensions\"],\n    \"channelGrouping\": [\"channelGrouping\"],\n    \"date\":[\"date\"],\n    \"visitNumber\":[\"visitNumber\"],\n    \"visitStartTime\":[\"visitStartTime\"]\n}","bc25c179":"columns_selected = []\nfor key in final_selected_columns.keys():\n    if len(final_selected_columns[key]) == 1:\n        columns_selected.append(key)\n    else:\n        for sub_col in final_selected_columns[key]:\n            columns_selected.append(f\"{key}.{sub_col}\")\ncolumns_selected","8605022c":"prep_pipeline = {col: [] for col in columns_selected}","74b4b147":"for col_ in [\"browser\", \"deviceCategory\", \"operatingSystem\"]:\n    lb = LabelEncoder()\n    column_as_df[col_] = lb.fit_transform(column_as_df[col_].values)\n    prep_pipeline[f\"device.{col_}\"].append(lb)","2df88583":"for col_ in [\"browser\", \"deviceCategory\", \"operatingSystem\"]:\n    rsc = RobustScaler()\n    column_as_df[col_] = rsc.fit_transform(column_as_df[col_].values.reshape(-1, 1)).reshape(1, -1)[0]\n    prep_pipeline[f\"device.{col_}\"].append(rsc)","7393200f":"column_as_df.head()","50d6666c":"import pickle\nwith open(\"pipeline.pickle\", \"wb\") as fle:\n    pickle.dump(prep_pipeline, fle)\n!ls","296c9190":"del df, column_as_df\ngc.collect()","5b2a8aa4":"PATH2 = \"..\/input\/googleanalbigfile\/\"\n!ls {PATH2}","0e6b11d7":"with open(PATH2+\"preprocessing_pipeline (1).pickle\", \"rb\") as fle:\n    preprocessing_pipeline = pickle.load(fle)","b0197572":"final_selected_columns = {\n    \"device\": [\"browser\", \"deviceCategory\", \"isMobile\", \"operatingSystem\"],\n    \"geoNetwork\":[\"city\", \"continent\", \"country\", \"metro\", \"networkDomain\", \"region\", \"subContinent\"],\n    \"totals\":[\"transactions\", \"sessionQualityDim\", \"hits\", \"pageviews\", \"totalTransactionRevenue\", \"timeOnSite\", \"transactionRevenue\"],\n    \"trafficSource\":[\"adContent\", \"adwordsClickInfo.adNetworkType\", \"adwordsClickInfo.page\", \"adwordsClickInfo.slot\",\n                     \"campaign\", \"keyword\",\t\"medium\", \"referralPath\", \"source\"],\n    \"customDimensions\": [\"customDimensions\"],\n    \"channelGrouping\": [\"channelGrouping\"],\n    \"date\":[\"date\"],\n    \"visitNumber\":[\"visitNumber\"],\n    \"visitStartTime\":[\"visitStartTime\"]\n}","f35b76d8":"def browser_mapping(x):\n    browsers = ['chrome','safari','firefox','explorer','edge','opera','coc coc','maxthon','iron']\n    for br in browsers: \n      if br in x.lower(): return br\n    if  ('android' in x) or ('samsung' in x) or ('mini' in x) or ('iphone' in x) or ('in-app' in x) or ('playstation' in x): return 'mobile browser'\n    elif  ('mozilla' in x) or ('chrome' in x) or ('blackberry' in x) or ('nokia' in x) or ('browser' in x) or ('amazon' in x): return 'mobile browser'\n    elif  ('lunascape' in x) or ('netscape' in x) or ('blackberry' in x) or ('konqueror' in x) or ('puffin' in x) or ('amazon' in x): return 'mobile browser'\n    elif '(not set)' in x: return '(not set)'\n    else: return 'others'\n\ndef adcontents_mapping(x):\n    if 'google' in x: return 'google'\n    elif ('placement' in x) | ('placememnt' in x): return 'placement'\n    elif '(not set)' in x: return '(not set)'\n    elif 'na' == x: return 'na'\n    elif 'ad' in x: return 'ad'\n    else: return 'others'\n    \ndef keyword_mapping(x):\n  if '(not provided)' in x: return '(not provided)'\n  elif 'na' == x: return 'na'\n  elif 'google' in x: return 'google'\n  elif 'shart' in x: return 't-shirt'\n  elif 'sticker' in x: return 'sticker'\n  elif 'lav' in x: return 'lava-lamp'\n  elif 'pen' in x: return 'pen'\n  elif 'merc' in x: return 'merchandise'\n  elif 'bag' in x: return 'bagpack'\n  elif 'laptop' in x: return 'laptop'\n  elif 'onesie' in x: return 'onesie'\n  elif 'sunglass' in x: return 'sunglasses'\n  elif 'cup' in x: return 'cup'\n  elif 'cap' in x: return 'cap'\n  elif 'arrel' in x: return 'apparel'\n  elif 'arel' in x: return 'apparel'\n  elif 'shirt' in x: return 't-shirt'\n  elif 'jac' in x: return 'jacket'\n  elif 'bank' in x: return 'powerbank'\n  elif 'dre' in x: return 'dress'\n  elif 'todd' in x: return 'toddler'\n  elif 'bic' in x: return 'bicycle'\n  elif 'earp' in x: return 'earphone'\n  elif 'head' in x: return 'headphones'\n  elif 'virt' in x: return 'virtual-reality'\n  elif 'reali' in x: return 'virtual-reality'\n  elif 'bott' in x: return 'bottle'\n  elif 'book' in x: return 'notebook'\n  elif 'men' in x: return 'men'\n  elif 'women' in x: return 'women'\n  elif 'wom' in x: return 'women'\n  elif 'goo' in x: return 'google'\n  elif 'gle' in x: return 'google'\n  elif 'you' in x: return 'youtube'\n  elif 'yu' in x: return 'youtube'\n  elif 'yi' in x: return 'youtube'\n  elif 'ube' in x: return 'youtube'\n  elif 'tb' in x: return 'youtube'\n  else: return 'others'\n  \ndef referralPath_mapping(x):\n  if 'intl' in x: return 'intl'\n  elif 'yt' in x: return 'yt'\n  elif 'golang' in x: return 'golang'\n  elif 'document' in x: return 'document'\n  elif '\/r\/' in x: return 'reddit'\n  elif 'google' in x: return 'google'\n  elif '\/ads' in x: return 'ads'\n  elif 'ads\/' in x: return 'ads'\n  elif 'adsense\/' in x: return 'ads'\n  elif 'mail' in x: return 'mail'\n  elif 'hangouts' in x: return 'hangouts'\n  elif 'iphone' in x: return 'iphone'\n  elif 'webapps\/' in x: return 'webapp'\n  elif 'maps\/' in x: return 'maps'\n  elif 'webmasters\/' in x: return 'webapp'\n  elif 'android\/' in x: return 'android'\n  elif 'websearch\/' in x: return 'websearch'\n  elif 'youtube\/' in x: return 'youtube'\n  elif 'spreadsheets\/' in x: return 'spreadsheets'\n  elif 'presentation\/' in x: return 'presentation'\n  elif 'chromebook\/' in x: return 'chromebook'\n  elif 'bookmarks\/' in x: return 'bookmark'\n  elif 'drive\/' in x: return 'drive'\n  elif 'baiduid' in x: return 'baidu'\n  elif 'calendar\/' in x: return 'calendar'\n  elif 'pin\/' in x: return 'pinterest'\n  elif 'entry\/' in x: return 'entry'\n  elif 'chrome\/' in x: return 'chrome'\n  elif '.html' in x: return 'html'\n  elif '.pdf' in x: return 'pdf'\n  elif 'wiki\/' in x: return 'wiki'\n  elif 'appserve\/' in x: return 'webapp'\n  elif 'web\/' in x: return 'web'\n  elif 'edit\/' in x: return 'edit'\n  elif 'feed\/' in x: return 'feed'\n  elif 'feedback' in x: return 'feedback'\n  elif 'user\/' in x: return 'user'\n  elif '.htm' in x: return 'htm'\n  elif '.php' in x: return 'php'\n  elif 'class_section' in x: return 'class_section'\n  elif 'messages\/' in x: return 'message'\n  elif 'na' == x: return 'na'\n  else: return 'others'\n  \ndef source_mapping(x):\n    if 'google' in x:return 'google'\n    elif 'youtube' in x:return 'youtube'\n    elif 'yahoo' in x:return 'yahoo'\n    elif 'facebook' in x:return 'facebook'\n    elif 'reddit' in x:return 'reddit'\n    elif 'bing' in x:return 'bing'\n    elif 'quora' in x:return 'quora'\n    elif 'outlook' in x:return 'outlook'\n    elif 'linkedin' in x:return 'linkedin'\n    elif 'pinterest' in x:return 'pinterest'\n    elif 'ask' in x:return 'ask'\n    elif 'siliconvalley' in x:return 'siliconvalley'\n    elif 'lunametrics' in x:return 'lunametrics'\n    elif 'amazon' in x:return 'amazon'\n    elif 'mysearch' in x:return 'mysearch'\n    elif 'qiita' in x:return 'qiita'\n    elif 'messenger' in x:return 'messenger'\n    elif 'twitter' in x:return 'twitter'\n    elif 't.co' in x:return 't.co'\n    elif 'vk.com' in x:return 'vk.com'\n    elif 'search' in x:return 'search'\n    elif 'edu' in x:return 'edu'\n    elif 'mail' in x:return 'mail'\n    elif 'ad' in x:return 'ad'\n    elif 'golang' in x:return 'golang'\n    elif 'direct' in x:return 'direct'\n    elif 'dealspotr' in x:return 'dealspotr'\n    elif 'sashihara' in x:return 'sashihara'\n    elif 'phandroid' in x:return 'phandroid'\n    elif 'baidu' in x:return 'baidu'\n    elif 'mdn' in x:return 'mdn'\n    elif 'duckduckgo' in x:return 'duckduckgo'\n    elif 'seroundtable' in x:return 'seroundtable'\n    elif 'metrics' in x:return 'metrics'\n    elif 'sogou' in x:return 'sogou'\n    elif 'businessinsider' in x:return 'businessinsider'\n    elif 'github' in x:return 'github'\n    elif 'gophergala' in x:return 'gophergala'\n    elif 'yandex' in x:return 'yandex'\n    elif 'msn' in x:return 'msn'\n    elif 'dfa' in x:return 'dfa'\n    elif 'feedly' in x:return 'feedly'\n    elif 'arstechnica' in x:return 'arstechnica'\n    elif 'squishable' in x:return 'squishable'\n    elif 'flipboard' in x:return 'flipboard'\n    elif 't-online.de' in x:return 't-online.de'\n    elif 'sm.cn' in x:return 'sm.cn'\n    elif 'wow' in x:return 'wow'\n    elif 'baidu' in x:return 'baidu'\n    elif 'partners' in x:return 'partners'\n    elif '(not set)' in x: return \"(not set)\"\n    elif 'nan' in x: return 'na'\n    else: return 'others'","965b5de8":"dict_columns = [ \"device\", \"geoNetwork\", \"totals\", \"trafficSource\"]\nid_columns = [\"fullVisitorId\", \"visitId\"]\nnum_columns = ['totals.transactions', 'totals.sessionQualityDim', 'totals.hits',\n               'totals.pageviews', 'totals.totalTransactionRevenue', 'totals.timeOnSite', \n              'trafficSource.adwordsClickInfo.page', 'totals.transactionRevenue', 'visitNumber']\nprocess_columns = {'device.browser': browser_mapping, \n                   'trafficSource.adContent': adcontents_mapping, \n                   'trafficSource.source': source_mapping,\n                   'trafficSource.keyword': keyword_mapping, \n                   'trafficSource.referralPath': referralPath_mapping}\ndrop_columns = [\"hits\", \"socialEngagementType\"]","de42165b":"all_columns = columns_selected","5beab2bf":"# Now that you have a dictionary which has all the columns you need as keys\n# and corresponding methods you want to apply to that column sequentially,\n# you can make a preprocesing method which will be used to clean data at\n# every incremental step.\n\ndef preprocess(df):\n  df.reset_index(drop=True, inplace=True)\n  \n  for col_ in drop_columns:\n    if col_ in df.columns: df = df.drop([col_], axis=1)\n  \n  for col in dict_columns:\n    col_df = json_normalize(df[col])\n    col_df.columns = [f\"{col}.{subcolumn}\" for subcolumn in col_df.columns]\n    \n    to_drop = [temp for temp in col_df.columns if temp not in all_columns]\n    col_df = col_df.drop(to_drop, axis=1)\n    \n    df = df.drop([col], axis=1).merge(col_df, right_index=True, left_index=True)\n    \n    for temp_col in col_df.columns:\n      if temp_col in num_columns: df[temp_col] = df[temp_col].fillna(0.0).astype(float)\n      elif temp_col == \"device.isMobile\": df[temp_col] = df[temp_col].astype(bool)\n      else: df[temp_col] = df[temp_col].fillna('NA').astype(str)\n        \n      if temp_col in process_columns.keys(): df[temp_col] = df[temp_col].map(lambda x: process_columns[temp_col](str(x).lower())).astype('str')        \n      \n      if len(preprocessing_pipeline[temp_col]) >= 4:\n        for i in range(4):\n          if preprocessing_pipeline[temp_col][i] is None: continue\n          elif i == 0: \n            lb = preprocessing_pipeline[temp_col][i]\n            df[temp_col] = lb.transform(df[temp_col])\n          elif i == 1:\n            df[temp_col] = np.log1p(df[temp_col])\n          elif i == 3:\n            pass\n            #rsc = preprocessing_pipeline[temp_col][i]\n            #df[temp_col] = rsc.transform(df[temp_col].values.reshape(-1, 1)).reshape(1, -1)[0]\n      elif len(preprocessing_pipeline[temp_col]) == 3:\n        for i in range(3):\n          if preprocessing_pipeline[temp_col][i] is None: continue\n          elif i == 0: \n            lb = preprocessing_pipeline[temp_col][i]\n            df[temp_col] = lb.transform(df[temp_col])\n          elif i == 2:\n            pass\n            #rsc = preprocessing_pipeline[temp_col][i]\n            #df[temp_col] = rsc.transform(df[temp_col].values.reshape(-1, 1)).reshape(1, -1)[0]\n            \n  df['visitNumber'] = df['visitNumber'].fillna(0.0).astype(float)\n  df['channelGrouping'] = df['channelGrouping'].fillna('NA').astype(str)\n  df['customDimensions'] = df['customDimensions'].fillna('NA').astype(str)\n  \n  for col_ in [\"visitNumber\", \"customDimensions\", \"channelGrouping\"]:\n    for i in range(3):\n      if preprocessing_pipeline[col_][i] is None: continue\n      elif i==0:\n        lb = preprocessing_pipeline[col_][i]\n        df[col_] = lb.transform(df[col_])\n      elif i == 2:\n        pass\n        #rsc = preprocessing_pipeline[col_][i]\n        #df[col_] = rsc.transform(df[col_].values.reshape(-1, 1)).reshape(1, -1)[0]\n  \n  df['date_new'] = pd.to_datetime(df['visitStartTime'], unit='s')\n  df.drop(['visitStartTime'], axis=1, inplace=True)\n  df['sess_date_dow'] = df['date_new'].dt.dayofweek\n  df['sess_date_hours'] = df['date_new'].dt.hour\n  df['sess_month'] = df['date_new'].dt.month\n  df['sess_year'] = df['date_new'].dt.year\n  \n  for col_ in ['totals.transactions', 'totals.totalTransactionRevenue']:\n    if col_ in df.columns: df = df.drop([col_], axis=1)\n  \n  return df","11285a54":"import lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb # CatBoost is currently making its incremental learner: https:\/\/github.com\/catboost\/catboost\/issues\/464","53c0b3c5":"dict_columns = [ \"device\", \"geoNetwork\", \"totals\", \"trafficSource\"]\nid_columns = [\"fullVisitorId\", \"visitId\"]","0ecb3646":"incremental_dataframe = pd.read_csv(PATH + \"train_v2.csv\",\n                                   dtype={\"fullVisitorId\":\"str\", \"visitId\":\"str\"},\n                                   converters = {col_: json.loads for col_ in dict_columns},\n                                   chunksize=100000) # Number of lines to read.\n# This method will return a sequential file reader reading 'chunksize' lines every time.\n# To read file from starting again, you will have to call this method again.","f76d1d94":"lgb_params = {\n  \"objective\" : \"regression\",\n  \"n_estimators\":3000,\n  \"max_depth\": 4,\n  \"metric\" : \"rmse\",\n  \"learning_rate\" : 0.01,\n  'subsample':.8,\n  'colsample_bytree':.9\n}\n# First three are necessary for incremental learning.\nxgb_params = {\n  'update':'refresh',\n  'process_type': 'update',\n  'refresh_leaf': True,\n  'silent': True,\n  }","646760db":"def get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","6eb287f8":"drop_cols =['date', 'visitId', 'date_new', 'fullVisitorId', 'totals.transactionRevenue']","38c3f669":"# For saving regressor for next use.\nlgb_estimator = None\n\ni=0\nprev=None\nfor df in incremental_dataframe:\n  print(i)\n  df = preprocess(df)\n  print(\"Preprocessed.\")\n  #folds = get_folds(df=df, n_splits=5)\n  #for fold_, (trn_, val_) in enumerate(folds):\n  #    trn_x, trn_y = df.drop(drop_cols, axis=1).iloc[trn_], df['totals.transactionRevenue'].iloc[trn_]\n  #    val_x, val_y = df.drop(drop_cols, axis=1).iloc[val_], df['totals.transactionRevenue'].iloc[val_]\n  if i==0:\n    prev = df\n    print(\"First prev\")\n    i += 1\n    continue\n  trn_x, trn_y = prev.drop(drop_cols, axis=1), prev['totals.transactionRevenue']\n  val_x, val_y = df.drop(drop_cols, axis=1), df['totals.transactionRevenue']\n  print(\"Starting to train...\")\n  lgb_estimator = lgb.train(lgb_params,\n                         init_model=lgb_estimator, # Pass partially trained model\n                         train_set=lgb.Dataset(trn_x, trn_y),\n                         valid_sets=[lgb.Dataset(val_x, val_y)],\n                         valid_names=[\"Valid\"],\n                         early_stopping_rounds = 50,\n                         keep_training_booster=True, # For incremental learning\n                         num_boost_round=70,\n                         verbose_eval=50) # Output after each of 50th time\n  print(\"Continuing...\")\n  prev = df\n  del df, trn_x, trn_y, val_x, val_y\n  gc.collect()\n  i += 1\n\n#trn_x, trn_y = prev.(drop_cols, axis=1), prev['totals.transactionRevenue']\n#lgb_estimator = lgb.train(lgb_params,\n#                         init_model=lgb_estimator, # Pass partially trained model\n#                         train_set=lgb.Dataset(trn_x, trn_y),\n#                         early_stopping_rounds = 50,\n#                         keep_training_booster=True, # For incremental learning\n#                         num_boost_round=70,\n#                         verbose_eval=50) # Output after each of 50th time","9b14f05b":"del prev\ngc.collect()","e2a6b43b":"incremental_test = pd.read_csv(PATH + \"test_v2.csv\",\n                                   dtype={\"fullVisitorId\":\"str\", \"visitId\":\"str\"},\n                                   converters = {col_: json.loads for col_ in dict_columns},\n                                   chunksize=100000)\ntest_df = pd.DataFrame()\nfor df in incremental_test:\n  df = preprocess(df)\n  test_df = pd.concat([test_df, df], axis=0, sort=False).reset_index(drop=True)\n\ndel df\ngc.collect()\ntest_df.head()","87902ce2":"from sklearn.metrics import mean_squared_error as mse\npreds = lgb_estimator.predict(test_df.drop(drop_cols, axis=1))\n#preds = preprocessing_pipeline['totals.transactionRevenue'][3].inverse_transform(preds.reshape(-1, 1)).reshape(1, -1)[0]\npreds[preds<0] = 0\n\ntrue = test_df['totals.transactionRevenue']\n#true = preprocessing_pipeline['totals.transactionRevenue'][3].inverse_transform(true.values.reshape(-1, 1)).reshape(1, -1)[0]\n\nmse(true, preds) ** 0.5","7ea08a9e":"del preds, true, lgb_estimator, test_df\ngc.collect()","7e1aec06":"%reset -f","2428e66a":"import dask\nimport gc\ngc.enable()","3c15e59c":"import dask.dataframe as dd\nfrom dask.distributed import Client\nclient = Client(processes=False, threads_per_worker=4, n_workers=4, memory_limit='4GB')\nclient","77ec0f6c":"dict_cols = [\"device\", \"geoNetwork\", \"totals\", \"trafficSource\"]\nPATH = \"..\/input\/ga-customer-revenue-prediction\/\"\ndf = dd.read_csv(PATH+\"train_v2.csv\",\n                dtype={'fullVisitorId': 'str','date': 'str', \n                    **{c: 'str' for c in dict_cols}\n                },\n                parse_dates=['date'], blocksize=1e9)","6a9a9e54":"df.npartitions","dfcba9ac":"#df.visualize(size=\"15,15!\")","43743b51":"df.head()","66c38b43":"dict_cols = [\"device\", \"geoNetwork\", \"totals\", \"trafficSource\"]\ndrop_columns = [\"hits\", \"socialEngagementType\"]","a24cf8ce":"# Convert string Series to dictionary Series\nfor col_ in dict_cols:\n    df[col_] = df[col_].apply(lambda x: eval(x.replace('false', 'False')\n                                                    .replace('true', 'True')\n                                                    .replace('null', 'np.nan')), meta=('', 'object'))","89085598":"final_selected_columns = {\n    \"device\": [\"browser\", \"deviceCategory\", \"isMobile\", \"operatingSystem\"],\n    \"geoNetwork\":[\"city\", \"continent\", \"country\", \"metro\", \"networkDomain\", \"region\", \"subContinent\"],\n    \"totals\":[\"sessionQualityDim\", \"hits\", \"pageviews\", \"timeOnSite\", \"transactionRevenue\"],\n    \"trafficSource\":[\"adContent\", \"adwordsClickInfo.adNetworkType\", \"adwordsClickInfo.page\", \"adwordsClickInfo.slot\",\n                     \"campaign\", \"keyword\",\t\"medium\", \"referralPath\", \"source\"],\n    \"customDimensions\": [\"customDimensions\"],\n    \"channelGrouping\": [\"channelGrouping\"],\n    \"date\":[\"date\"],\n    \"visitNumber\":[\"visitNumber\"],\n    \"visitStartTime\":[\"visitStartTime\"]\n}","b5d2ffec":"#non_str_cols = ['isMobile', 'isTrueDirect', 'pageviews', 'hits', 'bounces', 'newVisits',\n#               'transactionRevenue', 'visits', 'timeOnSite','sessionQualityDim']\n#default = 0\n#for col_ in dict_cols:\n#    for key in final_selected_columns[col_]:\n#        if key in non_str_cols: \n#            default = 0\n#            df[f'{col_}.{key}'] = df[col_].to_bag().pluck(key, default=default).to_dataframe().iloc[:,0]\n#            df[f'{col_}.{key}'] = df[f'{col_}.{key}'].astype(int)\n#        else: \n#            default = \"NaN\"\n#            df[f'{col_}.{key}'] = df[col_].to_bag().pluck(key, default=default).to_dataframe().iloc[:,0]\n#            df[f'{col_}.{key}'] = df[f'{col_}.{key}'].astype(str)\n#    del df[col_]\n#    gc.collect()","f68a8647":"#df = df.drop(drop_columns, axis=1)\n#df.head()","ba94a622":"#df.visualize(size=\"20,10!\")","07ff3981":"#df.isnull().sum().compute()","f6a734fc":"# Necessary for converting dataframe to array. Takes specified length from each block.\n#lengths = []\n#for part in df.partitions:\n#  l = part.shape[0].compute()\n#  lengths.append(l)\n#  #print(l, part.shape[1])","d9f08831":"#drop_cols =['date', 'visitId', 'fullVisitorId', 'totals.transactionRevenue']","bb1f2e1f":"#df.head()","dad2ef2a":"#lcols = [\"customDimensions\", \"channelGrouping\", \"visitStartTime\", \"device.browser\", \"device.deviceCategory\",\n#        \"device.operatingSystem\", \"geoNetwork.city\", \"geoNetwork.continent\", \"geoNetwork.metro\", \"geoNetwork.networkDomain\",\n#        \"geoNetwork.region\", \"geoNetwork.subContinent\", \"trafficSource.adContent\", \"trafficSource.adwordsClickInfo.adNetworkType\", \n#         \"trafficSource.adwordsClickInfo.page\", \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n#        \"trafficSource.keyword\", \"trafficSource.medium\", \"trafficSource.referralPath\", \"trafficSource.source\"]","bcf6fa63":"#lcols_index = []\n#for col_ in lcols:\n#    lcols_index.append(np.where(col_ == df.columns)[0][0])","4f0d3d0c":"# You cannot assign an array to a column of DataFrame, it has to be a Series.\n# So, we are going to convert our data to arrays, scale and learn from array only.\n#X, y = df.drop(drop_cols, axis=1).to_dask_array(lengths=lengths) , df['totals.transactionRevenue'].to_dask_array(lengths=lengths)","4b8bd61e":"#del df\n#gc.collect()","b20b0b26":"#from dask_ml.preprocessing import RobustScaler, LabelEncoder\n#Xo = dask.array.zeros((X.shape[0],1), chunks=(200000,1))\n#for i in range(X.shape[1]):\n#    if i in lcols_index:\n#        lb = LabelEncoder()\n#        temp = lb.fit_transform(X[:i])\n#        Xo = dask.array.concatenate([Xo, temp], axis=1)","2fb708b5":"#X = Xo[:, 1:]","3e70dce1":"#Xo = dask.array.zeros((X.shape[0],1), chunks=(200000,1))\n#for i in range(len(X.shape[1])):\n#  if i == X.shape[1]-1:\n#    rsc = RobustScaler()\n#    y = rsc.fit_transform(y.reshape(-1, 1)).reshape(1, -1)[0]\n#  else:\n#    rsc = RobustScaler()\n#    temp = rsc.fit_transform(X[:,i].reshape(-1, 1))\n#    Xo = dask.array.concatenate([Xo, temp], axis=1)","1921ac3d":"#del X\n#gc.collect()","1e15a074":"#Xo = Xo[:, 1:]","c19b6a03":"#Xo = Xo.rechunk({1: Xo.shape[1]})\n#Xo = Xo.rechunk({0: 200000})\n#y = y.rechunk({0: 200000})","3ad8c879":"#tr_len = 0.8*Xo.shape[0]\n#xtrain, ytrain = Xo[:tr_len], y[:tr_len]\n#xvalid, yvalid = Xo[tr_len:], y[tr_len:]\n#xtrain.shape, ytrain.shape, xvalid.shape, yvalid.shape","d7e9393b":"#from dask_ml.linear_model import LinearRegression","1dd4275c":"#est = LinearRegression()","0a4c3f64":"#est.fit(xtrain, y=ytrain)","d523410f":"#preds = est.predict(xvalid)","3ccc3d14":"#preds[0:10].compute()","240233ae":"**Note:**\n\nIf even one column of dataset is larger than your memory, then you can open that column incrementally (by using `chunksize` parameter in `pd.read_csv`), applying some transformation (maybe keeping it in a `np.float32` if its a number or maybe keeping only relevant part of a string and throwing all other away.)\n\nBut if that column is not fitting into your memory, you can use `Dask`. (see last section)","7eea728a":"### Data Exploration:","0d4e0313":"You cannot re-input scaled values from your dataframe directly into its columns (It has to be a series). So, we will Scale Array and directly use it to train our model.","5c506d0a":"These functions are taken from [here](https:\/\/www.kaggle.com\/prashantkikani\/teach-lightgbm-to-sum-predictions-fe).","4ba2a78e":"This kernel goes with a medium posts I write, on how to *`Tackle`* problems faced in DataScience,published in Towards Data Science. Here are all the posts:\n1. [How to handle BigData Files on Low Memory](https:\/\/towardsdatascience.com\/how-to-learn-from-bigdata-files-on-low-memory-incremental-learning-d377282d38ff)","b3a61313":"# Import","83d168ce":"This method is taken from [here](https:\/\/www.kaggle.com\/mlisovyi\/bigdata-dask-pandas-flat-json-trim-data-upd).","a82cf151":"# Incremental Learning","009424c2":"This function is taken from [here](https:\/\/www.kaggle.com\/ogrellier\/i-have-seen-the-future).","857ed599":"**NOTE:**\n\n`Dask` doesn't have equivalent fucntion of `pandas`'s `json_normalize`. But we can use `Dask`'s `to_bag` function and `bag`'s capability to handle JSON to our advantage.","71c05e4d":"### Preprocessing method:","dbb25c9e":"### Method 1: Using Pandas","87563315":"**Note:**\n\nOne thing to notice here is that we fitted methods (like LabelEncoder's, Scalars's etc.) during exploration to **whole** data column and we will use that to transform data at every incremental step here. Because, in each batch, there might be some data missing and if we had used different LabelEncoder's, Scaler's etc. for each batch, these methods wouldn't have given same result for same category (say). That's why to be on safe side, we already have fitted to whole columns during exploration.","b2b6ca42":"To make blocks for both of equal size. Otherwise you might get broadcast error.\n\n","26ca842d":"# Method 2: Using Dask:\n\n\n\nFor intro on Dask read my post [here](https:\/\/towardsdatascience.com\/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef).\n\n**Note:**\nYou should only use `Dask` in case of Big Data, where it is not able to fit in your memory.","98f4d721":"# Initial"}}