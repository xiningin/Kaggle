{"cell_type":{"789c0d18":"code","36ed5d5b":"code","646f73fa":"code","0a4d6070":"code","295efdce":"code","d2efde42":"code","de22ecdc":"code","14e30a8e":"code","2f5c70f9":"code","99d525d8":"code","6e3e824b":"code","4342e5bc":"code","098e00f4":"code","e16b2893":"code","ea45dd69":"code","c43e6259":"code","be1dd6ef":"code","a1963e94":"code","a886fc24":"code","6a930f5b":"code","eac2e5f4":"code","13557997":"code","a30272c8":"code","9d190b69":"code","b8a20734":"code","a2a05e64":"code","beed4681":"code","488543b7":"code","bdc38920":"code","6a455acd":"code","48ab7d97":"code","e6f01d75":"code","21a8ffa5":"code","4a183fc6":"code","5abe9380":"code","a215e7da":"code","66e5824f":"code","715b26d7":"code","d40f6b4a":"code","ea0ebbbe":"code","92d28c3d":"code","31a71a78":"code","eb65b991":"code","49223d50":"code","4d39bb19":"code","762e39a8":"code","c1e35ac1":"code","29e3b6b9":"code","7745bf36":"code","7ddb0f84":"code","e29b2cc1":"code","aa027ab5":"code","833ae38d":"code","4a4a2390":"markdown","7bf2dc0f":"markdown","833b9956":"markdown","8a174e2c":"markdown","c23bb339":"markdown","f62c1c38":"markdown","1c6a5f66":"markdown","edfa314e":"markdown","ce6818bb":"markdown","20bed15a":"markdown","46a628f1":"markdown","f372e4f4":"markdown","968a8ecc":"markdown","6575daab":"markdown","d093936f":"markdown","1a0cc575":"markdown","285d7907":"markdown","26e54c60":"markdown","a6787c60":"markdown","f46e9f01":"markdown","ede5ee01":"markdown","f381405c":"markdown","3b8355f2":"markdown","9ba9439b":"markdown","4a039ea4":"markdown","8ba9840e":"markdown","fc9302a4":"markdown","c4f1afb4":"markdown","15d9061b":"markdown","c395b2b6":"markdown","b9976e12":"markdown","c3a9c22d":"markdown","90b63ddf":"markdown","1dfea57b":"markdown","bf43d096":"markdown","c720fb76":"markdown","5bb7fa83":"markdown","ea126464":"markdown","b0b4660a":"markdown","003ebe5f":"markdown","d6e71c7a":"markdown","a78ea24d":"markdown","33153a2e":"markdown","99f1b70f":"markdown","fb516136":"markdown","b286a656":"markdown","cd369bc6":"markdown","e50a27b6":"markdown","2f779469":"markdown","e05ebffc":"markdown","09a87924":"markdown","be8469d3":"markdown","b8edb73c":"markdown","757e70e4":"markdown","1ab25603":"markdown","178ba624":"markdown","5d618f9a":"markdown","831ae149":"markdown","80619a46":"markdown","603e2c00":"markdown","a4b729f5":"markdown","8f79628c":"markdown","ca3a476c":"markdown","30b842c6":"markdown","345c7713":"markdown","da51dd44":"markdown","2bfaa8e8":"markdown","648bf623":"markdown","36628ce7":"markdown","6a25e095":"markdown","5e68992d":"markdown","b588d5ac":"markdown","544bd824":"markdown","e2488901":"markdown","bf6a523e":"markdown","bcb6bce4":"markdown","d1805458":"markdown","7dbd9575":"markdown","14679843":"markdown","0bfc2a6c":"markdown","297291d6":"markdown","3ac2406e":"markdown","31634b57":"markdown","c7566c4e":"markdown","84a34595":"markdown","507fb0fa":"markdown","8b149fc9":"markdown","3ed03ffb":"markdown","9190f32c":"markdown","4d0e6284":"markdown","e8b8c0ea":"markdown","105b4cf9":"markdown","d5e7cd45":"markdown"},"source":{"789c0d18":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\n\n#libraries for NLP\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\nfrom IPython.display import HTML\n!pip install chart_studio\nimport plotly\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nplotly.offline.init_notebook_mode(connected=True)\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\nimport plotly.express as px\nfrom collections import defaultdict\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\n\nimport warnings\nwarnings.filterwarnings('ignore')","36ed5d5b":"data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","646f73fa":"data.head(10)","0a4d6070":"data.info()","295efdce":"data = data[['text','target']]\ndata.head()","d2efde42":"fig = px.bar(x=[\"0\",\"1\"], y=data[\"target\"].value_counts(),color=[\"red\", \"goldenrod\"])\n\n#Change this value for bar widths\nfor dt in fig.data:\n    dt[\"width\"] = 0.4 \n\nfig.update_layout(\n    title_text = \"Counts for Disaster and Non-Disaster Tweets\",\n    title_x=0.5,\n    width=800,\n    height=550,\n    xaxis_title=\"Targets\",\n    yaxis_title=\"Count\",\n    showlegend=False\n).show()\n\n# py.plot(fig,filename='Counts for Disaster and Non-Disaster Tweets',auto_open=False,show_link=False)","de22ecdc":"from plotly.subplots import make_subplots\n\nword_len_dis = data[data['target']==1]['text'].str.split().map(lambda x : len(x))\n\nword_len_non_dis = data[data['target']==0]['text'].str.split().map(lambda x : len(x))\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n\nfig.add_trace(\n            go.Histogram(x=word_len_dis,marker_line=dict(color='black'),marker_line_width=1.2),\n            row=1, col=1\n).add_trace(\n            go.Histogram(x=word_len_non_dis,marker_line=dict(color='black'),marker_line_width=1.2),\n            row=1, col=2\n).update_layout(title_text=\"Length of words in Tweets\",title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Length of words in Tweets',auto_open=False,show_link=False)","14e30a8e":"def avgwordlen(strlist):\n    sum=[]\n    for i in strlist:\n        sum.append(len(i))\n    return sum\n\navgword_len_dis = data[data['target']==1]['text'].str.split().apply(avgwordlen).map(lambda x: np.mean(x))\n\navgword_len_non_dis = data[data['target']==0]['text'].str.split().apply(avgwordlen).map(lambda x: np.mean(x))\n\ngroup_labels = ['Disaster', 'Non-Disaster']\ncolors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)']\n\nfig = ff.create_distplot([avgword_len_dis, avgword_len_non_dis], group_labels, bin_size=.2, colors=colors,)\n\nfig.update_layout(title_text=\"Average word length in tweets\",title_x=0.5,xaxis_title=\"Text\",yaxis_title=\"Density\").show()\n\n# py.plot(fig,filename='Average word length in tweets',auto_open=False,show_link=False)","2f5c70f9":"def create_corpus(target):\n    corpus = []\n    for i in data[data['target']==target]['text'].str.split():\n        for x in i:\n            corpus.append(x)\n    return corpus","99d525d8":"values_list = []\n\ndef analyze_stopwords(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        dic[word] += 1\n    \n    top = sorted(dic.items(),key = lambda x: x[1],reverse=True)[:10]\n    x_items,y_values = zip(*top)\n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing stopwords for 0 and 1 target labels\nanalyze_stopwords(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n\nfig.add_trace(\n      go.Bar(x=values_list[1],y=values_list[0],orientation='h',marker=dict(color= 'rgba(152, 255, 74,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=values_list[3],y=values_list[2],orientation='h',marker=dict(color= 'rgba(255, 143, 92,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=2\n).update_layout(title_text=\"Top stop words in the text\",title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Top stop words in the text',auto_open=False,show_link=False)","6e3e824b":"#The above Bar Charts displays the top 10 stop words in tweets where the occurs the most in both groups\n# Anaysing Punctuations\nfrom string import punctuation\nvalues_list = []\ndef analyze_punctuations(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        if word in punctuation:\n            dic[word] += 1 \n    x_items, y_values = zip(*dic.items())\n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing punctuations for 0 and 1 target labels\nanalyze_punctuations(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n  \nfig.add_trace(\n      go.Bar(x=values_list[0],y=values_list[1],\n             marker=dict(color= 'rgba(196, 94, 255,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=values_list[2],y=values_list[3],\n             marker=dict(color= 'rgba(255, 163, 102,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=2\n).update_layout(title_text=\"Top Punctuations in the text\",title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Top Punctuations in the text',auto_open=False,show_link=False)","4342e5bc":"stemmer = SnowballStemmer(\"english\")\n\ndef preprocess_data(data):\n    \n    #removal of url\n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+|http?:\/\/\\S+',' ',data) \n    \n    #decontraction\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    \n    #removal of html tags\n    text = re.sub(r'<.*?>',' ',text) \n    \n    # Match all digits in the string and replace them by empty string\n    text = re.sub(r'[0-9]', '', text)\n    text = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',text)\n    \n    # filtering out miscellaneous text.\n    text = re.sub('[^a-zA-Z]',' ',text) \n    text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n    \n    # remove mentions\n    text = re.sub('@\\S+', '', text)  \n    \n    # remove punctuations\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~\"\"\"), '', text)  \n    \n\n    # Lowering all the words in text\n    text = text.lower()\n    text = text.split()\n    \n    text = [stemmer.stem(words) for words in text if words not in stopwords.words('english')]\n    \n    # Removal of words with length<2\n    text = [i for i in text if len(i)>2] \n    text = ' '.join(text)\n    return text\n\ndata[\"Cleaned_text\"] = data[\"text\"].apply(preprocess_data)","098e00f4":"data.head()","e16b2893":"def wordcloud(data,title):\n    words = ' '.join(data['Cleaned_text'].astype('str').tolist())\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(stopwords = stopwords,width= 512, height = 512).generate(words)\n    plt.figure(figsize=(10,8),frameon=True)\n    plt.imshow(wc)\n    plt.axis('off')\n    plt.title(title,fontsize=20)\n    plt.show()\n    \ndata_disaster = data[data['target'] == 1]\ndata_non_disaster = data[data['target'] == 0]","ea45dd69":"wordcloud(data_disaster,\"Disaster Tweets\")","c43e6259":"wordcloud(data_non_disaster,\"Non-Disaster Tweets\")","be1dd6ef":"common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest'\n                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']\n\ndef text_cleaning(data):\n    return ' '.join(i for i in data.split() if i not in common_words)\n\ndata[\"Cleaned_text\"] = data[\"Cleaned_text\"].apply(text_cleaning)","a1963e94":"def top_ngrams(data,n,grams):\n    count_vec = CountVectorizer(ngram_range=(grams,grams)).fit(data)\n    bow = count_vec.transform(data)\n    add_words = bow.sum(axis=0)\n    word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True) \n    return word_freq[:n]","a886fc24":"common_uni = top_ngrams(data[\"Cleaned_text\"],10,1)\ncommon_bi = top_ngrams(data[\"Cleaned_text\"],10,2)\ncommon_tri = top_ngrams(data[\"Cleaned_text\"],10,3)\ncommon_uni_df = pd.DataFrame(common_uni,columns=['word','freq'])\ncommon_bi_df = pd.DataFrame(common_bi,columns=['word','freq'])\ncommon_tri_df = pd.DataFrame(common_tri,columns=['word','freq'])","6a930f5b":"fig = make_subplots(rows=3, cols=1,subplot_titles=(\"Top 20 Unigrams in Text\", \"Top 20 Bigrams in Text\",\"Top 20 Trigrams in Text\"))\n  \nfig.add_trace(\n      go.Bar(x=common_uni_df[\"word\"],y=common_uni_df[\"freq\"],\n             marker=dict(color= 'rgba(255, 170, 59,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=common_bi_df[\"word\"],y=common_bi_df[\"freq\"],\n             marker=dict(color= 'rgba(89, 255, 147,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=2, col=1\n).add_trace(\n      go.Bar(x=common_tri_df[\"word\"],y=common_tri_df[\"freq\"],\n             marker=dict(color= 'rgba(89, 153, 255,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=3, col=1\n).update_layout(title_text=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\",\n                title_x=0.5,showlegend=False,width=800,height=1600,).update_xaxes(tickangle=-90).show()\n\n# py.plot(fig,filename='Visualization of Top 20 Unigrams, Bigrams and Trigrams',auto_open=False,show_link=False)","eac2e5f4":"X_inp_clean = data['Cleaned_text']\nX_inp_original = data['text']\ny_inp = data['target']","13557997":"X_train, X_valid, y_train, y_valid = train_test_split(X_inp_clean, y_inp, test_size=0.2, random_state=42, stratify=y_inp)\ny_train = np.array(y_train)\ny_valid = np.array(y_valid)","a30272c8":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","9d190b69":"def encoding(train_data,valid_data,bow=False,n=1,tf_idf=False):\n    if bow==True:\n        cv = CountVectorizer(ngram_range=(n,n))\n        cv_df_train = cv.fit_transform(train_data).toarray()\n        train_df = pd.DataFrame(cv_df_train,columns=cv.get_feature_names())\n        cv_df_valid = cv.transform(valid_data).toarray()\n        valid_df = pd.DataFrame(cv_df_valid,columns=cv.get_feature_names())\n        \n    elif tf_idf==True:\n        \n        tfidf = TfidfVectorizer(ngram_range=(n, n), use_idf=1,smooth_idf=1,sublinear_tf=1)    \n        tf_df_train = tfidf.fit_transform(train_data).toarray()\n        train_df = pd.DataFrame(tf_df_train,columns=tfidf.get_feature_names())\n        tf_df_valid = tfidf.transform(valid_data).toarray()\n        valid_df = pd.DataFrame(tf_df_valid,columns=tfidf.get_feature_names())\n        \n    return train_df,valid_df ","b8a20734":"X_train_bow1 , X_valid_bow1 = encoding(X_train,X_valid,bow=True) \nX_train_bow2 , X_valid_bow2 = encoding(X_train,X_valid,bow=True,n=2) \nX_train_bow3 , X_valid_bow3 = encoding(X_train,X_valid,bow=True,n=3) \nX_train_tfidf1 , X_valid_tfidf1 = encoding(X_train,X_valid,tf_idf=True) \nX_train_tfidf2 , X_valid_tfidf2 = encoding(X_train,X_valid,tf_idf=True,n=2) \nX_train_tfidf3 , X_valid_tfidf3 = encoding(X_train,X_valid,tf_idf=True,n=3)","a2a05e64":"def c_report(y_true,y_pred):\n    print(\"Classifictaion Report\")\n    print(classification_report(y_true, y_pred))\n    acc_scr = accuracy_score(y_true, y_pred)\n    print(\"Accuracy : \"+ str(acc_scr))\n    return acc_scr\n\ndef plot_cm(y_true,y_pred,cmap = \"Blues\"):\n    mtx = confusion_matrix(y_true, y_pred)\n    sns.heatmap(mtx, annot = True, fmt='d', linewidth=0.5,\n               cmap=cmap, cbar = False)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')","beed4681":"model_bow1_logreg = LogisticRegression()\nmodel_bow1_logreg.fit(X_train_bow1,y_train)\npred_bow1_logreg = model_bow1_logreg.predict(X_valid_bow1)","488543b7":"acc_bow1_logreg = c_report(y_valid,pred_bow1_logreg)\nplot_cm(y_valid,pred_bow1_logreg)","bdc38920":"model_bow2_logreg = LogisticRegression()\nmodel_bow2_logreg.fit(X_train_bow2,y_train)\npred_bow2_logreg = model_bow2_logreg.predict(X_valid_bow2)","6a455acd":"acc_bow2_logreg = c_report(y_valid,pred_bow2_logreg)\nplot_cm(y_valid,pred_bow2_logreg)","48ab7d97":"model_bow3_logreg = LogisticRegression()\nmodel_bow3_logreg.fit(X_train_bow3,y_train)\npred_bow3_logreg = model_bow3_logreg.predict(X_valid_bow3)","e6f01d75":"acc_bow3_logreg = c_report(y_valid,pred_bow3_logreg)\nplot_cm(y_valid,pred_bow3_logreg)","21a8ffa5":"model_tfidf1_logreg = LogisticRegression(C=1.0)\nmodel_tfidf1_logreg.fit(X_train_tfidf1,y_train)\npred_tfidf1_logreg = model_tfidf1_logreg.predict(X_valid_tfidf1)","4a183fc6":"acc_tfidf1_logreg = c_report(y_valid,pred_tfidf1_logreg)\nplot_cm(y_valid,pred_tfidf1_logreg)","5abe9380":"model_bow1_NB = MultinomialNB(alpha=0.7)\nmodel_bow1_NB.fit(X_train_bow1,y_train)\npred_bow1_NB = model_bow1_NB.predict(X_valid_bow1)","a215e7da":"acc_bow1_NB = c_report(y_valid,pred_bow1_NB)\nplot_cm(y_valid,pred_bow1_NB)","66e5824f":"model_tfidf1_NB = MultinomialNB(alpha=0.7)\nmodel_tfidf1_NB.fit(X_train_tfidf1,y_train)\npred_tfidf1_NB = model_tfidf1_NB.predict(X_valid_tfidf1)","715b26d7":"acc_tfidf1_NB = c_report(y_valid,pred_tfidf1_NB)\nplot_cm(y_valid,pred_tfidf1_NB)","d40f6b4a":"model_tfidf1_RFC = RandomForestClassifier()\nmodel_tfidf1_RFC.fit(X_train_tfidf1,y_train)\npred_tfidf1_RFC = model_tfidf1_RFC.predict(X_valid_tfidf1)","ea0ebbbe":"acc_tfidf1_RFC = c_report(y_valid,pred_tfidf1_RFC)\nplot_cm(y_valid,pred_tfidf1_RFC)","92d28c3d":"model_tfidf1_XGB = XGBClassifier(eval_metric='mlogloss')\nmodel_tfidf1_XGB.fit(X_train_tfidf1,y_train)\npred_tfidf1_XGB = model_tfidf1_XGB.predict(X_valid_tfidf1)","31a71a78":"acc_tfidf1_XGB = c_report(y_valid,pred_tfidf1_XGB)\nplot_cm(y_valid,pred_tfidf1_XGB)","eb65b991":"model_tfidf1_CBC = CatBoostClassifier(iterations=100)\nmodel_tfidf1_CBC.fit(X_train_tfidf1,y_train)\npred_tfidf1_CBC = model_tfidf1_CBC.predict(X_valid_tfidf1)\n","49223d50":"acc_tfidf1_CBC = c_report(y_valid,pred_tfidf1_CBC)\nplot_cm(y_valid,pred_tfidf1_CBC)","4d39bb19":"model_tfidf1_SVC = SVC(kernel='linear', degree=3, gamma='auto')\nmodel_tfidf1_SVC.fit(X_train_tfidf1,y_train)\npred_tfidf1_SVC = model_tfidf1_SVC.predict(X_valid_tfidf1)","762e39a8":"acc_tfidf1_SVC = c_report(y_valid,pred_tfidf1_SVC)\nplot_cm(y_valid,pred_tfidf1_SVC)","c1e35ac1":"estimators = []\nestimators.append(('LR', \n                  LogisticRegression()))\nestimators.append(('NB', MultinomialNB(alpha=0.7)))\nestimators.append(('XBG', XGBClassifier(eval_metric='mlogloss')))\n\nmodel_tfidf1_VC = VotingClassifier(estimators=estimators,voting='soft')\nmodel_tfidf1_VC.fit(X_train_tfidf1,y_train)\npred_tfidf1_VC = model_tfidf1_VC.predict(X_valid_tfidf1)","29e3b6b9":"acc_tfidf1_VC = c_report(y_valid,pred_tfidf1_VC)\nplot_cm(y_valid,pred_tfidf1_VC,cmap = \"Greens\")","7745bf36":"results = pd.DataFrame([[\"Logistic Regression BoW1\",acc_bow1_logreg],[\"Logistic Regression BoW2\",acc_bow2_logreg],\n                       [\"Logistic Regression BoW3\",acc_bow3_logreg],[\"Logistic Regression Tf-Idf1\",acc_tfidf1_logreg],\n                       [\"Naive Bayes Tf-Idf1\",acc_tfidf1_NB],[\"Random Forest Tf-Idf1\",acc_tfidf1_RFC],\n                       [\"XGBClassifier Tf-Idf1\",acc_tfidf1_XGB],[\"CatBoost Tf-Idf1\",acc_tfidf1_CBC],\n                        [\"SVC Tf-Idf1\",acc_tfidf1_SVC],[\"Voting Tf-Idf1\",acc_tfidf1_VC]],\n                       columns = [\"Models\",\"Accuracy Score\"]).sort_values(by='Accuracy Score',ascending=False)\n\nresults.style.background_gradient(cmap='Blues')","7ddb0f84":"#lets import test data\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest[\"Cleaned_text\"] = test[\"text\"].apply(preprocess_data)","e29b2cc1":"test[\"Cleaned_text\"] = test[\"Cleaned_text\"].apply(text_cleaning)\ntfidf = TfidfVectorizer(ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1)    \ntf_df_data = tfidf.fit_transform(X_inp_clean).toarray()\ndata_df = pd.DataFrame(tf_df_data,columns=tfidf.get_feature_names())\ntf_df_test = tfidf.transform(test['Cleaned_text']).toarray()\ntest_df = pd.DataFrame(tf_df_test,columns=tfidf.get_feature_names())","aa027ab5":"model_VC = VotingClassifier(estimators=estimators,voting='soft')\nmodel_VC.fit(data_df,y_inp)\nsubmission = model_VC.predict(test_df)","833ae38d":"predictions_test = pd.DataFrame(submission)\ntest_id = pd.DataFrame(test[\"id\"])\nsubmission = pd.concat([test_id,predictions_test],axis=1)\nsubmission.columns = [\"id\",\"target\"]\nsubmission.to_csv(\"Submission.csv\",index=False)","4a4a2390":"Reading [data](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data) and choosing important columns using [pandas](https:\/\/pandas.pydata.org\/)","7bf2dc0f":"## 4.3 Visualising words inside Fake Disaster Tweets","833b9956":"\n### What is a corpus?\n\nIn linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts.   \nSuch collections may be formed of a single language of texts, or can span multiple languages\n   \nFunction for creating sample [corpus](https:\/\/21centurytext.wordpress.com\/home-2\/special-section-window-to-corpus\/what-is-corpus\/) for further analysis.    ","8a174e2c":"### About Voting Classifier\n\nA Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on    \ntheir highest probability of chosen class as the output. It simply aggregates the findings of each classifier passed into Voting Classifier    \nand predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n\n#### Voting Classifier supports two types of votings :  \n\n\n* **Hard Voting** : In hard voting, the predicted output class is a class with the highest majority of votes i.e the class    which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n\n\n* **Soft Voting** : In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier\n \n \nNow let's create a [VotingClassifier](https:\/\/www.geeksforgeeks.org\/ml-voting-classifier-using-sklearn\/) with soft voting and train it ","c23bb339":"## 2.5 Visualising most common punctuations in the text data","f62c1c38":"## 5.1 Spliting original data after cleaning ","1c6a5f66":"## If you like my work then make sure to upvote it , any kind of suggestions are welcome ","edfa314e":"# 8. Conclusion","ce6818bb":"### About CatBoostClassifier\n\nCatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google\u2019s TensorFlow and Apple\u2019s Core ML. It can work with diverse data types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy.    \n\nIt yields state-of-the-art results without extensive data training typically required by other machine learning methods. \n   \n\u201cBoost\u201d comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library.      \nGradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.\n\n\nNow let's create a [CatBoostClassifier](https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostclassifier.html) model with 100 iterations and training it","20bed15a":"Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=1) model","46a628f1":"## 4.5 Analysing top 10 N-grams where N is 1,2,3","f372e4f4":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/15\/?share_key=9JgPThmm677jJmNjJTc0BZ\" target=\"_blank\" title=\"Top Punctuations in the text\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/15.png?share_key=9JgPThmm677jJmNjJTc0BZ\" alt=\"Top Punctuations in the text\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:15\" sharekey-plotly=\"9JgPThmm677jJmNjJTc0BZ\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>\n","968a8ecc":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/5\/?share_key=tfNQPMyUblqOh7JL1sEiqW\" target=\"_blank\" title=\"Average word length in tweets\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/5.png?share_key=tfNQPMyUblqOh7JL1sEiqW\" alt=\"Average word length in tweets\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:5\" sharekey-plotly=\"tfNQPMyUblqOh7JL1sEiqW\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>\n","6575daab":"#  Table of Contents\n\n\n 1. Dependancies and Dataset\n\n 2. Data Exploration\n \n 3. Data Cleaning\n\n 4. Extra Data Exploration and Analysis on Cleaned Text\n\n 5. Spliting data\n\n 6. Machine Learning Models\n\n 7. Comparing accuracies of all models\n \n 8. Conclusion\n","d093936f":"## 2.1 Visualising counts of real and fake tweets","1a0cc575":"### What is BoW?   \nBoW stands for \"*bag of words*\" which is a representation of text that describes the occurrence of words within a document.   \nWe just keep track of word counts and disregard the grammatical details and the word order.   \nIt is called a \u201cbag\u201d of words because any information about the order or structure of words in the document is discarded. \nThe model is only concerned with whether known words occur in the document, not where in the document.\n    \n    \n### What is TF-IDF?\nTF-IDF which means Term Frequency and Inverse Document Frequency, is a scoring measure widely used in information retrieval (IR) or summarization.     \nTF-IDF is intended to reflect how relevant a term is in a given document. It is a technique in Natural Language Processing for converting words in Vectors and with some semantic information and it gives weighted to uncommon words , used in various NLP applications.    \n\nFor [BoW](https:\/\/www.mygreatlearning.com\/blog\/bag-of-words\/) approach we use scikit-learn's [CountVectorizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) and for [TF-IDF](https:\/\/towardsdatascience.com\/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76) we use [TfidfVectorizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html)","285d7907":"From the [distplot](https:\/\/plotly.com\/python\/distplot\/), average word countss for real disaster tweets are found to be in the range(5-7.5)                 \nwhile for fake disaster tweets are in the range of (4-6).","26e54c60":"Analyzing lengths of words in a tweets according to it being real or fake target value by ploting [histograms](https:\/\/plotly.com\/python\/histograms\/)","a6787c60":"Printing classification report and ploting confusion matrix for VotingClasssifier","f46e9f01":"# 5. Data Preprocessing ","ede5ee01":"## 6.1 Logistic Regression","f381405c":"## 2. Data Exploration","3b8355f2":"## 2.4 Visualising most common stop words in the text data","9ba9439b":"From the plot we can say that the number of words in the tweets ranges from 2 to 30 in both cases","4a039ea4":"Creating a MultinomialNB model and training it with TF-IDF approach","8ba9840e":"## 1.2 Reading and preparation of data","fc9302a4":"## 4.1  Creating function and data for visualising words","c4f1afb4":"## 5.2 Creating function to encode data using BoW or TF-IDF","15d9061b":"The [Bar Charts](https:\/\/plotly.com\/python\/bar-charts\/) displays the top 10 stop words in tweets where **'the'** is most frequent in both groups","c395b2b6":"Let's plot the counts of values under the target column","b9976e12":"# 1. Dependancies and Dataset","c3a9c22d":"## 6.6 Support Vector CLassifier ","90b63ddf":"## 2.3 Visualising average word lengths of tweets","1dfea57b":"Displaying Cleaned Data ","bf43d096":"We encode our data in all possible combinations provided by our function","c720fb76":"## 4.6 Visualising top 10 N-grams for N = 1, 2, 3","5bb7fa83":"Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(TF-IDF,n-grams=1) model","ea126464":"## 3.1 Removing unwanted text using regular expressions","b0b4660a":"# 3. Data Cleaning","003ebe5f":"Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=3) model","d6e71c7a":"## 5.3 Encoding training and validation data","a78ea24d":"## 6.2 Multinomial Naive Bayes","33153a2e":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/17\/?share_key=rHBUmASeWITErHR7rEdZqJ\" target=\"_blank\" title=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/17.png?share_key=rHBUmASeWITErHR7rEdZqJ\" alt=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:17\" sharekey-plotly=\"rHBUmASeWITErHR7rEdZqJ\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>\n","99f1b70f":"Using [scikit-learn's train_test_split](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) to split the data into training and validation dataset","fb516136":"we can see that most common words in disaster tweets are fire,storm,flood , police etc. ","b286a656":"Displaying first 10 rows of our data using [DataFrame.head()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.head.html)","cd369bc6":"\n<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/1\/?share_key=hgjA8Zkl35RjZtywNHe0jm\" target=\"_blank\" title=\"Counts for Disaster and Non-Disaster Tweets\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/1.png?share_key=hgjA8Zkl35RjZtywNHe0jm\" alt=\"Counts for Disaster and Non-Disaster Tweets\" style=\"max-width: 100%;width: 1000px;\"  width=\"1000\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:1\" sharekey-plotly=\"hgjA8Zkl35RjZtywNHe0jm\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>","e50a27b6":"### About Random Forest Classifier   \n\nRandom forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n\nRandom forests has a variety of applications, such as recommendation engines, image classification and feature selection. It can be used to classify loyal loan applicants, identify fraudulent activity and predict diseases. It lies at the base of the Boruta algorithm, which selects important features in a dataset.   \n\nNow let's create a [RandomForestClassifier](https:\/\/www.geeksforgeeks.org\/random-forest-classifier-using-scikit-learn\/) model and train it.","2f779469":"## 6.4 eXtreme Gradient Boosting Classifier","e05ebffc":"### What is Stemming?  \nStemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.   \nStemming is important in natural language understanding (NLU) and natural language processing (NLP). Here we use SnowballStemmer.\n\nFunction for cleaning the data, we use [RegEx](https:\/\/docs.python.org\/3\/library\/re.html) i.e. re python library and [SnowballStemmer()](https:\/\/www.nltk.org\/_modules\/nltk\/stem\/snowball.html) to stem the words.","09a87924":"## 6.7 Voting Classifier","be8469d3":"# 4. Extra Data Exploration and Analysis on Cleaned Text","b8edb73c":"## 4.4 Removing unwanted words with high frequency","757e70e4":"From the above results it's clear that using n = 1 will always give us more accuray,  \nnow let's use tfidf approach with n = 1 to train our Logistic Regression model","1ab25603":"### What do you mean by N-grams?  \nN-grams of texts are extensively used in text mining and natural language processing tasks.     They are basically a set of co-occurring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios).  \n\nFor example, for the sentence \u201cThe cow jumps over the moon\u201d. If N=2 (known as bigrams), then the ngrams would be:  \n* the cow \n* cow jumps \n* jumps over \n* over the \n* the moon\n\nBelow we perform [N-grams](https:\/\/en.wikipedia.org\/wiki\/N-gram#:~:text=In%20the%20fields%20of%20computational,a%20text%20or%20speech%20corpus.) analysis on cleaned data","178ba624":"love,new,time etc are the most common words as we can see in wordcloud of Non-disaster tweets","5d618f9a":"Printing classification report and ploting confusion matrix for the predictions made by the above model","831ae149":"## 6.5 CatBoostClassifier","80619a46":"We only use text and target column of dataset for rest of our work as there lot's of null values inside other columns","603e2c00":"### What is a classification report?\nA Classification report is used to measure the quality of predictions from a classification algorithm.   \nThe report shows the main classification metrics precision, recall and f1-score on a per-class basis. The metrics are calculated by using true and false positives, true and false negatives. Positive and negative in this case are generic names for the predicted classes. \n### What is a confusion matrix?\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.  \n\n#### In a confusion matrix there are 4 basic terminologies :\n* **true positives (TP)** : We predicted yes (they are real tweets), and they are actually real.\n* **true negatives (TN)** : We predicted no, and they are fake.\n* **false positives (FP)**: We predicted yes, but they are't actually real. (Also known as a \"Type I error.\")\n* **false negatives (FN)**: We predicted no, but they are real. (Also known as a \"Type II error.\")\n\nNow let's create functions to display model's [classification report](https:\/\/datascience.stackexchange.com\/questions\/64441\/how-to-interpret-classification-report-of-scikit-learn) and [confusion matrix](https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/)","a4b729f5":"From Logistic Regression we saw n-grams = 1 gives the best results ","8f79628c":"# 7. Comparing the Accuracy of all models","ca3a476c":"Now training another Logistic Regression model with n-grams=2 and BoW ","30b842c6":"## 4.2 Visualising words inside Real Disaster Tweets","345c7713":"### About Logistic Regression   \n\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).\n\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n\nNow let's create a [Logistic Regression](https:\/\/medium.com\/analytics-vidhya\/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640) model and train it.","da51dd44":"### About Multinomial Naive Bayes  \n\nMultinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output.\n\nNaive Bayes classifier is a collection of many algorithms where all the algorithms share one common principle, and that is each feature being classified is not related to any other feature. The presence or absence of a feature does not affect the presence or absence of the other feature.\n    \nNow let's create [MultinomialNB](https:\/\/www.upgrad.com\/blog\/multinomial-naive-bayes-explained\/) model and train it.","2bfaa8e8":"# 6. Training and tuning Machine Learining Models","648bf623":"Printing classification report and ploting confusion matrix for predictions of RandomForestClassifier model","36628ce7":"Creating data of top 10 n-grams for n = 1, 2, 3","6a25e095":"# Project Description\n\n[Twitter](https:\/\/twitter.com\/?lang=en) has become an important communication channel in times of emergency.   \nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time.    \nBecause of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\n# Objective     \n\n[Sentiment analysis](https:\/\/en.wikipedia.org\/wiki\/Sentiment_analysis) is a common use case of [NLP](https:\/\/machinelearningmastery.com\/natural-language-processing\/) where the idea is to classify the tweet as positive, negative or neutral depending upon the text in the tweet.     \nThis problem goes a way ahead and expects us to also determine the words in the tweet which decide the polarity of the tweet.\n\nIn this project [Machine Learning](https:\/\/www.geeksforgeeks.org\/machine-learning\/) models are implemented for predicting that a tweet regarding a disaster is real or fake    \nWhole code below is in [Python](https:\/\/www.python.org\/) using various libraries. Open source library [Scikit-Learn](https:\/\/scikit-learn.org\/) is used for creating the models.\n\n<p align=\"center\">\n    <br clear=\"right\"\/>\n    <img src=\"https:\/\/geeklevel1000.com\/wp-content\/uploads\/2019\/01\/Machine-learning-768x512.jpg\" alt=\"Tweets\" width=\"800\" height=\"1000\" \/>\n<\/p>","5e68992d":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/3\/?share_key=c65IIAyuBQBfgU1Rfovdfb\" target=\"_blank\" title=\"Length of words in Tweets\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/3.png?share_key=c65IIAyuBQBfgU1Rfovdfb\" alt=\"Length of words in Tweets\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:3\" sharekey-plotly=\"c65IIAyuBQBfgU1Rfovdfb\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>\n","b588d5ac":"We can observe that as n is increasing model accuracy is decreasing   \nlet's try to increase n one last time just to be sure","544bd824":"Among all Simple classification models used above Voting Classifier performed best with tf-idf and ngrams = 1","e2488901":"Now let's have a look at the punctuations inside our data","bf6a523e":"## 2.2 Visualising lengths of tweets","bcb6bce4":"Using the popular [WordCloud](https:\/\/www.python-graph-gallery.com\/wordcloud\/) python library for visulaising the cleaned data","d1805458":"## 6.3 Random Forest Classifier","7dbd9575":"Printing classification report and ploting confusion matrix for the predictions of MultinomialNB(TF-IDF,n-grams=1) model","14679843":"The plot shows that our data is quite balanced, you can also click on the plot to explore more about [interactive plots](https:\/\/plotly.com\/) ","0bfc2a6c":"Printing classification report and ploting confusion matrix  for the predictions of MultinomialNB(BoW,n-grams=1) model","297291d6":"### About XGBClassifier\nThe XGBoost stands for eXtreme Gradient Boosting, which is a boosting algorithm based on gradient boosted decision trees algorithm.     \nXGBoost applies a better regularization technique to reduce overfitting, and it is one of the differences from the gradient boosting.    \n\nNow let's create a [XGBClassifier](https:\/\/machinelearningmastery.com\/develop-first-xgboost-model-python-scikit-learn\/) model and train it.","3ac2406e":"### What are stopwords?\n\nIn computing, stop words are words that are filtered out before or after the natural language data (text) are processed.       \nWhile \u201cstop words\u201d typically refers to the most common words in a language, all-natural language processing tools don't use a single universal list of stop words.  \n\nAnalysing most occuring [stop words](https:\/\/en.wikipedia.org\/wiki\/Stop_word) in the text using corpus creating function(create_corpus)","31634b57":"Printing classification report and ploting confusion matrix for the SVC model","c7566c4e":"Our cleaned text still contains some unnecessary words (such as: like, amp, get, would etc.) that aren't relevant and can confuse our model,    \nresulting in false prediction. Now, we will further remove some words with high frequency from text based on above charts.","84a34595":"checking size of data after train test split","507fb0fa":"Printing classification report and ploting confusion matrix for the predictions made by LogisticRegression(BoW,n-grams=2) model","8b149fc9":"## 1.1 Importing dependancies","3ed03ffb":"Checking average word length for both type of tweets","9190f32c":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/13\/?share_key=icoxxtajqMGbKIizrTLUX0\" target=\"_blank\" title=\"Top stop words in the text\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/13.png?share_key=icoxxtajqMGbKIizrTLUX0\" alt=\"Top stop words in the text\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:13\" sharekey-plotly=\"icoxxtajqMGbKIizrTLUX0\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>","4d0e6284":"### About SVC\nSVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. It is known for its kernel trick to handle nonlinear input spaces. It is used in a variety of applications such as face detection, intrusion detection, classification of emails, news articles and web pages, classification of genes, and handwriting recognition.\n\nSVM is an exciting algorithm and the concepts are relatively simple. The classifier separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier. SVM finds an optimal hyperplane which helps in classifying new data points.   \n\nNow let's create a [SVC](https:\/\/pythonprogramming.net\/linear-svc-example-scikit-learn-svm-python\/) model and training it","e8b8c0ea":"# **Machine Learning approach to predict real or fake tweets about disaster**\n---","105b4cf9":"Concise summarization of total information provided by the data using [DataFrame.info()](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23\/generated\/pandas.DataFrame.info.html)","d5e7cd45":"Printing classification report and ploting confusion matrix for the predictions made by the XGBClassifier model"}}