{"cell_type":{"d6900b6b":"code","1da14e34":"code","e03365d4":"code","6e546b13":"code","dcf1c023":"code","b0a858a1":"code","1da8ee00":"code","08c7b515":"code","0f6db2d8":"code","84eab8cf":"code","b81a77e7":"code","4f1df1b5":"code","771c01ca":"code","850fcc77":"code","d6aff82b":"code","dc4cc2e3":"code","b8f234e2":"code","79660f52":"code","31fe893e":"code","5c0e351f":"code","7de7b9f8":"code","5572d676":"code","cd13be04":"code","213ce753":"code","6af0f382":"code","8f7ce1e6":"code","9192081d":"code","066bc20d":"code","314510c8":"code","ee0dc813":"code","b89cba98":"code","ff53b8f8":"code","eb5bb50c":"code","d11506bb":"code","2223c652":"code","dd8999e0":"code","a0c43bae":"code","08f42e68":"code","5d69a35c":"code","9ebea8e3":"code","38c324a4":"code","65314cd8":"code","57a4d865":"code","cbc51fdb":"code","7bd4a2bf":"code","8c1e4063":"code","198dd0c0":"code","3d1ef093":"code","da4620d8":"code","1fdc9c44":"code","ca26895c":"code","ddca5cd3":"code","4e416dd7":"code","1d902a1b":"code","71a12013":"code","b3fd5173":"code","806f7063":"code","292e9875":"code","f286f8b1":"code","af248aa3":"code","8aae465a":"code","01e7c07f":"code","35ad2add":"code","511a25b5":"code","200ca141":"code","9bd2e160":"code","dab77b44":"code","dff048a1":"code","6767cd38":"code","e16c0408":"code","22a1f827":"code","eb02d5ac":"code","56f073b0":"code","b06db3a7":"code","64258fb2":"code","f64ca0ba":"code","81772ae3":"code","11c469de":"code","aa391d1d":"code","d984b227":"code","6eec4026":"code","f6ceca4b":"code","4e220d66":"code","6cc6682d":"code","ebcdf2aa":"code","d581e538":"code","b609c518":"code","177dc726":"code","03587ed4":"code","0a9baad0":"code","35d64577":"code","4bb2344d":"code","7c07ca54":"code","bbb0c5b9":"code","267b9dc0":"code","2238ff03":"code","37e730d9":"code","f200cde7":"code","672fd41c":"code","2ca0ee2a":"code","f2681423":"markdown","3e6831bf":"markdown","08b73d75":"markdown","46dcf995":"markdown","b8df7d28":"markdown","9adab6e3":"markdown","e0a8712f":"markdown","48ec1742":"markdown","e66e5aa4":"markdown","7ae1243a":"markdown","0c49b3b1":"markdown","ce54e4c9":"markdown","6ee9be07":"markdown","548052ef":"markdown","bc9ec359":"markdown","178ca6ee":"markdown","4ebc49c9":"markdown","7c7def71":"markdown","be24d2e0":"markdown","c85237eb":"markdown"},"source":{"d6900b6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\n\nwarnings.filterwarnings('ignore') \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n","1da14e34":"#Importing other packages\n\nfrom datetime import *\nimport datetime as dt\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\nimport matplotlib.cm as cm\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, silhouette_samples, silhouette_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeRegressor  \nfrom sklearn.cluster import KMeans\nfrom sklearn.naive_bayes import GaussianNB\n\nimport xgboost as xgb\n#from kneed import KneeLocator","e03365d4":"#Reading data\ndf=pd.read_csv(\"\/kaggle\/input\/nse-dataset-analyctis\/NSE-TATAGLOBAL11.csv\",parse_dates=['Date'])\ndf.head()","6e546b13":"#Information of data\ndf.info()","dcf1c023":"#Data description\ndf.describe()","b0a858a1":"#Column Names\ndf.columns","1da8ee00":"#to know the type of \"Date\"\n\ntype(df['Date'][0])","08c7b515":"df.isnull().sum()","0f6db2d8":"fig, ax = plt.subplots()\nax.plot(df['Date'], df['Close'])\n \n \nplt.title('Close Stock Price History [2013 - 2018]', fontsize=16)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Closing Stock Price', fontsize=14)\nplt.show()","84eab8cf":"df['Date'] = df['Date'].map(dt.datetime.toordinal)\ndf.head()","b81a77e7":"df1 = df.copy()\n\n# Reset index column so that we have integers to represent time for later analysis\ndf1.drop(columns=['Open','High','Low','Last','Total Trade Quantity','Turnover (Lacs)'],inplace=True)\ndf1.head()","4f1df1b5":"type(df1['Date'][0])","771c01ca":"df1.sort_values(by=['Date'],axis=0,inplace=True)","850fcc77":"plt.title('Close Price vs Date')\nplt.scatter(df1['Date'], df1['Close'], edgecolor='w', label='Actual Price')\n# plt.plot(X_train, model.predict(X_train), color='r', label='Predicted Price')\nplt.xlabel('Ordinal Date')\nplt.ylabel('Stock Price')\nplt.legend()","d6aff82b":"x = df1['Date'].values\nX = x.reshape(-1,1)\ny=df1['Close'].values\ny = y.reshape(-1,1)\nss = StandardScaler()\nX = ss.fit_transform(X)","dc4cc2e3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)","b8f234e2":"linreg = LinearRegression()\nlinreg.fit(X_train, y_train)\ny_pred= linreg.predict(X_test)\ny_pred[:5]","79660f52":"plt.figure(figsize=(16, 8))\nplt.scatter(X_train,y_train,color='m',edgecolor='w')\nplt.plot(X_test,y_pred,color='g')\nplt.title(\"Linear Regression\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"price\")\nplt.show()","31fe893e":"LR_r2_score = r2_score(y_test,y_pred)\nLR_mean_abs_err = mean_absolute_error(y_test,y_pred)\nLR_mean_sq_err= mean_squared_error(y_test,y_pred)\nLR_r_mean_sq_err= np.sqrt(mean_squared_error(y_test,y_pred))\n\nprint(\"R-Square Value :  \",LR_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",LR_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",LR_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",LR_r_mean_sq_err,\"\\n\")","5c0e351f":"def plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n    train_errors, val_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n        val_errors.append(mean_squared_error(y_val, y_val_predict))\n\n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n    plt.legend(loc=\"upper right\", fontsize=14)   \n    plt.xlabel(\"Training set size\", fontsize=14) \n    plt.ylabel(\"RMSE\", fontsize=14)         \n","7de7b9f8":"plot_learning_curves(linreg, X_train, y_train)","5572d676":"y_mul = df['Close']\nX_mul = df.drop(['Close'],axis=1)\n","cd13be04":"ss = StandardScaler()\nX_mul = ss.fit_transform(X_mul)","213ce753":"X_train_mul, X_test_mul, y_train_mul, y_test_mul = train_test_split(X_mul,y_mul,test_size=0.3)","6af0f382":"X_train_mul","8f7ce1e6":"mul_linreg = LinearRegression()\nmul_linreg.fit(X_train_mul, y_train_mul)\ny_pred_mul= mul_linreg.predict(X_test_mul)\ny_pred_mul[:5]","9192081d":"y_test_mul[:5]","066bc20d":"X_test_mul","314510c8":"MLR_r2_score = r2_score(y_test_mul,y_pred_mul)\nMLR_mean_abs_err = mean_absolute_error(y_test_mul,y_pred_mul)\nMLR_mean_sq_err= mean_squared_error(y_test_mul,y_pred_mul)\nMLR_r_mean_sq_err= np.sqrt(mean_squared_error(y_test_mul,y_pred_mul))\n\nprint(\"R-Square Value :  \",MLR_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",MLR_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",MLR_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",MLR_r_mean_sq_err,\"\\n\")","ee0dc813":"plot_learning_curves(mul_linreg, X_train_mul, y_train_mul)","b89cba98":"X_train[:5]","ff53b8f8":"y","eb5bb50c":"alpha_values = {'alpha':[0.001, 0.01,0.02,0.03,0.04, 0.05, 0.06, 0.08, 1, 2, 3, 5, 8, 10, 20, 50, 100]}","d11506bb":"# Passing in a Ridge estimator, potential alpha values, scoring method and cross validation parameters to the GridSearchCV\nridge= GridSearchCV(Ridge(), alpha_values, scoring='neg_mean_squared_error', cv=10 )\n\n# Fitting the model to the data and extracting best value of alpha\nprint('The best value of alpha for Ridge is:',ridge.fit(X_train,y_train).best_params_)\n\n# Printing the average neg_mean_squared_error of a 10-fold cross validation\nprint('The best score for the best Ridge estimator is:',ridge.fit(X_train,y_train).best_score_)","2223c652":"feature_names = df.drop('Close',axis=1).columns\nfeature_names","dd8999e0":"best_ridge_model= Ridge(alpha=0.001)\n\n# Extracting the coefficient estimates for all ten features\nbest_ridge_coeffs = best_ridge_model.fit(X_train_mul,y_train_mul).coef_\n\n# plotting the coefficient estimates for all ten features\nprint('Best Ridge Coefficients are',best_ridge_coeffs)","a0c43bae":"plt.figure(figsize=(10,6))\nplt.plot(range(len(feature_names)),best_ridge_coeffs)\nplt.axhline(0, color='r', linestyle='solid')\nplt.xticks(range(len(feature_names)),feature_names,rotation=50)\nplt.title(\"Coefficient estimates from Ridge Regression\")\nplt.ylabel(\"Coefficients\")\nplt.xlabel(\"Features\")\nplt.show()","08f42e68":"# Passing in a Lasso estimator, potential alpha values, scoring method and cross validation parameters to the GridSearchCV\nlasso= GridSearchCV(Lasso(), alpha_values, scoring='neg_mean_squared_error', cv=10 )\n\n# Fitting the model to the data and extracting best value of alpha\nprint('The best value of alpha for Lasso is:',lasso.fit(X_train,y_train).best_params_)\n\n# Printing the average neg_mean_squared_error of a 10-fold cross validation\nprint('The best score for the best Lasso estimator is:',lasso.fit(X_train,y_train).best_score_)","5d69a35c":"best_Lasso_model= Lasso(alpha=0.001)\n\n# Extracting the coefficient estimates for all ten features\nbest_Lasso_coeffs = best_Lasso_model.fit(X_train_mul,y_train_mul).coef_\n\n# plotting the coefficient estimates for all ten features\nprint('Best Lasso Coefficients are ',best_Lasso_coeffs)","9ebea8e3":"plt.figure(figsize=(10,6))\nplt.plot(range(len(feature_names)),best_Lasso_coeffs)\nplt.axhline(0, color='r', linestyle='solid')\nplt.xticks(range(len(feature_names)),feature_names,rotation=50)\nplt.title(\"Coefficient estimates from Lasso Regression\")\nplt.ylabel(\"Coefficients\")\nplt.xlabel(\"Features\")\nplt.show()","38c324a4":"x = df1['Date'].values\nX = x.reshape(-1,1)\ny=df1['Close'].values\ny = y.reshape(-1,1)\nX","65314cd8":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)","57a4d865":"poly_reg  = PolynomialFeatures(degree=7)\nX_poly = poly_reg.fit_transform(X)\nX_train,X_test,y_train,y_test = train_test_split(X_poly,y,test_size=0.30, random_state = 42)\nlinreg  = linreg.fit(X_train, y_train)\ny_pred_poly = linreg.predict(X_test)\ny_pred_poly[:5]","cbc51fdb":"plt.figure(figsize=(16, 8))\nplt.scatter(X, y, color = 'm',edgecolor='w') \nplt.plot(X, linreg.predict(poly_reg.fit_transform(X)), color = 'g') \nplt.title('Polynomial Regression')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.show()","7bd4a2bf":"PR_r2_score = r2_score(y_test,y_pred_poly)\nPR_mean_abs_err = mean_absolute_error(y_test,y_pred_poly)\nPR_mean_sq_err= mean_squared_error(y_test,y_pred_poly)\nPR_r_mean_sq_err= np.sqrt(mean_squared_error(y_test,y_pred_poly))\n\nprint(\"R-Square Value :  \",PR_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",PR_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",PR_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",PR_r_mean_sq_err,\"\\n\")","8c1e4063":"plot_learning_curves(linreg, X_train, y_train)","198dd0c0":"ss = StandardScaler()\nX_svr = ss.fit_transform(X)\ny_svr = ss.fit_transform(y)\n\nX_train_svr,X_test_svr,y_train_svr,y_test_svr = train_test_split(X_svr,y_svr,test_size=0.30)\ny_svr","3d1ef093":"regressor = SVR(kernel='rbf')\nregressor.fit(X_train_svr,y_train_svr)\ny_pred_sv=regressor.predict(X_test_svr)\ny_pred_sv[:5]","da4620d8":"SVR_r2_score = r2_score(y_test_svr,y_pred_sv)\nSVR_mean_abs_err = mean_absolute_error(y_test_svr,y_pred_sv)\nSVR_mean_sq_err= mean_squared_error(y_test_svr,y_pred_sv)\nSVR_r_mean_sq_err= np.sqrt(mean_squared_error(y_test_svr,y_pred_sv))\n\nprint(\"R-Square Value : \",SVR_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",SVR_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",SVR_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",SVR_r_mean_sq_err,\"\\n\")","1fdc9c44":"plot_learning_curves(regressor, X_train_svr, y_train_svr)","ca26895c":"plt.figure(figsize=(16, 8))\nplt.scatter(X_svr, y_svr, color = 'm',edgecolor='w')\nplt.plot(X_svr, regressor.predict(X_svr), color = 'g')\nplt.title('SVR Model Performance')\nplt.xlabel('Date')\nplt.ylabel('Close Price')","ddca5cd3":"y","4e416dd7":"ss = StandardScaler()\nX_rf = ss.fit_transform(X)\ny_rf = ss.fit_transform(y)\n\nX_train_rf,X_test_rf,y_train_rf,y_test_rf = train_test_split(X_rf,y_rf,test_size=0.30)\ny_rf","1d902a1b":"rf = RandomForestRegressor(n_estimators = 1000, random_state = 0)\n# Train the model on training data\nrf.fit(X_train_rf, y_train_rf)\ny_pred_rf = rf.predict(X_test_rf)\ny_pred_rf[:5]","71a12013":"RFR_r2_score = r2_score(y_test_rf,y_pred_rf)\nRFR_mean_abs_err = mean_absolute_error(y_test_rf,y_pred_rf)\nRFR_mean_sq_err= mean_squared_error(y_test_rf,y_pred_rf)\nRFR_r_mean_sq_err= np.sqrt(mean_squared_error(y_test_rf,y_pred_rf))\n\nprint(\"R-Square Value : \",RFR_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",RFR_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",RFR_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",RFR_r_mean_sq_err,\"\\n\")","b3fd5173":"plot_learning_curves(rf, X_test_rf, y_test_rf)","806f7063":"plt.figure(figsize=(16, 8))\nplt.scatter(X_rf, y_rf, color = 'm',edgecolor='w')\nplt.plot(X_rf, rf.predict(X_rf), color = 'g')\nplt.title('Random Forest Model Performance')\nplt.xlabel('Date')\nplt.ylabel('Price')","292e9875":"ss = StandardScaler()\nX_dt = ss.fit_transform(X)\ny_dt = ss.fit_transform(y)\n\nX_train_dt,X_test_dt,y_train_dt,y_test_dt = train_test_split(X_dt,y_dt,test_size=0.30)","f286f8b1":"DtReg = DecisionTreeRegressor(random_state = 0)  \n\nDtReg.fit(X_train_dt, y_train_dt)\n\ny_pred_dtr = DtReg.predict((X_test_dt))\ny_pred_dtr[:5]","af248aa3":"DTR_r2_score = r2_score(y_test_dt,y_pred_dtr)\nDTR_mean_abs_err = mean_absolute_error(y_test_dt,y_pred_dtr)\nDTR_mean_sq_err= mean_squared_error(y_test_dt,y_pred_dtr)\nDTR_r_mean_sq_err= np.sqrt(mean_squared_error(y_test_dt,y_pred_dtr))\n\nprint(\"R-Square Value : \",DTR_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",DTR_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",DTR_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",DTR_r_mean_sq_err,\"\\n\")","8aae465a":"plot_learning_curves(DtReg, X_train_dt, y_train_dt)","01e7c07f":"X_val = np.arange(min(X_test_dt), max(X_test_dt), 0.01) \nX_val = X_val.reshape((len(X_val), 1))  \nplt.figure(figsize=(16,8))\nplt.scatter(X_train_dt, y_train_dt, color = 'm',edgecolor='w') \nplt.plot(X_val, DtReg.predict(X_val), color = 'g')  \nplt.title('Prediction using Decision Tree Regression')  \nplt.xlabel('Date') \nplt.ylabel('Close Price') \nplt.show()","35ad2add":"ss = StandardScaler()\nX_xg = ss.fit_transform(X)\ny_xg = ss.fit_transform(y)\n\nX_train_xg,X_test_xg,y_train_xg,y_test_xg = train_test_split(X_xg,y_xg,test_size=0.30)","511a25b5":"xgbr = xgb.XGBRegressor(verbosity=0) \nxgbr.fit(X_train_xg, y_train_xg)\ny_pred_xgbr = xgbr.predict(X_test_xg)\ny_pred_xgbr[:5]","200ca141":"XGBR_r2_score = r2_score(y_test_xg,y_pred_xgbr)\nXGBR_mean_abs_err = mean_absolute_error(y_test_xg,y_pred_xgbr)\nXGBR_mean_sq_err= mean_squared_error(y_test_xg,y_pred_xgbr)\nXGBR_r_mean_sq_err= np.sqrt(mean_squared_error(y_test_xg,y_pred_xgbr))\n\nprint(\"R-Square Value : \",XGBR_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",XGBR_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",XGBR_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",XGBR_r_mean_sq_err,\"\\n\")","9bd2e160":"plot_learning_curves(xgbr, X_train_xg, y_train_xg)","dab77b44":"plt.figure(figsize=(10, 6))\nplt.scatter(X_xg, y_xg, color = 'm',edgecolor='w')\nplt.plot(X_xg, xgbr.predict(X_xg), color = 'g')\nplt.title('XGBoost Model Performance')\nplt.xlabel('Date')\nplt.ylabel('Close Price')","dff048a1":"x_nb = df1.iloc[:,0].values\nX_nb = x_nb.reshape(-1,1)\n\ny_nb=df1.iloc[:,1].values\ny_nb=y_nb.astype('int')\ny_nb = y_nb.reshape(-1,1)\nss = StandardScaler()\nX_nb = ss.fit_transform(X_nb)\nX_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(X_nb, y_nb, test_size=0.3)\n","6767cd38":"gnb = GaussianNB()\ngnb.fit(X_train_nb, y_train_nb.ravel())\ny_pred_gnb=gnb.predict(X_test_nb)\ny_pred_gnb[:5]","e16c0408":"NB_r2_score = r2_score(y_test_nb,y_pred_gnb)\nNB_mean_abs_err = mean_absolute_error(y_test_nb,y_pred_gnb)\nNB_mean_sq_err= mean_squared_error(y_test_nb,y_pred_gnb)\nNB_r_mean_sq_err= np.sqrt(mean_squared_error(y_test_nb,y_pred_gnb))\n\nprint(\"R-Square Value : \",NB_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",NB_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",NB_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",NB_r_mean_sq_err,\"\\n\")","22a1f827":"plot_learning_curves(gnb, X_train_nb, y_train_nb.ravel())","eb02d5ac":"plt.figure(figsize=(16, 8))\nplt.scatter(X_nb, y_nb, color = 'm',edgecolor='w')\nplt.plot(X_nb, gnb.predict(X_nb), color = 'g')\nplt.title('Naive Bayes Model Performance')\nplt.xlabel('Date')\nplt.ylabel(' Close Price')","56f073b0":"df3=df.copy()\ndf3.head()","b06db3a7":"y_km = df3['Close']\nfeatures = df3.drop(['Close'],axis=1)\n\n# sc = StandardScaler()\n# X_km = sc.fit_transform(X_km)\nfeatures","64258fb2":"sse = []\nk_rng = range(1,10)\nfor k in k_rng:\n    km = KMeans(n_clusters=k)\n    km.fit(features)\n    sse.append(km.inertia_)","f64ca0ba":"#kl = KneeLocator(range(1, 10), sse, curve=\"convex\", direction=\"decreasing\")\n\n#kl.elbow","81772ae3":"ig, sub_p= plt.subplots()\n# sub_p.set_ylim(0,7)\n\nplt.xlabel('K')\nplt.ylabel('Sum of squared error')\nplt.plot(k_rng,sse)","11c469de":"clustering_kmeans = KMeans(n_clusters=3)\nfeatures['clusters'] = clustering_kmeans.fit_predict(features)\nfeatures['clusters'].value_counts()","aa391d1d":"features.head()","d984b227":"clustering_kmeans.cluster_centers_.shape","6eec4026":"colors = ['red','blue','green']\nplt.figure(figsize=(20, 10))\n\nax = sns.scatterplot(x=features.iloc[:, 4], y=features.iloc[:, 6], hue = 'clusters', palette=colors, s=20, data=features)\nax = sns.scatterplot(clustering_kmeans.cluster_centers_[:,4],clustering_kmeans.cluster_centers_[:,6], hue=range(3),palette=colors, s=200,marker='o',\n              legend=False,  c=\"white\", alpha=1, edgecolor='k')","f6ceca4b":"for k in (2, 3, 4, 5, 6, 7):\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (k + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=k, random_state=10)\n    cluster_labels = clusterer.fit_predict(features)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(features, cluster_labels)\n    print(\"For n_clusters =\", k,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(features, cluster_labels)\n\n    y_lower = 10\n    for i in range(k):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ k)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ k)\n\n    ax2.scatter(features.iloc[:, 4], features.iloc[:, 6], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 4], centers[:, 6], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        j=i+1\n        ax2.scatter(c[4], c[6], marker='$%d$' % j, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % k),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","4e220d66":"y_pca = df3['Close']\nX_pca = df3.drop(['Close'],axis=1)\n\nsc = StandardScaler()\nX_pca = sc.fit_transform(X_pca)","6cc6682d":"pca = PCA()\npca.fit(X_pca)\nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\nplt.plot(range(1, exp_var_cumul.shape[0] + 1),exp_var_cumul)\nplt.xlabel('Components')\nplt.ylabel('Explained Variance')\nplt.title('Components vs Explained Varaiance')","ebcdf2aa":"pca = PCA(n_components = 2)\nX_reduced_2D = pca.fit_transform(X_pca)\nX_reduced_2D[:5]","d581e538":"pca.components_","b609c518":"pca.explained_variance_ratio_","177dc726":"principal_df = pd.DataFrame(X_reduced_2D,columns=['pc1','pc2'])\n\nfinaldf= pd.concat([principal_df,df3[['Close']]],axis=1)\nfinaldf.head()","03587ed4":"plt.figure(figsize=(8,6))\nplt.scatter(finaldf.iloc[:,0],finaldf.iloc[:,1],c=finaldf['Close'])\nplt.xlabel('First principle component')\nplt.ylabel('Second principle component')","0a9baad0":"X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca,y_pca,test_size=0.3)","35d64577":"X_train_pca = sc.fit_transform(X_train_pca)\nX_test_pca = sc.transform(X_test_pca)\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_pca)\nX_test_pca = pca.transform(X_test_pca)\nexplained_variance = pca.explained_variance_ratio_","4bb2344d":"PCA_linreg = LinearRegression()\nPCA_linreg.fit(X_train_pca, y_train_pca)\ny_pred_pca= PCA_linreg.predict(X_test_pca)\n","7c07ca54":"y_pred_pca[:5]","bbb0c5b9":"PCA_r2_score = r2_score(y_test_pca,y_pred_pca)\nPCA_mean_abs_err = mean_absolute_error(y_test_pca,y_pred_pca)\nPCA_mean_sq_err= mean_squared_error(y_test_pca,y_pred_pca)\nPCA_r_mean_sq_err= np.sqrt(mean_squared_error(y_test_pca,y_pred_pca))\n\nprint(\"R-Square Value : \",PCA_r2_score,\"\\n\")\nprint(\"Mean Absolute Error : \",PCA_mean_abs_err,\"\\n\")\nprint(\"Mean Squared Error : \",PCA_mean_sq_err,\"\\n\")\nprint(\"Root Mean Squared Error : \",PCA_r_mean_sq_err,\"\\n\")","267b9dc0":"plot_learning_curves(PCA_linreg, X_train_pca, y_train_pca)","2238ff03":"data_plot = {'ModelName':['Linear Regression', 'Polynomial Regression','Multiple Linear Regression',\n                          'SVR', 'Random Forest Regressor','Decsion Tree Regressor',\n                          'XGBoost Regressor', 'Naive Bayes','Multiple Linear Regression with PCA',  ], \n        'R2_Score':[LR_r2_score,PR_r2_score,MLR_r2_score,SVR_r2_score,RFR_r2_score,DTR_r2_score,\n                    XGBR_r2_score,NB_r2_score,PCA_r2_score],\n        'MAE':[LR_mean_abs_err,PR_mean_abs_err,MLR_mean_abs_err,SVR_mean_abs_err,RFR_mean_abs_err,DTR_mean_abs_err,\n               XGBR_mean_abs_err,NB_mean_abs_err,PCA_mean_abs_err],\n        'MSE':[LR_mean_sq_err,PR_mean_sq_err,MLR_mean_sq_err,SVR_mean_sq_err,RFR_mean_sq_err,DTR_mean_sq_err,\n              XGBR_mean_sq_err,NB_mean_sq_err,PCA_mean_sq_err],\n        'RMSE':[LR_r_mean_sq_err,PR_r_mean_sq_err,MLR_r_mean_sq_err,SVR_r_mean_sq_err,RFR_r_mean_sq_err,DTR_r_mean_sq_err,\n                XGBR_r_mean_sq_err,NB_r_mean_sq_err,PCA_r_mean_sq_err]}\n\n\n\ndf_plot = pd.DataFrame(data_plot)\ndf_plot","37e730d9":"plt.figure(figsize=(24, 8))\nplt.grid()\nplt.plot(df_plot.ModelName, df_plot.R2_Score, c='red')\nplt.bar(df_plot.ModelName, df_plot.R2_Score)\nplt.title('R2 Score Comparison')\nplt.xlabel('Model Name')\nplt.ylabel('R2 Score')","f200cde7":"plt.figure(figsize=(24, 8))\nplt.grid()\nplt.plot(df_plot.ModelName, df_plot.MAE, c=\"red\")\nplt.bar(df_plot.ModelName, df_plot.MAE)\nplt.title('Mean Absolute Error Comparison')\nplt.xlabel('Model Name')\nplt.ylabel('Mean Absolute Error')","672fd41c":"plt.figure(figsize=(24, 8))\n# plt.grid(zorder=5)\nplt.grid()\nplt.plot(df_plot.ModelName, df_plot.MSE,c =\"red\")\nplt.bar(df_plot.ModelName, df_plot.MSE)\nplt.title('Mean Squared Error Comparison')\nplt.xlabel('Model Name')\nplt.ylabel('Mean Squared Error')","2ca0ee2a":"plt.figure(figsize=(24, 8))\nplt.grid()\nplt.plot(df_plot.ModelName, df_plot.RMSE,c=\"red\")\nplt.bar(df_plot.ModelName, df_plot.RMSE)\nplt.title('RMSE Comparison')\nplt.xlabel('Model Name')\nplt.ylabel('RMSE')","f2681423":"# Hyperparameter Tuning","3e6831bf":"# Data Preprocessing","08b73d75":"# Regression Techniques","46dcf995":"# Random Forest Regressor","b8df7d28":"# PCA","9adab6e3":"# Multiple Linear Regression","e0a8712f":"# Naive Bayes ","48ec1742":"# Clean Data","e66e5aa4":"# Polynomial Regression","7ae1243a":"# Support Vector Regressor","0c49b3b1":"# Stock Market Price Prediction Using Machine Learning Approach","ce54e4c9":"Please note that performing Random Forest Regressor take more computation time.","6ee9be07":"# Decison Tree Regressor","548052ef":"# Obtain and Explore data","bc9ec359":"# Multiple Linear Regression After PCA","178ca6ee":"# XGBoost Regressor","4ebc49c9":"# Comparsion among Prediction Models","7c7def71":"# Linear Regression\n","be24d2e0":"# Clustering","c85237eb":"# Visualisation"}}