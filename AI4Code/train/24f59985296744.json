{"cell_type":{"99c88624":"code","5000f9d1":"code","fa26f3db":"code","e107529e":"code","33db3b31":"code","c5e03693":"code","31f33e23":"code","77bee5e7":"code","c4a10c76":"code","b2eb166c":"code","f25d386f":"code","292435c2":"code","a3da9dc8":"code","ef2f50db":"code","4a4c7f9e":"code","bf0d5c14":"code","38a3ae84":"code","f33d269c":"code","623bb348":"code","ee50d646":"code","8497b390":"code","f9c8e40e":"code","af7270a1":"code","007044f8":"code","6b3a63d0":"code","fc2a71ba":"code","18dfa7af":"code","f64f3c0d":"code","367679cc":"code","58602911":"code","eda5907f":"code","b783fb64":"code","56c3579d":"code","2631f802":"code","83907207":"code","689385c0":"code","e6f63253":"code","89a2e85f":"code","0d78411b":"code","51a6d0d4":"code","37862ae6":"code","04aec6ff":"code","e3e9f54d":"code","cbc3f63f":"code","900f4e68":"code","cf8f95da":"code","2b8c9ef7":"code","f00d358e":"code","af3addf0":"code","cdaa6a1c":"code","14e803c6":"code","cf626a91":"code","c650c898":"code","2579ab4a":"code","d4aa3c1d":"code","9f42b93a":"code","b76a6d06":"code","27fd01f7":"code","cbf9a2df":"code","100b2074":"code","01474f9e":"code","8442415d":"code","43bd940c":"markdown","8c7c5e54":"markdown","1c389577":"markdown","b10e9431":"markdown","b1029dfb":"markdown","abf0eb83":"markdown","3655b2f3":"markdown","3e31ac9f":"markdown","512fb6f2":"markdown","59c1b4c0":"markdown","0062ce2d":"markdown","8c20171b":"markdown","2039a3b0":"markdown","b5db1ba5":"markdown","52300213":"markdown"},"source":{"99c88624":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5000f9d1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_curve, auc\nfrom sklearn.neural_network import MLPClassifier","fa26f3db":"heart=pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv')\nheart=pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","e107529e":"heart.head()","33db3b31":"heart.shape","c5e03693":"Display the data type of attributes","31f33e23":"heart.dtypes","77bee5e7":"heart.info()","c4a10c76":"Exploratory Data Analysis\nMissing Data","b2eb166c":"heart.isnull()","f25d386f":"heart[heart.duplicated()]","292435c2":"# heart.drop_duplicates(inplace=True)\nheart.reset_index(drop=True, inplace=True)\nheart.shape","a3da9dc8":"## Convert the dataset into Numerical and Categorical Columns\n\nheart.columns","ef2f50db":"numerical_columns=['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh',\n       'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output']","4a4c7f9e":"### Separate Dependent and Independent Variables\n\nnumerical_cols = ['age', 'trtbps', 'chol', 'thalach', 'oldpeak']\ncategorical_cols = ['sex', 'cp', 'caa', 'fbs', 'restecg', 'exng', 'slp', 'thall']\ntarget = ['output']","bf0d5c14":"heart.plot()","38a3ae84":"## HeatMap Plot\nf,ax = plt.subplots(figsize=(5, 5))\nsns.heatmap(heart.corr(), annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.2f',ax=ax)\nplt.show()","f33d269c":"corr = heart.corr()\ncorr.style.background_gradient(cmap='coolwarm')","623bb348":"print(y_train.value_counts(normalize=True))\nprint(y_test.value_counts(normalize=True))","ee50d646":"X=heart.iloc[:,0:13]\ny=heart.iloc[:,13]","8497b390":"# Split the data into trainset and testset         \n\nX_train, X_test,y_train,y_test,=train_test_split(X,y,test_size=0.3,random_state=123)","f9c8e40e":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nle = LabelEncoder()","af7270a1":"le. fit (y_train)\n\ny_train = pd.Series(le.transform(y_train))\n\ny_test = pd.Series(le.transform(y_test))","007044f8":"y_train.value_counts(normalize=True)","6b3a63d0":"y_test.value_counts(normalize=True)","fc2a71ba":"## Get numerical cols and store in num_cols\n\nnum_cols = X_train.select_dtypes(include=['float64','int64']) .columns\nnum_cols","18dfa7af":"### Get categorial cols and store in cat_cols\n\nnum_cols = X_train.select_dtypes(exclude=['float64','int64']) .columns\nnum_cols","f64f3c0d":"cat_cols = X_train.select_dtypes(exclude=['float64', 'int64']).columns\ncat_cols","367679cc":"ohe = OneHotEncoder(drop='first', sparse=False)\n\nohe.fit(X_train[cat_cols])","58602911":"ohe.transform(X_train[cat_cols])","eda5907f":"pd.DataFrame(ohe.transform(X_train[cat_cols]), \n                            columns = ohe.get_feature_names()).set_index(X_train.index)","b783fb64":"X_train[num_cols]","56c3579d":"cat_df_train = pd.DataFrame(ohe.transform(X_train[cat_cols]), \n                            columns = ohe.get_feature_names()).set_index(X_train.index)","2631f802":"cat_df_test = pd.DataFrame(ohe.transform(X_test[cat_cols]), \n                           columns = ohe.get_feature_names()).set_index(X_test.index)","83907207":"X_train[num_cols].index","689385c0":"cat_df_train.index","e6f63253":"X_train_final = X_train[num_cols].join(cat_df_train)\nX_test_final = X_test[num_cols].join(cat_df_test)","89a2e85f":"X_train_final.isnull().sum().sum()","0d78411b":"## Models\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier","51a6d0d4":"from sklearn.ensemble import RandomForestClassifier\n\nrf1 = RandomForestClassifier()","37862ae6":"X_train_final","04aec6ff":"X_train_final.isnull().sum().sum()","e3e9f54d":"rf1.fit(X, y)","cbc3f63f":"importances = rf1.feature_importances_\nprint(importances)\n\nindices = np.argsort(importances)[::-1]\nprint(indices)","900f4e68":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000,\n                           n_features=10,\n                           n_informative=3,\n                           n_redundant=0,\n                           n_repeated=0,\n                           n_classes=2,\n                           random_state=0,\n                           shuffle=False)\n\n# Build a forest and compute the impurity-based feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","cf8f95da":"rf1.predict(X,y)","2b8c9ef7":"from sklearn.metrics import classification_report\n\n\ny_true = [0, 1, 2, 2, 2]\ny_pred = [0, 0, 2, 2, 1]\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_true, y_pred, target_names=target_names))","f00d358e":"y_pred = [1, 1, 0]\ny_true = [1, 1, 1]\nprint(classification_report(y_true, y_pred, labels=[1, 2, 3]))","af3addf0":"from sklearn.linear_model import LogisticRegression","cdaa6a1c":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","14e803c6":"predictions = logmodel.predict(X_test)","cf626a91":"from sklearn.metrics import confusion_matrix","c650c898":"accuracy=confusion_matrix(y_test,predictions)","2579ab4a":"accuracy","d4aa3c1d":"from sklearn.metrics import accuracy_score","9f42b93a":"accuracy=accuracy_score(y_test,predictions)\naccuracy","b76a6d06":"from sklearn.metrics import classification_report","27fd01f7":"print(classification_report(y_test,predictions))","cbf9a2df":"# from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4,\n                           n_informative=2, n_redundant=0,\n                            random_state=0, shuffle=False)\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(X, y)\nAdaBoostClassifier(n_estimators=100, random_state=0)\nclf.score(X, y)","100b2074":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    max_depth=1, random_state=0).fit(X_train, y_train)\nclf.score(X_test, y_test)","01474f9e":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\nclf.fit(X, y)\nclf.score(X, y)","8442415d":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nX, y = make_classification(n_samples=100, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)\nclf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\nclf.predict_proba(X_test[:1])\nclf.predict(X_test[:5, :])\nclf.score(X_test, y_test)","43bd940c":"**Store categorical attributes name**","8c7c5e54":"T**arget attribute distribution for trainset and testset**","1c389577":"**List important features***","b10e9431":"**Predict**","b1029dfb":"**Standardize the numerical attributes**","abf0eb83":"**Target attribute distribution after the split**","3655b2f3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_curve, auc\nfrom sklearn.neural_network import MLPClassifier","3e31ac9f":"**Concatenate numerical & encoded categorical features**","512fb6f2":"**Model building\u00b6**","59c1b4c0":"**Train Model**","0062ce2d":"**Concatenate numerical & encoded categorical features\u00b6**","8c20171b":"**Plot Visualization**","2039a3b0":"**Evaluation\u00b6**","b5db1ba5":"**Converting Categorical attributes to Numeric attributes**","52300213":"**Evaulate**"}}