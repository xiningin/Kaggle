{"cell_type":{"403bc379":"code","fecc8845":"code","4453a8fa":"code","054534ab":"code","61418f78":"code","f335eace":"code","5c1114a8":"code","0b770181":"code","d2147b77":"code","eb525def":"code","c2155719":"code","080ce205":"code","550fb1ba":"code","08798872":"code","c1b6da9d":"code","1f1f81da":"code","5acece6e":"code","0a8cdcba":"code","97400e97":"code","1a10ab21":"code","0e1eee08":"code","a1419e07":"code","850dd17f":"code","bfba4788":"code","dab5f909":"code","36f392db":"code","d7543c95":"code","3ca903b1":"code","2254a1a7":"code","0de77600":"code","5a2b797c":"code","c2de3d7d":"code","96b4db14":"code","dd939a91":"code","1faffe0d":"code","8b394503":"code","a198b7d6":"code","58f57b4d":"code","7d26bc49":"code","cc55f5a7":"code","1b6c1d87":"code","f2ef3588":"code","8b3f36df":"code","2802f267":"code","6634559c":"code","8eb3d29b":"code","30f63fdf":"code","5c2d8103":"code","c6141abd":"code","b9efada5":"code","7eeffb1f":"code","1600f45c":"code","139cbfac":"code","2e932e74":"code","ea35b621":"code","009624fc":"code","a5621ea9":"code","02904a9d":"code","358d319c":"code","356843bc":"code","ea0f2ebc":"code","cbc1a3b2":"code","6931125e":"code","0088d505":"code","5f3dcb1b":"code","518f2c32":"code","800130f8":"code","7a66d951":"code","c7e8c0a1":"code","89b1ceea":"code","e5b5705d":"code","18e253e8":"code","1a14e35c":"code","5c6de402":"code","1d57efba":"code","1ac13969":"code","9abb4ee4":"code","05f3769e":"code","fa874ac1":"code","a4b4e799":"code","62ee2af1":"code","7f19d7b3":"code","5e794ac1":"code","3673a79d":"code","5eead5cc":"code","a8f11caf":"code","928e94e0":"code","45892659":"code","047bcefa":"code","a72c052d":"code","410d6829":"code","99d92546":"code","397171cd":"code","db94c40d":"code","2a59e281":"code","b573e4c8":"code","5d2aed32":"code","5ccfae5a":"code","61567b6f":"code","60b2a7b0":"code","c78b9ea1":"code","fc5a051d":"code","b427bd5e":"code","6aeecb8e":"code","6367d148":"code","06c8314b":"code","4a5ce5a6":"code","e6a4e37e":"code","b0a3d782":"code","3845dfd0":"code","89d6ef64":"code","c29d5597":"code","af7d7960":"code","f06f4abe":"markdown","b5e6d77e":"markdown","c0ab24ad":"markdown","7b37a117":"markdown","616ceba8":"markdown","9ed6a201":"markdown","d17e7337":"markdown","6f59e5c0":"markdown","c4ac1e8c":"markdown","1d382f38":"markdown","a7597da0":"markdown","e1cc9872":"markdown","5ba33982":"markdown","7729281d":"markdown","22a5dab4":"markdown","cf61dea9":"markdown","69773176":"markdown","c0daecff":"markdown","065ded7d":"markdown","cf154f06":"markdown","98b244f2":"markdown","c3b16666":"markdown","70453ea6":"markdown","dafed376":"markdown","94ccb81f":"markdown","3e410ba4":"markdown","2c78e4c4":"markdown"},"source":{"403bc379":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport math\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","fecc8845":"Titanic = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n","4453a8fa":"Titanic.columns","054534ab":"Titanic.head()","61418f78":"Titanic.tail()","f335eace":"Titanic.shape","5c1114a8":"Titanic.info()","0b770181":"Titanic.describe()","d2147b77":"Titanic.describe(include=['O'])","eb525def":"Titanic[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c2155719":"Titanic[[\"Sex\" , \"Survived\"]].groupby([\"Sex\"] , as_index = False).mean().sort_values(by=\"Survived\" , ascending = False)","080ce205":"Titanic[[\"Parch\" , \"Survived\"]].groupby([\"Parch\"] , as_index = False) .mean().sort_values(by=\"Survived\" , ascending = False)","550fb1ba":"Titanic[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","08798872":"Titanic[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c1b6da9d":"\nTitanic.hist(bins=10,figsize=(9,7),grid=False)","1f1f81da":"print(\"No of Passengers in original data:  \" , str(len(Titanic.index)))","5acece6e":"sns.countplot(x=\"Survived\" , data=Titanic , color=\"orange\")","0a8cdcba":"sns.countplot(x=\"Survived\", hue=\"Parch\",data=Titanic)","97400e97":"sns.countplot(x=\"Survived\" ,hue=\"Sex\" , data=Titanic )","1a10ab21":"sns.countplot(x=\"Survived\" ,hue=\"Pclass\" , data=Titanic )","0e1eee08":"sns.countplot(x=\"Survived\" ,hue=\"Embarked\" , data=Titanic )","a1419e07":"sns.countplot(x=\"Survived\", hue=\"SibSp\",data=Titanic)","850dd17f":"Titanic[\"Parch\"].plot.hist( figsize=(5,4))","bfba4788":"Titanic[\"SibSp\"].plot.hist()","dab5f909":"Titanic.isnull()","36f392db":"Titanic.isnull().sum()","d7543c95":"sns.heatmap(Titanic.isnull())","3ca903b1":"null_var = Titanic.isnull().sum()\/Titanic.shape[0] *100\nnull_var","2254a1a7":"drop_column = null_var[null_var >20].keys()\ndrop_column\n##null_var = Titanic_datA.isnull().sum()\/Titanic_datA.shape[0] *100\n#null_var","0de77600":"#drop_column = null_var[null_var >20].keys()\n#drop_column\n##null_var = Titanic_datA.isnull().sum()\/Titanic_datA.shape[0] *100\n#null_var\nN_Titanic_datA = Titanic.drop(columns = drop_column)","5a2b797c":"Titanic_copy = Titanic.copy()\nTitanic_copy2 = Titanic.copy()\nTitanic_Deep = Titanic_copy.copy()","c2de3d7d":"sns.heatmap( N_Titanic_datA.isnull())","96b4db14":"N_Titanic_datA.isnull().sum()\/Titanic_Deep.shape[0] *100","dd939a91":"N_Titanic_datAA = N_Titanic_datA.dropna()","1faffe0d":"sns.heatmap( N_Titanic_datAA.isnull())","8b394503":"Categorical_Values = N_Titanic_datAA.select_dtypes(include=[\"object\"]).columns\nCategorical_Values_test = test.select_dtypes(include=[\"object\"]).columns","a198b7d6":"Numarical_Values = N_Titanic_datAA.select_dtypes(include=['int64','float64']).columns\nNumarical_Values_test = test.select_dtypes(include=['int64','float64']).columns","58f57b4d":"test.shape","7d26bc49":"def cat_var_dist(var):\n    return pd.concat([Titanic_Deep[var].value_counts()\/Titanic_Deep.shape[0] * 100, \n          N_Titanic_datAA[var].value_counts()\/N_Titanic_datAA.shape[0] * 100], axis=1,\n         keys=[var+'_org', var+'clean'])\n    ","cc55f5a7":"cat_var_dist(\"Ticket\")","1b6c1d87":"Imputer_mean = SimpleImputer(strategy='mean')\n","f2ef3588":"Imputer_mean.fit(Titanic_Deep[Numarical_Values])\n","8b3f36df":"Imputer_mean.statistics_","2802f267":"Imputer_mean.transform(Titanic_Deep[Numarical_Values])","6634559c":"Titanic_Deep[Numarical_Values] = Imputer_mean.transform(Titanic_Deep[Numarical_Values])\nnnnn = Titanic_Deep[Numarical_Values]","8eb3d29b":"Titanic_Deep[Numarical_Values].isnull().sum()","30f63fdf":"Imputer_mean = SimpleImputer(strategy='most_frequent')","5c2d8103":"Titanic_Deep[Categorical_Values] = Imputer_mean.fit_transform(Titanic_Deep[Categorical_Values])","c6141abd":"Titanic_Deep[Categorical_Values].isnull().sum()","b9efada5":"New_Titanic_datA = pd.concat([Titanic_Deep[Numarical_Values] , Titanic_Deep[Categorical_Values]] , axis=1)\n","7eeffb1f":"New_Titanic_datA.isnull().sum()","1600f45c":"skip_column = null_var[null_var >20].keys()\nskip_column\n","139cbfac":"Nn_Titanic_datA = Titanic_copy.drop(columns = skip_column)\n","2e932e74":"Titanic_mean = Nn_Titanic_datA.fillna(Nn_Titanic_datA.mean())\nTitanic_mean = Titanic_mean.dropna()\n","ea35b621":"print(Titanic_mean.isnull().sum())","009624fc":"Titanic_median = Nn_Titanic_datA.fillna(Nn_Titanic_datA.median())\ntest_median =  test.fillna(test.median())\nTitanic_median = Titanic_median.dropna()\nTitanic_median.isnull().sum()","a5621ea9":"print(\"*\"*30 , \"Data Cleaning Using Different Method\" , \"*\"*30)\nprint(\"*\"*30 , \"Simple Row Delete Mehtod\" , \"*\"*30)\nprint(N_Titanic_datAA.isnull().sum())\nprint(\"*\"*30 , \"SimpleImputer Method\" , \"*\"*30)\nprint(New_Titanic_datA.isnull().sum())\nprint(\"*\"*30 , \"Median\" , \"*\"*30)\nprint(Titanic_median.isnull().sum())\nprint(\"*\"*30 , \"Mean\" , \"*\"*30)\nprint(Titanic_mean.isnull().sum())","02904a9d":"N_Titanic_datAA.tail()","358d319c":"sex = pd.get_dummies(N_Titanic_datAA[\"Sex\"] , drop_first=True)\nsexx =  pd.get_dummies(test_median[\"Sex\"] , drop_first=True)","356843bc":"pclass = pd.get_dummies(N_Titanic_datAA[\"Pclass\"] , drop_first=True)\npclasss = pd.get_dummies(test_median[\"Pclass\"] , drop_first=True)","ea0f2ebc":"embarked = pd.get_dummies(N_Titanic_datAA[\"Embarked\"] , drop_first=True)\nembarkedd = pd.get_dummies(test_median[\"Embarked\"] , drop_first=True)","cbc1a3b2":"N_Titanic_datAA_copy = N_Titanic_datAA.copy()","6931125e":"N_Titanic_datAA_copy.drop(['Embarked', 'Pclass' ,\"Sex\" , \"Ticket\" , \"Name\"], axis=1 , inplace=True)","0088d505":"test_median.drop(['Embarked', 'Pclass' ,\"Sex\" , \"Ticket\" , \"Name\"], axis=1 , inplace=True)","5f3dcb1b":"N_Titanic_datAA_copy = pd.concat([N_Titanic_datAA_copy ,sex ,pclass ,embarked] ,axis=1)\nN_Titanic_datAA_copy.head()\ntest_median = pd.concat([test_median ,sexx ,pclasss ,embarkedd] ,axis=1)\ntest_median.head()","518f2c32":"test_median.drop([\"Cabin\"], axis=1 , inplace=True)","800130f8":"test1= test_median.copy()","7a66d951":"test_median.head()","c7e8c0a1":"X = N_Titanic_datAA_copy.drop(\"Survived\" , axis=1)\ny = N_Titanic_datAA_copy[\"Survived\"]","89b1ceea":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","e5b5705d":"y.shape","18e253e8":"#KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train,y_train) * 100, 2)\nacc_knn","1a14e35c":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","5c6de402":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","1d57efba":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","1ac13969":"#LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nacc_logreg = round(logreg.score(X_train, y_train) * 100, 2)\nacc_logreg","9abb4ee4":"models = pd.DataFrame({\n    'Model': [ 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Decision Tree'],\n    'Score': [ acc_knn, acc_logreg, \n              acc_random_forest, acc_gaussian,\n              acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","05f3769e":"New_Titanic_datA.head()\n","fa874ac1":"embarked = pd.get_dummies(New_Titanic_datA[\"Embarked\"] , drop_first=True)\npclass = pd.get_dummies(New_Titanic_datA[\"Pclass\"] , drop_first=True)\nsex = pd.get_dummies(New_Titanic_datA[\"Sex\"] , drop_first=True)\n","a4b4e799":"New_Titanic_datA_copy= New_Titanic_datA.copy()","62ee2af1":"New_Titanic_datA_copy.drop(['Embarked', 'Pclass' ,\"Sex\" , \"Ticket\" , \"Name\"], axis=1 , inplace=True)","7f19d7b3":"New_Titanic_datA_copy = pd.concat([New_Titanic_datA_copy ,sex ,pclass ,embarked] ,axis=1)\nNew_Titanic_datA_copy.head()","5e794ac1":"XXX = New_Titanic_datA_copy.drop(\"Survived\" , axis=1)\nyyy = New_Titanic_datA_copy[\"Survived\"]","3673a79d":"XXX_train, XXX_test, yyy_train, yyy_test = train_test_split(XXX, yyy, test_size=0.2, random_state=42)","5eead5cc":"#KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(XXX_train, yyy_train)\nyyy_pred = knn.predict(XXX_test)\nacc_knn = round(knn.score(XXX_train, yyy_train) * 100, 2)\nacc_knn","a8f11caf":"#LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(XXX_train, yyy_train)\nyyy_pred = logreg.predict(XXX_test)\nacc_logreg = round(logreg.score(XXX_train, yyy_train) * 100, 2)\nacc_logreg","928e94e0":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(XXX_train, yyy_train)\nyyy_pred = random_forest.predict(XXX_test)\nrandom_forest.score(XXX_train, yyy_train)\nacc_random_forest = round(random_forest.score(XXX_train, yyy_train) * 100, 2)\nacc_random_forest","45892659":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(XXX_train, yyy_train)\nyyy_pred = gaussian.predict(XXX_test)\nacc_gaussian = round(gaussian.score(XXX_train, yyy_train) * 100, 2)\nacc_gaussian","047bcefa":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(XXX_train, yyy_train)\nyyy_pred = decision_tree.predict(XXX_test)\nacc_decision_tree = round(decision_tree.score(XXX_train, yyy_train) * 100, 2)\nacc_decision_tree","a72c052d":"models = pd.DataFrame({\n    'Model': [ 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Decision Tree'],\n    'Score': [ acc_knn, acc_logreg, \n              acc_random_forest, acc_gaussian,\n              acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","410d6829":"Titanic_median.head()","99d92546":"embarked = pd.get_dummies(Titanic_median[\"Embarked\"] , drop_first=True)\npclass = pd.get_dummies(Titanic_median[\"Pclass\"] , drop_first=True)\nsex = pd.get_dummies(Titanic_median[\"Sex\"] , drop_first=True)\n","397171cd":"Titanic_median_copy= Titanic_median.copy()","db94c40d":"Titanic_median_copy.drop(['Embarked', 'Pclass' ,\"Sex\" , \"Ticket\" , \"Name\"], axis=1 , inplace=True)\n\n","2a59e281":"Titanic_median_copy = pd.concat([Titanic_median_copy ,sex ,pclass ,embarked] ,axis=1)\nTitanic_median_copy.head()","b573e4c8":"XX = Titanic_median_copy.drop(\"Survived\" , axis=1)\nyy = Titanic_median_copy[\"Survived\"]","5d2aed32":"XXX_train, XXX_test, yyy_train, yyy_test = train_test_split(XX, yy, test_size=0.2, random_state=42)","5ccfae5a":"XXX_train.shape","61567b6f":"#KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(XXX_train, yyy_train)\nyyy_pred = knn.predict(XXX_test)\nacc_knn = round(knn.score(XXX_train, yyy_train) * 100, 2)\nacc_knn","60b2a7b0":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(XXX_train, yyy_train)\nyyy_pred = gaussian.predict(XXX_test)\nacc_gaussian = round(gaussian.score(XXX_train, yyy_train) * 100, 2)\nacc_gaussian","c78b9ea1":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(XXX_train, yyy_train)\nyyy_pred = decision_tree.predict(XXX_test)\nacc_decision_tree = round(decision_tree.score(XXX_train, yyy_train) * 100, 2)\nacc_decision_tree","fc5a051d":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(XXX_train, yyy_train)\nyyy_pred = random_forest.predict(XXX_test)\nrandom_forest.score(XXX_train, yyy_train)\nacc_random_forest = round(random_forest.score(XXX_train, yyy_train) * 100, 2)\nacc_random_forest","b427bd5e":"#LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(XXX_train, yyy_train)\nyyy_pred = logreg.predict(XXX_test)\nacc_logreg = round(logreg.score(XXX_train, yyy_train) * 100, 2)\nacc_logreg","6aeecb8e":"models = pd.DataFrame({\n    'Model': [ 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Decision Tree'],\n    'Score': [ acc_knn, acc_logreg, \n              acc_random_forest, acc_gaussian,\n              acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","6367d148":"test_median.isnull().sum()","06c8314b":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nfrom sklearn.svm import SVC\n\nsvc = SVC()\n","4a5ce5a6":"# Gradient Boosting Classifier\ngbkk = GradientBoostingClassifier()\ngbkk.fit(X_train, y_train)\ngbk_pred = gbkk.predict(X_test)\nacc_gbkk = round(gbkk.score(X_train, y_train) * 100, 2)\npredictions = gbkk.predict(test_median)\nacc_gbkk","e6a4e37e":"gbk_pred = gbkk.predict(X_test)\ngbk_pred","b0a3d782":"sgd.fit(XXX_train, yyy_train)\nyyy_pred = sgd.predict(XXX_test)\nacc_gbk = round(sgd.score(XXX_train, yyy_train) * 100, 2)\nacc_gbk","3845dfd0":"perceptron.fit(XXX_train, yyy_train)\nyyy_pred = perceptron.predict(XXX_test)\nacc_gbk = round(perceptron.score(XXX_train, yyy_train) * 100, 2)\nacc_gbk","89d6ef64":"linear_svc.fit(XXX_train, yyy_train)\nyyy_pred = linear_svc.predict(XXX_test)\nacc_gbk = round(linear_svc.score(XXX_train, yyy_train) * 100, 2)\nacc_gbk","c29d5597":"svc.fit(XXX_train, yyy_train)\nyyy_pred = svc.predict(XXX_test)\nacc_gbk = round(svc.score(XXX_train, yyy_train) * 100, 2)\nacc_gbk","af7d7960":"#set ids as PassengerId and predict survival \nids = test1['PassengerId']\npredictions = gbkk.predict(test_median)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","f06f4abe":"## Part 5.1 : Using Three Different Methods","b5e6d77e":"### Part 5.1.3 : Mean Median and Mode Method","c0ab24ad":"# Part 4 : Data Analyze by visualization Method","7b37a117":"## Part 6.2 : SimpleImputer Method","616ceba8":"### Part 6.2.1 : Finding categorical feature","9ed6a201":"### Part 6.3.1 : Finding categorical feature","d17e7337":"# Titanic Data Peprocessing Step by Step\n## Intro\n I'm talking just from *my experience on Titanic* so the following may not be true for you, so be cautious.\n\n - \"Has_Cabin\" feature does not help. I engineered a feature with 0 if a passenger has no Cabin (NaN) and 1 if he got one may make sense as cabin data of 1st class passengers was found (IRL) on the body of steward Herbert Cave, so I tried it. But that doesn't seem to help.\n - \"Deck\" feature does not help. Based on letters found in Cabin column we may engineer a Deck feature, indicating which deck (A - G, T or U for Unknown) the passenger was on. But it's rather noisy, it doesn't help the score.\n - \"Embarked\" does not help, I have no idea why people even include it in their kernels. It has no impact on survival chances.\n - *Edit*: actually certain algorithms may perform better if you turn categorical features into ordinal ones (like turning Pclass to Pclass_1, Pclass_2 and Pclass_3 features with possible values {0, 1}). Pros are higher accuracy in certain cases, cons are - you lose relation between Pclasses (meaning the algorithm will think those are independent, unordered classes, when in fact they are ordered - Pclass=1 is \"better\" than Pclass=3) and you add dimensions which is not always good because of the curse of dimensionality. In my specific case turning Pclass into 3 features did not help, but as I learned it's a good idea to try both approaches and see what's better in your case.\n - I don't know about feature scaling in R, maybe R methods scale them by default? If not, and if you're using R, try scaling, it may help.\n - There is not much sence in scaling features that are already 0 or 1 like Sex, but for now I scale them all. You can try to pick features for scaling. If you don't use bins (if you use Age or Fare \"as is\"), scaling may help to boost your score a bit, try it.\n\n\n## Workflow goals\n\nThe data science solutions workflow solves for seven major goals.\n\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https:\/\/en.wikiversity.org\/wiki\/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.","6f59e5c0":"## Part 6.3 : Mean Median and Mode Method","c4ac1e8c":"### Part 6.3.3 : Finding The Accuracy","1d382f38":"### Part 6.1.2 : Training  & Testing","a7597da0":"### Part 5.1..2 : SimpleImputer Method ","e1cc9872":"**Data Dictionary**\n\n* survived: 0 = No, 1 = Yes\n* pclass:Ticket Class 1=1st, 2=2nd, 3=3rd\n* SibSp: # of Sibilings\/Spouses aboard the titanic (0 mentions neither have have Spuose nor Sibilings)\n* parch: # of parents\/children aboard the titanic\n* ticket: Ticket number\n* Cabin: Cabin Number\n* embarked: Port of Embarkation C= Cherboug, S= Southamptom, Q = Queenstown","5ba33982":"### Part 5.1.1 : Simple Row Del Method ","7729281d":"### Part 6.1.1 : Finding categorical feature","22a5dab4":"## Part 6.1 : Simple Row Del Method","cf61dea9":"### Part 6.1.3 : Finding The Accuracy","69773176":"# Part 2 : Importing & Exploring Data","c0daecff":"We've got a sense of our variables, their class type, and the first few observations of each. We know we're working with 1309 observations of 12 variables. In which 891 observations are from train data set, and 418 observations are from test data set. When separate the variables by type, we have ordinal variable PassengerId, lable variable Name and Ticket, numeric variables such as Age, SibSp, Parch, Fare, and categorical variables like Survived ,Pclass, Sex ,Cabin, and Embarked.","065ded7d":"# Part 1 : Importing Libraries","cf154f06":"\n\nWe can see from the Countplot above that female's survival rate is greater than male's.\n","98b244f2":"### Part 6.2.2 : Training & Testing","c3b16666":"### Part 6.2.3 : Finding The Accuracy","70453ea6":"### Part 6.3.2 : Training &Testing","dafed376":" **Data Cleaning**\n\nFrom the data set, we notice that there are missing values in Age, Cabin ,Fare and Embarked column. We are going to replace missing values in Age with a random sample from existing ages. For Cabin, since cabin number makes little sense to the result, we are going to create a new Cabin column to indicate how many cabins the passenger has.","94ccb81f":"# Part 6 : Finding categorical feature, Training Testing, and Accuracy Using Three Different Methods","3e410ba4":"# Part 5 : Data Cleaning Or Filling The Missing Values","2c78e4c4":"# Part 3 : Data Analyze by pivoting features"}}