{"cell_type":{"d2cfe10b":"code","1a4d2a40":"code","7c405cba":"code","b1f1c1bc":"code","af8dd586":"code","0e21fc94":"code","bda220aa":"code","6d8868cc":"code","6aea3326":"code","1333ce23":"code","63f12b9d":"code","9d45b9e2":"code","66658f17":"code","9a0cbc66":"code","5dab43e3":"code","c2a3f8af":"code","b6fec1e5":"code","1176069f":"code","45956054":"code","b6f113bc":"code","f1ffeb83":"code","6a52306a":"code","d87bc7e9":"code","aceb8641":"code","647fa80f":"code","6c9d67c5":"code","267fc984":"code","7385db0b":"code","7222f526":"code","ed2d99d3":"code","790d05d5":"code","a95957e8":"markdown","b75970c1":"markdown","03f76725":"markdown","07e9f280":"markdown","abe89019":"markdown","d7fc4835":"markdown","03247dfe":"markdown","df377957":"markdown","e80e4ad0":"markdown","af8a0ddf":"markdown","c2154b41":"markdown","9f4a99c3":"markdown","199467f7":"markdown","2a408774":"markdown","18243151":"markdown","af30fee5":"markdown","3eda200a":"markdown","c0cca0ee":"markdown","93a37f3c":"markdown","ee2b9bd3":"markdown","434d6bcd":"markdown","15be4ae9":"markdown","a8695154":"markdown","c94bdf0a":"markdown"},"source":{"d2cfe10b":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom PIL import Image\nimport re\nimport datetime \n\n\n\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom wordcloud import WordCloud\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\n\ncolor = sns.color_palette()\n%matplotlib inline","1a4d2a40":"episode = pd.read_csv('..\/input\/chai-time-data-science\/Episodes.csv')\ndesc = pd.read_csv('..\/input\/chai-time-data-science\/Description.csv')\nctds_episodes = episode[episode['episode_id'].str.match('E')]\nctds_episodes = ctds_episodes.drop([0])\nctds_episodes = ctds_episodes.drop([78])","7c405cba":"episode[episode['episode_id']==\"E69\"][\"release_date\"]","b1f1c1bc":"episode[episode['episode_id']==\"E0\"][\"release_date\"]","af8dd586":"episode_1 = pd.read_csv('..\/input\/chai-time-data-science\/Cleaned Subtitles\/E1.csv')\nx=episode_1.Speaker.value_counts()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel('Times spoken')","0e21fc94":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nsany_len = episode_1[episode_1[\"Speaker\"]==\"Sanyam Bhutani\"][\"Text\"].str.len()\nax1.hist(sany_len,color='red')\nax1.set_title('Sanyanam Bhutani')\nabhi_len = episode_1[episode_1[\"Speaker\"]==\"Abhishek Thakur\"][\"Text\"].str.len()\nax2.hist(abhi_len,color='green')\nax2.set_title('Abhishek Thakur')\nfig.suptitle('Characters in Conversations')\nplt.show()\n","bda220aa":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nsany_len = episode_1[episode_1[\"Speaker\"]==\"Sanyam Bhutani\"][\"Text\"].str.split().map(lambda x: len(x))\nax1.hist(sany_len,color='red')\nax1.set_title('Sanyanam Bhutani')\nabhi_len = episode_1[episode_1[\"Speaker\"]==\"Abhishek Thakur\"][\"Text\"].str.split().map(lambda x: len(x))\nax2.hist(abhi_len,color='green')\nax2.set_title('Abhishek Thakur')\nfig.suptitle('Words in each conversation')\nplt.show()","6d8868cc":"cleaned_st_files = os.listdir('..\/input\/chai-time-data-science\/Cleaned Subtitles\/')\n\ndef add_duration(df):\n    df['colon_count'] = df['Time'].str.count(':')\n    df.loc[df['colon_count'] == 1, 'Time'] = '0:' + df.loc[df['colon_count'] == 1]['Time']\n    df['Time_dt'] = df['Time'].apply(lambda x: datetime.datetime.strptime(x, \"%H:%M:%S\"))\n    df['Duration'] = (df['Time_dt'] - datetime.datetime(1900, 1, 1)) \\\n        .apply(lambda x: x.total_seconds()).astype('int')\n    return df\n\nc_fs = []\nfor f in tqdm(cleaned_st_files):\n    df = pd.read_csv(f'..\/input\/chai-time-data-science\/Cleaned Subtitles\/{f}')\n    df = add_duration(df)\n    df['E'] = f.replace('.csv','')\n    c_fs.append(df)\nall_subs = pd.concat(c_fs)\n\nhost_subs = all_subs[all_subs['Speaker']=='Sanyam Bhutani']\nhost_subs['temp_list'] = host_subs['Text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in host_subs['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(50))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","6aea3326":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","1333ce23":"def create_corpus(target):\n    corpus = []\n    for x in target.str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ncorpus = create_corpus(host_subs[\"Text\"])\n\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:25] \n\nx,y=zip(*top)\nplt.figure(figsize=(20,10))\nplt.bar(x,y)","63f12b9d":"def get_top_word_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_word_trigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nplt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_word_bigrams(host_subs['Text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","9d45b9e2":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_word_trigrams(host_subs['Text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","66658f17":"print(episode[episode['heroes']=='Jeremy Howard']['episode_id'])\nprint(episode[episode['heroes']=='Parul Pandey']['episode_id'])\nprint(episode[episode['heroes']=='Abhishek Thakur']['episode_id'])\nprint(episode[episode['heroes']=='Gilberto Titericz']['episode_id'])\nprint(episode[episode['heroes']=='Mikel Bober-Irizar']['episode_id'])","9a0cbc66":"most_viewed_episode = ['E27.csv', 'E49.csv', 'E1.csv', 'E33.csv', 'E38.csv']\nc_files = []\nfor f in most_viewed_episode:\n    df = pd.read_csv(f'..\/input\/chai-time-data-science\/Cleaned Subtitles\/{f}')\n    c_files.append(df)\n\nall_likes = pd.concat(c_files)","5dab43e3":"def plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), \n                   title = None, colormap=None, title_size=40, image_color=False):\n    \"\"\"Code to plot wordcloud with image mask and title.\n       Adopted code from : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes\n    \"\"\"\n    stopwords = stop\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown', 'I'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    colormap=colormap,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(text)\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"gaussian\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nd = '..\/input\/masks\/masks-wordclouds\/'\n","c2a3f8af":"upvote_mask = np.array(Image.open(d + 'upvote.png'))\nmost_likes_text = \" \".join(all_likes[all_likes['Speaker']=='Sanyam Bhutani']['Text'])\nplot_wordcloud(most_likes_text, upvote_mask, max_words=250, max_font_size=300, colormap='jet',\n               title = 'Most common words by host in most liked videos')","b6fec1e5":"print(episode[episode['heroes']=='Tim Dettmers']['episode_id'])\nprint(episode[episode['heroes']=='Jeremy Howard']['episode_id'])\nprint(episode[episode['heroes']=='Tuatini Godard']['episode_id'])\nprint(episode[episode['heroes']=='Daniel Bourke']['episode_id'])\nprint(episode[episode['heroes']=='Radek Osmulski']['episode_id'])\n\n","1176069f":"most_avg_duration = ['E5', 'E27', 'E12', 'E61', 'E44']\nc_files = []\nfor f in most_viewed_episode:\n    df = pd.read_csv(f'..\/input\/chai-time-data-science\/Cleaned Subtitles\/{f}')\n    c_files.append(df)\n\nall_likes = pd.concat(c_files)","45956054":"user = np.array(Image.open(d + 'user.png'))\nmost_likes_text = \" \".join(all_likes[all_likes['Speaker']=='Sanyam Bhutani']['Text'])\nplot_wordcloud(most_likes_text, user, max_words=250, max_font_size=300, colormap='PuRd',\n               title = 'Most common words in question by host in videos w\/ highest avg. watch duration')","b6f113bc":"no_twitter_followers = 8613 #from twitter profile https:\/\/twitter.com\/bhutanisanyam1\/ as on 10-July\nno_subscriber = episode['youtube_subscribers'].sum() + 863\n# add kaggle followers\nno_kaggle_followers = 416\nno_blogpost_views = 1000000\n\nfig = go.Figure()\n\nfig.add_trace(go.Indicator(\n    title = 'Twitter Followers',\n    mode = \"number\",\n    value = no_twitter_followers,\n    domain = {'row': 0, 'column': 0}))\n\n\nfig.add_trace(go.Indicator(\n    title = \"Youtube subscriber\",\n    mode = \"number\",\n    value = no_subscriber,\n    domain = {'row': 0, 'column': 1}))\n\nfig.add_trace(go.Indicator(\n    title = 'Total user visits of Sanyam blogs',\n    mode = \"number\",\n    value = no_blogpost_views,\n    domain = {'row': 1, 'column': 0}))\n\nfig.add_trace(go.Indicator(\n    title = 'Kaggle Followers',\n    mode = \"number\",\n    value = no_kaggle_followers,\n    domain = {'row': 1, 'column': 1}))\n\n\n\nfig.update_layout(width=700,height=400,title='<b>Social profile of Sanyam Bhutani<\/b>',\n                  template='seaborn',margin=dict(t=60,b=10,l=10,r=10),\n                  grid = {'rows': 2, 'columns': 2, 'pattern': \"independent\"},paper_bgcolor='#99ff66')","f1ffeb83":"ama_episode = pd.read_csv('..\/input\/chai-time-data-science\/Cleaned Subtitles\/E69.csv')\nama_episode['Text'][1]","6a52306a":"df = ctds_episodes.heroes_kaggle_username.value_counts()\nkagglers = df.shape[0]\n# pie chart kagglers vs non kagglers\nnon_kagglers = ctds_episodes.shape[0] - kagglers\n\nlabels = ['Kagglers','Non Kagglers']\nvalues = [kagglers, non_kagglers]\nnight_colors = ['#0bf', '#95628f',]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, pull=[0, 0.2,], marker_colors=night_colors)])\nfig.show()\n","d87bc7e9":"df_meta = pd.read_csv('\/kaggle\/input\/meta-kaggle\/Users.csv')\ndf_users= df_meta[df_meta['UserName'].isin(list(ctds_episodes.heroes_kaggle_username.unique()[1:]))].copy()\ncolors = {0:'#5ac995',1:'#0bf',2:'#95628f',3:'#f96517',4:'#dca917',5:'#008abc'}\nlabels = {0:'Novice',1:'Contributor',2:'Expert',3:'Master',4:'Grandmaster',5:'Kaggle team'}\ngroup = df_users.groupby('PerformanceTier',as_index=False)['Id'].count()\ngroup['labels'] = group['PerformanceTier'].map(labels)\ngroup['colors'] = group['PerformanceTier'].map(colors)\nfig = go.Figure(data=[go.Pie(labels=group.labels,values=group.Id)])\nfig.update_traces(hoverinfo='label+value', textinfo='percent', textfont_size=20,\n                  marker=dict(colors=group.colors, line=dict(color='#000000', width=1.5)))\nfig.update_layout(title=\"Heroes Kaggle Tier Distribution\",width=700,height=400,barmode='stack',template='seaborn',hovermode='x unified',\n                 legend=dict(title='<b><i>Performance Tier<\/i><\/b>',x=0.835),margin=dict(t=35,b=10,l=10,r=10),\n                 xaxis=dict(title='Number of Heroes',mirror=True,linewidth=2,linecolor='black',showgrid=False),\n                 yaxis=dict(mirror=True,linewidth=2,linecolor='black',gridcolor='darkgray'))\nfig.show()\n","aceb8641":"# most of code is adoped from: https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-back-at-your-kaggle-journey\nuser_name = 'init27'\nusers_df = pd.read_csv(\"..\/input\/meta-kaggle\/Users.csv\")\nfollowers_df = pd.read_csv('..\/input\/meta-kaggle\/UserFollowers.csv')\nuser_df = users_df[users_df[\"UserName\"]==user_name]\nuser_id = user_df[\"Id\"].values[0]\nuser_display = user_df[\"DisplayName\"].values[0]\nprint(\"The user id for the given user name is : \",user_id)\nprint(\"The display name for the given user name is : \",user_display)","647fa80f":"followers_df[followers_df['UserId']==1790018].UserId.value_counts()","6c9d67c5":"team_members_df = pd.read_csv(\"..\/input\/meta-kaggle\/TeamMemberships.csv\")\nteam_df = pd.read_csv(\"..\/input\/meta-kaggle\/Teams.csv\")\ncomp_df = pd.read_csv(\"..\/input\/meta-kaggle\/Competitions.csv\")\n\ntemp_df = team_members_df[team_members_df[\"UserId\"]==user_id]\ntemp_df = pd.merge(temp_df, team_df, left_on=\"TeamId\", right_on=\"Id\", how=\"left\")\ntemp_df = pd.merge(temp_df, comp_df, left_on=\"CompetitionId\", right_on=\"Id\", how=\"left\")\ntemp_df[\"DeadlineDate\"] = pd.to_datetime(temp_df[\"DeadlineDate\"], format=\"%m\/%d\/%Y %H:%M:%S\")\ntemp_df[\"DeadlineYear\"] = temp_df[\"DeadlineDate\"].dt.year\ntemp_df[\"DeadlineDate\"] = temp_df[\"DeadlineDate\"].apply(lambda x: datetime.date(x.year,x.month,1))\n\ntemp_df = temp_df[~np.isnan(temp_df[\"PrivateLeaderboardRank\"])]\ntemp_df.head()\n\ndef scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x=cnt_srs.index[::-1],\n        y=cnt_srs.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ncnt_df = temp_df.groupby('DeadlineYear')['PrivateLeaderboardRank'].agg([\"size\", \"mean\", \"min\"])\ncnt_srs = cnt_df[\"size\"]\ncnt_srs = cnt_srs.sort_index()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=\"green\",\n    ),\n)\n\nlayout = go.Layout(\n    title='Count of competitions over years',\n    font=dict(size=14),\n    width=800,\n    height=500,\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n\n### Mean Private Rank ###\ncnt_srs = cnt_df[\"mean\"]\ncnt_srs = cnt_srs.sort_index()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=\"blue\",\n    ),\n)\n\nlayout = go.Layout(\n    title='Mean Rank over years',\n    font=dict(size=14),\n    width=800,\n    height=500,\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n\n\n### Best rank each year ###\ncnt_srs = cnt_df[\"min\"]\ncnt_srs = cnt_srs.sort_index()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=\"red\",\n    ),\n)\n\nlayout = go.Layout(\n    title='Best Rank in each year',\n    font=dict(size=14),\n    width=800,\n    height=500,\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n","267fc984":"def bar_chart(cnt_srs, color):\n    trace = go.Bar(\n        x=cnt_srs.index,\n        y=cnt_srs.values,\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\nkernels_df = pd.read_csv(\"..\/input\/meta-kaggle\/Kernels.csv\")\ntemp_df = kernels_df[kernels_df[\"AuthorUserId\"]==user_id]\ntemp_df[\"MadePublicDate\"] = pd.to_datetime(temp_df[\"MadePublicDate\"], format=\"%m\/%d\/%Y\")\ntemp_df[\"MadePublicYear\"] = temp_df[\"MadePublicDate\"].dt.year\ntemp_df.head()\n\n# Number of kernels\ncnt_srs = temp_df[\"MadePublicYear\"].value_counts()\ntraces = [bar_chart(cnt_srs, \"blue\")]\nlayout = go.Layout(\n    title='Number of kernels in each year',\n    font=dict(size=14),\n    width=800,\n    height=500,\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.iplot(fig, filename='stacked-bar')\n\n\n# Number of views\ncnt_srs = temp_df.groupby(\"MadePublicYear\")[\"TotalViews\"].mean()\ntraces = [bar_chart(cnt_srs, \"green\")]\nlayout = go.Layout(\n    title='Mean number of views per kernel',\n    font=dict(size=14),\n    width=800,\n    height=500,\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.iplot(fig, filename='stacked-bar')\n\n# Number of votes\ncnt_srs = temp_df.groupby(\"MadePublicYear\")[\"TotalVotes\"].mean()\ntraces = [bar_chart(cnt_srs, \"red\")]\nlayout = go.Layout(\n    title='Mean number of votes per kernel',\n    font=dict(size=14),\n    width=800,\n    height=500,\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.iplot(fig, filename='stacked-bar')\n\n","7385db0b":"forum_message_df = pd.read_csv(\"..\/input\/meta-kaggle\/ForumMessages.csv\")\ntemp_df = forum_message_df[forum_message_df[\"PostUserId\"]==user_id]\ntemp_df[\"PostDate\"] = pd.to_datetime(temp_df[\"PostDate\"], format=\"%m\/%d\/%Y %H:%M:%S\")\ntemp_df[\"PostYear\"] = temp_df[\"PostDate\"].dt.year\ntemp_df.head()\n\n\n\ncnt_df = temp_df.pivot_table(index=\"PostYear\", columns=\"Medal\", values=\"PostUserId\", aggfunc=\"count\")\ncnt_df = cnt_df.fillna(0)\n\ndef get_bar_chart(cnt_srs, name, color):\n    trace = go.Bar(\n        x=cnt_srs.index,\n        y=cnt_srs.values,\n        name=name,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\nmedal_map = {1.:\"Gold\", 2.:\"Silver\", 3.:\"Bronze\"}\ncolor_map = {1.:\"gold\", 2.:\"silver\", 3.:\"darkorange\"}\ntraces = []\nfor col in np.array(cnt_df.columns)[::-1]:\n    cnt_srs = cnt_df[col]\n    traces.append(get_bar_chart(cnt_srs, medal_map[col], color_map[col]))\n\nlayout = go.Layout(\n    title='Discussion Medals in each year',\n    font=dict(size=14),\n    barmode='stack',\n    width=800,\n    height=500,\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.iplot(fig, filename='stacked-bar')","7222f526":"import re\ndef clean_string(txt):\n    txt = str(txt)\n    txt = re.sub(\"<.*?>\", \"\", txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt\n\ntemp_df[\"Message\"] = temp_df[\"Message\"].apply(lambda x: clean_string(x))\n#temp_df.head()\ntext = \" \".join(temp_df[\"Message\"])\n\n\nsanyam_array = np.array(Image.open(d+'comment.png'))\nplot_wordcloud(text, sanyam_array, title=\"Word Cloud of the common words in forum messages\")","ed2d99d3":"df = ctds_episodes.recording_time.value_counts()\nsunflowers_colors = ['rgb(177, 127, 38)', 'rgb(205, 152, 36)', 'rgb(99, 79, 37)',\n                     'rgb(129, 180, 179)', 'rgb(124, 103, 37)']\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=df.index, values=df.values, hole=.3, marker_colors=sunflowers_colors)])\nfig.show()","790d05d5":"fig = px.bar(ctds_episodes,x = \"heroes_location\",color = 'recording_time', title=\"Bar_Graph : Recording Time Analysis\")\nfig.show()","a95957e8":"<div class=h3>Number of Characters in Episode1<\/div>","b75970c1":"<div class=h2> Social proof<\/div>\n\n- It's very important in current world to have a very vibrant social profile to have success in the field you are working on.\n- Sanyam has more views in platform youtube from external links than people organically watching his videos in youtube which is a sign of his social profile sucess.","03f76725":"Let's look out of that, how much are **filler words or stop words**.","07e9f280":"<style type=\"text\/css\">\n\ndiv.h2 {\n    background-color: steelblue; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 24px; \n    max-width: 1500px; \n    margin-top: 50px;\n    margin-bottom:4px;\n}\n    \nbody {\n  font-size: 12px;\n}    \n     \n                                    \n                                      \ndiv.h3 {\n    color: #159957; \n    font-size: 18px; \n    margin-top: 20px; \n    margin-bottom:4px;\n    text-decoration: underline;\n}\n   \n                                      \ndiv.h1 {\n    background-color: steelblue; \n    color: white;\n    padding: 8px; \n    padding-right: 300px; \n    font-size: 32px; \n    margin-top: 20px; \n    max-width: 1500px;\n    margin-bottom: 8px;\n}    \n<\/style>    \n ","abe89019":"- Sanyanam conduct a special Ask me anything episode on his birthay, on finding that episode. We are able to see his birthay is 27th May.\n- Our host said he had turned into ago of 23 during that episode. So his DOB is: (27\/05\/1997)\n\n<div class=h3> When is the first anniversary of CTDS show \u2753<\/div>","d7fc4835":"<div class=h3> What are the future plans for Chai time data science?<\/div>\n\n- what are the top enhancements or changes you're working on for CTDS? Anything to look forward to, in the near future? (asked by Rohan Rao)\n\n> There are a lot of exciting things that I am I've been working on. So three things. I'm launching a new podcast. Yes, a new podcast called \"Chai Time Data Science News\" CTDS.news. You can find another write up that will live with this blog post. You can read all about it. But the idea is to give you a short news podcast, ideally in three to five minutes, always less than 10 minutes for you to be able to be on top of data science news, basically. Now if you know me, I am completely community driven. So if you have any thoughts around around that, please let me know the first episode is supposed to go live somewhere in the first week of June, so in a few days from now, but we'll see how that goes. Secondly, you as you might know, I have been subtitling, every single one of the interview that goes out is all of the two interviews that go out and have been going on since January. And now I will be starting a blog version release of these also in the first week of June or probably before that. So you can read you can expect blog releases and also short, interesting parts of the conversation short clips. I was going to call it cutting chai shorts from \"Chai Time Data Science.\\' So really excited about that. I also have a new set a new camera a new mic my new look. But we'll see how that goes.","03247dfe":"<div class=h2> Does Sanyam have a bias towards having Kagglers for his podcast?<\/div>\n\nThis question came to my mind when I heard Sebastin Ruder describe in his NLP newsletter:\n\n**Chai Time Data Science** \u2014Sanyam Bhutani\u2019s interviews with kagglers and ML researchers","df377957":"<div class=h1> Kaggle Journey of Sanyam<\/div>\n\nSanyam Bhutani is an active Kaggler himself, obviously inspired by overhelming majority of Kagglers he have interviewed so far. Sanyam himself is a 1X Kaggle Master and 2X Kaggle expert. Let's take a brief look at his Kaggle journey throughout the years which was possible only by [SRK's fabulous notebook](https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-back-at-your-kaggle-journey).\n\n![Screenshot_2020-07-13%20Sanyam%20Bhutani%20Kaggle.png](attachment:Screenshot_2020-07-13%20Sanyam%20Bhutani%20Kaggle.png)\n\n<div class=h3> Competition Journey<\/div>","e80e4ad0":"<div class=h2> About Lex Fridman<\/div>\n\nOne of most prominent interview series with people in artifical intelligence is being run by Lex Fridman which titled Artificial Intelligence podcast . Let's briefly take a look at some of unique things about Lex Fridman:\n\n- Lex is a AI researcher working on autonomous vehicles, human-robot interaction, and machine learning at MIT and beyond. He has published numerous research papers through out the years which can be [found here](https:\/\/lexfridman.com\/).\n- Lex through his connections have interviewed numerous people all walks of life like Elon Musk(CEO of Tesla), Bjarne Stroustrup(creator of C++) and other who comes from diverse backgrounds. Which in times has pushed the boundaries of his AI in his podcast.\n- So far very few Kagglers have been interviewed by Lex Fridman, which is very interesting.\n\n","af8a0ddf":"From the above chart it's clear most of the interviews happend at Night time in India. Yet if you look at the locations and their local times there are a few interesting patterns:\n- Most of the heroes prefer having interviews setup at **morning in their local time**.\n- It seems a few heroes like Ryan Chesler, Amo Candel had a late night interview at their local time\n- Our host has sacrificied his nights for atleast 45% of the entire episodes.","c2154b41":"<div class=h1>Understanding the CTDS host(Sanyam Bhutani)<\/div>\n\n[Chai Time Data science show](https:\/\/chaitimedatascience.com\/) is a podcast series hosted by [Sanyam Bhutani.](https:\/\/www.kaggle.com\/init27).This show features interviews researchers, practitioners and Kagglers in Data science community.\n\n\n![sanyanam.png](attachment:sanyanam.png)\n\nI personally have been watching Chai time data science show regularly. It has helped me a lot while learning new concepts & approaching practically new\nproblems with insights gained from this interviews.\n\n<b>My Submission:<\/b> Following are parts of Kernels Submissions in order:<br>\n\n<ul>\n    <li><a href=\"https:\/\/www.kaggle.com\/kurianbenoy\/analysing-ctds-show-views-and-reach\/\" target=\"_blank\">Part 1: Analysing CTDS show - views and reach <\/a>  <\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/kurianbenoy\/understanding-the-ctds-host-sanyam-bhutani\" target=\"_blank\">Part 2: Understanding the CTDS host - Sanyam Bhutani<\/a>  <\/li>\n<\/ul>","9f4a99c3":"<div class=h2> Episode 1: Abhishek Thakur <\/div>\n    \n<div class=h3> No of times spoken<\/div>","199467f7":"<div class=h2>Analysing host's question pattern during Interviews<\/div>\n\n<div class=h3> Common words spoken by our host<\/div>","2a408774":"<div class=h2> Importing Libraries<\/div>","18243151":"In this notebook we will be looking to learn more about Sanyanam Bhutani who is the host of Chai time datascience. Sanyanam calls himself a community-made data scientist. He is currently working as ML Engineer and Content creator at H20.ai. Sanyam is a avid kaggler and proud fastai student.\n<hr>","af30fee5":"Let's look at the most commonly occuring Bigrams and Trigrams spoken by host by frequency\n\n- Bigrams are most frequently used two common words spoken by host\n- Trigrams are most frequently used three words spoken by host","3eda200a":"<div class=h2> Happy birtday host<\/div>\n\n![birthday.jpeg](attachment:birthday.jpeg)\n\n","c0cca0ee":"<div class=h3> Notebooks journey so far<\/div>","93a37f3c":"<div class=h3>Number of words in the Episode1 <\/div>","ee2b9bd3":"<div class=h3> Discussion Journey So far<\/div>","434d6bcd":"<div class=h3> Acknowledgments<\/div>\n\n- https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes\n- https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","15be4ae9":"> So the first anniversary of CTDS show is on July 21st, 2020.","a8695154":"<div class=h2>When were the episode being recorded\u2753<\/div>","c94bdf0a":"- This graph shows almost 38 people who have been featured in the podcast have spend significant amount of time to be a Kaggler(who are atleast a Kaggle expert or above in tiers)\n- Having so many Kagglers has turned this into a interview series of Kagglers with about 50% of people being actively involved in Kaggle(above contributor and novice level)."}}