{"cell_type":{"2f227e57":"code","cd19a798":"code","d487031b":"code","0d00bd96":"code","dfec2d3f":"code","96477531":"code","7c40c968":"code","1288326a":"code","5576194f":"code","bf1be9b4":"code","514a129e":"code","96d7315d":"code","b431b924":"code","0c2fd853":"code","4a4543e6":"code","70096655":"code","dd13fd15":"code","fd7b0c2b":"code","57607ab3":"code","ce12a419":"code","be1fc766":"code","0c6b48aa":"code","001f063b":"code","3a0088b4":"code","4e8a84db":"code","6195384f":"code","e5927db3":"code","f75bd0d8":"code","518aca53":"code","b9fbf90c":"code","bd11ddcb":"code","9da8acea":"code","5afac8a9":"markdown","9f3e5095":"markdown","9e333ddd":"markdown","789d5a9a":"markdown","a0cd5ffe":"markdown","22a51649":"markdown"},"source":{"2f227e57":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","cd19a798":"#load data \ndf=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","d487031b":"#counting no. of rows and column in dataset\ndf.shape\n","0d00bd96":"#count no. of empty (NaN,na)values in each column\ndf.isna().sum()","dfec2d3f":"#Droping colomn with all missing value \ndf=df.dropna(axis=1)","96477531":"df.describe()","7c40c968":"#Showing updated dataset\ndf.shape","1288326a":"#Get count of the number of Malignant(M) or Benign(B) cells\ndf[\"diagnosis\"].value_counts()","5576194f":"#visualize the count \nsns.countplot(df[\"diagnosis\"],label='count')","bf1be9b4":"#Look at the data type to see which columns are encoded \ndf.dtypes","514a129e":"#encoding categorical data value\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_Y=LabelEncoder()\ndf.iloc[:,1]=labelencoder_Y.fit_transform(df.iloc[:,1].values) #get all rows of column 1\n\ndf.iloc[:,1]","96d7315d":"#let Create a pair plot of some columns \n\nsns.pairplot(df.iloc[:,1:7],hue='diagnosis',height = 4)\n\n\n","b431b924":"#Correlation Table of the dataframe\ndf.iloc[:,1:].corr()","0c2fd853":"#Correaltion matrix of diagnosis with other features\ncorr_matrix=df.corr()   \ncorr_matrix['diagnosis'].sort_values(ascending=False)","4a4543e6":"#Visualizing corrrealtion \nplt.subplots(figsize=(30,25))\n\nsns.heatmap(df.iloc[:,1:].corr(),annot=True,fmt=\".0%\")  #fmt gives us percentage ","70096655":"#Split the dataset into X and Y \nX=df.drop([\"diagnosis\"],axis=1)\nY=df[\"diagnosis\"]","dd13fd15":"#split data set into 75% training and 25% test dataset\nfrom sklearn.model_selection import train_test_split \n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=0)","fd7b0c2b":"test_id=X_test[\"id\"] #for storing patient id \nX_train=X_train.drop([\"id\"],axis=1)\nX_test=X_test.drop([\"id\"],axis=1)","57607ab3":"#feature scaling \nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.fit_transform(X_test)","ce12a419":"#Logistic regression \nfrom sklearn.linear_model import LogisticRegression\nlog=LogisticRegression(random_state=0)\nlog.fit(X_train,Y_train)\n\n\n#Decison tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\ntree.fit(X_train,Y_train)\n    \n#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nforest=RandomForestClassifier(n_estimators=10,criterion=\"entropy\",random_state=0)\nforest.fit(X_train,Y_train)\n\n\n#SVM\nfrom sklearn.svm import SVC \nsvc  =SVC()\nsvc.fit(X_train,Y_train) \n\n#Prediction using XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train, Y_train)","be1fc766":"from sklearn.metrics import accuracy_score\n\nprint(\" accuracy report log\",accuracy_score(Y_test, log.predict(X_test)))\nprint(\"\\n accuracy report tree\",accuracy_score(Y_test, tree.predict(X_test)))\nprint(\"\\n accuracy report forest\",accuracy_score(Y_test, forest.predict(X_test)))\nprint(\"\\n accuracy report SVC\",accuracy_score(Y_test, svc.predict(X_test)))\nprint(\"\\n accuracy report XGB\",accuracy_score(Y_test, xgb.predict(X_test)))","0c6b48aa":"from sklearn.metrics import classification_report\n\nprint(\"Classification report of Random Forest Classfier :-\\n\")\ncla_rep=classification_report(Y_test, forest.predict(X_test))\nprint(cla_rep)","001f063b":"#print the prediction of random forest classifier\npred=forest.predict(X_test)\nprint(\"Predicted values are-\\n\",pred)\nprint(\"------\")\nprint(\"True values are-\\n\",(Y_test).values)\n","3a0088b4":"submission = pd.DataFrame({\n        \"id\": test_id,\n        \"diagnosis\": pred\n        })\nsubmission.head()","4e8a84db":"'''Making a Csv file for our submission'''\n#submission.to_csv('my_submission.csv', index=False)\n","6195384f":"#Visualising our predictions  \nprint(submission[\"diagnosis\"].value_counts())\nsns.countplot(submission[\"diagnosis\"],label='count')","e5927db3":"Xsel=X[[\"concave points_worst\",\"perimeter_worst\",\"concave points_mean\",\n\"radius_worst\",\n\"perimeter_mean\",\n\"area_worst\"]]","f75bd0d8":"#split data set into 75% training and 25% test dataset\n\nX_train_sel,X_test_sel,Y_train_sel,Y_test_sel=train_test_split(Xsel,Y,test_size=0.25,random_state=0)","518aca53":"#feature scaling \nsc=StandardScaler()\nX_train=sc.fit_transform(X_train_sel)\nX_test=sc.fit_transform(X_test_sel)","b9fbf90c":"#Logistic regression \nfrom sklearn.linear_model import LogisticRegression\nlog=LogisticRegression(random_state=0)\nlog.fit(X_train_sel,Y_train_sel)\n\n\n#Decison tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\ntree.fit(X_train_sel,Y_train_sel)\n    \n#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nforest=RandomForestClassifier(n_estimators=10,criterion=\"entropy\",random_state=0)\nforest.fit(X_train_sel,Y_train_sel)\n\n\n#SVM\nfrom sklearn.svm import SVC \nsvc  =SVC()\nsvc.fit(X_train_sel,Y_train_sel) \n\n#Prediction using XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train_sel,Y_train_sel)","bd11ddcb":"from sklearn.metrics import accuracy_score\n\nprint(\" accuracy report log\",accuracy_score(Y_test_sel, log.predict(X_test_sel)))\nprint(\"\\n accuracy report tree\",accuracy_score(Y_test_sel, tree.predict(X_test_sel)))\nprint(\"\\n accuracy report forest\",accuracy_score(Y_test_sel, forest.predict(X_test_sel)))\nprint(\"\\n accuracy report SVC\",accuracy_score(Y_test_sel, svc.predict(X_test_sel)))\nprint(\"\\n accuracy report XGB\",accuracy_score(Y_test_sel, xgb.predict(X_test_sel)))","9da8acea":"print(\"Classification report of XGBoost  :-\\n\")\nxgb_rep=classification_report(Y_test_sel, forest.predict(X_test_sel))\nprint(xgb_rep)","5afac8a9":"* **concave points_worst**  -     0.793566 <br>\n* **perimeter_worst**       -     0.782914  <br>\n* **concave points_mean**   -     0.776614 <br>\n* **radius_worst**          -     0.776454 <br>\n* **perimeter_mean**        -     0.742636  <br>\n* **area_worst**            -     0.733825  <br>","9f3e5095":"# **Importing Libraries**","9e333ddd":"M =1 and B=0","789d5a9a":"![image.png](attachment:image.png)","a0cd5ffe":"# The END ","22a51649":"# Let's Train Model with Feature Selction"}}