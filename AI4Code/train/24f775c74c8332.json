{"cell_type":{"874e2cd7":"code","47cfa7c6":"code","9b9f35d6":"code","fae24862":"code","62b800ac":"code","edc7fadb":"code","87c6ada2":"code","2b69d804":"code","640b51d1":"code","f22bc6b4":"code","7af47e7b":"code","2a2af176":"code","9b99f53b":"code","cc208793":"code","6ae5f8ba":"code","a2c9641f":"code","67af0fdd":"code","b625d920":"code","d3d379f4":"code","5cf226ce":"code","4d791fe1":"code","b44c1aef":"code","c4eb8e58":"code","e7216d34":"code","dbe9cf36":"code","bae67240":"code","c81812d5":"code","5ad49af6":"code","3da9240e":"code","f584a1fe":"code","3b6a080b":"code","d9887639":"code","9cecc8e7":"code","bb975d94":"code","4c9074c1":"code","1da76a90":"code","9c57a919":"code","7799577d":"code","cd582823":"code","11e6f081":"code","d8743004":"code","c6162040":"code","99bde441":"code","80d47d79":"code","5eae6a5f":"code","90015e38":"code","87176769":"code","f0978fbc":"code","bca3d358":"code","a53d1d66":"code","4a8cc410":"code","b6301812":"code","125accd0":"code","9758a229":"code","57a15ef6":"code","57fef761":"code","85b6de9b":"code","59ca75e6":"code","b06acf11":"code","23c6c28f":"code","233dcdf9":"code","59e5ba77":"code","ba306790":"code","66c5bcf8":"code","dfce1255":"code","7e72c905":"code","a311ade4":"code","b50e9cc3":"code","5ff322e5":"code","e0078844":"code","3f712eb9":"code","69f731a2":"code","d6cde01a":"code","682c3593":"code","e75e7fd9":"code","dc99e68c":"code","0338c3e0":"code","6fd9c2d2":"code","7e529f9c":"code","ec639a0d":"code","3b40f462":"code","31777c57":"code","6035d807":"code","0be8d725":"code","03709d13":"code","93b6503f":"code","e9abb771":"code","05ec089a":"code","fcf5aab3":"code","b7b333c8":"code","b184bb95":"code","cbf9074a":"code","252422ce":"code","2f257824":"code","9ecd1ee1":"code","495a4a41":"code","43c4fdc9":"code","34293d66":"code","2e0650c8":"code","e1d94d13":"code","36e34c23":"code","a118dc35":"code","fc3d7483":"code","999e0b6f":"code","0e4503d7":"code","a574f166":"code","3b25c1ed":"code","84e71839":"code","7f42d9a6":"code","591085dd":"code","3e22b186":"code","4ec66a58":"code","9427347d":"code","27105430":"code","768b539a":"code","98ceb24a":"code","b906476e":"code","f665dd5a":"code","627234bd":"code","7089e432":"code","f97a5f95":"code","93c403dc":"code","b7e3e6b9":"code","8bcc9ff2":"code","299cf109":"code","fbcca9ec":"code","1747d0e9":"code","3277b792":"code","1417d7ea":"code","3bdd7e71":"code","ea8617f3":"code","994e845f":"code","c87fd235":"code","58788973":"code","9a8c3dbd":"code","3a987b3f":"code","dc48dd63":"code","c7114a3d":"code","f89216b1":"code","329e1046":"code","eb4841ef":"code","d97ffbac":"code","b0bb75d0":"code","4ed42947":"code","5fdd0123":"code","c3e4e870":"code","73ee9814":"code","dff71f18":"code","a01da4eb":"code","6a3ed7c8":"code","e1a920e9":"code","cdb7e426":"code","1c81e137":"code","f946b634":"code","2371d9f7":"code","44ddd53b":"code","ce4f51e2":"code","c4c11da3":"code","b7a428c6":"code","04b9de73":"code","8989f54f":"code","4fc089a5":"code","fcecd58b":"code","e9e23b27":"code","39c502cf":"code","5aed901e":"code","694d3f0a":"code","9a249041":"code","738c1241":"code","890e8ca4":"code","28d6187c":"code","f8019916":"code","768464f6":"code","9aeae84d":"code","97bf493c":"code","5e501df5":"code","42ea1308":"code","e6d0ef36":"markdown","bc67e557":"markdown","f7a1d07e":"markdown","e71944e7":"markdown","0ced86af":"markdown","845cacf2":"markdown","b768908e":"markdown","0e0cf295":"markdown","acfc7ca9":"markdown","28ee5be7":"markdown","0c41525b":"markdown","7707dbaa":"markdown","0605e067":"markdown","06019a11":"markdown","1b29553f":"markdown","85fa6f0b":"markdown","5bf1cdeb":"markdown","bd1c9244":"markdown","84f9f800":"markdown","4e08f680":"markdown","427fea5f":"markdown","20f5445e":"markdown","1b3b8847":"markdown","7ea40e8b":"markdown","a8c8f7d1":"markdown","a346a08e":"markdown","d1f9f238":"markdown","f11ee654":"markdown","4a13bcfe":"markdown","a11362b7":"markdown","e05b1b7d":"markdown","df5e256b":"markdown","ed1f7d19":"markdown","dce177f8":"markdown","76dd6cd8":"markdown","aafd6fb0":"markdown","8c20bf80":"markdown","72b1e57c":"markdown","4b39d250":"markdown","f9129cc1":"markdown","37b8f5ac":"markdown","fc8ebe90":"markdown","cb1c5888":"markdown","4693f9fc":"markdown","4cc1ac33":"markdown","b4e338fd":"markdown","88666771":"markdown","10b3c8ab":"markdown","b757c803":"markdown","31f804ae":"markdown","677fc91e":"markdown","ca212bc1":"markdown","aa0b82bc":"markdown","d7018a4d":"markdown","affc8b45":"markdown","cdf1e115":"markdown","1d43481e":"markdown","0075e8b1":"markdown","ca4d77e9":"markdown","cef82baf":"markdown","f6be9885":"markdown","7d0eece7":"markdown","76792a63":"markdown","f90fb41a":"markdown","2d1f4e1d":"markdown","b008b9f6":"markdown","29b9e1de":"markdown","30f3309f":"markdown","ee2ab417":"markdown","72dbceac":"markdown","a1c4ae86":"markdown","8db3272d":"markdown","dac991ff":"markdown","133980f7":"markdown","bf40555b":"markdown","9211c81e":"markdown","06d68990":"markdown","dc07c47e":"markdown","31c90b5b":"markdown","363272e5":"markdown"},"source":{"874e2cd7":"import pandas as pd\nimport numpy as np\nimport math\nimport pickle\nfrom scipy.stats import kruskal, pearsonr, randint, uniform, chi2_contingency, boxcox\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer, StandardScaler, power_transform\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nfrom sklearn.model_selection import cross_val_score, cross_validate, TimeSeriesSplit, RandomizedSearchCV, GridSearchCV, cross_val_predict\nfrom datetime import datetime\nfrom statsmodels.tsa.stattools import grangercausalitytests, adfuller, kpss, acf, pacf\nfrom collections import defaultdict, OrderedDict\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\nfrom sklearn.decomposition import PCA\nfrom statsmodels.tsa.ar_model import AR\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt \n\n%matplotlib inline\n","47cfa7c6":"# diplaying all columns without truncation in dataframes\npd.set_option('display.max_columns', 500)\n","9b9f35d6":"# read in bike sharing dataset\nbike_df = pd.read_csv('\/kaggle\/input\/bike-sharing-washington-dc\/bike_sharing_dataset.csv')\nbike_df.head()\n","fae24862":"# print descriptive statistics\nbike_df.describe()\n","62b800ac":"# check the datatypes of each variable\nbike_df.dtypes\n","edc7fadb":"# Check for missing values\nbike_df.isnull().sum()\n","87c6ada2":"# fill NAs with 0 where applicable\nwt_feats = [x for x in bike_df.columns if 'wt' in x]\nbike_df['holiday'] = bike_df['holiday'].fillna(0)\nbike_df[wt_feats] = bike_df[wt_feats].fillna(0)\n","2b69d804":"# check casual, registered and total_cust missing rows\nmissing_target = bike_df[bike_df['total_cust'].isna()]\nmissing_target\n","640b51d1":"# filling the missing values in the customer variables with forward fill method\nbike_df[['total_cust', 'casual', 'registered']] = bike_df[['total_cust', 'casual', 'registered']].fillna(\n                                                            method='ffill')\nbike_df.isnull().sum()\n","f22bc6b4":"# check what the correlation between the different temperature features and total_cust is\n# maybe I could just use one of the other temperature features instead of temp_avg\n\n# correlation between temp_avg and total_cust\n# I'm excluding the first 820 rows because they contain missing temp_avg values\nprint('temp_avg:', pearsonr(bike_df['temp_avg'][821:], bike_df['total_cust'][821:]))\n\n# correlation between temp_min and total_cust\nprint('temp_min:', pearsonr(bike_df['temp_min'], bike_df['total_cust']))\nprint('temp_min, without first 820 rows:', pearsonr(bike_df['temp_min'][821:], bike_df['total_cust'][821:]))\n\n# correlation between temp_max and total_cust\nprint('temp_max:', pearsonr(bike_df['temp_max'], bike_df['total_cust']))\nprint('temp_max, without first 820 rows:', pearsonr(bike_df['temp_max'][821:], bike_df['total_cust'][821:]))\n\n# correlation between temp_observ and total_cust\nprint('temp_observ:', pearsonr(bike_df['temp_observ'], bike_df['total_cust']))\nprint('temp_observ, without first 820 rows:', pearsonr(bike_df['temp_observ'][821:], bike_df['total_cust'][821:]))\n","7af47e7b":"# calculating the Granger causality between the temperature feats and the target\nprint('Average temperature and target:')\ngrangercausalitytests(bike_df[['total_cust', 'temp_avg']][821:], maxlag=1);\nprint('\\nMaximum temperature and target:')\ngrangercausalitytests(bike_df[['total_cust', 'temp_max']], maxlag=1);\nprint('\\nMinimum temperature and target:')\ngrangercausalitytests(bike_df[['total_cust', 'temp_min']], maxlag=1);\nprint('\\nObserved temperature and target:')\ngrangercausalitytests(bike_df[['total_cust', 'temp_observ']], maxlag=1);\n","2a2af176":"# function to create seasons for dataframe\ndef seasons(df):\n    '''\n    Function to create new features for seasons based on months\n    Args: df = dataframe\n    Returns: df = dataframe\n    '''\n    \n    # create a season features\n    df['season_spring'] = df['date'].apply(lambda x: 1 if '01' in x[5:7] else 1 if '02' in x[5:7] else 1 \n                                                     if '03' in x[5:7] else 0)\n    df['season_summer'] = df['date'].apply(lambda x: 1 if '04' in x[5:7] else 1 if '05' in x[5:7] else 1 \n                                                     if '06' in x[5:7] else 0)\n    df['season_fall'] = df['date'].apply(lambda x: 1 if '07' in x[5:7] else 1 if '08' in x[5:7] else 1 \n                                                     if '09' in x[5:7] else 0)\n    \n    return df\n","9b99f53b":"### create new features for seasons\nbike_df = seasons(bike_df)\n","cc208793":"### create new feature weekday\nbike_df['date_datetime'] = bike_df['date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n\nbike_df['weekday'] = bike_df['date_datetime'].apply(lambda x: x.weekday())\n","6ae5f8ba":"### one hot encode the feature weekday\nweekday_dummies = pd.get_dummies(bike_df['weekday'], prefix='weekday', drop_first=True)\nbike_df = bike_df.join(weekday_dummies, how='left')\nbike_df.head()\n","a2c9641f":"### create new feature working_day\nbike_df['working_day'] = bike_df['weekday'].apply(lambda x: 0 if x > 5 or x == 0 else 1)\nbike_df['working_day'] = bike_df[['holiday', 'working_day']].apply(\n    lambda x: 0 if x['holiday'] == 1 else x['working_day'], axis=1)\n","67af0fdd":"# Dropping date, registered and casual features because this is in string format \nbike_df.drop(columns=['date', 'temp_avg', 'registered', 'casual'], inplace=True)\n","b625d920":"# variable to be used below to iterate through the columns and plot them\nseason_names = ['season_spring', 'season_summer', 'season_fall']\n\n# plot boxplots for season versus number of users\nplt.figure(figsize = [15, 5])\n\n# boxplot for feature workingday\nplt.subplot(1, 3, 1)\nsb.boxplot(data = bike_df, x = 'season_spring', y = 'total_cust')\nplt.xlabel('Spring')\nplt.ylabel('Number of customers')\n\n# boxplot for feature weekday\nplt.subplot(1, 3, 2)\nsb.boxplot(data = bike_df, x = 'season_summer', y = 'total_cust')\nplt.xlabel('Summer')\nplt.ylabel('Number of customers')\n\n# boxplot for feature holiday\nplt.subplot(1, 3, 3)\nsb.boxplot(data = bike_df, x = 'season_fall', y = 'total_cust')\nplt.xlabel('Fall')\nplt.ylabel('Number of customers');\n","d3d379f4":"# Correlation between season features and the maximum temperature\n# using the Kruskal Wallis H test for correlations between a continuous and categorical variable\nkruskal(bike_df['temp_max'], bike_df['season_summer'])\n","5cf226ce":"# plotting the customer statistics in form of a boxplot for the holiday feature\nsb.boxplot(data = bike_df, x = 'holiday', y = 'total_cust');\n","4d791fe1":"# Correlation between holiday feature and the number of customers per day\n# using the Kruskal Wallis H test for correlations between a continuous and categorical variable\nkruskal(bike_df['holiday'], bike_df['total_cust'])\n","b44c1aef":"# plotting the customer statistics in form of a boxplot for the weekday feature\nplt.figure(figsize = [15, 5])\nsb.boxplot(data = bike_df, x = 'weekday', y = 'total_cust')\nplt.xlabel('Weekday')\nplt.ylabel('Number of customers');\n","c4eb8e58":"# Correlation between weekday feature and the number of customers per day\n# using Pearson's correlation coefficient because I'm assuming that weekday can be \n# considered a continuous variable\npearsonr(bike_df['weekday'], bike_df['total_cust'])\n","e7216d34":"# plotting workday feature in boxplot against the count of customers\nsb.boxplot(data = bike_df, x = 'working_day', y = 'total_cust')\nplt.xlabel('Working day or not')\nplt.ylabel('Number of customers');\n","dbe9cf36":"# Correlation between working_day feature and the number of customers per day\n# using the Kruskal Wallis H test for correlations between a continuous and categorical variable\nkruskal(bike_df['working_day'], bike_df['total_cust'])\n","bae67240":"# create a new dataframe that encodes the weekday feature with 0 for monday through friday\n# and 1 for saturday and sunday\nweekend_distinct_df = bike_df.copy()\nweekend_distinct_df['weekday'] = weekend_distinct_df['weekday'].apply(lambda x: 1 if (x == 6 or x == 0) else 0)\n\n# plot boxplots for comparison between the weekday and workingday feature\nplt.figure(figsize = [15, 5])\n\n# boxplot for feature workingday\nplt.subplot(1, 3, 1)\nsb.boxplot(data = bike_df, x = 'working_day', y = 'total_cust');\n\n# boxplot for feature weekday\nplt.subplot(1, 3, 2)\nsb.boxplot(data = weekend_distinct_df, x = 'weekday', y = 'total_cust');\n\n# boxplot for feature holiday\nplt.subplot(1, 3, 3)\nsb.boxplot(data = bike_df, x = 'holiday', y = 'total_cust');\n","c81812d5":"# plot the means of each instance of workingday\nbike_df.groupby('working_day')['total_cust'].mean()\n","5ad49af6":"# plot the means of each instance of weekday\nweekend_distinct_df.groupby('weekday')['total_cust'].mean()\n","3da9240e":"# plot the means of each instance of holiday\nbike_df.groupby('holiday')['total_cust'].mean()\n","f584a1fe":"# plotting the revenue of the most common production companies vs. the rest\nfig, ax = plt.subplots(6, 3, figsize = [16, 25])\n\n# create list with all feature names \nwt_feat_list = [x for x in bike_df.columns if 'wt_' in x]\n\n# company counter\ncounter = 0\n\nfor j in range(len(ax)):\n    for i in range(len(ax[j])):\n        if j == 5 and i == 3:\n            break\n        else:\n            ax[j][i] = sb.boxplot(data = bike_df, x = wt_feat_list[counter], y = 'total_cust', ax=ax[j][i])\n            ax[j][i].set_ylabel('Number of customers')\n            ax[j][i].set_xlabel(wt_feat_list[counter])\n            counter += 1\n            ","3b6a080b":"# fog, heavy fog, hail, haze, high wind\nbike_df['foggy'] = bike_df['wt_fog'] + bike_df['wt_heavy_fog'] + bike_df['wt_hail'] + bike_df['wt_haze'] + bike_df['wt_high_wind']\nbike_df['foggy'] = bike_df['foggy'].apply(lambda x: 0 if x == 0 else 1)\n\n# thunder\nbike_df['thunder'] = bike_df['wt_thunder']\n\n# ice_fog, unknown, freeze_drizzle, freeze_rain, drift_snow\nbike_df['ice'] = bike_df['wt_ice_fog'] + bike_df['wt_unknown'] + bike_df['wt_freeze_drizzle'] + bike_df['wt_freeze_rain'] + bike_df['wt_drift_snow']\nbike_df['ice'] = bike_df['ice'].apply(lambda x: 0 if x == 0 else 1)\n\n# sleet, glaze, snow\nbike_df['sleet'] = bike_df['wt_sleet'] + bike_df['wt_glaze'] + bike_df['wt_snow']\nbike_df['sleet'] = bike_df['sleet'].apply(lambda x: 0 if x == 0 else 1)\n\n# mist, drizzle, rain, ground fog\nbike_df['rain'] = bike_df['wt_mist'] + bike_df['wt_drizzle'] + bike_df['wt_rain'] + bike_df['wt_ground_fog']\nbike_df['rain'] = bike_df['rain'].apply(lambda x: 0 if x == 0 else 1)\n\n","d9887639":"# drop the old wt features\nbike_df.drop(columns=wt_feats, inplace=True)\n","9cecc8e7":"# plot boxplots for comparison between the new weather type features and target\nplt.figure(figsize = [12, 10])\n\n# boxplot for feature foggy\nplt.subplot(2, 2, 1)\nsb.boxplot(data = bike_df, x = 'foggy', y = 'total_cust')\nplt.xlabel('Weather type foggy')\nplt.ylabel('Number of customers')\n\n# boxplot for feature thunder\nplt.subplot(2, 2, 2)\nsb.boxplot(data = bike_df, x = 'thunder', y = 'total_cust')\nplt.xlabel('Weather type thunder')\nplt.ylabel('Number of customers')\n\n# boxplot for feature ice\nplt.subplot(2, 2, 3)\nsb.boxplot(data = bike_df, x = 'ice', y = 'total_cust')\nplt.xlabel('Weather type ice')\nplt.ylabel('Number of customers')\n\n# boxplot for feature sleet\nplt.subplot(2, 2, 4)\nsb.boxplot(data = bike_df, x = 'sleet', y = 'total_cust')\nplt.xlabel('Weather type sleet')\nplt.ylabel('Number of customers');\n\n","bb975d94":"# plotting the acf of wt_ features\nplt.figure(figsize=[15,6])\n\nplot_acf(bike_df['foggy'], title='ACF: All samples in metro DC area',)\nplt.show()\n\nplot_acf(bike_df['thunder'], title='ACF: All samples in metro DC area',)\nplt.show()\n\nplot_acf(bike_df['ice'], title='ACF: All samples in metro DC area',)\nplt.show()\n\nplot_acf(bike_df['sleet'], title='ACF: All samples in metro DC area',)\nplt.show()\n\nplot_acf(bike_df['rain'], title='ACF: All samples in metro DC area',)\nplt.show()\n","4c9074c1":"# plotting all newly created features\nplt.figure(figsize=[20,9])\n\nplt.subplot(3,2,1)\nplt.plot(bike_df['thunder'])\n\nplt.subplot(3, 2, 2)\nplt.plot(bike_df['foggy'])\n\nplt.subplot(3, 2, 3)\nplt.plot(bike_df['ice'])\n\nplt.subplot(3, 2, 4)\nplt.plot(bike_df['sleet'])\n\nplt.subplot(3, 2, 5)\nplt.plot(bike_df['rain'])\n","1da76a90":"bike_df.drop(columns=['rain'], inplace=True)","9c57a919":"# get list for all rolling sums between the weather features and the target\n\ndef best_window_sum(x, y, max_window):\n    corr_temp_cust = []\n    for i in range(1, max_window):\n        roll_val = list(x.rolling(i).sum()[i-1:-1])\n        total_cust_ti = list(y[i:])\n        corr, p_val = pearsonr(total_cust_ti, roll_val)\n        corr_temp_cust.append(corr)\n    # get the optimal window size for rolling mean\n    max_val = np.argmax(corr_temp_cust)\n    min_val = np.argmin(corr_temp_cust)\n    opt_corr_min = corr_temp_cust[min_val]\n    opt_corr_max = corr_temp_cust[max_val]\n    \n    results = {max_val+1: opt_corr_max, min_val+1: opt_corr_min}\n    \n    return results\n    ","7799577d":"# get the optimal window for rolling sum for foggy\nprint(best_window_sum(bike_df['foggy'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by foggy\nfoggy_mean = bike_df['foggy'].rolling(8).sum()[7:-1]\npearsonr(foggy_mean, bike_df['total_cust'][8:])\n","cd582823":"# get the optimal window for rolling sum for thunder\nprint(best_window_sum(bike_df['thunder'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by thunder\nthunder_mean = bike_df['thunder'].rolling(8).sum()[7:-1]\npearsonr(thunder_mean, bike_df['total_cust'][8:])\n","11e6f081":"# get the optimal window for rolling std for ice\nprint(best_window_sum(bike_df['ice'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by ice\nice_mean = bike_df['ice'].rolling(8).sum()[7:-1]\npearsonr(ice_mean, bike_df['total_cust'][8:])\n","d8743004":"# get the optimal window for rolling std for sleet\nprint(best_window_sum(bike_df['sleet'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by sleet\nsleet_mean = bike_df['sleet'].rolling(8).sum()[7:-1]\npearsonr(sleet_mean, bike_df['total_cust'][8:])\n","c6162040":"# drop any non-categorical variables\nbike_df_corr_cat = bike_df.drop(columns=['date_datetime', 'weekday', 'temp_min', 'temp_max',\n                                         'temp_observ', 'precip', 'wind', 'total_cust'], axis=1)\n\n\n# this code snippet was taken from https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\n\n# create correlation matrix with cramer's V coefficients\ncorr_matrix = pd.DataFrame(data = None, index=np.arange(len(bike_df_corr_cat.columns)), \n                            columns=bike_df_corr_cat.columns)\n\nfor col in bike_df_corr_cat.columns:\n    count = 0\n    for val in bike_df_corr_cat.columns:\n        corr_cat = cramers_v(bike_df_corr_cat[col], bike_df_corr_cat[val])\n        corr_matrix[col][count] = corr_cat\n        count += 1\n    corr_matrix = corr_matrix.astype('float')\n\n\n# add an index to the dataframe\ncorr_matrix['columns'] = bike_df_corr_cat.columns\ncorr_matrix.set_index('columns', inplace=True)\n\n# plot a heatmap for correlations between categorical variables\nplt.figure(figsize=[15,10])\nsb.heatmap(corr_matrix, annot=True,\n          vmin=-1, vmax=1, center=0,\n          fmt='.2g', cmap='YlGnBu')\nplt.title('Heatmap of correlations between categorical variables')\nplt.xlabel('Columns')\nplt.ylabel('Columns');\n","99bde441":"# plot all distributions and scatterplot between each continuous variable pair\nsb.pairplot(bike_df, vars=['temp_min', 'temp_max', 'temp_observ', 'wind', 'precip', 'total_cust']);\n","80d47d79":"# create a correlation matrix\nbike_df_corr = bike_df[['temp_min', 'temp_max', 'temp_observ', 'wind', 'precip', 'total_cust']].corr()\n\n# create a heatmap to visualize the results\nplt.figure(figsize=[10,6])\nsb.heatmap(bike_df_corr, annot=True,\n          vmin=-1, vmax=1, center=0,\n          cmap='YlGnBu')\nplt.title('Heatmap of correlations between continuous variables')\nplt.xlabel('Columns')\nplt.ylabel('Columns');\n","5eae6a5f":"# get list for all correlations between a feature and total_cust with different rolling means\n\ndef best_window(x, y, max_window):\n    corr_temp_cust = []\n    for i in range(1, max_window):\n        roll_val = list(x.rolling(i).mean()[i-1:-1])\n        total_cust_ti = list(y[i:])\n        corr, p_val = pearsonr(total_cust_ti, roll_val)\n        corr_temp_cust.append(corr)\n    # get the optimal window size for rolling mean between a feature and total_cust\n    max_val = np.argmax(corr_temp_cust)\n    min_val = np.argmin(corr_temp_cust)\n    opt_corr_min = corr_temp_cust[min_val]\n    opt_corr_max = corr_temp_cust[max_val]\n    \n    results = {max_val+1: opt_corr_max, min_val+1: opt_corr_min}\n    \n    return results\n    ","90015e38":"# get list for all correlations between a feature and total_cust with different rolling standard deviations\n\ndef best_window_std(x, y, max_window):\n    corr_temp_cust = []\n    for i in range(2, max_window):\n        roll_val = list(x.rolling(i).std()[i-1:-1])\n        total_cust_ti = list(y[i:])\n        corr, p_val = pearsonr(total_cust_ti, roll_val)\n        corr_temp_cust.append(corr)\n    # get the optimal window size for rolling std between a feature and total_cust\n    max_val = np.argmax(corr_temp_cust)\n    min_val = np.argmin(corr_temp_cust)\n    opt_corr_min = corr_temp_cust[min_val]\n    opt_corr_max = corr_temp_cust[max_val]\n    \n    results = {max_val+1: opt_corr_max, min_val+1: opt_corr_min}\n    \n    return results\n","87176769":"# plot the overall total_cust values for entire timeseries\nplt.figure(figsize=[15,6])\nax = sb.lineplot(x='date_datetime', y='total_cust', data=bike_df)\n","f0978fbc":"# plot only last two years of timeseries\nplt.figure(figsize=[15,6])\nax = sb.lineplot(x='date_datetime', y='total_cust', data=bike_df[-718:])\n","bca3d358":"# plotting the partial autocorrelation for target\nplot_pacf(bike_df['total_cust'], title='Partical autocorrelation of total_cust',)\nplt.xlabel('Number of days in the past')\nplt.ylabel('Partial autocorrelation coefficient')\nplt.show()\n","a53d1d66":"# plotting the autocorrelation for target\nplot_acf(bike_df['total_cust'], title='Autocorrelation of total_cust',)\nplt.xlabel('Number of days in the past')\nplt.ylabel('Autocorrelation coefficient')\nplt.show()\n","4a8cc410":"# get the optimal window for rolling std for total_cust\nprint(best_window_std(bike_df['total_cust'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by total_cust\ncust_mean = bike_df['total_cust'].rolling(8).std()[7:-1]\npearsonr(cust_mean, bike_df['total_cust'][8:])","b6301812":"# get the optimal number for rolling mean for total_cust\nprint(best_window(bike_df['total_cust'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by total_cust\ncust_mean = bike_df['total_cust'].rolling(8).mean()[7:-1]\npearsonr(cust_mean, bike_df['total_cust'][8:])\n","125accd0":"# add the value from t-1\nbike_df['total_cust_t-1'] = bike_df['total_cust'].shift()\n","9758a229":"# create series that group the mean temperature per season\ntemp_spring = bike_df.groupby('season_spring')['temp_max'].mean().rename({1: 'Spring'})\ntemp_summer = bike_df.groupby('season_summer')['temp_max'].mean().rename({1: 'Summer'})\ntemp_fall = bike_df.groupby('season_fall')['temp_max'].mean().rename({1: 'Fall'})\n\n# add them to one series and drop the rows with index 0\ntemp_seasons = temp_spring.append(temp_summer).append(temp_fall)\ntemp_seasons.drop(labels=[0], inplace=True)\n","57a15ef6":"# plot average temp_max per season\nplt.figure(figsize=[10,6])\nsb.barplot(x=temp_seasons.index, y=temp_seasons.values);\n","57fef761":"# create series that groups average users per season\ncust_spring = bike_df.groupby('season_spring')['total_cust'].mean().rename({1: 'Spring'})\ncust_summer = bike_df.groupby('season_summer')['total_cust'].mean().rename({1: 'Summer'})\ncust_fall = bike_df.groupby('season_fall')['total_cust'].mean().rename({1: 'Fall'})\n\n# add them to one series and drop the rows with index 0\ncust_seasons = cust_spring.append(cust_summer).append(cust_fall)\ncust_seasons.drop(labels=[0], inplace=True)\n","85b6de9b":"# assign x and y1 and y2\nx = list(temp_seasons.index)\ny1 = cust_seasons.values\ny2 = temp_seasons.values\n\n# below code adapted from https:\/\/matplotlib.org\/gallery\/api\/two_scales.html\n# creat plot containing both average count of customers\n# and average temp per month\nplt.figure(figsize=[15,7])\nfig, ax1 = plt.subplots()\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor1 = 'tab:red'\nax1.set_xlabel('Seasons')\nax1.set_ylabel('Average number of customers', color=color1)\nax1.bar(x, y1, color=color1)\nax1.tick_params(axis='y', labelcolor=color1)\n\ncolor2 = 'tab:blue'\nax2.set_ylabel('Average maximum temperature', color=color2)\nax2.plot(x, y2, color=color2)\nax2.tick_params(axis='y', labelcolor=color2)\n\nfig.tight_layout()\nplt.show()\n","59ca75e6":"# plotting temp feature against the target label total_cust\nplt.figure(figsize=[7,5])\n\nsb.scatterplot(data = bike_df, x = 'temp_max', y = 'total_cust', color='green')\nplt.xlabel('Maximum temperature')\nplt.ylabel('Number of customers')\nplt.title('Maximum temperature vs. number of customers');\n\n","b06acf11":"# plot the partial autocorrelation of temp_max\nplot_pacf(bike_df['temp_max'], title='PACF: All samples in metro DC area',)\nplt.show()\n","23c6c28f":"# autocorrelation of temp_max\nplot_acf(bike_df['temp_max'], title='ACF: All samples in metro DC area',)\nplt.show()\n","233dcdf9":"# get the optimal window for rolling std for temperature\nprint(best_window_std(bike_df['temp_max'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by temp_max\ntemp_mean = bike_df['temp_max'].rolling(8).std()[7:-1]\npearsonr(temp_mean, bike_df['total_cust'][8:])","59e5ba77":"# get the optimal window for rolling mean for temperature\nbest_window(bike_df['temp_max'], bike_df['total_cust'], 30)\n","ba306790":"# plot the overall temp_max values for entire timeseries\nplt.figure(figsize=[15,6])\nax = sb.lineplot(x='date_datetime', y='temp_max', data=bike_df)\nplt.ylabel('Maximum temperature [\u00b0C]')\nplt.xlabel('Time');\n","66c5bcf8":"# create plot of rolling means\nplt.figure(figsize=[15,6])\n\nplt.plot(bike_df['temp_max'].rolling(16).mean());\n","dfce1255":"# create plot of rolling stds\nplt.figure(figsize=[15,6])\n\nplt.plot(bike_df['temp_max'].rolling(16).std());\n","7e72c905":"# plotting temp_min feature against the target label\nplt.figure(figsize=[7,5])\n\nsb.scatterplot(data = bike_df, x = 'temp_min', y = 'total_cust', color='red')\nplt.xlabel('Minimum temperature')\nplt.ylabel('Number of customers')\nplt.title('Minimum temperature vs. number of customers');\n\n","a311ade4":"# engineer new categorical temp features\nbike_df_cat = bike_df.copy()\nbike_df_cat['very_cold'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 0 else 0)\nbike_df_cat['cold'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 10 and x >= 0 else 0)\nbike_df_cat['cool'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 20 and x >= 10 else 0)\nbike_df_cat['warm'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 30 and x >= 20 else 0)\nbike_df_cat['hot'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x >= 30 else 0)\n\nbike_df_cat[['very_cold', 'cold', 'cool', 'warm', 'hot']] = bike_df_cat[['very_cold', 'cold', 'cool', 'warm', 'hot']].shift()\n\n\nbike_df_cat = bike_df_cat.iloc[1:,:]","b50e9cc3":"pearsonr(bike_df_cat['total_cust'], bike_df_cat['cold'])","5ff322e5":"# plot categorized temperature\nplt.figure(figsize=[15,10])\nplt.subplot(2,3,1)\nsb.boxplot(bike_df_cat['very_cold'], bike_df_cat['total_cust'])\nplt.xlabel('Very cold')\nplt.ylabel('Number of customers')\n\nplt.subplot(2,3,2)\nsb.boxplot(bike_df_cat['cold'], bike_df_cat['total_cust'])\nplt.xlabel('Cold')\nplt.ylabel('Number of customers')\n\nplt.subplot(2,3,3)\nsb.boxplot(bike_df_cat['cool'], bike_df_cat['total_cust'])\nplt.xlabel('Cool')\nplt.ylabel('Number of customers')\n\nplt.subplot(2,3,4)\nsb.boxplot(bike_df_cat['warm'], bike_df_cat['total_cust'])\nplt.xlabel('Warm')\nplt.ylabel('Number of customers')\n\nplt.subplot(2,3,5)\nsb.boxplot(bike_df_cat['hot'], bike_df_cat['total_cust'])\nplt.xlabel('Hot')\nplt.ylabel('Number of customers')\n","e0078844":"# plotting the distribution of precip\nplt.figure(figsize=[15,6])\n\nplt.subplot(1,2,1)\nplt.hist(bike_df['precip'], bins=15)\nplt.title('Distribution of precip')\n\nplt.subplot(1,2,2)\nsb.scatterplot(data = bike_df, x = 'precip', y = 'total_cust')\nplt.xlabel('Precipitation')\nplt.ylabel('Number of customers')\nplt.title('Precipitation vs. number of customers');\n","3f712eb9":"# plotting the distribution of precip\nplt.figure(figsize=[10,6])\n\nx = np.log10(bike_df['precip'] + 1)\nplt.hist(x)\nplt.title('Distribution of precip');\n","69f731a2":"# plot the overall precip values for entire timeseries\nplt.figure(figsize=[15,6])\nax = sb.lineplot(x='date_datetime', y='precip', data=bike_df)\n","d6cde01a":"# get the optimal number for rolling mean window\nprint(best_window(bike_df['precip'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by precip\nprecip_mean = bike_df['precip'].rolling(8).mean()[7:-1]\npearsonr(precip_mean, bike_df['total_cust'][8:])\n","682c3593":"# get the optimal window for rolling std for precip\nprint(best_window_std(bike_df['precip'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by precip\nprecip_mean = bike_df['precip'].rolling(8).std()[7:-1]\npearsonr(precip_mean, bike_df['total_cust'][8:])\n","e75e7fd9":"# plot the partial autocorrelation of precip\nplot_pacf(bike_df['precip'], title='PACF: All samples in metro DC area',)\nplt.show()\n","dc99e68c":"# create plot of rolling means\nplt.figure(figsize=[15,6])\n\nplt.plot(bike_df['precip'].rolling(16).mean());\n","0338c3e0":"# create plot of rolling stds\nplt.figure(figsize=[15,6])\n\nplt.plot(bike_df['precip'].rolling(16).std());\n","6fd9c2d2":"# plotting the distribution of wind\nplt.figure(figsize=[15,6])\n\nplt.subplot(1,2,1)\nplt.hist(bike_df['wind'], bins=15)\nplt.title('Distribution of wind')\n\nplt.subplot(1,2,2)\nsb.scatterplot(data = bike_df, x = 'wind', y = 'total_cust')\nplt.xlabel('Wind')\nplt.ylabel('Number of customers')\nplt.title('Wind vs. number of customers');\n\n","7e529f9c":"# plot the overall wind values for entire timeseries\nplt.figure(figsize=[15,6])\nax = sb.lineplot(x='date_datetime', y='wind', data=bike_df)\n","ec639a0d":"# get the optimal number for rolling mean window\nprint(best_window(bike_df['wind'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by wind\nwind_mean = bike_df['wind'].rolling(8).mean()[7:-1]\npearsonr(wind_mean, bike_df['total_cust'][8:])[0]\n","3b40f462":"# get the optimal window for rolling std for temperature\nprint(best_window_std(bike_df['wind'], bike_df['total_cust'], 30))\n\n# get the correlation for window size determined by wind\nprecip_mean = bike_df['wind'].rolling(8).std()[7:-1]\npearsonr(precip_mean, bike_df['total_cust'][8:])\n","31777c57":"# plot the partial autocorrelation of wind\nplot_pacf(bike_df['wind'], title='PACF: All samples in metro DC area',)\nplt.show()\n","6035d807":"# create plot of rolling means\nplt.figure(figsize=[15,6])\n\nplt.plot(bike_df['wind'].rolling(16).mean());\n","0be8d725":"# create plot of rolling means\nplt.figure(figsize=[15,6])\n\nplt.plot(bike_df['wind'].rolling(16).std());\n","03709d13":"# code based on implementation on https:\/\/www.analyticsvidhya.com\/blog\/2018\/09\/non-stationary-time-series-python\/\ndef adf_test(df, col_names):\n    '''\n    Function to perform Augmented Dickey-Fuller test on selected timeseries\n    Args: df = dataframe with timeseries to be tested\n          col_names = list of names of the timeseries to be tested\n    Returns: None\n    '''\n    for name in col_names:\n        print ('Results of Augmented Dickey-Fuller Test for {}'.format(name))\n        result_test = adfuller(df[name], autolag='AIC')\n        result_output = pd.Series(result_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n        for key, val in result_test[4].items():\n            result_output['Critical Value (%s)'%key] = val\n        print (result_output)\n","93b6503f":"# create the features that need to be tested\n# total_cust_t-1 was already added to the dataframe\n\ntesting_feat = ['wind', 'precip', 'total_cust', 'temp_min', 'temp_max', 'foggy', 'ice', 'thunder', 'sleet']\n\ntesting_df = pd.DataFrame()\n\nfor col in testing_feat:\n    col_mean = bike_df[col].rolling(16).mean()[15:-1]\n    col_std = bike_df[col].rolling(16).std()[15:-1]\n    testing_df[col+'_mean16'] = col_mean.values\n    testing_df[col+'_std16'] = col_std.values\n","e9abb771":"# adf test for total_cust_t-1\ntemp_cust_1 = bike_df['total_cust_t-1'].fillna(0)\nbike_df_temp = pd.DataFrame(temp_cust_1, columns=['total_cust_t-1'])\nadf_test(bike_df_temp, ['total_cust_t-1'])\n","05ec089a":"# adf test for total_cust\nadf_test(bike_df, ['total_cust'])\n","fcf5aab3":"# adf test for all engineered features\nadf_test(testing_df, testing_df.columns)\n","b7b333c8":"# code based on implementation on https:\/\/www.analyticsvidhya.com\/blog\/2018\/09\/non-stationary-time-series-python\/\ndef kpss_test(df, col_names):\n    '''\n    Function to perform KPSS test on selected timeseries\n    Args: df = dataframe with timeseries to be tested\n          col_names = list of names of the timeseries to be tested\n    Returns: None\n    '''\n    for name in col_names:\n        print ('Results of KPSS Test for {}'.format(name))\n        result_test = kpss(df[name], regression='c', lags='legacy')\n        result_output = pd.Series(result_test[0:3], index=['Test Statistic','p-value','Lags Used'])\n        for key, val in result_test[3].items():\n            result_output['Critical Value (%s)'%key] = val\n        print (result_output)\n\n# kpss test for total_cust\nkpss_test(testing_df, testing_df.columns)\n","b184bb95":"# kpss test for total_cust_t-1\nkpss_test(bike_df_temp, ['total_cust_t-1'])\n\n# kpss test for total_cust\nkpss_test(bike_df, ['total_cust'])\n","cbf9074a":"# Removing positive upward trend from total_cust_mean16, total_cust_std16 and total_cust_t-1\ntesting_df['total_cust_std16_log'] = [np.log1p(x+1) for x in testing_df['total_cust_std16']]\ntesting_df['total_cust_mean16_log'] = [np.log1p(x+1) for x in testing_df['total_cust_mean16']]\nbike_df_temp['total_cust_t-1_log'] = [np.log1p(x+1) for x in bike_df_temp['total_cust_t-1']]\n","252422ce":"# applying differencing to remove trend from total_cust\nbike_df_check = bike_df[['total_cust']]\nbike_df_check['total_cust_diff'] = bike_df_check['total_cust'] - bike_df_check['total_cust'].shift()\nbike_df_check = bike_df_check.iloc[1:,]","2f257824":"# kpss test for total_cust\nkpss_test(testing_df, ['total_cust_std16_log', 'total_cust_mean16_log'])\nkpss_test(bike_df_temp, ['total_cust_t-1_log'])\n\n# adf test for total_cust\nadf_test(testing_df, ['total_cust_std16_log', 'total_cust_mean16_log'])\nadf_test(bike_df_temp, ['total_cust_t-1_log'])\nadf_test(bike_df_check, ['total_cust_diff'])\n","9ecd1ee1":"# should keep this before the cleaning function has been created\nbike_df.drop(columns=['temp_observ', 'weekday'], axis=1, inplace=True)\n","495a4a41":"# drop the timestamp variable\nbike_df.drop(columns=['date_datetime'], inplace=True)\n","43c4fdc9":"# specify the window for rolling values\nwindow = 8\n","34293d66":"# engineer new categorical temp features\nbike_df_cat = bike_df[['temp_max']].copy()\nbike_df_cat['very_cold'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 0 else 0)\nbike_df_cat['cold'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 10 and x >= 0 else 0)\nbike_df_cat['cool'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 20 and x >= 10 else 0)\nbike_df_cat['warm'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 30 and x >= 20 else 0)\nbike_df_cat['hot'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x >= 30 else 0)\n\nbike_df_cat[['very_cold', 'cold', 'cool', 'warm', 'hot']] = bike_df_cat[['very_cold', 'cold', 'cool', 'warm', 'hot']].shift()\n\nbike_df_cat.drop(columns=['temp_max'], inplace=True)\n","2e0650c8":"# creating rolling values\nnew_feat = ['wind', 'precip', 'total_cust', 'temp_max', 'temp_min', 'foggy', 'ice', 'thunder', 'sleet']\n\ntemp_df = pd.DataFrame()\n\nfor col in new_feat:\n    col_mean = bike_df[col].rolling(window).mean()[(window-1):-1]\n    col_std = bike_df[col].rolling(window).std()[(window-1):-1]\n    temp_df[col+'_mean'+str(window)] = col_mean.values\n    temp_df[col+'_std'+str(window)] = col_std.values\n","e1d94d13":"# remember to remove the first row from 16 rows from total_cust (target label)\nnew_bike_df = bike_df.iloc[window:,:]\nbike_df_cat = bike_df_cat.iloc[window:,:]\nnew_bike_df.reset_index(drop=True, inplace=True)\nbike_df_cat.reset_index(drop=True, inplace=True)\nnew_bike_df.head()\n","36e34c23":"# drop all columns which we cannot use because of lookahead bias\nnew_bike_df.drop(columns=['temp_max', 'wind', 'temp_min', 'precip', \n                          'thunder', 'foggy', 'ice', 'sleet'], inplace=True)\n","a118dc35":"# merging both dataframes with features\nfinal_bike_df = new_bike_df.join(temp_df, how='left')\nfinal_bike_df = final_bike_df.join(bike_df_cat, how='left')\nfinal_bike_df.head()\n","fc3d7483":"# assigning X and y\ny = final_bike_df['total_cust']\nX = final_bike_df.drop(columns=['total_cust'])\n","999e0b6f":"# creating a class that I can use in the ML pipeline that prints out the transformed data that will enter the model\nclass Debug(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        self.shape = X.shape\n        print(self.shape)\n        X_df = pd.DataFrame(X)\n        print(X_df)\n        # print(X_df.to_string()) # can only be print like this without running LagVars() to avoid crashing\n        # what other output you want\n        return X\n    ","0e4503d7":"# assigning X and y for the univariate naive prediction\ny_naive = (final_bike_df['total_cust'].copy())\nX_naive = y_naive.copy()\nX_naive = pd.DataFrame(data=X_naive, columns=['total_cust'])\n\n# getting the start time\nstart_time = datetime.now()\n\n# creating y_pred\ny_pred = (X_naive.shift())[1:]\n# adjusting length of the actual target values\ny_naive = y_naive[1:]\n\n# get final time\nend_time = datetime.now()\nprint('Total running time of naive predictor:', (end_time - start_time).total_seconds())\n\n# calculating the scores for the last value method\nprint('RMSE:', np.sqrt(mean_squared_error(y_naive, y_pred)))\nprint('RMSLE:', np.sqrt(mean_squared_log_error(y_naive, y_pred)))\nprint('MAE:', mean_absolute_error(y_naive, y_pred))\n","a574f166":"y_log = final_bike_df[['total_cust']].copy()\ny_log['total_cust'] = y_log['total_cust'].apply(lambda x: np.log1p(x+1))\ny_shift = y_log.shift(1)\ny_diff = (y_log - y_shift)[1:]\nX_diff = X[1:]\n\ny_diff.reset_index(drop=True, inplace=True)\nX_diff.reset_index(drop=True, inplace=True)\n","3b25c1ed":"# initializing the model which is a Random Forest model and uses default hyperparameters\nmodel_rf_diff = RandomForestRegressor(random_state=42)\n","84e71839":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_rf_diff)\n])\n\npipeline_rf_diff = pipeline.fit(X_diff, y_diff)\n","7f42d9a6":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# doing cross validation on the chunks of data and calculating scores\nscores_rf_diff = cross_validate(pipeline_rf_diff, X_diff, y_diff, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n                         return_train_score=True, n_jobs=-1)\n","591085dd":"# root mean squared error\nprint('Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf_diff['train_neg_mean_squared_error']])\/len(scores_rf_diff['train_neg_mean_squared_error']))\nprint('Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf_diff['test_neg_mean_squared_error']])\/len(scores_rf_diff['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('Average MAE train data:', \n      sum([(-1 * x) for x in scores_rf_diff['train_neg_mean_absolute_error']])\/len(scores_rf_diff['train_neg_mean_absolute_error']))\nprint('Average MAE test data:', \n      sum([(-1 * x) for x in scores_rf_diff['test_neg_mean_absolute_error']])\/len(scores_rf_diff['test_neg_mean_absolute_error']))\n","3e22b186":"# getting indices for all folds from timeseriessplit\nX_train_over = defaultdict()  \nX_test_over = defaultdict()\ny_train_over = defaultdict()\ny_test_over = defaultdict()\n\ny_test_index = []\ny_train_index = []\n\ncount = 0\n\nfor train_index, test_index in time_split.split(X_diff):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    \n    y_test_index.append(test_index)\n    y_train_index.append(train_index)\n    \n    X_train, X_test = X_diff.iloc[train_index], X_diff.iloc[test_index]\n    y_train, y_test = (np.array(y_diff))[train_index], (np.array(y_diff))[test_index]\n    X_train_over[count] = X_train\n    X_test_over[count] = X_test\n    y_train_over[count] = y_train\n    y_test_over[count] = y_test\n    \n    count += 1","4ec66a58":"def split_predict_test(X_train, y_train, X_test, y_test, split):\n    \n    pipeline_part = pipeline.fit(X_train[split], y_train[split].ravel())\n    prediction_test = pipeline_part.predict(X_test[split])\n    #print('transformed RMSE:', np.sqrt(mean_squared_error(y_test[split], prediction_test)))\n        \n    # what are the predictions in absolute terms\n    y_test_pred_log = prediction_test + y_log.iloc[y_test_index[split]]['total_cust']\n    y_test_pred = np.exp(y_test_pred_log) - 2\n\n    # what are the actual values in absolute terms\n    y_test_true_log = y_test[split] + y_log.iloc[y_test_index[split]]\n    y_test_true = np.exp(y_test_true_log) - 2 # minus 2 because we added 1 each when doing the log function earlier\n\n    # what is the rmse, rmsle and mae\n    #print('actual RMSE:', np.sqrt(mean_squared_error(y_test_true, y_test_pred)))\n    #print('actual RMSLE:', np.sqrt(mean_squared_log_error(y_test_true, y_test_pred)))\n    #print('actual MAE:', mean_absolute_error(y_test_true, y_test_pred))\n    \n    rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n    rmsle = np.sqrt(mean_squared_log_error(y_test_true, y_test_pred))\n    mae = mean_absolute_error(y_test_true, y_test_pred)\n    \n    list_scores = []\n    list_scores.extend([rmse, rmsle, mae])\n    \n    return list_scores\n","9427347d":"def split_predict_train(X_train, y_train, X_test, y_test, split):\n    \n    pipeline_part = pipeline.fit(X_train[split], y_train[split].ravel())\n    prediction_train = pipeline_part.predict(X_train[split])\n    #print('transformed RMSE:', np.sqrt(mean_squared_error(y_test[split], prediction_test)))\n        \n    # what are the predictions in absolute terms\n    y_train_pred_log = prediction_train + y_log.iloc[y_train_index[split]]['total_cust']\n    y_train_pred = np.exp(y_train_pred_log) - 2\n\n    # what are the actual values in absolute terms\n    y_train_true_log = y_train[split] + y_log.iloc[y_train_index[split]]\n    y_train_true = np.exp(y_train_true_log) - 2 # minus 2 because we added 1 each when doing the log function earlier\n\n    # what is the rmse, rmsle and mae\n    #print('actual RMSE:', np.sqrt(mean_squared_error(y_test_true, y_test_pred)))\n    #print('actual RMSLE:', np.sqrt(mean_squared_log_error(y_test_true, y_test_pred)))\n    #print('actual MAE:', mean_absolute_error(y_test_true, y_test_pred))\n    \n    rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n    rmsle = np.sqrt(mean_squared_log_error(y_train_true, y_train_pred))\n    mae = mean_absolute_error(y_train_true, y_train_pred)\n    \n    list_scores = []\n    list_scores.extend([rmse, rmsle, mae])\n    \n    return list_scores","27105430":"# initializing the model which is a Random Forest model and uses default hyperparameters\nmodel_rf = RandomForestRegressor(bootstrap=False, random_state=15)\n","768b539a":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_rf)\n])\n\npipeline_rf = pipeline.fit(X, y)\n","98ceb24a":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# doing cross validation on the chunks of data and calculating scores\nscores_rf = cross_validate(pipeline_rf, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","b906476e":"# root mean squared error\nprint('Random Forests: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf['train_neg_mean_squared_error']])\/len(scores_rf['train_neg_mean_squared_error']))\nprint('Random Forests: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf['test_neg_mean_squared_error']])\/len(scores_rf['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('Random Forests: Average MAE train data:', \n      sum([(-1 * x) for x in scores_rf['train_neg_mean_absolute_error']])\/len(scores_rf['train_neg_mean_absolute_error']))\nprint('Random Forests: Average MAE test data:', \n      sum([(-1 * x) for x in scores_rf['test_neg_mean_absolute_error']])\/len(scores_rf['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('Random Forests: Average RMSLE train data:', \n      sum([(-1 * x) for x in scores_rf['train_neg_mean_squared_log_error']])\/len(scores_rf['train_neg_mean_squared_log_error']))\nprint('Random Forests: Average RMSLE test data:', \n      sum([(-1 * x) for x in scores_rf['test_neg_mean_squared_log_error']])\/len(scores_rf['test_neg_mean_squared_log_error']))\n","f665dd5a":"# number of trees in random forest\nn_estimators = randint(1, 2000)\n# maximum number of features included in the model\nmax_features = randint(1, 20)\n# maximum number of levels in tree\nmax_depth = randint(1,10)\n# minimum number of samples required to split a node\nmin_samples_leaf = randint(1, 10)\n\n# create the random grid\nrandom_grid_rf = {'model__n_estimators': n_estimators,\n                   'model__max_depth': max_depth,\n                   'model__min_samples_leaf': min_samples_leaf,\n                   'model__max_features': max_features}\n","627234bd":"# check the start time\nstart_time = datetime.now()\nprint(start_time)\n\n# instantiate and fit the randomizedsearch class to the random parameters\nrs_rf = RandomizedSearchCV(pipeline_rf, \n                        param_distributions=random_grid_rf, \n                        scoring='neg_mean_squared_error', \n                        n_jobs=-1,\n                        cv=time_split,\n                        n_iter = 5,\n                        verbose=10,\n                        random_state=49)\nrs_rf = rs_rf.fit(X, y)\n\n# print the total running time\nend_time = datetime.now()\nprint('Total running time:', end_time-start_time)\n","7089e432":"# Saving the best RandomForest model\npickle.dump(rs_rf.best_estimator_, open('randomforest.sav', 'wb'))\n\n# change the keys of the best_params dictionary to allow it to be used for the final model\nfix_best_params_rf = {key[7:]: val for key, val in rs_rf.best_params_.items()}\n\nprint(fix_best_params_rf)\n\n# fit the randomforestregressor with the best_params as hyperparameters\nmodel_rf_rs = RandomForestRegressor(**fix_best_params_rf, bootstrap=False)\n","f97a5f95":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_rf_rs)\n])\n\n# fitting x and y to pipeline\npipeline_rf_rs = pipeline.fit(X, y)\n","93c403dc":"# doing cross validation on the chunks of data and calculating scores\nscores_rf = cross_validate(pipeline_rf_rs, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","b7e3e6b9":"# root mean squared error\nprint('Random Forests: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf['train_neg_mean_squared_error']])\/len(scores_rf['train_neg_mean_squared_error']))\nprint('Random Forests: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf['test_neg_mean_squared_error']])\/len(scores_rf['test_neg_mean_squared_error']))\n\n# absolute mean error\nprint('Random Forests: Average MAE train data:', \n      sum([(-1 * x) for x in scores_rf['train_neg_mean_absolute_error']])\/len(scores_rf['train_neg_mean_absolute_error']))\nprint('Random Forests: Average MAE test data:', \n      sum([(-1 * x) for x in scores_rf['test_neg_mean_absolute_error']])\/len(scores_rf['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('Random Forests: Average RMSLE train data:', \n      sum([(-1 * x) for x in scores_rf['train_neg_mean_squared_log_error']])\/len(scores_rf['train_neg_mean_squared_log_error']))\nprint('Random Forests: Average RMSLE test data:', \n      sum([(-1 * x) for x in scores_rf['test_neg_mean_squared_log_error']])\/len(scores_rf['test_neg_mean_squared_log_error']))\n","8bcc9ff2":"# assign lists of parameters to be used in gridsearch\nparam_grid_rf = {'model__max_depth': [8, 9],\n                 'model__max_features': [18, 19],\n                 'model__min_samples_leaf': [5, 6],\n                 'model__n_estimators': [1926, 1930]\n}\n","299cf109":"# use gridsearch to search around the randomizedsearch best parameters and further improve the model\ngs_rf = GridSearchCV(pipeline_rf, \n                  param_grid=param_grid_rf, \n                  scoring='neg_mean_squared_error', \n                  verbose = 10,\n                  n_jobs=-1, \n                  cv=time_split)\ngs_rf = gs_rf.fit(X, y)\n","fbcca9ec":"# Saving the best RandomForest model\npickle.dump(gs_rf.best_estimator_, open('randomforest.sav', 'wb'))\n\n# change the keys of the best_params dictionary to allow it to be used for the final model\nfix_best_params_rf = {key[7:]: val for key, val in gs_rf.best_params_.items()}\n\nprint(fix_best_params_rf)\n\n# fit the randomforestregressor with the best_params as hyperparameters\nmodel_rf_tuned = RandomForestRegressor(**fix_best_params_rf, bootstrap=False)\n","1747d0e9":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_rf_tuned)\n])\n\n# fitting x and y to pipeline\npipeline_rf_tuned = pipeline.fit(X, y)\n","3277b792":"# check the start time\nstart_time = datetime.now()\n\n# doing cross validation on the chunks of data and calculating scores\nscores_rf_tuned = cross_validate(pipeline_rf_tuned, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n\n# print the total running time\nend_time = datetime.now()\nprint('Total cross validation time for random forests:', (end_time-start_time).total_seconds())\n","1417d7ea":"# root mean squared error\nprint('Random Forests: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_error']])\/len(scores_rf_tuned['train_neg_mean_squared_error']))\nprint('Random Forests: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_error']])\/len(scores_rf_tuned['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('Random Forests: Average MAE train data:', \n      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_absolute_error']])\/len(scores_rf_tuned['train_neg_mean_absolute_error']))\nprint('Random Forests: Average MAE test data:', \n      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_absolute_error']])\/len(scores_rf_tuned['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('Random Forests: Average RMSLE train data:', \n      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_log_error']])\/len(scores_rf_tuned['train_neg_mean_squared_log_error']))\nprint('Random Forests: Average RMSLE test data:', \n      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_log_error']])\/len(scores_rf_tuned['test_neg_mean_squared_log_error']))\n\n","3bdd7e71":"# plot feature importance\nplt.figure(figsize=[12,9])\nimportances = list(pipeline_rf_tuned.steps[1][1].feature_importances_)\n\nimp_dict = {key: val for key, val in zip(list(X.columns), importances)}\nsorted_feats = sorted(imp_dict.items(), key=lambda x:x[1], reverse=True)\nx_val = [x[0] for x in sorted_feats]\ny_val = [x[1] for x in sorted_feats]\n\nsb.barplot(y_val, x_val)\n","ea8617f3":"# get the final scores for the differenced data\nsplit = 10\nall_scores_test = []\nall_scores_train = []\n\nfor i in range(split):\n    scores_test = split_predict_test(X_train_over, y_train_over, X_test_over, y_test_over, i)\n    all_scores_test.append(scores_test)\n    scores_train = split_predict_train(X_train_over, y_train_over, X_test_over, y_test_over, i)\n    all_scores_train.append(scores_train)\n    \nrmse_test = []\nrmsle_test = []\nmae_test = []\nrmse_train = []\nrmsle_train = []\nmae_train = []\n\nfor vals in all_scores_test:\n    rmse_test.append(vals[0])\n    rmsle_test.append(vals[1])\n    mae_test.append(vals[2])\n    \nfor vals in all_scores_train:\n    rmse_train.append(vals[0])\n    rmsle_train.append(vals[1])\n    mae_train.append(vals[2])\n    \nprint('Overall Test RMSE:', sum(rmse_test)\/split)\nprint('Overall Test RMSLE:', sum(rmsle_test)\/split)\nprint('Overall Test MAE:', sum(mae_test)\/split)\n\nprint('Overall Train RMSE:', sum(rmse_train)\/split)\nprint('Overall Train RMSLE:', sum(rmsle_train)\/split)\nprint('Overall Train MAE:', sum(mae_train)\/split)\n    ","994e845f":"# how many features are in the dataset currently:\nlen(x_val)\nweekday_feats = ['weekday_3', 'weekday_1', 'weekday_2', 'weekday_4', 'weekday_5', 'weekday_6']","c87fd235":"# check how the model performance without certain features that are very unimportant\nX_feat_imp = X.drop(columns=weekday_feats)\n","58788973":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window)]#, 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_rf_tuned)\n])\n\n# fitting x and y to pipeline\npipeline_rf_tuned = pipeline.fit(X_feat_imp, y)\n","9a8c3dbd":"# doing cross validation on the chunks of data and calculating scores\nscores_rf_tuned = cross_validate(pipeline_rf_tuned, X_feat_imp, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","3a987b3f":"# root mean squared error\nprint('Random Forests: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_error']])\/len(scores_rf_tuned['train_neg_mean_squared_error']))\nprint('Random Forests: Average RMSLE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_error']])\/len(scores_rf_tuned['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('Random Forests: Average MAE train data:', \n      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_absolute_error']])\/len(scores_rf_tuned['train_neg_mean_absolute_error']))\nprint('Random Forests: Average MAE test data:', \n      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_absolute_error']])\/len(scores_rf_tuned['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('Random Forests: Average RMSLE train data:', \n      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_log_error']])\/len(scores_rf_tuned['train_neg_mean_squared_log_error']))\nprint('Random Forests: Average RMSLE test data:', \n      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_log_error']])\/len(scores_rf_tuned['test_neg_mean_squared_log_error']))\n\n","dc48dd63":"# initializing AdaBoost model with default hyperparameters\nmodel_ada = AdaBoostRegressor(random_state=42, base_estimator=DecisionTreeRegressor())\n\n# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_ada)\n])\n\npipeline_ada = pipeline.fit(X, y)\n","c7114a3d":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# doing cross validation on the chunks of data and calculating scores\nscores_ada = cross_validate(pipeline_ada, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","f89216b1":"# root mean squared error\nprint('AdaBoost: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada['train_neg_mean_squared_error']])\/len(scores_ada['train_neg_mean_squared_error']))\nprint('AdaBoost: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada['test_neg_mean_squared_error']])\/len(scores_ada['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('AdaBoost: Average MAE train data:', \n      sum([(-1 * x) for x in scores_ada['train_neg_mean_absolute_error']])\/len(scores_ada['train_neg_mean_absolute_error']))\nprint('AdaBoost: Average MAE test data:', \n      sum([(-1 * x) for x in scores_ada['test_neg_mean_absolute_error']])\/len(scores_ada['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('AdaBoost: Average RMSLE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada['train_neg_mean_squared_log_error']])\/len(scores_ada['train_neg_mean_squared_log_error']))\nprint('AdaBoost: Average RMSLE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada['test_neg_mean_squared_log_error']])\/len(scores_ada['test_neg_mean_squared_log_error']))\n","329e1046":"# number of estimators in AdaBoost model\nn_estimators = randint(100, 1000)\n# learning rate\nlearning_rate = uniform(0.001, 0.05)\n# max_depth\nmax_depth = randint(1, 8)\n\n# Create the random grid\nrandom_grid_ada = {'model__n_estimators': n_estimators,\n                   'model__learning_rate': learning_rate,\n                   'model__base_estimator__max_depth': max_depth}\n","eb4841ef":"# check the start time\nstart_time = datetime.now()\nprint(start_time)\n\n# instantiate and fit the randomizedsearch class to the random parameters\nrs_ada = RandomizedSearchCV(pipeline_ada, \n                        param_distributions=random_grid_ada, \n                        scoring='neg_mean_squared_error', \n                        n_jobs=-1,\n                        cv=time_split,\n                        n_iter = 5,\n                        verbose=10,\n                        random_state=40)\nrs_ada = rs_ada.fit(X, y)\n\n# print the total running time\nend_time = datetime.now()\nprint('Total running time:', end_time-start_time)\n    ","d97ffbac":"# Saving the best RandomForest model\npickle.dump(rs_ada.best_estimator_, open('adaboost.sav', 'wb'))\n\n# change the keys of the best_params dictionary to allow it to be used for the final model\nfix_best_params_ada = {key[7:]: val for key, val in rs_ada.best_params_.items()}\n\nprint(fix_best_params_ada)\nmax_depth = fix_best_params_ada['base_estimator__max_depth']\n\n# fit the randomforestregressor with the best_params as hyperparameters\nmodel_ada_tuned = AdaBoostRegressor(DecisionTreeRegressor(max_depth=max_depth),\n                                    learning_rate=fix_best_params_ada['learning_rate'], \n                                    n_estimators=fix_best_params_ada['n_estimators'])\n","b0bb75d0":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_ada_tuned)\n])\n\npipeline_ada_tuned = pipeline.fit(X, y)\n","4ed42947":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# doing cross validation on the chunks of data and calculating scores\nscores_ada_tuned = cross_validate(pipeline_ada_tuned, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","5fdd0123":"# root mean squared error\nprint('AdaBoost: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_error']])\/len(scores_ada_tuned['train_neg_mean_squared_error']))\nprint('AdaBoost: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_error']])\/len(scores_ada_tuned['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('AdaBoost: Average MAE train data:', \n      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_absolute_error']])\/len(scores_ada_tuned['train_neg_mean_absolute_error']))\nprint('AdaBoost: Average MAE test data:', \n      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_absolute_error']])\/len(scores_ada_tuned['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('AdaBoost: Average RMSLE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_log_error']])\/len(scores_ada_tuned['train_neg_mean_squared_log_error']))\nprint('AdaBoost: Average RMSLEtest data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_log_error']])\/len(scores_ada_tuned['test_neg_mean_squared_log_error']))\n","c3e4e870":"# specify some values for hyperparameters around the values chosen by randomizedsearch\ngrid_param_ada = {'model__learning_rate': [0.003, 0.004, 0.0332],\n                  'model__n_estimators': [100, 265],\n                  'model__base_estimator__max_depth': [5, 6, 7]}\n\n\n# use gridsearch to search around the randomizedsearch best parameters and further improve the model\ngs_ada = GridSearchCV(pipeline_ada, \n                  param_grid=grid_param_ada, \n                  scoring='neg_mean_squared_error',\n                  verbose = 10,\n                  n_jobs=-1, \n                  cv=time_split)\n\ngs_ada = gs_ada.fit(X, y)\n","73ee9814":"# Saving the best RandomForest model\n#pickle.dump(gs_ada.best_estimator_, open('adaboost.sav', 'wb'))\n\n# change the keys of the best_params dictionary to allow it to be used for the final model\nfix_best_params_ada = {key[7:]: val for key, val in gs_ada.best_params_.items()}\n\nprint(fix_best_params_ada)\n\nmax_depth = fix_best_params_ada['base_estimator__max_depth']\n\n# fit the randomforestregressor with the best_params as hyperparameters\nmodel_ada_tuned = AdaBoostRegressor(DecisionTreeRegressor(max_depth=max_depth),\n                                    learning_rate=fix_best_params_ada['learning_rate'], \n                                    n_estimators=fix_best_params_ada['n_estimators'])\n","dff71f18":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_ada_tuned)\n])\n\npipeline_ada_tuned = pipeline.fit(X, y)\n","a01da4eb":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# check the start time\nstart_time = datetime.now()\n\n# doing cross validation on the chunks of data and calculating scores\nscores_ada_tuned = cross_validate(pipeline_ada_tuned, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n\n# print the total running time\nend_time = datetime.now()\nprint('Total cross validation time for AdaBoost:', (end_time-start_time).total_seconds())\n","6a3ed7c8":"# root mean squared error\nprint('AdaBoost: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_error']])\/len(scores_ada_tuned['train_neg_mean_squared_error']))\nprint('AdaBoost: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_error']])\/len(scores_ada_tuned['test_neg_mean_squared_error']))\n\n# mean average error\nprint('AdaBoost: Average MAE train data:', \n      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_absolute_error']])\/len(scores_ada_tuned['train_neg_mean_absolute_error']))\nprint('AdaBoost: Average MAE test data:', \n      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_absolute_error']])\/len(scores_ada_tuned['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('AdaBoost: Average RMSLE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_log_error']])\/len(scores_ada_tuned['train_neg_mean_squared_log_error']))\nprint('AdaBoost: Average RMSLE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_log_error']])\/len(scores_ada_tuned['test_neg_mean_squared_log_error']))\n\n","e1a920e9":"# plot feature importance\nplt.figure(figsize=[12,9])\nimportances = list(pipeline_ada_tuned.steps[1][1].feature_importances_)\n\nimp_dict = {key: val for key, val in zip(list(X.columns), importances)}\nsorted_feats = sorted(imp_dict.items(), key=lambda x:x[1], reverse=True)\nx_val = [x[0] for x in sorted_feats]\ny_val = [x[1] for x in sorted_feats]\n\n#importances.sort(reverse=True)\nsb.barplot(y_val, x_val)\n#importances","cdb7e426":"# get the final scores for the differenced data\nsplit = 10\nall_scores_test = []\nall_scores_train = []\n\nfor i in range(split):\n    scores_test = split_predict_test(X_train_over, y_train_over, X_test_over, y_test_over, i)\n    all_scores_test.append(scores_test)\n    scores_train = split_predict_train(X_train_over, y_train_over, X_test_over, y_test_over, i)\n    all_scores_train.append(scores_train)\n    \nrmse_test = []\nrmsle_test = []\nmae_test = []\nrmse_train = []\nrmsle_train = []\nmae_train = []\n\nfor vals in all_scores_test:\n    rmse_test.append(vals[0])\n    rmsle_test.append(vals[1])\n    mae_test.append(vals[2])\n    \nfor vals in all_scores_train:\n    rmse_train.append(vals[0])\n    rmsle_train.append(vals[1])\n    mae_train.append(vals[2])\n    \nprint('Overall Test RMSE:', sum(rmse_test)\/split)\nprint('Overall Test RMSLE:', sum(rmsle_test)\/split)\nprint('Overall Test MAE:', sum(mae_test)\/split)\n\nprint('Overall Train RMSE:', sum(rmse_train)\/split)\nprint('Overall Train RMSLE:', sum(rmsle_train)\/split)\nprint('Overall Train MAE:', sum(mae_train)\/split)\n    ","1c81e137":"# check how the model performance without certain features that are very unimportant\nX_feat_imp = X.drop(columns=['weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6'])\n","f946b634":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window)]#, 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_ada_tuned)\n])\n\npipeline_ada_tuned = pipeline.fit(X_feat_imp, y)\n","2371d9f7":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# doing cross validation on the chunks of data and calculating scores\nscores_ada_tuned = cross_validate(pipeline_ada_tuned, X_feat_imp, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","44ddd53b":"# root mean squared error\nprint('AdaBoost: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_error']])\/len(scores_ada_tuned['train_neg_mean_squared_error']))\nprint('AdaBoost: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_error']])\/len(scores_ada_tuned['test_neg_mean_squared_error']))\n\n# mean average error\nprint('AdaBoost: Average MAE train data:', \n      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_absolute_error']])\/len(scores_ada_tuned['train_neg_mean_absolute_error']))\nprint('AdaBoost: Average MAE test data:', \n      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_absolute_error']])\/len(scores_ada_tuned['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('AdaBoost: Average RMSLE train data:', \n      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_log_error']])\/len(scores_ada_tuned['train_neg_mean_squared_log_error']))\nprint('AdaBoost: Average RMSLE test data:', \n      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_log_error']])\/len(scores_ada_tuned['test_neg_mean_squared_log_error']))\n\n","ce4f51e2":"# initializing XGBoost model with default hyperparameters\nmodel_xgb = xgb.XGBRegressor(random_state=42)\n\n# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('scaler', MinMaxScaler()),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_xgb)\n])\n\npipeline_xgb = pipeline.fit(X, y)\n","c4c11da3":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# doing cross validation on the chunks of data and calculating scores\nscores_xgb = cross_validate(pipeline_xgb, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","b7a428c6":"# root mean squared error\nprint('XGBoost: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb['train_neg_mean_squared_error']])\/len(scores_xgb['train_neg_mean_squared_error']))\nprint('XGBoost: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb['test_neg_mean_squared_error']])\/len(scores_xgb['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('XGBoost: Average MAE train data:', \n      sum([(-1 * x) for x in scores_xgb['train_neg_mean_absolute_error']])\/len(scores_xgb['train_neg_mean_absolute_error']))\nprint('XGBoost: Average MAE test data:', \n      sum([(-1 * x) for x in scores_xgb['test_neg_mean_absolute_error']])\/len(scores_xgb['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('XGBoost: Average RMSLE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb['train_neg_mean_squared_log_error']])\/len(scores_xgb['train_neg_mean_squared_log_error']))\nprint('XGBoost: Average RMSLE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb['test_neg_mean_squared_log_error']])\/len(scores_xgb['test_neg_mean_squared_log_error']))\n","04b9de73":"# alpha\nalpha = uniform(0.2, 0.5)\n# learning rate\nlearning_rate = uniform(0, 1)\n# colsample_bytree\ncolsample_bytree = uniform(0, 1)\n# max depth\nn_estimators = randint(300, 1000)\n# min child weight\nmin_child_weight = randint(5, 10)\n# max_depth\nmax_depth = randint(1, 5)\n# subsample\n# subsample = uniform(0, 1)\n\n# Create the random grid\nrandom_grid_xgb = {'model__n_estimators': n_estimators,\n                   'model__learning_rate': learning_rate,\n                   'model__reg_alpha': alpha,\n                   'model__colsample_bytree': colsample_bytree,\n                   'model__min_child_weight': min_child_weight,\n                   'model__max_depth': max_depth}\n                   #'model__subsample': subsample}\n","8989f54f":"# check the start time\nstart_time = datetime.now()\nprint(start_time)\n\n# instantiate and fit the randomizedsearch class to the random parameters\nrs_xgb = RandomizedSearchCV(pipeline_xgb, \n                        param_distributions=random_grid_xgb, \n                        scoring='neg_mean_squared_error', \n                        n_jobs=-1,\n                        cv=time_split,\n                        n_iter = 10,\n                        verbose=10,\n                        random_state=10)\nrs_xgb = rs_xgb.fit(X, y)\n\n# print the total running time\nend_time = datetime.now()\nprint('Total running time:', end_time-start_time)\n    ","4fc089a5":"# Saving the best RandomForest model\npickle.dump(rs_xgb.best_estimator_, open('xgboost.sav', 'wb'))\n\n# change the keys of the best_params dictionary to allow it to be used for the final model\nfix_best_params_xgb = {key[7:]: val for key, val in rs_xgb.best_params_.items()}\n\nprint(fix_best_params_xgb)\n\n# fit the randomforestregressor with the best_params as hyperparameters\nmodel_xgb_rs = xgb.XGBRegressor(**fix_best_params_xgb)\n","fcecd58b":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('scaler', MinMaxScaler()),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_xgb_rs)\n])\n\npipeline_xgb_rs = pipeline.fit(X, y)\n","e9e23b27":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# doing cross validation on the chunks of data and calculating scores\nscores_xgb_rs = cross_validate(pipeline_xgb_rs, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","39c502cf":"# root mean squared error\nprint('XGBoost: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_rs['train_neg_mean_squared_error']])\/len(scores_xgb_rs['train_neg_mean_squared_error']))\nprint('XGBoost: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_rs['test_neg_mean_squared_error']])\/len(scores_xgb_rs['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('XGBoost: Average MAE train data:', \n      sum([(-1 * x) for x in scores_xgb_rs['train_neg_mean_absolute_error']])\/len(scores_xgb_rs['train_neg_mean_absolute_error']))\nprint('XGBoost: Average MAE test data:', \n      sum([(-1 * x) for x in scores_xgb_rs['test_neg_mean_absolute_error']])\/len(scores_xgb_rs['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('XGBoost: Average RMSLE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_rs['train_neg_mean_squared_log_error']])\/len(scores_xgb_rs['train_neg_mean_squared_log_error']))\nprint('XGBoost: Average RMSLE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_rs['test_neg_mean_squared_log_error']])\/len(scores_xgb_rs['test_neg_mean_squared_log_error']))\n\n","5aed901e":"# specify some values for hyperparameters around the values chosen by randomizedsearch\ngrid_param_xgb = {'model__n_estimators': [664, 650],\n                   'model__learning_rate': [0.35, 0.04],\n                   'model__alpha': [0.636, 0.6],\n                   'model__colsample_bytree': [0.85, 0.9],\n                   'model__min_child_weight': [7, 8],\n                   'model__max_depth': [1, 2]}\n                   #'model__subsample': [0.98, 0.90]}\n\n# use gridsearch to search around the randomizedsearch best parameters and further improve the model\ngs_xgb = GridSearchCV(pipeline_xgb, \n                  param_grid=grid_param_xgb, \n                  scoring='neg_mean_squared_error', \n                  verbose = 10,\n                  n_jobs=-1, \n                  cv=time_split)\n\ngs_xgb = gs_xgb.fit(X, y)\n","694d3f0a":"# Saving the best XGB model\npickle.dump(gs_xgb.best_estimator_, open('xgboost.sav', 'wb'))\n\n# change the keys of the best_params dictionary to allow it to be used for the final model\nfix_best_params_xgb = {key[7:]: val for key, val in gs_xgb.best_params_.items()}\n\nprint(fix_best_params_xgb)\n\n# fit the randomforestregressor with the best_params as hyperparameters\nmodel_xgb_tuned = xgb.XGBRegressor(**fix_best_params_xgb)\n","9a249041":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('scaler', MinMaxScaler()),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_xgb_tuned)\n])\n\npipeline_xgb_tuned = pipeline.fit(X, y)\n","738c1241":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# check the start time\nstart_time = datetime.now()\n\n# doing cross validation on the chunks of data and calculating scores\nscores_xgb_tuned = cross_validate(pipeline_xgb_tuned, X, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n\n# print the total running time\nend_time = datetime.now()\nprint('Total cross validation time for XGBBoost:', (end_time-start_time).total_seconds())\n","890e8ca4":"# root mean squared error\nprint('XGBoost: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['train_neg_mean_squared_error']])\/len(scores_xgb_tuned['train_neg_mean_squared_error']))\nprint('XGBoost: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['test_neg_mean_squared_error']])\/len(scores_xgb_tuned['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('XGBoost: Average MAE train data:', \n      sum([(-1 * x) for x in scores_xgb_tuned['train_neg_mean_absolute_error']])\/len(scores_xgb_tuned['train_neg_mean_absolute_error']))\nprint('XGBoost: Average MAE test data:', \n      sum([(-1 * x) for x in scores_xgb_tuned['test_neg_mean_absolute_error']])\/len(scores_xgb_tuned['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('XGBoost: Average RMSLE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['train_neg_mean_squared_log_error']])\/len(scores_xgb_tuned['train_neg_mean_squared_log_error']))\nprint('XGBoost: Average RMSLE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['test_neg_mean_squared_log_error']])\/len(scores_xgb_tuned['test_neg_mean_squared_log_error']))\n","28d6187c":"# map the feature importances to the feature names\nxgb_feats = X.columns\nxgb_feats_dict = defaultdict()\ncount = 0\n\nfor item in xgb_feats:\n    xgb_feats_dict['f'+str(count)] = item\n    count += 1\n    ","f8019916":"# Plot feature importance\nfig, ax = plt.subplots(figsize=(20, 15))\nxgb.plot_importance(model_xgb_tuned, importance_type='weight', ax=ax)\n\n# rename the y axis labels to the actual feature names\nlocs, labels = plt.yticks()\n\nnew_names = []\nfor item in labels:\n    for key, name in xgb_feats_dict.items():\n        if key == item.get_text():\n            new_names.append(name)\n\nax.set_yticklabels(new_names);\n            \nplt.show()\n","768464f6":"# get the final scores for the differenced data\nsplit = 10\nall_scores_test = []\nall_scores_train = []\n\nfor i in range(split):\n    scores_test = split_predict_test(X_train_over, y_train_over, X_test_over, y_test_over, i)\n    all_scores_test.append(scores_test)\n    scores_train = split_predict_train(X_train_over, y_train_over, X_test_over, y_test_over, i)\n    all_scores_train.append(scores_train)\n    \nrmse_test = []\nrmsle_test = []\nmae_test = []\nrmse_train = []\nrmsle_train = []\nmae_train = []\n\nfor vals in all_scores_test:\n    rmse_test.append(vals[0])\n    rmsle_test.append(vals[1])\n    mae_test.append(vals[2])\n    \nfor vals in all_scores_train:\n    rmse_train.append(vals[0])\n    rmsle_train.append(vals[1])\n    mae_train.append(vals[2])\n    \nprint('Overall Test RMSE:', sum(rmse_test)\/split)\nprint('Overall Test RMSLE:', sum(rmsle_test)\/split)\nprint('Overall Test MAE:', sum(mae_test)\/split)\n\nprint('Overall Train RMSE:', sum(rmse_train)\/split)\nprint('Overall Train RMSLE:', sum(rmsle_train)\/split)\nprint('Overall Train MAE:', sum(mae_train)\/split)\n    ","9aeae84d":"# check how the model performance without certain features that are very unimportant\nX_feat_imp = X.drop(columns=['weekday_1', 'weekday_3', 'weekday_4', 'weekday_6',\n                             'hot', 'cold', 'cool', 'warm'])\n","97bf493c":"# creating and fitting the ML pipeline\ntrend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n\npreprocessor = ColumnTransformer([\n    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('scaler', MinMaxScaler()),\n    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n    ('model', model_xgb_tuned)\n])\n\npipeline_xgb_tuned = pipeline.fit(X_feat_imp, y)\n","5e501df5":"# creating a timeseries split of the datasets\ntime_split = TimeSeriesSplit(n_splits=10)\n\n# doing cross validation on the chunks of data and calculating scores\nscores_xgb_tuned = cross_validate(pipeline_xgb_tuned, X_feat_imp, y, cv=time_split,\n                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n                                  'neg_mean_squared_log_error'],\n                         return_train_score=True, n_jobs=-1)\n","42ea1308":"# root mean squared error\nprint('XGBoost: Average RMSE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['train_neg_mean_squared_error']])\/len(scores_xgb_tuned['train_neg_mean_squared_error']))\nprint('XGBoost: Average RMSE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['test_neg_mean_squared_error']])\/len(scores_xgb_tuned['test_neg_mean_squared_error']))\n\n# mean absolute error\nprint('XGBoost: Average MAE train data:', \n      sum([(-1 * x) for x in scores_xgb_tuned['train_neg_mean_absolute_error']])\/len(scores_xgb_tuned['train_neg_mean_absolute_error']))\nprint('XGBoost: Average MAE test data:', \n      sum([(-1 * x) for x in scores_xgb_tuned['test_neg_mean_absolute_error']])\/len(scores_xgb_tuned['test_neg_mean_absolute_error']))\n\n# root mean squared log error\nprint('XGBoost: Average RMSLE train data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['train_neg_mean_squared_log_error']])\/len(scores_xgb_tuned['train_neg_mean_squared_log_error']))\nprint('XGBoost: Average RMSLE test data:', \n      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['test_neg_mean_squared_log_error']])\/len(scores_xgb_tuned['test_neg_mean_squared_log_error']))\n","e6d0ef36":"##### Checking for and dealing with missing values","bc67e557":"# Bike Sharing Demand","f7a1d07e":"The feature **temp_avg** is missing 821 values. I will investigate whether keeping this feature is even necessary given the other three temperature features. There are also 4 values missing for **casual, registered and total_cust** features and the target, respectively. I will check why these are missing and exclude those observations because if the target variable is missing, I cannot estimate it. \n\nThe **holiday** feature has NAs but these should actually 0s, which I will convert. The **wt_** features also have NAs where 0s should be which I will insert.\n","e71944e7":"##### Removing some features based on feature importance","0ced86af":"##### Dropping unnecessary columns\n\nI will drop the date feature and keep the date_datetime feature to have the feature with the correct date formatting. The registered and casual features will also be dropped because they are sub-features of the total_cust target label. I will also drop the temp_avg variable.","845cacf2":"From the above graph we can see that the month variable is correlated with the average number of customers per that month as well as the average temperature. The temperature strongly determines the number of customers. The temperature is likely a very important feature for predicting the target variable.","b768908e":"I'm using a try and except argument to load an already tuned model or, if none is available, to run randomsearch.","0e0cf295":"##### weather type features wt","acfc7ca9":"#### 2.4. Checking and dealing with stationarity","28ee5be7":"### 2. Data Understanding","0c41525b":"Based on the distributions of the target value depending on the weather feature as well as based on the assessment of similar descriptive weather names\/patterns, I will merge some of the weather features together.","7707dbaa":"I will use randomizedsearch and gridsearch to tune my hyperparameters for the Random Forest model.","0605e067":"#### 4.4. Random Forests","06019a11":"Based on the above plot, a lag of 1 shows significant correlation with t+0. Because of this high correlation, I will use the target variable at t-1 in my models.","1b29553f":"Removing seemingly unimportant features does not lead to a better result. Unimportant features are simply not considered.","85fa6f0b":"Based on the above distributions of number of customers per weekday, it appears that there are slight difference in demand depending on what weekday it is. Thus, the weekday will be considered to forecast the bike demand. But I still need to onehot encode the weekday because it is a categorical feature.","5bf1cdeb":"The season feature clearly determines the customer demand for bikes, so this feature will be used for the final model.","bd1c9244":"##### running XGBoost with undifferenced y","84f9f800":"#### 2.6. Feature Selection\n\nBased on the EDA above, I will choose the following features\n* holiday: t0\n* weekdays: each day at t0\n* workingday: t0\n* temp_max: rolling mean for 8 days\n* temp_max: rolling std for 8 days\n* temp_min:rolling mean for 8 days\n* temp_min: rolling std for 8 days\n* season_spring: t0\n* season_summer: t0\n* season_fall: t0\n* very cold, cold, cool, warm, hot: t-1\n* foggy, thunder, ice, sleet: rolling mean and std for 8 days\n* wind: rolling mean for 8 days\n* wind: rolling std for 8 days\n* precip: rolling mean for 8 days\n* precip: rolling std for 8 days\n* total_cust: t-1\n* total_cust: rolling mean for 8 days\n* total_cust: rolling std for 8 days\n","4e08f680":"Null hypothesis: data is stationary\n\nAlternative hypothesis: data is not stationary\n\nThe results of the KPSS Test can be intepreted as follows:\n* test-statistic > critical value --> reject null hypothesis (data is not stationary)\n* test_statistic < critical value --> fail to reject null hypothesis (data is stationary)\n\nBased on these results, all total_cust and rain features are not trend stationary at the 1%-level.\n\nThis has the following implications for the evaluated timeseries:\n* difference and trend stationary: **no** transformations are needed\n* difference stationary and **not** trend stationary: total_cust_std16 --> transformations needed\n* **not** difference stationary but trend stationary: total_cust --> transformations needed\n* **not** difference and **not** trend stationary: total_cust_mean16, total_cust_t-1 --> transformations needed\n    ","427fea5f":"#### 3.3. Assinging X and y","20f5445e":"##### checking how certain features influence the overall performance","1b3b8847":"For the holiday feature, we can clearly see that there is on average a higher demand for bikes on days that are not holidays. This feature will be used in the final model to predict the overall demand.","7ea40e8b":"### 1. Business Understanding","a8c8f7d1":"#### 3.2. Creating new features","a346a08e":"##### doing run through with untransformed y","d1f9f238":"##### temp_min\nI will continue to use temp_min for my analysis as well, even though there is a high correlation between the temp_max and temp_min feature. Collinearity does not matter with tree-based methods, therefore, I can keep it.\n","f11ee654":"##### weekday","4a13bcfe":"#### 2.2. Preprocessing","a11362b7":"##### Season","e05b1b7d":"These are the meanings of each variable in the dataset:\n- date: date\n- temp_avg: average temperature in Celcius\n- temp_min: minimum temperature in Celcius\n- temp_max: maximum temperature in Celcius\n- temp_observ: temperature at time of observation\n- precip: precipitation in mm \n- wind: average windspeed\n- wt: weather types\n    - fog: fog, ice fog, or freezing fog (may include heavy fog)\n    - heavy_fog: heavy fog or heaving freezing fog (not always distinguished from fog)\n    - thunder: thunder\n    - sleet: ice pellets, sleet, snow pellets, or small hail\n    - hail: hail (may include small hail)\n    - glaze: glaze or rime\n    - haze: smoke or haze\n    - drift_snow: blowing or drifting snow \n    - high_wind: high or damaging winds\n    - mist: mist\n    - drizzle: drizzle\n    - rain: rain (may include freezing rain, drizzle, and freezing drizzle)\n    - freeze_rain: freezing rain\n    - snow: snow, snow pellets, snow grains, or ice crystals\n    - ground_fog: ground fog\n    - ice_fog: ice for or freezing fog\n    - freeze_drizzle: freezing drizzle\n    - unknown: unknown source of precipitation\n- casual: count of casual users\n- registered: count of registered users\n- total_cust: count of total rental bikes including both casual and registered\n- holiday: holiday in Washington D.C.\n","df5e256b":"#### 4.1. Creating transformation classes","ed1f7d19":"##### checking how certain features influence the overall performance","dce177f8":"Below I will engineer some new features from the categorical and numeric features","76dd6cd8":"Although precip is only correlated with total_cust in a weak sense, I will keep this in the model. This distribution is also left-skewed, so a logarithmic transformation will be necessary.","aafd6fb0":"##### running XGBoost with differenced y","8c20bf80":"##### total_cust","72b1e57c":"Tuning the hyperparameters of AdaBoost","4b39d250":"The features holiday, weekday and workingday have some overlaps in their prediction of customer demand, thus, I will first analyze each individual feature and then investigate their correlation.","f9129cc1":"The data set is similar to the bike sharing dataset provided by the [UCI machine learning repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset). However, the aforementioned dataset only contains data for 2011 and 2012. It was originally created by Fanaee-T and Gama (2013) in their study [\"Event labeling combining ensemble detectors and background knowledge\"](https:\/\/link.springer.com\/article\/10.1007\/s13748-013-0040-3). \n\nTo have a larger dataset, I collected the following data from the following sources for the time period of January 1, 2011 until December 31, 2018:\n* The bike demand data comes from [Capital Bike Share](http:\/\/capitalbikeshare.com\/system-data)\n* The weather data was taken from [NOAA's National Climatic Data Center](https:\/\/www.ncdc.noaa.gov\/cdo-web\/search)\n* The holiday data is from the [DC Department of Human Resources](http:\/\/dchr.dc.gov\/page\/holiday-schedule). \n\nThe dataset uses data from the bike sharing stations of Capital Bike Share in the metro D.C. area in the United States. The weather data and holiday data refer to the same location.\n","37b8f5ac":"Null hypothesis: data is not stationary\n\nAlternative hypothesis: data is stationary\n\nThe results of the ADF Test can be intepreted as follows:\n* test-statistic < critical value --> reject null hypothesis (data is stationary)\n* test_statistic > critical value --> fail to reject null hypothesis (data is not stationary)\n\nBased on these results, total_cust, total_cust_t-1 and total_cust_mean16 are not stationary at the 1%-level. This test does not look at the trend stationarity but rather at the difference stationarity. This means that total_cust_t-1 and total_cust_mean16 are **not difference stationary** at the 1%-level. The KPSS test is necessary to detemine the trend stationarity of the timeseries.\n    ","fc8ebe90":"#### 2.3. Exploratory Data Analysis","cb1c5888":"The above shows that disregarding the first 820 rows leads to the highest correlation between temp_max and total_cust. Comparing temp_avg with the entire dataset and the respective correlations with total_cust, temp_avg has the highest correlation.\n\nThe Granger causality is a more appropriate measure for timeseries data and understanding how one variable can determine another variable. Thus, below I am using the Granger causality test to see how the temperature features cause total_cust.","4693f9fc":"#### 4.3. XGBoost","4cc1ac33":"##### Augmented Dickey Fuller Test","b4e338fd":"#### Differencing","88666771":"The main issue bike sharing operators have is how to meet the demand of customers when it occurs. Bike sharing systems have a high level of flexibility and thus, being able to predict future demand becomes vital in ensuring customer satisfaction and profitability in a crowded market. ","10b3c8ab":"A very important part before prediction can be accurate and successful is to make the time series stationary. Examples of how to do this can be found [here](https:\/\/www.analyticsvidhya.com\/blog\/2018\/09\/non-stationary-time-series-python\/) for example.","b757c803":"It is very obvious that this time series is non-stationary. I need to deal with this later on before using the data in my model.","31f804ae":"### 4. Model","677fc91e":"##### Trend\n","ca212bc1":"The values of less than 1 for correlations between a feature and itself can be explained by very small sample sizes for the chi-squared test that cramer's v builds on. Moreover, there are a few correlations between the weather type features which I should exploit and merge together.","aa0b82bc":"The features holiday, weekday and workingday are correlated with each other as well as with the target variable total_cust. However, they contain slightly different information that may be useful for predicting the target variable. Thus, I will keep all three variables for my model.","d7018a4d":"The transformations I implemented above can be used later on to make the timeseries stationary. I will create classes to make the necessary transformations inside the ML pipeline.","affc8b45":"##### holiday","cdf1e115":"The wt_ features contain the following features and meanings:\n- wt_fog: fog, ice fog, or freezing fog (may include heavy fog)\n- wt_heavy_fog: heavy fog or heaving freezing fog (not always distinguished from fog)\n- wt_thunder: thunder\n- wt_sleet: ice pellets, sleet, snow pellets, or small hail\n- wt_hail: hail (may include small hail)\n- wt_glaze: glaze or rime\n- wt_haze: smoke or haze\n- wt_drift_snow: blowing or drifting snow \n- wt_high_wind: high or damaging winds\n- wt_mist: mist\n- wt_drizzle: drizzle\n- wt_rain: rain (may include freezing rain, drizzle, and freezing drizzle)\n- wt_freeze_rain: freezing rain\n- wt_snow: snow, snow pellets, snow grains, or ice crystals\n- wt_ground_fog: ground fog\n- wt_ice_fog: ice for or freezing fog\n- wt_freeze_drizzle: freezing drizzle\n- wt_unknown: unknown source of precipitation","1d43481e":"There seem to be four days where no data was captured for the rented bikes. Because this is a time series, I am not sure whether I can drop these rows or whether I should interpolate the target values for these four days. More research is necessary to determine this.","0075e8b1":"##### temp_max","ca4d77e9":"#### 4.2. AdaBoost","cef82baf":"##### wind","f6be9885":"For this timeseries, I will also use a test to determine whether this timeseries is non-stationary.","7d0eece7":"##### running AdaBoost with differenced y","76792a63":"#### Preparation code for using differenced y for predictions","f90fb41a":"##### Kwiatkowski-Phillips-Schmidt-Shin Test","2d1f4e1d":"I will categorize the temperature similarly to what the paper by El-Assi et al. (2015) did:\n* Very Cold (below 0) \n* Cold (between 0 and 10)\n* Cool (between 10 and 20) \n* Warm (between 20 to 30)\n* Hot (30 or more)\n","b008b9f6":"##### workingday","29b9e1de":"Although windspeed is only correlated with cnt in a weak sense, similar to precip, I will keep this in the model. To determine stationarity, I will use a statistical test.","30f3309f":"Based on the results of the above pairplot, the following things are apparent:\n* **wind looks like a Weibull distribution**\n* there are almost **perfect linear relationships among the three temp features**\n* **precip feature is left skewed**\n* there is **no linear relationship between precip and any other feature**\n* **wind** has **no linear relationship with any other feature**\n* the **temp features** have a **medium strong linear relationship with the total_cust target**\n\n","ee2ab417":"**PACF to determine optimal lag for total_cust target label**\n\nDetails on the process can be found [here](https:\/\/towardsdatascience.com\/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8) and [here](https:\/\/towardsdatascience.com\/understanding-partial-auto-correlation-fa39271146ac). We want to avoid using variables with multicolinearity and thus, we do not want to use too many lag variables. ","72dbceac":"#### 2.1. Loading the data and visualizing and describing it","a1c4ae86":"##### precip","8db3272d":"The heatmap underlines the indications of the pairplot and a number of steps need to be taken:\n* the temp features are highly correlated with each other. I will keep both temp_max and temp_min because I tried it in the model and it was working better with keeping both features and Zeng et al. 2012 also use both temp_min and temp_max. \n* there are only low to very low negative correlations between wind and precip with the target label, respectively. However, I will keep both features in my model.\n","dac991ff":"#### Categorical variables","133980f7":"The newly created rain feature seems to have been discontinued after two years into the time series. I will therefore drop this feature.","bf40555b":"#### Continuous variables\n","9211c81e":"### 3. Data Preparation\n\n#### 3.1. Dropping unnecessary columns\n\nI am only going to remove any columns which I no longer need and split the dataframes into X and y.","06d68990":"As I am not forecasting wind, but rather trying to understand whether wind could be able to forecast total_cust, the partial autocorrelation is not of much interest to me.","dc07c47e":"#### 4.2. Baseline model: naive univariate prediction\n\nWhen using autoregression models, we're only focusing on the target variable and aim to predict this variable with it previous values.","31c90b5b":"##### running model with differenced y","363272e5":"##### running AdaBoost with undifference y"}}