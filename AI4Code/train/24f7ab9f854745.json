{"cell_type":{"d6b73e31":"code","6b151e0b":"code","ab1395f8":"code","b868d6f5":"code","14e0a623":"code","2b4b29c7":"code","2838c78f":"code","e73882df":"code","4416fad6":"code","12127309":"code","27079d98":"code","916c096c":"code","f5890dc0":"code","a82a2255":"code","1020bf5a":"code","f8710dfd":"code","8beb5ba7":"code","96c8d172":"code","9d3f5f9c":"code","7ee3c048":"code","dc6f8d1b":"code","d9eef5f5":"code","0082e667":"code","61750713":"code","2f739a0d":"code","82a24a3f":"code","441e3ace":"code","e291a33b":"code","f71f6608":"code","d6c5af12":"code","2e031c20":"code","bcb90136":"code","ee26a619":"code","57ce0b4e":"code","eeb36987":"code","c605db68":"code","0fe4d6e6":"code","01917909":"code","71b976f2":"code","13757c86":"code","34843add":"code","d05cfa57":"code","b0a53262":"code","6bbbfc27":"code","5fe4499a":"code","d051c1b5":"code","dbd64ca5":"code","bf7bb5c7":"code","99f9754d":"code","68bae4a3":"code","f9546eec":"code","e8ba6376":"code","9fa68506":"code","59893fde":"code","832abfd5":"code","34796f54":"code","83800a2f":"code","044f4fa3":"code","991b4869":"code","e62111b1":"code","290ebafc":"code","c8fe80df":"code","d103d635":"code","311c62cc":"code","2a046211":"code","c9bbe816":"code","6b1ec71b":"code","ab3d1748":"code","1f7449f4":"markdown","7b4114d2":"markdown","a384dd6f":"markdown","99de64bf":"markdown","19042bca":"markdown","6dd1bc4d":"markdown","92dcf137":"markdown","505691c8":"markdown","da79894c":"markdown","b5df61c2":"markdown","22f0372a":"markdown","f570a6ee":"markdown","4c91f36d":"markdown","9fcdac45":"markdown","323b38a5":"markdown","474607ce":"markdown","6b4ba09b":"markdown","b0261fcd":"markdown","24b2a033":"markdown","f4dd0f08":"markdown","5ebe2695":"markdown","4178b60c":"markdown","2ba86d58":"markdown","928ccdb6":"markdown","e3d9f7b4":"markdown","1614d946":"markdown","767f7e71":"markdown","4263b843":"markdown","f4e04bd3":"markdown","e6d8e4cc":"markdown","262a0f74":"markdown"},"source":{"d6b73e31":"# imports all the necessary pyhton modules\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize \nnltk.download('brown')\nfrom nltk.corpus import brown\nimport random\nimport time\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport numpy as np\nimport math\nimport pandas as pd\nfrom collections import defaultdict\nfrom multiprocessing import Pool\nprint(brown.sents()[:5])\n\n\n","6b151e0b":"list_of_words = [j for i in brown.sents() for j in i]","ab1395f8":"print(list_of_words[:20])\n","b868d6f5":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nst = stopwords.words('english')\nlist_of_words1= [i.lower() for i in list_of_words if i.isalpha()== True]\nnew_list_of_words = [i for i in list_of_words1 if i not in st]\n\nlen(new_list_of_words)\n","14e0a623":"#Generates a big dictionary so that it can be used to generate different documents\ndict_ = {}\nfor i in new_list_of_words:\n  dict_[i] = dict_.get(i,0)+1\ndict_key_list = list(dict_.keys())\n#print(dict_)\nprint(len(dict_))\n","2b4b29c7":"# This function creates a pair of dictionary of a perticular size, here that size is taken as input 'doc_key_len'\n\ndef doc_creation(doc_key_len,number_of_doc = 2):\n  list_doc = []\n  for i in range(number_of_doc):\n    rand_list_numbersi = random.sample(list(range(len(dict_key_list))), doc_key_len)\n    \n    \n    doc_i = {dict_key_list[j]:dict_[dict_key_list[j]] for j in rand_list_numbersi}\n    list_doc.append(doc_i)\n\n  return list_doc \n    \n\n\n","2838c78f":"doc_creation(5)[0]","e73882df":"# This function creates list of pair of documents where each pair has a certain size. The starting pair has a size of 500 and the final pair will have a size of 'doc_len'\ndef doc_set_collection(doc_len):\n  list_pair_doc = []\n  for i in range(500,doc_len):\n    list_pair_doc.append(doc_creation(i,2))\n\n  return list_pair_doc","4416fad6":"doc_pair_list = doc_set_collection(900)\nlist_of_documents = [j for i in doc_pair_list for j in i]","12127309":" # Loops through each item in the dictionary to find the total number of words in the bag\ndef totalfreq(dict1):\n  total = 0\n  for item in dict1:\n    total += dict1[item]\n  return total   \n\n#jaccard's algorithm\ndef jaccard(dict1,dict2):\n  intersection = {}\n  # for each key in dictionary1, if it is in dictionary2, then the words belong to both the dictionaries\n  for item in dict1.keys():\n    if item in dict2.keys():\n      intersection[item] = min(dict1[item],dict2[item])# minimum frequency is taken so that in the intersection only the minimum number belongs to both the bags\n  \n  intersectiontotal = totalfreq(intersection)\n  union = totalfreq(dict1)+totalfreq(dict2) - intersectiontotal\n  return round(intersectiontotal\/union,4)","27079d98":"#Function to find average running time of a target function\ndef time_it(somefunc,*arg,number_of_repeats=100,**kwarg):\n  list_=[]\n  \n#in each iteration the runtime of the function is noted and appended to a list\n  for i in range(number_of_repeats):\n    starttime = time.time()\n    somefunc(*arg,**kwarg)\n    endtime = time.time()\n    list_.append(endtime-starttime)\n\n  a = 0\n  for i in list_:#finds the average of the list\n    a += i\n    \n  mean = a\/len(list_)\n\n  return mean","916c096c":"list_jaccard_time =[]\n#Pair of documents with each pair having different lenght is passed in the loop so that time for different length of the document is captured\nfor i in range(len(doc_pair_list)):\n  # for each pair of documents, time_it function runs for 100 times\n  k =time_it(jaccard,doc_pair_list[i][0],doc_pair_list[i][1],number_of_repeats = 100)\n  list_jaccard_time.append(k)\nprint(list_jaccard_time)","f5890dc0":"#This list stores value of the jaccard similarity computed\nlist_jaccard_values = [jaccard(doc_pair_list[i][0],doc_pair_list[i][1]) for i in list(range(len(doc_pair_list)))]\nprint(list_jaccard_values)","a82a2255":"%matplotlib inline\nx = list(range(500,len(doc_pair_list)+500))#list of bag size\ny = list_jaccard_time\n\nplt.xlabel('Bag Size')\nplt.ylabel('Mean Jaccard Run Time (s)')\nplt.title('Run Time of \\n Jaccard similarity vs bag size')\nplt.plot(x,y)\n\n\nplt.show()\n\n","1020bf5a":"# Converting each element in the bag size list and the run time list to logarithmic vlaues with base 10\nlogx = [ np.log10(i )for i in x]\nlogy = [np.log10(i) for i in y]\n#plotting the graph between the log values\nplt.plot(logx,logy)\nplt.ylabel(\"Log mean Jaccard Runtime\")\nplt.xlabel('Log of Bag Size')\nplt.show()","f8710dfd":"slope_jaccard,intercept, r_value, p_value ,std_err = stats.linregress(logx,logy)\nconstant = np.power(10,intercept)\nprint(\"The slope is {} and the Constant for the computation is {}\".format(slope_jaccard,constant))","8beb5ba7":"#Function which generates dense vector pairs as numpy arrays for finding out the cosine similarity\ndef densevectors_asnparray(dict1,dict2):\n  list1 = list(dict1.keys())\n  list2 = list(dict2.keys())\n  listtogather = list(set(list1 + list2)) # creating a list of unique keys from both the dict.keys() list\n  list1f =[]\n  list2f= []\n  for i in listtogather: #for keys in dict1, it gives the corresponding values and otherwise it appends 0\n    if i in dict1.keys():\n      list1f.append(dict1[i])\n    else:\n      list1f.append(0)\n\n  for i in listtogather: #for keys in dict2, it gives the corresponding values and otherwise it appends 0\n    if i in dict2.keys():\n      list2f.append(dict2[i])\n    else:\n      list2f.append(0)\n\n  return np.asarray(list1f) ,np.asarray(list2f) #finally the funciton returns numpy vectors corresponding to each dictionary\n","96c8d172":"# To generate list of densevector pair in numpy array form\nlist_densevector_nppair = []\nfor i in doc_pair_list:\n  list_densevector_nppair.append(densevectors_asnparray(i[0],i[1]))\n\n\n","9d3f5f9c":"#stores the lenght of vector pairs created so that later it can be used to plot the list lenth vs run time graph\nlist_len_densevector_nppair = [len(i[0]) for i in list_densevector_nppair]\n","7ee3c048":"def modvec(vec): # Function to calculate the modulus of a vector\n  sum = 0\n  \n  for i in vec:\n    sum += i**2\n  return np.sqrt(sum)\n\ndef cosinesim_dot(A,B):# function to calculate the cosine similarity by looping over individual element in the vector ie vector iteration\n  sum = 0\n  for i in range(len(A)):\n    sum += A[i]*B[i]\n\n  modA = modvec(A)\n  modB = modvec(B)\n\n  k = modA*modB\n\n  return round(sum\/k,4)\n","dc6f8d1b":"list_cosinesim_dot_time =[]\nfor i in list_densevector_nppair:\n  k = time_it(cosinesim_dot,i[0],i[1],number_of_repeats = 100)\n  list_cosinesim_dot_time.append(k)","d9eef5f5":"#This list retains the cosine similarity values for vector iteration\nlist_cosinesim_dot_values = [cosinesim_dot(list_densevector_nppair[i][0],list_densevector_nppair[i][1]) for i in list(range(len(list_densevector_nppair)))]","0082e667":"\nplt.subplots_adjust(1,0,2.5,0.5)\n\nplt.subplot(121)\nplt.plot(list_len_densevector_nppair,list_cosinesim_dot_time)\n\n\nplt.xlabel(' Dense Vector Length')\nplt.ylabel('Mean Cosine Run Time (s)')\nplt.title('Vector lenght vs Run time for \\n cosine similarity with vector iteraton')\n\nplt.subplot(122)\nx = list_len_densevector_nppair\ny = list_cosinesim_dot_time\n\nlogX = np.log10(x)\nlogY = np.log10(y)\n\nplt.scatter(logX,logY)\nplt.title('log Vector lenght vs log Run time for \\n cosine similarity with vector iteraton')\nplt.xlabel('Log Dense Vector Length')\nplt.ylabel('Log Mean Cosine Run Time (s)')\nplt.show()","61750713":"slope_cosine_vec, intercept, r_value, p_value, std_errd = stats.linregress(logX,logY)\nconstant = np.power(10, intercept)\nprint('The slope is {} and the constant for the computation is {}'.format(slope_cosine_vec,constant))","2f739a0d":"# funcion to calculate cosine similarity using numpy module\ndef cosinesim_numpy(A,B):\n  \n  \n  modA = np.sqrt(np.dot(A,A))\n  modB = np.sqrt(np.dot(B,B))\n  AdotB = np.dot(A,B)\n  x = modA * modB\n\n  cosinesimilarity = AdotB\/x\n\n  return round(cosinesimilarity,4)\n","82a24a3f":"list_cosinenp_time = []\nfor i in list_densevector_nppair:\n  k = time_it(cosinesim_numpy,i[0],i[1],number_of_repeats=100)\n  list_cosinenp_time.append(k)\n","441e3ace":"#This list retains the values of cosine similarity by using numpy array dot product\nlist_cosinesim_numpy_values = [ cosinesim_numpy(list_densevector_nppair[i][0],list_densevector_nppair[i][1]) for i in list(range(len(list_densevector_nppair)))]","e291a33b":"plt.subplots_adjust(1,0,2.5,0.5)\n\nplt.subplot(121)\nplt.plot(list_len_densevector_nppair,list_cosinenp_time)\nplt.ylabel('Mean Cosine Runtime (s)')\nplt.xlabel('Size of Dense Vector')\nplt.title('Vector lenght v\/s Runtime for \\n cosine similarity with numpy array')\n\nplt.subplot(122)\nx = list_len_densevector_nppair\ny = list_cosinenp_time\nlogX = np.log10(x)\nlogY = np.log10(y)\nplt.scatter(logX,logY)\nplt.xlabel('Log Size of Dense Vector')\nplt.ylabel('Log Mean Cosine Runtime')\nplt.title('log Vector lenght v\/s log Runtime for \\n cosine similarity with numpy array')\nplt.show()","f71f6608":"slope_cosine_np, intercept, r_value , p_value , std_err = stats.linregress(logX,logY)\nconstant = np.power(10,intercept)\nprint('The slope is {} and the Constant for the computation is {}'.format(slope_cosine_np,constant))","d6c5af12":"plt.plot(list_len_densevector_nppair,list_cosinesim_dot_time)\nplt.plot(list_len_densevector_nppair,list_cosinenp_time)\n\nplt.xlabel('Vector Dimension')\nplt.ylabel('Mean run time (s)')\nplt.title('Vector dimension v\\s mean Runtime')\nlabel =['cosine vector iteration', 'cosine numpy array']\nplt.legend(label, loc = 1, ncol = 2)\nplt.show()\n","2e031c20":"def dot(d1,d2):#Here the inputs are dictionaries\n    if len(d1) < len(d2):\n        a = d1\n        b = d2\n    else:\n        a = d2\n        b = d1\n    dotproduct = 0\n    #for each key in the dictionary1, if there is similar key in dictionary2, then the values of both the keys are multiplied and added to the 'dotproduct' \n    for word,count in a.items():\n        dotproduct += count * b.get(word,0)\n    return dotproduct\n\ndef cosinesim_sparse(d1,d2):\n    numerator = dot(d1,d2)# calls the dot function created earlier\n    denominator = np.sqrt(dot(d1,d1)*dot(d2,d2))\n    return round(numerator\/denominator,4)\n  \n    \n","bcb90136":"#To get the list of average running time for cosine similarity using sparse(dictionary) representation\nlist_cosinesim_sparse_time = []\nfor i in doc_pair_list:\n  k = time_it(cosinesim_sparse,i[0],i[1],number_of_repeats=100)\n  list_cosinesim_sparse_time.append(k)\n","ee26a619":"#This list retains the cosine similarity values for the sparse comparison\nlist_cosinesim_sparse_values = [cosinesim_sparse(doc_pair_list[i][0],doc_pair_list[i][1])    for i in list(range(len(doc_pair_list)))]","57ce0b4e":"#The following list retains the list of lenght of the document pair list\nlist_len_doc_pair_list = [len(i[0]) for i in doc_pair_list]","eeb36987":"x = list_len_doc_pair_list\ny = list_cosinesim_sparse_time\nplt.subplots_adjust(1,0,2.5,0.5)\nplt.subplot(121)\nplt.plot(x,y)\nplt.xlabel(\"Document size\")\nplt.ylabel(\"Mean Cosine Run time (s)\")\nplt.title('Document size v\\s Runtime for \\n cosine similarity using sparse dictionary')\nplt.subplot(122)\nlogx = np.log10(x)\nlogy = np.log10(y)\n\nplt.scatter(logx,logy)\nplt.xlabel('Log Dictionary Size')\nplt.ylabel('Log Mean Cosine Run Time')\nplt.title(' log Document size v\\s log Runtime for \\n cosine similarity using sparse dictionary')\n\nplt.show()","c605db68":"slope_cosine_sparse , intercept, r_value , p_value , std_err = stats.linregress(logx,logy)\nconstant = np.power(10,intercept)\nprint('The slope is {} and the constant for the computation is {}'.format(slope_cosine_sparse,constant))","0fe4d6e6":"data = {'Jaccard similarity': list_jaccard_values,'cosine similarity(using vector iteration)':list_cosinesim_dot_values, 'cosine similarity(using numpy array)':list_cosinesim_numpy_values,'cosine similarity(using sparse representation)':list_cosinesim_sparse_values}","01917909":"df = pd.DataFrame(data)\ndf","71b976f2":"cosinesim_numpy(list_cosinesim_numpy_values,list_cosinesim_sparse_values)","13757c86":" plt.plot(list_len_doc_pair_list,list_jaccard_time)\n plt.plot(list_len_doc_pair_list,list_cosinesim_dot_time)\n plt.plot(list_len_doc_pair_list,list_cosinenp_time)\n plt.plot(list_len_doc_pair_list,list_cosinesim_sparse_time)\n label = ['Jaccad time','Cosine (vector iteration)','Cosine (numpy )','Cosine (sparse)']\n plt.legend(label, loc = 1, ncol =4)\n plt.xlabel('Basic Documnet Size')\n plt.ylabel('Mean Run Time (sec)')\n plt.title('Runtime comparison b\\w various similarity measures')\n plt.show()\n","34843add":"def all_pair_comparison(list_of_documents, method= 'jaccard'):\n  all_pair_similarity = []\n  #for jaccard similarity\n  if method =='jaccard':\n    for i in list_of_documents:\n      for j in list_of_documents:\n        all_pair_similarity.append(jaccard(i,j))\n  #for cosine similarity using vector iteration      \n  if method == 'cosine_vector':\n    for i in list_of_documents:\n      for j in list_of_documents:\n        a,b = densevectors_asnparray(i,j)\n        k = cosinesim_dot(a,b)\n        all_pair_similarity.append(k)\n  #for cosine similarity using numpy dot operation\n  if method == 'cosine_numpy':\n    for i in list_of_documents:\n      for j in list_of_documents:\n        a,b = densevectors_asnparray(i,j)\n        k = cosinesim_numpy(a,b)\n        all_pair_similarity.append(k)\n  #for cosine similarity using sparse dictionaries\n  if method == 'cosine_sparse':\n    for i in list_of_documents:\n      for j in list_of_documents:\n        k = cosinesim_sparse(i,j)\n        all_pair_similarity.append(k)\n\n\n  return all_pair_similarity\n\n\n","d05cfa57":"# created a list of 16 documents with each dictionary has a bag size of 1000\nall_pair_doc_list = doc_creation(1000,16)","b0a53262":"# Here the method can be changed to other similarity methods and all pair similarity for different methods can be found for the 'all_pair_doc_list'\nprint(all_pair_comparison(all_pair_doc_list,method = 'cosine_numpy'))","6bbbfc27":"#genereates a list of list of dictionaries where the lenght of individual dictionary is same as the size of list of dictionary\nlist_of_list_all_pair_doc = []\nfor i in range(50,65):\n  list_of_list_all_pair_doc.append(doc_creation(i,i))","5fe4499a":"#The run time of all pair similarity  using jaccard's similarity for list of list of dictionaries is stored in 'list_of_list_all_pair_run_time'\nlist_of_list_all_pair_run_time =[]\nfor i in range(len(list_of_list_all_pair_doc)):\n  k = time_it(all_pair_comparison,list_of_list_all_pair_doc[i])\n  list_of_list_all_pair_run_time.append(k)","d051c1b5":"y = list_of_list_all_pair_run_time\nx = range(50,65)\n\nplt.subplots_adjust(1,0,2.5,0.5)\nplt.subplot(121)\nplt.plot(x,y)\nplt.xlabel('Len of list of documents')\nplt.ylabel('Run time of all-pair similarity (s)')\nplt.title('Runtime of all pair similarity for \\n Jaccards')\n\nplt.subplot(122)\nlogx = np.log10(x)\nlogy = np.log10(y)\nplt.scatter(logx,logy)\nplt.xlabel('log len of list of documents')\nplt.ylabel('log run time of all-pair similarity (s)')\nplt.title('log Runtime of all pair similarity for \\n Jaccards')\n\nplt.show()","dbd64ca5":"slope, intercept, r_value, p_value , std_err = stats.linregress(logx,logy)\nconstant = np.power(10, intercept)\nprint('The slope is {} and the constant for the computation is {}'.format(slope,constant))","bf7bb5c7":"list_of_list_all_pair_run_time_cosine =[]\nfor i in range(len(list_of_list_all_pair_doc)):\n  k = time_it(all_pair_comparison,list_of_list_all_pair_doc[i],method = 'cosine_sparse')\n  list_of_list_all_pair_run_time_cosine.append(k)","99f9754d":"y = list_of_list_all_pair_run_time_cosine\nx = list(range(50,65))\n\nplt.subplots_adjust(1,0,2.5,0.5)\n\nplt.subplot(121)\nplt.plot(x,y)\nplt.ylabel('Runtime of all-pair similarity (s)')\nplt.xlabel('lenght of list of documents')\nplt.title('Runtime for all pair similarity for \\n cosine')\nplt.subplot(122)\n\nlogx = np.log10(x)\nlogy = np.log10(y)\n\nplt.scatter(x,y)\nplt.xlabel('log of lenght of documents')\nplt.ylabel('log Runtime of all-pair similarity (s)')\nplt.title(' log Runtime for all pair similarity for \\n cosine')\nplt.show()","68bae4a3":"slope , intercept, r_value, p_value, std_err = stats.linregress(logx, logy)\nconstant = np.power(10,intercept)\nprint('The slope is {} and the constant for the computation is {}'.format(slope , constant))","f9546eec":"plt.plot(list(range(50,65)),list_of_list_all_pair_run_time)\nplt.plot(list(range(50,65)),list_of_list_all_pair_run_time_cosine)\nplt.ylabel('mean run time for all-pair similarity')\nplt.xlabel('len of list of dictionaries')\nlabel = ['Jaccard all-pair', 'cosine_sparse all-pair']\nplt.title('comparison between jaccard and cosine runtime for all pair similariy')\nplt.legend(label, loc =1 , ncol = 2)\nplt.show()","e8ba6376":"import multiprocessing\n\ncores = multiprocessing.cpu_count()\ncores","9fa68506":"\n\n# Defining the mapper function for the map-reduce implementation\ndef Mapper_all_pair_jaccard(doc_):\n  list_output = []\n  (i,j,doc_i,doc_j) = doc_\n  \n\n  similarity_ = jaccard(doc_i,doc_j)\n  list_output.append(((i,j),similarity_))\n  return list_output","59893fde":"def map_reduce_parallel(inputs,mapper,reducer,mapprocesses=1,reduceprocesses=1):\n    \n    collector=defaultdict(list)  #this dictionary is where we will store intermediate results\n                                 #it will map keys to lists of values (default value of a list is [])\n                                 #in a real system, this would be stored in individual files at the map nodes\n                                 #and then transferred to the reduce nodes\n    \n    mappool = Pool(processes=mapprocesses)\n    #map stage\n    \n    mapresults=mappool.map(mapper,inputs)\n    mappool.close()\n    \n    for mapresult in mapresults:\n        for (key, value) in mapresult:     #pass each input to the mapper function and receive back each key,value pair yielded\n            collector[key].append(value)     #append the value to the list for that key in the intermediate store\n            \n    #reduce stage \n    outputs=[]\n    reducepool = Pool(processes=reduceprocesses)\n    \n    reduceresults=reducepool.map(reducer,collector.items())\n    reducepool.close()\n    for reduceresult in reduceresults:\n        outputs+=reduceresult\n   \n    return outputs","832abfd5":"#Defining the reducer function for the map-reduce implementation\ndef reducer_all_pair(item):\n  (keys,values) = item\n  output_reducer = [(keys,values)]\n  return (output_reducer)","34796f54":"def mapreduce_allpair_jaccard (all_pair_docc_list):\n  mapper_iterator = []\n  for i in range(len(all_pair_docc_list)):\n    for j in range (len(all_pair_docc_list)):\n      mapper_iterator.append((i,j,all_pair_docc_list[i],all_pair_docc_list[j]))\n\n  return map_reduce_parallel(mapper_iterator,Mapper_all_pair_jaccard,reducer_all_pair)","83800a2f":"#stores result as a list of tuples with doument index and corresponding similarity measure\nresult_jaccard = mapreduce_allpair_jaccard(all_pair_doc_list)","044f4fa3":"print(result_jaccard)","991b4869":"jaccard_values =all_pair_comparison(all_pair_doc_list,method='jaccard')\ndocument_index = [i[0] for i in result_jaccard]\njaccard_similarity_mr = [i[1] for i in result_jaccard]\n","e62111b1":"data = {'document index': document_index, 'jaccard similarity using mapreduce': jaccard_similarity_mr, 'jaccard similarity without mapreduce': jaccard_values }\ndf_jacc = pd.DataFrame(data)\ndf_jacc","290ebafc":"#defining mapper function for cosine similarity \ndef Mapper_all_pair_cosine(doc_):\n  list_output = []\n  (i,j,doc_i,doc_j) = doc_\n  \n\n  similarity_ = cosinesim_sparse(doc_i,doc_j)\n  list_output.append(((i,j),similarity_))\n  return list_output","c8fe80df":"#defining the main mapreduce function to be called for computing cosine similarity\ndef mapreduce_allpair_cosine (all_pair_docc_list):\n  mapper_iterator = []\n  for i in range(len(all_pair_docc_list)):\n    for j in range (len(all_pair_docc_list)):\n      mapper_iterator.append((i,j,all_pair_docc_list[i],all_pair_docc_list[j]))\n\n  return map_reduce_parallel(mapper_iterator,Mapper_all_pair_cosine,reducer_all_pair)","d103d635":"#stores result as a list of tuples with doument index and corresponding similarity measure\nresult_cosine = mapreduce_allpair_cosine(all_pair_doc_list)","311c62cc":"print(result_cosine) ","2a046211":"cosine_values =all_pair_comparison(all_pair_doc_list,method='cosine_sparse')\ndocument_index = [i[0] for i in result_cosine]\ncosine_similarity_mr = [i[1] for i in result_cosine]\n","c9bbe816":"data = {'document index': document_index, 'cosine similarity using mapreduce': cosine_similarity_mr, 'cosine similarity without mapreduce': cosine_values }\ndf_cos = pd.DataFrame(data)\ndf_cos","6b1ec71b":"#creates list of 100 dictionaries with bag size of 100\nall_pair_doc_list_100 = doc_creation(100,100)\nall_pair_doc_list_100[0]","ab3d1748":"#the average time taken to run the all pair similarity using MapReduce is found out\n\na=time_it(mapreduce_allpair_jaccard,all_pair_doc_list_100,number_of_repeats=5)\nb=time_it(mapreduce_allpair_cosine,all_pair_doc_list_100,number_of_repeats=5)\nprint('The average time taken to find all pair similarity of 100 documents with bagsize of 100: \\n {} seconds using jaccard \\n {} seconds using cosine'.format(a,b))","1f7449f4":"# Theoretical worst case running time for computing all-pair similariy\n\nIn order to run the all-pair similarity we should have a nested loop to iterate through all the documents vs all the documents $O(n^2)$. In the inner loop then the corresponding similarity measure $O(n)$ has to run. (Note : both '$n$' are different, first '$n$' is size of list of dictionaries and the second '$n$' is size of largest individual dictionary in the list)\n\nSo, the time complexity of all-pair similarity depends on the size of the list of dictionaries and the size of individual dictionary in the list.\n\nAssuming the size of individual dictionary is similar to the size of the list of dictionaries, the worst case time complexity for all the similarity measures we considered except cosine similarity implementation using numpy module will be $\\longrightarrow$ $O(n^3)$\n\nBut, since the size of individual dictionary doesn't affect the cosine numpy implementation, the time complexity for all-pair similarity for it will be $\\longrightarrow O(n^2)$\n\n\nHowever if we keep the size of dictionaries constant and we only vary the size of list of dictionaries, then the time complexity for all the similarity measure methods will be $\\longrightarrow O(n^2)$\n\n","7b4114d2":"From the emperical analysis of all-pair time complexity for cosine similarity with sparse representation\n* $\\alpha \\, \\approx 2.9  $ and  $C_a \\approx 10^{-6} \\implies$ the worst case time complexity of all-pair similarity is $\\longrightarrow O(n^{2.9})$ which justifies the theoretical worst case run time prediction.\n\n* The run time vs length of all pair similarity can be represented as $y=10^{-6} x^{2.9}$\n\n\n\n","a384dd6f":"# Emperical time complexity analysis for the cosine similarity implementation using numpy array methods.\n\nThe $\\alpha $ value for the function is fluctuating between $0.01$ to $0.3$ in multiple runs of the algorithm, with the mode value $\\approx 0.11$. The constant for the computation is of the order of $10^{-5}$.\n\nSo, emperically we can write $y= 10^{-5} x^{0.11}$ which is $\\approx 10^{-5}$. which means that emperically also we got that the run time of the cosine similarity using numpy array methods are almost independent of the vector dimension.","99de64bf":"By running the **Jaccard's** algorithm multiple times, I have got $\\alpha$ values ranging from $0.8$ to $1.2$ with mode value $\\approx \\, 0.95$ for multiple run  and the constant for the computation is of the order of $10^{-7}$. So the time complexity of our implementation is almost $O(n^{0.95})$ which is still better than the worst case run time of jaccards algorithm if the dict.keys() is implemented as list or if it has significant hash collisions.\n\nNow from the emperical analysis we can write $y = 10^{-7} x^{0.95 }$ , where $y$ is the run time of the function and $x$ is the bag size.","19042bca":"# Theoretical worst case running time of cosine similarity for vector iteration.\n\nHere first I have created a cosine similarity function which doesn't use the full power of numpy array module. The function takes in two list of numpy array, but loops through each element in the array to find out the dot product of the vector as well as to find the modulus of each vector.\n\nwhile running the created cosine similarity function `cosinesim_dot(A,B)` there are 4 for loops with time complexity of $O(n)$ which makes the entire function to have a time complexity of $\\longrightarrow O(n)$\n\n","6dd1bc4d":"# Introduction\n\nThe documents similarity measurements have several applications in the real world like to find the **mirror site**, **plagarism checking** etc. Here in this report I will descuss about the time complexities of two of the similarity measurements: **Jaccard's** and **cosine** similarity measurement theoretically as well as emperically. Also, the various types of implementation of cosine similarity measures like vector representation and sparse dictionary representaion will be compared with each other. After that all pair similarity measurements and its time complexity will be compared for both Jaccard's and cosine implemention and various other observations and how emperical findings differ from the theoretical predictions also will be dicussed. Finally, Map Reduce paradigm will be used to implement parallel processing for calculating all pair similarity.","92dcf137":"The list of average running time `list_jaccard_time` is ploted against the bag size","505691c8":"From the above graph it is evident that cosine similarity using vector iteration takes higher time to compute than the numpy implementation. ","da79894c":"# All pair similarity","b5df61c2":"From the above emperical analysis of time complexity of all-pair similarity using Jaccard's method the following conclusion can be made\n\n* $\\alpha \\, \\approx 3.1  $ and  $C_a \\approx 10^{-7} \\implies$ the worst case time complexity of all-pair similarity is $\\longrightarrow O(n^{3.1})$ which justifies the theoretical worst case run time prediction.\n\n* The run time vs length of all pair similarity can be represented as $y=10^{-7} x^{3.1}$\n\n","22f0372a":"The run time for all pair similarity we calculated for 100 documents is actually greater than the runtime required without using map reduce. This is because for small number of documents map reduce is less efficient than normal process. But as we take large number of documents mapreduce will give better efficiency than normal calculations.","f570a6ee":"To find the time taken by any algorithm, we have to run the algorithm multiple time for the same input and take the average running time, to make sure that the running time of the algorithm is not greatly affected by the sudden fluctuations in the other bakground processess of the computer. The `time_it` function is created to takle this.","4c91f36d":"# Emperical analysis of the Jaccard's algorithm runtime.\n\n","9fcdac45":"From the graph it is evident that for smaller size of list of list of dictionaries, Jaccard's similarity takes less time to run compared to cosine sparse implementation. Also, from the graph it seems like the slope of cosine sparse all-pair run time increases with increasing size of list of list of dictonaries.","323b38a5":"From the above comparison it is evident that jaccar similarity with mapreduce gives same results as jaccard similarity without mapreduce. So, the Map Reduce implementation is correct\n\n# All pair similarity using cosine similarity using parallel processing","474607ce":"# Summary\nIn this report I have analysed the time complexity of Jaccard's and different cosine smiliarity implementations theoretically as well as emperically. It is found that cosine similarity using numpy array has the least time complexity $O(1)$ and other similarity measures are of $O(n)$. However, cosine similarity using vector iteration is very time consuming compared to other methods event though other methods (except cosine using numpy) also has time complexity of $O(n)$.\n\nFurther, I have created function to calculate all pair similarity with parameter specifying similarity measure. And finally a map reduce paradigm is implemented for implementing parallel processing.\nHowever the map-reduce computing paradigm has to be tested for mass data processing with high number of cores\/computers in order to know the full potential of it, which is beyond the scope of this report.\n","6b4ba09b":"\n\n\n\n\n# Theoretical comparison of time complexity of cosine implementation using sparse dictionaries with other similarity implementations.\n\n* The time complexity of the cosine similarity implementation using sparse dictionary will be $O(n)$ as there is a loop in the code to iterate through items in the dictionary\n\n* While implementing the cosine similarity directly from sparse dictionaries, the size of the document we compare will be less since we don't have to convert it to the dense representation, as dense representation has more size. So, the run time will be better than the cosine implementation with vector iteration\n\n* However, the run time of this will be not better than the run time of the cosine implementation using numpy, because while using numpy method each element can be accessed directly.\n\n* The jaccard and cosine sparse have same time complexity $O(n)$\n\nThe following code takes in two dictionaries as input and directly calculate the  cosine similarity between them.","b0261fcd":"\n\n# Jaccard's Similarity\n\n* Jaccards's similarity is a mathematical concept which is used to find out the similarity between two sets or bag of words by taking the ratio of common elements in the set( $intersection$ ) to the total number of elements in the sets ( $union$ ).\n\n* $J(A,B)=\\cfrac{\\mid A\\cap B\\mid}{\\mid A \\cup B\\mid}$\n\n\n# Theoretical worst case running time of Jaccard's similarity\n\nWhile running the jaccard's similarity we have to find the union of the set and the intersection of the set. While finding the union of the set we have to iterate through each item in the dictionary, ie the worst case running time for finding the union is $O(n)$. Now while finding the intersection of the two sets, we have to loop through through each item of one of the dictionary to find if there is same key in the other dictionary which has a complexity of $O (n)$ on it's own. But, within the loop there is a possibility of hash collision. During the worst case scenario, hash collision has a complexity of $O (n)$. As the size of the dictionary increases, there is more chance of hash collisions. \nSo, if we assume that there is no hash collisions, then the worst case time complexity of jaccard's algorithm we use will be $\\longrightarrow O(n)$. But, as the document size increases and there is significant hash collisions, then the worst case time complexity will be $\\longrightarrow O(n^2)$.\n\n","24b2a033":"By observing the pandas Data Frame it is evident that though there is difference between the similarity measure of jaccard and cosine implementations(which is actually expected, since the fundamental method in which they calculate the similarity is different), all the different cosine implementations give the same similariy values for each pair of documents.\n\nWe can further use the cosine similarity for dense representation, which is already implemented to make sure the correctness of cosine similarity using sparse representation. We can consider the list of similarities we obtained already as two dense vectors and can be passed to the arguments of `cosinesim_numpy`. If the sparse representation is correct, the `cosinesim_numpy`should give an output equal to 1 or very close to 1","f4dd0f08":"# Emperical time complexity observation of cosine similarity using vector iteration.\n\nThe $\\alpha $ value of the equation has a mode value $\\approx 0.98$ for multiple run $\\implies O(n^{0.98})$, which makes justification to our theoretical prediction of the complexity as $O(n)$\n\nAlso the constant $C_a$ for the computation is of the order of $10^{-6}$.\n\nSo, emperically I can write the function as $y = 10^{-6} \\, x^{0.98}$ , where $y$ is the run time of the function and $x$ is the length of the vectors.\n\n\n","5ebe2695":"\n# The Cosine Similarity\n\n* Cosine similarity is also a mathematical concept which can be used to determine how similar two documents are by finding out the angle between the two vectors (the documents can be represented as vectors in the euclidean space). Smaller angle $\\implies$ higher the $cos \\, \\theta$ and more similar the documents are.\n\n* $cos\\, \\theta\\, =\\cfrac { \\vec A\\, \\cdot \\vec B}{ |A| * |B|} $","4178b60c":"# Theoretical time complexity of cosine similarity using numpy array\n\nThe advantage of numpy array is that the time complexity to access an element in an numpy array by using numpy methods is $O(1)$. Here in the new cosine similarity function I created, all the operations are carried out by using numpy methods, especially the `numpy.dot()` method. Unlike the cosine similarity with vector iteration, here the time complexity to get the dot product done is independent of the lenght of the vector ( numpy array). Also, I have given special attention to make sure that there is no conversion of list to numpy array is taking place inside the cosine function. If there is any list is converted to numpy array inside the cosine function, then the time complexity of the cosine function will be $O(n)$.\n\nBut, since in our function, There is no list to array conversion is taking place, the theoretical time complexity of our function should be $\\longrightarrow O(1)$","2ba86d58":"\n\n# All pair similarity for Jaccard using parallel processing (Map Reduce Paradigm)\n\n## Map Reduce paradigm\n\nIs used to do parallel processing of the given task by splitting the data into differnt chunks and by making use of all the cores at the same time and completing the whole task efficiently. Here, the map reduce is used in such a way that the mapper will calculate the similarity and will iterate over the input given and reducer will just collect output from the collector and join it.Also, I have used google colab and collab has assigned only 2 cores. So the sum of number of mappers and reducers should be equal to 2 and hence I have assigned 1 mapper and 1 reducer processess.","928ccdb6":"The above created `densevectors_asnparray` is used to create a list of numpy vector pairs and it is stored in a new list `list_densevector_nppair `","e3d9f7b4":"\n\nA function `all_pair_comparison` which computes all pair similarities for a collection of documents is created. The function takes a list of dictionaries as the input `list_of_documnents` and there is a parameter which can be varied to specify the similarity measure to be used `method`\n\nThe parameter `method` is \n* \"jaccard\" $\\longrightarrow $        jaccard similarity (default)\n* \"cosine_vector\" $\\longrightarrow $ cosine similarity using vector iteration\n* \"cosine_numpy\" $\\longrightarrow $ cosine similariy using numpy array method\n* \"cosine_sparse\" $\\longrightarrow $ cosine similarity using sparse dictionary","1614d946":"For calculating the cosine similarity, the list of pair of dictionaries which we already created is converted into **dense representation** ie as vectors.\n\n","767f7e71":"From the above graph the following conclusions can be made\n* Cosine similarity using the numpy array module is has the least running time followed by jaccard's algorithm which is followed by cosine sparse.\n\n* Cosine similarity using vector iteration has the worst running time for a given document size among all the four similarity implementations.\n","4263b843":"# Time prediction for running all-pair similarity for 200k documents\nFrom the obtained equations for Jaccard's and cosine sparse and substituting 200,000\n\n### If individual document size is also in the order of 200k:\n\n* If jaccard's is used: $y \\approx 10^{-7} 200000^{3.1} \\implies 2.7\\times 10^{9}s$\n* If cosine_sparse is used: $y \\approx 10^{-6} 200000^{2.9} \\implies 2.36\\times 10^{8}s$ $\\longrightarrow$ But this value is probably a wrong prediction because the slope of the cosine sparse increases as the lenght of list of list increases, and it is supposed to give a higher run time than the Jaccard's algorithm\n\n### If individual document size is very small compared to 200k:\nthen the time complexity $\\approx O(n^2)$ \n* If Jaccard's is used: $y \\approx 10^{-7} 200000^{2} \\implies 4\\times 10^{3}s$\n* If cosine_sparse is used: $y \\approx 10^{-6} 200000^{2} \\implies 4\\times 10^{4} s$ ","f4e04bd3":"We have obtained the out put to be equal to 1, which means that the sparse computation is gives exactly same result as the dense vector computaion and the implementation is correct.","e6d8e4cc":"Emperically we have the time complexity of the cosine similarity implementation using sparse dictionary $\\approx O(n^{0.9})$ which is slightly better than the theoretical worst case of $O(n)$","262a0f74":"To find the time complexity we have to assume that the curve is in the form of $y = C_a.x^\\alpha$.\n\nNow we can take $log$ on both sides of the equation and make it in the form $log \\,y = log(C_a) \\, + \\alpha \\, log(x)$\n\nThis is now equation of a straight line where $log \\, (C_a)$ is the intercept and $\\alpha $ is the slope."}}