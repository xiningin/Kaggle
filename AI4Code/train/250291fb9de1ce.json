{"cell_type":{"bdc86798":"code","afc04fb7":"code","11f5e6fa":"code","763fc355":"code","5b97ecbd":"code","a56209c5":"code","a4987058":"code","b9d9dfc3":"code","fd7a594c":"code","8aa88446":"code","48cacc40":"code","b96e4bf0":"code","8bb5478f":"code","dbc58f08":"code","1311db48":"code","1a2b58b7":"code","45d40cee":"code","899ef567":"code","e39fc239":"code","68893cbe":"code","c4515283":"code","ffc28d55":"code","a030fb53":"code","e629513c":"code","80dbb0f3":"code","8a1831ab":"code","99a6f3de":"code","6dfe0812":"code","433eab72":"code","e7b05140":"code","74c9767b":"code","afbe33e8":"code","c59be23d":"code","5658762f":"code","54e636bf":"code","36b95127":"code","567e263f":"code","4c631e37":"code","e856a769":"code","643fc062":"code","f9ecdc9d":"code","57e15ce9":"code","6baa4621":"code","e67f7b74":"code","cecfe3aa":"code","011848fd":"code","29cab832":"code","9ded7479":"code","22ee820a":"code","7f047ae9":"markdown","880a8583":"markdown","04f7b6b2":"markdown","b3128665":"markdown","ceeebf10":"markdown","b2423d3d":"markdown","5860e6eb":"markdown","81425fff":"markdown","4ea8f9b7":"markdown","ea8e7fd4":"markdown","224e1cee":"markdown","a6de9f2c":"markdown","8b5fbabf":"markdown","ffbcb285":"markdown","7b1f40f7":"markdown","10cd6c46":"markdown","9aa60f81":"markdown","01386f84":"markdown","7829b47c":"markdown","309e7e6b":"markdown","7f1ec67a":"markdown","ad65681e":"markdown","c4f90a4d":"markdown","a4b358ed":"markdown","4f4ae796":"markdown","3a2d42ef":"markdown"},"source":{"bdc86798":"# Uncomment and run the appropriate command for your operating system, if required\n# No installation is reqiured on Google Colab \/ Kaggle notebooks\n\n# Linux \/ Binder \/ Windows (No GPU)\n# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n\n# Linux \/ Windows (GPU)\n# pip install numpy matplotlib torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n \n# MacOS (NO GPU)\n# !pip install numpy matplotlib torch torchvision torchaudio","afc04fb7":"#!pip install opendatasets","11f5e6fa":"# import opendatasets as od\n# dataset_url = 'https:\/\/www.kaggle.com\/karnikakapoor\/art-portraits'\n# od.download(dataset_url)","763fc355":"import os\n\nDATA_DIR = '..\/input\/art-portraits'\nprint(os.listdir(DATA_DIR))","5b97ecbd":"print(os.listdir(DATA_DIR+'\/Portraits')[:10])","a56209c5":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef show_jpg(jpg_name):\n    img = cv2.imread(jpg_name)\n    plt.imshow(img)\n    plt.show()","a4987058":"show_jpg(DATA_DIR+'\/Portraits\/'+'bc1cf30d88ee3291af791f77eac70582c.jpg')","b9d9dfc3":"from torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as T","fd7a594c":"image_size = 64\nbatch_size = 128\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)","8aa88446":"train_ds = ImageFolder(DATA_DIR, transform=T.Compose([\n    T.Resize(image_size),\n    T.CenterCrop(image_size),\n    T.ToTensor(),\n    T.Normalize(*stats)]))\n\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)","48cacc40":"import torch\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n%matplotlib inline","b96e4bf0":"def denorm(img_tensors):\n    return img_tensors * stats[1][0] + stats[0][0]","8bb5478f":"def show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n\ndef show_batch(dl, nmax=64):\n    for images, _ in dl:\n        show_images(images, nmax)\n        break","dbc58f08":"show_batch(train_dl)","1311db48":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","1a2b58b7":"device = get_default_device()\ndevice","45d40cee":"train_dl = DeviceDataLoader(train_dl, device)","899ef567":"import torch.nn as nn","e39fc239":"discriminator = nn.Sequential(\n    # in: 3 x 64 x 64\n\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 64 x 32 x 32\n\n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 128 x 16 x 16\n\n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 256 x 8 x 8\n\n    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 512 x 4 x 4\n\n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    # out: 1 x 1 x 1\n\n    nn.Flatten(),\n    nn.Sigmoid())","68893cbe":"discriminator = to_device(discriminator, device)","c4515283":"latent_size = 128","ffc28d55":"generator = nn.Sequential(\n    # in: latent_size x 1 x 1\n\n    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    # out: 512 x 4 x 4\n\n    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # out: 256 x 8 x 8\n\n    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # out: 128 x 16 x 16\n\n    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # out: 64 x 32 x 32\n\n    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.Tanh()\n    # out: 3 x 64 x 64\n)","a030fb53":"xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\nfake_images = generator(xb)\nprint(fake_images.shape)\nshow_images(fake_images)","e629513c":"generator = to_device(generator, device)","80dbb0f3":"def train_discriminator(real_images, opt_d):\n    # Clear discriminator gradients\n    opt_d.zero_grad()\n\n    # Pass real images through discriminator\n    real_preds = discriminator(real_images)\n    real_targets = torch.ones(real_images.size(0), 1, device=device)\n    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n    real_score = torch.mean(real_preds).item()\n    \n    # Generate fake images\n    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n    fake_images = generator(latent)\n\n    # Pass fake images through discriminator\n    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n    fake_preds = discriminator(fake_images)\n    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n    fake_score = torch.mean(fake_preds).item()\n\n    # Update discriminator weights\n    loss = real_loss + fake_loss\n    loss.backward()\n    opt_d.step()\n    return loss.item(), real_score, fake_score","8a1831ab":"def train_generator(opt_g):\n    # Clear generator gradients\n    opt_g.zero_grad()\n    \n    # Generate fake images\n    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n    fake_images = generator(latent)\n    \n    # Try to fool the discriminator\n    preds = discriminator(fake_images)\n    targets = torch.ones(batch_size, 1, device=device)\n    loss = F.binary_cross_entropy(preds, targets)\n    \n    # Update generator weights\n    loss.backward()\n    opt_g.step()\n    \n    return loss.item()","99a6f3de":"from torchvision.utils import save_image","6dfe0812":"sample_dir = 'generated'\nos.makedirs(sample_dir, exist_ok=True)","433eab72":"def save_samples(index, latent_tensors, show=True):\n    fake_images = generator(latent_tensors)\n    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n   # print('Saving', fake_fname)\n    if show:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))","e7b05140":"fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)","74c9767b":"save_samples(0, fixed_latent)","afbe33e8":"from tqdm import tqdm\nimport torch.nn.functional as F","c59be23d":"def fit(epochs, lr, start_idx=1):\n    torch.cuda.empty_cache()\n    \n    # Losses & scores\n    losses_g = []\n    losses_d = []\n    real_scores = []\n    fake_scores = []\n    \n    # Create optimizers\n    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    for epoch in range(epochs):\n        for real_images, _ in tqdm(train_dl):\n            # Train discriminator\n            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n            # Train generator\n            loss_g = train_generator(opt_g)\n            \n        # Record losses & scores\n        losses_g.append(loss_g)\n        losses_d.append(loss_d)\n        real_scores.append(real_score)\n        fake_scores.append(fake_score)\n        \n        # Log losses & scores (last batch)\n        #print(\"Epoch [{}\/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n         #    epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n    \n        # Save generated images\n        save_samples(epoch+start_idx, fixed_latent, show=False)\n    \n    return losses_g, losses_d, real_scores, fake_scores","5658762f":"# lr = 0.0002\n# epochs = 25","54e636bf":"import gc \ngc.collect()","36b95127":"# lr = 0.0001\n# epochs = 20","567e263f":"import gc \ngc.collect()","4c631e37":"# lr = 0.01\n# epochs = 250","e856a769":"lr = 0.0002\nepochs = 300","643fc062":"import gc \ngc.collect()","f9ecdc9d":"history = fit(epochs, lr)","57e15ce9":"torch.save(generator.state_dict(), 'G.pth')\ntorch.save(discriminator.state_dict(), 'D.pth')","6baa4621":"from IPython.display import Image","e67f7b74":"Image('.\/generated\/generated-images-0001.png')","cecfe3aa":"Image('.\/generated\/generated-images-0300.png')","011848fd":"import cv2\nimport os\n\nvid_fname = 'gans_training.avi'\n\nfiles = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]\nfiles.sort()\n\nout = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (530,530))\n[out.write(cv2.imread(fname)) for fname in files]\nout.release()","29cab832":"# plt.plot(losses_d, '-')\n# plt.plot(losses_g, '-')\n# plt.xlabel('epoch')\n# plt.ylabel('loss')\n# plt.legend(['Discriminator', 'Generator'])\n# plt.title('Losses')\n# plt.savefig('1.png')","9ded7479":"# plt.plot(real_scores, '-')\n# plt.plot(fake_scores, '-')\n# plt.xlabel('epoch')\n# plt.ylabel('score')\n# plt.legend(['Real', 'Fake'])\n# plt.title('Scores')\n# plt.savefig('2.png')","22ee820a":"from IPython.display import HTML\n#<iframe width=\"896\" height=\"679\" src=\"https:\/\/www.youtube.com\/embed\/c-w4VeFxFp8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>\n# you only change video ID : 'TNzDMOg_zsw' , and width and height as you want\ndisplay(HTML('<iframe class=\"center\" width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/c-w4VeFxFp8?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"1\" allowfullscreen><\/iframe>'))","7f047ae9":"<div class=\"alert alert-block alert-warning\"  style=\"text-align:center;font-size:300%;\">Let's Set our data directory<\/div>","880a8583":"<div class=\"alert alert-block alert-warning\"><b>Here's how the generated images look, after the 1st, 5th and 10th epochs of training.<\/b><\/div>","04f7b6b2":"<div class=\"alert alert-block alert-warning\"  style=\"text-align:center;font-size:300%;\">Let's install required libraries<\/div>","b3128665":"<div class=\"alert alert-block alert-warning\">Here's what it looks like:\n\n\n\nWe can also visualize how the loss changes over time. Visualizing losses is quite useful for debugging the training process. For GANs, we expect the generator's loss to reduce over time, without the discriminator's loss getting too high.<\/div>","ceeebf10":"<div style ='text-align:center'class=\"alert alert-block alert-warning\"><h1>Save the model checkpoints <\/h1><\/div>","b2423d3d":"<div class=\"alert alert-block alert-warning\"><h1 style=\"font-size:400%;text-align:center;color:green;\">Training Generative Adversarial Networks (GANs) in PyTorch<\/h1>\n\n<h3 style=\"font-size:200%;text-align:right;color:green;\"> - by Vivek Patel<\/h3>\n\n<h1 class=\"alert alert-block alert-warning\"  style=\"font-size:400%;\">Before we start...<\/h1>\n\n<h1 class=\"alert alert-block alert-success\"style=\"font-size:200%;\">What is Generative adversarial networks (GANs)?<\/h1>\n\n\n<b>Generative adversarial networks (GANs) are an exciting recent innovation in machine learning. GANs are generative models: they create new data instances that resemble your training data. For example, GANs can create images that look like photographs of human faces, even though the faces don't belong to any real person.\n\nThese images were created by a GAN:<\/b>\n <div class=\"column\">\n    <img src=\"https:\/\/developers.google.com\/machine-learning\/gan\/images\/gan_faces.png\" style=\"width:500px;height:250px;\">\n  <\/div>\n <b>more info at <a href='https:\/\/thispersondoesnotexist.com\/'>https:\/\/thispersondoesnotexist.com\/<\/a> <\/b>\n<p class=\"alert alert-success\">This notebook is a minimum demo for GAN. Since <a href='https:\/\/www.paperspace.com'>PaperSpace<\/a> allows maximum run time limit of 6 hrs in free tier, we will only train a lightweight model in this notebook.<\/p>\n\n<b><h1 class=\"alert alert-block alert-success\" style=\"font-size:200%;\">Background: What is a Generative Model?<\/h1>\nWhat does \"generative\" mean in the name \"Generative Adversarial Network\"? \"Generative\" describes a class of statistical models that contrasts with discriminative models.<\/b>\n<b>Informally:<\/b>\n<ul>\n  <li><b>Generative models can generate new data instances.<\/b><\/li>\n  <li><b>Discriminative models discriminate between different kinds of data instances.<\/b><\/li>\n<\/ul>\nin other words There are two neural networks: a <b>Generator<\/b> and a <b>Discriminator<\/b>. \n<ul>\n<li><b>The generator generates a latent vector(random vector\/random tensor\/random matrix)<\/b><\/li>\n    <li><b>the discriminator attempts to detect whether a given sample is \"real\" (picked from the training data) or \"fake\" (generated by the generator).<\/b><\/li> <\/ul>\n    <img src=\"https:\/\/i.imgur.com\/6NMdO9u.png\"><\/img>\n<b>Training happens in tandem: we train the discriminator for a few epochs, then train the generator for a few epochs, and repeat. This way both <b>the generator<\/b> and <b>the discriminator<\/b> get better at doing their jobs.<\/b>\n<h1 class=\"alert alert-block alert-warning\" style=\"font-size:400%;\">1. About dataset<\/h1>\n\n\n<div class=\"alert alert-info\" role=\"alert\">\n   Dataset used for this notebook is predominantly a subset of  <a href='https:\/\/www.kaggle.com\/ipythonx\/wikiart-gangogh-creating-art-gan?select=mythological-painting'>Wiki-Art : Visual Art Encyclopedia dataset<\/a>.\n   <a href='https:\/\/www.kaggle.com\/karnikakapoor'>Karnika Kapoor<\/a> cleaned and picked out data that was somewhat aligned in the same way. subset dataset can be found at following link <i>https:\/\/www.kaggle.com\/karnikakapoor\/art-portraits<\/i><\/div>\n\n<h1 class=\"alert alert-success\" >Orignal dataset  :-<a href='https:\/\/www.kaggle.com\/ipythonx\/wikiart-gangogh-creating-art-gan?select=mythological-painting'>Wiki-Art : Visual Art Encyclopedia<\/a><\/h1>\n\n<h1 class=\"alert alert-success\">Subset dataset :- <a href='https:\/\/www.kaggle.com\/karnikakapoor\/art-portraits'>art-portraits<\/a><\/h1>\n\n\n<h1 style=\"border:2px solid Tomato;\">Orignal dataset size :- 38GB<\/h1>\n<h1 style=\"border:2px solid green;\">Subset dataset size  :- 1.46GB<\/h1><\/div>\n<h1 class=\"alert alert-block alert-warning\"  style=\"text-align:left;font-size:300%;\">Folder Structure<\/h1>\n<div class=\"alert alert-block alert-warning\">\n<code>.\n\/root\n\u251c\u2500\u2500 Training GAN on gpu.ipynb\n\u251c\u2500\u2500 art-portraits\n\u2502   \u2514\u2500\u2500 Portraits\n            \u2514\u2500\u2500 image-1    \n            \u2514\u2500\u2500 image-2    \n            \u2514\u2500\u2500 image-n <\/code>\n            <\/div>","5860e6eb":"We are now ready to train the model.","81425fff":"<div class=\"alert alert-block alert-warning\">The dataset has a single folder called <b>images<\/b> which contains all 4,000+ images in JPG format.<\/div>","4ea8f9b7":"<h1 class=\"alert alert-block alert-warning\" style=\"font-size:400%;text-align:center;color:green;\">objective<\/h1>\n<div class=\"alert alert-block alert-warning\"><b>modeling objective<\/b>\nGenerative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset. <\/div>","ea8e7fd4":"<div class=\"alert alert-block alert-warning\">Let's move the discriminator model to the chosen device.<\/div>","224e1cee":"<div class=\"alert alert-block alert-warning\"  style=\"text-align:center;font-size:300%;\">Exploring the Data<\/div>","a6de9f2c":"<div class=\"alert alert-block alert-warning\">We can now move our training data loader using DeviceDataLoader for automatically transferring batches of data to the GPU (if available).<\/div>","8b5fbabf":"<div class=\"alert alert-block alert-warning\">\n<h1>Thanks again, I couldn\u2019t have pulled this off without you<\/h1>\n    \n<b>[Submitted on 10 Jun 2014]<\/b> Generative Adversarial Networks Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n\n- <b>https:\/\/arxiv.org\/abs\/1406.2661<\/b>\n\n<h1>I\u2019m so thankful for everything you bring to the table<\/h1>\n\n<b>\"Deep Learning with PyTorch: Zero to GANs\"<\/b> is a beginner-friendly online course offering a practical and coding-focused introduction to deep learning using the PyTorch framework.\n\n- <b>https:\/\/jovian.ai\/learn\/deep-learning-with-pytorch-zero-to-gans<\/b>\n\n\n<h1>Thank you for going above and beyond!<\/h1>\n\nJoin the one-stop destination for self-guided coding exercises, quizzes, technical articles, and video tutorials. Gain the software skills you need to develop with Google\u2019s technology.\n\n- <b>https:\/\/developers.google.com\/machine-learning\/gan<\/b>  <\/div>","ffbcb285":"<div class=\"alert alert-block alert-warning\"><h1>Full Training Loop<\/h1>\n\nLet's define a `fit` function to train the discriminator and generator in tandem for each batch of training data. We'll use the Adam optimizer with some custom parameters (betas) that are known to work well for GANs. We will also save some sample generated images at regular intervals for inspection.\n\n<img src=\"https:\/\/i.imgur.com\/6NMdO9u.png\" style=\"max-width:420px; margin-bottom:32px\"\/><\/div>\n\n","7b1f40f7":"<div class=\"alert alert-block alert-warning\">Let's create helper functions to denormalize the image tensors and display some sample images from a training batch.<\/div>","10cd6c46":"<div class=\"alert alert-block alert-warning\">Let's create a directory where we can save intermediate outputs from the generator to visually inspect the progress of the model. We'll also create a helper function to export the generated images.<\/div>","9aa60f81":"<div class=\"alert alert-block alert-warning\"><h1>Discriminator Training<\/h1>\n\nSince the discriminator is a binary classification model, we can use the binary cross entropy loss function to quantify how well it is able to differentiate between real and generated images.\n\n<img src=\"https:\/\/image.slidesharecdn.com\/chrishokamp-dublinnlp3-160805110319\/95\/task-based-learning-for-nlp-going-beyond-cross-entropy-chris-hokamp-10-638.jpg?cb=1470395213\" width=\"420\" ><\/div>","01386f84":"<div class=\"alert alert-block alert-warning\">Here are the steps involved in training the discriminator.\n<b>\n    \n- We expect the discriminator to output 1 if the image was picked from the real MNIST dataset, and 0 if it was generated using the generator network.\u00a0\n\n- We first pass a batch of real images, and compute the loss, setting the target labels to 1.\u00a0\n\n- Then we pass a batch of fake images (generated using the generator) pass them into the discriminator, and compute the loss, setting the target labels to 0.\u00a0\n\n- Finally we add the two losses and use the overall loss to perform gradient descent to adjust the weights of the discriminator.\n\nIt's important to note that we don't change the weights of the generator model while training the discriminator (`opt_d` only affects the `discriminator.parameters()`) <\/b><\/div>","7829b47c":"<div class=\"alert alert-block alert-warning\">We'll use a fixed set of input vectors to the generator to see how the individual generated images evolve over time as we train the model. Let's save one set of images before we start training our model.<\/div>","309e7e6b":"<div class=\"alert alert-block alert-warning\"><h1>Generator Network<\/h1>\n\nThe input to the generator is typically a vector or a matrix of random numbers (referred to as a latent tensor) which is used as a seed for generating an image. The generator will convert a latent tensor of shape `(128, 1, 1)` into an image tensor of shape `3 x 28 x 28`. To achive this, we'll use the `ConvTranspose2d` layer from PyTorch, which is performs to as a *transposed convolution* (also referred to as a *deconvolution*). [Learn more](https:\/\/github.com\/vdumoulin\/conv_arithmetic\/blob\/master\/README.md#transposed-convolution-animations)\n\n<img src='https:\/\/i.imgur.com\/DRvK546.gif'><\/div>","7f1ec67a":"<div class=\"alert alert-block alert-warning\">Based on where you're running this notebook, your default device could be a CPU <b>(torch.device('cpu'))<\/b> or a GPU <b>(torch.device('cuda'))<\/b>.<\/div>","ad65681e":"<div class=\"alert alert-block alert-warning\"><b>Using a GPU<\/b>\nTo seamlessly use a GPU, if one is available, we define a couple of helper functions (get_default_device & to_device) and a helper class DeviceDataLoader to move our model & data to the GPU, if one is available.<\/div>","c4f90a4d":"<div class=\"alert alert-block alert-warning\"><h1>Discriminator Network<\/h1><br>\nThe discriminator is a <b>classifier<\/b> that determines if the input samples are real or fake. These input samples can be real samples coming from the training data, or fake coming from the generator. We'll use a convolutional neural networks (CNN) which outputs a single number output for every image We'll use stride of 2 to progressively reduce the size of the output feature map, and we're using the Leaky ReLU activation for the discriminator.<\/div>\n<img src='https:\/\/raw.githubusercontent.com\/vdumoulin\/conv_arithmetic\/master\/gif\/padding_strides_odd.gif'><\/img>","a4b358ed":"<div class=\"alert alert-block alert-warning\"><h1>Generator Training<\/h1>\n\nSince the outputs of the generator are images, it's not obvious how we can train the generator. This is where we employ a rather elegant trick, which is to use the discriminator as a part of the loss function. Here's how it works:\n    \n<b>\n    \n- We generate a batch of images using the generator, pass the into the discriminator.\n\n- We calculate the loss by setting the target labels to 1 i.e. real. We do this because the generator's objective is to \"fool\" the discriminator.\u00a0\n\n- We use the loss to perform gradient descent i.e. change the weights of the generator, so it gets better at generating real-like images to \"fool\" the discriminator.\n\n    <\/b>\nHere's what this looks like in code.<\/div>","4f4ae796":"<div class=\"alert alert-block alert-warning\"  style=\"text-align:center;font-size:300%;\">Let's Download Dataset with <br><br>opendatasets<\/div>","3a2d42ef":"<div class=\"alert alert-block alert-warning\">Let's load this dataset using the <b>ImageFolder<\/b> class from <b>torchvision<\/b>. We will also resize and crop the images to 64x64 px, and normalize the pixel values with a mean & standard deviation of 0.5 for each channel. This will ensure that pixel values are in the range <b>(-1, 1)<\/b>, which is more convenient for training the discriminator. We will also create a data loader to load the data in batches.<\/div>"}}