{"cell_type":{"58379f28":"code","f6e061f0":"code","b5cee436":"code","80cfb9f7":"code","0d393146":"code","51a4be76":"code","887c9a28":"code","d4673313":"code","f031a135":"code","9d377866":"code","7b673088":"code","24bc827b":"code","cd09d78c":"code","3a67984e":"code","357163f6":"code","da6a642c":"code","a252f6e6":"code","be37a660":"code","f5c4b6d0":"code","1e13ed37":"code","289da7e4":"code","24226707":"code","a7b35ee9":"code","7b4af52e":"code","824fb7dc":"code","c9e488c2":"code","efe773ee":"code","73679126":"code","430a8ac6":"code","45129759":"code","7e6f8ac0":"code","b8f4e853":"code","7eea61cf":"code","3f8ded29":"code","6d134863":"code","7eb17987":"code","64f3064d":"code","c0cb1e3f":"code","54c39e11":"code","c5341ac0":"code","b119f514":"code","8388eb93":"code","8add56b3":"code","2105e414":"code","044d55bc":"code","dcffdd36":"code","932da1dd":"code","7fd0ec63":"code","a8324794":"code","9eeadf4d":"code","0c8f5df5":"code","4a787f7d":"code","3963a36f":"code","02baddd3":"code","52cb0910":"code","9b171551":"code","e3158597":"code","0a55e540":"code","c08adb20":"code","375fcfe0":"code","0acc271b":"code","0f37ea69":"code","3472e971":"code","77753d60":"code","e1c87516":"code","c444cd2b":"code","d8c925b3":"code","4ef8ae2e":"code","4c3da7ff":"code","1a4d053c":"code","d4d74859":"code","0796ed74":"code","3587af9d":"code","eb2d0e18":"code","c1c72a9f":"code","b01a818d":"code","5e80d651":"code","b4668a31":"code","3d0a5cd4":"code","b4150a79":"code","c034c524":"code","f2d6163c":"code","6eabd7f6":"code","299295e4":"code","ec527361":"code","9da23d98":"code","e8fd3fd9":"code","8bd52330":"code","56705a8e":"code","9a33e8a8":"code","e6c4c3d2":"code","d7874e4d":"code","77867dca":"code","d7a1c83d":"code","14f5892f":"code","e5841ff1":"code","8f9e4190":"code","0bf17333":"code","09b654db":"code","e47deb65":"code","06e44069":"code","f1dcfd93":"code","68433fd5":"code","e1d806d5":"code","b5765d6d":"code","8e5ee89c":"code","b2e34b6b":"code","419b8d4f":"code","d4aa0f5e":"code","ec9c898e":"code","170b1cbb":"code","81150cdd":"code","b5d9c40f":"code","25fc1c4f":"code","c0679879":"code","0efa3551":"code","b35600c9":"code","2263ef7d":"code","1488f9d5":"code","6a3520a5":"code","a008386d":"code","1c310222":"code","e0783254":"code","58b606c1":"code","9a1f81b8":"code","37bd59da":"code","07ca0583":"code","77ce7d94":"code","d4f87f36":"code","e9c863bc":"code","b731f7cf":"code","6db3d172":"code","7e115e02":"code","e000a822":"code","49f2a23d":"code","d44e81e3":"code","09508c0a":"code","93319244":"code","6ab63f3e":"code","72505f5b":"code","ac2d267b":"code","e954e560":"code","03ca2266":"code","4f0e364a":"code","ad8fa6e2":"code","76294683":"code","237516dd":"code","9f57f434":"code","f97093bc":"code","7bd7c3eb":"code","8ab9bc78":"code","5558f9aa":"code","887687c3":"code","9ee9b48c":"code","f9caf5c2":"code","39b07c16":"markdown","7e9319ee":"markdown","1c742486":"markdown","708d5470":"markdown","3ae999a1":"markdown","9c4cccb5":"markdown","4fcecdc8":"markdown","32b071cd":"markdown","cfc35b3a":"markdown","bfd4a2e5":"markdown","c063ab2d":"markdown","daf47b80":"markdown","a246f722":"markdown","db0ea819":"markdown","8b500e57":"markdown","785567e1":"markdown","f93b5c25":"markdown","d0a7eeb8":"markdown","4a0cf855":"markdown","6ef13bd2":"markdown","759a100c":"markdown","7781dc92":"markdown","3d64064b":"markdown","f72df44b":"markdown","425588ba":"markdown","e4e9cccb":"markdown","731df835":"markdown","bb88afa9":"markdown","e41e26db":"markdown","58ff9efb":"markdown","3dc563f5":"markdown","46086481":"markdown","6bfd54f1":"markdown","670b8240":"markdown","599b9339":"markdown","69dc945b":"markdown","bd3dae30":"markdown","18e88766":"markdown","0bf3eeed":"markdown","fef635a3":"markdown","2d3442a4":"markdown","dc86a386":"markdown","77851e5e":"markdown","048ff08f":"markdown","1da67fe6":"markdown","db800379":"markdown","272da260":"markdown","2ccaa012":"markdown","5a91cc44":"markdown","e72a9ccf":"markdown","0a8ba7b9":"markdown","d5efdff1":"markdown","7e8ffa7d":"markdown","dc9ac41e":"markdown","e87ca9da":"markdown","60adf4bf":"markdown","db3b2ceb":"markdown","a7f0674d":"markdown","2a1c286d":"markdown","76f8d9e1":"markdown","53d59e39":"markdown","d6325d5c":"markdown","89739f16":"markdown","4c6743af":"markdown","f3339a4f":"markdown","8a32bf06":"markdown","a6bde1b4":"markdown","3d1ffdce":"markdown","c50ebe58":"markdown","ea07b47a":"markdown","ebc134da":"markdown","6e382098":"markdown","9d79f502":"markdown","a16790a9":"markdown","3ca0b403":"markdown","658337e0":"markdown","1cc51683":"markdown","dd1b5581":"markdown","e464ef10":"markdown","5a97c92c":"markdown","7a6c6fbb":"markdown","8ff172cd":"markdown","6c0063af":"markdown","cff913e5":"markdown","adc53458":"markdown","416337dc":"markdown","448e3d35":"markdown","471f0980":"markdown","fb5f9ce2":"markdown","96d12fdb":"markdown"},"source":{"58379f28":"#For numerical libraries\nimport numpy as np\n#To handle data in the form of rows and columns\nimport pandas as pd\n#importing seaborn for statistical plots\nimport seaborn as sns\n#importing ploting libraries\nimport matplotlib.pyplot as plt\n#styling figures\nplt.rc('font',size=14)\nsns.set(style='white')\nsns.set(style='whitegrid',color_codes=True)\n#To enable plotting graphs in Jupyter notebook\n%matplotlib inline\n#importing the feature scaling library\nfrom sklearn.preprocessing import StandardScaler\n#Import Sklearn package's data splitting function which is based on random function\nfrom sklearn.model_selection import train_test_split\n# Import Linear Regression, Ridge and Lasso machine learning library\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\n# Import KNN Regressor machine learning library\nfrom sklearn.neighbors import KNeighborsRegressor\n# Import Decision Tree Regressor machine learning library\nfrom sklearn.tree import DecisionTreeRegressor\n# Import ensemble machine learning library\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,AdaBoostRegressor,BaggingRegressor)\n# Import support vector regressor machine learning library\nfrom sklearn.svm import SVR\n#Import the metrics\nfrom sklearn import metrics\n#Import the Voting regressor for Ensemble\nfrom sklearn.ensemble import VotingRegressor\n# Import stats from scipy\nfrom scipy import stats\n# Import zscore for scaling\nfrom scipy.stats import zscore\n#importing the metrics\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n#importing the K fold\nfrom sklearn.model_selection import KFold\n#importing the cross validation score\nfrom sklearn.model_selection import cross_val_score\n#importing the preprocessing library\nfrom sklearn import preprocessing\n# importing the Polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\n#importing kmeans clustering library\nfrom sklearn.cluster import KMeans\nfrom sklearn.utils import resample","f6e061f0":"#reading the CSV file into pandas dataframe\nconcrete_df=pd.read_csv('..\/input\/concrete-data-set\/concrete.csv')","b5cee436":"#Check top few records of the dataset\nconcrete_df.head()","80cfb9f7":"#Check the last few records of the dataset\nconcrete_df.tail()","0d393146":"#To show the detailed summary \nconcrete_df.info()","51a4be76":"# Data types information\nconcrete_df.dtypes","887c9a28":"#To get the shape \nconcrete_df.shape","d4673313":"#To get the columns name\nconcrete_df.columns","f031a135":"#Analyze the distribution of the dataset\nconcrete_df.describe().T","9d377866":"print('Range of values: ', concrete_df['cement'].max()-concrete_df['cement'].min())","7b673088":"print('Minimum age: ', concrete_df['cement'].min())\nprint('Maximum age: ',concrete_df['cement'].max())\nprint('Mean value: ', concrete_df['cement'].mean())\nprint('Median value: ',concrete_df['cement'].median())\nprint('Standard deviation: ', concrete_df['cement'].std())","24bc827b":"Q1=concrete_df['cement'].quantile(q=0.25)\nQ3=concrete_df['cement'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['cement']))","cd09d78c":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in cement: ', L_outliers)\nprint('Upper outliers in cement: ', U_outliers)","3a67984e":"print('Number of outliers in cement upper : ', concrete_df[concrete_df['cement']>586.4375]['cement'].count())\nprint('Number of outliers in cement lower : ', concrete_df[concrete_df['cement']<-44.0625]['cement'].count())\nprint('% of Outlier in cement upper: ',round(concrete_df[concrete_df['cement']>586.4375]['cement'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in cement lower: ',round(concrete_df[concrete_df['cement']<-44.0625]['cement'].count()*100\/len(concrete_df)), '%')","357163f6":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='cement',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Cement', fontsize=15)\nax1.set_title('Distribution of cement', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['cement'],ax=ax2)\nax2.set_xlabel('Cement', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Cement vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['cement'])\nax3.set_xlabel('Cement', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Cement vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","da6a642c":"print('Minimum slag: ', concrete_df['slag'].min())\nprint('Maximum slag: ',concrete_df['slag'].max())\nprint('Mean value: ', concrete_df['slag'].mean())\nprint('Median value: ',concrete_df['slag'].median())\nprint('Standard deviation: ', concrete_df['slag'].std())\nprint('Null values: ',concrete_df['slag'].isnull().any())","a252f6e6":"Q1=concrete_df['slag'].quantile(q=0.25)\nQ3=concrete_df['slag'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['slag']))","be37a660":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in slag: ', L_outliers)\nprint('Upper outliers in slag: ', U_outliers)","f5c4b6d0":"print('Number of outliers in slag upper : ', concrete_df[concrete_df['slag']>357.375]['slag'].count())\nprint('Number of outliers in slag lower : ', concrete_df[concrete_df['slag']<-214.425]['slag'].count())\nprint('% of Outlier in slag upper: ',round(concrete_df[concrete_df['slag']>357.375]['slag'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in slag lower: ',round(concrete_df[concrete_df['slag']<-214.425]['slag'].count()*100\/len(concrete_df)), '%')","1e13ed37":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='slag',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Slag', fontsize=15)\nax1.set_title('Distribution of slag', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['slag'],ax=ax2)\nax2.set_xlabel('Slag', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Slag vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['slag'])\nax3.set_xlabel('Slag', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Slag vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","289da7e4":"print('Range of values: ', concrete_df['ash'].max()-concrete_df['ash'].min())","24226707":"print('Minimum ash: ', concrete_df['ash'].min())\nprint('Maximum ash: ',concrete_df['ash'].max())\nprint('Mean value: ', concrete_df['ash'].mean())\nprint('Median value: ',concrete_df['ash'].median())\nprint('Standard deviation: ', concrete_df['ash'].std())","a7b35ee9":"Q1=concrete_df['ash'].quantile(q=0.25)\nQ3=concrete_df['ash'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['ash']))","7b4af52e":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in ash: ', L_outliers)\nprint('Upper outliers in ash: ', U_outliers)","824fb7dc":"print('Number of outliers in ash upper : ', concrete_df[concrete_df['ash']>295.75]['ash'].count())\nprint('Number of outliers in ash lower : ', concrete_df[concrete_df['ash']<-177.45]['ash'].count())\nprint('% of Outlier in ash upper: ',round(concrete_df[concrete_df['ash']>295.75]['ash'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in ash lower: ',round(concrete_df[concrete_df['ash']<-177.45]['ash'].count()*100\/len(concrete_df)), '%')","c9e488c2":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='ash',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Ash', fontsize=15)\nax1.set_title('Distribution of ash', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['ash'],ax=ax2)\nax2.set_xlabel('Ash', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Ash vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['ash'])\nax3.set_xlabel('Ash', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Ash vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","efe773ee":"print('Range of values: ', concrete_df['water'].max()-concrete_df['water'].min())","73679126":"print('Minimum water: ', concrete_df['water'].min())\nprint('Maximum water: ',concrete_df['water'].max())\nprint('Mean value: ', concrete_df['water'].mean())\nprint('Median value: ',concrete_df['water'].median())\nprint('Standard deviation: ', concrete_df['water'].std())\nprint('Null values: ',concrete_df['water'].isnull().any())","430a8ac6":"Q1=concrete_df['water'].quantile(q=0.25)\nQ3=concrete_df['water'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['water']))","45129759":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in water: ', L_outliers)\nprint('Upper outliers in water: ', U_outliers)","7e6f8ac0":"print('Number of outliers in water upper : ', concrete_df[concrete_df['water']>232.65]['water'].count())\nprint('Number of outliers in water lower : ', concrete_df[concrete_df['water']<124.25]['water'].count())\nprint('% of Outlier in water upper: ',round(concrete_df[concrete_df['water']>232.65]['water'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in water lower: ',round(concrete_df[concrete_df['water']<124.25]['water'].count()*100\/len(concrete_df)), '%')","b8f4e853":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='water',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Water', fontsize=15)\nax1.set_title('Distribution of water', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['water'],ax=ax2)\nax2.set_xlabel('Water', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Water vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['water'])\nax3.set_xlabel('Water', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Water vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","7eea61cf":"print('Range of values: ', concrete_df['superplastic'].max()-concrete_df['superplastic'].min())","3f8ded29":"print('Minimum superplastic: ', concrete_df['superplastic'].min())\nprint('Maximum superplastic: ',concrete_df['superplastic'].max())\nprint('Mean value: ', concrete_df['superplastic'].mean())\nprint('Median value: ',concrete_df['superplastic'].median())\nprint('Standard deviation: ', concrete_df['superplastic'].std())\nprint('Null values: ',concrete_df['superplastic'].isnull().any())","6d134863":"Q1=concrete_df['superplastic'].quantile(q=0.25)\nQ3=concrete_df['superplastic'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['superplastic']))","7eb17987":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in superplastic: ', L_outliers)\nprint('Upper outliers in superplastic: ', U_outliers)","64f3064d":"print('Number of outliers in superplastic upper : ', concrete_df[concrete_df['superplastic']>25.5]['superplastic'].count())\nprint('Number of outliers in superplastic lower : ', concrete_df[concrete_df['superplastic']<-15.3]['superplastic'].count())\nprint('% of Outlier in superplastic upper: ',round(concrete_df[concrete_df['superplastic']>25.5]['superplastic'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in superplastic lower: ',round(concrete_df[concrete_df['superplastic']<-15.3]['superplastic'].count()*100\/len(concrete_df)), '%')","c0cb1e3f":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='superplastic',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Superplastic', fontsize=15)\nax1.set_title('Distribution of superplastic', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['superplastic'],ax=ax2)\nax2.set_xlabel('Superplastic', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Superplastic vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['superplastic'])\nax3.set_xlabel('Superplastic', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Superplastic vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","54c39e11":"print('Range of values: ', concrete_df['coarseagg'].max()-concrete_df['coarseagg'].min())","c5341ac0":"print('Minimum value: ', concrete_df['coarseagg'].min())\nprint('Maximum value: ',concrete_df['coarseagg'].max())\nprint('Mean value: ', concrete_df['coarseagg'].mean())\nprint('Median value: ',concrete_df['coarseagg'].median())\nprint('Standard deviation: ', concrete_df['coarseagg'].std())\nprint('Null values: ',concrete_df['coarseagg'].isnull().any())","b119f514":"Q1=concrete_df['coarseagg'].quantile(q=0.25)\nQ3=concrete_df['coarseagg'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['coarseagg']))","8388eb93":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in coarseagg: ', L_outliers)\nprint('Upper outliers in coarseagg: ', U_outliers)","8add56b3":"print('Number of outliers in coarseagg upper : ', concrete_df[concrete_df['coarseagg']>1175.5]['coarseagg'].count())\nprint('Number of outliers in coarseagg lower : ', concrete_df[concrete_df['coarseagg']<785.9]['coarseagg'].count())\nprint('% of Outlier in coarseagg upper: ',round(concrete_df[concrete_df['coarseagg']>1175.5]['coarseagg'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in coarseagg lower: ',round(concrete_df[concrete_df['coarseagg']<785.9]['coarseagg'].count()*100\/len(concrete_df)), '%')","2105e414":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='coarseagg',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Coarseagg', fontsize=15)\nax1.set_title('Distribution of coarseagg', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['coarseagg'],ax=ax2)\nax2.set_xlabel('Coarseagg', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Coarseagg vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['coarseagg'])\nax3.set_xlabel('Coarseagg', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Coarseagg vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","044d55bc":"print('Range of values: ', concrete_df['fineagg'].max()-concrete_df['fineagg'].min())","dcffdd36":"print('Minimum value: ', concrete_df['fineagg'].min())\nprint('Maximum value: ',concrete_df['fineagg'].max())\nprint('Mean value: ', concrete_df['fineagg'].mean())\nprint('Median value: ',concrete_df['fineagg'].median())\nprint('Standard deviation: ', concrete_df['fineagg'].std())\nprint('Null values: ',concrete_df['fineagg'].isnull().any())","932da1dd":"Q1=concrete_df['fineagg'].quantile(q=0.25)\nQ3=concrete_df['fineagg'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['fineagg']))","7fd0ec63":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in fineagg: ', L_outliers)\nprint('Upper outliers in fineagg: ', U_outliers)","a8324794":"print('Number of outliers in fineagg upper : ', concrete_df[concrete_df['fineagg']>963.575]['fineagg'].count())\nprint('Number of outliers in fineagg lower : ', concrete_df[concrete_df['fineagg']<591.37]['fineagg'].count())\nprint('% of Outlier in fineagg upper: ',round(concrete_df[concrete_df['fineagg']>963.575]['fineagg'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in fineagg lower: ',round(concrete_df[concrete_df['fineagg']<591.37]['fineagg'].count()*100\/len(concrete_df)), '%')","9eeadf4d":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='fineagg',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Fineagg', fontsize=15)\nax1.set_title('Distribution of fineagg', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['fineagg'],ax=ax2)\nax2.set_xlabel('Fineagg', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Fineagg vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['fineagg'])\nax3.set_xlabel('Fineagg', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Fineagg vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","0c8f5df5":"print('Range of values: ', concrete_df['age'].max()-concrete_df['age'].min())","4a787f7d":"print('Minimum age: ', concrete_df['age'].min())\nprint('Maximum age: ',concrete_df['age'].max())\nprint('Mean value: ', concrete_df['age'].mean())\nprint('Median value: ',concrete_df['age'].median())\nprint('Standard deviation: ', concrete_df['age'].std())\nprint('Null values: ',concrete_df['age'].isnull().any())","3963a36f":"Q1=concrete_df['age'].quantile(q=0.25)\nQ3=concrete_df['age'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['age']))","02baddd3":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in age: ', L_outliers)\nprint('Upper outliers in age: ', U_outliers)","52cb0910":"print('Number of outliers in age upper : ', concrete_df[concrete_df['age']>129.5]['age'].count())\nprint('Number of outliers in age lower : ', concrete_df[concrete_df['age']<-66.5]['age'].count())\nprint('% of Outlier in age upper: ',round(concrete_df[concrete_df['age']>129.5]['age'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in age lower: ',round(concrete_df[concrete_df['age']<-66.5]['age'].count()*100\/len(concrete_df)), '%')","9b171551":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='age',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Age', fontsize=15)\nax1.set_title('Distribution of age', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['age'],ax=ax2)\nax2.set_xlabel('Age', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Age vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['age'])\nax3.set_xlabel('Age', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Age vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout() ","e3158597":"# Distplot\nfig, ax2 = plt.subplots(3, 3, figsize=(16, 16))\nsns.distplot(concrete_df['cement'],ax=ax2[0][0])\nsns.distplot(concrete_df['slag'],ax=ax2[0][1])\nsns.distplot(concrete_df['ash'],ax=ax2[0][2])\nsns.distplot(concrete_df['water'],ax=ax2[1][0])\nsns.distplot(concrete_df['superplastic'],ax=ax2[1][1])\nsns.distplot(concrete_df['coarseagg'],ax=ax2[1][2])\nsns.distplot(concrete_df['fineagg'],ax=ax2[2][0])\nsns.distplot(concrete_df['age'],ax=ax2[2][1])\nsns.distplot(concrete_df['strength'],ax=ax2[2][2])","0a55e540":"# Histogram \nconcrete_df.hist(figsize=(15,15))","c08adb20":"# pairplot- plot density curve instead of histogram in diagonal\nsns.pairplot(concrete_df, diag_kind='kde')  ","375fcfe0":"# corrlation matrix \ncor=concrete_df.corr()\ncor","0acc271b":"#heatmap\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\n\nsns.heatmap(cor, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap=\"BuPu\",linecolor=\"black\")\nplt.title('Correlation between features');","0f37ea69":"# water vs cement\n#lm plot\nsns.lmplot(x=\"cement\",y=\"water\",data=concrete_df)\nplt.show()","3472e971":"#Checking for missing values\nconcrete_df.isnull().sum()","77753d60":"#Creating copy of original dataset\nconcrete_df1=concrete_df.copy()","e1c87516":"# again check for outliers in dataset after handling missing values using boxplot\nconcrete_df1.boxplot(figsize=(35,15))","c444cd2b":"#Number of outliers present in the dataset\nprint('Number of outliers in cement: ',concrete_df1[((concrete_df1.cement - concrete_df1.cement.mean()) \/ concrete_df1.cement.std()).abs() >3]['cement'].count())\nprint('Number of outliers in slag: ',concrete_df1[((concrete_df1.slag - concrete_df1.slag.mean()) \/ concrete_df1.slag.std()).abs() >3]['slag'].count())\nprint('Number of outliers in ash: ',concrete_df1[((concrete_df1.ash - concrete_df1.ash.mean()) \/ concrete_df1.ash.std()).abs() >3]['ash'].count())\nprint('Number of outliers in water: ',concrete_df1[((concrete_df1.water - concrete_df1.water.mean()) \/ concrete_df1.water.std()).abs() >3]['water'].count())\nprint('Number of outliers in superplastic: ',concrete_df1[((concrete_df1.superplastic - concrete_df1.superplastic.mean()) \/ concrete_df1.superplastic.std()).abs() >3]['superplastic'].count())\nprint('Number of outliers in coarseagg: ',concrete_df1[((concrete_df1.coarseagg - concrete_df1.coarseagg.mean()) \/ concrete_df1.coarseagg.std()).abs() >3]['coarseagg'].count())\nprint('Number of outliers in fineagg: ',concrete_df1[((concrete_df1.fineagg - concrete_df1.fineagg.mean()) \/ concrete_df1.fineagg.std()).abs() >3]['fineagg'].count())\nprint('Number of outliers in age: ',concrete_df1[((concrete_df1.age - concrete_df1.age.mean()) \/ concrete_df1.age.std()).abs() >3]['age'].count())","d8c925b3":"#Records which contains the outliers in slag attribute\nprint('Records containing outliers in slag: \\n',concrete_df1[((concrete_df1.slag - concrete_df1.slag.mean()) \/ concrete_df1.slag.std()).abs() >3]['slag'])","4ef8ae2e":"#Records which contains the outliers in water attribute\nprint('Records containing outliers in water: \\n',concrete_df1[((concrete_df1.water - concrete_df1.water.mean()) \/ concrete_df1.water.std()).abs() >3]['water'])","4c3da7ff":"#Records which contains the outliers in superplastic attribute\nprint('Records containing outliers in superplastic: \\n',concrete_df1[((concrete_df1.superplastic - concrete_df1.superplastic.mean()) \/ concrete_df1.superplastic.std()).abs() >3]['superplastic'])","1a4d053c":"#Records which contains the outliers in age attribute\nprint('Records containing outliers in age: \\n',concrete_df1[((concrete_df1.age - concrete_df1.age.mean()) \/ concrete_df1.age.std()).abs() >3]['age'])","d4d74859":"#Replacing the outliers by median\nfor col_name in concrete_df1.columns[:-1]:\n    q1 = concrete_df1[col_name].quantile(0.25)\n    q3 = concrete_df1[col_name].quantile(0.75)\n    iqr = q3 - q1\n    \n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    concrete_df1.loc[(concrete_df1[col_name] < low) | (concrete_df1[col_name] > high), col_name] = concrete_df1[col_name].median()","0796ed74":"# again check for outliers in dataset using boxplot\nconcrete_df1.boxplot(figsize=(35,15))","3587af9d":"#Scaling the dataset\nconcrete_df_z = concrete_df1.apply(zscore)\nconcrete_df_z=pd.DataFrame(concrete_df_z,columns=concrete_df.columns)","eb2d0e18":"#independent and dependent variables\nX=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]","c1c72a9f":"# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","b01a818d":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)","5e80d651":"#printing the feature importance\nprint('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns))","b4668a31":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","3d0a5cd4":"from scipy.stats import pearsonr\nsns.set(style=\"darkgrid\", color_codes=True)   \nwith sns.axes_style(\"white\"):\n    sns.jointplot(x=y_test, y=y_pred, stat_func=pearsonr,kind=\"reg\", color=\"k\");","b4150a79":"#Store the accuracy results for each model in a dataframe for final comparison\nresults = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT},index={'1'})\nresults = results[['Method', 'accuracy']]\nresults","c034c524":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(dt_model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","f2d6163c":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Decision Tree k fold'], 'accuracy': [accuracy]},index={'2'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","6eabd7f6":"concrete_df_z.info()","299295e4":"#Create a copy of the dataset\nconcrete_df2=concrete_df_z.copy()","ec527361":"#independent and dependent variable\nX = concrete_df2.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\ny = concrete_df2['strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","9da23d98":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)","e8fd3fd9":"#printing the feature importance\nprint('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns))","8bd52330":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)","56705a8e":"from scipy.stats import pearsonr\nsns.set(style=\"darkgrid\", color_codes=True)   \nwith sns.axes_style(\"white\"):\n    sns.jointplot(x=y_test, y=y_pred, stat_func=pearsonr,kind=\"reg\", color=\"k\");","9a33e8a8":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Decision Tree2'], 'accuracy': [acc_DT]},index={'3'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults\n","e6c4c3d2":"#independent and dependent variables\nX=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","d7874e4d":"# Regularizing the Decision tree classifier and fitting the model\nreg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\nreg_dt_model.fit(X_train, y_train)","77867dca":"print (pd.DataFrame(reg_dt_model.feature_importances_, columns = [\"Imp\"], index = X_train.columns))","d7a1c83d":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\nimport graphviz\nbank_df=concrete_df_z\nxvar = bank_df.drop('strength', axis=1)\nfeature_cols = xvar.columns","14f5892f":"dot_data = StringIO()\nexport_graphviz(reg_dt_model, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('concrete_pruned.png')\nImage(graph.create_png())","e5841ff1":"y_pred = reg_dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RDT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","8f9e4190":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree'], 'accuracy': [acc_RDT]},index={'4'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","0bf17333":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(reg_dt_model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","09b654db":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree k fold'], 'accuracy': [accuracy]},index={'5'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","e47deb65":"#Create a copy of the dataset\nconcrete_df3=concrete_df_z.copy()","06e44069":"#independent and dependent variable\nX = concrete_df3.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\ny = concrete_df3['strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","f1dcfd93":"# Regularizing the Decision tree classifier and fitting the model\nreg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\nreg_dt_model.fit(X_train, y_train)","68433fd5":"y_pred = reg_dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RDT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","e1d806d5":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree2'], 'accuracy': [acc_RDT]},index={'6'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","b5765d6d":"cluster_range = range( 1, 15 )  \ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(concrete_df1)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","8e5ee89c":"# Elbow plot\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","b2e34b6b":"# k=6\ncluster = KMeans( n_clusters = 6, random_state = 2354 )\ncluster.fit(concrete_df_z)","419b8d4f":"# Creating a new column \"GROUP\" which will hold the cluster id of each record\nprediction=cluster.predict(concrete_df_z)\nconcrete_df_z[\"GROUP\"] = prediction     \n# Creating a mirror copy for later re-use instead of building repeatedly\nconcrete_df_z_copy = concrete_df_z.copy(deep = True)  ","d4aa0f5e":"centroids = cluster.cluster_centers_\ncentroids","ec9c898e":"centroid_df = pd.DataFrame(centroids, columns = list(concrete_df1) )\ncentroid_df","170b1cbb":"## Instead of interpreting the neumerical values of the centroids, let us do a visual analysis by converting the \n## centroids and the data in the cluster into box plots.\nimport matplotlib.pylab as plt\nconcrete_df_z.boxplot(by = 'GROUP',  layout=(3,3), figsize=(15, 10))","81150cdd":"#independent and dependent variables\nX=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","b5d9c40f":"model=RandomForestRegressor()\nmodel.fit(X_train, y_train)","25fc1c4f":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using RFR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using RFR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RFR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RFR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","c0679879":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor'], 'accuracy': [acc_RFR]},index={'7'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","0efa3551":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","b35600c9":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor k fold'], 'accuracy': [accuracy]},index={'8'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","2263ef7d":"model=GradientBoostingRegressor()\nmodel.fit(X_train, y_train)","1488f9d5":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_GBR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_GBR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","6a3520a5":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor'], 'accuracy': [acc_GBR]},index={'9'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","a008386d":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","1c310222":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor k fold'], 'accuracy': [accuracy]},index={'10'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","e0783254":"model=AdaBoostRegressor()\nmodel.fit(X_train, y_train)","58b606c1":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_ABR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_ABR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","9a1f81b8":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Ada Boosting Regressor'], 'accuracy': [acc_ABR]},index={'11'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","37bd59da":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","07ca0583":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Ada Boosting Regressor k fold'], 'accuracy': [accuracy]},index={'12'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","77ce7d94":"model=BaggingRegressor()\nmodel.fit(X_train, y_train)","d4f87f36":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_BR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_BR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","e9c863bc":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Bagging Regressor'], 'accuracy': [acc_BR]},index={'13'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","b731f7cf":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","6db3d172":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Bagging Regressor k fold'], 'accuracy': [accuracy]},index={'14'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","7e115e02":"error=[]\nfor i in range(1,30):\n    knn = KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i!=y_test))","e000a822":"plt.figure(figsize=(12,6))\nplt.plot(range(1,30),error,color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","49f2a23d":"#k=3\nmodel = KNeighborsRegressor(n_neighbors=3)\nmodel.fit(X_train, y_train)","d44e81e3":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using KNNR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using KNNR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_K=metrics.r2_score(y_test, y_pred)\nprint('Accuracy KNNR: ',acc_K)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","09508c0a":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['KNN Regressor'], 'accuracy': [acc_K]},index={'15'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","93319244":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","6ab63f3e":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['KNN Regressor k fold'], 'accuracy': [accuracy]},index={'16'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","72505f5b":"model = SVR(kernel='linear')\nmodel.fit(X_train, y_train)","ac2d267b":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using SVR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using SVR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_S=metrics.r2_score(y_test, y_pred)\nprint('Accuracy SVR: ',acc_S)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","e954e560":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Support Vector Regressor'], 'accuracy': [acc_S]},index={'17'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","03ca2266":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","4f0e364a":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['SVR k fold'], 'accuracy': [accuracy]},index={'18'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","ad8fa6e2":"#Multiple model Ensemble\nfrom sklearn import svm\nLR=LinearRegression()\nKN=KNeighborsRegressor(n_neighbors=3)\nSVM=svm.SVR(kernel='linear') ","76294683":"evc=VotingRegressor(estimators=[('LR',LR),('KN',KN),('SVM',SVM)])\nevc.fit(X_train, y_train)","237516dd":"y_pred = evc.predict(X_test)\n# performance on train data\nprint('Performance on training data using ensemble:',evc.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using ensemble:',evc.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_E=metrics.r2_score(y_test, y_pred)\nprint('Accuracy ensemble: ',acc_E)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","9f57f434":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Ensemble'], 'accuracy': [acc_E]},index={'19'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","f97093bc":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(evc,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","7bd7c3eb":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Ensemble k fold'], 'accuracy': [accuracy]},index={'20'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","8ab9bc78":"concrete_XY = X.join(y)","5558f9aa":"values = concrete_XY.values\n# Number of bootstrap samples to create\nn_iterations = 1000        \n# size of a bootstrap sample\nn_size = int(len(concrete_df_z) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nstats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    gbmTree = GradientBoostingRegressor(n_estimators=50)\n    # fit against independent variables and corresponding target values\n    gbmTree.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbmTree.score(test[:, :-1] , y_test)\n    predictions = gbmTree.predict(test[:, :-1])  \n\n    stats.append(score)","887687c3":"# plot scores\n\nfrom matplotlib import pyplot\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","9ee9b48c":"values = concrete_XY.values\n# Number of bootstrap samples to create\nn_iterations = 1000        \n# size of a bootstrap sample\nn_size = int(len(concrete_df_z) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nstats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    rfTree = RandomForestRegressor(n_estimators=100)\n    # fit against independent variables and corresponding target values\n    rfTree.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = rfTree.score(test[:, :-1] , y_test)\n    predictions = rfTree.predict(test[:, :-1])  \n\n    stats.append(score)","f9caf5c2":"# plot scores\n\nfrom matplotlib import pyplot\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","39b07c16":"#### Outlier detection from Interquartile range (IQR) in original data","7e9319ee":"## Superplastic\n ","1c742486":"* Here, we have used Standard deviation method to detect the outliers.If we have any data point that is more than 3 times the standard deviation, then those points are very likely to be outliers.\n* We can see that slag, water, superplastic and age contain outliers.","708d5470":"## Cement","3ae999a1":"# K fold cross validation","9c4cccb5":"### Central values","4fcecdc8":"# K fold cross validation","32b071cd":"# Feature Engineering, Model Building and Model Tuning","cfc35b3a":"### Quartiles","bfd4a2e5":"# Gradient Boosting Regressor","c063ab2d":"# K fold cross validation","daf47b80":"### Range of values observed","a246f722":"# Description of independent attributes","db0ea819":"# Drop the least significant variable","8b500e57":"# Iteration 2","785567e1":"## Splitting the data into independent and dependent attributes","f93b5c25":"Data Types and Description","d0a7eeb8":"* So, cement, age and water are significant attributes.\n* Here, ash, coarseagg, fineagg, superplastic and slag are the less significant variable.These will impact less to the strength column. This we have seen in pairplot also.","4a0cf855":"# Strategies to handle different data challenges","6ef13bd2":"#### Central values","759a100c":"# K fold cross validation","7781dc92":"# K fold cross validation","3d64064b":"# K Means Clustering","f72df44b":"# K fold cross validation","425588ba":"### Quartiles","e4e9cccb":"# Exploratory data quality report","731df835":"# Random Forest Regressor","bb88afa9":"# Fineagg","e41e26db":"### Quartiles","58ff9efb":"**Data Description:**\n\nThe actual concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory. Data is in raw form (not scaled). The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations).\n\n**Domain:**\n\nCement manufacturing\n\n\n**Context:**\n\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\nAttribute Information:\n\uf0b7 Cement : measured in kg in a m3 mixture\n\uf0b7 Blast : measured in kg in a m3 mixture\n\uf0b7 Fly ash : measured in kg in a m3 mixture\n\uf0b7 Water : measured in kg in a m3 mixture\n\uf0b7 Superplasticizer : measured in kg in a m3 mixture\n\uf0b7 Coarse Aggregate : measured in kg in a m3 mixture\n\uf0b7 Fine Aggregate : measured in kg in a m3 mixture\n\uf0b7 Age : day (1~365)\n\uf0b7 Concrete compressive strength measured in MPa\n\n**Objective:**\n\nModeling of strength of high performance concrete using Machine Learning","3dc563f5":"# Ada Boosting Regressor","46086481":"#### Outlier detection from Interquartile range (IQR) in original data","6bfd54f1":"The bootstrap random forest  classification model performance is between 84%-90.8% which is better than other classification algorithms.","670b8240":"### Range of values observed","599b9339":"# Checking for Missing Values","69dc945b":"# K fold cross validation","bd3dae30":"#### Range of values observed","18e88766":"### Central values","0bf3eeed":"# Age","fef635a3":"# Support Vector Regressor","2d3442a4":"# Coarseagg","dc86a386":"# KNN Regressor","77851e5e":"* The acuracy on testing dataset is not improved, still it is an overfit model.","048ff08f":"# Handling the outliers","1da67fe6":"# Bagging Regressor","db800379":"### Central values","272da260":"### Outlier detection from Interquartile range (IQR) in original data","2ccaa012":"* After applying all the models we can see that Random Forest Regressor, Random Forest Regressor k fold, Gradient Boost Regressor, Gradient Boost Regressor k fold, Bagging Regressor are giving better results as compared to other models.\n* Now as the dataset have different gaussians, we can apply k means clustering and then we can apply the models and compare the accuracy.","5a91cc44":"# Checking for outliers","e72a9ccf":"# Bootstrap Sampling","0a8ba7b9":"### Quartiles","d5efdff1":"### Quartiles","7e8ffa7d":"#### Range of values observed ","dc9ac41e":"### Outlier detection from Interquartile range (IQR) in original data","e87ca9da":"### Quartiles","60adf4bf":"### Scaling the features","db3b2ceb":"### Outlier detection from Interquartile range (IQR) in original data","a7f0674d":"# K fold cross validation","2a1c286d":"# K fold cross validation","76f8d9e1":"##### Central values ","53d59e39":"* Here, None of the dimensions are good predictor of target variable.\n* For all the dimensions (variables) every cluster have a similar range of values except in one case.\n* We can see that the body of the cluster are overlapping.\n* So in k means, though, there are clusters in datasets on different dimensions. But we can not see any distinct characteristics of these clusters which tell us to break data into different clusters and build separate models for them.","d6325d5c":"* There is a overfitting in the model as the dataset is performing 99% accurately in trainnig data. However, the accuracy on test data drops.","89739f16":"# Ensemeble KNN Regressor, SVR, LR","4c6743af":"# Regularising\/Pruning of Decision Tree","f3339a4f":"### Central values\n","8a32bf06":"# Water","a6bde1b4":"* It is also giving the same information we observed in pairplot analysis. \n* water shows significant negative relationship with superplastic and fineagg. It also shows some kind of positive relationship with slag and age.\n","3d1ffdce":"# Quartiles","c50ebe58":"# We can see observe that :\n- cement is almost normal. \n- slag has  three gausssians and rightly skewed.\n- ash has two gaussians and rightly skewed.\n- water has three guassians and slighly left skewed.\n- superplastic has two gaussians and rightly skewed.\n- coarseagg has three guassians and almost normal.\n- fineagg has almost two guassians and looks like normal.\n- age has multiple guassians and rightly skewed.","ea07b47a":"#### Quartiles","ebc134da":"### Outlier detection from Interquartile range (IQR) in original data","6e382098":"# Using Gradient Boosting Regressor","9d79f502":"# Iteration2","a16790a9":"# Univariate analysis","3ca0b403":"## Splitting the data into three sets","658337e0":"#### Range of values observed","1cc51683":"### Range of values observed","dd1b5581":"#### Central values\n","e464ef10":"### Slag","5a97c92c":"# Import the Libraries","7a6c6fbb":"# Using Random Forest Regressor","8ff172cd":"### Range of values observed","6c0063af":"### Outlier detection from Interquartile range (IQR) in original data","cff913e5":"#### Outlier detection from Interquartile range (IQR) in original data ","adc53458":"# Ash","416337dc":"### Range of values observed","448e3d35":"# Visualizing the Regularized Tree","471f0980":"# Multivariate Analysis","fb5f9ce2":"# DecisionTree Regression","96d12fdb":"* Here, we can see that ash,coarseagg and fineagg are least significant variable."}}