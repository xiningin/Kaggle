{"cell_type":{"2621684d":"code","6df8c2f9":"code","24a1e6fb":"code","13992310":"code","070aaa8b":"code","eaa60c05":"code","d892665e":"code","e96e0b41":"code","7e57d8d5":"code","48a86db5":"code","7f4a418f":"code","d69a5723":"code","f6bfd635":"code","f82f08f2":"code","58234f7d":"code","1fd18f27":"code","bfb1102e":"code","6cdb3159":"code","c6380edd":"code","affc7914":"markdown","cd0b2712":"markdown","153ecc4e":"markdown","12671945":"markdown","54478e34":"markdown","f3f5fa07":"markdown","f5fac95f":"markdown","2e67f3fe":"markdown","52bc70b3":"markdown","368bc523":"markdown","4c274a47":"markdown","6c645c38":"markdown","474168e1":"markdown","6fd47121":"markdown","d54cb727":"markdown","724ae0e8":"markdown","df596be6":"markdown","0d52e961":"markdown"},"source":{"2621684d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nsns.set_style(\"darkgrid\")\n%matplotlib inline\n\n\nfrom sklearn.cluster import KMeans\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport optuna\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","6df8c2f9":"X_full = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv', index_col='id')\nX_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv', index_col='id')","24a1e6fb":"target = \"target\"\nX_full.dropna(axis=0, subset=[target], inplace=True)\ny_full = X_full.pop(target)","13992310":"cat_features = [col for col in X_full.columns if X_full[col].dtype == \"object\"]","070aaa8b":"num_features = [col for col in X_full.columns if X_full[col].dtype in [\"int\", \"float\"]]","eaa60c05":"nc = 4\nnr = int(len(num_features)\/nc+1)\n\nfig, axes = plt.subplots(nrows=nr, ncols=nc, figsize=(18,4*nr))\n\nfor count, feature in enumerate(num_features):\n    ks_score = stats.ks_2samp(X_full[feature], X_test[feature])[0]\n    i, j = count\/\/nc, count%nc\n    sns.kdeplot(X_full[feature], color='Blue', ax=axes[i, j])\n    sns.kdeplot(X_test[feature], color='Red', ax=axes[i, j])\n\n    axes[i, j].legend([\"Train\", \"Test\"], facecolor=\"White\")\n    axes[i, j].set_title(f\"{feature} ks stat : {np.round(ks_score,3)}\")\n\nplt.tight_layout()","d892665e":"def loo_encode(X_full, X_test, column):\n    loo = LeaveOneOutEncoder()\n    new_feature = f\"{column}_loo\"\n    loo.fit(X_full[column], y_full)\n    X_full[new_feature] = loo.transform(X_full[column])\n    X_test[new_feature] = loo.transform(X_test[column])\n    return new_feature\n\nloo_features = []\nfor feature in cat_features:\n    loo_features.append(loo_encode(X_full, X_test, feature))","e96e0b41":"nc = 4\nnr = int(len(loo_features)\/nc+1)\n\nfig, axes = plt.subplots(nrows=nr, ncols=nc, figsize=(18,4*nr))\n\nfor count, feature in enumerate(loo_features):\n    ks_score = stats.ks_2samp(X_full[feature], X_test[feature])[0]\n    i, j = count\/\/nc, count%nc\n    sns.kdeplot(X_full[feature], color='Blue', ax=axes[i, j])\n    sns.kdeplot(X_test[feature], color='Red', ax=axes[i, j])\n\n    axes[i, j].legend([\"Train\", \"Test\"], facecolor=\"White\")\n    axes[i, j].set_title(f\"{feature} ks stat : {np.round(ks_score,3)}\")\n\nplt.tight_layout()","7e57d8d5":"def label_encode(X_full, X_test, column):\n    le = LabelEncoder()\n    new_feature = f\"{column}_le\"\n    le.fit(X_full[column])\n    le.fit(X_full[column].unique().tolist() + X_test[column].unique().tolist())\n    X_full[new_feature] = le.transform(X_full[column])\n    X_test[new_feature] = le.transform(X_test[column])\n    return new_feature\n\nle_list = ['cat16']\nle_features = []\nfor feature in le_list:\n    le_features.append(label_encode(X_full, X_test, feature))","48a86db5":"clusters = [ \n    (\"cat16_loo\", 2)\n]\n\nkmeans_features = []\nfor var in clusters:\n    kmeans = KMeans(n_clusters=var[1])\n    X_full[f\"{var[0]}_kmeans\"] = kmeans.fit_predict( np.array(X_full[var[0]]).reshape(-1, 1) )\n    X_test[f\"{var[0]}_kmeans\"] = kmeans.predict( np.array(X_test[var[0]]).reshape(-1, 1) )\n    kmeans_features.append(f\"{var[0]}_kmeans\")","7f4a418f":"my_features = num_features + loo_features + le_features + kmeans_features","d69a5723":"def objective(trial, data=X_full[my_features], target=y_full):\n    seed = 2021\n    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n\n    for train_index, valid_index in split.split(X_full[my_features], y_full):\n        X_train = X_full[my_features].iloc[train_index]\n        y_train = y_full.iloc[train_index]\n        X_valid = X_full[my_features].iloc[valid_index]\n        y_valid = y_full.iloc[valid_index]\n\n\n    lgbm_params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 11, 333),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.01, 0.02, 0.05, 0.1]),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 5000),\n        'random_state': seed,\n        'boosting_type': 'gbdt',\n        'metric': 'AUC',\n        #'device': 'gpu'\n    }\n    \n\n    model = LGBMClassifier(**lgbm_params)  \n    \n    model.fit(\n            X_train,\n            y_train,\n            early_stopping_rounds=100,\n            eval_set=[(X_valid, y_valid)],\n            verbose=False\n        )\n\n    y_valid_pred = model.predict_proba(X_valid)[:,1]\n    \n    roc_auc = roc_auc_score(y_valid, y_valid_pred)\n    \n    return roc_auc","f6bfd635":"#study = optuna.create_study(direction = 'maximize')\n#study.optimize(objective, n_trials = 10)\n#print('Number of finished trials:', len(study.trials))\n#print('Best trial:', study.best_trial.params)\n#print('Best value:', study.best_value)","f82f08f2":"#optuna.visualization.plot_optimization_history(study)","58234f7d":"#optuna.visualization.plot_param_importances(study)","1fd18f27":"seed = 2021\n#paramsLGBM = study.best_trial.params\nparamsLGBM = {'reg_alpha': 1.9553269755200153, \n              'reg_lambda': 6.667487742284949, \n              'num_leaves': 173, \n              'min_child_samples': 86, \n              'max_depth': 23, \n              'learning_rate': 0.01, \n              'colsample_bytree': 0.15433885172555964, \n              'n_estimators': 3473}\nparamsLGBM['boosting_type'] = 'gbdt'\nparamsLGBM['metric'] = 'AUC'\nparamsLGBM['random_state'] = seed\n\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\nfor train_index, valid_index in split.split(X_full[my_features], y_full):\n    X_train = X_full[my_features].iloc[train_index]\n    y_train = y_full.iloc[train_index]\n    X_valid = X_full[my_features].iloc[valid_index]\n    y_valid = y_full.iloc[valid_index]\n\n\nlgbm_clf = LGBMClassifier(**paramsLGBM)\nlgbm_clf.fit(X_train[my_features], y_train, \n             early_stopping_rounds=100, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","bfb1102e":"#perm = PermutationImportance(lgbm_clf, random_state=seed).fit(X_valid, y_valid)\n#eli5.show_weights(perm, feature_names = X_valid.columns.tolist())","6cdb3159":"test_preds = lgbm_clf.predict_proba(X_test[my_features])[:,1]","c6380edd":"output = pd.DataFrame({'Id': X_test.index,\n                       target: test_preds})\noutput.to_csv('submission.csv', index=False)","affc7914":"# Encode categorical features","cd0b2712":"# Optuna visualization","153ecc4e":"# Categorical features","12671945":"# Specify all features to use","54478e34":"# Make some EDA plots","f3f5fa07":"# Make predictions","f5fac95f":"This notebook is a combination of different ideas I have learnt from:\n* https:\/\/www.kaggle.com\/craigmthomas\/tps-mar-2021-stacked-starter\/comments\n* https:\/\/www.kaggle.com\/dmitryuarov\/catboost-vs-xgb-vs-lgbm-tps-mar-21\n* And many other Kagglers","2e67f3fe":"# K-cluster 'cat16_loo'","52bc70b3":"# Save predictions to file","368bc523":"# Label encode 'cat16'","4c274a47":"# Numerical features","6c645c38":"# Fit model with Optuna best parameters","474168e1":"Also, the distribution is quite even between train and test datasets for the leave-one-out encoded categorical features.","6fd47121":"# Specify target","d54cb727":"As can be seen, the distribution is quite even between train and test datasets for the numerical features. ","724ae0e8":"# Load data","df596be6":"# Optuna hyperparameter optimization","0d52e961":"# Acknowledgement"}}