{"cell_type":{"3a8d1a1a":"code","a5b080dd":"code","8b64e824":"code","92d1b2df":"code","6e0cc168":"code","1bf34981":"code","78d1642f":"code","54a021d4":"code","d05957ac":"code","2ab8460c":"code","2e17ce4c":"code","a4a73572":"code","231abf0a":"code","4abd69f3":"code","789bc59b":"code","ee4ba40c":"code","83286e2f":"code","5fd830d0":"code","213c6ec2":"code","0551beb5":"code","8fd7afdf":"code","d0d3117c":"code","c996a40b":"code","dd5abd0d":"markdown","f9ce7518":"markdown","1d472afd":"markdown","8d9713e6":"markdown","cae8c5b0":"markdown","dc1cec6b":"markdown","261b6944":"markdown"},"source":{"3a8d1a1a":"import random\nimport re\nimport os\nimport string\n\nimport unidecode\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom pprint import pprint\n\nrandom.seed(0)\nnp.random.seed(0)\ntf.random.set_seed(0)\n\npd.set_option('display.max_colwidth', None)","a5b080dd":"%%time\ndtype = {    \n    'title': 'category',\n    'label_quality': 'category',\n    'category': 'category',    \n}\n\ntrain_data = pd.read_csv('..\/input\/i2a2-nlp-2021-text-classification\/train.csv', \n                         usecols=['title', 'label_quality', 'category'],\n                         dtype=dtype)\n\ntest_data = pd.read_csv('..\/input\/i2a2-nlp-2021-text-classification\/test.csv', \n                        usecols=['id', 'title'], \n                        dtype={'title': 'category'})","8b64e824":"train_data.head()","92d1b2df":"test_data.head()","6e0cc168":"train_data['label_quality'].value_counts(dropna=False)","1bf34981":"train_data = train_data[train_data['label_quality'] == 'reliable'].copy(deep=True)\ntrain_data.reset_index(drop=True, inplace=True)\n# train_data = train_data.sample(frac=0.01, random_state=0)\ntrain_data.shape","78d1642f":"remove_num = re.compile(r'\\d+')\nremove_extra_space = re.compile(r'\\s+')\nremove_puct = re.compile(r'[^\\w\\s]')\nremove_unique = re.compile(r'\\b\\w\\b')\n\ndef preprocess_text(text):\n    \n    text = text.lower()\n    text = remove_num.sub(' ', text)\n    text = remove_puct.sub(' ', text)\n    text = remove_unique.sub(' ', text)\n    \n    # Convert accented characters to their ASCII values\n    text = unidecode.unidecode(text)\n    \n    text = remove_extra_space.sub(' ', text)\n    \n    return text","54a021d4":"%%time\ntrain_data['title'] = train_data['title'].apply(preprocess_text)\ntest_data['title'] = test_data['title'].apply(preprocess_text)","d05957ac":"train_data.head(20)","2ab8460c":"test_data.head()","2e17ce4c":"corpus = ' '.join([*train_data['title']])\nvocab = nltk.FreqDist(TweetTokenizer().tokenize(corpus))\n\nprint(f'The training data has {len(vocab):,} unique words.', '---' , sep='\\n', end='\\n\\n')\n\n# Show the 100 most common words\npprint(vocab.most_common(100),  compact=True)","a4a73572":"train_data.isna().sum()","231abf0a":"sentence_len = [len(sentence.split()) for sentence in train_data['title']]\nbins = [*set(sentence_len)]\n\n_ = sns.histplot(sentence_len, bins=bins)","4abd69f3":"tokenize_params = {\n    'tokenizer': {'oov_token': '<UNK>'},\n    'pad_sequences': {'maxlen': 16, 'padding': 'post', 'truncating': 'post'},\n}\n\n\ntokenize = Tokenizer(**tokenize_params['tokenizer'])\ntokenize.fit_on_texts(train_data['title'])\n\nword_index = tokenize.word_index\n\n# Transform text to sequences\nX_train = tokenize.texts_to_sequences(train_data['title'])\nX_test = tokenize.texts_to_sequences(test_data['title'])\n\n# Pad sequences\nX_train = pad_sequences(X_train, **tokenize_params['pad_sequences'])\nX_test = pad_sequences(X_test, **tokenize_params['pad_sequences'])","789bc59b":"le = LabelEncoder()\n\ny_train = le.fit_transform(train_data['category'])\n\ntarget_names = le.classes_\n\nprint(y_train)\nprint(target_names[:10])","ee4ba40c":"## Sanity check\nX_train.shape, y_train.shape, X_test.shape","83286e2f":"tf.keras.backend.clear_session()\n\n# Model parameters\nnum_classes = train_data['category'].nunique()\nvocab_size = len(word_index) + 1 \nembedding_dim = 100\n\n\n# The classifier model\nmodel = tf.keras.models.Sequential([\n    \n    tf.keras.layers.Embedding(vocab_size,\n                              embedding_dim,\n                              input_length=X_train.shape[1]),\n\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embedding_dim)),\n    \n    tf.keras.layers.Dense(num_classes, activation='softmax'),\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","5fd830d0":"%%time\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)\n]\n\nhistory = model.fit(X_train,\n                    y_train,\n                    batch_size=128,\n                    epochs=10,\n                    callbacks=callbacks,\n                    validation_split=0.1,\n                    workers=os.cpu_count(), \n                    use_multiprocessing=True);","213c6ec2":"hist = pd.DataFrame(history.history)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.flat\n\nfor ii, metric in enumerate(['loss', 'accuracy']):\n    sns.lineplot(x=hist.index, y=f'{metric}', data=hist, label=f'Training {metric}', ax=ax[ii])\n    sns.lineplot(x=hist.index, y=f'val_{metric}', data=hist, label=f'Validation {metric}', ax=ax[ii])\n    ax[ii].legend(frameon=False)\n\nsns.despine()\nplt.tight_layout()","0551beb5":"model.load_weights('.\/best_model.h5')","8fd7afdf":"preds = model.predict(X_test)\npreds = le.inverse_transform(preds.argmax(axis=1))\n\nprint(preds)","d0d3117c":"test_data['category'] = preds\ntest_data.head()","c996a40b":"test_data[['id', 'category']].to_csv('submission03.csv', index=False)","dd5abd0d":"## Preparing the Submission","f9ce7518":"## Preprocessing Raw Text Data","1d472afd":"## Training and Evaluating the Model","8d9713e6":"## Plotting Results","cae8c5b0":"## Loading Useful Libraries and Modules","dc1cec6b":"## Preparing Data to Feed the Model","261b6944":"## Loading Raw Text Data"}}