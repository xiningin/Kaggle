{"cell_type":{"756e9173":"code","80563b6a":"code","6d25e841":"code","113d1c37":"code","43c7032d":"code","843af14f":"code","e70a3b08":"code","b3804edd":"code","a7da1b3b":"code","e14bd845":"code","c33f824b":"code","613218db":"code","96fded4d":"code","13e5e636":"code","4527d198":"code","c98ff865":"code","fc5efdd8":"code","42db6c34":"code","2308ecbf":"code","396900be":"code","6fcc964e":"code","03bcae90":"code","0a11446e":"code","c6c21a87":"code","5fb46d5b":"code","8dadd95a":"code","9e980d75":"code","d6b16315":"code","50a3d0d9":"code","2736b3f4":"code","5f09634b":"code","f374553e":"markdown","7d8f9d30":"markdown","a150ebb9":"markdown","97a6c377":"markdown","71e60bef":"markdown","d1f60b21":"markdown","59b6a51c":"markdown","33dccf91":"markdown","a46b907a":"markdown","e72c6b84":"markdown","44e8b40b":"markdown","74fa57f5":"markdown","b37b5d84":"markdown","5455168d":"markdown","094fd6c4":"markdown","b4ada616":"markdown","a1061d01":"markdown","1a1fe6e9":"markdown","af181c1e":"markdown","e21dcc32":"markdown","83c50668":"markdown","6fa423ab":"markdown","1e84173d":"markdown","714cbaae":"markdown","4bfb6ded":"markdown","507dfc52":"markdown","3551fe9d":"markdown","0d96e43c":"markdown","47ef206b":"markdown"},"source":{"756e9173":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = (12.0,9.0)\n\npokemon_df = pd.read_csv('..\/input\/pokemon\/pokemon.csv')","80563b6a":"pokemon_df.describe()","6d25e841":"pokemon_df = pokemon_df.drop(columns= ['Generation'] , axis= 1)\npokemon_df","113d1c37":"ansur = pd.read_csv('..\/input\/ansur-ii\/ANSUR_II_MALE.csv')","43c7032d":"ansur_num_df = ansur.select_dtypes(include=['int', 'float'])","843af14f":"ansur_num_df.shape","e70a3b08":"from sklearn.feature_selection import VarianceThreshold\n\nansur_num_df.var()  ## Seeing the variances of all features ansur_num_df dataset to estimate threshold","b3804edd":"sel = VarianceThreshold(threshold=10)\nsel.fit(ansur_num_df)\n\n","a7da1b3b":"mask = sel.get_support()","e14bd845":"print(mask)","c33f824b":"reduced_df = ansur_num_df.loc[: , mask]\nreduced_df.shape","613218db":"bangalore_house_df = pd.read_csv('..\/input\/bangalore-real-estate-price\/blr_real_estate_prices.csv')\nbangalore_house_df.shape","96fded4d":"bangalore_house_df.isnull().sum()","13e5e636":"sns.heatmap(bangalore_house_df.isnull())","4527d198":"missing_val_per = (bangalore_house_df.isnull().sum() \/ bangalore_house_df.shape[0] ) * 100\nmissing_val_per","c98ff865":"drop_columns = missing_val_per[missing_val_per > 25].keys()\ndrop_columns","fc5efdd8":"new_bangalore_house_df = bangalore_house_df.drop(columns= drop_columns)\nnew_bangalore_house_df.shape","42db6c34":"ansur.head()","2308ecbf":"height_arr = ['acromialheight', 'axillaheight', 'buttockheight', 'cervicaleheight' , 'chestheight', 'poplitealheight', 'sittingheight', 'tenthribheight', 'tibialheight', 'wristheight']","396900be":"height_df = ansur[height_arr]\nheight_df.head()","6fcc964e":"height_df.shape","03bcae90":"## Creating positive correlation matrix\ncorr_matrix = height_df.corr().abs()\nprint(corr_matrix)","0a11446e":"sns.heatmap(height_df.corr() , cmap = 'Wistia' , annot = True)","c6c21a87":"#Create and apply mask\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ntri_df = corr_matrix.mask(mask)","5fb46d5b":"tri_df","8dadd95a":"sns.heatmap(height_df.corr() , mask = mask, cmap = 'Wistia' , annot = True)","9e980d75":"## finding Columns that meets threshold\nto_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.93)]","d6b16315":"print(to_drop)","50a3d0d9":"##Dropping these columns\nnew_height_df = height_df.drop(to_drop, axis=1)\nnew_height_df.head()","2736b3f4":"new_height_df.shape","5f09634b":"sns.heatmap(new_height_df.corr() , cmap = 'Wistia' , annot = True)","f374553e":"Now we are creating a square matrix with dimensions equal to the number of features. In which we will have the elements as the absolute value of correlation between the features.","7d8f9d30":"### **1.*Removing low variance features***","a150ebb9":"We can also visualize correlation using heatmap","97a6c377":"Std(Standard Deviation) will be so low or zero for low variance that means mazimum and minimum value will be so close or equal.\nHere in **Generation** column we can see standard deviation is so low compared to other columns.As a result we can drop these column to reduce dimensionality as it contains very less information in every observation.","71e60bef":"### **2.*Features having a lot of missing values***","d1f60b21":"Here we can see **society** column have 41.3% of missing values that means almost half of the values are missing in that column.So we can remove it from the dataset to reduce dimensionality.\n\nNow droping the columns which has greater than 25% missing values","59b6a51c":"Here we can see 3 features with noticable low variance has been reduced.\nThreshold value depends on the variation of dataset","33dccf91":"Real world dataset contains a lot of missing or NAN values.We have to impute those missing values using various techniques. But Theoretically, features having 25 to 30%  maximum missing values are allowed to drop from the dataset.\n\nWe can remove that column from the dataset to reduce dimensionality if we want. But again it\u2019s totally depends upon the importance of the variable.","a46b907a":"Here we can see the correlation value r of the height_df.The value of r always lies between minus one and plus one. Minus one describes a perfectly negative correlation, zero describes no correlation at all and plus one stands for a perfect positive correlation.\nFrom the highly correlated features pair, one of the feature can be dropped to reduce dimensionality.","e72c6b84":"We can use this mask to reduce the number of dimensions in our dataset.","44e8b40b":"# Dimensionality Reduction\n","74fa57f5":"In many datasets we find some of the features which are highly correlated that means which are some what linearly dependent with other features. These features contribute very less in predicting the output but increses the computational cost.Correlated features in general don't improve models. So we can reduce one of them between highly correlated two features.","b37b5d84":"Seeing the percentage of missing values","5455168d":"Checking the missing values and seeing it in the heatmap","094fd6c4":"Dimensionality means the number of columns you have in your dataset.\n\nWhen you have many columns in your dataset (say more than 10), the data is considered high dimensional.If you are new to that dataset it could be hard to find the most important patterns because complexity comes with high dimensionality.","b4ada616":"**If you like this notebook, please support me by sharing your feedback.It will inspire me a lot. Thank you!**","a1061d01":".get_support() method will give us a True or False value on whether each feature's variance is above the threshold or not","1a1fe6e9":"### Few Techniques to Reduce Dimensionality\n","af181c1e":"We can then use a list comprehension to find all columns that have a correlation to any feature stronger than the threshold value. We will set threshold value 0.93 here.","e21dcc32":"We can automate the selection of features that have sufficient variance and not too many missing values by using\nScikit-learn's built-in feature selection tools called VarianceThreshold().\n\n","83c50668":"To get a quick overview which column contain little variance we can use describe method.","6fa423ab":"### **3.*Removing highly correlated features***","1e84173d":"We can improve this plot further by removing duplicate and unnecessary information like the correlation coefficients of one on the diagonal.\nTo do so we'll create a boolean mask. We use NumPy's ones_like() function to create a matrix filled with True values with the same dimensions as our correlation matrix and then pass this to NumPy's triu(), for triangle upper, function to set all non-upper triangle values to False.\n","714cbaae":"### Why to reduce dimensionality? ","4bfb6ded":"When we create the selector we can set the minimal variance threshold. Here, we've set it to a variance of 10\n","507dfc52":"- **Dataset will become simpler**\n- **Requires less disk space**\n- **Requires less computation time**\n-**Have lower chance of overfitting**\n","3551fe9d":"### What is dimensionality? ","0d96e43c":"Scikit-learn's built-in VarianceThreshold() method only accept numerical column or feature","47ef206b":"If the variance is low or close to zero, then a feature is approximately constant and will not improve the performance of the model. We can reduce dimensionality by droping that column."}}