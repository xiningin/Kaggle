{"cell_type":{"20271e92":"code","44faf53e":"code","b7f44c94":"code","afc1d5fa":"code","2f65ea30":"code","7d59eb5e":"code","e5f385db":"code","6537f680":"code","83b513fd":"code","62267c93":"code","ca5a4eb4":"code","ce81880f":"code","bb064888":"code","fce71815":"code","9524c5e3":"code","c3909a47":"code","43f7d169":"code","c54a8f2b":"code","a67258a5":"code","c3c186a4":"code","c9a2db6d":"code","6c743168":"code","5e8d4ec8":"code","4f52d542":"code","234a407f":"code","2d595fff":"code","17d74220":"code","acdf649a":"code","67fe1255":"code","1a2e114e":"code","b2f7fb5d":"code","718748ee":"code","0793d1c6":"code","72da89eb":"code","d2b59174":"code","7c25639d":"code","a022ccdf":"code","be010bf9":"code","5e9aa51a":"code","2388dd55":"code","cba0e674":"code","b50f503c":"code","5f3b1022":"code","02109b82":"code","5a0e82a0":"code","9927dbd2":"markdown","2a531e78":"markdown","550fb799":"markdown","b08c0e48":"markdown","2773d1fb":"markdown","cb4478ec":"markdown","49157ecf":"markdown","16c8fd69":"markdown","10a5dec1":"markdown","fe7d8695":"markdown","c3ac0a8a":"markdown","3eeba5bd":"markdown","90f8392c":"markdown","7a1b0764":"markdown","cddaf5fd":"markdown"},"source":{"20271e92":"import matplotlib.pyplot as plt \nfrom PIL import Image\nimport tensorflow as tf\nimport numpy as np","44faf53e":"img = Image.open('..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/10042.jpg')\nfig,ax = plt.subplots(figsize=(6,6))\nax.imshow(img)\nax.set_title('Original Figure')\nplt.show()","b7f44c94":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            rotation_range=20,\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img,axis=0), batch_size=1)\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Horizonal\/Vertical Flipped')","afc1d5fa":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img,axis=0), batch_size=1)\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Horizonal\/Vertical Flipped')","2f65ea30":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            brightness_range=[0.2,1.9],\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img,axis=0), batch_size=1)\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Horizonal\/Vertical Flipped')","7d59eb5e":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            zoom_range=0.7,\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img,axis=0), batch_size=1)\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Horizonal\/Vertical Flipped')","e5f385db":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n                horizontal_flip=True,\n                vertical_flip=True,\n                fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img,axis=0), batch_size=1)\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Horizonal\/Vertical Flipped')","6537f680":"tf.keras.backend.clear_session()","83b513fd":"from keras.preprocessing.image import ImageDataGenerator\n\ninput_shape = (120, 120, 3)\nimg_width = 120\nimg_height = 120\n\ntrain_data_dir = '..\/input\/intel-image-classification\/seg_train\/seg_train'\nvalidation_data_dir = '..\/input\/intel-image-classification\/seg_test\/seg_test'\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255)\n\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(train_data_dir,\n                                                    target_size = (img_width, img_height),\n                                                    class_mode = 'categorical',\n                                                    shuffle = True)\n\nvalidation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n                                                              target_size = (img_width, img_height),\n                                                              class_mode = 'categorical',\n                                                              shuffle = False)","62267c93":"EPOCHS = 30\nBATCH_SIZE = 128\nclasses = 6\n\n#STEPS_PER_EPOCH = train_generator.n\/\/BATCH_SIZE (14034 \/ 128)\nSTEPS_PER_EPOCH = 119\n#VALIDATION_STEPS = val_generator.n\/\/BATCH_SIZE\nVALIDATION_STEPS = 25\n\nLEARNING_RATE = 1e-3\n\nmodel = tf.keras.models.Sequential([\n\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(img_width, img_height, 3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    #tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    #tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    #tf.keras.layers.Dropout(0.5),\n    #tf.keras.layers.BatchNormalization(),\n    #tf.keras.layers.MaxPooling2D(2,2),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(classes, activation='softmax')\n])","ca5a4eb4":"from keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n                   \ncheckpoint = ModelCheckpoint(\"without_augmentation.h5\",\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             save_freq = 'epoch',\n                             verbose=1)\n\nlr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\",\n                                 factor=0.2,\n                                 patience=5,\n                                 verbose=1,\n                                 mode='min',\n                                 min_delta=1e-4,\n                                 cooldown=0,\n                                 min_lr=1e-5,)\n\nearlystop = EarlyStopping(monitor = 'val_loss', \n                          min_delta=1e-4, \n                          patience = 4,\n                          verbose = 1,\n                          mode='min',\n                          restore_best_weights = True)\n\n# we put our call backs into a callback list\ncallbacks = [earlystop, checkpoint, lr_scheduler]\n\n# Note we use a very small learning rate \nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = Adam(),\n              metrics = ['accuracy'])","ce81880f":"%%time\nhistory = model.fit(train_generator, \n                    epochs=EPOCHS, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data = validation_generator, \n                    verbose = 1, \n                    validation_steps=VALIDATION_STEPS,\n                    callbacks = callbacks)","bb064888":"result = history.history\n\nprint(max(result['accuracy']))\n\nprint(max(result['val_accuracy']))\n\nprint(min(result['loss']))\nprint(min(result['val_loss']))","fce71815":"history_dict = history.history\n\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nepochs = range(1, len(loss_values) + 1)\n\nline1 = plt.plot(epochs, acc_values, label='Training Accuracy')\nline2 = plt.plot(epochs, loss_values, label='Training Loss')\n\nline3 = plt.plot(epochs, val_acc_values, label='Validation Accuracy')\nline4 = plt.plot(epochs, val_loss_values, label='Validation Loss')\n\n\nplt.xlabel('Epochs') \nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend()\nplt.show()","9524c5e3":"tf.keras.backend.clear_session()","c3909a47":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   rotation_range=20)\n\ntrain_generator = train_datagen.flow_from_directory(train_data_dir,\n                                                    target_size = (img_width, img_height),\n                                                    class_mode = 'categorical',\n                                                    shuffle = True)","43f7d169":"checkpoint = ModelCheckpoint(\"rotation.h5\",\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             save_freq = 'epoch',\n                             verbose=1)\n\ncallbacks = [earlystop, checkpoint, lr_scheduler]\n\n# Note we use a very small learning rate \nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = Adam(),\n              metrics = ['accuracy'])","c54a8f2b":"%%time\nhistory = model.fit(train_generator, \n                    epochs=EPOCHS, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data = validation_generator, \n                    verbose = 1, \n                    validation_steps=VALIDATION_STEPS,\n                    callbacks = callbacks)","a67258a5":"result = history.history\n\nprint(max(result['accuracy']))\n\nprint(max(result['val_accuracy']))\n\nprint(min(result['loss']))\nprint(min(result['val_loss']))","c3c186a4":"history_dict = history.history\n\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nepochs = range(1, len(loss_values) + 1)\n\nline1 = plt.plot(epochs, acc_values, label='Training Accuracy')\nline2 = plt.plot(epochs, loss_values, label='Training Loss')\n\nline3 = plt.plot(epochs, val_acc_values, label='Validation Accuracy')\nline4 = plt.plot(epochs, val_loss_values, label='Validation Loss')\n\n\nplt.xlabel('Epochs') \nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend()\nplt.show()","c9a2db6d":"tf.keras.backend.clear_session()","6c743168":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2)\n\ntrain_generator = train_datagen.flow_from_directory(train_data_dir,\n                                                    target_size = (img_width, img_height),\n                                                    class_mode = 'categorical',\n                                                    shuffle = True)","5e8d4ec8":"checkpoint = ModelCheckpoint(\"Width_HeightShift.h5\",\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             save_freq = 'epoch',\n                             verbose=1)\n\n# we put our call backs into a callback list\ncallbacks = [earlystop, checkpoint, lr_scheduler]\n\n# Note we use a very small learning rate \nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = Adam(),\n              metrics = ['accuracy'])","4f52d542":"%%time\nhistory = model.fit(train_generator, \n                    epochs=EPOCHS, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data = validation_generator, \n                    verbose = 1, \n                    validation_steps=VALIDATION_STEPS,\n                    callbacks = callbacks)","234a407f":"result = history.history\n\nprint(max(result['accuracy']))\n\nprint(max(result['val_accuracy']))\n\nprint(min(result['loss']))\nprint(min(result['val_loss']))","2d595fff":"history_dict = history.history\n\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nepochs = range(1, len(loss_values) + 1)\n\nline1 = plt.plot(epochs, acc_values, label='Training Accuracy')\nline2 = plt.plot(epochs, loss_values, label='Training Loss')\n\nline3 = plt.plot(epochs, val_acc_values, label='Validation Accuracy')\nline4 = plt.plot(epochs, val_loss_values, label='Validation Loss')\n\n\nplt.xlabel('Epochs') \nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend()\nplt.show()","17d74220":"tf.keras.backend.clear_session()","acdf649a":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   brightness_range=[0.2,1.9])\n\ntrain_generator = train_datagen.flow_from_directory(train_data_dir,\n                                                    target_size = (img_width, img_height),\n                                                    class_mode = 'categorical',\n                                                    shuffle = True)","67fe1255":"checkpoint = ModelCheckpoint(\"Brightness.h5\",\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             save_freq = 'epoch',\n                             verbose=1)\n\n# we put our call backs into a callback list\ncallbacks = [earlystop, checkpoint, lr_scheduler]\n\n# Note we use a very small learning rate \nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = Adam(),\n              metrics = ['accuracy'])","1a2e114e":"%%time\nhistory = model.fit(train_generator, \n                    epochs=EPOCHS, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data = validation_generator, \n                    verbose = 1, \n                    validation_steps=VALIDATION_STEPS,\n                    callbacks = callbacks)","b2f7fb5d":"result = history.history\n\nprint(max(result['accuracy']))\n\nprint(max(result['val_accuracy']))\n\nprint(min(result['loss']))\nprint(min(result['val_loss']))","718748ee":"history_dict = history.history\n\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nepochs = range(1, len(loss_values) + 1)\n\nline1 = plt.plot(epochs, acc_values, label='Training Accuracy')\nline2 = plt.plot(epochs, loss_values, label='Training Loss')\n\nline3 = plt.plot(epochs, val_acc_values, label='Validation Accuracy')\nline4 = plt.plot(epochs, val_loss_values, label='Validation Loss')\n\n\nplt.xlabel('Epochs') \nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend()\nplt.show()","0793d1c6":"tf.keras.backend.clear_session()","72da89eb":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   zoom_range=0.3)\n\ntrain_generator = train_datagen.flow_from_directory(train_data_dir,\n                                                    target_size = (img_width, img_height),\n                                                    class_mode = 'categorical',\n                                                    shuffle = True)","d2b59174":"checkpoint = ModelCheckpoint(\"zoom_range.h5\",\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             save_freq = 'epoch',\n                             verbose=1)\n\ncallbacks = [earlystop, checkpoint, lr_scheduler]\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = Adam(),\n              metrics = ['accuracy'])","7c25639d":"%%time\nhistory = model.fit(train_generator, \n                    epochs=EPOCHS, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data = validation_generator, \n                    verbose = 1, \n                    validation_steps=VALIDATION_STEPS,\n                    callbacks = callbacks)","a022ccdf":"result = history.history\n\nprint(max(result['accuracy']))\n\nprint(max(result['val_accuracy']))\n\nprint(min(result['loss']))\nprint(min(result['val_loss']))","be010bf9":"result = history.history\n\nprint(max(result['accuracy']))\n\nprint(max(result['val_accuracy']))\n\nprint(min(result['loss']))\nprint(min(result['val_loss']))","5e9aa51a":"history_dict = history.history\n\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nepochs = range(1, len(loss_values) + 1)\n\nline1 = plt.plot(epochs, acc_values, label='Training Accuracy')\nline2 = plt.plot(epochs, loss_values, label='Training Loss')\n\nline3 = plt.plot(epochs, val_acc_values, label='Validation Accuracy')\nline4 = plt.plot(epochs, val_loss_values, label='Validation Loss')\n\n\nplt.xlabel('Epochs') \nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend()\nplt.show()","2388dd55":"tf.keras.backend.clear_session()","cba0e674":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,)\n\ntrain_generator = train_datagen.flow_from_directory(train_data_dir,\n                                                    target_size = (img_width, img_height),\n                                                    class_mode = 'categorical',\n                                                    shuffle = True)","b50f503c":"checkpoint = ModelCheckpoint(\"horizontal_vertical_flip.h5\",\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             save_freq = 'epoch',\n                             verbose=1)\n\ncallbacks = [earlystop, checkpoint, lr_scheduler]\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = Adam(),\n              metrics = ['accuracy'])","5f3b1022":"%%time\nhistory = model.fit(train_generator, \n                    epochs=EPOCHS, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data = validation_generator, \n                    verbose = 1, \n                    validation_steps=VALIDATION_STEPS,\n                    callbacks = callbacks)","02109b82":"result = history.history\n\nprint(max(result['accuracy']))\n\nprint(max(result['val_accuracy']))\n\nprint(min(result['loss']))\nprint(min(result['val_loss']))","5a0e82a0":"history_dict = history.history\n\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nepochs = range(1, len(loss_values) + 1)\n\nline1 = plt.plot(epochs, acc_values, label='Training Accuracy')\nline2 = plt.plot(epochs, loss_values, label='Training Loss')\n\nline3 = plt.plot(epochs, val_acc_values, label='Validation Accuracy')\nline4 = plt.plot(epochs, val_loss_values, label='Validation Loss')\n\n\nplt.xlabel('Epochs') \nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend()\nplt.show()","9927dbd2":"**This is the image, we will apply augmentation on.**","2a531e78":"# Horizontal Flip and Vertical Flip","550fb799":"# Training with Brightness Range","b08c0e48":"# Training with Zoom Range","2773d1fb":"# Brightness Range","cb4478ec":"# Rotation","49157ecf":"# What is Image Augmentation\n\n### Image augmentation is a technique to aritificially diversify image data in the dataset. It is really helpful for small dataset as it can boost accuracy. You can use the to exapnd the size of your dataset. \n\n<img src=\"http:\/\/media5.datahacker.rs\/2020\/02\/download.png\" alt=\"Augmmentation\">","16c8fd69":"# Training with Rotation","10a5dec1":"# Width Shift Range and Height Shift Range","fe7d8695":"# Training with Horizontal Flip and Vertical Flip","c3ac0a8a":"# Thank you!\n![The End](https:\/\/media.giphy.com\/media\/hcpVSCSwDcKju\/giphy.gif)","3eeba5bd":"# Augmentation Techniques\n\nThe are may image augmentation techinique. You can find most of the in [Keras](https:\/\/keras.io\/api\/preprocessing\/image\/). The most important Augmentation Techniques are:\n* Rotation\n* Width Shift Range\n* Height Shift Range\n* Brightness Range\n* Zoom Range\n* Horizontal Flip\n* Vertical Flip","90f8392c":"# Training with Width and Height shift range","7a1b0764":"# Training without augmentation","cddaf5fd":"# Zoom Range"}}