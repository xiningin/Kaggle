{"cell_type":{"66d4eb7b":"code","ad838e49":"code","0f0fcf72":"code","ebc218ff":"code","9b044819":"code","9b441b5f":"code","76e2d9e7":"code","729aaf66":"code","bc707b54":"code","94c4e803":"code","d3b805dd":"code","000dc996":"code","fb592705":"code","fc8c9d13":"code","19132c42":"code","f984e7b0":"code","1981733c":"code","778f15b9":"code","be6828e5":"code","3d8df098":"code","3b9e5365":"code","3a58b18b":"markdown","69e64b31":"markdown","e4872082":"markdown","9dae6205":"markdown","d913ca2f":"markdown","3632b9f8":"markdown","6b3138ee":"markdown","9ce28dc4":"markdown","f57fcbb0":"markdown","85db3a46":"markdown","e36a7f75":"markdown","69a12d4e":"markdown"},"source":{"66d4eb7b":"import numpy as np","ad838e49":"def unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='latin')\n    return dict\n\nimport os\ndef load_batch_file(batch_filename):\n    filepath = os.path.join('..\/input\/cifar-10-batches-py\/', batch_filename)\n    unpickled = unpickle(filepath)\n    return unpickled","0f0fcf72":"train_batch_1 = load_batch_file('data_batch_1')\ntrain_batch_2 = load_batch_file('data_batch_2')\ntrain_batch_3 = load_batch_file('data_batch_3')\ntrain_batch_4 = load_batch_file('data_batch_4')\ntrain_batch_5 = load_batch_file('data_batch_5')\ntest_batch = load_batch_file('test_batch')","ebc218ff":"from keras.utils import np_utils\nnum_classes = 10\ntrain_x = np.concatenate([train_batch_1['data'], train_batch_2['data'], train_batch_3['data'], train_batch_4['data'], train_batch_5['data']])\ntrain_x = train_x.astype('float32') # this is necessary for the division below\ntrain_x \/= 255\ntrain_y = np.concatenate([np_utils.to_categorical(labels, num_classes) for labels in [train_batch_1['labels'], train_batch_2['labels'], train_batch_3['labels'], train_batch_4['labels'], train_batch_5['labels']]])","9b044819":"test_x = test_batch['data'].astype('float32') \/ 255\ntest_y = np_utils.to_categorical(test_batch['labels'], num_classes)","9b441b5f":"from keras.models import Sequential\nfrom keras.layers import Dense","76e2d9e7":"img_rows = img_cols = 32\nchannels = 3","729aaf66":"simple_model = Sequential()\nsimple_model.add(Dense(10_000, input_shape=(img_rows*img_cols*channels,), activation='relu'))\nsimple_model.add(Dense(10, activation='softmax'))\n\nsimple_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_model_history = simple_model.fit(train_x, train_y, batch_size=100, epochs=8, validation_data=(test_x, test_y))","bc707b54":"import matplotlib.pyplot as plt\ndef plot_history(history, title):\n    plt.figure(figsize=(10,3))\n    # Plot training & validation accuracy values\n    plt.subplot(121)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n\n    # Plot training & validation loss values\n    plt.subplot(122)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","94c4e803":"plot_history(simple_model_history, 'Simple NN with 100 batch size')","d3b805dd":"simple_model_smaller_batch = Sequential()\nsimple_model_smaller_batch.add(Dense(10_000, input_shape=(img_rows*img_cols*channels,), activation='relu'))\nsimple_model_smaller_batch.add(Dense(10, activation='softmax'))\n\nsimple_model_smaller_batch.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_model_smaller_batch_history = simple_model_smaller_batch.fit(train_x, train_y, batch_size=50, epochs=8, validation_data=(test_x, test_y))","000dc996":"plot_history(simple_model_smaller_batch_history, 'Simple NN with 50 batch size')","fb592705":"simple_model_more_layers = Sequential()\nsimple_model_more_layers.add(Dense(10_000, input_shape=(img_rows*img_cols*channels,), activation='relu'))\nsimple_model_more_layers.add(Dense(1_000, activation='relu'))\nsimple_model_more_layers.add(Dense(100, activation='relu'))\nsimple_model_more_layers.add(Dense(10, activation='softmax'))\n\nsimple_model_more_layers.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_model_more_layers_history = simple_model_more_layers.fit(train_x, train_y, batch_size=100, epochs=8, validation_data=(test_x, test_y))","fc8c9d13":"plot_history(simple_model_more_layers_history, 'Simple NN with more layers')","19132c42":"train_x_reshaped = train_x.reshape(len(train_x), img_rows, img_cols, channels)\ntest_x_reshaped = test_x.reshape(len(test_x), img_rows, img_cols, channels)","f984e7b0":"from keras.layers import Conv2D, Flatten\nsimple_cnn_model = Sequential()\nsimple_cnn_model.add(Conv2D(32, (3,3), input_shape=(img_rows,img_cols,channels), activation='relu'))\nsimple_cnn_model.add(Conv2D(32, (3,3), activation='relu'))\nsimple_cnn_model.add(Conv2D(32, (3,3), activation='relu'))\nsimple_cnn_model.add(Flatten())\nsimple_cnn_model.add(Dense(10, activation='softmax'))\n\nsimple_cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_cnn_model_history = simple_cnn_model.fit(train_x_reshaped, train_y, batch_size=100, epochs=8, validation_data=(test_x_reshaped, test_y))","1981733c":"plot_history(simple_cnn_model_history, 'CNN with 3 convolution layers')","778f15b9":"from keras.layers import Dropout\nsimple_cnn_model_2 = Sequential()\nsimple_cnn_model_2.add(Conv2D(32, (3,3), input_shape=(img_rows,img_cols,channels), activation='relu'))\nsimple_cnn_model_2.add(Dropout(0.2))\n\nsimple_cnn_model_2.add(Conv2D(32, (3,3), activation='relu'))\nsimple_cnn_model_2.add(Dropout(0.2))\n\nsimple_cnn_model_2.add(Conv2D(32, (3,3), activation='relu'))\nsimple_cnn_model_2.add(Dropout(0.2))\n\nsimple_cnn_model_2.add(Flatten())\nsimple_cnn_model_2.add(Dense(10, activation='softmax'))\n\nsimple_cnn_model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_cnn_model_2_history = simple_cnn_model_2.fit(train_x_reshaped, train_y, batch_size=100, epochs=8, validation_data=(test_x_reshaped, test_y))","be6828e5":"plot_history(simple_cnn_model_2_history, '3 convolution layers with dropout')","3d8df098":"simple_cnn_model_3 = Sequential()\nsimple_cnn_model_3.add(Conv2D(64, (3,3), input_shape=(img_rows,img_cols,channels), activation='relu'))\nsimple_cnn_model_3.add(Dropout(0.2))\n\nsimple_cnn_model_3.add(Conv2D(64, (3,3), activation='relu'))\nsimple_cnn_model_3.add(Dropout(0.2))\n\nsimple_cnn_model_3.add(Conv2D(64, (3,3), activation='relu'))\nsimple_cnn_model_3.add(Dropout(0.2))\n\nsimple_cnn_model_3.add(Flatten())\nsimple_cnn_model_3.add(Dense(128, activation='relu'))\nsimple_cnn_model_3.add(Dense(10, activation='softmax'))\n\nsimple_cnn_model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_cnn_model_3_history = simple_cnn_model_3.fit(train_x_reshaped, train_y, batch_size=100, epochs=8, validation_data=(test_x_reshaped, test_y))","3b9e5365":"plot_history(simple_cnn_model_3_history, 'CNN with more layers and filters')","3a58b18b":"### Simple Convolution Neural Network\n\nOur input is an image and by treating it as a flat array we lose the locality on the features. Moreover, having over 3000 inputs adds computational overhead. \n\nConvolution Neural Networks on the other hand are ideal for images (among others) and by learning on the filter parameters and not on the transformation of the image itself we can achieve deeper and more complex architectures with better results.\n\nA C-NN is fed each image as a tensor i.e a multidimensional matrix so we need to _reshape_ the flattened arrays into stacked matrices for each colour channel. This way we end up with a \\\\(60000 \\times 32 \\times 32 \\times 3 \\\\) tensor.\n\nIn the example below, we train a _C-NN_ with 3 convolution layers of \\\\(3 \\times 3 \\\\) convolution matrices. These of course will be _flattened_ to feed the usual output layer.","69e64b31":"We can see that by adding dropout to the neural network, no overfitting seems to take place in such early stage and we can seemingly leave the training continue for a few more epochs. I suspect the accuracy to slowly increase at this rate. ","e4872082":"### Changing the batch size\n\nWhen setting _batch size_ we tell the neural network to train with samples in batches of fixed size. The error from those samples accumulates and the network backprops the gradient descent after each step. That makes computations faster because we don't have to update the weights after each input. \n\nBut what about accuracy?","9dae6205":"To help us with visualising the loss and accuracy of the model over time, Keras provides a _History_ object to use that has stored the various metrics in each epoch. The following code was taken from the Keras documentation","d913ca2f":"As expected, using smaller batch size (slightly) increases accuracy but hinders performance. More steps are needed to complete an epoch but the model updates more often as well.","3632b9f8":"### Convolution Neural Network with Dropout layers\n\nSince the previous neural network had overfitting issues, I thought I'd use dropout layers to combat this. With dropout we essentially tell the network to randomly leave out a fraction of the nodes from updating the learned parameters. In each step we chose another set of nodes to leave out. \n\nHere, I've added dropout after each convolution layer to leave out 20% of nodes from updating in each step.","6b3138ee":"## Attempts at training Neural Networks\n\n### Simple fully connected neural network with one hidden layer\n\nThe first (and naively basic) attempt was to train a dense neural network of 1 hidden layer, feeding the flattened arrays of the images as input. So we have the input layer with \\\\(32*32*3\\\\) inputs, a dense hidden layer of 10,000 nodes and _ReLU_ activation and the output layer with the 10 class nodes with _Softmax_ activation function to give out probabilities for the different classes.","9ce28dc4":"### Increasing the depth of the simple neural network\n\nThe previous configurations had only one hidden layer. Due to hardware constraints I couldn't test it with more nodes but this is already an extremely shallow network. Could adding-in more layers improve the performance?","f57fcbb0":"# Neural Networks training on CIFAR-10\nVassilis Krikonis\n\nCIFAR-10 is a dataset containing 60,000 images of 10 classes and is considered a typical dataset for computer vision problems.\n\nIn this assignment, I'll try out a few neural network configurations and hyperparameters and compare the results.\n\n## Preperation\n\nThe dataset comes in 6 pickled files. To use them, we need to serialize them using `pickle.load()` and store them into one big numpy array.\n\nEach image is originally stored as a flattened array of \\\\(width*height*channels\\\\) elements representing each pixel in one colour channel. The colour intensity i.e. value in each pixel varies from 0 to 255. By normalising the values to the range \\\\([0,1]\\\\) the performance of the neural networks I've tried training, increased immensely. Without this normalisation, I would get near random accuracy of 10% for the 10 classes in the dataset.\n\nAs a last step for the preparation of the data, we need to  convert the labels to a _one-hot-vector_ encoding, that is, a vector representing each possible label with binary flags. This step is necessary because the output of the neural network is actually a vector like this, a prediction vector with probability for each label.","85db3a46":"It's not easy to tell if it improves accuracy. It seems that validation accuracy doesn't diverge from the training accuracy so if the model was left training for more epochs it could give out better results.","e36a7f75":"Even that shallow neural network, in such a few epochs, was able to achieve near 50% success and it's a hilariously dumb version of a neural network. Judging by the plots below, more epochs would achieve better results.","69a12d4e":"Plotting how well this configuration goes reveals something interesting: the model overfits after the 3rd epoch! We can see the validation accuracy slowly dropping while the training accuracy steadily increases. This is more visible in the plot of the model loss during the epochs. The accumulated Loss in the validation set clearly goes up after the 3rd epoch while the network keeps minimising the training set loss value.\n\nIn situation where our model overfits we need some sort of regularisation and in the next attempt I add in dropout layers.\n\nThat said, we do use _Adam_ as the model's optimizer so it may need some tuning."}}