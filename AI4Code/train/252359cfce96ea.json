{"cell_type":{"2e7fae6d":"code","188d6f00":"code","bd4e0031":"code","cf95b865":"code","ab274626":"code","c2334f2c":"code","a0c4f253":"code","894f6061":"code","f09b9f05":"code","f4361f8d":"code","35093dd6":"code","38b667a8":"code","3a053e49":"code","9c6249fa":"code","e74b48bb":"code","8cad880f":"code","479b8ab1":"code","1ad796c7":"code","810723cb":"code","a2c51ea6":"code","ccecf86a":"code","83efe28f":"code","9fb2f505":"code","e1c0c2d8":"code","df88a624":"code","ddb4550c":"markdown","c8bf5b71":"markdown","66ce0920":"markdown","88299fbd":"markdown","fe90212b":"markdown","d051f380":"markdown","6017d4d8":"markdown","38a29f4e":"markdown","c16d84c4":"markdown","f82febf0":"markdown","6432782e":"markdown","719c3a65":"markdown","827b2d4c":"markdown","ff037808":"markdown","0d0aab72":"markdown","399f7887":"markdown","a2ebda18":"markdown","559ce5e7":"markdown","36b82ee9":"markdown","c688e80d":"markdown","68f154db":"markdown","bafab589":"markdown","13ee5467":"markdown","24eaf9b6":"markdown","9b0d06d2":"markdown","5489ed0b":"markdown","fd5a77aa":"markdown","e7c8c107":"markdown"},"source":{"2e7fae6d":"import numpy as np\r\nimport pandas as pd\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\n\r\n##  Math & Statistics\r\nimport math\r\nfrom collections import Counter\r\nimport scipy.stats as ss\r\nfrom scipy.stats import chi2, \\\r\n                        chi2_contingency\r\n\r\n#Metrics\r\nfrom sklearn.metrics import accuracy_score, \\\r\n                            precision_score, \\\r\n                            recall_score, \\\r\n                            confusion_matrix\r\n\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")","188d6f00":"# Function that execute the qui-square test\r\ndef test_dependency(alpha, cont):\r\n    conf = 1-alpha\r\n    X2, p, dof, expected = chi2_contingency(cont)\r\n    critical = chi2.ppf(conf, dof)\r\n    if abs(X2) >= critical:\r\n        print('dependent (reject H0)')\r\n    else:\r\n        print('independent (fail to reject H0)')\r\n\r\n\r\ndef conditional_entropy(x,y):\r\n    # entropy of x given y\r\n    y_counter = Counter(y)\r\n    xy_counter = Counter(list(zip(x,y)))\r\n    total_occurrences = sum(y_counter.values())\r\n    entropy = 0\r\n    for xy in xy_counter.keys():\r\n        p_xy = xy_counter[xy] \/ total_occurrences\r\n        p_y = y_counter[xy[1]] \/ total_occurrences\r\n        entropy += p_xy * math.log(p_y\/p_xy)\r\n    return entropy\r\n\r\n# Uncertainty coefficient or Thiel's U\r\ndef theil_u(x,y):\r\n    s_xy = conditional_entropy(x,y)\r\n    x_counter = Counter(x)\r\n    total_occurrences = sum(x_counter.values())\r\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\r\n    s_x = ss.entropy(p_x)\r\n    if s_x == 0:\r\n        return 1\r\n    else:\r\n        return (s_x - s_xy) \/ s_x","bd4e0031":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname,filename))","cf95b865":"df = pd.read_csv(\"\/kaggle\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")","ab274626":"df.isnull().sum()","c2334f2c":"df.info()","a0c4f253":"df.describe()","894f6061":"df.head(4)","f09b9f05":"fig, axs = plt.subplots(figsize=(5,4))\r\ng = sns.countplot(x='Revenue', data=df, ax=axs)","f4361f8d":"fig, axs = plt.subplots(figsize=(5,4))\r\ng = sns.countplot(x='Month', data=df, ax=axs, hue='Revenue')","35093dd6":"fig, axs = plt.subplots(figsize=(5,4))\r\ng = sns.countplot(x='SpecialDay', data=df, ax=axs, hue='Revenue')","38b667a8":"df[['SpecialDay','Month','Revenue']].groupby('Month').sum()","3a053e49":"pd.crosstab(df['Weekend'],df['Revenue'], margins=True)","9c6249fa":"df[['BounceRates','Revenue']].groupby('Revenue').mean()","e74b48bb":"df[['ExitRates','Revenue']].groupby('Revenue').mean()","8cad880f":"df[['PageValues','Revenue']].groupby('Revenue').mean()","479b8ab1":"from sklearn.model_selection import train_test_split\r\n# Divide X and y\r\ny = df['Revenue']\r\nX = df.drop(['Revenue'], axis=1)\r\n\r\n# Split dataset\r\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","1ad796c7":"y_train = y_train.map({True:1, False:0})\r\ny_valid = y_valid.map({True:1, False:0})","810723cb":"import fuzzywuzzy\r\nfrom fuzzywuzzy import process\r\n\r\nclass CustomPreProcessing():\r\n\r\n    def __init__(self, min_ratio = 80, verbose = False):\r\n        if(verbose):\r\n            print(\"__init__\")\r\n        self.__class__.__name__ = 'CustomPreProcessing'\r\n        self.min_ratio = min_ratio\r\n        self.data = pd.DataFrame({'No data' : []})\r\n        self.verbose = verbose\r\n\r\n    \r\n    def replace_matches_in_column(self, column, string_to_match, verbose=False):\r\n        # get a list of unique strings\r\n        strings = self.data[column].unique()\r\n        \r\n        # get the top 10 closest matches to our input string\r\n        matches = fuzzywuzzy.process.extract(string_to_match, strings, \r\n                                            limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\r\n\r\n        # only get matches with a ratio > 90\r\n        close_matches = [matches[0] for matches in matches if matches[1] >= self.min_ratio]\r\n        if (verbose):\r\n            print(f'Searching for:{string_to_match}, close match: {close_matches}')\r\n        # get the rows of all the close matches in our dataframe\r\n        rows_with_matches = self.data[column].isin(close_matches)\r\n\r\n        # replace all rows with close matches with the input matches \r\n        self.data.loc[rows_with_matches, column] = string_to_match\r\n\r\n    def rem_inconsistent_date(self, X, values):\r\n        if(self.verbose):\r\n            print(\"rem_inconsistentdata\")\r\n\r\n        for val in values:\r\n            self.replace_matches_in_column('Month', val)\r\n    \r\n    def map_months(self, months):\r\n        if(self.verbose):    \r\n            print('map_months')\r\n        self.data['Month'] = self.data['Month'].map(dict((v,k+1) for k,v in enumerate(months)))\r\n    \r\n    def map_visitors(self, visitor_type):\r\n        if(self.verbose):\r\n            print('map_visitor_type')\r\n        self.data['VisitorType'] = self.data['VisitorType'].map(dict((v,k) for k,v in enumerate(visitor_type)))\r\n    \r\n    def map_weekend(self):\r\n        if(self.verbose):\r\n            print('map_weekeend')     \r\n        self.data['Weekend'] = self.data['Weekend'].map({True:1, False:0})\r\n       \r\n\r\n    def fit(self, X, y = None):\r\n        if(self.verbose):\r\n            print('fit')\r\n        return self\r\n    \r\n    def transform(self, X, y = None):\r\n        if(self.verbose):\r\n            print(\"transform\")\r\n\r\n        self.data = X.copy()\r\n\r\n        months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Oct','Sep','Nov','Dec']\r\n        visitor_type = ['Returning_Visitor', 'New_Visitor','Other']\r\n\r\n        try:\r\n            \r\n            # remove inconsistentdata\r\n            self.rem_inconsistent_date(self.data, months)\r\n\r\n            # mapping months from str to numeric\r\n            self.map_months(months)\r\n\r\n            # mapping visitortype from str to numeric\r\n            self.map_visitors(visitor_type)\r\n\r\n            # mapping Weekend from boolean to numeric\r\n            self.map_weekend()\r\n\r\n        except KeyError as ke:\r\n            print(ke)\r\n            pass\r\n        finally:\r\n            return self.data\r\n\r\n\r\n\r\n    def __repr__(self):\r\n        return 'CustomPreProcessing'\r\n\r\n    def getdata(self):\r\n        return self.data","a2c51ea6":"from imblearn.pipeline import Pipeline\r\nfrom xgboost import XGBClassifier\r\nfrom sklearn.model_selection import cross_validate\r\nfrom sklearn.metrics import make_scorer\r\n\r\nscoring = ('roc_auc','accuracy','precision','recall')\r\n\r\nbaseline = Pipeline([\r\n                ('Preprocessor',CustomPreProcessing(min_ratio=80, verbose=False)),\r\n                ('model',XGBClassifier(eval_metric='logloss'))])\r\nscores = cross_validate(baseline, X_train_full, y_train, cv=5, scoring=scoring)","ccecf86a":"for k, v in scores.items():\r\n    print(k, np.round(v.mean(),4)) ","83efe28f":"cat_var = ['SpecialDay','Month','OperatingSystems','Browser','Region','TrafficType','VisitorType','Weekend']\r\nfor var in cat_var:\r\n    cont = pd.crosstab(df['Revenue'], df[var])\r\n    print(\"Revenue and \"+var+\" are:\",end=\" \")\r\n    test_dependency(0.05, cont)","9fb2f505":"df['TotalPages'] = df['Administrative'] + df['Informational'] + df['ProductRelated']\r\n\r\ndf['TotalTime'] = df['Administrative_Duration'] + df['Informational_Duration'] + df['ProductRelated_Duration']\r\n\r\ndf['Adm_time_per_page'] = (df['Administrative_Duration']\/df['Administrative']).fillna(0)\r\n\r\ndf['Info_time_per_page'] = (df['Informational_Duration']\/df['Informational']).fillna(0)\r\n\r\ndf['Pr_time_per_page'] = (df['ProductRelated_Duration']\/df['ProductRelated']).fillna(0)","e1c0c2d8":"theilu = pd.DataFrame(index=['Revenue'],columns=df.columns)\r\ncolumns = df.columns\r\nfor j in range(0,len(columns)):\r\n    u = theil_u(df['Revenue'].tolist(),df[columns[j]].tolist())\r\n    theilu.loc[:,columns[j]] = u\r\ntheilu.fillna(value=np.nan,inplace=True)\r\nplt.figure(figsize=(20,1))\r\nsns.heatmap(theilu,annot=True,fmt='.2f')\r\nplt.show()","df88a624":"from imblearn.over_sampling import SMOTE\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nmodel = Pipeline([ \r\n                ('Preprocessor', CustomPreProcessing(min_ratio=80, verbose=False)),\r\n                ('Scaler', StandardScaler()),\r\n                ('SMOTE()', SMOTE()),\r\n                ('model', XGBClassifier(eval_metric='logloss'))])\r\n                \r\nscores = cross_validate(model, X_train_full, y_train, cv=5, scoring=scoring)\r\n\r\nfor k, v in scores.items():\r\n    print(k, np.round(v.mean(),4)) ","ddb4550c":"#### As we have categorical variables, we can verify the dependence between it. This can be made using the chi-square test","c8bf5b71":"#### The chi-square test shows that Region and Revenue are independent. This variable may be unuseful to use in ML algorithms","66ce0920":"## Taking a look at Google Analytics metrics","88299fbd":"![online_shoppers.jpg](attachment:ccc4b292-7d9f-4494-a0f5-98c4205e0e86.jpg)","fe90212b":"<a id='Convert' a\/>\r\n<h1> Prepare Data <\/h1>\r\n<hr><\/hr>\r\n\r\n<h4> In order to use Machine Learning Algorithms, we need to convert categorical-nominal to numerical data<\/h4>","d051f380":"<a id='Functions' a\/>\r\n<h1> Functions <\/h1>\r\n<hr><\/hr>","6017d4d8":"# A class to preprocess dataset\r\n### Convert categorical and boolean to numeric","38a29f4e":"<a id='Dependency' a\/>\r\n<h2> Test dependency of Revenue and categorical variables <\/h2>","c16d84c4":"# Summary\r\n---\r\n - [Import libs](#Libs)\r\n - [Functions](#Functions)\r\n - [The subject](#TheProblem)\r\n - [Read Data](#Import)\r\n - [Let's look at the Data!](#Explore)\r\n - [Transfrom Data](#Convert)\r\n    * [Test Dependency ](#Dependency)\r\n    * [Buiding New Features](#NewFeatures)\r\n    \r\n- [What variables are more important?](#ThielU)\r\n- [Prepare Data to ML](#ML)\r\n- [Modeling](#ML2)","f82febf0":"#### If we take a look at the \"SpecialDay\" column, we can see that most of the often revenued sessions were made 13 days after or 1 day after especial days, considering the dynamics of e-commerce such as the duration between the order date and delivery date.","6432782e":"#### The number of sessions close to special days was greater in May, and near of 51% of these sessions where revenued","719c3a65":"#### Exit Rate  refer to the percentage of visitors to a page on the website from which they exit the website to a different website.\r\n#### For revenued sessions, near of 20% of the sessions has started and ended at the same page, for non-revenued 50%. In other words, engaged people, who see more than only one page in the your site, has more chance to generate revenue\r\n","827b2d4c":"#### The bounce rate measure the percentage of visitors who enter the site then leave. If this page is only your home page, it's not good, this means that your visitors don't take any action and leave your page.\r\n#### Bounce Rate is greater in non-revened sessions (25% to non-revenue against 5% to revenue). Sessions, when the amount of people visiting the site is greater than these leaving, is more likely to be revenue","ff037808":"# Make a baseline model","0d0aab72":"#### The Months with most online sessions are March, May, November and December. This may be caused by the important holiday, and special days in these months like: \r\n- **Woman's Day (Mar)**\r\n- **Mother's day (May)**\r\n- **Brack Friday, and Thanks Giving (Nov)**\r\n- **Christmas , and Happy New Year (Dec)**\r\n\r\n#### Although there is a discrepancy between Revenue and Non-Revenue visitors, the distributions are similar. Showing that most of purchases was made in these months","399f7887":"<a id='Import' a\/>\r\n<h1> Read data <\/h1>\r\n<hr><\/hr>","a2ebda18":"<a id='NewFeatures' a\/>\r\n<h2> Building New Features <\/h2>","559ce5e7":"#### Page Value is the average value for a page that a user visited before landing on the goal page or completing an Ecommerce transaction. For revenued sessions, the average Page Value is 27.00 and for non-revenued is about 2.00","36b82ee9":"<a id='Libs' a\/>\r\n<h1> Import Libs <\/h1>\r\n<hr><\/hr>","c688e80d":"#### The Data is inbalanced, with roughly 85% (10422) of Non-Revenue vs 15% (1902) of Revenue sessions","68f154db":"<a id='ThielU' a\/>\r\n<h1> What variables are most important to explain Revenued sessions?<\/h1>\r\n<hr><\/hr>\r\n\r\nTheil's U or Uncertainty Coefficient is a measure of nominal association. Sometimes expressed as U(x|y). Is the measure of entropy in variable y that variable x explains. Thiel's U is measured in the range of [0,1], where 0 means that feature y provides no information about feature x, and 1 means that feature y provides full information abpout features x's value.","bafab589":"### Dataset has no null values","13ee5467":"<h1> Online Shoppers Intention Dataset <\/h1>","24eaf9b6":"#### And about week days, weekend improves Revenues by about 2,5% (17.40 % more Revenues in weekends against 14.90 % on weekdays)","9b0d06d2":"## References:\r\n\r\nhttps:\/\/en.wikipedia.org\/wiki\/Uncertainty_coefficient,\r\n\r\nhttps:\/\/www.statisticshowto.com\/uncertainty-coefficient\/\r\n\r\nhttps:\/\/www.kaggle.com\/shakedzy\/alone-in-the-woods-using-theil-s-u-for-survival","5489ed0b":"<a id='TheProblem' a\/>\r\n\r\n## **About Data**\r\n\r\nEach data entry (row) is refer to one online session, that can be from the same user or not within a period of 1-year. Totalizing 12,330 sessions. \r\n\r\nThe data has a set of variables like the type of page visited, amount of time spent, proximity to special dates, Browser used, and also Google Analytics Metrics like \"Bounce Rate\", \"Exit Rate\", and \"Page Value\". \r\n\r\nFor each entry, the column \"Revenue\" is marked as 1 if that session generated revenue.  \r\n\r\n## **Some questions our stakeholders may ask, in order to predict when a visitor is susceptible to buy, or what strategies they may use to improve the chance of sessions generate revenues**\r\nWhat was the behavior of sessions resulting in purchases, and non-purchases?\r\n\r\nHow much time visitors spent at our sites in revenued sessions?\r\n\r\nWhat kind of pages they access?\r\n\r\nAre the weekends or special day's proximity relevant to they decision?\r\n\r\n","fd5a77aa":"<a id='Explore' a\/>\r\n<h1> Let's look at the Data! <\/h1>\r\n<hr><\/hr>","e7c8c107":"#### Seems like TotalTime, Adm_time_per_page, and Pr_time_per_page, was most significative to Revenue"}}