{"cell_type":{"77690b50":"code","ae0c92b0":"code","aa033f6c":"code","6177272d":"code","cdc879da":"code","5aac37ff":"code","c06a8faf":"code","30f6945b":"code","14cc77ea":"code","9ba13904":"code","f24d7da5":"code","6898e16b":"code","7064a947":"code","3c5b1d3c":"code","8b07831e":"code","02c0e374":"code","65f3fb90":"code","a6505328":"code","7d02ec25":"code","ad00e5b3":"code","233ad18e":"code","aa2ea614":"code","963d0dfe":"code","8196cd6f":"code","50d8459e":"code","adc719c9":"code","f487b799":"code","51b5545d":"code","eff51761":"code","3e2b661b":"code","13757eff":"code","ddc4ebaf":"code","aa8f0142":"code","2e0c485e":"code","60397e26":"code","45c427bc":"code","503d91d1":"code","355f4a4e":"code","bdaa68c6":"code","878f7815":"code","8081802d":"code","5a3000a6":"code","df6b1e64":"markdown","d4964df7":"markdown","11301c6d":"markdown","74203f4c":"markdown","fff032fa":"markdown","c4326523":"markdown","36518dea":"markdown","1fd1d7fb":"markdown","e2bed0bb":"markdown","113e5129":"markdown","e2e02b9e":"markdown","abe515bf":"markdown","570990d7":"markdown","f7e7121e":"markdown","d7aba0b9":"markdown","c875a94d":"markdown","fb09c747":"markdown","da3dfa59":"markdown","00616ba7":"markdown","6ceeffac":"markdown","0bee02f6":"markdown","d67f8dc7":"markdown","9245d5ab":"markdown","7609bdbb":"markdown","862f0cb8":"markdown","b3017c0b":"markdown","b86d1173":"markdown","7f5a1ef3":"markdown","3a38118d":"markdown","3001c2a7":"markdown","c64ceac7":"markdown","92727717":"markdown","746cd8c8":"markdown","7c22a642":"markdown","dcd33f7f":"markdown","d8aa3507":"markdown","9d93df9e":"markdown"},"source":{"77690b50":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport cv2","ae0c92b0":"DATA_ROOT = '\/kaggle\/input\/lyft-udacity-challenge\/'","aa033f6c":"img = plt.imread(DATA_ROOT + 'dataA\/dataA\/CameraSeg\/02_00_000.png')\nplt.imshow(img[..., 0]);","6177272d":"np.unique(img * 255)","cdc879da":"labels = ['Unlabeled','Building','Fence','Other',\n          'Pedestrian', 'Pole', 'Roadline', 'Road',\n          'Sidewalk', 'Vegetation', 'Car','Wall',\n          'Traffic sign']","5aac37ff":"for i in range(13):\n    mask = plt.imread(DATA_ROOT + 'dataA\/dataA\/CameraSeg\/02_00_000.png') * 255\n    mask = np.where(mask == i, 255, 0)\n    mask = mask[:,:,0]\n    print(np.unique(mask))\n    plt.title(f'class: {i} {labels[i]}')\n    plt.imshow(mask)\n    plt.show()","c06a8faf":"cameraRGB = []\ncameraSeg = []\nfor root, dirs, files in os.walk(DATA_ROOT):\n    for name in files:\n        f = os.path.join(root, name)\n        if 'CameraRGB' in f:\n            cameraRGB.append(f)\n        elif 'CameraSeg' in f:\n            cameraSeg.append(f)\n        else:\n            break","30f6945b":"df = pd.DataFrame({'cameraRGB': cameraRGB, 'cameraSeg': cameraSeg})\n# \u041e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c  \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\ndf.sort_values(by='cameraRGB',inplace=True)\n\ndf.reset_index(drop=True, inplace=True)\n# \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u043f\u0435\u0440\u0432\u044b\u0435 \u043f\u044f\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043d\u0430\u0448\u0435\u0433\u043e \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430\ndf.head(5)","14cc77ea":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torch\nfrom torch.nn import functional as F","9ba13904":"class SelfDrivingDataset(Dataset):\n    def __init__(self, data, preprocessing=None):\n        # \u041f\u043e\u0434\u0430\u0435\u043c \u043d\u0430\u0448 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\n        self.data = data\n        \n        # \u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043d\u0430 rgb \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \n        self.image_arr = self.data.iloc[:,0]\n        # \u0438 \u043d\u0430 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n        self.label_arr = self.data.iloc[:,1]\n        \n        # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430-\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f\n        self.data_len = len(self.data.index)\n        \n        self.preprocessing = preprocessing\n\n        \n    def __getitem__(self, index):\n        # \u0427\u0438\u0442\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0438 \u0441\u0440\u0430\u0437\u0443 \u0436\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0435\u0435 \u0432 \u0432\u0438\u0434\u0435 numpy-\u043c\u0430\u0441\u0441\u0438\u0432\u0430 \n        # \u0440\u0430\u0437\u043c\u0435\u0440\u0430 600\u0445800 float-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n        img = cv2.cvtColor(cv2.imread(self.image_arr[index]), cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (256, 256))\n\n        img = np.asarray(img).astype('float')\n        \n        if self.preprocessing:\n            img = self.preprocessing(img)\n            img = torch.as_tensor(img)\n        else:\n            # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0432 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u0445 [0, 1]\n            img = torch.as_tensor(img) \/ 255.0\n        img = img.permute(2,0,1)\n        \n        # \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0443\u044e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443\n        masks = []\n        mask = cv2.cvtColor(cv2.imread(self.label_arr[index]), cv2.COLOR_BGR2RGB)\n#         mask = Image.open(self.label_arr[index])\n#         mask = mask.resize((256, 256))\n#         mask = np.asarray(mask)\n        \n        for i in range(13):\n            cls_mask = np.where(mask == i, 255, 0)\n            cls_mask = cls_mask.astype('float')\n            cls_mask = cv2.resize(cls_mask, (256, 256))\n\n            masks.append(cls_mask[:,:,0] \/ 255)\n            \n        masks = torch.as_tensor(masks, dtype=torch.uint8)    \n        \n            \n        return (img.float(), masks)\n\n    def __len__(self):\n        return self.data_len","f24d7da5":"dataset = SelfDrivingDataset(df)\nimg, masks = dataset[0]\nprint(img.shape, masks.shape)\nfig, ax = plt.subplots(1, 2, figsize=(15, 7))\nax[0].imshow(img.permute(1, 2, 0))\nax[1].imshow(masks.permute(1, 2, 0)[..., 10])\nplt.show()","6898e16b":"from sklearn.model_selection import train_test_split\n\n# 70 % \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443, 30 - \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e\nX_train, X_test = train_test_split(df, test_size=0.3)\n\n# \u0423\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0438\u0432\u0430\u0435\u043c \u0438\u043d\u0434\u0435\u043a\u0441\u0430\u0446\u0438\u044e\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\n# \u041e\u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u043a\u0430\u0436\u0434\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0432 \u043d\u0430\u0448 \u043a\u0430\u0441\u0442\u043e\u043c\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\ntrain_data = SelfDrivingDataset(X_train)\ntest_data = SelfDrivingDataset(X_test)","7064a947":"train_data_loader = DataLoader(\n    train_data,\n    batch_size=8,\n    shuffle=True\n)\ntest_data_loader = DataLoader(\n    test_data,\n    batch_size=4,\n    shuffle=False\n)","3c5b1d3c":"for img, target in train_data_loader:\n    print(img.shape, target.shape)\n    print(img[0].min(), img[0].max())\n    print(target[0].min(), target[0].max())\n    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n    ax[0].imshow(img[0].permute(1, 2, 0))\n    ax[1].imshow(target[0].permute(1, 2, 0)[..., 0])\n    break","8b07831e":"def crop_tensor(target_tensor, tensor):\n    target_size = target_tensor.size()[2]\n    tensor_size = tensor.size()[2]\n    delta = tensor_size - target_size\n    delta = delta \/\/ 2\n    \n    return tensor[:,:, delta:tensor_size-delta, delta:tensor_size-delta]","02c0e374":"import torch\nimport torch.nn as nn\n\nclass UNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n\n        # \u041b\u0435\u0432\u0430\u044f \u0441\u0442\u043e\u0440\u043e\u043d\u0430 (\u041f\u0443\u0442\u044c \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438)\n        self.down_conv_11 = self.conv_block(in_channels=3,\n                                            out_channels=64)\n        self.down_conv_12 = nn.MaxPool2d(kernel_size=2,\n                                         stride=2)\n        self.down_conv_21 = self.conv_block(in_channels=64,\n                                            out_channels=128)\n        self.down_conv_22 = nn.MaxPool2d(kernel_size=2,\n                                         stride=2)\n        self.down_conv_31 = self.conv_block(in_channels=128,\n                                            out_channels=256)\n        self.down_conv_32 = nn.MaxPool2d(kernel_size=2,\n                                         stride=2)\n        self.down_conv_41 = self.conv_block(in_channels=256,\n                                            out_channels=512)\n        self.down_conv_42 = nn.MaxPool2d(kernel_size=2,\n                                         stride=2)\n        \n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        \n        # \u041f\u0440\u0430\u0432\u0430\u044f \u0441\u0442\u043e\u0440\u043e\u043d\u0430 (\u041f\u0443\u0442\u044c \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438)\n        self.up_conv_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512,\n                                             kernel_size=3, stride=2,\n                                             padding=1, output_padding=1)\n        self.up_conv_12 = self.conv_block(in_channels=1024,\n                                          out_channels=512)\n        self.up_conv_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256,\n                                             kernel_size=3, stride=2,\n                                             padding=1, output_padding=1)\n        self.up_conv_22 = self.conv_block(in_channels=512,\n                                          out_channels=256)\n        self.up_conv_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n                                             kernel_size=3, stride=2,\n                                             padding=1, output_padding=1)\n        self.up_conv_32 = self.conv_block(in_channels=256,\n                                          out_channels=128)\n        self.up_conv_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n                                             kernel_size=3, stride=2,\n                                             padding=1, output_padding=1)\n        self.up_conv_42 = self.conv_block(in_channels=128,\n                                          out_channels=64)\n        \n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes,\n                                kernel_size=3, stride=1,\n                                padding=1)\n        self.softmax = nn.Softmax()\n    \n    @staticmethod\n    def conv_block(in_channels, out_channels):\n        block = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels,\n                      out_channels=out_channels,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.Conv2d(in_channels=out_channels,\n                      out_channels=out_channels,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    @staticmethod\n    def crop_tensor(target_tensor, tensor):\n        target_size = target_tensor.size()[2]\n        tensor_size = tensor.size()[2]\n        delta = tensor_size - target_size\n        delta = delta \/\/ 2\n\n        return tensor[:,:, delta:tensor_size-delta, delta:tensor_size-delta]\n\n\n    def forward(self, X):\n        # \u041f\u0440\u043e\u0445\u043e\u0434 \u043f\u043e \u043b\u0435\u0432\u043e\u0439 \u0441\u0442\u043e\u0440\u043e\u043d\u0435\n        x1 = self.down_conv_11(X) # [-1, 64, 256, 256]\n        x2 = self.down_conv_12(x1) # [-1, 64, 128, 128]\n        x3 = self.down_conv_21(x2) # [-1, 128, 128, 128]\n        x4 = self.down_conv_22(x3) # [-1, 128, 64, 64]\n        x5 = self.down_conv_31(x4) # [-1, 256, 64, 64]\n        x6 = self.down_conv_32(x5) # [-1, 256, 32, 32]\n        x7 = self.down_conv_41(x6) # [-1, 512, 32, 32]\n        x8 = self.down_conv_42(x7) # [-1, 512, 16, 16]\n        \n        middle_out = self.middle(x8) # [-1, 1024, 16, 16]\n\n        # \u041f\u0440\u043e\u0445\u043e\u0434 \u043f\u043e \u043f\u0440\u0430\u0432\u043e\u0439 \u0441\u0442\u043e\u0440\u043e\u043d\u0435\n        x = self.up_conv_11(middle_out) # [-1, 512, 32, 32]\n        y = self.crop_tensor(x, x7)\n        x = self.up_conv_12(torch.cat((x, y), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n        \n        x = self.up_conv_21(x) # [-1, 256, 64, 64]\n        y = self.crop_tensor(x, x5)\n        x = self.up_conv_22(torch.cat((x, y), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n        \n        x = self.up_conv_31(x) # [-1, 128, 128, 128]\n        y = self.crop_tensor(x, x3)\n        x = self.up_conv_32(torch.cat((x, y), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n        \n        x = self.up_conv_41(x) # [-1, 64, 256, 256]\n        y = self.crop_tensor(x, x1)\n        x = self.up_conv_42(torch.cat((x, y), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n        \n        output = self.output(x) # [-1, num_classes, 256, 256]\n        output = self.softmax(output)\n\n        return output","65f3fb90":"learning_rate = 0.001\nepochs = 1","a6505328":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","7d02ec25":"Umodel = UNet(num_classes=13).to(device)","ad00e5b3":"sample = (next(iter(train_data_loader)))\nsample[1].shape","233ad18e":"out = Umodel(sample[0].to(device))\nout.shape","aa2ea614":"plt.imshow(out[0][2].detach().cpu());","963d0dfe":"optimizer = torch.optim.Adam(Umodel.parameters())","8196cd6f":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, logits, targets):\n        smooth = 1\n        num = targets.size(0)\n        probs = logits\n        m1 = probs.reshape(num, -1)\n        m2 = targets.reshape(num, -1)\n        intersection = (m1 * m2)\n\n        score = (2. * intersection.sum(1) + smooth) \/ (m1.sum(1) + m2.sum(1) + smooth)\n        score = 1 - (score.sum() \/ num)\n        return score","50d8459e":"total_steps = len(train_data_loader)\nprint(f\"{epochs} epochs, {total_steps} total_steps per epoch\")","adc719c9":"criterion = DiceLoss()","f487b799":"#\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 time \u0434\u043b\u044f \u0440\u0430\u0441\u0447\u0435\u0442\u0430, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0443 \u043d\u0430\u0441 \u0443\u0445\u043e\u0434\u0438\u0442 \u043d\u0430 \u043e\u0434\u043d\u0443 \u044d\u043f\u043e\u0445\u0443\nimport time\n\n\n# \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0433\u043b\u0430\u0432\u043d\u044b\u0439 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0446\u0438\u043a\u043b\nepoch_losses = []\nfor epoch in range(epochs):\n    start_time = time.time()\n    epoch_loss = []\n    \n    for batch_idx, (data, labels) in enumerate(train_data_loader):\n        \n        data, labels = data.to(device), labels.to(device)        \n        \n        optimizer.zero_grad()\n        outputs = Umodel(data)                \n        \n        #loss = nn.CrossEntropyLoss(outputs,labels)# - torch.log(DiceLoss(outputs, labels))\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss.append(loss.item())\n        \n        if batch_idx % 200 == 0:\n            print(f'batch index : {batch_idx} | loss : {loss.item()}')\n\n    print(f'Epoch {epoch+1}, loss: ', np.mean(epoch_loss))\n    end_time = time.time()\n    print(f'Spend time for 1 epoch: {start_time - end_time} sec')\n    \n    epoch_losses.append(epoch_loss)","51b5545d":"save_model_path = '.\/Unet_Model_dice_loss.pth'","eff51761":"torch.save(Umodel.state_dict(), save_model_path)","3e2b661b":"net = UNet(13).to(device)\nnet.load_state_dict(torch.load(save_model_path))","13757eff":"def get_orig(image):\n    image = image.permute(1, 2, 0)\n    image = image.numpy()\n    image = np.clip(image, 0, 1)\n    return image","ddc4ebaf":"class_idx = 1\n\nfor i, data in enumerate(test_data_loader):\n    images, labels = data\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = net(images)\n    f, axarr = plt.subplots(1,3, figsize=(15, 6))\n\n    for j in range(0, 4):\n        axarr[0].imshow(outputs.squeeze().detach().cpu().numpy()[j,class_idx,:,:])\n        axarr[0].set_title('Guessed labels')\n        axarr[1].imshow(labels.detach().cpu().numpy()[j,class_idx, :,:])\n        axarr[1].set_title('Ground truth labels')\n\n        original = get_orig(images[j].cpu())\n        axarr[2].imshow(original)\n        axarr[2].set_title('Original Images')\n        plt.show()\n    if i > 3:\n        break","aa8f0142":"!pip install segmentation-models-pytorch","2e0c485e":"import segmentation_models_pytorch as smp\n\n# \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\nBACKBONE = 'resnet34'\nsegmodel = smp.Unet(BACKBONE, classes=13, activation='softmax').to(device)\npreprocess_input = smp.encoders.get_preprocessing_fn(BACKBONE, pretrained='imagenet')","60397e26":"dataset = SelfDrivingDataset(df, preprocessing=preprocess_input)\nimg, masks = dataset[0]\nprint(img.shape, masks.shape)\nfig, ax = plt.subplots(1, 2, figsize=(15, 7))\nax[0].imshow(img.permute(1, 2, 0))\nax[1].imshow(masks.permute(1, 2, 0)[..., 10])\nplt.show()","45c427bc":"# 70 % \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443, 30 - \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e\nX_train, X_test = train_test_split(df, test_size=0.3)\n\n# \u0423\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0438\u0432\u0430\u0435\u043c \u0438\u043d\u0434\u0435\u043a\u0441\u0430\u0446\u0438\u044e\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\n# \u041e\u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u043a\u0430\u0436\u0434\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0432 \u043d\u0430\u0448 \u043a\u0430\u0441\u0442\u043e\u043c\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\ntrain_data = SelfDrivingDataset(X_train,\n                                preprocessing=preprocess_input)\ntest_data = SelfDrivingDataset(X_test,\n                               preprocessing=preprocess_input)","503d91d1":"train_data_loader = DataLoader(\n    train_data,\n    batch_size=8,\n    shuffle=True\n)\ntest_data_loader = DataLoader(\n    test_data,\n    batch_size=4,\n    shuffle=False\n)","355f4a4e":"for img, target in train_data_loader:\n    print(img.shape, target.shape)\n    print(img[0].min(), img[0].max())\n    print(target[0].min(), target[0].max())\n    break","bdaa68c6":"criterion = smp.utils.losses.DiceLoss()\nmetrics = [smp.utils.metrics.IoU(),]\n\noptimizer = torch.optim.Adam(params=segmodel.parameters(), lr=0.001)","878f7815":"# it is a simple loop of iterating over dataloader`s samples\ntrain_epoch = smp.utils.train.TrainEpoch(\n    segmodel, \n    loss=criterion, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=device,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    segmodel, \n    loss=criterion, \n    metrics=metrics, \n    device=device,\n    verbose=True,\n)","8081802d":"# train model\n\nmax_score = 0\n\nfor i in range(0, 1):\n    print(f'Epoch: {i + 1}')\n    train_logs = train_epoch.run(train_data_loader)\n    valid_logs = valid_epoch.run(test_data_loader)\n    \n    # do something (save model, change lr, etc.)\n    if max_score < valid_logs['iou_score']:\n        max_score = valid_logs['iou_score']\n        torch.save(segmodel, '.\/best_model.pth')\n        print('Model saved!')","5a3000a6":"class_idx = 1\n\nfor i, data in enumerate(test_data_loader):\n    images, labels = data\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = net(images)\n    f, axarr = plt.subplots(1,3, figsize=(15, 6))\n\n    for j in range(0, 4):\n        axarr[0].imshow(outputs.squeeze().detach().cpu().numpy()[j,class_idx,:,:])\n        axarr[0].set_title('Guessed labels')\n        axarr[1].imshow(labels.detach().cpu().numpy()[j,class_idx, :,:])\n        axarr[1].set_title('Ground truth labels')\n\n        original = get_orig(images[j].cpu())\n        axarr[2].imshow(original)\n        axarr[2].set_title('Original Images')\n        plt.show()\n    if i > 3:\n        break","df6b1e64":"\u0428\u0430\u0433 2.\n\n\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441 Unet() \u0438 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0441\u043b\u043e\u0438 \u043b\u0435\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u0438 maxpool \u0441\u043b\u043e\u0438. \u0412 \u043a\u0430\u0436\u0434\u043e\u043c \u0441\u043b\u043e\u0435 \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c conv_block(). \u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u043d\u0430\u0437\u043e\u0432\u0435\u043c  \u0441\u043b\u043e\u0438 conv_down (4 \u0441\u043b\u043e\u044f \u0432 \u043b\u0435\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438): \n\n\n\n```\nclass Unet(nn.Module):\n    def __init__(self, num_classes):\n        super(Unet, self).__init__()\n        \n        self.num_classes = num_classes\n        self.down_conv_11 = conv_block(in_channels=3, out_channels=64)\n        self.down_conv_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_21 = conv_block(in_channels=64, out_channels=128)\n        self.down_conv_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_31 = conv_block(in_channels=128, out_channels=256)\n        self.down_conv_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_41 = conv_block(in_channels=256, out_channels=512)\n        self.down_conv_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.middle = conv_block(in_channels=512, out_channels=1024)\n```\n\n","d4964df7":"\u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0441\u0430\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f:","11301c6d":"\u0422\u0435\u043f\u0435\u0440\u044c \u0437\u0430\u0432\u0435\u0440\u043d\u0435\u043c \u044d\u0442\u0438 \u0434\u0432\u0430 \u0441\u043f\u0438\u0441\u043a\u0430 \u0432 DataFrame \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 pandas.\n\u0412 \u0438\u0442\u043e\u0433\u0435 \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u043f\u0435\u0440\u0432\u044b\u0435 \u043f\u044f\u0442\u044c \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0435\u0433\u043e\u0441\u044f \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430:","74203f4c":"\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043a\u0430\u0441\u0442\u043e\u043c\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430:","fff032fa":"\u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u043b\u0430\u0441\u0441\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044e forward(), \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043c\u044b \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043c \u0432\u0445\u043e\u0434\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0432 \u043b\u0435\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c:\n\n\n```\n        def forward(self, X):\n        \n            x1 = self.down_conv_11(X) # [-1, 64, 256, 256]\n            x2 = self.down_conv_12(x1) # [-1, 64, 128, 128]\n            x3 = self.down_conv_21(x2) # [-1, 128, 128, 128]\n            x4 = self.down_conv_22(x3) # [-1, 128, 64, 64]\n            x5 = self.down_conv_31(x4) # [-1, 256, 64, 64]\n            x6 = self.down_conv_32(x5) # [-1, 256, 32, 32]\n            x7 = self.down_conv_41(x6) # [-1, 512, 32, 32]\n            x8 = self.down_conv_42(x7) # [-1, 512, 16, 16]\n            middle_out = self.middle(x8) # [-1, 1024, 16, 16]\n```\n\n","c4326523":"## \u0427\u0430\u0441\u0442\u044c 2. \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","36518dea":"\u0428\u0430\u0433 4.\n\n\u0422\u0435\u043f\u0435\u0440\u044c \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0437\u0430\u0434\u0435\u043a\u043b\u0430\u0440\u0438\u0440\u0443\u0435\u043c 4 \u0441\u043b\u043e\u044f \u043f\u0440\u0430\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044e\u044e 1\u04451 conv \u0432 \u043d\u0430\u0448\u0435\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 __init__() \u043a\u043b\u0430\u0441\u0441\u0430. \u0412\u043c\u0435\u0441\u0442\u043e maxpool \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c 2\u04452 transpose convolution, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u0432\u044b\u0448\u0430\u0442\u044c \u043d\u0430\u0448\u0443 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c:","1fd1d7fb":"\u0428\u0430\u0433 1.\n\n\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043d\u0430\u0448 \u0434\u0430\u0442\u0430\u0441\u0435\u0442. \u0412\u043d\u0443\u0442\u0440\u0438 \u043a\u0443\u0447\u0430 \u0441\u0445\u043e\u0436\u0438\u0445 \u043f\u043e \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044e \u043f\u0430\u043f\u043e\u043a, \u043a\u0430\u0436\u0434\u0430\u044f \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438. \n\u041d\u043e \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0432\u044b\u0434\u0435\u043b\u0438\u0442\u044c \u0432 \u044d\u0442\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0434\u0432\u0430 \u0432\u0438\u0434\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a. \n<br>1) \u042d\u0442\u043e \u043e\u0431\u044b\u0447\u043d\u044b\u0435 \u0446\u0432\u0435\u0442\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438.\n\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 dataA\/dataA\/CameraRGB\/02_00_000.png\n<br>2) \u0418 \u0435\u0441\u0442\u044c \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 \u043d\u0438\u043c\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0440\u0430\u0437\u0431\u0438\u0442\u044b\u0435 \u043d\u0430 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u0441 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u043c\u0438 \u044f\u0440\u043a\u043e\u0441\u0442\u044f\u043c\u0438 \u043f\u0438\u043a\u0441\u0435\u043b\u0435\u0439.\n<br>\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 dataA\/dataA\/CameraSeg\/02_00_000.png, \u0432 \u043d\u0435\u0439 \u0432\u0441\u0435 \u0442\u043e\u0436\u0435 \u0441\u0430\u043c\u043e\u0435, \u0447\u0442\u043e \u0438 \u0432 \u043f\u0435\u0440\u0432\u043e\u0439, \u043d\u043e  \u043e\u043d\u0430 \u043f\u0440\u043e\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0430.\n\n\u0418 \u0435\u0449\u0435 \u0437\u0430\u043c\u0435\u0442\u0438\u043c, \u0447\u0442\u043e \u043d\u0438\u0433\u0434\u0435 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043d\u0435\u0442 \u044f\u0432\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u043a\u043b\u0430\u0441\u0441\u0430\u0445. \u041c\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u0434\u0430\u0442\u044c \u0438\u043c \u0438\u043c\u0435\u043d\u0430 \u0441\u0430\u043c\u0438.","e2bed0bb":"\u041f\u043e\u0434 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u043c \u043c\u044b \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u043c \u0441\u043a\u0430\u0440\u043c\u043b\u0438\u0432\u0430\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u044e\u0449\u0435\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u044e\u0449\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c (\u0446\u0435\u043b\u0435\u0432\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f):","113e5129":"## \u0427\u0430\u0441\u0442\u044c 1. \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","e2e02b9e":"\u0412 \u0438\u0442\u043e\u0433\u0435 \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0443 \u043d\u0430\u0441 13 \u043a\u043b\u0430\u0441\u0441\u043e\u0432. \u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0442\u044c \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u043c\u0430\u0441\u043a\u0443 \n\u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043a\u043e\u0434 \u043d\u0438\u0436\u0435:","abe515bf":"\u0412\u044b\u0431\u0435\u0440\u0435\u043c \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e,\u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c:","570990d7":"```\nimport torch\nimport torch.nn as nn\n\ndef conv_block(in_channels,  out_channels):\n    conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3),\n        nn.ReLU(),\n        nn.BatchNorm2d(num_features=out_channels),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3),\n        nn.ReLU(),\n        nn.BatchNorm2d(num_features=out_channels)\n    )\n    return conv\n```","f7e7121e":"\u0417\u0430\u0442\u0435\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u043d\u0430\u0448 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0438.\n\u0418 \u043e\u0431\u0435\u0440\u043d\u0435\u043c \u0438\u0445 \u0432 \u043d\u0430\u0448 \u043a\u0430\u0441\u0442\u043e\u043c\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441.","d7aba0b9":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c Unet \u0434\u043b\u044f 13 \u043a\u043b\u0430\u0441\u0441\u043e\u0432:","c875a94d":"\u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c:","fb09c747":"\u0415\u0441\u043b\u0438 \u0443 \u0432\u0430\u0441 \u043d\u0435\u0442 \u044d\u0442\u043e\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438, \u0442\u043e \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u0432\u0430\u043c \u043d\u0430\u0434\u043e \u0435\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u0447\u0435\u0440\u0435\u0437 pip","da3dfa59":"\u0412\u043e\u0442, \u043e\u0442\u043b\u0438\u0447\u043d\u043e. \u041c\u044b \u0441\u043e\u0437\u0434\u0430\u043b\u0438 \u043b\u0435\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438. \u041e\u0441\u0442\u0430\u043b\u043e\u0441\u044c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0430\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c.","00616ba7":"## \u0427\u0430\u0441\u0442\u044c 3. \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435","6ceeffac":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445:","0bee02f6":"```\n        self.up_conv_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512,kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_12 = conv_block(in_channels=1024, out_channels=512)\n        self.up_conv_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_22 = conv_block(in_channels=512, out_channels=256)\n        self.up_conv_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_32 = conv_block(in_channels=256, out_channels=128)\n        self.up_conv_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_42 = conv_block(in_channels=128, out_channels=64)\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n```\n","d67f8dc7":"\u041a\u0430\u043a \u043c\u044b \u043e\u0442\u043c\u0435\u0442\u0438\u043b\u0438 \u0440\u0430\u043d\u0435\u0435, \u0432 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0435 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 3\u04453 \u0434\u0432\u043e\u0439\u043d\u043e\u0439 \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0439 \u0441\u043b\u043e\u0439 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0437\u0430 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 Relu \u0432 \u043e\u0431\u0435\u0438\u0445 \u0447\u0430\u0441\u0442\u044f\u0445 \u0441\u0435\u0442\u043a\u0438.\n","9245d5ab":"\u0428\u0430\u0433 1.\n\n\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e conv_block(), \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0438 \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u0430\u043d\u0430\u043b\u043e\u0432. \u0412\u043d\u0443\u0442\u0440\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u043b\u043e\u0438 \u0441 \u044f\u0434\u0440\u043e\u043c 3 (3\u04453) \u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u0440\u0435\u0434\u0448\u0435\u0441\u0442\u0432\u0443\u0435\u0442 Relu \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0438 \u0434\u043b\u044f \u043b\u0443\u0447\u0448\u0435\u0439 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0441\u043b\u043e\u0438 BatchNorm2d:","7609bdbb":"\u0428\u0430\u0433 3. \n\n\u0422\u0435\u043f\u0435\u0440\u044c \u043e\u0431\u0435\u0440\u043d\u0435\u043c \u0432\u0441\u0435 \u0432 \u043a\u0430\u0441\u0442\u043e\u043c\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u0443\u0434\u043e\u0431\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 PyTorch.","862f0cb8":"\u0428\u0430\u0433 5.\n\n\u041a\u0430\u043a \u043c\u044b \u0432\u0438\u0434\u0438\u043c, \u0432 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0435 \u0432\u0445\u043e\u0434\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0432 \u043f\u0440\u0430\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 - \u044d\u0442\u043e \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043b\u0435\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438\n\u0438 \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0433\u043e \u0441\u043b\u043e\u044f. \u041d\u043e \u0434\u043b\u044f \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u043e\u043d\u0438 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0445 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u0432. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e crop_tensor() \u0434\u043b\u044f \u0432\u044b\u0440\u0435\u0437\u0430\u043d\u0438\u044f \u044d\u0442\u0438\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439. \u0412\u043d\u0443\u0442\u0440\u0438 \u044d\u0442\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043c\u044b \u043f\u043e\u0434\u0440\u0430\u0437\u0443\u043c\u0435\u0432\u0430\u0435\u043c, \u0447\u0442\u043e \u043d\u0430\u0448\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f - \u044d\u0442\u043e \u0442\u0435\u043d\u0437\u043e\u0440\u044b.","b3017c0b":"\u0427\u0442\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 crop_tensor() ?\n\ntensor = \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0441 \u043b\u0435\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u0431\u0440\u0435\u0437\u0430\u0442\u044c\ntarget tensor = \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0432 \u043f\u0440\u0430\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u0432\u044b\u0440\u0435\u0437\u0430\u043d\u043d\u044b\u043c \u043b\u0435\u0432\u044b\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\u043c\n\n\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u043e\u0431\u043e\u0438\u0445 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432 target_size \u0438 tensor_size, \u0442.\u043a. \u0438\u0445 \u0432\u044b\u0441\u043e\u0442\u0430 \u0438 \u0448\u0438\u0440\u0438\u043d\u0430 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b. \n\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440: x=torch.Size([1,512,64,64]), \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c x[2] = 64\n\n\u0422\u0435\u043f\u0435\u0440\u044c \u043c\u044b \u0438\u043c\u0435\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u043e\u0431\u043e\u0438\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, \u0432\u044b\u0447\u0442\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440 \u043c\u0435\u043d\u044c\u0448\u0435\u0433\u043e \u0442\u0435\u043d\u0437\u043e\u0440\u0430 \u0438\u0437 \u0431\u043e\u043b\u044c\u0448\u0435\u0433\u043e. \u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0438\u043c\ntarget_size = 56 \u0438 tensor_size = 64 -> delta(\u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u043c\u0438) \u0431\u0443\u0434\u0435\u0442 8.\n\n\u041d\u043e \u043c\u044b \u0432\u0435\u0434\u044c \u0431\u0443\u0434\u0435\u043c \u0432\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0438\u0437 \u0432\u0441\u0435\u0445 \u0443\u0433\u043b\u043e\u0432 'height' * 'width', \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c delta \u043d\u0430 2. \n\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, height \u0438 width \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0432\u044b\u0440\u0435\u0437\u0430\u043d\u044b \u0440\u0430\u0432\u043d\u043e:\n    8 => h * w=4 * 4\n\n\u0442\u0435\u043f\u0435\u0440\u044c \u0432\u0435\u0440\u043d\u0435\u043c \u0432\u044b\u0440\u0435\u0437\u0430\u043d\u043d\u044b\u0439 \u0442\u0435\u043d\u0437\u043e\u0440\n[:,:,] = \u0432\u0441\u0435 \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u044f\n[delta:tensor_size-delta, delta:tensor_size-delta] = \u0432\u044b\u0440\u0435\u0437\u0430\u043d\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\n\n[4:64-4, 4:64-4] => 4:60, 4:60 \n\u0432 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0432\u044b\u0448\u0435 \u043d\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 56\u044556\n\n\u041d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \u043d\u0438\u0436\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u043d \u043f\u0440\u0438\u043c\u0435\u0440 \u0432\u044b\u0440\u0435\u0437\u0430\u043d\u043d\u043e\u0439 \u0432\u044b\u0441\u043e\u0442\u044b:","b86d1173":"\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0433\u043e\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u043e\u0431\u0443\u0447\u0438\u0442\u044c. \u041f\u0440\u0438\u0448\u043b\u043e \u0432\u0440\u0435\u043c\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0439 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440.","7f5a1ef3":"\u0422\u0435\u043f\u0435\u0440\u044c \u0434\u043b\u044f \u0432\u0438\u0434\u0430 \u0437\u0430\u043f\u0438\u0448\u0435\u043c \u043d\u0430\u0448\u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u044b\u0435 \u0440\u0430\u043d\u0435\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0432\u043d\u0443\u0442\u0440\u044c \u043a\u043b\u0430\u0441\u0441\u0430. \u0412 \u0438\u0442\u043e\u0433\u0435 \u043d\u0430\u0448 \u043a\u043b\u0430\u0441\u0441 Unet \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:","3a38118d":"\n\n```\n        x = self.up_conv_11(middle_out) # [-1, 512, 32, 32]\n        y = crop_tensor(x, x7)\n        x = self.up_conv_12(torch.cat((x, y), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n        \n        x = self.up_conv_21(x) # [-1, 256, 64, 64]\n        y = crop_tensor(x, x5)\n        x = self.up_conv_22(torch.cat((x, y), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n        \n        x = self.up_conv_31(x) # [-1, 128, 128, 128]\n        y = crop_tensor(x, x3)\n        x = self.up_conv_32(torch.cat((x, y), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n        \n        x = self.up_conv_41(x) # [-1, 64, 256, 256]\n        y = crop_tensor(x, x1)\n        x = self.up_conv_42(torch.cat((x, y), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n        \n        output = self.output(x) # [-1, num_classes, 256, 256]\n        \n        return output\n```\n\n","3001c2a7":"\u041d\u0430\u0439\u0434\u0435\u043c \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0438\u0441\u043a\u0441\u0435\u043b\u0435\u0439 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435, \u0438 \u043a\u0430\u0436\u0434\u043e\u0435 \u0442\u0430\u043a\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0431\u0443\u0434\u0435\u0442 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0446\u0435\u043b\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443.","c64ceac7":"\u0428\u0430\u0433 6.\n\n\u0422\u0435\u043f\u0435\u0440\u044c \u0434\u043e\u043f\u0438\u0448\u0435\u043c \u043d\u0430\u0448 forward \u043f\u0440\u0430\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438.\n\n\n\u041a\u043e\u043c\u0431\u0438\u043d\u0438\u0440\u0443\u0435\u043c \u043e\u0431\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f torch.cat() \u0438 \u043f\u043e\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0432 up_conv():","92727717":"\u0418 \u0442\u0435\u043f\u0435\u0440\u044c \u0443\u0436\u0435 \u043e\u0431\u0435\u0440\u043d\u0435\u043c \u0442\u043e, \u0447\u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u0432 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u043d\u0430\u043c \u0432 pytorch \u0434\u0430\u0442\u0430\u043b\u043e\u0430\u0434\u0435\u0440\u044b:","746cd8c8":"\u0428\u0430\u0433 2.\n\n\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043c \u043d\u0430\u0448 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043a \u0443\u0434\u043e\u0431\u043d\u043e\u043c\u0443 \u0432\u0438\u0434\u0443, \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0432\u0441\u0435 \u043d\u0430 \u0434\u0432\u0430 \u0441\u043f\u0438\u0441\u043a\u0430 \u0441 rgb \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438 \u0438 seg.","7c22a642":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0448\u0430\u0433\u043e\u0432 \u0432\u043d\u0443\u0442\u0440\u0438 \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0438:","dcd33f7f":"\u0421\u0442\u043e\u0438\u0442 \u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0447\u0442\u043e \u0443\u0436\u0435 \u0435\u0441\u0442\u044c \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f Unet \u0432 PyTorch. \u041e\u043d\u0430 \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0432 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 [segmentation_models_pytorch](https:\/\/segmentation-modelspytorch.readthedocs.io\/en\/latest\/index.html)","d8aa3507":"<img src='https:\/\/drive.google.com\/uc?export=view&id=1AURG8EdTu1OHHj8nxSRhEsrGqc4WNb5V' width=500>","9d93df9e":"\u0412 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u043d\u0430\u0448\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u0430\u0432\u0442\u043e\u043f\u0438\u043b\u043e\u0442\u0438\u0440\u0443\u0435\u043c\u044b\u0445 \u043c\u0430\u0448\u0438\u043d."}}