{"cell_type":{"e418bee5":"code","7e9c47b4":"code","4baeb39e":"code","8d13e410":"code","6c2a681c":"code","25f39f72":"code","6c138f66":"code","09dff4e9":"code","41abb555":"code","754ac6c3":"code","b3db927f":"code","f4195b6e":"code","16aac1c4":"code","512b44e0":"code","8233649a":"code","b4c4bdf2":"code","7e065149":"code","8afbb39f":"code","a345c9f7":"code","c7971731":"code","10466d30":"code","f92185d8":"code","eb871336":"code","69d1f00e":"code","6cdd5744":"code","0bd7e807":"code","13b9464b":"code","df082b41":"code","96c5a5b4":"code","6bdda118":"code","6cd7438d":"code","5474aa07":"code","61ed224b":"code","5784a2c9":"code","6c3cfeb0":"code","ff1b4b13":"code","c0ca335d":"code","37eb4d8e":"code","af476e02":"code","5131c4aa":"code","53a5502b":"code","a139d8e6":"code","6ee5cca3":"code","7334aa8d":"code","7dc7bec7":"code","ec3ccc68":"code","6d0c7a6a":"code","56ad0fb2":"code","973b1914":"code","92b26d96":"code","0643cb28":"code","439d6d20":"code","8b983ff5":"code","db978839":"code","9e23c185":"code","cb0e3eaa":"code","889dc724":"code","5bb00322":"code","6d8ea026":"code","658bec33":"code","099b024d":"code","12ccfde7":"code","826aa0f7":"code","c5e8d931":"code","3e42eb93":"code","eab771f2":"code","7f34469e":"code","6e32a62e":"code","c6971928":"code","75b3ccd3":"code","36e129b5":"code","eec35423":"code","1d3bb671":"code","dfb79018":"code","52a93473":"code","2d471258":"code","d2332bf0":"code","cc623f10":"code","34a2f1da":"code","44a4affd":"code","b3efbfa2":"markdown","2e52ac71":"markdown","c616af26":"markdown","8ddfaff7":"markdown","7fe6c415":"markdown","b851f47c":"markdown","c280003b":"markdown","154bee33":"markdown","40266256":"markdown","17e4bdc4":"markdown","cafc8b39":"markdown","564a7ec9":"markdown","65e1a960":"markdown"},"source":{"e418bee5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e9c47b4":"!pip install cvxopt","4baeb39e":"import os\nimport glob\nimport matplotlib.pyplot as plt\nfrom random import random\nfrom random import randint\n#from glob import  glob\nfrom cvxopt import matrix\nfrom cvxopt import solvers\nfrom tabulate import tabulate","8d13e410":"sample='OS'\nlist_group = glob.glob('..\/input\/10-alphas-for-testing\/group1\/*.csv'.format(sample))\nlist_group.sort()\nlist_nomal = glob.glob('..\/input\/10-alphas-for-testing\/group2\/*.csv'.format(sample))\nlist_nomal.sort()\nprint(list_nomal)\nfileList = list_group + list_nomal\nprint(fileList)\nprint(len(fileList))","6c2a681c":"sample = \"OS\"\nm = pd.DataFrame()\n    #it= infoTest()\nfor file in fileList:\n    tempDf = pd.read_csv(file,parse_dates=['datetime'],index_col=3)\n        # print(file,tempDf)\n    tempPnl = tempDf[['value']]\n    tempPnl = tempPnl[tempPnl.index.dayofweek < 5]\n    tempPnl['ret'] = (tempPnl.value - tempPnl.value.shift(1))\n    tempPnl = tempPnl[['ret']].resample(\"1D\").apply(lambda x : x.sum() if len(x) else np.nan).dropna(how=\"all\")\n        # print(\"strat \" + file[9:], calculate_sharp(merge=tempPnl))\n    if len(m) == 0:\n        m = tempPnl\n    else:\n        m = pd.merge(m, tempPnl, how='inner', left_index=True, right_index=True)\ncolList = []\nfor i in fileList:\n    colList.append(i.split('{}_'.format(sample))[-1][55:-4])\nm.columns = colList\n#print(m)","25f39f72":"print(fileList)","6c138f66":"m.columns=['alpha_41_1h','correlation_rank','gm_rsi_high_low_15m','percent_rank_bank_30m',\n'vn30f1m_kc_thep_not_close_5m','VolImbalance','bank_close_volume_5m','compare_ohlc_30m','rsi_bb_refractor','standing_wave_20m']","09dff4e9":"#m.columns=['alpha_41_1h','correlation_rank','second_last_int_close_15m','vn30f1m_kc_thep_not_close_5m',\n# 'Tcal_in_day','VOI','compare_ohlc_30m','percent_rank_bank_30m','rsi_bb_refractor','standing_wave_20m']","41abb555":"#m.columns","754ac6c3":"#m.columns=['alpha_41_1h','correlation_rank','second_last_int_close_15m','vn30f1m_kc_thep_not_close_5m',\n #         'Tcal_in_day','VOI','bank_close_volume_5m','compare_ohlc_30m','rsi_bb_refractor','standing_wave_20m']","b3db927f":"#m.columns=['alpha_41_1h','correlation_rank','second_last_int_close_15m','vn30f1m_kc_thep_not_close_5m',\n          #'VOI','bank_close_volume_5m','compare_ohlc_30m','rsi_bb_refractor','standing_wave_20m']","f4195b6e":"m=m\/(10**6)","16aac1c4":"m.tail()","512b44e0":"m.head()","8233649a":"#Train \ntrain_1=m['2020-01-01':'2020-12-31']\ntrain_2=m['2020-02-01':'2021-01-31']\ntrain_3=m['2020-03-01':'2021-02-28']\ntrain_4=m['2020-04-01':'2021-03-31']\ntrain_5=m['2020-05-01':'2021-04-30']\ntrain_6=m['2020-06-01':'2021-05-31']\ntrain_7=m['2020-07-01':'2021-06-30']\ntrain_8=m['2020-08-01':]","b4c4bdf2":"test_1=m['2021-01-01':'2021-01-31']\ntest_2=m['2021-02-01':'2021-02-28']\ntest_3=m['2021-03-01':'2021-03-31']\ntest_4=m['2021-04-01':'2021-04-30']\ntest_5=m['2021-05-01':'2021-05-31']\ntest_6=m['2021-06-01':'2021-06-30']\ntest_7=m['2021-07-01':]","7e065149":"#Export train sets\ntrain_1.to_csv('future_train_1.csv')\ntrain_2.to_csv('future_train_2.csv')\ntrain_3.to_csv('future_train_3.csv')\ntrain_4.to_csv('future_train_4.csv')\ntrain_5.to_csv('future_train_5.csv')\ntrain_6.to_csv('future_train_6.csv')\ntrain_7.to_csv('future_train_7.csv')\ntrain_8.to_csv('future_train_8.csv')\n\n\n\n\n\n","8afbb39f":"#Export test set\ntest_1.to_csv('future_test_1.csv')\ntest_2.to_csv('future_test_2.csv')\ntest_3.to_csv('future_test_3.csv')\ntest_4.to_csv('future_test_4.csv')\ntest_5.to_csv('future_test_5.csv')\ntest_6.to_csv('future_test_6.csv')\ntest_7.to_csv('future_test_7.csv')\n","a345c9f7":"simul_G1=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X1.csv')\nsimul_G1=simul_G1.iloc[:,1:]\n\nsimul_G2=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X2.csv')\nsimul_G2=simul_G2.iloc[:,1:]\n\n\nsimul_G3=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X3.csv')\nsimul_G3=simul_G3.iloc[:,1:]\n\n\nsimul_G4=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X4.csv')\nsimul_G4=simul_G4.iloc[:,1:]\n\n\nsimul_G5=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X5.csv')\nsimul_G5=simul_G5.iloc[:,1:]\n\n\nsimul_G6=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X6.csv')\nsimul_G6=simul_G6.iloc[:,1:]\n\n\nsimul_G7=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X7.csv')\nsimul_G7=simul_G7.iloc[:,1:]\n\nsimul_G8=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X8.csv')\nsimul_G8=simul_G8.iloc[:,1:]","c7971731":"simul_C1=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X1.csv')\nsimul_C1=simul_C1.iloc[:,1:]\n\n\nsimul_C2=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X2.csv')\nsimul_C2=simul_C2.iloc[:,1:]\n\n\nsimul_C3=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X3.csv')\nsimul_C3=simul_C3.iloc[:,1:]\n\n\nsimul_C4=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X4.csv')\nsimul_C4=simul_C4.iloc[:,1:]\n\n\nsimul_C5=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X5.csv')\nsimul_C5=simul_C5.iloc[:,1:]\n\n\nsimul_C6=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X6.csv')\nsimul_C6=simul_C6.iloc[:,1:]\n\n\nsimul_C7=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X7.csv')\nsimul_C7=simul_C7.iloc[:,1:]\n\nsimul_C8=pd.read_csv('..\/input\/new-phung-simulation-10-alphas\/X8.csv')\nsimul_C8=simul_C8.iloc[:,1:]","10466d30":"class infoTest:\n    def __init__(self):\n        pass\n    def calculateSharpe(self,npArray):\n        sr = npArray.mean()\/npArray.std() * np.sqrt(252)\n        print(npArray.std())\n        return sr\n    def max_drawdown(self,booksize,returnSeries):\n        mdd = 0\n        a = np.cumsum(returnSeries)\n        X = a+booksize\n        peak = X[0]\n        dds = []\n        for x in X:\n            if x > peak:\n                peak = x\n            dd = (peak - x) \/ booksize\n            if dd > mdd:\n                mdd = dd\n                dds.append(X[X == x])\n        print(\"MDD AT \",dds[-1].index[0] if len(dds) else None)\n        print(X)\n        # X.to_csv(r'\/home\/hoainam\/PycharmProjects\/multi_strategy\/v_multi\/X.csv')\n        return mdd\n        # rets is array of returns\n    def randomAllocateWeigh(self,rets):\n        remaining = 1\n        weigh = []\n        for i in range(len(rets)):\n            tempWeigh = round(random(),2)\n            weigh.append(tempWeigh)\n            remaining = remaining-tempWeigh\n        weigh = np.asarray(weigh) \/ np.sum(weigh)\n        # print(np.sum(weigh))\n        portfolio = []\n        for i in range(len(rets)):\n            if len(portfolio) ==0:\n                portfolio = rets[i] * weigh[i]\n            else:\n                portfolio += rets[i] * weigh[i]\n        # portfolio = np.asarray(portfolio)\/np.sum(weigh)\n        return weigh,portfolio\n    def randomAllocateListReturns(self,df):\n        remaining = 1\n        weigh = []\n        counter =0\n        ret = []\n        portfolio = []\n        for (columnName, columnData) in df.iteritems():\n            tempWeigh = round(random(), 2)\n            weigh.append(tempWeigh)\n        weigh = np.asarray(weigh) \/ np.sum(weigh)\n        # print(np.sum(weigh))\n        portfolio = []\n        counter = 0\n        for (columnName, columnData) in df.iteritems():\n            if len(portfolio) == 0:\n                portfolio = columnData * weigh[counter]\n            else:\n                portfolio += columnData * weigh[counter]\n            counter+=1\n        # portfolio = np.asarray(portfolio)\/np.sum(weigh)\n        return weigh, portfolio\n    def allocateForMaxSharpe(self,df,itertimes):\n        maxSharpe = 0\n        maxWeigh = []\n        finalPnl = []\n        for i in range(itertimes):\n            weigh, mergePnl = self.randomAllocateListReturns(df)\n            tempSharpe = self.calculateSharpe(mergePnl)\n            if tempSharpe >= maxSharpe:\n                maxSharpe = tempSharpe\n                maxWeigh = weigh\n                finalPnl = mergePnl\n        # maxWeigh = np.asarray(maxWeigh) \/ np.sum(maxWeigh)\n        return maxSharpe, maxWeigh,finalPnl\n    def allocateForMinDD(self,df,itertimes,booksize):\n        minDD = 1;\n        minDDWeigh = []\n        finalPnl = []\n        for i in range(itertimes):\n            weigh, mergePnl = self.randomAllocateListReturns(df)\n            tempDD = self.max_drawdown(booksize,mergePnl)\n            if tempDD < minDD:\n                minDD = tempDD\n                minDDWeigh = weigh\n                finalPnl = mergePnl\n        return minDD, minDDWeigh,finalPnl","f92185d8":"def allocate_weight_sharp_with_bounded_group_alpha(list_group,list_normal,dataframe,booksize,upperbound, bounded_list,df2):\n    #print(tabulate(dataframe.corr(method='pearson'), tablefmt=\"pipe\", headers=\"keys\"))\n    # upperbound = 0.3\n    it = infoTest()\n    cov = (dataframe.cov()).to_numpy()\n    meanvec = (dataframe.mean()).to_numpy()\n    P = matrix(cov, tc='d')\n    # print(P)\n    q = matrix(np.zeros(len(meanvec)), (len(meanvec), 1), tc='d')\n    G = []\n    for i in range(len(meanvec)):\n        k = [0 for x in range(len(meanvec) - 1)]\n        k.insert(i, -1)\n        G.append(k)\n    for i in range(len(meanvec)):\n        k = [-upperbound for x in range(len(meanvec) - 1)]\n        k.insert(i, 1 - upperbound)\n        G.append(k)\n    k = [-bounded_list for i in range(len(list_normal))]\n    for i in range(len(list_group)):\n        k.insert(i,1-bounded_list)\n    G.append(k)\n    k = [bounded_list for i in range(len(list_normal))]\n    for i in range(len(list_group)):\n        k.insert(i,  bounded_list-1)\n    G.append(k)\n    G = matrix(np.array(G))\n    H = np.zeros(2 * len(meanvec)+2)\n    h = matrix(H, tc='d')\n    A = (matrix(meanvec)).trans()\n    b = matrix([1], (1, 1), tc='d')\n    #print('G',G)\n    #print('h',h)\n    #print('A',A)\n    #print('b',b)\n    sol = (solvers.qp(P, q, G, h, A, b))['x']\n    solution = [x for x in sol]\n    sum = 0\n    for i in range(len(solution)):\n        sum += solution[i]\n    optimizedWeigh = [x \/ sum for x in solution]\n    print(optimizedWeigh)\n    merge = []\n    counter = 0\n    for (columnName, columnData) in df2.iteritems():\n        # print(real[counter])\n        if len(merge) == 0:\n            merge = df2[columnName] * optimizedWeigh[counter]\n        else:\n            merge = merge + df2[columnName] * optimizedWeigh[counter]\n        counter += 1\n    #print(merge)\n    print('dd,', it.max_drawdown(booksize=booksize, returnSeries=merge))\n    print('sharpe,', it.calculateSharpe(merge))\n    merge = merge*10\n    # print('value',merge)\n    #merge.to_csv(r'\/home\/hoainam\/PycharmProjects\/multi_strategy\/v_multi\/f1m.csv')\n    # print(np.cumsum(merge))\n    plt.plot(np.cumsum(merge))\n    plt.grid(True)\n    plt.legend(('old', 'maxsharpe', 'minDD'))\n    plt.show()\n    return optimizedWeigh","eb871336":"def allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_normal,dataframe,booksize,upperbound, bounded_list,df2):\n    #print(tabulate(dataframe.corr(method='pearson'), tablefmt=\"pipe\", headers=\"keys\"))\n    # upperbound = 0.3\n    it = infoTest()\n    cov = (dataframe.cov()).to_numpy()\n    meanvec = (dataframe.std()).to_numpy()\n    P = matrix(cov, tc='d')\n    # print(P)\n    q = matrix(np.zeros(len(meanvec)), (len(meanvec), 1), tc='d')\n    G = []\n    for i in range(len(meanvec)):\n        k = [0 for x in range(len(meanvec) - 1)]\n        k.insert(i, -1)\n        G.append(k)\n    for i in range(len(meanvec)):\n        k = [-upperbound for x in range(len(meanvec) - 1)]\n        k.insert(i, 1 - upperbound)\n        G.append(k)\n    k = [-bounded_list for i in range(len(list_normal))]\n    for i in range(len(list_group)):\n        k.insert(i,1-bounded_list)\n    G.append(k)\n    k = [bounded_list for i in range(len(list_normal))]\n    for i in range(len(list_group)):\n        k.insert(i,  bounded_list-1)\n    G.append(k)\n    G = matrix(np.array(G))\n    H = np.zeros(2 * len(meanvec)+2)\n    h = matrix(H, tc='d')\n    A = (matrix(meanvec)).trans()\n    b = matrix([1], (1, 1), tc='d')\n    #print('G',G)\n    #print('h',h)\n    #print('A',A)\n    #print('b',b)\n    sol = (solvers.qp(P, q, G, h, A, b))['x']\n    solution = [x for x in sol]\n    sum = 0\n    for i in range(len(solution)):\n        sum += solution[i]\n    optimizedWeigh = [x \/ sum for x in solution]\n    print(optimizedWeigh)\n    merge = []\n    counter = 0\n    for (columnName, columnData) in df2.iteritems():\n        # print(real[counter])\n        if len(merge) == 0:\n            merge = df2[columnName] * optimizedWeigh[counter]\n        else:\n            merge = merge + df2[columnName] * optimizedWeigh[counter]\n        counter += 1\n    #print(merge)\n    print('dd,', it.max_drawdown(booksize=booksize, returnSeries=merge))\n    print('sharpe,', it.calculateSharpe(merge))\n    merge = merge*10\n    # print('value',merge)\n    #merge.to_csv(r'\/home\/hoainam\/PycharmProjects\/multi_strategy\/v_multi\/f1m.csv')\n    # print(np.cumsum(merge))\n    plt.plot(np.cumsum(merge))\n    plt.grid(True)\n    plt.legend(('old', 'maxsharpe', 'minDD'))\n    plt.show()\n    return optimizedWeigh","69d1f00e":"trad1=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,train_1,10**3,0.16,0.45,test_1)","6cdd5744":"Gauss1=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_G1,10**3,0.16,0.45,test_1)","0bd7e807":"Gauss1_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G1,10**3,0.16,0.45,test_1)","13b9464b":"Clay1=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_C1,10**3,0.16,0.45,test_1)","df082b41":"Clay1_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_C1,10**3,0.16,0.45,test_1)","96c5a5b4":"trad2=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,train_2,10**3,0.16,0.45,test_2)","6bdda118":"Gauss2=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_G2,10**3,0.16,0.45,test_2)","6cd7438d":"Gauss2_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G2,10**3,0.16,0.45,test_2)","5474aa07":"Clay2=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_C2,10**3,0.16,0.45,test_2)","61ed224b":"Clay2_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_C2,10**3,0.16,0.45,test_2)","5784a2c9":"trad3=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,train_3,10**3,0.16,0.45,test_3)","6c3cfeb0":"Gauss3=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_G3,10**3,0.16,0.45,test_3)","ff1b4b13":"Gauss3_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G3,10**3,0.16,0.45,test_3)","c0ca335d":"Clay3=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_C3,10**3,0.16,0.45,test_3)","37eb4d8e":"Clay3_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_C3,10**3,0.16,0.45,test_3)","af476e02":"trad4=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,train_4,10**3,0.16,0.45,test_4)","5131c4aa":"Gauss4=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_G4,10**3,0.16,0.45,test_4)","53a5502b":"Gauss4_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G4,10**3,0.16,0.45,test_4)","a139d8e6":"Clay4=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_C4,10**3,0.16,0.45,test_4)","6ee5cca3":"Clay4_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G4,10**3,0.16,0.45,test_4)","7334aa8d":"trad5=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,train_5,10**3,0.16,0.45,test_5)","7dc7bec7":"Gauss5=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_G5,10**3,0.16,0.45,test_5)","ec3ccc68":"Gauss5_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G5,10**3,0.16,0.45,test_5)","6d0c7a6a":"Clay5=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_C5,10**3,0.16,0.45,test_5)","56ad0fb2":"Clay5_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G5,10**3,0.16,0.45,test_5)","973b1914":"trad6=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,train_6,10**3,0.16,0.45,test_6)","92b26d96":"Gauss6=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_G6,10**3,0.16,0.45,test_6)","0643cb28":"Gauss6_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G6,10**3,0.16,0.45,test_6)","439d6d20":"Clay6=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_C6,10**3,0.16,0.45,test_6)","8b983ff5":"Clay6_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G6,10**3,0.16,0.45,test_6)","db978839":"trad7=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,train_7,10**3,0.16,0.45,test_7)","9e23c185":"Gauss7=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_G7,10**3,0.16,0.45,test_7)","cb0e3eaa":"Gauss7_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G7,10**3,0.16,0.45,test_7)","889dc724":"Clay7=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_C7,10**3,0.16,0.45,test_7)","5bb00322":"Clay7_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G7,10**3,0.16,0.45,test_7)","6d8ea026":"trad8=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,train_8,10**3,0.16,0.45,train_8)","658bec33":"Gauss8=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_G8,10**3,0.16,0.45,train_8)","099b024d":"Gauss8_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G8,10**3,0.16,0.45,train_8)","12ccfde7":"Clay8=allocate_weight_sharp_with_bounded_group_alpha(list_group,list_nomal,simul_C8,10**3,0.16,0.45,train_8)","826aa0f7":"Clay8_std=allocate_weight_sharp_with_bounded_group_alpha_std(list_group,list_nomal,simul_G8,10**3,0.16,0.45,train_8)","c5e8d931":"port1 = {'trad': trad1,\n        'Clay':Clay1,\n        'Gaus': Gauss1,\n        'Std Clay': Clay1_std,\n        'Std Gaus': Gauss1_std}\nport1 = pd.DataFrame(port1)\nport1.index = m.columns\n\nport2 = {'trad': trad2,\n        'Clay':Clay2,\n        'Gaus': Gauss2,\n        'Std Clay': Clay2_std,\n        'Std Gaus': Gauss2_std}\nport2 = pd.DataFrame(port2)\nport2.index = m.columns\n\n\nport3 = {'trad': trad3,\n        'Clay':Clay3,\n        'Gaus': Gauss3,\n        'Std Clay': Clay3_std,\n        'Std Gaus': Gauss3_std}\nport3 = pd.DataFrame(port3)\nport3.index = m.columns\n\nport4 = {'trad': trad4,\n        'Clay':Clay4,\n        'Gaus': Gauss4,\n        'Std Clay': Clay4_std,\n        'Std Gaus': Gauss4_std}\nport4 = pd.DataFrame(port4)\nport4.index = m.columns\n\n\nport5 = {'trad': trad5,\n        'Clay':Clay5,\n        'Gaus': Gauss5,\n        'Std Clay': Clay5_std,\n        'Std Gaus': Gauss5_std}\nport5 = pd.DataFrame(port5)\nport5.index = m.columns\n\n\nport6 = {'trad': trad6,\n        'Clay':Clay6,\n        'Gaus': Gauss6,\n        'Std Clay': Clay6_std,\n        'Std Gaus': Gauss6_std}\nport6 = pd.DataFrame(port6)\nport6.index = m.columns\n\n\nport7 = {'trad': trad7,\n        'Clay':Clay7,\n        'Gaus': Gauss7,\n        'Std Clay': Clay7_std,\n        'Std Gaus': Gauss7_std}\nport7 = pd.DataFrame(port7)\nport7.index = m.columns\n\nport8 = {'trad': trad8,\n        'Clay':Clay8,\n        'Gaus': Gauss8,\n        'Std Clay': Clay8_std,\n        'Std Gaus': Gauss8_std}\nport8 = pd.DataFrame(port8)\nport8.index = m.columns\n\n\n","3e42eb93":"port1.to_csv('Port_test_1.csv')\nport2.to_csv('Port_test_2.csv')\nport3.to_csv('Port_test_3.csv')\nport4.to_csv('Port_test_4.csv')\nport5.to_csv('Port_test_5.csv')\nport6.to_csv('Port_test_6.csv')\nport7.to_csv('Port_test_7.csv')\nport8.to_csv('Port_test_8.csv')","eab771f2":"def disret(df1, weight1, df2, weight2, df3, weight3,df4, weight4, df5, weight5, df6, weight6,df7, weight7):\n    it=infoTest()\n    return_1 = np.dot(df1, weight1).reshape(-1,1)\n    return_2 = np.dot(df2, weight2).reshape(-1,1)\n    return_3 = np.dot(df3, weight3).reshape(-1,1)\n    return_4 = np.dot(df4, weight4).reshape(-1,1)\n    return_5 = np.dot(df5, weight5).reshape(-1,1)\n    return_6 = np.dot(df6, weight6).reshape(-1,1)\n    return_7 = np.dot(df7, weight7).reshape(-1,1)\n    dis_ret = np.vstack((return_1, return_2,return_3,return_4,return_5,return_6,return_7))\n    return dis_ret","7f34469e":"train_1_new=pd.concat([train_1,test_1],axis=0)\ntrain_1_new.shape","6e32a62e":"train_2.shape","c6971928":"dis_ret=disret(train_1_new, Gauss1,test_2, Gauss2,test_3,Gauss3,test_4, Gauss4, test_5, Gauss5,test_6,Gauss6,test_7,Gauss7)\ndis_ret","75b3ccd3":"len(m)","36e129b5":"new_dis_ret=pd.DataFrame(dis_ret)\nnew_dis_ret.index=m.index\nnew_dis_ret.columns=['Gauss_dis_ret']\nnew_dis_ret","eec35423":"new_dis_ret.to_csv('dis_return.csv')","1d3bb671":"def frontier_plot(df,meanvec,cov_mat,port):\n    #define list\n    #cov_matrix = (dataframe.cov())\n    #meanvec = (dataframe.mean())\n    np.random.seed(42)\n    num_ports = 10000\n    num_alpha=df.shape[1]\n    all_weights = np.zeros((num_ports,num_alpha))\n    ret_arr = np.zeros(num_ports)\n    vol_arr = np.zeros(num_ports)\n    sharpe_arr = np.zeros(num_ports)\n    for x in range(num_ports):\n        \n        weights = np.array(np.random.random(num_alpha))\n        weights = weights\/np.sum(weights)\n    \n    # Save weights\n        all_weights[x,:] = weights\n    \n    # Expected return\n        ret_arr[x] = np.sum((meanvec * weights*252))\n    \n    # Expected volatility\n        vol_arr[x] = np.sqrt(np.dot(weights.T, np.dot(cov_mat*252, weights)))\n    \n    # Sharpe Ratio\n        sharpe_arr[x] = ret_arr[x]\/vol_arr[x]\n        \n    tradition_weight=port['trad']\n    clayton_weight=port['Clay']\n    gauss_weight=port['Gaus']\n    std_clay_w=port['Std Clay']\n    std_gaus_w=port['Std Gaus']\n   \n    \n    #Volatility\n    max_trad_sr_vol=np.sqrt(np.dot(tradition_weight.T, np.dot(cov_mat*252, tradition_weight)))\n    max_clayton_sr_vol=np.sqrt(np.dot(clayton_weight.T, np.dot(cov_mat*252, clayton_weight)))\n    max_gauss_sr_vol=np.sqrt(np.dot(gauss_weight.T, np.dot(cov_mat*252, gauss_weight)))\n    \n    \n    max_clayton_std_vol=np.sqrt(np.dot(std_clay_w.T, np.dot(cov_mat*252, std_clay_w)))\n    max_gaus_std_vol=np.sqrt(np.dot(std_gaus_w.T, np.dot(cov_mat*252,std_gaus_w)))\n\n\n\n\n    #Return\n    max__trad_sr_ret=np.sum((meanvec * tradition_weight*252))\n    max__clayton_sr_ret=np.sum((meanvec * clayton_weight*252))\n    max__gauss_sr_ret=np.sum((meanvec * gauss_weight*252))\n    \n    max__clayton_std_ret=np.sum((meanvec * std_clay_w*252))\n    max__gaus_std_ret=np.sum((meanvec * std_gaus_w*252))\n\n\n\n    \n    \n    real_sr_ret=ret_arr[sharpe_arr.argmax()]\n    \n    real_sr_vol=vol_arr[sharpe_arr.argmax()]\n    \n    print('The Traditional Sharpe : ',max__trad_sr_ret\/max_trad_sr_vol)\n    print('The Clayton Sharpe : ',max__clayton_sr_ret\/max_clayton_sr_vol)\n    print('The Gauss Sharpe : ',max__gauss_sr_ret\/max_gauss_sr_vol)\n    \n    print('The STD Clayton Sharpe : ',max__clayton_std_ret\/max_clayton_std_vol)\n    print('The STD Gauss Sharpe : ', max__gaus_std_ret\/max_gaus_std_vol)\n\n\n\n    print('The Real Sharpe :',real_sr_ret\/real_sr_vol)\n    \n    plt.figure(figsize=(12,8))\n    plt.scatter(vol_arr, ret_arr, c=sharpe_arr, cmap='viridis')\n    plt.colorbar(label='Sharpe Ratio')\n    plt.xlabel('Volatility')\n    plt.ylabel('Return')\n    plt.scatter(max_trad_sr_vol, max__trad_sr_ret,c='red', s=50)#red dot\n    plt.scatter(max_clayton_sr_vol, max__clayton_sr_ret,c='pink', s=50)\n    plt.scatter(max_gauss_sr_vol, max__gauss_sr_ret,c='pink', s=50)\n    \n    plt.scatter(max_clayton_sr_vol, max__clayton_sr_ret,c='orange', s=50)\n    plt.scatter(max_gauss_sr_vol, max__gauss_sr_ret,c='orange', s=50)\n    plt.scatter(real_sr_vol, real_sr_ret,c='black', s=50)# black dot\n    plt.show()\n","dfb79018":"frontier_plot(test_1,test_1.mean(),test_1.cov(),port1)","52a93473":"frontier_plot(test_2,test_2.mean(),test_2.cov(),port2)","2d471258":"frontier_plot(test_3,test_3.mean(),test_3.cov(),port3)","d2332bf0":"frontier_plot(test_4,test_4.mean(),test_4.cov(),port4)","cc623f10":"frontier_plot(test_5,test_5.mean(),test_5.cov(),port5)","34a2f1da":"frontier_plot(test_6,test_6.mean(),test_6.cov(),port6)","44a4affd":"frontier_plot(test_7,test_7.mean(),test_7.cov(),port7)","b3efbfa2":"# TEST 2","2e52ac71":"# SPLIT TRAIN\/TEST DATA","c616af26":"# TEST 1","8ddfaff7":"# TEST 4","7fe6c415":"# Import simulation data","b851f47c":"# Implementation","c280003b":"# TEST 3","154bee33":"# Port Weight","40266256":"# TEST 5","17e4bdc4":"# TEST 7","cafc8b39":"# FRONTIER PLOT","564a7ec9":"Train 8","65e1a960":"# TEST 6"}}