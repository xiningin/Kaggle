{"cell_type":{"accd82ab":"code","260cffe2":"code","a85be216":"code","dac47b2b":"code","2c32626f":"code","01c758f8":"code","40fbd1a2":"code","7f0b2ebc":"code","d9eed6ae":"code","fd0221dc":"code","6078fe21":"code","cfe93e2e":"code","e5fb7802":"code","c10bed5e":"code","31eca8e8":"code","8ae18038":"code","1ae4bf95":"code","a84f535f":"code","329b6d4e":"code","c2942e47":"code","84b168c5":"code","d9212781":"code","f1119966":"code","fe809620":"code","741de62d":"code","ce80eaa5":"code","22d4ea3a":"code","2f237aae":"code","d068627f":"code","dc4a854e":"code","3de51d76":"code","4a2ed6b8":"code","23fa4e46":"code","e793c3ad":"markdown","5f9fb292":"markdown","78291e04":"markdown","9273e180":"markdown","d1796d1e":"markdown","89076ad3":"markdown","c369c112":"markdown","6d9e61f2":"markdown","a952c322":"markdown","6217d7f0":"markdown","e0120794":"markdown","e999cfc2":"markdown","0a49aa7c":"markdown","ddfdf1b5":"markdown","c9044132":"markdown","b1e47a15":"markdown","499451b4":"markdown","f18a9024":"markdown","7303a137":"markdown","70b3aefd":"markdown","66a0fbbe":"markdown"},"source":{"accd82ab":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import Dense","260cffe2":"\n### Removes warnings that occassionally show up\nimport warnings\nwarnings.filterwarnings('ignore')","a85be216":"data_bank=pd.read_csv('\/kaggle\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv')\ndata_bank.head()","dac47b2b":"data_bank.shape","2c32626f":"data_bank.info()","01c758f8":"data_bank.isna().sum()","40fbd1a2":"# 5 point summary\ndata_bank.describe().T","7f0b2ebc":"# CORRELATION - HEAT MAP\ncolormap = plt.cm.plasma\nplt.figure(figsize=(17,10))\nplt.title('Correlation of Cutomer Exiting Bank', y=1.05, size=15)\nsns.heatmap(data_bank.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, \n            linecolor='white', annot=True)","d9eed6ae":"sns.countplot(x=\"Geography\", data=data_bank,hue=\"Exited\")","fd0221dc":"gendermap = sns.FacetGrid(data_bank,hue = 'Exited')\n(gendermap.map(plt.hist,'Age',edgecolor=\"w\").add_legend())","6078fe21":"bank_data_new = data_bank.drop(['RowNumber', 'CustomerId', 'Surname'], axis =1)","cfe93e2e":"\nbank_data_new.head()","e5fb7802":"numerical_distribution = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\nfor i in numerical_distribution:\n    plt.hist(data_bank[i])\n    plt.title(i)\n    plt.show()","c10bed5e":"## Label Encoding of all the columns\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# Categorical boolean mask\ncategorical_feature_mask = bank_data_new.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = bank_data_new.columns[categorical_feature_mask].tolist()\nbank_data_new[categorical_cols] = bank_data_new[categorical_cols].apply(lambda col: le.fit_transform(col))\nprint(bank_data_new.info())","31eca8e8":"\ndf_scaled = bank_data_new.apply(zscore)\nX_columns =  df_scaled.columns.tolist()[1:10]\nY_Columns = bank_data_new.columns.tolist()[-1:]\n\nX = df_scaled[X_columns].values\ny = np.array(bank_data_new['Exited']) # Exited\n\nprint(y)\nprint(X)","8ae18038":"#splitting the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state= 8)","1ae4bf95":"X_train.shape","a84f535f":"#Encoding the output class label (One-Hot Encoding)\ny_train=to_categorical(y_train,2)\ny_test=to_categorical(y_test,2)","329b6d4e":"from sklearn.preprocessing import Normalizer\nnormalize=Normalizer(norm=\"l2\")\nX_train=normalize.transform(X_train)\n\nprint(X_train)","c2942e47":"\nX_test=normalize.transform(X_test)\nprint(X_test)","84b168c5":"#Initialize Sequential Graph (model)\nmodel = tf.keras.Sequential()","d9212781":"model.add(Dense(units=6, activation='relu', input_shape=(9,)))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))","f1119966":"model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","fe809620":"history=model.fit(X_train, y_train, batch_size=45, epochs=200, validation_data=(X_test,y_test))","741de62d":"score = model.evaluate(X_test, y_test,verbose=1)\n\nprint(score)","ce80eaa5":"\nscore = model.evaluate(X_train, y_train,verbose=1)\n\nprint(score)","22d4ea3a":"\ny_pred = model.predict(X_test)","2f237aae":"y_pred = (y_pred > 0.5)","d068627f":"from sklearn.metrics import confusion_matrix\n\nconfmatrx= confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))","dc4a854e":"confmatrx","3de51d76":"plt.plot(np.array(history.history['accuracy']) * 100)\nplt.plot(np.array(history.history['val_accuracy']) * 100)\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['train', 'validation'])\nplt.title('Accuracy over epochs')\nplt.show()","4a2ed6b8":"print (((confmatrx[0][0]+confmatrx[1][1])*100)\/(len(y_test)), '% of testing data was classified correctly')","23fa4e46":"# Checking the accuracy using accuracy_score as well to check if the above calculation is correct or not.\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","e793c3ad":"#### Removing CustomerId, RowNumber and Surname. Removing Surname is basically because One Hot Encoding will behave unusual if this column is retained(give error)","5f9fb292":"### Normalize the data","78291e04":"### Confusion Matrix Calculation","9273e180":"### Reading the dataset","d1796d1e":"### Testing the Neural Network","89076ad3":"Now, Checking number of people exited from the bank via Geography. i.e. from mentioned Countries","c369c112":"## PRINT THE ACCURACY","6d9e61f2":"### Building the Model","a952c322":"#### Distribution of Numerical Column","6217d7f0":"### Train Accuracy","e0120794":"### Drop the columns which are unique for all users like IDs","e999cfc2":"#### Label Encoding the Bank.csv columns to make them integers","0a49aa7c":"The above information indicates that there are no missing values","ddfdf1b5":"### Import the necessary Libraries","c9044132":"# Summary\n#### The Train and Test set are almost similar. It shows that the model did not overfit on the train set.\n\n#### On Compiling the Neural Network, I have used optimizer as \"adam\" as it is very efficient to Stochastic Gradient Decent. The loss function used is \"binary_crossentropy which is used within adam.\n#### The accuracy metrics which will be evaluated(minimized) by the model. The \"Accuracy\" is used as a criteria to improve model performance.\n\n#### On calculation of the accuracy based on the Confusion Matrix, it came out as 86.45(APPROX). which matches with the score calculated through various EPOCH\n\n#### We can used Optimizer as \"SGD\" as well to check if we get better results and accuracy.\n\n#### HOWEVER, FOR BETTER RESULT WE CAN YOUR HYPER PARAMETER TUNING FOR BETTER RESULT.","b1e47a15":"### Model Training","499451b4":"### EXPLORATORY DATA ANALYSIS","f18a9024":"# Given Dataset consisting of Bank Customer Information, we need to create a model that will predict if a customer will leave the bank or not.","7303a137":"\nThe above code indicates:\n\nX_train is the independent variable portion of the data which needs to be fitted with the model.\n\ny_train is the output portion of the data which the model needs to produce after fitting.\n\nbatch_size: How often we want to back-propogate the error values so that individual node weights can be adjusted.\n\nepochs: The number of times we want to run the entire test data over again to tune the weights. This is like the fuel of the algorithm.\n\nvalidation_split: 0.1 The fraction of data to use for validation data.","70b3aefd":"### Checking the Accuracy for Test","66a0fbbe":"### One Hot Encoding"}}