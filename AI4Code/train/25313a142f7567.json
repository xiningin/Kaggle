{"cell_type":{"2b107633":"code","8605230c":"code","2bf01fb9":"code","1c3cb8a5":"code","cb8e7e78":"code","4fe24140":"code","59a1257f":"code","0b9845a6":"code","0696966b":"code","8e67f6a1":"code","e8031799":"code","089029d6":"code","554c6a1a":"code","a3eef06d":"code","f1b0016e":"code","a3e29850":"code","3f214196":"code","da81e558":"code","fd3a4d01":"code","9a69ad6c":"code","49d0ba39":"code","c57d7bba":"code","0e2da8e3":"code","df7970a4":"code","a72858cf":"code","779d7319":"code","1de8f9be":"code","1f342098":"code","50133799":"code","2be10331":"code","e6973717":"code","4f18f44a":"code","786e27d1":"code","b90296d9":"code","cd17e3f5":"code","56b75a05":"code","076c519a":"code","0daf7420":"code","2ff913c5":"markdown","72469ff6":"markdown","c313b763":"markdown","c53790a6":"markdown","2790b89a":"markdown","4e909729":"markdown","5fd8575f":"markdown","6ccceba3":"markdown","88a5a522":"markdown","4c9c6e39":"markdown"},"source":{"2b107633":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler,Normalizer,StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom lightgbm import LGBMRegressor\n\n\nimport optuna\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8605230c":"train=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')","2bf01fb9":"print(\"Train\",train.shape)\nprint(\"test\",test.shape)","1c3cb8a5":"train.describe().T.style.background_gradient(axis=None,cmap='YlOrRd',low=0.75, high=1.0)","cb8e7e78":"train.head()","4fe24140":"sns.countplot(train['claim'],color='aqua')\n","59a1257f":"#Check for Null values for bith test and train\ntrain.drop(['id'],axis=1,inplace=True)\ntest.drop(['id'],axis=1,inplace=True)","0b9845a6":"def null_values(df):\n    lst=[]\n    for col in df.columns:\n        count=df[col].isnull().sum()\n        perc=round((count\/len(df))*100,2)\n        dic={'Feature':col,\n            'Total_null':count,\n            'Prcentage':f'{perc} %'}\n        lst.append(dic)\n        \n    return pd.DataFrame(lst,index=None)\n \n    \ntrain_null=null_values(train)\ntest_null=null_values(test)    ","0696966b":"df_null=pd.merge(train_null,test_null,on='Feature',suffixes=('_train','_test'))\ndf_null","8e67f6a1":"plt.figure(figsize=(18,7))\nplt.plot(df_null['Feature'],df_null['Total_null_train'],'ro')\nplt.plot(df_null['Feature'],df_null['Total_null_test'],'go')","e8031799":"fig = plt.figure(figsize = (15, 60))\ni=1\nfor feature in train.columns[:-1]:\n    plt.subplot(24, 5, i)\n    ax = sns.kdeplot(train[feature], shade=True, color='cyan',  alpha=0.5, label='train')\n    ax = sns.kdeplot(test[feature], shade=True, color='yellow',  alpha=0.5, label='test')\n    plt.xlabel(feature, fontsize=9)\n    plt.legend()\n    plt.xticks([])\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        ax.spines[j].set_visible(False)\n        ax.spines['bottom'].set_linewidth(1.2)\n\n    i += 1\n    \nplt.suptitle('train test distribution', fontsize=20)\nfig.tight_layout(h_pad = 3)\nplt.show()","089029d6":"\ncor=train.corr()\nmask = np.triu(np.ones_like(cor, dtype=bool))\nmask","554c6a1a":"plt.figure(figsize=(20,20))\nsns.heatmap(train.corr(),cmap='YlGnBu',mask=mask ,vmax=.3, center=0,\n            square=True, linewidths=.5,)","a3eef06d":"target=train['claim']\n\n#drop col\ntrain.drop(train[['id','claim']],axis=1,inplace=True)\ntest.drop(['id'],axis=1,inplace=True)\ncol=train.columns","f1b0016e":"#imputing null values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\n\ntrain[col]=imputer.fit_transform(train[col])\ntest[col]=imputer.transform(test[col])","a3e29850":"train_scale=train.copy()\ntest_scale=test.copy()","3f214196":"#since some features value range is different , lets standardise it\ntrain_scale=train.copy()\ntest_scale=test.copy()\n\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ntrain_scale[col] = scale.fit_transform(train_scale[col])\ntest_scale[col] = scale.transform(test_scale[col])\n","da81e558":"def objective(trial):\n    x_train,x_valid,y_train,y_valid=train_test_split(train,target,train_size=0.80,random_state=42)\n    params = {'objective': 'binary:logistic',\n              'use_label_encoder': False,\n              'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n              'learning_rate': trial.suggest_discrete_uniform('learning_rate',0.01,0.1,0.01),\n              'subsample': trial.suggest_discrete_uniform('subsample', 0.3, 1.0, 0.1),\n              'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1.0, 0.1),\n              'max_depth': trial.suggest_int('max_depth', 2, 13),\n              'booster': 'gbtree',\n              'gamma': trial.suggest_uniform('gamma',1.0,10.0),\n              'reg_alpha': trial.suggest_int('reg_alpha',50,100),\n              'reg_lambda': trial.suggest_int('reg_lambda',50,100),\n              'random_state': 42,\n                 }\n    model = XGBClassifier(**params, tree_method='gpu_hist', predictor='gpu_predictor',eval_metric=['auc'])\n    model.fit(x_train, y_train, verbose=False)\n    val_pred = model.predict_proba(x_valid)[:, 1]\n    return roc_auc_score(y_valid, val_pred)\n\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n","fd3a4d01":"\noof=np.zeros((len(train),))\nfinal_pred=[]\nsf=StratifiedKFold(5,shuffle=True,random_state=2)\nfold=0\n    \nfor train_ind,val_ind in sf.split(train[col],target):\n    x_train,x_val=train[col].iloc[train_ind],train[col].iloc[val_ind]\n    y_train=target[train_ind]\n    y_val=target[val_ind]\n        \n    model = XGBClassifier(random_state=22, eval_metric= 'auc',**study.best_params ,tree_method='gpu_hist',predictor = 'gpu_predictor')\n        \n    model.fit(x_train,y_train)\n        \n    pred=model.predict_proba(x_val)[:,1]\n        \n    oof[val_ind]=pred\n        \n    print(f'Fold {fold} AUC: ', roc_auc_score(y_val, pred))\n        \n    fold+=1\n    test_pred=model.predict_proba(test[col])[:,1]\n    final_pred.append(test_pred)    \nprint('oof score',roc_auc_score(target, oof))","9a69ad6c":"prediction=np.mean(final_pred,axis=0)\nsub=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub['claim']=prediction\nsub.to_csv('submission.csv',index=False)\n\n","49d0ba39":"from catboost import CatBoostClassifier\ndef objective(trial):\n    x_train,x_valid,y_train,y_valid=train_test_split(train,target,train_size=0.80,random_state=42)\n    params = {'iterations': trial.suggest_int('iterations', 200, 1000),\n              'task_type':'GPU',\n              'learning_rate': trial.suggest_uniform('learning_rate' , 1e-5 , 1.0),\n              'max_depth': trial.suggest_int('max_depth', 2, 13)  }\n    model = CatBoostClassifier(**params)\n    model.fit(x_train, y_train, verbose=False)\n    val_pred = model.predict_proba(x_valid)[:, 1]\n    return roc_auc_score(y_valid, val_pred)\n\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n","c57d7bba":"oof=np.zeros((len(train),))\nfinal_pred=[]\nsf=StratifiedKFold(5,shuffle=True,random_state=2)\nfold=0\n    \nfor train_ind,val_ind in sf.split(train[col],target):\n    x_train,x_val=train[col].iloc[train_ind],train[col].iloc[val_ind]\n    y_train=target[train_ind]\n    y_val=target[val_ind]\n        \n    model = CatBoostClassifier(task_type='GPU',**study.best_params,verbose=1000)\n        \n    model.fit(x_train,y_train)\n        \n    pred=model.predict_proba(x_val)[:,1]\n        \n    oof[val_ind]=pred\n        \n    print(f'Fold {fold} AUC: ', roc_auc_score(y_val, pred))\n        \n    fold+=1\n    test_pred=model.predict_proba(test[col])[:,1]\n    final_pred.append(test_pred)    \nprint('oof score',roc_auc_score(target, oof))","0e2da8e3":"prediction=np.mean(final_pred,axis=0)\nsub=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub['claim']=prediction\nsub.to_csv('submission_CATBOOST.csv',index=False)\n","df7970a4":"study.best_params","a72858cf":"def  objective(trial):\n    x_train,x_valid,y_train,y_valid=train_test_split(train,train['claim'],train_size=.75,random_state=22)\n    \n    params = {\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 6, 64),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        'metric':'binary_logloss',\n        'learning_rate': trial.suggest_categorical('learning_rate',[0.01,0.15,0.1])\n    }\n    lgb=LGBMClassifier(device= 'gpu',**params)\n    lgb.fit(x_train, y_train, verbose=False)\n    val_pred = lgb.predict_proba(x_valid)[:, 1]\n    return roc_auc_score(y_valid, val_pred)\n\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=5)\n","779d7319":"oof=np.zeros((len(train),))\nfinal_pred=[]\nsf=StratifiedKFold(7,shuffle=True,random_state=22)\nfold=0\n    \nfor train_ind,val_ind in sf.split(train[col],target):\n    x_train,x_val=train[col].iloc[train_ind],train[col].iloc[val_ind]\n    y_train=target[train_ind]\n    y_val=target[val_ind]\n        \n    model = LGBMClassifier(device='GPU',**study.best_params,verbose=1000)\n        \n    model.fit(x_train,y_train)\n        \n    pred=model.predict_proba(x_val)[:,1]\n        \n    oof[val_ind]=pred\n        \n    print(f'Fold {fold} AUC: ', roc_auc_score(y_val, pred))\n        \n    fold+=1\n    test_pred=model.predict_proba(test[col])[:,1]\n    final_pred.append(test_pred)    \nprint('oof score',roc_auc_score(target, oof))","1de8f9be":"prediction=np.mean(final_pred,axis=0)\nsub=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub['claim']=prediction\nsub.to_csv('submission_LGB.csv',index=False)\n\n#80.06","1f342098":"lgm_param=study.best_params","50133799":"#will try LGB with some features Engeneering\ntrain=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')","2be10331":"#we can create a new column if a row has missing value\ncolumns=[col for col in train.columns if col not in ['id','claim']]\ntrain['total_missing']=train[columns].isnull().sum(axis=1)\ntest['total_missing']=test[columns].isnull().sum(axis=1)","e6973717":"'''\npip=Pipeline(\n    [('scale', StandardScaler())])\n\n\nX=pd.DataFrame(data=pip.fit_transform(train[columns]),columns=columns)\nTest=pd.DataFrame(data=pip.transform(test[columns]),columns=columns)\n'''","4f18f44a":"'''\noof=np.zeros((len(X),))\nfinal_pred=[]\nsf=StratifiedKFold(7,shuffle=True,random_state=22)\nfold=0\n    \nfor train_ind,val_ind in sf.split(X[columns],target):\n    x_train,x_val=X[columns].iloc[train_ind],X[columns].iloc[val_ind]\n    y_train=target[train_ind]\n    y_val=target[val_ind]\n        \n    model = LGBMClassifier(device='GPU',**lgm_param,verbose=1000)\n        \n    model.fit(x_train,y_train)\n        \n    pred=model.predict_proba(x_val)[:,1]\n        \n    oof[val_ind]=pred\n        \n    print(f'Fold {fold} AUC: ', roc_auc_score(y_val, pred))\n        \n    fold+=1\n    test_pred=model.predict_proba(Test[columns])[:,1]\n    final_pred.append(test_pred)    \nprint('oof score',roc_auc_score(target, oof))\n'''","786e27d1":"'''\nprediction=np.mean(final_pred,axis=0)\nsub=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub['claim']=prediction\nsub.to_csv('submission_LGB.csv',index=False)\n\n#80.06\n'''","b90296d9":"study.best_params","cd17e3f5":"'''\ndef baseline_model(model,train,test,columns,target=train['claim']):\n    oof=np.zeros((len(train),))\n    final_pred=[]\n    sf=StratifiedKFold(10,shuffle=True,random_state=22)\n    fold=0\n    \n    for train_ind,val_ind in sf.split(train[columns],target):\n        x_train,x_val=train[columns].iloc[train_ind],train[columns].iloc[val_ind]\n        y_train=target[train_ind]\n        y_val=target[val_ind]\n        \n        \n        \n        model.fit(x_train,y_train)\n        \n        pred=model.predict_proba(x_val)[:,1]\n        \n        oof[val_ind]=pred\n        \n        print(f'Fold {fold} AUC: ', roc_auc_score(y_val, pred))\n        \n        fold+=1\n        test_pred=model.predict_proba(test[columns])[:,1]\n        final_pred.append(test_pred)    \n    print('oof score',roc_auc_score(target, oof))\n    prediction=np.mean(final_pred,axis=0)\n    sub=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n    sub['claim']=prediction\n    return sub\n\n'''","56b75a05":"'''\ncolumns\npip_2=Pipeline([('mean',SimpleImputer(strategy='median')),\n    ('scale', StandardScaler())])\nprint('transformation_done')\n\nX=pd.DataFrame(data=pip_2.fit_transform(train[columns]),columns=columns)\nTest=pd.DataFrame(data=pip_2.transform(test[columns]),columns=columns)\n'''\n","076c519a":"'''\nmodel = LGBMClassifier(**study.best_params,verbose=-1)\n\nsample_sub=baseline_model(model,X,Test,columns)\n'''","0daf7420":"sub=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub['claim']=prediction\nsub.to_csv('submission_LGB_fe.csv',index=False)\n    ","2ff913c5":"Seems like correlation among features is very less","72469ff6":"#Can see that almost same percentage of values are missing from train and test","c313b763":"# LGBM optuna","c53790a6":"# DONE\n*  Still LGM needs to be used\n*  try some feature eng","2790b89a":"# XGB Hyperparameter","4e909729":"#data looks fairly balanced","5fd8575f":"# PLease UPVOTE if you like this notebook","6ccceba3":"# CATBOOST OPTUNA","88a5a522":"* some of the plots are normal distributed,\n* some are skewed either to left or right\n* some are bimodel","4c9c6e39":"#needs to be done\n*  stacking them\n*  feature imp"}}