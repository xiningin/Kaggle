{"cell_type":{"cb4b4e3a":"code","8e13ac37":"code","ef4b4837":"code","5e677103":"code","f376e033":"code","24207c7b":"code","bd88cd28":"code","ecdb47bf":"code","228f1777":"code","eda56954":"code","d2f6681d":"code","9ee179a6":"code","26988f90":"code","be99bade":"code","201d0481":"code","26862080":"code","1cbadb8a":"code","93ce16f1":"code","5bea8036":"code","86fe0788":"code","2cf50dd0":"code","bfca86c6":"code","514024f6":"code","49ff5887":"code","59d9a136":"code","47e49f04":"code","3ac3192b":"code","e5afa8b5":"code","67cc9ab5":"code","75707a59":"code","77b9d1f8":"code","061e05f9":"code","7dbe57b2":"code","6ffc01e4":"code","085343c1":"code","6fe5a2c5":"code","60d81aec":"code","7622ca1b":"code","7827ebbd":"code","574795b8":"code","6a068e30":"code","24a1ee93":"code","da0be4c8":"code","949c88c8":"code","6cbd848d":"code","91a944a5":"code","3911b132":"code","9f78bd80":"code","50a83715":"code","bf58ec19":"code","7234b284":"code","3bd18c8d":"code","f3201109":"code","510a8ecf":"code","4dd41c44":"code","b379bc60":"code","f53a830f":"code","77c658d9":"code","57ea2ac4":"code","0abbcc87":"code","2db2715b":"code","cb5e545b":"code","15443679":"code","5318d71a":"code","b55038ad":"code","4fc72f6a":"code","63c6f563":"code","74997544":"code","9ee0f0c0":"code","ea17a5ee":"code","6c15c4e3":"code","2b88b585":"code","5da177a9":"code","907171cc":"code","69741a2a":"code","92fe5b54":"code","ddb1f139":"code","62ff666c":"code","b4018ab8":"code","9437f8d1":"code","04f9a16f":"code","31ad9d46":"code","0913a266":"code","ff3f4a64":"code","10e16aa5":"code","9569b57e":"code","f2f5b2dc":"code","322ff752":"code","9acc4eb8":"code","9920a4b1":"code","d2fd8dc2":"code","698444ff":"code","71f6a0f7":"code","66ad3425":"code","52767ca9":"code","c3ab912d":"code","7553c792":"code","4720d16f":"code","2036bbd4":"code","89f03d15":"code","68a0e641":"code","9ac9b45a":"code","0ee3d658":"code","5ee4f781":"code","28cda5f4":"code","2b108534":"code","0dd5892f":"code","9cf4afb2":"code","36294b1c":"code","3f941d05":"code","61d5ce89":"code","6b35b4fc":"code","73bdb52e":"code","3213c504":"code","b6de2ee4":"code","9988d0d5":"markdown","41ebb014":"markdown","3aa360ca":"markdown","f9b600dd":"markdown","d93e64cc":"markdown","d9bbfa6a":"markdown","60c30b90":"markdown","6b39ccb1":"markdown","58c5a6d6":"markdown","ecbbc627":"markdown","6143cc15":"markdown","955ecb06":"markdown","6dbcce88":"markdown","a9dc652f":"markdown","f3914651":"markdown","4fe5ddcf":"markdown","bc888f74":"markdown","3d0fbcf0":"markdown","4c41a134":"markdown","c82bc9e2":"markdown","e0ea0d33":"markdown","770d3ecf":"markdown","64756feb":"markdown","79f4c4ad":"markdown","9955dbb3":"markdown","76fe802d":"markdown","20fe4d39":"markdown","5499ebfd":"markdown","1d97c1e8":"markdown","835064cd":"markdown","7d8847cd":"markdown","5be7458f":"markdown","f1c9e4f6":"markdown","5c08413f":"markdown","56258a51":"markdown","93fbba88":"markdown","8b4afe71":"markdown","d5cee8c6":"markdown","82c29f76":"markdown","8e8ad58a":"markdown","0fa2af28":"markdown","4a58e7e9":"markdown","24dd5767":"markdown","affc996e":"markdown","6ecf5bf8":"markdown","0cfdfdb0":"markdown","41f9fa41":"markdown","2d96565d":"markdown","2f043c61":"markdown","e857e281":"markdown","594c2aa9":"markdown","26663082":"markdown","99831238":"markdown","1efbef78":"markdown","9f48370e":"markdown","4534dd17":"markdown","9bc670f4":"markdown","cd2f9405":"markdown","c39225c8":"markdown","466196c5":"markdown"},"source":{"cb4b4e3a":"#importing main libraries;\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#importing preprocessing features;\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#scaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\n\n#encoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n#imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n#importing algortihms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n\n#pipeline\nfrom sklearn.pipeline import Pipeline\nfrom IPython.core.display import HTML\nfrom sklearn.utils import estimator_html_repr\n\n#outlier detection\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\n\n#metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n%matplotlib inline","8e13ac37":"import os\nprint(os.listdir(\"..\/input\"))","ef4b4837":"df = pd.read_csv('..\/input\/iba-ml2-mid-project\/train.csv', index_col='Id') #We don't really need the ID column to be found in the train dataset, \n                                              #therefore I setting it as index","5e677103":"df_test = pd.read_csv('..\/input\/iba-ml2-mid-project\/test.csv', index_col='Id') #For reporting purposes, I am keeping ID column.","f376e033":"df.head(7) #can obviously see the many \"Not a Number\" values in the dataset.","24207c7b":"df.tail(7) #can obviously see the many \"Not a Number\" values here as well.","bd88cd28":"df.shape","ecdb47bf":"print(f'The shape of the train dataset is: {df.shape}.')","228f1777":"df.columns.to_list()    # We can see the list of names of columns.","eda56954":"df.info() ","d2f6681d":"df.isna().sum()  ","9ee179a6":"df.isna().sum().sum()","26988f90":"print(f'Total number of missing values is {df.isna().sum().sum()}.')","be99bade":"sns.set(rc={'figure.figsize':(12,8)})\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap= 'cividis')  #We can see the visualized versin of missing values.","201d0481":"df.count() #numer of non-missing values","26862080":"df.describe()","1cbadb8a":"df.corr()","93ce16f1":"x = df.corr()\nfig = plt.figure(figsize=(12,7))\nax = sns.heatmap(x,annot=True) \n","5bea8036":"import warnings\nwarnings.filterwarnings('ignore')  #just to ignore warnings related to graph.","86fe0788":"df['age'].value_counts()","2cf50dd0":"print(df.age.min())\nprint(df.age.max())","bfca86c6":"print(f'Minimum age in our dataset is {df.age.min()}.')\nprint(f'Maximum age in our dataset is {df.age.max()}.')","514024f6":"df['age'].nunique() # We can see there are 82 unique ages in this column.\nprint(f'Number of unique values in the \"Age\" column is {df.age.nunique()}.')","49ff5887":"sns.distplot(df['age'], bins=45)\nplt.show()  ","59d9a136":"print(f'Median age is: {df.age.median()}')\nprint(f'Mean age is: {df.age.mean()}')","47e49f04":"sns.boxplot(df['age'],color='navy')","3ac3192b":"df[df['age'] > 100] #there are 3 people over age 100 which we can call outliers.","e5afa8b5":"df['number_dependent_family_members'].value_counts()","67cc9ab5":"df['number_dependent_family_members'].value_counts(normalize='True')","75707a59":"print(df['number_dependent_family_members'].min())\nprint(df['number_dependent_family_members'].max())","77b9d1f8":"print(f'Minimum age in our dataset is {df.number_dependent_family_members.min()}.')\nprint(f'Maximum age in our dataset is {df.number_dependent_family_members.max()}.')","061e05f9":"df['number_dependent_family_members'].nunique() # We can see there are 12 unique values in this column.\nprint(f'Number of unique values in the \"number_dependent_family_members\" column is {df.number_dependent_family_members.nunique()}.')","7dbe57b2":"sns.histplot(df['number_dependent_family_members'], bins=12)\nplt.show()  ","6ffc01e4":"print(f'Median \"number_dependent_family_members\" is: {df.number_dependent_family_members.median()}')\nprint(f'Mean \"number_dependent_family_members\" is: {df.number_dependent_family_members.mean()}')","085343c1":" sns.boxplot(df['number_dependent_family_members'],color='navy') ","6fe5a2c5":"df[df['number_dependent_family_members'] > 8]   ","60d81aec":"df['monthly_income'].value_counts()  #As it is truly numeric column, we cannot say anything from here.","7622ca1b":"print(df['monthly_income'].min())\nprint(df['monthly_income'].max())  ","7827ebbd":"print(f'Minimum monthly_income in our dataset is {df.monthly_income.min()}')\nprint(f'Maximum monthly_income in our dataset is {df.monthly_income.max()}')","574795b8":"plt.xlim(0,30000)\nsns.histplot(df['monthly_income'])","6a068e30":"print(f'Median monthly_income is: {df.monthly_income.median()}')\nprint(f'Mean monthly_income is: {df.monthly_income.mean()}')","24a1ee93":"plt.xlim(0,30000)\nsns.boxplot(df['monthly_income'],color='navy')","da0be4c8":"df[df['monthly_income'] > 100000]","949c88c8":"df['number_of_credit_lines'].value_counts()","6cbd848d":"print(df.number_of_credit_lines.min())\nprint(df.number_of_credit_lines.max())","91a944a5":"print(f'Minimum number of credit lines in our dataset is {df.number_of_credit_lines.min()}.')\nprint(f'Maximum number of credit lines in our dataset is {df.number_of_credit_lines.max()}.')","3911b132":"df['number_of_credit_lines'].nunique() # We can see there are 56 unique values in this column.\nprint(f'Number of unique values in the \"number_of_credit_lines\" column is {df.number_of_credit_lines.nunique()}.')","9f78bd80":"sns.distplot(df['number_of_credit_lines'])","50a83715":"print(f'Median number_of_credit_lines is: {df.number_of_credit_lines.median()}')\nprint(f'Mean number_of_credit_lines is: {df.number_of_credit_lines.mean()}')","bf58ec19":"sns.boxplot(df['number_of_credit_lines'],color='navy')","7234b284":"df[df['number_of_credit_lines'] > 30]","3bd18c8d":"df['real_estate_loans'].value_counts()","f3201109":"df['real_estate_loans'].value_counts(normalize='True')","510a8ecf":"print(df['real_estate_loans'].min())\nprint(df['real_estate_loans'].max())","4dd41c44":"print(f'Minimum real_estate_loans in our dataset is {df.real_estate_loans.min()}.')\nprint(f'Maximum real_estate_loans in our dataset is {df.real_estate_loans.max()}.')","b379bc60":"df['real_estate_loans'].nunique() # We can see there are 20 unique values in this column.\nprint(f'Number of unique values in the \"real_estate_loans\" column is {df.real_estate_loans.nunique()}.')","f53a830f":"plt.xlim(0,15)\nsns.distplot(df['real_estate_loans'])","77c658d9":"print(f'Median real_estate_loans is: {df.real_estate_loans.median()}')\nprint(f'Mean real_estate_loans is: {df.real_estate_loans.mean()}')","57ea2ac4":"sns.boxplot(df['real_estate_loans'],color='navy')","0abbcc87":"df[df['real_estate_loans'] > 10]","2db2715b":"df['ratio_debt_payment_to_income'].value_counts()","cb5e545b":"print(df['ratio_debt_payment_to_income'].min())\nprint(df['ratio_debt_payment_to_income'].max())","15443679":"print(f'Minimum ratio_debt_payment_to_income in our dataset is {df.ratio_debt_payment_to_income.min()}.')\nprint(f'Maximum ratio_debt_payment_to_income in our dataset is {df.ratio_debt_payment_to_income.max()}.')","5318d71a":"plt.xlim(0,12)\nsns.distplot(df['ratio_debt_payment_to_income'])","b55038ad":"print(f'Median ratio_debt_payment_to_income is: {df.ratio_debt_payment_to_income.median()}')\nprint(f'Mean ratio_debt_payment_to_income is: {df.ratio_debt_payment_to_income.mean()}')","4fc72f6a":"plt.xlim(0,10)\nsns.boxplot(df['ratio_debt_payment_to_income'],color='navy')","63c6f563":"df[df['ratio_debt_payment_to_income'] > 1000]","74997544":"df['credit_line_utilization'] = df['credit_line_utilization'].str.replace(',' , '.').astype(float) ","9ee0f0c0":"df_test['credit_line_utilization'] = df_test['credit_line_utilization'].str.replace(',' , '.').astype(float)","ea17a5ee":"df['credit_line_utilization'].value_counts()","6c15c4e3":"print(df['credit_line_utilization'].min())\nprint(df['credit_line_utilization'].max())","2b88b585":"print(f'Minimum credit_line_utilization in our dataset is {df.credit_line_utilization.min()}.')\nprint(f'Maximum credit_line_utilization in our dataset is {df.credit_line_utilization.max()}.')","5da177a9":"plt.xlim(0,3)\nsns.distplot(df['credit_line_utilization'])","907171cc":"print(f'Median credit_line_utilization is: {df.credit_line_utilization.median()}')\nprint(f'Mean credit_line_utilization is: {df.credit_line_utilization.mean()}')","69741a2a":"plt.xlim(0,3)\nsns.boxplot(df['credit_line_utilization'],color='navy')","92fe5b54":"df['number_of_previous_late_payments_up_to_59_days'].value_counts()","ddb1f139":"df['number_of_previous_late_payments_up_to_59_days'].value_counts(normalize=True)","62ff666c":"print(df['number_of_previous_late_payments_up_to_59_days'].min())\nprint(df['number_of_previous_late_payments_up_to_59_days'].max())","b4018ab8":"print(f'Minimum number_of_previous_late_payments_up_to_59_days in our dataset is {df.number_of_previous_late_payments_up_to_59_days.min()}.')\nprint(f'Maximum number_of_previous_late_payments_up_to_59_days in our dataset is {df.number_of_previous_late_payments_up_to_59_days.max()}.')","9437f8d1":"df['number_of_previous_late_payments_up_to_59_days'].nunique() # We can see there are 13 unique values in this column.\nprint(f'Number of unique values in the \"number_of_previous_late_payments_up_to_59_days\" column is {df.number_of_previous_late_payments_up_to_59_days.nunique()}.')","04f9a16f":"plt.xlim(0,10)\nsns.distplot(df['number_of_previous_late_payments_up_to_59_days'])\nplt.show()  ","31ad9d46":"print(f'Median number_of_previous_late_payments_up_to_59_days is: {df.number_of_previous_late_payments_up_to_59_days.median()}')\nprint(f'Mean number_of_previous_late_payments_up_to_59_days is: {df.number_of_previous_late_payments_up_to_59_days.mean()}')","0913a266":"sns.boxplot(df['number_of_previous_late_payments_up_to_59_days'],color='navy')","ff3f4a64":"df['number_of_previous_late_payments_up_to_89_days'].value_counts()","10e16aa5":"df['number_of_previous_late_payments_up_to_89_days'].value_counts(normalize=True)","9569b57e":"print(df['number_of_previous_late_payments_up_to_89_days'].min())\nprint(df['number_of_previous_late_payments_up_to_89_days'].max())","f2f5b2dc":"print(f'Minimum number_of_previous_late_payments_up_to_89_days in our dataset is {df.number_of_previous_late_payments_up_to_89_days.min()}.')\nprint(f'Maximum number_of_previous_late_payments_up_to_89_days in our dataset is {df.number_of_previous_late_payments_up_to_89_days.max()}.')","322ff752":"df['number_of_previous_late_payments_up_to_89_days'].nunique() # We can see there are 11 unique values in this column.\nprint(f'Number of unique values in the \"number_of_previous_late_payments_up_to_89_days\" column is {df.number_of_previous_late_payments_up_to_89_days.nunique()}.')","9acc4eb8":"plt.xlim(0,10)\nsns.distplot(df['number_of_previous_late_payments_up_to_89_days'])\nplt.show()  ","9920a4b1":"print(f'Median number_of_previous_late_payments_up_to_89_days is: {df.number_of_previous_late_payments_up_to_89_days.median()}')\nprint(f'Mean number_of_previous_late_payments_up_to_89_days is: {df.number_of_previous_late_payments_up_to_89_days.mean()}')","d2fd8dc2":"# plt.xlim(0,3)\nsns.boxplot(df['number_of_previous_late_payments_up_to_89_days'],color='navy')","698444ff":"df['number_of_previous_late_payments_90_days_or_more'].value_counts()","71f6a0f7":"df['number_of_previous_late_payments_90_days_or_more'].value_counts(normalize=True)","66ad3425":"print(df['number_of_previous_late_payments_90_days_or_more'].min())\nprint(df['number_of_previous_late_payments_90_days_or_more'].max())","52767ca9":"print(f'Minimum number_of_previous_late_payments_90_days_or_more in our dataset is {df.number_of_previous_late_payments_90_days_or_more.min()}.')\nprint(f'Maximum number_of_previous_late_payments_90_days_or_more in our dataset is {df.number_of_previous_late_payments_90_days_or_more.max()}.')","c3ab912d":"df['number_of_previous_late_payments_90_days_or_more'].nunique() # We can see there are 18 unique values in this column.\nprint(f'Number of unique values in the \"number_of_previous_late_payments_90_days_or_more\" column is {df.number_of_previous_late_payments_90_days_or_more.nunique()}.')","7553c792":"plt.xlim(0,10)\nsns.distplot(df['number_of_previous_late_payments_90_days_or_more'])\nplt.show()  ","4720d16f":"print(f'Median number_of_previous_late_payments_90_days_or_more is: {df.number_of_previous_late_payments_90_days_or_more.median()}')\nprint(f'Mean number_of_previous_late_payments_90_days_or_more is: {df.number_of_previous_late_payments_90_days_or_more.mean()}')","2036bbd4":"# plt.xlim(0,3)\nsns.boxplot(df['number_of_previous_late_payments_90_days_or_more'],color='navy')","89f03d15":"df['defaulted_on_loan'].value_counts()","68a0e641":"df['defaulted_on_loan'].value_counts(normalize=True)","9ac9b45a":"df['defaulted_on_loan'].value_counts().plot(kind='bar', title='defaulted_on_loan',color='purple')\nplt.show()","0ee3d658":"# %%time\n# sns.pairplot(df, hue='defaulted_on_loan')","5ee4f781":"numeric = df.loc[:, df.columns!= 'defaulted_on_loan'].columns.to_list() #(All of our columns are numeric except target column)","28cda5f4":"categorical = []","2b108534":"df.isna().sum()","0dd5892f":"df.isna().sum().sum()","9cf4afb2":"X = df.drop(columns=\"defaulted_on_loan\",axis=1)\ny = df[\"defaulted_on_loan\"]   ","36294b1c":"X.isna().sum()","3f941d05":"X.isna().sum().sum()","61d5ce89":"X['age_z_score'] = (X['age'] - X.age.mean()) \/ X.age.std()\nX['number_dependent_family_members_score'] = (X['number_dependent_family_members'] - X.number_dependent_family_members.mean()) \/ X.number_dependent_family_members.std()\nX['monthly_income_score'] = (X['monthly_income'] - X.monthly_income.mean()) \/ X.monthly_income.std()\nX['number_of_credit_lines_score'] = (X['number_of_credit_lines'] - X.number_of_credit_lines.mean()) \/ X.number_of_credit_lines.std()\nX['real_estate_loans_score'] = (X['real_estate_loans'] - X.real_estate_loans.mean()) \/ X.real_estate_loans.std()\nX['ratio_debt_payment_to_income_score'] = (X['ratio_debt_payment_to_income'] - X.ratio_debt_payment_to_income.mean()) \/ X.ratio_debt_payment_to_income.std()\nX['credit_line_utilization_score'] = (X['credit_line_utilization'] - X.credit_line_utilization.mean()) \/ X.credit_line_utilization.std()\nX['number_of_previous_late_payments_up_to_59_days_score'] = (X['number_of_previous_late_payments_up_to_59_days'] - X.number_of_previous_late_payments_up_to_59_days.mean()) \/ X.number_of_previous_late_payments_up_to_59_days.std()\nX['number_of_previous_late_payments_up_to_89_days_score'] = (X['number_of_previous_late_payments_up_to_89_days'] - X.number_of_previous_late_payments_up_to_89_days.mean()) \/ X.number_of_previous_late_payments_up_to_89_days.std()\nX['number_of_previous_late_payments_90_days_or_more_score'] = (X['number_of_previous_late_payments_90_days_or_more'] - X.number_of_previous_late_payments_90_days_or_more.mean()) \/ X.number_of_previous_late_payments_90_days_or_more.std()\n","6b35b4fc":"X.loc[X['number_dependent_family_members_score'] > 3, 'number_dependent_family_members'] = np.nan\nX.loc[X['monthly_income_score'] > 3, 'monthly_income'] = np.nan\nX.loc[X['number_of_credit_lines_score'] > 3, 'number_of_credit_lines'] = np.nan\nX.loc[X['real_estate_loans_score'] > 3, 'real_estate_loans'] = np.nan\nX.loc[X['ratio_debt_payment_to_income_score'] > 3, 'ratio_debt_payment_to_income'] = np.nan\nX.loc[X['credit_line_utilization_score'] > 3, 'credit_line_utilization'] = np.nan\nX.loc[X['number_of_previous_late_payments_up_to_59_days_score'] > 3, 'number_of_previous_late_payments_up_to_59_days'] = np.nan\nX.loc[X['number_of_previous_late_payments_up_to_89_days_score'] > 3, 'number_of_previous_late_payments_up_to_89_days'] = np.nan\nX.loc[X['number_of_previous_late_payments_90_days_or_more_score'] > 3, 'number_of_previous_late_payments_90_days_or_more'] = np.nan","73bdb52e":"X = X.drop(columns=['age_z_score', 'number_dependent_family_members_score','monthly_income_score','number_of_credit_lines_score','real_estate_loans_score','ratio_debt_payment_to_income_score','credit_line_utilization_score','number_of_previous_late_payments_up_to_59_days_score','number_of_previous_late_payments_up_to_89_days_score','number_of_previous_late_payments_90_days_or_more_score'])","3213c504":"X.isna().sum() # We can see the change in amounts of missing values for each column.","b6de2ee4":"X.isna().sum().sum() ","9988d0d5":"<i> We can see quite a lot missing values existing in each of the columns.","41ebb014":"#### <font size='5'><i> 9. Number_of_previous_late_payments_up_to_89_days","3aa360ca":"<i> Number of real estate loans are mainly consist of 0 , 1 and 2","f9b600dd":"<i>As we have missing values for all of the columns, we have to thoroughly analyze each column in order to choose best <font color='red'>imputation strategy.","d93e64cc":"<font size='3'><i>We definitely have huge range of income and therefore outliers.<\/font>","d9bbfa6a":"## Target column (y_train)","60c30b90":"<i> <font size='4'>Previously total number of missing values was 52,951, however after detecting outliers per each column and changing these values to NaN, total number of NaN values increased to 56,026.","6b39ccb1":"<i><font size='3'> I have dealt with each of the column separately in case of outliers. ","58c5a6d6":"<i><font size='4'> During run of Models, I have tried detect Outliers through different methods, such us Isolation Forest and Elliptic Envelope. Hovewer, my model accuracy <font color='red'> increased significantly <\/font> a bit after dealing with the Outliers manually.","ecbbc627":"<i> <font size='3'> Here we can see the outliers as well. <\/font>","6143cc15":"### Dealing with Outliers","955ecb06":"<b> Except our <font color='red'>target<\/font> column (defaulted_on_loan), we have one column (credit_line_utilization) with  <font color='red'>object<\/font> DataType.","6dbcce88":"#### <font size='5'><i> 3. Monthly_income","a9dc652f":"#### <font size='5'><i> 7. Credit_line_utilization","f3914651":"<i> <font size='4'>Creating z-score columns for each of the columns, and later these z-score column will help us filter these values and replace them with NaN.","4fe5ddcf":"<I> In order to see the boxplot clearly, I put limit for xlim between 0 and 3","bc888f74":"<i>We have missing values in all of the columns except target column.","3d0fbcf0":"#### <font size='5'><i> Defaulted on Loan","4c41a134":"<i><font size='3.5'>For each column, I calculated z-score and changed value of the rows of column to 'NaN' whereas z-score is more than 3 which we can call Outliers. Later, inside of the pipeline, these NaN values will be imputed, which means we are not losing any rows with dropping outlier values.","c82bc9e2":"<font size='3'><i> I had to set xlim in order to have clear plot, without setting limit <font color='red' > outliers <\/font> are affecting so much. We have very much right skewed data.","e0ea0d33":"#### <font size='5'><i> 4. Number_of_credit_lines","770d3ecf":"<i> <font size='3'> Here we can see the outliers as well. <\/font>","64756feb":"<i> Having our Mean and Median values so close is powering our idea of having Normal distribution.","79f4c4ad":"<i>Same applies to the BoxPlot as well. We have very much outliers and in case of model builing we definitely need to deal with these Outliers.","9955dbb3":"<i><font size='4' color='black'> Credit_line_utilization column had \"object\" datatype. Therefore, after removing unnecessary commas, I have changed the type to float <\/font>","76fe802d":"<i> As, more than 80 percent rows has 0 late payments, we cannot see much from the boxplot, but the existing outliers","20fe4d39":"<i> As, more than 80 percent rows has 0 late payments, we cannot see much from the boxplot, but the existing outliers","5499ebfd":" <font color='purple'><h1><center>Explatory Data Analysis for [Step Project](https:\/\/www.kaggle.com\/c\/iba-ml2-mid-project)<\/center><\/h1>","1d97c1e8":"<i>There are 4 rows which has \"Number_dependent_family_members\" over 8.","835064cd":"<i>Importing  the Train and Test datasets.","7d8847cd":"<i><font size='4'>There are <font color='red' > almost no strong relationships <\/font>  among columns, however there are strong relationship among late payments columns which is quite understandable. <\/font>","5be7458f":"<i><font size='4'> It kind of looks like <font color='red'> \"Exponential Distribution\"","f1c9e4f6":"<i> <font size='4'>Dropping the z-score columns as we achieved our goal.","5c08413f":"<i><font size='4'>We definitely have very <font color='red'>imbalanced dataset<\/font>. Proportion of 1 is 7 percent whereas proportion of 0 is 93 percent.","56258a51":"<i><font size='4'>There are very much outliers in this column.","93fbba88":"#### <font size='5'><i> 5. Real_estate_loans","8b4afe71":"<i><font size='4'>There are many outliers in this column.","d5cee8c6":"<i> <font size='4'>Replacing values of columns where z-score is more than 3.","82c29f76":"<i> Because of existing outliers in this column, we need to use Median as Strategy for imputation.","8e8ad58a":"<i> We can say we have some <font color='red'> Outliers.","0fa2af28":"<i><font size='3'> We have some outliers as well.","4a58e7e9":"<i><font size='4'> Now, let's analyze each column one by one.<\/font>","24dd5767":"#### <font size='5'><i>2. Number_dependent_family_members","affc996e":"<i>We have 37 rows with monthly_income over than 100,000$.","6ecf5bf8":"<i><font size='3'>As it can be seen from above, more than half of number of dependents is <font color='red'>zero. ","0cfdfdb0":"## Train dataset","41f9fa41":"<i>Same applies to the BoxPlot as well. We have very much outliers and in case of model builing we definitely need to deal with these Outliers.","2d96565d":"<b><i> There are <font color='red'>11<\/font> columns and <font color='red'>72,161<\/font> rows in our train dataset.","2f043c61":"<i> Defining X and y for the pipeline and outlier handling will be done on X dataframe.","e857e281":"<i><font size='3'> As we can see from the Distribution plot, our distribution is normal and we have few outliers in our dataset. <\/font>","594c2aa9":"<i><font size='3'> As it is seen from the distribution plot, we have right-skewed distribution and a few outliers.","26663082":"<i>Basic look on dataset","99831238":"#### <font size='5'><i>1. Age","1efbef78":"#### <font size='5'><i> 6. Ratio_debt_payment_to_income","9f48370e":"<i> As, more than 94 percent rows has 0 late payments, we cannot see much from the boxplot, but the existing outliers","4534dd17":"<i>Later, we will change(astype) type of <font color='red'>credit_line_utilization<\/font> column to float.","9bc670f4":"#### <font size='5'><i> 10. Number_of_previous_late_payments_90_days_or_more","cd2f9405":"#### <font size='5'><i> 8. Number_of_previous_late_payments_up_to_59_days","c39225c8":"<i> We can see our target column is very <font color='red'> imbalanced <\/font>, therefore we can use Stratified Shuffle if we want to increase our accuracy.","466196c5":"<i> Dividing data into Categorical and Numeric parts"}}