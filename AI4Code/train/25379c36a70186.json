{"cell_type":{"e7b3daea":"code","eb416a29":"code","9b7ea3eb":"code","ce591fa7":"code","437e1675":"code","4ac99e2c":"code","cb3a7021":"code","403ef38b":"code","7de4dc15":"code","cdbc5cb1":"code","159a7de2":"code","7436e67b":"code","a51e769d":"code","31f11f15":"code","6ee1c334":"code","5f3b8442":"code","c5fa5d83":"code","60e4e3bb":"code","0467efa7":"code","ee5882d0":"code","39cf2e23":"markdown","c68a3825":"markdown","7a505712":"markdown","9cbfe4e9":"markdown","594b3710":"markdown","03eee7a1":"markdown","5720304c":"markdown","364e84c4":"markdown","c10491c1":"markdown","41b3839f":"markdown","5cc8b012":"markdown","3c4fefa8":"markdown","aaecf937":"markdown","683b669a":"markdown","d3249315":"markdown","cb3250df":"markdown","74d5ba92":"markdown","38f74006":"markdown"},"source":{"e7b3daea":"import gc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","eb416a29":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\n\nall_data = pd.concat([train, test])","9b7ea3eb":"all_data2 = all_data.drop(columns = ['id', 'claim'])\nall_data2","ce591fa7":"# Distribution\n\nplt.figure(figsize = (12, 6))\nmissing_values = all_data2.isnull().sum()[:-1]\nsns.histplot(missing_values, color='violet');\nplt.show()\n\nprint('\\n')\nprint('-------- Distribution of Missing values --------')\nprint('Min:', missing_values.min())\nprint('Max:', missing_values.max())\nprint('Mean:', missing_values.mean())\nprint('------------------------------------------------')","437e1675":"all_data2['n_missing'] = all_data2.isna().sum(axis=1)\nall_data2['std'] = all_data2.std(axis=1)\nall_data2['min'] = all_data2.min(axis=1)\nall_data2['max'] = all_data2.max(axis=1)\nall_data2","4ac99e2c":"miss_one_hot = all_data2.iloc[:, :118].isna()\nmiss_one_hot.columns = [f'missing_f_{i}' for i in range(118)]\nmiss_one_hot","cb3a7021":"all_data3 = pd.concat([all_data2, miss_one_hot], axis = 1)","403ef38b":"del all_data, all_data2, test, miss_one_hot\ngc.collect()","7de4dc15":"train2 = all_data3[:len(train)]\ntest2 = all_data3[len(train):]\ny = train['claim']","cdbc5cb1":"del train\ngc.collect()","159a7de2":"sc = StandardScaler()\nsi = SimpleImputer()\n\ntrain2.iloc[:, :118] = si.fit_transform(sc.fit_transform(train2.iloc[:, :118]))\ntest2.iloc[:, :118] = si.fit_transform(sc.fit_transform(test2.iloc[:, :118]))","7436e67b":"def Stacking_Data_Loader(model, model_name, x_train, y_train, x_test, fold):\n    '''\n    Put your train, test datasets and fold value!\n    This function returns train, test datasets for stacking ensemble :)\n    '''\n\n    stk = StratifiedKFold(n_splits = fold, random_state = 42, shuffle = True)\n    \n    # Declaration Pred Datasets\n    train_fold_pred = np.zeros((x_train.shape[0], 1))\n    test_pred = np.zeros((x_test.shape[0], fold))\n    \n    for counter, (train_index, valid_index) in enumerate(stk.split(x_train, y_train)):\n        x_train, y_train = train2.iloc[train_index], y[train_index]\n        x_valid, y_valid = train2.iloc[valid_index], y[valid_index]\n\n        print('------------ Fold', counter+1, 'Start! ------------')\n        if model_name == 'cat':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)])\n        elif model_name == 'xgb':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric = 'auc', verbose = 500, early_stopping_rounds = 200)\n        else:\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric = 'auc', verbose = 100, early_stopping_rounds = 200)\n            \n        print('------------ Fold', counter+1, 'Done! ------------')\n        \n        train_fold_pred[valid_index, :] = model.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        test_pred[:, counter] = model.predict_proba(x_test)[:, 1]\n    \n    test_pred_mean = np.mean(test_pred, axis = 1).reshape(-1, 1)\n\n    print('Done!')\n    \n    return train_fold_pred, test_pred_mean","a51e769d":"lgb1_params = {\n    'objective': 'binary',\n    'n_estimators': 10000,\n    'random_state': 42,\n    'learning_rate': 0.095,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device' : 'gpu',\n    'max_depth' : 3,\n    'num_leaves' : 7\n}\n\nlgb2_params = {\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'random_state' : 42,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n    'device' : 'gpu',\n    'objective' : 'binary'\n}\n\nxgb1_params = {\n      'tree_method' : 'gpu_hist', \n      'learning_rate' : 0.01,\n      'n_estimators' : 50000,\n      'colsample_bytree' : 0.3,\n      'subsample' : 0.75,\n      'reg_alpha' : 19,\n      'reg_lambda' : 19,\n      'max_depth' : 5, \n      'predictor' : 'gpu_predictor'\n}\n\ncat1_params = {\n     'depth' : 5,\n     'grow_policy' : 'SymmetricTree',\n     'l2_leaf_reg' : 3.0,\n     'random_strength' : 1.0,\n     'learning_rate' : 0.02,\n     'iterations' : 10000,\n     'loss_function' : 'CrossEntropy',\n     'eval_metric' : 'AUC',\n     'use_best_model' : True,\n     'early_stopping_rounds' : 200,\n     'task_type' : 'GPU',\n     'verbose' : 1000,\n}\n\ncat2_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli', \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'eval_metric' : 'AUC',\n    'verbose' : 1000,\n    'early_stopping_rounds' : 200,\n}","31f11f15":"lgbm_1 = LGBMClassifier(**lgb1_params)\nlgbm_2 = LGBMClassifier(**lgb2_params)\n\n# xgb = XGBClassifier(**xgb1_params)\n\n# cat_1 = CatBoostClassifier(**cat1_params)\n# cat_2 = CatBoostClassifier(**cat2_params)","6ee1c334":"del all_data3\ngc.collect()","5f3b8442":"# cat1_train, cat1_test = Stacking_Data_Loader(cat_1, 'cat', train2, y, test2, 5)\n# del cat_1\n# gc.collect()\n\n# cat2_train, cat2_test = Stacking_Data_Loader(cat_2, 'cat', train2, y, test2, 5)\n# del cat_2\n# gc.collect()\n\nlgbm1_train, lgbm1_test = Stacking_Data_Loader(lgbm_1, 'lgbm', train2, y, test2, 5)\ndel lgbm_1\ngc.collect()\n\nlgbm2_train, lgbm2_test = Stacking_Data_Loader(lgbm_2, 'lgbm', train2, y, test2, 5)\ndel lgbm_2\ngc.collect()\n\n# xgb_train, xgb_test = Stacking_Data_Loader(xgb, 'xgb', train2, y, test2, 5)\n# del xgb\n# gc.collect()","c5fa5d83":"stack_x_train = np.load('..\/input\/catboost-xgboost-stacking-datasets\/stack_x_train.npy')\nstack_x_test = np.load('..\/input\/catboost-xgboost-stacking-datasets\/stack_x_test (1).npy')\n\nstack_x_train = np.concatenate((stack_x_train, lgbm1_train, lgbm2_train), axis = 1)\nstack_x_test = np.concatenate((stack_x_test, lgbm1_test, lgbm2_test), axis = 1)\n\nstack_x_train","60e4e3bb":"# stack_x_train = np.concatenate((cat1_train, cat2_train, xgb_train, lgbm1_train, lgbm2_train), axis = 1)\n# stack_x_test = np.concatenate((cat1_test, cat2_test, xgb_test, lgbm1_test, lgbm2_test), axis = 1)\n# stack_x_train","0467efa7":"stk = StratifiedKFold(n_splits = 5)\n\ntest_pred = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train[valid_index], y[valid_index]\n    \n    lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 1000, max_iter = 1000)\n    lr.fit(x_train, y_train)\n    \n    valid_pred = lr.predict_proba(x_valid)[:, 1]\n    test_pred += lr.predict_proba(stack_x_test)[:, 1]\n    auc = roc_auc_score(y_valid, valid_pred)\n    total_auc += auc \/ 5\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","ee5882d0":"sub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub['claim'] = test_pred\nsub.to_csv('sub.csv', index = 0)","39cf2e23":"# **Import Library**","c68a3825":"### **Stacking**\n\n* Making train, test prediction array!\n* Concat 5 arrays in 1 dataset\n* Thanks to kenneth Q's nice notebook (https:\/\/www.kaggle.com\/kennethquisado\/xgboost-10fold-cv-blend)","7a505712":"# **Load Data**","9cbfe4e9":"### **Distribution of Missing values**","594b3710":"### **Stacking Data Loader**","03eee7a1":"#### **Stacking Ensemble** is a nice technique for forwarding you score.\n#### As you can see below image, Stacking Ensemble needs some models for classification and meta-model for final prediction!","5720304c":"### Thank you for visiting my notebook :)\n### This notebook is for beginner like me **who wants to study stacking ensemble!**","364e84c4":"# Feature Generation\n\n*   Thanks to BIZEN's notebook, we can use those missing value counts for feature! [Check BIZEN's Notebook!](https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm)\n\n*   **I made Missing-Value's One-Hot Encoded columns for training missing value(Y\/N)**","c10491c1":"# **Submission!**","41b3839f":"#### Here's what you need to do.\n**Step1. Make your train, test data for training & prediction (Preprocessing)**\n\n**Step2. Select some models for making stacking datasets!! (Train models and Making Datasets)**\n\n**Step3. Select final model for meta-model!**\n\n**Step4. With your meta-model, Train & Predict with stacking datasets ;)**","5cc8b012":"# **TPS - Sep 2021**\n\n## **XGBoost & LightGBM & CatBoost Stacking**","3c4fefa8":"# **Handle missing values**\n\n*   We can use mean values to handle missing values.\n*   Or, we can predict missing values with clean data.\n","aaecf937":"# **Modeling**","683b669a":"# Done!\n\n\n## If you think this notebook is helpful for you, Please do not forget upvote!","d3249315":"![Stacking Ensemble](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier_files\/stackingclassification_overview.png)","cb3250df":"### **Modeling**","74d5ba92":"#### Model's HyperParameters\n* LGBM2 Param : https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm\n* Cat2 Param : https:\/\/www.kaggle.com\/mlanhenke\/tps-09-single-catboostclassifier-0-81676\n\nThanks for Sharing!","38f74006":"### **Final Stacking Datasets!**"}}