{"cell_type":{"cd183d83":"code","66899cfe":"code","61119e5a":"code","a2394b84":"code","fd804bd1":"code","4bc5c91d":"code","fe4a6bf3":"code","22593bff":"code","68e42b5b":"code","ed51253e":"code","be956ccb":"code","42365da2":"code","67d11d16":"code","89f95728":"code","d09eed83":"code","509fe43e":"code","7b2e223c":"code","e66432ec":"code","b89320a1":"code","4488e453":"code","74eab01f":"code","7cb8e6ad":"code","d089b78d":"code","9d5bdca5":"code","e5668226":"code","5acccd51":"code","64402068":"code","4b0de693":"code","ffdffd56":"code","22bb4e95":"code","3da49e36":"code","b112d4f8":"code","df0fa879":"code","7f0bcfcb":"code","a7462154":"code","bee6af59":"markdown","7bfa88a0":"markdown","74397fb7":"markdown","8af0c869":"markdown","7b5c1757":"markdown","1196e5af":"markdown","1e840f88":"markdown","ef002a05":"markdown","57457080":"markdown","1e64ac44":"markdown","2ad5fa60":"markdown","d8e73cff":"markdown","24273aed":"markdown","28f0ac32":"markdown","62f48ea0":"markdown","438f202f":"markdown","647c6faa":"markdown"},"source":{"cd183d83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66899cfe":"%cd \/kaggle\/working","61119e5a":"!pip install transformers","a2394b84":"!pip install simpletransformers==0.32.3","fd804bd1":"import pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (GPT2Config,GPT2LMHeadModel,GPT2Tokenizer)\nimport torch\nfrom string import punctuation as pnc\nfrom collections import Counter\nfrom scipy import spatial\nfrom bs4 import BeautifulSoup\nfrom tqdm.notebook import tqdm\nimport torch\nimport pylab as pl\npd.set_option('display.max_colwidth', -1)","4bc5c91d":"import nltk\nnltk.download('stopwords')","fe4a6bf3":"questions = pd.read_csv(\"\/kaggle\/input\/pythonquestions\/Questions.csv\", encoding = \"ISO-8859-1\")\nprint(len(questions))\ndisplay(questions.head(5))","22593bff":"print(\"Number of unique Questions : \", questions['Id'].nunique())","68e42b5b":"questions['qLen'] = questions['Title'].apply(lambda x : len(x.split(\" \")))\nquestions['qBodyLen'] = questions['Body'].apply(lambda x : len(x.split(\" \")))","ed51253e":"questions['qLen'].hist(bins=35)\nplt.title(\"No. of words in Title\")","be956ccb":"questions[questions['qBodyLen']<500]['qBodyLen'].hist(bins=100)\nplt.title(\"No. of words in Body\")","42365da2":"def getWordCloud(df,col):\n  comment_words = '' \n  stopwords = set(STOPWORDS) \n    \n  for val in tqdm(df[col]): \n        \n      val = str(val) \n      tokens = val.split() \n        \n      for i in range(len(tokens)): \n          tokens[i] = tokens[i].lower() \n        \n      comment_words += \" \".join(tokens)+\" \"\n    \n  wordcloud = WordCloud(width = 800, height = 800, \n                  background_color ='white', \n                  stopwords = stopwords, \n                  min_font_size = 10).generate(comment_words) \n    \n                       \n  plt.figure(figsize = (5, 5), facecolor = None) \n  plt.imshow(wordcloud) \n  plt.axis(\"off\")\n  plt.tight_layout(pad = 0) \n    \n  plt.show()","67d11d16":"getWordCloud(questions,'Title')","89f95728":"stop = stopwords.words('english')\ndef preprocess(df, col):\n  df['preprocessed'+col] = df[col].apply(lambda x : \" \".join([word for word in x.split(\" \") if word not in stop]))\n  df['preprocessed'+col] = df['preprocessed'+col].str.replace('[^a-zA-Z0-9 ]', '')\n  df['preprocessed'+col] = df['preprocessed'+col].str.lower()\n  return df","d09eed83":"questions = preprocess(questions, 'Title')","509fe43e":"tags = pd.read_csv(\"\/kaggle\/input\/pythonquestions\/Tags.csv\", encoding = \"ISO-8859-1\")\nprint(len(tags))\ndisplay(tags.head(5))","7b2e223c":"print(\"Number of unique Tags : \", tags['Tag'].nunique())","e66432ec":"fig, ax = plt.subplots()\ntags[tags['Tag']!='python']['Tag'].value_counts().sort_values(ascending = False)[:20].plot(ax=ax, kind='bar')","b89320a1":"config_class, model_class, tokenizer_class = GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\nmodel = model_class.from_pretrained('gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","4488e453":"preprocessedTitle = questions['preprocessedTitle'].values\nQID = questions['Id'].values\nprint(len(preprocessedTitle), len(QID))","74eab01f":"encodedpreprocessedTitle = tokenizer.batch_encode_plus(preprocessedTitle)['input_ids']\nprint(len(encodedpreprocessedTitle))","7cb8e6ad":"embeddigs = model.transformer.wte\nprint(\"Shape of embedding matrix : \",embeddigs.weight.shape)\nprint(\"Type of embedding matrix : \", type(embeddigs))","d089b78d":"TitleEmbeddingList = []\nQIDList = []\nfor idx, (qid, encodedTitle) in tqdm(enumerate(zip(QID, encodedpreprocessedTitle))):\n  if len(encodedTitle) > 0 :\n    embeddedTitle = embeddigs(torch.tensor(encodedTitle).to(torch.int64)).mean(axis=0)\n    TitleEmbeddingList.append(embeddedTitle)\n    QIDList.append(qid)","9d5bdca5":"numQ = len(TitleEmbeddingList)\nembedDim = len(TitleEmbeddingList[0])\nprint(\"Number of Titles : \",numQ,\" and Length of vector of each Title : \",embedDim)","e5668226":"print(\"Type of TitleEmbeddingList : \",type(TitleEmbeddingList))","5acccd51":"TitleEmbeddingTensor = torch.cat(TitleEmbeddingList, dim=0)\nTitleEmbeddingTensor = torch.reshape(TitleEmbeddingTensor, (numQ, embedDim))\nprint(\"Shape of TitleEmbeddingTensor : \",TitleEmbeddingTensor.shape)\nprint(\"Type of TitleEmbeddingTensor : \", type(TitleEmbeddingTensor))","64402068":"def preprocesstext(text):\n  text =  \" \".join([word for word in text.split(\" \") if word not in stop])\n  text = re.sub(r'[^a-zA-Z0-9 ]','',text)\n  text = text.lower()\n  return text","4b0de693":"def getMostSimilarQuestionsIdx(K, a, b):\n  a_norm = a \/ a.norm(dim=1)[:, None]\n  b_norm = b \/ b.norm(dim=1)[:, None]\n  res = torch.mm(a_norm, b_norm.transpose(0,1)).squeeze(0)\n  res = res.tolist()\n  mostSimIdx = sorted(range(len(res)), key=lambda x: res[x])[-K:]\n  return mostSimIdx","ffdffd56":"def getMostSimilarQuestions(K, input, QuestionDF, QIDList):\n  input = input\n  preprocessedinput = preprocesstext(input)\n  inputEncoded = tokenizer.batch_encode_plus([preprocessedinput])['input_ids']\n  inputEmbedded = embeddigs(torch.tensor(inputEncoded).to(torch.int64)).squeeze(0).mean(axis=0).unsqueeze(0)\n  mostSimilarIdx = getMostSimilarQuestionsIdx(K, inputEmbedded, TitleEmbeddingTensor)\n  mostSimilarIdx.reverse()\n  print(\"Most similar \",K, \" questions : \")\n  for idx, simidx in enumerate(mostSimilarIdx):\n    IDQ = QuestionDF[QuestionDF['Id']==QIDList[simidx]][['Id','Title']].values\n    parentId = IDQ[0][0]\n    simQuestion = IDQ[0][1]\n    print((idx+1), \"Question Id : \", parentId, \"Question : \",simQuestion)","22bb4e95":"getMostSimilarQuestions(5, \"How to MUltiply 2 columns pandas ?\", questions ,QIDList)","3da49e36":"getMostSimilarQuestions(5, \"regex pandas\", questions ,QIDList)","b112d4f8":"getMostSimilarQuestions(5, \"logistic regression sklearn\", questions ,QIDList)","df0fa879":"getMostSimilarQuestions(5, \"covert csv to json file pandas\", questions ,QIDList)","7f0bcfcb":"getMostSimilarQuestions(5, \"Build website using python\", questions ,QIDList)","a7462154":"getMostSimilarQuestions(5, \"How to install Pandas\", questions ,QIDList)","bee6af59":"# In this Notebook, I have created a search engine that looks through the Collection of StackOverFlow Python Questions(collected between 2008-2016) and retrieves the Questions most similar to input python related question.   ","7bfa88a0":"## In the below code, I have taken the mean across embeddings of all the tokens in a particular Title. So after taking the mean every Title would be represented by a 768 length vector. And this same thing has been done for all the 607282 Titles in the for loop. \n## Note -- There might be cases where number of tokens in the processed Title can be 0, hence the condition of len(encodedTitle) > 0 has been applied.","74397fb7":"## Print out the Most Similar Question Titles With the Question ID","8af0c869":"## PreProcess the Input text","7b5c1757":"## 20  Most frequent Tags Except Python which is obviously the most frequent.","1196e5af":"## Using tokenizer.batch_encode_plus to encode all the titles in 1 go. Instead of tokenizer.batch_encode_plus, tokenizer.encode can be used to encode 1 instance at a time","1e840f88":"## Loading the embedding from GPT2 Model. Each token in the embedding matrix is 768 length vector and the embedding has 50257 unique tokens.","ef002a05":"## Perform Cosine Similarity between the input question and all the StackOverFlow Titles and Get index of the most simillar K Titles ","57457080":"## Number of words in Title and Body","1e64ac44":"## Most of the Titles have around 7 words while for Body it stands at around 50. So, to get the most similar questions I will use Title instead  of Body.","2ad5fa60":"## Tags","d8e73cff":"# Once we created the embedding representation of each title. Now we are going to feed in an input question and search among StackOverflow Question Titles which ones are most similar to the input by using cosine similarity between embedding of input question and the titles. ","24273aed":"## Converting  TitleEmbeddingList from List of tensors to tensor.","28f0ac32":"## Word Cloud","62f48ea0":"## Lets test !!!","438f202f":"## Encoding the Processed Question Title and Embedding it using GPT2 Tokenizer. ","647c6faa":"## Preprocessing the title -- Removal of Stop Words, non alphanumeric charcters, and lower case all the charcters."}}