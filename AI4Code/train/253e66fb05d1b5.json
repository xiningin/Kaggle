{"cell_type":{"9fe68548":"code","7d9b73c9":"code","cd28e2d0":"code","49cba1bf":"code","aa024679":"code","64b0964f":"code","6eb702d6":"code","a2f8a2df":"code","7983414e":"code","05a62031":"code","0745d235":"code","1d1c6f76":"code","f8db61ed":"code","dc2a6867":"code","22bbf764":"code","ae201452":"code","314fbc7a":"code","dc4a861c":"code","8a06f444":"code","3f85fcfe":"code","4b0e94d2":"code","499bba5b":"code","6af3adbc":"code","30ccc080":"code","4a031b1d":"code","05caa545":"code","0a87b77f":"code","88fa304a":"code","11a71637":"code","86951d10":"code","347b40c0":"code","b4f0a4b4":"code","26e40148":"code","c8593851":"code","5cde1dd0":"code","4b9eae93":"code","f61cfe46":"code","f90fb675":"code","7c6fd5b1":"code","4948a65a":"code","14480db9":"code","bf380034":"code","db0b37cc":"code","58eb6e3c":"code","027ca4cd":"code","94f508b9":"code","272b3440":"code","7ec45d9d":"code","a508480e":"code","6d2d2798":"code","89ed65db":"code","a39f1966":"code","9e6f6795":"code","c7ab7f56":"code","c68cc462":"code","044b0fcd":"code","ff6c2522":"code","ab385fa2":"code","84b4cb1e":"code","4ac27c0f":"code","00c42eff":"code","cc478897":"code","dbf93fcf":"code","87f0e6c6":"code","6bca050c":"code","5ada8814":"markdown","cf10dfe9":"markdown","65fecca4":"markdown","bef95009":"markdown","b00b01ed":"markdown","42c952d8":"markdown","0deb4e85":"markdown","103f30ba":"markdown","1b0887ee":"markdown","72bf3828":"markdown","84cfa4b9":"markdown","75bac4e2":"markdown","d03062eb":"markdown","43941ddd":"markdown","689f4f2e":"markdown","5cedc9d4":"markdown","6fe29130":"markdown","d25fee19":"markdown","3ff9234a":"markdown","22e3ebda":"markdown","d81af476":"markdown","167a192c":"markdown","1fd31c48":"markdown","e111d151":"markdown","56b054d6":"markdown","4e95459f":"markdown","902d4a85":"markdown","9b09e21a":"markdown","6fc4f827":"markdown","1395a47a":"markdown","84b8a096":"markdown","bb2aeeaf":"markdown","918a95ca":"markdown","a0174b57":"markdown","141fb52a":"markdown","661cce05":"markdown"},"source":{"9fe68548":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7d9b73c9":"# Read the data\ndframe = pd.read_csv(\"..\/input\/Admission_Predict_Ver1.1.csv\")","cd28e2d0":"dframe.head()","49cba1bf":"# Check whether there are empty rows or not.\ndframe.info()","aa024679":"dframe.describe()","64b0964f":"# Correlation \n# We excluded \"Serial No\" with data.iloc[:,1:]) )\n\ndframe.iloc[:,1:].corr()","6eb702d6":"# Correlation map\nf, axx = plt.subplots(figsize=(10,10))\nsns.heatmap(dframe.iloc[:,1:].corr(), linewidths=0.5, cmap=\"Blues\", annot=True,fmt=\".1f\", ax=axx)\nplt.show()","a2f8a2df":"# Drop the duplicated values of the Chance of Admit.\ndf= dframe.drop_duplicates(subset=[\"Chance of Admit \"])\ndf.info()","7983414e":"df= df.drop_duplicates(subset=\"CGPA\")\ndf= df.drop_duplicates(subset=\"GRE Score\")\ndf= df.drop_duplicates(subset=\"TOEFL Score\")\ndf.info()","05a62031":"df.describe()","0745d235":"# Correlation \n# We excluded \"Serial No\" with data.iloc[:,1:]) )\n\ndf.iloc[:,1:].corr()","1d1c6f76":"# Correlation map\nf, axx = plt.subplots(figsize=(10,10))\nsns.heatmap(df.iloc[:,1:].corr(), linewidths=0.5, cmap=\"Blues\", annot=True,fmt=\".2f\", ax=axx)\nplt.show()","f8db61ed":"df.columns","dc2a6867":"# Mean value of \"Chance of Admit \" is 0.677368.\n# Output is on above; df.describe()\n\n# Create a new column for High and Low.\n\ndf[\"Admit Level\"] = [\"Low\" if each < 0.677368 else \"High\" for each in df[\"Chance of Admit \"]]\ndf.head()","22bbf764":"df.info()","ae201452":"# Vizualization\n# CGPA, GRE Score and TOEFL Scores \/ Chance of Admit\n\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter(\n                        x = df[\"Chance of Admit \"],\n                        y = df.CGPA,\n                        mode = \"markers\",\n                        name = \"CGPA\",\n                        marker = dict(color=\"rgba(255, 100, 128, 0.8)\"),\n                        text = df[\"Admit Level\"]\n                        )\ntrace2 = go.Scatter(\n                        x = df[\"Chance of Admit \"],\n                        y = df[\"GRE Score\"],\n                        mode = \"markers\",\n                        name = \"GRE Score\",\n                        marker = dict(color=\"rgba(80, 80, 80, 0.8)\"),\n                        text = df[\"Admit Level\"]\n                        )\ntrace3 = go.Scatter(\n                        x = df[\"Chance of Admit \"],\n                        y = df[\"TOEFL Score\"],\n                        mode = \"markers\",\n                        name = \"TOEFL Score\",\n                        marker = dict(color=\"rgba(0, 128, 255, 0.8)\"),\n                        text = df[\"Admit Level\"]\n                        )\ndata = [trace1, trace2, trace3]\nlayout = dict(title=\"CGPA, GRE Score and TOEFL Scores v Chance of Admit\",\n             xaxis=dict(title=\"Chance of Admit\", ticklen=5, zeroline=False),\n             yaxis=dict(title=\"Values\", ticklen=5, zeroline=False)\n             )\nfig = dict(data=data, layout=layout)\niplot(fig)","314fbc7a":"# Sklearn library\nfrom sklearn.linear_model import LinearRegression\n\nlinear_reg = LinearRegression()","dc4a861c":"print(df.CGPA.values.shape)\nprint(df[\"Chance of Admit \"].values.shape)\n\n# Reshape\nx = df.CGPA.values.reshape(-1,1)\ny = df[\"Chance of Admit \"].values.reshape(-1,1)\nprint(\"After resphape:\\nX:\", x.shape)\nprint(\"Y:\", y.shape)","8a06f444":"linear_reg.fit(x,y)","3f85fcfe":"# Formula\n# y = b0 + b1*x\n\nb0 = linear_reg.intercept_\nprint(\"b0:\", b0) # the spot where the linear line cuts the y-axis\n\nb1 = linear_reg.coef_\nprint(\"b1:\", b1) # slope\n\nprint(\"Linear Regression Formula:\", \"y = {0} + {1}*x\".format(b0,b1))","4b0e94d2":"x[0:5]","499bba5b":"# CGPA-9.65 = Chance of Admit -0.92\ndf[df.CGPA == 9.65].loc[:,\"Chance of Admit \"]","6af3adbc":"linear_reg.predict([[9.8]])","30ccc080":"print(min(x), max(x))","4a031b1d":"# CGPA values that will be predicted.\n\n# Chance of Admit (predicted values)\ny_head = linear_reg.predict(x)\n\nplt.figure(figsize=(10,10))\nplt.scatter(x,y, alpha=0.7)  # Real values (blue)\nplt.plot(x,y_head, color=\"red\") # Predicted values for numpay array (arr).\nplt.show()","05caa545":"# Same shapes\nprint(y.shape, y_head.shape)","0a87b77f":"# R Square Library\nfrom sklearn.metrics import r2_score\n# y: Chance of Admit values\n# y_head: predicted Chance of Admit values with LR\nprint(\"r_square score: \", r2_score(y, y_head))","88fa304a":"# Sklearn library\n# we already imported -- > from sklearn.linear_model import LinearRegression","11a71637":"# Define and reshape the variables\n\nx1 = df.loc[:, [\"CGPA\", \"GRE Score\", \"TOEFL Score\"]]\ny1 = df[\"Chance of Admit \"].values.reshape(-1,1)","86951d10":"# Creat the model and fit the x&y values.\nmultiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x1,y1)","347b40c0":"# Formula\n# y = b0 + b1*x1 + b2*x2 + ... bn*xn\nb0 = multiple_linear_regression.intercept_\nb1,b2,b3 = zip(*multiple_linear_regression.coef_) \nprint(\"b1:\", b1, \"b2:\", b2, \"b3:\", b3)\nprint(\"b0:\", multiple_linear_regression.intercept_)\nprint(\"b1, b2:\", multiple_linear_regression.coef_)\nprint(\"Multiple Linear Regression Formula:\", \"y = {0} + {1}*x1 + {2}*x2 + {3}*x3\".format(b0,b1,b2,b3))","b4f0a4b4":"print(\"CGPA:\", min(x1[\"CGPA\"]),\"-\", max(x1[\"CGPA\"]))\nprint(\"GRE Score:\", min(x1[\"GRE Score\"]),\"-\", max(x1[\"GRE Score\"]))\nprint(\"TOEFL Score:\", min(x1[\"TOEFL Score\"]), \"-\", max(x1[\"TOEFL Score\"]))\nplt.figure(figsize=(10,5))\nplt.scatter(df[\"Chance of Admit \"], df.CGPA, color=\"blue\", label=\"CGPA\")\nplt.scatter(df[\"Chance of Admit \"], df[\"GRE Score\"], color=\"green\", label=\"GRE Score\")\nplt.scatter(df[\"Chance of Admit \"], df[\"TOEFL Score\"], color=\"orange\", label=\"TOEFL Score\")\nplt.legend()\nplt.show()","26e40148":"# 1st: CGPA: 6.8 - 9.92\n# 2nd: GRE Score: 290 - 340\n# 3rd: TOEFL Score: 92 - 120\n# Prediction: Chance of Admit\n\nprint(\"Values= np.array( [[6,280,90]])) Prediction =\",\n      multiple_linear_regression.predict(np.array( [[6,280,90]])))\n\nprint(\"Values= np.array( [[8,300,100]])) Prediction =\",\n      multiple_linear_regression.predict(np.array( [[8,300,100]])))\n\nprint(\"Values= np.array( [[10,350,130]])) Prediction =\",\n      multiple_linear_regression.predict(np.array( [[10,350,130]])))","c8593851":"x1.head()","5cde1dd0":"y1_head = multiple_linear_regression.predict(x1)\ny1_head[:5]","4b9eae93":"plt.figure(figsize=(10,20))\n\nplt.scatter(y, x1.iloc[:,0], color=\"blue\", alpha=0.7) # CGPA\nplt.scatter(y1_head, x1.iloc[:,0], color=\"black\", alpha=0.7)\n\nplt.scatter(y, x1.iloc[:,1], color=\"green\", alpha=0.7) # GRE Score\nplt.scatter(y1_head, x1.iloc[:,1], color=\"black\", alpha=0.7)\n\nplt.scatter(y, x1.iloc[:,2],color=\"orange\", alpha=0.7) # TOEFL  Score\nplt.scatter(y1_head, x1.iloc[:,2], color=\"black\", alpha=0.7)\nplt.show()","f61cfe46":"# R Square Library\n\n# Imported on previous sections\n# from sklearn.metrics import r2_score\n\n# y: Chance of Admit values\n# y1_head: predicted Chance of Admit values with MLR\nprint(\"r_square score: \", r2_score(y,y1_head))","f90fb675":"# Sklearn library \nfrom sklearn.preprocessing import PolynomialFeatures\n\n# We have chose the second degree equation with (degree=2)\npolynomial_regression = PolynomialFeatures(degree=2)\n# y = b0 + b1*x + b2*x^2\nx = df[\"TOEFL Score\"].values.reshape(-1,1)\n# y = df[\"Chance of Admit \"].values.reshape(-1,1)\nx_ploynominal = polynomial_regression.fit_transform(x)\n\nlinear_regression_poly = LinearRegression()\nlinear_regression_poly.fit(x_ploynominal, y)","7c6fd5b1":"# Linear Regression (LR) section: x = df.CGPA.values.reshape(-1,1)\n# Linear Regression (LR) section: y = df[\"Chance of Admit \"].values.reshape(-1,1)\nprint(\"x:\\n\", x[:5], \"\\ny:\\n\",y[:5])","4948a65a":"# Predicted values\ny_head_poly = linear_regression_poly.predict(x_ploynominal)\ny_head_poly[:5]","14480db9":"plt.figure(figsize=(10,10))\nplt.scatter(x, y, color=\"blue\", alpha=0.7) # CGPA\nplt.scatter(x, y_head_poly, label=\"poly (degree=2)\", color=\"black\") # predicted Chance of Admit\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"chance\")\nplt.legend()\nplt.show()","bf380034":"# y = b0 + b1*x + b2*x^2 + ..... b10*x^10\npolynomial_regression7 = PolynomialFeatures(degree=7)\n\n# x = df.CGPA.values.reshape(-1,1)\nx_ploynominal_7 = polynomial_regression7.fit_transform(x)\n\nlinear_regression_poly_7 = LinearRegression()\nlinear_regression_poly_7.fit(x_ploynominal_7, y)\n\n# Predicted values\ny_head_poly_7 = linear_regression_poly_7.predict(x_ploynominal_7)","db0b37cc":"# y = b0 + b1*x + b2*x^2 + ..... b30*x^30\npolynomial_regression30 = PolynomialFeatures(degree=30)\n\n# x = df.CGPA.values.reshape(-1,1)\nx_ploynominal_30 = polynomial_regression30.fit_transform(x)\n\nlinear_regression_poly_30 = LinearRegression()\nlinear_regression_poly_30.fit(x_ploynominal_30, y)\n\n# Predicted values\ny_head_poly_30 = linear_regression_poly_30.predict(x_ploynominal_30)","58eb6e3c":"plt.figure(figsize=(12,12))\nplt.scatter(x, y, color=\"blue\", alpha=0.7) # TOEFL Score\nplt.scatter(x, y_head_poly, label=\"poly (degree=2)\", color=\"black\", alpha=\"0.7\") # predicted Chance of Admit\nplt.scatter(x, y_head_poly_7, label=\"poly (degree=7)\", color=\"red\", alpha=\"0.7\") # predicted Chance of Admit\nplt.scatter(x, y_head_poly_30, label=\"poly (degree=30)\", color=\"green\", alpha=\"0.7\") # predicted Chance of Admit\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"chance\")\nplt.legend()\nplt.show()","027ca4cd":"# R Square Library\n\n# Imported on previous sections\n# from sklearn.metrics import r2_score\n\nprint(\"r_square score for degree=2: \", r2_score(y, y_head_poly))\nprint(\"r_square score for degree=7: \", r2_score(y, y_head_poly_7))\nprint(\"r_square score for degree=30: \", r2_score(y, y_head_poly_30))\n","94f508b9":"df.head()","272b3440":"# Decision Tree Library\nfrom sklearn.tree import DecisionTreeRegressor\n\nx = df[\"TOEFL Score\"].values.reshape(-1,1)\ny = df[\"Chance of Admit \"].values.reshape(-1,1)\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)","7ec45d9d":"plt.scatter(df[\"TOEFL Score\"] , df[\"Chance of Admit \"],alpha=0.8)\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","a508480e":"y_head_dtr = tree_reg.predict(x)","6d2d2798":"plt.scatter(x, y, color=\"blue\", alpha = 0.7)\nplt.scatter(x, y_head_dtr, color=\"black\", alpha = 0.4)\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","89ed65db":"# Let's make a new array in the range of TOEFL Score values increased by 0.01\nx001= np.arange(min(x), max(x), 0.01).reshape(-1,1) # (start, end, increase value)\ny_head001dtr = tree_reg.predict(x001)","a39f1966":"len(np.unique(y_head001dtr))\n\n# 19 unique values for all values","9e6f6795":"plt.figure(figsize=(20,10))\nplt.scatter(x,y, color=\"blue\", s=100, label=\"real TOEFL Score\") # real y (Chance of Admit) values\nplt.scatter(x001,y_head001dtr, color=\"red\", alpha = 0.7, label=\"predicted TOEFL Score\") # to see the predicted values one by one\nplt.plot(x001,y_head001dtr, color=\"black\", alpha = 0.7)  # to see the average values for each leaf.\nplt.legend()\nplt.show()","c7ab7f56":"# Same shapes, y and y_head_dtr\nprint(y.shape, y_head_dtr.shape, y_head001dtr.shape)","c68cc462":"from sklearn.metrics import r2_score\n\nprint(\"r_score: \", r2_score(y,y_head_dtr))","044b0fcd":"from sklearn.model_selection import cross_val_score\n#cross_val_score(tree_reg, boston.data, boston.target, cv=10)\nprint(tree_reg.score(x001, y_head001dtr))\nprint(tree_reg.score(x, y))","ff6c2522":"from sklearn.metrics import r2_score\nprint(\"r_score: \", r2_score(y,y_head_dtr))\n\nfrom sklearn.model_selection import cross_val_score\nprint(tree_reg.score(x001, y_head001dtr))\nprint(tree_reg.score(x, y))","ab385fa2":"plt.scatter(x, y, color=\"blue\", alpha = 0.7)\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","84b4cb1e":"x = df[\"TOEFL Score\"].values.reshape(-1,1)\ny = df[\"Chance of Admit \"].values.reshape(-1,1)\n\nprint(min(x), max(x))\nprint(min(y), max(y))","4ac27c0f":"# Random Forest Regression Library\n\nfrom sklearn.ensemble import RandomForestRegressor\n \nrandom_forest_reg = RandomForestRegressor(n_estimators = 100, random_state = 42)\n# n_estimators = 100 --> Tree number\n# random_state = 42  --> Sample number\nrandom_forest_reg.fit(x,y)\n\nprint(random_forest_reg.predict([[98]]))","00c42eff":"# New prediction examples with (Start, End, Increase)\nx001 = np.arange(min(x), max(x), 0.01).reshape(-1,1)\ny_head001rf = random_forest_reg.predict(x001)\n\nprint(min(x001), max(x001))\nprint(min(y_head001rf), max(y_head001rf))","cc478897":"len(np.unique(y_head001rf))\n\n\n# 46 unique values for all values","dbf93fcf":"plt.figure(figsize=(20,10))\nplt.scatter(x,y, color=\"blue\", label=\"real TOEFL Score\")\nplt.scatter(x001,y_head001rf, color=\"red\", label=\"predicted TOEFL Score\")\nplt.plot(x001,y_head001rf, color=\"black\")\nplt.legend()\nplt.xlabel(\"TOEFL Score\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","87f0e6c6":"from sklearn.model_selection import cross_val_score\n\nprint(tree_reg.score(x001, y_head001rf))\nprint(tree_reg.score(x, y))","6bca050c":"from sklearn.metrics import r2_score\n\ny_headrf = random_forest_reg.predict(x)\nprint(\"r_score: \", r2_score(y,y_headrf))\n\nfrom sklearn.model_selection import cross_val_score\nprint(tree_reg.score(x001, y_head001rf))\nprint(tree_reg.score(x, y))","5ada8814":"Compare the predicted values of different equations.","cf10dfe9":"<a id=\"31\"><\/a> <br>\n**3.1. Prediction**","65fecca4":"Success ratio is **%76** for degree=2 and it is **%78.84** for degree=7 for PR.","bef95009":"<a id=\"3\"><\/a> <br>\n3. **Polynomial Regression (PR)**","b00b01ed":"Success ratio is **%100** for DTR.","42c952d8":"<a id=\"51\"><\/a> <br>\n**5.1. Prediction**","0deb4e85":"y = b0 + b1x+ b2x^2 + b3x^3 + ... + bnx^n\n","103f30ba":"**sklearn.tree.DecisionTreeRegressor:**\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html\n\n**score(self, X, y, sample_weight=None)**\n\nReturns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u\/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n\nParameters:\t\nX : array-like, shape = (n_samples, n_features)\nTest samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.\n\ny : array-like, shape = (n_samples) or (n_samples, n_outputs)\nTrue values for X.\n\nsample_weight : array-like, shape = [n_samples], optional\nSample weights.\n\nReturns:\t\nscore : float\nR^2 of self.predict(X) wrt. y.\n\nNotes\n\nThe R2 score used when calling score on a regressor will use multioutput='uniform_average' from version 0.23 to keep consistent with metrics.r2_score. This will influence the score method of all the multioutput regressors (except for multioutput.MultiOutputRegressor). To specify the default value manually and avoid the warning, please either call metrics.r2_score directly or make a custom scorer with metrics.make_scorer (the built-in scorer 'r2' uses multioutput='uniform_average').","1b0887ee":"<a id=\"21\"><\/a> <br>\n**2.1. Prediction**","72bf3828":"Random forest regression (RFR) uses more trees (100), it gave more accurate predicted values than Decision Tree Regression (DTR).","84cfa4b9":"<a id=\"12\"><\/a> <br>\n**1.2. R Square (LR)**\n\nWe can evaluate the linear regression model performance with R Square.\n* y: Chance of Admit values\n* y_head: predicted Chance of Admit value\n\nFirst, we must be sure that y and y_head values are using the same number of samples. If not, we will get an error like this:\n\nValueError: Found input variables with inconsistent numbers of samples: [500, 312]","75bac4e2":"CGPA, GRE Score and TOEFL Scores are 3 most correlated features for the \"Chance of Admit\".\n\nLet's drop all the duplicated values from the data frame.","d03062eb":"Success ratio is around **% 70** for the LR prediction.","43941ddd":"Black dots are predicted values and they overlap the real values. Because our prediction values (x) are same with the real values. So, we need some other range to predict according to real x values.","689f4f2e":"Success ratio is **% 75** for the  MLR prediction.","5cedc9d4":"<a id=\"4\"><\/a> <br>\n4. **Decision Tree Regression (DTR)**","6fe29130":"<a id=\"41\"><\/a> <br>\n**4.1. Prediction**","d25fee19":"This document includes below examples;\n1. [**Linear Regression (LR)**](#1)\n   1. [Prediction](#11)\n   1. [R Square (LR)](#12)\n1. [**Multiple Linear Regression**](#2)\n   1. [Prediction](#21)\n   1. [R Square (LR)](#22)\n1. [**Polynomial Regression (PR)**](#3)\n   1. [Prediction](#31)\n   1. [R Square (LR)](#32)\n1. [**Decision Tree Regression (DTR)**](#4)\n   1. [Prediction](#41)\n   1. [R Square (LR)](#42)\n1. [**Random Forest Regression (RFR)**](#5)\n   1. [Prediction](#51)\n   1. [R Square (LR)](#52)","3ff9234a":"2nd degre equationd didn't give a proper model. But, we can modify the degree of the equation to converge to real values.","22e3ebda":"Black plot shows the predicted values, red points show them one by one. As you can see, TOEFL Score values (x) divided between leaves (ranges) and each leaf have an average value as the predicted value. So, we see a constant line for each leaf.","d81af476":"<a id=\"22\"><\/a> <br>\n**2.2. R Square**","167a192c":"<a id=\"2\"><\/a> <br>\n2. **Multiple Linear Regression**\n\ny = b0 + b1x1 + b2x2 + ... bnxn","1fd31c48":"<a id=\"11\"><\/a> <br>\n**1.1. Prediction**\n\nWe will predict the values according to linear_reg model.","e111d151":"<a id=\"5\"><\/a> <br>\n5. **Random Forest Regression (RFR)**","56b054d6":"Random forest regression combined by  multiple regression. \n\nIt chooses n examples, divides the data to sub datas and uses multiple trees.\n\n                     data\n                       |\n                       |\n                    n sample\n                       |\n                       |\n                    sub_data\n         tree1   tree2  tree3 .... tree n\n         ________________________________\n        |           average               |\n         ________________________________\n                     result\n                     \n            \n                 \n                 \n\nRandomForestRegressor(**n_estimators** = 100, **random_state** = 42)\n\nThis means we will use 100 tree (DTR) and 42 sample. The algorithm chooses the n samples randomly. We gave a constant number for the random state, therefore the algorithm will select the same 42 examples on the next time.","4e95459f":"According to the new data frame (with non-duplicated values);\n\nTOEFL Scores, CGPA and GRE Score are 3 most correlated features for the \"Chance of Admit\".","902d4a85":"<a id=\"52\"><\/a> <br>\n**5.2. R Square**","9b09e21a":"<a id=\"42\"><\/a> <br>\n**4.2. R Square**","6fc4f827":"y1_head keeps the prediction values of x1 which has CGPA, GRE Score and\tTOEFL Score values.  ","1395a47a":"Black values shows the predicted values, other colors are the real values. As you can see, the predicted values are converging to the real values.","84b8a096":"<a id=\"1\"><\/a> <br>\n1. **Linear Regression (LR)**\n\ny = b0 + b1*x","bb2aeeaf":"We can see that red predicted values (degree=7) are more convergent on the bottom-left of the graph and they are similar with the green predicted values (degre=30) in the middle and upper-right of the graph.\n\nThe most proper degree may differ between different datas. We don't always have to increase it to get more accurate prediction.","918a95ca":"Now, we can use above x&y axises on the fit operation of the linear regression model.","a0174b57":"\"Decision Tree Regression\" method divides the areas between values based on the conditions, assing the average value of the values for each area which is called \"leaf\".\n\nFor example let's take below tree as an example. We can divide the conditions as [0,30), (30,40), (40,50) and (50,100].\n\nIf x = 51 or 100, it means y is 100. It shows that y values for all \"x>50\" means 100 for the model. \nIf x = 31 or 39, it means y is 35. (Y values are chosen arbitrary, it doesnt have to be linear propotional.)\n\n\nx=[0, 100]\n\n                          x1 > 50\n                yes                no\n               y=100              x>30\n                              yes       no\n                              x<40      y=25\n                            yes   no\n                            y=35  y=45","141fb52a":"<a id=\"32\"><\/a> <br>\n**3.2. R Square**","661cce05":"For the fit operation, we need to use numpy arrays on the x and y axixes, so we will use \"df.ColumnName.values\". But, its type will be \"(500,)\" so we will reshape it. The reason we write -1 is because we may don't know the size, we only need to set the second value to 1.\n\nThe most correlated feature with \"Chance of Admit\" is \"CGPA\"."}}