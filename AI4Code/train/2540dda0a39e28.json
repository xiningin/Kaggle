{"cell_type":{"0ef45166":"code","6e137ad1":"code","0286db5b":"code","967a8f50":"code","baf9a46a":"code","e3b6ce4e":"code","7c718064":"code","0bf506fd":"code","5beeb1fe":"code","aecca04c":"code","8d4ef7b2":"code","d5891793":"code","79a82464":"code","efaa2f20":"code","136286db":"code","fa69e8e5":"code","ea1fbdc2":"code","d04f5cc5":"code","7d4262c3":"code","4b5406ae":"code","70946ece":"code","fe20064f":"code","d9cf7c59":"code","2c6485ac":"code","651eea41":"code","0e71d569":"code","f88ded32":"code","25223a9a":"markdown","c1ba6de7":"markdown","f992aaa2":"markdown","ba457b57":"markdown","32115356":"markdown","8da435ec":"markdown","a769599f":"markdown","1e91d6d5":"markdown","77310fbf":"markdown","355b0fb6":"markdown","0bb2c23a":"markdown","938d49ea":"markdown","9df1d63d":"markdown","deb2d9a9":"markdown","9f67f109":"markdown","e7efd795":"markdown","71e76543":"markdown","f05bde40":"markdown","6727ebc9":"markdown","50378223":"markdown"},"source":{"0ef45166":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport nltk\nfrom textblob import TextBlob\nfrom nltk.tokenize import word_tokenize\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport re\nimport string\nimport glob\nimport emoji\nimport warnings\nwarnings.filterwarnings('ignore')","6e137ad1":"\n# get karachi tweets\nkarachi = pd.read_csv('..\/input\/tweets\/Karachi_travel_tweets.csv')\nkarachi1 = pd.read_csv('..\/input\/tweets\/Karachi_visit_tweets.csv')\n\n# concatenate karachi tweets\nkarachi_df = pd.concat([karachi, karachi1], ignore_index=True)\nkarachi_df['city'] = 'karachi'\n\n# get lahore tweets\nlahore = pd.read_csv('..\/input\/tweets\/visit_lahore_tweets converted.csv')\nlahore1 = pd.read_csv('..\/input\/tweets\/travel_lahore_tweets converted.csv')\n\n# concatenate lahore tweets\nlahore_df = pd.concat([lahore, lahore1], ignore_index=True)\nlahore_df['city'] = 'lahore'\n\n# get gilgit tweets\ngilgit = pd.read_csv('..\/input\/tweets\/Gilgit_tweets.csv')\ngilgit['city'] = 'gilgit'\n\n# get neelum valley tweets\nneelum_valley = pd.read_csv('..\/input\/tweets\/NeelumValley_tweets.csv')\nneelum_valley['city'] = 'neelum_valley'\n\n# get northern tweets\nnorthern = pd.read_csv('..\/input\/tweets\/Pakistan_north_tweets.csv')\nnorthern1 = pd.read_csv('..\/input\/tweets\/Pakistan_northern_tweets.csv')\n\n# concatenate northern tweets\nnorthern_df = pd.concat([northern, northern1], ignore_index=True)\nnorthern_df['city'] = 'northern area'\n\n# # Concatenate all data into one DataFrame\ndf = pd.concat([karachi_df, lahore_df, gilgit, neelum_valley, northern_df], ignore_index=True)\ndf.head()","0286db5b":"# df = pd.read_csv('..\/input\/Gilgit_tweets.csv')\n# df.head()","967a8f50":"def clean_text(text,replace=''):\n    lemmatizer = WordNetLemmatizer() \n    stop_words = set(stopwords.words('english'))\n    REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\n    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n    \n    text = re.sub(r'\\d+', '', str(text))\n    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    text = \"\".join(i for i in text if ord(i) < 128) # remove non-ascii characters\n    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation characters\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))', replace, text) # removes URL from string\n    text = emoji.demojize(text) # remove emojies\n    # remove stop words andapplying Lemmatization\n    text = ' '.join(lemmatizer.lemmatize(word, pos =\"a\") for word in text.split() if word not in stop_words) \n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    return text","baf9a46a":"df = df.drop(['hashtags', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\ndf['text'] = df['text'].apply(clean_text)\ndf['text'].replace('nan', np.nan, inplace=True)\ndf['text'].replace('', np.nan, inplace=True)\ndf = df.dropna()\ndf.reset_index(inplace = True, drop = True)\ndf.head()","e3b6ce4e":"tweet = ' '.join(df[\"text\"])\n#function to split text into word\ntokens = word_tokenize(tweet)\nvocabulary = set(tokens)\nfrequency_dist = nltk.FreqDist(tokens)\n\nwordcloud = WordCloud(width=1000, height=500).generate_from_frequencies(frequency_dist)\nplt.figure( figsize=(20,10), facecolor='k' )\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","7c718064":"polarity_score = []\n\nfor text in df['text']:    \n    blob = TextBlob(text)\n    for sentence in blob.sentences:\n        if sentence.sentiment.polarity > 0.0:\n            polarity_score.append('positive')\n        elif sentence.sentiment.polarity < 0.0:\n            polarity_score.append('negative')\n        else:\n            polarity_score.append('neutral')\n\ndf['target'] = pd.DataFrame(polarity_score)\ndf.head()","0bf506fd":"df['target'].value_counts().plot.bar(color=['purple', 'blue', 'red'])","5beeb1fe":"tweet = ' '.join(df['text'][df['target'] == 'positive'] )\n#function to split text into word\ntokens = word_tokenize(tweet)\nvocabulary = set(tokens)\nfrequency_dist = nltk.FreqDist(tokens)\n\nwordcloud = WordCloud(width=1000, height=500).generate_from_frequencies(frequency_dist)\nplt.figure( figsize=(20,10), facecolor='k' )\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","aecca04c":"tweet = ' '.join(df['text'][df['target'] == 'negative'] )\n#function to split text into word\ntokens = word_tokenize(tweet)\nvocabulary = set(tokens)\nfrequency_dist = nltk.FreqDist(tokens)\n\nwordcloud = WordCloud(width=1000, height=500).generate_from_frequencies(frequency_dist)\nplt.figure( figsize=(20,10), facecolor='k' )\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","8d4ef7b2":"label = LabelEncoder()\ndf_ = df.copy()\ndf_['target'] = label.fit_transform(df_['target'])\n# dummies = pd.get_dummies(df.target)\n# df = pd.concat([df, dummies], axis=1)\ndf_.head()","d5891793":"#Seperate data into feature and results\nX, y = df_['text'], df_['target']\n\n#Split data in train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","79a82464":"my_categories=['0','1','2']\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 1))\n\n# TF-IDF BASED FEATURE REPRESENTATION\ntfidf.fit_transform(X_train)\n        \ntrain_feature_set=tfidf.transform(X_train)\ntest_feature_set=tfidf.transform(X_test)","efaa2f20":"# instantiate the model (using the default parameters)\nlogreg = LogisticRegression(random_state=0)\n\nlogreg.fit(train_feature_set,y_train)\n\n#\ny_pred=logreg.predict(test_feature_set)\nlrm1 = accuracy_score(y_pred, y_test)\n\nprint('accuracy %s' % lrm1)\nprint(classification_report(y_test, y_pred,target_names=my_categories))\nprint('Confusion Matrix \\n',confusion_matrix(y_test,y_pred))","136286db":"nb = MultinomialNB()\n\nnb.fit(train_feature_set, y_train)\n\ny_pred = nb.predict(test_feature_set)\nnb1 = accuracy_score(y_pred, y_test)\n\nprint('accuracy %s' % nb1)\nprint(classification_report(y_test, y_pred,target_names=my_categories))\nprint('Confusion Matrix \\n',confusion_matrix(y_test,y_pred))","fa69e8e5":"rclf = RandomForestClassifier()\nrclf.fit(train_feature_set,y_train)\ny_pred = rclf.predict(test_feature_set)\nprint('accuracy %s' % accuracy_score(y_test,y_pred))\nprint(classification_report(y_test, y_pred,target_names=my_categories))\nprint('Confusion Matrix \\n',confusion_matrix(y_test,y_pred))","ea1fbdc2":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n\n# TF-IDF BASED FEATURE REPRESENTATION\ntfidf.fit_transform(X_train)\n        \ntrain_feature_set=tfidf.transform(X_train)\ntest_feature_set=tfidf.transform(X_test)","d04f5cc5":"# instantiate the model (using the default parameters)\nlogreg = LogisticRegression(random_state=0)\n\nlogreg.fit(train_feature_set,y_train)\n\n#\ny_pred=logreg.predict(test_feature_set)\nlrm1 = accuracy_score(y_pred, y_test)\n\nprint('accuracy %s' % lrm1)\nprint(classification_report(y_test, y_pred,target_names=my_categories))\nprint('Confusion Matrix \\n',confusion_matrix(y_test,y_pred))","7d4262c3":"nb = MultinomialNB()\n\nnb.fit(train_feature_set, y_train)\n\ny_pred = nb.predict(test_feature_set)\nnb1 = accuracy_score(y_pred, y_test)\n\nprint('accuracy %s' % nb1)\nprint(classification_report(y_test, y_pred,target_names=my_categories))\nprint('Confusion Matrix \\n',confusion_matrix(y_test,y_pred))","4b5406ae":"rclf = RandomForestClassifier()\nrclf.fit(train_feature_set,y_train)\ny_pred = rclf.predict(test_feature_set)\nprint('accuracy %s' % accuracy_score(y_test,y_pred))\nprint(classification_report(y_test, y_pred,target_names=my_categories))\nprint('Confusion Matrix \\n',confusion_matrix(y_test,y_pred))","70946ece":"print('Proportion of the classes in the data:')\nprint(df.target.value_counts() \/ len(df))","fe20064f":"def city_wise(city):\n    print(city, 'sentiemnt percentage pie chart')\n    val = df[df['city'] == city]['target']\n    (val.value_counts() \/ len(val)).plot.pie(autopct='%1.1f%%')","d9cf7c59":"city_wise('gilgit') # pass city name to function","2c6485ac":"cities = ['lahore', 'karachi', 'gilgit', 'northern area', 'neelum_valley']\n\npercentage = []\nfor city in cities:\n    val = df[df['city'] == city]['target']\n    percentage.append({city : dict((val.value_counts() \/ len(val)))})\n\npercentage","651eea41":"positive_tweet_percentage = [] \nfor p in percentage:\n    for city in p:\n        positive_tweet_percentage.append((city,p[city]['positive']))","0e71d569":"cities =  pd.DataFrame(positive_tweet_percentage, columns =['city', 'percentage'])\ncities = cities.sort_values('percentage', ascending=False)\ncities = cities.set_index('city')\ncities","f88ded32":"cities.plot.bar(color = 'rgbkymc')\nplt.title('Best to worst city for visiting')","25223a9a":"## Random Forest Classifier","c1ba6de7":"## Positive Tweets Word Cloud","f992aaa2":"## Bi-Gram TF-IDF Transforamtion ","ba457b57":"## Split the Data in train and test","32115356":"## Logistic Regression","8da435ec":"## Logistic Regression ","a769599f":"## Class Labels Encoding","1e91d6d5":"## City wise percentage","77310fbf":"## Uni-Gram TF-IDF Transormation","355b0fb6":"## Word Cloud Data","0bb2c23a":"## Classes Frequency Graph","938d49ea":"## Negative Tweets Word Cloud","9df1d63d":"## Finding Best City for Visiting","deb2d9a9":"## Importing Data","9f67f109":"## Naive Bayes","e7efd795":"## Find Target Class for Tweets","71e76543":"## Random Forest Classifier","f05bde40":"## Import Libraries","6727ebc9":"## Data Preprocessing","50378223":"## Naive Bayes "}}