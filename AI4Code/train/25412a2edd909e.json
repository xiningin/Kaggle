{"cell_type":{"d689bada":"code","522acc0a":"code","fa4cb118":"code","9786d966":"code","1971fb38":"code","c8e4a110":"code","be18aa4c":"code","494c4434":"code","3b31ff99":"code","28095162":"code","32d7a7e6":"code","d7364a5a":"code","43fffbe0":"code","ec7f373a":"code","de83655a":"code","5a2f89b7":"code","1d91b28c":"code","90d68069":"code","10dee36c":"code","a6fa5b87":"code","8ab05e62":"code","0cfa707e":"code","668361d7":"code","4c560eff":"code","d02d9a61":"code","5197f6dc":"code","6c0324ae":"code","11dececd":"code","00af9cee":"code","a1d35ede":"code","acba8de6":"code","96810e45":"code","99aaebad":"code","63ec7199":"markdown","2e3c89f6":"markdown","9a92ffda":"markdown","1b0fd5dd":"markdown","eaa09fc0":"markdown","951079ec":"markdown","79e97db7":"markdown","8b74cfc0":"markdown","cdb28a35":"markdown","cff308e3":"markdown","76ac9cab":"markdown","447e33d2":"markdown","04173eb3":"markdown"},"source":{"d689bada":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","522acc0a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport gc\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")","fa4cb118":"def evaluate_model(model, x, y):\n    y_pred_prob = model.predict_proba(x)[:, 1]\n    auc_roc = roc_auc_score(y, y_pred_prob)\n    return {'auc_roc_curve' : auc_roc}","9786d966":"seed = 47","1971fb38":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv', sep=',')","c8e4a110":"x_train = train_df.drop(['id', 'target'], axis=1).values\ny_train = train_df['target'].values \nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed)","be18aa4c":"params = {'colsample_bytree': 0.1,\n          'eta': 0.12,\n          'gamma': 5, \n          'max_depth': 2,\n          'min_child_weight': 9,\n          'n_estimators': 11000, \n          'subsample': 0.9}\n\nmodel = XGBClassifier(**params, \n                      random_state=seed, \n                      tree_method='gpu_hist', \n                      predictor='gpu_predictor',\n                      use_label_encoder=False, \n                      verbosity=0)\n\nmodel.fit(x_train, y_train)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","494c4434":"model = LogisticRegression(random_state=seed, solver='liblinear')\nmodel.fit(x_train, y_train)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","3b31ff99":"geomean = lambda x, axis : np.exp(np.mean(np.log(x), axis=axis))\nharmonic_mean = lambda x, axis : len(x) \/ np.sum(1.0\/x, axis=axis) \n\nfuncs = {'mean' : np.mean, \n         'std' : np.std, \n         'var' : np.var, \n         'geo_mean' : geomean, \n         'harmonic_mean' : harmonic_mean, \n         'median' : np.median}","28095162":"results, names = list(), list()\n\nparams = {'colsample_bytree': 0.1,\n          'eta': 0.12,\n          'gamma': 5, \n          'max_depth': 2,\n          'min_child_weight': 9,\n          'n_estimators': 11000, \n          'subsample': 0.9}\n\nfor key in funcs.keys():\n    x_train = train_df.drop(['id', 'target'], axis=1)\n    x_train[key] = funcs[key](x_train, axis=1)\n    y_train = train_df['target']\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed)       \n    model = XGBClassifier(**params, \n                      random_state=seed, \n                      tree_method='gpu_hist', \n                      predictor='gpu_predictor',\n                      use_label_encoder=False, \n                      verbosity=0)\n\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    names.append(key)\n    results.append(result['auc_roc_curve'])\n    \nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))","32d7a7e6":"results, names = list(), list()\n\nfor key in funcs.keys():\n    x_train = train_df.drop(['id', 'target'], axis=1)\n    x_train[key] = funcs[key](x_train, axis=1)\n    y_train = train_df['target']\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed)       \n    model = LogisticRegression(random_state=seed, solver='liblinear')\n    \n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    names.append(key)\n    results.append(result['auc_roc_curve'])\n\nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))","d7364a5a":"x_train = train_df.drop(['id', 'target'], axis=1).values\ny_train = train_df['target'].values","43fffbe0":"params = {'n_estimators' : [10000, 11000],\n          'max_depth' : [2, 3],\n          'subsample' : [0.8, 0.9, 1.0],\n          'eta' : [0.11, 0.12],\n          'colsample_bytree' : [0.3, 0.4],\n          'min_child_weight': [5],\n          'gamma': [5]\n         }\n\nmodel = XGBClassifier(random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\nsearch = GridSearchCV(model, param_grid=params, scoring='roc_auc', refit='roc_auc', n_jobs=-1, cv=cv)","ec7f373a":"result = search.fit(x_train, y_train)\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","de83655a":"x_train = train_df.drop(['id', 'target'], axis=1).values\ny_train = train_df['target'].values \nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed)","5a2f89b7":"params = {'colsample_bytree': 0.1,\n          'eta': 0.12,\n          'gamma': 5, \n          'max_depth': 2,\n          'min_child_weight': 9,\n          'n_estimators': 11000, \n          'subsample': 0.9}\n\nxgboost_model = XGBClassifier(**params, \n                      random_state=seed, \n                      tree_method='gpu_hist', \n                      predictor='gpu_predictor',\n                      use_label_encoder=False, \n                      verbosity=0)\n\nxgboost_model.fit(x_train, y_train)\nresults = evaluate_model(xgboost_model, x_test, y_test)\nprint(results)","1d91b28c":"logistic_reg_model = LogisticRegression(random_state=seed, solver='liblinear')\nlogistic_reg_model.fit(x_train, y_train)\nresults = evaluate_model(logistic_reg_model, x_test, y_test)\nprint(results)","90d68069":"y_pred_prob_xgboost = xgboost_model.predict_proba(x_test)[:, 1]\ny_pred_prob_logistic_reg = logistic_reg_model.predict_proba(x_test)[:, 1]\ny_pred_prob = np.mean([y_pred_prob_xgboost, y_pred_prob_logistic_reg], axis=0)\nauc_roc = roc_auc_score(y_test, y_pred_prob)\nprint({'auc_roc_curve' : auc_roc})","10dee36c":"x_train = train_df.drop(['id', 'target'], axis=1).values\ny_train = train_df['target'].values \nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed)","a6fa5b87":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(256, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(128, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(64, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[tf.keras.metrics.AUC(from_logits=True)])\nmodel.fit(x_train, y_train, batch_size=128, epochs=25)","8ab05e62":"results = model.evaluate(x_test, y_test)[1]\nprint(results)","0cfa707e":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(256, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(128, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(64, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), \n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.AUC(from_logits=True)])\nmodel.fit(x_train, y_train, batch_size=128, epochs=35)","668361d7":"results = model.evaluate(x_test, y_test)[1]\nprint(results)","4c560eff":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(256, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(128, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(64, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.AUC(from_logits=True)])\nmodel.fit(x_train, y_train, batch_size=1024, epochs=40)","d02d9a61":"results = model.evaluate(x_test, y_test)[1]\nprint(results)","5197f6dc":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(256, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(128, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(64, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.AUC(from_logits=True)])\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=35)","6c0324ae":"results = model.evaluate(x_test, y_test)[1]\nprint(results)","11dececd":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(256, activation='selu'))\nmodel.add(tf.keras.layers.Dense(128, activation='selu'))\nmodel.add(tf.keras.layers.Dense(64, activation='selu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.AUC(from_logits=True)])\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=35)\nresults = model.evaluate(x_test, y_test)[1]\nprint(results)","00af9cee":"x_train = train_df.drop(['id', 'target'], axis=1)\ny_train = train_df['target']","a1d35ede":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(256, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(128, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(64, activation='swish'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.AUC(from_logits=True)])\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=25)","acba8de6":"test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv', sep=',')\nx_test = test_df.drop(['id'], axis=1)\ntarget = model.predict(x_test).squeeze()\nids = test_df['id'].values\nsubmission = pd.DataFrame({'id' : ids, 'target' : target})","96810e45":"submission.head()","99aaebad":"submission.to_csv('submission.csv', index=False)","63ec7199":"<h4>Fitting XGBoost<\/h4>","2e3c89f6":"<h3>Grid Search Hyperparameters - XGBoost<h3>","9a92ffda":"<h4>Fitting Logistic Regression<\/h4>","1b0fd5dd":"# Neural Nets","eaa09fc0":"# Grid Search Hyperparameters","951079ec":"<h4>Ensembling both models<\/h4>","79e97db7":"<h3>Feature Engineering - XGBoost<\/h3>","8b74cfc0":"# Ensemble Models","cdb28a35":"# XGBoost Baseline","cff308e3":"# Submission","76ac9cab":"# Logistic Regression Baseline","447e33d2":"# Feature Engineering\n\nHere wee will experiment creating synthetic features using central tendency statistics.","04173eb3":"<h3>Feature Engineering - Logistic Regression<\/h3>"}}