{"cell_type":{"8ee138cf":"code","914567cd":"code","8f445502":"code","1575c270":"code","21ce87ae":"code","52f04ac1":"code","0093308d":"code","e3a8ad34":"code","38a54f0f":"code","d28f5719":"code","a716228e":"code","4e96f834":"code","749358f2":"code","1aa678da":"code","bb3e430f":"markdown","c72c7108":"markdown"},"source":{"8ee138cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","914567cd":"train_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-4\/train.csv\", low_memory=False)\ntest_data =  pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-4\/eval.csv\", low_memory=False)\nsample_submission =  pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-4\/sample_submission.csv\", low_memory=False)","8f445502":"train_data.describe()","1575c270":"train_data.head()","21ce87ae":"y = train_data['label']\ntrain_data = train_data.drop(['label','id'],axis=1)","52f04ac1":"tdc = train_data\n\ntdc1 = tdc.values.reshape(-1,28,28,1)\n","0093308d":"import matplotlib.pyplot as plt\nfrom skimage.color import rgb2gray, gray2rgb\nfrom skimage.filters import try_all_threshold, threshold_otsu, difference_of_gaussians, apply_hysteresis_threshold\nfrom skimage import measure, data\n\nimport copy\ntrain_data_copy = copy.deepcopy(train_data)\n\n\ndef showFilter(train_data_copy, length, showBinary=False, showThresh=False,Blur = False, show = False):\n    for k in range(length):\n        testImage = []\n        testImageAppended = []\n        row = 1\n        column = 1\n        for i in range(28):\n            testImage.append([])\n        for i in train_data_copy.iloc[k]:\n            testImage[row-1].append(i)\n            column +=1\n            if(column % 28 == 1):\n                row+=1\n        testImage = np.array(testImage)\n        if(Blur):\n            testImage = difference_of_gaussians(testImage,.1,10)\n        if(showBinary):\n            thresh = threshold_otsu(testImage)\n            binary = testImage > thresh\n            testImageAppended.append(binary)\n            if(show):\n                plt.imshow(binary)  \n        if(showThresh):\n            fig, ax = try_all_threshold(testImage, verbose=False)\n        if(show):\n            plt.show()\n    if(show == False):\n        return testImageAppended\ntestImageFiltered = showFilter(train_data_copy,25,showBinary=True,showThresh=False,Blur=True, show= True)","e3a8ad34":"testImageFiltered = np.array(testImageFiltered)\nprint(testImageFiltered)","38a54f0f":"from sklearn.model_selection import train_test_split\nrs = 123\n\ndef dataSplit(y,train_data):\n    X = train_data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .95, random_state=rs)\n    return (X_train,X_test,y_train,y_test)","d28f5719":"from keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Dropout, LayerNormalization, BatchNormalization, Flatten, MaxPooling2D\nimport math\nmodel = Sequential()\n\ndef normalizeModel(model, batch=False):\n    if(batch):\n        return model.add(BatchNormalization())\n    else:\n        return model.add(LayerNormalization())\ndef buildEasyModel(activation):   \n    model.add(Dense(320,activation=activation, input_shape=(784,)))\n    normalizeModel(model,True)\n    model.add(Dropout(.2))\n    model.add(Dense(160, activation=activation))\n    model.add(Dense(160, activation=activation))\n    normalizeModel(model,True)\n    model.add(Dropout(.2))\n    model.add(Dense(80, activation=activation))\n    model.add(Dense(80, activation=activation))\n    normalizeModel(model,True)\n    model.add(Dropout(.2))\n    model.add(Dense(40, activation=activation))\n    model.add(Dense(40, activation=activation))\n    normalizeModel(model,True)\n    model.add(Dropout(.2))\n    model.add(Dense(20, activation=activation))\n    model.add(Dense(20, activation=activation))\n    normalizeModel(model,True)\n    model.add(Dropout(.2))\n    \n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\ndef buildEasyModelCNN(activation,optimizer,normalize=False,batch=False):   \n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    model.add(Conv2D(280, kernel_size = 2, activation=activation, padding='same', input_shape=(28,28,1)))\n    model.add(MaxPooling2D(2))\n    model.add(Dense(140, activation=activation))\n    model.add(Dropout(.4))\n    if(normalize):\n        normalizeModel(model,batch)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    model.add(Conv2D(140,  kernel_size = 2,activation=activation, padding='same', input_shape=(28,28,1)))\n    model.add(MaxPooling2D(2))\n    model.add(Dense(70,  activation=activation))\n    model.add(Dropout(.3))\n    if(normalize):\n        normalizeModel(model,batch)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    model.add(Conv2D(70,  kernel_size = 2,activation=activation, padding='same',  input_shape=(28,28,1)))\n    model.add(MaxPooling2D(2))\n    model.add(Dense(35,  activation=activation))\n    model.add(Dropout(.2))\n    if(normalize):\n        normalizeModel(model,batch)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    model.add(Conv2D(36, kernel_size = 2,activation=activation, padding='same',  input_shape=(28,28,1)))\n    model.add(MaxPooling2D(2))\n    model.add(Dense(18, activation=activation))\n    model.add(Dropout(.1))\n    if(normalize):\n        normalizeModel(model,batch)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    model.add(Dropout(.2))\n    model.add(Flatten())\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# buildEasyModel('elu')\nbuildEasyModelCNN('relu','adam', normalize=True)\n\nmodel.summary()","a716228e":"\ny_encoded = pd.get_dummies(y)\nbuiltModels = []\nfor i in range(5):\n    (X_train,X_test,y_train,y_test) = dataSplit(y_encoded,tdc1)\n    firstModel = model.fit(X_train,y_train, validation_split=.1, epochs=10)\n    builtModels.append(firstModel)\n    plt.plot(firstModel.history['loss'])\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show()\n\n    plt.plot(firstModel.history['accuracy'])\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.show()\n\n    print(model.evaluate(X_test,y_test))\nprint(model.history)","4e96f834":"test_data = test_data.drop('id',axis=1)\ntest_data = test_data.values.reshape(-1,28,28,1)","749358f2":"submissionY = model.predict(test_data)","1aa678da":"def processOutput(sample_submission,submissionY, submissionName):\n    newOutput = []\n    for i in submissionY:\n        newOutput.append(i.argmax())\n\n    output = pd.DataFrame({'id':sample_submission.id, 'label':newOutput})\n    output.to_csv(submissionName,index=False)\n    return print(output)\nsubmissionName = 'firstModelOutput4.csv'\nprocessOutput(sample_submission,submissionY,submissionName)","bb3e430f":"# !!! Load Data !!!","c72c7108":"# !!! Describe Data !!!"}}