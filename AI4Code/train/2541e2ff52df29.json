{"cell_type":{"0057d692":"code","46fdc5e4":"code","76aecfd1":"code","9694d059":"code","304416eb":"code","6adc0cfd":"code","eb67ad7c":"code","947fe48e":"code","8a18a3b2":"code","c442410a":"code","a4eb1208":"code","4fa746fb":"code","4593a1ae":"code","5af218f8":"code","7e340c7f":"code","d57b0112":"code","a425bad3":"code","da8b3c83":"code","7328d990":"code","5ce7ff43":"code","b6a0d1af":"markdown","195cf307":"markdown","55c4e3c8":"markdown","7bf9dc90":"markdown","27f1c824":"markdown","29d75ce7":"markdown","2453f600":"markdown","cd190266":"markdown","5981376f":"markdown","466fa360":"markdown","25c8063f":"markdown","bf32518e":"markdown","55bcd1e9":"markdown","cc806b02":"markdown","d0913fa8":"markdown","77d0937a":"markdown","9948f48c":"markdown","969519e8":"markdown","b81ded7a":"markdown","394b2c45":"markdown"},"source":{"0057d692":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","46fdc5e4":"from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import Nadam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler","76aecfd1":"train_file = \"..\/input\/train.csv\"\ntest_file = \"..\/input\/test.csv\"\noutput_file = \"submission.csv\"","9694d059":"raw_data = np.loadtxt(train_file, skiprows=1, dtype='int', delimiter=',')\nx_train, x_val, y_train, y_val = train_test_split(\n    raw_data[:,1:], raw_data[:,0], test_size=0.1)","304416eb":"fig, ax = plt.subplots(2, 1, figsize=(12,6))\nax[0].plot(x_train[0])\nax[0].set_title('784x1 data')\nax[1].imshow(x_train[0].reshape(28,28), cmap='gray')\nax[1].set_title('28x28 data')","6adc0cfd":"x_train = x_train.reshape(-1, 28, 28, 1)\nx_val = x_val.reshape(-1, 28, 28, 1)","eb67ad7c":"x_train = x_train.astype(\"float32\")\/255.\nx_val = x_val.astype(\"float32\")\/255.","947fe48e":"y_train = to_categorical(y_train)\ny_val = to_categorical(y_val)\n#example:\nprint(y_train[0])","8a18a3b2":"model = Sequential()\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n                 input_shape = (28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\n#model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\n#model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))","c442410a":"datagen = ImageDataGenerator(zoom_range = 0.1,\n                            height_shift_range = 0.1,\n                            width_shift_range = 0.1,\n                            rotation_range = 10)","a4eb1208":"model.compile(loss='categorical_crossentropy', optimizer = Nadam(lr=1e-4), metrics=[\"accuracy\"])","4fa746fb":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","4593a1ae":"hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=16),\n                           steps_per_epoch=500,\n                           epochs=20, #Increase this when not on Kaggle kernel\n                           verbose=2,  #1 for ETA, 0 for silent\n                           validation_data=(x_val[:400,:], y_val[:400,:]), #For speed\n                           callbacks=[annealer])","5af218f8":"final_loss, final_acc = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))","7e340c7f":"plt.plot(hist.history['loss'], color='b')\nplt.plot(hist.history['val_loss'], color='r')\nplt.title('train_logLoss and Val_loss')\nplt.legend(['train_logLoss','Val_loss'],loc= 'upper right')\nplt.show()\nplt.plot(hist.history['acc'], color='b')\nplt.plot(hist.history['val_acc'], color='r')\nplt.title('train_acc and Val_accuracy')\nplt.legend(['train_acc','Val_accuracy'], loc = 'center right')\nplt.show()","d57b0112":"y_hat = model.predict(x_val)\ny_pred = np.argmax(y_hat, axis=1)\ny_true = np.argmax(y_val, axis=1)\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)","a425bad3":"mnist_testset = np.loadtxt(test_file, skiprows=1, dtype='int', delimiter=',')\nx_test = mnist_testset.astype(\"float32\")\nx_test = x_test.reshape(-1, 28, 28, 1)\/255.","da8b3c83":"y_hat = model.predict(x_test, batch_size=64)\ny_hat[:10]","7328d990":"y_pred = np.argmax(y_hat,axis=1)\ny_pred[:10]","5ce7ff43":"with open(output_file, 'w') as f :\n    f.write('ImageId,Label\\n')\n    for i in range(len(y_pred)) :\n        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))","b6a0d1af":"Not too bad, considering the minimal amount of training so far. In fact, we have only gone through the training data approximately five times. With proper training we should get really good results.\n\nAs you can see there are quite a few parameters that could be tweaked (number of layers, number of filters, Dropout parameters, learning rate, augmentation settings). This is often done with trial and error, and there is no easy shortcut. \n\nGetting convergence should not be a problem, unless you use an extremely large learning rate. It's easy, however, to create a net that overfits, with perfect results on the training set and very poor results on the validation data. If this happens, you could try increasing the Dropout parameters, increase augmentation, or perhaps stop training earlier. If you instead wants to increase accuracy, try adding on two more layers, or increase the number of filters.","195cf307":"We only used a subset of the validation set during training, to save time. Now let's check performance on the whole validation set.","55c4e3c8":"Another important method to improve generalization is augmentation. This means generating more training data by randomly perturbing the images. If done in the right way, it can force the net to only learn translation-invariant features. If you train this model over hundreds of epochs, augmentation will definitely improve your performance. Here in the Kernel, we will only look at each image 4-5 times, so the difference is smaller. We use a Keras function for augmentation.","7bf9dc90":"If you don't already have [Keras][1], you can easily install it through conda or pip. It relies on either tensorflow or theano, so you should have these installed first. Keras is already available here in the kernel and on Amazon deep learning AMI.\n\n  [1]: https:\/\/keras.io\/","27f1c824":"## Load the data","29d75ce7":"Submitting from this notebook usually gives you a result around 99%, with some randomness depending on weight initialization and test\/train data split. I achieved 99.3% by averaging over 5 good runs, and you can get higher than that if you train overnight.\n\nIf you've successfully come this far, you can now create similar CNN for all kinds of image recognition problems. Good luck!","2453f600":"## Evaluate","cd190266":"The labels were given as integers between 0 and 9. We need to convert these to one-hot encoding, i.e. a 10x1 array with one 1 and nine 0:s, with the position of the 1 showing us the value. See the example, with the position of the 1 showing the correct value for the digit in the graph above.","5981376f":"## Submit","466fa360":"## Convolutional Neural Networks\nIf you want to apply machine learning to image recognition, convolutional neural networks (CNN) is the way to go. It has been sweeping the board in competitions for the last several years, but perhaps its first big success came in the late 90's when [Yann LeCun][1] used it to solve MNIST with 99.5% accuracy. I will show you how it is done in Keras, which is a user-friendly neural network library for python.\n\nMany other notebooks here use a simple fully-connected network (no convolution) to achieve 96-97%, which is a poor result on this dataset. In contrast, what I will show you here is nearly state-of-the-art. In the Kernel (<20 minutes training) we will achieve 99%, but if you train it overnight (or with a GPU) you should reach 99.5. If you then ensemble over several runs, you should get close to the best published accuracy of 99.77% . (Ignore the 100% results on the leaderboard; they were created by learning the test set through repeat submissions)\n\nHere goes:\n\n\n  [1]: http:\/\/yann.lecun.com\/exdb\/lenet\/","25c8063f":"To easily get to the top half of the leaderboard, just follow these steps, go to the Kernel's output, and submit \"submission.csv\"","bf32518e":"## Train the model\n\nKeras offers two different ways of defining a network. We will the Sequential API, where you just add on one layer at a time, starting from the input.\n\nThe most important part are the convolutional layers Conv2D. Here they have 16-32 filters that use nine weights each to transform a pixel to a weighted average of itself and its eight neighbors. As the same nine weights are used over the whole image, the net will pick up features that are useful everywhere. As it is only nine weights, we can stack many convolutional layers on top of each other without running out of memory\/time. \n\nThe MaxPooling layers just look at four neighboring pixels and picks the maximal value. This reduces the size of the image by half, and by combining convolutional and pooling layers, the net be able to combine its features to learn more global features of the image. In the end we use the features in two fully-connected (Dense) layers.\n\nBatch Normalization is a technical trick to make training faster. Dropout is a regularization method, where the layer randomly replaces  a proportion of its weights to zero for each training sample. This forces the net to learn features in a distributed way, not relying to much on a particular weight, and therefore improves generalization. 'relu' is the activation function x -> max(x,0).","55bcd1e9":"We now reshape all data this way. Keras wants an extra dimension in the end, for channels. If this had been RGB images, there would have been 3 channels, but as MNIST is gray scale it only uses one.\n\nThis notebook is written for the tensorflow channel ordering. If you have Keras installed for Theano backend, you might start seeing some error message soon related to channel ordering. This can easily be [solved][1].\n\n\n  [1]: https:\/\/keras.io\/backend\/#set_image_dim_ordering","cc806b02":"We train once with a smaller learning rate to ensure convergence. We then speed things up, only to reduce the learning rate by 10% every epoch. Keras has a function for this: ","d0913fa8":"y_hat consists of class probabilities (corresponding to the one-hot encoding of the training labels). I now select the class with highest probability","77d0937a":"Each data point consists of 784 values. A fully connected net just treats all these values the same, but a CNN treats it as a 28x28 square. Thes two graphs explain the difference: It's easy to understand why a CNN can get better results.","9948f48c":"We will use a very small validation set during training to save time in the kernel.","969519e8":"The model needs to be compiled before training can start. As our loss function, we use logloss which is called ''categorical_crossentropy\" in Keras. Metrics is only used for evaluation. As optimizer, we could have used ordinary stochastic gradient descent (SGD), but Adam is faster.","b81ded7a":"It would be possible to train the net on the original data, with pixel values 0 to 255. If we use the standard initialization methods for weights, however, data between 0 and 1 should make the net converge faster. ","394b2c45":"As always, we split the data into a training set and a validation set, so that we can evaluate the performance of our model."}}