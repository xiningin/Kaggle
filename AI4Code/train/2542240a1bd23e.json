{"cell_type":{"00522271":"code","fdcb65bd":"code","aded5991":"code","a395ed43":"code","0947788f":"code","d73bd716":"markdown"},"source":{"00522271":"! pip install hvplot","fdcb65bd":"import pandas as pd\nimport holoviews as hv\nimport hvplot.pandas\nfrom sklearn.datasets import load_digits\nX, y = load_digits(return_X_y=True)\nX = X\/X.max()","aded5991":"from typing import List, Tuple\n\nimport numpy as np\nfrom sklearn.base import TransformerMixin\nfrom sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS\nfrom sklearn.neural_network._multilayer_perceptron import BaseMultilayerPerceptron\nfrom sklearn.utils import check_X_y\nfrom sklearn.utils.extmath import check_array, safe_sparse_dot\nfrom sklearn.utils.validation import column_or_1d\n\n\ndef kl_loss_delta(z_mean: np.ndarray, z_log_sigma: np.ndarray = 0.0) -> np.ndarray:\n    \"\"\"\n    see: https:\/\/keras.io\/examples\/variational_autoencoder\/\n    \n    \n    \"\"\"\n    return np.hstack((-2 * z_mean, 1 - np.exp(z_log_sigma)))\n\n\nclass VAE(BaseMultilayerPerceptron, TransformerMixin):\n    def __init__(\n        self,\n        encoder_layer_sizes: Tuple[int] = (100, 2),\n        activation: str = \"relu\",\n        latent_regularizer: str = \"kl\",\n        elbo_weight: float = 0.01,\n        out_activation: str = \"identity\",\n        loss: str = \"squared_loss\",\n        solver: str = \"adam\",\n        alpha: float = 0.0001,\n        batch_size: str = \"auto\",\n        learning_rate: str = \"constant\",\n        learning_rate_init: float = 0.001,\n        power_t: float = 0.5,\n        max_iter: int = 200,\n        shuffle: bool = True,\n        random_state=None,\n        tol: float = 1e-4,\n        verbose: bool = False,\n        warm_start: bool = False,\n        momentum: float = 0.9,\n        nesterovs_momentum: bool = True,\n        early_stopping: bool = False,\n        validation_fraction: float = 0.1,\n        beta_1: float = 0.9,\n        beta_2: float = 0.999,\n        epsilon: float = 1e-8,\n        n_iter_no_change: int = 10,\n        max_fun: int = 15000,\n    ):\n        \"\"\"See the documentation for\n        https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html\n        for more information information on the parameters\n\n        :param encoder_layer_sizes: The ith element represents the number of neurons\n            in the ith hidden layer with the last representing the latent space, \n            defaults to (100, 2)\n        :param latent_regularizer: This is the regularization schema on the latent space,\n            defaults to \"kl\"\n        :param elbo_weight: This is the weight on the latent_regularizer, defaults to 0.01\n        :param out_activation: This is the output activation which should map to the domain \n            of the data, defaults to \"identity\"\n        \"\"\"\n        self.encoder_layer_sizes = encoder_layer_sizes\n        self.latent_regularizer = latent_regularizer\n        self.elbo_weight = 0.01\n        self.out_activation = out_activation\n        self.out_activation_ = out_activation\n\n        self.activation = activation\n        self.solver = solver\n        self.alpha = alpha\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.learning_rate_init = learning_rate_init\n        self.power_t = power_t\n        self.max_iter = max_iter\n        self.loss = loss\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self.tol = tol\n        self.verbose = verbose\n        self.warm_start = warm_start\n        self.momentum = momentum\n        self.nesterovs_momentum = nesterovs_momentum\n        self.early_stopping = early_stopping\n        self.validation_fraction = validation_fraction\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n        self.n_iter_no_change = n_iter_no_change\n        self.max_fun = max_fun\n\n    def fit(self, X: np.array):\n        super().__init__(\n            hidden_layer_sizes=[\n                *[\n                    *self.encoder_layer_sizes[:-1],\n                    self.encoder_layer_sizes[-1] * 2,\n                    *reversed(self.encoder_layer_sizes[:-1]),\n                ],\n                X.shape[1],\n            ],\n            activation=self.activation,\n            solver=self.solver,\n            alpha=self.alpha,\n            loss=self.loss,\n            batch_size=self.batch_size,\n            learning_rate=self.learning_rate,\n            learning_rate_init=self.learning_rate_init,\n            power_t=self.power_t,\n            max_iter=self.max_iter,\n            shuffle=self.shuffle,\n            random_state=self.random_state,\n            tol=self.tol,\n            verbose=self.verbose,\n            warm_start=self.warm_start,\n            momentum=self.momentum,\n            nesterovs_momentum=self.nesterovs_momentum,\n            early_stopping=self.early_stopping,\n            validation_fraction=self.validation_fraction,\n            beta_1=self.beta_1,\n            beta_2=self.beta_2,\n            epsilon=self.epsilon,\n            n_iter_no_change=self.n_iter_no_change,\n            max_fun=self.max_fun,\n        )\n\n        super().fit(X=X, y=X)\n\n    def _backprop(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        activations: List[np.ndarray],\n        deltas: List[np.ndarray],\n        coef_grads: List[np.ndarray],\n        intercept_grads: List[np.ndarray],\n    ) -> Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:\n        \"\"\"Compute the MLP loss function and its corresponding derivatives\n        with respect to each parameter: weights and bias vectors.\n        Parameters\n        \n        :param X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input data.\n        :param y: ndarray of shape (n_samples,)\n            The target values.\n        :param activations: list, length = n_layers - 1\n             The ith element of the list holds the values of the ith layer.\n        :param deltas: list, length = n_layers - 1\n            The ith element of the list holds the difference between the\n            activations of the i + 1 layer and the backpropagated error.\n            More specifically, deltas are gradients of loss with respect to z\n            in each layer, where z = wx + b is the value of a particular layer\n            before passing through the activation function\n        :param coef_grads: list, length = n_layers - 1\n            The ith element contains the amount of change used to update the\n            coefficient parameters of the ith layer in an iteration.\n        intercept_grads : list, length = n_layers - 1\n        :param intercept_grads: The ith element contains the amount of change used to update the\n            intercept parameters of the ith layer in an iteration.\n        :coef_grads: The ith element contains the amount of change used to update the\n            intercept parameters of the ith layer in an iteration\n        :return: loss : float\n                 coef_grads : list, length = n_layers - 1\n                 intercept_grads : list, length = n_layers - 1\n        \"\"\"\n        n_samples = X.shape[0]\n\n        # Forward propagate\n        activations = self._forward_pass(activations)\n\n        # Get loss\n        loss_func_name = self.loss\n        if loss_func_name == \"log_loss\" and self.out_activation_ == \"logistic\":\n            loss_func_name = \"binary_log_loss\"\n        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])\n        # Add L2 regularization term to loss\n        values = np.sum(np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))\n        loss += (0.5 * self.alpha) * values \/ n_samples\n\n        # Backward propagate\n        last = self.n_layers_ - 2\n\n        # The calculation of delta[last] here works with following\n        # combinations of output activation and loss function:\n        # sigmoid and binary cross entropy, softmax and categorical cross\n        # entropy, and identity with squared loss\n        deltas[last] = activations[-1] - y\n\n        # Compute gradient for the last layer\n        coef_grads, intercept_grads = self._compute_loss_grad(\n            last, n_samples, activations, deltas, coef_grads, intercept_grads\n        )\n\n        # Iterate over the hidden layers\n        for i in range(self.n_layers_ - 2, 0, -1):\n\n            # VAE\n            # latent activity regulizer\n            if i == (len(self.encoder_layer_sizes) - 1):\n                assert (\n                    deltas[len(self.encoder_layer_sizes) - 1].shape[1]\n                    == self.encoder_layer_sizes[-1] * 2\n                )\n                if self.latent_regularizer == \"kl\":\n                    z = activations[len(self.encoder_layer_sizes)]\n                    deltas[len(self.encoder_layer_sizes) - 1] -= (\n                        self.elbo_weight\n                        * kl_loss_delta(\n                            z[:, : self.encoder_layer_sizes[-1]],\n                            z[:, -self.encoder_layer_sizes[-1] :],\n                        )\n                    )\n\n                elif self.latent_regularizer == \"mmd\":\n                    raise NotImplementedError(\n                        \"That is not a implemented activity regularizer,\\\n                                        reverting to l2.\"\n                    )\n                elif self.latent_regularizer == \"l2\":\n                    deltas[len(self.encoder_layer_sizes) - 1] -= self.elbo_weight * (\n                        -activations[len(self.encoder_layer_sizes)]\n                    )\n                else:\n                    raise ValueError(\n                        \"That is not a supported, activity regularizer,\\\n                                        non is being applied. \"\n                    )\n\n            deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)\n\n            if i == (len(self.encoder_layer_sizes) - 1):\n                inplace_derivative = DERIVATIVES[self.activation]\n                inplace_derivative(activations[i], deltas[i - 1])\n\n            coef_grads, intercept_grads = self._compute_loss_grad(\n                i - 1, n_samples, activations, deltas, coef_grads, intercept_grads\n            )\n\n        return loss, coef_grads, intercept_grads\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Predict using the trained model\n        Parameters\n        \n        :param X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input data.\n        :return: ndarray of (n_samples, n_outputs)\n            The latent space of the samples\n        \"\"\"\n        X = check_array(X, accept_sparse=[\"csr\", \"csc\", \"coo\"])\n\n        # Make sure self.hidden_layer_sizes is a list\n        hidden_layer_sizes = self.hidden_layer_sizes\n        if not hasattr(hidden_layer_sizes, \"__iter__\"):\n            hidden_layer_sizes = [hidden_layer_sizes]\n        hidden_layer_sizes = list(hidden_layer_sizes)\n\n        layer_units = [X.shape[1]] + hidden_layer_sizes + [self.n_outputs_]\n\n        # Initialize layers\n        activations = [X]\n\n        for i in range(self.n_layers_ - 1):\n            activations.append(np.empty((X.shape[0], layer_units[i + 1])))\n        # forward propagate\n        self._forward_pass(activations)\n\n        # VAE\n        y_transform = activations[len(self.encoder_layer_sizes)]\n        assert y_transform.shape[1] == self.encoder_layer_sizes[-1] * 2\n\n        return y_transform[:, : self.encoder_layer_sizes[-1]]\n\n    def _forward_pass(self, activations: List[np.ndarray]) -> List[np.ndarray]:\n        \"\"\"Perform a forward pass on the network by computing the values\n        of the neurons in the hidden layers and the output layer.\n        Parameters\n        \n        :param activations: list, length = n_layers - 1\n            The ith element of the list holds the values of the ith layer.\n        \"\"\"\n        hidden_activation = ACTIVATIONS[self.activation]\n        # Iterate over the hidden layers\n        for i in range(self.n_layers_ - 1):\n\n            # VAE\n            # ignore setop_gradient for latent variance weights on forward-pass\n            if i == len(self.encoder_layer_sizes):\n                assert self.coefs_[i].shape[0] == self.encoder_layer_sizes[-1] * 2\n                self.coefs_[i][: self.encoder_layer_sizes[-1], :] = 0.0\n\n            activations[i + 1] = safe_sparse_dot(activations[i], self.coefs_[i])\n            activations[i + 1] += self.intercepts_[i]\n\n            # VAE\n            # For the hidden layers that are not latent Z layers\n            if (i + 1) != (self.n_layers_ - 1):\n                if (i + 1) != len(self.encoder_layer_sizes):\n                    activations[i + 1] = hidden_activation(activations[i + 1])\n                else:\n                    assert (\n                        activations[i + 1].shape[1] == self.encoder_layer_sizes[-1] * 2\n                    )\n                    activations[i + 1] = activations[i + 1]\n\n        # For the last layer\n        output_activation = ACTIVATIONS[self.out_activation_]\n        activations[i + 1] = output_activation(activations[i + 1])\n\n        return activations\n\n    def _validate_input(\n        self, X: np.ndarray, y: np.ndarray, incremental\n    ) -> Tuple[np.ndarray]:\n        X, y = check_X_y(\n            X, y, accept_sparse=[\"csr\", \"csc\", \"coo\"], multi_output=True, y_numeric=True\n        )\n        if y.ndim == 2 and y.shape[1] == 1:\n            y = column_or_1d(y, warn=True)\n        return X, y\n","a395ed43":"model = VAE(encoder_layer_sizes=(25,5, 2,), \n            learning_rate_init=0.0001,\n            activation='tanh', \n            elbo_weight=1.5, \n            max_iter=1000,\n            out_activation='sigmoid',\n            loss='log_loss',\n            latent_regularizer='kl')\nmodel.fit(X)","0947788f":"hv.extension('bokeh')\n\n(pd.DataFrame(model.transform(X), columns=['x','y'])\n .assign(digit=y.astype(str))\n .hvplot.scatter(x='x',y='y', c='digit'))","d73bd716":"Don't get me wrong, I love Tensorflow 2.0 (TF), and I love PyTorch and for complex deep learning problems they are incredible. Still, having grown up using Scikit-Learn and loving the API, I really miss the pipelining and hyperparameter optimization features that come with that ecosystem. If you want to do deep learning in TF, it is great, and you can iterate fast, but not every problem is a deep learning problem and its very hard to mix and match between Scikit-learn and Tensorflow cause you can't just pickle Tensorflow models in the same way.  \n\nVariational Autoencoders are a wonderful bread-and-butter for deep learning dimensionality reduction. While I really try and avoid overcomplicated my modelling, there is a time and place where these models can shine.  One thing I like about Scikit-learn models is how easy it is to extend on them and customize their brilliant suite of tools for your own applications.  Doing large model search and testing can be really fast and fun in Scikit-learn and is really easy for beginners getting used to modelling in Python.  \n\nVariational Autoencoders are a wonderful case of where Deep Learning steals elegantly from Statistics.  In Autoencoders a model is trained to map a high-dimensional dataset to a low-dimensional space and reconstruct it.  With Variational Autoencoders, we try to contain the low-dimensional space using an 'activity regularizer' which limits how extreme the values of the latent space can be and how the latent space is shaped.  In other model architectures, this could be the l2 loss, but in the case of the VAE, this loss function measures the 'distance' between the latent data and a normal distribution. This means that if we penalize this latent space enough and our reconstruction loss is low enough, we can try to generate new 'never-seen-before' data by sampling from a normal distribution and decoding it into the high-dimensional space.  \n\nThis is by no means a definative implementation, but I would love your input and idea on Scikit-learn, where you use it and why you love it!"}}