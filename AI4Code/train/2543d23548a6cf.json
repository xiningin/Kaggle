{"cell_type":{"1619a560":"code","9659e10c":"code","ee414774":"code","9ccfbd03":"code","f6fc07e2":"code","07ffe8c4":"code","d63f5605":"code","899e185a":"code","e979778f":"code","a01a55e6":"code","41ae103b":"code","f81c109b":"code","6875197a":"code","e0ff04ec":"code","1240f424":"code","97db75f5":"code","10239bd9":"code","a4c25ad0":"code","2fa3a2b3":"code","2083bd58":"code","8a05cb6b":"code","92fd0381":"code","2343b0da":"code","210bc058":"code","1c819050":"code","a8c60800":"code","39535ddc":"code","714e83a1":"code","8369fa8e":"code","1b148349":"code","3f878831":"code","01dac464":"code","5bba074a":"code","9c9c9a1f":"code","27c699b4":"code","6bf530c4":"code","b7fa2705":"code","81df17f3":"code","ecd48f11":"markdown","a22ebabd":"markdown","1819e544":"markdown","4feb372d":"markdown","be053700":"markdown","ee0fc295":"markdown","83a95ac9":"markdown","efe0ee20":"markdown","ee4db844":"markdown","e6caa1ab":"markdown","8c616c1c":"markdown","d3ea6585":"markdown","b1e71023":"markdown","50970465":"markdown","eb1df461":"markdown","17feeef3":"markdown","3dc24519":"markdown","b738463f":"markdown","f206dc89":"markdown","6d79c584":"markdown","5e161305":"markdown","7941f1c2":"markdown","64294666":"markdown","6d427057":"markdown","13bf7e86":"markdown","5274a9ca":"markdown"},"source":{"1619a560":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","9659e10c":"import pandas as pd","ee414774":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","9ccfbd03":"ad = pd.read_csv(\"..\/input\/Advertising.csv\", index_col=0)","f6fc07e2":"ad.info()","07ffe8c4":"ad.describe()","d63f5605":"ad.head()","899e185a":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(ad.TV, ad.Sales, color='blue', label=\"TV\")\nplt.scatter(ad.Radio, ad.Sales, color='green', label='Radio')\nplt.scatter(ad.Newspaper, ad.Sales, color='red', label='Newspaper')\n\nplt.legend(loc=\"lower right\")\nplt.title(\"Sales vs. Advertising\")\nplt.xlabel(\"Advertising [1000 $]\")\nplt.ylabel(\"Sales [Thousands of units]\")\nplt.grid()\nplt.show()","e979778f":"ad.corr()","a01a55e6":"plt.imshow(ad.corr(), cmap=plt.cm.Blues, interpolation='nearest')\nplt.colorbar()\ntick_marks = [i for i in range(len(ad.columns))]\nplt.xticks(tick_marks, ad.columns, rotation='vertical')\nplt.yticks(tick_marks, ad.columns)","41ae103b":"import statsmodels.formula.api as sm","f81c109b":"modelAll = sm.ols('Sales ~ TV + Radio + Newspaper', ad).fit()\nmodelAll.params","6875197a":"  # we need first to calculate the Residual Sum of Squares (RSS)\ny_pred = modelAll.predict(ad)\nimport numpy as np\nRSS = np.sum((y_pred - ad.Sales)**2)\nRSS","e0ff04ec":"y_mean = np.mean(ad.Sales) # mean of sales\nTSS = np.sum((ad.Sales - y_mean)**2)\nTSS","1240f424":"p=3 # we have three predictors: TV, Radio and Newspaper\nn=200 # we have 200 data points (input samples)\n\nF = ((TSS-RSS)\/p) \/ (RSS\/(n-p-1))\nF","97db75f5":"RSE = np.sqrt((1\/(n-2))*RSS); \nRSE","10239bd9":"np.mean(ad.Sales)","a4c25ad0":"R2 = 1 - RSS\/TSS; \nR2","2fa3a2b3":"modelAll.summary()","2083bd58":"def evaluateModel (model):\n    print(\"RSS = \", ((ad.Sales - model.predict())**2).sum())\n    print(\"R2 = \", model.rsquared)","8a05cb6b":"modelTV = sm.ols('Sales ~ TV', ad).fit()\nmodelTV.summary().tables[1]","92fd0381":"evaluateModel(modelTV)","2343b0da":"modelRadio = sm.ols('Sales ~ Radio', ad).fit()\nmodelRadio.summary().tables[1]","210bc058":"evaluateModel(modelRadio)","1c819050":"modelPaper = sm.ols('Sales ~ Newspaper', ad).fit()\nmodelPaper.summary().tables[1]","a8c60800":"evaluateModel(modelPaper)","39535ddc":"modelTVRadio = sm.ols('Sales ~ TV + Radio', ad).fit()\nmodelTVRadio.summary().tables[1]","714e83a1":"evaluateModel(modelTVRadio)","8369fa8e":"modelTVPaper = sm.ols('Sales ~ TV + Newspaper', ad).fit()\nmodelTVPaper.summary().tables[1]","1b148349":"evaluateModel(modelTVPaper)","3f878831":"evaluateModel(modelAll)","01dac464":"modelTVRadio.summary()","5bba074a":"modelTVRadio.params","9c9c9a1f":"normal = np.array([0.19,0.05,-1])\npoint  = np.array([-15.26,0,0])\n# a plane is a*x + b*y +c*z + d = 0\n# [a,b,c] is the normal. Thus, we have to calculate\n# d and we're set\nd = -np.sum(point*normal) # dot product\n# create x,y\nx, y = np.meshgrid(range(50), range(300))\n# calculate corresponding z\nz = (-normal[0]*x - normal[1]*y - d)*1.\/normal[2]","27c699b4":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nfig.suptitle('Regression: Sales ~ Radio + TV Advertising')\nax = Axes3D(fig)\n\nax.set_xlabel('Radio')\nax.set_ylabel('TV')\nax.set_zlabel('Sales')\nax.scatter(ad.Radio, ad.TV, ad.Sales, c='red')\n\nax.plot_surface(x,y,z, color='cyan', alpha=0.3)","6bf530c4":"modelSynergy = sm.ols('Sales ~ TV + Radio + TV*Radio', ad).fit()\nmodelSynergy.summary().tables[1]","b7fa2705":"evaluateModel(modelSynergy)","81df17f3":"modelSynergy.params","ecd48f11":"The M2 model can be described by this equation:  \nSales = 0.19 Radio + 0.05 TV + 2.9 which I can write as:  \n0.19x + 0.05y - z + 2.9 = 0  \nIts normal is (0.19, 0.05, -1)  \nand a point on the plane is (-2.9\/0.19,0,0) = (-15.26,0,0)","a22ebabd":"Let's plot the actual values as red points and the model predictions as a cyan plane:","1819e544":"When there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1.  \nOn the other hand, if Ha is true, then we expect F to be greater than 1.  \nIn this case, **F is far larger than 1: at least one of the three advertising media must be related to sales.**","4feb372d":"The F-statistic is the ratio between (TSS-RSS)\/p and RSS\/(n-p-1):","be053700":"## Import Advertising data","ee0fc295":"We can interpret \u03b23 as the increase in the effectiveness of TV advertising for a one unit increase in radio advertising (or vice-versa).\n","83a95ac9":"Unfortunately, there are a total of 2^p models that contain subsets of p variables.  \n\nFor three predictors, it would still be manageable: only 8 models to fit and evaluate but as p increases, the number of models grows exponentially.  \n\nInstead, we can use other approaches. The three classical ways are the forward selection (start with no features and add one after the other until a threshold is reached); the backward selection (start with all features and remove one by one) and the mixed selection (a combination of the two).  \n\nWe try here the forward selection.\n\n### Forward selection\n\nWe start with a null model (no features), we then fit three (p=3) simple linear regressions and add to the null model the variable that results in the lowest RSS.","efe0ee20":"The results strongly suggest that the model that includes the interaction term is superior to the model that contains only main effects. The p-value for the interaction term, TV\u00d7radio, is extremely low, indicating that there is strong evidence for Ha : \u03b23 not zero. In other words, it is clear that the true relationship is not additive.","ee4db844":"## How strong is the relationship?\nOnce we have rejected the null hypothesis in favour of the alternative hypothesis, it is natural to want to quantify the extent to which the model fits the data.  \nThe quality of a linear regression fit is typically assessed using two related quantities: the **residual standard error (RSE)** and the **R-squared** statistic (the square of the correlation of the response and the variable, when close to 1 means high correlation).","e6caa1ab":"We interpret these results as follows: for a given amount of TV and newspaper advertising, spending an additional 1000 dollars on radio advertising leads to an increase in sales by approximately 189 units.  \nIn contrast, the coefficient for newspaper represents the average effect (negligible) of increasing newspaper spending by 1000 dollars while holding TV and radio fixed.","8c616c1c":"## Is at least one of the features useful in predicting Sales?\nWe use a hypothesis test to answer this question.  \nThe most common hypothesis test involves testing the null hypothesis of:  \nH0: There is **no relationship** between the media and sales versus the alternative hypothesis Ha: There **is some relationship** between the media and sales.  \n  \nMathematically, this corresponds to testing  \nH0: \u03b21 = \u03b22 = \u03b23 = \u03b24 = 0 versus  \nHa: at least one \u03b2i is non-zero.  \nThis hypothesis test is performed by computing the F-statistic:","d3ea6585":"Following is an example of features selection for the linear regression.    \nIt is based on the Advertising Dataset, taken from the masterpiece book *Introduction to Statistical Learning by Hastie, Witten, Tibhirani, James.*  \nThe dataset contains statistics about the sales of a product in 200 different markets, together with advertising budgets in each of these markets for different media channels: TV, radio and newspaper.  \nImaging being the Marketing responsible and you need to prepare a new advertising plan for next year.  \nYou may be interested in answering questions such as:\n\n * which media contribute to sales? \n * Do all three media\u2014TV, radio, and newspaper\u2014contribute to sales, or do just one or two of the media contribute?","b1e71023":"## Summary\nstatsmodels has a handy function that provides the above metrics in one single table:","50970465":"First of all, we fit a regression line using the Ordinary Least Square algorithm, i.e. the line that minimises the squared differences between the actual Sales and the line itself.  \nThe multiple linear regression model takes the form:\nSales = \u03b20 + \u03b21*TV + \u03b22*Radio + \u03b23*Newspaper + \u03b5, where Beta are the regression coefficients we want to find and epsilon is the error that we want to minimise.","eb1df461":"Well, the model with TV AND Radio greatly decreased RSS and increased R2, so that will be our M2 model.  \nNow, we have only three variables here. We can decide to stop at M2 or use an M3 model with all three variables.  \nRecall that we already fitted and evaluated a model with all features, just at the beginning.","17feeef3":"The R-squared for this model is 96.8 %, compared to only 89.7% for the model M2 that predicts sales using TV and radio without an interaction term. This means that (96.8 \u2212 89.7)\/(100 \u2212 89.7) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term.  \n\nA linear model that uses radio, TV, and an interaction between the two to predict sales takes the form:\nsales = \u03b20 + \u03b21 \u00d7 TV + \u03b22 \u00d7 radio + \u03b23 \u00d7 (radio\u00d7TV) + \u03b5","3dc24519":"## Which media contribute to sales?\nTo answer this question, we could examine the p-values associated with each predictor\u2019s t-statistic. In the multiple linear regression above, the p-values for TV and radio are low, but the p-value for newspaper is not. This suggests that only TV and radio are related to sales.  \n\nBut as just seen, if p is large then we are likely to make some false discoveries.  \n\nThe task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as **variable \/ feature selection.**  \n\nIdeally, we could perform the variable selection by trying out a lot of different models, each containing a different subset of the features.  \nWe can then select the best model out of all of the models that we have considered (for example, the model with the smallest RSS and the biggest R-squared; other used metrics are the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and the adjusted R2.  All of them are visible in the summary model.","b738463f":"The lowest RSS and the highest R2 are for the TV medium.  \nNow we have a best model M1 which contains TV advertising.   \nWe then add to this M1 model the variable that results in the lowest RSS for the new two-variable model.   \nThis approach is continued until some stopping rule is satisfied.","f206dc89":"## Plotting the model\n\nThe M2 model has two variables therefore can be plotted as a plane in a 3D chart.","6d79c584":"M3 is slightly better than M2 (but remember that R2 always increases when adding new variables) so we call the approach completed and decide that the M2 model with TV and Radio is the good compromise. Adding the newspaper could possibly overfits on new test data.  \nNext year no budget for newspaper advertising and that amount will be used for TV and Radio instead.","5e161305":"## Is there a relationship between sales and advertising?","7941f1c2":"## Is there synergy among the advertising media?\n\nAdding radio to the model leads to a substantial improvement in R-squared. This implies that a model that uses TV and radio expenditures to predict sales is substantially better than one that uses only TV advertising.  \n\nHowever, this simple model may be incorrect. Suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases. In this situation, given a fixed budget of $100K spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio.  \n\nIn marketing, this is known as a **synergy effect.** The figure above suggests that such an effect may be present in the advertising data. Notice that when levels of either TV or radio are low, then the true sales are lower than predicted by the linear model. But when advertising is split between the two media, then the model tends to underestimate sales.\n","64294666":"You can find [more notebook examples around linear regression on my GitHub.][1]\n\n\n  [1]: https:\/\/github.com\/Mashimo\/datascience","6d427057":"One thing to note is that R-squared will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.  \nTherefore an **adjusted R-squared** is provided, which is R-squared adjusted by the number of predictors.  \n\nAnother thing to note is that the summary table provides also a **t-statistic and a p-value** for each single feature.\nThese provide information about whether each individual predictor is related to the response (high t-statistic or low p-value).  \nBut be careful looking only at these individual p-values instead of looking at the overall F-statistic.  \nIt seems likely that if any one of the p-values for the individual features is very small, then at least one of the predictors is related to the response.   \nHowever, this logic is flawed, especially when you have many predictors; statistically about 5 % of the p-values will be below 0.05 by chance (this is the effect infamously leveraged by the so-called *p-hacking*).  \n\nThe F-statistic does not suffer from this problem because it adjusts for the number of predictors.","13bf7e86":"Now we need the Total Sum of Squares (TSS): the total variance in the response Y, and can be thought of as the amount of variability inherent in the response before the regression is performed.  \nThe distance from any point in a collection of data, to the mean of the data, is the deviation.","5274a9ca":"RSE is 1.68 units while the mean value for the response is 14,022, indicating a percentage error of roughly 12%.  \nSecond, the R2 statistic records the percentage of variability in the response that is explained by the predictors.  \nThe predictors explain almost 90% of the variance in sales."}}