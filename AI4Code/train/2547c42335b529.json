{"cell_type":{"372b3b89":"code","6c247d97":"code","c3412783":"code","29943beb":"code","94d81f52":"code","f4f5d633":"code","f74b0bb2":"code","692e6653":"code","94395492":"code","a07adeb0":"code","f13c9df1":"code","884b1b47":"code","1dabede1":"code","c50d2b50":"code","a0b58e32":"code","8f8ffcc1":"code","8eb4e38d":"code","76101d12":"code","5a1fa7d7":"code","e933bf21":"code","01cc66d7":"code","c8c768a9":"code","03ffb6bd":"code","d4fa2f70":"code","3c0334fe":"code","19971540":"code","7d773e2f":"code","641b7850":"code","1eeaf591":"code","8f054f12":"code","5ac3215d":"code","f8ffea98":"code","e70418d8":"code","5bcbb31e":"code","8d463d5a":"code","b4c500e6":"code","2bb6f217":"code","0ffd7993":"code","939cfae6":"code","d6dfa8dc":"code","c8b6fc33":"code","ce904d5d":"code","0e6a00f5":"code","fc5ae5d1":"code","77480c12":"code","67d1c02f":"code","0e5c3799":"code","aa72eb6c":"code","4015c553":"code","aabc4ace":"code","96f32e6e":"code","cf83a490":"code","5347600b":"code","778f4987":"code","b57d986e":"code","2267d343":"code","816b13ea":"code","925e2ac1":"code","ef0d3cf3":"code","b0c1aad9":"code","fb233346":"code","deec204d":"code","45569c4d":"markdown","397ffdae":"markdown","c1663c07":"markdown","bc4aacde":"markdown","bb2704ac":"markdown"},"source":{"372b3b89":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom sklearn import preprocessing\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier","6c247d97":"os.getcwd()","c3412783":"raw_data = pd.read_csv(\"..\/input\/my-dataset\/credit_train.csv\")","29943beb":"raw_data.info()","94d81f52":"raw_data.describe()","f4f5d633":"data = raw_data.drop(['Loan ID', 'Customer ID'], axis=1)","f74b0bb2":"# Loan Status dropna\n\ndata.dropna(subset=['Loan Status'], inplace = True)","692e6653":"data.info()","94395492":"# Credit Score 850 \uc774\uc0c1 \ub370\uc774\ud130 \ucc98\ub9ac \n\ndata['Credit Score'][data['Credit Score'] > 850] = data['Credit Score'][data['Credit Score'] > 850] \/ 10","a07adeb0":"data['Credit Score'].fillna(0, inplace = True)\ndata['Annual Income'].fillna(0, inplace = True)\ndata['Monthly Debt'].fillna(0, inplace = True)\ndata['Number of Open Accounts'].fillna(0, inplace = True)\ndata['Current Credit Balance'].fillna(0, inplace = True)\ndata['Maximum Open Credit'].fillna(0, inplace = True)\n\ndata['Number of Credit Problems'].fillna(0, inplace = True)\ndata['Bankruptcies'].fillna(0, inplace = True)\ndata['Tax Liens'].fillna(0, inplace = True)\ndata['Years in current job'].fillna(0, inplace = True)","f13c9df1":"data['Months since last delinquent'].max()","884b1b47":"data['Months since last delinquent'] = data['Months since last delinquent'] - 177\ndata['Months since last delinquent'].fillna(0, inplace = True)","1dabede1":"plt.figure(10*10)\nfor i in range(len(data.columns)):\n  try:\n    sns.distplot(data.loc[data['Loan Status']=='Fully Paid', data.columns[i]], label='Fully Paid')\n    sns.distplot(data.loc[data['Loan Status']=='Charged Off', data.columns[i]], label = 'Charged Off')\n    plt.legend(loc='best')\n    plt.show()\n\n  except:\n    pass","c50d2b50":"data.info()","a0b58e32":"data.describe()","8f8ffcc1":"# labeling\ndata['Loan Status'] = data['Loan Status'].replace(['Fully Paid', 'Charged Off'], [1, 0])","8eb4e38d":"data['Loan Status'].unique()","76101d12":"data['Term'].replace((\"Short Term\",\"Long Term\"),(0,1), inplace=True)\ndata.head()","5a1fa7d7":"# data = data.join(pd.get_dummies(data['Purpose'],drop_first = True))\ndata = data.drop(['Purpose'], axis=1)","e933bf21":"data = data.join(pd.get_dummies(data['Home Ownership'],drop_first = True))\ndata = data.drop(['Home Ownership'], axis=1)","01cc66d7":"# Years in current job\ndata['Years in current job'] = data['Years in current job'].str.lower()\ndata['Years in current job'] = data['Years in current job'].str.extract(r\"(\\d+)\")\ndata['Years in current job'] = data['Years in current job'].astype(float)","c8c768a9":"data['Years in current job'].fillna(0, inplace = True)\ndata","03ffb6bd":"data.columns","d4fa2f70":"#\ub3c5\ub9bd\ubcc0\uc218(Purpose \ubd84\ub958)\ndebt_invest= ['renewable_energy', 'small_business', 'Business Loan']\ndebt_consume = ['Buy House', 'Take a Trip', 'Home Improvements', 'Buy a Car', 'Medical Bills', 'wedding', 'major_purchase', 'vacation', 'Educational Expenses', 'moving', 'Other', 'other']\ndebt_change = ['Debt Consolidation']","3c0334fe":"data","19971540":"# con_short = list()\n# con_long = list()\n# cha_short = list()\n# cha_long = list()\n# inv_short = list()\n# inv_long = list()\n\n# # \ubaa9\uc801\ubcc4 \uae30\uac04\ubcc4 \ubd84\ub958\n# for index, row in data.iterrows():\n\n#   if row['Purpose'] in debt_change:\n#     if row['Term'] == 0:\n#       cha_short.append(row.values)\n#     elif row['Term'] == 1:\n#       cha_long.append(row.values)\n\n#   elif row['Purpose'] in debt_consume:\n#     if row['Term'] == 0:\n#       con_short.append(row.values)\n#     elif row['Term'] == 1:\n#       con_long.append(row.values)\n\n#   elif row['Purpose'] in debt_invest:\n#     if row['Term'] == 0:\n#       inv_short.append(row.values)\n#     elif row['Term'] == 1:\n#       inv_long.append(row.values)","7d773e2f":"# all_list = [con_short,\n#             con_long,\n#             cha_short,\n#             cha_long,\n#             inv_short,\n#             inv_long]","641b7850":"# n = 1\n# for li in all_list:\n#   globals()[\"df_{}\".format(n)] = pd.DataFrame(li, columns=data.columns)\n\n#   # Home Ownership\n#   globals()[\"df_{}\".format(n)]['Home Ownership'] = globals()[\"df_{}\".format(n)]['Home Ownership'].str.lower()\n#   globals()[\"df_{}\".format(n)]['Home Ownership'] = globals()[\"df_{}\".format(n)]['Home Ownership'].replace([val for val in globals()[\"df_{}\".format(n)]['Home Ownership'].unique()],\n#                                                                                                           [num for num in range(len(globals()[\"df_{}\".format(n)]['Home Ownership'].unique()))])\n\n#   # Purpose\n#   globals()[\"df_{}\".format(n)]['Purpose'] = globals()[\"df_{}\".format(n)]['Purpose'].str.lower()\n#   globals()[\"df_{}\".format(n)]['Purpose'] = globals()[\"df_{}\".format(n)]['Purpose'].replace([val for val in globals()[\"df_{}\".format(n)]['Purpose'].unique()],\n#                                                                                             [num for num in range(len(globals()[\"df_{}\".format(n)]['Purpose'].unique()))])\n  \n#   # Years in current job\n#   globals()[\"df_{}\".format(n)]['Years in current job'] = globals()[\"df_{}\".format(n)]['Years in current job'].str.lower()\n#   globals()[\"df_{}\".format(n)]['Years in current job'] = globals()[\"df_{}\".format(n)]['Years in current job'].str.extract(r\"(\\d+)\")\n#   globals()[\"df_{}\".format(n)]['Years in current job'] = globals()[\"df_{}\".format(n)]['Years in current job'].astype(float)\n#   globals()[\"df_{}\".format(n)]['Years in current job'].fillna(globals()[\"df_{}\".format(n)]['Years in current job'].mean(), inplace = True)\n\n  \n#   n += 1","1eeaf591":"# from sklearn.preprocessing import MinMaxScaler\n# mmscaler = MinMaxScaler()\n\n# for i in range(1,7):\n#   globals()[\"df_{}\".format(i)]\n#   globals()[\"x_{}\".format(i)] = globals()[\"df_{}\".format(i)].drop('Loan Status', axis=1)\n#   globals()[\"y_{}\".format(i)] = globals()[\"df_{}\".format(i)]['Loan Status']\n\n#   mmscaler.fit(globals()[\"x_{}\".format(i)])\n#   globals()[\"mmScaled_{}\".format(i)] =pd.DataFrame(mmscaler.transform(globals()[\"x_{}\".format(i)]), columns= globals()[\"x_{}\".format(i)].columns)\n\n","8f054f12":"# from keras.models import Sequential\n# from keras.layers import Activation, Dense\n# from keras import optimizers\n# from sklearn.model_selection import train_test_split","5ac3215d":"# for i in range(1, 7):\n#   (x_train, x_test, y_train, y_test) = train_test_split(globals()[\"mmScaled_{}\".format(i)], globals()[\"y_{}\".format(i)], train_size=0.8, random_state=1)\n#   globals()['model_{}'.format(i)] = Sequential()\n#   globals()['model_{}'.format(i)].add(Dense(128, input_shape = (x_train.shape[1],), activation = 'relu'))\n#   globals()['model_{}'.format(i)].add(Dense(1, activation = 'sigmoid'))\n#   globals()['model_{}'.format(i)].compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n#   globals()['model_{}'.format(i)].fit(x_train, y_train, epochs=10, verbose=1, batch_size=32)\n#   print(\"model Score: \",globals()['model_{}'.format(i)].evaluate(x_test, y_test, batch_size=32))","f8ffea98":"ke = data.dropna()","e70418d8":"data","5bcbb31e":"# ke = ke.drop(['Years in current job', 'Home Ownership', 'Purpose'], axis=1)","8d463d5a":"ke['Term'] = ke['Term'].replace([i for i in ke['Term'].unique()], [i for i in range(len(ke['Term'].unique()))])","b4c500e6":"ke['Loan Status'] = ke['Loan Status'].replace([i for i in ke['Loan Status'].unique()], [1,0])","2bb6f217":"ke","0ffd7993":"x = ke.drop(['Loan Status'], axis=1)\ny = ke['Loan Status']","939cfae6":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","d6dfa8dc":"x1 = pd.DataFrame(scaler.fit_transform(x), columns=x.columns)","c8b6fc33":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n(x_train, x_test, y_train, y_test) = train_test_split(x1, y, train_size=0.8, random_state=1)\nlog = LogisticRegression()\nlog.fit(x_train, y_train)\nlog.score(x_test, y_test)\n","ce904d5d":"from sklearn.metrics import confusion_matrix\npre = log.predict(x_test)\nconfusion_matrix(y_true=y_test,y_pred=pre)","0e6a00f5":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\ngbrt.fit(x_train, y_train)\n\nprint(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4: {:.3f}\".format(gbrt.score(x_train, y_train)))\nprint(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4: {:.3f}\".format(gbrt.score(x_test, y_test)))","fc5ae5d1":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics    \nforest = RandomForestClassifier(n_estimators=100)\nforest.fit(x_train, y_train)\ny_pred = forest.predict(x_test)\nprint(y_pred)\n# \uc815\ud655\ub3c4 \ud655\uc778\nprint('\uc815\ud655\ub3c4 :', metrics.accuracy_score(y_test, y_pred))","77480c12":"x_train.shape","67d1c02f":"import tensorflow as tf\nfrom tensorflow.keras import layers\nimport numpy as np\n\n# Our vectorized labels\n# y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n# y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))\n\n(x_train, x_test, y_train, y_test) = train_test_split(x1, y, train_size=0.8, random_state=1)\n\nmodel = tf.keras.Sequential()\n\nmodel.add(layers.Input(shape=x_train.shape[1]))\nmodel.add(layers.Dense(128, activation='sigmoid')) \nmodel.add(layers.Dense(1, activation='sigmoid')) \n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train, epochs=30, verbose=1, batch_size=30)","0e5c3799":"history.history.keys() # \ucd9c\ub825 \uac00\ub2a5 \uac12 \ud655\uc778","aa72eb6c":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\n","4015c553":"plt.plot(history.history['binary_accuracy'])","aabc4ace":"loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)","96f32e6e":"predictions = model.predict(x_test)","cf83a490":"predictions","5347600b":"y_pred = np.argmax(predictions, axis=1)","778f4987":"for i, j in enumerate(predictions):\n    if j > 0.5:\n        predictions[i] = 1\n    else:\n        predictions[i] = 0","b57d986e":"from sklearn.metrics import confusion_matrix\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, predictions))","2267d343":"predictions.shape","816b13ea":"predictions","925e2ac1":"predictions = predictions.reshape(1,7285)\nser_test = pd.Series(predictions[0])","ef0d3cf3":"ser_test","b0c1aad9":"import seaborn as sns\nsns.kdeplot(ser_test, cumulative=True, bw=1.5)","fb233346":"predictions","deec204d":"predictions","45569c4d":"# draw distplot","397ffdae":"* 3.drop purpose Home Ownership Year in current job","c1663c07":"# fill missing values\n\n* Credit Score : mean\n* Annual Income : mean\n* Maximum Open Credit : mean\n\n* Tax Liens : 0\n* Bankruptcies : 0\n* Years in current job : 0\n* Months since last delinquent : 0\n","bc4aacde":"\uba38\uc2e0\ub7ec\ub2dd \uae30\ubc18 \ub370\uc774\ud130 \ubd84\uc11d \ubc15\uc900\uc131","bb2704ac":"* Months since last delinquentd\uc758 \uac12\uc774 \ub9c8\uc9c0\ub9c9 \uc5f0\uccb4\ub85c \ubd80\ud130\uc758 \uae30\uac04\uc744 \uc758\ubbf8\ud568\n* null\uc774 \ub9ce\uc740 \uc774\uc720\uac00 \uc5f0\uccb4\ub97c \ud574\ubcf8\uc801 \uc5c6\ub294 \uc0ac\ub78c\uc77c \uac70\ub77c\uace0 \ud310\ub2e8\ud568\n* null\uc744 0\uc73c\ub85c \ucc44\uc6cc\uc11c \uac00\uc7a5 \ud070 \uac12\uc73c\ub85c \uc0bc\uace0 \ub098\uba38\uc9c0\ub294 max\uac12\uc744 \ube7c\uc11c \uc74c\uc218\ub85c \ub9cc\ub4ec"}}