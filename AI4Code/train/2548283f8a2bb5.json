{"cell_type":{"49fa0785":"code","c9dbe1a0":"code","d94236d9":"code","e756b643":"code","daba4a72":"code","35eeaa42":"code","db33e39d":"code","6428d121":"code","1dae9463":"code","166d8f0e":"code","daededa8":"code","82499265":"code","cdcc0978":"code","6790ee3d":"code","1fc90cb0":"code","f9822497":"code","03fb7426":"code","91b77e3f":"code","35be268b":"code","94d9aa11":"code","bcefa193":"code","f14646f8":"code","1ad48a0b":"code","a594f356":"code","45984e17":"code","9f812459":"code","3a25e29e":"code","f09a1d24":"code","b1a84c72":"code","e19e390f":"code","fa37b5a3":"code","4f25120f":"code","1079a268":"code","c4e8911e":"code","3bca4d73":"code","e65b9492":"code","be97f871":"code","909171f1":"code","9999d6ea":"code","9e5585fe":"code","85404888":"code","2c7bbec1":"code","95231fe9":"code","a68ad424":"code","8eaf44ff":"markdown","f68d72a0":"markdown","fe2e490e":"markdown","cb3bdb5c":"markdown","c3883895":"markdown"},"source":{"49fa0785":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 50)\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn import linear_model\nfrom sklearn.naive_bayes import GaussianNB","c9dbe1a0":"df = pd.read_csv('..\/input\/data.csv')\ndf.sample(10)","d94236d9":"print(df.diagnosis.nunique())\ndf.diagnosis.unique()","e756b643":"df['diagnosis_cat'] = pd.factorize(df['diagnosis'])[0]","daba4a72":"df.shape","35eeaa42":"df.info()","db33e39d":"df.columns","6428d121":"df.isnull().any()","1dae9463":"df = df.drop(['id', 'Unnamed: 32'], 1)","166d8f0e":"df.describe()","daededa8":"plt.rcParams['figure.figsize']=(20,19)\nsns.heatmap(df.corr(), annot=True, linewidths=.5, fmt = \".2f\", cmap=\"BuPu\");","82499265":"plt.rcParams['figure.figsize']=(8,8)\nax = sns.countplot(x = 'diagnosis', data = df, palette = 'hls');\nax.set_title(label='Diagnosis distribution', fontsize=15);","cdcc0978":"names= 'B', 'M'\nsize=df['diagnosis'].value_counts()\n\nmy_circle=plt.Circle((0,0), 0.7, color='white')\n\nplt.pie(size, labels=names, colors=['skyblue','red'])\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","6790ee3d":"g = sns.pairplot(df.iloc[:,0:11], hue = 'diagnosis');\ng = g.map_diag(plt.hist, histtype=\"step\", linewidth=3)","1fc90cb0":"sns.pairplot(df.iloc[:,11:21]);","f9822497":"sns.pairplot(df.iloc[:,21:31]);","03fb7426":"v = sns.PairGrid(df.iloc[:,21:31])\nv.map_lower(sns.kdeplot);\nv.map_upper(plt.scatter);\nv.map_diag(sns.kdeplot);","91b77e3f":"def feats(df):\n    feats_from_df = set(df.select_dtypes([np.int, np.float]).columns.values)\n    bad_feats = {'diagnosis_cat'}\n    return list(feats_from_df - bad_feats)\n\ndf_scaled = df\ndf_scaled[feats(df)] = preprocessing.scale(df[feats(df)])","35be268b":"plt.subplots(figsize=(20,5))\ndf_melted = pd.melt(df_scaled, id_vars = \"diagnosis\", \n                      value_vars = ('radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n                                    'smoothness_mean', 'compactness_mean', 'concavity_mean',\n                                    'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean'))\nsns.violinplot(x = \"variable\", y = \"value\", hue=\"diagnosis\",data= df_melted);","94d9aa11":"plt.subplots(figsize=(20,5))\ndf_melted = pd.melt(df_scaled, id_vars = \"diagnosis\", \n                      value_vars = ('radius_se', 'texture_se', 'perimeter_se', 'area_se',\n                                    'smoothness_se', 'compactness_se', 'concavity_se',\n                                    'concave points_se', 'symmetry_se', 'fractal_dimension_se'))\nsns.violinplot(x = \"variable\", y = \"value\", hue=\"diagnosis\",data= df_melted);","bcefa193":"plt.subplots(figsize=(20,5))\ndf_melted = pd.melt(df_scaled, id_vars = \"diagnosis\", \n                      value_vars = ('radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n                                    'smoothness_worst', 'compactness_worst', 'concavity_worst',\n                                    'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst'))\nsns.violinplot(x = \"variable\", y = \"value\", hue=\"diagnosis\",data= df_melted);","f14646f8":"plt.rcParams['figure.figsize']=(20,5)\n\nmean_value = ('radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n              'smoothness_mean', 'compactness_mean', 'concavity_mean',\n               'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean')\n\nfor i, feat in enumerate(mean_value):\n    m = plt.hist(df[df[\"diagnosis\"] == \"M\"][feat],bins=30,fc = (1,0,0,0.5),label = \"Malignant\")\n    b = plt.hist(df[df[\"diagnosis\"] == \"B\"][feat],bins=30,fc = (0,1,0,0.5),label = \"Bening\")\n    plt.legend()\n    plt.xlabel(mean_value[i] + ' values')\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of \" + mean_value[i] +  \" for bening and malignant breast cancer\")\n    plt.show()","1ad48a0b":"def feats(df):\n    feats_from_df = set(df.select_dtypes([np.int, np.float]).columns.values)\n    bad_feats = {'diagnosis', 'diagnosis_cat'}\n    return list(feats_from_df - bad_feats)\n\ndef model_train_predict(model, X, y, success_metric=accuracy_score):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    return success_metric(y_val, y_pred)\n\ndef plot_learning_curve(model, title, X, y, ylim=None, cv = None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure(figsize=(12,8))\n    plt.title(title)\n    if ylim is not None:plt.ylim(*ylim)\n\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Testing score\")\n\n    plt.legend(loc=\"best\")\n    return plt","a594f356":"X = df_scaled[feats(df_scaled)].values\ny = df_scaled['diagnosis_cat']","45984e17":"models = [\n    LogisticRegression(penalty = 'l2'),\n    DecisionTreeClassifier(max_depth=10),\n    RandomForestClassifier(max_depth=10)\n]\n\nfor model in models:\n    print(str(model) + \": \")\n    %time score = model_train_predict(model, X, y)\n    print(str(score) + \"\\n\")\n    plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.2), cv=15, n_jobs=4)\n    plt.show()","9f812459":"models = [\n    DecisionTreeClassifier(max_depth=10),\n    RandomForestClassifier(max_depth=10)\n]\n\nfor model in models:\n    model.fit(X, y)\n    importances = model.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    plt.figure(figsize=(10, 5))\n    plt.title('Feature importances: ' + str(model).split('(')[0])\n    plt.bar(range(X.shape[1]), model.feature_importances_[indices],\n           color = 'b', align = 'center')\n    plt.xticks(range(X.shape[1]), [ feats(df_scaled)[x] for x in indices])\n    plt.xticks(rotation=90)\n    plt.xlim([-1, X.shape[1]])\n    plt.show()","3a25e29e":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\ndef compute(params):\n    model = xgb.XGBClassifier(**params)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_val)\n    score = accuracy_score(y_val, y_pred)\n    #print(\"Score: %.2f\" % score)\n    #print(params)\n    return (1 - score)\n\nspace = {\n        'max_depth':  hp.choice('max_depth', range(4,6)),\n        'min_child_weight': hp.uniform('min_child_weight', 0, 10),\n        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n        'gamma': hp.quniform('gamma', 0.5, 1, 0.05)\n    }\n\nbest = fmin(compute, space, algo=tpe.suggest, max_evals=250)\nprint(best)","f09a1d24":"model = xgb.XGBClassifier(**best)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_val)\naccuracy_score(y_val, y_pred)","b1a84c72":"plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.2), cv=15, n_jobs=4)\nplt.show()","e19e390f":"def confusion_matrix(y_val,y_pred):\n    confusion_matrix = metrics.confusion_matrix(y_val, y_pred)\n\n    plt.figure(figsize=(5,5))\n    ax= plt.subplot()\n    sns.heatmap(confusion_matrix, annot=True,fmt='g', ax = ax);\n    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n    ax.set_title('Confusion Matrix'); \n    plt.show()","fa37b5a3":"confusion_matrix(y_val, y_pred)","4f25120f":"auc = roc_auc_score(y_val, y_pred)\nprint('AUC: %.3f' % auc)\nfpr, tpr, thresholds = roc_curve(y_val, y_pred)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.show()","1079a268":"pca = PCA(n_components=3)\nX_train_pca = pca.fit_transform(X_train)\nX_val_pca = pca.fit_transform(X_val)\n\nmodel_pca = LogisticRegression()\nmodel_pca.fit(X_val_pca, y_val)\nmodel_pca.score(X_val_pca, y_val)","c4e8911e":"per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n\nplt.bar(x=range(1, len(per_var)+1), height = per_var, tick_label = labels)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot')\nplt.show()","3bca4d73":"df_pca = pd.DataFrame(data = X_train_pca, columns = ['PCA_1', 'PCA_2', 'PCA_3'])\ndf_pca = pd.concat([df_pca, df_scaled['diagnosis']], axis =1)","e65b9492":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1: {0}%'.format(per_var[0]), fontsize = 15)\nax.set_ylabel('Principal Component 2: {0}%'.format(per_var[1]), fontsize = 15)\n\ntargets = ['B', 'M']\ncolors = ['g', 'r',]\nfor target, color in zip(targets,colors):\n    indicesToKeep = df_pca['diagnosis'] == target\n    ax.scatter(df_pca.loc[indicesToKeep, 'PCA_1']\n               , df_pca.loc[indicesToKeep, 'PCA_2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","be97f871":"model = SVC(gamma='auto')\nmodel.fit(X_train, y_train)\nmodel.score(X_val, y_val)","909171f1":"plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.2), cv=15, n_jobs=4)\nplt.show()","9999d6ea":"model = LinearSVC(random_state=0, tol=1e-5)\nmodel.fit(X_train, y_train)\nmodel.score(X_val, y_val)","9e5585fe":"plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.2), cv=15, n_jobs=4)\nplt.show()","85404888":"model = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\nmodel.fit(X_train, y_train)\nmodel.score(X_val, y_val)","2c7bbec1":"plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.2), cv=15, n_jobs=4)\nplt.show()","95231fe9":"model = GaussianNB()\nmodel.fit(X_train, y_train)\nmodel.score(X_val, y_val)","a68ad424":"plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.2), cv=15, n_jobs=4)\nplt.show()","8eaf44ff":"### Visualisations","f68d72a0":"#### Conclusion\n\n1. Dataset has no NaN value.\n2. Target is predict wheather the cancer is. We have two type of answer: benign and malignant.\n3. Target is categorial variable, so we must factorize it.\n4. Dataset contains 10 features with their mean, se and worst dimension.\n5. We saw that the dataset has outlayers so we make standarisation.\n6. In dataset are correlated features, so we could make dimentional reduction.\n7. In dataset are more benign nobservation than malgnant.\n8. If we saw largest mean amount we have high probability, that the cancer is malignant. Especially parameters tell us about it: radius, perimeter, area, compactness, concavity.\nSmoothness, symmetry show bening cancer.","fe2e490e":"# Breast Cancer Wisconsin (Diagnostic)\n\nTarget: Predict whether the cancer is benign or malignant\n\nSource: https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\n\nIn this notebook, I will touch medicine in practice. I analyze breast cancer Wisconsin. It is a diagnostic data set.\nI studied computer science in medicine, so now I would like to combine medical knowledge with machine learning.\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. Fine-needle aspiration (FNA) is a diagnostic procedure used to investigate lumps or masses. In this technique, a thin (23\u201325 gauge), hollow needle is inserted into the mass for sampling of cells that, after being stained, will be examined under a microscope (biopsy).\n\nhttps:\/\/en.wikipedia.org\/wiki\/Fine-needle_aspiration","cb3bdb5c":"### Models!","c3883895":"### Basic intro"}}