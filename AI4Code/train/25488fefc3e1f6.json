{"cell_type":{"4441d0c2":"code","3d6e2da9":"code","94275920":"code","c5c99ccc":"code","a87cad64":"code","24c57969":"code","50dedc5d":"code","49b796c0":"code","16e50f68":"code","9af02f13":"code","10529e36":"code","b4c4fc77":"code","ee7edf92":"code","f3824fb1":"code","44780837":"code","b6915cd4":"code","240a9ad5":"code","7be94b2a":"markdown","812ec376":"markdown","8031068b":"markdown","8df4ac98":"markdown","7bb8ba2e":"markdown","a9d7af5a":"markdown","971f04e5":"markdown"},"source":{"4441d0c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","3d6e2da9":"data = pd.read_csv(\"..\/input\/heart.csv\")\ndata.info() # to see type of the features\n","94275920":"\n\ndata.head() # to see features and target variable (1: Male  | 0: Famale)\ndata.describe()\n","c5c99ccc":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'sex'], data.loc[:,'sex']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","a87cad64":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.5,random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'sex'], data.loc[:,'sex']\nknn.fit(x_train,y_train)\ny_prediction = knn.predict(x_test)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","24c57969":"# Model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","50dedc5d":"y_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_prediction)\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","49b796c0":"data.columns","16e50f68":"\n\nx = np.array(data.loc[:,'trestbps']).reshape(-1,1)\ny = np.array(data.loc[:,'chol']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('trestbps')\nplt.ylabel('chol')\nplt.show()","9af02f13":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('trestbps')\nplt.ylabel('chol')\nplt.show()","10529e36":"# CV\nfrom sklearn.model_selection import cross_val_score\nreg = LinearRegression()\nk = 5\ncv_result = cross_val_score(reg,x,y,cv=k) # uses R^2 as score \nprint('CV Scores: ',cv_result)\nprint('CV scores average: ',np.sum(cv_result)\/k)","b4c4fc77":"# Ridge\nfrom sklearn.linear_model import Ridge\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 2, test_size = 0.3)\nridge = Ridge(alpha = 0.1, normalize = True)\nridge.fit(x_train,y_train)\nridge_predict = ridge.predict(x_test)\nprint('Ridge score: ',ridge.score(x_test,y_test))","ee7edf92":"# Lasso\nfrom sklearn.linear_model import Lasso\nx = np.array(data.loc[:,['trestbps','chol']])\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 3, test_size = 0.3)\nlasso = Lasso(alpha = 0.1, normalize = True)\nlasso.fit(x_train,y_train)\nridge_predict = lasso.predict(x_test)\nprint('Lasso score: ',lasso.score(x_test,y_test))\nprint('Lasso coefficients: ',lasso.coef_)\n\n","f3824fb1":"# Confusion matrix with random forest\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = data.loc[:,data.columns != 'sex'], data.loc[:,'sex']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))\n# visualize with seaborn library\nsns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","44780837":"# ROC Curve with logistic regression\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n# abnormal = 1 and normal = 0\n\nx,y = data.loc[:,(data.columns != 'sex') & (data.columns != 'sex')], data.loc[:,'sex']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","b6915cd4":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x,y)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","240a9ad5":"# grid search cross validation with 2 hyperparameter\n# 1. hyperparameter is C:logistic regression regularization parameter\n# 2. penalty l1 or l2\n# Hyperparameter grid\nparam_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 12)\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=3)\nlogreg_cv.fit(x_train,y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\nprint(\"Best Accuracy: {}\".format(logreg_cv.best_score_))","7be94b2a":"**ROC Curve with Logistic Regression**\n\n\n* ROC is receiver operationg characteristic. In this curve x axis is false positive rate and y axis is true positive rate\n* If the curve in plot is closer to left-top corner, test is more accurate.\n* Roc curve score is auc that is computation area under the curve from prediction scores\n* We want auc to closer 1\n* fpr = False Positive Rate\n* tpr = True Positive Rate","812ec376":"**Regularized Regression**\n\nIf linear regression thinks that one of the feature is important, it gives high coefficient to this feature. However, this can cause overfitting that is like memorizing in KNN. In order to avoid overfitting, we use regularization that penalize large coefficients.\n","8031068b":"**Introduction**\n\n\nWelcome to my supervised learning implementation kernel. In this kernel, I will carry out supervised learning  algorithms to predict the gender of the person according to features in \"Heart Disease UCI\" dataset and compare accuracy of those algorithms  confusion matrix.","8df4ac98":"**REGRESSION**\n\nLinear regression\n\n*  y = ax + b where y = target, x = feature and a = parameter of model\n*  We choose parameter of model(a) according to minimum error function that is lost function\n*  In linear regression we use Ordinary Least Square (OLS) as lost function.\n* OLS: sum all residuals but some positive and negative residuals can cancel each other so we sum of square of residuals. It is called OLS\n* Score: Score uses R^2 method that is ((y_pred - y_mean)^2 )\/(y_actual - y_mean)^2","7bb8ba2e":"**HYPERPARAMETER TUNING**\n\nthere are hyperparameters that are need to be tuned\n\nFor example:\nk at KNN\nalpha at Ridge and Lasso\nRandom forest parameters like max_depth\nlinear regression parameters(coefficients)\nHyperparameter tuning:\n* try all of combinations of different parameters\n* fit all of them\n* measure prediction performance\n* see how well each performs\n* finally choose best hyperparameters\n","a9d7af5a":"**K-NEAREST NEIGHBORS (KNN)**\n\nKNN: Look at the K closest labeled data points\nClassification method","971f04e5":"**CROSS VALIDATION**\n\n* K folds = K fold CV.\n* Look at this image it defines better than me :)\n* When K is increase, computationally cost is increase\n* cross_val_score(reg,x,y,cv=5): use reg(linear regression) with x and y that we define at above and K is 5. It means 5 times(split, train,predict)"}}