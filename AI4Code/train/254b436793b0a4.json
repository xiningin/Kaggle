{"cell_type":{"3920f32e":"code","e026fb3d":"code","52b7b62b":"code","e3623178":"code","b1df4d83":"code","1591d8cd":"code","8c2d0cd3":"code","c90825bd":"code","c3cb77b5":"code","a09267ab":"code","f6876cfb":"code","b226d176":"code","7dd73ba3":"code","2cab19e2":"code","6f0da495":"code","f008c824":"code","7e212b57":"code","7adfccf5":"code","3f18fca1":"code","d07d8e19":"code","3d190e94":"code","fdb2e07e":"code","2465ac61":"code","ec99d263":"code","7d99da13":"code","16415a5c":"code","b19084fe":"code","dd59de88":"code","7e72d3e8":"code","d7a23ed6":"code","b43a8b11":"code","e096f8de":"code","37223014":"code","661928f8":"code","eb3138ff":"code","01791b6d":"code","455a6520":"code","0a56d5ba":"code","50c1ef46":"code","f8da096d":"code","4938be4a":"code","4bc96ad2":"code","b525d0fd":"code","68d624a3":"code","a2c24988":"code","66b5a072":"markdown","205b6618":"markdown","cb83623f":"markdown","38744318":"markdown","81f2d712":"markdown","972b1973":"markdown","514a386e":"markdown","f02fbe73":"markdown","b1c76cd0":"markdown","a96cc132":"markdown","4d9ba857":"markdown"},"source":{"3920f32e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e026fb3d":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\nplt.rcParams['figure.figsize'] = (8, 6)\nplt.rcParams['font.size'] = 14","52b7b62b":"iris=pd.read_csv(\"..\/input\/Iris.csv\")\niris.head()","e3623178":"# Copy of iris dataset\niris_data= iris.drop(['Id'],axis=1)\niris_data.head()","b1df4d83":"# Descriptive statistics \ndf_summary=iris_data.describe()\ndf_summary","1591d8cd":"boxplot=iris_data.plot(kind='box', subplots=True, layout=(2,3), sharex=False, sharey=False, colormap='bwr', figsize=(15,10))\n\nprint('From the boxplot, it is visible that only sepal width is having the outlier, whereas sepallength, petallength, petalwidth having datapoints within min and maxrange.')\nprint('It is visible that values in sepallength and sepalwidth are tightly distributed and IQ range is small, whereas in petallength and petalwidth the values are distibuted widely,so IQ range are high.')","8c2d0cd3":"# Correlation Matrix for Iris_dataset\niris_data.corr()","c90825bd":"# Scatterplot with best fit line to explore relation b\/w sepallength(dependent)& sepalwidth,petalwidth,petallengh(Independent)\n\na=sns.lmplot(x='SepalWidthCm', y='SepalLengthCm', data=iris_data, aspect=1.5, scatter_kws={'alpha':0.2})\nb=sns.lmplot(x='PetalWidthCm', y='SepalLengthCm', data=iris_data, aspect=1.5, scatter_kws={'alpha':0.2})\nc=sns.lmplot(x='PetalLengthCm', y='SepalLengthCm', data=iris_data, aspect=1.5, scatter_kws={'alpha':0.2})  \n\n","c3cb77b5":"#Combine scatterplot to explore relation b\/w sepallength, sepalwidth, petalwidth, petallength\n\nscatter=pd.plotting.scatter_matrix(iris_data,figsize=(15,10))\n","a09267ab":"#Scatterplot EDA b\/w sepallength, sepalwidth, petalwidth, petallength and how species are reacting to it.\n\nscatterplot=sns.pairplot(iris_data,hue=\"Species\")","f6876cfb":"#Checking the relationship between Sepal width and Sepal length\ninput_cols = ['SepalWidthCm']\noutput_variable = ['SepalLengthCm']\nX = iris_data[input_cols]\nY = iris_data[output_variable]\n#Creating the Linear Regression Model\nlinreg = LinearRegression()\nlinreg.fit(X,Y)\nprint (linreg.intercept_)\nprint (linreg.coef_)\nprint('\\n')\nprint('Sepal.length = -0.2088*Sepal.Width + 6.481')\nprint('\\n')\nprint('Holding constant fixed, a 1 centimeter increase in sepalwidth lead to a decrease in Sepalength by 0.208centimeter')\n","b226d176":"#Checking the relationship between Petal length and Sepal length\ninput_cols = ['PetalLengthCm']\noutput_variable = ['SepalLengthCm']\nX = iris_data[input_cols]\nY = iris_data[output_variable]\n#Creating the Linear Regression Model\nlinreg = LinearRegression()\nlinreg.fit(X,Y)\nprint (linreg.intercept_)\nprint (linreg.coef_)\nprint('\\n')\nprint('Sepal.length = 0.409*Petal.length + 4.305')\nprint('\\n')\nprint('Holding constant fixed, a 1 centimeter increase in petallength lead to a increase in Sepalength by 0.409 centimeter')","7dd73ba3":"#Checking the relationship between Petal width and Sepal length\ninput_cols = ['PetalWidthCm']\noutput_variable = ['SepalLengthCm']\nX = iris_data[input_cols]\nY = iris_data[output_variable]\n#Creating the Linear Regression Model\nlinreg = LinearRegression()\nlinreg.fit(X,Y)\nprint (linreg.intercept_)\nprint (linreg.coef_)\nprint('\\n')\nprint('Sepal.length = 0.887*Petal.Width + 4.77')\nprint('\\n')\nprint('Holding constant fixed, a 1 centimeter increase in petalwidth lead to a increase in Sepalength by 0.887 centimeter')","2cab19e2":"#Checking the relationship between Petal width,Petal Length and Sepal length (Multivariate Model)\ninput_cols = ['PetalWidthCm','PetalLengthCm','SepalWidthCm']\noutput_variable = ['SepalLengthCm']\nX = iris_data[input_cols]\nY = iris_data[output_variable]\n#Creating the Linear Regression Model\nlinreg = LinearRegression()\nlinreg.fit(X,Y)\nprint (linreg.intercept_)\nprint (linreg.coef_)\n\nprint('\\n')\n\nprint('Sepal.length = -0.56*Petal.Width + 0.71*Petal.Length + 0.65*Sepal.Width + 1.84')\n\nprint('\\n')\n\nprint('Holding all other independent variable fixed, a 1 centimeter decrease in petalwidth lead to a increase in Sepalength by -0.56cm')\nprint('Holding all other independent variable fixed, a 1 centimeter increase in petallength lead to a increase in Sepalength by 0.71cm')\nprint('Holding all other independent variable fixed, a 1 centimeter increase in sepalwidth lead to a increase in Sepalength by 0.65cm')\nprint('\\n')\nprint('In given mutivariate model, the petalwidth is showing a negative relation with sepal.length, which proves a level of correlation or dependency between petallength and petalwidth.')\nprint('This is proved by the correlation matrix as well. Therefore, we have a multicollinearity issue with this model and need to drop one of them.')","6f0da495":"# Multivariate model with Sepallength, Sepalwidth, Petallength\n\ninput_cols = ['PetalLengthCm','SepalWidthCm']\noutput_variable = ['SepalLengthCm']\nX = iris_data[input_cols]\nY = iris_data[output_variable]\n#Creating the Linear Regression Model\nlinreg = LinearRegression()\nlinreg.fit(X,Y)\nprint (linreg.intercept_)\nprint (linreg.coef_)\nprint('\\n')\nprint('Sepal.length = 0.47*Petal.Length + 0.59*Sepal.Width + 2.25')\nprint('\\n')\nprint('Holding all other independent variable fixed, a 1 centimeter increase in petallength lead to a increase in Sepalength by 0.47 centimeter')\nprint('Holding all other independent variable fixed, a 1 centimeter increase in sepalwidth lead to a increase in Sepalength by 0.59 centimeter')","f008c824":"# Multivariate model with Sepallength, Sepalwidth, Petalwidth\n\ninput_cols = ['PetalWidthCm','SepalWidthCm']\noutput_variable = ['SepalLengthCm']\nX = iris_data[input_cols]\nY = iris_data[output_variable]\n#Creating the Linear Regression Model\nlinreg = LinearRegression()\nlinreg.fit(X,Y)\nprint (linreg.intercept_)\nprint (linreg.coef_)\nprint('\\n')\nprint('Sepal.length = 0.96*Petal.Width + 0.39*Sepal.Width + 3.46')\nprint('\\n')\nprint('Holding all other independent variable fixed, a 1 centimeter increase in petalwidth lead to a increase in Sepalength by 0.96 centimeter')\nprint('Holding all other independent variable fixed, a 1 centimeter increase in sepalwidth lead to a increase in Sepalength by 0.39 centimeter')","7e212b57":"#Check for multicollinearity thrrough Determinant value\nimport numpy as np\ninput_cols = ['PetalWidthCm','PetalLengthCm', 'SepalWidthCm']\noutput_variable = ['SepalLengthCm']\nX = iris_data[input_cols]\nY = iris_data[output_variable]\ncorr = np.corrcoef(X, rowvar=0)\nprint(corr)\nprint('\\n')\nprint (np.linalg.det(corr))\n\nprint('\\n')\n\nprint('petallength and petalwidth are highly correlated=96% and lead to multicollinearity.')\nprint('\\n')\nprint('Run the input variable in different combination to find the determinant value.The deteminant of correlation matrix is 0<=D<=1.')\nprint('D=0, then it indicates exact interdependence of expalanatory variable. D=1, then expalanatory variable independent to each other and have no multicollinearity issue.')\nprint('\\n')\nprint('1.Determinant value for petalwidth, petallength, sepalwidth =0.057.')\nprint('2.Determinant value for petallength, sepalwidth =0.816.')\nprint('3.Determinant value for petalwidth, sepalwidth =0.865.')\nprint('4.Determinant value for petalwidth, petallength =0.072.')\nprint('\\n')\nprint('We will avoid the model 1 & 4, as the value of D is close to 0, which indicates the multicollinearity issue. Whereas model 2 & 3 are acceptable, as the value of D is close to 1 and independent variables are not dependent to eachother.')\nprint('In general, when threshold level is D>0.7, then we can take all the input variables in the model.If D<0.4, then we can say that there is lot of interdependency between variables and we need to drop those variables which are highly correlated and causing multicollinearity.')","7adfccf5":"input_cols = ['PetalLengthCm', 'SepalWidthCm']\noutput_variable = ['SepalLengthCm']\nX = iris_data[input_cols]\ny = iris_data[output_variable]\ny=iris_data['SepalLengthCm']","3f18fca1":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=12)","d07d8e19":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","3d190e94":"# Buliding the Linear model with the algorithm\nlin_reg=LinearRegression()\nmodel=lin_reg.fit(X_train,y_train)","fdb2e07e":"# Coefficient of determination or R squared value  # input_cols = ['Petal.Length', 'Sepal.Width']\n\nprint ('R-Squared for training dataset model:', model.score(X_train,y_train))\n\nprint('\\n')\n\nprint('The high R-Squared value (0.834) from petallength & sepalwidth, implies that petallength & sepalwidth can be relied to explain 83.4% of the variations in sepallength.')","2465ac61":"# input_cols = ['Petal.Length', 'Sepal.Width']\nprint(model.intercept_)\nprint (model.coef_)","ec99d263":"## Predicting the x_test with the model\npredicted=model.predict(X_test)\n","7d99da13":"# Input variable petallength and Sepal width\n\nprint ('MAE:', metrics.mean_absolute_error(y_test, predicted))\nprint ('MSE:', metrics.mean_squared_error(y_test, predicted))\nprint ('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predicted)))","16415a5c":"## R Squared value or coefficient of determination\nprint(metrics.r2_score(y_test,predicted))\n\n","b19084fe":"#Compute null RMSE\n# split X and y into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=12)\n\n# create a NumPy array with the same shape as y_test\ny_null = np.zeros_like(y_test, dtype=float)\n\n# fill the array with the mean value of y_test\ny_null.fill(y_test.mean())\ny_null\n","dd59de88":"print(y_test.shape)\nprint(y_null.shape)","7e72d3e8":"# compute null RMSE\nnp.sqrt(metrics.mean_squared_error(y_test, y_null))","d7a23ed6":"feature_cols=['PetalWidthCm','PetalLengthCm', 'SepalWidthCm']","b43a8b11":"# define a function that accepts a list of features and returns testing RMSE\ndef train_test_rmse(feature_cols):\n    X = iris_data[feature_cols]\n    y=iris_data['SepalLengthCm']\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=12)\n    linreg = LinearRegression()\n    linreg.fit(X_train, y_train)\n    y_pred = linreg.predict(X_test)\n    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n","e096f8de":"# compare different sets of features\nprint (train_test_rmse(['PetalWidthCm','PetalLengthCm', 'SepalWidthCm']))\nprint (train_test_rmse(['PetalWidthCm', 'SepalWidthCm']))\nprint (train_test_rmse(['PetalLengthCm', 'SepalWidthCm']))","37223014":"# define a function that accepts a list of features and returns testing MSE\ndef train_test_mse(feature_cols):\n    X = iris_data[feature_cols]\n    y=iris_data['SepalLengthCm']\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=12)\n    linreg = LinearRegression()\n    linreg.fit(X_train, y_train)\n    y_pred = linreg.predict(X_test)\n    return metrics.mean_squared_error(y_test, y_pred)","661928f8":"# compare different sets of features\nprint (train_test_mse(['PetalWidthCm','PetalLengthCm', 'SepalWidthCm']))\nprint (train_test_mse(['PetalWidthCm', 'SepalWidthCm']))\nprint (train_test_mse(['PetalLengthCm', 'SepalWidthCm']))","eb3138ff":"# define a function that accepts a list of features and returns testing MAE\ndef train_test_mae(feature_cols):\n    X = iris_data[feature_cols]\n    y=iris_data['SepalLengthCm']\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=12)\n    linreg = LinearRegression()\n    linreg.fit(X_train, y_train)\n    y_pred = linreg.predict(X_test)\n    return metrics.mean_absolute_error(y_test, y_pred)","01791b6d":"# compare different sets of features\nprint (train_test_mae(['PetalWidthCm','PetalLengthCm', 'SepalWidthCm']))\nprint (train_test_mae(['PetalWidthCm', 'SepalWidthCm']))\nprint (train_test_mae(['PetalLengthCm', 'SepalWidthCm']))","455a6520":"iris_data.head(5)","0a56d5ba":"# create dummy variables\nSpecies_dummies = pd.get_dummies(iris_data.Species, prefix='Species')\n\n# print 5 random rows from seed value 12\nSpecies_dummies.sample(n=5, random_state=12)","50c1ef46":"Species_dummies.drop(Species_dummies .columns[0], axis=1, inplace=True)\niris_data = pd.concat([iris_data, Species_dummies], axis=1)\niris_data.head()\n#iris_data.sample(n=5, random_state=12)","f8da096d":"iris_data.corr()","4938be4a":"feature_dummies1 =['PetalLengthCm', 'SepalWidthCm', 'Species_Iris-versicolor','Species_Iris-virginica']\n\n# create X and y\nX = iris_data[feature_dummies1]\ny = iris_data['SepalLengthCm']\n\n# instantiate and fit\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=12)\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\ny_pred = linreg.predict(X_test)\n\n\n\n# print the coefficients\nprint (linreg.intercept_)\nprint (list(zip(feature_dummies1, linreg.coef_)))\n#print (linreg.coef_)\n\nprint('\\n')\n\n# the predicted value of Sepallength\n\nprint('Predicted value of Sepal length:', y_pred)","4bc96ad2":"feature_dummies2 =['PetalWidthCm','SepalWidthCm', 'Species_Iris-versicolor','Species_Iris-virginica']\n\n# create X and y\nX = iris_data[feature_dummies2]\ny = iris_data['SepalLengthCm']\n\n# instantiate and fit\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=12)\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\ny_pred = linreg.predict(X_test)\n\n\n\n# print the coefficients\nprint (linreg.intercept_)\nprint (list(zip(feature_dummies2, linreg.coef_)))\n#print (linreg.coef_)\n\nprint('\\n')\n\n# the predicted value of Sepallength\n\nprint('Predicted value:', y_pred)\n","b525d0fd":"feature_dummies3 =['PetalLengthCm', 'PetalWidthCm','SepalWidthCm', 'Species_Iris-versicolor','Species_Iris-virginica']\n\n# create X and y\nX = iris_data[feature_dummies3]\ny = iris_data['SepalLengthCm']\n\n# instantiate and fit\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=12)\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\ny_pred = linreg.predict(X_test)\n\n\n\n# print the coefficients\nprint (linreg.intercept_)\nprint (list(zip(feature_dummies3, linreg.coef_)))\n#print (linreg.coef_)\n\nprint('\\n')\n\n# the predicted value of Sepallength\n\nprint('Predicted value:', y_pred)\n\nprint('\\n')\n\nprint('This model is not effective due to multicollinearity b\/w the independent variables, petal length and width.')","68d624a3":"print (train_test_rmse(['PetalLengthCm', 'Species_Iris-versicolor','Species_Iris-virginica','SepalWidthCm']))\nprint (train_test_rmse(iris_data.columns[iris_data.columns.str.startswith('Species_')]))\nprint (train_test_rmse(['PetalWidthCm', 'Species_Iris-versicolor','Species_Iris-virginica','SepalWidthCm']))\n\nprint('\\n')\n\nprint('We can see among the calculated rmse value, the least rmse value is for the petal length, species_ and it is better than other rmse value. So, model#1 it is a best model to consider.')","a2c24988":"print('END')","66b5a072":"The iris(the flower)dataset attached contains five variables namely,\n1.SepalLength(Cm)\n2.SepalWidth(Cm)\n3.PetalLength(Cm)\n4.PetalWidth(Cm)\n5.Species\n\nIn this notebook, Linear Regression method is performed in python with Iris dataset taking Sepal Length as Response or dependent variable and rest of the variables as independent ones.\n\nBefore performing Linear Regression, following are checked:-\n1.> Whether  any multicollinearity exists in the independent variables.\n2.> correlation between dependent variable and each independent variable.\n3.> Any outlier in the variables given with suitable boxplots.\n\nHope you find this notebook to be useful for your learning. If you find this useful, Please Upvote!!!\n\n\n","205b6618":"From correlation matrix and scatterplot, it is clear that independent variable Petal.Length and Petal.width are positively and strongly correlated to Sepal.Length. Whereas Sepal.Width is negatively and weakly correlated to Sepal.Length. It also provide the insight of multicollinearity between the variable Petal.Length and Petal.Width","cb83623f":"When we don't have the input variable, we are taking mean and calculating null RMSE. Null RMSE is the RMSE that could be achieved by **always predicting the mean response value**. It is a benchmark against which you may want to measure your regression model.\n","38744318":"![](http:\/\/)We can see that Species Versiclor and Virginica are negativley and moderately correlated to each other.Whereas versicolor is weakly correlated to other variables, therefore a minimal impact on it with variations in other variables. However, virginica having a effective + ve correlation with petal length and width and weak negative correlation with sepalwidth. But it shows that a 64% changes in sepal length of virginca species can be explained by the variation in independent variables.","81f2d712":"**We have to ensure that our machine learning model RMSE value is lower than the manually computed RMSE value. If its not than our model is not effective and we better be okie with using central tendency method.**","972b1973":"# The accuracy of model \n\nwe look at the boxplot for Sepallength, the IQ range is between (5.2 to 6.4 cm). So most of the values lies b\/w (5.2 to 6.4 cm) with in 1.2cm. Therefore, the output will fall with in the variation range of +\/- 0.307 cm. We can say that the model is a good fit because the RMSE value is < than the null RMSE value.","514a386e":"R-squared, measure how close the data are fitted to the regression line.It lies between 0 to 1.\n\n0 indicates that the model explains none of the variability of the response data around its mean.\n\n1 indicates that the model explains all the variability of the response data around its mean.\n\nIn general, the higher the R-squared, the better the model fits your data. But, its not always true that a Low or high R-squared Values Inherently bad or good?","f02fbe73":"## 1.1. Form of linear regression  <a id='form'>\n\n$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n\n- $y$ is the response\n- $\\beta_0$ is the intercept\n- $\\beta_1$ is the coefficient for $x_1$ (the first feature)\n- $\\beta_n$ is the coefficient for $x_n$ (the nth feature)\n\nThe $\\beta$ values are called the **model coefficients**:\n\n- These values are estimated (or \"learned\") during the model fitting process using the **least squares criterion**.\n- Specifically, we are going to find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n- And once we've learned these coefficients, we can use the model to predict the response.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/upxacademy\/ML_with_Python\/master\/images\/estimating_coefficients.png?token=AH0Y7JmsKZPG913jPp4rVGpOba5gxbonks5ZFEq7wA%3D%3D\"><\/img>\n\nIn the diagram above:\n\n- The black dots are the **observed values** of x and y.\n- The blue line is our **least squares line**.\n- The red lines are the **residuals**, which are the vertical distances between the observed values and the least squares line.","b1c76cd0":"**To interpret the species coefficients? They are measured against the baseline (Setosa):**\n\nHolding all other features fixed, versicolor is associated with a decrease of 1.06cm in Sepallength compared to the Setosa.\n\nHolding all other features fixed, virginica is associated with a decrease of 1.56cm in Sepallength compared to the Setosa.","a96cc132":"# 2. Choosing between models \n\n2.1 Feature selection \nHow do we choose which features to include in the model? We're going to use train\/test split (and eventually cross-validation).\nWhy not use of p-values or R-squared for feature selection?\nLinear models rely upon a lot of assumptions (such as the features being independent), and if those assumptions are violated, p-values and R-squared are less reliable. Train\/test split relies on fewer assumptions.\nFeatures that are unrelated to the response can still have significant p-values.\nAdding features to your model that are unrelated to the response will always increase the R-squared value, and adjusted R-squared does not sufficiently account for this.\np-values and R-squared are proxies for our goal of generalization, whereas train\/test split and cross-validation attempt to directly estimate how well the model will generalize to out-of-sample data.\nMore generally:\nThere are different methodologies that can be used for solving any given data science problem, and this part follows a machine learning methodology.\nThe focuse is on general purpose approaches that can be applied to any model, rather than model-specific approaches.\n\n2.2. Evaluation metrics for regression problems \nEvaluation metrics for classification problems, such as accuracy, are not useful for regression problems. We need evaluation metrics designed for comparing continuous values.\nHere are three common evaluation metrics for regression problems:\nMean Absolute Error (MAE) is the mean of the absolute value of the errors:\nEvaluation metrics for classification problems, such as **accuracy**, are not useful for regression problems. We need evaluation metrics designed for comparing **continuous values**.\n\nHere are three common evaluation metrics for regression problems:\n\n**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n\nCharacteristics:\n\u2022Neutral to outliers\n\u2022Error is in same units as that of the data points.\n\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n**Mean Squared Error** (MSE) is the mean of the squared errors:\n\nCharacteristics:\n\u2022Accounts impact of outliers.\n\u2022Error is NOT in same unit as that of the data points .\n\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n\nCharacteristics:\n\u2022Accounts impact of outliers.\n\u2022Error is in same unit as that of the data points.\n\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\n\n\n\n\n**Only in training dataset, the bestfit line is formed to minimize the SSE, not in the testdata. We use RMSE in testdata to evaluate how good is the model. In other words, SSE is for the bestfit line and RMSE for the best model.** ","4d9ba857":"**Conclusion**\n\nIn statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.\n"}}