{"cell_type":{"23559547":"code","35e8624b":"code","105f7219":"code","61164e6b":"code","ed4f1389":"code","8c319fac":"code","1b076dd6":"code","48cad938":"code","bc1b7cc3":"code","d0b37555":"code","8ae67c6b":"code","f5a431d1":"code","ee348caa":"code","b1e291d7":"code","11bae6c2":"code","675789d5":"code","5509017b":"code","9f0ecd64":"code","1037f588":"code","178e9851":"code","3a73b1f0":"code","a5ec6bcf":"code","600d5f21":"code","961a3a45":"code","82b10e1e":"code","aa0523ab":"code","e63c1a49":"code","d70cd098":"code","91a8b3c8":"code","4b80e11a":"code","0d196852":"code","f8cdc64d":"code","e4f48e1b":"code","5ec204c7":"code","a05778a4":"code","f6e892f5":"code","46f1afa2":"code","3ec0585e":"code","5e9847b8":"code","a53833dd":"code","2b6a13c0":"code","4d65cf8d":"code","204e5e88":"code","d162db90":"code","d3a544ce":"code","f71d69cc":"markdown","903d189f":"markdown","eccc862d":"markdown","dac85f68":"markdown","e75aa8d5":"markdown","110fc8c0":"markdown","8867fcc0":"markdown","44c06cee":"markdown","b9935fc7":"markdown","be748240":"markdown","4fd48fd5":"markdown","9b17f9a4":"markdown","81d296e6":"markdown","0e02f5bc":"markdown","e7317c4b":"markdown","29b53106":"markdown","bccdb5b8":"markdown","1ba6dfbf":"markdown","1ea07d96":"markdown","b3cffa66":"markdown","1b4ddd3d":"markdown","c26ada3d":"markdown","473af301":"markdown","6b0331f2":"markdown","0f844f1e":"markdown","cff8d7ad":"markdown","daf61879":"markdown","5f9425e1":"markdown","8d2d0f1f":"markdown","f05030fd":"markdown","bc210f02":"markdown","0d921695":"markdown","75c520fa":"markdown","15318414":"markdown","aac89ed7":"markdown"},"source":{"23559547":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","35e8624b":"#Data Visualization\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline","105f7219":"#Loading Data\ntrain= pd.read_csv(\"..\/input\/train.csv\")\ntest= pd.read_csv(\"..\/input\/test.csv\")\n#Displaying the first five rows of the dataset so as to get a feel of the data.\ntrain.head()","61164e6b":"train.info()","ed4f1389":"train['Target'].value_counts()","8c319fac":"train.describe()","1b076dd6":"test.info()","48cad938":"test.describe()","bc1b7cc3":"#A plot to visualise the Target Distribution.\nsns.countplot('Target',data=train)","d0b37555":"from collections import OrderedDict\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\nplt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'meaneduc', data = train);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Average Schooling by Target')","8ae67c6b":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'overcrowding', data = train);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Overcrowding by Target');","f5a431d1":"#We are doing this because the test doesn't have the Target column.\ntrain2=train.drop('Target',axis=1)","ee348caa":"# Appending the data\ndata = train2.append(test,sort=True)","b1e291d7":"data['dependency'].value_counts()","11bae6c2":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Fill in the values with the correct mapping\ndata['dependency'] = data['dependency'].replace(mapping).astype(np.float64)\ndata['edjefa'] = data['edjefa'].replace(mapping).astype(np.float64)\ndata['edjefe'] = data['edjefe'].replace(mapping).astype(np.float64)\n\ndata[['dependency', 'edjefa', 'edjefe']].describe()","675789d5":"#outlier in test set which rez_esc is 99.0\ndata.loc[data['rez_esc'] == 99.0 , 'rez_esc'] = 5","5509017b":"# Number of missing in each column\nmissing = pd.DataFrame(data.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(data)\n\nmissing.sort_values('percent', ascending = False).head(10)","9f0ecd64":"data['v18q1'] = data['v18q1'].fillna(0)\n\ndata.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\ndata['v2a1-missing'] = data['v2a1'].isnull()\n\ndata.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","1037f588":"#electricity columns\nelec = []\n\nfor i, row in data.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \ndata['elec'] = elec\ndata['elec-missing'] = data['elec'].isnull()","178e9851":"#remove already present electricity columns\ndata = data.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])\n","3a73b1f0":"#walls ordinal\ndata['walls'] = np.argmax(np.array(data[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)\ndata = data.drop(columns = ['epared1', 'epared2', 'epared3'])","a5ec6bcf":"#roof ordinal\ndata['roof'] = np.argmax(np.array(data[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\ndata = data.drop(columns = ['etecho1', 'etecho2', 'etecho3'])","600d5f21":"#floor ordinal\ndata['floor'] = np.argmax(np.array(data[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\ndata = data.drop(columns = ['eviv1', 'eviv2', 'eviv3'])","961a3a45":"#Flushing system\ndata['flush'] = np.argmax(np.array(data[[\"sanitario1\",'sanitario5', 'sanitario2', 'sanitario3',\"sanitario6\"]]),\n                           axis = 1)\ndata = data.drop(columns = [\"sanitario1\",'sanitario5', 'sanitario2', 'sanitario3',\"sanitario6\"])","82b10e1e":"#Drop columns with squared variables\ndata = data[[x for x in data if not x.startswith('SQB')]]\ndata = data.drop(columns = ['agesq'])","aa0523ab":"#waterprovision\ndata['waterprovision'] = np.argmax(np.array(data[['abastaguano', 'abastaguafuera', 'abastaguadentro']]),\n                           axis = 1)\ndata = data.drop(columns = ['abastaguano', 'abastaguafuera', 'abastaguadentro'])","e63c1a49":"#Education Level\ndata['inst'] = np.argmax(np.array(data[[c for c in data if c.startswith('instl')]]), axis = 1)\ndata = data.drop(columns = [c for c in data if c.startswith('instlevel')])\n","d70cd098":"#cooking\ndata['waterprovision'] = np.argmax(np.array(data[['energcocinar1','energcocinar4', 'energcocinar3', 'energcocinar2']]),\n                           axis = 1)\ndata = data.drop(columns = ['energcocinar1','energcocinar4', 'energcocinar3', 'energcocinar2'])","91a8b3c8":"#meaneduc is defined as average years of education for adults (18+)\ndata.loc[pd.isnull(data['meaneduc']), 'meaneduc'] = data.loc[pd.isnull(data['meaneduc']), 'escolari']","4b80e11a":"train2=data.iloc[0:9557,:]\ntest2=data.iloc[9557:33413,:]","0d196852":"test2.drop(['Id','idhogar'],axis=1,inplace=True)","f8cdc64d":"X=train2.drop(['Id','idhogar'],axis=1)","e4f48e1b":"y=train['Target']","5ec204c7":"import xgboost as xgb # Importing XGboost Library","a05778a4":"xg=xgb.XGBClassifier(n_estimators=200)","f6e892f5":"xg.fit(X,y)","46f1afa2":"preds = xg.predict(test2)","3ec0585e":"def macro_f1_score(\n    \n    \n    labels, predictions):\n    # Reshape the predictions as needed\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","5e9847b8":"# Libraries for LightGBM\nimport lightgbm as lgb\nimport sklearn.model_selection as model_selection\nfrom sklearn.metrics import f1_score, make_scorer","a53833dd":"lgmodel = lgb.LGBMClassifier(metric = \"\",num_class = 4)","2b6a13c0":" hyp_OPTaaS = { 'boosting_type': 'dart',\n              'colsample_bytree': 0.9843467236959204,\n              'learning_rate': 0.11598629586769524,\n              'min_child_samples': 44,\n              'num_leaves': 49,\n              'reg_alpha': 0.35397370408131534,\n              'reg_lambda': 0.5904910774606467,\n              'subsample': 0.6299872254632797,\n              'subsample_for_bin': 60611}\n","4d65cf8d":"model = lgb.LGBMClassifier(**hyp_OPTaaS, class_weight = 'balanced',max_depth=-1,objective = 'multiclass', n_jobs = -1, n_estimators = 100)","204e5e88":"model.fit(X, y)","d162db90":"pred=model.predict(test2)","d3a544ce":"my_submission = pd.DataFrame({'Id': test.Id, 'Target': pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","f71d69cc":"From the above plot we can conclude that the data is unbalanced in nature.","903d189f":"# Importing Libraries","eccc862d":"Custom Evaluation Metric","dac85f68":"<font size=\"4\">This is where the actual fun begins. We start off by importing all the libraries that we will need later on. We will be using Numpy and pandas for data analysis and matplotlib (Matlab for python), seaborn for data visualisation.<\/font>","e75aa8d5":"# LightGBM","110fc8c0":"# Modeling with XGboost and LightGBM","8867fcc0":"To check the count based on groups of income levels from the Target Variable","44c06cee":"This gives us the no of rows and columns as well as the data types present.","b9935fc7":"Now we perform the same for the test as well.","be748240":"As you can tell we have one column less than the training dataset. This is because of the absence of the 'Target' column which is what we are gonna be predicting.","4fd48fd5":"<h1> Objective<\/h1>","9b17f9a4":"That gives us a look at all of the columns which don't appear to be in any order. To get a quick overview of the data we use  .info()","81d296e6":"<h1> Problem<\/h1>","0e02f5bc":"One of the most basic yet important step in EDA is to find the missing values in the data.","e7317c4b":"<font size=\"4\">The core Data Fields are as follows:\n* Id - a unique identifier for each row.<br>\n* Target - the target is an ordinal variable indicating groups of income levels.<br>\n    * 1 = extreme poverty <br>\n    * 2 = moderate poverty <br>\n    * 3 = vulnerable households <br> \n    * 4 = non vulnerable households <br>\n* idhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.<br>\n* parentesco1 - indicates if this person is the head of the household.<br>\n* This data contains 142 total columns.<br>\n    <\/font>","29b53106":"Outliers are the values which are really from the distribution of the data. We have to remove these outliers as they affect our Model. There is only one outlier in this data i.e on the rez_esc column and acorrding to the answer from competition host(https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403), we can safely change the value to 5.","bccdb5b8":"Splitting the data","1ba6dfbf":"This gives us the statistical summary of the train dataset.","1ea07d96":"Here we are gonna use the two best classification models but for the final submission we will use LightGBM as it produces a better score ","b3cffa66":"<font size=\"4\">This is a supervised multi-class classification machine learning problem.<\/font>","1b4ddd3d":"# Outliers","c26ada3d":"1. From the above information we can see that the dependancy column has yes and no values.  For this we map the 1's to yes and 0's to no. ","473af301":"# XGBoost","6b0331f2":"We examined how education affected the poverty level of the household. We have a feature called \u201cmeaneduc\u201d which is the average amount of education in the family. When we plot this feature against the Target variable we can see that the families the least at risk for poverty\u200a tend to have higher education levels.","0f844f1e":"The data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty.","cff8d7ad":"The above value displays all the missing values in the data. Now we need to fill this with appopriate values that are derived from a concrete hypothesis.","daf61879":"<font size=\"4\">As how the norm goes we will be training our data on the train dataset and test our model against the test dataset. The Kernel is divided into three major parts<\/font>","5f9425e1":"Let's have a look at the dependancy rate column.","8d2d0f1f":"# Part I : Exploratory Data Analysis","f05030fd":"The Household size and how it affected the poverty level of a household was also examined. There is a feature called \u201covercrowding\u201d which is basically depicts high person per room ratio. This feature was plotted against the Target variable and the resulting plot established the fact that larger the household size the more susceptible it is to poverty.","bc210f02":"Now we are gonna combine the test and train dataset as this way we can reduce the redundancy of performing the same operations of the train on the test dataset. We will separate them after we clean the data.","0d921695":"<font size=\"4\">The objective is to predict poverty on a household level i.e the Target Variable. <\/font>","75c520fa":"# Missing Values","15318414":"# Submission","aac89ed7":"Assigning the X which are the features and y which is our Target."}}