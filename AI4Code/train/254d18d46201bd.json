{"cell_type":{"2e7ceb48":"code","90913e6e":"code","44b7cb5a":"code","eea32831":"code","6cb0f2e1":"code","1c2f8d47":"code","b8ed9bcb":"code","dc90fc7b":"code","8ee8f200":"code","b2784a62":"code","73ab399e":"code","51a90bfc":"code","0bc13ac0":"code","6a1c02df":"code","46966f64":"code","5fc3de39":"code","0ed9f5b1":"code","e2903ce0":"code","5d3b94ce":"code","1d0aa68e":"code","b6d097fd":"code","0c82842c":"code","697815aa":"code","29787143":"code","b1fd7d60":"code","abbeaa01":"code","84c98572":"code","e75fc8e2":"code","315524bb":"code","79b6a585":"code","e46d02cc":"code","529b5c33":"markdown","2026bb27":"markdown","e3be9069":"markdown","cfaeeaf0":"markdown","b9bc7b1c":"markdown","2f25ab71":"markdown","800835f2":"markdown","02537e01":"markdown","03aaa3d8":"markdown","d8fc0aba":"markdown","a79a4fb4":"markdown","94b038f8":"markdown","3fb0e83d":"markdown","68f8c556":"markdown","fec325d3":"markdown","689e1e90":"markdown","6bdd28dd":"markdown","2a0b90f7":"markdown","23e83d1f":"markdown","171767e3":"markdown","afe59fa8":"markdown","03a4e11b":"markdown"},"source":{"2e7ceb48":"import pandas as pd\nimport numpy as np\n\n#tqdm for progress bars\nfrom tqdm import tqdm\n\n#scikit-learn library\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\n\n#gradient boosting\nimport xgboost as xgb\n\n#keras library\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM,GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import GlobalMaxPooling1D,Conv1D,MaxPooling1D,Flatten,Bidirectional,SpatialDropout1D,Embedding\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n#nltk library\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords","90913e6e":"#ignore the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","44b7cb5a":"#gensim library allow us to access pre trained embeddings\nfrom gensim.models.word2vec import Word2Vec\nimport gensim.downloader as api","eea32831":"#download the dataset and return it as object\nmodel_twitter_glove = api.load(\"glove-twitter-25\") #here 25 is dimenssion of the data","6cb0f2e1":"#let's see which are top 10 most similar words to apple.\nmodel_twitter_glove.wv.most_similar(\"apple\",topn=10)","1c2f8d47":"#Let's get fruit this time\nmodel_twitter_glove.wv.most_similar(\"pineapple\",topn=5)","b8ed9bcb":"model_twitter_glove.wv.most_similar(\"politics\",topn=5)","dc90fc7b":"model_twitter_glove.wv.doesnt_match([\"car\",\"truck\",\"bike\",\"orange\"])","8ee8f200":"# now let's try that king and queen example\nmodel_twitter_glove.wv.most_similar(positive=['king','woman'],negative=['man'])","b2784a62":"#loading data\nPATH = '..\/input\/spooky-author-identification'\ntrain = pd.read_csv(f'{PATH}\/train.zip')\ntest = pd.read_csv(f'{PATH}\/test.zip')\nsample = pd.read_csv(f'{PATH}\/sample_submission.zip')\n\n\n#data preprocssing\nencoder = preprocessing.LabelEncoder()\ny = encoder.fit_transform(train[\"author\"].values)\n\n#data split\nX_train, X_test, y_train, y_test = train_test_split(train.text.values,y,random_state=42,test_size=0.1,shuffle=True)","73ab399e":"#our loss function\ndef multiclass_logloss(actual,predicted,eps=1e-15):\n    \n    #converting the 'actual' values to binary values if it's \n    #not binary values\n    \n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0],predicted.shape[1]))\n        \n        for i, val in enumerate(actual):\n            actual2[i,val] = 1\n        actual = actual2\n    \n    #clip function truncates the number between\n    #a max number and min number\n    clip = np.clip(predicted,eps,1-eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0\/ rows * vsota ","51a90bfc":"#we need to word, vec dictionary before fitting it to models\nembedding_index = {}\n\nall_words = list(model_twitter_glove.wv.vocab.keys())\n#words in gensim model is stored as \"key\":vector object pair\n\nfor word in all_words:\n    embedding_index[word] = model_twitter_glove.wv.get_vector(word)\n\nprint('Total words in embeddings %d' % len(embedding_index))","0bc13ac0":"#getting stop words from nltk library\nstop_words = stopwords.words('english')\n\ndef sen2vec(s):\n    # lowe the letters, tokenize them , remove stop_words, remove numbers\n    words = str(s).lower()\n    words = word_tokenize(s)\n    words = [w for w in words if w not in stop_words]\n    words = [w for w in words if w.isalpha()]\n    \n    M = []\n    for w in words:\n        #try because word might not present in index.\n        try:\n            M.append(embedding_index[w])\n        except:\n            continue\n    \n    M = np.array(M)\n    v = M.sum(axis=0)\n    \n    if type(v) != np.ndarray:\n        #25 because that is dimension of out word embedding\n        return np.zeros(25)\n    \n    return v\/np.sqrt((v** 2).sum())","6a1c02df":"#tqdm is libray which shows you progress bar\n#you can write this code without it\n\n#converting every sentence to word embedding\nX_train_glove = [sen2vec(s) for s in tqdm(X_train)]\nX_test_glove = [sen2vec(s) for s in tqdm(X_test)]","46966f64":"X_train_glove = np.array(X_train_glove)\nX_test_glove = np.array(X_test_glove)","5fc3de39":"clf = xgb.XGBClassifier(n_estimators=200,nthread=10,silent=False)\nclf.fit(X_train_glove, y_train)\n\npredictions = clf.predict_proba(X_test_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(y_test, predictions))","0ed9f5b1":"#scaling data\nscl = preprocessing.StandardScaler()\n\nX_train_glove_scl = scl.fit_transform(X_train_glove)\nX_test_glove_scl = scl.transform(X_test_glove)\n\ny_train_enc = np_utils.to_categorical(y_train)\ny_test_enc = np_utils.to_categorical(y_test)","e2903ce0":"model = Sequential()\n\nmodel.add(Dense(25,input_dim=25,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(25,input_dim=25,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',optimizer='adam')","5d3b94ce":"#Just 5 epochs for testing\nmodel.fit(X_train_glove_scl,y=y_train_enc,batch_size=50,epochs=5,verbose=1,validation_data=(X_test_glove_scl,y_test_enc))","1d0aa68e":"#use keras tokenizer\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(X_train)+list(X_test))\nX_train_sec = token.texts_to_sequences(X_train)\nX_test_sec = token.texts_to_sequences(X_test)\n\nX_train_pad = sequence.pad_sequences(X_train_sec,maxlen=max_len)\nX_test_pad = sequence.pad_sequences(X_test_sec,maxlen=max_len)\n\nword_index = token.word_index","b6d097fd":"model = Sequential()\n# we are not using pretrainde embedding yet.\nmodel.add(Embedding(len(word_index)+1,25,input_length=max_len))\nmodel.add(SimpleRNN(100))\nmodel.add(Dense(3))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam')","0c82842c":"model.fit(X_train_pad,y=y_train_enc,epochs=5,batch_size=100,validation_data=(X_test_pad,y_test_enc))","697815aa":"embedding_matrix = np.zeros((len(word_index)+1,25)) #25 because we have word vector of dim 25\n\nfor word,i in word_index.items():\n    #we use get() so it returns None if word is not found\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","29787143":"model = Sequential()\n# we are not using pretrainde embedding yet.\nmodel.add(Embedding(len(word_index)+1,25,\n                    weights=[embedding_matrix],\n                    input_length=max_len,\n                    trainable=False))\n#as the weight are predefined trainable is False\nmodel.add(SimpleRNN(100))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam')","b1fd7d60":"model.fit(X_train_pad,y=y_train_enc,epochs=5,batch_size=100,validation_data=(X_test_pad,y_test_enc))","abbeaa01":"model = Sequential()\n\nmodel.add(Embedding(len(word_index)+1,25,\n                    weights=[embedding_matrix],\n                    input_length=max_len,\n                    trainable=False))\n\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100,dropout=0.3,recurrent_dropout=0.3))\n\nmodel.add(Dense(1024,activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024,activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n","84c98572":"model.fit(X_train_pad,y=y_train_enc,batch_size=100,epochs=10,verbose=1,\n          validation_data=(X_test_pad,y_test_enc))","e75fc8e2":"model = Sequential()\n\nmodel.add(Embedding(len(word_index)+1,25,\n                    weights=[embedding_matrix],\n                    input_length=max_len,\n                    trainable=False))\n\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(100))\n\nmodel.add(Dense(1024,activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024,activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n","315524bb":"model.fit(X_train_pad,y=y_train_enc,batch_size=100,epochs=5,verbose=1,validation_data=(X_test_pad,y_test_enc))","79b6a585":"model = Sequential()\n\nmodel.add(Embedding(len(word_index)+1,25,\n                    weights=[embedding_matrix],\n                    input_length=max_len,\n                    trainable=False))\n\nmodel.add(Bidirectional(LSTM(25, dropout=0.3, recurrent_dropout=0.3)))\n    \nmodel.add(Dense(1024,activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024,activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n","e46d02cc":"model.fit(X_train_pad,y=y_train_enc,batch_size=100,epochs=5,verbose=1,validation_data=(X_test_pad,y_test_enc))","529b5c33":"## Word Vectors\n\nI was not sure what I am getting myself into when I started to understand word vectors<br\/>\nI thought that i will read an article or 2 and that's it I will get it. But  it took me 2 hours<br\/>\nof reading and watching videos to get my head around it. So tie your seat bealt we are going on a rolercoster ride.\n\n\n**Word Vectors** as the name suggest is representaion of the words in form of vector(sequence of numbers).\n\nA simplest form of word vector is one-hot encode or 1-of-N vector which work like this<br\/>\nsuppose that we have 10,000 words and we want to give unique vector<br\/>\nto each word. To do that we take array of 10,000 columns and assign the value 1 to the location of word and <br\/>\nvalue zero to rest of the columns and we do that for each word.\n\nSuppose we have words like \"a\", \"abjure\", \"abjudicate\", \"anoy\" (I don't know why I choose these words!)<br\/>\nthe vector for word \"a\" is (1,0,0,0) and for \"abjure\" is (0,1,0,0) for \"abjudicate\" (0,0,1,0) and for \"anoy\"<br\/>\n(0,0,0,1). \n\nThis is the most simple representation of the words to vectors but, there are two cons of representing words like this\n\n1. If we have too many words say like million words vector will be too big.\n2. This kind of representation does not give information about context of the word.\n\nTo understand second point let us consider this sentence \"I like to play with my pet ______\"<br\/>\ntwo possible answer to the blank in the sentence are \"cat\" and \"dog\".<br\/>\nSo that means that words \"cat\" and \"dog\" are of similar contex and that is the context of pet.<br\/>\n\nSo to solve this problem there are special kind of **word vectors** which has been given an<br\/>\nfancy name **word embeddings**?\n\nIdea of word embeddings is to represent the word vector in such a way that two word in similar<br\/>\ncontext must have similar values in their vectors so distance (euclidean or cosine)  between those vector<br\/>\nis small.\n\nfor ex suppose the word \"dog\" has vector of values => (1.00,-0.333,.9,2.44) then word cat shoud have values => (1.01,-0.34,3,3)<br\/>\nas you can see numbers in those vectors are close.<br\/>\n**Note** the dimension of the vectors are chosen arbitarily.\n\nBut there are several question that arises looking at those vectors.<br\/>\n\n1. How do we get such vectors ?\n2. What is dimension of these vector?\n3. How does it solves problem of having smaller dimension?\n\nAnswer to first question is where Neural Networks comes into the show<br\/>\n\nConsider that we have 10,000 words and we want to vectorize those words, for that we will create a Neural Netowrk<br\/>\nthat has 10,000 input layers, 300 (arbitary selection) hidden layers and again 10,000 output layers which will <br\/>\npredict next word in the sentence.\n\n![image](https:\/\/miro.medium.com\/max\/2800\/0*3DFDpaXoglalyB4c.png)\n\nNow as we train this feed foreword Neural Network, on let's say millions of sentences which contains those 10,000 words<br\/>\nand try to predict next word. A suprising and amazing thing happens, that is as the hidden layer in between the NN is smaller (300)<br\/>\ncompared to 10,000, the information of those 10,000 words gets compresed in that hidden layer and another suprising thing <br\/>\nthat happens is that when a words like \"dog\" and \"cat\" are multiplied to that hidden layer we get vector (word embedding)<br\/>\nwhich have similar values (values with less distance) and that solves our problem of getting context out of simple words<br\/>\nand we also solved the problem of large dimenssion of the vector. \n\nSo answer to the above 3 questions are.\n1. We use Neural Network to get such vector.\n2. Dimension of these vector is dimension of hidden layer.\n3. Size of hidden layer is much smaller comopared to total no of words.\n\nNow, another amazing thing that this hidden layer does.It allows us to add and subtract context from the words.<br\/>\nSo what does that mean? \n\nSuppose we take word vector of \"king\" , \"man\" and \"woman\"<br\/>\nnow if we calculate vector \"king\" - \"man\" + \"women\" ans we get is very close to word \"Queen\".<br\/> \nThat's the power of this NN base Word embeddings.\na\nIt shows us that what \"king\" is to \"queen\" is \"man\" is to \"woman\".\nand also what \"car\" is to \"cars\" is \"dog\" to \"dogs\". \n\nNow the question is how do we train such a NN,actually we don't need to do that<br\/>\nand that is because lot of people has done it for us and other is<br\/>\nwe require huge data and machine power to train excellent Word embedding NN.\n\nThe two Most famous Word embedding techniques are word2vec and GloVe.<br\/>\nWord2vec is exactly what is explained in above explanation of NN.<br\/>\nGloVe is slightly different technique. GloVe uses Decomposition along with NN.\n\nAs explaining both the techniques is out of the scope of my writing skills and intellect<br\/>\nI will link some papers along with every thing I read and saw in below links.\n\nThese are some papers.<br\/>\n[Efficient Estimation of Word Representations in Vector Space](https:\/\/arxiv.org\/pdf\/1301.3781.pdf) \u2013 Mikolov et al. 2013,<br\/>\n[Distributed Representations of Words and Phrases and their Compositionality](https:\/\/papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) \u2013 Mikolov et al. 2013<br\/>\n[Linguistic Regularities in Continuous Space Word Representations](https:\/\/www.aclweb.org\/anthology\/N13-1090\/) Mikolov et al. 2013<br\/>\n[word2vec Parameter Learning Explained](https:\/\/arxiv.org\/pdf\/1411.2738v3.pdf) \u2013 Rong 2014<br\/>\n[word2vec Explained: Deriving Mikolov et al\u2019s Negative Sampling Word-Embedding Method](https:\/\/arxiv.org\/pdf\/1402.3722v1.pdf)\u2013 Goldberg and Levy 2014\n\nvideos:<br\/>\n[12.1: What is word2vec? - Programming with Text](https:\/\/www.youtube.com\/watch?v=LSS_bos_TPI)<br\/>\n[Word Embeddings](https:\/\/www.youtube.com\/watch?v=5PL0TmQhItY)<br\/>\n[Vectoring Words (Word Embeddings) - Computerphile](https:\/\/www.youtube.com\/watch?v=gQddtTdmG_8)\n\nThis is the best ariticle to understand word vector<br\/>\n[Understanding word vectors](Understanding word vectors)\n\nThis is my overall understanding of word vectors, please comment if I have misinterpret something.<br\/>\nor if my understanding of word vector is compeletly wrong! ;).","2026bb27":"## Pretrained embeddings\n\nWe are goinng to play with some pretrained GloVe embeddings which are provided by genism library.","e3be9069":"Now we will create a matrix which has glove vector at the row number which it is proveded by word_index above","cfaeeaf0":"we can also search for the odd word out.","b9bc7b1c":"### Simple Dense model","2f25ab71":"What we did above is basically getting gist of whole sentence in 25 dim.","800835f2":"## Training models using word embeddings\n\nWe are going to create sentence vectors and then use it in our machine learning models. we are going to use GloVe vector.\nwe are going to use same twitter Glove Vector But we do have better options.\nyou can download one from here http:\/\/www-nlp.stanford.edu\/data\/glove.840B.300d.zip","02537e01":"# Deep Learning\n\nNow we are going to train some neural networks.<br\/>\n\nIf you want an introduction to neural network befor going ahead Here is the link to the videos<br\/>\n1. [But what is a Neural Network? ](https:\/\/www.youtube.com\/watch?v=aircAruvnKk&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=2&t=0s)\n2. [Gradient descent, how neural networks learn](https:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=2)\n3. [What is backpropagation really doing? ]( https:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=2)\n4. [Backpropagation calculus](https:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=2)\n\nHere we are going to built and underdtand this models\n\n* Simple NN\n* Simple RNN's\n* Word Embeddings : Definition and How to get them\n* LSTM's\n* GRU's\n* BI-Directional RNN's","03aaa3d8":"**RNN with pretrained embeddings**","d8fc0aba":"## GRU\n\nGRU's also like LSTM were developed to solve vanishing gradient problem and remembering<br\/>\nlarge sequence of data but GRU have different gete mechanish as compared to LSTM.\n\nTo understand more about GRU watch this videos.\n1. [Illustrated Guide to LSTM's and GRU's: A step by step explanation](https:\/\/www.youtube.com\/watch?v=QciIcRxJvsM) \n2. [RNN W1L09 : Gated Recurrent Unit GRU](https:\/\/www.youtube.com\/watch?v=xSCy3q2ts44)\n\n","a79a4fb4":"we need to tokenize the words before feeding it to RNN","94b038f8":"# Simple RNN \n\nRecurent Neural Network basically is the type of NN which remembers what happend in past.<br\/>\nand based on that memory of past it understands the context.for example, the most famous<br\/>\napplication of RNN is prediction of next word . But how does RNN know what happen before<br\/>\n\nThe answer is architecture of RNN. RNN is provided output of previous stage along with the<br\/>\npresent input and then it is trained on (input + previous output) so it eastablish relationships<br\/>\nbetween series of inputs as oppose to simple NN where inputs are independent.\n\n![image.png](attachment:image.png)\n\n\n\nTo understand RNN in detail go to this links<br\/>\n\n1. [Why use RNN instead of simple NN?](https:\/\/www.quora.com\/Why-do-we-use-an-RNN-instead-of-a-simple-neural-network)\n2. [RNN W1L03 ](https:\/\/www.youtube.com\/watch?v=2E65LDnM2cA&list=PL1F3ABbhcqa3BBWo170U4Ev2wfsF7FN8l)","3fb0e83d":"Basically what we did with keras tokenizer is that we created a number for the word<br\/>\nsimply because neural network can't understand word it understands numbers.\n\nNow we will create a matrix which has glove vector at the row number which it is proveded by word_index above <br\/>\n\n\nNow we have unique number for each sentence.\n\nLet's create simple RNN","68f8c556":"I thought queen will appear at the top of list and I will create an amazing example.<br\/>\nBut apparantly there seems to be some noise in the data, but we got queen at 3rd position.\n\n[here](https:\/\/www.kaggle.com\/ankitswarnkar\/word-embedding-using-glove-vector) is one good notebook on word embeddings.","fec325d3":"Now we need a function which creates normalized vector for whole sentence.","689e1e90":"Ha Ha I really forgot that apple is name of a company too.<br\/>\nI was expecting fruits but it's apparant that people talk less about fruits on twitter. ;)","6bdd28dd":"## LSTM (Long Short Term Memory)\n\nRNN where good at predicting the sequence of word or where they.<br\/>\nRNN performance was really good but there where cons attached to it.<br\/>\n\nRNN are not able to remember long term dependencies, it knows what is going around <br\/>\nbut does not know what happend long back. What I means suppose we are trying to predict next <br\/>\nnext word RNN knows that what happend 5 to 10 words before but, it dosen't have memory of past 1000 words.\n\nAnd another problem with RNN was vanishing gradient. look at [this](https:\/\/www.youtube.com\/watch?v=3Hn_hEPtciQ&list=PL1w8k37X_6L_s4ncq-swTBvKDWnRSrinI&index=8) video.\n\nSo to solve above two problems LSTM comes in, LSTM has capability of remembering long term dependencies<br\/>\nand also solved the problem of vanishing gradient.\n\nSo what is LSTM?\n\nLSTM are basically RNN with gates, if you have watched previous videos on RNN<br\/>\nyou might have notices RNN passes same weight matrix to each next recurrent unit.<br\/>\nHere in LSTM along with weight matrix an another parameter is passed call which regulates<br\/>\nhow much of the information from the previous state should be passed to next state.\n\nTo understand more about LSTM look at this videos.\n\n1. [LSTM Network - Explained](https:\/\/www.youtube.com\/watch?v=QciIcRxJvsM)\n2. [Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)](https:\/\/www.youtube.com\/watch?v=QciIcRxJvsM)\n\n\n","2a0b90f7":"# NLP Challenge (Part-2)\n\nI have challenged my self to learn NLP in one week and this is my second notebook<br\/>\n[Here](https:\/\/www.kaggle.com\/maunish\/nlp-challenge-part-1) is the link to first notebook.<br\/>\n\nIn first notebook we learned about topics like\n1. what is NLP?\n2. Tf (term frequency) and idf (inverse document frequency)\n3. CountVectorizer\n4. TfidfVectorizer and TfidfTransformer\n5. Training Logistic Regression, SVM, XGboost, Navie Bayes, \n6. GridSearch.\n\nIn this second notebook we are going to cover some advance NLP topics like<br\/>\n1. What is Word vectors and word embeddings?\n2. Simple NN\n3. RNN\n4. LSTM\n5. GRU\n6. Bidirectional LSTM","23e83d1f":"This is the end of second notebook next notebook we will learn about more advance stuff and state-of-the-art models.","171767e3":"## XGboost\n\nBefore going for deep learning we will see performance of xgboost","afe59fa8":"# Bidirectional LSTM\n\nSometimes inorder to understand the context of the sequence we need to see things in past as well<br\/>\nas the future for example \"I like Apple they make very good phones\" so inorder to understand what <br\/>\napple here stands for architecture need to look at the word phone and that is what bidirectional LSTM does<br\/>\nit inputs sequence from before and after the current word.\n\nTo know more on Bidirectional Networks watch this<br\/>\n1. [RNN W1L11 : Bidirectional RNN](https:\/\/www.youtube.com\/watch?v=bTXGpATdKRY)\n","03a4e11b":"**pretrained: twitter embeddings**: this embedding is trained on twitter data as the name suggest.<br\/>\nwe need to download the dataset using api."}}