{"cell_type":{"29934b76":"code","c37122e4":"code","01043b89":"code","80162ecd":"code","ad32f3c4":"code","bda09eb1":"code","524190ee":"code","b5176fc2":"code","def1f0ee":"code","57a3bf12":"code","97f15784":"code","dc6285c7":"code","72093057":"code","14aa36bd":"code","060768c9":"code","f2d18a4c":"code","da28fb34":"code","81a9fb91":"code","4fcb9caa":"code","55cf2244":"code","1536838d":"code","94604b65":"code","27702f15":"code","4a7d9e9a":"code","c1b5af7f":"code","abf8a83d":"code","2728f32f":"code","4c6782c2":"code","9bf0a6a9":"code","101c68b4":"code","1f8a8745":"code","042324ea":"code","1f093803":"code","31f4d77f":"code","f93041d7":"code","7ae704fb":"code","0e259fb3":"code","6716a37a":"code","3a15f71f":"code","e57cfcaf":"code","2e1863f3":"code","6377efad":"code","31d017f4":"code","e84843ec":"markdown","41a92338":"markdown","b109aae2":"markdown","62cab474":"markdown","caf84ad7":"markdown","bc97feb6":"markdown","4b82de8a":"markdown","28d6d4f7":"markdown","b325b3cf":"markdown","7d66c70c":"markdown","0443e9eb":"markdown","7dcc3a18":"markdown","773036f6":"markdown","9952df19":"markdown","03c1b344":"markdown"},"source":{"29934b76":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import naive_bayes\nfrom sklearn.svm import SVC\n\nimport os\nprint(os.listdir(\"..\/input\"))","c37122e4":"### Importing the input datasets\n\ntitanic_train = pd.read_csv(\"..\/input\/train.csv\",index_col='PassengerId')\ntitanic_test = pd.read_csv(\"..\/input\/test.csv\",index_col='PassengerId')\ntitanic_gender = pd.read_csv(\"..\/input\/gender_submission.csv\")","01043b89":"titanic_train.sample(n=5)","80162ecd":"titanic_test.head()","ad32f3c4":"# Example output dataset\ntitanic_gender.head()","bda09eb1":"titanic_train.info()\n\n# Age, Cabin and Embarked have null values.\n# Cabin has a lot of null values. After EDA, we can decide whether to drop this column.","524190ee":"titanic_test.info()\n\n# Age, Cabin, Embarked and Fare have null values","b5176fc2":"# Same as above but represented in a heatmap\n\nax1 = plt.figure(figsize=(16,5))\n\nax1.add_subplot(121)\nsns.heatmap(titanic_train.isnull(), yticklabels=False, cmap='viridis')\nplt.title('Train data values')\n\n\nax1.add_subplot(122)\nsns.heatmap(titanic_test.isnull(), yticklabels=False, cmap='viridis')\nplt.title('Test data values')\n\nplt.show()","def1f0ee":"ax2 = plt.figure(figsize=(16,5))\n\nax2.add_subplot(121)\nsns.countplot(x='Survived', data=titanic_train, palette=\"magma\", hue='Sex')\n\nax2.add_subplot(122)\nsns.countplot(x='Survived', data=titanic_train, palette='magma', hue='Pclass')\n\nplt.show()","57a3bf12":"sns.distplot(titanic_train['Age'].dropna(),bins=30, kde=False, color='darkviolet')\nplt.show()","97f15784":"plt.figure(figsize=(16,5))\nsns.countplot(x='Survived', hue='SibSp', data=titanic_train, palette='magma')\nplt.show()","dc6285c7":"plt.figure(figsize=(16,5))\nsns.countplot(x='Survived', hue='Embarked', data=titanic_train, palette='magma')\nplt.show()","72093057":"plt.figure(figsize=(12,8))\ntitanic_train.plot(x='Fare',y='Survived', kind='scatter')\nplt.show()","14aa36bd":"titanic_train['Sex'] = titanic_train['Sex'].apply(lambda i: 0 if i == 'male' else 1)\ntitanic_test['Sex'] = titanic_test['Sex'].apply(lambda i: 0 if i == 'male' else 1)","060768c9":"titanic_train.head()","f2d18a4c":"dummy_Embarked = pd.get_dummies(titanic_train.Embarked, prefix='Embarked')\ndummy_Embarked.drop(dummy_Embarked.columns[0], axis=1, inplace=True)\ntitanic_train = pd.concat([titanic_train, dummy_Embarked], axis=1)\ntitanic_train.drop(['Embarked'],axis=1, inplace=True)","da28fb34":"titanic_train.head()","81a9fb91":"dummy_Embarked = pd.get_dummies(titanic_test.Embarked, prefix='Embarked')\ndummy_Embarked.drop(dummy_Embarked.columns[0], axis=1, inplace=True)\ntitanic_test = pd.concat([titanic_test, dummy_Embarked], axis=1)\ntitanic_test.drop(['Embarked'],axis=1, inplace=True)","4fcb9caa":"titanic_test.head()","55cf2244":"plt.figure(figsize=(12, 8))\nsns.boxplot(x='Pclass',y='Age',data=titanic_train,palette='magma')","1536838d":"def age_null(cols):\n    age = cols[0]\n    pclass = cols[1]\n    \n    if pd.isnull(age):\n        if pclass == 3:\n            return titanic_train.Age[titanic_train['Pclass'] == 3].dropna().mean()\n        elif pclass == 2:\n            return titanic_train.Age[titanic_train['Pclass'] == 2].dropna().mean()\n        else:\n            return titanic_train.Age[titanic_train['Pclass'] == 1].dropna().mean()\n    else:\n        return age","94604b65":"titanic_train['Age'] = titanic_train[['Age','Pclass']].apply(age_null, axis=1)\ntitanic_test['Age'] = titanic_test[['Age','Pclass']].apply(age_null, axis=1)","27702f15":"# Same as above but represented in a heatmap\n\nax1 = plt.figure(figsize=(16,5))\n\nax1.add_subplot(121)\nsns.heatmap(titanic_train.isnull(), yticklabels=False, cmap='viridis')\nplt.title('Train data values')\n\n\nax1.add_subplot(122)\nsns.heatmap(titanic_test.isnull(), yticklabels=False, cmap='viridis')\nplt.title('Test data values')\n\nplt.show()","4a7d9e9a":"feature_cols = ['Pclass','Sex','Age','SibSp','Embarked_Q','Embarked_S']","c1b5af7f":"X_train, X_test, y_train, y_test = train_test_split(titanic_train[feature_cols], titanic_train.Survived)","abf8a83d":"logreg = LogisticRegression(solver='liblinear')\nlogreg.fit(X_train, y_train)\ny_logreg = logreg.predict(X_test)","2728f32f":"print (metrics.f1_score(y_test.values, y_logreg))\nprint (metrics.classification_report(y_test.values, y_logreg))","4c6782c2":"dectree = DecisionTreeClassifier(criterion='entropy', max_depth=3)\ndectree.fit(X_train, y_train)\ny_dectree = dectree.predict(X_test)","9bf0a6a9":"print (metrics.f1_score(y_test.values, y_dectree))\nprint (metrics.classification_report(y_test.values, y_dectree))","101c68b4":"randfor = RandomForestClassifier(criterion=\"entropy\", max_depth=5, n_estimators=100)\nrandfor.fit(X_train, y_train)\ny_randfor = randfor.predict(X_test)","1f8a8745":"print (metrics.f1_score(y_test.values, y_randfor))\nprint (metrics.classification_report(y_test.values, y_randfor))","042324ea":"nbayes = naive_bayes.BernoulliNB()\nnbayes.fit(X_train, y_train)\ny_nbayes = nbayes.predict(X_test)","1f093803":"print (metrics.f1_score(y_test.values, y_nbayes))\nprint (metrics.classification_report(y_test.values, y_nbayes))","31f4d77f":"svecto = SVC(kernel='linear')\nsvecto.fit(X_train, y_train)\ny_svecto = svecto.predict(X_test)","f93041d7":"print (metrics.f1_score(y_test.values, y_svecto))\nprint (metrics.classification_report(y_test.values, y_svecto))","7ae704fb":"def addplot(y, pred):\n    sns.heatmap(metrics.confusion_matrix(y,pred), annot=True, cmap='viridis', fmt=\"d\",\n            xticklabels=['Not Survived','Survived'],\n            yticklabels=['Not Survived','Survived'])","0e259fb3":"fig1 = plt.figure(figsize=(14,15))\n\nfig1.add_subplot(3,2,1)\naddplot(y_test, y_logreg)\nplt.title('Logistic Regression: F1 score - %0.2f'% metrics.f1_score(y_test.values, y_logreg))\n\nfig1.add_subplot(3,2,2)\naddplot(y_test, y_dectree)\nplt.title('Decision Tree: F1 score - %0.2f'% metrics.f1_score(y_test.values, y_dectree))\n\nfig1.add_subplot(3,2,3)\naddplot(y_test, y_randfor)\nplt.title('Random Forest: F1 score - %0.2f'% metrics.f1_score(y_test.values, y_randfor))\n\nfig1.add_subplot(3,2,4)\naddplot(y_test, y_nbayes)\nplt.title('Naive Bayes: F1 score - %0.2f'% metrics.f1_score(y_test.values, y_nbayes))\n\nfig1.add_subplot(3,2,5)\naddplot(y_test, y_svecto)\nplt.title('SVM: F1 score - %0.2f'% metrics.f1_score(y_test.values, y_svecto))\n\nplt.show()","6716a37a":"fpr_lg, tpr_lg, _ = metrics.roc_curve(y_test, y_logreg)\nfpr_dt, tpr_dt, _ = metrics.roc_curve(y_test, y_dectree)\nfpr_rf, tpr_rf, _ = metrics.roc_curve(y_test, y_randfor)\nfpr_nb, tpr_nb, _ = metrics.roc_curve(y_test, y_nbayes)\nfpr_sv, tpr_sv, _ = metrics.roc_curve(y_test, y_svecto)\n\nauc_lg = metrics.auc(fpr_lg, tpr_lg)\nauc_dt = metrics.auc(fpr_dt, tpr_dt)\nauc_rf = metrics.auc(fpr_rf, tpr_rf)\nauc_nb = metrics.auc(fpr_nb, tpr_nb)\nauc_sv = metrics.auc(fpr_sv, tpr_sv)","3a15f71f":"plt.title('Receiver Operating Characteristic')\n\nplt.plot(fpr_lg, tpr_lg, 'b', label='LG AUC = %0.3f'% auc_lg)\nplt.plot(fpr_dt, tpr_dt, 'g', label='DT AUC = %0.3f'% auc_dt)\nplt.plot(fpr_rf, tpr_rf, 'y', label='RF AUC = %0.3f'% auc_rf)\nplt.plot(fpr_nb, tpr_nb, 'c', label='NB AUC = %0.3f'% auc_nb)\nplt.plot(fpr_nb, tpr_nb, 'm', label='SV AUC = %0.3f'% auc_sv)\n\nplt.legend(loc='lower right')\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\n\nplt.show()","e57cfcaf":"logreg = LogisticRegression(solver='liblinear')\nlogreg.fit(titanic_train[feature_cols], titanic_train.Survived)\ny_pred = logreg.predict(titanic_test[feature_cols])\n\n#randfor = RandomForestClassifier(criterion=\"entropy\", max_depth=5, n_estimators=100)\n#randfor.fit(titanic_train[feature_cols], titanic_train.Survived)\n#y_pred = randfor.predict(titanic_test[feature_cols])","2e1863f3":"predictions = pd.concat([pd.DataFrame(titanic_test.index, columns=['PassengerId']), pd.DataFrame(y_pred, columns=['Survived'])],\n           axis=1)","6377efad":"predictions.sample(n=5)","31d017f4":"predictions.to_csv('predictions.csv', index=False)","e84843ec":"## Exploratory Data Analysis`","41a92338":"## Decision Tree","b109aae2":"## Naive Bayes","62cab474":"## Import Libraries","caf84ad7":"From the plot, we can infer that infants and people between the ages 20-40 have a higher chance of survival.","bc97feb6":"## Logistic Regression","4b82de8a":"#### Dummy encoding Categorical column 'Embarked'","28d6d4f7":"## Support Vector Machines","b325b3cf":"<b>Plot 1:<\/b> depicts that Cabin column has a lot of null values. With lesser non-null values, predicting the null values would be difficult and also may add false value to the outcome. So we drop this column from our predictions.\n\n<b>Plot 2:<\/b> Age on the other hand, has about 20% missing values only, so we will consider this for the predictions. However, the column needs to be imputed.","7d66c70c":"<b>Plot 1:<\/b> shows that females have a higher survival rate than males<br\/>\n\n<b>Plot 2:<\/b> Class 3 the % not survived is very high\n<p>\nSo definitely, these columns add value to the predictions<\/p>","0443e9eb":"## Predictions","7dcc3a18":"## Import Datasets","773036f6":"## Random Forest ","9952df19":"<b>Logistic Regression Model gives better AUC. Applying the same model on test set to predict the survivors<\/b>","03c1b344":"## Data cleaning\n\n#### Dummy encoding Categorical column 'Sex'"}}