{"cell_type":{"71c0171e":"code","ad46a325":"code","aae0c8a1":"code","7a938fc7":"code","2e86b0e7":"code","003f8989":"code","66dc39df":"code","ebcda1f2":"code","9ddd7924":"code","4048e3df":"code","c1b8497f":"code","51223f14":"code","0da5f9ce":"code","2595be2d":"code","24ada8aa":"code","6a6c7536":"code","dfffbcaa":"code","95d66709":"code","31bf2461":"code","7f0464e6":"code","350100de":"code","96464850":"code","6e447994":"code","e015a033":"code","1e4df313":"code","e8c4347f":"code","99d6afc0":"code","92dcd65e":"code","fbf82409":"code","dad6db3f":"code","6e4f34d5":"code","316e0789":"code","12851587":"code","14df0db2":"code","70906cd5":"code","86e106d0":"code","c2bce3d3":"code","ca91cf50":"code","7112b946":"code","2fe3719f":"code","fac3a755":"code","94dfb05e":"code","d38aeed8":"code","d158b6b6":"code","6ff53d64":"code","0cc305f9":"code","14787468":"code","9420b834":"code","6885abfc":"code","c2ad4719":"code","29fc3dc0":"code","881efbda":"code","4bbcf5e7":"code","4446dfd7":"code","67c0362a":"code","f3f0f133":"code","beef5630":"code","810ae8c5":"code","ebfdba30":"code","10805ab8":"code","1d9df584":"code","a79e1350":"code","316fb4a9":"code","5cfff90a":"code","89cc278e":"code","ccc59ba3":"code","89513ab6":"code","9908103d":"code","7d7a5f2d":"code","d7c9fce7":"code","10cadf3d":"code","46db349f":"code","2e5800a4":"code","5b5e8d32":"code","8db19579":"code","98f588bc":"code","85361779":"code","b7d9f43e":"code","461a9938":"code","be15be65":"code","47723914":"code","8d078caa":"code","6ab427c1":"code","da5bd48b":"code","533eb5fe":"code","e1403056":"markdown","2913f7d5":"markdown","ce99d99a":"markdown","bf2953ec":"markdown","87e92b22":"markdown","28c01f9b":"markdown"},"source":{"71c0171e":"# Import necessary libraries\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport glob\nimport random\nfrom matplotlib.patches import Rectangle\nfrom lxml import etree\n\n# Enable Matplotlib backend for displaying static images\n%matplotlib inline","ad46a325":"# Subfolders for the class labels\n\nimport os\nos.listdir(\"\/kaggle\/input\/defects-class-and-location\/images\/images\")","aae0c8a1":"# Annotations of the class labels\n\nos.listdir(\"\/kaggle\/input\/defects-class-and-location\/label\/label\")","7a938fc7":"# Read all the images and count the number of image paths in the training subfolders\n\nimage_path = glob.glob(\"\/kaggle\/input\/defects-class-and-location\/images\/images\/*\/*.jpg\")\nlen(image_path)","2e86b0e7":"# Read all the annotation files and count the number of annotations in the training label folder\n\nxmls_path = glob.glob(\"\/kaggle\/input\/defects-class-and-location\/label\/label\/*.xml\")\nlen(xmls_path)","003f8989":"# Sort the annotation and image files in ascending order\n\nxmls_path.sort(key = lambda x:x.split(\"\/\")[-1].split(\".xml\")[0])\nimage_path.sort(key = lambda x:x.split(\"\/\")[-1].split(\".jpg\")[0])","66dc39df":"# Extract the annotation filename without file extension\n\nxmls_train = [path.split(\"\/\")[-1].split(\".\")[0] for path in xmls_path]\nxmls_train[:5]","ebcda1f2":"# Retrieve the image filename to match the annotation file counts\n# Ensure unique counts of training images with the annotations\n\nimgs_train = [img for img in image_path if (img.split(\"\/\")[-1].split)(\".jpg\")[0] in xmls_train]\nimgs_train[:5]","9ddd7924":"len(imgs_train),len(xmls_path)","4048e3df":"# Extract label names as DataFrame column\n\nlabels = [label.split(\"\/\")[-2] for label in imgs_train]\nlabels[:5]","c1b8497f":"labels = pd.DataFrame(labels, columns = [\"Defect Type\"])\nlabels","51223f14":"# One-hot encoding for multiple classes\nfrom sklearn.preprocessing import LabelBinarizer\n\n# Obtain training labels without duplication\nClass = labels[\"Defect Type\"].unique()\n# Store data values in key:value pairs with Python dictionaries\nClass_dict = dict(zip(Class, range(1,len(Class) + 1)))\nlabels[\"Class\"] = labels[\"Defect Type\"].apply(lambda x: Class_dict[x])\n\nlb = LabelBinarizer()\n# Fit label binarizer\nlb.fit(list(Class_dict.values()))\n# Convert multi-class labels to binary labels (belong or does not belong to the class)\ntransformed_labels = lb.transform(labels[\"Class\"])\ny_bin_labels = []  \n\nfor i in range(transformed_labels.shape[1]):\n    y_bin_labels.append(\"Class\" + str(i))\n    labels[\"Class\" + str(i + 1)] = transformed_labels[:, i]","0da5f9ce":"Class_dict","2595be2d":"labels.drop(\"Class\", axis = 1, inplace = True)\nlabels.drop(\"Defect Type\", axis = 1, inplace = True)\nlabels.head()","24ada8aa":"# Function to parse and extract information from annotation files\n\ndef to_labels(path):\n    # Read the annotation file\n    xml = open(\"{}\".format(path)).read()                         \n    sel = etree.HTML(xml)\n    # Obtain the image width\n    width = int(sel.xpath(\"\/\/size\/width\/text()\")[0])\n    # Obtain the image height\n    height = int(sel.xpath(\"\/\/size\/height\/text()\")[0])  \n    # Extract the bounding box coordinates\n    xmin = int(sel.xpath(\"\/\/bndbox\/xmin\/text()\")[0])\n    xmax = int(sel.xpath(\"\/\/bndbox\/xmax\/text()\")[0])\n    ymin = int(sel.xpath(\"\/\/bndbox\/ymin\/text()\")[0])\n    ymax = int(sel.xpath(\"\/\/bndbox\/ymax\/text()\")[0])\n    # Return the relative coordinates\n    return [xmin\/width, ymin\/height, xmax\/width, ymax\/height]","6a6c7536":"# Display the relative bounding box coordinates\n\ncoors = [to_labels(path) for path in xmls_path]\ncoors[:5]","dfffbcaa":"# Set four coordinate points as outputs\n\nxmin, ymin, xmax, ymax = list(zip(*coors))\n\n# Convert to Numpy array\nxmin = np.array(xmin)\nymin = np.array(ymin)\nxmax = np.array(xmax)\nymax = np.array(ymax)\nlabel = np.array(labels.values)","95d66709":"# Creates a Dataset whose elements are slices of the given tensors\n# Slicing a 1D tensor produces scalar tensor elements\n\nlabels_dataset = tf.data.Dataset.from_tensor_slices((xmin, ymin, xmax, ymax, label))\nlabels_dataset","31bf2461":"# Load the image from image path\n\ndef load_image(path):\n    image = tf.io.read_file(path)                           \n    image = tf.image.decode_jpeg(image,3)               \n    image = tf.image.resize(image,[224,224])               \n    image = tf.cast(image,tf.float32)  \n    image = image \/ 255\n    return image","7f0464e6":"# Build the dataset\n\ndataset = tf.data.Dataset.from_tensor_slices(imgs_train)\ndataset = dataset.map(load_image)\n\ndataset_label = tf.data.Dataset.zip((dataset, labels_dataset))\ndataset_label","350100de":"# Shuffle the images and extract the images by the defined batch size\n\nbatch_size = 32\ndataset_label = dataset_label.repeat().shuffle(500).batch(batch_size)\n\n# Creates a Dataset that prefetches elements from this dataset\n# Most dataset input pipelines should end with a call to prefetch\n# This allows later elements to be prepared while the current element is being processed\n# This often improves latency and throughput, at the cost of using additional memory to store prefetched elements\n\ndataset_label = dataset_label.prefetch(tf.data.experimental.AUTOTUNE)","96464850":"# Split the dataset into 80% for training and 20% for testing\n\ntrain_count = int(len(imgs_train) * 0.8)\ntest_count = int(len(imgs_train) * 0.2)\ntrain_count, test_count","6e447994":"train_dataset = dataset_label.skip(test_count)\ntest_dataset = dataset_label.take(test_count)","e015a033":"class_dict = {v:k for k,v in Class_dict.items()}\nclass_dict","1e4df313":"# Display a sample training image with its label\n\nfor img, label in train_dataset.take(1):\n    plt.imshow(keras.preprocessing.image.array_to_img(img[0]))     \n    out1, out2, out3, out4, out5 = label                            \n    xmin, ymin, xmax, ymax = out1[0].numpy()*224, out2[0].numpy()*224, out3[0].numpy()*224, out4[0].numpy()*224\n    rect = Rectangle((xmin,ymin),(xmax - xmin),(ymax - ymin), fill = False, color = \"r\")  \n    ax = plt.gca()                      \n    ax.axes.add_patch(rect)   \n    pred_imglist = []\n    pred_imglist.append(class_dict[np.argmax(out5[0])+1])\n    plt.title(pred_imglist)\n    plt.show()","e8c4347f":"# Import VGG16 model with pre-trained weights from ImageNet\n\nbase_vgg16 = tf.keras.applications.VGG16(weights = \"imagenet\",\n                                         include_top = False,\n                                         input_shape = (224,224,3),\n                                         pooling = 'avg')","99d6afc0":"# Train the base model with fully-connected layers\n\nbase_vgg16.trainable = True","92dcd65e":"# Define the base model with fully-connected layers\ninputs = keras.Input(shape = (224,224,3))\nx = base_vgg16(inputs)\n\nx1 = keras.layers.Dense(1024, activation = \"relu\")(x)\nx1 = keras.layers.Dense(512, activation = \"relu\")(x1)\nout1 = keras.layers.Dense(1, name = \"xmin\")(x1)\nout2 = keras.layers.Dense(1, name = \"ymin\")(x1)\nout3 = keras.layers.Dense(1, name = \"xmax\")(x1)\nout4 = keras.layers.Dense(1, name = \"ymax\")(x1)\n\nx2 = keras.layers.Dense(1024,activation = \"relu\")(x)\nx2 = keras.layers.Dropout(0.5)(x2)\nx2 = keras.layers.Dense(512,activation = \"relu\")(x2)\nout_class = keras.layers.Dense(10,activation = \"softmax\", name = \"class\")(x2)\n\nout = [out1, out2, out3, out4, out_class]\n\nvgg16 = keras.models.Model(inputs = inputs, outputs = out)\nvgg16.summary()","fbf82409":"# Compile the model with optimizer and loss functions\n\nvgg16.compile(keras.optimizers.Adam(0.0005),\n              loss = {\"xmin\": \"mse\",\n                      \"ymin\": \"mse\",\n                      \"xmax\": \"mse\",\n                      \"ymax\": \"mse\",\n                      \"class\": \"categorical_crossentropy\"},\n              metrics = [\"mae\",\"acc\"])","dad6db3f":"# Reduce learning rate\n\nlr_reduce = keras.callbacks.ReduceLROnPlateau(\"val_loss\", patience = 5, factor = 0.5, min_lr = 1e-6)","6e4f34d5":"# Start training the model\n\nhistory = vgg16.fit(train_dataset,\n                    steps_per_epoch = train_count\/\/batch_size,\n                    epochs = 200,\n                    validation_data = test_dataset,\n                    validation_steps = test_count\/\/batch_size,\n                    callbacks = [lr_reduce])","316e0789":"# Visualize the results after training\n\ndef plot_history(history):                \n    hist = pd.DataFrame(history.history)           \n    hist[\"epoch\"] = history.epoch\n    \n    plt.figure()                                     \n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Mean Squared Error (MSE)\")               \n    plt.plot(hist[\"epoch\"], hist[\"loss\"], label = \"training Loss\")\n    plt.plot(hist[\"epoch\"], hist[\"val_loss\"], label = \"validation Loss\")                           \n    plt.legend()\n    \n    plt.figure()                                      \n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Mean Absolute Error (MAE)\")               \n    plt.plot(hist[\"epoch\"], hist[\"val_xmin_mae\"], label = \"xmin_MAE\")\n    plt.plot(hist[\"epoch\"], hist[\"val_ymin_mae\"], label = \"ymin_MAE\")\n    plt.plot(hist[\"epoch\"], hist[\"val_xmax_mae\"], label = \"xmax_MAE\")\n    plt.plot(hist[\"epoch\"], hist[\"val_ymax_mae\"], label = \"ymax_MAE\")\n    plt.legend()      \n    \n    plt.figure()                                      \n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy for Class\")               \n    plt.plot(hist[\"epoch\"],hist[\"val_class_acc\"], label = \"class_accuracy\")\n    \n    plt.show()\n    \nplot_history(history)  ","12851587":"results = vgg16.evaluate(test_dataset)","14df0db2":"print(\"MAE of xmin value in test set:{}\".format(results[6]))\nprint(\"MAE of ymin value in test set:{}\".format(results[8]))\nprint(\"MAE of xmax value in test set:{}\".format(results[10]))\nprint(\"MAE of ymax value in test set:{}\".format(results[12]))\nprint(\"Testing accuracy of predicted label:{}\".format(results[15]))","70906cd5":"vgg16.save(\"vgg16.h5\")","86e106d0":"plt.figure(figsize = (10, 24))\nfor img, _ in test_dataset.take(1):\n    out1, out2, out3, out4, label = vgg16.predict(img)\n    for i in range(3):\n        plt.subplot(3, 1, i + 1)            \n        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n        pred_imglist = []\n        pred_imglist.append(class_dict[np.argmax(out5[i]) + 1])\n        plt.title(pred_imglist)\n        xmin, ymin, xmax, ymax = out1[i]*224, out2[i]*224, out3[i]*224, out4[i]*224\n        rect = Rectangle((xmin,ymin), (xmax - xmin), (ymax - ymin), fill = False, color = \"r\") \n        ax = plt.gca()                   \n        ax.axes.add_patch(rect)","c2bce3d3":"# Append all the true labels into a list\n\ntrue_labels = []\n\nfor _, label in test_dataset:    \n    out1, out2, out3, out4, out5 = label                              \n    true_labels.append(class_dict[np.argmax(out5) + 1])","ca91cf50":"true_labels","7112b946":"test_labels = []\n\nfor img, label in test_dataset:\n    out1, out2, out3, out4, out5 = label\n    label = vgg16.predict(img)\n    test_labels.append(class_dict[np.argmax(out5) + 1])","2fe3719f":"test_labels","fac3a755":"target_names = ['cresent_gap', 'crease', 'silk_spot', 'water_spot', 'welding_line', \n                'inclusion', 'oil_spot', 'waist_folding', 'rolled_pit', 'punching_hole']","94dfb05e":"from sklearn.metrics import classification_report\n\nprint(classification_report(true_labels, test_labels, target_names = target_names))","d38aeed8":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(true_labels, test_labels, labels = target_names)","d158b6b6":"# Import Xception model with pre-trained weights from ImageNet\n\nbase_xception = tf.keras.applications.Xception(weights = \"imagenet\",\n                                               include_top = False,\n                                               input_shape = (224,224,3),\n                                               pooling = 'avg')","6ff53d64":"# Train the base model with fully-connected layers\n\nbase_xception.trainable = True","0cc305f9":"# Define the base model with fully-connected layers\ninputs = keras.Input(shape = (224,224,3))\nx = base_xception(inputs)\n\nx1 = keras.layers.Dense(1024, activation = \"relu\")(x)\nx1 = keras.layers.Dense(512, activation = \"relu\")(x1)\nout1 = keras.layers.Dense(1, name = \"xmin\")(x1)\nout2 = keras.layers.Dense(1, name = \"ymin\")(x1)\nout3 = keras.layers.Dense(1, name = \"xmax\")(x1)\nout4 = keras.layers.Dense(1, name = \"ymax\")(x1)\n\nx2 = keras.layers.Dense(1024, activation = \"relu\")(x)\nx2 = keras.layers.Dropout(0.5)(x2)\nx2 = keras.layers.Dense(512, activation = \"relu\")(x2)\nout_class = keras.layers.Dense(10, activation = \"softmax\", name = \"class\")(x2)\n\nout = [out1, out2, out3, out4, out_class]\n\nxception = keras.models.Model(inputs = inputs, outputs = out)\nxception.summary()","14787468":"# Compile the model with optimizer and loss functions\n\nxception.compile(keras.optimizers.Adam(0.0005),\n                 loss = {\"xmin\": \"mse\",\n                         \"ymin\": \"mse\",\n                         \"xmax\": \"mse\",\n                         \"ymax\": \"mse\",\n                         \"class\": \"categorical_crossentropy\"},\n                 metrics = [\"mae\",\"acc\"])","9420b834":"# Start training the model\n\nhistory = xception.fit(train_dataset,\n                       steps_per_epoch = train_count\/\/batch_size,\n                       epochs = 200,\n                       validation_data = test_dataset,\n                       validation_steps = test_count\/\/batch_size,\n                       callbacks = [lr_reduce])","6885abfc":"plot_history(history)  ","c2ad4719":"results = xception.evaluate(test_dataset)","29fc3dc0":"print(\"MAE of xmin value in test set:{}\".format(results[6]))\nprint(\"MAE of ymin value in test set:{}\".format(results[8]))\nprint(\"MAE of xmax value in test set:{}\".format(results[10]))\nprint(\"MAE of ymax value in test set:{}\".format(results[12]))\nprint(\"Testing accuracy of predicted label:{}\".format(results[15]))","881efbda":"xception.save(\"xception.h5\")","4bbcf5e7":"plt.figure(figsize = (10, 24))\nfor img, _ in test_dataset.take(1):\n    out1, out2, out3, out4, label = xception.predict(img)\n    for i in range(3):\n        plt.subplot(3, 1, i + 1)            \n        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n        pred_imglist = []\n        pred_imglist.append(class_dict[np.argmax(out5[i]) + 1])\n        plt.title(pred_imglist)\n        xmin, ymin, xmax, ymax = out1[i]*224, out2[i]*224, out3[i]*224, out4[i]*224\n        rect = Rectangle((xmin,ymin), (xmax - xmin), (ymax - ymin), fill = False, color = \"r\") \n        ax = plt.gca()                   \n        ax.axes.add_patch(rect)","4446dfd7":"test_labels = []\n\nfor img, label in test_dataset:\n    out1, out2, out3, out4, out5 = label\n    label = xception.predict(img)\n    test_labels.append(class_dict[np.argmax(out5) + 1])","67c0362a":"test_labels","f3f0f133":"from sklearn.metrics import classification_report\n\nprint(classification_report(true_labels, test_labels, target_names = target_names))","beef5630":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(true_labels, test_labels, labels = target_names)","810ae8c5":"# Import InceptionResNetV2 model with pre-trained weights from ImageNet\n\nbase_inceptionresnetv2 = tf.keras.applications.InceptionResNetV2(weights = \"imagenet\",\n                                                                 include_top = False,\n                                                                 input_shape = (224,224,3),\n                                                                 pooling = 'avg')","ebfdba30":"# Train the base model with fully-connected layers\n\nbase_inceptionresnetv2.trainable = True","10805ab8":"# Define the base model with fully-connected layers\ninputs = keras.Input(shape = (224,224,3))\nx = base_inceptionresnetv2(inputs)\n\nx1 = keras.layers.Dense(1024, activation = \"relu\")(x)\nx1 = keras.layers.Dense(512, activation = \"relu\")(x1)\nout1 = keras.layers.Dense(1, name = \"xmin\")(x1)\nout2 = keras.layers.Dense(1, name = \"ymin\")(x1)\nout3 = keras.layers.Dense(1, name = \"xmax\")(x1)\nout4 = keras.layers.Dense(1, name = \"ymax\")(x1)\n\nx2 = keras.layers.Dense(1024, activation = \"relu\")(x)\nx2 = keras.layers.Dropout(0.5)(x2)\nx2 = keras.layers.Dense(512, activation = \"relu\")(x2)\nout_class = keras.layers.Dense(10, activation = \"softmax\", name = \"class\")(x2)\n\nout = [out1, out2, out3, out4, out_class]\n\ninceptionresnetv2 = keras.models.Model(inputs = inputs, outputs = out)\ninceptionresnetv2.summary()","1d9df584":"# Compile the model with optimizer and loss functions\n\ninceptionresnetv2.compile(keras.optimizers.Adam(0.0005),\n                          loss = {\"xmin\": \"mse\",\n                                  \"ymin\": \"mse\",\n                                  \"xmax\": \"mse\",\n                                  \"ymax\": \"mse\",\n                                  \"class\": \"categorical_crossentropy\"},\n                          metrics = [\"mae\",\"acc\"])","a79e1350":"# Start training the model\n\nhistory = inceptionresnetv2.fit(train_dataset,\n                                steps_per_epoch = train_count\/\/batch_size,\n                                epochs = 200,\n                                validation_data = test_dataset,\n                                validation_steps = test_count\/\/batch_size,\n                                callbacks = [lr_reduce])","316fb4a9":"plot_history(history) ","5cfff90a":"results = inceptionresnetv2.evaluate(test_dataset)","89cc278e":"print(\"MAE of xmin value in test set:{}\".format(results[6]))\nprint(\"MAE of ymin value in test set:{}\".format(results[8]))\nprint(\"MAE of xmax value in test set:{}\".format(results[10]))\nprint(\"MAE of ymax value in test set:{}\".format(results[12]))\nprint(\"Testing accuracy of predicted label:{}\".format(results[15]))","ccc59ba3":"inceptionresnetv2.save(\"inceptionresnetv2.h5\")","89513ab6":"plt.figure(figsize = (10, 24))\nfor img, _ in test_dataset.take(1):\n    out1, out2, out3, out4, label = inceptionresnetv2.predict(img)\n    for i in range(3):\n        plt.subplot(3, 1, i + 1)            \n        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n        pred_imglist = []\n        pred_imglist.append(class_dict[np.argmax(out5[i]) + 1])\n        plt.title(pred_imglist)\n        xmin, ymin, xmax, ymax = out1[i]*224, out2[i]*224, out3[i]*224, out4[i]*224\n        rect = Rectangle((xmin,ymin), (xmax - xmin), (ymax - ymin), fill = False, color = \"r\") \n        ax = plt.gca()                   \n        ax.axes.add_patch(rect)","9908103d":"test_labels = []\n\nfor img, label in test_dataset:\n    out1, out2, out3, out4, out5 = label\n    label = inceptionresnetv2.predict(img)\n    test_labels.append(class_dict[np.argmax(out5) + 1])","7d7a5f2d":"test_labels","d7c9fce7":"from sklearn.metrics import classification_report\n\nprint(classification_report(true_labels, test_labels, target_names = target_names))","10cadf3d":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(true_labels, test_labels, labels = target_names)","46db349f":"# Import InceptionResNetV2 model with pre-trained weights from ImageNet\n\nbase_resnet152v2 = tf.keras.applications.ResNet152V2(weights = \"imagenet\",\n                                                     include_top = False,\n                                                     input_shape = (224,224,3),\n                                                     pooling = 'avg')","2e5800a4":"# Train the base model with fully-connected layers\n\nbase_resnet152v2.trainable = True","5b5e8d32":"# Define the base model with fully-connected layers\ninputs = keras.Input(shape = (224,224,3))\nx = base_resnet152v2(inputs)\n\nx1 = keras.layers.Dense(1024, activation = \"relu\")(x)\nx1 = keras.layers.Dense(512, activation = \"relu\")(x1)\nout1 = keras.layers.Dense(1, name = \"xmin\")(x1)\nout2 = keras.layers.Dense(1, name = \"ymin\")(x1)\nout3 = keras.layers.Dense(1, name = \"xmax\")(x1)\nout4 = keras.layers.Dense(1, name = \"ymax\")(x1)\n\nx2 = keras.layers.Dense(1024, activation = \"relu\")(x)\nx2 = keras.layers.Dropout(0.5)(x2)\nx2 = keras.layers.Dense(512, activation = \"relu\")(x2)\nout_class = keras.layers.Dense(10, activation = \"softmax\", name = \"class\")(x2)\n\nout = [out1, out2, out3, out4, out_class]\n\nresnet152v2 = keras.models.Model(inputs = inputs, outputs = out)\nresnet152v2.summary()","8db19579":"# Compile the model with optimizer and loss functions\n\nresnet152v2.compile(keras.optimizers.Adam(0.0005),\n                    loss = {\"xmin\": \"mse\",\n                            \"ymin\": \"mse\",\n                            \"xmax\": \"mse\",\n                            \"ymax\": \"mse\",\n                            \"class\": \"categorical_crossentropy\"},\n                    metrics = [\"mae\",\"acc\"])","98f588bc":"# Start training the model\n\nhistory = resnet152v2.fit(train_dataset,\n                          steps_per_epoch = train_count\/\/batch_size,\n                          epochs = 200,\n                          validation_data = test_dataset,\n                          validation_steps = test_count\/\/batch_size,\n                          callbacks = [lr_reduce])","85361779":"plot_history(history)","b7d9f43e":"results = resnet152v2.evaluate(test_dataset)","461a9938":"print(\"MAE of xmin value in test set:{}\".format(results[6]))\nprint(\"MAE of ymin value in test set:{}\".format(results[8]))\nprint(\"MAE of xmax value in test set:{}\".format(results[10]))\nprint(\"MAE of ymax value in test set:{}\".format(results[12]))\nprint(\"Testing accuracy of predicted label:{}\".format(results[15]))","be15be65":"resnet152v2.save(\"resnet152v2.h5\")","47723914":"plt.figure(figsize = (10, 24))\nfor img, _ in test_dataset.take(1):\n    out1, out2, out3, out4, label = resnet152v2.predict(img)\n    for i in range(3):\n        plt.subplot(3, 1, i + 1)            \n        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n        pred_imglist = []\n        pred_imglist.append(class_dict[np.argmax(out5[i]) + 1])\n        plt.title(pred_imglist)\n        xmin, ymin, xmax, ymax = out1[i]*224, out2[i]*224, out3[i]*224, out4[i]*224\n        rect = Rectangle((xmin,ymin), (xmax - xmin), (ymax - ymin), fill = False, color = \"r\") \n        ax = plt.gca()                   \n        ax.axes.add_patch(rect)","8d078caa":"test_labels = []\n\nfor img, label in test_dataset:\n    out1, out2, out3, out4, out5 = label\n    label = resnet152v2.predict(img)\n    test_labels.append(class_dict[np.argmax(out5) + 1])","6ab427c1":"test_labels","da5bd48b":"from sklearn.metrics import classification_report\n\nprint(classification_report(true_labels, test_labels, target_names = target_names))","533eb5fe":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(true_labels, test_labels, labels = target_names)","e1403056":"**ii. Xception**","2913f7d5":"**References:**\n1. https:\/\/www.fatalerrors.org\/a\/tensorflow-learning-notes-no.10.html\n2. https:\/\/valueml.com\/steel-surface-inspection-in-keras-python\/\n3. https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#from_tensor_slices\n4. https:\/\/github.com\/tensorflow\/tensorflow\/issues\/14857\n5. https:\/\/stackoverflow.com\/questions\/56613155\/tensorflow-tf-data-autotune\n6. https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#prefetch\n7. https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/cast\n8. https:\/\/realpython.com\/python-map-function\/\n9. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelBinarizer.html\n10. https:\/\/stackoverflow.com\/questions\/50473381\/scikit-learns-labelbinarizer-vs-onehotencoder","ce99d99a":"**iii. InceptionResNetV2**","bf2953ec":"**iv. ResNet152V2**","87e92b22":"**i. VGG16**","28c01f9b":"# Project Description - Automatic Defects Classification for Steel Defects in Manufacturing\n\n**Introduction** <br>\nThis is a project of defects classification for steel defects on the GC10-DET surface dataset using transfer learning, which the model is pre-trained on ImageNet dataset. A few models such as VGG16, Xception, InceptionResNetV2 and ResNet152V2 will be used for training the dataset and provide comparison between them. \n\n**Outline** <br>\nThis project will have the presentation outlined as follows,\n1. Dataset visualization\n2. Model training and validation\n3. Model testing\n4. Results evaluation\n5. Model prediction\n6. Conclusion and acknowledgements"}}