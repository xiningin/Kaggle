{"cell_type":{"2c024b27":"code","ef440548":"code","cbfee2ad":"code","77d761ab":"code","b29ec20a":"code","3392b997":"code","956deb58":"code","82c0a03d":"code","0cb8ffed":"code","0f3522cb":"code","fa0fe6e2":"code","af719e83":"code","0fafbf2a":"code","fe2f6b50":"code","91148976":"markdown","ffaad3c7":"markdown","6e885017":"markdown","90335e3b":"markdown","8c04ce09":"markdown","66518f00":"markdown","405acb1d":"markdown","d8e583ab":"markdown","1ed448ed":"markdown","3316f0bc":"markdown","3c53eb60":"markdown"},"source":{"2c024b27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef440548":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp","cbfee2ad":"dat = pd.read_csv('..\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')\ndat.describe()","77d761ab":"pp.ProfileReport(dat)","b29ec20a":"# drop duplicate rows\ndf = dat.drop_duplicates()\ndf.describe()","3392b997":"# define target and predictors\n# np.random.seed(12345)\n# df = df.reindex(np.random.permutation(df.index))\ny = df['target']\nX = df.drop(['target'],axis=1)\n\nprint(X.info())","956deb58":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, classification_report\nfrom sklearn.metrics import plot_roc_curve, plot_confusion_matrix, plot_precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","82c0a03d":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","0cb8ffed":"def model_fit_summarize(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    y = model.predict(X_test)\n\n    confmat = confusion_matrix(y_test,y)\n    acc = accuracy_score(y_test, y)*100\n    pre = precision_score(y_test, y)*100\n    rec = recall_score(y_test, y)*100\n    roc_auc = roc_auc_score(y_test, y)*100\n\n#     print('Confusion Matrix :\\n',confmat)\n    print('Accuracy score : {:.2f} %'.format(acc))\n    print('Precision score : {:.2f} %'.format(pre))\n    print('Recall score : {:.2f} %'.format(rec))\n    print('ROC_AUC score : {:.2f} %'.format(roc_auc))\n    \n    print('\\nClassification report :\\n', classification_report(y_test, y))\n\n    plot_roc_curve(model, X_test, y_test)\n    plot_precision_recall_curve(model, X_test, y_test)\n    plot_confusion_matrix(model, X_test, y_test)\n    \n    return acc, pre, rec, roc_auc","0f3522cb":"lr = LogisticRegression(solver='liblinear')\na,b,c,d = model_fit_summarize(lr, X_train, y_train, X_test, y_test)\nsummary = pd.DataFrame([a,b,c,d],index=['Accuracy','Precision','Recall','ROC_AUC'],columns=['LogisticRegression'])","fa0fe6e2":"dt = DecisionTreeClassifier(random_state=0)\na,b,c,d, = model_fit_summarize(dt,X_train, y_train, X_test, y_test)\nsummary['DecisionTree'] = [a,b,c,d]","af719e83":"rf = RandomForestClassifier(random_state=101)\na,b,c,d = model_fit_summarize(rf,X_train, y_train, X_test, y_test)\nsummary['RandomForest']=[a,b,c,d]","0fafbf2a":"xg = XGBClassifier(random_state=123, n_estimators=25, learning_rate=0.01)\na,b,c,d = model_fit_summarize(xg,X_train, y_train, X_test, y_test)\nsummary['XGBoost'] = [a,b,c,d]","fe2f6b50":"pd.set_option('precision',1)\nprint(summary)","91148976":"**XGBoost**","ffaad3c7":"# A Silent Killer?\n\nHeart disease is the leading cause of death across ethnicities with one person succumbing to it every **36 seconds** in the US. Over half a million Americans die of heart disease each year - almost **1 in every 4** deaths - with costs of health services, care and productivity loss amounting to over **200M$** each year.\n\nMore specifically, **1 in every 5** heart attacks is silent - with the damage being done and the person impacted is unaware of it. Early action is critical and knowing the signs and symptoms of a heart attack could help. It would be even more beneficial if we can effectively predict the possibility of a heart attack and take preventive action.\n\nHeart attack (a.k.a *myocardial infraction*) happens when a part of the heart muscle doesn't get enough blood. And the more time passes without restoration of blood flow, the greater the damage to the heart. Coronary artery disease (**CAD**) is the major cause for heart attacks and is caused by plaque build-up in the arterial walls and other parts of the body. Plaque is made up of deposits of cholesterol and other substances. Plaque build up causes the arteries to narrow and over time, this can block (partially or wholly) blood flow.\n\nRisk factors include age, family history and lifestyle choices. While age and family history can't be controlled, appropriate lifestyle choices can help mitigate the incidence of heart attacks.","6e885017":"**Summary**","90335e3b":"**Build Functions**","8c04ce09":"**Decision Tree**","66518f00":"# About the data set\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date.The \"target\" field refers to the presence of heart disease in the patient. \n\n**Attribute Information**\n1. Age\n2. Sex - 1 : male, 0: female\n3. CP - Chest pain type, 0 : typical angina,1 : atypical angina, 2 : non-anginal pain, 3: asymptomatic\n4. trestbps - Resting BP\n5. chol - Serum Cholesterol (mg\/dl)\n6. fbs - Fasting blood glucose (>120 mg\/dl), 1: true, 0: false\n7. restecg -Resting ECG results, 0: normal,1 : having ST-T, 2: hypertrophy\n8. thalach - Maximum heart rate achieved\n9. exang - Exercise induced angina, 1: yes, 0: no\n10. OldPeak - ST depression induced by exercise relative to rest\n11. Slope - slope of the peak exercise ST segment, 0: upsloping, 1: flat, 2: downsloping\n12. ca - Number of major vessels colored by fluroscopy\n13. Thal - 0 : normal, 1 : fixed defect, 2 : reversible defect\n14. Target - 0 : less chance of heart attack, 1 : more chance of attack\n\nDataset is taken for learning purpose. Source of the data : https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease","405acb1d":"**Logistic Regression**","d8e583ab":"**Random Forest**","1ed448ed":"# Knowing the data\n\n","3316f0bc":"# Preprocessing the data","3c53eb60":"# Modeling"}}