{"cell_type":{"44196644":"code","33b275d5":"code","7374f9d5":"code","1e1d55cb":"code","b3b07e7f":"code","27369d76":"code","b018eb03":"code","52e04648":"code","5c79af57":"code","ac1a118e":"code","e474a8be":"code","315b8a35":"code","445062f2":"code","e2c67b22":"code","3f908659":"code","08047343":"code","d85fdc0f":"code","a0fe463d":"code","33ee8fc3":"code","41f33a4e":"code","fc49936c":"code","6e25086d":"code","1a108976":"code","139f7e40":"code","6525dd5b":"code","cdc9efec":"code","4a44d7be":"code","bd5d50b2":"code","6bbd0dd0":"code","8942415d":"code","5e16af6a":"code","ec51a013":"code","3487edde":"code","f797e2de":"code","6c1390fe":"code","b22fa404":"code","410af955":"code","6ba4c0e6":"code","6c5cf5c4":"code","15ce1e65":"code","2f44974b":"code","7ea89c87":"code","55ce389a":"code","7cc091de":"code","f4efab92":"code","8ac3352a":"code","c0c0bc0a":"code","afb614bc":"code","c78cafbe":"code","270fc42a":"code","1e332b07":"code","714dffc2":"code","bf8b2e59":"code","7803a05e":"code","c06c92bb":"code","5ea7321b":"code","3abc8978":"code","e6c466f0":"code","1f2fecd1":"code","ce7f5816":"code","a8c9ecbe":"code","ac5a670b":"code","bd4efe4d":"code","3ff6bd9e":"code","239cb9ec":"code","3d79b559":"code","98524a1b":"code","4f6fb05a":"code","b4a8e5b0":"code","11abfedd":"code","ec169ff3":"code","ea6fc0c2":"code","48b0aaa9":"code","1122e7c6":"code","0721b028":"code","263bee41":"code","572e156b":"code","633b87c2":"code","05bb2ab2":"code","8a2fa7fa":"code","3f6cf753":"code","6193cd90":"code","fc2a8ac2":"code","0ee054e7":"code","225eb956":"code","9ed92ad4":"code","e997a6f3":"code","28d1a0da":"code","218b4beb":"code","d0493ead":"code","e0629b5d":"code","77bdfc4d":"code","ea0f7c17":"code","5d89427b":"code","b5bf8e2f":"markdown","8653050c":"markdown","a1e0673d":"markdown","8f882c83":"markdown","1dc8ead8":"markdown","a5f083a6":"markdown","aee02252":"markdown","88d94879":"markdown","d33142b3":"markdown","9dc778a0":"markdown","f380f01b":"markdown","83fb9163":"markdown","104cb155":"markdown","5e9d4584":"markdown","eb7e18d9":"markdown"},"source":{"44196644":"# Importing Libraries for Loading Dataset\nimport numpy as np\nimport pandas as pd","33b275d5":"# Importing required libraries for data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom io import StringIO\nfrom sklearn.ensemble import RandomForestRegressor","7374f9d5":"objects = pd.read_csv(\"..\/input\/startup-investments\/objects.csv\", low_memory=False)\nobjects.head()","1e1d55cb":"objects.dtypes","b3b07e7f":"objects['entity_type'].value_counts()","27369d76":"objects['status'].value_counts()","b018eb03":"# Rename id in objects.csv to founded_object_id\nobjects.rename(columns={'id':'funded_object_id'}, inplace=True)\nobjects.head()","52e04648":"objects.drop([\"created_at\",\"updated_at\", \"logo_url\", \"logo_width\",\"overview\", \"entity_id\",\"parent_id\",\"normalized_name\", \"logo_height\",\"short_description\", \"created_at\", \"updated_at\", \"twitter_username\",\"relationships\", \"domain\", \"homepage_url\", \"overview\", \"tag_list\",\"city\", \"region\", \"state_code\"], axis=\"columns\", inplace=True)\nobjects.info()","5c79af57":"objects['category_code'].value_counts()","ac1a118e":"objects['country_code'].value_counts()","e474a8be":"investments = pd.read_csv(\"..\/input\/startup-investments\/investments.csv\")\ninvestments.head()","315b8a35":"investments['funded_object_id'].value_counts()","445062f2":"# Loading and merging the required dataset\n\ndf = investments.merge(objects, on='funded_object_id')\ndf.head()","e2c67b22":"df.info()","3f908659":"df['status'].value_counts()","08047343":"df.drop([\"closed_at\", \"first_investment_at\",\"invested_companies\", \"investment_rounds\", \"created_at\", \"updated_at\"], axis=\"columns\", inplace= True)","d85fdc0f":"df.tail()","a0fe463d":"df['funded_object_id'].value_counts()","33ee8fc3":"A = pd.read_csv(\"..\/input\/startup-investments\/funding_rounds.csv\")\nA.head()","41f33a4e":"A.info()","fc49936c":"A.drop(['id', 'funding_round_id', 'funding_round_code', 'raised_amount', 'raised_currency_code', 'pre_money_valuation_usd', 'pre_money_valuation', 'pre_money_currency_code', 'post_money_valuation_usd', 'post_money_currency_code', 'participants', 'is_first_round', 'is_last_round', 'source_url', 'source_description', 'created_by', 'updated_at', 'created_at'], axis='columns', inplace=True)","6e25086d":"A.drop(['post_money_valuation'], axis='columns', inplace=True)","1a108976":"A.rename(columns={'object_id':'funded_object_id'}, inplace=True)\nA.head()","139f7e40":"df2 = df.merge(A, on='funded_object_id')\ndf2.head()","6525dd5b":"df2.info()","cdc9efec":"len(df2)","4a44d7be":"df2.isna().sum()","bd5d50b2":"# Check in percentage the missing data\n# summing up the missing values (column-wise) and displaying fraction of NaNs\nround(100*(df2.isnull().sum()\/len(df2.index)), 2)","6bbd0dd0":"df2.drop(['created_by', 'first_milestone_at', 'last_milestone_at', 'last_investment_at'], axis='columns', inplace=True)","8942415d":"df2.head()","5e16af6a":"df2['category_code'].value_counts()","ec51a013":"# Check in percentage the missing data\n# summing up the missing values (column-wise) and displaying fraction of NaNs\nround(100*(df2.isnull().sum()\/len(df2.index)), 2)","3487edde":"#Dropping rows based on null columns\ndf2 = df2[~(df2['country_code'].isnull() | df2['description'].isnull() | df2['funded_at'].isnull() | df2['founded_at'].isnull())]","f797e2de":"df2.isna().sum()","6c1390fe":"df2 = df2[~(df2['category_code'].isnull())]","b22fa404":"df2.isna().sum()","410af955":"df2['status'].value_counts()","6ba4c0e6":"#Identify duplicates records in the data\ndupes=df2.duplicated()\nsum(dupes)","6c5cf5c4":"df2=df2.drop_duplicates()","15ce1e65":"#Identify duplicates records in the data\ndupes2=df2.duplicated()\nsum(dupes2)","2f44974b":"# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.rcParams['figure.figsize'] = 10,10\nlabels = df2['status'].value_counts().index.tolist()\nsizes = df2['status'].value_counts().tolist()\nexplode = (0, 0.2, 0, 0)\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')\nplt.title(\"What is start up companies current status\", fontdict=None, position= [0.48,1], size = 'x-large')\n\nplt.show()","7ea89c87":"len(df['category_code'].unique())","55ce389a":"df2['category_code'].value_counts()[:5]","7cc091de":"plt.rcParams['figure.figsize'] = 15,8\n\nheight = df2['category_code'].value_counts()[:45].tolist()\nbars =  df2['category_code'].value_counts()[:45].index.tolist()\ny_pos = np.arange(len(bars))\nplt.bar(y_pos, height , width=0.7 ,color= ['c']+['paleturquoise']*14)\nplt.xticks(y_pos, bars)\nplt.xticks(rotation=90)\nplt.title(\"All Start-Up market category\", fontdict=None, position= [0.48,1.05], size = 'x-large')\nplt.show()","f4efab92":"plt.rcParams['figure.figsize'] = 15,8\n\nheight = df2['category_code'].value_counts()[:15].tolist()\nbars =  df2['category_code'].value_counts()[:15].index.tolist()\ny_pos = np.arange(len(bars))\nplt.bar(y_pos, height , width=0.7 ,color= ['c']+['paleturquoise']*14)\nplt.xticks(y_pos, bars)\nplt.xticks(rotation=90)\nplt.title(\"Top 15 Start-Up market category\", fontdict=None, position= [0.48,1.05], size = 'x-large')\nplt.show()","8ac3352a":"def count_word(df2, ref_col, liste):\n    keyword_count = dict()\n    for s in liste: keyword_count[s] = 0\n    for liste_keywords in df[ref_col].str.split('|'):        \n        if type(liste_keywords) == float and pd.isnull(liste_keywords): continue        \n        for s in [s for s in liste_keywords if s in liste]: \n            if pd.notnull(s): keyword_count[s] += 1\n    #______________________________________________________________________\n    # convert the dictionary in a list to sort the keywords by frequency\n    keyword_occurences = []\n    for k,v in keyword_count.items():\n        keyword_occurences.append([k,v])\n    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n    return keyword_occurences, keyword_count\n\n\ndef makeCloud(Dict,name,color):\n    words = dict()\n\n    for s in Dict:\n        words[s[0]] = s[1]\n\n        wordcloud = WordCloud(width=1500, height=750, background_color=color, max_words=50, max_font_size=500, normalize_plurals=False)\n        wordcloud.generate_from_frequencies(words)\n\n\n    fig = plt.figure(figsize=(12, 8))\n    plt.title(name)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n\n    plt.show()","c0c0bc0a":"set_keywords = set()\nfor liste_keywords in df['category_code'].str.split('|').values:\n    if isinstance(liste_keywords, float): continue  # only happen if liste_keywords = NaN\n    set_keywords = set_keywords.union(liste_keywords)\n#_________________________\n","afb614bc":"keyword_occurences, dum = count_word(df, 'category_code', set_keywords)","c78cafbe":"makeCloud(keyword_occurences[0:15],\"Keywords\",\"White\")","270fc42a":"df2.head()","1e332b07":"df2['raised_amount_usd'].head()","714dffc2":"plt.rcParams['figure.figsize'] = 15,6\nplt.hist(df2['raised_amount_usd'].dropna(), bins=30)\nplt.ylabel('Count')\nplt.xlabel('Fnding (usd)')\nplt.title(\"Distribution of total funding \", fontdict=None, position= [0.48,1.05], size = 'x-large')\nplt.show()","bf8b2e59":"Q1 = df2['raised_amount_usd'].quantile(0.25)\nQ3 = df2['raised_amount_usd'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = (Q1 - 1.5 * IQR)\nupper_bound = (Q3 + 1.5 * IQR)","7803a05e":"without_outlier = df2[(df2['raised_amount_usd'] > lower_bound ) & (df2['funding_total_usd'] < upper_bound)]","c06c92bb":"plt.rcParams['figure.figsize'] = 15,6\nplt.hist(without_outlier['raised_amount_usd'].dropna(), bins=30,color = 'paleturquoise' )\n\nplt.ylabel('Count')\nplt.xlabel('Funding (usd)')\nplt.title(\"Distribution of total funding \", fontdict=None, position= [0.48,1.05], size = 'x-large')\nplt.show()","5ea7321b":"df2.info()","3abc8978":"df2.head()","e6c466f0":"df2['name'].value_counts()[:1000]","1f2fecd1":"plt.rcParams['figure.figsize'] = 15,8\n\nheight = df2['name'].value_counts()[:60].tolist()\nbars =  df2['name'].value_counts()[:60].index.tolist()\ny_pos = np.arange(len(bars))\nplt.bar(y_pos, height , width=0.7 ,color= ['c']+['paleturquoise']*14)\nplt.xticks(y_pos, bars)\nplt.xticks(rotation=90)\nplt.title(\"Top 60 Company names\", fontdict=None, position= [0.48,1.05], size = 'x-large')\nplt.show()","ce7f5816":"plt.rcParams['figure.figsize'] = 15,8\n\nheight = df2['name'].value_counts()[:15].tolist()\nbars =  df2['name'].value_counts()[:15].index.tolist()\ny_pos = np.arange(len(bars))\nplt.bar(y_pos, height , width=0.7 ,color= ['c']+['paleturquoise']*14)\nplt.xticks(y_pos, bars)\nplt.xticks(rotation=90)\nplt.title(\"Top 15 Company names\", fontdict=None, position= [0.48,1.05], size = 'x-large')\nplt.show()","a8c9ecbe":"df2['founded_at'] = pd.to_datetime(df2['founded_at'], errors = 'coerce' )","ac5a670b":"plt.rcParams['figure.figsize'] = 15,6\ndf2['name'].groupby(df2[\"founded_at\"].dt.year).count().plot(kind=\"line\")\n\nplt.ylabel('Count')\nplt.title(\"Founded distribution \", fontdict=None, position= [0.48,1.05], size = 'x-large')\nplt.show()","bd4efe4d":"Facebook_founded_year = df2['founded_at'][df2['name']==\"Facebook\"].dt.year\nTwitter_founded_year  = df2['founded_at'][df2['name']==\"Twitter\"].dt.year\nPinterest_founded_year  = df2['founded_at'][df2['name']==\"Pinterest\"].dt.year","3ff6bd9e":"Facebook_founded_year","239cb9ec":"Twitter_founded_year","3d79b559":"Pinterest_founded_year","98524a1b":"len(df2['country_code'].unique())","4f6fb05a":"df2['country_code'].value_counts()[:10]","b4a8e5b0":"df2['count'] = 1\ncountry_market = df2[['count','country_code','category_code']].groupby(['country_code','category_code']).agg({'count': 'sum'})\n# Change: groupby state_office and divide by sum\ncountry_market_pct = country_market.groupby(level=0).apply(lambda x:\n                                                 100 * x \/ float(x.sum()))\ncountry_market_pct.reset_index(inplace = True)","11abfedd":"USA_market_pct = country_market_pct[country_market_pct['country_code'] == \"USA\"]\nUSA_market_pct = USA_market_pct.sort_values('count',ascending = False)[0:10]","ec169ff3":"## USA\nplt.rcParams['figure.figsize'] =10,10\nlabels = list(USA_market_pct['category_code'])+['Other...']\nsizes = list(USA_market_pct['count'])+[100-USA_market_pct['count'].sum()]\nexplode = (0.18, 0.12, 0.09,0,0,0,0,0,0,0,0.01)\ncolors =  ['royalblue','mediumaquamarine','moccasin'] +['oldlace']*8\n\nplt.pie(sizes, explode = explode, colors = colors ,labels=labels, autopct='%1.1f%%',\n        shadow=False, startangle=30)\nplt.axis('equal')\nplt.tight_layout()\nplt.title(\"USA start up market\", fontdict=None, position= [0.48,1.1], size = 'x-large')\n\nplt.show()\nprint('For USA, Most of start up market is about Software & Technology')","ea6fc0c2":"NGA_market_pct = country_market_pct[country_market_pct['country_code'] == \"NGA\"]\nNGA_market_pct = NGA_market_pct.sort_values('count',ascending = False)[0:10]","48b0aaa9":"plt.rcParams['figure.figsize'] = 10,10\nlabels = list(NGA_market_pct['category_code'])+['Other...']\nsizes = list(NGA_market_pct['count'])+[100-USA_market_pct['count'].sum()]\nNones = (0.18, 0.12, 0.09,0,0,0,0.01)\ncolors =  ['royalblue','violet','gold'] +['oldlace']*8\n\nplt.pie(sizes, explode = Nones, colors = colors ,labels=labels, autopct='%1.1f%%',shadow=False, startangle=30)\nplt.axis('equal')\nplt.tight_layout()\nplt.title(\"Nigeria start up market\", fontdict=None, position= [0.48,1.1], size = 'x-large')\nplt.show()\nprint('For NGA, Most of start up market is about Social mainly')","1122e7c6":"df2.to_csv('EDA.csv', index=False, header=1)","0721b028":"# Import data again but this time parse dates\ndf3 = pd.read_csv(\".\/EDA.csv\",\n                 low_memory=False,\n                 parse_dates=[\"funded_at\", \"founded_at\"])","263bee41":"df3.head().T","572e156b":"df3[\"saleYear\"] = df3.funded_at.dt.year\ndf3[\"saleMonth\"] = df3.funded_at.dt.month\ndf3[\"saleDay\"] = df3.funded_at.dt.day\ndf3[\"saleDayOfWeek\"] = df3.funded_at.dt.dayofweek\ndf3[\"saleDayOfYear\"] = df3.funded_at.dt.dayofyear","633b87c2":"df3.head().T","05bb2ab2":"# Let's build a machine learning model \nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_jobs=-1,\n                              random_state=42)\n\nmodel.fit(df3.drop(\"funding_total_usd\", axis=1), df3[\"funding_total_usd\"])","8a2fa7fa":"# Find the columns which contain strings\nfor label, content in df3.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label) ","3f6cf753":"# This will turn all of the string value into category values\nfor label, content in df3.items():\n    if pd.api.types.is_string_dtype(content):\n        df3[label] = content.astype(\"category\").cat.as_ordered()","6193cd90":"df3.info()","fc2a8ac2":"# Check for columns which aren't numeric\nfor label, content in df3.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)","0ee054e7":"# Turn categorical variables into numbers and fill missing\nfor label, content in df3.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to indicate whether sample had missing value\n        df3[label+\"_is_missing\"] = pd.isnull(content)\n        # Turn categories into numbers and add +1\n        df3[label] = pd.Categorical(content).codes+1","225eb956":"%%time\n# Instantiate model \nmodel = RandomForestRegressor(n_jobs=-1,\n                              random_state=42)\n\nmodel.fit(df3.drop(\"funding_total_usd\", axis=1), df3[\"funding_total_usd\"])","9ed92ad4":"df3['funding_total_usd'].value_counts()","e997a6f3":"df3['raised_amount_usd'].value_counts()","28d1a0da":"# Score the model\nmodel.score(df3.drop(\"funding_total_usd\", axis=1), df3[\"funding_total_usd\"])","218b4beb":"df3.saleYear.value_counts()","d0493ead":"# Split data into training and validation\ndf_val = df3\ndf_train = df3\n\nlen(df_val), len(df_train)","e0629b5d":"# Split data into X & y\nX_train, y_train = df_train.drop(\"funding_total_usd\", axis=1), df_train.funding_total_usd\nX_valid, y_valid = df_val.drop(\"funding_total_usd\", axis=1), df_val.funding_total_usd\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","77bdfc4d":"# Create evaluation function (the competition uses RMSLE)\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n\ndef rmsle(y_test, y_preds):\n    \"\"\"\n    Caculates root mean squared log error between predictions and\n    true labels.\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n\n# Create function to evaluate model on a few different levels\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_valid)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n              \"Valid MAE\": mean_absolute_error(y_valid, val_preds),\n              \"Training RMSLE\": rmsle(y_train, train_preds),\n              \"Valid RMSLE\": rmsle(y_valid, val_preds),\n              \"Training R^2\": r2_score(y_train, train_preds),\n              \"Valid R^2\": r2_score(y_valid, val_preds)}\n    return scores","ea0f7c17":"%%time\n# Cutting down on the max number of samples each estimator can see improves training time\nmodel.fit(X_train, y_train)","5d89427b":"show_scores(model)","b5bf8e2f":"**STARTUP INVESTMENT**\n*Venture deals, organizations, people and exits*\n\n**PROBLEM DEFINITION**\n> Tracking and analyzing investment trends over time\n\n**DATA**\n> This data is downloaded from kaggle notebook with the url\n  https:\/\/www.kaggle.com\/justinas\/startup-investments\n  \n**CONTENT**\n> This diverse dataset contains information about the startup ecosystem: organizations, individuals, company news, funding rounds, acquisitions, and IPOs.\n\n> There are 11 tables that can be joined using unique IDs (schema to follow). More information about the individual data variables can be found on the Crunchabse Data website (under the API Entities Types section).\n\n> No extensive data quality check have been performed yet. The information is available up to December 2013.\n\n> While Kaggle contains other datasets focused on startup investments, to the best of my knowledge, this dataset has not yet been published on the platform and is unique.\n\n**AKNOWLEDGEMENT**\n> This Crunchbase 2013 Snapshot \u00a9 2013 dataset is fully attributed to Crunchbase.\n\n**EVALUATION**\n> Time Series would be used to focast\n\n\n**INSPIRATIONS**\n> There are multiple avenues for exploration:\nEDA of the startup ecosystem.\nTracking and analyzing investment trends over time\nClustering VC funds based on their existing investments.\nPredicting which startup will proceed to raise further rounds \/ will get acquired \/ will file for an IPO.\nMapping the network of individuals involved in the startup ecosystem.","8653050c":"We have 94 unique code in the dataset","a1e0673d":"# **LETS DO SOME ANALYSIS**","8f882c83":"# **Lets do some Time Series Analysis**","1dc8ead8":"Most of the companies came from USA","a5f083a6":"**Now let me check for my counry Nigeria**","aee02252":"**Now lets look at the top 15 startup category market**","88d94879":"The most popular category is still about Software, Biotech & Web,\n\nIt maybe because these 3 categories are easily to scalable ? ","d33142b3":" **Country Code**","9dc778a0":"these two pie charts show how different of interest trend between Nigeria and America ","f380f01b":"Lets do something interesting","83fb9163":"**Split the data**","104cb155":"Most of company (78.5 %) in this dataset is operating,\n\nand around 3.0 % company is already closed.","5e9d4584":"**TOTAL FUNDING USD**","eb7e18d9":"because we have around 43 categories of start up,\n\nThen just plot all   : )"}}