{"cell_type":{"f1e73c4a":"code","b8cdef15":"code","cb6de8e5":"code","77ca0225":"code","fa772a0e":"code","e0aec6df":"code","574174a1":"code","99f70bb5":"code","8fdd39d5":"code","cbfac290":"code","f888d94e":"code","7150eacc":"code","5a32b89e":"code","b3809f0c":"code","41cbea5e":"code","7ab7c447":"code","c76a023a":"code","243140c4":"code","61171f12":"code","6b5a0397":"code","caf9fb58":"code","e2c9a634":"code","3682e2e9":"code","1159843e":"code","79dd9ee3":"code","81197ec1":"code","db816208":"code","ac1bb975":"code","ff6f5e13":"code","1c097e4f":"code","f91fb98d":"markdown","77da0d3a":"markdown","198e2c3d":"markdown","8b4752ba":"markdown","cfe0c47e":"markdown","92b17ee2":"markdown","4649eada":"markdown","79e42796":"markdown","6aef9741":"markdown"},"source":{"f1e73c4a":"! cp ..\/input\/my-python\/* ..\/working\/\n! ls ..\/working ","b8cdef15":"# Generic imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nfrom tqdm import tqdm_notebook\n%matplotlib inline\nsns.set_style('whitegrid')\n\n# Sklearn imports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Keras imports\nfrom keras.applications import VGG16, InceptionV3\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Input\nfrom keras.optimizers import RMSprop, Adam, SGD\nfrom keras.preprocessing.image import load_img,img_to_array\n\n# cyclical learning rates\nfrom clr import LRFinder\nfrom clr_callback import CyclicLR","cb6de8e5":"# Constants\nDATA_DIR = '..\/input\/dog-breed-identification\/'\nTRAIN_DIR = DATA_DIR + 'train\/'\nTEST_DIR = DATA_DIR + 'test\/'\nBATCH_SIZE = 32\nINPUT_SIZE = 224\nNUM_CLASSES = 120\nSEED = 42","77ca0225":"# Let's check what's in the data directory\n! ls $DATA_DIR","fa772a0e":"# Read the train data set, which has the ids of the images and their labels (breeds)\n# (adding the extension .jpg to the id becomes the file name of the image) \ntrain = pd.read_csv(DATA_DIR + 'labels.csv')\ntrain.head()","e0aec6df":"# The submission file contains one column for the image id, and then one column \n# each breed in alphabetical order, with the probability of the dog in the image beeing of that breed\nsubmission = pd.read_csv(DATA_DIR + 'sample_submission.csv')\nsubmission.head()","574174a1":"# Create a map of breeds to labels in the same order as the columns of the submission file\n# and create a new column 'label' in the train data frame with the breeds mapped to this labels.\n# This will make easier build the submission file from the predicted probabilities of the trained model\nbreed_labels = {breed:label for label,breed in enumerate(submission.columns[1:].values)}\ntrain['label'] = train['breed'].map(breed_labels)\ntrain.head()","99f70bb5":"# Frequency of each breed in the train set. We can see that the most frequent breed\n# has just above 120 images and the less frequent just above 60 images.\ncounts = train.breed.value_counts()\nplt.figure(figsize=(10,40))\nplt.xticks(np.arange(0, 130, 5))\nsns.barplot(x=counts.values, y=counts.index);","8fdd39d5":"# Let's plot some random images\nfig, axs = plt.subplots(5,5, figsize=(20,20), squeeze=True)\nplt.subplots_adjust(wspace=0.1, hspace=0.1)\naxs = axs.reshape(-1)\nindices = np.random.choice(train.shape[0], 25, replace=False)\nfor ax, i in zip(axs, indices):\n    img = cv2.imread(os.path.join(DATA_DIR, 'train', train.iloc[i].id + '.jpg'))\n    h, w, c = img.shape\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel(train.iloc[i].breed, fontsize=12)\n    ax.imshow(img)\n    ","cbfac290":"base = VGG16(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3))\nbase.summary()","f888d94e":"# Make the batchsize of the data generator a divisor of the number of images, as we have\n# to make just one pass for feature extraction\nbatch_size = 269           # 10222 = 2 * 19 * 269\n\n# No data augmentation, just rescaling the image\ndatagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = datagen.flow_from_dataframe(dataframe=train, \n                                              directory= TRAIN_DIR,\n                                              x_col='id',\n                                              y_col='label',\n                                              class_mode='categorical',\n                                              has_ext=False,\n                                              batch_size=batch_size,   \n                                              shuffle=False,\n                                              seed=42,\n                                              target_size=(224,224)                                              \n                                             )","7150eacc":"%%time\n\n# Let's read all the images and labels into arrays in memory\n\ntrain_generator.reset()\ntrain_size = train.shape[0]\nfeatures = np.zeros(shape=(train_size, 7,7,512))\nlabels = np.zeros(shape=(train_size, NUM_CLASSES))\ni = 0\nfor inputs_batch, labels_batch in tqdm_notebook(train_generator):\n    features_batch = base.predict(inputs_batch)\n    features[i * batch_size:(i+1) * batch_size] = features_batch\n    labels[i * batch_size:(i+1) * batch_size] = labels_batch\n    i += 1\n    if i * batch_size >= train_size:\n        break;\n   \n# Flatten the output of the VGG16 base model\nfeatures = features.reshape(train_size, -1)","5a32b89e":"X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.20, random_state=SEED)","b3809f0c":"# Build the classifier\n\ndef create_model(dropout=None):\n    model = Sequential()\n    model.add(Dense(1024, activation='relu'))\n    if dropout:\n        model.add(Dropout(dropout))\n    model.add(Dense(512, activation='relu'))\n    if dropout:\n        model.add(Dropout(dropout))\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n\n    adam = Adam(lr=0.001)\n    sgd = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['acc'],  )\n    return model\n","41cbea5e":"# Run a lr range test to find good learning rate margins\n\nmodel = create_model()\n\nBATCH_SIZE = 32\nSTEP_SIZE_TRAIN = X_train.shape[0] \/\/ BATCH_SIZE\nSTEP_SIZE_VALID = X_val.shape[0] \/\/ BATCH_SIZE\n\nEPOCHS = 1\nbase_lr=0.0001\nmax_lr=100\nstep_size = EPOCHS * STEP_SIZE_TRAIN \nlrf = LRFinder(X_train.shape[0], BATCH_SIZE,\n                       base_lr, max_lr,\n                       # validation_data=(X_val, Yb_val),\n                       lr_scale='exp', save_dir='.\/lr_find\/', verbose=False)\n\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, steps_per_epoch = STEP_SIZE_TRAIN,\n                    validation_data=[X_val, y_val], validation_steps = STEP_SIZE_VALID,\n                   callbacks=[lrf])","7ab7c447":"# Training\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=[X_val, y_val])","c76a023a":"fig = plt.figure(figsize=(15,7))\nlrf.plot_schedule(clip_beginning=95, clip_endding=60)","243140c4":"10**(-1.7)","61171f12":"model = create_model()\nEPOCHS=10\nBATCH_SIZE=32\nclr = CyclicLR(base_lr=0.005, max_lr=0.02, step_size=2*STEP_SIZE_TRAIN)\n\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=[X_val, y_val], validation_steps=STEP_SIZE_VALID,\n                    callbacks=[clr])\n","6b5a0397":"def plt_history(history, metric, title, ax, val=True):\n    ax.plot(history[metric])\n    if val:\n        ax.plot(history['val_' + metric])\n    ax.grid(True)\n    ax.set_title(title)\n    ax.set_xlabel('epoch')\n    ax.set_ylabel(metric)\n    \nhist = history.history\nfig, ax = plt.subplots(1,2, figsize=(15,6))\nplt_history(hist, 'loss', 'LOSS', ax[0])\nplt_history(hist, 'acc', 'ACCURACY', ax[1])","caf9fb58":"base = InceptionV3(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3), pooling='avg')\nbase.summary()","e2c9a634":"# Make the batchsize of the data generator a divisor of the number of images, as we have\n# to make just one pass for feature extraction\nbatch_size = 269           # 10222 = 2 * 19 * 269\n\n# No data augmentation, just rescaling the image\ndatagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = datagen.flow_from_dataframe(dataframe=train, \n                                              directory= TRAIN_DIR,\n                                              x_col='id',\n                                              y_col='label',\n                                              class_mode='categorical',\n                                              has_ext=False,\n                                              batch_size=batch_size,   \n                                              shuffle=False,\n                                              seed=42,\n                                              target_size=(224,224)                                              \n                                             )","3682e2e9":"%%time\n\n# Let's read all the images and labels into arrays in memory. We can use the same generator,\n# but this time the features array will have shape (train_size, 512) instead of (train_size, 7*7*512)\n\n\ntrain_generator.reset()\ntrain_size = train.shape[0]\nfeatures = np.zeros(shape=(train_size, 2048))\nlabels = np.zeros(shape=(train_size, NUM_CLASSES))\ni = 0\nfor inputs_batch, labels_batch in tqdm_notebook(train_generator):\n    features_batch = base.predict(inputs_batch)\n    features[i * batch_size:(i+1) * batch_size] = features_batch\n    labels[i * batch_size:(i+1) * batch_size] = labels_batch\n    i += 1\n    if i * batch_size >= train_size:\n        break;\n   \n# This time the features array doesn't need flattening\n","1159843e":"# Build the classifier. This time, the classifier has a lot fewer parameters\ndef create_model_pool(lr=0.001):\n    model = Sequential()\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n\n    sgd = SGD(lr=lr, momentum=0.9)\n    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['acc'],  )\n    return model","79dd9ee3":"X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.25, random_state=SEED)","81197ec1":"# Run a lr range test to find good learning rate margins\n\nmodel = create_model_pool()\n\nBATCH_SIZE = 32\nSTEP_SIZE_TRAIN = X_train.shape[0] \/\/ BATCH_SIZE\nSTEP_SIZE_VALID = X_val.shape[0] \/\/ BATCH_SIZE\n\nEPOCHS = 1\nbase_lr=0.001\nmax_lr=1\nstep_size = EPOCHS * STEP_SIZE_TRAIN \nlrf = LRFinder(X_train.shape[0], BATCH_SIZE,\n                       base_lr, max_lr,\n                       validation_data=(X_val, y_val),\n                       lr_scale='exp', save_dir='.\/lr_find\/', verbose=False)\n\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, steps_per_epoch = STEP_SIZE_TRAIN,\n                    validation_data=[X_val, y_val], validation_steps = STEP_SIZE_VALID,\n                   callbacks=[lrf])","db816208":"fig = plt.figure(figsize=(15,7))\nlrf.plot_schedule(clip_beginning=50)","ac1bb975":"10**(-1.5)","ff6f5e13":"# Training\nmodel = create_model_pool()\nEPOCHS=20\nclr = CyclicLR(base_lr=0.01, max_lr=0.03, step_size=2*STEP_SIZE_TRAIN)\n\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=[X_val, y_val], validation_steps=STEP_SIZE_VALID,\n                    callbacks=[clr])\n","1c097e4f":"hist = history.history\nfig, ax = plt.subplots(1,2, figsize=(15,6))\nplt_history(hist, 'loss', 'LOSS', ax[0])\nplt_history(hist, 'acc', 'ACCURACY', ax[1])","f91fb98d":"<h1>Transfer learning for dog breed classification - Part I<\/h1>\n\nIn every single computer vision competition, top rank competitors almost never train their networks from scratch. They transfer knowledge from pretrained models to theirs. This is something I painfully learned in the first official competition I took place in (TGS Salt Identification), and I wish someone would have told me then. For that reason, I'm planning to create a series of kernels for beginners in which I'll cover the basic techniques in transfer learning. I will only touch the problem of image classification, but these techniques could be applied to any other computer vision problem (image segmentation, object detection, ...). I will be using a pretrained model for the dog breed classification problem. At the same time, I will be covering some useful techniques which I also painfully learned about such as using <code>ImageDataGenerator<\/code> for reading images into memory from disk and data augmentation.\n\nThe series will have three parts, in each of which I will cover three basic techniques and some of its variants:\n\n<olist>\n    <li>Part I - Extract features from the last convolutional block of VGG16 and use them to train a NN classifier, without data augmentation<\/li>\n    <li>Part II - Freeze the VGG16 base model, put a classifier on top of it and train the model with data augmentation.<\/li>\n    <li>Part III - Starting from the model trained in Part 2, unfreeze the last convolutional block and finetune the network with a small learning rate.<\/li>\n<\/olist>","77da0d3a":"As we can see in the history plot, there's a lot of overfitting. Varying the size of the FC layer in the network between 256 and 1024, and dropout between 0.1 and 0.5 doesn't change anything: train accuracy reaches around 0.99 and validation accuracy just below 0.3.","198e2c3d":"<h3>flow_from_dataframe<\/h3>\nWe don't have train and validation folders, so we cannot use directly the method <code>frow_from_directory<\/code> from <code>ImageDataGenerator<\/code>. Instead we'll be using the method <code>flow_from_dataframe<\/code>. Here's an <a href=\"https:\/\/medium.com\/@vijayabhaskar96\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\">article<\/a> on how to use it by the guy who wrote it, Vijayabhaskar J.","8b4752ba":"<h2>Variant 2 - Feature extraction from the last convolutional layer with average pooling<\/h2>\n\nWe'll extract features from the last convolutional layer in VGG16, wich has shape (None, 7, 7, 512), but average pooling this layer, so the output of the VGG16 model will be (None, 512).","cfe0c47e":"As we can see, with this approach we still have a lot of overfitting and the results are even worse for the train set. It seems that we will need a wiser strategy (data augmentation) for this problem in part II of this series.","92b17ee2":"<h3>Fitting a CNN with the generated features<\/h3>","4649eada":"*<h2>Variant 1 - Feature extraction from the last convolutional layer without pooling<\/h2>\n\nWe'll extract features directly from the last convolutional layer in VGG16, wich has shape (None, 7, 7, 512). This means that for every image\nwe'll extract 7*7*512 = 25088 features, and connect them to our classifier.","79e42796":"<h2>A bit of EDA<\/h2>","6aef9741":"<h3>Create custom estimator<\/h3>\nBased on this excellent <a href=\"http:\/\/danielhnyk.cz\/creating-your-own-estimator-scikit-learn\/\">article<\/a> by Daniel Hnyk, and the Scikit-learn <a href=\"https:\/\/scikit-learn.org\/dev\/developers\/contributing.html#rolling-your-own-estimator\">documentation<\/a>."}}