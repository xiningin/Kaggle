{"cell_type":{"aeb329cb":"code","d62d568f":"code","4068b272":"code","cc0a6e29":"code","34e6d1a1":"code","248fccf2":"code","75f52869":"code","1d7f2f6f":"code","bc62eb16":"code","be7ef5b8":"code","bfcf62d4":"code","de8ea6ec":"code","9ae6a84f":"code","c89dade7":"code","119a3322":"code","5eece7c1":"code","96324731":"code","1bc5791e":"code","8395dfa0":"code","e323b39e":"code","37010500":"code","0b744ca5":"code","245f1870":"code","ff0ba7a1":"code","e18ef1f2":"code","aef22e06":"code","54057a63":"code","56773bb1":"code","aef776b9":"code","ee76eb4f":"code","8542adf2":"code","b1018851":"code","14d5f31a":"code","34732d62":"code","03c8f30b":"code","e6aa5f58":"code","eecdd1b2":"code","8821dc88":"code","82c8bc98":"code","64aa0d32":"code","d47634ba":"code","f538d90a":"code","c3a277b6":"code","4f807b70":"code","76b3eec5":"code","4f2e85c8":"code","17fda1f3":"code","2724a89a":"code","2f9b3617":"code","0d178745":"code","ec1e165d":"code","a8cf6f57":"code","11112d13":"code","1000b4aa":"code","1d3a905d":"code","b00dd22b":"code","b43b51b9":"code","d30db042":"code","59045eaa":"code","ae999cd3":"code","ae905cd0":"code","dfcc9fe5":"code","32f8fc02":"code","b4fe8704":"code","87bcb581":"code","350eaff5":"code","56222309":"code","f7fd42c1":"code","d31feeea":"code","918d2ba6":"code","4b47973d":"code","fc030447":"markdown","55cde208":"markdown","69866fe8":"markdown","6dbe7219":"markdown","243a78b7":"markdown","7bd22fff":"markdown","dd661457":"markdown","05d61bef":"markdown","ef5e44eb":"markdown","c7cef3f4":"markdown","f7f6b83f":"markdown","d71bdbc8":"markdown","91a5d3c5":"markdown","4f7e3c34":"markdown","bf329fc2":"markdown","c1e866d6":"markdown","540e2a2c":"markdown"},"source":{"aeb329cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d62d568f":"import xgboost as xgb\nimport lightgbm as lgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers import Dense, Dropout, SpatialDropout1D, Activation, BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import CuDNNLSTM, CuDNNGRU\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, model_selection, decomposition, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, AveragePooling1D, MaxPooling1D, Flatten, Bidirectional\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words(\"english\")","4068b272":"train = pd.read_csv('..\/input\/spooky-author-identification\/train.csv')\ntest = pd.read_csv('..\/input\/spooky-author-identification\/test.csv')\nsample = pd.read_csv('..\/input\/spooky-author-identification\/sample_submission.csv')","cc0a6e29":"train.head()","34e6d1a1":"test.head()","248fccf2":"sample.head()","75f52869":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 \/ rows * vsota","1d7f2f6f":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)","bc62eb16":"print('Training shape', train.shape)\nprint('Test shape', test.shape)","be7ef5b8":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y,\n                                                 stratify = y,\n                                                 random_state = 42,\n                                                 test_size = 0.1, shuffle = True) ","bfcf62d4":"print(xtrain.shape)\nprint(xvalid.shape)","de8ea6ec":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df = 3, max_features = None,\n                     strip_accents = 'unicode', analyzer = 'word', token_pattern = r'\\w{1,}',\n                     ngram_range = (1, 3), use_idf = 1, smooth_idf = 1, sublinear_tf = 1,\n                     stop_words = 'english')\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv = tfv.transform(xtrain)\nxvalid_tfv = tfv.transform(xvalid)","9ae6a84f":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C = 1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\nprint('Logloss: %0.3f' % multiclass_logloss(yvalid, predictions))","c89dade7":"ctv = CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}',\n                     ngram_range = (1, 3), stop_words = 'english')\n# Fitting Count Vectorizer to both trainning and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv = ctv.transform(xtrain)\nxvalid_ctv = ctv.transform(xvalid)","119a3322":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","5eece7c1":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\nprint ('logloss: %0.3f' % multiclass_logloss(yvalid, predictions))","96324731":"# Fitting a simple Naive Bayes on Counts\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","1bc5791e":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components = 120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n# Scale the data obtained from SVD. Remaning variable to reuse without scaling\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","8395dfa0":"# Fitting a simple SVM\nclf = SVC(C = 1.0, probability = True) # scine we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\n\nprint('logloss: %0.3f' % multiclass_logloss(yvalid, predictions))","e323b39e":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth = 7, n_estimators = 200, colsample_bytree = 0.8,\n                       subsample = 0.8, nthread = 10, learning_rate = 0.1)\nclf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_tfv.tocsc())\nprint('logloss: %0.3f' % multiclass_logloss(yvalid, predictions))","37010500":"# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(nthread=10)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","0b744ca5":"mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)","245f1870":"# Initialize SVD\nsvd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('lr', lr_model)])","ff0ba7a1":"param_grid = {'svd__n_components' : [120, 180],\n              'lr__C': [0.1, 1.0, 10], \n              'lr__penalty': ['l1', 'l2']}","e18ef1f2":"# Initialize Grid Search Model\nmodel = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = mll_scorer, \n                    verbose = 10, n_jobs = -1, iid = True, refit = True, cv = 2)\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv, ytrain)\nprint('Best Score: %0.3f' % model.best_score_)\nprint('Best parameters set:')\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print('\\t%s: %r' % (param_name, best_parameters[param_name]))","aef22e06":"nb_model = MultinomialNB()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# parameter grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","54057a63":"EMB_PATH = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype = 'float32')\n\ndef load_embedding(embed_dir = EMB_PATH):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n    return embedding_index\n\ndef build_embedding_matrix(word_index, embedding_index, max_features, lower = True, verbose = True):\n    embedding_matrix = np.zeros((max_features, 300))\n    for word, i in tqdm(word_index.items(), disable = not verbose):\n        if lower:\n            word = word.lower()\n        if i >= max_features: continue\n        try:\n            embedding_vector = embedding_index[word]\n        except:\n            embedding_vector = embedding_index['unknown']\n        if embedding_vector is not None:\n            embedding_matrixp[i] = embeeding_vector\n    return embedding_matrix\n\ndef build_matrix(word_index, embeddings_index):\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except:\n            embedding_matrix[i] = embedding_index['unknown']\n    return embedding_matrix","56773bb1":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt', encoding='utf8')\nfor line in tqdm(f):\n    values = line.split()\n    word = ''.join(values[:-300])\n    coefs = np.asarray(values[-300:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","aef776b9":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower().encode().decode('utf-8')\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v \/ np.sqrt((v ** 2).sum())","ee76eb4f":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\nxvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]","8542adf2":"# Fitting a simple xgboost on glove features\nxtrain_glove = np.asarray(xtrain_glove)\nclf = xgb.XGBClassifier(nthread = 10, silent = False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\nprint('looloss: %0.3f' % multiclass_logloss(yvalid, predictions))","b1018851":"# Fitting a simple xgboost on glove features\nxtrain_glove = np.asarray(xtrain_glove)\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","14d5f31a":"# scale the data before any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.fit_transform(xvalid_glove)","34732d62":"# we need to binarize the labels for the neural net\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","03c8f30b":"# create a simple 3 layer sequential neural net\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmodel.summary()","e6aa5f58":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=5, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","eecdd1b2":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=50, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","8821dc88":"model = Sequential()\nmodel.add(Dense(300, input_dim = 300, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(56, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n          \nmodel.add(Dense(3, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'RMSprop')\nmodel.summary()         ","82c8bc98":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size = 64, \n          epochs=50, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","64aa0d32":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","d47634ba":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","f538d90a":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1, \n         300, weights = [embedding_matrix],\n         input_length = max_len,\n         trainable = False))\n\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","c3a277b6":"model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, \n          validation_data=(xvalid_pad, yvalid_enc))","4f807b70":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","76b3eec5":"# A simple bidirectional LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\n\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size = 512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","4f2e85c8":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","17fda1f3":"from keras.models import Model, load_model\nfrom keras.layers import Input, Dense, Embedding, concatenate, CuDNNGRU, CuDNNLSTM, Bidirectional, GlobalAveragePooling1D \nfrom keras.layers import GlobalMaxPooling1D, SpatialDropout1D, Flatten, Lambda, Permute, Reshape, merge, Dropout, Conv2D \nfrom keras.layers import MaxPool2D, Concatenate, Conv1D, MaxPool1D, add, MaxPooling1D\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy  # thay binary_crossentropy bang categorical_crossentropy\n\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.initializers import glorot_uniform, he_uniform, he_normal\nfrom keras.optimizers import Adam\nfrom keras.layers import LeakyReLU, multiply\nfrom keras.layers import Reshape, Permute, multiply","2724a89a":"def build_gru_model(embedding_matrix, name = \"gru\"):\n    \n    inp = Input(shape=(max_len, ))\n    x = Embedding(len(word_index) + 1, 300 , weights=[embedding_matrix],\n                  input_length = max_len,trainable=False)(inp)\n    x = SpatialDropout1D(0.4,seed=1234)(x)\n    x = CuDNNGRU(128, return_sequences=True,kernel_initializer=glorot_uniform(seed=1234))(x)\n    x = CuDNNGRU(64, return_sequences=True,kernel_initializer=glorot_uniform(seed=1234))(x)\n    last = Flatten()(Lambda(lambda x: x[:,-1:,:]) (x))\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool, last])\n    #x = Dropout(.3,seed=seed_nb)(x)\n    x = Dense(32, activation = 'relu')(conc)\n    outp = Dense(3, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=[inp], outputs=outp, name = name)\n    return model","2f9b3617":"K.clear_session()\nbatch_size = 512\nmodel = build_gru_model(embedding_matrix, name = 'gru_stage0')\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer=Adam())\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs = 50, \n          verbose = True, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","0d178745":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","ec1e165d":"def squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)\/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x \/ scale\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","a8cf6f57":"def build_gru_lstm_model(embedding_matrix, name = \"\"):\n    \n    inp = Input(shape=(max_len, ))\n    x = Embedding(len(word_index) + 1, 300 , weights=[embedding_matrix],\n                  input_length = max_len,trainable=False)(inp)\n    x = SpatialDropout1D(0.3, seed = 123)(x)\n    x = Bidirectional(CuDNNGRU(128, return_sequences=True,kernel_initializer=glorot_uniform(seed= 123)))(x)\n    x = Dropout(0.2, seed = 123)(x)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True,kernel_initializer=glorot_uniform(seed=123)))(x)\n\n    last = Flatten()(Lambda(lambda x: x[:,-1:,:]) (x))\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool,max_pool,last])\n    x = Dropout(0.3, seed = 123)(x)\n    outp = Dense(3, activation=\"sigmoid\",kernel_initializer=he_uniform(seed = 123))(conc)\n    \n    model = Model(inputs=[inp], outputs=outp, name = name)\n    return model","11112d13":"K.clear_session()\nbatch_size = 512\nmodel = build_gru_lstm_model(embedding_matrix, name = 'gru_lstm_model')\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer=Adam())\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs = 50, \n          verbose = True, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","1000b4aa":"from sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport pandas as pd\nimport os\nimport sys\nimport logging\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n    datefmt=\"%H:%M:%S\", stream=sys.stdout)\nlogger = logging.getLogger(__name__)\n\n\nclass Ensembler(object):\n    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n                 lower_is_better=False, save_path=None):\n        \"\"\"\n        Ensembler init function\n        :param model_dict: model dictionary, see README for its format\n        :param num_folds: the number of folds for ensembling\n        :param task_type: classification or regression\n        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n        :param lower_is_better: is lower value of optimization function better or higher\n        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n        \"\"\"\n\n        self.model_dict = model_dict\n        self.levels = len(self.model_dict)\n        self.num_folds = num_folds\n        self.task_type = task_type\n        self.optimize = optimize\n        self.lower_is_better = lower_is_better\n        self.save_path = save_path\n\n        self.training_data = None\n        self.test_data = None\n        self.y = None\n        self.lbl_enc = None\n        self.y_enc = None\n        self.train_prediction_dict = None\n        self.test_prediction_dict = None\n        self.num_classes = None\n\n    def fit(self, training_data, y, lentrain):\n        \"\"\"\n        :param training_data: training data in tabular format\n        :param y: binary, multi-class or regression\n        :return: chain of models to be used in prediction\n        \"\"\"\n\n        self.training_data = training_data\n        self.y = y\n\n        if self.task_type == 'classification':\n            self.num_classes = len(np.unique(self.y))\n            logger.info(\"Found %d classes\", self.num_classes)\n            self.lbl_enc = LabelEncoder()\n            self.y_enc = self.lbl_enc.fit_transform(self.y)\n            kf = StratifiedKFold(n_splits=self.num_folds)\n            train_prediction_shape = (lentrain, self.num_classes)\n        else:\n            self.num_classes = -1\n            self.y_enc = self.y\n            kf = KFold(n_splits=self.num_folds)\n            train_prediction_shape = (lentrain, 1)\n\n        self.train_prediction_dict = {}\n        for level in range(self.levels):\n            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n\n        for level in range(self.levels):\n\n            if level == 0:\n                temp_train = self.training_data\n            else:\n                temp_train = self.train_prediction_dict[level - 1]\n\n            for model_num, model in enumerate(self.model_dict[level]):\n                validation_scores = []\n                foldnum = 1\n                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n\n                    if level != 0:\n                        l_training_data = temp_train[train_index]\n                        l_validation_data = temp_train[valid_index]\n                        model.fit(l_training_data, self.y_enc[train_index])\n                    else:\n                        l0_training_data = temp_train[0][model_num]\n                        if type(l0_training_data) == list:\n                            l_training_data = [x[train_index] for x in l0_training_data]\n                            l_validation_data = [x[valid_index] for x in l0_training_data]\n                        else:\n                            l_training_data = l0_training_data[train_index]\n                            l_validation_data = l0_training_data[valid_index]\n                        model.fit(l_training_data, self.y_enc[train_index])\n\n                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n\n                    if self.task_type == 'classification':\n                        temp_train_predictions = model.predict_proba(l_validation_data)\n                        self.train_prediction_dict[level][valid_index,\n                        (model_num * self.num_classes):(model_num * self.num_classes) +\n                                                       self.num_classes] = temp_train_predictions\n\n                    else:\n                        temp_train_predictions = model.predict(l_validation_data)\n                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n                    validation_scores.append(validation_score)\n                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n                                validation_score)\n                    foldnum += 1\n                avg_score = np.mean(validation_scores)\n                std_score = np.std(validation_scores)\n                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n                            avg_score, std_score)\n\n            logger.info(\"Saving predictions for level # %d\", level)\n            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n                                        index=False, header=None)\n\n        return self.train_prediction_dict\n\n    def predict(self, test_data, lentest):\n        self.test_data = test_data\n        if self.task_type == 'classification':\n            test_prediction_shape = (lentest, self.num_classes)\n        else:\n            test_prediction_shape = (lentest, 1)\n\n        self.test_prediction_dict = {}\n        for level in range(self.levels):\n            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n        self.test_data = test_data\n        for level in range(self.levels):\n            if level == 0:\n                temp_train = self.training_data\n                temp_test = self.test_data\n            else:\n                temp_train = self.train_prediction_dict[level - 1]\n                temp_test = self.test_prediction_dict[level - 1]\n\n            for model_num, model in enumerate(self.model_dict[level]):\n\n                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n                if level == 0:\n                    model.fit(temp_train[0][model_num], self.y_enc)\n                else:\n                    model.fit(temp_train, self.y_enc)\n\n                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n\n                if self.task_type == 'classification':\n                    if level == 0:\n                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n                    else:\n                        temp_test_predictions = model.predict_proba(temp_test)\n                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n                                                                                        self.num_classes] = temp_test_predictions\n\n                else:\n                    if level == 0:\n                        temp_test_predictions = model.predict(temp_test[0][model_num])\n                    else:\n                        temp_test_predictions = model.predict(temp_test)\n                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n\n            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n                                       index=False, header=None)\n\n        return self.test_prediction_dict","1d3a905d":"# specify the data to be used for every level of ensembling:\nxtrain_glove = np.asarray(xtrain_glove)\nxvalid_glove = np.asarray(xvalid_glove)\ntrain_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\ntest_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n\nmodel_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n\n              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n\nens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n\nens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\npreds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])","b00dd22b":"multiclass_logloss(yvalid, preds[1])","b43b51b9":"import base64\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom collections import Counter\nfrom scipy.misc import imread\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk","d30db042":"z = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\ndata = [go.Bar(\n            x = train.author.map(z).unique(),\n            y = train.author.value_counts().values,\n            marker= dict(colorscale='Jet',\n                         color = train.author.value_counts().values\n                        ),\n            text='Text entries attributed to Author'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","59045eaa":"all_words = train['text'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in the training dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","ae999cd3":"# Define helper function to print top words\ndef print_top_words(model, feature_names, n_top_words):\n    for index, topic in enumerate(model.components_):\n        message = \"\\nTopic #{}:\".format(index)\n        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n        print(message)\n        print(\"=\"*70)","ae905cd0":"from nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nprint(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))","dfcc9fe5":"lemm = WordNetLemmatizer()\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))","32f8fc02":"# Storing the entire training text in a list\ntext = list(train.text.values)\n# Calling our overwritten Count vectorizer\ntf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\ntf = tf_vectorizer.fit_transform(text)","b4fe8704":"feature_names = tf_vectorizer.get_feature_names()\ncount_vec = np.asarray(tf.sum(axis=0)).ravel()\nzipped = list(zip(feature_names, count_vec))\nx, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n# Now I want to extract out on the top 15 and bottom 15 words\nY = np.concatenate([y[0:15], y[-16:-1]])\nX = np.concatenate([x[0:15], x[-16:-1]])\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[0:50],\n            y = y[0:50],\n            marker= dict(colorscale='Jet',\n                         color = y[0:50]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[-100:],\n            y = y[-100:],\n            marker= dict(colorscale='Portland',\n                         color = y[-100:]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Bottom 100 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","87bcb581":"lda = LatentDirichletAllocation(n_components=11, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)","350eaff5":"lda.fit(tf)","56222309":"n_top_words = 40\nprint(\"\\nTopics in LDA model: \")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)","f7fd42c1":"first_topic = lda.components_[0]\nsecond_topic = lda.components_[1]\nthird_topic = lda.components_[2]\nfourth_topic = lda.components_[3]","d31feeea":"first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\nsecond_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]\nthird_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\nfourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]","918d2ba6":"# Word Cloud of first topic\n# Generating the wordcloud with the values under the category dataframe\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(first_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","4b47973d":"# Generating the wordcloud with the values under the category dataframe\ncloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(second_topic_words))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","fc030447":"**Grid Search CV**\n\nIts a technique for hyperparameter optimization. Not so effective but can give good results if you know the grid you want to use. I specify the parameters that should usually be used in this post: \n\nhttp:\/\/blog.kaggle.com\/2016\/07\/21\/approaching-almost-any-machine-learning-problem-abhishek-thakur\/ \n\nPlease keep in mind that these are the parameters I usually use. There are many other methods of hyperparameter optimization which may or may not be as effective.\n\nIn this section, I'll talk about grid search using logistic regression.\n\nBefore starting with grid search we need to create a scoring function. This is accomplished using the make_scorer function of scikit-learn.","55cde208":"**Cross Validation - Train-Test- Split**","69866fe8":"**Reference:**\n\nhttps:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle\n\nhttps:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial","6dbe7219":"This is now the perfect opportunity to piece together all the text preprocessing steps that was mentioned in the previous section. So you're asking yourself, do we really need to go through all the effort and steps again in defining tokenization, stopword removals, stemming\/lemmatizing etc?\n\nThankfully we do not need to go through all of that again. I conveniently omitted a key detail about Sklearn vectorizers but will mention it at this juncture. When you vectorize the raw text with CountVectorizer, the dual stages of tokenizing and stopwords filtering are automatically included as a high-level component. Here unlike the NLTK tokenizer that you were introduced to in the Section 2a earlier, Sklearn's tokenizer discards all single character terms like ('a', 'w' etc) and also lower cases all terms by default. Filtering out stopwords in Sklearn is as convenient as passing the value 'english' into the argument \"stop_words\" where a built-in English stopword list is automatically used.\n\nUnfortunately, there is no built-in lemmatizer in the vectorizer so we are left with a couple of options. Either implementing it separately everytime before feeding the data for vectorizing or somehow extend the sklearn implementation to include this functionality. Luckily for us, we have the latter option where we can extend the CountVectorizer class by overwriting the \"build_analyzer\" method as follows:","243a78b7":"**Attention Layer**","7bd22fff":"**Ensemble Modeling**","dd661457":"** Deep Learning **\n\nBut this is an era of deep learning! We cant live without training a few neural networks. Here, we will train LSTM and a simple dense network on the GloVe features. Let's start with the dense network first","05d61bef":"**Don't forget to upvote if you like it ^^ **","ef5e44eb":"**Building Basic Models**\n\nLet's start building our very first model.\n\nOur very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.","c7cef3f4":"You need to keep on tuning the parameters of the neural network, add more layers, increase dropout to get better results. Here, I'm just showing that its fast to implement and run and gets better result than xgboost without any optimization :)\n\nTo move further, i.e. with LSTMs we need to tokenize the text data","f7f6b83f":"**Latent Dirichlet Allocation**\n\nFinally we arrive on the subject of topic modelling and the implementation of a couple of unsupervised learning algorithms. The first method that I will touch upon is Latent Dirichlet Allocation. Now there are a couple of different implements of this LDA algorithm but in this notebook, I will be using Sklearn's implementation. Another very well-known LDA implementation is Radim Rehurek's gensim, so check it out as well.\n\nC**orpus - Document - Word : Topic Generation**\n\nIn LDA, the modelling process revolves around three things: the text corpus, its collection of documents, D and the words W in the documents. Therefore the algorithm attempts to uncover K topics from this corpus via the following way (illustrated by the diagram)\n\n![image.png](attachment:image.png)\n\nModel each topic,  \u03ba  via a Dirichlet prior distribution given by  \u03b2k :\n\n\n\nModel each document d by another Dirichlet distribution parameterized by  \u03b1 :\n\n\n\nSubsequently for document d, we generate a topic via a multinomial distribution which we then backtrack and use to generate the correspondings words related to that topic via another multinomial distribution:\n\n\n\n(Image source: http:\/\/scikit-learn.org\/stable\/modules\/decomposition.html#latentdirichletallocation)\n\nThe LDA algorithm first models documents via a mixture model of topics. From these topics, words are then assigned weights based on the probability distribution of these topics. It is this probabilistic assignment over words that allow a user of LDA to say how likely a particular word falls into a topic. Subsequently from the collection of words assigned to a particular topic, are we thus able to gain an insight as to what that topic may actually represent from a lexical point of view.\n\nFrom a standard LDA model, there are really a few key parameters that we have to keep in mind and consider programmatically tuning before we invoke the model:\n\n1. n_components: The number of topics that you specify to the model\n2. \u03b1  parameter: This is the dirichlet parameter that can be linked to the document topic prior\n3. \u03b2  parameter: This is the dirichlet parameter linked to the topic word prior\n\nTo invoke the algorithm, we simply create an LDA instance through the Sklearn's LatentDirichletAllocation function. The various parameters would ideally have been obtained through some sort of validation scheme. In this instance, the optimal value of n_components (or topic number) was found by conducting a KMeans + Latent Semantic Analysis Scheme (as shown in this paper here) whereby the number of Kmeans clusters and number of LSA dimensions were iterated through and the best silhouette mean score.","d71bdbc8":"**Topic Modeling**\n\n1. Latent Dirichlet Allocation - Probabilistic, generative model which uncovers the topics latent to a dataset by assigning weights to words in a corpus, where each topic will assign different probability weights to each word.\n\n2. Non-negative Matrix Factorization - Approximation method that takes an input matrix and approximates the factorization of this matrix into two other matrices, with the caveat that the values in the matrix be non-negative.","91a5d3c5":"**Revisiting our Term frequencies**\n\nHaving implemented our lemmatized count vectorizer, let us revist the plots for the term frquencies of the top 50 words (by frequency). As you can see from the plot, all our prior preprocessing efforts have not gone to waste. With the removal of stopwords, the remaining words seem much more meaningful where you can see that all the stopwords in the earlier term frequency plot","4f7e3c34":"**Topics generated by LDA**\n\nWe will utilise our helper function we defined earlier \"print_top_words\" to return the top 10 words attributed to each of the LDA generated topics. To select the number of topics, this is handled through the parameter n_components in the function.","bf329fc2":"**To be continued........**","c1e866d6":"**To be continued........^_^**","540e2a2c":"**Capsule Layer**"}}