{"cell_type":{"a36dd9c1":"code","1ac77cf5":"code","a7b3c374":"code","e87a9471":"code","c0ebb533":"code","796143d0":"code","0df65e4e":"code","dd0fa5fb":"code","0f62d029":"code","9b10fef1":"markdown","c25096ee":"markdown"},"source":{"a36dd9c1":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport tensorflow as tf\nfrom keras.preprocessing import text, sequence\nfrom nltk import word_tokenize\nfrom sklearn.metrics import roc_auc_score\nfrom IPython.display import clear_output","1ac77cf5":"MAX_LENGTH = 200\nBATCH_SIZE = 64\nEPOCHS = 10\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\ntwo_hunnid_k = False\ntrain_file = \"reddit_200k_train.csv\" if two_hunnid_k else \"reddit_train.csv\"\ntest_file = \"reddit_200k_test.csv\" if two_hunnid_k else \"reddit_test.csv\"\nx_col = \"body\" if two_hunnid_k else \"BODY\"\ny_col = \"REMOVED\"","a7b3c374":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype=\"float32\")\n\ndef load_embeddings(file):\n    with open(file) as f:\n        return dict(get_coefs(*line.strip().split(\" \")) for line in f)\n\ndef build_matrix(word_index, file):\n    embedding_index = load_embeddings(file)\n    embedding_matrix = np.zeros((len(word_index)+1, 300), dtype=np.float32)\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            continue\n    return embedding_matrix","e87a9471":"train_df = pd.read_csv(\"..\/input\/rscience-popular-comment-removal\/\" + train_file, encoding = \"ISO-8859-1\")\ntest_df = pd.read_csv(\"..\/input\/rscience-popular-comment-removal\/\" + test_file, encoding = \"ISO-8859-1\")\nx_train = train_df[x_col].values\ny_train = train_df[y_col].values.astype(np.int32)\nonehot = np.zeros((y_train.size, y_train.max()+1))\nonehot[np.arange(y_train.size), y_train] = 1\ny_train = onehot\nx_test = test_df[x_col].values\ny_test = test_df[y_col].values.astype(np.int32)\nonehot = np.zeros((y_test.size, y_test.max()+1))\nonehot[np.arange(y_test.size), y_test] = 1\ny_test = onehot\n\ndel train_df, test_df","c0ebb533":"tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\n# On one hand, including text from the test set in the \ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LENGTH)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LENGTH)","796143d0":"embedding_matrix = build_matrix(tokenizer.word_index, '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec')","0df65e4e":"val_size = 10000 if two_hunnid_k else 2000\nshuffled_indices = np.random.choice(x_train.shape[0], x_train.shape[0], replace=False)\nx_validation = x_train[shuffled_indices[-val_size:]]\ny_validation = y_train[shuffled_indices[-val_size:]]\nx_train = x_train[shuffled_indices[:-val_size]]\ny_train = y_train[shuffled_indices[:-val_size]]","dd0fa5fb":"tf.reset_default_graph()\ninputs = tf.placeholder(shape=(None, MAX_LENGTH), dtype=tf.int32)\nkeep_prob = tf.placeholder_with_default(1.0, shape=())\nkeep_prob_input = tf.placeholder_with_default(1.0, shape=())\nkeep_prob_conv = tf.placeholder_with_default(1.0, shape=())\ntargets = tf.placeholder(shape=(None, 2), dtype=tf.float32)\n\nembedding_layer = tf.Variable(embedding_matrix, trainable=False, name=\"embedding_layer\")\ninput_embeddings = tf.nn.embedding_lookup(embedding_layer, inputs)\nexpanded_inputs = tf.expand_dims(input_embeddings, 3)\nexpanded_inputs = tf.nn.dropout(expanded_inputs, keep_prob=keep_prob)\n\nconv_layer2 = tf.layers.conv2d(expanded_inputs, 16, (2, 300), (1, 1), activation=tf.nn.relu)\nconv_layer2 = tf.nn.dropout(conv_layer2, keep_prob=keep_prob_conv)\nconv_layer4 = tf.layers.conv2d(expanded_inputs, 16, (4, 300), (1, 1), activation=tf.nn.relu)\nconv_layer4 = tf.nn.dropout(conv_layer4, keep_prob=keep_prob_conv)\nconv_layer6 = tf.layers.conv2d(expanded_inputs, 16, (6, 300), (1, 1), activation=tf.nn.relu)\nconv_layer6 = tf.nn.dropout(conv_layer6, keep_prob=keep_prob_conv)\nconv_layer8 = tf.layers.conv2d(expanded_inputs, 16, (8, 300), (1, 1), activation=tf.nn.relu)\nconv_layer8 = tf.nn.dropout(conv_layer8, keep_prob=keep_prob_conv)\nsqueeze2 = tf.squeeze(conv_layer2, 2)\nsqueeze4 = tf.squeeze(conv_layer4, 2)\nsqueeze6 = tf.squeeze(conv_layer6, 2)\nsqueeze8 = tf.squeeze(conv_layer8, 2)\npool2 = tf.layers.max_pooling1d(squeeze2, MAX_LENGTH-2+1, 1)\npool4 = tf.layers.max_pooling1d(squeeze4, MAX_LENGTH-4+1, 1)\npool6 = tf.layers.max_pooling1d(squeeze6, MAX_LENGTH-6+1, 1)\npool8 = tf.layers.max_pooling1d(squeeze8, MAX_LENGTH-8+1, 1)\npools = [pool2, pool4, pool6, pool8]\npools = [tf.squeeze(x, 1) for x in pools]\nconcat_layers = tf.concat(pools, axis=1)\nhidden_layer = tf.layers.dense(concat_layers, 256, activation=tf.nn.relu)\nfinal_layer = tf.layers.dense(hidden_layer, 2, activation=tf.nn.softmax)\n\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.math.argmax(targets, 1), tf.math.argmax(final_layer,1)), tf.float32))\noptimizer = tf.train.AdamOptimizer(0.001)\nloss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=targets, logits=final_layer))\nvar = tf.trainable_variables() \nlossl2 = tf.add_n([ tf.nn.l2_loss(v) for v in var\n                    if 'bias' not in v.name and \"embedding\" not in v.name]) * 0.003\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\ntraining_op = optimizer.minimize(loss + lossl2)\ntraining_op = tf.group([training_op, update_ops])","0f62d029":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(EPOCHS):\n        avg_train_error = []\n        avg_train_acc = []\n        train_acc = 0\n        t0 = time.time()\n        for i in range((len(x_train) \/\/ BATCH_SIZE)):\n            x_batch = x_train[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n            y_batch = y_train[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n            feed = {inputs: x_batch, keep_prob: 0.5, keep_prob_input: 0.9, keep_prob_conv: 0.5, targets: y_batch}\n            _, train_loss, train_acc = sess.run([training_op, loss, accuracy], feed)\n            avg_train_error.append(train_loss)\n            avg_train_acc.append(train_acc)\n\n        clear_output(wait=True)\n        feed = {inputs:x_validation, targets: y_validation}\n        validation_err, validation_acc = sess.run([loss, accuracy], feed)            \n        shuffled_indices = np.random.choice(x_train.shape[0], x_train.shape[0], False)\n        x_train = x_train[shuffled_indices]\n        y_train = y_train[shuffled_indices]\n        print(\"Epoch: \" + str(epoch + 1))\n        print(\"Average error: {:.5f}\".format(sum(avg_train_error)\/len(avg_train_error)))\n        print(\"Average accuracy: {:.5f}\".format(sum(avg_train_acc)\/len(avg_train_acc)))\n        print(\"Validation error: {:.5f}\".format(validation_err))\n        print(\"Validation accuracy: {:.5f}\".format(validation_acc))\n\n    clear_output(wait=True)\n    feed = {inputs: x_test, targets: y_test}\n    test_loss, test_acc, pred = sess.run([loss, accuracy, final_layer], feed)\n    auc_score = roc_auc_score(y_test[:,1], pred[:,1])\n    print(\"Test error: {:.5f}\".format(test_loss))\n    print(\"Test accuray: {:.5f}\".format(test_acc))\n    print(\"AUC score: {:.5f}\".format(auc_score))    ","9b10fef1":"The architecture for this model is based on [A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification\n](https:\/\/arxiv.org\/abs\/1510.03820\/). The basic idea is to use a CNN with varying heights of kernels, with the kernel width being fixed to the embeddings size of the text. From this use a 1d max pooling to get a single value from each filter and concatenating theses filters together. Thus, if you have four different convolutional layers, the dimension of the concatenation would be 4 $\\cdot$ filter_size, which for my model is 4 $\\cdot$ 16. In the paper, the concatenated filters are feed directly into a classification layer, however I used an intermediary layer between the two.\n\nEven though using a sigmoid activation seems more logical here for binary classification, I notice that using softmax with softmax cross entropy outperforms sigmoid with sigmoid cross entropy by a fair margin.","c25096ee":"I initially wrote this up fairly quickly and looking back I have learned how to make the code a bit nicer, so here is an updated version."}}