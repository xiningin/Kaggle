{"cell_type":{"4b180161":"code","2e3348a2":"code","d9f02e46":"code","765ad4ca":"code","6c41882c":"code","d7f0d489":"code","eaefe93a":"code","6d84b1c2":"code","04397fc3":"code","e4a97d8f":"code","9f32b161":"code","afe114ac":"code","da7b4909":"code","e7c8fcca":"code","25aaf240":"code","ebddf2c2":"code","c7716a5c":"code","b46e698c":"code","33509974":"code","ec4caa88":"code","cfe08b3c":"code","8c51cc35":"code","9c8da147":"code","07b934c6":"code","f747fcb3":"code","45199fac":"code","e8c091c3":"code","77f3f0a3":"code","6a6826d7":"code","b3b6d4a8":"code","08028d73":"code","ac577235":"code","257f2b7f":"code","119d6d81":"code","4568b8e0":"code","530c0bee":"code","5b4b3ee7":"code","78b9e572":"code","3ce4384c":"code","8a916f08":"code","64314ef8":"code","31c80925":"code","7d34cabf":"code","0f06bf8e":"code","36e7bd7f":"code","be5df0ae":"code","4df59340":"code","4f989bb1":"code","2fa319db":"code","197e31d4":"code","fce3aad4":"code","3dc2e37b":"code","66ff2cc1":"code","406c5211":"code","17e52858":"code","3a7dd023":"code","71f933e6":"code","34e3245d":"code","39f862e1":"code","d3e30e4b":"code","d7a9857d":"code","71de46bf":"code","e98eaf83":"code","69c57a12":"code","203aa7ea":"code","cacee0a1":"code","1e39fe6d":"code","f21028bc":"code","016f5d72":"code","5ab235e5":"code","6e8d3295":"code","6d155ce4":"code","bcc3d0f0":"code","a3893bc7":"code","3490cd6d":"code","262cc9b1":"markdown","17089aa3":"markdown","86af6015":"markdown","7822994a":"markdown","c2d2d539":"markdown","1ba10787":"markdown","61ed7418":"markdown","d9629975":"markdown","00d4b2ec":"markdown","dacc5e10":"markdown","f29871e3":"markdown","72752b73":"markdown","411ecec1":"markdown","5edee74c":"markdown","9a1363ab":"markdown","b7bb8ddd":"markdown","60c7999d":"markdown","d7bb821b":"markdown","3221dc8b":"markdown","bbab71c7":"markdown","c502e96a":"markdown","b99f3c76":"markdown","a00bb87f":"markdown","20da9bf5":"markdown","c561ca3c":"markdown"},"source":{"4b180161":"# #for dicom files\n# !pip install -q pydicom","2e3348a2":"#importing the necessary libraries\nimport os\nimport re \nimport glob\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Pydicom related imports\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport tensorflow as tf\n# W&B for experiment tracking\n","d9f02e46":"# import wandb\n# wandb.login()","765ad4ca":"#reading the data\ntrain_data = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv')\nprint('Number of records: ', len(train_data))\ntrain_data.head()","6c41882c":"#checking the distribution of the class\nplt.figure(figsize=(5, 5))\nax = sns.countplot(\"MGMT_value\",data=train_data)\nax.set_title(\"Distribution of class Labels\");\nprint(train_data.MGMT_value.value_counts())","d7f0d489":"#checking total number of .dcm files\nfilenames = glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/*\/*\/*')\nprint(f'Total number of files: {len(filenames)}')","eaefe93a":"scan_label_dist = {\n    'FLAIR': [],\n    'T1w': [],\n    'T1wCE': [],\n    'T2w': []\n}\n\nfor filename in tqdm(filenames):\n    scan = filename.split('\/')[-2]\n    if scan=='FLAIR':\n        scan_label_dist['FLAIR'].append(filename)\n    elif scan=='T1w':\n        scan_label_dist['T1w'].append(filename)\n    elif scan=='T1wCE':\n        scan_label_dist['T1wCE'].append(filename)\n    else:\n        scan_label_dist['T2w'].append(filename)\n        \nprint(\"Number of files per scan:\")\nprint(\"<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>\")\nprint(\"\\n\")\nprint('Size of FLAIR scan: {}, T1w scan: {}, T1wCE scan: {}, T2w scan: {}'.format(len(scan_label_dist['FLAIR']), \n                                                                                  len(scan_label_dist['T1w']),\n                                                                                  len(scan_label_dist['T1wCE']),\n                                                                                  len(scan_label_dist['T2w'])))","6d84b1c2":"# CONFIG = {'IMG_SIZE': 224, \n#           'competition': 'rsna-miccai-brain', \n#           '_wandb_kernel': 'sakil786'}","04397fc3":"# run = wandb.init(project='brain-tumor-viz', config=CONFIG)\n# data = [['FLAIR', 74248], ['T1w', 77627], ['T1wCE', 96766], ['T2w', 100000]]\n# table = wandb.Table(data=data, columns = [\"Scan Type\", \"Num Files\"])\n# wandb.log({\"my_bar_chart_id\" : wandb.plot.bar(table, \"Scan Type\", \"Num Files\",\n#                                title=\"Scan Types vs Number of Dicom files\")})\n# run.finish()","e4a97d8f":"\ndef dicomFileTransformation(path, voi_lut = True, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","9f32b161":"data = dicomFileTransformation(filenames[16])\nprint('Shape of data: ', data.shape)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(data, cmap='gray');","afe114ac":"patient_ids = os.listdir('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train')\nIDX = np.random.choice(len(patient_ids))\n\npatient_id = patient_ids[IDX]\nprint(f'Patient ID: {patient_id}')","da7b4909":"def sorted_nicely(l): \n    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n    convert = lambda text: int(text) if text.isdigit() else text \n    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n    return sorted(l, key = alphanum_key)","e7c8fcca":"# #checking the number of images per patient id\n# flair_data = []\n# flair_filenames = os.listdir(f'..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{patient_id}\/FLAIR\/')\n# flair_filenames = sorted_nicely(flair_filenames)\n# for filename in flair_filenames:\n#     flair_data.append(dicomFileTransformation(f'..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/{patient_id}\/FLAIR\/{filename}'))\n    \n# print(f'Number of FLAIR images for the patient: {len(flair_data)}')","25aaf240":"# run = wandb.init(project='brain-tumor-viz', config=CONFIG)\n# wandb.log({f\"flair_{patient_id}\": [wandb.Image(image) for image in flair_data]})\n# run.finish()","ebddf2c2":"#copying the data\ntrain=train_data.copy()","c7716a5c":"#adding imfolder in the dataframe\ntrain[\"imfolder\"] = ['{0:05d}'.format(s) for s in train[\"BraTS21ID\"]]\ntrain.head()","b46e698c":"#adding the path in the dataframe\ntrain_path = \"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\"\n\ntrain[\"path\"] = [os.path.join(train_path,s) for s in train[\"imfolder\"]  ]\ntrain.head()","33509974":"scans_label = [\"FLAIR\",\"T1w\",\"T1wCE\",\"T2w\"]","ec4caa88":"#counting the number of dcm files per patient\nfor scan in scans_label:\n    train[scan +\"_count\"] = [ len(os.listdir(os.path.join(train[\"path\"].iloc[s],scan))) for s in tqdm(range(len(train))) ]","cfe08b3c":"train.head()","8c51cc35":"train.shape","9c8da147":"#checking which patient has same files in all types of scans\nallsame_column = [train[\"FLAIR_count\"].iloc[s] ==   train[\"T1w_count\"].iloc[s] ==train[\"T1wCE_count\"].iloc[s] ==train[\"T2w_count\"].iloc[s] \n          for s in range(len(train))]","07b934c6":"train[\"allsame_column\"] = allsame_column\ntrain","f747fcb3":"train[\"allsame_column\"].sum()","45199fac":"#checking the percentage of same dcm files\ntrain[\"allsame_column\"].sum()\/len(train) * 100","e8c091c3":"#printing same dcm files\ntrain[train[\"allsame_column\"]]","77f3f0a3":"def makepath(row_ID,scan):\n    \"\"\" Function to keep all categories files in their respective path\"\"\" \n    temp_folder = train[\"path\"].iloc[row_ID]\n    temp_folder2 = os.path.join(temp_folder,scan)\n    temp_files = os.listdir(temp_folder2)\n    imagenum = [s.split(\"-\")[1] for s in temp_files]\n    imagenum = [s.split(\".\")[0] for s in imagenum]\n    temp_path = [os.path.join(temp_folder2,s) for s in temp_files]\n    tempdf = pd.DataFrame()\n    tempdf[\"image_num\"] = imagenum\n    tempdf[\"image_num\"] = tempdf[\"image_num\"].astype(\"int\")\n    tempdf[\"temp_path\"] = temp_path\n    tempdf = tempdf.sort_values(\"image_num\").reset_index(drop=True)\n    finpath = tempdf[\"temp_path\"]\n    return finpath","6a6826d7":"scans_label = [\"FLAIR\",\"T1w\",\"T1wCE\",\"T2w\"]","b3b6d4a8":"row_id=64\n\nsampledf = pd.DataFrame()\nfor scan in scans_label:\n    sampledf[scan + \"_path\"] = makepath(row_id,scan)","08028d73":"sampledf.head(3)\n","ac577235":"sampledf['FLAIR_path'].head()","257f2b7f":"def makeimg(path):\n    dataset = pydicom.filereader.dcmread(path)\n    img = dataset.pixel_array\n    return img","119d6d81":"print(\"MGMT_value = \" + str(train[\"MGMT_value\"].iloc[row_id]))\n\n#showing the scans for patient_id=100 and row_id=64 ,whose MGMT_value=1\nfor row in range(len(sampledf)):\n    plt.figure(figsize=(80,10))\n    for num,scan in enumerate(scans_label):\n        img = makeimg(sampledf[scan + \"_path\"].iloc[row])\n        plt.subplot(4,25,num+1)\n        #plt.axis(\"off\")\n        plt.imshow(img)\n        \n        if row==0:\n            plt.title(scan,fontsize=18)\n        if num==0:\n            plt.ylabel(\"row=\" + str(row),fontsize=18)","4568b8e0":"row_id = 65\n\nsampledf = pd.DataFrame()\nfor scan in scans_label:\n    sampledf[scan + \"_path\"] = makepath(row_id,scan)\n#showing the scans for patient_id=100 and row_id=65 ,whose MGMT_value=0\nprint(\"MGMT_value = \" + str(train[\"MGMT_value\"].iloc[row_id]))\n\nfor row in range(len(sampledf)):\n    plt.figure(figsize=(80,10))\n    for num,scan in enumerate(scans_label):\n        img = makeimg(sampledf[scan + \"_path\"].iloc[row])\n        plt.subplot(4,25,num+1)\n        #plt.axis(\"off\")\n        plt.imshow(img)\n        \n        if row==0:\n            plt.title(scan,fontsize=18)\n        if num==0:\n            plt.ylabel(\"row=\" + str(row),fontsize=18)\n\n","530c0bee":"import os\nimport glob\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms, utils\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport imgaug as ia\nimport imgaug.augmenters as iaa\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","5b4b3ee7":"!pip install efficientnet-pytorch","78b9e572":"# rm efficientnet-pytorch","3ce4384c":"import sys\nsys.path.append('..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D')\nsys.path.append('..\/input\/efficientnet-pytorch2')\nsys.path.append('..\/input\/efficientnet\/EfficientNet-PyTorch-master\/')","8a916f08":"path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\ntrain_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\nprint('Num of train samples:', len(train_data))\ntrain_data.head()","64314ef8":"sometimes = lambda aug: iaa.Sometimes(0.1, aug)\n\nseq = iaa.Sequential(\n    [\n        # apply the following augmenters to most images\n        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n        iaa.Flipud(0.5), # vertically flip 20% of all images\n        # crop images by -5% to 10% of their height\/width\n        sometimes(iaa.CropAndPad(\n            percent=(-0.05, 0.05),\n            pad_mode=ia.ALL,\n            pad_cval=(0, 255)\n        )),\n        sometimes(iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n            rotate=(-45, 45), # rotate by -45 to +45 degrees\n            shear=(-16, 16), # shear by -16 to +16 degrees\n            order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n            cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n        )),\n        # execute 0 to 5 of the following (less important) augmenters per image\n        # don't execute all of them, as that would often be way too strong\n        iaa.SomeOf((0, 5),\n            [\n                sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n                iaa.OneOf([\n                    iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n                    iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n                    iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n                ]),\n                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n                # search either for all edges or for directed edges,\n                # blend the result with the original image using a blobby mask\n                iaa.SimplexNoiseAlpha(iaa.OneOf([\n                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n                    iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n                ])),\n                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n                iaa.OneOf([\n                    iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n                    iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n                ]),\n                iaa.Invert(0.05, per_channel=True), # invert color channels\n                iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n                \n                # either change the brightness of the whole image (sometimes\n                # per channel) or change the brightness of subareas\n                iaa.OneOf([\n                    iaa.Multiply((0.5, 1.5), per_channel=0.5),\n                    iaa.FrequencyNoiseAlpha(\n                        exponent=(-4, 0),\n                        first=iaa.Multiply((0.5, 1.5), per_channel=True),\n                        second=iaa.LinearContrast((0.5, 2.0))\n                    )\n                ]),\n                iaa.LinearContrast((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n                sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n            ],\n            random_order=True\n        )\n    ],\n    random_order=True\n)","31c80925":"def dicom2array(paths, voi_lut=True, fix_monochrome=True, remove_black_boundary=True, aug = False):\n    \n    for path in paths:\n        dicom = pydicom.read_file(path)\n        # VOI LUT (if available by DICOM device) is used to\n        # transform raw DICOM data to \"human-friendly\" view\n        if voi_lut:\n            data = apply_voi_lut(dicom.pixel_array, dicom)\n        else:\n            data = dicom.pixel_array\n        if data.max() > 0.0: # avoiding black images (if possible)\n            break\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    if remove_black_boundary: # we get slightly more details\n        (x, y) = np.where(data > 0)\n        if len(x) > 0 and len(y) > 0:\n            x_mn = np.min(x)\n            x_mx = np.max(x)\n            y_mn = np.min(y)\n            y_mx = np.max(y)\n            if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n                data = data[:,np.min(y):np.max(y)]\n    data = cv2.resize(data, (512, 512))\n    if aug and random.randint(0,1) == 1: # augmenting only 50% of the time\n        data = seq(images=data)\n    return data\n\n\ndef load_3d_dicom_images(scan_id, split = \"train\", channel_expand = True):\n    \"\"\"\n    we will use some heuristics to choose the slices to avoid any numpy zero matrix (if possible)\n    \"\"\"\n    flair = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/FLAIR\/*.dcm\"))\n    t1w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1w\/*.dcm\"))\n    t1wce = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1wCE\/*.dcm\"))\n    t2w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T2w\/*.dcm\"))\n    \n    \n    flair_img = np.array([dicom2array(a) for a in flair[len(flair)\/\/2 - 32:len(flair)\/\/2 + 32]]).T\n    \n    if len(flair_img) == 0:\n        flair_img = np.zeros((256, 256, 64))\n    elif flair_img.shape[-1] < 64:\n        n_zero = 64 - flair_img.shape[-1]\n        flair_img = np.concatenate((flair_img, np.zeros((256, 256, n_zero))), axis = -1)\n    #print(flair_img.shape)\n        \n    \n    \n    t1w_img = np.array([dicom2array(a) for a in t1w[len(t1w)\/\/2 - 32:len(t1w)\/\/2 + 32]]).T\n    \n    if len(t1w_img) == 0:\n        t1w_img = np.zeros((256, 256, 64))\n    elif t1w_img.shape[-1] < 64:\n        n_zero = 64 - t1w_img.shape[-1]\n        t1w_img = np.concatenate((t1w_img, np.zeros((256, 256, n_zero))), axis = -1)\n    #print(t1w_img.shape)\n    \n    \n    t1wce_img = np.array([dicom2array(a) for a in t1wce[len(t1wce)\/\/2 - 32:len(t1wce)\/\/2 + 32]]).T\n    \n    if len(t1wce_img) == 0:\n        t1wce_img = np.zeros((256, 256, 64))\n    elif t1wce_img.shape[-1] < 64:\n        n_zero = 64 - t1wce_img.shape[-1]\n        t1wce_img = np.concatenate((t1wce_img, np.zeros((256, 256, n_zero))), axis = -1)\n    #print(t1wce_img.shape)\n    \n    \n    t2w_img = np.array([dicom2array(a) for a in t2w[len(t2w)\/\/2 - 32:len(t2w)\/\/2 + 32]]).T\n    \n    if len(t2w_img) == 0:\n        t2w_img = np.zeros((256, 256, 64))\n    elif t2w_img.shape[-1] < 64:\n        n_zero = 64 - t2w_img.shape[-1]\n        t2w_img = np.concatenate((t2w_img, np.zeros((256, 256, n_zero))), axis = -1)\n    #print(t2w_img.shape)\n    \n    return np.concatenate((flair_img, t1w_img, t1wce_img, t2w_img), axis = -1) if not channel_expand else np.moveaxis(np.array((flair_img, t1w_img, t1wce_img, t2w_img)), 0, -1)\n\n\ndef load_rand_dicom_images(scan_id, split = \"train\", aug = False):\n    \"\"\"\n    send 4 random slices of each modality\n    \"\"\"\n    if split != \"train\" and split != \"test\":\n        split = \"train\"\n    flair = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/FLAIR\/*.dcm\"))\n    flair_img = dicom2array(random.sample(flair, max(len(flair)\/\/2, 1)), aug = aug)\n    t1w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1w\/*.dcm\"))\n    t1w_img = dicom2array(random.sample(t1w, max(len(t1w)\/\/2, 1)), aug = aug)\n    t1wce = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1wCE\/*.dcm\"))\n    t1wce_img = dicom2array(random.sample(t1wce, max(len(t1wce)\/\/2, 1)), aug = aug)\n    t2w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T2w\/*.dcm\"))\n    t2w_img = dicom2array(random.sample(t2w, max(len(t2w)\/\/2, 1)), aug = aug)\n    \n    return np.array((flair_img, t1w_img, t1wce_img, t2w_img)).T","7d34cabf":"# let's write a simple pytorch dataloader\n\nclass BrainTumor4C(Dataset): # 4 channel data-loader\n    def __init__(self, path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification', split = \"train\", validation_split = 0.2):\n        # labels\n        train_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n        self.labels = {}\n        brats = list(train_data[\"BraTS21ID\"])\n        mgmt = list(train_data[\"MGMT_value\"])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        if split == \"valid\":\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/train\/\" + \"\/*\"))]\n            self.ids = self.ids[:int(len(self.ids)* validation_split)] # first 20% as validation\n        elif split == \"train\":\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            self.ids = self.ids[int(len(self.ids)* validation_split):] # last 80% as train\n        else:\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            \n    \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        imgs = load_rand_dicom_images(self.ids[idx], self.split, aug = False)\n        \n        transform = transforms.Compose([transforms.ToTensor()]) # transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5))\n        imgs = transform(imgs)\n        \n        imgs = imgs - imgs.min()\n        imgs = (imgs + 1e-5) \/ (imgs.max() - imgs.min() + 1e-5)\n        \n        if self.split != \"test\":\n            label = self.labels[self.ids[idx]]\n            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n        else:\n            return torch.tensor(imgs, dtype = torch.float32), self.ids[idx]","0f06bf8e":"# testing the dataloader\ntest_dataset = BrainTumor4C(split = \"test\")\ntest_bs = 8\ntest_loader = DataLoader(test_dataset, batch_size = test_bs, shuffle=True)","36e7bd7f":"for img, idx in test_loader:\n    print(img.shape)\n    print(img.max())\n    print(img.mean())\n    print(img.min())\n    print(idx)\n    break","be5df0ae":"!pip install efficientnet_pytorch","4df59340":"MODEL = \"4C\" # [\"4C\", \"3D\", \"4C+3D\"]","4f989bb1":"# !mkdir rsna-efficientnet3db0","2fa319db":"if MODEL == \"3D\":\n    PATH = \"..\/input\/rsna-efficientnet3db0\/best_roc_0.29_loss_1826.83.pt\"\n    model = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=4)\n    model.load_state_dict(torch.load(PATH))\n    model.eval()","197e31d4":"# !pip install --upgrade efficientnet-pytorch\n","fce3aad4":"# !mkdir effb14c1\/best_roc_0.35_loss_39.450.pt","3dc2e37b":"# PATH = \"best_roc_0.35_loss_39.450.pt\"\n# torch.save(model.state_dict(), PATH)","66ff2cc1":"# PATH=\"best_roc_0.35_loss_39.451.pt\"\n# torch.save(model.state_dict(), PATH)","406c5211":"# !pip install efficientnet_pytorch","17e52858":"# pwd","3a7dd023":"# cd ..\/input\/efficientnetpytorch","71f933e6":"# cd EfficientNet-PyTorch-master","34e3245d":"# cd efficientnet_pytorch","39f862e1":"# pip install -e","d3e30e4b":"# cd EfficientNet-PyTorch-master","d7a9857d":"# cd efficientnet_pytorch","71de46bf":"# pip install -e .","e98eaf83":"# ..\/input\/efficientnetpytorch\/EfficientNet-PyTorch-master\/efficientnet_pytorch","69c57a12":"# cd ..\/input\/efficientnetpytorch\n","203aa7ea":"# !pip install efficientnet_pytorch","cacee0a1":"# cd ..\/input\/efficientnetpytorch\/EfficientNet-PyTorch-master\/efficientnet_pytorch","1e39fe6d":"# if MODEL == \"4C\":\n#     from efficientnet_pytorch import EfficientNet\n#     from efficientnet_pytorch.utils import Conv2dStaticSamePadding\n\n#     PATH = \"best_roc_0.35_loss_39.451.pt\"\n#     model = EfficientNet.from_name('efficientnet-b1')\n\n#     # augment model with 4 channels\n\n#     model._conv_stem = Conv2dStaticSamePadding(4, 32, kernel_size = (3,3), stride = (2,2), \n#                                                                  bias = False, image_size = 512)\n#     model._fc = torch.nn.Linear(in_features=1280, out_features=2, bias=True)\n    \n#     model.load_state_dict(torch.load(PATH))\n#     model.eval()","f21028bc":"# PATH = \"effb14c1\/best_roc_0.35_loss_39.451.pt\"\n# torch.save(model.state_dict(), PATH)","016f5d72":"if MODEL == \"4C\":\n    from efficientnet_pytorch import EfficientNet\n    from efficientnet_pytorch.utils import Conv2dStaticSamePadding\n\n    PATH = \"..\/input\/effb14c1\/best_roc_0.35_loss_39.45.pt\"\n    model = EfficientNet.from_name('efficientnet-b1')\n\n    # augment model with 4 channels\n\n    model._conv_stem = Conv2dStaticSamePadding(4, 32, kernel_size = (3,3), stride = (2,2), \n                                                                 bias = False, image_size = 512)\n    model._fc = torch.nn.Linear(in_features=1280, out_features=2, bias=True)\n    \n    model.load_state_dict(torch.load(PATH))\n    model.eval()","5ab235e5":"# test\ngpu = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(gpu)\nn_bootstrap = 2\n\nif MODEL == \"4C\":\n    labels = {}\n\n    model.eval()\n    for i_b in range(n_bootstrap):\n        for i, data in tqdm(enumerate(test_loader, 0)):\n\n            x, idx = data\n\n            x = x.to(gpu)\n\n            # forward\n            outputs = model(x)\n\n            label = torch.argmax(outputs, dim = -1)\n\n            # print(idx)\n            # print(label)\n\n            label = label.tolist()\n            for i_, idx_ in enumerate(list(idx)):\n                labels[idx_] = labels.get(idx_, []) + [label[i_]]\n                \n            # break","6e8d3295":"import collections\nlabels_od = collections.OrderedDict(sorted(labels.items()))\nprint(labels_od)","6d155ce4":"f_idxs = []\nf_labels = []\nfor idx in labels_od.keys():\n    f_idxs.append(int(idx))\n    f_labels.append(np.array(labels_od[idx], dtype = np.float32).mean())    ","bcc3d0f0":"submission = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")\n\nfor i, row in submission.iterrows():\n    idx = int(row['BraTS21ID'])\n    try:\n        new_label = f_labels[f_idxs.index(idx)]\n        submission.loc[i, 'MGMT_value'] = float(new_label)\n    except:\n        pass","a3893bc7":"submission.head()","3490cd6d":"submission.to_csv(\"submission.csv\", index=False)\nsubmission","262cc9b1":"# Data Loader","17089aa3":"# Model: EfficientNet-3D B0 \/ EfficientNet B1","86af6015":"# **Training data Information**\n\n* There are 585 patients and each patient is identified by unique id,BraTS21ID.\n* Each patient consists of four structural multi-parametric MRI (mpMRI) scans.They are:\n    * Fluid Attenuated Inversion Recovery (FLAIR)\n    * T1-weighted pre-contrast (T1w)\n    * T1-weighted post-contrast (T1Gd)\n    * T2-weighted (T2)\n* Each sub-folder contains multiple scans in Dicom format with name - Image-X.dcm where X increases from 1 to N. \n* MGMT_value 1 corresponds to the presence of MGMT promoter and 0 corresponds to absence.","7822994a":"# **Analysis**\n\n* There are 348641 dcm files","c2d2d539":"# **Model Training**","1ba10787":"# Barchart for different types of Scan\n\n","61ed7418":"![kaggle.png](attachment:9e927883-8b04-43a1-8572-269675ce32ae.png)","d9629975":"# **Different types of Scan**\n![kaggle1.png.png](attachment:43d1d73d-b530-4356-ab7d-8482ac53bf5d.png)","00d4b2ec":"# **MRI and Scan Types**\n\n* Magnetic resonance imaging (MRI) is one of the most commonly used tests in neurology and neurosurgery. \n* MRI provides exquisite detail of brain, spinal cord and vascular anatomy, and has the advantage of being able to visualize anatomy in all three planes: axial, sagittal and coronal.\n* T1, T2 and FLAIR are different MRI Imaging","dacc5e10":"# **Checking the distribution of files based on scan types**","f29871e3":"# **Analysis**\n\n* 63 patients have same number of dcm files in all scan","72752b73":"# **Checking total number of files (.dcm)**","411ecec1":"# **Analysis**\n* There are 23 dcm files for the patient_id 102 whose MGMT_value=0","5edee74c":"# **Note**\n* MONOCHROME1 indicates that the greyscale ranges from bright to dark with ascending pixel values\n*  MONOCHROME2 ranges from dark to bright with ascending pixel values.\n* The Value Of Interest(VOI) LUT transformation transforms the modality pixel values into pixel values which are meaningful for the user or the application. \n","9a1363ab":"# **Analysis**\n* There are 22 dcm files for the patient_id 100 whose MGMT_value=1","b7bb8ddd":"# **Brain Tumor Radiogenomic Classification**\n\n* A malignant tumor in the brain is a life-threatening condition which is known as glioblastoma, it's both the most common form of brain cancer in adults and the one with the worst prognosis, with median survival being less than a year.\n* Glioblastoma is a life-threatning brain tumor which is caused by the presence of a specific genetic sequence in the tumor known as MGMT promoter methylation.\n* In today's world ,genetic analysis of cancer requires surgery to extract a tissue sample and it can take several weeks to determine the genetic characterization of the tumor.\n* Depending upon the results and type of initial therapy chosen, a subsequent surgery may be necessary. ","60c7999d":"# **Objective**\n\n* To develope an accurate method in  predicting the genetics of the cancer through imaging which will help in reducing the sugeries required to perform an biopsy to determine if there is a tumor or not. ","d7bb821b":"# **Reading DICOM File**","3221dc8b":"# **Analysis**\n\n* The dataset is not entirely balanced.","bbab71c7":"\n   ![kaggle4.png](attachment:b0833cec-8283-4fa6-a357-e46365f83295.png)","c502e96a":"# MRI Slice Loading\/Processing","b99f3c76":"# inspecting Labels","a00bb87f":"# **Analysis**\n\n* Each MRI image has a resolution of 512x512 pixels.","20da9bf5":"# **Analysis**\n\n* There are 74248 FAIR scan\n* There are 77627 T1w scan\n* There are 96766 T1wCE scan\n* There are 100000 T2w scan\n","c561ca3c":"# Augmentation"}}