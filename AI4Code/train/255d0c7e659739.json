{"cell_type":{"9ee4dc2e":"code","c40f3017":"code","9f0f3505":"code","9f510276":"code","e481013d":"code","dc99a6c6":"code","e0446e0b":"code","8bfa304b":"code","845a76e9":"code","c43c39f8":"code","1435f2d4":"code","8752024e":"code","66d76f93":"code","66072d13":"code","a6957e58":"code","3536a927":"code","f94f02db":"code","4f9087ad":"code","eac1f1f4":"code","1f240ba5":"code","54b87c09":"code","87c30c16":"code","99f994c8":"code","ef3a8c6f":"code","2fb15b4f":"code","9821c8f1":"code","3f88f5c6":"code","8e5f29a0":"code","77464b11":"code","c8fa67e2":"code","fc9cb873":"code","e1d9f8a8":"code","ff3b6449":"code","e1382f3d":"code","6e578ae2":"code","1b8c7b2b":"code","b9e4bccd":"code","b5175470":"code","3d8077ad":"code","3a55b117":"code","a595d431":"code","41fb6bc9":"code","ad2cb465":"code","2b45469c":"code","d9b8abe2":"code","9c676d21":"code","db623cc0":"code","e37223cd":"code","895d0bcc":"code","097497a6":"code","e89e3d81":"code","72f7cee6":"code","b8f4e3e6":"code","cc7a8628":"code","c71d934d":"code","c0065887":"code","a8f1760d":"code","53012682":"code","3e86f759":"code","e102e649":"code","16e17e76":"code","350afb40":"code","321a66be":"code","b076de41":"code","c18a87c6":"code","c08b7cbd":"code","3fa2fe02":"code","6ff77cc1":"code","453ee0f4":"code","4279c2e2":"code","81edafdf":"code","034db54c":"code","0a4eb151":"code","24d97b47":"code","779215ad":"code","759c7f78":"code","4c783bf1":"code","75707adf":"code","7dfcb807":"markdown","b7cc1db0":"markdown","80422f64":"markdown","013b5cd0":"markdown","91b14823":"markdown","83df0b05":"markdown","e32d5fad":"markdown","5d4f02d3":"markdown","498cef11":"markdown","bd018e20":"markdown","f883ac48":"markdown","d3a1fa40":"markdown","4511a5b9":"markdown","b011b129":"markdown","1815a14d":"markdown","cbe273aa":"markdown","da670ee8":"markdown","9205e710":"markdown","8f58ddbf":"markdown","92fe3eb8":"markdown","6f924b29":"markdown","64c5d56a":"markdown","76d8652b":"markdown","f9fb6f02":"markdown","0e041cc0":"markdown","b3862545":"markdown","3034ab0d":"markdown","c0dbca72":"markdown","235c317b":"markdown","8f7c103c":"markdown","35ea53a7":"markdown","ac310c09":"markdown","d406d81d":"markdown","6edc433a":"markdown","62b17965":"markdown","38288c3c":"markdown","2e9a6e4e":"markdown","99719e5d":"markdown","6b65f703":"markdown","ab534e5b":"markdown","844e28b6":"markdown","baa59e0e":"markdown","ad6812d0":"markdown","4ffdff0d":"markdown","49eebec2":"markdown","1addc58f":"markdown","d01938bd":"markdown","69fd4ff3":"markdown","b27ca915":"markdown","b4a611e4":"markdown","12064f3a":"markdown","6666975d":"markdown","3d6beee7":"markdown","c0cbbba0":"markdown","16ececc7":"markdown","b85b86ab":"markdown","4796d488":"markdown","12be2111":"markdown","695cbabb":"markdown","aa51b357":"markdown","cd00772e":"markdown","7fa49dee":"markdown","c4b27e38":"markdown","ac29b071":"markdown","b352182d":"markdown","02bbdab1":"markdown","980c857a":"markdown","6861175f":"markdown","b2be1d5d":"markdown","7feb8b92":"markdown","80dfe3f4":"markdown","162634ea":"markdown","95b6adfe":"markdown","0de163b7":"markdown","e4cc02a3":"markdown","e47b2d2c":"markdown"},"source":{"9ee4dc2e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# preprocessing\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\n\n# ML\nfrom sklearn. model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier","c40f3017":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_set = [train, test]","9f0f3505":"train.head()","9f510276":"test.head()","e481013d":"train.describe().T","dc99a6c6":"# Correlation matrix\nheatmap = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","e0446e0b":"pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))","8bfa304b":"train.info()","845a76e9":"msno.matrix(train, figsize = (30,10));","c43c39f8":"msno.heatmap(train);","1435f2d4":"train.isnull().sum()","8752024e":"# Survived\nfig = plt.figure( figsize = (17,1))\nsns.countplot( y = \"Survived\", data = train);\nprint(train.Survived.value_counts())","66d76f93":"# Pclass \ntrain.Pclass.unique()","66072d13":"# Barplot for categorical feature\nprint(train[\"Survived\"][train['Pclass'] == 1].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train['Pclass'] == 2].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train['Pclass'] == 3].value_counts(normalize = True)[1])\nsns.barplot(x = \"Pclass\", y = \"Survived\", data = train);","a6957e58":"a = train[\"Sex\"].value_counts()\ncolors = ['#ff9999', '#ffcc99']\ngenders = [\"female\",\"male\"]\nplt.pie(a, labels= genders, colors = colors, startangle=90, autopct='%.1f%%', shadow = True )\nplt.tight_layout()\nplt.show()","3536a927":"sns.barplot(x = \"Sex\", y = \"Survived\", data = train);\nprint(\"Probability of Survived Male Passenger: \", train[\"Survived\"][train['Sex'] == \"male\"].value_counts(normalize = True)[1])\nprint(\"Probability of Survived Female Passenger: \", train[\"Survived\"][train['Sex'] == \"female\"].value_counts(normalize = True)[1])","f94f02db":"for dataset in df_set:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","4f9087ad":"train['Age'].hist(bins=30, color='red',alpha=0.8, edgecolor='black');","eac1f1f4":"train.Age.value_counts()","1f240ba5":"train[\"Age\"].isnull().sum()","54b87c09":"heatmap = sns.heatmap(train[[\"Survived\",\"Sex\",\"Pclass\",\"Age\",\"Fare\",\"SibSp\",\"Parch\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","87c30c16":"print(train[[\"Pclass\",\"Age\"]].groupby([\"Pclass\"], as_index = False).median())\nsns.boxplot(x = \"Pclass\", y= \"Age\" , data = train);","99f994c8":"print(train[[\"SibSp\",\"Age\"]].groupby([\"SibSp\"], as_index = False).median())\nsns.boxplot(x = \"SibSp\", y= \"Age\" , data = train);","ef3a8c6f":"for dataset in df_set:\n    miss_age_index = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n    for i in miss_age_index :\n        median_age= dataset[\"Age\"].median()\n        fill_age = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n        if not np.isnan(fill_age) :\n            dataset['Age'].iloc[i] = fill_age\n        else :\n            dataset['Age'].iloc[i] = median_age\n    dataset['Age'] = dataset['Age'].astype(int)","2fb15b4f":"train[\"Age\"].isnull().sum()\ntrain['Age'].hist(bins=30, color='red',alpha=0.8, edgecolor='black');","9821c8f1":"sns.factorplot(x=\"Survived\", y = \"Age\", data = train, kind=\"violin\");\n","3f88f5c6":"# After we filled the missing values, we can convert it into integer which was float data type and make binning.\ntrain['Age_bin'] = pd.cut(train['Age'], 5)\ntrain[['Age_bin', 'Survived']].groupby(['Age_bin'], as_index=False).mean().sort_values(by='Age_bin', ascending=True)","8e5f29a0":" for dataset in df_set:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), \"Age\"] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), \"Age\"] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), \"Age\"] = 3\n    dataset.loc[ dataset['Age'] > 64, \"Age\"] = 4\n\ntrain = train.drop([\"Age_bin\"], axis = 1)\ndf_set = [train, test]","77464b11":"print(train[\"Survived\"][train[\"Age\"] == 0 ].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train[\"Age\"] == 1].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train[\"Age\"] == 2 ].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train[\"Age\"] == 3 ].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train[\"Age\"] == 4 ].value_counts(normalize = True)[1])\nsns.barplot(x = train[\"Age\"], y = train[\"Survived\"], data = train );","c8fa67e2":"sns.barplot(x = train[\"Age\"], y = train[\"Survived\"], hue = train[\"Sex\"], data = train);","fc9cb873":"print(train[[\"Embarked\", \"Survived\"]].groupby([\"Embarked\"], as_index = False).mean())\nsns.barplot(x='Embarked', y='Survived', data=train ,order=['S','C','Q']);","e1d9f8a8":"pd.crosstab([train.Embarked, train.Pclass], [train.Sex, train.Survived], margins = True).style.background_gradient(cmap = \"Reds\")","ff3b6449":"train[\"Embarked\"].isnull().sum()","e1382f3d":"train[\"Embarked\"].value_counts()","6e578ae2":"for dataset in df_set:\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")","1b8c7b2b":"for dataset in df_set:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","b9e4bccd":"train[\"Parch\"].value_counts()","b5175470":"sns.barplot(x='Parch', y='Survived', data=train);","3d8077ad":"train[\"SibSp\"].value_counts()","3a55b117":"sns.barplot(x= \"SibSp\", y=\"Survived\", data = train);","a595d431":"for dataset in df_set:\n    dataset['Has_Family'] =  dataset[\"Parch\"] + dataset[\"SibSp\"]\n    dataset['Has_Family'].loc[dataset['Has_Family'] > 0] = 1\n    dataset['Has_Family'].loc[dataset['Has_Family'] == 0] = 0\n\ntrain = train.drop([\"SibSp\",\"Parch\"], axis = 1 )\ntest = test.drop(['Parch', 'SibSp'], axis=1)\ndf_set = [train, test]","41fb6bc9":"fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,6))\nsns.countplot(x='Has_Family', data=train, order=[0,1], ax = axis1);\nmean_family = train[[\"Has_Family\", \"Survived\"]].groupby(['Has_Family'],as_index=False).mean()\nsns.barplot(x='Has_Family', y='Survived', data=mean_family, order=[0,1], ax = axis2);\naxis1.set_xticklabels([\"Alone\",\"With Family\"], rotation=0);","ad2cb465":"train[\"Name\"].nunique()","2b45469c":"train = train.drop([\"Name\", \"PassengerId\"], axis = 1)\ntest = test.drop([\"Name\"], axis = 1)\ndf_set = [train, test]","d9b8abe2":"test.Fare.isnull().sum()","9c676d21":"test['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)","db623cc0":"train[\"Fare\"].nunique()","e37223cd":"train['Fare_bin'] = pd.qcut(train['Fare'], 4)\ntrain[['Fare_bin', 'Survived']].groupby(['Fare_bin'], as_index=False).mean().sort_values(by='Fare_bin', ascending=True)","895d0bcc":"for dataset in df_set:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \ntrain = train.drop([\"Fare_bin\"], axis = 1)\ndf_set = [train, test]","097497a6":"print(train[\"Survived\"][train[\"Fare\"] == 0 ].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train[\"Fare\"] == 1].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train[\"Fare\"] == 2 ].value_counts(normalize = True)[1])\nprint(train[\"Survived\"][train[\"Fare\"] == 3 ].value_counts(normalize = True)[1])\n\nsns.barplot(x = train[\"Fare\"], y = train[\"Survived\"], data = train );","e89e3d81":"sns.barplot(x = train[\"Fare\"], y = train[\"Survived\"], hue = train[\"Sex\"]);","72f7cee6":"train.Cabin.value_counts()","b8f4e3e6":"train = train.drop([\"Cabin\"], axis = 1)\ntest = test.drop([\"Cabin\"], axis = 1)\ndf_set = [train, test]","cc7a8628":"train.Ticket.value_counts()","c71d934d":"train = train.drop([\"Ticket\"], axis=1)\ntest = test.drop([\"Ticket\"], axis=1)\ndf_set = [train, test]","c0065887":"train.head()","a8f1760d":"test.head()","53012682":"X_train = train.drop([\"Survived\"], axis=1)\ny_train = train[\"Survived\"]\nX_test = test.drop(\"PassengerId\", axis = 1).copy()\n","3e86f759":"knn = KNeighborsClassifier()\nknn_model = knn.fit(X_train, y_train)\n\ny_pred_knn = knn_model.predict(X_test)\naccuracy = round(knn.score(X_train, y_train)*100, 2)\nprint(\"Accuracy before the model tuning is :\" + str(accuracy))\n#model tuning\nknn_params = {'n_neighbors' : np.arange(1, 50),\n              'weights': ['uniform', 'distance']}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv= 10)\nknn_cv.fit(X_train, y_train)\nprint(knn_cv.best_params_)","e102e649":"knn = KNeighborsClassifier(n_neighbors = 48, weights = \"distance\")\nknn_tuned = knn.fit(X_train, y_train)\ny_pred_knn_tuned = knn.predict(X_test)\ntuned_accuracy = round(knn_tuned.score(X_train, y_train)*100, 2)\nprint(\"Accuracy after the model tuning : \" + str(tuned_accuracy))\nknn_cv_score = cross_val_score(knn_tuned, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(knn_cv_score))","16e17e76":"loj = LogisticRegression(solver = \"liblinear\")\nloj_model = loj.fit(X_train, y_train)\ny_pred_lr = loj_model.predict(X_test)\naccuracy = round(loj.score(X_train, y_train)*100, 2)\nprint(\"Accuracy is :\" + str(accuracy))\nloj_cv_score = cross_val_score(loj, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(loj_cv_score))\n","350afb40":"nb = GaussianNB()\nnb_model = nb.fit(X_train, y_train)\n\ny_pred_nb = nb_model.predict(X_test)\naccuracy = round(nb.score(X_train, y_train)*100, 2)\nprint(\"Accuracy is :\" + str(accuracy))\nnb_cv_score = cross_val_score(nb, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(nb_cv_score))\n\n","321a66be":"svm= SVC()\nsvm_model = svm.fit(X_train, y_train)\ny_pred_svc = svm_model.predict(X_test)\naccuracy = round(svm.score(X_train, y_train)*100, 2)\nprint(\"Accuracy before the model tuning : \" + str(accuracy))\n\n#model tuning\nsvc_params = {\"C\": np.arange(1,5),\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n             'gamma': [.1, .25, .5, .75, 1.0]}\n\nsvc = SVC()\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1,)\n\nsvc_cv_model.fit(X_train, y_train)\nprint(svc_cv_model.best_params_)","b076de41":"svc_tuned = SVC(kernel = \"rbf\", C =3 , gamma = 0.1).fit(X_train, y_train)\ny_pred_svc_tuned = svc_tuned.predict(X_test)\ntuned_accuracy = round(svc_tuned.score(X_train, y_train)*100, 2)\nprint(\"Accuracy after the model tuning : \" + str(tuned_accuracy))\nsvc_cv_score = cross_val_score(svc_tuned, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(svc_cv_score))","c18a87c6":"cart = DecisionTreeClassifier()\ncart_model = cart.fit(X_train, y_train)\naccuracy = round(cart.score(X_train, y_train)*100, 2)\nprint(\"Accuracy before the model tuning : \" + str(accuracy))\n#model tuning\ncart_grid = {\"max_depth\" : range(1,10),\n              \"min_samples_split\" : list(range(2, 50))}\ncart = tree.DecisionTreeClassifier()\ncart_cv = GridSearchCV(cart, cart_grid, cv = 10, n_jobs = -1)\ncart_cv_model = cart_cv.fit(X_train, y_train)\nprint(cart_cv_model.best_params_)","c08b7cbd":"cart = tree.DecisionTreeClassifier(max_depth = 4, min_samples_split = 2)\ncart_tuned = cart.fit(X_train, y_train)\ny_pred_cart_tuned = cart_tuned.predict(X_test)\ntuned_accuracy = round(cart_tuned.score(X_train, y_train)*100, 2)\nprint(\"Accuracy after the model tuning : \" + str(tuned_accuracy))\ncart_cv_score = cross_val_score(cart_tuned, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(cart_cv_score))","3fa2fe02":"rf_model = RandomForestClassifier().fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\naccuracy = round(rf_model.score(X_train, y_train)*100, 2)\nprint(\"Accuracy before the model tuning :\"+ str(accuracy))\n\n#model tuning\nrf_params = {\"max_depth\": [2,5,8,10],\n             'criterion' : ['gini', 'entropy'],\n            \"max_features\" : [1,2,3,4,5,6],\n            \"n_estimators\" : [10,500,1000],\n            \"min_samples_split\": [2,5,10]}\nrf_model = RandomForestClassifier()\n\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1) \nrf_cv_model.fit(X_train, y_train)\nprint(\"Best parameters are: \" + str(rf_cv_model.best_params_))","6ff77cc1":"rf_tuned = RandomForestClassifier(max_depth = 5, \n                                  max_features = 4, \n                                  min_samples_split = 5,\n                                  n_estimators = 500,\n                                 criterion = 'entropy')\n\nrf_tuned.fit(X_train, y_train)\ny_pred_rf_tuned = rf_tuned.predict(X_test)\ntuned_accuracy = round(rf_tuned.score(X_train, y_train)*100, 2)\nprint(\"Accuracy after model tuning\", str(tuned_accuracy))\nrf_cv_score = cross_val_score(rf_tuned, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(rf_cv_score))\nImportance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = X_train.columns)\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"b\")\n\nplt.xlabel(\"Importances of Parameters\")","453ee0f4":"gbm_model = GradientBoostingClassifier().fit(X_train, y_train)\ny_pred_gbm = gbm_model.predict(X_test)\naccuracy = round(gbm_model.score(X_train, y_train)*100, 2)\nprint(\"Accuracy before model tuning : \" + str(accuracy))\ngbm_params = {\"learning_rate\" : [ 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,1000],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\ngbm = GradientBoostingClassifier()\n\ngbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1)\ngbm_cv.fit(X_train, y_train)\nprint(\"Best Parameters are: \" + str(gbm_cv.best_params_))","4279c2e2":"gbm = GradientBoostingClassifier(learning_rate = 0.01, \n                                 max_depth = 3,\n                                min_samples_split = 2,\n                                n_estimators = 1000)\ngbm_tuned =  gbm.fit(X_train,y_train)\ny_pred_gbm_tuned = gbm_tuned.predict(X_test)\ntuned_accuracy = round(gbm_tuned.score(X_train, y_train)*100, 2)\nprint(\"Accuracy after model tuning :\" + str(tuned_accuracy))\ngbm_cv_score = cross_val_score(gbm_tuned, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(gbm_cv_score))","81edafdf":"xgb_model = XGBClassifier().fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\naccuracy = round(xgb_model.score(X_train, y_train)*100, 2)\nprint(\"Accuracy before the model tuning : \" + str(accuracy))\n\n#model tuning\nxgb_params = {\n        'n_estimators': [100, 500, 1000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.05],\n        \"min_samples_split\": [2,5,10]}\nxgb = XGBClassifier()\n\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1)\nxgb_cv_model.fit(X_train, y_train)\nprint(\"Best parameters are : \" + str(xgb_cv_model.best_params_))","034db54c":"xgb = XGBClassifier(learning_rate = 0.1, \n                    max_depth = 3,\n                    min_samples_split = 2,\n                    n_estimators = 500,\n                    subsample = 0.8)\nxgb_tuned =  xgb.fit(X_train,y_train)\ny_pred_xgb_tuned = xgb_tuned.predict(X_test)\ntuned_accuracy = round(xgb_tuned.score(X_train, y_train)*100, 2)\nprint(\"Accuracy after the model tuning : \" + str(tuned_accuracy))\nxgb_cv_score = cross_val_score(xgb_tuned, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(xgb_cv_score))","0a4eb151":"lgbm_model = LGBMClassifier().fit(X_train, y_train)\ny_pred_lgbm = lgbm_model.predict(X_test)\naccuracy = round(knn.score(X_train, y_train)*100, 2)\nprint(\"Accuracy before the model tuning : \" + str(accuracy))\nlgbm_params = {\n        'n_estimators': [100, 500, 1000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.05],\n        \"min_child_samples\": [5,10,20]}\nlgbm = LGBMClassifier()\n\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_params, \n                             cv = 10, \n                             n_jobs = -1)\nlgbm_cv_model.fit(X_train, y_train)\nprint(\" Best parameters are : \" + str(lgbm_cv_model.best_params_))","24d97b47":"\nlgbm = LGBMClassifier(learning_rate = 0.05, \n                       max_depth = 3,\n                       num_leaves =10,\n                       subsample = 0.6,\n                       n_estimators = 500,\n                       min_child_samples = 10)\nlgbm_tuned = lgbm.fit(X_train,y_train)\ny_pred_lgbm_tuned = lgbm_tuned.predict(X_test)\ntuned_accuracy = round(lgbm_tuned.score(X_train, y_train)*100, 2)\nprint(\"Accuracy after the model tuning : \" + str(tuned_accuracy))\nlgbm_cv_score = cross_val_score(lgbm_tuned, X_train, y_train, cv = 10).mean()*100\nprint(\"Cross validation score is \" + str(lgbm_cv_score))","779215ad":"models = [\n    knn_tuned,\n    loj_model,\n    svc_tuned,\n    nb_model,\n    cart_tuned,\n    rf_tuned,\n    gbm_tuned,\n    lgbm_tuned,\n    xgb_tuned\n    \n]\n\n\nfor model in models:\n    names = model.__class__.__name__\n    accuracy = round(model.score(X_train, y_train),2)\n    print(\"-\"*28)\n    print(names + \":\" )\n    print(\"Accuracy: {:.4%}\".format(accuracy))","759c7f78":"result = []\n\nresults = pd.DataFrame(columns= [\"Models\",\"Accuracy\"])\n\nfor model in models:\n    names = model.__class__.__name__\n    accuracy = round(model.score(X_train, y_train),2)    \n    result = pd.DataFrame([[names, accuracy*100]], columns= [\"Models\",\"Accuracy\"])\n    results = results.append(result)\n    \nsns.set_color_codes(\"muted\")\nsns.barplot(x= 'Accuracy', y = 'Models', data=results, color=\"b\")\nplt.xlabel('Accuracy %')\nplt.title('Accuracy Ratio Of Models');   ","4c783bf1":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Gaussian Naive Bayes', \n              'XGBoost', 'Linear SVC', \n              'Random Forest', 'Gradient Boosting Trees', 'CART', 'LGBM'],\n    'Score': [\n        knn_cv_score, \n        loj_cv_score,      \n        nb_cv_score, \n        xgb_cv_score, \n        svc_cv_score, \n        rf_cv_score,\n        gbm_cv_score,\n        cart_cv_score,\n        lgbm_cv_score\n    ]})\nprint('---Cross-validation Accuracy Scores---')\nprint(cv_models.sort_values(by='Score', ascending=False))\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x= 'Score', y = 'Model', data=cv_models, color=\"b\")\nplt.xlabel('Accuracy %')\nplt.title('Cross-validation Accuracy Scores');   ","75707adf":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred_xgb_tuned\n    })\nsubmission.to_csv('submission.csv', index=False)","7dfcb807":"In order to understand the relationship between features, we can look at the correlations by using heatmap.","b7cc1db0":"\nEven I look both SibSp and Parch features, I can not say so much about them. However, if we merge them, we can say that having a relative can change the chance of survival or not? ","80422f64":"After binning our Fare feature, we can mapping our **Fare** values as respect to bins that we divide.","013b5cd0":"Lets check them briefly","91b14823":"As we can see it more clearly in violin plot, the more younger people are survived in disaster.","83df0b05":"### SibSp","e32d5fad":"When we are looking at the info of our data, we can see that there are some features which need to be transformed into other data types but for now, lets look at the missing values first. ","5d4f02d3":"## ML","498cef11":"Sex feature is referring to the gender of the passengers. ","bd018e20":"Since there are so many unique values in Fare feature, we can split them into group. It makes the Fare feature more easy to understand and useful for our operations.","f883ac48":"Embarked feature is showing the port where the passenger boarded the Titanic.","d3a1fa40":"So that, when we looked at the correlations, we can say that Pclass and Fare are most important features for Survived target feature.","4511a5b9":"### Fare","b011b129":"### KNN","1815a14d":"As we expect, Age feature has many missing values by looking at the graph which is above. In addition, our Cabin feature has lots of missing values as well. So lets see is there any correlation between Age and Cabin features","cbe273aa":"### Titanic EDA & ML","da670ee8":"Age is referring to ages of the passengers as you can predict. However, we know Age feature has many missing values in dataset. So that, we need to fill these missing values with respect to correlated features","9205e710":"### Gaussian Naive Bayes","8f58ddbf":"Even though, there are more number of female passenger than male passenger, we can see that females have more chance to survive than males have. Also, its understandable when we are thinking gender difference when a disaster occurs. Women and children has precision at evacuation.","92fe3eb8":"### Logistic Regression","6f924b29":"When we are looking at the heatmap, we can see the most correlated features with Age are Pclass and SibSp. So that, we can use Pclass or SibSp for filling the missing values in Age feature.","64c5d56a":"### Cabin","76d8652b":"### Family","f9fb6f02":"When we looked at the nullity correlation which is the graph at above, we can see there is 0.1 correlation between Cabin and Age features which can be ignored. Thus, We do not consider both of them are together while we are filling the missing values at each part.","0e041cc0":"Then, import our dataset as well.","b3862545":"Now, we can say that people who bought more expensive tickets are more likely to survive. However, we can also think that \"Can we be more spesific about that estimation?\" and add another feature.","3034ab0d":"### Support Vector Classifier","c0dbca72":"At the above, we mapped our feature as integer.","235c317b":"Now, we can use our **df_set** to make prediction with our ML techniques. However, lets select our dependent and indepent variables for classification first.","8f7c103c":"Cabin feature has the cabin number where passenger stayed in Titanic. It can be divided into groups, but I think, more information about Titanic architecture is needed to make some assumption. Maybe I am so picky about these features since it is my first time. It can also be used for making new features. However, I rather think to drop from datasets.","35ea53a7":"Finally, lets make our submission.","ac310c09":"As we can see, there are only two missing values in \"Embarked\" feature and it can be useful for our prediction even if it is nothing to do with the disaster. So that, we can fill them with most occured place in our dataset. ","d406d81d":"Age can be useful for making some prediction like in disasters \"Children first\" sentences are used during the evacuations.","6edc433a":"Now , lets look at the how many unique value do we have.","62b17965":"**Survived** feature is consist of two integer which are 0 is referring to the people who are not survived and 1 is referring to the people who are survived at the disaster. Thus, there is no need to change in this feature. Lets just visualize and see the number of survivals.","38288c3c":"**SibSp** is the number of siblings\/spouses the passenger has aboard the Titanic. So it is very relatable feature with **Parch**.","2e9a6e4e":"When we looked at the barplot, we can see the effect of **Sex** feature clearly. Ratio between gender are increased rapidly passenger are older. However, when the age is low, gender has less precision.","99719e5d":"As we can see, even the people those who are with their family with are less than who are alone, people who are with their family are more likely to survive. So that we keep **Has_Family** feature as a combination of **Parch** and **SibSp**. ","6b65f703":"We can fill the missing values with the median for **Fare**","ab534e5b":"   In this notebook, I will be study on Titanic Dataset on Kaggle. Since this is my first time on EDA & ML notebook on kaggle, I chose the most worked on dataset on Kaggle. So that, I am very glad to share this notebook with everyone and be thankful anyone who were shared on Kaggle which was really helpful for me.\n\n   So firstly, I wil be focusing on the data in general. Then, I will work on the features separately. While I am working on features, I make my own dataset for ML techniques which I will focusing on ML part. So that, sorry for that disorganized structure which you maybe need to jump from branch(feature) to branch(feature) :).\n   So lets import our libraries first.","844e28b6":"Pclass is reffering to socieconomic class of passengers. Lets see how many class on dataset.","baa59e0e":"### Light GBM","ad6812d0":"### Extreme Gradient Boosting(XGBoost)\n","4ffdff0d":"Ticket feature has the ticket number of the boarding passenger on the dataset. It can be grouped within themselves, but I dont think it can be useful due to the lack of information. If we know which ticket refers to where in the Titanic, we can make it more useful. Maybe I misunderstood this feature.","49eebec2":"Parch is the number of parents\/children the passenger has aboard the Titanic. Lets look at the distribution of parent\/children.","1addc58f":"For a beginning, it is okay to leave it there and try to analyze features first. While we are analysing the data, we can make our own dataset for the prediction after our EDA.","d01938bd":"### Classification And Regression Trees (CART)","69fd4ff3":"### Name ","b27ca915":"## Correlations","b4a611e4":"### Sex","12064f3a":"Sadly, we can say that socioeconomic condition of a passenger has a large impact of him\/her chance of survival.","6666975d":"### Embarked","3d6beee7":"Unfortunately, there are many people who are not survived at the disaster. Best thing we can do for them is to make sure this type of accidents will never happen again. This is also why we are doing this kind of analysis. ","c0cbbba0":"When we are looking at the dataset, we can say that \"age\" feature has some missing values. However, lets see that in a visual way to see it more clearly.","16ececc7":"### Parch","b85b86ab":"### Age","4796d488":"Before we make a comment, lets look at the **SibSp** feature too because of the similarity between them.","12be2111":"For model selection, KNN is very good before K-fold CV operation but, after the CV, XGBoost looks more consistent and successful. The reason why we care about K-fold CV so much, it makes the model more robust.For me, the robustness is more important than high accuracy. Thus, I pick XGBoost over KNN and others.","695cbabb":"As we can see, there are more women then men who are survived even though they bought same price of ticket.","aa51b357":"For the first impressions on the dataset, has consists of 891 rows and 12 features with some of the features has missing values. We will look at those in **Missing Values** part. \n\nWe can also make some analysis before we look at the dataset thoroughly.\n    * PassengerId looks like a unique for each individual when we look at that values.\n    * Age derivated between 15 to 40 so that most of the passengers are young.\n    * Fare has very wide range and it can be useful for visualization and analyzing.\n    * Pclass looks like a categorical variable which can be take 1, 2 or 3 values and 3 are more than other two values( %50 and %75 are the\n    same).So that it can use also to analyzing the dataset. \n    ","cd00772e":"## EDA","7fa49dee":"Lets fill that two missing values with \"S\" value which is referring to Southampton and save it our df.","c4b27e38":"### Random Forest","ac29b071":"## Missing Values","b352182d":"Name feature is referring to the name of passengers.","02bbdab1":"### Pclass","980c857a":"Since, we did not satisfy with **Parch** and **SibSp** features, we can make more general feature since they were related. ","6861175f":"### Gradient Boosting Machine (GBM)","b2be1d5d":"I selected the **X_train** , **y_train** and **X_test** from the dataset. I will use 9 techniques on our model and which are :\n    * KNN\n    * Logistic Regression\n    * Gaussian Naive Bayes\n    * Support Vector Classifier\n    * Classification and Regression Trees\n    * Random Forest\n    * Gradient Boosting Machine\n    * Extreme Gradient Boost (XGBoost)\n    * Light GBM\nI will also use Grid Search Cross-Validation for finding best parameters for the our model. Then, I will get the score of them and sort with barplot. Finally, I will decide which model is the best for our data.","7feb8b92":"### Ticket","80dfe3f4":"Now, lets look at the bin size and survived ratio of them.","162634ea":"**Fare** is referring to cost of the ticket.Since we know the socio-economic status is the most important feature for our analysis,  we can think of the cost can be important too. Then, we can say that the people who bought more expensive or more cheaper tickets are more likely to survive or not.","95b6adfe":"In order to fill the missing values, we will use median of **Age** and **SibSp** features which are get from boxplot at above.","0de163b7":"### Survived","e4cc02a3":"Since we know, there are 891 rows in our dataset, that means all the values on Name feature are unique values. So that, it can be bothersome for grouping them into categories. It can be done with many ways. If you are interested in a way to do it, I would recommend https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\/. Since, this is my first EDA, I dont work on these feature due to my lack of knowledge.","e47b2d2c":"That concludes the EDA part. Now, we need to check our dataset before the ML part."}}