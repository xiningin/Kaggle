{"cell_type":{"bda9937f":"code","0f7f4462":"code","3e949909":"code","f58a4a0f":"code","9026aae5":"code","b440fba5":"code","0c761eee":"code","891bae62":"code","9d5f2ff4":"code","45231ea8":"code","da914b04":"code","02b9855a":"code","a99d8f0a":"markdown"},"source":{"bda9937f":"# Importing modules\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential, Input, Model\nfrom tensorflow.keras.layers import MaxPooling2D, Conv2D, Flatten, Dense, Dropout, Average\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nos.chdir('\/kaggle\/input\/petfinder-pawpularity-score')","0f7f4462":"# Loading meta data and response variable\ndf = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')","3e949909":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Showing first 5 images\nfor i in range(1, 6):\n    img = mpimg.imread(os.path.join('train\/', os.listdir('train\/')[i]))\n    plt.subplot(1,5,i)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","f58a4a0f":"import PIL\nfrom PIL import Image\n\nwidths = []\nheights = []\n\nfor img in os.listdir(\"train\/\"):\n    img_path = os.path.join(\"train\/\", img) # Making image file path\n    im = Image.open(img_path)\n    widths.append(im.size[0])\n    heights.append(im.size[1])\n\nAVG_HEIGHT = round(sum(heights)\/len(heights))\nAVG_WIDTH = round(sum(widths)\/len(widths))","9026aae5":"df['file_name'] = df['Id'].astype(str) + '.jpg' \ntest['file_name'] = test['Id'].astype(str) + '.jpg'","b440fba5":"image_size = [int(AVG_HEIGHT\/3), int(AVG_WIDTH\/3)]\ninput_shape = image_size + [3]\nbatch = 32\n\ntrain_datagen = ImageDataGenerator(samplewise_center=True, \n                                   samplewise_std_normalization=True,\n                                   validation_split = 0.1)\n\ntest_datagen = ImageDataGenerator(samplewise_center=True,\n                                  samplewise_std_normalization=True,)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe = df,\n    directory = 'train\/',\n    x_col = 'file_name',\n    y_col = 'Pawpularity',\n    batch_size = batch,\n    shuffle = True,\n    subset = 'training',\n    color_mode = 'rgb',\n    class_mode = 'raw',\n    target_size = image_size)\n\nvalid_generator = train_datagen.flow_from_dataframe(\n    dataframe = df,\n    directory = 'train\/',\n    x_col = 'file_name',\n    y_col = 'Pawpularity',\n    batch_size = batch,\n    shuffle = True,\n    subset = 'validation',\n    color_mode = 'rgb',\n    class_mode = 'raw',\n    target_size = image_size)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe = test,\n    directory = 'test\/',\n    x_col = 'file_name',\n    batch_size = batch,\n    shuffle = False,\n    color_mode = 'rgb',\n    class_mode = None,\n    target_size = image_size)","0c761eee":"def cnn_model(num_filters, neurons, input_shape):\n    model = Sequential([\n        Conv2D(num_filters[0], (3,3), activation='relu', input_shape=input_shape, name='conv_layer_1'),\n        MaxPooling2D(2,2),\n        \n        Conv2D(num_filters[1], (3,3), activation='relu', name='conv_layer_2'),\n        MaxPooling2D(2,2),\n        \n        Conv2D(num_filters[2], (3,3), activation='relu', name='conv_layer_3'),\n        MaxPooling2D(2,2),\n        \n        Conv2D(num_filters[3], (3,3), activation='relu', name='conv_layer_4'),\n        MaxPooling2D(2,2),\n        \n        Flatten(),\n        Dense(neurons, activation='relu', name='hidden_layer'),\n        Dropout(.5),\n        \n        Dense(1, name = 'output')\n    ])\n    \n    model.compile(\n        loss = 'mse',\n        optimizer=Adam(),\n        metrics=[RootMeanSquaredError()])\n    return model","891bae62":"params = np.array([[16, 32, 64, 128, 128],\n                   [32, 64, 64, 128, 128],\n                   [32, 64, 128, 256, 512]])\n\nfor i in range(3):\n    model = cnn_model(params[i, 0:4].astype(int), params[i, 4], input_shape)\n    \n    STEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\n    STEP_SIZE_VALID=valid_generator.n\/\/valid_generator.batch_size\n    \n    checkpoint_filepath = '..\/..\/working\/tmp\/checkpoint-model_' + str(i+1)\n    \n    model_checkpoint_callback = ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_weights_only=True,\n        monitor='val_root_mean_squared_error',\n        mode='min',\n        save_best_only=True) # Saving best checkpoint\n    \n    earlystopping_callback = EarlyStopping(\n        monitor='val_loss',\n        patience=5) # stopping training when val_loss doesn't decrease in 5 epochs\n    \n    reducelronplateau_callback = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3) # reducing learning rate when val_loss doesn't improve for 3 epochs\n    \n    model.fit(\n            train_generator,\n            steps_per_epoch=STEP_SIZE_TRAIN,\n            validation_data=valid_generator,\n            validation_steps=STEP_SIZE_VALID,\n            epochs=100,\n            callbacks=[model_checkpoint_callback, earlystopping_callback, reducelronplateau_callback])","9d5f2ff4":"models = []\nfor i in range(3):\n    filepath = '..\/..\/working\/tmp\/checkpoint-model_' + str(i+1)\n    model = cnn_model(params[i, 0:4].astype(int), params[i, 4], input_shape)\n    model.load_weights(filepath)\n    models.append(model)","45231ea8":"# Ensemble model\nmodel_input = Input(shape=(input_shape))\nmodel_outputs = [model(model_input) for model in models]\nensemble_output = Average()(model_outputs)\nensemble_model = Model(inputs=model_input, outputs=ensemble_output)\n\n","da914b04":"tf.keras.utils.plot_model(ensemble_model, to_file='..\/..\/working\/model.png', show_shapes=True)","02b9855a":"# Making submission file\nSTEP_SIZE_TEST = test_generator.n \/\/ 8\npred = ensemble_model.predict(test_generator, steps=STEP_SIZE_TEST)\nresults = pd.DataFrame({'Id':test['Id']})\nresults['Pawpularity'] = pred\n\nos.chdir('..\/..\/working')\n\nresults.to_csv('submission.csv', index=False)","a99d8f0a":"## Approach\n\nIn a previous attempt I used a basic ensemble of a Convolutional Neural Network (CNN) and a Deep Nerual Network (DNN), trained on the image data and meta data respectively. This attempt didn't yield the desired result with a Root Mean Squared Error (RMSE) of $\\approx$ 20.44, so I will now try a different approach. One of the problems with the previous attempt was, that DNN had a hard time minimizing loss i.e., predicting the `Pawpularity` based on the meta data. This led me to thinking \"maybe there isn't great coherence between the meta data and response variable\", which is why I will now try an approach solely based on the image data. \n\nFor this attempt I will create an ensemble model consisting of three CNN's with varying filters in the convolutional layers and neurons in the hidden layer before output.  "}}