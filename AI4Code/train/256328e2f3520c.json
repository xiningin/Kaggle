{"cell_type":{"2c94b003":"code","6fccb5b3":"code","3b339da9":"code","963cf4ea":"code","b7a11ab2":"code","2a276d1b":"code","c8f0104f":"code","7f2cadf3":"code","2ccec73d":"code","35039c1f":"code","561dd380":"code","2bc36402":"code","d705701b":"code","c347e5bf":"code","3e3836b9":"code","1b7646e5":"code","912bce40":"code","e42867d8":"code","4a75c6be":"code","1d686313":"code","05ec6607":"code","19f9e89a":"code","85db2c45":"code","4f4cff0f":"code","f9ee3069":"code","2ec5e3ab":"markdown","b205fdaa":"markdown","949e7043":"markdown","f9edd7fb":"markdown","e66bbb38":"markdown","54d7857c":"markdown","b0040650":"markdown","737e8198":"markdown","920314e0":"markdown","67979fdb":"markdown","12951463":"markdown","3af77e99":"markdown","a8df8456":"markdown","868ae6ec":"markdown","5076442f":"markdown","07f32d94":"markdown","00701b62":"markdown","48862ac1":"markdown","7f6bf137":"markdown","59c84603":"markdown","0efad664":"markdown","f03c1cdb":"markdown","cd6fc7c1":"markdown","79a9bca6":"markdown","ca88b8ff":"markdown","088c8d77":"markdown","5995e160":"markdown","25285f0f":"markdown","b3c94c4f":"markdown","71705e20":"markdown","c552245f":"markdown","da763f6f":"markdown","988d2e8c":"markdown","e5b9bf24":"markdown","aeb1072e":"markdown"},"source":{"2c94b003":"# Importing packages\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom zipfile import ZipFile as zipper\nimport PIL\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\nfrom tqdm import tqdm\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\nwarnings.filterwarnings('ignore')","6fccb5b3":"os.listdir('..\/input\/dogs-vs-cats')","3b339da9":"train_path = '..\/input\/dogs-vs-cats\/train.zip'\ntest_path = '..\/input\/dogs-vs-cats\/test1.zip'\n\ndestination = '\/kaggle\/files\/images'\n\nwith zipper(train_path, 'r') as zipp:\n    zipp.extractall(destination)\n    \nwith zipper(test_path, 'r') as zipp:\n    zipp.extractall(destination)","963cf4ea":"train = pd.DataFrame({'file': os.listdir('\/kaggle\/files\/images\/train')})\nlabels = []\nbinary_labels = []\nfor i in os.listdir('\/kaggle\/files\/images\/train'):\n    if 'dog' in i:\n        labels.append('dog')\n        binary_labels.append(1)\n    else:\n        labels.append('cat')\n        binary_labels.append(0)\n\ntrain['labels'] = labels\ntrain['binary_labels'] = binary_labels\ntest = pd.DataFrame({'file': os.listdir('\/kaggle\/files\/images\/test1')})","b7a11ab2":"train.head()\n","2a276d1b":"test.head()","c8f0104f":"filepath = '\/kaggle\/files\/images\/train\/'\nfig = plt.figure(1, figsize = (20, 20))\nfor i in range(10):\n    plt.subplot(5, 5, i + 1)\n    pic = PIL.Image.open(filepath + os.listdir(filepath)[i])\n    plt.imshow(pic)\n    plt.axis('off')\n\nplt.show()","7f2cadf3":"train_set, val_set = train_test_split(train,\n                                     test_size=0.1)\nprint(len(train_set), len(val_set))","2ccec73d":"img = PIL.Image.open(filepath + os.listdir(filepath)[1])\nimg_new = img.filter(PIL.ImageFilter.UnsharpMask(radius=2, percent=150))\nfig = plt.figure(1, figsize = (20, 20))\nplt.subplot(2,2,1)\nplt.imshow(img)\nplt.axis('off')\nplt.subplot(2,2,2)\nplt.imshow(img_new)\nplt.axis('off')","35039c1f":"img = PIL.Image.open(filepath + os.listdir(filepath)[1])\nimg_new = img.filter(PIL.ImageFilter.UnsharpMask(radius=1, percent=100))\nfig = plt.figure(1, figsize = (20, 20))\nplt.subplot(2,2,1)\nplt.imshow(img)\nplt.axis('off')\nplt.subplot(2,2,2)\nplt.imshow(img_new)\nplt.axis('off')","561dd380":"for i in tqdm(range(len(os.listdir(filepath)))):\n        pic_path = filepath + os.listdir(filepath)[i]\n        pic = PIL.Image.open(pic_path)\n        pic_sharp = pic.filter(PIL.ImageFilter.UnsharpMask(radius=2, percent=100))\n        pic_sharp.save(pic_path)","2bc36402":"train_gen = ImageDataGenerator(rescale=1.\/255)\nval_gen = ImageDataGenerator(rescale=1.\/255)\nbatch_size = 128\n\n\ntrain_generator = train_gen.flow_from_dataframe(\n    dataframe = train_set,\n    directory = destination + '\/train\/',\n    x_col = 'file',\n    y_col = 'labels',\n    class_mode = 'categorical',\n    target_size = (224,224),\n    batch_size = batch_size\n)\n\n\nvalidation_generator = val_gen.flow_from_dataframe(\n    dataframe = val_set,\n    directory = destination + '\/train\/',\n    x_col = 'file',\n    y_col = 'labels',\n    class_mode = 'categorical',\n    target_size = (224,224),\n    batch_size = batch_size,\n    shuffle = False\n)","d705701b":"data_augmentation = keras.Sequential(\n  [\n    layers.RandomFlip(\"horizontal\",\n                      input_shape=(224,\n                                  224,\n                                  3)),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n  ]\n)","c347e5bf":"input_shape = (224, 224, 3)\nn_class = 2\nkorobka = Sequential([\n    data_augmentation,\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(2, activation = 'softmax')\n])","3e3836b9":"korobka.compile(optimizer='adam', \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])","1b7646e5":"korobka.summary()","912bce40":"early_stopping = EarlyStopping(\n    monitor = \"val_loss\",\n    patience = 3,\n    verbose = 1,\n    mode = \"min\")\n\ncheckpoint = ModelCheckpoint(\n    monitor = \"val_accuracy\",\n    filepath = \"catdog_vgg16_.{epoch:02d}-{val_accuracy:.6f}.hdf5\",\n    verbose = 1,\n    save_best_only = True, \n    save_weights_only = True\n)","e42867d8":"with tf.device('\/device:GPU:0'):\n    epochs=40\n    history = korobka.fit_generator(train_generator, \n                            validation_data=validation_generator, \n                            epochs=epochs,\n                            validation_steps = val_set.shape[0] \/\/ batch_size,\n                            steps_per_epoch = train_set.shape[0] \/\/ batch_size, callbacks = [early_stopping])","4a75c6be":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(28)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","1d686313":"with tf.device('\/device:GPU:0'):\n    val_pred = korobka.predict(validation_generator, steps = np.ceil(val_set.shape[0] \/ batch_size))","05ec6607":"val_set['normalpreds'] = np.argmax(val_pred, axis = -1)\nlabels = dict((v,k) for k,v in train_generator.class_indices.items())\n\nval_set['normalpreds'] = val_set['normalpreds'].map(labels)","19f9e89a":"val_set.head(20)","85db2c45":"fig, ax = plt.subplots(figsize = (9, 6))\n\ncm = confusion_matrix(val_set[\"labels\"], val_set[\"normalpreds\"])\n\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"cat\", \"dog\"])\ndisp.plot(cmap = plt.cm.Blues, ax = ax)\n\nax.set_title(\"Validation Set\")\nplt.show()","4f4cff0f":"print(classification_report(val_set[\"labels\"], val_set[\"normalpreds\"]))","f9ee3069":"!mkdir -p saved_models\nkorobka.save('saved_models\/korobka_v3')\nos.listdir('saved_models\/korobka_v3')","2ec5e3ab":"As you can see the transofmation mentioned above results to make too much useless noise on the image, so we set the *percent* value to 100 instead of 150.","b205fdaa":"# Intoduction\nThe main goal of this work is to develop a CNN capable of solving the Dogs vs Cats classification task. <br> I am not taking this work too serious, I consider it to be a decent practice, so, here it is a hand-made meme:\n\n\n![meme_low_q.png](attachment:6857f6c3-6343-420f-b5ba-614ea8edb649.png)\n","949e7043":"![korobka_v1.jpg](attachment:faf7c7ce-4537-4ba3-92dd-dccbce0a23ea.jpg)","f9edd7fb":"## Build the model","e66bbb38":"# Interpret the results","54d7857c":"# Visualize the data\n\nWe will only plot some sample images from train data to not to enlarge the notebook with useless plots.\n","b0040650":"## Train the model","737e8198":"As usual, train_test_split() method is used to split the trainig and test data sets by 90% and 10% respectivelly.\n\n","920314e0":"![korobka_v2.png](attachment:9e653a91-1d98-4a0e-a8cb-4223c81b5137.png)","67979fdb":"## VGG16 overall & architecture","12951463":"Now we apply our pre-processing to each image in train folder and rewrite them using the code below:","3af77e99":"## Callbacks","a8df8456":"## Data augmentation\nIn this section we will define some light data augmentation to avoid the possible overfitting of the model and in further cells, when building it, data_augmentation will be its first step","868ae6ec":"# Data Preparation","5076442f":"![korobka_v3.png](attachment:fe7b89cc-daa2-4790-8c89-ba8f100d3d88.png)","07f32d94":"# Predict on validation set","00701b62":"# Create a dataset\nFirst of all we should deal with the .zip files and extract its content into two different folders: train and test.\n","48862ac1":"Recently I came up with adding another step to our image pre-processing which, in theory, could make the job easier for the model to distinguish a dog from the cat. In the cell below, you can see some easy code lines to make the image a little bit sharper. We will see if this helps our model later on.","7f6bf137":"## KOROBKA using pre-processing, data augmentation and dropout","59c84603":"## Confusion matrix","0efad664":"Early stopping callback is used to avoid overfitting during the training process of many neural networks, not only the CNN. It basically stops the model if an indicator (we can shoose one we want to monitor) doesn\u00b4t improve within the specified epochs of patience (3 in my case). Note that when setting the early stopping callback, we would rather monitor the validation set loss, than its accuracy score. In case we still want to monitor the accuracy instead of the loss (this may vary depending on the task) the *mode* argument needs to be set to *max*, because we want the maximum accuracy, but the minimum loss.","f03c1cdb":"# Afterword\nThis is the last section of this notebook, and frankly, I am pretty satisfied with the work done so far. The final model shows 94% accuracy without trasfer learning neither complex structure or pre-processing, which is basically an outstanding result. Glad to claim this notebook has achieved all its prior goals.\nThank you very much for you attention, all the comments and suggestions are welcome.","cd6fc7c1":"# Visualize training results","79a9bca6":"## Generate training - validation sets\nIn the cell below, I define the generators for both train and validation sets. Note that the normalization of input values (1.\/255 rescaling) is used in both cases. Also, I set the batchsize value to 128. The reason for it is the model I use is similar to VGG16 which uses 128 batchsize for training, but we'll cover this later on.\n","ca88b8ff":"## KOROBKA using pre-processing and data augmentation","088c8d77":"Train dataframe includes each photo filename, category and binary labels, and it\u00b4s the data we are goind to use for training our CNN, while test dataframe doesn\u00b4t have any labels and could be used only to submit our predictions to the competition. In further cells we will split our labeled data into train set and a validation one to properly measure our model performance.","5995e160":"# Save the model","25285f0f":"## Making sharper edges","b3c94c4f":"# Model","71705e20":"# Conclusions\nThis section is dedicated to show all the versions of the model and the corresponding classification reports. It's easy to follow step by step either how I managed to improve the initial model or the changes and decisions made to do so.","c552245f":"## KOROBKA using data augmentation ","da763f6f":"## Model summary","988d2e8c":"Now we define a pandas dataframe adding a binary and a categorical label for each image.\n","e5b9bf24":"The model I build in the following section is inspired by the VGG16 architecture. Long story short, after AlexNet publication (the first CNN in using more than one consecutive convolutional layers)the VGG16 by K. Simonyan and A. Zisserman proposed similar idea of stacking convolutional layers without pooling in between, in order to improve the feature caption and overall model performance. My model is very similar to one of the VGG16 model configurations, yet sifnificantly more simple and doesn't require as much computational power as does his VGG16 analogue. You can find the standart VGG16 Architecture in the image down below:\n![StandartVGG16Architecture](https:\/\/www.researchgate.net\/profile\/Max-Ferguson\/publication\/322512435\/figure\/fig3\/AS:697390994567179@1543282378794\/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png)","aeb1072e":"## Classification report"}}