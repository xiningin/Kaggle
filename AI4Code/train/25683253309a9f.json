{"cell_type":{"0f6f1bda":"code","4a1eca66":"code","f6db9266":"code","7984cf4e":"code","4a9b3136":"code","16101329":"code","b1166299":"markdown","d95fe4ab":"markdown","98a12805":"markdown","9cf0dbde":"markdown","ed9ed52e":"markdown","ba64e875":"markdown","d02f7532":"markdown","7a19f0bb":"markdown","98dcd02c":"markdown","d5d665e1":"markdown","5bce3639":"markdown","c280ded1":"markdown","3584d2f0":"markdown","bc6a975a":"markdown","e0f60934":"markdown","4816bcdb":"markdown","62003bfa":"markdown"},"source":{"0f6f1bda":"import tensorflow as tf #for building models  \nfrom IPython.display import Latex  # writing mathematical equations.\nimport matplotlib.pyplot as plt # plotting \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","4a1eca66":"#Tensors :\n# 0,1d Tensors\nnumber=tf.constant(1.45353453,tf.float64) # constant number(or point) has no dimension, hence 0 rank.\nnames=tf.constant(['alex','burtin'],tf.string)\n\nprint (\"Rank of number is {} \\t & Rank of names is {}\".format(tf.rank(number).numpy(),\n                                                              tf.rank(names).numpy()))\n\n# 2d tensor\nmatrix=tf.constant([[1,2,3],[2,3,45]],tf.int64)\n#assertion used for sanity checks ,\nassert isinstance(matrix,tf.Tensor) \nassert tf.rank(matrix).numpy()==2\n\n#4d tensor\nimage=tf.random.normal([4,512,512,3]) \n# this is how collection of images are stored - no of images(10),pixels(256),pixels(256),Channels(RGB)(3)\n\nassert isinstance(image,tf.Tensor)\nassert tf.rank(image).numpy()==4\nassert tf.shape(image).numpy().tolist()==[4,512,512,3]\n\n# Let's see any 1 image out of 4\nimage1=image[0,:,:,1]\nplt.imshow(image1)\nplt.title(\"Random Image\")","f6db9266":"def func(a,b):\n    c=tf.add(a,b)\n    d=tf.subtract(b,1)\n    e=tf.multiply(c,d)\n    return e\n\na,b=tf.constant(1.5),tf.constant(2.5)\ne_out=func(a,b)\n\nprint(\"if a=1.5 & b=2.5 then(a+b)*(b-1) ={}\".format(e_out.numpy()))","7984cf4e":"\nclass SimpleNN(tf.keras.layers.Layer):\n    \"\"\"Defining the Model - initialize,feedforward,Backpropogation\"\"\"\n    def __init__(self,input_shape,n_output_nodes,):\n        \"\"\"Defining the shape & Initializing  - W,b\"\"\"\n        super(SimpleNN,self).__init__()\n        self.n_output_nodes=n_output_nodes\n        d=int(input_shape[-1])\n        self.W=tf.Variable(tf.random.normal([d,self.n_output_nodes]),\n                           trainable=True,name=\"weights\")\n        self.b=tf.Variable(tf.random.normal([1,self.n_output_nodes]),\n                           trainable=True,name=\"bias\")\n        print(\"Value of Weight is:\\n {}\".format(self.W.numpy()))\n        print(\" Value of  Bias matrix is:{}\".format(self.b.numpy()))\n    \n    def call(self,x):\n        \"\"\" 1st FeedForward : y=xW+b\"\"\"\n        z=tf.add(tf.matmul(x,self.W),self.b)\n        y=tf.sigmoid(z)\n        return y\n    \n    def train(self,x_input,output_actual,learning_rate):\n        \"\"\"Backpropogation (updating W,b) using tf.GradientTape\"\"\"\n        for i in range(500):\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch([self.W,self.b])\n                output_predicted=NN_sample.call(x_input)\n                # Defining Cost Function\n                loss=tf.math.squared_difference(output_actual,output_predicted)\n                \n            # Computing gradient - derivative of Cost Function wrt W,b\n            grad_w=tape.gradient(loss,self.W)\n            grad_b=tape.gradient(loss,self.b)\n            \n            # Updating W,b\n            self.W=self.W-learning_rate*grad_w\n            self.b=self.b-learning_rate*grad_b\n\n            history.append(output_predicted.numpy()[0,0])\n            history_1.append(output_predicted.numpy()[0,1])\n            \n        print(\"Value of Final Weight is:\\n {} \\n Value of Final Bias matrix is:{}\".format(self.W.numpy(),\n                                                                                          self.b.numpy()))\n        return NN_sample.call(x_input)\n    \n# define few imp variables\ntf.random.set_seed(1)\nhistory=[]\nhistory_1=[]\nlearning_rate=0.1\n# opt=tf.keras.optimizers.SGD(learning_rate=0.01,name='SGD')\n\n# Initialize the SimpleNN class & call build to initialize W,b\nNN_sample=SimpleNN((1,3),2)      # input,output shape\nx_input=tf.constant([[1,5,2.]],shape=(1,3))\nprint(\"Value of X is {}\".format(x_input.numpy()))\n\n# Define Output and Train the model\ny_actual=tf.constant([[0.9,0.2]],shape=(1,2))\ny_predicted=NN_sample.train(x_input,y_actual,learning_rate)\nnp.set_printoptions(precision=3,suppress=True) \nprint(\"Actual Output (y):{}\\n Predicted Output : {}\".format(y_actual.numpy(),y_predicted.numpy())) \n\n\n# Plot the training over iterations vs Actual Output \nplt.plot([y_actual.numpy()[0][0]]*len(history))\nplt.plot([y_actual.numpy()[0][1]]*len(history))\nplt.plot(history,linestyle=\"dashed\")\nplt.plot(history_1,linestyle=\"dashed\")\nplt.legend((\"Actual\",\"Actual\",\"Predicted\",\"Predicted\"))\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"precicted  values+ Actual values\")\nplt.title(\"Training Over time\",fontsize=30)","4a9b3136":"# initialize few things: \nhistory=[]\nhistory1=[]\nx_input=tf.constant([[1,9,5.]],shape=(1,3))\nn_output_nodes=3 # defining number of outputs\ny_actual=tf.constant([[1,0.]],shape=(1,2))\nlearning_rate=1e-2\nopt=tf.keras.optimizers.SGD(learning_rate=0.01,name='SGD')\n\n# Define the Model \nmodel2=keras.Sequential()\n# input_shape is only passed for 1st layer\ndense_layer1=layers.Dense(2,use_bias=True,\n                  kernel_initializer=\"normal\",bias_initializer=\"normal\",\n                  activation=\"sigmoid\",input_shape=(3,),trainable=True)\n#add 1st layer\nmodel2.add(dense_layer1)\n\n\n#Comile the model which initialize the W,b\nmodel_output=model2(x_input)\nprint(\"Value of Input: x= {}\".format(x_input))\nprint(\"Value of Weight matrix W= :\\n{} \".format(model2.layers[0].get_weights()[0]))\nprint(\"Value of Bias matrix b=:{}\".format(model2.layers[0].get_weights()[1]))\nprint(\"Model Output y={}\".format(model_output))    \n      \n# BackPropogation using tf.GradientTape()\nfor i in range(500):\n    with tf.GradientTape(persistent=False) as tape:\n        tape.watch(model2.variables)\n        # Defining Cost Function\n        loss=tf.math.squared_difference(y_actual,model2(x_input))\n     \n    # Calculating Gradient (partial derivative of Loss wrt W,b)\n    grads = tape.gradient(loss, model2.trainable_variables)    \n    # Updating W,b\n    opt.apply_gradients(zip(grads, model2.trainable_variables))\n\n    history.append(model2(x_input).numpy()[0][0])\n    history1.append(model2(x_input).numpy()[0][1])\n\ny_predicted=model2(x_input)\nprint(\"Actual Output (y):{}\\n Predicted Output : {}\".format(y_actual.numpy(),y_predicted.numpy())) \n\n# Plot the training over iterations vs Actual Output \nplt.plot([y_actual.numpy()[0][0]]*len(history))\nplt.plot([y_actual.numpy()[0][1]]*len(history))\nplt.plot(history,linestyle=\"dashed\")\nplt.plot(history1,linestyle=\"dashed\")\nplt.legend((\"Actual\",\"Actual\",\"Predicted\",\"Predicted\"))\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"precicted  values+ Actual values\")\nplt.title(\"Training Over time\",fontsize=25)","16101329":"# output and its shape\nn_output_nodes=2 # defining number of outputs\ny_actual=y_actual=tf.constant([[0.645,0.355]],shape=(1,2))\nhistory1,history2=[],[]\n\n# Defining the architecture of NN\nmodel=keras.Sequential()\n# input_shape is only passed for 1st layer\n# initialize Weight =1 & bias=0 to get same result.\ndense_layer1=layers.Dense(2,use_bias=True,\n                  kernel_initializer=\"normal\",bias_initializer=\"normal\",\n                  activation=\"sigmoid\",input_shape=(3,),trainable=True)\nmodel.add(dense_layer1)\n\n# Let's train the model using RMSprop\nmodel.compile(loss='binary_crossentropy',\n              optimizer=tf.keras.optimizers.SGD(learning_rate=0.01,name='SGD'),\n              metrics=[tf.keras.metrics.MeanSquaredError()])\n\nx_input=tf.constant([[1,9,5.]],shape=(1,3))\n\n# define your custom callback for prediction at each step\nclass PredictionCallback(tf.keras.callbacks.Callback):    \n    def on_epoch_end(self, epoch, logs={}):\n        history1.append(model.predict(x_input)[0][0])\n        history2.append(model.predict(x_input)[0][1])\n    \n\nhistory= model.fit(x_input,y_actual,epochs=30,verbose=0,callbacks=[PredictionCallback()])\n\nprint(\"Value of weights W:\\n{}\".format(model.get_weights()[0]))\nprint(\"Value of weights b:\\n{}\".format(model.get_weights()[1]))\ny_predicted=model(x_input)\nprint(\"Actual Output (y):{}\\n Predicted Output : {}\".format(y_actual.numpy(),y_predicted.numpy())) \n\n# Plot the training over iterations vs Actual Output \nplt.plot([y_actual.numpy()[0][0]]*len(history1))\nplt.plot([y_actual.numpy()[0][1]]*len(history2))\nplt.plot(history1,linestyle=\"dashed\")\nplt.plot(history2,linestyle=\"dashed\")\nplt.legend((\"Actual\",\"Actual\",\"Predicted\",\"Predicted\"))\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"precicted  values+ Actual values\")\nplt.title(\"Training Over time\",fontsize=25)","b1166299":"<a id =\"Step3\"><\/a>\n<h2  style=\"text-align:center;color:green;\" >Tensorflow<\/h2>","d95fe4ab":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content!<\/h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Step1\" role=\"tab\" aria-controls=\"profile\" \/a>History<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Step2\" role=\"tab\" aria-controls=\"profile\" \/a>Import Libraries<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Step3\" role=\"tab\" aria-controls=\"messages\">Tensorflow<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#Step4\" role=\"tab\" aria-controls=\"settings\">Neural Network using TensorFlow<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Step5\" role=\"tab\" aria-controls=\"settings\">Using Tensorflow (Manually defining Layers,Backprop)<span class=\"badge badge-primary badge-pill\">5<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Step6\" role=\"tab\" aria-controls=\"settings\">Using Sequential API +Tf.GradientTape<span class=\"badge badge-primary badge-pill\">6<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Step7\" role=\"tab\" aria-controls=\"settings\">Using Keras :Bulding and Training with Few Lines of Code<span class=\"badge badge-primary badge-pill\">7<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Step8\" role=\"tab\" aria-controls=\"settings\">Conclusion <span class=\"badge badge-primary badge-pill\">8<\/span><\/a><\/div>","98a12805":"<a id =\"Step2\"><\/a>\n<h2  style=\"text-align:center;color:green;\" >Import Libraries<\/h2>","9cf0dbde":"<div class='alert alert-success' role='alert'>\n    <b>What is  Tensorflow?<\/b>\nThe core open source library to help us develop and train ML models.\n<\/div>\n\n<div class='alert alert-warning' role='alert'>\n    <b>Why it is called Tensorflow?<\/b>\n    Because it handles the flow of tensors , think of <b>tensors<\/b> as a multi -dimensional array where all values have same datatype . The rank of tensors provides the number of dimensions.\n<\/div>","ed9ed52e":"<h3 style=\"border:2px solid Tomato;color:\">Simple Perceptron with 1 Dense Layer<\/h3>\nSimple Perceptron is defined by $y\\hat{}= \\sigma(xW+b)$ where $y\\hat{}$ is the output(prediction) & W- matrix of weights , b - bias ,x- is input $\\sigma$ -activation function.\n\n> It is xW not Wx for a reason,(borrowing from Linear Algebra) in any layer there is certain rule that we need to follow for matrix shape & multiplication - x's shape:(r,n) W's shape:(n,m) & b's shape(1,m) & y's shape : (r,m) so y=xW+b , shapes are (r,m)=(r,n)* (n,m) + (1,m) (broadcasting)\n\n![Perceptron](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRIs4oMIpFg0P25vaJ3arGNJWrakIMRd0WX4w&usqp=CAU)","ba64e875":"<div class=\"alert alert-block alert-warning\"> We can see that it was a rigrious task where we had to manually define the layer, parameter and updation steps,we can imagine how difficult it would be to build Multi-Layer Perceptron.<\/div>\n\n<div class=\"alert alert-block alert-warning\">For building Multi-layer Neural Networks we have the <b>Sequential API.<\/b>,<\/div>\n    \n<div class=\"alert alert-block alert-warning\"><b>Tf.GradientTape<\/b> gives you full low-level control over all aspects of training\/running your model, allowing easier debugging as well as more complex architectures etc., but we need to write more boilerplate code.Generally Used for Research Purpose. ,<\/div>\n\n<div class=\"alert alert-block alert-warning\">Whereas <b>Keras interface (using compile, fite)<\/b> allows for quick and easy building, training & evaluation of standard models. However, it is very high-level and doesn't give low-level control and also makes it difficult to debug <\/div>\n","d02f7532":"<a id =\"Step6\"><\/a>\n<h2  style=\"text-align:center;color:green;\" >Using Sequential API +Tf.GradientTape<\/h2>\n    \n<h4> There is also Subclassing APIs - which give us a lot of flexibility to define custom models.<\/h4>","7a19f0bb":"<h2  style=\"text-align:left;color:Brown;\" >Tensors & its Ranks<\/h2>","98dcd02c":"<ul class=\"list-group\">\n  <li class=\"list-group-item disabled\" aria-disabled=\"true\">The Perceptron Algorithm was invented by Frank Rosenblatt<\/li>\n  <li class=\"list-group-item\"> First implementation was in software for the IBM 704<\/li>\n  <li class=\"list-group-item\">Single layer perceptrons are only capable of learning linearly separable patterns <\/li>\n  <li class=\"list-group-item\">In 1969, Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function<\/li>\n  <li class=\"list-group-item\">multi-layer perceptrons are capable of producing an XOR function<\/li>\n  <li class=\"list-group-item\">XOR can be solved by a single Perceptron but only by using some type of Transformation<\/li>\n    \n[Solving XOR with a single Perceptron - Medium Article ](https:\/\/medium.com\/@lucaspereira0612\/solving-xor-with-a-single-perceptron-34539f395182)\n![XOR Function](https:\/\/miro.medium.com\/max\/700\/1*Tc8UgR_fjI_h0p3y4H9MwA.png)    ","d5d665e1":"<a id =\"Step8\"><\/a>\n<h2  style=\"text-align:center;color:green;\" >Conclusion<\/h2>","5bce3639":"<a id =\"Step7\"><\/a>\n<h2  style=\"text-align:center;color:green;\" >Using Keras :Bulding and Training with Few Lines of Code<\/h2>","c280ded1":"<h4 style=\"allign:left;color:Brown;\">Automatic Differentiation with Tensorflow(tf.GradientTape)<h4>\n    \n> Automatic Differentiation is the backbone for  **backpropogation**. Automatic differentiation (also called computational differentiation) refers to a set of techniques that can automatically compute the derivative of a function by repeatedly applying the chain rule.\n    \n>We will use tf.GradientTape which stores the forward pass in a \"tape\" and then to compute the gradient the tape is played backward.","3584d2f0":"<a id =\"Step4\"><\/a>\n<h2  style=\"text-align:center;color:Green;\" >Neural Networks using Tensorflow<\/h2>","bc6a975a":"<div class='alert alert-info' role='alert'> Tensorflow uses a high-level API called \n    <b>Keras<\/b> which is a powerfull framework for building and training  deep neural networks.\n<\/div>","e0f60934":"<a id =\"Step1\"><\/a>\n<h2  style=\"text-align:center;color:green;\" >History<\/h2>","4816bcdb":"<h2  style=\"text-align:left;color:Brown;\" >Computations on Tensors<\/h2>","62003bfa":"<a id =\"Step5\"><\/a>\n<h2  style=\"text-align:center;color:green;\" >Using Tensorflow (Manually defining Layers,Backprop)<\/h2>"}}