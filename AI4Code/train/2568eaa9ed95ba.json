{"cell_type":{"082e74f7":"code","03b07ff8":"code","05cd05f8":"code","d5567c69":"code","bf352317":"code","8f582fb3":"code","953af879":"code","54693846":"code","7fd22930":"code","cb933acb":"code","b835ee36":"code","70658176":"code","28f668cb":"code","8750323d":"code","42b9004d":"code","71466c99":"code","8929fd75":"code","a89637f8":"code","7550bc9a":"code","6e1b9ee6":"code","bf8b6432":"code","fa4d017a":"code","35608c4d":"code","19b352d9":"code","e04a523f":"code","05396768":"code","38644c61":"code","a56033e4":"code","1ecfae6c":"code","d52789e3":"code","9e9b54fa":"code","dc9d38da":"code","3829b548":"code","53e17003":"code","fb0195ab":"code","7d571f77":"code","c5d5bee9":"code","6621e50e":"code","9250a280":"code","0ef37c93":"code","3cd2094b":"code","3bb470f6":"code","4f0acd32":"code","be12618b":"markdown","145f3cd0":"markdown","6d082458":"markdown","61826d82":"markdown","7e28bbd6":"markdown","1d22bc10":"markdown","f1c5a282":"markdown","3d065aac":"markdown","8e052dcb":"markdown","0caf0478":"markdown","cf1126e5":"markdown","534aa333":"markdown","12e91420":"markdown","2bbcc5a7":"markdown","99c7a9fa":"markdown","c76dbd5a":"markdown","9ee95c40":"markdown","da9e809c":"markdown","5dacf4ac":"markdown","61522782":"markdown","747db9c2":"markdown"},"source":{"082e74f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03b07ff8":"import numpy as np\nimport pandas as pd\n\nfrom IPython.display import display, Markdown\nfrom pathlib import Path\nimport matplotlib.pyplot\nimport missingno as msno\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\nfrom fastai import *  \nfrom fastai.tabular.all import *\nimport torch\n\n# disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\npio.templates.default = 'ggplot2'","05cd05f8":"dpath = Path('\/kaggle\/input\/titanic')\n\n# sometimes it is advantageous to keep in the original data, such that it can be recoverd easily\ntrain_raw = pd.read_csv(dpath \/ 'train.csv')\ntest_raw = pd.read_csv(dpath \/ 'test.csv')","d5567c69":"full = pd.concat([train_raw.copy(deep=True), test_raw.copy(deep=True)], ignore_index=True)","bf352317":"full.head(3)","8f582fb3":"def split_data(df):\n    \"\"\"Splits the DataFrame into x, y, and passengerId\"\"\"\n    df = df.reset_index(drop=True) # this makes the index going from 0 .. n-1 independently of any transformation before\n    passengerId = df['PassengerId']\n    df = df.drop(columns=['PassengerId'])\n    \n    # transform 'Survived' into category, so that fast.ai applies automatically the correct loss function\n    if 'Survived' in df.columns:\n        df['Survived'] = df['Survived'].astype('category')\n    \n    return (df, passengerId)","953af879":"def remove_wordy_cols(df):\n    \"\"\"Remove all columns with wordy features.\"\"\"\n    df = df.drop(columns=['Name', 'Ticket'])\n    return df","54693846":"def fill_fare_mean(df):\n    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())\n    return df","7fd22930":"def apply_all(df, funs, debug=False):\n    \"\"\"Helper function to apply a series of functions onto a DataFrame\"\"\"\n    for fun in funs:\n        if debug:\n            print(f'Apply {fun.__name__}')\n        df = fun(df)\n    return df","cb933acb":"prep_nn_1 = lambda x : apply_all(x, [remove_wordy_cols, fill_fare_mean, split_data])\n\ntrain = train_raw.copy(deep=True)\ntrain, train_pId = prep_nn_1(train)\n\n# display renders the contant it the typical notebook format\ndisplay(train.head(3))\ndisplay(train_pId.head(3).to_frame())","b835ee36":"torch.device('cuda') # enable cuda, (activate GPU usage)\n\ncont_names = ['Fare','Age','Pclass','SibSp','Parch'] # set the continous variables\ncat_names = ['Sex','Cabin','Embarked'] # set the categoriall variables\nprocs = [Categorify, Normalize, FillMissing] # different fast.ai preprocessing steps\ndep_var = 'Survived' # our target variable\n\nsplits = RandomSplitter(valid_pct=0.2, seed=42)(train.index) # to validate the results we use randomly 20% of the training set\n\ncfg = tabular_config(embed_p=0.1, ps=0.1) # enable drop-out layers and set droupout to 10%\n\ndls = TabularPandas(train,\n                    cont_names=cont_names,\n                    cat_names=cat_names,\n                    procs=procs,\n                    y_names=dep_var,\n                    splits=splits).dataloaders(bs=64)\n\n\ncallbacks = [SaveModelCallback(min_delta=0.005, monitor='accuracy', fname='model_triv_best')]\n\nlearn = tabular_learner(dls, layers=[500,200,100], metrics=[accuracy], config=cfg)","70658176":"dls.show_batch(), learn.loss_func","28f668cb":"learn.lr_find()","8750323d":"learn.fit_one_cycle(5, lr_max=slice(0.004, 5e-2, 0.005))\nlearn.fit_one_cycle(25, lr_max=slice(0.004, 1e-2, 0.005), cbs=callbacks)","42b9004d":"learn.recorder.plot_loss()","71466c99":"test, test_pId = prep_nn_1(test_raw.copy(deep=True))\n\ntest_dl = learn.dls.test_dl(test)\n\npreds, _ = learn.get_preds(dl=test_dl)\npreds = np.argmax(preds, 1).numpy()\n\nsubmission = pd.DataFrame(\n    {'PassengerId': test_pId,\n     'Survived': preds}\n)\nsubmission.to_csv('submission_trivial.csv', index=False)","8929fd75":"Markdown(f\"\"\"\nWhen looking at he `Name` column ({full['Name'].head(3).tolist()} ...) one can see that the second word is the title.\nHowever, using the second string leads to many wrong associations, as can be seen in the following figure wich count how often which title occurs.\n\n\nInstead, using a regular expression to extract the title works much better.\n\"\"\")","a89637f8":"title_count = full['Name'].str.split().apply(lambda x: x[1]).value_counts().to_frame(name='Count')\nfig = px.bar(title_count, labels={'index': 'Title', 'value': 'Count'}, title='Title count by second word')\nfig.show(y='Count')","7550bc9a":"def title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","6e1b9ee6":"full['Title'] = full['Name'].apply(title)\nfull['Title'] = full['Title'].replace({'Ms': 'Miss', 'Mlle': 'Miss', 'Mme': 'Mrs'})\n# all title which occur less than 8 times\nrare = list((full['Title'].value_counts() < 8).replace({False: np.nan}).dropna().index)\nfull['Title'] = full['Title'].replace(rare, 'Rare')\nfull['Title'] = full['Title'].fillna('Unknown')\n\n\n# survival rate vs. title\nfig = px.bar(\n    full.groupby('Title')['Survived'].mean().to_frame(name='survival rate'),\n    width=800, height=400, labels={'value': 'Survival Rate'}, title='Survival rate by `Title`'\n)\nfig.show()","bf8b6432":"# create preprocess function for the title\n\n# sinse which title count as rare should not depend on if the training or the test set is used,\n# a global variable is created which later on is constant on all calls to `extract_title`\nfull['Title'] = full['Name'].apply(title)\nfull['Title'] = full['Title'].replace({'Ms': 'Miss', 'Mlle': 'Miss', 'Mme': 'Mrs'})\nRARE_TITLE = list((full['Title'].value_counts() < 8).replace({False: np.nan}).dropna().index)\nfull['Title'] = full['Title'].replace(RARE_TITLE, 'Rare')\n\ndef extract_title(df, reduce=True):\n    df['Title'] = df['Name'].apply(title)\n    if reduce:\n        df['Title'] = df['Title'].replace({'Ms': 'Miss', 'Mlle': 'Miss', 'Mme': 'Mrs'})\n        # all title which occur less than 8 times\n        df['Title'] = df['Title'].replace(RARE_TITLE, 'Rare')\n    \n    # just to be on the save side, fill all NAs\n    df['Title'] = df['Title'].fillna('Unknown')\n        \n    return df","fa4d017a":"def extract_name_length(df):\n    df['Name_Length'] = df['Name'].str.len()\n    return df","35608c4d":"fig = px.bar(\n    full.corr(method='pearson')['Age'].abs(),\n    width=600, height=400, labels={'value': 'Correlation', 'index': 'Feature'}, title='Correlation to Age'\n)\nfig.show()","19b352d9":"fig = px.bar(\n    full.groupby(['Title'])['Age'].agg(['mean', 'std']),\n    width=800, height=400, barmode='group', labels={'value': 'Age'}, title='Mean ans Std of Age vs. Title'\n)\nfig.show()","e04a523f":"# this data must be calculated upfront, such that it does not depend on the transformed dataset\nAGE_TITLE = full.groupby(['Title'])['Age'].agg(['mean', 'std'])\nAGE_TITLE.columns = [f'AGE_{i}' for i in AGE_TITLE.columns]\n\ndef fill_age_with_normal_rand(df):\n    df = df.merge(AGE_TITLE,\n                  how='left',\n                  left_on='Title',\n                  right_index=True)\n    df['RAND_AGE'] = df.apply(lambda x: np.random.normal(x['AGE_mean'], x['AGE_std']\/2.0), axis=1)\n    df['Age'] = df['Age'].fillna(df['RAND_AGE'])\n    df = df.drop(columns=['RAND_AGE', 'AGE_mean', 'AGE_std'])\n    return df","05396768":"# if the above is working, the following table has to be comparable with the first title-age table\nfig = px.bar(\n    fill_age_with_normal_rand(full[full['Age'].isna()]).groupby(['Title'])['Age'].agg(['mean', 'std']),\n    width=800, height=400, barmode='group', labels={'value': 'Age'}, title='Mean ans Std of randomly drawn Age vs. Title'\n)\nfig.show()","38644c61":"full['FamilySize'] =  full['Parch'] + full['SibSp'] + 1\n\nfig = px.bar(\n    full.groupby('FamilySize')['Survived'].mean(),\n    width=800, height=400, labels={'value': 'Survival Rate'}, title='Survival rate vs. FamilySize'\n)\nfig.show()","a56033e4":"def add_family_size(df):\n    df['FamilySize'] =  df['Parch'] + df['SibSp'] + 1\n    df['Single'] = (df['FamilySize'] == 1).astype(int)\n    # the size is transformed into a string, such that fast.ai handles it later\n    # autmatically as categorial variable\n    df['FamilySize'] = df['FamilySize'].astype(str)\n    return df","1ecfae6c":"full['HasCabin'] = full['Cabin'].notna()\nfull.groupby('HasCabin')['Survived'].mean()","d52789e3":"def cabin_information(df):\n    df['HasCabin'] = df['Cabin'].notna().astype(str)\n    df['CabinType'] = df['Cabin'].str.split(r'(^[A-Z])',expand = True)[1]\n    df['CabinType'] = df['CabinType'].fillna('Unknown')\n    df = df.drop(columns='Cabin')\n    return df","9e9b54fa":"full['CabinType'] = full['Cabin'].str.split(r'(^[A-Z])',expand = True)[1]\n\nfig = px.bar(\n    full.groupby('CabinType')['Survived'].mean(),\n    width=800, height=400, labels={'value': 'Survival Rate'}, title='Survival rate vs. Cabin Type'\n)\nfig.show()","dc9d38da":"Markdown(f\"\"\"\nThere are only {full['Embarked'].isna().sum()} missing 'Embarked' values. Any kind of model to fill these seems to be wrong, so we just fill in the mode.\n\"\"\")","3829b548":"EMBARKED_MODE = full['Embarked'].mode()[0]\n\ndef fill_embarked(df):\n    df['Embarked'] = df['Embarked'].fillna(EMBARKED_MODE)\n    \n    return df","53e17003":"Markdown(f\"\"\"\nAlso Fare has only {full['Fare'].isna().sum()} missing value, so we do the same here.\n\"\"\")","fb0195ab":"FARE_MEDIAN = full['Fare'].median()\n\ndef fill_fare(df):\n    df['Fare'] = df['Fare'].fillna(FARE_MEDIAN)\n    \n    return df","7d571f77":"full['TicketCat'] = full['Ticket'].str[0:2]\nRARE_TICKET = list((full['Ticket'].value_counts() < 20).replace({False: np.nan}).dropna().index)","c5d5bee9":"def ticket_cat(df, reduce=True):\n    df['TicketCat'] = df['Ticket'].str[0:2]\n    \n    df['TicketCat'] = df['TicketCat'].replace(RARE_TICKET, 'Rare')\n    \n    # just to be on the save side, fill all NAs\n    df['TicketCat'] = df['TicketCat'].fillna('Unknown')\n        \n    return df","6621e50e":"torch.device('cuda') # enable cuda, (activate GPU usage)\n\npreps = [extract_title,\n         extract_name_length,\n         fill_age_with_normal_rand,\n         add_family_size,\n         cabin_information,\n         fill_embarked,\n         fill_fare, \n         ticket_cat,\n         remove_wordy_cols,\n         split_data]\nprep_nn_2 = lambda x : apply_all(x, preps)\n\ntrain = train_raw.copy(deep=True)\ntrain, train_pId = prep_nn_2(train)\n\ncont_names = ['Pclass', 'Age', 'Name_Length', 'Parch', 'SibSp'] # set the continous variables\ncat_names =  ['Sex', 'TicketCat', 'CabinType', 'HasCabin', 'Single', 'Title', 'Embarked', 'Fare',  'FamilySize'] # set the categoriall variables\nprocs = [Categorify, Normalize] # different fast.ai preprocessing steps\ndep_var = 'Survived' # our target variable\n\nsplits = RandomSplitter(valid_pct=0.2, seed=42)(train.index) # to validate the results we use randomly 20% of the training set\n\ncfg = tabular_config(embed_p=0.25, ps=0.05) # enable drop-out layers and set droupout to 10%\n\ndls = TabularPandas(train,\n                    cont_names=cont_names,\n                    cat_names=cat_names,\n                    procs=procs,\n                    y_names=dep_var,\n                    splits=splits).dataloaders(bs=64)\n\n\ncallbacks = [SaveModelCallback(min_delta=0.005, monitor='accuracy', fname='model_feat01_best')]\n\nlearn = tabular_learner(dls, layers=[500,200,100], metrics=[accuracy], config=cfg)\n\nlearn.lr_find()","9250a280":"learn.fit_one_cycle(5, lr_max=slice(0.004, 5e-2, 0.05))\nlearn.fit_one_cycle(45, lr_max=slice(0.005, 1e-2, 0.005), cbs=callbacks)","0ef37c93":"test, test_pId = prep_nn_2(test_raw.copy(deep=True))\n\ntest_dl = learn.dls.test_dl(test)\n\npreds, _ = learn.get_preds(dl=test_dl)\npreds = np.argmax(preds, 1).numpy()\n\nsubmission = pd.DataFrame(\n    {'PassengerId': test_pId,\n     'Survived': preds}\n)\nsubmission.to_csv('submission_feature_eng.csv', index=False)","3cd2094b":"splits = RandomSplitter(valid_pct=0.01, seed=42)(train.index)\ncfg = tabular_config(embed_p=0.25, ps=0.05) # enable drop-out layers and set droupout to 10%\n\ndls = TabularPandas(train,\n                    cont_names=cont_names,\n                    cat_names=cat_names,\n                    procs=procs,\n                    y_names=dep_var,\n                    splits=splits).dataloaders(bs=64)\n\n\nlearn = tabular_learner(dls, layers=[500,200,100], metrics=[accuracy], config=cfg)","3bb470f6":"learn.fit_one_cycle(5, lr_max=slice(0.004, 5e-2, 0.05))\nlearn.fit_one_cycle(15, lr_max=slice(0.005, 1e-2, 0.005))","4f0acd32":"test, test_pId = prep_nn_2(test_raw.copy(deep=True))\n\ntest_dl = learn.dls.test_dl(test)\n\npreds, _ = learn.get_preds(dl=test_dl)\npreds = np.argmax(preds, 1).numpy()\n\nsubmission = pd.DataFrame(\n    {'PassengerId': test_pId,\n     'Survived': preds}\n)\nsubmission.to_csv('submission_feature_eng_full.csv', index=False)","be12618b":"## Replacing NAs from Age\n\nIn the first model we filled the missing `Age` values with the mean value of the age, but one can do better.\nOn the one hand the Pclass variable has a good correlation to the age, but the title is even better.\nSince it is not a great idea to fill a lot of NAs with the same value, we draw the values randomly from\na normal distribution with the mean value and standard deviation of the all passengers with the same title.","145f3cd0":"## Extracting the title","6d082458":"This time we retrain our model with almost all data, after we found a good set of hyperparameters.\n\nThe kaggle results were at an accuracy of 0.79, although not much gain for this amount of feature engineering. I guess due to the small\ndataset we cannot get much better with neural networks.","61826d82":"## Starting the first NN\n\nTo set up the training of a neural network with fast.ai, a DataLoader and a learner is needed.\n`TabularPandas` let us create a dataset from a pandas DataFrame which is converted to a dataloader via\n`.dataloaders`(). This dataloader is then used to create the neural network for tabular data by\n`tabular_leaerner`. Fast.ai provides  us we all kind of neat tricks for model optimization and improved learning.\nFor example, the neural network contains batch normalization layers, drop out layers, and categorial embeddings.\nAlso, the one cycle fit policy together with the Adam optimizer does great work.\n\nWhen you run the following lines multiple time, you get different results for each run, the training is not deterministic.\nUsually it is possible to make it deterministic by setting all kind of seed; however, id didn't worked  out for me\non kaggle.\nOn larger dataset this is also not a real problem. Due to the small sizes of the Titanic dataset, the results fluctuates  quite a bit.\nFor that reason, I applied a `SaveModelCallback` which saves each iteration of the training, and loads in the best result\nat the end of the training. This mitigates the random effects a bit.\n\nNotes:\n* `toch.device('cuda')` just enable the usage of the GPU (don't forget to enable the GPU inside kaggle).\n* Using `dls.show_batch()` one can check that creating the dataloader works and\n  since fast.ai is based on pytorch one can easily inspect the generated neural network.\n* `learn.loss_func` shows that fast.ai automatically chooses CrossEntropyLoss for classifcation tasks\n","7e28bbd6":"To create the submission we need to let fast.ai run over the test data.\nThus we need to apply the same preprocessing as before `prep_nn_1`.\nThen we have to put in the DataFrame into the learners test data loader and run the predictions.\nThe result of the prediction is a two-dimensional array, where the first columns is the probability of surviving in the\nsecond column is the probability of not surviving. `np.argmax()` gives the most likely result for each passenger.\nFinally, we need to stack the prediction and the passengerIds to create the submission.\n\nThis submission achieved a accuracy of 0.77, not too good, but also not bad for doing almost nothing.\nPlease note, due to the non-deterministic behavior your results may fluctuate.","1d22bc10":"# Seting up the environment\n\nThe first steps are loading all necessary python packes and, of course, loading the Titanic data itself.","f1c5a282":"For all furhter investigations I stack the training and test data. This helps cleaning the data and try out different things.\nIf I have to distinguish training and test data I can use `full['Survived'].isna()` as a mask.\n\n**In the real world situation, one should never do this.**\nCalculating statistics from the test set will influence how you handle the dataset.\nFor example, when filling missing values with the mean value of all other occurrences is wrong.\n1The test set's measured accuracy is not comparable with data you have never seen if you do so.\nHowever, this kaggle, and I guess there will be no more *new* Titanic data in the future.","3d065aac":"The training in fast.ai is applied by the so-called `one cycle fit policy`. Which means the learning rate will first increased and then decreases throughout the fitting.\nTo find a good learning rate, the learning rate finder will go through different learning rates and show the results, `learn.lr_find()`.","8e052dcb":"## Preface\n\nThe notebook uses the tabular model from fast.ai to predict the survivors of the Titanic. Fast.ai [https:\/\/www.fast.ai\/] is a library sitting on top of pytorch [pytorch.org]\nwhich makes it very easy to apply deep learning to standard tasks as image recognition, nlp, or handling tabular data.\n\nHere are a few links to other kaggle notebooks exploring the dataset:\n* [https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic] - gives a good overview of the dataset itself\n* [https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions] - applies diffrenet `classic` machine learning algorithms\n* [https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial] - here you can find a lot of good feature engineering\n* [https:\/\/www.kaggle.com\/ssykiotis\/solving-titanic-using-deep-learning-and-fast-ai] - another approach using fast.ai\n\nIn this notebook I will first apply fast.ai as early as possible, which means with only the bare minimum of feature engineering.\nThen I want to compare these results with and a seconda approach with an decent amount of feature engineering and a third\napproach where I applied all kind of hand wrangling the data I can think of, or I've found in other notebooks.\n\nThe notebook will not contain a detailed analysis of the data itself.","0caf0478":"The following plot shows the validation and training loss for each epoch.","cf1126e5":"A brief check if the chaining above has worked:","534aa333":"## Family Size\n\nEither 'Singles' or large families (>4) have the highest risk to `not survive`. Which means there is \"no\" linear correlation between family size\nand survival probability. For that reason, I transform it into a categorial variable. Later we could try use weighting of evidance to create even\na better feateure out of it.","12e91420":"Setting up the preprocessing by chaining different function help later to play arround with different preprocess steps.","2bbcc5a7":"# Basic feature engineering\n\nNow I repeat the whole process, but this time with a little bit of feature engineering.","99c7a9fa":"# Neural network without feature engineering\n\nAlthough I don't want to investigate the data in detail, a brief look is necessary to apply a minimum of data wrangling.\n\n`full.head(3)` shows the first 3 rows od the dataset\n* PassengerId: Is just a monotonically increasing Id, which needs to be removed before training.\n  However, the PassengerId is needed later when creating the submission for the kaggle competition.\n* Survived: Is the target variable.\n    * 1 == survived\n    * 0 == not survived\n* Pclass: Socio-Economic status of the passenger.\n    * 1 == upper class\n    * 2 == middle class\n    * 3 == lower class\n* Name: The name of the passenger. Obviously, the name cannot be used as a feature for the training, as every passenger has its own name.\n  But one can derive several other features from it, such as the title, the length of the name, or even the last name.\n* Sex: The sex of the passenger.\n    * male == male\n    * female == female, what a surprise\n* Age: The age of the passenger.\n* SibSp: The total number of the passengers' siblings and spouse\n* Parch: The total number of the passengers parents and children.\n* Ticket: The ticket number.\n* Cabin: The cabin number\n* Embarked. The port of embarkation\n    * C == Cherbourg\n    * Q == Queenstown\n    * S == Southamption","c76dbd5a":"## Ticket\nThere are quite many different tickets. Here we just use the first two letter as own category","9ee95c40":"Even without feature engineering, we need to drop a few columns as they are not useful without any further handling.\n* Name\n* Ticket\n* Cabin\n* PassengerId\n\nAll these columns are either almost unique per row or at least to diverse per row. Of course, later on, one can maybe derive\nuseful features from them.\n\nI will put all feature handling in their own function, such that I can easier apply them on different datasets.","da9e809c":"## Build NN with engineered features\n\nWe do exactly the same as the first time, but now with all the new features. But the accuracy on the validation set has only improved\na tiny bit, honestly, only within the fluctuation of the errors.","5dacf4ac":"## Fare","61522782":" ## Embarked\n \n","747db9c2":"## Cabin\n\nWhereas the cabin number is hard to use as feature, the information if a passenger has a cabina at all is a good indicator. Also some letter in the cabin\ncontains information of the cabin type."}}