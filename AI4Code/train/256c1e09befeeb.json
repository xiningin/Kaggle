{"cell_type":{"7b771634":"code","7c69db3b":"code","704f0f70":"code","77089f4f":"code","8773c71e":"code","6e169d34":"code","0d270d00":"code","6611edb7":"code","125ee80c":"code","77bb5271":"code","46cd9f64":"code","bce2fa48":"code","be997630":"code","6a98bae4":"code","c9088287":"code","10619a7a":"code","1c71b07f":"code","d02b6f3d":"code","9e03c64e":"code","90beab6b":"code","4cf26d22":"code","72dc2c85":"code","7b43d0d1":"code","c7b63ac1":"code","e973fd5d":"code","d61ec026":"code","e4b2d09c":"code","13277ff0":"code","34229712":"code","5c4b40a0":"code","bbb2abe2":"code","2f7cde14":"code","30182942":"code","a0107ad4":"code","072b7466":"code","ef89081d":"code","ba696056":"code","23fa3fcc":"code","9509ba1f":"code","c4706ad9":"code","57540ed3":"code","2568910b":"code","5c367886":"code","c0891b96":"code","a9d1ed82":"code","2e9cabd6":"code","058d450b":"code","4f88fc69":"code","f8aae7d2":"code","ac4c25b9":"code","611601dd":"code","f8bf7e9a":"code","8ff1f5ea":"code","63326a30":"code","c06bc723":"code","c4c152b5":"code","fafe7bd2":"code","84e40ba8":"code","7b309d44":"code","b695a77b":"code","673dacd1":"code","c0fa5d48":"code","6d363f83":"code","5ba0e0e7":"code","ac8a7ac9":"code","9322697c":"code","9daf2ee9":"code","9945b2c3":"code","fa3c9ca6":"code","1c11b123":"code","89af9a63":"code","38ef6c08":"code","52e8f456":"code","9a8e9222":"code","a74f43d0":"code","bbd506d7":"code","bd0cb062":"code","b1ee6705":"code","31970578":"code","d09a575c":"code","1637b9bd":"code","29f0966f":"code","7205fb3c":"code","7415d5eb":"code","2eb2787c":"code","d46b524f":"code","93ffc6b2":"code","7e35c282":"code","6fa9053c":"code","d90d1d51":"markdown","ee917d48":"markdown","2cc057d8":"markdown","44ff653c":"markdown","308df4fc":"markdown","ec1f68e3":"markdown","1ebfd783":"markdown","2469edf0":"markdown","e1a39ca4":"markdown","a5d0d0e7":"markdown","471a1aff":"markdown","c1fe65c4":"markdown","15605617":"markdown","78c657d8":"markdown","5a095f9a":"markdown","1ffe70a3":"markdown","9f47ed60":"markdown","0e31a432":"markdown","641a2040":"markdown","408a23b6":"markdown","c723be1b":"markdown","969cdac7":"markdown","f0bf5f1b":"markdown","4195ac39":"markdown","045b2516":"markdown","3e3d3c2a":"markdown","7de4b4c1":"markdown","06e1ebbd":"markdown","16f491cc":"markdown","494838fb":"markdown","298787d4":"markdown","1324f608":"markdown","6326c29b":"markdown","9a709986":"markdown"},"source":{"7b771634":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import Counter\nimport missingno as msno\nimport seaborn as sns","7c69db3b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","704f0f70":"#pd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)\n\nsignal=pd.read_csv('\/kaggle\/input\/uci-semcom\/uci-secom.csv')","77089f4f":"signal.head()","8773c71e":"signal.info()","6e169d34":"#the blank space in the matrix that gets plotted indicate the number of missing values visually\nmsno.matrix(signal);","0d270d00":"signal.isnull().sum().sum()","6611edb7":"signal.describe().T","125ee80c":"signal.dtypes.value_counts()","77bb5271":"Counter(signal['Pass\/Fail'])","46cd9f64":"empty_cols=signal.columns[signal.isna().mean()>=.25]\nempty_cols.shape","bce2fa48":"signal_df=signal.drop(empty_cols,axis=1)\nsignal_df.shape","be997630":"column=signal.columns","6a98bae4":"nunique = signal_df.apply(pd.Series.nunique)\nnunique","c9088287":"const_cols = nunique[nunique == 1].index\nconst_cols.shape","10619a7a":"signal_df=signal_df.drop(const_cols,axis=1)\nsignal_df.shape","1c71b07f":"from datetime import datetime\nsignal_df['year'] = pd.DatetimeIndex(signal_df['Time']).year\nsignal_df['month'] = pd.DatetimeIndex(signal_df['Time']).month\nsignal_df['date'] = pd.DatetimeIndex(signal_df['Time']).day\nsignal_df['week_day'] = pd.DatetimeIndex(signal_df['Time']).weekday\nsignal_df['start_time'] = pd.DatetimeIndex(signal_df['Time']).time\nsignal_df['hour'] = pd.DatetimeIndex(signal_df['Time']).hour\nsignal_df['min'] = pd.DatetimeIndex(signal_df['Time']).minute\nsignal_df=signal_df.drop('Time',axis=1)","d02b6f3d":"signal_df.head()","9e03c64e":"signal_df.year.unique()","90beab6b":"signal_df.month.unique()","4cf26d22":"signal_df.date.unique()","72dc2c85":"signal_df.week_day.unique()","7b43d0d1":"sns.histplot(data=signal_df, x=\"month\", hue=\"Pass\/Fail\",stat=\"probability\", discrete=True,);\nsns.despine(right=True,top=True)","c7b63ac1":"sns.histplot(data=signal_df, x=\"date\", hue=\"Pass\/Fail\",stat=\"probability\", discrete=True,);\nsns.despine(right=True,top=True)","e973fd5d":"sns.histplot(data=signal_df, x=\"week_day\", hue=\"Pass\/Fail\",stat=\"probability\", discrete=True,);\nsns.despine(right=True,top=True)","d61ec026":"sns.histplot(data=signal_df, x=\"hour\", hue=\"Pass\/Fail\",stat=\"probability\", discrete=True,);\nsns.despine(right=True,top=True)","e4b2d09c":"signal_df1=signal_df.drop(['year','month','date','week_day','start_time','hour','min'],1)","13277ff0":"signal_df1.isnull().sum().sum()","34229712":"msno.matrix(signal_df1);","5c4b40a0":"empty_cols=signal_df1.columns[signal_df1.isna().mean()>=.15]\nempty_cols","bbb2abe2":"signal_df2=signal_df1.drop(empty_cols,axis=1)\nsignal_df2.shape","2f7cde14":"msno.matrix(signal_df2);","30182942":"signal_df2.isnull().sum().sum()","a0107ad4":"from sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=3)\narray=imputer.fit_transform(signal_df2)","072b7466":"cols=signal_df2.columns","ef89081d":"signal_df3 = pd.DataFrame(array, columns = cols)","ba696056":"signal_df3.head()","23fa3fcc":"msno.matrix(signal_df3);","9509ba1f":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(v):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = v.columns\n    vif[\"VIF\"] = [variance_inflation_factor(v.values,i) for i in range(v.shape[1])]\n\n    high_vif = vif[vif[\"VIF\"] > 5].sort_values(\"VIF\",ascending=False)\n\n    \n    return(high_vif)","c4706ad9":"# Create correlation matrix\ncorr_matrix = signal_df3.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\n# Find features with correlation greater than 0.95\nto_drop1 = [column for column in upper.columns if any(upper[column] < -0.9)]\nto_drop2 = [column for column in upper.columns if any(upper[column] > 0.9)]\nprint(to_drop1)\nprint(to_drop2)\ndrop=to_drop1+to_drop2","57540ed3":"df=signal_df3.drop(drop,1)","2568910b":"df.shape","5c367886":"feature=df.drop(\"Pass\/Fail\",1)\ny=df[\"Pass\/Fail\"]\ncol=list(feature.columns)","c0891b96":"from scipy import stats\n\ndef outliers_high(feat):\n    q1= feat.quantile(.25)\n    q2= feat.quantile(.5)\n    q3= feat.quantile(.75)\n    feat = np.where(feat > q3+ stats.iqr(feat),q2,feat)\n    return feat\n\ndef outliers_low(feat):\n    q1= feat.quantile(.25)\n    q2= feat.quantile(.5)\n    q3= feat.quantile(.75)\n    feat = np.where(feat < q1 - stats.iqr(feat),q2,feat)\n    return feat","a9d1ed82":"feature.plot(kind='box', subplots=True,layout=(52,5), fontsize=10, figsize=(15,150));","2e9cabd6":"features=feature.copy()\nfor column in feature:\n    features[column] = outliers_high(features[column])\n    features[column] = outliers_low(features[column])","058d450b":"features.plot(kind='box', subplots=True,layout=(52,5), fontsize=10, figsize=(15,150));","4f88fc69":"#for i in df.describe().columns:\n#    sns.displot(data=df,x=i, hue=\"Pass\/Fail\",kind=\"kde\", height=3,multiple=\"fill\", clip=(0, None),\n#            palette=\"ch:rot=-.25,hue=1,light=.75\");\n#    plt.show()","f8aae7d2":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaled_feature = scaler.fit_transform(features)\nscaled_feature = pd.DataFrame(scaled_feature,columns=col)","ac4c25b9":"#for i in scaled_feature.describe().columns:\n#    sns.kdeplot(data=scaled_feature,x=i)\n#    plt.show();","611601dd":"sns.jointplot(data=df,x=\"18\", y=\"51\", hue=\"Pass\/Fail\",kind=\"kde\");","f8bf7e9a":"sns.jointplot(data=df,x=\"75\", y=\"20\", hue=\"Pass\/Fail\",kind=\"kde\");","8ff1f5ea":"sns.jointplot(data=df,x=\"125\", y=\"120\", hue=\"Pass\/Fail\",kind=\"kde\");","63326a30":"sns.relplot(x=\"356\", y=\"268\", hue=\"Pass\/Fail\", size=\"hour\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=signal_df);","c06bc723":"sns.relplot(x=\"176\", y=\"267\", hue=\"Pass\/Fail\", size=\"hour\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=signal_df);","c4c152b5":"from sklearn.decomposition import PCA\n\n# Creating a covariance matrix\n\ncov_matrix = np.cov(scaled_feature.T)\nprint('Covariance Matrix \\n', cov_matrix)\n      \n#perform an eigendecomposition on the covariance matrix:\n\neig_vals, eig_vecs = np.linalg.eig(cov_matrix)      ","fafe7bd2":"tot = sum(eig_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(\"Cumulative Variance Explained\", cum_var_exp)","84e40ba8":"plt.figure(figsize=(12 ,6))\nplt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()  ","7b309d44":"pca = PCA(n_components=scaled_feature.shape[1],random_state=1).fit(scaled_feature.values)\na = np.cumsum(pca.explained_variance_ratio_)","b695a77b":"i = 0\nwhile a[i] < 0.9:\n    i = i+1\nf'Number of dimensions needed to capture 90%% of variance:{i}'","673dacd1":"# PCA with reduced number of components\n\npca = PCA(n_components=i,random_state=1)\npca.fit(scaled_feature)\nprint(pca.components_)\n\npca_df= pd.DataFrame(pca.fit_transform(scaled_feature))","c0fa5d48":"pca_df.shape","6d363f83":"f, (ax_heatmap, ax_heatmap2) = plt.subplots(2,figsize=(16,12))\nax_heatmap.set_title('Before PCA')\nax_heatmap2.set_title('After PCA')\n\nsns.heatmap(scaled_feature,ax=ax_heatmap,cmap='plasma');\nsns.heatmap(pca_df,ax=ax_heatmap2,cmap='plasma');","5ba0e0e7":"#for i in pca_df.describe().columns:\n#    sns.kdeplot(data=pca_df,x=i)\n#    plt.show();","ac8a7ac9":"calc_vif(scaled_feature)","9322697c":"calc_vif(pca_df)","9daf2ee9":"# Univariate feature selection using K best features\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nX_uv = SelectKBest(chi2, k=50).fit(scaled_feature, y)\nX_uv.get_support(indices=True)\n\nX_uv = pd.DataFrame(SelectKBest(chi2, k=50).fit_transform(scaled_feature, y))\nprint(X_uv.shape)","9945b2c3":"cov_matrix2 = np.cov(X_uv.T)\neig_vals, eig_vecs = np.linalg.eig(cov_matrix2)    \ntot2 = sum(eig_vals)\nvar_exp2 = [( i \/tot ) * 100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp2 = np.cumsum(var_exp2)\nprint(\"Cumulative Variance Explained\", cum_var_exp2)","fa3c9ca6":"plt.figure(figsize=(12 ,6))\nplt.bar(range(1, eig_vals.size + 1), var_exp2, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, eig_vals.size + 1), cum_var_exp2, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()  ","1c11b123":"#We had initially note a class imbalance, we should first try and resolve that before we start making a model.\nCounter(signal['Pass\/Fail'])","89af9a63":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import scorer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split","38ef6c08":"X_train,X_test,y_train,y_test=train_test_split(pca_df, y, test_size =0.2,random_state =1,stratify=y)\nX_train.shape,X_test.shape","52e8f456":"classifiers = [['Naive Bayes :', GaussianNB()],\n               ['KNeighbours :', KNeighborsClassifier()],\n               ['SVM :', SVC()],\n               ['LogisticRegression :', LogisticRegression()],\n               ['DecisionTree :',DecisionTreeClassifier()],\n               ['RandomForest :',RandomForestClassifier()]]","9a8e9222":"from imblearn.over_sampling import SMOTE\n\nosmote=SMOTE()\nXs_train,ys_train=osmote.fit_resample(X_train,y_train)\nXs_test,ys_test=osmote.fit_resample(X_test,y_test)\n\nprint(Counter(ys_train))","a74f43d0":"for name,classifier in classifiers:\n    clf=classifier.fit(Xs_train,ys_train)\n    y_pred=classifier.predict(Xs_test)\n    print(f'\\n {name} \\n')\n    print(f'Training Score for {name}  {clf.score(Xs_train,ys_train) * 100:.2f}' )\n    print(f'Testing Score for {name} {clf.score(Xs_test,ys_test) * 100:.2f}' )\n    print(f'Classification report  \\n {classification_report(ys_test,y_pred)}' )\n    print(f'Confusion matrix  \\n {confusion_matrix(ys_test,y_pred)}' )\n    print(f'ROC AUC  : {roc_auc_score(ys_test,y_pred)}' )\n","bbd506d7":"from imblearn.under_sampling import RandomUnderSampler\n\nrus=RandomUnderSampler()\nXrus_train,yrus_train =rus.fit_resample (X_train,y_train)\nXrus_test,yrus_test   =rus.fit_resample (X_test,y_test)\n\nprint(Counter(yrus_train))","bd0cb062":"for name,classifier in classifiers:\n    clf=classifier.fit(Xrus_train,yrus_train)\n    y_pred=classifier.predict(Xrus_test)\n    print(f'\\n {name} \\n')\n    print(f'Training Score for {name}  {clf.score(Xrus_train,yrus_train) * 100:.2f}' )\n    print(f'Testing Score for {name} {clf.score(Xrus_test,yrus_test) * 100:.2f}' )\n    print(f'Classification report  \\n {classification_report(yrus_test,y_pred)}' )\n    print(f'Confusion matrix  \\n {confusion_matrix(yrus_test,y_pred)}' )\n    print(f'ROC AUC  : {roc_auc_score(yrus_test,y_pred)}' )","b1ee6705":"from imblearn.combine import SMOTETomek\n\nsmt=SMOTETomek()\nXsmt_train,ysmt_train =smt.fit_resample (X_train,y_train)\nXsmt_test,ysmt_test   =smt.fit_resample (X_test,y_test)\n\nprint(Counter(ysmt_train))","31970578":"for name,classifier in classifiers:\n    clf=classifier.fit(Xsmt_train,ysmt_train)\n    y_pred=classifier.predict(Xsmt_test)\n    print(f'\\n {name} \\n')\n    print(f'Training Score for {name}  {clf.score(Xsmt_train,ysmt_train) * 100:.2f}' )\n    print(f'Testing Score for {name} {clf.score(Xsmt_test,ysmt_test) * 100:.2f}' )\n    print(f'Classification report  \\n {classification_report(ysmt_test,y_pred)}' )\n    print(f'Confusion matrix  \\n {confusion_matrix(ysmt_test,y_pred)}' )\n    print(f'ROC AUC  : {roc_auc_score(ysmt_test,y_pred)}' )","d09a575c":"from imblearn.under_sampling import ClusterCentroids\n\ncc=ClusterCentroids()\nXcc_train,ycc_train =cc.fit_resample (X_train,y_train)\nXcc_test,ycc_test   =cc.fit_resample (X_test,y_test)\n\nprint(Counter(ycc_train))","1637b9bd":"for name,classifier in classifiers:\n    clf=classifier.fit(Xcc_train,ycc_train)\n    y_pred=classifier.predict(Xcc_test)\n    print(f'\\n {name} \\n')\n    print(f'Training Score for {name}  {clf.score(Xcc_train,ycc_train) * 100:.2f}' )\n    print(f'Testing Score for {name} {clf.score(Xcc_test,ycc_test) * 100:.2f}' )\n    print(f'Classification report  \\n {classification_report(ycc_test,y_pred)}' )\n    print(f'Confusion matrix  \\n {confusion_matrix(ycc_test,y_pred)}' )\n    print(f'ROC AUC  : {roc_auc_score(ycc_test,y_pred)}' )","29f0966f":"nb_classifier = GaussianNB()\n\nparams_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\ngs_NB = GridSearchCV(estimator=nb_classifier, \n                 param_grid=params_NB, \n                 cv=10,   \n                 verbose=1, \n                 scoring='accuracy',\n                 n_jobs=-1) \ngrid_fit=gs_NB.fit(Xcc_train, ycc_train)\n\ntuned_nb=grid_fit.best_params_","7205fb3c":"tuned_nb = GaussianNB()\ntuned_nb.fit(Xcc_train,ycc_train)\n\nprint(f'Training Score : {tuned_nb.score(Xcc_train,ycc_train) * 100:.2f}' )\nprint(f'Testing Score : {tuned_nb.score(Xcc_test,ycc_test) * 100:.2f}' )","7415d5eb":"nb_pred=tuned_nb.predict(Xcc_test)\nacc = accuracy_score(ycc_test,nb_pred)\nprint(\"Accuracy: {0:.2f} %\".format(100 * acc))\nprint(classification_report(ycc_test, nb_pred))","2eb2787c":"svm_classifier = SVC()\n\nparams_svm =  {'kernel':['rbf','poly'],'degree':[3,4,5],'C':[0.01,0.05,0.1,0.2,0.5,0.7,1,1.5,2]}\ngs_svm = GridSearchCV(estimator=svm_classifier, \n                 param_grid=params_svm, \n                 cv=10,   \n                 verbose=1, \n                 scoring='accuracy',\n                 n_jobs=-1) \ngrid_fit=gs_svm.fit(Xcc_train, ycc_train)\n\ntuned_svm=gs_svm.best_params_","d46b524f":"tuned_svm = SVC()\ntuned_svm.fit(Xcc_train,ycc_train)\n\nprint(f'Training Score : {tuned_svm.score(Xcc_train,ycc_train) * 100:.2f}' )\nprint(f'Testing Score : {tuned_svm.score(Xcc_test,ycc_test) * 100:.2f}' )","93ffc6b2":"svm_pred=tuned_svm.predict(Xcc_test)\nacc = accuracy_score(ycc_test,svm_pred)\nprint(\"Accuracy: {0:.2f} %\".format(100 * acc))","7e35c282":"print(classification_report(ycc_test, svm_pred))","6fa9053c":"import pickle \n\n#our best model has been\nbest_model=tuned_nb\nbest_model.fit(Xcc_train,ycc_train)\n\n# Save the trained model as a pickle string. \nsaved_model = pickle.dumps(best_model) \n\n# Save to file in the current working directory\npkl_filename = \"pickle_model.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(best_model, file)\n\n# Load from file\nwith open(pkl_filename, 'rb') as file:\n    pickle_model = pickle.load(file)\n    \n# Calculate the accuracy score and predict target values\nscore = pickle_model.score(Xcc_test, ycc_test)\nprint(\"Test score: {0:.2f} %\".format(100 * score))\nYpredict = pickle_model.predict(Xcc_test)","d90d1d51":"As we can see dimensions have now been reduced from 591 to 143 features which cover 90 percent of variance in data and also show uniform distribution.The VIF from the dataset is also removed after doing PCA as expected.This combination should help us predict accurately with faster speed.\n\nAlternatively\n\n143 Features still might be a lot to visulaize and check for intuitive patterns, we can try using sklearn feature selection methods to analyse the top features of the datasets and maybe we can get an even smaller dataset without losing valuable information","ee917d48":"Repeating the PCA steps from earlier we can see the Univariate feature selection has probably shortlisted the best 50 features but given the nature of the dataset these are too little to capture the overall variance, so we wont be pursuing this further.","2cc057d8":"104 vlaues belong to the fail Category and 1463 to Pass. Large Class imbalance can be seen.<br>\nWe earlier saw a large number of missing values,Accurately imputing them will be a challenge plus it will be difficult to even verify so we should remove some features manually.","44ff653c":"We now have to deal with outliers and multcollinearity.<br>\nDealing with outlier would be challenging as none of features are named.<br>\nPCA would be effective in reducing dimensions and can easily identify principal component as all highly colinear features have been eliminated.","308df4fc":"We can see two lists of highly correlated features.160 positvely correlated and 5 negatively above a threshold of 95 percent.We will be dropping them both.","ec1f68e3":"To recap and conclude:\n\nThe original dataframe consists of 1567 examples of recordings from 590 sensors in a semiconductor manufacturing plant each with a label of PASS or FAIL.It did not have labels for any of the features we could use to derive meaning from data. <br>\nThe data volume was recorded over the period of an entire year in which the production happened 24\/7 throughout the year and we saw a peak durig the months of August and September.<br>\n\nThe dataframe had its share of faulty values and null recordings which we had to impute or drop given the situation.<br>\nThere was also a huge class imbalance and as we explored the data it became apparent that this was a problem of anamoly detection.There was no significant predictor as such which could show a single reason for Failure in manufacturing.\n\nTherefore we had to trim down to a smaller subset in order to find patterns between the two labelled outcomes.\nThe model we have concluded on encapsulates 90 percent of the total variation.\n\nA labelled dataset would have helpful to get a feel of the feature importance and to identify critical points in the entire manufacturing process. Since the manufacture of a single item is a long process and every step in the process adds more value to the product it would be ideal if we could check for faults earliear or at critical points of the entire process instead of checking at the end. This can greatly help help in reducing manufacturing cost by identifying False positives early in the production flow.\n\nAnother reason for having to trim down the data size was the amount of noise in the data, the company needs to take a call if it is worth spending on maintaining these sensors better as it will give a much more robust model if we can calculate for a larger dataset.\n\nThe dataset exhibited high collinearity and multicollinearity. This might indicate that we may not need sensors at every stage of the production rocess because they might be recording redundant repetative data. This can be a possible solution of cost control as well.One possible way is we should focus on installing high quality sensors at critical points of the manufacturing process and remove the already faulty ones which provide duplicate data instead of having to spend on repairing them.\n\nPCA was able to identify 143 points out of 592 which covered 90 percent variance, with domain expertise we can surely reduce that further to actual critical points to measure accurate data from.","1ebfd783":"RandomForest  has performed the best so far,followed by Naive Bayes and SVM on this dataset.These models are identifying classes very accurately. <br>\nWe will use the GaussianNB Model to further tune on ClusterCentroids dataset since this has shown the highest accuracy on base model over all variations so far consistently.","2469edf0":"We can see a clear distinction in heatmaps for before and after pca was done.<br>\nPost PCA visually from the color map we can easily conclude that most of the dataframe now lies in the acceptable range of colinearity and therefore can be termed as independent features suitable for robust model making.","e1a39ca4":"As we can observe almost all the features have outliers present in them which is not a good indication.","a5d0d0e7":"We are now left with fewer nan values which we can now consider imputing.\nAt the started the dataset had 41951 missing values which we have reduced to 8008 along with dealing 116 faulty features.\nWe can visually with the graph it has lesser gaps and those existing seem to be following a pattern therefore imputing an average value can make sense here","471a1aff":"GaussianNB model on transformation done to balance the imbalance via CLusterCentroid has worked best for test and train with a close follow up by the svm model.<br>\nOur best model has been easy to get as most without any need for extensive hyperparameter tuning.<br>\nMost of the work in feature enginnering and cleaning the errors in data has been useful to get such high values of accuracy.","c1fe65c4":"As we can see from the shape we have almost reduced the data into half its original size in order to take care of all irregularities and make it suitable for a robust predictive model. The columns have reduced from 592 to 258.","15605617":"Our data has been finally cleansed to check for patterns, considering it has many features there might be those with very high collinearity or muticollinearity.We should find these features and eliminate them as well to crate a more robust model.","78c657d8":"We can see  that 32 columsn have more than a quarter of the data missing, so we will be dropping them.","5a095f9a":"Lets try SVM also","1ffe70a3":"By going through the set of distributions we get an intuition which features can help predict pass\/fail.We can see a higher frequency of failures for these following 33 features out of 258.<br>\n\n22,25,26,33,67,75,87,89,99,114,163,164,166,167,195,196,197,199,200,201,203,204,253,331,337,348,430,431,434,438,471,476,521","9f47ed60":"These look the columns with high number of missing values seen on the right end of the graph, we will elimainate these as well to get an overall clean dataset.","0e31a432":"So far we have seen that the errors in manufacturing process do not show a significant pattern pointing to a  reason of failure.It is possible that the initial set of features we eliminated which had no\/faulty sensor values could have given a clue  but it is highly unlikely a pattern would have emerged.We also couldnt compress the dataset to a small number as the variance wasnt getting captured.<br>\nThese faults seem to be more like random anamolies so far, lets see if the models can find a pattern.","641a2040":"Naive Bayes is performing best again on this dataset.<br>\nWe can see from the classification report some of the models cannot decipher the minority class even after SMOTETomek.\n\nTrying an alternative of **Cluster Centroids** if they can differentiate the data from the anamolies.","408a23b6":"As we can see the number of missing values are quite large and sometimes more than half the values of the columns are entirely missing, we can consider dropping them altogether as the inconsistensies in between the rows of processes will not help in predicting the target variable.<br>\nSome of the columns are entirely filled with the value of 0 which we should eliminate as well.<br>\nSome features even exhibit negative values and many may have outliers on the higher side as we can see from the gap between max and 75%","c723be1b":"As we can see of the 560 remaining features 116 of them only have 1 unique value which indicates a  faulty signal across all batches and therefore we will frop these also.","969cdac7":"On deeper analysis we can observe:<br>\nThis is a 24\/7 around the clock production facility working on all days of the year.<br>\nNo significant pattern can be seen as such on a daily basis, except end of the weekday has a slightly higher value than most days.<br>\nUsually the first 10 days of every month is the busiest.<br>\nThe production facility seems to be the busiest during the months of August and September, the abnormally high volumes can be a market response of the product.<br>\n\nThese explorations help us understand the profile of the facility but do not provide significant insight into why a semiconductor failed in the production cyle.The distribution of success to failure follows the same distribution whether we analyse it hourly\/daily\/monthly or over the period of the entire year of 2008.Therefore we will drop these features.","f0bf5f1b":"We can assume there are 590 independent processes that together add up to create a semiconductor and at the end of the process they are tagged as success or failure in terms of creating 1 new functional semiconductor unit.<br>\nThere are quite a few NaN entries and entries which are equal to the value 0.<br>\nThe 0s are not sparse you can see quite a few of them in continuation which may indicate an error or lack of signal i\/o.<br>\nWe must either impute or drop them because these duplicate values will not help in determining\/predicting target.<br>\nLets find out which are the most important features in terms of predicting our target,and also the ones we can eliminate which have most of these error or missing values.<br>\nAlso the datetime feature doesnt any significance here so we should be able to drop it without any problem.","4195ac39":"Data recieved is from all the months of the 2008 calender year cycle.Although it is not recorder everyday of the month we can see that production happens throughout the week even on sundays.","045b2516":"Naive Bayes shows the best model for Smote Data.It is underfitting Perhaps the accuracy can be optimised.\n\nCreating another optimised copy with **RandomUnderSampler** to undersample majority class.","3e3d3c2a":"Most of the columns show a normal distribution which looks good\nFew columns show a bimodal distibution:<br>\n4,12,18,21,22,24,25,26,47,58,80,87,89,119,122,125,169,183,221,254,267,419,433,482,488,560\n\nAnd the following show multimodal distribution:<br>\n51,94.95,130","7de4b4c1":"As we can see most of the outliers have been resolved,those indicated as outliers now are the new ones that have emerged due to manipulation.Further to this we can now move on to applying PCA but first we should scale the data.","06e1ebbd":"KNeighbours is performing best on the undersampled data followed by svm and LogisticRegression which are overfitting so can be tuned.\n\nCreating one optimised copy with **SMOTETomek** to combine oversampling and undersampling undersample techniques.","16f491cc":"The feature \"Time\" may or may not be useful in predicting the target, we can visually explore it to eee if it provides any meanigful data.","494838fb":"Create an optimised copy of data by oversampling the minority class using **SMOTE**.","298787d4":"As the number and the graph suggest we have dealt with most of the missing data the rest 2700 odd values can be easily imputed.","1324f608":"Across all the charts above ,it is clear that even while comparing features which have higher proportion of Failures than Passing standard, they still follow similar distribution as other features.Even comparing features with bimodal or mmulti modal distribution isnt revelaing any significant pattern to differentiate between labels. Since there isnt much we can conclude from visual EDA we must transform the data.\n\n# PCA","6326c29b":"# Splitting model for training","9a709986":"# UCI SECOM Dataset\n### Semiconductor manufacturing process dataset\n\n**DOMAIN**: Semiconductor manufacturing process\n\n**CONTEXT:**\nA complex modern semiconductor manufacturing process is normally under constant surveillance via the monitoring of signals\/\nvariables collected from sensors and or process measurement points. However, not all of these signals are equally valuable in a specificmonitoring system. The measured signals contain a combination of useful information, irrelevant information as well as noise. Engineerstypically have a much larger number of signals than are actually required. If we consider each type of signal as a feature, then featureselection may be applied to identify the most relevant signals. The Process Engineers may then use these signals to determine key factor contributing to yield excursions downstream in the process. This will enable an increase in process throughput, decreased time to learningand reduce the per unit production costs. These signals can be used as features to predict the yield type. And by analysing and trying out different combinations of features, essential signals that are impacting the yield type can be identified.\n\n**DATA DESCRIPTION:** sensor-data.csv : (1567, 592)\nThe data consists of 1567 examples each with 591 features.\nThe dataset presented in this case represents a selection of such features where each example represents a single production entity with associated measured features and the labels represent a simple pass\/fail yield for in house line testing. Target column \u201c \u20131\u201d corresponds to a pass and \u201c1\u201d corresponds to a fail and the data time stamp is for that specific test point.\n\n**PROJECT OBJECTIVE:**\nWe will build a classifier to predict the Pass\/Fail yield of a particular process entity and analyse whether all the\nfeatures are required to build the model or not."}}