{"cell_type":{"0e74689e":"code","a601aa64":"code","2ed3b9b8":"code","13fadba6":"code","2a45ce14":"code","6422701c":"code","7588dfe4":"code","2efe7879":"code","c8c4cfee":"code","a9709562":"code","b4501095":"code","b0289cc7":"code","198b1be5":"code","030b4e8e":"code","d91b8256":"code","0dd4fba5":"code","64cf0813":"code","8e7e2de5":"code","0a9bb38f":"code","cc85f5f8":"code","59ac47b4":"code","410b1287":"code","c3eb9849":"code","9da6ee7a":"code","3a01fc7a":"code","609a34f6":"code","c4027a10":"code","a96fa1d9":"code","e8f1f13f":"code","e5b42635":"code","72c03d0c":"code","82b6ee03":"code","fb61224e":"code","e46334f7":"code","16e567b4":"code","88bacf7b":"code","112fec29":"code","bf879fd5":"code","f4b462e4":"code","222d4998":"code","4685e7e1":"code","e5b628f4":"code","bbe1eeef":"code","7349e7ac":"code","2e99ef7a":"code","af16a4cc":"code","a57a70e8":"code","d8664844":"code","70ae6fc0":"code","d44fae8c":"code","69450af8":"code","6e2e78c9":"code","1a68eca2":"code","644b4147":"code","f5d6149e":"code","b8586d2e":"code","2082707e":"code","2de4b3c4":"code","b65bf1b9":"code","0dcdac89":"code","0495f31d":"code","cb115920":"code","d94b1e5b":"code","3cb7ae10":"code","4c300c7c":"code","ffadd9f8":"code","0717accf":"code","ac3ef9ce":"code","efffceb9":"markdown","613ae0b8":"markdown","f3d96aa6":"markdown","9960b84d":"markdown","ee709fe2":"markdown","07ffda21":"markdown","1bbd79da":"markdown","dfceb725":"markdown","ceb6a358":"markdown","c708e83a":"markdown","3b0a5538":"markdown","40657854":"markdown"},"source":{"0e74689e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# TextBlob - Python library for processing textual data\nfrom textblob import TextBlob\n\n#GeoText to get country alpha-2 codes and identify cities and countries in text\n!pip install GeoText\nfrom geotext import GeoText\n\n#libraries to extract country name from cities and textual data\n!pip install geopandas\n!pip install geopy\nimport geopandas\nimport geopy\nfrom geopy.extra.rate_limiter import RateLimiter\nfrom geopy.geocoders import Nominatim\n\n#time library to space-out requests\nimport time\n\n#pycountry library to get alpha-2 country and continent codes\n!pip install pycountry-convert\nfrom pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n\n#Using folium maps to create visualization\n!pip install folium\nimport folium\nfrom folium.plugins import MarkerCluster\n\nfrom tqdm.notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\nimport matplotlib.pyplot as plt","a601aa64":"data = pd.read_csv('..\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')\ndata.head()","2ed3b9b8":"data2 = data.copy()","13fadba6":"data.shape","2a45ce14":"print('no. of unique users:', len(data['user_name'].unique()))","6422701c":"username_counts = data['user_name'].value_counts()\npd.DataFrame(username_counts[username_counts>1]).reset_index().rename(columns = {'index': 'username', 'user_name': 'counts'})","7588dfe4":"usernamecounts_dict = dict(username_counts[username_counts>1])\nusernamecounts_dict","2efe7879":"data['user_location'].value_counts()","c8c4cfee":"data_loc = data[['id', 'user_name', 'user_location']]\ndata_loc.head()","a9709562":"data_loc.shape","b4501095":"loc_obj_types = list()\nloc_obj_types = [type(x) for x in data_loc['user_location'] if (type(x) != str)]\nprint('no. of data types other than string', len(set(loc_obj_types)))\nprint('no. of objects that are not string', len(loc_obj_types))\nprint('non-string \"user_locations\" are ', (len(loc_obj_types)\/data_loc.shape[0])*100, '%')","b0289cc7":"data_loc.dropna(subset=['user_location'], inplace = True)\ndata_loc.shape","198b1be5":"data_loc['user_location'] = data_loc['user_location'].progress_apply(lambda x: x.title() if (type(x) == str) else 'Unknown')\ndata_loc['country_location'] = data_loc['user_location'].progress_apply(lambda x: GeoText(str(x)).countries[0] if len(GeoText(str(x)).countries) != 0 else 'Unknown')\ndata_loc['city_location'] = data_loc['user_location'].progress_apply(lambda x: GeoText(str(x)).cities[0] if len(GeoText(str(x)).cities) != 0 else 'Unknown')\ndata_loc.head()","030b4e8e":"data_loc = data_loc[~((data_loc['country_location'] == 'Unknown') & (data_loc['city_location'] == 'Unknown'))]","d91b8256":"data_loc.shape","0dd4fba5":"locator = Nominatim(user_agent='myGeocoder')\ngeocode = RateLimiter(locator.geocode, min_delay_seconds=1)","64cf0813":"data_loc['city_location'].value_counts()","8e7e2de5":"data_loc['country_location'].value_counts()","0a9bb38f":"data_loc.head()","cc85f5f8":"data_loc['country_location2'] = data_loc.progress_apply(lambda x: geocode(x['city_location']).address.split(', ')[-1:][0] if((x['country_location'] == 'Unknown') and (x['city_location'] != 'Unknown')) else x['country_location'], axis = 1)\ndata_loc.head()","59ac47b4":"data_loc.drop(['country_location'], axis = 1, inplace = True)\ndata_loc.rename(columns = {'country_location2': 'country_location'}, inplace = True)\ndata_loc.head()","410b1287":"data_loc['country_location'].value_counts()","c3eb9849":"def translate_text(text):\n    if ('\/' in text):\n        text = text.split('\/')[1].strip()\n    Text = TextBlob(u'\"'+text+'\"')\n    time.sleep(1)\n    if(Text.detect_language() != 'en'):\n        print(text)\n        if(Text.detect_language() == 'el'):\n            return 'Greece'        \n        time.sleep(1)\n        try:    \n            return str(Text.translate(to='en')).strip('\"')\n        except:\n            return text\n    else:\n        time.sleep(1)\n        return text","9da6ee7a":"locs = list(data_loc['country_location'])\nu_locs = list(set(locs))\nprint(len(locs))\nprint(len(u_locs))","3a01fc7a":"loc_trans = {text: translate_text(text) for text in u_locs}\nprint(loc_trans)","609a34f6":"data_loc['country_location2'] = data_loc['country_location'].apply(lambda x: loc_trans[x])\ndata_loc.head()","c4027a10":"data_loc['country_location2'].value_counts()","a96fa1d9":"def get_continent_code(text):\n    try:\n        return country_alpha2_to_continent_code(text)\n    except:\n        return ' Un'","e8f1f13f":"def get_country_code(text):\n    try:\n        return country_name_to_country_alpha2(text)\n    except:\n        return 'Un'","e5b42635":"data_loc['country_code'] = data_loc['country_location2'].progress_apply(lambda x: get_country_code(x))\ndata_loc.head()","72c03d0c":"data_loc[data_loc['country_code'] == 'Un']['country_location2'].value_counts()","82b6ee03":"difcount_codes = {'The United Arab Emirates' : 'AE', 'The Netherlands': 'NL', 'Asia': 'IN', 'Saudi': 'SA', 'Chili': 'CL', 'Luzon': 'PH', 'Sri Lanka Sri Lanka': 'LK', 'Free Kashmir': 'IN', 'Swiss': 'CH'}","fb61224e":"list(difcount_codes.keys())","e46334f7":"data_loc['country_code'] = data_loc.progress_apply(lambda x: difcount_codes[x['country_location2']] if (x['country_code'] == 'Un') else x['country_code'], axis = 1)\ndata_loc.head()","16e567b4":"data_loc['country_code'].value_counts()","88bacf7b":"data_loc['continent_code'] = data_loc['country_code'].progress_apply(lambda x: get_continent_code(x))\ndata_loc.head()","112fec29":"u_countrycodes = list(set(list(data_loc['country_code'])))\nprint(len(u_countrycodes))\nprint(u_countrycodes)","bf879fd5":"geolocator = Nominatim(user_agent='myGeocoder')\ndef geolocate(country):\n    try:\n        # Geolocate the center of the country\n        loc = geolocator.geocode(country)\n        # And return latitude and longitude\n        return (loc.latitude, loc.longitude)\n    except:\n        # Return missing value\n        return (np.nan, np.nan)","f4b462e4":"country_coords = {cc: geolocate(cc) for cc in u_countrycodes}\nprint(country_coords)","222d4998":"for i,v in country_coords.items():\n    if(v == (np.nan, np.nan)):\n        print(i)\n        if( i == 'IN'):\n            country_coords[i] = geolocate('India')\n        elif( i == 'IL'):\n            country_coords[i] = geolocate('Israel')\n        elif(i == 'ET'):\n            country_coords[i] = geolocate('Ethiopia')\nprint(country_coords)","4685e7e1":"country_data = pd.read_csv('https:\/\/gist.githubusercontent.com\/cpl\/3dc2d19137588d9ae202d67233715478\/raw\/3d801e76e1ec3e6bf93dd7a87b7f2ce8afb0d5de\/countries_codes_and_coordinates.csv')\ncountry_data.head()","e5b628f4":"country_data['Country'] = country_data['Country'].progress_apply(lambda x: str(x))\ncountry_data['Alpha-2 code'] = country_data['Alpha-2 code'].progress_apply(lambda x: str(x.replace('\"', \"\").strip(' ')))\ncountry_data['Alpha-3 code'] = country_data['Alpha-3 code'].progress_apply(lambda x: str(x.replace('\"', \"\").strip(' ')))\ncountry_data['Numeric code'] = country_data['Numeric code'].progress_apply(lambda x: int(x.replace('\"', \"\").strip(' ')))\ncountry_data['Latitude (average)'] = country_data['Latitude (average)'].progress_apply(lambda x: float(x.replace('\"', \"\").strip(' ')))\ncountry_data['Longitude (average)'] = country_data['Longitude (average)'].progress_apply(lambda x: float(x.replace('\"', \"\").strip(' ')))","bbe1eeef":"country_data['Country'] = country_data['Country'].astype(str)\n\ncountry_data['country_code'] = country_data['Alpha-2 code'].astype(str)\ncountry_data.drop(['Alpha-2 code'], axis = 1, inplace = True)\ncountry_data['Alpha-3 code'] = country_data['Alpha-3 code'].astype(str)\ncountry_data['Numeric code'] = country_data['Numeric code'].astype(int)\ncountry_data['Latitude (average)'] = country_data['Latitude (average)'].astype(float)\ncountry_data['Longitude (average)'] = country_data['Longitude (average)'].astype(float)\ncountry_data.head()","7349e7ac":"n_tweets = data_loc.shape[0]\nprint('tweets with locations: ', n_tweets)","2e99ef7a":"wrld_map = data_loc.groupby(['country_code']).size().to_frame(name = 'count').reset_index()\nwrld_map.head()","af16a4cc":"wrld_map['percentage'] = wrld_map['count'].progress_apply(lambda x: (x\/n_tweets))\nwrld_map.drop(['count'], axis = 1, inplace = True)\nwrld_map.head()","a57a70e8":"wrld_map = pd.merge(wrld_map, country_data, on='country_code')\nwrld_map.head()","d8664844":"wrld_map['continent_code'] = wrld_map['country_code'].progress_apply(lambda x: get_continent_code(x))\nwrld_map.head()","70ae6fc0":"#empty map\nworld_map= folium.Map(tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(world_map)","d44fae8c":"#for each coordinate, create circlemarker of user percent\nfor i in range(len(wrld_map)):\n        lat = wrld_map.iloc[i]['Latitude (average)']\n        long = wrld_map.iloc[i]['Longitude (average)']\n        radius=5\n        popup_text = \"\"\"Country : {}<br>\n                    %of Users : {}<br>\"\"\"\n        popup_text = popup_text.format(wrld_map.iloc[i]['country_code'],\n                                   wrld_map.iloc[i]['percentage']\n                                   )\n        folium.CircleMarker(location = [lat, long], radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\n#show the map\nworld_map","69450af8":"data['user_name'].isna().sum()","6e2e78c9":"data['user_location'].isna().sum()","1a68eca2":"data['user_created'].isna().sum()","644b4147":"data['user_followers'].isna().sum()","f5d6149e":"import plotly.express as px\nfig = px.histogram(data, x=\"user_followers\", range_x  = (0,500000))\nfig.show()","b8586d2e":"# Cleaning the tweets\n\ndef cleanUpTweet(txt):\n    # Remove mentions\n    txt = re.sub(r'@[A-Za-z0-9_]+', '', txt)\n    # Remove hashtags\n    txt = re.sub(r'#', '', txt)\n    # Remove retweets:\n    txt = re.sub(r'RT : ', '', txt)\n    # Remove urls\n    txt = re.sub(r'https?:\\\/\\\/[A-Za-z0-9\\.\\\/]+', '', txt)\n    return txt","2082707e":"data['text'] = data['text'].apply(cleanUpTweet)","2de4b3c4":"def getTextSubjectivity(txt):\n    return TextBlob(txt).sentiment.subjectivity\n\ndef getTextPolarity(txt):\n    return TextBlob(txt).sentiment.polarity","b65bf1b9":"data['Subjectivity'] = data['text'].apply(getTextSubjectivity)\ndata['Polarity'] = data['text'].apply(getTextPolarity)","0dcdac89":"data.head()","0495f31d":"# negative, nautral, positive analysis\ndef getTextAnalysis(a):\n    if a < 0:\n        return \"Negative\"\n    elif a == 0:\n        return \"Neutral\"\n    else:\n        return \"Positive\"","cb115920":"data['Sentiment'] = data['Polarity'].apply(getTextAnalysis)","d94b1e5b":"data.head()","3cb7ae10":"positive_tweets = data[data['Sentiment'] == 'Positive']\n\nprint(str(positive_tweets.shape[0]\/(data.shape[0])*100) + \" % of positive tweets\")","4c300c7c":"labels = data.groupby('Sentiment').count().index.values\n\nvalues = data.groupby('Sentiment').size().values\n\nplt.bar(labels, values)","ffadd9f8":"for index, row in data.iterrows():\n    if row['Sentiment'] == 'Positive':\n        plt.scatter(row['Polarity'], row['Subjectivity'], color=\"green\")\n    elif row['Sentiment'] == 'Negative':\n        plt.scatter(row['Polarity'], row['Subjectivity'], color=\"red\")\n    elif row['Sentiment'] == 'Neutral':\n        plt.scatter(row['Polarity'], row['Subjectivity'], color=\"blue\")\n\nplt.title('Vaccine Sentiment Analysis')\nplt.xlabel('Polarity')\nplt.ylabel('Subjectivity')\n# add legend\nplt.show()","0717accf":"data['Sentiment'].value_counts()","ac3ef9ce":"def tweet_influence(row):\n    #print(row['text'])\n    followers = row['user_followers']\n    retweets = row['retweets']\n    is_retweet = int(row['is_retweet'])\n    #print(is_retweet)\n    friends = row['user_friends']\n    #print('tweet influence: ', ((followers + retweets)\/pow(2, is_retweet)) + friends)\n    tweet_influence = ((followers + retweets)\/pow(2, is_retweet)) + friends\n    return tweet_influence","efffceb9":"***There are no records with empty user names.***","613ae0b8":"# *User Followers seem fine.*","f3d96aa6":"# **Dropping user_locations feature as the number of missing columns are more than 34% of data**","9960b84d":"# *There are 1250 missing user locations*","ee709fe2":"*There are users who have tweeted more than once.*","07ffda21":"*Determining Subjectivity and Polarity of text using TextBlob*","1bbd79da":"*Using RateLimiter to gap-out requests to geocode servers.*","dfceb725":"# *We will drop user_descriptions as well, as they don't seem to contribute much.*","ceb6a358":"*Geopy's Geocode function could not return the correct coordinates for regions based on their country codes. Regions such as Cayman Islands ('KY'), Albania ('AL') etc were mis-identified as U.S. states.*\n\n*I recently came across a gist of country data much like what we are after over here. I'm using the coordinates specified through this publicly available csv file.*","c708e83a":"# *All the user creation dates seem to be present.*","3b0a5538":"**Calculating Influence of Tweets by User Profile Size**","40657854":"*Initial Analysis using TextBlob shows more Positive tweets than Negative or Neutral*"}}