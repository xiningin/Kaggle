{"cell_type":{"6d9091ba":"code","1726551d":"code","420df371":"code","7d2c6012":"code","221af4fe":"code","2921400d":"code","980e908a":"code","d73b768e":"code","cd34c41c":"code","c7d2ae92":"code","25df7cb3":"code","4e2ad8ee":"code","17cc0da3":"code","e903c345":"code","fb857f60":"markdown","58486fcc":"markdown","f9b4ae5a":"markdown","afca09a8":"markdown","1b23aa21":"markdown","b6e66a87":"markdown","deed6936":"markdown","dbc9b3ca":"markdown","6ecef184":"markdown","a181964c":"markdown","65f35d5f":"markdown","06e3fa53":"markdown","0d313f0d":"markdown","6abab1a3":"markdown","fd0ed6a4":"markdown"},"source":{"6d9091ba":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# importing evaluation and data split packages\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# importing modelling packages\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB","1726551d":"# taking only 10000 rows as sample\n\ntrain = pd.read_csv(r'..\/input\/tabular-playground-series-nov-2021\/train.csv',nrows=10000)","420df371":"X = train.drop(['id','target'],axis=1)\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=2,stratify=y)\n\nfeatures = X_train.columns","7d2c6012":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, QuantileTransformer, StandardScaler\n\npipe_1 = Pipeline([('scaler', StandardScaler()), ('nb', GaussianNB())])\npipe_2 = Pipeline([('scaler', MinMaxScaler()), ('nb', GaussianNB())])\npipe_3 = Pipeline([('scaler', RobustScaler()), ('nb', GaussianNB())])\npipe_4 = Pipeline([('scaler', QuantileTransformer()), ('nb', GaussianNB())])","221af4fe":"print('No. of rows with all zeroes: ',((X_train == 0).sum(axis=1)==100).sum())","2921400d":"# fit a probability distribution to a univariate data sample\n\nimport scipy.stats as stats\ndef fit_distribution(data):\n    # estimate parameters\n    mu = np.mean(data)\n    sigma = np.std(data)\n    #fit distribution\n    dist = stats.norm.pdf(mu, sigma)\n    return dist","980e908a":"# sort data into classes\n\nXy0 = train[train['target'] == 0]\nXy1 = train[train['target'] == 1]\nXy0.drop(['id','target'],axis=1,inplace=True)\nXy1.drop(['id','target'],axis=1,inplace=True)\nprint(Xy0.shape, Xy1.shape)\n\n# calculate priors\n\npriory0 = len(Xy0) \/ len(X)\npriory1 = len(Xy1) \/ len(X)\nprint(priory0, priory1)","d73b768e":"# create PDFs\n\nXy0_pdf = Xy0.apply(lambda x:fit_distribution(x))\nXy1_pdf = Xy1.apply(lambda x:fit_distribution(x))","cd34c41c":"print('Default Parameters: ',GaussianNB().get_params())\nmodel = GaussianNB()\nmodel.fit(X_train,y_train)\nprint('ROC score with No Scaler: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))\npipe_1.fit(X_train,y_train)\nprint('ROC score with StandardScaler: ',roc_auc_score(y_test,pipe_1.predict_proba(X_test)[:,1]))\npipe_2.fit(X_train,y_train)\nprint('ROC score with MinMaxScaler: ',roc_auc_score(y_test,pipe_2.predict_proba(X_test)[:,1]))\npipe_3.fit(X_train,y_train)\nprint('ROC score with RobustScaler: ',roc_auc_score(y_test,pipe_3.predict_proba(X_test)[:,1]))\npipe_4.fit(X_train,y_train)\nprint('ROC score with QuantileTransformer: ',roc_auc_score(y_test,pipe_4.predict_proba(X_test)[:,1]))","c7d2ae92":"var_smooth = [0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001,0.00000001,0.000000001]\n\nfor var in var_smooth:\n    pipe = Pipeline([('scaler', QuantileTransformer()), ('nb', GaussianNB(var_smoothing=var))])\n    pipe.fit(X_train,y_train)\n    print('Var Smoothing: ',var,\" \",'AUC: ',roc_auc_score(y_test,pipe.predict_proba(X_test)[:,1]))","25df7cb3":"from sklearn.inspection import permutation_importance\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\nimps = permutation_importance(model, X_test, y_test)\nimportances = imps.importances_mean\nstd = imps.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\n#print(\"Feature ranking:\")\n#for f in range(X_test.shape[1]):\n #   print(\"%d. %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))","4e2ad8ee":"# fitting our pipeline with a subset of the data with these variables\n# first 39 variables have positive importance, hence ignoring them\n\nimportant_features = features[indices[39:]]\nprint('Important Features: ',important_features)\n\nX_train_new = X_train[important_features]\nX_test_new = X_test[important_features]\n\npipe_4.fit(X_train_new,y_train)\nprint('ROC score with QuantileTransformer and Selected Features: ',\n      roc_auc_score(y_test,pipe_4.predict_proba(X_test_new)[:,1]))","17cc0da3":"bins = [2,3,4,5,6,7,8,9,10]\nprint('GAUSSIAN WITH DISCRETE, CUT')\nmodel = GaussianNB()\nfor bin in bins:\n    X_train_binned = X_train.apply(lambda x:pd.cut(x,bins=bin,labels=False))\n    X_test_binned = X_test.apply(lambda x:pd.cut(x,bins=bin,labels=False))\n    model.fit(X_train_binned,y_train)\n    print('No. of Bins: ',bin,\" \",'AUC: ',\n          roc_auc_score(y_test,pipe.predict_proba(X_test_binned)[:,1]))\nprint('GAUSSIAN WITH DISCRETE, QCUT')\nmodel = GaussianNB()\nquantiles = [2,3,4,5,6,7,8,9,10]\nfor quantile in quantiles:\n    X_train_binned = X_train.apply(lambda x:pd.qcut(x,q=quantile,labels=False,precision=0))\n    X_test_binned = X_test.apply(lambda x:pd.qcut(x,q=quantile,labels=False,precision=0))\n    model.fit(X_train_binned,y_train)\n    print('No. of Quantiles: ',quantile,\" \",'AUC: ',\n          roc_auc_score(y_test,pipe.predict_proba(X_test_binned)[:,1]))","e903c345":"from scipy import stats\ndef percentiler(col):\n    ranked = stats.rankdata(col)\n    data_percentile = ranked\/len(col)*100\n    bins_percentile = np.linspace(0,100,6)\n    data_binned_indices = np.digitize(data_percentile, bins_percentile, right=True)\n    return data_binned_indices\n\nprint('Percentiled Bins as per columns')\nmodel = GaussianNB()\nX_train_binned = X_train.apply(lambda x:percentiler(x))\nX_test_binned = X_test.apply(lambda x:percentiler(x))\nmodel.fit(X_train_binned,y_train)\nprint(roc_auc_score(y_test,pipe.predict_proba(X_test_binned)[:,1]))","fb857f60":"# Splitting Data","58486fcc":"# Converting to Discrete\n\n6 bins, without any scaling, give good results, even better than simple quantile transformer scaling on continuous variables.","f9b4ae5a":"# Importing Packages and Sample Data","afca09a8":"# Best Variable Transformation\nQuantile Transformer works best because it helps variables assume normal distribution - useful for NB","1b23aa21":"It is a classification technique based on Bayes\u2019 Theorem with an **assumption of independence among predictors**. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Since our dataset has weakly correlated variables, NB might work well here.\n\n![](http:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2015\/09\/Bayes_rule-300x172-300x172.png)\n\nIt perform wells in case of categorical input variables compared to numerical variable(s). **For numerical variable, normal distribution is assumed** (bell curve, which is a strong assumption), hence we will do variable transformation first.\n\nFollowing are the types of Naive Bayes algorithms-\n1. **Gaussian** - Assumes that features follow a normal distribution.\n2. **Multinomial** - Used for discrete counts\n3. **Bernoulli** - Used when features are binary in nature (0s and 1s)\n\nWe will **use Gaussian NB** after transforming our features.","b6e66a87":"Methods of Improvement -\n\n![](http:\/\/www.baeldung.com\/wp-content\/ql-cache\/quicklatex.com-8ebe947b0538431322197ecd5324bade_l3.svg)","deed6936":"# Making Pipelines with Transformers","dbc9b3ca":"In this month's TPS, I am checking out some new algorithms I came across. I am choosing Naive Bayes right now. The purpose of this exercise is to become better at extracting maximum power from it and see if non-NN models can be used too.\n\nI would make changes to the important parameters and mention their impact. **Please note that these parameter observations are made independent of each other and only for the current data we have**. For speed, I am choosing a simple test split of 30% size on 10000 samples. I have shared references towards the end.\n\nI did a similar experiment last month with Random Forest - https:\/\/www.kaggle.com\/raahulsaxena\/tps-oct-21-understand-random-forest-parameters\n\n**Feel free to run your own experiments and upvote if you find this code useful :)**","6ecef184":"# Permutation Importance\n\nThe permutation feature importance is defined to be the **decrease in a model score when a single feature value is randomly shuffled**. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. ","a181964c":"# About Naive Bayes","65f35d5f":"<div style=\"background-color:rgba(110, 129, 21, 0.5);\">\n    <h1><center>Gaussian Naive Bayes<\/center><\/h1>\n<\/div>","06e3fa53":"<div style=\"background-color:rgba(110, 129, 21, 0.5);\">\n    <h1><center>Understand the Models You Love<\/center><\/h1>\n<\/div>","0d313f0d":"# Analysing the X_train data (Experimenting)","6abab1a3":"# References\n\nLinks - \n1. https:\/\/www.geeksforgeeks.org\/naive-bayes-classifiers\/\n2. https:\/\/towardsdatascience.com\/introduction-to-na%C3%AFve-bayes-classifier-fa59e3e24aaf\n3. https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/naive-bayes-explained\/\n4. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html\n5. https:\/\/machinelearningmastery.com\/better-naive-bayes\/\n6. https:\/\/inblog.in\/Feature-Importance-in-Naive-Bayes-Classifiers-5qob5d5sFW\n\nNotebooks -\n1. https:\/\/www.kaggle.com\/rayhanlahdji\/tps-1121-naive-bayes-for-naive-souls by Rayhan Lahdji\n2. https:\/\/www.kaggle.com\/markosthabit\/tbs-november-naive-bayes by Markos Thabit\n3. https:\/\/www.kaggle.com\/prashant111\/naive-bayes-classifier-in-python by Prashant Banerjee\n\nVideos -\n1. https:\/\/www.youtube.com\/watch?v=H3EjCKtlVog on Gaussian Naive Bayes\n2. https:\/\/www.youtube.com\/watch?v=O2L2Uv9pdDA on understanding NB","fd0ed6a4":"# Var Smoothing\nPortion of the largest variance of all features that is added to variances for calculation stability.\nIn statistics, Laplace Smoothing is a technique to smooth categorical data. Laplace Smoothing is introduced to solve the problem of zero probability."}}