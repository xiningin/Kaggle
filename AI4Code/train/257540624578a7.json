{"cell_type":{"c154c091":"code","7226235a":"code","a330126f":"code","b72418f1":"code","98c1580a":"code","d8a19ca6":"code","73d52080":"code","e1d6327f":"code","73576975":"code","ed57fea7":"code","67cabc33":"code","9b5029e2":"code","256e77d1":"code","9118d124":"code","b0058ab0":"code","5893a163":"code","649f0438":"code","56760fab":"code","510920af":"code","a8188139":"code","d4df7350":"code","9c47d4bd":"code","2aacbbdd":"markdown"},"source":{"c154c091":"# Load libraries\nimport numpy\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","7226235a":"# Load dataset\nurl = \"..\/input\/sonar.all-data.csv\"\ndataset = read_csv(url, header=None)","a330126f":"# Shape\nprint(dataset.shape)","b72418f1":"# Types \nset_option('display.max_rows', 500)\nprint(dataset.dtypes)","98c1580a":"# Head\nset_option('display.width', 100)\nprint(dataset.head(20))","d8a19ca6":"# Descriptions, change precision to 3 places\nset_option('precision', 3)\nprint(dataset.describe())","73d52080":"# Class distribution\nprint(dataset.groupby(60).size())","e1d6327f":"# Histograms\ndataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1)\npyplot.show()","73576975":"# Density\ndataset.plot(kind='density', subplots=True, layout=(8,8), sharex=False, sharey=False, legend=False,\n            fontsize=1)\npyplot.show()","ed57fea7":"# Correlation matrix\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none')\nfig.colorbar(cax)\npyplot.show()","67cabc33":"# Split-out validation dataset\narray = dataset.values\nX = array[:,0:60].astype(float)\nY = array[:,60]\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\ntest_size=validation_size, random_state=seed)","9b5029e2":"# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'accuracy'","256e77d1":"# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))","9118d124":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","b0058ab0":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","5893a163":"# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',\nLogisticRegression())])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA',\nLinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\nKNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\nDecisionTreeClassifier())])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',\nGaussianNB())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n kfold = KFold(n_splits=num_folds, random_state=seed)\n cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n results.append(cv_results)\n names.append(name)\n msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n print(msg)","649f0438":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","56760fab":"# Tune scaled KNN\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nneighbors = [1,3,5,7,9,11,13,15,17,19,21]\nparam_grid = dict(n_neighbors=neighbors)\nmodel = KNeighborsClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","510920af":"# Tune scaled SVM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","a8188139":"# ensembles\nensembles = []\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\nensembles.append(('RF', RandomForestClassifier()))\nensembles.append(('ET', ExtraTreesClassifier()))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","d4df7350":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","9c47d4bd":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = SVC(C=1.5)\nmodel.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","2aacbbdd":"We can see that we achieve an accuracy of nearly 86% on the held-out validation dataset. A\nscore that matches closely to our expectations estimated above during the tuning of SVM."}}