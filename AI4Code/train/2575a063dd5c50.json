{"cell_type":{"f246a2b5":"code","46b4c161":"code","957b375b":"code","5133dd57":"code","ebaa703c":"code","84df896d":"code","7217586d":"code","2685d1e9":"code","79388cb4":"code","f509c4ff":"code","c56414a1":"code","6d78c35c":"code","6fe856f8":"code","e0bc6509":"code","21e0c368":"code","644e6646":"code","e659a592":"code","7d9c435b":"code","f4d6569d":"markdown","b529e760":"markdown","4bf08b5b":"markdown","d53bb51a":"markdown","e542b716":"markdown","e42fe542":"markdown","e3730d7e":"markdown","cd6d2aa0":"markdown","69b4801d":"markdown","16076de4":"markdown","f20f9984":"markdown","21b7efbe":"markdown","922389d7":"markdown","9dfcb6d6":"markdown","9afccb4c":"markdown","92a0ea55":"markdown"},"source":{"f246a2b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport warnings; warnings.simplefilter('ignore')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.width',1000000)\npd.set_option('display.max_columns', 500)\n\nscore_df = pd.DataFrame(columns={'Model Description','Score'})\n# Any results you write to the current directory are saved as output.","46b4c161":"df_train= pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","957b375b":" print(df_train.head(5))","5133dd57":"print(df_train.info())","ebaa703c":"print(df_train.isnull().any())","84df896d":"print(df_test.isnull().any())","7217586d":"print(df_train.shape)","2685d1e9":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\nplt.tight_layout()\n\nlabels=['Disaster Tweet','No Disaster']\nsize=  [df_train['target'].mean()*100,abs(1-df_train['target'].mean())*100]\nexplode = (0, 0.1)\n#ig1,ax1 = plt.subplots()\naxes[0].pie(size,labels=labels,explode=explode,shadow=True,\n            startangle=90,autopct='%1.1f%%')\nsns.countplot(x=df_train['target'], hue=df_train['target'], ax=axes[1])\nplt.show()","79388cb4":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nX_train,X_test,y_train,y_test = train_test_split(df_train['text'],df_train['target'])\nvector = TfidfVectorizer().fit(X_train)\n\n#print(vector.get_feature_names())\nX_train_vector = vector.transform(X_train)\nX_test_vector = vector.transform(X_test)\n\nmodel = LogisticRegression().fit(X_train_vector,y_train)\nprint('Logistic Regression ROC Auc Score with TFIDF - %3f'%(roc_auc_score(y_test,model.predict(X_test_vector))))\nprint('F1Score - %3f'%(f1_score(y_test,model.predict(X_test_vector))))\nscore_df = score_df.append({'Model Description':'Basic LR Model - Basline - TFIDF',\n                           'Score':roc_auc_score(y_test,model.predict(X_test_vector))}\n                           ,ignore_index=True)\n\n####### Now let's try with count vectorizer\n\ncv_vector = CountVectorizer().fit(X_train)\nX_train_vector = cv_vector.transform(X_train)\nX_test_vector = cv_vector.transform(X_test)\n\nmodel = LogisticRegression().fit(X_train_vector,y_train)\npredict = model.predict(X_test_vector)\nscore = roc_auc_score(y_test,predict)\nprint('Logistic Regression Roc AUC Score with countvectorizer - %3f'%score)\n\nscore_df = score_df.append({'Model Description':'Basic LR Model - Basline - CV',\n                          'Score':score}\n                          ,ignore_index=True)","f509c4ff":"\ndef clean_text(text):\n    import re\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"i'll\", \"i will\", text)\n    text = re.sub(r\"she'll\", \"she will\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n    return text\n\n\ndf_train['clean_text'] = df_train['text'].apply(clean_text)\ndf_test['clean_text'] = df_test['text'].apply(clean_text)","c56414a1":"def massage_text(text):\n    import re\n    from nltk.corpus import stopwords\n    ## remove anything other then characters and put everything in lowercase\n    tweet = re.sub(\"[^a-zA-Z]\", ' ', text)\n    tweet = tweet.lower()\n    tweet = tweet.split()\n\n    from nltk.stem import WordNetLemmatizer\n    lem = WordNetLemmatizer()\n    tweet = [lem.lemmatize(word) for word in tweet\n             if word not in set(stopwords.words('english'))]\n    tweet = ' '.join(tweet)\n    return tweet\n    print('--here goes nothing')\n    print(text)\n    print(tweet)\n\ndf_train['clean_text'] = df_train['text'].apply(massage_text)\ndf_test['clean_text'] = df_test['text'].apply(massage_text)\n","6d78c35c":"df_train.iloc[0:10][['text','clean_text']]","6fe856f8":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nvector = TfidfVectorizer().fit(df_train['clean_text'])\ndf_train_vector = vector.transform(df_train['clean_text'])\ndf_test_vector = vector.transform(df_test['clean_text'])\nlr_model = LogisticRegression()\ngrid_values =  {'penalty':['l1', 'l2'],'C':[0.01, 0.1, 1, 10, 100]}\ngrid_search_model = GridSearchCV(lr_model,param_grid=grid_values,cv=3)\ngrid_search_model.fit(df_train_vector,df_train['target'])\n\nprint(grid_search_model.best_estimator_)\nprint(grid_search_model.best_score_)\nprint(grid_search_model.best_params_)\n\n## dumping the output to a file \npredict_df = pd.DataFrame()\npredict = grid_search_model.predict(df_test_vector)\npredict_df['id'] = df_test['id']\npredict_df['target'] = predict\npredict_df.to_csv('sample_submission_2.csv', index=False)\nscore_df = score_df.append({'Model Description':'LR Model - with data cleaning and Grid Search',\n                           'Score':grid_search_model.best_score_}\n                           ,ignore_index=True)\n\n\n### let's have another model with some ngram's though \nX_train,X_test,y_train,y_test = train_test_split(df_train['clean_text'],df_train['target'])\nvector = TfidfVectorizer(ngram_range=(1,3)).fit(X_train)\nX_train_vector = vector.transform(X_train)\nX_test_vector = vector.transform(X_test)\n\nlr_model = LogisticRegression(C=1,penalty='l2').fit(X_train_vector,y_train)\npredict = lr_model.predict(X_test_vector)\nscore = roc_auc_score(y_test,predict)\nprint('Roc AUC curve for LR and TFIDF with ngrams  - %3f'%score)\n\nscore_df = score_df.append({'Model Description':'LR Model - with ngram range',\n                           'Score':score}\n                           ,ignore_index=True)\n","e0bc6509":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n### let's have another model with some ngram's though \nX_train,X_test,y_train,y_test = train_test_split(df_train['clean_text'],df_train['target'])\nvector = TfidfVectorizer(ngram_range=(1,3)).fit(X_train)\nX_train_vector = vector.transform(X_train)\nX_test_vector = vector.transform(X_test)\n\nlr_model = LogisticRegression(C=1,penalty='l2').fit(X_train_vector,y_train)\npredict = lr_model.predict(X_test_vector)\nscore = roc_auc_score(y_test,predict)\nprint('Roc AUC curve for LR and TFIDF with ngrams  - %3f'%score)\n\nscore_df = score_df.append({'Model Description':'LR Model - with ngram range',\n                           'Score':grid_search_model.score}\n                           ,ignore_index=True)\n\nvector = TfidfVectorizer(ngram_range=(1,3)).fit(df_train['clean_text'])\nX_train_vector = vector.transform(df_train['clean_text'])\nX_test_vector = vector.transform(df_test['clean_text'])\nlr_model = LogisticRegression(C=1,penalty='l2').fit(X_train_vector,df_train['target'])\npredict = lr_model.predict(X_test_vector)\n\n\n## dumping the output to a file \npredict_df = pd.DataFrame()\npredict_df['id'] = df_test['id']\npredict_df['target'] = predict\npredict_df.to_csv('sample_submission_001.csv', index=False)\n","21e0c368":"pd.concat([df_test,predict_df['target']],axis=1)\n\n### you could dump this in a csv and do further analysis to check what\n### misclassifications are there manually ,observations could then be used \n### to further tweak stuff\n","644e6646":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\n\nX_train, X_test, y_train, y_test = \\\n        train_test_split(df_train['clean_text'], df_train['target'], random_state=20)\n## Apply Tfidf tranformation\nvector = TfidfVectorizer().fit(X_train)\nX_train_vector = vector.transform(X_train)\nX_test_vector  = vector.transform(X_test)\ndf_test_vector = vector.transform(df_test['clean_text'])\n\ngb_model= GaussianNB().fit(X_train_vector.todense(),y_train)\npredict = gb_model.predict(X_test_vector.todense())\n\nprint('Roc AUC score - %3f'%(roc_auc_score(y_test,predict)))\nscore_df = score_df.append({'Model Description':'Naive Bayes',\n                           'Score':roc_auc_score(y_test,predict)}\n                           ,ignore_index=True)\n","e659a592":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nvector = TfidfVectorizer().fit(df_train['clean_text'])\ndf_train_vector = vector.transform(df_train['clean_text'])\ndf_test_vector = vector.transform(df_test['clean_text'])\n\nsvc_model = SVC()\ngrid_values={'kernel':['linear', 'poly', 'rbf'],'C':[0.001,0.01,1,10]}\ngrid_search_model= GridSearchCV(svc_model,param_grid=grid_values,cv=3)\ngrid_search_model.fit(df_train_vector,df_train['target'])\n\nprint(grid_search_model.best_estimator_)\nprint(grid_search_model.best_score_)\nprint(grid_search_model.best_params_)\n\nscore_df = score_df.append({'Model Description':'SVC - with Grid Search',\n                           'Score':grid_search_model.best_score_}\n                           ,ignore_index=True)\n\npredict = grid_search_model.predict(df_test_vector)\npredict_df = pd.DataFrame()\npredict_df['id'] = df_test['id']\npredict_df['target'] = predict\n\n# # print(predict_df.head(5))\npredict_df.to_csv('sample_submission_4.csv', index=False)\n","7d9c435b":"score_df[['Model Description','Score']]","f4d6569d":"Also we are going to store this text in a seperate column as we want to keep the orignal text in case we want to do some feature engineering down the line.","b529e760":"Let's look at score_df which has scores of all models till now and let's sort the output in ascending based on the Score","4bf08b5b":"In the next step we are going to do some further massaging which would make Job of Prediction Algorithm easy\n\n* Let us remove any characters other then alphabets\n* Convert all dictionary to lower case - for consistency \n* Lemmatize - More details on Stemming and Lemmatization [here](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html)\n","d53bb51a":"Before analyzing the data further would be nice to have a baseline model driven off just the tweet Text and TFIDF transformer. once we have some baseling we would look at some more visualization and feature engineering","e542b716":"Exploring the data distribution of tweets","e42fe542":"Check for Null\/NAN Values","e3730d7e":"**Please Upvote if you found the notebook usefull.Also please leave a comment if you think something could be improved\/done in a better way. **\n\nI have written another notebook to implement Word Embeddings using Word2Vec and then doing prediction implemention LR\/ RF , it would save you a lot of time if you are new to Embeddings here is the [link](https:\/\/www.kaggle.com\/slatawa\/simple-implementation-of-word2vec)","cd6d2aa0":"**3. Clean Data**  - So we have a baseline score of 79% to work with , let's get to clean data and see if we can improve the score\n\nAs first step in cleaning - let us replace some commonly occuring shorthands ","69b4801d":"**1. Loading data set **","16076de4":"Let's take a look at the data now ","f20f9984":"4.2 Let's apply Gaussian NB to the data ","21b7efbe":"**2. Let's take initial look at the data **","922389d7":"4.1 Start by creating a Logistic Regression model again , this time we will use Grid Seach for hyper-parameter optimization","9dfcb6d6":"**4. Creation of more Models**","9afccb4c":"**The Objective of this kernel is to get you on your feet. I would be using Logisitic Regression to create a basic baseline prediction model. Once we have that in place we would be doing some basic data cleaning and applying SVM,Naive Bayes to the data.**\n\nWould be adding - data visualization , feature engineering steps in the future.","92a0ea55":"4.3 Support Vector Classifier - with Grid search to Optimize parameters"}}