{"cell_type":{"37ef41a3":"code","0d22b790":"code","0d17ac2d":"code","cea42237":"code","52ccfe3b":"code","079b80fa":"code","6dd7661a":"code","906eed5a":"code","ee0e2d88":"code","56020d9e":"code","59d2e1cb":"code","72296db6":"code","b7141ebc":"code","d8f5d8e7":"code","979595fb":"code","b30466ec":"code","0204ee87":"code","78461bd3":"code","c9e9b7bd":"code","1bdc33c0":"code","7e24e9e6":"code","6b7a5e5f":"code","2a1bb178":"code","f0136fa1":"code","865d5530":"code","65f42436":"code","b0b0444f":"code","1d7b1d19":"code","cfecdbb7":"code","9b37ea57":"code","8c9e3021":"code","ac37f061":"code","0c0cc5bd":"code","2ba93da0":"code","8ecc81b6":"code","75281d4b":"code","da2918f2":"code","2b944901":"code","dce59d84":"code","f8ca0437":"code","c5c20520":"code","8aab03d3":"code","0c37668c":"code","6f7f8123":"code","bbecf8b9":"code","525f4da1":"code","b579e674":"code","00ac8ed3":"code","acf461a1":"code","683c7076":"code","e708ed3e":"code","c9c938ab":"code","4493f905":"code","c86a36c4":"code","56d85b63":"code","5e20410e":"code","4c379a9f":"code","d12bff1c":"code","a2433cae":"code","6bb8bd3d":"code","feb0e8a6":"code","5252dedb":"code","6819ea69":"code","b490f3c3":"markdown","3aaa04ee":"markdown","11fb74eb":"markdown","a603be12":"markdown","b3b3f49a":"markdown","eae4045b":"markdown","55f4a479":"markdown","968c4f62":"markdown","d4d1dde7":"markdown","350e8875":"markdown","04ffaeb8":"markdown","4b8d4927":"markdown","d028286e":"markdown","fa962043":"markdown","395a2f06":"markdown","9c999742":"markdown","666f21ec":"markdown","99e61199":"markdown","5de3c1de":"markdown","571dfa41":"markdown","1a583a72":"markdown","1f9192bb":"markdown","cb023f4e":"markdown","1d1b7e7e":"markdown","988d37dc":"markdown","96030724":"markdown","331f56fa":"markdown","9f467c95":"markdown","60d8d7e4":"markdown","62d8e184":"markdown","e6a6eb37":"markdown","67f64bef":"markdown","540d2b38":"markdown","cf870a68":"markdown","d5f57496":"markdown","dc07be78":"markdown","3847f03d":"markdown","3ea06ef0":"markdown","f536d4b0":"markdown","9947ef3e":"markdown","2bcfe72f":"markdown","d3528392":"markdown","7e8bb1df":"markdown","b4651076":"markdown","9832c0f5":"markdown","337de378":"markdown","f5f12495":"markdown","ceda729f":"markdown"},"source":{"37ef41a3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore')\n\n%matplotlib inline","0d22b790":"train_df = pd.read_csv('..\/input\/widsdatathon2022\/train.csv')\ntest_df = pd.read_csv('..\/input\/widsdatathon2022\/test.csv')\ntrain_df.head()","0d17ac2d":"train_df.shape","cea42237":"train_df.info()","52ccfe3b":"train_df.describe()","079b80fa":"train_df.dtypes.value_counts()","6dd7661a":"train_df.dtypes","906eed5a":"cat_features = train_df.select_dtypes(include=['object']).columns.to_list()","ee0e2d88":"cat_features","56020d9e":"num_features = [i for i in train_df.columns if i not in cat_features]","59d2e1cb":"cat_df = train_df.loc[:, cat_features]","72296db6":"cat_df['site_eui'] = train_df['site_eui']","b7141ebc":"cat_df","d8f5d8e7":"for i in cat_df.columns:\n    if i != 'site_eui':\n        print(i.upper())\n        print(cat_df[i].value_counts(), '\\n')","979595fb":"def cat_plots(df):\n    for i in cat_features:\n        plt.figure(figsize=(20,5))\n        sns.countplot(df[i])\n        plt.xticks(rotation=90)\n        plt.show()","b30466ec":"cat_plots(train_df)","0204ee87":"cat_plots(test_df)","78461bd3":"def missing(df, hex1, hex2, text):\n    plt.figure(figsize=(15,5))\n    sns.heatmap(df[cat_features].isna().values, cmap = [hex1, hex2], xticklabels=cat_features)\n    plt.title('Missing values in {}'.format(text), fontsize=20)\n    plt.show()","c9e9b7bd":"missing(train_df, '#ffff99', '#009933', 'Training Set')","1bdc33c0":"missing(test_df, '#ffccff',  '#66ccff', 'Test Set')","7e24e9e6":"def catplot(df, color):\n    for i, col in enumerate(['facility_type', 'State_Factor', 'building_class']):\n        sns.catplot(x=col, y='site_eui', data=df, kind='point', aspect=3, color=color)\n        plt.xticks(rotation=90)\n        plt.show()","6b7a5e5f":"catplot(train_df, 'red')","2a1bb178":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,OrdinalEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.impute import SimpleImputer","f0136fa1":"def encoder(train, test):\n    le = LabelEncoder()\n    for col in train.columns:\n        if train[col].dtypes == 'object':\n            train[col] = le.fit_transform(train[col])\n            le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n            test[col] = test[col].apply(lambda x: le_dict.get(x, -1))\n    return train, test","865d5530":"train_encoded, test_encoded = encoder(train_df, test_df)","65f42436":"train_encoded","b0b0444f":"test_encoded","1d7b1d19":"final_train = train_encoded.copy()\nfinal_test = test_encoded.copy()","cfecdbb7":"num_df = train_df[num_features]\nnum_df['site_eui'] = train_df['site_eui']\nnum_df.head()","9b37ea57":"corr_dict = num_df.corr(method='spearman').loc[:,'site_eui'].to_dict()","8c9e3021":"num_df.head()","ac37f061":"corr_dict","0c0cc5bd":"for i, val in corr_dict.items():\n    if i != 'id':\n        if (val < 0.3 and val>0.0) or (val > -0.3 and val<0):\n            final_train.drop(i, axis=1, inplace=True)\n            final_test.drop(i, axis=1, inplace=True)","2ba93da0":"final_train.head()","8ecc81b6":"final_test.head()","75281d4b":"final = final_train.drop('id', axis=1)\nsns.heatmap(final.corr())","da2918f2":"final_train.isnull().sum()","2b944901":"final_train.info()","dce59d84":"plt.figure(figsize=(15,5))\nsns.heatmap(final_train.isna().values, cmap = ['#ffff99', '#009933'], xticklabels=final_train.columns)\nplt.title('Missing values in {}'.format('Final Training Set'), fontsize=20)\nplt.show()","f8ca0437":"plt.figure(figsize=(15,5))\nsns.heatmap(final_test.isna().values, cmap = ['#99ccff', '#ff99ff'], xticklabels=final_test.columns)\nplt.title('Missing values in {}'.format('Final Test Set'), fontsize=20)\nplt.show()","c5c20520":"final_train['energy_star_rating'].describe()","8aab03d3":"final_train['energy_star_rating'].hist()","0c37668c":"sns.boxplot(final_train['energy_star_rating'], color = 'lightgreen')","6f7f8123":"final_train1 = final_train.copy()","bbecf8b9":"final_train1['energy_star_rating']=final_train1['energy_star_rating'].fillna(final_train1['energy_star_rating'].mean())","525f4da1":"final_train1.isnull().sum()","b579e674":"final_test1 = final_test.copy()","00ac8ed3":"final_test1['energy_star_rating']=final_test1['energy_star_rating'].fillna(final_test1['energy_star_rating'].mean())","acf461a1":"final_test1.isnull().sum()","683c7076":"scaler = StandardScaler()\nnum_feat = ['energy_star_rating']\nfinal_train1[num_feat] = scaler.fit_transform(final_train1[num_feat])\nfinal_test1[num_feat] = scaler.transform(final_test1[num_feat])","e708ed3e":"final_train1.head()","c9c938ab":"final_test1.head()","4493f905":"final_train1['site_eui'].hist()\nplt.show()","c86a36c4":"plt.hist(np.log(final_train1['site_eui']), color='green');\nplt.title('Log Transformation', fontsize=20)\nplt.show()","56d85b63":"plt.hist(np.sqrt(final_train1['site_eui']), color='purple');\nplt.title('Sqaure Root Transformation', fontsize=20)\nplt.show()","5e20410e":"from sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import (StandardScaler, \n                                   PolynomialFeatures)\nfrom sklearn.metrics import mean_squared_error","4c379a9f":"from sklearn.model_selection import train_test_split\n\nX= final_train1.drop('site_eui',axis=1)\ny= final_train1['site_eui']\nx_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=42)","d12bff1c":"def model(obj):\n    obj.fit(x_train, y_train)\n    obj_y_pred = lreg.predict(x_test)\n    mean_squared_error = np.mean((obj_y_pred - y_test)**2)\n    obj_coefficient = pd.DataFrame()\n    obj_coefficient[\"Columns\"] = x_train.columns\n    obj_coefficient['Coefficient Estimate'] = pd.Series(lreg.coef_)\n    return obj_coefficient, mean_squared_error","a2433cae":"def visualize(obj_coeff, title, color):\n    fig, ax = plt.subplots(figsize =(20, 8))\n\n    ax.bar(lreg_coefficient[\"Columns\"],\n    obj_coeff['Coefficient Estimate'],\n    color = color)\n    ax.spines['bottom'].set_position('zero')\n    plt.title(title, fontsize=25) \n    plt.style.use('ggplot')\n    plt.show()","6bb8bd3d":"lreg = LinearRegression()\nlreg_coefficient, lreg_mse = model(lreg)\nvisualize(lreg_coefficient, 'Linear Regression with Mean squared Error: {}'.format(lreg_mse), 'lightgreen')","feb0e8a6":"lasso = Lasso(alpha = 100000000000000)\nlasso_coefficient, lasso_mse = model(lasso)\nvisualize(lasso_coefficient, 'Lasso Regression with Mean squared Error: {}'.format(lasso_mse), 'lightblue')","5252dedb":"ridge = Ridge(alpha = 1)\nridge_coefficient, ridge_mse = model(lasso)\nvisualize(ridge_coefficient, 'Ridge Regression with Mean squared Error: {}'.format(ridge_mse), 'pink')","6819ea69":"alpha= [10 ** x for x in range(-6,8)]\ncv_error_array=[]\nfor i in alpha:\n    reg= Lasso(alpha=i,random_state=42)\n    reg.fit(x_train,y_train)\n    y_pred= reg.predict(x_test)\n    loss= mean_squared_error(y_test,y_pred)\n    cv_error_array.append(loss)\n    print(\"For Alpha : \", i ,\"Loss :\",round(loss,2))","b490f3c3":"## Tuning the alpha values and checking the variations in Lasso Regression","3aaa04ee":"Following points can be considered while remodelling:\n\n    1. Instead of Label Encoding we can try using on-hot or dummy encoding techniques\n    2. Missing Value Imputation for energy_star_rating can be done using median or by filling with regression model\n    3. We can consider other features which were dropped due to low correlation values\n    4. Try Box cox instead of Sqrt transformation of target variable\n    5. We can also compare the performance by training our model with ElasticNet\n    5. Try out other modelling algorithms like Random Forest\n    \nWe can consider this as our base model and keep building on top of this by tweaking the hyperparameters and chaning the data cleaning techniques. Also, we can compare different algorithms' performance to come up with the best one.","11fb74eb":"### Lasso Regression","a603be12":"#### In this notebook, I will be implementing Regression Models to predict the site-wise electricity consumption. The notebook has been divided into the following sections:\n    1. Undedrstanding the Data\n    2. EDA\n    3. Preprocessing\n    4. Key takeaways from the analysis\n    5. Summary of Data Cleaning steps\n    6. Modelling\n    7. Key Findings\n    8. Future Plan of Action","b3b3f49a":"# 6. Modelling","eae4045b":"# 1. Understanding the Data","55f4a479":"### Function to visualize the coefficient values","968c4f62":"# 2. EDA\n\n    1. Analyse the categorical features of train and test sets\n    2. Visualize the spread of the data\n    3. Encoding\n    4. Group the temperature columns together and try to perform feature engineering\n    5. days_below and days_above (see whether we can bring down the number of features)\n    6. Analyse the outliers\n    7. Drop the unnececssary columns\n    8. Analyse the distribution of the features (whether they are normally distributed)\n    9. Handle missing values","d4d1dde7":"    1. Categorical Features were Label Encoded\n    2. After studying the correlation, most of the numerical features were dropped due to low effectiveness\n    3. Missing Value Imputation for energy_star_rating\n    4. Scaled Numerical Features\n    5. Log transformation of the target variable","350e8875":"### Read train and test files","04ffaeb8":"## Analysing the numerical features","4b8d4927":"## Introduction","d028286e":"Test set interpretations:\n\n    1. state_11 dominates in the state_factor. \n    2. Multifamily_Uncategorized and Office_Uncategorized have dominated the facility_type feature. \n    3. Almost equal distribution of commercial and residential buildings, though Commercial surpasses in this case.","fa962043":"Degree of correlation:\n\n    1. Perfect: If the value is near \u00b1 1, then it said to be a perfect correlation.\n    2. High degree: If the coefficient value lies between \u00b1 0.50 and \u00b1 1, then it is said to be a strong correlation.\n    3. Moderate degree: If the value lies between \u00b1 0.30 and \u00b1 0.49, then it is said to be a medium correlation.\n    4. Low degree: When the value lies below + .29, then it is said to be a small correlation.\n    5. No correlation: When the value is zero.","395a2f06":"The below catplot shows the correlation of the categorical features with the target variable. We can infer that:\n\n    1. state_4 has the highest energy consumption whereas state_11 has the least.\n    2. There is a lot of variation in terms of energy consumptions among different facility type. \n    3. Commercial buildings consume more energy as compared to residential ones.","9c999742":"As we see from the above observations the energy_star_rating has no outliers and all the values lies within the range of 0 to 100. So, we will be using the mean to fill in the missing values.","666f21ec":"### Linear regression","99e61199":"### Creating a separate dataframe with just the categorical features and the target variable","5de3c1de":"Using the countplot we can see the distribution of the groups within a particular feature. As shown below:\n\n    1. state_6 has the most number of occurances int the state_factor. \n    2. Multifamily_Uncategorized has dominated the facility_type feature. \n    3. Almost equal distribution of commercial and residential buildings in our training dataset.","571dfa41":"### As per the degree of correlation, we are dropping the features having correlation value less than +\/-0.30 as they hardly have any effect on the target value","1a583a72":"## Comparison between log and square root transformation","1f9192bb":"## After Transformation:","cb023f4e":"## Before Transformation:","1d1b7e7e":"### Test Set","988d37dc":"### Encoding the categorical features in both train and test set","96030724":"### Base Model's Library Import","331f56fa":"# 5. Summary of Data Cleaning steps","9f467c95":"# 8. Future Plan of Action","60d8d7e4":"## Transforming Target","62d8e184":"# Check for missing values","e6a6eb37":"# 3. Preprocessing","67f64bef":"### Import Libraries","540d2b38":"# 4. Key takeaways from the analysis","cf870a68":"### Apply StandardScalar on numerical Features","d5f57496":"- Encoding Categorical Features\n- Impute missing values\n- Scale numerical feature\n- Transformation\n\nBefore modeling we need to get our dataset ready. We we try to apply linear regression algorithm on dataset with missing values it will throw an error. Similarly, it is advisable to to have standardized features meaning all the features are brought to the same scale having mean as 0 and standard deviation as 1. This improves the model learning rate. Below, we will perform few operations to clean our data and make it ready for modelling","dc07be78":"### Training Set ","3847f03d":"### Handling missing values","3ea06ef0":"# WiDS - Base Regression Model","f536d4b0":"### Missing Value Imputation","9947ef3e":"From the below transformation we see that in this case the square root transformation has given a more normally distributed data than log transformation","2bcfe72f":"Suprisingly there are no missing values in the categorical features of the datasets!","d3528392":"    1. The original dataset had a total of 64 columns (62 indedpendent variables + 1 target variable + 1 id column)\n    2. Only 3 categorical features with no missing values\n    3. Disparity in the categorical features of test and train set(state_6 dominates in train unlike the test set)\n    4. The numerical features (except energy_star_rating) doesn't contribute much in determining the target variable\n    5. No outliers were detected in energy_star_rating though it has a lot of missing data\n    6. From the heatmap we see that Energy_star_rating is highly correlated and Star_Factor is the least\n    7. Square Root Transformation worked better than Log transformation in normalizing the data","7e8bb1df":"### Ridge Regression","b4651076":"We are splitting our training set into train and test data to verify the model's performance. Firstly, we are creating x and y.\nx is a dataframe of predictors and y is our target variable set.\n\ntrain_test_split always divides the dataframe into 2 parts in the given ratio. Here we have splitted into 4:1.","9832c0f5":"# 7. Key Findings:","337de378":"### Function to build model","f5f12495":"### Train CV split for cross validation","ceda729f":"- We found that for the considered scenario all the models performed similarly\n\n- The Lasso Regression Model's performace was best with alpha=0.1. The performance didn't improve on reducing the hyperparameter beyond 1. But as we increase the alpha value the performance deteriorates gradually upto 100000 beyond which it remains constant.\n\n- There wasn't any difference in the mse and the coefficients of various regression model\n\n- We can try elastic regression model as well and compare the results\n\n- We can consider the Linear Regression model as our base model and further try to reduce the MSE "}}