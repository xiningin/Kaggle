{"cell_type":{"ff0a1033":"code","cea414bb":"code","182b7f98":"code","dca24c08":"code","2f23d0dc":"code","41208e99":"code","7bc98fa0":"code","aa4de2ee":"code","c1901ea4":"code","c23e8ee1":"code","39b5b6f8":"code","f1a2ef75":"code","27317044":"code","9ec0f52e":"code","bcc0db7a":"code","47591cad":"code","937902ca":"code","f3f154c2":"code","89852209":"code","f9a87655":"markdown","61a080db":"markdown","605c6c71":"markdown","5f7c633b":"markdown","f05f6912":"markdown","5e1922ae":"markdown","33b9fc44":"markdown","dbee8471":"markdown","67749b68":"markdown","7aec50fd":"markdown","fac1d8c9":"markdown","8dff2219":"markdown","e94d5645":"markdown","ce45edea":"markdown","8ed731b1":"markdown","6dee8dc5":"markdown","8a2e2b83":"markdown","a11a41e4":"markdown","f0465611":"markdown"},"source":{"ff0a1033":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","cea414bb":"train_df= pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/train.csv')\ntest_df = pd.read_csv(\"\/kaggle\/input\/bike-sharing-demand\/test.csv\")","182b7f98":"map_to_uniqueness = {}\nfor cols in train_df.columns: \n    map_to_uniqueness[cols] = train_df[cols].nunique()\nprint(map_to_uniqueness)\n","dca24c08":"null_values = {}\nfor cols in train_df.columns:\n    null_values[cols] = train_df[cols].isnull().sum()\nprint(null_values)\n","2f23d0dc":"import seaborn as sns\nsns.distplot(train_df['count'], bins = 100);\nprint(train_df[\"count\"].skew())\n\ntrain_df.datetime = pd.to_datetime(train_df.datetime)\ntrain_df[\"month\"] = train_df.datetime.dt.month\ntrain_df[\"year\"] = train_df.datetime.dt.year\ntrain_df[\"hour\"] = train_df.datetime.dt.hour\n\ntest_df.datetime = pd.to_datetime(test_df.datetime)\ntest_df[\"month\"] = test_df.datetime.dt.month\ntest_df[\"year\"] = test_df.datetime.dt.year\ntest_df[\"hour\"] = test_df.datetime.dt.hour","41208e99":"import matplotlib.pyplot as plt\ncor = train_df.corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(cor, vmax=.8, square=True, annot= True);","7bc98fa0":"# Removing colums from train which are not in test_df and column which are too much correlated with some other variable\ncols = [\"casual\", \"registered\", \"atemp\", \"datetime\"]\ntrain_df = train_df.drop(cols, axis=1)\ntest_df = test_df.drop([\"atemp\", \"datetime\"], axis =1)\ny = train_df.pop(\"count\")","aa4de2ee":"# Draw a histogram of training data\n\ntrain_cols = [col for col in list(train_df)]\ntrain_df[train_cols].hist(figsize=(20,20), bins=100, color='blue', alpha=0.5)\nplt.show()\n\n# Just to take a quicklook at the data by looking at their distributions\n\nprint(len(train_df[train_df[\"holiday\"] == 1].holiday))\nprint(len(train_df[train_df[\"holiday\"] == 0].holiday))\n","c1901ea4":"\ntest_cols = [col for col in list(test_df)]\ntest_df[test_cols].hist(figsize=(20,20), bins=100, color='red', alpha=0.5)\nplt.show()\nprint(test_df.columns)","c23e8ee1":"#standardizing data\nfrom sklearn.preprocessing import StandardScaler","39b5b6f8":"\nfrom scipy.stats import norm\nfrom scipy import stats\nsns.distplot(y, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)\n\n","f1a2ef75":"numerical_cols = [\"windspeed\", \"temp\", \"humidity\"]\ncat_cols = [cols for cols in train_df.columns.to_list() if cols not in numerical_cols]","27317044":"train_df[cat_cols] = train_df[cat_cols].astype('category')\ntest_df[cat_cols] = test_df[cat_cols].astype('category')","9ec0f52e":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_df, y, random_state=5, test_size = 0.2)\n\"\"\"\ncols = [\"temp\", \"humidity\", \"windspeed\"]\nfor col in cols:\n    X_train[cols] = np.log1p(X_train[cols]) \n    X_valid[cols] = np.log1p(X_valid[cols])\n    refined_test_df[cols] = np.log1p(refined_test_df[cols])   \n\"\"\"\n","bcc0db7a":"y_train = np.log1p(y_train) \ny_valid = np.log1p(y_valid)\nprint(X_train.shape)\nprint(X_valid.shape)","47591cad":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n        ('std_scaler', StandardScaler())\n])\nfull_pipeline = ColumnTransformer([\n    ('cat', OneHotEncoder(), cat_cols),\n    ('num', num_pipeline, numerical_cols)\n])\n\n\nX_train_prepared = full_pipeline.fit_transform(X_train)\nX_valid_prepared = full_pipeline.transform(X_valid)\n\nprint(X_train_prepared.shape)\nprint(X_valid_prepared.shape)\n\nxg = XGBRegressor(random_state = 0, learning_rate = 0.2, n_estimators = 150)\n\nxg.fit(X_train_prepared, y_train)\ny_pred = xg.predict(X_valid_prepared)\ny_train_pred = xg.predict(X_train_prepared)\n     \nprint(\"These are rmsle for train and validation sets {}, {}\"\n      .format(np.sqrt(mean_squared_log_error(y_train_pred, y_train)), \n              np.sqrt(mean_squared_log_error(y_pred, y_valid))\n             ))","937902ca":"\nX_test_prepared = full_pipeline.transform(test_df)\nres  = xg.predict(X_test_prepared)\nprint(res)\nres = np.expm1(res).astype(int)\nprint(res)","f3f154c2":"submission = pd.read_csv(\"\/kaggle\/input\/bike-sharing-demand\/sampleSubmission.csv\")","89852209":"output = pd.DataFrame({'datetime': submission.datetime,\n                       'count': res })\nprint(output.shape)\noutput.to_csv('submission.csv', index=False)","f9a87655":"From above we feel tempted to get rid of few of the outliers. We can say that temp>40 falls prey to outlier category and so is windspeed>50 and when weather is of 4th category that is in winters. We will say, it's not worth it, let's live with it","61a080db":"### Splitting categorical and numerical data","605c6c71":"We can see from above histograms that both the test data and train data are distributed in pretty much the same way.","5f7c633b":"### Checking null values","f05f6912":"## Let's do some feature analysis","5e1922ae":"## Let's have a look at the histogram!","33b9fc44":"### Let's look for bivariate variables","dbee8471":"There are no null values in any column. Yayy! We are good to go without bothering about imputations and methods!","67749b68":"## Splitting the data into train and test set","7aec50fd":"In search of Normality!","fac1d8c9":"We can see that data is not normally distributed so we need to take care of skewness and transform out target variable to a normal distributed target variable. It might also come to mind that why I did not create another column of day but but but since it is already given in the question statemnet that for the training data the days would be from 1 to 19 and for test data, it would be 20 above and since datetime series comes under categorical variable so while one hot encoding training and test variable the two different categories will create a problem for us and we want to simple model.","8dff2219":"## Checking the number of unique number for categorical variables","e94d5645":"## Let's draw some correlation from the data","ce45edea":"## Taking care of datetime series","8ed731b1":"### Transforming the train and validation data to make their distribution normal","6dee8dc5":"## Model tarining","8a2e2b83":"We can see that there a correlation between temp and atemp so we don't need both of them. Looking at the correlation we can also see that registered has a string correlation with target i.e count and causal has a strong correlation with registred so we do not need both of them. Let's get rid of them!","a11a41e4":"From the above data we can observe that there are both kinds of variables present in the feature\n1. Categorical Variables\n2. Numerical Variables\n1st caegory includes columns: season, holiday, working_day, datetime, \n2nd category includes columns: temp, atmep, humidity, windspeed, registered, causal","f0465611":"##  **Getting train and test data**"}}