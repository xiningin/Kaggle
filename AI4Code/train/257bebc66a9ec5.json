{"cell_type":{"92e8796f":"code","19520c0b":"code","ec25e105":"code","e446e8b5":"code","0c41cfaa":"code","fae92cfb":"code","7c5533ec":"code","5e0c28c6":"code","b138e14e":"code","3187a071":"code","8e78e595":"code","88312baf":"code","55b19959":"code","07829c98":"code","10782b00":"code","53e4bbc5":"code","483d0132":"code","95df8b80":"code","23863ab2":"code","63727f7c":"code","a1c84b21":"code","5ebf11fc":"code","b4805384":"code","44ad5315":"markdown","3c90f419":"markdown","f42b3ca6":"markdown","93a7baac":"markdown","619007ec":"markdown","578cac80":"markdown","735140de":"markdown","117f92d2":"markdown","822c4d72":"markdown","497ccb8b":"markdown","88ce2a3f":"markdown","de69bd09":"markdown"},"source":{"92e8796f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","19520c0b":"test_path = '\/kaggle\/input\/titanic\/test.csv'\ntrain_path = '\/kaggle\/input\/titanic\/train.csv'\n\ntrain_data = pd.read_csv(train_path)\ntest_data = pd.read_csv(test_path)","ec25e105":"train_data.head(10)","e446e8b5":"train_data.head()\ntrain_data.shape#(891, 12)\ntrain_shape = train_data.shape[0]\ntest_shape = test_data.shape[0]","0c41cfaa":"test_data.shape","fae92cfb":"train_data.isnull().sum()","7c5533ec":"train_data['Embarked'].fillna('S', inplace = True)\ntest_data['Embarked'].fillna('S', inplace = True)","5e0c28c6":"cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\nX_Train = train_data[:train_shape][cols]\nX_test = test_data[:test_shape][cols]\ny = train_data[:train_shape]['Survived'].astype(int) ","b138e14e":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_X_Train = X_Train.copy()\nlabel_X_test = X_test.copy()\nGender_col = ['Sex']\nEmbarked_col = ['Embarked']\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col1 in Gender_col:\n    label_X_Train[col1] = label_encoder.fit_transform(X_Train[col1])\n    label_X_test[col1] = label_encoder.transform(X_test[col1])\n    \nfor col2 in Embarked_col:\n    label_X_Train[col2] = label_encoder.fit_transform(X_Train[col2])\n    label_X_test[col2] = label_encoder.transform(X_test[col2])    \n","3187a071":"label_X_test.head()","8e78e595":"label_X_Train.head()","88312baf":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer()","55b19959":"imputed_label_X_Train_data = pd.DataFrame(imputer.fit_transform(label_X_Train))\nimputed_label_X_test_data = pd.DataFrame(imputer.transform(label_X_test))\nimputed_label_X_Train_data.columns = label_X_Train.columns\nimputed_label_X_test_data.columns = label_X_test.columns","07829c98":"#imputed_label_X_Train_data.shape\nimputed_label_X_test_data.shape","10782b00":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(imputed_label_X_Train_data, y, test_size = 0.2)","53e4bbc5":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nprint(tf.__version__)","483d0132":"my_ann = Sequential()#initialising the ANN\nmy_ann.add(Dense(units = 4, kernel_initializer = 'glorot_uniform', activation = 'relu', input_dim = 7))\nmy_ann.add(Dense(units = 2, kernel_initializer = 'glorot_uniform', activation = 'relu'))\nmy_ann.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))","95df8b80":"my_ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmy_ann.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose = 1)","23863ab2":"y_pred = my_ann.predict(X_valid)\ny_pred = [1 if y>=0.5 else 0 for y in y_pred]#list comprehension","63727f7c":"from sklearn.metrics import f1_score\nF1Score = f1_score(y_pred, y_valid)\nprint(F1Score)#f1_score of validation dataset","a1c84b21":"my_ann.fit(imputed_label_X_Train_data, y, batch_size = 10, epochs = 100, verbose = 1)","5ebf11fc":"y_final = my_ann.predict(imputed_label_X_test_data)\ny_final = [1 if y>=0.5 else 0 for y in y_final]","b4805384":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_final })\noutput.to_csv(\"submission.csv\", index = False)\nprint(\"done!\")","44ad5315":"# **Hello guys, I'm Jesudas DSouza, a mentor at SkillConnect, Mumbai. This is a demo tensorflow + keras tutorial that should get you started in Tensorflow. I aim to simplify everything as much as I can. This should be enough for you to get started.**\n**\n\n*Tools i used in this notebook*\n\n matplotlib ---> data visualization ::: you can use this resource on kaggle learn https:\/\/www.kaggle.com\/learn\/data-visualization\n \n pandas ---> dataframe ::: you can utilise this resource on kaggle learn https:\/\/www.kaggle.com\/learn\/pandas\n \n numpy ---> easy scientific computing with python ::: #will update this kernel if i find a good course\n \n keras API with TensorFlow backend ---> making the neural network ::: #you should pretty much understand through this starter notebook\n \n *If you found this kernel useful, please upvote it*\n *If you found a step \/ that i havent covered in my explanation, please leave a comment. I'll add an explanation in an update*\n \n **An update : In th initial kernel, I had not taken the Embarked feature. In this kernel, I'm including it.**","3c90f419":"![](https:\/\/cdn.tinymind.com\/static\/img\/learn\/relu.png)**For this graph, its basically a rectifier graph or more commonly known as a relu activtion**","f42b3ca6":"**Lets do some Visualization!** #visualization will be another update","93a7baac":"Oh Boy! Out of 891 entries (rows), Age data for 177 entries and Cabin data for 687 entries is missing! How should we deal with missing data? well, there are 3 ways\n1. Ignore features with missing data\n2. Replace missing values with average\/median of the feature values\n3. Assume a value for the feature\nfor now, we'll use method 2, because it is the most logical option right now.\n\nBut, before that, lets select our most important features.\n","619007ec":"the head() function gives the top 5 rows in the dataset. You can display any no. of rows, by inserting the no. of rows you want inside the head() function. eg: train_data.head(10) will return 10 rows","578cac80":"**Now, we'll need to encode labels i.e, male and female. Since An ANN cannot really process a dataset which has string in a dataframe, we need to encode it.**","735140de":"On running the above code cell, we get the paths to the datasets. Load these paths into train and test path variables. Here, we have test_path and train_path. Now, using Pandas read_csv function, we load our csv files into a dataframe. Here, the train_data contains the dataframe for train.csv and likewise, test_data.  ","117f92d2":"Now, let us observe our data. Are there missing values? lets check","822c4d72":"# Now, let us create a train-validation split. We'll use sklearn's train_test_split. Note, this is just made for testing our model. In a later kernel i'll be covering overfitting and underfitting","497ccb8b":"**Notice how the curve goes from 0 to 1 on the y axis? This shows that the last layer of our ANN model will have values between 0 and 1**","88ce2a3f":"![This is a sigmoid graph](https:\/\/hvidberrrg.github.io\/deep_learning\/activation_functions\/assets\/sigmoid_function.png)","de69bd09":"**We perform Stochastic Gragient Descent, more specifically 'adam' optimizer. You must have heard about Gradient Descent, right? For those uninitiated, it deals with finding the minima of the equation. Now, some of you must have already found a problem with this, i.e. a situation where we are stuck at the local minima of the graph rather than finding the global minima of the graph. This is where SGD or stochastic gradient descent comes into picture**\n\n**With SGD, or any normal GD for the matter, the loss is always calculated in the Logarithm of the values. Here, we use binary cross entropy becayse out classification deals with the survival of a passenger i.e dead or alive**"}}