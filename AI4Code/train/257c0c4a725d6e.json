{"cell_type":{"88099f37":"code","91f19143":"code","5c137bce":"code","60b91de3":"code","ef8bb20f":"code","2f463a7a":"code","6d452988":"code","f613b4bc":"code","d623915e":"code","acb49cff":"code","b9f10041":"markdown"},"source":{"88099f37":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nimport math\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n%matplotlib inline","91f19143":"class SimpleModel(torch.nn.Module):\n    def __init__(self, dropout_rate, decay):\n        super(SimpleModel, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.decay = decay\n        self.f = torch.nn.Sequential(\n            torch.nn.Linear(1,20),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=self.dropout_rate),\n            torch.nn.Linear(20, 20),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=self.dropout_rate),\n            torch.nn.Linear(20,2)  # mean, variance \u51fa\u3059\n        )\n    def forward(self, X):\n        y_hat = self.f(X)\n        mean, variance = y_hat[:,0], y_hat[:,1]\n        #print(\"mean:\", mean)\n        #print(\"variance:\", variance)\n        variance = F.softplus(variance) + 1e-6  # \u5206\u6563\u306f F.softplus \u3067\u5e38\u306b\u6b63\u306b\u306a\u308b\u3088\u3046\u306b\u3059\u308b\n        return (mean, variance)\n    \nSimpleModel(0.5, 1e-3)","5c137bce":"class MSEAleatoricLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, y_pred, y_true):\n        \"\"\"\n        \u30c7\u30fc\u30bf\u306e\u30ce\u30a4\u30ba\u306b\u3088\u308b\u4e0d\u78ba\u5b9f\u6027\uff08aleatoric uncertainity\uff09\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306eloss\n        mse\uff08\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u640d\u5931\u95a2\u6570\uff09\u3092\u62e1\u5f35\n\n        y_true, y_pred \u3069\u3061\u3089\u3082\u51fa\u529b\u306b\u5e73\u5747\u3068\u5206\u6563\u304c\u5fc5\u8981\uff08= \u51fa\u529b\u5c64\u306f1\u30af\u30e9\u30b9\u306b2node\u5fc5\u8981\uff09\n        \u4e0d\u78ba\u5b9f\u6027\u6700\u5c0f\u5316\u3057\u305f\u3044\u304b\u3089\u304b\uff1f\u5206\u6563\u306e\u6559\u5e2b\u30c7\u30fc\u30bf\u306f\u3059\u3079\u30660\u3067\u3044\u3044\u307f\u305f\u3044\n\n        loss = 0.5 * (1\/N) * SUM_i{ (y_i-y_i_hat)^2 \/ N*std_i^2 + log(std_i^2) }\n        \"\"\"\n        N = y_true.shape[0]\n        se = torch.pow((y_true[:,0]-y_pred[0]),2)\n        inv_std = torch.exp(-y_pred[1])\n        mse = torch.mean(inv_std*se)\n        reg = torch.mean(y_pred[1])\n        return 0.5*(mse + reg)","60b91de3":"def uncertainity_estimate(x, model, num_samples, l2):\n    \"\"\"\n    MC dropout\u3067\u4e88\u6e2c\u306e\u4e0d\u78ba\u5b9f\u6027\u8a08\u7b97\n    Args:\n        x: \u4e88\u6e2c\u3057\u305f\u3044\u8aac\u660e\u5909\u6570\n        model: Dropout\u3001L2\u6b63\u5247\u5316\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3057\u305fDL\u30e2\u30c7\u30eb\n        num_samples: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u56de\u6570\n        l2: MC dropout\u306e\u5206\u6563\u51fa\u3059\u305f\u3081\u306el2\u6b63\u5247\u5316\u4fc2\u6570\n    \"\"\"\n    model.train()  # \u8a13\u7df4\u30e2\u30fc\u30c9\u306b\u3057\u3066dropout\u6d3b\u6027\u306b\u3059\u308b\n    outputs = np.hstack([model(x)[0].reshape(model(x)[0].shape[0], 1).cpu().detach().numpy() for i in range(num_samples)]) # n\u56de inference, output.shape = [20, N]\n    y_mean = outputs.mean(axis=1)  # n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u5e73\u5747\n    y_variance = outputs.var(axis=1)  # n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u5206\u6563\n    # \u5206\u6563\u306f\u3001\u300c\u666e\u901a\u306e\u5206\u6563\u300d+\u300c\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387\u3001L2\u6b63\u5247\u5316\u4fc2\u6570\u3001\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\u7b49\u306b\u95a2\u9023\u3057\u305f\u91cf\u300d\u3067\u6c42\u3081\u308b\n    tau = l2 * (1. - model.dropout_rate) \/ (2. * N * model.decay)\n    y_variance += (1. \/ tau)\n    y_std = np.sqrt(y_variance)\n    return y_mean, y_std","ef8bb20f":"# \u56de\u5e30\u554f\u984c\u3092\u8003\u3048\u308b\nx = np.linspace(-3, 3, 100).reshape(100, 1)\ny = x**3\ny = np.hstack([y, np.zeros(y.shape)])  # reshape tensors to support two outputs\nprint(f\"x: {x.shape}, y: {y.shape}\")\nprint(x[:5])\nprint(y[:5])\n\nN = 200 ##  number of points\n\nx_obs = np.linspace(-2, 2, N)\nnoise = np.random.normal(loc=0, scale=3., size=N)\ny_obs =  x_obs**3 + noise\nprint(f\"x_obs: {x_obs.shape}, y_obs: {y_obs.shape}\")\nprint(x_obs[:5])\nprint(y_obs[:5])\n\nx_test = np.linspace(-3, 3, N)\ny_test = x_test**3 + noise\nprint(f\"x_test: {x_test.shape}, x_test: {x_test.shape}\")\n\n## Normalise data:\n#x_mean, x_std = x_obs.mean(), x_obs.std()\n#y_mean, y_std = y_obs.mean(), y_obs.std()\n#x_obs = (x_obs - x_mean) \/ x_std\n#y_obs = (y_obs - y_mean) \/ y_std\n#x_test = (x_test - x_mean) \/ x_std\n#y_test = (y_test - y_mean) \/ y_std","2f463a7a":"plt.figure(figsize=(12,6))\nplt.plot(x, y[:,0], ls=\"--\", color=\"r\", label='ground truth: $y=x^3$')\nplt.plot(\n    x_obs,\n    y_obs,\n    \"or\",\n    marker=\"o\",\n    color=\"0.1\",\n    alpha=0.8,\n    label=\"data points\",\n)\nplt.grid()\nplt.legend()\nplt.show()","6d452988":"model = SimpleModel(dropout_rate=0.5, decay=1e-6).to(device)\ncriterion  = MSEAleatoricLoss().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=model.decay)","f613b4bc":"# \u5b66\u7fd2\nfor iter in range(20000):\n    y_pred = model(torch.Tensor(x_obs).view(-1,1).to(device))\n    y_true = Variable(torch.Tensor(y_obs).view(-1,1).to(device))\n    optimizer.zero_grad()\n    loss = criterion(y_pred, y_true)\n    loss.backward()\n    optimizer.step()\n    \n    if iter % 2000 == 0:\n        print(\"Iter: {}, Loss: {:.4f}\".format(iter, loss.item()))","d623915e":"model.eval()  # aleatoric uncertainity \u306b\u3064\u3044\u3066\u306f\u63a8\u8ad6\u30e2\u30fc\u30c9\u306b\u3057\u3066dropout\u975e\u6d3b\u6027\u306b\u3059\u308b\ny_pred = model(torch.Tensor(x_obs).view(-1, 1).to(device))\ny_mean = y_pred[0].cpu().detach().numpy()\ny_std = np.sqrt(y_pred[1].cpu().detach().numpy())\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y[:,0], ls=\"--\", color=\"r\", label=\"ground truth $y=x^3$\")\nplt.plot(\n    x_obs, y_obs, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"data points\"\n)\nplt.plot(x_obs, y_mean, ls=\"-\", color=\"b\", label=\"MLP (MSEAleatoricLoss)\")\nfor i in range(2):\n    plt.fill_between(\n        x_test,\n        y_mean - y_std * ((i + 1.0)),\n        y_mean + y_std * ((i + 1.0)),\n        color=\"b\",\n        alpha=0.1,\n        label=f\"aleatoric uncertainity {i+1}$\\sigma$\",\n    )\nplt.legend()\nplt.grid()\nplt.show()","acb49cff":"# n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u4e0d\u78ba\u5b9f\u6027\u8a08\u7b97\n\niters_uncertainty = 200\n\nlengthscale = 0.01\ny_mean, y_std = uncertainity_estimate(\n    torch.Tensor(x_test).view(-1, 1).to(device), model, iters_uncertainty, lengthscale\n)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y[:,0], ls=\"--\", color=\"r\", label=\"ground truth $y=x^3$\")\nplt.plot(x_obs, y_obs, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"data points\")\nplt.plot(x_test, y_mean, ls=\"-\", color=\"b\", label=\"MC Dropout mean\")  # \u9752\u7dda\u304c\u4e88\u6e2c\u306e\u5e73\u5747\n#plt.plot(x_test, y_test, ls=\"--\", color=\"r\", label=\"true\")  # \u8d64\u7dda\u304c\u6b63\u89e3\nfor i in range(2):\n    plt.fill_between(\n        x_test,\n        y_mean - y_std * ((i + 1.0)),\n        y_mean + y_std * ((i + 1.0)),\n        color=\"b\",\n        alpha=0.1,\n        label=f\"epistemic uncertainty {i+1}$\\sigma$\",\n    )\nplt.legend()\nplt.grid()\nplt.show()","b9f10041":"Combining Aleatoric and Epistemic Uncertainty in One Model\n- https:\/\/arxiv.org\/abs\/1703.04977\n- https:\/\/towardsdatascience.com\/my-deep-learning-model-says-sorry-i-dont-know-the-answer-that-s-absolutely-ok-50ffa562cb0b"}}