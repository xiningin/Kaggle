{"cell_type":{"c04c9267":"code","5270fdb8":"code","35bbcc4a":"code","16e2e801":"code","207dba49":"code","84a730d1":"code","4d1904eb":"code","d5a1dd9e":"code","4c8700a4":"code","760becea":"code","3a9a6ea8":"code","03d2c7e8":"code","6b1baef7":"code","b3b4078c":"code","c1574a98":"code","73f3b3dc":"code","c279874e":"code","609059e1":"code","cc5cc994":"code","8a146756":"code","c76b2070":"code","46b9d94d":"code","b13f78e2":"code","b758ffca":"code","d4e01338":"code","1db56f26":"code","2317d647":"code","59f0e84d":"code","98ac1da7":"code","fd62870f":"code","e1d3992b":"code","07945801":"code","89d9587f":"code","9edb4802":"code","c71615c9":"code","38985d91":"code","9dace9e1":"code","c693ee6f":"code","170c2452":"code","e342416c":"code","a481dd99":"code","5aa9a60f":"code","ad0e9df7":"markdown","032a2395":"markdown","0138163f":"markdown","2127cae1":"markdown"},"source":{"c04c9267":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5270fdb8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)","35bbcc4a":"train = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/sampleSubmission.csv', sep=\",\")","16e2e801":"train.head(15)","207dba49":"test.head()","84a730d1":"train.loc[train.SentenceId == 2]","4d1904eb":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))","d5a1dd9e":"print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))","4c8700a4":"print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","760becea":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext_trigrams = [i for i in ngrams(text.split(), 3)]","3a9a6ea8":"Counter(text_trigrams).most_common(30)","03d2c7e8":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext = [i for i in text.split() if i not in stopwords.words('english')]\ntext_trigrams = [i for i in ngrams(text, 3)]\nCounter(text_trigrams).most_common(30)","6b1baef7":"tokenizer = TweetTokenizer()","b3b4078c":"vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train['Phrase'])\ntest_vectorized = vectorizer.transform(test['Phrase'])","c1574a98":"y = train['Sentiment']","73f3b3dc":"logreg = LogisticRegression()\novr = OneVsRestClassifier(logreg)","c279874e":"%%time\novr.fit(train_vectorized, y)","609059e1":"scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))","cc5cc994":"%%time\nsvc = LinearSVC(dual=False)\nscores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))","8a146756":"ovr.fit(train_vectorized, y)\nsvc.fit(train_vectorized, y)","c76b2070":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU \nfrom keras.layers import CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","46b9d94d":"tk = Tokenizer(lower = True, filters = '')\ntk.fit_on_texts(full_text)","b13f78e2":"train_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])","b758ffca":"max_len = 50\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)","d4e01338":"embedding_path = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"","1db56f26":"embed_size = 300\nmax_features = 30000","2317d647":"\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","59f0e84d":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))","98ac1da7":"def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, \n                 dense_units=128, dr=0.0,conv_size=32):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n    \n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units \/ 2), activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","fd62870f":"model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, \n                      kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)","e1d3992b":"model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, \n                      spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.2, conv_size=32)","07945801":"def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    \n    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n    \n    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n    \n    \n    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n    \n    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units \/ 2), activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","89d9587f":"model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, \n                      kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)","9edb4802":"model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, \n                      kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)","c71615c9":"model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, \n                      kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)","38985d91":"pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\npred = pred1\npred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred2\npred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred3\npred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred4\npred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred5\n","9dace9e1":"#predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n#sub['Sentiment'] = predictions\n#sub.to_csv(\"blend.csv\", index=False)","c693ee6f":"class Attention(Layer):\n    \"\"\"\n    Keras Layer that implements an Attention mechanism for temporal data.\n    Supports Masking.\n    Follows the work of Raffel et al. [https:\/\/arxiv.org\/abs\/1512.08756]\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    :param kwargs:\n    Just put it on top of an RNN Layer (GRU\/LSTM\/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(Attention())\n    \"\"\"\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","170c2452":"def build_model(maxlen, max_features, embed_size, embedding_matrix):\n    input_words = Input((max_len, ))\n    x_words = Embedding(19479,\n                        embed_size,\n                        weights=[embedding_matrix],\n                        mask_zero=True,\n                        trainable=False)(input_words)\n    x_words = SpatialDropout1D(0.2)(x_words)\n    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n    \n    x = Attention(maxlen)(x_words)\n    #x = GlobalMaxPooling1D()(x)\n    #x = GlobalAveragePooling1D()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    pred = Dense(5, activation='softmax')(x)\n\n    model = Model(inputs=input_words, outputs=pred)\n    return model\n\nmodel = build_model(max_len, max_features, embed_size, embedding_matrix)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","e342416c":"save_file = 'model_by_tyk.h5'\n#history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.2, \n                        #verbose = 1, callbacks = [check_point, early_stop])\nhistory = model.fit(X_train, y_ohe,\n                    epochs=20, verbose=1,\n                    batch_size=512, shuffle=True)","a481dd99":"pred = model.predict(X_test, batch_size = 1024, verbose = 1)","5aa9a60f":"sub['Sentiment'] = pred\nsub.to_csv(\"blend.csv\", index=False)","ad0e9df7":"**Model 1 Architecture**","032a2395":"**Attention + LSTM**","0138163f":"**Deep Learning**","2127cae1":"**Feature processing and engineering****"}}