{"cell_type":{"bf306b52":"code","9f158f51":"code","f0909b6c":"code","bcaa1adf":"code","8965c384":"code","7c57b666":"code","b760dec3":"code","a11bb9ce":"code","176307ec":"code","ad569c5b":"code","27627237":"code","f174c477":"code","0626c8fe":"code","3b245db5":"code","aa26c83c":"code","4dac71aa":"code","55b5c804":"code","df6c9b85":"code","988eb821":"code","1cbf9393":"code","c50d12e2":"code","4e9e4792":"code","0cd29510":"code","36155e20":"code","8cf9619d":"code","64cb0271":"code","be823cec":"code","6912e63f":"code","78c4d63b":"code","b5993b8d":"code","f879490f":"code","98461721":"code","2ad97d6e":"code","f871d669":"code","cf229f81":"code","5c345e94":"code","289961c4":"code","848454e7":"code","ecbcc88e":"code","73485715":"code","9ab4cf4e":"code","8ec89602":"code","6bbfddf8":"code","5d9148fe":"code","1493cdeb":"code","38eed06b":"code","f9d01663":"code","0b6c3e65":"code","024a4cec":"code","98b9a5aa":"code","ac5869bf":"code","c407af16":"markdown"},"source":{"bf306b52":"# Load the dataset\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nspam = pd.read_csv(\"..\/input\/spam-email\/spam.csv\")","9f158f51":"# Exploramos los datos\nspam.head()","f0909b6c":"recuento = spam['Category'].value_counts()\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.barh(recuento.index, recuento.values, color = ['blue', 'orange'])\nplt.subplot(1,2,2)\nplt.pie(recuento.values, labels = recuento.index, autopct = '%.2f%%')\nplt.show()","bcaa1adf":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\nfor train_index, test_index in split.split(spam, spam['Category']):\n    print(train_index, test_index)\n    strat_train_set = spam.iloc[train_index]\n    strat_test_set = spam.iloc[test_index]","8965c384":"pct_test = strat_test_set['Category'].value_counts() \/ len(strat_test_set)\npct_train = strat_train_set['Category'].value_counts() \/ len(strat_train_set)\npct_total = spam['Category'].value_counts() \/ len(spam)\nspam_pct = [list(pct_total.values)[1]] + [list(pct_train.values)[1]] + [list(pct_test.values)[1]]\nham_pct = [list(pct_total.values)[0]] + [list(pct_train.values)[0]] + [list(pct_test.values)[0]]\ncat = ['spam', 'ham']\ntipo = ['total', 'train', 'test']\ndic = {}\ndic['spam\/ham'] = ['spam']*3 + ['ham']*3\ndic['type'] = tipo * 2\ndic['pct'] = spam_pct + ham_pct\ndf = pd.DataFrame(dic)\n\n\nimport seaborn as sns\nplt.figure(figsize=(10, 6))\nsns.barplot(x = 'type', hue = 'spam\/ham', y = 'pct', data = df)\nplt.show()","7c57b666":"#Preparing data for algorithms\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\npipeline_text = Pipeline([('text_extraction', CountVectorizer(max_features = 5000))])\nfull_pipeline = ColumnTransformer([('encoder', OrdinalEncoder(), ['Category']), ('text_extraction', pipeline_text, 'Message')])","b760dec3":"#Preparing train_set\ntrain_set_prepared = full_pipeline.fit_transform(strat_train_set)","a11bb9ce":"train_set_prepared_labels = train_set_prepared[:,0].copy()\ntrain_set_prepared_predictors = train_set_prepared[:,1:].copy()\nlen(train_set_prepared_labels.toarray()), len(train_set_prepared_predictors.toarray())","176307ec":"# Evaluating different models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nforest_clf = RandomForestClassifier()\nsgd_clf = SGDClassifier()\nsvc_clf = SVC()\nkn_clf = KNeighborsClassifier()\nnb_clf = GaussianNB()\nlg_clf = LogisticRegression()","ad569c5b":"from sklearn.model_selection import cross_val_score\n\nmodels = ['lg_clf', 'nb_clf','forest_clf','sgd_clf','svc_clf','kn_clf']\ndic_models_scores = {}\n\nfor model in models:\n    scores = cross_val_score(eval(model), train_set_prepared_predictors.toarray(), train_set_prepared_labels.toarray().ravel(),\n                             cv = 5, scoring = \"accuracy\")\n\n    dic_models_scores[model] = list(scores)\n    dic_models_scores[f'{model}_means'] = scores.mean()\n    dic_models_scores[f'{model}_std'] = scores.std()","27627237":"dic_models_scores","f174c477":"plt.figure(figsize=(8,5))\nplt.title('Precisi\u00f3n de los modelos')\nplt.grid(axis = 'y')\nplt.yticks([0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 1.0])\nsns.barplot(models, [dic_models_scores['lg_clf_means'], dic_models_scores['nb_clf_means'], dic_models_scores['forest_clf_means'],\n                 dic_models_scores['sgd_clf_means'], dic_models_scores['svc_clf_means'], dic_models_scores['kn_clf_means']])","0626c8fe":"plt.figure(figsize=(10,5))\nfor model in models:\n    plt.plot(range(1,6), dic_models_scores[model], label = model)\n    \nplt.title('5 cross-validation with 6 models')\nplt.legend()\nplt.show()","3b245db5":"# Let's look some classifiers\nwords = full_pipeline.named_transformers_['text_extraction']['text_extraction'].get_feature_names()\nvoc = full_pipeline.named_transformers_['text_extraction']['text_extraction'].vocabulary_\nfull_pipeline.named_transformers_['encoder'].categories_\nlen(words), train_set_prepared_predictors.toarray().shape[1] # Words == Columns","aa26c83c":"# Fine-tune SGD\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'penalty' : ['l2', 'l1', 'elasticnet'],\n               'loss' : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], 'max_iter': [2000]}]\n\ngrid_search = GridSearchCV(sgd_clf, param_grid, cv = 5, scoring = \"accuracy\")\ngrid_search.fit(train_set_prepared_predictors.toarray(), train_set_prepared_labels.toarray().ravel())","4dac71aa":"# Fine-tune lg\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid2 = [{'C' : [0.8, 1, 1.2],\n               'solver': ['lbfgs', 'liblinear'],\n               'max_iter': [200]}]\n\ngrid_search2 = GridSearchCV(lg_clf, param_grid2, cv = 5, scoring = \"accuracy\")\ngrid_search2.fit(train_set_prepared_predictors.toarray(), train_set_prepared_labels.toarray().ravel())","55b5c804":"# Fine-tune random forest\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid3 = [{'n_estimators' : [95,100,150], # much time \n                'criterion' : ['gini', 'entropy'],\n                'max_features' : ['auto', 'sqrt', 'log2']}]\n                \n\ngrid_search3 = GridSearchCV(forest_clf, param_grid3, cv = 5, scoring = \"accuracy\")\ngrid_search3.fit(train_set_prepared_predictors.toarray(), train_set_prepared_labels.toarray().ravel())","df6c9b85":"#Wich words are more importante in the model for predictions\nimport sys\n!{sys.executable} -m pip install wordcloud\nfrom wordcloud import WordCloud\nfeature_importance = sorted(zip(grid_search3.best_estimator_.feature_importances_, words), reverse = True)","988eb821":"new_dict = {}\nfor k, v in dict(feature_importance).items():\n    new_dict[v] = k\n    \nword_cloud = WordCloud(collocations = False, background_color = 'white').fit_words(new_dict)\nword_cloud.to_file('Importance.jpg')","1cbf9393":"x = [elem[1] for elem in feature_importance]\ny = [elem[0] for elem in feature_importance]\nplt.figure(figsize=(15,5))\nplt.bar(x[:20], y[:20])","c50d12e2":"# Final models\nforest_final = grid_search3.best_estimator_\nsgd_final = grid_search.best_estimator_\nlg_final = grid_search2.best_estimator_","4e9e4792":"# other metrics for models\nfrom sklearn.model_selection import cross_val_predict\n\ndic_preds = {}\nfor model in ['sgd_final', 'lg_final', 'forest_final']:\n    y_train_pred = cross_val_predict(eval(model), train_set_prepared_predictors.toarray(),\n                                     train_set_prepared_labels.toarray().ravel(), cv = 3)\n    dic_preds[model] = y_train_pred ","0cd29510":"from sklearn.metrics import confusion_matrix\ndic_cm = {}\nfor model in dic_preds:\n    dic_cm[model] = confusion_matrix(train_set_prepared_labels.toarray().ravel(), dic_preds[model])\ndic_cm\n\nfor model in dic_cm:\n    print(f'{model} cm')\n    print(dic_cm[model])\n    print('\\n')","36155e20":"from sklearn.metrics import precision_score, recall_score, f1_score\ndic_scores = {}\nfor model in dic_cm:\n    p = precision_score(train_set_prepared_labels.toarray().ravel(), dic_preds[model])\n    r = recall_score(train_set_prepared_labels.toarray().ravel(), dic_preds[model])\n    f = f1_score(train_set_prepared_labels.toarray().ravel(), dic_preds[model])\n    dic_scores[model] = [p,r,f]\n    \ndf = pd.DataFrame(dic_scores, index = ['Precision', 'Recall', 'F1 score'])\nplt.matshow(df, cmap = plt.cm.gray)\ndf","8cf9619d":"# Precission, recall and thresholds\nfrom sklearn.metrics import precision_recall_curve\n\ndic_decision = {}\nfor model in ['sgd_final', 'lg_final', 'forest_final']:\n    try: \n        decisions = cross_val_predict(eval(model), train_set_prepared_predictors.toarray(),\n                                     train_set_prepared_labels.toarray().ravel(), cv = 3, method = \"decision_function\")\n    except:\n         print(model, 'usa predict_proba')\n         decisions = cross_val_predict(eval(model), train_set_prepared_predictors.toarray(),\n                                     train_set_prepared_labels.toarray().ravel(), cv = 3, method = \"predict_proba\")\n    dic_decision[model] = decisions","64cb0271":"dic_decision","be823cec":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label = 'precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label = 'recall')\n    plt.grid(axis = 'both')\n    \nfor model in ['sgd_final', 'lg_final', 'forest_final']:\n    if model is 'forest_final':\n        decisions = dic_decision[model][:,1]\n    else: \n        decisions = dic_decision[model]\n\n    precisions, recalls, thresholds = precision_recall_curve(train_set_prepared_labels.toarray().ravel(), decisions)\n    f1score = []\n    for precision, recall in zip(precisions, recalls):\n        f = 2 *(precision * recall) \/ (precision + recall)\n        f1score.append(f)\n        \n    plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n    plt.plot(thresholds, f1score[:-1], 'r-', label = \"f1-score\")\n    plt.title(f'{model}, recall vs precision vs f1_score')\n    plt.legend()\n    plt.show()\n    ","6912e63f":" precisions, recalls, thresholds","78c4d63b":"def roc_curves(fpr, tpr, thresholds, model, area):\n    plt.plot(fpr, tpr, label = f'{model} --> {area}')\n    plt.plot([0,1], [0,1])\n    plt.grid(axis = 'both')","b5993b8d":"#Roc Curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nplt.figure(figsize=(18,6))\nfor model in ['sgd_final', 'lg_final', 'forest_final']:\n    areas = []\n    if model == 'forest_final':\n        decisions = 'predict_proba'\n    else:\n        decisions = 'decision_function'\n        \n    y_scores = cross_val_predict(eval(model), train_set_prepared_predictors.toarray(),\n                                 train_set_prepared_labels.toarray().ravel(), cv = 3, method = decisions)\n    \n    if model == 'forest_final':\n        y_scores = y_scores[:,1]\n    fpr, tpr, thresholds = roc_curve(train_set_prepared_labels.toarray().ravel(), y_scores)\n    area = roc_auc_score(train_set_prepared_labels.toarray().ravel(), y_scores)\n    roc_curves(fpr, tpr, thresholds, model, area)\n    plt.title(f'roc curve')\nplt.legend()\nplt.show()","f879490f":"np.round(fpr, 3), tpr, thresholds","98461721":"# \u00bfMore recall? --> less precision\nthresholds_90recall = thresholds[np.argmax(recalls <= 0.9)]\nrecalls","2ad97d6e":"thresholds_90recall # With that probability, we will obtain 0.9 recall","f871d669":"decisions = cross_val_predict(forest_clf, train_set_prepared_predictors.toarray(),\n                         train_set_prepared_labels.toarray().ravel(), cv = 3, method = \"predict_proba\")","cf229f81":"# Force to 0.9 recall\npreds = (decisions[:,1] >= thresholds_90recall)\npreds_ = [int(elem) for elem in list(preds)]\nprecision_score(train_set_prepared_labels.toarray().ravel(), preds_), recall_score(train_set_prepared_labels.toarray().ravel(), preds_)","5c345e94":"# Predictions with test set\ntest_set_prepared = full_pipeline.transform(strat_test_set)\ntest_set_prepared = pd.DataFrame(test_set_prepared.toarray())\n\ntest_set_prepared_pred = test_set_prepared.drop(0, axis = 1)\ntest_set_prepared_labels = test_set_prepared.iloc[:,0]","289961c4":"dic_final_pred = {}\nfor model in ['sgd_final', 'lg_final', 'forest_final']:\n    pred = eval(model).predict(test_set_prepared_pred)\n    dic_final_pred[model] = pred\n    scores = []\n    p = precision_score(test_set_prepared_labels, pred)\n    r = recall_score(test_set_prepared_labels, pred)\n    f = f1_score(test_set_prepared_labels, pred)\n    for i in [p, r, f]:\n        scores.append(i)\n    dic_final_pred[f'{model} scores'] = scores","848454e7":"dic_final_pred","ecbcc88e":"def combinating_model_predictions(dic, pred_true):\n    preds = []\n    for model in ['sgd_final', 'lg_final', 'forest_final']:\n        preds.append(list(dic[model]))\n    pred_final = []\n    for pred1, pred2, pred3 in zip(preds[0],preds[1],preds[2]):\n        if pred1 == pred2 and pred1 == pred3:\n            pred_final.append(pred1)\n        elif pred1 == pred2 and pred1 != pred3:\n            pred_final.append(pred1)\n        elif pred1 == pred3 != pred2:\n            pred_final.append(pred1)\n        else:\n            pred_final.append(pred2)\n        \n    p = precision_score(pred_true, pred_final)\n    r = recall_score(pred_true, pred_final)\n    f = f1_score(pred_true, pred_final)\n            \n    return pred, [p,r,f]","73485715":"pred, scores = combinating_model_predictions(dic_final_pred, test_set_prepared_labels)\nscores # Same as lg_final","9ab4cf4e":"# Some individual prediction with 3 classifiers\nstrat_test_set[-10:]","8ec89602":"test_set_prepared[-10:]","6bbfddf8":"sgd_final.predict(test_set_prepared_pred[-10:])","5d9148fe":"lg_final.predict(test_set_prepared_pred[-10:])","1493cdeb":"forest_final.predict(test_set_prepared_pred[-10:])","38eed06b":"forest_final.predict_proba(test_set_prepared_pred[-10:])","f9d01663":"# More predictions random\nstrat_test_set.reset_index(drop = 'True', inplace = True)\nfilas = [3,12,24,36,40,44,83] # spam 3, 23, 35, 40, 44, 65, 67, 83, 89, 94\nstrat_test_set.iloc[filas]","0b6c3e65":"for index, row in strat_test_set.iloc[filas].iterrows():\n    print(row['Message'])\n    print('\\n')","024a4cec":" prep = test_set_prepared_pred.iloc[filas]","98b9a5aa":"forest_final.predict(prep)","ac5869bf":"pd.DataFrame(forest_final.predict_proba(prep), columns = ['no spam', 'spam'])","c407af16":"<img src='Importance.jpg' width=\"600\" height=\"400\">"}}