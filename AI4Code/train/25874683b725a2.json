{"cell_type":{"9c9b57da":"code","cfdccf9c":"code","ba3f13c4":"code","b7610a00":"code","15ad2290":"code","d3c3b0cb":"code","22923f57":"code","cdaf5159":"code","086393f1":"code","aa80f387":"code","7df79f02":"code","c6417be2":"code","2ec5da2a":"code","12852061":"code","accebc46":"code","4d85e9ed":"code","7481e659":"code","13fa1e6b":"code","7113e042":"code","2a50287a":"code","75f1ab04":"code","f4059e60":"code","be4215af":"code","aac43a0f":"code","0b9fdfee":"code","286b1ffb":"code","8d5f0a90":"code","f99ee19a":"code","347d6157":"code","8970cb52":"code","52bab637":"code","58b6adef":"code","a9d66837":"code","98010803":"code","0d92791a":"code","ddab0fcb":"code","a6526f03":"code","46e7d104":"code","a76bd7c0":"code","4ce9529d":"code","978c75eb":"code","2a40ec19":"code","909a8ca0":"code","407c1436":"code","ecfbeec2":"code","5a3b4cbe":"code","47c7bbdf":"code","15559b76":"code","c28a33d6":"code","624cb33c":"code","b0f9345e":"markdown","edc85edc":"markdown","1599572c":"markdown","d7064f14":"markdown","8f488c27":"markdown","5b41ba74":"markdown","a57178d1":"markdown","81bf4845":"markdown","e4726fc1":"markdown","04afdb4a":"markdown","fea8657b":"markdown","8bd29647":"markdown","7b3053c0":"markdown","2498800b":"markdown","e42af461":"markdown","8647adc5":"markdown","a6672958":"markdown","916b31f1":"markdown","635b2712":"markdown","1f4a68c6":"markdown","efb3c4bd":"markdown","85f2c954":"markdown","010aedd3":"markdown","a01bf849":"markdown","d9213c49":"markdown"},"source":{"9c9b57da":"# First, we need to install pycocotools. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n# The version by default in Colab has a bug fixed in https:\/\/github.com\/cocodataset\/cocoapi\/pull\/354\n!pip install cython\n!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","cfdccf9c":"import json\nimport math\nimport os\nimport random\nimport sys\nimport time\n\nimport numpy as np\nimport torch\nimport torch.utils.data\nimport torchvision\n\nfrom PIL import Image, ImageDraw\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom torchvision import transforms\nfrom torchvision.ops import batched_nms\nfrom torchvision.transforms import functional as F","ba3f13c4":"from typing import List, Tuple\n\nimport torchvision.models as models\n\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import MultiScaleRoIAlign","b7610a00":"\n# Download TorchVision repo to use some files from references\/detection\n!git clone https:\/\/github.com\/pytorch\/vision.git\n\n# copying files \n!cp .\/vision\/references\/detection\/utils.py \/kaggle\/working\n!cp .\/vision\/references\/detection\/coco_eval.py \/kaggle\/working","15ad2290":"import utils\nfrom coco_eval import CocoEvaluator","d3c3b0cb":"# check if cuda GPU is available, make sure you're using GPU runtime on Google Colab\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device) # you should output \"cuda\"","22923f57":"# Kaggle dataset\n\ntraining_path = \"..\/input\/dsta-brainhack-2021\/c1_release\/c1_release\"","cdaf5159":"class TILDataset(torch.utils.data.Dataset):\n    def __init__(self, root, annotation, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        self.coco = COCO(annotation)\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        cats = self.coco.loadCats(self.coco.getCatIds())\n        self.cat2name = {cat['id']:cat['name'] for cat in cats} # maps category id to category name (useful for visualization)\n\n    def __getitem__(self, index):\n        coco = self.coco\n        img_id = self.ids[index] # Image ID\n        ann_ids = coco.getAnnIds(imgIds=img_id) # get annotation id from coco\n        coco_annotation = coco.loadAnns(ann_ids) # target coco_annotation file for an image\n        path = coco.loadImgs(img_id)[0]['file_name'] # path for input image\n        img = Image.open(os.path.join(self.root, 'images', path)).convert('RGB') # open the input image\n\n        # number of objects in the image\n        num_objs = len(coco_annotation)\n\n        # Bounding boxes for objects\n        # In coco format, bbox = [xmin, ymin, width, height]\n        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n        boxes = []\n        for i in range(num_objs):\n            xmin = coco_annotation[i]['bbox'][0]\n            ymin = coco_annotation[i]['bbox'][1]\n            xmax = xmin + coco_annotation[i]['bbox'][2]\n            ymax = ymin + coco_annotation[i]['bbox'][3]\n            boxes.append([xmin, ymin, xmax, ymax])\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n\n        # Labels\n        labels = []\n        for i in range(num_objs):\n            labels.append(coco_annotation[i]['category_id'])\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        # Tensorise img_id\n        img_id = torch.tensor([img_id])\n\n        # Size of bbox (Rectangular)\n        areas = []\n        for i in range(num_objs):\n            areas.append(coco_annotation[i]['area'])\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Annotation is in dictionary format\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = img_id\n        target[\"area\"] = areas\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)","086393f1":"import os\nprint(os.listdir(\"..\/input\"))","aa80f387":"til_root = \"..\/input\/dsta-brainhack-2021\/c1_release\/c1_release\" # extracted training dataset path\ntrain_annotation = os.path.join(til_root, \"train.json\")\nprint(train_annotation==\"..\/input\/dsta-brainhack-2021\/c1_release\/c1_release\/train.json\")\nval_annotation = os.path.join(til_root, \"val.json\")\n\ndataset = TILDataset(til_root, train_annotation)\ndataset[30]","7df79f02":"source_img, img_annots = dataset[30]\ndraw = ImageDraw.Draw(source_img)\nfor i in range(len(img_annots[\"boxes\"])):\n    x1, y1, x2, y2 = img_annots[\"boxes\"][i]\n    label = int(img_annots[\"labels\"][i])\n\n    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n    text = f'{dataset.cat2name[label]}'\n    draw.text((x1+5, y1+5), text)\ndisplay(source_img)","c6417be2":"# hyper-parameters\n# change here for model\nparams = {'BATCH_SIZE': 32, # affect the most\n          'LR': 0.01, # affect\n          'OPTIM_MOMENTUM':0.9,\n          'CLASSES': 6,\n          'MAXEPOCHS': 15,\n          'BACKBONE': 'resnet101',\n          'FPN': True,\n          'ANCHOR_SIZE': ((32,), (64,), (128,), (256,), (512,)),\n          'ASPECT_RATIOS': ((0.5, 1.0, 2.0),),\n          'MIN_SIZE': 512,\n          'MAX_SIZE': 512,\n          'IMG_MEAN': [0.485, 0.456, 0.406],\n          'IMG_STD': [0.229, 0.224, 0.225],\n          'IOU_THRESHOLD': 0.5\n          }","2ec5da2a":"def get_resnet_backbone(backbone_name: str):\n    \"\"\"\n    Returns a resnet backbone pretrained on ImageNet.\n    Removes the average-pooling layer and the linear layer at the end.\n    \"\"\"\n    if backbone_name == 'resnet18':\n        pretrained_model = models.resnet18(pretrained=True, progress=False)\n        out_channels = 512\n    elif backbone_name == 'resnet34':\n        pretrained_model = models.resnet34(pretrained=True, progress=False)\n        out_channels = 512\n    elif backbone_name == 'resnet50':\n        pretrained_model = models.resnet50(pretrained=True, progress=False)\n        out_channels = 2048\n    elif backbone_name == 'resnet101':\n        pretrained_model = models.resnet101(pretrained=True, progress=False)\n        out_channels = 2048\n    elif backbone_name == 'resnet152':\n        pretrained_model = models.resnet152(pretrained=True, progress=False)\n        out_channels = 2048\n\n    backbone = torch.nn.Sequential(*list(pretrained_model.children())[:-2])\n    backbone.out_channels = out_channels\n\n    return backbone","12852061":"def get_resnet_fpn_backbone(backbone_name: str):\n    \"\"\"\n    Returns a specified ResNet backbone with FPN pretrained on ImageNet.\n    \"\"\"\n    return resnet_fpn_backbone(backbone_name, pretrained=True, trainable_layers=3)","accebc46":"def get_anchor_generator(anchor_size: Tuple[tuple] = None, aspect_ratios: Tuple[tuple] = None):\n    \"\"\"Returns the anchor generator.\"\"\"\n    if anchor_size is None:\n        anchor_size = ((16,), (32,), (64,), (128,))\n    if aspect_ratios is None:\n        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_size)\n\n    anchor_generator = AnchorGenerator(sizes=anchor_size,\n                                       aspect_ratios=aspect_ratios)\n    return anchor_generator","4d85e9ed":"def get_roi_pool(featmap_names: List[str] = None, output_size: int = 7, sampling_ratio: int = 2):\n    \"\"\"Returns the ROI Pooling\"\"\"\n    if featmap_names is None:\n        # default for resnet with FPN\n        featmap_names = ['0', '1', '2', '3']\n\n    roi_pooler = MultiScaleRoIAlign(featmap_names=featmap_names,\n                                    output_size=output_size,\n                                    sampling_ratio=sampling_ratio)\n\n    return roi_pooler","7481e659":"def get_fasterRCNN(backbone: torch.nn.Module,\n                   anchor_generator: AnchorGenerator,\n                   roi_pooler: MultiScaleRoIAlign,\n                   num_classes: int,\n                   image_mean: List[float] = [0.485, 0.456, 0.406],\n                   image_std: List[float] = [0.229, 0.224, 0.225],\n                   min_size: int = 512,\n                   max_size: int = 1024,\n                   **kwargs\n                   ):\n    \"\"\"Returns the Faster-RCNN model. Default normalization: ImageNet\"\"\"\n    model = FasterRCNN(backbone=backbone,\n                       rpn_anchor_generator=anchor_generator,\n                       box_roi_pool=roi_pooler,\n                       num_classes=num_classes,\n                       image_mean=image_mean,  # ImageNet\n                       image_std=image_std,  # ImageNet\n                       min_size=min_size,\n                       max_size=max_size,\n                       **kwargs\n                       )\n    model.num_classes = num_classes\n    model.image_mean = image_mean\n    model.image_std = image_std\n    model.min_size = min_size\n    model.max_size = max_size\n\n    return model","13fa1e6b":"def get_fasterRCNN_resnet(num_classes: int,\n                          backbone_name: str,\n                          anchor_size: List[float],\n                          aspect_ratios: List[float],\n                          fpn: bool = True,\n                          min_size: int = 512,\n                          max_size: int = 1024,\n                          **kwargs\n                          ):\n    \"\"\"Returns the Faster-RCNN model with resnet backbone with and without fpn.\"\"\"\n\n    # Backbone\n    if fpn:\n        backbone = get_resnet_fpn_backbone(backbone_name=backbone_name)\n    else:\n        backbone = get_resnet_backbone(backbone_name=backbone_name)\n\n    # Anchors\n    anchor_size = anchor_size\n    aspect_ratios = aspect_ratios * len(anchor_size)\n    anchor_generator = get_anchor_generator(anchor_size=anchor_size, aspect_ratios=aspect_ratios)\n\n    # ROI Pool\n    with torch.no_grad():\n        backbone.eval()\n        random_input = torch.rand(size=(1, 3, 512, 512))\n        features = backbone(random_input)\n\n    if isinstance(features, torch.Tensor):\n        from collections import OrderedDict\n\n        features = OrderedDict([('0', features)])\n\n    featmap_names = [key for key in features.keys() if key.isnumeric()]\n\n    roi_pool = get_roi_pool(featmap_names=featmap_names)\n\n    # Model\n    return get_fasterRCNN(backbone=backbone,\n                          anchor_generator=anchor_generator,\n                          roi_pooler=roi_pool,\n                          num_classes=num_classes,\n                          min_size=min_size,\n                          max_size=max_size,\n                          **kwargs)\n","7113e042":"model = get_fasterRCNN_resnet(num_classes=params['CLASSES'],\n                              backbone_name=params['BACKBONE'],\n                              anchor_size=params['ANCHOR_SIZE'],\n                              aspect_ratios=params['ASPECT_RATIOS'],\n                              fpn=params['FPN'],\n                              min_size=params['MIN_SIZE'],\n                              max_size=params['MAX_SIZE'],\n                              image_mean=params['IMG_MEAN'],\n                              image_std=params['IMG_STD'])","2a50287a":"# load pretrained weights for FasterRCNN ResNet50 FPN\npretrained_dict = torch.hub.load_state_dict_from_url('https:\/\/download.pytorch.org\/models\/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth', progress=True)\n# pretrained_dict = torch.hub.load_state_dict_from_url('https:\/\/download.pytorch.org\/models\/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth', progress=True)\nmodel_dict = model.state_dict()\n\n# filter out roi_heads.box_predictor weights\npretrained_dict = {k: v for k, v in pretrained_dict.items() if not k.startswith('roi_heads.box_predictor')}\n# overwrite entries in the existing state dict\nmodel_dict.update(pretrained_dict)\n# load the new state dict\nmodel.load_state_dict(model_dict)","75f1ab04":"# move model to the right device\nmodel.to(device)","f4059e60":"# construct an optimizer\nmodel_params = [p for p in model.parameters() if p.requires_grad]\n\n# using SGD\noptimizer = torch.optim.SGD(model_params, \n                            lr=params['LR'],\n                            momentum=0.9, \n                            weight_decay=0.0005)\n\n# Using Adam Optimizer\n# optimizer = torch.optim.Adam(model_params)\n\n\n\n# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","be4215af":"import albumentations as A\n# Albumentations\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","aac43a0f":"import albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# note to using albumentations, if you perform a transformation to the image,\n# you may need to do an appropriate transformation on the bounding box too\n# for example in the random horizontal flip function above, the bounding box is flipped using the below:\n# bbox = target[\"boxes\"]\n# bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n# target[\"boxes\"] = bbox\n\nclass Albumentations(object):\n    def __init__(self, prob=0):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        a_transform = A.Compose([\n#             A.RandomCrop(width=256, height=256),\n#             A.HorizontalFlip(p=0.5),\n#             A.CLAHE(),\n#             A.RandomRotate90(),\n#             A.Transpose(),\n#             A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n              A.Blur(blur_limit=3),\n#             A.OpticalDistortion(),\n#             A.GridDistortion(),\n#             A.HueSaturationValue(),\n#             A.RandomRotate90(),\n#             A.Flip(),\n#             A.Transpose(),\n#             A.OneOf([\n#                 A.IAAAdditiveGaussianNoise(),\n#                 A.GaussNoise(),\n#             ], p=0.2),\n#             A.OneOf([\n#                 A.MotionBlur(p=.2),\n#                 A.MedianBlur(blur_limit=3, p=0.1),\n#                 A.Blur(blur_limit=3, p=0.1),\n#             ], p=0.2),\n#             A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n#             A.OneOf([\n#                 A.OpticalDistortion(p=0.3),\n#                 A.GridDistortion(p=.1),\n#                 A.IAAPiecewiseAffine(p=0.3),\n#             ], p=0.2),\n#             A.OneOf([\n#                 A.CLAHE(clip_limit=2),\n#                 A.IAASharpen(),\n#                 A.IAAEmboss(),\n#                 A.RandomBrightnessContrast(),\n#             ], p=0.3),\n#             A.HueSaturationValue(p=0.3),\n#                 A.RandomBrightnessContrast(p=0.2),\n              ])\n        image = torch.tensor(a_transform(image=image.numpy())['image'])\n        return image, target","0b9fdfee":"class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target","286b1ffb":"# converts the image, a PIL image, into a PyTorch Tensor\nclass ToTensor(object):\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target","8d5f0a90":"# randomly horizontal flip the images and ground-truth labels\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n#         elif random.random() > self.prob:\n#             height, width = image.shape[-2:]\n#             image = image.flip(1)\n#             bbox = target[\"boxes\"]\n#             bbox[[0,2], :] = height - bbox[[2,0], :]\n#             target[\"boxes\"] = bbox\n        return image, target","f99ee19a":"# mirror images and ground-truth labels\nclass RandomMirror(object):\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.mirror()\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n        return image, target","347d6157":"def get_transform(train):\n    if train:\n        transforms = Compose([\n            ToTensor(), \n            RandomHorizontalFlip(0.5)\n        ])\n    else: # during evaluation, no augmentations will be done\n        transforms = Compose([\n            ToTensor()\n        ])\n    \n    return transforms","8970cb52":"NUM_WORKERS = 4\n\n# use our dataset and defined transformations\ntrain_dataset = TILDataset(til_root, train_annotation, get_train_transform())\nval_dataset = TILDataset(til_root, val_annotation, get_valid_transform())\n\n# define training and validation data loaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=NUM_WORKERS,\n    collate_fn=utils.collate_fn)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS,\n    collate_fn=utils.collate_fn)","52bab637":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. \/ 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses, **loss_dict)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])","58b6adef":"@torch.no_grad()\ndef evaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    torch.set_num_threads(1)\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Test:'\n\n    coco = data_loader.dataset.coco\n    iou_types = [\"bbox\"]\n    coco_evaluator = CocoEvaluator(coco, iou_types)\n\n    for image, targets in metric_logger.log_every(data_loader, 100, header):\n        image = list(img.to(device) for img in image)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        outputs = model(image)\n\n        outputs = [{k: v for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n        evaluator_time = time.time()\n        coco_evaluator.update(res)\n        evaluator_time = time.time() - evaluator_time\n        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    coco_evaluator.synchronize_between_processes()\n\n    # accumulate predictions from all images\n    coco_evaluator.accumulate()\n    coco_evaluator.summarize()\n    torch.set_num_threads(n_threads)\n    return coco_evaluator\n","a9d66837":"for epoch in range(params['MAXEPOCHS']):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n\n    # update the learning rate\n    lr_scheduler.step() \n\n    # evaluate on the test dataset\n    evaluate(model, val_loader, device=device)\n    \n#     for i, data in enumerate(trainloader, 0):\n#         running_loss =+ loss.item() * images.size(0)\n\n#     loss_values.append(running_loss \/ len(train_dataset))\n","98010803":"# plot number of epochs (x axis) vs. accuracy (y axis). \nplt.plot(loss_values)","0d92791a":"# pick one image from the validation set\nimg, _ = val_dataset[391]\n\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])\n\nprediction","ddab0fcb":"# convert the image, which has been rescaled to 0-1 and had the channels flipped\npred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\ndraw = ImageDraw.Draw(pred_img)\n\nimg_preds = prediction[0]\nfor i in range(len(img_preds[\"boxes\"])):\n    x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n    label = int(img_preds[\"labels\"][i])\n    score = float(img_preds[\"scores\"][i])\n\n    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n    text = f'{dataset.cat2name[label]}: {score}'\n    draw.text((x1+5, y1+5), text)\n\ndisplay(pred_img)","a6526f03":"# img_preds = prediction[0]\n# keep_idx = batched_nms(boxes=img_preds[\"boxes\"], scores=img_preds[\"scores\"], idxs=img_preds[\"labels\"], iou_threshold=params['IOU_THRESHOLD'])","46e7d104":"# # convert the image, which has been rescaled to 0-1 and had the channels flipped\n# pred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n# draw = ImageDraw.Draw(pred_img)\n\n# for i in range(len(img_preds[\"boxes\"])):\n#     if i in keep_idx:\n#         x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n#         label = int(img_preds[\"labels\"][i])\n#         score = float(img_preds[\"scores\"][i])\n\n#         draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n#         text = f'{dataset.cat2name[label]}: {score}'\n#         draw.text((x1+5, y1+5), text)\n\n# display(pred_img)","a76bd7c0":"# det_threshold = 0.5\n\n# # convert the image, which has been rescaled to 0-1 and had the channels flipped\n# pred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n# draw = ImageDraw.Draw(pred_img)\n\n# for i in range(len(img_preds[\"boxes\"])):\n#     if i in keep_idx:\n#         x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n#         label = int(img_preds[\"labels\"][i])\n#         score = float(img_preds[\"scores\"][i])\n\n#         # filter out non-confident detections\n#         if score > det_threshold:\n#             draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n#             text = f'{dataset.cat2name[label]}: {score}'\n#             draw.text((x1+5, y1+5), text)\n\n# display(pred_img)","4ce9529d":"# save model weights\nbase_folder = \".\/\"\nsave_path = os.path.join(base_folder, \"c1_weights.pth\")\ntorch.save(model.state_dict(), save_path)","978c75eb":"# load model weights\nbase_folder = \".\/\"\nsave_path = os.path.join(base_folder, \"c1_weights.pth\")\n# model = TheModelClass(*args, **kwargs)\nmodel.load_state_dict(torch.load(save_path))\nmodel.eval()","2a40ec19":"with open(val_annotation) as json_file:\n    val_data = json.load(json_file)\n\nmodel.eval()\ndetections = []\nwith torch.no_grad():\n    for image in val_data[\"images\"]:\n        img_name = image[\"file_name\"]\n        img_id = image[\"id\"]\n\n        img = Image.open(os.path.join(til_root, 'images', img_name)).convert('RGB')\n        img_tensor = transforms.ToTensor()(img)\n\n        preds = model([img_tensor.to(device)])[0]\n\n        for i in range(len(preds[\"boxes\"])):\n            x1, y1, x2, y2 = preds[\"boxes\"][i]\n            label = int(preds[\"labels\"][i])\n            score = float(preds[\"scores\"][i])\n\n            left = int(x1)\n            top = int(y1)\n            width = int(x2 - x1)\n            height = int(y2 - y1)\n\n            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})","909a8ca0":"validation_json = os.path.join(base_folder, \"validation_preds.json\")\nwith open(validation_json, 'w') as f:\n    json.dump(detections, f)","407c1436":"# Get evaluation score against validation set to make sure your prediction json file is in the correct format\ncoco_gt = COCO(val_annotation)\ncoco_dt = coco_gt.loadRes(validation_json)\ncocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()","ecfbeec2":"til_test_root = \"..\/input\/dsta-brainhack-2021\/c1_test_release\/c1_test_release\" # extracted testing images path\ntest_img_root = os.path.join(til_test_root, \"images\")\nimg_dir = os.scandir(test_img_root)","5a3b4cbe":"# load model weights (if not using the current trained model)\nmodel.load_state_dict(torch.load(save_path, map_location=device))\nmodel.to(device)\nmodel.eval()","47c7bbdf":"img = Image.open(next(img_dir).path).convert('RGB')\ndraw = ImageDraw.Draw(img)\ndet_threshold = 0.5\n\n# do the prediction\nwith torch.no_grad():\n    img_tensor = transforms.ToTensor()(img)\n    img_preds = model([img_tensor.to(device)])[0]\n\nfor i in range(len(img_preds[\"boxes\"])):\n    x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n    label = int(img_preds[\"labels\"][i])\n    score = float(img_preds[\"scores\"][i])\n\n    # filter out non-confident detections\n    if score > det_threshold:\n        draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n        text = f'{dataset.cat2name[label]}: {score}'\n        draw.text((x1+5, y1+5), text)\n\ndisplay(img)","15559b76":"# generate detections on the folder of test images (this will be used for submission)\ndetections = []\nwith torch.no_grad():\n    for image in img_dir:\n        img_id = int(image.name.split('.')[0])\n\n        img = Image.open(image.path).convert('RGB')\n        img_tensor = transforms.ToTensor()(img)\n\n        preds = model([img_tensor.to(device)])[0]\n\n        for i in range(len(preds[\"boxes\"])):\n            x1, y1, x2, y2 = preds[\"boxes\"][i]\n            label = int(preds[\"labels\"][i])\n            score = float(preds[\"scores\"][i])\n\n            left = int(x1)\n            top = int(y1)\n            width = int(x2 - x1)\n            height = int(y2 - y1)\n\n            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})","c28a33d6":"test_pred_json = os.path.join(base_folder, \"test_preds.json\")\nwith open(test_pred_json, 'w') as f:\n    json.dump(detections, f)","624cb33c":"sample_json_path = os.path.join(til_test_root, \"c1_test_sample.json\")\n\ncoco_gt = COCO(sample_json_path)\ncoco_dt = coco_gt.loadRes(test_pred_json)\ncocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()","b0f9345e":"## Data Augmentation\n\nLet's write some helper functions for data augmentation \/ transformation.\n\nDo not just stop here, add in your own data augmentations! Remember to also augment the bounding boxes accordingly.","edc85edc":"### Defining the Dataset\n\nThe [torchvision reference scripts for training object detection](https:\/\/github.com\/pytorch\/vision\/tree\/v0.3.0\/references\/detection) allows for easily supporting adding new custom datasets.\nThe dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n\nThe only specificity that we require is that the dataset `__getitem__` should return:\n\n* image: a PIL Image of size (H, W)\n* target: a dict containing the following fields\n    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n\nIf your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n\nAdditionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n\nLet's write a `torch.utils.data.Dataset` class for this dataset.","1599572c":"Let's visualize some predictions on the test images. Run this a few times to visualize different images.","d7064f14":"## Data Loaders\n\nLet's now set up our data loaders so that we can streamline the batch loading of data for our model training later on.\n\nWe now have the dataset class, the models and the data transforms. Let's instantiate them","8f488c27":"So we can see that by default, the dataset returns a `PIL.Image` and a dictionary containing several fields, including `boxes` and `labels`.\n\nLet's next look at one example from the training set.","5b41ba74":"## Generate Predictions on Test Images","a57178d1":"That's all for the dataset. Let's see how the outputs are structured for this dataset","81bf4845":"## Check Your Submission JSON Format (RUN THIS BEFORE SUBMISSION)\n\nRun this function first with the given sample json (change this path for the different challenges) to make sure everything works before you submit. Just need to ensure that there are no errors when you run this. **If you get any errors, check your generated json file.**","e4726fc1":"## Setting up the Model\n\nIn this object detection example, we will make use of Faster R-CNN model with a ResNet50-FPN backbone. To understand the underlying code structure, you can read this [article](https:\/\/zhuanlan.zhihu.com\/p\/145842317) (right click and translate to English).\n\nFeel free to explore with different hyper-parameters to see what works best!","04afdb4a":"## Downloading Dependencies","fea8657b":"## Model Training\n\nAnd now let's train the model, evaluating at the end of every epoch.","8bd29647":"Now, let's further filter out the non-confident detections.","7b3053c0":"Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\nThe dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, and `scores` as fields.\n\nLet's inspect the image and the predicted boxes.\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.","2498800b":"## Evaluation on Validation Set\n\nAs a sanity check, let's evaluate the model performance on the validation set","e42af461":"## Submission of Results\n\nSubmission json file should be in [COCO format](https:\/\/cocodataset.org\/#format-results).\n\n```\n[{\n    \"image_id\": int, \n    \"category_id\": int, \n    \"bbox\": [x,y,width,height], \n    \"score\": float,\n}]\n```\n\nRefer to **sample_submission_cv.json** for an example.\n\nFor this competition, the metric for evaluation will be mAP @ 0.50:0.95","8647adc5":"In `references\/detection\/,` we have a number of helper functions to simplify training and evaluating detection models.\nHere, we will use `references\/detection\/utils.py` and `references\/detection\/coco_eval.py`.\n\nLet's copy those files (and their dependencies) in here so that they are available in the notebook","a6672958":"Let's have a look at the dataset and how it is layed down.\n\nThe data is structured as follows\n```\nc1_release\/\n  images\/\n    0001e6adc4fbab0c.jpg\n    00067fe83e3e21c8.jpg\n    0008ab3d8674f6ca.jpg\n    ...\n  labels.json\n  train.json\n  val.json\n```\n\n`labels.json` contains the labels for the whole dataset (`train.json` + `val.json`) if you need it.","916b31f1":"Much better! Once you are satisfied with the results, save your model weights.","635b2712":"## Object Detection Dataset\nWe will be providing the base dataset that will be used for the first task of the Object Detection competition.","1f4a68c6":"Check the predictions again after applying nms.\n\n** Update: You should not see any difference unless you have specified a lower IoU threshold than the default of 0.5.","efb3c4bd":"Note that we do not need to add a mean\/std normalization nor image rescaling in the data transforms, as those are handled by the R-CNN model.","85f2c954":"## Visualization of results\n\nNow that training has finished, let's have a look at what it actually predicts.","010aedd3":"If you get an `AssertionError: Results do not correspond to current coco set`, it most likely means that some of the \"image_id\" are out of range (either 0 or higher than number of test images).","a01bf849":"# EPOCH TRAINING","d9213c49":"## Post-processing\n\nWe might notice that there are duplicate detections in the image. Let's post-process the detections with non-maximum suppression.\n\n** Update: FasterRCNN already has NMS built into it, so you actually do not need to do NMS again."}}