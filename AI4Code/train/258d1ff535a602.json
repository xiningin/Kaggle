{"cell_type":{"75afb3f7":"code","901dc6fa":"code","0e4232b5":"code","e24cb082":"code","8e38be43":"code","c995e7df":"code","3aa932db":"code","c1e9d501":"code","0ae72129":"code","b5c3d036":"code","e1f46613":"code","16d69752":"code","819968c2":"code","45eed589":"code","eaf0d4ca":"code","e86b21a9":"code","57729765":"code","230358f7":"code","dee2677d":"code","aa37cbbf":"code","fc3ee50b":"code","0037cc97":"code","dbbe52f7":"code","d1795260":"markdown","969febcb":"markdown","dd709f40":"markdown","5abe17b8":"markdown","ec483301":"markdown","6050d956":"markdown","457398bb":"markdown","5f28d893":"markdown","91b5e2ae":"markdown","e30219bc":"markdown"},"source":{"75afb3f7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#Models import\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss \n\n#plot imports\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nsns.set(style='white', context='notebook', palette='deep')\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n#constants\nnp.random.seed(0)\nNFOLDS=5\nSEED=0","901dc6fa":"original_train=pd.read_csv(\"..\/input\/cardiovascular-study-dataset-predict-heart-disea\/train.csv\")\noriginal_train=original_train.drop(['id'], axis=1) #drop 'id'\n\noriginal_test=pd.read_csv(\"..\/input\/cardiovascular-study-dataset-predict-heart-disea\/test.csv\")\nIDtest=original_test['id']\noriginal_test=original_test.drop(['id'], axis=1) #drop 'id'\n","0e4232b5":"#convert binary values to 0 and 1\ndef give_binary_values(dataset,col,one,zero): \n    dataset[col].loc[dataset[col]==one]=1\n    dataset[col].loc[dataset[col]==zero]=0\n    return dataset\n\n#fill missing values\ndef handle_missing(dataset): \n    #convert sex and is_smoking to numerical category\n    dataset=give_binary_values(dataset,'sex','F','M')\n    dataset=give_binary_values(dataset,'is_smoking','YES','NO')\n    \n    \n    dataset['education']= dataset['education'].fillna(dataset['education'].median())\n\n    dataset.loc[dataset['is_smoking']==0,'cigsPerDay']=0\n\n\n    for i in range(2):\n        correct_sex=(dataset['sex'] == i)\n        for col in ['heartRate','BMI','totChol']:\n            avg=dataset[col][correct_sex].mean()\n            dataset.loc[correct_sex, col]=dataset.loc[correct_sex, col].fillna(avg)\n        for col in ['BPMeds','cigsPerDay','glucose']:\n            med=dataset[col][correct_sex].median()\n            dataset.loc[correct_sex, col]=dataset.loc[correct_sex, col].fillna(med)\n        dataset.loc[dataset[\"is_smoking\"]==0,\"cigsPerDay\"]=0\n    return dataset","e24cb082":"#fill missing values in the original files of train and test\noriginal_train=handle_missing(original_train)\noriginal_test=handle_missing(original_test)\n\noriginal_train=original_train.drop(['is_smoking'], axis=1) #drop 'is_smoking'\noriginal_test=original_test.drop(['is_smoking'], axis=1) #drop 'is_smoking'","8e38be43":"#devision features into groups\nnumerical_features=[\"age\",\"education\",\"cigsPerDay\",\"BPMeds\",\"diabetes\",\"sysBP\",\"diaBP\",\"BMI\",\"heartRate\",\"glucose\"] #numerical features\ncategorical_features=[\"sex\",\"prevalentStroke\",\"prevalentStroke\",\"prevalentHyp\"] #categorical features\nnumerical_dataset=original_train.drop(categorical_features,axis=1) #partial dataset that includes onley the numerical features\ncategorical_dataset=original_train.drop(numerical_features,axis=1) #partial dataset that includes onley the categorical features","c995e7df":"def preset_correlation_of_Xy(dataset,title): #present correlation matrix of features and lable in a dataset \n    f,ax = plt.subplots(figsize=(24,20))\n    corr=dataset.corr()    \n    sns.heatmap(corr, cmap='coolwarm_r', annot=True, annot_kws={'size':20}, ax=ax)\n    ax.set_title(title, fontsize=20)\n    plt.show()\n    \n","3aa932db":"#present correlation matrix of numerical features and lable \npreset_correlation_of_Xy(numerical_dataset,\"corelation matrix - numerical features\")\n","c1e9d501":"#present correlation matrix of categorical features and lable \npreset_correlation_of_Xy(categorical_dataset,\"corelation matrix - categorical features\")","0ae72129":"#according the correlation matrices above, it can be seen that 'diabetes' is quite correlated with 'glucose' and 'diaBP' is quite correlated with 'sysBP'.\n#Therefore 'diabetes' and 'diaBP' are dropped from the original train and test files\n\n\noriginal_train=original_train.drop(['diabetes','diaBP'], axis=1) #drop 'diabetes','diaBP' from train file\noriginal_train.head(10)","b5c3d036":"original_test=original_test.drop(['diabetes','diaBP'], axis=1) #drop 'diabetes','diaBP' from test file\noriginal_test.head(10)","e1f46613":"y_on_train = original_train['TenYearCHD'].to_frame()\nX_on_train = original_train.drop(['TenYearCHD'], axis=1)\nX_train_on_train, X_test_on_train, y_train_on_train, y_test_on_train = train_test_split(X_on_train, y_on_train, test_size=0.2, random_state=42)\nkf = KFold( n_splits= NFOLDS, random_state=SEED, shuffle=False)","16d69752":"# Modeling step Test differents algorithms \nrandom_state = 42\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state,warm_start=True))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state,warm_start=True))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state,warm_start=True))\nclassifiers.append(MLPClassifier(random_state=random_state,warm_start=True))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state,warm_start=True))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train_on_train, y = y_train_on_train, scoring = \"accuracy\", cv = kf, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})    \ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\nplt.show()","819968c2":"\ndef get_oof(clf, x_train, y_train, x_test,kf):\n    oof_train = np.zeros((x_train.shape[0],))\n    oof_test = np.zeros((x_test.shape[0],))\n    oof_test_skf = np.empty((NFOLDS, x_test.shape[0]))\n    i=0\n    \n    for (train_index, test_index) in kf.split(x_train):\n        x_tr = x_train.iloc[train_index]\n        y_tr = y_train.iloc[train_index]\n        x_te = x_train.iloc[test_index]\n        clf.fit(x_tr.values, y_tr.values.ravel())\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n        i+=1\n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","45eed589":"# Create 5 objects that represent our 4 models\nrf=RandomForestClassifier(n_estimators=1000,max_depth=6,min_samples_leaf=2,max_features='sqrt',n_jobs=-1,random_state=0,verbose=0,warm_start=True)\net = ExtraTreesClassifier(n_estimators=1000,max_depth=8,min_samples_leaf=2,n_jobs=-1,random_state=0,verbose=0,warm_start=True)\nada =AdaBoostClassifier(n_estimators=1000,learning_rate=0.75,random_state=0)\ngb = GradientBoostingClassifier(n_estimators=1000,max_depth=5,min_samples_leaf=2,random_state=0,verbose=0,warm_start=True)\nsvc = SVC(kernel='linear',C=0.025)","eaf0d4ca":"# Create Numpy arrays of train, test and target (TenYearCHD) dataframes to feed into our models\ny_train=original_train['TenYearCHD'].to_frame()\nX_train=original_train.drop(['TenYearCHD'], axis=1)\n\n#Balance by oversampling by SMOTE\nsm=SMOTE()\nX_train, y_train = sm.fit_sample(X_train, y_train)\n\nX_test=original_test","e86b21a9":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test, kf) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test, kf) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test, kf) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test, kf) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,X_train, y_train, X_test, kf) # Support Vector Classifier","57729765":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head(10)","230358f7":"data = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","dee2677d":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1).astype(int)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1).astype(int)\n","aa37cbbf":"x_train","fc3ee50b":"x_test","0037cc97":"def feature_importances(clf,x,y):\n        print(clf.fit(x,y).feature_importances_)\n        \nrf_feature = feature_importances(rf,X_train,y_train)\net_feature = feature_importances(et, X_train, y_train)\nada_feature = feature_importances(ada, X_train, y_train)\ngb_feature = feature_importances(gb, X_train,y_train)\n\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\ngbm.fit(x_train, y_train)\npredictions = gbm.predict(x_test)","dbbe52f7":"# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'id': IDtest,\n                            'TenYearCHD': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)","d1795260":"> # Modeling","969febcb":"# Out-of-Fold Predictions","dd709f40":"# Imports","5abe17b8":"# Aim\nThe aim is to predict whether patient have 10 year risk of coronary heart disease CHD or not. Additionally, participants also asked to create some data visualization about the data to gained actionable insight about the topic.","ec483301":"# Submission","6050d956":"# Load Data","457398bb":"# Second-Level Predictions from the First-level Output\n\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.","5f28d893":"# Cross Correlation\nChecking correlation between features","91b5e2ae":"Correlation Heatmap of the Second Level Training set\n","e30219bc":"**Second level learning model via XGBoost\n**\n\n\n\nI choose XGBoost in order to built to optimize large-scale boosted tree algorithms. I call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:"}}