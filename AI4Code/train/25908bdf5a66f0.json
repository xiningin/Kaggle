{"cell_type":{"2e65b27d":"code","0f1fd9ba":"code","d609be60":"code","8634a1ef":"code","36819887":"code","2db14620":"code","824d3bf2":"code","75595c30":"code","722ddafb":"code","fabd9928":"code","d9e3dc3b":"code","72432685":"code","8567e7fa":"markdown","0670ca7e":"markdown","7989a5c7":"markdown","85862c4e":"markdown","70597ac6":"markdown","111142de":"markdown","935fda0d":"markdown","6c9f7d28":"markdown","412ca5cb":"markdown"},"source":{"2e65b27d":"import os\nHAM_DIR = \"..\/input\/ham-and-spam-dataset\/hamnspam\/ham\/\"\nSPAM_DIR = \"..\/input\/ham-and-spam-dataset\/hamnspam\/spam\/\"\nham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\nspam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]","0f1fd9ba":"print('Number of ham files:' , len(ham_filenames) )\nprint('Number of spam files:' , len(spam_filenames) )","d609be60":"import email\nimport email.policy\n\ndef load_email( is_spam, filename ):\n    directory = SPAM_DIR if is_spam else HAM_DIR\n    with open(os.path.join( directory, filename ), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n\nham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]","8634a1ef":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = np.array(ham_emails + spam_emails)\ny = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","36819887":"import re\nfrom html import unescape\n\ndef html_to_plain_text(html):\n    text = re.sub('<head.*?>.*?<\/head>', '', html, flags=re.M | re.S | re.I)\n    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n    return unescape(text)\n\ndef email_to_text(email):\n    html = None\n    for part in email.walk():\n        ctype = part.get_content_type()\n        if not ctype in (\"text\/plain\", \"text\/html\"):\n            continue\n        try:\n            content = part.get_content()\n        except: # in case of encoding issues\n            content = str(part.get_payload())\n        if ctype == \"text\/plain\":\n            return content\n        else:\n            html = content\n    if html:\n        return html_to_plain_text(html)","2db14620":"import nltk\nstemmer = nltk.PorterStemmer()\n\nfrom collections import Counter\n\nimport re\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n                 replace_urls=True, replace_numbers=True, stemming=True):\n        self.strip_headers = strip_headers\n        self.lower_case = lower_case\n        self.remove_punctuation = remove_punctuation\n        self.replace_urls = replace_urls\n        self.replace_numbers = replace_numbers\n        self.stemming = stemming\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_transformed = []\n        for email in X:\n            text = email_to_text(email) or \"\"\n            if self.lower_case:\n                text = text.lower()\n            if self.replace_urls:\n                urls = list(set(re.findall( r'(https?:\/\/\\S+)' , text )))\n                urls.sort(key=lambda url: len(url), reverse=True)\n                for url in urls:\n                    text = text.replace(url, \" URL \")\n            if self.replace_numbers:\n                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n            if self.remove_punctuation:\n                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n            word_counts = Counter(text.split())\n            if self.stemming and stemmer is not None:\n                stemmed_word_counts = Counter()\n                for word, count in word_counts.items():\n                    stemmed_word = stemmer.stem(word)\n                    stemmed_word_counts[stemmed_word] += count\n                word_counts = stemmed_word_counts\n            X_transformed.append(word_counts)\n        return np.array(X_transformed)","824d3bf2":"from scipy.sparse import csr_matrix\n\nclass WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_count[word] += min(count, 10)\n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.most_common_ = most_common\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n    def transform(self, X, y=None):\n        rows = []\n        cols = []\n        data = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(count)\n        solu = csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n        return solu","75595c30":"from sklearn.pipeline import Pipeline\n\npreprocess_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n])\n\nX_train_transformed = preprocess_pipeline.fit_transform(X_train)","722ddafb":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nscore = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\nprint('\\nScores for all folds: ', score )\nprint('\\nAverage Score: ', score.mean() )\nprint('\\nStandard deviation of Scores: ', score.std() )","fabd9928":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\n\ny_scores = cross_val_predict(log_clf, X_train_transformed, y_train, cv=3, method=\"decision_function\")\n\nfrom sklearn.metrics import precision_recall_curve\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)","d9e3dc3b":"import matplotlib.pyplot as plt\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n    plt.xlabel(\"Threshold\", fontsize=16)\n    plt.legend(loc=\"upper left\", fontsize=16)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(8, 4))\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","72432685":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n\nplt.figure(figsize=(8, 6))\nplot_precision_vs_recall(precisions, recalls)\nplt.show()","8567e7fa":"After defining the preprocessing functions, the data are run through the following data transformation steps.","0670ca7e":"The email objects are collected via email parser library.","7989a5c7":"The precision-recall vs. threshold and the precision-recall plots are presented respectively.","85862c4e":"The transformed test dataset is now used to make new predictions. Precision, recall, thresholds, and scores will be computed accordingly.","70597ac6":"Using the following functions, the emails' content are preprocesssed, making certain that the resulting texts do not contain html codes, upper case letters, digit numbers, or punctuations.","111142de":"The following functions computes the most frequent vocabularies in the body of ham emails and determines their frequency in each email. Transformed emails will be vectors of numbers.","935fda0d":"The transformed emails are fed to a logistic regression classifier. The data are trained using *k*-folds cross-validation technique.","6c9f7d28":"In this study, we are training a logistic regression classifier on a set of ham and spam emails to flag spam emails. The cross-validation method is implemented to obtain a better evaluation of the model performance. In the first step, the data are fetched.","412ca5cb":"The main dataset is split into training and test datasets."}}