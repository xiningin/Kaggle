{"cell_type":{"a96b23aa":"code","678ced42":"code","fa42d965":"code","f8fdedd6":"code","9ac3fadf":"code","6003c202":"code","bb9d8be4":"code","31648ac8":"code","d5097b51":"code","f91487a7":"code","7b739a81":"code","e597787b":"code","216534ee":"code","a0daa673":"code","6ca9db07":"code","ad5ad472":"code","11951bca":"markdown","79841c49":"markdown","71ed2a87":"markdown","a23b942e":"markdown","38bced77":"markdown","2cea90be":"markdown","c87cad7a":"markdown","f5cdafad":"markdown","3f9b70d3":"markdown","6e277ac0":"markdown","7fd8b992":"markdown","8b2c6095":"markdown","3ced046e":"markdown","a781f406":"markdown","7398ca4e":"markdown","f4ecd883":"markdown","82d49d98":"markdown","16ae2bfb":"markdown","33eaf275":"markdown","a8d904ca":"markdown","c3d574fc":"markdown","4003fdb7":"markdown","66b52b64":"markdown","cd30ea48":"markdown","839ca865":"markdown","8702e20c":"markdown","37104df3":"markdown"},"source":{"a96b23aa":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom datetime import date\nimport matplotlib\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn import metrics\nfrom sklearn.mixture import GaussianMixture\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_folder = \"\/kaggle\/input\/arketing-campaign\/\"","678ced42":"data=pd.read_csv(data_folder+'marketing_campaign.csv',header=0,sep=';') \ndata.head(10)","fa42d965":"#Spending variable creation\ndata['Spending']=data['MntWines']+data['MntFruits']+data['MntMeatProducts']+data['MntFishProducts']+data['MntSweetProducts']+data['MntGoldProds']\n#Seniority variable creation\nlast_date = date(2014,10, 4)\ndata['Seniority']=pd.to_datetime(data['Dt_Customer'], dayfirst=True,format = '%Y-%m-%d')\ndata['Seniority'] = pd.to_numeric(data['Seniority'].dt.date.apply(lambda x: (last_date - x)).dt.days, downcast='integer')\/30\n\ndataset=data[['Income','Spending','Seniority']]","f8fdedd6":"pd.options.display.float_format = \"{:.2f}\".format\ndataset.describe()","9ac3fadf":"#Remove rows with missing values\ndataset=dataset.dropna(subset=['Income'])\n\n#Remove the only outlier in the dataset\ndataset=dataset[dataset['Income']<600000]\ndataset.describe()","6003c202":"nd = pd.melt(dataset, value_vars =dataset)\nn1 = sns.FacetGrid (nd, col='variable', col_wrap=5, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\nn1","bb9d8be4":"scaler=StandardScaler()\ndataset=dataset[['Income','Seniority','Spending']]\n\nX_std=scaler.fit_transform(dataset)\nX = normalize(X_std,norm='l2') ","31648ac8":"df = pd.DataFrame(data=X, columns=['Income','Spending','Seniority'])\nnd = pd.melt(df, value_vars =df )\nn1 = sns.FacetGrid (nd, col='variable', col_wrap=5, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\nn1","d5097b51":"Covariance=['full','tied','diag','spherical']\nnumber_clusters=np.arange(1,21)\nresults_=pd.DataFrame(columns=['Covariance type','Number of Clusters','Silhouette Score','Davies Bouldin Score'])\nfor i in Covariance:\n    for n in number_clusters:       \n        gmm_cluster=GaussianMixture(n_components=n,covariance_type=i,random_state=5)\n        clusters=gmm_cluster.fit_predict(X)\n        if len(np.unique(clusters))>=2:\n            results_=results_.append({\"Covariance type\":i,'Number of Clusters':n,\"Silhouette Score\":metrics.silhouette_score(X,clusters),'Davies Bouldin Score':metrics.davies_bouldin_score(X,clusters)},ignore_index=True)\n\ndisplay(results_.sort_values(by=[\"Silhouette Score\"], ascending=False)[:10])","f91487a7":"sns.set()\nnumber_clusters = np.arange(1, 10)\nmodels = [GaussianMixture(n, covariance_type='spherical',max_iter=2000, random_state=5).fit(X) for n in number_clusters]\nplt.plot(number_clusters, [m.bic(X) for m in models], label='BIC')\nplt.plot(number_clusters, [m.aic(X) for m in models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('number_clusters')","7b739a81":"gmm=GaussianMixture(n_components=4, covariance_type='spherical',max_iter=2000, random_state=5).fit(X)\nlabels = gmm.predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis');","e597787b":"proba = gmm.predict_proba(X)\nprint(proba[:].round(2))","216534ee":"dataset['Cluster'] = labels\n\nProbability=pd.DataFrame(proba.max(axis=1))\ndataset = dataset.reset_index().merge(Probability, left_index=True, right_index=True)\ndataset=dataset.rename(columns={0: \"Probability\"}).drop(columns=['index'])\ndataset","a0daa673":"pd.options.display.float_format = \"{:.0f}\".format\nsummary=dataset[['Income','Spending','Seniority','Cluster']]\nsummary.set_index(\"Cluster\", inplace = True)\nsummary=summary.groupby('Cluster').describe().transpose()\nsummary","6ca9db07":"#Rename clusters\ndataset=dataset.replace({0:'Stars',1:'Need attention',2:'High potential',3:'Leaky bucket'})","ad5ad472":"PLOT = go.Figure()\nfor C in list(dataset.Cluster.unique()):\n    \n\n    PLOT.add_trace(go.Scatter3d(x = dataset[dataset.Cluster == C]['Income'],\n                                y = dataset[dataset.Cluster == C]['Seniority'],\n                                z = dataset[dataset.Cluster == C]['Spending'],                        \n                                mode = 'markers',marker_size = 6, marker_line_width = 1,\n                                name = str(C)))\nPLOT.update_traces(hovertemplate='Income: %{x} <br>Seniority: %{y} <br>Spending: %{z}')\n\n    \nPLOT.update_layout(width = 850, height = 850, autosize = True, showlegend = True,\n                   scene = dict(xaxis=dict(title = 'Income', titlefont_color = 'black'),\n                                yaxis=dict(title = 'Seniority', titlefont_color = 'black'),\n                                zaxis=dict(title = 'Spending', titlefont_color = 'black')),\n                   font = dict(family = \"Gilroy\", color  = 'black', size = 12))","11951bca":"### C. Clusters interpretation <a class=\"anchor\" id=\"section_2_3\"><\/a>","79841c49":"The clusters are equally weighted :\n- __Cluster 0__ is composed of __old customers__ with __high income__ and __high spending amount__<br>\n- __Cluster 1__ is composed of __new customers__ with __below average income__ and __small spending amount__<br>\n- __Cluster 2__ is composed of __new customers__ with __high income__ and __high spending amount__<br>\n- __Cluster 3__ is composed of __old customers__ with __below average income__  and __small spending amount__<br>","71ed2a87":"Having a first look at the row data enables us to start thinking at some useful variables we could create in order to better understand our dataset and choose the features to cluster the customers.  \n\nWe wrill create two variables :\n\n>- Variable __*Spending*__ as the sum of the amount spent on the 6 product categories\n>- Variable __*Seniority*__ as the number of months the customer is enrolled with the company\n\nWe will remove the unused variables for this analysis","a23b942e":"### B. Statistical summary <a class=\"anchor\" id=\"section_1_2\"><\/a>","38bced77":"We can see below the probability for each observation to belong to each cluster","2cea90be":"Our variables do not have the same units. we need to normalize them.  \nMoreover, we saw from my previous Notebook that _Income_ Variable has both __outliers__ and __missing value__ <br>\n\nLink to my previous Notebook : https:\/\/www.kaggle.com\/raphael2711\/data-prep-visual-eda-and-statistical-hypothesis","c87cad7a":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#005097; border:0' role=\"tab\" aria-controls=\"home\"><center>Customer Segmentation <\/center><\/h1>","f5cdafad":">We will select the Covariance type and number of cluster where :\n - The Silhouette score is maximized <br>\n - The Davies Bouldin score is minimized <br>\n>\n>We choose the __spherical__ covariance type with __4__ clusters","3f9b70d3":"# 2. Clustering Algorithm <a class=\"anchor\" id=\"section_2\"><\/a>","6e277ac0":">We can see from the BIC Score curve a decline improvement at cluster __n=4__.<br> We therefore validate our choice in order to keep a manageable number of clusters.","7fd8b992":"### Table of Contents\n\n* [Data Preprocessing](#section_1)\n    * [Feature Engineering](#section_1_1)\n    * [Statistical summary](#section_1_2)\n    * [Outliers and missing values treatment](#section_1_3)\n    * [Data normalization](#section_1_4)\n    ___\n* [Clustering Algorithm](#section_2)\n    * [Number of clusters selection](#section_2_1)\n    * [Clusters creation](#section_2_2)\n    * [Clusters interpretation](#section_2_3)\n    * [Clusters visualization](#section_2_4)\n    \n    ---\nLink to my previous Notebook : https:\/\/www.kaggle.com\/raphael2711\/data-prep-visual-eda-and-statistical-hypothesis","8b2c6095":"We fit and predit the data specifying the __number of clusters__ and the __covariance type__  ","3ced046e":"### A. Number of clusters selection <a class=\"anchor\" id=\"section_2_1\"><\/a>","a781f406":"# 1. Data Preprocessing <a class=\"anchor\" id=\"section_1\"><\/a>","7398ca4e":"We define the number of clusters using the Silhoutte score and Davies Bouldin score","f4ecd883":">We use __*Standard Scaler*__ to transform column features by removing the mean and scale to unit variance.<br>\nWe use __*Normalize*__ to rescale each row independently of other rows so that its norm equals one.","82d49d98":"We associate to each customer the cluster with the highest probability","16ae2bfb":"### A. Feature Engineering <a class=\"anchor\" id=\"section_1_1\"><\/a>","33eaf275":"One of the advantage of GMM clustering algortihm over K-means is to assume that an observation can belong to several clusters and hence is able to calculate the probability for each observation associated to each of the clusters.\n\nWe can therefore perform Hard clustering or Soft clustering with GMM clustering.<br>\nHard clustering assigns each observations to the cluster yielding the highest probability. Each observation is assigned to one cluster and we can retrieve the probability associated.\n\nIn our example, we will define a marketing strategy for each of the clusters generated. We need to perform Hard clustering to associate each customer to a strategy but we will keep the probability in case we want to assign a customer into another cluster and try another marketing strategy. ","a8d904ca":"We will normalize our data in both rows and columns","c3d574fc":"### D. Clusters visualization <a class=\"anchor\" id=\"section_2_4\"><\/a>","4003fdb7":"### B. Clusters creation <a class=\"anchor\" id=\"section_2_2\"><\/a>","66b52b64":"We can see the 4 clusters are well defined.<br> \nSome customers with low income are spending a lot, meaning we could try to applicate a marketing strategy initially defined for *Stars* customers to them","cd30ea48":"### C. Outliers and missing values treatment <a class=\"anchor\" id=\"section_1_3\"><\/a>","839ca865":"We plot a statistical summary of the 4 clusters to understand their meaning and give our segment a name","8702e20c":"### D. Data normalization <a class=\"anchor\" id=\"section_1_4\"><\/a>","37104df3":"For a clustering analysis, simply removing the rows with missing values can be an option.\n\nWe will therefore just remove the 24 rows which don't have Income values and the row where the Income is equal to 666K"}}