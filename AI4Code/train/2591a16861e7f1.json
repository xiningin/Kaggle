{"cell_type":{"4e12b4f8":"code","4c7a56ad":"code","fafcfcd6":"code","096290b4":"code","b75e6f04":"code","ca59ff8b":"code","045d6452":"code","7708154b":"code","9a13e7fe":"code","e9d1d837":"code","7ccfe2ad":"code","7e520ea3":"code","6d42f2f2":"code","3115ae86":"code","d425d728":"markdown","3fa6ab63":"markdown","16f780a7":"markdown","503500d5":"markdown","c5925514":"markdown","e6f368fc":"markdown","4cc2476c":"markdown","86032600":"markdown","388c7851":"markdown","4ae4da17":"markdown","8287d30c":"markdown","2b7d6201":"markdown","27ccb25c":"markdown","da2a81d5":"markdown","cf4515cf":"markdown","bffdc92a":"markdown","21f5753f":"markdown"},"source":{"4e12b4f8":"!pip install -q \/kaggle\/input\/efficientnet-keras-dataset\/efficientnet_kaggle","4c7a56ad":"import sys\nsys.path.append('\/kaggle\/input\/vit-keras-dataset')\n!pip install -q \/kaggle\/input\/validators0182-dataset\/validators-0.18.2-py3-none-any.whl","fafcfcd6":"import pandas as pd, numpy as np, random,os, shutil\nimport tensorflow as tf, re, math\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nimport sklearn\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nimport yaml\n\nfrom vit_keras import vit, utils, visualize, layers\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.metrics import roc_auc_score","096290b4":"print('np:', np.__version__)\nprint('pd:', pd.__version__)\nprint('sklearn:', sklearn.__version__)\nprint('tf:',tf.__version__)\nprint('tfa:', tfa.__version__)","b75e6f04":"class CFG:\n    # DBUG OR not\n    debug = False\n    \n    # DEVICE\n    device = 'GPU'\n\n\n    # IMAGE SIZE\n    img_size = [512, 512]\n\n    # BATCH SIZE AND EPOCHS\n    batch_size  = 32\n\n    # CFG.augmentATION\n    augment   = True\n    transform = False\n\n    # TRANSFORMATION\n    fill_mode = 'nearest'\n    rot    = 10.0\n    shr    = 5.0\n    hzoom  = 30.0\n    wzoom  = 30.0\n    hshift = 30.0\n    wshift = 30.0\n\n    # FLIP\n    hflip = True\n    vflip = False\n\n    # CLIP [0, 1]\n    clip = False\n\n    # Dropout\n    drop_prob   = 0.75\n    drop_cnt    = 10\n    drop_size   = 0.05\n\n    #bri, contrast\n    sat  = [0.7, 1.3]\n    cont = [0.8, 1.2]\n    bri  =  0.15\n    hue  = 0.05\n\n    # TEST TIME CFG.augmentATION STEPS\n    tta = 11\n    \n    tab_cols    = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n                   'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n    target_col  = ['Pawpularity']","ca59ff8b":"if CFG.device == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        CFG.device = \"GPU\"\n\nif CFG.device != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif CFG.device == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","045d6452":"BASE_PATH = '\/kaggle\/input\/petfinder-pawpularity-score'","7708154b":"# Train Data\ndf = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ndf['image_path'] = BASE_PATH + '\/train\/' + df.Id + '.jpg'\ndisplay(df.head(2))\n\n# Test Data\ntest_df  = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\ntest_df['image_path'] = BASE_PATH + '\/test\/' + test_df.Id + '.jpg'\n\ndisplay(test_df.head(2))","9a13e7fe":"print('train_files:',df.shape[0])\nprint('test_files:',test_df.shape[0])","e9d1d837":"def get_mat(shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    #rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n#     c1   = tf.math.cos(rotation)\n#     s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n#     rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n#                                    -s1,  c1,   zero, \n#                                    zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                               zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n\n    return  K.dot(shear_matrix,K.dot(zoom_matrix, shift_matrix)) #K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))                  \n\ndef transform(image, DIM=CFG.img_size):#[rot,shr,h_zoom,w_zoom,h_shift,w_shift]):\n    if DIM[0]!=DIM[1]:\n        pad = (DIM[0]-DIM[1])\/\/2\n        image = tf.pad(image, [[0, 0], [pad, pad+1],[0, 0]])\n        \n    NEW_DIM = DIM[0]\n    \n    rot = CFG.rot * tf.random.normal([1], dtype='float32')\n    shr = CFG.shr * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ CFG.hzoom\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ CFG.wzoom\n    h_shift = CFG.hshift * tf.random.normal([1], dtype='float32') \n    w_shift = CFG.wshift * tf.random.normal([1], dtype='float32') \n    \n    transformation_matrix=tf.linalg.inv(get_mat(shr,h_zoom,w_zoom,h_shift,w_shift))\n    \n    flat_tensor=tfa.image.transform_ops.matrices_to_flat_transforms(transformation_matrix)\n    \n    image=tfa.image.transform(image,flat_tensor, fill_mode=CFG.fill_mode)\n    \n    rotation = math.pi * rot \/ 180.\n    \n    image=tfa.image.rotate(image,-rotation, fill_mode=CFG.fill_mode)\n    \n    if DIM[0]!=DIM[1]:\n        image=tf.reshape(image, [NEW_DIM, NEW_DIM,3])\n        image = image[:, pad:DIM[1]+pad,:]\n    image = tf.reshape(image, [*DIM, 3])    \n    return image\n\ndef dropout(image,DIM=CFG.img_size, PROBABILITY = 0.6, CT = 5, SZ = 0.1):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image with CT squares of side size SZ*DIM removed\n    \n    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n    if (P==0)|(CT==0)|(SZ==0): \n        return image\n    \n    for k in range(CT):\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM[1]),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM[0]),tf.int32)\n        # COMPUTE SQUARE \n        WIDTH = tf.cast( SZ*min(DIM),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH\/\/2)\n        yb = tf.math.minimum(DIM[0],y+WIDTH\/\/2)\n        xa = tf.math.maximum(0,x-WIDTH\/\/2)\n        xb = tf.math.minimum(DIM[1],x+WIDTH\/\/2)\n        # DROPOUT IMAGE\n        one = image[ya:yb,0:xa,:]\n        two = tf.zeros([yb-ya,xb-xa,3], dtype = image.dtype) \n        three = image[ya:yb,xb:DIM[1],:]\n        middle = tf.concat([one,two,three],axis=1)\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM[0],:,:]],axis=0)\n        image = tf.reshape(image,[*DIM,3])\n\n#     image = tf.reshape(image,[*DIM,3])\n    return image","7ccfe2ad":"def build_decoder(with_labels=True, target_size=CFG.img_size, ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.image.resize(img, target_size)\n        img = tf.cast(img, tf.float32) \/ 255.0\n        img = tf.reshape(img, [*target_size, 3])\n\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), tf.cast(label, tf.float32)\n    \n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True, dim=CFG.img_size):\n    def augment(img, dim=dim):\n        img = transform(img,DIM=dim) if CFG.transform else img\n        img = tf.image.random_flip_left_right(img) if CFG.hflip else img\n        img = tf.image.random_flip_up_down(img) if CFG.vflip else img\n        img = tf.image.random_hue(img, CFG.hue)\n        img = tf.image.random_saturation(img, CFG.sat[0], CFG.sat[1])\n        img = tf.image.random_contrast(img, CFG.cont[0], CFG.cont[1])\n        img = tf.image.random_brightness(img, CFG.bri)\n        img = dropout(img, DIM=dim, PROBABILITY = CFG.drop_prob, CT = CFG.drop_cnt, SZ = CFG.drop_size)\n        img = tf.clip_by_value(img, 0, 1)  if CFG.clip else img         \n        img = tf.reshape(img, [*dim, 3])\n        return img\n    \n    def augment_with_labels(img, label):    \n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels=None, batch_size=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\", drop_remainder=False):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n    \n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n    ds = ds.cache(cache_dir) if cache else ds\n    ds = ds.repeat() if repeat else ds\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.seed)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.prefetch(AUTO)\n    return ds","7e520ea3":"BASE_DIRS = [\n    (512, '\/kaggle\/input\/tf-petfinder-vit-cls-tpu-train'),\n]\n\nMODEL_CONFIGS = []\nfor dim, base_dir in  BASE_DIRS:\n    paths = sorted(glob(os.path.join(base_dir, '*h5')))\n    if len(paths)==0:\n        print('no model found for :',base_dir)\n    MODEL_CONFIGS.append([dim, paths])","6d42f2f2":"print('='*35)\nprint('### Inference')\nprint('='*35)\npreds=[]\nfor dim, model_paths in tqdm(MODEL_CONFIGS):\n    test_paths = test_df.image_path.tolist()\n    if len(test_paths)<=8:\n        CFG.batch_size = 1\n    elif dim>=768:\n        CFG.batch_size = REPLICAS * 16\n    elif dim>=640:\n        CFG.batch_size = REPLICAS * 24\n    else:\n        CFG.batch_size = REPLICAS * 32\n    dtest = build_dataset(\n        test_paths, \n        batch_size=CFG.batch_size, repeat=True, \n        shuffle=False, augment=True if CFG.tta>1 else False, cache=False,\n        decode_fn=build_decoder(with_labels=False, target_size=[dim,dim]),\n        augment_fn=build_augmenter(with_labels=False, dim=[dim, dim])\n    )\n    for model_path in model_paths:\n        print(f'Model: {model_path}')\n        with strategy.scope():\n            print('Loading Model...')\n            model = tf.keras.models.load_model(model_path, compile=False)\n        print('Predicting...');\n        pred = model.predict(dtest, steps = CFG.tta*len(test_paths)\/CFG.batch_size, verbose=1)\n        pred = pred[:CFG.tta*len(test_paths),:]\n        pred = np.mean(pred.reshape(CFG.tta, len(test_paths), -1), axis=0)\n        preds.append(pred*100.0) # denormalizing from [0-1] to [0-100]\n        print()\npreds = np.mean(preds, axis=0)","3115ae86":"pred_df = pd.DataFrame({'Id':test_df.Id,\n                        'Pawpularity':preds.reshape(-1)})\nsub_df = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')\ndel sub_df['Pawpularity']\nsub_df = sub_df.merge(pred_df, on='Id', how='left')\nsub_df.to_csv('submission.csv',index=False)\nsub_df.head(2)","d425d728":"## Notebook:\n* CNN + Regression:\n    * train: [[TF] PetFinder: Image [TPU][Train] \ud83d\udc36](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-image-tpu-train)\n    * infer: [[TF] PetFinder: Image [TPU][Infer] \ud83d\udc36](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-image-tpu-infer)\n* Transformer + Classification:\n    * train: [[TF] PetFinder: ViT+Cls [TPU][Train] \ud83d\ude3a](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-vit-cls-tpu-train\/)","3fa6ab63":"# Data Augmentation\nUsed simple augmentations, some of them may hurt the model.\n* RandomFlip (Left-Right)\n* No Rotation\n* RandomBrightness\n* RndomContrast\n* Shear\n* Zoom\n* Coarsee Dropout\/Cutout","16f780a7":"# Version Check","503500d5":"# Content:\n* Install Libraries.\n* Import Libraries.\n* Libraries Version Check\n* Configuration.\n* DEVICE Configs.\n* Meta Data.\n* Train-Test Distrubution\n* Data Augmentation.\n* Data Pipeline.\n* Model Config.\n* Inference\n\n\n\n","c5925514":"# Model Configs","e6f368fc":"# DEVICE Configs","4cc2476c":"# Submission","86032600":"## Train-Test Ditribution","388c7851":"# Import Libraries","4ae4da17":"# Base Path for Dataset","8287d30c":"# [PetFinder.my - Pawpularity Contest](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score)\n> Predict the popularity of shelter pet photos\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25383\/logos\/header.png)","2b7d6201":"# Meta Data","27ccb25c":"# Install Libraries","da2a81d5":"# Configuration","cf4515cf":"# Idea:\n* Use **Classification** instead of **Regression**.\n    * Normalize `Pawpularity` from `[0,100]` range to `[0,1]`.\n    * Use `BinaryCrossentropy` loss intead of `RootMeanSquaredError`\n    * Calculate competition metric after **denormalizing** the prediction\n* Use **Transformer** instead of **CNN**\n* Use only **Image** Feature.\n","bffdc92a":"## Data Pipeline\n* Reads the raw file and then decodes it to tf.Tensor\n* Resizes the image in desired size\n* Chages the datatype to **float32**\n* Caches the Data for boosting up the speed.\n* Uses Augmentations to reduce overfitting and make model more robust.\n* Finally, splits the data into batches.\n","21f5753f":"# Inference"}}