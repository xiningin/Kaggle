{"cell_type":{"28c37b69":"code","3583bd47":"code","010631d7":"code","2f5be61e":"code","d8ce20c3":"code","d107cc1e":"code","24c93a1a":"code","29dec65d":"code","dae0a951":"code","5dc38506":"code","0ab35557":"code","8dc020d0":"code","ee2337ef":"code","7c79e355":"code","454fc050":"code","7cb84d22":"code","26f592b0":"code","d5e700a5":"code","ed71ccf2":"code","d426893a":"code","ef98732c":"code","76ea47a2":"code","0be3ce8b":"code","20248d60":"code","746701de":"code","faa6f442":"code","6f1f1729":"code","5bb226dd":"code","1014ba74":"code","defb7548":"code","bea7ac07":"code","dfd8346d":"code","e7b7c91e":"code","f4138b2d":"code","38a93706":"code","cea6658e":"markdown","195fa3d5":"markdown","7a1c3d64":"markdown","876dd363":"markdown","d5ca529e":"markdown","51a2ce8a":"markdown","4fd759a2":"markdown","b9c9f07f":"markdown","59eae654":"markdown","084aafe1":"markdown","d4cf47e9":"markdown","05bddff7":"markdown","28883a5b":"markdown","cf4da7d9":"markdown","fa6274df":"markdown","a584eb34":"markdown","27954949":"markdown","8efcf95c":"markdown","6a00b86e":"markdown","b6faf599":"markdown","afd94d9c":"markdown","56bb6ac6":"markdown","a3a02c91":"markdown","bc8d90fe":"markdown","3678b892":"markdown","76144497":"markdown","b3f75564":"markdown","647e1b1c":"markdown","ef64dd44":"markdown","55be4496":"markdown","74609732":"markdown","48e8408a":"markdown","6a9cd1a4":"markdown","59404d36":"markdown","0a1dd7a3":"markdown","9851e789":"markdown","0f1ba3d6":"markdown","e9e26b48":"markdown"},"source":{"28c37b69":"# packages\n!pip install shap \n!pip install yellowbrick\n!pip install imblearn","3583bd47":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.metrics import classification_report, recall_score, precision_score ,average_precision_score, plot_precision_recall_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\nfrom sklearn.impute import SimpleImputer\nfrom yellowbrick.classifier import PrecisionRecallCurve, ConfusionMatrix\nfrom sklearn.model_selection import train_test_split, cross_validate ,KFold, StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer, QuantileTransformer, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier \nimport shap\nshap.initjs()\n\n%matplotlib inline \nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)","010631d7":"# import data\npath = '..\/input\/creditcardfraud\/creditcard.csv' \ncredit = pd.read_csv(path)\ncredit.head()","2f5be61e":"print('Rows: {} | Columns: {} '.format(credit.shape[0], credit.shape[1]))","d8ce20c3":"# Descritive stats \ncredit.describe()","d107cc1e":"# class\nsns.countplot(x=credit['Class'], palette='coolwarm')\nprint('Normal: {} |  Fraud: {}'.format(credit[credit['Class']==0].shape[0] , credit[credit['Class']==1].shape[0]))","24c93a1a":"def missing_values(data):\n\n    \"\"\" Summary of null data\n        contained in the dataset \"\"\"\n\n    # total nulls     \n    missing = data.isnull().sum()\n    total = missing.sort_values(ascending=True)\n    \n    # percentage  \n    percent = (missing \/ len(data.index ) * 100).round(2).sort_values(ascending=True)\n\n    # concatenation \n    table_missing = pd.concat([total, percent], axis=1, keys=['NA numbers', 'NA percentage'])\n\n    return table_missing","29dec65d":"missing_values(credit)","dae0a951":"# data types\ncredit.dtypes","5dc38506":"# Compare Distributions\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(2,1,1)\nplt.title('Normal Transaction Seconds')\nsns.distplot(credit[credit['Class']==0]['Time'], color='purple')\n\nprint('\\n')\nprint('\\n')\n\nplt.subplot(2,1,2)\nplt.title('Seconds Fraudulent Transaction')\nsns.distplot(credit[credit['Class']==1]['Time'], color='red')","0ab35557":"# Repeated fraud?\n\nfraud = credit[credit['Class']==1].loc[credit.duplicated()]\nprint('Repeated scams: {} '.format(len(fraud)))\nprint('\\n')\nfraud","8dc020d0":"# cmap\ncmap = sns.diverging_palette(120, 40, sep=20, as_cmap=True, center='dark')","ee2337ef":"# Correlation\ncorr = credit.corr(method='pearson')\n\nfig, ax = plt.subplots(figsize=(23,15))\n\n# cmap=Greys\n\nplt.title('Correlation matrix', fontsize=16)\nprint('\\n')\ncorrelacao = sns.heatmap(corr, annot=True, cmap='Blues', ax=ax, lw=3.3, linecolor='lightgray')\ncorrelacao","7c79e355":"# cols plot  \ncols_names = credit.drop(['Class', 'Amount', 'Time'], axis=1)\nidx = 0\n\n# Spliting classes\nfraud = credit[credit['Class']==1]\nnormal = credit[credit['Class']==0]\n\n# figure plot  \nfig, ax = plt.subplots(nrows=7, ncols=4, figsize=(18,18))\nfig.subplots_adjust(hspace=1, wspace=1)\n\nfor col in cols_names:\n    idx += 1\n    plt.subplot(7, 4, idx)\n    sns.kdeplot(fraud[col], label=\"Normal\", color='blue', shade=True)\n    sns.kdeplot(normal[col], label=\"Fraud\", color='orange', shade=True)\n    plt.title(col, fontsize=11)\n    plt.tight_layout()","454fc050":"# Range of fraud values \ncredit[credit['Class']==1]['Amount'].value_counts","7cb84d22":"# What is the average value of the fraudulent transaction?\nprint('Average Fraud: {} | Average Normal: {}'.format(credit[credit['Class']==1]['Amount'].mean() , credit[credit['Class']==0]['Amount'].mean()))","26f592b0":"# What is the highest fraud value?\nprint('Higher fraud value: {}  | Higher normal value: {}'.format(credit[credit['Class']==1]['Amount'].max(), credit[credit['Class']==0]['Amount'].max()))","d5e700a5":"# Transaction Amount\n\nplt.figure(figsize=(11,7))\nplt.title('Value of Transactions by Class (Normal | Fraud)', fontsize=15)\nsns.barplot(x='Class', y='Amount', data=credit, palette='GnBu')","ed71ccf2":"# Distribution of Transaction amounts\nplt.figure(figsize=(10,5))\nplt.title('Transactions', fontsize=14)\nplt.grid(False)\nsns.kdeplot(credit['Amount'], color='lightblue', shade=True)","d426893a":"# Separating feature | class\n\nX = credit.drop('Class', axis=1)\ny = credit['Class']\n\n# Train|Test\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)\n\n\n# StandardScaler \nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# Encoder \nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.transform(y_test)","ef98732c":"# Baseline model\n\nbaseline = LogisticRegression(random_state=42)\nbaseline.fit(X_train, y_train)\ny_baseline = baseline.predict(X_test)\n\n# Probabilities \ny_proba_baseline = baseline.predict_proba(X_test)[:,1]\n\nprint(classification_report(y_test, y_baseline))\nprint('\\n')\nprint('AUC: {}%'.format(roc_auc_score(y_test, y_proba_baseline)))\nprint('Precision-Recall: {}'.format(average_precision_score(y_test, y_proba_baseline)))","76ea47a2":"X = credit.drop('Class', axis=1)\ny = credit['Class']\n\n# Validation \nKFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\nfold = 0\nfor train_index, test_index in KFold.split(X,y):\n      fold += 1 \n      print('Fold: ', fold)\n      print('Train: ',train_index.shape[0])\n      print('Test: ', test_index[0])\n\n      # Split\n      X = credit.drop('Class', axis=1)\n      y = credit['Class']\n\n      # OverSampling SMOTE \n      smote = SMOTE(random_state=42)\n      X, y = smote.fit_sample(X, y)\n      print('Normal: {}  |  Fraud: {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n\n      # spliting data  \n      X_train, X_test = X.loc[train_index], X.loc[test_index]\n      y_train, y_test = y[train_index], y[test_index] \n\n      \n      # pre-processing \n      scaler = QuantileTransformer(random_state=42)\n      X_train = scaler.fit_transform(X_train)\n      X_test = scaler.transform(X_test)\n\n      # build model \n      forest = RandomForestClassifier(n_estimators=200, max_depth=13, min_samples_split=9,\n                                    random_state=42)\n      forest.fit(X_train, y_train)\n      y_pred_forest = forest.predict(X_test)\n      y_proba_forest = forest.predict_proba(X_test)[:,1]\n\n\n      # metrics \n      print('\\n')\n      print(classification_report(y_test, y_pred_forest))\n      print('--------------'*5)\n      print('\\n')\n      auc_forest = roc_auc_score(y_test, y_proba_forest)\n      precision_forest = precision_score(y_test, y_pred_forest)\n      recall_forest = recall_score(y_test, y_pred_forest)\n      auprc_forest = average_precision_score(y_test, y_proba_forest)","0be3ce8b":"# Random Forest Validation  \nprint('Random Forest')\nprint('\\n')\n\nprint('AUC: ', np.mean(auc_forest))\nprint('Precision: ', np.mean(precision_forest))\nprint('Recall: ', np.mean(recall_forest))\nprint('Precision-Recall: ', np.mean(auprc_forest))\n\n\nprint('\\n')\nprint('\\n')\n\n# Curva ROC random forest \nauc_forest = np.mean(auc_forest)\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_test, y_proba_forest)\n\n# plot \nplt.figure(figsize=(12,7))\nplt.plot(fpr_forest, tpr_forest, color='blue', label='AUC: {}'.format(auc_forest))\nplt.fill_between(fpr_forest, tpr_forest, color='skyblue', alpha=0.3)\nplt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('ROC Random Forest', fontsize=16)\nplt.legend(loc=4, fontsize=14)\nplt.grid(False)\nplt.show()","20248d60":"# Precision-Recall Random Forest \n\nplt.figure(figsize=(10,5))\nplt.title('Precision-Recall Random Forest')\nviz = PrecisionRecallCurve(forest)\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test)","746701de":"# SVM \nX = credit.drop('Class', axis=1)\ny = credit['Class']\n\n# UnderSampling  \nunder = NearMiss()\nX, y = under.fit_sample(X, y)\n\n\n# Validation\nKFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\nfold = 0\nfor train_index, test_index in KFold.split(X,y):\n      fold += 1 \n      print('Fold: ', fold)\n      print('Train: ', train_index.shape[0])\n      print('Test: ', test_index[0])\n\n      # Unbalanced class\n      print('Normal: {}  |  Fraud {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n\n      # spliting data  \n      X_train, X_test = X.loc[train_index], X.loc[test_index]\n      y_train, y_test = y[train_index], y[test_index] \n\n      \n      # pre-processing  \n      scaler = RobustScaler()\n      X_train = scaler.fit_transform(X_train)\n      X_test = scaler.transform(X_test)\n\n      # build model \n      svm = SVC(C=1.0, gamma=0.5, random_state=42, probability=True)\n      svm.fit(X_train, y_train)\n      y_pred_svm = svm.predict(X_test)\n      y_proba_svm = svm.predict_proba(X_test)[:,1]\n\n\n      # metrics \n      print('\\n')\n      print(classification_report(y_test, y_pred_svm))\n      print('-------------'*5)\n      print('\\n')\n      auc_svm = roc_auc_score(y_test, y_proba_svm)\n      precision_svm = precision_score(y_test, y_pred_svm)\n      recall_svm = recall_score(y_test, y_pred_svm)\n      auprc_svm = average_precision_score(y_test, y_proba_svm)","faa6f442":"# SVM Validation \nprint('SVM')\nprint('\\n')\n\nprint('AUC: ', np.mean(auc_svm))\nprint('Precision: ', np.mean(precision_svm))\nprint('Recall: ', np.mean(recall_svm))\nprint('Precision-Recall: ', np.mean(auprc_svm))\n\n\nprint('\\n')\nprint('\\n')\n\n# Curva ROC random forest \nauc_svm = np.mean(auc_svm)\nfpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_proba_svm)\n\n# plot \nplt.figure(figsize=(12,7))\nplt.plot(fpr_svm, tpr_svm, color='blue', label='AUC: {}'.format(auc_svm))\nplt.fill_between(fpr_svm, tpr_svm, color='skyblue', alpha=0.3)\nplt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('ROC SVM', fontsize=16)\nplt.legend(loc=4, fontsize=14)\nplt.grid(False)\nplt.show()","6f1f1729":"# Precision-Recall SVM \n\nplt.figure(figsize=(10,5))\nplt.title('Precision-Recall SVM', fontsize=16)\nviz = PrecisionRecallCurve(svm)\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test)","5bb226dd":"X = credit.drop('Class', axis=1)\ny = credit['Class']\n\n\n# Validation\nKFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n \nprecision_xgboost = []\nrecall_xgboost = []\nauc_xgboost = []\nprecision_recall_xgboost = []\n\n\nfold = 0\nfor train_index, test_index in KFold.split(X,y):\n      fold += 1 \n      print('Fold: ', fold)\n      print('Train: ',train_index.shape[0])\n      print('Test: ', test_index[0])\n\n      # OverSampling SMOTE \n      smt = SMOTE(random_state=42)\n      X, y = smt.fit_sample(X, y)\n      print('Normal: {}  |  Fraud: {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n\n      # spliting data  \n      X_train, X_test = X.loc[train_index], X.loc[test_index]\n      y_train, y_test = y[train_index], y[test_index] \n\n      \n      # pre-processing  \n      scaler = QuantileTransformer()\n      X_train = scaler.fit_transform(X_train)\n      X_test = scaler.transform(X_test)\n\n      # XGboost \n      xgb = XGBClassifier(n_estimators=300, max_delta_step=1 ,eval_metric='aucpr', \n                          cpu_history='gpu', random_state=42)\n      xgb.fit(X_train, y_train)\n      y_pred = xgb.predict(X_test)\n  \n\n      # metrics \n      precision_recall_xgboost = average_precision_score(y_test, y_pred)\n      precision_xgboost = precision_score(y_test, y_pred)\n      recall_xgboost = recall_score(y_test, y_pred)\n      auc_xgboost  = roc_auc_score(y_test, y_pred)\n      print('Precision-Recall: ', average_precision_score(y_test, y_pred))\n      print('\\n')\n      print('\\n')\n\n\n\n# Final Validation \nprint('Precision-Recall: ', np.mean(precision_recall_xgboost))\nprint('Recall: ', np.mean(recall_xgboost))\nprint('Precision: ', np.mean(precision_xgboost))\nprint('AUC: ', np.mean(auc_xgboost))","1014ba74":"# Validation XGboost + SMOTE\nprint('XGboost')\nprint('\\n')\n\nprint('AUC: ', np.mean(auc_xgboost))\nprint('Precision: ', np.mean(precision_xgboost))\nprint('Recall: ', np.mean(recall_xgboost))\nprint('Precision-Recall: ', np.mean(precision_recall_xgboost))\n\n\nprint('\\n')\nprint('\\n')\n\n# Curva ROC random forest \nroc_auc_xgboost = np.mean(auc_xgboost)\nfpr_xgboost, tpr_xgboost, thresholds_xgboost = roc_curve(y_test, y_pred)\n\n# plot \nplt.figure(figsize=(12,7))\nplt.plot(fpr_xgboost, tpr_xgboost, color='blue', label='AUC: {}'.format(roc_auc_xgboost))\nplt.fill_between(fpr_xgboost, tpr_xgboost, color='skyblue', alpha=0.3)\nplt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('ROC XGboost', fontsize=16)\nplt.legend(loc=4, fontsize=14)\nplt.grid(False)\nplt.show()","defb7548":"# Precision-Recall XGboost + SMOTE \n\nplt.figure(figsize=(10,5))\nplt.title('Precision-Recall XGboost + SMOTE')\nviz = PrecisionRecallCurve(xgb)\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test)","bea7ac07":"X = credit.drop('Class', axis=1)\ny = credit['Class']\n\n# UnderSampling NearMiss\nunder = NearMiss()\nX,y = under.fit_sample(X, y)\n\n# Validation\nKFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nresultados = []\nfold = 0\nfor train_index, test_index in KFold.split(X,y):\n      fold += 1 \n      print('Fold: ', fold)\n      print('Train: ',train_index.shape[0])\n      print('Test: ', test_index[0])\n\n      # Unbalanced class \n      print('Normal: {} | Fraud: {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n\n\n      # spliting data \n      X_train, X_test = X.loc[train_index], X.loc[test_index]\n      y_train, y_test = y[train_index], y[test_index] \n\n      \n      # pre-processing \n      scaler = StandardScaler()\n      X_train = scaler.fit_transform(X_train)\n      X_test = scaler.transform(X_test)\n\n\n      # Encoder \n      encoder = LabelEncoder()\n      y_train = encoder.fit_transform(y_train)\n      y_test = encoder.transform(y_test)\n\n\n      # XGboost \n      xgb = XGBClassifier(n_estimators=300, max_delta_step=1 ,eval_metric='aucpr', \n                          cpu_history='gpu', random_state=42)\n      xgb.fit(X_train, y_train)\n      y_pred = xgb.predict(X_test)\n\n    # Metrics\n      precision_recall_xgboost = average_precision_score(y_test, y_pred)\n      precision_xgboost = precision_score(y_test, y_pred)\n      recall_xgboost = recall_score(y_test, y_pred)\n      auc_xgboost  = roc_auc_score(y_test, y_pred)\n      print('Precision-Recall: ', average_precision_score(y_test, y_pred))\n      print('\\n')\n      print('\\n')\n\n\n\n# Final Validation   \nprint('Precision-Recall: ', np.mean(precision_recall_xgboost))\nprint('Recall: ', np.mean(recall_xgboost))\nprint('Precision: ', np.mean(precision_xgboost))\nprint('AUC: ', np.mean(auc_xgboost))","dfd8346d":"# Validation XGboost + NearMiss  \nprint('XGboost')\nprint('\\n')\n\nprint('AUC: ', np.mean(auc_xgboost))\nprint('Precision: ', np.mean(precision_xgboost))\nprint('Recall: ', np.mean(recall_xgboost))\nprint('Precision-Recall: ', np.mean(precision_recall_xgboost))\n\n\nprint('\\n')\nprint('\\n')\n\n# Curva ROC random forest \nroc_auc_xgboost = np.mean(auc_xgboost)\nfpr_xgboost, tpr_xgboost, thresholds_xgboost = roc_curve(y_test, y_pred)\n\n# plot \nplt.figure(figsize=(12,7))\nplt.plot(fpr_xgboost, tpr_xgboost, color='blue', label='AUC: {}'.format(roc_auc_xgboost))\nplt.fill_between(fpr_xgboost, tpr_xgboost, color='skyblue', alpha=0.3)\nplt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('ROC XGboost', fontsize=16)\nplt.legend(loc=4, fontsize=14)\nplt.grid(False)\nplt.show()","e7b7c91e":"# Precision-Recall XGboost + NearMiss \n\nplt.figure(figsize=(10,5))\nplt.title('Precision-Recall XGboost + NearMiss')\nviz = PrecisionRecallCurve(xgb)\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test)","f4138b2d":"# Confusion Matrix  \n\n\nplt.figure(figsize=(15,10))\n\nplt.subplot(2, 1, 1)\nplt.title('Confusion Matrix XGboost', fontsize=15)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, cmap='BuPu')\n\nprint('\\n')\nprint('\\n')","38a93706":"# Features Importance \nfrom xgboost import plot_importance\n\n\n\nfig, ax = plt.subplots(figsize=(14,8))\nplot_importance(xgb, ax=ax)\nplt.title('Feature importance | XGboost + NearMiss')\nplt.show()","cea6658e":"<p align=center>\n<img src=\"https:\/\/news.mit.edu\/sites\/mit.edu.newsoffice\/files\/styles\/news_article_image_top_slideshow\/public\/images\/2018\/MIT-Fraud-Detection-PRESS_0.jpg?itok=laiU-5nR\" width=\"60%\"><\/p>\n\n\nhttps:\/\/www.eastwestbank.com\/ReachFurther\/NewsArticleStore\/519\/Credit-card-fraud-top.jpg\n\n\n<hr>","195fa3d5":"### Random Forest \n\nLet's build a Random Forest model like <b> 200 trees <\/b> combined with the OverSampling <b> SMOTE <\/b> technique and see the results of the metrics we are looking to optimize which are: \n\n* AUC \n* Precision\n* Recall \n* AUPRC  \n\n<hr>","7a1c3d64":"### Baseline\n\nI will create a base model, so that I can compare the next results of other experiments, with this pure and simple model that we are going to build.","876dd363":"### *About* dataset \n\nThe data sets contain transactions carried out with credit cards in September 2013 by European cardholders.\nThis data set <b> presents transactions that took place in two days <\/b>, in which we have 492 frauds in 284,807 transactions. The data set is highly unbalanced, the positive class (fraud) represents 0.172% of all transactions.\n\n<br>\n\nIt contains only numeric input variables that are the result of a PCA transformation. Unfortunately, due to <b> confidentiality issues <\/b>, we are unable to provide the original resources and more basic information about the data.\n\n\nFeatures V1, V2,\u2026 V28 are the main components obtained with the PCA, the only features that have not been transformed with the PCA are 'Time' and 'Amount' (Transaction amount).\n<br> \n\n\n* The 'Time' feature contains the seconds between each transaction and the first transaction in the data set.\n\n* The 'Amount' feature is the transaction amount.\n\n* The 'Class' feature is the response variable and assumes a value of 1 in case of fraud and 0 if not.\n\n<br>\n<b>\n  0: Normal transaction |\n  1: Fraudulent transaction\n  <\/b>\n\n<br>\n<hr>","d5ca529e":"### Data analysis\n\nIn this step I will try to identify some patterns, I already have in mind that I will do some experiments treating the Outliers later, create some models and compare the approaches made.\n\n<hr>\n<br>","51a2ce8a":"### SVM\n\nWe are going to apply SVM which is another robust algorithm, we are going to combine another preprocessing in the data, we are going to use UnderSampling now, where we are going to decrease the majority class, in an attempt to equalize the classes.\n\nSVM has the kernel function, which allows you to build a Hyperplane that best fits the data.\n\n<br>\n<hr>","4fd759a2":"# Credit Card Fraud Detection\n\n\nIn this project we will apply Machine learning for Credit Card fraud detection, a very big problem that financial institutions and Fintechs have daily, which is to identify whether a transaction is a fraud or not, it is a difficult and extremely delicate task. It is extremely important that credit card companies are prepared for this type of crime, constantly monitoring the behavior of cards.\n\n<hr>","b9c9f07f":"Many companies have already invested heavily in Artificial Intelligence, to solve financial problems and also create products that minimize risks and maximize profits.\n\nThere are several cases in the industry, about the importance of Artificial Intelligence for companies that deal with financial data, which need scalability and speed in approving transactions made by credit cards, it is a much greater challenge than just creating a model of Machine learning, but also allow for high performance of these platforms.\n\n\nWith the growing need to have a robust system that can tell you whether a purchase was a fraud or not, opportunities to apply Artificial Intelligence are increasingly necessary, and it is becoming essential to use AI to combat a problem so big.","59eae654":"<hr>\n<br>\n\n### XGboost + NearMiss\n\nWe will combine XGboost with the UnderSampling NearMiss technique, I will use the same parameters as the previous model, in this case we will minimize the majority class to a prortion equal to our minority class.","084aafe1":"All distributions of variables that have a \"mask\", we do not know the real representation of these variables because they were hidden, but through the distribution with the kernel density, you can see very well the curves of each one, comparing with a normal transaction or Fraud.\n\n\n\nFraud: Some approach a Gaussian Distribution, with a larger peak and a very long tail, possibly we have a greater variation of the data, in these features.\n","d4cf47e9":"### XGboost + SMOTE   \n\nWe will now apply the OverSampling technique with XGboost, I will join an experiment with <b> SMOTE <\/b> that will be responsible for applying OverSampling, I will match the classes maximizing the proportions of class 1.\n\n<br>","05bddff7":"<br>\nThere were <b> 19 repeated frauds <\/b> in this two days, there are cases where there were three frauds with the same value, the rest is just two, something that stands out is the low values of the transactions that there was a greater number of fraud, perhaps because the fraudster considers himself the risk of being caught compared to the number of attempts.\n\n<br>","28883a5b":"<hr>\n<br>\n<hr>\n<br>","cf4da7d9":"Looking at the two Distributions, we can see that the period of a transaction labeled as Normal, occurs in an average in a trivial way which is considered something normal, since fraudulent transactions occur in a longer period of time, <b> 50,000 seconds <\/b> a new fraudulent transaction is made (<b> In two days of transactions <\/b>). It is very important to identify that it is a distribution very close to <b> Uniform <\/b> because it has a curve of density, almost at the same constant angle.\n\n\nWe need to check later, if a fraudulent transaction occurs more than once, with the same transaction data.","fa6274df":"<hr>\n<br>\n\n### XGboost Summary\n\nThe best combination so far was <b> XGboost + UnderSampling (NearMiss) <\/b> we were able to obtain very good values, we reduced our majority class, balancing the classes the algorithm managed to better generalize the data, with an AUPRC of <b> 94% <\/b> in the validation, it was an excellent result of the model, the synthetic data that SMOTE ends up generating, ends up not making much sense for the algorithm, as they are \"repeated data\" and that shows, very little generalization of Frauds.","a584eb34":"<hr>\n<br>\n<hr>\n<br>\n<br>","27954949":"<hr>\n<br>\n","8efcf95c":"\n<p align=center>\n<img src=\"https:\/\/blog.strands.com\/hs-fs\/hubfs\/Screenshot%202019-07-18%20at%2014.15.15.png?width=600&name=Screenshot%202019-07-18%20at%2014.15.15.png\" width=\"60%\"><\/p>\n\n\n<br>\n<br>","6a00b86e":"The objective is to make use of Machine learning to anticipate fraud, subsequently reducing the number of frauds that end up taking place and that are not identified in time, it is a task that will require many experiments, techniques and algorithms that best perform over the data we have, in addition to the model must have the same level of performance when inserted into production. \n\n<hr>\n<br>","b6faf599":"### Analysis Summary\n\nAfter this stage of data exploration, we were able to have good insights into the whole, fraudulent transactions tend to have lower values than normal transactions, repeated fraud contains lesser values, another point that we found is that the time, between one fraud and another, as the data tell us about just two days of transactions, it is difficult to infer a point value, in which the fraudster makes a transfer, but the time had a greater variation than normal transactions.\n\nWe will create several experiments, in search of a robust model that achieves our objective.\n\n\n\n<br>\n<hr>\n<br>","afd94d9c":"### XGboost \n\nXGBoost is a Machine Learning algorithm, based on a decision tree and that uses a Gradient boosting structure.\n\nIn forecasting problems involving unstructured data, such as images, texts and videos, artificial neural networks tend to outperform all other algorithms or frameworks.\n\nHowever, when it comes to structured \/ tabular data, decision tree-based algorithms are considered to be the best in their class at the moment.\n\nLet's create a model with XGBoost.\n\n\n(In a next project, I will explain how XGboost works in full)\n<p align=center>\n<img src=\"https:\/\/pythonawesome.com\/content\/images\/2018\/06\/xgboost.png\" width=\"60%\"><\/p>\n\n\n\n<br>\n<hr>","56bb6ac6":"<br>\n<br>\n<hr>","a3a02c91":"### Solution","bc8d90fe":"\n### Summary Random Forest\n\nThe model with <b> 200 trees <\/b>, combined with a pre-processing method called <b> QuantileTransformer <\/b> that aims to reduce the impact of possible Outliers on the data, it will approximate the distribution of the features through the IQR (interquartile range), I also made use of the <b> SMOTE <\/b> technique that creates synthetic data, of the minority class equaling with the majority class.\n\n<br>\n\nIn summary, the model did not achieve a satisfactory result, the AUC of the model was high, in the validation the result was <b> 97% <\/b>, but the other metrics did not reach an expected value. Precision was high but Recall didn\u2019t go up much compared to baseline, the Precision-Recall trade-off is the main metric in our evaluation, and difficult to optimize, because we want a model that can separate fraud well from a normal transaction, and also to identify a fraud well when it actually occurs.","3678b892":"Let's first apply a Random Forest, combined with an OverSampling technique that basically will, create synthetic data in the minority class which is the fraud class, let's test this approach and see the model's performance. So on, let's apply UnderSampling as well to measure performance in another approach. ","76144497":"We were able to visualize our confusion matrix, which shows that the model was able to classify frauds well, we minimized the FN that contains Frauds that were classified incorrectly.\n\nVisualizing the most important features, which the XGboost + NearMiss model identified, <b> V14 <\/b> is considered by the model as the most important Features, it is the one with the greatest magnitude, then it has two more features with the prefix V, unfortunately these are features that contain a mask and we are unable to interpret so deeply, the time between one transaction and another and the value of that transaction are also being considered.\n\n<b>(In my private notebook he showed the name of the features, when I uploaded it to kaggle he gave this error)<\/b>\n\n<hr>","b3f75564":"I will check the distribution of seconds, I want to identify the period of time, in which a fraudulent transaction is made, compared to normal transactions.","647e1b1c":"<p align=center>\n<img src=\"https:\/\/miro.medium.com\/max\/1000\/0*_6WEDnZubsQfTMlY.png\" width=\"70%\"><\/p>\n\nhttps:\/\/ai-journey.com\/wp-content\/uploads\/2019\/06\/fraud-EMV-chip-credit-card.jpg","ef64dd44":"<br>\n\n### Baseline\n\n* AUC: 0.97\n* AUPRC: 0.78\n* Precision: 0.88\n* Recall: 0.63\n\nA pure baseline model gave us an idea of how much we can improve the base model, for a better model, we will create some experiments to obtain a better performance.\n\n<hr>\n<br>","55be4496":"Looking at Pearson's correlations, we can see that there are no big positive correlations, only a few features go above <b> 0.30 correlation, which are <b> V7 with Amount | V20 with Amount <\/b>.\n\n(We'll look at the correlations later)\n\n<br>","74609732":"Our model's learning curve, with which we can see from how many examples seen the model started to converge, the model trained with 788 examples, notice that our Precision-Recall minimum, which is our target metric, is 0.5. excellent, from the 400 training examples, Precision-Recall went up a lot, reaching 98% that we previously validated by applying cross-validation.\n\n\n<hr>\n<br>\n<hr>","48e8408a":"### Conclusion\n\n\nIt was a challenging project, for dealing with a very high class imbalance, and dealing with a problem so delicate that it is bank fraud, the biggest challenge was to ponder the insights I had about the data, understand which techniques and algorithms to apply was of no importance , it is very sensitive to deal with transactions that occur on such a high and fast frequency, our final result was satisfactory, I presented a model with a <b> Precision-Recall of: 94% <\/b> using the <b> XGboost combination + NearMiss <\/b> which was what I sought from the beginning, to make the model identify a fraud among the frauds, and which was also able to separate a normal transaction from a fraud, the experiments I did were necessary to create a baseline where I could guide myself to improve the results, in the end I got an excellent result, the project was successfully concluded and the learning was great.\n\n\n<hr>\n","6a9cd1a4":"### Data cleaning\n\nBefore starting an analysis of the data, we will clean the data, it will not be necessary to delete duplicate transactions as it makes total sense, to contain transactions that are done routinely.\n\n* Missing values\n* Noises\n* Data type","59404d36":"<hr>\n<br>\n<hr>\n<br>","0a1dd7a3":"### Experimentation\n\nLet's test some algorithms like:\n\n* Random Forest\n* SVM\n\nWe will apply them separately, but we will try other alternatives in the modeling, apply data balancing techniques, and also validate our models from Cross validation.\n\n<br>\n<hr>","9851e789":"#### Distributions\n\nLet's explore the distributions, of the masked features <b> (V) <\/b> a little, to see how they behave, in these two days of financial transactions. ","0f1ba3d6":"<hr>\n<br>\n\n### SVM Summary\n\nWith SVM we were able to improve our Recall to: <b> 95% <\/b>, consequently our Precision dropped, but in general the performance of this model was superior to Random Forest, AUC was the only one with a discretion, Precision-Recall: <b > 96% <\/b> was a very good result. Combining SVM with the UnderSampling technique, we obtained good results in this experiment.\n\nI will create two more models using two Gradient Boosting algorithms to see if we can do it, even better results than SVM.\n","e9e26b48":"From the beginning it is clear the unbalance of classes, this is what makes Fraud modeling so delicate, an event that is rare and that happens in a subtle way, we will create hypotheses to do Feature engineering with this data, in order to maximize the metrics : <b> ROC AUC | Precision | Recall <\/b>.\n\nTrying to measure the performance of the model with Accuracy would be a mistake because we would have a high accuracy, which in fact would not solve our problem, as the set had unbalanced classes, we will focus on these three metrics listed above, which do not vary with the unbalance of classes."}}