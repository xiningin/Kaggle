{"cell_type":{"0c348ae0":"code","30622dad":"code","e54c3102":"code","f42e893e":"code","93e9fb6d":"code","2204c854":"code","dbfb1891":"code","cc12a4f4":"code","0059b555":"code","017904c1":"code","5c27be0c":"code","95708687":"code","7580c9e5":"code","3f1a7e6b":"code","58e33b44":"code","7c38852f":"code","5d2b4814":"code","c112de3c":"code","29047dd7":"code","72e7fc84":"code","eb5a9d4b":"code","10721da2":"code","8a2849ae":"code","09c5a1be":"code","76af669e":"code","e8f752fc":"markdown","6eff5999":"markdown"},"source":{"0c348ae0":"!pip install nuscenes-devkit","30622dad":"from pathlib import Path\nfrom PIL import Image","e54c3102":"# dir with all input data from Kaggle\nINP_DIR = Path('\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/')\nprint(INP_DIR)","f42e893e":"# dir with index json tables (scenes, categories, logs, etc...)\nTABLES_DIR = INP_DIR.joinpath('train_data')","93e9fb6d":"# Adjust the dataroot parameter below to point to your local dataset path.\n# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps\n!ln -s {INP_DIR}\/train_images images\n!ln -s {INP_DIR}\/train_maps maps\n!ln -s {INP_DIR}\/train_lidar lidar","2204c854":"DATA_DIR = Path().absolute() \n# Empty init equals '.'.\n# We use this because we link train dirs to current dir (cell above)","dbfb1891":"# dir to write KITTY-style dataset\nSTORE_DIR = DATA_DIR.joinpath('kitti_format')","cc12a4f4":"!pip install -U git+https:\/\/github.com\/lyft\/nuscenes-devkit","0059b555":"import lyft_dataset_sdk\nimport lyft_dataset_sdk.utils.export_kitti","017904c1":"!python -m lyft_dataset_sdk.utils.export_kitti nuscenes_gt_to_kitti -h","5c27be0c":"# convertation to KITTY-format\n!python -m lyft_dataset_sdk.utils.export_kitti nuscenes_gt_to_kitti \\\n        --lyft_dataroot {DATA_DIR} \\\n        --table_folder {TABLES_DIR} \\\n        --samples_count 20 \\\n        --parallel_n_jobs 2 \\\n        --get_all_detections True \\\n        --store_dir {STORE_DIR}","95708687":"# check created (converted) files. velodyne = LiDAR poinclouds data (in binary)\n!ls {STORE_DIR}\/velodyne | head -2","7580c9e5":"# render converted data for check. Currently don't support multithreading :(\n!python -m lyft_dataset_sdk.utils.export_kitti render_kitti \\\n        --store_dir {STORE_DIR}","3f1a7e6b":"# Script above write images to 'render' folder\n# in store_dir (where we have converted dataset)\nRENDER_DIR = STORE_DIR.joinpath('render')","58e33b44":"# get all rendered files\nall_renders = list(RENDER_DIR.glob('*'))\nall_renders.sort()","7c38852f":"# render radar data (bird view) and camera data with bboxes","5d2b4814":"#Image.open(all_renders[0])","c112de3c":"#Image.open(all_renders[1])","29047dd7":"!ls .\/kitti_format","72e7fc84":"!cat .\/kitti_format\/label_2\/095d5bb88eb9cdd223b90d2a1475c0cf2f4b4c2a8aca82ba0ae51f6fba540440.txt","eb5a9d4b":"!ls kitti_format\/image_2\/","10721da2":"path = 'kitti_format\/image_2\/095d5bb88eb9cdd223b90d2a1475c0cf2f4b4c2a8aca82ba0ae51f6fba540440.png'\nfrom IPython.display import Image\nImage(path)","8a2849ae":"!ls .\/kitti_format\/label_2\/ | wc -l","09c5a1be":"!ls .\/kitti_format\/image_2\/ | wc -l","76af669e":"!ls .\/kitti_format\/render | wc -l","e8f752fc":"### 40 denotes, 20 samples were taken - which was then splitted into 20 lidar + 20 camera images","6eff5999":"## In this kernel we convert LEVEL5 Lyft data (NuScenes format) to KITTI format, which is usually used in public repositories. After this you can search for repos, that solve KITTI 3d-detection task.\n\n## Thanks to https:\/\/www.kaggle.com\/stalkermustang\/converting-lyft-dataset-to-kitty-format\n### This above version by user - stalkermustang was not working for me so made little changes below"}}