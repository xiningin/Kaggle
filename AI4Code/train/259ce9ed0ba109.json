{"cell_type":{"ec0189f5":"code","7581c732":"code","e8947a9f":"code","a6c3035a":"code","2f0d2715":"code","b3f8eee9":"code","c038b3c3":"code","0be92b75":"code","f80c607b":"code","8738fa05":"code","88ae69a6":"code","5c370409":"code","a60d3100":"code","1d6ad5c0":"code","1deb4516":"code","839e74ea":"code","982ede90":"code","a521656d":"code","7d38657a":"code","51a31641":"code","7a330d79":"code","9c438e11":"code","5497f102":"code","43c808e7":"code","f6c10dd9":"code","1ed707c0":"code","f3765bb3":"code","00b321c6":"code","56cabc0a":"code","751fa0fb":"code","5368db5a":"code","1e4d3ed3":"code","7ba11ef3":"code","adee9c99":"code","d45af6a8":"code","614abf0d":"code","852898f5":"code","0bda2e15":"code","04574892":"code","4dacb9a7":"code","3d9a0921":"markdown","14f2f6b2":"markdown","08d6996c":"markdown","29857868":"markdown","a86f34cf":"markdown","09ff5876":"markdown","611f8e8f":"markdown","2c67c963":"markdown","02186744":"markdown","8c9c1f87":"markdown","07926e75":"markdown","267b274e":"markdown","24c29b49":"markdown","aee77fff":"markdown","8fc68dc7":"markdown","1f974431":"markdown","80fe549a":"markdown","87845c76":"markdown","cf8485bc":"markdown","6b27e252":"markdown","0faf618d":"markdown","f00fc575":"markdown","88f3a959":"markdown","830c9a33":"markdown","1597faff":"markdown","1a44eac3":"markdown","4b51e9ec":"markdown","66901106":"markdown","791569f6":"markdown"},"source":{"ec0189f5":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport math\nimport tensorflow as tf\nimport sys\nimport gc\nimport math\nfrom tqdm import tqdm","7581c732":"!cp ..\/input\/rapids\/rapids.0.18.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\nprint('RAPIDS 0.18 installation complete')","e8947a9f":"import cuml, cudf, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer","a6c3035a":"train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntest = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\nsubmission = pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')\n\nprint('Train shape: {}'.format(train.shape))\ntrain.head()","2f0d2715":"print('Test shape: {}'.format(test.shape))\ntest.head()","b3f8eee9":"print('Submission shape: {}'.format(submission.shape))\nsubmission.head()","c038b3c3":"def show_samples(data_df, num, title, img_path = '..\/input\/shopee-product-matching\/train_images\/'):\n    cols = 6\n    rows = math.ceil(num \/ cols)\n    height = 5 * rows\n    fig, axs = plt.subplots(rows, cols, figsize = (20, height))\n    fig.suptitle('{}\\n'.format(title), fontsize = 25)\n    for row in range(rows):\n        for col in range(cols):     \n            if rows > 1:\n                ax = axs[row, col]\n            else:\n                ax = axs[col]\n                \n            ax.axis('off')\n            \n            df_row = col + row * cols # examine row by row for dataframe\n            title = data_df.iloc[df_row, 3]\n            title_with_return = ''\n            for i, ch in enumerate(title):\n                title_with_return += ch\n                if (i != 0) & (i % 20 == 0): \n                    title_with_return += '\\n'\n            ax.set_title(title_with_return)\n            \n            posting_id = data_df.iloc[df_row, 1]\n            img_bgr = cv2.imread(img_path + posting_id)\n            img_rgb = cv2.cvtColor(img_bgr,  cv2.COLOR_BGR2RGB) # convert image back to RGB for visualization\n            ax.imshow(img_rgb)    ","0be92b75":"show_samples(data_df = train, num = 12, title = 'Product Samples')","f80c607b":"groups = train['label_group'].value_counts()\n\nplt.figure(figsize = (20, 5))\nplt.plot(np.arange(len(groups)),groups.values)\nplt.title('Number of Duplicates',size = 25)\nplt.ylabel('Count', size = 15)\nplt.xlabel('Duplicate Products', size = 15)\nplt.show()\n\nplt.figure(figsize = (20, 5))\nplt.bar(groups.index.values[:50].astype('str'), groups.values[:50])\nplt.xticks(rotation = 45)\nplt.title('Top 50 Duplicates', size = 25)\nplt.ylabel('Count', size = 15)\nplt.xlabel('Label Group', size = 15)\nplt.show()","8738fa05":"TOP_N = 3\nCOUNT = 6\n\ntop_duplicates = train['label_group'].value_counts().index[:TOP_N]\nfor i, duplicate_label in enumerate(top_duplicates):\n    duplicate_label_df = train.loc[train['label_group'] == duplicate_label]\n    duplicate_title = 'Top #{} Product With Duplicates'.format(i + 1)\n    show_samples(data_df = duplicate_label_df, num = COUNT, title = duplicate_title)","88ae69a6":"LIMIT = 1 # 1 GB of GPU for TensorFlow, 15 GB of GPU for RAPIDS (GPU total: 16 GB)\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 1024 * LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print('GPU is online.')\n    except RuntimeError as e:\n        print(e)\nelse:\n    print('GPU is offline.')\nprint('TensorFlow GPU memory limit set to {} GB'.format(LIMIT))\nprint('RAPIDS GPU memory limit set to {} GB'.format(16 - LIMIT))","5c370409":"def get_text_embeddings(text_df):\n    text_gf = cudf.from_pandas(text_df)\n    text_embed_model = TfidfVectorizer(\n        stop_words = 'english', \n        binary = True) # binary occurence since short titles\n    text_embeddings = text_embed_model.fit_transform(text_gf)\n    \n    return text_embeddings","a60d3100":"title_embeddings = get_text_embeddings(train['title'])\ntitle_embeddings.shape","1d6ad5c0":"def knn_predict_embeddings(embeddings, metric, n_neighbors = 50):\n    BATCH_SIZE = 1024 * 4\n    n_texts = embeddings.shape[0]\n    n_batches = math.ceil(n_texts \/ BATCH_SIZE)\n    \n    knn_model = cuml.NearestNeighbors(n_neighbors = n_neighbors, metric = metric)\n    knn_model.fit(embeddings)\n    \n    embed_distances = np.zeros((n_texts, n_neighbors))\n    embed_indices = np.zeros((n_texts, n_neighbors))\n\n    with cuml.using_output_type('numpy'): # to output as numpy arrays\n        for i in tqdm(range(n_batches)):\n            a = i * BATCH_SIZE\n            b = min((i + 1) * BATCH_SIZE, n_texts)\n            distances, indices = knn_model.kneighbors(embeddings[a:b])\n            embed_distances[a:b] = distances\n            embed_indices[a:b] = indices\n        \n    return embed_distances, embed_indices","1deb4516":"title_distances, title_indices = knn_predict_embeddings(title_embeddings, 'cosine')\ndel title_embeddings","839e74ea":"def show_distance_samples(dist_type, distances, indices,  metric = '', n_items = 3, n_neighbors = 5, random = False):\n    for i in range(n_items): \n        if random: \n            i = np.random.randint(0, len(train))  \n        item_title = train.loc[indices[i, 0], 'title']\n        \n        plt.figure(figsize=(15,3))\n        plt.plot(np.arange(50), distances[i,],'o-')\n        plt.title('{} Distances From \"{}\" to Neighbors'.format(dist_type, item_title),size=16)\n        plt.ylabel('{} Distance'.format(metric),size = 20)\n        plt.xlabel('Closest 50 Neighbors',size = 15)\n        plt.show()\n\n        print(train.loc[indices[i, :n_neighbors],['title','label_group']] )","982ede90":"show_distance_samples('Title', title_distances, title_indices, 'Cosine')","a521656d":"def show_match_distances(title, distances, indices):\n    match_distances = []\n    for i in tqdm(range(distances.shape[0])):\n        for j in range(distances.shape[1]):\n            if train.loc[i, 'label_group'] == train.loc[indices[i, j], 'label_group']:\n                match_distances.append(distances[i, j])\n    if type(match_distances) != list:\n        match_distances = [dist for dist_list in match_distances for dist in dist_list]\n    plt.hist(match_distances, bins = 20)\n    plt.title('{} Neighbor Distance Distribution'.format(title), size = 20)\n    plt.show()\n    print(pd.DataFrame(match_distances, columns=['distances']).describe())\n    del match_distances\n\ndef get_match_distances(distances, indices):\n    match_distances = []\n    for i in tqdm(range(distances.shape[0])):\n        for j in range(distances.shape[1]):\n            if train.loc[i, 'label_group'] == train.loc[indices[i, j], 'label_group']:\n                match_distances.append(distances[i, j])\n    return match_distances\n\ndef show_match_distance_distribution(title, match_distances):\n    if type(match_distances) != list:\n        match_distances = [dist for dist_list in match_distances for dist in dist_list]\n    plt.hist(match_distances, bins = 20)\n    plt.title('{} Neighbor Distance Distribution'.format(title), size = 20)\n    plt.show()\n    print(pd.DataFrame(match_distances, columns=['distances']).describe())","7d38657a":"show_match_distances('Title', title_distances, title_indices)","51a31641":"target_dict = train.groupby('label_group')['posting_id'].agg('unique').to_dict()\ntrain['target'] = train['label_group'].map(target_dict)","7a330d79":"def get_f1_score(row, pred_col):\n    true_preds = len(np.intersect1d(row['target'], row[pred_col + '_pred']))\n    f1_score = (2 * true_preds) \/ (len(row['target']) + len(row[pred_col + '_pred']))\n           # = 2 * TP \/ ((2 * TP) + FN + FP) \n           # = 2 * TP \/ (TP + TP + FN + FP) \n           # = 2 * TP \/ ((TP + FN) + (TP + FP)) \n           # = 2 * TP \/ (len(target) + len(predictions))\n    return f1_score","9c438e11":"def get_predictions(data_df, distances, indices, max_distance):\n    predictions = []\n    for i in tqdm(range(distances.shape[0])): \n        pred_distances = np.where(distances[i] < max_distance)[0]\n        pred_indices = indices[i, pred_distances]\n        row_pred = data_df.iloc[pred_indices]['posting_id'].values.tolist()\n        predictions.append(row_pred)\n    return predictions\n\ndef show_max_distance_f1_scores(distances, indices, max_distances, pred_col):\n    for max_distance in max_distances:\n        train[pred_col + '_pred'] = get_predictions(train, distances, indices, max_distance)\n        train['f1_score_' + pred_col] = train.apply(get_f1_score, axis = 'columns', pred_col = pred_col)\n        print('{} max distance: {}, {} f1 score = {}'.format(pred_col, max_distance, pred_col, train['f1_score_' + pred_col].mean()))\n    train.drop(columns = 'f1_score_' + pred_col, inplace = True)","5497f102":"MAX_TITLE_DISTANCES = [0.46861,\n                       0.46862,\n                       0.46863,\n                       0.46864,\n                       0.46865]\n\nshow_max_distance_f1_scores(title_distances, title_indices, MAX_TITLE_DISTANCES, 'title')","43c808e7":"MAX_TITLE_DISTANCE = 0.46863 # best f1 score of 0.6610465706375579","f6c10dd9":"train['title_pred'] = get_predictions(train, title_distances, title_indices, MAX_TITLE_DISTANCE)\ndel title_distances, title_indices\n_ = gc.collect()\ntrain.head()","1ed707c0":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size, batch_size, path): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange(len(self.df))\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) \/\/ self.batch_size\n        ct += int(((len(self.df)) % self.batch_size) != 0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes), self.img_size, self.img_size, 3), dtype = 'float32')\n        df = self.df.iloc[indexes]\n        for i,(index, row) in enumerate(df.iteritems()):\n            img = cv2.imread(self.path + row)\n            X[i,] = cv2.resize(img, (self.img_size, self.img_size)) \n        return X","f3765bb3":"def get_image_embeddings(image_df, image_path):\n    image_embed_model = tf.keras.applications.EfficientNetB0( # image distances most evenly distributed among pretrained models\n        weights = '..\/input\/shopee-data\/efficientnetb0_notop.h5', # same as calling 'imagenet', but need file import because internet must be disabled\n        include_top = False, \n        pooling = 'avg', \n        input_shape = None)\n    \n    n_images = image_df.shape[0]\n    n_features = image_embed_model.layers[-1].output_shape[1] # size of final output layer\n    image_embeddings = np.zeros((n_images, n_features))\n    \n    input_img_size = 256 # input size of EfficientNetB0\n    BATCH_SIZE = 1024 * 4\n    EPOCHS = math.ceil(n_images \/ BATCH_SIZE)\n    for i in tqdm(range(EPOCHS)):\n        a = i * BATCH_SIZE\n        b = min((i + 1) * BATCH_SIZE, n_images)\n\n        image_gen = DataGenerator(df = image_df.iloc[a:b], img_size = input_img_size, batch_size = 32, path = image_path)\n        batch_embeddings = image_embed_model.predict(image_gen, verbose = 1, use_multiprocessing = True, workers = 4)\n        image_embeddings[a:b] = batch_embeddings\n    \n    return image_embeddings","00b321c6":"TRAIN_PATH = '..\/input\/shopee-product-matching\/train_images\/'\n\nimage_embeddings = get_image_embeddings(train['image'], TRAIN_PATH)\nimage_embeddings.shape","56cabc0a":"image_distances, image_indices = knn_predict_embeddings(image_embeddings, 'euclidean')\ndel image_embeddings","751fa0fb":"show_distance_samples('Image', image_distances, image_indices, 'Euclidean')","5368db5a":"show_match_distances('Image', image_distances, image_indices)","1e4d3ed3":"MAX_IMAGE_DISTANCES = [6.88564,\n                       6.88566,\n                       6.88568,\n                       6.88570,\n                       6.88572]\n\nshow_max_distance_f1_scores(image_distances, image_indices, MAX_IMAGE_DISTANCES, 'image')","7ba11ef3":"MAX_IMAGE_DISTANCE = 6.88568 # best f1 score of 0.6469574990306197","adee9c99":"train['image_pred'] = get_predictions(train, image_distances, image_indices, MAX_IMAGE_DISTANCE)\ndel image_distances, image_indices\n_ = gc.collect()\ntrain.head()","d45af6a8":"phash_dict = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['phash_pred'] = train.image_phash.map(phash_dict)\ndel phash_dict\n_ = gc.collect()\ntrain.head()","614abf0d":"def combine_preds_validation(row):\n    pred = np.concatenate([row['title_pred'], row['image_pred'], row['phash_pred']])\n    return np.unique(pred)\n\ndef combine_preds_submission(row):\n    pred = np.concatenate([row['title_pred'], row['image_pred'], row['phash_pred']])\n    return ' '.join(np.unique(pred)) # submission must be in 'pred_A pred_B pred_C ...' format","852898f5":"train['match_pred'] = train.apply(combine_preds_validation, axis = 'columns')\ntrain['f1_score'] = train.apply(get_f1_score, axis = 'columns', pred_col = 'match')\ntrain.head()","0bda2e15":"print('validation f1 score = {}'.format(train['f1_score'].mean()))","04574892":"test_neighbors = min(50, test.shape[0])\n\ntitle_embeddings = get_text_embeddings(test['title'])\n\ntitle_distances, title_indices = knn_predict_embeddings(title_embeddings, 'cosine', test_neighbors)\ndel title_embeddings\n\ntest['title_pred'] = get_predictions(test, title_distances, title_indices, MAX_TITLE_DISTANCE)\ndel title_distances, title_indices\n_ = gc.collect()\n\nTEST_PATH = '..\/input\/shopee-product-matching\/test_images\/'\n\nimage_embeddings = get_image_embeddings(test['image'], TEST_PATH)\n\nimage_distances, image_indices = knn_predict_embeddings(image_embeddings, 'euclidean', test_neighbors)\ndel image_embeddings\n\ntest['image_pred'] = get_predictions(test, image_distances, image_indices, MAX_IMAGE_DISTANCE)\ndel image_distances, image_indices\n_ = gc.collect()\n\nphash_dict = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['phash_pred'] = test.image_phash.map(phash_dict)\ndel phash_dict\n_ = gc.collect()\n\ntest['matches'] = test.apply(combine_preds_submission, axis = 'columns')\ntest.head()","4dacb9a7":"test[['posting_id','matches']].to_csv('submission.csv',index = False)","3d9a0921":"### Make Predictions and Choose Max Title Distance\nWe make predictions on various maximum distances and find the distance obtaining the highest f1 score.","14f2f6b2":"## RAPIDS Installation & Imports\nKaggle has the 0.16 version of RAPIDS installed, but this version's `fit` and `fit_transform` methods [do not support sparse cupy matrices](https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/230152) like more recent versions. It only supports dense matrices which require much more memory, so we install version 0.18 in order to save GPU memory.","08d6996c":"## Attach Title Predictions","29857868":"***\n# Image Predictions\n## Create Data Generator\nWe create a data generator to pass on the image data to our embedding model in batches to prevent memory errors.","a86f34cf":"# Examine Data\n## Load Data","09ff5876":"### Text Distance Samples\nWe check the distances of each neighbor from a number of samples.","611f8e8f":"## Check Duplicates","2c67c963":"# Setup RAPIDS\nWe setup RAPIDS by distributing the memory limits for TensorFlow and RAPIDS. Since we perform the KNN computations on the GPU, we distribute more memory to RAPIDS.\n## Setup GPU Memory","02186744":"### Extract Match Title Distances\nWe extract the distances of neighbors which are true duplicate matches of each product by grouping them according to each `label_group`.","8c9c1f87":"***\n# Get Test Predictions and Submit\nWe now perform the same process on the test set and submit these predictions to the competition. We are only provided 3 rows of data for the test set and will be provided the rest hidden upon submission. Thus, we adjust the number of neighbors accordingly.","07926e75":"# Imports","267b274e":"The maximum distance value of `0.46863` obtained the highest f1 score, so we use this distance for our final predictions.","24c29b49":"The maximum distance value of `6.88568` obtained the highest f1 score, so we use this distance for our final predictions.","aee77fff":"### Extract Match Image Distances\nWe extract the distances of neighbors which are true duplicate matches of each product by grouping them according to each `label_group`.","8fc68dc7":"## Determine Best Image Distance\nWe need to decide upon a maximum distance in order to predict whether a certain neighbor is a duplicate of the product or not. To do so we calculate the mean **f1 score,** which is this competition's evluation metric, of the image predictions based on different maximum distances.","1f974431":"### F1 Score Metric\nWe create a function to calculate the f1 score of each prediction.","80fe549a":"***\n# Compute Validation F1 Score\nWe combine the three predictions we have made to obtain a single match prediction to evalute our model.","87845c76":"### Setup Target Values\nWe setup the target values by grouping every product to each `label_group`.","cf8485bc":"## Attach Image Predictions","6b27e252":"### Top Duplicates","0faf618d":"## Get Image Distances with KNN\nLike what we did for the titles, we get the distances and indices of the 50 closest neighbors to each image embedding of a product's image. This time we use euclidean distance.","f00fc575":"## Determine Best Title Distance\nWe need to decide upon a maximum distance in order to predict whether a certain neighbor is a duplicate of the product or not. To do so we calculate the mean **f1 score,** which is this competition's evluation metric, of the title predictions based on different maximum distances.","88f3a959":"# Shopee - Detecting Product Similarity (Texts, Images, and Phash)\nThe task of this competition is to \"build a model that predicts which items are the same products.\" We tackle this task by implementing KNN to group similar products by their title and images, and [image phash](https:\/\/en.wikipedia.org\/wiki\/Perceptual_hashing). We also use RAPIDS to accelerate KNN computations on the GPU. A huge thanks to the authors of the following kernel as well.\n* Chris Deoette - [[PART 2] - RAPIDS TfidfVectorizer - [CV 0.700]](https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700\/notebook#Use-Image-Embeddings)","830c9a33":"## Extract Image Embeddings with Pretrained Model\nWe use the EfficientNetB0 model to extract image features as vectors to use for KNN. Since submissions must disable internet connection, we need to load the model weights as an input file.","1597faff":"## Check Samples","1a44eac3":"***\n# Phash Predictions\nWe use the [perceptual hashes](https:\/\/en.wikipedia.org\/wiki\/Perceptual_hashing) of the product images as duplicate predictions by grouping them together.","4b51e9ec":"***\n# Title Predictions\n## Extract Text Embeddings with Tf-idf Vectorizer\nWe use tf-idf [to scale down the impact of tokens that occur very frequently in a given corpus](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer). This helps the model to disregard less informative tokens and emphasize more informative ones.","66901106":"## Get Title Distances with KNN\nWe get the distances and indices of the 50 closest neighbors to each text embedding of a product's title. ([Group sizes were capped at 50, so there is no benefit to predict more than 50 matches](https:\/\/www.kaggle.com\/c\/shopee-product-matching\/overview\/evaluation)) Also, for text classification the cosine distance metric performs well since it disregards the magnitudes of the embeddings, meaning it is better at capturing similarities across varying lengths of text which is our case.","791569f6":"### Title Distance Samples\nWe check the distances of each neighbor from a number of samples."}}