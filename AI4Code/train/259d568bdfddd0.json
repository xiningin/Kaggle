{"cell_type":{"19c35573":"code","f0e9b4b1":"code","cdbd2a64":"code","89488bb2":"code","af2836ce":"code","abe458bd":"code","3f00c734":"code","55327955":"code","43419bb7":"code","b326ac5f":"code","4ebf25ac":"code","802b3250":"code","d4f2393a":"code","4cc9ce92":"code","52b8b0b7":"code","f1ee3b2b":"code","80b49a49":"code","cd7e9cbc":"code","dd90f42a":"code","bdc79f90":"code","e8a2c5bf":"code","b1131cf7":"code","60487724":"code","3ac6295e":"code","5289f057":"code","d3364d8f":"markdown","74fa0e93":"markdown","baa06233":"markdown","e92628f7":"markdown","c7580408":"markdown","3db534b3":"markdown","cb6218b2":"markdown","abf1ccd1":"markdown","e6f0c97e":"markdown","9f78ea59":"markdown","59a813c7":"markdown","3c967d63":"markdown","7a8d71c0":"markdown","9c1d19b4":"markdown","20dd72bb":"markdown","26648269":"markdown","31c95780":"markdown","abdb84e6":"markdown","3bdc1339":"markdown","37941c55":"markdown","9c46ada2":"markdown","d0aa3d02":"markdown","28cb77a7":"markdown","d9cd10e2":"markdown","d3001c24":"markdown","b7cc3286":"markdown","458477e2":"markdown","c72b6a99":"markdown","adebdece":"markdown","ec7c577d":"markdown","7bd7e750":"markdown","26172413":"markdown","2876b775":"markdown","ecbab2ca":"markdown"},"source":{"19c35573":"import os\nimport sys\n\n# Data treatment\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# Model\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\nfrom scipy.stats import norm\nfrom scipy.stats import t\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\n\n!pip install arch\nfrom arch import arch_model\n\n!pip install git+git:\/\/github.com\/khrapovs\/skewstudent\nfrom skewstudent import SkewStudent\n\n!pip install copulas\n\n    \n# Goodness of Fit test\nfrom scipy import stats\nfrom scipy.stats import kstest\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom astropy.stats import kuiper\n\n# Graph\nimport seaborn as sns\nfrom itertools import permutations \n\n# Optimization\n!pip install cvxopt\nimport cvxopt as opt\nfrom cvxopt import blas, solvers\n\n# Plot\nimport plotly\nimport cufflinks\n# (*) To communicate with Plotly's server, sign in with credentials file\nimport chart_studio.plotly as py  \n# (*) Useful Python\/Plotly tools\nimport plotly.tools as tls   \n# (*) Graph objects to piece together plots\nfrom plotly.graph_objs import *","f0e9b4b1":"hpimsa=pd.read_csv(\"..\/input\/hpi-20msa\/Totalseries_msa_cshpi_extended_2020.csv\")","cdbd2a64":"# Look into the name of MSAs \nhpimsa.columns\n\n# Describe the data and change the name of MSAs\nhpimsa.describe()\nhpimsa.rename(columns={\"Unnamed: 21\": \"US\", 'Effective date ': \"date\", ' AZ-Phoenix ': \"Phoenix\", 'CA-Los Angeles ': \"Los Angeles\", 'CA-San Diego ': \"San Diego\",'CA-San Francisco ': \"San Francisco\",  ' NV-Las Vegas ': \"Las Vegas\", 'FL-Miami ':\"Miami\", 'FL-Tampa ':\"Tampa\"  }, inplace=True)\nhpimsa.drop(\"US\", axis=1, inplace=True)\nhpimsa.head()","89488bb2":"# Inspect the first 5 rows of November-December 2006\n\nhpimsa[\"date2\"]=pd.to_datetime(hpimsa[\"date\"])\nnew_hpimsa=hpimsa.loc[hpimsa[\"date2\"]>=pd.Timestamp(\"2000-01-01\")]\nnew_hpimsa.head()\n","af2836ce":"# Monthly returns using method 1\nmonthly_r = pd.DataFrame(new_hpimsa.iloc[:,1:-1].pct_change())\n\n# Monthly returns using method 2\nmonthly_r2 = np.log(new_hpimsa.iloc[1:,1:-1])-np.log(new_hpimsa.iloc[1:,1:-1].shift(1))\n\n# Number of na in data\nprint(monthly_r.isna().sum())\nprint(monthly_r2.isna().sum())\n\n# Replace NA values with 0\nmonthly_r.fillna(0, inplace=True)\nmonthly_r2.fillna(0, inplace=True)\n\n# show part of monthly returns\nmonthly_r.head()\n\n# put date again\nnew_monthly_r= pd.concat([new_hpimsa[[\"date2\"]], monthly_r], axis=1)\nnew_monthly_r2= pd.concat([new_hpimsa[[\"date2\"]], monthly_r2], axis=1)\n\n","abe458bd":"new_monthly_r2.head()","3f00c734":"# set index with date2\nnew_monthly_r.index=new_monthly_r[\"date2\"]\nnew_monthly_r2.index=new_monthly_r2[\"date2\"]\nnew_monthly_r2 = new_monthly_r2.iloc[1:,:]\n\n\n# Resample to quarter level (by taking average of returns)\nqtr_r = new_monthly_r.resample('4M').mean()\n\n# Print `monthly_aapl`\nprint(qtr_r)\n\n# Resample to quarter level (using the last observation)\nqtr_r_last = new_monthly_r.resample('4M', convention='end')\n\n# Print `monthly_aapl`\nprint(qtr_r_last)\n\n","55327955":"# Import Matplotlib's `pyplot` module as `plt`\nimport matplotlib.pyplot as plt\n\n# Plot the closing prices for `aapl`\nqtr_r.plot(grid=True, figsize=(12,8))\nplt.title(\"Quarterly MSA returns\")\nplt.legend(title='MSA', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Return\")\n\n# Show the plot\nplt.show()","43419bb7":"# Calculate the moving average\nmoving_avg = new_monthly_r.rolling(window=4).mean()\n\n# Inspect the result\nmoving_avg[-10:]","b326ac5f":"# Import Matplotlib's `pyplot` module as `plt`\nimport matplotlib.pyplot as plt\n\n# Plot the closing prices for `aapl`\nmoving_avg.plot(grid=True, figsize=(12,8))\nplt.title(\"4 months moving averages of HPI returns\")\nplt.legend(title='MSA', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Return\")\n\n# Show the plot\nplt.show()","4ebf25ac":"# Define the minumum of periods to consider \nmin_periods = 4 \n\n# Calculate the volatility\nvol = new_monthly_r.rolling(window = min_periods).std() \n\n# Plot the volatility\nvol.plot(figsize=(10, 8))\n\nplt.title(\"Volatilities (stdev) of 4 months moving averages of HPI returns\")\nplt.legend(title='MSA', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"stdev\")\n\n\n# Show the plot\nplt.show()","802b3250":"# Dickey\u2013Fuller test\n\ndef DFtest(name_msa):\n    \n    result = adfuller(new_monthly_r[name_msa], regression='c') \n    \n    print('ADF Statistic: {}'.format(result[0]))\n    print('p-value: {}'.format(result[1]))\n    print('Critical Values:')\n    \n    for key, value in result[4].items():\n        print('\\t{}: {}'.format(key, value))\n    \n    res= result[1] \n    return   res\n\n# DF test for each series of HPI return of MSAs\nset_name_msa= list(new_monthly_r.columns)[1:]\nset_pvalue=[]\n\nfor name in set_name_msa:   \n    res= DFtest(name)\n    set_pvalue.append(res)\n\nprint(set_pvalue)\n\n","d4f2393a":"# pvalues od DF test\ndat = pd.DataFrame(set_pvalue)\n\n# set index\ndat.set_index\n\n# Select the MSA whose HPI returns are stationary (selected 10 MSAs)\nidc = dat.loc[dat.iloc[:,0]<=0.1].index\n\n# Extract the monthly hpi returns of those MSAs\nstat_monthly_r=new_monthly_r.iloc[:,idc+1]\nstat_monthly_r.head()","4cc9ce92":"## Definition of the function 'ARIMA modeling'\n\ndef search_best_lags(num_ar, num_ma, msaname):\n    \n       \n    tmp_mdl = ARIMA(stat_monthly_r[msaname], order=(num_ar,0,num_ma)).fit(disp=0,maxiter=100)\n    tmp_aic = tmp_mdl.aic\n\n        \n    return tmp_aic, num_ar, num_ma, tmp_mdl      \n\n                \nset_name_msa= list(stat_monthly_r.columns)\nlist_arlag = list(range(1,14))\nlist_malag = list(range(1,2))       \nlist_arma=np.array(np.meshgrid(list_arlag,list_malag)).T.reshape(-1,2)\n\n\ndef calcul_arima(name): \n    \n    res=list(map(lambda x,y: search_best_lags(x,y,name), list_arma[:,0],list_arma[:,1]))\n    sub_res=[]\n    \n    for i in range(0,len(res)): \n        sub_res.append(res[i][0])\n\n    idx=sub_res.index(min(sub_res)) # select the index for the lowest AIC value\n    best_mdl= res[idx][3] \n    best_aic= best_mdl.aic\n    best_order_ar=res[idx][1]\n    best_order_ma=res[idx][2]\n    \n    return best_aic, best_order_ar, best_order_ma, best_mdl\n\n## Application of the fuction above to all MSAs\n\nresid_msa=stat_monthly_r.copy()\nset_name_msa= list(stat_monthly_r.columns)\n\nset_info=[]\n\nfor name in set_name_msa:   \n    \n    best_aic, best_order_ar, best_order_ma, best_mdl = calcul_arima(name)\n    set_info.append([best_aic, best_order_ar, best_order_ma, best_mdl])\n    residseries= pd.DataFrame(best_mdl.resid)\n    resid_msa[name]=residseries\n    \n\n\n","52b8b0b7":"# We fit the GARCH(1,1) model and using AIC, find the best distribution of residuals.\n\ndef get_best_model(num_ar, num_ma, distt, msaname):    \n   \n    tmp_mdl = arch_model(resid_msa[msaname], p=num_ar, o=0, q=num_ma, dist=distt).fit(update_freq=5, disp='off') # modeling\n    tmp_aic = tmp_mdl.aic\n    \n    return tmp_aic, num_ar, num_ma, distt, tmp_mdl    \n\n\nset_name_msa= list(stat_monthly_r.columns)\nlist_arlag = list(range(1,10))\nlist_malag = list(range(1,10))\nd_rng = list(['Normal', 'StudentsT', 'skewt'])\nlist_armadist = np.array(np.meshgrid(list_arlag,list_malag,d_rng)).T.reshape(-1,3)\narlag = list(map(lambda x: list_armadist[:,0].astype(int)[x].item(), range(1, len(list_armadist[:,0]))))\nmalag = list(map(lambda x: list_armadist[:,1].astype(int)[x].item(), range(1, len(list_armadist[:,1]))))\n\n\ndef calcul_garch(name): \n    \n    res=list(map(lambda x,y,z: get_best_model(x,y,z, name), arlag, malag, list_armadist[:,2]))\n \n    sub_res=[]\n    \n    for i in range(0,len(res)): \n        sub_res.append(res[i][0])\n\n    idx=sub_res.index(min(sub_res)) # select the index for the lowest AIC value\n    best_mdl= res[idx][4] \n    best_aic= best_mdl.aic\n    best_order_ar=res[idx][1]\n    best_order_ma=res[idx][2]\n    best_dist=res[idx][3]\n    \n    return best_aic, best_order_ar, best_order_ma, best_dist, best_mdl\n\n\n\n       \n# For all the MSAs, we fit the GARCH(1,1) model and find the best distribution of residuals.\n\nresid_msa_degarch=stat_monthly_r.copy()\ndistset=[]\nPIT = stat_monthly_r.copy()\nset_name_msa= list(stat_monthly_r.columns)\n\nfor name in set_name_msa:   \n    aic, order_ar, order_ma, dist, mdl = calcul_garch(name)\n    resid_msa_degarch[name] = mdl.std_resid\n    distset.append(dist)\n    stdzd_resid = mdl.std_resid\n    \n    if dist == 'Normal':\n        PIT[name] = norm.cdf(stdzd_resid)\n    elif dist ==  'StudentsT':\n        PIT[name] = t.cdf(stdzd_resid, mdl.params[-2])\n    elif dist== 'skewt': # skewt distribution\n        skewt = SkewStudent(eta = mdl.params[-2], lam = mdl.params[-1])\n        PIT[name] = skewt.cdf(stdzd_resid)\n       \n\n    \n        \nprint(resid_msa_degarch)\n","f1ee3b2b":"# Plotting residuals de-GARCHed \n\ndef tsplot(y, lags=None, figsize=(10, 8), style='bmh'):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title('Time Series Analysis Plots')\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n    return \n\n","80b49a49":"# KS test\n\nres_ks={}\n\nfor name in resid_msa_degarch.columns:\n    sPIT=PIT[name]\n    res_ks[name]= kstest(sPIT, \"uniform\").pvalue\n    \nres_ks","cd7e9cbc":"# Kuiper\u2019s test (KP)\n\nres_kp={}\n\nfor name in resid_msa_degarch.columns:\n    sPIT=PIT[name]\n    res_kp[name]=kuiper(sPIT)[0]\n\nres_kp","dd90f42a":"# Ljung Box test \n\nljbox_stat ={}\n\nfor name in resid_msa_degarch.columns:\n    stdResid=resid_msa_degarch[name]\n    ljbox_stat[name]=acorr_ljungbox(stdResid**2, lags=[1,2,3,4,5])[1]\n\nljbox_stat","bdc79f90":"# Correlation (Pearson's correlation)\n\ndfcorr= new_monthly_r.iloc[:,1:].corr()\nf= plt.figure(figsize=(10, 10))\n\nax1=plt.matshow(dfcorr, fignum=f.number)\nplt.xticks(range(dfcorr.shape[1]), dfcorr.columns, fontsize=8, rotation=45)\nplt.yticks(range(dfcorr.shape[1]), dfcorr.columns, fontsize=8)\nttl=plt.title('Correlation Matrix', fontsize=15);\nttl.set_position([.5, 1.1])\n\n\n\ncbar_ax = f.add_axes([1.005, 0.15, 0.04, 0.7])\n\ncb = plt.colorbar(cax=cbar_ax)\ncb.ax.tick_params(labelsize=10)\n\n\n","e8a2c5bf":"## Froming an equally weighted portfolio\n# The variation in the return and volatility of this portfolio \n\ndef eqw_portfolio(returns):\n    \n    ''' \n    Returns the mean and standard deviation of returns for an equally weighted portfolio\n    \n    '''\n\n    p = np.asmatrix(np.mean(returns, axis=0))\n    w = np.asmatrix([1\/len(returns.columns)]*len(returns.columns))\n    C = np.asmatrix(np.cov(returns.T))\n    \n    mu = w * p.T\n    sigma = np.sqrt(w * C * w.T)\n    \n    return mu, sigma\n\n\ndf = new_monthly_r.iloc[:,1:]\n \nset_mu=[]\nset_sig=[]\nset_date=[]\n\nfor i in range(len(df)-13):\n    \n    returns = df.iloc[1+i:1+i+12,:]\n    mu, sigma = eqw_portfolio(returns)\n    set_mu.append(float(mu))\n    set_sig.append(float(sigma))\n    set_date.append(new_monthly_r.index[1+i+12])\n\n\n\n\n# Plot the mean of the equally weighted portfolio\nmean_port = pd.concat([pd.DataFrame({\"date\":set_date}),pd.DataFrame({\"mean\":set_mu})],axis=1)\nmean_port.date = pd.to_datetime(mean_port[\"date\"])\nmean_port.set_index(\"date\", inplace=True)\nmean_port.plot()\n\nplt.title(\"Annual return of the equally weighted portfolio\")\nplt.legend(title='returns', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"returns\")\n\n\n\n# Plot the stdev of the annual return of the equally weighted portfolio\nstd_port = pd.concat([pd.DataFrame({\"date\":set_date}),pd.DataFrame({\"std\":set_sig})],axis=1)\nstd_port.date = pd.to_datetime(std_port[\"date\"])\nstd_port.set_index(\"date\", inplace=True)\nstd_port.plot()\n\nplt.title(\"Volatility of the annual return of the equally weighted portfolio\")\nplt.legend(title='volatility', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"volatility\")\n\n","b1131cf7":"## Find the efficient portfolio using Markowitz portfolio theory (Modern portfolio theory)\n\ndef optimal_portfolio(returns):\n    n = len(returns.T)\n    returns = np.array(returns)\n    \n    N = 100\n    mus = [(10**(5.0 * t\/N - 1.0))\/100 for t in range(N)]\n   \n    \n    # Convert to cvxopt matrices\n    S = opt.matrix(np.cov(returns.T))\n    pbar = opt.matrix(np.mean(returns, axis=0))\n    \n    # Create constraint matrices (inequality such as <0 - G,h, equality such as =0, A,b)\n    G = -opt.matrix(np.eye(n))   # Negative n x n identity matrix. w_0, w_1, ... w_n>0\n    h = opt.matrix(0.0, (n ,1))\n    A = opt.matrix(1.0, (1, n))  # w_0 + w_1 + ... + w_n=1\n    b = opt.matrix(1.0)\n    \n    # Calculate efficient frontier weights using quadratic programming\n    portfolios = [solvers.qp(mu*S, -pbar, G, h, A, b)['x'] \n                  for mu in mus]\n    \n    ## Calculate risks and returns for frontier \n    returns = [blas.dot(pbar, x) for x in portfolios]\n    risks = [np.sqrt(blas.dot(x, S*x)) for x in portfolios]\n    \n    ## Calculate the 2nd degree polynomial of the frontier curve \n    m1 = np.polyfit(returns, risks, 2)\n    x1 = np.sqrt(m1[2] \/ m1[0]) # coeff of x^2 \/ coeff of C\n    \n    # Calculate the optimal portfolio \n    try:\n        wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n    except (ArithmeticError, ValueError):\n        wt = 'NA'\n        pass\n    \n    return np.asarray(wt), returns, risks\n\ndf = new_monthly_r.iloc[:,1:]\n\nset_w_opt=[]\nset_mu_opt=[]\nset_sig_opt=[]\nset_date_opt=[]\n\n\nfor i in range(len(df)-13):\n\n    \n\n    return_vec = df.iloc[1+i:1+i+12,:]\n    weights, returns, risks = optimal_portfolio(return_vec)\n\n    if (weights != 'NA'):\n        \n        meanr=np.asmatrix(np.mean(return_vec, axis=0))\n        weights=np.asmatrix(weights) \n        covm= np.asmatrix(np.cov(return_vec.T)) \n        \n        set_w_opt.append(weights)\n        set_mu_opt.append(float(meanr * weights))\n        set_sig_opt.append(float(np.sqrt(weights.T * covm * weights)))\n        set_date_opt.append(new_monthly_r.index[1+i+12])\n    \n","60487724":"# Plot the mean of the equally weighted portfolio\nmean_port_opt = pd.concat([pd.DataFrame({\"date\":set_date_opt}),pd.DataFrame({\"mean_opt\":set_mu_opt})],axis=1)\nmean_port_opt.date = pd.to_datetime(mean_port_opt[\"date\"])\nmean_port_opt.set_index(\"date\", inplace=True)\nmean_port_opt.plot()\n\nplt.title(\"Annual return of the mean variance optimal portfolio\")\nplt.legend(title='returns', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"returns\")\n\n\n\n# Plot the stdev of the annual return of the equally weighted portfolio\nstd_port_opt = pd.concat([pd.DataFrame({\"date\":set_date_opt}),pd.DataFrame({\"std_opt\":set_sig_opt})],axis=1)\nstd_port_opt.date = pd.to_datetime(std_port_opt[\"date\"])\nstd_port_opt.set_index(\"date\", inplace=True)\nstd_port_opt.plot()\n\nplt.title(\"Volatility of the annual return of the mean variance optimal portfolio\")\nplt.legend(title='volatility', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"volatility\")\n    \n","3ac6295e":"# The returns\nmean_port_together= pd.concat([mean_port, mean_port_opt[\"mean_opt\"]], axis=1)\nmean_port_together.plot()\n\n\nplt.title(\"Annual return of the portfolios\")\nplt.legend(title='returns', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"returns\")\n\n\n# The stds\nstd_port_together= pd.concat([std_port, std_port_opt[\"std_opt\"]], axis=1)\nstd_port_together.plot()\n\nplt.title(\"Volatility of the annual return of the portfolios\")\nplt.legend(title='volatility', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"volatility\")\n    ","5289f057":"# Drwo shapre ratios of two portfolios \nport_together= pd.concat([mean_port, mean_port_opt[\"mean_opt\"], std_port, std_port_opt[\"std_opt\"]], axis=1)\nport_together[\"shR1\"] = port_together[\"mean\"]\/port_together[\"std\"]\nport_together[\"shR2\"] = port_together[\"mean_opt\"]\/port_together[\"std_opt\"]\n\nport_together2 = port_together[[\"shR1\",\"shR2\"]]\nport_together2.set_index=port_together.index\nport_together2.plot()\n\n\nplt.title(\"Sharpe ratio\")\nplt.legend(title='Sharpe ratio', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='x-small')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Sharpe ratio\")\n","d3364d8f":"<a id='importingdata'><\/a>\n# Importing HPI data of the greatest 20 MSAs in the US\n\n## Importing Data\n\n* We import a data set that include house price indices of the 20 biggest Metropolitan Statistical Area (MSA) in the US.\n   - Time span is from 1990 to 2019.","74fa0e93":"* `Comment`: we found less peaky returns than the quarterly based (calculated) returns","baa06233":"<a id='movingwindow'><\/a>\n## Moving windows","e92628f7":"* In order to take into consideration the means and stds at the same time, we look into the sharpe ratio of two portfolio making strategies.","c7580408":"* `Comment`\n\n    - We can see that the comovement of house prices in southern california area (LA, SF, SD, LV) is more distinct thant the one of other regions.\n    - We can also see that the HPI returns of Phoenix (AZ), Miami and Tampa (Florida) area also comove with the ones of those regions.\n    - If you are thinking of putting your money in the portfolio composed of the mortgages originated in the US, you might compose your portfolio with mortgages from southern california area only.\n\n","3db534b3":"* Since Dallas issued hpi data from 2000, we only use the subset of the original data dating from 2000 to 2020","cb6218b2":"### Summary of results (Goodness of Fitness)\n","abf1ccd1":"* The results of the `Goodness of Fit` tests.\n    - `Kolmogorov-Smirnov (KS)`: the results show that the PITs calculated using the standardized residuals from the GRACH, follow a Uniform [0,1].\n    - `Kuiper\u2019s test (KP)` results also give us the same results (we can not reject the null hypothesis that the PITs follow a Unform [0,1] at the 5% level of significance. \n    - We did `Ljung-Box test` for lack of autocorrelation in the squared standardized residuals of the GARCH(1,1) models at orders 1, 2, 3, 4 and 5. The results show that there is no autocorrelation in the squared standardized residuals of the GARCH modes at each order for all series except Atlanta**","e6f0c97e":"### Goodness of Fit\n\n* We now check the `Goodness of Fit`.\n  - If the data is well fitted, the distribution (cdf) of the residual (PIT) should follow the uniform distribution.\n  - We do some `Goodness of Fit (GoF)` test. \n    - [`Kolmogorov-Smirnov (KS)`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.kstest.html): this test evaluates departures from the null hypothesis that the cumulative density function (cdf) of                                the marginal models follow a Uniform [0,1].    \n    - [`Kuiper\u2019s test (KP)`](https:\/\/docs.astropy.org\/en\/stable\/api\/astropy.stats.kuiper.html) for uniformity: This test puts more weight on the tails of the distribution.  \n    - [`Ljung-Box test`](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.stats.diagnostic.acorr_ljungbox.html): for lack of autocorrelation in the squared standardized residuals of the GARCH(1,1) models at orders 1, 2, 3, 4 and 5.","9f78ea59":"### ARMA modeling ","59a813c7":"* With the stationary HPI return series, we model (or predict) the time variation of the means of hpi returns using ARMA model\n\n    - We search the best lags for the AR and MA terms here.","3c967d63":"<a id='compareportfolios'><\/a>\n# Compare different investing strategies\n* Return and volatility\n* Risk measures\n    - Sharpe ratio","7a8d71c0":"<a id='dependence'><\/a>\n## Dependence analysis\n\n* [Linear correlation (Pearson's correlation)](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.pearsonr.html)\n    - [mapping](https:\/\/stackoverflow.com\/questions\/29432629\/plot-correlation-matrix-using-pandas)\n* [Dependence with copula](https:\/\/pypi.org\/project\/copulas\/)\n    - The reason why we do the Goodness of Fit tests above is to estimate the correlation (dependence) between HPI using copula functions and the PITs","9c1d19b4":"<a id='visualizationdat'><\/a>\n# Visualization of HPI data\n\n* <a id='return'><\/a>\n## Return","20dd72bb":"# Analyzing U.S. House price index data \n\n\n* [Importing HPI data of the greatest 20 MSAs in the US ](#importingdata)\n\n* [Visualization of HPI data](#visualizationdat) \n\n    - [Returns](#return)\n    - [Moving Windows](#movingwindow)\n    - [Volatility](#volatility) \n    - [Marginal distribution of HPI returns](#marginals)\n    - [Dependence (correlation) between HPI returns](#dependence) \n    \n    \n* [Building mortgage portfolio making strategies](#makingportfolios)\n\n    - [Equally weighted portfolio](#equallyweighted)\n    - [Optimally weighted portfolio (Markowitz portfolio theory)](#optimalportfolios)\n    \n\n* [Compare the returns of two different investing strategies](#compareportfolios)\n\n   - [Sharpe Ratio](#sharpe)\n\n* [Conclusion](#conclusion)\n","26648269":"<a id='sharpe'><\/a>    \n\n### Sharpe ratio","31c95780":"### Compare the returns and volatilities of two portfolio making strategy","abdb84e6":"* We found that the equally weighted portfolio performed as well as the optimally weighted portfolio in terms of Sharpe ratio before the subprime mortgage crisis.\n* However, we can see that the optimally weighted portfolio perform better since the 2008 financial crisis for most of periods. ","3bdc1339":"* To analyse (or predict) the timevariations in the volatilities of hpi returns, we use the GARCH model.\n    - We search for the best lag for the GARCH terms here.\n    - We also search for the best distribution of residuals and fit the model with the best distribution.\n","37941c55":"* We make a graph for the time variations in the hpi returns.\n    - the boom of housing market in the US occurred in several MSAs from 2004 to 2006.\n    - the collapse in hpi return happened for all the regions in the USA almost at the same time from 2007.","9c46ada2":"* We look into the name of MSAs\n* We unified the format of the name of MSAs and drop some columns that won't be used in this analysis.","d0aa3d02":"* Frist, we calculate the monthly HPI returns for each MSA","28cb77a7":"<a id='optimalportfolios'><\/a>    \n## Optimally weighted portfolio \n\n* For more information on the fct `solver.qp()` in `cvxopt` used here to find the optimal portfolio using Markowitz portfolio theory, see [this.](https:\/\/courses.csail.mit.edu\/6.867\/wiki\/images\/a\/a7\/Qp-cvxopt.pdf)","d9cd10e2":"* Then, we change the frequency of hpi data from `monthly` to `quarterly`.","d3001c24":"<a id='equallyweighted'><\/a>\n\n## Equally weighted Portfolio","b7cc3286":"<a id='volatility'><\/a>\n## Volatility","458477e2":"<a id='makingportfolios'><\/a>\n# Building mortgage portfolio making strategies\n\n#### We assume that we took one of those two strategy to make a portfolio from 2000.\n\n* Equally weighted portfolios\n    - We took this strategy every year\n\n* [Efficient portfolio (mean variance analysis)](https:\/\/plotly.com\/python\/v3\/ipython-notebooks\/markowitz-portfolio-optimization\/)\n    - We took this strategy every year\n\n\n#### What are the returns of and volatilities of these two portfolios since then? \n","c72b6a99":"* We calculate moving averages of HPI returns (4 months) and compare this with quarterly returns","adebdece":"* Figures (mean and volatility of the optimally weighted portfolio)","ec7c577d":"* We looked into the time variations in the volatility of house price returns, and found that the volatility of house price returns increased duing the subprime mortgage crisis.","7bd7e750":"### Linear correlation\n\n* Static correlation ","26172413":"<a id='conclusion'><\/a>\n# Conclusion","2876b775":"* Time varying mean and volatilities of the series can be modeled with ARMA-GARCH models, and this model can be applied with stationary series.\n* Thus, before doing the ARMA modeling, we first do the [Dickey Fuller test](https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.stattools.adfuller.html) to check the stationarity of the monthly HPI returns.\n* For more details on the ARIMA model, [see this reference.](https:\/\/machinelearningmastery.com\/arima-for-time-series-forecasting-with-python\/) ","ecbab2ca":"<a id='marginals'><\/a>\n## Marginal distributin of returns\n\n* We analyze the marginal distribution of the MSA HPI returns\n\n    -[ARMA modeling](https:\/\/towardsdatascience.com\/machine-learning-part-19-time-series-and-autoregressive-integrated-moving-average-model-arima-c1005347b0d7)\n    \n    -[GARCH modeling](https:\/\/medium.com\/@ranjithkumar.rocking\/time-series-model-s-arch-and-garch-2781a982b448)\n"}}