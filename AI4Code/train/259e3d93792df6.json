{"cell_type":{"eba7f387":"code","79742ef4":"code","558f8a2a":"code","65e9c176":"code","ad7549d7":"code","bbc3fdf8":"code","8e3a6a2c":"code","bbee3d9f":"code","afa953ca":"code","5b39770f":"code","e9374a3b":"code","c8e04441":"code","6344d0b3":"code","f7b0991a":"markdown","02ea60b9":"markdown","ab9854e2":"markdown","921d6f54":"markdown","7422dd58":"markdown","b1edbf61":"markdown","7d706aad":"markdown","5f25947a":"markdown","71d67dc3":"markdown","e7d68c12":"markdown","5f2d69cf":"markdown","a539380e":"markdown","d08d334a":"markdown","ba7467d2":"markdown"},"source":{"eba7f387":"import random\n\nimport numpy as np\nfrom numpy.linalg import inv\n\nimport pandas as pd\n\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy.optimize import differential_evolution\nfrom scipy import linalg\n\nimport matplotlib.pyplot as plt","79742ef4":"path = '..\/input\/city-taxi-trip-pricing-and-distances\/Cab_Data.csv'\ndf = pd.read_csv(path)\ndf = shuffle(df)\ndf.head()","558f8a2a":"df = df[['KM Travelled', 'Price Charged','Cost of Trip']]\ndf.head()","65e9c176":"scaler = MinMaxScaler()\nscaler.fit(df)\ndf = scaler.transform(df)","ad7549d7":"X = df[:,:-1]\ny = df[:,-1]\n\nX_train, X_test, y_train, y_test = train_test_split(\n                                        X, y, test_size=0.999, random_state=42)","bbc3fdf8":"class KernelRidge():\n    \"\"\"\n        Simple implementation of a Kernel Ridge Regression using the\n        closed form for training.\n        Doc: https:\/\/www.ics.uci.edu\/~welling\/classnotes\/papers_class\/Kernel-Ridge.pdf\n    \"\"\"\n\n    def __init__(self, kernel='linear', alpha=1.0, gamma=5.0):\n        \"\"\"\n        :param kernel_type: Kernel type to use in training.\n                        'linear' use linear kernel function.\n                        'quadratic' use quadratic kernel function.\n                        'gaussian' use gaussian kernel function\n        :param alpha: Value of regularization parameter C\n        :param gamma: parameter for gaussian kernel or Polynomial kernel\n        \"\"\"\n        self.kernels = {\n            'linear': self.kernel_linear,\n            'quadratic': self.kernel_quadratic,\n            'gaussian': self.kernel_gaussian\n        }\n        self.kernel_type = kernel\n        self.kernel = self.kernels[self.kernel_type]\n        self.C = alpha\n        self.gamma = gamma\n\n    # Define kernels\n    def kernel_linear(self, x1, x2):\n        return np.dot(x1, x2.T)\n\n    def kernel_quadratic(self, x1, x2):\n        return (np.dot(x1, x2.T) ** 2)\n\n    def kernel_gaussian(self, x1, x2, gamma=5.0):\n        gamma = self.gamma\n        return np.exp(-linalg.norm(x1 - x2) ** 2 \/ (2 * (gamma ** 2)))\n\n    def compute_kernel_matrix(self, X1, X2):\n        \"\"\"\n        compute kernel matrix (gram matrix) give two input matrix\n        \"\"\"\n\n        # sample size\n        n1 = X1.shape[0]\n        n2 = X2.shape[0]\n\n        # Gram matrix\n        K = np.zeros((n1, n2))\n        for i in range(n1):\n            for j in range(n2):\n                K[i, j] = self.kernel(X1[i], X2[j])\n\n        return K\n\n\n    def fit(self, X, y):\n        \"\"\"\n        training KRR\n        :param X: training X\n        :param y: training y\n        :return: alpha vector, see document TODO\n        \"\"\"\n        K = self.compute_kernel_matrix(X, X)\n\n        self.alphas = np.dot(inv(K + self.C * np.eye(np.shape(K)[0])),\n                        y.transpose())\n\n        return self.alphas\n\n    def predict(self, x_train, x_test):\n        \"\"\"\n        :param x_train: DxNtr array of Ntr train data points\n                        with D features\n        :param x_test:  DxNte array of Nte test data points\n                        with D features\n        :return: y_test, D2xNte array\n        \"\"\"\n\n        k = self.compute_kernel_matrix(x_test, x_train)\n\n        y_test = np.dot(k, self.alphas)\n        return y_test.transpose()","8e3a6a2c":"def kkr_function(hyperparams, X, y, kkr_type, printing):\n    alpha_value,gamma_value = hyperparams\n    kf = KFold(n_splits=3,shuffle=True,random_state=2020)\n    rmses = []\n    \n    # kf-fold cross-validation loop\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        kkr = KernelRidge(kernel=kkr_type,alpha=alpha_value,gamma=gamma_value)\n        alpha = kkr.fit(X_train, y_train)\n        y_hat = kkr.predict(X_train, X_test)\n        \n        loss = np.sqrt(mean_squared_error(y_hat, y_test))\n        rmses.append(loss)\n    \n    if(printing):\n        print('alpha:{:3.3f}, gamma:{:3.3f}, rmse:{:3.3f}'\n                  .format(alpha_value, gamma_value, rmses[-1]))\n    \n    rmse = np.sum(rmses) \/ len(rmses)\n    del rmses\n    \n    return rmse","bbee3d9f":"krr_alpha_lim = (0.00001,100.0)\nkrr_gamma_lim = (0.00001,20.0)\n\nboundaries = [krr_alpha_lim] + [krr_gamma_lim]","afa953ca":"def train_evolution(X, y, kkr_type, printing):\n\n    extra_variables = (X, y, kkr_type, printing)\n\n    solver = differential_evolution(kkr_function,boundaries,args=extra_variables,strategy='best1bin',\n                                    popsize=10,mutation=0.5,recombination=0.7,tol=0.01,seed=2020)\n\n    best_hyperparams = solver.x\n    best_rmse = solver.fun\n\n    alpha = best_hyperparams[0]\n    gamma = best_hyperparams[1]\n\n    print(\"Type:{:}\".format(kkr_type))\n    print(\"Converged Hyperparameters: alpha:{:.6f}, gamma:{:3.6f}\"\n             .format(alpha, gamma))\n    print(\"Minimum rmse:{:3.6}\".format(best_rmse))\n    print(\"-\" * 55)\n    \n    return best_rmse, alpha, gamma","5b39770f":"results = {}\nfor kkr_type in ['linear', 'quadratic', 'gaussian']:\n    rmse, alpha, gamma = train_evolution(X_train, y_train, kkr_type, False)\n    results[kkr_type] = rmse, alpha, gamma","e9374a3b":"X, X_val, y, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)","c8e04441":"models = {}\nfor k in results:\n    alpha = results[k][1]\n    beta = results[k][2]\n\n    kkr = KernelRidge(kernel=k,alpha=alpha,gamma=gamma)\n    _ = kkr.fit(X, y)\n    \n    models[k] = kkr","6344d0b3":"predicts = {}\nfor k in models:\n    predicts[k] = models[k].predict(X, X_val)\n\nplt.figure(figsize=(14,8))\n_ = plt.plot(predicts['linear'], 'bo', label='linear')\n_ = plt.plot(predicts['quadratic'], 'co', label='quadratic')\n_ = plt.plot(predicts['gaussian'], 'mo', label='gaussian')\n_ = plt.plot(y_train[:108], 'go', label='actual')\nplt.legend()\nplt.show()","f7b0991a":"## Kernel Ridge Hyperparameters","02ea60b9":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/6960\/logos\/header.png\"\/>\n<\/div>","ab9854e2":"## Training function","921d6f54":"## Load Data","7422dd58":"## Data Sanitization","b1edbf61":"<h1 id=\"ridge\" style=\"color:#db9a44; background:#575c60; border:0.5px dotted;\"> \n    <center>Kernel Ridge\n        <a class=\"anchor-link\" href=\"#ridge\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","7d706aad":"# Training for Linear\/Quadratic\/Gaussian","5f25947a":"## Feature Engineering","71d67dc3":"<h1 id=\"dataset\" style=\"color:#db9a44; background:#575c60; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","e7d68c12":"## Genetic Differential Evolution","5f2d69cf":"## Training Models with Hyperparameters","a539380e":"<h1 id=\"training\" style=\"color:#db9a44; background:#575c60; border:0.5px dotted;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","d08d334a":"## Split Data Train\/Test","ba7467d2":"<h1 id=\"analyze\" style=\"color:#db9a44; background:#575c60; border:0.5px dotted;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}