{"cell_type":{"f62e909c":"code","b0d181ce":"code","3884e4cc":"code","8507ed10":"code","b4700a83":"code","d9de9ec0":"code","4d8e3aaf":"code","4f308edf":"code","31103e43":"code","1930db66":"code","fd2cf7d5":"code","c25d7256":"code","a8bfc9f5":"code","c9338df1":"code","38ba5883":"code","82a0d5e6":"code","670709a3":"code","a16a719f":"code","d85ba345":"code","d0a6a369":"code","85b81e9e":"code","fac479b4":"code","8a302a2b":"code","557c1912":"code","9e3eaedc":"code","7225f74e":"markdown","4cf37c54":"markdown","5865969a":"markdown"},"source":{"f62e909c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize,sent_tokenize\n\n#preprocessing\nfrom nltk.corpus import stopwords  #stopwords\nfrom nltk import word_tokenize,sent_tokenize # tokenizing\nfrom nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n\n# for named entity recognition (NER)\nfrom nltk import ne_chunk\n\n# vectorizers for creating the document-term-matrix (DTM)\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n\n#stop-words\nstop_words=set(nltk.corpus.stopwords.words('english'))\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0d181ce":"df = pd.read_csv('\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv')\ndf.head()","3884e4cc":"print(df.shape)\nprint(df.isnull().values.any())","8507ed10":"df.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time','Summary'],axis=1,inplace=True)","b4700a83":"df.head()","d9de9ec0":"def clean_text(headline):\n  le=WordNetLemmatizer()\n  word_tokens=word_tokenize(headline)\n  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n  cleaned_text=\" \".join(tokens)\n  return cleaned_text","4d8e3aaf":"df['Text']=df['Text'].apply(clean_text)","4f308edf":"vect =TfidfVectorizer(stop_words=stop_words,max_features=1000,ngram_range = (1,3)) # to play with. min_df,max_df,max_features etc...\nvect_text=vect.fit_transform(df['Text'])\n#vectorizer = TfidfVectorizer(ngram_range = (1,3)) ","31103e43":"print(vect_text.shape)\nprint(vect_text)","1930db66":"from sklearn.decomposition import TruncatedSVD\nlsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n\nlsa_top=lsa_model.fit_transform(vect_text)","fd2cf7d5":"print(lsa_top)\nprint(lsa_top.shape)  # (no_of_doc*no_of_topics)","c25d7256":"l=lsa_top[0]\nprint(\"Document 0 :\")\nfor i,topic in enumerate(l):\n  print(\"Topic \",i,\" : \",topic*100)","a8bfc9f5":"vocab = vect.get_feature_names()\n\nfor i, comp in enumerate(lsa_model.components_):\n    vocab_comp = zip(vocab, comp)\n    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n    print(\"Topic \"+str(i)+\": \")\n    for t in sorted_words:\n        print(t[0],end=\" \")\n    print(\"\\n\")","c9338df1":"Topics = []\nfor i, comp in enumerate(lsa_model.components_):\n    vocab_comp = zip(vocab, comp)\n    st=''\n    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n    for t in sorted_words:\n        st = st + t[0] + \" \"\n    Topics.append(st)\nTopics","38ba5883":"from wordcloud import WordCloud","82a0d5e6":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[0])\nplt.imshow(vineet)","670709a3":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[1])\nplt.imshow(vineet)","a16a719f":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[2])\nplt.imshow(vineet)","d85ba345":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[3])\nplt.imshow(vineet)","d0a6a369":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[4])\nplt.imshow(vineet)","85b81e9e":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[5])\nplt.imshow(vineet)","fac479b4":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[6])\nplt.imshow(vineet)","8a302a2b":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[7])\nplt.imshow(vineet)","557c1912":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[8])\nplt.imshow(vineet)","9e3eaedc":"vineet = WordCloud(background_color=\"white\", width=800, height=800, random_state=1).generate(Topics[9])\nplt.imshow(vineet)","7225f74e":"# TASKS:-\n1. Download the dataset for Amazon Fine Food Reviews from Kaggle.\n2. Subset it to contain only text field.\n3. Apply LSA on that field to categorize it into 10 topics.\n4. Visualize those topics using woordcloud.","4cf37c54":"# THANK YOU","5865969a":"# SUYASH PRATAP SINGH(181B226)"}}