{"cell_type":{"1e6b68e1":"code","7a62e429":"code","0c3638ff":"code","706e3e19":"code","56caf10d":"code","ad4705ab":"code","4875f961":"code","92e12e1f":"code","a578f7a6":"code","9fe05427":"code","65ef948b":"code","5e7fa6c4":"code","aa980062":"code","b2bdc55e":"code","e4653df8":"code","8871fa7c":"code","d2e6cf6f":"code","ecf1042d":"code","113e6773":"code","4fe73c92":"code","3f5b01df":"code","ca67cdcb":"code","93916990":"code","223fe46d":"code","741418ce":"code","01f052a6":"code","72222a2f":"code","19f083f6":"code","fc13283a":"code","36c139ef":"code","d741281b":"code","e2122900":"code","11b5bede":"code","22a8b44f":"code","51453fb9":"code","dbed681b":"code","eb1421f7":"code","14d25640":"code","a3efb948":"code","1b1f57f6":"code","907aa18f":"code","46975095":"code","229c2761":"code","0b1c0e53":"code","ae6ed443":"code","27b87f03":"code","43a85a3c":"code","80dc9f0d":"code","44515e53":"code","3c408783":"code","863fece7":"code","b83a7efa":"code","3c8b047c":"code","fdf8b4bd":"code","6c5cfe53":"code","2549ecce":"code","3fa8e020":"code","ea8a6c02":"code","b929e4a7":"code","a05d4c25":"code","7f5c16e4":"code","a13ac24e":"code","25fee3f9":"code","7c6ff05e":"code","63df7619":"code","e30057b4":"code","3254aaba":"code","2a497340":"code","4b33234d":"code","d8679efd":"code","62e16ff3":"code","ec781a0a":"code","27a6b2e3":"code","cde77685":"code","701d5b94":"code","e566bb63":"code","590ddd84":"code","2a86452f":"markdown","d0ab798d":"markdown","6654ea05":"markdown","e2f23800":"markdown","2bf46af8":"markdown","ef73468f":"markdown","0c31ee90":"markdown","78f7e63f":"markdown","ee01263a":"markdown","38dd1ff8":"markdown","954677fb":"markdown","6264f87a":"markdown","0dc41c1e":"markdown","613e946f":"markdown","dc9811a8":"markdown","61119b8f":"markdown","b31a4446":"markdown","100db55a":"markdown","f9be8826":"markdown","4edec0bc":"markdown","8f0bf246":"markdown","288fcf56":"markdown","389964e6":"markdown","e6cdae27":"markdown","e5e407ba":"markdown","49bd8aa9":"markdown","3e1d70f1":"markdown","695cbdb5":"markdown","7965a6d0":"markdown","6ba8f549":"markdown","37f5a123":"markdown","574087b9":"markdown"},"source":{"1e6b68e1":"import os\n\nos.mkdir(\".\/mobilenet\")","7a62e429":"!git clone https:\/\/github.com\/tensorflow\/models {'.\/mobilenet\/models'}","0c3638ff":"!pip install wget\nimport wget\nurl = \"https:\/\/github.com\/protocolbuffers\/protobuf\/releases\/download\/v3.15.6\/protoc-3.15.6-win64.zip\"\nwget.download(url)","706e3e19":"os.mkdir(\".\/mobilenet\/protobuf\")\n\n!cp \".\/protoc-3.15.6-win64.zip\" \".\/mobilenet\/protobuf\"\n\n!unzip \".\/mobilenet\/protobuf\/protoc-3.15.6-win64.zip\" -d \".\/mobilenet\/protobuf\"\n\n# Copy the file to the labeled directory and unzip it.","56caf10d":"os.environ['PATH'] += os.pathsep + \".\/mobilenet\/protobuf\/bin\/protoc.exe\"","ad4705ab":"!cd .\/mobilenet\/models\/research && protoc object_detection\/protos\/*.proto --python_out=.\n!cd .\/mobilenet\/models\/research && cp object_detection\/packages\/tf2\/setup.py . && python -m pip install .","4875f961":"import object_detection\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nfrom object_detection.utils import dataset_util\nimport ast\nimport cv2\nimport wget\nimport os\nfrom shutil import copyfile \nfrom tqdm.notebook import tqdm\ntqdm.pandas()","92e12e1f":"model_url = \"http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\"\nwget.download(model_url)","a578f7a6":"os.mkdir(\".\/mobilenet\/my_model\")\nMODEL_NAME = \"ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\"\n!mv .\/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz .\/mobilenet\/my_model\n!cd .\/mobilenet\/my_model && tar -zxvf {MODEL_NAME}","9fe05427":"TRAIN_PATH = '..\/input\/tensorflow-great-barrier-reef\/train_images'","65ef948b":"# Lets Load the dataset\ndf = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/train.csv')\ndf.head()","5e7fa6c4":"df['annotations'].unique()","aa980062":"# Lets see for how many values we have the annotations\ndf['number_bbox'] = df['annotations'].apply(lambda x: x.count('x'))\ndf.head()","b2bdc55e":"df['number_bbox'].unique()   # These are the number of bounding boxes","e4653df8":"proportion = lambda x: round(x\/len(df) * 100,2)\nnot_available = len(df.loc[df['number_bbox'] == 0])\nproportion(not_available)  \n# almost for 80% of the data we dont have the bounding boxes so we will use only\n# 20% of data to train the model","8871fa7c":"# Lets seperate those out\ndf_train = df.loc[df['number_bbox'] != 0,:].reset_index(drop=True)\ndf_train.sample(5)","d2e6cf6f":"df_train['annotations'][0]  # Its in form of string we need to use literal_eval","ecf1042d":"import ast\n\neval_ = lambda x:ast.literal_eval(x)\ndf_train['annotations'] = df_train['annotations'].progress_apply(eval_)","113e6773":"df_train['annotations'][0]  # Looks good now","4fe73c92":"\n# Now lets first get the coordinates of the bounding boxes\ndef give_bbox_loc(instance):   #instance could be a single dictionary or lists of dictionary\n    return [list(ins.values()) for ins in instance]\n\ndef get_path_image(instance):\n    return os.path.join(TRAIN_PATH,f'video_{instance[\"video_id\"]}',f'{instance[\"video_frame\"]}')\n\ndef load_image(path):\n    return cv2.cvtColor(cv2.imread(path+'.jpg'), cv2.COLOR_BGR2RGB)","3f5b01df":"df_train['box_location'] = df_train['annotations'].progress_apply(give_bbox_loc)","ca67cdcb":"df_train.sample(5)","93916990":"df_train[\"video_id\"].value_counts()","223fe46d":"df_train['path'] =  df_train.progress_apply(get_path_image, axis=1)","741418ce":"df_train.sample(5)","01f052a6":"def normalize_image(bboxes):\n    \n    bboxes = bboxes.astype('float')\n    bboxes[...,[0,2]] = bboxes[...,[0,2]] \/ 1280   # width is 1280\n    bboxes[...,[1,3]] = bboxes[...,[1,3]] \/ 720   # height is 720\n    \n    # Now we need to center the image\n    # Simply add half of width and height of image to current x and y it will be the center\n    bboxes[...,[0,1]] = bboxes[...,[0,1]] + bboxes[...,[2,3]] \/ 2\n    return bboxes\n\n\ndef draw_boxes(image, cordinates, classes = None, legend=False):\n    img = image.copy()\n    for idx in range(len(cordinates)):\n        bbox = cordinates[idx]\n        clas = classes[idx]\n        colors = [np.random.randint(0,255) for _ in range(3)]\n        # Always remeber that we are using yolo format so they are coordinates for center i.e. mid\n        x = round(float(bbox[0]) * img.shape[1],2)\n        y = round(float(bbox[1]) * img.shape[0],2)\n        w = round(float(bbox[2]) * img.shape[1]\/2,2)\n        h = round(float(bbox[3]) * img.shape[1]\/2,2)\n\n        box = (x-w,y-h,x+w,y+h) # Becasue note that the coordinates comes after the normalize function.\n        r_a = int(box[0])\n        r_b = int(box[1])\n        r_c = int(box[2])\n        r_d = int(box[3])\n        cv2.rectangle(img,(r_a, r_b),(r_c, r_d), thickness = 3, color = colors)\n        cv2.putText(img, clas, org=(r_a, r_d-3),color=(255, 0, 0),\n                    fontFace = cv2.FONT_HERSHEY_SIMPLEX,thickness = 2,fontScale=0.66)\n    return img","72222a2f":"# Now lets see some of the datapoints in the image\npoints = df_train.loc[df_train['number_bbox'] >= 8,:].sample(4)\nn_rows = 2\nn_cols = 2\nplt.figure(figsize=(15,15))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = row*n_cols + col\n        plt.subplot(n_rows, n_cols, index+1)\n        \n        instance = points.iloc[index]\n       \n        image = load_image(instance['path'])\n        box_cordinates = np.array(instance['box_location'])\n        height = 720\n        width = 1280\n        classes = ['COTS'] * len(box_cordinates)\n        normalized_coordinates = normalize_image(box_cordinates)\n        image = draw_boxes(image,\n                          normalized_coordinates,\n                           classes = classes,\n                           legend=True\n                          )\n        plt.imshow(image)\n        plt.axis('off')\nplt.subplots_adjust(wspace = 0.01, hspace = 0.01)","19f083f6":"os.mkdir(\".\/mobilenet\/train\")\nos.mkdir(\".\/mobilenet\/val\")","fc13283a":"df_train['fold'] = 'train'\ntrain_instances = int(len(df_train) * 0.8)\nval_instances = len(df_train) - train_instances\nval_index = df_train.sample(val_instances).index\ndf_train.loc[val_index,'fold'] = 'val'","36c139ef":"df_train.fold.value_counts()","d741281b":"for idx in tqdm(range(df_train.shape[0])):\n    instance = df_train.iloc[idx]\n    if instance['fold'] == 'train':\n      copyfile(f'{instance[\"path\"]}.jpg', '.\/mobilenet\/train\/{}.jpg'.format(instance[\"image_id\"]))\n    else:\n      copyfile(f'{instance[\"path\"]}.jpg', '.\/mobilenet\/val\/{}.jpg'.format(instance[\"image_id\"]))","e2122900":"print(len(os.listdir('.\/mobilenet\/val')))\nprint(len(os.listdir('.\/mobilenet\/train')))","11b5bede":"# Lets now create annotations for our dataset first. I will use .xml format to save the data.\nimport xml.etree.cElementTree as ET\n\nPATH = \".\/mobilenet\/\"\nfor _, instance in tqdm(df_train.iterrows()):\n  fold = instance['fold']\n  root = ET.Element(\"annotation\")\n  ET.SubElement(root,\"folder\").text = fold\n  ET.SubElement(root,\"filename\").text = instance[\"image_id\"] + '.jpg'\n  ET.SubElement(root,\"path\").text = '.\/mobilenet\/{0}\/{1}.jpg'.format(fold,instance[\"image_id\"])\n  source = ET.SubElement(root,'source')\n  ET.SubElement(source, 'database').text = 'Unknown'\n  size = ET.SubElement(root,\"size\")\n  ET.SubElement(size, \"width\").text = str(1280)   # We will convert into int later while ggenerating tfrecord.\n  ET.SubElement(size, \"height\").text = str(720)\n  ET.SubElement(size, \"depth\").text = str(3)\n  ET.SubElement(root,'segmented').text = str(0)\n  classname = ET.SubElement(root,\"object\")\n  for element in instance['box_location']:\n    ET.SubElement(classname, \"name\").text = 'COTS'\n    ET.SubElement(classname, \"pose\").text = 'Unspecified'\n    ET.SubElement(classname, \"truncated\").text = '0'\n    ET.SubElement(classname, \"difficult\").text = '0'\n    bndbox = ET.SubElement(classname, \"bndbox\")\n\n    # Dont worry about normalizing it when we will create tfrecord we will\n    # normalize then and there only\n\n    ET.SubElement(bndbox, \"xmin\").text = str(element[0])\n    ET.SubElement(bndbox, \"ymin\").text = str(element[1])\n\n    # If xmax goes out of the boundary we scale it. Very Important\n    xmax = element[0] + element[2]\n    if xmax >= 1280:\n      ET.SubElement(bndbox, \"xmax\").text = str(1280)\n    else:\n      ET.SubElement(bndbox, \"xmax\").text = str(xmax)\n\n    ymax = element[1] + element[3]\n    if ymax >= 720:\n      ET.SubElement(bndbox, \"ymax\").text = str(720) \n    else:   \n      ET.SubElement(bndbox, \"ymax\").text = str(ymax)\n\n  tree = ET.ElementTree(root)\n  tree.write(os.path.join(os.path.join(PATH,fold), instance[\"image_id\"]+'.xml'))","22a8b44f":"!git clone https:\/\/github.com\/Gnopal1132\/Generate_TFRecord.git","51453fb9":"os.mkdir('.\/mobilenet\/annotations\/')","dbed681b":"labels = [{'name':'COTS', 'id':1}]\nLABEL_MAP = '.\/mobilenet\/annotations\/label_map.pbtxt'\nwith open(LABEL_MAP, 'w') as f:\n    for label in labels:\n        f.write('item { \\n')\n        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n        f.write('\\tid:{}\\n'.format(label['id']))\n        f.write('}\\n')","eb1421f7":"TF_RECORD_SCRIPT = '.\/Generate_TFRecord\/tfrecord.py'\nIMAGE_PATH_TRAIN = \".\/mobilenet\/train\"\nIMAGE_PATH_VAL = \".\/mobilenet\/val\"\nANNOTATION_PATH = \".\/mobilenet\/annotations\/\"\nLABELMAP = os.path.join(ANNOTATION_PATH, 'label_map.pbtxt')\n\n!python {TF_RECORD_SCRIPT} -x {IMAGE_PATH_TRAIN} -l {LABELMAP} -o {os.path.join(ANNOTATION_PATH, 'train.record')} \n!python {TF_RECORD_SCRIPT} -x {IMAGE_PATH_VAL} -l {LABELMAP} -o {os.path.join(ANNOTATION_PATH, 'val.record')}","14d25640":"import tensorflow as tf\nfrom object_detection.utils import config_util\nfrom object_detection.protos import pipeline_pb2\nfrom google.protobuf import text_format","a3efb948":"\nMODEL_NAME = 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\nos.mkdir('.\/mobilenet\/{}'.format(MODEL_NAME))  # This is where we will store our model checkpoint and configuration.\n!cp \".\/mobilenet\/my_model\/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\/pipeline.config\" \".\/mobilenet\/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\"","1b1f57f6":"PIPELINE_CONFIG = '.\/mobilenet\/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\/pipeline.config' \nconfig = config_util.get_configs_from_pipeline_file(PIPELINE_CONFIG)  # Loading the configuration file","907aa18f":"pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\nwith tf.io.gfile.GFile(PIPELINE_CONFIG, \"r\") as f:                                                                                                                                                                                                                     \n    proto_str = f.read()                                                                                                                                                                                                                                          \n    text_format.Merge(proto_str, pipeline_config)","46975095":"BATCH = 8\nEPOCHS = 2000","229c2761":"# Loading basic information to the pipeline\nPATH = '.\/mobilenet'\npipeline_config.model.ssd.num_classes = 1\npipeline_config.train_config.batch_size = BATCH\npipeline_config.train_config.fine_tune_checkpoint = os.path.join(os.path.join(PATH, 'my_model'), MODEL_NAME, 'checkpoint', 'ckpt-0')\npipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\npipeline_config.train_input_reader.label_map_path= os.path.join(PATH,'annotations','label_map.pbtxt')\npipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(os.path.join(PATH,'annotations'), 'train.record')]\npipeline_config.eval_input_reader[0].label_map_path = os.path.join(PATH,'annotations','label_map.pbtxt')\npipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(os.path.join(PATH,'annotations'), 'val.record')]","0b1c0e53":"config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \nwith tf.io.gfile.GFile(PIPELINE_CONFIG, \"wb\") as f:                                                                                                                                                                                                                     \n    f.write(config_text)","ae6ed443":"TRAINING_SCRIPT = os.path.join(\".\/mobilenet\/models\", 'research', 'object_detection', 'model_main_tf2.py')","27b87f03":"CHECKPOINT_PATH = '.\/mobilenet\/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\ncommand = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps={}\".format(TRAINING_SCRIPT, CHECKPOINT_PATH, PIPELINE_CONFIG, EPOCHS)","43a85a3c":"command","80dc9f0d":"!python .\/mobilenet\/models\/research\/object_detection\/model_main_tf2.py --model_dir=.\/mobilenet\/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8 --pipeline_config_path=.\/mobilenet\/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\/pipeline.config --num_train_steps=2000","44515e53":"command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, CHECKPOINT_PATH, PIPELINE_CONFIG, CHECKPOINT_PATH)","3c408783":"command","863fece7":"from IPython.display import Image\nImage(filename='..\/input\/mobilenetresulttensorflowbeef\/result\/board1.png')","b83a7efa":"Image(filename='..\/input\/mobilenetresulttensorflowbeef\/result\/board2.png')","3c8b047c":"Image(filename='..\/input\/mobilenetresulttensorflowbeef\/result\/board3.png')","fdf8b4bd":"Image(filename='..\/input\/mobilenetresulttensorflowbeef\/result\/board4.png')","6c5cfe53":"Image(filename='..\/input\/mobilenetresulttensorflowbeef\/result\/board5.png')","2549ecce":"Image(filename='..\/input\/mobilenetresulttensorflowbeef\/result\/board6.png')","3fa8e020":"from shutil import rmtree\nrmtree('.\/mobilenet')","ea8a6c02":"# lets first download YOLO V5\n# Download YOLOv5\n!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n\n# Install dependencies\n%pip install -r requirements.txt  \n# change directory\n\n%cd ..\/\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","b929e4a7":"!pip install wandb\nimport wandb\n!wandb login 0e3d646e6d324479398418c8659583d46b1013ec","a05d4c25":"df_train['sequence'].unique()\n# The sequence represents ID of a gap-free subset of a given video\ndf_train['folds'] = -1","7f5c16e4":"# Lets split the dataset now.\nfrom sklearn.model_selection import GroupKFold\n\nsplit = GroupKFold(n_splits=4)\nX = df_train\nY = df_train.video_id.to_list()\ngroups = df_train.sequence.to_list()\nfor fold,(train_idx, val_idx) in enumerate(split.split(X,Y,groups)):\n    df_train.loc[val_idx, 'folds'] = fold\n\ndf_train['folds'].value_counts()","a13ac24e":"df_train.sample(5)","25fee3f9":"#Some hyperparameters\nBATCH = 16\nIMG_SHAPE = 1280\nVAL_FOLD = 2\nEPOCH = 5","7c6ff05e":"#Lets first create some directories and place our image there accordingly.\nos.makedirs('Starfish\/images\/train', exist_ok=True)\nos.makedirs('Starfish\/images\/val', exist_ok=True)\nos.makedirs('Starfish\/labels\/train', exist_ok=True)\nos.makedirs('Starfish\/labels\/val', exist_ok=True)","63df7619":"for idx in tqdm(range(df_train.shape[0])):\n    instance = df_train.iloc[idx]\n    if instance[\"folds\"] != VAL_FOLD:\n        copyfile(f'{instance[\"path\"]}.jpg', 'Starfish\/images\/train\/{}.jpg'.format(instance[\"image_id\"]))\n    else:\n        copyfile(f'{instance[\"path\"]}.jpg', 'Starfish\/images\/val\/{}.jpg'.format(instance[\"image_id\"]))","e30057b4":"# Lets Analayse the instances\nTRAIN_PATH = '\/kaggle\/working\/Starfish\/images\/train'\nVAL_PATH = '\/kaggle\/working\/Starfish\/images\/val'\n\nprint('Number of training instances are: {}'.format(len(os.listdir(TRAIN_PATH))))\nprint('Number of validation instances are: {}'.format(len(os.listdir(VAL_PATH))))","3254aaba":"import yaml,glob\n\nwith open('\/kaggle\/working\/train.txt', 'w') as work_train:\n    for path in glob.glob(f\"{TRAIN_PATH}\/*\"):\n        work_train.write(path+'\\n')\n    \nwith open('\/kaggle\/working\/val.txt', 'w') as work_val:\n    for path in glob.glob(f\"{VAL_PATH}\/*\"):\n        work_val.write(path+'\\n')","2a497340":"config = dict(   # from yolo github repo\n    train = TRAIN_PATH,\n    val = VAL_PATH,\n    nc = 1,  #classes\n    names = ['COTS']\n)","4b33234d":"with open(\".\/yolov5\/data\/data.yaml\",'w') as configuration:\n    yaml.dump(config, configuration, default_flow_style=False)\n\n# If you want collections to be always serialized in the block style,\n#    set the parameter default_flow_style of dump() to False","d8679efd":"os.listdir(\".\/yolov5\/data\")","62e16ff3":"df_train.sample(2)","ec781a0a":"All_boxes = []\nfor idx in tqdm(range(df_train.shape[0])):\n    instance = df_train.iloc[idx]\n    \n    image_id = instance['image_id']\n    height = 720\n    width = 1280\n    boxes = np.array(instance['box_location']).astype(np.float).copy()\n    num_boxes = len(boxes)\n    names = ['COTS'] * num_boxes\n    labels = [0] * num_boxes\n    if instance[\"folds\"] != VAL_FOLD:\n        filename = '\/kaggle\/working\/Starfish\/labels\/train\/{}.txt'.format(instance[\"image_id\"])\n    else:\n        filename = '\/kaggle\/working\/Starfish\/labels\/val\/{}.txt'.format(instance[\"image_id\"])\n    \n    with open(filename, 'w') as file:\n        normalized_boxes = normalize_image(boxes)\n        cliped_boxes = np.clip(normalized_boxes, 0, 1)\n        All_boxes.extend(cliped_boxes)\n        for box_idx in range(len(cliped_boxes)):\n            bb = str(cliped_boxes[box_idx])[1:-1]   \n            annot = str(str(labels[box_idx])) + ' ' + bb + '\\n'\n            annot = ''.join(annot)\n            annot = annot.strip('')\n            file.write(annot)","27a6b2e3":"train_instances = len(os.listdir('\/kaggle\/working\/Starfish\/labels\/train'))\nval_instances = len(os.listdir('\/kaggle\/working\/Starfish\/labels\/val'))\nprint('training instances are: {}'.format(train_instances))\nprint('validation instances are: {}'.format(val_instances))","cde77685":"%cd yolov5\/","701d5b94":"import torch\ntorch.cuda.empty_cache()","e566bb63":"!python train.py --img {IMG_SHAPE} \\\n                 --batch {BATCH} \\\n                 --epochs {EPOCH} \\\n                 --data data.yaml \\\n                 --weights yolov5s.pt \\\n                 --project tf-reef","590ddd84":"os.listdir(\".\/kaggle\/yolov5\")","2a86452f":"You can get other models [here](https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/g3doc\/tf2_detection_zoo.md)\n\nAfter downloading it store it the right directory and unzip it.","d0ab798d":"Label map simply contains the number of classes and the id","6654ea05":"We will create xml file for every image and store it in the same directory as the image itself.\n\nThe format for our annotations will be as [follows](https:\/\/drive.google.com\/file\/d\/1V2rBGit_xQ-jjBMrSg2atIhYCbKqnJ5R\/view?usp=sharing)\n You might be wondering why to use extra terms like segmented etc. The point is I used the repository  in my other projects and I didnt want to change lines there so I place something extra here so that things dont change. Important thing is folder,path,size,object and bndbox.","e2f23800":"\nINFO:tensorflow:Step 100 per-step time 2.361s\n\nI1230 19:11:27.819197 140299792009088 model_lib_v2.py:707] Step 100 per-step time 2.361s\nINFO:tensorflow:{'Loss\/classification_loss': 0.53248906,\n 'Loss\/localization_loss': 0.4413896,\n 'Loss\/regularization_loss': 0.1521543,\n 'Loss\/total_loss': 1.126033,\n 'learning_rate': 0.0319994}\nI1230 19:11:27.819756 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.53248906,\n 'Loss\/localization_loss': 0.4413896,\n 'Loss\/regularization_loss': 0.1521543,\n 'Loss\/total_loss': 1.126033,\n 'learning_rate': 0.0319994}\n \n \nINFO:tensorflow:Step 200 per-step time 1.842s\nI1230 19:14:31.999863 140299792009088 model_lib_v2.py:707] Step 200 per-step time 1.842s\nINFO:tensorflow:{'Loss\/classification_loss': 0.57567745,\n 'Loss\/localization_loss': 0.420439,\n 'Loss\/regularization_loss': 0.15256938,\n 'Loss\/total_loss': 1.1486858,\n 'learning_rate': 0.0373328}\nI1230 19:14:32.000326 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.57567745,\n 'Loss\/localization_loss': 0.420439,\n 'Loss\/regularization_loss': 0.15256938,\n 'Loss\/total_loss': 1.1486858,\n 'learning_rate': 0.0373328}\n \n \nINFO:tensorflow:Step 300 per-step time 1.867s\nI1230 19:17:38.737374 140299792009088 model_lib_v2.py:707] Step 300 per-step time 1.867s\nINFO:tensorflow:{'Loss\/classification_loss': 0.82316196,\n 'Loss\/localization_loss': 0.38124648,\n 'Loss\/regularization_loss': 0.15306622,\n 'Loss\/total_loss': 1.3574746,\n 'learning_rate': 0.0426662}\nI1230 19:17:38.737965 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.82316196,\n 'Loss\/localization_loss': 0.38124648,\n 'Loss\/regularization_loss': 0.15306622,\n 'Loss\/total_loss': 1.3574746,\n 'learning_rate': 0.0426662}\n \n \nINFO:tensorflow:Step 400 per-step time 1.853s\nI1230 19:20:44.022718 140299792009088 model_lib_v2.py:707] Step 400 per-step time 1.853s\nINFO:tensorflow:{'Loss\/classification_loss': 0.5168445,\n 'Loss\/localization_loss': 0.30190247,\n 'Loss\/regularization_loss': 0.15343656,\n 'Loss\/total_loss': 0.9721835,\n 'learning_rate': 0.047999598}\nI1230 19:20:44.023141 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.5168445,\n 'Loss\/localization_loss': 0.30190247,\n 'Loss\/regularization_loss': 0.15343656,\n 'Loss\/total_loss': 0.9721835,\n 'learning_rate': 0.047999598}\n \n \nINFO:tensorflow:Step 500 per-step time 1.865s\nI1230 19:23:50.505155 140299792009088 model_lib_v2.py:707] Step 500 per-step time 1.865s\nINFO:tensorflow:{'Loss\/classification_loss': 0.50315356,\n 'Loss\/localization_loss': 0.4802518,\n 'Loss\/regularization_loss': 0.15407632,\n 'Loss\/total_loss': 1.1374817,\n 'learning_rate': 0.053333}\nI1230 19:23:50.505642 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.50315356,\n 'Loss\/localization_loss': 0.4802518,\n 'Loss\/regularization_loss': 0.15407632,\n 'Loss\/total_loss': 1.1374817,\n 'learning_rate': 0.053333}\n \n \nINFO:tensorflow:Step 600 per-step time 1.853s\nI1230 19:26:55.792762 140299792009088 model_lib_v2.py:707] Step 600 per-step time 1.853s\nINFO:tensorflow:{'Loss\/classification_loss': 0.2856278,\n 'Loss\/localization_loss': 0.15508056,\n 'Loss\/regularization_loss': 0.15469529,\n 'Loss\/total_loss': 0.5954037,\n 'learning_rate': 0.0586664}\nI1230 19:26:55.793148 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.2856278,\n 'Loss\/localization_loss': 0.15508056,\n 'Loss\/regularization_loss': 0.15469529,\n 'Loss\/total_loss': 0.5954037,\n 'learning_rate': 0.0586664}\n \n \nINFO:tensorflow:Step 700 per-step time 1.856s\nI1230 19:30:01.431559 140299792009088 model_lib_v2.py:707] Step 700 per-step time 1.856s\nINFO:tensorflow:{'Loss\/classification_loss': 0.44273925,\n 'Loss\/localization_loss': 0.26241776,\n 'Loss\/regularization_loss': 0.15521659,\n 'Loss\/total_loss': 0.8603736,\n 'learning_rate': 0.0639998}\nI1230 19:30:01.432031 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.44273925,\n 'Loss\/localization_loss': 0.26241776,\n 'Loss\/regularization_loss': 0.15521659,\n 'Loss\/total_loss': 0.8603736,\n 'learning_rate': 0.0639998}\n \n \nINFO:tensorflow:Step 800 per-step time 1.853s\nI1230 19:33:06.761075 140299792009088 model_lib_v2.py:707] Step 800 per-step time 1.853s\nINFO:tensorflow:{'Loss\/classification_loss': 0.7442434,\n 'Loss\/localization_loss': 0.26918525,\n 'Loss\/regularization_loss': 0.15582325,\n 'Loss\/total_loss': 1.1692519,\n 'learning_rate': 0.069333196}\nI1230 19:33:06.761445 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.7442434,\n 'Loss\/localization_loss': 0.26918525,\n 'Loss\/regularization_loss': 0.15582325,\n 'Loss\/total_loss': 1.1692519,\n 'learning_rate': 0.069333196}\n \n \nINFO:tensorflow:Step 900 per-step time 1.848s\nI1230 19:36:11.598434 140299792009088 model_lib_v2.py:707] Step 900 per-step time 1.848s\nINFO:tensorflow:{'Loss\/classification_loss': 0.47391683,\n 'Loss\/localization_loss': 0.23772612,\n 'Loss\/regularization_loss': 0.1564635,\n 'Loss\/total_loss': 0.8681065,\n 'learning_rate': 0.074666604}\nI1230 19:36:11.598908 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.47391683,\n 'Loss\/localization_loss': 0.23772612,\n 'Loss\/regularization_loss': 0.1564635,\n 'Loss\/total_loss': 0.8681065,\n 'learning_rate': 0.074666604}\n \n \nINFO:tensorflow:Step 1000 per-step time 1.866s\nI1230 19:39:18.184226 140299792009088 model_lib_v2.py:707] Step 1000 per-step time 1.866s\nINFO:tensorflow:{'Loss\/classification_loss': 0.56101376,\n 'Loss\/localization_loss': 0.32822332,\n 'Loss\/regularization_loss': 0.1571509,\n 'Loss\/total_loss': 1.0463879,\n 'learning_rate': 0.08}\nI1230 19:39:18.184702 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.56101376,\n 'Loss\/localization_loss': 0.32822332,\n 'Loss\/regularization_loss': 0.1571509,\n 'Loss\/total_loss': 1.0463879,\n 'learning_rate': 0.08}\n \n \nINFO:tensorflow:Step 1100 per-step time 1.857s\nI1230 19:42:23.911181 140299792009088 model_lib_v2.py:707] Step 1100 per-step time 1.857s\nINFO:tensorflow:{'Loss\/classification_loss': 0.34899262,\n 'Loss\/localization_loss': 0.29847386,\n 'Loss\/regularization_loss': 0.15807542,\n 'Loss\/total_loss': 0.8055419,\n 'learning_rate': 0.07999918}\nI1230 19:42:23.911592 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.34899262,\n 'Loss\/localization_loss': 0.29847386,\n 'Loss\/regularization_loss': 0.15807542,\n 'Loss\/total_loss': 0.8055419,\n 'learning_rate': 0.07999918}\n \n \nINFO:tensorflow:Step 1200 per-step time 1.846s\nI1230 19:45:28.488472 140299792009088 model_lib_v2.py:707] Step 1200 per-step time 1.846s\nINFO:tensorflow:{'Loss\/classification_loss': 0.4211598,\n 'Loss\/localization_loss': 0.2639899,\n 'Loss\/regularization_loss': 0.15880023,\n 'Loss\/total_loss': 0.8439499,\n 'learning_rate': 0.079996705}\nI1230 19:45:28.488929 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.4211598,\n 'Loss\/localization_loss': 0.2639899,\n 'Loss\/regularization_loss': 0.15880023,\n 'Loss\/total_loss': 0.8439499,\n 'learning_rate': 0.079996705}\n \n \nINFO:tensorflow:Step 1300 per-step time 1.846s\nI1230 19:48:33.054533 140299792009088 model_lib_v2.py:707] Step 1300 per-step time 1.846s\nINFO:tensorflow:{'Loss\/classification_loss': 0.27099043,\n 'Loss\/localization_loss': 0.2084263,\n 'Loss\/regularization_loss': 0.15907411,\n 'Loss\/total_loss': 0.63849086,\n 'learning_rate': 0.0799926}\nI1230 19:48:33.054928 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.27099043,\n 'Loss\/localization_loss': 0.2084263,\n 'Loss\/regularization_loss': 0.15907411,\n 'Loss\/total_loss': 0.63849086,\n 'learning_rate': 0.0799926}\n \n \nINFO:tensorflow:Step 1400 per-step time 1.841s\nI1230 19:51:37.191095 140299792009088 model_lib_v2.py:707] Step 1400 per-step time 1.841s\nINFO:tensorflow:{'Loss\/classification_loss': 0.30937624,\n 'Loss\/localization_loss': 0.16050465,\n 'Loss\/regularization_loss': 0.15938759,\n 'Loss\/total_loss': 0.62926847,\n 'learning_rate': 0.07998685}\nI1230 19:51:37.191522 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.30937624,\n 'Loss\/localization_loss': 0.16050465,\n 'Loss\/regularization_loss': 0.15938759,\n 'Loss\/total_loss': 0.62926847,\n 'learning_rate': 0.07998685}\n \n \nINFO:tensorflow:Step 1500 per-step time 1.867s\nI1230 19:54:43.897237 140299792009088 model_lib_v2.py:707] Step 1500 per-step time 1.867s\nINFO:tensorflow:{'Loss\/classification_loss': 0.3307212,\n 'Loss\/localization_loss': 0.24422823,\n 'Loss\/regularization_loss': 0.16004217,\n 'Loss\/total_loss': 0.7349916,\n 'learning_rate': 0.07997945}\nI1230 19:54:43.897655 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.3307212,\n 'Loss\/localization_loss': 0.24422823,\n 'Loss\/regularization_loss': 0.16004217,\n 'Loss\/total_loss': 0.7349916,\n 'learning_rate': 0.07997945}\n \n \nINFO:tensorflow:Step 1600 per-step time 1.844s\nI1230 19:57:48.281800 140299792009088 model_lib_v2.py:707] Step 1600 per-step time 1.844s\nINFO:tensorflow:{'Loss\/classification_loss': 0.32862264,\n 'Loss\/localization_loss': 0.19054511,\n 'Loss\/regularization_loss': 0.16015424,\n 'Loss\/total_loss': 0.679322,\n 'learning_rate': 0.079970405}\nI1230 19:57:48.282171 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.32862264,\n 'Loss\/localization_loss': 0.19054511,\n 'Loss\/regularization_loss': 0.16015424,\n 'Loss\/total_loss': 0.679322,\n 'learning_rate': 0.079970405}\n \n \nINFO:tensorflow:Step 1700 per-step time 1.866s\nI1230 20:00:54.930995 140299792009088 model_lib_v2.py:707] Step 1700 per-step time 1.866s\nINFO:tensorflow:{'Loss\/classification_loss': 0.30334967,\n 'Loss\/localization_loss': 0.24827217,\n 'Loss\/regularization_loss': 0.16041782,\n 'Loss\/total_loss': 0.7120397,\n 'learning_rate': 0.07995972}\nI1230 20:00:54.931398 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.30334967,\n 'Loss\/localization_loss': 0.24827217,\n 'Loss\/regularization_loss': 0.16041782,\n 'Loss\/total_loss': 0.7120397,\n 'learning_rate': 0.07995972}\n \n \nINFO:tensorflow:Step 1800 per-step time 1.878s\nI1230 20:04:02.684715 140299792009088 model_lib_v2.py:707] Step 1800 per-step time 1.878s\nINFO:tensorflow:{'Loss\/classification_loss': 0.29834068,\n 'Loss\/localization_loss': 0.1236572,\n 'Loss\/regularization_loss': 0.16077736,\n 'Loss\/total_loss': 0.58277524,\n 'learning_rate': 0.0799474}\nI1230 20:04:02.685132 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.29834068,\n 'Loss\/localization_loss': 0.1236572,\n 'Loss\/regularization_loss': 0.16077736,\n 'Loss\/total_loss': 0.58277524,\n 'learning_rate': 0.0799474}\n \n \nINFO:tensorflow:Step 1900 per-step time 1.853s\nI1230 20:07:07.961340 140299792009088 model_lib_v2.py:707] Step 1900 per-step time 1.853s\nINFO:tensorflow:{'Loss\/classification_loss': 0.3075142,\n 'Loss\/localization_loss': 0.17589329,\n 'Loss\/regularization_loss': 0.16102299,\n 'Loss\/total_loss': 0.6444305,\n 'learning_rate': 0.07993342}\nI1230 20:07:07.961811 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.3075142,\n 'Loss\/localization_loss': 0.17589329,\n 'Loss\/regularization_loss': 0.16102299,\n 'Loss\/total_loss': 0.6444305,\n 'learning_rate': 0.07993342}\n \n \nINFO:tensorflow:Step 2000 per-step time 1.860s\nI1230 20:10:13.916106 140299792009088 model_lib_v2.py:707] Step 2000 per-step time 1.860s\nINFO:tensorflow:{'Loss\/classification_loss': 0.18447536,\n 'Loss\/localization_loss': 0.14183624,\n 'Loss\/regularization_loss': 0.16143182,\n 'Loss\/total_loss': 0.4877434,\n 'learning_rate': 0.07991781}\nI1230 20:10:13.916568 140299792009088 model_lib_v2.py:708] {'Loss\/classification_loss': 0.18447536,\n 'Loss\/localization_loss': 0.14183624,\n 'Loss\/regularization_loss': 0.16143182,\n 'Loss\/total_loss': 0.4877434,\n 'learning_rate': 0.07991781}","2bf46af8":"# Step 8: Generating TFRecord","ef73468f":"**Training with W&B**","0c31ee90":"# Step 10: Loading the confiuration file,checkpoints and basic settings for training the model","78f7e63f":"For every example in your dataset, you should have the following information:\n\n1. An RGB image for the dataset encoded as jpeg or png.\n\n2. A list of bounding boxes for the image. Each bounding box should contain:\nA bounding box coordinates (with origin in top left corner) defined by 4 floating point numbers [ymin, xmin, ymax, xmax]. Note that we store the normalized coordinates (x \/ width, y \/ height) in the TFRecord dataset. The class of the object in the bounding box.","ee01263a":"# Create Labels for YOLOv5\nTo label your images,a .txt file with the same name of the image,will be created (if no objects in image, no *.txt file is required) The *.txt file specifications are:\n\nOne row per object\n\nEach row is class x_center y_center width height format.\n\nBox coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\n\nClass numbers are zero-indexed (start from 0).\n\n\n\ud83d\udccd Note: We don't have to remove the images without bounding boxes from the training or validation sets.","38dd1ff8":"Note: If Importing libraries gives error try restarting kernel and import again.","954677fb":"In the dataset, all the images has height and width as 720 and 1280","6264f87a":"I will be using mthe standard apporach to generate the two tfrecord file one for train and one for val.\n\nSpecial thanks to [Nick](https:\/\/github.com\/nicknochnack\/) for the file\n\nIf you see the repository all we do is to iterate through all the xml file and generating two tfrecord. For getting tfrecord file. We extract certain information like xmin,xmax etc and we encode it it tf_example(in the repository)\n\nTo know how it works refer [here](https:\/\/tensorflow-object-detection-api-tutorial.readthedocs.io\/en\/latest\/training.html)","0dc41c1e":"![yolo](https:\/\/user-images.githubusercontent.com\/26456083\/86477109-5a7ca780-bd7a-11ea-9cb7-48d9fd6848e7.jpg)","613e946f":"Welcome to the tutorial. Here we are going to train two different model on the given dataset. YoLo V5 and MobileNet V2 to see which does good, compare the results and proceed.","dc9811a8":"# Step 5: Import the object detection API","61119b8f":"# Step 1: Cloning the repository","b31a4446":"# Step 6: Download the mobile net v2 model from tensorflow.","100db55a":"![mobilenet](https:\/\/machinethink.net\/images\/mobilenet-v2\/FeatureExtractor@2x.png)","f9be8826":"# Evaluate the Model","4edec0bc":"**LETS START WITH MOBILE NET FIRST:**\n\nWe create a directory called mobilenet inside which we will keep all our relevant files.","8f0bf246":"![work in progress](https:\/\/simpro.it\/wp-content\/uploads\/2015\/07\/Work-in-progress-1024x603.png)","288fcf56":"# Step 7: Creating Annotations","389964e6":"Using library wget you can download urls. To install protoc either you can donwload the file and set it to the environment variable of os so that we can use the protoc command. We store it in its seperate directory as shown below. Protoc is used to compile .proto files.","e6cdae27":"To Execute the model run the below line. I am not going to train the model. As I already did it on colab I am going to print the highlight of training and tensorboard to see the output of how the model behaved. I uploaded the final checkpoints and event files.","e5e407ba":"Next step is to create yaml file which is the configuration file that YOLO V5 will read\nIt contains three minimal things:\n\n    1. Path to the train\/val\/test images we can give in .txt format.\n    \n    2. Number of classes\n    \n    3. List of the names of classes","49bd8aa9":"Now we will start by installing tensorflow object detection API.","3e1d70f1":"# Step 9: Create the Label Map","695cbdb5":"# Step 2: Install protobufs","7965a6d0":"For creating XML Annotations we will use below xml library. Its very simple to use. Just follow tree like structure.\n\nX = ET.Element('Hello') # This actually creates a root tag 'Hello'\n\nET.SubElement(X,'Bye').text = 'Kaggle' # It creates a subtag within Hello named 'Bye' with value Kaggle.","6ba8f549":"Now Lets see **YOLO V5**","37f5a123":"Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.019\n\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.055\n \n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.007\n \n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006\n \n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.024\n \n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.128\n \n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.059\n \n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.150\n \n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208\n \n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n \n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250\n \n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.592\n \nINFO:tensorflow:Eval metrics at step 2000\nI1231 18:49:47.775108 139774748125056 model_lib_v2.py:1015] Eval metrics at step 2000\nINFO:tensorflow:\t+ DetectionBoxes_Precision\/mAP: 0.019225\nI1231 18:49:47.794445 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision\/mAP: 0.019225\n\n\nINFO:tensorflow:\t+ DetectionBoxes_Precision\/mAP@.50IOU: 0.055222\nI1231 18:49:47.796933 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision\/mAP@.50IOU: 0.055222\nINFO:tensorflow:\t+ DetectionBoxes_Precision\/mAP@.75IOU: 0.006860\nI1231 18:49:47.799041 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision\/mAP@.75IOU: 0.006860\n\n\nINFO:tensorflow:\t+ DetectionBoxes_Precision\/mAP (small): 0.005941\nI1231 18:49:47.801052 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision\/mAP (small): 0.005941\nINFO:tensorflow:\t+ DetectionBoxes_Precision\/mAP (medium): 0.023949\nI1231 18:49:47.803217 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision\/mAP (medium): 0.023949\n\n\nINFO:tensorflow:\t+ DetectionBoxes_Precision\/mAP (large): 0.127765\nI1231 18:49:47.805278 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision\/mAP (large): 0.127765\nINFO:tensorflow:\t+ DetectionBoxes_Recall\/AR@1: 0.058537\nI1231 18:49:47.807327 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall\/AR@1: 0.058537\n\n\nINFO:tensorflow:\t+ DetectionBoxes_Recall\/AR@10: 0.150000\nI1231 18:49:47.809374 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall\/AR@10: 0.150000\nINFO:tensorflow:\t+ DetectionBoxes_Recall\/AR@100: 0.207520\nI1231 18:49:47.811417 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall\/AR@100: 0.207520\n\n\nINFO:tensorflow:\t+ DetectionBoxes_Recall\/AR@100 (small): 0.004687\nI1231 18:49:47.813374 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall\/AR@100 (small): 0.004687\nINFO:tensorflow:\t+ DetectionBoxes_Recall\/AR@100 (medium): 0.250448\nI1231 18:49:47.815356 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall\/AR@100 (medium): 0.250448\n\n\nINFO:tensorflow:\t+ DetectionBoxes_Recall\/AR@100 (large): 0.592308\nI1231 18:49:47.817360 139774748125056 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall\/AR@100 (large): 0.592308\nINFO:tensorflow:\t+ Loss\/localization_loss: 0.681640\nI1231 18:49:47.818885 139774748125056 model_lib_v2.py:1018] \t+ Loss\/localization_loss: 0.681640\n\n\nINFO:tensorflow:\t+ Loss\/classification_loss: 6.248461\nI1231 18:49:47.820402 139774748125056 model_lib_v2.py:1018] \t+ Loss\/classification_loss: 6.248461\nINFO:tensorflow:\t+ Loss\/regularization_loss: 0.161301\n\n\nI1231 18:49:47.821904 139774748125056 model_lib_v2.py:1018] \t+ Loss\/regularization_loss: 0.161301\nINFO:tensorflow:\t+ Loss\/total_loss: 7.091401\nI1231 18:49:47.823434 139774748125056 model_lib_v2.py:1018] \t+ Loss\/total_loss: 7.091401","574087b9":"# Step 3: Is to compile all the protocs in the object detection directory as shown below and building the object detection API"}}