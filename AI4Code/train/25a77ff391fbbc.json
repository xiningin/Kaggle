{"cell_type":{"1147ea8e":"code","86e44786":"code","8181084f":"code","1b9bb429":"code","bba1bc10":"code","3796a30d":"code","5d342f61":"code","18fc159c":"code","625016f3":"code","c977bd00":"code","aef2d8fa":"code","e36f6baa":"code","1628e800":"code","b76e8d77":"code","123edaa6":"code","dc1e0375":"code","e556db85":"code","e63a294c":"code","80c7637e":"code","d4b1c2cc":"code","6506f0bd":"code","13c21123":"code","e2922462":"code","794a5fc2":"code","652769de":"code","d5c8476f":"markdown","35e78c1a":"markdown","973b9fbf":"markdown","f5f24186":"markdown","686dd0dd":"markdown"},"source":{"1147ea8e":"# Welcome to the Car Purchase Amount Prediction case study. This is the very basic case study.\n# Here we will simply see how to load data, Visualize data, clean data, train test split, prepare model, predict and evaluate.\n# Upvote and share if your like it.\n# https:\/\/www.facebook.com\/codemakerz","86e44786":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8181084f":"df = pd.read_csv('\/kaggle\/input\/car-purchase-data\/Car_Purchasing_Data.csv', encoding='cp1252')","1b9bb429":"df.head()","bba1bc10":"sns.pairplot(df)","3796a30d":"# We do not need customer name, email and country as these factors will not affect your purchase.\n# also we will remove car purchase amount column as it is our target column and target column should not be in input matrix.\n# in y we will only store our target column.\n\nX = df.drop([\"Customer Name\", \"Customer e-mail\", \"Country\", \"Car Purchase Amount\"], axis=1)\ny = df[\"Car Purchase Amount\"]","5d342f61":"X[0:10]","18fc159c":"y[0:10]","625016f3":"print(X.shape)\nprint(y.shape)","c977bd00":"# If you will not scale your data, model will predict terrible things. Your data should be in same scale. IT makes your model more accurate and faster.\n# So lets scale our data with min-max scaler.\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\ny = y.values.reshape(-1, 1)\ny_scaled = scaler.fit_transform(y)","aef2d8fa":"scaler.data_max_","e36f6baa":"from sklearn.model_selection import train_test_split","1628e800":"# Splitting our data. 30% test and 70% training data.\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.3)","b76e8d77":"# importing tensorflow library to create our ANN model. If your model has more than 2 layers it is called deep learning network.\nimport tensorflow.keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","123edaa6":"model = Sequential()\n# input dim is number of inputs. It is always of the same size as feature size. (X)\n# 40 is number of neurons or units.\nmodel.add(Dense(40, input_dim=5, activation='relu'))\nmodel.add(Dense(40, activation='relu'))\nmodel.add(Dense(1, input_dim=5, activation='linear'))","dc1e0375":"# You can check your model details using summary function\nmodel.summary()","e556db85":"model.compile(optimizer=\"adam\", loss='mean_squared_error') # Compile your model.","e63a294c":"epoch_hist = model.fit(X_train, y_train, epochs=100, verbose=1, validation_split=0.2, batch_size=25)","80c7637e":"epoch_hist.history.keys()","d4b1c2cc":"epoch_hist.history","6506f0bd":"# So according to below figure you will see that for the first 20 epochs our model showing a big drop in loss or you can say improvement.\n# but after that improvement is constant or not that much significant. So from here we can conclude that for given number of nuerons we can only use 20-25 epochs to get a good model.\nplt.figure(figsize=(10, 8))\nplt.plot(epoch_hist.history['loss'])\nplt.plot(epoch_hist.history['val_loss'])\nplt.legend([\"Training Loss\", \"Validation Loss\"])\nplt.title(\"Training and Validation Loss Throughout the epochs\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Info Loss\")\n","13c21123":"# Predict Test Records\ny_pred = model.predict(X_test)\ny_pred[0:10]","e2922462":"# Predict Custom Value\nX_test_data = np.array([[1, 50, 45000, 3000, 500000]])\npredict = model.predict(X_test_data)\nprint(predict)","794a5fc2":"# At the end i will suggest you to play with model's hyper paramters. You should try with different number or models and epochs and try to see what result you get. There is not best\n# value. It all depends upon your data and your observations.","652769de":"# Thank you ... upvote if you liked.","d5c8476f":"# Scaling","35e78c1a":"# Load Data","973b9fbf":"# Model Training","f5f24186":"# Data Cleaning","686dd0dd":"# Visualization"}}