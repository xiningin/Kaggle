{"cell_type":{"74b63bea":"code","867e3523":"code","e7af8206":"code","86312d85":"code","646af94c":"code","b1405503":"code","64cf17c7":"code","60d4ad06":"code","8d802b3e":"code","7be80773":"code","480aee2f":"code","36e394c5":"code","ff2b892c":"code","c7c7b7db":"code","f4c52fa2":"code","32cca98b":"code","7554eb06":"code","f57f5f0c":"code","94a9f3ef":"code","0c5a1a2c":"code","b0f6a6db":"code","b14e20c8":"code","ad5efd56":"code","3e54eec5":"markdown","b83f5c10":"markdown","83804430":"markdown","3b41157b":"markdown","e528cbbf":"markdown","95a44486":"markdown","7d183a7f":"markdown","796fd90d":"markdown","585e32c3":"markdown","9e41479d":"markdown","e536ab35":"markdown","e0647f91":"markdown","876e9955":"markdown","cfc6d48f":"markdown","4a9a99f2":"markdown","ddb45d06":"markdown","3730db9d":"markdown","445a0af0":"markdown"},"source":{"74b63bea":"import numpy as np\nimport pandas as pd \nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as patches\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 150)\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport os\nimport random\nimport math\nimport psutil\nimport pickle\nimport timeit\n\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimport pandas_profiling\n\n","867e3523":"%%time\n\nmetadata_dtype = {'site_id':\"uint8\",'building_id':'uint16','square_feet':'float32','year_built':'float32','floor_count':\"float16\"}\nweather_dtype = {\"site_id\":\"uint8\",'air_temperature':\"float16\",'cloud_coverage':\"float16\",'dew_temperature':\"float16\",'precip_depth_1_hr':\"float16\",\n                 'sea_level_pressure':\"float32\",'wind_direction':\"float16\",'wind_speed':\"float16\"}\ntrain_dtype = {'meter':\"uint8\",'building_id':'uint16','meter_reading':\"float32\"}\n\nstart_time = timeit.default_timer()\n\nweather_train = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\", parse_dates=['timestamp'], dtype=weather_dtype)\n# weather_test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\", parse_dates=['timestamp'], dtype=weather_dtype)\nmetadata = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\", dtype=metadata_dtype)\ntrain = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/train.csv\", parse_dates=['timestamp'], dtype=train_dtype)\n# test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/test.csv\", parse_dates=['timestamp'], usecols=['building_id','meter','timestamp'], dtype=train_dtype)\n\nprint('Size of train_df data', train.shape)\nprint('Size of weather_train_df data', weather_train.shape)\n# print('Size of weather_test_df data', weather_test.shape)\nprint('Size of building_meta_df data', metadata.shape)\n\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)\n","e7af8206":"weather_train.head()\nmetadata.head()\ntrain.head()","86312d85":"# start_time = timeit.default_timer()\n\n# missing_weather = pd.DataFrame(weather_train.isna().sum()\/len(weather_train),columns=[\"Weather_Train_Missing_Pct\"])\n# # missing_weather[\"Weather_Test_Missing_Pct\"] = weather_test.isna().sum()\/len(weather_test)\n# missing_weather\n\n# missing_metadata = pd.DataFrame(metadata.isna().sum()\/len(metadata),columns=[\"Metadada_Missing\"])\n# missing_metadata\n\n# weather_train_report = weather_train.profile_report(style={'full_width':True},title='Weather Data Profiling Report')\n# weather_train_report\n\n# metadata_report = metadata.profile_report(style={'full_width':True},title='Metadata Profiling Report')\n# metadata_report\n\n# elapsed = timeit.default_timer() - start_time\n# print(elapsed)\n\n# # In[5]:\n\n# weather_train.head()\n# metadata.head()\n# train.head()\n\n# del missing_weather\n# del missing_metadata\n# del weather_train_report\n# del metadata_report\n# gc.collect()\n","646af94c":"# cols = list(weather_train.columns[2:])\n# cols_imputed = weather_train[cols].isnull().astype('bool_').add_suffix('_imputed')\n\n# imp = IterativeImputer(max_iter=10, verbose=0)\n# imp.fit(weather_train.iloc[:,2:])\n# weather_train_imputed = imp.transform(weather_train.iloc[:,2:])\n# weather_train_imputed = pd.concat([weather_train.iloc[:,0:2],pd.DataFrame(weather_train_imputed, columns=weather_train.columns[2:]), cols_imputed], axis=1)\n# # weather_train_imputed = pd.concat([weather_train.iloc[:,0:2],pd.DataFrame(weather_train_imputed, columns=weather_train.columns[2:])], axis=1)\n# pd.DataFrame(weather_train_imputed.isna().sum()\/len(weather_train_imputed),columns=[\"Weather_Train_Missing_Imputed\"])\n\n# # cols = list(weather_test.columns[2:])\n# # cols_imputed = weather_test[cols].isnull().astype('bool_').add_suffix('_imputed')\n\n# # imp = IterativeImputer(max_iter=10, verbose=0)\n# # imp.fit(weather_test.iloc[:,2:])\n# # weather_test_imputed = imp.transform(weather_test.iloc[:,2:])\n# # weather_test_imputed = pd.concat([weather_test.iloc[:,0:2],pd.DataFrame(weather_test_imputed, columns=weather_test.columns[2:]), cols_imputed], axis=1)\n# # weather_test_imputed = pd.concat([weather_test.iloc[:,0:2],pd.DataFrame(weather_test_imputed, columns=weather_test.columns[2:])], axis=1)\n# # pd.DataFrame(weather_test_imputed.isna().sum()\/len(weather_test_imputed),columns=[\"Weather_Train_Missing_Imputed\"])\n\n# # imputation floor_count & year built\n\n# cols = list(metadata.columns[4:])\n# cols_imputed = metadata[cols].isnull().astype('uint8').add_suffix('_imputed')\n\n# imp = IterativeImputer(max_iter=10, verbose=0)\n# imp.fit(metadata.iloc[:,3:])\n# metadata_imputed = imp.transform(metadata.iloc[:,3:])\n# metadata_imputed = pd.concat([metadata.iloc[:,0:3],pd.DataFrame(metadata_imputed, columns=metadata.columns[3:]), cols_imputed], axis=1)\n# #metadata_imputed = pd.concat([metadata.iloc[:,0:3],pd.DataFrame(metadata_imputed, columns=metadata.columns[3:])], axis=1)\n# pd.DataFrame(metadata_imputed.isna().sum()\/len(metadata_imputed),columns=[\"Metadata_Missing_Imputed\"])\n\n# metadata_imputed.year_built = metadata_imputed.year_built.round()\n# metadata_imputed.floor_count = metadata_imputed.floor_count.round()\n\n# del weather_train\n# # del weather_test\n# del metadata\n# del cols\n# del cols_imputed\n\n# gc.collect()\n","b1405503":"# for df in [train, test]:\nfor df in [train]:\n    df['Month'] = df['timestamp'].dt.month.astype(\"uint8\")\n    df['DayOfMonth'] = df['timestamp'].dt.day.astype(\"uint8\")\n    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n    df['Hour'] = df['timestamp'].dt.hour.astype(\"uint8\")\n    df['timestamp_2'] = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() \/\/ 3600\n    df['timestamp_2'] = df.timestamp_2.astype('uint16')\n    \n# Code to read and combine the standard input files, converting timestamps to number of hours since the beginning of 2016.\n\n# weather_train_imputed['timestamp_2'] = (weather_train_imputed.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() \/\/ 3600\n# weather_train_imputed['timestamp_2'] = weather_train_imputed.timestamp_2.astype('int16')\n\nweather_train['timestamp_2'] = (weather_train.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() \/\/ 3600\nweather_train['timestamp_2'] = weather_train.timestamp_2.astype('int16')\n\n# weather_test_imputed['timestamp_2'] = (weather_test_imputed.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() \/\/ 3600\n# weather_test_imputed['timestamp_2'] = weather_test_imputed.timestamp_2.astype('int16')\n\n#fix timestamps\n\nsite_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]\nGMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\nweather_train.timestamp_2 = weather_train.timestamp_2 + weather_train.site_id.map(GMT_offset_map)\n# weather_test_imputed.timestamp_2 = weather_test_imputed.timestamp_2 + weather_test_imputed.site_id.map(GMT_offset_map)\n\nweather_train.drop('timestamp',axis=1,inplace=True)\n# weather_test_imputed.drop('timestamp',axis=1,inplace=True)\ngc.collect()\n\n","64cf17c7":"# Dropping floor_count variable as it has 75% missing values\n# metadata_imputed.drop('floor_count',axis=1,inplace=True)\n\ntrain['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)\n#test['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)\n\ntrain.rename(columns={'timestamp':'timestamp_train'}, inplace=True)\n# test.rename(columns={'timestamp':'timestamp_test'}, inplace=True)\n\ntrain['meter_reading'] = np.log1p(train['meter_reading'])\n\nmetadata['primary_use'].replace({\"Healthcare\":\"Other\",\"Parking\":\"Other\",\"Warehouse\/storage\":\"Other\",\"Manufacturing\/industrial\":\"Other\",\n                                \"Retail\":\"Other\",\"Services\":\"Other\",\"Technology\/science\":\"Other\",\"Food sales and service\":\"Other\",\n                                \"Utility\":\"Other\",\"Religious worship\":\"Other\"},inplace=True)\nmetadata['square_feet'] = np.log1p(metadata['square_feet'])\n# metadata_imputed['year_built'].fillna(-999, inplace=True)\n# metadata_imputed['square_feet'] = metadata_imputed['square_feet'].astype('float16')\n# metadata_imputed['year_built'] = metadata_imputed['year_built'].astype('uint16')\n# metadata_imputed['floor_count'] = metadata_imputed['floor_count'].astype('uint8')\n\ngc.collect()\n","60d4ad06":"%%time\ntrain = pd.merge(train,metadata,on='building_id',how='left')\n# test  = pd.merge(test,metadata_imputed,on='building_id',how='left')\nprint (\"Training Data+Metadata Shape {}\".format(train.shape))\n# print (\"Testing Data+Metadata Shape {}\".format(test.shape))\ndel metadata\ngc.collect()\n\ntrain = pd.merge(train,weather_train,on=['site_id','timestamp_2'],how='left')\ndel weather_train\ngc.collect()\n\n# test  = pd.merge(test,weather_test_imputed,on=['site_id','timestamp_2'],how='left')\nprint (\"Training Data+Metadata+Weather Shape {}\".format(train.shape))\n# print (\"Testing Data+Metadata+Weather Shape {}\".format(test.shape))\n\n# del weather_test_imputed\n# gc.collect()\n\n#missing_train = pd.DataFrame(train.isna().sum()\/len(train),columns=[\"Train_Missing\"])\n#missing_train\n\n#missing_test = pd.DataFrame(test.isna().sum()\/len(test),columns=[\"Train_Missing\"])\n#missing_test\n\n","8d802b3e":"# Save space\n# commented since already done above\n#for df in [train,test]:\n#    df['square_feet'] = df['square_feet'].astype('float16')\n    \n# Fill NA\n\n#cols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed']\n#for col in cols:\n#    train[col].fillna(train[col].mean(),inplace=True)\n#    test[col].fillna(test[col].mean(),inplace=True)\n    \n# Drop nonsense entries\n# As per the discussion in the following thread, https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/117083, there is some discrepancy in the meter_readings for different ste_id's and buildings. It makes sense to delete them\nidx_to_drop = list((train[(train['site_id'] == 0) & (train['timestamp_train'] < \"2016-05-21 00:00:00\")]).index)\nprint (len(idx_to_drop))\ntrain.drop(idx_to_drop,axis='rows',inplace=True)\n\n# dropping all the electricity meter readings that are 0, after considering them as anomalies.\nidx_to_drop = list(train[(train['meter'] == \"Electricity\") & (train['meter_reading'] == 0)].index)\n# idx_to_drop = list(train[(train['meter'] == 0) & (train['meter_reading'] == 0)].index)\nprint(len(idx_to_drop))\ntrain.drop(idx_to_drop,axis='rows',inplace=True)\n\n##train.drop('timestamp',axis=1,inplace=True)\n##test.drop('timestamp',axis=1,inplace=True)\n\ntrain.drop('timestamp_train',axis=1,inplace=True)\n# test.drop('timestamp_test',axis=1,inplace=True)\n\ntrain.drop('timestamp_2',axis=1,inplace=True)\ntrain = train.reset_index()\ntrain.drop('index',axis=1,inplace=True)\n\n# test.drop('timestamp_2',axis=1,inplace=True)\n\ndel idx_to_drop\ngc.collect()\n\n# Encode features\nle = LabelEncoder()\n\ntrain['meter']= le.fit_transform(train['meter']).astype(\"uint8\")\n# test['meter']= le.fit_transform(test['meter']).astype(\"uint8\")\ntrain['primary_use']= le.fit_transform(train['primary_use']).astype(\"uint8\")\n# test['primary_use']= le.fit_transform(test['primary_use']).astype(\"uint8\")\n\n# print (train.shape, test.shape)\nprint (train.shape)\n","7be80773":"cols = list(train.columns[10:])\ncols_imputed = train[cols].isnull().astype('uint8').add_suffix('_imputed')\n\nimp = IterativeImputer(max_iter=10, verbose=0)\nimp.fit(train.iloc[:,8:])\ntrain_temp = imp.transform(train.iloc[:,8:])\ntrain = pd.concat([train.iloc[:,0:8],pd.DataFrame(train_temp, columns=train.columns[8:]), cols_imputed], axis=1)\n# weather_train_imputed = pd.concat([weather_train.iloc[:,0:2],pd.DataFrame(weather_train_imputed, columns=weather_train.columns[2:])], axis=1)\npd.DataFrame(train.isna().sum()\/len(train),columns=[\"Weather_Train_Missing_Imputed\"])\n\ndel train_temp\ndel cols\ndel cols_imputed\ngc.collect()\n\ntrain['square_feet'] = train['square_feet'].astype('float16')\ntrain['year_built'] = train['year_built'].astype('uint16')\ntrain['floor_count'] = train['floor_count'].astype('uint8')\ntrain['primary_use'] = train['primary_use'].astype('uint8')\ntrain['air_temperature'] = train['air_temperature'].astype('float16')\ntrain['cloud_coverage'] = train['cloud_coverage'].astype('float16')\ntrain['dew_temperature'] = train['dew_temperature'].astype('float16')\ntrain['precip_depth_1_hr'] = train['precip_depth_1_hr'].astype('float16')\ntrain['wind_direction'] = train['wind_direction'].astype('float16')\ntrain['wind_speed'] = train['wind_speed'].astype('float16')\n","480aee2f":"%%time\nnumber_unique_meter_per_building = train.groupby('building_id')['meter'].nunique()\ntrain['number_unique_meter_per_building'] = train['building_id'].map(number_unique_meter_per_building)\nmean_meter_reading_per_building = train.groupby('building_id')['meter_reading'].mean()\ntrain['mean_meter_reading_per_building'] = train['building_id'].map(mean_meter_reading_per_building)\n# median_meter_reading_per_building = train.groupby('building_id')['meter_reading'].median()\n# train['median_meter_reading_per_building'] = train['building_id'].map(median_meter_reading_per_building)\nstd_meter_reading_per_building = train.groupby('building_id')['meter_reading'].std()\ntrain['std_meter_reading_per_building'] = train['building_id'].map(std_meter_reading_per_building)\nmean_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].mean()\ntrain['mean_meter_reading_on_year_built'] = train['year_built'].map(mean_meter_reading_on_year_built)\n# median_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].median()\n# train['median_meter_reading_on_year_built'] = train['year_built'].map(median_meter_reading_on_year_built)\nstd_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].std()\ntrain['std_meter_reading_on_year_built'] = train['year_built'].map(std_meter_reading_on_year_built)\nmean_meter_reading_per_meter = train.groupby('meter')['meter_reading'].mean()\ntrain['mean_meter_reading_per_meter'] = train['meter'].map(mean_meter_reading_per_meter)\n# median_meter_reading_per_meter = train.groupby('meter')['meter_reading'].median()\n# train['median_meter_reading_per_meter'] = train['meter'].map(median_meter_reading_per_meter)\nstd_meter_reading_per_meter = train.groupby('meter')['meter_reading'].std()\ntrain['std_meter_reading_per_meter'] = train['meter'].map(std_meter_reading_per_meter)\nmean_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].mean()\ntrain['mean_meter_reading_per_primary_usage'] = train['primary_use'].map(mean_meter_reading_per_primary_usage)\n# median_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].median()\n# train['median_meter_reading_per_primary_usage'] = train['primary_use'].map(median_meter_reading_per_primary_usage)\nstd_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].std()\ntrain['std_meter_reading_per_primary_usage'] = train['primary_use'].map(std_meter_reading_per_primary_usage)\nmean_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].mean()\ntrain['mean_meter_reading_per_site_id'] = train['site_id'].map(mean_meter_reading_per_site_id)\n# median_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].median()\n# train['median_meter_reading_per_site_id'] = train['site_id'].map(median_meter_reading_per_site_id)\nstd_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].std()\ntrain['std_meter_reading_per_site_id'] = train['site_id'].map(std_meter_reading_per_site_id)\n\n\n# test['number_unique_meter_per_building'] = test['building_id'].map(number_unique_meter_per_building)\n\n# test['mean_meter_reading_per_building'] = test['building_id'].map(mean_meter_reading_per_building)\n# test['median_meter_reading_per_building'] = test['building_id'].map(median_meter_reading_per_building)\n# test['std_meter_reading_per_building'] = test['building_id'].map(std_meter_reading_per_building)\n\n# test['mean_meter_reading_on_year_built'] = test['year_built'].map(mean_meter_reading_on_year_built)\n# test['median_meter_reading_on_year_built'] = test['year_built'].map(median_meter_reading_on_year_built)\n# test['std_meter_reading_on_year_built'] = test['year_built'].map(std_meter_reading_on_year_built)\n\n# test['mean_meter_reading_per_meter'] = test['meter'].map(mean_meter_reading_per_meter)\n# test['median_meter_reading_per_meter'] = test['meter'].map(median_meter_reading_per_meter)\n# test['std_meter_reading_per_meter'] = test['meter'].map(std_meter_reading_per_meter)\n\n# test['mean_meter_reading_per_primary_usage'] = test['primary_use'].map(mean_meter_reading_per_primary_usage)\n# test['median_meter_reading_per_primary_usage'] = test['primary_use'].map(median_meter_reading_per_primary_usage)\n# test['std_meter_reading_per_primary_usage'] = test['primary_use'].map(std_meter_reading_per_primary_usage)\n\n# test['mean_meter_reading_per_site_id'] = test['site_id'].map(mean_meter_reading_per_site_id)\n# test['median_meter_reading_per_site_id'] = test['site_id'].map(median_meter_reading_per_site_id)\n# test['std_meter_reading_per_site_id'] = test['site_id'].map(std_meter_reading_per_site_id)","36e394c5":"%%time\n# for df in [train, test]:\nfor df in [train]:\n    df['mean_meter_reading_per_building'] = df['mean_meter_reading_per_building'].astype(\"float16\")\n#     df['median_meter_reading_per_building'] = df['mean_meter_reading_per_building'].astype(\"float16\")\n    df['std_meter_reading_per_building'] = df['std_meter_reading_per_building'].astype(\"float16\")\n    df['mean_meter_reading_on_year_built'] = df['mean_meter_reading_on_year_built'].astype(\"float16\")\n#     df['median_meter_reading_on_year_built'] = df['median_meter_reading_on_year_built'].astype(\"float16\")\n    df['std_meter_reading_on_year_built'] = df['std_meter_reading_on_year_built'].astype(\"float16\")\n    df['mean_meter_reading_per_meter'] = df['mean_meter_reading_per_meter'].astype(\"float16\")\n#     df['median_meter_reading_per_meter'] = df['median_meter_reading_per_meter'].astype(\"float16\")\n    df['std_meter_reading_per_meter'] = df['std_meter_reading_per_meter'].astype(\"float16\")\n    df['mean_meter_reading_per_primary_usage'] = df['mean_meter_reading_per_primary_usage'].astype(\"float16\")\n#     df['median_meter_reading_per_primary_usage'] = df['median_meter_reading_per_primary_usage'].astype(\"float16\")\n    df['std_meter_reading_per_primary_usage'] = df['std_meter_reading_per_primary_usage'].astype(\"float16\")\n    df['mean_meter_reading_per_site_id'] = df['mean_meter_reading_per_site_id'].astype(\"float16\")\n#     df['median_meter_reading_per_site_id'] = df['median_meter_reading_per_site_id'].astype(\"float16\")\n    df['std_meter_reading_per_site_id'] = df['std_meter_reading_per_site_id'].astype(\"float16\")\n    df['number_unique_meter_per_building'] = df['number_unique_meter_per_building'].astype('uint8')\n\ngc.collect()\n\n# Following columns can be dropped \n# ['site_id', 'median_meter_reading_per_building', 'median_meter_reading_on_year_built', 'median_meter_reading_per_meter', \n# 'median_meter_reading_per_primary_usage', 'median_meter_reading_per_site_id']\n","ff2b892c":"# %%time\n# # Let's check the correlation between the variables and eliminate the one's that have high correlation\n# # Threshold for removing correlated variables\n# threshold = 0.9\n\n# # Absolute value correlation matrix\n# corr_matrix = train.corr().abs()\n# # Upper triangle of correlations\n# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# del corr_matrix\n# gc.collect()\n\n# # Select columns with correlations above threshold\n# to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n# del upper \n# gc.collect()\n\n# print('There are %d columns to remove.' % (len(to_drop)))\n# print (\"Following columns can be dropped {}\".format(to_drop))\n\n# train.drop(to_drop, axis=1, inplace=True)\n# # test.drop(to_drop,axis=1,inplace=True)\n# # del to_drop\n# gc.collect()\n\n","c7c7b7db":"%%time\ny = train['meter_reading']\ntrain.drop('meter_reading',axis=1,inplace=True)\ntrain.drop('site_id',axis=1,inplace=True)\ntrain.drop('floor_count_imputed',axis=1,inplace=True)\n\ncategorical_cols = ['building_id','meter','Month','DayOfMonth','DayOfWeek','Hour','primary_use',\n                    'year_built','floor_count','year_built_imputed','air_temperature_imputed',\n                    'cloud_coverage_imputed','precip_depth_1_hr_imputed','sea_level_pressure_imputed','wind_direction_imputed',\n                    'wind_speed_imputed']\n\n","f4c52fa2":"meter_cut, bins = pd.cut(y, bins=50, retbins=True)\nmeter_cut.value_counts()","32cca98b":"x_train,x_test,y_train,y_test = train_test_split(train,y,test_size=0.2,random_state=42, stratify=meter_cut)\nprint (x_train.shape)\nprint (y_train.shape)\nprint (x_test.shape)\nprint (y_test.shape)\n\ntrain_columns = train.columns\ndel train\ndel meter_cut\ndel bins\ngc.collect()","7554eb06":"from sklearn.ensemble import RandomForestRegressor as RF\nimport lightgbm as lgb","f57f5f0c":"lgb_train = lgb.Dataset(x_train, y_train,categorical_feature=categorical_cols)\nlgb_test = lgb.Dataset(x_test, y_test,categorical_feature=categorical_cols)\ndel x_train, x_test , y_train, y_test\ngc.collect()\n\nparams = {'feature_fraction': 0.75,\n          'bagging_fraction': 0.75,\n          'objective': 'regression',\n          'max_depth': -1,\n          'learning_rate': 0.15,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'rmse',\n          \"verbosity\": -1,\n          'reg_alpha': 0.5,\n          'reg_lambda': 0.5,\n          'random_state': 47\n         }\n\nreg = lgb.train(params, lgb_train, num_boost_round=3000, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=100, verbose_eval = 100)\n\ndel lgb_train, lgb_test\ngc.collect() \n","94a9f3ef":"# ser = pd.DataFrame(reg.feature_importance(),train.columns,columns=['Importance']).sort_values(by='Importance')\nser = pd.DataFrame(reg.feature_importance(),train_columns,columns=['Importance']).sort_values(by='Importance')\nser['Importance'].plot(kind='bar',figsize=(10,6))\n\n#del train\ndel ser\ndel train_columns\ngc.collect() \n","0c5a1a2c":"test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/test.csv\", parse_dates=['timestamp'], usecols=['building_id','meter','timestamp'], dtype=train_dtype)\n\n#\nfor df in [test]:\n    df['Month'] = df['timestamp'].dt.month.astype(\"uint8\")\n    df['DayOfMonth'] = df['timestamp'].dt.day.astype(\"uint8\")\n    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n    df['Hour'] = df['timestamp'].dt.hour.astype(\"uint8\")\n    df['timestamp_2'] = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() \/\/ 3600\n    df['timestamp_2'] = df.timestamp_2.astype('uint16')\n\nsite_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]\nGMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n\n# weather_test_imputed.timestamp_2 = weather_test_imputed.timestamp_2 + weather_test_imputed.site_id.map(GMT_offset_map)\n# weather_test_imputed.drop('timestamp',axis=1,inplace=True)\n# weather_test.timestamp_2 = weather_test.timestamp_2 + weather_test.site_id.map(GMT_offset_map)\n# weather_test.drop('timestamp',axis=1,inplace=True)\n\n#\ntest.rename(columns={'timestamp':'timestamp_test'}, inplace=True)\n\n#\n##########\nmetadata = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\", dtype=metadata_dtype)\n# cols = list(metadata.columns[4:])\n# cols_imputed = metadata[cols].isnull().astype('uint8').add_suffix('_imputed')\n\n# imp = IterativeImputer(max_iter=10, verbose=0)\n# imp.fit(metadata.iloc[:,3:])\n# metadata_imputed = imp.transform(metadata.iloc[:,3:])\n# metadata_imputed = pd.concat([metadata.iloc[:,0:3],pd.DataFrame(metadata_imputed, columns=metadata.columns[3:]), cols_imputed], axis=1)\n# #metadata_imputed = pd.concat([metadata.iloc[:,0:3],pd.DataFrame(metadata_imputed, columns=metadata.columns[3:])], axis=1)\n# pd.DataFrame(metadata_imputed.isna().sum()\/len(metadata_imputed),columns=[\"Metadata_Missing_Imputed\"])\n\n# metadata_imputed.year_built = metadata_imputed.year_built.round()\n# metadata_imputed.floor_count = metadata_imputed.floor_count.round()\n\n# del metadata\n# del cols\n# del cols_imputed\n# gc.collect()\n\n##########\n\ntest  = pd.merge(test,metadata,on='building_id',how='left')\nprint (\"Testing Data+Metadata Shape {}\".format(test.shape))\ndel metadata\ngc.collect()\n\nweather_test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\", parse_dates=['timestamp'], dtype=weather_dtype)\nprint('Size of weather_test_df data', weather_test.shape)\n\nweather_test['timestamp_2'] = (weather_test.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() \/\/ 3600\nweather_test['timestamp_2'] = weather_test.timestamp_2.astype('int16')\n\nweather_test.timestamp_2 = weather_test.timestamp_2 + weather_test.site_id.map(GMT_offset_map)\nweather_test.drop('timestamp',axis=1,inplace=True)\n\ntest  = pd.merge(test,weather_test,on=['site_id','timestamp_2'],how='left')\nprint (\"Testing Data+Metadata+Weather Shape {}\".format(test.shape))\n\ntest['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)\ndel weather_test\ngc.collect()\n\n#\ntest.drop('timestamp_test',axis=1,inplace=True)\ntest.drop('timestamp_2',axis=1,inplace=True)\ngc.collect()\n\nle = LabelEncoder()\ntest['meter']= le.fit_transform(test['meter']).astype(\"uint8\")\ntest['primary_use']= le.fit_transform(test['primary_use']).astype(\"uint8\")\nprint(test.shape)\n\n#\n\n","b0f6a6db":"################################################\n# Imputation\n\ncols = list(test.columns[9:])\ncols_imputed = test[cols].isnull().astype('uint8').add_suffix('_imputed')\n\nimp = IterativeImputer(max_iter=10, verbose=0)\nimp.fit(test.iloc[:,7:])\ntest_temp = imp.transform(test.iloc[:,7:])\ntest = pd.concat([test.iloc[:,0:7],pd.DataFrame(test_temp, columns=test.columns[7:]), cols_imputed], axis=1)\ntest.drop('site_id',axis=1,inplace=True)\ntest.drop('floor_count_imputed',axis=1,inplace=True)\n# weather_train_imputed = pd.concat([weather_train.iloc[:,0:2],pd.DataFrame(weather_train_imputed, columns=weather_train.columns[2:])], axis=1)\npd.DataFrame(test.isna().sum()\/len(test),columns=[\"Weather_Train_Missing_Imputed\"])\n\ndel test_temp\ndel cols\ndel cols_imputed\ngc.collect()\n\ntest['square_feet'] = test['square_feet'].astype('float16')\ntest['year_built'] = test['year_built'].astype('uint16')\ntest['floor_count'] = test['floor_count'].astype('uint8')\ntest['primary_use'] = test['primary_use'].astype('uint8')\ntest['air_temperature'] = test['air_temperature'].astype('float16')\ntest['cloud_coverage'] = test['cloud_coverage'].astype('float16')\ntest['dew_temperature'] = test['dew_temperature'].astype('float16')\ntest['precip_depth_1_hr'] = test['precip_depth_1_hr'].astype('float16')\ntest['wind_direction'] = test['wind_direction'].astype('float16')\ntest['wind_speed'] = test['wind_speed'].astype('float16')\n\n\n\n################################################\n\n\n#\ntest['number_unique_meter_per_building'] = test['building_id'].map(number_unique_meter_per_building)\ntest['number_unique_meter_per_building'] = test['number_unique_meter_per_building'].astype('uint8')\ntest['mean_meter_reading_per_building'] = test['building_id'].map(mean_meter_reading_per_building)\ntest['mean_meter_reading_per_building'] = test['mean_meter_reading_per_building'].astype(\"float16\")\n# test['median_meter_reading_per_building'] = test['building_id'].map(median_meter_reading_per_building)\ntest['std_meter_reading_per_building'] = test['building_id'].map(std_meter_reading_per_building)\ntest['std_meter_reading_per_building'] = test['std_meter_reading_per_building'].astype(\"float16\")\ntest['mean_meter_reading_on_year_built'] = test['year_built'].map(mean_meter_reading_on_year_built)\ntest['mean_meter_reading_on_year_built'] = test['mean_meter_reading_on_year_built'].astype(\"float16\")\n# test['median_meter_reading_on_year_built'] = test['year_built'].map(median_meter_reading_on_year_built)\ntest['std_meter_reading_on_year_built'] = test['year_built'].map(std_meter_reading_on_year_built)\ntest['std_meter_reading_on_year_built'] = test['std_meter_reading_on_year_built'].astype(\"float16\")\ntest['mean_meter_reading_per_meter'] = test['meter'].map(mean_meter_reading_per_meter)\ntest['mean_meter_reading_per_meter'] = test['mean_meter_reading_per_meter'].astype(\"float16\")\n# test['median_meter_reading_per_meter'] = test['meter'].map(median_meter_reading_per_meter)\ntest['std_meter_reading_per_meter'] = test['meter'].map(std_meter_reading_per_meter)\ntest['std_meter_reading_per_meter'] = test['std_meter_reading_per_meter'].astype(\"float16\")\ntest['mean_meter_reading_per_primary_usage'] = test['primary_use'].map(mean_meter_reading_per_primary_usage)\ntest['mean_meter_reading_per_primary_usage'] = test['mean_meter_reading_per_primary_usage'].astype(\"float16\")\n# test['median_meter_reading_per_primary_usage'] = test['primary_use'].map(median_meter_reading_per_primary_usage)\ntest['std_meter_reading_per_primary_usage'] = test['primary_use'].map(std_meter_reading_per_primary_usage)\ntest['std_meter_reading_per_primary_usage'] = test['std_meter_reading_per_primary_usage'].astype(\"float16\")\ntest['mean_meter_reading_per_site_id'] = test['site_id'].map(mean_meter_reading_per_site_id)\ntest['mean_meter_reading_per_site_id'] = test['mean_meter_reading_per_site_id'].astype(\"float16\")\n# test['median_meter_reading_per_site_id'] = test['site_id'].map(median_meter_reading_per_site_id)\ntest['std_meter_reading_per_site_id'] = test['site_id'].map(std_meter_reading_per_site_id)\ntest['std_meter_reading_per_site_id'] = test['std_meter_reading_per_site_id'].astype(\"float16\")\n\n#\n# for df in [test]:\n#     df['mean_meter_reading_per_building'] = df['mean_meter_reading_per_building'].astype(\"float16\")\n#     df['median_meter_reading_per_building'] = df['mean_meter_reading_per_building'].astype(\"float16\")\n#     df['std_meter_reading_per_building'] = df['std_meter_reading_per_building'].astype(\"float16\")\n#     df['mean_meter_reading_on_year_built'] = df['mean_meter_reading_on_year_built'].astype(\"float16\")\n#     df['median_meter_reading_on_year_built'] = df['median_meter_reading_on_year_built'].astype(\"float16\")\n#     df['std_meter_reading_on_year_built'] = df['std_meter_reading_on_year_built'].astype(\"float16\")\n#     df['mean_meter_reading_per_meter'] = df['mean_meter_reading_per_meter'].astype(\"float16\")\n#     df['median_meter_reading_per_meter'] = df['median_meter_reading_per_meter'].astype(\"float16\")\n#     df['std_meter_reading_per_meter'] = df['std_meter_reading_per_meter'].astype(\"float16\")\n#     df['mean_meter_reading_per_primary_usage'] = df['mean_meter_reading_per_primary_usage'].astype(\"float16\")\n#     df['median_meter_reading_per_primary_usage'] = df['median_meter_reading_per_primary_usage'].astype(\"float16\")\n#     df['std_meter_reading_per_primary_usage'] = df['std_meter_reading_per_primary_usage'].astype(\"float16\")\n#     df['mean_meter_reading_per_site_id'] = df['mean_meter_reading_per_site_id'].astype(\"float16\")\n#     df['median_meter_reading_per_site_id'] = df['median_meter_reading_per_site_id'].astype(\"float16\")\n#     df['std_meter_reading_per_site_id'] = df['std_meter_reading_per_site_id'].astype(\"float16\")\n#     df['number_unique_meter_per_building'] = df['number_unique_meter_per_building'].astype('uint8')\n\n\n#\n# le = LabelEncoder()\n# test['meter']= le.fit_transform(test['meter']).astype(\"uint8\")\n# test['primary_use']= le.fit_transform(test['primary_use']).astype(\"uint8\")\n# print(test.shape)\n\n# #\n# test.drop(to_drop,axis=1,inplace=True)\n# del to_drop\ngc.collect()\n\n","b14e20c8":"%%time\npredictions = []\nstep = 50000\nfor i in range(0, len(test), step):\n    predictions.extend(np.expm1(reg.predict(test.iloc[i: min(i+step, len(test)), :], num_iteration=reg.best_iteration)))","ad5efd56":"%%time\nSubmission = pd.DataFrame(test.index,columns=['row_id'])\nSubmission['meter_reading'] = predictions\nSubmission['meter_reading'].clip(lower=0,upper=None,inplace=True)\nSubmission.to_csv(\"lgbm.csv\",index=None)","3e54eec5":"Data Overview ","b83f5c10":"x_train = pd.get_dummies(x_train, columns=categorical_cols, sparse=True)\n\nx_test = pd.get_dummies(x_test, columns=categorical_cols, sparse=True)\n\ngc.collect()\n\nx_train.shape","83804430":"Imports","3b41157b":"Construct date features","e528cbbf":"Split the data for train and validation with stratification by meter reading bins","95a44486":"Drop correlated variables","7d183a7f":"Prepare data","796fd90d":"Data treatment\n\n.- Drop some columns ased on EDA  \n.- Convert target to log scale  \n.- Preprocess metadata  \n\n\n","585e32c3":"Team: RF  \nMembers: Jose Rodrigo Flores Espinosa  \n\nThis kernel addreses the challenge posed by ASHRAE and titled: \"ASHRAE - Great Energy Predictor III - How much energy will a building consume?\"\nDetails about the problem posed can be found in the [main page of the competition](https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction).\n\n","9e41479d":"Check feature importance","e536ab35":"Merge data","e0647f91":"Make dummies if necessary -- for RF","876e9955":"## Model\n\nTrain baseline model","cfc6d48f":"#### Imputation","4a9a99f2":"Measure meter stats","ddb45d06":"Load the data reducing its size","3730db9d":"## Predict","445a0af0":"#### loading and processing of test objects"}}