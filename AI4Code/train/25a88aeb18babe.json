{"cell_type":{"5e72e311":"code","330dd421":"code","8d072266":"code","079ad3dc":"code","7db7bac6":"code","d0fbf303":"code","74d1c749":"code","0f707833":"code","e4147324":"code","2f9e7a90":"code","692d590a":"code","130f2668":"code","3dc3843c":"code","61e489a0":"code","6e956073":"code","9df2063d":"code","7d075b6c":"code","6dea7d3c":"code","e7208de8":"code","84501855":"code","55c63caf":"markdown","24948f08":"markdown","ad3666f4":"markdown","65f86433":"markdown","5e896b47":"markdown","21146abb":"markdown","9712dea6":"markdown","88d67d25":"markdown","0f9e0690":"markdown","e8a8889b":"markdown","e0a8f12e":"markdown","182fdead":"markdown","7904351f":"markdown","b6df773c":"markdown","8bc8b90d":"markdown","b54f6a86":"markdown","c8578db3":"markdown","882fd1e0":"markdown","743b3792":"markdown","aaeda07d":"markdown","c8b81ed0":"markdown"},"source":{"5e72e311":"!ls ..\/input\/sarcasm\/","330dd421":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.preprocessing import FunctionTransformer","8d072266":"train_df = pd.read_csv('..\/input\/sarcasm\/train-balanced-sarcasm.csv')","079ad3dc":"train_df.head()","7db7bac6":"train_df.info()","d0fbf303":"train_df.dropna(subset=['comment'], inplace=True)","74d1c749":"train_df['label'].value_counts()","0f707833":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","e4147324":"from wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud(stopwords=STOPWORDS, max_words=100, max_font_size=100, width=1000, height=500)","2f9e7a90":"plt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df[train_df['label'] == 1]['comment']))\nplt.axis(\"off\")\nplt.imshow(wordcloud);","692d590a":"plt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df[train_df['label'] == 0]['comment']))\nplt.axis(\"off\")\nplt.imshow(wordcloud);","130f2668":"stopwords = []\nfor word in STOPWORDS:\n    for splited in word.split(\"'\"):\n        stopwords.append(splited)","3dc3843c":"def get_most_frequent_words(data, n, amount):\n    cv = TfidfVectorizer(analyzer='word', stop_words=stopwords, ngram_range=(n, n))\n    cv.fit(data)\n    wl = cv.transform(data).sum(axis=0).A[0,:]\n    n_grams_list = []\n    \n    for item in cv.vocabulary_.items():\n        n_grams_list.append([item[0], wl[item[1]]])    \n    n_grams_list.sort(key=lambda x: x[1], reverse=True)\n    \n    return pd.DataFrame(data=sorted(n_grams_list[:amount], key=lambda x: x[1]), columns=['Words', 'Amount'])","61e489a0":"get_most_frequent_words(train_df[train_df[\"label\"] == 0][\"comment\"], 2, 30).plot(kind='barh', x='Words', figsize=(13,8));","6e956073":"get_most_frequent_words(train_df[train_df[\"label\"] == 1][\"comment\"], 2, 30).plot(kind='barh', x='Words', figsize=(13,8));","9df2063d":"tfidf = TfidfVectorizer(ngram_range=(1, 2))\nlogistic_regression = LogisticRegression(random_state=17, solver='lbfgs')\npipeline = Pipeline([('tfidf', tfidf), ('logistic_regression', logistic_regression)])\npipeline.fit(train_texts, y_train)\ntfidf.fit(train_texts)\nvalid_pred = pipeline.predict(valid_texts)\naccuracy_score(y_valid, valid_pred)","7d075b6c":"import eli5\neli5.show_weights(logistic_regression, vec=tfidf)","6dea7d3c":"train_texts_improved, valid_texts_improved = train_test_split(train_df, random_state=17)","e7208de8":"comment_tfidf = Pipeline([('comment',FunctionTransformer(lambda x: x['comment'], validate=False)), ('tfidf', TfidfVectorizer(ngram_range=(1, 2)))])\nsubreddit_tfidf = Pipeline([('subreddit', FunctionTransformer(lambda x: x['subreddit'], validate=False)), ('tfidf', TfidfVectorizer(ngram_range=(1, 1)))])\ntransformer = FeatureUnion([('comment_tfidf', comment_tfidf), ('subreddit_tfidf', subreddit_tfidf)]) \nlogistic_regression_improved = LogisticRegression(random_state=17, solver='lbfgs')\npipeline_improved = Pipeline([('transformer', transformer), ('logistic_regression_improved', logistic_regression_improved)])","84501855":"pipeline_improved.fit(train_texts_improved, y_train)\nvalid_pred = pipeline_improved.predict(valid_texts_improved)\naccuracy_score(y_valid, valid_pred)","55c63caf":"Splitting data into training and validation parts.","24948f08":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","ad3666f4":"First of all we to need slightly improve the stopword-list","65f86433":"Some comments are missing, so we drop the corresponding rows.","5e896b47":"Creating pipeline","21146abb":"**3. Bigrams which are most predictive of sarcasm**","9712dea6":"We notice that the dataset is indeed balanced","88d67d25":"Now we need to write function to get most frequent words","0f9e0690":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https:\/\/arxiv.org\/abs\/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https:\/\/www.kaggle.com\/danofer\/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https:\/\/habrastorage.org\/webt\/1f\/0d\/ta\/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" \/>","e8a8889b":"**2. Tf-Idf + logistic regression**","e0a8f12e":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words\/bigrams which a most predictive of sarcasm (you can use [eli5](https:\/\/github.com\/TeamHG-Memex\/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-2-classification) and its applications to [text classification](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https:\/\/github.com\/TeamHG-Memex\/eli5) to explain model predictions","182fdead":"WordCloud of non-sarcastic comments","7904351f":"**Most frequent bigrams for normal and saracastic comments.**","b6df773c":"**WordCloud**","8bc8b90d":"**1. Plots**","b54f6a86":"WordCloud of sarcastic comments","c8578db3":"We split data into training and validation parts.","882fd1e0":"Most freequent words plot for normal comments","743b3792":"Training the model","aaeda07d":"Most freequent words plot for sarcastic comments","c8b81ed0":"**4. Improving model performance**"}}