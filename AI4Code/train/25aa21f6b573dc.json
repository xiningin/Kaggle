{"cell_type":{"e47c5b09":"code","4b4cdb88":"code","d704a68c":"code","def0f5f4":"code","c17e75e4":"code","6f53893e":"code","d649fde4":"code","e880845a":"code","71a22847":"code","0474a49a":"code","2e296dd2":"code","cd34958e":"code","ad6023f2":"markdown","4c521992":"markdown","983a422d":"markdown","1a938d1a":"markdown","eb541425":"markdown","9c592a5c":"markdown","459f8d8c":"markdown","a74ad9bd":"markdown","3bc82971":"markdown"},"source":{"e47c5b09":"#Importanto as bibliotecas e definindo padr\u00f5es de sa\u00edda\n\n%matplotlib inline\nimport math,sys,os,numpy as np\nfrom numpy.random import random\nfrom matplotlib import pyplot as plt, rcParams, animation, rc\nfrom __future__ import print_function, division\nfrom ipywidgets import interact, interactive, fixed\nfrom ipywidgets.widgets import *\nrc('animation', html='html5')\nrcParams['figure.figsize'] = 3, 3\n%precision 4\nnp.set_printoptions(precision=4, linewidth=100)","4b4cdb88":"# Fun\u00e7\u00e3o para a equa\u00e7\u00e3o y = ax + b\ndef lin(a,b,x): \n    return a*x+b","d704a68c":"a=3.\nb=8.","def0f5f4":"n=30\nx = random(n)\ny = lin(a,b,x)","c17e75e4":"x","6f53893e":"y","d649fde4":"plt.scatter(x,y)","e880845a":"def sse(y,y_pred): return ((y-y_pred)**2).sum()\ndef loss(y,a,b,x): return sse(y, lin(a,b,x))\ndef mre(y,a,b,x): return np.sqrt(loss(y,a,b,x)\/n)","71a22847":"a_guess=-1.\nb_guess=1.\nmre(y, a_guess, b_guess, x)","0474a49a":"lr=1","2e296dd2":"def back_propagation():\n    global a_guess, b_guess\n    y_pred = lin(a_guess, b_guess, x)\n    dydb = 2 * (y_pred - y)\n    dyda = x*dydb\n    a_guess -= lr*dyda.mean()\n    b_guess -= lr*dydb.mean()","cd34958e":"back_propagation()\nplt.scatter(x,y)\nline, = plt.plot(x,lin(a_guess,b_guess,x))\nprint(a_guess, b_guess, mre(y,x,a_guess, b_guess))","ad6023f2":"Agora inicializamos nossos par\u00e2metros \"aleatoriamente\" com os valores -1 e +1 e calculamos a m\u00e9trica de custo","4c521992":"Gerando 30 pontos aleatoriamente para X e encontrando o Y correspondente usando <b>y=3x+8<\/b>","983a422d":"Nessa demonstra\u00e7\u00e3o explicamos com um exemplo de uma equa\u00e7\u00e3o linear simples, como funciona a otimiza\u00e7\u00e3o dos par\u00e2metros de um modelo matem\u00e1tico pelo m\u00e9todo do Gradiente Descendente. Mais informa\u00e7\u00f5es podem ser encontradas aqui: <a href=\"https:\/\/en.wikipedia.org\/wiki\/Gradient_descent\">https:\/\/en.wikipedia.org\/wiki\/Gradient_descent<\/a>","1a938d1a":"$$\\frac{\\partial{(y-(a*x + b))^2}}{\\partial{b}} = 2*(b + a*x - y)$$\n\n$$\\frac{\\partial{(y-(a*x + b))^2}}{\\partial{a}} = 2*x*(b + a*x - y) = x* \\frac{\\partial{y}}{\\partial{b}}$$","eb541425":"A fun\u00e7\u00e3o `back_propagation` atualiza os par\u00e2metros `a_guess` e `b_guess`","9c592a5c":"# Demonstra\u00e7\u00e3o - Stochastic Gradient Descent\n ","459f8d8c":"Aqui definimos quais os par\u00e2metros queremos perseguir. Ou seja, a equa\u00e7\u00e3o \u00f3tima \u00e9 <b>y=3x+8<\/b>","a74ad9bd":"Aqui definimos a nossa fun\u00e7\u00e3o de custo, a que desejamos minimizar, definida por $MRE = \\sqrt{\\frac{1}{n}*{\\sum(Yreal - Ypred)^2}}$","3bc82971":"Aqui definimos a taxa de aprendizado, que \u00e9 o quanto vamos \"caminhar\" na dire\u00e7\u00e3o oposta ao gradiente, que define o vetor de crescimento de uma fun\u00e7\u00e3o. \n\nO gradiente \u00e9 calculado pela derivada parcial da equa\u00e7\u00e3o em rela\u00e7\u00e3o a cada par\u00e2metro:\n"}}