{"cell_type":{"1dd2622e":"code","9360e0e8":"code","bc2c7645":"code","7a82d6cb":"code","ff18f675":"code","17c948fe":"code","f69c650f":"code","a26edb71":"code","5db26ccb":"code","79156451":"code","875e2fad":"code","1b0a6093":"code","45464220":"code","ed89b293":"code","53f138c2":"code","cc8f7f79":"code","64509583":"code","7478f49a":"code","73743248":"markdown","ff780457":"markdown","94b74bf8":"markdown","da388fe2":"markdown","2d2ef797":"markdown"},"source":{"1dd2622e":"import numpy as np\nimport os\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import make_pipeline, FeatureUnion, Pipeline\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV, cross_validate, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, make_scorer, mean_squared_error, mean_absolute_error \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, roc_auc_score, auc, log_loss, r2_score\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom scipy.stats import ks_2samp\nfrom imblearn.over_sampling import SMOTE","9360e0e8":"train = pd.read_csv('..\/input\/dont-overfit-ii\/train.csv')\ntest = pd.read_csv('..\/input\/dont-overfit-ii\/test.csv')","bc2c7645":"train.head()","7a82d6cb":"train.shape","ff18f675":"train.isnull().sum()[train.isnull().sum() > 0]","17c948fe":"train.info()","f69c650f":"#from kernel  \"https:\/\/www.kaggle.com\/nanomathias\/distribution-of-test-vs-training-data\"\ndef get_diff_columns(train_df, test_df, show_plots=True, show_all=False, threshold=0.1):\n    \"\"\"Use KS to estimate columns where distributions differ a lot from each other\"\"\"\n\n    # Find the columns where the distributions are very different\n    diff_data = []\n    for col in tqdm(train_df.columns):\n        statistic, pvalue = ks_2samp(\n            train_df[col].values, \n            test_df[col].values\n        )\n        if pvalue > 0.05 and np.abs(statistic) < threshold:\n            diff_data.append({'feature': col, 'p': np.round(pvalue, 5), 'statistic': np.round(np.abs(statistic), 2)})\n\n    # Put the differences into a dataframe\n    diff_df = pd.DataFrame(diff_data).sort_values(by='statistic', ascending=False)\n    print(f\"number of features with diff distribution : {len(diff_df)}\")\n    if show_plots:\n        # Let us see the distributions of these columns to confirm they are indeed different\n        n_cols = 5\n        n_rows = 5\n        _, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n        axes = [x for l in axes for x in l]\n\n        # Create plots\n        for i, (_, row) in enumerate(diff_df.iterrows()):\n            if i >= len(axes):\n                break\n            extreme = np.max(np.abs(train_df[row.feature].tolist() + test_df[row.feature].tolist()))\n            train_df.loc[:, row.feature].apply(np.log1p).hist(\n                ax=axes[i], alpha=0.5, label='Train', density=True,\n                bins=np.arange(-extreme, extreme, 0.25)\n            )\n            test_df.loc[:, row.feature].apply(np.log1p).hist(\n                ax=axes[i], alpha=0.5, label='Test', density=True,\n                bins=np.arange(-extreme, extreme, 0.25)\n            )\n            axes[i].set_title(f\"Statistic = {row.statistic}, p = {row.p}\")\n            axes[i].set_xlabel(f'Log({row.feature})')\n            axes[i].legend()\n\n        plt.tight_layout()\n        plt.show()\n        \n    return diff_df\n\n# Get the columns which differ a lot between test and train\ndiff_df = get_diff_columns(train.drop(['id','target'], axis=1), test.drop(['id'], axis=1))","a26edb71":"corr_with_y = pd.DataFrame(train.drop(['id','target'], axis=1).corrwith(train[\"target\"]).abs()).reset_index()\ncorr_with_y.columns = [\"Feature\", \"Correlation with Target\"]\ncorr_with_y = corr_with_y.sort_values(by=\"Correlation with Target\", ascending=False)\ncorr_with_y.head(10)","5db26ccb":"# Drop identity and target columns\nvariables_train = train.drop(['id','target'], axis=1).values\nvar_resp = train[\"target\"].copy()\nvariables_test = test.drop(['id'], axis=1).values","79156451":"(var_resp.value_counts()\/var_resp.count())*100","875e2fad":"def with_statistics(X):\n    statistics = pd.DataFrame()\n    statistics['mean']   = X.mean(axis=1)\n    statistics['std']    = X.std(axis=1)\n    statistics['kurt']   = X.kurt(axis=1)\n    statistics['mad']    = X.mad(axis=1)\n    statistics['median'] = X.median(axis=1)\n    statistics['max']    = X.max(axis=1)\n    statistics['min']    = X.min(axis=1)\n    statistics['skew']   = X.skew(axis=1)\n    statistics['sem']    = X.sem(axis=1)\n    \n    from sklearn.neighbors import NearestNeighbors\n    neigh = NearestNeighbors(5, n_jobs=-1)\n    neigh.fit(X)\n\n    dists, _ = neigh.kneighbors(X, n_neighbors=5)\n    dists = np.delete(dists, 0, 1)\n    statistics['minDist'] = dists.mean(axis=1)\n    statistics['maxDist'] = dists.max(axis=1)\n    statistics['meanDist'] = dists.min(axis=1)\n\n# Trigometric FE\n    sin_temp = np.sin(X)\n    cos_temp = np.cos(X)\n    tan_temp = np.tan(X)\n    statistics['mean_sin'] = np.mean(sin_temp, axis=1)\n    statistics['mean_cos'] = np.mean(cos_temp, axis=1)\n    statistics['mean_tan'] = np.mean(tan_temp, axis=1)\n# Hyperbolic FE\n    sinh_temp = np.sinh(X)\n    cosh_temp = np.cosh(X)\n    tanh_temp = np.tanh(X)\n    statistics['mean_sinh'] = np.mean(sinh_temp, axis=1)\n    statistics['mean_cosh'] = np.mean(cosh_temp, axis=1)\n    statistics['mean_tanh'] = np.mean(tanh_temp, axis=1)\n# Exponents FE\n    exp_temp = np.exp(X)\n    expm1_temp = np.expm1(X)\n    exp2_temp = np.exp2(X)\n    statistics['mean_exp'] = np.mean(exp_temp, axis=1)\n    statistics['mean_expm1'] = np.mean(expm1_temp, axis=1)\n    statistics['mean_exp2'] = np.mean(exp2_temp, axis=1)\n# Polynomial FE\n    # X**2\n    statistics['mean_x2'] = np.mean(np.power(X, 2), axis=1)\n    # X**3\n    statistics['mean_x3'] = np.mean(np.power(X, 3), axis=1)\n    # X**4\n    statistics['mean_x4'] = np.mean(np.power(X, 4), axis=1)\n    \n    X = pd.concat([X, statistics], axis=1)\n    return X","1b0a6093":"# Here I apply a Pipeline to standardize the scale on numerical data\n# As we don't have missings and categorical data, I don't need to worry about this part\n# As we have 282\/301 variables with different distribution on training and test basis, we will standardize with RobustScaler\n\npreprocessor = Pipeline([\n        ('selector', VarianceThreshold()),\n        ('std_scaler', RobustScaler())\n    ])","45464220":"data = preprocessor.fit_transform(np.concatenate((variables_train, variables_test), axis=0))\nvariables_train = data[:variables_train.shape[0]]\nvariables_test = data[variables_train.shape[0]:]","ed89b293":"variables_train = with_statistics(pd.DataFrame(variables_train)).values\nvariables_test = with_statistics(pd.DataFrame(variables_test)).values","53f138c2":"# define roc_auc_metric robust to only one class in y_pred\ndef scoring_roc_auc(y, y_pred):\n    try:\n        return roc_auc_score(y, y_pred)\n    except:\n        return 0.5\n\nrobust_roc_auc = make_scorer(scoring_roc_auc)","cc8f7f79":"param_grid = [\n    {\n        'C': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],\n        'tol': [0.00009, 0.0001, 0.00011],\n        'max_iter': [int(x) for x in np.linspace(start = 100, stop = 10000, num = 32)],\n        'penalty': ['l1', 'l2', 'elasticnet'],\n        'solver': ['liblinear','sag']\n    }\n]\n\nmodel = LogisticRegression(random_state=42, class_weight='balanced')\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=robust_roc_auc, cv=20)\ngrid_search.fit(variables_train, var_resp)\n\nfeature_selector = RFECV(grid_search.best_estimator_, verbose=0, min_features_to_select=10, scoring=robust_roc_auc, step=15, cv=20, n_jobs=-1)","64509583":"print(\"counter | val_mse  |  val_mae  |  val_roc  |  val_cos  |  val_dist  |  val_r2    | feature_count \")\nprint(\"-------------------------------------------------------------------------------------------------\")\n\npredictions = pd.DataFrame()\ncounter = 0\n# split training data to build one model on each traing-data-subset\nfor train_index, val_index in StratifiedKFold(n_splits=20, shuffle=True).split(variables_train, var_resp):\n    X, val_X = variables_train[train_index], variables_train[val_index]\n    y, val_y = var_resp[train_index], var_resp[val_index]\n\n    # get the best features for this data set\n    feature_selector.fit(X, y)\n    \n    # remove irrelevant features from X, val_X and test\n    X_important_features        = feature_selector.transform(X)\n    val_X_important_features    = feature_selector.transform(val_X)\n    test_important_features     = feature_selector.transform(variables_test)\n\n    # run grid search to find the best model parameters for this subset of training data and subset of features \n    grid_search = GridSearchCV(feature_selector.estimator_, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=robust_roc_auc, cv=20)\n    grid_search.fit(X_important_features, y)\n\n    # score our fitted model on validation data\n    val_y_pred = grid_search.best_estimator_.predict_proba(val_X_important_features)[:,1]\n    val_mse = mean_squared_error(val_y, val_y_pred)\n    val_mae = mean_absolute_error(val_y, val_y_pred)\n    val_roc = roc_auc_score(val_y, val_y_pred)\n    val_cos = cosine_similarity(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n    val_dst = euclidean_distances(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n    val_r2  = r2_score(val_y, val_y_pred)\n\n    # if model did well on validation, save its prediction on test data, using only important features\n    # r2_threshold (0.2) is a heuristic threshold for r2 error\n    # you can use any other metric\/metric combination that works for you\n    if val_r2 > 0.185:\n        message = '<-- OK'\n        prediction = grid_search.best_estimator_.predict_proba(test_important_features)[:,1]\n        predictions = pd.concat([predictions, pd.DataFrame(prediction)], axis=1)\n    else:\n        message = '<-- skipping'\n\n\n    print(\"{0:2}      | {1:.4f}   |  {2:.4f}   |  {3:.4f}   |  {4:.4f}   |  {5:.4f}    |  {6:.4f}    |  {7:3}         {8}  \".format(counter, val_mse, val_mae, val_roc, val_cos, val_dst, val_r2, feature_selector.n_features_, message))\n    \n    counter += 1","7478f49a":"mean_pred = pd.DataFrame(predictions.mean(axis=1))\nmean_pred.index += 250\nmean_pred.columns = ['target']\nmean_pred.to_csv('submission.csv', index_label='id', index=True)","73743248":"# Loading the data","ff780457":"# Imports","94b74bf8":"## Feature engineering","da388fe2":"# Preprocessing","2d2ef797":"# Modeling with hyperparameter tuning"}}