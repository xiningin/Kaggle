{"cell_type":{"49412cbd":"code","123b9c04":"code","7fa3e287":"code","afa08143":"code","fa525946":"code","477d3c7b":"code","be113351":"code","784f3227":"code","65903852":"code","4761010a":"code","628943b6":"code","c4b34aac":"code","574f8ec7":"code","84a2980d":"code","9dc48eeb":"code","d24d504c":"code","4f7cc950":"code","76ef0657":"code","9588c527":"code","dc1399f2":"code","30e3413f":"code","c80415ac":"code","748d30a0":"code","77e07443":"code","cd7a3960":"code","ff72614f":"code","21ea91f3":"code","912e9a91":"code","77475950":"code","cc84b5b2":"code","7031b3f6":"code","8453597d":"markdown","4bb3b939":"markdown","a952754b":"markdown","83a02bb0":"markdown","a77d68b3":"markdown","9539a936":"markdown","56ae445d":"markdown","47e4d5de":"markdown","0a50839e":"markdown","e16de27b":"markdown","6e5cf2f8":"markdown","ec514905":"markdown","d3b1a16d":"markdown","5dcb6476":"markdown"},"source":{"49412cbd":"import numpy as np\nimport seaborn as sns\nimport pandas as pd \nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import SGDRegressor, Ridge, TweedieRegressor, PoissonRegressor, ElasticNet\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVR","123b9c04":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","7fa3e287":"f,ax = plt.subplots(figsize=(12,2))\nplt.xticks(size = 14)\nplt.yticks(size = 14)\nbar1 =  ax.barh('train', train.shape[0], color=\"indianred\")\nbar2 =  ax.barh('test', test.shape[0], color=\"green\")\nax.set_title(\"Train and test datasets size comparison\", fontsize=20, pad=5)\nax.bar_label(bar1, [\"{0:.2f}%\".format((train.shape[0]\/(train.shape[0]+test.shape[0]))*100)], label_type=\"center\",\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.bar_label(bar2,[\"{0:.2f}%\".format((test.shape[0]\/(train.shape[0]+test.shape[0]))*100)], label_type=\"center\",\n             fontsize=20, color=\"white\", weight=\"bold\")\nplt.show()","afa08143":"train.head()","fa525946":"print(\" Shape \".center(100,'*'))\nprint('Rows: {}'.format(train.shape[0]))\nprint('Columns: {}'.format(train.shape[1]))\nprint(\" Head \".center(100,'*'))\nprint(train.head())\nprint(\" Types \".center(100,'*'))\nprint(train.dtypes)\nprint(\" Missing values \".center(100,'*'))\nprint(\"Missing values %:   {}%\".format(train.isna().sum().sum()\/(train.shape[0]*train.shape[1])*100))\nprint(train.isna().sum())\nprint(' Duplicated'.center(100,'*'))\nprint(train.duplicated().sum())","477d3c7b":"to_desc = train.drop(columns = ['id'])\ndesc = to_desc.describe().T.drop(columns = ['count'])\n\ndesc_df = pd.DataFrame(index= [col for col in to_desc.columns], \n                   columns= desc.describe().T.columns.tolist().remove('count'), data= desc )\n\nf,ax = plt.subplots(figsize=(10,50))\nsns.heatmap(desc_df, annot=True,cmap = \"coolwarm\", fmt= '.0f',\n            ax=ax,linewidths = 4, cbar = True,\n            annot_kws={\"size\": 8})\nax.xaxis.tick_top()\nplt.xticks(size = 14)\nplt.yticks(size = 14, rotation = 0)\nplt.title(\"Descriptive Statistics\", size = 16)\nplt.show()","be113351":"# correlation heatmap is too large\n\n\n# df_train = train.drop(columns = ['id','loss' ])\n# corr_target_pearson = df_train.corr(method='pearson')\n# corr_target_spearman = df_train.corr(method='spearman')\n\n# fig = plt.figure(figsize = (25,21))\n# sns.heatmap(corr_target_pearson, annot=True, cmap='YlGn',linewidth = 0.2, vmin=-1, vmax=+1, fmt = \".1f\")\n# plt.xticks(rotation=45)\n# plt.yticks(rotation=0)\n# plt.title('Pearson Correlation')\n# plt.show()\n\n# fig = plt.figure(figsize = (25,21))\n# sns.heatmap(corr_target_spearman, annot=True, cmap='YlGn',linewidth = 0.2, vmin=-1, vmax=+1, fmt = \".1f\")\n# plt.xticks(rotation=45)\n# plt.title('Spearman Correlation')\n# plt.show()","784f3227":"g = sns.displot(data = train, x = train.loss, color='forestgreen',kde=True, stat = 'density',aspect=3)\nplt.show()","65903852":"target = train.loss.copy()\ntarget","4761010a":"df_train = train.drop(columns = ['id','loss'])\ndf_train","628943b6":"df_test = test.drop(columns = ['id'])\ndf_test","c4b34aac":"features = []\nfor feature in df_train.columns:\n    features.append(feature)","574f8ec7":"scaler = StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","84a2980d":"pca = PCA()\npca.fit(df_train.to_numpy())\n\nplt.figure(figsize =(12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')  # for each component\nplt.title('Segmentation Dataset Explained Variance')\nplt.show(block=True)","9dc48eeb":"pca = PCA(n_components=42)\npca.fit(df_train.to_numpy())\ndf_train_pca = pca.transform(df_train)","d24d504c":"lda = LinearDiscriminantAnalysis()\nlda.fit(df_train.to_numpy(), target.to_numpy())\n# Plotting the Cumulative Summation of the Explained Variance\nplt.figure(figsize =(12,8))\nplt.plot(np.cumsum(lda.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')  # for each component\nplt.title('Segmentation Dataset Explained Variance')\nplt.show(block=True)","4f7cc950":"lda = LinearDiscriminantAnalysis(n_components=42)\nlda.fit(df_train.to_numpy(), target.to_numpy())\ndf_train_lda = lda.transform(df_train)","76ef0657":"def objective(trial):\n    rf_max_depth = trial.suggest_int('rf_max_depth', 10, 20)\n    rf_n_estimators = trial.suggest_int('n_estimators', 40, 150)\n    regressor_obj = RandomForestRegressor(max_depth=rf_max_depth, n_estimators = rf_n_estimators, n_jobs=-1)\n\n    X_train, X_val, y_train, y_val = train_test_split(df_train_lda, target, random_state=18)\n\n    regressor_obj.fit(X_train, y_train)\n    y_pred = regressor_obj.predict(X_val)\n\n    error = np.sqrt(mean_squared_error(y_val, y_pred))\n\n    return error  # An objective value linked with the Trial object.\n","9588c527":"# Trial 0 finished with value: 7.925259211426888 and parameters: {'rf_max_depth': 13, 'n_estimators': 97}. Best is trial 0 with value: 7.925259211426888.\nOPTUNA_OPTIMIZATION = True\nstudy = optuna.create_study(study_name = 'RFR',direction=\"minimize\")\nstudy.optimize(objective, n_trials=40)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","dc1399f2":"def objective2(trial):\n    rf_max_depth = trial.suggest_int('rf_max_depth', 2, 20)\n    rf_n_estimators = trial.suggest_int('n_estimators', 40, 200)\n    regressor_obj = ExtraTreesRegressor(max_depth=rf_max_depth, n_estimators = rf_n_estimators,n_jobs=-1)\n\n    X_train, X_val, y_train, y_val = train_test_split(df_train_lda, target, random_state=18)\n\n    regressor_obj.fit(X_train, y_train)\n    y_pred = regressor_obj.predict(X_val)\n\n    error = np.sqrt(mean_squared_error(y_val, y_pred))\n\n    return error  # An objective value linked with the Trial object.","30e3413f":"# Trial 0 finished with value: 7.924378637627466 and parameters: {'rf_max_depth': 17, 'n_estimators': 96}. Best is trial 0 with value: 7.924378637627466.\n# Trial 12 finished with value: 7.92502305558478 and parameters: {'rf_max_depth': 15, 'n_estimators': 99}. Best is trial 11 with value: 7.923335363078516.\n# Number of finished trials: 50\n# Best trial: score 7.915917139032413, params {'rf_max_depth': 10, 'n_estimators': 149}\nOPTUNA_OPTIMIZATION = True\nstudy = optuna.create_study(study_name = 'ERFR',direction=\"minimize\")\nstudy.optimize(objective2, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","c80415ac":"def objective3(trial):\n    sgd_loss = trial.suggest_categorical(\"loss\", [\"squared_loss\", \"huber\",\"epsilon_insensitive\",\"squared_epsilon_insensitive\"])\n    sgd_penalty = trial.suggest_categorical(\"penatly\", ['l1', 'l2', 'elasticnet'] )\n    sgd_alpha = trial.suggest_float('alpha', 0.0001, 1000)\n    sgd_learning_rate = trial.suggest_categorical(\"learning_rate\", ['constant', 'optimal', 'invscaling', 'adaptive'] )\n    sgd_eta0 = trial.suggest_int('eta0', 1, 100)\n    regressor_obj = SGDRegressor(loss=sgd_loss, penalty = sgd_penalty,alpha=sgd_alpha,learning_rate=sgd_learning_rate,eta0=sgd_eta0)\n\n    X_train, X_val, y_train, y_val = train_test_split(df_train_lda, target, random_state=18)\n\n    regressor_obj.fit(X_train, y_train)\n    y_pred = regressor_obj.predict(X_val)\n\n    error = np.sqrt(mean_squared_error(y_val, y_pred))\n\n    return error  # An objective value linked with the Trial object.","748d30a0":"OPTUNA_OPTIMIZATION = True\nstudy = optuna.create_study(study_name = 'SGD',direction=\"minimize\")\nstudy.optimize(objective3, n_trials=100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","77e07443":"def objective4(trial):\n    ridge_alpha = trial.suggest_float('alpha', 1, 10)\n\n    regressor_obj = Ridge(alpha=ridge_alpha, random_state=18)\n    \n\n    X_train, X_val, y_train, y_val = train_test_split(df_train_lda, target, random_state=18)\n    \n        \n    regressor_obj.fit(X_train, y_train)\n    y_pred = regressor_obj.predict(X_val)\n\n    error = np.sqrt(mean_squared_error(y_val, y_pred))\n\n    return error  # An objective value linked with the Trial object.","cd7a3960":"OPTUNA_OPTIMIZATION = True\nstudy = optuna.create_study(study_name = 'ridge',direction=\"minimize\")\nstudy.optimize(objective4, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","ff72614f":"def objective5(trial):\n    tr_alpha = trial.suggest_float('alpha', 1, 10)\n    tr_power = trial.suggest_float('power', 1, 2)\n    tr_max_iter = trial.suggest_int('max_iter', 100, 1000)\n    \n    regressor_obj = TweedieRegressor(power = tr_power, alpha = tr_alpha, max_iter = tr_max_iter)\n\n    X_train, X_val, y_train, y_val = train_test_split(df_train_lda, target, random_state=18)\n\n    regressor_obj.fit(X_train, y_train)\n    y_pred = regressor_obj.predict(X_val)\n\n    error = np.sqrt(mean_squared_error(y_val, y_pred))\n\n    return error  # An objective value linked with the Trial object.","21ea91f3":"OPTUNA_OPTIMIZATION = True\nstudy = optuna.create_study(study_name = 'TweedieR',direction=\"minimize\")\nstudy.optimize(objective5, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","912e9a91":"def objective6(trial):\n    pp_alpha = trial.suggest_float('alpha', 0.01, 10)\n    pp_max_iter = trial.suggest_int('max_iter', 100, 1000)\n    regressor_obj = PoissonRegressor(alpha = pp_alpha, max_iter = pp_max_iter)\n\n    X_train, X_val, y_train, y_val = train_test_split(df_train_lda, target, random_state=18)\n\n    regressor_obj.fit(X_train, y_train)\n    y_pred = regressor_obj.predict(X_val)\n\n    error = np.sqrt(mean_squared_error(y_val, y_pred))\n\n    return error  # An objective value linked with the Trial object.","77475950":"OPTUNA_OPTIMIZATION = True\nstudy = optuna.create_study(study_name = 'poiss\/gamma',direction=\"minimize\")\nstudy.optimize(objective6, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","cc84b5b2":"def objective7(trial):\n    net_alpha = trial.suggest_float('alpha', 1, 100)\n    net_ratio = trial.suggest_float('l1_ratio', 0, 1)\n    net_max_iter = trial.suggest_int('max_iter', 100, 1000)\n    regressor_obj = ElasticNet(alpha = net_alpha, l1_ratio = net_ratio, max_iter = net_max_iter, random_state=18)\n\n    X_train, X_val, y_train, y_val = train_test_split(df_train_lda, target, random_state=18)\n\n    regressor_obj.fit(X_train, y_train)\n    y_pred = regressor_obj.predict(X_val)\n\n    error = np.sqrt(mean_squared_error(y_val, y_pred))\n\n    return error  # An objective value linked with the Trial object.","7031b3f6":"OPTUNA_OPTIMIZATION = True\nstudy = optuna.create_study(study_name = 'Net',direction=\"minimize\")\nstudy.optimize(objective7, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","8453597d":"**SGDregressor**","4bb3b939":"**PCA is not working well here.**","a952754b":"**ElasticNet**","83a02bb0":"**RF very slow even after dimensionality reduction.\\\nPCA < LDA**","a77d68b3":"**Ridge**","9539a936":"**PoissonRegressor**","56ae445d":"**RondomForest**","47e4d5de":"**ExtraRondomForest**","0a50839e":"# author Dmitry Uarov https:\/\/www.kaggle.com\/dmitryuarov\/tps-aug-2021-eda-cb-vs-xgb-vs-lgbm \nfig = plt.figure(figsize = (20, 80))\nfig.suptitle('Train & Test', fontsize=16 , y =1)\nfor i in range(len(train.columns.tolist()[1:101])):\n    ax = plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[1:101][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(train[train.columns.tolist()[1:101][i]], color = '#16a5b8', shade = True, alpha = 0.5, linewidth = 0.3, edgecolor = 'black',ax=ax,label = 'train')\n    sns.kdeplot(test[test.columns.tolist()[1:101][i]], color = '#14a314', shade = True, alpha = 0.5, linewidth = 0.3, edgecolor = 'black',label = 'test',  ax = ax)\n    plt.ylabel('')\n    plt.xlabel('')\n    if i == 0:\n        fig.legend(prop={'size': 10})\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","e16de27b":"**TweedieRegressor**","6e5cf2f8":"## Target","ec514905":"# Exploratory data analysis","d3b1a16d":"**Scalers on data**","5dcb6476":"# Preprocessing"}}