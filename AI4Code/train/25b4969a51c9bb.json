{"cell_type":{"56db6a66":"code","155b69ce":"code","5fd76766":"code","8bd43636":"code","46dc9459":"code","7c5e12c1":"code","39f642b2":"markdown","2c1f12e0":"markdown"},"source":{"56db6a66":"## Read the data\nimport numpy as np #linear algebra\nimport pandas as pd #data processing\n\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns #data visualization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") #to ignore the warnings\n\n#for model building\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\ndataset=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndataset.head(2)","155b69ce":"#Dropping the unwanted Unnamed:32 column\ndataset=dataset.drop(columns={'Unnamed: 32'})\n\ndataset['diagnosis']=dataset['diagnosis'].replace({'M':1,'B':0})\n# Finding the correraltion between the features\ncorr = dataset.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(dataset.corr(), cmap='YlGnBu', annot = True)","5fd76766":"# We will now find out the features that have a higher correlation to diagnosis\ncorr[abs(corr['diagnosis']) > 0.59].index","8bd43636":"from sklearn.neighbors import LocalOutlierFactor\n\n# split the data to X and y before Local Outlier Factorization\n\ny=dataset[\"diagnosis\"]\nX=dataset.drop([\"diagnosis\"],axis=1)\ncolumns= dataset.columns.tolist()\nlof= LocalOutlierFactor()\ny_pred=lof.fit_predict(X)\ny_pred[0:30]\nx_score= lof.negative_outlier_factor_\noutlier_score= pd.DataFrame()\noutlier_score[\"score\"]=x_score\n\nlofthreshold= -2.5\nloffilter= outlier_score[\"score\"]< lofthreshold\noutlier_index= outlier_score[loffilter].index.tolist()\nX= X.drop(outlier_index)\ny= y.drop(outlier_index).values","46dc9459":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n\n# Dont fit the scaler while standardizate X_test !\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n\nkey = ['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier','XGBClassifier']\nvalue = [LogisticRegression(), KNeighborsClassifier(n_neighbors = 2, weights ='uniform'), SVC(kernel=\"rbf\",random_state=15), DecisionTreeClassifier(random_state=10), RandomForestClassifier(n_estimators=60, random_state=0), GradientBoostingClassifier(random_state=20), AdaBoostClassifier(), xgb.XGBClassifier(random_state=0,booster=\"dart\")]\nmodels = dict(zip(key,value))\nmodels","7c5e12c1":"predicted =[]\nfor name,algo in models.items():\n    model=algo\n    model.fit(X_train,y_train)\n    predict = model.predict(X_test)\n    acc = accuracy_score(y_test, predict)\n    predicted.append(acc)\n    print(name,acc)","39f642b2":"# The Local Outlier Factor\n\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors.","2c1f12e0":"As we noticed ADaBoost Classifier have the best accuracy, we will be going ahead with that"}}