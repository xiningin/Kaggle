{"cell_type":{"d7151235":"code","4acb27be":"code","2a88016c":"code","ecba3519":"code","c5778736":"code","6eb5f7df":"code","51d21a86":"code","d23e81bc":"code","8ff3764e":"code","76eae294":"code","4f4754a2":"code","0b8d2a16":"code","d3211ccc":"code","65a4a8d3":"code","2afe516d":"code","195ec2d0":"code","d3fbdfa2":"code","6f0721df":"code","033e15e5":"code","1f65f031":"code","3eabf772":"code","6a889369":"code","1d37b48f":"code","ebbe73a5":"markdown","ff8cd2d0":"markdown","20cdb4c4":"markdown","4966d238":"markdown","c47f7811":"markdown","d2e531a8":"markdown","db88f539":"markdown","8365f190":"markdown","82af38a7":"markdown"},"source":{"d7151235":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestRegressor\n","4acb27be":"train = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv', index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv', index_col=0)","2a88016c":"train.head()","ecba3519":"train.info()","c5778736":"test.info()","6eb5f7df":"# Select the numeric columns\nnumeric_features = [col for col in train.columns if 'cont' in col]\nnumeric_features","51d21a86":"# Select the categorical columns\ncat_features = [col for col in train.columns if 'cat' in col]\ncat_features","d23e81bc":"sns.histplot(train['target'], kde=False);\nplt.title('Target variable distribution', fontsize=18);\nplt.xlabel('Value', fontsize=16);\nplt.ylabel('Frequency', fontsize=16);","8ff3764e":"train.target.describe()","76eae294":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(train[numeric_features].corr(), cmap=\"YlGnBu\");","4f4754a2":"train[cat_features].describe(include='all')","0b8d2a16":"train[cat_features].head()","d3211ccc":"msno.bar(train[numeric_features])","65a4a8d3":"msno.bar(train[cat_features])","2afe516d":"features = train.drop(['target'], axis=1)\ny = train.target\n\nfeatures.head()","195ec2d0":"X = features.copy()\nX_test = test.copy()\n\nordinal_encoder = OrdinalEncoder()\nX[cat_features] = ordinal_encoder.fit_transform(features[cat_features])\nX_test[cat_features] = ordinal_encoder.transform(test[cat_features])\n\nX.head()","d3fbdfa2":"X.info()","6f0721df":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)","033e15e5":"rf = RandomForestRegressor(random_state=1)\nrf.fit(X_train, y_train)","1f65f031":"# Get score for predictions on the validation set\npredictions = rf.predict(X_val)\nrmse = mean_squared_error(y_val, predictions, squared=False)\nrmse","3eabf772":"test_preds = rf.predict(X_test)","6a889369":"submission = pd.DataFrame({'Id': X_test.index,\n                       'target': test_preds})","1d37b48f":"submission.to_csv('submission.csv', index=False)","ebbe73a5":"We need to check for missing values.","ff8cd2d0":"Since this a very large dataset and the training is taking too long, we'll take a small sample of the data. See this [post](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/discussion\/265752) for more information.","20cdb4c4":"## Training a random forest","4966d238":"Now, let's look at the correlation of target with the rest of numeric features.","c47f7811":"As we can see, the features are not really correlated and so we can select a non linear algorithm as base model. But let's explore the categorical variables as well.","d2e531a8":"## Preparing submission","db88f539":"## Preparing the training and test data\n\nWe enconde the categorical features.","8365f190":"# EDA\n\nFirst, we need to explore the distribution of the features.","82af38a7":"From the quick EDA we performed we identified the following transformations:\n\n* Encode the categorical features\n\nAlso, we'll be using a random forest as base line model and RMSE as metric."}}