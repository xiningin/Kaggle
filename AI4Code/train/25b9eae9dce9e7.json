{"cell_type":{"b7a84087":"code","8f2bebf8":"code","c845a529":"code","64657279":"code","62e95f68":"code","fc2e4f76":"code","3079434b":"code","783d4140":"code","e61c7a3b":"code","766ae1a7":"code","45672073":"code","98335b97":"code","e1ca94b2":"code","9124cfe1":"code","eb35461e":"code","a91466ff":"code","96573f58":"code","3d9740ee":"code","649c0f34":"code","0ba48957":"code","461d18ad":"code","43835cd1":"code","f5a43f04":"code","be27095f":"code","c394b567":"code","c8874cdc":"code","d5ebb0cd":"code","dbff8821":"code","5e9b8a0c":"code","1013dce5":"code","b8d69d71":"code","2f6376fe":"code","1dbeae82":"code","e9a186dc":"code","c0ea29fc":"code","75702e2b":"code","300a3280":"code","e1da8017":"code","b8b34b8a":"markdown","8fb9add1":"markdown","95d974b2":"markdown","aa2866c7":"markdown"},"source":{"b7a84087":"!nvidia-smi","8f2bebf8":"pip install gdown","c845a529":"# !git clone https:\/\/tranduchuy682@github.com\/tranduchuy682\/NICS_DATA.git\n#     Tranhuy682\n    ","64657279":"!pip install imutils","62e95f68":"import numpy as np\nimport pandas as pd\nfrom scipy.spatial import distance as dist\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport seaborn as sns\nfrom tqdm import tqdm \nfrom sklearn.utils import shuffle\n#from sklearn import decomposition\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\nimport tensorflow as tf\nimport keras\n#from keras.applications.vgg16 import VGG16 \nfrom keras.preprocessing import image\n#from keras.applications.vgg16 import preprocess_input\nfrom keras.models import Sequential, Model \n#from keras.initializers import he_normal\nfrom keras.layers import Lambda, SeparableConv2D, BatchNormalization, Dropout, MaxPooling2D, Input, Dense, Conv2D, Activation, Flatten \nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n#from keras.preprocessing.image import ImageDataGenerator\nimport imutils\nimport itertools","fc2e4f76":"!git clone https:\/\/github.com\/tranduchuy682\/NICS_DATA.git\n# !gdown https:\/\/drive.google.com\/u\/2\/uc?id=1-5dmQNnb0fqJ5qtzDGek665xSLCODv0w  #aff\n# !gdown https:\/\/drive.google.com\/u\/2\/uc?id=1-1T1TLEZbdV-OV78m1Nodx34As5w8I4I  #aff\n# !gdown https:\/\/drive.google.com\/u\/2\/uc?id=1OvFXh8V2uMSv0UzD-8c68db-Jgtctbhu  #BAC\n# !gdown https:\/\/drive.google.com\/u\/2\/uc?id=1xbzyWlaKplTFmzjyrKhtrO9bCCxRQlcO  #BAC\n# !gdown https:\/\/drive.google.com\/u\/2\/uc?id=1qSuHvOu996_mm2OntN0vZZOajrIQ6dGq  #VAE\n# !gdown https:\/\/drive.google.com\/u\/2\/uc?id=1muVqeiTUIIh0SWWPyoLSYjXOmha4_c56  #VAE\n# !gdown https:\/\/drive.google.com\/u\/2\/uc?id=1-2wPW42nutUVZnCQ0vF2h-yNz3Lckpxa  #BACaff\n# !gdown https:\/\/drive.google.com\/u\/2\/uc?id=1dGKKp-nLb1VkN3BiAeuJ-mXEc3-8qVGJ  #BACaff","3079434b":"folder_path=\".\/NICS_DATA\/data_NICS\" \nclass_names = ['1 H\u1ea7u h\u1ecdng', '2 Th\u1ef1c qu\u1ea3n', '3 T\u00e2m v\u1ecb', '4 Th\u00e2n v\u1ecb','5 Ph\u00ecnh v\u1ecb',\n               '6 Hang v\u1ecb','7 B\u1edd cong l\u1edbn','8 B\u1edd cong nh\u1ecf','9 H\u00e0nh t\u00e1 tr\u00e0ng','10 T\u00e1 tr\u00e0ng']\nfolder_names = ['Vung hau hong', 'Thuc quan', 'Tam vi', 'Than vi','Phinh vi',\n               'Hang vi','Bo cong lon','Bo cong nho','Hanh ta trang','Ta trang']\nnb_classes = len(class_names)","783d4140":"IMAGE_SIZE = 128\nEPOCHS = 50","e61c7a3b":"def loadimage(dataset):\n    images = []\n    labels = []\n    count =0\n    for i in range(nb_classes):\n        label = i\n        fold = f'{folder_path}\/{dataset}\/{class_names[i]}'\n        for file in tqdm(os.listdir(fold)):\n            img_path = os.path.join(fold, file)\n            image = cv2.imread(img_path, cv2.IMREAD_ANYCOLOR)\n            # image = cv2.cvtColor(image, cv2.COLOR_2RGB)\n            image = cv2.resize(image, (IMAGE_SIZE,IMAGE_SIZE))\n            images.append(image)\n            labels.append(label)\n\n    images = np.array(images, dtype = 'int32')\n    labels = np.array(labels, dtype = 'int32')\n    return images, labels\ndef load_augmented_image(dataset):\n    images = []\n    labels = []\n    count =0\n    for i in range(nb_classes):\n        label = i\n        fold = f'{dataset}\/{folder_names[i]}'\n        for file in tqdm(os.listdir(fold)):\n            img_path = os.path.join(fold, file)\n            image = cv2.imread(img_path, cv2.IMREAD_ANYCOLOR)\n            # image = cv2.cvtColor(image, cv2.COLOR_2RGB)\n            image = cv2.resize(image, (IMAGE_SIZE,IMAGE_SIZE))\n            images.append(image)\n            labels.append(label)\n\n    images = np.array(images, dtype = 'int32')\n    labels = np.array(labels, dtype = 'int32')\n    return images, labels","766ae1a7":"# BAC_images, BAC_labels = load_augmented_image('\/content\/drive\/MyDrive\/Lab co\u0302 Ha\u0309i\/augment_BAC')\n# aff_images, aff_labels = load_augmented_image('\/content\/drive\/MyDrive\/Lab co\u0302 Ha\u0309i\/augment_aff')\n# print(len(BAC_images))\n# print(len(aff_images))","45672073":"train_images, train_labels = loadimage('train')\ntest_images, test_labels = loadimage('test')\nprint(len(train_images))\nprint(len(test_images))","98335b97":"# import pickle\n\n# def _save_pkl(path, obj):\n#   with open(path, 'wb') as f:\n#     pickle.dump(obj, f)\n\n\n# # L\u01b0u l\u1ea1i c\u00e1c files\n# _save_pkl('\/content\/drive\/MyDrive\/Lab co\u0302 Ha\u0309i\/BAC_images', BAC_images)\n# _save_pkl('\/content\/drive\/MyDrive\/Lab co\u0302 Ha\u0309i\/BAC_labels', BAC_labels)\n# _save_pkl('\/content\/drive\/MyDrive\/Lab co\u0302 Ha\u0309i\/aff_images', aff_images)\n# _save_pkl('\/content\/drive\/MyDrive\/Lab co\u0302 Ha\u0309i\/aff_labels', aff_labels)","e1ca94b2":"import pickle\n\ndef _load_pkl(path):\n  with open(path, 'rb') as f:\n    obj = pickle.load(f)\n  return obj\n\n# # Load l\u1ea1i d\u1eef li\u1ec7u\n# train_images = _load_pkl('.\/VAE_images')\n# train_labels = _load_pkl('.\/VAE_labels')","9124cfe1":"# train_images, train_labels = shuffle(train_images, train_labels, random_state=100)\n\n# train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size = 0.2, stratify = labels)\ntrain_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size = 0.1, stratify = train_labels)","eb35461e":"print(\"Training images: {}\".format(train_images.shape))\nprint(\"Training labels: {}\".format(train_labels.shape))\nprint(\"Validation images: {}\".format(val_images.shape))\nprint(\"Validation labels: {}\".format(val_labels.shape))\nprint(\"Test images: {}\".format(test_images.shape))\nprint(\"Test labels: {}\".format(test_labels.shape))","a91466ff":"#hi\u1ec3n th\u1ecb train image  \nprint(len(train_images))\nindex = np.random.randint(train_images.shape[0])\nplt.figure()\nplt.imshow(train_images[index])\nplt.title('Image #{}: '.format(index) + class_names[train_labels[index]])\nplt.show()\n    ","96573f58":"# Ground truth\n# Hi\u1ec3n th\u1ecb \u1ea3nh t\u1eeb train data\nplt.subplot(121)\nindex = np.random.randint(train_images.shape[0])\nplt.imshow(train_images[index,:,:], cmap='gray')\nplt.title(\"Ground Truth : {}\".format(train_labels[index]))\n\n# Hi\u1ec3n th\u1ecb \u1ea3nh t\u1eeb test data\nplt.subplot(122)\nindex = np.random.randint(test_images.shape[0])\nplt.imshow(test_images[index,:,:], cmap='gray')\nplt.title(\"Ground Truth : {}\".format(test_labels[index]))\n\n","3d9740ee":"_, train_counts = np.unique(train_labels, return_counts = True)\n_, val_counts = np.unique(val_labels, return_counts = True)\n_, test_counts = np.unique(test_labels, return_counts = True)\n\npd.DataFrame({'train': train_counts, \"val\": val_counts, \"test\": test_counts}, index = class_names).plot.bar()\n\nplt.show()","649c0f34":"plt.pie(train_counts,\n        explode= np.zeros(nb_classes) , \n        labels=class_names,\n        autopct='%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of each observed category')\nplt.show()","0ba48957":"def display_random_image (class_names, images, labels):\n    print(len(images))\n    index = np.random.randint(images.shape[0])\n    plt.figure()\n    plt.imshow(images[index])\n    plt.title('Image #{}: '.format(index) + class_names[labels[index]])\n    plt.show()\n    \ndisplay_random_image (class_names, train_images, train_labels)","461d18ad":"def display_examples(class_names, images, labels):\n    fig = plt.figure(figsize = (10,10))\n    fig.suptitle(\"Examples of images in the dataset\", fontsize=16)\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(images[i], cmap=plt.cm.binary)\n        plt.xlabel(class_names[labels[i]])\n    plt.show()\n    \ndisplay_examples(class_names, train_images, train_labels)","43835cd1":"#Normalization\ntrain_images = train_images \/ 255.0 \nval_images = val_images \/ 255.0\ntest_images = test_images \/ 255.0","f5a43f04":"from tensorflow import keras\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam,SGD,RMSprop","be27095f":"base_model = MobileNetV2(include_top=False,weights=\"imagenet\",    \n                         input_shape=(IMAGE_SIZE,IMAGE_SIZE,3))","c394b567":"# s\u1ed1 layer\nprint(len(base_model.layers))","c8874cdc":"#140 96.22\n#100 96.89\n# for layer in base_model.layers[:100]:\n#     layer.trainable = False\nbase_model.summary()","d5ebb0cd":"model = keras.Sequential()\n\n# model.add(layers.Input(shape=(IMAGE_SIZE,IMAGE_SIZE,3)))\n# model.add(layers.Lambda(keras.applications.resnet50.preprocess_input))\nmodel.add(base_model)\nmodel.add(layers.GlobalAveragePooling2D())\nmodel.add(layers.Dense(nb_classes,activation=\"softmax\"))","dbff8821":"# for layer in model.layers:\n#     layer.trainable = False\nmodel.summary()","5e9b8a0c":"%%time\nmodel.compile(optimizer = \"adam\", \n               loss = 'sparse_categorical_crossentropy' , \n               metrics = ['accuracy'])\n\ncallback1 = ModelCheckpoint(filepath='best_model.hdf5', save_best_only=True, save_weights_only=False)\ncallback2 = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=3, verbose = 1, mode='min', restore_best_weights = True)\ncallback3 = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    patience = 2, \n    verbose = 1, \n    factor = 0.5, \n    min_lr = 0.000001)\ncallback4 = ModelCheckpoint(filepath='last_model.hdf5', save_best_only=False, save_weights_only=False)\n\n\nhistory = model.fit(\n    train_images, \n    train_labels, \n    batch_size = 32, \n    epochs = EPOCHS, \n    validation_data=(val_images, val_labels), \n    callbacks=[callback1, callback3, callback4]\n    )","1013dce5":"def plot_accuracy_loss_chart(history):\n    epochs = [i for i in range(EPOCHS)]\n    fig , ax = plt.subplots(1,2)\n    train_acc = history.history['accuracy']\n    train_loss = history.history['loss']\n    val_acc = history.history['val_accuracy']\n    val_loss = history.history['val_loss']\n    fig.set_size_inches(20,10)\n    ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n    ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n    ax[0].set_title('Training & Validation Accuracy')\n    ax[0].legend()\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"Accuracy\")\n\n    ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n    ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n    ax[1].set_title('Training & Validation Loss')\n    ax[1].legend()\n    ax[1].set_xlabel(\"Epochs\")\n    ax[1].set_ylabel(\"Training & Validation Loss\")\n    plt.show()\n","b8d69d71":"plot_accuracy_loss_chart(history)","2f6376fe":"%%time\nresults = model.evaluate(test_images, test_labels)\n\nprint(\"Loss of test model is  \", results[0])\nprint(\"Accuracy of test model is \", results[1]*100, \"%\")\n\n\n# results = model.evaluate(val_images, val_labels)\n\n# print(\"Loss of validation model is \", results[0])\n# print(\"Accuracy of validation model is \", results[1]*100, \"%\")\n\n# results = model.evaluate(train_images, train_labels)\n\n# print(\"Loss of train model  is \", results[0])\n# print(\"Accuracy of train model is \", results[1]*100, \"%\")","1dbeae82":"from sklearn.metrics import classification_report\n\npredictions = model.predict(test_images)\npredictions = np.argmax(predictions,axis=1)\npredictions[:15]","e9a186dc":"print(classification_report(\n    test_labels, \n    predictions, \n    target_names = class_names))","c0ea29fc":"cm = confusion_matrix(test_labels, predictions)\ndef calculate_sen_spec(cm):\n    size = len(cm)\n    TP, F, TN, TF, f1score, total = 0, 0, 0, 0, 0, 0\n    total += sum([sum([cm[i][j] for i in range(size)]) for j in range(size)])\n    TP += sum([cm[i][i] for i in range(size)])\n    F = total - TP\n    TN = (size-2)*total+TP\n    sen = TP\/(TP+F)\n    spec = TN\/(TN+F)\n    acc = (TP+TN)\/(size*total)\n    pre = TP\/(TP+F)\n    f1score = 2*TP\/(2*TP+2*F)\n\n    return sen, spec, acc, pre, f1score\ncalculate_sen_spec(cm)","75702e2b":"cm = pd.DataFrame(cm, index = class_names, columns = class_names)\ncm","300a3280":"def plot_confusion_matrix (cm):\n    plt.figure(figsize = (10,10))\n    sns.heatmap(\n        cm, \n        cmap = 'Blues', \n        linecolor = 'black', \n        linewidth = 1, \n        annot = True, \n        fmt = '', \n        xticklabels = class_names, \n        yticklabels = class_names)\n    \nplot_confusion_matrix(cm)","e1da8017":"%cd \/kaggle\/working\nfrom IPython.display import FileLink\nFileLink(r'.\/last_model.hdf5')\n\n","b8b34b8a":"# Libs\n","8fb9add1":"# MobileNet Model\n","95d974b2":"**Build Model** ","aa2866c7":"# Load Image\n"}}