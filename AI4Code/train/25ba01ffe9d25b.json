{"cell_type":{"0663ff2c":"code","8f853c81":"code","35b24392":"code","a7b82377":"code","63d06fb3":"code","28d1b6ec":"code","1a2f5ae5":"code","e401210d":"code","d5edcbc5":"markdown","7642ec83":"markdown","c9be5e26":"markdown","ca83aea6":"markdown","ac895bab":"markdown","c0dc5177":"markdown"},"source":{"0663ff2c":"import numpy as np\nimport pandas as pd\n\npapers = pd.read_csv(\"..\/input\/acl_papers.csv\", delimiter=\"\\t\")\npapers.head(5)","8f853c81":"from sklearn.feature_extraction.text import CountVectorizer\n\n\nyear_counts = {}\nall_papers = pd.concat([papers[\"title\"][papers[\"year\"] == y] for y in [2016, 2017, 2018]], ignore_index=True)\nvectorizer = CountVectorizer(stop_words=\"english\", max_df=0.8, min_df=7, ngram_range=(1, 2))\nvectorizer.fit(all_papers)\n\nwords = vectorizer.get_feature_names()\nfor y in [2016, 2017, 2018]:\n    vectors = vectorizer.transform(papers[\"title\"][papers[\"year\"] == y])\n    counts = vectors.toarray().sum(axis=0)\n    word_counts = {}\n    for i in range(len(words)):\n        word_counts[words[i]] = counts[i]\n    year_counts[y] = word_counts\n\nyear_count_df = pd.DataFrame(year_counts)\nyear_count_df.head(5)","35b24392":"def show_rate(year_freq_df, max_limit=0.5, min_limit=0):\n    num_columns = year_freq_df.shape[1]\n    total = np.repeat(year_freq_df.sum(axis=1).values.reshape(-1, 1), num_columns, axis=1)\n    year_freq_df_rate = (year_freq_df \/ total)\n    first_index = year_freq_df.columns[0]\n    year_freq_df_rate.sort_values(by=first_index, inplace=True)\n    limited = year_freq_df_rate[(year_freq_df_rate.max(axis=1) > max_limit) & (year_freq_df_rate.min(axis=1) > min_limit)]\n    limited.plot.barh(stacked=True, figsize=(8, 12))\n    return limited\n\nyear_count_rate_df = show_rate(year_count_df)","a7b82377":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\nyear_idfs = {}\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.8, min_df=7, ngram_range=(1, 2))\nvectorizer.fit(all_papers)\n\nwords = vectorizer.get_feature_names()\nfor y in [2016, 2017, 2018]:\n    vectors = vectorizer.transform(papers[\"title\"][papers[\"year\"] == y])\n    idfs = vectors.toarray().mean(axis=0)\n    word_idfs = {}\n    for i in range(len(words)):\n        word_idfs[words[i]] = idfs[i]\n    year_idfs[y] = word_idfs\n\nyear_idf_df = pd.DataFrame(year_idfs)\nyear_idf_df.head(5)","63d06fb3":"year_idf_rate_df = show_rate(year_idf_df)","28d1b6ec":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\nacl_2018s = papers[\"title\"][papers[\"year\"] == 2018] + papers[\"summary\"][papers[\"year\"] == 2018]\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.6, min_df=0.01, ngram_range=(1, 2))\nvectors = vectorizer.fit_transform(acl_2018s)\n\nwords = vectorizer.get_feature_names()\nacl2018_idfs = {}\nfor i in range(len(words)):\n    if not words[i].isdigit():\n        acl2018_idfs[words[i]] = vectorizer.idf_[i]\n\nacl2018_idfs= pd.Series(acl2018_idfs)\nacl2018_idfs.nlargest(10)","1a2f5ae5":"from sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score\n\n\nscores = []\nnum_clusters = range(2, 10)\nfor n_clusters in num_clusters:\n    labels = SpectralClustering(n_clusters=n_clusters).fit_predict(vectors)\n    score = silhouette_score(vectors, labels)\n    scores.append(score)\n\npd.Series(data=scores, index=list(num_clusters)).plot.bar()","e401210d":"from sklearn.decomposition import TruncatedSVD\n\n\nnum_cluster = 7\ntransformed = TruncatedSVD(n_components=2).fit_transform(vectors)\nlabels = SpectralClustering(n_clusters=num_cluster).fit_predict(transformed)\n\nax = None\ncolors = [\"salmon\", \"lightskyblue\", \"mediumaquamarine\", \"wheat\", \"gray\", \"violet\", \"darkblue\", \"lime\", \"cadetblue\"]\nfor cluster in range(num_cluster):\n    c = pd.DataFrame(transformed[labels == cluster], columns=[\"pc1\", \"pc2\"])\n    if ax is None:\n        ax = c.plot.scatter(x=\"pc1\", y=\"pc2\", color=colors[cluster], label=\"cluster:{}\".format(cluster))\n    else:\n        ax = c.plot.scatter(x=\"pc1\", y=\"pc2\", color=colors[cluster], label=\"cluster:{}\".format(cluster), ax=ax)","d5edcbc5":"### Calculate frequent words\n\nCount the frequent word by [CountVectorizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfVectorizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n","7642ec83":"There is no explicit cluster.","c9be5e26":"## ACL Accepted Papers Analysis\n\nLet's analyze its tendency of the research theme from the title of paper.\n\n1. Read papers dataset\n2. Calculate frequent words\n3. Analyze ACL 2018 abstracts\n\n### Read papers dataset","ca83aea6":"Both are similar result.\n\n\n### Analyze ACL 2018 abstracts\n\nWe can use abstracts of ACL 2018 papers (because these are published on official site).\nSo let's analyze it by above way.\n","ac895bab":"Evaluate by TF-IDF","c0dc5177":"#### Clustering"}}