{"cell_type":{"5be9affb":"code","f516f1e9":"code","fb80827a":"code","64706ddb":"code","9541d6c8":"markdown","434d629d":"markdown","05d7a2d4":"markdown","5cbded49":"markdown"},"source":{"5be9affb":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('..\/input\/homedataformlcourse\/train.csv', index_col='Id') \nX_test = pd.read_csv('..\/input\/homedataformlcourse\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\n# cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n# X.drop(cols_with_missing, axis=1, inplace=True)\n# X_test.drop(cols_with_missing, axis=1, inplace=True)","f516f1e9":"TrSF = list(X.columns[X.isnull().any()])\nTsSF = list(X_test.columns[X_test.isnull().any()])\nnTrS = len(TrSF)\nnTsS = len(TsSF)\n\nprint('There are', nTrS, 'features containing NaN values in the training set:\\n', list(X.columns[X.isnull().any()]))\nprint('\\nThere are', nTsS, 'features containing NaN values in the test set:\\n', list(X_test.columns[X_test.isnull().any()]))\n","fb80827a":"NaN_Features_Test = list(pd.Series(TsSF)[pd.Series([col in TrSF for col in TsSF]) == False])\nprint('The',len(NaN_Features_Test),'features with NaN only in the test set:\\n', NaN_Features_Test)","64706ddb":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Read the data\nX = pd.read_csv('..\/input\/homedataformlcourse\/train.csv', index_col='Id') \nX_test = pd.read_csv('..\/input\/homedataformlcourse\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values either in train\n# or test data\ncols_with_missing = [col for col in X.columns if (X[col].isnull().any() or X_test[col].isnull().any())] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Locate categorical columns\nobj_cols = [col for col in X.columns if X[col].dtypes == 'object']\n\n# Locate high cardinality features taking into account the train \n# and test data.\nhigh_card = [col for col in obj_cols if (X[col].nunique()>10 or X_test[col].nunique()>10)]\n\n# Find low cardinality features\nlow_card = list(set(obj_cols) - set(high_card))\n\n# Label encode the high cardinality features\nlenc = LabelEncoder()\nfor col in high_card:\n    lenc.fit(X[col].append(X_test[col]))\n    X[col] = lenc.transform(X[col])\n    X_test[col] = lenc.transform(X_test[col])\n\n# Seperate the low cardinality columns to one-hot encode them\nX_cat = X[low_card]\nX_test_cat = X_test[low_card]\n\n# One-hot encode. Fit the encoder to all the data.\nOHenc = OneHotEncoder(sparse = False)\nOHenc.fit(pd.concat([X_cat, X_test_cat]))\nX_cat = pd.DataFrame(OHenc.transform(X_cat))\nX_test_cat = pd.DataFrame(OHenc.transform(X_test_cat))\n    \n# Recover the indexes\nX_cat.index = X[low_card].index\nX_test_cat.index = X_test[low_card].index\n\n# Drop the categorical data and replace with one-hot encoded data\nX.drop(low_card, axis = 1, inplace = True)\nX_test.drop(low_card, axis = 1, inplace = True)\nX = pd.concat([X, X_cat], axis = 1)\nX_test = pd.concat([X_test, X_test_cat], axis = 1)\n\n# Fit a random forest regressor on the data\nrfr = RandomForestRegressor()\nrfr.fit(X, y)\n\n# Use the model on the test data\npred_test = rfr.predict(X_test)\n\n# Output the results\noutput = pd.DataFrame({'Id':X_test.index, \n                        'SalePrice':pred_test})\noutput.to_csv('submission.csv', index = False)","9541d6c8":"The problem in this dataset is the fact that there are some features with no NaN values in the training data that contain NaN values in the test data. This makes the test data incompatible with all the preparation previously done in the excerise, causing the random forest regressor to respond with an error. In particular, we can check the following report.","434d629d":"The code below shows a way to create a working submission and illustrates the deviations from the work and preparation done in the exercise.","05d7a2d4":"Since these features contain NaN values only in the test set, they are not dropped by the initialization code. As a result, the one-hot encoder and more importantly the random forest regressor created during the exercise cannot be applied to the test data in order to see their effect in the competition. In order to submit something to the competition, one has to discard all the previous results and start from the beginning with a new analysis (for example, by dropping all the columns that have NaN values either in the training or the test data, locating any high cardinality categorical features anew and maybe label encode them, one-hot encode the rest categorical features, re-train the regressor and apply it to the test data).\n\nI do not know if this was intended by the exercise's design, but I am under the impression that it is quite complicated as a closure to the exercise where one should expect (as it was done in the previous exercises) to be able to see the effects of the practised methods in the competition in a straightforward way.","5cbded49":"![](http:\/\/)In the above, we can see that the 15 following features have NaN values in the test set but not in the training set."}}