{"cell_type":{"7c4d84af":"code","f1d0e6bc":"code","7fab52be":"code","98655e89":"code","632f8ec0":"code","103a5760":"code","0bce2df5":"code","475e804d":"code","1da8641d":"code","96bf53db":"code","caced7d2":"code","ec290b3e":"code","6ceaf1cc":"code","854227bb":"code","732aa9df":"code","8296d758":"code","34fdebb0":"code","23720b4a":"code","af10637c":"code","7a915c02":"code","bf53a034":"code","2a2170b1":"code","e07cb647":"code","3f8fc394":"code","8b9f508d":"code","fde503c4":"code","e8ea53e3":"code","b042f0f8":"code","f772ad58":"code","11e5f371":"code","95ba8231":"code","accddc0c":"code","13283048":"code","ad21f78a":"code","9d611916":"code","49d50087":"code","59e8b90f":"code","83f0904a":"code","481d4c16":"code","3ceba179":"code","eb7cb628":"code","db8593c5":"code","b3efda8a":"code","a8c0c4aa":"code","338c50de":"code","3929a18f":"code","ed479e64":"code","5da68ca7":"code","4063d29e":"code","e0f67835":"code","465d41e1":"code","156ce72d":"code","4cee29d4":"code","ccd0c32c":"code","7f58893b":"code","2239119c":"code","fd3db52e":"code","a8c2e317":"code","63701b79":"code","a4df2771":"code","6a6feff6":"code","cbb7b499":"markdown","841b7035":"markdown","9a8dd71e":"markdown","51b7a45f":"markdown","19506119":"markdown","d35bdb8c":"markdown","91b0a112":"markdown","87f36b61":"markdown","b9e3107f":"markdown","88079b65":"markdown","a722afc7":"markdown","1c1c0b5d":"markdown","d66d5a88":"markdown","6199c01b":"markdown","0410c263":"markdown","d6bfa63d":"markdown","8e01f182":"markdown","85a19a35":"markdown","74772ec9":"markdown","e97935e2":"markdown","fc2b215a":"markdown","d1d49b06":"markdown","7c8339cf":"markdown","40171af2":"markdown","f8aa9ee3":"markdown","9de2bb0f":"markdown","d69c9b44":"markdown","5f05677d":"markdown","61ba6f61":"markdown","2bada034":"markdown","bf5990f3":"markdown","1d9ff4e2":"markdown","f50948d6":"markdown","57050cb8":"markdown","a4e32b73":"markdown","64776afa":"markdown","9538d12f":"markdown","d5248446":"markdown","8b97adf7":"markdown","bf6106cf":"markdown","bed09a38":"markdown","306f3a42":"markdown","587de525":"markdown","4ae459d8":"markdown","9ed41f07":"markdown"},"source":{"7c4d84af":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import preprocessing\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score # AUC ROC\nfrom sklearn.metrics import average_precision_score # AUC PRC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC","f1d0e6bc":"# Setting data path and filenames\ndata_path = '..\/input\/loan-prediction-based-on-customer-behavior\/'\nfilename_train = 'Training Data.csv'\nfilename_test = 'Test Data.csv'\nimg_out_path = '.\/' #ouput path\n\n# Loading data\ndf_train_org = pd.read_csv(data_path+filename_train)\ndf_test_org = pd.read_csv(data_path+filename_test)\n\n# Wrangled dataframe\ndf_train_wrg = df_train_org.copy()\ndf_test_wrg = df_test_org.copy()\n","7fab52be":"print('Train:', df_train_org.shape)\nprint('Test:', df_test_org.shape)","98655e89":"hist_risk = df_train_org['Risk_Flag'].value_counts(normalize=True) \nplt.bar(x=hist_risk.index, height=hist_risk)\nplt.xticks([0,1],['No Risk', 'Risk'])\nplt.ylabel('Percentage samples (%)')\nplt.title('Operations proportions')\n##plt.savefig(img_out_path+'hist_operations_proportions')","632f8ec0":"def check_train_test_data(df_train: pd.DataFrame, df_test:pd.DataFrame):\n    \"\"\"Print the difference between each dataset columns\"\"\"\n    col_names_train = set(df_train.columns)\n    col_names_test = set(df_test.columns)\n\n    diff_train_test = col_names_train.difference(col_names_test)\n    diff_test_train = col_names_test.difference(col_names_train)\n\n    msg_text = 'Columns on training dataset NOT available on test dataset:\\n'\n    print(msg_text, diff_train_test)\n\n    msg_text = '\\nColumns on test dataset NOT available on training dataset:\\n'\n    print(msg_text, diff_test_train)","103a5760":"check_train_test_data(df_train_org, df_test_org)","0bce2df5":"def check_dtypes(df: pd.DataFrame, n_samples=5):\n    \"\"\"\n    Print a summary of dataframe datatype and return object\n    and no-object colum datatype names\n    \"\"\"\n    df_dtypes = df.dtypes\n    \n    dtype_resume = df_dtypes.value_counts()\n    print('Data types summary:\\n', dtype_resume)\n\n    mask_obj = (df_dtypes == 'object')\n    obj_cols = df_dtypes[mask_obj]\n    non_obj_cols = df_dtypes[~mask_obj]\n    \n    display(df.sample(n_samples))\n    print('\\nPossible categorical columns (object):\\n', obj_cols)    \n    print('\\n\\nNumerical columns:\\n', non_obj_cols)\n    \n    return obj_cols, non_obj_cols","475e804d":"obj_cols, non_obj_cols = check_dtypes(df_train_org)","1da8641d":"def plot_multiple_histogram(df, cols_to_histogram):\n    for colname in cols_to_histogram:\n        hist_data = df[colname].value_counts(normalize=True)\n        plt.title('Histogram: ' + colname)\n        plt.barh(y=hist_data.index, width=hist_data*100)\n        plt.xlabel('Percentage (%)')\n        plt.tight_layout()\n        plt.show()","96bf53db":"cols_to_histogram = ['Married\/Single', 'House_Ownership', 'Car_Ownership', 'STATE']\nplot_multiple_histogram(df_train_org, cols_to_histogram)","caced7d2":"def plot_missing_values_sorted(df):\n    na_series = (df.isna().sum() \/ df.shape[0])\n    na_series = 100*na_series\n    na_series = na_series.sort_values(ascending=False)\n\n    if(na_series.sum() == 0):\n        print('No missing values in any columns')\n        \n    else:\n        plt.bar(x=np.arange(len(na_series)), height=na_series)\n\n        plt.ylabel('Percentage of missing values (%)')\n        plt.xlabel('Feature n\u00b0 sorted by missing values')\n        plt.tight_layout()\n        plt.show()","ec290b3e":"plot_missing_values_sorted(df_train_org)","6ceaf1cc":"single_transformer = preprocessing.LabelEncoder()\ncar_owner_transformer = preprocessing.LabelEncoder()\n\nsingle_transformer.fit(df_train_wrg['Married\/Single'])\ncar_owner_transformer.fit(df_train_wrg['Car_Ownership'])\n\nprint('Single classes:', single_transformer.classes_)\nprint('Has car classes:', car_owner_transformer.classes_)","854227bb":"df_train_wrg['Is_Single'] = single_transformer.transform(df_train_wrg['Married\/Single'])\ndf_train_wrg['Has_Car'] = car_owner_transformer.transform(df_train_wrg['Car_Ownership'])","732aa9df":"df_train_wrg.head()","8296d758":"cols_to_drop = ['Married\/Single', 'Car_Ownership', 'Id']\ndf_train_wrg = df_train_wrg.drop(columns=cols_to_drop)\n\ndf_train_wrg.head()","34fdebb0":"def basic_data_cleaning(df):\n    single_transformer = preprocessing.LabelEncoder()\n    car_owner_transformer = preprocessing.LabelEncoder()\n\n    single_transformer.fit(df['Married\/Single'])\n    car_owner_transformer.fit(df['Car_Ownership'])\n    \n    df['Is_Single'] = single_transformer.transform(df['Married\/Single'])\n    df['Has_Car'] = car_owner_transformer.transform(df['Car_Ownership'])\n    \n    cols_to_drop = ['Married\/Single', 'Car_Ownership']\n    df = df.drop(columns=cols_to_drop)\n    \n    return df","23720b4a":"df_train_wrg = df_train_org.copy()\ndf_test_wrg = df_test_org.copy()\n\ndf_train_wrg = basic_data_cleaning(df_train_wrg)\ndf_train_wrg = df_train_wrg.drop(columns=['Id'])","af10637c":"risk_pct = df_train_wrg['Risk_Flag'].value_counts(normalize=True)\nrisk_pct *= 100\n\nplt.title('Operation comparisons')\nplt.bar(x=risk_pct.index, height=risk_pct)\nplt.xticks([0, 1],['No risk', 'Risk operation'])\nplt.ylabel('Percentage of samples (%)')\nplt.show()","7a915c02":"def split_val_train_test(df, col_target, divison_pct=[0.1,0.2], rdn_seed=42):\n    \"\"\"\n    Split dataframe into X and Y and \n    then split into validation, train and test\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    \n    X = df.drop(columns=col_target).copy()\n    y = df[col_target].copy().values.ravel()\n    \n    # Validation dataset\n    if(divison_pct[0] > 0):\n        X_not_val, X_val, y_not_val, y_val = train_test_split(X, y,\n                                                              test_size=divison_pct[0],\n                                                              random_state=rdn_seed)\n    else:\n        X_not_val = X\n        y_not_val = y\n        X_val = []\n        y_val = []\n        \n    # Train and test dataset\n    X_train, X_test, y_train, y_test = train_test_split(X_not_val, y_not_val,\n                                                        test_size=divison_pct[1],\n                                                        random_state=rdn_seed)\n    \n    # Return splitted data as dictionary \n    splitted_data = [X_train, X_test, X_val, y_train, y_test, y_val]\n    key_names = ['X_train', 'X_test', 'X_val', 'y_train', 'y_test', 'y_val']\n    splitted_data_dict = dict(zip(key_names, splitted_data))\n    \n    return splitted_data_dict","bf53a034":"X_train, X_test, y_train, y_test = [],[],[],[]\ncol_target = ['Risk_Flag']\n\nsplitted_data_dict = split_val_train_test(df_train_wrg, col_target)\n\n# Assigning values\nX_train, y_train = splitted_data_dict['X_train'], splitted_data_dict['y_train']\nX_test, y_test = splitted_data_dict['X_test'], splitted_data_dict['y_test']\nX_val, y_val = splitted_data_dict['X_val'], splitted_data_dict['y_val']","2a2170b1":"\n# data prep\nval_prop = y_val.sum()\/len(y_val) * 100\ntest_prop = y_test.sum()\/len(y_test) * 100\ntrain_prop = y_train.sum()\/len(y_train) * 100\n\n# data concatenation\nlist_risk_prop = [train_prop, test_prop, val_prop]\nnames_risk_prop = ['Training', 'Test', 'Validation']\ndf_risk_prop_data_split = pd.DataFrame(list_risk_prop, index=names_risk_prop)\n\n# plot\nplt.title('Risk operations proportions by dataset')\nplt.bar(x=names_risk_prop, height=df_risk_prop_data_split[0])\nplt.ylabel('Data proportion (%)')\n#plt.savefig(img_out_path+'dataset_risk_prop')\nplt.show()\n\n","e07cb647":"def prepare_categorized_data(df, cols_to_categorize):\n    \"\"\" Categorize column data\n    \"\"\"\n    \n    df_categorized = df.copy()\n    \n    for colname in cols_to_categorize:  \n        le = preprocessing.LabelEncoder()\n        le.fit(df[colname])\n        df_categorized.loc[:, colname] = le.transform(df[colname].copy())\n\n    return df_categorized","3f8fc394":"# Apply categorization\ncols_to_categorize = ['House_Ownership', 'Profession', 'CITY', 'STATE']\ndf_categorized_train = prepare_categorized_data(df_train_wrg, cols_to_categorize)\n\n# Splitting data\ncol_target = ['Risk_Flag']\nsplitted_data_dict = split_val_train_test(df_categorized_train, col_target)\n\n# Assigning values\nX_train, X_test, y_train, y_test = [],[],[],[]\nX_train, y_train = splitted_data_dict['X_train'], splitted_data_dict['y_train']\nX_test, y_test = splitted_data_dict['X_test'], splitted_data_dict['y_test']\nX_val, y_val = splitted_data_dict['X_val'], splitted_data_dict['y_val']\n\nX_train.head()","8b9f508d":"clf_logst_base_1 = LogisticRegression(random_state=0,\n                               class_weight='balanced',\n                               max_iter=500)\n\nclf_logst_base_1.fit(X_train, y_train)","fde503c4":"def plot_clf_metrics(clf, model_name, X_test, y_true, class_pred=1):\n    y_pred = clf.predict_proba(X_test)[:, class_pred]\n\n    roc_auc = round(roc_auc_score(y_true, y_pred), 2)\n    prc_auc = round(average_precision_score(y_true, y_pred), 2)\n\n    print('AUC ROC = {}\\nAUC PRC = {}'.format(roc_auc, prc_auc))\n\n    # ROC plot\n    metrics.plot_roc_curve(clf, X_test, y_true)\n    plt.title('ROC Curve - ' + model_name)\n    plt.tight_layout()\n    #plt.savefig(img_out_path+'roc_auc_+'+model_name)\n    plt.show()           \n\n\n    # PRC plot\n    disp = metrics.plot_precision_recall_curve(clf, X_test, y_true)\n    disp.ax_.set_title('2-class Precision-Recall curve: '\n                       'AP={0:0.2f}'.format(prc_auc))\n    plt.tight_layout()\n    #plt.savefig(img_out_path+'prc_auc_+'+model_name)\n    plt.show()\n\n\n    # Confusion Matrix plot\n    plot_confusion_matrix(clf, X_test, y_true) \n    plt.tight_layout()\n    #plt.savefig(img_out_path+'confusion_mtx_+'+model_name)\n    plt.show()","e8ea53e3":"plot_clf_metrics(clf_logst_base_1, '#1 Baseline encoding', X_test, y_test)","b042f0f8":"df_train_wrg_one_hot = pd.get_dummies(df_train_wrg)\nprint('new dataframe shape:', df_train_wrg_one_hot.shape)\n\ncol_target = ['Risk_Flag'] \nsplitted_data_dict = split_val_train_test(df_train_wrg_one_hot, col_target)\n\n# Assigning values\nX_train, X_test, y_train, y_test = [],[],[],[]\nX_train, y_train = splitted_data_dict['X_train'], splitted_data_dict['y_train']\nX_test, y_test = splitted_data_dict['X_test'], splitted_data_dict['y_test']\nX_val, y_val = splitted_data_dict['X_val'], splitted_data_dict['y_val']\n\nX_train.head()","f772ad58":"clf_logst_base_2 = LogisticRegression(random_state=0,\n                               class_weight='balanced',\n                               max_iter=500)\n\nclf_logst_base_2.fit(X_train, y_train)","11e5f371":"plot_clf_metrics(clf_logst_base_2, '#2 Baseline one-hot encoding', X_test, y_test)","95ba8231":"# Apply categorization\ncol_target = ['Risk_Flag']\ndf_categorized_train = prepare_categorized_data(df_train_wrg, cols_to_categorize)\n\n# Splitting data\ncols_to_categorize = ['House_Ownership', 'Profession', 'CITY', 'STATE']\nsplitted_data_dict = split_val_train_test(df_categorized_train, col_target)\n\n# Assigning values\nX_train, X_test, y_train, y_test = [],[],[],[]\nX_train, y_train = splitted_data_dict['X_train'], splitted_data_dict['y_train']\nX_test, y_test = splitted_data_dict['X_test'], splitted_data_dict['y_test']\nX_val, y_val = splitted_data_dict['X_val'], splitted_data_dict['y_val']\n\nX_train.head()","accddc0c":"from sklearn.tree import DecisionTreeClassifier\n\nclf_tree_base_3 = DecisionTreeClassifier(random_state=0,\n                               class_weight='balanced')\n\nclf_tree_base_3.fit(X_train, y_train)","13283048":"plot_clf_metrics(clf_tree_base_3, '#3 Baseline encoding Decision Tree',\n                 X_test, y_test)","ad21f78a":"# Applyng basic data cleaning\ndf_train_wrg = df_train_org.copy()\ndf_test_wrg = df_test_org.copy()\n\ndf_train_wrg = basic_data_cleaning(df_train_wrg)\ndf_train_wrg = df_train_wrg.drop(columns=['Id'])\n\n# Encoding category columns\ndf_train_wrg_encoded = df_train_wrg.copy()\ncols_to_categorize = ['House_Ownership', 'Profession', 'CITY', 'STATE']\n\nfor colname in cols_to_categorize:  \n    le = preprocessing.LabelEncoder()\n    le.fit(df_train_wrg_encoded[colname])\n    df_train_wrg_encoded.loc[:, colname] = le.transform(df_train_wrg_encoded[colname].copy())","9d611916":"def plot_corr_mtx_pandas(df, method='spearman'):\n    corr_mtx = df.corr(method=method)\n    return corr_mtx.style.background_gradient(cmap='coolwarm')","49d50087":"corr_mtx = plot_corr_mtx_pandas(df_train_wrg_encoded)\ncorr_mtx","59e8b90f":"# ---- Saving figure for medium text\n#plt.subplots(figsize=(8,8))\n#plt.title('Correlation between features')\n#sns.heatmap(df_train_wrg_encoded.corr(method='spearman').round(2),annot=True)\n#plt.tight_layout()\n##plt.savefig(img_out_path+'correlation_plot')","83f0904a":"cols_demography = ['Age', 'Income', 'CITY', 'STATE', 'Risk_Flag']\ndf_demography = df_train_org[cols_demography].copy()\n\ndf_demography_NO_risk = df_demography.query(\"Risk_Flag == 0\").copy()\ndf_demography_risk  = df_demography.query(\"Risk_Flag == 1\").copy()","481d4c16":"def plot_series_df(df, column):\n    pd.DataFrame(df[column]).boxplot()","3ceba179":"plt.suptitle('Boxplot of variables')\n\nplt.subplot(121)\nplot_series_df(df_demography, 'Income')\n\nplt.subplot(122)\nplot_series_df(df_demography, 'Age')","eb7cb628":"df_demography.describe().round(2)","db8593c5":"# Aggregating data \nagg_demography_col = ['Age', 'Income']\n\nagg_df_demography = df_demography[['Age', 'Income', 'Risk_Flag']] \\\n                            .groupby(['Age', 'Risk_Flag']) \\\n                            .mean() \\\n                            .sort_values(by='Age') \\\n                            .reset_index()\n\nagg_df_demography['Risk_Flag'].replace({0: \"No Risk\",\n                                        1: \"Risk\"\n                                       }, inplace=True)","b3efda8a":"mean_general = df_demography_risk['Income'].mean()\nmean_array = mean_general * np.ones(agg_df_demography['Age'].nunique())\n\npalette ={\"No Risk\": \"C9\", \"Risk\": \"C7\"}\n\nplt.subplots(figsize=(15,5))\nsns.barplot(data=agg_df_demography, x='Age', y='Income', hue='Risk_Flag',\n           palette=palette)\nplt.plot(mean_array, label='General average', color='red')\nplt.title('Average Income by Age')\nplt.legend(loc='best')\n#plt.savefig(img_out_path+'avg_income_age')\nplt.show()","a8c0c4aa":"plt.subplots(figsize=(15,5))\n\nsub_set_low_age = agg_df_demography.query(\"Age <= 35\")\nsns.barplot(data=sub_set_low_age, x='Age', y='Income', hue='Risk_Flag',\n           palette=palette)\nplt.plot(mean_array[0:sub_set_low_age['Age'].nunique()],\n        label='Average Income', color='red')\n\nplt.title('Average Income by Age: Below 36 years')\nplt.legend(loc='best')\n#plt.savefig(img_out_path+'avg_income_age_below_36')\nplt.show()","338c50de":"plt.subplots(figsize=(15,5))\n\nsub_set_older_age = agg_df_demography.query(\"Age >= 50\")\n\nsns.barplot(data=sub_set_older_age, x='Age', y='Income',hue='Risk_Flag', \n            palette=palette)\n\nplt.plot(mean_array[0:sub_set_older_age['Age'].nunique()],\n        label='Average Income', color='red')\n\nplt.title('Average Income by Age: Above 50 years')\nplt.legend(loc='best')\n#plt.savefig(img_out_path+'avg_income_age_above_50')\nplt.show()","3929a18f":"cols_geography = ['CITY', 'STATE', 'Risk_Flag']\nagg_by_state = df_demography[cols_geography].groupby(['STATE', 'Risk_Flag']) \\\n                                            .count() \\\n                                            .reset_index() \\\n                                            .rename(columns={\n                                                        'CITY': '#People'\n                                                            })\n\nagg_by_state['Percentage_Total'] = (agg_by_state['#People'] \\\n                                \/ agg_by_state['#People'].sum())*100\n\nagg_by_state['Risk_Flag'] = agg_by_state['Risk_Flag'].replace({0:'No Risk',\n                                                               1:'Risk'})","ed479e64":"cols_geography = ['STATE', 'Risk_Flag', 'Income']\n\nagg_by_state_income = df_demography[cols_geography].groupby(['STATE', 'Risk_Flag']) \\\n                                            .mean() \\\n                                            .reset_index() \\\n                                            .rename(columns={\n                                                        'Income': 'Avg Income'\n                                                            })\n\nagg_by_state['Avg Income'] = agg_by_state_income['Avg Income']","5da68ca7":"plt.subplots(figsize=(10,12))\n\nagg_by_state = agg_by_state.sort_values(by=['Risk_Flag', 'Avg Income'], ascending=False)\n\nsns.barplot(data=agg_by_state, y='STATE',\n            x='Avg Income', hue='Risk_Flag', palette=palette)\n\n#risk_mean = agg_by_state_only_risk['Percentage_Risk'].mean()\n\n#plt.bar(x=risk_mean, height = len(agg_by_state_only_risk), width=0.1,\n#        label='Average Risk by state', color='red')\n\nplt.title('Average income by state and risk')\nplt.legend(loc='best')\nplt.tight_layout()\n#plt.savefig(img_out_path+'avg_income_state')\nplt.show()","4063d29e":"# Count number of clients in each state\nstate_count = df_demography['STATE'].value_counts().sort_index()\n\n# Aggregate data to get proportion of risk operations in each state\nagg_by_state_only_risk = agg_by_state.query(\"Risk_Flag == 'Risk'\").copy()\nagg_by_state_only_risk = agg_by_state_only_risk.sort_values(by=['STATE'])\n\nagg_by_state_only_risk['#People_State'] = state_count.values\n\nagg_by_state_only_risk['Percentage_Risk'] = (agg_by_state_only_risk['#People'].values \\\n                                            \/ state_count.values) * 100\n                \nagg_by_state_only_risk = agg_by_state_only_risk.sort_values(\n                                                by=['Percentage_Risk'],\n                                                ascending=False)","e0f67835":"agg_by_state_only_risk.head()","465d41e1":"plt.subplots(figsize=(10,8))\n\nsns.barplot(data=agg_by_state_only_risk, y='STATE',\n            x='Percentage_Risk', hue='Risk_Flag',palette=palette)\n\nrisk_mean = agg_by_state_only_risk['Percentage_Risk'].mean()\n\nplt.bar(x=risk_mean, height = len(agg_by_state_only_risk), width=0.1,\n        label='Average Risk by state', color='red')\n\nplt.title('Proportion of risk operations by state')\nplt.legend(loc='best')\nplt.tight_layout()\n#plt.savefig(img_out_path+'risk_state')\nplt.show()","156ce72d":"## Basic data cleaning\ndf_train_wrg = df_train_org.copy()\ndf_train_wrg = basic_data_cleaning(df_train_wrg)\ndf_train_wrg = df_train_wrg.drop(columns=['Id'])\n\n# Apply categorization\ncol_target = ['Risk_Flag']\ndf_categorized_train = prepare_categorized_data(df_train_wrg, cols_to_categorize)\n\n# Splitting data\ncols_to_categorize = ['House_Ownership', 'Profession', 'CITY', 'STATE']\nsplitted_data_dict = split_val_train_test(df_categorized_train, col_target)\n\n# Assigning values\nX_train, X_test, y_train, y_test = [],[],[],[]\nX_train, y_train = splitted_data_dict['X_train'], splitted_data_dict['y_train']\nX_test, y_test = splitted_data_dict['X_test'], splitted_data_dict['y_test']\nX_val, y_val = splitted_data_dict['X_val'], splitted_data_dict['y_val']","4cee29d4":"# Classifier\nclf_rdn_forest_1 = RandomForestClassifier(class_weight='balanced',\n                                          n_estimators=300,\n                                          random_state=0,\n                                          verbose=1)\n\n# Training\nclf_rdn_forest_1.fit(X_train, y_train)\n\n# Predict probability\ny_pred_rdn_forest_1 = clf_rdn_forest_1.predict_proba(X_test)[:, 1]","ccd0c32c":"# Classifier\nclf_svm_1 = make_pipeline(StandardScaler(), SVC(gamma='auto',\n                                                class_weight='balanced',\n                                                probability=True,\n                                                verbose=1,\n                                                max_iter=200))\n# Training\nclf_svm_1.fit(X_train, y_train)\n\n# Predict probability\ny_pred_svm_1 = clf_svm_1.predict_proba(X_test)[:, 1]","7f58893b":"def plot_roc_curve_multiple_classifiers(X_test, y_true, clf_list,\n                                        clf_names, class_num=1):\n    for i, clf in enumerate(clf_list):\n        pred = clf.predict_proba(X_test)[:, class_num]\n        fpr, tpr, thresh = metrics.roc_curve(y_true, pred)\n        auc = metrics.roc_auc_score(y_true, pred)\n        label_plot = clf_names[i] + ' auc=' +str(auc.round(2))\n        plt.plot(fpr,tpr,label=label_plot)\n        plt.legend(loc='best')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        #plt.savefig(img_out_path+'roc_curve_comparison')","2239119c":"def plot_precision_recall_curve_multiple_classifiers(X_test, y_true,\n                                                     clf_list,\n                                                     clf_names,\n                                                     class_num=1):\n    for i, clf in enumerate(clf_list):\n        pred = clf.predict_proba(X_test)[:, class_num]\n        precision, recall, thresholds = metrics.precision_recall_curve(y_true, pred)\n        auc = metrics.average_precision_score(y_true, pred)\n        \n        label_plot = clf_names[i] + ' auc=' +str(auc.round(2))\n        plt.plot(recall,precision,label=label_plot)\n        plt.legend(loc='best')\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        #plt.savefig(img_out_path+'prc_curve_comparison')","fd3db52e":"def plot_multiple_model_evaluation(X_test, y_true,\n                                   clf_list, clf_names, class_num=1):\n\n    plt.subplots(figsize=(15, 4))\n\n    plt.subplot(121)\n    plt.title('ROC Curve')\n    plot_roc_curve_multiple_classifiers(X_test, y_test, clf_list, clf_names)\n\n    plt.subplot(122)\n    plt.title('Precision Recall Curve')\n    plot_precision_recall_curve_multiple_classifiers(X_test, y_test, clf_list, clf_names)\n    ","a8c2e317":"# Saving fig\nclf_names = ['Decision Tree', 'Random Forest', 'SVM']\nclf_list = [clf_tree_base_3, clf_rdn_forest_1, clf_svm_1]\nplt.title('ROC Curve')\nplot_roc_curve_multiple_classifiers(X_test, y_test, clf_list, clf_names)\nplt.show()\nplt.title('Precision Recall Curve')\nplot_precision_recall_curve_multiple_classifiers(X_test, y_test, clf_list, clf_names)","63701b79":"clf_names = ['Decision Tree', 'Random Forest', 'SVM']\nclf_list = [clf_tree_base_3, clf_rdn_forest_1, clf_svm_1]\nplot_multiple_model_evaluation(X_test, y_test, clf_list, clf_names)\n#plt.savefig(img_out_path+'model_curves')","a4df2771":"threshold_prediction = 0.85\n\n# Predicting validation data\ny_pred_val = clf_rdn_forest_1.predict_proba(X_val)[:, 1]\nval_classif = (y_pred_val >= threshold_prediction).astype(int)","6a6feff6":"tn, fp, fn, tp = metrics.confusion_matrix(y_val, val_classif).ravel()\nval_conf_mtx = pd.DataFrame([[tp, fp], [fn, tn]],\n                            index=['Pred: Risk', 'Pre: NO Risk'],\n                            columns=['True: Risk','True: No Risk'])\n\nval_conf_mtx","cbb7b499":"**Logistic regression algorithm**","841b7035":"**Prepare data for training**","9a8dd71e":"--- \n# Creating a baseline model\n\n1. Sanity check on data;\n2. Basic data cleaning;\n3. Modeling the problem.\n\n\nAt the end of this section, our baseline model achieved those metrics with a decision tree algorithm:\n- AUC ROC -- 0.84\n- AUC PRC -- 0.4","51b7a45f":"\n**_All ages - Average income by age and group_**\n","19506119":"**_Above 50 years - Average income by age and group_**","d35bdb8c":"#### **Does younger people who have low incame are more risk?** \n\n**Answer:** No. In some ages people who have **more income** then the samples average are more risk. Some groups are:\n\nYounger people:\n- 21 - 22 years;\n- 27 - 28 years;\n- 31 years;\n\nOlder people:\n- 51 and 55 years;\n- 60 - 63 years;\n- 74 - 79 years;\n\nOther conclusions:\n- The average income doens't increase when you get older. Maybe there are more groups that can be determined by region or profession.\n- People very young (below 23) and people older (more than 50 years) may be a risk group.\n----\n**Getting the results**\n\nBefore we start the analysis let's plot a boxplot of the variables for check if there'are outliers in our data:\n\n- Age: no outliers\n- Income: no outliers","91b0a112":"#### #3 - Decision tree categorize encoding (AUC = 0.84) \n\nSince our first two baseline models weren't good in terms of ROC AUC and PRC AUC, let's try a different algorithm: a simple decision tree with categorize encoding.\n\nLet's categorize the columns: \n\n- House_Ownership    \n- Profession         \n- CITY               \n- STATE              \n","87f36b61":"**Basic data cleaning pipeline**","b9e3107f":"####  #2 - Logistic Regression with one-hot encoding (AUC = 0.5)\n\nLet's apply one-hot enconding at the columns: \n\n- House_Ownership    \n- Profession         \n- CITY               \n- STATE              \n\nAfter evaluating the baseline model we'll test differents modeling strategies","88079b65":"---\n### Demography of clients\n\n1. Does younger people who have low incame are more risk?\n\n**Answer:** No. In some ages people who have **more income** then the samples average are more risk.\n\n\n2. Is there any state or city with more loans when we analyse the percentual of risk\/population?\n\n\n**Answer:** Yes. There's a risk difference among states. Besides, it is possible to categorize states by default customers with income, low income, and income doesn't make a difference.","a722afc7":"**Class balance**: unbalanced dataset\n- Risk operations - 12.3%\n- No risk - 87.7%\n\nBalancing techniques are required like:\n- Balance during training;\n- Apply undersample techniques like SMOTE.","1c1c0b5d":"### **What is the percentage of missing values?** \n\nNone.","d66d5a88":"**Evaluating model**\n\n1. AUC ROC -- 0.5\n2. AUC PRC -- 0.13\n3. Confusion matrix: the model didn't detect any loan default\n\nOur baseline model behaves like a dummy classifier","6199c01b":"---\n## Next steps for our solution\n\nTo improve our solution, we could:\n\n**Optimize our current model**\n- Find new hyperparameters that improve the roc AUC metric;\n\n**Explore our data and have new insights**\n- Explore the other bucket's of our problem: patrimony and professions;\n- Apply clustering techniques and find the different types of risk operations.\n\n**Create a new model**\n- Use the findings in the last topic to create new features for the model and try different algorithms comparing them with the current solution.","0410c263":"**Evaluating model**\n\n1. AUC ROC -- 0.84\n2. AUC PRC -- 0.4\n\nThis the best model so far!","d6bfa63d":"---\n#### Is there any state or city with more loans when analyzing the percentual of risk\/population?\n\n\n**Answer:** Yes. There's a risk difference among states. Besides, it is possible to categorize states by default customers with income, low income, and income doesn't make a difference.\n\nOther conclusions:\n1. States with the **risk associated with high income**: Manipur to Jharkhand, including Puducherry e Himachal_Pradesh;\n2. States with the **risk associated with low income**: states like Uttarakhand, Punjab, Chhattisgarh, etc.\n3. States were the **risk is not associated with income**: West_Bangal, Tripura, entre outros.\n\n----\n**Getting the results**\n\n1. Average income by state;\n2. Average risk by state;","8e01f182":"### Evaluating the Random Forest model on Validation data\n\nAnalysing the result in the validation dataset with a prediction threshold of 85%\n\n","85a19a35":"### SVM","74772ec9":"**Decision tree regression algorithm**","e97935e2":"---\n# Improving our solution\n\n1. Train two new models and compare with our baseline;\n2. Select the best model;\n3. Next steps for our solution;","fc2b215a":"---\n# Understanding our data\n\n1. Apply a correlation matrix between target and input variables;\n2. Apply exploratory data analysis (EDA) driven by hypothesis.","d1d49b06":"---\n## Modeling the problem\n1. Evaluation metric\n2. Prepare data for training\n3. Create baseline model","7c8339cf":"**2. Droping columns**\n   - ID: will not be used for predictions    \n   - Married\/Single\n   - Car_Ownership","40171af2":"### Baseline model\n\nA baseline model is the simplest solution possible for a machine learning problem. We'll do the following steps:\n\n1. Prepare data:\n    - Using categorize encoding;\n    - Using one-hot encoding;\n2. Apply the logistic regression algorithm;\n3. Evaluate the model using ROC AUC, PRC AUC and plot the confusion matrix.\n\n\n\n\n#### #1 - Logistic Regression with categorize encoding (AUC = 0.5)\n\nFor this problem we'll use the logistic regression algorithm considering the difference between classes. Since this algorithm doesn't allow text data let's categorize the columns: \n\n- House_Ownership    \n- Profession         \n- CITY               \n- STATE              \n\nAfter evaluating the baseline model we'll test differents modeling strategies","f8aa9ee3":"---\n## Applying a correlation matrix between target and input variables \n\n- No relevant correlations between target variable and features;\n- High correlation between experience and current job years: makes sense because the longer you're in a position more experience you're expected to have;\n- We'll need to analyze visually the relationship between the features and risk ","9de2bb0f":"**Split between validation, train and test**\n\n- 10% validation: with 12.4% risk (seed=42)\n- 20% test: with 12.3% risk       (seed=42)\n- 70% training: with 12.2% risk   (seed=42)\n\nValidation data will be used for our final predicitons since we don't have the answer for the 'test' data.","d69c9b44":"## Select the best model\n\n- Best model: Random forest (ROC AUC =  0.94 and PR AUC = 0.59)\n- Sugested threshold:\n\n### Evaluating models","5f05677d":"### Evaluation metric\n\nFor this problem:\n\n- True Positive (TP): our model predicted a high probability of loan default, and it would be a default;\n- True Negative (TN): our model predicted that the operation has a low risk and it had a low risk;\n- False Positive (FP): our model predicted RISK, and it hasn't happened a loan default;\n- False Negative (FN): our model predicted NO RISK, and the operation was RISK <--- **Worst case scenario**.\n\nSince we want to avoid the False Negatives and maximize the True Positive and True Negative, we'll use two main metrics to compare models:\n\n- AUC ROC: area under curve ROC \n- AUC PRC: area under the precision-recall curve\n\nWith these metrics, we can compare different classifications thresholds. Along the way, we'll also use the confusion matrix to analyse the results of the prediction with a standard threshold of 0.5","61ba6f61":"**Evaluating model**\n\n1. AUC ROC -- 0.5\n2. AUC PRC -- 0.13\n3. Confusion matrix: the model didn't detect any loan default\n\nOur baseline model behaves like a dummy classifier","2bada034":"## Sanity check on data\n\n1. There are different data at training and test datasets?  \n2. What are the data types on the training dataset?\n3. What is the percentage of missing values?","bf5990f3":"## Train two new models and compare with our baseline\n\nWe'll compare this algorithms:\n\n1. Random Forest;\n2. SVM;\n3. Decision Tree (baseline).","1d9ff4e2":"**Operations proportions between classes**\n- Risk: 12.3%\n- No-Risk: 87.7%","f50948d6":"**1. Casting columns to numerical**\n\n    - Married\/Single --> Is_Single\n    - Car_Ownership --> Has_Car","57050cb8":"### Prepare data for training\n\n1. Apply a basic data cleaning pipeline;\n2. Check class balance;\n3. Split data between train and test.","a4e32b73":"---\n**Loading libraries and data**","64776afa":"**Plotting**\n1. All ages - Average income by age and group;\n2. Below 36 years - Average income by age and group;\n3. Above 50 years - Average income by age and group;","9538d12f":"**_Below 36 years - Average income by age and group_**","d5248446":"### Random Forest","8b97adf7":"### **There are different data at training and test datasets?**  \n\nNot in the input data, only at the target variable.\n\n- There's no difference in data that may be used as features for the model;\n- The Id (training) and ID (test) columns have different names;\n- We don't have the answer (target variable) for the test dataset.","bf6106cf":"### Dealing with columns properties: dropping, change names or data types\n\nThere are no missing values in this dataset","bed09a38":"### **What are the data types on the training datasets?**\n\n- Numeric and categorical\n- Categorical data that can be cast to numerical due to a low number of categories:\n    - Married\/Single: married or single;\n    - Car_Ownership: yes, no;\n    - House ownership: rented, owned, norent_nown.\n","306f3a42":"\n---\n## Basic data cleaning\n\n1. Dealing with columns properties: drop, change datatype or name;\n2. Dealing with missing values.\n\n### Dealing with columns properties: dropping, change names or data types\n\n1. Casting columns to numerical:\n    - Married\/Single --> Is_Single\n    - Car_Ownership --> Has_Car\n    \n    \n2. Drop columns:\n   - ID: will not be used for predictions    \n   - Married\/Single\n   - Car_Ownership","587de525":"**Logistic regression algorithm**","4ae459d8":"Author Linkedin: https:\/\/www.linkedin.com\/in\/hernane-braga-pereira\/\n\nThis notebook was provided by this Kaggle dataset on loan default predicions: https:\/\/www.kaggle.com\/subhamjain\/loan-prediction-based-on-customer-behavior\n\n---\n\n# Loan Prediction Based on Customer Behavior\n_Predict possible client defaults for Loans Product_\n\n**Main question**: _Is it possible to determine which consumer may default on a loan?_ \n\n\n\n**_Answer:_** Yes. With a random forest model that achieves: ROC AUC = 0.94 and PR AUC = 0.54\n\n---\nTo understand how we got these results, we'll dive into three main topics:\n\n1. Creating a baseline model;\n2. Understanding our data;\n3. Improving our solution.\n\n","9ed41f07":"---\n## Apply exploratory data analysis (EDA) driven by hypothesis\n\nThis topic should explore three main aspects of our dataset:\n\n**1. Demography of clients**\n\n   1. Does younger people who have low income have more risk?\n   2. Is there any state or city with more loans when analyzing the percentual of risk\/population?\n    \n    \n**2. Professional aspects**\n    \n   1. Professionals with a lower time of experience are more at risk?\n   2. Is there a profession or a set of professions more risk?\n\n**3. The inheritance of people who asks for a loan**\n\n   1. People with more patrimony have lower risk? More patrimony is equal to having their car and\/or house.\n\n**_Note_**: although I listed the three main aspects of our problem, I'll only make an analysis based on the first one: demography of clients"}}