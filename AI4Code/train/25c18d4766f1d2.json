{"cell_type":{"e30c5dc6":"code","b29b5c00":"code","23b32d10":"code","0b3c13f5":"code","b401ee92":"code","21b78f86":"code","f92431db":"code","eb2cc509":"code","c07c42a4":"code","20edad1a":"code","8262ea46":"code","9d6d54f4":"code","3331d830":"code","b4f46b19":"markdown"},"source":{"e30c5dc6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b29b5c00":"stats=pd.read_csv('..\/input\/nba-regular-season-stats-20182019\/nbastats2018-2019.csv')","23b32d10":"X=stats.iloc[:,6].values\nY=stats.iloc[:,16].values\n","0b3c13f5":"\ndict={stats.Name[i]:i for i in range(521)}","b401ee92":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 1\/5, random_state = 0)\n","21b78f86":"plt.scatter(X_train, y_train)\n\n\nplt.xlabel('FGA')\nplt.ylabel('Points')\nplt.show()","f92431db":"plt.scatter(X_test, y_test)\nplt.xlabel('FGA')\nplt.ylabel('Points')\nplt.show()","eb2cc509":"\nX_train=X_train.reshape(416,1)\ny_train=y_train.reshape(416,1)\nX_train = np.c_[np.ones((X_train.shape[0])), X_train]\n","c07c42a4":"def gradientDescent(x, y, theta, alpha, m, numIterations):\n    xTrans = x.transpose()\n    for i in range(0, numIterations):\n        hypothesis = np.dot(x, theta)\n        loss = hypothesis - y\n        \n        cost = np.sum(loss ** 2) \/ (2 * m)\n       \n        \n        gradient = np.dot(xTrans, loss) \/ m\n       \n        theta = theta - alpha * gradient\n    return theta\n","20edad1a":"import warnings\nwarnings.filterwarnings(\"ignore\")\nm=X_train.shape[0]\nalpha=0.01\nnum=10000\ntheta=np.zeros((2,1))\ntheta2=gradientDescent(X_train,y_train,theta,alpha,m,num)\n\n","8262ea46":"theta2","9d6d54f4":"X_test=X_test.reshape(105,1)\ny_test=y_test.reshape(105,1)\nX_test = np.c_[np.ones((X_test.shape[0])), X_test]\n","3331d830":"print(theta2[0]+theta2[1]*X_test[50,1])\nprint(y_test[50])\n","b4f46b19":"## "}}