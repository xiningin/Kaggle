{"cell_type":{"48722f6c":"code","65c37a8f":"code","8dda7ac0":"code","0a1c70ad":"code","61b78434":"code","4212513a":"code","745b70fc":"code","ffc82155":"code","ec50b7cc":"code","166eab1d":"code","0e0a9af7":"code","5e075399":"code","e48e04de":"code","e9fa7c5e":"code","33965446":"code","13a3bf96":"code","0d665458":"code","87fdf472":"code","1c2185b4":"code","105aaa93":"code","5b8c3b9f":"code","527279e7":"code","92177cd9":"code","36a9ab47":"code","cc0b2ba9":"code","40934989":"code","b9038815":"code","c6b08ba3":"code","2339689f":"code","115fba38":"code","3c3cd7c4":"code","4f9345d5":"code","3f37300d":"code","8ca243a3":"code","1cd10488":"code","130e0662":"code","82679711":"code","938df326":"code","33f3ca22":"code","0fa829ba":"code","d8748c4f":"code","ba76c542":"code","b1f38770":"code","53891345":"code","a28f160f":"code","5b4f989d":"code","45db8708":"code","c26ba7f0":"code","a68dc440":"code","47a8a578":"code","a5c312b6":"code","5640397d":"code","69743ead":"code","bcffc9db":"code","73177f1f":"code","e2359569":"code","6978c0b5":"code","cc7f1b03":"code","19877ffd":"code","d2249e1f":"code","fc381678":"markdown","34695203":"markdown","165af5d6":"markdown","2452259c":"markdown","3b995803":"markdown","f6fa41fe":"markdown","45e9788f":"markdown","7ed4308a":"markdown","e0970cd9":"markdown","eff7e28a":"markdown","90500779":"markdown","b87aebe1":"markdown","8149fc00":"markdown","e0a4eee8":"markdown","987c14d9":"markdown","74d059d6":"markdown","a05663fc":"markdown","d2fe1cb8":"markdown","b5870bde":"markdown","b36962c5":"markdown","878d81b1":"markdown","0ecbae27":"markdown","c18f29f3":"markdown","bfac2e77":"markdown","2e20c95a":"markdown","cb05e24a":"markdown","3a20f40e":"markdown","492b87db":"markdown","da724b1c":"markdown","10a0baa2":"markdown","a2c54ec7":"markdown","83df6bb9":"markdown","00178457":"markdown","b9f6dd55":"markdown","df15743d":"markdown","74ca3aa5":"markdown","70ac01ac":"markdown","2275a6c7":"markdown","652f096f":"markdown","99aaf2a4":"markdown","8b50ebf7":"markdown","8282f8e5":"markdown","54e26954":"markdown","0bb71cde":"markdown","c57d5b07":"markdown","98918603":"markdown","db8c6d72":"markdown","ba174f99":"markdown","a9abb35c":"markdown","b2274d03":"markdown","d82f1b2b":"markdown","184305a8":"markdown","ab9beb94":"markdown","ed119fb7":"markdown","d4bbf53c":"markdown","dd8aa97e":"markdown","338fa45e":"markdown","a0351626":"markdown","181e61c1":"markdown","0219cbba":"markdown","55cd26f5":"markdown","ba67befa":"markdown","0ac54e87":"markdown","ac20b24d":"markdown","3ee0dba9":"markdown","533409f4":"markdown","9857a336":"markdown","9dd5c3b2":"markdown","338850d6":"markdown","6035a956":"markdown"},"source":{"48722f6c":"class KernelSettings:\n    \n    def __init__(self, fit_baseline=False, fit_improved_baseline=False):\n        self.fit_baseline = fit_baseline\n        self.fit_improved_baseline = fit_improved_baseline","65c37a8f":"kernelsettings = KernelSettings(fit_baseline=False, fit_improved_baseline=False)","8dda7ac0":"use_dropout=False","0a1c70ad":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\nfrom scipy.misc import imread\n\nimport tensorflow as tf\nsns.set()\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Any results you write to the current directory are saved as output.","61b78434":"train_labels = pd.read_csv(\"..\/input\/human-protein-atlas-image-classification\/train.csv\")\ntrain_labels.head()","4212513a":"train_labels.shape[0]","745b70fc":"label_names = {\n    0:  \"Nucleoplasm\",  \n    1:  \"Nuclear membrane\",   \n    2:  \"Nucleoli\",   \n    3:  \"Nucleoli fibrillar center\",   \n    4:  \"Nuclear speckles\",\n    5:  \"Nuclear bodies\",   \n    6:  \"Endoplasmic reticulum\",   \n    7:  \"Golgi apparatus\",   \n    8:  \"Peroxisomes\",   \n    9:  \"Endosomes\",   \n    10:  \"Lysosomes\",   \n    11:  \"Intermediate filaments\",   \n    12:  \"Actin filaments\",   \n    13:  \"Focal adhesion sites\",   \n    14:  \"Microtubules\",   \n    15:  \"Microtubule ends\",   \n    16:  \"Cytokinetic bridge\",   \n    17:  \"Mitotic spindle\",   \n    18:  \"Microtubule organizing center\",   \n    19:  \"Centrosome\",   \n    20:  \"Lipid droplets\",   \n    21:  \"Plasma membrane\",   \n    22:  \"Cell junctions\",   \n    23:  \"Mitochondria\",   \n    24:  \"Aggresome\",   \n    25:  \"Cytosol\",   \n    26:  \"Cytoplasmic bodies\",   \n    27:  \"Rods & rings\"\n}\n\nreverse_train_labels = dict((v,k) for k,v in label_names.items())\n\ndef fill_targets(row):\n    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n    for num in row.Target:\n        name = label_names[int(num)]\n        row.loc[name] = 1\n    return row","ffc82155":"for key in label_names.keys():\n    train_labels[label_names[key]] = 0","ec50b7cc":"train_labels = train_labels.apply(fill_targets, axis=1)\ntrain_labels.head()","166eab1d":"target_counts = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=0).sort_values(ascending=False)\nplt.figure(figsize=(15,15))\nsns.barplot(y=target_counts.index.values, x=target_counts.values, order=target_counts.index)","0e0a9af7":"train_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)\ncount_perc = np.round(100 * train_labels[\"number_of_targets\"].value_counts() \/ train_labels.shape[0], 2)\nplt.figure(figsize=(20,5))\nsns.barplot(x=count_perc.index.values, y=count_perc.values, palette=\"Reds\")\nplt.xlabel(\"Number of targets per image\")\nplt.ylabel(\"% of data\")","5e075399":"plt.figure(figsize=(15,15))\nsns.heatmap(train_labels[train_labels.number_of_targets>1].drop(\n    [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n).corr(), cmap=\"RdYlBu\", vmin=-1, vmax=1)","e48e04de":"def find_counts(special_target, labels):\n    counts = labels[labels[special_target] == 1].drop(\n        [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n    ).sum(axis=0)\n    counts = counts[counts > 0]\n    counts = counts.sort_values()\n    return counts","e9fa7c5e":"lyso_endo_counts = find_counts(\"Lysosomes\", train_labels)\n\nplt.figure(figsize=(10,3))\nsns.barplot(x=lyso_endo_counts.index.values, y=lyso_endo_counts.values, palette=\"Blues\")","33965446":"rod_rings_counts = find_counts(\"Rods & rings\", train_labels)\nplt.figure(figsize=(15,3))\nsns.barplot(x=rod_rings_counts.index.values, y=rod_rings_counts.values, palette=\"Greens\")","13a3bf96":"peroxi_counts = find_counts(\"Peroxisomes\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=peroxi_counts.index.values, y=peroxi_counts.values, palette=\"Reds\")","0d665458":"tubeends_counts = find_counts(\"Microtubule ends\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=tubeends_counts.index.values, y=tubeends_counts.values, palette=\"Purples\")","87fdf472":"nuclear_speckles_counts = find_counts(\"Nuclear speckles\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=nuclear_speckles_counts.index.values, y=nuclear_speckles_counts.values, palette=\"Oranges\")\nplt.xticks(rotation=\"70\")","1c2185b4":"from os import listdir\n\nfiles = listdir(\"..\/input\/human-protein-atlas-image-classification\/train\")\nfor n in range(10):\n    print(files[n])","105aaa93":"len(files) \/ 4 == train_labels.shape[0]","5b8c3b9f":"train_path = \"..\/input\/human-protein-atlas-image-classification\/train\/\"","527279e7":"def load_image(basepath, image_id):\n    images = np.zeros(shape=(4,512,512))\n    images[0,:,:] = imread(basepath + image_id + \"_green\" + \".png\")\n    images[1,:,:] = imread(basepath + image_id + \"_red\" + \".png\")\n    images[2,:,:] = imread(basepath + image_id + \"_blue\" + \".png\")\n    images[3,:,:] = imread(basepath + image_id + \"_yellow\" + \".png\")\n    return images\n\ndef make_image_row(image, subax, title):\n    subax[0].imshow(image[0], cmap=\"Greens\")\n    subax[1].imshow(image[1], cmap=\"Reds\")\n    subax[1].set_title(\"stained microtubules\")\n    subax[2].imshow(image[2], cmap=\"Blues\")\n    subax[2].set_title(\"stained nucleus\")\n    subax[3].imshow(image[3], cmap=\"Oranges\")\n    subax[3].set_title(\"stained endoplasmatic reticulum\")\n    subax[0].set_title(title)\n    return subax\n\ndef make_title(file_id):\n    file_targets = train_labels.loc[train_labels.Id==file_id, \"Target\"].values[0]\n    title = \" - \"\n    for n in file_targets:\n        title += label_names[n] + \" - \"\n    return title","92177cd9":"class TargetGroupIterator:\n    \n    def __init__(self, target_names, batch_size, basepath):\n        self.target_names = target_names\n        self.target_list = [reverse_train_labels[key] for key in target_names]\n        self.batch_shape = (batch_size, 4, 512, 512)\n        self.basepath = basepath\n    \n    def find_matching_data_entries(self):\n        train_labels[\"check_col\"] = train_labels.Target.apply(\n            lambda l: self.check_subset(l)\n        )\n        self.images_identifier = train_labels[train_labels.check_col==1].Id.values\n        train_labels.drop(\"check_col\", axis=1, inplace=True)\n    \n    def check_subset(self, targets):\n        return np.where(set(targets).issubset(set(self.target_list)), 1, 0)\n    \n    def get_loader(self):\n        filenames = []\n        idx = 0\n        images = np.zeros(self.batch_shape)\n        for image_id in self.images_identifier:\n            images[idx,:,:,:] = load_image(self.basepath, image_id)\n            filenames.append(image_id)\n            idx += 1\n            if idx == self.batch_shape[0]:\n                yield filenames, images\n                filenames = []\n                images = np.zeros(self.batch_shape)\n                idx = 0\n        if idx > 0:\n            yield filenames, images\n            ","36a9ab47":"your_choice = [\"Lysosomes\", \"Endosomes\"]\nyour_batch_size = 3","cc0b2ba9":"imageloader = TargetGroupIterator(your_choice, your_batch_size, train_path)\nimageloader.find_matching_data_entries()\niterator = imageloader.get_loader()","40934989":"file_ids, images = next(iterator)\n\nfig, ax = plt.subplots(len(file_ids),4,figsize=(20,5*len(file_ids)))\nif ax.shape == (4,):\n    ax = ax.reshape(1,-1)\nfor n in range(len(file_ids)):\n    make_image_row(images[n], ax[n], make_title(file_ids[n]))","b9038815":"train_files = listdir(\"..\/input\/human-protein-atlas-image-classification\/train\")\ntest_files = listdir(\"..\/input\/human-protein-atlas-image-classification\/test\")\npercentage = np.round(len(test_files) \/ len(train_files) * 100)\n\nprint(\"The test set size turns out to be {} % compared to the train set.\".format(percentage))","c6b08ba3":"from sklearn.model_selection import RepeatedKFold\n\nsplitter = RepeatedKFold(n_splits=3, n_repeats=1, random_state=0)","2339689f":"partitions = []\n\nfor train_idx, test_idx in splitter.split(train_labels.index.values):\n    partition = {}\n    partition[\"train\"] = train_labels.Id.values[train_idx]\n    partition[\"validation\"] = train_labels.Id.values[test_idx]\n    partitions.append(partition)\n    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))","115fba38":"partitions[0][\"train\"][0:5]","3c3cd7c4":"class ModelParameter:\n    \n    def __init__(self, basepath,\n                 num_classes=28,\n                 image_rows=512,\n                 image_cols=512,\n                 batch_size=200,\n                 n_channels=1,\n                 row_scale_factor=4,\n                 col_scale_factor=4,\n                 shuffle=False,\n                 n_epochs=1):\n        self.basepath = basepath\n        self.num_classes = num_classes\n        self.image_rows = image_rows\n        self.image_cols = image_cols\n        self.batch_size = batch_size\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.row_scale_factor = row_scale_factor\n        self.col_scale_factor = col_scale_factor\n        self.scaled_row_dim = np.int(self.image_rows \/ self.row_scale_factor)\n        self.scaled_col_dim = np.int(self.image_cols \/ self.col_scale_factor)\n        self.n_epochs = n_epochs","4f9345d5":"parameter = ModelParameter(train_path)","3f37300d":"from skimage.transform import resize\n\nclass ImagePreprocessor:\n    \n    def __init__(self, modelparameter):\n        self.parameter = modelparameter\n        self.basepath = self.parameter.basepath\n        self.scaled_row_dim = self.parameter.scaled_row_dim\n        self.scaled_col_dim = self.parameter.scaled_col_dim\n        self.n_channels = self.parameter.n_channels\n    \n    def preprocess(self, image):\n        image = self.resize(image)\n        image = self.reshape(image)\n        image = self.normalize(image)\n        return image\n    \n    def resize(self, image):\n        image = resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n        return image\n    \n    def reshape(self, image):\n        image = np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n        return image\n    \n    def normalize(self, image):\n        image \/= 255 \n        return image\n    \n    def load_image(self, image_id):\n        image = np.zeros(shape=(512,512,4))\n        image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n        image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n        image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n        image[:,:,3] = imread(self.basepath + image_id + \"_yellow\" + \".png\")\n        return image[:,:,0:self.parameter.n_channels]\n        ","8ca243a3":"preprocessor = ImagePreprocessor(parameter)","1cd10488":"example = images[0,0]\npreprocessed = preprocessor.preprocess(example)\nprint(example.shape)\nprint(preprocessed.shape)\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nax[0].imshow(example, cmap=\"Greens\")\nax[1].imshow(preprocessed.reshape(parameter.scaled_row_dim,parameter.scaled_col_dim), cmap=\"Greens\")","130e0662":"import keras\n\nclass DataGenerator(keras.utils.Sequence):\n    \n    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor):\n        self.params = modelparameter\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n        self.batch_size = self.params.batch_size\n        self.n_channels = self.params.n_channels\n        self.num_classes = self.params.num_classes\n        self.shuffle = self.params.shuffle\n        self.preprocessor = imagepreprocessor\n        self.on_epoch_end()\n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    def get_targets_per_image(self, identifier):\n        return self.labels.loc[self.labels.Id==identifier].drop(\n                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n            \n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n        # Generate data\n        for i, identifier in enumerate(list_IDs_temp):\n            # Store sample\n            image = self.preprocessor.load_image(identifier)\n            image = self.preprocessor.preprocess(image)\n            X[i] = image\n            # Store class\n            y[i] = self.get_targets_per_image(identifier)\n        return X, y\n    \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y","82679711":"class PredictGenerator:\n    \n    def __init__(self, predict_Ids, imagepreprocessor, predict_path):\n        self.preprocessor = imagepreprocessor\n        self.preprocessor.basepath = predict_path\n        self.identifiers = predict_Ids\n    \n    def predict(self, model):\n        y = np.empty(shape=(len(self.identifiers), self.preprocessor.parameter.num_classes))\n        for n in range(len(self.identifiers)):\n            image = self.preprocessor.load_image(self.identifiers[n])\n            image = self.preprocessor.preprocess(image)\n            image = image.reshape((1, *image.shape))\n            y[n] = model.predict(image)\n        return y","938df326":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import Adadelta\nfrom keras.models import load_model\n\n\nclass BaseLineModel:\n    \n    def __init__(self, modelparameter):\n        self.params = modelparameter\n        self.num_classes = self.params.num_classes\n        self.img_rows = self.params.scaled_row_dim\n        self.img_cols = self.params.scaled_col_dim\n        self.n_channels = self.params.n_channels\n        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n        self.my_metrics = ['accuracy']\n    \n    def build_model(self):\n        self.model = Sequential()\n        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n        self.model.add(Dropout(0.25))\n        self.model.add(Flatten())\n        self.model.add(Dense(64, activation='relu'))\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n    \n    def compile_model(self):\n        self.model.compile(loss=keras.losses.binary_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=self.my_metrics)\n    \n    def set_generators(self, train_generator, validation_generator):\n        self.training_generator = train_generator\n        self.validation_generator = validation_generator\n    \n    def learn(self):\n        return self.model.fit_generator(generator=self.training_generator,\n                    validation_data=self.validation_generator,\n                    epochs=self.params.n_epochs, \n                    use_multiprocessing=True,\n                    workers=8)\n    \n    def score(self):\n        return self.model.evaluate_generator(generator=self.validation_generator,\n                                      use_multiprocessing=True, \n                                      workers=8)\n    \n    def predict(self, predict_generator):\n        y = predict_generator.predict(self.model)\n        return y\n    \n    def save(self, modeloutputpath):\n        self.model.save(modeloutputpath)\n    \n    def load(self, modelinputpath):\n        self.model = load_model(modelinputpath)","33f3ca22":"# Datasets\npartition = partitions[0]\nlabels = train_labels\n\nprint(\"Number of samples in train: {}\".format(len(partition[\"train\"])))\nprint(\"Number of samples in validation: {}\".format(len(partition[\"validation\"])))","0fa829ba":"training_generator = DataGenerator(partition['train'], labels, parameter, preprocessor)\nvalidation_generator = DataGenerator(partition['validation'], labels, parameter, preprocessor)\npredict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)","d8748c4f":"# Run computation and store results as csv\nif kernelsettings.fit_baseline == True:\n    model = BaseLineModel(parameter)\n    model.build_model()\n    model.compile_model()\n    model.set_generators(training_generator, validation_generator)\n    history = model.learn()\n    #model.save(\"baseline_model.h5\")\n    proba_predictions = model.predict(predict_generator)\n    baseline_proba_predictions = pd.DataFrame(proba_predictions, columns=train_labels.drop(\n        [\"Target\", \"number_of_targets\", \"Id\"], axis=1).columns)\n    baseline_proba_predictions.to_csv(\"baseline_predictions.csv\")\n# If you already have done a baseline fit once, \n# you can load predictions as csv and further fitting is not neccessary:\nelse:\n    baseline_proba_predictions = pd.read_csv(\"..\/input\/protein-atlas-eab-predictions\/baseline_predictions.csv\", index_col=0)","ba76c542":"validation_labels = train_labels.loc[train_labels.Id.isin(partition[\"validation\"])]","b1f38770":"print(validation_labels.shape)\nprint(baseline_proba_predictions.shape)","53891345":"baseline_proba_predictions.tail()","a28f160f":"proba_predictions = baseline_proba_predictions.values","5b4f989d":"hot_values = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).values.flatten()\none_hot = (hot_values.sum()) \/ hot_values.shape[0] * 100\nzero_hot = (hot_values.shape[0] - hot_values.sum()) \/ hot_values.shape[0] * 100\n\nfig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.distplot(proba_predictions.flatten() * 100, color=\"DodgerBlue\", ax=ax[0])\nax[0].set_xlabel(\"Probability in %\")\nax[0].set_ylabel(\"Density\")\nax[0].set_title(\"Predicted probabilities\")\nsns.barplot(x=[\"label = 0\", \"label = 1\"], y=[zero_hot, one_hot], ax=ax[1])\nax[1].set_ylim([0,100])\nax[1].set_title(\"True target label count\")\nax[1].set_ylabel(\"Percentage\")","45db8708":"mean_predictions = np.mean(proba_predictions, axis=0)\nstd_predictions = np.std(proba_predictions, axis=0)\nmean_targets = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).mean()\n\nlabels = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).columns.values\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.barplot(x=labels,\n            y=mean_predictions,\n            ax=ax[0])\nax[0].set_xticklabels(labels=labels,\n                      rotation=90)\nax[0].set_ylabel(\"Mean predicted probability\")\nax[0].set_title(\"Mean predicted probability per class over all samples\")\nsns.barplot(x=labels,\n           y=std_predictions,\n           ax=ax[1])\nax[1].set_xticklabels(labels=labels,\n                      rotation=90)\nax[1].set_ylabel(\"Standard deviation\")\nax[1].set_title(\"Standard deviation of predicted probability per class over all samples\")","c26ba7f0":"fig, ax = plt.subplots(1,1,figsize=(20,5))\nsns.barplot(x=labels, y=mean_targets.values, ax=ax)\nax.set_xticklabels(labels=labels,\n                      rotation=90)\nax.set_ylabel(\"Percentage of hot (1)\")\nax.set_title(\"Percentage of hot counts (ones) per target class\")","a68dc440":"feature = \"Cytosol\"","47a8a578":"plt.figure(figsize=(20,5))\nsns.distplot(baseline_proba_predictions[feature].values[0:-10], color=\"Purple\")\nplt.xlabel(\"Predicted probabilites of {}\".format(feature))\nplt.ylabel(\"Density\")\nplt.xlim([0,1])","a5c312b6":"wishlist = [\"Nucleoplasm\", \"Cytosol\", \"Plasma membrane\"]","5640397d":"class ImprovedDataGenerator(DataGenerator):\n    \n    # in contrast to the base DataGenerator we add a target wishlist to init\n    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor, target_wishlist):\n        super().__init__(list_IDs, labels, modelparameter, imagepreprocessor)\n        self.target_wishlist = target_wishlist\n    \n    def get_targets_per_image(self, identifier):\n        return self.labels.loc[self.labels.Id==identifier][self.target_wishlist].values","69743ead":"import keras.backend as K\n\ndef base_f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return f1\n\ndef f1_min(y_true, y_pred):\n    f1 = base_f1(y_true, y_pred)\n    return K.min(f1)\n\ndef f1_max(y_true, y_pred):\n    f1 = base_f1(y_true, y_pred)\n    return K.max(f1)\n\ndef f1_mean(y_true, y_pred):\n    f1 = base_f1(y_true, y_pred)\n    return K.mean(f1)\n\ndef f1_std(y_true, y_pred):\n    f1 = base_f1(y_true, y_pred)\n    return K.std(f1)","bcffc9db":"class TrackHistory(keras.callbacks.Callback):\n    \n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))","73177f1f":"class ImprovedModel(BaseLineModel):\n    \n    def __init__(self, modelparameter,\n                 use_dropout,\n                 my_metrics=[f1_mean, f1_std, f1_min, f1_max]):\n        \n        super().__init__(modelparameter)\n        self.my_metrics = my_metrics\n        self.use_dropout = use_dropout\n        \n    def learn(self):\n        self.history = TrackHistory()\n        return self.model.fit_generator(generator=self.training_generator,\n                    validation_data=self.validation_generator,\n                    epochs=self.params.n_epochs, \n                    use_multiprocessing=True,\n                    workers=8,\n                    callbacks = [self.history])\n    \n    def build_model(self):\n        self.model = Sequential()\n        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n        if self.use_dropout:\n            self.model.add(Dropout(0.25))\n        self.model.add(Flatten())\n        self.model.add(Dense(64, activation='relu'))\n        if self.use_dropout:\n            self.model.add(Dropout(0.5))\n        self.model.add(Dense(self.num_classes, activation='sigmoid'))","e2359569":"parameter = ModelParameter(train_path, num_classes=len(wishlist), n_epochs=5, batch_size=64)\npreprocessor = ImagePreprocessor(parameter)\nlabels = train_labels","6978c0b5":"training_generator = ImprovedDataGenerator(partition['train'], labels,\n                                           parameter, preprocessor, wishlist)\nvalidation_generator = ImprovedDataGenerator(partition['validation'], labels,\n                                             parameter, preprocessor, wishlist)\npredict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)","cc7f1b03":"# Run computation and store results as csv\nif kernelsettings.fit_improved_baseline == True:\n    model = ImprovedModel(parameter, use_dropout=use_dropout)\n    model.build_model()\n    model.compile_model()\n    model.set_generators(training_generator, validation_generator)\n    epoch_history = model.learn()\n    proba_predictions = model.predict(predict_generator)\n    #model.save(\"improved_model.h5\")\n    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n    improved_proba_predictions.to_csv(\"improved_predictions.csv\")\n# If you already have done a baseline fit once, \n# you can load predictions as csv and further fitting is not neccessary:\nelse:\n    improved_proba_predictions = pd.read_csv(\"..\/input\/protein-atlas-eab-predictions\/improved_predictions.csv\", index_col=0)","19877ffd":"if kernelsettings.fit_improved_baseline == True:\n    print(epoch_history.history.keys())\n    fig, ax = plt.subplots(2,1,figsize=(20,10))\n    ax[0].plot(model.history.losses, color=\"Red\")\n    ax[0].set_xlabel(\"Update step after one batch\")\n    ax[0].set_ylabel(\"Train loss\")\n    ax[0].set_title(\"Loss evolution batch per batch\")\n    ax[1].plot(epoch_history.history[\"val_loss\"], color=\"Green\")\n    ax[1].plot(epoch_history.history[\"loss\"])\n    ax[1].set_title(\"Loss evolution per epoch\")\n    ax[1].set_xlabel(\"Update step after one epoch\")\n    ax[1].set_ylabel(\"Loss\")\n    fig.savefig(\"losses_improved_model\", format=\"eps\")","d2249e1f":"fig, ax = plt.subplots(3,1,figsize=(25,15))\nsns.distplot(improved_proba_predictions.values[:,0], color=\"Orange\", ax=ax[0])\nax[0].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[0]))\nax[0].set_xlim([0,1])\nsns.distplot(improved_proba_predictions.values[:,1], color=\"Purple\", ax=ax[1])\nax[1].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[1]))\nax[1].set_xlim([0,1])\nsns.distplot(improved_proba_predictions.values[:,2], color=\"Limegreen\", ax=ax[2])\nax[2].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[2]))\nax[2].set_xlim([0,1])","fc381678":"## Improved model settings\n\nWithin this kernel you can play with different improvement steps of the baseline model in a plug and play style. To make it easier to understand which improvements you currently use, let's set them here in front of the analysis:","34695203":"Our last losses were obtained by using dropout with high percentage (25 % and 50 %) of dropped neurons. **What will happen if we do not use our dropout layers?**\n\n![LossWithoutDropout](https:\/\/www.kaggleusercontent.com\/kf\/7212722\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..ZSQ_3svPXTPSUJWGcnFFjQ.IfPN45Kb77RLHEkt96pfewTm-O_B93fu-vw-weY_q8_pXlM8HdkyiFr1bhbJ9BmzzSckS4KvDKa2HNRb4aCJwa9btO4vwA4lUToAsCpRqZeHE5w6u4K1-AG_K6FWdDACoYvy7tP52dJku4zJsEySvTuoUt1qcb4xAsDoIlUMedg.z5L-MIPi65oUJmTJIRqXig\/__results___files\/__results___120_1.png)","165af5d6":"## Which targets are correlated?\n\nLet's see if we find some correlations between our targets. This way we may already see that some proteins often come together.","2452259c":"Let's try to visualize specific target groups. **In this example we will see images that contain the protein structures lysosomes or endosomes**. Set target values of your choice and the target group iterator will collect all images that are subset of your choice:","3b995803":"### Take-away\n\n* Most train images only have 1 or two target labels.\n* More than 3 targets are very seldom!","f6fa41fe":"## How do the images look like?\n\n","45e9788f":"## One-Step-Improvement\n\n\nOk, again let's go one step back and choose the most common target proteins that are present in our data: nucleoplasm, cytosol and plasma membrane. If we are not able to predict them we can go home and stay in bed ;-) .","7ed4308a":"## How many targets are most common?","e0970cd9":"### Data Generator\n\nI highly build upon the [nice data generator presented by Shervine Amidi.](https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly) Thank you! :-) \n","eff7e28a":"### Lysosomes and endosomes\n\nLet's start with these high correlated features!","90500779":"### What does the loss tell us?\n![Losses](https:\/\/www.kaggleusercontent.com\/kf\/7185016\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..3D5GOiotTaFacOc_0ztMkQ.xELnVFi-FwGr8RjlVpZt65KXnyCK5GtztyHgErbUR1iSjxSROPp2jai_q_MHvZMSZ56KGVPQPtCx5j4AegtwsDtZRa8QAT3urcmvDW80qKcuApN7DJWnV7xJxGvfVIInJS_ml5Rma43kvo5NWA3OgXE9qfTJ_zG4C1p95FMUJN8.1bNcwgsup-EREVBNPymfFA\/__results___files\/__results___118_1.png)\n\nThe loss is very noisy! While decreasing the batch size we increased the number of learning steps. Hence our model learns faster. But... with smaller batch size there **are fewer samples to learn from, to compute gradients from**! The gradients we obtain may be very specific to the images and class labels that are covered by the batch of the current learning step. **There was a tradeoff we made**. We gained more learning speed but payed with a reduced gradient quality. Before increasing the batch size again and waiting too long for predictions we might improve by choosing another way:\n\n1. Weight regularization\n2. Gradient clipping\n\nThese two will be the next improvement steps. Nonetheless, one question remains: Has our model started learning? Can we see a separating force that tries to split zero and one predictions?","b87aebe1":"### To which targets do the high and small predicted probabilities belong to?","8149fc00":"### Our goal\n\n* Predict various protein structures in cellular images\n* there are 28 different target proteins\n* multiple proteins can be present in one image (multilabel classification)\n* 27 different cell types of highly different morphology","e0a4eee8":"### Plug and Play\n\n:-) \n\nThis part makes fun! Actually I don't know if our model learns something meaningful. But we can try to find out by improving our model adding **new features** and playing with **different parameter settings**. The latter I like to do in a **plug-and-play style**: Setting the flag *improve=True* adds a change to our model whereas *improve=False* uses the old concept we already used in the baseline. \n\n","987c14d9":"## Kernel settings\n\nThis notebook contains model fitting that may take some time. **If you don't like to wait for compution, you can set fit_baseline and\/or fit_improved_baseline of the KernelSettings class to False**:","74d059d6":"### Peroxisomes","a05663fc":"### Training the baseline on the first cv-fold","d2fe1cb8":"### The target wish list\n\nTo introduce a target wishlist that we can change whenever we want we need to improve the data generator. For this purpose we're going to extend the class we have already written. Taking a closer look at the base generator you can see that there is just one line code in def data_generation(self, list_IDs_temp) we have to change, namely the part with y[i] = ... inside the for loop over temp list ids (image identifiers of the batch). To make things easier, I added a small method to the DataGenerator we already had:\n\n```\ndef get_targets_per_image(self, identifier):\n        return self.labels.loc[self.labels.Id==identifier].drop(\n                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n```\n\nThis method just avoids the direct pass to y[i], the targets per image in a batch. Now, we can overwrite this method in our ImprovedDataGenerator without loosing functionality:","b5870bde":"You can see that we have lost a lot of information by downscaling the image!","b36962c5":"### Microtubule ends","878d81b1":"### Take-away\n\n* We can see that many targets only have very slight correlations. \n* In contrast, endosomes and lysosomes often occur together and sometimes seem to be located at the endoplasmatic reticulum. \n* In addition we find that the mitotic spindle often comes together with the cytokinetic bridge. This makes sense as both are participants for cellular division. And in this process microtubules and thier ends are active and participate as well. Consequently we find a positive correlation between these targets.\n\nIf you like to dive deeper into target correlations you may like to take a look at my kernel notebook about [target clustering.](https:\/\/www.kaggle.com\/allunia\/in-depth-protein-correlations) :-)","0ecbae27":"As our test data is 38 % of size compared to the train set it makes sense to use 3-Fold cross validation where the test set is 33 % of size compared to the train set. As we are working with neural networks that can be demanding in computational resources, let's only use 2 repetitions. ","c18f29f3":"## What do the results tell us?\n\nLet's have a look at predicted probabilites per target class:","bfac2e77":"No, it does not seem that our model starts to separate well. The mode is close to the fraction of one-hot-counts over all samples. At least the flat tail gives hope that learning could be in progress.  But even though our next goal should be to find out what to tune in such a way that our model really starts learning! ","2e20c95a":"### Take-away\n\n* We can see that even with very seldom targets we find some kind of grouping with other targets that reveal where the protein structure seems to be located. \n* For example, we can see that rods and rings have something to do with the nucleus whereas peroxisomes may be located in the nucleus as well as in the cytosol.\n* Perhaps this patterns might help to build a more robust model!  ","cb05e24a":"We are computing the change of the loss with respect to a change in the weights for each sample one after another. Consequently in original gradient descent we need to pass the whole dataset once for just one single update step of gradient descent. As our initial weights are not sufficent to solve the classification task we need many such update steps. **But what if the dataset it too large to wait a long time just for one of these steps?**\n\nPerhaps it would be sufficient and already good to use only some $M < N$ of the $N$ samples to compute the gradients $\\partial_{w_{i,j}} E$:\n\n$$ \\partial_{w_{i,j}} E = \\sum_{m=1}^{M} \\partial_{w_{i,j}} E_{m} $$\n\nThis way **we could use only a batch of samples, compute the gradients and perform the update of weights**. After that we continue by computing the gradients with the next batch. This could be done in sequence of samples or random with replacement. This means that we already start learning without passing the whole dataset. In my case I chose a batch_size of 200. With a total number of samples of around 20000 in my training set this means that I have already done 200 weight updates. **After doing so I have used each sample once and this means the whole dataset was passed through gradient descent**. As far as I know this means, **one epoch**. In addition we can now say that we want to shuffle the samples before doing weight updates after each batch again in the next epoch. This way our model sees a different set, computes some other gradients and hence the loss minimziation could be more robust.   ","3a20f40e":"### Shared Parameter class","492b87db":"## How can we tackle gradient jiggles?\n\nLet's try to dive deeper into the problem. We compute the gradients with respect to the weights after processing each batch this way:\n\n$$ \\partial_{w_{i,j}} E = \\sum_{m=1}^{M} \\partial_{w_{i,j}} E_{m} $$","da724b1c":"## Which proteins occur most often in images?","10a0baa2":"This splitter is now a generator. Hence if you call splitters split method it will yield one Fold of the repeated K-Folds. Consequently if we choose n_repeats=2 we will end up with 6 Folds in total: 3 Folds for the first cross validation and again 3 Folds for the repeated cross validation. We will perform the splitting on the image ids. This way we can easily load images and targets given the chunk ids. **Due to performance reasons I will only use one cv-fold to explore results and one repeat!**","a2c54ec7":"#### Looking at a preprocessed example image","83df6bb9":"To understand the performance of our model we will use **k-fold cross validation**. The train data is splitted into k chunks and each chunk is used once for testing the prediction performance whereas the others are used for training. As our targets show relationships seemed to be grouped somehow the performance per test chunk probably highly depends on the target distribution per test chunk. For example there could be chunks with very seldom targets that may obtain a bad score and some chunks with very common targets and a very good score. To reduce this effect, we will **repeat the K-Fold several times** and look at scoing distributions in the end.","00178457":"Let's see how many test and train samples we have in this competition:","b9f6dd55":"### Collecting ideas\n\nNext we need to setup a simple baseline model. This need not be very complex or very good. Its our first attempt to play with and to figure out how to improve. For this purpose let's use the deep learning library [keras](https:\/\/keras.io\/). This tools makes it easy for us to build and train neural networks. First of all, we should collect some ideas:\n\n* To **stay simple let's use only the green channel image of our images per id**. The competition says that it shows the stained target proteins and consequently it's hopefully the most informative one. The other images are like references showing microtubules, nucleus and endoplasmatic reticulum. We don't acutally now how informative they are and in our current state they would blow up our neural network with a huge amount of network weigths that we might not need.\n* Let's use **generators to only load data images of our batch and not all in once**. Using keras fit_generator, evaluate_generator and predict_generator we can directly connect them to keras without worrying much about how keras does its job. For this purpose I highly follow a descprition of a post in the www for which you will find the link below.\n* It could be advantegous to write a **small class that does simple preprocessing per image.** This way we can easily change something of this phase without producing chaos in the model itself or during data loading.   \n* I'm going to use a **small class that hold parameters that are used or shared between the data loader, the image preprocessor and the baseline model**. Passing an instance of this class to them reduced the risk of setting different parameters and obtaining mismatch errors for example during build & compile of the network layers. \n","df15743d":"Even though this accuracy looks nice it's an illusion! We are far away from a good model. Let's try to understand why...","74ca3aa5":"### CNN Baseline model using keras","70ac01ac":"Ok, now we will create an instance of this class and pass it to the DataGenerator, the BaseLineModel and the ImagePreprocessor.","2275a6c7":"This way the prediction probabilities of the corresponding model are loaded as csv from added data source. ","652f096f":"## How do images of specific targets look like?\n\nWhile looking at examples, we can build an batch loader:","99aaf2a4":"# Welcome to the Human Protein Atlas Competition!\n\nI started this kernel notebook to explore the data and build a simple baseline model to play with. I have never mind that the results are useful for many kagglers that like to start with the competition but need some starter code or some inspiration. Thank you for pushing this kernel that far! :-)\n\nIf you have just found this kernel, here is a **short summary of what you can find**:\n\n1. Encoding of binary target labels out of the given multilabel list per image,\n2. Visual analysis of target protein distribution in the train set,\n3. A simple image generator that yields images of a target-protein-wishlist. Each sample that has at least one match with this list is returned.\n4. Some ideas on validation.\n5. A baseline model build with keras that is supported by:\n    * A modelparameter class that holds all parameters that are necessary to build the model, to load the data and to preprocess the images.\n    * A data generator that can be used with CPU\/GPU computing to perform training and validation.\n    * An image preprocessor that rescales, reshapes and normalizes the images for feeding into the model.\n6. Ideas on how to improve the baseline model by tracking loss with a keras callback. \n7. Some ideas on how to proceed. (coming soon)","8b50ebf7":"### Dropout Layer\n\nI often read that dropout helps to avoid overfitting but for me it seems that there is one more useful advantage: Imagine the cat-problem above - Given a batch full of cats we only compute gradients for making the predictions of cats better. And this can be done to the detriment of the dogs as both classes share weights. Hence changing the weights with batches of imbalanced classes can lead to jiggles. \n\nNow let's consider a dropout-layer: It randomly selects a given percentage of input neurons and drops them during the current training step. Taking a look at [this paper](http:\/\/www.jmlr.org\/papers\/volume15\/srivastava14a\/srivastava14a.pdf) (linked at keras documentation) you can see that the forward and backward propagation is done only with this reduced, thinned kind of network. In our case this means that each learning step some weights will be untouched and not used to compute gradients. This is great as this would mean that some weight that could be good for predicting dogs will not change after we process a batch with cats only. ;-)\n\nConsequently if you have a **problem with overfitting** (learning too much out of your training data and loosing generalization performance) **or you have very small, imbalanced batches you should consider dropout** as a strategy. But before turning happy and starting to use dropout frequently, we should think about its downside: Dropping neruons during training and learning with a thinned kind of networks means that **we freeze learning each step a bit**. And the randomness of frosty the dropout snowman can turn to a problem difficult to graps: Which neurons should be dropped, are there some for which dropping is good and some for which it is bad? After which layer in our network does it make sense to use it?Perhaps it would have been better to use some neurons that were dropped a random dropout session during one batch learning step... we don't know. Perhaps we have prevented the success of a learning step given one batch and improved learning given another batch. The information flow through the network is somehow a blackbox for us and this randomness of thinned network learning makes it more difficult to understand what's going on. \n\nYou can see that in my baseline model I'm already using dropout. This choice was somehow arbitrarily as I used an example network that can be found to classify mnist digits as a starting point. Hence let's improve again and **turn dropout to a plug-and-play feature**:\n\n```\nclass ImprovedModel(BaseLineModel):\n    \n    def __init__(self, modelparameter,\n                 my_metrics=[f1_mean, f1_std, f1_min, f1_max],\n                 use_dropout=True):\n        \n        super().__init__(modelparameter)\n        self.my_metrics = my_metrics\n        self.use_dropout = use_dropout\n\n    def build_model(self):\n        self.model = Sequential()\n        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n        if self.use_dropout:\n            self.model.add(Dropout(0.25))\n        self.model.add(Flatten())\n        self.model.add(Dense(64, activation='relu'))\n        if self.use_dropout:\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n```","8282f8e5":"Improve! :-) We can already see that we might need **more than 1 epoch to learn the pattern in the data**. Computing the gradient with batches is a stochastic process: Depending on the batch samples the gradients may sometimes lead to some good update directions in weight space that points to some minimum of loss.... and sometimes they could lead to the opposite. Well this really depends on the samples within one batch. This becomes more robust with **more samples per batch.** But then we will **need even more epochs** as we make less weight updata aka learning steps! But even if this stochastic gradient descent looks wiggly it has an advantage too: It can escape from local minima of the loss function. That's nice. \n\nBefore we improve the model using more epochs and smaller batches, let's **implement a new feature: a loss callback**. This way we can see if the loss decreases during weight update steps! And we can see if we need more epochs or if the loss has already converged and settled down. ","54e26954":"### Image Preprocessor\n\nLet's write a simple image preprocessor that handles for example the rescaling of the images. Perhaps we can expand its functionality during improvement of the baseline model. ","0bb71cde":"Let's create an instance of this preprocessor and pass it to the data generator.","c57d5b07":"### K-Fold Cross-Validation","98918603":"### Rods and rings","db8c6d72":"How many samples do we have?","ba174f99":"### Does our model try to classify?\n\nIf this is the case and our model starts learning we should see more bimodal distributions of the predicted probability per target label:","a9abb35c":"To keep the kernel dense, the target group iterator has a batch size which stands for the number of examples you like to look at once. In this example you can see a maximum amount of 3 images at one iteration.  **To observe the next 3 examples of your target group, just run the cell below again.** This way you can run the cell until you have seen all images of your group without polluting the kernel:","b2274d03":"### Take-Away\n\n* We can see that our model was always very uncertain to predict the presence of a target protein. All probabilities are close to zero and there are only a few with targets where our model predicted a protein structure with higher than 10 %.\n* If we take a look at the true target label count we can see that most of our targets are filled with zero. This corresponds to an absence of corresponding target proteins. This makes sense: For each image we have a high probability to contain either 1 or 2 target protein structures. Their label values are one whereas all others are zero. \n* Consequently our high accuracy belongs to the high correct prediction of the absence of target proteins. In contrast we weren't able to predict the presence of a target protein which is the most relevant part! \n* Now a bell should ring :-) Have you ever heard about imbalanced classes and model confusion? ","d82f1b2b":"Looking at the sum we can see one disadvantage... it's mainly driven by high contributions. **An image in the batch that causes very high positive or negative gradients for the weight $w_{i,j}$ have more impact on the overall gradient than images with low absolute values**. This can be bad especially in the case of outlier images that are not representative to explain the pattern in the data. Consequently our model may try to learn from exotics. In addition we have to be very **careful with small batches as its target distribution might not reflect the overall pattern**. Imagine we would try to distinguish dogs from cats. With a batch size of 10 we are likely to fill up these places with imbalanced targets. For example it could be occupied with cats only. This would yield gradients that try to improve the detection of cats thereby changing the weights we might need to identify dogs. Hence beside image outliers the target distribution itself influences the learning as well. This can cause jiggles as well. One step we try to improve nucleoplasmn and the next perhaps cytosol but with a downgrade of the nucleoplasmn predictions and the next steps it could be the other way round. \n\nHow to solve this jiggle-wiggle problem?\n\nWell, first of all we might choose a higher batch_size again :-) Decreasing the batch size we made a tradeoff: We increased the learning speed but increased the risk of low quality gradients. Hence before playing with further strategies, we should make a step backwards again. With a batch_size of 128 and number of epochs 10, we obtain these losses:","184305a8":"Hmmmm this still looks not good :-( Have to search even further. ","ab9beb94":"Jeahy! This looks far better than the distributions we obtained with the baseline model! :-) **Thus increasing epochs while decreasing the batch size helped our model as we made more learning steps.** But... as we have seen by the noise of our losses as well we make large jumps in weight space with each update step. That's not nice! Sometimes these jumps could even lead to exploding losses as well. Then the jump was so big that we escaped from nice regions that lead to a local minimum of loss.","ed119fb7":"## Building a baseline model","d4bbf53c":"With our new strategy to perform predictions we should obtain the same shape[0] of the true targets and the predicted ones:","dd8aa97e":"### Take-Away\n\n* Looking at this few examples we can already obtain some insights:\n    * The staining of target proteins in the green channel was not equally successful. The **images differ in their intensities and the target proteins are not always located the same way**. The first image you can get by the loader shows endosomes that are spread all over the cells and in the second and third you can find endosomes and lysosomes more concetrated around the nucleus. \n    * Especially **in the red channel we can see morphological differences**. It looks like if the cells are of different types. This is just an assumption but perhaps one could use the red channel information to reveal cell types. ","338fa45e":"## Latest important updates\n\nBeside building competition code I'm still updating this kernel with ideas and code. To make it easier for you to checkout new content, here is an **update summary** of the latest changes:\n\n* Ideas how to solve jiggle-wiggle losses and gradients: dropout\n* DataGenerator: The **multithreaded version to perform predictions with DataGenerator was dropped**. I've tried out various different methods the last days (using ImageGenerator, flow_form_dataframe etc.) but every time there were some annoying problems. As training is the most expensive part, I decided to keep multithreading for fitting but to drop if for making predictions. \n* **Bug-fix** in modelparameter class. **Shuffle was always set to True**. Now default is False and you can change it interactively. \n* **Modelparameter class** has now an **attribute basepath** that points to your desired image directory. If you perform a prediction with same modelparameters, the path is overwritten during prediction with predict_path. Feels dirty, this will be changed soon. \n* The **image preprocessor is now responsible for loading the images**. With default model parameter n_channels=1 it only returns the green image. If you like to change that increase n_channels as you like or do it by hand but make sure than modelparameters has a suitable n_channels. \n* The **f1 score** of the improved metric is extended by **min, max, std** besides the mean to gain more insights of the f1 score distribution between different target classes. \n* Some ideas what causes the gradient noise and how to set up solutions. ","a0351626":"Ah, ok, great! It seems that for one image id, there are different color channels present. Looking into the data description of this competition we can find that:\n\n* Each image is actually splitted into 4 different image files. \n* These 4 files correspond to 4 different filter:\n    * a **green** filter for the **target protein structure** of interest\n    * **blue** landmark filter for the **nucleus**\n    * **red** landmark filter for **microtubules**\n    * **yellow** landmark filter for the **endoplasmatic reticulum**\n* Each image is of size 512 x 512","181e61c1":"### Add scoring metrics\n\nWe have already seen that the accuracy score is an illusion and does not help to figure out how good our predictions are. Let's take a closer look to the competition scoring and alternatives:\n\n* **F1 macro score**: Check out this [nice implementation of Guglielmo Camporese](https:\/\/www.kaggle.com\/guglielmocamporese\/macro-f1-score-keras). Thank you very much! We can easily add it to our model.\n* But even with that score we should be careful! We have 28 different classes that are **very different in their frequency of being present**.  In addition we have to deal with **highly imbalanced classes per single target**. Even for the most common target nucleoplasm there are only 40 % of samples that show it and 60 % not. This imbalance becomes even more dramatic for seldom targets like rods and rings. We should **attach more importance to true positives**. \n* Well, there is **one problem with the competition score** that we can use to measure the performance of our model: **The f1 mean**. The mean is not robust towards outliers and consequently not very informative to understand the distribution of f1 scores for each target class. Perhaps we will make nice predictions for Cytosol but bad ones for Nucleoplasmn... who knows? Consequently it could be nice to introduce some further statistical quantities like **min** as well as **max** and the **standard deviation** . This way we can see the worst and gain some insights how the scores are spread over the classes. ","0219cbba":"### Take-Away\n\n* We can see that most common protein structures belong to coarse grained cellular components like the plasma membrane, the cytosol and the nucleus. \n* In contrast small components like the lipid droplets, peroxisomes, endosomes, lysosomes, microtubule ends, rods and rings are very seldom in our train data. For these classes the prediction will be very difficult as we have only a few examples that may not cover all variabilities and as our model probably will be confused during ins learning process by the major classes. Due to this confusion we will make less accurate predictions on the minor classes.\n* Consequently accuracy is not the right score here to measure your performance and validation strategy should be very fine. ","55cd26f5":"### Nuclear speckles","ba67befa":"### Track losses and scores\n\nOk, after adding the metrics we like to observe we should try to obtain more insights into the learning process of our model. One question on my mind draws circles: **What happened to the loss after each batch during one epoch?** Does it converge? Has our model started to learn or does nothing happen? Currently we obtain a history after calling fit_generator, but this history only contains the loss of train and validation data after one epoch. It does not contain losses that are obtained after computing each batch. But wait a minute... **How does our model update gradients? After each batch? After one epoch?** \n\nIf we take a look at simple feedforward networks that are close related to CNNs add gradient descent, we can see that learning means to compute the derivatives of the loss with respect to the weights over all samples:\n\n$$ w_{i,j}^{new} = w_{i,j}^{old} - \\eta \\cdot \\partial_{w_{i,j}} E$$\n\nWith a set of independent observation samples, we can obtain the gradients this way:\n\n$$ \\partial_{w_{i,j}} E = \\sum_{n=1}^{N} \\partial_{w_{i,j}} E_{n}$$\n","0ac54e87":"Ok, now let's increase the number of epochs and decrease the batch_size. This way we use more weight update steps and hopefully makes our model learn more than before: ","ac20b24d":"### Take-Away\n\n* Our baseline model seemed to learn something even if this something does not look very nice. \n* Taking a look at the standard deviation we can see that all samples have nearly the same predicted values. There is no deviation, no difference between them. This is of course very bad! :-(\n\nLet's go one step deeper and take a look at the Cytosol (choose another feature if you like ;-)). Here we can see a higher standard deviation than for all other samples and perhaps its corresponding distribution starts to diverge, trying to get bimodal. This would be great at it indicates that the model starts solving the problem of binary classification for this target:","3ee0dba9":"## Where to go next?\n\n* optimization objective alias loss choice\n* hyperparameter search","533409f4":"Let's check if the number of files divided by 4 yields the number of target samples:","9857a336":"## Loading packages and data","9dd5c3b2":"### Peek into the directory\n\nBefore we start loading images, let's have a look into the train directory to get an impression of what we can find there:","338850d6":"## Helper code","6035a956":"## How are special and seldom targets grouped?"}}