{"cell_type":{"e84a4977":"code","36b61878":"code","4fcde39d":"code","b6bab394":"code","20e46a12":"code","caa4d46a":"code","52ff32a0":"code","a6cfb708":"code","7701466c":"code","81342e4c":"code","09b69de0":"code","30abd3ee":"code","245ceba5":"code","761b680f":"code","0c606d95":"markdown","f9c5e399":"markdown","2a6773eb":"markdown","46089f07":"markdown","3372ea34":"markdown","e23c6b2e":"markdown","a1e9e133":"markdown","b132e53d":"markdown","c3d71e1a":"markdown","a83933b4":"markdown","c360867e":"markdown","e3ebaf71":"markdown","eb631043":"markdown","9c022fd3":"markdown","b56bef40":"markdown","5932ac67":"markdown","acb5cf3f":"markdown","f3d94f86":"markdown","54d1fd84":"markdown","353a1900":"markdown","ef3e5bfc":"markdown"},"source":{"e84a4977":"import pandas as pd\nimport numpy as np\nfrom numpy.random import RandomState\nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#CLASSIFIERS FOR TRAINING\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression","36b61878":"data = pd.read_csv(\"..\/input\/fetal-health-classification\/fetal_health.csv\") \nprint(\"\u2666 LOOK AND CHECK DATA:\")\nprint(data.head())\nprint()\nprint(\"\u2666 DATASET LENGTH = \", len(data))","4fcde39d":"cols = data.columns\nprint(cols)","b6bab394":"mean = data.mean(axis=0)\nprint(mean)","20e46a12":"median = data.median(axis=0)\nprint(median)","caa4d46a":"mode = data.mode(axis=0)\nprint(mode)","52ff32a0":"std = data.std(axis=0)\nprint(std)","a6cfb708":"print(data.corrwith(data[\"fetal_health\"]))","7701466c":"#STEP-1\nplt.figure(figsize=(18,5))\nplt.title('FETAL HEALTH CLASSES')\nplt.xlabel('Fetal health class')\nplt.ylabel('count')\n\n#STEP-2\nvalue_counts = data[\"fetal_health\"].value_counts()\nprint(value_counts) \n\n#STEP-3\nvalue_counts.plot.bar()\n\n#STEP-4\nplt.grid()\nplt.show()","81342e4c":"#STEP-1\nscaler = MinMaxScaler(feature_range=(0, 1))\n#STEP-2\ntemp = data[\"fetal_health\"]\n#STEP-3\nnorm_data = scaler.fit_transform(data)\n#STEP-4\ndata = pd.DataFrame(data=norm_data, columns=cols)\n#STEP-5\ndata[\"fetal_health\"] = temp\n#STEP-6\nprint(data.head())","09b69de0":"#STEP-1\nrng = RandomState()\n\n#STEP-2\ntrain = data.sample(frac=0.7, random_state=rng)\nval = data.loc[~data.index.isin(train.index)]\n\n#STEP-3\ntrain.reset_index(drop=True, inplace=True)\nval.reset_index(drop=True, inplace=True)\n\n#STEP-4\nprint(\"\u2666 TRAIN SET:\")\nprint(train.head())\nprint()\nprint(\"\u2666 VALIDATION SET:\")\nprint(val.head())","30abd3ee":"#STEP-1\nx_columns = cols[:-1]\ny_column = cols[-1]\n\n#STEP-2\nx_raw_train = train[x_columns]\ny_raw_train = train[y_column]\n\n#STEP-3\nX_train = x_raw_train.copy()\nY_train = y_raw_train.copy()\n\n#STEP-4\nprint(\"\u2666 X_TRAIN: \")\nprint(X_train.head())\nprint()\nprint(\"\u2666 Y_TRAIN: \")\nprint(Y_train.head())\nprint()\n\n#STEP-5\nx_raw_val = val[x_columns]\ny_raw_val = val[y_column]\n\n#STEP-6\nX_val = x_raw_val.copy()\nY_val = y_raw_val.copy()\n\n#STEP-7\nprint(\"\u2666 X_VAL: \")\nprint(X_val.head())\nprint()\nprint(\"\u2666 Y_VAL: \")\nprint(Y_val.head())","245ceba5":"#STEP-1\nall_classifers = [\n    KNeighborsClassifier(3),\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()\n]\n\n#STEP-2\nall_acc = {}\n\n#STEP-3\nfor classifier in all_classifers:\n    #STEP-4\n    model = classifier\n    model.fit(X_train, Y_train)\n    #STEP-5\n    model_pred = model.predict(X_val)\n    model_acc = accuracy_score(Y_val, model_pred)\n    #STEP-6\n    classfier_name = classifier.__class__.__name__\n    #STEP-7\n    all_acc[classfier_name] = model_acc\n    #STEP-8\n    filename = classfier_name+'_model.pickle'\n    pickle.dump(model, open(filename, 'wb'))  \n    #STEP-9\n    loaded_model = pickle.load(open(filename, 'rb'))\n    result = loaded_model.score(X_val, Y_val)     \n    #STEP-10\n    print(\"\u2666 {:<30} = {:<12} {:>25} = {:>12}\".format(classfier_name, model_acc, 'loaded pickle model', result))","761b680f":"#STEP-1\nall_acc = dict(sorted(all_acc.items(), key=lambda item: item[1], reverse=True))\n\n#STEP-2\nkeys = all_acc.keys()\nvalues = all_acc.values()\n\n#STEP-3\nplt.figure(figsize=(10,5))\nplt.title('ACCURCY OF CLASSIFIERS')\nplt.xlabel('classifiers')\nplt.ylabel('accuracy')\nplt.bar(keys, values, color=\"g\")\n\n#STEP-4\nplt.xticks(rotation=90)\n\n#STEP-5\nplt.grid()\nplt.show()","0c606d95":"# 9. CLASSIFIERS INFO\nHere we will use learn a little bit about built-in sklearn classifiers for our dataset. Below you can see a little description about them. Also, I added some interesting articles. You can read them for easier understanding of classfiers' work. \n\n# 9.1. k nearest neighbors\nK-Nearest Neighbors, or KNN for short, is one of the simplest machine learning algorithms and is used in a wide array of institutions. **Sourses-to-read:** [article-1 with illustrations](https:\/\/towardsdatascience.com\/k-nearest-neighbor-python-2fccc47d2a55), [arcticle-2](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm).\n# 9.2. support vector machine  \nSupport vector machines are a set of supervised learning methods used for classification, regression, and outliers detection. **Sourses-to-read:** [article-1](https:\/\/www.freecodecamp.org\/news\/svm-machine-learning-tutorial-what-is-the-support-vector-machine-algorithm-explained-with-code-examples\/), [article-2](https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/), [article-3](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine).\n# 9.3. decision tree\nDecision Tree Analysis is a general, predictive modelling tool that has applications spanning a number of different areas. **Sourses-to-read:** [article-1](https:\/\/www.geeksforgeeks.org\/decision-tree-introduction-example\/), [arcicle-2](https:\/\/www.hackerearth.com\/practice\/machine-learning\/machine-learning-algorithms\/ml-decision-tree\/tutorial\/), [article-3](https:\/\/www.edureka.co\/blog\/decision-trees\/).\n# 9.4. random forest\nRandom forest classifier creates a set of decision trees from randomly selected subset of training set. It then aggregates the votes from different decision trees to decide the final class of the test object. **Sourses-to-read:** [article-1](https:\/\/medium.com\/machine-learning-101\/chapter-5-random-forest-classifier-56dc7425c3e1), [arcicle-2](https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2), [article-3](https:\/\/en.wikipedia.org\/wiki\/Random_forest).\n# 9.5. adaboost\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. **Sourses-to-read:** [article-1](https:\/\/towardsdatascience.com\/understanding-adaboost-2f94f22d5bfe), [arcicle-2](https:\/\/en.wikipedia.org\/wiki\/AdaBoost).\n# 9.6. gradient boosting\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. **Sourses-to-read:** [article-1](https:\/\/stackabuse.com\/gradient-boosting-classifiers-in-python-with-scikit-learn\/), [arcicle-2](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting).\n# 9.7. gaussian naive bayes\nCan perform online updates to model parameters via partial_fit. **Sourse-to-read:** [article-1](http:\/\/i.stanford.edu\/pub\/cstr\/reports\/cs\/tr\/79\/773\/CS-TR-79-773.pdf).\n# 9.8. linear discriminant analysis\nA classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the transform method. **Sourse-to-read:** [article-1](https:\/\/scikit-learn.org\/stable\/modules\/lda_qda.html).\n# 9.9. quadratic discriminant analysis\nA classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class. **Sourse-to-read:** [article-1](https:\/\/scikit-learn.org\/stable\/modules\/lda_qda.html).\n# 9.10. logistic regression\nLogistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross-entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018newton-cg\u2019 solvers.) **Sourse-to-read:** [article-1](https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_iris_logistic.html).","f9c5e399":"# 2. OTHER LIBRARIES\nThen, let think, which other libraries we should use to make our project right:\n1. [Pandas](https:\/\/pandas.pydata.org\/) - for data manipulation and analysis.\n2. [NumPy](https:\/\/numpy.org\/) - for matrices and arrays, has a lot of math functions.\n3. [Matplotlib](https:\/\/matplotlib.org\/) - for visualizations, like plots.\n4. [Pickle](https:\/\/docs.python.org\/3\/library\/pickle.html) - this one is used or serializing and de-serializing a Python object structure. We will save trained model with pickle.","2a6773eb":"# 12. CONCLUSION\nThank you so much for reading my new article! I hope, it was interesting and you liked it! \n\nHere you can read my previous Kaggle Notebook: [Retail Trade Report Department Stores (LSTM)](https:\/\/www.kaggle.com\/maricinnamon\/retail-trade-report-department-stores-lstm).","46089f07":"# 6. NORMALIZE DATA\n1. It is necessary to normalize data before making some classification tasks. Here we will normalize it in a range from 0 to 1, it is one of the most popular normalization intervals. So, we will initialize it.\n2. Output column **\"fetal_health\"** is the last column and it shouldn't be normalized, because it has target values. So, we will remember and save this all column values in temporary variable.\n3. As the result, we will get data numpy array, which will be normalized. \n4. But, for our future work it is easier to store data in the Pandas dataframe, so, we will convert normalized numpy array **norm_data** back to dataframe. The columns in our dataframe will be the same as we had earlier, their names were stored in a variable **cols** (see \"3. Read Data\")\n5. Then, we should change our normalized column **\"fetal_health\"** with non-normalized colum **\"fetal_health\"**, saved in temporary variable (in the step 2).\n6. Also, we can check and look our results.","3372ea34":"# 8. FINAL PREPARATIONS\nSo, here we will make some last steps before making and training our model.\n1. We know that the last column in our target output column (Y) and all columns before are input columns (X). So, we can store in variables their names in order to have accses to them in future.\n2. So, we should remember in variables our columns with input variables and column with output variable. We work with our train set, because it is preparations are made for training.\n3. For safety, we will copy them to new variables.\n4. Look and check X and Y train values.\n5. We should repeat step 2 for validation set, because in future we should check our model performance on this set.\n6. We should repeat step 3 for validation set.\n7. We should repeat step 4 for validation set.","e23c6b2e":"So, as we can see, the first one was the best in this task!","a1e9e133":"After that, we can print all our dataset columns for future use: ","b132e53d":"# 4.3. mode\nHere we will use **mode()** Pandas function.","c3d71e1a":"# 3. READ DATA\nSo, now, we can get data from our dataset, using Pandas fuction **read_csv()**, because our data was in .csv format. Function **head()** returns top 5 records from your dataset, here it is used just to check that we read our data correctly. ","a83933b4":"# 7. SPLIT DATASET INTO TRAIN AND TEST DATA\nSo, here we will divide our dataset into two parts (for model training and validation). Let it will be 70%:30% respectively. You can also try it with another ratio, like 60%:40%, 80%:20%, 90%:10% and so on. \n1. Firlstly, we should initalize our random variable. It will be used to generate random train samples after each code run. \n2. As I said earier, train set will consist of 70% of initial dataset. We will use **sample()** function. [Here](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.sample.html) you can read more about it.\n3. As the result, we will have 2 dataframes (train and validation), but records in them will have old ID indexes, as they had in the initial dataset. To reset indexes in train and val sets we will use **reset_index()** function. \n4. Then, we can check and look results.","c360867e":"# 4.1. mean\nIn the code below we used **mean()** Pandas function with axis=0 parameter. It is the the axis to iterate over while searching.\nIt means that you want to find mean across all your indexes (in our case indexes are the names of the colunms). So, it finds statistical data from up to down, via all rows for a column.\nIf you will write axis=1 you will find statistical data from left to right, via all columns, for a row.\n![O5hBF.jpg](attachment:O5hBF.jpg)","e3ebaf71":"# 10. TRAIN & VALIDATE, SAVE & LOAD MODELS\nSo, this is our important part of the article. Here we will implement our main stage of our article.\n1. As we going to to same operations for each classfier (learn and predict, save and load model), it will be eassier to run a loop for all these classfiers. So, they will be in our list.\n2. Also, we should have a special place, were we will be store information about classifer and its accuracy for future work with it. I think, that it will be easier to have a dictionary, where we will have **\"key:value\"** as **\"classifier:accuracy\"**.\n3. So, here we go into the loop for each classifier that we have.\n4. Then we get our classifier **fit()** function adjusts weights according to data values so that better accuracy can be achieved. It is our learning stage.\n5. In this step make prediction about our trained model with **predict()**, for this task we use validation X set, and also we should know the accuracy of our learned model with **accuracy_score()**, and we count accuracy using validation set.\n6. Here we will get the name of classifier, because it will be our dictionary key, so we need it.\n7. Also, we should save accuracy results to our dictionary, made in the step-2.\n8. For future use, we can easily save our model, using **pickle**.\n9. In this step we can load our model and see its score with **score()** function. \n10. We can check and look the accurracy of the trained model. So, we print it to the screen. Also, we can print the accuracy of loaded model. And it will same as it was found in the step-5. For better understanding, we will print it it the fashionable format: {:<30} means left-aligned with width 30. Also, {:>12} means right-aligned with width 12. ","eb631043":"#  1. SKLEARN PYTHON LIBRARY\n[This](https:\/\/scikit-learn.org\/stable\/) module has simple and efficient tools for predictive data analysis. It has some built-in classifiers which we will use to make predictions. \nMoreover, [here](https:\/\/scikit-learn.org\/stable\/supervised_learning.html#supervised-learning) you can find some useful information about classification methods available in this library.","9c022fd3":"# 4.2. median\nHere we will use **median()** Pandas function.","b56bef40":"# 4.4. standart deviation\nHere we will use **std()** Pandas function.","5932ac67":"So, as we can see, we have 22 columns here (21 columns are our input data and the last one column will be used as prediction column). Also, it has 2126 rows, it is 2126 measurements extracted from cardiotocograms and classified by expert obstetricians into 3 categories:\n1. Normal\n2. Suspect\n3. Pathological","acb5cf3f":"# 4. ANALYZE DATA\nOne of the most important things is to understand data which you work with. Here we will use some well-known methods for easier understanding of our data.\n\nFor all dataset columns we will find some statistical information, like: Mean, Median, Mode (a.k.a [The Three M's of Statistics](https:\/\/www.dummies.com\/education\/math\/pre-algebra\/the-three-types-of-average-median-mode-and-mean\/)), [Standard Deviation](https:\/\/en.wikipedia.org\/wiki\/Standard_deviation) and [Correlation](https:\/\/www.bmj.com\/about-bmj\/resources-readers\/publications\/statistics-square-one\/11-correlation-and-regression) using Pandas functions.","f3d94f86":"# 4.5. correlation\nPandas **corrwith()** is [used](https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-corrwith\/) to compute pairwise correlation between rows or columns of two DataFrame objects.\n","54d1fd84":"# 5. VISUALIZE DATA\nFor easier understanding data we can build some plots. In this example, we will check how many records we have in each class. For this task we can use built-in plots to Pandas library. [Here](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.plot.html) you can read more about them.\n\n1. Firstly, we set the size of our picture for our consideration, set the title of the plot and name the OX and OY axises. \n2. Then, we should count total sum of records for each class, in our case, classes are named as 1.0, 2.0, 3.0 via **value_counts()**. For exact counts for each class we can easily print our result to the screen.\n3. Build a bar chart for these classes.\n4. Turn on the grid via **grid()** and show our plot via **show()**","353a1900":"# FETAL HEALTH CLASSIFICATION USING SKLEARN\nHello, everyone! In this my second public notebook we will learn how to use **sklearn python library** to make some classification tasks. We will explore some well-known classificators to make predictions. You will learn some basic classificators which you can find in this library and which you will be able to use in your future studies. \n\nIn this case, we will use dataset [\"Fetal Health Classification\"](https:\/\/www.kaggle.com\/andrewmvd\/fetal-health-classification) as an example. It can be used to classify fetal health in order to prevent child and maternal mortality. \n\n**This article has a lot of explanations, so it will be very helpful for beginners.**\n\n\n# !!!DISCLAIMER!!!\nThis kaggle article is made only in educational purposes. It can't be used as only one right and sure way to check mother and child health problems. If you have some questions, please, visit your health doctor in order to get medical treatment!\n\n\n\nSo, let's start!","ef3e5bfc":"# 11. VIZUALIZE PERFORMANCE RESULTS\nIn the previous stage we have found all accuracies for all trained models. Now it is time to think, which is the best one, which has the highest accuracy. For better understanding we can build a bar chart, sorted in descending order.\n1. Firstly, we should rewrite our dictionary in descending order.\n2. Also, we can find keys and values from the dictionary.\n3. We set the figure size, name OX  and OY, set the title for our plot. I decided to make green bar chart, so the color is \"g\", \"g\" stands for green.\n4. Names of all classifiers are too long. We can rotate them. They will be laying vertically.\n5. We set the grid and build the plot."}}