{"cell_type":{"d960e2d8":"code","14713e7d":"code","18e13f5f":"code","0416bcf9":"code","ebeb897a":"code","d49909f1":"code","77b6a727":"code","e03e7604":"code","6f0c8bc1":"code","6bc5ab50":"code","636db659":"code","7cf8e0e2":"code","d482388d":"code","a405d2f9":"code","ffd61c67":"code","f113ce49":"code","4aa8dc05":"code","8bde4ffb":"code","42d9b4ec":"code","250ea1a6":"code","12e00e76":"markdown","ea404f61":"markdown","81324580":"markdown","31c3d113":"markdown","697a287c":"markdown","10c52866":"markdown","27789372":"markdown","90501341":"markdown","5901b609":"markdown","f484e82e":"markdown","f9733366":"markdown","7c13cf55":"markdown","c417367b":"markdown","30ede16a":"markdown","dc2b34fc":"markdown","e5c6b03f":"markdown","299e3b8f":"markdown","4bdb269e":"markdown","2ef08367":"markdown","a92a9cf5":"markdown","519f4621":"markdown","0fbb137b":"markdown","5d33ae77":"markdown","2f0ef65f":"markdown","3319407f":"markdown","f96138ca":"markdown","7a2addf0":"markdown","472988a7":"markdown","10a9e89e":"markdown","5068f41c":"markdown","f33f277d":"markdown","867ff36f":"markdown"},"source":{"d960e2d8":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","14713e7d":"VERBOSE=1","18e13f5f":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","0416bcf9":"rf_clf = RandomForestClassifier(verbose=VERBOSE,\n                                random_state=RANDOM_STATE,\n                                n_jobs=N_JOBS)","ebeb897a":"parameters = {\n    'n_estimators': [200, 300, 400, 500, 550, 600, 650, 700, 750, 800, 900]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","d49909f1":"parameters = {\n    'criterion': ['gini', 'entropy']\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","77b6a727":"parameters = {\n    'max_depth': [10, 20, 25, 30, 35, 40, 50, 60, None]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","e03e7604":"parameters = {\n    'min_samples_split': [2, 3, 4, 5, 8, 13, 21, 34, 55, 89]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","6f0c8bc1":"parameters = {\n    'min_samples_leaf': [1, 2, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","6bc5ab50":"parameters = {\n    'min_weight_fraction_leaf': [x \/ 10 for x in range(0, 6)]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","636db659":"parameters = {\n    'max_features': ['auto', 'sqrt', 'log2', 2, 5, 8, 10, 13, 21, 34, None]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","7cf8e0e2":"parameters = {\n    'max_leaf_nodes': [2, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, None]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","d482388d":"parameters = {\n    'min_impurity_decrease': [x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","a405d2f9":"parameters = {\n    'bootstrap': [True, False]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","ffd61c67":"parameters = {\n    'oob_score': [True, False]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","f113ce49":"parameters = {\n    'warm_start': [True, False]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","4aa8dc05":"parameters = {\n    'class_weight': ['balanced', 'balanced_subsample', None]\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\n# plot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","8bde4ffb":"parameters = {\n    'n_estimators': [650, 700],\n    'max_depth': [30, None],\n    'max_features': [13, 21, 34, None],\n    'bootstrap': [True, False],\n}\nclf = GridSearchCV(rf_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","42d9b4ec":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","250ea1a6":"clf.best_estimator_","12e00e76":"These four parameters are better at some value different as its default value.\nThe number of estimators turns out to be better 700 than 10, `max_depth` 30 than None, `max_features` 21 than None, and `bootstrap` False than True.","ea404f61":"# Search over parameters","81324580":"# warm_start\n##### : bool, optional (default=False)\n\nWhen set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.","31c3d113":"# max_leaf_nodes\n##### : int or None, optional (default=None)\n\nGrow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.","697a287c":"The optimal value is near 700 estimators. Until this value, there is a progressive increase, with the exception of an abnormally low value at 600 estimators. After 700 estimators, it plummets. ","10c52866":"As all the categories has the same number of samples,\n`balanced` and `None` options has the same result.","27789372":"The best score is for the default value.","90501341":"It has the same score.","5901b609":"# Introduction\n\nThe aim of this notebook is to optimize the Random Forest model.\n\nFirst, all [Random forest classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","f484e82e":"# min_samples_split\n##### : int, float, optional (default=2)\n\nThe minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.\n","f9733366":"# Prepare data","7c13cf55":"## Export grid search results","c417367b":"The better score is for `max_features` 13.\nIt's a local maximum, before and after, the scores decreases.","30ede16a":"Values higher than 20 have a good score.\nThe value of 30 has an outstanding good score (more than None).","dc2b34fc":"# bootstrap\n##### : boolean, optional (default=True)\n\nWhether bootstrap samples are used when building trees. If False, the\nwhole datset is used to build each tree.\n","e5c6b03f":"More leaf nodes, more score. None has unlimited leaf nodes and has the maximum score.","299e3b8f":"# n_estimators\n##### : integer, optional (default=10)\n\nThe number of trees in the forest.","4bdb269e":"Forcing not to split when there is a low number of samples, makes the score descent.\nAnd maybe could avoid overfit.","2ef08367":"# criterion\n##### : string, optional (default=\"gini\")\n\nThe function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"entropy\" for the information gain.\nNote: this parameter is tree-specific.","a92a9cf5":"# max_features\n##### : int, float, string or None, optional (default=\"auto\")\n\nThe number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `int(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.","519f4621":"# min_weight_fraction_leaf\n##### : float, optional (default=0.)\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.","0fbb137b":"# class_weight\n##### : dict, list of dicts, \"balanced\", \"balanced_subsample\" or None, optional (default=None)\n\nWeights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples \/ (n_classes * np.bincount(y))``\n\nThe \"balanced_subsample\" mode is the same as \"balanced\" except that\nweights are computed based on the bootstrap sample for every tree\ngrown.\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.","5d33ae77":"The best score is with the default value.","2f0ef65f":"The `gini` value offers a slightly better result.","3319407f":"# min_samples_leaf\n###### : int, float, optional (default=1)\n\nThe minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.","f96138ca":"# Exhaustive search","7a2addf0":"The best score is with the default value.","472988a7":"Preventing to have leafs with low number of samples, makes the score descent.\nAnd maybe could avoid overfit.","10a9e89e":"The score is the same. The fit time is greater when it's true. The score is less than the score with bootstrap equal to true.","5068f41c":"# min_impurity_decrease\n##### : float, optional (default=0.)\n\nA node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity\n                        - N_t_L \/ N_t * left_impurity)\n                        \nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.","f33f277d":" # max_depth\n ##### : integer or None, optional (default=None)\n \nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.","867ff36f":"# oob_score\n##### : bool (default=False)\n  \nWhether to use out-of-bag samples to estimate\nthe generalization accuracy."}}