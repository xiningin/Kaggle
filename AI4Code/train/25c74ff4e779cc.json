{"cell_type":{"02059fe0":"code","cb59585c":"code","c644cffe":"code","5ce74c78":"code","69c6baec":"code","0d28dbcf":"code","71eda781":"code","54824f94":"code","d929366d":"code","02ba5be0":"code","1807df02":"code","c0872e91":"code","fefa2f40":"code","5f8fd8af":"code","1196296d":"code","061c6851":"code","531865e7":"code","d7d601e8":"code","822c1142":"code","7349ad02":"code","9cc00760":"code","c0290670":"code","763dc3b0":"markdown","97664029":"markdown","2c87a6d4":"markdown","17162a81":"markdown","50d14c2b":"markdown","36540acc":"markdown","9aa31e9a":"markdown","26416093":"markdown","37553868":"markdown","e9dede73":"markdown","1f65e572":"markdown","b24795b1":"markdown","31667512":"markdown","b97950e7":"markdown","25ab7b92":"markdown","a7c8fa59":"markdown","ac5c4c91":"markdown","ad114432":"markdown","ae34077b":"markdown","4e95c704":"markdown"},"source":{"02059fe0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import classification_report, f1_score, plot_roc_curve","cb59585c":"adult = pd.read_csv('..\/input\/adult-census-income\/adult.csv')\nadult.sample()","c644cffe":"adult.info()","5ce74c78":"binary_encoder_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'NC', missing_values = '?')),\n    ('binary', ce.BinaryEncoder())\n])\n\ntransformer = ColumnTransformer([\n    ('one hot', OneHotEncoder(drop = 'first'), ['relationship', 'race', 'sex']),\n    ('binary', binary_encoder_pipe, ['workclass', 'marital.status', 'occupation', 'native.country'])],\n    remainder = 'passthrough')","69c6baec":"adult['income'].value_counts()","0d28dbcf":"X = adult.drop(['fnlwgt', 'education', 'income'], axis = 1)\ny = np.where(adult['income'] == '>50K', 1, 0)","71eda781":"X.shape","54824f94":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,\n                                                    test_size = 0.3, random_state = 1212)","d929366d":"adaboost = AdaBoostClassifier(DecisionTreeClassifier(), random_state = 1212)\npipe_ada = Pipeline([\n    ('transformer', transformer),\n    ('adaboost', adaboost)])\n\ngradboost = GradientBoostingClassifier(random_state = 1212)\npipe_grad = Pipeline([\n    ('transformer', transformer),\n    ('gradboost', gradboost)])\n\nXGBOOST = XGBClassifier(random_state = 1212)\npipe_XGB = Pipeline([\n    ('transformer', transformer),\n    ('XGBOOST', XGBOOST)])","02ba5be0":"def model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\npipe_ada_cv = model_evaluation(pipe_ada, 'f1')\npipe_grad_cv = model_evaluation(pipe_grad, 'f1')\npipe_XGB_cv = model_evaluation(pipe_XGB, 'f1')","1807df02":"for model in [pipe_ada, pipe_grad, pipe_XGB]:\n    model.fit(X_train, y_train)","c0872e91":"score_mean = [pipe_ada_cv.mean(), pipe_grad_cv.mean(), pipe_XGB_cv.mean()]\nscore_std = [pipe_ada_cv.std(), pipe_grad_cv.std(), pipe_XGB_cv.std()]\nscore_f1 = [f1_score(y_test, pipe_ada.predict(X_test)),\n            f1_score(y_test, pipe_grad.predict(X_test)), \n            f1_score(y_test, pipe_XGB.predict(X_test))]\nmethod_name = ['Ada Boost Classifier', 'Gradient Boost Classifier ',\n              'XGB Classifier']\nsummary = pd.DataFrame({'method': method_name, 'mean score': score_mean,\n                        'std score': score_std, 'f1 score': score_f1})\nsummary","fefa2f40":"plot_roc_curve(pipe_XGB, X_test, y_test)","5f8fd8af":"features = list(pipe_ada[0].transformers_[0][1].get_feature_names()) + pipe_ada[0].transformers_[1][1][1].get_feature_names() + ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']","1196296d":"imptab_ada = pd.DataFrame(pipe_ada[1].feature_importances_, columns = ['imp'], index = features)\nimptab_ada.sort_values('imp').plot(kind = 'barh', figsize = (15,8))\nplt.title('Importance Table For Ada Boost Classifier Model')\nplt.show()","061c6851":"imptab_grad = pd.DataFrame(pipe_grad[1].feature_importances_, columns = ['imp'], index = features)\nimptab_grad.sort_values('imp').plot(kind = 'barh', figsize = (15,8))\nplt.title('Importance Table For Gradient Boost Classifier Model')\nplt.show()","531865e7":"imptab_XGB = pd.DataFrame(pipe_XGB[1].feature_importances_, columns = ['imp'], index = features)\nimptab_XGB.sort_values('imp').plot(kind = 'barh', figsize = (15,8))\nplt.title('Importance Table For XGB Classifier Model')\nplt.show()","d7d601e8":"XGBOOST = XGBClassifier(random_state = 1212)\nestimator = Pipeline([('transformer', transformer), ('XGBOOST', XGBOOST)])\n\nhyperparam_space = {\n    'XGBOOST__learning_rate': [0.1, 0.05, 0.01, 0.005],\n    'XGBOOST__n_estimators': [50, 100, 150, 200],\n    'XGBOOST__max_depth': [3, 5, 7, 9]\n}\n\nrandom = RandomizedSearchCV(\n                estimator,\n                param_distributions = hyperparam_space,\n                cv = StratifiedKFold(n_splits = 5),\n                scoring = 'f1',\n                n_iter = 10,\n                n_jobs = -1)\n\nrandom.fit(X_train, y_train)","822c1142":"print('best score', random.best_score_)\nprint('best param', random.best_params_)","7349ad02":"estimator.fit(X_train, y_train)\ny_pred_estimator = estimator.predict(X_test)\nprint(classification_report(y_test, y_pred_estimator))","9cc00760":"random.best_estimator_.fit(X_train, y_train)\ny_pred_random = random.best_estimator_.predict(X_test)\nprint(classification_report(y_test, y_pred_random))","c0290670":"score_list = [f1_score(y_test, y_pred_estimator), f1_score(y_test, y_pred_random)]\nmethod_name = ['XGB Classifier Before Tuning', 'XGB Classifier After Tuning']\nbest_summary = pd.DataFrame({\n    'method': method_name,\n    'f1 score': score_list\n})\nbest_summary","763dc3b0":"After all, HyperParameter Tuning doesn't work good in this data. So if I have to choose, I pick the **XGB Classifier score Before Tuning, which is 0.71**. I know the number isn't good enough either because the data is imbalance and I don't process any resampling on it.","97664029":"# Cross Validation","2c87a6d4":"*Summary*","17162a81":"*Fitting Data*","50d14c2b":"*Model Evaluation*","36540acc":"# Define Model","9aa31e9a":"*Handling Missing Value In Pipeline*","26416093":"Income is the target data and **indicated with imbalance data**. I define **income with 1 if income is >50K and 0 if income is <50K**.","37553868":"From Importance Features Table, the **XGB Classifier can boost almost all the features**. It's has a consistency with the cross validation result. Now, see if the HyperParameter Tuning process can boost until getting the maximum score.","e9dede73":"After HyperParameter Tuning, the best score is 0.6996, which getting lower. N estimator is 150, Max depth is 5, and Learning rate is 0.1. Let's compare the result.","1f65e572":"From these scores, **XGB Classifier is the best one** with the highest f1 score and mean score, also the lowest std score. Let's cross-check with the important features, see if the model is correct.","b24795b1":"*Preprocessing scheme:*\n* Encode all columns\n* Drop education because it's already encoded on education.num\n* Drop fnlwgt because it's unique","31667512":"# Before VS After Tuning Comparison","b97950e7":"# Importance Features","25ab7b92":"*Splitting Data*","a7c8fa59":"I use 3 Boosting Algorithms Models:\n* Ada Boost Classifier\n* Gradient Boosting Classifier\n* XGB Classifier","ac5c4c91":"I use 0.3 as default score for test_size and X.shape for random_state so the data will be devided equally.","ad114432":"# HyperParameter Tuning","ae34077b":"# PreProcessing","4e95c704":"*In this info detail, indicate that there is no missing value at all. But if you see the whole data carefully, you will find **missing value with '?'**.*"}}