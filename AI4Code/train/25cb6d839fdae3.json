{"cell_type":{"7e73e12a":"code","b2634e88":"code","10966060":"code","4de8715a":"code","9abf5a15":"code","44b8cee8":"code","83590550":"code","b7e98aa1":"code","b4b9eade":"code","e97d5d5f":"code","831269be":"code","10fdd6b4":"code","9ccf027e":"code","7545fada":"code","1e40c6ec":"code","8eb9afa9":"code","e863495a":"code","490dc884":"code","aabc43dc":"code","a8a4ec31":"code","7f4723a7":"code","a4d6f6b5":"code","ffc3f3d5":"code","d17cbaea":"code","fd694c12":"code","fbd7e5a3":"code","4b0cf312":"code","76ce6587":"code","6a287f40":"markdown","a71d4e5b":"markdown","a75d495a":"markdown","7ea79d56":"markdown","cc41501a":"markdown","69757b06":"markdown","e276a9ed":"markdown","341d2a32":"markdown","f5f57a67":"markdown","781a955e":"markdown","255d4149":"markdown","1338b03d":"markdown","fa6a30fe":"markdown","8b2d2da6":"markdown","3b17b435":"markdown","85a97a5e":"markdown","3fefd6bd":"markdown"},"source":{"7e73e12a":"import nltk\nfrom nltk.probability import FreqDist\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nimport numpy as np\nimport random\nimport string\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import shuffle\nimport re\nimport urllib \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nnltk.download('gutenberg')\nnltk.download('punkt')\nnltk.download('stopwords')","b2634e88":"#generalization of the code\ndef DataPartitioning(booksName, books, partitionsNum, wordNum):\n    partitions=[]\n    labels=[]\n    for i in range(0, len(booksName)):\n    \n        text = books[i]\n        label= booksName[i]\n        #remove \\n and punctuation\n        text= text.replace(\"\\n\",\" \")\n        #text= text.translate(str.maketrans('', '', string.punctuation))\n        text = re.sub(r'[^\\w\\s]', '', text)\n        text= text.lower()\n        \n        #tokenize\n        word_tokens = word_tokenize(text)\n        #remove stop words\n        stop_words = set(stopwords.words('english')) \n        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n        filtered_sentence = []  \n        for w in word_tokens: \n            if w not in stop_words: \n                filtered_sentence.append(w)\n        x=0\n        #print(len(filtered_sentence))\n        #print(filtered_sentence)\n    #getting 200 unique partitions\n        while x < partitionsNum:\n            x=x+1\n            r = random.randint(0,len(filtered_sentence)-wordNum)\n            #getting unique 100 word for 1 partition\n            partition=' '.join(filtered_sentence[r:r+wordNum])\n            if partition in partitions:\n                x=x-1\n            else:\n                partitions.append(partition)\n                labels.append(label)\n                \n    #add the data to dataframe\n    data={ \"Sequence\":partitions,\"Label\":labels}\n    df = pd.DataFrame(data)\n    df = shuffle(df)\n    #print(df)\n    return df","10966060":"#def GetBookText(url):\n#  file = urllib.request.urlopen(url)\n#  text=\"\"\n#  check=False\n#  for line in file:\n#    decoded_line = line.decode(\"utf-8\")\n#    if check:\n#      text+=decoded_line\n#    if check==False and \"GUTENBERG EBOOK\" in decoded_line:\n#      check=True\n#  return text","4de8715a":"def GetBookText(url):\n  file = urllib.request.urlopen(url)\n  text=\"\"\n  StartRead = 0\n  for line in file:\n    decoded_line = line.decode(\"utf-8\")\n    if \"*** START\" in decoded_line:\n      StartRead = 1\n      continue\n    if \"*** END\" in decoded_line:\n      break\n    if(StartRead == 0):\n      continue\n    text+=decoded_line\n    #print(text)\n  return text","9abf5a15":"labels=['Richard Ashby', 'C. M. Kornbluth', 'Tom W. Harris', 'Philip K. Dick',\n'Edmond Hamilton']\nbooks=[]\nbooks.append(GetBookText(\"https:\/\/www.gutenberg.org\/files\/65246\/65246-0.txt\"))\nbooks.append(GetBookText(\"https:\/\/www.gutenberg.org\/cache\/epub\/51233\/pg51233.txt\"))\nbooks.append(GetBookText(\"https:\/\/www.gutenberg.org\/files\/65176\/65176-0.txt\"))\nbooks.append(GetBookText(\"https:\/\/www.gutenberg.org\/cache\/epub\/31516\/pg31516.txt\"))\nbooks.append(GetBookText(\"https:\/\/www.gutenberg.org\/files\/65331\/65331-0.txt\"))\n\n#for i in labels:\n#    books.append(str(nltk.corpus.gutenberg.raw('chesterton-thursday.txt')))\nresult=pd.DataFrame()\n\n#Passing the kabels and the books to our fuction\nresult= DataPartitioning(labels,books,200,100)\nx = result.iloc[:, 0]\ny = result.iloc[:, 1]\n","44b8cee8":"text_clf = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', svm.SVC(kernel='linear', C=1))\n])\n\ntext_clf2 = Pipeline([\n    ('tfidf2', TfidfVectorizer()),\n    ('clf2', DecisionTreeClassifier(random_state=0)),\n])\n\ntext_clf3 = Pipeline([\n    ('tfidf3', TfidfVectorizer()),\n    ('clf3', KNeighborsClassifier(n_neighbors=10)),\n])\n\ntext_clf4 = Pipeline([\n    ('tfidf4', TfidfVectorizer()),\n    ('clf4', GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=10, random_state=0)),\n])","83590550":"le = preprocessing.LabelEncoder()\nle.fit(y)\ny_encoding = le.transform(y)\n\nprint(y_encoding)","b7e98aa1":"\nX_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.33, random_state=0)\n","b4b9eade":"text_clf.fit(X_train, y_train)","e97d5d5f":"testResult= pd.DataFrame()\n\nprint(\"10 Fold Cross Validation Score For SVM \\n\")\nprint(cross_val_score(text_clf, X_train, y_train, cv=10).mean())\nprint(\"\\nTesting Score for SVM\")\nprint(text_clf.score(X_test,y_test))\ntestResult=testResult.append({'testScore': text_clf.score(X_test,y_test) , 'Algorithm': 'SVM'},ignore_index=True)\n","831269be":"text_clf2.fit(X_train, y_train)\n\nprint(\"10 Fold Cross Validation Score for DecisionTree \\n\")\nprint(cross_val_score(text_clf2, X_train, y_train, cv=10).mean())\nprint(\"\\nTesting Score for DecisionTree\")\nprint(text_clf2.score(X_test,y_test))\ntestResult=testResult.append({'testScore': text_clf2.score(X_test,y_test) , 'Algorithm': 'DecisionTree'},ignore_index=True)\n","10fdd6b4":"text_clf3.fit(X_train, y_train)\ntext_clf3.score(X_test,y_test)\nprint(\"10 Fold Cross Validation Score for KNeighbors \\n\")\nprint(cross_val_score(text_clf3, X_train, y_train, cv=10).mean())\nprint(\"\\nTesting Score for KNeighbors\")\nprint(text_clf3.score(X_test,y_test))\ntestResult=testResult.append({'testScore': text_clf3.score(X_test,y_test) , 'Algorithm': 'KNeighbors'},ignore_index=True)\n","9ccf027e":"text_clf4.fit(X_train, y_train)\ntext_clf4.score(X_test,y_test)\nprint(\"10 Fold Cross Validation Score for GradientBoosting \\n\")\nprint(cross_val_score(text_clf4, X_train, y_train, cv=10).mean())\nprint(\"\\nTesting Score for GradientBoosting\")\nprint(text_clf4.score(X_test,y_test))\ntestResult=testResult.append({'testScore': text_clf4.score(X_test,y_test) , 'Algorithm': 'GradientBoosting'},ignore_index=True)\n","7545fada":"testResult","1e40c6ec":"champion_index = testResult[\"testScore\"].idxmax()\nprint(\"The best algorithm is: \" + testResult.iloc[champion_index]['Algorithm']) ","8eb9afa9":"from sklearn import metrics\ndef ErrorAnalysis(x, y,alg):\n    xtrain = x.reset_index(drop = True)\n    ytrain= y.reset_index(drop = True)\n    y_predict=pd.Series(alg.predict(x), name='y_predict')\n    Error_Analysis=pd.concat([xtrain, ytrain,y_predict], axis=1)\n    print(\"Confusion Matrix\\n\")\n    print(metrics.confusion_matrix(Error_Analysis['Label'], Error_Analysis['y_predict']))\n    print(\"\\n\")\n    print(\"Classification Report\\n\")\n    print(metrics.classification_report(Error_Analysis['Label'], Error_Analysis['y_predict']))\n    Error_Analysis['Error']= (Error_Analysis['Label']!=Error_Analysis['y_predict'])\n    error= Error_Analysis.loc[Error_Analysis['Error'] == True]\n    return error","e863495a":"Error = ErrorAnalysis(X_test,y_test,text_clf)\nError","490dc884":"Error = ErrorAnalysis(X_test,y_test,text_clf2)\nError","aabc43dc":"Error = ErrorAnalysis(X_test,y_test,text_clf3)\nError","a8a4ec31":"Error = ErrorAnalysis(X_test,y_test,text_clf4)\nError","7f4723a7":"# Reference: https:\/\/www.kaggle.com\/pamin2222\/tf-idf-svm-exploration\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=y_train, data=X_train)\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Class Count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Classes\", fontsize=15)\nplt.show()","a4d6f6b5":"# Reference: https:\/\/www.kaggle.com\/pamin2222\/tf-idf-svm-exploration\ndef top_tfidf_feats(row, features, top_n=20):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ndef top_feats_in_doc(Xtr, features, row_id, top_n=20):\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)\n\ndef top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=10):\n    if grp_ids:\n        D = Xtr[grp_ids].toarray()\n    else:\n        D = Xtr.toarray()\n\n    D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_feats(tfidf_means, features, top_n)\n\ndef top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=20):\n    dfs = []\n    labels = np.unique(y)\n    for label in labels:\n        ids = np.where(y==label)\n        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        feats_df.label = label\n        dfs.append(feats_df)\n    return dfs\n\ndef plot_tfidf_classfeats_h(dfs, num_class=9):\n    fig = plt.figure(figsize=(12, 100), facecolor=\"w\")\n    x = np.arange(len(dfs[0]))\n    for i, df in enumerate(dfs):\n        #z = int(str(int(i\/3)+1) + str((i%3)+1))\n        ax = fig.add_subplot(num_class, 1, i+1)\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        ax.set_frame_on(False)\n        ax.get_xaxis().tick_bottom()\n        ax.get_yaxis().tick_left()\n        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=16)\n        ax.set_ylabel(\"Word\", labelpad=16, fontsize=16)\n        ax.set_title(\"Class = \" + str(df.label), fontsize=25)\n        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n        ax.barh(x, df.tfidf, align='center')\n        ax.set_yticks(x)\n        ax.set_ylim([-1, x[-1]+1])\n        yticks = ax.set_yticklabels(df.feature)\n        \n        for tick in ax.yaxis.get_major_ticks():\n                tick.label.set_fontsize(20) \n        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n    plt.show()\nclass_Xtr = text_clf[0].transform(x)\nclass_y = y\nclass_features = text_clf[0].get_feature_names()\nclass_top_dfs = top_feats_by_class(class_Xtr,class_y, class_features)\nplot_tfidf_classfeats_h(class_top_dfs, 7)\n","ffc3f3d5":"# Reference https:\/\/machinelearningmastery.com\/calculate-the-bias-variance-trade-off\/\nfrom mlxtend.evaluate import bias_variance_decomp\ndef bias (model, X_train, y_train, X_test, y_test):\n  mse, bias, var = bias_variance_decomp(model, X_train, y_train, X_test, y_test, loss='mse', num_rounds=100, random_seed=1)\n  # summarize results\n  print('MSE: %.3f' % mse)\n  print('Bias: %.3f' % bias)\n  print('Variance: %.3f' % var)","d17cbaea":"# estimate the bias and variance\nfrom mlxtend.evaluate import bias_variance_decomp\ndef bias(model, x,y):\n    X, y = x, y\n\n    # split the data\n    X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.33, random_state=1)\n    \n    # estimate bias and variance\n    mse, bias, var = bias_variance_decomp(model, X_train, y_train, X_test, y_test, loss='mse', num_rounds=200, random_seed=1)\n    # summarize results\n    print('MSE: %.3f' % mse)\n    print('Bias: %.3f' % bias)\n    print('Var: %.3f' % var)","fd694c12":"bias(text_clf,x,y_encoding)","fbd7e5a3":"bias(text_clf2,x,y_encoding)","4b0cf312":"bias(text_clf3,x,y_encoding)","76ce6587":"bias(text_clf4,x,y_encoding)","6a287f40":"# Train, Cross Validation and Test results for SVM","a71d4e5b":"# SVM Error Analysis ","a75d495a":"# Train,Cross Validation and Test results for GradientBoosting","7ea79d56":"# Label Encoding","cc41501a":"# Create Pipelines","69757b06":"# Train,Cross Validation and Test results for DecisionTree","e276a9ed":"# Split DATA 80% training 20% testing","341d2a32":"# Data Partitioning","f5f57a67":"# GradientBoosting Error Analysis ","781a955e":"# this function is for getting books text from its url","255d4149":"# Train,Cross Validation and Test results for KNeighbors","1338b03d":"# Import 5 books with science fiction genre from ['Richard Ashby', 'C. M. Kornbluth', 'Tom W. Harris', 'Philip K. Dick', 'Edmond Hamilton']","fa6a30fe":"# DecisionTree Error Analysis ","8b2d2da6":"# By analyzing incorrect cases we found high similarities in some words between the input sequence and predicted books","3b17b435":"# Bias and Variability","85a97a5e":"# Import Data ","3fefd6bd":"# KNeighbors Error Analysis  "}}