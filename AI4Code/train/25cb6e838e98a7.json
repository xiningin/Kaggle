{"cell_type":{"43451c31":"code","6de825e7":"code","aebf5165":"code","44eaae45":"code","684ed8f6":"code","28d28d4b":"code","d6998591":"code","6716a448":"code","fe5cb54a":"code","6bef56f3":"code","d31d1db0":"code","57828071":"code","46a3898f":"code","2aa49837":"code","82a797e1":"code","96aaa19b":"code","004cc543":"code","a9c7f039":"code","8e9175a6":"code","ab0e1a64":"code","6cdcd864":"code","ea375f90":"code","fc08323f":"code","0f65b56f":"code","baa767d2":"code","dca85562":"code","c9f88be2":"code","d62ef671":"code","ea806c99":"code","80f07a83":"code","6e986372":"code","eb7ef187":"code","91fc1e4e":"code","12704539":"code","e01427bb":"code","f2584138":"code","347012e0":"code","bbb27983":"code","a7a73c3a":"code","0eb5e355":"code","4fe4292a":"code","74d28adb":"code","1dad1cfb":"code","0f32e2fb":"code","34ccef34":"code","c2c90bbb":"code","c3493309":"markdown","798dbfbc":"markdown","a01b51b1":"markdown","85a0dcaa":"markdown"},"source":{"43451c31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport matplotlib.pyplot as plt","6de825e7":"df =pd.read_csv('..\/input\/fake-news\/train.csv')","aebf5165":"df.head()","44eaae45":"x= df.drop('label',axis=1)","684ed8f6":"x.head(2)","28d28d4b":"y = df['label']","d6998591":"df.shape","6716a448":"df.info()","fe5cb54a":"df.isnull().sum()","6bef56f3":"df=df.dropna()","d31d1db0":"df.head()","57828071":"df['title'][3]","46a3898f":"messeges =df.copy()","2aa49837":"messeges.reset_index(inplace=True)","82a797e1":"messeges.head()","96aaa19b":"corpus = []\nfor i in range(0, len(messeges)):\n    review = re.sub('[^a-zA-Z]', ' ', messeges['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","004cc543":"corpus[6]","a9c7f039":"cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\nX = cv.fit_transform(corpus).toarray()","8e9175a6":"# show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\ncv.vocabulary_","ab0e1a64":"X.shape","6cdcd864":"y=messeges['label']","ea375f90":"## Divide the dataset into Train and Test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","fc08323f":"cv.get_feature_names()[:20]","0f65b56f":"cv.get_params()","baa767d2":"count_df = pd.DataFrame(X_train, columns=cv.get_feature_names())","dca85562":"count_df.head()","c9f88be2":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","d62ef671":"from sklearn.naive_bayes import MultinomialNB\nclassifier=MultinomialNB()\nfrom sklearn import metrics\nimport numpy as np\nimport itertools","ea806c99":"classifier.fit(X_train, y_train)\npred = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","80f07a83":"classifier.fit(X_train, y_train)\npred = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nscore","6e986372":"y_train.shape","eb7ef187":"classifier=MultinomialNB(alpha=0.1)","91fc1e4e":"previous_score=0\nfor alpha in np.arange(0,1,0.1):\n    sub_classifier=MultinomialNB(alpha=alpha)\n    sub_classifier.fit(X_train,y_train)\n    y_pred=sub_classifier.predict(X_test)\n    score = metrics.accuracy_score(y_test, y_pred)\n    if score>previous_score:\n        classifier=sub_classifier\n    print(\"Alpha: {}, Score : {}\".format(alpha,score))","12704539":"## Get Features names\nfeature_names = cv.get_feature_names()","e01427bb":"classifier.coef_[0]","f2584138":"### Most real\nsorted(zip(classifier.coef_[0], feature_names), reverse=True)[:20]","347012e0":"### Most fake\nsorted(zip(classifier.coef_[0], feature_names))[:5000]","bbb27983":"train=pd.read_csv('..\/input\/fake-news\/train.csv')\ntest=pd.read_csv('..\/input\/fake-news\/test.csv')\ntest.info()\ntest['label']='t'\ntrain.info()","a7a73c3a":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntest=test.fillna(' ')\ntrain=train.fillna(' ')\ntest['total']=test['title']+' '+test['author']+test['text']\ntrain['total']=train['title']+' '+train['author']+train['text']\n\n#tfidf\ntransformer = TfidfTransformer(smooth_idf=False)\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\ncounts = count_vectorizer.fit_transform(train['total'].values)\ntfidf = transformer.fit_transform(counts)","0eb5e355":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n#data prep\ntest=test.fillna(' ')\ntrain=train.fillna(' ')\ntest['total']=test['title']+' '+test['author']+test['text']\ntrain['total']=train['title']+' '+train['author']+train['text']\n\n#tfidf\ntransformer = TfidfTransformer(smooth_idf=False)\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\ncounts = count_vectorizer.fit_transform(train['total'].values)\ntfidf = transformer.fit_transform(counts)","4fe4292a":"targets = train['label'].values\ntest_counts = count_vectorizer.transform(test['total'].values)\ntest_tfidf = transformer.fit_transform(test_counts)\n\n#split in samples\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0)","74d28adb":"from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\n\nExtr = ExtraTreesClassifier(n_estimators=5,n_jobs=4)\nExtr.fit(X_train, y_train)\nprint('Accuracy of ExtrTrees classifier on training set: {:.2f}'\n     .format(Extr.score(X_train, y_train)))\nprint('Accuracy of Extratrees classifier on test set: {:.2f}'\n     .format(Extr.score(X_test, y_test)))","1dad1cfb":"from sklearn.naive_bayes import MultinomialNB\n\nNB = MultinomialNB()\nNB.fit(X_train, y_train)\nprint('Accuracy of NB  classifier on training set: {:.2f}'\n     .format(NB.score(X_train, y_train)))\nprint('Accuracy of NB classifier on test set: {:.2f}'\n     .format(NB.score(X_test, y_test)))","0f32e2fb":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e5)\nlogreg.fit(X_train, y_train)\nprint('Accuracy of Lasso classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Lasso classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))","34ccef34":"targets = train['label'].values\nlogreg = LogisticRegression()\nlogreg.fit(counts, targets)\n\nexample_counts = count_vectorizer.transform(test['total'].values)\npredictions = logreg.predict(example_counts)\npred=pd.DataFrame(predictions,columns=['label'])\npred['id']=test['id']\npred.groupby('label').count()","c2c90bbb":"pred.to_csv('countvect5.csv', index=False)","c3493309":"### Multinomial Classifier with Hyperparameter","798dbfbc":"## Mulinomial Naive Bayes Theorem","a01b51b1":"### Check any Null Values in the dataframe","85a0dcaa":"## Counter Vectorization\n### Bag of Words"}}