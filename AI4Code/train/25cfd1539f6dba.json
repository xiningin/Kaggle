{"cell_type":{"6a91fa7f":"code","e55a429e":"code","cca2f70a":"code","6c4dbf59":"code","f08dff54":"code","bb78b918":"code","03f6779d":"code","6c4f9db9":"code","389f3ccd":"code","a9b92959":"code","6e0c2c2b":"code","ece659ba":"code","e8c42b57":"code","3f836fb8":"code","cb4c55ba":"code","d8b7ee7b":"code","bb4718f2":"code","30230ddc":"code","2b1b0349":"code","c511a915":"code","adb4e6a9":"code","7da66ce2":"code","a5969afb":"code","6633e324":"code","1c27cc5b":"code","ae25764f":"code","e02e3f59":"code","3713d09b":"code","588b9125":"code","9599f622":"code","559bcd29":"code","d8879370":"code","a3a115e3":"code","82053cf1":"code","eb0a9304":"code","8ae8311a":"code","0bd761b8":"code","9151fffc":"code","9850b596":"code","6a53d409":"code","ad4d4816":"code","a5782054":"code","bc7e87a6":"code","34a13e4c":"code","c425461d":"code","e1956e5a":"code","ddf2a9c7":"code","d8aafa7d":"code","d0fd642f":"code","01ab43a6":"code","75ced5cd":"code","613988b1":"code","115a6d0c":"code","7be33eaf":"code","664d1931":"code","f4f6e04b":"code","99dda669":"code","756551ab":"code","ea15f9b3":"code","b49b3727":"code","5ef6059a":"code","5b5113f8":"code","d33709b3":"code","71d4139b":"code","1318996c":"code","1bcc868a":"code","36d5e5e6":"markdown","db01c127":"markdown","51e01b68":"markdown","5b22a8ce":"markdown","e7b5edd2":"markdown","31ae96d8":"markdown","d1420949":"markdown","346cec1a":"markdown","aeb39c89":"markdown","bc6a7542":"markdown","c33544b8":"markdown","4b372a45":"markdown","6e4e69b9":"markdown","b6ea604d":"markdown","0c5178be":"markdown","23dc71af":"markdown","cfe46ede":"markdown","cc9c6364":"markdown","fe9b83da":"markdown","ebd0a73c":"markdown","6ca18961":"markdown","1fd2389a":"markdown","356b6b39":"markdown","9ff7532b":"markdown","9a9c4cfe":"markdown","07ca76b9":"markdown","42f16f80":"markdown","b734b320":"markdown","4d0fce0f":"markdown","33609d20":"markdown","d8107327":"markdown","443d9823":"markdown","813d5fb9":"markdown","2cfb1119":"markdown","7ccd7e56":"markdown","2e2441e1":"markdown","0c19bda4":"markdown","6a734d3b":"markdown","40a1af62":"markdown","a669cb8c":"markdown","8ea03bfc":"markdown","4ec10514":"markdown","4d6936e8":"markdown","c3e736c0":"markdown","0b16b4ff":"markdown","d2685f8d":"markdown","0b55754d":"markdown","643f62c3":"markdown","8e321f86":"markdown","7e04af48":"markdown","0ff72473":"markdown","6cc09b99":"markdown","c3155fa6":"markdown"},"source":{"6a91fa7f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, learning_curve, cross_validate, train_test_split, KFold\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom yellowbrick.features import Rank2D\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport missingno as msno\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")","e55a429e":"train_df=pd.read_csv('..\/input\/titanic\/train.csv',encoding='us-ascii')\ntest_df=pd.read_csv('..\/input\/titanic\/test.csv',encoding='us-ascii')","cca2f70a":"#Preview of data\ntrain_df.head(5)","6c4dbf59":"train_df.shape, test_df.shape","f08dff54":"train_df.describe()","bb78b918":"train_df.dtypes","03f6779d":"sns.countplot(train_df.Survived)","6c4f9db9":"# number of survived and non-survived passengers\ntrain_df.Survived.value_counts()","389f3ccd":"def correlation_heatmap(train):\n    correlations = train.corr()\n    fig, ax = plt.subplots(figsize=(15,15))\n    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', cmap=\"YlGnBu\",\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70}\n                )\n    plt.show()","a9b92959":"correlation_heatmap(train_df)","6e0c2c2b":"msno.matrix(train_df)","ece659ba":"def check_missing(df):\n    df_miss_numerbers=df.isnull().sum()\n    df_miss_percents=round( (df_miss_numerbers\/len(df))*100, 2)\n    miss_df=pd.concat([df_miss_numerbers,df_miss_percents ], axis=1)\n    miss_df.rename(columns={0: 'numbers',  1: 'percent'}, inplace=True)\n    return miss_df","e8c42b57":"missing_df=check_missing(train_df)\nnn=missing_df[missing_df.numbers>0].sort_values(by=['percent'],ascending=False)\ndisplay(nn.style.background_gradient())","3f836fb8":"# test dataset\nmissing_df=check_missing(test_df)\nnn=missing_df[missing_df.numbers>0].sort_values(by=['percent'],ascending=False)\ndisplay(nn.style.background_gradient())","cb4c55ba":"sns.distplot(test_df.Fare,kde=False,fit=norm)","d8b7ee7b":"print('Test data Fare mean:',test_df.Fare.mean(),' and median:',test_df.Fare.median())","bb4718f2":"fare_median=test_df.Fare.median(skipna=True)\ntest_df.Fare.fillna(fare_median,inplace=True)","30230ddc":"train_df.Embarked.value_counts().plot.bar()","2b1b0349":"train_df.Embarked.fillna('S',inplace=True)\ntest_df.Embarked.fillna('S',inplace=True)","c511a915":"sns.distplot(train_df.Age,kde=False,fit=norm)","adb4e6a9":"print('Train data Age mean:',train_df.Age.mean(),' and median:',train_df.Age.median())","7da66ce2":"print('Skweness Train data Age skewness:',train_df.Age.skew(),' Test data Fare skewness:',test_df.Fare.skew())","a5969afb":"train_df.groupby('Sex').Age.mean().plot.bar()","6633e324":"sns.kdeplot(train_df[train_df.Survived==0]['Age'], shade=True)\nsns.kdeplot(train_df[train_df.Survived==1]['Age'], shade=True)\nplt.legend(title='Survived', loc='upper right', labels=['No', 'Yes'])\nplt.show()","1c27cc5b":"train_df[train_df.Sex=='male'].Age.plot.hist()\ntrain_df[train_df.Sex=='female'].Age.plot.hist()","ae25764f":"male_age_median=train_df[train_df.Sex=='male'].Age.median(skipna=True)\nfemale_age_median=train_df[train_df.Sex=='female'].Age.median(skipna=True)\n\ntrain_df.loc[ (train_df.Sex=='male') & (pd.isnull(train_df.Age)), 'Age']=male_age_median\ntrain_df.loc[ (train_df.Sex=='female') & (pd.isnull(train_df.Age)), 'Age']=female_age_median\n\ntest_df.loc[ (test_df.Sex=='male') & (pd.isnull(test_df.Age)), 'Age']=male_age_median\ntest_df.loc[ (test_df.Sex=='female') & (pd.isnull(test_df.Age)), 'Age']=female_age_median\n","e02e3f59":"print('Female median:',female_age_median, ' Male median:', male_age_median)","3713d09b":"train_df.drop(columns=['Cabin'],inplace=True)\ntest_df.drop(columns=['Cabin'],inplace=True)","588b9125":"train_df.describe(include=['O'])","9599f622":"train_df.describe()","559bcd29":"train_df.drop(columns=['PassengerId','Ticket','Name'],inplace=True,axis=1)\n# we keep PassengerID in test we need for submission\ntest_df.drop(columns=['Ticket','Name'],inplace=True,axis=1)","d8879370":"train_df[train_df.Survived==0].hist()\ntrain_df[train_df.Survived==1].hist()","a3a115e3":"def age_groups(s):\n    if (s['Age'] < 18):\n        return 0\n    elif (s['Age'] >=18) and (s['Age'] <24) :\n        return 1\n    elif (s['Age'] >=24) and (s['Age'] <35) :\n        return 2\n    elif (s['Age'] >=35) and (s['Age'] <45) :\n        return 3\n    elif (s['Age'] >=45) and (s['Age'] <55) :\n        return 4\n    elif (s['Age'] >=55) and (s['Age'] <65) :\n        return 5\n    elif (s['Age'] >=65):\n        return 6\n    else:\n        return 7","82053cf1":"train_df['AgeGrp']=train_df.apply(age_groups, axis=1)\ntest_df['AgeGrp']=test_df.apply(age_groups, axis=1)","eb0a9304":"test_df.head()","8ae8311a":"train_df.drop(columns=['Age'],inplace=True,axis=1)\ntest_df.drop(columns=['Age'],inplace=True,axis=1)","0bd761b8":"train_df['relatives']=train_df['SibSp'] + train_df['Parch']\ntrain_df['Alone']=np.where(train_df['relatives']>0,0,1)\n\n\ntest_df['relatives']=test_df['SibSp'] + test_df['Parch']\ntest_df['Alone']=np.where(test_df['relatives']>0,0,1)\n\n\ntrain_df.drop(columns=['relatives'],axis=1,inplace=True)\ntest_df.drop(columns=['relatives'],axis=1,inplace=True)","9151fffc":"cols=['SibSp','Parch']\ntrain_df=train_df.drop(cols,axis=1)\ntest_df=test_df.drop(cols,axis=1)","9850b596":"tmp_train1 = pd.get_dummies(train_df, columns=[\"Pclass\"])\ntmp_train2 = pd.get_dummies(tmp_train1, columns=[\"Embarked\"])\ntmp_train3 = pd.get_dummies(tmp_train2, columns=[\"Sex\"])\ntmp_train4 = pd.get_dummies(tmp_train3, columns=[\"AgeGrp\"])\n\n","6a53d409":"tmp_test1 = pd.get_dummies(test_df, columns=[\"Pclass\"])\ntmp_test2 = pd.get_dummies(tmp_test1, columns=[\"Embarked\"])\ntmp_test3 = pd.get_dummies(tmp_test2, columns=[\"Sex\"])\ntmp_test4 = pd.get_dummies(tmp_test3, columns=[\"AgeGrp\"])","ad4d4816":"final_train=tmp_train4\nfinal_test=tmp_test4","a5782054":"final_train.columns","bc7e87a6":"final_train.Survived.value_counts()","34a13e4c":"y=final_train['Survived']\n\ncols=['Fare', 'Alone', 'Pclass_1', 'Pclass_2', 'Pclass_3','Embarked_C', 'Embarked_Q', 'Embarked_S', 'Sex_female', 'Sex_male','AgeGrp_0', 'AgeGrp_1', \n      'AgeGrp_2', 'AgeGrp_3', 'AgeGrp_4', 'AgeGrp_5','AgeGrp_6']\nX=final_train[cols]\n\nX_pred=final_test[cols]","c425461d":"from scipy.stats import probplot,skew, norm\n\ndef plot_feat(data,title,log=False):\n    if log:\n        data=np.log1p(data)\n    fig, axes = plt.subplots(1, 3, figsize=(20,4))\n    sns.distplot(data,kde=False, fit=norm,ax=axes[0])\n    sns.boxplot(data, ax=axes[1])\n    probplot(data, plot=axes[2])\n    skew_val=round(data.skew(), 1)\n    axes[1].set_yticklabels([])\n    axes[1].set_yticks([])\n    axes[0].set_title(title + \" | Distplot\")\n    axes[1].set_title(title + \" | Boxplot\")\n    axes[2].set_title(title + \" | Probability Plot - Skew: \"+str(skew_val))\n    plt.show()","e1956e5a":"plot_feat(X.Fare,'Fare')","ddf2a9c7":"plot_feat(X.Fare,'Fare',True)","d8aafa7d":"X['Fare']=np.log1p(X['Fare'])","d0fd642f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","01ab43a6":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled=scaler.transform(X_train)\nX_test_scaled=scaler.transform(X_test)","75ced5cd":"X_train=pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\nX_test=pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)","613988b1":"rf=RandomForestClassifier(random_state=55)\nlr=LogisticRegression(random_state=55)\nsv = SVC(probability=True,random_state=55)\nlogreg = LogisticRegression(n_jobs=-1, solver='newton-cg',random_state=55) \nmlp = MLPClassifier(random_state=55)\ngb = GradientBoostingClassifier(random_state=55)\ngnb = GaussianNB()\nxgb= XGBClassifier(random_state=55)\nlgm= LGBMClassifier(random_state=55)","115a6d0c":"models=[rf, lr, sv, logreg, gb, gnb, mlp, xgb, lgm]\ncv = StratifiedKFold(10, shuffle=True, random_state=42)","7be33eaf":"model_results = pd.DataFrame()\nrow_number = 0\nresults = []\nnames = []\n\nfor ml in models:\n    model_name=ml.__class__.__name__\n    cv_results = cross_validate(ml, X, y, cv=cv, scoring='accuracy', return_train_score=True, n_jobs=-1 )\n    model_results.loc[row_number,'Model Name']=model_name\n    model_results.loc[row_number, 'Train Accuracy Mean']=cv_results['train_score'].mean()\n    model_results.loc[row_number, 'Test Accuracy Mean']=cv_results['test_score'].mean()\n    model_results.loc[row_number, 'Fit Time Mean']=cv_results['fit_time'].mean()\n    results.append(cv_results)\n    names.append(model_name)\n    \n    row_number+=1","664d1931":"cv_results_array = []\n#we get test_scores out of CV scores for benchmarking\nfor tt in results:\n  cv_results_array.append(tt['test_score'])\n\n\n\nfig = plt.figure(figsize=(18, 6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(cv_results_array)\nax.set_xticklabels(names)\nplt.show()","f4f6e04b":"display(model_results.style.background_gradient(cmap='summer_r'))","99dda669":"import optuna\nfrom optuna.samplers import TPESampler\nfrom sklearn.metrics import roc_auc_score,accuracy_score\n\nsampler = TPESampler(\n    seed=55\n)\n\ndef create_model(trial):\n    booster =  trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"])\n    alpha = trial.suggest_loguniform(\"alpha\", 1e-8, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 9)\n    eta = trial.suggest_loguniform(\"eta\", 1e-8, 1.0)\n    gamma = trial.suggest_loguniform(\"gamma\", 1e-8, 1.0)\n    n_estimators = trial.suggest_int(\"n_estimators\", 5, 300)\n    \n    model = XGBClassifier(\n        booster = booster,\n        alpha = alpha,\n        max_depth = max_depth,\n        eta = eta,\n        gamme = gamma,\n        n_estimators=n_estimators, \n        random_state=55\n    )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_train, y_train)\n    score = roc_auc_score(\n        y_test, \n        model.predict_proba(X_test)[:,1]\n    )\n    return score","756551ab":"study = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=150)\nparams = study.best_params\nparams['random_state'] = 55","ea15f9b3":"params","b49b3727":"from yellowbrick.model_selection import FeatureImportances\n\nmodel_yb = GradientBoostingClassifier(random_state=55)\nviz = FeatureImportances(model_yb)\nviz.fit(X, y)\nviz.show()\n","5ef6059a":"from yellowbrick.model_selection import LearningCurve\n\n#model_yb = GradientBoostingClassifier(random_state=55)\nmodel_yb = XGBClassifier(**params)\n\ncv = StratifiedKFold(n_splits=12)\nsizes = np.linspace(0.3, 1.0, 10)\n\n# Instantiate the classification model and visualizer\nvisualizer = LearningCurve(\n    model_yb, cv=cv, scoring='accuracy', train_sizes=sizes, n_jobs=4\n)\n\nvisualizer.fit(X, y)        # Fit the data to the visualizer\nvisualizer.show()           # Finalize and render the figure","5b5113f8":"X_pred['Fare']=np.log1p(X_pred['Fare'])\nX_pred_scaled=scaler.transform(X_pred)\nX_pred=pd.DataFrame(X_pred_scaled, index=X_pred.index, columns=X_pred.columns)","d33709b3":"best_model= XGBClassifier(**params)\n\nbest_model.fit(X,y)\n\npredictions = best_model.predict(X_pred)\n\n\noutput = pd.DataFrame({'PassengerId': final_test.PassengerId, 'Survived': predictions.ravel()})\noutput.to_csv('submission_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","71d4139b":"from sklearn.decomposition import PCA\nfrom mlxtend.plotting import plot_decision_regions","1318996c":"# for visualization reduce number of features to 2 (2D)\npca = PCA(n_components = 2)\nX_train2 = pca.fit_transform(X_train)","1bcc868a":"plot_gb= XGBClassifier(**params)\nplot_gb.fit(X_train2, y_train)\nplot_decision_regions(X_train2, y_train.values, clf=plot_gb, legend=2)\n\nplt.xlabel(X.columns[0], size=14)\nplt.ylabel(X.columns[1], size=14)\nplt.title('GB Decision Region Boundary', size=16)","36d5e5e6":"We have 891 rows and 12 columns in train dataset. <br>\nTest dataset 418 rows and 11 columns. <br>\nTrain dataset have 12 features and Test have 11 features. <br>\nOne of the features called 'Survived' is our target feature.<br>","db01c127":"we have left skewed data, filling nulls with mean is not useful here and again median looks more dominated value in here.\nBut let's see if whats happens depeding on gender.","51e01b68":"# **FamilySize**\n\nAs we see above explorations having relatives is important in survival. So we can create a future to tell if passenger have relatives or not called alone.","5b22a8ce":"# **Dummy Categorical Features**","e7b5edd2":"Name and Ticket features having high cardinality, so safe to drop","31ae96d8":"## **Age missing values**","d1420949":"# **Cabin missing values**","346cec1a":"# **AGE**","aeb39c89":"# **Submission**","bc6a7542":"GradientBoosting looks best scoring in terms of cross validation test scroes below in boxplots","c33544b8":"# **Learning Curve**\n\nA learning curve shows the relationship of the training score versus the cross validated test score for an estimator with a varying number of training samples. ","4b372a45":"# Target Feature\n\nOur target feature Survived is binary type. 0 - Not Survived and 1 - Survived. This is a classification task.<br>\n38% of passengers survived and our dataset looks balanced.","6e4e69b9":"# **Run Models**\n\nRun models and save results in a custom pandas dataset","b6ea604d":"We have missings in Age, Cabin and Embarked columns in train data","0c5178be":"We have missing in Cabin,Age and Fare in test data","23dc71af":"# **Correlation**\n","cfe46ede":"# **Feature Importance For GradientBoosting Classifier**\n\n\nFeature importance techniques is finding importance of each feature when making a prediction depending on input features\/variables.\nGender, Fair, Pclass=3 and Age Group 0 (age less than 18) is most important features effecting survival results according to GradientBoosting Classifier feature importance.","cc9c6364":"<center> <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fd\/RMS_Titanic_3.jpg\" width=\"300px\"> <\/center>","fe9b83da":"# **Basic Feature Stats**","ebd0a73c":"# **Models**","6ca18961":"# **Extra - Model Decision Boundary Examples**","1fd2389a":"# **Handling Missing Data\/Impute**","356b6b39":"We can see Age and Gender have an effect on Survival rate","9ff7532b":"Embarked is a categorical variable so we can just use domination value for imputation.<br>\nWe just have 2 missings in Embarked and we can fill it with dominating value 'S'","9a9c4cfe":"As we compare features Survied vs Not Survived below, we can say that **Age, Parch, SibSp** have some effects.\nSo being young and having relatives looks important on survival.","07ca76b9":"# **Basic Statistics**\nSome basic statistics for numerical features in data with pandas describe function.\nWe see Survived our target feature shows 38% of passengers survived the disaster.","42f16f80":" and Age distribution per Gender is different so filling the nulls with Gender specific values","b734b320":"![Optuna](https:\/\/optuna.org\/assets\/img\/optuna-logo.png)\n\n# **Optimize XGBoost with Optuna**\n\nOptuna is a open source hyperparameter optimization framework to automate hyperparameter search","4d0fce0f":"# **Loading Data**","33609d20":"~77% of missing, for now I drop it. Later can work on it.","d8107327":"After defining models for testing, it's time to create a stacking model","443d9823":"General thumb if we have less than 5% filling ratio we can drop the variable.<br>\nReference: https:\/\/www.kdnuggets.com\/2020\/06\/missing-values-dataset.html <br>\nWe can see missings in Age,Cabin and Embarked<br>","813d5fb9":"PassengerId is ","2cfb1119":"# **Scaling Data**","7ccd7e56":"## **Missing Values**","2e2441e1":"## **Constant and Not useful Features**","0c19bda4":"Skewness of Age in train data is not that high like Fare in test data","6a734d3b":"Pandas dtypes shows columns\/variables and their default data type determined by pandas.\n","40a1af62":"# **Final Dataset**","a669cb8c":"PassengerId\t feature having high cardinality, so safe to drop","8ea03bfc":"# **Train Test Split**","4ec10514":"# **Age in groups**","4d6936e8":"# **Checking missings and constants**","c3e736c0":"# **Model Results**","0b16b4ff":"# **Feature Engineering**","d2685f8d":"Again for each distribution looks like left skewed data so filling Age missings with new medians","0b55754d":"# **EDA**\n","643f62c3":"# **Preparing Model Data**\n\nNow we select our crafted data columns\/features.","8e321f86":"Also we can see here younger people have more survive rate below","7e04af48":"# **Drop columns**","0ff72473":"## **Embarked Missing Values**","6cc09b99":"Let's see if we have any missing values in variables. Depending on percentage of missing values we can decide to drop or impute\/replace with meaningful values.\nIf we don't handle missing values this can lead inaccruate data or misleading results. \nIn case constants we generally drop those variables which will give us no meaningful or useful information.","c3155fa6":"## **Fare Missing Values**\n\nWe have only 1 null in test so far.\nDistribution plot shows that we have left skewed data. If it was normal distribution, mean was the value we can take for imputation. But in left skewed data median is best candidate for imputation\/replacing the null values"}}