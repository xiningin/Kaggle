{"cell_type":{"41e530d1":"code","fec8d9c0":"code","6b5b4622":"code","24e0af6e":"code","0ab23339":"code","b294d6ae":"code","9f01a8e8":"code","7bcec8a5":"code","0aea0a4a":"code","2df83f78":"code","f0fd60f4":"code","93615532":"code","1788baed":"code","3f9fd526":"code","0f0f82bb":"code","7cde9a51":"code","2bc0ccc9":"code","ac5bf888":"code","ee7b7ca5":"code","7e0fc67e":"code","e3fd0325":"code","bb4ec4ee":"code","359e15e9":"code","60710bbe":"code","b8d27327":"code","1f1d4267":"code","92d92af2":"code","28af21a5":"code","6fc83669":"code","19ae77e9":"code","b1cd8b6b":"code","d8d0dbdc":"code","5b4c8999":"markdown","6876f903":"markdown","c54f873b":"markdown","f5dc8c24":"markdown","9b7691eb":"markdown","c9b44e36":"markdown","f019868f":"markdown"},"source":{"41e530d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        pass\n\nwarnings.filterwarnings(\"ignore\")\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fec8d9c0":"train_df = pd.read_csv('\/kaggle\/input\/feedback-prize-2021\/train.csv')\n\ntrain_df.columns","6b5b4622":"test_names, test_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/test'))):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('..\/input\/feedback-prize-2021\/test\/' + f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n# test_texts['text'] = test_texts['text'].apply(lambda x:x.split())\ntest_texts.head()","24e0af6e":"# test_names, train_texts = [], []\n# for f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))[:100]):\n#     test_names.append(f.replace('.txt', ''))\n#     train_texts.append(open('..\/input\/feedback-prize-2021\/train\/' + f, 'r').read())\n# train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n# # train_texts['text'] = test_texts['text'].apply(lambda x:x.split())\n# train_text_df.head()","0ab23339":"# all_entities = []\n# for i in tqdm(train_text_df.iterrows()):\n#     total = i[1]['text'].split().__len__()\n# #     entities = []\n#     entities = [\"O\" for i in range(total)]\n#     for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n#         discourse = j[1]['discourse_type']\n#         list_ix = j[1]['predictionstring'].split()\n#         for li in list_ix[1:]:\n# #             print(li, entities)\n#             entities[int(li)] = f\"I-{discourse}\"\n#         entities[int(list_ix[0])] = f\"B-{discourse}\"\n#     all_entities.append(entities)","b294d6ae":"train_text_df = pd.read_csv(\"\/kaggle\/input\/feedback-prize-ner-tagged-data\/feedback_prize_ner_tagged_data.csv\")\ntrain_text_df.head()","9f01a8e8":"import ast\ntrain_text_df['entities'] = train_text_df['entities'].apply(lambda x:ast.literal_eval(x))\n\nprint(train_text_df['entities'].values[0])","7bcec8a5":"train_text_df = train_text_df[:]","0aea0a4a":"import datasets\nfrom transformers import Trainer, TrainingArguments, DataCollatorWithPadding\nfrom transformers import BigBirdForTokenClassification, BigBirdTokenizerFast\nfrom torch import cuda\nimport torch","2df83f78":"config = {'model_name': '\/kaggle\/input\/huggingfacebigbirdrobertabase\/',\n         'max_length': 1024,\n         'train_batch_size':4,\n         'valid_batch_size':8,\n         'epochs':5,\n         'learning_rate':5e-05,\n         'max_grad_norm':10,\n          'warmup':0.1,\n          \"grad_acc\":8,\n          \"model_save_path\":\"big-bird\",\n         'device': 'cuda' if cuda.is_available() else 'cpu'}\n\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}","f0fd60f4":"train_text_df['labels'] = train_text_df['entities'].apply(lambda x: [labels_to_ids[i] for i in x])","93615532":"train_text_df.head()","1788baed":"tokenizer = BigBirdTokenizerFast.from_pretrained(config['model_name'])\nmodel = BigBirdForTokenClassification.from_pretrained(config['model_name'],\n                                                     num_labels=len(output_labels))","3f9fd526":"converted = tokenizer(train_text_df.loc[0].values[1].split(),\n                      is_split_into_words=True, return_offsets_mapping=True)","0f0f82bb":"ix = 0\nfor i,j in zip(tokenizer.convert_ids_to_tokens(converted['input_ids']), converted['offset_mapping']):\n    print(i, j)\n    ix += 1\n    if ix == 15:\n        break","7cde9a51":"def tokenizer_data(example):\n    encoding = tokenizer(example['text'].split(),\n                         is_split_into_words=True,\n                         truncation=True,\n                         padding='max_length', \n                         return_offsets_mapping=True,\n                         max_length=config['max_length'])\n    i = 0\n    labels = example['labels']\n    encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n    for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n        if mapping[0] == 0 and mapping[1] != 0:\n            try:\n                encoded_labels[idx] = labels[i]\n            except:\n                pass\n            i += 1\n    item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n    item['labels'] = torch.as_tensor(encoded_labels)\n    return item\n","2bc0ccc9":"dataset = datasets.Dataset.from_pandas(train_text_df)","ac5bf888":"dataset = dataset.train_test_split(test_size=0.1)\ndataset","ee7b7ca5":"text = dataset['train'][1]\n\n# print(text['text'], text['entities'], text['labels'])","7e0fc67e":"converted = tokenizer_data(text)\n\nconverted","e3fd0325":"converted\n\ni=0\nfor token, label in zip(tokenizer.convert_ids_to_tokens(converted[\"input_ids\"]), \n                        converted[\"labels\"]):\n    print(token, label, converted['offset_mapping'][i])\n    i+=1\n    if i == 15:\n        break","bb4ec4ee":"dataset = dataset.map(tokenizer_data)\n\ndataset","359e15e9":"dataset.set_format(type='torch', columns=['input_ids', 'attention_mask',\n                                         'labels'])\n\ndataset","60710bbe":"for a in dataset['train']:\n    if a['input_ids'].shape[0] != a['attention_mask'].shape[0]:\n        print(a)\n        break","b8d27327":"trainer_args = TrainingArguments('test_trainer',\n                                report_to='none',\n                                 num_train_epochs=config['epochs'],\n                                evaluation_strategy ='epoch',\n                                per_device_train_batch_size=config['train_batch_size'],\n                                per_device_eval_batch_size=config['valid_batch_size'],\n                                fp16=True,\n                                save_strategy = \"epoch\",\n                                 warmup_ratio= config['warmup'],\n                                 gradient_accumulation_steps=config['grad_acc'],\n                                 logging_strategy=\"epoch\",\n                                 save_total_limit=1\n                                )\n\ntrainer = Trainer(model=model,\n                  args=trainer_args, \n                  train_dataset = dataset['train'],\n                  eval_dataset=dataset['test'],\n#                   data_collator = data_collator,\n                  tokenizer=tokenizer)","1f1d4267":"trainer.train()","92d92af2":"device = config['device']","28af21a5":"trainer.model.eval()\ndef inference(sentence):\n    inputs = tokenizer(sentence.split(),\n                        is_split_into_words=True, \n                        return_offsets_mapping=True, \n                        padding='max_length', \n                        truncation=True, \n                        max_length=4096,\n                        return_tensors=\"pt\")\n\n    # move to gpu\n    ids = inputs[\"input_ids\"].to(device)\n    mask = inputs[\"attention_mask\"].to(device)\n    # forward pass\n    outputs = trainer.model(input_ids=ids, attention_mask=mask, return_dict=False)\n#     print(outputs)\n    logits = outputs[0]\n    \n    active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n    print(logits.shape, active_logits.shape, flattened_predictions.shape)\n    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n    token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\n    prediction = []\n    out_str = []\n    off_list = inputs[\"offset_mapping\"].squeeze().tolist()\n    for idx, mapping in enumerate(off_list):\n#         print(mapping, token_pred[1], token_pred[0],\"####\")\n\n#         only predictions on first word pieces are important\n        if mapping[0] == 0 and mapping[1] != 0:\n#             print(mapping, token_pred[1], token_pred[0])\n            prediction.append(wp_preds[idx][1])\n            out_str.append(wp_preds[idx][0])\n        else:\n            if idx == 1:\n                prediction.append(wp_preds[idx][1])\n                out_str.append(wp_preds[idx][0])\n            continue\n    return prediction, out_str","6fc83669":"final_preds = []\nimport pdb\nfor i in tqdm(range(len(test_texts))):\n    idx = test_texts.id.values[i]\n    pred, _ = inference(test_texts.text.values[i])\n    pred = [x.replace('B-','').replace('I-','') for x in pred]\n    preds = []\n    j = 0\n    while j < len(pred):\n        cls = pred[j]\n        if cls == 'O':\n            j += 1\n        end = j + 1\n        while end < len(pred) and pred[end] == cls:\n            end += 1\n            \n        if cls != 'O' and cls != '' and end - j > 7:\n            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n        \n        j = end\n        \n# print(final_preds[1])","19ae77e9":"len(final_preds)","b1cd8b6b":"test_df = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')\ntest_df\n\nsub = pd.DataFrame(final_preds)\nsub.columns = test_df.columns\n\nsub.head()","d8d0dbdc":"sub.to_csv(\"submission.csv\", index=False)","5b4c8999":"### Test to make sure is_split_into_words, return_offsets parameters are working properly","6876f903":"### Define config and Load model, tokenizer","c54f873b":"### Define training argument and train the model","f5dc8c24":"### Tokenize the data\n\n* Load data into huggingface datasets.\n* Make sure you take care of sub-word tokenizing problem when adding labels to tokenized data.\n* use .map to map tokenizer_data function to data\n* Look at one sample to see whether mapping is done correctly or not\n","9b7691eb":"## Notebook explaining how to train with minimal code using datasets and Trainer API \n\n* Loads datafrom dataframe to Datasets\n* Uses the BigBird using with 1024 tokens\n* Trainer API with fp16 enabled so as to optimize the training processes.\n* \n\n### Improvements\n* change the hyperparameter-tunning of the Trainer \n* Improving post processing of labels as mentioned here https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615\n\n## Do Upvote if you find it usefull, It keeps me motivated to do more quality work, Thanks!\n\n\n### load data and convert text into dataframe, then convert predictionstring to NER IOB format look at this [notebook](http:\/\/https:\/\/www.kaggle.com\/raghavendrakotala\/pre-processed-data-for-ner-modeling) to prepare data ","c9b44e36":"### Check the shape of inputs_ids and attention_mask are same or not","f019868f":"### Write inference function, loop through the test_text and dump into submission file"}}