{"cell_type":{"277d03fe":"code","74be0763":"code","d36c4dd1":"code","903634bd":"code","4f38c963":"code","5522a8e8":"code","111982b0":"code","4be9b509":"code","24fe5bc0":"code","8ea5cc03":"code","814f8660":"code","ce1f525f":"code","910be550":"code","bea4aa0e":"code","b9825f96":"code","c3c949ea":"code","c8d7255f":"code","338e53f9":"code","e29b050c":"code","a2d40fa5":"code","0fff5d39":"code","776cd656":"code","3872a74a":"code","d71485df":"code","5e51232b":"code","17d82d31":"code","55a160fa":"code","11b3ff83":"markdown","a27db751":"markdown","66983b54":"markdown","fe939085":"markdown","27463fc4":"markdown","f91990cb":"markdown","74ca1820":"markdown","35be7031":"markdown","7db2d867":"markdown","87fbe72a":"markdown","35e4aded":"markdown","8708ccbe":"markdown","71f469f2":"markdown","5afb645d":"markdown","3441ffa1":"markdown","d9d7724d":"markdown","50f45dc5":"markdown","749e37d5":"markdown","6c13ca32":"markdown","6604f900":"markdown","7a82751e":"markdown","33b458a7":"markdown"},"source":{"277d03fe":"import pandas as pd\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression  # For Logistic Regression\nfrom sklearn.ensemble import RandomForestClassifier # For RFC\nfrom sklearn.svm import SVC                               #For SVM\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import matthews_corrcoef    \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score,roc_curve,auc\nsns.set(style=\"ticks\", color_codes=True)","74be0763":"df = pd.read_csv(\"..\/input\/phishing-data\/combined_dataset.csv\")\ndf.head()","d36c4dd1":"df.isnull().sum()\ndf.isna().sum()\n#df.info()","903634bd":"df.describe()","4f38c963":"sns.countplot(df['label'])","5522a8e8":"X= df.drop(['label', 'domain'], axis=1)\nY= df.label\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.40)\nprint(\"Training set has {} samples.\".format(x_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(x_test.shape[0]))\n","111982b0":"sns.heatmap(df.corr(),annot=True)","4be9b509":"#df.corr()\ndf.corr()['label'].sort_values()","24fe5bc0":"a = df.corr()['label']\n# saving column names in a variable\nvariables = x_train.columns\nvariable = [ ]\n#Taking features only if they have higher than +-0.1\nfor i in range(len(variables)):\n    if a[i]>0.1  or a[i]<=(-0.1):   #setting the threshold as 0.1\n        variable.append(variables[i])\nvariable","8ea5cc03":"def RFC(x_train, y_train, x_test, y_test):\n    #create RFC object\n    RFClass1 = RandomForestClassifier(max_depth=5, random_state=0)\n    #Train the model using training data \n    RFClass1.fit(x_train,y_train)\n\n    #Test the model using testing data\n    y_pred_rfc1 = RFClass1.predict(x_test)\n\n    cm=confusion_matrix(y_test,y_pred_rfc1)\n    sns.heatmap(cm,annot=True)\n    print(\"f1 score is \",f1_score(y_test,y_pred_rfc1,average='weighted'))\n    print(\"matthews correlation coefficient is \",matthews_corrcoef(y_test,y_pred_rfc1))\n    print(\"The accuracy Random forest classifier on testing data is: \",100.0 *accuracy_score(y_test,y_pred_rfc1))\n    return;\n\ndef SVM_C(x_train, y_train, x_test, y_test):\n    #create SVM object\n    svc = SVC()\n    svc.fit(x_train,y_train)\n    y_pred_svc = svc.predict(x_test)\n    cm=confusion_matrix(y_test,y_pred_svc)\n    sns.heatmap(cm,annot=True)\n    print(\"f1 score is \",f1_score(y_test,y_pred_svc,average='weighted'))\n    print(\"matthews correlation coefficient is \",matthews_corrcoef(y_test,y_pred_svc))\n    print(\"The accuracy SVC on testing data is: \",100.0 *accuracy_score(y_test,y_pred_svc))\n    return;\ndef LogReg(x_train, y_train, x_test, y_test):\n    LogReg1=LogisticRegression(random_state= 0, multi_class='multinomial' , solver='newton-cg')\n    #Train the model using training data \n    LogReg1.fit(x_train,y_train)\n    #Test the model using testing data\n    y_pred_log = LogReg1.predict(x_test)\n    cm=confusion_matrix(y_test,y_pred_log)\n    sns.heatmap(cm,annot=True)\n    print(\"f1 score is \",f1_score(y_test,y_pred_log,average='weighted'))\n    print(\"matthews correlation coefficient is \",matthews_corrcoef(y_test,y_pred_log))\n    print(\"The accuracy Logistic Regression on testing data is: \",100.0 *accuracy_score(y_test,y_pred_log))\n    return;","814f8660":"RFC(x_train=x_train[variable],y_train=y_train,x_test=x_test[variable],y_test=y_test)\n#SVM_C(x_train=x_train[variable],y_train=y_train,x_test=x_test[variable],y_test=y_test)","ce1f525f":" variable.remove(\"nosOfSubdomain\")","910be550":"RFC(x_train=x_train[variable],y_train=y_train,x_test=x_test[variable],y_test=y_test)","bea4aa0e":"X= df.drop(['label', 'domain'], axis=1).values\nY= df.label.values\nfrom sklearn.preprocessing import StandardScaler \nx_std = StandardScaler().fit_transform(X)\n#X_std values are standardized in the range of -1 to +1.\nx_std","b9825f96":"mean_vec = np.mean(x_std, axis=0) \ncov_mat = (x_std - mean_vec).T.dot((x_std - mean_vec)) \/ (x_std.shape[0]-1) \n#print('Covariance matrix \\n%s' %cov_mat) \nprint('Covariance matrix \\n') \ncov_mat= np.cov(x_std, rowvar=False) \ncov_mat","c3c949ea":"cov_mat = np.cov(x_std.T) \neig_vals, eig_vecs = np.linalg.eig(cov_mat) \nprint('Eigenvectors \\n%s' %eig_vecs) \nprint('\\nEigenvalues \\n%s' %eig_vals)","c8d7255f":"sq_eig=[] \nfor i in eig_vecs: \n    sq_eig.append(i**2)\n    print(sq_eig) \nsum(sq_eig) \nprint(\"sum of squares of each values in an eigen vector is \\n\", 0.27287211+ 0.13862096+0.51986524+ 0.06864169) \nfor ev in eig_vecs: np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))","338e53f9":"eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))] \n#print(type(eig_pairs))\n#Sort the (eigenvalue, eigenvector) tuples from high to low eig_pairs.sort() \neig_pairs.sort() \neig_pairs.reverse() \n#Visually confirm that the list is correctly sorted by decreasing eigenvalues \nprint('\\n\\n\\nEigenvalues in descending order:') \nfor i in eig_pairs: \n    print(i[0])","e29b050c":"tot = sum(eig_vals) \nprint(\"\\n\",tot) \nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)] \nprint(\"\\n\\n1. Variance Explained\\n\",var_exp) \ncum_var_exp = np.cumsum(var_exp) \nprint(\"\\n\\n2. Cumulative Variance Explained\\n\",cum_var_exp) \nprint(\"\\n\\n3. Percentage of variance the first 2 principal components each contain\\n \",var_exp[0:2]) \nprint(\"\\n\\n4. Percentage of variance the first 2 principal components together contain\\n\",sum(var_exp[0:2]))","a2d40fa5":"#print(eig_pairs[i][1] for i in range(0,5)) \n#print(eig_pairs[2][1]) \nmatrix_w = np.hstack((eig_pairs[0][1].reshape(10,1), eig_pairs[1][1].reshape(10,1)))  #hstack: Stacks arrays in sequence horizontally (column wise). print('Matrix W:\\n', matrix_w)\nprint('Matrix W:\\n', matrix_w)","0fff5d39":"a = x_std.dot(matrix_w) \nprincipalDf = pd.DataFrame(data = a , columns = ['principal component 1', 'principal component 2']) \nprincipalDf.head()","776cd656":"finalDf = pd.concat([principalDf,pd.DataFrame(Y,columns = ['label'])], axis = 1) \nfinalDf.head()\nfinalDf.isna().sum()","3872a74a":"#Dropping null value\nfinalDf.dropna(inplace=True)\nsns.countplot(finalDf['label'])","d71485df":"X=finalDf.drop(['label'], axis=1)\nY=finalDf.label\nx_train_pc, x_test_pc, y_train_pc, y_test_pc = train_test_split(X,Y,test_size=0.40)\nprint(\"Training set has {} samples.\".format(x_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(x_test.shape[0]))\n","5e51232b":"LogReg(x_train=x_train_pc,y_train=y_train_pc,x_test=x_test_pc,y_test=y_test_pc)\n#RFC(x_train=x_train_pc,y_train=y_train_pc,x_test=x_test_pc,y_test=y_test_pc)\n","17d82d31":"fig = plt.figure(figsize = (8,5)) \nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15) \nax.set_ylabel('Principal Component 2', fontsize = 15) \nax.set_title('2 Component PCA', fontsize = 20) \ntargets = [0, 1] \ncolors = ['r', 'g', 'b'] \nfor target, color in zip(targets,colors): \n    indicesToKeep = finalDf['label'] == target  \n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1'] , finalDf.loc[indicesToKeep, 'principal component 2'] , c = color , s = 50) \n    ax.legend(targets) \n    ax.grid()\n    #print(indicesToKeep)","55a160fa":"pca = PCA(n_components=2) \n# Here we can also give the percentage as a paramter to the PCA function as pca = PCA(.95). .95 means that we want to include 95% of the variance. Hence PCA will return the no of components which describe 95% of the variance. However we know from above computation that 2 components are enough so we have passed the 2 components.\nprincipalComponents = pca.fit_transform(x_std) \nprincipalDDf = pd.DataFrame(data = principalComponents , columns = ['principal component 1', 'principal component 2'])\nfinalDDf = pd.concat([principalDDf, pd.DataFrame(Y,columns = ['label'])], axis = 1)\nfinalDDf.head(5) # prints the top 5 rows","11b3ff83":"### Create a covariance matrix for Eigen decomposition","a27db751":"- We can Select the feature by concedering the corelation factor with targate variable\n- We can also drop the features if both are genrating same effoct on targate or having high correation factor between them","66983b54":"- No null Value","fe939085":"## Linear Dimensionality Reduction Methods\n- PCA (Principal Component Analysis) : Popularly used for dimensionality reduction in continuous data, PCA rotates and projects data along the direction of increasing variance. The features with the maximum variance are the principal components.\n- Factor Analysis : a technique that is used to reduce a large number of variables into fewer numbers of factors. The values of observed data are expressed as functions of a number of possible causes in order to find which are the most important. The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise.\n- LDA (Linear Discriminant Analysis): projects data in a way that the class separability is maximised. Examples from same class are put closely together by the projection. Examples from different classes are placed far apart by the projection","27463fc4":"## Loding complete data in Panda's Dataframe","f91990cb":"### Steps Involved in PCA\n\n    - Standardize the data. (with mean =0 and variance = 1)\n    - Compute the Covariance matrix of dimensions.\n    - Obtain the Eigenvectors and Eigenvalues from the covariance matrix (we can also use correlation matrix or even Single value decomposition, however in this post will focus on covariance matrix).\n    - Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues (k will become the number of dimensions of the new feature subspace k\u2264d, d is the number of original dimensions).\n    - Construct the projection matrix W from the selected k Eigenvectors.\n    - Transform the original data set X via W to obtain the new k-dimensional feature subspace Y.","74ca1820":"# Importing Required Libraries\n\n- **Pandas** :  For data processing, CSV file I\/O (e.g. pd.read_csv)\n- **Numpy**  :  For linear algebra\n- **Matplotlib** : For Data visualization\n- **sklearn.model_selection**  : For spliting data in Train & Test\n- **sklearn.linear_mode.LogisticRegression**   : For Logistic Regression \n- **sklearn.metrics**  : Evaluation metrics ","35be7031":"### Explained Variance ","7db2d867":"# Dimensionality Reduction\n\n","87fbe72a":"- PCA is not helpful as already dimenssion is less","35e4aded":"# EDA","8708ccbe":"### Sorting eigenvalues","71f469f2":"### Construct the projection matrix W from the selected k eigenvectors","5afb645d":"## Feature Selection Methods","3441ffa1":"### Standardization of data\n![title](https:\/\/miro.medium.com\/max\/700\/0*IQtYrjdIiNl88F7K)","d9d7724d":"- Feature selection by using Filter Methods is not provided any rise in our accuracy ","50f45dc5":"Feature selection is the process of identifying and selecting relevant features for your sample. Feature engineering is manually generating new features from existing features, by applying some transformation or performing some operation on them.","749e37d5":"-- Heatmaps that show the correlation between features is a good idea.","6c13ca32":"The description of data are as follows:\n- Domain: The URL itself.\n- Ranking: Page Ranking\n- isIp: Is there an IP address in the weblink\n- valid: This data is fetched from google's whois API that tells us more about the current status of the URL's registration.\n- activeDuration: Also from whois API. Gives the duration of the time since the registration up until now.\n- urlLen: It is simply the length of the URL\n- is@: If the link has a '@' character then it's value = 1\n- isredirect: If the link has double dashes, there is a chance that it is a redirect. 1-> multiple dashes present together.\n- haveDash: If there are any dashes in the domain name.\n- domainLen: The length of just the domain name.\n- noOfSubdomain: The number of subdomains preset in the URL.\n- Labels: 0 -> Legitimate website , 1 -> Phishing Link\/ Spam Link","6604f900":"## Prepration Of Data","7a82751e":"###  Filter Methods\n\n\n![title](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/08\/Screenshot-from-2018-08-10-12-07-43.png)\n\n![title](https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/11\/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png)\n\n-- Filter feature selection methods apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable","33b458a7":"- Result\/target are distribute in aprox 4-6 ratio"}}