{"cell_type":{"235ae1c4":"code","75df3133":"code","198ac8e7":"code","47dfbc9c":"code","0bfdbe99":"code","3d3c8346":"code","40690c25":"code","f000bf46":"code","b3df8b03":"code","62034e8e":"code","e8f25d7e":"code","31cce38f":"code","7885bc43":"code","1343e1bb":"code","2b63f84d":"code","658bc9b5":"code","77d3911b":"code","71edecbc":"code","08a8d672":"code","43609040":"code","45ccdfe0":"code","8d2d48bf":"code","3830b50f":"code","b37d7cd1":"code","db145f24":"code","9d6ec952":"code","2d655dbd":"code","e0caecaa":"code","5f7b14a2":"code","f24a0220":"code","693d682c":"markdown","594f5613":"markdown","c89faf63":"markdown","9149fc98":"markdown","8f13cfc0":"markdown","7218ddce":"markdown","68691153":"markdown","16278ff9":"markdown","d281e6b8":"markdown","a0457b88":"markdown","eab0367e":"markdown","3bfa560f":"markdown","00d65542":"markdown","637baf71":"markdown","77ba6efa":"markdown","dea8c7c1":"markdown","1eeac6b9":"markdown","229fed30":"markdown","477c6bf9":"markdown","be0474f4":"markdown","bcda7b7e":"markdown","5b8df057":"markdown","50cbcefe":"markdown","54949fab":"markdown","0021b6c7":"markdown","d3a74877":"markdown","3f8d8e48":"markdown","be42ee6b":"markdown","8b7a93d5":"markdown","51c3f677":"markdown","0f536c2b":"markdown","c378a10e":"markdown","11ff0a3c":"markdown","cf11050b":"markdown"},"source":{"235ae1c4":"# !pip install newspaper3k","75df3133":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.metrics import mean_squared_error as RMSE\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom newspaper import Article\nimport requests\nfrom bs4 import BeautifulSoup","198ac8e7":"data = pd.read_csv(\"..\/input\/uci-online-news-popularity-data-set\/OnlineNewsPopularity.csv\")","47dfbc9c":"def cleancols(cols):\n    x = [y.lower().strip() for y in cols]\n    return x","0bfdbe99":"data.columns = cleancols(data.columns)\ny_labels = data['shares'] #the feature to be predicted\ndf = data.drop(columns = ['url','timedelta','shares'],axis = 1) #url and timedelta are of no use to us","3d3c8346":"url = \"https:\/\/www.bbc.com\/news\/world\"\npage = requests.get(url)\nsoup = BeautifulSoup(page.content,'html.parser')\nimgnews = soup.findAll('a',attrs = {'class':'gs-c-promo-heading gs-o-faux-block-link__overlay-link gel-pica-bold nw-o-link-split__anchor'})","40690c25":"newsarts = []\nfor arts in imgnews:\n    newsarts.append(\"https:\/\/www.bbc.com\"+arts['href'])","f000bf46":"pca = PCA(n_components = 12)\ndf_scaled = pd.DataFrame(preprocessing.scale(df),columns = df.columns)\ndf_red = pca.fit_transform(df_scaled) #applying PCA on the standardized data\nexplainedfeats = pd.DataFrame(pca.components_,index = ['PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8','PC-9','PC-10','PC-11','PC-12'],columns = df.columns)","b3df8b03":"xgb = XGBRegressor(max_depth = 10,random_state = 42)\nxgb.fit(df,y_labels)\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nimpplot = plot_importance(xgb,ax = ax)\nplt.show()","62034e8e":"impfeats = [impplot.get_yticklabels()[::-1][i].get_text() for i in range(0,20)]\nprint(impfeats)","e8f25d7e":"from nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\nstopwords=set(stopwords.words('english'))\nfrom textblob import TextBlob #for subjectivity and polarity purpose","31cce38f":"def tokenizetext(text):\n    return word_tokenize(text)\ndef words(text):\n    l = [word for word in word_tokenize(text) if word.isalpha()]\n    return l\ndef unique_words(text):\n    return list(set(words(text)))\ndef rate_uni_words(text):\n    uni_words = len(unique_words(text))\/len(words(text))\n    return uni_words\ndef avglengthtoken(text):\n    w = words(text)\n    sum = 0\n    for item in w:\n        sum+=len(item)\n    avglen = sum\/len(w)\n    return avglen\ndef n_non_stop_unique_tokens(text):\n    uw = unique_words(text)\n    n_uw = [item for item in uw if item not in stopwords]\n    w = words(text)\n    n_w = [item for item in w if item not in stopwords]\n    rate_nsut = len(n_uw)\/len(n_w)\n    return rate_nsut\ndef numlinks(article):\n    return len(BeautifulSoup(sampletext.html).findAll('link'))\ndef get_subjectivity(a_text):\n    return a_text.sentiment.subjectivity\ndef get_polarity(a_text):\n    return a_text.sentiment.polarity\ndef word_polarity(words):\n    pos_words = []\n    ppos_words = [] # polarity of pos words\n    neg_words = []\n    pneg_words = [] # polarity of negative words\n    neu_words = []\n    pneu_words = [] # polarity of neutral words\n    for w in words:\n        an_word = TextBlob(w)\n        val = an_word.sentiment.polarity\n        if val > 0:\n            pos_words.append(w)\n            ppos_words.append(val)\n        if val < 0:\n            neg_words.append(w)\n            pneg_words.append(val)\n        if val == 0 :\n            neu_words.append(w)\n            pneu_words.append(val)\n    return pos_words,ppos_words,neg_words,pneg_words,neu_words,pneu_words\ndef avg_pol_pw(text):    \n    totalwords = words(text)\n    res = word_polarity(totalwords)\n    return np.sum(res[1])\/len(res[0])\ndef avg_pol_nw(text):    \n    totalwords = words(text)\n    res = word_polarity(totalwords)\n    return np.sum(res[3])\/len(res[2])","7885bc43":"finrows = []\nfor article in newsarts[0:25]:\n    sampletext = Article(article, language = 'en')\n    sampletext.download()\n    sampletext.parse()\n    sampletext.nlp() \n    \n    row = {}\n    row['n_tokens_title'] = len(words(sampletext.title))\n    row['n_tokens_content'] = len(words(sampletext.text))\n    row['n_unique_tokens'] = len(unique_words(sampletext.text))\n    row['average_token_length'] = avglengthtoken(sampletext.text)\n    row['n_non_stop_unique_tokens'] = n_non_stop_unique_tokens(sampletext.text)\n    row['num_hrefs'] = numlinks(sampletext)\n    \n    analysed_text = TextBlob(sampletext.text)\n    row['global_subjectivity'] = get_subjectivity(analysed_text)\n    row['avg_positive_polarity'] = avg_pol_pw(sampletext.text)\n    row['global_sentiment_polarity'] = get_polarity(analysed_text)\n    finrows.append(row)\n#converting the list to a dataframe\nmasterdf = pd.DataFrame(finrows, columns = ['n_tokens_title','n_tokens_content','n_unique_tokens','average_token_length','n_non_stop_unique_tokens','num_hrefs','global_subjectivity',\n                                   'avg_positive_polarity','global_sentiment_polarity'])","1343e1bb":"df_reduced = df[masterdf.columns]","2b63f84d":"xtrain, xtest, ytrain, ytest = train_test_split(df_reduced, y_labels, test_size = 0.2, shuffle = True, random_state = 42)","658bc9b5":"xgb2 = XGBRegressor(random_state = 42)\nparamsxgb = {'max_depth':[5,20,50,100]}\ngsc = GridSearchCV(estimator = xgb2,param_grid = paramsxgb, cv = 3, scoring = 'neg_root_mean_squared_error')\ngscres = gsc.fit(xtrain,ytrain)\ngscres.best_params_","77d3911b":"xgb2.max_depth = gscres.best_params_['max_depth']\nboost = ['gbtree','gblinear']\nrmsescores = {}\nfor b in boost:\n    xgb2.booster = b\n    xgb2.fit(xtrain,ytrain)\n    predicted = xgb2.predict(xtest)\n    rmsescores['xgb-'+b] = RMSE(ytest,predicted,squared = False)\n    print(\"RMSE error with {} booster is {} :\".format(b,RMSE(ytest,predicted,squared = False)))","71edecbc":"xgb2.booster = 'gblinear'","08a8d672":"rf = RandomForestRegressor(random_state = 42)\nparamsrf = {'max_depth':[5,20,50,100]}\ngsc = GridSearchCV(estimator = rf,param_grid = paramsrf, cv = 3, scoring = 'neg_root_mean_squared_error')\ngscres = gsc.fit(xtrain,ytrain)\ngscres.best_params_","43609040":"rf.max_depth = gscres.best_params_['max_depth']\nrf.fit(xtrain,ytrain)\npredictedrf = rf.predict(xtest)\nrmsescores['rf'] = RMSE(ytest,predictedrf,squared = False)\nprint(\"RMSE error with Random Forest Regressor is {} :\".format(RMSE(ytest,predicted,squared = False)))","45ccdfe0":"lr = RidgeCV(alphas = [0.001,0.1,1,5,10,100],scoring = 'neg_root_mean_squared_error', cv = None, store_cv_values = True)\nlr.fit(xtrain,ytrain)","8d2d48bf":"predictedlr = lr.predict(xtest)\nrmsescores['ridgecv'] = RMSE(ytest,predictedlr,squared = False)\nprint(\"RMSE error with Linear Regression via RidgeCV is {} :\".format(RMSE(ytest,predictedlr,squared = False)))","3830b50f":"cb = CatBoostRegressor(verbose = 0,random_state = 42,eval_metric = 'RMSE')\nparamscb = {'iterations':[1,10,50,100],'learning_rate':[0.03,0.1,0.5,1],'depth':[3,5,8,10]}\ngsc = GridSearchCV(estimator = cb,param_grid = paramscb, cv = 3, scoring = 'neg_root_mean_squared_error')\ngscres = gsc.fit(xtrain,ytrain)\ngscres.best_params_","b37d7cd1":"cb.iterations = gscres.best_params_['iterations']\ncb.learning_rate = gscres.best_params_['learning_rate']\ncb.depth = gscres.best_params_['depth']\ncb.fit(xtrain,ytrain)","db145f24":"predictedcb = cb.predict(xtest)\nrmsescores['catboost'] = RMSE(ytest,predictedcb,squared = False)\nprint(\"RMSE error with CatBoost is {} :\".format(RMSE(ytest,predictedcb,squared = False)))","9d6ec952":"ax = sns.barplot(x = list(rmsescores.keys()),y = list(rmsescores.values()))\nax.set_ylim(10750,12000)\nax.set_xlabel('Regressors')\nax.set_ylabel('RMSE Scores')\nplt.show()","2d655dbd":"predlr = lr.predict(masterdf)\nlrdata = {'Links':list(newsarts[:25]),'Predicted Virality':list(predlr)}\npd.DataFrame(lrdata).reindex(np.arange(0,25,1))","e0caecaa":"predcb = cb.predict(masterdf)\ncbdata = {'Links':list(newsarts[0:25]),'Predicted Virality':list(predcb)}\npd.DataFrame(cbdata).reindex(np.arange(0,25,1))","5f7b14a2":"predrf = rf.predict(masterdf)\nrfdata = {'Links':list(newsarts[0:25]),'Predicted Virality':list(predrf)}\npd.DataFrame(rfdata).reindex(np.arange(0,25,1))","f24a0220":"predxgb = xgb2.predict(masterdf)\nxgbdata = {'Links':list(newsarts[0:25]),'Predicted Virality':list(predxgb)}\npd.DataFrame(xgbdata).reindex(np.arange(0,25,1))","693d682c":"# 2. RandomForestRegressor","594f5613":"# Defining functions that would return feature values","c89faf63":"# Importing libraries for analyzing the article text","9149fc98":"# Creating dataset with news articles ","8f13cfc0":"## Best depth is found to be 5 with RMSE score ~ 10900, similar to XGBoost with gblinear regressor","7218ddce":"# Scraping news articles from the web","68691153":"## Catboost gives the second worse RMSE ~ 11200 after xgboost-via-gbtree whose RMSE was ~11430","16278ff9":"# However, before we predict we cannot ascertain if the prediction is right or not","d281e6b8":"## Best depth is found to be 5 and on testing setting the booster to 'gblinear' gives lower RMSE score ~ 10900 compared to 'gbtree' with a RMSE score ~ 11430. Hence, I have set the booster to 'gblinear'","a0457b88":"### Using XGBoost to find the absolute importance of each feature. This is done so that we can minimize our effort of making features while converting extracted news article in the format of the Online News Popularity Dataset","eab0367e":"# 3. Linear Regression (RidgeCV): linear regression with l2 regularization and in-built CV","3bfa560f":"# Splitting the UCI dataset into training and testing","00d65542":"### We use the BBC News website for testing our model that would be trained on UCI dataset. We observe the feature that we find useful, here the specific 'class' attribute","637baf71":"# 1. XGBoost ","77ba6efa":"# REFERENCES:\n\n* http:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\n* https:\/\/newspaper.readthedocs.io\/en\/latest\/\n* https:\/\/www.geeksforgeeks.org\/newspaper-article-scraping-curation-python\/\n* https:\/\/machinelearningmastery.com\/clean-text-machine-learning-python\/\n* https:\/\/textblob.readthedocs.io\/en\/dev\/quickstart.html#sentiment-analysis\n* https:\/\/scikit-learn.org\/stable\/index.html","dea8c7c1":"# FINAL PREDICTION","1eeac6b9":"## 1. Other features, with low importance, could be experimented with.\n## 2. Used prediction models could be improvised with time","229fed30":"# EVALUATION RESULTS","477c6bf9":"# Converting UCI dataset","be0474f4":"# Installing the newspaper3k library","bcda7b7e":"# REGRESSOR TESTING","5b8df057":"# 4. CatBoost","50cbcefe":"### Out of these, let us try to construct the features which are defined clearly in the details of the UCI dataset website.\n### 9 features could be figured out.","54949fab":"## RMSE score for RidgeCV regression is found to be lowest compared to XGBoost and Random Forest Regressor","0021b6c7":"# Cleansing the column names","d3a74877":"# OBSERVATION\n\n## On a whole, we observe that RidgeCV gives us predictions that are more than 10x the predictions by other models, on an average.","3f8d8e48":"### Creating the list of news articles","be42ee6b":"## We would therefore prefer using RidgeCV for prediction.","8b7a93d5":"# Importing libraries","51c3f677":"# FURTHER IMPROVEMENTS","0f536c2b":"## This dataset will now be used in the end for prediction purpose","c378a10e":"### Finding the top 20 important features: (DNC : did not consider as unable to figure out calculation steps for that feature )\n*   n_tokens_title : Number of words in the title\n*   n_tokens_content: Number of words in the content \n*   n_unique_tokens : Rate of unique words in the content ( #unique words \/ #words )\n*   average_token_length : Average length of the words in the content \n*   n_non_stop_unique_tokens : Rate of unique non-stop words in the content\n*   num_hrefs : Number of links\n*   global_subjectivity : subjectivity of article content\n*   avg_positive_polarity : Average polarity of positive words\n*   global_sentiment_polarity : Text sentiment polarity\n*   kw_max_avg : DNC\n*   kw_avg_avg : DNC\n*   kw_avg_max : DNC\n*   kw_max_min : DNC\n*   lda_03 : DNC\n*   self_reference_min_shares : DNC\n*   lda_00 : DNC\n*   lda_01 : DNC\n*   lda_04 : DNC\n*   kw_avg_min : DNC\n*   lda_02 : DNC","11ff0a3c":"### Before we move on to extraction of information, we need to figure out the features of the UCI Dataset that we would be creating, out the extracted news, for prediction purpose","cf11050b":"# Importing dataset: Online news popularity from UCI library"}}