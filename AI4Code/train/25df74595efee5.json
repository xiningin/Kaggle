{"cell_type":{"43167c01":"code","0abc7a68":"code","7a9c64ac":"code","296fb319":"code","6f801416":"code","9bba1134":"code","796520cb":"code","b5058ead":"code","cc161ecd":"code","545f9be0":"code","df1ab31f":"code","544a7d14":"code","ea4df09b":"code","384e0843":"code","d373d695":"code","4e756ae9":"code","b8a24c34":"code","53365cb6":"code","0a907cc4":"code","39dbb43e":"code","b13b0c7f":"code","9b7fb09d":"code","850a2569":"code","f2a3cf3e":"code","f707db75":"code","9118bf67":"code","dc90b9cc":"code","b6dd908b":"code","8723f08f":"code","1002fde2":"code","14985362":"code","0b24e42c":"code","70d1080a":"markdown","092604cf":"markdown","5962a3e2":"markdown","de311c27":"markdown","17e389c7":"markdown","1fc5968d":"markdown","81c4a929":"markdown","06346217":"markdown","dddf2c8d":"markdown","3be5f3c7":"markdown","15ab4c7a":"markdown","5da6c929":"markdown","2885ac93":"markdown","c55eb4f9":"markdown"},"source":{"43167c01":"!pip install transformers==4.12.5","0abc7a68":"\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup, AutoModel, AutoTokenizer \n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\n#to_preprocessing\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm, trange,tnrange,tqdm_notebook\nimport random\nimport os\nimport io\n%matplotlib inline\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","7a9c64ac":"# identify and specify the GPU as the device, later in training loop we will load data into device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)","296fb319":"# Hugging face dataset\n# df_train = pd.read_csv(\"\/kaggle\/input\/emotions-dataset-for-nlp\/train.txt\", delimiter=';', header=None, names=['Text','emotion'])\n# df_test = pd.read_csv(\"\/kaggle\/input\/emotions-dataset-for-nlp\/test.txt\", delimiter=';', header=None, names=['Text','emotion'])\n# df_val = pd.read_csv(\"\/kaggle\/input\/emotions-dataset-for-nlp\/val.txt\", delimiter=';', header=None, names=['Text','emotion'])\n# df = pd.concat([df_train, df_val, df_test], axis = 0)\n# Kaggle dataset\ndf = pd.read_csv(\"..\/input\/heyooo\/out.csv\")\ndf = df.dropna(axis=0)","6f801416":"# Decontract words im -> i am; didnt -> did not\ndef decontracted(phrase):\n    \"\"\"\n    We first define a function to expand the contracted phrase into normal words\n    \"\"\"\n        \n    phrase = re.sub(r\"wont\", \"will not\", phrase)\n    phrase = re.sub(r\"wouldnt\", \"would not\", phrase)\n    phrase = re.sub(r\"shouldnt\", \"should not\", phrase)\n    phrase = re.sub(r\"couldnt\", \"could not\", phrase)\n    phrase = re.sub(r\"cudnt\", \"could not\", phrase)\n    phrase = re.sub(r\"cant\", \"can not\", phrase)\n    phrase = re.sub(r\"dont\", \"do not\", phrase)\n    phrase = re.sub(r\"doesnt\", \"does not\", phrase)\n    phrase = re.sub(r\"didnt\", \"did not\", phrase)\n    phrase = re.sub(r\"wasnt\", \"was not\", phrase)\n    phrase = re.sub(r\"werent\", \"were not\", phrase)\n    phrase = re.sub(r\"havent\", \"have not\", phrase)\n    phrase = re.sub(r\"hadnt\", \"had not\", phrase)\n    phrase = re.sub(r\"neednt\", \"need not\", phrase)\n    phrase = re.sub(r\"isnt\", \"is not\", phrase)\n    phrase = re.sub(r\"arent\", \"are not\", phrase)\n    phrase = re.sub(r\"hasnt\", \"are not\", phrase)\n    \n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n# Get rid of user handles, tags, link, punctuation\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = decontracted(text)\n    return text","9bba1134":"# Apple cleaning function for the dataframe\ndf[\"Text\"] = df[\"Text\"].apply(lambda x: clean_text(x))","796520cb":"# Transform emotion label into encoded label\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf['label_enc'] = labelencoder.fit_transform(df['emotion'])\ndf.rename(columns={'label':'label_desc'},inplace=True)\ndf.rename(columns={'label_enc':'label'},inplace=True)","b5058ead":"# Split dataset\ntrain_text, val_text, train_labels, val_labels = train_test_split(df['Text'], df['label'], \n                                                                    random_state=2021, \n                                                                    test_size=0.1, \n                                                                    stratify=df['label'])\n","cc161ecd":"# from imblearn.over_sampling import RandomOverSampler\n# ros = RandomOverSampler(random_state=1)\n# train_text, train_labels = ros.fit_resample(np.array(train_text).reshape(-1, 1), np.array(train_labels).reshape(-1, 1))\n# t = [s[0] for s in train_text.tolist()]\n# train_text = t","545f9be0":"MAX_LEN = 256\n#change to AutoTokenizer if train Bertweet\n# tokenizer = AutoTokenizer.from_pretrained(\"vinai\/bertweet-base\", use_fast=False)\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", lower = True)","df1ab31f":"# tokenize and encode sequences in the training set (for Bertbase fine tuning)\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = MAX_LEN,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = MAX_LEN,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n# For Bertweet fine tuning\n# # tokenize and encode sequences in the training set\n# tokens_train = tokenizer.batch_encode_plus(\n#     train_text,\n#     padding='longest',\n#     truncation=True,\n#     return_token_type_ids=False\n# )\n\n# # tokenize and encode sequences in the validation set\n# tokens_val = tokenizer.batch_encode_plus(\n#     val_text.tolist(),\n#     padding='longest',\n#     truncation=True,\n#     return_token_type_ids=False\n# )\n","544a7d14":"# for train set\ntrain_inputs = torch.tensor(tokens_train['input_ids'])\ntrain_y = torch.tensor(train_labels.tolist())\ntrain_masks = torch.tensor(tokens_train['attention_mask'])\n\n\n# for validation set\nval_inputs = torch.tensor(tokens_val['input_ids'])\nval_y = torch.tensor(val_labels.tolist())\nval_masks = torch.tensor(tokens_val['attention_mask'])\n\nbatch_size = 32\ntrain_data = TensorDataset(train_inputs,train_masks,train_y)\ntrain_sampler =  RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_inputs,val_masks,val_y)\nval_sampler =  RandomSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","ea4df09b":"from transformers import BertModel\n# Use AutoModel for Bertweet\n# bert = AutoModel.from_pretrained(\"vinai\/bertweet-base\").to(device)\nbert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)","384e0843":"class BERT_14(torch.nn.Module):\n\n    def __init__(self, bert):\n      super(BERT_14, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = torch.nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  torch.nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = torch.nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = torch.nn.Linear(512,6)\n\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n      #pass the inputs to the model  \n      output = self.bert(sent_id,attention_mask=mask)\n      x = output[0][:, 0, :]\n      x = self.fc1(x)\n\n      x = self.relu(x)\n\n      x = self.dropout(x)\n\n      # output layer\n      x = self.fc2(x)\n      \n#       # apply softmax activation\n#       x = F.softmax(x, dim = 1)\n\n      return x","d373d695":"model = BERT_14(bert)\nmodel.to(device)","4e756ae9":"epochs = 4\noptimizer = AdamW(model.parameters(),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\nnum_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=num_steps)","b8a24c34":"\ncriteria = torch.nn.CrossEntropyLoss()\n\ndef train(model, train_dataloader, val_dataloader, epochs):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch in range(epochs):\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        model.train()\n        best = 0\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            optimizer.zero_grad()\n\n            logits = model(b_input_ids, b_attn_mask)\n\n            loss = criteria(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            scheduler.step()\n            if step % 50 == 0:\n                # Calculate time elapsed for 20 batches\n\n                # Print training results\n                print(\"Train:Epoch \", epoch, \": \", batch_loss\/batch_counts) \n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss \/ len(train_dataloader)\n        val_loss, val_accuracy = evaluate(model, val_dataloader)\n        if val_accuracy > best:\n            best = val_accuracy\n            model_save_name = 'fineTuneModel.bin'\n            path = path_model = F'\/kaggle\/working\/{model_save_name}'\n            torch.save(model.state_dict(),path);\n        print(\"Val loss: \",val_loss,\"; Val accuracy: \", val_accuracy)\n\ndef evaluate(model, val_dataloader):\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = criteria(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","53365cb6":"train(model, train_dataloader, val_dataloader, epochs=4)","0a907cc4":"\nmodel_save_name = 'fineTuneModel.bin'\npath = F'\/kaggle\/working\/{model_save_name}'\ntorch.save(model.state_dict(),path);","39dbb43e":"model = BERT_14(bert)\nmodel.load_state_dict(torch.load(\".\/fineTuneModel.bin\"))\nmodel.to(device)","b13b0c7f":"save = []\nfor batch in val_dataloader:\n    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n    with torch.no_grad():\n        logits = model(b_input_ids, b_attn_mask)\n    predict = logits.to(\"cpu\").numpy()\n    predict = np.argmax(predict, axis=1).flatten()\n    labels_flat = b_labels.to(\"cpu\").numpy().flatten()\n    temp = pd.DataFrame({'Actual_class':labels_flat,'Predicted_class':predict})\n    save.append(temp)\n    ","9b7fb09d":"# save = []\n# for batch in val_dataloader:\n#     b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n#     with torch.no_grad():\n#         logits = model(b_input_ids, b_attn_mask)\n#     predict = logits.to(\"cpu\").numpy()\n#     predict = np.argmax(predict, axis=1).flatten()\n#     labels_flat = b_labels.to(\"cpu\").numpy().flatten()\n#     temp = pd.DataFrame({'Actual_class':labels_flat,'Predicted_class':predict})\n#     save.append(temp)","850a2569":"## emotion labels\nlabel2int = {\n  \"sadness\": 4,\n  \"joy\": 2,\n  \"anger\": 0,\n  \"fear\": 1,\n  \"surprise\": 5,\n  \"love\": 3\n}","f2a3cf3e":"df_metrics = pd.concat(save, axis = 0)\ndf_metrics = df_metrics.reset_index()","f707db75":"print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))","9118bf67":"from sklearn.metrics import accuracy_score,matthews_corrcoef\nprint(accuracy_score(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values))\nprint(matthews_corrcoef(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values))","dc90b9cc":"# from sklearn.metrics import classification_report\n# with torch.no_grad():\n#   preds = model(val_inputs.to(device),val_masks.to(device))\n#   preds = preds.detach().cpu().numpy()\n# preds = np.argmax(preds, axis = 1)\n# print(classification_report(val_y, preds))","b6dd908b":"from sklearn.metrics import confusion_matrix\ncf = confusion_matrix(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values)","8723f08f":"import seaborn as sns\nsns.heatmap(cf, annot=True)","1002fde2":"import numpy as np\nfrom datasets import load_metric\n\nmetric = load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","14985362":"def preprocessing(text):\n    input_id = tokenizer.encode(text, add_special_tokens=True,padding='longest',truncation=True,return_token_type_ids=False)\n    attention = [float(i>0) for i in input_id]\n    input_id = torch.tensor([input_id]).to(device)\n    attention = torch.tensor([attention]).to(device)\n    return input_id, attention","0b24e42c":"text = clean_text(\"I can tell her about the subject\")\ninput, attention = preprocessing(text)\nresult = model(input, attention)\nprint(result)","70d1080a":"# Create iterator of data with DataLoader","092604cf":"# Load model","5962a3e2":"# Training","de311c27":"# Preprocess","17e389c7":"# Read dataset","1fc5968d":"# Import necessary libraries","81c4a929":"# Add fully connected layer on top of the pretrained BERT","06346217":"# Quantitative evaluation","dddf2c8d":"# Qualitative evaluation","3be5f3c7":"# Instantiate optimizer and scheduler ","15ab4c7a":"# Load model","5da6c929":"# Tokenizer","2885ac93":"# Generate more data by RandomOverSampler","c55eb4f9":"# Save model"}}