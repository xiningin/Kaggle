{"cell_type":{"510c1e43":"code","d62bc5e7":"code","02904a5c":"code","12cbdb39":"code","c65b1cb4":"code","abba7668":"code","3b0822b5":"code","86a0d994":"code","6800c207":"code","9a70f384":"code","2e152c7a":"code","006a0a9b":"code","0644799f":"code","6fd3dfdf":"code","2e1e7208":"code","56196286":"code","5ec49b6d":"code","8b5b3e31":"code","4085278e":"code","ce719dbf":"code","a79d9346":"code","2256dc15":"code","7527ed90":"code","6dff73f4":"code","f67c8354":"code","f3a35952":"code","e9539fed":"code","61e119b8":"markdown","eba7b07a":"markdown","c04704eb":"markdown","361818e4":"markdown","8cfb29b7":"markdown","b2078015":"markdown","49c679b9":"markdown","0f5f7c7c":"markdown","5c34af61":"markdown","de7523aa":"markdown","433d9b3f":"markdown","28a58b64":"markdown","612d1676":"markdown","f881c847":"markdown","244be802":"markdown","226dc546":"markdown","55251db2":"markdown","552d6576":"markdown"},"source":{"510c1e43":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Handle table-like data and matrices\nimport numpy as np # if you want to more learn numpy: https:\/\/github.com\/mukeshRar\/numpy\nimport pandas as pd # if you want more learn about pandas: https:\/\/github.com\/mukeshRar\/pandas_tutorial\n\n# Modelling Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n\n# Modelling Helpers\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import Normalizer , scale\nfrom sklearn.model_selection import train_test_split , StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n\n# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nsns.set_style( 'white' )\npylab.rcParams[ 'figure.figsize' ] = 8 , 6","d62bc5e7":"def plot_histograms( df , variables , n_rows , n_cols ):\n    fig = plt.figure( figsize = ( 16 , 12 ) )\n    for i, var_name in enumerate( variables ):\n        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n        df[ var_name ].hist( bins=10 , ax=ax )\n        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n        ax.set_xticklabels( [] , visible=False )\n        ax.set_yticklabels( [] , visible=False )\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()\n\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n\ndef plot_correlation_map( df ):\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n    \ndef describe_more( df ):\n    var = [] ; l = [] ; t = []\n    for x in df:\n        var.append( x )\n        l.append( len( pd.value_counts( df[ x ] ) ) )\n        t.append( df[ x ].dtypes )\n    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n    levels.sort_values( by = 'Levels' , inplace = True )\n    return levels\n\ndef plot_variable_importance( X , y ):\n    tree = DecisionTreeClassifier( random_state = 99 )\n    tree.fit( X , y )\n    plot_model_var_imp( tree , X , y )\n    \ndef plot_model_var_imp( model , X , y ):\n    imp = pd.DataFrame( \n        model.feature_importances_  , \n        columns = [ 'Importance' ] , \n        index = X.columns \n    )\n    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n    imp[ : 10 ].plot( kind = 'barh' )\n    print (model.score( X , y ))\n    ","02904a5c":"train= pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest= pd.read_csv(\"..\/input\/titanic\/test.csv\")\nfull= train.append(test, ignore_index=True)\ntitanic= full[:891]\n\nprint('Datasets:', 'full:', full.shape, 'titanic: ', titanic.shape)","12cbdb39":"titanic.head()","c65b1cb4":"titanic.isnull().sum()","abba7668":"titanic.describe()","3b0822b5":"plot_correlation_map(train)","86a0d994":"plot_distribution(titanic, var='Age', target='Survived', row ='Sex')","6800c207":"plot_distribution(titanic, var='Fare', target='Survived', row='Sex')","9a70f384":"plot_categories(titanic,cat='Embarked',target='Survived')","2e152c7a":"plot_categories(titanic,cat='Sex',target='Survived')","006a0a9b":"plot_categories(titanic,cat='Pclass',target='Survived')","0644799f":"plot_categories(titanic,cat='SibSp',target='Survived')","6fd3dfdf":"plot_categories(titanic,cat='SibSp',target='Survived')","2e1e7208":"#Transform sex into binary values 0 or 1:\nsex=pd.Series(np.where(full.Sex=='male',1,0),name='Sex')\n","56196286":"# create new variable for every unique value of embarked\nembarked=pd.get_dummies(full.Embarked,prefix='Embarked')\nembarked.head()","5ec49b6d":"# create new variable for pclass variable:\npclass=pd.get_dummies(full.Pclass,prefix='Pclass')\npclass.head()","8b5b3e31":"#create dataset\nimputed=pd.DataFrame()\nimputed['Age']=full.Age.fillna(full.Age.mean())\nimputed['Fare']=full.Fare.fillna(full.Fare.mean())\nimputed.head()","4085278e":"title=pd.DataFrame()\ntitle['Title']=full['Name'].map(lambda name: name.split(',')[1].split('.')[0].strip())\n\n# a map of more aggregated titles:\nTitle_Dictionary={\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"\n    }\n# we map each title:\ntitle['Title']=title.Title.map(Title_Dictionary)\ntitle=pd.get_dummies(title.Title)\ntitle.head()","ce719dbf":"cabin=pd.DataFrame()\ncabin['Cabin']=full.Cabin.fillna('U')\ncabin['Cabin']=cabin['Cabin'].map(lambda c:c[0])\n\n#dummy encoding\ncabin=pd.get_dummies(cabin['Cabin'],prefix='Cabin')\ncabin.head()","a79d9346":"# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\ndef cleanTicket(ticket):\n    ticket=ticket.replace('.','')\n    ticket=ticket.replace('\/','')\n    ticket=ticket.split()\n    ticket= map(lambda t: t.strip(), ticket)\n    ticket=list(filter(lambda t: not t.isdigit(),ticket))\n    if len(ticket)>0:\n        return ticket[0]\n    else:\n        return 'XXX'\n    \nticket=pd.DataFrame()\nticket['Ticket']=full['Ticket'].map(cleanTicket)\nticket=pd.get_dummies(ticket['Ticket'],prefix='Ticket')\n\nticket.shape\nticket.head()","2256dc15":"family=pd.DataFrame()\nfamily['FamilySize']=full['Parch']+full['SibSp']+1\nfamily['Family_Single']=family['FamilySize'].map(lambda s: 1 if s==1 else 0)\nfamily['Family_Small']=family['FamilySize'].map(lambda s:1 if 2<=s<=4 else 0)\nfamily['Family_Large']=family['FamilySize'].map(lambda s:1 if 5<=s else 0)\nfamily.head()","7527ed90":"# Select which features\/variables to include in the dataset from the list below:\n# imputed , embarked , pclass , sex , family , cabin , ticket\nfull_X=pd.concat([imputed,embarked,cabin,sex],axis=1)\nfull_X.head()","6dff73f4":"train_valid_X=full_X[0:891]\ntrain_valid_y=titanic.Survived\ntest_X=full_X[891:]\ntrain_X,valid_X,train_y,valid_y=train_test_split(train_valid_X,train_valid_y,train_size=.7)\nprint(full_X.shape, train_X.shape,valid_X.shape,train_y.shape,valid_y.shape,test_X.shape)","f67c8354":"# Selecting the optimal features in the model is important. We will now try to evaluate what the most important variables are for the model to make the prediction.\nplot_variable_importance(train_X, train_y)\n","f3a35952":"train_X.head()","e9539fed":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\ny = train_y\nX = pd.get_dummies(train_X)\nX_test = pd.get_dummies(test_X)\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions.astype(int)})\noutput.to_csv('my_submission1.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","61e119b8":"## Embarked:\n  We can also look at categorical variables like Embarked and their relationship with survival.\n\nC = Cherbourg\n\nQ = Queenstown\n\nS = Southampton","eba7b07a":"### Extract ticket class from ticket number: \n   ###### As we know higher class--> higher social status --> Priority to first save them.","c04704eb":"## Statistical summaries and visualisations\nTo understand the data we are now going to consider some key facts about variables including their relationship with the target variable, i.e. survival.","361818e4":"## Assemble final datasets for modelling\nSplit dataset by rows into test and train in order to have a holdout set to do model evaluation on. The dataset is also split by columns in a matrix (X) containing the input data and a vector (y) containing the target (or labels).","8cfb29b7":"## Load Data:","b2078015":"#### Fill missing values in variables\nMost machine learning alghorims require all variables to have values in order to use it for training the model. The simplest method is to fill missing values with the average of the variable across all observations in the training set.","49c679b9":"## Heatmap: A heat map of correlation may give us a understanding of which variables are important","0f5f7c7c":"## VARIABLE DESCRIPTION:\n\nWe have got a sense of our variables, their class type, and the first few observations of each. We know we're working with 1309 observations of 12 variables. To make things a bit more explicit since a couple of the variable names are not 100 percent illuminating, here's what we have go to deal with:\n\n### Variable Description\n\nSurvived: Survived (1) or died (0)\n\nPclass: Passenger's class\n\nName: Passenger's name\n\nSex: Passenger's sex\n\nTicket: Ticket number\n\nSibSp: Number of siblings\/spouses aboard\n\nParch: Number of parents\/children aboard\n\nFare: Fare\n\nCabin: Cabin\n\nEmbarked: Port of embarkation\n\nAge: Passenger's age","5c34af61":"### Create family size and category for family size\nThe two variables Parch and SibSp are used to create the famiy size variable","de7523aa":"### Extract Cabin category information from the Cabin number:","433d9b3f":"## Data Preparation:\n### Categorical variables need to be transformed to numeric variables\n The variables Embarked, Pclass and Sex are treated as categorical variables. Some of our model algorithms can only handle numeric values and   so we need to create a new variable (dummy) for every unique value of the categorical variables.\n\n This variable will have a value 1 if the row has a particular value and a value 0 if not. Sex is a old school gender theory and will be       encoded as 0 or 1.","28a58b64":"### Variable selection\nSelect which features\/variables to inculde in the dataset from the list below:\n\nimputed\n\nembarked\n\npclass\n\nsex\n\nfamily\n\ncabin\n\nticket","612d1676":"## Create datasets:\nBelow we will seperate the data into training and test datasets.","f881c847":"## Modeling:\nWe will now select a model we would like to try then use the training dataset to train this model and thereby check the performance of the model using the test set.\n\n### Model Selection:\nThen there are several options to choose from when it comes to models.","244be802":"##  Setup helper Functions:\nIf you are unable to understand then, there is no need to understand this code. Just run it to simplify the code later in the tutorial.","226dc546":"## Feature Engineering :","55251db2":"### Extract titles from passenger names:","552d6576":"## Let's further explore the relationship between the features and survival of passengers\n    We start by looking at the relationship between age and survival."}}