{"cell_type":{"2dc7a143":"code","a3415efc":"code","ec36c249":"code","862d9ff4":"code","3f2fbf95":"code","4a051da6":"code","606d4dbb":"code","810ecfe2":"code","07f3e65e":"code","74c57509":"code","370e77d8":"markdown","9d445ad7":"markdown","5e48d0c8":"markdown","fc8b8f70":"markdown","5b151e83":"markdown","44f49f43":"markdown","600fe4dd":"markdown","727d9006":"markdown"},"source":{"2dc7a143":"!pip install p_tqdm\n!pip install dateparser\n!pip install tqdm\n!pip install pandarallel\n!pip install nltk\n","a3415efc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\njson_files = [] #We store all the publications based json files into here.\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/CORD-19-research-challenge'):\n    for filename in filenames:\n        filepath = os.path.join(dirname, filename)\n        extension = os.path.splitext(filepath)[1]\n        if os.path.splitext(filepath)[1] == '.json':\n            json_files.append(filepath)\n\n# Any results you write to the current directory are saved as output.","ec36c249":"import os\nimport json\n\nfrom p_tqdm import p_map\n\nfrom dateparser.search import search_dates\nfrom pandarallel import pandarallel\npandarallel.initialize()\n\n#1. Process CORD19 Metadata\ncord19_metadata = pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\")\n\ndef extractDateTime(text):\n    if text == \"nan\":\n        text = str(2030)               #For publication with missing date, return 2030\n    thedate = search_dates(text)\n    return thedate[0][1]               #Return datetime\n\ncord19_metadata['publish_time_str'] = cord19_metadata['publish_time'].astype(str)\ncord19_metadata['publish_datetime'] = cord19_metadata['publish_time_str'].parallel_map(extractDateTime)\n\n#2. Process every json file into a new list of dataframe\npublications_df = pd.DataFrame()\n\n# to process all files, uncomment the next line and comment the line below\n# selected_json_files = json_files\nselected_json_files = json_files[0:5000]\n\nselected_columns = ['paper_id', 'metadata.title', 'body_text', 'abstract']\n\ndef newDF(file):\n    with open(file) as json_file:\n        json_data = json.load(json_file)\n        json_data_df = pd.io.json.json_normalize(json_data)\n        json_data_df_selected = json_data_df[selected_columns]\n        return json_data_df_selected\n\nlist_df = p_map(newDF, selected_json_files)             #Append every dataframe into a list\npublications_df = pd.concat(list_df)                    #Merge every dataframe in one big dataframe\n\n#3. Join publications_df + cord19_metdata based on sha (paper_id)\npublications_published_time_df = publications_df.merge(cord19_metadata[['sha','publish_datetime']], how='inner', left_on=\"paper_id\", right_on='sha')\n\n#Extract abstract and body text.\npublications_published_time_df['abstract_text'] = publications_published_time_df['abstract'].parallel_apply(lambda x: x[0]['text'] if x else \"\")\npublications_published_time_df['all_body_text'] = publications_published_time_df['body_text'].parallel_apply(lambda x: \" \".join([(t['text']) for t in x]))","862d9ff4":"import pickle\n#publications_published_time_df.to_csv(\"all_publications_time_text.csv\")\npickle.dump( publications_published_time_df, open( \"publications.p\", \"wb\" ) )","3f2fbf95":"publications_published_time_df.head()","4a051da6":"# timeunit = 1 #Represent one year, for now, forget about this\npublications_published_time_df['year'] = publications_published_time_df['publish_datetime'].parallel_apply(lambda x: x.year)\n\n#Text preprocessing\n#In the paper, we used nltk, but in Kaggle, we used spacy.\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nnlp = English()\ntokenizer = nlp.Defaults.create_tokenizer(nlp)\n\ndef textToListProcessing(text):\n    new_text = text.lower()\n    tokens = tokenizer(new_text)\n    tokens_list = [t.text for t in tokens]\n    return tokens_list\n\n\npublications_published_time_df['body_list_of_terms'] = publications_published_time_df['all_body_text'].parallel_apply(textToListProcessing)\npublications_published_time_df.head()\n\n\n","606d4dbb":"#Let's compare two years\n\nlist_of_texts_to_2010 = publications_published_time_df[publications_published_time_df['year'] < 2010 ]\nlist_of_texts_to_2020 = publications_published_time_df[publications_published_time_df['year'] < 2020 ]\n\nlist_of_texts_to_2010 = list_of_texts_to_2010[\"body_list_of_terms\"].tolist()\nlist_of_texts_to_2020 = list_of_texts_to_2020[\"body_list_of_terms\"].tolist()\n\n#Word2Vec\nfrom gensim.test.utils import common_texts, get_tmpfile\nimport multiprocessing\nfrom gensim.models import Word2Vec\nfrom time import time  # To time our operations\n\nw2v_model = Word2Vec(min_count=10,\n             window=2,\n             size=300,\n             sample=6e-5, \n             alpha=0.03, \n             min_alpha=0.0007, \n             negative=10,\n             workers=multiprocessing.cpu_count())\n\n#Update vocabulary\nw2v_model.build_vocab(list_of_texts_to_2010)\nw2v_model.build_vocab(list_of_texts_to_2020, update=True)\n\nw2v_model.train(list_of_texts_to_2010, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\nw2v_model.save(\"2010.w2v\")\nw2v_model.train(list_of_texts_to_2020, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\nw2v_model.save(\"2020.w2v\")\n","810ecfe2":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nstop_words = list(set(stopwords.words('english')))\nstop_words.extend([\"et\", \"al\", \"de\", \"fig\", \"en\", \"use\"])\n\n#%% Get embedding vector\nfrom scipy import spatial\n\ndef semanticDivergence(a, b):\n    cos_lib = 1 - spatial.distance.cosine(a, b)\n    return cos_lib\n\nmodel_2010 = Word2Vec.load(\"2010.w2v\")\nmodel_2020 = Word2Vec.load(\"2020.w2v\")\n\nlist_frequent_terms = model_2020.wv.index2entity\nlist_frequent_terms = [w for w in list_frequent_terms if not w in stop_words]\ntop1000terms = list_frequent_terms[:1000]\n\ntop1000terms_vector_2010 = [(w,model_2010.wv[w]) for w in top1000terms]\ntop1000terms_vector_2020 = [(w,model_2020.wv[w]) for w in top1000terms]\n\n#Get semantic divergence\nword_DV = []\nfor wv1, wv2 in zip(top1000terms_vector_2010, top1000terms_vector_2020):\n    dv = semanticDivergence(wv1[1], wv2[1])\n    word_DV.append((wv1[0], dv))\n\n#Sort based on highest divergence to lowest\nsorted_word_DV = sorted(word_DV, key=lambda tup: tup[1], reverse=True)\nsorted_word_DV[:10]\n\n","07f3e65e":"import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom sklearn.manifold import TSNE\nimport math\n\ndef visualizeclusterClosestWords_tsne(word, models, years):\n    Colors = []\n    Labels = []\n    Xs = []\n    Ys = []\n    \n    list_color = ['g','r']\n    for model, year, cr in zip(models, years, list_color):\n        vector_dim = model.vector_size\n        arr = np.empty((0,vector_dim), dtype='f')\n        theword = word + \"\\n(\" + str(year) + \")\"\n        word_labels = [theword]\n    \n        # get close words\n        close_words = model.wv.similar_by_word(word, topn=3)\n        \n        # add the vector for each of the closest words to the array\n        arr = np.append(arr, np.array([model[word]]), axis=0)\n        for wrd_score in close_words:\n            wrd_vector = model[wrd_score[0]]\n            word_labels.append(wrd_score[0])\n            arr = np.append(arr, np.array([wrd_vector]), axis=0)\n            \n        # find tsne coords for 2 dimensions\n        tsne = TSNE(n_components=2, random_state=0)\n        np.set_printoptions(suppress=True)\n        Y = tsne.fit_transform(arr)\n    \n        x_coords = Y[:, 0]\n        y_coords = Y[:, 1]\n        \n        colors = [ cr for i in range(len(x_coords))]\n        # colors[0] = 'r'\n        \n        #Append to list\n        Labels.append(word_labels)\n        Xs.append(x_coords)\n        Ys.append(y_coords)\n        Colors.append(colors)\n    \n    title = 'The semantic divergence for word \"' + word + '\"'\n    plt.title(title)\n    for xs, ys, labels, clrs in zip(Xs, Ys, Labels, Colors): \n        plt.scatter(xs, ys, color=clrs, s=300)\n        for label, x, y in zip(labels, xs, ys):\n            plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points') \n        plt.show()\n    \nvisualizeclusterClosestWords_tsne(sorted_word_DV[1][0], [model_2010, model_2020], [2010, 2020] )\n\n","74c57509":"#%% Cluster top m where m = 100   \nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nterms = [(w,model_2020.wv[w]) for w in top1000terms[:100]]\nterms_w =  [w for w,v in terms]\nterms_wv = [v for w,v in terms]\n\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(terms_wv)\ny_kmeans = kmeans.predict(terms_wv)\n\ncluster_to_list_terms_dict = defaultdict(list)\nfor k, term in zip(y_kmeans,terms_w):\n    cluster_to_list_terms_dict[k].append(term)\n\nfor k, v in cluster_to_list_terms_dict.items():\n    print(\"Cluster: \", k,v)\n    ","370e77d8":"Save the data into a file","9d445ad7":"This is my first notebook published in Kaggle. In this notebook, we are going to merge publish_time data from metadata.csv with every publication into one dataframe.","5e48d0c8":"Get frequent terms and compare its semantic divergence, such that term in 2010 vs term in 2020.","fc8b8f70":"**Temporal summarization\n**First, we are going to organize dafaframe into timeunit, t. For now, we specify t = 1 year. Then, perform several nlp preprocessing","5b151e83":"Merge JSON files and metadata into one dataframe. The new dataframe consists of ['paper_id', 'metadata.title', 'body_text', 'abstract', 'published_time', 'published_year']\n","44f49f43":"Perform kmeans clustering","600fe4dd":"References:\n\n[1]Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, and Slav Petrov. 2014.Temporal analysis of language through neural language models.arXiv preprintarXiv:1405.3515(2014).\n\n[2]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.Distributed representations of words and phrases and their compositionality. InAdvances in neural information processing systems. 3111\u20133119.\n\n[3]M Pope, PA Marsden, E Cole, S Sloan, LS Fung, Q Ning, JW Ding, JL Leibowitz,MJ Phillips, and GA Levy. 1998. Resistance to murine hepatitis virus strain 3 isdependent on production of nitric oxide.Journal of virology72, 9 (1998), 7084\u20137090.\n\n[4]Magnus Sahlgren. 2006.The Word-Space Model: Using distributional analysis torepresent syntagmatic and paradigmatic relations between words in high-dimensionalvector spaces. Ph.D. Dissertation.\n\n[5]Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE.Journal of Machine Learning Research9 (2008), 2579\u20132605.  http:\/\/www.jmlr.org\/papers\/v9\/vandermaaten08a.ht","727d9006":"Visualize using t-SNE\n\n"}}