{"cell_type":{"d69feffd":"code","2c38a1fb":"code","daa91563":"code","b0cc8b08":"code","cc92de9c":"code","6279a595":"code","957ab22d":"code","43a6e586":"code","5b52b44b":"code","2bf4dc4b":"code","99d63dbc":"code","9e4cb21c":"code","b74a2703":"code","5d48b97e":"code","64550603":"code","1886ecf5":"code","2be111e1":"code","df189886":"code","75a046fb":"code","1f9cb1df":"code","48707747":"code","43a94d0b":"code","fbc239ed":"code","cfb9f0a3":"code","91bf8bda":"code","8b9a87c5":"code","7a5acec8":"code","a38561bc":"code","6f0cac0d":"code","cf1f450d":"code","ae1b54f9":"code","498b10fd":"code","72cb0396":"code","c396f946":"code","fad36986":"code","9960a2fe":"code","5ac47e13":"code","56d92025":"markdown","1247b16d":"markdown","1fda9967":"markdown","16b5c829":"markdown","2e8570cd":"markdown","b4b01316":"markdown","9f21b414":"markdown","f5d22c9d":"markdown","9c46a133":"markdown","4199fa85":"markdown","31879063":"markdown","e3dede38":"markdown","df3262b8":"markdown","ea696099":"markdown","432f1bf6":"markdown","e08f05c9":"markdown","5e8e879c":"markdown","ce18d766":"markdown","29c1f176":"markdown","3829a962":"markdown","d6feb5b3":"markdown","72ec25fc":"markdown","5c3b7876":"markdown","cb2f31ae":"markdown","2aa44680":"markdown","f09d48cf":"markdown","b34cf768":"markdown","7fce8060":"markdown","2f1fc41c":"markdown","a2e728b7":"markdown","00dd6343":"markdown"},"source":{"d69feffd":"#!pip install --upgrade sympy","2c38a1fb":"!pip install autofeat","daa91563":"import pandas as pd\nimport numpy as np \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom autofeat import AutoFeatRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import explained_variance_score\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('max_columns', 100)","b0cc8b08":"traindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)","cc92de9c":"df.head(5)","6279a595":"#Thanks to:\n# https:\/\/www.kaggle.com\/mauricef\/titanic\n# https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n#\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - \\\n                                    df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.replace(np.nan, 0)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\n\n#Thanks to https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n#\"Title\" improvement\ndf['Title'] = df['Title'].replace('Ms','Miss')\ndf['Title'] = df['Title'].replace('Mlle','Miss')\ndf['Title'] = df['Title'].replace('Mme','Mrs')\n# Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n# Cabin, Deck\ndf['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf.loc[(df['Deck'] == 'T'), 'Deck'] = 'A'\n\n# Thanks to https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Fare\nmed_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf['Fare'] = df['Fare'].fillna(med_fare)\n#Age\ndf['Age'] = df.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n# Family_Size\ndf['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n\n# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin']\ndf = df.drop(cols_to_drop, axis=1)\n\ndf.WomanOrBoySurvived = df.WomanOrBoySurvived.fillna(0)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.fillna(0)\ndf.FamilySurvivedCount = df.FamilySurvivedCount.fillna(0)\ndf.Alone = df.Alone.fillna(0)\ndf.Alone = df.Alone*1","957ab22d":"df.head(5)","43a6e586":"target = df.Survived.loc[traindf.index]\ndf = df.drop(['Survived'], axis=1)\ntrain, test = df.loc[traindf.index], df.loc[testdf.index]","5b52b44b":"# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","2bf4dc4b":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","99d63dbc":"train.head()","9e4cb21c":"test.head()","b74a2703":"model = AutoFeatRegressor()\nmodel","5d48b97e":"X_train_feature_creation = model.fit_transform(train, target)\nX_test_feature_creation = model.transform(test)\nX_train_feature_creation.head()","64550603":"# Number of new features\nprint('Number of new features -',X_train_feature_creation.shape[1] - train.shape[1])","1886ecf5":"df2 = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, df.Sex.replace({'male': 0, 'female': 1})], axis=1)\ntrain2, test2 = df2.loc[traindf.index], df2.loc[testdf.index]","2be111e1":"X_train_feature_creation2 = model.fit_transform(train2.to_numpy(), target.to_numpy().flatten())\nX_test_feature_creation2 = model.transform(test2.to_numpy())\nX_train_feature_creation2.head()","df189886":"# Number of new features\nprint('Number of new features -',X_train_feature_creation2.shape[1] - train2.shape[1])","75a046fb":"test_x = df2.loc[testdf.index]\n\n# The one line of the code for prediction : LB = 0.83253 (Titanic Top 3%) \ntest_x['Survived'] = (((test_x.WomanOrBoySurvived <= 0.238) & (test_x.Sex > 0.5) & (test_x.Alone > 0.5)) | \\\n          ((test_x.WomanOrBoySurvived > 0.238) & \\\n           ~((test_x.WomanOrBoySurvived > 0.55) & (test_x.WomanOrBoySurvived <= 0.633))))\n\n# Saving the result\npd.DataFrame({'Survived': test_x['Survived'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('survived.csv', index=False)","1f9cb1df":"LB_simple_rule = 0.83253","48707747":"# Linear Regression without Autofeat\nmodel_LR = LinearRegression().fit(train,target.to_numpy().flatten())\n\n# Linear Regression with Autofeat\nmodel_Autofeat = LinearRegression().fit(X_train_feature_creation, target.to_numpy().flatten())","43a94d0b":"test['Survived_LR'] = np.clip(model_LR.predict(test),0,1)\ntest['Survived_AF'] = np.clip(model_Autofeat.predict(X_test_feature_creation),0,1)","fbc239ed":"def hyperopt_gb_score(params):\n    clf = GradientBoostingClassifier(**params)\n    current_score = cross_val_score(clf, X_train_feature_creation, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_gb = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', np.arange(2, 10, dtype=int))            \n        }\n \nbest = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","cfb9f0a3":"params = space_eval(space_gb, best)\nparams","91bf8bda":"# Gradient Boosting Classifier\n\ngradient_boosting = GradientBoostingClassifier(**params)\ngradient_boosting.fit(X_train_feature_creation, target)\nY_pred = gradient_boosting.predict(X_test_feature_creation).astype(int)\ngradient_boosting.score(X_train_feature_creation, target)\nacc_gradient_boosting = round(gradient_boosting.score(X_train_feature_creation, target) * 100, 2)\nacc_gradient_boosting","8b9a87c5":"# Saving the results\npd.DataFrame({'Survived': test['Survived_LR'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('survived_LR16.csv', index=False)\npd.DataFrame({'Survived': test['Survived_AF'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('survived_Autofeat16.csv', index=False)\npd.DataFrame({'Survived': Y_pred}, \\\n             index=testdf.index).reset_index().to_csv('survived_GBC16.csv', index=False)","7a5acec8":"# After download solutions in Kaggle competition:\nLB_LR16 = 0.69377\nLB_Autofeat16 = 0.67942\nLB_GBC16 = 0.82296","a38561bc":"# Linear Regression without Autofeat\nmodel_LR2 = LinearRegression().fit(train2,target.to_numpy().flatten())\n\n# Linear Regression with Autofeat\nmodel_Autofeat2 = LinearRegression().fit(X_train_feature_creation2, target.to_numpy().flatten())","6f0cac0d":"test2['Survived_LR'] = np.clip(model_LR2.predict(test2),0,1)\ntest2['Survived_AF'] = np.clip(model_Autofeat2.predict(X_test_feature_creation2),0,1)","cf1f450d":"def hyperopt_gb_score(params):\n    clf = GradientBoostingClassifier(**params)\n    current_score = cross_val_score(clf, X_train_feature_creation2, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_gb = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', np.arange(2, 10, dtype=int))            \n        }\n \nbest = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","ae1b54f9":"params2 = space_eval(space_gb, best)\nparams2","498b10fd":"# Gradient Boosting Classifier\n\ngradient_boosting2 = GradientBoostingClassifier(**params2)\ngradient_boosting2.fit(X_train_feature_creation2, target)\nY_pred2 = gradient_boosting2.predict(X_test_feature_creation2).astype(int)\ngradient_boosting2.score(X_train_feature_creation2, target)\nacc_gradient_boosting2 = round(gradient_boosting2.score(X_train_feature_creation2, target) * 100, 2)\nacc_gradient_boosting2","72cb0396":"# Saving the results\npd.DataFrame({'Survived': test2['Survived_LR'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('survived_LR3.csv', index=False)\npd.DataFrame({'Survived': test2['Survived_AF'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('survived_Autofeat3.csv', index=False)\npd.DataFrame({'Survived': Y_pred2}, \\\n             index=testdf.index).reset_index().to_csv('survived_GBC3.csv', index=False)","c396f946":"# After download solutions in Kaggle competition:\nLB_LR3 = 0.67942\nLB_Autofeat3 = 0.69377\nLB_GBC3 = 0.83253","fad36986":"models = pd.DataFrame({\n    'Model': ['Simple rule','Linear Regression without Autofeat', 'Linear Regression with Autofeat',\n              'GradientBoostingClassifier with Autofeat'],\n    \n    'LB_for_16_features': [LB_simple_rule, LB_LR16, LB_Autofeat16, LB_GBC16],\n\n    'LB_for_3opt_features': [LB_simple_rule, LB_LR3, LB_Autofeat3, LB_GBC3]})","9960a2fe":"models.sort_values(by=['LB_for_3opt_features', 'LB_for_16_features'], ascending=False)","5ac47e13":"models.sort_values(by=['LB_for_16_features', 'LB_for_3opt_features'], ascending=False)","56d92025":"I hope you find this kernel useful and enjoyable.","1247b16d":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","1fda9967":"**Gradient Boosting Classifier with HyperOpt tuning** - from my kernel: [Titanic (0.83253) - Comparison 20 popular models](https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models) ","16b5c829":"### 4.2. For optimal 3 features <a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","2e8570cd":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","b4b01316":"## 5. Modeling and comparison <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","9f21b414":"### 4.1. For all 16 features <a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","f5d22c9d":"### 3.2. Encoding categorical features <a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","9c46a133":"As you can see, the **Gradient Boosting Classifier** equally well extracts features without Autofeat library.","4199fa85":"### 5.2. The LR and BGC for all 16 features <a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","31879063":"## 4. Automatic FE <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","e3dede38":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg)","df3262b8":"**Liner Regression with Autofeat** from kernel [Autofeat Regression](https:\/\/www.kaggle.com\/lachhebo\/autofeat-feature-engineering\/comments#652596)","ea696099":"## 7. Conclusion <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","432f1bf6":"## 3. Preparing to modeling with manual FE <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","e08f05c9":"Your comments and feedback are most welcome.","5e8e879c":"**Comparison of 4 models, including 3 new models**","ce18d766":"### 3.1. Manual FE <a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","29c1f176":"[Go to Top](#0.0)","3829a962":"### 5.1. The simple rule - very accurate model <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","d6feb5b3":"My kernels outline traditional approaches to FE:\n\n1) the consolidated result of EDA and FE optimization from many authors:\n* https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\n* https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n* https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-15\n* https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20\n\n2) the result of the formation of many features and their processing by 20 models (boosting, regression, simple neural networks, etc.):\n\n* [Titanic (0.83253) - Comparison 20 popular models](https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models)\n\n\nThe kernel [Autofeat Regression](https:\/\/www.kaggle.com\/lachhebo\/autofeat-feature-engineering) provides an example of using library Autofeat for automatic FE. Let us analyze whether this application will produce comparable results.","72ec25fc":"## 6. Comparison of 23 models <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","5c3b7876":"<a class=\"anchor\" id=\"0.0\"><\/a>\n# Titanic : Comparison of automatic FE efficiency with Autofeat and tradicional approaches","cb2f31ae":"**Comparison with 20 other models**","2aa44680":"# Competition [\"Titanic - Machine Learning from Disaster\"](https:\/\/www.kaggle.com\/c\/titanic)","f09d48cf":"**Linear Regression**","b34cf768":"From my kernel: [Titanic Top 3% : one line of the prediction code](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code)","7fce8060":"The analysis makes the following conclusions:\n\n- Autofeat methods should only be used to find new dependencies and features, but **this technology does not replace traditional FE methods**\n\n- The **Gradient Boosting Classifier equally well extracts features without Autofeat library**, essentially negating the its benefits for this model (and apparently for other advanced decision tree models with hyperparameter optimization)\n\n- the **Linear Regression model** has low accuracy and **Autofeat methods allow it to be slightly improved, but only with the condition of preliminary FE**, otherwise its application can also impair the accuracy of prediction\n\n- The **Autofeat methods study should be repeated** if you are trying to tuning other conversion features and make the Autofeat classifier's hyperparameter optimization\n\n- It is advisable to try to **apply Autofeat methods to the results of other methods of manual or automatic feature extraction**, such as Featurestools","2f1fc41c":"**Gradient Boosting Classifier with HyperOpt tuning** ","a2e728b7":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [Preparing to modeling with manual FE](#3)\n -  [Manual FE](#3.1)\n -  [Encoding categorical features](#3.2)\n1. [Automatic FE](#4)\n -  [For all 16 features](#4.1)\n -  [For optimal 3 features](#4.2)\n1. [Modeling](#5)\n -  [The simple rule - very accurate model](#5.1)\n -  [The LR and BGC for all 16 features](#5.2)\n -  [The LR and BGC for optimal 3 features](#5.3)\n1. [Comparison of 23 models](#6)\n1. [Conclusion](#7)","00dd6343":"### 5.3. The LR and BGC for optimal 3 features <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}