{"cell_type":{"282d96c7":"code","5ef1b4ec":"code","bdfc5dfc":"code","1d01fed4":"code","b2d89018":"code","76dce73d":"code","69d5e657":"code","0e014d99":"code","f502d737":"code","0cd799f7":"code","3e548180":"code","dd56f17c":"code","59809e3f":"code","b63b7754":"code","ab1ffcfd":"code","9eff86f1":"code","908ac884":"code","77a24af3":"code","4fcbd19a":"code","4a0c0eeb":"code","e13f6404":"code","0b911232":"code","92a35f17":"code","505c548d":"code","74ce0315":"code","d265b002":"code","38ad8ec4":"code","1f75ad28":"code","e17a3f27":"code","c35f4cd8":"code","230401a5":"code","c983a1f0":"code","241846b3":"code","26af7762":"code","1d750878":"code","a4e5b58a":"code","55f82f48":"code","68adc472":"code","0904ce1a":"code","530129ad":"code","dac86514":"code","4fa7faee":"code","1482a6fc":"code","32694582":"code","c622cfcb":"code","0800177e":"code","68804534":"code","13486c7f":"markdown","c61e4253":"markdown","25f2ae50":"markdown","a85ae98f":"markdown","6b814e41":"markdown","73cfe950":"markdown","4dd7a3c7":"markdown","0cddeb87":"markdown","6d1191d1":"markdown","766e9d72":"markdown","44010aec":"markdown","af55e459":"markdown","df386b9b":"markdown","ecc1f7cc":"markdown","35dfd612":"markdown","02cca36f":"markdown","53183eeb":"markdown","5d5c8235":"markdown","2031ff7a":"markdown","bbe1a755":"markdown"},"source":{"282d96c7":"%%html\n<marquee style='width: 100%; color: red;'><H1>prostate-cancer-grade-assessment<\/H1><\/marquee>","5ef1b4ec":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport openslide\nimport os\nimport cv2\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16,preprocess_input\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten,BatchNormalization,Activation\nfrom keras.layers import GlobalMaxPooling2D,GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.callbacks.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import cohen_kappa_score\nimport tensorflow as tf\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.metrics import *\ntrain_df = pd.read_csv(\"..\/input\/prostate-cancer-grade-assessment\/train.csv\")\nimage_path = \"..\/input\/prostate-cancer-grade-assessment\/train_images\/\"\nPATH = \"..\/input\/prostate-cancer-grade-assessment\/\"\ntrain_df = pd.read_csv(os.path.join(PATH,'train.csv'))\ntest_df =  pd.read_csv(os.path.join(PATH,'test.csv'))\ntrain_img_path = '..\/input\/prostate-cancer-grade-assessment\/train_images'\ntrain_read_img= pd.read_csv(PATH+\"train.csv\")\nmasks = '..\/input\/prostate-cancer-grade-assessment\/train_label_masks'\nimages_train_list = os.listdir(os.path.join(PATH, 'train_images'))\nmasks_list = os.listdir(os.path.join(PATH, 'train_label_masks'))\nsns.set_style(\"darkgrid\")","bdfc5dfc":"print(train_df)","1d01fed4":"print(test_df)","b2d89018":"train_df.head()","76dce73d":"test_df.head()","69d5e657":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\nsns.countplot(ax=ax1, x=\"data_provider\", data=train_df)\nax1.set_title(\"distribution de data_provider  dans  training data\")\nsns.countplot(ax=ax2, x=\"data_provider\", data=test_df)\nax2.set_title(\"distribution de data_provider dans test data\")\nplt.show()","0e014d99":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\nsns.countplot(ax=ax1, x=\"isup_grade\", data=train_df)\nax1.set_title(\"ISUP Grade distribution dans  Data Provider\")\nsns.countplot(ax=ax2, x=\"gleason_score\", data=train_df)\nax2.set_title(\"Gleason_Score distribution dans  Data Provider\")\nplt.show()","f502d737":"from tqdm import tqdm\n\nimg_dim= []\n\nfor i,row in tqdm(train_df.iterrows()):\n    slide = openslide.OpenSlide(os.path.join(train_img_path, train_df.image_id.iloc[i]+'.tiff'))\n    img_dim.append(slide.dimensions)\n    slide.close()\n    \nwidth = [dimensions[0] for dimensions in img_dim] \nheight = [dimensions[1] for dimensions in img_dim] \n\ntrain_df['width'] = width\ntrain_df['height'] = height","0cd799f7":"fig = plt.figure(figsize=(20,5))\nax = sns.scatterplot(x='width', y='height', data=train_df, hue='data_provider', alpha=0.70)\nax.tick_params(labelsize=10)\n\nplt.title('Dimensions des images')\nplt.show()","3e548180":"fig, ax = plt.subplots(1, 2)\nfig.set_size_inches(20, 5)\n\nsns.stripplot(train_df['width'],train_df['data_provider'],ax=ax[0],jitter=True)\nsns.stripplot(train_df['height'],train_df['data_provider'],ax=ax[1],jitter=True)\n\nax[0].tick_params(labelsize=10)\nax[1].tick_params(labelsize=10)\nax[0].tick_params(labelrotation=90)\nax[1].tick_params(labelrotation=90)\nplt.show()","dd56f17c":"data_file_masks = pd.Series(masks_list).to_frame()\ndata_file_masks.columns = ['mask_file_name']\ndata_file_masks.head()","59809e3f":"data_file_masks['image_id'] =data_file_masks.mask_file_name.apply(lambda x: x.split('_')[0])\ndata_file_masks.head()","b63b7754":"train_df = pd.merge(train_df, data_file_masks, on='image_id', how='outer')\ntrain_df.head()","ab1ffcfd":"del data_file_masks\nprint(f\"Il y a {len(train_df[train_df.mask_file_name.isna()])} images sans masque.\")","9eff86f1":"print(f\"Train data avant la r\u00e9duction: {len(train_df)}\")\ndf_train_reduction= train_df[~train_df.mask_file_name.isna()]\nprint(f\"Train data apr\u00e8s la r\u00e9duction: {len(df_train_reduction)}\")","908ac884":"fig,ax=plt.subplots(1,2,figsize=(20,5))\ntrain_df['data_provider'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0])\nax[0].set_ylabel('')\ndf_train_reduction['data_provider'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[1])\nax[1].set_ylabel('')\nplt.show()","77a24af3":"images_without_masks=train_df[train_df.mask_file_name.isna()]\nwithout_masks=images_without_masks.groupby('image_id').data_provider.unique().to_frame()\nwithout_masks.to_csv(\"new_test.csv\",index=False)\nwithout_masks\n","4fcbd19a":"df_train_reduction.groupby('isup_grade').gleason_score.unique().to_frame()","4a0c0eeb":"df_train_reduction[(df_train_reduction.isup_grade == 2) & (df_train_reduction.gleason_score == '4+3')].reset_index()","e13f6404":"df_train_reduction.reset_index(inplace=True)\ndf_train_reduction = df_train_reduction[df_train_reduction.image_id !='b0a92a74cb53899311acc30b7405e101']","0b911232":"df_train_reduction[(df_train_reduction.isup_grade == 2) & (df_train_reduction.gleason_score == '4+3')].reset_index()","92a35f17":"df_train_reduction.groupby('isup_grade').gleason_score.unique().to_frame()","505c548d":"temp = df_train_reduction.groupby('isup_grade').count()['image_id'].reset_index().sort_values(by='image_id',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","74ce0315":"temp = df_train_reduction.groupby('gleason_score').count()['image_id'].reset_index().sort_values(by='image_id',ascending=False)\ntemp.style.background_gradient(cmap='Reds')","d265b002":"df_train_reduction[(df_train_reduction.isup_grade == 0) & (df_train_reduction.gleason_score =='negative')].reset_index()","38ad8ec4":"sns.set_style(\"darkgrid\")\nfig= plt.subplots(figsize=(20,5))\nsns.countplot(x='gleason_score', hue=\"data_provider\", data=df_train_reduction)\nplt.show()","1f75ad28":"df_train_reduction[\"gleason_score\"]= df_train_reduction[\"gleason_score\"].replace(\"negative\", \"0+0\")","e17a3f27":"df_train_reduction.groupby('isup_grade').gleason_score.unique().to_frame()","c35f4cd8":"temp = df_train_reduction.groupby('gleason_score').count()['image_id'].reset_index().sort_values(by='image_id',ascending=False)\ntemp.style.background_gradient(cmap='Reds')","230401a5":"def show_images(df, read_region=(1780,1950)):\n    \n    data = df\n    f, ax = plt.subplots(3,3, figsize=(20,20))\n    for i,data_row in enumerate(data.iterrows()):\n        image = str(data_row[1][0])+'.tiff'\n        image_path = os.path.join(PATH,\"train_images\",image)\n        image = openslide.OpenSlide(image_path)\n        spacing = 1 \/ (float(image.properties['tiff.XResolution']) \/ 10000)\n        patch = image.read_region(read_region, 0, (256, 256))\n        ax[i\/\/3, i%3].imshow(patch) \n        image.close()       \n        ax[i\/\/3, i%3].axis('off')\n        ax[i\/\/3, i%3].set_title('ID: {}\\nSource: {} ISUP: {} Gleason: {}'.format(\n                data_row[1][0], data_row[1][1], data_row[1][2], data_row[1][3]))\n\n    plt.show()\n    \nimages = [\n    '07a7ef0ba3bb0d6564a73f4f3e1c2293',\n    '037504061b9fba71ef6e24c48c6df44d',\n    '035b1edd3d1aeeffc77ce5d248a01a53',\n    '059cbf902c5e42972587c8d17d49efed',\n    '06a0cbd8fd6320ef1aa6f19342af2e68',\n    '06eda4a6faca84e84a781fee2d5f47e1',\n    '0a4b7a7499ed55c71033cefb0765e93d',\n    '0838c82917cd9af681df249264d2769c',\n    '046b35ae95374bfb48cdca8d7c83233f'\n]\ndata_sample = train_df.loc[train_df.image_id.isin(images)]\nshow_images(data_sample)","c983a1f0":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\nsns.countplot(ax=ax1, x=\"isup_grade\", data=df_train_reduction)\nax1.set_title(\"ISUP Grade Count by Data Provider\")\nsns.countplot(ax=ax2, x=\"gleason_score\", data=df_train_reduction)\nax2.set_title(\"Gleason_Score Count by Data Provider\")\nplt.show()","241846b3":"def show_masks(slides): \n    f, ax = plt.subplots(5,3, figsize=(18,22))\n    for i, slide in enumerate(slides):\n        mask = openslide.OpenSlide(os.path.join(mask_dir, f'{slide}_mask.tiff'))\n        mask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n        cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n        ax[i\/\/3, i%3].imshow(np.asarray(mask_data)[:,:,0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5) \n        mask.close()       \n        ax[i\/\/3, i%3].axis('off')    \n        image_id = slide\n        data_provider = data_sample_mask.loc[slide, 'data_provider']\n        isup_grade = data_sample_mask.loc[slide, 'isup_grade']\n        gleason_score = data_sample_mask.loc[slide, 'gleason_score']\n        ax[i\/\/3, i%3].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score}\")\n        f.tight_layout()\n        \n    plt.show()","26af7762":"images_mask  = [\n    '07a7ef0ba3bb0d6564a73f4f3e1c2293',\n    '037504061b9fba71ef6e24c48c6df44d',\n    '035b1edd3d1aeeffc77ce5d248a01a53',\n    '059cbf902c5e42972587c8d17d49efed',\n    '06a0cbd8fd6320ef1aa6f19342af2e68',\n    '06eda4a6faca84e84a781fee2d5f47e1',\n    '0a4b7a7499ed55c71033cefb0765e93d',\n    '0838c82917cd9af681df249264d2769c',\n    '028098c36eb49a8c6aa6e76e365dd055',\n    '0280f8b612771801229e2dde52371141',\n    '028dc05d52d1dd336952a437f2852a0a',\n    '02a2dcd6ad8bc1d9ad7fdc04ffb6dff3',\n    '049031b0ea0dede1ca1e5ca470c1332d',\n    '05f4e9415af9fdabc19109c980daf5ad',\n    '07fd8d4f02f9b95d86da4bc89563e077'\n]\n\nmask_dir = os.path.join(PATH,\"train_label_masks\")\ndata_sample_mask = df_train_reduction.set_index('image_id')\nshow_masks(images_mask)","1d750878":"def mask_img(image,max_size=(600,400)):\n    slide = openslide.OpenSlide(os.path.join(train_img_path, f'{image}.tiff'))\n    mask =  openslide.OpenSlide(os.path.join(mask_dir, f'{image}_mask.tiff'))\n    f,ax =  plt.subplots(1,2 ,figsize=(18,22))\n    spacing = 1 \/ (float(slide.properties['tiff.XResolution']) \/ 10000)\n    img = slide.get_thumbnail(size=(600,400)) \n    mask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n    cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n    \n    \n    ax[0].imshow(img)\n    ax[1].imshow(np.asarray(mask_data)[:,:,0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5) \n    \n    image_id = image\n    data_provider = data_sample_mask.loc[image, 'data_provider']\n    isup_grade = data_sample_mask.loc[image, 'isup_grade']\n    gleason_score = data_sample_mask.loc[image, 'gleason_score']\n    ax[0].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score} IMAGE\")\n    ax[1].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score} IMAGE_MASK\")","a4e5b58a":"images1= [\n    '08ab45297bfe652cc0397f4b37719ba1',\n    '090a77c517a7a2caa23e443a77a78bc7',\n    '07fd8d4f02f9b95d86da4bc89563e077'\n]\n\nfor image in images1:\n    mask_img(image)","55f82f48":"train_df=df_train_reduction\nAccuracies_list=[]\nlabels=[]\ndata=[]\ndata_dir='..\/input\/panda-resized-train-data-512x512\/train_images\/train_images\/'\nfor i in range(train_df.shape[0]):\n    data.append(data_dir + train_df['image_id'].iloc[i]+'.png')\n    labels.append(train_df['isup_grade'].iloc[i])\ndf=pd.DataFrame(data)\ndf.columns=['images']\ndf['isup_grade']=labels","68adc472":"df.head()","0904ce1a":"print(len(df))","530129ad":"print(labels)","dac86514":"X_train, X_val, y_train, y_val = train_test_split(df['images'],df['isup_grade'], test_size=0.1, random_state=42)","4fa7faee":"train=pd.DataFrame(X_train)\ntrain.columns=['images']\ntrain['isup_grade']=y_train\n\nvalidation=pd.DataFrame(X_val)\nvalidation.columns=['images']\nvalidation['isup_grade']=y_val\n\ntrain['isup_grade']=train['isup_grade'].astype(str)\nvalidation['isup_grade']=validation['isup_grade'].astype(str)","1482a6fc":"print(\"train size \",len(train))\nprint(\"validation size \",len(validation))","32694582":"print(train)","c622cfcb":"print(validation)","0800177e":"sns.set(style=\"darkgrid\")\na = ['TRAIN DATA ','TEST DATA ']\nb = [len((train)),len((validation))]\nax = sns.barplot(x=a, y=b)","68804534":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\nsns.countplot(ax=ax1, x=\"isup_grade\", data=train)\nax1.set_title(\"distribution de Grade ISUP dans le TRAIN DATA apr\u00e8s le divisiment\")\nsns.countplot(ax=ax2, x=\"isup_grade\", data=validation)\nax2.set_title(\"distribution de Grade ISUP dans le TEST DATA apr\u00e8s le divisiment\")\nplt.show()","13486c7f":"# 1. Objectifs\nD\u00e9tecter et classer la gravit\u00e9 du cancer de la prostate sur des images d'\u00e9chantillons de tissus prostatiques.\n\nEn pratique, les \u00e9chantillons de tissus sont examin\u00e9s et not\u00e9s par les pathologistes selon le syst\u00e8me de notation dit de Gleason, qui est ensuite converti en grade ISUP.","c61e4253":"# Affichage de quelques images","25f2ae50":"# Sommaire\n1. Objectifs\n2. Comprendre la base de donn\u00e9es\n   * Comprendre la base de donn\u00e9es\n3. Pr\u00e9paration de la base de donn\u00e9es\n * Visualisation de donn\u00e9es\n * Fixer quelques probl\u00e8mes dans la base de donn\u00e9es\n    * Images sans masque\n    * ISUP = 2 Gleason score = 4 + 3 \n    * remplacer \"n\u00e9gatif\" par \"0+0\"\n    * Quelques probl\u00e8mes dans la base de donn\u00e9es","a85ae98f":"*   nous pouvons voir que radboud n'a pas de valeurs \"0+0\" alors que karolinska n'a pas de valeurs \"negative\".\n*    conclusion : \"negative\" correspond \u00e0 la fa\u00e7on dont le radbound repr\u00e9sente \"0+0\" (c'est-\u00e0-dire l'absence de cancer) ; il serait donc plus logique de remplacer \"negative\" par \"0+0\".","6b814e41":"## Visualisation de donn\u00e9es","73cfe950":"* 1. ISUP grade = 0  Gleason score 0+0 or negative.\n* 1. ISUP grade = 1  Gleason score 3+3.\n* 1. ISUP grade = 2  Gleason score 3+4.\n* 1. ISUP grade = 3  Gleason score 4+3.\n* 1. ISUP grade = 4  Gleason score 4+4 (majority), 3+5 or 5+3.\n* 1. ISUP grade = 5  Gleason score 4+5 (majority), 5+4 or 5+5.","4dd7a3c7":"## Fixer quelques probl\u00e8mes dans la base de donn\u00e9es","0cddeb87":"### diviser notre data set","6d1191d1":"# 3.Pr\u00e9paration de la base de donn\u00e9es","766e9d72":"# Affichage de quelques masques pour loacaliser le cancer et comprendre chaque grade de la maladie","44010aec":"inspir\u00e9 de : [Links](https:\/\/medium.com\/@kvnamipara\/a-better-visualisation-of-pie-charts-by-matplotlib-935b7667d77f)","af55e459":"# Images sans masque\nil y a des images sans masque dans la base de ddonn\u00e9es","df386b9b":"# Affichage de quelques images et leurs masques","ecc1f7cc":"### apr\u00e8s le divisiment de notre data set","35dfd612":"# ISUP = 2 Gleason score = 4 + 3 \n** Il n'y a pas de ISUP = 2 , Gleason score = 4+3 dans le syst\u00e8me de notation Gleason + il n'y a qu'une seule image de ce type et elle semble \u00eatre une erreur, je vais donc la supprimer.**","02cca36f":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/PANDA\/Screen%20Shot%202020-04-08%20at%202.03.53%20PM.png\" height=\"100px\">","53183eeb":"# remplacer \"negative\" par \"0+0\"","5d5c8235":"# 2.Comprendre la base de donn\u00e9es\n\n\ntrain.csv et test.csv:\n\n* image_id: Code d'identification de l'image.\n\n* data_provider: Le nom de l'institution qui a fourni les donn\u00e9es. L'Institut **Karolinska** et le Centre m\u00e9dical universitaire **Radboud** \n\n\n\n*   uniquement dans train.csv\n\n* isup_grade: La gravit\u00e9 du cancer sur une \u00e9chelle de 0 \u00e0 5.\n\n* gleason_score: Un syst\u00e8me alternatif d'\u00e9valuation de la gravit\u00e9 du cancer avec plus de niveaux que l'\u00e9chelle ISUP. \n\n* train_images:\n* 10616 images de type .tiff \n  * Karolinska=5455 images\n  * Radboud=5060 images\n* test_images:\n3 images de type .tiff\n\ntrain_label_masks: Segmentation masks showing which parts of the image led to the ISUP grade. Not all training images have label masks, and there may be false positives or false negatives in the label masks for a variety of reasons. These masks are provided to assist with the development of strategies for selecting the most useful subsamples of the images. The mask values depend on the data provider:","2031ff7a":"* le test data contient uniquement 3 images , donc  je vais cr\u00e9er un autre fichier new_test.csv  avec les 100 images que j'ai supprim\u00e9 (images sans masque)","bbe1a755":"\npanda-resized-train-data-512x512 , code source : [Links](https:\/\/www.kaggle.com\/xhlulu\/panda-resize-and-save-train-data)"}}