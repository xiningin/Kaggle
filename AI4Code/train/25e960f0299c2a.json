{"cell_type":{"26a60c5f":"code","d7ca8939":"code","2dbe3cba":"code","58f066ce":"code","79886b21":"code","f697f041":"code","1567f74a":"code","33693035":"code","c6fbd9bd":"code","5175ab96":"code","e9d2deb6":"code","835349d6":"code","d5287163":"code","7a324f37":"code","4c739413":"code","51a0b265":"code","73b4f23f":"code","b01a21f2":"code","413bb46f":"code","78538245":"code","fcf44e51":"code","23869612":"code","2c0b89dd":"code","1bda13ee":"code","f3ab8163":"code","3a9be8cb":"code","8e0bfc5b":"code","b06d47fe":"code","5aa2b3c8":"code","df133052":"markdown","d3c0659e":"markdown","5e8ae4b7":"markdown","04726375":"markdown","67866e79":"markdown","82dcb5a3":"markdown","1f6749b5":"markdown","68c7f2b0":"markdown","1cbe6607":"markdown","a80ea90e":"markdown","7d9402d1":"markdown","48c3c643":"markdown","cb0a449d":"markdown","5ecf89bb":"markdown","471be59b":"markdown","29acf9f1":"markdown","1f5c252b":"markdown","0aecf553":"markdown","1294eec4":"markdown","ee42f1c5":"markdown","5182f381":"markdown","9644e422":"markdown","2222ddcd":"markdown","91f71f41":"markdown","3f4d7196":"markdown","e05a8c78":"markdown","77d1302a":"markdown","1cc848d9":"markdown","54b8574f":"markdown","c687606e":"markdown","cc20d7f7":"markdown","8a88b7b5":"markdown","5f66c228":"markdown","dadcff3a":"markdown","486231ec":"markdown","3a44b56e":"markdown","d8b13bb9":"markdown","599934f7":"markdown","96d3e0cf":"markdown","da4a2a61":"markdown","c52c683a":"markdown","7a3fc68c":"markdown","dbc51446":"markdown","dfbd3203":"markdown"},"source":{"26a60c5f":"import os, random, re, math, time\nrandom.seed(a=42)\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\n\nimport PIL\n\nfrom kaggle_datasets import KaggleDatasets\n\nfrom tqdm import tqdm\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score","d7ca8939":"BASEPATH = \"..\/input\/siim-isic-melanoma-classification\"\ndf_train = pd.read_csv(os.path.join(BASEPATH, 'train.csv'))\ndf_test  = pd.read_csv(os.path.join(BASEPATH, 'test.csv'))\n\nGCS_PATH    = KaggleDatasets().get_gcs_path('melanoma-256x256')\nfiles_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '\/train*.tfrec')))\nfiles_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '\/test*.tfrec')))","2dbe3cba":"df_train.head()","58f066ce":"df_train.target.value_counts()","79886b21":"df_train['sex'] = df_train['sex'].fillna('na')\ndf_train['age_approx'] = df_train['age_approx'].fillna(0)\ndf_train['anatom_site_general_challenge'] = df_train['anatom_site_general_challenge'].fillna('na')\n\n\ndf_test['sex'] =df_test['sex'].fillna('na')\ndf_test['age_approx'] = df_test['age_approx'].fillna(0)\ndf_test['anatom_site_general_challenge'] = df_test['anatom_site_general_challenge'].fillna('na')","f697f041":"df_train['sex'] = df_train['sex'].astype(\"category\").cat.codes +1\ndf_train['anatom_site_general_challenge'] = df_train['anatom_site_general_challenge'].astype(\"category\").cat.codes +1\ndf_train.head()\n\ndf_test['sex'] = df_test['sex'].astype(\"category\").cat.codes +1\ndf_test['anatom_site_general_challenge'] = df_test['anatom_site_general_challenge'].astype(\"category\").cat.codes +1\ndf_test.head()","1567f74a":"x_train = df_train[['sex', 'age_approx','anatom_site_general_challenge']]\ny_train = df_train['target']\n\nx_test = df_test[['sex', 'age_approx','anatom_site_general_challenge']]","33693035":"xgb_model = xgb.XGBClassifier(n_estimators=2000, \n                        max_depth=8, \n                        objective='multi:softprob',\n                        seed=0,  \n                        nthread=-1, \n                        learning_rate=0.15, \n                        num_class = 2, \n                        scale_pos_weight = (32542\/584))\n\n\nxgb_model.fit(x_train, y_train)","c6fbd9bd":"xgb_pred_result = xgb_model.predict_proba(x_test)[:,1]\nprint(xgb_pred_result)","5175ab96":"xgb_df = pd.DataFrame({\n        \"image_name\": df_test[\"image_name\"],\n        \"target\": xgb_pred_result\n    })\n\nxgb_df.to_csv('tuned_XGBClassifier_submission.csv', index=False)","e9d2deb6":"CFG = dict(\n    net_count         =   7,\n    batch_size        =  16,\n    \n    read_size         = 256, \n    crop_size         = 250, \n    net_size          = 256, \n    \n    LR_START          =   0.000005,\n    LR_MAX            =   0.000020,\n    LR_MIN            =   0.000001,\n    LR_RAMPUP_EPOCHS  =   5,\n    LR_SUSTAIN_EPOCHS =   0,\n    LR_EXP_DECAY      =   0.8,\n    epochs            =  15,\n    \n    rot               = 180.0,\n    shr               =   2.0,\n    hzoom             =   8.0,\n    wzoom             =   8.0,\n    hshift            =   8.0,\n    wshift            =   8.0,\n\n    optimizer         = 'adam',\n    label_smooth_fac  =   0.05,\n    \n    tta_steps         =  25    \n)\n\nAUTO     = tf.data.experimental.AUTOTUNE\nstrategy = tf.distribute.get_strategy()\nREPLICAS = strategy.num_replicas_in_sync","835349d6":"!pip install -q efficientnet","d5287163":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\ndef transform(image, cfg):    \n    DIM = cfg[\"read_size\"]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = cfg['rot'] * tf.random.normal([1], dtype='float32')\n    shr = cfg['shr'] * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ cfg['hzoom']\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ cfg['wzoom']\n    h_shift = cfg['hshift'] * tf.random.normal([1], dtype='float32') \n    w_shift = cfg['wshift'] * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM\/\/2, -DIM\/\/2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM\/\/2, DIM\/\/2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM\/\/2+XDIM+1, DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM, DIM,3])\n","7a324f37":"def read_labeled_tfrecord(example):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n    }           \n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['target']\n\n\ndef read_unlabeled_tfrecord(example, return_image_name):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['image_name'] if return_image_name else 0\n","4c739413":"def prepare_image(img, cfg=None, augment=True):    \n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [cfg['read_size'], cfg['read_size']])\n    img = tf.cast(img, tf.float32) \/ 255.0\n    \n    if augment:\n        img = transform(img, cfg)\n        img = tf.image.random_crop(img, [cfg['crop_size'], cfg['crop_size'], 3])\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_hue(img, 0.01)\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n\n    else:\n        img = tf.image.central_crop(img, cfg['crop_size'] \/ cfg['read_size'])\n                                   \n    img = tf.image.resize(img, [cfg['net_size'], cfg['net_size']])\n    img = tf.reshape(img, [cfg['net_size'], cfg['net_size'], 3])\n    return img","51a0b265":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)","73b4f23f":"def get_dataset(files, cfg, augment = False, shuffle = False, repeat = False, \n                labeled=True, return_image_names=True):\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n    \n    if repeat:\n        ds = ds.repeat()\n    \n    if shuffle: \n        ds = ds.shuffle(1024*8)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n        \n    if labeled: \n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), \n                    num_parallel_calls=AUTO)      \n    \n    ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, cfg=cfg), \n                                               imgname_or_label), \n                num_parallel_calls=AUTO)\n    \n    ds = ds.batch(cfg['batch_size'] * REPLICAS)\n    ds = ds.prefetch(AUTO)\n    return ds","b01a21f2":"def show_dataset(thumb_size, cols, rows, ds):\n    mosaic = PIL.Image.new(mode='RGB', size=(thumb_size*cols + (cols-1), \n                                             thumb_size*rows + (rows-1)))\n   \n    for idx, data in enumerate(iter(ds)):\n        img, target_or_imgid = data\n        ix  = idx % cols\n        iy  = idx \/\/ cols\n        img = np.clip(img.numpy() * 255, 0, 255).astype(np.uint8)\n        img = PIL.Image.fromarray(img)\n        img = img.resize((thumb_size, thumb_size), resample=PIL.Image.BILINEAR)\n        mosaic.paste(img, (ix*thumb_size + ix, \n                           iy*thumb_size + iy))\n\n    display(mosaic)\n    \nds = get_dataset(files_train, CFG).unbatch().take(12*5)   \nshow_dataset(64, 12, 5, ds)","413bb46f":"ds = tf.data.TFRecordDataset(files_train, num_parallel_reads=AUTO)\nds = ds.take(1).cache().repeat()\nds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\nds = ds.map(lambda img, target: (prepare_image(img, cfg=CFG, augment=True), target), \n            num_parallel_calls=AUTO)\nds = ds.take(12*5)\nds = ds.prefetch(AUTO)\n\nshow_dataset(64, 12, 5, ds)","78538245":"def get_lr_callback(cfg):\n    lr_start   = cfg['LR_START']\n    lr_max     = cfg['LR_MAX'] * strategy.num_replicas_in_sync\n    lr_min     = cfg['LR_MIN']\n    lr_ramp_ep = cfg['LR_RAMPUP_EPOCHS']\n    lr_sus_ep  = cfg['LR_SUSTAIN_EPOCHS']\n    lr_decay   = cfg['LR_EXP_DECAY']\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","fcf44e51":"def get_model(cfg):\n    model_input = tf.keras.Input(shape=(cfg['net_size'], cfg['net_size'], 3), name='imgIn')\n\n    dummy = tf.keras.layers.Lambda(lambda x:x)(model_input)\n    \n    outputs = []    \n    for i in range(cfg['net_count']):\n        constructor = getattr(efn, f'EfficientNetB{i}')\n        \n        x = constructor(include_top=False, weights='imagenet', \n                        input_shape=(cfg['net_size'], cfg['net_size'], 3), \n                        pooling='avg')(dummy)\n        \n        x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n        outputs.append(x)\n        \n    model = tf.keras.Model(model_input, outputs, name='aNetwork')\n    model.summary()\n    return model","23869612":"def compile_new_model(cfg):    \n    with strategy.scope():\n        model = get_model(cfg)\n     \n        losses = [tf.keras.losses.BinaryCrossentropy(label_smoothing = cfg['label_smooth_fac'])\n                  for i in range(cfg['net_count'])]\n        \n        model.compile(\n            optimizer = cfg['optimizer'],\n            loss      = losses,\n            metrics   = [tf.keras.metrics.AUC(name='auc')])\n        \n    return model","2c0b89dd":"import efficientnet.tfkeras as efn\nds_train     = get_dataset(files_train, CFG, augment=True, shuffle=True, repeat=True)\nds_train     = ds_train.map(lambda img, label: (img, tuple([label] * CFG['net_count'])))\n\nsteps_train  = count_data_items(files_train) \/ (CFG['batch_size'] * REPLICAS)\n\nmodel        = compile_new_model(CFG)\nhistory      = model.fit(ds_train, \n                         verbose          = 1,\n                         steps_per_epoch  = steps_train, \n                         epochs           = CFG['epochs'],\n                         callbacks        = [get_lr_callback(CFG)])","1bda13ee":"CFG['batch_size'] = 256\n\ncnt_test   = count_data_items(files_test)\nsteps      = cnt_test \/ (CFG['batch_size'] * REPLICAS) * CFG['tta_steps']\nds_testAug = get_dataset(files_test, CFG, augment=True, repeat=True, \n                         labeled=False, return_image_names=False)\n\nprobs = model.predict(ds_testAug, verbose=1, steps=steps)\n\nprobs = np.stack(probs)\nprobs = probs[:,:cnt_test * CFG['tta_steps']]\nprobs = np.stack(np.split(probs, CFG['tta_steps'], axis=1), axis=1)\nprobs = np.mean(probs, axis=1)","f3ab8163":"ds = get_dataset(files_test, CFG, augment=False, repeat=False, \n                 labeled=False, return_image_names=True)\n\nimage_names = np.array([img_name.numpy().decode(\"utf-8\") \n                        for img, img_name in iter(ds.unbatch())])","3a9be8cb":"for i in range(CFG[\"net_count\"]):\n    submission = pd.DataFrame(dict(\n        image_name = image_names,\n        target     = probs[i,:,0]))\n\n    submission = submission.sort_values('image_name') \n    submission.to_csv(f'submission_model_{i}.csv', index=False)","8e0bfc5b":"submission = pd.DataFrame(dict(\n    image_name = image_names,\n    target     = np.mean(probs[:,:,0], axis=0)))\n\nsubmission = submission.sort_values('image_name') \nsubmission.to_csv('submission_models_blended.csv', index=False)","b06d47fe":"final_target =  submission * 0.9 + 0.1*xgb_df","5aa2b3c8":"result = pd.DataFrame({\n        \"image_name\": test[\"image_name\"],\n        \"target\": final_target\n    })\n\nresult.to_csv('final_submission_blend.csv', index=False)","df133052":"# <font size=\"+2\" color=\"indigo\"><b>Handling Categorical Features<\/b><\/font><br>","d3c0659e":"> ### Function to return the count of files inside a directory","5e8ae4b7":"Preparing the submission file.\n\nNote that probs array created above will have predictions from all the 7 efficientnet models.\n\nWe can either take predictions from a model and report it as a final prediction or we can take mean of all the models output and report that as the output. I found the later approach more effective.","04726375":"> ### The above declared functions takes a TFRecord input and outputs values like its target,image_name etc.","67866e79":"> ### This shows that the dataset is highly imbalanced which was expected as this is a medical domain dataset.","82dcb5a3":"# <font size=\"+2\" color=\"navy\"><b>Setting up the Schedular<\/b><\/font><br>","1f6749b5":"> ### Here we are basically creating a schedular which sets the learning rate according to the number of epochs. Refer to the below diagram to understand how the learning rate is changing wrt. the number of epochs.\n\n![image.png](attachment:image.png)\n\n\n> ### You can also try to setup your own schedular.","68c7f2b0":"# <font size=\"+2\" color=\"green\"><b>Augmentation on images<\/b><\/font><br>\n\nBasically, Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n\nIt could be possible that our model is able to predict more accurately on an augmentated image than the actual image. This ensures that the model can give high score on one of the augmented form of actual image & hence will be able to predict class of an image more accurately.\n\nBelow function is performing the same on an image with preset augmentation parameters.","1cbe6607":"*Competition dataset comprises of:*\n\n1. Images(train+test) available in jpeg format.\n2. Images(train+test) in TFRecords format.\n3. Metadata available in csv format.","a80ea90e":"> ### Extracting all the image name from test file","7d9402d1":"> ### Storing outputs from individual models","48c3c643":"# <font size=\"+2\" color=\"grey\"><b>Let's have a look at few sample images<\/b><\/font><br>","cb0a449d":"We are going to use the images in the TFRecords format created by Chris. He has distributed all the images into 15 different TFRecords for both train & test dataset. His dataset also ensures that 1.7% of the images in each TFRecord  belongs to Melanoma class. Additionally, he has also created these record for different size of images. We are using here images of size 256 X 256. For more information on this dataset refer here - https:\/\/www.kaggle.com\/cdeotte\/melanoma-256x256","5ecf89bb":"# <font size=\"+2\" color=\"blue\"><b>Handling missing values<\/b><\/font><br>","471be59b":"# <font size=\"+2\" color=\"brown\"><b>Preparing the dataset in TFRecord format<\/b><\/font><br>","29acf9f1":"> ### Saving results","1f5c252b":"> ### Taking mean of all the outputs","0aecf553":"# <font size=\"+2\" color=\"aqua\"><b>Defining the architecture of our Ensembled(all B0-B7 models) EfficientNet CNN Model<\/b><\/font><br>","1294eec4":"> ### Predicting on Test set","ee42f1c5":"> ### I would recommend you to experiment with the above parameters to find the best fit set. Please ensure that you don't overfit your model. \n\n> ### Image size also plays an important role in classifying accurately. We are using images of size 256 X 256(read_size)but there are other size of images also available. To understand the importance of different size of images refer here - https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/discussion\/160147 ","5182f381":"# <font size=\"+2\" color=\"red\"><b>Dataset<\/b><\/font><br>","9644e422":"# <font size=\"+2\" color=\"coral\"><b>Let's perform some augmentation on them<\/b><\/font><br>","2222ddcd":"> This is the function which performs some additional augmentaion techniques on the input image in addition to our previously defined tranform function which was also performing image augmentation only. Note that the augmentaion on the input image will be only performed if the augment parameter is enabled. ","91f71f41":"# <font size=\"+2\" color=\"blue\"><b>Predictions on test set<\/b><\/font><br>","3f4d7196":"> ### Here only 1 output file will be generated and we are going to use these predictions as the final predictions made on images","e05a8c78":"> ### Features like age_approx,sex,anatom_site_general_challenge could be useful in classifying","77d1302a":"# <font size=\"+2\" color=\"red\"><b>Let's talk about the images now<\/b><\/font><br>\n\nAs mentioned earlier, we are using here the 256X256 size TFRecord files created by Chris.\n\nThe CNN model that we are going to use here is an ensembled model. Thanks to AgentAuers for suggesting this idea.\n\nBasically we are passing the input image to all the B0-B6 EfficientNets models individually.Each model generates its own output. Finally we can either take prediction from an individual model(one of B0-B6) or we can take mean of all the predictions as our final result. I achieved better result by implementing the later approach.\n\n\n\n![image.png](attachment:image.png)\n\n\nAfter getting the results from the CNN model we will ensemble it with the results obtained from the XGBoost model.","1cc848d9":"# \u2714\ufe0fPLEASE GIVE THIS NOTEBOOK AN UPVOTE IF YOU LIKED IT!!!","54b8574f":"> ### You will find 7 outputs files generated for each EfficientNet model respectively","c687606e":"The idea here is to make prediction on each test set image with augmentation performed on it. Each image is augmented let say 'N' number of times and prediction is made for every augmentation & finally the mean of all the predictions is reported as the actual output. Same step is performed for all the test set images.","cc20d7f7":"# <font size=\"+2\" color=\"green\"><b>References of other notebooks used in this project<\/b><\/font><br>\n\nhttps:\/\/www.kaggle.com\/agentauers\/incredible-tpus-finetune-effnetb0-b6-at-once\n\nTFRecords - https:\/\/www.kaggle.com\/cdeotte\/melanoma-256x256","8a88b7b5":"> ### Substituting \"na\"\/0 in case no data is available in that field","5f66c228":"![](http:\/\/s1.thingpic.com\/images\/nv\/SebfTwBfe86kd2YeuiDmWxwf.jpeg)","dadcff3a":"# <font size=\"+2\" color=\"indigo\"><b>Training the model<\/b><\/font><br>","486231ec":"# <font size=\"+2\" color=\"teal\"><b>Importing relevant Libraries<\/b><\/font><br>","3a44b56e":"# <font size=\"+2\" color=\"black\"><b>Building XGBoost model<\/b><\/font><br>","d8b13bb9":"> ### Installing EfficientNet Model","599934f7":"> ### Taking only the relevant features","96d3e0cf":"# <font size=\"+2\" color=\"indigo\"><b>Approach we are going to follow<\/b><\/font><br>\n\nThis competition aims to detect whether a patient has skin cancer or not by analyzing an image of a part of skin & some additional information(metadata) related to the patient like age,gender etc.\n\nWe can build a model based upon either image\/meta data or both.Obviously, you will get more accurate results after using both the data for predictions. We are going to use both the information.\n\nWe will build 2 models(one for metadata & another for image data). At the end we will combine prediction from both the models & report it as the final predictions. We will discuss this in more detail later on.\n\nFor building a model on the metadata information I have used XGBoost classifier. Thanks to Paras Varshney for suggesting this approach. Link to his kernel - https:\/\/www.kaggle.com\/blurredmachine\/siim-isic-an-ensemble-beginner-s-approach\n\n\nLets first build this model","da4a2a61":"> ### Note how are we defining loss for each efficientnet model. You may refer to the logs generated at the time of training this model. You will find different losses there corresponding to each efficientnet model.","c52c683a":"\n# <font size=\"+2\" color=\"indigo\"><b>Configuration Setup<\/b><\/font><br>","7a3fc68c":">### LB Score of approx 0.70 can be achieved just by submitting the predictions by this model","dbc51446":">### The above Transform function accept an image as input and outputs an image randomly rotated,sheared & zoomed.","dfbd3203":"# <font size=\"+2\" color=\"red\"><b>Ensembling with the meta data predictions<\/b><\/font><br>"}}