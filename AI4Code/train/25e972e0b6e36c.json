{"cell_type":{"dc870b1b":"code","d09a2e27":"code","119f79aa":"code","4405272d":"code","78a85060":"code","125b12f6":"code","a021eb95":"code","e640fcae":"code","18dec758":"code","cf244879":"code","18b47f3f":"code","32f376ab":"code","d159c459":"code","32239f0d":"code","459b0ec2":"code","975c4fe8":"code","2bf222d4":"code","bf17133d":"code","30bc1ac6":"code","442fa124":"code","44426fa1":"code","fef38b08":"code","ab333e12":"code","90b74e78":"code","3a3f32e7":"code","98d857f2":"code","21a7fa69":"code","9277d068":"code","d3cf2fa9":"code","0faf2f58":"code","aad2d3cc":"code","8de1e1aa":"code","790c9ec4":"code","0f0ae9e6":"code","9a266f70":"code","2d50e4fd":"code","0ae3dc82":"code","ba9a3b83":"code","c8b33a81":"code","08aacd30":"code","862fae86":"code","7a837e07":"code","b436de5e":"code","773542aa":"code","5cbf7044":"code","520c2960":"code","e0660e0c":"code","b71fd543":"code","88b9143f":"code","2dbab304":"code","5470221b":"code","c2aab769":"code","bf2336a4":"code","248b0ac8":"code","ba64996c":"code","4f9f0348":"code","c6dacb99":"code","302c1b16":"markdown","cad21b86":"markdown","7c8bc199":"markdown","5a114cef":"markdown","b6fd275b":"markdown","f312f956":"markdown","5fbccbfd":"markdown","0f0ee436":"markdown","e9c26b8c":"markdown","fa1aeca3":"markdown","e0884c9f":"markdown","6a38ba5e":"markdown","e65a4e69":"markdown","e266cd31":"markdown","6a7eaab5":"markdown","624858fd":"markdown","d90709f5":"markdown","4d5e1466":"markdown","3bb60b09":"markdown","98b2efb9":"markdown","249c8987":"markdown","da7c4909":"markdown","c780a385":"markdown","09431b95":"markdown","aa438f64":"markdown","a39caa13":"markdown","13b90934":"markdown","832c6ef3":"markdown","4c30f044":"markdown","d90b2ff2":"markdown","09983bbc":"markdown","bb6916a0":"markdown","5dc91d29":"markdown","253a6b8b":"markdown","97e6d1a1":"markdown","a1c7b9b3":"markdown","bce0bc59":"markdown","3cd8a165":"markdown","477a76fc":"markdown","c9ce1553":"markdown","67a6ff26":"markdown"},"source":{"dc870b1b":"import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport warnings\nfrom scipy.stats import skew,boxcox\nwarnings.filterwarnings('ignore')\n\n#import findspark\n#findspark.init()\n#import pyspark\n\n#from sklearn.metrics import classification_report, confusion_matrix\n\npd.options.display.max_columns = 35\npd.options.display.max_rows = 100","d09a2e27":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\noriginal_df = df.copy()","119f79aa":"def nlarges_nulls(df,n=20):\n    total_lines = len(df)\n    nulls = pd.DataFrame(\n        df.T.apply(lambda x: round(np.count_nonzero(x.isnull())*100\/total_lines,2),axis=1),\n        columns=['Percent of nulls']\n        )\n    return nulls.sort_values(by='Percent of nulls',ascending=False).head(n)\n    \nnlarges_nulls(df)","4405272d":"garage_check = df[df['GarageCond'].isnull()]\ngarage_check[['GarageYrBlt','GarageCond','GarageType','GarageFinish','GarageFinish','GarageCars']].head(100)","78a85060":"total_lines = len(df)\nprint(\"Percent of the corresponding nulls = {:.2f}%\".format(len(garage_check)\/total_lines*100))","125b12f6":"basement_check = df[df['BsmtCond'].isnull()]\nbasement_check[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].head(100)","a021eb95":"featureset = ['PoolQC','Alley','Fence','FireplaceQu']\nf,ax = plt.subplots(2,2,figsize=(14,10))\n\nfor row in range(2):\n    for col in range(2):\n        ax[row,col].hist(df[featureset[row*2+col]].replace(np.nan,'Absent'))\n        ax[row,col].title.set_text(featureset[row*2+col])\n        ax[row,col].grid(True,linestyle=':')\nplt.show()","e640fcae":"f,ax = plt.subplots(figsize=(12,8))\nplt.hist(df['MiscFeature'].replace(np.nan,'None'))\nplt.grid(True,linestyle=':')","18dec758":"f, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(df.corr(), vmax=.8, square=True)","cf244879":"cols_max = list(df.corr().nlargest(10, 'SalePrice')['SalePrice'].index)\ncols_min = list(df.corr().nsmallest(10, 'SalePrice')['SalePrice'].index)\ncumcols = [cols_max,cols_min]\n\nif 'Id' in cols_min or 'SalePrice' not in cols_min:\n    for i in range(len(cols_min)):\n        if cols_min[i] == 'Id':\n            cols_min[i]='SalePrice'\n            continue\n\nf,ax = plt.subplots(1,2,figsize=(12,8))\nfor i in range(2): \n    cm = np.corrcoef(df[cumcols[i]].values.T)\n    sns.set(font_scale=1.25)\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, \n                     fmt='.2f', annot_kws={'size': 10}, yticklabels=cumcols[i], xticklabels=cumcols[i],\n                    ax=ax[i])\nf.tight_layout(pad=3)\nplt.show()","18b47f3f":"df['Electrical'].value_counts()","32f376ab":"from collections import Counter\nprint('Most common Electrical values for the neighbourhood in descending (by frequency) order: ')\nTRAIN_ELECTRICITY_FREQ = df.groupby('Neighborhood')['Electrical'].apply(lambda x: Counter(x).keys())\nTRAIN_ELECTRICITY_FREQ","d159c459":"TRAIN_LOTFRNT_MEDIAN = df.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.median())\nTRAIN_LOTFRNT_MEDIAN","32239f0d":"def prep_fill_nulls(df):\n    #Define the features which nas' will be substituted with string value\n    no_features_cols = df.select_dtypes(object)\n    #Define the features which nas' will be substituted with numerical value\n    zero_feature_cols = df.select_dtypes(np.number)\n    \n    #Apply imputing\n    for col in no_features_cols:\n        df[col] = df[col].fillna('None')\n    \n    for col in zero_feature_cols:\n        df[col] = df[col].fillna(0)\n    \n    #Apply transforamtion using the data collections based on neigborhoods\n    df['Electrical'] = df.groupby('Neighborhood')['Electrical'].transform(\n                lambda x: x.fillna( list(TRAIN_ELECTRICITY_FREQ[x.name])[0]))\n    \n    df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(\n        lambda x: x.fillna( TRAIN_LOTFRNT_MEDIAN[x.name]))\n    return df","459b0ec2":"df = prep_fill_nulls(df)\nnlarges_nulls(df,n=5)","975c4fe8":"from sklearn.ensemble import IsolationForest","2bf222d4":"continuous_df = df.select_dtypes(np.number)\ndescrete_df = df.select_dtypes(object)\n\n#We know that there should not be too many of the outliers thus contamination is set to be very low\ndetector = IsolationForest(n_estimators=150, max_samples='auto', contamination=float(0.005),max_features=1,\n                          random_state=777)\ndetector.fit(continuous_df)","bf17133d":"df['cont_anomaly'] = detector.predict(df[continuous_df.columns])","30bc1ac6":"def display_continuous_outliers(fsize=(8,200)):\n    nfeatures = len(continuous_df.columns)\n    f,ax = plt.subplots(nfeatures,1,figsize=fsize)\n    for row in range(nfeatures):\n        sns.scatterplot(data=df, x=continuous_df.columns[row], y=\"SalePrice\", hue=\"cont_anomaly\",ax=ax[row])\n    plt.tight_layout(pad=3)\n    plt.show()","442fa124":"# -1 - means outlier\ndisplay_continuous_outliers()","44426fa1":"nfeatures = len(descrete_df.columns)\nf,ax = plt.subplots(nfeatures,1,figsize=(8,280))\nfor row in range(nfeatures):\n    sns.boxplot(data=df, x=descrete_df.columns[row], y=\"SalePrice\",ax=ax[row])\n    ax[row].set_xticklabels(ax[row].get_xticklabels(),rotation=60)\nplt.tight_layout(pad=3)\nplt.show()","fef38b08":"#Lets display points that are outliers according to the IQR critera on the graph\niqr_magnitude = 2.0\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3-Q1\ncontinuous_df\nbefore_iqr_df = df.copy()\n\ncontinuous_df = continuous_df.drop(columns=['Id','MSSubClass','OverallCond','YearRemodAdd','GarageYrBlt',\n                                                'MoSold','YrSold','LotFrontage','GarageCars',\n                                           'Fireplaces','TotRmsAbvGrd','KitchenAbvGr','BedroomAbvGr',\n                                           'HalfBath','FullBath','BsmtHalfBath','BsmtFullBath','YearBuilt',\n                                           'OverallQual'])\n\n#This function assigns the marker -2 to the \n#cont_anomaly of row if its value of a feature\n#lies further away than iqr_magnitude*IQR from \n#median of the feature.\ndef add_iqr_outlier_marker(omit_marker=None):\n    for feature,i in zip(continuous_df\n                         .columns,range(2,len(continuous_df.columns)+2)):\n        for index, row in df.iterrows():\n            if omit_marker:\n                if row[\"cont_anomaly\"] in omit_marker:\n                    continue\n            if row[feature] > Q3[feature]+iqr_magnitude*IQR[feature] or \\\n            row[feature] < Q1[feature]-iqr_magnitude*IQR[feature]:\n                df.loc[index,\"cont_anomaly\"] = -2\nadd_iqr_outlier_marker(omit_marker=[-1])","ab333e12":"print(\"There are {} out of total {} rows which are outliers according to the IQR statistical technique.\".format(\n        df[df[\"cont_anomaly\"]<0][\"cont_anomaly\"].count(),len(df)))","90b74e78":"before_iqr_df['cont_anomaly'].unique()","3a3f32e7":"iqr_magnitude = 10.0\ndf = before_iqr_df.copy()\nadd_iqr_outlier_marker(omit_marker=[-1])","98d857f2":"print(\"There are {} out of total {} rows which are outliers according to the IQR statistical technique.\".format(\n        df[df[\"cont_anomaly\"]<0][\"cont_anomaly\"].count(),len(df)))","21a7fa69":"display_continuous_outliers(fsize=(8,100))","9277d068":"continuous_df = continuous_df.drop(columns=['BsmtFinSF2','LowQualFinSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea',\n                                           'MiscVal','SalePrice'])\n\niqr_magnitude = 4.5\ndf = before_iqr_df.copy()\nadd_iqr_outlier_marker(omit_marker=[-1])","d3cf2fa9":"print(\"There are {} out of total {} rows which are outliers according to the IQR statistical technique.\".format(\n        df[df[\"cont_anomaly\"]<0][\"cont_anomaly\"].count(),len(df)))","0faf2f58":"display_continuous_outliers(fsize=(8,50))","aad2d3cc":"df = df[~df['Id'].isin([935,1299])] #yeah this weird but df = df[df['LotFrontage']<300] deletes 200+ rows instead just the two for some reason.\ndf = df[df['LotArea']<100000]\ndf = df[df['MasVnrArea']<1400]\ndf = df[df['BsmtFinSF2']<1400]\ndf = df[~((df['OpenPorchSF']>500) & (df['SalePrice']<100000))]\n\ndf = df[df['cont_anomaly']!=(-1)]\n#Optional\ndf = df[df['cont_anomaly']!=(-2)]","8de1e1aa":"df.describe()","790c9ec4":"def prep_cont_to_desc(df):\n    cont_to_descr = ['MSSubClass','OverallCond','YearRemodAdd','GarageYrBlt',\n                    'MoSold','YrSold','GarageCars',\n                    'Fireplaces','TotRmsAbvGrd','KitchenAbvGr','BedroomAbvGr',\n                    'HalfBath','FullBath','BsmtHalfBath','BsmtFullBath','YearBuilt',\n                    'OverallQual']\n\n\n    for feature in cont_to_descr:\n        df[feature] = df[feature].astype(str)\n    df[cont_to_descr].info()\n    return df\ndf = prep_cont_to_desc(df)","0f0ae9e6":"df = df.drop(columns=['cont_anomaly'])\ndf.to_csv('train_clean.csv',index=False)","9a266f70":"numeric_feats = list(df.select_dtypes(np.number))\n\nskewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(15)","2d50e4fd":"most_skewed = skewness.index[:15]\nf,ax = plt.subplots(15,4,figsize=(15,60))\nplt.tight_layout(pad=3)\n\nfor feature,index in zip(most_skewed,range(15)):\n    ax[index,0].title.set_text('Original distribution')\n    sns.histplot(data=df, x=feature, kde=True, ax=ax[index,0])\n    \n    ax[index,1].title.set_text('Log-transformed distribution')\n    df[feature+'_log'] = df[feature].replace(0,1e-6)\n    df[feature+'_log'] = np.log(df[feature+'_log'])\n    sns.histplot(data=df, x=feature+'_log', kde=True, ax=ax[index,1])\n    \n    ax[index,2].title.set_text('Sqrt-transformed distribution')\n    df[feature+'_sqrt'] = np.sqrt(df[feature])\n    sns.histplot(data=df, x=feature+'_sqrt', kde=True, ax=ax[index,2])\n    \n    if df[feature].min() > 0:\n        ax[index,3].title.set_text('BoxCox-transformed distribution')\n        df[feature+'_bx'] = boxcox(df[feature])[0]\n        sns.histplot(data=df, x=feature+'_bx', kde=True, ax=ax[index,3])","0ae3dc82":"df_train = prep_fill_nulls(original_df)\ndf_train = prep_cont_to_desc(df_train)\nnlarges_nulls(df_train,n=5)","ba9a3b83":"df_test = prep_fill_nulls(df_test)\ndf_test = prep_cont_to_desc(df_test)\nnlarges_nulls(df_test,n=5)","c8b33a81":"Y_train = df_train['SalePrice']\nX_train = df_train.drop(columns=['SalePrice','Id'])\ndf_test = df_test.drop(columns=['Id'])\nX_train","08aacd30":"from sklearn.preprocessing import RobustScaler,LabelEncoder\nLABEL_ENCODERS = {}\n\ndef build_encoder(train_df,test_df):\n    cat_features = df.select_dtypes(object)\n    for col in cat_features:\n        LABEL_ENCODERS[col] = LabelEncoder()\n        LABEL_ENCODERS[col].fit(pd.concat([train_df[col], test_df[col]], axis=0, sort=False))\n\ndef factorize_df(df):\n    cat_features = df.select_dtypes(object)\n    for cat_feature in cat_features:\n        df[cat_feature] = LABEL_ENCODERS[cat_feature].transform(df[cat_feature])\n    return df\n\nbuild_encoder(X_train,df_test)\nX_train = factorize_df(X_train)\nX_train","862fae86":"scaler = RobustScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train),\n                       columns=X_train.columns)\nX_train = X_train.to_numpy()","7a837e07":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression,ElasticNet\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n\ndef check_model(regressor,error_func,nfold,X_train=X_train,Y_train=Y_train):\n    kfold=KFold(n_splits=nfold, shuffle=False)\n    fold_scores = []\n    fold_error = []\n\n    for train_index, test_index in kfold.split(X_train):\n        Xtr,Xval = X_train[train_index],X_train[test_index]\n        ytr,yval = Y_train[train_index],Y_train[test_index]\n\n        model = regressor.fit(Xtr,ytr)\n        fold_scores.append(model.score(Xval,yval))\n        fold_error.append(error_func(yval,model.predict(Xval)))\n\n    print(\"Mean MAE is: {}\\nMean scroe is: {}\".format(np.mean(fold_error),np.mean(fold_scores)))\n\n    f,ax = plt.subplots(1,2,figsize=(12,6))\n\n    ax[0].plot(range(len(fold_error)),\n             fold_error,'r-',\n             label='Error')\n    ax[1].plot(range(len(fold_scores)),\n             fold_scores,'b--',\n             label='Score')\n    ax[0].legend()\n    ax[1].legend()\n    plt.show()\n    \ncheck_model(LinearRegression(),mean_absolute_error,5)","b436de5e":"check_model(ElasticNet(alpha=0.0005,l1_ratio=.9,random_state=777),mean_absolute_error,5)","773542aa":"check_model(KernelRidge(alpha=0.1,kernel='polynomial',degree=2),\n            mean_absolute_error,\n            5\n           )","5cbf7044":"from sklearn.preprocessing import PolynomialFeatures\n\ndef PolynomialFeatures_labeled(input_df,power):\n    '''Basically this is a cover for the sklearn preprocessing function. \n    The problem with that function is if you give it a labeled dataframe, it ouputs an unlabeled dataframe with potentially\n    a whole bunch of unlabeled columns. \n    Inputs:\n    input_df = Your labeled pandas dataframe (list of x's not raised to any power) \n    power = what order polynomial you want variables up to. (use the same power as you want entered into pp.PolynomialFeatures(power) directly)\n    Ouput:\n    Output: This function relies on the powers_ matrix which is one of the preprocessing function's outputs to create logical labels and \n    outputs a labeled pandas dataframe   \n    '''\n    poly = PolynomialFeatures(power)\n    output_nparray = poly.fit_transform(input_df)\n    powers_nparray = poly.powers_\n\n    input_feature_names = list(input_df.columns)\n    target_feature_names = [\"Constant Term\"]\n    for feature_distillation in powers_nparray[1:]:\n        intermediary_label = \"\"\n        final_label = \"\"\n        for i in range(len(input_feature_names)):\n            if feature_distillation[i] == 0:\n                continue\n            else:\n                variable = input_feature_names[i]\n                power = feature_distillation[i]\n                intermediary_label = \"%s^%d\" % (variable,power)\n                if final_label == \"\":         #If the final label isn't yet specified\n                    final_label = intermediary_label\n                else:\n                    final_label = final_label + \" x \" + intermediary_label\n        target_feature_names.append(final_label)\n    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n    return output_df\n\nY_train_poly = df_train['SalePrice']\nX_train_poly = df_train.drop(columns=['SalePrice','Id'])\n\nX_train_poly = factorize_df(X_train_poly)\n\nX_train_poly = PolynomialFeatures_labeled(X_train_poly,2)\n\nscaler_poly = RobustScaler()\nX_train_poly = pd.DataFrame(scaler_poly.fit_transform(X_train_poly),\n                            columns=X_train_poly.columns)\n\nX_train_poly","520c2960":"check_model(GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =777),\n            mean_absolute_error,\n            5\n           )","e0660e0c":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,regularizers\nfrom sklearn.model_selection import train_test_split\n\nY_train = df_train['SalePrice']\nX_train = df_train.drop(columns=['SalePrice','Id'])\nX_train = factorize_df(X_train)\nscaler = RobustScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train),\n                       columns=X_train.columns)\nX_train = X_train.to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X_train, Y_train, test_size=0.33, random_state=666)","b71fd543":"model_input = keras.Input(shape=X_train.shape[1],name='model_input')\nx = layers.Dense(1024,\n                 activation='selu',\n                 bias_initializer=\"glorot_uniform\")(model_input)\nx = layers.Dense(1024,\n                 activation='selu',\n                 bias_initializer=\"glorot_uniform\")(x)\nx = layers.Dense(512,\n                 activation='selu',\n                 bias_initializer=\"glorot_uniform\")(x)\nx = layers.Dense(512,\n                 activation='selu',\n                 bias_initializer=\"glorot_uniform\")(x)\nx = layers.Dense(128,\n                 activation='selu',\n                 bias_initializer=\"glorot_uniform\")(x)\nx = layers.Dense(128,\n                 activation='selu',\n                 bias_initializer=\"glorot_uniform\")(x)\nx = layers.Dense(64,                 \n                 activation='selu',\n                 bias_initializer=\"glorot_uniform\")(x)\noutput = layers.Dense(1)(x)\nmodel = keras.Model(model_input,output)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001,amsgrad=True),\n              loss='mse',\n              metrics=[keras.metrics.MeanSquaredError()])\n\ncallbacks = [keras.callbacks.ReduceLROnPlateau(\n            monitor=\"mean_squared_error\",\n            factor=0.5,\n            patience=10,\n            min_delta=0.0001,\n            min_lr=0.0001,\n            verbose=False\n            ),\n            keras.callbacks.ModelCheckpoint(\n            'weights', \n            monitor='mean_squared_error',\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=False\n            )]","88b9143f":"history = model.fit(\n          x=X_train,\n          y=y_train,\n          batch_size=978,\n          epochs=300,\n          callbacks=callbacks,\n          shuffle=True,\n          validation_data=(X_test,y_test),\n          verbose = False\n         )","2dbab304":"plt.plot(history.history['mean_squared_error'],\n        'r-',label = 'Trining error')\n\nplt.plot(history.history['val_mean_squared_error'],\n        'b--',label = 'Validation error')\n\nplt.legend(loc='best')\nplt.show()","5470221b":"model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001,amsgrad=True),\n              loss='mae',\n              metrics=[keras.metrics.MeanAbsoluteError()])","c2aab769":"history = model.fit(\n          x=X_train,\n          y=y_train,\n          batch_size=978,\n          epochs=120,\n          callbacks=callbacks,\n          shuffle=True,\n          validation_data=(X_test,y_test),\n          verbose = False\n         )","bf2336a4":"plt.plot(history.history['mean_absolute_error'],\n        'r-',label = 'Trining error')\n\nplt.plot(history.history['val_mean_absolute_error'],\n        'b--',label = 'Validation error')\n\nplt.legend(loc='best')\nplt.show()","248b0ac8":"model_for_test = model.load_weights('.\/weights')","ba64996c":"from sklearn.metrics import mean_absolute_error\npred = model.predict(X_test)\nprint(\"MAE: {}\".format(mean_absolute_error(y_test, pred)))","4f9f0348":"df_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test = prep_fill_nulls(df_test)\ndf_test = prep_cont_to_desc(df_test)\nx = df_test.drop(columns=['Id'])\nx = factorize_df(x)\nx\nx = pd.DataFrame(scaler.transform(x),\n                       columns=x.columns)\nx = x.to_numpy()\ntest_prediction = model.predict(x)","c6dacb99":"predicted_df = pd.DataFrame(test_prediction,columns=['SalePrice'])\npredicted_df['Id'] = df_test['Id']\npredicted_df.to_csv('submission.csv',index=False,columns=['Id','SalePrice'])","302c1b16":"Given all the finding from above, let's build the function which we will be able to use later to preprocess test-data in as well as the training data at this point.","cad21b86":"### Imputing the data","7c8bc199":"### Feature engineering ","5a114cef":"We have defined and trained the IsolationForest model to predict of row is an outlier. Lets check how well the predictions worked out on graph.","b6fd275b":"Earlier we found out that some of the numerical values are actully rather categorical. Clearly we will have to factorize all of the categorical values anyway later in the modelling phase. However, explicitly separating them from numerical in the initial dataset will aleviate working with it during the later iterations of explaratory phase and analisys.\n\nHaving this said, let's transfrom the features which we discovered to have categorical nature.","f312f956":"There are quite a few of the features and thus the relations between them. In order to clean the data from outliers let us automate the process by means of the IsolationForest with a very small value for the data contaminatiopn level, assuming that there are a tiny number of outliers and see evaluate it while we will be studying the relation of the features with the target value.","5fbccbfd":"Lets build up a base-line model to track the progress we have. We can start with linear regression model for the begining.However, before start building the model, we would need to normalize the data.","0f0ee436":"Now that seems to be an overwhelming number of feature to check manually lets try to reduce the number of the feature to few the most significant ones. ","e9c26b8c":"We have set the magnitude of the IQR to be 2 instaed of conventional 1.5.\nAs we can see there are approximately 40% of the datarows where the value of one of the features is out of the admissible range. If we were to rely on it we would have to delete most of the data, which is why I consider this method inappropriate here. \nBut if we loof at the scatter plot it has marked many points which are located on a a big distances from the majority.\nSo lets try increasing the admissible range once again and see how it looks.","fa1aeca3":"At the end, lets quickly check if there are some abnormal values in the dataset now, we woul dbe looking for negativa values of infitites, but the data seems to be fine.","e0884c9f":"Finally we are loading the saved \"best\" weights and predicting the prices for the testing set.","6a38ba5e":"We can see that mostly all of the features are positively correlated with each otehr, what's important many of them have high correlation, which might be serve us well in the future. Since, he stronger the correlation, the more difficult it is to change one variable without changing another. It becomes difficult for the model to estimate the relationship between each independent variable and the dependent variable independently because the independent variables tend to change in unison. To address that we will have to use feature selection techniques later along the working process. \n\nSome of the other insights we can infer from the heatmap:\n\n- GarageArea and GarageCars have strong correlation with SalePrice, so we might want to only consider one later.\n- FirePlaces indicates a strong correlation with the target value as well so delting FirePlaceQU we will not loose much if anything at all.\n- BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath  missing values are likely due to having no basement so we can impute zero value.\n- BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 For all these categorical basement-related features, NaN most likely means that there is no basement.","e65a4e69":"Now tha leaves us a much cleaner data distribution especailly for LotArea feture where most of the huge and and not so expensive buildings were marked as outliers, same goes for MasVnrArea, so I think we may delete those newly added outliers. Let's save the final dataframe.","e266cd31":"We can see that even though the number of missing values in each of the columns above is overwhelming, there some portion of samples is distributed among all the other classes, so deleting those features complitely may well lead to lost of a valuable parameter. Thus, we will be applying imputing in these cases as well, leaving the possibility of reconsidering that decision on the following iteration of the analisys.","6a7eaab5":"As we can observe, linear regression and elastic net models provide some very similar results in turms of MAE in the folds and the scores. Things change negatively when we add polynomial kernel trick to the scene, perhaps in order it to work better we might consider cleaning data a little bit better (e.g. eliminating highly correlated values).","624858fd":"Same can be applied to LotFrontage feature, the value is naturally the same for the same neighbor. So we will group the rows with the missing values by the neighbor and impute the median value.","d90709f5":"Absolutely the same applies to the basement-absent fields.\n\nAll in all, we believe that substituting the absent values in these case is reasonambe at this point. On the modelling\/training phase we maight reconsinder this depending on the performance or furhter discovered insights.","4d5e1466":"After examining the numerical features, lets have a brief look at the distribulion in the categorical features.\nSome of the findings:\n\n- Based on IQR criteria there migth be too many of the outliers to delete them all, we will look at it closer on the graph. (We migth increase the threshold up to 2IQRs)\n- LotShape: lots with slightly irregular shapes are among the most expensive items in the dataset, which might be due to some architectural solutions and might not be such a critical point.\n- LandContour: same as above.\n- LandSlope: the most convenient type of surface is Gtl.\n- Even though there are a lot of point out of the 1.5*IQR range we would not consider those 100% outliers, we rather would make this range wider and consider only the most extremal cases.\n","3bb60b09":"Now we have a suitable for ML models data. Lets build some basic, very simple regression models employing Kfold technique to see how well they perform perfprm and see if we can get better results gradually complicating the resulting model.","98b2efb9":"Validation MAE has fallen 5k on average and the score on validation data is more than 0.85 now which is a great improvement.","249c8987":"In general we agree with the predictions, the points it has marked as outliers (-1) show up in the extremities of one or the other plot. We might even add to the level opf the contamination on one of the next iteration. However, while the examination we identified several points that may be delted manualy as they apparently are outliers.\n\n- LotFrontage: there is one big house (>300) for a very low price that was not marked as outlier. There are 2 in total so we could delete points with LotFrontage>300.\n- LotArea: there are couple of point of the right side as well, huge areas but the price is low. Such data is more likely to introduce noise into the model. We could consider deleting points with LotArea>100000\n- MasVnrArea: there is the point with MasVnrArea~1600 wich is by far more than the avg for the dataset and it has a pretty low saleprice. Likely to introduce disturbance to the model.\n- BsmtFinSF2: there are 2 points that might be considered as additional outliers. Or rather just one >1400\n- OpenPorchSF: the point with huge porch area > 500 and terribly low price.\n\n- MSSubCLass, OverallCond, [YearRemodAdd],[GarageYrBlt], [MoSold], [YrSold] are in fact categorical and should be transformed into strings later on.","da7c4909":"We can see that there quite a bit of skeweness in the data, lets see if we can make it normal-like.","c780a385":"Lets clear the dataset from the predicted and identified outliers.","09431b95":"Modelling phase.","aa438f64":"We can see that due to predominant absense of the feature in many of the houses the houses which have the feature become too distinct so by IQR approach they are outliers. Lets exclude those features from consideration. ","a39caa13":"We are going to be building a fully-connected artificail nural network using a framework over tensorflow - Keras.\nTo do that we need to factorizee and normalize data first:\n- factorization is done by a custom function which uses distinct label encoders for each feature so that the test data could be factorized accordingly.\n- we are performing normalization using a RobustScaler instance provided by skleran library. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile)\n\nNext we split training set onto two subsets a larger one - training set and a smaller one (33% of the data) - validation set.\n","13b90934":"Most of the feature seem to not benifit much from any of the transformations, may be just LotArea (with BoxCox-transformed version),OpenPorchSF (sqrt), WoodDescSF (sqrt),BsmtUnfSF(sqrt),1stFlr(BoxCox) and GrLivArea (BoxCo)can be substitutedlater on after the baseline model is built-just to check if the model score impoves after the transformation.","832c6ef3":"Here we may consider 10 highest correlations in the dataset and 10 least.\n- It looks natural, since the features most responsible for price constitution are the year house was built, overal quality, overal area of the house, number of buthrooms etc.\n- There is no high negative correlation values in the dataset, which makes sense as well, usually presence of a feature or its higher quality\/value would add up to the cost of the building.","4c30f044":"### Outliers filerting ","d90b2ff2":"The structure of the nueral model is pretty simple:\n- one input layer connected to six fully connected hidden layer and an output layer;\n- hidden layer have different number of nurons;\n- all hidden layers uses the same activation function - selu (Scaled Exponential Linear Unit ). Which is defined as: if x > 0: return scale * x and if x < 0: return scale * alpha * (exp(x) - 1);\n- We are using Adam optimizer to compute the optimal weights for the network\n- mean squared error is used as loss and metrics\n- the model we use also have a couple of checkpoints: one for saving the weights corresponding to the best validation error, the second for adjusting learning rate when the model start stagnating. \n\nThe number of neurons in every layers and the layers number were gradually increased in order to get the modell overfitted and after reaching overfitting point adjast all the other parameter to get the best performance.","09983bbc":"What about the rest of the features where percentage of nulls <= 5.55?\n\n- Garage...: all fields have the same number of missing values so it is highly likely that they refer to the same set of data.\n  \n- I suspect that with fileds Bsmt... we are dealing with a simillar case.\nLets verify those theories.","bb6916a0":"There is one value missing in the Electrical column we could wither substitute it with the overwelminghly predominant SBrkr or delete the row since its only one. \nHowever we could also substitute if the value common for the neighbor of the building.","5dc91d29":"Since we are going to be building a baseline model, where the first choise is most probably going to be Linear-regression, we may as well check how skewed our data and apply some means in order to fix the most skewed features. This is not necessery and crutial for us, but, since some models ( Linear Discriminant Analysis, or Quadratic Discriminant Analysis) rely on the assumption of the distribution being a multivariate normal. We might want to know how much normally our data is distributed.\nThis way we might descover some transformation which we can benifit from later on. However this step could be omitted, during the modelling phase it may come in handy.","253a6b8b":"We might want to use mean absolute error as metric instead to get a better understanding of how close to the real price we get while training. Although, typically squared error allows to get better results due to heavier penalty to the larger errors. ","97e6d1a1":"### Preprocessing","a1c7b9b3":"Let's drill into what the absaent values mean and how we can address them.\nThere are several features (PoolQC,MiscFeature,Alley,Fence,FireplaceQu,[LotFrontage]) missing overwhelming number of values, at this point we could delete the column from consideration complitely but lets check with data description on whether we can impute them instead. \n\n- NA in PoolQC means No pool, so we could just substitute it with a corresponding string value\n- MiscFeature is too arbitrary to include \n- NA in Alley means - no access, could be substituted with a string value\n- NA in Fence = no fance \n- NA in FirePlaceQU = no fireplace\n- LotFrontage is a continuous value (I have read this area is mostly the same for all the houses in the neighborhood, so we could substitute it with the median for the neigborhood).\n\n","bce0bc59":"Lets transform string values to numerical and scale the dateset.","3cd8a165":"This proves that the missing data in the GarageX field corresponds to the same set of data referring to the absence of the garage. So we could safely impute a string value for instance 'None' in this case. Let's proceed with the basement fields.","477a76fc":"We are wil be marking detected outliers with negative values. In this case the algorithm assigned -1 value, later on we will assigne -2 for a method based on IQR.","c9ce1553":"# \u0410\u043b\u0435\u043d, \u0432\u043e\u0442 \u043e\u0442 \u0441\u044e\u0434\u0430 \u0431\u0443\u0434\u0435\u0442 \u043c\u043e\u0436\u043d\u043e \u0431\u0440\u0430\u0442\u044c \u0432\u0441\u0435, \u0434\u043e \u044d\u0442\u043e\u0433\u043e \u0438\u0434\u0443\u0442 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0438 (\u0421\u0430\u0441\u0438\u043d\u0434\u0443 \u0438\u0445 \u0441\u0434\u0435\u043b\u0430\u043b \u0432 \u043b\u044e\u0431\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u0435\u0435) \u0438 \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438; \u0442\u0430\u043a \u0447\u0442\u043e \u044f \u0434\u0443\u043c\u0430\u044e \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u0442\u044c \u0438\u0445 \u0432 \u0444\u0438\u043d\u0430\u043b\u043a\u0443 \u0434\u0443\u043c\u0430\u044e \u043d\u0435 \u0441\u0442\u043e\u0438\u0442.","67a6ff26":"The graph above display the learning process in terms of error. We can clearly see that the overfitting occurs somewhere around 50th epoch, so when making final predictions of the test dataset we would set the number of epoch close to that number."}}