{"cell_type":{"bc51d69e":"code","987fedeb":"code","b0f24269":"code","f0cdc112":"code","45339554":"code","83d22c20":"code","7bb55331":"code","65874dee":"code","22cdb705":"code","b5b719ef":"code","8061e455":"code","375b493b":"code","519c4147":"code","7e1e9a6c":"code","5084d40f":"code","1c99c30d":"code","e5d52c37":"code","2c6a9b6e":"code","57d7cf27":"code","c7f4a13e":"code","df5cf800":"code","a18cdc6c":"code","b9753204":"code","03b10f63":"code","fdebf87a":"code","b591dc59":"code","d651b69b":"code","bb120db5":"code","7f1d4e80":"code","bfc78ef2":"code","63b5fef9":"code","f3237773":"code","e8b4be7d":"code","cf9986c3":"code","64437df5":"code","18de51a6":"code","c13451e6":"code","819c296b":"code","12b97a76":"code","2525c092":"code","d6552218":"code","22d2d515":"code","ea77ea2a":"code","a97962c2":"code","69bb7992":"code","5d1abb37":"code","6972bb9d":"code","9d8859f0":"code","3af1a789":"code","5a66d5c6":"code","119614ca":"code","519642fe":"code","0bc09047":"code","b603a073":"markdown","4f3c1e4a":"markdown","82343ef8":"markdown","f00a5564":"markdown","3e2b36ca":"markdown","4b552b9e":"markdown","c10ebbba":"markdown","dcba7cfc":"markdown","597f250b":"markdown","25ad289b":"markdown","98cb48e4":"markdown","b156476e":"markdown","d1a7fc44":"markdown","295bd0d1":"markdown","87a0fde5":"markdown","46731a0b":"markdown","87b405b4":"markdown","be5d46d6":"markdown","d282882d":"markdown","84e05efc":"markdown","3f0076d1":"markdown","a65b977d":"markdown","7af34478":"markdown"},"source":{"bc51d69e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","987fedeb":"calendar = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\")\nsell_prices = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\nsales_train = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\")\nsample_sub = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")\n","b0f24269":"def memory_reduction(dataset):\n    column_types = dataset.dtypes\n    temp = None\n    for x in range(len(column_types)):\n        column_types[x] = str(column_types[x])\n    for x in range(len(column_types)):\n        temp = dataset.columns[x]\n        if dataset.columns[x] == \"date\":\n            dataset[temp] = dataset[temp].astype(\"datetime64\")\n        if column_types[x] == \"int64\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"int16\")\n        if column_types[x] == \"object\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"category\")\n        if column_types[x] == \"float64\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"float16\")\n    return dataset","f0cdc112":"calendar_df = memory_reduction(calendar)","45339554":"sell_prices[\"id\"] = sell_prices[\"item_id\"] + \"_\" + sell_prices[\"store_id\"] + \"_validation\" \nsell_prices = pd.merge(sell_prices, sales_train[[\"cat_id\", \"id\", \"state_id\"]], on = \"id\")\nsell_prices_df = memory_reduction(sell_prices)","83d22c20":"sales_train_df = memory_reduction(sales_train)","7bb55331":"calendar_df = calendar_df[:1913]\ncalendar_df[\"day\"] = pd.DatetimeIndex(calendar_df[\"date\"]).day\ncalendar_df[\"day\"] = calendar_df[\"day\"].astype(\"int8\")\ncalendar_df[\"week_num\"] = (calendar_df[\"day\"] - 1) \/\/ 7 + 1\ncalendar_df[\"week_num\"] = calendar_df[\"week_num\"].astype(\"int8\")","65874dee":"import gc","22cdb705":"def make_dataframe():\n    # Wide format dataset \n    df_wide_train = sales_train_df.drop(columns=[\"item_id\", \"dept_id\", \"cat_id\", \"state_id\",\"store_id\", \"id\"]).T\n    df_wide_train.index = calendar_df[\"date\"]\n    df_wide_train.columns = sales_train_df[\"id\"]\n    \n   \n    # Convert wide format to long format\n    df_long = df_wide_train.stack().reset_index(1)\n    df_long.columns = [\"id\", \"value\"]\n\n    del df_wide_train\n    gc.collect()\n    \n    df = pd.merge(pd.merge(df_long.reset_index(), calendar_df, on=\"date\"), sell_prices_df, on=[\"id\", \"wm_yr_wk\"])\n    df = df.drop(columns=[\"d\"])\n    #df[[\"cat_id\", \"store_id\", \"item_id\", \"id\", \"dept_id\"]] = df[[\"cat_id\"\", store_id\", \"item_id\", \"id\", \"dept_id\"]].astype(\"category\")\n    df[\"sell_price\"] = df[\"sell_price\"].astype(\"float16\")   \n    df[\"value\"] = df[\"value\"].astype(\"int32\")\n    df[\"state_id\"] = df[\"store_id\"].str[:2].astype(\"category\")\n\n\n    del df_long\n    gc.collect()\n\n    return df\n\ndf = make_dataframe()","b5b719ef":"del calendar, sales_train, sell_prices\ngc.collect()","8061e455":"df.head()","375b493b":"#importing all necessary libraries\nimport numpy as np\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n%matplotlib inline","519c4147":"df.columns","7e1e9a6c":"df.shape","5084d40f":"plt.figure(figsize = (12,12))\nsn.heatmap(df.corr(), annot=True)","1c99c30d":"df.dtypes","e5d52c37":"df.isnull().sum()","2c6a9b6e":"sales_train_df.describe()","57d7cf27":"calendar_df.describe()","c7f4a13e":"sell_prices_df.describe()","df5cf800":"#value\ntemp = df.groupby([\"cat_id\", \"date\"])[\"value\"].sum()","a18cdc6c":"x = temp[temp.index.get_level_values(\"cat_id\") == 'FOODS'].values","b9753204":"plt.hist(x)","03b10f63":"x = temp[temp.index.get_level_values(\"cat_id\") == 'HOUSEHOLD'].values\nplt.hist(x)","fdebf87a":"gc.collect()","b591dc59":"x = temp[temp.index.get_level_values(\"cat_id\") == 'HOBBIES'].values\nplt.hist(x)","d651b69b":"del x, temp\ngc.collect()","bb120db5":"# ALTERNATE PROCEDURE Of DOING THIS WILL BE SHOWN HERE after some time","7f1d4e80":"calendar_df.head()","bfc78ef2":"calendar_df['event_name_1'].value_counts().plot.bar()","63b5fef9":"calendar_df['event_type_1'].value_counts().plot.bar()","f3237773":"calendar_df['event_name_2'].value_counts().plot.bar()","e8b4be7d":"calendar_df['event_type_2'].value_counts().plot.bar()","cf9986c3":"calendar_df[\"snap_CA\"].value_counts()","64437df5":"calendar_df[\"snap_TX\"].value_counts()","18de51a6":"calendar_df[\"snap_WI\"].value_counts()","c13451e6":"df[\"snap_CA\"].value_counts()","819c296b":"df[\"snap_TX\"].value_counts()","12b97a76":"df[\"snap_WI\"].value_counts()","2525c092":"df[\"cat_id\"].value_counts()","d6552218":"sell_prices_df[\"cat_id\"].value_counts()","22d2d515":"sell_prices_df[\"cat_id\"].value_counts().plot.bar()","ea77ea2a":"sell_prices_df[\"state_id\"].value_counts()","a97962c2":"sell_prices_df[\"state_id\"].value_counts().plot.bar()","69bb7992":"sn.countplot(sell_prices_df.store_id)","5d1abb37":"sell_prices_df.store_id.value_counts()","6972bb9d":"df.head()","9d8859f0":"temp  = df.groupby([\"cat_id\", \"date\"])[\"value\"].sum()","3af1a789":"plt.figure(figsize = (8,6))\nplt.plot(temp[temp.index.get_level_values('cat_id') == \"FOODS\"].index.get_level_values(\"date\"), temp[temp.index.get_level_values('cat_id') == \"FOODS\"].values, label =\"FOODS\")\nplt.plot(temp[temp.index.get_level_values('cat_id') == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp[temp.index.get_level_values('cat_id') == \"HOUSEHOLD\"].values, label =\"HOUSEHOLD\")\nplt.plot(temp[temp.index.get_level_values('cat_id') == \"HOBBIES\"].index.get_level_values(\"date\"), temp[temp.index.get_level_values('cat_id') == \"HOBBIES\"].values, label =\"HOBBIES\")\nplt.legend()\nplt.show()","5a66d5c6":"df.head()","119614ca":"#So lets analyze events first like how do they affect the data \n#first we will take event_type_1 in considerations\nevent_call = df.groupby([\"event_name_1\", \"date\"])[\"value\"].sum()","519642fe":"plt.figure(figsize = (8,6))\nplt.plot(temp[temp.index.get_level_values('cat_id') == \"FOODS\"].index.get_level_values(\"date\"), temp[temp.index.get_level_values('cat_id') == \"FOODS\"].values, label =\"FOODS\")\nplt.plot(temp[temp.index.get_level_values('cat_id') == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp[temp.index.get_level_values('cat_id') == \"HOUSEHOLD\"].values, label =\"HOUSEHOLD\")\nplt.plot(temp[temp.index.get_level_values('cat_id') == \"HOBBIES\"].index.get_level_values(\"date\"), temp[temp.index.get_level_values('cat_id') == \"HOBBIES\"].values, label =\"HOBBIES\")\nplt.legend()\nplt.show()","0bc09047":"pd.get_dummies(calendar_df, columns=[\"event_name_1\"]).head()","b603a073":"#### We have \n* 3181789 products in foods category\n* 2375427 products in HouseHold category\n* 1283905 products in Hobbies category\n\nto conclude this we can say highest no. of products have been registered in Foods category while lowest in Hobbies category","4f3c1e4a":"## BASIC ANALYSIS","82343ef8":"## UNIVARIATE ANALYSIS","f00a5564":"In this we got to know \n* TX_2 has highest no. of products followed by TX_1\n* At third comes CA_1\n* last position is attained by CA_2","3e2b36ca":"#### Therefore having same ratio in snap indicate that our data has been correctly been fitted","4b552b9e":"#### EVENT CLEANING UP SEQUENCE","c10ebbba":"# BIVARIATE ANALYSIS","dcba7cfc":"The correlation before feature engineering can be seen here -\n* snap has good correlation with day and week num\n* value has a little correlation with sell price at the moment","597f250b":"### TIME SERIES ANALYSIS W.R.T cat_id","25ad289b":"1. We have to find relations of these with respect to the column value.\n2. Inter category relations","98cb48e4":"#### Next is EVENTS UNIVARIATE ANALYSIS","b156476e":"Lets check this in df ","d1a7fc44":"#### first as whole ","295bd0d1":"### CAT_ID","87a0fde5":"### Lets move on to SNAP analyis ","46731a0b":"# EDA STARTS","87b405b4":"* Canada has highest no. of products registered\n* Texas and West indies have apporximately the same no. of products registered if round of by 10,000","be5d46d6":"#### So as we can see in event type one in event type one -\n* The highest frequncy is of 7 events - lentweek2 , superbowl, StPatricksDay, Purim End, President start, LentStart, Valentines day\n* The highest frequency is of Relegious events in whole\n\n#### And in event type 2 - \n\n* All events have same frequency\n* although as whole cultral events can be seen more","d282882d":"### STATE_ID","84e05efc":"#### There are equal no. of times when Snap has happened in each of the states","3f0076d1":"we can leave all the null values of the events as it isnt possible for each day of the year to have a event","a65b977d":"### so  as we can note few points in this section\n* Food has the highest frequency among the others \n* Food and Hobbies are highly left skewed \n* Household is little left skewed","7af34478":"### STORE_ID"}}