{"cell_type":{"8456f119":"code","71729393":"code","c0a3b82f":"code","5c25ca6e":"code","b3fbd96a":"code","b214532d":"code","6c401c83":"code","28cdb94f":"code","1dcf110d":"code","19130f4f":"code","84c1b156":"code","a4ef82ea":"code","ff8da128":"code","c4dba5f0":"code","3b5285fe":"code","c3a74305":"code","ea3956af":"code","0444d617":"code","d4fba4ee":"code","984724e8":"code","86e93631":"code","a442251e":"code","15ea5962":"code","2e17a165":"code","a699a690":"code","190fcaf1":"code","a3247f8f":"code","29b14537":"code","799c398b":"code","8169c346":"code","5be40c5c":"code","ee856621":"code","57149e69":"code","ecba86aa":"code","9bcde586":"code","84b063ee":"code","eee6055b":"code","f08a3066":"code","39fbf034":"code","ed8724f6":"code","9fd6299e":"code","e63f6fb2":"code","60d1202f":"code","0ea2df0d":"code","22cc267f":"code","835c3db4":"code","8abec79c":"code","b871edaa":"code","8fe8744d":"markdown","af632559":"markdown","51939359":"markdown","9d294908":"markdown","c5bb9492":"markdown","bfc748b6":"markdown","2985faae":"markdown","59286d24":"markdown","5bcc08e6":"markdown","21104473":"markdown","564a23b6":"markdown","72a1ef47":"markdown","82dd8a95":"markdown","e7b74121":"markdown","b0edc40a":"markdown","fb700e48":"markdown","50131f3d":"markdown","256467e8":"markdown","773e10eb":"markdown","39044385":"markdown","3858f4c8":"markdown","8a9e143a":"markdown","80bd35cc":"markdown","ac3f0957":"markdown","c640cf23":"markdown","136b0d8a":"markdown","f797fc87":"markdown","c8337330":"markdown","cde848da":"markdown","b7569bf6":"markdown","17845686":"markdown","58434c22":"markdown","feafe7fb":"markdown","c9bd5270":"markdown","d8601f9c":"markdown","7724e862":"markdown","edcaa23b":"markdown","d83133fa":"markdown","3659e478":"markdown","1d0b9f66":"markdown","3ec87be3":"markdown","91f39cde":"markdown","99e3970e":"markdown"},"source":{"8456f119":"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC","71729393":"iris = datasets.load_iris()","c0a3b82f":"iris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  #dimensoes da petala\ny = iris[\"target\"]\n\nsetosa_or_versicolor = (y == 0) | (y == 1) #selecionamos apenas os dois tipos que temos interesse\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]","5c25ca6e":"svm_clf = SVC(C=100, kernel='linear')\nsvm_clf.fit(X, y)","b3fbd96a":"svm_clf.predict([[2.5, 1.1]])","b214532d":"import matplotlib.pyplot as plt\n%matplotlib inline","6c401c83":"def plot_svc_decision_boundary(svm_clf, xmin, xmax):    \n    w = svm_clf.coef_[0]\n    b = svm_clf.intercept_[0]\n\n    # a reta de nossa superf\u00edcie de decis\u00e3o tem como equacao w0*x0 + w1*x1 + b = 0, logo\n    # x1 = -w0\/w1 * x0 - b\/w1\n    x0 = np.linspace(xmin, xmax, 200)\n    decision_boundary = -w[0]\/w[1] * x0 - b\/w[1] #todos os valores de x1 em funcao dos valores de x0 na sup de decisao\n    \n    margin = 1\/w[1] #estamos desenhando a superficie em funcao de x1, logo a margem tamb\u00e9m sera em funcao dele, por isso divido por 1\/w[1], que \u00e9 o deslocamento da superficie de decisao para as maregsn\n    margin_up = decision_boundary + margin\n    margin_down = decision_boundary - margin\n\n    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n    plt.plot(x0, margin_up, \"k--\", linewidth=2)\n    plt.plot(x0, margin_down, \"k--\", linewidth=2)","28cdb94f":"plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.axis([0, 5.5, 0, 2])\n","1dcf110d":"svm_clf = SVC(C=0.1, kernel='linear')\nsvm_clf.fit(X, y)","19130f4f":"plt.figure(figsize=(12,2.7))\n\nplt.subplot(121)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.axis([0, 5.5, 0, 2])","84c1b156":"X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\ny_outliers = np.array([0, 0])\nX1_with_outlier = np.concatenate([X, X_outliers[:1]], axis=0)\ny1_with_outlier = np.concatenate([y, y_outliers[:1]], axis=0)\nX2_with_outlier = np.concatenate([X, X_outliers[1:]], axis=0)\ny2_with_outlier = np.concatenate([y, y_outliers[1:]], axis=0)","a4ef82ea":"svm_clf = SVC(kernel=\"linear\", C=10**9)\nsvm_clf.fit(X2_with_outlier, y2_with_outlier)","ff8da128":"plt.plot(X2_with_outlier[:, 0][y2_with_outlier==1], X2_with_outlier[:, 1][y2_with_outlier==1], \"bs\")\nplt.plot(X2_with_outlier[:, 0][y2_with_outlier==0], X2_with_outlier[:, 1][y2_with_outlier==0], \"yo\")\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[1][0], X_outliers[1][1]),\n             xytext=(3.2, 0.08),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])","c4dba5f0":"from sklearn.preprocessing import StandardScaler","3b5285fe":"X_new = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\ny_new = np.array([0, 0, 1, 1])\n\nsvm_clf2 = SVC(kernel=\"linear\", C=100)\nsvm_clf2.fit(X_new, y_new)\n\nplt.figure(figsize=(12,3.2))\nplt.subplot(121)\nplt.plot(X_new[:, 0][y_new==1], X_new[:, 1][y_new==1], \"bo\")\nplt.plot(X_new[:, 0][y_new==0], X_new[:, 1][y_new==0], \"rs\")\nplot_svc_decision_boundary(svm_clf2, 0, 6)\nplt.xlabel(\"$x_0$\", fontsize=20)\nplt.ylabel(\"$x_1$  \", fontsize=20, rotation=0)\nplt.title(\"Sem normaliza\u00e7\u00e3o\", fontsize=16)\nplt.axis([0, 6, 0, 90])\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_new)\nsvm_clf2.fit(X_scaled, y_new)\n\nplt.subplot(122)\nplt.plot(X_scaled[:, 0][y_new==1], X_scaled[:, 1][y_new==1], \"bo\")\nplt.plot(X_scaled[:, 0][y_new==0], X_scaled[:, 1][y_new==0], \"rs\")\nplot_svc_decision_boundary(svm_clf2, -2, 2)\nplt.xlabel(\"$x_0$\", fontsize=20)\nplt.title(\"Com normaliza\u00e7\u00e3o\", fontsize=16)\nplt.axis([-2, 2, -2, 2])","c3a74305":"x1 = [1.5, 0.7]\nx2 = [4.1, 1.1]\n\nsvm_clf = SVC(kernel=\"linear\", C=100)\nsvm_clf.fit(X, y)","ea3956af":"plt.figure(figsize=(12,2.7))\n\nplt.subplot(121)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\nplt.scatter(x1[0], x1[1], s=50, alpha=0.5, c=\"g\")\nplt.scatter(x2[0], x2[1], s=50, alpha=0.5, c=\"m\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.axis([0, 5.5, 0, 2])","0444d617":"score = np.dot(svm_clf.coef_, x1) + svm_clf.intercept_\nclasse = 1 if score > 0 else 0\nprint(\"O ponto \" + str(x1) + \" pertence \u00e0 classe \" + str(classe) + \" com score \" + str(score[0]))","d4fba4ee":"score = np.dot(svm_clf.coef_, x2) + svm_clf.intercept_\nclasse = 1 if score > 0 else 0\nprint(\"O ponto \" + str(x2) + \" pertence \u00e0 classe \" + str(classe) + \" com score \" + str(score[0]))","984724e8":"from mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3D_decision_function(ax, w, b, x1_lim=[0, 5.0], x2_lim=[0, 2.0], plot_margin=True):\n    x1_in_bounds = (X[:, 0] > x1_lim[0]) & (X[:, 0] < x1_lim[1])\n    X_crop = X[x1_in_bounds]\n    y_crop = y[x1_in_bounds]\n    x1s = np.linspace(x1_lim[0], x1_lim[1], 40)\n    x2s = np.linspace(x2_lim[0], x2_lim[1], 40)\n    x1, x2 = np.meshgrid(x1s, x2s)\n    xs = np.c_[x1.ravel(), x2.ravel()]\n    df = (xs.dot(w) + b).reshape(x1.shape)\n    m = 1 \/ np.linalg.norm(w)\n    if plot_margin:\n        boundary_x2s = -x1s*(w[0]\/w[1])-b\/w[1]\n        margin_x2s_1 = -x1s*(w[0]\/w[1])-(b-1)\/w[1]\n        margin_x2s_2 = -x1s*(w[0]\/w[1])-(b+1)\/w[1]\n        boundary_in_bounds = (boundary_x2s > x1_lim[0]) & (boundary_x2s <= x2_lim[1])\n        m1_in_bounds = (margin_x2s_1 > x1_lim[0]) & (margin_x2s_1 < x2_lim[1])\n        m2_in_bounds = (margin_x2s_2 > x1_lim[0]) & (margin_x2s_2 < x2_lim[1])\n        ax.plot(x1s[boundary_in_bounds], boundary_x2s[boundary_in_bounds], 0, \"k-\", linewidth=2, label=r\"Decis\u00e3o\")\n        ax.plot(x1s[m1_in_bounds], margin_x2s_1[m1_in_bounds], 0, \"k--\", linewidth=2, label=r\"Margem\")\n        ax.plot(x1s[m2_in_bounds], margin_x2s_2[m2_in_bounds], 0, \"k--\", linewidth=2)\n    scores_1 = svm_clf.decision_function(X_crop[y_crop==1])\n    scores_2 = svm_clf.decision_function(X_crop[y_crop==0])\n    ax.plot_surface(x1, x2, df, alpha=0.1, color=\"b\")\n    ax.plot_wireframe(x1, x2, df, alpha=0.05, color=\"k\")\n    ax.plot_surface(x1s, x2, np.zeros_like(x1),\n                color=\"g\", alpha=0.2)\n    ax.plot(X_crop[:, 0][y_crop==1], X_crop[:, 1][y_crop==1], scores_1, \"bs\")\n    ax.plot(X_crop[:, 0][y_crop==0], X_crop[:, 1][y_crop==0], scores_2, \"yo\")\n    ax.axis(x1_lim + x2_lim)\n    ax.set_xlabel(r\"Petal length\", fontsize=15)\n    ax.set_ylabel(r\"Petal width\", fontsize=15)\n    ax.set_zlabel(r\"$h = \\mathbf{w}^T \\mathbf{x} + b$\", fontsize=18)\n    ax.legend(loc=\"upper left\", fontsize=16)\n    \n\nfig = plt.figure(figsize=(20, 6))\nax1 = fig.add_subplot(121, projection='3d')\nax1.elev = 40\nplot_3D_decision_function(ax1, w=svm_clf.coef_[0], b=svm_clf.intercept_[0], plot_margin=True)\n\nax2 = fig.add_subplot(122, projection='3d')\nax2.elev = 10\nax2.azim = 100\nplot_3D_decision_function(ax2, w=svm_clf.coef_[0], b=svm_clf.intercept_[0], plot_margin=True)","86e93631":"from sklearn.linear_model import LogisticRegression","a442251e":"svm_clf_probs = SVC(kernel=\"linear\", probability=True)\nsvm_clf_probs.fit(X, y)","15ea5962":"svm_scores = svm_clf_probs.decision_function(X)\nsvm_scores_expanded = np.expand_dims(svm_scores, axis=1)","2e17a165":"lr = LogisticRegression()\nlr.fit(svm_scores_expanded, y)","a699a690":"lr.predict_proba(svm_scores_expanded)[0:10]","190fcaf1":"svm_clf_probs.predict_proba(X)[0:10]","a3247f8f":"import matplotlib.pyplot as plt\n%matplotlib inline","29b14537":"c1 = np.array([-4, -3, 3, 4])\nc2 = np.array([-2, -1, 0, 1, 2])","799c398b":"plt.scatter(x=c1, y=np.zeros_like(c1), c='blue')\nplt.scatter(x=c2, y=np.zeros_like(c2), c='red')\nplt.yticks([])","8169c346":"c1_2 = c1**2\nc2_2 = c2**2","5be40c5c":"plt.scatter(x=c1, y=c1_2, c='blue')\nplt.scatter(x=c2, y=c2_2, c='red')\nplt.axhline(y=6, ls='--', c='gray')","ee856621":"from sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures","57149e69":"polynomial_svm_clf = Pipeline([\n    (\"poly_fetaures\", PolynomialFeatures(degree=3)),\n    (\"scaler\", StandardScaler()),\n    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n])","ecba86aa":"polynomial_svm_clf.fit(X, y)","9bcde586":"from sklearn.svm import SVC","84b063ee":"poly_kernel_svm_clf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))\n])","eee6055b":"poly_kernel_svm_clf.fit(X, y)","f08a3066":"x1 = -1","39fbf034":"plt.scatter(x=c1, y=np.zeros_like(c1), c='blue')\nplt.scatter(x=[-1], y=[0], s=150, alpha=0.5, c=\"m\")\nplt.scatter(x=c2, y=np.zeros_like(c2), c='red')\nplt.yticks([])","ed8724f6":"def gaussian_rbf(x, ell, gamma=0.3):\n    return np.exp(-gamma * np.linalg.norm(x-ell)**2)","9fd6299e":"c1_g1 = np.array([gaussian_rbf(x,-2) for x in c1])\nc1_g2 = np.array([gaussian_rbf(x, 1) for x in c1])\nc2_g1 = np.array([gaussian_rbf(x,-2) for x in c2])\nc2_g2 = np.array([gaussian_rbf(x, 1) for x in c2])","e63f6fb2":"x1_g = gaussian_rbf(x1, -2)\nx2_g = gaussian_rbf(x1, 1)","60d1202f":"plt.scatter(x=c1_g1, y=c1_g2, c='blue')\nplt.scatter(x=[x1_g], y=[x2_g], s=150, alpha=0.5, c=\"m\")\nplt.scatter(x=c2_g1, y=c2_g2, c='red')\nx1, y1 = [-0.1, 1.1], [0.57, -0.1]\nplt.plot(x1, y1, c='gray', ls='--')\nplt.axis([-0.1, 1.1, -0.1, 1.1])","0ea2df0d":"rbf_kernel_svm_clf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm_clf', SVC(kernel='rbf', C=5))\n])\nrbf_kernel_svm_clf.fit(X, y)","22cc267f":"from sklearn.svm import LinearSVR","835c3db4":"svm_reg = LinearSVR(epsilon=1.5)\nsvm_reg.fit(X, y)","8abec79c":"from sklearn.svm import SVR","b871edaa":"svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\nsvm_reg.fit(X,y)","8fe8744d":"Observe que agora as amostras s\u00e3o linearmente separ\u00e1veis. Neste caso nos perguntamos como escolher os pontos de refer\u00eancia. Geralmente esses pontos s\u00e3o calculados como todos os pontos do dataset, ou seja, cada exemplo se torna um ponto de refer\u00eancia e calculamos a RBF para todos. ","af632559":"Observe a imagem abaixo, extra\u00edda do livro Introduction to Statistical Learning. Neste caso desejamos separar os pontos da classe azul dos pontos da classe roxa. As duas retas pontilhadas s\u00e3o as margens de cada classe (que o SVM busca aprender), enquanto a reta cont\u00ednua \u00e9 nossa super\u00edcie de decis\u00e3o.","51939359":"O SVM funciona muito bem em muitos casos, por\u00e9m quase todos os datasets n\u00e3o s\u00e3o linearmente separ\u00e1veis. Uma abordagem para lidar com este problema \u00e9 a utiliza\u00e7\u00e3o de features polinomiais, assim como fazemos com regress\u00e3o linear (polinomial). Veja o exemplo abaixo. Como separar estes dados utilizando o SVM?","9d294908":"$\n\\begin{equation}\n  y^{(i)}_{predito} ==\\left\\{\n  \\begin{array}{@{}ll@{}}\n    0, & \\text{se}\\ w_1x_1 + w_2x_2 + \\dots + w_mx_m + b <  0 \\\\\n    1, & \\text{se}\\ w_1x_1 + w_2x_2 + \\dots + w_mx_m + b \\geq 0 \n  \\end{array}\\right.\n\\end{equation}\n$","c5bb9492":"<h3>Obtendo probabilidades<\/h3>","bfc748b6":"Note que neste caso estamos sendo mais permissivos, o que nos gera margens maiores.","2985faae":"Adicionar features polinomiais funciona muito bem na maioris dos algoritmos de Machine Learning (n\u00e3o apenas n\u00e3o apenas no SVM), por\u00e9m um polin\u00f4mio de baixo grau n\u00e3o \u00e9 capaz de lidar com problemas complexos e polin\u00f4mios de graus muito altos geram um n\u00famero muito grande de features, prejudicando a performance do modelo. O svm nos permite aplicar quase todas as t\u00e9cnincas matem\u00e1ticas sem necessariamente adicionarmos dimens\u00f5es a nosso espa\u00e7o, apenas modificando as m\u00e9tricas de dist\u00e2ncia\/similaridade por meio dos <b>kernel tricks<\/b>, evitando a explos\u00e3o combinat\u00f3ria do n\u00famero de features.","59286d24":"\u00c9 importante destacar que quando utilizamos as fun\u00e7\u00f5es de kernel n\u00f3s n\u00e3o adicionamos features ao nosso conjunto de dados, mas sim calculamos uma dist\u00e2ncia \"similuando\" que estes dados tenham sido mapeados a num novo espa\u00e7o. O gr\u00e1fico acima apenas ilustra a distancia entre os pontos utilizando o kernel rbf em rela\u00e7\u00e3o aos pontos de refer\u00eancia $\\ell_1$ e $\\ell_2$.","5bcc08e6":"onde $W$ e $b$ s\u00e3o os coeficientes da superf\u00edcie de decis\u00e3o, encontrados pelo SVM.","21104473":"$\n\\begin{equation}\n  \\hat{y} ==\\left\\{\n  \\begin{array}{@{}ll@{}}\n    0, & \\text{se}\\ W^T\\cdot  x + b <  0 \\\\\n    1, & \\text{se}\\ W^T\\cdot x + b \\geq  0  \n  \\end{array}\\right.\n\\end{equation}\n$","564a23b6":"Ou, de forma vetorial","72a1ef47":"Uma outra t\u00e9cnica importante de ser empregada com o SVM \u00e9 escalar (normalizar subtraindo a m\u00e9dia e dividindo pelo desvio padr\u00e3o) os dados. Este m\u00e9todo \u00e9 muito sens\u00edvel \u00e0 escala, uma vez que busca maximizar a margem de separa\u00e7\u00e3o utilizando uma m\u00e9trica de dist\u00e2ncia do ponto \u00e0 margem, ele provavelmente ir\u00e1 negligenciar features com pequenas dimens\u00f5es, enquanto features com valores altos dominar\u00e3o o c\u00e1lculo da dist\u00e2ncia. Veja o exemplo abaixo:","82dd8a95":"Vamos ver um exemplo para os pontos $x_1 = (1.5, 0.7)$ e $x_2 = (4.1, 1.1)$, em destaque no gr\u00e1fico seguinte.","e7b74121":"Note que o nosso modelo conseguiu aprender um hiperplano de separa\u00e7\u00e3o entre as duas classes, que fica exatamente no centro das duas margens m\u00e1ximas encontradas por ele. Desta forma a, superf\u00edcie de decis\u00e3o encontrada \u00e9 aquela que fica o mais distante poss\u00edvel das duas classes.","b0edc40a":"<h3>SVM<\/h3>","fb700e48":"Observe o que aconteceria se houvesse um outlier no nosso modelo","50131f3d":"Vamos implementar manualmente o c\u00e1lculo de classifica\u00e7\u00e3o do SVM, por\u00e9m com o Scikit podemos obter a classe rapidamente utilizando o m\u00e9todo predict()","256467e8":"<img src='https:\/\/github.com\/Afmansano\/Codes\/blob\/master\/Scikit%20Book\/images\/svm_margin.png?raw=1' width=\"300\">","773e10eb":"Note como neste caso nossa margem se tornou extramemente pequena devido apenas a um exemplo. Isto pode causar erros de classifica\u00e7\u00e3o durante o processo de produ\u00e7\u00e3o, pois o modelo n\u00e3o conseguir\u00e1 classificar corretamente um ponto da classe azul que esteja mais mais deslocado sentido classe amararela.","39044385":"Assim como nos exemplos anteriores, vamos nos prender em dataset bi-dimensionais, pois isso nos permitir\u00e1 uma ampla visualiza\u00e7\u00e3o do que acontece por baixo dos panos nesses classificadores. Poderemos visualizar os pontos no espa\u00e7o de busca\/classifica\u00e7\u00e3o e as superf\u00edcies de decis\u00e3o. No final do tutorial aplicaremos esses m\u00e9todos a problemas com mais dimens\u00f5es. Voc\u00eas ver\u00e3o que o c\u00f3digo \u00e9 exatamente o mesmo para estes casos.","3858f4c8":"que \u00e9 uma fun\u00e7\u00e3o gaussiana (sim\u00e9trica em forma de sino) que assume valores entra 0 e 1","8a9e143a":"Neste tutorial, vamos trabalhar com o classificador SVM, ou M\u00e1quina de Vetores de Suporte. Um modelo muito usado em Machine Learning e capaz de lidar com problemas linear e n\u00e3o linearmente separ\u00e1veis.","80bd35cc":"A nossa fun\u00e7\u00e3o de decis\u00e3o $h = W^T\\cdot x + b$ pode ser visualizada no gr\u00e1fico abaixo (plano azul hachurado). Quando calculamos o valor de $h$ estamos gerando um score que obedece a regra de classifica\u00e7\u00e3o do SVM (foi a partir desta regra que ele encontrou os par\u00e2metros $W$) de acordo com as posi\u00e7\u00e3o do exemplo em quest\u00e3o no espa\u00e7o multidimensional. Observe que nossa superf\u00edcie de decis\u00e3o \u00e9 o plano onde $h = \n0$ e a nosso limite de decis\u00e3o (decision boundery) \u00e9 o hiperplano (reta no nosso caso) onde as duas regi\u00f5es em destaque se cruzam.","ac3f0957":"$\\phi\\gamma(x, \\ell) = exp(-\\gamma\\|x-\\ell\\|^2)$","c640cf23":"Uma outra forma de lidarmos com problemas n\u00e3o lineares \u00e9 criarmos features de semelhan\u00e7a computadas por meio de uma fun\u00e7\u00e3o de similaridade. A Gaussian Radial Bases Function (RBF) \u00e9 um exemplo de fun\u00e7\u00e3o de similaridade :\n","136b0d8a":"<h5>Como o SVM classifica os dados?<\/h5>","f797fc87":"Observe que a escala da feature $x_1$ na imagem da esquerda \u00e9 muito maior que a da feature $x_0$, e que a margem do modelo sem normaliza\u00e7\u00e3o \u00e9 significativamente menor (em propor\u00e7\u00e3o) e horizontal, uma vez que $x_1$ est\u00e1 dominando o c\u00e1lculo da dist\u00e2ncia.Note tamb\u00e9m que no segundo caso mais pontos participam da margem (s\u00e3o vetores de suporte), j\u00e1 que a feature $x_0$ passa a contribuir igualmente para o c\u00e1lculo de dist\u00e2nca.","c8337330":"Note que com a utiliza\u00e7\u00e3o de kernels n\u00e3o adicionamos dimens\u00f5es a nosso espa\u00e7o, mas sim computamos novas features de acordo com uma fun\u00e7\u00e3o de kernel.","cde848da":"Voltando ao nosso primeiro conjunto de dados:","b7569bf6":"Vamos analisar este exemplo em rela\u00e7\u00e3o a dois pontos de refer\u00eancia, $\\ell_1 = -2$ e $\\ell_2 = 1$. O exemplo $x_1 = -1$ (em destaque) est\u00e1 originalmente localizado a uma dist\u00e2ncia 1 da primeira refer\u00eancia e 2 da segunda. Por\u00e9m suas novas features s\u00e3o $x_{1,1} =exp(-0.3\\times 1^2) \\approx 0.74$ e $x_{1,2} =exp(-0.3\\times 2^2) \\approx 0.30$ Como pode ser visto no gr\u00e1fico abaixo, com isso elas s\u00e3o linearmente separ\u00e1veis.","17845686":"Matematicamente, ap\u00f3s treinado o modelo, a tarefa de classifica\u00e7\u00e3o do SVM \u00e9 bem simples e segue de acordo com a equa\u00e7\u00e3o abaixo: ","58434c22":"Vamos visualizar esses dados para entender melhor o comportamento do nosso classificador.","feafe7fb":"Imagine que estejam construindo uma rua em seu bairro e o prefeito quer que ela seja o mais larga poss\u00edvel, de forma a separar as casas do lado direito e esquerdo da rua. O que o SVM busca fazer \u00e9 muito similar a isso, encontrar as margens mais disnte poss\u00edveis entre si que separem os exemplos de cada uma das classes. A reta no centro destas margens (ou seja, a faixa que separa as duas m\u00e3os da nossa rua), \u00e9 a nossa superf\u00edcie de decis\u00e3o.","c9bd5270":"No exemplo utilizando o dataset Iris, acima, n\u00e3o normalizamos o dados apenas para fins did\u00e1dicos de facilitar a visualiza\u00e7\u00e3o.","d8601f9c":"Tamb\u00e9m \u00e9 poss\u00edvel utilizarmos o SVM para problemas de regress\u00e3o. Neste caso, o SVM busca coeficientes que minimizem uma loss onde apenas os res\u00edduos mair que uma contante pr\u00e9 definida contribuiem par a fun\u00e7\u00e3so de loss.","7724e862":"Mas matematicamente, o que isto quer dizer?","edcaa23b":"<h3>SVM Regression<\/h3>","d83133fa":"Observe que o par\u00e2metro C=100 foi empregado no nosso modelo. Essa par\u00e2mtro indica o qu\u00e3o rigoroso eu desejo que o modelo seja em rela\u00e7\u00e3o a viola\u00e7\u00f5es \u00e0 margem, ou seja, a pontos que fiquem do lado oposto \u00e0 margem encontrada para sua classe. Isto \u00e9 importante pois em alguns casos pode ser que n\u00e3o seja poss\u00edvel encontrar uma margem que separe todos os pontos, ou, se essa margem existir, ela seja muito pequena, tornando a generaliza\u00e7\u00e3o do modelo para novos exemplos muito ruim. Um outro problema que pode ser causado com um hiperpar\u00e2metro muito rigoroso \u00e9 gerar uma grande sensibilidade a outliers.","3659e478":"Assim como na regress\u00e3o polinomial, podemos lidar com problemas n\u00e3o lineares pelo SVR, apenas se faz necess\u00e1rio o uso de kernels.","1d0b9f66":"Observe como ficaria nosso modelo com um par\u00e2metro C=1","3ec87be3":"Antes da explica\u00e7\u00e3o matem\u00e1tica do SVM, vamos sentir o gostinho de execut\u00e1-lo com o Scikit Learning utilizando um dataset cl\u00e1ssico, o Iris. Neste modelo vamos classificar se uma \u00edris (flor) \u00e9 do tipo Setosa ou Versicolor","91f39cde":"Utilizando o Scikit-Learn conseguimos implementar o SVM com essa caracter\u00edstica facilmente, \u00e9 s\u00f3 adicionarmos o PolynomialFeatures a nosso pipeline:","99e3970e":"Se aumentarmos uma dimens\u00e3o no nosso espa\u00e7o, adicionando uma feature $x_2 = x^2$, projetamos nossos dados em um espa\u00e7o onde eles s\u00e3o linearmente separ\u00e1veis."}}