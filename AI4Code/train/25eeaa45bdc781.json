{"cell_type":{"c6bfe7b1":"code","7a17eedb":"code","ee0f9252":"code","82a307b3":"code","bb8a4807":"code","cbcb31d9":"code","7ffc8665":"code","a7e8d9ed":"code","8f7c717d":"code","160748d1":"code","04fb57b6":"code","db279245":"code","77f0a494":"code","7bdf19a4":"code","b6031f0f":"code","9f37d302":"code","3dc96394":"code","ecb822b5":"code","9243038f":"code","84b8c2c3":"code","2b68f0bd":"code","1e4a7156":"code","9c37f373":"code","3d1bf50a":"code","c9f1db57":"code","6d63def4":"code","645933ab":"code","d848822d":"code","5ceef908":"code","f594d4d0":"code","dc6a064d":"code","9ede67ef":"code","b4ab20fb":"code","9135ebba":"code","6cead0cc":"code","d1d63982":"code","4bf1e116":"code","b2c23d57":"code","8e8b06ba":"code","5b43bfdb":"code","9fb678af":"code","bef82ea7":"code","6b8d1519":"markdown","7da7441f":"markdown","b7072b47":"markdown","d78a5258":"markdown","d8b2aae6":"markdown","f6395f50":"markdown","acd1ca1e":"markdown","3f74d163":"markdown","26179a4c":"markdown","8764e8bd":"markdown","65d2aeec":"markdown","3676d946":"markdown","3b6baae1":"markdown","61bbe2ec":"markdown","b00aa1ca":"markdown","a1600c78":"markdown","986b4881":"markdown","84a35dfe":"markdown","b128ca42":"markdown","7115e351":"markdown"},"source":{"c6bfe7b1":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a17eedb":"import pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport lightgbm as lgb\nimport shap\nfrom sklearn.model_selection import train_test_split","ee0f9252":"def create_missing_table(input_dataframe: pd.DataFrame):\n    total = len(input_dataframe)\n    naCount = input_dataframe.isnull().sum()\n    zeroCount = len(input_dataframe) - input_dataframe.fillna(1).astype(bool).sum()\n    zeroPercent = (zeroCount\/len(input_dataframe)*100).round().map(lambda n: '{0:.1f} %'.format(n))\n    naPercent = (input_dataframe.isnull().sum()\/len(input_dataframe)*100).round().map(lambda n: '{0:.1f} %'.format(n))\n    uniqCount = input_dataframe.nunique()\n    hitRate = (input_dataframe.notnull().sum()\/len(input_dataframe)*100).round().map(lambda n: '{0:.1f} %'.format(n))\n    return pd.DataFrame({'count_total': total, 'count_unique': uniqCount, 'count_zero':zeroCount,'percentile_zero':zeroPercent, 'count_missing': naCount,'percentile_missing':naPercent, 'hit_rate':hitRate})","82a307b3":"def describe_category(dataframe, column_name, ignore_zero=False, figsize=(11,7)):\n    \"\"\"\n    plot describe category with percentage\n    \"\"\"\n    if ignore_zero:\n        dataframe = dataframe[dataframe[column_name] != 0]\n    value_count = dataframe[column_name].value_counts().sort_index()\n    df_value_count = pd.DataFrame({column_name: value_count.index, \"count\": value_count.values})\n    sum_class = df_value_count[\"count\"].sum()\n    df_value_count[\"percentage\"] = df_value_count[\"count\"]\/sum_class*100\n    display(df_value_count)\n    \n    fig, ax = plt.subplots(figsize = figsize)\n    ax = sns.barplot(data=df_value_count, x=column_name, y=\"count\")\n    ax.set_ylim(0, df_value_count[\"count\"].max()*1.2)\n    for p, percentage in zip(ax.patches, list(df_value_count[\"percentage\"])):\n        ax.annotate(\"%.2f\" % percentage +\" %\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n             ha='center', va='center', rotation=0, xytext=(0, 20), textcoords='offset points')  #vertical bars\n    plt.show()","bb8a4807":"train_path = \"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\"\ntest_path = \"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\"","cbcb31d9":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","7ffc8665":"train_df.head()","a7e8d9ed":"test_df.head()","8f7c717d":"train_df[\"target\"].value_counts()","160748d1":"create_missing_table(train_df)","04fb57b6":"train_df[\"feature_1\"].value_counts()","db279245":"train_df[\"feature_10\"].value_counts()","77f0a494":"# describe_category(train_df, \"feature_10\", ignore_zero=True, figsize=(20,7))","7bdf19a4":"feature_list = list(train_df.columns)\nfeature_list.remove(\"id\")\nfeature_list.remove(\"target\")\nfeature_list","b6031f0f":"for feature_name in feature_list:\n    print(\"=============  \" + feature_name + \"  ===================\")\n    describe_category(train_df, feature_name, ignore_zero=True, figsize=(20,7))\n    print(\"=========================================================\")","9f37d302":"train_corr = train_df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(train_corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(25, 20))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(train_corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","3dc96394":"lgbm_params = {\n    'boosting': 'gbdt',\n    'learning_rate': 0.01, \n    'num_leaves': 300, \n    'objective': 'multiclass',\n    'num_class':4,\n    'metric': 'multi_logloss',\n}","ecb822b5":"def convert_text_to_class(str_class):\n    if str_class == \"Class_1\":\n        return 0\n    elif str_class == \"Class_2\":\n        return 1\n    elif str_class == \"Class_3\":\n        return 2\n    elif str_class == \"Class_4\":\n        return 3","9243038f":"X = train_df[feature_list]\ny = train_df[\"target\"].apply(convert_text_to_class)","84b8c2c3":"y.value_counts()","2b68f0bd":"# for feature_name in feature_list:\n#     X[feature_name] = X[feature_name].astype(np.float32)","1e4a7156":"data = lgb.Dataset(X, label=y, free_raw_data=False)","9c37f373":"boost_round = 200\ncv_result = lgb.cv(lgbm_params, data, num_boost_round=boost_round, early_stopping_rounds=20, nfold=5, verbose_eval=100)","3d1bf50a":"print(\"CV 5 Fold result\")\nprint(\"multi_logloss-mean :\" ,cv_result[\"multi_logloss-mean\"][-1])\nprint(\"multi_logloss-stdv :\" ,cv_result[\"multi_logloss-stdv\"][-1])\nprint(cv_result.keys())","c9f1db57":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=3041975)","6d63def4":"train_data = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\nval_data = lgb.Dataset(X_val, label=y_val, reference=train_data ,free_raw_data=False)","645933ab":"%%time\nboost_round = 500\nmodel = lgb.train(lgbm_params, train_data, valid_sets=[val_data], num_boost_round = boost_round, verbose_eval=100, early_stopping_rounds=50)","d848822d":"%%time\nexplainer = shap.TreeExplainer(model)","5ceef908":"X_very_small = X.sample(500)","f594d4d0":"%%time\nshap_values = explainer.shap_values(X_very_small)","dc6a064d":"shap.summary_plot(shap_values[1], X_very_small, plot_type='dot', max_display=50)","9ede67ef":"X_test = test_df[feature_list]","b4ab20fb":"pred = model.predict(X_test)","9135ebba":"y_test = pd.DataFrame(pred)","6cead0cc":"submission = y_test.copy()","d1d63982":"submission.columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"]","4bf1e116":"submission[\"id\"] = test_df[\"id\"]","b2c23d57":"# submission.columns = [\"id\", \"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"]","8e8b06ba":"submission.head()","5b43bfdb":"submission = submission[[\"id\", \"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"]]","9fb678af":"submission.head()","bef82ea7":"submission.to_csv(\"submission.csv\", index=False)","6b8d1519":"# 2D ","7da7441f":"# Baseline Model","b7072b47":"Let use LightGBM because it train fast and decent performance. ","d78a5258":"## 5 Fold cross validation ","d8b2aae6":"This is multi-classification problem.<br>\nEach data point have 1 class only. <br>\nClass are not balance.<br>","f6395f50":"## Correlation","acd1ca1e":"# Load data","3f74d163":"We do not see strong correlation between feature.","26179a4c":"## Draw distribution of feature","8764e8bd":"## Check zeros and missing","65d2aeec":"# Helpful function ","3676d946":"A lot of zero. Most feature have > 80% zero.\n","3b6baae1":"## Shape value ","61bbe2ec":"Let see value of some feature","b00aa1ca":"They have very small number of unique value. Seem like all category","a1600c78":"# Make submission","986b4881":"## Preprocess data","84a35dfe":"# Training data","b128ca42":"## 1D eda","7115e351":"### Negative value\n\nFeature 42, 39, 38, 35, 31, 30, 19"}}