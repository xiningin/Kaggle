{"cell_type":{"3baed908":"code","18ad2f3e":"code","4bd87515":"code","aaa55f01":"code","a89e5d5a":"code","aea8c328":"code","22352a87":"code","d7ed026f":"code","e9c3f80e":"code","46ef0618":"code","a9f17897":"code","3448f423":"code","484bc192":"code","4d4d3274":"code","01bd2912":"code","e55c5519":"code","a1e09d2b":"code","f0e2f042":"code","5c093e6d":"code","84cea7aa":"code","10acc2f3":"code","d3250478":"code","68df4329":"code","e4ef7038":"code","ca54fb9b":"code","3bc613bf":"markdown","6df887d5":"markdown","97842e65":"markdown","8cc58121":"markdown","3fad70a1":"markdown","4914d3b6":"markdown","cc3c8221":"markdown","d4900f56":"markdown","877da140":"markdown","44d4d545":"markdown"},"source":{"3baed908":"!pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz\nimport fastbook\nfastbook.setup_book()\n\nimport seaborn as sns\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG, clear_output\nfrom tqdm.auto import tqdm\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 10\n\n\ndef predict_batch(self, df):\n    dl = self.dls.test_dl(df_test)\n    dl.dataset.conts = dl.dataset.conts.astype(np.float32)\n    preds, targs = self.get_preds(dl=dl)\n    return preds, targs\n\nLearner.predict_batch = predict_batch","18ad2f3e":"path = Path(\"..\/input\/ventpressure2\")\nsave_path = Path(\"\/kaggle\/working\")\n\ndf_nn_final = pd.read_csv(\"..\/input\/ventpressure2\/train_preprocessed.csv\")\nto_drop = ['breathId_uIn_diffmean', 'uIn_diff3', 'breathId_uIn_diffmax']\ntry: df_nn_final = df_nn_final.drop(to_drop, axis=1)\nexcept Exception: pass","4bd87515":"dep_var = \"pressure\"\nsplits = load_pickle(\"..\/input\/ventpressure1\/split.pkl\")\ncont_nn, cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\ncont_nn","aaa55f01":"df_nn_final[cat_nn].nunique()","a89e5d5a":"procs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, \n                     y_names=dep_var)\ndls = to_nn.dataloaders(1024)","aea8c328":"y = to_nn.train.y\ny.min(), y.max()","22352a87":"learn = tabular_learner(dls, y_range=(-2, 65), layers=[400, 300, 200, 100], \n                       n_out=1, loss_func=F.mse_loss, metrics=mae)\nlearn.lr_find()","d7ed026f":"learn.fit_one_cycle(7, 1e-2)","e9c3f80e":"preds, targs = learn.get_preds()\nmean_absolute_error(targs, preds)","46ef0618":"learn.save(save_path\/\"nn\")","a9f17897":"def rf(xs, y, n_estimators=40, max_samples=200_000, \n      max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\n\ndef m_mae(m, xs, y): return mean_absolute_error(y, m.predict(xs))","3448f423":"cont, cat = cont_cat_split(df_nn_final, 5, dep_var=dep_var)\nprocs = [Categorify, FillMissing]\nto = TabularPandas(df_nn_final, procs, cat, cont, y_names=dep_var, splits=splits)\nxs_final, y = to.train.xs, to.train.y\nvalid_xs_final, valid_y = to.valid.xs, to.valid.y","484bc192":"try:\n    to_drop = [\"breathId_uIn_diffmean\", \"uIn_diff3\", \"breathId_uIn_diffmax\"]\n    xs_final = xs_final.drop(to_drop, axis=1)\n    valid_xs_final = valid_xs_final.drop(to_drop, axis=1)\n    print(\"Dropped\")\nexcept Exception as e: print(f\"{type(e)}: {e}\")\n\nm = rf(xs_final, y, n_estimators=45)\nm_mae(m, valid_xs_final, valid_y)","4d4d3274":"rf_preds = m.predict(valid_xs_final)\nens_preds = (to_np(preds.squeeze()) + rf_preds) \/ 2\nmean_absolute_error(valid_y, ens_preds)","01bd2912":"df_test = pd.read_csv(\"..\/input\/ventpressure2\/test_preprocessed.csv\")\ndf_test.columns","e55c5519":"df_nn_final.columns","a1e09d2b":"to_drop_test = list(set(df_test.columns).difference(set(df_nn_final.columns)))\ndf_test = df_test.drop(to_drop_test, axis=1)\ndf_test.columns","f0e2f042":"preds, targs = learn.predict_batch(df_test)","5c093e6d":"cont, cat = cont_cat_split(df_test, 5, dep_var=dep_var)\nto_test = TabularPandas(df_test, procs, cat, cont)\nto_test.xs","84cea7aa":"rf_preds = m.predict(to_test.xs)","10acc2f3":"# del rf_preds, ens_preds\nrf_preds = m.predict(to_test.xs)\nens_preds = (to_np(preds.squeeze()) + rf_preds) \/ 2","d3250478":"plt.plot(rf_preds[:80])","68df4329":"plt.plot(preds.squeeze()[:80])","e4ef7038":"ens_preds","ca54fb9b":"submission = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/sample_submission.csv\")\nsubmission.pressure = to_np(preds.squeeze())\nsubmission.to_csv(\"submission.csv\", index=False)","3bc613bf":"## Ensembling","6df887d5":"From the graph above we see that Random Forest does not \"protrude\" where we want it to (particularly when breathing in, if we check other people's notebooks such as [this notebook](https:\/\/www.kaggle.com\/dmitryuarov\/ventilator-pressure-eda-lstm-0-189\/data?select=lstm.csv) we see that the range goes from 6 to 25 (roughly) rather than staying around 16-18. This makes random forest a worse result when trying to predict the MAE for test set, making worse the ensemble model (around MAE = 16 after ensembling). ","97842e65":"It's good idea to set `y_range` for regression models. Let's find min and max of variable. ","8cc58121":"Although this upper range is not around 25, but this at least shows the graph we would like to see: a peak at the beginning, reaches a maxima, before dropping exponentially when breathing out to some mean values of breathing-out pressure (which not necessarily be zero, because in one of the previous notebook we see the out pressure most seen is less than 10, which is as expected; though how far it deviates from original is another question. ","3fad70a1":"We can already see that it is not starting with a good result. Check out the loss is just super high. However, we might just as well train for a while and see if the loss is always that high since super high loss without much training doesn't mean much. ","4914d3b6":"As we have a much much larger dataset than the original notebook, not only will we boost the hidden units but also changes the linear layer numbers as well. ","cc3c8221":"We won't be dropping anything, but you could try to drop either `R` or `C` if you would like to. ","d4900f56":"## Ensemble Predictions.\n\nAnd we can now do ensemble prediction for our test set. ","877da140":"Note that currently the result isn't great. One is trying to find the reasoning behind it. It may be due to the `procs` does not `Categorify` it with the same categorical variable, resulting in different categories (non-meaningful). These are all perhaps and work still in progress. ","44d4d545":"# Google Brain Ventilation Pressure Final Notebook\n## Neural Networks\nWe shall try training with basic Linear network and see how it does. \n\nAnd as usual, one doesn't know of any fastai's function to predict batch, so this is a modified function to \"predict_batch\" (based on \"predict\")"}}