{"cell_type":{"d2ff3662":"code","bc4cc629":"code","7c4dc0c6":"code","7598622c":"code","106e9314":"code","cb2ec5c5":"markdown","b7b994ca":"markdown","b633735f":"markdown","f684d490":"markdown","3aba0046":"markdown","20f3d56b":"markdown","a527a66c":"markdown"},"source":{"d2ff3662":"import numpy as np\nfrom sklearn.datasets import make_blobs\n \nX_all, y_all = make_blobs(\n    n_samples=200,\n    n_features=2,\n    centers=2, #classes\n    random_state=123\n)\n \n \n#Note: We are not adding bias column as webwont be using vectors.\n \n \n# plot\nimport matplotlib.pyplot as plt\nplt.scatter(x=X_all[:, 0], y=X_all[:, 1], c=y_all,\n            cmap=plt.cm.Paired)\nplt.show();\n \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=123)\n \nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape, y_test.shape)","bc4cc629":"#TRAINING (difft than ususal)\n#1. check if y(z_i) > k or < k\n#2. calculate mistakes accordingly. \n#3. Correct them until convergence\n\n#PREDICTING\n#3. predict using sign of z_i.\n#      y(z_i) cant be used for prediction\n\n#Note\n#We are not using vectors","7c4dc0c6":"class SVM:\n    def __init__(self, lr=1e-2, epochs=1000, lambda_val=1e-2, k=1):\n        self.lr = lr\n        self.epochs = epochs\n        self.lambda_val = lambda_val\n        self.k = k\n        self.w = None\n        self.b = None\n        \n    def fit(self, X_train, y_train):\n        n_feats = X_train.shape[1]\n        self.w = np.ones(n_feats)\n        self.b = np.ones(1)\n        # make sure class labels are -1 and +1\n        y_train = np.where(y_train==0, -1, 1)\n        \n        for e in range(self.epochs):\n            for i, x_i in enumerate(X_train):\n                #1. predict\n                k_hat = y_train[i] * (np.dot(x_i, self.w) + self.b)\n                correct_classification = k_hat > self.k\n                #2. find mistakes\n                if correct_classification:\n                    dw = 2 * self.lambda_val * self.w\n                    db = 0\n                else:\n                    dw = (2 * self.lambda_val * self.w) - (y_train[i] * x_i) #lazy to change wrong eqn above in latex\n                    db = (-1) * (y_train[i])\n                #3. correct\n                self.w = self.w - self.lr * dw\n                self.b = self.b - self.lr * db\n        \n    def predict(self, X_test):\n        return [self._predict(x_q) for x_q in X_test]\n        \n    def _predict(self, x_q):\n        pred = np.dot(x_q, self.w) + self.b\n        return 0 if pred<0 else 1","7598622c":"clf = SVM()\nclf.fit(X_train, y_train)\ny_test_preds = clf.predict(X_test)\n\nacc = np.sum(y_test_preds == y_test) \/ len(y_test)\nprint(acc) ","106e9314":"# Plot results\n\nw = clf.w\nb = clf.b\nk = clf.k\n\nx_min, x_max = X_train[:,0].min(), X_train[:, 0].max()\nxs = np.linspace(x_min, x_max, 3)\n\n# Decision boundary\n# wx + b = 0\n# => w[0]x1 + w[1]x2 + b = 0\n# => y = - (w[1]\/w[0])x - (b\/w[0])\nys = -1 * (1\/w[0]) * (w[1]*xs + b)\nplt.plot(xs, ys)\n\n# SVs\n# y = mx + b +- (k\/w^2)\ndist = k \/ np.sqrt(np.sum(w**2))\nsv1 = ys + dist\nsv2 = ys - dist\nplt.plot(xs, sv1, color='r')\nplt.plot(xs, sv2, color='r')\n\n# x_test\nplt.scatter(x=X_test[:,0], y=X_test[:,1], c=y_test)\n#plt.scatter(x=X_train[:,0], y=X_train[:,1], c=y_train, cmap=plt.cm.Paired)\n\n#Note: from fig. train acc is less than 1.0 but test acc is 1.0 ","cb2ec5c5":"### Prepare Dataset","b7b994ca":"<a href=\"https:\/\/colab.research.google.com\/github\/rakesh4real\/Applied-Machine-Learning\/blob\/master\/005_Linear_SVM.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","b633735f":"Consider linearly separable data for binary classification. \n \nWe will find line $l_0: wx+b = 0$ which separates them so that it aligns itself exactly in middle of support vectors $l_1: wx+b+k=0$ and $l_2: wx+b-k=0$ where $\\frac{2k}{||w||^2}$ is distance between SVs.\n \n$$\n\\begin{align}\nl_0: wx_i + b = 0 \\\\\nl_2: wx_i + b = k \\\\\nl_1: wx_i + b = -k \\\\\n\\end{align}\n$$\n \nFor simplicity, if wanted, we can take $k=1$. Note that $k=1$ doesn't mean distance between SVs is always $2k$ and it is hard coded. Distance is $\\frac{2k}{||w||^2}$ It always varies with $W$ during trainig","f684d490":"#### Before we begin\n \nA bit of Linear algebra. Understand parallel lines!\n \n- Two lines are are parallel if their slopes ( $m$ in $y=mx+c$ ) are same. \n \n- So, the only change we can do in eqn. of parallel lines is to $c$ (Done below)\n \n- Consider line $l_0: y=mx+c$\n \n    - $ l_1: y = mx + (c+k) $ is parallel to $l_0$ and lies on *top* of it.\n \n  -  $ l_2: y = mx + (c-k) $ is parallel to $l_0$ and lies *below* it.\n \n- $k$ is the equal distance of separarion between $l_1$ & $l_0$ and $l_2$ & $l_0$ when we use direction cosines; otherwise it is $\\frac{k}{||w||^2}$","3aba0046":"### Loss Function \n \n- Let class 1 be denoted by $+1$ and class 0 by $-1$\n \n- Conditions for correct classification\n    - $wx_i + b > k \\implies y=+1 \\; \\text{i.e class 1}$\n \n  - $wx_i + b < k \\implies y=-1 \\; \\text{i.e class 0}$\n \n- Above two conditions can be combined into one equation. For correct classification,\n \n$$ \n\\begin{align}\ny_i(wx_i + b) > k \\\\\n\\text{(or)} \\\\\ny_i(z_i) > k \\\\\n\\end{align}\n$$\n \n- Minimize **Hinge Loss** to find optimal $l_0$\n    - $Loss = 0$ for all correcly classified points i.e if $y_i(z_i)>k$\n \n  - $Loss$ should linearly increase for all incorrectly classified point i.e if $y_i(z_i)<k \\implies Loss = k-y_i(z_i)$ \n \n  - Can combine the above two into one eqn $$ Hinge Loss = max(0, \\, k-y_i(z_i)) $$\n \n- Maximize **Margin** to allign $l_0$ correctly. $$ argmax(\\frac{k}{||w||^{2}}) \\implies argmin(||w||^{2}) $$\n \nHence, **Loss Function** is \n \n$$ J = \\lambda{||w||^{2}} + \\frac{1}{m} \\sum max(0, k-y(z_i)) $$","20f3d56b":"### Linear SVM\n\n*We call it SVM because we consider only support vectors(lines made by support datapoints) for classification","a527a66c":"### Update\n \nActual loss-value is not important for updation but it's derivatives are important. As, hinge loss is not contninous, derivative formula changes at non-continous position of loss curve.\n \n1.When hinge loss = 0 (i.e when $y(z_i) >= k$)\n \n$$\n\\begin{align}\n J = \\lambda ||w||^{2} \\\\\n \\implies \\frac{dJ}{dw} = 2 \\lambda w   \\\\\n \\implies \\frac{dJ}{db} = 0 \\\\\n\\end{align}\n$$ \n \n2. When hinge loss = $k - y(z_i)$ (i.e when $k(y_i) < k$)\n \n$$\n\\begin{align}\n    J = \\lambda w^{2} + \\frac{1}{m} k - y_i(wx_i) \\\\\n  \\implies \\frac{dJ}{dw} = 2w\\lambda + \\frac{1}{m} x_iy_i \\\\\n  \\implies \\frac{dJ}{db} = - \\frac{1}{m} y \\\\\n\\end{align}\n$$\n \n**Update using** \n \n$$\n\\begin{align}\n w = w - \\alpha \\cdot gradient \\\\\n  b = b - \\alpha \\cdot gradient\n\\end{align}\n$$"}}