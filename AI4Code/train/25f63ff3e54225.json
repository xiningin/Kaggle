{"cell_type":{"4c10cbb1":"code","1c84c855":"code","2462d59e":"code","af501027":"code","33ad9899":"code","32e10fcf":"code","ef006ed1":"code","84d4697d":"code","acd1f508":"code","e004b8c1":"code","6a3aab3e":"code","c15ada1e":"code","3a309eb2":"code","d52e6e0c":"code","a7ce2bbb":"code","efdc3098":"code","9a173c9a":"code","f0e495ab":"code","81a612d8":"code","d250bd07":"code","a78b9452":"code","825e6997":"code","59a3aba8":"code","703b689c":"code","60fa644e":"code","568be602":"code","f71b3f53":"code","117c1d8c":"code","4f6595d7":"code","6bcaf572":"code","aa5707b6":"code","768ef854":"code","c60b9e7c":"code","a17b23d3":"code","1d9c8fcc":"code","de650534":"code","6a06ff50":"code","29d1f120":"code","dfe84399":"code","7f97e682":"code","3067a8d5":"code","a977be37":"code","067fdc94":"code","6ae66fd9":"code","a3445350":"code","96a79f61":"code","c27b557f":"code","a7fe5409":"code","e1eb1e4c":"code","8603cbec":"code","b3342ac7":"code","03a23eb0":"code","5243343b":"code","e5bb2ec7":"code","1ae3cc36":"code","ac8f6bd2":"code","47a3039d":"code","0899570c":"code","335e2785":"code","02b70fb1":"code","a76f7506":"code","f2691caa":"code","136214bd":"code","fae099d4":"code","4606e286":"code","1eb4f6e6":"code","6b5a9977":"code","3b195493":"code","7fc1763b":"code","0efc0c13":"code","bded9474":"code","410574b3":"code","25b40dbf":"code","e48724d5":"code","7be2839e":"code","eb003477":"code","0072796d":"code","0ed2eb2e":"code","6c1beb6f":"code","3224363b":"code","7a4bf6f9":"code","e433e368":"code","1ae7fd48":"code","4ed29fb6":"code","a312d656":"code","a0c65790":"code","de04cd9b":"code","c8f98c54":"code","636d6c01":"code","75cd55e9":"code","67cb3075":"code","5535d78d":"code","545f0866":"code","5c036b7c":"code","61913c53":"code","b0ff2a05":"code","f1c68888":"code","87012e46":"code","b424dba1":"code","6c42f598":"code","454f0fbe":"code","a2bc3e7b":"code","f567a2a7":"code","01581cb7":"code","3993e6c6":"code","bf6e0abe":"code","51e8a1a3":"code","0788dee3":"code","fd28418f":"code","01f3092e":"code","e4bd257c":"code","5ce552e6":"code","d160f0ee":"code","be62e9cc":"code","90ced1d3":"code","e9b8d8b2":"code","1df0ccd0":"code","baf1b254":"code","4647d7c9":"code","c6d003e8":"code","de64cbdb":"code","725a8101":"code","65bcb7b8":"code","33c0c7f7":"code","c1701900":"code","8ca953da":"code","cae328fa":"code","561a6f72":"code","5745cfdf":"code","095fc46f":"code","bc754fec":"code","2eb7bda0":"code","607a21e6":"code","8aace35e":"code","866ecfbd":"code","1efac5b9":"code","354e2b66":"code","fa9ee94f":"code","57110495":"code","d46ae37f":"code","981b8f3f":"code","b24cac58":"code","ec5f226a":"code","c1f06965":"code","26308b95":"code","830d69cc":"code","9f9a3c3f":"code","36f17b59":"code","2843d294":"code","2545c171":"code","1d124f72":"code","d96b0479":"code","013ca96a":"markdown","f65b6ec8":"markdown","53194874":"markdown","3bc0c312":"markdown","05fc7bc1":"markdown","74fa142c":"markdown","197c7d1a":"markdown","fd6d9fa9":"markdown","bf18cddf":"markdown","b49968bb":"markdown","3fb1f500":"markdown","ab4bf464":"markdown","770d6733":"markdown","341e050b":"markdown","ce2a408f":"markdown","749b3131":"markdown","6e97f6ad":"markdown","757dd814":"markdown","8205f667":"markdown","95472137":"markdown","36f89e8b":"markdown","0ddbe419":"markdown","94ed4288":"markdown","889bd7af":"markdown","acf9b281":"markdown","976de765":"markdown","8ecf762b":"markdown","9e24a6ef":"markdown","d8ebf5f2":"markdown","d5d80828":"markdown","b43d2c6b":"markdown"},"source":{"4c10cbb1":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1c84c855":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ast\nimport json\n\nfrom collections import Counter\n\nimport itertools\nfrom itertools import zip_longest\n\nimport re\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nimport eli5\n\n%matplotlib inline\n%precision 3\npd.set_option('precision', 3)\n\nimport warnings\nwarnings.filterwarnings('ignore')","2462d59e":"#\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u53d6\u308b\n#\ntrain = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\n#\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')","af501027":"print(train.shape,test.shape)\ntrain.columns","33ad9899":"train.loc[train['id'] == 391,'runtime'] = 96 #The Worst Christmas of My Life\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 592,'runtime'] = 90 #\u0410 \u043f\u043e\u0443\u0442\u0440\u0443 \u043e\u043d\u0438 \u043f\u0440\u043e\u0441\u043d\u0443\u043b\u0438\u0441\u044c\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 925,'runtime'] = 86 #\u00bfQui\u00e9n mat\u00f3 a Bambi?\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 978,'runtime'] = 93 #La peggior settimana della mia vita\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 1256,'runtime'] = 92 #Cry, Onion!\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 1542,'runtime'] = 93 #All at Once\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 1875,'runtime'] = 93 #Vermist\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 2151,'runtime'] = 108 #Mechenosets\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 2499,'runtime'] = 86 #Na Igre 2. Novyy Uroven\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 2646,'runtime'] = 98 #My Old Classmate\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 2786,'runtime'] = 111 #Revelation\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntrain.loc[train['id'] == 2866,'runtime'] = 96 #Tutto tutto niente niente\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b","32e10fcf":"test.loc[test['id'] == 3244,'runtime'] = 93 #La caliente ni\u00f1a Julietta\t\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 4490,'runtime'] = 90 #Pancho, el perro millonario\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 4633,'runtime'] = 108 #Nunca en horas de clase\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 6818,'runtime'] = 90 #Miesten v\u00e4lisi\u00e4 keskusteluja\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\n\ntest.loc[test['id'] == 4074,'runtime'] = 103 #Shikshanachya Aaicha Gho\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 4222,'runtime'] = 91 #Street Knight\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 4431,'runtime'] = 96 #Plus one\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 5520,'runtime'] = 86 #Glukhar v kino\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 5845,'runtime'] = 83 #Frau M\u00fcller muss weg!\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 5849,'runtime'] = 140 #Shabd\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 6210,'runtime'] = 104 #The Last Breath\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 6804,'runtime'] = 140 #Chaahat Ek Nasha...\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b\ntest.loc[test['id'] == 7321,'runtime'] = 87 #El truco del manco\u306e\u4e0a\u6620\u6642\u9593\u3092\u8abf\u3079\u3066\u5165\u529b","ef006ed1":"df = pd.concat([train, test]).set_index(\"id\")","84d4697d":"#columns\u3092\u78ba\u8a8d\u3057\u3001\u9664\u5916\u3059\u308b\u5909\u6570\u3092drop\nprint(df.columns)\n# \u4f7f\u308f\u306a\u3044\u5217\u3092\u6d88\u3059\ndf = df.drop([\"poster_path\", \"status\", \"original_title\"], axis=1) # \"overview\",  \"imdb_id\", ","acd1f508":"# log\u3092\u53d6\u3063\u3066\u304a\u304f\ndf[\"log_revenue\"] = np.log10(df[\"revenue\"])\n# homepage: \u6709\u7121\u306b\ndf[\"homepage\"] = ~df[\"homepage\"].isnull()","e004b8c1":"dfdic_feature = {}","6a3aab3e":"%%time\n# JSON text \u3092\u8f9e\u66f8\u578b\u306e\u30ea\u30b9\u30c8\u306b\u5909\u63db\nimport ast\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\nfor col in dict_columns:\n       df[col]=df[col].apply(lambda x: [] if pd.isna(x) else ast.literal_eval(x) )","c15ada1e":"# \u5404\u30ef\u30fc\u30c9\u306e\u6709\u7121\u3092\u8868\u3059 01 \u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210\ndef count_word_list(series):\n    len_max = series.apply(len).max() # \u30b8\u30e3\u30f3\u30eb\u6570\u306e\u6700\u5927\u5024\n    tmp = series.map(lambda x: x+[\"nashi\"]*(len_max-len(x))) # list\u306e\u9577\u3055\u3092\u305d\u308d\u3048\u308b\n    \n    word_set = set(sum(list(series.values), [])) # \u5168\u30b8\u30e3\u30f3\u30eb\u540d\u306eset\n    for n in range(len_max):\n        word_dfn = pd.get_dummies(tmp.apply(lambda x: x[n]))\n        word_dfn = word_dfn.reindex(word_set, axis=1).fillna(0).astype(int)\n        if n==0:\n            word_df = word_dfn\n        else:\n            word_df = word_df + word_dfn\n    \n    return word_df#.drop(\"nashi\", axis=1)","3a309eb2":"#budget\u304c0\u306e\u7269\u3092\u4e88\u6e2c\uff08\u30c6\u30b9\u30c8\uff09\u30010\u3067\u306a\u3044\u7269\u3092training\u30c7\u30fc\u30bf\u3068\u3059\u308b\nbudget0 = df[df[\"budget\"] == 0]\nbudget = df[df[\"budget\"] != 0]\ntrain_X = budget[[\"popularity\",\"runtime\"]]\ntrain_y = budget[\"budget\"]\ntest_X = budget0[[\"popularity\",\"runtime\"]]\ntest_y = budget0[\"budget\"]","d52e6e0c":"train_X.fillna(0, inplace = True)\ntest_X.fillna(0, inplace = True)","a7ce2bbb":"#budget\u304c0\u306e\u7269\u3092\u7dda\u5f62\u56de\u5e30\u3067\u4e88\u6e2c\nfrom sklearn.linear_model import RidgeCV\nrcv= RidgeCV(cv=3, alphas = 10**np.arange(-2, 2, 0.1))\nrcv.fit(train_X, train_y)\ny_pred = rcv.predict(test_X)","efdc3098":"budget0","9a173c9a":"budget0.index = range(0,2023)","f0e495ab":"budget_pred = pd.DataFrame(y_pred,columns=[\"pred\"])\nbudget_pred = pd.concat([budget.index,budget_pred],axis = 1)\nbudget_pred","81a612d8":"#\u4e88\u7b97\u304c0\u3092\u4e0b\u56de\u3063\u3066\u3044\u308b\u3082\u306e\u306f\u304a\u304b\u3057\u3044\u306e\u30670\u306b\u623b\u3059\u3002\nbudget_pred.loc[budget_pred[\"pred\"] < 0, \"pred\"] = 0","d250bd07":"df = pd.merge(df, budget_pred, on=\"id\", how=\"left\") \ndf.loc[budget_pred[\"id\"]-1, \"budget\"] = df.loc[budget_pred[\"id\"]-1, \"pred\"]\ndf = df.drop(\"pred\", axis=1)","a78b9452":"df[\"genre_names\"] = df[\"genres\"].apply(lambda x : [ i[\"name\"] for i in x])\n","825e6997":"dfdic_feature[\"genre\"] = count_word_list(df[\"genre_names\"])\n# TV movie \u306f1\u4ef6\u3057\u304b\u306a\u3044\u306e\u3067\u524a\u9664\ndfdic_feature[\"genre\"] = dfdic_feature[\"genre\"].drop(\"TV Movie\", axis=1)\ndfdic_feature[\"genre\"].head()","59a3aba8":"# train\u5185\u306e\u4f5c\u54c1\u6570\u304c10\u4ef6\u672a\u6e80\u306e\u8a00\u8a9e\u306f \"small\" \u306b\u96c6\u7d04\nn_language = df.loc[:train.index[-1], \"original_language\"].value_counts()\nlarge_language = n_language[n_language>=10].index\ndf.loc[~df[\"original_language\"].isin(large_language), \"original_language\"] = \"small\"","703b689c":"df[\"original_language\"] = df[\"original_language\"].astype(\"category\")","60fa644e":"# one_hot_encoding\ndfdic_feature[\"original_language\"] = pd.get_dummies(df[\"original_language\"])\n#dfdic_feature[\"original_language\"] = dfdic_feature[\"original_language\"].loc[:, dfdic_feature[\"original_language\"].sum()>0]\ndfdic_feature[\"original_language\"].head()","568be602":"df[\"production_names\"] = df[\"production_companies\"].apply(lambda x : [ i[\"name\"] for i in x])\n#.fillna(\"[{'name': 'nashi'}]\").map(to_name_list)","f71b3f53":"%time tmp = count_word_list(df[\"production_names\"])","117c1d8c":"# train\u5185\u306e\u4ef6\u6570\u304c\u591a\u3044\u7269\u306e\u307f\u9078\u3076\ndef select_top_n(df, topn=9999, nmin=2):  # topn:\u4e0a\u4f4dtopn\u4ef6, nmin:\u4f5c\u54c1\u6570nmin\u4ee5\u4e0a\n#    if \"small\" in df.columns:\n#        df = df.drop(\"small\", axis=1)\n    n_word = (df.loc[train[\"id\"]]>0).sum().sort_values(ascending=False)\n    # \u4f5c\u54c1\u6570\u304cnmin\u4ef6\u672a\u6e80\n    smallmin = n_word[n_word<nmin].index\n    # \u4e0a\u4f4dtopn\u4ef6\u306b\u5165\u3063\u3066\u3044\u306a\u3044\n    smalln = n_word.iloc[topn+1:].index\n    small = set(smallmin) | set(smalln)\n    # \u4ef6\u6570\u306e\u5c11\u306a\u3044\u30bf\u30b0\u306e\u307f\u306e\u4f5c\u54c1\n    df[\"small\"] = df[small].sum(axis=1) #>0\n    \n    return df.drop(small, axis=1)","4f6595d7":"# train\u306b50\u672c\u4ee5\u4e0a\u4f5c\u54c1\u306e\u3042\u308b\u4f1a\u793e\ndfdic_feature[\"production_companies\"] = select_top_n(tmp, nmin=50)\ndfdic_feature[\"production_companies\"].head()","6bcaf572":"# \u56fd\u540d\u306e\u30ea\u30b9\u30c8\u306b\ndf[\"country_names\"] = df[\"production_countries\"].apply(lambda x : [ i[\"name\"] for i in x])\ndf_country = count_word_list(df[\"country_names\"])","aa5707b6":"# 2\u304b\u56fd\u3060\u3063\u305f\u3089\u30010.5\u305a\u3064\u306b\ndf_country = (df_country.T\/df_country.sum(axis=1)).T.fillna(0)","768ef854":"# 30\u4f5c\u54c1\u4ee5\u4e0a\u306e\u56fd\u306e\u307f\ndfdic_feature[\"production_countries\"] = select_top_n(df_country, nmin=30)\ndfdic_feature[\"production_countries\"].head()","c60b9e7c":"df[\"keyword_list\"] = df[\"Keywords\"].apply(lambda x : [ i[\"name\"] for i in x])","a17b23d3":"def encode_topn_onehot(series, topn):\n    # \u591a\u3044\u30ef\u30fc\u30c9\u9806\u306b\n    word_count = pd.Series(collections.Counter(sum(list(series.values), [])))\n    word_count = word_count.sort_values(ascending=False)\n    \n    df_topn = df[[]].copy()  # index \u306e\u307f\u306eDF\n    # \u4e0a\u4f4dtopn\u4ef6\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u306e\u307f\n    for word in word_count.iloc[:topn].index:  # .drop(\"nashi\")\n        df_topn[word] = series.apply(lambda x: word in x)*1\n    \n    return df_topn\n    ","1d9c8fcc":"dfdic_feature[\"Keywords\"] = encode_topn_onehot(df[\"keyword_list\"], 100)","de650534":"df[\"num_Keywords\"] = df[\"keyword_list\"].apply(len)","6a06ff50":"df[\"language_names\"] = df[\"spoken_languages\"].apply(lambda x : [ i[\"name\"] for i in x])\ndf[\"n_language\"] = df[\"language_names\"].apply(len)\n# \u6b20\u640d\u5024\u306f\uff11\u306b\u3059\u308b(\u30c7\u30fc\u30bf\u3092\u898b\u308b\u3068\u7121\u58f0\u6620\u753b\u3067\u306f\u306a\u3044)\ndf.loc[df[\"n_language\"]==0, \"n_language\"] = 1","29d1f120":"# \u82f1\u8a9e\u304c\u542b\u307e\u308c\u308b\u304b\u5426\u304b\ndf[\"speak_English\"] = df[\"language_names\"].apply(lambda x : \"English\" in x)","dfe84399":"import datetime","7f97e682":"# \u516c\u958b\u65e5\u306e\u6b20\u640d1\u4ef6 id=3829\n# May,2000 (https:\/\/www.imdb.com\/title\/tt0210130\/) \n# \u65e5\u306f\u4e0d\u660e\u30021\u65e5\u3092\u5165\u308c\u3066\u304a\u304f\ndf.loc[3829, \"release_date\"] = \"5\/1\/00\"","3067a8d5":"df[\"release_year\"] = pd.to_datetime(df[\"release_date\"]).dt.year.astype(int)\n# \u5e74\u306e20\u4ee5\u964d\u3092\u30012020\u5e74\u3088\u308a\u5f8c\u306e\u672a\u6765\u3068\u5224\u5b9a\u3057\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u88dc\u6b63\u3002\ndf.loc[df[\"release_year\"]>2020, \"release_year\"] = df.loc[df[\"release_year\"]>2020, \"release_year\"]-100\n\ndf[\"release_month\"] = pd.to_datetime(df[\"release_date\"]).dt.month.astype(int)\ndf[\"release_day\"] = pd.to_datetime(df[\"release_date\"]).dt.day.astype(int)","a977be37":"# datetime\u578b\u306b\ndf[\"release_date\"] = df.apply(lambda s: datetime.datetime(\n    year=s[\"release_year\"],month=s[\"release_month\"],day=s[\"release_day\"]), axis=1)","067fdc94":"df[\"release_dayofyear\"] = df[\"release_date\"].dt.dayofyear\ndf[\"release_dayofweek\"] = df[\"release_date\"].dt.dayofweek","6ae66fd9":"# \u6708\u3001\u66dc\u65e5\u306f \u30ab\u30c6\u30b4\u30ea\u578b\u306b\ndf[\"release_month\"] = df[\"release_month\"].astype('category')\ndf[\"release_dayofweek\"] = df[\"release_dayofweek\"].astype('category')","a3445350":"# collection \u540d\u3092\u62bd\u51fa\ndf[\"collection_name\"] = df[\"belongs_to_collection\"].apply(lambda x : x[0][\"name\"] if len(x)>0 else \"nashi\")\n# \u7121\u3044\u5834\u5408\u3001\"nashi\"\u306b","96a79f61":"# \u30b7\u30ea\u30fc\u30ba\u306e\u4f5c\u54c1\u6570\n#df = pd.merge( df, df.groupby(\"collection_name\").count()[[\"budget\"]].rename(columns={\"budget\":\"count_collection\"}), \n#         on=\"collection_name\", how=\"left\")\n# index\u304c\u305a\u308c\u308b\u306e\u3067\u3001\u623b\u3059\n#df.index = df.index+1\n\ndf[\"count_collection\"] = df[\"collection_name\"].apply(lambda x : (df[\"collection_name\"]==x).sum())\n# \u30b7\u30ea\u30fc\u30ba\u4ee5\u5916\u306e\u5834\u54080\ndf.loc[df[\"collection_name\"]==\"nashi\", \"count_collection\"] = 0\n\n","c27b557f":"# \u30b7\u30ea\u30fc\u30ba\u4f55\u4f5c\u76ee\u304b\ndf[\"number_in_collection\"] = df.sort_values(\"release_date\").groupby(\"collection_name\").cumcount()+1\n# \u30b7\u30ea\u30fc\u30ba\u4ee5\u5916\u306e\u5834\u54080\ndf.loc[df[\"collection_name\"]==\"nashi\", \"number_in_collection\"] = 0\n\n","a7fe5409":"%%time\n# \u540c\u30b7\u30ea\u30fc\u30ba\u306e\u81ea\u5206\u3088\u308a\u524d\u306e\u4f5c\u54c1\u306e\u5e73\u5747log(revenue)\ndf[\"collection_av_logrevenue\"] = [ df.loc[(df[\"collection_name\"]==row[\"collection_name\"]) & \n                                          (df[\"number_in_collection\"]<row[\"number_in_collection\"]),\n                                          \"log_revenue\"].mean() \n     for key,row in df.iterrows() ]\n","e1eb1e4c":"# \u6b20\u640d(nashi) \u306e\u5834\u5408\u3001nashi \u3067\u306e\u5e73\u5747\ndf.loc[df[\"collection_name\"]==\"nashi\", \"collection_av_logrevenue\"] = df.loc[df[\"collection_name\"]==\"nashi\", \"log_revenue\"].mean()","8603cbec":"# train \u306b\u7121\u304ftest\u3060\u3051\u306b\u3042\u308b\u30b7\u30ea\u30fc\u30ba\u306e\u5834\u5408\u3001\u30b7\u30ea\u30fc\u30ba\u3082\u306e\u5168\u90e8\u306e\u5e73\u5747\ncollection_mean = df.loc[df[\"collection_name\"]!=\"nashi\", \"log_revenue\"].mean()  # \u30b7\u30ea\u30fc\u30ba\u3082\u306e\u5168\u90e8\u306e\u5e73\u5747\ndf[\"collection_av_logrevenue\"] = df[\"collection_av_logrevenue\"].fillna(collection_mean)  \n","b3342ac7":"df_features = pd.concat(dfdic_feature, axis=1)","03a23eb0":"# \u6b20\u6e2c\u30680\u306f\u30010\u3067\u306f\u306a\u3044\u3082\u306e\u306e\u5e73\u5747\u3067\u57cb\u3081\u308b\ndf[\"runtime\"] = df[\"runtime\"].fillna(df.loc[df[\"runtime\"]>0, \"runtime\"].mean())\ndf.loc[df[\"runtime\"]==0, \"runtime\"] = df.loc[df[\"runtime\"]>0, \"runtime\"].mean()","5243343b":"#plt.scatter(df[\"budget\"]+1, df[\"log_revenue\"], s=1)\n#plt.xscale(\"log\")\n#plt.xrange([])","e5bb2ec7":"df.columns","1ae3cc36":"df[[\"original_language\", \"collection_name\"]] = df[[\"original_language\", \"collection_name\"]].astype(\"category\")","ac8f6bd2":"df_use = df[['budget', 'homepage', 'popularity','runtime','n_language', \n             \"num_Keywords\", \"speak_English\",\n             'release_year', 'release_month','release_dayofweek', \n             'collection_av_logrevenue' ,\"count_collection\",\"number_in_collection\"\n            ]]\ndf_use.head()","47a3039d":"df_use = pd.get_dummies(df_use)","0899570c":"train_add = pd.read_csv('..\/input\/tmdb-competition-additional-features\/TrainAdditionalFeatures.csv')\ntest_add = pd.read_csv('..\/input\/tmdb-competition-additional-features\/TestAdditionalFeatures.csv')\ntrain_add.head()","335e2785":"df = pd.merge(df, pd.concat([train_add, test_add]), on=\"imdb_id\", how=\"left\")","02b70fb1":"add_cols = [\"popularity2\", \"rating\", \"totalVotes\"]\ndf[add_cols] = df[add_cols].fillna(df[add_cols].mean())","a76f7506":"train2 = pd.read_csv('..\/input\/tmdb-box-office-prediction-more-training-data\/additionalTrainData.csv')\ntrain3 = pd.read_csv('..\/input\/tmdb-box-office-prediction-more-training-data\/trainV3.csv')\ntrain3.head()","f2691caa":"#\u5168\u3066\u5c0f\u6587\u5b57\u306b\u5909\u63db\ndef lower_text(text):\n    return text.lower()\n\n#\u8a18\u53f7\u306e\u6392\u9664\ndef remove_punct(text):\n    text = text.replace('-', ' ')  # - \u306f\u5358\u8a9e\u306e\u533a\u5207\u308a\u3068\u307f\u306a\u3059\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef remove_stopwords(words, stopwords):#\u4e0d\u8981\u306a\u5358\u8a9e\u3092\u524a\u9664\n    words = [word for word in words if word not in stopwords]\n    return words","136214bd":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","fae099d4":"# \u82f1\u8a9e\u3067\u3088\u304f\u4f7f\u3046\u5358\u8a9e\u304c\u5165\u3063\u3066\u3044\u306a\u3044\u6587\u7ae0\u3092\u78ba\u8a8d\ndf.loc[df[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)\n                                ).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]\n#train3.loc[train3[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]","4606e286":"no_english_overview_id = [157, 2863, 4616]   # \u4e0a\u306e\u30c7\u30fc\u30bf\u3092\u76ee\u3067\u78ba\u8a8d\nno_english_tagline_id = [3255, 3777, 4937]   # Tfidf \u3067\u975e\u82f1\u8a9e\u306e\u5358\u8a9e\u304c\u3042\u3063\u305f\u3082\u306e","1eb4f6e6":"from gensim.models import word2vec","6b5a9977":"col_text = [\"overview\", \"tagline\"] # \"title\", \nall_text = pd.concat([df[col_text], train2[col_text], train3[col_text]])","3b195493":"# \u82f1\u8a9e\u4ee5\u5916\u3068\"nan\"\u306f\u9664\u5916\nall_text.loc[no_english_overview_id, \"overview\"] = np.nan\nall_text.loc[no_english_tagline_id, \"tagline\"] = np.nan\nall_text.loc[all_text[\"tagline\"]==\"nan\", \"tagline\"] = np.nan","7fc1763b":"all_texts = all_text.stack()\nall_texts=all_texts.apply(lambda x : str(x))\nall_texts=all_texts.apply(lambda x : lower_text(x))\nall_texts=all_texts.apply(lambda x : remove_punct(x))","0efc0c13":"all_texts.to_csv(\".\/alltexts_for_w2v.txt\", index=False, header=False)\ndocs = word2vec.LineSentence(\"alltexts_for_w2v.txt\")\n","bded9474":"%%time\n\nmodel = word2vec.Word2Vec(docs, sg=1, size=100, min_count=5, window=5, iter=100)\nmodel.save(\".\/alltexts_w2v1_sg.model\")","410574b3":"# model = word2vec.Word2Vec.load(\".\/alltexts_w2v1_cbow.model\")\n# model = word2vec.Word2Vec.load(\".\/alltexts_w2v1_sg.model\")","25b40dbf":"model.most_similar(positive=['father'])","e48724d5":"model.most_similar(positive=['human'])","7be2839e":"# \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e mean, max \u3092\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u306b\u3059\u308b\ndef get_doc_vector(doc, method=\"mean\", weight=None):\n    split_doc = doc.split(\" \")\n    if weight==None:\n        weight = dict(zip(model.wv.vocab.keys(), np.ones(len(model.wv.vocab))))\n        \n    word_vecs = [ model[word]*weight[word] for word in split_doc if word in model.wv.vocab.keys() ]\n    \n    if len(word_vecs)==0:\n        doc_vec = []\n    elif method==\"mean\":\n        doc_vec =  np.mean(word_vecs, axis=0)\n    elif method==\"max\":\n        doc_vec =  np.max(word_vecs, axis=0)\n    elif method==\"meanmax\":\n        doc_vec =  np.mean(word_vecs, axis=0)+np.max(word_vecs, axis=0)\n    return doc_vec","eb003477":"#\u5358\u8a9e\u6570\ndf['overview_word_count'] = df['overview'].apply(lambda x: len(str(x).split()))\n#\u6587\u5b57\u6570\ndf['overview_char_count'] = df['overview'].apply(lambda x: len(str(x)))\n# \u8a18\u53f7\u306e\u500b\u6570\ndf['overview_punctuation_count'] = df['overview'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","0072796d":"# \u524d\u51e6\u7406\ndf['_overview']=df['overview'].apply(lambda x : str(x)\n                            ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","0ed2eb2e":"#\u77ed\u7e2e\u5f62\u3092\u5143\u306b\u623b\u3059\nshortened = {\n    '\\'m': ' am',\n    '\\'re': ' are',\n    'don\\'t': 'do not',\n    'doesn\\'t': 'does not',\n    'didn\\'t': 'did not',\n    'won\\'t': 'will not',\n    'wanna': 'want to',\n    'gonna': 'going to',\n    'gotta': 'got to',\n    'hafta': 'have to',\n    'needa': 'need to',\n    'outta': 'out of',\n    'kinda': 'kind of',\n    'sorta': 'sort of',\n    'lotta': 'lot of',\n    'lemme': 'let me',\n    'gimme': 'give me',\n    'getcha': 'get you',\n    'gotcha': 'got you',\n    'letcha': 'let you',\n    'betcha': 'bet you',\n    'shoulda': 'should have',\n    'coulda': 'could have',\n    'woulda': 'would have',\n    'musta': 'must have',\n    'mighta': 'might have',\n    'dunno': 'do not know',\n}\ndf[\"overview\"] = df[\"overview\"].replace(shortened)\ntrain[\"overview\"] = train[\"overview\"].replace(shortened)","6c1beb6f":"df[\"overview\"]=df[\"overview\"].apply(lambda x : remove_punct(x))\ntrain[\"overview\"]=train[\"overview\"].apply(lambda x : remove_punct(x))","3224363b":"# \u9023\u7d9a\u3057\u305f\u6570\u5b57\u30920\u3067\u7f6e\u63db\ndef normalize_number(text):\n    replaced_text = re.sub(r'\\d+', '0', text)\n    return replaced_text","7a4bf6f9":"df[\"overview\"]=df[\"overview\"].apply(lambda x : normalize_number(x))\ntrain[\"overview\"]=train[\"overview\"].apply(lambda x : normalize_number(x))","e433e368":"#\u30ec\u30f3\u30de\u5316\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nwnl = WordNetLemmatizer()\ndf[\"overview\"]=df[\"overview\"].apply(wnl.lemmatize)\ntrain[\"overview\"]=train[\"overview\"].apply(wnl.lemmatize)","1ae7fd48":"#\u7a7a\u767d\u3054\u3068\u306e\u6587\u7ae0\u306e\u5206\u5272\ndf[\"overview\"]=df[\"overview\"].apply(lambda x : str(x).split())\ntrain[\"overview\"]=train[\"overview\"].apply(lambda x : str(x).split())","4ed29fb6":"df_overview = df[\"overview\"]","a312d656":"def most_common(docs, n=100):#(\u6587\u7ae0\u3001\u4e0a\u4f4dn\u500b\u306e\u5358\u8a9e)#\u4e0a\u4f4dn\u500b\u306e\u5358\u8a9e\u3092\u62bd\u51fa\n    fdist = Counter()\n    for doc in docs:\n        for word in doc:\n            fdist[word] += 1\n    common_words = {word for word, freq in fdist.most_common(n)}\n    print('{}\/{}'.format(n, len(fdist)))\n    return common_words","a0c65790":"most_common(df_overview,100)","de04cd9b":"def get_stop_words(docs, n=100, min_freq=1):#\u4e0a\u4f4dn\u500b\u306e\u5358\u8a9e\u3001\u983b\u5ea6\u304cmin_freq\u4ee5\u4e0b\u306e\u5358\u8a9e\u3092\u5217\u6319\uff08\u3042\u307e\u308a\u7279\u5fb4\u306e\u306a\u3044\u5358\u8a9e\u7b49\uff09\n    fdist = Counter()\n    for doc in docs:\n        for word in doc:\n            fdist[word] += 1\n    common_words = {word for word, freq in fdist.most_common(n)}\n    rare_words = {word for word, freq in fdist.items() if freq <= min_freq}\n    stopwords = common_words.union(rare_words)\n    print('{}\/{}'.format(len(stopwords), len(fdist)))\n    return stopwords","c8f98c54":"stopwords = get_stop_words(df_overview)\nstopwords","636d6c01":"def remove_stopwords(words, stopwords):#\u4e0d\u8981\u306a\u5358\u8a9e\u3092\u524a\u9664\n    words = [word for word in words if word not in stopwords]\n    return words","75cd55e9":"df[\"overview\"]=df[\"overview\"].apply(lambda x : remove_stopwords(x,stopwords))\ntrain[\"overview\"]=train[\"overview\"].apply(lambda x : remove_stopwords(x,stopwords))","67cb3075":"df[\"overview\"]=[\" \".join(review) for review in df[\"overview\"].values]\ntrain[\"overview\"]=[\" \".join(review) for review in train[\"overview\"].values]","5535d78d":"from sklearn.feature_extraction.text import TfidfVectorizer#\u30d9\u30af\u30c8\u30eb\u5316\nvec_tfidf = TfidfVectorizer()\nX = vec_tfidf.fit_transform(df[\"overview\"])\nTfid_overview = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n\nX2 = vec_tfidf.fit_transform(df[\"overview\"])\nTfid_train_overview = pd.DataFrame(X2.toarray(), columns=vec_tfidf.get_feature_names())","545f0866":"df['_tagline']=df['tagline'].apply(lambda x : str(x)\n                                 ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","5c036b7c":"#\u30d9\u30af\u30c8\u30eb\u5316\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# vec_tfidf = TfidfVectorizer()\n# X = vec_tfidf.fit_transform(df['tagline'])\n# Tfidf_tagline = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n# X = vec_tfidf.fit_transform(df['overview'].dropna())\n# Tfidf_overview = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())","61913c53":"%time df_tagline =  df[\"_tagline\"].apply(get_doc_vector, method=\"meanmax\").apply(pd.Series)","b0ff2a05":"df_tagline = df_tagline.fillna(0).add_prefix(\"tagline_\")","f1c68888":"#\u5358\u8a9e\u6570\ndf['title_word_count'] = df['title'].apply(lambda x: len(str(x).split()))\n#\u6587\u5b57\u6570\ndf['title_char_count'] = df['title'].apply(lambda x: len(str(x)))\n# \u8a18\u53f7\u306e\u500b\u6570\ndf['title_punctuation_count'] = df['title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","87012e46":"df['tagline']=df['tagline'].apply(lambda x : str(x))\ndf[\"tagline\"] = df[\"tagline\"].replace(shortened)\ndf['tagline']=df['tagline'].apply(lambda x : lower_text(x))\ndf['tagline']=df['tagline'].apply(lambda x : remove_punct(x))\ndf[\"tagline\"]=df[\"tagline\"].apply(lambda x : normalize_number(x))\ndf['tagline']=df['tagline'].apply(lambda x : str(x).split())","b424dba1":"tagline = df[\"tagline\"]","6c42f598":"most_common(tagline)","454f0fbe":"stopwords = get_stop_words(tagline)","a2bc3e7b":"df['tagline']=df['tagline'].apply(lambda x : remove_stopwords(x,stopwords))","f567a2a7":"nan = {\"nan\"}\ndef remove_nan(words):\n    words = [word for word in words if word not  in nan]\n    return words","01581cb7":"df['tagline']=df['tagline'].apply(lambda x : remove_nan(x))","3993e6c6":"df['tagline']=[\" \".join(review) for review in df['tagline'].values]","bf6e0abe":"#\u30d9\u30af\u30c8\u30eb\u5316\nX = vec_tfidf.fit_transform(df['tagline'])\nTfid_tagline = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())","51e8a1a3":"df_use2 = df[[\"runtime\",'budget','tagline_char_count']]","0788dee3":"df_use2 = pd.concat([df_use2,Tfid_overview],axis=1)","fd28418f":"#\u4f7f\u7528\u3059\u308b\u5909\u6570\ndf_use2 = df_use2.loc[:,~df_use.columns.duplicated()]","01f3092e":"# Keywords \u3092\u5168\u90e8\u4e26\u3079\u305f\u3082\u306e\u3092\u3001\u6587\u3068\u307f\u306a\u3057\u3066\u30d9\u30af\u30c8\u30eb\u5316\n%time df_keyword_w2v = df[\"keyword_list\"].apply(\" \".join).apply(get_doc_vector, method=\"mean\").apply(pd.Series)\ndf_keyword_w2v = df_keyword_w2v.fillna(0).add_prefix(\"keyword_\")","e4bd257c":"#cast\u306e\u4e2d\u306b\u3042\u308b\u4ff3\u512a\u306e\u540d\u524d\u3092\u30ea\u30b9\u30c8\u5316\u3055\u305b\u308b\nlist_of_cast_names = list(df['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndf['num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\ndf['all_cast'] = df['cast'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n\n\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(30)]\nfor g in top_cast_names:\n    df['cast_name_' + g] = df['all_cast'].apply(lambda x: 1 if g in x else 0)","5ce552e6":"list_of_cast_genders = list(df['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\n\ndf['genders_0_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ndf['genders_1_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ndf['genders_2_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))    \n\n#df = df.drop(['cast'], axis=1)\n\ndf['cast_gen0_ratio'] = df['genders_0_cast'].sum()\/df['num_cast'].sum()\ndf['cast_gen1_ratio'] = df['genders_1_cast'].sum()\/df['num_cast'].sum()\ndf['cast_gen2_ratio'] = df['genders_2_cast'].sum()\/df['num_cast'].sum()","d160f0ee":"#crew\u306ename\nlist_of_crew_names = list(df['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndf['num_crew'] = df['crew'].apply(lambda x: len(x) if x != {} else 0)\ndf['all_crew'] = df['crew'].apply(lambda x: ','.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\nfor g in top_crew_names:\n    df['crew_name_' + g] = df['all_crew'].apply(lambda x: 1 if g in x else 0)","be62e9cc":"list_of_crew_department = list(df['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\ndf['all_department'] = df['crew'].apply(lambda x: '\u3000'.join(sorted([i['department']for i in x])) if x != {} else '')\ntop_crew_department = [m[0] for m in Counter(i for j in list_of_crew_department for i in j).most_common(12)]\nfor g in top_crew_department:\n    df['crew_department_' + g] = df['crew'].apply(lambda x: sum([1 for i in x if i['department'] == g]))","90ced1d3":"list_of_crew_job = list(df['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\ntop_crew_job = [m[0] for m in Counter(i for j in list_of_crew_job for i in j).most_common(10)]\nfor g in top_crew_job:\n    df['crew_job_' + g] = df['crew'].apply(lambda x: sum([1 for i in x if i['job'] == g]))","e9b8d8b2":"df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ndf['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ndf['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ndf['crew_gen0_ratio'] = df['genders_0_crew'].sum()\/df['num_crew'].sum()\ndf['crew_gen1_ratio'] = df['genders_1_crew'].sum()\/df['num_crew'].sum()\ndf['crew_gen2_ratio'] = df['genders_2_crew'].sum()\/df['num_crew'].sum()","1df0ccd0":"all_crew_job = [m[0] for m in Counter([i for j in list_of_crew_job for i in j]).most_common()]","baf1b254":"all_crew_department = [m[0] for m in Counter([i for j in list_of_crew_department for i in j]).most_common()]\n","4647d7c9":"def select_department(list_dict, department):\n    return [ dic['name'] for dic in list_dict if dic['department']==department]","c6d003e8":"for z in all_crew_department:\n    df['{}_list'.format(z)] = df[\"crew\"].apply(select_department, department=z)\n    globals()[z] = [m[0] for m in Counter([i for j in df['{}_list'.format(z)] for i in j]).most_common(15)]\n    for i in globals()[z]:\n        df['crew_{}_{}'.format(z,i)] = df['{}_list'.format(z)].apply(lambda x: sum([1 for i in x]))","de64cbdb":"def select_job(list_dict, job):\n    return [ dic[\"name\"] for dic in list_dict if dic[\"job\"]==job]","725a8101":"for z in top_crew_job:\n    df['{}_list'.format(z)] = df[\"crew\"].apply(select_job, job=z)\n    globals()[z] = [m[0] for m in Counter([i for j in df['{}_list'.format(z)] for i in j]).most_common(15)]\n    for i in globals()[z]:\n        df['crew_{}_{}'.format(z,i)] = df['{}_list'.format(z)].apply(lambda x: sum([1 for i in x]))","65bcb7b8":"df.columns","33c0c7f7":"df_use3=df[['num_cast','all_cast','cast_name_Samuel L. Jackson','cast_name_Robert De Niro','cast_name_Bruce Willis',\n'cast_name_Morgan Freeman','cast_name_Liam Neeson','cast_name_Willem Dafoe','cast_name_Steve Buscemi',\n'cast_name_Sylvester Stallone','cast_name_Nicolas Cage','cast_name_Matt Damon','cast_name_J.K. Simmons',\n'cast_name_John Goodman','cast_name_Julianne Moore','cast_name_Christopher Walken','cast_name_Robin Williams',\n'cast_name_Johnny Depp','cast_name_Stanley Tucci','cast_name_Harrison Ford','cast_name_Richard Jenkins',\n'cast_name_Ben Stiller','cast_name_Susan Sarandon','cast_name_Brad Pitt','cast_name_Tom Hanks',\n'cast_name_Keith David','cast_name_John Leguizamo','cast_name_Woody Harrelson','cast_name_Bill Murray','cast_name_Dennis Quaid','cast_name_James Franco','cast_name_Dustin Hoffman','genders_0_cast','genders_1_cast',\n'genders_2_cast','cast_gen0_ratio','cast_gen1_ratio','cast_gen2_ratio','num_crew','all_crew','crew_name_Avy Kaufman','crew_name_Steven Spielberg',\n'crew_name_Robert Rodriguez','crew_name_Mary Vernieu','crew_name_Deborah Aquila','crew_name_Bob Weinstein','crew_name_Harvey Weinstein','crew_name_Hans Zimmer','crew_name_Tricia Wood','crew_name_James Newton Howard',\n'crew_name_James Horner','crew_name_Luc Besson','crew_name_Francine Maisler','crew_name_Kerry Barden','crew_name_Jerry Goldsmith','all_department','crew_department_Production','crew_department_Sound',\n'crew_department_Art','crew_department_Crew','crew_department_Writing','crew_department_Costume & Make-Up','crew_department_Camera','crew_department_Directing','crew_department_Editing','crew_department_Visual Effects','crew_department_Lighting','crew_department_Actors','crew_job_Producer','crew_job_Executive Producer','crew_job_Director','crew_job_Screenplay','crew_job_Editor','crew_job_Casting','crew_job_Director of Photography','crew_job_Original Music Composer','crew_job_Art Direction','crew_job_Production Design',\n'genders_0_crew','genders_1_crew','genders_2_crew','crew_gen0_ratio','crew_gen1_ratio','crew_gen2_ratio',\n'crew_Production_Avy Kaufman','crew_Production_Mary Vernieu','crew_Production_Deborah Aquila','crew_Production_Bob Weinstein','crew_Production_Harvey Weinstein','crew_Production_Tricia Wood','crew_Production_Francine Maisler','crew_Production_Kerry Barden','crew_Production_Billy Hopkins','crew_Production_Steven Spielberg','crew_Production_Suzanne Smith',\n'crew_Production_Arnon Milchan','crew_Production_Scott Rudin','crew_Production_John Papsidera','crew_Production_Tim Bevan','crew_Sound_James Newton Howard','crew_Sound_Hans Zimmer','crew_Sound_James Horner','crew_Sound_Jerry Goldsmith','crew_Sound_John Williams',\n'crew_Sound_Alan Silvestri','crew_Sound_Danny Elfman',\"crew_Sound_Dan O'Connell\",'crew_Sound_Mark Isham','crew_Sound_John Debney','crew_Sound_Marco Beltrami',\n'crew_Sound_Kevin Kaska','crew_Sound_Christophe Beck','crew_Sound_Graeme Revell','crew_Sound_Carter Burwell','crew_Art_Helen Jarvis','crew_Art_Ray Fisher','crew_Art_Rosemary Brandenburg',\n'crew_Art_Cedric Gibbons','crew_Art_Walter M. Scott','crew_Art_Nancy Haigh','crew_Art_Robert Gould','crew_Art_J. Michael Riva','crew_Art_Maher Ahmad','crew_Art_Henry Bumstead','crew_Art_Leslie A. Pope',\n'crew_Art_Gene Serdena','crew_Art_Jann Engel','crew_Art_David F. Klassen','crew_Art_Cindy Carr','crew_Crew_J.J. Makaro','crew_Crew_Brian N. Bentley',\n'crew_Crew_Brian Avery','crew_Crew_James Bamford','crew_Crew_Mark Edward Wright','crew_Crew_Karin Silvestri','crew_Crew_Gregory Nicotero','crew_Crew_G.A. Aguilar',\n'crew_Crew_Doug Coleman','crew_Crew_Sean Button',\"crew_Crew_Chris O'Connell\",'crew_Crew_Tim Monich','crew_Crew_Denny Caira',\n'crew_Crew_Susan Hegarty','crew_Crew_Michael Queen','crew_Writing_Luc Besson','crew_Writing_Stephen King','crew_Writing_Woodyallen','crew_Writing_John Hughes',\n'crew_Writing_Ian Fleming','crew_Writing_Robert Mark Kamen','crew_Writing_Sylvester Stallone','crew_Writing_David Koepp','crew_Writing_Terry Rossio',\n'crew_Writing_George Lucas','crew_Writing_Stan Lee','crew_Writing_Akiva Goldsman','crew_Writing_Brian Helgeland','crew_Writing_Ted Elliott','crew_Writing_William Goldman','crew_Costume & Make-Up_Ve Neill',\n'crew_Costume & Make-Up_Bill Corso','crew_Costume & Make-Up_Colleen Atwood','crew_Costume & Make-Up_Camille Friend','crew_Costume & Make-Up_Edith Head','crew_Costume & Make-Up_Louise Frogley','crew_Costume & Make-Up_Ellen Mirojnick',\n'crew_Costume & Make-Up_Mary Zophres','crew_Costume & Make-Up_Edouard F. Henriques','crew_Costume & Make-Up_Jean Ann Black','crew_Costume & Make-Up_Marlene Stewart','crew_Costume & Make-Up_Ann Roth','crew_Costume & Make-Up_Deborah La Mia Denaver',\n'crew_Costume & Make-Up_Alex Rouse','crew_Costume & Make-Up_Shay Cunliffe','crew_Camera_Hans Bjerno','crew_Camera_Roger Deakins','crew_Camera_Dean Semler',\n'crew_Camera_David B. Nowell','crew_Camera_Mark Irwin','crew_Camera_John Marzano','crew_Camera_Matthew F. Leonetti','crew_Camera_Dean Cundey',\n'crew_Camera_Frank Masi','crew_Camera_Oliver Wood','crew_Camera_Robert Elswit','crew_Camera_Pete Romano','crew_Camera_Merrick Morton',\n'crew_Camera_Robert Richardson','crew_Camera_Philippe Rousselot','crew_Directing_Steven Spielberg','crew_Directing_Clint Eastwood','crew_Directing_Woodyallen',\n'crew_Directing_Ridley Scott','crew_Directing_Karen Golden','crew_Directing_Alfred Hitchcock','crew_Directing_Kerry Lyn McKissick','crew_Directing_Ron Howard','crew_Directing_Dianne Dreyer','crew_Directing_Wilma Garscadden-Gahret',\n'crew_Directing_Martin Scorsese','crew_Directing_Brian De Palma','crew_Directing_Ana Maria Quintana','crew_Directing_Dug Rotstein',\n'crew_Directing_Tim Burton','crew_Editing_Michael Kahn','crew_Editing_Chris Lebenzon','crew_Editing_Jim Passon',\n'crew_Editing_Gary Burritt','crew_Editing_Dale E. Grahn','crew_Editing_Joel Cox','crew_Editing_Mark Goldblatt',\n'crew_Editing_Conrad Buff IV','crew_Editing_John C. Stuver','crew_Editing_Pietro Scalia','crew_Editing_Paul Hirsch',\n'crew_Editing_Don Zimmerman','crew_Editing_Robert Troy','crew_Editing_Steven Rosenblum','crew_Editing_Dennis McNeill',\n'crew_Visual Effects_Dottie Starling','crew_Visual Effects_Phil Tippett','crew_Visual Effects_James Baker','crew_Visual Effects_Hugo Dominguez',\n'crew_Visual Effects_Larry White','crew_Visual Effects_Ray McIntyre Jr.','crew_Visual Effects_James Baxter','crew_Visual Effects_Aaron Williams',\"crew_Visual Effects_Julie D'Antoni\",'crew_Visual Effects_Frank Thomas','crew_Visual Effects_Milt Kahl','crew_Visual Effects_Peter Chiang','crew_Visual Effects_Chuck Duke','crew_Visual Effects_Dave Kupczyk','crew_Visual Effects_Craig Barron','crew_Lighting_Justin Hammond','crew_Lighting_Howard R. Campbell',\n'crew_Lighting_Arun Ram-Mohan','crew_Lighting_Chuck Finch','crew_Lighting_Russell Engels','crew_Lighting_Frank Dorowsky',\n'crew_Lighting_Bob E. Krattiger','crew_Lighting_Ian Kincaid','crew_Lighting_Thomas Neivelt','crew_Lighting_Dietmar Haupt','crew_Lighting_James J. Gilson',\n'crew_Lighting_Dan Cornwall','crew_Lighting_Andy Ryan','crew_Lighting_Lee Walters','crew_Lighting_Jay Kemp','crew_Actors_Francois Grobbelaar',\n\"crew_Actors_Mick 'Stuntie' Milligan\",'crew_Actors_Sol Gorss','crew_Actors_Mark De Alessandro','crew_Actors_Leigh Walsh',\n'crew_Producer_Joel Silver','crew_Producer_Brian Grazer','crew_Producer_Scott Rudin','crew_Producer_Neal H. Moritz',\n'crew_Producer_Tim Bevan','crew_Producer_Eric Fellner','crew_Producer_Jerry Bruckheimer','crew_Producer_Arnon Milchan',\n'crew_Producer_Gary Lucchesi','crew_Producer_John Davis','crew_Producer_Jason Blum','crew_Producer_Tom Rosenberg','crew_Producer_Kathleen Kennedy',\n'crew_Producer_Luc Besson','crew_Producer_Steven Spielberg','crew_Executive Producer_Bob Weinstein','crew_Executive Producer_Harvey Weinstein','crew_Executive Producer_Bruce Berman',\n'crew_Executive Producer_Steven Spielberg','crew_Executive Producer_Toby Emmerich','crew_Executive Producer_Stan Lee','crew_Executive Producer_Ryan Kavanaugh','crew_Executive Producer_Ben Waisbren','crew_Executive Producer_Michael Paseornek','crew_Executive Producer_Thomas Tull','crew_Executive Producer_Arnon Milchan','crew_Executive Producer_Nathan Kahane','crew_Executive Producer_John Lasseter','crew_Executive Producer_Tessa Ross',\n'crew_Executive Producer_Gary Barber','crew_Director_Steven Spielberg','crew_Director_Clint Eastwood','crew_Director_Woodyallen','crew_Director_Ridley Scott',\n'crew_Director_Alfred Hitchcock','crew_Director_Ron Howard','crew_Director_Brian De Palma','crew_Director_Martin Scorsese','crew_Director_Tim Burton',\n'crew_Director_Blake Edwards','crew_Director_Joel Schumacher','crew_Director_Oliver Stone','crew_Director_Robert Zemeckis','crew_Director_Steven Soderbergh',\n'crew_Director_Wes Craven','crew_Screenplay_Sylvester Stallone','crew_Screenplay_Luc Besson','crew_Screenplay_John Hughes','crew_Screenplay_Akiva Goldsman','crew_Screenplay_David Koepp','crew_Screenplay_William Goldman','crew_Screenplay_Robert Mark Kamen','crew_Screenplay_Oliver Stone',\n'crew_Screenplay_Woodyallen','crew_Screenplay_Richard Maibaum','crew_Screenplay_John Logan','crew_Screenplay_Terry Rossio','crew_Screenplay_Harold Ramis',\n'crew_Screenplay_Brian Helgeland','crew_Screenplay_Ted Elliott','crew_Editor_Michael Kahn','crew_Editor_Chris Lebenzon','crew_Editor_Joel Cox',\n'crew_Editor_Mark Goldblatt','crew_Editor_Conrad Buff IV','crew_Editor_Pietro Scalia','crew_Editor_Paul Hirsch','crew_Editor_Don Zimmerman',\n'crew_Editor_Christian Wagner','crew_Editor_Anne V. Coates','crew_Editor_William Goldenberg','crew_Editor_Michael Tronick','crew_Editor_Daniel P. Hanley',\n'crew_Editor_Paul Rubell','crew_Editor_Stephen Mirrione','crew_Casting_Avy Kaufman','crew_Casting_Mary Vernieu','crew_Casting_Deborah Aquila',\n'crew_Casting_Tricia Wood','crew_Casting_Kerry Barden','crew_Casting_Francine Maisler','crew_Casting_Billy Hopkins','crew_Casting_Suzanne Smith',\n'crew_Casting_John Papsidera','crew_Casting_Denise Chamian','crew_Casting_Jane Jenkins','crew_Casting_Janet Hirshenson','crew_Casting_Mike Fenton',\n'crew_Casting_Mindy Marin','crew_Casting_Sarah Finn','crew_Director of Photography_Dean Semler','crew_Director of Photography_Roger Deakins','crew_Director of Photography_Mark Irwin',\n'crew_Director of Photography_Matthew F. Leonetti','crew_Director of Photography_Dean Cundey','crew_Director of Photography_Oliver Wood','crew_Director of Photography_Robert Elswit','crew_Director of Photography_Robert Richardson',\n'crew_Director of Photography_Philippe Rousselot','crew_Director of Photography_Dante Spinotti','crew_Director of Photography_Julio Macat','crew_Director of Photography_Dariusz Wolski','crew_Director of Photography_Don Burgess',\n'crew_Director of Photography_Janusz Kami\u2248\u00d1ski','crew_Director of Photography_Peter Deming','crew_Original Music Composer_James Newton Howard','crew_Original Music Composer_James Horner','crew_Original Music Composer_Hans Zimmer',\n'crew_Original Music Composer_Jerry Goldsmith','crew_Original Music Composer_John Williams','crew_Original Music Composer_Danny Elfman','crew_Original Music Composer_Christophe Beck','crew_Original Music Composer_Alan Silvestri',\n'crew_Original Music Composer_John Powell','crew_Original Music Composer_Marco Beltrami','crew_Original Music Composer_Howard Shore','crew_Original Music Composer_Graeme Revell','crew_Original Music Composer_John Debney',\n'crew_Original Music Composer_Carter Burwell','crew_Original Music Composer_Mark Isham','crew_Art Direction_Cedric Gibbons','crew_Art Direction_Hal Pereira','crew_Art Direction_Helen Jarvis',\n'crew_Art Direction_Lyle R. Wheeler','crew_Art Direction_David Lazan','crew_Art Direction_Andrew Max Cahn','crew_Art Direction_Jack Martin Smith','crew_Art Direction_Robert Cowper',\n'crew_Art Direction_Stuart Rose','crew_Art Direction_David F. Klassen','crew_Art Direction_Dan Webster','crew_Art Direction_Steven Lawrence','crew_Art Direction_Jesse Rosenthal',\n'crew_Art Direction_Richard L. Johnson','crew_Art Direction_Kevin Constant','crew_Production Design_J. Michael Riva','crew_Production Design_Jon Hutman','crew_Production Design_Carol Spier',\n'crew_Production Design_Ida Random','crew_Production Design_Dennis Gassner','crew_Production Design_Perry Andelin Blake','crew_Production Design_David Gropman',\n'crew_Production Design_Mark Friedberg','crew_Production Design_Rick Carter','crew_Production Design_Stuart Craig','crew_Production Design_Jim Clay',\n'crew_Production Design_Kristi Zea','crew_Production Design_David Wasco','crew_Production Design_Wynn Thomas','crew_Production Design_Dante Ferretti]]","c1701900":"df","8ca953da":"df_features.index = df.index\n\ndf_use.index = df.index\ndf_use2.index = df.index\n","cae328fa":"df_use4 = df[add_cols]","561a6f72":"df_input = pd.concat([df_use, df_use2, df_use3, df_use4, df_features], axis=1) # .drop(\"belongs_to_collection\", axis=1)","5745cfdf":"#Tfid_tagline.index = df_use.index\n#df_use_Tfid = Tfid_tagline.loc[:, Tfid_tagline[:3000].nunique()>1]\n#df_use_Tfid.shape","095fc46f":"# \u5168\u3066\u7e4b\u3052\u305f\u7279\u5fb4\u91cf\ndf_input = pd.concat([df_input, df_tagline, df_overview, df_keyword_w2v, df_castname, df_crewname], axis=1)","bc754fec":"# \u6b20\u6e2c\u30ca\u30b7\u3092\u78ba\u8a8d\ndf_input.isnull().sum().sum()","2eb7bda0":"#cols = df_input.loc[:, df_input.isnull().sum()>0].columns\n#df_input.loc[:, cols] = df_input[cols].fillna(df_input[cols].mean())","607a21e6":"# \u4fdd\u5b58\nimport pickle\nwith open('df_input.pkl', 'wb') as f:\n      pickle.dump(df_input , f)","8aace35e":"df[\"ln_revenue\"] = np.log(df[\"revenue\"]+1)","866ecfbd":"# \u6570\u5024\u5316\u3067\u304d\u3044\u5217\u3092\u78ba\u8a8d\nno_numeric = df_input.apply(lambda s:pd.to_numeric(s, errors='coerce')).isnull().all()\nno_numeric[no_numeric]","1efac5b9":"X_all = df_input  # .drop([\"collection_av_logrevenue\"], axis=1)\ny_all = df[\"ln_revenue\"]\ny_all.index = X_all.index","354e2b66":"[ c for c in X_all.columns if \"revenue\" in str(c)]","fa9ee94f":"# \u6a19\u6e96\u5316\n# X_train_all_mean = X_all[:3000].mean()\n# X_train_all_std  = X_all[:3000].std()\n# X_all = (X_all-X_train_all_mean)\/X_train_all_std","57110495":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.preprocessing import StandardScaler","d46ae37f":"train_X, val_X, train_y, val_y = train_test_split(X_all[:train.index[-1]], \n                                                  y_all[:train.index[-1]], \n                                                  test_size=0.25, random_state=1)","981b8f3f":"from sklearn.ensemble import RandomForestRegressor","b24cac58":"clf2 = RandomForestRegressor(n_jobs=3, random_state=1)  # max_depth=, min_samples_split=, \nclf2.fit(train_X, train_y)","ec5f226a":"val_pred = clf2.predict(val_X)\nprint(\"RMSLE score for validation data\")\nnp.sqrt(mean_squared_error(val_pred, val_y))","c1f06965":"plt.scatter(np.exp(val_pred)+1, np.exp(val_y)+1, s=3)\nplt.xlabel(\"prediction\")\nplt.ylabel(\"true revenue\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")","26308b95":"clf2 = RandomForestRegressor(n_jobs=3, random_state=1, n_estimators=500)  # \nclf2.fit(X_all[:train.index[-1]], y_all[:train.index[-1]])","830d69cc":"df_importance = pd.DataFrame([clf2.feature_importances_], columns=train_X.columns, index=[\"importance\"]).T\ndf_importance.sort_values(\"importance\", ascending=False).head(20)","9f9a3c3f":"test_pred = clf2.predict(X_all[3000:])","36f17b59":"test_revenue = np.exp(test_pred)-1","2843d294":"sample_submission = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')","2545c171":"submission_RF = sample_submission.copy()\nsubmission_RF[\"revenue\"] = test_revenue","1d124f72":"submission_RF","d96b0479":"submission_RF.to_csv('submission_RF.csv', index=False)","013ca96a":"# crew n department","f65b6ec8":"## budget","53194874":"## df\u4f5c\u6210","3bc0c312":"## title\u306e\u524d\u51e6\u7406","05fc7bc1":"# \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u4f5c\u6210","74fa142c":"## release_date","197c7d1a":"### \u82f1\u8a9e\u4ee5\u5916","fd6d9fa9":"## genres","bf18cddf":"# \u5404\u5217\u306e\u51e6\u7406","b49968bb":"## Crew","3fb1f500":"# \u8a00\u8a9e\u51e6\u7406","ab4bf464":"### word2vec","770d6733":"## runtime\u3000\u6b20\u6e2c\u51e6\u7406","341e050b":"# randomforest","ce2a408f":"## Additional data","749b3131":"## \u6574\u7406","6e97f6ad":"## production contries","757dd814":"## cast","8205f667":"## \u8abf\u3079\u305f\u6b20\u6e2c\u30c7\u30fc\u30bf","95472137":"## \u6574\u5f62","36f89e8b":"## \u9023\u7d50","0ddbe419":"## spoken laguages","94ed4288":"## keyword\u3082word2vec\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b\u3068\uff1f","889bd7af":"## belongs to collection","acf9b281":"## original language","976de765":"## production company","8ecf762b":"## Keyword","9e24a6ef":"# TMDB prediction","d8ebf5f2":"# submit","d5d80828":"## overview","b43d2c6b":"# budget"}}