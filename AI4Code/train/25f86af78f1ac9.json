{"cell_type":{"cbef069b":"code","b8558d3c":"code","0763e3a6":"code","897793f0":"code","9eb52bd8":"code","f00730e6":"code","8736c721":"code","82e469c0":"code","9e05721c":"code","fdf8d24f":"code","5f020d86":"code","732ab64d":"code","4387f65f":"code","e7a3d30c":"code","0b262fe0":"code","9a3a1d3a":"code","53ebad68":"code","7de83d20":"code","50d4d4e6":"code","4497b3d7":"code","df5f1da1":"code","11f4e30b":"code","9bc3d967":"code","8dac7b0d":"code","c500ac05":"code","4f41d47f":"code","eca97b6c":"code","09897f73":"code","4cf0813a":"code","7f096ff1":"code","43172e57":"code","72848b89":"code","37496dd2":"code","1ac27ccb":"code","144b2796":"code","a9f8feab":"code","198b2794":"code","badb70d3":"code","34193ce8":"code","f9b2c41e":"code","60bb3e1e":"code","0d1beb0d":"code","cc30d47c":"code","774ff3dc":"code","d9ecdb8c":"code","820d6902":"code","44cb6e47":"code","01428704":"code","af7e3e9e":"code","e13cb0b6":"code","bff6f24d":"code","a04b8e22":"code","5dd3c214":"code","3f949dc9":"code","f88434fb":"code","2513106f":"markdown","78964c1f":"markdown","b4febb1f":"markdown","05a20661":"markdown","9db74507":"markdown","d1e3ce76":"markdown","fc6f146b":"markdown","aa63b272":"markdown","d6536ba8":"markdown","9dbe09f5":"markdown","60c798f2":"markdown","8cef375f":"markdown","25db50f4":"markdown","33744a4c":"markdown","ba7f871c":"markdown","86149e77":"markdown"},"source":{"cbef069b":"from math import ceil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.palplot(sns.husl_palette(10))\nfrom datetime import datetime, date\nfrom dateutil.relativedelta import relativedelta\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom math import ceil\nfrom keras.callbacks import LambdaCallback\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\n\n%matplotlib inline\n\n#datalar\u0131 okuma\n\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsample = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')","b8558d3c":"def bilgi(data):\n    print(\"-----\u0130lk 5 Kay\u0131t-----\")\n    print(data.head(5))\n    print(\"-----Bilgiler-----\")\n    print(data.info())\n    print(\"-----Data T\u00fcrleri-----\")\n    print(data.dtypes)\n    print(\"-----Eksik De\u011ferler-----\")\n    print(data.isnull().sum())\n    print(\"-----Null De\u011ferler-----\")\n    print(data.isna().sum())\n    ","0763e3a6":"bilgi(train)\n","897793f0":"bilgi(test)","9eb52bd8":"bilgi(sample)","f00730e6":"bilgi(items)","8736c721":"bilgi(item_cats)","82e469c0":"bilgi(shops)","9e05721c":"#outliers\ntrain = train[train.item_price <= 100000]\ntrain = train[train.item_cnt_day <= 1000]\n\n\nmedian = train[(train.shop_id == 32) & (train.item_id == 2973) & (train.date_block_num == 4) & (train.item_price > 0)].item_price.median()\ntrain.loc[train.item_price < 0, 'item_price'] = median\n\ntest_shops = test.shop_id.unique()\ntrain = train[train.shop_id.isin(test_shops)]\ntest_items = test.item_id.unique()\ntrain = train[train.item_id.isin(test_items)]\ntrain.head()","fdf8d24f":"\ndef drop_duplicate(data, subset):\n    print('Before drop shape:', data.shape)\n    before = data.shape[0]\n    data.drop_duplicates(subset,keep='first', inplace=True) \n    data.reset_index(drop=True, inplace=True)\n    print('After drop shape:', data.shape)\n    after = data.shape[0]\n    print('Total Duplicate:', before-after)","5f020d86":"subset = ['date', 'date_block_num', 'shop_id', 'item_id','item_cnt_day']\ndrop_duplicate(train, subset = subset)","732ab64d":"sns.scatterplot(train['item_cnt_day'],train['item_price'])","4387f65f":"item_cats.head()","e7a3d30c":"def r(x):\n    if 'PC' in x:\n        return 'PC'\n    elif 'Live!' in x:\n        return 'Live!'\n    elif 'CD' in x:\n        return 'CD'\n    elif 'PS2' in x:\n        return 'PS2'\n    elif 'PSVita' in x:\n        return 'PSVita'\n    elif 'Windows' in x:\n        return 'Windows'\n    elif 'DVD' in x:\n        return 'DVD'\n    elif 'MP3' in x:\n        return 'MP3'\n    elif 'PS3' in x:\n        return 'PS3'\n    elif 'PSP' in x:\n        return 'PSP'\n    elif 'PS4' in x:\n        return 'PS4'\n    elif 'PSVita' in x:\n        return 'PSVita'\n    elif 'XBOX 360' in x:\n        return 'XBOX 360'\n    elif 'XBOX ONE' in x:\n        return 'XBOX ONE'\n    elif 'Blu-Ray 3D' in x:\n        return 'Blu-Ray 3D'\n    elif 'Blu-Ray 4K' in x:\n        return 'Blu-Ray 4K'\n    \n    else:\n        return 'Others'\n    \n\n    \nitem_cats['item_category_name']=item_cats['item_category_name'].apply(r)","0b262fe0":"plt.figure(figsize=(15,10))\nsns.countplot(item_cats['item_category_name'])","9a3a1d3a":"\nax = sns.distplot(train.groupby('date_block_num').sum()['item_cnt_day'], color=\"r\")\nplt.show()\n\n\n","53ebad68":"train2=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\",index_col='date',parse_dates=['date'])\ntrain2 = train2[train2.shop_id.isin(test_shops)]\ntest_items = test.item_id.unique()\ntrain2 = train2[train2.item_id.isin(test_items)]\ntrain2 = train2[train2.item_price <= 100000]\ntrain2 = train2[train2.item_cnt_day <= 1000]\nmedian = train2[(train2.shop_id == 32) & (train2.item_id == 2973) & (train2.date_block_num == 4) & (train2.item_price > 0)].item_price.median()\ntrain2.loc[train2.item_price < 0, 'item_price'] = median\n\n\ntrain2[\"item_price\"][:'2014-01-01'].plot(figsize=(16,10),legend=True,color='r')\ntrain2[\"item_price\"]['2014-01-01':'2015-01-01'].plot(figsize=(16,10),legend=True,color='b')\ntrain2[\"item_price\"]['2015-01-01':].plot(figsize=(16,10),legend=True,color='g')\nplt.xlabel(\"Dates\")\nplt.ylabel(\"Item price rise\")\nplt.title(\"Item pices vs Date\")","7de83d20":"from catboost import CatBoostRegressor\nfrom catboost import Pool\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport gc\nimport itertools\nimport xgboost\nfrom xgboost import plot_importance\nfrom xgboost import XGBRegressor\n\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n","50d4d4e6":"#https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n  \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col]#.astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","4497b3d7":"shops = reduce_mem_usage(shops)","df5f1da1":"shops.head()","11f4e30b":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nitem_cats['split'] = item_cats['item_category_name'].str.split('-')\nitem_cats['type'] = item_cats['split'].map(lambda x: x[0].strip())\nitem_cats['type_code'] = LabelEncoder().fit_transform(item_cats['type'])\n\nitem_cats['subtype'] = item_cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_cats['subtype_code'] = LabelEncoder().fit_transform(item_cats['subtype'])\nitem_cats = item_cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","9bc3d967":"%%time\nfull_data = []\nfor date_block_num in range(34):\n    all_shop_id = train2[train2.date_block_num==date_block_num].shop_id.unique()\n    all_item_id = train2[train2.date_block_num==date_block_num].item_id.unique()\n    full_data.append(np.array(list(itertools.product([date_block_num], all_shop_id, all_item_id))))\n\nfull_data = np.vstack(full_data)\nfull_data = pd.DataFrame(full_data, columns=[\"date_block_num\", \"shop_id\", \"item_id\"])\nfull_data.fillna(0, inplace=True)\nfull_data.sort_values([\"date_block_num\", \"shop_id\", \"item_id\"])","8dac7b0d":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","c500ac05":"full_data = reduce_mem_usage(full_data)","4f41d47f":"group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nfull_data = pd.merge(full_data, group, on=['date_block_num','shop_id','item_id'], how='left')\nfull_data['item_cnt_month'] = (full_data['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20)\n                                .astype(np.float16))","eca97b6c":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","09897f73":"full_data = pd.concat([full_data, test], ignore_index=True, sort=False, keys=['date_block_num','shop_id','item_id'])\nfull_data.fillna(0, inplace=True)","4cf0813a":"full_data.columns, item_cats.columns","7f096ff1":"full_data = pd.merge(full_data, shops, on=['shop_id'], how='left')\nfull_data = pd.merge(full_data, items, on=['item_id'], how='left')\nfull_data = pd.merge(full_data, item_cats, on=['item_category_id'], how='left')\nfull_data['city_code'] = full_data['city_code'].astype(np.int8)\nfull_data['item_category_id'] = full_data['item_category_id'].astype(np.int8)\nfull_data['type_code'] = full_data['type_code'].astype(np.int8)\nfull_data['subtype_code'] = full_data['subtype_code'].astype(np.int8)","43172e57":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","72848b89":"%%time\nfull_data = lag_feature(full_data, [1,2,3,6,12], 'item_cnt_month')","37496dd2":"%%time\ndate_item_group = full_data.groupby([\"date_block_num\", \"item_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ndate_item_group.columns = [\"date_item_avg_item_cnt\"]\ndate_item_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_item_group, on=[\"date_block_num\", \"item_id\"], how=\"left\")\nfull_data = lag_feature(full_data, [1, 2, 3, 6, 12], \"date_item_avg_item_cnt\")\nfull_data.drop([\"date_item_avg_item_cnt\"], axis=1, inplace=True)\n\ndate_item_group = full_data.groupby([\"date_block_num\", \"shop_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ndate_item_group.columns = [\"date_shop_avg_item_cnt\"]\ndate_item_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_item_group, on=[\"date_block_num\", \"shop_id\"], how=\"left\")\nfull_data = lag_feature(full_data, [1, 2, 3, 6, 12], \"date_shop_avg_item_cnt\")\nfull_data.drop([\"date_shop_avg_item_cnt\"], axis=1, inplace=True)\n\ndate_item_group = full_data.groupby([\"date_block_num\", \"item_id\", \"shop_id\"]).agg({\"item_cnt_month\": [\"mean\"]})\ndate_item_group.columns = [\"date_item_shop_avg_item_cnt\"]\ndate_item_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_item_group, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\")\nfull_data = lag_feature(full_data, [1, 2, 3], \"date_item_shop_avg_item_cnt\")\nfull_data.drop([\"date_item_shop_avg_item_cnt\"], axis=1, inplace=True)\n\ndate_shop_cat_group = full_data.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ndate_shop_cat_group.columns = ['date_shop_cat_avg_item_cnt']\ndate_shop_cat_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_shop_cat_group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nfull_data = lag_feature(full_data, [1], 'date_shop_cat_avg_item_cnt')\nfull_data.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\n\nitem_group = train.groupby([\"item_id\"]).agg({\"item_price\": [\"mean\"]})\nitem_group.columns = [\"item_avg_item_price\"]\nitem_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, item_group, on=[\"item_id\"], how=\"left\")\n\ndate_item_group = train.groupby([\"date_block_num\", \"item_id\"]).agg({\"item_price\": [\"mean\"]})\ndate_item_group.columns = [\"date_item_avg_item_price\"]\ndate_item_group.reset_index(inplace=True)\nfull_data = pd.merge(full_data, date_item_group, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n\nlags = [1,2,3,4,5,6]\nfull_data = lag_feature(full_data, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    full_data['delta_price_lag_'+str(i)] = \\\n        (full_data['date_item_avg_item_price_lag_'+str(i)] - full_data['item_avg_item_price']) \/ full_data['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nfull_data['delta_price_lag'] = full_data.apply(select_trend, axis=1)\nfull_data['delta_price_lag'] = full_data['delta_price_lag'].astype(np.float16)\nfull_data['delta_price_lag'].fillna(0, inplace=True)","1ac27ccb":"group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nfull_data = pd.merge(full_data, group, on=['date_block_num','shop_id'], how='left')\nfull_data['date_shop_revenue'] = full_data['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nfull_data = pd.merge(full_data, group, on=['shop_id'], how='left')\nfull_data['shop_avg_revenue'] = full_data['shop_avg_revenue'].astype(np.float32)\n\nfull_data['delta_revenue'] = (full_data['date_shop_revenue'] - full_data['shop_avg_revenue']) \/ full_data['shop_avg_revenue']\nfull_data['delta_revenue'] = full_data['delta_revenue'].astype(np.float16)\n\nfull_data = lag_feature(full_data, [1], 'delta_revenue')\n\nfull_data.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","144b2796":"full_data['month'] = full_data['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nfull_data['days'] = full_data['month'].map(days).astype(np.int8)","a9f8feab":"full_data = full_data[full_data.date_block_num > 11]","198b2794":"%%time\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nfull_data = fill_na(full_data)","badb70d3":"full_data.drop([\"ID\"], axis=1, inplace=True)","34193ce8":"cat_col = [\"date_block_num\", \"shop_id\", \"item_id\", \"item_category_id\", \"month\", \"days\"]\nfull_data[cat_col] = full_data[cat_col].astype(np.int32)","f9b2c41e":"X_train = full_data[full_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = full_data[full_data.date_block_num < 33]['item_cnt_month']\nX_valid = full_data[full_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_valid = full_data[full_data.date_block_num == 33]['item_cnt_month']\nX_test = full_data[full_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","60bb3e1e":"X_test.describe().T","0d1beb0d":"X_train.drop([\"delta_price_lag_1\", \"delta_price_lag_2\", \"delta_price_lag_3\", \"delta_price_lag_4\", \"delta_price_lag_5\", \"delta_price_lag_6\"], axis=1, inplace=True)\nX_valid.drop([\"delta_price_lag_1\", \"delta_price_lag_2\", \"delta_price_lag_3\", \"delta_price_lag_4\", \"delta_price_lag_5\", \"delta_price_lag_6\"], axis=1, inplace=True)\nX_test.drop([\"delta_price_lag_1\", \"delta_price_lag_2\", \"delta_price_lag_3\", \"delta_price_lag_4\", \"delta_price_lag_5\", \"delta_price_lag_6\"], axis=1, inplace=True)","cc30d47c":"cat_feats = [i for i, c in enumerate(X_train.columns) if c in cat_col]","774ff3dc":"del full_data\n\ngc.collect()","d9ecdb8c":"X_train.to_csv(\"X_train.csv\", index=False)\nX_valid.to_csv(\"X_valid.csv\", index=False)\nX_test.to_csv(\"X_test.csv\", index=False)\n\ny_train.to_csv(\"y_train.csv\", index=False)\ny_valid.to_csv(\"y_valid.csv\", index=False)","820d6902":"%%time\nxgb_model = XGBRegressor(max_depth=10,\n                         n_estimators=1000, \n                         min_child_weight=1000,\n                         colsample_bytree=0.7, \n                         subsample=0.7,\n                         eta=0.3, \n                         seed=0)\nxgb_model.fit(X_train, \n              y_train, \n              eval_metric=\"rmse\", \n              eval_set=[(X_train, y_train), (X_valid, y_valid)], \n              verbose=20, \n              early_stopping_rounds=10)","44cb6e47":"plt.rcParams[\"figure.figsize\"] = (10, 14)\nplot_importance(xgb_model)\nplt.show()","01428704":"%%time\n\nxgb_train_pred = xgb_model.predict(X_train)\nxgb_val_pred = xgb_model.predict(X_valid)\nxgb_test_pred = xgb_model.predict(X_test)\n\nprint('Train rmse:', np.sqrt(mean_squared_error(y_train, xgb_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(y_valid, xgb_val_pred)))","af7e3e9e":"xgb_model.save_model(\"xgboost\")\ndel xgb_model, xgb_train_pred\ngc.collect()","e13cb0b6":"submission = pd.DataFrame(test['ID'], columns=['ID'])\nsubmission['item_cnt_month'] = xgb_test_pred.clip(0., 20.)\nsubmission.to_csv('submission_xgb.csv', index=False)\nsubmission.head(5)","bff6f24d":"%%time\nmodel=LGBMRegressor(\n        n_estimators=200,\n        learning_rate=0.03,\n        num_leaves=42,\n        colsample_bytree=0.95,\n        subsample=0.8,\n        max_depth=9,\n        reg_alpha=0.03,\n        reg_lambda=0.07,\n        min_split_gain=0.02,\n        min_child_weight=40,\n        silent=False)\n\n\nmodel.fit(X_train, y_train)","a04b8e22":"%%time\n\nlgb_train_pred = model.predict(X_train)\nlgb_val_pred = model.predict(X_valid)\nlgb_test_pred = model.predict(X_test)\n\nprint('Train rmse:', np.sqrt(mean_squared_error(y_train, lgb_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(y_valid, lgb_val_pred)))","5dd3c214":"model.booster_.save_model('lgb_classifier.txt') \ndel model, lgb_train_pred\ngc.collect()","3f949dc9":"final_test_preds = 0.5 * lgb_test_pred + 0.5 * xgb_test_pred \nfinal_test_preds = final_test_preds.clip(0., 20.)","f88434fb":"submission = pd.DataFrame(test['ID'], columns=['ID'])\nsubmission['item_cnt_month'] = final_test_preds\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(5)","2513106f":"Veri setinde biraz d\u00fczenleme yapmak i\u00e7in a\u015fa\u011f\u0131daki kodlar\u0131 \u00e7al\u0131\u015ft\u0131rd\u0131m.","78964c1f":"Amac\u0131m\u0131z ayl\u0131k sat\u0131\u015f tahmini oldu\u011fundan dolay\u0131 her ay ki sat\u0131\u015flar\u0131 g\u00f6rmek i\u00e7in bir g\u00f6rselle\u015ftirme ad\u0131m\u0131 ger\u00e7ekle\u015ftirilmelidir.","b4febb1f":"# B\u00fcy\u00fck Veri Final\n\n## Kaggle \u00dczerinde Proje Geli\u015ftirme\n\nMerve TAFRALI - 160202100","05a20661":"Verileri fonksiyon tan\u0131mlayarak inceledim","9db74507":"LightGBM","d1e3ce76":"Kategoriler rus\u00e7a oldu\u011fundan daha kolay ve daha rahat g\u00f6rselle\u015ftirmek i\u00e7in uygulanan ad\u0131mlar,","fc6f146b":"Bu fonksiyon https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage adresinde vard\u0131r","aa63b272":"\u0130kiliyen datalar\u0131n \u00e7\u0131akr\u0131lmas\u0131 i\u00e7in basit bir fonksiyon","d6536ba8":"Veri setinde d\u00fczenlemer","9dbe09f5":"Date tipini de\u011fi\u015ftirmemiz gerekiyor, yukar\u0131da yapt\u0131\u011f\u0131m\u0131z analizleri tekrar uyguluyorum","60c798f2":"XGBoost","8cef375f":"# Makine \u00d6\u011frenmesinin Uygulanmas\u0131","25db50f4":"\u00dc\u00e7 t\u00fcr makine \u00f6\u011frenimi vard\u0131r\n\n1. Denetimli Makine \u00d6\u011frenimi\n2. Denetimsiz Makine \u00d6\u011frenimi\n3. Takviye Makinesi \u00d6\u011frenimi\n\n**Denetimli Makine \u00d6\u011frenimi**\n\nHem girdi hem de istenen \u00e7\u0131kt\u0131 verilerinin sa\u011fland\u0131\u011f\u0131 bir \u00f6\u011frenme t\u00fcr\u00fcd\u00fcr. Girdi ve \u00e7\u0131kt\u0131 verileri, gelecekteki veri i\u015fleme i\u00e7in bir \u00f6\u011frenme temeli sa\u011flamak \u00fczere s\u0131n\u0131fland\u0131rma i\u00e7in etiketlenir. Bu algoritma, belirli bir yorday\u0131c\u0131 k\u00fcmesinden (ba\u011f\u0131ms\u0131z de\u011fi\u015fkenler) tahmin edilmesi gereken bir hedef \/ sonu\u00e7 de\u011fi\u015fkeninden (veya ba\u011f\u0131ml\u0131 de\u011fi\u015fken) olu\u015fur. Bu de\u011fi\u015fkenler setini kullanarak, giri\u015fleri istenen \u00e7\u0131k\u0131\u015flarla e\u015fle\u015ftiren bir fonksiyon \u00fcretiyoruz. E\u011fitim s\u00fcreci, model e\u011fitim verileri \u00fczerinde istenen bir do\u011fruluk seviyesine ula\u015fana kadar devam eder.\n\n**Denetimsiz Makine \u00d6\u011frenimi**\n\nG\u00f6zetimsiz \u00f6\u011frenme, ne s\u0131n\u0131fland\u0131r\u0131lmam\u0131\u015f ne de etiketlenmemi\u015f bilgileri kullanan ve algoritman\u0131n rehberlik olmadan bu bilgiler \u00fczerinde hareket etmesine izin veren bir e\u011fitimdir. Denetimsiz \u00f6\u011frenmenin arkas\u0131ndaki ana fikir, makineleri b\u00fcy\u00fck miktarlarda \u00e7e\u015fitli verilere maruz b\u0131rakmak ve \u00f6\u011frenmesine izin vermektir. ve verilerden \u00e7\u0131kar. Ancak, makineler \u00f6ncelikle verilerden \u00f6\u011frenilecek \u015fekilde programlanmal\u0131d\u0131r.\n\nG\u00f6zetimsiz \u00f6\u011frenme sorunlar\u0131 ayr\u0131ca k\u00fcmeleme ve ili\u015fkilendirme sorunlar\u0131 olarak grupland\u0131r\u0131labilir.\n\n**Takviye Makinesi \u00d6\u011frenimi - Semi-Supervised**\n\nVerisetinin bir k\u0131sm\u0131n\u0131 denetimle bir k\u0131sm\u0131n\u0131 da denetimsiz \u00f6\u011frenme yard\u0131m\u0131yla e\u011fiterek ger\u00e7ekle\u015ftirilir.\n","33744a4c":"Bu projedeki amac\u0131m veri setinde olan verilerden yararlanarak bir sonraki ay\u0131n sat\u0131\u015f tahminlerini ger\u00e7ekle\u015ftirmektir. \u00d6ncelikle versetini tan\u0131yabilmek ad\u0131na veri g\u00f6rselle\u015ftirme ad\u0131mlar\u0131n\u0131 kullanaca\u011f\u0131m.","ba7f871c":"Veri setinin olu\u015fturulmas\u0131","86149e77":"### Veri G\u00f6rselle\u015ftirme"}}