{"cell_type":{"73eb5c7c":"code","4e0ba4f8":"code","1f1c8775":"code","1354044a":"code","14b0bd7f":"code","815e293b":"code","9a5e06ce":"code","dffca98f":"code","6bfed9fd":"code","0910ebf1":"code","be8be888":"code","8f87f07a":"code","b6019119":"code","6372254d":"code","4ba05ca6":"code","d9b98b0c":"code","bd781aae":"code","f8d85d5a":"code","d1244864":"code","5eeafe9b":"code","b7c829a4":"code","2d69a9e2":"code","ea8a1f8e":"code","638dbbd4":"code","9e1e766a":"code","3439ca51":"code","37b13a33":"code","4ea0064c":"code","142dde61":"code","92c0c9cc":"code","6406f7ef":"code","871e12b5":"code","2783b836":"code","109bab48":"code","805a4832":"code","1fa67029":"markdown","129caa33":"markdown","a950ba29":"markdown","33127538":"markdown","092aa579":"markdown","cbbc56f9":"markdown","2beccb2b":"markdown","fd978c69":"markdown","dcc1a6c1":"markdown","a836a17d":"markdown","41845004":"markdown","1341c2df":"markdown","ff500dac":"markdown","1b8d972e":"markdown"},"source":{"73eb5c7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import pairwise_distances\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nfrom scipy.stats import multivariate_normal as mvn\nimport nltk\nimport os\nimport random\n\n\n\n\nimport string\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# email module has some useful functions\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\n\nimport os, sys, email,re\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4e0ba4f8":"df = pd.read_csv('..\/input\/emails.csv',nrows = 35000)\ndf.shape","1f1c8775":"# create list of email objects\nemails = list(map(email.parser.Parser().parsestr,df['message']))\n\n# extract headings such as subject, from, to etc..\nheadings  = emails[0].keys()\n\n# Goes through each email and grabs info for each key\n# doc['From'] grabs who sent email in all emails\nfor key in headings:\n    df[key] = [doc[key] for doc in emails]","1354044a":"##Useful functions\ndef get_raw_text(emails):\n    email_text = []\n    for email in emails.walk():\n        if email.get_content_type() == 'text\/plain':\n            email_text.append(email.get_payload())\n    return ''.join(email_text)\n\ndf['body'] = list(map(get_raw_text, emails))\ndf.head()\ndf['user'] = df['file'].map(lambda x: x.split('\/')[0])","14b0bd7f":"df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\ndf.head()\ndf.dtypes","815e293b":"df['Month'] = df['Date'].dt.month\ndf['Year'] = df['Date'].dt.year\ndf['Day'] = df['Date'].dt.dayofweek\n\n# reduce the sample period\n# looks like the total number of emails really ramped up in 2000 and 2001\nindices = (df['Year'] > 1995) & (df['Year'] <= 2004)\nplt.figure(figsize = (10,6))\nfigure1 = df.loc[indices].groupby('Year')['body'].count().plot()","9a5e06ce":"df.Year.agg({'max': max, 'min': min})\n# really should not be dates up to 2044\ndf[df['Year']==2044]","dffca98f":"plt.figure(figsize = (10,6))\nfigure2 = df.groupby('Month')['body'].count().plot()","6bfed9fd":"plt.figure(figsize = (10,6))\nfigure3 = df.groupby('Day')['body'].count().plot()","0910ebf1":"#Unique to and From\nprint('Total number of emails: %d' %len(df))\nprint('------------')\nprint('Number of unique received: %d '%df['To'].nunique())\nprint('------------')\nprint('Number of unique Sent: %d '%df['From'].nunique())","be8be888":"top_10_frequent = df.groupby('user')['file'].count().sort_values(ascending = False)[:30]\ntop_10_frequent","8f87f07a":"plt.figure(figsize = (10,6))\ntop_10_frequent.plot(kind = 'bar')","b6019119":"df.groupby(['user', 'Year'])['file'].count()","6372254d":"def split_data(data):\n    if data is not None:\n        temp = data.split(',')\n        if len(temp) == 1:\n            return 'Direct'\n        else:\n            return 'Multiple'\n    else:\n        return 'Empty'\ndf['Direct_or_multi'] = df['To'].apply(split_data)","4ba05ca6":"df.groupby('user')['Direct_or_multi'].value_counts().sort_values(ascending=False)[:15]","d9b98b0c":"def clean_column(data):\n    if data is not None:\n        stopwords_list = stopwords.words('english')\n        #exclusions = ['RE:', 'Re:', 're:']\n        #exclusions = '|'.join(exclusions)\n        data =  data.lower()\n        data = re.sub('re:', '', data)\n        data = re.sub('-', '', data)\n        data = re.sub('_', '', data)\n        # Remove data between square brackets\n        data =re.sub('\\[[^]]*\\]', '', data)\n        # removes punctuation\n        data = re.sub(r'[^\\w\\s]','',data)\n        data = re.sub(r'\\n',' ',data)\n        data = re.sub(r'[0-9]+','',data)\n        # strip html \n        p = re.compile(r'<.*?>')\n        data = re.sub(r\"\\'ve\", \" have \", data)\n        data = re.sub(r\"can't\", \"cannot \", data)\n        data = re.sub(r\"n't\", \" not \", data)\n        data = re.sub(r\"I'm\", \"I am\", data)\n        data = re.sub(r\" m \", \" am \", data)\n        data = re.sub(r\"\\'re\", \" are \", data)\n        data = re.sub(r\"\\'d\", \" would \", data)\n        data = re.sub(r\"\\'ll\", \" will \", data)\n        data = re.sub('forwarded by phillip k allenhouect on    pm', '',data)\n        data = re.sub(r\"httpitcappscorpenroncomsrrsauthemaillinkaspidpage\", \"\", data)\n        \n        data = p.sub('', data)\n        if 'forwarded by:' in data:\n            data = data.split('subject')[1]\n        data = data.strip()\n        return data\n    return 'No Subject'\n\n\ndf['Subject_new'] = df['Subject'].apply(clean_column)\ndf['body_new'] = df['body'].apply(clean_column)","bd781aae":"df['body_new'].head(5)","f8d85d5a":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\nto_add = ['FW', 'ga', 'httpitcappscorpenroncomsrrsauthemaillinkaspidpage', 'cc', 'aa', 'aaa', 'aaaa',\n         'hou', 'cc', 'etc', 'subject', 'pm']\n\nfor i in to_add:\n    stopwords.add(i)","d1244864":"wordcloud = WordCloud(\n                          collocations = False,\n                          width=1600, height=800,\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=150,\n                          #max_font_size=40, \n                          random_state=42\n                         ).generate(' '.join(df['Subject_new'])) # can't pass a series, needs to be strings and function computes frequencies\nprint(wordcloud)\nplt.figure(figsize=(9,8))\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","5eeafe9b":"stemmer = PorterStemmer()\ndef stemming_tokenizer(str_input):\n    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n    words = [porter_stemmer.stem(word) for word in words]\n    return words","b7c829a4":"def tokenize_and_stem(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    return stems","2d69a9e2":"from sklearn.feature_extraction.text import TfidfVectorizer\ndata = df['body_new']\n# data.head()\n\ntf_idf_vectorizor = TfidfVectorizer(stop_words = stopwords,#tokenizer = tokenize_and_stem,\n                             max_features = 5000)\n%time tf_idf = tf_idf_vectorizor.fit_transform(data)\ntf_idf_norm = normalize(tf_idf)\ntf_idf_array = tf_idf_norm.toarray()\npd.DataFrame(tf_idf_array, columns=tf_idf_vectorizor.get_feature_names()).head()","ea8a1f8e":"# initial_centroids = np.random.permutation(tf_idf_array.shape[0])[:3]\n# initial_centroids\n# centroids = tf_idf_array[initial_centroids]\n# centroids.shape\n# dist_to_centroid =  pairwise_distances(tf_idf_array,centroids, metric = 'euclidean')\n# cluster_labels = np.argmin(dist_to_centroid, axis = 1)","638dbbd4":"class Kmeans:\n    \"\"\" K Means Clustering\n    \n    Parameters\n    -----------\n        k: int , number of clusters\n        \n        seed: int, will be randomly set if None\n        \n        max_iter: int, number of iterations to run algorithm, default: 200\n        \n    Attributes\n    -----------\n       centroids: array, k, number_features\n       \n       cluster_labels: label for each data point\n       \n    \"\"\"\n    \n    def __init__(self, k, seed = None, max_iter = 200):\n        self.k = k\n        self.seed = seed\n        if self.seed is not None:\n            np.random.seed(self.seed)\n        self.max_iter = max_iter\n        \n            \n    \n    def initialise_centroids(self, data):\n        \"\"\"Randomly Initialise Centroids\n        \n        Parameters\n        ----------\n        data: array or matrix, number_rows, number_features\n        \n        Returns\n        --------\n        centroids: array of k centroids chosen as random data points \n        \"\"\"\n        \n        initial_centroids = np.random.permutation(data.shape[0])[:self.k]\n        self.centroids = data[initial_centroids]\n\n        return self.centroids\n    \n    \n    def assign_clusters(self, data):\n        \"\"\"Compute distance of data from clusters and assign data point\n           to closest cluster.\n        \n        Parameters\n        ----------\n        data: array or matrix, number_rows, number_features\n        \n        Returns\n        --------\n        cluster_labels: index which minmises the distance of data to each\n        cluster\n            \n        \"\"\"\n        \n        if data.ndim == 1:\n            data = data.reshape(-1, 1)\n        \n        dist_to_centroid =  pairwise_distances(data, self.centroids, metric = 'euclidean')\n        self.cluster_labels = np.argmin(dist_to_centroid, axis = 1)\n        \n        return  self.cluster_labels\n    \n    \n    def update_centroids(self, data):\n        \"\"\"Computes average of all data points in cluster and\n           assigns new centroids as average of data points\n        \n        Parameters\n        -----------\n        data: array or matrix, number_rows, number_features\n        \n        Returns\n        -----------\n        centroids: array, k, number_features\n        \"\"\"\n        \n        self.centroids = np.array([data[self.cluster_labels == i].mean(axis = 0) for i in range(self.k)])\n        \n        return self.centroids\n    \n    \n    def convergence_calculation(self):\n        \"\"\"\n        Calculates \n        \n        \"\"\"\n        pass\n    \n    def predict(self, data):\n        \"\"\"Predict which cluster data point belongs to\n        \n        Parameters\n        ----------\n        data: array or matrix, number_rows, number_features\n        \n        Returns\n        --------\n        cluster_labels: index which minmises the distance of data to each\n        cluster\n        \"\"\"\n        \n        return self.assign_clusters(data)\n    \n    def fit_kmeans(self, data):\n        \"\"\"\n        This function contains the main loop to fit the algorithm\n        Implements initialise centroids and update_centroids\n        according to max_iter\n        -----------------------\n        \n        Returns\n        -------\n        instance of kmeans class\n            \n        \"\"\"\n        self.centroids = self.initialise_centroids(data)\n        \n        # Main kmeans loop\n        for iter in range(self.max_iter):\n\n            self.cluster_labels = self.assign_clusters(data)\n            self.centroids = self.update_centroids(data)          \n            if iter % 100 == 0:\n                print(\"Running Model Iteration %d \" %iter)\n        print(\"Model finished running\")\n        return self    ","9e1e766a":"from sklearn.datasets import make_blobs\n# create blobs\ndata = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.6, random_state=50)\n# create np array for data points\npoints = data[0]\n# create scatter plot\nplt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')\nplt.xlim(-15,15)\nplt.ylim(-15,15)\n\nX = data[0]\nX[2]","3439ca51":"temp_k  = Kmeans(4, 1, 600)\ntemp_fitted  = temp_k.fit_kmeans(X)\nnew_data = np.array([[1.066, -8.66],\n                    [1.87876, -6.516],\n                    [-1.59728965,  8.45369045],\n                    [1.87876, -6.516]])\ntemp_fitted.predict(new_data)","37b13a33":"sklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(tf_idf_array)\ntest_e = Kmeans(3, 1, 600)\n%time fitted = test_e.fit_kmeans(Y_sklearn)\npredicted_values = test_e.predict(Y_sklearn)\n\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=predicted_values, s=50, cmap='viridis')\n\ncenters = fitted.centroids\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);","4ea0064c":"from sklearn.cluster import KMeans\nn_clusters = 3\nsklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(tf_idf_array)\nkmeans = KMeans(n_clusters= n_clusters, max_iter=600, algorithm = 'auto')\n%time fitted = kmeans.fit(Y_sklearn)\nprediction = kmeans.predict(Y_sklearn)\n\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction ,s=50, cmap='viridis')\n\ncenters2 = fitted.cluster_centers_\nplt.scatter(centers2[:, 0], centers2[:, 1],c='black', s=300, alpha=0.6);","142dde61":"number_clusters = range(1, 7)\n\nkmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters]\nkmeans\n\nscore = [kmeans[i].fit(Y_sklearn).score(Y_sklearn) for i in range(len(kmeans))]\nscore = [i*-1 for i in score]\n\nplt.plot(number_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Method')\nplt.show()","92c0c9cc":"def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n    labels = np.unique(prediction)\n    dfs = []\n    for label in labels:\n        id_temp = np.where(prediction==label) # indices for each cluster\n        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n        features = tf_idf_vectorizor.get_feature_names()\n        best_features = [(features[i], x_means[i]) for i in sorted_means]\n        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n        dfs.append(df)\n    return dfs\ndfs = get_top_features_cluster(tf_idf_array, prediction, 20)","6406f7ef":"import seaborn as sns\nplt.figure(figsize=(8,6))\nsns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[0][:15])","871e12b5":"plt.figure(figsize=(8,6))\nsns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[1][:15])","2783b836":"plt.figure(figsize=(8,6))\nsns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[2][:15])","109bab48":"for i, df in enumerate(dfs):\n    df.to_csv('df_'+str(i)+'.csv')","805a4832":"def plot_features(dfs):\n    fig = plt.figure(figsize=(14,12))\n    x = np.arange(len(dfs[0]))\n    for i, df in enumerate(dfs):\n        ax = fig.add_subplot(1, len(dfs), i+1)\n        ax.set_title(\"Cluster: \"+ str(i), fontsize = 14)\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        ax.set_frame_on(False)\n        ax.get_xaxis().tick_bottom()\n        ax.get_yaxis().tick_left()\n        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n        ax.barh(x, df.score, align='center', color='#40826d')\n        yticks = ax.set_yticklabels(df.features)\n    plt.show();\nplot_features(dfs)","1fa67029":"# Optimal Clusters","129caa33":"# My Implementation","a950ba29":"## TF-IDF tranformation for K-means algorithm","33127538":"### Most frequent Senders and receivers of Emails","092aa579":"## Check whether emails were to a single person or multiple people","cbbc56f9":"## Users and number of emails they sent by year.\n- huge increase in quantity of emails in the early 2000's","2beccb2b":"# Main Analysis Starts Here\n## Use the Email module to extract raw text","fd978c69":"## test pairwise function","dcc1a6c1":"## Clean the subject columns","a836a17d":"## Top 10 most frequent emailers\n- Interestingly all kaminski's emails were sent directly to people\n- This could warrent a closer look: who were the ppl he was emailing.","41845004":"## Visualise Email Subject","1341c2df":"# Extracting top features","ff500dac":"# Kmeans Class ","1b8d972e":"# SK learn Implementation"}}