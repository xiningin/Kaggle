{"cell_type":{"bf4d2c54":"code","a38e383d":"code","ae724bc7":"code","ab6ee9bb":"code","a5f8a441":"code","de9613bb":"code","2c06f370":"code","4528f3aa":"code","50a46d8f":"code","f454e809":"code","54d49647":"code","5db8256a":"code","4377b303":"code","ee7d3d5a":"code","356a260b":"code","767e47c6":"code","07a47442":"code","8b886039":"code","a5a40c77":"code","fee3d3e9":"code","ff79510d":"code","6e0cf165":"code","57d20991":"code","734e6b3f":"code","eb67d218":"code","0c39a1c7":"code","77737c91":"code","08fe9a6f":"code","6f4ae792":"code","e5279d3e":"code","af078f21":"code","7a900c36":"code","5bd14b95":"code","42e1e6d1":"code","ea628169":"code","a569b8b9":"code","8c370219":"code","e937ab72":"code","1b13a099":"code","f4d9c6ed":"code","5f0cf89d":"code","34180243":"code","0f72faaa":"markdown","86dc8a1c":"markdown","60ddc309":"markdown","dea15e87":"markdown","777bc36e":"markdown","2ded7336":"markdown","c1320682":"markdown","e9d4af38":"markdown","1b61a77b":"markdown","c3e1c4bf":"markdown","6835826d":"markdown","0f530aa3":"markdown"},"source":{"bf4d2c54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a38e383d":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","ae724bc7":"print(\"Training shape: \", train.shape)\nprint(\"Training info: \")\ntrain.info()\nprint(\"\\n-----------------------------------------\\n\")\nprint(\"Test shape: \", test.shape)\nprint(\"Test info: \")\ntest.info()","ab6ee9bb":"print(\"Check NaN values in Test set:\")\nisnull = test.isnull().sum().reset_index()\n#isnull[isnull>0]\nisnull.columns = ['Feature', 'Total_null']\ntotal_null = isnull[isnull['Total_null']>0]\ntotal_null","a5f8a441":"print(\"Check NaN values in Train set:\")\nisnull = train.isnull().sum().reset_index()\n#isnull[isnull>0]\nisnull.columns = ['Feature', 'Total_null']\ntotal_null = isnull[isnull['Total_null']>0]\ntotal_null","de9613bb":"traintest = pd.concat([train, test], axis=0, sort=False)","2c06f370":"traintest[traintest.duplicated()].index","4528f3aa":"pd.concat([traintest.nunique(dropna=False), traintest.count(), traintest.nunique()\/traintest.count()], axis=1)","50a46d8f":"train = train.replace('male', 0)\ntrain = train.replace('female', 1)\ntest = test.replace('male', 0)\ntest = test.replace('female', 1)","f454e809":"import seaborn as sns\nimport math\n\ncols = traintest.drop(columns=['Survived', 'Name', 'Ticket', 'Cabin', 'Embarked']).columns\nnrows = math.ceil(len(cols)\/3)\nfig, axs = plt.subplots(ncols=3, nrows=nrows)\nfig.set_size_inches(16, 7*nrows)\n\nindex = 0\nfor col in cols:\n    sns.kdeplot(train[col], ax=axs[math.floor(index\/3), index%3], label='Train_'+col)\n    sns.kdeplot(test[col], ax=axs[math.floor(index\/3), index%3], label='Test_'+col)\n    index += 1\n\n#sns.kdeplot(train.Age, ax=axs[1], label='Train_Age')\n#sns.kdeplot(test.Age, ax=axs[1],label='Test_Age')\n\n#sns.kdeplot(train.SibSp, ax=axs[2], label='Train_SibSp')\n#sns.kdeplot(test.SibSp, ax=axs[2], label='Test_SibSp')\n","54d49647":"# plot survival vs sex\ntemp = train[['Survived', 'Sex']].groupby('Survived').count().plot.bar()","5db8256a":"train['Age'].describe()","4377b303":"temp = train[['Survived', 'Age']].groupby('Age').count().plot(figsize=(16,7), kind='bar')\nplt.title('Number of people per Age group')","ee7d3d5a":"temp = train[['Survived', 'Age']].groupby('Age').sum().plot(figsize=(16,7), kind='bar')\nplt.title('Number of people survived per Age group')","356a260b":"temp = train[['Survived', 'Age']].groupby('Age').sum()\ntemp2 = train['Age'].value_counts()\n\nplt.figure(figsize=(16,7))\nplt.bar(temp.index, align='center', height=temp['Survived'].astype('float')\/temp2)\nplt.title('The probability of Survived over Age group')","767e47c6":"temp = train[['Survived', 'SibSp']].groupby('SibSp').sum().plot(kind='bar', figsize=(16,7))\nplt.title('Number of people survived per SibSp group')","07a47442":"#temp = train[['Survived', 'SibSp']].groupby('SibSp').sum().plot(kind='bar', figsize=(16,7))\n\ntemp = train[['Survived', 'SibSp']].groupby('SibSp').sum()\ntemp2 = train[['Survived', 'SibSp']].groupby('SibSp').count()\n\nplt.figure(figsize=(16,7))\nplt.bar(temp.index, align='center', height=temp['Survived'].astype('float')\/temp2['Survived'])\nplt.title('The probability of Survived over SibSp group')","8b886039":"temp2 = train[['Survived', 'Parch']].groupby('Parch').count().plot(figsize=(16,7), kind='bar')\nplt.title('Number of people per Parch group')","a5a40c77":"temp = train[['Survived', 'Parch']].groupby('Parch').sum().plot(kind='bar', figsize=(16,7))\nplt.title('Number of people survived per Parch group')","fee3d3e9":"#temp = train[['Survived', 'SibSp']].groupby('SibSp').sum().plot(kind='bar', figsize=(16,7))\n\ntemp = train[['Survived', 'Parch']].groupby('Parch').sum()\ntemp2 = train[['Survived', 'Parch']].groupby('Parch').count()\n\nplt.figure(figsize=(16,7))\nplt.bar(temp.index, align='center', height=temp['Survived'].astype('float')\/temp2['Survived'])\nplt.title('The probability of Survived over Parch group')","ff79510d":"train['SibSp_parch'] = train.SibSp + train.Parch\ntest['SibSp_parch'] = test.SibSp + train.Parch\nprint(train['SibSp_parch'].describe())\ntemp = train[['Survived', 'SibSp_parch']].groupby('SibSp_parch').sum()\ntemp2 = train[['Survived', 'SibSp_parch']].groupby('SibSp_parch').count()\n\nplt.figure(figsize=(16,7))\nplt.bar(temp.index, align='center', height=temp['Survived'].astype('float')\/temp2['Survived'])\nplt.title('The probability of Survived over family size')","6e0cf165":"temp2 = train[['Survived', 'SibSp_parch']].groupby('SibSp_parch').count().plot(figsize=(16,7), kind='bar')\nplt.title('Number of people per Family size')","57d20991":"# merge Train and Test dataframe to get one-hot encoding\n# add a column to indicate whether the row is in Train or Test set\ntrain['is_train'] = 1\ntrain['origin_index'] = train.index \ntest['is_train'] = 0\ntest['origin_index'] = test.index \n\ntraintest = pd.concat([train, test], axis=0, sort=False)","734e6b3f":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n\nnum_cols = ['Age', 'Fare', 'SibSp_parch']\n# one-hot coding categorical features\ncat_cols = ['Sex', 'Cabin', 'Embarked', 'Pclass', 'SibSp', 'Parch']\n\n# Generate new feature\ntrain['SibSp_parch'] = train.SibSp + train.Parch\ntest['SibSp_parch'] = test.SibSp + train.Parch\n\n# create a temp set\nscaled_train = train[num_cols+cat_cols].copy()\nscaled_test = test[num_cols+cat_cols].copy()\n\none_hot_data = pd.get_dummies(traintest[cat_cols+['is_train', 'origin_index']], columns=cat_cols)\ntrain_one_hot = one_hot_data[one_hot_data.is_train==1]\ntest_one_hot = one_hot_data[one_hot_data.is_train==0]\n\nscaled_train = scaled_train.merge(train_one_hot, left_index=True, right_on='origin_index')\nscaled_test = scaled_test.merge(test_one_hot, left_index=True, right_on='origin_index')\n\nscaled_train = scaled_train.drop(columns=cat_cols + ['origin_index', 'is_train'])\nscaled_test = scaled_test.drop(columns=cat_cols+ ['origin_index', 'is_train'])\n\n# replace all NaN value to -1\n#train.fillna(-1, inplace=True)\n#test.fillna(-1, inplace=True)\nscaled_train = scaled_train.fillna(scaled_train.mean())\nscaled_test = scaled_test.fillna(scaled_train.mean())\n\ny_train = train['Survived']\nX_train = scaled_train\n\nX_test = scaled_test\n\n# fit scaler\nX_train[num_cols] = min_max_scaler.fit_transform(X_train[num_cols])\n#min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\nX_test[num_cols] = min_max_scaler.fit_transform(X_test[num_cols])","eb67d218":"print(\"X_train.shape: \", X_train.shape)\nprint(\"y_train.shape: \", y_train.shape)\n\nprint(\"X_test.shape: \", X_test.shape)","0c39a1c7":"# split train\/test set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression","77737c91":"x_train, X_val, Y_train, Y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)","08fe9a6f":"model_svm_linear = SVC(kernel='linear', C=1).fit(x_train, Y_train)\nprint(\"Training score: \", model_svm_linear.score(x_train, Y_train))\nprint(\"Validation score: \", model_svm_linear.score(X_val, Y_val))","6f4ae792":"model_svm_sigmoid = SVC(kernel='sigmoid').fit(x_train, Y_train)\nprint(\"Training score: \", model_svm_sigmoid.score(x_train, Y_train))\nprint(\"Validation score: \", model_svm_sigmoid.score(X_val, Y_val))","e5279d3e":"model_svm_rbf = SVC(kernel='rbf').fit(x_train, Y_train)\nprint(\"Training score: \", model_svm_rbf.score(x_train, Y_train))\nprint(\"Validation score: \", model_svm_rbf.score(X_val, Y_val))","af078f21":"model_logistic = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(x_train, Y_train)\nprint(\"Training score: \", model_logistic.score(x_train, Y_train))\nprint(\"Validation score: \", model_logistic.score(X_val, Y_val))","7a900c36":"model = SVC(kernel='linear', C=1).fit(X_train, y_train)\nprint(\"Training score: \", model_svm_linear.score(X_train, y_train))\n\nclass_predict = model.predict(X_test)","5bd14b95":"test_ID = test['PassengerId']","42e1e6d1":"temp = {'PassengerID': test_ID, 'Survived': class_predict}\nresult = pd.DataFrame(temp)","ea628169":"result.to_csv('result.csv', index=False)","a569b8b9":"# import library\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.callbacks import EarlyStopping\nfrom sklearn import preprocessing","8c370219":"model_neuron = Sequential() \n#model_neuron.add(Dense(output_dim=512, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=256, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=128, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=64, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dropout(0.5))\nmodel_neuron.add(Dense(output_dim=32, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=16, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=1, input_shape=(X_train.shape[1],), activation='sigmoid'))\nmodel_neuron.compile(loss='mse', optimizer=Adam(lr=1e-5), metrics=['accuracy'])\nmodel_neuron.summary()","e937ab72":"history = model_neuron.fit(X_train, y_train, nb_epoch=10000, validation_split=0.2, callbacks=[EarlyStopping(patience=10)])","1b13a099":"import matplotlib.pyplot as plt\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","f4d9c6ed":"model_neuron = Sequential() \n#model_neuron.add(Dense(output_dim=512, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=256, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=128, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=64, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=32, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=16, input_shape=(X_train.shape[1],), activation='relu'))\nmodel_neuron.add(Dense(output_dim=1, input_shape=(X_train.shape[1],), activation='sigmoid'))\nmodel_neuron.compile(loss='mse', optimizer=Adam(lr=1e-5), metrics=['accuracy'])\nmodel_neuron.summary()\n\nhistory = model_neuron.fit(X_train, y_train, nb_epoch=200, callbacks=[EarlyStopping(patience=10)])","5f0cf89d":"class_predict = model_neuron.predict_classes(X_test)\nclass_predict = class_predict.reshape((class_predict.shape[0],))\ntest_ID = test['PassengerId']\ntemp = {'PassengerID': test_ID, 'Survived': class_predict}\nresult = pd.DataFrame(temp)\nresult.to_csv('result_neuron.csv', index=False)","34180243":"# fill nan value to easy separate them\ncabins_list = traintest.Cabin.fillna('NaN')\ncabins = []\nfor value in cabins_list:\n    if value != 'NaN':\n        cabins += value.split(' ')\n        \nunique_cabins = set(cabins)\nprint(\"Number unique in cabins: \", len(unique_cabins))\nprint(\"Number of Cabins: \", len(cabins))\nprint(\"Number of nan in Cabin: \", traintest.Cabin.isnull().sum())\nprint(\"Number of row not nan in Cabin: \", traintest.Cabin.shape[0] - traintest.Cabin.isnull().sum())\n       ","0f72faaa":"**Find number of unique values each column**","86dc8a1c":"## First Train Model","60ddc309":"**Find Duplicated Row**","dea15e87":"# Test Area","777bc36e":"**Final Prediction Neural Network**","2ded7336":"-> There is an overlap in Train\/Test set. Mean there are some IDs that appear in both Train and Test set.","c1320682":"### Cabin Feature\n\nFound that in Cabin column: a row can have multiple values in Cabin which separate by a whitespace. Here, I will split them by whitespace and check the unique values. ","e9d4af38":"As can be seen, there is no duplicated row in the dataset.","1b61a77b":"**Check distribution of train\/test set**","c3e1c4bf":"# Final Prediction","6835826d":"To do:\n- One-hot encoder categorical features\n- Train demo","0f530aa3":"### EDA\n"}}