{"cell_type":{"9048f23f":"code","d0e58971":"code","274bb71b":"code","bc697ed4":"code","07b99781":"code","8882e5ef":"code","fbd08348":"code","0d28068b":"code","11ddabc7":"code","30c4f5ee":"code","4d9f7532":"code","59a0dd5d":"code","40cc9a94":"code","e560aabf":"code","67d5ed4f":"code","e78e14bc":"code","4ac09b2a":"code","3cab4cf3":"code","fd228398":"code","825ed9b9":"code","e42f7d63":"code","7e83790a":"markdown","fb1f7a6a":"markdown","3dc420ca":"markdown","91dd0aaa":"markdown","628435f5":"markdown","b5208e2c":"markdown","5d26a16b":"markdown","15f72bc3":"markdown","a26a161f":"markdown","361a579f":"markdown","da8aea97":"markdown","457d2b37":"markdown","d3ac3a73":"markdown","9d7da0c3":"markdown"},"source":{"9048f23f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0e58971":"df_train= pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","274bb71b":"df_train.head()","bc697ed4":"y_train=df_train['label']\nx_train= df_train.drop(labels=['label'], axis=1) # axis=1 means a column\nprint('Shape of the y_train is:', y_train.shape)\nprint('Shape of the x_train is:', x_train.shape)\nprint('Shape of the x_train is:', df_test.shape)","07b99781":"y_train.value_counts()","8882e5ef":"from tensorflow.keras.utils import to_categorical\ny_train_encoded= to_categorical(y_train)\nprint(' New shape for y_train is:', y_train_encoded.shape)","fbd08348":"x_train_mean= x_train.mean()\nx_train_std= x_train.std()\nepsilon= 1e-10\n\nx_train_norm= (x_train-x_train_mean)\/ (x_train_std+ epsilon)\nX_test= (df_test-x_train_mean)\/ (x_train_std+ epsilon)","0d28068b":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\nmodel= Sequential([\n    Dense(128, activation= 'relu', input_shape=(784,)),\n    Dense(128, activation= 'relu'),\n    Dense(10, activation='softmax')\n])","11ddabc7":"model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics= ['acc'])","30c4f5ee":"from sklearn.model_selection import train_test_split\nX=x_train_norm\nY= y_train_encoded ## keeping a copy of our last worked datasets\n\nX_train, X_val, Y_train, Y_val= train_test_split(x_train_norm, y_train_encoded,test_size= 0.2, random_state=0)","4d9f7532":"print (X_train.shape, X_val.shape)","59a0dd5d":"h=model.fit(X_train,Y_train, epochs=10)","40cc9a94":"loss, accuracy= model.evaluate(X_val, Y_val)\nprint('Accuracy of our model is: ', (accuracy*100))","e560aabf":" model_1= Sequential([\n    Dense(128, activation= 'relu', input_shape=(784,)),\n    Dense(128, activation= 'relu'),\n    Dense(128, activation= 'relu'),\n    Dense(10, activation='softmax')\n])\nmodel_1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics= ['acc'])\nh=model_1.fit(X_train,Y_train, epochs=10)","67d5ed4f":"loss, accuracy= model_1.evaluate(X_val, Y_val)\nprint('Accuracy of our model is: ', (accuracy*100))","e78e14bc":"from tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.layers import Dropout\nmodel_1= Sequential([\n    Dense(128, activation= 'relu', input_shape=(784,)),\n    Dense(128, activation= 'relu'),\n    Dense(128, activation= 'relu'),\n])\nmodel_1.add(Dropout(0.15))\nmodel_1.add (Dense(10, activation='softmax'))\n\nmodel_1.compile(optimizer=RMSprop (lr=0.001), loss='categorical_crossentropy', metrics= ['acc'])\nh=model_1.fit(X_train,Y_train, epochs=15)","4ac09b2a":"loss, accuracy= model_1.evaluate(X_val, Y_val)\nprint('Accuracy of our model is: ', (accuracy*100))","3cab4cf3":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dropout # gave worst results\nmodel_2= Sequential([\n    Dense(128, activation= 'relu', input_shape=(784,)),\n    Dense(128, activation= 'relu'),\n    Dense(128, activation= 'relu'),\n])\n\nmodel_2.add (Dense(10, activation='softmax'))\n\nmodel_2.compile(optimizer=Adam (lr=0.001), loss='categorical_crossentropy', metrics= ['acc'])\nh=model_2.fit(X_train,Y_train, epochs=10)","fd228398":"loss, accuracy= model_2.evaluate(X_val, Y_val)\nprint('Accuracy of our model is: ', (accuracy*100))","825ed9b9":"from tensorflow.keras.optimizers import RMSprop\n\nmodel_2= Sequential([\n    Dense(128, activation= 'relu', input_shape=(784,)),\n    Dense(128, activation= 'relu'),\n    Dense(128, activation= 'relu'),\n])\n\nmodel_2.add (Dense(10, activation='softmax'))\n\nmodel_2.compile(optimizer=Adam (lr=0.001), loss='categorical_crossentropy', metrics= ['acc'])\nh=model_2.fit(X,Y, epochs=10)","e42f7d63":"predictions = model_2.predict_classes(X_test, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"DR_submissions.csv\", index=False, header=True)","7e83790a":"# This notebook is a revision to what I have studied about image classification using Tensorflow and keras.\n\nAs a revision, and well to check how the same code applies to a new dataset, I will follow the code from the tutorial and maybe then do some experiments with changes in optimizer or epochs.","fb1f7a6a":"Since Model 2 gave the best accuracy, lets use that.\n\n\n*PS: We have to take a run with the full dataset before (X and Y saved earlier) before we make a submission.*","3dc420ca":"## Step 7: Training the model\nTo check the accuracy of the model, and whether the model simply learned the\nweights and biases, we will first split the training data.","91dd0aaa":"## Trial 3: Using RMSProp\n\nWe see similar results. \n\nWould changing the optimizer from a common Stochastic Gradient descent to a iterative RMSProp have a difference?\n\nTo reduce overfitting, lets add a Dropout step of the factor 0.15 to our train set.","628435f5":"## Step 6: Compile the model\nAny model compiler needs\n1. Loss: difference between predicted and real values \n2. Optimizer : Define the algorithm used to minimize the loss\n3. Metric: To monitor the performance of the network","b5208e2c":"Minor leap! ","5d26a16b":"## Step 5: Create the model\n### Trial 1: \nUsing Sequential model and splitting the model into 2 hidden layers","15f72bc3":"## Trial 4: Using Adam\n\nAs a last trial, lets use Adam, a similar optimizer to RMS Prop.\n","a26a161f":"## Step 8: Validation and Predictions:\nThis is the most crucial step, to check if our model has simply learned the weights and biases, or our network is successful","361a579f":"## Step 4: Normalizing\nWhen the test and train data are already split, we should apply same preprocessing to both the datasets to avoid any anomoly","da8aea97":"## Trial 2: Lets add a dense layer and see if there is any variations","457d2b37":"## Step 1: Load the data set","d3ac3a73":"## Step 2: Preparing a labels dataset for training \n\nThe dataset has been set-up as:\n1. Label \n2. Pixel distribution, ranging from 0 to 783 i.e. 784 pixels (28 *28)\n\nWe need to extract the 'label' column into another dataframe.","9d7da0c3":"## Step 3: Use One-hot encoding using to_categorical"}}