{"cell_type":{"6f2a2410":"code","951289b2":"code","ee67dfea":"code","202ea7cd":"code","526be597":"code","08ecc4ea":"code","50ecfc52":"code","16dd2369":"code","90849fa5":"code","edcc68df":"code","0445042f":"code","28d2220f":"code","440815c2":"code","66fdfe5e":"code","736c3e4d":"code","b724d046":"code","42548028":"code","b27a6d83":"code","7bd04075":"code","7d01645e":"code","1fb0196a":"code","c993ed6f":"code","6f3fe396":"code","cddcfa5f":"code","52f8121b":"code","9e3cce7b":"code","68f902bf":"code","a86e69fe":"code","8b610405":"code","43a2c4ba":"code","209772e1":"code","ba3f8893":"code","d774a4b4":"code","325725a4":"markdown","6915c1fc":"markdown","82661817":"markdown","678d9401":"markdown","20f8f3cf":"markdown","0cc79ceb":"markdown","73fbfcbe":"markdown","5c838404":"markdown","fa3a1e61":"markdown","0b6f3e1b":"markdown","58a020be":"markdown","5d4d940e":"markdown"},"source":{"6f2a2410":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","951289b2":"df = pd.read_csv(\"..\/input\/reviews\/Restaurant_Reviews.tsv\",delimiter=\"\\t\",quoting=3)\n# we use delimiter=\"\\t\" in order to read tab separated data\n# we use quoting=3 because there a lot of quotes that can confuse nlp algprithm\ndf.head(6)","ee67dfea":"df.describe(include=\"all\")","202ea7cd":"df.groupby(\"Liked\").describe() # Our data is equally distributed between liked and disliked","526be597":"df[\"Length\"]= df[\"Review\"].apply(len) # Here we get the length of each review\ndf","08ecc4ea":"import cufflinks as cf\ncf.go_offline()\ndf[\"Length\"].iplot(kind=\"histogram\") \n# Here we get a hisgtogram of the length of the reviews","50ecfc52":"\nsns.set_style(\"darkgrid\")\ndf.hist(column=\"Length\",by=\"Liked\",figsize=(12,9))\n# We see that there is no big differences between the length of both liked and disliked reviews","16dd2369":"df[\"Length\"].max()","90849fa5":"import re\nimport nltk \nfrom nltk.corpus import stopwords #these are the words like \"the\", \"a\" which are not meaningful\nfrom nltk.stem.porter import PorterStemmer # this the algorithm to detect the meaning of the word from their roots\n# for example loved will be transformed into love","edcc68df":"corpus = list()\nfor i in range(0, len(df)):\n    #sub(pattern, repl, string, count=0, flags=0)\n    review = re.sub(\"[^a-zA-Z]\", \" \", df[\"Review\"][i]) # we will replace all the punction by space\n    # [^a-zA-Z] shows all the elements that are made by letters\n    review = review.lower()\n    review = review.split()\n    #Next step to get the stems or roots of each word, get rid of tenses added to the words\n    ps= PorterStemmer()\n    all_nonstop_words = stopwords.words(\"english\")\n    all_nonstop_words.remove(\"not\") #we need to remove not from the list of stop words\n    review = [ ps.stem(word) for word in review if word not in all_nonstop_words]\n    review = \" \".join(review) # we reconvert every review for list into a string\n    corpus.append(review)\n","0445042f":"print(corpus[:3]) #This is the latest version of review after data cleaning process above\n#These ar ethe threes first reviews","28d2220f":"df.head(3) #This is the original version of the three first reviews","440815c2":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()\nX","66fdfe5e":"X.shape #This shows that we have 1000 reviews and 1566 different words","736c3e4d":"y = df[\"Liked\"].values # This our target\ny.shape","b724d046":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)\nprint(X_train.shape,X_test.shape)\nprint(y_train.shape,y_test.shape)","42548028":"from sklearn.naive_bayes import GaussianNB\nmodel1 = GaussianNB()\nmodel1.fit(X_train,y_train)\npredictions= model1.predict(X_test)\npredictions","b27a6d83":"predictions_df = pd.DataFrame(predictions)\noriginal_reviews = pd.DataFrame(y_test)\n","7bd04075":"comparison = pd.concat([predictions_df,original_reviews],axis=1,ignore_index=True)\ncomparison.columns = [\"Predictions of Gaussian Naive Bayes Model\",\"Original reviews\"]\ncomparison","7d01645e":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","1fb0196a":"from sklearn.naive_bayes import MultinomialNB\nmodel2 = MultinomialNB()\nmodel2.fit(X_train,y_train)\npredictions2 = model2.predict(X_test)","c993ed6f":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,predictions2))\nprint(classification_report(y_test,predictions2))","6f3fe396":"new_review = 'I love this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = model1.predict(new_X_test)\nprint(new_y_pred) #It is positive according to the first model","cddcfa5f":"new_review = 'I love this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = model2.predict(new_X_test)\nprint(new_y_pred) #It is also positive according to the second model","52f8121b":"new_review = 'I hate this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = model1.predict(new_X_test)\nprint(new_y_pred) # This is a negative review according to our first model","9e3cce7b":"new_review = 'I hate this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = model2.predict(new_X_test)\nprint(new_y_pred) # This is a negative review according to our second model","68f902bf":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)\nX_train","a86e69fe":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nann = Sequential()\nann.add(Dense(units=1566, activation=\"relu\"))\nann.add(Dropout(0.1))\nann.add(Dense(units=1, activation=\"sigmoid\"))\nann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","8b610405":"history=ann.fit(x= X_train, y= y_train, batch_size=32, epochs=8, validation_data=(X_test, y_test))","43a2c4ba":"pd.DataFrame(ann.history.history).plot()","209772e1":"predictions3=ann.predict(X_test)\npredictions_df=pd.DataFrame(predictions3, columns=[\"Pred\"])\npredictions_df.head()","ba3f8893":"\noriginal_reviews = pd.DataFrame(y_test)\ncomparison = pd.concat([predictions_df,original_reviews],axis=1,ignore_index=True)\ncomparison.columns = [\"Predictions of Artifical Neural Networks\",\"Original reviews\"]\ncomparison # Artifical Neural Networks outperforms the others with %80 accuracy in test set","d774a4b4":"new_review = 'I hate this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = ann.predict(new_X_test)\nprint(new_y_pred) # This is a negative review according to our second model","325725a4":"## 1. What is Natural Language Processing:","6915c1fc":"## 2. Implementation of NLP:","82661817":"<font color =\"red\">\nPredicting if a single review is positive or negative:","678d9401":"<font color=\"blue\">\nYou use NLP on a text review to predict if the review is a good one or a bad one. You can use NLP on an article to predict some categories of the articles you are trying to segment. You can use NLP on a book to predict the genre of the book. And it can go further, you can use NLP to build a machine translator or a speech recognition system, and in that last example you use classification algorithms to classify language.","20f8f3cf":"<font color=\"blue\">\nNow we will tranform our data into vectors so that we can apply our NLP algorithm in three steps using the bag-of-words model:\n\n4.1.Count how many times does a word occur in each message (Known as term frequency)\n\nThis represents the number of times a word appears in a document, divided by the total number of words in that document\n\nTerm Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n\nTF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document).","0cc79ceb":"<font color =\"red\">\nLets try another algorithm:","73fbfcbe":"## 2.3. Creating Bag of Words:","5c838404":"## 2.1. Exploring the Data:","fa3a1e61":"## 2.2. Cleaning the Data:","0b6f3e1b":"## 2.4. Applying Machine Learning Algorithm:","58a020be":"The Application with NLP:","5d4d940e":"<font color=\"blue\">\nNatural Language Processing (or NLP) is applying Machine Learning models to text and language. Teaching machines to understand what is said in spoken and written word is the focus of Natural Language Processing. Whenever you dictate something into your iPhone \/ Android device that is then converted to text, that\u2019s an NLP algorithm in action."}}