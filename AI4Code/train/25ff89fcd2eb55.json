{"cell_type":{"1e92dfcd":"code","0d23b05e":"code","dcba8b9e":"code","2857be64":"code","0a6faedb":"code","fc37024e":"code","aa6ff84e":"code","968629c0":"code","731ea887":"code","8414780e":"code","70b00ddb":"code","172a589a":"code","be7f4cf1":"code","e16907dc":"code","369f0390":"code","69d5f46f":"code","0f243682":"code","4f2f6753":"markdown","f855174f":"markdown","4631bf3d":"markdown","39c84c52":"markdown","eca85892":"markdown","0548b7aa":"markdown"},"source":{"1e92dfcd":"import tensorflow\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import SGD, Adagrad, Adam\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport keras\nimport math\nimport argparse","0d23b05e":"# Generator\ndef create_generator(latent_size):\n    gen = Sequential()\n    #build a model layer by layer, each layer has weights that correspond to the layer that follows it\n    gen.add(\n        Dense(latent_size,\n              input_dim=latent_size,\n              activation='relu',\n              kernel_initializer=keras.initializers.Identity(gain=1.0)))\n    #Dense: layer type, amm nodes from previous layer connect to the nodes of the current layer\n    #latent_size: number of nodes in each input layer, ALSO: nombre des features\n    #activation:relu: rectified linear activation\n    gen.add(\n        Dense(latent_size,\n              activation='relu',\n              kernel_initializer=keras.initializers.Identity(gain=1.0)))\n    latent = Input(shape=(latent_size, ))\n    fake_data = gen(latent)\n    return Model(latent, fake_data)\n\n\n# Discriminator\ndef create_discriminator(latent_size, data_size):\n    dis = Sequential()\n    dis.add(\n        Dense(math.ceil(math.sqrt(data_size)),\n              input_dim=latent_size,\n              activation='relu',\n              kernel_initializer=keras.initializers.VarianceScaling(\n                  scale=1.0, mode='fan_in', distribution='normal', seed=None)))\n    dis.add(\n        Dense(1,\n              activation='sigmoid',\n              kernel_initializer=keras.initializers.VarianceScaling(\n                  scale=1.0, mode='fan_in', distribution='normal', seed=None)))\n    data = Input(shape=(latent_size, ))\n    fake = dis(data)\n    return Model(data, fake)\n\n\n# combine model\ndef create_combine_model(latent_size, optimizer_d, optimizer_g):\n    # Create discriminator\n    discriminator = create_discriminator()\n    discriminator.compile(optimizer=optimizer_d, loss='binary_crossentropy')\n\n    # Create combine model\n    generator = create_generator(latent_size)\n    latent = Input(shape=(latent_size, ))\n    fake = generator(latent)\n    discriminator.trainable = False\n    fake = discriminator(fake)\n    combine_model = Model(latent, fake)\n    combine_model.compile(optimizer=optimizer_g, loss='binary_crossentropy')\n\n    return combine_model","dcba8b9e":"# Load data\ndef load_data(filename):\n\n    # preprocess all databases other than creditcard\n    if not filename.startswith('credit'):\n        data = pd.read_table(\n            'C:\/Users\/hp\/Downloads\/Data\/' + filename,\n            sep=',',\n            header=None)\n        data = data.sample(frac=1).reset_index(drop=True)\n        id = data.pop(0)\n        y = data.pop(1)\n        data_x = data.as_matrix()\n        data_id = id.values\n        data_y = y.values\n    else:\n        # preprocess creditcard dataset\n        data = pd.read_table(\n            'C:\/Users\/hp\/Downloads\/Data\/' + filename,\n            sep=',')\n        mask_nor = data['Class'] == 0\n        mask_out = data['Class'] == 1\n        column_name = 'Class'\n        data.loc[mask_nor, column_name] = 'nor'\n        data.loc[mask_out, column_name] = 'out'\n        cols = data.columns.tolist()\n        cols = cols[-1:] + cols[:-1]\n        data = data[cols]\n        data.insert(loc=0, column='indx', value=0)\n        for index, row in data.iterrows():\n            data.at[index, 'indx'] = index\n        data = data.sample(frac=1).reset_index(drop=True)\n        id = data.pop(\"indx\")\n        y = data.pop(\"Class\")\n        data_x = data.as_matrix()\n        data_id = id.values\n        data_y = y.values\n\n    return data_x, data_y, data_id\n\n\n# Plot loss history\ndef plot(train_history, name):\n    dy = train_history['discriminator_loss']\n    gy = train_history['generator_loss']\n    aucy = train_history['auc']\n    x = np.linspace(1, len(dy), len(dy))\n    fig, ax = plt.subplots()\n    ax.plot(x, dy, color='green', label='discriminator')\n    ax.plot(x, gy, color='red', label='generator')\n    ax.plot(x, aucy, color='yellow', linewidth='3', label='AUC')\n    plt.show()","2857be64":"# tuning function\ndef tune(filename,\n         k_ranges,\n         lr_d_range,\n         lr_g_range,\n         optimizers_name,\n         stop_epochs_range,\n         plot=False):\n    '''\n    tunes the models hyperparams on the dataset specified in filename;\n    Arguments:\n        filename: dataset to tune on\n        k_ranges: list of k range of values\n        lr_d_range: list of lr_d range of values\n        lr_g_range: list of lr_g range of values\n        optimizers_name: list of strings specifying the optimizers to use: ('sgd', 'adam')\n        stop_epochs_range: list of stop_epochs range of values\n    Returns:\n        final_results_df: pandas dataframe recap of the tuning process\n    '''\n    print('\\tTUNING ON ---->', filename)\n\n    results = {}\n    final_results_df = pd.DataFrame()\n\n    decay = 1e-6\n    momentum = 0.9\n    train = True\n    data_x, data_y, data_id = load_data(filename)\n    latent_size = data_x.shape[1]  #nombre des features\n    data_size = data_x.shape[0]  #nombre d'enregistrements\n    print(\"The dimension of the training data :{}*{}\".format(\n        data_x.shape[0], data_x.shape[1]))\n    print()\n\n    for k in k_ranges:\n        results['k'] = k\n        for stop_epochs in stop_epochs_range:\n            results['stop_epochs'] = stop_epochs\n            epochs = stop_epochs * 3\n            for lr_d, lr_g in zip(lr_d_range, lr_g_range):\n                results['lr_d'] = lr_d\n                results['lr_g'] = lr_g\n                for optimizer_name in optimizers_name:\n                    results['optimizer'] = optimizer_name\n                    if optimizer_name == 'sgd':\n                        optimizer_d = SGD(lr_d, decay=decay, momentum=momentum)\n                        optimizer_g = SGD(lr_g, decay=decay, momentum=momentum)\n                        optimizer = SGD(lr_d, decay=decay, momentum=momentum)\n                    else:\n                        optimizer = Adam(lr_d, decay=decay)\n                        optimizer_d = Adam(lr_d, decay=decay)\n                        optimizer_g = Adam(lr_g, decay=decay)\n\n                    if train:\n                        train_history = defaultdict(list)\n                        names = locals()\n                        stop = 0\n\n                        # Create discriminator\n                        discriminator = create_discriminator(\n                            latent_size, data_size)\n                        discriminator.compile(optimizer=optimizer_d,\n                                              loss='binary_crossentropy')\n\n                        # Create k combine models\n                        for i in range(k):\n                            names['sub_generator' +\n                                  str(i)] = create_generator(latent_size)\n                            latent = Input(shape=(latent_size, ))\n                            names['fake' + str(i)] = names['sub_generator' +\n                                                           str(i)](latent)\n                            discriminator.trainable = False\n                            names['fake' + str(i)] = discriminator(\n                                names['fake' + str(i)])\n                            names['combine_model' + str(i)] = Model(\n                                latent, names['fake' + str(i)])\n                            names['combine_model' + str(i)].compile(\n                                optimizer=optimizer_g,\n                                loss='binary_crossentropy')\n\n                        # Start iteration\n                        for epoch in range(epochs):\n                            print('Epoch {} of {}'.format(epoch + 1, epochs))\n                            batch_size = min(500, data_size)\n                            num_batches = int(data_size \/ batch_size)\n\n                            for index in range(num_batches):\n                                print(\n                                    '\\nTesting for epoch {} index {}:'.format(\n                                        epoch + 1, index + 1))\n\n                                # Generate noise\n                                noise_size = batch_size\n                                noise = np.random.uniform(\n                                    0, 1, (int(noise_size), latent_size))\n\n                                # Get training data\n                                data_batch = data_x[index *\n                                                    batch_size:(index + 1) *\n                                                    batch_size]\n\n                                # Generate potential outliers\n                                block = ((1 + k) * k) \/\/ 2\n                                for i in range(k):\n                                    if i != (k - 1):\n                                        noise_start = int(\n                                            (((k + (k - i + 1)) * i) \/ 2) *\n                                            (noise_size \/\/ block))\n                                        noise_end = int(\n                                            (((k + (k - i)) * (i + 1)) \/ 2) *\n                                            (noise_size \/\/ block))\n                                        names['noise' + str(\n                                            i)] = noise[noise_start:noise_end]\n                                        names['generated_data' +\n                                              str(i)] = names[\n                                                  'sub_generator' +\n                                                  str(i)].predict(\n                                                      names['noise' + str(i)],\n                                                      verbose=0)\n                                    else:\n                                        noise_start = int(\n                                            (((k + (k - i + 1)) * i) \/ 2) *\n                                            (noise_size \/\/ block))\n                                        names['noise' + str(\n                                            i)] = noise[noise_start:noise_size]\n                                        names['generated_data' +\n                                              str(i)] = names[\n                                                  'sub_generator' +\n                                                  str(i)].predict(\n                                                      names['noise' + str(i)],\n                                                      verbose=0)\n\n                                # Concatenate real data to generated data\n                                for i in range(k):\n                                    if i == 0:\n                                        X = np.concatenate(\n                                            (data_batch,\n                                             names['generated_data' + str(i)]))\n                                    else:\n                                        X = np.concatenate(\n                                            (X,\n                                             names['generated_data' + str(i)]))\n                                Y = np.array([1] * batch_size +\n                                             [0] * int(noise_size))\n\n                                # Train discriminator\n                                discriminator_loss = discriminator.train_on_batch(\n                                    X, Y)\n                                train_history['discriminator_loss'].append(\n                                    discriminator_loss)\n\n                                # Get the target value of sub-generator\n                                p_value = discriminator.predict(data_x)\n                                p_value = pd.DataFrame(p_value)\n                                for i in range(k):\n                                    names['T' + str(i)] = p_value.quantile(i \/\n                                                                           k)\n                                    names['trick' + str(i)] = np.array(\n                                        [float(names['T' + str(i)])] *\n                                        noise_size)\n\n                                # Train generator\n                                noise = np.random.uniform(\n                                    0, 1, (int(noise_size), latent_size))\n                                if stop == 0:\n                                    for i in range(k):\n                                        names['sub_generator' + str(i) +\n                                              '_loss'] = names[\n                                                  'combine_model' +\n                                                  str(i)].train_on_batch(\n                                                      noise,\n                                                      names['trick' + str(i)])\n                                        train_history['sub_generator{}_loss'.\n                                                      format(i)].append(\n                                                          names['sub_generator'\n                                                                + str(i) +\n                                                                '_loss'])\n                                else:\n                                    for i in range(k):\n                                        names['sub_generator' + str(i) +\n                                              '_loss'] = names[\n                                                  'combine_model' +\n                                                  str(i)].evaluate(\n                                                      noise,\n                                                      names['trick' + str(i)])\n                                        train_history['sub_generator{}_loss'.\n                                                      format(i)].append(\n                                                          names['sub_generator'\n                                                                + str(i) +\n                                                                '_loss'])\n\n                                generator_loss = 0\n                                for i in range(k):\n                                    generator_loss = generator_loss + names[\n                                        'sub_generator' + str(i) + '_loss']\n                                generator_loss = generator_loss \/ k\n                                train_history['generator_loss'].append(\n                                    generator_loss)\n\n                                # Stop training generator\n                                #if epoch +1  > args.stop_epochs:\n                                if epoch + 1 > stop_epochs:\n                                    stop = 1\n\n                            # Detection result\n                            data_y = pd.DataFrame(data_y)\n                            result = np.concatenate((p_value, data_y), axis=1)\n                            result = pd.DataFrame(result, columns=['p', 'y'])\n                            result = result.sort_values('p', ascending=True)\n\n                            # Calculate the AUC\n                            inlier_parray = result.loc[lambda df: df.y ==\n                                                       \"nor\", 'p'].values\n                            outlier_parray = result.loc[lambda df: df.y ==\n                                                        \"out\", 'p'].values\n                            sum = 0.0\n                            for o in outlier_parray:\n                                for i in inlier_parray:\n                                    if o < i:\n                                        sum += 1.0\n                                    elif o == i:\n                                        sum += 0.5\n                                    else:\n                                        sum += 0\n                            AUC = float('{:.4f}'.format(\n                                sum \/\n                                (len(inlier_parray) * len(outlier_parray))))\n                            print('AUC:{}'.format(AUC))\n                            for i in range(num_batches):\n                                train_history['auc'].append(AUC)\n\n                    if plot:\n                        plot(train_history, 'loss')\n                    print('{} ---> AUC:{}'.format(filename, AUC))\n                    results['AUC'] = AUC\n                    print('results', results)\n\n                    final_results_df = final_results_df.append(\n                        pd.DataFrame(results.copy(),\n                                     index=np.arange(len(results))))\n\n    return final_results_df.drop_duplicates().reset_index().drop('index',\n                                                                 axis=1)\n\n\ndef highlight_max(x):\n    '''\n    highlight the max AUC value in the final dataframe\n    '''\n    return [\n        'background-color: lightblue'\n        if x.name == 'AUC' and v == x.max() else '' for v in x\n    ]","0a6faedb":"file_list = ['kddcup.data_10_percent.gz',\n             'creditcard_copy_2_mod.csv',\n             'Annthyroid.csv',\n             'onecluster.csv',\n             'SpamBase.csv',\n             'WDBC.csv',\n             'creditcard_1_1.csv']","fc37024e":"np.random.seed(19960916)\nfilename = 'onecluster.txt'\nstop_epochs = [4, 6]\nk_ranges = [3, 4]\nlr_d_range = [0.01, 0.1]\nlr_g_range = [0.0001, 0.001]\noptimizer_names = ['sgd', 'adam']\nresults_df = tune(filename, k_ranges, lr_d_range, lr_g_range, optimizer_names, stop_epochs).style.apply(highlight_max)","aa6ff84e":"print(\"Tuning results on\", filename)\nresults_df","968629c0":"np.random.seed(19960916)\nfilename = 'kddCup.txt'\nstop_epochs = [4, 6]\nk_ranges = [3, 4]\nlr_d_range = [0.01, 0.1]\nlr_g_range = [0.0001, 0.001]\noptimizer_names = ['sgd', 'adam']\nresults_df = tune(filename, k_ranges, lr_d_range, lr_g_range, optimizer_names, stop_epochs).style.apply(highlight_max)","731ea887":"print(\"Tuning results on\", filename)\nresults_df","8414780e":"np.random.seed(19960916)\nfilename = 'Annthyroid.txt'\nstop_epochs = [4, 6]\nk_ranges = [3, 4]\nlr_d_range = [0.01, 0.1]\nlr_g_range = [0.0001, 0.001]\noptimizer_names = ['sgd', 'adam']\nresults_df = tune(filename, k_ranges, lr_d_range, lr_g_range, optimizer_names, stop_epochs).style.apply(highlight_max)","70b00ddb":"print(\"Tuning results on\", filename)\nresults_df","172a589a":"np.random.seed(19960916)\nfilename = 'SpamBase.txt'\nstop_epochs = [4, 6]\nk_ranges = [3, 4]\nlr_d_range = [0.01, 0.1]\nlr_g_range = [0.0001, 0.001]\noptimizer_names = ['sgd', 'adam']\nresults_df = tune(filename, k_ranges, lr_d_range, lr_g_range, optimizer_names, stop_epochs).style.apply(highlight_max)","be7f4cf1":"print(\"Tuning results on\", filename)\nresults_df","e16907dc":"np.random.seed(19960916)\nfilename = 'WDBC.TXT'\nstop_epochs = [4, 6]\nk_ranges = [3, 4]\nlr_d_range = [0.01, 0.1]\nlr_g_range = [0.0001, 0.001]\noptimizer_names = ['sgd', 'adam']\nresults_df = tune(filename, k_ranges, lr_d_range, lr_g_range, optimizer_names, stop_epochs).style.apply(highlight_max)","369f0390":"print(\"Tuning results on\", filename)\nresults_df","69d5f46f":"np.random.seed(19960916)\nfilename = 'copy_2_mod.csv'\nstop_epochs = [4, 6]\nk_ranges = [3, 4]\nlr_d_range = [0.01, 0.1]\nlr_g_range = [0.0001, 0.001]\noptimizer_names = ['sgd', 'adam']\nresults_df = tune(filename, k_ranges, lr_d_range, lr_g_range, optimizer_names, stop_epochs).style.apply(highlight_max)","0f243682":"print(\"Tuning results on\", filename)\nresults_df","4f2f6753":"**KDDCUP**","f855174f":"**WDBC.TXT**","4631bf3d":"**SpamBase.txt**","39c84c52":"**onecluster.txt**","eca85892":"**creditcard_1_1.csv**","0548b7aa":"**Annthyroid.txt**"}}