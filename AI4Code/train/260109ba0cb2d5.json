{"cell_type":{"4c257ce9":"code","3917ba5a":"code","c2e8983e":"code","5ff6792a":"code","cb30cf3f":"code","b6b84351":"code","31ecf720":"code","b7000fd4":"code","83a3543f":"code","7ac0ee5e":"code","bb411400":"code","5ea82a9d":"code","299c4eed":"code","a06c4098":"code","a77d083d":"code","98a50820":"code","cdd70331":"code","bcf11c72":"code","3f8a7315":"code","b5f46f96":"code","4eda8cd5":"code","4ed8afd5":"code","d0642f94":"code","7f0ef826":"code","1dd8d744":"code","76132057":"code","c2c3a99a":"code","c14e0817":"code","19917d0f":"code","7d2a7cc8":"code","a5d7fd1b":"code","1becb194":"code","b53da36d":"code","2a26427c":"code","b056e5ca":"code","0eb61b23":"code","10b38820":"markdown","5ce5725d":"markdown","0e95217e":"markdown","9dc7302d":"markdown","c06abc9b":"markdown","8e5799ab":"markdown","42256908":"markdown","37efba5d":"markdown","cbdacf41":"markdown","8a937579":"markdown","61263f34":"markdown","ae7e4967":"markdown","29776e4b":"markdown","83afcf32":"markdown","ef6fb673":"markdown"},"source":{"4c257ce9":"!pip install sentence-transformers","3917ba5a":"# !pip install rank_bm25","c2e8983e":"from sentence_transformers import SentenceTransformer, CrossEncoder\nimport os\nimport re\nimport csv\nimport time\nimport torch\nimport pickle\n\nimport numpy as np\nimport pandas as pd\n\nfrom torch import Tensor, device\nfrom typing import Callable\nfrom tqdm.autonotebook import tqdm\n","5ff6792a":"def get_stopwords_list(stop_file_path):\n    \"\"\"load stop words \"\"\"\n    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n        stopwords = f.readlines()\n        stop_set = set(m.strip() for m in stopwords)\n        return list(frozenset(stop_set))","cb30cf3f":"stopwords_path = \"..\/input\/combined-indonesian-stopwords\/combined_indonesian.txt\"\nstopwords = get_stopwords_list(stopwords_path)","b6b84351":"def stopwordRemove(text):\n    tokenized_doc = []\n    for token in text.split():\n        if len(token) > 0 and token not in stopwords:\n            tokenized_doc.append(token)\n    fullSentence = ' '.join(map(str, tokenized_doc))\n    return fullSentence","31ecf720":"stopwordRemove(\"Halo 132 eheheh kamu makan apa hari ini\")","b7000fd4":"def preprocessing(s):\n    s = re.sub(r\"(<br\\s*\/><br\\s*\/>)|(\\-)|(\\\/)\", ' ', s)\n    s = re.sub(r'<([A-Z][A-Z0-9]*)\\b[^>]*>(.*?)<\/\\1>', ' ', s) # html tag\n    s = s.strip().lower() # case folding dan menghilangkan new line\n    s = s.replace(\"\\n\", \" \") # menggantikan \\n dengan spasi\n    s = re.sub(r'[^a-zA-Z0-9 ]', ' ', s) # menghapus simbol\n    s = re.sub(' +', ' ', s) # menghapus repetitive space\n\n#     s = stopwordRemove(s)\n    return s","83a3543f":"print(preprocessing(\"Sore dok saya mau nanya ,ada bayangan di dada setelah ronsen itu karena jatuh dari kereta terbentur kepalanya tapi batuk darah.itu darah darimana ya dok kepala atau paru.di cek paru nya hasil \\jantung tidak membesar (ctr 50 %)aorta elogansi.mediastunum superior tidak melebar.trakea di tengah.kedua hillus suram.infiltrat parahiller dan parakardial kanan-kiri.kedua hemidiafragma licin.kedua sinus kostofrenikus lancip.tidak tampak jelas fraktur costae.kesimpulan:infiltrat paru (bagaimana lab?).Foto ronsen nya mau saya kirim kemana dokKata dokter kemungkinan TBC,bisa resepkan obat ga dok.terima kasih.\"))","7ac0ee5e":"if not torch.cuda.is_available():\n    print(\"Warning: No GPU detected. Processing will be slow. Please add a GPU to this notebook\")\n\nmodel_name = '..\/input\/transfer-learning-multilingual-sbert\/output\/quora-indo-sbert-v1-en-id-2021-04-30_16-31-06\/'\nmodel = SentenceTransformer(model_name)\n\n# url = \"http:\/\/qim.fs.quoracdn.net\/quora_duplicate_questions.tsv\"\ndataset_path = \"..\/input\/medical-qa\/qa.csv\"\nmax_corpus_size = 430000\n\n# Get all unique sentences from the file\ncorpus_sentences = set()\ncorpus_normal = set()\nwith open(dataset_path, encoding='utf8') as fIn:\n    reader = csv.DictReader(fIn, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n    for row in tqdm(reader):\n        sentence = preprocessing(row['question'])\n        corpus_sentences.add(sentence)\n        corpus_normal.add(row['question'])\n        if len(corpus_sentences) >= max_corpus_size:\n            break\n            \ncorpus_normal = list(corpus_normal)\ncorpus_sentences = list(corpus_sentences)\nprint(\"Encode the corpus. This might take a while\")\ncorpus_embeddings = model.encode(corpus_sentences, show_progress_bar=True, convert_to_tensor=True)\n\nwith open('corpus_embeddings.pickle', 'wb') as handle:\n    pickle.dump(corpus_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n###############################\nprint(\"Corpus loaded with {} sentences \/ embeddings\".format(len(corpus_sentences)))","bb411400":"# cross_encoder = CrossEncoder('..\/input\/cross-encoder-question-pair\/output\/training_quora-2021-06-04_08-01-09\/')\n# cross_encoder = CrossEncoder('..\/input\/cross-encoder-question-pair\/output\/training_PAWS-2021-06-17_16-42-53\/')\n# cross_encoder = CrossEncoder('..\/input\/cross-encoder-question-pair\/output\/training_PAWS-2021-06-24_17-38-09\/')\ncross_encoder = CrossEncoder('..\/input\/cross-encoder-question-pair\/output\/training_PAWS-2021-07-05_11-50-40\/')","5ea82a9d":"def cos_sim(a: Tensor, b: Tensor):\n    \"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n    \"\"\"\n    if not isinstance(a, torch.Tensor):\n        a = torch.tensor(a)\n\n    if not isinstance(b, torch.Tensor):\n        b = torch.tensor(b)\n\n    if len(a.shape) == 1:\n        a = a.unsqueeze(0)\n\n    if len(b.shape) == 1:\n        b = b.unsqueeze(0)\n\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n    return torch.mm(a_norm, b_norm.transpose(0, 1))\n","299c4eed":"def dot_score(a: Tensor, b: Tensor):\n    \"\"\"\n    Computes the dot-product dot_prod(a[i], b[j]) for all i and j.\n    :return: Matrix with res[i][j]  = dot_prod(a[i], b[j])\n    \"\"\"\n    if not isinstance(a, torch.Tensor):\n        a = torch.tensor(a)\n\n    if not isinstance(b, torch.Tensor):\n        b = torch.tensor(b)\n\n    if len(a.shape) == 1:\n        a = a.unsqueeze(0)\n\n    if len(b.shape) == 1:\n        b = b.unsqueeze(0)\n\n    return torch.mm(a, b.transpose(0, 1))","a06c4098":"def semantic_search(query_embeddings: Tensor,\n                    corpus_embeddings: Tensor,\n                    query_chunk_size: int = 100,\n                    corpus_chunk_size: int = 500000,\n                    top_k: int = 10,\n                    score_function: Callable[[Tensor, Tensor], Tensor] = cos_sim):\n    \"\"\"\n    This function performs a cosine similarity search between a list of query embeddings  and a list of corpus embeddings.\n    It can be used for Information Retrieval \/ Semantic Search for corpora up to about 1 Million entries.\n    :param query_embeddings: A 2 dimensional tensor with the query embeddings.\n    :param corpus_embeddings: A 2 dimensional tensor with the corpus embeddings.\n    :param query_chunk_size: Process 100 queries simultaneously. Increasing that value increases the speed, but requires more memory.\n    :param corpus_chunk_size: Scans the corpus 100k entries at a time. Increasing that value increases the speed, but requires more memory.\n    :param top_k: Retrieve top k matching entries.\n    :param score_function: Funtion for computing scores. By default, cosine similarity.\n    :return: Returns a sorted list with decreasing cosine similarity scores. Entries are dictionaries with the keys 'corpus_id' and 'score'\n    \"\"\"\n\n    if isinstance(query_embeddings, (np.ndarray, np.generic)):\n        query_embeddings = torch.from_numpy(query_embeddings)\n    elif isinstance(query_embeddings, list):\n        query_embeddings = torch.stack(query_embeddings)\n\n    if len(query_embeddings.shape) == 1:\n        query_embeddings = query_embeddings.unsqueeze(0)\n\n    if isinstance(corpus_embeddings, (np.ndarray, np.generic)):\n        corpus_embeddings = torch.from_numpy(corpus_embeddings)\n    elif isinstance(corpus_embeddings, list):\n        corpus_embeddings = torch.stack(corpus_embeddings)\n\n\n    #Check that corpus and queries are on the same device\n    if corpus_embeddings.device != query_embeddings.device:\n        query_embeddings = query_embeddings.to(corpus_embeddings.device)\n\n    queries_result_list = [[] for _ in range(len(query_embeddings))]\n\n    for query_start_idx in range(0, len(query_embeddings), query_chunk_size):\n        # Iterate over chunks of the corpus\n        for corpus_start_idx in range(0, len(corpus_embeddings), corpus_chunk_size):\n            # Compute cosine similarites\n            cos_scores = score_function(query_embeddings[query_start_idx:query_start_idx+query_chunk_size], corpus_embeddings[corpus_start_idx:corpus_start_idx+corpus_chunk_size])\n\n            # Get top-k scores\n            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(top_k, len(cos_scores[0])), dim=1, largest=True, sorted=False)\n            cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n            cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n\n            for query_itr in range(len(cos_scores)):\n                for sub_corpus_id, score in zip(cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]):\n                    corpus_id = corpus_start_idx + sub_corpus_id\n                    query_id = query_start_idx + query_itr\n                    queries_result_list[query_id].append({'corpus_id': corpus_id, 'score': score})\n\n    #Sort and strip to top_k results\n    for idx in range(len(queries_result_list)):\n        queries_result_list[idx] = sorted(queries_result_list[idx], key=lambda x: x['score'], reverse=True)\n        queries_result_list[idx] = queries_result_list[idx][0:top_k]\n\n    return queries_result_list","a77d083d":"# We also compare the results to lexical search (keyword search). Here, we use \n# the BM25 algorithm which is implemented in the rank_bm25 package.\n\n# from rank_bm25 import BM25Okapi\n# import string","98a50820":"# We lower case our text and remove stop-words from indexing\n# def bm25_tokenizer(text):\n#     tokenized_doc = []\n#     for token in text.lower().split():\n#         token = token.strip(string.punctuation)\n\n#         if len(token) > 0 and token not in stopwords:\n#             tokenized_doc.append(token)\n#     return tokenized_doc\n\n# tokenized_corpus = []\n# for passage in tqdm(corpus_sentences):\n#     tokenized_corpus.append(bm25_tokenizer(passage))\n\n# bm25 = BM25Okapi(tokenized_corpus)","cdd70331":"topk = 10","bcf11c72":"# Function that searches the corpus and prints the results\ndef search(query):\n    query = preprocessing(query)\n    print(\"Input question:\", query)\n    \n    ### BM25 search (lexical search) ###\n#     start_time = time.time()\n#     bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n#     top_n = np.argpartition(bm25_scores, -5)[-5:]\n#     bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n#     bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n#     end_time = time.time()\n\n#     print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n#     print(\"\\n======================= Top-5 lexical search (BM25) hits =======================\")\n#     for hit in bm25_hits[0:5]:\n#         print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']].replace(\"\\n\", \" \")))\n        \n    ### Semantic Search ###\n    start_time = time.time()\n    question_embedding = model.encode(query, convert_to_tensor=True)\n    hits = semantic_search(question_embedding, corpus_embeddings, top_k = topk)\n    end_time = time.time()\n    hits = hits[0]  #Get the hits for the first query\n    \n    print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n    print(\"\\n======================= Top-5 Semantic Search hits =======================\")\n    for hit in hits[0:5]:\n        print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n    \n    ##### Re-Ranking #####\n    #Now, score all retrieved passages with the cross_encoder\n    \n#     length_tolerance = 5\n#     cross_inp = []\n#     for hit in hits:\n#         if len(corpus_sentences[hit['corpus_id']].split()) <= len(query.split()) + length_tolerance or len(corpus_sentences[hit['corpus_id']].split()) >= len(query.split()) - length_tolerance:\n#             cross_inp.append([query, corpus_sentences[hit['corpus_id']]])\n#         else:\n#             continue\n#     print(\"pjg = \", len(cross_inp))\n            \n    cross_inp = [[query, corpus_sentences[hit['corpus_id']]] for hit in hits]\n    cross_scores = cross_encoder.predict(cross_inp)\n\n    #Sort results by the cross-encoder scores\n    for idx in range(len(cross_scores)):\n        hits[idx]['cross-score'] = cross_scores[idx]\n    \n    print(\"\\n======================= Top-5 Cross-Encoder Re-ranker hits =======================\")\n    ceHits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n    for hit in ceHits[0:5]:\n        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], corpus_sentences[hit['corpus_id']].replace(\"\\n\", \" \")))\n","3f8a7315":"# search(\"Saat ini saya serang menggunakan KB IUD, sebelumnya menstruasi lancar dengan siklus 28 hari, tapi sudah 8 bulan terakhir saya tdk mensruasi. Saya sempat konsultasi dengan bidan di puskesmas, tapi bid an tdk bisa mendiagnosa sebabnya. Sebagai inform as I, saya per nah menderita mioma tapi sudah diangkat sekitar 9 tahun laku. Pertanyaan saya, mungkinkah saya sudah memasuki masa manapouse, Sekarang umur saya 44 tahun. Adakah kemungkinan mioma tumbuh kembali? Mohon penjelasannya, Terima kasih.\")","b5f46f96":"import re\n\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nfrom gensim.models import KeyedVectors\n\nimport gensim\n\nimport numpy as np\n\nimport itertools\nimport tensorflow as tf\nfrom itertools import chain","4eda8cd5":"def text_to_word_list(text):\n    # Pre process and convert texts to a list of words\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    text = text.split()\n\n    return text\n\n\ndef make_w2v_embeddings(df, embedding_dim=100, empty_w2v=False):\n    vocabs = {}\n    vocabs_cnt = 0\n\n    vocabs_not_w2v = {}\n    vocabs_not_w2v_cnt = 0\n\n\n    # Load word2vec\n#     print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n\n    if empty_w2v:\n        word2vec = EmptyWord2Vec\n    else:\n#         word2vec = KeyedVectors.load_word2vec_format(\".\/data\/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n#         word2vec = gensim.models.word2vec.Word2Vec.load(\".\/data\/Quora-Question-Pairs.w2v\").wv\n        word2vec = gensim.models.word2vec.Word2Vec.load('..\/input\/word2vec-100-indonesian\/idwiki_word2vec_100.model')\n\n    for index, row in df.iterrows():\n        # Print the number of embedded sentences.\n        if index != 0 and index % 1000 == 0:\n            print(\"{:,} sentences embedded.\".format(index), flush=True)\n\n        # Iterate through the text of both questions of the row\n        for question in ['sentence1', 'sentence2']:\n\n            q2n = []  # q2n -> question numbers representation\n            for word in text_to_word_list(row[question]):\n                # Check for unwanted words\n                if word in stopwords:\n                    continue\n\n                # If a word is missing from word2vec model.\n#                 if word not in word2vec.vocab:\n                if word not in word2vec.wv.index_to_key:\n                    if word not in vocabs_not_w2v:\n                        vocabs_not_w2v_cnt += 1\n                        vocabs_not_w2v[word] = 1\n\n                # If you have never seen a word, append it to vocab dictionary.\n                if word not in vocabs:\n                    vocabs_cnt += 1\n                    vocabs[word] = vocabs_cnt\n                    q2n.append(vocabs_cnt)\n                else:\n                    q2n.append(vocabs[word])\n\n            # Append question as number representation\n            df.at[index, question + '_n'] = q2n\n\n    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n    embeddings[0] = 0  # So that the padding will be ignored\n\n    # Build the embedding matrix\n    for word, index in vocabs.items():\n#         if word in word2vec.vocab:\n        if word in word2vec.wv.index_to_key:\n#             embeddings[index] = word2vec.word_vec(word)\n            embeddings[index] = word2vec.wv[word]            \n    del word2vec\n\n    return df, embeddings\n\n\ndef split_and_zero_padding(df, max_seq_length):\n    # Split to dicts\n    X = {'left': df['sentence1_n'], 'right': df['sentence2_n']}\n\n    # Zero padding\n    for dataset, side in itertools.product([X], ['left', 'right']):\n        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n\n    return dataset\n\n\n#  --\n\nclass ManDist(Layer):\n    \"\"\"\n    Keras Custom Layer that calculates Manhattan Distance.\n    \"\"\"\n\n    # initialize the layer, No need to include inputs parameter!\n    def __init__(self, **kwargs):\n        self.result = None\n        super(ManDist, self).__init__(**kwargs)\n\n    # input_shape will automatic collect input shapes to build layer\n    def build(self, input_shape):\n        super(ManDist, self).build(input_shape)\n\n    # This is where the layer's logic lives.\n    def call(self, x, **kwargs):\n        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n        return self.result\n\n    # return output shape\n    def compute_output_shape(self, input_shape):\n        return K.int_shape(self.result)\n\n\nclass EmptyWord2Vec:\n    \"\"\"\n    Just for test use.\n    \"\"\"\n    vocab = {}\n    word_vec = {}","4ed8afd5":"# Function that searches the corpus and prints the results\ndef programSearch(query, category, wordcount):\n    query = preprocessing(query)\n    \n#     ### BM25 search (lexical search) ###\n#     start_time = time.time()\n#     bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n#     top_n = np.argpartition(bm25_scores, -5)[-5:]\n#     bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n#     bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n#     end_time = time.time()\n    \n#     bm25Data = [query, category, wordcount, \"{:.3f}\".format(end_time-start_time)]\n\n#     for hit in bm25_hits[0:5]:\n#         bm25Data.append(\"{:.3f}\".format(hit['score']))\n#         bm25Data.append(corpus_sentences[hit['corpus_id']].replace(\"\\n\", \" \"))\n#     bm25rows.append(bm25Data)\n\n    ### Semantic Search ###\n    start_time = time.time()\n    question_embedding = model.encode(query, convert_to_tensor=True)\n    hits = semantic_search(question_embedding, corpus_embeddings, top_k = topk)\n    end_time = time.time()\n    hits = hits[0]  #Get the hits for the first query\n        \n    data = [query, category, wordcount, \"{:.3f}\".format(end_time-start_time)]\n\n    for hit in hits[0:5]:\n        data.append(\"{:.3f}\".format(hit['score']))\n        data.append(corpus_sentences[hit['corpus_id']])\n    rows.append(data)\n    \n    ##### Re-Ranking #####\n    #Now, score all retrieved passages with the cross_encoder\n    start_time = time.time()\n    cross_inp = [[query, corpus_sentences[hit['corpus_id']]] for hit in hits]\n    cross_scores = cross_encoder.predict(cross_inp)\n    \n    #Sort results by the cross-encoder scores\n    for idx in range(len(cross_scores)):\n        hits[idx]['cross-score'] = cross_scores[idx]    \n    \n    ceHits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n    ce_end_time = time.time()\n    \n    ceData = [query, category, wordcount, \"{:.3f}\".format(ce_end_time-start_time)]\n#     print(ceHits)\n    \n    for hit in ceHits[0:5]:\n        ceData.append(\"{:.3f}\".format(hit['cross-score']))\n        ceData.append(corpus_sentences[hit['corpus_id']].replace(\"\\n\", \" \"))\n    \n    ceRows.append(ceData)\n    \n    \n    ############################ LSTM ##################################\n    \n#     test_df = pd.DataFrame.from_records(cross_inp, columns=['sentence1', 'sentence2'])\n    \n#     for q in ['sentence1', 'sentence2']:\n#         test_df[q + '_n'] = test_df[q]\n    \n#     embedding_dim = 100\n#     max_seq_length = 20\n    \n#     start_timeL = time.time()\n#     test_df, embeddings = make_w2v_embeddings(test_df, embedding_dim=embedding_dim, empty_w2v=False)\n\n#     # Split to dicts and append zero padding.\n#     X_test = split_and_zero_padding(test_df, max_seq_length)\n\n#     # Make sure everything is ok\n#     assert X_test['left'].shape == X_test['right'].shape\n\n#     lstm = tf.keras.models.load_model('..\/input\/malstm-model\/SiameseLSTM.h5', custom_objects={'ManDist': ManDist})\n# #     lstm.summary()\n\n#     prediction = lstm.predict([X_test['left'], X_test['right']])\n#     lstm_result = list(chain.from_iterable(prediction))\n    \n#     for idx in range(len(cross_scores)):\n#         hits[idx]['lstm-score'] = lstm_result[idx] \n        \n#     lstmHits = sorted(hits, key=lambda x: x['lstm-score'], reverse=True)\n#     end_timeL = time.time()\n    \n#     lstmData = [query, category, wordcount, \"{:.3f}\".format(end_timeL-start_timeL)]\n    \n#     for hit in lstmHits[0:5]:\n#         lstmData.append(\"{:.3f}\".format(hit['lstm-score']))\n#         lstmData.append(corpus_sentences[hit['corpus_id']].replace(\"\\n\", \" \"))\n    \n#     lstmRows.append(lstmData)\n#     print(lstmHits)\n    \n    return query","d0642f94":"rows = []\nlstmRows = []\nceRows = []\nprogramSearch(\"Perbedaan asam lambung dan maag?\", \"\", \"\")","7f0ef826":"lstmRows","1dd8d744":"ceRows","76132057":"test_path = '..\/input\/qatest-fix\/qaTest umum.csv'\n# test_path = '..\/input\/qatest-fix\/qaTest.csv'\ndf_test = pd.read_csv(test_path, sep=';')\ndf_test.dtypes","c2c3a99a":"cat1 = df_test['category'].unique()\ncat2 = df_test['length'].unique()\n\nnewDF = pd.DataFrame(columns = df_test.columns)\nfor item1 in cat1:\n    for item2 in cat2:\n        temp = df_test.loc[(df_test['category'] == item1) & (df_test['length'] == item2)][:3]\n        newDF = pd.concat([newDF,temp])\nnewDF.reset_index(drop=True, inplace=True)\nnewDF","c14e0817":"# field names \nfields = ['query', ' category', ' wordcount', ' time', ' score_q1', ' q1', ' score_q2', ' q2', ' score_q3', ' q3', ' score_q4', ' q4', ' score_q5', ' q5'] \n    \n# data rows of csv file \nbm25rows = []\nrows = []\nceRows = []\nlstmRows = []\n\nres = [programSearch(row[2], row[0], row[1]) for row in zip(newDF['category'], newDF['wordCount'], newDF['question'])]","19917d0f":"with open('lstmResult.csv', 'w') as f: \n    # using csv.writer method from CSV package\n    write = csv.writer(f)\n      \n    write.writerow(fields)\n    write.writerows(lstmRows)","7d2a7cc8":"with open('semanticSearchResult.csv', 'w') as f: \n    # using csv.writer method from CSV package\n    write = csv.writer(f)\n      \n    write.writerow(fields)\n    write.writerows(rows)","a5d7fd1b":"with open('crossEncoderResult.csv', 'w') as f: \n    # using csv.writer method from CSV package\n    write = csv.writer(f)\n      \n    write.writerow(fields)\n    write.writerows(ceRows)","1becb194":"programSearch(\"malem dok saya mau tanya tante saya mengeluh nyeri bahu karna kerja disawah sampai sore lalu beliau beli obat d apotik dexametaaone proxicam dan natrium diclofenac sdh diminum 2 hari dgn dosis 3x sehari lalu sekarang beliau mengeluh nyeri ulu hati dan susah tidur setelah satu hari stop minum obat2 itu lalu beliau minum lg malam ini promag sebelun makan dan dexa sesudah makan apakah aman promag dan dexa dok diminum\", \"0\", \"0\")","b53da36d":"search(\"Saat ini saya serang menggunakan KB IUD, sebelumnya menstruasi lancar dengan siklus 28 hari, tapi sudah 8 bulan terakhir saya tdk mensruasi. Saya sempat konsultasi dengan bidan di puskesmas, tapi bid an tdk bisa mendiagnosa sebabnya. Sebagai inform as I, saya per nah menderita mioma tapi sudah diangkat sekitar 9 tahun laku. Pertanyaan saya, mungkinkah saya sudah memasuki masa manapouse, Sekarang umur saya 44 tahun. Adakah kemungkinan mioma tumbuh kembali? Mohon penjelasannya, Terima kasih.\")","2a26427c":"search(\"Sore dok saya mau nanya, ada bayangan di dada setelah ronsen itu karena jatuh dari kereta terbentur kepalanya tapi batuk darah. itu darah darimana ya dok kepala atau paru. di cek parunya hasil jantung tidak membesar ctr 50 aorta elogansi. mediastunum superior tidak melebar. trakea di tengah. kedua hillus suram. infiltrat parahiller dan parakardial kanan kiri. kedua hemidiafragma licin. kedua sinus kostofrenikus lancip. tidak tampak jelas fraktur costae. kesimpulan infiltrat paru. Foto ronsen nya mau saya kirim kemana dok Kata dokter kemungkinan TBC, bisa resepkan obat ga dok. terima kasih.\")","b056e5ca":"search(\"\")","0eb61b23":"search(\"Apa saja perbedaan asam lambung dengan maag?\")","10b38820":"## Import test data","5ce5725d":"## 1. Semantic Search","0e95217e":"# Search Functions","9dc7302d":"# Saving testing output to CSV","c06abc9b":"# Cross Encoder","8e5799ab":"# Encoding corpus menjadi embedding","42256908":"# Sandbox","37efba5d":"## Testing process","cbdacf41":"## 2. BM25 Search","8a937579":"# Similarity Functions","61263f34":"# Stopwords","ae7e4967":"# Semantic Search using SBERT on Quora Questions dataset\n\nThis script contains an example how to perform semantic search with PyTorch. It performs exact nearest neighborh search.\n\n\nAs embeddings model, we use the translated transfer learning SBERT model from 'quora-distilbert-multilingual',\nthat it aligned for 100 languages. I.e., you can type in a question in various languages and it will\nreturn the closest questions in the corpus (questions in the corpus are mainly in English).\n","29776e4b":"# Testing","83afcf32":"# Search Functions","ef6fb673":"# Preproses"}}