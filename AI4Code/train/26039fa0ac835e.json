{"cell_type":{"be150200":"code","fe6aaa54":"code","7dbfa3dd":"code","18f243c9":"code","e8f37bcc":"markdown","c3f0084f":"markdown","4a6e2ffa":"markdown","bd5c0c51":"markdown","c2c0429d":"markdown","8110658d":"markdown","5c52a5b8":"markdown","a74b02e5":"markdown","7ba2c3c9":"markdown","87efdd20":"markdown","ca6a64dc":"markdown","cc6ce8bf":"markdown","02ae7328":"markdown","bfba49b6":"markdown","3b660072":"markdown","6a5e585c":"markdown","7b5aff8d":"markdown","a1e4a4cd":"markdown","7bf2cc68":"markdown","43416406":"markdown","48724f27":"markdown","90436b29":"markdown","0fb74f27":"markdown","66f8b070":"markdown","6512475d":"markdown","84aeb94d":"markdown"},"source":{"be150200":"import logging\nimport time\nimport warnings\n\nimport catboost as cb\nimport joblib\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.compose import (\n    ColumnTransformer,\n    make_column_selector,\n    make_column_transformer,\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\", level=logging.INFO\n)\n\nwarnings.filterwarnings(\"ignore\")","fe6aaa54":"import optuna  # pip install optuna\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\n\ndef objective(trial, X, y):\n    param_grid = {}  # to be filled in later\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = lgbm.LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"binary_logloss\",\n            early_stopping_rounds=100,\n        )\n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = preds\n\n    return np.mean(cv_scores)","7dbfa3dd":"def objective(trial, X, y):\n    param_grid = {\n        #         \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 300),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.95, step=0.1\n        ),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.95, step=0.1\n        ),\n    }\n\n    ...","18f243c9":"from optuna.integration import LightGBMPruningCallback\n\n\ndef objective(trial, X, y):\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.95, step=0.1\n        ),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.95, step=0.1\n        ),\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = lgbm.LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"binary_logloss\",\n            early_stopping_rounds=100,\n            callbacks=[\n                LightGBMPruningCallback(trial, \"binary_logloss\")\n            ],  # Add a pruning callback\n        )\n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = log_loss(y_test, preds)\n\n    return np.mean(cv_scores)","e8f37bcc":"That\u2019s it! You are now a pro LGBM user. If you implement the things you learned in these two notebooks, believe me, you are already better than many Kagglers who use LightGBM.\n\nThat\u2019s because you have a deeper understanding of how the library works, what its parameters represent, and skillfully tune them. This type of fundamental knowledge of a library is always better than rampant code reuse without an ounce of understanding.\n\nTo move from pro to master, I suggest spending some time on the [documentation](https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html). Thank you for reading!\n\n![image.png](attachment:5a2d78d9-213d-4594-89ed-ac6f1577ab60.png)","c3f0084f":"Generally, hyperparameters of the most tree-based models can be grouped into 4 categories:\n1. Parameters that affect the structure and learning of the decision trees\n2. Parameters that affect the training speed\n3. Parameters for better accuracy\n4. Parameters to combat overfitting\n\nMost of the time, these categories have a lot of overlap, and increasing efficiency in one may risk a decrease in another. That\u2019s why tuning them manually is a giant mistake and should be avoided.\n\nFrameworks like Optuna can automatically find the \u201csweet medium\u201d between these categories if given a good enough parameter grid.","4a6e2ffa":"To this grid, I also added `LightGBMPruningCallback` from Optuna's `integration` module. This callback class is handy - it can detect unpromising hyperparameter sets before training them on the data, reducing the search time significantly.\n\nYou should pass it to LGBM\u2019s `fit` method under `callbacks` and set the `trial` object and the evaluation metric you are using as parameters.\n\nNow, let\u2019s create the study and run a few trials:","bd5c0c51":"```python\nprint(f\"\\tBest value (rmse): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")\n    \n-----------------------------------------------------\nBest value (rmse): 0.34926\n\tBest params:\n\t\tdevice_type: gpu\n\t\tn_estimators: 10000\n\t\tlearning_rate: 0.2423075935828885\n\t\tnum_leaves: 2260\n\t\tmax_depth: 9\n\t\tmin_data_in_leaf: 8600\n\t\tlambda_l1: 70\n\t\tlambda_l2: 35\n\t\tmin_gain_to_split: 0.11775633820897208\n\t\tbagging_fraction: 0.6000000000000001\n\t\tbagging_freq: 1\n\t\tfeature_fraction: 0.6000000000000001\n```","c2c0429d":"# More hyperparameters to control overfitting","8110658d":"In the previous notebook, we talked about the basics of LightGBM and creating LGBM models that beat XGBoost in almost every aspect. This notebook focuses on the last stage of any machine learning project \u2014 hyperparameter tuning (if we omit model ensembling).\n\nFirst, we will look at the most important LGBM hyperparameters, grouped by their impact level and area. Then, we will see a hands-on example of tuning LGBM parameters using Optuna \u2014 the next-generation bayesian hyperparameter tuning framework.\n\nMost importantly, we will do this in a similar way to how top Kagglers tune their LGBM models that achieve impressive results.","5c52a5b8":"LGBM also has important regularization parameters.\n\n`lambda_l1` and `lambda_l2` specifies L1 or L2 regularization, like XGBoost's `reg_lambda` and `reg_alpha`. The optimal value for these parameters is harder to tune because their magnitude is not directly correlated with overfitting. However, a good search range is (0, 100) for both.\n\nNext, we have `min_gain_to_split`, similar to XGBoost's `gamma`. A conservative search range is (0, 15). It can be used as extra regularization in large parameter grids.\n\nLastly, we have `bagging_fraction` and `feature_fraction`. `bagging_fraction` takes a value within (0, 1) and specifies the percentage of training samples to be used to train each tree (exactly like `subsample` in XGBoost). To use this parameter, you also need to set `bagging_freq` to an integer value, explanation [here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html#:~:text=frequency%20for%20bagging).\n\n`feature_fraction` specifies the percentage of features to sample when training each tree. So, it also takes a value between (0, 1) - XGBoost alias is `colsample_bytree`.\n\nWe have already covered other parameters that affect overfitting (`max_depth`, `num_leaves`, etc.) in earlier sections.","a74b02e5":"In the above `objective` function, we haven't specified the grid yet.\n\nIt is optional, but we are performing training inside cross-validation. This ensures that each hyperparameter candidate set gets trained on full data and evaluated more robustly. It also enables us to use early stopping. At the last line, we are returning the mean of the CV scores, which we want to optimize.\n\nLet\u2019s focus on creating the grid now. We will include the hyperparameters introduced today with their recommended search ranges:","7ba2c3c9":"# You might also be interested...\n- [Tired of Clich\u00e9 Datasets? Here are 18 Awesome Alternatives From All Domains](https:\/\/towardsdatascience.com\/tired-of-clich%C3%A9-datasets-here-are-18-awesome-alternatives-from-all-domains-196913161ec9)\n- [Love 3Blue1Brown Animations? Learn How to Create Your Own in Python in 10 Minutes](https:\/\/towardsdatascience.com\/love-3blue1brown-animations-learn-how-to-create-your-own-in-python-in-10-minutes-8e0430cf3a6d)\n- [Yes, These Unbelievable Masterpieces Are Created With Matplotlib](https:\/\/ibexorigin.medium.com\/yes-these-unbelievable-masterpieces-are-created-with-matplotlib-2256a4c54b12)\n- [7 Cool Python Packages Kagglers Are Using Without Telling You](https:\/\/towardsdatascience.com\/7-cool-python-packages-kagglers-are-using-without-telling-you-e83298781cf4)\n- [25 Pandas Functions You Didn\u2019t Know Existed | P(Guarantee) = 0.8](https:\/\/towardsdatascience.com\/25-pandas-functions-you-didnt-know-existed-p-guarantee-0-8-1a05dcaad5d0)\n- [20 Burning XGBoost FAQs Answered to Use the Library Like a Pro](https:\/\/towardsdatascience.com\/20-burning-xgboost-faqs-answered-to-use-the-library-like-a-pro-f8013b8df3e4?source=your_stories_page-------------------------------------)","87efdd20":"The optimization process in Optuna requires a function called *objective* that:\n- includes the parameter grid to search as a dictionary\n- creates a model to try hyperparameter combination sets\n- fits the model to the data with a single candidate set\n- generates predictions using this model\n- scores the predictions based on user-defined metric and returns it\n\nHere is how it looks like in code:","ca6a64dc":"# Kaggler's Guide to LightGBM Hyperparameter Tuning with Optuna in 2021\n![](https:\/\/cdn-images-1.medium.com\/max\/1500\/1*9yLM3nI8IR5I7w2A2mdQlQ.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/www.pexels.com\/@pixabay?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pixabay<\/a>\n        on \n        <a href='https:\/\/www.pexels.com\/photo\/silhouette-of-person-holding-sparkler-digital-wallpaepr-266429\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels.<\/a> All images are by the author unless specified otherwise.\n    <\/strong>\n<\/figcaption>","cc6ce8bf":"```python\nstudy = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\nfunc = lambda trial: objective(trial, X, y)\nstudy.optimize(func, n_trials=20)\n```","02ae7328":"It is time to start the search. Here is the full objective function for reference:","bfba49b6":"> I highly suggest reading the [first part](https:\/\/www.kaggle.com\/bextuychiev\/how-to-beat-the-heck-out-of-xgboost-with-lightgbm) of the notebook if you are new to LGBM. Although I will briefly explain how Optuna works, I also recommend reading my [separate notebook](https:\/\/www.kaggle.com\/bextuychiev\/no-bs-guide-to-hyperparameter-tuning-with-optuna) on it to get the best out this one.","3b660072":"> If you are not familiar with decision trees, check out [this legendary video](https:\/\/www.youtube.com\/watch?v=_L39rN6gz7Y) by StatQuest.\n\nIn LGBM, the most important parameter to control the tree structure is `num_leaves`. As the name suggests, it controls the number of decision leaves in a single tree. The decision leaf of a tree is the node where the 'actual decision' happens.\n\nThe next is `max_depth`. The higher `max_depth`, the more levels the tree has, which makes it more complex and prone to overfit. Too low, and you will underfit. Even though it sounds hard, it is the easiest parameter to tune \u2014 just choose a value between 3 and 12 (this range tends to work well on Kaggle for any dataset).\n\nTuning `num_leaves` can also be easy once you determine `max_depth`. There is a simple formula given in LGBM documentation - the maximum limit to `num_leaves` should be 2^(max_depth). This means the optimal value for `num_leaves` lies within the range $(2^3, 2^{12})$ or $(8, 4096)$.\n\nHowever, `num_leaves` impacts the learning in LGBM more than `max_depth`. This means you need to specify a more conservative search range like (20, 3000) - that's what I mostly do.\n\nAnother important structural parameter for a tree is `min_data_in_leaf`. Its magnitude is also correlated to whether you overfit or not. In simple terms, `min_data_in_leaf` specifies the minimum number of observations that fit the decision criteria in a leaf.\n\nFor example, if the decision leaf checks whether one feature is greater than, let\u2019s say, 13 \u2014 setting `min_data_in_leaf` to 100 means we want to evaluate this leaf only if at least 100 training observations are bigger than 13. This is the gist in my lay terms.\n\nThe optimal value for `min_data_in_leaf` depends on the number of training samples and `num_leaves`. For large datasets, set a value in hundreds or thousands.\n\nCheck out [this section](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html#tune-parameters-for-the-leaf-wise-best-first-tree) of the LGBM documentation for more details.","6a5e585c":"# Hyperparameters that control the tree structure","7b5aff8d":"# Overview of the most important parameters","a1e4a4cd":"# Creating the search grid in Optuna","7bf2cc68":"A common strategy for achieving higher accuracy is to use many decision trees and decrease the learning rate. In other words, find the best combination of `n_estimators` and `learning_rate` in LGBM.\n\n`n_estimators` controls the number of decision trees while `learning_rate` is the step size parameter of the gradient descent.\n\nEnsembles like LGBM build trees in iterations, and each new tree is used to correct the \u201cerrors\u201d of the previous trees. This approach is fast and powerful, and prone to overfitting.\n\nThat\u2019s why gradient boosted ensembles have a `learning_rate` parameter that controls the learning speed. Typical values lie within 0.01 and 0.3, but it is possible to go beyond these, especially towards 0.\n\nSo, the perfect setup for these 2 parameters (`n_estimators` and `learning_rate`) is to use many trees with early stopping and set a low value for `learning_rate`. We will see an example later.\n\nYou can also increase `max_bin` than the default (255) but again, at the risk of overfitting.\n\nCheck out [this section](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html#for-better-accuracy) of the LGBM documentation for more details.","43416406":"# Hyperparameters for better accuracy","48724f27":"# Conclusion","90436b29":"# Introduction","0fb74f27":"# Setup","66f8b070":"> If you don't understand the above grid or the `trial` object, check out my [article](https:\/\/towardsdatascience.com\/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c) on Optuna.","6512475d":"# Creating Optuna study and runing trials","84aeb94d":"After the search is done, just call `best_value` and `bast_params` attributes and you will get an output similar to this:"}}