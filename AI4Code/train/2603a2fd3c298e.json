{"cell_type":{"be8c3fb5":"code","7d46567b":"code","b4cb9900":"code","588ddfd7":"code","c98e249d":"code","c8be9e51":"code","69281e1e":"code","6fb5c93c":"code","b055a4de":"code","c4434940":"code","288f0c46":"code","a56a834c":"code","81ac380d":"code","cca47ed1":"code","2abaadaa":"code","3102d0e2":"code","9d6c270f":"code","0a71d231":"code","9cf88f22":"code","baf57879":"code","1cd2a663":"code","7bd4e1a8":"code","4e87b612":"code","822af703":"code","73ac5949":"code","7de233ba":"code","579e5dfb":"code","1f955861":"code","51ebcb39":"code","579f5b6b":"code","c4ae9078":"code","40d77a37":"code","72bdc481":"code","5c1ed1ee":"code","e30717a5":"code","2c70e5bd":"code","cac30b58":"code","3fb050ce":"code","73e09fef":"code","5fd56209":"code","30378e75":"code","2b383988":"code","9230a17b":"code","4c0da851":"markdown","1db4fc57":"markdown","68a76421":"markdown","a40abb1a":"markdown","6e0ca09b":"markdown","de3e6406":"markdown","70f56720":"markdown","64f9348b":"markdown","14e05efc":"markdown","daecc07e":"markdown","3882bb58":"markdown","b664cfb8":"markdown","641ea1fe":"markdown","0abb7f6f":"markdown","3a0b2200":"markdown","4a8b6136":"markdown","efe1b926":"markdown"},"source":{"be8c3fb5":"# The usual imports\nimport os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob ","7d46567b":"#\u00a0Some constants\ndebug = False\nDATA_DIR = '\/kaggle\/input\/airbus-ship-detection'\n\n# Directory to save logs and trained model\nROOT_DIR = '\/kaggle\/working'\nSEED = 314\n#\u00a0Probably corrupted images\nIMGS_TO_EXCLUDE = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n                   '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n                   'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n                   'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg']\n#\u00a0The image size (before augmentation)\nIMG_SIZE = (768, 768)","b4cb9900":"!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git\nos.chdir('Mask_RCNN')","588ddfd7":"# Import Mask RCNN\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","c98e249d":"train_dicom_dir = os.path.join(DATA_DIR, 'train_v2')\ntest_dicom_dir = os.path.join(DATA_DIR, 'test_v2')","c8be9e51":"!wget --quiet https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.0\/mask_rcnn_coco.h5\n!ls -lh mask_rcnn_coco.h5\n\nCOCO_WEIGHTS_PATH = \"mask_rcnn_coco.h5\"","69281e1e":"# TODO: Tweak these and see what happens.\n\nclass DetectorConfig(Config):    \n    # Give the configuration a recognizable name  \n    NAME = 'airbus'\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8\n    \n    BACKBONE = 'resnet50'\n    \n    NUM_CLASSES = 2  # background and ship classes\n    \n    IMAGE_MIN_DIM = 384\n    IMAGE_MAX_DIM = 384\n    RPN_ANCHOR_SCALES = (8, 16, 32, 64)\n    TRAIN_ROIS_PER_IMAGE = 64\n    MAX_GT_INSTANCES = 14\n    DETECTION_MAX_INSTANCES = 14\n    DETECTION_MIN_CONFIDENCE = 0.95\n    DETECTION_NMS_THRESHOLD = 0.0\n\n    STEPS_PER_EPOCH = 12 if debug else 120\n    VALIDATION_STEPS = 10 if debug else 100\n\nconfig = DetectorConfig()\nconfig.display()","6fb5c93c":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import get_cmap\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util import montage\nfrom skimage.morphology import binary_opening, disk, label\nimport gc; gc.enable() # memory is tight\n\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n\ndef multi_rle_encode(img, **kwargs):\n    '''\n    Encode connected regions as separated masks\n    '''\n    labels = label(img)\n    if img.ndim > 2:\n        return [rle_encode(np.sum(labels==k, axis=2), **kwargs) for k in np.unique(labels[labels>0])]\n    else:\n        return [rle_encode(labels==k, **kwargs) for k in np.unique(labels[labels>0])]\n\n# ref: https:\/\/www.kaggle.com\/paulorzp\/run-length-encode-and-decode\ndef rle_encode(img, min_max_threshold=1e-3, max_mean_threshold=None):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    if np.max(img) < min_max_threshold:\n        return '' ## no need to encode if it's all zeros\n    if max_mean_threshold and np.mean(img) > max_mean_threshold:\n        return '' ## ignore overfilled mask\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef masks_as_image(in_mask_list):\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros((768, 768), dtype = np.uint8)\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks |= rle_decode(mask)\n    return all_masks\n\ndef masks_as_color(in_mask_list):\n    # Take the individual ship masks and create a color mask array for each ships\n    all_masks = np.zeros((768, 768), dtype = np.float)\n    scale = lambda x: (len(in_mask_list)+x+1) \/ (len(in_mask_list)*2) ## scale the heatmap image to shift \n    for i,mask in enumerate(in_mask_list):\n        if isinstance(mask, str):\n            all_masks[:,:] += scale(i) * rle_decode(mask)\n    return all_masks","b055a4de":"from PIL import Image\nfrom sklearn.model_selection import train_test_split\n\ntrain_names = [f for f in os.listdir(train_dicom_dir)]\ntest_names = [f for f in os.listdir(test_dicom_dir)]\nfor el in IMGS_TO_EXCLUDE:\n    if(el in train_names): train_names.remove(el)\n    if(el in test_names): test_names.remove(el)","c4434940":"f'There are {len(train_names)} train images, and {len(test_names)} test ones'","288f0c46":"# Segmentation train dataset: link between train images and train RLE masks.\nSEGMENTATION_PATH = os.path.join(DATA_DIR, 'train_ship_segmentations_v2.csv')\nsegmentation_df = pd.read_csv(SEGMENTATION_PATH)\nsegmentation_df.sample(5)","a56a834c":"segmentation_df.groupby('ImageId')['EncodedPixels'].count().hist()","81ac380d":"(segmentation_df.groupby('ImageId')['EncodedPixels']\n               .count()\n               .value_counts(normalize=True)\n               .mul(100)\n               .to_dict())","cca47ed1":"#\u00a0Select unique train files with at least one ship mask.\nships_train_names = (segmentation_df.loc[segmentation_df.EncodedPixels.notnull(), \n                                        'ImageId']\n                                    .unique())\nno_ships_train_names = (segmentation_df.loc[segmentation_df.EncodedPixels.isnull(), \n                                        'ImageId']\n                                    .unique())","2abaadaa":"(f'There are {len(ships_train_names)} unique train images with at least one shipe and '\n  f'{len(no_ships_train_names)} without any')","3102d0e2":"# TODO: Add some comments.\ntest_size = config.VALIDATION_STEPS * config.IMAGES_PER_GPU\nimage_fps_train, image_fps_val = train_test_split(ships_train_names, \n                                                  test_size=test_size, \n                                                  random_state=SEED)\n\nprint(len(image_fps_train), len(image_fps_val), len(test_names))","9d6c270f":"#\u00a0TODO: Add some documentation\n\nclass DetectorDataset(utils.Dataset):\n    \"\"\"Dataset class for training our dataset.\n    \"\"\"\n\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n        \n        # Add classes\n        self.add_class('ship', 1, 'Ship')\n        \n        # add images \n        for i, fp in enumerate(image_fps):\n            annotations = image_annotations.query('ImageId==\"' + fp + '\"')['EncodedPixels']\n            self.add_image('ship', image_id=i, path=os.path.join(train_dicom_dir, fp), \n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        image = imread(fp)\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n        return image\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                mask[:, :, i] = rle_decode(a)\n                class_ids[i] = 1\n        return mask.astype(np.bool), class_ids.astype(np.int32)","0a71d231":"# Reassign variables for Mask R-CNN\nimage_fps, image_annotations = train_names, segmentation_df","9cf88f22":"ds = imread(os.path.join(train_dicom_dir, train_names[10])) \n_ = plt.imshow(ds)","baf57879":"%%time\n# prepare the training dataset\ndataset_train = DetectorDataset(image_fps_train, image_annotations, *IMG_SIZE)\ndataset_train.prepare()","1cd2a663":"%%time\n# prepare the validation dataset\ndataset_val = DetectorDataset(image_fps_val, image_annotations, *IMG_SIZE)\ndataset_val.prepare()","7bd4e1a8":"# Load and display random sample and their bounding boxes\n\nclass_ids = [0]\nwhile class_ids[0] == 0:  ## look for a mask\n    image_id = random.choice(dataset_val.image_ids)\n    image_fp = dataset_val.image_reference(image_id)\n    image = dataset_val.load_image(image_id)\n    mask, class_ids = dataset_val.load_mask(image_id)\n\nprint(image.shape)\n\nplt.figure(figsize=(10, 10))\nplt.subplot(1, 2, 1)\nplt.imshow(image)\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nmasked = np.zeros(image.shape[:2])\nfor i in range(mask.shape[2]):\n    masked += mask[:, :, i] ## * image[:, :, 0]\nplt.imshow(masked, cmap='gray')\nplt.axis('off')\n\nprint(image_fp)\nprint(class_ids)","4e87b612":"# Image augmentation (light but constant)\naugmentation = iaa.Sequential([\n    iaa.OneOf([ ## rotate\n        iaa.Affine(rotate=0),\n        iaa.Affine(rotate=90),\n        iaa.Affine(rotate=180),\n        iaa.Affine(rotate=270),\n    ]),\n    iaa.Fliplr(0.5),\n    iaa.Flipud(0.5),\n    iaa.OneOf([ ## brightness or contrast\n        iaa.Multiply((0.9, 1.1)),\n        iaa.ContrastNormalization((0.9, 1.1)),\n    ]),\n    iaa.OneOf([ ## blur or sharpen\n        iaa.GaussianBlur(sigma=(0.0, 0.1)),\n        iaa.Sharpen(alpha=(0.0, 0.1)),\n    ]),\n])\n\n# test on the same image as above\nimggrid = augmentation.draw_grid(image, cols=5, rows=2)\nplt.figure(figsize=(30, 12))\n_ = plt.imshow(imggrid.astype(int))","822af703":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)\n\n# Exclude the last layers because they require a matching\n# number of classes\nmodel.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\n    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n    \"mrcnn_bbox\", \"mrcnn_mask\"])","73ac5949":"#\u00a0TODO: Is this the optimal one? Investigate...\nSTART_LEARNING_RATE = 0.006\n\n# Train Mask-RCNN Model \nimport warnings \nwarnings.filterwarnings(\"ignore\")","7de233ba":"def training_strategy(start_learning_rate):\n\n    model.train(dataset_train, dataset_val,\n                learning_rate=start_learning_rate,\n                epochs=2,\n                layers='heads',\n                augmentation=None)  ## no need to augment yet\n\n\n    model.train(dataset_train, dataset_val,\n                learning_rate=start_learning_rate \/ 2,\n                epochs=12,\n                layers='all',\n                augmentation=augmentation)\n\n\n    model.train(dataset_train, dataset_val,\n                learning_rate= start_learning_rate \/ 4,\n                epochs=20,\n                layers='all',\n                augmentation=augmentation)\n    \n\n\n    return model","579e5dfb":"%time model = training_strategy(START_LEARNING_RATE)","1f955861":"history = model.keras_model.history.history","51ebcb39":"plt.figure(figsize=(17,5))\n\nplt.subplot(131)\nplt.plot(epochs, history[\"loss\"], label=\"Train loss\")\nplt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\nplt.legend()\nplt.subplot(132)\nplt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train class ce\")\nplt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid class ce\")\nplt.legend()\nplt.subplot(133)\nplt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train box loss\")\nplt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid box loss\")\nplt.legend()\n\nplt.show()","579f5b6b":"best_epoch = np.argmin(history[\"val_loss\"])\nscore = history[\"val_loss\"][best_epoch]\nprint(f'Best Epoch:{best_epoch+1} val_loss:{score}')","c4ae9078":"# select trained model \ndir_names = next(os.walk(model.model_dir))[1]\nkey = config.NAME.lower()\ndir_names = filter(lambda f: f.startswith(key), dir_names)\ndir_names = sorted(dir_names)\n\nif not dir_names:\n    import errno\n    raise FileNotFoundError(\n        errno.ENOENT,\n        \"Could not find model directory under {}\".format(self.model_dir))\n    \nfps = []\n# Pick last directory\nfor d in dir_names: \n    dir_name = os.path.join(model.model_dir, d)\n    # Find the last checkpoint\n    checkpoints = next(os.walk(dir_name))[2]\n    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n    checkpoints = sorted(checkpoints)\n    if not checkpoints:\n        print('No weight files in {}'.format(dir_name))\n    else:\n        checkpoint = os.path.join(dir_name, checkpoints[best_epoch])\n        fps.append(checkpoint)\n\nmodel_path = sorted(fps)[-1]\nprint('Found model {}'.format(model_path))","40d77a37":"class InferenceConfig(DetectorConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","72bdc481":"# set color for class\ndef get_colors_for_class_ids(class_ids):\n    colors = []\n    for class_id in class_ids:\n        if class_id == 1:\n            colors.append((.941, .204, .204))\n    return colors","5c1ed1ee":"# Show few example of ground truth vs. predictions on the validation dataset \ndataset = dataset_val\nfig = plt.figure(figsize=(10, 40))\n\nfor i in range(8):\n\n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    \n    print(original_image.shape)\n    plt.subplot(8, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names,\n                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n    \n    plt.subplot(8, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], \n                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])","e30717a5":"# Get filenames of test dataset images\ntest_image_fps = test_names","2c70e5bd":"DETECTION_TEST_PRED = '\/kaggle\/input\/fine-tuning-resnet34-on-ship-detection-new-data\/ship_detection.csv'\nship_detection = pd.read_csv(DETECTION_TEST_PRED)\nship_detection.sample(5)","cac30b58":"SHIP_THRESHOLD = 0.5\ntest_names = ship_detection.loc[ship_detection['p_ship'] > SHIP_THRESHOLD, \n                                ['id']]['id'].values.tolist()\ntest_names_nothing = ship_detection.loc[ship_detection['p_ship'] <= SHIP_THRESHOLD, \n                                        ['id']]['id'].values.tolist()\n\n\nprint(f'The classification model predicted {len(test_names)} images with ships and ' \n      f'{len(test_names_nothing)} without')","3fb050ce":"# TODO: Check if it is possible to make this run faster...\n# Make predictions on test images, write out sample submission\ndef predict(image_fps, filepath='submission.csv', \n            min_conf=config.DETECTION_MIN_CONFIDENCE):\n    # assume square image\n    resize_factor = IMG_SIZE[0] \/ config.IMAGE_SHAPE[0]\n    with open(filepath, 'w') as file:\n        file.write(\"ImageId,EncodedPixels\\n\")\n\n        for image_id in tqdm(image_fps):\n            found = False\n            \n            image = imread(os.path.join(test_dicom_dir, image_id))\n            # If grayscale. Convert to RGB for consistency.\n            if len(image.shape) != 3 or image.shape[2] != 3:\n                image = np.stack((image,) * 3, -1)\n            image, window, scale, padding, crop = utils.resize_image(\n                image,\n                min_dim=config.IMAGE_MIN_DIM,\n                min_scale=config.IMAGE_MIN_SCALE,\n                max_dim=config.IMAGE_MAX_DIM,\n                mode=config.IMAGE_RESIZE_MODE)\n\n            results = model.detect([image])\n            r = results[0]\n\n            assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n            if len(r['rois']) == 0 or image_id in test_names_nothing:\n                pass  ## no ship\n            else:\n                num_instances = len(r['rois'])\n\n                for i in range(num_instances):\n                    if r['scores'][i] > min_conf:\n                        file.write(image_id + \",\" + rle_encode(r['masks'][...,i]) + \"\\n\")\n                        found = True\n\n            if not found:\n                file.write(image_id + \",\\n\")  ## no ship","73e09fef":"submission_fp = os.path.join(ROOT_DIR, 'submission.csv')\npredict(test_image_fps, filepath=submission_fp)\nprint(submission_fp)","5fd56209":"submission_df = pd.read_csv(submission_fp)\nsubmission_df.sample(5)","30378e75":"# show a few test image detection example\ndef visualize_test(): \n    image_id = random.choice(test_names)\n    \n    # original image\n    image = imread(os.path.join(test_dicom_dir, image_id))\n    \n    # assume square image \n    resize_factor = IMG_SIZE[0] \/ config.IMAGE_SHAPE[0]\n    \n    # If grayscale. Convert to RGB for consistency.\n    if len(image.shape) != 3 or image.shape[2] != 3:\n        image = np.stack((image,) * 3, -1) \n    resized_image, window, scale, padding, crop = utils.resize_image(\n        image,\n        min_dim=config.IMAGE_MIN_DIM,\n        min_scale=config.IMAGE_MIN_SCALE,\n        max_dim=config.IMAGE_MAX_DIM,\n        mode=config.IMAGE_RESIZE_MODE)\n\n    results = model.detect([resized_image])\n    r = results[0]\n    for bbox in r['rois']: \n        x1 = int(bbox[1] * resize_factor)\n        y1 = int(bbox[0] * resize_factor)\n        x2 = int(bbox[3] * resize_factor)\n        y2 = int(bbox[2]  * resize_factor)\n        cv2.rectangle(image, (x1,y1), (x2,y2), (77, 255, 9), 3, 1)\n        width = x2 - x1 \n        height = y2 - y1 \n    fig, ax = plt.subplots()\n    ax.set_title(f\"{len(r['rois'])}: {image_id}\")\n    plt.imshow(image)\n\n","2b383988":"for i in range(10):\n    visualize_test()","9230a17b":"# remove files to allow committing (hit files limit otherwise)\n!rm -rf \/kaggle\/working\/Mask_RCNN","4c0da851":"### Image Augmentation. Try finetuning some variables to custom values","1db4fc57":"### Create and prepare the training dataset using the DetectorDataset class.","68a76421":"I have forked the great work from [Henrique Mendon\u00e7a](https:\/\/www.kaggle.com\/hmendonca) and added some annotations (pun not-intended :p)\nmainly for myself since I am new to the [Mask R-CNN](https:\/\/github.com\/matterport\/Mask_RCNN) model.\n\nDon't forget to check (and upvote) the original notebook [here](https:\/\/www.kaggle.com\/hmendonca\/airbus-mask-rcnn-and-coco-transfer-learning).\n\nEnjoy!","a40abb1a":"As you can see, most train images are empty. What are the percentages?","6e0ca09b":"### Install Matterport's Mask-RCNN model from github.\nSee the [Matterport's implementation of Mask-RCNN](https:\/\/github.com\/matterport\/Mask_RCNN).","de3e6406":"The strategy here is to train the model in three phases: \n    \n1. Bigger learning rate with only the heads layers and no augmentation for few epochs (for 2 epochs here)\n2. Smaller learning rate (half the previous one) with all the layers (for 12 epochs here)\n3. Even smaller learning rate (half the previous one) with all the layers (for 20 epochs here)\n\nNotice that this strategy could be refined of course and the various \nhyperparamters could be improved.","70f56720":"### Examine the annotation data, parse the dataset, and view dicom fields","64f9348b":"### Download COCO pre-trained weights","14e05efc":"#\u00a0Train, test, and segmentation files exploration","daecc07e":"# Check the trained model's predictions","3882bb58":"### Now it's time to train the model. Note that training even a basic model can take a few hours. \n\nNote: the following model is for demonstration purpose only. We have limited the training to one epoch, and have set nominal values for the Detector Configuration to reduce run-time. \n\n- dataset_train and dataset_val are derived from DetectorDataset \n- DetectorDataset loads images from image filenames and  masks from the annotation data\n- model is Mask-RCNN","b664cfb8":"### Some setup functions and classes for Mask-RCNN\n\n- dicom_fps is a list of the dicom image path and filenames \n- image_annotions is a dictionary of the annotations keyed by the filenames\n- parsing the dataset returns a list of the image filenames and the annotations dictionary","641ea1fe":"##\u00a0Load ship vs no-ship classification result first","0abb7f6f":"Before you start exploring this notebook, here are some useful resources:\n    \n- What is the Mask R-CNN loss function? https:\/\/stackoverflow.com\/questions\/46272841\/what-is-the-loss-function-of-the-mask-rcnn. \n- Mask R-CNN original paper: https:\/\/arxiv.org\/pdf\/1703.06870.pdf. \n- A good blog post explaining the history behin Mask R-CNN: https:\/\/blog.athelas.com\/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4. \n- Mask R-CNN video presentation by [Kaiming He](http:\/\/kaiminghe.com\/): https:\/\/www.youtube.com\/watch?v=g7z4mkfRjI4","3a0b2200":"TODO: Group the various training variations into a single function.","4a8b6136":"#\u00a0Create the submission file","efe1b926":"### Display a random image with bounding boxes"}}