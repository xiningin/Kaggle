{"cell_type":{"2d8ba7d0":"code","701fde48":"code","872977ca":"code","9461641a":"code","e5db6de8":"code","72d9bf6f":"code","25671e79":"code","e56c9695":"code","d1fe7d58":"code","36b0d00d":"code","bef44ecd":"code","fbeca930":"markdown","e2803265":"markdown","625510b1":"markdown","38286e38":"markdown","b4667775":"markdown","d0080179":"markdown","f6bbd76f":"markdown","8f0a8fda":"markdown","74cb3f0d":"markdown"},"source":{"2d8ba7d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","701fde48":"data = pd.read_csv(\"\/kaggle\/input\/genderheightweightcsv\/weight-height.csv\")\ndata","872977ca":"data.Gender=[1 if each=='Male' else 0 for each in data.Gender]\ny = data.Gender.values\nx = data.drop([\"Gender\"],axis=1)\ny","9461641a":"x = (x-np.min(x))\/(np.max(x)-np.min(x)).values\nx","e5db6de8":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n","72d9bf6f":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b=0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","25671e79":"def forward_backward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -(y_train*np.log(y_head)+(1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss)\/(x_train.shape[1])\n    \n    derivative_weight = (np.dot(x_train,(y_head-y_train).T))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    \n    grad = {\"derivative_weight\":derivative_weight,\"derivative_bias\":derivative_bias}\n    \n    return cost,grad","e56c9695":"def update(w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list=[]\n    index=[]\n    for i in range(number_of_iteration):\n        cost , gradients = forward_backward_propagation(w,b,x_train,y_train)\n        w = w - learning_rate*gradients[\"derivative_weight\"]    \n        b = b - learning_rate*gradients[\"derivative_bias\"]\n        if i % 100 == 0:\n            cost_list.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    parameters={\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list","d1fe7d58":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if(z[0,i]<=0.5):\n            prediction[0,i]=0\n        else:\n            prediction[0,i]=1\n    return prediction","36b0d00d":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,number_of_iteration):\n    w,b = initialize_weights_and_bias(x_train.shape[0])\n    parameters,gradients,cost_list = update(w,b,x_train,y_train,learning_rate,number_of_iteration)\n    \n    prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(prediction - y_test)) * 100))","bef44ecd":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=0.3,number_of_iteration=3000)","fbeca930":"Let's import our data and look into it.","e2803265":"As you can see the table  genders are there.","625510b1":"Forward and backward propagation function.","38286e38":"Training and testing.","b4667775":"Let's do it all shorter.","d0080179":"Normalization.","f6bbd76f":" Did forward and backward propagataion and i got my costs and gradients , i need to update my weights and biases.","8f0a8fda":"It's time to start defining my functions.First of all i need to initialize my weights and bias, then i will need a sigmoid function.\n\n","74cb3f0d":"Prediction time for testing."}}