{"cell_type":{"64711a22":"code","eac652dc":"code","fdb07db4":"code","2bfafa8e":"code","a5de85db":"code","12493f79":"code","97c2b2dc":"code","1fbb549a":"code","00736e34":"code","65b876ce":"code","87e02cc6":"code","b1f9b8d5":"code","d69c5982":"code","44d43e02":"code","f29b0700":"code","b910d6e7":"code","bfee59fd":"code","d61b2a0b":"code","a8eba399":"code","99f04b99":"code","69b3234e":"code","55306a89":"code","92311247":"code","40ee53eb":"code","1df9d9f9":"code","35cd366b":"code","94c499b9":"code","44137e5b":"code","fc3e1bfe":"code","4f4529fc":"code","44bf6121":"code","18899345":"code","31d6fa77":"code","d58dcb1e":"code","cb592925":"code","c86095a0":"code","729ad633":"code","0861a36b":"code","724e1526":"code","28829a60":"code","2dacd897":"code","45bef26a":"code","ad485009":"code","7829ed04":"code","50eb2128":"code","69ce2f60":"code","f5bf6f20":"code","57919793":"code","86fe91f9":"code","f4b928f6":"code","2f25ea31":"code","92d06b96":"markdown","36e17f7d":"markdown","76d50ee2":"markdown","1b8e9b1d":"markdown","a91bc94b":"markdown","341cc078":"markdown","b39df04c":"markdown","7c157197":"markdown","9ac1f9dd":"markdown","d98b00f6":"markdown","cff817f0":"markdown","1d740aa1":"markdown","efd3af66":"markdown","750b2f3c":"markdown","434c77c3":"markdown","0d5cdeb9":"markdown","4a543321":"markdown","fb3eb652":"markdown","3d8ba217":"markdown","d81f39c1":"markdown","d0af14ed":"markdown","bb8c6108":"markdown","d5e9ad9e":"markdown","b6166437":"markdown","bd622f80":"markdown","ce424909":"markdown","7c86f3ed":"markdown","c2deff82":"markdown","d1928e7f":"markdown","83ac2e5a":"markdown","826699e0":"markdown","0ca37f4f":"markdown","73097b5c":"markdown","097d5470":"markdown","fbb45694":"markdown","60ba1e74":"markdown","daab3001":"markdown","01128a14":"markdown","d343c5ba":"markdown"},"source":{"64711a22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\n!pip install geohash2\n!pip install xlrd\n!pip install openpyxl\nimport geopy\nfrom geopy.distance import geodesic\nimport time\nimport shap\nimport operator\nimport math\n# load JS visualization code to notebook\nshap.initjs()\nimport gc\n#plot in a map\nimport folium\nfrom folium.plugins import MarkerCluster\nfrom folium import Choropleth, Circle, Marker\nfrom folium import plugins\nfrom folium.plugins import HeatMap\n# Models\n# Algorithmns models to be compared\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,BaggingClassifier, VotingClassifier, RandomTreesEmbedding\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.utils import resample\nfrom sklearn.metrics import roc_curve\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import make_scorer\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n#SIMILARITY IN ORDER TO CLEAN REDUNDANT VALUES\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nimport itertools\n# Model selection\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn import metrics\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, KFold\nfrom sklearn.model_selection import cross_validate, ShuffleSplit, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nimport statsmodels.api as sm\nfrom sklearn.metrics import precision_score,recall_score\nimport plotly.figure_factory as ff\n# Hyperparameter tuning and optimization\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom sklearn.feature_selection import RFE, RFECV\n#plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns #Graph library that use matplot in background\n# plotly libraries\nimport cufflinks as cf\ncf.go_offline()\nimport plotly.offline as py \npy.init_notebook_mode(connected=True) # this code, allow us to work with offline plotly version\nimport plotly.graph_objs as go # it's like \"plt\" of matplot\n# these two lines allow your code to show up in a notebook\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\nimport widgetsnbextension\nfrom IPython.display import display, HTML\n# normality tests\nfrom scipy import stats\n\n\n#display full dataframe\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eac652dc":"telco = pd.read_excel('\/kaggle\/input\/telco-customer-churn-ibm-dataset\/Telco_customer_churn.xlsx')","fdb07db4":"'''Set a seed for reproducibility'''\nseed = 1\n\n'''Set path where all data files are located'''\npath = 'C:\/Users\/Nikitas\/Desktop\/Nikitas\/other_pet_projects\/telco\/'\n\n'''Create load data function to easy import all necessary'''\ndef LoadData(path,file):\n    \"\"\"Load Data from defined directory.\n    Output is initial data dataframe.\"\"\"\n    data = pd.read_excel(path + file + '.xlsx')\n    print (\"Data is loaded!\")\n    print (\"Data: \",data.shape[0],\"observations, and \",data.shape[1],\"features\")\n    #\n    return data\n\n'''Create function for providing summary statistics in a table'''\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary\n\n'''Plot Numeric Features vs target churn in order to assess differences between churners and non churners'''\ndef TargetvsNumeric(data,numeric,target,numeric_name):\n\n\n    df1 = data[(data[numeric] >= 0) & \n                                  (data[target] == 'Yes')]\n    df2 = data[(data[numeric] >= 0) & \n                                  (data[target] == 'No')]\n\n    #figure size\n    title = 'Distribution of ' + numeric_name\n    suptitle = numeric_name + ' Distribution'\n    xlabel = numeric_name + ' Range'\n    plt_title = \"Distribution of \" + numeric_name + \" by Churn\"\n    plt.figure(figsize=(20,5))\n\n    plt.subplot(121)\n    plt.suptitle(suptitle, fontsize=20)\n    sns.distplot(data[(data[numeric] >= 0)][numeric], bins=24)\n    plt.title(title,fontsize=18)\n    plt.xlabel(xlabel,fontsize=15)\n    plt.ylabel(\"Probability\",fontsize=15)\n\n    plt.subplot(122)\n\n    sns.distplot(df1[numeric], bins=24, color='r', label='Churned')\n    sns.distplot(df2[numeric], bins=24, color='blue', label='Not Churned')\n    plt.title(plt_title,fontsize=18)\n    plt.xlabel(numeric_name,fontsize=15)\n    plt.ylabel(\"Probability\",fontsize=15)\n    plt.legend()\n\n\n    plt.show()\n\n'''Plot Categorical Features vs target churn in order to assess differences between churners and non churners'''\n\ndef plot_categoricals(df, col=None, cont = None, binary=None, dodge=True):\n    tmp = pd.crosstab(df[col], df[binary], normalize='index') * 100\n    tmp = tmp.reset_index()\n\n    plt.figure(figsize=(16,12))\n\n    plt.subplot(221)\n    g= sns.countplot(x=col, data=df, order=list(tmp[col].values) , color='green')\n    g.set_title(f'{col} Distribution', \n                fontsize=20)\n    g.set_xlabel(f'{col} Values',fontsize=17)\n    g.set_ylabel('Count Distribution', fontsize=17)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.0f}'.format(height),\n                ha=\"center\", fontsize=15) \n    g.set_ylim(0,max(sizes)*1.15)\n\n    plt.subplot(222)\n    g1= sns.countplot(x=col, data=df, order=list(tmp[col].values),\n                     hue=binary,palette=\"hls\")\n    g1.set_title(f'{col} Distribution by {binary} ratio %', \n                fontsize=20)\n    gt = g1.twinx()\n    gt = sns.pointplot(x=col, y='Yes', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False)\n    gt.set_ylim(0,tmp['Yes'].max()*1.1)\n    gt.set_ylabel(\"Churned %Ratio\", fontsize=16)\n    g1.set_ylabel('Count Distribution',fontsize=17)\n    g1.set_xlabel(f'{col} Values', fontsize=17)\n    \n    sizes = []\n    \n    for p in g1.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=10) \n    g1.set_ylim(0,max(sizes)*1.15)\n\n    plt.subplot(212)\n    g2= sns.swarmplot(x=col, y=cont, data=df, dodge=dodge, order=list(tmp[col].values),\n                     hue=binary,palette=\"hls\")\n    g2.set_title(f'{cont} Distribution by {col} and {binary}', \n                fontsize=20)\n    g2.set_ylabel(f'{cont} Distribution',fontsize=17)\n    g2.set_xlabel(f'{col} Values', fontsize=17)\n\n\n    plt.suptitle(f'{col} Distributions', fontsize=22)\n    plt.subplots_adjust(hspace = 0.4, top = 0.90)\n    \n    plt.show()\n\n'''Plot categories distribution ratio along with percentage of monthly charges by category. This way it can be identified which category results into higher loss in revenue'''\n    \ndef PlotDistRatio(df, col, lim=200):\n    tmp = pd.crosstab(df[col], df['Churn Value'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoChurn', 1:'Churn'}, inplace=True)\n\n    plt.figure(figsize=(20,5))\n    plt.suptitle(f'{col} Distributions ', fontsize=22)\n\n    plt.subplot(121)\n    g = sns.countplot(x=col, data=df, order=list(tmp[col].values))\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n    g.set_title(f\"{col} Distribution\\nCount and %Churn by each category\", fontsize=18)\n    g.set_ylim(0,4000)\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Churn', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,80)\n    gt.set_ylabel(\"% of Churn\", fontsize=16)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    for p in gt.patches:\n        height = p.get_height()\n        gt.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=14) \n    total_amt = df.groupby(['Churn Value'])['Monthly Charges'].sum()   \n    perc_amt = (df.groupby(['Churn Value',col])['Monthly Charges'].sum() \/ total_amt * 100).unstack('Churn Value')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoChurn', 1:'Churn'}, inplace=True)\n\n    plt.subplot(122)\n    g1 = sns.boxplot(x=col, y='Monthly Charges', hue='Churn Value', \n                     data=df[df['Monthly Charges'] <= lim], order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Churn', data=perc_amt, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,100)\n    g1t.set_ylabel(\"%Churn Total Monthly Charges\", fontsize=16)\n    g1.set_title(f\"{col} by Monthly Charges\", fontsize=18)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Monthly Charges($)\", fontsize=16)\n        \n    plt.subplots_adjust(hspace=.4, wspace = 0.35, top = 0.80)\n    \n    plt.show()    \n    \n    \n    \n    \n    \n'''Plot customers in the map to check density of customers in the map'''\n\ndef PlotLocations(data):\n    data['Latitude'] = data['Latitude'].astype(float)\n    data['Longitude'] = data['Longitude'].astype(float)\n    # Create the map\n    m_3 = folium.Map(location=[33.964131,-118.272783], tiles='cartodbpositron', zoom_start=13)\n\n    # Add points to the map\n    mc = MarkerCluster()\n    for idx, row in data.iterrows():\n        if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n            mc.add_child(Marker([row['Latitude'], row['Longitude']]))\n    m_3.add_child(mc)\n\n\n    data_churn = data[data['Churn Label'] == 'Yes']\n\n\n    data_churn = data_churn[['Latitude', 'Longitude']]\n    data_churn = data_churn.dropna(axis=0, subset=['Latitude','Longitude'])\n\n    # List comprehension to make out list of lists\n    heat_data = [[row['Latitude'],row['Longitude']] for index, row in data_churn.iterrows()]\n\n    # Plot it on the map\n    HeatMap(heat_data).add_to(m_3)\n\n    # Display the map\n    return m_3\n\n\n'''Calculate distance between two given locations with lat long coordinates'''\ndef calculate_distance(location, set_coord):\n    \"\"\" take two tuples of latitude and longitude and\n    calculate distance in km\"\"\"\n    locations = []\n\n    distance = geodesic(location, set_coord)\n    locations.append([distance.kilometers])\n    df = pd.DataFrame(locations, columns=['Distance'])\n    \n    return df['Distance'][0]\n\n\n'''Feature Engineer extra variables ready for ML fit'''\n\ndef FEngineering(df):\n    df = df.copy()\n    cols_binary = ['Online Security','Online Backup','Device Protection','Tech Support',\n              'Streaming TV','Streaming Movies','Phone Service','Multiple Lines',\n              'Senior Citizen','Partner','Dependents','Paperless Billing']\n    for col in cols_binary:\n\n        df[col] = np.where((df[col] == 'Yes'), 1,0)\n    #create binary if it is not month to month    \n    df['Engaged'] = np.where(df['Contract'] != 'Month-to-month', 1,0)\n    #male binary var    \n    df['Male'] = np.where((df['Gender'] == 'Male'), 1,0)\n    df['No internet service' + ' ohe'] = np.where((df['Online Security'] == 'No internet service'), 1,0)\n    # create one hot encoding for special cases in floor feature\n    special_cases_contract = ['Month-to-month', 'One year', 'Two year']\n    #\n    for case in special_cases_contract:\n        df[case + ' ohe'] = np.where((df['Contract'] == case), 1,0)\n\n    # create one hot encoding for special cases for every feature\n    special_payment_method = ['Mailed check', 'Electronic check', 'Bank transfer (automatic)','Credit card (automatic)']\n    #\n    for case in special_payment_method:\n        df[case + ' ohe'] = np.where((df['Payment Method'] == case), 1,0)\n\n    # create one hot encoding for special cases for every feature\n    special_internet_service = ['Fiber optic', 'DSL']\n\n    for case in special_internet_service:\n        df[case + ' ohe'] = np.where((df['Internet Service'] == case), 1,0)\n        \n        \n    df['No Protection'] = np.where((df['Online Backup'] != 'Yes') |\\\n                                    (df['Device Protection'] != 'Yes') |\\\n                                    (df['Tech Support'] != 'Yes'), 1,0)\n\n    df['Total Services'] = (df[['Online Security','Online Backup','Device Protection','Tech Support',\n              'Streaming TV','Streaming Movies','Phone Service','Multiple Lines','Paperless Billing']]== 'Yes').sum(axis=1)\n    \n    \n    #calculate dinstances\n    \n    san_francisco = (37.773972, -122.431297)\n    los_angeles = (34.052235, -118.243683)\n    san_diego = (32.715736, -117.161087)\n    sacramento = (38.575764, -121.478851)\n    # calculate distance features\n    df['san_francisco_distance'] = df.apply(lambda x: calculate_distance((x['Latitude'],x['Longitude']),san_francisco), axis = 1)\n    df['los_angeles_distance'] = df.apply(lambda x: calculate_distance((x['Latitude'],x['Longitude']),los_angeles), axis = 1)\n    df['san_diego_distance'] = df.apply(lambda x: calculate_distance((x['Latitude'],x['Longitude']),san_diego), axis = 1)\n    df['sacramento_distance'] = df.apply(lambda x: calculate_distance((x['Latitude'],x['Longitude']),sacramento), axis = 1)\n    \n    \n    #keep certain columns\n    df = df[['CustomerID','Churn Value',\n    'Tenure Months','Monthly Charges','CLTV', 'Engaged', 'Male',\n       'No internet service ohe', 'Month-to-month ohe', 'One year ohe',\n       'Two year ohe', 'Mailed check ohe', 'Electronic check ohe',\n       'Bank transfer (automatic) ohe', 'Credit card (automatic) ohe',\n       'Fiber optic ohe', 'DSL ohe', 'No Protection', 'Total Services',\n       'san_francisco_distance', 'los_angeles_distance', 'san_diego_distance',\n       'sacramento_distance','Online Security','Online Backup','Device Protection','Tech Support',\n              'Streaming TV','Streaming Movies','Phone Service','Multiple Lines',\n              'Senior Citizen','Partner','Dependents','Paperless Billing']].drop_duplicates()\n    print (\"Data is transformed!\")\n    print (\"Data: \",df.shape[0],\"observations, and \",df.shape[1],\"features\")\n\n    return df\n\n\ndef remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model \n        to generalize and improves the interpretability of the model.\n\n    Inputs: \n        x: features dataframe\n        threshold: features with correlations greater than this value are removed\n\n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n\n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i+1):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n\n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                #print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns=drops)\n    print('Removed Columns {}'.format(drops))\n    return x\n\n\n\n#dataframe from upper step and number of best variables to output\ndef variableselection(df,best,target,features_to_select,rand):\n\n        start = time.time()\n\n        #Creating the X and y variables\n\n        def XyCreation(df):\n               y = df[target].ravel()\n               y = np.array(y).astype(int)\n               X = pd.DataFrame(df.drop([target,'CustomerID'], axis=1))\n               x_train, x_test, y_train, y_test = train_test_split(X, y,test_size = 0.1, random_state=rand, stratify=y)\n               return x_train,y_train\n\n    \n        X,y = XyCreation(df)\n\n # =============================================================================\n #        #Remove missing\n # =============================================================================\n        print('Remove Missing..')\n        for col in X.columns:\n               if X[col].isnull().sum()\/X.shape[0] > 0.9:\n                      print(col)\n                      del X[col]\n # =============================================================================\n #        #Remove low variance features\n # =============================================================================       \n        for col in X.columns:\n               if X[col].nunique()==1:\n                      print(col)\n                      del X[col]\n # =============================================================================\n #        #Pearson Correlation\n # =============================================================================\n        print('Pearson Correlation..')\n\n        def cor_selector(X, y):\n            cor_list = []\n            # calculate the correlation with y for each feature\n            for i in X.columns.tolist():\n                cor = np.corrcoef(X[i], y)[0, 1]\n                cor_list.append(cor)\n            # replace NaN with 0\n            cor_list = [0 if np.isnan(i) else i for i in cor_list]\n            # feature name\n            cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-features_to_select:]].columns.tolist()\n            # feature selection? 0 for not select, 1 for select\n            cor_support = [True if i in cor_feature else False for i in feature_name]\n            return cor_support, cor_feature\n\n\n        feature_name = X.columns.tolist()     \n\n        cor_support, cor_feature = cor_selector(X, y)\n\n # =============================================================================\n #        #Chi-2\n # =============================================================================\n        print('Chi-2..')\n\n        X_norm = MinMaxScaler().fit_transform(X)\n        chi_selector = SelectKBest(chi2, k=features_to_select)\n        chi_selector.fit(X_norm, y)\n\n        chi_support = chi_selector.get_support()\n        chi_feature = X.loc[:,chi_support].columns.tolist()\n        print(str(len(chi_feature)), 'selected features')\n\n\n # =============================================================================\n #        #Wrapper RFE selector\n # =============================================================================\n        print('RFE selector..')\n        rfe_selector = RFE(estimator=LogisticRegression(class_weight = 'balanced'), n_features_to_select=features_to_select, step=150, verbose=5)\n        rfe_selector.fit(X_norm, y)\n\n        rfe_support = rfe_selector.get_support()\n        rfe_feature = X.loc[:,rfe_support].columns.tolist()\n        print(str(len(rfe_feature)), 'selected features')\n\n # =============================================================================\n #        #Embedded\n # =============================================================================\n\n        print('LogisticRegression l2 ..')\n\n        embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\",class_weight = 'balanced'))\n        embeded_lr_selector.fit(X_norm, y)\n\n        embeded_lr_support = embeded_lr_selector.get_support()\n        embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n        print(str(len(embeded_lr_feature)), 'selected features')\n # =============================================================================\n #        #Random Forest\n # =============================================================================\n        print('Random Forest ..')\n\n        embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=features_to_select,class_weight = 'balanced'))\n        embeded_rf_selector.fit(X, y)\n\n        embeded_rf_support = embeded_rf_selector.get_support()\n        embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n        print(str(len(embeded_rf_feature)), 'selected features')\n\n # =============================================================================\n #        #Light GBM\n # =============================================================================\n        print('Light GBM ..')\n\n\n        lgbc=LGBMClassifier(n_estimators=features_to_select, class_weight = 'balanced',learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n                    reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\n        embeded_lgb_selector = SelectFromModel(lgbc)\n        embeded_lgb_selector.fit(X, y)\n\n        embeded_lgb_support = embeded_lgb_selector.get_support()\n        embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\n        print(str(len(embeded_lgb_feature)), 'selected features')\n\n\n\n\n        pd.set_option('display.max_rows', None)\n        # put all selection together\n        feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n                                            'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n        # count the selected times for each feature\n        feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n        # display the top x\n        feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n        feature_selection_df.index = range(1, len(feature_selection_df)+1)\n\n        best_features = feature_selection_df.head(best)\n        best_features = list(best_features['Feature'])\n        print(best_features)\n        Xbest = X[best_features]\n\n        # remove uncorrelated features \n        Xbest = remove_collinear_features(Xbest,0.9)\n        \n\n        end = time.time()\n        print((end - start)\/60)\n\n        return Xbest\n    \n    \n'''Convert main dataframe to train test split ready to train\/validate model'''\ndef ModelCreation(df,testsize,target,selected_features,rand):\n    \"\"\"\n    Return 4 dataframes for train and test\n    param df:\n        dataframe to split into train test\n    return:\n        return 4 frames\n    \"\"\"\n    #set x and y\n    y=df[target].values\n    x=df.drop([target,'CustomerID'], axis=1)\n    x= x[selected_features]\n    # train test split\n    x_train, x_test, y_train, y_test = train_test_split(x, y,test_size = testsize, random_state=rand, stratify=y)\n\n    \n    return x_train, x_test, y_train, y_test\n\n\ndef plot_confusion_matrix(y_true, y_pred,normalize=False,title = None,cmap=plt.cm.Blues):\n\n\n        \"\"\"\n        This function prints and plots the confusion matrix.\n        Normalization can be applied by setting `normalize=True`.\n        \"\"\"\n        if not title:\n            if normalize:\n                title = 'Normalized confusion matrix'\n            else:\n                title = 'Confusion matrix, without normalization'\n\n        # Compute confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n\n        if normalize:\n            cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n            print(\"Normalized confusion matrix\")\n        else:\n            print('Confusion matrix, without normalization')\n\n        print(cm)\n\n        fig, ax = plt.subplots()\n        im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n        ax.figure.colorbar(im, ax=ax)\n        # We want to show all ticks...\n        ax.set(xticks=[],\n               yticks=[],\n               # ... and label them with the respective list entries\n               #xticklabels=classes, yticklabels=classes,\n               title=title,\n               ylabel='True label',\n               xlabel='Predicted label')\n\n        # Rotate the tick labels and set their alignment.\n        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n                 rotation_mode=\"anchor\")\n\n\n        # Loop over data dimensions and create text annotations.\n        fmt = '.2f' if normalize else 'd'\n        thresh = cm.max() \/ 2.\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                ax.text(j, i, format(cm[i, j], fmt),\n                        ha=\"center\", va=\"center\",\n                        color=\"white\" if cm[i, j] > thresh else \"black\")\n        fig.tight_layout()\n        return ax\n    \n    \ndef plotscoreclf(x,y,clf):\n                     \n        #Predict on test set\n        #Predicting proba\n        predictions_clf_prob = clf.predict(x)\n        predictions_clf_01 = np.where(predictions_clf_prob > 0.5, 1, 0) #Turn probability to 0-1 binary output\n\n\n        print(\"accuracy is:\", accuracy_score(y,predictions_clf_01))\n        print(\"\\n\")\n        print(\"confusion matrix is:\", confusion_matrix(y, predictions_clf_01))\n        print(\"\\n\")\n        print(\"fbeta is:\", fbeta_score(y, predictions_clf_01, beta=2))\n        print(\"\\n\")\n        print(classification_report(y, predictions_clf_01))\n\n        # Generate ROC curve values: fpr, tpr, thresholds\n        fpr, tpr, thresholds = roc_curve(y, predictions_clf_prob)\n\n        # Plot ROC curve\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.plot(fpr, tpr)\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC Curve')\n        plt.show()\n\n        # Plot non-normalized confusion matrix\n        plot_confusion_matrix(y, predictions_clf_01,\n                              title='Confusion matrix, without normalization')\n\n        # Plot normalized confusion matrix\n        plot_confusion_matrix(y, predictions_clf_01, normalize=True,\n                              title='Normalized confusion matrix')\n\n        plt.show()","2bfafa8e":"#Load data\npath = '\/kaggle\/input\/telco-customer-churn-ibm-dataset\/'\ndata = LoadData(path,'Telco_customer_churn')","a5de85db":"data.head()","12493f79":"data.info()","97c2b2dc":"data.describe()","1fbb549a":"resumetable(data)","00736e34":"total = len(data)\nplt.figure(figsize=(12,7))\n\ng = sns.countplot(x='Churn Label', data=data, color='blue')\ng.set_title(f\"Churn Distribution \\nTotal Customers: {total}\", \n            fontsize=20)\ng.set_xlabel(\"Customer Churned?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 4,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=15) \ng.set_ylim(0, total *1.00)\n\nplt.show()","65b876ce":"TargetvsNumeric(data,\"Tenure Months\",\"Churn Label\",\"Tenure\")","87e02cc6":"TargetvsNumeric(data,\"Monthly Charges\",\"Churn Label\",\"Monthly Charges\")","b1f9b8d5":"TargetvsNumeric(data,\"Monthly Charges\",\"Churn Label\",\"Total Charges\")","d69c5982":"TargetvsNumeric(data,\"CLTV\",\"Churn Label\",\"CLTV\")","44d43e02":"plot_categoricals(data, col=\"Gender\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","f29b0700":"plot_categoricals(data, col=\"Senior Citizen\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","b910d6e7":"plot_categoricals(data, col=\"Partner\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","bfee59fd":"plot_categoricals(data, col=\"Dependents\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","d61b2a0b":"plot_categoricals(data, col=\"Phone Service\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","a8eba399":"plot_categoricals(data, col=\"Multiple Lines\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","99f04b99":"plot_categoricals(data, col=\"Internet Service\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","69b3234e":"plot_categoricals(data, col=\"Online Security\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","55306a89":"plot_categoricals(data, col=\"Online Backup\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","92311247":"plot_categoricals(data, col=\"Device Protection\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","40ee53eb":"plot_categoricals(data, col=\"Tech Support\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","1df9d9f9":"plot_categoricals(data, col=\"Streaming TV\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","35cd366b":"plot_categoricals(data, col=\"Streaming Movies\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","94c499b9":"plot_categoricals(data, col=\"Contract\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","44137e5b":"plot_categoricals(data, col=\"Paperless Billing\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","fc3e1bfe":"plot_categoricals(data, col=\"Payment Method\", cont = \"Tenure Months\", binary=\"Churn Label\", dodge=True)","4f4529fc":"PlotLocations(data)","44bf6121":"PlotDistRatio(data, 'Contract', lim=2500)","18899345":"for col in ['Gender', 'Senior Citizen',\n       'Partner', 'Dependents', 'Phone Service',\n       'Multiple Lines', 'Internet Service', 'Online Security',\n       'Online Backup', 'Device Protection', 'Tech Support', 'Streaming TV',\n       'Streaming Movies', 'Contract', 'Paperless Billing', 'Payment Method']:\n\n    PlotDistRatio(data, col, lim=2500)","31d6fa77":"data_fe = FEngineering(data)","d58dcb1e":"data_fe.head(10)","cb592925":"corr = data_fe.corr() # Examine correlations\nplt.figure(figsize=(20, 16))\n\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.5)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","c86095a0":"seed = 2\n#Load data\ndata = LoadData(path,'Telco_customer_churn')\n#Data Cleaning-Feature Engineering\ndata_fe = FEngineering(data)\n#Variable Selection\nprepared_data_best = variableselection(data_fe,21,'Churn Value',11,seed)\n#keep those features only\nfeat_list= list(prepared_data_best.columns)\n# Split train test\nx_train, x_test, y_train, y_test = ModelCreation(data_fe,0.15,'Churn Value',feat_list,seed)","729ad633":"feat_list","0861a36b":"\nclfs = []\n\n\nx_train, x_test, y_train, y_test = ModelCreation(data_fe,0.1,'Churn Value',feat_list,seed)\n\nclfs.append((\"LogReg\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())])))\nclfs.append((\"KNN\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())])))\n\nclfs.append((\"DecisionTreeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())])))\n\nclfs.append((\"RandomForestClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())])))\n\nclfs.append((\"GradientBoostingClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\",\n                        GradientBoostingClassifier())])))\nclfs.append((\"LGBMClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LGBMClassifier\",\n                        LGBMClassifier())])))\n\nclfs.append((\"RidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = \"f1\"\nn_folds = 10\ntest_size = 0.2\n\nresults, names = [], []\n\nfor name, model in clfs:\n    kfold = StratifiedKFold(n_splits=n_folds, random_state=seed)\n    cv = ShuffleSplit(n_splits=n_folds, test_size=test_size, random_state=seed)\n    cv_results = cross_val_score(model,\n                                 x_train,\n                                 y_train,\n                                 cv=cv,\n                                 scoring=scoring,\n                                 n_jobs=-1)\n    names.append(name)\n    results.append(cv_results)\n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15, 6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\n\n\nfoo = pd.DataFrame({'Names':names,'Results':results})\nresult = foo.explode('Results').reset_index(drop=True)\nresult = result.assign(Names=result['Names'].astype('category'), \n                       Values=result['Results'].astype(np.float32))\n\nsns.boxplot(x='Names', y='Results', data=result)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n\nplt.show()","724e1526":"#gives model report in dataframe\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    roc_auc      = roc_auc_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    kappa_metric = cohen_kappa_score(testing_y,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df\n\n\n\n\nresults, names  = [], [] \n\nfor name, model  in clfs:    \n    names.append(name)\n    model_results = model_report(model, x_train, x_test, y_train, y_test,name)\n     #concat all models\n    results.append(model_results)\n    \n    \nmodel_performances = pd.concat(results)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","28829a60":"def output_tracer(metric,color) :\n    tracer = go.Bar(y = model_performances[\"Model\"] ,\n                    x = model_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"#6699FF\")\ntrace2  = output_tracer('Recall_score',\"red\")\ntrace3  = output_tracer('Precision',\"#33CC99\")\ntrace4  = output_tracer('f1_score',\"lightgrey\")\ntrace5  = output_tracer('Kappa_metric',\"#FFCC99\")\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","2dacd897":"fig = plt.figure(figsize=(13,15))\nfig.set_facecolor(\"#F3F3F3\")\n    \nfor i in range(len(clfs)):\n    plt.subplot(4,3,(i + 1))\n    predictions = clfs[i][1].predict(x_test)\n    conf_matrix = confusion_matrix(predictions,y_test)\n    sns.heatmap(conf_matrix,annot=True,fmt = \"d\",square = True,\n                xticklabels=[\"not churn\",\"churn\"],\n                yticklabels=[\"not churn\",\"churn\"],\n                linewidths = 2,linecolor = \"w\",cmap = plt.cm.Blues)\n    plt.title(clfs[i][0],color = \"b\")\n    plt.subplots_adjust(wspace = .3,hspace = .3)","45bef26a":"# Define searched space\nhyper_space = {'objective': 'binary',\n               'metric':'f1',\n               'boosting':'gbdt', \n               'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\n               'scale_pos_weight': hp.choice('scale_pos_weight', [3, 5, 8, 13, 16]),\n               'max_depth':  hp.choice('max_depth', list(range(6, 18, 2))),\n               'num_leaves': hp.choice('num_leaves', list(range(20, 180, 20))),\n               'subsample': hp.choice('subsample', [.7, .8, .9, 1]),\n               'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1),\n               'learning_rate': hp.uniform('learning_rate', 0.03, 0.12),\n               'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),\n               'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               \n               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])}","ad485009":"def to_minimize(hyperparameters, features, target):\n    # create an instance of the model \n    clf = LGBMClassifier(**hyperparameters)\n    # train with cross-validation\n    result = cross_val_score(estimator = clf, \n                                X = features, \n                                y = target, \n                                scoring = \"f1\",\n                                cv = cv, \n                                n_jobs = -1,\n                                error_score='raise')\n    \n    return -result.mean()","7829ed04":"%%time\nfrom functools import partial\n## run optimization\noptimization = fmin(fn = partial(to_minimize, features = x_train, target = y_train),\n                  space = hyper_space, \n                  algo = tpe.suggest,\n                  trials = Trials(),\n                  max_evals = int(60), \n                  rstate = np.random.RandomState(42))","50eb2128":"# Print best parameters\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nbest_params = space_eval(hyper_space, optimization)\nbest_params","69ce2f60":"clf_best = LGBMClassifier(**best_params)\nclf_best.fit(x_train, y_train)","f5bf6f20":"plotscoreclf(x_train, y_train,clf_best)","57919793":"plotscoreclf(x_test, y_test,clf_best)","86fe91f9":"%time shap_values = shap.TreeExplainer(clf_best).shap_values(x_test)","f4b928f6":"shap.summary_plot(shap_values, x_test)","2f25ea31":"explainer = shap.TreeExplainer(clf_best)\nshap.force_plot(explainer.expected_value[0], shap_values[1][1,:], x_test.iloc[1,:])","92d06b96":"No significant difference can be found regarding phone service binary value","36e17f7d":"### Plot Locations in a map","76d50ee2":"### Objectives:\n\n***\nExplore the data and try to answer some questions like:\n***\n\n- What's the % of Churn Customers and customers that keep in with the active services.\n- We can see different patterns in Churn Customers based on the type of service provided?\n- Get advantage of lat\/long coordinates and obtain useful features to predict churn.\n- We have difference pattern of churn between genders ?\n- What's the difference between customers that pay monthly and by year?\n-what's the most profitable service types?\n- What's the amount lose in revenue?\n- What's the mean age of papeless customers? they are more propense to churn?\n- A lot of other questions that will raise trought the exploration","1b8e9b1d":"### Plot numerical features vs churn","a91bc94b":"#### ML Process","341cc078":"There is no significant effect between gender for customer churn","b39df04c":"There is a small difference between customers with multiple service lines and not multiple lines. But it seems that customers with multiple service lines are also older customers.","7c157197":"#### Churn Distribution","9ac1f9dd":"I will build a pipeline to find a model that better fits our data. With the best models I will predict the result and verify the scores of the models.","d98b00f6":"### Dictionary\n\n***\n__Demographics__\n***\n\nCustomerID: A unique ID that identifies each customer.\n\nCount: A value used in reporting\/dashboarding to sum up the number of customers in a filtered set.\n\nGender: The customer\u2019s gender: Male, Female\n\nSenior Citizen: Indicates if the customer is 65 or older: Yes, No\n\nPartner: Indicates if the customer has a partner: Yes,No\n\nDependents: Indicates if the customer lives with any dependents: Yes, No. Dependents could be children, parents, grandparents, etc.\n\n\n\n***\n__Location__\n***\nCountry: The country of the customer\u2019s primary residence.\n\nState: The state of the customer\u2019s primary residence.\n\nCity: The city of the customer\u2019s primary residence.\n\nZip Code: The zip code of the customer\u2019s primary residence.\n\nLat Long: The combined latitude and longitude of the customer\u2019s primary residence.\n\nLatitude: The latitude of the customer\u2019s primary residence.\n\nLongitude: The longitude of the customer\u2019s primary residence.\n\n\n\n***\n__Services__\n***\n\nTenure Months: Indicates the total amount of months that the customer has been with the company by the end of the quarter specified above.\n\nMultiple Lines: Indicates if the customer subscribes to multiple telephone lines with the company: Yes, No\n\nInternet Service: Indicates if the customer subscribes to Internet service with the company: No, DSL, Fiber Optic, Cable.\n\nOnline Security: Indicates if the customer subscribes to an additional online security service provided by the company: Yes, No\n\nOnline Backup: Indicates if the customer subscribes to an additional online backup service provided by the company: Yes, No\n\nDevice Protection: Indicates if the customer subscribes to an additional device protection plan for their Internet equipment provided by the company: Yes, No\n\nTech Support: Indicates if the customer subscribes to an additional technical support plan from the company with reduced wait times: Yes, No\n\nStreaming TV: Indicates if the customer uses their Internet service to stream television programing from a third party provider: Yes, No. The company does not charge an additional fee for this service.\n\nStreaming Movies: Indicates if the customer uses their Internet service to stream movies from a third party provider: Yes, No. The company does not charge an additional fee for this service.\n\nStreaming Music: Indicates if the customer uses their Internet service to stream music from a third party provider: Yes, No. The company does not charge an additional fee for this service.\n\nContract: Indicates the customer\u2019s current contract type: Month-to-Month, One Year, Two Year.\n\nPaperless Billing: Indicates if the customer has chosen paperless billing: Yes, No\n\nPayment Method: Indicates how the customer pays their bill: Bank Withdrawal, Credit Card, Mailed Check\n\nMonthly Charge: Indicates the customer\u2019s current total monthly charge for all their services from the company.\n\nTotal Charges: Indicates the customer\u2019s total charges, calculated to the end of the quarter specified above.\n\n\n\n***\n__Status__\n***\n\nChurn Label: Yes = the customer left the company this quarter. No = the customer remained with the company. Directly related to Churn Value.\n\nChurn Value: 1 = the customer left the company this quarter. 0 = the customer remained with the company. Directly related to Churn Label.\n\nChurn Score: A value from 0-100 that is calculated using the predictive tool IBM SPSS Modeler. The model incorporates multiple factors known to cause churn. The higher the score, the more likely the customer will churn.\n\nCLTV: Customer Lifetime Value. A predicted CLTV is calculated using corporate formulas and existing data. The higher the value, the more valuable the customer. High value customers should be monitored for churn.\n\nChurn Reason: A customer\u2019s specific reason for leaving the company. Directly related to Churn Category.","cff817f0":"In order to ensure that we produced the best possible version of lightgbm and have the fit that minimizes the residuals we need to search over the hyperparameter space and find those minima which accomplish this goal.","1d740aa1":"### Telco Churn Customers analysis and Prediction","efd3af66":"### Clean - Transform Dataset","750b2f3c":"### Next Steps\n\n1. Feature Engineering. It is always worthwhile to spend more time into finding new ways for new features and exploring new techniques like PCA or different encoding techniques for the categorical variables.\n2. Model Tuning. In this particural notebook only LightGBM has been tuned further. This needs to be done foe every candidate model and then chose the best one.\n3. Ensembling. Use ensembling techiques like averaging or voting or even stacking them using every candidate model. This would probably result into a better model.","434c77c3":"Customers with __dependent__ family members are significantly less likely to churn compared with customers with no dependents (5% -> 33%)","0d5cdeb9":"### Optimize Tuning for LightGBM model","4a543321":"### Churn Model Selection Process\n\n- The best of models will be decided based on multiple performance metrics. \n- Then the best model will be optimized by choosing its best hyperparameters.\n- Last, we will check performance metrics on optimized model and then we will check the best features which characterize the majority of decisions made and the subsequent probabilities for every customer.","fb3eb652":"Customers with higher Monthly and Total Charges are also more likely to churn","3d8ba217":"__Notes__\n\n1. We can see that we have one entry for each **CustomerID** (7.043)\n2. The dataset don't have missing values except **Churn Reason** feature\n3. Most of features are categorical\n4. The target the we will use to guide the exploration is **Churn**","d81f39c1":"Here for every customer in the test unseen dataset you can check why and how he got the churn label and probability","d0af14ed":"Senior customers (>65 years old) have a higher tendecy to churn compared to non senior ones","bb8c6108":"### Explain Machine Learning Model","d5e9ad9e":"Variables Selected are:","b6166437":"Below using __folium__ library very useful map every customer is represented based on their respective lat long coordinates. You can zoom in\/out and check where most customers are located. Also a heatmap is applied where all the churners are located. You can get a good idea on which areas concentrate churners and get more immediate actions. Based on this idea at the feature engineering part of code distances are calculated for every customer from four major parts of California. This allows us to get advantage of the lat\/long coordinates giving useful information.","bd622f80":"### Plot categorical features vs churn","ce424909":"### Check Correlation","7c86f3ed":"Customers with __Fiber optic__ Internet Service have a significantly higher probability to churn compared to customers with __DSL__ or with no Internet Service at all.","c2deff82":"### Check metrics and assess fit","d1928e7f":"- Customers with __month-to-month__ contract type are significantly more probable to churn compared to __1-year__ and __2-year__ contracts (42% -> 10% -> 3%)\n- As expected __tenure__ is directly related with __contract type__ (The longest the duration of the contract the higher the overall tenure of the customer).","83ac2e5a":"### Mean Monthly Charges by Categorical Features","826699e0":"Lets check target distribution","0ca37f4f":"__26.5%__ of customers have churned in our dataset. We will attempt to understand the underlying reasons which lead to churn and eventually can make us proactively identify those customers.","73097b5c":"Customers with higher Lifetime Value are less likely to churn","097d5470":"Interesting finding is that older customers are less likely to churn. Seems that customer loyalty plays an important role in churn decision.","fbb45694":"Customers with __No Online Security__ Internet Service have a significantly higher probability to churn compared to customers with __Online Security__ or with no Internet Service at all.","60ba1e74":"### Description of dataset\n***\n#### Context\n***\nIdentify which features lead to churn - create a pipeline to select an optimal model able to generalize well in unseen data - create a procedure that can explain model's decision for every customer\n***\n#### Content\n***\n\n#### The data set includes information about:\n- __Customers who left within the last month__ \u2013 the column is called Churn(target variable)\n- __Services that each customer has signed up for__ \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n- __Customer account information__ \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n- __Demographics__ \u2013 gender, age range, and if they have partners and dependents\n- __Location related information__ \u2013 Country,State,Lat-Long coordinates\n\n***\n__link__: __https:\/\/community.ibm.com\/community\/user\/businessanalytics\/blogs\/steven-macko\/2019\/07\/11\/telco-customer-churn-1113__\n***","daab3001":"### Motivation\n\n- @kabure and his amazing notebook https:\/\/www.kaggle.com\/kabure\/insightful-eda-churn-customers-models-pipeline has been a great source of ideas to start a notebook of my own and try to expand with my approach.\n- https:\/\/www.kaggle.com\/ridhoaryo\/telco-churn-analysis-and-modeling has been also a great source of inspiration.","01128a14":"#### Feature Importance","d343c5ba":"Customers with a partner are less likely to churn compared to customers with no partner (20% -> 34%)"}}