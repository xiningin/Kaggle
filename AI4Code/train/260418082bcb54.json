{"cell_type":{"5cadb2ad":"code","0ffbfe56":"code","ca7bfa08":"code","6f139853":"code","723fc528":"code","d87c655f":"code","d094a6bd":"code","f5be4bff":"code","c76a8ea8":"code","ffd99a7c":"code","282989fd":"code","96feb773":"code","fac8032b":"code","9ead5602":"code","dbd1115f":"code","aba82e40":"code","9b213032":"code","4c614434":"code","9581f75c":"code","8ad115ea":"code","e28322ba":"code","47c1c5e0":"code","5cbfdfcb":"markdown"},"source":{"5cadb2ad":"%cd \/kaggle\/working\/","0ffbfe56":"import codecs\nimport os\nimport pandas as pd\nimport numpy as np\nimport regex\nfrom collections import Counter\nimport tqdm as tqdm\n\n#loading in the text from both versions of the bible\nnew_bib = pd.read_csv(\"\/kaggle\/input\/bible\/t_web.csv\").t\nold_bib = pd.read_csv(\"\/kaggle\/input\/bible\/t_kjv.csv\").t\n","ca7bfa08":"% cd \/kaggle\/input\/tensorflow-transformer\/repository\/Kyubyong-transformer-ed2deb8\/","6f139853":"# this chunk from Kaggle User ryches\n# original kerenl here https:\/\/www.kaggle.com\/ryches\/newbib2oldbib\n\n# a regex to remove the curly braces and the notes inside them. \ndef remove_notes(notes):\n    return regex.sub(\"[\\{\\[].*?[\\}\\]]\", \"\", notes)\n\n# remove notes from the bible text\nnew_bib = new_bib.apply(remove_notes)\nold_bib = old_bib.apply(remove_notes)","723fc528":"# this line is blank in the king james version\nprint(new_bib[30673])\nprint(old_bib[30673])\n\n# cleaned data\nold_bib_cleaned = old_bib.drop([30673])","d87c655f":"os.mkdir(\"..\/..\/..\/logdir\")","d094a6bd":"class Hyperparams:\n    '''Hyperparameters'''\n    # data\n    source_train = new_bib[0:30000]\n    target_train = old_bib_cleaned[0:30000]\n    source_test = new_bib[30000:]\n    target_test = old_bib_cleaned[30000:]\n    \n    # training\n    batch_size = 32 # alias = N\n    lr = 0.0001 # learning rate. In paper, learning rate is adjusted to the global step.\n    logdir = \"..\/..\/..\/logdir\" # log directory\n    \n    # model\n    maxlen = 50 # Maximum number of words in a sentence. alias = T.\n                # Feel free to increase this if you are ambitious.\n    min_cnt = 2 # words whose occurred less than min_cnt are encoded as <UNK>.\n    hidden_units = 300 # alias = C\n    num_blocks = 6 # number of encoder\/decoder blocks\n    num_epochs = 1\n    num_heads = 10\n    dropout_rate = 0.1\n    sinusoid = False # If True, use sinusoid. If false, positional embedding.\n","f5be4bff":"def make_vocab(fname, text):\n    '''Constructs vocabulary.\n    \n    Args:\n      fpath: A string. Input file path.\n      fname: A string. Output file name.\n    \n    Writes vocabulary line by line to `preprocessed\/fname`\n    '''  \n    text = regex.sub(\"[^\\s\\p{Latin}']\", \"\", text)\n    words = text.split()\n    word2cnt = word_index\n    if not os.path.exists('\/kaggle\/working\/preprocessed'): \n        os.mkdir('\/kaggle\/working\/preprocessed')\n    with codecs.open('\/kaggle\/working\/preprocessed\/{}'.format(fname), 'w', 'utf-8') as fout:\n        fout.write(\"{}\\t100000\\n{}\\t100000\\n{}\\t100000\\n{}\\t100000\\n\".format(\"<PAD>\", \"<UNK>\", \"<S>\", \"<\/S>\"))\n        for word, cnt in word2cnt.items():\n            fout.write(u\"{}\\t{}\\n\".format(word, cnt))\n\n","c76a8ea8":"os.mkdir('\/kaggle\/working\/preprocessed')\n\n","ffd99a7c":"# -*- coding: utf-8 -*-\n#\/usr\/bin\/python2\n'''\nJune 2017 by kyubyong park. \nkbpark.linguist@gmail.com.\nhttps:\/\/www.github.com\/kyubyong\/transformer\n'''\nimport tensorflow as tf\nimport numpy as np\nimport codecs\nimport regex\n\n# load source vocab\ndef load_de_vocab():\n    word2idx, idx2word = vocab_set, index_set\n    return word2idx, idx2word\n\n# load target vocab\ndef load_en_vocab():\n    word2idx, idx2word = vocab_set, index_set\n    return word2idx, idx2word\n\ndef create_data(source_sents, target_sents):\n    print(len(source_sents))\n    de2idx, idx2de = load_de_vocab()\n    en2idx, idx2en = load_en_vocab()\n    \n    # Index\n    x_list, y_list, Sources, Targets = [], [], [], []\n    for source_sent, target_sent in zip(source_sents, target_sents):\n        x = [de2idx.get(word, 1) for word in (source_sent + u\" <\/S>\").split()] # 1: OOV, <\/S>: End of Text\n        y = [en2idx.get(word, 1) for word in (target_sent + u\" <\/S>\").split()] \n        if max(len(x), len(y)) <=hp.maxlen:\n            x_list.append(np.array(x))\n            y_list.append(np.array(y))\n            Sources.append(source_sent)\n            Targets.append(target_sent)\n    # Pad   \n    X = np.zeros([len(x_list), hp.maxlen], np.int32)\n    Y = np.zeros([len(y_list), hp.maxlen], np.int32)\n    for i, (x, y) in enumerate(zip(x_list, y_list)):\n        X[i] = np.lib.pad(x, [0, hp.maxlen-len(x)], 'constant', constant_values=(0, 0))\n        Y[i] = np.lib.pad(y, [0, hp.maxlen-len(y)], 'constant', constant_values=(0, 0))\n    \n    return X, Y, Sources, Targets\n\ndef load_train_data():\n    de_sents = hp.source_train\n    en_sents = hp.target_train\n    \n    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n    return X, Y\n    \ndef load_test_data():\n    def _refine(line):\n        line = regex.sub(\"<[^>]+>\", \"\", line)\n        line = regex.sub(\"[^\\s\\p{Latin}']\", \"\", line) \n        return line.strip()\n    \n    de_sents = hp.source_test\n    en_sents = hp.target_test\n    \n    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n    return X, Sources, Targets # (1064, 150)\n\ndef get_batch_data():\n    # Load data\n    X, Y = load_train_data()\n    \n    # calc total batch count\n    num_batch = len(X) \/\/ hp.batch_size\n    \n    # Convert to tensor\n    X = tf.convert_to_tensor(X, tf.int32)\n    Y = tf.convert_to_tensor(Y, tf.int32)\n    \n    # Create Queues\n    input_queues = tf.train.slice_input_producer([X, Y])\n            \n    # create batch queues\n    x, y = tf.train.shuffle_batch(input_queues,\n                                num_threads=8,\n                                batch_size=hp.batch_size, \n                                capacity=hp.batch_size*64,   \n                                min_after_dequeue=hp.batch_size*32, \n                                allow_smaller_final_batch=False)\n    \n    return x, y, num_batch # (N, T), (N, T), ()\n","282989fd":"import gensim\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\n\nspell_model = gensim.models.KeyedVectors.load_word2vec_format('\/kaggle\/input\/fasttext-wikinews\/wiki-news-300d-1M.vec')\nwords = spell_model.index2word\nword_index = {}\nindex_word = {}\nfor i,word in enumerate(words):\n    word_index[word] = i\n    index_word[i] = word","96feb773":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=None, filters = '', lower = False)\ntokenizer.fit_on_texts(list(old_bib_cleaned.str.lower() + list(new_bib.str.lower())))\nvocab_set = tokenizer.word_index\nindex_set = tokenizer.index_word","fac8032b":"\n# Use fast text as vocabulary\ndef words(text): return re.findall(r'\\w+', text.lower())\ndef P(word): \n    \"Probability of `word`.\"\n    # use inverse of rank as proxy\n    # returns 0 if the word isn't in the dictionary\n    return - word_index.get(word, 0)\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or [word])\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in word_index)\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\ndef singlify(word):\n    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n\ndef load_fasttext(word_dict):\n    EMBEDDING_FILE =  '\/kaggle\/input\/fasttext-wikinews\/wiki-news-300d-1M.vec'\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr[:-1], dtype='float32')\n#     embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in (open(EMBEDDING_FILE, encoding = \"utf8\")) if o.split(\" \")[0] in word_dict) \n    embed_size = 300\n    nb_words = len(word_dict)+1\n    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n    print(unknown_vector[:5])\n    for key in word_dict:\n        word = key\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.lower()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.upper()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.capitalize()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        if len(key) > 1:\n            word = correction(key)\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[word_dict[key]] = embedding_vector\n                continue\n        embedding_matrix[word_dict[key]] = unknown_vector                    \n    return embedding_matrix, nb_words \nembedding_matrix, nb_words = load_fasttext(vocab_set)","9ead5602":"embedding_matrix.shape","dbd1115f":"# from tqdm import tqdm\n# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\n# def load_glove(word_index,embed_dir = \"\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt\"):\n#     embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(embed_dir, encoding = \"utf8\")) if o.split(\" \")[0] in word_index) \n#     return embeddings_index\n\n# def build_embedding_matrix(word_index,embeddings_index, max_features, maxlen, lower = False):\n#     embedding_matrix = np.zeros((max_features, 200))\n#     for word, i in word_index.items():\n#         if lower:\n#             word = word.lower()\n#         if i >= max_features: continue\n#         embedding_vector = embeddings_index.get(word)\n#         if embedding_vector is not None: \n#             embedding_matrix[i] = embedding_vector\n#     return embedding_matrix\n\n# def build_glove_matrix():\n#     embeddings_index = load_glove(word_index=word_index,embed_dir = \"\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt\")\n#     embedding_matrix = build_embedding_matrix(word_index,embeddings_index, 100000, None, lower = False)\n#     return embedding_matrix\n\n\n\n# embedding_matrix = build_glove_matrix()\n\n\n","aba82e40":"# -*- coding: utf-8 -*-\n#\/usr\/bin\/python2\n'''\nJune 2017 by kyubyong park. \nkbpark.linguist@gmail.com.\nhttps:\/\/www.github.com\/kyubyong\/transformer\n'''\n\nfrom __future__ import print_function\nimport tensorflow as tf\n\ndef normalize(inputs, \n              epsilon = 1e-8,\n              scope=\"ln\",\n              reuse=None):\n    '''Applies layer normalization.\n    \n    Args:\n      inputs: A tensor with 2 or more dimensions, where the first dimension has\n        `batch_size`.\n      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n      \n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n    \n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta= tf.Variable(tf.zeros(params_shape))\n        gamma = tf.Variable(tf.ones(params_shape))\n        normalized = (inputs - mean) \/ ( (variance + epsilon) ** (.5) )\n        outputs = gamma * normalized + beta\n        \n    return outputs\n\ndef embedding(inputs, \n              vocab_size, \n              num_units, \n              zero_pad=True, \n              scale=True,\n              scope=\"embedding\", \n              reuse=None):\n    '''Embeds a given tensor.\n    Args:\n      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n         to be looked up in `lookup table`.\n      vocab_size: An int. Vocabulary size.\n      num_units: An int. Number of embedding hidden units.\n      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n        should be constant zeros.\n      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n    Returns:\n      A `Tensor` with one more rank than inputs's. The last dimensionality\n        should be `num_units`.\n        \n    For example,\n    \n    ```\n    import tensorflow as tf\n    \n    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n    outputs = embedding(inputs, 6, 2, zero_pad=True)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        print sess.run(outputs)\n    >>\n    [[[ 0.          0.        ]\n      [ 0.09754146  0.67385566]\n      [ 0.37864095 -0.35689294]]\n     [[-1.01329422 -1.09939694]\n      [ 0.7521342   0.38203377]\n      [-0.04973143 -0.06210355]]]\n    ```\n    \n    ```\n    import tensorflow as tf\n    \n    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n    outputs = embedding(inputs, 6, 2, zero_pad=False)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        print sess.run(outputs)\n    >>\n    [[[-0.19172323 -0.39159766]\n      [-0.43212751 -0.66207761]\n      [ 1.03452027 -0.26704335]]\n     [[-0.11634696 -0.35983452]\n      [ 0.50208133  0.53509563]\n      [ 1.22204471 -0.96587461]]]    \n    ```    \n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        lookup_table = tf.get_variable('lookup_table',\n                                       dtype=tf.float32,\n                                       shape=[nb_words, num_units],\n                                       initializer=tf.constant_initializer(np.array(embedding_matrix)))\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n                                      lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n        \n        if scale:\n            outputs = outputs * (num_units ** 0.5) \n            \n    return outputs\n    \n\ndef positional_encoding(inputs,\n                        num_units,\n                        zero_pad=True,\n                        scale=True,\n                        scope=\"positional_encoding\",\n                        reuse=None):\n    '''Sinusoidal Positional_Encoding.\n    Args:\n      inputs: A 2d Tensor with shape of (N, T).\n      num_units: Output dimensionality\n      zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\n      scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n    Returns:\n        A 'Tensor' with one more rank than inputs's, with the dimensionality should be 'num_units'\n    '''\n\n    N, T = inputs.get_shape().as_list()\n    with tf.variable_scope(scope, reuse=reuse):\n        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n\n        # First part of the PE function: sin and cos argument\n        position_enc = np.array([\n            [pos \/ np.power(10000, 2.*i\/num_units) for i in range(num_units)]\n            for pos in range(T)])\n\n        # Second part, apply the cosine to even columns and sin to odds.\n        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n\n        # Convert to a tensor\n        lookup_table = tf.convert_to_tensor(position_enc)\n\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n                                      lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n\n        if scale:\n            outputs = outputs * num_units**0.5\n\n        return outputs\n\n\n\ndef multihead_attention(queries, \n                        keys, \n                        num_units=None, \n                        num_heads=8, \n                        dropout_rate=0,\n                        is_training=True,\n                        causality=False,\n                        scope=\"multihead_attention\", \n                        reuse=None):\n    '''Applies multihead attention.\n    \n    Args:\n      queries: A 3d tensor with shape of [N, T_q, C_q].\n      keys: A 3d tensor with shape of [N, T_k, C_k].\n      num_units: A scalar. Attention size.\n      dropout_rate: A floating point number.\n      is_training: Boolean. Controller of mechanism for dropout.\n      causality: Boolean. If true, units that reference the future are masked. \n      num_heads: An int. Number of heads.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n        \n    Returns\n      A 3d tensor with shape of (N, T_q, C)  \n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        # Set the fall back option for num_units\n        if num_units is None:\n            num_units = queries.get_shape().as_list()[-1]\n        \n        # Linear projections\n        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n        \n        # Split and concat\n        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C\/h) \n        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C\/h) \n        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C\/h) \n\n        # Multiplication\n        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n        \n        # Scale\n        outputs = outputs \/ (K_.get_shape().as_list()[-1] ** 0.5)\n        \n        # Key Masking\n        key_masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1)) # (N, T_k)\n        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n        \n        paddings = tf.ones_like(outputs)*(-2**32+1)\n        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n  \n        # Causality = Future blinding\n        if causality:\n            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n   \n            paddings = tf.ones_like(masks)*(-2**32+1)\n            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n  \n        # Activation\n        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n         \n        # Query Masking\n        query_masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1)) # (N, T_q)\n        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n        outputs *= query_masks # broadcasting. (N, T_q, C)\n          \n        # Dropouts\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n               \n        # Weighted sum\n        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C\/h)\n        \n        # Restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n              \n        # Residual connection\n        outputs += queries\n              \n        # Normalize\n        outputs = normalize(outputs) # (N, T_q, C)\n \n    return outputs\n\ndef feedforward(inputs, \n                num_units=[2048, 512],\n                scope=\"multihead_attention\", \n                reuse=None):\n    '''Point-wise feed forward net.\n    \n    Args:\n      inputs: A 3d tensor with shape of [N, T, C].\n      num_units: A list of two integers.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n        \n    Returns:\n      A 3d tensor with the same shape and dtype as inputs\n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        # Inner layer\n        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n                  \"activation\": tf.nn.relu, \"use_bias\": True}\n        outputs = tf.layers.conv1d(**params)\n        \n        # Readout layer\n        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n                  \"activation\": None, \"use_bias\": True}\n        outputs = tf.layers.conv1d(**params)\n        \n        # Residual connection\n        outputs += inputs\n        \n        # Normalize\n        outputs = normalize(outputs)\n    \n    return outputs\n\ndef label_smoothing(inputs, epsilon=0.1):\n    '''Applies label smoothing. See https:\/\/arxiv.org\/abs\/1512.00567.\n    \n    Args:\n      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n      epsilon: Smoothing rate.\n    \n    For example,\n    \n    ```\n    import tensorflow as tf\n    inputs = tf.convert_to_tensor([[[0, 0, 1], \n       [0, 1, 0],\n       [1, 0, 0]],\n      [[1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0]]], tf.float32)\n       \n    outputs = label_smoothing(inputs)\n    \n    with tf.Session() as sess:\n        print(sess.run([outputs]))\n    \n    >>\n    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n        [ 0.03333334,  0.93333334,  0.03333334],\n        [ 0.93333334,  0.03333334,  0.03333334]],\n       [[ 0.93333334,  0.03333334,  0.03333334],\n        [ 0.93333334,  0.03333334,  0.03333334],\n        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n    ```    \n    '''\n    K = inputs.get_shape().as_list()[-1] # number of channels\n    return ((1-epsilon) * inputs) + (epsilon \/ K)","9b213032":"'''\nJune 2017 by kyubyong park. \nkbpark.linguist@gmail.com.\nhttps:\/\/www.github.com\/kyubyong\/transformer\n'''\nfrom __future__ import print_function\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n# from hyperparams import Hyperparams as hp\nhp = Hyperparams\n## TODO: rewrite \"load___vocab\" fucntions to work with our\n# file system\n# from data_load import get_batch_data\n# this is the meat of the transformer code, hoepfully\n# can just use as is\n#from modules import *\nimport os, codecs\n\n\nclass Graph():\n    def __init__(self, is_training=True):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            with tf.device(\"\/device:GPU:0\"):\n                if is_training:\n                    self.x, self.y, self.num_batch = get_batch_data() # (N, T)\n                else: # inference\n                    self.x = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n                    self.y = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n\n                # define decoder inputs\n                self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 2:<S>\n\n                # Load vocabulary    \n                de2idx, idx2de = load_de_vocab()\n                en2idx, idx2en = load_en_vocab()\n\n                # Encoder\n                with tf.variable_scope(\"encoder\"):\n                    ## Embedding\n                    self.enc = embedding(self.x, \n                                          vocab_size=len(de2idx), \n                                          num_units=hp.hidden_units, \n                                          scale=True,\n                                          scope=\"enc_embed\")\n\n                    ## Positional Encoding\n                    if hp.sinusoid:\n                        self.enc += positional_encoding(self.x,\n                                          num_units=hp.hidden_units, \n                                          zero_pad=False, \n                                          scale=False,\n                                          scope=\"enc_pe\")\n                    else:\n                        self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n                                          vocab_size=hp.maxlen, \n                                          num_units=hp.hidden_units, \n                                          zero_pad=False, \n                                          scale=False,\n                                          scope=\"enc_pe\")\n\n\n                    ## Dropout\n                    self.enc = tf.layers.dropout(self.enc, \n                                                rate=hp.dropout_rate, \n                                                training=tf.convert_to_tensor(is_training))\n\n                    ## Blocks\n                    for i in range(hp.num_blocks):\n                        with tf.variable_scope(\"num_blocks_{}\".format(i)):\n                            ### Multihead Attention\n                            self.enc = multihead_attention(queries=self.enc, \n                                                            keys=self.enc, \n                                                            num_units=hp.hidden_units, \n                                                            num_heads=hp.num_heads, \n                                                            dropout_rate=hp.dropout_rate,\n                                                            is_training=is_training,\n                                                            causality=False)\n\n                            ### Feed Forward\n                            self.enc = feedforward(self.enc, num_units=[4*hp.hidden_units, hp.hidden_units])\n\n                # Decoder\n                with tf.variable_scope(\"decoder\"):\n                    ## Embedding\n                    self.dec = embedding(self.decoder_inputs, \n                                          vocab_size=len(en2idx), \n                                          num_units=hp.hidden_units,\n                                          scale=True, \n                                          scope=\"dec_embed\")\n\n                    ## Positional Encoding\n                    if hp.sinusoid:\n                        self.dec += positional_encoding(self.decoder_inputs,\n                                          vocab_size=hp.maxlen, \n                                          num_units=hp.hidden_units, \n                                          zero_pad=False, \n                                          scale=False,\n                                          scope=\"dec_pe\")\n                    else:\n                        self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0), [tf.shape(self.decoder_inputs)[0], 1]),\n                                          vocab_size=hp.maxlen, \n                                          num_units=hp.hidden_units, \n                                          zero_pad=False, \n                                          scale=False,\n                                          scope=\"dec_pe\")\n\n                    ## Dropout\n                    self.dec = tf.layers.dropout(self.dec, \n                                                rate=hp.dropout_rate, \n                                                training=tf.convert_to_tensor(is_training))\n\n                    ## Blocks\n                    for i in range(hp.num_blocks):\n                        with tf.variable_scope(\"num_blocks_{}\".format(i)):\n                            ## Multihead Attention ( self-attention)\n                            self.dec = multihead_attention(queries=self.dec, \n                                                            keys=self.dec, \n                                                            num_units=hp.hidden_units, \n                                                            num_heads=hp.num_heads, \n                                                            dropout_rate=hp.dropout_rate,\n                                                            is_training=is_training,\n                                                            causality=True, \n                                                            scope=\"self_attention\")\n\n                            ## Multihead Attention ( vanilla attention)\n                            self.dec = multihead_attention(queries=self.dec, \n                                                            keys=self.enc, \n                                                            num_units=hp.hidden_units, \n                                                            num_heads=hp.num_heads,\n                                                            dropout_rate=hp.dropout_rate,\n                                                            is_training=is_training, \n                                                            causality=False,\n                                                            scope=\"vanilla_attention\")\n\n                            ## Feed Forward\n                            self.dec = feedforward(self.dec, num_units=[4*hp.hidden_units, hp.hidden_units])\n\n                # Final linear projection\n                self.logits = tf.layers.dense(self.dec, len(en2idx))\n                self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n                self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n                self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)\/ (tf.reduce_sum(self.istarget))\n                tf.summary.scalar('acc', self.acc)\n\n                if is_training:  \n                    # Loss\n                    self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=len(en2idx)))\n                    self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.y_smoothed)\n                    self.mean_loss = tf.reduce_sum(self.loss*self.istarget) \/ (tf.reduce_sum(self.istarget))\n\n                    # Training Scheme\n                    self.global_step = tf.Variable(0, name='global_step', trainable=False)\n                    self.optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n                    self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n\n                    # Summary \n                    tf.summary.scalar('mean_loss', self.mean_loss)\n                    self.merged = tf.summary.merge_all()\n\nif __name__ == '__main__':                \n    # Load vocabulary    \n    de2idx, idx2de = load_de_vocab()\n    en2idx, idx2en = load_en_vocab()\n    \n    # Construct graph\n    with tf.device('\/gpu:0'):\n        g = Graph(\"train\"); print(\"Graph loaded\")\n\n    # Start session\n    sv = tf.train.Supervisor(graph=g.graph, \n                             logdir=hp.logdir,\n                             save_model_secs=0)\n    with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        for epoch in range(1, hp.num_epochs+1):\n            print(epoch)\n            if sv.should_stop(): break\n            for step in tqdm(range(g.num_batch), total=g.num_batch, ncols=70, leave=False, unit='b'):\n                sess.run(g.train_op)\n\n            gs = sess.run(g.global_step)   \n            sv.saver.save(sess, hp.logdir + '\/model_epoch_%02d_gs_%d' % (epoch, gs))\n    \n    print(\"Done\")    ","4c614434":"from nltk.translate.bleu_score import corpus_bleu","9581f75c":"def eval(): \n    # Load graph\n    g = Graph(is_training=False)\n    print(\"Graph loaded\")\n    \n    # Load data\n    X, Sources, Targets = load_test_data()\n    print(X)\n    de2idx, idx2de = load_de_vocab()\n    en2idx, idx2en = load_en_vocab()\n     \n#     X, Sources, Targets = X[:33], Sources[:33], Targets[:33]\n     \n    # Start session         \n    with g.graph.as_default():    \n        sv = tf.train.Supervisor()\n        with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n            ## Restore parameters\n            sv.saver.restore(sess, tf.train.latest_checkpoint(hp.logdir))\n            print(\"Restored!\")\n              \n            ## Get model name\n            mname = open(hp.logdir + '\/checkpoint', 'r').read().split('\"')[1] # model name\n             \n            ## Inference\n            if not os.path.exists('\/kaggle\/working\/results\/'): os.mkdir('\/kaggle\/working\/results\/')\n            with codecs.open(\"\/kaggle\/working\/results\/\" + mname, \"w\", \"utf-8\") as fout:\n                list_of_refs, hypotheses = [], []\n                for i in range(len(X) \/\/ hp.batch_size):\n                     \n                    ### Get mini-batches\n                    x = X[i*hp.batch_size: (i+1)*hp.batch_size]\n                    sources = Sources[i*hp.batch_size: (i+1)*hp.batch_size]\n                    targets = Targets[i*hp.batch_size: (i+1)*hp.batch_size]\n                     \n                    ### Autoregressive inference\n                    preds = np.zeros((hp.batch_size, hp.maxlen), np.int32)\n                    for j in range(hp.maxlen):\n                        _preds = sess.run(g.preds, {g.x: x, g.y: preds})\n                        preds[:, j] = _preds[:, j]\n                     \n                    ### Write to file\n                    file_name = \"\/kaggle\/working\/results\/results.csv\"\n                    for source, target, pred in zip(sources, targets, preds): # sentence-wise\n                        got = \" \".join(idx2en[idx] for idx in pred).split(\"<\/S>\")[0].strip()\n                        pred_df = pd.DataFrame([[source, target, got]], columns = [\"Source\", \"Target\", \"Prediction\"])\n                        if not os.path.exists(file_name):\n                            with open(file_name, mode = \"w\") as f:\n                                pred_df.to_csv(f, header = True, index = False)\n                        else:\n                            with open(file_name, mode = \"a\") as f:\n                                pred_df.to_csv(f, header = False, index = False)\n                        # bleu score\n                        ref = target.split()\n                        hypothesis = got.split()\n                        if len(ref) > 3 and len(hypothesis) > 3:\n                            list_of_refs.append([ref])\n                            hypotheses.append(hypothesis)\n              \n#                ## Calculate bleu score\n #               score = corpus_bleu(list_of_refs, hypotheses)\n #               fout.write(\"Bleu Score = \" + str(100*score))\n                                          \nif __name__ == '__main__':\n    eval()\n    print(\"Done\")","8ad115ea":"results = pd.read_csv(\"\/kaggle\/working\/results\/results.csv\")","e28322ba":"results","47c1c5e0":"word_index","5cbfdfcb":"Based on code from GitHub, here: https:\/\/github.com\/Kyubyong\/transformer \n\nForked from the work done in Rachael's live coding sessions : https:\/\/www.twitch.tv\/rctatman"}}