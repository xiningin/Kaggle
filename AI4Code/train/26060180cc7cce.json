{"cell_type":{"2c1d1821":"code","8125cc28":"code","cc430d94":"code","8e4b6d08":"code","9faf5c4e":"code","74bda20a":"code","25f0a0a1":"code","3660da1e":"code","19316e86":"code","0ec9a765":"code","bd080c83":"code","c5592935":"code","97ebbfa9":"code","102c47b2":"code","c1e026a8":"code","45a1bea7":"code","433ec0af":"code","b6687798":"code","a07fcd72":"code","73ddf7fe":"code","88e5be24":"code","894264e3":"code","af2c2758":"code","46ca0e1f":"code","7932fcd8":"code","d558222a":"code","e847f207":"markdown","4b80b66b":"markdown","367dbb41":"markdown","44823a52":"markdown","0539c1c4":"markdown","4323264b":"markdown","e44c1328":"markdown","74e2c3b5":"markdown","512786ac":"markdown","a253fd84":"markdown","f298adbb":"markdown","2d6d097a":"markdown","4c0cf51c":"markdown"},"source":{"2c1d1821":"%%HTML\n<a id=\"Analysis\"><\/a>\n<center>\n<iframe width=\"700\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/vEzQ2NBq3fA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" style=\"position: relative;top: 0;left: 0;\" allowfullscreen ng-show=\"showvideo\"><\/iframe>\n<\/center>","8125cc28":"# pip install sentencepiece\n# pip install tensorflow-hub\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nprint(\"TF version: \", tf.__version__)\nprint(\"Hub version: \", hub.__version__)","cc430d94":"import pandas as pd\nimport numpy as np\n\n# For cleaning the text\nimport spacy\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport regex as re\nimport string\n\n# For visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_colwidth', None)\n\n# For building our model\nimport tensorflow.keras\nimport sklearn\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D","8e4b6d08":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nids = test.id\n\nprint('Total length of the dataset: ', len(train)+len(test))\nprint('shape of training set: ', train.shape)\nprint('shape of testing set: ', test.shape)","9faf5c4e":"# Detecting null values and records\n\ndf_concat = pd.concat([train, test], axis = 0).reset_index(drop = True)\nnulls = pd.DataFrame(np.c_[df_concat.isnull().sum(), (df_concat.isnull().sum()\/ len(df_concat))*100],\n                     columns = ['# of nulls', '% of nulls'],\n                     index = df_concat.columns)\nnulls","74bda20a":"for df in [train, test, df_concat]:\n\n    df.keyword.fillna('no_keyword', inplace = True)\n    df.location.fillna('no_location', inplace = True)","25f0a0a1":"df_concat.groupby(['location']).count().text.sort_values(ascending = False)","3660da1e":"# top 30 locations in the dataset\n\ntop_30 = df_concat.groupby(['location']).count().text.sort_values(ascending = False)[:30]\n\n# plot the top 30\n\nplt.figure(figsize = (6,10))\nsns.barplot(x = top_30, y = top_30.index);\nplt.xlabel('number of tweets');\n","19316e86":"# top 20 keywords in disastrous and non_disastrous tweets\n# We'll use training set for this \n\ncount_dis_keywords = train[train.target == 1].groupby(['keyword']).count().sort_values(by = 'target', ascending = False)[:20]\n\ncount_non_dis_keywords =  train[train.target == 0].groupby(['keyword']).count().sort_values(by = 'target', ascending = False)[:20]\n\nsns.set(style=\"white\")\n\n\nfig, ax_ = plt.subplots(1, 2, figsize = (25,10));\n\n# left side, the plot for keywords in disastrous tweets\n\nsns.barplot(x = count_dis_keywords.target, # count of each keyword\n            y = count_dis_keywords.index, # index of this df is our keywords\n            ax = ax_[0],\n            palette = 'Reds_r', label = 'dis')\n    \n\n\n# right side, the plot for non_disastrous tweets\n\nsns.barplot(x = count_non_dis_keywords.target, y = count_non_dis_keywords.index, \n            ax = ax_[1], palette = 'Greens_d', label = 'non_dis')\n\n\nfor ax in [ax_[0], ax_[1]]:\n    \n    ax.set_title('Number of tweets per keyword', fontsize = 20) # setting title\n    \n    ax.set_ylabel('') \n    ax.set_xlabel('')\n\n    ax.set_yticklabels(labels =ax.get_yticklabels() ,\n                       fontsize = 15)","0ec9a765":"for df in [train, test, df_concat]:\n\n    df.drop(columns = ['location', 'keyword', 'id'], inplace = True)","bd080c83":"nlp = spacy.load(\"en\")\nsp = spacy.load('en_core_web_sm')\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# spacy (362 words)\nspacy_st = nlp.Defaults.stop_words\n# nltk(179 words)\nnltk_st = stopwords.words('english')\n\ndef clean(tweet, http = True, punc = True, lem = True, stop_w = True):\n    \n    if http is True:\n        tweet = re.sub(\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]*\", '', tweet)\n\n    # stop words\n    # in here I changed the placement of lower for those of you who want to use\n    # Cased BERT later on.\n    if stop_w == 'nltk':\n        tweet = [word for word in word_tokenize(tweet) if not word.lower() in nltk_st]\n        tweet = ' '.join(tweet)\n\n    elif stop_w == 'spacy':\n        tweet = [word for word in word_tokenize(tweet) if not word.lower() in spacy_st]\n        tweet = ' '.join(tweet)\n\n    # lemmitizing\n    if lem == True:\n        lemmatized = [word.lemma_ for word in sp(tweet)]\n        tweet = ' '.join(lemmatized)\n\n    # punctuation removal\n    if punc is True:\n        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n        \n    # removing extra space\n    tweet = re.sub(\"\\s+\", ' ', tweet)\n    \n    return tweet","c5592935":"df_concat['cleaned_text'] = df_concat.text.apply(lambda x: clean(x, lem = False, stop_w = 'nltk', http = True, punc = True))","97ebbfa9":"cleaned_train = df_concat[:train.shape[0]]\ncleaned_test = df_concat[train.shape[0]:]","102c47b2":"%%HTML\n<a id = \"Word_Embeddings\"><\/a>\n<center>\n<iframe width=\"700\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/t5wdTK-QtLA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" style=\"position: relative;top: 0;left: 0;\" allowfullscreen ng-show=\"showvideo\"><\/iframe>\n<\/center>","c1e026a8":"%%HTML\n<a id = \"BERT\"><\/a>\n<center>\n<iframe width=\"700\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/wh6x2UrCO4Y\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" style=\"position: relative;top: 0;left: 0;\" allowfullscreen ng-show=\"showvideo\"><\/iframe>\n<\/center>","45a1bea7":"# Bert Tokenizer for all of them\n\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nimport tokenization\nFullTokenizer = tokenization.FullTokenizer\n","433ec0af":"ans = input(\"Which Bert should I use? \\n a. Base uncased \\n b. Large uncased \\n c. Basic cased \\n d. Large cased \\n\")\n\nif ans is 'a':\n    BERT_MODEL_HUB = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2'\n    disc = 'Base_uncased'\nelif ans is 'b':\n    BERT_MODEL_HUB = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/2' \n    disc = 'Large_uncased'\nelif ans is 'c':\n    BERT_MODEL_HUB = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/2'\n    disc = 'Base_cased'\nelif ans is 'd':\n    BERT_MODEL_HUB = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-24_H-1024_A-16\/2'\n    disc = 'Large_cased'\n\nbert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)\nprint('Bert layer is ready to use!')\n\n\n\nif ans =='a' or ans =='b':\n    to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n\n    tokenizer = FullTokenizer(vocabulary_file, to_lower_case)\n    \n\n    \nelif ans =='c' or ans =='d':\n    \n    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    \n    tokenizer = FullTokenizer(vocabulary_file, do_lower_case=False)\n\n\nprint('Bert Tokenizer is ready!!!')\n","b6687798":"# put your own sentence here, try words like openminded, undercover, etc., and see what you get\nsentence = 'Terrorist will crush the Tower'\nprint('Tokenized version of {} is : \\n {} '.format(sentence, tokenizer.tokenize(sentence)))","a07fcd72":"def tokenize_tweets(text_):\n    return tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenizer.tokenize(text_) + ['[SEP]'])\n\ndf_concat['tokenized_tweets'] = df_concat.cleaned_text.apply(lambda x: tokenize_tweets(x))\n\ncleaned_train.head(2)","73ddf7fe":"# what is the maximum length of our tokenized text?\n\nmax_len = len(max(df_concat.tokenized_tweets, key = len))\n\n\nprint('The maximum length of each sequence besed on tokenized tweets is:', max_len)\n\ndf_concat['padded_tweets'] = df_concat.tokenized_tweets.apply(lambda x: x + [0] * (max_len - len(x)))\ndf_concat.head(2)","88e5be24":"class TweetClassifier:\n    \n    def __init__(self, tokenizer, bert_layer, max_len, lr = 0.0001,\n                 epochs = 15, batch_size = 32,\n                 activation = 'sigmoid', optimizer = 'SGD',\n                 beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n                 metrics = 'accuracy', loss = 'binary_crossentropy'):\n        \n        self.lr = lr\n        self.epochs = epochs\n        self.max_len = max_len\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.bert_layer = bert_layer\n        \n\n        self.activation = activation\n        self.optimizer = optimizer\n        \n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon =epsilon\n        \n        self.metrics = metrics\n        self.loss = loss\n\n        \n    def encode(self, texts):\n        \n        all_tokens = []\n        masks = []\n        segments = []\n        \n        for text in texts:\n            \n            tokenized = self.tokenizer.convert_tokens_to_ids(['[CLS]'] + self.tokenizer.tokenize(text) + ['[SEP]'])\n            \n            len_zeros = self.max_len - len(tokenized)\n            \n            \n            padded = tokenized + [0] * len_zeros\n            mask = [1] * len(tokenized) + [0] * len_zeros\n            segment = [0] * self.max_len\n            \n            all_tokens.append(padded)\n            masks.append(mask)\n            segments.append(segment)\n        \n        return np.array(all_tokens), np.array(masks), np.array(segments)\n\n\n    def make_model(self):\n        \n        # Shaping the inputs to our model\n        \n        input_ids = Input(shape = (self.max_len, ), dtype = tf.int32, name = 'input_ids')\n        \n        input_mask = Input(shape = (self.max_len, ), dtype = tf.int32, name = 'input_mask')\n        \n        segment_ids = Input(shape = (self.max_len, ), dtype = tf.int32,  name = 'segment_ids')\n\n        \n        pooled_output, sequence_output = bert_layer([input_ids, input_mask, segment_ids] )\n\n\n\n        clf_output = sequence_output[:, 0, :]\n        \n        out = tf.keras.layers.Dense(1, activation = self.activation)(clf_output)\n        \n        \n        model = Model(inputs = [input_ids, input_mask, segment_ids], outputs = out)\n        \n        # define the optimizer\n\n        if self.optimizer is 'SGD':\n            optimizer = SGD(learning_rate = self.lr)\n\n        elif self.optimizer is 'Adam': \n            optimizer = Adam(learning_rate = self.lr, beta_1=self.beta_1, beta_2=self.beta_2, epsilon=self.epsilon)\n\n        model.compile(loss = self.loss, optimizer = self.optimizer, metrics = [self.metrics])\n        \n        print('Model is compiled with {} optimizer'.format(self.optimizer))\n        \n        return model\n    \n    \n    \n    \n    def train(self, x):    \n        \n        checkpoint = ModelCheckpoint('model.h5', monitor='val_loss',\n                                     save_best_only=True)\n            \n        \n        model = self.make_model()\n        \n        X = self.encode(x['cleaned_text'])\n        Y = x['target']\n        \n        model.fit(X, Y, shuffle = True, validation_split = 0.2, \n                  batch_size=self.batch_size, epochs = self.epochs,\n                  callbacks=[checkpoint])\n                \n        print('Model is fit!')\n        \n            \n    def predict(self, x):\n        \n        X_test_encoded = self.encode(x['cleaned_text'])\n        best_model = tf.keras.models.load_model('model.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n        y_pred = best_model.predict(X_test_encoded)\n        \n        \n        return y_pred\n","894264e3":"classifier = TweetClassifier(tokenizer = tokenizer, bert_layer = bert_layer,\n                              max_len = max_len, lr = 0.0001,\n                              epochs = 3,  activation = 'sigmoid',\n                              batch_size = 32,optimizer = 'SGD',\n                              beta_1=0.9, beta_2=0.999, epsilon=1e-07)","af2c2758":"classifier.train(cleaned_train)","46ca0e1f":"!git clone https:\/\/github.com\/mitramir55\/Kaggle_NLP_competition.git\nperfection = pd.read_csv('Kaggle_NLP_competition\/perfect_submission.csv')","7932fcd8":"y_pred = np.round(classifier.predict(cleaned_test))\nprint('The score of prediction: ', sklearn.metrics.f1_score(perfection.target, y_pred, average = 'micro'))","d558222a":"# Submission\nsample_sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nids = sample_sub.id\nfinal_submission = pd.DataFrame(np.c_[ids, y_pred.astype('int')], columns = ['id', 'target'])\nfinal_submission.to_csv('final_submission.csv', index = False)\nfinal_submission.head()","e847f207":"In this Kernel we'll take a look at the data, analyze it, clean it and then at the end make predictions for its testing set with a BRET model. Watch the videos to gain a better insight of the coding process and learn about the model. I hope this kernel will be useful for you and if you found it helpful please give this kernel and the videos an upvote! You can ask your questions here or on YouTube. I'll also link the most inspiring resources at the end of this notebook.\n\nAlso, if you'd like to read and learn more about Word Embeddings and read all the instructions check out [this medium article](https:\/\/medium.com\/p\/e22043291ac4\/edit).\n\n*  [Analysis](#Analysis)\n*  [Word Embeddings](#Word_Embeddings)\n*  [BERT in code](#BERT)","4b80b66b":"![disastrous%20tweets2-2.jpg](attachment:disastrous%20tweets2-2.jpg)","367dbb41":"**How we are going to clean:**\n\nclean(x, lem = False, stop_w = 'nltk', http = True, punc = True)","44823a52":"Definition of Word embeddings and how Bert uses them:","0539c1c4":"# Constructing the model!","4323264b":"## Exploring The Locations and Keywords","e44c1328":"# **Cleaning the Data!**","74e2c3b5":"### **KeyWord**","512786ac":"* **Optional**: You can check your score with the perfect_submission file available on Kaggle. I copied the file and cleaned it for easier use.","a253fd84":"Follow along with the coding:","f298adbb":"###  **Location**","2d6d097a":"Some inspiring and very useful notebooks:\n* [Gunes Evitan](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n* [Shahules](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)\n* [help in Stackoverflow](https:\/\/stats.stackexchange.com\/questions\/246512\/convolutional-layers-to-pad-or-not-to-pad)","4c0cf51c":"# **BERT and Word Embeddings!**"}}