{"cell_type":{"93ba7987":"code","49b28a66":"code","04e1746f":"code","16299add":"code","90214fb3":"code","769e708a":"code","f5f1f57d":"markdown","7773ee6a":"markdown","742a2285":"markdown","e15ecc63":"markdown"},"source":{"93ba7987":"import numpy as np\nimport pandas as pd\nimport torch as tr\nimport torch.nn as nn\nfrom torch.autograd import Variable","49b28a66":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ny = train.target\ntrain.drop([\"target\", \"id\"], axis=1, inplace=True)\ntest.drop([\"id\"], axis=1, inplace=True)\nx = tr.Tensor(train.append(test).values)\nx = Variable(x).cuda()\ndel train, test","04e1746f":"class autoencoder(nn.Module):\n    def __init__(self):\n        super(autoencoder, self).__init__()\n        indim = 300\n        layers = []\n        for nunits in [400, 350, 300, 295, 290]:\n            layers.append(nn.Linear(indim, nunits))\n            layers.append(nn.ELU(True))\n            layers.append(nn.BatchNorm1d(nunits))\n            indim = nunits\n        layers.pop()\n        self.encoder = nn.Sequential(*layers)\n        layers = []\n        for nunits in [290, 293, 295, 297, 300]:\n            layers.append(nn.Linear(indim, nunits))\n            layers.append(nn.ELU(True))\n            layers.append(nn.BatchNorm1d(nunits))\n            indim = nunits\n        layers.pop()\n        self.decoder = nn.Sequential(*layers)\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","16299add":"model = autoencoder().cuda()\ncriterion = nn.MSELoss()\noptimizer = tr.optim.Adadelta(model.parameters())\nmse = []\nlast_improvement = 0\nbest_loss = float(\"Inf\")\nwhile last_improvement<100:\n    loss = criterion(model(x), x)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    mse.append(loss.data.item())\n    if best_loss > mse[-1]: \n        last_improvement=0\n        best_loss = mse[-1]\n    else: last_improvement+=1\n    # print(\"MSE: \", mse[-1], \"\\tLast improvement: \", last_improvement, end=\"\\r\")\nprint(\"best MSE achieved: \", best_loss)","90214fb3":"criterion = nn.L1Loss()\nmae = criterion(model(x), x).data.item()\nprint(\"MAE: \", mae)","769e708a":"tr.save(model.state_dict(), '.\/autoencoder.pth')\nmodel.eval()\nz = model.encoder(x).cpu().data.numpy()\ndf = pd.DataFrame(z).join(pd.DataFrame({\"target\" : y}))\ndf.to_csv(\".\/encoded_features.csv\")","f5f1f57d":"# Are variables independant?\nMany kernels have shown that the variables are uncorrelated, but other kind non-linear relashionship between features could exists. An equivalent question could be: the data lays in any kind of manifold? I believe that this information could help to choose the best aproach to tackle this competition. \n\nThis kernel try to answer that quetion using an autoencoder. The hipothesys is that, if the data lays in any kind of manifold, an autoencoder should be able to reduce dimensionality without too much error.\n\nSpoiler: the variables are independant.","7773ee6a":"Well, no...","742a2285":"Lets see if the mean absolute error is small (it is more interpretable than the MSE)","e15ecc63":"## The model used\nWe only reduce the dimensionality from 300 to 290."}}