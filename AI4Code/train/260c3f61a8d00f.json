{"cell_type":{"36a400d1":"code","48bc1086":"code","2fe6278b":"code","0f9bf224":"code","5ac298b2":"code","d38d7263":"code","c2263f7d":"code","a81fb87c":"code","115cb084":"code","001f7cce":"code","35826aa8":"code","2a6d66b3":"code","21b9a78d":"code","137d2dc7":"code","989ecda1":"code","8a8507a9":"code","1f4aa791":"code","def19793":"code","4c3d0c84":"code","d2fb6fa4":"code","ef51de76":"code","05f3bed4":"code","1e58a5cf":"code","9c75d65d":"code","9892207b":"code","19ae3953":"code","f4172b6a":"code","2185d618":"code","1e120059":"code","16fe6690":"code","2c6fe43f":"code","cdb431b4":"code","a2cf987a":"code","4b3f5d06":"code","4aea8f0c":"code","651d5618":"code","b1679af0":"code","ea074822":"code","8889abbd":"code","7a7f13eb":"code","cda321f8":"code","edc921b3":"code","5b704bb1":"code","3bdf8c60":"code","6b51844a":"code","8ac18fe7":"code","c23898fa":"code","aa68c807":"code","26fb9e34":"code","7541dfb2":"code","7168b5d1":"code","84e1e249":"code","3e7486aa":"code","eb3086fc":"code","b0fdc9c8":"code","8fc449ef":"code","3627e130":"code","9025b55f":"code","9901c308":"markdown","e7d3a6ff":"markdown","750cb631":"markdown","4b54b6f8":"markdown","b286626d":"markdown","d1128cbf":"markdown","8324227f":"markdown","50c0e5fb":"markdown","be8de581":"markdown","76ad2e7d":"markdown","edea42e5":"markdown","e50483e6":"markdown","93aeb166":"markdown","47f013f8":"markdown","4240f387":"markdown","2320625e":"markdown","e90fe25a":"markdown","1d58172a":"markdown","af65989a":"markdown","b72f016d":"markdown","c0cfabbc":"markdown","b4a240d1":"markdown"},"source":{"36a400d1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.gofplots import qqplot\nimport statsmodels.api as sm\nfrom scipy.stats import shapiro\n%matplotlib inline","48bc1086":"houses = pd.read_csv(\"..\/input\/brasilian-houses-to-rent\/houses_to_rent_v2.csv\")","2fe6278b":"houses.info()","0f9bf224":"houses.head()","5ac298b2":"houses.describe()","d38d7263":"houses['city'].unique() ### No problem on cities names.","c2263f7d":"rio = houses[houses['city'] == \"Rio de Janeiro\"]","a81fb87c":"rio['animal'].unique()   ## Boolean variable, so we are going to transform this text variable into a dummy variable","115cb084":"rio.loc[:,'animal'] = rio['animal'].apply(lambda x: 1 if x == 'acept' else 0) # if 1 -> place accepts animals.","001f7cce":"rio.head()","35826aa8":"rio.loc[:, 'furniture'].unique()   # Same happens.That said we can make that column a dummy variable. \n                                   # 1 is going to represent furnished places. ","2a6d66b3":"rio.loc[:, 'furniture'] = rio['furniture'].apply(lambda x: 1 if x == 'furnished' else 0)","21b9a78d":"rio.head()","137d2dc7":"rio['floor'].unique() # We need to deal with the '-' value.","989ecda1":"rio['floor'].value_counts() # We still get a lot of data points if we drop those rows in which we have the\n                            # \"-\" value for the floor variable, so we might as well drop them.","8a8507a9":"bye = rio[rio['floor'] == \"-\"].index ## Getting the indexes of the rows which have the value \"-\" for the floor variable","1f4aa791":"rio = rio.drop(bye)","def19793":"rio['floor'] = rio['floor'].apply(lambda x: int(x))","4c3d0c84":"rio.head()","d2fb6fa4":"rio.info()","ef51de76":"rio = rio.drop(axis = 1, columns = ['property tax (R$)', 'total (R$)', 'hoa (R$)', 'fire insurance (R$)'])","05f3bed4":"plt.figure(figsize = (15,8))\nsns.heatmap(rio.corr(), annot = True) ## Floor and animal variables seems not quite related to anything","1e58a5cf":"sns.pairplot(rio)\nplt.tight_layout()","9c75d65d":"plt.figure(figsize = (15,6))\n\n\nplt.subplot(1, 2, 1)\nplt.title(\"Comparison between the standard distriution and Log10 transformed distribution\")\nsns.distplot(a = rio['rent amount (R$)'])\n\nplt.subplot(1, 2, 2)\nsns.distplot(a = np.log(rio['rent amount (R$)']+1))   \n\n","9892207b":"plt.figure(figsize = (15,6))\n\n\nplt.subplot(1, 2, 1)\nplt.title(\"Comparison between the standard distriution and Log10 transformed distribution\")\nsns.distplot(a = rio['floor'])\n\nplt.subplot(1, 2, 2)\nsns.distplot(a = np.log(rio['floor']+1))","19ae3953":"newOrder = ['city','area','rooms','bathroom','parking spaces','floor','rent amount (R$)', 'furniture', 'animal']\nrio = rio[newOrder]\nrio = rio.drop(labels = 'city', axis = 1)\nrio.head()     #Changing order of columns in order to separate continous variables and dummy variables","f4172b6a":"rioCont, rioDummy = rio.loc[:, 'area':'rent amount (R$)'], rio.loc[:, 'furniture':'animal'] ","2185d618":"rioCont = np.log(rioCont + 1)","1e120059":"newRio = pd.concat([rioCont, rioDummy], axis = 1, join = 'inner')\nnewRio.head()","16fe6690":"plt.figure(figsize = (10,6))\nsns.heatmap(newRio.corr())","2c6fe43f":"sns.pairplot(newRio)","cdb431b4":"sns.distplot(a = newRio['parking spaces'])  #Parking spaces is still messed up =( ","a2cf987a":"sns.lmplot(data = newRio, x = 'area', y = 'rent amount (R$)')  # Data suggests strong linear relation between the rent values and the area","4b3f5d06":"# First we create a simple function for getting the VIFs for each feature of the dataset\n\ndef vif(dataframe, add_intercept = True):\n    if add_intercept == True:\n        dataframe = sm.add_constant(dataframe)\n        \n    for i in dataframe.columns:\n        y = dataframe[[i]]\n        x = dataframe.drop(labels = i, axis = 1)\n        model = sm.OLS(y, x)\n        results = model.fit()\n        rSquared = results.rsquared\n        vifValue = round(1\/(1 - rSquared), 2)\n\n        print('---------------------------------------------------------------------------------------------')\n        print(\"The regression of the independent variable \", str.upper(i), \" returns a R squared value of: \", rSquared)\n        print('\\nThat said, the VIF for this variable is: ', vifValue)\n        ","4aea8f0c":"vif(rio)   # VIFs are low, so we can assume that multicolinearity is not a problem (we can ignore the constant VIF).","651d5618":"from sklearn.model_selection import train_test_split","b1679af0":"list(newRio.columns)\n\nnewCols = ['area',\n 'rooms',\n 'bathroom',\n 'parking spaces',\n 'floor',\n 'furniture',\n 'animal','rent amount (R$)']\n\nnewRio = newRio[newCols]\nnewRio.head()                  #Changing order of cols to put rent value in the end","ea074822":"X = newRio.loc[:, 'area':'animal']\ny = newRio['rent amount (R$)']\n\nprint(X.head())\nprint(y.head())","8889abbd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)","7a7f13eb":"import statsmodels.api as sm       # Statsmodels for the regression summary","cda321f8":"X_constant = sm.add_constant(X_train) ","edc921b3":"model = sm.OLS(y_train, X_constant).fit()\npredictions = model.predict(X_constant) ","5b704bb1":"print_model = model.summary()    \nprint(print_model)","3bdf8c60":"X_constant = X_constant.drop(['parking spaces'], axis = 1) ","6b51844a":"model = sm.OLS(y_train, X_constant).fit()\npredictions = model.predict(X_constant) \nprint_model = model.summary()    \nprint(print_model)","8ac18fe7":"#Evaluating Serial Autocorrelation\nresiduals = y_train - predictions\nsns.scatterplot(x = predictions, y = residuals)\nplt.title(\"Fitted values x Residuals\")\nplt.ylabel(\"residuals\")\nplt.xlabel(\"fitted values\")\n\n#Residuals seem random, no signs of autocorrelation.","c23898fa":"#Evaluating residuals distribution\nsns.distplot(residuals)\nplt.title(\"Residuals Distribuition\")\nplt.xlabel(\"Residuals\")\n\n# Distribution seems roughly normal, so we are fine.","aa68c807":"X_test_no_parking = X_test.drop(axis = 1, labels = 'parking spaces')","26fb9e34":"X_test_no_parking_constant = sm.add_constant(X_test_no_parking)","7541dfb2":"forecasting = model.predict(X_test_no_parking_constant)\nforecasting","7168b5d1":"test_residuals = y_test - forecasting","84e1e249":"RSS = sum((test_residuals)**2)   #Residual Sum of Squares\nTSS = sum((y_test - y_test.mean())**2)   #Total Sum of Squares\nrSquared = (TSS - RSS)\/TSS\nRMSE = np.sqrt(RSS\/len(test_residuals))\nratio = RMSE\/y_test.mean()\n\nprint(\"The Residual Sum of Squares: \", RSS)\nprint(\"The total sum of squares: \", TSS)\nprint(\"The R squared: \", rSquared)\nprint(\"The Root-mean squared error: \", RMSE)\nprint(\"The ratio between the RMSE and the mean of the test data (y_test.mean()): \", round(ratio,4))","3e7486aa":"X_test_no_parking_constant.loc[495,:]","eb3086fc":"guess = model.get_prediction(X_test_no_parking_constant.loc[495,:])","b0fdc9c8":"guess.summary_frame(alpha = 0.05)  #setting the value for alpha. Our confidence interval will be of 95%","8fc449ef":"np.exp(X_test_no_parking_constant.loc[495,:])-1   #just taking the row labeled as 495 as a sample.","3627e130":"print(\"Upper confidence interval\",np.exp(7.880193)-1)\nprint(\"Expected value: \", np.exp(7.83814)-1)\nprint(\"Lower confidence interval: \", np.exp(7.796087)-1)","9025b55f":"print(\"Real data point: \",np.exp(7.824446)-1)  #Very good!","9901c308":"# Checking for Serial correlation and Normality of residuals","e7d3a6ff":"We can see that some of the distributions are positive skewed, so we are going to use the Log transformation to reduce the high leverage values impact, and possible future problems with heterocedasticity.\n\nIn fact, for the model, we are going to use the x = log(x+1) transformation, since the parking spaces \nvariable assume 0 values, and the log of 0 is not defined.\n\nThat said, our model will be a log-log linear model.","750cb631":"# Evaluating the model","4b54b6f8":"# References","b286626d":"The model provided satisfying prediction for the rental values. A 6% ration between the RMSE and the mean of the Y real data is pretty satisfying and the model can make good and useful predictions. It would be very interesting to have access to more features, like the neighborhood of the apartment, for example. With new useful information, we should be able to increase the amount of explained variability and improve the model's interpretation.\n\nThat said, that was a really fun and interesting work to do. Feel free to leave comments and feedbacks. Thank you!","d1128cbf":"# Exploratory Data Analysis (EDA)","8324227f":"Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning : with Applications in R. New York :Springer, 2013.\n","50c0e5fb":"# Model Estimation","be8de581":"For a quick example of predicting new data, i'll select a row of the test data and check our predictions for that, and the confidence intervals of our predictions.\n\nWe could do the same process for new data that appears.","76ad2e7d":"We are going to begin the analysis doing some EDA, just to take a look at our data, and see what is going on, what are the correlations between variables, and check for possible multicolinearity.","edea42e5":"For a confidence of 5%, the \"parking spaces\" variable isn't statistically relevant (p-value of 18,6%), so, we are dropping it.","e50483e6":"# Conclusion","93aeb166":"Hello! My name is Renan. I'm Brazilian and I actually live in Rio de Janeiro.\n\nThis dataset made me wonder how some of the independent variables are related to the rental prices in my city. That said, i am going to limit the analysis to the city of Rio de Janeiro.\n\nSo, I'm going to perform some Exploratory Data Analysis and also some Multiple Linear Regression Analysis using the Ordinary Least Squares method. By doing this, we are going to try to explain how the dataset features impact the rental prices.\n\nI am considering the categorical variables, such as if the apartment has furniture or not, as dummy variables.\n\nThe model: ![image.png](attachment:image.png)\n\n\nIn which variables X1 - X6 are the continuos variables, and X7 and X8 are the dummies.","47f013f8":"Since we have a log-log model, we need to undo the log(x+1) transformations, both on the dependent and on the independent variables, in order to have access to the real data. Ignore the transformations on const and on the dummy variables (furniture and animal). ","4240f387":"Models seems pretty good. We have a R squared of ~ 57%, that means, amongst the total variability of the output variable, the variance explained by the model correspond to 57% of the total.\n\nBesides, our ratio is roughly 6%. We can interpret that as: If we are using our model to predict the rental prices, our predictions will be, on average, off by 6% of the real data.","2320625e":"# Introduction","e90fe25a":"# Forecasting","1d58172a":"For evaluating the prediction capabilities of the model, we are going to use the train_test_split process. This method consists in dividing the available data in two parts: The training data and the test data.\n\nThe training data is the one we are going to use to estimate the parameters and fit our model. We should do that in order to have a set of data available for testing the model after its development, so we can check if our model's prediction capacity is good only because it's biased to our data.\n\nThat said, the test data is the one we are using for actually testing the model's prediction capability to a new set of data. That's the performance we should expect after testing the model on new real world data.\n\nBesides that, we are going to build some statistics in order to evaluate the model's precision and accuracy. Since we have many lines (n > 1000), we are using a ratio between the Root-mean squared error (RMSE) and the mean of the predicted values to estimate how off our fitted values are from the real data, on average.\n\nSo the ratio formula is: ratio = RMSE \/ mean(y_test)","af65989a":"# Checking for multicolinearity","b72f016d":"Here we are going to use the function sm.add_constant in order to add a column of 1s in the dataset. That is necessary for fitting a linear model with intercept in statsmodels. For more details check the documentation for the sm.OLS function on their website:\n\nhttps:\/\/www.statsmodels.org\/devel\/generated\/statsmodels.regression.linear_model.OLS.html","c0cfabbc":"In order to look for annoying multicolinearity in our data, we are going to take a look on the Variance Inflation Factor (VIF), which is really nothing fancy. The VIF is just an statistic generated from the R squared value of the linear regression of the independent variable we are checking for multicolinearity onto the remaining independent variables. \n\nUsual VIF thresholds are 5 or 10 (HASTIE et al, 2013). If we have values greater than those, we should deal with multicolinearity, even discarding the said variable, or mixing it with other independent variables and creating a new indicator. For more reading on the VIF: https:\/\/en.wikipedia.org\/wiki\/Variance_inflation_factor","b4a240d1":"In order to adjust the skewness in the dependent variable and in some of the independent variables, we are going to use the log-log linear model for the Linear Regression, but keeping the dummy variables in the regular form (because we can't take the log of 0, and log 1 = 0).\n\nAlso, our parking places variable takes 0 values, so our transformation actually is going to be: Bi * Xi => Bi* Ln * (Xi + 1) \n\nThat said, our transformed linear regression model is: Ln(y+1) = Ln (B0 + 1) + B1Ln(x+1) + B2Ln(x+1) + ... + B7X7 + B8X8  -> Last two variables are the dummy ones."}}