{"cell_type":{"9e565a84":"code","540980f1":"code","a450bd53":"code","0bf4d1b5":"code","3e7c5ea6":"code","75cf86a3":"code","608a9534":"code","f70a620d":"code","f219c5bd":"code","041c4689":"code","98bed071":"code","0dbe5169":"code","88f59fe2":"code","28e9b272":"code","ca11be66":"code","b5376882":"code","41a5176a":"code","2891f670":"code","c4ae8602":"code","dadf4181":"code","852768b9":"code","4841e669":"code","f7d8843e":"code","39e36db6":"code","45cb1476":"code","549fe0c7":"code","821b4925":"code","d6f4736c":"code","66db6066":"code","3dc33135":"code","adb165ae":"code","966dd221":"code","3c01fffc":"code","3a0a653e":"code","57a51d8d":"code","d587d63d":"code","04ab3c0e":"code","8d8f35a8":"code","f55e8e2d":"code","089f80b2":"code","b0a4e719":"code","7c92e1ae":"code","c544031f":"code","e347371d":"code","4b961e6f":"code","17709168":"code","d37d78c8":"code","c5ac1dfe":"code","fcf4ae6d":"code","e67c2855":"code","9081b0cf":"code","4a45de7d":"code","7041026a":"code","41bba29d":"code","dae4b857":"code","f51836bd":"code","4c9c5c35":"code","93693202":"code","367ba829":"code","4726ad65":"code","dfee4c8a":"code","b604d33d":"code","9bc59b5f":"code","f674cc97":"code","eb11ef0f":"code","deacdab4":"code","c96a70cb":"code","e448a21c":"code","0f86cc71":"code","7c248fde":"code","893e8cd8":"code","b20d2e67":"code","4389b4af":"code","a33a86b7":"code","9d1f063f":"code","96248235":"code","42506422":"code","2f5478b4":"code","daccb8b2":"code","71fb48a9":"code","8e49f95b":"code","22fd7598":"code","d54cd78a":"code","e9d18cc1":"code","0a84b845":"code","67c20cc8":"code","c579c8ca":"code","210144a9":"code","8a3ecd5c":"code","60d74380":"code","f3aceb5e":"code","0c7609c1":"code","26782d50":"code","2367942b":"code","b18a33c5":"code","077cec24":"code","c6f39631":"code","1d95f97c":"code","b1464ebf":"code","125a9c39":"code","57705a65":"code","eb349fdb":"code","82edaaa1":"code","9caadc29":"code","6835a104":"code","9906c121":"code","33e6c016":"code","6f4ff6fd":"code","d08862e2":"code","419a2640":"code","ee18c365":"code","43ecc2db":"code","05ec2089":"code","26cb9b62":"code","c36962cc":"code","28812158":"code","2b8e452f":"code","df6a99c8":"code","faeceef9":"code","7a48b0de":"code","6812e5e2":"code","07ff37a2":"code","47eacb15":"code","cef1b386":"code","dca5d788":"code","02ab4c7b":"code","0497bff7":"code","c9cdb613":"code","66d5316f":"code","95b5de38":"code","8ad4b931":"code","50529138":"code","81b23bc6":"code","a8523371":"code","c9b9ac9c":"code","196c76ac":"code","11c51715":"code","cee89445":"code","04d28893":"code","367a93e6":"code","e8f3b98b":"code","3100f8e8":"markdown","2f03f975":"markdown","65456d45":"markdown","3d59f8b4":"markdown","3fabd421":"markdown","7dd994d7":"markdown","7d9195ed":"markdown","a9dacee1":"markdown","72cf08ed":"markdown","f3be9f04":"markdown","bfd5b908":"markdown","43e40799":"markdown","f615191c":"markdown","bbff3012":"markdown","0a623929":"markdown","ffe27d7c":"markdown","8e36ab38":"markdown","94914d19":"markdown","e99b6a7f":"markdown","ad2faa93":"markdown","27d38752":"markdown","a980af77":"markdown","8bf4316d":"markdown","26396da6":"markdown","b45cf13d":"markdown","d6898f18":"markdown","03171769":"markdown","d83424a3":"markdown","dbb65393":"markdown","9f245c5b":"markdown","d9a4be1f":"markdown","8c4eb99b":"markdown","532d90f7":"markdown","2639dacf":"markdown","d2753b01":"markdown","095525bc":"markdown","17e7524a":"markdown","132cb2f6":"markdown","37a990bf":"markdown","e1a8907e":"markdown","8985e40c":"markdown","bd7d8103":"markdown","9a17c1a2":"markdown","71b33944":"markdown","4164c751":"markdown","9111c1a9":"markdown","63f7f0c1":"markdown","2b54e032":"markdown","36910c35":"markdown","d88e2a55":"markdown","a88e1b47":"markdown","c6c1f695":"markdown","a7c00df6":"markdown","fc980a9a":"markdown","6d4cb128":"markdown","6d79a46f":"markdown","a555b015":"markdown","3f4e4be0":"markdown","86c19e06":"markdown","980a929a":"markdown","8427e619":"markdown","6371d88d":"markdown","5ac80fb6":"markdown","cf269443":"markdown","aa11d3d4":"markdown","d3f2c7bf":"markdown","b41293d3":"markdown","41723f00":"markdown","076b25f3":"markdown","5ec88ebf":"markdown","b24acc04":"markdown","293805a0":"markdown","af7f5459":"markdown","a2273c78":"markdown","eb320bea":"markdown","530c444d":"markdown","24e33d83":"markdown","97c54232":"markdown","35ad95d7":"markdown","974ecca6":"markdown","fd77999b":"markdown","484a90f5":"markdown","0e82d94c":"markdown","a9071f0e":"markdown","b4dcb774":"markdown","93803dae":"markdown","efa47c21":"markdown","705e5362":"markdown","eefdb541":"markdown","abaffd0c":"markdown","6476e151":"markdown","2e063a85":"markdown","37eda2bc":"markdown","8c083cb9":"markdown","77720a95":"markdown","3986b7c6":"markdown","4bd41de0":"markdown","c5bee0af":"markdown","7a789640":"markdown","daf7d49f":"markdown","b4b0a1a6":"markdown","b93b1b2c":"markdown","e01c9f3c":"markdown","eac3a397":"markdown","1ecb8590":"markdown","dac0bad8":"markdown","6153cd58":"markdown","eb288bb1":"markdown","ecc081f3":"markdown","5ff2b3d8":"markdown","474bad16":"markdown","f1e6a8ae":"markdown","71733da8":"markdown","ba543ed9":"markdown","ddb04d15":"markdown","1364779f":"markdown","a3cee6d8":"markdown","52836065":"markdown","892bb498":"markdown","435a044b":"markdown","5acde449":"markdown","7c7ef52e":"markdown","20fae2a6":"markdown","7433a5b9":"markdown","f2b44bd9":"markdown","97616545":"markdown","87f3b4fb":"markdown","6ae20ee6":"markdown","57192a82":"markdown","0ef3e99d":"markdown","02a7374d":"markdown","b537ae4c":"markdown","f602773d":"markdown","c1c1d50b":"markdown","99ebcab3":"markdown"},"source":{"9e565a84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","540980f1":"# plotting and stats libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\n\n# report warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Encoders\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder","a450bd53":"KEEP_COLS_WITH_MAX_PERCENT_OF_SAME_FEATURE=.99\nDROP_ROWS_WITH_1_NULL_ITEM = False","0bf4d1b5":"# read in the training data\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n# read in the testing data\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3e7c5ea6":"# before I start, I know there this is a typo in the Exterior2nd column\n# Cement Board is labeled as CmentBd instead of CemntBd. Therefore we must\n# update it.\ndf_train = df_train.replace({\"Exterior2nd\":{\"CmentBd\":\"CemntBd\"}})\ndf_test = df_test.replace({\"Exterior2nd\":{\"CmentBd\":\"CemntBd\"}})","75cf86a3":"combined = pd.concat([df_train, df_test], sort=False)\n\nexterior1_uniqVals = list(combined[\"Exterior1st\"].dropna().unique())\nexterior2_uniqVals = list(combined[\"Exterior2nd\"].dropna().unique())\nexterior_uniqVals = [x for x in exterior2_uniqVals if x not in exterior1_uniqVals]\nexterior_uniqVals = exterior_uniqVals +exterior1_uniqVals\n\nexterior_cols = []\nfor exterior_type in exterior_uniqVals:\n    new_exType_col = \"Exterior_\"+exterior_type\n    exterior_cols.append(new_exType_col)\n    df_train.loc[((df_train[\"Exterior1st\"]==exterior_type) |\n                  (df_train[\"Exterior2nd\"]==exterior_type)),new_exType_col] = 1\n    df_train.loc[((df_train[\"Exterior1st\"]!=exterior_type) &\n                  (df_train[\"Exterior2nd\"]!=exterior_type)),new_exType_col] = 0\n    df_test.loc[((df_test[\"Exterior1st\"]==exterior_type) |\n                  (df_test[\"Exterior2nd\"]==exterior_type)),new_exType_col] = 1\n    df_test.loc[((df_test[\"Exterior1st\"]!=exterior_type) &\n                  (df_test[\"Exterior2nd\"]!=exterior_type)),new_exType_col] = 0","608a9534":"exterior_df = df_train.loc[:,exterior_cols+[\"SalePrice\"]].copy()\nfor ex_col in exterior_cols:\n    exterior_df.loc[:,ex_col] = exterior_df[ex_col] * exterior_df[\"SalePrice\"]\nexterior_df = exterior_df.drop([\"SalePrice\"],axis=1)\nexterior_melted_df = pd.melt(exterior_df)\nexterior_melted_df = exterior_melted_df.loc[exterior_melted_df[\"value\"]>0,:]\nexterior_melted_df.columns=[\"Exterior\",\"SalePrice\"]\nexterior_melted_df.sort_values(\"Exterior\", inplace=True)\n\nplt.figure(figsize=(10,5))\nsns.boxplot(x=\"Exterior\", y=\"SalePrice\", data=exterior_melted_df)\nplt.xticks(rotation=90);\nplt.show()","f70a620d":"df_train.loc[df_train[\"Exterior2nd\"].notnull(),\"TwoExteriorMaterials\"]=1\ndf_train.loc[df_train[\"TwoExteriorMaterials\"].isnull(),\"TwoExteriorMaterials\"]=0\nplt.figure(figsize=(5,5))\nsns.boxplot(x=\"TwoExteriorMaterials\", y=\"SalePrice\", data=df_train)\nplt.xticks(rotation=90);\nplt.show()","f219c5bd":"df_train = df_train.drop([\"TwoExteriorMaterials\"], axis=1)","041c4689":"df_train = df_train.drop([\"Exterior1st\",\"Exterior2nd\"],axis=1)\ndf_test = df_test.drop([\"Exterior1st\",\"Exterior2nd\"],axis=1)","98bed071":"condition1_uniqVals = list(combined[\"Condition1\"].dropna().unique())\ncondition2_uniqVals = list(combined[\"Condition2\"].dropna().unique())\ncondition_uniqVals = [x for x in condition2_uniqVals if x not in condition1_uniqVals]\ncondition_uniqVals = condition_uniqVals + condition1_uniqVals\n\ncondition_cols=[]\nfor condition_type in condition_uniqVals:\n    print(condition_type)\n    new_condType_col = \"Condition_\"+condition_type\n    condition_cols.append(new_condType_col)\n    df_train.loc[((df_train[\"Condition1\"]==condition_type) |\n                  (df_train[\"Condition2\"]==condition_type)),new_condType_col] = 1\n    df_train.loc[df_train[new_condType_col].isnull(),new_condType_col] = 0\n    df_test.loc[((df_test[\"Condition1\"]==condition_type) |\n                  (df_test[\"Condition2\"]==condition_type)),new_condType_col] = 1\n    df_test.loc[df_test[new_condType_col].isnull(),new_condType_col] = 0","0dbe5169":"condition_df = df_train.loc[:,condition_cols+[\"SalePrice\"]].copy()\nfor cond_col in condition_cols:\n    condition_df.loc[:,cond_col] = condition_df[cond_col] * condition_df[\"SalePrice\"]\ncondition_df = condition_df.drop([\"SalePrice\"],axis=1)\ncondition_melted_df = pd.melt(condition_df)\ncondition_melted_df = condition_melted_df.loc[condition_melted_df[\"value\"]>0,:]\ncondition_melted_df.columns=[\"Condition\",\"SalePrice\"]\ncondition_melted_df.sort_values(\"Condition\", inplace=True)\n\nplt.figure(figsize=(10,5))\nsns.boxplot(x=\"Condition\", y=\"SalePrice\", data=condition_melted_df)\nplt.xticks(rotation=90);\nplt.show()","88f59fe2":"df_train = df_train.drop([\"Condition1\",\"Condition2\"],axis=1)\ndf_test = df_test.drop([\"Condition1\",\"Condition2\"],axis=1)","28e9b272":"mssubclass_vals = {\"MSSubClass\":     {20:\"1-STORY 1946 & NEWER ALL STYLES\",\n        30:\"1-STORY 1945 & OLDER\",\n        40:\"1-STORY W\/FINISHED ATTIC ALL AGES\",\n        45:\"1-1\/2 STORY - UNFINISHED ALL AGES\",\n        50:\"1-1\/2 STORY FINISHED ALL AGES\",\n        60:\"2-STORY 1946 & NEWER\",\n        70:\"2-STORY 1945 & OLDER\",\n        75:\"2-1\/2 STORY ALL AGES\",\n        80:\"SPLIT OR MULTI-LEVEL\",\n        85:\"SPLIT FOYER\",\n        90:\"DUPLEX - ALL STYLES AND AGES\",\n       120:\"1-STORY PUD (Planned Unit Development) - 1946 & NEWER\",\n       150:\"1-1\/2 STORY PUD - ALL AGES\",\n       160:\"2-STORY PUD - 1946 & NEWER\",\n       180:\"PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\",\n       190:\"2 FAMILY CONVERSION - ALL STYLES AND AGES\",\n}}\ncombined = combined.replace(mssubclass_vals)\nmssubclass_counts = combined[\"MSSubClass\"].value_counts()","ca11be66":"#plt.figure(figsize=(5,20))\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=mssubclass_counts.index, \n                 y=mssubclass_counts)\n#x=0\n#for index, value in mssubclass_counts.iteritems():\n#    #print(index+\":\"+str(value))\n#    ax.text(0, x, va, color='black',\n#           va='center')\n#    x+=1\n#ax.set_xlim(0,1)\nplt.xticks(rotation=90);","b5376882":"#box plot overallqual\/saleprice\ndf_train_transMSSubClass = df_train.replace(mssubclass_vals).copy()\nvar = 'MSSubClass'\ndata = pd.concat([df_train_transMSSubClass['SalePrice'], df_train_transMSSubClass[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90);","41a5176a":"\ndf_train_transMSSubClass[\"MSSubClass\"].unique()","2891f670":"df_train.loc[((df_train[\"MSSubClass\"]==20) | \n             (df_train[\"MSSubClass\"]==60) |\n             (df_train[\"MSSubClass\"]==120) |\n             (df_train[\"MSSubClass\"]==160)),\"MSSubClass_1946nNewer\"] = 1\n\ndf_train.loc[df_train[\"MSSubClass_1946nNewer\"].isnull()==True,\"MSSubClass_1946nNewer\"]=0\ndf_train_transMSSubClass[\"MSSubClass_1946nNewer\"]=df_train[\"MSSubClass_1946nNewer\"]","c4ae8602":"var = 'MSSubClass_1946nNewer'\ndata = pd.concat([df_train_transMSSubClass['SalePrice'], df_train_transMSSubClass[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90);","dadf4181":"df_test.loc[((df_test[\"MSSubClass\"]==20) | \n             (df_test[\"MSSubClass\"]==60) |\n             (df_test[\"MSSubClass\"]==120) |\n             (df_test[\"MSSubClass\"]==160)),\"MSSubClass_1946nNewer\"] = 1\n\ndf_test.loc[df_test[\"MSSubClass_1946nNewer\"].isnull()==True,\"MSSubClass_1946nNewer\"]=0","852768b9":"df_train.loc[((df_train[\"MSSubClass\"]==120) | \n             (df_train[\"MSSubClass\"]==150) |\n             (df_train[\"MSSubClass\"]==160) |\n             (df_train[\"MSSubClass\"]==180)),\"MSSubClass_PUD\"] = 1\n\ndf_train.loc[df_train[\"MSSubClass_PUD\"].isnull()==True,\"MSSubClass_PUD\"]=0\ndf_train_transMSSubClass[\"MSSubClass_PUD\"]=df_train[\"MSSubClass_PUD\"]\ndf_train[\"MSSubClass_PUD\"].value_counts()\n","4841e669":"var = 'MSSubClass_PUD'\ndata = pd.concat([df_train_transMSSubClass['SalePrice'], df_train_transMSSubClass[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90);","f7d8843e":"df_train = df_train.drop(\"MSSubClass_PUD\",axis=1)","39e36db6":"df_train.loc[((df_train[\"MSSubClass\"]==90) | \n             (df_train[\"MSSubClass\"]==190)),\"Stories\"] = 0.5\n\ndf_train.loc[((df_train[\"MSSubClass\"]==20) | \n             (df_train[\"MSSubClass\"]==30) |\n             (df_train[\"MSSubClass\"]==40) |\n             (df_train[\"MSSubClass\"]==120)),\"Stories\"] = 1.0\n\ndf_train.loc[((df_train[\"MSSubClass\"]==50) | \n             (df_train[\"MSSubClass\"]==45) |\n             (df_train[\"MSSubClass\"]==150)),\"Stories\"] = 1.5\n\ndf_train.loc[((df_train[\"MSSubClass\"]==60) | \n             (df_train[\"MSSubClass\"]==70) |\n             (df_train[\"MSSubClass\"]==80) |\n             (df_train[\"MSSubClass\"]==85) |\n             (df_train[\"MSSubClass\"]==160) |\n             (df_train[\"MSSubClass\"]==180)),\"Stories\"] = 2.0\n\ndf_train.loc[df_train[\"MSSubClass\"]==75,\"Stories\"] = 2.5\ndf_train_transMSSubClass[\"Stories\"]=df_train[\"Stories\"]\ndf_train[\"Stories\"].value_counts()","45cb1476":"var = 'Stories'\ndata = pd.concat([df_train_transMSSubClass['SalePrice'], df_train_transMSSubClass[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90);","549fe0c7":"df_train = df_train.drop(\"Stories\",axis=1)","821b4925":"df_train.loc[((df_train[\"MSSubClass\"]==90) | \n             (df_train[\"MSSubClass\"]==190)),\"ShareFloor\"] = 1\n\ndf_train.loc[df_train[\"ShareFloor\"].isnull()==True,\"ShareFloor\"]=0\ndf_train_transMSSubClass[\"ShareFloor\"]=df_train[\"ShareFloor\"]","d6f4736c":"var = 'ShareFloor'\ndata = pd.concat([df_train_transMSSubClass['SalePrice'], df_train_transMSSubClass[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90);","66db6066":"df_test.loc[((df_test[\"MSSubClass\"]==90) | \n             (df_test[\"MSSubClass\"]==190)),\"ShareFloor\"] = 1\n\ndf_test.loc[df_test[\"ShareFloor\"].isnull()==True,\"ShareFloor\"]=0","3dc33135":"df_train = df_train.drop(\"MSSubClass\",axis=1)\ndf_test = df_test.drop(\"MSSubClass\",axis=1)","adb165ae":"combined.loc[combined[\"MasVnrArea\"]==0,\"MasVnrType\"].value_counts()","966dd221":"df_train.loc[(df_train[\"MasVnrArea\"]==0) & \\\n             (df_train[\"MasVnrType\"]!=\"None\"),\"MasVnrArea\"] = \\\n    combined.loc[(combined[\"MasVnrArea\"]>0),\"MasVnrArea\"].mean()\n\ndf_test.loc[(df_test[\"MasVnrArea\"]==0) & \\\n             (df_test[\"MasVnrType\"]!=\"None\"),\"MasVnrArea\"] = \\\n    combined.loc[(combined[\"MasVnrArea\"]>0),\"MasVnrArea\"].mean()","3c01fffc":"#missing training data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.loc[missing_data[\"Total\"]>0,:]","3a0a653e":"#missing testing data\ntotal = df_test.isnull().sum().sort_values(ascending=False)\npercent = (df_test.isnull().sum()\/df_test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.loc[missing_data[\"Total\"]>0,:]","57a51d8d":"LotFrontage_ser = df_train.loc[:,\"LotFrontage\"] + \\\n    df_test.loc[:,\"LotFrontage\"]\nnotNullLotFrontage_ser = LotFrontage_ser.dropna()\nnotNullLotFrontage_ser.describe()","d587d63d":"sns.distplot(notNullLotFrontage_ser, fit=stats.norm)\nfig = plt.figure()","04ab3c0e":"if DROP_ROWS_WITH_1_NULL_ITEM:\n    row2drop = df_train.loc[df_train[\"Electrical\"].isnull(),:].index\n    df_train = df_train.drop(row2drop)","8d8f35a8":"cat_candidates = df_train.dtypes[df_train.dtypes==\"object\"].index.values\ncat_candidates","f55e8e2d":"frequencies = []\nfor col in cat_candidates:\n    overall_freq = combined.loc[:, col].value_counts().max() \/ combined.shape[0]\n    frequencies.append([col, overall_freq])\n\nfrequencies = np.array(frequencies)\nfreq_df = pd.DataFrame(index=frequencies[:,0], data=frequencies[:,1], columns=[\"frequency\"])\nsorted_freq = freq_df.frequency.sort_values(ascending=False).astype(float)","089f80b2":"plt.figure(figsize=(5,20))\nsns.set(style=\"whitegrid\")\nax = sns.barplot(y=sorted_freq.index[0:30], \n                 x=sorted_freq[0:30].astype(np.float))\nx=0\nfor index, value in sorted_freq[0:30].iteritems():\n    #print(index+\":\"+str(value))\n    ax.text(0, x, round(float(value),4), color='black',\n           va='center')\n    x+=1\nax.set_xlim(0,1)\nplt.xticks(rotation=90);","b0a4e719":"lowInfoGainCols = list(sorted_freq[sorted_freq.astype(float) > KEEP_COLS_WITH_MAX_PERCENT_OF_SAME_FEATURE].index)\nprint(\"Low Info Gain Columns:\")\nprint(lowInfoGainCols)\n\nprint(\"\\nValue Amounts\\n\")\nfor col in lowInfoGainCols:\n    print(col)\n    print(combined[col].value_counts())\n    print(\"\\n\")","7c92e1ae":"df_train = df_train.drop(lowInfoGainCols , axis=1)\ndf_test = df_test.drop(lowInfoGainCols , axis=1)","c544031f":"df_train['Neighborhood'].value_counts()","e347371d":"var = 'Neighborhood'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90);","4b961e6f":"df_train['SalePrice'].describe()","17709168":"neighboorhoodBySalePrice_df = df_train.groupby('Neighborhood')['SalePrice'].median().sort_values(ascending=False)\nneighboorhoodBySalePrice_df","d37d78c8":"top_tier_n = list(neighboorhoodBySalePrice_df[neighboorhoodBySalePrice_df.values >= df_train['SalePrice'].quantile(.75)].index)\nmid_upper_tier_n = list(neighboorhoodBySalePrice_df[(neighboorhoodBySalePrice_df.values < df_train['SalePrice'].quantile(.75)) & \\\n                                              (neighboorhoodBySalePrice_df.values >= df_train['SalePrice'].quantile(.50))].index)\nmid_low_tier_n = list(neighboorhoodBySalePrice_df[(neighboorhoodBySalePrice_df.values < df_train['SalePrice'].quantile(.50)) & \\\n                                              (neighboorhoodBySalePrice_df.values >= df_train['SalePrice'].quantile(.25))].index)\nlowest_tier_n = list(neighboorhoodBySalePrice_df[neighboorhoodBySalePrice_df.values < df_train['SalePrice'].quantile(.25)].index)\nprint(top_tier_n)\nprint(mid_upper_tier_n)\nprint(mid_low_tier_n)\nprint(lowest_tier_n)\n\ndf_train.loc[df_train['Neighborhood'].isin(top_tier_n), \"neighborhoodTier\"]=3\ndf_train.loc[df_train['Neighborhood'].isin(mid_upper_tier_n), \"neighborhoodTier\"]=2\ndf_train.loc[df_train['Neighborhood'].isin(mid_low_tier_n), \"neighborhoodTier\"]=1\ndf_train.loc[df_train['Neighborhood'].isin(lowest_tier_n), \"neighborhoodTier\"]=0\ndf_test.loc[df_test['Neighborhood'].isin(top_tier_n), \"neighborhoodTier\"]=3\ndf_test.loc[df_test['Neighborhood'].isin(mid_upper_tier_n), \"neighborhoodTier\"]=2\ndf_test.loc[df_test['Neighborhood'].isin(mid_low_tier_n), \"neighborhoodTier\"]=1\ndf_test.loc[df_test['Neighborhood'].isin(lowest_tier_n), \"neighborhoodTier\"]=0","c5ac1dfe":"num_candidates = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',\n       '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'YearBuilt', \n        'YearRemodAdd', 'GarageYrBlt', 'YrSold']\nfor num_feat in num_candidates:\n    fig = plt.figure()\n    sns.distplot(df_train.loc[df_train[num_feat]>0,\n        num_feat].dropna().values, fit=stats.norm).set_title(\\\n        num_feat);\n","fcf4ae6d":"# print shape before removing duplicates\nprint(df_train.drop([\"Id\"], axis=1).shape)\n# remove the duplicates and print shape\nprint(df_train.drop([\"Id\"], axis=1).drop_duplicates().shape)","e67c2855":"# print shape before removing duplicates\nprint(df_test.drop([\"Id\"], axis=1).shape)\n# remove the duplicates and print shape\nprint(df_test.drop([\"Id\"], axis=1).drop_duplicates().shape)","9081b0cf":"# add one-hot encoded columns based on `col` in `df` to `df`\ndef one_hot_encode(df, col):\n    df[col] = pd.Categorical(df[col])\n    dfDummies = pd.get_dummies(df[col], prefix = col)\n    df = pd.concat([df, dfDummies], axis=1)\n    #df = df.drop([col],axis=1)\n    return(df)","4a45de7d":"cat_ord_vars_all=[ \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \\\n             \"LotConfig\", \"Neighborhood\", \"BldgType\", \\\n             \"RoofStyle\", \"RoofMatl\", \"MasVnrType\", \\\n             \"Foundation\", \"Electrical\", \"Heating\", \"GarageType\", \"MiscFeature\", \\\n             \"SaleType\", \"SaleCondition\", \"MoSold\"]\ncat_ord_vars = [x for x in cat_ord_vars_all if x not in lowInfoGainCols]\nfor cat_ord_col in cat_ord_vars:\n        df_train = one_hot_encode(df_train,cat_ord_col)\n        df_test = one_hot_encode(df_test,cat_ord_col)","7041026a":"cleanup_nums = {\"LotShape\":     {\"Reg\": 0, \"IR1\": 1, \"IR2\" : 2, \"IR3\" : 3},\n               \"LandSlope\":{\"Gtl\":0,\"Mod\":1,\"Sev\":2},\n               \"HouseStyle\":{\"1Story\":0.00, \"1.5Unf\":0.25, \"1.5Fin\":0.50,\n                             \"2Story\":1.00, \"2.5Unf\":1.25, \"2.5Fin\": 1.50,\n                            \"SFoyer\":1.50, \"SLvl\":2.0},\n               \"ExterQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n               \"ExterCond\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n               \"BsmtQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n               \"BsmtCond\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n               \"BsmtExposure\": {\"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4},\n               \"BsmtFinType1\": {\"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5,\n                               \"GLQ\":6},\n               \"BsmtFinType2\": {\"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5,\n                               \"GLQ\":6},\n               \"HeatingQC\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n               \"CentralAir\": {\"N\": 0, \"Y\": 1},\n               \"KitchenQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n                \"Functional\": {\"Sal\": 0, \"Sev\": 1, \"Maj2\": 2, \"Maj1\": 3, \"Mod\": 4,\n                              \"Min2\": 5, \"Min1\": 6, \"Typ\": 7},\n                \"FireplaceQu\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                 \"GarageFinish\": {\"Unf\": 1, \"RFn\": 2, \"Fin\": 3},\n                \"GarageQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                \"GarageCond\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                \"PavedDrive\": {\"N\": 0, \"P\": 1, \"Y\": 2},\n                \"PoolQC\": {\"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n                \"Fence\": {\"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4},\n               }","41bba29d":"df_train = df_train.replace(cleanup_nums)\ndf_test = df_test.replace(cleanup_nums)","dae4b857":"df_train.columns[df_train.isnull().any()]","f51836bd":"df_test.columns[df_test.isnull().any()]","4c9c5c35":"if \"LotFrontage\" not in lowInfoGainCols:\n    df_train.loc[df_train[\"LotFrontage\"].isnull()==True,\"HasLotFrontage\"]=0\n    df_train.loc[df_train[\"LotFrontage\"].isnull()==True,\"LotFrontage\"]=0\n    df_train.loc[df_train[\"LotFrontage\"]>0,\"HasLotFrontage\"]=1\n\n    df_test.loc[df_test[\"LotFrontage\"].isnull()==True,\"HasLotFrontage\"]=0\n    df_test.loc[df_test[\"LotFrontage\"].isnull()==True,\"LotFrontage\"]=0\n    df_test.loc[df_test[\"LotFrontage\"]>0,\"HasLotFrontage\"]=1","93693202":"if \"Fireplaces\" not in lowInfoGainCols:\n    df_train.loc[df_train[\"Fireplaces\"]==0,\"FireplaceQu\"]=0\n    df_test.loc[df_test[\"Fireplaces\"]==0,\"FireplaceQu\"]=0","367ba829":"if \"PoolQC\" not in lowInfoGainCols:\n    df_train.loc[df_train[\"PoolArea\"]==0,\"PoolQC\"]=0\n    df_test.loc[df_test[\"PoolArea\"]==0,\"PoolQC\"]=0","4726ad65":"if \"PoolQC\" not in lowInfoGainCols:\n    print(df_train[\"PoolQC\"].isnull().sum())\n    print(df_test[\"PoolQC\"].isnull().sum())","dfee4c8a":"if \"PoolQC\" not in lowInfoGainCols:\n    sd_of_pool_sizes = np.mean(df_train.loc[df_train[\"PoolArea\"] > 0 , \"PoolArea\"].append(\n        df_test.loc[df_test[\"PoolArea\"] > 0 , \"PoolArea\"]))\n    print (sd_of_pool_sizes)\n    for idx, row in df_test.loc[df_test[\"PoolQC\"].isnull(),:].iterrows():\n        minPoolArea = row[\"PoolArea\"] - sd_of_pool_sizes\n        if minPoolArea < 0:\n            minPoolArea = 0\n        maxPoolArea = row[\"PoolArea\"] + sd_of_pool_sizes\n        df_train_poolqc_df = df_train.loc[(df_train[\"PoolArea\"] > minPoolArea) & \\\n                                          (df_train[\"PoolArea\"] < maxPoolArea),\n                                          [\"PoolQC\"]]\n        df_test_poolqc_df = df_test.loc[(df_test[\"PoolArea\"] > minPoolArea) & \\\n                                          (df_test[\"PoolArea\"] < maxPoolArea),\n                                          [\"PoolQC\"]]\n        meanPoolQC = np.round(df_train_poolqc_df.append(df_test_poolqc_df).mean(skipna=True),0)\n        df_test.loc[idx,\"PoolQC\"] = meanPoolQC.values[0]\n        #.mean(skipna=True)\n","b604d33d":"if \"Functional\" not in lowInfoGainCols:\n    df_train.loc[df_train[\"Functional\"].isnull(),\"Functional\"]=7\n    df_test.loc[df_test[\"Functional\"].isnull(),\"Functional\"]=7","9bc59b5f":"if \"Fence\" not in lowInfoGainCols:\n    df_train.loc[df_train[\"Fence\"].isnull(),\"Fence\"]=0\n    df_test.loc[df_test[\"Fence\"].isnull(),\"Fence\"]=0","f674cc97":"if \"MasVnrType\" not in lowInfoGainCols:\n    df_train.loc[ \\\n        df_train.loc[:,[\"MasVnrArea\",\"MasVnrType\"]].isnull().all(axis=1), \\\n        \"HasMasVnr\"]=0\n    df_train.loc[ \\\n                 df_train.loc[:,[\"MasVnrArea\",\"MasVnrType\"]].isnull().all(axis=1),\n                 \"MasVnrArea\"]=0\n    df_train.loc[(df_train[\"MasVnrArea\"]>0) | (df_train[\"MasVnrType\"].isnull()==False),\"HasMasVnr\"]=1\n\n    df_test.loc[ \\\n        df_test.loc[:,[\"MasVnrArea\",\"MasVnrType\"]].isnull().all(axis=1), \\\n        \"HasMasVnr\"]=0\n    df_test.loc[ \\\n                 df_test.loc[:,[\"MasVnrArea\",\"MasVnrType\"]].isnull().all(axis=1),\n                 \"MasVnrArea\"]=0\n    df_test.loc[(df_test[\"MasVnrArea\"]>0) | (df_test[\"MasVnrType\"].isnull()==False),\"HasMasVnr\"]=1\nelse:\n    df_train.loc[ \\\n        df_train.loc[:,[\"MasVnrArea\"]].isnull().all(axis=1), \\\n        \"HasMasVnr\"]=0\n    df_train.loc[ \\\n                 df_train.loc[:,[\"MasVnrArea\"]].isnull().all(axis=1),\n                 \"MasVnrArea\"]=0\n    df_train.loc[(df_train[\"MasVnrArea\"]>0),\"HasMasVnr\"]=1\n\n    df_test.loc[ \\\n        df_test.loc[:,[\"MasVnrArea\"]].isnull().all(axis=1), \\\n        \"HasMasVnr\"]=0\n    df_test.loc[ \\\n                 df_test.loc[:,[\"MasVnrArea\"]].isnull().all(axis=1),\n                 \"MasVnrArea\"]=0\n    df_test.loc[(df_test[\"MasVnrArea\"]>0),\"HasMasVnr\"]=1","eb11ef0f":"garage_cat_features_all=[\"GarageType\",\"GarageYrBlt\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"GarageFinish\"]\ngarage_cat_features = [x for x in garage_cat_features_all if x not in lowInfoGainCols]\ngarage_features = garage_cat_features + [\"GarageArea\", \"GarageCars\"]\ndf_train.loc[ \\\n    (df_train.loc[:,garage_cat_features].isnull().all(axis=1)) & \\\n             ((df_train[\"GarageArea\"] == 0) |  (df_train[\"GarageArea\"].isnull() == True)) & \\\n             ((df_train[\"GarageCars\"] == 0) |  (df_train[\"GarageCars\"].isnull() == True)), \\\n    \"HasGarage\"]=0\ndf_train.loc[ \\\n    (df_train.loc[:,garage_cat_features].notnull().any(axis=1)) | \\\n             (df_train[\"GarageArea\"] > 0) | \\\n             (df_train[\"GarageCars\"] > 0), \\\n    \"HasGarage\"]=1\n\ndf_test.loc[ \\\n    (df_test.loc[:,garage_cat_features].isnull().all(axis=1)) & \\\n             ((df_test[\"GarageArea\"] == 0) |  (df_test[\"GarageArea\"].isnull() == True)) & \\\n             ((df_test[\"GarageCars\"] == 0) |  (df_test[\"GarageCars\"].isnull() == True)), \\\n    \"HasGarage\"]=0\ndf_test.loc[ \\\n    (df_test.loc[:,garage_cat_features].notnull().any(axis=1)) | \\\n             (df_test[\"GarageArea\"] > 0) | \\\n             (df_test[\"GarageCars\"] > 0), \\\n    \"HasGarage\"]=1","deacdab4":"for gar_feat in garage_features:\n    if gar_feat not in cat_ord_vars:\n        df_train.loc[df_train[\"HasGarage\"]==0,gar_feat]=0\n        df_test.loc[df_test[\"HasGarage\"]==0,gar_feat]=0","c96a70cb":"df_train.loc[df_train[\"GarageYrBlt\"].isnull(),\"GarageYrBlt\"] = df_train[\"GarageYrBlt\"].mode(dropna=True)\ndf_test.loc[df_test[\"GarageYrBlt\"].isnull(),\"GarageYrBlt\"] = df_test[\"GarageYrBlt\"].mode(dropna=True)","e448a21c":"low_cols=[]\nfor gcol in garage_features:\n    print(\"Number of %s Nulls in Training + Testing Respectively:\" % gcol)\n    print(df_train[gcol].isnull().sum())\n    print(df_test[gcol].isnull().sum())\n    if df_train[gcol].isnull().sum() + df_test[gcol].isnull().sum() < 3:\n        low_cols.append(gcol)\nprint(\"rows with low amount of null values\")\nprint(low_cols)","0f86cc71":"garage_corrmat = df_train.loc[:,garage_features+[\"SalePrice\"]].corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(garage_corrmat, square=True);","7c248fde":"df_test.loc[df_test.loc[:,low_cols].isnull().any(axis=1), \\\n            low_cols]","893e8cd8":"for grow in low_cols:\n    mean_val = df_train[grow].append(df_test[grow]).mean(skipna=True)\n    if (df_train[grow].dtypes == \"int64\"):\n        mean_val = int(np.round(mean_val,0))\n    df_test.loc[df_test[grow].isnull()==True,grow]=mean_val\n","b20d2e67":"df_test.iloc[[666,1116],:].loc[:,low_cols]","4389b4af":"basement_cat_features_all=[\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\"]\nbasement_cat_features = [x for x in basement_cat_features_all if x not in lowInfoGainCols]\nbasement_features= basement_cat_features + \\\n    [\"HasBasement\",\"BsmtFinSF1\",\"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\"]\n\ndf_train.loc[ \\\n    (df_train.loc[:,basement_cat_features].isnull().all(axis=1)) & \\\n             ((df_train[\"BsmtFinSF1\"] == 0) | (df_train[\"BsmtFinSF1\"].isnull() == True)) & \\\n             ((df_train[\"BsmtFinSF2\"] == 0) | (df_train[\"BsmtFinSF2\"].isnull() == True)) & \\\n             ((df_train[\"BsmtUnfSF\"] == 0) | (df_train[\"BsmtUnfSF\"].isnull() == True)) & \\\n             ((df_train[\"TotalBsmtSF\"] == 0) | (df_train[\"TotalBsmtSF\"].isnull() == True)) & \\\n             ((df_train[\"BsmtFullBath\"] == 0) | (df_train[\"BsmtFullBath\"].isnull() == True)) & \\\n             ((df_train[\"BsmtHalfBath\"] == 0) | (df_train[\"BsmtHalfBath\"].isnull() == True)), \\\n    \"HasBasement\"]=0\n#df_train.loc[ \\\n#    (df_train.loc[:,basement_features].isnull().all(axis=1)), \\\n#    \"HasBasement\"]=0\ndf_train.loc[ \\\n    (df_train.loc[:,basement_cat_features].notnull().any(axis=1)) | \\\n             (df_train[\"BsmtFinSF1\"] > 0) | \\\n             (df_train[\"BsmtFinSF2\"] > 0) | \\\n             (df_train[\"BsmtUnfSF\"] > 0) | \\\n             (df_train[\"TotalBsmtSF\"] > 0) | \\\n             (df_train[\"BsmtFullBath\"] > 0) | \\\n             (df_train[\"BsmtHalfBath\"] > 0), \\\n    \"HasBasement\"]=1\n\ndf_test.loc[ \\\n    (df_test.loc[:,basement_cat_features].isnull().all(axis=1)) & \\\n             ((df_test[\"BsmtFinSF1\"] == 0) | (df_test[\"BsmtFinSF1\"].isnull() == True)) & \\\n             ((df_test[\"BsmtFinSF2\"] == 0) | (df_test[\"BsmtFinSF2\"].isnull() == True)) & \\\n             ((df_test[\"BsmtUnfSF\"] == 0) | (df_test[\"BsmtUnfSF\"].isnull() == True)) & \\\n             ((df_test[\"TotalBsmtSF\"] == 0) | (df_test[\"TotalBsmtSF\"].isnull() == True)) & \\\n             ((df_test[\"BsmtFullBath\"] == 0) | (df_test[\"BsmtFullBath\"].isnull() == True)) & \\\n             ((df_test[\"BsmtHalfBath\"] == 0) | (df_test[\"BsmtHalfBath\"].isnull() == True)), \\\n    \"HasBasement\"]=0\n#df_test.loc[ \\\n#    (df_test.loc[:,basement_features].isnull().all(axis=1)), \\\n#    \"HasBasement\"]=0\ndf_test.loc[ \\\n    (df_test.loc[:,basement_cat_features].notnull().any(axis=1)) | \\\n             (df_test[\"BsmtFinSF1\"] > 0) | \\\n             (df_test[\"BsmtFinSF2\"] > 0) | \\\n             (df_test[\"BsmtUnfSF\"] > 0) | \\\n             (df_test[\"TotalBsmtSF\"] > 0) | \\\n             (df_test[\"BsmtFullBath\"] > 0) | \\\n             (df_test[\"BsmtHalfBath\"] > 0), \\\n    \"HasBasement\"]=1","a33a86b7":"for basem_feat in basement_features:\n    if basem_feat not in cat_ord_vars:\n        df_train.loc[df_train[\"HasBasement\"]==0,basem_feat]=0\n        df_test.loc[df_test[\"HasBasement\"]==0,basem_feat]=0","9d1f063f":"for bcol in basement_features:\n    print(\"Number of %s Nulls in Training + Testing Respectively:\" % bcol)\n    print(df_train[bcol].isnull().sum())\n    print(df_test[bcol].isnull().sum())","96248235":"basement_corrmat = df_train[basement_features].corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(basement_corrmat, square=True);","42506422":"if \"BsmtQual\" not in lowInfoGainCols and \"BsmtCond\" not in lowInfoGainCols:\n    print(df_train.loc[:,[\"BsmtQual\",\"BsmtCond\"]].isnull().all(axis=1).shape)","2f5478b4":"if \"BsmtQual\" not in lowInfoGainCols and \"BsmtCond\" not in lowInfoGainCols:\n    df_train.loc[df_train[\"BsmtQual\"].isnull(),\"BsmtQual\"]=df_train.loc[df_train[\"BsmtQual\"].isnull(),\"BsmtCond\"]\n    df_test.loc[df_test[\"BsmtQual\"].isnull(),\"BsmtQual\"]=df_test.loc[df_test[\"BsmtQual\"].isnull(),\"BsmtCond\"]\n    df_train.loc[df_train[\"BsmtCond\"].isnull(),\"BsmtCond\"]=df_train.loc[df_train[\"BsmtCond\"].isnull(),\"BsmtQual\"]\n    df_test.loc[df_test[\"BsmtCond\"].isnull(),\"BsmtCond\"]=df_test.loc[df_test[\"BsmtCond\"].isnull(),\"BsmtQual\"]\nelif \"BsmtQual\" not in lowInfoGainCols and \"BsmtCond\" in lowInfoGainCols:\n    combined_BsmtQual = df_train.loc[df_train[\"BsmtQual\"] > 0, \"BsmtQual\"].append(\n        df_test.loc[df_test[\"BsmtQual\"] > 0, \"BsmtQual\"])\n    df_train.loc[df_train[\"BsmtQual\"].isnull(),\"BsmtQual\"]=np.argmax(combined_BsmtQual.value_counts())\n    df_test.loc[df_test[\"BsmtQual\"].isnull(),\"BsmtQual\"]=np.argmax(combined_BsmtQual.value_counts())","daccb8b2":"if \"BsmtExposure\" not in lowInfoGainCols:\n    avg_basement_exposure_given_basement = np.round(\n        df_train.loc[df_train[\"HasBasement\"]==1,\"BsmtExposure\"].append(\n           df_test.loc[df_test[\"HasBasement\"]==1,\"BsmtExposure\"]).mean(skipna=True),0)\n    print(avg_basement_exposure_given_basement)\n    df_train.loc[df_train[\"BsmtExposure\"].isnull(),\"BsmtExposure\"] = avg_basement_exposure_given_basement\n    df_test.loc[df_test[\"BsmtExposure\"].isnull(),\"BsmtExposure\"] = avg_basement_exposure_given_basement","71fb48a9":"if \"BsmtFinType2\" not in lowInfoGainCols:\n    print(df_train.loc[df_train[\"BsmtFinType2\"].isnull(),\"BsmtFinSF2\"])","8e49f95b":"if \"BsmtFinType2\" not in lowInfoGainCols:\n    BsmtFinSF2_train_test_df = \\\n        df_train.loc[df_train[\"BsmtFinSF2\"]>0,[\"BsmtFinSF2\",\"BsmtFinType2\"]].append( \\\n        df_test.loc[df_test[\"BsmtFinSF2\"]>0,[\"BsmtFinSF2\",\"BsmtFinType2\"]])\n    np.std(BsmtFinSF2_train_test_df[\"BsmtFinSF2\"])","22fd7598":"if \"BsmtFinType2\" not in lowInfoGainCols:\n    min_BsmtFinSF2 = df_train.loc[df_train[\"BsmtFinType2\"].isnull(),\"BsmtFinSF2\"] - \\\n        np.std(BsmtFinSF2_train_test_df[\"BsmtFinSF2\"])\n    max_BsmtFinSF2 = df_train.loc[df_train[\"BsmtFinType2\"].isnull(),\"BsmtFinSF2\"] + \\\n        np.std(BsmtFinSF2_train_test_df[\"BsmtFinSF2\"])\n    subset_BsmtFinSF2_train_test_df = BsmtFinSF2_train_test_df.loc[ \\\n        (BsmtFinSF2_train_test_df[\"BsmtFinSF2\"] > min_BsmtFinSF2.values[0]) & \\\n        (BsmtFinSF2_train_test_df[\"BsmtFinSF2\"] < max_BsmtFinSF2.values[0]),\"BsmtFinType2\"]\n\n    df_train.loc[df_train[\"BsmtFinType2\"].isnull(),\"BsmtFinType2\"] = \\\n        np.round(subset_BsmtFinSF2_train_test_df.mean(skipna=True),0)","d54cd78a":"if \"KitchenQual\" not in lowInfoGainCols:\n    print(df_train[\"KitchenQual\"].isnull().sum())\n    print(df_test[\"KitchenQual\"].isnull().sum())","e9d18cc1":"if \"KitchenQual\" not in lowInfoGainCols:\n    df_train.loc[df_train[\"KitchenQual\"].isnull(),\"KitchenQual\"] = np.round(df_train[\"KitchenQual\"].append(df_test[\"KitchenQual\"]).mean(skipna=True),0)\n    df_test.loc[df_test[\"KitchenQual\"].isnull(),\"KitchenQual\"] = np.round(df_train[\"KitchenQual\"].append(df_test[\"KitchenQual\"]).mean(skipna=True),0)","0a84b845":"train_cols = list(df_train.columns.sort_values().unique())\ntest_cols = list(df_test.columns.sort_values().unique())\nuniq_train_cols = [x for x in train_cols if (x not in test_cols and x != \"SalePrice\")]\nuniq_test_cols = [x for x in test_cols if x not in train_cols]\n\nprint(\"length of unique training columns: \"+ str(len(uniq_train_cols)))\nprint(\"length of unique test columns: \"+ str(len(uniq_test_cols)))\ndf_train = df_train.drop(uniq_train_cols,axis=1)\ndf_test = df_test.drop(uniq_test_cols,axis=1)","67c20cc8":"# Overall quality of the house\ndf_train[\"OverallGrade\"] = df_train[\"OverallQual\"] * df_train[\"OverallCond\"]\ndf_test[\"OverallGrade\"] = df_test[\"OverallQual\"] * df_test[\"OverallCond\"]\n\n#scatter plot OverallGrade\/saleprice\nvar = 'OverallGrade'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","c579c8ca":"# Overall quality of the garage\ndf_train[\"GarageGrade\"] = df_train[\"GarageQual\"] * df_train[\"GarageCond\"]\ndf_test[\"GarageGrade\"] = df_test[\"GarageQual\"] * df_test[\"GarageCond\"]\n\n#scatter plot GarageGrade\/saleprice\nvar = 'GarageGrade'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","210144a9":"# Overall quality of the exterior\ndf_train[\"ExterGrade\"] = df_train[\"ExterQual\"] * df_train[\"ExterCond\"]\ndf_test[\"ExterGrade\"] = df_train[\"ExterQual\"] * df_train[\"ExterCond\"]\n\n#scatter plot ExterGrade\/saleprice\nvar = 'ExterGrade'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","8a3ecd5c":"# Overall kitchen score\ndf_train[\"KitchenScore\"] = df_train[\"KitchenAbvGr\"] * df_train[\"KitchenQual\"]\ndf_test[\"KitchenScore\"] = df_train[\"KitchenAbvGr\"] * df_train[\"KitchenQual\"]\n\n#scatter plot KitchenScore\/saleprice\nvar = 'KitchenScore'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","60d74380":"# Overall fireplace score\ndf_train[\"FireplaceScore\"] = df_train[\"Fireplaces\"] * df_train[\"FireplaceQu\"]\ndf_test[\"FireplaceScore\"] = df_train[\"Fireplaces\"] * df_train[\"FireplaceQu\"]\n\n\n#scatter plot FireplaceScore\/saleprice\nvar = 'FireplaceScore'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","f3aceb5e":"# Overall garage score\ndf_train[\"GarageScore\"] = df_train[\"GarageArea\"] * df_train[\"GarageQual\"]\ndf_test[\"GarageScore\"] = df_train[\"GarageArea\"] * df_train[\"GarageQual\"]\n\n#scatter plot GarageScore\/saleprice\nvar = 'GarageScore'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","0c7609c1":"# Total number of bathrooms\ndf_train[\"TotalBath\"] = df_train[\"BsmtFullBath\"] + (0.5 * df_train[\"BsmtHalfBath\"]) + df_train[\"FullBath\"] + (0.5 * df_train[\"HalfBath\"])\ndf_test[\"TotalBath\"] = df_train[\"BsmtFullBath\"] + (0.5 * df_train[\"BsmtHalfBath\"]) + df_train[\"FullBath\"] + (0.5 * df_train[\"HalfBath\"])\n#scatter plot TotalBath\/saleprice\nvar = 'TotalBath'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","26782d50":"# Total SF for house (incl. basement)\ndf_train[\"AllSF\"] = df_train[\"GrLivArea\"] + df_train[\"TotalBsmtSF\"]\ndf_test[\"AllSF\"] = df_train[\"GrLivArea\"] + df_train[\"TotalBsmtSF\"]\n#scatter plot GarageScore\/saleprice\nvar = 'GarageScore'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","2367942b":"# Total SF for 1st + 2nd floors\ndf_train[\"AllFlrsSF\"] = df_train[\"1stFlrSF\"] + df_train[\"2ndFlrSF\"]\ndf_test[\"AllFlrsSF\"] = df_train[\"1stFlrSF\"] + df_train[\"2ndFlrSF\"]\n#scatter plot GarageScore\/saleprice\nvar = 'GarageScore'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","b18a33c5":"# Total SF for porch\ndf_train[\"AllPorchSF\"] = df_train[\"OpenPorchSF\"] + df_train[\"EnclosedPorch\"] + df_train[\"3SsnPorch\"] + df_train[\"ScreenPorch\"]\ndf_test[\"AllPorchSF\"] = df_train[\"OpenPorchSF\"] + df_train[\"EnclosedPorch\"] + df_train[\"3SsnPorch\"] + df_train[\"ScreenPorch\"]\n#scatter plot GarageScore\/saleprice\nvar = 'GarageScore'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","077cec24":"# House completed before sale or not\ndf_train[\"BoughtOffPlan\"] = df_train.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\ndf_test[\"BoughtOffPlan\"] = df_train.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n\nvar = 'BoughtOffPlan'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","c6f39631":"# drop categorical ordinal columns that are not encoded\ndf_train_numerical = df_train.drop(cat_ord_vars, axis=1)\n# drop categorical ordinal columns that are not encoded\ndf_test_numerical = df_test.drop(cat_ord_vars, axis=1)","1d95f97c":"df_train_numerical[\"SalePrice\"].describe()","b1464ebf":"sns.distplot(df_train_numerical['SalePrice'], fit=stats.norm)\nfig = plt.figure()\nres = stats.probplot(df_train_numerical['SalePrice'], plot=plt)","125a9c39":"print(\"Skewness: %f\" % df_train_numerical['SalePrice'].skew())","57705a65":"print(\"Kurtosis: %f\" % df_train_numerical['SalePrice'].kurt())","eb349fdb":"#transformed histogram and normal probability plot\nsns.distplot(np.log(df_train_numerical['SalePrice']), fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(df_train_numerical['SalePrice'], plot=plt)\n\nprint(\"Skewness: %f\" % np.log(df_train_numerical['SalePrice']).skew())\nprint(\"Kurtosis: %f\" % np.log(df_train_numerical['SalePrice']).kurt())","82edaaa1":"#applying log transformation\n#df_train_numerical['logSalePrice'] = np.log(df_train_numerical['SalePrice'])","9caadc29":"#dropping log transformation\n#df_train_numerical = df_train_numerical.drop('logSalePrice', axis=1)","6835a104":"combined = pd.concat([df_train_numerical, df_test_numerical])\none_hot_cols_lofl = []\nfor car_ord_predix in cat_ord_vars + [\"Exterior\", \"Condition\"]:\n    one_hot_cols_lofl.append(\n        [x for x in list(combined.columns) if car_ord_predix in x])\none_hot_cols = [item for sublist in one_hot_cols_lofl for item in sublist]","9906c121":"# set minimum threshold of percent of 0's in the column we consider as low info gain\nMIN_PER_INFO_GAIN=.80\n\nfrequencies = []\n\nfor col in one_hot_cols:\n    overall_freq = combined.loc[:, col].value_counts()[0] \/ combined.shape[0]\n    frequencies.append([col, overall_freq])\n\nfrequencies = np.array(frequencies)\nfreq_df = pd.DataFrame(index=frequencies[:,0], data=frequencies[:,1], columns=[\"frequency\"])\nsorted_freq = freq_df.frequency.sort_values(ascending=False)\n\nlowInfoGainCols2 = list(sorted_freq[sorted_freq.astype(float) > KEEP_COLS_WITH_MAX_PERCENT_OF_SAME_FEATURE].index)\n\ntotal_catNom = len(sorted_freq)\ntotal_catNom_OverPerSameValue = len(lowInfoGainCols2)\n\nprint(\"total number of categorical-norminal columns:\")\nprint(total_catNom)\n\nprint(\"number of categorical-nominal columns with over %.0f%% of 0:\" % (round(KEEP_COLS_WITH_MAX_PERCENT_OF_SAME_FEATURE*100,0)))\nprint(total_catNom_OverPerSameValue)\n\nprint(\"percent of categorical-nominal columns with over %.0f%% of the same value over total:\" % (round(KEEP_COLS_WITH_MAX_PERCENT_OF_SAME_FEATURE*100,0)))\nprint(round(float(total_catNom_OverPerSameValue)\/float(total_catNom),2))\n\n#plt.figure(figsize=(5,20))\n#sns.set(style=\"whitegrid\")\n#ax = sns.barplot(y=sorted_freq.index[0:30], \n#                 x=sorted_freq[0:30].astype(np.float))\n#x=0\n#for index, value in sorted_freq[0:30].iteritems():\n#    #print(index+\":\"+str(value))\n#    ax.text(0, x, round(float(value),4), color='black',\n#           va='center')\n#    x+=1\n#ax.set_xlim(0,1)\n#plt.xticks(rotation=90);","33e6c016":"print(len(list(df_train_numerical.columns)))\ndf_train_numerical = df_train_numerical.drop(lowInfoGainCols2, axis=1)\ndf_test_numerical = df_test_numerical.drop(lowInfoGainCols2, axis=1)\ndf_train = df_train.drop(lowInfoGainCols2, axis=1)\ndf_test = df_test.drop(lowInfoGainCols2, axis=1)\n\nprint(len(list(df_train_numerical.columns)))","6f4ff6fd":"one_hot_cols = [x for x in one_hot_cols if x in list(df_train_numerical.columns)]","d08862e2":"# plot all the one_hot columns:\nif 1==0:\n    for var in one_hot_cols:\n        data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n        f, ax = plt.subplots(figsize=(8, 6))\n        fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n        fig.axis(ymin=0, ymax=800000)\n        plt.xticks(rotation=90);","419a2640":"# plot only the one hot cols where the mean SalePrice of the group is\n# greater than 1 SD above the mean SalePrice of not containing that\n# group\nbest_one_hot_cols=[]\nfor var in one_hot_cols:\n    val0_mean = df_train.groupby(var)['SalePrice'].mean()[0]\n    val0_std = df_train.groupby(var)['SalePrice'].std()[0]\n    val1_mean = df_train.groupby(var)['SalePrice'].mean()[1]\n    if val1_mean < (val0_mean-val0_std) or val1_mean >(val0_mean+val0_std):\n        best_one_hot_cols.append(var)\n        data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n        f, ax = plt.subplots(figsize=(8, 6))\n        fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n        fig.axis(ymin=0, ymax=800000)\n        plt.xticks(rotation=90);","ee18c365":"list(df_train_numerical.columns)","43ecc2db":"# correlation matrix\ncorrmat = df_train_numerical.corr()\ncorrSalePrice = corrmat.loc[abs(corrmat[\"SalePrice\"]) > .50, \"SalePrice\"]\ncorrSalePrice = corrSalePrice.sort_values(ascending=False)\n\n# plot correlation\nplt.figure(figsize=(5,20))\nsns.set(style=\"whitegrid\")\nax = sns.barplot(y=corrSalePrice.index, \n                 x=corrSalePrice.astype(np.float))\nx=0\nfor index, value in corrSalePrice.iteritems():\n    #print(index+\":\"+str(value))\n    ax.text(0, x, round(float(value),4), color='black',\n           va='center')\n    x+=1\nax.set_xlim(0,1)\nplt.xticks(rotation=90);","05ec2089":"len(corrSalePrice)","26cb9b62":"# how much are they correlated to each other?\ncorrSalePrice = corrmat.loc[abs(corrmat[\"SalePrice\"]) > 0.5, \"SalePrice\"]\ncorrSalePrice = corrSalePrice.sort_values(ascending=False)\ncorr2SalePrice = list(corrSalePrice.index)\n\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat.loc[corr2SalePrice,corr2SalePrice], square=True);","c36962cc":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","28812158":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","2b8e452f":"df_train = df_train.drop(list(df_train.loc[df_train['GrLivArea']>4000,:].index))","df6a99c8":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","faeceef9":"#box plot overallqual\/saleprice\nvar = 'ExterQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","7a48b0de":"#box plot overallqual\/saleprice\nvar = 'KitchenQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","6812e5e2":"#box plot overallqual\/saleprice\nvar = 'GarageCars'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","07ff37a2":"garageCarsReplaceDict = {\"GarageCars\":{4:3}}\n\ndf_train = df_train.replace(garageCarsReplaceDict)\ndf_test = df_test.replace(garageCarsReplaceDict)","47eacb15":"#box plot overallqual\/saleprice\nvar = 'GarageCars'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","cef1b386":"#scatter plot grlivarea\/saleprice\nvar = 'GarageArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","dca5d788":"#scatter plot grlivarea\/saleprice\nvar = 'GarageArea'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","02ab4c7b":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","0497bff7":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], np.log(df_train[var])], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","c9cdb613":"#box plot overallqual\/saleprice\nvar = 'FullBath'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","66d5316f":"FullBathReplaceDict = {\"FullBath\":{0:1}}\n\ndf_train = df_train.replace(FullBathReplaceDict)\ndf_test = df_test.replace(FullBathReplaceDict)","95b5de38":"#box plot overallqual\/saleprice\nvar = 'FullBath'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","8ad4b931":"#box plot overallqual\/saleprice\nvar = 'GarageFinish'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","50529138":"df_train['TotRmsAbvGrd'].describe()","81b23bc6":"#box plot overallqual\/saleprice\nvar = 'TotRmsAbvGrd'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","a8523371":"#scatter plot grlivarea\/saleprice\nvar = 'TotRmsAbvGrd'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","c9b9ac9c":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","196c76ac":"var = 'FireplaceQu'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nplt.xticks(rotation=90);","11c51715":"var = 'YearRemodAdd'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","cee89445":"fav_variables=['OverallQual','YearRemodAdd','FireplaceQu', \\\n               'GarageFinish', 'FullBath', 'GarageCars', \\\n               'KitchenQual', 'ExterQual', 'ExterQual',\n              'GrLivArea'] + \\\n                best_one_hot_cols","04d28893":"from sklearn.linear_model import LinearRegression\nif 1==1:\n    X_train = df_train_numerical.loc[:,fav_variables].copy()\n    y_train = df_train_numerical.loc[:,\"SalePrice\"].copy()\n    X_pred = df_test_numerical.loc[:,fav_variables].copy()\n    reg = LinearRegression().fit(X_train, y_train)\n    pred = reg.predict(X_pred)\n    #pred=np.power(10,pred)\n    df_test.loc[:,\"SalePrice\"]=pd.Series(pred)\n    sub_df = df_test.loc[:,[\"Id\",\"SalePrice\"]]\n    sub_df.to_csv(\"submission.csv\", index=False)","367a93e6":"if 1==0:\n    X_train_n2 = df_train_numerical.loc[df_train_numerical['neighborhoodTier']==2,fav_variables].copy()\n    y_train_n2 = df_train_numerical.loc[df_train_numerical['neighborhoodTier']==2,\"SalePrice\"].copy()\n    X_sub_n2 = df_test_numerical.loc[df_test_numerical['neighborhoodTier']==2,[\"Id\",\"neighborhoodTier\"]+fav_variables]\n    X_pred_n2 = df_test_numerical.loc[df_test_numerical['neighborhoodTier']==2,fav_variables].copy()\n    print(\"Top Tier\")\n    print(X_train_n2.shape)\n    print(X_sub_n2.shape)\n    reg_n2 = LinearRegression().fit(X_train_n2, y_train_n2)\n    pred_n2 = reg_n2.predict(X_pred_n2)\n    print(len(pred_n2))\n    X_sub_n2.loc[:,\"SalePrice\"]=pred_n2\n    #X_sub_n2 = X_sub_n2.loc[:,[\"Id\",\"SalePrice\"]]\n    \n\n    X_train_n1 = df_train_numerical.loc[df_train_numerical['neighborhoodTier']==1,fav_variables].copy()\n    y_train_n1 = df_train_numerical.loc[df_train_numerical['neighborhoodTier']==1,\"SalePrice\"].copy()\n    X_sub_n1 = df_test_numerical.loc[df_test_numerical['neighborhoodTier']==1,[\"Id\",\"neighborhoodTier\"]+fav_variables]\n    X_pred_n1 = df_test_numerical.loc[df_test_numerical['neighborhoodTier']==1,fav_variables].copy()\n    print(\"Mid Tier\")\n    print(X_train_n1.shape)\n    print(X_sub_n1.shape)\n    reg_n1 = LinearRegression().fit(X_train_n1, y_train_n1)\n    pred_n1 = reg_n1.predict(X_pred_n1)\n    X_sub_n1.loc[:,\"SalePrice\"]=pred_n1\n    #X_sub_n1 = X_sub_n1.loc[:,[\"Id\",\"SalePrice\"]]\n    \n\n    X_train_n0 = df_train_numerical.loc[df_train_numerical['neighborhoodTier']==0,fav_variables].copy()\n    y_train_n0 = df_train_numerical.loc[df_train_numerical['neighborhoodTier']==0,\"SalePrice\"].copy()\n    X_sub_n0 = df_test_numerical.loc[df_test_numerical['neighborhoodTier']==0,[\"Id\",\"neighborhoodTier\"]+fav_variables]\n    X_pred_n0 = df_test_numerical.loc[df_test_numerical['neighborhoodTier']==0,fav_variables].copy()\n    print(\"Low Tier\")\n    print(X_train_n0.shape)\n    print(X_sub_n0.shape)\n    reg_n0 = LinearRegression().fit(X_train_n0, y_train_n0)\n    pred_n0 = reg_n0.predict(X_pred_n0)\n    X_sub_n0.loc[:,\"SalePrice\"]=pred_n0\n    #X_sub_n0 = X_sub_n0.loc[:,[\"Id\",\"SalePrice\"]]\n    \n    sub_unordered_df = pd.concat([X_sub_n2,X_sub_n1,X_sub_n0])\n    sub_df = df_test.merge(sub_unordered_df,on=\"Id\").loc[:,[\"Id\",\"SalePrice\"]]\n    sub_df.to_csv(\"submission.csv\", index=False)","e8f3b98b":"sub_df.head()","3100f8e8":"I am also interested how the number of stories affect Sales Price. Note that I called \"Split\" homes as 2 stories and I call \"2 family conversion\" or \"Duplex\" homes as \".5\" floor since you share a floor with other families.","2f03f975":"There does seem to be some form of a linear pattern up until we get to 11. Given that 75% of the rows have `TotRmsAbvGrd` <=7, 12 and 14 seem to be outliers. It may be easier to visualize in a scatter plot actually.","65456d45":"Set Pool Quality to 0 if there is no pool","3d59f8b4":"## Information Gain of Categorical Norminal Variables\nSince we have One-Hot Encoded our Categorical Nominal Variables, which columns have the least information gain?","3fabd421":"There is 1 row in the testing dataframe with no value for \"KitchenQual\"","7dd994d7":"It is confusing to me that the minimum of this is 2. Is there no 1 bed room places? Like a studio? I am very confused by this variable.","7d9195ed":"### Functionality\nIn the \"data_description.txt\" file it says to \"Assume typical unless deductions are warranted\". Therefore, we set the rows with null functionality to 7 which means its typical","a9dacee1":"For `LotFrontage` I am confused why some of these values would be null. Could it be possible that these homes do not have a street connected to their property. I need to visualize the distribution of values of Lot Frontage.","72cf08ed":"I definitley want to drop the \"Utilities\" column","f3be9f04":"I took the average of \"BsmtFinType2\" values within 1 SD around the `BsmtFinSF2` value","bfd5b908":"## TotalBsmtSF","43e40799":"There are 3 rows in the testing set which have null \"PoolQC\", but contain a pool.","f615191c":"## Continuous Variables Distribution\nI am interested in looking at the distribution of the continuous variables. Note that I remove 0's since 0's can skew the distribution.","bbff3012":"Interesting... This does seem to be generally linear...","0a623929":"I feel like I could merge 3 and 4 into a single category as \"Above 2\"...","ffe27d7c":"So I noticed that the columns with the most null values are features where a null value actually contains information. For example, if null is found in PoolQC, it means that the house does not have a pool. This observation led me to believe I am going to have to fill-in the rows that have NULL variables that mean something (see cleaning below).","8e36ab38":"We expect that if the \"MasVnrArea\" is 0 that the \"MasVnrType\" is none.","94914d19":"I am also interested in comparing PUD homes verses not.","e99b6a7f":"Oh I guess they all have 2 materials...","ad2faa93":"Observations:\n* The minimum is price is not 0 which make sense since houses aren't free. If there were free houses we may question our target values and drop those rows for training purposes\n* The median is about $20,000 less than the mean which means that the distribution will have at least a slight positive skew. I am interested in checking the \"normality\" of the target variable since this will effect the statistical tests I can do down the line.","27d38752":"I feel like that may have helped! The relation looks more exponential than linear though.","a980af77":"This leads me to believe that I should have a column for \"1946 & NEWER\" style.","8bf4316d":"Convert null variables which actually just mean that there is no basement.","26396da6":"Since there are only 2 rows with null values in the GarageFinish,GarageQual, GarageCond, GarageArea, and GarageCars variables in the testing data I'll just set them to the average values since there are so few rows with nulls anyway.","b45cf13d":"# Feature Engineering\n## Exterior Variables\nI noticed that the features `Exterior1st` and `Exterior2nd` are dependent of another. For example if `Exterior1st` is CemntBd then `Exterior2nd` cannot be. Therefore, I want to convert these columns so that we can check if the exterior has a specific type of covering. For example, `Exterior_CBlock` would be 1 if there is a Cinder Block covering on the house and 0 would be if `Exterior1st` and `Exterior2nd` are both not equal to \"CBlock\".","d6898f18":"### Fence","03171769":"## FullBath","d83424a3":"Given our new features, we can drop \"Condition1\" and \"Condition2\"","dbb65393":"## TotRmsAbvGrd","9f245c5b":"this feature seems to not be related to price so I am going to drop it.","d9a4be1f":"Add `HasGarage` variable to keep track of whether house has a Garage\n* Set `HasGarage` to 0 if \n    * `GarageArea` is 0\n    * `GarageCars` is 0\n    * the following are Null:\n        * GarageType\n        * GarageYrBlt\n        * GarageFinish\n        * GarageCars\n        * GarageQual\n        * GarageCond\n* Otherwise set `HasGarage` to 1 ","8c4eb99b":"### Drop Columns not found in Training or Testing Set","532d90f7":"Let's drop these columns","2639dacf":"In one-hot encoding, we made values of categorical nominal features their own row. For example, if our category was \"marital_status\" and the row was  [\u2018single,\u2019single\u2019,`married`,`divorced`] we would one-hot encode transform the sequence into the rows:\n* marital_status_single: [1,1,0,0]\n* marital_status_married: [0,0,1,0]\n* marital_status_divorced: [0,0,0,1]\n\nUnfotunately, not all values for every column that are found in the training set are also found in the test set. For example if those values were found in the training set, the test values may include some of the same values but not all and may even include values not found in the traning set. For example the row in the test set could be: [`married`,`single`,`seperated`] and have a new column called \"marital_status_seperated\" but not contain \"marital_status_divorced\". Since we don't have a \"marital_status_seperated\" column in the training set, we cannot train for that feature so it should be dropped from the test set. Similiarly, if we don't have a feature for \"marital_status_divorced\" in the test set then can't use it for prediction anyway so it should be dropped from the test set.\n","d2753b01":"Yep! Looks better now!","095525bc":"# Exploring Dependent Features\n","17e7524a":"So the number of stories does not have a linear relationship with price. That being said I want to remove the \"Stories\" feature but I want to have a binary column for sharing a floor (AKA if the house is a 2 family conversion or a duplex)","132cb2f6":"Kurtosis is used to describe the extreme values in one tail versus the other.\n\nHigh kurtosis means there are a lot of outliers. (>3)\n\nLow kurtosis means there are not a lot of outliers. (<3)","37a990bf":"There are:\n* 2 rows with null values for `BsmtQual` in the testing set \n* 3 rows with null values for `BsmtCond` in the testing set \n* 1 row with null values for `BsmtExposure` in the training set \n* 2 rows with null values for `BsmtExposure` in the testing set\n* 1 row with null values for `BsmtFinType2` in the training set \n\nWe can either use the average value to replace the null values (since isBasement is 1 we cannot set these to 0), or we can base them off other features that are not null.","e1a8907e":"If there is no Kitchen Quality value then set it to the average kitchen quality value","8985e40c":"# Exploratory Analysis (Post-Cleaning)\n## Target Variable (Sales Price)","bd7d8103":"This narrows down the features significantly. Let's identify their relationship with sale price.","9a17c1a2":"There seem to be a lot of nulls in GarageType. GarageType is fine because I dealth with that by turning it into multiple columns with One Hot Coding.","71b33944":"perform linear regression on all the data at once","4164c751":"Add `HasMasVnr` variable to keep track of whether house has a Masonry veneer area\n* Change `HasMasVnr` to 0 if `MasVnrArea` and `MasVnrType` are Null.\n* Change `MasVnrArea` to 0 if `MasVnrArea` is Null\n* Change `HasMasVnr` to 0 if `MasVnrArea` > 0 or `MasVnrType` is not Null.","9111c1a9":"Cool so our data is positively skewed and has lots of outliers, but what happens if we transform the Sales Price using the log transformation?","63f7f0c1":"Looks pretty linear to me!","2b54e032":"Next we can open the training and testing data","36910c35":"## Deal with Null Values that contain information content","d88e2a55":"There are 3 rows where this is the case so I am going to assume the data collector accidentally forgot to calculate the \"MasVnrArea\". Therefore, I want to reset these values to the average \"MasVnrArea\".","a88e1b47":"Add `HasBasement` variable to keep track of whether house has a Basement\n* Set `HasBasement` to 0 if\n    * the following are Null:\n        * BsmtQual\n        * BsmtCond\n        * BsmtExposure\n        * BsmtFinType1\n        * BsmtFinType2\n    * BsmtFinSF1 is 0 or Null\n    * BsmtFinSF2 is 0 or Null\n    * BsmtUnfSF is 0 or Null\n    * TotalBsmtSF is 0 or Null\n    * BsmtFullBath is 0 or Null\n    * BsmtHalfBath is 0 or Null\n* Otherwise set `HasBasement` to 1 ","c6c1f695":"Unfortunately we still have not dealt with all of the null values for basement features.\n","a7c00df6":"## OverallQual","fc980a9a":"# Weird values","6d4cb128":"### KitchenQual\n","6d79a46f":"## Location? Location? Location?","a555b015":"Looks pretty linear to me!","3f4e4be0":"Great! Since there are only 2 and 3 missing values in these columns respectively and their values have the same range I just set the null values to the value of the other. However, if we dropped \"BsmtCond\" we will just have to set \"BsmtQual\" to the average \"BsmtQual\" given a basement.","86c19e06":"If the there is no Garage, just set GarageYrBlt to the average year that Garages are built.","980a929a":"How do these categories affect price?","8427e619":"# Garage Area","6371d88d":"Set Fireplace Quality to 0 if there is no fireplace","5ac80fb6":"### PoolQC","cf269443":"I am just interested in the values of the features that have >90% of the same value.","aa11d3d4":"Hmmm there are definitely better neighborhoods...","d3f2c7bf":"Some Observations:\n* It make sense the `AllSF` is corelated to `GrLivArea` and `TotalBsmtSF` since it literally the sum of these two features. \n* It is interesting that  `AllSF`, `TotRmsAbvGrf` and `GrLivArea` are highly correlated to `AllFlrsSF`.\n* It makes sense the `ExterQual` and `ExterGrade` are correlated since `ExterGrade` is literally a multiple of `ExterQual`.\n* It makes sense that `KitchenScore` and `KitchenQu` are correlated\n* GarageArea, Garage Cars, and GarageScore are correlated which makes sense.\n* `TotalBath` and `FullBath` are correlated\n\n\n\nI am interested in looking at some of these variables more specifically...","b41293d3":"### LotFrontage","41723f00":"looks like a strong linear relationship!","076b25f3":"Oh Damn. I didn't realize you could have 0 FullBath. You need a bath in a house right? Does this mean we are missing information. Probably. Oy vey... Well at least its linear when we have the information... Maybe I should set the 0's to 1's?","5ec88ebf":"## Year Built","b24acc04":"So much better! The data points are now fairly symmetrical and there isn't as many outliers on one particular tail.","293805a0":"Therefore we take the average pool quality of pools that are around the same Area of the Pool (+\/-1SD) and set the pool quality manually to whatever the average pool quality is of pools that are that size.","af7f5459":"convert categorical-ordinal variables to integers","a2273c78":"Given our new features, we can drop \"Exterior1st\" and \"Exterior2nd\"","eb320bea":"### Barement Features: 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'","530c444d":"check for duplicate rows if there are any...","24e33d83":"Convert categorical-nominal variables into multiple columns using One Hot Encoding","97c54232":"# Introduction\nThis kernel was inspired from the following notebooks:\n* https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n* https:\/\/www.kaggle.com\/allunia\/house-prices-tutorial-with-catboost\n* https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n\nIn this text-based competition we are given 79 features about houses located in Ames, Iowa and expected to predict the prices of these houses. Our training set and testing set are relatively small with about 1458 rows each (assuming there are no redundancies).","35ad95d7":"I want to create a columns that tells what type of \"Tier\" neigborhood the house is located in. \n* The top Tier will have an average Sale price above or equal to the 75% percentile of the SalePrices. \n* The middle upper Tier will have an average Sale price between the 50th and 75th percentile. \n* The middle low Tier will have an average Sale price between the 25th and 50th percentile. \n* The bottom tier will have an average Sale price below the 25th percentile.","974ecca6":"Since there is only 1 row in the training set that has a null Electrical value, we could just drop that row.","fd77999b":"There are a lot of different ways to explore the variables so I am just going to start with the ones I think is important. Obviously, since SalePrice is the target variable it will be the first one I am interested in.","484a90f5":"# Modeling\nOur target value, 'SalesPrice' is continuous, therefore this challenge is a regression problem.\n\n## Linear Regression","0e82d94c":"We see that the sales price:\n* Deviates from the normal distribution.\n* Has appreciable positive skewness.\n* Shows peakedness.","a9071f0e":"Let's look at how much skew is in Sales Price. In case you forgot, here are the general rules for skewness:\n* If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n* If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n* If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.","b4dcb774":"For basement exposure, I just set it to the average value based on both the training and test set where HasBasement is true.","93803dae":"and now I can drop \"MSSubClass\"","efa47c21":"It looks like the data is positively skewed","705e5362":"Add `HasLotFrontage` variable to keep track of whether house has a Front\n* Set `HasLotFrontage` to 0 if `LotFrontage` is Null.\n* Set `LotFrontage` to 0 if `LotFrontage` is Null\n* Change `HasLotFrontage` to 1 if `LotFrontage` > 0 ","eefdb541":"This looks rough. Lets log transform it... It still look questionable...","abaffd0c":"### MasVnrType and MasVnrArea","6476e151":"There are about 24 features with correlation values > 0.5. The next question is how much they are correlated to one another?","2e063a85":"## Condition","37eda2bc":"unfortunately we are not done with the null features in the Garage Columns","8c083cb9":"## FireplaceQu","77720a95":"Homes with missing garage values most likely do not have garages. Therefore, I will impute 0 for most of these columns. Unfortunately setting the GarageYrBuilt to 0 doesn't make any sense so I'll just set these values to the mode. ","3986b7c6":"## duplicate rows","4bd41de0":"*Although it's not a strong tendency, I'd say that 'SalePrice' is more prone to spend more money in new stuff than in old relics.*\n\n**Note**: we don't know if 'SalePrice' is in constant prices. Constant prices try to remove the effect of inflation. If 'SalePrice' is not in constant prices, it should be, so than prices are comparable over the years.","c5bee0af":"Hmm so it looks like this features has a pretty normal distribution. I'm assuming that every property has a lot (since LotFrontage is always >0) so I'll just imput the mean.","7a789640":"## Deal with Categorical Variables","daf7d49f":"First we must set some parameters for cleaning","b4b0a1a6":"I am only going to use the variables that I belive to show strong LINEAR correlation with Saleprice","b93b1b2c":"### Garage Features","e01c9f3c":"## Categorical feature importance\nI want to remove features that contain almost no information gain. A feature has no information gain if every row has the same value. Therefore, to measure information gain we can compute the frequency of the most common level in the train & test dat.","eac3a397":"# Cleaning and Reorganizing the training data","1ecb8590":"We see using the correlation plot that `BsmtCond` and `BsmtQual` are strongly correlated. I check to make sure that these two features are never both null when in our current dataframe.","dac0bad8":"I don't think I should use this variable...","6153cd58":"Set Fence Quality to 0 if there is no fence","eb288bb1":"## Correlation between Target Variable and Features\ncompare correlation between the features and the target variable `SalePrice`","ecc081f3":"Yeah, I think that makes more sense.","5ff2b3d8":"The easiest way I know to summarize the variables is by using the `.describe()` function","474bad16":"Then I found the standard deviation of the `BsmtFinSF2` in houses with basements","f1e6a8ae":"Similarly, I noticed that the features `Condition1` and `Condition2` are also dependent of another so I want to split their values into their own columns.","71733da8":"Yeah I'll keep this feature.","ba543ed9":"## YearRemodAdd","ddb04d15":"Hm, I don't know how I feel about this feature.","1364779f":"perform 3 seperate linear regression models for each neighborhood tier","a3cee6d8":"## MSSubClass\nThis feature contains a lot of information and the numbering system it use to identify this information is ridiculous so I will break down this one column into new features. Before I do that, though I want to value counts of each category.**","52836065":"## GarageFinish","892bb498":"### FireplaceQu","435a044b":"This plot shows a pretty good linear relationship! However, there seems to be 2 extreme outliers on the bottom right. The author of the dataset specifically recommends removing 'any houses with more than 4000 square feet' from the dataset.\nReference : https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf","5acde449":"# Feature Engineering Part II\n## Combinations of Existing Features\nThe code was adapted came from https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset","7c7ef52e":"This may be a useful feature so I am going to keep it. ","20fae2a6":"I also wonder if price is affected by whether the Exterior covering has 1 or 2 materials on it.","7433a5b9":"# Exploratory Analysis (Pre-Cleaning)","f2b44bd9":"\nWe have a lot more columns in the testing data with null variables. Since we need to predict these rows we cannot drop rows. Our options are to either drop the column or fill in the null values.\n\nI decided to fill in these values since I believe I have a general idea of how to go about doing that and I don't want to lose any information yet.","97616545":"## Missing values","87f3b4fb":"Since there is only 1 null row with `BsmtFinType2` and `BsmtFinType2` is highly correlated with `BsmtFinSF2`, I got the \"BsmtFinSF2\" value of the null row:","6ae20ee6":"# Garage Cars","57192a82":"This relationship isn't super clear. I wonder what would happen if we log transform","0ef3e99d":"turns out there were none...","02a7374d":"# ExterQual","b537ae4c":"# KitchenQual","f602773d":"## GrLivArea","c1c1d50b":"Looking at these plots, I think the following variables are strongly correlated to price:\n* MSZoning_RL\n* MasVnrType_None\n* GarageType_Attchd\n* Foundation_PConc\n* Exterior_VinylSd","99ebcab3":" I'd say that 'SalePrice' is more prone to spend more money if it was recently renovated."}}