{"cell_type":{"19737842":"code","8c442326":"code","d97b1292":"code","426255ec":"code","997cc5a2":"code","f24d0476":"code","f9229e61":"code","77ec3191":"code","8aa53ae6":"code","7f7a00db":"code","bacd5b19":"code","346ea761":"code","3e79522c":"code","84086b8b":"code","6850887f":"code","0b2e739f":"code","65779d99":"code","66bb5cc7":"code","f636124f":"code","bd6c7345":"code","007b286b":"code","7ff8fcbb":"code","2bda65b0":"code","6efb5401":"code","3960df98":"code","e5c1eae4":"code","58e4ffeb":"code","b88e598f":"code","e6e78627":"code","63e11a1a":"code","9dd2cdfd":"code","36d9beaa":"code","2e97d57a":"code","10e83b56":"code","496549cb":"code","550f0e37":"code","0132aac5":"markdown","4a0dbb11":"markdown","567a04c3":"markdown","700f7135":"markdown","cb461e6e":"markdown"},"source":{"19737842":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c442326":"#data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#string\nimport string\nimport re\n\n\n#text processing\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nfrom textblob import TextBlob","d97b1292":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain.head()","426255ec":"train.info()","997cc5a2":"test.info()","f24d0476":"#dropping the null valued columns \ntrain.drop(['keyword','location'],inplace=True,axis=1)\ntest.drop(['keyword','location'],inplace=True,axis=1)","f9229e61":"#count statistics\ndef words_count(df):\n    df['word_count']=df['text'].apply(lambda x: len([x for x in x.split()]))\n    print(df[['word_count','text']])","77ec3191":"words_count(train)","8aa53ae6":"words_count(test)","7f7a00db":"train['word_count'].describe()","bacd5b19":"#lowercasing\ndef lowercasing(df):\n    df['cleaned']=df['text'].apply(lambda x:' '.join([x.lower() for x in x.split()]))\n    \n#removing url links\ndef remove_URL(df):\n    df['cleaned']=df['cleaned'].apply(lambda x:' '.join([x for x in x.split() if x[:3]!='http']))\n\n#remove punctuation\ndef remove_punctuation(df):\n    df['cleaned']=df['cleaned'].str.replace('[^\\w\\s]','')\n\n#removing stopwords\ndef remove_stopwords(df):\n    stop=stopwords.words('english')\n    df['cleaned']=df['cleaned'].apply(lambda x:' '.join([x for x in x.split() if x not in stop]))\n\n#lemmatization\ndef lemmatization(df):\n    lemm=WordNetLemmatizer()\n    df['cleaned']=df['cleaned'].apply(lambda x:' '.join([lemm.lemmatize(x) for x in x.split()]))\n    return (df[['text','cleaned']].head())\n\n\n    \n","346ea761":"lowercasing(train)\nremove_URL(train)\nremove_punctuation(train)\nremove_stopwords(train)\nlemmatization(train)\n","3e79522c":"lowercasing(test)\nremove_URL(test)\nremove_punctuation(test)\nremove_stopwords(test)\nlemmatization(test)\n\n","84086b8b":"X_train=train['cleaned'][:-2284]\nX_val=train['cleaned'][-2284:]","6850887f":"y_train=train['target'][:-2284]\ny_val=train['target'][-2284:]","0b2e739f":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(max_features=10000)\ncv.fit(train['cleaned'])\nX_train_cv=cv.transform(X_train)\nX_val_cv=cv.transform(X_val)","65779d99":"test_cv=cv.transform(test['cleaned'])\ntest_cv","66bb5cc7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n\n\nclf=LogisticRegression()\nclf.fit(X_train_cv,y_train)\npredictions=clf.predict(X_val_cv)","f636124f":"clf.score(X_train_cv,y_train)","bd6c7345":"from sklearn.metrics import accuracy_score,classification_report,precision_score\nprint('accuracy score: ',accuracy_score(predictions,y_val))\nprint('claasification report: \\n',classification_report(y_val,predictions))\nprint('confusion matrix \\n',confusion_matrix(y_val,predictions))","007b286b":"df=pd.concat([train['cleaned'],test['cleaned']])\nlen(df)","7ff8fcbb":"from nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\n\ncorpus=[]\ndef create_corpus(df):\n    for tweet in tqdm(df):\n        words=[word for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus","2bda65b0":"corpus=create_corpus(df)","6efb5401":"embedding_dict={}\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()\n        ","3960df98":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nMAX=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX,padding='post',truncating='post')\n\n","e5c1eae4":"word_index=tokenizer_obj.word_index\nword_index","58e4ffeb":"print('Total no of unique words are: ',len(word_index))","b88e598f":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i>num_words:\n        continue\n        \n    embedding_vec=embedding_dict.get(word)\n    \n    if embedding_vec is not None:\n        embedding_matrix[i]=embedding_vec\n    ","e6e78627":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.initializers import Constant","63e11a1a":"model=Sequential()\nembedding=layers.Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),input_length=MAX,trainable=False)\nmodel.add(embedding)\nmodel.add(layers.SpatialDropout1D(0.2))\nmodel.add(layers.LSTM(64,dropout=0.2,recurrent_dropout=0.2))\nmodel.add(layers.Dense(1,activation='sigmoid'))\n","9dd2cdfd":"model.summary()","36d9beaa":"from tensorflow.keras.optimizers import Adam\nmodel.compile(optimizer=Adam(lr=1e-5),loss='binary_crossentropy',metrics=['accuracy'])","2e97d57a":"train_df=tweet_pad[:train.shape[0]]\ntest_df=tweet_pad[train.shape[0]:]","10e83b56":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(train_df,train['target'],test_size=0.2,random_state=123)\nX_train.shape","496549cb":"X_test.shape","550f0e37":"hist=model.fit(X_train,y_train,epochs=15,verbose=1,validation_data=(X_test,y_test))","0132aac5":"# Bag of words","4a0dbb11":"# GloVe for vectorization","567a04c3":"# Text preprocessing","700f7135":"# Model","cb461e6e":"## Baseline model"}}