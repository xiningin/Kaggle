{"cell_type":{"54603eb4":"code","c75c2050":"code","8ea1e848":"code","5f8f9d05":"code","df36d9f1":"code","7aec4c75":"code","a64b7281":"code","7285ebaf":"code","5e417b1b":"code","a921eb69":"code","f255e9ee":"code","2db0f514":"code","1efa62cb":"code","8b504d97":"code","c5b209ac":"code","38fe4600":"code","26bf0199":"code","f12b009e":"code","0309de77":"code","21516697":"markdown","a3347ae1":"markdown","18a102bd":"markdown","06e421e6":"markdown","91a9d251":"markdown","89be058e":"markdown","49c7034e":"markdown","c91caa33":"markdown","28b9d1d8":"markdown"},"source":{"54603eb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c75c2050":"# CUDA: Let's check that Nvidia CUDA drivers are already pre-installed and which version is it.\n!\/usr\/local\/cuda\/bin\/nvcc --version\n# We need to install the correct cuDNN according to this output","8ea1e848":"!nvidia-smi","5f8f9d05":"# Change the number depending on what GPU is listed above, under NVIDIA-SMI > Name.\n# Tesla K80: 30\n# Tesla P100: 60\n# Tesla T4: 75\n%env compute_capability=60","df36d9f1":"%cd \/kaggle\/input\/content\/\n%rm -rf darknet","7aec4c75":"#we clone the fork of darknet maintained by roboflow\n#small changes have been made to configure darknet for training\n!git clone https:\/\/github.com\/roboflow-ai\/darknet.git","a64b7281":"%cd \/content\/darknet\/\n%rm Makefile","7285ebaf":"#colab occasionally shifts dependencies around, at the time of authorship, this Makefile works for building Darknet on Colab\n\n%%writefile Makefile\nGPU=1\nCUDNN=1\nCUDNN_HALF=0\nOPENCV=1\nAVX=0\nOPENMP=0\nLIBSO=1\nZED_CAMERA=0\nZED_CAMERA_v2_8=0\n\n# set GPU=1 and CUDNN=1 to speedup on GPU\n# set CUDNN_HALF=1 to further speedup 3 x times (Mixed-precision on Tensor Cores) GPU: Volta, Xavier, Turing and higher\n# set AVX=1 and OPENMP=1 to speedup on CPU (if error occurs then set AVX=0)\n# set ZED_CAMERA=1 to enable ZED SDK 3.0 and above\n# set ZED_CAMERA_v2_8=1 to enable ZED SDK 2.X\n\nUSE_CPP=0\nDEBUG=0\n\nARCH= -gencode arch=compute_30,code=sm_30 \\\n      -gencode arch=compute_35,code=sm_35 \\\n      -gencode arch=compute_50,code=[sm_50,compute_50] \\\n      -gencode arch=compute_52,code=[sm_52,compute_52] \\\n      -gencode arch=compute_61,code=[sm_61,compute_61]\n\nOS := $(shell uname)\n\n# Tesla V100\n# ARCH= -gencode arch=compute_70,code=[sm_70,compute_70]\n\n# GeForce RTX 2080 Ti, RTX 2080, RTX 2070, Quadro RTX 8000, Quadro RTX 6000, Quadro RTX 5000, Tesla T4, XNOR Tensor Cores\n# ARCH= -gencode arch=compute_75,code=[sm_75,compute_75]\n\n# Jetson XAVIER\n# ARCH= -gencode arch=compute_72,code=[sm_72,compute_72]\n\n# GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4\n# ARCH= -gencode arch=compute_61,code=sm_61 -gencode arch=compute_61,code=compute_61\n\n# GP100\/Tesla P100 - DGX-1\n# ARCH= -gencode arch=compute_60,code=sm_60\n\n# For Jetson TX1, Tegra X1, DRIVE CX, DRIVE PX - uncomment:\n# ARCH= -gencode arch=compute_53,code=[sm_53,compute_53]\n\n# For Jetson Tx2 or Drive-PX2 uncomment:\n# ARCH= -gencode arch=compute_62,code=[sm_62,compute_62]\n\n\nVPATH=.\/src\/\nEXEC=darknet\nOBJDIR=.\/obj\/\n\nifeq ($(LIBSO), 1)\nLIBNAMESO=libdarknet.so\nAPPNAMESO=uselib\nendif\n\nifeq ($(USE_CPP), 1)\nCC=g++\nelse\nCC=gcc\nendif\n\nCPP=g++ -std=c++11\nNVCC=nvcc\nOPTS=-Ofast\nLDFLAGS= -lm -pthread\nCOMMON= -Iinclude\/ -I3rdparty\/stb\/include\nCFLAGS=-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC\n\nifeq ($(DEBUG), 1)\n#OPTS= -O0 -g\n#OPTS= -Og -g\nCOMMON+= -DDEBUG\nCFLAGS+= -DDEBUG\nelse\nifeq ($(AVX), 1)\nCFLAGS+= -ffp-contract=fast -mavx -mavx2 -msse3 -msse4.1 -msse4.2 -msse4a\nendif\nendif\n\nCFLAGS+=$(OPTS)\n\nifneq (,$(findstring MSYS_NT,$(OS)))\nLDFLAGS+=-lws2_32\nendif\n\nifeq ($(OPENCV), 1)\nCOMMON+= -DOPENCV\nCFLAGS+= -DOPENCV\nLDFLAGS+= `pkg-config --libs opencv4 2> \/dev\/null || pkg-config --libs opencv`\nCOMMON+= `pkg-config --cflags opencv4 2> \/dev\/null || pkg-config --cflags opencv`\nendif\n\nifeq ($(OPENMP), 1)\nCFLAGS+= -fopenmp\nLDFLAGS+= -lgomp\nendif\n\nifeq ($(GPU), 1)\nCOMMON+= -DGPU -I\/usr\/local\/cuda\/include\/\nCFLAGS+= -DGPU\nifeq ($(OS),Darwin) #MAC\nLDFLAGS+= -L\/usr\/local\/cuda\/lib -lcuda -lcudart -lcublas -lcurand\nelse\nLDFLAGS+= -L\/usr\/local\/cuda\/lib64 -lcuda -lcudart -lcublas -lcurand\nendif\nendif\n\nifeq ($(CUDNN), 1)\nCOMMON+= -DCUDNN\nifeq ($(OS),Darwin) #MAC\nCFLAGS+= -DCUDNN -I\/usr\/local\/cuda\/include\nLDFLAGS+= -L\/usr\/local\/cuda\/lib -lcudnn\nelse\nCFLAGS+= -DCUDNN -I\/usr\/local\/cudnn\/include\nLDFLAGS+= -L\/usr\/local\/cudnn\/lib64 -lcudnn\nendif\nendif\n\nifeq ($(CUDNN_HALF), 1)\nCOMMON+= -DCUDNN_HALF\nCFLAGS+= -DCUDNN_HALF\nARCH+= -gencode arch=compute_70,code=[sm_70,compute_70]\nendif\n\nifeq ($(ZED_CAMERA), 1)\nCFLAGS+= -DZED_STEREO -I\/usr\/local\/zed\/include\nifeq ($(ZED_CAMERA_v2_8), 1)\nLDFLAGS+= -L\/usr\/local\/zed\/lib -lsl_core -lsl_input -lsl_zed\n#-lstdc++ -D_GLIBCXX_USE_CXX11_ABI=0\nelse\nLDFLAGS+= -L\/usr\/local\/zed\/lib -lsl_zed\n#-lstdc++ -D_GLIBCXX_USE_CXX11_ABI=0\nendif\nendif\n\nOBJ=image_opencv.o http_stream.o gemm.o utils.o dark_cuda.o convolutional_layer.o list.o image.o activations.o im2col.o col2im.o blas.o crop_layer.o dropout_layer.o maxpool_layer.o softmax_layer.o data.o matrix.o network.o connected_layer.o cost_layer.o parser.o option_list.o darknet.o detection_layer.o captcha.o route_layer.o writing.o box.o nightmare.o normalization_layer.o avgpool_layer.o coco.o dice.o yolo.o detector.o layer.o compare.o classifier.o local_layer.o swag.o shortcut_layer.o activation_layer.o rnn_layer.o gru_layer.o rnn.o rnn_vid.o crnn_layer.o demo.o tag.o cifar.o go.o batchnorm_layer.o art.o region_layer.o reorg_layer.o reorg_old_layer.o super.o voxel.o tree.o yolo_layer.o gaussian_yolo_layer.o upsample_layer.o lstm_layer.o conv_lstm_layer.o scale_channels_layer.o sam_layer.o\nifeq ($(GPU), 1)\nLDFLAGS+= -lstdc++\nOBJ+=convolutional_kernels.o activation_kernels.o im2col_kernels.o col2im_kernels.o blas_kernels.o crop_layer_kernels.o dropout_layer_kernels.o maxpool_layer_kernels.o network_kernels.o avgpool_layer_kernels.o\nendif\n\nOBJS = $(addprefix $(OBJDIR), $(OBJ))\nDEPS = $(wildcard src\/*.h) Makefile include\/darknet.h\n\nall: $(OBJDIR) backup results setchmod $(EXEC) $(LIBNAMESO) $(APPNAMESO)\n\nifeq ($(LIBSO), 1)\nCFLAGS+= -fPIC\n\n$(LIBNAMESO): $(OBJDIR) $(OBJS) include\/yolo_v2_class.hpp src\/yolo_v2_class.cpp\n\t$(CPP) -shared -std=c++11 -fvisibility=hidden -DLIB_EXPORTS $(COMMON) $(CFLAGS) $(OBJS) src\/yolo_v2_class.cpp -o $@ $(LDFLAGS)\n\n$(APPNAMESO): $(LIBNAMESO) include\/yolo_v2_class.hpp src\/yolo_console_dll.cpp\n\t$(CPP) -std=c++11 $(COMMON) $(CFLAGS) -o $@ src\/yolo_console_dll.cpp $(LDFLAGS) -L .\/ -l:$(LIBNAMESO)\nendif\n\n$(EXEC): $(OBJS)\n\t$(CPP) -std=c++11 $(COMMON) $(CFLAGS) $^ -o $@ $(LDFLAGS)\n\n$(OBJDIR)%.o: %.c $(DEPS)\n\t$(CC) $(COMMON) $(CFLAGS) -c $< -o $@\n\n$(OBJDIR)%.o: %.cpp $(DEPS)\n\t$(CPP) -std=c++11 $(COMMON) $(CFLAGS) -c $< -o $@\n\n$(OBJDIR)%.o: %.cu $(DEPS)\n\t$(NVCC) $(ARCH) $(COMMON) --compiler-options \"$(CFLAGS)\" -c $< -o $@\n\n$(OBJDIR):\n\tmkdir -p $(OBJDIR)\nbackup:\n\tmkdir -p backup\nresults:\n\tmkdir -p results\nsetchmod:\n\tchmod +x *.sh\n\n.PHONY: clean\n\nclean:\n\trm -rf $(OBJS) $(EXEC) $(LIBNAMESO) $(APPNAMESO)","5e417b1b":"#install environment from the Makefile\n#note if you are on Colab Pro this works on a P100 GPU\n#if you are on Colab free, you may need to change the Makefile for the K80 GPU\n#this goes for any GPU, you need to change the Makefile to inform darknet which GPU you are running on.\n#note the Makefile above should work for you, if you need to tweak, try the below\n%cd darknet\/\n#!sed -i 's\/OPENCV=0\/OPENCV=1\/g' Makefile\n#!sed -i 's\/GPU=0\/GPU=1\/g' Makefile\n#!sed -i 's\/CUDNN=0\/CUDNN=1\/g' Makefile\n!#sed -i \"s\/ARCH= -gencode arch=compute_60,code=sm_60\/ARCH= -gencode arch=compute_${compute_capability},code=sm_${compute_capability}\/g\" Makefile\n!make","a921eb69":"#download the newly released yolov4 ConvNet weights\n%cd \/content\/darknet\n!wget https:\/\/github.com\/AlexeyAB\/darknet\/releases\/download\/darknet_yolo_v3_optimal\/yolov4.conv.137","f255e9ee":"#if you already have YOLO darknet format, you can skip this step\n%cd \/content\/darknet\n!curl -L \"https:\/\/app.roboflow.com\/ds\/9de7m4Cgrj?key=ocDVBAueR8\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n#!curl -L [YOUR LINK HERE] > roboflow.zip; unzip roboflow.zip; rm roboflow.zip","2db0f514":"#Set up training file directories for custom dataset\n%cd \/content\/darknet\/\n%cp train\/_darknet.labels data\/obj.names\n%mkdir data\/obj\n#copy image and labels\n%cp train\/*.jpg data\/obj\/\n%cp valid\/*.jpg data\/obj\/\n\n%cp train\/*.txt data\/obj\/\n%cp valid\/*.txt data\/obj\/\n\nwith open('data\/obj.data', 'w') as out:\n  out.write('classes = 3\\n')\n  out.write('train = data\/train.txt\\n')\n  out.write('valid = data\/valid.txt\\n')\n  out.write('names = data\/obj.names\\n')\n  out.write('backup = backup\/')\n\n#write train file (just the image list)\nimport os\n\nwith open('data\/train.txt', 'w') as out:\n  for img in [f for f in os.listdir('train') if f.endswith('jpg')]:\n    out.write('data\/obj\/' + img + '\\n')\n\n#write the valid file (just the image list)\nimport os\n\nwith open('data\/valid.txt', 'w') as out:\n  for img in [f for f in os.listdir('valid') if f.endswith('jpg')]:\n    out.write('data\/obj\/' + img + '\\n')","1efa62cb":"#we build config dynamically based on number of classes\n#we build iteratively from base config files. This is the same file shape as cfg\/yolo-obj.cfg\ndef file_len(fname):\n  with open(fname) as f:\n    for i, l in enumerate(f):\n      pass\n  return i + 1\n\nnum_classes = file_len('train\/_darknet.labels')\nprint(\"writing config for a custom YOLOv4 detector detecting number of classes: \" + str(num_classes))\n\n#Instructions from the darknet repo\n#change line max_batches to (classes*2000 but not less than number of training images, and not less than 6000), f.e. max_batches=6000 if you train for 3 classes\n#change line steps to 80% and 90% of max_batches, f.e. steps=4800,5400\nif os.path.exists('.\/cfg\/custom-yolov4-detector.cfg'): os.remove('.\/cfg\/custom-yolov4-detector.cfg')\n\n\nwith open('.\/cfg\/custom-yolov4-detector.cfg', 'a') as f:\n  f.write('[net]' + '\\n')\n  f.write('batch=64' + '\\n')\n  #####smaller subdivisions help the GPU run faster. 12 is optimal, but you might need to change to 24,36,64####\n  f.write('subdivisions=24' + '\\n')\n  f.write('width=416' + '\\n')\n  f.write('height=416' + '\\n')\n  f.write('channels=3' + '\\n')\n  f.write('momentum=0.949' + '\\n')\n  f.write('decay=0.0005' + '\\n')\n  f.write('angle=0' + '\\n')\n  f.write('saturation = 1.5' + '\\n')\n  f.write('exposure = 1.5' + '\\n')\n  f.write('hue = .1' + '\\n')\n  f.write('\\n')\n  f.write('learning_rate=0.001' + '\\n')\n  f.write('burn_in=1000' + '\\n')\n  ######you can adjust up and down to change training time#####\n  ##Darknet does iterations with batches, not epochs####\n  max_batches = num_classes*2000\n  #max_batches = 2000\n  f.write('max_batches=' + str(max_batches) + '\\n')\n  f.write('policy=steps' + '\\n')\n  steps1 = .8 * max_batches\n  steps2 = .9 * max_batches\n  f.write('steps='+str(steps1)+','+str(steps2) + '\\n')\n\n#Instructions from the darknet repo\n#change line classes=80 to your number of objects in each of 3 [yolo]-layers:\n#change [filters=255] to filters=(classes + 5)x3 in the 3 [convolutional] before each [yolo] layer, keep in mind that it only has to be the last [convolutional] before each of the [yolo] layers.\n\n  with open('cfg\/yolov4-custom2.cfg', 'r') as f2:\n    content = f2.readlines()\n    for line in content:\n      f.write(line)    \n    num_filters = (num_classes + 5) * 3\n    f.write('filters='+str(num_filters) + '\\n')\n    f.write('activation=linear')\n    f.write('\\n')\n    f.write('\\n')\n    f.write('[yolo]' + '\\n')\n    f.write('mask = 0,1,2' + '\\n')\n    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\\n')\n    f.write('classes=' + str(num_classes) + '\\n')\n\n  with open('cfg\/yolov4-custom3.cfg', 'r') as f3:\n    content = f3.readlines()\n    for line in content:\n      f.write(line)    \n    num_filters = (num_classes + 5) * 3\n    f.write('filters='+str(num_filters) + '\\n')\n    f.write('activation=linear')\n    f.write('\\n')\n    f.write('\\n')\n    f.write('[yolo]' + '\\n')\n    f.write('mask = 3,4,5' + '\\n')\n    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\\n')\n    f.write('classes=' + str(num_classes) + '\\n')\n\n  with open('cfg\/yolov4-custom4.cfg', 'r') as f4:\n    content = f4.readlines()\n    for line in content:\n      f.write(line)    \n    num_filters = (num_classes + 5) * 3\n    f.write('filters='+str(num_filters) + '\\n')\n    f.write('activation=linear')\n    f.write('\\n')\n    f.write('\\n')\n    f.write('[yolo]' + '\\n')\n    f.write('mask = 6,7,8' + '\\n')\n    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\\n')\n    f.write('classes=' + str(num_classes) + '\\n')\n    \n  with open('cfg\/yolov4-custom5.cfg', 'r') as f5:\n    content = f5.readlines()\n    for line in content:\n      f.write(line)\n\nprint(\"file is written!\")    \n","8b504d97":"#here is the file that was just written. \n#you may consider adjusting certain things\n\n#like the number of subdivisions 64 runs faster but Colab GPU may not be big enough\n#if Colab GPU memory is too small, you will need to adjust subdivisions to 16\n%cat cfg\/custom-yolov4-detector.cfg","c5b209ac":"!.\/darknet detector train data\/obj.data cfg\/custom-yolov4-detector.cfg yolov4.conv.137 -dont_show -map\n#If you get CUDA out of memory adjust subdivisions above!\n#adjust max batches down for shorter training above","38fe4600":"#define utility function\ndef imShow(path):\n  import cv2\n  import matplotlib.pyplot as plt\n  %matplotlib inline\n\n  image = cv2.imread(path)\n  height, width = image.shape[:2]\n  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n\n  fig = plt.gcf()\n  fig.set_size_inches(18, 10)\n  plt.axis(\"off\")\n  #plt.rcParams['figure.figsize'] = [10, 5]\n  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n  plt.show()","26bf0199":"#check if weigths have saved yet\n#backup houses the last weights for our detector\n#(file yolo-obj_last.weights will be saved to the build\\darknet\\x64\\backup\\ for each 100 iterations)\n#(file yolo-obj_xxxx.weights will be saved to the build\\darknet\\x64\\backup\\ for each 1000 iterations)\n#After training is complete - get result yolo-obj_final.weights from path build\\darknet\\x64\\bac\n!ls backup\n#if it is empty you haven't trained for long enough yet, you need to train for at least 100 iterations","f12b009e":"#coco.names is hardcoded somewhere in the detector\n%cp data\/obj.names data\/coco.names","0309de77":"\n#\/test has images that we can test our detector on\ntest_images = [f for f in os.listdir('test') if f.endswith('.jpg')]\nimport random\nimg_path = \"test\/\" + random.choice(test_images);\n\n#test out our detector!\n!.\/darknet detect cfg\/custom-yolov4-detector.cfg backup\/custom-yolov4-detector_last.weights {img_path} -dont-show\nimShow('predictions.jpg')","21516697":"# Infer Custom Objects with Saved YOLOv4 Weights","a3347ae1":"# Write Custom Training Config for YOLOv4","18a102bd":"# Step 2: Installing Darknet for YOLOv4 on Colab","06e421e6":"# Configuring cuDNN on Colab for YOLOv4","91a9d251":"# Set up Custom Dataset for YOLOv4","89be058e":"# Train Custom YOLOv4 Detector","49c7034e":"# STEP 1. Install cuDNN according to the current CUDA version\nKaggle added cuDNN as an inherent install - so you don't have to do a thing - major win","c91caa33":"## Introduction\n\n\nIn this notebook, we implement [YOLOv4](https:\/\/arxiv.org\/pdf\/2004.10934.pdf) for training on your own dataset.\n\nWe also recommend reading our blog post on [Training YOLOv4 on custom data](https:\/\/blog.roboflow.ai\/training-yolov4-on-a-custom-dataset\/) side by side.\n\nWe will take the following steps to implement YOLOv4 on our custom data:\n* Configure our GPU environment on Google Colab\n* Install the Darknet YOLOv4 training environment\n* Download our custom dataset for YOLOv4 and set up directories\n* Configure a custom YOLOv4 training config file for Darknet\n* Train our custom YOLOv4 object detector\n* Reload YOLOv4 trained weights and make inference on test images\n\nWhen you are done you will have a custom detector that you can use. It will make inference like this:\n\n#### ![Roboflow Workmark](https:\/\/i.imgur.com\/L0n564N.png)\n\n","28b9d1d8":"We'll use Roboflow to convert our dataset from any format to the YOLO Darknet format. \n\n1. To do so, create a free [Roboflow account](https:\/\/app.roboflow.ai).\n2. Upload your images and their annotations (in any format: VOC XML, COCO JSON, TensorFlow CSV, etc).\n3. Apply preprocessing and augmentation steps you may like. We recommend at least `auto-orient` and a `resize` to 416x416. Generate your dataset.\n4. Export your dataset in the **YOLO Darknet format**.\n5. Copy your download link, and paste it below.\n\nSee our [blog post](https:\/\/blog.roboflow.ai\/training-yolov4-on-a-custom-dataset\/) for greater detail.\n\nIn this example, I used the open source [BCCD Dataset](https:\/\/public.roboflow.ai\/object-detection\/bccd). (You can `fork` it to your Roboflow account to follow along.)"}}