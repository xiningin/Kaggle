{"cell_type":{"a848400a":"code","6b1892ee":"code","19d95912":"code","56c959c6":"code","23780ea5":"code","68963337":"code","5d1da06c":"code","4c05ccb9":"code","70b4d919":"code","38a53857":"code","7e6bf999":"code","238e66f6":"code","93b0e4f1":"code","559c73f8":"code","6a74379a":"code","be14d9cd":"code","a34ad701":"code","900b3035":"code","670299c3":"code","3a0ee590":"markdown","c90b7893":"markdown","3aee2796":"markdown","7c4380d2":"markdown","0a7e9ec1":"markdown","a824d9da":"markdown","dba080b8":"markdown","e1830486":"markdown","280f9cb8":"markdown","6bf695b8":"markdown","a0a137ea":"markdown","372f06d2":"markdown","fa79801d":"markdown"},"source":{"a848400a":"\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nimport warnings\nfrom sklearn import metrics\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","6b1892ee":"df=pd.read_csv(\"\/kaggle\/input\/bank-marketing\/bank-additional-full.csv\", sep=\";\")","19d95912":"df.head()","56c959c6":"df.info()","23780ea5":"\nsns.countplot(x='education', data= df)\nsns.despine()\nprint(df['previous'].value_counts())\nprint(\" Job title : \", df['job'].unique())\n","68963337":"# Education state\nfig, ax=plt.subplots()\nfig.set_size_inches(10,8)\nsns.countplot(x='education',data=df)\nax.set_xlabel('Education', fontsize=15)\nax.set_ylabel('Count', fontsize=15)\nax.set_title(\"Education State\", fontsize=15)\nsns.despine()","5d1da06c":"# marital\nprint(\"\\nMarital\")\nfig, ax=plt.subplots()\nfig.set_size_inches(10,8)\nsns.countplot(x='marital', data=df)\nax.set_xlabel(\"marital\", fontsize=10)\nax.set_ylabel(\"Count\", fontsize=10)\nax.set_title(\"Marital State\", fontsize=15)\nsns.despine()\n","4c05ccb9":"# Encoding \n\nfrom sklearn.preprocessing import  LabelEncoder\n\nEncoder=LabelEncoder()\n\ndf['job']=Encoder.fit_transform(df['job'])\n#df['marital']=Encoder.fit_transform(df['marital'])\ndf['education']=Encoder.fit_transform(df['education'])\ndf['default']=Encoder.fit_transform(df['default'])\ndf['housing']=Encoder.fit_transform(df['housing'])\ndf['loan']=Encoder.fit_transform(df['loan'])\ndf['month']=Encoder.fit_transform(df['month'])\ndf['contact']=Encoder.fit_transform(df['contact'])\ndf['day_of_week']=Encoder.fit_transform(df[\"day_of_week\"])\ndf['poutcome']=Encoder.fit_transform(df['poutcome'])\n\n# transform to binary the target attribute\ndf['y']=Encoder.fit_transform(df['y'])\n\n# setting value of marital\n\ndf['marital'].replace(['married', 'single', 'divorced','unknown'],[1,2,3,4], inplace=True)","70b4d919":"df.loc[df['age'] <26, 'age'] = 1\ndf.loc[(df['age'] >25) & (df['age']< 49 ),'age']=2\ndf.loc[(df['age']>48)&(df['age']<71), 'age']=3\ndf.loc[(df['age'] >70)&(df['age']<98), 'age']=4","38a53857":"\nfig, (ax1,ax2) = plt.subplots(ncols=2,nrows=1, figsize = (13, 5))\n\nsns.boxplot(x=df['education'],ax=ax2)\nax2.set_xlabel(\"education\", fontsize=15)\nsns.despine(ax=ax2)\nax1.tick_params(labelsize=10)\n\nsns.distplot(df['duration'], ax = ax1)\nsns.despine(ax = ax1)\nprint(\"Max : {} and Min duration  : {} \" .format(max(df['duration']), min(df['duration'])))\n\n# Setting value of duration\ndf.loc[df['duration']<120 , 'duration'] = 1 \ndf.loc[(df['duration'] > 119)&( df['duration'] <= 200) , 'duration'] =2\ndf.loc[(df['duration'] >200)&( df['duration'] <=350), 'duration']=3\ndf.loc[(df['duration'] >350)&( df['duration']<=550), 'duration']=4\ndf.loc[df['duration'] > 550, 'duration']=5\n\n# Target Attribute\ny=df.iloc[:,df.columns=='y']\nx=df.iloc[:,df.columns!='y']\n","7e6bf999":"df.head()","238e66f6":"\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3, random_state=0)\n\n# proccess of Standardize\nfrom sklearn.preprocessing import StandardScaler\nS_Scaler=StandardScaler()\nx_train=S_Scaler.fit_transform(x_train)\nx_test=S_Scaler.fit_transform(x_test)\n\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","93b0e4f1":"#grid={'n_neighbors': np.arange(1,20,1)}\nKnnClass=KNeighborsClassifier(n_neighbors=9)\n#KnnCV=GridSearchCV(KnnClas, grid, cv=10)\nKnnClass.fit(x_train,np.ravel(y_train,order='C'))\n#print(\"Best Parameters : {}\\nBest Score {} \".format(KnnCV.best_params_, KnnCV.best_score_) )","559c73f8":"#%% Xgboosting model \n\nXgboost=XGBClassifier(learning_rate=0.1,max_depth=4, n_estimators=100,verbosity=1)\nXgboost.fit(x_train,np.ravel(y_train,order='C'))\ny_pred=Xgboost.predict(x_test)\nprint(\"Test accuracy with XGBoos: \",accuracy_score(y_test,y_pred))","6a74379a":"\ngrid={'C':[0.0001,0.001,0.01,1] ,'gamma':['auto','scale'],'kernel':['rbf','linear','sigmoid'] , 'max_iter':[10,100]}\nSVCModel=SVC(probability=True)\nSVCGCV=GridSearchCV(SVCModel,grid,cv=10)\nSVCGCV.fit(x_train,np.ravel(y_train, order='C'))\nprint(\"Best Params {} and best score {}\".format(SVCGCV.best_params_, SVCGCV.best_score_))\n#print(SVCModel.score(x_test,np.ravel(y_test, order='C')))","be14d9cd":"RForest=RandomForestClassifier(n_estimators=20)\nRForest.fit(x_train,np.ravel(y_train,order='C'))\nRFPred=RForest.predict(x_test)\nprint(\"Random Forest Accuracy : \" ,accuracy_score(y_test,RFPred))\nprint(\"Cross Valudate Score : \", cross_val_score(RForest,x_test,y_test.values.ravel()))\nprint(\"Confisuon matrix :\", confusion_matrix(y_test,RFPred.ravel()))","a34ad701":"LRegression=LogisticRegression()\nLRegression.fit(x_train,y_train.values.ravel())\ny_predLR=LRegression.predict(x_test)\nprint(\"Logistic Regression Accuracy :\", accuracy_score(y_test,y_predLR.ravel()))\nprint(\"Cross val score :\", cross_val_score(LRegression,x_train,y_train.values.ravel(),cv=10,n_jobs=1,scoring='accuracy').mean())","900b3035":"\nBayesModel=GaussianNB()\nBayesModel.fit(x_train,y_train.values.ravel())\nbayesPred=BayesModel.predict(x_test)\nprint(\"Bayes Model Accuracy :\", accuracy_score(y_test,bayesPred.ravel()))\nprint(\"Cross val score :\", cross_val_score(BayesModel,x_train,y_train.values.ravel(),cv=10,n_jobs=2,scoring='accuracy').mean())\n","670299c3":"fig, ax_Array = plt.subplots(nrows = 1,  figsize = (8,6))\n\n# bayes roc\nprobs = BayesModel.predict_proba(x_test)\npreds = probs[:,1]\nfprbayes, tprxbayes, thresholdbayes = metrics.roc_curve(y_test, preds)\nroc_aucbayes = metrics.auc(fprbayes, tprxbayes)\n\n# LR roc\n\nprobs=LRegression.predict_proba(x_test)\npredicts=probs[:,1]\nfprLR,tprLR, thresholdLR=metrics.roc_curve(y_test,predicts)\nroc_aucLR=metrics.auc(fprLR,tprLR)\n\n# KNN roc\nprobs=KnnClass.predict_proba(x_test)\npredKnn=probs[:,1]\nfprKnn, tprKnn, thresholdKnn=metrics.roc_curve(y_test,predKnn)\nroc_aucknn=metrics.auc(fprKnn,tprKnn)\n\n\n# Random Forest \nprobs=RForest.predict_proba(x_test)\npred_RForest=probs[:,1]\nfprRF,tprRF,thresholfRF= metrics.roc_curve(y_test,pred_RForest)\nroc_aucRF=metrics.auc(fprRF,tprRF)\n\n # SVM model roc\n \nprob=SVCGCV.predict_proba(x_test)\npred_Svm=prob[:,1]\nfprSvm,tprsvm,tresholdSvm=metrics.roc_curve(y_test,pred_Svm)\nroc_aucSvm=metrics.auc(fprSvm,tprsvm) \n\n\n\n\nax_Array.plot(fprSvm,tprsvm, 'b', label='SMV Auc %0.2f' %roc_aucSvm, color=\"green\")\nax_Array.plot(fprRF,tprRF,'b', label=\"RF Auc %0.2f\"%roc_aucRF, color=\"blue\")\nax_Array.plot(fprKnn,tprKnn,'b', label=\"Knn Auc %0.2f\" %roc_aucknn, color=\"red\")\nax_Array.plot(fprLR,tprLR, 'b', label='LR Auc = %0.2f' % roc_aucLR, color='pink')\nax_Array.plot(fprbayes,tprxbayes,'b', label='Bayes Auc %0.2f' % roc_aucbayes, color=\"black\")\nax_Array.set_title('Receiver Operating Characteristic LR ',fontsize=10)\nax_Array.set_ylabel('True Positive Rate',fontsize=20)\nax_Array.set_xlabel('False Positive Rate',fontsize=15)\nax_Array.legend(loc = 'lower right', prop={'size': 10})\n\n\n\nplt.subplots_adjust(wspace=1)\n\n\n\n","3a0ee590":"## Random Forest Model And Confusion Matrix","c90b7893":"\nWe can use graphic roc to see the accuracy score of the applied methods. The Roc method shows us the accuracy shape with the infographic shape.","3aee2796":"## SVM model with best parameters","7c4380d2":"\n## Splitting the data","0a7e9ec1":"## Setting value of age","a824d9da":"## Naive Bayes Model","dba080b8":"## last state the data","e1830486":"## In this study, the preprocessing of the data, the application of the models with best parameters and the performance scores of the models were determined.","280f9cb8":"\n## KNN model implamentation with best parameters GridSourceCV","6bf695b8":"## **Best Params for SVM model** {'C': 1, 'gamma': 'scale', 'kernel': 'sigmoid', 'max_iter': 100} and best score : 0.8270261536011716\n\n\nThis parameters are detection with method of 'GridSourceCV'","a0a137ea":"## Logistic Regression Model","372f06d2":"## Seeing best model by use roc curve.","fa79801d":"### Xgboosting model"}}