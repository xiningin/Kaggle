{"cell_type":{"457b6969":"code","15fb68be":"code","8e0aa9ea":"code","d5944e15":"code","d63eb35b":"code","1a0a167a":"code","b3ab43f8":"code","9191fd5e":"code","125eeff5":"code","303ee447":"code","dc2507b9":"code","19b98293":"code","790259ff":"code","6a1167db":"code","bf9251b9":"code","b701a8d0":"code","b3532d91":"code","c924ddc3":"code","0524a3b0":"markdown","958f0582":"markdown","e6a53e77":"markdown","700fe26b":"markdown","6e2ddd76":"markdown","e10e54e2":"markdown","e36a6b6e":"markdown","ee67ff61":"markdown","d347bbc3":"markdown","f92a2ffa":"markdown"},"source":{"457b6969":"from zipfile import ZipFile  # working with zipped input\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules  # MBA\nfrom scipy import sparse  # sparse matrices\nimport numpy as np\nimport pandas as pd\nimport os","15fb68be":"# Loading & processing data\n\ndef preDot(text):\n    return text.rsplit('.', 1)[0]\n\nnp.random.seed(73)\npd.options.mode.chained_assignment = None\ndataDict = {}\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        with ZipFile(os.path.join(dirname, filename), 'r') as zipf:\n            unzipped_fn = preDot(filename)\n            with zipf.open(unzipped_fn) as f:\n                dataDict[preDot(unzipped_fn)] = pd.read_csv(f)\n\ntrain_orders = dataDict['orders'][dataDict['orders']['eval_set'] == 'train'].drop('eval_set', axis=1)\nprior_orders = dataDict['orders'][dataDict['orders']['eval_set'] == 'prior'].drop('eval_set', axis=1)\ntest_orders = dataDict['orders'][dataDict['orders']['eval_set'] == 'test'].drop('eval_set', axis=1)","8e0aa9ea":"# limiting and splitting the dataframe into three relatively equal parts for memory efficiency below\nsmall_train = dataDict['order_products__train'][['order_id', 'product_id']]\nsmall_train_split = (small_train[:461543], small_train[461543:461543*2-1], small_train[461543*2-1:])","d5944e15":"# heuristical prep of data\n# use of sparse matrices for memory efficency\n\npivots = []\nfor df in small_train_split:\n    pvt = ~(df.pivot(index='order_id', columns='product_id', values='product_id').isna())\n    pivots.append(pvt.astype(pd.SparseDtype(bool)))\ndel pvt\n\nproduct_cols = sorted(small_train.product_id.unique())","d63eb35b":"for i in range(len(pivots)):\n    # reindexing to add extra columns and standardize the format for vstack\n    # we sparse them again here b\/c otherwise we would end up having regular boolean columns\n    pivots[i] = pivots[i].reindex(columns=product_cols, fill_value=False).astype(pd.SparseDtype(bool))\n    pivots[i] = sparse.csr_matrix(pivots[i])\n# concat vertically\npivots = sparse.vstack(pivots)","1a0a167a":"# re-map and densify for algos\ntruth_table = pd.DataFrame(pivots.todense(), index=small_train.order_id.unique(), columns=product_cols)","b3ab43f8":"# takes less than a minute to execute\nfrequent_itemsets = fpgrowth(truth_table, min_support=5\/len(truth_table), use_colnames=True)","9191fd5e":"frequent_itemsets","125eeff5":"rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.8)","303ee447":"print(\"\u03bc number of consequents:\", rules['consequents'].apply(len).mean())\nrules","dc2507b9":"# selecting out rules that might potentially not be enhancing\nrules = rules[rules.lift > 1]","19b98293":"# a simplification of the table\nrules_ante_cons = rules[['antecedents', 'consequents']]","790259ff":"# creating customers' baskets\nbaskets = small_train.groupby('order_id')['product_id'].apply(frozenset)\nbaskets.name = \"basket\"  # antecedents","6a1167db":"recommendations = train_orders.join(baskets, on=\"order_id\")\nrecommendations[\"recommendations\"] = [frozenset() for _ in range(len(recommendations))]","bf9251b9":"# computationally-intensive; might require an optimization\nfor idx, antecedent in enumerate(rules_ante_cons[\"antecedents\"]):\n    lookup = antecedent <= recommendations.basket, \"recommendations\"\n    recommendations.loc[lookup] = recommendations.loc[lookup].apply(\n        frozenset.union,\n        args=(rules_ante_cons.loc[idx, \"consequents\"],)\n    )\n# recommendations = recommendations.rename(columns={\"antecedents\": \"basket\"})\n# this may be changed earlier\nrecommendations.loc[:, \"recommendations\"] = recommendations.recommendations - recommendations.basket","b701a8d0":"# non-empty recommendations\nnon_empty_recs = recommendations[recommendations.recommendations.apply(bool)]\nprint(\"1 out of approx.\", round(1\/(len(non_empty_recs) \/ len(recommendations))), \"transactions will result in a recommendation being suggested to a customer.\")\n# mapping codes to product names\ndef map_products(codes):\n    if isinstance(codes, pd.Series):\n        return codes.apply(map_products)\n    return frozenset(map(products.get, codes))\n\nproducts = dataDict[\"products\"]\nproducts = products.set_index(\"product_id\")[\"product_name\"].to_dict()\nnon_empty_recs.loc[:, [\"basket\", \"recommendations\"]] = non_empty_recs[[\"basket\", \"recommendations\"]].apply(map_products)\ndisplay(non_empty_recs)","b3532d91":"def mba_diagram(sample_basket, sample_recommendation):\n    import matplotlib.pyplot as plt\n\n    def get_text_box_coords(txt):\n        we = plt.Text.get_window_extent(txt, renderer=fig.canvas.get_renderer())\n        return ax.transAxes.inverted().transform(we)\n    def get_rightmost_vmid(box):\n        return box[1][0], (box[0][1] + box[1][1]) \/ 2\n\n    fig, ax = plt.subplots(figsize=(20,10))\n    title = ax.set_title(\"An illustration of a recommendation system for a sample customer basket\\n(basket \u2190 suggestion)\", fontsize=18)\n    ax.axis('off')\n    basket_txt = ax.text(.05, .95, sample_basket, ha='left', va='top', wrap=True,size=12,\n                  bbox=dict(boxstyle='round,pad=1', fc='w', ec='lightblue'))\n\n    basket_rightmost, basket_vmid = get_rightmost_vmid(get_text_box_coords(basket_txt))\n\n    arrow_txt = ax.text(\n        basket_rightmost*1.4, basket_vmid, \"Add\", ha=\"center\", va=\"center\", size=35,\n        bbox=dict(boxstyle=\"larrow,pad=0.6\", fc=\"lightgreen\", ec=\"g\", lw=2))\n    arrow_rightmost, arrow_vmid = get_rightmost_vmid(get_text_box_coords(arrow_txt))\n\n    recommendation_txt = ax.text(arrow_rightmost * 1.14, arrow_vmid, sample_recommendation, ha='left', va='top', wrap=True, fontsize=25,\n                  bbox=dict(boxstyle='round,pad=1', fc='w', ec='r'))\n    recommendation_txt_pos = recommendation_txt.get_position()\n    recommendation_txt.set_position((\n        recommendation_txt_pos[0],\n        recommendation_txt_pos[1] + (get_text_box_coords(recommendation_txt)[1][1]-get_text_box_coords(recommendation_txt)[0][1]) \/ 2\n    ))","c924ddc3":"sample_index = np.random.randint(len(non_empty_recs))\nsample_basket = \"\\n\".join(non_empty_recs.iloc[sample_index].loc[\"basket\"])\nsample_recommendation = \"\\n\".join(non_empty_recs.iloc[sample_index].loc[\"recommendations\"])\nmba_diagram(sample_basket, sample_recommendation)","0524a3b0":"# Setup","958f0582":"# Practical single-basket MBA Example","e6a53e77":"We need to check if antecedents of each rule are **a subset** (<=) of some client's basket, e.g.\n\n```recommendations.loc[frozenset({4605, 21903, 47626, 49683}) <= recommendations.recommendations, \"basket\"]```.","700fe26b":"# Recommendations","6e2ddd76":"# About MBA with Association Rules\n\nMarket Basket Analysis with Association Rules is a technique that enables one to find sets of items that are often found together within a customer's basket (a transaction) across all orders.\nIt is primarily used in business (albeit recently less so) to create, augument or improve:\n* bundles of products\n* cashier suggestions for in-store clients after a completed product scan but before payment\n* in-store product placement","e10e54e2":"# Generating Association Rules","e36a6b6e":"Setting up rules from item sets with 80% confidence.","ee67ff61":"It essentially means removing infrequent itemsets (i.e., those below the minimum support specfied at 5 occurences in the transactional table).\n\nQuestions to keep in mind while mining rules:\n* how to determine the minimum support value?\n* how many item sets \/ rules should be obtained?\n* what metric to pick for rules? what should be its threshold value?\n* should one focus on account for the the base popularity of antecendents (*confidence*) or should consequents be involved as well (*lift*)?\n\n[Here](https:\/\/paginas.fe.up.pt\/~ec\/files_0506\/slides\/04_AssociationRules.pdf) one can find a short summary of how association rule mining works.","d347bbc3":"# Transactional Table Prep","f92a2ffa":"# Generating Frequent Item Sets"}}