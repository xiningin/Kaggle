{"cell_type":{"e9880bed":"code","3884bf4b":"code","917993a0":"code","6addaacd":"code","b02a9d63":"code","78492d1a":"code","37a0471a":"code","a689641f":"code","9bd1ed9a":"code","0c7d7cf5":"code","7a257f62":"code","f077ea0d":"code","d240d001":"code","bf994b30":"code","4e494689":"code","553d6d18":"code","469828e7":"code","30c68495":"code","e247c551":"code","3bd1ed78":"code","55ed8104":"code","b1a011e7":"code","32ab50b1":"code","dba8015c":"code","07bebfc5":"code","e0ad0555":"code","947c04de":"code","db464de2":"code","d4b491a6":"code","71236440":"code","29c4a833":"code","3bccbb25":"code","d338cf23":"code","501d1350":"code","51528a1a":"code","0e963f3c":"code","bf5ed633":"code","2f0765ab":"code","a7db349a":"code","31a1af05":"code","a1977c57":"code","c4a3e23a":"code","399d5080":"code","2199f221":"code","45e6291f":"code","b094e3cc":"code","f9845cd5":"code","93ac6edf":"code","11abdca6":"code","79076b97":"code","d093ab9b":"code","04eb5676":"code","8a7bcd35":"code","fab82b88":"code","68177aca":"code","e62f3087":"code","84d5db3a":"code","423ab5ce":"code","5f5ad447":"code","4ecb242b":"code","592ba2dd":"markdown","30e36249":"markdown","a3d0c4d5":"markdown","9390302a":"markdown","1d6479a1":"markdown","ba43c7fb":"markdown","5a433ab0":"markdown","7eb443f4":"markdown","59a474a2":"markdown","47bd0458":"markdown","b5dbd442":"markdown","9ab2ee39":"markdown","6d9052bc":"markdown","19530cd7":"markdown","552b3144":"markdown","700f3359":"markdown","87a488ab":"markdown","a9e8cca3":"markdown","8f8522f7":"markdown","ad24f8ca":"markdown","4f01668e":"markdown","ade89491":"markdown","cc809643":"markdown","b68d3d42":"markdown","337a9502":"markdown","120151fd":"markdown","b244daf9":"markdown","9737a406":"markdown","4f8e0105":"markdown","c98c310b":"markdown","62219def":"markdown","b4dcb26d":"markdown","ead5f998":"markdown","d2b1a77d":"markdown","73d44fec":"markdown","d19f54fc":"markdown","d27ada5b":"markdown","d65e53b7":"markdown","48cf96e4":"markdown","e4e7fa94":"markdown","8f21b169":"markdown","e7c4cb70":"markdown","add0ead3":"markdown","f6f62e4d":"markdown","6e42b3f0":"markdown","c2423b75":"markdown","777c3517":"markdown","c8a03ead":"markdown","84725ae7":"markdown"},"source":{"e9880bed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3884bf4b":"# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","917993a0":"test_df = pd.read_csv(\"..\/input\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")","6addaacd":"train_df.info()","b02a9d63":"train_df.describe()","78492d1a":"train_df.head(8)","37a0471a":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","a689641f":"train_df.columns.values","9bd1ed9a":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Kad\u0131n')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Erkek')","0c7d7cf5":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","7a257f62":"sns.barplot(x='Pclass', y='Survived', data=train_df)","f077ea0d":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","d240d001":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain_df['not_alone'].value_counts()","bf994b30":"axes = sns.factorplot('relatives','Survived', \n                      data=train_df, aspect = 2.5, )","4e494689":"train_df = train_df.drop(['PassengerId'], axis=1)","553d6d18":"dataset.Cabin","469828e7":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","30c68495":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\ntrain_df[\"Age\"].isnull().sum()","e247c551":"train_df['Embarked'].describe()","3bd1ed78":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","55ed8104":"train_df.info()","b1a011e7":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","32ab50b1":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","dba8015c":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","07bebfc5":"train_df['Ticket'].describe()","e0ad0555":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","947c04de":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","db464de2":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed train_df['Age'].value_counts()","d4b491a6":"train_df['Age'].value_counts()","71236440":"train_df.head(10)","29c4a833":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","3bccbb25":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","d338cf23":"for dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\ntrain_df.head(10)","501d1350":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()","51528a1a":"sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)","0e963f3c":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","bf5ed633":"gender_submis=pd.read_csv('..\/input\/gender_submission.csv')","2f0765ab":"gender_submis.Survived=Y_prediction","a7db349a":"gender_submis.to_csv('submission.csv',index=False)\nprint('ok')","31a1af05":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)","a1977c57":"# KNN \nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)","c4a3e23a":"gaussian = GaussianNB() \ngaussian.fit(X_train, Y_train)  \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)","399d5080":"perceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)","2199f221":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)","45e6291f":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)","b094e3cc":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","f9845cd5":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","93ac6edf":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","11abdca6":"importances.plot.bar()","79076b97":"train_df  = train_df.drop(\"not_alone\", axis=1)\ntest_df  = test_df.drop(\"not_alone\", axis=1)\n\ntrain_df  = train_df.drop(\"Parch\", axis=1)\ntest_df  = test_df.drop(\"Parch\", axis=1)","d093ab9b":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","04eb5676":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\nclf.fit(X_train, Y_train)\nclf.best_params_","8a7bcd35":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"entropy\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 35,   \n                                       n_estimators=400, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","fab82b88":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","68177aca":"Y_prediction.size","e62f3087":"!ls ..\/input","84d5db3a":"gender_submis=pd.read_csv('..\/input\/gender_submission.csv')","423ab5ce":"gender_submis.Survived=Y_prediction","5f5ad447":"gender_submis.to_csv('submission.csv',index=False)\n","4ecb242b":"!ls ..\/input","592ba2dd":"# 'Fare' kolonunu grupland\u0131rma","30e36249":"# Karar A\u011fac\u0131","a3d0c4d5":"'Embarked' kolonundaki S,C,Q de\u011ferlerini numerik veri ile ifade edelim.","9390302a":"Ki\u015fi Ba\u015f\u0131na \u00dccret","1d6479a1":"## \u00d6zelli\u011fin \u00d6nemi\nRandom Forest'in bir di\u011fer b\u00fcy\u00fck kalitesi, her bir \u00f6zelli\u011fin g\u00f6receli \u00f6nemini \u00f6l\u00e7meyi \u00e7ok kolayla\u015ft\u0131rmas\u0131d\u0131r. Sklearn, bu \u00f6zelli\u011fi kullanan a\u011fa\u00e7 d\u00fc\u011f\u00fcm d\u00fc\u011f\u00fcmlerinin ne kadar\u0131na bak\u0131larak, ormandaki t\u00fcm a\u011fa\u00e7larda kirlili\u011fi ortalama olarak azaltarak \u00f6zelliklerin \u00f6nemini \u00f6l\u00e7er. E\u011fitimden sonra her \u00f6zellik i\u00e7in otomatik olarak bu puan\u0131 otomatik olarak hesaplar ve sonu\u00e7lar\u0131 t\u00fcm \u00f6zellik \u00f6nemlerinin toplam\u0131n\u0131n 1'e e\u015fit olmas\u0131 i\u00e7in \u00f6l\u00e7eklendirir","ba43c7fb":"\u015eimdi hayatta kalan-kalmayanlar\u0131 kad\u0131n ve erkek olarak grafiksel g\u00f6relim","5a433ab0":"not_alone ve Parch, Random Forest s\u0131n\u0131fland\u0131r\u0131c\u0131lar\u0131 tahmin s\u00fcrecimizde \u00f6nemli bir rol oynam\u0131yor. Bu y\u00fczden onlar\u0131 veri setinden ataca\u011f\u0131m ve s\u0131n\u0131fland\u0131r\u0131c\u0131y\u0131 tekrar e\u011fitece\u011fim. Daha fazla veya daha az \u00f6zelli\u011fi de kald\u0131rabiliriz, ancak bunun modelimiz \u00fczerindeki \u00f6zellik etkisinin daha ayr\u0131nt\u0131l\u0131 bir \u015fekilde ara\u015ft\u0131r\u0131lmas\u0131 gerekir. Ama bence sadece Alone ve Parch\u2019lar\u0131 kald\u0131rmak iyi.","7eb443f4":"681 farkl\u0131 de\u011fer ald\u0131\u011f\u0131ndan bunu silelim.","59a474a2":"Toplam 12 kolonumuz var ve bunlardan Survived bizim bulmam\u0131z gereken de\u011fer Y_pred de\u011feri. \n\n'PassengerId', 'Ticket' ve 'Name' \u00f6zellikleri hayatta kalmayla pek alakal\u0131 g\u00f6r\u00fcnm\u00fcyor. Ama di\u011ferleri anlaml\u0131.","47bd0458":"Bilet numaras\u0131n\u0131n alakas\u0131 olmad\u0131\u011f\u0131 i\u00e7in onu \u00e7\u0131kar\u0131yoruz.","b5dbd442":"### Missing Data Eksik verileri d\u00fczenleme\n","9ab2ee39":"# Random Forest","6d9052bc":"Ya\u015f * S\u0131n\u0131f","19530cd7":"Bu \u00f6ncekinden \u00e7ok daha ger\u00e7ek\u00e7i g\u00f6r\u00fcn\u00fcyor. Modelimiz,% 4'l\u00fck bir standart sapma ile% 82'lik bir ortalama do\u011frulu\u011fa sahiptir. Standart sapma bize tahminlerin ne kadar kesin oldu\u011funu g\u00f6steriyor.\n\nBu, bizim durumumuzda modelimizin do\u011frulu\u011funun % +\u200a -% 4 aras\u0131nda de\u011fi\u015febilece\u011fi anlam\u0131na gelir .\n\nDo\u011fruluk oran\u0131n\u0131n hala \u00e7ok iyi oldu\u011funu d\u00fc\u015f\u00fcn\u00fcyorum ve rastgele orman kullan\u0131m\u0131 kolay bir model oldu\u011fundan, bir sonraki b\u00f6l\u00fcmde performans\u0131n\u0131 daha da artt\u0131rmaya \u00e7al\u0131\u015faca\u011f\u0131z","552b3144":"# Logistik Regresyon","700f3359":"Burada, akraba say\u0131s\u0131 1 ile 3 aras\u0131  hayatta kalma olas\u0131l\u0131\u011f\u0131n\u0131n y\u00fcksek oldu\u011funu, ancak 1'den az veya 3'ten fazla durumda (6 akrabal\u0131 durum hari\u00e7) daha d\u00fc\u015f\u00fck oldu\u011funu g\u00f6rebiliyoruz","87a488ab":"# 8 Makine \u00d6\u011frenme modelinin kar\u015f\u0131la\u015ft\u0131r\u0131lmas\u0131","a9e8cca3":"Tablodan da g\u00f6r\u00fclece\u011fi \u00fczere baz\u0131 veriler NAN de\u011fere sahip. Hangi s\u00fctunda ne kadar NAN de\u011fer var g\u00f6relim","8f8522f7":" ## SibSp ve Parch:\n\nSibSp ve Parch, bir insan\u0131n Titanik'teki toplam akraba say\u0131s\u0131n\u0131 g\u00f6steren bir \u00f6zellik. 'relatives' ve 'not_alone' ad\u0131nda yeni bir \u00f6zellik olu\u015fturup hayatta kalmayla ili\u015fkilerini \u00e7\u0131karal\u0131m","ad24f8ca":"Embarked \u00f6zelli\u011fi, kolayca doldurulabilen yaln\u0131zca 2 eksik de\u011fere sahiptir. 177 eksik de\u011feri olan 'Age' \u00f6zelli\u011fi ile u\u011fra\u015fmak \u00e7ok daha zor olacak. 'Kabin' \u00f6zelli\u011fi daha fazla ara\u015ft\u0131rmaya ihtiya\u00e7 duyuyor, ancak% 77'si eksik oldu\u011fu i\u00e7in veri setinden d\u00fc\u015f\u00fcrmek isteyebiliriz gibi g\u00f6r\u00fcn\u00fcyor.","4f01668e":"Art\u0131k eksik verimiz kalmad\u0131.","ade89491":"## Kolon adlar\u0131 ve a\u00e7\u0131klamalar\u0131","cc809643":"## Train verilerinin bilgilerini alal\u0131m","b68d3d42":"# Stokastik Grade \u0130ni\u015fi (SGD):","337a9502":"# Random Forest i\u00e7in Hiper Parametre ayarlar\u0131\n","120151fd":"Yukar\u0131daki \u00f6zet tablodan bir\u00e7ok yorum \u00e7\u0131karabiliriz. \u00d6rne\u011fin yolcular\u0131n %38'i kurtulmu\u015f. Yolcular\u0131n ya\u015f aral\u0131\u011f\u0131 4 ayl\u0131k ile 80 .... gibi","b244daf9":"# Lineer Destek Vekt\u00f6r Makinesi","9737a406":"Kabin numaralar\u0131 harflendirilmi\u015f. Bunlar\u0131 kategorik olarak grupland\u0131r\u0131p bo\u015f de\u011fer olanlar\u0131 0 ile kategorize edelim","4f8e0105":"## Train ve Test dosyalar\u0131m\u0131z\u0131 alal\u0131m","c98c310b":"## Kategorik i\u015flemler\nFare kolonunda  kategorik d\u00f6n\u00fc\u015f\u00fcmleri yapmak i\u00e7in \u00f6ncelikle float'da int e d\u00f6n\u00fc\u015f\u00fcm yapmal\u0131y\u0131z.","62219def":"Title kolonundan \u00fcnvanlar\u0131 alal\u0131m.","b4dcb26d":"# Perseptron","ead5f998":"## Gerekli K\u00fct\u00fcphaneler","d2b1a77d":"# Kategorik D\u00f6n\u00fc\u015f\u00fcmler\n## Ya\u015f \n\u015eimdi 'Age' \u00f6zelli\u011fini de\u011fi\u015ftirmemiz gerekiyor. \u0130lk \u00f6nce floattan tamsay\u0131ya d\u00f6n\u00fc\u015ft\u00fcrece\u011fiz. Sonra her ya\u015f grubunu bir gruba ay\u0131rarak yeni 'AgeGroup' de\u011fi\u015fkenini yarataca\u011f\u0131z. \u00d6rne\u011fin, verilerinizin% 80'inin grup 1'e d\u00fc\u015fmesini istemedi\u011finizden, bu gruplar\u0131 nas\u0131l olu\u015fturdu\u011funuza dikkat etmenizin \u00f6nemli oldu\u011funu unutmay\u0131n.","73d44fec":"## Ya\u015f:\nStandart sapma ve ortalama ya\u015f de\u011ferine g\u00f6re hesaplanan rasgele say\u0131lar i\u00e7eren bir dizi olu\u015fturaca\u011f\u0131z.","d19f54fc":"## S\u0131n\u0131f \u0131n hayatta kalma ile ili\u015fkisi","d27ada5b":"G\u00f6rd\u00fc\u011f\u00fcm\u00fcz gibi, Random Forest s\u0131n\u0131fland\u0131r\u0131c\u0131s\u0131 en iyi sonu\u00e7 veren s\u0131n\u0131fland\u0131r\u0131c\u0131. Fakat ilk \u00f6nce, K-Fold Cross Validation kulland\u0131\u011f\u0131m\u0131z zaman Random Forest'in nas\u0131l \u00e7al\u0131\u015ft\u0131\u011f\u0131n\u0131 kontrol edelim.  K-Fold Cross Validation, e\u011fitim verilerini rastgele olarak katlama ad\u0131 verilen K altk\u00fcmelerine ay\u0131r\u0131r . Diyelim ki verilerimizi 4 kata b\u00f6ld\u00fck (K = 4). Rastgele orman modelimiz her seferinde de\u011ferlendirme i\u00e7in farkl\u0131 bir kat kullan\u0131larak 4 kez e\u011fitilecek ve de\u011ferlendirilecek ve kalan 3 kat \u00fczerinde e\u011fitilecektir.\n\nA\u015fa\u011f\u0131daki kodRandom Forest modelimizde 10 kat (K = 10) kullanarak  K-Fold Cross Validation ger\u00e7ekle\u015ftirir. Bu nedenle, 10 farkl\u0131 skorla bir dizi \u00e7\u0131kar\u0131r","d65e53b7":"# Gauss Naive Bayes","48cf96e4":"# Makine \u00f6\u011frenme\n## veriyi X_train ve Y_train olarak ay\u0131ral\u0131m","e4e7fa94":"## Binilen Liman 'Embarked' ve Bilet s\u0131n\u0131f\u0131n\u0131n \u0130ncelenmesi","8f21b169":"Cinsiyet bilgisini numerik 1-0 de\u011ferleri ile ifade edelim.","e7c4cb70":"# Yeni \u00d6zellikler Yaratmak","add0ead3":"Embarked'\u0131n - binilen liman- cinsiyete ba\u011fl\u0131 olarak, hayatta kalma ile ili\u015fkisi var  gibi g\u00f6r\u00fcn\u00fcyor.\n\nQ liman\u0131ndaki ve S liman\u0131ndaki kad\u0131nlar\u0131n hayatta kalma \u015fans\u0131 daha y\u00fcksek. C liman\u0131nda ise erkeklerin hayatta kalma olas\u0131l\u0131klar\u0131 y\u00fcksek., ancak Erkeklerin Q veya S limanlar\u0131nda ise d\u00fc\u015f\u00fck olas\u0131l\u0131klar\u0131 var.\n\nPclass da hayatta kalma ile ili\u015fkili gibi g\u00f6r\u00fcn\u00fcyor. A\u015fa\u011f\u0131da  Pclass hayatta kalma ili\u015fkisi net g\u00f6r\u00fcl\u00fcyor.","f6f62e4d":"# KNN","6e42b3f0":"Erkeklerin 18 ila 30 ya\u015flar\u0131 aras\u0131nda hayatta kalma olas\u0131l\u0131klar\u0131n\u0131n y\u00fcksek oldu\u011funu g\u00f6rebilirsiniz, bu ayn\u0131 zamanda kad\u0131nlar i\u00e7in de ge\u00e7erli ama bire bir ayn\u0131 de\u011fil.. Kad\u0131nlar i\u00e7in hayatta kalma \u015fans\u0131 14 ile 40 aras\u0131ndad\u0131r.\n\nErkekler i\u00e7in ya\u015fama olas\u0131l\u0131\u011f\u0131 5 ile 18 ya\u015flar\u0131 aras\u0131nda \u00e7ok d\u00fc\u015f\u00fckt\u00fcr, ancak bu kad\u0131nlar i\u00e7in do\u011fru de\u011fildir. Unutulmamas\u0131 gereken bir ba\u015fka \u015fey de bebeklerin hayatta kalma ihtimalinin biraz daha y\u00fcksek oldu\u011fudur.","c2423b75":"Embarked sadece 2 NAN de\u011fere sahip ve bunlar\u0131 da en \u00e7ok olan de\u011fer ile dolduraca\u011f\u0131z.","777c3517":"## Ticket bilgisi","c8a03ead":"Burada a\u00e7\u0131k\u00e7a g\u00f6r\u00fcyoruz ki, Pclass \u00f6zellikle de 1. s\u0131n\u0131ftaysa hayatta kalma \u015fans\u0131 olan insanlara katk\u0131da bulunuyor. A\u015fa\u011f\u0131daki grafik 1. s\u0131n\u0131f hakk\u0131ndaki varsay\u0131m\u0131m\u0131z\u0131 do\u011fruluyor, fakat ayn\u0131 zamanda 3. s\u0131n\u0131ftaki bir insan\u0131n hayatta kalmama ihtimalinin y\u00fcksek oldu\u011funu da tespit edebiliriz","84725ae7":"survival:    Survival \nPassengerId: Unique Id of a passenger. \n\nPclass:    Ticket class     \nsex:    Sex     \nAge:    Age in years     \nsibsp:    # of siblings \/ spouses aboard the Titanic     \nparch:    # of parents \/ children aboard the Titanic     \nticket:    Ticket number     \nfare:    Passenger fare     \ncabin:    Cabin number     \nembarked:    Port of Embarkation"}}