{"cell_type":{"fc286060":"code","d7bd62bb":"code","4b8783c7":"code","8653296a":"code","8784d1a1":"code","f1ca280c":"code","6eb6b26e":"code","03b6b9a5":"code","ce4f47b2":"code","278946b1":"code","beb7ea5b":"code","856da2d6":"code","1a86bdd7":"code","1143a55f":"code","14680048":"markdown","5881b904":"markdown","6946bee3":"markdown","02e8d5a6":"markdown","d1c10070":"markdown","7c84b0e6":"markdown","787a16d4":"markdown","eddfb905":"markdown","c2b1f674":"markdown"},"source":{"fc286060":"import pandas as pd\nimport numpy as np\nimport os\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical,plot_model\n\nfrom keras.models import Input,Model,Sequential\nfrom keras.layers import LSTM,Embedding,Dropout,Activation,Reshape,Dense,GRU,Add,Flatten,concatenate\n\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\n","d7bd62bb":"# DATASET_PATH = \"\/home\/abhinav\/fake_news_challenge\/fake_news_challenge\/dataset\"\nDATASET_PATH = \"..\/input\/fake-news-challenge\/\"\n\ntrain_bodies = pd.read_csv(os.path.join(DATASET_PATH,'train_bodies.csv'))\n# train_bodies.head()\ntrain_stance = pd.read_csv(os.path.join(DATASET_PATH,'train_stances.csv'))","4b8783c7":"#Code to combine the two csv file{train_bodies.csv,train_stances.csv} into data_combined.csv file\nfrom tqdm.notebook import tqdm\ncount=0\nfor i in tqdm(range(train_stance.shape[0])):\n    for j in range(train_bodies.shape[0]):\n        if train_bodies.loc[j,'Body ID']==train_stance.loc[i,'Body ID']:\n            train_stance.loc[i,'articleBody'] = train_bodies.loc[j,'articleBody']\n\n\ntrain_stance.to_csv('data_combined.csv',index=False)","8653296a":"data = pd.read_csv('data_combined.csv')#generated from Fake News stanford.ipynb","8784d1a1":"data.head()","f1ca280c":"data['stance_cat'] = data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\ndata['Stance'].value_counts()","6eb6b26e":"corpus = np.r_[data['Headline'].values,data['articleBody'].values]\nprint(49972*2)\nprint(len(corpus)) # first 49972 contains the Headline and next 49972 contains the articleBody\n\nvocabulary = []\nfor sentence in corpus:\n    vocabulary.extend(sentence.split(' '))\n\nvocabulary = list(set(vocabulary))\nvocab_length = len(vocabulary)\nprint(\"Vocabulary Length is {0}\".format(vocab_length))\n","03b6b9a5":"max_features = 5000\nmax_nb_words = 24000\nEMBEDDING_DIM = 50\nMAX_SEQUENCE_LENGTH_HEADLINE = 64\nMAX_SEQUENCE_LENGTH_BODY = 64","ce4f47b2":"\nencoded_docs_headline = [one_hot(sentence,vocab_length) for sentence in data.loc[:,'Headline'].tolist()]\npadded_docs_headline = pad_sequences(encoded_docs_headline,MAX_SEQUENCE_LENGTH_HEADLINE,padding='post')\n\nencoded_docs_body = [one_hot(sentence,vocab_length) for sentence in data.loc[:,'articleBody'].tolist()]\npadded_docs_body = pad_sequences(encoded_docs_body,MAX_SEQUENCE_LENGTH_BODY,padding='post')\n\n\nlabels = to_categorical(data.loc[:,'stance_cat'])\n","278946b1":"padded_docs_headline_train = padded_docs_headline[:int(len(padded_docs_headline)*0.8),:]\npadded_docs_headline_test = padded_docs_headline[int(len(padded_docs_headline)*0.8):,:]\n\npadded_docs_body_train = padded_docs_body[:int(len(padded_docs_body)*0.8),:]\npadded_docs_body_test = padded_docs_body[int(len(padded_docs_body)*0.8):,:]\n\nlabels_train = labels[:int(len(labels)*0.8),:]\nlabels_test = labels[int(len(labels)*0.8):,:]\n","beb7ea5b":"input_headline = Input(shape=[64],name='input_headline')\nembedding_headline = Embedding(vocab_length,50,input_length = MAX_SEQUENCE_LENGTH_HEADLINE)(input_headline)\n# dense_headline = Dense(16,activation='relu')(embedding_headline)\n\ninput_body = Input(shape=[64],name='input_body')\nembedding_body = Embedding(vocab_length,50,input_length = MAX_SEQUENCE_LENGTH_BODY)(input_body)\n# dense_body = Dense(16,activation='relu')(embedding_body)\n\naddition_layer = concatenate([embedding_headline,embedding_body])\nlstm = LSTM(units=64)(addition_layer)\n# drop = Dropout(0.25)(lstm)\n\n# flatten = Flatten()(addition_layer)\noutput = Dense(4,activation='sigmoid')(lstm)\n\nmodel_combined = Model(inputs=[input_headline,input_body],outputs=output)\n\nmodel_combined.compile(optimizer = 'adam',loss ='categorical_crossentropy',metrics = ['accuracy'])\n","856da2d6":"model_combined.summary()","1a86bdd7":"plot_model(model_combined, to_file='model_one_hot.png', show_shapes=True, show_layer_names=True)\n","1143a55f":"model_combined.fit([padded_docs_headline_train,padded_docs_body_train],labels_train,epochs=15,verbose=1,validation_data=([padded_docs_headline_test,padded_docs_body_test],labels_test))","14680048":"# Model Training","5881b904":"# Defining Model Architecture","6946bee3":"# Importing Libraries","02e8d5a6":"# Model Architecture","d1c10070":"# Dataset Preparation","7c84b0e6":"We are given a dataset consisting of two csv files train_bodies.csv which contains the set of news articles bodies,while train-stances.csv resembles the articles for each of these bodies being identified using the body id.\n\nAfter training from these samples we need to detect whether the given headline agrees,disagrees,discusses,unrelated with the body id","787a16d4":"# Problem Statement","eddfb905":"# BASELINE - ONE HOT ENCODING","c2b1f674":"**Please upvote the notebook if you find it useful**"}}