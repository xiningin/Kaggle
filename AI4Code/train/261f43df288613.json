{"cell_type":{"30e64458":"code","b2542d3f":"code","97cca84e":"code","2547d394":"code","7edafda5":"code","bf527529":"code","d53ebb7d":"code","dd325afe":"code","a85e68b6":"code","71e333f8":"code","48928a3d":"code","b5dc323d":"code","4511ad5e":"code","8a840aa2":"code","05aa8f42":"code","2cfc4246":"code","afc92fc6":"code","9db3cf54":"code","7b312fc6":"code","2aa74586":"code","5862abfe":"code","740a2c7a":"code","070ac1ed":"code","8120b737":"code","52b9cdd7":"code","e32644aa":"code","af02d7ca":"code","916761fb":"code","94ac9b74":"code","f752ca45":"code","cbda5085":"code","cbddd264":"code","3a896278":"code","99cb324e":"code","f1d4bf5d":"code","44fad489":"code","2114731c":"markdown","0b8a7d71":"markdown","a4c32c5e":"markdown","7299e0bb":"markdown","9a878bc2":"markdown","e570b137":"markdown","d7389322":"markdown","1f671e29":"markdown","2ef8dc4f":"markdown","1efc2060":"markdown","e1d6dc7d":"markdown","d9570f8a":"markdown","df9513f0":"markdown","0b388d4d":"markdown","92096009":"markdown","cccfc9e2":"markdown","9af75170":"markdown","cf87e0f3":"markdown","7e5dcb4b":"markdown","197ed643":"markdown","87d03ec4":"markdown","00f2cb5f":"markdown","d4eb90b7":"markdown","687ea328":"markdown","0a43eed8":"markdown","c1c72a95":"markdown","1a4df7be":"markdown","6ed2743f":"markdown","669e517c":"markdown","dca791df":"markdown","1695a052":"markdown","39f6dbc2":"markdown"},"source":{"30e64458":"import numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom tqdm import tqdm\nprint(\"Setup Done\")","b2542d3f":"raw_train_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\nraw_test_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\n#\u0111\u1ecdc file d\u1eef li\u1ec7u\nraw_train_data.head()","97cca84e":"raw_test_data.head()","2547d394":"feature_name = ['question_text','target']\n#l\u1ea5y d\u1eef li\u1ec7u \u1edf 2 c\u1ed9t question_text v\u00e0 target\ntrain_data = raw_train_data[feature_name]\n#t\u1ea1o ra m\u1ea3ng m\u1edbi c\u00f3 2 c\u1ed9t d\u1eef li\u1ec7u question_text v\u00e0 target\nprint(train_data.head())\n#in k\u1ebft qu\u1ea3 ra theo \u0111\u1ecbnh d\u1ea1ng","7edafda5":"### chia ra th\u00e0nh 2 t\u1eadp,t\u1eadp train v\u00e0 t\u1eadp validation\ntrain,val=train_test_split(train_data,test_size=0.2,stratify=train_data.target,random_state=123)\n#Random 80% l\u00e0 m\u1ea3ng train v\u00e0 20% l\u00e0 m\u1ea3ng validation\nprint(\"Shape of the Training set :\",train.shape)\n#in ra k\u00edch th\u1ee9c c\u1ee7a m\u1ea3ng d\u1eef li\u1ec7u Train\nprint(\"Shape of the Validation set :\",val.shape)\n#in ra k\u00edch th\u1ee9c c\u1ee7a m\u1ea3ng d\u1eef li\u1ec7u Validation\nprint(train.head())\n#in d\u1eef li\u1ec7u ra theo \u0111\u1ecbnh d\u1ea1ng c\u1ee7a m\u1ea3ng Train\nprint(train.question_text[224356])\n#in d\u1eef li\u1ec7u c\u1ee7a question c\u00f3 id l\u00e0 224356","bf527529":"#print(train.columns)\n#print(train.question_text[602217])\n#for i in range(0,10,1):\n#    randIndex = random.randrange(0,train.question_text.size,1)\n#   print(train.question_text[randIndex])","d53ebb7d":" #s\u1eafp x\u1ebfp l\u1ea1i c\u00e1c c\u1ed9t ch\u1ec9 s\u1ed1\ntrain = train.reset_index()\nval = val.reset_index()\n#in ra 10 gtri test th\u1eed\nprint(train.head(10))","dd325afe":"#Lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb vi\u1ebft t\u1eaft \n\n#T\u1ea1o m\u1ed9t directory c\u00f3 key l\u00e0 c\u00e1c t\u1eeb vi\u1ebft t\u1eaft v\u00e0 value l\u00e0 c\u00e1c t\u1eeb kh\u00f4ng vi\u1ebft t\u1eaft c\u1ee7a key\n#M\u1ee5c \u0111\u00edch \u0111\u1ec3 thay th\u1ebf c\u00e1c t\u1eeb vi\u1ebft t\u1eaft b\u1eb1ng c\u00e1c t\u1eeb chu\u1ea9n,\u0111\u1ec3 chu\u1ea9n b\u1ecb t\u1ea1o danh s\u00e1ch t\u1eeb v\u1ef1ng\ncontractions={\"I'm\": 'I am',\n \"I'm'a\": 'I am about to',\n \"I'm'o\": 'I am going to',\n \"I've\": 'I have',\n \"I'll\": 'I will',\n \"I'll've\": 'I will have',\n \"I'd\": 'I would',\n \"I'd've\": 'I would have',\n 'Whatcha': 'What are you',\n \"amn't\": 'am not',\n \"ain't\": 'are not',\n \"aren't\": 'are not',\n \"'cause\": 'because',\n \"can't\": 'can not',\n \"can't've\": 'can not have',\n \"could've\": 'could have',\n \"couldn't\": 'could not',\n \"couldn't've\": 'could not have',\n \"daren't\": 'dare not',\n \"daresn't\": 'dare not',\n \"dasn't\": 'dare not',\n \"didn't\": 'did not',\n 'didn\u2019t': 'did not',\n \"don't\": 'do not',\n 'don\u2019t': 'do not',\n \"doesn't\": 'does not',\n \"e'er\": 'ever',\n \"everyone's\": 'everyone is',\n 'finna': 'fixing to',\n 'gimme': 'give me',\n \"gon't\": 'go not',\n 'gonna': 'going to',\n 'gotta': 'got to',\n \"hadn't\": 'had not',\n \"hadn't've\": 'had not have',\n \"hasn't\": 'has not',\n \"haven't\": 'have not',\n \"he've\": 'he have',\n \"he's\": 'he is',\n \"he'll\": 'he will',\n \"he'll've\": 'he will have',\n \"he'd\": 'he would',\n \"he'd've\": 'he would have',\n \"here's\": 'here is',\n \"how're\": 'how are',\n \"how'd\": 'how did',\n \"how'd'y\": 'how do you',\n \"how's\": 'how is',\n \"how'll\": 'how will',\n \"isn't\": 'is not',\n \"it's\": 'it is',\n \"'tis\": 'it is',\n \"'twas\": 'it was',\n \"it'll\": 'it will',\n \"it'll've\": 'it will have',\n \"it'd\": 'it would',\n \"it'd've\": 'it would have',\n 'kinda': 'kind of',\n \"let's\": 'let us',\n 'luv': 'love',\n \"ma'am\": 'madam',\n \"may've\": 'may have',\n \"mayn't\": 'may not',\n \"might've\": 'might have',\n \"mightn't\": 'might not',\n \"mightn't've\": 'might not have',\n \"must've\": 'must have',\n \"mustn't\": 'must not',\n \"mustn't've\": 'must not have',\n \"needn't\": 'need not',\n \"needn't've\": 'need not have',\n \"ne'er\": 'never',\n \"o'\": 'of',\n \"o'clock\": 'of the clock',\n \"ol'\": 'old',\n \"oughtn't\": 'ought not',\n \"oughtn't've\": 'ought not have',\n \"o'er\": 'over',\n \"shan't\": 'shall not',\n \"sha'n't\": 'shall not',\n \"shalln't\": 'shall not',\n \"shan't've\": 'shall not have',\n \"she's\": 'she is',\n \"she'll\": 'she will',\n \"she'd\": 'she would',\n \"she'd've\": 'she would have',\n \"should've\": 'should have',\n \"shouldn't\": 'should not',\n \"shouldn't've\": 'should not have',\n \"so've\": 'so have',\n \"so's\": 'so is',\n \"somebody's\": 'somebody is',\n \"someone's\": 'someone is',\n \"something's\": 'something is',\n 'sux': 'sucks',\n \"that're\": 'that are',\n \"that's\": 'that is',\n \"that'll\": 'that will',\n \"that'd\": 'that would',\n \"that'd've\": 'that would have',\n 'em': 'them',\n \"there're\": 'there are',\n \"there's\": 'there is',\n \"there'll\": 'there will',\n \"there'd\": 'there would',\n \"there'd've\": 'there would have',\n \"these're\": 'these are',\n \"they're\": 'they are',\n \"they've\": 'they have',\n \"they'll\": 'they will',\n \"they'll've\": 'they will have',\n \"they'd\": 'they would',\n \"they'd've\": 'they would have',\n \"this's\": 'this is',\n \"those're\": 'those are',\n \"to've\": 'to have',\n 'wanna': 'want to',\n \"wasn't\": 'was not',\n \"we're\": 'we are',\n \"we've\": 'we have',\n \"we'll\": 'we will',\n \"we'll've\": 'we will have',\n \"we'd\": 'we would',\n \"we'd've\": 'we would have',\n \"weren't\": 'were not',\n \"what're\": 'what are',\n \"what'd\": 'what did',\n \"what've\": 'what have',\n \"what's\": 'what is',\n \"what'll\": 'what will',\n \"what'll've\": 'what will have',\n \"when've\": 'when have',\n \"when's\": 'when is',\n \"where're\": 'where are',\n \"where'd\": 'where did',\n \"where've\": 'where have',\n \"where's\": 'where is',\n \"which's\": 'which is',\n \"who're\": 'who are',\n \"who've\": 'who have',\n \"who's\": 'who is',\n \"who'll\": 'who will',\n \"who'll've\": 'who will have',\n \"who'd\": 'who would',\n \"who'd've\": 'who would have',\n \"why're\": 'why are',\n \"why'd\": 'why did',\n \"why've\": 'why have',\n \"why's\": 'why is',\n \"will've\": 'will have',\n \"won't\": 'will not',\n \"won't've\": 'will not have',\n \"would've\": 'would have',\n \"wouldn't\": 'would not',\n \"wouldn't've\": 'would not have',\n \"y'all\": 'you all',\n \"y'all're\": 'you all are',\n \"y'all've\": 'you all have',\n \"y'all'd\": 'you all would',\n \"y'all'd've\": 'you all would have',\n \"you're\": 'you are',\n \"you've\": 'you have',\n \"you'll've\": 'you shall have',\n \"you'll\": 'you will',\n \"you'd\": 'you would',\n \"you'd've\": 'you would have',\n 'jan.': 'january',\n 'feb.': 'february',\n 'mar.': 'march',\n 'apr.': 'april',\n 'jun.': 'june',\n 'jul.': 'july',\n 'aug.': 'august',\n 'sep.': 'september',\n 'oct.': 'october',\n 'nov.': 'november',\n 'dec.': 'december',\n 'I\u2019m': 'I am',\n 'I\u2019m\u2019a': 'I am about to',\n 'I\u2019m\u2019o': 'I am going to',\n 'I\u2019ve': 'I have',\n 'I\u2019ll': 'I will',\n 'I\u2019ll\u2019ve': 'I will have',\n 'I\u2019d': 'I would',\n 'I\u2019d\u2019ve': 'I would have',\n 'amn\u2019t': 'am not',\n 'ain\u2019t': 'are not',\n 'aren\u2019t': 'are not',\n '\u2019cause': 'because',\n 'can\u2019t': 'can not',\n 'can\u2019t\u2019ve': 'can not have',\n 'could\u2019ve': 'could have',\n 'couldn\u2019t': 'could not',\n 'couldn\u2019t\u2019ve': 'could not have',\n 'daren\u2019t': 'dare not',\n 'daresn\u2019t': 'dare not',\n 'dasn\u2019t': 'dare not',\n 'doesn\u2019t': 'does not',\n 'e\u2019er': 'ever',\n 'everyone\u2019s': 'everyone is',\n 'gon\u2019t': 'go not',\n 'hadn\u2019t': 'had not',\n 'hadn\u2019t\u2019ve': 'had not have',\n 'hasn\u2019t': 'has not',\n 'haven\u2019t': 'have not',\n 'he\u2019ve': 'he have',\n 'he\u2019s': 'he is',\n 'he\u2019ll': 'he will',\n 'he\u2019ll\u2019ve': 'he will have',\n 'he\u2019d': 'he would',\n 'he\u2019d\u2019ve': 'he would have',\n 'here\u2019s': 'here is',\n 'how\u2019re': 'how are',\n 'how\u2019d': 'how did',\n 'how\u2019d\u2019y': 'how do you',\n 'how\u2019s': 'how is',\n 'how\u2019ll': 'how will',\n 'isn\u2019t': 'is not',\n 'it\u2019s': 'it is',\n '\u2019tis': 'it is',\n '\u2019twas': 'it was',\n 'it\u2019ll': 'it will',\n 'it\u2019ll\u2019ve': 'it will have',\n 'it\u2019d': 'it would',\n 'it\u2019d\u2019ve': 'it would have',\n 'let\u2019s': 'let us',\n 'ma\u2019am': 'madam',\n 'may\u2019ve': 'may have',\n 'mayn\u2019t': 'may not',\n 'might\u2019ve': 'might have',\n 'mightn\u2019t': 'might not',\n 'mightn\u2019t\u2019ve': 'might not have',\n 'must\u2019ve': 'must have',\n 'mustn\u2019t': 'must not',\n 'mustn\u2019t\u2019ve': 'must not have',\n 'needn\u2019t': 'need not',\n 'needn\u2019t\u2019ve': 'need not have',\n 'ne\u2019er': 'never',\n 'o\u2019': 'of',\n 'o\u2019clock': 'of the clock',\n 'ol\u2019': 'old',\n 'oughtn\u2019t': 'ought not',\n 'oughtn\u2019t\u2019ve': 'ought not have',\n 'o\u2019er': 'over',\n 'shan\u2019t': 'shall not',\n 'sha\u2019n\u2019t': 'shall not',\n 'shalln\u2019t': 'shall not',\n 'shan\u2019t\u2019ve': 'shall not have',\n 'she\u2019s': 'she is',\n 'she\u2019ll': 'she will',\n 'she\u2019d': 'she would',\n 'she\u2019d\u2019ve': 'she would have',\n 'should\u2019ve': 'should have',\n 'shouldn\u2019t': 'should not',\n 'shouldn\u2019t\u2019ve': 'should not have',\n 'so\u2019ve': 'so have',\n 'so\u2019s': 'so is',\n 'somebody\u2019s': 'somebody is',\n 'someone\u2019s': 'someone is',\n 'something\u2019s': 'something is',\n 'that\u2019re': 'that are',\n 'that\u2019s': 'that is',\n 'that\u2019ll': 'that will',\n 'that\u2019d': 'that would',\n 'that\u2019d\u2019ve': 'that would have',\n 'there\u2019re': 'there are',\n 'there\u2019s': 'there is',\n 'there\u2019ll': 'there will',\n 'there\u2019d': 'there would',\n 'there\u2019d\u2019ve': 'there would have',\n 'these\u2019re': 'these are',\n 'they\u2019re': 'they are',\n 'they\u2019ve': 'they have',\n 'they\u2019ll': 'they will',\n 'they\u2019ll\u2019ve': 'they will have',\n 'they\u2019d': 'they would',\n 'they\u2019d\u2019ve': 'they would have',\n 'this\u2019s': 'this is',\n 'those\u2019re': 'those are',\n 'to\u2019ve': 'to have',\n 'wasn\u2019t': 'was not',\n 'we\u2019re': 'we are',\n 'we\u2019ve': 'we have',\n 'we\u2019ll': 'we will',\n 'we\u2019ll\u2019ve': 'we will have',\n 'we\u2019d': 'we would',\n 'we\u2019d\u2019ve': 'we would have',\n 'weren\u2019t': 'were not',\n 'what\u2019re': 'what are',\n 'what\u2019d': 'what did',\n 'what\u2019ve': 'what have',\n 'what\u2019s': 'what is',\n 'what\u2019ll': 'what will',\n 'what\u2019ll\u2019ve': 'what will have',\n 'when\u2019ve': 'when have',\n 'when\u2019s': 'when is',\n 'where\u2019re': 'where are',\n 'where\u2019d': 'where did',\n 'where\u2019ve': 'where have',\n 'where\u2019s': 'where is',\n 'which\u2019s': 'which is',\n 'who\u2019re': 'who are',\n 'who\u2019ve': 'who have',\n 'who\u2019s': 'who is',\n 'who\u2019ll': 'who will',\n 'who\u2019ll\u2019ve': 'who will have',\n 'who\u2019d': 'who would',\n 'who\u2019d\u2019ve': 'who would have',\n 'why\u2019re': 'why are',\n 'why\u2019d': 'why did',\n 'why\u2019ve': 'why have',\n 'why\u2019s': 'why is',\n 'will\u2019ve': 'will have',\n 'won\u2019t': 'will not',\n 'won\u2019t\u2019ve': 'will not have',\n 'would\u2019ve': 'would have',\n 'wouldn\u2019t': 'would not',\n 'wouldn\u2019t\u2019ve': 'would not have',\n 'y\u2019all': 'you all',\n 'y\u2019all\u2019re': 'you all are',\n 'y\u2019all\u2019ve': 'you all have',\n 'y\u2019all\u2019d': 'you all would',\n 'y\u2019all\u2019d\u2019ve': 'you all would have',\n 'you\u2019re': 'you are',\n 'you\u2019ve': 'you have',\n 'you\u2019ll\u2019ve': 'you shall have',\n 'you\u2019ll': 'you will',\n 'you\u2019d': 'you would',\n 'you\u2019d\u2019ve': 'you would have'}\n\n#H\u00e0m chuy\u1ec3n \u0111\u1ed5i c\u00e1c t\u1eeb vi\u1ebft t\u1eaft th\u00e0nh c\u00e1c c\u1ee5m t\u1eeb chu\u1ea9n,kh\u00f4ng vi\u1ebft t\u1eaft\ndef contraction_fix(word):\n    try:\n        a=contractions[word]#n\u1ebfu word l\u00e0 t\u1eeb vi\u1ebft t\u1eaft c\u00f3 trong b\u1ed9 t\u1eeb vi\u1ebft t\u1eaft => a s\u1ebd l\u00e0 c\u1ee5m t\u1eeb kh\u00f4ng vi\u1ebft t\u1eaft c\u1ee7a word\n    except KeyError:\n        a=word # n\u1ebfu kh\u00f4ng c\u00f3 key n\u00e0o trong directory ph\u00f9 h\u1ee3p v\u1edbi word \u0111\u00e3 cho=> a s\u1ebd v\u1eabn l\u00e0 word\n    return a #tr\u1ea3 v\u1ec1 t\u1eeb v\u1ef1ng(C\u1ee5m t\u1eeb v\u1ef1ng kh\u00f4ng vi\u1ebft t\u1eaft)","a85e68b6":"###Lo\u1ea1i b\u1ecf c\u00e1c ch\u1eef s\u1ed1,c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n\ndef Preprocess(doc): #H\u00e0m lo\u1ea1i b\u1ecf c\u00e1c ch\u1eef s\u1ed1,c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n    corpus=[]\n    for text in tqdm(doc):\n        text=\" \".join([contraction_fix(w) for w in text.split()])   #t\u00e1ch c\u00e1c t\u1eeb,thay th\u1ebf c\u00e1c t\u1eeb vi\u1ebft t\u1eaft b\u1eb1ng c\u00e1c t\u1eeb \u0111\u00fang,sau \u0111\u00f3 n\u1ed1i ch\u00fang l\u1ea1i\n        \n        #re l\u00e0 m\u1ed9t module \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh bi\u1ec3u th\u1ee9c ch\u00ednh quy(l\u00e0 m\u1ed9t \u0111o\u1ea1n c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t d\u00f9ng \u0111\u1ec3 so kh\u1edbp c\u00e1c chu\u1ed7i ho\u1eb7c m\u1ed9t t\u1eadp c\u00e1c chu\u1ed7i)\n        text=re.sub(r'[^a-z0-9A-Z]',\" \",text)#Lo\u1ea1i b\u1ecf c\u00e1c d\u1ea5u nh\u01b0 !,?.... thay b\u1eb1ng c\u00e1c ' '\n        text=re.sub(r'[0-9]{1}',\"#\",text)#Lo\u1ea1i b\u1ecf c\u00e1c s\u1ed1,thay b\u1eb1ng c\u00e1c '#'\n        text=re.sub(r'[0-9]{2}','##',text)\n        text=re.sub(r'[0-9]{3}','###',text)\n        text=re.sub(r'[0-9]{4}','####',text)\n        text=re.sub(r'[0-9]{5,}','#####',text)\n        corpus.append(text) #th\u00eam d\u00f2ng v\u1eeba r\u1ed3i v\u00e0o m\u1ea3ng corpus\n    \n    return corpus #tr\u1ea3 v\u1ec1 m\u1ea3ng c\u00e1c c\u00e2u \u0111\u00e3 \u0111\u01b0\u1ee3c l\u01b0\u1ee3c b\u1ecf s\u1ed1 v\u00e0 c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t","71e333f8":"###Sau khi \u0111\u00e3 c\u00f3 h\u00e0m ti\u1ec1n x\u1eed l\u00fd c\u1ea7n thi\u1ebft,t\u1ea1o h\u00e0m l\u1ea5y ra v\u1ed1n t\u1eeb v\u1ef1ng\n\ndef get_vocab(corpus):\n    vocab={}#\u0110\u00e2y l\u00e0 v\u1ed1n t\u1eeb v\u1ef1ng c\u1ee7a ch\u00fang ta(Directory)\n    for text in tqdm(corpus): #L\u1eb7p qua c\u00e1c c\u00e2u trong danh s\u00e1ch c\u00e2u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd\n        for word in text.split(): #L\u1eb7p qua c\u00e1c t\u1eeb trong c\u00e1c c\u00e2u\n            try:\n                vocab[word]+=1 #N\u1ebfu t\u1eeb \u0111\u00f3 \u0111\u00e3 c\u00f3 r\u1ed3i,+1 th\u00eam v\u00e0o s\u1ed1 l\u01b0\u1ee3ng c\u1ee7a t\u1eeb \u0111\u00f3\n            except KeyError:\n                vocab[word]=1 #N\u1ebfu ch\u01b0a c\u00f3 t\u1eeb \u0111\u00f3,t\u1ea1o ra 1 key = word v\u00e0 value = 1 m\u1edbi trong vocab(Directory)\n    vocab=dict(sorted(vocab.items(),reverse=True ,key=lambda item: item[1]))#S\u1eafp k\u1ebfp l\u1ea1i c\u00e1c Key\n    return vocab","48928a3d":"#ch\u1ea1y h\u00e0m lo\u1ea1i b\u1ecf k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t ph\u00eda tr\u00ean\ntrain_processed_doc = Preprocess(train.question_text)\nval_processed_doc = Preprocess(val.question_text)","b5dc323d":"for i in range(0,10,1):\n    randIndex = random.randrange(0,train.question_text.size,1)\n    print(\"Raw:\" + train.question_text[randIndex])\n    #in ra question ban \u0111\u1ea7u\n    print(\"Process:\" + train_processed_doc[randIndex])\n    #in ra h\u00e0m ti\u1ec1n s\u1eed l\u00fd","4511ad5e":"vocabulary = get_vocab(train_processed_doc)","8a840aa2":"#Test th\u1eed l\u1ea5y \u0111\u1ed9 d\u00e0i c\u1ee7a danh s\u00e1ch v\u1ed1n t\u1eeb\nlen(vocabulary)","05aa8f42":"#vocabulary.items()\nprint(\"B\u1ecf comment n\u1ebfu c\u1ea7n xem qua c\u00e1c item\")","2cfc4246":"##Import\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom scipy.sparse import coo_matrix\nfrom sklearn.metrics import accuracy_score ","afc92fc6":"##th\u1eed x\u1eed l\u00fd d\u1eef li\u1ec7u theo ki\u1ec3u nguy\u00ean th\u1ee7y nh\u1ea5t\ndef naiveProcessData(raw_data):\n    processedData = []\n    for data in tqdm(raw_data):\n        #t\u1ea1o m\u1edbi directory v\u1edbi c\u00e1c key l\u00e0 vocabulary v\u00e0 value = 0;\n        processedSentence = vocabulary.copy()\n        for key in dictOfVocabulary.keys():\n            processedSentence[key] = 0\n        \n        #c\u1ed9ng 1 v\u1edbi m\u1ed7i word c\u00f3 trong c\u00e2u.\n        for word in data.split():\n            try:\n                processedSentence[word] += 1;    \n            except:\n                print(word + \"is not in bag of word\")\n        processedData.append(np.array(processedSentence.values()))\n    return processedData","9db3cf54":"###T\u1ea1o h\u00e0m kh\u1edfi t\u1ea1o c\u00e1c gi\u00e1 tr\u1ecb c\u1ea7n thi\u1ebft cho ma tr\u1eadn th\u01b0a\n\n#\u0111\u00e1nh d\u1ea5u s\u1ed1 th\u1ee9 t\u1ef1 v\u1ecb tr\u00ed c\u1ee7a c\u00e1c t\u1eeb tr\u01b0\u1edbc\nindex_Vocabulary = vocabulary.copy()\ndef initIndexVocabulary(indexVocabulary):\n    i = 0;\n    for key in indexVocabulary.keys():\n        indexVocabulary[key] = i\n        i += 1","7b312fc6":"initIndexVocabulary(index_Vocabulary)\n#print(index_Vocabulary.items())","2aa74586":"##T\u1ea1o m\u1ed9t m\u1ea3ng d\u1eef li\u1ec7u 3 gi\u00e1 tr\u1ecb nh\u01b0 sau: H\u00e0ng(d\u00f2ng d\u1eef li\u1ec7u th\u1ee9 m\u1ea5y),c\u1ed9t(t\u1eeb \u0111\u00f3 l\u00e0 t\u1eeb th\u1ee9 bao nhi\u00eau trong vocabulary),gi\u00e1 tr\u1ecb(t\u1eeb \u0111\u00f3 xu\u1ea5t hi\u1ec7n bao nhi\u00eau l\u1ea7n)\ndef createConfigValueForMatixSprase(doc):\n    rows = []\n    columns = []\n    values = []\n    \n    indexSentence = 0; # index c\u1ee7a d\u1eef li\u1ec7u c\u00e2u h\u1ecfi\n    for sentence in tqdm(doc): #l\u1eb7p qua t\u1eebng c\u00e2u h\u1ecfi trong kho d\u1eef li\u1ec7u\n        dictOfWord = {}  #Kh\u1edfi t\u1ea1o t\u1eadp h\u1ee3p c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u\n        for word in sentence.split(): #X\u00e9t c\u00e1c t\u1eeb b\u00ean trong m\u1ed9t c\u00e2u\n            try: # N\u1ebfu t\u1eadp h\u1ee3p \u0111\u00e3 c\u00f3 t\u1eeb \u0111ang x\u00e9t,s\u1ed1 l\u01b0\u1ee3ng t\u1eeb \u0111\u00f3 trong dict t\u0103ng l\u00ean 1\n                dictOfWord[word] += 1;\n            except: # N\u1ebfu t\u1eadp h\u1ee3p ch\u01b0a c\u00f3 t\u1eeb \u0111ang x\u00e9t,th\u00eam t\u1eeb \u0111\u00f3 v\u00e0o trong dict,v\u1edbi s\u1ed1 l\u01b0\u1ee3ng t\u1eeb b\u1eb1ng 1\n                dictOfWord.update({word:1})\n        #L\u1eb7p qua t\u1ea5t c\u1ea3 c\u00e1c t\u1eeb c\u00f3 trong t\u1eeb \u0111i\u1ec3n c\u1ee7a c\u00e2u.\n        for word in dictOfWord.keys():\n            rows.append(indexSentence) #Th\u00eam d\u1eef li\u1ec7u th\u1ee9 t\u1ef1 d\u00f2ng\n            try:\n                columns.append(index_Vocabulary[word]) #Th\u00eam d\u1eef li\u1ec7u th\u1ee9 th\u1ef1 c\u1ed9t(V\u1ecb tr\u00ed c\u1ee7a t\u1eeb trong index_Vocabulary)\n            except:\n                columns.append(len(index_Vocabulary)) # N\u1ebfu t\u1eeb \u0111\u00f3 kh\u00f4ng c\u00f3 trong vocabulary,cho n\u00f3 xu\u1ed1ng hang cu\u1ed1i c\u00f9ng\n            values.append(dictOfWord[word]) # Th\u00eam d\u1eef li\u1ec7u v\u1ec1 s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb \u0111\u00f3\n        indexSentence += 1;\n    return (rows,columns,values)","5862abfe":"##T\u1ea1o ma tr\u1eadn th\u01b0a \ndef createSparseMatrix(data):\n    rows,columns,values = createConfigValueForMatixSprase(data)\n    return coo_matrix((values,(rows,columns)),shape=(len(data),len(index_Vocabulary) + 1))","740a2c7a":"#Kh\u1edfi t\u1ea1o ma tr\u1eadn traning\ntrain_X = createSparseMatrix(train_processed_doc) #t\u1ea1o ma tr\u1eadn th\u01b0a cho t\u1eadp train\nlabel_X = train.target","070ac1ed":"test_y = createSparseMatrix(val_processed_doc) #t\u1ea1o ma tr\u1eadn th\u01b0a cho t\u1eadp validation\nvalid_y = val.target","8120b737":"#Training Model MultinomialNB()\nmulti_NB_model = MultinomialNB()\nmulti_NB_model.fit(train_X,label_X)","52b9cdd7":"#\u0110\u00e1nh gi\u00e1 model qua t\u1eadp validation\ndef validModel(Model):\n    pred_y = Model.predict(test_y)\n    print('Training size = %d, accuracy = %.2f%%' % \\\n          (train_X.shape[0],accuracy_score(valid_y, pred_y)*100))","e32644aa":"def compareModel(Models):\n    indexRand = random.randrange(0,len(val.target) - 20,1) #ch\u1ecdn random 1 gi\u00e1 tr\u1ecb\n    test_dataframe = val[indexRand:indexRand + 10] #l\u1ea5y 10 gi\u00e1 tr\u1ecb li\u00ean ti\u1ebfp t\u1eeb s\u1ed1 m\u1edbi random \u0111\u01b0\u1ee3c\n    test_data = test_dataframe.question_text\n    processed_test_data = Preprocess(test_data) #g\u1ecdi h\u00e0m lo\u1ea1i b\u1ecf c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n    test_data_X = createSparseMatrix(processed_test_data) #t\u1ea1o ma tr\u1eadn th\u01b0a v\u1edbi gi\u1eef li\u1ec7u v\u1eeba \u0111\u01b0\u1ee3c x\u1eed l\u00fd\n    \n    \n    for model in Models:\n        y_valid_test = test_dataframe.target\n        y_pred_test = model.predict(test_data_X)\n        print(type(model).__name__[0:4]+\":\" + str(list(y_pred_test)))\n    print(\"test:\" + str(list(y_valid_test))) #in ra k\u1ebft qu\u1ea3 test","af02d7ca":"#H\u00e0m th\u1eed nghi\u1ec7m 1 ch\u00fat\ndef testModel(Model):\n    indexRand = random.randrange(0,len(val.target) - 20,1) #random l\u1ea5y 1 gi\u00e1 tr\u1ecb\n    test_dataframe = val[indexRand:indexRand + 10] #l\u1ea5y 10 gi\u00e1 tr\u1ecb li\u00ean ti\u1ebfp t\u1eeb s\u1ed1 m\u1edbi random \u0111\u01b0\u1ee3c\n    test_data = test_dataframe.question_text\n    processed_test_data = Preprocess(test_data) #g\u1ecdi h\u00e0m lo\u1ea1i b\u1ecf c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n    test_data_X = createSparseMatrix(processed_test_data) #t\u1ea1o ma tr\u1eadn th\u01b0a v\u1edbi gi\u1eef li\u1ec7u v\u1eeba \u0111\u01b0\u1ee3c x\u1eed l\u00fd\n\n\n    y_valid_test = test_dataframe.target\n    y_pred_test = Model.predict(test_data_X)\n    print(test_dataframe)\n    print(\"test:\" + str(list(y_valid_test))) #in ra k\u1ebft qu\u1ea3 test\n    print(\"pred:\" + str(list(y_pred_test))) #in ra k\u1ebft qu\u1ea3 th\u1ef1c t\u1ebf","916761fb":"#\u0111\u00e1nh gi\u00e1 MultinomialNB()\nvalidModel(multi_NB_model)","94ac9b74":"testModel(multi_NB_model)","f752ca45":"ber_NB_model = BernoulliNB()\nber_NB_model.fit(train_X,label_X)","cbda5085":"validModel(ber_NB_model)","cbddd264":"testModel(ber_NB_model)","3a896278":"all_train_preprocess_data = Preprocess(train_data.question_text)\nall_train_x = createSparseMatrix(all_train_preprocess_data)\nall_train_y = raw_train_data.target\nber_NB_model.fit(all_train_x,all_train_y)","99cb324e":"validModel(ber_NB_model)","f1d4bf5d":"raw_test_data.head()\npreprocessTestData = Preprocess(raw_test_data.question_text) #l\u1ea5y t\u1eadp \u0111\u00e3 lo\u1ea1i b\u1ecf h\u1ebft d\u1eef li\u1ec7u\ntestt_padded = createSparseMatrix(preprocessTestData) #\u0111\u01b0a d\u1eef li\u1ec7u v\u1eeba l\u1ea5y \u0111\u01b0\u1ee3c th\u00e0nh ma tr\u1eadn th\u01b0a","44fad489":"y_test_pre = ber_NB_model.predict(testt_padded)\n#print(y_test_pre)","2114731c":"    \u0110\u1ed9 ch\u00ednh x\u00e1c h\u01a1n 92%","0b8a7d71":"> # -Ch\u1ecdn model\n\n> \u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i v\u00e0 l\u00e0 b\u00e0i to\u00e1n v\u1ec1 ng\u00f4n ng\u1eef,n\u00ean s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh Naive Bayes Classifiers.\n\n> Vi\u1ec7c x\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o cho model \u0111\u00e3 \u0111\u01b0\u1ee3c gi\u1ea3i quy\u1ebft b\u1eb1ng c\u00e1ch x\u1eed d\u1ee5ng ma tr\u1eadn th\u01b0a \u1edf tr\u00ean ph\u1ea7n t\u1ea1o vector feature cho d\u1eef li\u1ec7u","a4c32c5e":"    Ti\u1ebfn h\u00e0nh l\u1ea5y ra c\u00e1c t\u1eeb v\u1ef1ng","7299e0bb":"> # -Training Model","9a878bc2":"* **Lo\u1ea1i b\u1ecf c\u00e1c ch\u1eef s\u1ed1,c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t**","e570b137":"\n ph\u1ea3i reset_index cho d\u1eef li\u1ec7u m\u1edbi \u0111\u1ec3 \u0111\u1ed3ng b\u1ed9.","d7389322":"* Test th\u1eed d\u1eef li\u1ec7u v\u1eeba t\u00e1ch","1f671e29":"    H\u00e0m ti\u1ec1n x\u1eed l\u00fd \u0111\u00e3 \u1ed5n","2ef8dc4f":"* **T\u1ea1o vector**\n\n    * X\u1eed l\u00fd theo ma tr\u1eadn th\u01b0a\n","1efc2060":"> # -Chia t\u1eadp train v\u00e0 t\u1eadp validation\n> * Ph\u1ea3i t\u00e1ch t\u1eadp train v\u00e0 t\u1eadp validation ra s\u1edbm,tr\u00e1nh t\u1eadp validation c\u00f3 th\u1ec3 b\u1ecb \u00f4 nhi\u1ec5m b\u1edfi t\u1eadp train trong qu\u00e1 tr\u00ecnh ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u","e1d6dc7d":"# Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u\n> # -C\u00e1c c\u00f4ng vi\u1ec7c c\u1ea7n thi\u1ebft\n> Tr\u01b0\u1edbc ti\u00ean,c\u1ea7n x\u1eed l\u00fd qua d\u1eef li\u1ec7u text n\u00e0y tr\u01b0\u1edbc.Nh\u1eefng vi\u1ec7c c\u1ea7n ph\u1ea3i l\u00e0m nh\u1eefng vi\u1ec7c sau:\n> > * **S\u01a1 ch\u1ebf d\u1eef li\u1ec7u th\u00f4.**\n> > * **Chia t\u1eadp train v\u00e0 t\u1eadp validation.**\n> > * **Lo\u1ea1i b\u1ecf c\u00e1c nhi\u1ec5u.**\n> > * **T\u1ea1o vector feature cho d\u1eef li\u1ec7u**","d9570f8a":"    Th\u1eed h\u00e0m ti\u1ec1n x\u1eed l\u00fd","df9513f0":"* **Th\u1eed nghi\u1ec7m \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a model th\u00f4ng qua validation**","0b388d4d":" c\u00f3 199881 t\u1eeb","92096009":"# Training Model\n> * Ch\u1ecdn model\n> * Training v\u00e0 th\u1eed nghi\u1ec7m","cccfc9e2":"> # -Vector h\u00f3a d\u1eef li\u1ec7u c\u1ea7n predict","9af75170":"# M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\n\nCho c\u00e1c d\u1eef li\u1ec7u v\u1ec1 c\u00e1c c\u00e2u h\u1ecfi tr\u00ean Quora d\u1ea1ng text v\u00e0 ph\u00e2n lo\u1ea1i c\u1ee7a ch\u00fang(C\u00f3 ph\u1ea3i toxic hay kh\u00f4ng?)\n\n\u0110\u00e1nh gi\u00e1 xem d\u1eef li\u1ec7u cho c\u00f3 ph\u1ea3i l\u00e0 c\u00e2u h\u1ecfi toxic hay kh\u00f4ng?","cf87e0f3":"* **S\u1eed d\u1ee5ng MultinomialNB**","7e5dcb4b":"> # -Predict d\u1eef li\u1ec7u","197ed643":"> # -T\u1ea1o c\u00e1c vector feature t\u1eeb d\u1eef li\u1ec7u\n> \n> Khi \u0111\u00e3 x\u1eed l\u00fd qua \u0111\u01b0\u1ee3c d\u1eef li\u1ec7u,c\u1ea7n ti\u1ebfp t\u1ee5c chuy\u1ec3n \u0111\u1ed5i d\u1eef li\u1ec7u v\u1eeba \u0111\u01b0\u1ee3c x\u1eed l\u00fd \u0111\u00f3 sang d\u1eef li\u1ec7u c\u00f3 th\u1ec3 training cho model\n> > * L\u1ea5y ra t\u1eeb v\u1ef1ng cho c\u00e1c c\u00e2u\n> > * Ki\u1ebfn t\u1ea1o vector cho c\u00e1c d\u1eef li\u1ec7u","87d03ec4":"* **Lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb vi\u1ebft t\u1eaft**","00f2cb5f":"    \u0110\u1ed9 ch\u00ednh x\u00e1c h\u01a1i t\u1ed1t h\u01a1n m\u1ed9t ch\u00fat","d4eb90b7":"* **T\u1ea1o vector**\n    * X\u1eed l\u00fd theo c\u00e1ch nguy\u00ean th\u1ee7y:\n\n        * T\u1ea1o ra m\u1ed9t m\u1ea3ng c\u00f3 k\u00edch th\u01b0\u1edbc b\u1eb1ng s\u1ed1 l\u01b0\u1ee3ng t\u1eeb v\u1ef1ng \u0111\u1ec3 ch\u1ee9a d\u1eef li\u1ec7u","687ea328":"> # -S\u01a1 ch\u1ebf d\u1eef li\u1ec7u th\u00f4\n> \n> C\u00f3 3 c\u1ed9t \u1edf \u0111\u00e2y\n> > * Id of question(Text)\n> > * Question text(Text)\n> > * Is question is sincere or not?(0\/1)","0a43eed8":"****","c1c72a95":"* **L\u1ea5y ra t\u1eeb v\u1ef1ng cho c\u00e1c c\u00e2u**\n","1a4df7be":"* **T\u1ea1o c\u00e1c h\u00e0m th\u1eed \u0111\u00e1nh gi\u00e1**","6ed2743f":"    Th\u1eed nghi\u1ec7m kh\u00e1 l\u00e0 chu\u1ea9n","669e517c":"* **s\u1eed d\u1ee5ng BernoulliNB**","dca791df":"> # -Lo\u1ea1i b\u1ecf c\u00e1c nhi\u1ec5u\n> Khi x\u1eed l\u00fd d\u1eef li\u1ec7u,c\u1ea7n l\u1ecdc ra nh\u1eefng d\u1eef li\u1ec7u c\u1ea7n thi\u1ebft v\u00e0 kh\u00f4ng c\u1ea7n thi\u1ebft cho vi\u1ec7c d\u1ef1 \u0111o\u00e1n \u0111\u00e1nh gi\u00e1 c\u1ee7a model.Trong b\u00e0i n\u00e0y,d\u1eef li\u1ec7u c\u1ea7n l\u00e0 c\u00e1c t\u1eeb,n\u00ean c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t v\u00e0 s\u1ed1 s\u1ebd b\u1ecb lo\u1ea1i b\u1ecf.\n> > * Lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb vi\u1ebft t\u1eaft\n> > * Lo\u1ea1i b\u1ecf c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t,kh\u00f4ng c\u00f3 trong b\u1ea3ng ch\u1eef c\u00e1i","1695a052":"train to\u00e0n b\u1ed9 th\u00ec \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n ch\u00fat n\u1eefa","39f6dbc2":"> # -Training model v\u1edbi to\u00e0n b\u1ed9 d\u1eef li\u1ec7u train\nch\u1ecdn ber_NB_model v\u00ec ch\u00ednh x\u00e1c h\u01a1n 1 ch\u00fat"}}