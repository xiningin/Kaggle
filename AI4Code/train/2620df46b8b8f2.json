{"cell_type":{"baed6bcb":"code","67857ab6":"code","415af477":"code","9c24e884":"code","02e682d0":"code","d2e2859d":"code","2e4febd6":"code","66e4bda0":"code","12e0ef43":"code","9bb3d314":"code","82ab96c2":"code","24d81351":"code","4dced3a3":"code","e78e7dd6":"code","32992147":"code","5acd426f":"code","949903a3":"code","5a68a2f9":"code","9f658ca2":"code","b4e497fd":"code","4ce6e3af":"code","dccac408":"code","13576531":"code","5c506069":"code","6aa7359f":"code","8db40730":"code","a35ab8c9":"code","e4aead98":"code","81a36097":"code","390dcba6":"code","a9074b35":"code","00396762":"code","c779d4f3":"code","aab2e899":"code","6a412541":"code","aead71a4":"code","e9d14fea":"code","4e65aa4c":"code","431a7c72":"code","d4b94ca0":"code","463c8155":"code","24d36d2d":"code","65be5898":"code","9c2295ba":"code","2c1912cd":"code","8aee02dc":"code","3e71f29c":"markdown","9915bb80":"markdown","04ca3a0f":"markdown","f1e4029f":"markdown","52e4c2ac":"markdown","09d60be3":"markdown","6de75d30":"markdown","c4f07bf6":"markdown","ae9a2c15":"markdown","4f716362":"markdown","77fb0017":"markdown","2e18d527":"markdown","f479ffd9":"markdown","d55d9c71":"markdown","8de7d09e":"markdown","87e0d4dc":"markdown"},"source":{"baed6bcb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","67857ab6":"import pandas as pd \n\n# import 'Numpy' \nimport numpy as np\n\n# import subpackage of Matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# import 'Seaborn' \nimport seaborn as sns\n\n# to suppress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n \n# to display the float values upto 6 decimal places     \npd.options.display.float_format = '{:.6f}'.format\n\n# import train-test split \nfrom sklearn.model_selection import train_test_split,GridSearchCV\n\n# import various functions from statsmodels\nimport statsmodels\nimport statsmodels.api as sm\n\n# import StandardScaler to perform scaling\nfrom sklearn.preprocessing import StandardScaler \n\n# import various functions from sklearn \nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\n\n# import function to perform feature selection\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom xgboost import XGBClassifier","415af477":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_train.head()","9c24e884":"print(\"No of Rows and Columns are: \",df_train.shape)","02e682d0":"df_train.info()","d2e2859d":"df_train.describe(include=np.object)","2e4febd6":"df_train.describe()","66e4bda0":"#In_Percentage:---\nprint((df_train.isnull().sum()\/len(df_train)*100).sort_values(ascending=False))","12e0ef43":"df_train.Cabin = df_train.Cabin.fillna(df_train.Cabin.mode()[0])","9bb3d314":"sns.histplot(df_train.Age,kde=4)\nplt.axvline(df_train.Age.mean(), color='Yellow', linestyle='dashed', linewidth=1)\nplt.axvline(df_train.Age.median(), color='red', linestyle='dashed', linewidth=1)\nplt.show()\nprint(\"Mean: \",df_train.Age.mean())\nprint(\"Median: \",df_train.Age.median())","82ab96c2":"df_train.Age = df_train.Age.fillna(df_train.Age.median())","24d81351":"df_train.Embarked = df_train.Embarked.fillna(method='ffill')\n# Used Forward Fill As 0.022% of Null Values Are Present...","4dced3a3":"print((df_train.isnull().sum()\/len(df_train)*100).sort_values(ascending=False))","e78e7dd6":"for i in df_train.select_dtypes(include = 'number'):\n    sns.boxplot(df_train[i])\n    plt.show() \n# Has OUTLIERS...\n#THE Best WE can Do is to Cap it...","32992147":"df_train.columns","5acd426f":"df_train.Survived.value_counts(1)*100","949903a3":"# Visualization...\nsns.countplot(df_train.Survived,palette=\"Set3\")\n#Target Variable is not much Imbalance...","5a68a2f9":"sns.pairplot(data=df_train,hue='Survived')","9f658ca2":"sns.barplot(df_train['Sex'],df_train['Survived'])","b4e497fd":"#fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(12,5))\n\nf, axes = plt.subplots(3,figsize=(9,15))\n#sns.set_style(\"darkgrid\")\nsns.countplot(df_train.Pclass,hue=df_train.Survived,ax=axes[0])\nsns.countplot(df_train.Sex,hue=df_train.Survived,ax=axes[1])\nsns.countplot(df_train.Embarked,hue=df_train.Survived,ax=axes[2])\nplt.show()","4ce6e3af":"plt.figure(figsize=(13,7))\nsns.heatmap(df_train.corr(),cmap='coolwarm',vmax=1,vmin=-1,annot=True)","dccac408":"df_train.head()","13576531":"import sklearn\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler ","5c506069":"lr = LabelEncoder()\ndf_train['Sex']= lr.fit_transform(df_train['Sex'])\ndf_train['Cabin']= lr.fit_transform(df_train['Cabin'])\ndf_train['Embarked']= lr.fit_transform(df_train['Embarked'])","6aa7359f":"X = df_train.drop(['PassengerId','Name','Ticket','Survived'],axis=1)\ny = df_train.Survived","8db40730":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)","a35ab8c9":"sc = StandardScaler()\nX_train_Scaled = sc.fit_transform(X_train)\nX_test_Scaled = sc.transform(X_test)","e4aead98":"#Scaled Data...\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train_Scaled,y_train)\ny_pred = log_reg.predict(X_test_Scaled)\nprint('Accuracy Score:\\t',accuracy_score(y_test,y_pred)*100)\n#accuracy Score is 81% ","81a36097":"#UnScaled Data...\nlog_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_train,y_train)\ny_pred = log_reg.predict(X_test)\nprint('Accuracy Score:\\t',accuracy_score(y_test,y_pred)*100)\n#accuracy Score is 82 hence ","390dcba6":"FP_rate,TP_rate,threshold=roc_curve(y_test,y_pred)\n\nplt.figure(figsize=(15,8))\nplt.plot(FP_rate,TP_rate,color='red')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Admission Prediction Classifier (Full Model)', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score:', round(metrics.roc_auc_score(y_test, y_pred),4)))                               \nplt.grid(True)\nplt.show()","a9074b35":"from sklearn.tree import DecisionTreeClassifier\n# UnSCaled Data\nDT=DecisionTreeClassifier()\nDT_model_full = DT.fit(X_train,y_train)\ny_pred_test = DT_model_full.predict(X_test)\nprint(\"Accuracy Score on Test dayta:\\t\",accuracy_score(y_test,y_pred_test))\ny_pred_train = DT_model_full.predict(X_train)","00396762":"# Scaled Data...\nDT=DecisionTreeClassifier()\nDT_model_full = DT.fit(X_train_Scaled,y_train)\ny_pred_test = DT_model_full.predict(X_test_Scaled)\nprint(\"Accuracy Score on Test dayta:\\t\",accuracy_score(y_test,y_pred_test))\ny_pred_train = DT_model_full.predict(X_train)","c779d4f3":"#Grid Search Using Decision Tree.....\n\ngrid={'criterion' : ['entropy', 'gini'], \n      'max_depth' : range(2, 10), \n      'min_samples_split' : range(2,10)} \nDT = DecisionTreeClassifier(random_state=10) \n\ngrid_cv = GridSearchCV(estimator=DT,param_grid=grid,cv=5,scoring='f1_weighted')\ngrid_model = grid_cv.fit(X_train,y_train) \ngrid_model.best_params_\n\n##### then build DT on the above best params","aab2e899":"DT=DecisionTreeClassifier(criterion='entropy',max_depth=8,min_samples_split=8)\nDT_model_full = DT.fit(X_train,y_train)\ny_pred_test = DT_model_full.predict(X_test)\nprint(\"Accuracy Score on Test dayta:\\t\",accuracy_score(y_test,y_pred_test))\ny_pred_train = DT_model_full.predict(X_train)","6a412541":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report \nRF = RandomForestClassifier(random_state=10)\nRF_model = RF.fit(X_train,y_train) \n\nprint(\"Classification Report : \\n\",classification_report(y_test,RF_model.predict(X_test)))\n\n\nRF = RandomForestClassifier(random_state=10)\n\ngrid = { 'criterion' : ['entropy', 'gini'], \n        'n_estimators' : [90, 100, 150, 200], \n        'max_depth' : [10, 15, 20], 'min_samples_split': [2, 5, 8] }\n\ngrid_cv = GridSearchCV(estimator=RF,param_grid=grid,cv=5,scoring='f1_weighted') \ncv_model = grid_cv.fit(X_train,y_train) \ncv_model.best_params_","aead71a4":"RF1 = RandomForestClassifier(random_state=10,criterion= 'gini',\nmax_depth= 10,\nmin_samples_split= 2,\nn_estimators= 100)\nRF_model1 = RF1.fit(X_train,y_train) \n\nprint(\"Classification Report : \\n\",classification_report(y_test,RF_model1.predict(X_test)))\nprint(\"Accuracy Score = \",accuracy_score(y_test,RF_model1.predict(X_test)))","e9d14fea":"from xgboost import XGBClassifier","4e65aa4c":"ada = AdaBoostClassifier(n_estimators=40)\nada_model = ada.fit(X_train,y_train)\nprint(\"Accuracy Score:-  \",accuracy_score(y_test,ada_model.predict(X_test)))\n\n#alpha val = learning rate\nprint(classification_report(y_test,ada_model.predict(X_test)))","431a7c72":"df = pd.read_csv('..\/input\/titanic\/test.csv')\ndf.head()","d4b94ca0":"df.Age = df.Age.fillna(df.Age.median())\ndf.Cabin = df.Cabin.fillna(df.Cabin.mode()[0])\ndf.Fare = df.Fare.fillna(method='ffill')","463c8155":"df1 = df.drop(['PassengerId','Name','Ticket'],axis=1)","24d36d2d":"lr = LabelEncoder()\ndf1['Sex']= lr.fit_transform(df1['Sex'])\ndf1['Cabin']= lr.fit_transform(df1['Cabin'])\ndf1['Embarked']= lr.fit_transform(df1['Embarked'])","65be5898":"df1.head()","9c2295ba":"y_pred = RF1.predict(df1)","2c1912cd":"y_pred = (y_pred > 0.5).astype(int).reshape(df.shape[0])\ny_pred[:], len(y_pred)","8aee02dc":"import csv\nSubmission = pd.DataFrame({ 'PassengerId': df['PassengerId'], 'Survived': y_pred})\nSubmission.to_csv(\"Titanic_Submission.csv\", index=False)\nSubmission.head()","3e71f29c":"### Random Forest Using Best_Parameters:","9915bb80":"Inferance:\n        From the feature Age , Cabin and Embarked We can See that There Are Null Values in there...","04ca3a0f":"## Logistic Regression","f1e4029f":"## Decision Tree:","52e4c2ac":"## ADA_Boost :","09d60be3":"# Model Building...","6de75d30":"### Decision Tree Using Best_Parameters:","c4f07bf6":"## Inferance:\nFrom All the Above models Can Use Random Forest As a Final Model For Prediction.","ae9a2c15":"## Outliers :-","4f716362":"Inferance:\n    - Age,Fare,SibSp[# of siblings \/ spouses aboard the Titanic] has Outliers..\n    - We can Either Cap Outliers Or Remove The Outliers.","77fb0017":"## Null Value :","2e18d527":"#### Test Data:-","f479ffd9":"### Summarizing important observations from the data set","d55d9c71":"### Target variable Balancing Checking...","8de7d09e":"## Random Forest:","87e0d4dc":"Inferance:\n        As There is not much Difference Between Mean And Median We can Take Either of them For Model Building..."}}