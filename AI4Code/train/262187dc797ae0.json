{"cell_type":{"c681ae4d":"code","bca42f91":"code","d74955b2":"code","a88e8813":"code","7131069d":"code","2877e9bf":"code","f1e34221":"code","6e47271f":"code","4d911e27":"code","99f57455":"code","1173e296":"code","4484e03a":"code","1f8570fa":"code","c5704b2c":"code","ca2adcea":"markdown","2f0f877d":"markdown","0d985b48":"markdown","4e8136b3":"markdown","f3b537cc":"markdown","0334d7f8":"markdown","b034a393":"markdown","9b198888":"markdown","a5ab3f6f":"markdown","c4400d21":"markdown","d1ee253c":"markdown","aa657656":"markdown","f3448b3e":"markdown","00797cd5":"markdown","dc9e7f69":"markdown"},"source":{"c681ae4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.neural_network import MLPRegressor as MLPR","bca42f91":"data_dir = '..\/input\/ieee-pes-bdc-datathon-year-2020'\ndf = pd.read_csv(f'{data_dir}\/train.csv')\ntest_df = pd.read_csv(f'{data_dir}\/test.csv')","d74955b2":"df.head(5)","a88e8813":"data_len = len(df)\npct = 1.0 # change it to 0.8~0.9\ntrain_len = int(1.0*data_len)\ntrain_df = df[:train_len]\nval_df = df[train_len:]","7131069d":"X_train = train_df.drop(['ID', 'global_horizontal_irradiance'], axis=1).values.reshape(-1, 6)\ny_train = train_df['global_horizontal_irradiance'].values.reshape(len(train_df))","2877e9bf":"X_val = val_df.drop(['ID', 'global_horizontal_irradiance'], axis=1).values.reshape(-1, 6)\ny_val = val_df['global_horizontal_irradiance'].values.reshape(len(val_df))","f1e34221":"X_test = test_df.drop(['ID'], axis=1).values.reshape(-1, 6)\ntest_ID = test_df['ID'].values.reshape(len(test_df))","6e47271f":"reg = LR(normalize=True)\nreg.fit(X_train, y_train)","4d911e27":"reg.coef_","99f57455":"preds = reg.predict(X_test)","1173e296":"regr = MLPR(random_state=1, hidden_layer_sizes = (32, 8, 2), max_iter=5, validation_fraction=0.1, learning_rate_init=0.02, verbose=True)\nregr.fit(X_train, y_train)","4484e03a":"preds = regr.predict(X_test)\npreds = [0 if p<0 else p for p in preds] # Since GHI can not be less than 0","1f8570fa":"zippedList =  list(zip(test_ID, preds))\nsubmission = pd.DataFrame(zippedList, columns = ['ID','global_horizontal_irradiance'])\nsubmission.to_csv('submission.csv', index=False)","c5704b2c":"submission.head(5)","ca2adcea":"# Generating Prediction","2f0f877d":"# Loading Necessary Libraries","0d985b48":"Welcome to the IEEE PES BDC Datathon 2020. In this competition, your task is to predict [Global Horizontal Irradiance (GHI)](https:\/\/www.3tier.com\/en\/support\/solar-prospecting-tools\/what-global-horizontal-irradiance-solar-prospecting\/#:~:text=Global%20Horizontal%20Irradiance%20(GHI)%20is,Diffuse%20Horizontal%20Irradiance%20(DIF).). This notebook provides a basic data processing and model training pipeline. Feel free to `Copy and Edit` this notebook.  ","4e8136b3":"# Creating Train, Validation and Test data","f3b537cc":"Train validation split is required for finding the optimum solution which will yield better performance on unseen test data. For now, I'm not using\nany data for validation. In the cell below, you may select a suitable value for `pct` to split your dataset into train and test. `pct=1.0` means 100% data is being used for training and `0%` is kept for validation.  ","0334d7f8":"For our first experiment, we are assuming that **GHI** has a linear dependency over the given set of parameters. For a better understanding about how linear regression works, look at this Diagram:![](https:\/\/miro.medium.com\/max\/1600\/1*xz_haBuJRSI2DaveNk-3gw.gif) Courtesy: [Introduction To Linear Regression and Polynomial Regression](https:\/\/towardsdatascience.com\/introduction-to-linear-regression-and-polynomial-regression-f8adc96f31cb) \n\n\nWe are going to use `Sklearn` library for building a linear regression model.","b034a393":"It is a known fact that the GHI score depends on several parameters such as precipitation,\tatmospheric pressure, relative humidity, air temperature, wind direction and wind speed. But their proper relation is unknown. Our objective is to find a relation between **GHI** and these parameters. \n\nThe `train.csv` file comes with an ID for each **GHI** recording events and 6 other parameters. You have to prepare an algorithm that can predict GHI with minimum error possible. You'll then predict over an test set for which the GHI scores are not provided. Your predictions will be compared against actual scores. The competition metric is RMS score. Lower RMS score implies better and more accurate prediction. ","9b198888":"You can now press the `Save Version` button and then select `Save & Run All (Commit)`. It will take around 15~20 mins for this notebook to run. After that, select `submission.csv` from the `Output` and then submit.","a5ab3f6f":"# Prediction over test set","c4400d21":"# Train-Validation Split","d1ee253c":"## Linear Regression","aa657656":"Neural Network is a widely used algorithm these days. It helps us to find complex underlying features within data and differentiate between different data types based on those features. The __gif__ below shows an example of how neural networks work. ![](https:\/\/blog.floydhub.com\/content\/images\/2018\/06\/playground.gif) You can play with this tool at tensorflow [playground](https:\/\/playground.tensorflow.org\/).\n\nWe are going to use `Sklearn`'s built in `Multi Layer Perceptron Regressor` for training our model.","f3448b3e":"# Creating Submission File","00797cd5":"# Neural Network","dc9e7f69":"# Loading Data"}}