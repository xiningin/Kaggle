{"cell_type":{"c0158d8d":"code","c1eca186":"code","923eebf7":"code","5a13b371":"code","f5f920f3":"code","3b01a827":"code","93e49998":"code","e167f7ab":"code","794c2caa":"code","f64fc3b8":"code","8680e69b":"code","010241de":"code","73a03aae":"code","fd34b51e":"code","309c2ddd":"code","a3445bc2":"code","c750f30b":"code","ca6d695b":"code","9c796c0f":"code","82ebbc15":"code","8c1e1e24":"markdown","ddaabdb1":"markdown","d5f39597":"markdown","a339f44f":"markdown","130d7def":"markdown","65c96760":"markdown","6da64422":"markdown","52d81746":"markdown","6e8de58e":"markdown","7a276d82":"markdown","8a728d7b":"markdown","f4f50fc1":"markdown","40677720":"markdown","dd76582c":"markdown","a296e827":"markdown","7914c50f":"markdown","7fba9fc8":"markdown","97869d51":"markdown","7ac00545":"markdown","2c02fb0b":"markdown"},"source":{"c0158d8d":"#For accessing files\nimport os\nimport glob\n\n#For Images\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#For checking progress\nfrom tqdm import tqdm_notebook\n\nimport datetime\n\n#PyTorch Packages\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.nn as nn\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","c1eca186":"def get_image(path,transform=False):\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if transform:\n        img = transform(img)\n    return img\n\ndef show_data(rows,cols,is_train=True,transform=False):\n    if is_train:\n        path = '\/kaggle\/input\/fruits-fresh-and-rotten-for-classification\/dataset\/train\/'\n    else:\n        path = '\/kaggle\/input\/fruits-fresh-and-rotten-for-classification\/dataset\/test\/'\n    path = os.path.join(path,'*','*.png')\n    img_paths = glob.glob(path)\n    np.random.seed(0)\n    img_paths = np.random.choice(img_paths,rows*cols)\n    fig = plt.figure(figsize=(8,8),dpi=150)\n    i = 1\n    for r in range(rows):\n        for c in range(cols):\n            image_path = img_paths[i-1]\n            if 'fresh' in image_path.split('\/')[-2]:\n                title = 'Fresh'\n            else:\n                title = 'Rotten'\n            ax = fig.add_subplot(rows,cols,i)\n            img = get_image(image_path,transform)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(title,fontsize=5)\n            ax.imshow(img)\n            i+=1\n    return fig","923eebf7":"fig = show_data(5,4)\nfig.tight_layout()","5a13b371":"fig = show_data(5,4,is_train=False)\nfig.tight_layout()","f5f920f3":"class FruitsDataset(Dataset):\n    def __init__(self,path,classifier_type='Rotten',subset='train',transforms=None):\n        self.subset = subset\n        if self.subset == 'train':\n            self.PATH = os.path.join(path,'train','*','*.png')\n        elif self.subset == 'test':\n            self.PATH = os.path.join(path,'test','*','*.png')\n        self.data = glob.glob(self.PATH)\n        self.height = 32\n        self.width = 32\n        self.labels = [] \n        if classifier_type == 'Rotten':\n            classes = ['fresh','rotten']\n            for fruit in self.data:\n                if classes[0] in fruit.split('\/')[-2]:\n                    self.labels.append(0)\n                else:\n                    self.labels.append(1)\n        else:\n            classes = ['apple','banana','orange']\n            for fruit in self.data:\n                if classes[0] in fruit:\n                    self.labels.append(0)\n                elif classes[1] in fruit:\n                    self.labels.append(1)\n                else:\n                    self.labels.append(2)\n        self.transforms = transforms\n      \n    def __getitem__(self,index):\n        img_path = self.data[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img,(self.width,self.height))\n        label = self.labels[index]\n        if self.transforms is not None:\n            img_as_tensor = self.transforms(img)\n            if self.transforms is not None:\n                return(img_as_tensor,label)\n            return(img,label)\n  \n    def __len__(self):\n        return(len(self.data))","3b01a827":"transformations = transforms.Compose([\n                                      transforms.ToTensor(),\n                                      transforms.Normalize((0.7321, 0.6322, 0.5291),\n                                                           (0.3302, 0.3432, 0.3701))\n                                      ])\ndataset = FruitsDataset('\/kaggle\/input\/fruits-fresh-and-rotten-for-classification\/dataset\/',transforms = transformations)","93e49998":"img_t, _ = dataset[1000]\nimg = img_t.permute(1,2,0)\nplt.imshow(img);","e167f7ab":"batch_size = 64\nvalidation_split = .2\nshuffle_dataset = True\nrandom_seed= 42\n\n# Creating data indices for training and validation splits:\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]\n\n# Creating PT data samplers and loaders:\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n                                           sampler=train_sampler)\nvalidation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                                sampler=valid_sampler)","794c2caa":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3,16,kernel_size=3,padding=1)\n        self.conv2 = nn.Conv2d(16,8,kernel_size=3,padding=1)\n        self.fc1 = nn.Linear(8*8*8,32)\n        self.fc2 = nn.Linear(32,2)\n    def forward(self,x):\n        out = F.max_pool2d(torch.tanh(self.conv1(x)),2)\n        out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n        out = out.view(-1,8*8*8)\n        out = torch.tanh(self.fc1(out))\n        out = self.fc2(out)\n        return out","f64fc3b8":"model = Net()\nnumel_list = [p.numel() for p in model.parameters()]\nsum(numel_list), numel_list","8680e69b":"device = (torch.device('cuda') if torch.cuda.is_available()\n  else torch.device('cpu'))\nprint(f\"Training on device {device}.\")","010241de":"def training_loop(n_epochs,optimizer,model,loss_fn,train_loader):\n    for epoch in tqdm_notebook(range(1,n_epochs+1)):\n        loss_train = 0.0\n        for imgs, labels in train_loader:\n            imgs = imgs.to(device=device)\n            labels = labels.to(device=device)\n            outputs = model(imgs)\n            loss = loss_fn(outputs,labels)\n            #get rid of gradients from last round\n            optimizer.zero_grad()\n            #performs backward step. Computes all the gradients\n            loss.backward()\n            #Updates the model\n            optimizer.step()\n            loss_train += loss.item()\n        print('{} Epoch {}, Training Loop {}'.format(\n          datetime.datetime.now(), epoch, loss_train\/len(train_loader)))\n\nmodel = Net().to(device=device) #was talking about this above\noptimizer = optim.SGD(model.parameters(), lr=1e-2)\nloss_fn = nn.CrossEntropyLoss()\ntraining_loop(\n  n_epochs = 50,\n  optimizer = optimizer,\n  model = model,\n  loss_fn = loss_fn,\n  train_loader = train_loader,\n)","73a03aae":"torch.save(model.state_dict(), 'FreshnessDetector.pt')","fd34b51e":"def validate(model,train_loader,val_loader):\n    for name, loader in [('train',train_loader),('val',validation_loader)]:\n        correct = 0\n        total = 0\n\n        #gradients nor required, as we don't want to train our parameters\n        with torch.no_grad():\n            for imgs, labels in loader:\n                imgs = imgs.to(device=device)\n                labels = labels.to(device=device)\n                outputs = model(imgs)\n                #max_index,value\n                _,predicted = torch.max(outputs,dim=1)\n                total+=labels.shape[0]\n                correct+=int((predicted==labels).sum())\n    \n        print('Accuracy {}: {:.2f}'.format(name, correct\/total))\n\nvalidate(model,train_loader,validation_loader)","309c2ddd":"img,label = dataset[1]\nplt.imshow(img.permute(1,2,0))\nout = model(img.unsqueeze(0).to(device))\nprint('Actual: {}'.format(label))\nprint('Prediction: {}'.format(out))","a3445bc2":"transformations_test = transforms.Compose([\n                                      transforms.ToTensor(),\n                                      transforms.Normalize((0.7369, 0.6360, 0.5318),\n                                                           (0.3281, 0.3417, 0.3704))\n                                      ])\ntest = FruitsDataset('\/kaggle\/input\/fruits-fresh-and-rotten-for-classification\/dataset\/',subset='test',transforms=transformations_test)","c750f30b":"# imgs = torch.stack([img for img,_ in tqdm_notebook(test)], dim=3)\n# imgs.shape\n\n# #Mean\n# print(imgs.view(3,-1).mean(dim=1))\n\n# #Standard Deviation\n# print(imgs.view(3, -1).std(dim=1))","ca6d695b":"img,_ = test[400]\nplt.imshow(img.permute(1,2,0))\ns = nn.Softmax(dim=1)\nout = s(model(img.unsqueeze(0).to(device)))\nprint('Prediction: {}'.format(out))","9c796c0f":"correct = 0\ntotal = 0\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size)\nfor imgs, labels in train_loader:\n    imgs = imgs.to(device=device)\n    labels = labels.to(device=device)\n    out = model(imgs)\n    _,predicted = torch.max(out,dim=1)\n    correct += int((predicted==labels).sum())\n    total += len(labels)","82ebbc15":"(correct\/total)*100","8c1e1e24":"# Evaluating our model\nApply the same transformation on the test dataset. ","ddaabdb1":"# PyTorch Datasets and Dataloaders\nThe dataset which is provided to us is just images in separate folders: `freshapples`,`freshbanana`,`freshoranges`,`rottenapples`,`rottenbanana` & `rottenoranges`. These folders are common subfolders in train and test folders.\n\nA lot of effort in solving any machine learning problem goes in to preparing the data. PyTorch provides many tools to make data loading easy and hopefully, to make your code more readable.\n\n## Custom Dataset\n`torch.utils.data.Dataset` is an abstract class representing a dataset. The custom dataset should inherit Dataset and override the following methods:\n\n- `__len__` so that len(dataset) returns the size of the dataset.\n- `__getitem__` to support the indexing such that dataset[i] can be used to get ith sample\n\nSo, I'll be creating a custom dataset `FruitsDataset` which inherits Dataset class and overrides the above methods.","d5f39597":"You will be seeing a very distored image of a fruit. Its because of the transformations applied to have a common size for all the images. I have chosen such a small size for reducing the training time.","a339f44f":"# Detect freshness of fruits using PyTorch\n<br>\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/9\/96\/Pytorch_logo.png'\/>\n<br>\nThrough this tutorial I will be giving a step by step guide to tackle a simple deep learning problem. In this notebook I will train a model to detect the freshness of the fruit and then use this model to create a simple app deployed on the web so that anyone and play with it. For the latter part, I have created a [github project](https:\/\/github.com\/anshuls235\/freshness-detector) and the app can be accessed through this [link](https:\/\/freshness-detector.herokuapp.com\/).\n<br><br>\n\n### Now lets first train our model!!","130d7def":"## Training data","65c96760":"## DataLoaders\nWe can iterate over the created dataset with a simple `for` loop. However, we are losing a lot of features by using a simple `for` loop to iterate over the data. In particular, we are missing out on:\n\n- Batching the data\n- Shuffling the data\n- Load the data in parallel using multiprocessing workers.\n\n`torch.utils.data.DataLoader` is an iterator which provides all these features. I will create separate loaders for training and validation with sampling.","6da64422":"# Studying the data\nLet's take a look at the data on which we will be training our data and the one on which we will be doing our predictions.","52d81746":"# Importing the Libraries","6e8de58e":"# Training the Model\nLet's redifine the `nn.Module` class to create our custom NN.","7a276d82":"Let's check one sample.","8a728d7b":"Our final accuracy.","f4f50fc1":"Confirm if we are training on GPU.","40677720":"Training loop","dd76582c":"Lets check the number of parameters we have to train.","a296e827":"The values sent to normalize are calculated using the code below.","7914c50f":"Save our trained model to be deployed on the site later on.","7fba9fc8":"## Transforms\nOne issue we can face is the samples are not of the same size. Most neural networks expect the images of a fixed size. Therefore, we will need to write some prepocessing code. Let\u2019s create two transforms:\n\n- `ToTensor`: to convert the numpy images to torch images (we need to swap axes).\n- `Normalize`: so that the images have zero mean and one variance.\n\n**Please Note: ** For rescaling the images to 32x32 size(This is the size I have chosen) I have used cv2. Instead, we can simply use `Rescale()` transform ","97869d51":"## Test Data\nWe already have the labels so that we can test our model's accuracy. Just showing the labels to show the images in test data.","7ac00545":"Let's take a look at a random image from the dataset we have created. I have used permute function because images are usually represented as `Height x Width x #Channels` where #Channels is 3 for RGB images and 1 for grayscale images. While, pytorch tensors are represented as `#Channels x Height x Width`.","2c02fb0b":"Validating our model"}}