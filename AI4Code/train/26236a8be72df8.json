{"cell_type":{"73272d73":"code","378d32c2":"code","34ef97e4":"code","bc79372a":"code","b96413b5":"code","cca9c87e":"code","f1281240":"code","9f127eab":"code","6501353e":"code","b95b88da":"code","e5afcf71":"code","cb669999":"code","4798a8d2":"code","0880eafb":"code","e3344e7e":"code","a6cdacd8":"code","4f82dc0a":"code","056e673e":"code","a2f0b034":"code","84c417a7":"code","4a3f2222":"code","19980afd":"code","5b785387":"code","2ff4d2ee":"code","36a3cde8":"code","61d9c37a":"code","eb0be31e":"code","7988e573":"code","6412332a":"code","241eb5a4":"code","d0d21b4f":"code","a18df1b8":"code","e69b0123":"code","f78bf434":"code","d160c4d9":"code","b37e3184":"markdown","eb83b767":"markdown","fa99c270":"markdown","c1ac17dc":"markdown","94f65145":"markdown","9d6c57df":"markdown","b73d244e":"markdown","6206be9d":"markdown","56d878d7":"markdown","d09fd63a":"markdown","d28cd105":"markdown","1cf6e7a4":"markdown","87d10f63":"markdown","ed03bf94":"markdown","65a884d2":"markdown","d8f2ac8a":"markdown","0962f062":"markdown","0aa45dc8":"markdown","091e8528":"markdown","e002dd99":"markdown","6a850974":"markdown","f146bf0e":"markdown","9c8ddd04":"markdown","c83d0223":"markdown","725eb174":"markdown","6cdc36e8":"markdown","6156b136":"markdown","2f6088a9":"markdown","d40af5ee":"markdown","cbbb78c3":"markdown","e76126e2":"markdown","6fba4b43":"markdown","52a24e28":"markdown","ee238e5c":"markdown","99e915b3":"markdown","5cdd39ca":"markdown","cc51477c":"markdown","ac1b065c":"markdown","5f3b5410":"markdown","25ee4c45":"markdown","2cdc6ade":"markdown","f0262c89":"markdown","63857c62":"markdown","067fbcb7":"markdown","2fb20887":"markdown","f34a7d6f":"markdown","7194d4f8":"markdown","099e9225":"markdown"},"source":{"73272d73":"from IPython.display import Image\nImage(\"..\/input\/big-to-small-filepng\/big_to_small_file.png\")","378d32c2":"# Some imports\nimport pandas as pd\nDATA_PATH = \"..\/input\/elo-merchant-category-recommendation\/historical_transactions.csv\"","34ef97e4":"df = pd.read_csv(DATA_PATH)","bc79372a":"%%timeit\ndf = pd.read_csv(DATA_PATH)","b96413b5":"ls -lh {DATA_PATH}","cca9c87e":"# verbose is set to False here to avoid the metadata information\ndf.info(verbose=False)","f1281240":"df.info(memory_usage=\"deep\", verbose=False)","9f127eab":"# Verbose is left to the default True here since we want the columns metadata.\ndf.select_dtypes('object').info(memory_usage='deep')","6501353e":"for col in df.select_dtypes('object'):\n    print(df[col].sample(5))\n    print(f\"{df[col].nunique()} unique values for {col}, which has {len(df[col])} rows.\")","b95b88da":"df.purchase_date = pd.to_datetime(df.purchase_date)","e5afcf71":"df.info(memory_usage=\"deep\", verbose=False)","cb669999":"CATEGORICAL_COLS = [\"card_id\", \"category_3\", \"merchant_id\"]\nfor col in[\"card_id\", \"category_3\", \"merchant_id\"]:\n    df[col] = df[col].astype(\"category\")","4798a8d2":"df.info(memory_usage=\"deep\", verbose=False)","0880eafb":"for col in [\"authorized_flag\", \"category_1\"]:\n    # Each row having \"Y\" (short for yes) will get the value 1, otherwise, 0.\n    df[col] = pd.np.where(df[col] == \"Y\", 1, 0)","e3344e7e":"df.info(memory_usage=\"deep\", verbose=False)","a6cdacd8":"df.nunique().sort_values(ascending=True)","4f82dc0a":"# Be careful, even though it is tempting to turn the \"purchase_amount\" to\n#\u00a0categorical to gain more space, \n#\u00a0it isn't the best thing to do since we will be using this column to compute\n#\u00a0aggregations!\nfor col in [\"month_lag\", \"installments\", \"state_id\", \"subsector_id\", \n            \"city_id\", \"merchant_category_id\", \"merchant_id\"]:\n    df[col] = df[col].astype(\"category\")","056e673e":"df.info(memory_usage=\"deep\", verbose=False)","a2f0b034":"df.dtypes","84c417a7":"df.category_2.value_counts(dropna=False, normalize=True).plot(kind='bar')","4a3f2222":"df.category_2 = df.category_2.values.astype(int)","19980afd":"pd.__version__","5b785387":"df.info(memory_usage=\"deep\", verbose=False)","2ff4d2ee":"# You can also use the \"bool\" type (both take one byte for storage).\ndf.authorized_flag = df.authorized_flag.astype(pd.np.uint8)\ndf.category_1 = df.category_1.astype(pd.np.uint8)","36a3cde8":"df.info(memory_usage=\"deep\", verbose=False)","61d9c37a":"df.category_2 = df.category_2.astype(pd.np.uint8)","eb0be31e":"df.category_2.value_counts(normalize=True, dropna=False).plot(kind='bar')","7988e573":"df.info(memory_usage=\"deep\", verbose=False)","6412332a":"# This function could be made generic to almost any loaded CSV file with\n#\u00a0pandas. Can you see how to do it?\n\n#\u00a0Some constants\nPARQUET_ENGINE = \"pyarrow\"\nDATE_COL = \"purchase_date\"\nCATEGORICAL_COLS = [\"card_id\", \"category_3\", \"merchant_id\", \"month_lag\", \n                    \"installments\", \"state_id\", \"subsector_id\", \n                    \"city_id\", \"merchant_category_id\", \"merchant_id\"]\nCATEGORICAL_DTYPES = {col: \"category\" for col in CATEGORICAL_COLS}\nPOSITIVE_LABEL = \"Y\"\nINTEGER_WITH_NAN_COL = \"category_2\"\nBINARY_COLS = [\"authorized_flag\", \"category_1\"]\nINPUT_PATH = \"..\/input\/elo-merchant-category-recommendation\/historical_transactions.csv\"\nOUTPUT_PATH = \"historical_transactions.parquet\"\n\n\ndef smaller_historical_transactions(input_path, output_path):\n    # Load the CSV file, parse the datetime column and the categorical ones.\n    df = pd.read_csv(input_path, parse_dates=[DATE_COL], \n                    dtype=CATEGORICAL_DTYPES)\n    #\u00a0Binarize some columns and cast to the boolean type\n    for col in BINARY_COLS:\n        df[col] = pd.np.where(df[col] == POSITIVE_LABEL, 1, 0).astype('bool')\n    #\u00a0Cast the category_2 to np.uint8\n    df[INTEGER_WITH_NAN_COL] = df[INTEGER_WITH_NAN_COL].values.astype(pd.np.uint8)\n    #\u00a0Save as parquet file\n    df.to_parquet(output_path, engine=PARQUET_ENGINE)\n    return df\n    \ndef load_historical_transactions(path=None):\n    if path is None:\n        return smaller_historical_transactions(INPUT_PATH, OUTPUT_PATH)\n    else: \n        df = pd.read_parquet(path, engine=PARQUET_ENGINE)\n        # Categorical columns aren't preserved when doing pandas.to_parquet\n        #\u00a0(or maybe I am missing something?)\n        for col in CATEGORICAL_COLS:\n            df[col] = df[col].astype('cateogry')\n        return df\n","241eb5a4":"optimized_df = smaller_historical_transactions(INPUT_PATH, OUTPUT_PATH)","d0d21b4f":"optimized_df.info(memory_usage=\"deep\", verbose=False)","a18df1b8":"del df\ndel optimized_df","e69b0123":"#\u00a0TODO: There is a bug when reading the saved parquet file. Check why and fix it!\n#\u00a0Is it related to this issue: https:\/\/issues.apache.org\/jira\/browse\/ARROW-2369?\n#\u00a0%%timeit \n#\u00a0parquet_df = load_historical_transactions(INPUT_PATH)","f78bf434":"#\u00a0parquet_df.info(memory_usage=\"deep\", verbose=False)","d160c4d9":"ls -lh {OUTPUT_PATH}","b37e3184":"Before casting to the appropriate type, we need to explore the columns to find out the best \none. In what follows, I will display few values of each column, count the number of unique values and compare it to the length of the column. ","eb83b767":"Let's also time the loading process","fa99c270":"# Integer with NaNs","c1ac17dc":"Around **1 minute** to load the historical transactions dataset!\nThat's not negligible. Alright, let's check how much space it takes on \ndisk first. For that, will issue the following `bash`\u00a0command: `ls -lh` (the `h` flag is for getting a human-readable output). \n\nIf you don't know it, you can issue `bash` commands right from a[ jupyter notebook](https:\/\/jupyter.org\/) (with or without the `!` sign before the command since `automagic` is turned on by default). Check this great [blog post](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/01.05-ipython-and-shell-commands.html) for more details.\n                                        ","94f65145":"The correct answer is thus **13.1GB** (but you knew it already if you paid attention to the introduction). That's a huge DataFrame loaded into memory, one that is more than 4 times bigger than the original estimate. \n\nCan we do something about it? Of course, otherwise this notebook won't make sense. ;)","9d6c57df":"In this notebook, I will show you an easy workflow to get the historical transactions dataset \nfrom a **13.1GB loaded dataset** to a **much smaller one**. Notice that the techniques showcased in what follows are applicable to a wide range of datasets of course. \nLet's get started!","b73d244e":"The answer to the previous questions is: no, it isn't!\nAnd the `+` sign is here to indicate that the returned value is an **estimation**. \n\nOk, so why is that?\n\nThe answer to that could be summed up in one word: `object`.\nIn fact, when calling the `.info` method, one doesn't get the \"real\" memory footprint bur rather an estimation. \n\nFrom the [`pandas.DataFrame.memory`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.info.html#pandas.DataFrame.info) documentation, here is how it is computed: \n\n> True always show memory usage. False never shows memory usage. A value of \u2018deep\u2019 is equivalent to \u201cTrue with deep introspection\u201d. Memory usage is shown in human-readable units (base-2 representation). Without deep introspection a memory estimation is made based in column dtype and number of rows assuming values consume the same memory amount for corresponding dtypes. With deep memory introspection, a real memory usage calculation is performed at the cost of computational resources.\n\n\nSo what is the correct way to get it? \nIt isn't that hard either, just use the same method again but this time setting `memory_usage=\"deep\"`. By doing so, `pandas` will do the real memory usage computation (thus computing how much space `object` data takes). \nSimple!","6206be9d":"After more exploration, it appears that other columns aren't of `object` type but could be \nturned into categoricals to save some more space. Let's do it.!","56d878d7":"#\u00a0Before optimization","d09fd63a":"<center><h1>Sometimes, small is better!<\/h1><\/center>\nsource (with some adaptation): https:\/\/bulbapedia.bulbagarden.net","d28cd105":"Finally, let's time how long it takes to load the dataset from parquet, how much disk space it takes, and how big is its memory footprint. Notice that I need to remove old DataFrames, otherwise the kernel dies.","1cf6e7a4":"We are on a roll!","87d10f63":"![df_blocks.png](https:\/\/www.dataquest.io\/blog\/content\/images\/df_blocks.png)\n<center><h1>Pandas block representation<\/h1><\/center>\nsource: https:\/\/www.dataquest.io\/blog\/pandas-big-data\/ ","ed03bf94":"**2.7GB** on disk. That's a large dataset! Not yet \"big data\" but could make older computers flinch. \n\nAlright, the next question to ask is: how to get the size of this dataset when loaded into memory?","65a884d2":"We went from **13.1GB to 1GB**. How awesome is that!","d8f2ac8a":"Waw, 11.3GB! That's around **86%** of the total memory footprint!\nAlright, what can we do to reduce it?\n\nThere is probably only one solution I can think of, that is casting the `object`colums to another, **more efficient **type representation (for example, integer) ** while preserving the information**.  Let's do this. ","0962f062":"Remark: I guess it is even possible to get a smaller DataFrame by using smaller integer types for some of the categorical columns. I haven't done it, so let me know in the comments. ;)","0aa45dc8":"#\u00a0Memory footprint","091e8528":"Alright, what is the new memory footprint?","e002dd99":"#\u00a0Timestamps anyone?","6a850974":"# Categorical to the rescue","f146bf0e":"Same thing for the `category_2` column, where the `NaN`\u00a0value can be stored as 0\nand the column cast as `np.unit8`.","9c8ddd04":"#\u00a0What about other categorical columns?","c83d0223":"#\u00a0To wrap up","725eb174":"#\u00a0To go beyond\n\n* I have written a blog post about pandas and there is a section about memory optimization, so check it out [here](https:\/\/www.datacamp.com\/community\/tutorials\/pandas-idiomatic).\n* Here is [another](http:\/\/pbpython.com\/pandas_dtypes.html) blog post about pandas dtypes. \n* Check the pandas [internal archtitecture](https:\/\/github.com\/pydata\/pandas-design\/blob\/master\/source\/internal-architecture.rst) document for more details about the `Block` data structure, the `BlockManager`, and their drawbacks. \n* A great [blog post](https:\/\/jakevdp.github.io\/blog\/2014\/05\/09\/why-python-is-slow\/) to understand how pythons objects are stored. This explains why the pandas `object` has a variable size and can't be accurately estimated without the `memory_usage=\"deep\"`. \n\n","6cdc36e8":"#\u00a0Why should you bother?","6156b136":"Alright, the `purchase_date` contains temporal information, so let's turn it into a `datetime` type using the [`pandas.to_datetime`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.to_datetime.html) function. ","2f6088a9":"There is a new feature in the [0.24](http:\/\/pandas-docs.github.io\/pandas-docs-travis\/whatsnew\/v0.24.0.html#optional-integer-na-support) version that allows to do this \"natively\" but we need to wait \nfor Kaggle to update the [Dockerfile](https:\/\/github.com\/Kaggle\/docker-python\/blob\/master\/Dockerfile) ;)","d40af5ee":"# Binarize some features","cbbb78c3":"**3GB**. That's not that bad. \n\nBut wait, is this really the memory size? **Why is there a `+` sign at the end?** That looks suspecious...","e76126e2":"#\u00a0TL;DR: give me the optimization pipeline","6fba4b43":"Alright, all the values are integer ones, except some NaNs. It is possible to cast these to integer of one uses\nthe underlying numpy array. ","52a24e28":"No need to use the int64 for binary type, the numpy unit8 or the bool_ type are \nmore than enough. So let's do this!","ee238e5c":"Let's start by loading the dataset using the good old `pd.read_csv` and timeit (using the `%%timeit` magic command). ","99e915b3":"For that, I will be using a pandas method (of course, there is usually a pandas method for almost everything): [`pandas.DataFrame.info`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.info.html). Let's see what we get. ","5cdd39ca":"As mentionned earlier, the \"heavy\" load comes mostly from the `object` type (and the associated `ObjectBlock`). In simpler words, an `object` dtype is how pandas stores strings. For that, it uses python and not numpy (contrary to all the other types). Check this [thread](https:\/\/stackoverflow.com\/questions\/34881079\/pandas-distinction-between-str-and-object-types) for some explanations why.  \n\nLet's see which columns have this type and see how much they contribute to the overall memory footprint. ","cc51477c":"I hope you have enjoyed reading this memory optimization workflow and have learned something new. Stay tuned for upcoming kernels. ;)","ac1b065c":"#\u00a0Bonus: smaller integer types","5f3b5410":"For those only interested in the output and how to generate it, here is a function that you can add to your notebook\/script. ","25ee4c45":"#\u00a0Real memory footprint","2cdc6ade":"One last thing before leaving, there is the `category_2` that is a `float64`\u00a0column. Why is that? To see why, let's plot the distribution of the unique values. ","f0262c89":"# Exploring the object columns","63857c62":"# Dtypes","067fbcb7":"Not bad for a start!","2fb20887":"First, it is \"fun\" to do it, in the sense that it is challenging, interesting to learn \nto do it, and finally, could be useful. \nHow could it be useful?\nWell, let's see: \n\n* you need to train a gradient boosted trees model with every possible training dataset you have. Unfortunately, your laptop has only 8GB of RAM. \n\n* you have access to cloud resources but to get your model working, you need a much bigger instance which costs 3 times more.\n\n* you have access to a large cloud instance that fits everything but the training time is high and you want to reduce it. \n\nIn all cases, your boss will be happy that you have managed to train with the largest amount of data, using the least amout of resources necessary, and finished the training in a reasonable time. \n\n","f34a7d6f":"What about the other `object` columns? These are neither timestamps and have only 2 unique textual \nvalues. So what to do about thses? \nBinarize them! Let's see how to do it.","7194d4f8":"Next, any column with \"textual\" information and having more than 3 unique values \nand less than, say, 60% of the column length, should be transformed into the [categorical](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Categorical.html) \ntype. Some background information about this type: it is a fairly new addition to pandas \n(since version 0.21.0) and is inspired from the R one. \nTo do so, will use the `astype(\"category\")` method. ","099e9225":"That's a huge gain!"}}