{"cell_type":{"0af36c8f":"code","43a9a481":"code","acfe4cbd":"code","97c6e4ea":"code","bede5195":"code","2669f84e":"code","bdd2eb13":"code","85f155d9":"code","46c4845b":"code","0491704a":"code","3b45c37c":"code","9c77fa28":"code","67cf8aee":"code","e130a82a":"code","0d5b7ab8":"code","fd6478ad":"code","afaa1d4d":"code","625af573":"code","82311d9a":"code","7bc9b243":"code","ccdda454":"code","35b1dad4":"code","a991f5b1":"code","971932f1":"markdown","f7de1d70":"markdown","049709dd":"markdown","2823c3ba":"markdown","3da4d5d5":"markdown","4be3b4b3":"markdown","75ef51f3":"markdown","fa239ade":"markdown","7351a24a":"markdown","4d5157f0":"markdown","e1517a08":"markdown","047f5c84":"markdown","1a2b185d":"markdown","c243ee64":"markdown","9b002caf":"markdown","8933e3b3":"markdown","60f38f9e":"markdown","13b0a51f":"markdown","17447772":"markdown","e7cb12fd":"markdown","b6f01ec0":"markdown"},"source":{"0af36c8f":"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install scispacy\n    !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","43a9a481":"import numpy as np \nimport pandas as pd\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport scispacy\nimport spacy\nimport en_core_sci_lg\nfrom scipy.spatial.distance import jensenshannon\nimport joblib\nfrom IPython.display import HTML, display\nfrom ipywidgets import interact, Layout, HBox, VBox, Box\nimport ipywidgets as widgets\nfrom IPython.display import clear_output\nfrom tqdm import tqdm\nfrom os.path import isfile\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem import PorterStemmer\nimport regex as re\nfrom gensim.models import Word2Vec\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\n\nfrom contextlib import suppress\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pandas import HDFStore, DataFrame, read_csv, concat\nimport tables\nimport warnings\nimport csv\nfrom contextlib import suppress\nfrom sklearn.manifold import MDS\nfrom datetime import datetime\nfrom itertools import chain\nfrom collections import defaultdict\nimport itertools\nfrom itertools import chain, combinations\nimport random\nfrom collections import Counter\nimport sys\nimport nltk\nfrom IPython.utils import io\nwith io.capture_output() as captured:\n  nltk.download('punkt')\n  nltk.download('averaged_perceptron_tagger')\n  nltk.download('stopwords')\n  lemmatizer = WordNetLemmatizer()\n  porter = PorterStemmer()\npd.options.mode.chained_assignment = None \nwarnings.filterwarnings(\"ignore\", category=tables.NaturalNameWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport pickle","acfe4cbd":"df = pd.read_csv('..\/input\/covid32000\/cord19_df.csv')\nall_texts = df.body_text","97c6e4ea":"model = Word2Vec.load('..\/input\/word2vec-ti-abs-body\/covid_w2v_model_2')","bede5195":"# medium model\nnlp = en_core_sci_lg.load(disable=[\"tagger\", \"parser\", \"ner\"])\nnlp.max_length = 2000000\ndef spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]\n# New stop words list \ncustomize_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n    '-PRON-'\n]\n\n# Mark them as stop words\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = True\n\nif not (isfile('..\/input\/tm-lda\/vectorizer.csv') & isfile('..\/input\/tm-lda\/data_vectorized.csv')):\n    print('Files not there: generating')\n    vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, max_features=800000)\n    data_vectorized = vectorizer.fit_transform(tqdm(all_texts))\n    joblib.dump(vectorizer, '..\/input\/tm-lda\/vectorizer.csv')\n    joblib.dump(data_vectorized, '..\/input\/tm-lda\/data_vectorized.csv')\nelse:\n    vectorizer = joblib.load('..\/input\/tm-lda\/vectorizer.csv')\n    data_vectorized = joblib.load('..\/input\/tm-lda\/data_vectorized.csv')\nif not (isfile('..\/input\/tm-lda\/lda.csv')):\n  print('File not there: generating')\n  lda = LatentDirichletAllocation(n_components=50, random_state=0)\n  lda.fit(data_vectorized)\n  joblib.dump(lda, '..\/input\/tm-lda\/lda.csv')\nelse:\n  lda = joblib.load('..\/input\/tm-lda\/lda.csv') ","2669f84e":"\nif not (isfile('..\/input\/tm-lda\/doc_topic_dist.csv')):\n  print('File not there: generating')\n  doc_topic_dist = pd.DataFrame(lda.transform(data_vectorized))\n  doc_topic_dist.to_csv('..\/input\/tm-lda\/doc_topic_dist.csv', index=False)\nelse:\n   doc_topic_dist = pd.read_csv('..\/input\/tm-lda\/doc_topic_dist.csv')  \n\nis_covid19_article = df.body_text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus')\n\ndef get_k_nearest_docs(doc_dist, k=5, lower=1950, upper=2020, only_covid19=False, get_dist=False):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensen\u2013Shannon divergence in topic space). \n    '''\n    \n    relevant_time = df.publish_year.between(lower, upper)\n    \n    if only_covid19:\n        temp = doc_topic_dist[relevant_time & is_covid19_article]\n        \n    else:\n        temp = doc_topic_dist[relevant_time]\n         \n    distances = temp.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n    k_nearest = distances[distances != 0].nsmallest(n=k).index\n    \n    if get_dist:\n        k_distances = distances[distances != 0].nsmallest(n=k)\n        return k_nearest, k_distances\n    else:\n        return k_nearest","bdd2eb13":"def find_sentences(recommended):\n  recommended['abstract'] = recommended['abstract'].astype(str) \n  recommended['body_text'] = recommended['body_text'].astype(str) \n  recommended['title'] = recommended['title'].astype(str) \n  recommended['complete_text'] = recommended[['title','abstract', 'body_text']].agg('. '.join, axis=1)\n  recommended['sentences'] = ''\n  recommended['token_clean_sentences'] = ''\n  recommended['vector_set'] = ''\n  recommended['final_prc_sents'] = ''\n  recommended['prc_sents'] = ''\n  sentences_dataset = []\n  for i in range(0,len(recommended)):\n      paper_sentences = []\n      paper_sentences = recommended.complete_text.values[i].split('.')\n      sentences_dataset = sentences_dataset + paper_sentences\n      recommended.at[recommended.index[i], 'sentences'] = paper_sentences\n  #clean sentences which contain lower than 7 words \n  #tokenization\n  ind = -1\n  for paper_sentences in recommended.sentences.values:\n    tokenised_sents_set = []\n    clean_sent = []\n    ind = ind + 1\n    paper_id = recommended.index[ind]\n    for sentences in paper_sentences:\n      if bool(sentences):\n          sentences = sentences +\" \"+str(paper_id)\n          input_str = sentences\n          tokens = word_tokenize(input_str)\n          stop_words = set(stopwords.words('english'))\n          new_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI','-PRON-']\n          stop_words = stop_words.union(new_words)\n          tokenised_sents = [i for i in tokens if not i in stop_words]\n          tokenised_sents = [i for i in tokenised_sents if len(i)>2]\n          tokenised_sents = [lemmatizer.lemmatize(w) for w in tokenised_sents]\n          if len(tokenised_sents) > 7:\n            tokenised_sents_set.append(tokenised_sents)\n            clean_sent.append(sentences)\n    recommended.at[recommended.index[ind], 'token_clean_sentences'] = tokenised_sents_set\n    recommended.at[recommended.index[ind], 'prc_sents'] = clean_sent\n    #vectorization\n    k = 0\n    sentences_vector_set= []\n    result_sentences = []\n    for sentences_token in tokenised_sents_set:\n      #print(k)\n      words_vec = []\n      for words in sentences_token:\n          with suppress(Exception):\n              words_vec.append(model[words])\n      if bool(words_vec):  \n        #print('**********************')   \n        sents_vector = np.mean(words_vec, axis = 0)\n        sentences_vector_set.append(sents_vector)\n        result_sentences.append(clean_sent[k])\n      #else:\n        #print(ind,k)\n      k = k+1      \n    recommended.at[recommended.index[ind], 'vector_set'] = sentences_vector_set\n    recommended.at[recommended.index[ind], 'final_prc_sents'] = result_sentences\n  return(recommended,sentences_dataset)    ","85f155d9":"def query_result(data, u_query , top_result = 5):\n  #user_queries = [\"Seasonality of transmission\"]\n  user_queries = u_query \n  tokenised_queries = []\n  for query in user_queries:\n      cleaned_user_query = []\n      input_str = query\n      stop_words = set(stopwords.words('english'))\n      tokens = word_tokenize(input_str)\n      tokenised_query = [i for i in tokens if not i in stop_words]\n      tokenised_query = [i for i in tokenised_query if len(i)>2]\n      tokenised_query = [lemmatizer.lemmatize(w) for w in tokenised_query]\n      cleaned_user_query.append(tokenised_query)\n      tokenised_queries.append(cleaned_user_query)\n  #vectorization\n  queries_vector_set = []\n  for i in range (len(tokenised_queries)):\n      words_set = tokenised_queries[i][0]\n      words_vec = []\n      for words in words_set:\n          with suppress(Exception):\n              words_vec.append(model[words])\n      query_vector = np.mean(words_vec, axis = 0)\n      queries_vector_set.append(query_vector)\n    # find similar\n  mostsimilar_sentence_set = []\n  vec1 = queries_vector_set[0].reshape(1, -1)\n  sorted_results = {}\n  result = {}\n  top_k  = []\n  i = 0\n  for vector in data.vector_set.values:\n    vec2 = vector.reshape(1, -1)\n    dis = cosine_similarity(vec1, vec2)\n    result[i] = dis\n    i = i + 1\n  sorted_results = {k: result[k] for k in sorted(result, key=result.get , reverse=True)}\n  top_k = list(sorted_results.keys())[0:top_result]\n  #print(top_k)\n  mostsimilar_sentence = []\n  for keys in top_k:\n      #print(keys, \"==\" , data.final_prc_sents.values[keys])\n      mostsimilar_sentence.append(data.final_prc_sents.values[keys])\n  mostsimilar_sentence_set.append(mostsimilar_sentence)\n\n  return(mostsimilar_sentence_set)","46c4845b":"def related_papers():\n    '''\n    Creates a widget where you can select one of many papers about covid-19 and then displays related articles from the whole dataset.\n    '''\n    covid_papers = df[df.body_text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus')][['paper_id', 'title']] # are there more names?\n    title_to_id = covid_papers.set_index('title')['paper_id'].to_dict()\n    \n    def main_function(bullet, k=5, year_range=[1950, 2020], only_covid19=False):\n        recommendation(title_to_id[bullet], k, lower=year_range[0], upper=year_range[1], only_covid19=only_covid19)\n    \n    yearW = widgets.IntRangeSlider(min=1950, max=2020, value=[2010, 2020], description='Year Range', \n                                   continuous_update=False, layout=Layout(width='40%'))\n    covidW = widgets.Checkbox(value=False,description='Only COVID-19-Papers',disabled=False, indent=False, layout=Layout(width='20%'))\n    kWidget = widgets.IntSlider(value=10, description='k', max=50, min=1, layout=Layout(width='20%'))\n\n    bulletW = widgets.Select(options=title_to_id.keys(), layout=Layout(width='90%', height='200px'), description='Title:')\n\n    widget = widgets.interactive(main_function, bullet=bulletW, k=kWidget, year_range=yearW, only_covid19=covidW)\n\n    controls = VBox([Box(children=[widget.children[:-1][1], widget.children[:-1][2], widget.children[:-1][3]], \n                         layout=Layout(justify_content='space-around')), widget.children[:-1][0]])\n    output = widget.children[-1]\n    display(VBox([controls, output]))","0491704a":"task1 = [ \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\",\n\"Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\",\n\"Seasonality of transmission.\",\n\"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\",\n\"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\",\n\"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\",\n\"Natural history of the virus and shedding of it from an infected person\",\n\"Implementation of diagnostics and products to improve clinical processes\",\n\"Disease models, including animal models for infection, disease and transmission\",\n\"Tools and studies to monitor phenotypic change and potential adaptation of the virus\",\n\"Immune response and immunity\",\n\"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\",\n \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\",\n\"Role of the environment in transmission\"]\n\ntask2 = ['Data on potential risks factors',\n'Smoking, pre-existing pulmonary disease',\n'Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities',\n'Neonates and pregnant women',\n'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors', \n'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n'Susceptibility of populations',\n'Public health mitigation measures that could be effective for control']\n\ntask3 = ['Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.',\n'Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.',\n'Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.',\n'Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.',\n'Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.',\n'Experimental infections to test host range for this pathogen.',\n'Animal host(s) and any evidence of continued spill-over to humans',\n'Socioeconomic and behavioral risk factors for this spill-over',\n'Sustainable risk reduction strategies']\n\ntask4 = [\"Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\",\n\"Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.\",\n\"Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\",\n\"Methods to control the spread in communities, barriers to compliance and how these vary among different populations..\",\n\"Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\",\n\"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.\",\n\"Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\",\n\"Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"]\n\ntask5 = [\"Effectiveness of drugs being developed and tried to treat COVID-19 patients. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\",\n\"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\",\n\"Exploration of use of best animal models and their predictive value for a human vaccine.\",\n\"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\",\n\"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\",\n\"Efforts targeted at a universal coronavirus vaccine.\",\n\"Efforts to develop animal models and standardize challenge studies\",\n\"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\",\n\"Approaches to evaluate risk for enhanced disease after vaccination\",\n\"Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\"]\n\ntask6 = [\"Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\", \n\"Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\",\n\"Efforts to support sustained education, access, and capacity building in the area of ethics\",\n\"Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\",\n\"Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\",\n\"Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\",\n\"Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\"]\n\ntask7 = [\"How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).\",\n\"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\",\n\"Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\",\n\"National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).\",\n\"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\",\n\"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).\",\n\"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\",\n\"Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.\",\n\"Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\",\n\"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.\",\n\"Policies and protocols for screening and testing.\",\n\"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\",\n\"Technology roadmap for diagnostics.\",\n\"Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\",\n\"New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\",\n\"Coupling genomics and diagnostic testing on a large scale.\",\n\"Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\",\n\"Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\",\n\"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"]\n\ntask8 = [\"Resources to support skilled nursing facilities and long term care facilities.\",\n\"Mobilization of surge medical staff to address shortages in overwhelmed communities\",\n\"Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly for viral etiologies\",\n\"Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\",\n\"Outcomes data for COVID-19 after mechanical ventilation adjusted for age.\",\n\"Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.\",\n\"Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.\",\n\"Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.\",\n\"Best telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state boundaries.\",\n\"Guidance on the simple things people can do at home to take care of sick people and manage disease.\",\n\"Oral medications that might potentially work.\",\n\"Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.\",\n\"Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\",\n\"Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\",\n\"Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials\",\n\"Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\"]\n\ntask9 = [\"Methods for coordinating data-gathering with standardized nomenclature.\",\n\"Sharing response information among planners, providers, and others.\",\n\"Understanding and mitigating barriers to information-sharing.\",\n\"How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).\",\n\"Integration of federal\/state\/local public health surveillance systems.\",\n\"Value of investments in baseline public health response infrastructure preparedness\",\n\"Modes of communicating with target high-risk populations (elderly, health care workers).\",\n\"Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations\u2019 families too).\",\n\"Communication that indicates potential risk of disease to all population groups.\",\n\"Misunderstanding around containment and mitigation.\",\n\"Action plan to mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.\",\n\"Measures to reach marginalized and disadvantaged populations.\",\n\"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.\",\n\"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.\",\n\"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\"]\n\ntasks={'What is known about transmission, incubation, and environmental stability?': task1,\n       'What do we know about COVID-19 risk factors?': task2, \n       'What do we know about virus genetics, origin, and evolution?': task3, \n       'What do we know about non-pharmaceutical interventions?': task4,\n       'What do we know about vaccines and therapeutics?': task5, \n       'What has been published about ethical and social science considerations?': task6, \n       'What do we know about diagnostics and surveillance?': task7,\n       'What has been published about medical care?': task8, \n       'What has been published about information sharing and inter-sectoral collaboration?': task9}\n","3b45c37c":"def relevant_articles(tasks, k=3, lower=1950, upper=2020, only_covid19=False ):\n    top_paper_as_relerelevant = 50\n    top_sentences_number_for_primary_analyse = 80\n    sentences_number = top_sentences_number_for_primary_analyse\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n    tasks = [tasks] if type(tasks) is str else tasks \n    \n    tasks_vectorized = vectorizer.transform(tasks)\n    tasks_topic_dist = pd.DataFrame(lda.transform(tasks_vectorized))\n    Q_number = 1\n    for index, bullet in enumerate(tasks):\n        print (\"***************************************** Question %s *********************************************************\" %(Q_number))\n        print(\"\")\n        print(bullet)\n        print(\"\")\n        print (\"***************************************** Related papers *****************************************************\")\n        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], top_paper_as_relerelevant , lower, upper, only_covid19)\n        recommended = df.iloc[recommended]\n        df_result , all_sentences = find_sentences(recommended)\n        result_sentences_set = []\n        result_vector_set    = []\n        for sentences_set in df_result.final_prc_sents.values:\n          result_sentences_set =  result_sentences_set + sentences_set \n        for vectors in df_result.vector_set.values:\n          result_vector_set =  result_vector_set + vectors\n        data = {}\n        data = {'final_prc_sents' : result_sentences_set , 'vector_set' : result_vector_set }\n        df_allresults_sentences_vectors = pd.DataFrame(data)\n        query = [bullet]\n        \n        #step2 -------------search in top 50 realted paper to chose top k paper \n        primary_sentences_result = query_result(df_allresults_sentences_vectors, query , sentences_number)\n        result_paper_id = []\n        for result_sent  in primary_sentences_result[0]:\n          word_list = result_sent.split()\n          result_paper_id.append(word_list[-1])\n        top_k_result = Counter(result_paper_id).most_common(k)\n\n        top_k_paper_id = {}\n        for indx in range(len(top_k_result) ):\n          key = list(list(top_k_result)[indx])[0]\n          value = list(list(top_k_result)[indx])[1]\n          top_k_paper_id.update({key:value})\n        #print(top_k_paper_id)\n        #{'29312': 11, '29961': 10, '29541': 9, '29558': 8, '29966': 7, '29659': 7}\n        top_k_values = list(top_k_paper_id.values())\n        \n        keys_sentences_dict = {}\n        for keys in top_k_paper_id.keys():\n          top_sent_in_paper = []\n          for sentences in primary_sentences_result[0]:\n            word_list = sentences.split()\n            if str(keys) in word_list[-1]:  \n               top_sent_in_paper.append(sentences)\n          keys_sentences_dict.update ({keys : top_sent_in_paper})\n        #print(keys_sentences_dict)\n        #{'29312': ['\\nEstimates of incubation period and serial interval from other studies are shown in Table S4  29312',..]}\n        #list(keys_sentences_dict.values())[0])\n\n\n\n        top_k_index = list(top_k_paper_id.keys())\n        df_list = []\n        for indx in top_k_index:\n          df_list.append(recommended.loc[[int(indx)]])\n        frames = df_list\n        result_paper_df = pd.concat(frames)\n        \n        \n        \n        ids = 0\n        for l, n in result_paper_df[['url','title']].values:\n          h = '<br\/>'.join(['<a href=\"' + l + '\" target=\"_blank\">'+ n + '<\/a>'])\n          display(HTML(h))\n          print(\"\")\n          print(\"================= >>> Related sentences :\")\n          i = 0\n          for most_similar_sentences in (list(keys_sentences_dict.values())[ids]):\n              if i < 10 :\n                print(most_similar_sentences)\n                print(\"\")\n                i = i + 1\n          ids = ids +1\n        Q_number = Q_number + 1 ","9c77fa28":"def relevant_articles_for_text():    \n    textW = widgets.Textarea(\n        value='',\n        placeholder='Type something',\n        description='',\n        disabled=False,\n        layout=Layout(width='90%', height='200px')\n    )\n\n    yearW = widgets.IntRangeSlider(min=1950, max=2020, value=[2010, 2020], description='Year Range', \n                               continuous_update=False, layout=Layout(width='40%'))\n    covidW = widgets.Checkbox(value=True,description='Only COVID-19-Papers',disabled=False, indent=False, layout=Layout(width='25%'))\n    kWidget = widgets.IntSlider(value=6, description='k', max=50, min=1, layout=Layout(width='25%'))\n\n    button = widgets.Button(description=\"Search\")\n\n    display(VBox([HBox([kWidget, yearW, covidW], layout=Layout(width='90%', justify_content='space-around')),\n        textW, button], layout=Layout(align_items='center')))\n\n    def on_button_clicked(b):\n        clear_output()\n        display(VBox([HBox([kWidget, yearW, covidW], layout=Layout(width='90%', justify_content='space-around')),\n            textW, button], layout=Layout(align_items='center')))        \n        relevant_articles(textW.value, kWidget.value, yearW.value[0], yearW.value[1], covidW.value)\n\n    button.on_click(on_button_clicked)","67cf8aee":"pd.options.mode.chained_assignment = None \nwarnings.filterwarnings(\"ignore\", category=tables.NaturalNameWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n\nrelevant_articles(task1,6, only_covid19=True )","e130a82a":"relevant_articles(task2,6, only_covid19=True )","0d5b7ab8":"relevant_articles(task3,6, only_covid19=True )","fd6478ad":"relevant_articles(task4,6, only_covid19=True )","afaa1d4d":"relevant_articles(task5,6, only_covid19=True )","625af573":"relevant_articles(task6,6, only_covid19=True )","82311d9a":"relevant_articles(task7,6, only_covid19=True )","7bc9b243":"relevant_articles(task8,6, only_covid19=True )","ccdda454":"relevant_articles(task9,6, only_covid19=True )","35b1dad4":"relevant_articles_for_text()","a991f5b1":"relevant_articles_for_text()","971932f1":"![png](https:\/\/i.imgur.com\/7sODIMR.png)","f7de1d70":"## Task5 : What do we know about vaccines and therapeutics?","049709dd":"## Task 6 : What has been published about ethical and social science considerations?","2823c3ba":"## Sentence searching ","3da4d5d5":"## Task2: What do we know about COVID-19 risk factors?","4be3b4b3":"# Install\/Load Packages","75ef51f3":"## Task4 : What do we know about non-pharmaceutical interventions?","fa239ade":"##  Free Text Search","7351a24a":"## Task 9 : What has been published about information sharing and inter-sectoral collaboration?","4d5157f0":"## Latend Dirichlet Allocation","e1517a08":"## load word2vec model","047f5c84":"# All Tasks","1a2b185d":"## Task1: What is known about transmission, incubation, and environmental stability?","c243ee64":"# Load and Prepare Data","9b002caf":"##  Do you want to find important sentences regarding your query in the related articles? \n\n**Idea**:\n\nFinding the related artiles can not be very helpful to answer the questions, hence we decided to find the most related sentnces in the articles to help the reseacher quickly(without read the whole article) find their answers.\n\n**Approach**:\n\nWe first discover a number of topics using LDA (Latent Dirichlet Allocation) and then find a set of related articles. After that we use this set and build a data set of sentences. We use Word2vec and cosine similarity to select top K' sentences. Finally we select those top K articles which have more sentences in selected sentnces. In addition, we discover interseting sentences in the related articles.\n\n\n---\n**Analysis**:\n\nWe, as a **multi-disciplinary research group** (three program developers, a biologist, and a physician and a supervisor), conducted lots of surveys on the Coronavirus-related family on Kaggle provided articles. Our programmer members designed a dataset, as an efficient search engine, to find the most related answers to questions of the Kaggle\u2019s competition. Our biologist and physician members use questions of the competition to evaluate and score our search engine. We use two approaches to evaluate the accuracy and efficiency of our data set. First, we evaluated our result by searching according to the exact word and sentences of task questions, and compare its result by related keywords, which we found in related publications. Our analysis shows that by using lots of related keywords, we can access to the most relevant articles and relevant sentences.\nFurthermore, our programmers used two algorithms to find related articles. Our first algorithms prepared based on finding an article according to the title of the paper, and the second approach was finding paper by searching the content of the article. According to our scoring system for these two methods, we convinced that the searching content of the articles would provide more related articles.\nAs a final survey, we compare our results by another data set on this competition to fix our possible errors. fortunately, we are happy to announce that our results were meaningfully much more accurate compare to similar search engines as a final survey we compare our result by another data set on this competition in order to fix our possible errors. \n\n---\n\n___\n\n\n**Following are the steps we followed to build the model based on the searching content of the articles would provide more related articles-**\n\n\n**WORD2VEC-**\n\n* For each of the paper present, we used three column of the dataset- Title, Abstract and Body text and combined them to form a \u2018complete text\u2019  containing all the text present in each of the paper.\n*Then we performed pre-processing on the \u2018complete text\u2019.  We used scispaCy, \u00a0a Python package containing\u00a0spaCy\u00a0models for processing biomedical, scientific or clinical text. We removed the stop words and perform word lemmatization on the text data. We also removed some common unnecessary words such as author, figure, copyrights, license, fig. Etc from the  \u2018complete text\u2019. \n* We used \u2018Gensim\u2019 Python library to train our word2vec model on the  \u2018complete text\u2019 for each of the paper present in the dataset. We then save the word2vec model for the future uses.\n\n**LDA TOPIC MODELING and FINDING ANSWERS TO THE QUERY-**\n\n* We used a trained a LDA(Latent Dirichlet Allocation) model for topic modeling on the given dataset. In topic modeling each topic is a distribution over words and each document\/paper is a mixture of topics  We discovered 50 as optimum number of topics.\n*Each paper was assigned a set of topic that is, Each paper is a mixture of topics \/ a distribution. We have a LDA space, a simplex. The dimensionality of the space depends on the number of topics. .I.e. 50. Each paper is close to the most strong topics that represent it.  \n* We then used Jensen-Shannon distance to measure the similarity between two probability distribution.\n* Now, for the given query we used the Jensen-Shannon distance between the user query and the papers present in the topic space to find the top 50 relevant paper which is closest to the user query and might contain the answer to the query. As a similarity measure we use 1 - Jensen-Shannon distance. Higher the similarity score more close the paper is to the query.\n* We then convert each paper into a set of sentences. We combine all the sentences in a list that is generated from the relevant papers. Using our word2vec model to generate a vector representation of each sentence. \n* We also form a query vector from the user query. Finally using Cosine similarity score, out of all the generated sentences from the 50 relevant papers, based on the similarity sore we take top 80 sentences as the most probable answer to the query.\n* We then group the sentences based on the paper they belong to. We take maximum of top 10 best sentence for each paper and present it to the user. \n\n___\n\n\n**What's next**\n* launch our website \n* Use query rewriting tecniques to expand the query and add related information to the query from data set.\n* Use different exploration techniques like Query Morphing, Examples as query,queries as answers and a combination workflow of them.\n\n**Resources**\n\nWe would like to thank Daniel Wolffram for the amazing notebook on LDA [https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles](https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles) . We could increase the accuracy of their results.","8933e3b3":"# Search related papers and find related sentences","60f38f9e":"## Task 8 : What has been published about medical care?","13b0a51f":"## Task 7 :What do we know about diagnostics and surveillance?","17447772":"## Task3 : What do we know about virus genetics, origin, and evolution?","e7cb12fd":"## Selecting related articles according to their sentences","b6f01ec0":"## Create a vector for the query."}}