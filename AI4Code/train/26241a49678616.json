{"cell_type":{"9915ea0d":"code","9ab66b10":"code","3a9d8bde":"code","7149a158":"code","1c72c503":"code","d49af1c3":"code","53879330":"code","c09a9f12":"code","086d23bd":"code","778a220e":"code","b31731f5":"code","5e6692fb":"code","e4e9062f":"code","b62d2c06":"code","d36f47d8":"code","d8c058fd":"markdown","8b8ae3b0":"markdown"},"source":{"9915ea0d":"import os\nimport math\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nfrom transformers import AutoTokenizer, TFAutoModel \nwarnings.simplefilter('ignore')","9ab66b10":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","3a9d8bde":"SEED = 42\nEPOCHS = 20\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nTFM_PATH = 'bert-base-uncased'\nTOKENIZER_PATH = 'bert-base-uncased'\nLR = 1e-3\n\nAUTO = tf.data.experimental.AUTOTUNE","7149a158":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\nseed_everything(SEED)","1c72c503":"train_df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\nN_CLASSES = train_df['label_group'].nunique()","d49af1c3":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\ntokenizer.save_pretrained('tokenizer')","53879330":"def load_df():\n    train_df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    train_df['label_group'] = LabelEncoder().fit_transform(train_df['label_group'])\n    N_CLASSES = train_df['label_group'].nunique()\n    train_x, valid_x = train_test_split(train_df[['title', 'label_group']], shuffle=True, stratify=train_df['label_group'], random_state=SEED, test_size=0.33)\n    return train_x, valid_x","c09a9f12":"def tokenize(df):\n    inputs = tokenizer(df.title.tolist(), return_tensors='tf', max_length=64, padding='max_length', truncation=True)\n    return inputs['input_ids'].numpy(), inputs['attention_mask'].numpy()","086d23bd":"def load_ds(tokens, masks, labels, mode='train'):\n    text_ds = tf.data.Dataset.from_tensor_slices((tokens, masks, labels))\n    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n    ds = tf.data.Dataset.zip((text_ds, label_ds))\n    if mode == 'train':\n        ds = ds.repeat()\n        ds = ds.shuffle(len(tokens))\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(AUTO)\n    return ds","778a220e":"def load():\n    train_df, valid_df = load_df()\n    STEPS_PER_EPOCH = train_df.shape[0] \/\/ BATCH_SIZE\n    if train_df.shape[0] % BATCH_SIZE != 0: STEPS_PER_EPOCH += 1\n    train_x, valid_x = tokenize(train_df), tokenize(valid_df)\n    train_ds, valid_ds = load_ds(*train_x, train_df.label_group.values), load_ds(*valid_x, valid_df.label_group.values, mode='valid')\n    return train_ds, valid_ds, STEPS_PER_EPOCH","b31731f5":"class ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https:\/\/arxiv.org\/pdf\/1801.07698.pdf\n        https:\/\/github.com\/lyakaap\/Landmark2019-1st-and-3rd-Place-Solution\/\n            blob\/master\/src\/modeling\/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )[:, 0, :]\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","5e6692fb":"class RobertaArcFace(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.roberta = TFAutoModel.from_pretrained(TFM_PATH)\n        self.arc_margin = ArcMarginProduct(\n            n_classes=N_CLASSES, \n            s=30, \n            m = 0.5, \n            name='head\/arc_margin', \n            dtype='float32'\n        )\n        self.softmax = tf.keras.layers.Softmax(dtype='float32')\n    def call(self, inputs):\n        tokens, masks, labels = inputs\n        out = self.roberta(tokens, masks)\n        feats = out.last_hidden_state[:, 0, :]\n        out = self.arc_margin((feats, labels))\n        out = self.softmax(out)\n        return out","e4e9062f":"def build_model():\n    model = RobertaArcFace()\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=LR),\n                  loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\n                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n    return model","b62d2c06":"def main():\n    train_ds, valid_ds, STEPS_PER_EPOCH = load()\n    checkpoint = ModelCheckpoint(\n        f'bert-arcface.h5', \n        monitor = 'val_loss', \n        save_best_only = True,\n        save_weights_only = True, \n        mode = 'min'\n    )\n    reduce_lr = ReduceLROnPlateau()\n    \n    with strategy.scope():\n        model = build_model()\n    model.fit(\n        train_ds,\n        validation_data=valid_ds,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=[checkpoint, reduce_lr]\n    )","d36f47d8":"main()","d8c058fd":"I cannot train model properly.......","8b8ae3b0":"### Reference\n> @ragnar123 [bert-baseline](https:\/\/www.kaggle.com\/ragnar123\/bert-baseline)\n\nThank you for sharing"}}