{"cell_type":{"60f1e0ec":"code","7cdde60c":"code","dc3cb91b":"code","fe3c970d":"code","e61e300e":"code","c08c63b8":"code","1b8dd79e":"code","0b94fed4":"code","ceae52fd":"code","672d5d0b":"code","54a73399":"code","2284c48d":"code","fe334473":"code","7821a861":"code","68c7c0a2":"code","922ebf35":"code","e218f34b":"code","31f6e2c2":"code","02db0acd":"code","b74240a9":"code","8e7faa85":"code","4771bed7":"code","243841d9":"code","34f4fdbd":"code","e217c51a":"code","f546baef":"code","55856ed6":"code","36b551e4":"code","6e1e93d5":"code","b1630ce7":"code","38091978":"code","ecc0e13e":"code","a1b19747":"code","16b7ca4b":"code","ba92cd71":"code","b2fd3f4d":"code","65fc5804":"code","7b009f0a":"code","a8743c9b":"code","78bc4732":"code","46b5b97b":"code","10171b55":"code","4116a728":"code","bc5ad6c4":"code","e23589f4":"code","9b1c712a":"code","fae5d1ce":"code","3a651d01":"code","b62d6696":"code","04b631ed":"code","58ebcc4c":"markdown","6f075b4b":"markdown","7e6b948d":"markdown","f95c00f6":"markdown","1f405048":"markdown","d284f1a3":"markdown","e5a0a88e":"markdown","262be22c":"markdown","077b96e2":"markdown","6a5dfe91":"markdown","32a1b1ea":"markdown","cc1b9b0d":"markdown","bda0ad4d":"markdown","83b3f987":"markdown","7caf1367":"markdown","5d6d97bf":"markdown","c68c53a7":"markdown","4d327eb2":"markdown","019a0ae1":"markdown","33ecd048":"markdown","93e130e1":"markdown","b922df7f":"markdown","6fbc5314":"markdown","b9d35de1":"markdown","d9c705bc":"markdown","8bc84d1d":"markdown","e4dbf76f":"markdown","68b935e2":"markdown","d2310bd6":"markdown","a38342a3":"markdown","bf11e9ef":"markdown","29ad33f9":"markdown","f2834c92":"markdown"},"source":{"60f1e0ec":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","7cdde60c":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","dc3cb91b":"##display the first five rows of the train dataset.\ntrain.head(5)","fe3c970d":"##display the first five rows of the test dataset.\ntest.head(5)\n","e61e300e":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","c08c63b8":"\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n","1b8dd79e":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","0b94fed4":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","ceae52fd":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","672d5d0b":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","54a73399":"ntrain = train.shape[0]\n","2284c48d":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","fe334473":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","7821a861":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","68c7c0a2":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","922ebf35":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","e218f34b":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","31f6e2c2":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","02db0acd":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","b74240a9":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","8e7faa85":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","4771bed7":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","243841d9":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","34f4fdbd":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","e217c51a":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","f546baef":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","55856ed6":"all_data = all_data.drop(['Utilities'], axis=1)","36b551e4":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","6e1e93d5":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","b1630ce7":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","38091978":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","ecc0e13e":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","a1b19747":"\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n","16b7ca4b":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","ba92cd71":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n\n\n","b2fd3f4d":"all_data.dtypes","65fc5804":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\n\n\n","7b009f0a":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n","a8743c9b":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","78bc4732":"\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)\n","46b5b97b":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\ntrain.shape\ntrain.dtypes","10171b55":"\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n\n\n","4116a728":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","bc5ad6c4":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n","e23589f4":"\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9b1c712a":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","fae5d1ce":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\n","3a651d01":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] =xgb_pred\nsub.to_csv('submission2.csv',index=False)","b62d6696":"sub.to_csv('submission3.csv',index=False) \nfrom IPython.display import FileLink\nFileLink(r'submission3.csv')","04b631ed":"xy=pd.read_csv('submission3.csv')\nxy","58ebcc4c":"##Features engineering","6f075b4b":"###Note : \n Outliers removal is note always safe.  We decided to delete these two as they are very huge and  really  bad ( extremely large areas for very low  prices). \n\nThere are probably others outliers in the training data.   However, removing all them  may affect badly our models if ever there were also  outliers  in the test data. That's why , instead of removing them all, we will just manage to make some of our  models robust on them. You can refer to  the modelling part of this notebook for that. ","7e6b948d":"The target variable is right skewed.  As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","f95c00f6":"###More features engeneering","1f405048":" **Log-transformation of the target variable**","d284f1a3":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\nTherefore, we can safely delete them.","e5a0a88e":"We use the **cross_val_score** function of Sklearn. However this function has not a shuffle attribut, we add then one line of code,  in order to shuffle the dataset  prior to cross-validation","262be22c":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","077b96e2":"###Missing Data","6a5dfe91":"**Hope that at the end of this notebook, stacking will be clear for those, like myself, who found the concept not so easy to grasp**","32a1b1ea":"**Averaged base models score**","cc1b9b0d":"**Label Encoding some categorical variables that may contain information in their ordering set** ","bda0ad4d":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","83b3f987":"**Define a cross validation strategy**","7caf1367":"#Data Processing","5d6d97bf":"**Getting dummy categorical features**","c68c53a7":"let's first  concatenate the train and test data in the same dataframe","4d327eb2":"##Outliers","019a0ae1":"#Modelling","33ecd048":"**Submission**","93e130e1":"Let's see how these base models perform on the data by evaluating the  cross-validation rmsle error","b922df7f":"**Adding one more important feature**","6fbc5314":"**XGBoost:**","b9d35de1":"**Transforming some numerical variables that are really categorical**","d9c705bc":"**If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated -  That will keep me motivated to update it on a regular basis** :-)","8bc84d1d":"**Import librairies**","e4dbf76f":"Let's explore these outliers\n","68b935e2":"Getting the new train and test sets. ","d2310bd6":"- **XGBoost** :","a38342a3":"It remains no missing value.\n","bf11e9ef":"##Target Variable","29ad33f9":"**Data Correlation**\n","f2834c92":"**Skewed features**"}}