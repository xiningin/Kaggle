{"cell_type":{"53f4950a":"code","224506de":"code","5e2dc52a":"code","e1cfa126":"code","2331f869":"code","0c5eb18c":"code","1daa2ec8":"code","e994a149":"code","67e205aa":"code","f6debb5a":"code","b67c6b49":"code","f721e5f4":"code","408eb45b":"code","1a73ae04":"code","e80858f5":"code","6fe4c272":"code","55ffce00":"code","ec40e771":"code","aacfbfdc":"code","4a696f2f":"code","45da0b66":"code","fb865e2c":"code","eb669b1b":"code","d9a89a1e":"code","d109c618":"code","e2951bb4":"code","7c833eef":"code","36908030":"code","89c1429f":"code","7a582086":"code","bacb2b7d":"code","d54c321e":"code","8399e478":"code","e5f8bfd0":"code","0f140b05":"code","07c0a793":"code","60372a5e":"code","56f76e39":"code","4ec11526":"code","63273929":"code","3bef154d":"code","dca24c20":"code","7187068c":"code","b8e7e8e4":"code","c93d7324":"code","99e9bc29":"code","eb3714a7":"code","9cac2658":"code","b6cda79d":"code","420c14cb":"code","83bf4f32":"code","55ec533c":"code","d9bc37f1":"code","5c4a4ea8":"code","4e3df671":"code","c919ad18":"code","da6a8b7d":"code","d2982b83":"code","f46f70b6":"code","9e8ab962":"code","68496f9a":"code","17de84a0":"code","f86cf627":"code","af7e80a9":"code","8e24fda7":"code","0497205a":"code","930e57ac":"code","7eda640e":"code","e26171cd":"code","0eb31840":"code","96b06aae":"code","c08e4548":"code","6445d2a8":"code","4d47123a":"code","cc7dc059":"code","f56a2170":"code","ebdbc75c":"code","07927168":"code","4d993e7a":"code","476866b4":"code","0e305b6f":"code","232ab521":"code","cef32003":"code","e801521c":"code","365c573e":"code","9fa01097":"code","82fe0586":"code","35e7e19f":"code","3c7c0d22":"code","5959a75d":"code","9ad36040":"code","ab0e4018":"code","8e8eb009":"code","0ef13dc1":"code","507ff12d":"markdown"},"source":{"53f4950a":"# Update necessary packages first\n!pip3 uninstall --yes fbprophet\n!pip3 install fbprophet --no-cache-dir --no-binary :all:\n!pip3 install pydotplus --no-cache-dir --no-binary :all:\n\n        \n","224506de":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Don't want to see the warnings in the notebook\nfrom sklearn import svm\n\n","5e2dc52a":"# downloading and describing the dataset\n\ndf = pd.read_csv('..\/input\/avocado.csv')\n","e1cfa126":"df.head()","2331f869":"df.shape","0c5eb18c":"# Some relevant columns in the dataset:\n\n# Date - The date of the observation\n\n# AveragePrice - the average price of a single avocado\n\n# type - conventional or organic\n\n# year - the year\n\n# Region - the city or region of the observation\n\n# Total Volume - Total number of avocados sold\n\n# 4046 - Total number of avocados with PLU 4046 sold  (Small Hass)\n# 4225 - Total number of avocados with PLU 4225 sold  (Large Hass)\n# 4770 - Total number of avocados with PLU 4770 sold  (XLarge Hass)","1daa2ec8":"# Weekly 2018 retail scan data for National retail volume (units) and price.\n\n# Retail scan data comes directly from retailers\u2019 cash registers based on actual retail sales of Hass avocados. \n\n# The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. \n\n# The Product Lookup codes (PLU\u2019s) in the table are only for Hass avocados.\n\n# Other varieties of avocados (e.g. greenskins) are not included in this table.","e994a149":"df.isnull().sum()     # is there any NULL variable in the dataset?\n","67e205aa":"df.describe().round(2)\n","f6debb5a":"df.info() \n","b67c6b49":"# To summarise the dataset we see;\n\n# 14 columns (variables) and 18249 rows (observations)\n\n# There isn't any NULL variable\n\n# data types: float64(9), int64(2), object(3)\n\n# there are some unnamed\/undefined columns\n\n# 'region','type' and 'date' columns are in object format","f721e5f4":"# Target of this project is to predict the future price of avocados depending on those variables we have; \n\n# * Type     *Bags(4 units) vs Bundle(one unit)     *Region      *Volume      *Size     *Years\n","408eb45b":"# PREPROCESSING\n\n# drop unnamed column and rename undefined columns;\n\ndf = df.drop(['Unnamed: 0'], axis = 1)\n\ndf = df.rename(index=str, columns={\"4046\" : \"Small Hass\", \"4225\" : \"Large Hass\",\"4770\" : \"XLarge Hass\" })","1a73ae04":"# convert Date column's format;\n\ndf['Date'] =pd.to_datetime(df.Date)\n\ndf.sort_values(by=['Date'], inplace=True, ascending=True)\n\ndf.head()","e80858f5":"# Average price of Conventional Avocados over time\n\nmask = df['type']== 'conventional'\nplt.rc('figure', titlesize=50)\nfig = plt.figure(figsize = (26, 7))\nfig.suptitle('Average Price of Conventional Avocados Over Time', fontsize=25)\nax = fig.add_subplot(111)\nfig.subplots_adjust(top=0.93)\n\ndates = df[mask]['Date'].tolist()\navgPrices = df[mask]['AveragePrice'].tolist()\n\nplt.scatter(dates, avgPrices, c=avgPrices, cmap='plasma')\nax.set_xlabel('Date',fontsize = 15)\nax.set_ylabel('Average Price (USD)', fontsize = 15)\nplt.show()","6fe4c272":"# Average price of Organic Avocados over time\nmask = df['type']== 'organic'\nplt.rc('figure', titlesize=50)\nfig = plt.figure(figsize = (26, 7))\nfig.suptitle('Average Price of Organic Avocados Over Time', fontsize=25)\nax = fig.add_subplot(111)\nfig.subplots_adjust(top=0.93)\n\ndates = df[mask]['Date'].tolist()\navgPrices = df[mask]['AveragePrice'].tolist()\n\nplt.scatter(dates, avgPrices, c=avgPrices, cmap='plasma')\nax.set_xlabel('Date',fontsize = 15)\nax.set_ylabel('Average Price (USD)', fontsize = 15)\nplt.show()","55ffce00":"# TIME SERIES ANALYSIS\n\n# Since the data itself is a time series data, I first want to see time series analysis predictions, and then apply ML models.\n\n# Creating a two-column dataset to use in time series analysis;\n\ndf2 = df[['Date', 'AveragePrice']]\ndf2 = df2.set_index('Date')\n\nweekly_df = df2.resample('W').mean()\nw_df = weekly_df.reset_index().dropna()\n\nw_df.sort_values(by=['Date'])\nw_df.head()\n","ec40e771":"# Plotting the weekly average prices by month;\n\nimport matplotlib.dates as mdates\n\n\nfig = plt.figure(figsize = (27, 7))\nax = plt.axes()\n#set ticks every month\nax.xaxis.set_major_locator(mdates.MonthLocator())\n#set major ticks format\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\nplt.plot(w_df['Date'],w_df['AveragePrice'],color='b', linewidth=1)\nplt.xlabel(\"2015-2018\")\nplt.ylabel(\"Avocado Price USD\")\nplt.legend()\nplt.show()","aacfbfdc":"# Time Series Forecasts using Facebook's Prophet()\n\nw_df.columns = ['ds', 'y']\n\nfrom fbprophet import Prophet\n\nP=Prophet(interval_width=0.95, yearly_seasonality=True, weekly_seasonality=False, changepoint_range=1) \n#interval_width sets the uncertainty interval to produce a confidence interval around the forecast\n\nP.add_seasonality(name='monthly', period=30.5, fourier_order=5, prior_scale=0.02)\n\n\nP.fit(w_df)\n\nfuture = P.make_future_dataframe(freq='W', periods=4)  # Let's predict the next month's average prices\n\nfuture.tail()","4a696f2f":"from fbprophet.plot import add_changepoints_to_plot\n\nforecast = P.predict(future)\nfig = P.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), P, forecast)","45da0b66":"forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","fb865e2c":"fig2 = P.plot_components(forecast)\n","eb669b1b":"from fbprophet.diagnostics import cross_validation, performance_metrics\ndf_cv = cross_validation(P, initial = '365 days', period = '30 days', horizon = '30 days')\n\ndf_cv.tail()","d9a89a1e":"from fbprophet.diagnostics import performance_metrics\ndf_p = performance_metrics(df_cv)\ndf_p.tail()\n","d109c618":"np.mean(df_p['mape'])","e2951bb4":"np.mean(df_p['rmse'])","7c833eef":"from fbprophet.plot import plot_cross_validation_metric\nfig = plot_cross_validation_metric(df_cv, metric='mape')\n# mean absolute percentage error MAPE  \n","36908030":"forecastnew = forecast['ds']\nforecastnew2 = forecast['yhat']\n\nforecastnew = pd.concat([forecastnew,forecastnew2], axis=1)\n\nmask = (forecastnew['ds'] > \"2018-03-24\") & (forecastnew['ds'] <= \"2020-09-10\")\nforecastedvalues = forecastnew.loc[mask]\n\nmask = (forecastnew['ds'] > \"2015-01-04\") & (forecastnew['ds'] <= \"2018-03-25\")\nforecastnew = forecastnew.loc[mask]\n\nforecastedvalues\n\n# Predictions for the next month are as follows;","89c1429f":"# We already know that y=1.347 for 2018-03-25, and the model prediction is 1.388, which is actually over 3% of the real value.","7a582086":"fig, ax1 = plt.subplots(figsize=(21, 5))\nax1.plot(forecastnew.set_index('ds'), color='b')\nax1.plot(forecastedvalues.set_index('ds'), color='r')\nax1.set_ylabel('Average Prices')\nax1.set_xlabel('Date')\nprint(\"Red = Predicted Values, Blue = Base Values\")","bacb2b7d":"# With Facebook Prophet() we obtain forecasts which are off by 8% due to MAPE values. (Accuracy= 92%)\n\n# The Prophet predicts future prices in a downward trend.","d54c321e":"# Now, Let's see what we get from Machine Learning Algorithms!","8399e478":"# Dropping the Date column (date format is not suitable for next level analysis (i.e. OHE))\ndf = df.drop(['Date'], axis = 1)","e5f8bfd0":"# Checking if the sample is balanced;\ndf.groupby('region').size() # Approximately, there are 338 observations from each region, sample seems balanced.\n","0f140b05":"len(df.region.unique())\n","07c0a793":"df.region.unique() # There are 54 regions but some are subsets of the other regions, i.e: San Francisco-California\n","60372a5e":"# basically we can remove states and work on cities rather than analysing both (to prevent multicollinerarity)\n\nregionsToRemove = ['California', 'GreatLakes', 'Midsouth', 'NewYork', 'Northeast', 'SouthCarolina', 'Plains', 'SouthCentral', 'Southeast', 'TotalUS', 'West']\ndf = df[~df.region.isin(regionsToRemove)]\nlen(df.region.unique())","56f76e39":"# The average prices by regions\n\nplt.figure(figsize=(10,11))\nplt.title(\"Avg.Price of Avocado by Region\")\nAv= sns.barplot(x=\"AveragePrice\",y=\"region\",data= df)","4ec11526":"type_counts = df.groupby('type').size()\nprint(type_counts) \n\n# Types of avocados are also balanced since the ratio is almost 0.5","63273929":"# The average prices of avocados by types; organic or not\n\nplt.figure(figsize=(5,7))\nplt.title(\"Avg.Price of Avocados by Type\")\nAv= sns.barplot(x=\"type\",y=\"AveragePrice\",data= df)","3bef154d":"# Total Bags = Small Bags + Large Bags + XLarge Bags\n\n# To avoid multicollinearity I'll keep S-L-XL bags and drop Total Bags\n\n# But before droping we'd better to see the correlation between those columns:\n\ndf[['Small Hass', \"Large Hass\", \"XLarge Hass\",'Small Bags','Large Bags','XLarge Bags','Total Volume','Total Bags']].corr()","dca24c20":"plt.figure(figsize=(12,6))\nsns.heatmap(df.corr(),cmap='coolwarm',annot=True)\n\n# darker = stronger","7187068c":"# There is a high correlation between those pairs: \n# small hass & total volume  (0.89)      \n# total bags & total volume  (0.87)      \n# small bags & total bags    (0.96)      \n\n# Small Hass avocados are the most preferred\/sold type in the US and customers tend to buy those avocados as bulk, not bag.\n# Retailers want to increase the sales of bagged avocados instead of bulks. They think this is more advantageous for them.\n# Total Bags variable has a very high correlation with Total Volume (Total Sales) and Small Bags, so we can say that most of the bagged sales comes from the small bags.","b8e7e8e4":"df_V = df.drop(['AveragePrice', 'Total Volume', 'Total Bags'], axis = 1).groupby('year').agg('sum')\ndf_V\n","c93d7324":"indexes = ['Small Hass', 'Large Hass', 'XLarge Hass', 'Small Bags', 'Large Bags', 'XLarge Bags']\nseries = pd.DataFrame({'2015': df_V.loc[[2015],:].values.tolist()[0],\n                      '2016': df_V.loc[[2016],:].values.tolist()[0],\n                      '2017': df_V.loc[[2017],:].values.tolist()[0],\n                      '2018': df_V.loc[[2018],:].values.tolist()[0]}, index=indexes)\nseries.plot.pie(y='2015',figsize=(9, 9), autopct='%1.1f%%', colors=['silver', 'pink', 'orange', 'palegreen', 'aqua', 'blue'], fontsize=18, legend=False, title='2015 Volume Distribution').set_ylabel('')\nseries.plot.pie(y='2016',figsize=(9, 9), autopct='%1.1f%%', colors=['silver', 'pink', 'orange', 'palegreen', 'aqua', 'blue'], fontsize=18, legend=False, title='2016 Volume Distribution').set_ylabel('')\nseries.plot.pie(y='2017',figsize=(9, 9), autopct='%1.1f%%', colors=['silver', 'pink', 'orange', 'palegreen', 'aqua', 'blue'], fontsize=18, legend=False, title='2017 Volume Distribution').set_ylabel('')\nseries.plot.pie(y='2018',figsize=(9, 9), autopct='%1.1f%%', colors=['silver', 'pink', 'orange', 'palegreen', 'aqua', 'blue'], fontsize=18, legend=False, title='2018 Volume Distribution').set_ylabel('')","99e9bc29":"# Total Bags = Small Bags + Large Bags + XLarge Bags\n\ndf = df.drop(['Total Bags'], axis = 1)\n","eb3714a7":"# Total Volume = Small Hass +Large Hass +XLarge Hass + Total Bags , to avoid multicollinearity I also drop Total Volume column.\n\n\ndf = df.drop(['Total Volume'], axis = 1)","9cac2658":"df.info()","b6cda79d":"pd.set_option('display.width', 100)\npd.set_option('precision', 3)\ncorrelations = df.corr(method='pearson')\nprint(correlations)","420c14cb":"# Standardizing (scaling) the variables\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf.loc[:,'Small Hass':'XLarge Bags']= scaler.fit_transform(df.loc[:,'Small Hass':'XLarge Bags']) \ndf.head()","83bf4f32":"# Specifying dependent and independent variables\n\nX = df.drop(['AveragePrice'], axis = 1)\ny = df['AveragePrice']\ny=np.log1p(y)\n","55ec533c":"# Labeling the categorical variables\n\nXcat=pd.get_dummies(X[[\"type\",\"region\"]], drop_first = True)","d9bc37f1":"Xnum=X[[\"Small Hass\",\"Large Hass\",\"XLarge Hass\",\"Small Bags\",\"Large Bags\",\"XLarge Bags\"]]","5c4a4ea8":"X= pd.concat([Xcat, Xnum], axis = 1) # Concatenate dummy categorcal variables and numeric variables\nX.shape","4e3df671":"F_DF = pd.concat([y,X],axis=1)\nF_DF.head(2)","c919ad18":"# Just before the regression analysis, I want to visualise the highly correlated Variables with the Average Prices;\n\nimport seaborn as sns\nsns.set(color_codes=True)\nsns.jointplot(x=\"Small Hass\", y=\"AveragePrice\", data=F_DF, kind=\"reg\");\nsns.jointplot(x=\"Small Bags\", y=\"AveragePrice\", data=F_DF, kind=\"reg\");\nsns.jointplot(x=\"Large Bags\", y=\"AveragePrice\", data=F_DF, kind=\"reg\");","da6a8b7d":"sns.lmplot(x=\"Small Hass\", y=\"AveragePrice\", col=\"type_organic\", data=F_DF, col_wrap=2);\n\n# Graphs depict that organic avocados have less elasticity to the price, compared to conventional ones.","d2982b83":"# TRAIN and TEST SPLIT\n\n# Since the data is a time series data (gives weekly avocado prices between Jan 2015 and Apr 2018)\n# I sort it by Date and then split it due to date manually (not randomly), to preserve the 'times series effect' on it.\n# I determined the split ratio as 0.30, so train and test data are just as follows;\n\n\nX_train=X[0:10172]\ny_train=y[0:10172]\nX_test=X[10172:]\ny_test=y[10172:]\n","f46f70b6":"# Implementing machine learning models","9e8ab962":"# Multiple Linear Regression ","68496f9a":"from sklearn.linear_model import LinearRegression\n\nLinReg = LinearRegression()\nLinReg.fit(X_train,y_train)\n\nprint (\"R2 of Linear Regresson:\", LinReg.score(X_train,y_train) )","17de84a0":"print('MAE: ',metrics.mean_absolute_error(y_test, LinReg.predict(X_test)))\nprint('MSE: ',metrics.mean_squared_error(y_test, LinReg.predict(X_test)))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, LinReg.predict(X_test))))","f86cf627":"# Creating a Histogram of Residuals\nplt.figure(figsize=(6,4))\nsns.distplot(y_test - LinReg.predict(X_test))\nplt.title('Distribution of residuals');","af7e80a9":"plt.scatter(y_test,LinReg.predict(X_test))","8e24fda7":"# we can confirm the R2 value (moreover, get the R2 Adj.value) of the model by statsmodels library of python\nimport statsmodels.api as sm\nX_train = sm.add_constant(X_train) # adding a constant\nmodel = sm.OLS(y_train, X_train).fit()\nprint(model.summary())","0497205a":"X_train=X[0:10172]\ny_train=y[0:10172]\nX_test=X[10172:]\ny_test=y[10172:]","930e57ac":"# LASSO and RIDGE Regressions\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV\n\nalphas = np.logspace(-5,3,20)\n\nclf = GridSearchCV(estimator=linear_model.Ridge(), param_grid=dict(alpha=alphas), cv=10)\nclf.fit(X_train, y_train)\noptlamGSCV_R = clf.best_estimator_.alpha\nprint('Optimum regularization parameter (Ridge):', optlamGSCV_R)\n\nclf = GridSearchCV(estimator=linear_model.Lasso(), param_grid=dict(alpha=alphas), cv=10)\nclf.fit(X_train, y_train)\noptlamGSCV_L= clf.best_estimator_.alpha\nprint('Optimum regularization parameter (Lasso):', optlamGSCV_L)","7eda640e":"ridge = linear_model.Ridge(alpha = optlamGSCV_R) \nridge.fit(X_train, y_train)\nprint('RMSE value of the Ridge Model is: ',np.sqrt(metrics.mean_squared_error(y_test, ridge.predict(X_test))))","e26171cd":"ridge.score(X_train, y_train) #Returns the coefficient of determination (R2) of the prediction.","0eb31840":"# Creating a Histogram of Residuals\nplt.figure(figsize=(6,4))\nsns.distplot(y_test - ridge.predict(X_test))\nplt.title('Distribution of residuals');","96b06aae":"lasso = linear_model.Lasso(alpha = optlamGSCV_L)\nlasso.fit(X_train, y_train)\nprint('RMSE value of the Lasso Model is: ',np.sqrt(metrics.mean_squared_error(y_test, lasso.predict(X_test))))","c08e4548":"lasso.score(X_train, y_train) #Returns the coefficient of determination R^2 of the prediction.\n","6445d2a8":"# Creating a Histogram of Residuals\nplt.figure(figsize=(6,4))\nsns.distplot(y_test - lasso.predict(X_test))\nplt.title('Distribution of residuals');","4d47123a":"coef = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  \n      str(sum(coef == 0)) + \" variables\")\nimp_coef = pd.concat([coef.sort_values()]) #plot all\nmatplotlib.rcParams['figure.figsize'] = (7.0, 30.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","cc7dc059":"# According to the RMSE results, Ridge works best compared to linear regression and lasso.\n\n# Let's see the other ML Models' RMSE values;","f56a2170":"# KNN Regressor","ebdbc75c":"from sklearn import neighbors\nfrom math import sqrt\n\nKnn = neighbors.KNeighborsRegressor()\nKnn.fit(X_train, y_train)  \nerror = sqrt(metrics.mean_squared_error( y_test, Knn.predict(X_test))) \nprint('RMSE value of the KNN Model is:', error)","07927168":"Knn.score(X_train, y_train)  # R2 of the KNN model","4d993e7a":" # SVR Regressor","476866b4":"from sklearn.svm import SVR\n\n# First, let's choose which kernel is the best for our data\n\nfor k in ['linear','poly','rbf','sigmoid']:\n    clf = svm.SVR(kernel=k)\n    clf.fit(X_train, y_train)\n    confidence = clf.score(X_train, y_train)\n    print(k,confidence)","0e305b6f":"Svr=SVR(kernel='rbf', C=1, gamma= 0.5)   # Parameter Tuning to get the best accuracy\n\n# Intuitively, the gamma defines how far the influence of a single training example reaches, with low values meaning \u2018far\u2019 and high values meaning \u2018close\u2019.\n# The C parameter trades off correct classification of training examples against maximization of the decision function\u2019s margin. \n# For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. \n# A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. \n# In other words C behaves as a regularization parameter in the SVM.\n\nSvr.fit(X_train,y_train)\nprint(Svr.score(X_train,y_train))","232ab521":"error = sqrt(metrics.mean_squared_error(y_test,Svr.predict(X_test))) #calculate rmse\nprint('RMSE value of the SVR Model is:', error)","cef32003":"Svr.predict(X_test)[0:5]  # print the first 5 predictions of our test set\n","e801521c":"y_test[0:5]","365c573e":"# Decision Tree Regressor","9fa01097":"# Determining the best depth\nfrom sklearn.tree import DecisionTreeRegressor\n\nminDepth = 100\nminRMSE = 100000\n\n\nfor depth in range(2,10):\n  tree_reg = DecisionTreeRegressor(max_depth=depth)\n  tree_reg.fit(X_train, y_train)\n  y_pred = tree_reg.predict(X_test)\n  mse = mean_squared_error(y_test, y_pred)\n  rmse = np.sqrt(mse)\n  print(\"Depth:\",depth,\", MSE:\", mse)\n  print(\"Depth:\",depth, \",RMSE:\", rmse)\n  \n  if rmse < minRMSE:\n    minRMSE = rmse\n    minDepth = depth\n    \n      \nprint(\"MinDepth:\", minDepth)\nprint(\"MinRMSE:\", minRMSE)","82fe0586":"DTree=DecisionTreeRegressor(max_depth=minDepth)\nDTree.fit(X_train,y_train)\nprint(DTree.score(X_train,y_train))  \n","35e7e19f":"print('MAE:', metrics.mean_absolute_error(y_test, DTree.predict(X_test)))\nprint('MSE:', metrics.mean_squared_error(y_test, DTree.predict(X_test)))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, DTree.predict(X_test))))","3c7c0d22":"#!pip3 install pydotplus --no-cache-dir --no-binary :all:\n        \nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(DTree, out_file=dot_data, feature_names = X.columns, \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())  \n\n# We obtain a 2 depth figure :)","5959a75d":"from sklearn.ensemble import RandomForestRegressor\nRForest = RandomForestRegressor()\nRForest.fit(X_train,y_train)\nprint(RForest.score(X_train,y_train))  ","9ad36040":"print('MAE:', metrics.mean_absolute_error(y_test, RForest.predict(X_test)))\nprint('MSE:', metrics.mean_squared_error(y_test, RForest.predict(X_test)))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, RForest.predict(X_test))))","ab0e4018":"# CONCLUSION \n\n# Comparing The RMSE Values Of The Models","8e8eb009":"# Linear Regression RMSE : \nprint('RMSE value of the Linear Regr : ',round(np.sqrt(metrics.mean_squared_error(y_test, LinReg.predict(X_test))),4))\n\n# Ridge RMSE             : \nprint('RMSE value of the Ridge Model : ',round(np.sqrt(metrics.mean_squared_error(y_test, ridge.predict(X_test))),4))\n\n# Lasso RMSE             : \nprint('RMSE value of the Lasso Model : ',round(np.sqrt(metrics.mean_squared_error(y_test, lasso.predict(X_test))),4))\n\n# KNN RMSE               : \nprint('RMSE value of the KNN Model   : ',round(np.sqrt(metrics.mean_squared_error(y_test, Knn.predict(X_test))),4))\n\n# SVR RMSE               : \nprint('RMSE value of the SVR Model   : ',round(np.sqrt(metrics.mean_squared_error(y_test, Svr.predict(X_test))),4))\n\n# Decision Tree RMSE     : \nprint('RMSE value of the Decis Tree  : ',round(np.sqrt(metrics.mean_squared_error(y_test, DTree.predict(X_test))),4))\n\n# Random Forest RMSE     : \nprint('RMSE value of the Rnd Forest  : ',round(np.sqrt(metrics.mean_squared_error(y_test, RForest.predict(X_test))),4))\n\n# Times Series RMSE      : \nprint('RMSE value of the TS Analysis : ',round(np.mean(df_p['rmse']),4))\n","0ef13dc1":"# Now, it's time to decide wheather you're interested in short term predictions or long term!\n\n# When we analyse the results, due to the RMSE values, we see that:\n\n# If we need to get short period estimates, we'd better to use Time Series Analysis for this dataset. (MAPE= 8% and Accuracy= 92%)\n\n# But, if we need longer periods prices, SVR model gives the best predicts. (coefficient of determination of 82%)\n\n# That's all for now. Hope you like my work :) Thanks for your interest, take care!","507ff12d":"Hi Folks!\n\nIn this project, two types of analyzes for predicting avocado prices are displayed;\n\n**Time Series Analysis** and **Machine Learning Analysis**.\n\nI used Prophet for Time Series analysis (it's open source software released by Facebook's Core Data Science team).\n\nAnd then I applied Regression and Machine Learning Algorithms to the data; \n* Linear Regression, \n* Ridge Regression, \n* Lasso Regression, \n* KNN, \n* SVR, \n* Decision Tree, and \n* Random Forest.\n\nGuess which one gives the best accuracy! It depends, we will see :)\n\nUntil then, ENJOY MACHINE LEARNING!\n\n<img src=\"https:\/\/cognigen-cellular.com\/images\/avocado-clipart-4.png\" width=\"250px\"\/>\n"}}