{"cell_type":{"f7e43c31":"code","e93b886d":"code","3ae67fb1":"code","603fe657":"code","b91fdb9f":"code","578d2fa5":"code","16b5b934":"code","578b6978":"code","aaba9b61":"code","0b8dabbb":"code","d61f8393":"code","8285086e":"code","4c1a6295":"code","cb1b59c3":"code","1a84c2cb":"code","e54e2e9f":"code","92ce6ce7":"code","b68db6ab":"code","83dc2fdf":"code","d79f6470":"code","6fc0aebd":"code","b3de2b8f":"code","1ba9c851":"code","645a5a1a":"code","34a50eca":"code","ffd1b365":"markdown","7e446f29":"markdown","242100f7":"markdown","32ec0c25":"markdown","7260d595":"markdown"},"source":{"f7e43c31":"!conda remove -y greenlet\n!pip install pytorch-pretrained-bert\n!pip install allennlp\n!pip install https:\/\/github.com\/ceshine\/pytorch_helper_bot\/archive\/0.0.5.zip","e93b886d":"!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-development.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-test.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-validation.tsv -q","3ae67fb1":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"323\"\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\nfrom allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\n\nfrom helperbot import (\n    TriangularLR, BaseBot, WeightDecayOptimizerWrapper\n)","603fe657":"BERT_MODEL = 'bert-large-uncased'\nCASED = False","b91fdb9f":"def extract_target(df):\n    df[\"Neither\"] = 0\n    df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n    df[\"target\"] = 0\n    df.loc[df['B-coref'] == 1, \"target\"] = 1\n    df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n    print(df.target.value_counts())\n    return df\n\ndf_train = pd.concat([\n    pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\"),\n    pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\n], axis=0)\ndf_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\ndf_train = extract_target(df_train)\ndf_test = extract_target(df_test)\nsample_sub = pd.read_csv(\"..\/input\/sample_submission_stage_1.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","578d2fa5":"df_train[\"text_length\"] = df_train.Text.str.len()\ndf_test[\"text_length\"] = df_test.Text.str.len()\ndf_train.sort_values(\"text_length\", inplace=True)\ndf_test.sort_values(\"text_length\", inplace=True)","16b5b934":"df_train[\"A-offset\"].max(), df_train[\"B-offset\"].max(), df_train[\"Pronoun-offset\"].max()","578b6978":"def tokenize(row, tokenizer):\n    break_points = sorted(\n        [\n            (\"A\", row[\"A-offset\"], row[\"A\"]),\n            (\"B\", row[\"B-offset\"], row[\"B\"]),\n            (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n        ], key=lambda x: x[0]\n    )\n    tokens, spans, current_pos = [], {}, 0\n    for name, offset, text in break_points:\n        tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n        # Make sure we do not get it wrong\n        assert row[\"Text\"][offset:offset+len(text)] == text\n        # Tokenize the target\n        tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n        spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n        tokens.extend(tmp_tokens)\n        current_pos = offset + len(text)\n    tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n    assert spans[\"P\"][0] == spans[\"P\"][1]\n    return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            self.y = df.target.values.astype(\"uint8\")\n        \n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            tokens, offsets = tokenize(row, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n        \n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx]\n\n    \ndef collate_examples(batch, truncate_len=450):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"    \n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets\n    labels = torch.LongTensor(transposed[2])\n    return token_tensor, offsets, labels","aaba9b61":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n)","0b8dabbb":"class FeatureExtractionModel(nn.Module):\n    \"\"\"To extract features from BERT.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device, use_layer: int = -2):\n        super().__init__()\n        self.device = device\n        self.use_layer = use_layer\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.span_extractor = EndpointSpanExtractor(\n            self.bert_hidden_size, \"x+y\"\n        ).to(device)            \n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        offsets = offsets.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=True)\n        bert_outputs = bert_outputs[self.use_layer]\n        spans_contexts = self.span_extractor(\n            bert_outputs, \n            offsets[:, :4].reshape(-1, 2, 2)\n        )\n        emb_P = torch.gather(\n            bert_outputs, 1,\n            offsets[:, [4]].unsqueeze(2).expand(-1, -1, self.bert_hidden_size)\n        ).squeeze(1)\n        return spans_contexts, emb_P","d61f8393":"model = FeatureExtractionModel(BERT_MODEL, torch.device(\"cuda:0\"), use_layer=-2)\n# Make it deterministic\n_ = model.eval()","8285086e":"def extract_features(loader):\n    spc, embp = [], []\n    with torch.no_grad():\n        for token, offsets in tqdm_notebook(loader):\n            spans_contexts, emb_P = model(token, offsets)\n            spc.append(spans_contexts.cpu())\n            embp.append(emb_P.cpu())\n    return torch.cat(spc, dim=0), torch.cat(embp, dim=0)","4c1a6295":"train_ds = GAPDataset(df_train, tokenizer, labeled=False)\ntrain_loader = DataLoader(\n    train_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\nspc_train, embp_train = extract_features(train_loader)\nys_train = torch.from_numpy(df_train.target.values)\nspc_train.size(), embp_train.size(), ys_train.size()","cb1b59c3":"test_ds = GAPDataset(df_test, tokenizer, labeled=False)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\nspc_test, embp_test = extract_features(test_loader)\nys_test = torch.from_numpy(df_test.target.values)\nspc_test.size(), embp_test.size(), ys_test.size()","1a84c2cb":"torch.save([spc_train, embp_train, ys_train], \"train.pkl\")\ntorch.save([spc_test, embp_test, ys_test], \"test.pkl\")","e54e2e9f":"spc_train, embp_train, ys_train = torch.load(\"train.pkl\")\nspc_test, embp_test, ys_test = torch.load(\"test.pkl\")","92ce6ce7":"assert np.array_equal(df_test.target, ys_test.numpy())\nassert np.array_equal(df_train.target, ys_train.numpy())","b68db6ab":"class GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\".\/cache\/logs\/\", log_level=logging.INFO,\n        checkpoint_dir=\".\/cache\/model_cache\/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.6f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    def snapshot(self):\n        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir \/ \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n        self.logger.info(\"Saving checkpoint %s...\", target_path)\n        assert Path(target_path).exists()\n        return loss    ","83dc2fdf":"class GAPScoreModel(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, bert_hidden_size: int, device: torch.device):\n        super().__init__()\n        self.device = device   \n        self.bert_hidden_size = bert_hidden_size\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(bert_hidden_size * 3),\n            nn.Dropout(0.5),\n            nn.Linear(bert_hidden_size * 3, 64),           \n            nn.ReLU(),\n            nn.BatchNorm1d(64),             \n            nn.Dropout(0.5),\n            nn.Linear(64, 1)\n        ).to(device)\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                nn.init.kaiming_normal_(module.weight)\n                print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n                \n    def forward(self, spans_contexts, emb_P):\n        # A + P\n        act_AP = self.fc(torch.cat([\n            spans_contexts[:, 0], emb_P, emb_P * spans_contexts[:, 0, :]\n        ], dim=1))\n        # B + P\n        act_BP = self.fc(torch.cat([\n            spans_contexts[:, 1], emb_P, emb_P * spans_contexts[:, 1, :]\n        ], dim=1))        \n        return torch.cat([\n            act_AP,\n            act_BP,\n            torch.zeros_like(act_AP)\n        ], dim=1)","d79f6470":"test_ds = TensorDataset(spc_test, embp_test, ys_test)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","6fc0aebd":"skf = StratifiedKFold(n_splits=5, random_state=3, shuffle=True)\n\nval_preds, test_preds, val_ys, val_losses = [], [], [], []\nfor train_index, valid_index in skf.split(df_train, ys_train.numpy()):\n    print(\"=\" * 20)\n    print(f\"Fold {len(val_preds) + 1}\")\n    print(\"=\" * 20)\n    train_ds = TensorDataset(spc_train[train_index], embp_train[train_index], ys_train[train_index])\n    val_ds = TensorDataset(spc_train[valid_index], embp_train[valid_index], ys_train[valid_index])\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=128,\n        num_workers=0,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=128,\n        num_workers=0,\n        pin_memory=True,\n        shuffle=False\n    )\n    model = GAPScoreModel(spc_train.size(2), torch.device(\"cuda:0\"))\n    optimizer = WeightDecayOptimizerWrapper(\n        torch.optim.Adam(model.parameters(), lr=1e-3),\n        0.01\n    )\n#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.005)\n    bot = GAPBot(\n        model, train_loader, val_loader,\n        optimizer=optimizer, echo=False,\n        avg_window=50\n    )\n    steps_per_epoch = len(train_loader) \n    n_steps = steps_per_epoch * 30\n    bot.train(\n        n_steps,\n        log_interval=steps_per_epoch \/\/ 1,\n        snapshot_interval=steps_per_epoch \/\/ 1,\n        scheduler=TriangularLR(\n            optimizer, 100, ratio=3, steps_per_cycle=n_steps)\n    )\n    # Load the best checkpoint\n    bot.load_model(bot.best_performers[0][1])\n    bot.remove_checkpoints(keep=0)    \n    val_preds.append(torch.softmax(bot.predict(val_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n    val_ys.append(df_train.iloc[valid_index].target.astype(\"uint8\").values)\n    val_losses.append(log_loss(val_ys[-1], val_preds[-1]))\n    bot.logger.info(\"Confirm val loss: %.4f\", val_losses[-1])\n    test_preds.append(torch.softmax(bot.predict(test_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())","b3de2b8f":"val_losses","1ba9c851":"final_test_preds = np.mean(test_preds, axis=0)\n# final_test_preds = torch.stack(test_preds, dim=0).mean(dim=0).numpy()\nfinal_test_preds.shape","645a5a1a":"log_loss(df_test.target, final_test_preds)","34a50eca":"# Create submission file\ndf_sub = pd.DataFrame(final_test_preds, columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID.values\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","ffd1b365":"Sort the data to make feature extraction slightly faster:","7e446f29":"## Extract Features from BERT","242100f7":"\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead.","32ec0c25":"This one just shameless rips off the score layer in [Chanhu's kernel](https:\/\/www.kaggle.com\/chanhu\/bert-score-layer-lb-0-475)(go upvote his kernel!), but is rather a watered-down one since I did not implement distance features.","7260d595":"## Train a Model based on the Extracted Features"}}