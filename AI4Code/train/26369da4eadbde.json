{"cell_type":{"a4ac1f62":"code","dc0c7dae":"code","dff86ba0":"code","3f2d5d93":"code","6909c85e":"code","1838d3da":"code","20fc1f7f":"code","1a875558":"code","8074aa5b":"code","669c2acb":"code","6ae20482":"code","f799fbe2":"code","02fde75a":"code","21ac51d2":"code","7759bfcf":"code","a265830e":"code","18b1b57f":"code","333fe6b5":"code","9faf7376":"code","1334cb1b":"code","ece96cf3":"code","1a8bd91f":"code","388bafcf":"code","00dc2e70":"code","26c3cafe":"code","68f3927e":"code","3ddaa553":"code","bdbccaf8":"code","c164ba83":"code","7540ae5a":"code","748f13a5":"code","1766d85b":"code","fc10d67a":"code","57831cac":"code","6267f565":"code","6b44fd0c":"code","c73d2da2":"code","28ec3191":"code","73547010":"code","05172828":"code","4017de63":"code","7f03603b":"code","b6cc472a":"code","71ad6e62":"code","eeb816bf":"code","818fa373":"code","0e597629":"code","f871a344":"code","f1ee13d5":"code","80ef3a8a":"code","41d5e1a9":"code","d5e9ff15":"code","cc78084f":"code","fef87f02":"code","0667c245":"code","85510df6":"code","1a94127c":"code","074d3cba":"code","5128d077":"code","2012f5cc":"code","134b9370":"code","86207280":"code","76bbab16":"code","474fa92f":"code","1b860bbd":"code","d45a6bae":"code","c697028a":"code","0d39f1a2":"code","07bee1ca":"code","5fe78aed":"code","50d3e63c":"code","ab47eca7":"code","7ac47573":"code","2068e81d":"code","3c45f3c5":"code","5480a975":"code","5c2b36cc":"code","fc4b7bd8":"markdown","b1afa156":"markdown","51dabc63":"markdown","34940643":"markdown","5256aeb8":"markdown","43a4e6dd":"markdown","21c61b17":"markdown"},"source":{"a4ac1f62":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n\n#supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dc0c7dae":"#import data\naccident_data = pd.read_csv('..\/input\/us-accidents\/US_Accidents_Dec20_Updated.csv')","dff86ba0":"#look at dtype\naccident_data.info()","3f2d5d93":"#print number and percentage of null entries per variable\nprint('Null values per variable')\nfor column in accident_data.columns:\n    print('{}: {} ({}%)'.format(column,pd.isnull(accident_data[column]).sum(),(pd.isnull(accident_data[column]).sum()\/len(accident_data))*100))","6909c85e":"#look at distribution of data\naccident_data.describe()","1838d3da":"#look at formatting of entries\naccident_data.head()","20fc1f7f":"#looking to see ID format towards end\naccident_data.tail()","1a875558":"#checking to see if the missing end lat\/long are due having same start lat\/long\naccident_data[(accident_data.Start_Lat == accident_data.End_Lat) & (accident_data.Start_Lng == accident_data.End_Lng)]","8074aa5b":"#check to see if missing values are in same rows\naccident_data[np.logical_xor(accident_data.End_Lat.isna(),accident_data.End_Lng.isna()) == True]","669c2acb":"#look through some of the variables with low number of unique entries\n#Side has ' '\n#Wind direction has repeats with different spellings\n#Weather condition has some repeats (e.g., 'heavy rain shower' and 'heavy rain showers')\nfor col in accident_data.columns:\n    print('{}: {}'.format(col,accident_data[col].unique()))","6ae20482":"#investigating Side value, will replace with mode\naccident_data[accident_data.Side == ' ']","f799fbe2":"#create dataframe of variables collected at airport for airport_code with null values (i.e., no airport information)\nweather_info = accident_data[accident_data.Airport_Code.isna()][['Airport_Code','Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)',\n       'Humidity(%)', 'Pressure(in)','Visibility(mi)', 'Wind_Direction',\n       'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition']]","02fde75a":"#all records that are missing an airport_code are also missing weather information\nprint('Percentage null when Airport_Code is null:')\nfor weather in weather_info.columns:\n    print('{}: {}%'.format(weather, (weather_info[weather].isna().count()\/weather_info.Airport_Code.isna().count())*100))","21ac51d2":"del weather_info\ngc.collect()","7759bfcf":"#null percipitation values are not necessarily due to no rain\naccident_data[accident_data['Precipitation(in)'] == 0]","a265830e":"#looking for cases where Humidity is zero and Percipitation is null (i.e., precipitation should be set to zero)\naccident_data[[a and b for a,b in zip(accident_data['Humidity(%)'] == 0,accident_data['Precipitation(in)'].isna())]][['Humidity(%)','Precipitation(in)']]","18b1b57f":"#looking for cases where wind speed is zero and wind direction is null (i.e., no wind to have a wind direction)\naccident_data[[a and b for a,b in zip(accident_data['Wind_Speed(mph)'] == 0,accident_data['Wind_Direction'].isna())]][['Wind_Speed(mph)','Wind_Direction']]","333fe6b5":"#looking to see if wind_speed is zero, is wind_chill null\n#for these records, null wind_chill entries should be replaced with temperature\naccident_data[[a and b for a,b in zip(accident_data['Wind_Speed(mph)'] == 0,accident_data['Wind_Chill(F)'].isna())]][['Wind_Speed(mph)','Wind_Chill(F)']]","9faf7376":"#looking to see if wind_speed is zero if there is a wind chill\naccident_data[[a and not b for a,b in zip(accident_data['Wind_Speed(mph)'] == 0,accident_data['Wind_Chill(F)'].isna())]][['Wind_Speed(mph)','Wind_Chill(F)']]","1334cb1b":"#looking for null values in both wind speed and wind chill\naccident_data[[a and b for a,b in zip(accident_data['Wind_Speed(mph)'].isna(),accident_data['Wind_Chill(F)'].isna())]][['Wind_Speed(mph)','Wind_Chill(F)']]","ece96cf3":"#parse dates and times\nimport datetime\naccident_data['datetime_start_parsed'] = pd.to_datetime(accident_data['Start_Time'], format=\"%Y-%m-%d %H:%M:%S\")\naccident_data['datetime_end_parsed'] = pd.to_datetime(accident_data['End_Time'], format=\"%Y-%m-%d %H:%M:%S\")\naccident_data['datetime_weathertime_parsed'] = pd.to_datetime(accident_data['Weather_Timestamp'], format=\"%Y-%m-%d %H:%M:%S\")","1a8bd91f":"#for reference when categorizing based on numerical and categorical\naccident_data.columns","388bafcf":"#categorized variables based on numerical, categoric, and datetime\n#description is excluded\n\nnumerical = [\n       'Start_Lat', 'Start_Lng',\n       'End_Lat', 'End_Lng', 'Distance(mi)','Number','Temperature(F)', 'Wind_Chill(F)',\n       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)','Wind_Speed(mph)', 'Precipitation(in)'\n]\n\ncategorical = [\n    'Severity','Street', 'Side', 'City', 'Zipcode', 'County', 'State','Country','Timezone','Airport_Code','Wind_Direction','Weather_Condition', 'Amenity',\n       'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n       'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal',\n       'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n       'Astronomical_Twilight'\n]\n\ndatetime = [\n        'datetime_start_parsed', 'datetime_end_parsed', 'datetime_weathertime_parsed'\n]","00dc2e70":"#since dataset is over 1 GB, taking a sample of 1% of data for analysis\naccident_sample = accident_data.sample(int(len(accident_data)\/100))\nprint('Percentage of data sampled: {}%'.format((len(accident_sample)\/len(accident_data))*100))","26c3cafe":"#histograms for categorical data\nfor i in accident_sample[categorical].columns:\n    plt.figure(figsize=(28, 6))\n    sns.barplot(edgecolor='black',x=accident_sample[categorical][i].value_counts().index,y=accident_sample[categorical][i].value_counts())\n    plt.xlabel(i)\n    plt.ylabel('number of accidents')\n    plt.show()","68f3927e":"#re-plotting top 20 of difficult to see plots\ncat_top10 = ['Street','City', 'Zipcode', 'County', 'Airport_Code', 'Weather_Condition']\nfor i in cat_top10:\n    cat_grouped = accident_sample.groupby(i)['ID'].nunique().nlargest(20)\n    plt.figure(figsize=(28, 6))\n    sns.barplot(edgecolor='black',x=cat_grouped.index,y=cat_grouped)\n    plt.xlabel(i)\n    plt.ylabel('number of accidents')\n    plt.show()","3ddaa553":"del cat_grouped\ngc.collect()","bdbccaf8":"#histograms for numerical data\nfor i in accident_sample[numerical].columns:\n    plt.figure(figsize=(28, 6))\n    plt.hist(accident_sample[numerical][i], edgecolor='black')\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of accidents')\n    plt.show()","c164ba83":"#histograms for day datetime data\nfor i in accident_sample[datetime].columns:\n    plt.figure(figsize=(28, 6))\n    plt.hist(accident_sample[datetime][i].dt.day, edgecolor='black')\n    plt.xticks()\n    plt.xlabel('{} day'.format(i))\n    plt.ylabel('number of accidents')\n    plt.show()","7540ae5a":"#histograms for month datetime data\nfor i in accident_sample[datetime].columns:\n    plt.figure(figsize=(28, 6))\n    plt.hist(accident_sample[datetime][i].dt.month, edgecolor='black')\n    plt.xticks()\n    plt.xlabel('{} month'.format(i))\n    plt.ylabel('number of accidents')\n    plt.show()","748f13a5":"#histograms for year datetime data\nfor i in accident_sample[datetime].columns:\n    plt.figure(figsize=(28, 6))\n    plt.hist(accident_sample[datetime][i].dt.year, edgecolor='black', align='left', rwidth=0.5, bins=[2016, 2017,2018,2019,2020, 2021])\n    plt.xlabel('{} year'.format(i))\n    plt.xticks(ticks=[2016, 2017,2018,2019,2020, 2021])\n    plt.ylabel('number of accidents')\n    plt.show()","1766d85b":"#histograms for hour datetime data\nfor i in accident_sample[datetime].columns:\n    plt.figure(figsize=(28, 6))\n    plt.hist(accident_sample[datetime][i].dt.hour, edgecolor='black')\n    plt.xticks()\n    plt.xlabel('{} hour'.format(i))\n    plt.ylabel('number of accidents')\n    plt.show()","fc10d67a":"#histograms for minute datetime data\nfor i in accident_sample[datetime].columns:\n    plt.figure(figsize=(28, 6))\n    plt.hist(accident_sample[datetime][i].dt.minute, edgecolor='black')\n    plt.xticks()\n    plt.xlabel('{} minute'.format(i))\n    plt.ylabel('number of accidents')\n    plt.show()","57831cac":"#histograms for second datetime data\nfor i in accident_sample[datetime].columns:\n    plt.figure(figsize=(28, 6))\n    plt.hist(accident_sample[datetime][i].dt.second, edgecolor='black')\n    plt.xticks()\n    plt.xlabel('{} second'.format(i))\n    plt.ylabel('number of accidents')\n    plt.show()","6267f565":"del accident_sample\ngc.collect()","6b44fd0c":"#creating a dataframe for creating a correlation heatmap\nnumerical_heatmap = [\n       'Start_Lat', 'Start_Lng',\n       'End_Lat', 'End_Lng', 'Distance(mi)','Temperature(F)', 'Wind_Chill(F)',\n       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)','Wind_Speed(mph)', 'Precipitation(in)'\n]\n\naccident_numerical_datetime = accident_data[numerical_heatmap]\n\n#seperating dates and times into seperate columns\nfor col in ['datetime_start_parsed', 'datetime_end_parsed']:\n    accident_numerical_datetime['{} day'.format(col)] = accident_data[col].dt.day\n    accident_numerical_datetime['{} month'.format(col)] = accident_data[col].dt.month\n    accident_numerical_datetime['{} year'.format(col)] = accident_data[col].dt.year\n    accident_numerical_datetime['{} hour'.format(col)] = accident_data[col].dt.hour\n    accident_numerical_datetime['{} minute'.format(col)] = accident_data[col].dt.minute\n    accident_numerical_datetime['{} second'.format(col)] = accident_data[col].dt.second","c73d2da2":"#heat map to find extreme positive and negative correlations\nplt.figure(figsize=(20, 20))\nsns.heatmap(accident_numerical_datetime.corr(), annot=True)\nplt.title('Correlation Heatmap for numerical Variables', fontdict={'fontsize':12}, pad=12);","28ec3191":"#temperature and wind chill have a 0.99 correlation\n#since this dataset is combined from two sources, it looks like each had different methods for recording temperature and wind chill\naccident_sample = accident_data.sample(int(len(accident_data)\/100))\nsns.scatterplot(x='Wind_Chill(F)', y='Temperature(F)', data=accident_sample, palette='Set1')\nplt.show()","73547010":"del accident_sample\ngc.collect()","05172828":"del accident_numerical_datetime\ngc.collect()","4017de63":"#import geopandas and geoplot libraries\nimport geopandas\nimport geoplot as gplt\nimport geoplot.crs as gcrs","7f03603b":"#create dataframe of variables I'm interested in looking at on a map plus lat\/lng variables\nseverity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]","b6cc472a":"#create a GeoDataFrame\ngdf_severity = geopandas.GeoDataFrame(\n    severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))","71ad6e62":"#generate map with hue based on severity\ngdfs_sample = gdf_severity.sample(int(len(gdf_severity)\/10))\nprint('{} accidents'.format(int(len(gdfs_sample))))\n\ncontiguous_usa = geopandas.read_file(gplt.datasets.get_path('contiguous_usa'))\nax = gplt.polyplot(\n    contiguous_usa,\n    projection=gcrs.AlbersEqualArea(),\n    figsize=(20, 20)\n)\ngplt.pointplot(gdfs_sample, ax=ax, hue=gdfs_sample.Severity, scale=gdfs_sample.Severity, legend=True, legend_var='hue')\nplt.show()","eeb816bf":"#generate map with hue based on temperature\ngdfs_sample = gdf_severity.sample(int(len(gdf_severity)\/10))\nprint('{} accidents'.format(int(len(gdfs_sample))))\n\ncontiguous_usa = geopandas.read_file(gplt.datasets.get_path('contiguous_usa'))\nax = gplt.polyplot(\n    contiguous_usa,\n    projection=gcrs.AlbersEqualArea(),\n    figsize=(20, 20)\n)\ngplt.pointplot(gdfs_sample, ax=ax, hue=gdfs_sample['Temperature(F)'],scale=gdfs_sample['Temperature(F)'], legend=True, legend_var='hue')\nplt.show()","818fa373":"#generate a map with hue based on wind chill\ngdfs_sample = gdf_severity.sample(int(len(gdf_severity)\/10))\nprint('{} accidents'.format(int(len(gdfs_sample))))\n\ncontiguous_usa = geopandas.read_file(gplt.datasets.get_path('contiguous_usa'))\nax = gplt.polyplot(\n    contiguous_usa,\n    projection=gcrs.AlbersEqualArea(),\n    figsize=(20, 20)\n)\ngplt.pointplot(gdfs_sample, ax=ax, hue=gdfs_sample['Wind_Chill(F)'],scale=gdfs_sample['Wind_Chill(F)'], legend=True, legend_var='hue')\nplt.show()","0e597629":"del gdf_severity\ngc.collect()","f871a344":"del gdfs_sample\ngc.collect()","f1ee13d5":"del contiguous_usa\ngc.collect()","80ef3a8a":"#replace space with most common entry\naccident_data.Side = accident_data.Side.replace(' ','R')","41d5e1a9":"#view sorted Weather_Conditions to determine changes to be made\nweather_condition = accident_data.Weather_Condition.unique().astype('str')\nweather_condition.sort()\nweather_condition","d5e9ff15":"del weather_condition\ngc.collect()","cc78084f":"#make changes to Weather_Condition since some entries are similar\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('Thunder','Thunderstorm')\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('T-Storm','Thunderstorm')\n\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('T-Storm \/ Windy','Thunderstorm \/ Windy')\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('Thunder \/ Windy','Thunderstorm \/ Windy')\n\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('Heavy Rain Shower','Heavy Rain')\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('Heavy Rain Showers','Heavy Rain')\n\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('Light Rain Shower','Light Rain')\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('Light Rain Showers','Light Rain')\n\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('Rain Shower','Rain')\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('Rain Showers','Rain')","fef87f02":"#view unique values of Wind_Direction\naccident_data.Wind_Direction.unique()","0667c245":"#replace entries in Wind_Direction to be consistent with notation\naccident_data.Wind_Direction = accident_data.Wind_Direction.replace('West','W')\naccident_data.Wind_Direction = accident_data.Wind_Direction.replace('Variable','VAR')\naccident_data.Wind_Direction = accident_data.Wind_Direction.replace('South','S')\naccident_data.Wind_Direction = accident_data.Wind_Direction.replace('Calm','CALM')\naccident_data.Wind_Direction = accident_data.Wind_Direction.replace('East','E')\naccident_data.Wind_Direction = accident_data.Wind_Direction.replace('North','N')","85510df6":"#make all zipcodes 5 digit                           \nzipcodes = pd.DataFrame(accident_data[[not a for a in accident_data['Zipcode'].isna()]]['Zipcode'].str[:5])\nzipindex = np.array(accident_data[accident_data.Zipcode.notnull()].index)\naccident_data.loc[zipindex,'Zipcode'] = zipcodes.loc[:,'Zipcode']","1a94127c":"#remove start\/end\/weather time (already parsed)  and remove Number which has 65% null values\naccident_data = accident_data.drop(['ID','Start_Time','End_Time', 'Weather_Timestamp','Number'], axis=1)","074d3cba":"#remove Number from numerical array\nnumerical.remove('Number')","5128d077":"#N\/A Precipitation must mean virtually no rain\naccident_data[accident_data.Weather_Condition == 'N\/A Precipitation']['Precipitation(in)'].mean()","2012f5cc":"#replace N\/A Precipitation with NaN to be imputed with most frequent\naccident_data.Weather_Condition = accident_data.Weather_Condition.replace('N\/A Precipitation', np.NaN)","134b9370":"#determine index then lat\/lng for missing city, zipcode, and timezone data\ncitynullindex = np.array(accident_data[accident_data.City.isna()].index)\ncitynulllatlng = accident_data.loc[citynullindex, ['Start_Lat','Start_Lng']]\n\nzipnullindex = np.array(accident_data[accident_data.Zipcode.isna()].index)\nzipnulllatlng = accident_data.loc[zipnullindex, ['Start_Lat','Start_Lng']]\n\nacnullindex = np.array(accident_data[accident_data.Airport_Code.isna()].index)\n\ntznullindex = np.array(accident_data[accident_data.Timezone.isna()].index)\ntznulllatlng = accident_data.loc[tznullindex, ['Start_Lat','Start_Lng']]","86207280":"#initialize Nominatim for finding missing cities and zipcodes based on lat\/lng\nfrom geopy.geocoders import Nominatim\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")","76bbab16":"#fill in missing cities\nfor i in citynullindex:\n    location = geolocator.reverse(citynulllatlng.loc[i,'Start_Lat'].astype('str')+\",\"+citynulllatlng.loc[i,'Start_Lng'].astype('str'))\n    address = location.raw['address']\n    city = address.get('city', '')\n    accident_data.loc[i,'City'] = city\n    \n#fill in missing zipcodes\nfor i in zipnullindex:\n    location = geolocator.reverse(zipnulllatlng.loc[i,'Start_Lat'].astype('str')+\",\"+zipnulllatlng.loc[i,'Start_Lng'].astype('str'))\n    address = location.raw['address']\n    zipcode = address.get('postcode')\n    accident_data.loc[i,'Zipcode'] = zipcode\n    #if cannot locate zipcode with geolocator, fill in zipcode mode for the state\n    if zipcode == None:\n        accident_data.loc[i,'Zipcode'] = accident_data[accident_data['State'] == accident_data.loc[i,'State']]['Zipcode'].mode(dropna=True)[0]","474fa92f":"#replace missing airport_code data with most common airport_code for each state\nac_states = pd.DataFrame(accident_data.loc[acnullindex,'State'].unique(), columns=['State'])\nfor i in range(0,len(ac_states)):\n    ac_states.loc[i,'Mode'] = accident_data[accident_data.State == ac_states.loc[i,'State']]['Airport_Code'].mode(dropna=True)[0]\nfor i in acnullindex:\n    accident_data.loc[i,'Airport_Code'] = ac_states[ac_states['State'] == accident_data.loc[i,'State']]['Mode'].tolist()[0]","1b860bbd":"#install timezonefinder\n!pip install timezonefinder","d45a6bae":"#fill in missing time zones\nfrom timezonefinder import TimezoneFinder\nobj = TimezoneFinder()\nfor i in tznullindex:\n    timezone = obj.timezone_at(lng=tznulllatlng.loc[i,'Start_Lng'], lat=tznulllatlng.loc[i,'Start_Lat'])\n    accident_data.loc[i,'Timezone'] = timezone","c697028a":"#copy values from start lat\/lng to end lat\/lng for null values\nlatnull = np.array(accident_data[np.isnan(accident_data.End_Lat)].index)\naccident_data.loc[latnull,'End_Lat'] = accident_data.loc[latnull,'Start_Lat']\nlngnull = np.array(accident_data[np.isnan(accident_data.End_Lng)].index)\naccident_data.loc[lngnull,'End_Lng'] = accident_data.loc[lngnull,'Start_Lng']\n\n#replace missing wind chill data with temperature\nwcnull = np.array(accident_data[np.isnan(accident_data['Wind_Chill(F)'])].index)\naccident_data.loc[wcnull,'Wind_Chill(F)'] = accident_data.loc[i,'Temperature(F)']\n\n#replace missing temperature data with wind chill\ntempnull = np.array(accident_data[np.isnan(accident_data['Temperature(F)'])].index)\naccident_data.loc[wcnull,'Temperature(F)'] = accident_data.loc[i,'Wind_Chill(F)']\n\n#replace missing data for when the weather was collected based on end time of the accident\nweathertimenull = np.array(accident_data[np.isnan(accident_data.datetime_weathertime_parsed)].index)\naccident_data.loc[weathertimenull,'datetime_weathertime_parsed'] = accident_data.loc[weathertimenull,'datetime_end_parsed']","0d39f1a2":"#variables to impute with median or mode strategy\nmedian_impute = ['Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Precipitation(in)']\nmode_impute = ['Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight', 'Nautical_Twilight','Astronomical_Twilight']","07bee1ca":"#mode impute (SimpleImputer for loop is too slow)\nwdnull = np.array(accident_data[accident_data.Wind_Direction.isna()].index)\naccident_data.loc[wdnull,'Wind_Direction'] = 'CALM'\n\nwcnull = np.array(accident_data[accident_data.Weather_Condition.isna()].index)\naccident_data.loc[wcnull,'Weather_Condition'] = 'Fair'\n\nssnull = np.array(accident_data[accident_data.Sunrise_Sunset.isna()].index)\naccident_data.loc[ssnull,'Sunrise_Sunset'] = 'Day'\n\nctnull = np.array(accident_data[accident_data.Civil_Twilight.isna()].index)\naccident_data.loc[ssnull,'Civil_Twilight'] = 'Day'\n\nntnull = np.array(accident_data[accident_data.Nautical_Twilight.isna()].index)\naccident_data.loc[ssnull,'Nautical_Twilight'] = 'Day'\n\natnull = np.array(accident_data[accident_data.Astronomical_Twilight.isna()].index)\naccident_data.loc[atnull,'Astronomical_Twilight'] = 'Day'","5fe78aed":"#impute variables with median values\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\nimputer1 = SimpleImputer(missing_values=np.nan, strategy='median')\naccident_data_median_fit = imputer1.fit_transform(accident_data[median_impute])\naccident_data_median = imputer1.transform(accident_data_median_fit)\n\naccident_data[median_impute] = pd.DataFrame(accident_data_median)","50d3e63c":"#print number and percentage of null entries per variable\nprint('Null values per variable')\nfor column in accident_data.columns:\n    print('{}: {} ({}%)'.format(column,pd.isnull(accident_data[column]).sum(),(pd.isnull(accident_data[column]).sum()\/len(accident_data))*100))","ab47eca7":"#seperate variables with dates and times into seperate columns\nfor col in datetime:\n    accident_data['{} day'.format(col)] = accident_data[col].dt.day\n    accident_data['{} month'.format(col)] = accident_data[col].dt.month\n    accident_data['{} year'.format(col)] = accident_data[col].dt.year\n    accident_data['{} hour'.format(col)] = accident_data[col].dt.hour\n    accident_data['{} minute'.format(col)] = accident_data[col].dt.minute\n    accident_data['{} second'.format(col)] = accident_data[col].dt.second\n    numerical.append('{} day'.format(col))\n    numerical.append('{} month'.format(col))\n    numerical.append('{} year'.format(col))\n    numerical.append('{} hour'.format(col))\n    numerical.append('{} minute'.format(col))\n    numerical.append('{} second'.format(col))\n\n#remove datetime columns from dataframe     \naccident_data = accident_data.drop(datetime, axis=1)","7ac47573":"#sample dataframe\naccident_sample = accident_data[numerical].sample(int(len(accident_data)\/100))","2068e81d":"#standardize sample\naccident_data_for_PCA_standardized = (accident_sample - accident_sample.mean(axis=0)) \/ accident_sample.std(axis=0)\n\nfrom sklearn.decomposition import PCA\n\n# Create principal components\npca = PCA(6)\naccident_data_pca = pca.fit_transform(accident_data_for_PCA_standardized)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(accident_data_pca.shape[1])]\naccident_data_pca = pd.DataFrame(accident_data_pca, columns=component_names)\n\naccident_data_pca.head()","3c45f3c5":"loadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=numerical,  # and the rows are the original features\n)\nloadings\n\n#PC1: year\n#PC2: hour\n#PC3: month\n#PC4: day\n#PC5: latitude\n#PC6: longitude","5480a975":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\nplot_variance(pca);","5c2b36cc":"#create variables for the change in lat and lng for each accident\naccident_data['abs_lng_change'] = abs(accident_data.End_Lng - accident_data.Start_Lng)\naccident_data['abs_lat_change'] = abs(accident_data.End_Lat - accident_data.Start_Lat)\n\n#add new variables to numerical variables list\nnumerical.append('abs_lng_change')\nnumerical.append('abs_lat_change')","fc4b7bd8":"# EDA","b1afa156":"# GeoPandas Maps","51dabc63":"# Data Cleaning and Feature Engineering","34940643":"**Import libraries and data**","5256aeb8":"# PCA","43a4e6dd":"**Correlation heatmap**","21c61b17":"# Feature Engineering"}}