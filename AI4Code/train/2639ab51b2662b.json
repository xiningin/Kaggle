{"cell_type":{"fd8a0b8b":"code","54aee4bb":"code","e312b6bc":"code","1f0a672e":"code","9001f20f":"code","93c70634":"code","e543518f":"code","68816850":"code","38f2a2d4":"code","9aa1f206":"code","0b20669b":"code","3d349a45":"code","19e6b61f":"code","d1cd3933":"code","75ddd6c6":"code","50cc116a":"code","39278da8":"code","e81da244":"code","60621940":"code","63dd5adb":"code","6d6efbac":"code","9cd151d9":"code","03d82d4b":"code","fa15674e":"code","f0a9540a":"code","4ae7dd6b":"code","467a22fa":"code","3943d754":"code","bff913ff":"code","aca75fc8":"code","812937e2":"code","5f514589":"code","08fc9e8c":"code","211667cc":"code","4836c28f":"code","4bfb20d4":"code","30b67484":"code","43b4946e":"code","59a3e89e":"code","234b080d":"code","a0bfbf7a":"code","01c110b6":"code","970690ca":"code","3b424023":"code","4e912260":"code","9ba0c7c4":"code","47fb8dea":"code","7aed58fe":"code","8cc6bd7b":"code","5b728a97":"code","953c6c48":"code","4fac1752":"code","6c880b5e":"code","55f56fd5":"code","410c7358":"code","8df4b170":"code","3c3a7cb8":"code","23b87789":"code","8a239f71":"code","fc3702f6":"code","6b1728a0":"code","d688e4ed":"code","d90c4fd0":"code","15048123":"code","9070e239":"code","493e3ced":"code","cfa13724":"code","afd978e8":"code","50612d75":"code","4b1627da":"code","1ba504c5":"code","f0a81cb8":"code","db2c1c9f":"code","025a8634":"markdown","f4b3c4ac":"markdown","0644eb48":"markdown","76d27e13":"markdown","98c27aa5":"markdown","697ca465":"markdown","3f2df39c":"markdown","68949828":"markdown","8e61fde1":"markdown","13c8bbe6":"markdown","bde2d423":"markdown","11e24d03":"markdown","16f8d728":"markdown","5c11c8a0":"markdown","5dd912fe":"markdown","1f85e8e0":"markdown","84947ca7":"markdown","64bb5e7c":"markdown","fcec1352":"markdown","dc92b9e8":"markdown","4bd41c95":"markdown","29d24644":"markdown","5ee14242":"markdown","22757579":"markdown","107e3c82":"markdown","9ce899d1":"markdown","ce2710ca":"markdown"},"source":{"fd8a0b8b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\n\nSEED = 15","54aee4bb":"train = pd.read_csv(\"..\/input\/ai-hack1-preprocessed-data\/train_pre1.csv\")\ntest_data = pd.read_csv(\"..\/input\/ai-hack1-preprocessed-data\/test_pre1.csv\")","e312b6bc":"test_cate = test_data.copy()\ntest = test_data.copy()","1f0a672e":"test = test.drop(columns = ['continent', 'major', 'country'])","9001f20f":"X = train.drop(columns = ['continent', 'nerdy', 'major', 'country'])\ny = train['nerdy']","93c70634":"from sklearn.model_selection import train_test_split","e543518f":"X_train, X_val, y_train, y_val = train_test_split(X, y , test_size = 0.25, random_state = 15)\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.4, random_state = 15)","68816850":"from sklearn.preprocessing import MinMaxScaler\ncol2 = ['que_score', 'tipi_score', 'total_time_min', 'introelapse', 'surveyelapse', 'testelapse', 'screenh', 'screenw']\nmx = MinMaxScaler()\nfor col in col2:\n    X_train[col] = mx.fit_transform(np.array(X_train[col]).reshape(-1,1))\n    X_val[col] = mx.transform(np.array(X_val[col]).reshape(-1,1))\n    X_test[col] = mx.transform(np.array(X_test[col]).reshape(-1,1))\n    test[col] = mx.transform(np.array(test[col]).reshape(-1,1))\n    test_cate[col] = mx.transform(np.array(test_cate[col]).reshape(-1,1))","38f2a2d4":"import xgboost as xgb","9aa1f206":"# params = {\n #   'learning_rate' :[0.01, 0.05, 0.1, 0.2, 0.3],\n #   'min_child_weight': [1 , 3, 5, 7],\n #   'max_depth' : [5, 7, 9, 11, 13],\n #   'n_estimators': [100, 300, 500],}\n\n# reg = xgb.XGBRegressor()\n\n# random_search = RandomizedSearchCV(reg, params, scoring = 'neg_root_mean_squared_error', cv = 5, verbose = 3, n_iter = 5, n_jobs = -1)\n# random_search.fit(X_train, y_train)\n# random_search.best_params_","0b20669b":"xgbr = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.01, max_delta_step=0, max_depth=5,\n             min_child_weight=7,monotone_constraints='()',\n             n_estimators=500, n_jobs=8, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nxgbr.fit(X_train, y_train)\n\ny_xgb_train = xgbr.predict(X_train)\ny_xgb_val = xgbr.predict(X_val)\ny_xgb_test = xgbr.predict(X_test)\ntest_xgb = xgbr.predict(test)\n\nprint(mean_squared_error(y_train,y_xgb_train, squared = False))\nprint(mean_squared_error(y_val,y_xgb_val, squared = False))\nprint(mean_squared_error(y_test,y_xgb_test, squared = False))","3d349a45":"test_xgb","19e6b61f":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier","d1cd3933":"# rf = RandomForestRegressor()\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# max_features = ['auto', 'sqrt']\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# min_samples_split = [2, 5, 10]\n# min_samples_leaf = [1, 2, 4]\n# bootstrap = [True, False]\n\n# random_grid = {'n_estimators': n_estimators,\n#               'max_features': max_features,\n#               'max_depth': max_depth,\n#               'min_samples_split': min_samples_split,\n#               'min_samples_leaf': min_samples_leaf,\n#               'bootstrap': bootstrap}\n\n# random_search_rf = RandomizedSearchCV(rf, random_grid, cv = 10, scoring = 'neg_root_mean_squared_error', verbose = 3, n_jobs = -1)\n\n# random_search_rf.fit(X_train, y_train)\n# random_search_rf.best_params_","75ddd6c6":"rf = RandomForestRegressor(max_depth=90, min_samples_leaf=2, min_samples_split=10,\n                      n_estimators=400, bootstrap = True, max_features = 'auto')\n\nrf.fit(X_train, y_train)\n\ny_rf_train = rf.predict(X_train)\ny_rf_val = rf.predict(X_val)\ny_rf_test = rf.predict(X_test)\ntest_rf = rf.predict(test)\n\nprint(mean_squared_error(y_train,y_rf_train, squared = False))\nprint(mean_squared_error(y_val,y_rf_val, squared = False))\nprint(mean_squared_error(y_test,y_rf_test, squared = False))","50cc116a":"test_rf","39278da8":"from lightgbm import LGBMRegressor, LGBMClassifier","e81da244":"# params_lgbm = {\n#    'num_leaves' : [5 , 10, 15, 20, 30, 40],\n#    'min_data_in_leaf' : [10, 15, 20, 25, 30],\n#    'learning_rate' : [0.01, 0.1, 0.3, 0.05],\n#    'num_iterations' : [100, 300, 500, 700],\n# }\n\n# lgbm = LGBMRegressor(is_unbalance = True)\n\n# random_search_lgbm = RandomizedSearchCV(lgbm, params_lgbm, scoring = 'neg_root_mean_squared_error', cv = 10, verbose = 3,n_jobs = -1)\n# random_search_lgbm.fit(X_train, y_train)\n# random_search_lgbm.best_params_","60621940":"lgbm = LGBMRegressor(learning_rate=0.05, min_data_in_leaf=5, num_iterations=700,\n              num_leaves=5)\n\nlgbm.fit(X_train, y_train)\n\ny_lgbm_train = lgbm.predict(X_train)\ny_lgbm_val = lgbm.predict(X_val)\ny_lgbm_test = lgbm.predict(X_test)\ntest_lgbm = lgbm.predict(test)\n\nprint(mean_squared_error(y_train,y_lgbm_train, squared = False))\nprint(mean_squared_error(y_val,y_lgbm_val, squared = False))\nprint(mean_squared_error(y_test,y_lgbm_test, squared = False))","63dd5adb":"test_lgbm","6d6efbac":"from catboost import CatBoostRegressor","9cd151d9":"cat_x = train.drop(columns = ['nerdy', 'continent'])\ncat_y = train['nerdy']\n\ncat_X_train, cat_X_val, cat_y_train, cat_y_val = train_test_split(cat_x, cat_y , test_size = 0.25, random_state = 15)\ncat_X_val, cat_X_test, cat_y_val, cat_y_test = train_test_split(cat_X_val, cat_y_val, test_size = 0.4, random_state = 15)\n\ncat_feat = np.where(cat_X_train.dtypes == np.object)[0]","03d82d4b":"cat = CatBoostRegressor(learning_rate=0.01 , depth=7, iterations = 1500)\n\ncat.fit(cat_X_train, cat_y_train, cat_features = cat_feat, eval_set = (cat_X_val, cat_y_val), plot = True)\n\ny_cat_train = cat.predict(cat_X_train)\ny_cat_val = cat.predict(cat_X_val)\ny_cat_test = cat.predict(cat_X_test)\ntest_cat = cat.predict(test_cate.drop(columns = ['continent']))\n\nprint(mean_squared_error(cat_y_train,y_cat_train, squared = False))\nprint(mean_squared_error(cat_y_val,y_cat_val, squared = False))\nprint(mean_squared_error(cat_y_test,y_cat_test, squared = False))","fa15674e":"test_cat","f0a9540a":"train_le = pd.read_csv(\"..\/input\/ai-hack1-preprocessed-data\/train_prele.csv\")\ntest_le = pd.read_csv(\"..\/input\/ai-hack1-preprocessed-data\/test_prele.csv\")","4ae7dd6b":"X_le = train_le.drop(columns = ['nerdy', 'major', 'country'])\ny_le = train_le['nerdy']\n\ntest_le = test_le.drop(columns = ['major', 'country'])","467a22fa":"X_train_le, X_val_le, y_train_le, y_val_le = train_test_split(X_le, y_le , test_size = 0.25, random_state = 15)\nX_val_le, X_test_le, y_val_le, y_test_le = train_test_split(X_val_le, y_val_le, test_size = 0.4, random_state = 15)","3943d754":"xgbr = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.01, max_delta_step=0, max_depth=5,\n             min_child_weight=7,monotone_constraints='()',\n             n_estimators=500, n_jobs=8, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nxgbr.fit(X_train_le, y_train_le)\n\ny_xgb_train_le = xgbr.predict(X_train_le)\ny_xgb_val_le = xgbr.predict(X_val_le)\ny_xgb_test_le = xgbr.predict(X_test_le)\ntest_xgb_le = xgbr.predict(test_le)\n\nprint(mean_squared_error(y_train_le,y_xgb_train_le, squared = False))\nprint(mean_squared_error(y_val_le,y_xgb_val_le, squared = False))\nprint(mean_squared_error(y_test_le,y_xgb_test_le, squared = False))","bff913ff":"test_xgb_le","aca75fc8":"rf = RandomForestRegressor(max_depth=90, min_samples_leaf=2, min_samples_split=10,\n                      n_estimators=400, bootstrap = True, max_features = 'auto')\n\nrf.fit(X_train_le, y_train_le)\n\ny_rf_train_le = rf.predict(X_train_le)\ny_rf_val_le = rf.predict(X_val_le)\ny_rf_test_le = rf.predict(X_test_le)\ntest_rf_le = rf.predict(test_le)\n\nprint(mean_squared_error(y_train_le,y_rf_train_le, squared = False))\nprint(mean_squared_error(y_val_le,y_rf_val_le, squared = False))\nprint(mean_squared_error(y_test_le,y_rf_test_le, squared = False))","812937e2":"xgbclf = xgb.XGBClassifier()\n\nxgbclf.fit(X_train, y_train)\n\ny_xgbclf_train = xgbclf.predict(X_train)\ny_xgbclf_val = xgbclf.predict(X_val)\ny_xgbclf_test = xgbclf.predict(X_test)\ntest_xgbclf = xgbclf.predict(test)\n\nprint(mean_squared_error(y_train,y_xgbclf_train, squared = False))\nprint(mean_squared_error(y_val,y_xgbclf_val, squared = False))\nprint(mean_squared_error(y_test,y_xgbclf_test, squared = False))","5f514589":"lgbmclf = LGBMClassifier()\n\nlgbmclf.fit(X_train, y_train)\n\ny_lgbmclf_train = lgbmclf.predict(X_train)\ny_lgbmclf_val = lgbmclf.predict(X_val)\ny_lgbmclf_test = lgbmclf.predict(X_test)\ntest_lgbmclf = lgbmclf.predict(test)\n\nprint(mean_squared_error(y_train,y_lgbmclf_train, squared = False))\nprint(mean_squared_error(y_val,y_lgbmclf_val, squared = False))\nprint(mean_squared_error(y_test,y_lgbmclf_test, squared = False))","08fc9e8c":"rfclf = RandomForestClassifier(max_depth = 9)\n\nrfclf.fit(X_train, y_train)\n\ny_rfclf_train = rfclf.predict(X_train)\ny_rfclf_val = rfclf.predict(X_val)\ny_rfclf_test = rfclf.predict(X_test)\ntest_rfclf = rfclf.predict(test)\n\nprint(mean_squared_error(y_train,y_rfclf_train, squared = False))\nprint(mean_squared_error(y_val,y_rfclf_val, squared = False))\nprint(mean_squared_error(y_test,y_rfclf_test, squared = False))","211667cc":"stack_val = np.column_stack((y_xgb_val, y_cat_val, y_lgbm_val, y_rf_val, y_xgb_val_le, y_rf_val_le, y_xgbclf_val, y_lgbmclf_val, y_rfclf_val))\nstack_test = np.column_stack((y_xgb_test, y_cat_test, y_lgbm_test, y_rf_test, y_xgb_test_le, y_rf_test_le, y_xgbclf_test, y_lgbmclf_test, y_rfclf_test))\ntest_stack = np.column_stack((test_xgb, test_cat, test_lgbm, test_rf, test_xgb_le, test_rf_le, test_xgbclf, test_lgbmclf, test_rfclf))","4836c28f":"stack_train, stack_val, y_stack_train, y_stack_val = train_test_split(stack_val, y_val, test_size = 0.15, random_state = SEED)","4bfb20d4":"xgb_stack1 = xgb.XGBRegressor(n_estimators = 25, max_depth = 3, learning_rate = 0.3)\n\nxgb_stack1.fit(stack_train, y_stack_train)\n\ny_stack1_xgb_train = xgb_stack1.predict(stack_train)\ny_stack1_xgb_val = xgb_stack1.predict(stack_val)\ny_stack1_xgb_test = xgb_stack1.predict(stack_test)\ntest_stack1_Xgb = xgb_stack1.predict(test_stack)\n\nprint(mean_squared_error(y_stack_train,y_stack1_xgb_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_xgb_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_xgb_test, squared = False))","30b67484":"xgb2_stack1 = xgb.XGBRegressor(n_estimators = 100, max_depth = 4, learning_rate = 0.05)\n\nxgb2_stack1.fit(stack_train, y_stack_train)\n\ny_stack1_xgb2_train = xgb2_stack1.predict(stack_train)\ny_stack1_xgb2_val = xgb2_stack1.predict(stack_val)\ny_stack1_xgb2_test = xgb2_stack1.predict(stack_test)\ntest_stack1_Xgb2 = xgb2_stack1.predict(test_stack)\n\nprint(mean_squared_error(y_stack_train,y_stack1_xgb2_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_xgb2_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_xgb2_test, squared = False))","43b4946e":"xgb3_stack1 = xgb.XGBRegressor(n_estimators = 200, max_depth = 3, learning_rate = 0.06)\n\nxgb3_stack1.fit(stack_train, y_stack_train)\n\ny_stack1_xgb3_train = xgb3_stack1.predict(stack_train)\ny_stack1_xgb3_val = xgb3_stack1.predict(stack_val)\ny_stack1_xgb3_test = xgb3_stack1.predict(stack_test)\ntest_stack1_Xgb3 = xgb3_stack1.predict(test_stack)\n\nprint(mean_squared_error(y_stack_train,y_stack1_xgb3_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_xgb3_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_xgb3_test, squared = False))","59a3e89e":"y_stack1_xgbf_train = (y_stack1_xgb_train + y_stack1_xgb2_train + y_stack1_xgb3_train)\/3\ny_stack1_xgbf_val = (y_stack1_xgb_val + y_stack1_xgb2_val + y_stack1_xgb3_val)\/3\ny_stack1_xgbf_test = (y_stack1_xgb_test + y_stack1_xgb2_test + y_stack1_xgb3_test)\/3","234b080d":"print(mean_squared_error(y_stack_train,y_stack1_xgbf_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_xgbf_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_xgbf_test, squared = False))","a0bfbf7a":"test_stack1_xgbf = (test_stack1_Xgb + test_stack1_Xgb2 + test_stack1_Xgb3)\/3","01c110b6":"test_stack1_xgbf","970690ca":"rf_stack1 = RandomForestRegressor(max_depth = 3, n_estimators = 25, random_state = 15, min_samples_split = 4)\n\nrf_stack1.fit(stack_train, y_stack_train)\n\ny_stack1_rf_train = rf_stack1.predict(stack_train)\ny_stack1_rf_val = rf_stack1.predict(stack_val)\ny_stack1_rf_test = rf_stack1.predict(stack_test)\ntest_stack1_rf = rf_stack1.predict(test_stack)\n\nprint(mean_squared_error(y_stack_train,y_stack1_rf_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_rf_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_rf_test, squared = False))","3b424023":"rf2_stack1 = RandomForestRegressor(max_depth = 4, n_estimators = 25, random_state = 18, min_samples_split = 4)\n\nrf2_stack1.fit(stack_train, y_stack_train)\n\ny_stack1_rf2_train = rf2_stack1.predict(stack_train)\ny_stack1_rf2_val = rf2_stack1.predict(stack_val)\ny_stack1_rf2_test = rf2_stack1.predict(stack_test)\ntest_stack1_rf2 = rf2_stack1.predict(test_stack)\n\nprint(mean_squared_error(y_stack_train,y_stack1_rf2_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_rf2_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_rf2_test, squared = False))","4e912260":"rf3_stack1 = RandomForestRegressor(max_depth = 4, n_estimators = 50, random_state = 2021)\n\nrf3_stack1.fit(stack_train, y_stack_train)\n\ny_stack1_rf3_train = rf3_stack1.predict(stack_train)\ny_stack1_rf3_val = rf3_stack1.predict(stack_val)\ny_stack1_rf3_test = rf3_stack1.predict(stack_test)\ntest_stack1_rf3 = rf3_stack1.predict(test_stack)\n\nprint(mean_squared_error(y_stack_train,y_stack1_rf3_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_rf3_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_rf3_test, squared = False))","9ba0c7c4":"rf4_stack1 = RandomForestRegressor(max_depth = 4, n_estimators = 100, random_state = 2025)\n\nrf4_stack1.fit(stack_train, y_stack_train)\n\ny_stack1_rf4_train = rf4_stack1.predict(stack_train)\ny_stack1_rf4_val = rf4_stack1.predict(stack_val)\ny_stack1_rf4_test = rf4_stack1.predict(stack_test)\ntest_stack1_rf4 = rf4_stack1.predict(test_stack)\n\nprint(mean_squared_error(y_stack_train,y_stack1_rf4_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_rf4_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_rf4_test, squared = False))","47fb8dea":"y_stack1_rff_train = (y_stack1_rf_train + y_stack1_rf2_train + y_stack1_rf3_train + y_stack1_rf4_train)\/4\ny_stack1_rff_val = (y_stack1_rf_val + y_stack1_rf2_val + y_stack1_rf3_val + y_stack1_rf4_val)\/4\ny_stack1_rff_test = (y_stack1_rf_test + y_stack1_rf2_test + y_stack1_rf3_test + y_stack1_rf4_test)\/4","7aed58fe":"print(mean_squared_error(y_stack_train,y_stack1_rff_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_rff_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_rff_test, squared = False))","8cc6bd7b":"test_stack1_rff = (test_stack1_rf + test_stack1_rf2 + test_stack1_rf3 + test_stack1_rf4)\/4\ntest_stack1_rff","5b728a97":"cat_stack1 = CatBoostRegressor()\n\ncat_stack1.fit(stack_train, y_stack_train)\n\ny_stack1_cat_train = cat_stack1.predict(stack_train)\ny_stack1_cat_val = cat_stack1.predict(stack_val)\ny_stack1_cat_test = cat_stack1.predict(stack_test)\ntest_stack1_cat = cat_stack1.predict(test_stack)\n\nprint(mean_squared_error(y_stack_train,y_stack1_cat_train, squared = False))\nprint(mean_squared_error(y_stack_val,y_stack1_cat_val, squared = False))\nprint(mean_squared_error(y_test,y_stack1_cat_test, squared = False))","953c6c48":"stack2_val = np.column_stack((y_stack1_xgbf_val, y_stack1_rff_val, y_stack1_cat_val, stack_val)) \nstack2_test = np.column_stack((y_stack1_xgbf_test, y_stack1_rff_test, y_stack1_cat_test, stack_test)) \ntest_stack2 = np.column_stack((test_stack1_xgbf, test_stack1_rff, test_stack1_cat, test_stack))","4fac1752":"lgbm_stack2 = LGBMRegressor(num_leaves = 7, max_depth = 3, n_estimators = 25)\n\nlgbm_stack2.fit(stack2_val , y_stack_val)\n\ny_stack2_lgbm_val = lgbm_stack2.predict(stack2_val)\ny_stack2_lgbm_test = lgbm_stack2.predict(stack2_test)\ntest_stack2_lgbm = lgbm_stack2.predict(test_stack2)\n\nprint(mean_squared_error(y_stack_val,y_stack2_lgbm_val, squared = False))\nprint(mean_squared_error(y_test,y_stack2_lgbm_test, squared = False))","6c880b5e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor","55f56fd5":"lr_stack2 = LinearRegression()\n\nlr_stack2.fit(stack2_val , y_stack_val)\n\ny_stack2_lr_val = lr_stack2.predict(stack2_val)\ny_stack2_lr_test = lr_stack2.predict(stack2_test)\ntest_stack2_lr = lr_stack2.predict(test_stack2)\n\nprint(mean_squared_error(y_stack_val,y_stack2_lr_val, squared = False))\nprint(mean_squared_error(y_test,y_stack2_lr_test, squared = False))","410c7358":"kn_stack2 = KNeighborsRegressor()\n\nkn_stack2.fit(stack2_val , y_stack_val)\n\ny_stack2_kn_val = kn_stack2.predict(stack2_val)\ny_stack2_kn_test = kn_stack2.predict(stack2_test)\ntest_stack2_kn = kn_stack2.predict(test_stack2)\n\nprint(mean_squared_error(y_stack_val,y_stack2_kn_val, squared = False))\nprint(mean_squared_error(y_test,y_stack2_kn_test, squared = False))","8df4b170":"y_test_2level = (y_stack2_kn_test + y_stack2_lgbm_test + y_stack2_lr_test)\/3","3c3a7cb8":"test_2level = (test_stack2_kn + test_stack2_lgbm + test_stack2_lr)\/3","23b87789":"stack_test_3 = np.column_stack((stack2_test, y_stack2_kn_test, y_stack2_lgbm_test, y_stack2_lr_test, y_test_2level))\ntest_stack3 = np.column_stack((test_stack2, test_stack2_kn , test_stack2_lgbm, test_stack2_lr, test_2level))","8a239f71":"stack_test_3.shape","fc3702f6":"from tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf","6b1728a0":"def nn1():\n    \n    model = Sequential()\n    \n    model.add(Dense(16, input_dim = 16, activation = 'relu', kernel_initializer = 'normal'))\n    \n    model.add(Dense(1, kernel_initializer = 'normal', activation = 'linear'))\n    \n    model.summary()\n    \n    return model","d688e4ed":"nn1 = nn1()","d90c4fd0":"nn1.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\ncallback = tf.keras.callbacks.EarlyStopping(monitor='mean_squared_error', patience=7)\nnn1.fit(stack_test_3, y_test, epochs=20, validation_split = 0.2, callbacks=callback, shuffle = True)","15048123":"pred_test_stack3 = nn1.predict(test_stack3)","9070e239":"pred_test_stack3 = pred_test_stack3.reshape(-1)","493e3ced":"lr_3 = LinearRegression()\n\nlr_3.fit(stack_test_3, y_test)","cfa13724":"mean_squared_error(lr_3.predict(stack_test_3), y_test)","afd978e8":"test_stack3.shape","50612d75":"test_lr_3 = lr_3.predict(test_stack3)","4b1627da":"test_lr_3","1ba504c5":"test_stack3_pred = (pred_test_stack3 + test_lr_3)\/2","f0a81cb8":"test_stack3_pred.shape","db2c1c9f":"res8 = pd.DataFrame(test_stack3_pred, columns=['nerdy'])\nres8['id'] = test['id']\nres8 = res8[['id', 'nerdy']]\nres8.to_csv(\"res8.csv\",index=False)\nres8","025a8634":"As we know that our target value has only integer values from 0-7. Hence this dataset can also be treated as Multi-Class Classification and I would train some classifiers to get a better and more diverse stacked output.","f4b3c4ac":"### XGB","0644eb48":"First we will train several models like XGB, Random Forest, LightGBM, CatBoost, etc. and save their predictions","76d27e13":"### Classfiiers","98c27aa5":"## Base Models","697ca465":"#### XGB","3f2df39c":"### Stack - Level1","68949828":"### RF","8e61fde1":"### LGBM","13c8bbe6":"Finally completing our 3rd Level Stacking","bde2d423":"### RF","11e24d03":"#### RF","16f8d728":"Bagging XGB Models","5c11c8a0":"This method helped me to get a public leaderboard score of 1.112 while Private Leaderboard score of 1.101. This proves that this method (if performed carefully) does not lead to overfitting and actually is very helpful to get a good score.","5dd912fe":"### CatBoost","1f85e8e0":"Now we will stack predictions of the 1st level models and also the predictions of Base Models to make our 2nd Layer of Stacking","84947ca7":"#### Min- Max Scaler","64bb5e7c":"Now I have trained all the models, and now I will stack each of there outputs and make a new array, whose columns would be the predictions of the Base Models. All the Base models would have a very correlated predications, and hence we would train shallow networks to overcome overfitting.","fcec1352":"#### XGB","dc92b9e8":"Bagging Random Forest Models","4bd41c95":"#### CATBOOST","29d24644":"This data is from my last notebook of Feature Engineering and Preprocessing. Here I have used various models and Stacked them to get a good score","5ee14242":"### LabelEnconded DF","22757579":"It's a good practice to feed a bit different data to our models while stacking so that different models exploit different relations and we get a better result. Hence , uptil now, I was using One-Hot encoded Continent column, but below I will train some model with that column to be Label Encoded.","107e3c82":"### 3rd Level","9ce899d1":"#### Splitting the dataset","ce2710ca":"### 2nd Level"}}