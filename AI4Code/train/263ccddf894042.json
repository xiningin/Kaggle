{"cell_type":{"21b91b98":"code","636c601e":"code","e62448da":"code","97038c46":"code","226aa498":"code","d74951da":"code","32d62580":"code","91d7ab7f":"code","a4622e12":"code","bc7ebdf1":"code","dc9f38af":"code","bce720f4":"code","9dd7cf6d":"code","96e550a9":"code","bfea0426":"code","771b10e2":"code","50c32185":"code","436ab0fb":"code","ab2ba0f4":"code","12675700":"code","47105612":"code","52747e61":"code","693e6766":"code","909c1dce":"code","2165fa95":"code","117882bc":"code","58e24d68":"code","c521aed4":"code","50e95b2d":"code","75560e09":"code","b6e57e02":"code","6b0a15cc":"code","f7f26114":"code","5ac82b31":"code","507084f6":"code","ea8307ef":"code","eee21c80":"code","28abb7da":"code","c7bc4c5b":"code","92aef865":"code","e51d1a31":"code","8d2ed5bc":"code","0ef1eaa1":"code","bd010626":"code","c82e2af9":"code","cbcf397d":"code","d1c893c0":"code","0be5c0d8":"code","5611f846":"code","7d3830b5":"code","426dd94b":"code","743a5852":"code","8a2cd43e":"code","50ebee4b":"code","0d4a54c4":"code","cebf41cd":"markdown","462d8634":"markdown","f71c93e4":"markdown","21be81ba":"markdown","7b400d14":"markdown","e4ec2117":"markdown","1bf3bf2a":"markdown","3c5e090e":"markdown","4dd3bbec":"markdown","d14abbda":"markdown","64be8cc3":"markdown","df205db0":"markdown","985fbfb0":"markdown","2187c306":"markdown","83ba165a":"markdown"},"source":{"21b91b98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\n\nimport plotly as py\nfrom plotly import tools\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nsns.set()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","636c601e":"data=pd.read_csv(r'..\/input\/pakistans-largest-ecommerce-dataset\/Pakistan Largest Ecommerce Dataset.csv',parse_dates=['created_at'])\nprint(\"Data Dimensions are: \", data.shape)\ndata.columns=data.columns.str.replace(\" \", \"_\").str.lower()\nprint(\"Columns: \", data.columns)","e62448da":"data.head()","97038c46":"#Data exploration\ndata.info()","226aa498":"data.dtypes","d74951da":"# Quantifying null values\nprint(data.isnull().sum())","32d62580":"# Visualizing the percentage of null values\ndata.isnull().mean().plot.bar(figsize=(12,6))\nplt.ylabel('Percentage of missing values')\nplt.xlabel('Variables')\nplt.title('Quantifying missing data')","91d7ab7f":"data.drop([\"unnamed:_21\", \"unnamed:_22\", \"unnamed:_23\", \"unnamed:_24\", \"unnamed:_25\"], axis = 1, inplace=True)\ndata.dropna(how='all', axis=0, inplace=True)\ndata.rename(columns={\"_mv_\": \"mv\", \"category_name_1\": \"category_name\"}, inplace = True)","a4622e12":"# Quantifying null values\nprint(data.isnull().sum())","bc7ebdf1":"# inspect unique values - categorical variable\ndata['status'].unique()","dc9f38af":"data.groupby('bi_status')['status'].value_counts()","bce720f4":"data['status'] = data['status'].replace(r'\\\\N', 'Cancelled', regex=True)","9dd7cf6d":"# For simplicity we can merge all statuses into Completed, Cancelled and Refund\ndict_status = {'Completed':['complete','closed','received','cod','paid','exchange','payment_review','pending','processing','holded','pending_paypal'],'Refund':['order_refunded','refund'], 'Cancelled':['canceled','fraud',np.nan]}\nfor n in range(len(dict_status)):\n    key,value=list(dict_status.items())[n]\n    data['status'].replace(value, key,inplace=True)\n    n+=1","96e550a9":"data['status'].value_counts()","bfea0426":"# bar plots for status categorical variables\n\ndata['status'].value_counts().plot.bar()\nplt.xticks(rotation=0)\nplt.ylabel('Count')\nplt.title('Status  - Distinct Counts')","771b10e2":"# Check for relevance of different features of dataset\ndata[['created_at','working_date','sku','qty_ordered','price','grand_total','mv','discount_amount','sales_commission_code','customer_id']].head()","50c32185":"data.drop(['working_date', 'mv', 'increment_id','bi_status','sales_commission_code'],axis=1,inplace=True)","436ab0fb":"data.info()","ab2ba0f4":"data.isnull().sum()","12675700":"print(\"Count Different Categories: \")\nprint(data['category_name'].value_counts(dropna=False)) ","47105612":"# Extracting all unique categories for category value'\\N'\nskunique=data[data['category_name']==r'\\N']['sku'].unique().tolist()","52747e61":"#Now we check for sku's found against '\\N' category in other categories \nsku_nil=data[data['sku'].isin(skunique)]\nsku_nil['category_name'].value_counts()","693e6766":"# We found sku with '\\N' category also in categories as mentioned above\n# Updating the sku category '\\N' where same sku found in above categories and all remaining values of sku with Other's  \ndict_sku={}\ncat=[\"Men's Fashion\",'Others','Superstore','Mobiles & Tablets',\"Women's Fashion\",'Entertainment','Appliances']\nfor n in cat:\n    dict_sku[n]= n\nfor n in range(len(dict_sku)):\n    key,value=list(dict_sku.items())[n]\n    dict_sku[key]=sku_nil[sku_nil['category_name']== key]['sku'].unique().tolist()\n    data.loc[((data['sku'].isin(dict_sku[key])) & (data['category_name']==r'\\N')),'category_name']= key\n    n+=1\n","909c1dce":"data.loc[(data['category_name']==r'\\N'),'category_name']= 'Others'\ndata['category_name'] = data['category_name'].replace(np.nan, 'Others', regex=True)","2165fa95":"# Categories after updating all '\\N' and null categories\ndata['category_name'].value_counts(dropna=False).plot.bar(figsize=(12,6))\n#plt.xticks(rotation=0)\nplt.ylabel('Sales')\nplt.title('Sales Category Wise')","117882bc":"data[data['sku'].isnull()]","58e24d68":"# As out of 20 null sku values,  most of the order statuses are either cancelled or refund with grand_total 0.\n# Replacing nan values with sku_nan\ndata['sku'].fillna(\"sku_nan\",inplace=True)","c521aed4":"#Checking null values in columns customer_id and customer_since\ndata[data['customer_id'].isnull()]","50e95b2d":"#For customer_id null most of the orders are with order status as cancelled or refund\n#We can replace the null customer_id with value 0 and customer_since with 2018\ndata['customer_id'].fillna(\"0\",inplace=True)\ndata['customer_since'].fillna(\"1-2018\",inplace=True)","75560e09":"#Replacing values < 0 with 0\ndata.loc[(data['grand_total']< 0), 'grand_total']=0","b6e57e02":"print(data.describe())","6b0a15cc":"# Filter all competed orders for distibution spread \ndataordercomp= data[data['status']=='Completed']","f7f26114":"# Checking order statuses against price  \ng=(sns.FacetGrid(data[data['price'] >0],\n               hue='status', height=5, aspect=2)\n  .map(sns.kdeplot, 'price', shade=True)\n .add_legend()\n)\nplt.xlim(0,5000)\n","5ac82b31":"#For orders above 5000 we check order completion and cancel ratio\ng=(sns.FacetGrid(data[data['price'] >5000],\n               hue='status', height=5, aspect=2)\n  .map(sns.kdeplot, 'price', shade=True)\n .add_legend()\n)\n\nplt.xlim(5000,60000)\nplt.ylim(0.00000,0.00006)\n","507084f6":"# Scatter plot of category ad price for completed orders\nfig = px.scatter(dataordercomp, x='category_name', y='price')\nfig.show()","ea8307ef":"def displaygraph(df,groupbycol,groupbycriteria,title,xlabel,ylabel):\n    lsfilterby=df[groupbycriteria].unique().tolist()\n    df_dict = {}\n    for n in lsfilterby:\n        df1 = df[df[groupbycriteria]==n]\n        df_dict[n] = df1.groupby(groupbycol)[groupbycriteria].count().reset_index()\n\n    fig = go.Figure()\n\n    for n in lsfilterby:\n        fig.add_trace(go.Scatter(x=df_dict[n][groupbycol], y=df_dict[n][groupbycriteria],\n                    mode='lines+markers',\n                    name=n))    \n\n    fig.update_layout(\n    title_text=title, # title of plot\n    xaxis_title_text=xlabel, # xaxis label\n    yaxis_title_text=ylabel, # yaxis label\n    \n    )\n    fig.show()\n    ","eee21c80":"#Order Month year vs order status\n\ndisplaygraph(data,'m-y','status','Order Status by Month','Month','Count')","28abb7da":"#Order category vs order status\n\ndisplaygraph(data,'category_name','status','Order Status by Category','Order Category','Count')","c7bc4c5b":"#Order payment method vs order status\n\ndisplaygraph(data,'payment_method','status','Order Status by Payment Method','Payment Method','Count')","92aef865":"dataordercomp['price']=dataordercomp['price'].astype(int)\nprint('Categories Highest Income')\nprint(dataordercomp.groupby('category_name')['price'].sum().sort_values(ascending=False))\ndataordercomp.groupby('category_name')['price'].sum().sort_values(ascending=False).plot.bar(figsize=(12,6))#value_counts().sort_values()\nplt.ylabel('Sales')\nplt.title('Categories Sorted by Sales')\n","e51d1a31":"dict_traces={}\nlscategory=[\"Mobiles & Tablets\",'Appliances','Entertainment']\nfor n in lscategory:\n        dfcat=dataordercomp[dataordercomp['category_name']==n]\n        dict_traces[n] = go.Violin(x=dfcat['category_name'],y =dfcat['price'], meanline_visible = True)\n\nfig.update_layout(\n    title_text='Distribution of Top Income generating Categories',\n    yaxis_title_text='price', # yaxis label\n    showlegend=False\n    )        \n\n  \ncdata=[]\nfor n in range(len(dict_traces)):\n    key,value=list(dict_traces.items())[n]\n    cdata.append(dict_traces[key])\n    n+=1\n\nfig = go.Figure(data = cdata)\niplot(fig)","8d2ed5bc":"pd.set_option('mode.chained_assignment', None)\ndataordercomp['quarter']=dataordercomp['created_at'].dt.quarter\ndataordercomp['quarter']=dataordercomp['quarter'].astype(str)\ndataordercomp['quarter'].replace(['1','2','3','4'],['Q1','Q2','Q3','Q4'],inplace=True)\ndataordercomp['q-y']=dataordercomp['fy'].astype(str)+'-'+dataordercomp['quarter']","0ef1eaa1":"qigroup=dataordercomp.groupby(['q-y'])['price'].sum().reset_index(name='Total Sales')#plot.bar()\nfig = px.bar(qigroup, x='q-y', y='Total Sales', title=\"Quarterly Sales\",labels={'q-y':'Quarters'})\nfig.show()","bd010626":"qcgroup=dataordercomp.groupby(['q-y','category_name'],sort=True)['price'].sum().reset_index(name='Sales')\nqcgroup=qcgroup.sort_values(by = ['Sales'], ascending=[True])\nfig = px.bar(qcgroup, x='q-y', y='Sales', color='category_name',title=\"Quarterly Category Wise Sale\",labels={'q-y':'Quarters','category_name':'Category name'})\nfig.show()","c82e2af9":"dfq4=dataordercomp.loc[(dataordercomp['q-y']=='FY18-Q4'),['sku','category_name','price']].value_counts()[:15].reset_index(name='Number of Orders')\ndfq4=dfq4.sort_values(by = ['Number of Orders'], ascending=[False])\nprint('Top 15 selling Items of Quarter-FY18-Q4')\nprint(dfq4)\nfig = px.bar(dfq4, x='category_name', y='Number of Orders', color='sku',title=\"Top 15 selling Items of Quarter-FY18-Q4\",labels={'q-y':'Quarters','category_name':'Category name'},hover_name=\"sku\", hover_data=[\"category_name\", \"Number of Orders\", \"price\"])\nfig.show()","cbcf397d":"qorders=dataordercomp.groupby('q-y')['category_name'].value_counts().reset_index(name='Number of Orders')\nqorders=qorders.sort_values(by = ['Number of Orders'], ascending=[True])\nfig = px.bar(qorders, x='q-y', y='Number of Orders', color='category_name',title=\"Quarterly Category Wise Sale-Number of Orders\",labels={'q-y':'Quarters','category_name':'Category name'})\nfig.show()","d1c893c0":"def human_format(num):\n    magnitude = 0\n    while abs(num) >= 1000:\n        magnitude += 1\n        num \/= 1000.0\n    # add more suffixes if you need them\n    return '%.2f%s' % (num, ['', 'K', 'M', 'B', 'T', 'P'][magnitude])\n","0be5c0d8":"pabovefifty=dataordercomp.loc[(dataordercomp['price']>50000),'price']\npabovefifty['price']=dataordercomp.loc[(dataordercomp['price']>50000),'price']\npbins=pd.qcut(pabovefifty['price'], q=5).value_counts().reset_index(name='No of Orders')#plot.bar(figsize=(12,6))\npbins.rename(columns = {'index' : 'price'}, inplace = True)\npbins['price']=pbins['price'].astype(str)\n# plt.ylabel('Number of Orders')\n# plt.title('Price Bins For Orders having Price above Fifty Thousand- Total Sales: Rs%s' % human_format(pabovefifty['price'].sum()))\nfig = px.pie(pbins, values='No of Orders', names='price', title='Price Intervals For Orders having Price above Fifty Thousand- Total Sales: Rs%s' % human_format(pabovefifty['price'].sum()))\nfig.show()","5611f846":"pbelowfifty=dataordercomp.loc[(dataordercomp['price']<50000),'price']\npbelowfifty['price']=dataordercomp.loc[(dataordercomp['price']<50000),'price']\npbins=pd.qcut(pbelowfifty['price'], q=5).value_counts().reset_index(name='No of Orders')#.plot.bar(figsize=(12,6))\npbins.rename(columns = {'index' : 'price'}, inplace = True)\npbins['price']=pbins['price'].astype(str)\nfig = px.pie(pbins, values='No of Orders', names='price', title='Price Intervals For Orders having Price below Fifty Thousand- Total Sales: Rs%s' % human_format(pbelowfifty['price'].sum()))\nfig.show()","7d3830b5":"# Time based Sales analysis \ndf=pd.DataFrame({'order_count':dataordercomp.groupby(['created_at']).size()})\n","426dd94b":"weekly = df.resample('W').sum()\nweekly.plot(figsize=(12,6))\nplt.ylabel('Weekly Order count')\nplt.xlabel('Date');","743a5852":"daily = df.resample('D').sum()\ndaily.rolling(30, center=True).sum().plot(figsize=(12,6))\nplt.ylabel('Mean hourly order count')\nplt.xlabel('Date');","8a2cd43e":"by_weekday = df.groupby(df.index.dayofweek).mean()\nby_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']\nby_weekday.plot(figsize=(12,6)); ","50ebee4b":"daily = df.loc['11-2016'].resample('D').sum()\ndaily.plot(figsize=(12,6))\nplt.title('Daily Order count for November-2016')\nplt.ylabel('Order count')\nplt.xlabel('Date');","0d4a54c4":"daily = df.loc['11-2017'].resample('D').sum()\ndaily.plot(figsize=(12,6))\nplt.title('Daily Order count for November-2017')\nplt.ylabel('Order count')\nplt.xlabel('Date');","cebf41cd":"**Top 4 consistent profitable categories are Mobiles & Tablets, Appliances, Entertainment and Women's Fashion**","462d8634":"****We can drop the columns working date, mv, sales_commission_code & other irrelevant cols because we have all the required relevant information in columns created_at, grand_total and discount_amount ****","f71c93e4":"**From above observations it is concluded that all statuses falls under group Gross can be marked as Canceled, Net and Valid group of orders can be considered under complete category**","21be81ba":"**Checking null\/not defined values in categorical variable category_name**","7b400d14":"Let's check data types of columns","e4ec2117":"**Checking categorical variable sku for null entries**","1bf3bf2a":"Peak sales months are Nov 2016 and 2017, whereas Friday has on average highest orders as compared to other week days. This may be due to Friday sale.","3c5e090e":"**Checking for missing values**","4dd3bbec":"# **Exploring different consumer patterns**","d14abbda":"**Actual customer base for all top categories is below 40K with average price of around 11K for Mobiles & Tablets, 8,701 for Appliances and 17.7K for Entertainment. \nUpper fence for Mobiles & Tablets:34k\nUpper fence for Appliances:19.6k\nUpper fence for Entertainment:39.9k**","64be8cc3":"****Above visualization shows percentage of missing value for each column in dataset ****\n1. Columns Unnamed 21 to Unnamed 25 are 100% null\n2. Columns sales_commission_code has above 50% null values\n3. All the remaining columns have about 40% null values","df205db0":"# **Let's check columns for duplicate entries, non relevant data**","985fbfb0":"**Ratio of order cancellation is high for orders greater than 5000 but there is improvement in price range 20000 to 25000**","2187c306":"Working on further pattren exploration.  Please comment. ","83ba165a":"******Ratio of order cancellation and refund increases after price tag of 2000\/- "}}