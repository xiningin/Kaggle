{"cell_type":{"0a93f6cf":"code","916d292f":"code","a4066a67":"code","b738f79f":"code","2cec65e0":"code","5f58be77":"code","2723f275":"code","bbd7857b":"code","787ac75c":"code","67dce56b":"code","ea8735ec":"code","22c38f61":"code","1db08cd9":"code","59fc188e":"code","da3e31bc":"code","1ac2320c":"code","902cdfb8":"code","dd518d82":"code","cf64854d":"code","776ae64d":"code","2e3a836a":"code","efaa460b":"code","a4d6197c":"code","707a0c24":"code","b21b4a60":"code","d82ac6ce":"code","8edf96a4":"code","c89d3da5":"code","32b03a62":"code","f38afded":"code","5a506365":"code","1355d529":"code","04306a70":"code","22dd0b37":"code","21f10f59":"code","6a6da31f":"code","a238db02":"code","caf2ed57":"code","39de5854":"code","7048f731":"code","e3ff14aa":"code","5b8b4705":"code","6f6a5ab8":"code","3b65b74a":"code","fef4715b":"code","22597032":"code","0759fba2":"code","103a7813":"code","7f0dcc7c":"code","c8166ee2":"code","d084cbc1":"code","2993b10f":"code","9e2b7db5":"code","b1f3a9dc":"code","23de189b":"code","4de257fc":"markdown","3e8fc193":"markdown","c8ddec65":"markdown","411d7205":"markdown","da156ae1":"markdown","a08271c6":"markdown","6064f8b8":"markdown","8216eea3":"markdown","0743bc97":"markdown","00860d4c":"markdown","4dc066be":"markdown","00e0c36f":"markdown","035200db":"markdown","bafad8f5":"markdown","0c23649b":"markdown","9178623e":"markdown","11bb2a88":"markdown","107b0cf0":"markdown","b1be4a8d":"markdown","e4317d1b":"markdown","ebcf5ee0":"markdown","a80cc8fb":"markdown","a8349ac4":"markdown","a27fc9a6":"markdown","85ff5bc6":"markdown","8666ac27":"markdown","d363fc6e":"markdown","49cbc6e9":"markdown","3254f6df":"markdown","ba2fabe5":"markdown","b44ef236":"markdown","8ef2f019":"markdown","ce62267b":"markdown","d2d7465f":"markdown","fe11cff3":"markdown","1b13fc02":"markdown","ec8083fb":"markdown","3587e459":"markdown","11a865bb":"markdown","9d9ed92b":"markdown","50a77213":"markdown"},"source":{"0a93f6cf":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import norm, skew \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, LabelEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)","916d292f":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_train = df_train.iloc[:,:-1]\ny_train = df_train.iloc[:,-1]\nX_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')","a4066a67":"print('Nb of houses in train set: {}'.format(X_train.shape[0]))\nprint('Nb of houses in test set: {}'.format(X_test.shape[0]))","b738f79f":"def load_data(train=True):\n    # Read data\n    if train:\n      df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\n    else:\n      df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\n    # Preprocessing\n    df = to_categorical(df) # defined later\n    df = nan_impute(df) # defined later\n    df = special_clean(df) # defined later\n    df = aggregate_features(df) # defined later\n    df = remove_useless(df) # defined later\n\n    if train:\n      df = remove_outliers(df)\n\n    df = transform_features(df) # defined later\n    # Splitting\n    if train:\n      X_train = df.drop(['SalePrice'], axis=1)\n      y_train = np.log(df['SalePrice']) # log(y)\n      return X_train, y_train\n    else:\n      X_test = df\n      return X_test","2cec65e0":"# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \n                \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \n                \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n\n# The ordinal (ordered) categorical features \nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = [str(i) for i in range(1,11)]\n\nfeatures_ord_dict = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\nfeatures_ord = list(features_ord_dict.keys())\n\nfeatures_num = X_train.columns.difference(features_nom).difference(features_ord)\n\n# Needed for adding the 'NA' \nall_nan = pd.concat([X_train.isna().any(), X_test.isna().any()], axis=1)\nfeatures_with_nan = all_nan[all_nan.sum(axis=1) != False].index\n\ndef to_categorical(X):\n    df = X.copy()\n    # Nominal categories\n    for name in features_nom:\n      levels = list(df[name].dropna().unique().astype(str))\n      if name in features_with_nan:\n        levels = ['NA'] + levels\n      df[name] = pd.Categorical(df[name].astype(str), categories=levels, ordered=False)\n  \n    # Ordinal categories\n    for name, levels in features_ord_dict.items():\n      if name in features_with_nan:\n        levels = ['NA'] + levels\n      df[name] = pd.Categorical(df[name].astype(str), categories=levels, ordered=True)\n    return df","5f58be77":"X_train = to_categorical(X_train)\nX_test = to_categorical(X_test)","2723f275":"all_nan = pd.concat([(X_train.isna().sum(axis=0)\/X_train.shape[0]*100).rename('Train % NaN'),\n           (X_test.isna().sum(axis=0)\/X_test.shape[0]*100).rename('Test % NaN')],  axis=1)\\\n           .sort_values(['Train % NaN', 'Test % NaN'], ascending=False)\n\nfeatures_with_nan = all_nan[all_nan.sum(axis=1) != 0].index\n\nall_nan.style.background_gradient()","bbd7857b":"def compare_values_train_test(X_train, X_test):\n  '''\n  Categorical variable unique values comparaison for train and test sets\n  '''\n  unknown_in_test = pd.DataFrame(columns=['Train unique', 'NaN in train', 'UNK in test', 'NaN in test'])\n  for col in (features_nom + features_ord):\n    unique_train = list(X_train[col].unique())\n    unknown_in_test.loc[col, 'Train unique'] = unique_train\n    unknown_in_test.loc[col, 'NaN in train'] = X_train[col].isna().sum()\n    unknown_in_test.loc[col, 'UNK in test'] = [x for x in list(X_test[col].unique()) if x not in unique_train]\n    unknown_in_test.loc[col, 'NaN in test'] = X_test[col].isna().sum()\n  return unknown_in_test","787ac75c":"unknown_in_test = compare_values_train_test(X_train, X_test)\nunknown_in_test","67dce56b":"def nan_impute(X):\n  df = X.copy()\n  for feat in features_with_nan:\n    if (feat in features_nom) or (feat in features_ord):\n      if feat in ['MSZoning', 'Exterior1st', 'Exterior2nd', 'SaleType', 'KitchenQual', 'Utilities', 'Electrical']:\n        df[feat].fillna(df[feat].mode()[0], inplace=True)\n      elif feat == 'Functional':\n        df[feat].fillna('Typ', inplace=True)  \n      else:\n        df[feat].fillna('NA', inplace=True)\n    elif feat in features_num:\n      df[feat].fillna(0, inplace=True)\n  return df","ea8735ec":"X_train = nan_impute(X_train)\nX_test = nan_impute(X_test)","22c38f61":"X_train.isna().any().sum()","1db08cd9":"X_test.isna().any().sum()","59fc188e":"pd.concat([X_train.select_dtypes(exclude='category').describe().T[['min', 'max']].T,\n           X_test.select_dtypes(exclude='category').describe().T[['min', 'max']].T], keys=['Train', 'Test'])","da3e31bc":"X_test[(X_test['YearRemodAdd'] < X_test['YearBuilt'])]","1ac2320c":"def special_clean(X):\n  df = X.copy()\n  df['GarageYrBlt'] = df['GarageYrBlt'].where(df['GarageYrBlt'] <= 2010, df['YearBuilt'])\n  df['YearRemodAdd'] = df['YearRemodAdd'].where(df['GarageYrBlt'] >= df['YearBuilt'], df['YearBuilt'])\n  return df","902cdfb8":"from collections import Counter\ndef tukey_outliers(X):\n  df = X.copy()\n  outlier_indices = []\n  \n  for col in df.columns:\n      if not 'float' in str(df[col].dtype):\n        continue\n      Q1 = np.percentile(df[col], 25)\n      Q3 = np.percentile(df[col],75)\n      # Interquartile range (IQR)\n      IQR = Q3 - Q1\n      # outlier step\n      outlier_step = 1.5 * IQR\n      outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n      outlier_indices.extend(outlier_list_col)\n  two_times = (pd.Series(outlier_indices).value_counts() > 1)\n  index_outliers = two_times[two_times != False]\n\n  return index_outliers.index.tolist()","dd518d82":"idx_outliers = tukey_outliers(X_train)\nX_train.iloc[idx_outliers]","cf64854d":"px.scatter(X_train, x='GrLivArea', y=y_train, color='OverallQual')","776ae64d":"px.box(X_train, x='CentralAir', y=y_train, color='OverallQual')","2e3a836a":"# Deleting outliers\ndef remove_outliers(X):\n  df = X.copy()\n  df = df.drop(tukey_outliers(df))\n  df = df.drop(df[(df['GrLivArea']>4000)].index)\n  return df","efaa460b":"fig, ax = plt.subplots(figsize=(12,10))\nsns.heatmap(X_train.select_dtypes(exclude='category').corr());","a4d6197c":"from sklearn.feature_selection import mutual_info_regression\n\nX = X_train.copy()\n\ndiscrete_features = [str(X_train[col].dtype)=='category' for col in X_train.columns]\n\nX = OrdinalEncoder().fit_transform(X)\n\nmi_scores = mutual_info_regression(X, y_train, discrete_features=discrete_features , random_state=0)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_train.columns)\nmi_scores = mi_scores.sort_values(ascending=False)\nscores = mi_scores.sort_values(ascending=True)\nwidth = np.arange(len(scores))\nticks = list(scores.index)\nfig, ax = plt.subplots(figsize=(22,15))\nax.barh(width, scores)\nplt.yticks(width, ticks)\nplt.title(\"Mutual Information Scores\");","707a0c24":"def remove_useless(X):\n  df = X.copy()\n  df = df.drop(['GarageArea', 'MoSold', 'Utilities','PoolArea', 'BsmtHalfBath', '3SsnPorch', 'PoolQC', 'Street', 'MiscVal', 'LandSlope'], axis=1)\n  return df","b21b4a60":"X_train['TotalSF'] = X_train['TotalBsmtSF'] + X_train['1stFlrSF'] + X_train['2ndFlrSF']\nX_test['TotalSF'] = X_test['TotalBsmtSF'] + X_test['1stFlrSF'] + X_test['2ndFlrSF']","d82ac6ce":"X_train['TotalPorchArea'] = X_train['OpenPorchSF'] + X_train['EnclosedPorch'] + X_train['ScreenPorch'] + X_train['3SsnPorch']\nX_test['TotalPorchArea'] = X_test['OpenPorchSF'] + X_test['EnclosedPorch'] + X_test['ScreenPorch'] + X_test['3SsnPorch']","8edf96a4":"X_train['Indoor_ratio'] = (X_train['TotalBsmtSF']\/X_train['GrLivArea'])\nX_test['Indoor_ratio'] = (X_test['TotalBsmtSF']\/X_test['GrLivArea'])","c89d3da5":"px.scatter(X_train, x='TotalSF', y=y_train, color='GrLivArea')","32b03a62":"px.scatter(X_train, x='GrLivArea', y=y_train, color='Indoor_ratio')","f38afded":"def aggregate_features(X):\n  df = X.copy()\n  df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n  df['TotalPorchArea'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['3SsnPorch']\n  df['Indoor_ratio'] = (df['TotalBsmtSF']\/df['GrLivArea'])\n  return df","5a506365":"X_train = X_train.drop(['GarageArea', 'MoSold', 'Utilities','PoolArea', 'BsmtHalfBath', '3SsnPorch', 'PoolQC', 'Street', 'MiscVal', 'LandSlope'], axis=1)","1355d529":"skewed = X_train[X_train.dtypes[X_train.dtypes != \"category\"].index].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew': skewed})\nskewness = skewness[abs(skewness) > 0.75].dropna()\nskewness.T","04306a70":"px.histogram(X_train, x='TotalBsmtSF')","22dd0b37":"X = X_train.copy()\nX['TotalBsmtSF'] = np.sqrt(X['TotalBsmtSF'])\npx.histogram(X, x='TotalBsmtSF')","21f10f59":"def transform_features(X):\n  df = X.copy()\n  for col in skewness.index:\n    if 'Area' in col or 'SF' in col:\n      df[col] = np.sqrt(df[col])\n    elif col == 'GarageYrBlt':\n      continue\n    else:\n      df[col] = np.sqrt(df[col]) # there is 0 values so complicated for log -> why not to test box-cox ?\n  return df","6a6da31f":"X_train, y_train = load_data(train=True) # load_data defined at the beginning\nX_test = load_data(train=False)","a238db02":"X_train.info()","caf2ed57":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=7, random_state=42, shuffle=True)\n\ndef score(model, X):\n    rmsle_kfs = np.sqrt(-cross_val_score(model, X, y_train, cv=kf, scoring='neg_mean_squared_error'))\n    mean = np.mean(rmsle_kfs)\n    std = np.std(rmsle_kfs)\n    return mean, std","39de5854":"xgb_params = dict(\n    objective ='reg:squarederror',\n    max_depth=7,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)","7048f731":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n\n\ndef create_pipelines(X):\n\n  pipe_ridge = make_pipeline(\n      ColumnTransformer(\n      transformers=[\n          ('oh', OneHotEncoder(handle_unknown=\"ignore\", sparse=False), X.select_dtypes(['category']).columns)],\n          remainder='passthrough'),\n      RobustScaler(),\n      RidgeCV(cv=kf)\n  )\n\n  pipe_lasso = make_pipeline(\n      ColumnTransformer(\n      transformers=[\n          ('oh', OneHotEncoder(handle_unknown=\"ignore\", sparse=False), X.select_dtypes(['category']).columns)],\n          remainder='passthrough'),\n      RobustScaler(),\n      LassoCV(cv=kf)\n  )\n\n  pipe_el = make_pipeline(\n      ColumnTransformer(\n      transformers=[\n          ('oh', OneHotEncoder(handle_unknown=\"ignore\", sparse=False), X.select_dtypes(['category']).columns)],\n          remainder='passthrough'),\n      RobustScaler(),\n      ElasticNetCV(cv=kf)\n  )\n\n  pipe_gb = make_pipeline(\n      ColumnTransformer(\n      transformers=[\n          #('oh', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), X.select_dtypes(['category']).columns),\n          ('oh', OneHotEncoder(handle_unknown=\"ignore\", sparse=False), X.select_dtypes(['category']).columns)],\n          remainder='passthrough'),\n      GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=7, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber'),\n  )\n\n  pipe_ex = make_pipeline(\n      ColumnTransformer(\n      transformers=[\n          #('oh', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), X.select_dtypes(['category']).columns),\n          ('oh', OneHotEncoder(handle_unknown=\"ignore\", sparse=False), X.select_dtypes(['category']).columns)],\n          remainder='passthrough'),\n      ExtraTreesRegressor(n_estimators=500, max_depth=15)\n  )\n\n  pipe_xgb = make_pipeline(\n      ColumnTransformer(\n      transformers=[\n          #('oh', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), X.select_dtypes(['category']).columns),\n          ('oh', OneHotEncoder(handle_unknown=\"ignore\", sparse=False), X.select_dtypes(['category']).columns)],\n          remainder='passthrough'),\n      XGBRegressor(**xgb_params)\n  )\n  return pipe_ridge, pipe_lasso, pipe_el, pipe_gb, pipe_ex, pipe_xgb\n ","e3ff14aa":"pipe_ridge, pipe_lasso, pipe_el, pipe_gb, pipe_ex, pipe_xgb = create_pipelines(X_train)\n\nfor name, model in [('Linear Reg Ridge', pipe_ridge), ('Linear Reg Lasso', pipe_lasso), ('Linear Reg ElasticNet', pipe_el), ('GradientBoosting', pipe_gb), ('Extra Trees', pipe_ex), ('XGBoost', pipe_xgb)]:\n  mean, std = score(model, X_train)\n  print('Our baseline score for {}: {} ({})'.format(name, mean, np.round(std,4)))","5b8b4705":"!pip install delayed","6f6a5ab8":"import delayed\nfrom sklearn.inspection import permutation_importance\n\npipe_xgb.fit(X_train, y_train)\n\nresult = permutation_importance(\n    pipe_xgb, X_train, y_train, n_repeats=10, random_state=42, scoring='neg_root_mean_squared_error'\n)\nsorted_idx = result.importances_mean.argsort()\n\nres = pd.DataFrame(result.importances[sorted_idx], index=X_train.columns[sorted_idx]).melt(ignore_index=False)\nfig = px.box(res, x=res.index, y=res['value'])\nfig.update_xaxes(tickangle=45)","3b65b74a":"from sklearn.inspection import permutation_importance\n\npipe_ridge.fit(X_train, y_train)\n\nresult = permutation_importance(\n    pipe_ridge, X_train, y_train, n_repeats=10, random_state=42, scoring='neg_root_mean_squared_error'\n)\nsorted_idx = result.importances_mean.argsort()\n\nres = pd.DataFrame(result.importances[sorted_idx], index=X_train.columns[sorted_idx]).melt(ignore_index=False)\nfig = px.box(res, x=res.index, y=res['value'])\nfig.update_xaxes(tickangle=45)","fef4715b":"# !pip install optuna","22597032":"# # Optuna for hyperparameter tuning\n# import optuna\n\n# def objective(trial):\n#     xgb_params = dict(\n#         objective ='reg:squarederror',\n#         max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n#         learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n#         n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n#         min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n#         colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#         subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n#         reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#         reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#     )\n#     pipe_xgb = make_pipeline(\n#         ColumnTransformer(\n#         transformers=[\n#             # ('num', RobustScaler(), X_train.select_dtypes(['int64', 'float64']).columns),\n#             # ('cat', OneHotEncoder(handle_unknown=\"ignore\", sparse=False), features_nom)],\n#             ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), features_nom)],\n#             remainder='passthrough'),\n#         XGBRegressor(**xgb_params)\n#     )\n#     return score(pipe_xgb, X_train)\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=40)\n# xgb_params = study.best_params","0759fba2":"xgb_params={\n 'objective':'reg:squarederror',\n 'colsample_bytree': 0.6302128203421344,\n 'learning_rate': 0.003409411549745469,\n 'max_depth': 4,\n 'min_child_weight': 2,\n 'n_estimators': 3395,\n 'reg_alpha': 0.013958930749403808,\n 'reg_lambda': 0.03257412761380466,\n 'subsample': 0.7084852836190056}","103a7813":"# With Ordinal encoding\n\n#pipe_xgb = make_pipeline(\n#    ColumnTransformer(\n#    transformers=[\n#        ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), X_train.select_dtypes(['category']).columns)],\n#        remainder='passthrough'),\n#    XGBRegressor(**xgb_params)\n#)\n#score(pipe_xgb, X_train)","7f0dcc7c":"# With OneHot encoding \n\npipe_xgb = make_pipeline(\n    ColumnTransformer(\n    transformers=[\n        ('oh', OneHotEncoder(handle_unknown='ignore', sparse=False), X_train.select_dtypes(['category']).columns)],\n        remainder='passthrough'),\n    XGBRegressor(**xgb_params)\n)\nscore(pipe_xgb, X_train)","c8166ee2":"from sklearn.ensemble import StackingRegressor\n\nestimators = [\n     ('lasso', pipe_lasso),\n     ('ridge', pipe_ridge),\n     ('gb', pipe_gb),\n     ('xgb', pipe_xgb),\n ]\n\n\nstack = StackingRegressor(\n    estimators=estimators,\n    final_estimator=GradientBoostingRegressor(loss='huber')\n)\n\n\nnp.sqrt(-cross_val_score(stack, X_train, y_train, cv=5, scoring='neg_mean_squared_error')).mean()","d084cbc1":"pipe_ridge.fit(X_train, y_train)\npreds_ridge = pipe_ridge.predict(X_test)\n\npipe_lasso.fit(X_train, y_train)\npreds_lasso = pipe_lasso.predict(X_test)\n\npipe_el.fit(X_train, y_train)\npreds_el = pipe_el.predict(X_test)\n\npipe_gb.fit(X_train, y_train)\npreds_gb = pipe_gb.predict(X_test)\n\npipe_ex.fit(X_train, y_train)\npreds_ex = pipe_ex.predict(X_test)\n\npipe_xgb.fit(X_train, y_train)\npreds_xgb = pipe_xgb.predict(X_test)\n\nstack.fit(X_train, y_train)\npreds_stack = stack.predict(X_test)","2993b10f":"all_preds = pd.DataFrame({'Ridge':np.exp(preds_ridge),\n                         'Lasso':np.exp(preds_lasso),\n                         'ElasticNet':np.exp(preds_el),\n                         'GradientBoosting':np.exp(preds_gb),\n                         'Extra Trees':np.exp(preds_ex),\n                         'XGBoost':np.exp(preds_xgb),\n                         'Stacking':np.exp(preds_stack)})\nall_preds","9e2b7db5":"all_preds.corr()","b1f3a9dc":"preds = 0.6*preds_ridge + 0.2*preds_stack + 0.2*preds_xgb","23de189b":"my_submission = pd.DataFrame({'Id': X_test.index.values, 'SalePrice': np.exp(preds_el)})\nmy_submission.to_csv('submission.csv', index=False)","4de257fc":"## Non-sense data <a class=\"anchor\" id=\"nonsense\"><\/a>\n\n- (Look at GarageYrBlt max)","3e8fc193":"- We remove GarageArea because it gives quite the same info as GarageCars","c8ddec65":"- We can see that more *indoor_ratio* leads to increase in price in general","411d7205":"- Also there is a remod date before the built date in test set...","da156ae1":"- Interesting, so *TotalSF* is really important for XGBoost, makes sense actually. But the gap between 1st and 2nd is awesome\n\n- Let's see for a regularized linear regression: Ridge","a08271c6":"- We can see for example for *CentralAir*:\n  - No 10\/10 *OverallQual* house without *CentralAir*, makes sense.\n  - No 1\/10 *OverallQual* house when there is *CentralAir*, makes sense too.","6064f8b8":"- We can't compare between Ordinal and OnheHot encodings because of package conflicts but OneHot encoder slightly better","8216eea3":"## Mutual Info Regression <a class=\"anchor\" id=\"mutual_info\"><\/a>","0743bc97":"- We see clearly the correlation here. And we will remove the two outliers (big Area but very small prices)","00860d4c":"- We see that the test set isn't cleaned and there is some NaN in test set but no in train set for some categorical variable $\\rightarrow$ we have to take care of the \"NA\" case in the categorical encoding\n\n- Let's see if unknown values or new NaN are in the test set compared to train set","4dc066be":"\n## Baseline Models <a class=\"anchor\" id=\"baseline_models\"><\/a>\n\n- We will use the models:\n  - Ridge\n  - Lasso\n  - ElasticNet\n  - GradientBoosting\n  - Extra Trees\n  - XGBoost\n\n\n- **We won't use Neural Networks because we haven't enough data so the benefit of Deep Learning is limited here**. \n\n- Not the best possible set: the more the models give different types of error (more diversifiate), the best for ensemble methods\n\n- We one-hot encode the categorical nominative features (without order) for the linear models and ordinal encode for trees (node splitting so rank doesn't matter)","00e0c36f":"- Street, PoolQC, Utilities, Condition2, 3SsnPorch, MoSold seems useless with MI","035200db":"# <center> \ud83c\udfe0 Houses Price | \u26a1 Preprocess + EDA + Stacking Top 8% <\/center>\n\n### Table of Contents\n\n- [Get the data](#get_data)\n- [Prepocessing\/Exploring](#explore_data)\n    - [Categorical Encoding](#categorical_encoding)\n    - [NaN](#nan)\n    - [Non sense data](#non_sense)\n    - [Looking for outliers](#outliers)\n    - [EDA (plotly) (just a little here)](#eda)\n    - [Features correlation](#correlation)\n    - [Mutual information](#mutual_info)\n    - [New features](#mutual_info)\n    - [Features transformations (more gaussian)](#mutual_info)\n- [Modeling](#modeling)\n    - [Baseline Models](#baseline_models)\n    - [Permutation importance](#permutation)\n    - [Hyper-parameter tuning (optuna)](#hyper_tune)\n    - [Stacking](#stacking)\n    - [Submission](#submission)\n- [Next\/Ideas](#next_ideas)\n\n","bafad8f5":"---\n# Getting the data <a class=\"anchor\" id=\"get_data\"><\/a>\n","0c23649b":"## New features <a class=\"anchor\" id=\"new_features\"><\/a> ","9178623e":"## Hyper-parameter tuning <a class=\"anchor\" id=\"hyper_tune\"><\/a>\n\n- Using the awesome library *Optuna* for hyperparameter tuning\n- We hyper-tune only for XGBoost (we should normally to do it for all models)","11bb2a88":"## Looking for outliers <a class=\"anchor\" id=\"outliers\"><\/a>","107b0cf0":"## EDA (just a little here) <a class=\"anchor\" id=\"eda\"><\/a>\n\n- Then we explore the dataset\n\n- I really invite you to explore a lot the dataset, test different visualizations, get some insights. It's really interesting, and even more if you like the subject !\n\n- Below two examples","b1be4a8d":"- No NaN anymore","e4317d1b":"**If we combine decorrelated results, we can obtain a more robust one. It's the principle of diversification** (like a portfolio of financial assets for example). Se also [wisdom of the crowd](https:\/\/en.wikipedia.org\/wiki\/Wisdom_of_the_crowd)\n","ebcf5ee0":"- Interesting, here the gap is reduced, and the importances make sense too.","a80cc8fb":"---\n# Preprocessing\/Exploring <a class=\"anchor\" id=\"explore_data\"><\/a>\n\n- In this step we look at\n  - Cleaning\n  - Encoding\n  - NaN imputing\n\n\n- <font color=\"red\"> Here we will look at the NaN\/Unknown values in test set (Normally the test set should be set aside and we musn't look at it in order to avoid data leakage). <br>\nBut we will proceed as if we had preprocessed the data before doing the split (about 50% train - 50% test) <br>\nFor outliers search and EDA, of course only the train set is inspected\n<\/font>\n","a8349ac4":"- The following function will be defined at the end of the preprocessing and is useful funtion for reloading","a27fc9a6":"---\n# Libraries","85ff5bc6":"## We finally load (and can reload if needed) our data $X_{train}$, $Y_{train}$ and $X_{test}$\n","8666ac27":"## Numerical Features transformation for normality <a class=\"anchor\" id=\"transformations\"><\/a>\n\nHere are some good tips from IBM for features transform: [features transform](https:\/\/www.ibm.com\/support\/pages\/transforming-variable-normality-parametric-statistics)\n\n- Target variable in log (as told in evaluations section of the competition)\n\n- We prefer to transform skewed values into more gaussian\n\n- Details: \n  - All area features (in square feet) $\\rightarrow$ square root\n  - other $\\rightarrow$ log","d363fc6e":"## Stacking <a class=\"anchor\" id=\"stacking\"><\/a>\n\nLet's add the stacking regressor to the Lasso, Ridge, GradientBoosting and XGBoost pb","49cbc6e9":"## Categorical Encoding <a class=\"anchor\" id=\"categorical_encoding\"><\/a>\n\n- We have a lot of features, but knowing them is important\n- Categorical\n  - Nominative (not ordered) $\\rightarrow$ we'll use OneHot encoding (Ordinal for trees)\n  - Ordered $\\rightarrow$ we can use Ordinal or One Hot Encoding\n- We have to keep in mind that there is some unknown values in the test set !","3254f6df":"- We will add some new features which can be interesting","ba2fabe5":"# Next \/ Ideas: <a class=\"anchor\" id=\"next_ideas\"><\/a>\n  - Add more model and hyper-tune all the models\n  - Add some ratio features and try others things like adjusted price with inflation if possible\n  - SHAP analyse in order to get more insights and interpretability","b44ef236":"---\n# Modeling <a class=\"anchor\" id=\"modeling\"><\/a>","8ef2f019":"- Categorical:\n  - In train and test sets:\n    - Alley $\\rightarrow$ `.fillna('NA')`\n    - MiscFeature $\\rightarrow$ `.fillna('NA')`\n    - BsmtQual $\\rightarrow$ `.fillna('NA')`\n    - BsmtCond $\\rightarrow$ `.fillna('NA')`\n    - FirePlaceQu $\\rightarrow$ `.fillna('NA')`\n    - GarageQual $\\rightarrow$ `.fillna('NA')`\n    - GarageCond $\\rightarrow$ `.fillna('NA')`\n    - GarageFinish $\\rightarrow$ `.fillna('NA')`\n    - PoolQC $\\rightarrow$ `.fillna('NA')`\n    - BsmntExposure $\\rightarrow$ `.fillna('NA')`\n    - BsmntFinType1 $\\rightarrow$ `.fillna('NA')`\n    - BsmntFinType2 $\\rightarrow$ `.fillna('NA')`\n    - Fence $\\rightarrow$ `.fillna('NA')`\n\n  - Only in test set:\n    - MSSubclass has a unknown value of 150 in test set. Actually it will be consedered as unknown for our encoding. Let's keep it that way.\n    - MSZoning $\\rightarrow$ most frequent value (in train + test set).\n    - Exterior1st $\\rightarrow$ most frequent value (in train + test set).\n    - Exterior2nd $\\rightarrow$ most frequent value (in train + test set).\n    - SaleType $\\rightarrow$ most frequent value (in train + test set).\n    - KitchenQual $\\rightarrow$ most frequent value (in train + test set).\n    - Utilities $\\rightarrow$ most frequent value (in train + test set).\n    - Electrical $\\rightarrow$ most frequent value (in train + test set).\n    - Functional $\\rightarrow$ \"Typ\" (see data description).\n\n\n- All numerical features $\\rightarrow$ `.fillna(0)`","ce62267b":"- We align max GarageYrBlt (2207...) and max YearBuilt","d2d7465f":"## Permutation Importance <a class=\"anchor\" id=\"permutation\"><\/a>\n\n- Really intuitive, the goal of feature importance is to permute features and see how the loss is affected (if we remove a feature, does the loss increase ?\n\n- Imagine we have a validation set and see the features importance on train and valisation set: **if a feature is really important in traning set but not at all in validation set, then maybe this feature makes the model overfit !**\n\n\n\n","fe11cff3":"- We see the correlation here","1b13fc02":"## Submission <a class=\"anchor\" id=\"submission\"><\/a>\n\n- Now let's fit all the models and see the predictions correlations","ec8083fb":"- Our predictions are really correlated... We would like to give more weights to decorrelated results","3587e459":"- In first, we will use the Tukey method to detect ouliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values (IQR). An outlier is a row that have a feature outside the (IQR +- an outlier step). <br>\nWe will remove the houses that have > 1 outlier in their features.\n\n- We also have to visualize the data in order to detect some outliers (see after)","11a865bb":"- An example:","9d9ed92b":"## Correlation between features <a class=\"anchor\" id=\"correlation\"><\/a>\n\n","50a77213":"## NaN % <a class=\"anchor\" id=\"nan\"><\/a>\n\n- Let's check the missing values in both train and test set "}}