{"cell_type":{"b3cbb2aa":"code","7498e88f":"code","b066ae01":"code","ea95db47":"code","2a18dbb5":"code","09029a65":"code","922bdd28":"code","dd545dd4":"code","7de198e9":"code","383fbafe":"code","9b85ee4e":"code","d1d1224c":"code","b6d18c5f":"code","3fa68955":"code","13074ec7":"code","ccfc7e33":"code","def6533e":"code","3b147bd2":"code","0d0cf85e":"code","fef8a6e8":"code","05747bdd":"code","c5f9b40c":"code","b08c27a3":"code","60282f07":"code","8049a328":"code","636cb69a":"markdown","0d81f424":"markdown","6e63d773":"markdown","076e6d94":"markdown","10e6db23":"markdown","6272aed0":"markdown","28c53f7e":"markdown","7bfabdf8":"markdown","2ce99b85":"markdown","923c8f67":"markdown","aa729228":"markdown","5f674fea":"markdown","b76b1d6f":"markdown","41ede277":"markdown","3f316166":"markdown","23ea06fb":"markdown","36515387":"markdown","ede67173":"markdown","ff2dbdf3":"markdown","81334dfe":"markdown"},"source":{"b3cbb2aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7498e88f":"dataset = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","b066ae01":"dataset.info()","ea95db47":"dataset.head()","2a18dbb5":"dataset['quality'].unique()","09029a65":"import seaborn as sns\nimport matplotlib.pyplot as plt","922bdd28":"print(dataset['quality'].value_counts())\nsns.countplot(x='quality', data=dataset)\nplt.title('Wine Quality Count')\nplt.show()","dd545dd4":"sns.catplot(x=\"quality\", y=\"alcohol\", data=dataset)\nplt.show()","7de198e9":"sns.catplot(x=\"quality\", y=\"fixed acidity\", data=dataset)\nplt.show()","383fbafe":"plt.figure(figsize=(20,10)) \nsns.heatmap(dataset.corr(), annot=True)\nplt.show()","9b85ee4e":"X = dataset.drop(['quality'],axis=1)\nY = dataset['quality']","d1d1224c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas import DataFrame\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport catboost as cb","b6d18c5f":"# Set seed for reproducibility\nSEED = 42","3fa68955":"## oversampling\nfrom imblearn.over_sampling import SMOTE\nos=SMOTE()\nX_res,y_res=os.fit_sample(X, Y)","13074ec7":"y_res.value_counts()","ccfc7e33":"# Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test= train_test_split(X_res, y_res, test_size=0.2, random_state=SEED)","def6533e":"# fit scaler on training data\nnorm = MinMaxScaler().fit(X_train)\n\n# transform training data\nX_train_norm = norm.transform(X_train)\n\n# transform testing dataabs\nX_test_norm = norm.transform(X_test)","3b147bd2":"# fit scaler on training data\nstdscale = StandardScaler().fit(X_train)\n\n# transform training data\nX_train_std = stdscale.transform(X_train)\n\n# transform testing dataabs\nX_test_std = stdscale.transform(X_test)","0d0cf85e":"# Instantiate individual classifiers\nlr = LogisticRegression(max_iter = 500, n_jobs=-1, random_state=SEED)\nknn = KNN()\ndt = DecisionTreeClassifier(random_state=SEED)\nsvc = SVC(random_state=SEED)\nrf = RandomForestClassifier(random_state=SEED)\nxgbc = xgb.XGBClassifier(random_state=SEED)\nlgbmc = lgbm.LGBMClassifier(random_state=SEED)\ncbc = cb.CatBoostClassifier(random_state=SEED, verbose=False)\ngbc = GradientBoostingClassifier(random_state=SEED)\n\n# Define a list called classifier that contains the tuples (classifier_name, classifier)\nclassifiers = [('Logistic Regression', lr),\n('K Nearest Neighbours', knn),\n('SVM', svc),\n('Random Forest Classifier', rf),\n('Decision Tree', dt),\n('XGBClassifier', xgbc),\n('LGBMClassifier', lgbmc),\n('CatBoostClassifier', cbc),\n('GradientBoostingClassifier', gbc)]              ","fef8a6e8":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","05747bdd":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train_norm, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test_norm)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","c5f9b40c":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train_std, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test_std)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","b08c27a3":"votingC = VotingClassifier(estimators=[('Random Forest', rf), ('LightGBM', lgbmc), ('Catboost', cbc)], voting='soft', n_jobs=-1)\nvotingC = votingC.fit(X_train, y_train)\n# Predict the labels of the test set\ny_pred = votingC.predict(X_test)\n# Evaluate the accuracy of clf on the test set\naccuracy_score(y_test, y_pred)","60282f07":"votingC = VotingClassifier(estimators=[('Random Forest', rf), ('LightGBM', lgbmc), ('Catboost', cbc)], voting='soft', n_jobs=-1)\nvotingC = votingC.fit(X_train_norm, y_train)\n# Predict the labels of the test set\ny_pred = votingC.predict(X_test_norm)\n# Evaluate the accuracy of clf on the test set\naccuracy_score(y_test, y_pred)","8049a328":"votingC = VotingClassifier(estimators=[('Random Forest', rf), ('LightGBM', lgbmc), ('Catboost', cbc)], voting='soft', n_jobs=-1)\nvotingC = votingC.fit(X_train_std, y_train)\n# Predict the labels of the test set\ny_pred = votingC.predict(X_test_std)\n# Evaluate the accuracy of clf on the test set\naccuracy_score(y_test, y_pred)","636cb69a":"## Data Modeling","0d81f424":"From above we can that CatBoost Classifier gives us the best result irrespective of the Normalized or Standardized data","6e63d773":"## Over-Sampling the Imbalanced Data","076e6d94":"## Models prediction without any normalization or standardization","10e6db23":"### From above we can observe that the wine quality data is now properly balanced.","6272aed0":"## Data Analysis","28c53f7e":"## Feature Scaling","7bfabdf8":"### Normalize the data","2ce99b85":"## Combining various Models - Voting Classifier","923c8f67":"### Since the Red Wine datast is highly imbalanced, we will use oversampling the data so that it's balanced","aa729228":"From above, we can observe that\n\n1. There are 12 cols and 1599 data rows.\n2. The data doesn't contain any null values.\n3. quality col is our class label.","5f674fea":"## Models prediction with Standardized data","b76b1d6f":"There are 6 different qualities of wine. ","41ede277":"### The data looks very unbalanced for different wine quality classes. The data is very less for quality 3,4,7,8 as compared to 5,6.","3f316166":"## Models prediction with Normalized data","23ea06fb":"## Data Visualization","36515387":"### Standardize the data","ede67173":"## Model Selection","ff2dbdf3":"From this heatmap we can say that the wine features are not correlated to each other. ","81334dfe":"From above we can say that Voting Classifier marginally improves the accuracy compared to the CatBoost Classifier. "}}