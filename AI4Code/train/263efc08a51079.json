{"cell_type":{"b1366c70":"code","ecd366ff":"code","8e9573e8":"code","28c93960":"code","663d3a6e":"code","79eaba88":"code","13ef1648":"code","bf66ec77":"code","6e90203b":"code","25ced523":"code","1d38ebfb":"code","4433f014":"code","491b03e6":"code","0b9018e2":"code","83677e83":"code","c329524b":"code","1e8482e8":"code","865c35f0":"code","6aff324e":"code","44a7ed9d":"code","ce4d458f":"code","819d460b":"code","60e2467d":"code","f4b206bf":"code","35554de0":"code","3e2d3fe7":"code","75a60005":"code","445148a7":"code","9ea55327":"code","e5a01d74":"code","0d723510":"code","b6756a48":"code","bb7f47e4":"code","a5aac43d":"code","866f581b":"code","9760cb9e":"code","2a7dbe1e":"markdown","3d0bc2db":"markdown","614a8860":"markdown","079609ce":"markdown","d3f6e9ca":"markdown","cd57fce1":"markdown","456da76c":"markdown","620e9ca7":"markdown","fdfcdd5d":"markdown","5959081f":"markdown","0e52597c":"markdown","62d42690":"markdown","4d2b0f37":"markdown","003e615b":"markdown","f5e4fec3":"markdown","a88832e7":"markdown","085622cd":"markdown","86b3250e":"markdown","361612ab":"markdown","a8e78c5a":"markdown","bf61c0b1":"markdown","7e401e64":"markdown","df617675":"markdown","df324a61":"markdown","379b62ab":"markdown","00d59380":"markdown","a42a4888":"markdown","8170f5bf":"markdown","4c8e096c":"markdown","56fd319b":"markdown","71b1aa74":"markdown","22a7a536":"markdown","d54f0bf8":"markdown","9c811a79":"markdown"},"source":{"b1366c70":"#importing required packages and data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\ndata_train = pd.read_csv(\"..\/input\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/test.csv\")","ecd366ff":"print(\"Training data has {} rows and {} columns\".format(data_train.shape[0], data_train.shape[1]))\ndata_train.head()","8e9573e8":"print(\"Test data has {} rows and {} columns\".format(data_test.shape[0], data_test.shape[1]))\ndata_test.head()","28c93960":"data_train.isnull().sum()","663d3a6e":"data_test.isnull().sum()","79eaba88":"drop_cols = [\"PassengerId\", \"Ticket\", \"Cabin\"]\ndata_train.drop(drop_cols, inplace=True, axis=1, errors=\"ignore\")\ndata_test.drop(drop_cols, inplace=True, axis=1, errors=\"ignore\")\nprint(data_train.info())\nprint(\"==================================\")\nprint(data_test.info())","13ef1648":"#imputing embarked missing values in training data\ndata_train.loc[data_train[\"Embarked\"].isnull(),\"Embarked\"] = data_train[\"Embarked\"].mode()[0]\ndata_test.loc[data_test[\"Fare\"].isnull(),\"Fare\"] = data_test[\"Fare\"].median()\nprint(\"The 2 missing values in Embarked feature has been filled with {}\".format(data_train[\"Embarked\"].mode()[0]))\nprint(\"The 1 missing values in Fare feature has been filled with {}\".format(data_test[\"Fare\"].median()))","bf66ec77":"#Creating derived feature \"Title\"\ncombined = [data_train, data_test]\nfor dataset in combined:\n    dataset[\"Title\"] = dataset[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \npd.concat([data_train[\"Title\"], data_test[\"Title\"]]).value_counts()","6e90203b":"valid_title = [\"Mr\",\"Miss\",\"Mrs\",\"Master\"]\ndata_train[\"Title\"] = data_train.Title.apply(lambda x: x if x in valid_title else \"Other\")\ndata_test[\"Title\"] = data_test.Title.apply(lambda x: x if x in valid_title else \"Other\")\npd.concat([data_train[\"Title\"], data_test[\"Title\"]]).value_counts()","25ced523":"#Drop the name column\ndata_train.drop(\"Name\", inplace=True, axis=1, errors=\"ignore\")\ndata_test.drop(\"Name\", inplace=True, axis=1, errors=\"ignore\")","1d38ebfb":"f,axes= plt.subplots(1,2, figsize=(10,5))\np1 = sns.boxplot(x=\"Title\", y=\"Age\", data=data_train, ax=axes[0])\np2 = sns.boxplot(x=\"Title\", y=\"Age\", data=data_test, ax=axes[1])\np1.set(title=\"Training data\")\np2.set(title=\"Test data\")","4433f014":"for dataset in combined:\n    dataset.loc[(dataset[\"Age\"].isnull()) & (dataset[\"Title\"]==\"Mr\"),\"Age\"] = dataset.loc[dataset[\"Title\"]==\"Mr\", \"Age\"].median()\n    dataset.loc[(dataset[\"Age\"].isnull()) & (dataset[\"Title\"]==\"Mrs\"), \"Age\"] = dataset.loc[dataset[\"Title\"]==\"Mrs\", \"Age\"].median()\n    dataset.loc[(dataset[\"Age\"].isnull()) & (dataset[\"Title\"]==\"Miss\"), \"Age\"] = dataset.loc[dataset[\"Title\"]==\"Miss\", \"Age\"].median()\n    dataset.loc[(dataset[\"Age\"].isnull()) & (dataset[\"Title\"]==\"Master\"),\"Age\"] = dataset.loc[dataset[\"Title\"]==\"Master\", \"Age\"].median()\n    dataset.loc[(dataset[\"Age\"].isnull()) & (dataset[\"Title\"]==\"Other\"), \"Age\"] = dataset.loc[dataset[\"Title\"]==\"Other\", \"Age\"].median()\n\nprint(data_train.isnull().sum())\nprint(\"================================\")\nprint(data_test.isnull().sum())","491b03e6":"data_train.head()","0b9018e2":"f,axes = plt.subplots(1,2, figsize=(20,5))\np1=sns.boxplot(data=data_train, x=\"Age\", ax=axes[0])\np2=sns.boxplot(data=data_train, x=\"Fare\", ax=axes[1])\np2.set_xlim(right=100)","83677e83":"def bin_age(data):\n    labels = (\"0-5\",\"5-15\",\"15-24\",\"24-35\",\"35-45\",\"45-55\",\"55-65\",\"65-90\")\n    bins = (0,5,15,24,35,45,55,65,90)\n    data[\"Age\"] = pd.cut(data.Age, bins, labels=labels)\n    \ndef bin_fare(data):\n    labels = (\"very_low\", \"low\",\"moderate\",\"high\",\"very_high\")\n    bins=(-1,10,15,30,50,700)\n    data[\"Fare\"] = pd.cut(data.Fare, bins, labels=labels)","c329524b":"datasets = [data_train, data_test]\nfor dataset in datasets:\n    bin_age(dataset)\n    bin_fare(dataset)","1e8482e8":"data_train.head()","865c35f0":"def plot_graph(data, x, y, hue=None):\n    if hue==None:\n        f,axes = plt.subplots(1,2, figsize=(15,5))\n        sns.barplot(data=data, x=x, y=y, ax=axes[0])\n        sns.countplot(data=data, x=x, ax=axes[1])\n    else:\n        f,axes = plt.subplots(1,2,figsize=(15,5))\n        sns.barplot(data=data, x=x, y=y, hue=hue, ax=axes[0])\n        sns.countplot(data=data, x=x, hue=hue, ax=axes[1])","6aff324e":"plot_graph(data_train, \"Sex\",\"Survived\")","44a7ed9d":"plot_graph(data_train,\"Age\",\"Survived\", \"Sex\")","ce4d458f":"plot_graph(data_train, x=\"Pclass\", y=\"Survived\", hue=\"Sex\")","819d460b":"plot_graph(data=data_train, x=\"Title\",y=\"Survived\")","60e2467d":"plot_graph(data_train, \"Embarked\", \"Survived\")","f4b206bf":"plot_graph(data_train,\"Fare\",\"Survived\")","35554de0":"plot_graph(data_train,\"Fare\",\"Survived\",\"Pclass\")","3e2d3fe7":"#shuffling the data\ndata_train = data_train.reindex(np.random.permutation(data_train.index))\n\nfeatures = ['Pclass','Sex','Age','SibSp','Parch','Embarked','Title']\ndata_X = data_train[features]\ntest_X = data_test[features]\ndata_Y = data_train[\"Survived\"]\ndata_X.head()","75a60005":"from sklearn import preprocessing\n\ndef encode_features(data_X,test_X):\n    features_to_label = [\"Sex\",\"Age\",\"Embarked\",\"Title\"]\n    combined_X = pd.concat([data_X[features_to_label],test_X[features_to_label]])\n    for feature in features_to_label:\n        encoder = preprocessing.LabelEncoder()\n        encoder = encoder.fit(combined_X[feature])\n        data_X[feature] = encoder.transform(data_train[feature])\n        test_X[feature] = encoder.transform(test_X[feature])\n    return (data_X, test_X)\n\ndata_X,test_X = encode_features(data_X,test_X)\ndata_X.head()","445148a7":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(data_X, data_Y, test_size=0.15, shuffle=False)\nprint(\"Training set shape : {}\".format(x_train.shape))\nprint(\"Validation set shape : {}\".format(x_val.shape))","9ea55327":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, make_scorer\n\nmodel = RandomForestClassifier()\n\n#choosing a set of parameters to try on\nparams = {\n    'n_estimators':[4,5,9,64,100,250],\n    'criterion':['gini','entropy'],\n    'max_features':['sqrt','log2',None],\n    'max_depth':[4,8,16,32,None],\n    'max_depth':[3,5,7,9,11]\n}\n\naccuracy_scorer = make_scorer(accuracy_score)\n\nsearch_params = GridSearchCV(model, params, scoring=accuracy_scorer)\nsearch_params = search_params.fit(x_train, y_train)\n\nmodel = search_params.best_estimator_\nprint(\"The model has an accuracy of {}\".format(search_params.best_score_))\nmodel.fit(x_train, y_train)","e5a01d74":"predict = model.predict(x_val)\nprint(\"The validation set accuracy is {}\".format(accuracy_score(y_val, predict)))","0d723510":"features = ['Pclass','Sex','Age','SibSp','Parch','Embarked','Title']\nlr_data_X = data_train[features]\nlr_test_X = data_test[features]\nlr_data_Y = data_train[\"Survived\"]\nlr_data_X.head()","b6756a48":"pd.set_option('display.max_columns',100)\ndef lr_encoding(data_X,test_X):\n    len_train = len(data_X)\n    features_to_label = [\"Pclass\",\"Sex\",\"Age\",\"Embarked\",\"Title\"]\n    combined_X = pd.concat([data_X[features_to_label],test_X[features_to_label]])\n    combined_X = pd.get_dummies(combined_X, columns=features_to_label)\n    data_X = combined_X.iloc[0:len_train,:]\n    test_X = combined_X.iloc[len_train:,:]\n    return(data_X, test_X)\n\nlr_data_X,lr_test_X = lr_encoding(data_X,test_X)\nlr_data_X.head()","bb7f47e4":"x_train, x_val, y_train, y_val = train_test_split(lr_data_X, lr_data_Y, test_size=0.15, shuffle=False)\nprint(\"Training set shape : {}\".format(x_train.shape))\nprint(\"Validation set shape : {}\".format(x_val.shape))","a5aac43d":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(solver='liblinear')\nparams = {\n    'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    'penalty':[\"l1\",\"l2\"],\n    'max_iter':[100,150,200,300,400,500]\n}\n\naccuracy_scorer = make_scorer(accuracy_score)\n\nsearch_params = GridSearchCV(lr_model, params, scoring=accuracy_scorer)\nsearch_params = search_params.fit(x_train, y_train)\n\nlr_model = search_params.best_estimator_\nprint(\"The training set accuracy is {}\".format(search_params.best_score_))\nlr_model.fit(x_train, y_train)","866f581b":"predict = lr_model.predict(x_val)\nprint(\"The validation set accuracy is {}\".format(accuracy_score(y_val, predict)))","9760cb9e":"p_id = pd.read_csv(\"..\/input\/test.csv\")['PassengerId']\npredict = model.predict(test_X)\n\nout = pd.DataFrame({'PassengerId' : p_id, 'Survived': predict})\n\nout.head()","2a7dbe1e":"## Check for missing values and cleaning data","3d0bc2db":"From the dataset we shall drop the following features:\n- PassengerId : As it is a unique ID for each person it doesnt play any role for prediction\n- Ticket: Again due to unique ticket number being assigned for each ticket ticket number would not present any insight\n- Cabin: This feature is being dropeed due to the very high missing values, especially in the test dataset\n- Name: The name of the person doesn't give any value.However the edsignation might provide some insight. So Name would be dropped after deriving the salutation info","614a8860":"## Random Forest model\n\nThe slected features would be used to train the model. Before we proceed we shall shuffle the data randomly.","079609ce":"#### Predicting test data\nSince the Random Forest model has slightly higher accuracy than the Logistic Regression model we shall use the random forest model to make the predictions for the test data.","d3f6e9ca":"### Title\nFrom the graph below married women had slightly higher chances of survival compared to unmarried women.","cd57fce1":"# References\n- Scikit-Learn ML from Start to Finish (https:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish)\n- EASY ML WALKTHROUGH (Regression\/SVM\/KNN)(https:\/\/www.kaggle.com\/jojothebufferlo\/easy-ml-walkthrough-regression-svm-knn)","456da76c":"We shall consider the titles that appear frequently (Mr, Miss, Mrs and Master) and categorize all others into category called *Other*.","620e9ca7":"## Model\nLet's move to building a model for the prediction of titanic disaster survival\n- Random Forest model\n- Logistic Regresion model","fdfcdd5d":"Lets analyse the age distribution for each of the Title.","5959081f":"## Data Exploration\nNow that data has been cleaned and features has been derived, we shall anayse how each of the variables relate to the target variable *Survived* and how the variables relate to each other.","0e52597c":"As expected Males survival rate is significantly less than that of Female. Also age has an important role to play in the survival.","62d42690":"People embarked from port **C** seems to have higher chances of survival compared to other ports (15-20% higher)","4d2b0f37":"## Logistic Regression\nFor logistic regression we shall use one-hot encoding of the categorical features. Label encoding them might case bias.","003e615b":"Even though the Fare may seem to have a relation to the survival rate in the first graph, the pattern is not evident when we group based on Pclass. Even some of the lower class people have paid hugher fares for the trip. However the survival rate is pretty much same for a particulat class irrespective of the fare they paid. So survival would be closely related to Pclass rather than Fare.","f5e4fec3":"### Model and tuning","a88832e7":"### splitting data","085622cd":"### Gender\nAs we know from the movies children and women were the first one to be evacuated. Thus we can expect a strong relation between the gender and survival rate.","86b3250e":"We have age ranging from 0 to 80. Thus we can group them as 0-5,5-15,15-24,24-35,35-45,45-55,55-65,65-90. <br>\nFor **Fare** we see outliers in the data. Since we have less data we shall not remove the rows from the data, instead we shall cap it while we bin the values.","361612ab":"### Pclass","a8e78c5a":"### Embarked","bf61c0b1":"### Fare","7e401e64":"### One-hot encoding","df617675":"# Random Forest and Logistic regression on Titanic dataset\nBefore we perform the model evaluation we shall analyse the dataset. Also, clean the data and change to the right formats for the modeling ","df324a61":"For numerical features (**Age** and **Fare**) we shall bin them, rather than using the absolute values.","379b62ab":"The **1st** class people has higher survival rate, followed by **2nd** and **3rd** class people. So it can be infered that the higher class people had access to the evaculation facilities than the lower class.","00d59380":"### Model","a42a4888":"- **PassengerId**: The unique id assigned for each person\n- **Survived**: The target feature, which specifies whether the person survived or not\n- **Pclass**: The ticket class (which is synonymous to socio-economic class)\n- **Name**: Name of the person\n- **Sex**: Gender\n- **Age**: The age of the individual\n- **SibSp**: number of siblings \/ spouses aboard the Titanic\n- **Parch**: number of parents \/ children aboard the Titanic\n- **Ticket**: ticket number\n- **Fare**: ticket cost\n- **Cabin**: Cabin number\n- **Embarked**: The port at which the individual embarked<br>\n<br>\nFrom the above features **Ticket** and **Name** columns doesn't give much sense to the target variable. However, we can derive the salutaion of the person (Mr, Mrs etc.), which may give insight like social status of the person, married\/ unmarried (in females) etc.","8170f5bf":"We see a similar pattern for each category in both train and test dataset. So we shall impute them with the median ages in both the data sets.","4c8e096c":"## Glimpse the data\nLet's analyse the features available for analysis.","56fd319b":"There are still missing values in **Age** and **Embarked** in training set and **Fare** in test set. Since **embarked** is a categorical feature we shall impute the values with the mode of the data. For **Age** we shall use the Title category, which would be derived from the name to impute the median age for each category. For **Fare** the value can be filled with the median value.","71b1aa74":"### Age","22a7a536":"### Create vlaidation set\nTo validate the model as it runs we shall split the data into train and validation set. 15% of data would be used for validation.","d54f0bf8":"Since we are using a Random Forest model (tree based) we shall label encode the categrical featues. On hot encoding my reduce the performance of the tree based models (ref: https:\/\/towardsdatascience.com\/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769)","9c811a79":"### Encoding"}}