{"cell_type":{"cb1bcd4d":"code","0cc90271":"code","bbde6a62":"code","a954dc6f":"code","fbfe0137":"code","6c79a683":"code","5820e812":"code","3b3ecaf1":"code","cb35f4e1":"code","1dcd60e0":"code","e34fe349":"code","098ef82b":"code","dde3e21d":"code","107aa222":"code","45f740aa":"code","3adbd5fd":"code","ea14d827":"code","b7f8bc3f":"code","79a153f0":"code","4bc080fa":"code","dd77b84c":"code","9937f1af":"code","d3324c60":"code","d407853b":"code","5727e2de":"code","4404563f":"code","d1dfee94":"code","bf53cd67":"code","5c5fdc79":"markdown","d16ce6d0":"markdown","e0014fe5":"markdown","a19181dc":"markdown"},"source":{"cb1bcd4d":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator\nfrom sklearn.datasets import make_classification, make_regression, load_digits, load_boston\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, mean_squared_error","0cc90271":"RANDOM_STATE = 17","bbde6a62":"# Let's define quality criterion\n# entropy and gini criteria are used for classification\ndef entropy(y):\n    p = [len(y[y==k])\/len(y) for k in np.unique(y)]\n    return -np.dot(p,np.log2(p))\n\ndef gini(y):\n    p = [len(y[y==k])\/len(y) for k in np.unique(y)]\n    return 1 - np.dot(p,p)\n\n# Variance and median criteria are used for regression\n\ndef variance(y):\n    return np.var(y)\n\ndef mad_median(y):\n    return np.mean(np.abs(y - np.median(y)))","a954dc6f":"criteria_dict = {'entropy':entropy,'gini':gini,\n                'variance':variance,'mad_median':mad_median}","fbfe0137":"class Node():\n    \n    def __init__(self,feature_idx=0,threshold=0,labels=None,left=None,right=None):\n        self.feature_idx = feature_idx\n        self.threshold = threshold\n        self.labels = labels\n        self.left = left\n        self.right = right","6c79a683":"def regression_leaf(y):\n    return np.mean(y)\n\ndef classification_leaf(y):\n    return np.bincount(y).argmax()","5820e812":"class DecisionTree(BaseEstimator):\n    def __init__(self, max_depth=np.inf, min_samples_split=2,criterion='gini',debug=False):\n        params = {'max_depth':max_depth,\n                 'min_samples_split':min_samples_split,\n                 'criterion':criterion,\n                 'debug':debug}\n        self.set_params(**params)\n        \n        if self.debug:\n            print('\\nDecisionTree params:')\n            print(\"max_depth = {}, min_samples_split = {},criterion = {}\\n\"\\\n                 .format(max_depth,min_samples_split,criterion))\n            \n    def set_params(self,**params):\n        super().set_params(**params)\n        \n        self._criterion_function = criteria_dict[self.criterion]\n        \n        if self.criterion in ['variance','mad_median']:\n            self._leaf_value = regression_leaf\n        else:\n            self._leaf_value = classification_leaf\n        return self\n    \n    # Function for splitting the data by two parts\n    def _functional(self, X, y, feature_idx, threshold):\n        mask = X[:,feature_idx] < threshold\n        n_obj = X.shape[0]\n        n_left = np.sum(mask)\n        n_right = n_obj - n_left\n        if n_left > 0 and n_right>0:\n            return self._criterion_function(y) - (n_left \/ n_obj) * \\\n                   self._criterion_function(y[mask]) - (n_right\/n_obj) * \\\n                   self._criterion_function(y[~mask])\n        else:\n            return 0\n        \n    def _build_tree(self, X, y,depth=1):\n        max_functional = 0\n        best_feature_idx = None\n        best_threshold = None\n        n_samples, n_features = X.shape\n        \n        if len(np.unique(y))==1:\n            return Node(labels=y)\n        \n        if depth < self.max_depth and n_samples >= self.min_samples_split:\n            if self.debug:\n                print(\"depth = {}, n_samples = {}\".format(depth,n_samples))\n                \n            for feature_idx in range(n_features):\n                \n                threshold_values = np.unique(X[:,feature_idx])\n                functional_values = [self._functional(X,y,feature_idx,threshold)\n                                    for threshold in threshold_values]\n                \n                best_threshold_idx = np.nanargmax(functional_values)\n                \n                if functional_values[best_threshold_idx] > max_functional:\n                    max_functional = functional_values[best_threshold_idx]\n                    best_threshold = threshold_values[best_threshold_idx]\n                    \n                    best_feature_idx = feature_idx\n                    best_mask = X[:,feature_idx] < best_threshold\n                    \n        if best_feature_idx is not None:\n            if self.debug:\n                print(\"best feature = {}, best threshold = {}\"\\\n                     .format(best_feature_idx,best_threshold))\n                \n            return Node(feature_idx = best_feature_idx,threshold=best_threshold,\n                       left=self._build_tree(X[best_mask,:],y[best_mask],depth + 1),\n                       right=self._build_tree(X[~best_mask,:],y[~best_mask],depth+1))\n        \n        else:\n            return Node(labels=y)\n    \n    def fit(self,X,y):\n        \n        if self.criterion in ['gini','entropy']:\n            self._n_classes = len(np.unique(y))\n            \n        self.root = self._build_tree(X,y)\n        \n        return self\n    \n    def _predict_object(self,x,node=None):\n        \n        node = self.root\n        \n        while node.labels is None:\n            if x[node.feature_idx] < node.threshold:\n                node = node.left\n            else:\n                node = node.right\n                \n        return self._leaf_value(node.labels)\n    \n    def predict(self,X):\n        return np.array([self._predict_object(x) for x in X])\n    \n    def _predict_proba_object(self,x,node=None):\n        node = self.root\n        \n        while node.labels is None:\n            if x[node.feature_idx] < node.threshold:\n                node = node.left\n            else:\n                node = node.right\n                \n        return [len(node.labels[node.labels == k]) \/ \\\n               len(node.labels) for k in range(self._n_classes)]\n    \n    def predict_proba(self, X):\n        return np.array([self._predict_proba_object(x) for x in X])","3b3ecaf1":"X, y = make_classification(n_features=2,n_redundant=0,n_samples=400,\n                          random_state=RANDOM_STATE)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=RANDOM_STATE)\n\nclf = DecisionTree(max_depth=4, criterion='gini')\nclf.fit(X_train, y_train)","cb35f4e1":"y_pred = clf.predict(X_test)\nprob_pred = clf.predict_proba(X_test)\naccuracy = accuracy_score(y_test,y_pred)","1dcd60e0":"print(\"Accuracy:\",accuracy)","e34fe349":"if(sum(np.argmax(prob_pred,axis=1) - y_pred)==0):\n    print(\"predict_proba works!\")","098ef82b":"plt.suptitle(\"Accuracy = {0:.2f}\".format(accuracy))\nplt.subplot(121)\nplt.scatter(X_test[:,0],X_test[:,1],c=y_pred,cmap=plt.cm.coolwarm)\nplt.title(\"Predicted class labels\")\nplt.axis(\"equal\")\nplt.subplot(122)\nplt.scatter(X_test[:,0],X_test[:,1],c=y_test,cmap=plt.cm.coolwarm)\nplt.title(\"True class labels\")\nplt.axis('equal');","dde3e21d":"digits = load_digits()\n\nX = digits.data\ny = digits.target\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                   test_size=0.2,random_state=RANDOM_STATE)\n\n\nclf2 = DecisionTree(max_depth=2,criterion='gini',debug=True)\nclf2.fit(X_train, y_train)\nprint(accuracy_score(clf2.predict(X_test), y_test))","107aa222":"clf1 = DecisionTree(max_depth=2,criterion='entropy',debug=True)\nclf1.fit(X_train,y_train)\n\nprint(accuracy_score(clf1.predict(X_test), y_test))","45f740aa":"%%time\n\ntree_params = {'max_depth': list(range(3,11)),\n              'criterion':['gini','entropy']}\n\nclf = GridSearchCV(DecisionTree(),tree_params,cv=5,scoring='accuracy',verbose=True, n_jobs=8)\nclf.fit(X_train,y_train)","3adbd5fd":"clf.best_score_, clf.best_params_","ea14d827":"scores = np.array(clf.cv_results_['mean_test_score'])\nscores = scores.reshape(len(tree_params['criterion']),\n                       len(tree_params['max_depth']))\n\nfor ind,i in enumerate(tree_params['criterion']):\n    plt.plot(tree_params['max_depth'],scores[ind], label=str(i))\n\nplt.legend(loc='best')\nplt.xlabel('max_depth')\nplt.ylabel('Accuracy')\nplt.show();","b7f8bc3f":"clf = DecisionTree(max_depth=9,criterion='entropy')\nclf.fit(X_train,y_train)\nprobs = clf.predict_proba(X_test)","79a153f0":"mean_probs = np.mean(probs,axis=0)\nprint(mean_probs)","4bc080fa":"X, y = make_regression(n_features=1, n_samples=200,bias=0,noise=5,\n                      random_state=RANDOM_STATE)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n\nreg = DecisionTree(max_depth=6,criterion='mad_median')\nreg.fit(X_train,y_train)\ny_pred = reg.predict(X_test)","dd77b84c":"mse = mean_squared_error(y_test,y_pred)\nprint(\"Mean Squared Error:\",mse)","9937f1af":"plt.scatter(X_test[:,0],y_test,color='black')\nplt.scatter(X_test[:,0],y_pred,color='green')\nplt.title(\"MSE = {0:.2}\".format(mse));","d3324c60":"boston = load_boston()\n\nX = boston.data\ny = boston.target\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=RANDOM_STATE)\n\nclf1 = DecisionTree(max_depth=2,criterion='variance',debug=True)\nclf1.fit(X_train, y_train)","d407853b":"print(mean_squared_error(clf1.predict(X_test),y_test))","5727e2de":"clf2 = DecisionTree(max_depth=2, criterion='mad_median',debug=True)\nclf2.fit(X_train,y_train)\n\nprint(mean_squared_error(clf2.predict(X_test),y_test))","4404563f":"%%time\n\ntree_params = {'max_depth': list(range(2,9)),\n              'criterion':['variance','mad_median']}\n\nreg = GridSearchCV(DecisionTree(), tree_params,\n                  cv=5, scoring='neg_mean_squared_error',n_jobs=8)\n\nreg.fit(X_train, y_train)","d1dfee94":"scores = -np.array(reg.cv_results_['mean_test_score'])\nscores = scores.reshape(len(tree_params['criterion']),len(tree_params['max_depth']))\n\nfor ind, i in enumerate(tree_params['criterion']):\n    plt.plot(tree_params['max_depth'],scores[ind],label=str(i))\n    \nplt.legend()\nplt.xlabel('max_depth')\nplt.ylabel('MSE')\nplt.show();","bf53cd67":"print(\"Best params:\",reg.best_params_)\nprint(\"Best cross vaildation MSE\",abs(reg.best_score_))","5c5fdc79":"# Regression","d16ce6d0":"# Classification","e0014fe5":"# Decision Trees","a19181dc":"### The Node implements a node in the DS**"}}