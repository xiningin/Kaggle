{"cell_type":{"01560a01":"code","ceab0185":"code","ccf29582":"code","3239d29d":"code","4bd7b2e2":"code","90124b6f":"code","45a51b73":"code","3527c8ed":"code","8ff143ab":"code","37c69b4e":"code","436f91cc":"code","3944c1c0":"code","ef5546e0":"code","0ce4aa6d":"code","7750749f":"code","d0823511":"code","1a369129":"code","a763178d":"code","88efcb95":"code","14cf1530":"code","462a562f":"code","927fa55e":"code","10b022a7":"code","d2e23417":"code","ce1998f6":"code","8dad40d3":"code","91bb1364":"code","5aaaf4e8":"code","f72e3cdf":"code","e72c809b":"code","526183bc":"code","5ce98eb8":"code","774039ca":"code","9407bfe6":"code","2aefcf31":"code","29b811c7":"code","409a058a":"code","f2331dd5":"code","7f6534ed":"code","bccaa90c":"code","be5817dc":"code","e848b30c":"code","a72de50c":"code","b0d6ce4e":"code","45726a83":"code","5a10d892":"code","76d019e0":"code","8b4a3c1e":"code","309f47f2":"code","e376e10e":"code","ba7e0fd4":"code","7b08f950":"code","475f67dd":"code","0cb4f22f":"code","79eb72db":"code","2411384c":"code","bdcdc27c":"code","36f5ff2a":"code","208a6ef0":"code","37087eab":"code","a689929e":"code","10750ef3":"code","17830f53":"code","6ad27d48":"code","2053bce8":"code","6b53c20f":"code","83d753d7":"code","908f7672":"code","529f0dfb":"code","3f5b3e3e":"code","89d8bfba":"code","d20f9cfa":"code","ea080957":"code","88bb9bd0":"code","8763da24":"code","ce35c911":"code","3e27d98f":"code","246fb8ac":"markdown","3858b056":"markdown","0cc3dcc8":"markdown","968fe81f":"markdown","f67ef3fd":"markdown","d773ba63":"markdown","2524ec68":"markdown","4c554cc6":"markdown","ad445ac9":"markdown","50971ab5":"markdown","f67549b2":"markdown","9302844e":"markdown","8f9d0c6d":"markdown","c55d5388":"markdown","f846fbfa":"markdown","9aafe5d5":"markdown","fa463f47":"markdown","a09cb9c9":"markdown","752b8b74":"markdown","b188dec3":"markdown","a4fb8b26":"markdown","338021c2":"markdown","afeb94d2":"markdown","d76b5e5c":"markdown","219de349":"markdown","7d154ae5":"markdown","6134758e":"markdown","a701fec5":"markdown","9ac0e5df":"markdown","0a8d5e75":"markdown","9dd37ef1":"markdown","82d7127f":"markdown","a87e3f37":"markdown","5442064c":"markdown","1c54a7a9":"markdown","8ed76cd4":"markdown","37399e4b":"markdown","749b289f":"markdown","816f6372":"markdown","94d2658d":"markdown","ec36bc10":"markdown","536a14d0":"markdown","5141bbf6":"markdown","a73673ab":"markdown","4c176bc8":"markdown","9ae9b3bc":"markdown","949d169a":"markdown","e252788c":"markdown","b7a0a5d3":"markdown","39a122fb":"markdown","209beb05":"markdown","2c0b8985":"markdown","5c0dcca3":"markdown","9bc2d79d":"markdown","25573fc1":"markdown","82dd4739":"markdown","a498ed3b":"markdown","de373748":"markdown","fed70263":"markdown","d1b7ce47":"markdown","dc86cea0":"markdown","9406d7a3":"markdown","1cc4a0d5":"markdown","e48f5017":"markdown","413dac3d":"markdown","e483c78d":"markdown","dfad3ce8":"markdown","e5816563":"markdown","ccaec7f4":"markdown","1a40f730":"markdown","bbff698f":"markdown","e50c7fab":"markdown","4d01fdbf":"markdown","89aea77d":"markdown","382f3ad1":"markdown","8f185028":"markdown","152864b0":"markdown","c0b75fd2":"markdown","4297590a":"markdown","f57595b8":"markdown","4ef9d850":"markdown","2270aa13":"markdown","1e48fc9e":"markdown","4df2b84e":"markdown","be733cf1":"markdown","b4218c7a":"markdown","3d4ccd2a":"markdown","87d6e9ab":"markdown","7deec9bf":"markdown","f6749372":"markdown","337e1128":"markdown","b360ca3c":"markdown","3bb9a40d":"markdown","231805f5":"markdown"},"source":{"01560a01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport math\nimport pandas as pd\nimport numpy as  np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport warnings\nfrom IPython.core.display import display, HTML\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nsns.set()\n%matplotlib inline\nwarnings.filterwarnings(action='ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ceab0185":"df_2018_MCQ=pd.read_csv(r'..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv',low_memory=False)\ndf_2018_Text=pd.read_csv(r'..\/input\/kaggle-survey-2018\/freeFormResponses.csv',low_memory=False)\n\ndf_2019_MCQ=pd.read_csv(r'..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv',low_memory=False)\ndf_2019_Text=pd.read_csv(r'..\/input\/kaggle-survey-2019\/other_text_responses.csv',low_memory=False)\n\ndf_2020=pd.read_csv(r'..\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv',low_memory=False)\n\ndf_2021=pd.read_csv(r'..\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv',low_memory=False)","ccf29582":"q_2019=pd.concat([df_2019_MCQ.iloc[0,:].T,df_2019_Text.iloc[0,:].T])\nr_2019_ori=r_2019= pd.concat([df_2019_MCQ.iloc[1:,:],df_2019_Text.iloc[1:,:]],axis=1)\n# print(set(r_2019.columns).symmetric_difference(q_2019.index))\n\nq_2018=pd.concat([df_2018_MCQ.iloc[0,:].T,df_2018_Text.iloc[0,:].T])\nr_2018_ori=r_2018= pd.concat([df_2018_MCQ.iloc[1:,:],df_2018_Text.iloc[1:,:]],axis=1)\n# print(set(r_2018.columns).symmetric_difference(q_2018.index))\n\nq_2020 = df_2020.iloc[0, :].T\nr_2020_ori=r_2020 = df_2020.iloc[1:, :]\n\nq_2021 = df_2021.iloc[0, :].T\nr_2021_ori=r_2021 = df_2021.iloc[1:, :]","3239d29d":"Q2_MAPP={'Female':'Female', 'Male':'Male', 'Man':'Male', 'Nonbinary':'Nonbinary', 'Prefer not to say':'Prefer not to say',\n       'Prefer to self-describe':'Prefer not to say', 'Woman':'Female'}\n\n\n\nEDUCATION_MAP={'Bachelor\u2019s degree':\"Bachelor's\",\n 'Doctoral degree':\"Doctor's\",\n 'I prefer not to answer':\"None\",\n 'Master\u2019s degree':\"Master's\",\n 'No formal education past high school':\"High School\",\n 'Professional degree':\"Professional Degree\",\n 'Professional doctorate':\"Doctor's\",\n 'Some college\/university study without earning a bachelor\u2019s degree':\"Some College\"}\n\nEDUCATION_ORDER=[\"None\",\"High School\",\"Some College\",\"Professional Degree\",\"Bachelor's\", \"Master's\",\"Doctor's\"]\n\n\nCOMPENSATION={ \n        '$0-999': '0-10k',\n        '1,000-1,999': '0-10k',\n        '2,000-2,999': '0-10k',\n        '3,000-3,999': '0-10k',\n        '4,000-4,999': '0-10k',\n        '5,000-7,499': '0-10k',\n        '7,500-9,999': '0-10k',\n        '10,000-14,999': '10-20k',\n        '15,000-19,999': '10-20k',\n        '20,000-24,999': '20-30k',\n        '25,000-29,999': '20-30k',\n        '30,000-39,999': '30-40k',\n        '40,000-49,999': '40-50k',\n        '50,000-59,999': '50-60k',\n        '60,000-69,999': '60-70k',\n        '70,000-79,999': '70-80k',\n        '80,000-89,999': '80-90k',\n        '90,000-99,999': '90-100k',\n        '100,000-124,999': '100-125k',\n        '125,000-149,999': '125-150k',\n        '150,000-199,999': '150-200k',\n        '200,000-249,999': '200-250k',\n        '300,000-500,000': '300-500k',\n        '> $500,000': np.nan,\n        '0-10,000': '0-10k',\n        '10-20,000': '10-20k',\n        '20-30,000': '20-30k',\n        '30-40,000': '30-40k',\n        '40-50,000': '40-50k',\n        '50-60,000': '50-60k',\n        '60-70,000': '60-70k',\n        '70-80,000': '70-80k',\n        '80-90,000': '80-90k',\n        '90-100,000': '90-100k',\n        '100-125,000': '100-125k',\n        '125-150,000': '125-150k',\n        '150-200,00': '150-200k',\n        '200-250,000': '200-250k',\n        '300-400,000': '300-500k',\n        '400-500,000': '300-500k',\n        '500,000+': np.nan,\n        'I do not wish to disclose my approximate yearly compensation': np.nan,\n        '$0 ($USD)':'0-10k','$100,000 or more ($USD)':'100-125k','$10,000-$99,999':'10-100k','$1-$99':'0-10k','$1000-$9,999':'0-10k','$100-$999':'0-10k'\n        \n    }\n\nCOMPENSATION_VALUE={np.nan:0,\n                    '0-10k':5000,\n                    '10-20k':15000,\n                    '20-30k':25000,\n                    '30-40k':35000,\n                    '40-50k':45000,\n                    '50-60k':55000,\n                    '60-70k':65000,\n                    '70-80k':75000,\n                    '80-90k':85000,\n                    '90-100k':95000,\n                    '10-100k':50000,\n                    '100-125k':112500,\n                    '125-150k':137500,\n                    '150-200k':175000,\n                    '200-250k':225000,\n                    '300-500k':400000    \n                \n}\n\nEXPERIENCE_ORDER=['None','< 1 year','1-3 years','3-5 years','5-10 years','10-20 years','20+ years']\n\nEXPERIENCE_PROFILE=[\"Beginners\",'Seasoned Coders','Data Scientists','Seasoned Data Scientists', \"ML Veterans\"]\n\nPROGRAMMING_LANG=['Bash', 'C', 'C++', 'Java', 'Javascript', 'Julia', 'MATLAB', 'None', 'Other', 'Python', 'R', 'SQL', 'Swift']\n\nJOB_TITLE_MAP={'Business Analyst':'BA',\n 'Chief Officer':'CO',\n 'Consultant':'Consultant',\n 'Currently not employed':'Not employed',\n 'DBA\/Database Engineer':'DBA',\n 'Data Analyst':'DA',\n 'Data Engineer':'DE',\n 'Data Journalist':'DJ',\n 'Data Scientist':'DS',\n 'Developer Advocate':'Dev Advocate',\n 'Developer Relations\/Advocacy':'Dev Advocate',\n 'Machine Learning Engineer':'ML E',\n 'Manager':'Manager',\n 'Marketing Analyst':'Marketing Anyt',\n 'Not employed':'Not employed',\n 'Other':'Other',\n 'Principal Investigator':'Principal Invest',\n 'Product Manager':'PM',\n 'Product\/Project Manager':'PM',\n 'Program\/Project Manager':'PM',\n 'Research Assistant':'RA',\n 'Research Scientist':'RS',\n 'Salesperson':'Sales',\n 'Software Engineer':'Dev',\n 'Statistician':'Stats',\n 'Student':'Student'}\n\nEXPERIENCE_MAPP={'1-2 years':'1-3 years',\n '1-3 years':'1-3 years',\n '10-20 years':'10-20 years',\n '20+ years':'20+ years',\n '20-30 years':'20+ years',\n '3-5 years':'3-5 years',\n '30-40 years':'20+ years',\n '40+ years':'20+ years',\n '5-10 years': '5-10 years',\n '< 1 year': '< 1 year',\n '< 1 years': '< 1 year',\n 'I have never written code':'None',\n 'I have never written code and I do not want to learn':'None',\n 'I have never written code but I want to learn':'None',\n 'None':'None'\n}","4bd7b2e2":"plt.rcParams[\"figure.autolayout\"] = True\nplt.rcParams['font.serif'] = ['SerifFamily1', 'SerifFamily2']\nplt.rcParams['font.weight']='bold'\n\ndef pltSumm(df,i_groupby,nmax,SUBPLOT_ORDER,plot_title):\n   \n\n    fig, ax = plt.subplots( ncols=len(SUBPLOT_ORDER),tight_layout={'pad':1}, sharey=True,linewidth=2,constrained_layout=True, figsize=( len(SUBPLOT_ORDER)*5,4), #gridspec_kw={'height_ratios': [1,1]},\n           edgecolor='steelblue',\n           facecolor='w' )\n    incList=df.pct_change(axis='columns', periods=1).replace([np.inf,-np.inf,np.nan],0).groupby(i_groupby).idxmax(axis=0).bfill(axis=1)\n  \n    for idx,gd in enumerate(SUBPLOT_ORDER):\n            \n        maxList=df.loc[gd].idxmax(axis=0)\n        minList=df.loc[gd].idxmin(axis=0)\n        if incList.loc[gd].values is np.empty:\n            incListvals=[\"NA\"]\n        else:\n            incListvals=[x for _, x in incList.loc[gd].values]\n            \n         \n        \n        df_gp=pd.DataFrame({\"Topmost\":maxList,'Lowest ':minList,\"who's catching up\":incListvals})\n   \n \n        # Get some lists of color specs for row and column headers\n        rcolors = plt.cm.Blues(np.full(len(df_gp.T.index), 0.1))\n        ccolors = plt.cm.Blues(np.full(len(df_gp.T.columns), 0.1))\n        colors = [['lightblue']*len(df_gp.T.columns),['salmon']*len(df_gp.T.columns),['lightgreen']*len(df_gp.T.columns)]\n        c=['blue','red','green']\n        tb=ax[idx].table(cellText=df_gp.T.values,cellColours=colors,\n                      rowLabels=df_gp.T.index,\n                      rowColours=rcolors,\n                      rowLoc='center',\n                      colColours=ccolors,\n                      colLabels=df_gp.T.columns,\n                      loc='center')\n        \n        \n        ax[idx].axis(\"off\") \n        tb.auto_set_font_size(False)\n        tb.set_fontsize(13) \n        tb.scale(1, 1.5)\n        # for i in range(len(df_gp.T.index)):\n        #     for j in range(len(df_gp.T.columns)):\n        #         tb[(i+1, j)].get_text().set_color(c[i])\n        \n        # Add title\n        ax[idx].set_title(gd,fontweight=\"bold\", size=20)\n    fig.tight_layout()\n    fig.suptitle(plot_title)\n    \n    # return fig\n \n\n","90124b6f":"def drawplot(df,i_groupby,nmax,SUBPLOT_ORDER,plot_title, yaxistitle,annfmt):\n    \n    fig = make_subplots(rows=1, cols=len(df.index.get_level_values(0).unique().values),\n                        subplot_titles=SUBPLOT_ORDER,shared_yaxes=True)\n    \n    annotations=[ a.to_plotly_json() for a in fig.layout.annotations]\n    subplot_=df.index.get_level_values(0).unique().values\n    incList=df.pct_change(axis='columns', periods=1).groupby(i_groupby).idxmax(axis=0).bfill(axis=1)\n    \n    for idx,gd in enumerate(SUBPLOT_ORDER):\n       \n        maxList=  df.loc[gd].sort_values(by='2021', ascending=False)[:nmax]   \n\n        minList=df.loc[gd].idxmin(axis=0)\n        if incList.loc[gd].values is np.empty:\n            incListvals=[\"NA\"]\n        else:\n            incListvals=[x for _, x in incList.loc[gd].values]\n\n        xidx= \"\" if idx==0 else str(idx+1)\n        yidx= \"\" if idx==0 else str(idx)\n        dash=\"\"\n        for col,nme in enumerate(df.loc[gd,:].T.columns):\n            clr='rgb(115,115,115)'\n            if (nme in incListvals) &  (nme not in maxList.T.columns):\n                clr= 'rgb(49,130,189)' \n            elif nme in maxList.T.columns:\n                # dash= 'dash'\n                clr='green'\n          \n            else:\n                clr='rgb(189,189,189)'\n        \n            fig.add_trace(go.Scatter(x=df.loc[gd,:].T.index,\n                                y=df.loc[gd,:].T.loc[:,nme], mode='lines',\n                                name=nme,text=nme,\n                                line=dict(color=clr, width=2),\n                                connectgaps=True,\n                                ),row=1,col=idx+1)\n            \n                        \n            \n\n        for i,elm in enumerate(maxList.T.columns):\n     \n\n            text_pnt= list(map(round,[maxList.T[elm][0],maxList.T[elm][-1]],[1,2]))    \n\n            fig.add_trace(go.Scatter(\n                    x= [maxList.T.index[0],maxList.T.index[-1]],\n                    y= [maxList.T[elm][0],maxList.T[elm][-1]],\n                    mode='markers',name=elm,\n                    marker=dict(color='rgb(49,130,189)', size=8)\n                    ),row=1,col=idx+1)\n            \n            # endpoints \n            annotations.append(dict(xref= 'x'+xidx,yref='y'+yidx, x=0.05, y=maxList.T[elm][0], xanchor='right', yanchor='bottom',\n                                  text= annfmt.format(text_pnt[0]),font=dict(family='calibri', size=7),showarrow=False))\n            \n            annotations.append(dict(xref= 'x'+xidx,yref='y'+yidx, x=3, y=maxList.T[elm][-1],xanchor='left', yanchor='bottom',\n                                text= annfmt.format(text_pnt[-1]),font=dict(family='calibri',size=7),showarrow=False))\n            #line annotation                    \n            annotations.append(dict( xref= 'x'+xidx,yref='y'+yidx, x=2,y= maxList.T[elm][-1],xanchor='auto',yanchor='bottom',\n                                    text='<b>'+elm+'<\/b>', font=dict(family='Arial',size=7),showarrow=False)) \n                       \n                        \n    #axes layout\n    fig.update_layout(xaxis_title=\"Year\",yaxis_title=yaxistitle,\n                font=dict(family=\"Arial\",size=8,color=\"Black\"),\n                showlegend=False,yaxis={\"visible\":True, },\n                plot_bgcolor='white')\n    \n    # Title\n    annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.15,xanchor='left', yanchor='bottom',\n                                text=plot_title,\n                                font=dict(family='Arial',size=15,color='rgb(37,37,37)'),showarrow=False))\n    # Source\n    annotations.append(dict(xref='paper', yref='paper', x=0.5, y=-0.1, xanchor='center', yanchor='top',\n                               text='Data Source: Kaggle 2018-2021 Survey',font=dict(family='Arial', size=12,color='rgb(150,150,150)'), showarrow=False))\n         \n        \n    fig.update_layout(annotations =annotations)\n    \n    return fig\n\n\n","45a51b73":"def color(val):\n    if val > 0 :\n        color = 'lightgreen'\n    elif val == 0 :\n        color = 'yellow'\n    elif val < 0:\n        color = 'red'\n    return 'background-color: %s' % color","3527c8ed":"r_2018.replace({'Q3':{'United States of America' : 'USA', 'United Kingdom of Great Britain and Northern Ireland' : 'UK' }},inplace=True) \nr_2019.replace({'Q3':{'United States of America' : 'USA', 'United Kingdom of Great Britain and Northern Ireland' : 'UK' }},inplace=True) \nr_2020.replace({'Q3':{'United States of America' : 'USA', 'United Kingdom of Great Britain and Northern Ireland' : 'UK' }},inplace=True)  \nr_2021.replace({'Q3':{'United States of America' : 'USA', 'United Kingdom of Great Britain and Northern Ireland' : 'UK' }},inplace=True) \n\nr_2018.replace({'Q1':Q2_MAPP},inplace=True) \nr_2019.replace({'Q2':Q2_MAPP},inplace=True) \nr_2020.replace({'Q2':Q2_MAPP},inplace=True)  \nr_2021.replace({'Q2':Q2_MAPP},inplace=True) \n\nRes_Q3_top=r_2020['Q3'].value_counts().sort_values(ascending =False).drop(['Other'])[:10].index | \\\n           r_2020['Q3'].value_counts().sort_values(ascending =False).drop(['Other'])[:10].index | \\\n           r_2020['Q3'].value_counts().sort_values(ascending =False).drop(['Other'])[:10].index | \\\n           r_2021['Q3'].value_counts().sort_values(ascending =False).drop(['Other'])[:10].index\n","8ff143ab":"r_2018.replace({'Q1':Q2_MAPP},inplace=True) \nr_2019.replace({'Q2':Q2_MAPP},inplace=True) \nr_2020.replace({'Q2':Q2_MAPP},inplace=True)  \nr_2021.replace({'Q2':Q2_MAPP},inplace=True) \n\ndf_Q2=pd.DataFrame({  '2018':r_2018['Q1'].value_counts().fillna(0).sort_index(),\n                      '2019':r_2019['Q2'].value_counts().fillna(0).sort_index(),\n                      '2020':r_2020['Q2'].value_counts().fillna(0).sort_index(),\n                      '2021':r_2021['Q2'].value_counts().fillna(0).sort_index()})\ndf_Q2.fillna(0,inplace=True)\ndf_Q2.reset_index(inplace=True)\ndf_Q2['subplot']='Gender'\ndf_Q2.set_index(['subplot','index'], inplace=True)\ndf_Q2_total=df_Q2.sum(axis=0)\n\ndf_Q2_Per=pd.DataFrame({'2018':((df_Q2['2018']).div(df_Q2_total['2018'], 0)*100),\\\n                          '2019':((df_Q2['2019']).div(df_Q2_total['2019'], 0)*100),\\\n                          '2020':((df_Q2['2020']).div(df_Q2_total['2020'], 0)*100),\\\n                          '2021':((df_Q2['2021']).div(df_Q2_total['2021'], 0)*100)})\n\ndrawplot(df_Q2,'subplot',4,['Gender'],'<b>Fig 3.1 a).What is the gender group of Kaggle Survey 2018-2021 participants<\/b>',\"No. of kagglers\",\\\n    '<i><b>{}<\/b><\/i>').show()\n# df_Q2","37c69b4e":"drawplot(df_Q2_Per,'subplot',4,['Gender'],'<b>Fig Fig 3.1b).What is the Percentage gender group of Kaggle Survey 2018-2021 participants<\/b>',\"% of kagglers\",\\\n    '<i><b>{}%<\/b><\/i>').show()","436f91cc":"df_Q2Q1=pd.DataFrame({ '2018':r_2018.groupby(['Q1'])['Q2'].value_counts().fillna(0).sort_index()[['Male','Female']],\n                      '2019':r_2019.groupby(['Q2'])['Q1'].value_counts().fillna(0).sort_index()[['Male','Female']],\n                      '2020':r_2020.groupby(['Q2'])['Q1'].value_counts().fillna(0).sort_index()[['Male','Female']],\n                      '2021':r_2021.groupby(['Q2'])['Q1'].value_counts().fillna(0).sort_index()[['Male','Female']]})\n\ndf_Q2Q1.fillna(0,inplace=True)\ndf_Q2Q1.index.rename('Q2',level=0,inplace=True)\ndf_Q2Q1_total=df_Q2Q1.sum(axis=0)\n\ndf_Q2Q1_Per=pd.DataFrame({'2018':((df_Q2Q1['2018']).div(df_Q2Q1_total['2018'], 0)*100),\\\n                          '2019':((df_Q2Q1['2019']).div(df_Q2Q1_total['2019'], 0)*100),\\\n                          '2020':((df_Q2Q1['2020']).div(df_Q2Q1_total['2020'], 0)*100),\\\n                          '2021':((df_Q2Q1['2021']).div(df_Q2Q1_total['2021'], 0)*100)})\n\n\ndrawplot(df_Q2Q1_Per,'Q2',3,['Female','Male'],'<b>Fig3.2 What is the age group of Female & Male Participants in the Kaggle Survey 2018-2021<\/b>',\"% of kagglers\",\\\n    '<i><b>{}%<\/b><\/i>').show()","3944c1c0":"pltSumm(df_Q2Q1_Per,'Q2',2,['Female','Male'],'Table 3.2.Age group of kaggle survey respondents by Gender')","ef5546e0":"r_2018.replace({'Q3':{'United States of America' : 'USA', 'United Kingdom of Great Britain and Northern Ireland' : 'UK' }},inplace=True) \nr_2019.replace({'Q3':{'United States of America' : 'USA', 'United Kingdom of Great Britain and Northern Ireland' : 'UK' }},inplace=True) \nr_2020.replace({'Q3':{'United States of America' : 'USA', 'United Kingdom of Great Britain and Northern Ireland' : 'UK' }},inplace=True)  \nr_2021.replace({'Q3':{'United States of America' : 'USA', 'United Kingdom of Great Britain and Northern Ireland' : 'UK' }},inplace=True) \n\nRes_Q3_top=r_2020['Q3'].value_counts().sort_values(ascending =False).drop(['Other'])[:10].index | \\\n           r_2020['Q3'].value_counts().sort_values(ascending =False).drop(['Other'])[:10].index | \\\n           r_2020['Q3'].value_counts().sort_values(ascending =False).drop(['Other'])[:10].index | \\\n           r_2021['Q3'].value_counts().sort_values(ascending =False).drop(['Other'])[:10].index\n\ndf_Q2Q3=pd.DataFrame({ '2018':r_2018.groupby(['Q1'])['Q3'].value_counts().fillna(0).sort_index()[['Male','Female']],\n                      '2019':r_2019.groupby(['Q2'])['Q3'].value_counts().fillna(0).sort_index()[['Male','Female']],\n                      '2020':r_2020.groupby(['Q2'])['Q3'].value_counts().fillna(0).sort_index()[['Male','Female']],\n                      '2021':r_2021.groupby(['Q2'])['Q3'].value_counts().fillna(0).sort_index()[['Male','Female']]\n                      })\ndf_Q2Q3.fillna(0,inplace=True)\ndf_Q2Q3.index.rename('Q2',level=0,inplace=True)\n\ndf_Q2Q3_top=df_Q2Q3[df_Q2Q3.index.get_level_values(1).isin(Res_Q3_top)]\ndf_total=df_Q2Q3_top.sum(axis=0)\n\ndf_Q2Q3_Per=pd.DataFrame({'2018':((df_Q2Q3_top['2018']).div(df_total['2018'], 0)*100),\\\n                          '2019':((df_Q2Q3_top['2019']).div(df_total['2019'], 0)*100),\\\n                          '2020':((df_Q2Q3_top['2020']).div(df_total['2020'], 0)*100),\\\n                          '2021':((df_Q2Q3_top['2021']).div(df_total['2021'], 0)*100)})\n\n\ndrawplot(df_Q2Q3_Per,'Q2',4,['Female','Male'],'<b>Fig Fig 3.3.Demographic occupanyof Female & Male Participants in the Kaggle Survey 2018-2021<\/b>',\"% of kagglers\",\\\n    '<i><b>{}%<\/b><\/i>').show()","0ce4aa6d":"pltSumm(df_Q2Q3_Per,'Q2',2,['Female','Male'],'Table 3.3. Demographic occupany of Female & Male Participants in the Kaggle Survey 2018-2021,Highest,\\n lowest and which country is gaing traction in DS\/ML')","7750749f":"df_Q2Q4=pd.DataFrame({'2018':r_2018.replace(EDUCATION_MAP).groupby(['Q4'])['Q1'].value_counts().fillna(0).sort_index(),\n                      '2019':r_2019.replace(EDUCATION_MAP).groupby(['Q4'])['Q2'].value_counts().fillna(0).sort_index(),\n                      '2020':r_2020.replace(EDUCATION_MAP).groupby(['Q4'])['Q2'].value_counts().fillna(0).sort_index(),\n                      '2021':r_2021.replace(EDUCATION_MAP).groupby(['Q4'])['Q2'].value_counts().fillna(0).sort_index()})\n\ndf_Q2Q4.fillna(0,inplace=True)\ndf_Q2Q4.index.rename('Q2',level=0,inplace=True)\ndf_Q4Q2_fitered=df_Q2Q4[df_Q2Q4.index.get_level_values(1).isin(['Female','Male'])]\n\n\n# df_Q4Q2_fitered.reindex(axis='index', level=0, labels=['None','High School','Some College','Professional Degree',\"Bachelor's\",\"Master's\",\"Doctor's\"]) \n\ndrawplot(df_Q4Q2_fitered,'Q2',2,EDUCATION_ORDER,\"<b>Fig 3.4. Kagglers: What's the educational back ground? Is there gender diversity?<\/b>\",\"Count of kagglers\",\\\n    '<i><b>{}<\/b><\/i>')\n        \n","d0823511":"pltSumm(df_Q4Q2_fitered,'Q2',1,EDUCATION_ORDER,\"Table 3.4 Female & Male Participants Wrt Education who's leading, tailing and who's catching up?\")","1a369129":"r_2018['Q6_new']=r_2018['Q24'].fillna('None').replace(EXPERIENCE_MAPP)\nr_2019['Q6_new']=r_2019['Q15'].fillna('None').replace(EXPERIENCE_MAPP)\nr_2020['Q6_new']=r_2020['Q6'].fillna('None').replace(EXPERIENCE_MAPP)\nr_2021['Q6_new']=r_2021['Q6'].fillna('None').replace(EXPERIENCE_MAPP)\n\nr_2018['Q9'].fillna('$0 ($USD)',inplace=True)\nr_2019['Q10'].fillna('$0 ($USD)',inplace=True)\nr_2020['Q25'].fillna('$0 ($USD)',inplace=True)\nr_2021['Q25'].fillna('$0 ($USD)',inplace=True)\n\nr_2018['Q25_new']=r_2018['Q9'].map(COMPENSATION).map(COMPENSATION_VALUE) #compesation\nr_2019['Q25_new']=r_2019['Q10'].map(COMPENSATION).map(COMPENSATION_VALUE)#compesation  \nr_2020['Q25_new']=r_2020['Q25'].map(COMPENSATION).map(COMPENSATION_VALUE)\nr_2021['Q25_new']=r_2021['Q25'].map(COMPENSATION).map(COMPENSATION_VALUE)\n\ndf_Q6Q2Q25=pd.DataFrame({'2018':r_2018.groupby(['Q6_new','Q1'])['Q25_new'].mean().sort_index(),\n                         '2019':r_2019.groupby(['Q6_new','Q2',])['Q25_new'].mean().sort_index(),\n                         '2020':r_2020.groupby(['Q6_new','Q2',])['Q25_new'].mean().sort_index(),\n                         '2021':r_2021.groupby(['Q6_new','Q2',])['Q25_new'].mean().sort_index()})\n\n\ndf_Q6Q2Q25_fitered=df_Q6Q2Q25[df_Q6Q2Q25.index.get_level_values(1).isin(['Female','Male'])]\ndf_Q6Q2Q25_fitered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q2Q25_fitered,'subplot',2,EXPERIENCE_ORDER,\"<b> Fig 3.5. Post Pandemic era:How has the yearly CTC fluctuated over the years w.r.t Experience?<br> How is the gender bias performance?<b> \", '<b>AVG Yearly CTC($)<\/b>','<i><b>${:,}<\/b><\/i>')\n","a763178d":"pltSumm(df_Q6Q2Q25_fitered,'subplot',1,EXPERIENCE_ORDER,\"Table 3.5. Female & Male CTC wrt Experience :leading, tailing and who's catching up?\")","88efcb95":"r_2018['Q4_new']=r_2018['Q4'].replace(EDUCATION_MAP)\nr_2019['Q4_new']=r_2019['Q4'].replace(EDUCATION_MAP)\nr_2020['Q4_new']=r_2020['Q4'].replace(EDUCATION_MAP)\nr_2021['Q4_new']=r_2021['Q4'].replace(EDUCATION_MAP)\n\ndf_Q4Q6Q25=pd.DataFrame({'2018':r_2018.groupby(['Q4_new','Q6_new',])['Q25_new'].mean().sort_index(),\n                      '2019':r_2019.groupby(['Q4_new','Q6_new',])['Q25_new'].mean().sort_index() ,      \n                        '2020':r_2020.groupby(['Q4_new','Q6_new',])['Q25_new'].mean().sort_index(),\n                        '2021':r_2021.groupby(['Q4_new','Q6_new',])['Q25_new'].mean().sort_index()})\n\ndf_Q4Q6Q25.fillna(0,inplace=True)\ndf_Q4Q6Q25.index.rename('subplot',level=0,inplace=True)\ndrawplot(df_Q4Q6Q25,'subplot',2,EDUCATION_ORDER,\"<b>Fig 4.1.Post Pandemic era:How has the yearly CTC fluctuated over the years w.r.t Highest Education and Experience?<br>Which Counts Experience Or Education?<b> \", '<b>AVG Yearly CTC($)<\/b>','<i><b>${:,}<\/b><\/i>')\n","14cf1530":"r_2018['Q5_new']=r_2018['Q6'].replace(JOB_TITLE_MAP)\nr_2019['Q5_new']=r_2019['Q5'].replace(JOB_TITLE_MAP)\nr_2020['Q5_new']=r_2020['Q5'].replace(JOB_TITLE_MAP)\nr_2021['Q5_new']=r_2021['Q5'].replace(JOB_TITLE_MAP)\n\ndf_Q6Q5Q25=pd.DataFrame({'2018':r_2018.groupby(['Q6_new','Q5_new',])['Q25_new'].mean().sort_index(),\n                        '2019':r_2019.groupby(['Q6_new','Q5_new',])['Q25_new'].mean().sort_index(),\n                        '2020':r_2020.groupby(['Q6_new','Q5_new',])['Q25_new'].mean().sort_index(),\n                      '2021':r_2021.groupby(['Q6_new','Q5_new',])['Q25_new'].mean().sort_index()})\n\ndf_Q6Q5Q25.fillna(0,inplace=True)\ndf_Q6Q5Q25.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q5Q25,'subplot',3,EXPERIENCE_ORDER,\"<b>Fig Fig 4.2.Post Pandemic era:How has the yearly CTC fluctuated over the years w.r.t Job Title and Experience?<br>Which Counts Job Title Or Experience?<b> \", '<b>AVG Yearly CTC($)<\/b>','<i><b>${:,}<\/b><\/i>')\n","462a562f":"pltSumm(df_Q6Q5Q25,'subplot',1,EXPERIENCE_ORDER,\"Table 4.2. Which Job title earns higher, Which job pays less, & which job is getting noticed over the years wrt experience?\")","927fa55e":"df_Q6Q3Q25=pd.DataFrame({ '2018':r_2018.groupby(['Q6_new','Q3',],sort = True)['Q25_new'].mean().sort_index(),\n                          '2019':r_2019.groupby(['Q6_new','Q3',],sort = True)['Q25_new'].mean().sort_index(),\n                      '2020':r_2020.groupby(['Q6_new','Q3',],sort = True)['Q25_new'].mean().sort_index(),\n                      '2021':r_2021.groupby(['Q6_new','Q3',],sort=True)['Q25_new'].mean().sort_index()})\ndf_Q6Q3Q25_filtered=df_Q6Q3Q25[df_Q6Q3Q25.index.get_level_values(1).isin(Res_Q3_top)]\n\ndf_Q6Q3Q25_filtered.fillna(0,inplace=True)\ndf_Q6Q3Q25_filtered.index.rename('subplot',level=0,inplace=True)\n\n\ndrawplot(df_Q6Q3Q25_filtered,'subplot',3,EXPERIENCE_ORDER,\"<b>Fig 4.3.Post Pandemic era:How has the yearly CTC fluctuated over the years w.r.t Job Country and Experience?<br>Which Counts Job Title Or Experience?<b> \", '<b>AVG Yearly CTC($)<\/b>','<i><b>${:,}<\/b><\/i>')\n","10b022a7":"pltSumm(df_Q6Q3Q25_filtered,'subplot',1,EXPERIENCE_ORDER,\"Table 4.3.Which country is paying high, low, and contry that is hevyly investing on DS talent?\")","d2e23417":"df_all_Q5Q7=[]\n\ndf_all_Q5Q7.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q16.').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q18.').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q7.').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q7.').stack().value_counts(normalize=True)}))\n\ndf_all_Q5Q7.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q16.').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q18.').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q7.').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q7.').stack().value_counts(normalize=True)}))\n\n\ndf_all_Q5Q7.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q16.').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q18.').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q7.').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q7.').stack().value_counts(normalize=True)}))\n\ndf_all_Q5Q7.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q16.').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q18.').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q7.').stack().value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q7.').stack().value_counts(normalize=True)}))\ndf_all_Q5Q7.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q16.').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q18.').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q7.').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q7.').stack().value_counts(normalize=True)}))\n             \ndf_Q5Q7=pd.concat(df_all_Q5Q7,keys=EXPERIENCE_PROFILE)\ndf_Q5Q7_filtered=df_Q5Q7[df_Q5Q7.index.get_level_values(1).isin(PROGRAMMING_LANG)].fillna(0)\ndf_Q5Q7_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q5Q7_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 5.1: What are the Programming lang Used wrt Experience?<b> \", '<b> % Progg lang  (Regular)<\/b>','<b>{}%<\/b><\/i>')\n","ce1998f6":"pltSumm(df_Q5Q7_filtered,'subplot',1,EXPERIENCE_PROFILE,\"Table 5.1.Which the Programming lang Used wrt Experience high, low, and contry that is hevyly investing on DS talent?\")\n","8dad40d3":"df_all_Q5Q8=[]\n\ndf_all_Q5Q8.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q18').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q19').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q8').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q8').stack().value_counts(normalize=True)}))\n\ndf_all_Q5Q8.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q18').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q19').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q8').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q8').stack().value_counts(normalize=True)}))\n\n\ndf_all_Q5Q8.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q18').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q19').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q8').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q8').stack().value_counts(normalize=True)}))\n\ndf_all_Q5Q8.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q18').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q19').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q8').stack().value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q8').stack().value_counts(normalize=True)}))\ndf_all_Q5Q8.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q18').stack().value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q19').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q8').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q8').stack().value_counts(normalize=True)}))\n             \ndf_Q5Q8=pd.concat(df_all_Q5Q8,keys=EXPERIENCE_PROFILE)\ndf_Q5Q8_filtered=df_Q5Q8[df_Q5Q8.index.get_level_values(1).isin(PROGRAMMING_LANG)].fillna(0)\ndf_Q5Q8_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q5Q8_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>fig 5.2: What are the Programming lang Recommended In DS wrt Experience?<b> \", '<b> % Progg lang Recommended<\/b>','<b>{}%<\/b><\/i>')\n","91bb1364":"\nIDE_MAPP={'Atom':'Atom',\n 'IntelliJ': 'IntelliJ',\n 'Jupyter (JupyterLab, Jupyter Notebooks, etc)':'Jupyter',\n 'Jupyter Notebook':'Jupyter',\n 'Jupyter\/IPython':'Jupyter',\n 'MATLAB':'MATLAB',\n 'None':'None',\n 'Notepad++':'Notepad++',\n 'Other':'Other',\n 'PyCharm':'PyCharm',\n 'RStudio':'RStudio',\n 'Spyder': 'Spyder',\n 'Sublime Text': 'Sublime Text',\n 'Vim':'Vim',\n 'Vim \/ Emacs':'Vim',\n 'Visual Studio':'VSCode',\n 'Visual Studio \/ Visual Studio Code':'VSCode',\n 'Visual Studio Code':'VSCode',\n 'Visual Studio Code (VSCode)':'VSCode',\n 'nteract':'nteract'}\nIDEs=[item for _,item in IDE_MAPP.items()]","5aaaf4e8":"df_all_Q6Q9=[]\n\ndf_all_Q6Q9.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q13_P.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q16_P.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q9.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q13_P.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q16_P.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q9.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q13_P').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q16_P').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q9.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q13_P.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q16_P.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q9.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q13_P.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q16_P.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q9.').stack().str.strip().replace(IDE_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q9=pd.concat(df_all_Q6Q9,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q9_filtered=df_Q6Q9[df_Q6Q9.index.get_level_values(1).isin(IDEs)].fillna(0)\ndf_Q6Q9_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q9_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 5.3:What are the Development IDEs used In DS wrt Experience?<b> \", '<b> % Development IDEs<\/b>','<b>{}%<\/b><\/i>')","f72e3cdf":"pltSumm(df_Q6Q9_filtered,'subplot',1,EXPERIENCE_PROFILE,\"Table 5.3.Development Environments(IDE) used\")","e72c809b":"HOSTED_NOTEBOOK_MAPP={'AWS Notebook Products (EMR Notebooks, Sagemaker Notebooks, etc)':'AWS',\n 'Amazon EMR Notebooks':'AWS',\n 'Amazon Sagemaker Studio':'AWS',\n 'Amazon Sagemaker Studio Notebooks':'AWS',\n 'Azure Notebook':'Azure',\n 'Azure Notebooks':'Azure',\n 'Binder \/ JupyterHub':'JupyterHub',\n 'Code Ocean': 'CodeOcean',\n 'Colab Notebooks': 'G-Colab',\n 'Crestle': 'Crestle',\n 'Databricks Collaborative Notebooks':'Databricks',\n 'Deepnote Notebooks':'Deepnote',\n 'Domino Datalab':'Domino',\n 'FloydHub':'FloydHub',\n 'Floydhub':'FloydHub',\n 'Google Cloud AI Platform Notebooks':'GC Nbs',\n 'Google Cloud Datalab':'GC Nbs',\n 'Google Cloud Datalab Notebooks':'GC Nbs',\n 'Google Cloud Notebook Products (AI Platform, Datalab, etc)':'GC Nbs',\n 'Google Cloud Notebooks (AI Platform \/ Vertex AI)':'GC Nbs',\n 'Google Colab':'G-Colab',\n 'IBM Watson Studio':'IBMWatson',\n 'JupyterHub\/Binder':'JupyterHub',\n 'Kaggle Kernels':'kaggle',\n 'Kaggle Notebooks':'Kaggle',\n 'Kaggle Notebooks (Kernels)':'Kaggle',\n 'Microsoft Azure Notebooks':'Azure',\n 'None':'None',\n 'Observable Notebooks':'Observable',\n 'Other':'Other',\n 'Paperspace':'Paperspace',\n 'Paperspace \/ Gradient':'Paperspace',\n 'Zeppelin \/ Zepl Notebooks':'Zeppelin'}\n\nHOSTED_NOTEBOOKs=[item for _,item in HOSTED_NOTEBOOK_MAPP.items()]\n\ndf_all_Q6Q10=[]\n\ndf_all_Q6Q10.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q14_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q17_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q10.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q14_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q17_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q10.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q14_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q17_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q10.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q14_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q17_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q10.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q14_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q17_P.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q10.').stack().str.strip().replace(HOSTED_NOTEBOOK_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q10=pd.concat(df_all_Q6Q10,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q10_filtered=df_Q6Q10[df_Q6Q10.index.get_level_values(1).isin(HOSTED_NOTEBOOKs)].fillna(0)\ndf_Q6Q10_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q10_filtered*100,'subplot',5,EXPERIENCE_PROFILE,\"<b>Fig 5.4 a) What are the Hosted Notebooks used In DS wrt Experience?<b> \", '<b> % Share of Hosted Notebooks<\/b>','<b>{}%<\/b><\/i>')","526183bc":"df_Q6Q10PCT=df_Q6Q10_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0).sort_values(by='2021', ascending=False)\ndf_Q6Q10PCT=df_Q6Q10PCT[df_Q6Q10PCT>0].fillna(0)\ndrawplot(df_Q6Q10PCT,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 5.4 b) Percentage change in the usage of hosted notebooks over the years In DS wrt Experience?<b> \", '<b> % Share of Hosted Notebooks<\/b>','<b>{}%<\/b><\/i>')\n","5ce98eb8":"COMP_PLAT_MAPP={'A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc)':'Cloud',\n 'A deep learning workstation (NVIDIA GTX, LambdaLabs, etc)':'DL Workstation',\n 'A laptop':'PC\/Laptop',\n 'A personal computer \/ desktop':'PC\/Laptop',\n 'A personal computer or laptop':'PC\/Laptop',\n 'Alibaba Cloud':'Cloud',\n 'Amazon Web Services (AWS)':'Cloud',\n 'Google Cloud Platform (GCP)':'Cloud',\n 'IBM Cloud':'Cloud',\n 'Microsoft Azure':'Cloud',\n 'None':'None',\n 'Oracle Cloud':'Cloud',\n 'Other':'Other',\n 'Red Hat Cloud':'Cloud',\n 'SAP Cloud':'Cloud',\n 'Salesforce Cloud':'Cloud',\n 'VMware Cloud':'Cloud'}\n\nCOMP_PLATs=[item for _,item in COMP_PLAT_MAPP.items()]\n\ndf_all_Q6Q11=[]\n\ndf_all_Q6Q11.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q29_P.').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q11.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q29_P.').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q11.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q29_P.').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q11.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q29_P.').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q11.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q29_P.').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q11').stack().str.strip().replace(COMP_PLAT_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q11=pd.concat(df_all_Q6Q11,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q11_filtered=df_Q6Q11[df_Q6Q11.index.get_level_values(1).isin(COMP_PLATs)].fillna(0)\ndf_Q6Q11_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q11_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 5.5 What are the Computing Platforms used In DS wrt Experience?<b> \", '<b> % Share of Computing Platforms<\/b>','<b>{}%<\/b><\/i>')","774039ca":"SPECIAL_HARWARE_MAPP={'AWS Inferentia Chips':'AWS Chips',\n 'AWS Trainium Chips':'AWS Chips',\n 'Google Cloud TPUs':'TPUs',\n 'NVIDIA GPUs':'GPUs',\n 'GPUs':'GPUs',\n 'CPUs':'CPUs',\n 'None':'None',\n  'None \/ I do not know':'None',\n 'Other':'Other',\n 'TPUs':'TPUs'}\n\n\nSPECIAL_HARWAREs=[item for _,item in SPECIAL_HARWARE_MAPP.items()]\n\ndf_all_Q6Q12=[]\n\ndf_all_Q6Q12.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q21_P.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q12.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q12.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q12.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q21_P.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q12.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q12.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q12.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q21_P.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q12_P.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q12.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q12.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q21_P.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q12_P.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q12_P.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q12.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q21.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q12.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q12.').stack().str.strip().replace(SPECIAL_HARWARE_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q12=pd.concat(df_all_Q6Q12,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q12_filtered=df_Q6Q12[df_Q6Q12.index.get_level_values(1).isin(SPECIAL_HARWAREs)].fillna(0)\ndf_Q6Q12_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q12_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 5.6 a) What are the Special hardware used In DS wrt Experience?<b> \", '<b> % Share of Specialized Hardwares<\/b>','<b>{}%<\/b><\/i>')\n","9407bfe6":"df_all_Q6Q13=[]\n\ndf_all_Q6Q13.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q22').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q13').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q13').stack().value_counts(normalize=True)}))\n\ndf_all_Q6Q13.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q22').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q13').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q13').stack().value_counts(normalize=True)}))\n\n\ndf_all_Q6Q13.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q22').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q13').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q13').stack().value_counts(normalize=True)}))\n\ndf_all_Q6Q13.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q22').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q13').stack().value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q13').stack().value_counts(normalize=True)}))\ndf_all_Q6Q13.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q22').stack().value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q13').stack().value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q13').stack().value_counts(normalize=True)}))\n             \ndf_Q6Q13=pd.concat(df_all_Q6Q13,keys=EXPERIENCE_PROFILE)\ndf_Q6Q13.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q13*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 5.6 b) How many times haev the kagglers used TPUs?<b> \", '<b> % Share of Specialized Hardwares<\/b>','<b>{}%<\/b><\/i>')\n","2aefcf31":"DATA_VIZ_MAPP={'Altair':'Altar',\n 'Bokeh':'Bokeh',\n 'D3':'D3.js',\n 'D3 js':'D3.js',\n 'D3.js':'D3.js',\n 'Geoplotlib':'Geoplotlib',\n 'Ggplot \/ ggplot2':'Ggplot',\n 'Lattice': 'Lattice',\n 'Leaflet': 'Leaflet',\n 'Leaflet \/ Folium': 'Leaflet',\n 'Matplotlib': 'Matplotlib',\n 'None':'None',\n 'Other':'Other',\n 'Plotly': 'Plotly',\n 'Plotly \/ Plotly Express': 'Plotly',\n 'Seaborn': 'Seaborn',\n 'Shiny': 'Shiny',\n 'ggplot2':'Ggplot'}\n\nDATA_VIZ_libs=[item for _,item in DATA_VIZ_MAPP.items()]\n\ndf_all_Q6Q14=[]\n\ndf_all_Q6Q14.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q21_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q20_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q14.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q21_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q20_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q14.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q21_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q20_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q14.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q21_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q20_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q14.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q21_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q20_P.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q14.').stack().str.strip().replace(DATA_VIZ_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q14=pd.concat(df_all_Q6Q14,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q14_filtered=df_Q6Q14[df_Q6Q14.index.get_level_values(1).isin(DATA_VIZ_libs)].fillna(0)\ndf_Q6Q14_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q14_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 5.7 a) Which Data Viz Libs\/Tool used In DS wrt Experience?<b> \", '<b> % Share of Data Viz Lib\/Tools<\/b>','<b>{}%<\/b><\/i>')","29b811c7":"df_Q6Q14PCT=df_Q6Q14_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0).sort_values(by='2021', ascending=False)\ndf_Q6Q14PCT=df_Q6Q14PCT[df_Q6Q14PCT>0].fillna(0)\ndrawplot(df_Q6Q14PCT,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 5.7 b) What data Vizlib gaining popularity among data scientists?<b> \", '<b> % Share of data Viz lib<\/b>','<b>{}%<\/b><\/i>')\n","409a058a":"TIMEPERIOD_MLFRAME_MAPP={'1-2 years':'1-2 Y',\n '10-15 years':'10-20 Y',\n '10-20 years':'10-20 Y',\n '2-3 years':'2-3 Y',\n '20 or more years':'20+ Y',\n '20+ years':'20+ Y',\n '3-4 years':'3-4 Y',\n '4-5 years':'4-5 Y',\n '5-10 years':'5-10 Y',\n '< 1 year':'<1 Y',\n '< 1 years':'<1 Y',\n 'I do not use machine learning methods':'None',\n 'I have never studied machine learning and I do not plan to':'None',\n 'I have never studied machine learning but plan to learn in the future':'None',\n 'Under 1 year':'<1 Y'\n }\n\nTIMEPERIOD_MLFRAME=[item for _,item in TIMEPERIOD_MLFRAME_MAPP.items()]\n\ndf_all_Q6Q15=[]\n\ndf_all_Q6Q15.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q25').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q23').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q15.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q25').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q23').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q15.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q25').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q23').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q15.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q25').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q23').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q15.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q25').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q23').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q15').stack().str.strip().replace(TIMEPERIOD_MLFRAME_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q15=pd.concat(df_all_Q6Q15,keys=EXPERIENCE_PROFILE)\ndf_Q6Q15_filtered=df_Q6Q15[df_Q6Q15.index.get_level_values(1).isin(TIMEPERIOD_MLFRAME)].fillna(0)\n\ndf_Q6Q15_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q15_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 5.8 a)For how long the kagglers have been usingML methods wrt Experience?<b> \", '<b> % OF Kagglers using ML Methods in work<\/b>','<b>{}%<\/b><\/i>')","f2331dd5":"ML_FRAMEWORKS_MAPP={'CNTK':'CNTK',\n 'Caffe':'Caffe',\n 'Caret':'Caret',\n 'CatBoost': 'CatBoost',\n 'Fast.ai': 'Fastai',\n 'Fastai': 'Fastai',\n 'H20':'H20',\n 'H2O 3':'H20',\n 'Huggingface': 'Hface',\n 'JAX':'JAX',\n 'Keras':'Keras',\n 'LightGBM':'LightGBM',\n 'MXNet':'Mxnet',\n 'Mxnet':'Mxnet',\n 'None': 'None',\n 'Other' :'Other',\n 'Prophet':'Prophet',\n 'PyTorch':'Pytorch',\n 'PyTorch Lightning':'Pytorch',\n 'RandomForest':'RandomFor',\n 'Scikit-Learn':'SKlearn',\n 'Scikit-learn':'SKlearn',\n 'Spark MLib': 'SparkMLib',\n 'Spark MLlib':'SparkMLib',\n 'TensorFlow':'TF',\n 'Tidymodels':'Tidymodels',\n 'Xgboost':'Xgboost',\n 'catboost':'CatBoost',\n 'lightgbm':'LightGBM',\n 'mlr':'mlr',\n 'randomForest':'RandomFor'}\n\nML_FRAMEWORKS=[item for _,item in ML_FRAMEWORKS_MAPP.items()]\n\ndf_all_Q6Q16=[]\n\ndf_all_Q6Q16.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q19_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q28_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q16.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q19_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q28_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q16.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q19_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q28_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q16.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q19_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q28_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q16.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q19_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q28_P.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q16.').stack().str.strip().replace(ML_FRAMEWORKS_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q16=pd.concat(df_all_Q6Q16,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q16_filtered=df_Q6Q16[df_Q6Q16.index.get_level_values(1).isin(ML_FRAMEWORKS)].fillna(0)\ndf_Q6Q16_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q16_filtered*100,'subplot',5,EXPERIENCE_PROFILE,\"<b>Fig 5.8 b) Which ML Frameworks are mostly used In DS by kagglers wrt Experience?<b> \", '<b> % Share of ML Frameworks<\/b>','<b>{}%<\/b><\/i>')","7f6534ed":"ML_ALG_MAPP={'Bayesian Approaches':'Bayesian',\n 'Convolutional Neural Networks':'CNN',\n 'Decision Trees or Random Forests':'DT\/RF',\n 'Dense Neural Networks (MLPs, etc)':'DNN',\n 'Evolutionary Approaches':\"EA\",\n 'Generative Adversarial Networks':'GAN',\n 'Gradient Boosting Machines (xgboost, lightgbm, etc)':\"GBM\",\n 'Linear or Logistic Regression':\"LRs\",\n 'None':\"None\",\n 'Other':\"Other\",\n 'Recurrent Neural Networks':\"RNN\",\n 'Transformer Networks (BERT, gpt-2, etc)':\"TN\",\n 'Transformer Networks (BERT, gpt-3, etc)':\"TN\"}\n\nML_ALG=[item for _,item in ML_ALG_MAPP.items()]\n\ndf_all_Q6Q17=[]\n\ndf_all_Q6Q17.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q24.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q17.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q24.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q17.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q24.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q17.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q24.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q17.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q24.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q17.').stack().str.strip().replace(ML_ALG_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q17=pd.concat(df_all_Q6Q17,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q17_filtered=df_Q6Q17[df_Q6Q17.index.get_level_values(1).isin(ML_ALG)].fillna(0)\ndf_Q6Q17_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q17_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 5.9)Which ML Algorithms are used regularly In DS by kagglers wrt Experience?<b> \", '<b> % Share of ML Algorithms<\/b>','<b>{}%<\/b><\/i>')\n","bccaa90c":"CV_METHODS_MAPP={'General purpose image\/video tools (PIL, cv2, skimage, etc)': 'GPI\/V',\n 'Generative Networks (GAN, VAE, etc)':'GN',\n 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)':'IC&GPN',\n 'Image segmentation methods (U-Net, Mask R-CNN, etc)':'ImgSegment',\n 'None':'None',\n 'Object detection methods (YOLOv3, RetinaNet, etc)':'ObjDetect',\n 'Other':'Other'}\n\nCV_METHODS=[item for _,item in CV_METHODS_MAPP.items()]\n\ndf_all_Q6Q18=[]\n\ndf_all_Q6Q18.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q26_P.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q18.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q26_P.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q18.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q26_P.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q18.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q26_P.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q18.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q26_P.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q18.').stack().str.strip().replace(CV_METHODS_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q18=pd.concat(df_all_Q6Q18,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q18_filtered=df_Q6Q18[df_Q6Q18.index.get_level_values(1).isin(CV_METHODS)].fillna(0)\ndf_Q6Q18_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q18_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 5.10 Which Computer Vision(CV) Methods are used regularly In DS by kagglers wrt Experience?<b> \", '<b> % Share of CV Methods<\/b>','<b>{}%<\/b><\/i>')","be5817dc":"NLP_METHODS_MAPP={'Contextualized embeddings (ELMo, CoVe)':'CE',\n 'Encoder-decorder models (seq2seq, vanilla transformers)':'E-D',\n 'None':'None',\n 'Other':'Other',\n 'Transformer language models (GPT-3, BERT, XLnet, etc)':'TrFLModels',\n 'Transformer language models (GPT-2, BERT, XLnet, etc)':'TrFLModels',\n 'Word embeddings\/vectors (GLoVe, fastText, word2vec)':'WE\/Vectors'}\n\nNLP_METHODS=[item for _,item in NLP_METHODS_MAPP.items()]\n\ndf_all_Q6Q19=[]\n\ndf_all_Q6Q19.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q27_P.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q19.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q27_P.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q19.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q27_P.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q19.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q27_P.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q19.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q27_P.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q19.').stack().str.strip().replace(NLP_METHODS_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q19=pd.concat(df_all_Q6Q19,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q19_filtered=df_Q6Q19[df_Q6Q19.index.get_level_values(1).isin(NLP_METHODS)].fillna(0)\ndf_Q6Q19_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q19_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 5.11 Which NLP Methods are used regularly by kagglers wrt Experience?<b> \", '<b> % Share of NLP Methods<\/b>','<b>{}%<\/b><\/i>')","e848b30c":"df_all_Q6Q20=[]\n\ndf_all_Q6Q20.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020':0,\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q20').stack().value_counts(normalize=True)}))\n\ndf_all_Q6Q20.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018':0,        \n                    '2019':0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q20').stack().value_counts(normalize=True)}))\n\n\ndf_all_Q6Q20.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q20').stack().value_counts(normalize=True)}))\n\ndf_all_Q6Q20.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q20').stack().value_counts(normalize=True)}))\ndf_all_Q6Q20.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q20').stack().value_counts(normalize=True)}))\n             \ndf_Q6Q20=pd.concat(df_all_Q6Q20,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q20_filtered=df_Q6Q20.fillna(0)\ndf_Q6Q20_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q20_filtered*100,'subplot',5,EXPERIENCE_PROFILE,\"<b>Fig 6.1 Where are the kagglers working wrt Experience?, in which industry ?<b> \", '<b> % Share of Industry by Kagglers<\/b>','<b>{}%<\/b><\/i>')","a72de50c":"df_all_Q6Q21=[]\n\ndf_all_Q6Q21.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q6'].value_counts(normalize=True) ,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q20'].value_counts(normalize=True) ,\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q21'].value_counts(normalize=True) }))\n\ndf_all_Q6Q21.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q6'].value_counts(normalize=True) ,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q20'].value_counts(normalize=True) ,\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q21'].value_counts(normalize=True)}))\n\n\ndf_all_Q6Q21.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q6'].value_counts(normalize=True) ,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q20'].value_counts(normalize=True) ,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q21'].value_counts(normalize=True)}))\n\ndf_all_Q6Q21.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019':r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q6'].value_counts(normalize=True) ,\n                    '2020':r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q20'].value_counts(normalize=True) ,\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q21'].value_counts(normalize=True)}))\ndf_all_Q6Q21.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q6'].value_counts(normalize=True) ,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q20'].value_counts(normalize=True) ,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q21'].value_counts(normalize=True)}))\n             \ndf_Q6Q21=pd.concat(df_all_Q6Q21,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q21_filtered=df_Q6Q21.fillna(0)\ndf_Q6Q21_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q21_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 6.2 What is the size of the company in which kagglers are employed w.r.t Experience??<b> \", '<b> % Share of a typical Kaggler Company Size <\/b>','<b>{}%<\/b><\/i>')","b0d6ce4e":"df_all_Q6Q22=[]\n\ndf_all_Q6Q22.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q7'].value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q21'].value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q22'].value_counts(normalize=True)}))\n\ndf_all_Q6Q22.append(pd.DataFrame({#'Profile':'Senior',\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q7'].value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q21'].value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q22'].value_counts(normalize=True)}))\n\n\ndf_all_Q6Q22.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q7'].value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q21'].value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q22'].value_counts(normalize=True)}))\n\ndf_all_Q6Q22.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019':r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q7'].value_counts(normalize=True),\n                    '2020':r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q21'].value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q22'].value_counts(normalize=True)}))\ndf_all_Q6Q22.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q7'].value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q21'].value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q22'].value_counts(normalize=True)}))\n             \ndf_Q6Q22=pd.concat(df_all_Q6Q22,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q22_filtered=df_Q6Q22.fillna(0)\ndf_Q6Q22_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q22_filtered*100,'subplot',2,EXPERIENCE_PROFILE,\"<b>Fig 6.3 How may peers are resonsible for DS workloads in company kagglers are employed w.r.t Experience??<b> \", '<b> % Share of No of Emplyees in DS workload <\/b>','<b>{}%<\/b><\/i>')","45726a83":"IF_ML_METHODS_USED_MAPP={'I do not know':\"No Info\",\n 'No (we do not use ML methods)':'No',\n 'We are exploring ML methods (and may one day put a model into production)':'Exploring',\n 'We have well established ML methods (i.e., models in production for more than 2 years)':'Well Est',\n 'We recently started using ML methods (i.e., models in production for less than 2 years)':'Started',\n 'We use ML methods for generating insights (but do not put working models into production)':'Use ML'}\n\nIF_ML_METHODS_USED=[item for _,item in IF_ML_METHODS_USED_MAPP.items()]\n\ndf_all_Q6Q23=[]\n\ndf_all_Q6Q23.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q10'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q8'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q22'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q23'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q23.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q10'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q8'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q22'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q23'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q23.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q10'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),  \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q8'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q22'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q23'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q23.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q10'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q8'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q22'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q23'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q23.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q10'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q8'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q22'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q23'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q23=pd.concat(df_all_Q6Q23,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q23_filtered=df_Q6Q23[df_Q6Q23.index.get_level_values(1).isin(IF_ML_METHODS_USED)].fillna(0)\ndf_Q6Q23_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q23_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 6.4.Are ML Methods incorporated in the workplace of Kagglers?<b> \", '<b> % Share of Whether ML incorporated<\/b>','<b>{}%<\/b><\/i>')","5a10d892":"ACTIVITIES_WORKROLE_MAPP={'Analyze and understand data to influence product or business decisions':'Analyze',\n 'Build and\/or run a machine learning service that operationally improves my product or workflows':'Build ML Service',\n 'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data':'Build data Infra',\n 'Build prototypes to explore applying machine learning to new areas':'Build Proto',\n 'Do research that advances the state of the art of machine learning':'Do research',\n 'Experimentation and iteration to improve existing ML models':'Improve ML',\n 'None of these activities are an important part of my role at work':'None',\n 'Other':'Other'}\n\nACTIVITIES_WORKROLE=[item for _,item in ACTIVITIES_WORKROLE_MAPP.items()]\n\ndf_all_Q6Q24=[]\n\ndf_all_Q6Q24.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q11_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q9_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q23.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q24.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q24.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q11_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q9_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q23.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q24.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q24.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q11_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),  \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q9_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q23.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q24.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q24.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q11_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q9_P').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q23.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q24.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q24.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q11_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q9_P.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q23.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q24.').stack().str.strip().replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q24=pd.concat(df_all_Q6Q24,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q24_filtered=df_Q6Q24[df_Q6Q24.index.get_level_values(1).isin(ACTIVITIES_WORKROLE)].fillna(0)\ndf_Q6Q24_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q24_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.1. What Activivities do Kagglers undertake at their respective workplace?<b> \", '<b> % Share of Workplace activities<\/b>','<b>{}%<\/b><\/i>')","76d019e0":"MONEYSPENT_ML_Cloud_MAPP={'$0 ($USD)':'$0',\n '$1-$99':'$1-$99',\n '$10,000-$99,999':'$10,000-$99,999',\n '$100,000 or more ($USD)':'$100,000+',\n '$100-$999':'$100-$999',\n '$1000-$9,999':'$1000-$9,999',\n 112500:'$100,000+',\n 5000:'$1000-$9,999',\n 50000:'$10,000-$99,999'}\n\nMONEYSPENT_ML_Cloud=[item for _,item in MONEYSPENT_ML_Cloud_MAPP.items()]\n\ndf_all_Q6Q26=[]\n\ndf_all_Q6Q26.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q11'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q25'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q26'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q26.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q11'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q25'].replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q26'].replace(ACTIVITIES_WORKROLE_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q26.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q11'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q25'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q26'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q26.append(pd.DataFrame({\n                    '2018':0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q11'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q25'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q26'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q26.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q11'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q25'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q26'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q26=pd.concat(df_all_Q6Q26,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q26_filtered=df_Q6Q26[df_Q6Q26.index.get_level_values(1).isin(MONEYSPENT_ML_Cloud)].fillna(0)\ndf_Q6Q26_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q26_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 7.2.How much money do Kagglers or their team at work spend on ML &\/ Cloud computing servicess?<b> \", '<b> % Share of Money Spent on ML \/Cloud services<\/b>','<b>{}%<\/b><\/i>')","8b4a3c1e":"CLOUDCOMP_PLAT_MAPP={'Alibaba Cloud':'Alibaba',\n 'Amazon Web Services (AWS)':'AWS',\n 'Google Cloud Platform (GCP)':'GCP',\n 'I have not used any cloud providers':'Not Used Cloud',\n 'IBM Cloud':'IBM',\n 'IBM Cloud \/ Red Hat':'IBM',\n 'Microsoft Azure':'MSFT Azure',\n 'None':'None',\n 'Oracle Cloud':'Oracle',\n 'Other':'Other',\n 'Red Hat Cloud':'IBM',\n 'SAP Cloud':'SAP',\n 'Salesforce Cloud':'Salesforce',\n 'Tencent Cloud':'Tencent',\n 'VMware Cloud':'VMware'}\n\nCLOUDCOMP_PLATs=[item for _,item in CLOUDCOMP_PLAT_MAPP.items()]\n\ndf_all_Q6Q27A=[]\n\ndf_all_Q6Q27A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q15_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q29_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q26_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q27A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q15_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q29_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q26_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q27A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q15_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),  \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q29_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q26_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q27A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q15_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q29_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q26_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q27A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q15_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q29_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q26_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q27A=pd.concat(df_all_Q6Q27A,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q27A_filtered=df_Q6Q27A[df_Q6Q27A.index.get_level_values(1).isin(CLOUDCOMP_PLATs)].fillna(0)\ndf_Q6Q27A_filtered.index.rename('subplot',level=0,inplace=True)\n# display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q27A_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.3 .Which Cloud computing Platform have the kagglers been using?<b> \", '<b> % Share of Cloud Computing Platforms<\/b>','<b>{}%<\/b><\/i>')","309f47f2":"df_all_Q6Q28=[]\n\ndf_all_Q6Q28.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q28').stack().value_counts(normalize=True)}))\n\ndf_all_Q6Q28.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q28').stack().value_counts(normalize=True)}))\n\n\ndf_all_Q6Q28.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q28').stack().value_counts(normalize=True)}))\n\ndf_all_Q6Q28.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q28').stack().value_counts(normalize=True)}))\ndf_all_Q6Q28.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q28').stack().value_counts(normalize=True)}))\n             \ndf_Q6Q28=pd.concat(df_all_Q6Q28,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q28_filtered=df_Q6Q28.fillna(0)\ndf_Q6Q28_filtered.index.rename('subplot',level=0,inplace=True)\n# display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q28_filtered*100,'subplot',2,EXPERIENCE_PROFILE,\"<b>Fig 7.4.Which cloud platform have the best developer experience?<b> \", '<b> % Share of Cloud Platforms<\/b>','<b>{}%<\/b><\/i>')","e376e10e":"CLOUDCompPlatRegular_MAPP={'AWS Batch':'AWS (EC2)',\n 'AWS Elastic Beanstalk':'AWS (EC2)',\n 'AWS Elastic Compute Cloud (EC2)':'AWS (EC2)',\n 'AWS Lambda':'AWS (EC2)',\n 'Amazon EC2':'AWS (EC2)',\n 'Amazon Elastic Compute Cloud (EC2)':'AWS (EC2)',\n 'Amazon Elastic Container Service':'AWS (EC2)',\n 'Azure Batch':'MSFT Azure VMs',\n 'Azure Cloud Services':'MSFT Azure VMs',\n 'Azure Container Service':'MSFT Azure VMs',\n 'Azure Event Grid':'MSFT Azure VMs',\n 'Azure Functions':'MSFT Azure VMs',\n 'Azure Kubernetes Service':'MSFT Azure VMs',\n 'Azure Virtual Machines':'MSFT Azure VMs',\n 'Google App Engine':'GC Compute E',\n 'Google Cloud App Engine':'GC Compute E',\n 'Google Cloud Compute Engine':'GC Compute E',\n 'Google Cloud Functions':'GC Compute E',\n 'Google Cloud Run':'GC Compute E',\n 'Google Compute Engine':'GC Compute E',\n 'Google Compute Engine (GCE)':'GC Compute E',\n 'Google Kubernetes Engine':'GC Compute E',\n 'IBM Cloud Container Registry' :'Other',\n 'IBM Cloud Foundry' :'Other',\n 'IBM Cloud Kubernetes Service': 'Other',\n 'IBM Cloud Virtual Servers': 'Other',\n 'Microsoft Azure Container Instances':'MSFT Azure VMs',\n 'Microsoft Azure Virtual Machines':'MSFT Azure VMs',\n 'No \/ None':'None',\n 'None':'None',\n 'Other':'Other'}\n\n\nCLOUDCompPlatRegular=[item for _,item in CLOUDCompPlatRegular_MAPP.items()]\n\n\ndf_all_Q6Q29A=[]\n\ndf_all_Q6Q29A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q27_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q30_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q29_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q29A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q27_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q30_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q29_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q29A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q27_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),  \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q30_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q29_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q29A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q27_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q30_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q29_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q29A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q27_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q30_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q29_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q29A=pd.concat(df_all_Q6Q29A,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q29A_filtered=df_Q6Q29A[df_Q6Q29A.index.get_level_values(1).isin(CLOUDCompPlatRegular)].fillna(0)\ndf_Q6Q29A_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q29A_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>fig 7.5.a) Which Cloud computing Products have the kagglers been using?<b> \", '<b> % Share of Cloud Computing Products<\/b>','<b>{}%<\/b><\/i>')","ba7e0fd4":"df_Q6Q29APCT=df_Q6Q29A_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0).sort_values(by='2021', ascending=False)\ndf_Q6Q29APCT=df_Q6Q29APCT[df_Q6Q29APCT>0].fillna(0)\ndrawplot(df_Q6Q29APCT,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.5 b) what is the percentage change in Cloud computing products usage?<b> \", '<b> % Change in Usage<\/b>','<b>{}%<\/b><\/i>')","7b08f950":"\ndf_all_Q6Q30A=[]\n\ndf_all_Q6Q30A.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q30_A.').stack().value_counts(normalize=True) }))\n\ndf_all_Q6Q30A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q30_A.').stack().value_counts(normalize=True) }))\n\n\ndf_all_Q6Q30A.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q30_A.').stack().value_counts(normalize=True)}))\n\ndf_all_Q6Q30A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q30_A.').stack().value_counts(normalize=True)}))\ndf_all_Q6Q30A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q30_A.').stack().value_counts(normalize=True)}))\n             \ndf_Q6Q30A=pd.concat(df_all_Q6Q30A,keys=EXPERIENCE_PROFILE)\n\ndf_Q6Q30A_filtered=df_Q6Q30A.fillna(0)\ndf_Q6Q30A_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q6Q30A_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 7.6 Which Data Storage Products have the kagglers been using regularly?<b> \", '<b> % Share of Data storage Products Products<\/b>','<b>{}%<\/b><\/i>')","475f67dd":"MANAGED_ML_PROP_MAPP={'Algorithmia':'Other',\n 'Alteryx':'Alteryx',\n 'Amazon Comprehend':'AmzSM',\n 'Amazon Forecast':'AmzSM',\n 'Amazon Lex':'AmzSM',\n 'Amazon Rekognition':'AmzSM',\n 'Amazon Rekognition Video':'AmzSM',\n 'Amazon SageMaker':'AmzSM',\n 'Amazon Transcribe':'AmzSM',\n 'Amazon Translate':'AmzSM',\n 'Azure Bing Speech API': 'AzureMLS',\n 'Azure Cognitive Services': 'AzureMLS',\n 'Azure Computer Vision API':'AzureMLS',\n 'Azure Cortana Intelligence Suite':  'AzureMLS',\n 'Azure Face API': 'AzureMLS',\n 'Azure Machine Learning Studio':'AzureMLS',\n 'Azure Machine Learning Workbench': 'AzureMLS',\n 'Azure Speaker Recognition API': 'AzureMLS',\n 'Azure Video API': 'AzureMLS',\n 'Cloudera':'Other',\n 'DataRobot': 'DataRobot',\n 'Databricks':'Databricks',\n 'Dataiku':'Dataiku',\n 'Dataversity':'Other',\n 'Domino Datalab':'Other',\n 'Google Cloud AI Platform \/ Google Cloud ML Engine':'GCVertex',\n 'Google Cloud AutoML':'GCVertex',\n 'Google Cloud Machine Learning Engine':'GCVertex',\n 'Google Cloud Natural Language':'GCVertex',\n 'Google Cloud Natural Language API':'GCVertex',\n 'Google Cloud Speech-to-Text':'GCVertex',\n 'Google Cloud Speech-to-text API':'GCVertex',\n 'Google Cloud Translation':'GCVertex',\n 'Google Cloud Translation API':'GCVertex',\n 'Google Cloud Vertex AI':'GCVertex',\n 'Google Cloud Video AI':'GCVertex',\n 'Google Cloud Video Intelligence API':'GCVertex',\n 'Google Cloud Vision':'GCVertex',\n 'Google Cloud Vision AI':'GCVertex',\n 'Google Cloud Vision API':'GCVertex',\n 'Google Dialogflow Enterprise Edition':'GCVertex',\n 'H20 Driverless AI':'Other',\n 'IBM Watson Assistant':'Other',\n 'IBM Watson Discovery':'Other',\n 'IBM Watson Knowledge Catalog':'Other',\n 'IBM Watson Machine Learning':'Other',\n 'IBM Watson Studio':'Other',\n 'IBM Watson Text to Speech':'Other',\n 'IBM Watson Visual Recognition':'Other',\n 'Instabase':'Other',\n 'No \/ None':'None',\n 'None':'None',\n  'Other':'Other',\n 'RapidMiner':'Rapidminer',\n 'Rapidminer':'Rapidminer',\n 'SAS':'Other'}\n\n\n\n\n\nMANAGED_ML_PROPs=[item for _,item in MANAGED_ML_PROP_MAPP.items()]\n\n\ndf_all_Q6Q31A=[]\n\ndf_all_Q6Q31A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q28_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q32_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q28_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q31_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q31A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q28_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q32_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q28_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q31_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q31A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q28_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),  \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q32_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q28_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q31_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q31A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q28_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q32_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q28_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q31_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q31A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q28_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q32_P.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q28_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q31_A.').stack().str.strip().replace(MANAGED_ML_PROP_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q31A=pd.concat(df_all_Q6Q31A,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q31A_filtered=df_Q6Q31A[df_Q6Q31A.index.get_level_values(1).isin(MANAGED_ML_PROPs)].fillna(0)\ndf_Q6Q31A_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q31A_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.7 Which Managed ML Products have the kagglers been using?<b> \", '<b> % Share of ML Products<\/b>','<b>{}%<\/b><\/i>')","0cb4f22f":"df_Q6Q31APCT=df_Q6Q31A_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0).sort_values(by='2021', ascending=False)\ndf_Q6Q31APCT=df_Q6Q31APCT[df_Q6Q31APCT>0].fillna(0)\ndrawplot(df_Q6Q31APCT,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.7 b) what is the percentage change in Managed ML products usage?<b> \", '<b> % Change in Usage<\/b>','<b>{}%<\/b><\/i>')","79eb72db":"BIG_DATA_PROD_MAPP={'AWS Athena':'AWS Athena',\n 'AWS Batch':'AWS Batch',\n 'AWS Elastic MapReduce':'AWS ElasticMapR',\n 'AWS Kinesis':'AWS Kinesis',\n 'AWS Redshift':'AWS Redshift',\n 'Amazon Athena':'Amz Athena',\n 'Amazon Aurora':'Amz Aurora',\n 'Amazon DynamoDB':'Amz dynamoDB',\n 'Amazon RDS':'Amz RDS',\n 'Amazon Redshift':'Amz RedShift',\n 'Azure HDInsight':'Az HDI',\n 'Azure SQL Data Warehouse':'Az SQL DataW',\n 'Azure Stream Analytics':'Az Stream A',\n 'Databricks':'Databricks',\n 'Google BigQuery':'G BQ',\n 'Google Cloud BigQuery':'GC BQ',\n 'Google Cloud BigTable':'GC BT',\n 'Google Cloud Dataflow':'GC Dataflow',\n 'Google Cloud Dataprep':'GC Dataprep',\n 'Google Cloud Dataproc':'GC Dataproc',\n 'Google Cloud Firestore':'GC Fireb',\n 'Google Cloud Pub\/Sub':'GC Pub\/Sub',\n 'Google Cloud SQL':'GC SQL',\n 'Google Cloud Spanner':'GC Spanner',\n 'IBM Cloud Analytics Engine':'IBM CAE',\n 'IBM Cloud Streaming Analytics':'IBM SA',\n 'IBM Db2':'IBM Db2',\n 'IBM InfoSphere DataStorage':'IBM InfoSDS',\n 'Microsoft Access':'MSFTAcc',\n 'Microsoft Analysis Services':'MSFT AS',\n 'Microsoft Azure Cosmos DB':'MSFT AzCDB',\n 'Microsoft Azure Data Lake Storage':'MSFT AzDLS',\n 'Microsoft Azure SQL Database':'MSFT AzSQL',\n 'Microsoft SQL Server':'MSSQL',\n 'MongoDB':'MongoDB',\n 'MySQL':'MySQL',\n 'None':'None',\n 'Oracle Database':'OracleDB',\n 'Oracle Exadata':'Oracle Exadata',\n 'Oracle Warehouse Builder':'Oracle Wareh',\n 'Other':'Other',\n 'PostgreSQL':'PostgresSQl',\n 'PostgresSQL':'PostgresSQl',\n 'SAP IQ':'SAP IQ',\n 'SQLite':'SQLite',\n 'Snowflake':'Snowflake',\n 'Teradata':'Teradata'}\n\n\nBIG_DATA_PROD=[item for _,item in BIG_DATA_PROD_MAPP.items()]\n\n\ndf_all_Q6Q32A=[]\n\ndf_all_Q6Q32A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q30_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q31_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q29_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q32_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q32A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q30_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q31_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q29_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q32_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q32A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q30_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),  \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q31_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q29_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q32_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q32A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q30_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q31_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q29_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q32_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q32A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q30_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q31_P.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q29_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q32_A.').stack().str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q32A=pd.concat(df_all_Q6Q32A,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q32A_filtered=df_Q6Q32A[df_Q6Q32A.index.get_level_values(1).isin(BIG_DATA_PROD)].fillna(0)\ndf_Q6Q32A_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q32A_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.8 a)Which Big Data Products are regularly used by kagglers?<b> \", '<b> % Share of BigData Products<\/b>','<b>{}%<\/b><\/i>')","2411384c":"\ndf_all_Q6Q33=[]\n\ndf_all_Q6Q33.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q30'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q33'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q33.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q30'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q33'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q33.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019':0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q30'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q33'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q33.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q30'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q33'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q33.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019':0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q30'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q33'].str.strip().replace(BIG_DATA_PROD_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q33=pd.concat(df_all_Q6Q33,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q33_filtered=df_Q6Q33[df_Q6Q33.index.get_level_values(1).isin(BIG_DATA_PROD)].fillna(0)\ndf_Q6Q33_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q33_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.8 b)Which Big Data Products are most Oftenly used by kagglers?<b> \", '<b> % Share of BigData Products<\/b>','<b>{}%<\/b><\/i>')","bdcdc27c":"BITOOLS_MAPP={'Alteryx':'Alteryx',\n 'Amazon QuickSight':'Amz QuickS',\n 'Domo':'Domo',\n 'Einstein Analytics':'Einstein Analytics',\n 'Google Data Studio':'G DataStudio',\n 'Looker':'Looker',\n 'Microsoft Azure Synapse':'MSFT Az Synapse',\n 'Microsoft Power BI':'MSPBI',\n 'None':'None',\n 'Other':'Other',\n 'Qlik':'Qlik',\n 'SAP Analytics Cloud':'SAP',\n 'Salesforce':'Salesforce',\n 'Sisense':'Sisense',\n 'TIBCO Spotfire':'TIBCO',\n 'Tableau':'Tableau',\n 'Tableau CRM':'Tableau CRM',\n 'Thoughtspot':'Thoughtspot'}\n\n\nBITools=[item for _,item in BITOOLS_MAPP.items()]\n\ndf_all_Q6Q34A=[]\n\ndf_all_Q6Q34A.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q31_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q34_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q34A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q31_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q34_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q34A.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019':0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q31_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q34_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q34A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q31_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q34_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q34A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019':0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q31_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q34_A.').stack().str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q34A=pd.concat(df_all_Q6Q34A,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q34A_filtered=df_Q6Q34A[df_Q6Q34A.index.get_level_values(1).isin(BITools)].fillna(0)\ndf_Q6Q34A_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q34A_filtered*100,'subplot',5,EXPERIENCE_PROFILE,\"<b>fig 7.9 a)Which BI Tools are regularly used by kagglers?<b> \", '<b> % Share of BI tools <\/b>','<b>{}%<\/b><\/i>')","36f5ff2a":"df_all_Q6Q35=[]\n\ndf_all_Q6Q35.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q32'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])]['Q35'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q35.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q32'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])]['Q35'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q35.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q32'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])]['Q35'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q35.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q32'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])]['Q35'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q35.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q32'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])]['Q35'].str.strip().replace(BITOOLS_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q35=pd.concat(df_all_Q6Q35,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q35_filtered=df_Q6Q35[df_Q6Q35.index.get_level_values(1).isin(BITools)].fillna(0)\ndf_Q6Q35_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q35_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.9 b)Which BI Tools are most Oftenly used by kagglers?<b> \", '<b> % Share of BI Tools<\/b>','<b>{}%<\/b><\/i>')","208a6ef0":"AUTOML_TOPICS_MAPP={'Automated data augmentation (e.g. imgaug, albumentations)': 'AutoDataAug',\n 'Automated feature engineering\/selection (e.g. tpot, boruta_py)':'Auto FeatureEngg',\n 'Automated hyperparameter tuning (e.g. hyperopt, ray.tune, Vizier)':'Auto HyperPramT',\n 'Automated model architecture searches (e.g. darts, enas)':'Auto Model Arch',\n 'Automated model selection (e.g. auto-sklearn, xcessiv)':'Auto Model Sele',\n 'Automation of full ML pipelines (e.g. Google AutoML, H20 Driverless AI)':'AutoMLPipelines',\n 'Automation of full ML pipelines (e.g. Google AutoML, H2O Driverless AI)':'AutoMLPipelines',\n#  'No \/ None':'None',\n 'Other':'Other'}\n\nAUTOML_TOPICS=[item for _,item in AUTOML_TOPICS_MAPP.items()]\n\ndf_all_Q6Q36A=[]\n\ndf_all_Q6Q36A.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q33_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q36_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q36A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q33_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q36_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q36A.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019':0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q33_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q36_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q36A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q33_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q36_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q36A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019':0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q33_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q36_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q36A=pd.concat(df_all_Q6Q36A,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q36A_filtered=df_Q6Q36A[df_Q6Q36A.index.get_level_values(1).isin(AUTOML_TOPICS)].fillna(0)\ndf_Q6Q36A_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q36A_filtered*100,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 7.10 a)What AutoML Features are being used regularly by kagglers- Excempted 'None' ?<b> \", '<b> % AutoML Features<\/b>','<b>{}%<\/b><\/i>')","37087eab":"AUTOML_TOOLS_MAPP={'Amazon Sagemaker Autopilot':'Amz SageAutopilot',\n 'Auto-Keras':'Other',\n 'Auto-Sklearn':'Other',\n 'Auto_ml':'Other',\n 'Azure Automated Machine Learning':'Az AutoML',\n 'DataRobot AutoML':'DataRobot AutoML',\n 'Databricks AutoML':'Databricks AutoML',\n 'Google AutoML': 'GC AutoML',\n 'Google Cloud AutoML':'GC AutoML',\n 'H20 Driverless AI':'H2ODriverlessAI',\n 'H2O Driverless AI': 'H2ODriverlessAI',\n 'MLbox':'Other',\n#  'No \/ None':'None',\n#  'None':'None',\n 'Other':'Other',\n 'Tpot':'Other',\n 'Xcessiv':'Other'}\n \nAUTOML_Tools=[item for _,item in AUTOML_TOOLS_MAPP.items()]\n\ndf_all_Q6Q37A=[]\n\ndf_all_Q6Q37A.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q33_P.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q34_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q37_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q37A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q33_P.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q34_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q37_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q37A.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q33_P.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q34_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q37_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q37A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q33_P.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q34_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q37_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q37A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q33_P.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q34_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q37_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q37A=pd.concat(df_all_Q6Q37A,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q37A_filtered=df_Q6Q37A[df_Q6Q37A.index.get_level_values(1).isin(AUTOML_Tools)].fillna(0)\ndf_Q6Q37A_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q37A_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.10 b) What AutoML Tools are being used regularly by kagglers-Excempted 'None'?<b> \", '<b> % AutoML Tools<\/b>','<b>{}%<\/b><\/i>')","a689929e":"df_Q6Q37APCT=df_Q6Q37A_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0).sort_values(by='2021', ascending=False)\ndf_Q6Q37APCT=df_Q6Q37APCT[df_Q6Q37APCT >0].fillna(0)\n\ndrawplot(df_Q6Q37APCT,'subplot',3,EXPERIENCE_PROFILE,\"<b>Fig 7.10 c) Percentage change in usage of AutoML Tools ,being used regularly by kagglers-Excempted 'None'?<b> \", '<b> % change AutoML Tools<\/b>','<b>{}%<\/b><\/i>')\n","10750ef3":"MLEXP_TOOLS_MAPP={'ClearML':'ClearML',\n 'Comet.ml': 'Comet.ml',\n 'Domino Model Monitor':'Domino Model monitor',\n 'Guild.ai':'Guild.ai',\n 'MLflow':'MLFlow',\n 'Neptune.ai':'Neptune.ai',\n 'No \/ None':'None',\n 'Other':'Other',\n 'Polyaxon':'Polyaxon',\n 'Sacred + Omniboard':'Sacred + Omniboard',\n 'TensorBoard':'TensorBoard',\n 'Trains': 'Other',\n 'Weights & Biases':'Weights & Biases'}\n\nMLEXP_TOOLS=[item for _,item in MLEXP_TOOLS_MAPP.items()]\n\ndf_all_Q6Q38A=[]\n\ndf_all_Q6Q38A.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019':0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q35_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q38_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q38A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q35_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q38_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q38A.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q35_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q38_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q38A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q35_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q38_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q38A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q35_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q38_A.').stack().str.strip().replace(MLEXP_TOOLS_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q38A=pd.concat(df_all_Q6Q38A,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q38A_filtered=df_Q6Q38A[df_Q6Q38A.index.get_level_values(1).isin(MLEXP_TOOLS)].fillna(0)\ndf_Q6Q38A_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q38A_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.11 What Tools used to manage ML experiments, are being used regularly by kagglers?<b> \", '<b> % ML Experimentation Tools<\/b>','<b>{}%<\/b><\/i>')","17830f53":"PLATFORMTOSHARE_DA_MAPP={'Blogs (Towards Data Science, Analytics Vidhya, etc)':'Blogs',\n 'Colab':'Colab',\n 'Course Forums (forums.fast.ai, Coursera forums, etc)':'Course Forums',\n \"Email newsletters (Data Elixir, O'Reilly Data & AI, etc)\":'Email Newsletters',\n 'GitHub':'Github',\n 'I do not share my work publicly':'Do not Share',\n 'Journal Publications (peer-reviewed journals, conference proceedings, etc)':'Journals',\n 'Kaggle':'Kaggle',\n 'Kaggle (notebooks, forums, etc)':'Kaggle',\n 'NBViewer': 'NBViewer',\n 'None':'None',\n 'Other':'Other',\n 'Personal blog':'Personal Blog',\n 'Plotly Dash':'Plotly dash',\n 'Podcasts (Chai Time Data Science, O\u2019Reilly Data Show, etc)':'Podcasts',\n 'Reddit (r\/machinelearning, etc)':'Reddit',\n 'Shiny':'Shiny',\n 'Slack Communities (ods.ai, kagglenoobs, etc)':'Slack',\n 'Streamlit':'Streamlit',\n 'Twitter (data science influencers)':'Twitter',\n 'YouTube (Kaggle YouTube, Cloud AI Adventures, etc)':'YT'}\n\nPLATFORMTOSHARE_DA=[item for _,item in PLATFORMTOSHARE_DA_MAPP.items()]\n\ndf_all_Q6Q39=[]\n\ndf_all_Q6Q39.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019':0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q36.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q39.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q39.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q36.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q39.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q39.append(pd.DataFrame({\n                    '2018': 0,  \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q36.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q39.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q39.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q36.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True),\n                    '2021' :r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q39.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q39.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q36.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q39.').stack().str.strip().replace(PLATFORMTOSHARE_DA_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q39=pd.concat(df_all_Q6Q39,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q39_filtered=df_Q6Q39[df_Q6Q39.index.get_level_values(1).isin(PLATFORMTOSHARE_DA)].fillna(0)\ndf_Q6Q39_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q39_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.12 Where do kagglers share\/deploy ML applications?<b> \", '<b> % Platform to Share\/Deploy<\/b>','<b>{}%<\/b><\/i>')","6ad27d48":"DSCOURSE_PLAT_MAPP={'Cloud-certification programs (direct from AWS, Azure, GCP, or similar)':'Cloud-certiProg',\n 'Coursera':'Coursera',\n 'DataCamp':'Datacamp',\n 'DataQuest':'dataQuest',\n 'Fast.AI':'Fast.ai',\n 'Fast.ai':'Fast.ai',\n 'Kaggle Courses (i.e. Kaggle Learn)':'Kaggle',\n 'Kaggle Learn':'Kaggle',\n 'Kaggle Learn Courses':'Kaggle',\n 'LinkedIn Learning':'LinkedIn',\n 'None':'None',\n 'Online University Courses': 'Online Univ',\n 'Other':'Other',\n 'TheSchool.AI': 'TheSchool.AI',\n 'Udacity':'Udacity',\n 'Udemy':'Udemy',\n 'University Courses (resulting in a university degree)':'Univ Courses',\n 'developers.google.com': 'developers.google.com',\n 'edX':'edX'}\n\n\nDSCOURSE_PLAT=[item for _,item in DSCOURSE_PLAT_MAPP.items()]\n\ndf_all_Q6Q40=[]\n\ndf_all_Q6Q40.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q36_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q13_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q37.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q40.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q40.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q36_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q13_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q37.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q40.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q40.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q36_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q13_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q37.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q40.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q40.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q36_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q13_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q37.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q40.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q40.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q36_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q13_P.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q37.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q40.').stack().str.strip().replace(DSCOURSE_PLAT_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q40=pd.concat(df_all_Q6Q40,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q40_filtered=df_Q6Q40[df_Q6Q40.index.get_level_values(1).isin(DSCOURSE_PLAT)].fillna(0)\ndf_Q6Q40_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q40_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.13 Where do kagglers improve their knowledge from?<b> \", '<b> % Share of DS Course Platform<\/b>','<b>{}%<\/b><\/i>')","2053bce8":"PriTOOL_ANALYZE_MAPP={'Advanced statistical software (SPSS, SAS, etc.)':'Adv Stat Soft',\n 'Basic statistical software (Microsoft Excel, Google Sheets, etc.)':'Basic Stat Soft',\n 'Business intelligence software (Salesforce, Tableau, Spotfire, etc.)':'BI Software',\n 'Cloud-based data software & APIs (AWS, GCP, Azure, etc.)':'Cloud-based data Soft',\n 'Local development environments (RStudio, JupyterLab, etc.)':'Local Dev Env',\n 'Other':'Other'}\n\nPriTOOL_ANALYZE=[item for _,item in PriTOOL_ANALYZE_MAPP.items()]\n\ndf_all_Q6Q41=[]\n\ndf_all_Q6Q41.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q12').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q14').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q38').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q41').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q41.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q12').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q14').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q38').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q41').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q41.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q12').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q14').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q38').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q41').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q41.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q12').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q14').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q38').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q41').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q41.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q12').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q14').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q38').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q41').stack().str.strip().replace(PriTOOL_ANALYZE_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q41=pd.concat(df_all_Q6Q41,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q41_filtered=df_Q6Q41[df_Q6Q41.index.get_level_values(1).isin(PriTOOL_ANALYZE)].fillna(0)\ndf_Q6Q41_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q41_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.14. What Primary tools for data analysis do Kagglers Use?<b> \", '<b> % Share of data Analysis tools<\/b>','<b>{}%<\/b><\/i>')","6b53c20f":"MEDIA_SOURCES_MAPP={'ArXiv & Preprints':'ArXiv',\n 'Blogs (Towards Data Science, Analytics Vidhya, etc)':'Blogs',\n 'Blogs (Towards Data Science, Medium, Analytics Vidhya, KDnuggets etc)':'Blogs',\n 'Cloud AI Adventures (YouTube)':'YT',\n 'Course Forums (forums.fast.ai, Coursera forums, etc)':'Forums',\n 'Course Forums (forums.fast.ai, etc)':'Forums',\n 'DataTau News Aggregator':'Websites',\n \"Email newsletters (Data Elixir, O'Reilly Data & AI, etc)\":'Newsletter',\n 'FastML Blog':'Blogs',\n 'Fastai forums':'Forums',\n 'FiveThirtyEight.com':'Websites',\n 'Hacker News':'Newsletter',\n 'Hacker News (https:\/\/news.ycombinator.com\/)':'Newsletter',\n 'Journal Publications':'Journals',\n 'Journal Publications (peer-reviewed journals, conference proceedings, etc)':'Journals',\n 'Journal Publications (traditional publications, preprint journals, etc)':'Journals',\n 'KDnuggets Blog':'Blogs',\n 'Kaggle (forums, blog, social media, etc)':'Kaggle',\n 'Kaggle (notebooks, forums, etc)':'Kaggle',\n 'Kaggle forums':'Kaggle',\n 'Linear Digressions Podcast':'Podcast',\n 'Medium Blog Posts':'Blogs',\n 'None':'None',\n 'None\/I do not know':'None',\n \"O'Reilly Data Newsletter\":'Newsletter',\n 'Other':'Other',\n 'Partially Derivative Podcast':'Podcast',\n 'Podcasts (Chai Time Data Science, Linear Digressions, etc)':'Podcast',\n 'Podcasts (Chai Time Data Science, O\u2019Reilly Data Show, etc)':'Podcast',\n 'Reddit (r\/machinelearning, etc)':'Forums',\n 'Reddit (r\/machinelearning, r\/datascience, etc)':'Forums',\n 'Siraj Raval YouTube Channel':'YT',\n 'Slack Communities (ods.ai, kagglenoobs, etc)':'Social Media',\n 'The Data Skeptic Podcast':'Podcast',\n 'Twitter':'Social Media',\n 'Twitter (data science influencers)':'Social Media',\n 'YouTube (Cloud AI Adventures, Siraj Raval, etc)':'YT',\n 'YouTube (Kaggle YouTube, Cloud AI Adventures, etc)':'YT',\n 'r\/machinelearning':'Forums'}\n\n\nMEDIA_SOURCES=[item for _,item in MEDIA_SOURCES_MAPP.items()]\n\ndf_all_Q6Q42=[]\n\ndf_all_Q6Q42.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q38_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q12_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q39.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[0:2])].filter(regex='Q42.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q42.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q38_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q12_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q39.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin(EXPERIENCE_ORDER[2:4])].filter(regex='Q42.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True)}))\n\n\ndf_all_Q6Q42.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q38_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q12_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q39.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[4])])].filter(regex='Q42.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q6Q42.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q38_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q12_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q39.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[5])])].filter(regex='Q42.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True)}))\ndf_all_Q6Q42.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q38_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q12_P.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q39.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q6_new'].isin([str(EXPERIENCE_ORDER[6])])].filter(regex='Q42.').stack().str.strip().replace(MEDIA_SOURCES_MAPP).value_counts(normalize=True)}))\n             \ndf_Q6Q42=pd.concat(df_all_Q6Q42,keys=EXPERIENCE_PROFILE)\n# df_Q6Q31A\ndf_Q6Q42_filtered=df_Q6Q42[df_Q6Q42.index.get_level_values(1).isin(MEDIA_SOURCES)].fillna(0)\ndf_Q6Q42_filtered.index.rename('subplot',level=0,inplace=True)\n# # display(df_Q6Q27A_filtered)\ndrawplot(df_Q6Q42_filtered*100,'subplot',4,EXPERIENCE_PROFILE,\"<b>Fig 7.15 Which are the favorite media sources for kagglers on DS?<b> \", '<b> % Share of Media Sources<\/b>','<b>{}%<\/b><\/i>')","83d753d7":"\ndf_all_Q3Q20=[]\n\ndf_all_Q3Q20.append(pd.DataFrame({\n                    '2018': 0 ,   \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q3'].isin(['India'])].filter(regex='Q20').stack().value_counts(normalize=True)}))\ndf_all_Q3Q20.append(pd.DataFrame({\n                    '2018': 0 ,   \n                    '2019': 0,\n                    '2020': 0,\n                    '2021': r_2021[r_2021['Q3'].isin(['USA'])].filter(regex='Q20').stack().value_counts(normalize=True)}))\ndf_Q3Q20=pd.concat(df_all_Q3Q20,keys=['India','USA'])\n\ndf_Q3Q20_filtered=df_Q3Q20.fillna(0)\ndf_Q3Q20_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q3Q20_filtered*100,'subplot',3,['India','USA'],\"<b>Fig 8.1. Data Verticals of India & USA?<b> \", '<b> % Share Industry<\/b>','<b>{}%<\/b><\/i>')","908f7672":"\ndf_all_Q3Q23=[]\n\ndf_all_Q3Q23.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q3'].isin(['India'])]['Q10'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q3'].isin(['India'])]['Q8'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q3'].isin(['India'])]['Q22'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q3'].isin(['India'])]['Q23'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q3Q23.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q3'].isin(['USA'])]['Q10'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),       \n                    '2019': r_2019[r_2019['Q3'].isin(['USA'])]['Q8'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q3'].isin(['USA'])]['Q22'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q3'].isin(['USA'])]['Q23'].replace(IF_ML_METHODS_USED_MAPP).value_counts(normalize=True)}))\ndf_Q3Q23=pd.concat(df_all_Q3Q23,keys=['India','USA'])\n\ndf_Q3Q23_filtered=df_Q3Q23[df_Q3Q23.index.get_level_values(1).isin(IF_ML_METHODS_USED)].fillna(0)\ndf_Q3Q23_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q3Q23_filtered*100,'subplot',4,['India','USA'],\"<b>Fig 8.2 . What's the state of ML in USA & India?<b> \", '<b> % Share ML Features<\/b>','<b>{}%<\/b><\/i>')","529f0dfb":"df_Q3Q23_pct=df_Q3Q23_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0)\ndisplay(df_Q3Q23_pct.style.applymap(color, subset=['2020','2021']))","3f5b3e3e":"MONEYSPENT_ML_Cloud_MAPP={'$0 ($USD)':'$0',\n '$1-$99':'$1-$99',\n '$10,000-$99,999':'$10,000-$99,999',\n '$100,000 or more ($USD)':'$100,000+',\n '$100-$999':'$100-$999',\n '$1000-$9,999':'$1000-$9,999',\n 112500:'$100,000+',\n 5000:'$1000-$9,999',\n 50000:'$10,000-$99,999'}\n\nMONEYSPENT_ML_Cloud=[item for _,item in MONEYSPENT_ML_Cloud_MAPP.items()]\n\ndf_all_Q3Q26=[]\n\ndf_all_Q3Q26.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019':r_2019[r_2019['Q3'].isin(['India'])]['Q11'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2020':r_2020[r_2020['Q3'].isin(['India'])]['Q25'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2021':r_2021[r_2021['Q3'].isin(['India'])]['Q26'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q3Q26.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019':r_2019[r_2019['Q3'].isin(['USA'])]['Q11'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2020':r_2020[r_2020['Q3'].isin(['USA'])]['Q25'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True),\n                    '2021':r_2021[r_2021['Q3'].isin(['USA'])]['Q26'].replace(MONEYSPENT_ML_Cloud_MAPP).value_counts(normalize=True)}))\n\ndf_Q3Q26=pd.concat(df_all_Q3Q26,keys=['India','USA'])\n\ndf_Q3Q26_filtered=df_Q3Q26[df_Q3Q26.index.get_level_values(1).isin(MONEYSPENT_ML_Cloud)].fillna(0)\ndf_Q3Q26_filtered.index.rename('subplot',level=0,inplace=True)\ndisplay(df_Q3Q26_filtered)\ndrawplot(df_Q3Q26_filtered*100,'subplot',4,['India','USA'],\"<b>Fig 8.3 a). What's the total Investment ML in USA & India?<b> \", '<b> % Share ML Investment Range<\/b>','<b>{}%<\/b><\/i>')","89d8bfba":"df_Q3Q26_pct=df_Q3Q26_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0)\ndisplay(df_Q3Q26_pct.style.applymap(color, subset=['2020','2021']))\ndrawplot(df_Q3Q26_pct,'subplot',4,['India','USA'],\"<b>Fig 8.3 b) . What's the % change Investment ML computing devices in USA & India?<b> \", '<b> % Share ML Investment Range<\/b>','<b>{}%<\/b><\/i>')","d20f9cfa":"CLOUDCOMP_PLAT_MAPP={'Alibaba Cloud':'Alibaba',\n 'Amazon Web Services (AWS)':'AWS',\n 'Google Cloud Platform (GCP)':'GCP',\n 'I have not used any cloud providers':'Not Used Cloud',\n 'IBM Cloud':'IBM',\n 'IBM Cloud \/ Red Hat':'IBM',\n 'Microsoft Azure':'MSFT Azure',\n 'None':'None',\n 'Oracle Cloud':'Oracle',\n 'Other':'Other',\n 'Red Hat Cloud':'IBM',\n 'SAP Cloud':'SAP',\n 'Salesforce Cloud':'Salesforce',\n 'Tencent Cloud':'Tencent',\n 'VMware Cloud':'VMware'}\n\nCLOUDCOMP_PLATs=[item for _,item in CLOUDCOMP_PLAT_MAPP.items()]\n\ndf_all_Q3Q27A=[]\n\ndf_all_Q3Q27A.append(pd.DataFrame({\n                    '2019': r_2019[r_2019['Q3'].isin(['India'])].filter(regex='Q29_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q3'].isin(['India'])].filter(regex='Q26_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2018': r_2018[r_2018['Q3'].isin(['India'])].filter(regex='Q15_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),       \n                    '2021': r_2021[r_2021['Q3'].isin(['India'])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q3Q27A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q3'].isin(['USA'])].filter(regex='Q15_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q3'].isin(['USA'])].filter(regex='Q29_P.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q3'].isin(['USA'])].filter(regex='Q26_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q3'].isin(['USA'])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCOMP_PLAT_MAPP).value_counts(normalize=True)}))\n\n\n             \ndf_Q3Q27A=pd.concat(df_all_Q3Q27A,keys=['India','USA'])\n\ndf_Q3Q27A_filtered=df_Q3Q27A[df_Q3Q27A.index.get_level_values(1).isin(CLOUDCOMP_PLATs)].fillna(0)\ndf_Q3Q27A_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q3Q27A_filtered*100,'subplot',4,['India','USA'],\"<b>Fig 8.4 a).Which Cloud computing Platform do the Countries Prefer?<b> \", '<b> % Share of Cloud Computing Platforms<\/b>','<b>{}%<\/b><\/i>')","ea080957":"CLOUDCompPlatRegular_MAPP={'AWS Batch':'AWS (EC2)',\n 'AWS Elastic Beanstalk':'AWS (EC2)',\n 'AWS Elastic Compute Cloud (EC2)':'AWS (EC2)',\n 'AWS Lambda':'AWS (EC2)',\n 'Amazon EC2':'AWS (EC2)',\n 'Amazon Elastic Compute Cloud (EC2)':'AWS (EC2)',\n 'Amazon Elastic Container Service':'AWS (EC2)',\n 'Azure Batch':'MSFT Azure VMs',\n 'Azure Cloud Services':'MSFT Azure VMs',\n 'Azure Container Service':'MSFT Azure VMs',\n 'Azure Event Grid':'MSFT Azure VMs',\n 'Azure Functions':'MSFT Azure VMs',\n 'Azure Kubernetes Service':'MSFT Azure VMs',\n 'Azure Virtual Machines':'MSFT Azure VMs',\n 'Google App Engine':'GC Compute E',\n 'Google Cloud App Engine':'GC Compute E',\n 'Google Cloud Compute Engine':'GC Compute E',\n 'Google Cloud Functions':'GC Compute E',\n 'Google Cloud Run':'GC Compute E',\n 'Google Compute Engine':'GC Compute E',\n 'Google Compute Engine (GCE)':'GC Compute E',\n 'Google Kubernetes Engine':'GC Compute E',\n 'IBM Cloud Container Registry' :'Other',\n 'IBM Cloud Foundry' :'Other',\n 'IBM Cloud Kubernetes Service': 'Other',\n 'IBM Cloud Virtual Servers': 'Other',\n 'Microsoft Azure Container Instances':'MSFT Azure VMs',\n 'Microsoft Azure Virtual Machines':'MSFT Azure VMs',\n 'No \/ None':'None',\n 'None':'None',\n 'Other':'Other'}\n\n\nCLOUDCompPlatRegular=[item for _,item in CLOUDCompPlatRegular_MAPP.items()]\n\n\ndf_all_Q3Q29A=[]\n\ndf_all_Q3Q29A.append(pd.DataFrame({\n                    '2018':r_2018[r_2018['Q3'].isin(['India'])].filter(regex='Q27_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),       \n                    '2019':r_2019[r_2019['Q3'].isin(['India'])].filter(regex='Q30_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2020':r_2020[r_2020['Q3'].isin(['India'])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2021':r_2021[r_2021['Q3'].isin(['India'])].filter(regex='Q29_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q3Q29A.append(pd.DataFrame({\n                    '2018': r_2018[r_2018['Q3'].isin(['USA'])].filter(regex='Q27_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),        \n                    '2019': r_2019[r_2019['Q3'].isin(['USA'])].filter(regex='Q30_P.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q3'].isin(['USA'])].filter(regex='Q27_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q3'].isin(['USA'])].filter(regex='Q29_A.').stack().str.strip().replace(CLOUDCompPlatRegular_MAPP).value_counts(normalize=True)}))\n\n\n             \ndf_Q3Q29A=pd.concat(df_all_Q3Q29A,keys=[\"India\",\"USA\"])\n\ndf_Q3Q29A_filtered=df_Q3Q29A[df_Q3Q29A.index.get_level_values(1).isin(CLOUDCompPlatRegular)].fillna(0)\ndf_Q3Q29A_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q3Q29A_filtered*100,'subplot',4,['India','USA'],\"<b>fig 8.4.b) Which Cloud computing Products have the Countries been using?<b> \", '<b> % Share of Cloud Computing Products<\/b>','<b>{}%<\/b><\/i>')","88bb9bd0":"df_Q3Q29_pct=df_Q3Q29A_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0)\ndisplay(df_Q3Q29_pct.style.applymap(color, subset=['2020','2021']))\ndrawplot(df_Q3Q29_pct,'subplot',4,['India','USA'],\"<b>Fig 8.4 c) . What's the % change Usage of Cloud Computing Products in USA & India?<b> \", '<b> % Share Cloud Computing products<\/b>','<b>{}%<\/b><\/i>')","8763da24":"AUTOML_TOPICS_MAPP={'Automated data augmentation (e.g. imgaug, albumentations)': 'AutoDataAug',\n 'Automated feature engineering\/selection (e.g. tpot, boruta_py)':'Auto FeatureEngg',\n 'Automated hyperparameter tuning (e.g. hyperopt, ray.tune, Vizier)':'Auto HyperPramT',\n 'Automated model architecture searches (e.g. darts, enas)':'Auto Model Arch',\n 'Automated model selection (e.g. auto-sklearn, xcessiv)':'Auto Model Sele',\n 'Automation of full ML pipelines (e.g. Google AutoML, H20 Driverless AI)':'AutoMLPipelines',\n 'Automation of full ML pipelines (e.g. Google AutoML, H2O Driverless AI)':'AutoMLPipelines',\n 'No \/ None':'None',\n 'Other':'Other'}\n\nAUTOML_TOPICS=[item for _,item in AUTOML_TOPICS_MAPP.items()]\n\ndf_all_Q3Q36A=[]\n\ndf_all_Q3Q36A.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q3'].isin(['India'])].filter(regex='Q33_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q3'].isin(['India'])].filter(regex='Q36_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q3Q36A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': 0,\n                    '2020': r_2020[r_2020['Q3'].isin(['USA'])].filter(regex='Q33_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q3'].isin(['USA'])].filter(regex='Q36_A.').stack().str.strip().replace(AUTOML_TOPICS_MAPP).value_counts(normalize=True)}))\n\n\n             \ndf_Q3Q36A=pd.concat(df_all_Q3Q36A,keys=['India','USA'])\n\ndf_Q3Q36A_filtered=df_Q3Q36A[df_Q3Q36A.index.get_level_values(1).isin(AUTOML_TOPICS)].fillna(0)\ndf_Q3Q36A_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q3Q36A_filtered*100,'subplot',3,['India','USA'],\"<b>Fig 8.5 a)What AutoML Features are being used regularly by USA& India- Excempted 'None' ?<b> \", '<b> % AutoML Features<\/b>','<b>{}%<\/b><\/i>')","ce35c911":"df_Q3Q36A_pct=df_Q3Q36A_filtered.pct_change(axis='columns',fill_method='bfill', periods=1).replace([np.inf,-np.inf,np.nan],0) \ndisplay(df_Q3Q36A_pct.style.applymap(color, subset=['2020','2021']))\ndisplay(df_Q3Q36A_pct.groupby(['subplot'])['2021'].agg([('negative' , lambda x : x[x < 0].sum()) , ('positive' , lambda x : x[x > 0].sum())]))\ndrawplot(df_Q3Q36A_pct,'subplot',3,['India','USA'],\"<b>Fig 8.5 b)What is the % change inAutoML Features are being used regularly by USA & India- Excempted 'None' ?<b> \", '<b> % AutoML Features<\/b>','<b>{}%<\/b><\/i>')","3e27d98f":"AUTOML_TOOLS_MAPP={'Amazon Sagemaker Autopilot':'Amz SageAutopilot',\n 'Auto-Keras':'Other',\n 'Auto-Sklearn':'Other',\n 'Auto_ml':'Other',\n 'Azure Automated Machine Learning':'Az AutoML',\n 'DataRobot AutoML':'DataRobot AutoML',\n 'Databricks AutoML':'Databricks AutoML',\n 'Google AutoML': 'GC AutoML',\n 'Google Cloud AutoML':'GC AutoML',\n 'H20 Driverless AI':'H2ODriverlessAI',\n 'H2O Driverless AI': 'H2ODriverlessAI',\n 'MLbox':'Other',\n 'No \/ None':'None',\n 'None':'None',\n 'Other':'Other',\n 'Tpot':'Other',\n 'Xcessiv':'Other'}\n \nAUTOML_Tools=[item for _,item in AUTOML_TOOLS_MAPP.items()]\n\ndf_all_Q3Q37A=[]\n\ndf_all_Q3Q37A.append(pd.DataFrame({\n                    '2018': 0,       \n                    '2019': r_2019[r_2019['Q3'].isin(['India'])].filter(regex='Q33_P.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q3'].isin(['India'])].filter(regex='Q34_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q3'].isin(['India'])].filter(regex='Q37_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True)}))\n\ndf_all_Q3Q37A.append(pd.DataFrame({\n                    '2018': 0,        \n                    '2019': r_2019[r_2019['Q3'].isin(['USA'])].filter(regex='Q33_P.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2020': r_2020[r_2020['Q3'].isin(['USA'])].filter(regex='Q34_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True),\n                    '2021': r_2021[r_2021['Q3'].isin(['USA'])].filter(regex='Q37_A.').stack().str.strip().replace(AUTOML_TOOLS_MAPP).value_counts(normalize=True)}))\n\n\n             \ndf_Q3Q37A=pd.concat(df_all_Q3Q37A,keys=['India','USA'])\n\ndf_Q3Q37A_filtered=df_Q3Q37A[df_Q3Q37A.index.get_level_values(1).isin(AUTOML_Tools)].fillna(0)\ndf_Q3Q37A_filtered.index.rename('subplot',level=0,inplace=True)\n\ndrawplot(df_Q3Q37A_filtered*100,'subplot',4,['India','USA'],\"<b>Fig 8.5 c) What AutoML Tools are being used regularly by kagglers-Excempted 'None'?<b> \", '<b> % AutoML Tools<\/b>','<b>{}%<\/b><\/i>')","246fb8ac":"## <center> 7.7 What's data Science Munching In Companies?:g)Do Kagglers eat ML superfood product? <\/center>","3858b056":"# 5.Data Scientists\/ ML Enginners : What is their job entitled with?\n<div style=\"font-family:Arial; word-spacing:1.5px;color:black;display:fill;\n           border-radius:5px;\n           background-color:azure;\">\n<b>\n<ol> A labelling has been adopted in this section, combining experience categories:\n    <br>\n    <li>Beginners: None, < 1 year <\/li><br>\n    <li>Seasoned Coders: 1-3 years,3-5 years <\/li><br>\n    <li>Data Scientists:5-10 years <\/li><br>\n    <li>Seasoned Data Scientists:10-10 years<\/li><br>\n    <li>ML Veterns: 20+ yearience<\/li><br>\n<\/ol>\n<\/b>\n   \n<\/div>","0cc3dcc8":"<div style=\"font-size:15px; font-family:verdana;\"><b>I have taken this survey by asking a number of questions. The premise of questions are as below: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>\n    <li> In 2021, what is the state of gender diversity in the Kaggle Platform? <\/li>\n     <li>What is the experience level of kagglers, based on that how do they benchmark, ML sevices, Products, Platforms? <\/li>\n    <li> What is the state of data science in USA & India, how are they comparable being the 2 countries with most kaggler responses. <\/li>\n       \n<\/ol>\n    \n    Have tried answering the topics fore mentioned in a methodolical way using simple line charts with annotations. In some sections you can find data table to aid further clarity. Without Any delay, lets walk down the kaggle survey! \n    \n\n<\/div>\n<br>\n\n***","968fe81f":"## <center> 8.1 India & USA:a)What are the Favourite Verticals? <\/center>","f67ef3fd":"## <center> 5.11 The armories of Data Scientists:i)NLP Methods Regularly Used? <\/center>","d773ba63":"## <center> 7.8 What's data Science Munching In Companies?:h)Do Kagglers use big food storage tanks? <\/center>","2524ec68":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 8.4 a), Fig 8.4 b) & Fig 8.4 c): <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>\n    <li> AWS is the leading in share as compared to other Cloud Computing Platforms,by considerable margin in both the countries. <\/li>\n     <li> AWS(EC2) is the leading in share as comared to other Cloud Computing Products,by considerable margin in both the countries. <\/li>\n    <li> In the year 2021, the cloud computing products from companies other than Amazon,Google,Microsoft has outgrown. <\/li>\n       \n<\/ol>\n\n<\/div>\n<br>\n\n***","4c554cc6":"## <center> 7.14 What's data Science Munching In Companies?:m)Primary tools used to analyze data ? <\/center>","ad445ac9":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.10: <\/b><br>\n\n<ol>This following trend observed about Computer Vision methods used regularly\n    <li> Image Classification & general purpose networks,Image Segmentation, Object Detection, General Purpose methods ,Image and Video Tools are well sought CV methods among beginners and Experienced.<\/li>\n    <li>Apart from The fore Computer vision methods, Generative Networks are seeing a psitive edgesince 2020, Couldbe mecause of a varing number of CV\/ AR features being seen in  social media sites like Instagram, FB etc <\/li>\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; Image Classification & general purpose networks,Image Segmentation, Object Detection, General Purpose methods ,Image and Video Tools well seen , Generative networks also rising in usage.\n\n***","50971ab5":"## <center> 5.7 The armories of Data Scientists:f)Data Visualizatio Libraries\ud83d\udcdd? <\/center>","f67549b2":"## <center> 7.13 What's data Science Munching In Companies?:l)Favourable Course Platforms ? <\/center>","9302844e":"\n    \nData Science\/ML could be interpreted as the movement of Data. Ever since it gained its momentum in the past decade, the world has been able to harness the data and adress various issues. Platforms like kaggle are institutions for man to improve knowlegede on \"How to Harness the data?\". Since many years kaggle has conducted survey to know its audience and as a whole make certain idea of the general data science community. This includes, getting to know the educational aspect of the kagglers, their educational packgrould, aspects of field they are working in...and many others. So my take of this years kaggle survey is by asking questions to the data. This is not exhaustive, as there are various perspectives that can be thought about. However, I feel this is a humble approch. So , tighten your seat belts and check the beauty of this survey by navigating down the notebook!. \n    ","8f9d0c6d":"# 6.Data Scientists\/ ML Enginners : Where do they Flourish?","c55d5388":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.6: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf This following trend observed about the data storage products used \n<ol><b>Beginners<\/b>\ud83c\udfaf\n   As of 2021 following points noted\n    <li> ~24% don't use any data storage products <\/li>\n    <li> ~26% use Google Cloud Storage (GCS)<\/li>\n    <li> ~13% use Amazon Simple storage<\/li>\n   \n<\/ol>\n<ol><b>Experienced<\/b>\n    <li> ~30% use  Amazon Simple storage<\/li>\n    <li> ~22% use Goodle Cloud Storage (GCS)<\/li>\n    <li> ~19% use None<\/li>\n<\/ol>\n\n\n\n<\/div>\n\n<br>\n\n***","f846fbfa":"## <center> 8.4 India & USA:d)Which Cloud Computing Platform Does they Prefer? <\/center>","9aafe5d5":"## <center> 8.2 India & USA:b)What is the state of ML? <\/center>","fa463f47":"# \ud83d\udcda LIBRARIES ","a09cb9c9":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.4a) and 5.4 b): <\/b><br>\n\n<ol>This following trend observed about Hosted Notebooks\n    <li>kaggle,Google colab,Googe-cloud Notebooks, JupyterHub\/Binder are flourishig among the people who use hosted noebooks\/li>\n    <li>However there is small dip in the usage of the above mentioned hosted notebooks among Data scientists and Seasoned Data Scientists. <\/li>\n    <li>Also not to get away, manay are not using any hosted ntebooks.\n    <li>Ths could be attributed to people using powerful local machines liike GPUs, TPUs or any powerful local workstations<\/li>\n<li> CodeOcean Notebooks, Databricks gained the attention of beginners in the past year. While usage of Google cloud notebooks among the beginners have decreasd in the past year.\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; kaggle,Google colab, Google Cloud notebooks are the hosted notebooks popular among Data science individuals\n\n***","752b8b74":"## <center> 7.3. What's data Science Munching In Companies?:c)Who's in which cloud? <\/center>","b188dec3":"## <center> 7.5 What's data Science Munching In Companies?:e)Which cloud comuting product is your food? <\/center>","a4fb8b26":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 8.3 a), Fig 8.3 b): <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>\n    <li> The investment in ML? cloud coputing devices in the year 2021 is substantially greater than 2020 , both in India and USA,After a dip in the year 2020 <\/li>\n       \n<\/ol>\n\n<\/div>\n<br>\n\n***","338021c2":"## <center> 6.1 Lets Find Out the Whereabouts of Data Scientists:a)Industrial Segment? <\/center>","afeb94d2":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 4.3. and Table 4.3: <\/b><br>\n\n<ol>This following trend observed since 2020:\n    <li>In the year 2020, every country minimized their investment on data science talent.<\/li>\n    <li>2021:USA is still the country with power to relish data science minds of all experience levels. Followed by Germany , UK and Japan. <\/li>\n    <li>It is a need to highlight that even though most of the respondents are from India, still the average salary is no match to other countries. Perhaps the kagglers from India are in the early stages of their respective carrer<\/li>\n    <li>Another country in the elight list is china. <\/li>\n    <li>Japan is also paying higher salaries for experience range 5-10 years and above. even being a small country<\/li>\n    <li>year 2018,2019 Turkey is putting its efforts to harness the data putting to practice both freshers and experienced. <\/li>\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; 2021, ML\/ DS Job Title salaris are higher in countries USA, UK, germany, Japan\n\n***","d76b5e5c":"## <center> 4.2 Kagglers:What job title should you seek for higher \ud83d\udcb2salary in the DS\/ML industry? <\/center>","219de349":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 3.5. and Table 3.5: <\/b><br>\n\n<ol>\n    <li>Here male and female  average yearly salary is plotted overthe yars for each experience category.<\/li>\n    <li>It's clearly visibel that for the same experience males and females are paid unevenly.<\/li>\n    <li>Infact, as the experience gets higher, the difference between male and femlale renumeration increases.This is clearly visible from 5-10 years and above.<\/li>\n    <li>Not to slip, the male and female avg salaries saw a dip in the yaer 2020 for each experience category from a peak in the year 2019.<\/li>\n    <li>In the year 2021, the avg salaries wrt experience are getting back on track, with may economies adapting to the \"new normal\" and living with the pandamic<\/li>\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; Are \ud83d\udc69Females and \ud83d\udc68males paid equally? Unfortunately, we are living in the world that Males are paid higher than females.\n<\/div>\n\n***","7d154ae5":"## <center> 6.2 Lets Find Out the Whereabouts of Data Scientists:b)Number of Coworkers? <\/center>","6134758e":"## <center> 5.4 The armories of Data Scientists:C)Any hosted Notebok Enviroment\ud83d\udcdd? <\/center>","a701fec5":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 3.1a) Fig 3.1b):<\/b><br>\n<ol>\n    <li>Of the particants of kaggle survey, male and female are represent the gender proportion.<\/li>\n    <li>The % of respondents male and female respondents has increased lightly since 2020, after an year of pandamic( golden ratio: ~79%Male,~19% female)<\/li>\n    <li>female and male respondents in numbers has shwon to increase!. Does it mean there is going to be IT boom in this country?<\/li>\n    <li>Minority gendery categories are \"Prefer not to say: and \"non-binary\"<\/li>\n<\/ol>\n\n<\/div>\n\n<br>\n \n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b>Down the pipeline of analysis, we will consider only the male and female respondents.!<\/b>\n<\/div>","9ac0e5df":"# ARRANGEMENT OF THE ANALYSIS     ","0a8d5e75":"# 2.\ud83d\udcc8 THE STORY TELLING PIPELINE\n# 3.Lets Meet the Kagglers! \ud83d\udc68\u200d\ud83d\udcbb\n## <center> 3.1 Get to Know Kagglers: \ud83d\udc68\u200d\ud83d\udcbb What's the gender proportion, is it still 80:20 (Male:female)? <\/center>","9dd37ef1":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.11: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf This following trend observed on tools used to manage ML experiments by both Beginners and Experienced\n<ol>\ud83c\udfafIt's a surprise that many don't still use any ML experimentation tools. Among the used tools following are popular:\n    <li> Tensorbard<\/li>\n    <li>  MLflow <\/li>\n    <li>  Weights and Biases<\/li>\n  \n<\/ol>\n\n<\/div>\n<br>\n\n***\n","82d7127f":"## <center> 7.12 What's data Science Munching In Companies?:k)Platforms used to present public data? <\/center>","a87e3f37":"## <center> 3.2 Get to know Kagglers: \ud83d\udc68\u200d\ud83d\udcbbWhat's the age group? <\/center>\n","5442064c":"# 1.\ud83c\udf41MAPPINGS AND \ud83d\udcc8PLOTTING FUNCTION DEFINITIONs","1c54a7a9":"# A heart Note\n\nI would like to give my humble thanks to Kaggle for conducting survey every year, to bring to the forefront, the face of Data science and ML. This not only gives wide perspective on Data science but encompass understanding of begineer with the technical aspects. \n\nIf you find this notebook meaningful kindly upvote as an encouragement!\ud83d\ude0a","8ed76cd4":"## <center> 3.3  Kagglers Get Introduced: \ud83d\udc68\u200d\ud83d\udcbb where are they from \ud83d\uddfa? <\/center>","37399e4b":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 6.1: <\/b><br>\n\n<ol>This following trend observed about Industries where Kagglers can be found\n    <li> Main fields in which data science roles are found is Academics, IT, Accounting and Finance, manufaturing\/fabrication,<\/li>\n    <li> Experienced individuals also seem to work in Govt\/Public Service and medical\/Pharma.<\/li>\n    <li> There are also 'Others' not defined verticals, where data science has entered.<\/li>\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; 'Academics', IT companies, Finance are top employers in data science\n\n***","749b289f":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.5: <\/b><br>\n\n<ol>This following trend observed about Computing platform\n    <liPC\/>Persoal Computers\/Laptop is used by all experience level, Beginner to Veterans \/li>\n    <li>Cloud Computing platforms sudddenly reduced in the year 2020, may be because of COVID. <\/li>\n    <li>Deep learning becoming vbrant in these years, in various industries,  Seasoned Cders, Data Scientists use them. However, their adoption have not incread much since 2020..\n\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; PC\/Laptop are must among all leves of sftware profession, Cloud is optional , unless you need it for sepecialized development.Deep Learning workstations becoming common among experienced individuals.\n\n***","816f6372":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 3.2,Table 3.2:<\/b><br>\n<ol>\n    <li>In both male and female category, the respondents are mainly in the age groups '18-21','22-24','25-29'.<\/li>\n    <li> What is to be noted heer is that the % age group '18-21' has clearly increased since 2018. This is positive as various education sectors are introducing programming in the early years of education(STEM Undertakings) <\/li>\n    <li>Also its intresting that there are elderly people (70+) entering into data science!<\/li>\n<\/ol>\n\n<\/div>\n\n<br>\n \n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp;<b>respondents in the age group'18-21' are clearly on a rise, this clearly indicates ML\/ data science has gained its momentum over the years and impacted the lives!<\/b>\n<\/div>","94d2658d":"## <center> 7.10 What's data Science Munching In Companies?:j)Do Kagglers Automate the task the crunching the data? <\/center>","ec36bc10":"## <center> 8.3 India & USA:c)What is Investment on ML devices? <\/center>","536a14d0":"## <center> 7.11 What's data Science Munching In Companies?:j)What tools are being used for ML experimentation? <\/center>","5141bbf6":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.9: <\/b><br>\n\n<ol>This following trend observed about ML algorithms used regularly\n    <li> kagglers on all levels of experence, seem to regularl use linear\/logistic regression, decision trees, random forest, gradient Boosting methods, apart from CNNs.<\/li>\n    <li>Dense Neural Networks, Bayesian Approches, Transformer networks are also common<\/li>\n    <li>Its observed that Transformer Networks have steep growth slop, could be because of various advancements in NLP like BERT(2019),GPT-2,GPT3(2020) <\/li>\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; linear\/logistic regression, decision trees, random forest, gradient Boosting methods,CNN common among all levels of experience. Seems these methods are widely adoped in the data strategies.!\n\n***","a73673ab":"## <center> 6.3 Lets Find Out the Whereabouts of Data Scientists:c)Number of DS individuals in the company? <\/center>","4c176bc8":"<div style=\"font-size:15px; font-family:verdana;\">Inferences from the above Fig 3.3. and Table 3.3:<br><br>\n\n<ol>\n    <li>Of the top 2 countries most of respondents, both female and male, are mostly from  India and USA.<\/li>\n    <li>The number of respondents since 2020 has almost remainded the same. (India: ~35%Male,~10% female; USA: ~12%male, ~4% female)<\/li>\n    <li>Intrestingly and on positive note, female and male respondents in Egypt has shwn to increase!. Does it mean there is going to be IT boom in this country?<\/li>\n    <li>UK, Japan and China are other countries where data science has maintained its momentum.<\/li>\n    <li>While on the same aspect, the no. of respondents from Nigeria has slowd pacing up sice 2019 <\/l>\n<\/ol>\n\n<\/div>\n\n<br>\n    \n    \n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; India and USA being large democracies, and a wide number of IT companies, many are getting in to Data Science filed. However, since 2020, the growth has slowed down because of panamic and many losing their day to day jobs. We can see that before pandamic, the percentage increase in the respondents were quite high.(~4% female and ~ % male in india)\n<\/div>\n","9ae9b3bc":"# CONCLUSION","949d169a":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.1, 5.2 and Table 5.1: <\/b><br>\n\n<ol>This following trend observed about programing languages:\n    <li>since 2018, python has gained the attention of all levels of experienced category(beginners to ML veterns) of people in the data science induatry.<\/li>\n    <li>Python is followed by SQL(data base query language). This could be directly related to Database Engineer\/ DBA jobs in all experience category as found in section 4.1 above <\/li>\n    <li>R is another language in statistical data analysis found amoung students and industy professionals working in financial Data Analysis fiels,Medicine<\/li>\n    <li><span style=\"color:red\">C-C++ is another language that has gained serious adoption in data science field<\/span><\/li>\n    <li>C-C++ could be usesd mainly in the embedded fiels, now a days ML\/AI is used in embeeded (Hardware manufacturing , embedded software).<\/li>\n    <li>swift and julia is also picking up the momentum in datascience as these offer various Data analysis features in certain verticals <\/li>\n    <li>Also, kagglers on all level of experiece recmmend Python, R and SQL to be learnt in the data science field.<\/li>\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; Python, SQl ,R, C++ go-to languages in the data science field\n\n***","e252788c":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 4.1.: <\/b><br>\n\n<ol>\n    <li>year 2020, COVID hit the world and the world economies faced unprecedented challenges,many perished, lost jobs, faced monetory issues etc... .<\/li>\n    <li>In the year 2020, as almost all the verticals of the economies were hit, even data jobs saw a decending trend, many paid less for almost all qualification.<\/li>\n    <li>It can be nticed from the above fig that, a person with 20+ years of experience was paid equal to someone with less than 20 years. for any level of qualification<\/li>\n    <li>Also someone with 5-10 year experience was paid similar to 3-5 years<\/li>\n    <li>However In the year 2021, things are moving towards the brighter side, embracing the new normal, people with higher experience are valued more for any level of their qulaification <\/li>\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; 2021, ML\/ DS is regaining its momentum as jobs are opening up, economies are reviving , so are the salaries becoming higher from the previous year.\n<\/div>\n\n***","b7a0a5d3":"# 8.'THE COUNTRY SIDE DATA'-INDIA & USA ","39a122fb":"# 7. What's Data Science Munching In Companies?\n    \n## <center> 7.1 What's data Science Munching In Companies:a)Data Science activities? <\/center>","209beb05":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.10 a) and fig 7.10b) Fig 7.10 c): <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf This following trend observed Auto ML features being used and the tools used for the same \n<ol><b>Beginners<\/b>\ud83c\udfaf\nBeginners mainly use following topics of Auto ML\n    <li>  Model Selection <\/li>\n    <li>  Data Augmentation<\/li>\n    <li> Feature Engineering<\/li>\nTools used for Auto ML are as of 2021\n    <li> Google cloud Auto ML -increasing since 2019 <\/li>\n    <li> Azure Auto ML<\/li>\n    <li> Amazon Auto ML<\/li>\n    <li> Databricks Auto ML<\/li>\n   Which AutoML tools' usage is gaining its momentum top 3 is mentioned here. kindly hover over the graph to to check further details\n    <li> Google cloud Auto ML <\/li>\n    <li>H20Driverless Ai<\/li>\n    <li>Databricks AutoML<\/li>\n<\/ol>\n<ol><b>Experienced<\/b>\ud83c\udfaf\nExperienced  mainly use following topics of Auto ML\n    <li>  Hyperparameter tuning <\/li>\n    <li>  Full ML piprlines<\/li>\n    <li> Feature Engineering<\/li>\nTools used for Auto ML are as of 2021\n    <li> Google cloud Auto ML -increasing since 2019 <\/li>\n    <li> Azure Auto ML<\/li>\n    <li> Amazon Auto ML<\/li>\n    <li> Databricks Auto ML<\/li>\n    <li> DataRobot Auto ML<\/li>\n   Which AutoML tools' usage is gaining its momentum top 3 is mentioned here. kindly hover over the graph to to check further details\n    <li> Google cloud Auto ML <\/li>\n    <li>H20Driverless Ai<\/li>\n    <li>Databricks AutoML<\/li>\n    <li> DataRobot Auto ML<\/li>\n<\/ol>\n\n<\/div>\n\n<br>\n\n***","2c0b8985":"## <center> 5.8 The armories of Data Scientists:g)Machine learning libraries\ud83d\udcdd? <\/center>","5c0dcca3":"## <center> 7.9 What's data Science Munching In Companies?:i)Do Kagglers use to articulate taste of data? <\/center>","9bc2d79d":"# 4.Data Science As a Profession\n## <center> 4.1 Kagglers:How much are you paid \ud83d\udcb2 in the DS\/ML industry with experience? <\/center>","25573fc1":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.5 a) 7.5 b): <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf This following trend observed about the Cloud computing products used \n<ol><b>Beginners<\/b>\ud83c\udfaf\n   As of 2021 following points noted\n    <li> ~31% don't use any cloud computing products, 0.8% percentage increase since 2019 <\/li>\n    <li> ~28% use Goodle computing engine<\/li>\n    <li> ~21% use Azure Virtual Machines<\/li>\n    <li> ~18% use AWS (EC2) <\/li>\n    <li> There is a 0.4% increase in use of other computing Products since 2020, may be due to factors like cost cutting etc.<\/li>\n<\/ol>\n<ol><b>Experienced<\/b>\n    <li> ~35% use AWS(EC2), usage decreased since 2020(.2%% downfall)<\/li>\n    <li> ~27% use Goodle computing platform (GCP), usage decreased since 2020(.1%% downfall) <\/li>\n    <li> ~21% use Azure<\/li>\n    <li> ~19% use None<\/li>\n<\/ol>\n\n\n\n<\/div>\n\n<br>\n\n***","82dd4739":"<div style=\"font-size:15px; font-family:verdana;\"><b>A section wise conclusion have been made in the afore analysis.Just summing in a few points below <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>\n    <li>The state of gender diversity in kaggle has not improved since last survey. <\/li>\n     <li>The repreesentatives of Kaggle are mostly from India and USA.<\/li>\n     <li>The various vertical of the socities have acknowledged the potential of data, thus trying to lay their mark using it.<\/li>\n     <li>There is an overall growth in the usage and investment in ML strategies and Cloud Computing Technologies.<\/li>\n     <li>Countries like India and USA , who are the forefront runners in  this field have been tryng to improve the state of ML using advanced Methods and Features.<\/li>\n \n       \n<\/ol>\n\n<\/div>\n<br>\n\n***","a498ed3b":"## <center> 5.3 The armories of Data Scientists:B)the Development environment (IDEs)\ud83d\udcdd? <\/center>","de373748":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.9 a) and fig 7.9b): <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf This following trend observed about BI tools used \n<ol><b>Beginners<\/b>\ud83c\udfaf\n   As of 2021 following points noted about BI tools regularly useed by beginner respondents\n    <li> ~30% don't use any BI tools <\/li>\n    <li> ~23% use Tableau<\/li>\n    <li> ~20% use Microsoft Power BI<\/li>\n    <li> ~7.6%  use Google data studio <\/li>\n    <li> ~4% use Tableau CRM<\/li>\n   The BI tools often used:\n    <li>Tableau ~38.5%<\/li>\n    <li>Microsoft Power BI ~38.5%<\/li>\n    <li>Google Data Studio ~38.5%<\/li>\n<\/ol>\n<ol><b>Experienced<\/b>\n  Regularly used BI tools:\n    <li> ~38.5% use Microsoft Power BI<\/li>\n    <li> ~36% use Tableau<\/li>\n    <li> ~10%  use Google data studio <\/li>\n    <li> ~10%  use Amazon Quick Studio <\/li>\n  Often Used \n    <li> ~40% use Microsoft Power BI<\/li>\n    <li> ~33% use Tableau<\/li>\n    <li> ~13% use Google Data Studio<\/li>\n    <li> ~6% use Qlik<\/li>\n<\/ol>\n<\/ol>\n<\/div>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf Most preferred BI tools in the data science community are Microsoft Power BI , Tableau. \n<br>\n\n***","fed70263":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.3 and Table 5.3: <\/b><br>\n\n<ol>This following trend observed about IDEs\n    <li>since 2018,Jupter,Visual Studio\/Visual Studio Code,Pycharm are the top 3 prefered IDEs by data science individuals..<\/li>\n    <li>RStudio is also in the top 4 as R is also laguage that is used in data analysis(students\/academics\/biomedical analysis) <\/li>\n    <li>Jupyter is highly used in 2021 as compared to VSCode<\/li>\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; Jupyteer, Visual Studio Code, R Studio are popular amonf data scientists\n\n***","d1b7ce47":"## <center> 6.4 Lets Find Out the Whereabouts of Data Scientists:d)Does the company incorporate Data Job responsibilities? <\/center>","dc86cea0":"## <center> 8.5 India & USA:d)What is the state of AutoML in USA & India? <\/center>","9406d7a3":"# PREFACE","1cc4a0d5":"## <center> 4.3 Kagglers: Which country pay higher \ud83d\udcb2salary wrt experience in the DS\/ML industry? <\/center>","e48f5017":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 4.2. and Table 4.2: <\/b><br>\nAll the mappings for abbrevations are in the JOB_TITLE_MAP -mapping section (1)\n<ol>This following trend observed since 2020:\n    <li>In the year 2020, every job title for all the experience paid less than previous years.<\/li>\n    <li> For a fresh graduate or less than 1 year experience , Database Analyst, Product\/project manager, Developer Advocate, Business analyst are the ones which will earn more<\/li>\n    <li>For 1-3,3-5 year experience, Product manager \/ Business analyst\/ Data Scientist are the hottest jobs with higher payment.<\/li>\n    <li>Once enter 5-10,10-20 and above the most demaded job titles are Product manager, Data Engineer, Data Scientist, Statistian <\/li>\n    <li>Also someone with 5-10 year experience was paid similar to 3-5 years<\/li>\n    <li>However In the year 2021, things are moving towards the brighter side, embracing the new normal, people with higher experience are valued more and hence paid accordingly <\/li>\n    <li>Not to notice less, every Job title with experience gets paid higher. Ex. PM with 1-3 year experience are paid less as compared to the one with 3-5years. This could be related to job responsibilities!<\/li>\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; 2021, ML\/ DS Job Title salaris getting back to their numbers in the previous year(2019) or even higher for all the experience levels.\n\n***\n","413dac3d":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.13: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>This following trend observed platformns mainly used by Kagglers to improve their knowledge\n    <li> Coursera<\/li>\n    <li>  Kaggle <\/li>\n    <li> Udemy<\/li>\n    <li> Datacamp<\/li>\n  \n<\/ol>\n\n<\/div>\n<br>\n\n***","e483c78d":"## <center> 5.9 The armories of Data Scientists:g)Machine learning Algorithms? <\/center>","dfad3ce8":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.11: <\/b><br>\n\n<ol>This following trend observed about NLP methods used regularly\n    <li> The top to bottom oder of NLP methods mainly used by   beginner kagglers is 'Word embeddings\/vectors', 'Encoder-decorder models','Transformer language models'<\/li>\n    <li> Also many of the begginers(22%) don't use NLP methods.<\/li>\n    <li>Among the Expeienced, the top to bottom order is 'Word embeddings\/vectors','Transformer language models', 'Encoder-decorder models','Contextualized embeddings'<\/li>\n    <li> 'Word embeddings\/vectors' is showing anegative curve among both experienced and beginners. While,'Transformer language models' is on a rising usage. <\/li>\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; 'Transformer language models' is gaing usage at a very high rate since 2019.\n\n***","e5816563":"## <center> 5.5 The armories of Data Scientists:d)Any Computing Platform\ud83d\udcdd? <\/center>","ccaec7f4":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.7 a) and fig 7.7b): <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf This following trend observed about Managed ML products used \n<ol><b>Beginners<\/b>\ud83c\udfaf\n   As of 2021 following points noted\n    <li> ~65.74% don't use any managed ML products <\/li>\n    <li> ~6.5% Amazon Sagemaker products<\/li>\n    <li> ~6.5% Azure Machine Learning Service<\/li>\n    <li> ~7.87% use Google Cloud Vertex<\/li>\n   \n<\/ol>\n<ol><b>Experienced<\/b>\n    <li> ~60% don't use any managed ML products<\/li>\n    <li> ~10% Amazon Sagemaker products<\/li>\n    <li> ~9.2% Azure Machine Learning Service<\/li>\n    <li> ~8% use Google Cloud Vertex<\/li>\n<\/ol>\n<\/div>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf It worth to note that in the yrar 2021 many have opted to use 'other' which is not in the list of \"Google\", \"Amazon\",\"Azure\",\"Rapidminer\",\"Alteryx\" \n<br>\n\n***","1a40f730":"## <center> 7.4 What's data Science Munching In Companies?:d)Which cloud is Heaven? <\/center>","bbff698f":"## <center> 7.2 What's data Science Munching In Companies?:b)How much does it take to setup the resource for data pipelines locally and on the cloud? <\/center>","e50c7fab":"## <center> 5.1 The armories of Data Scientists:A) The programming languages they use \ud83d\udcdd? <\/center>","4d01fdbf":"# REFERENCES\n\n<div style=\"font-size:15px; font-family:verdana;\"><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol> \n    <li><a href=\"https:\/\/www.kaggle.com\/c\/kaggle-survey-2021\" target=\"_blank\"> Kaggle 2021 Survey<\/a><\/li>\n    <li><a href=\"https:\/\/pandas.pydata.org\/\" target=\"_blank\"> Pandas<\/a><\/li>\n    <li><a href=\"https:\/\/matplotlib.org\/\" target=\"_blank\"> Matplotlib<\/a><\/li>\n    <li><a  href=\"https:\/\/stackoverflow.com\/questions\/\" target=\"_blank\"> stackoverflow questions<\/a><\/li>\n    <li><a  href=\"https:\/\/community.plotly.com\/\" target=\"_blank\"> plotly community<\/a><\/li>\n<\/ol>\n\n<\/div>\n<br>\n\n***\n\n","89aea77d":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.6a) Fig 5.6b): <\/b><br>\n\n<ol>This following trend observed about Specialized hardware\n    <li>Beginners(none,< 1 year expeirnce)>) mostly don't use any specialized  hardware.GPUs adoption after seeing a growth in 2020 has reduced in 2021. TPUs like Google TPUs on seeing increased usageove the Years <\/li>\n    <li>The Use of TPUs,GPUs among experienced coders and senior data scientists are also seen.<\/li>\n    <li>AWS Chips is also gaining traction since 2020.<\/li>\n    <li>Also its a surprise that over the last 3 years, ony rougly around 15% of the experienced data scientists have used TPus 2-5 times and the majprity have not used at all.(60%)<\/li>\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; GPUs, TPUs are becooming prudent among data science indiviuals, AWS chips also gaining attention.\n\n***","382f3ad1":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.12: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfafMany in the experienced and beginneer segment don't share their public code.  \n<ol>This following trend observed platformns used by Kagglers to share their public codes ((top 3))\n    <li> Github<\/li>\n    <li>  Kaggle <\/li>\n    <li> Colab<\/li>\n  \n<\/ol>\n\n<\/div>\n<br>\n\n***","8f185028":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.15: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>Clearly Kagglers consider following media sources as favorables \n    <li> Kaggle<\/li>\n    <li>  Youtube <\/li>\n    <li> Social Media<\/li>\n    <li> Forums<\/li>\n    <li> Blogs<\/li>\n    <li> Journals<\/li>    \n<\/ol>\n\n<\/div>\n<br>\n\n***","152864b0":"# DATA LOADING","c0b75fd2":"## <center> 3.5 Kagglers get introduced: \ud83d\udc68\u200d\ud83d\udcbb How much is their pocket filled \ud83d\udcb2 Are Males\ud83d\udc68 and females\ud83d\udc69 equally Wrt Experience? <\/center>","4297590a":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.7a) Fig 5.7 b): <\/b><br>\n\n<ol>This following trend observed about Data Visualization libraries\n    <li>Beginners(none,< 1 year expeirnce)>) are familiar with matplotlib, seaborn, Plotly and ggplot.<\/li>\n    <li>For experienced, they see to use matplotlib, ggplot,plotly,geoplotlib,Altar.<\/li>\n    <li>Beginners seem to have a penchant for D3.js library, more experierd not yet <\/li>\n\n\n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; Matplotlib, seaborn,plotly,ggplot go to Data visualization libraries.\n\n***","f57595b8":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 5.8a) Fig 5.8b): <\/b><br>\n\n<ol>This following trend observed about Data Visualization libraries\n    <li>Beginners(none,< 1 year expeirnce)>) 60% have less than a year of experience in ML frameworks, while 32% have no experience, 6% have 1-2Y experience.<\/li>\n    <li>Beginners are also getting more familiar with frameworks which are very well used in industries like tensorflow, Keras, Pytorch, SKlearn among others. <\/li>\n    <li>Experienced Datascientists also have experience in the ML frameworks corresponding to their respective experience level. <\/li>\n<\/ol>\n\n<\/div>\n<br>\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; SKLeaern, tensorflow, Pytorch, Xgboost,Keras- go to frameworks in ML\n\n***","4ef9d850":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.1,7.2: <\/b><br>\n \ud83d\udccc\ud83d\udc40 This following trend observed about the job responsibilities of kagglers at work\n\n<ol><b>Data Analysis<\/b>\n    <li> A mojor part of work on all levels of experiencce is to analyze the data At hand<\/li>\n    <li> Roughly around 22% to 30% of what constitutes their job is involved in data analysis<\/li>\n    <li> As the expererience increase the percentage o responsiblilities that consititutes analysis on data is reduced(~25%)<\/li>\n<\/ol>\n<ol><b>Build Data Infrastructure<\/b>\n    <li>For any data science pipeline, having a good data infrastructure is seminal, to further assert the credibility of ML models <\/li>\n    <li>So ~10-~15% percent of work is in data infrastructuring by all level of data science individuals <\/li>\n\n<\/ol>\n\n<ol><b>Build Prototype<\/b>\n    <li> ML  Prototype development is also important, involves conducting experimnts on different ML models <\/li>\n    <li> choosing the right models that can fit to the requirement of the problem, hhyperparameter tuning etc<\/li>\n    <li> ~10- ~20% of work constitues Building Prototype <\/li>\n\n<\/ol>\n<ol><b>Build ML Service<\/b>\n    <li> After building prototype, How is ML model used also quite important, could be as a service, consumed by various front end applications\/ consumbed by a single application <\/li>\n    <li> ML service are to be deployed in the right infrastructure, serving web applications and mobile apps<\/li>\n    <li> ~7- ~13% of work constitues Building ML Service <\/li>\n\n<\/ol>\n<ol><b>Do research<\/b>\n    <li> Various levels of study are required in data science.<\/li>\n    <li> Could involve new concepts not known to indidual. DS is evolving every day<\/li>\n    <li> ~7- ~12% of work constitues doing ML research <\/li>\n\n<\/ol>\n\n \ud83d\udccc\ud83d\udc40 This following trend observedon Money Spent on ML\/Cloud infrastructure by individual or Team\n\n<ol><b>Beginner<\/b>\n    <li> ~53% kDon't spent much money<\/li>\n    <li> Since covid in 2020, till 2021 ~15% of them started to spend about 1-1000$ on mL\/cloud infrastructure<\/li>\n<\/ol>\n\n<ol><b>Experienced<\/b>\n    <li> ~30% kDon't spent much money<\/li>\n    <li> Since covid in 2020, till 2021 ~21% of them started to spend about 1-1000$ on mL\/cloud infrastructure<\/li>\n<\/ol>\n\n\n<\/div>\n\n<br>\n\n***","2270aa13":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.14: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>This following primary data analysis tools are used by kagglers\n    <li> Basic statistical software<\/li>\n    <li>  Local Development environment <\/li>\n    <li> BI tools<\/li>\n    <li> Cloud Based data software and APIs<\/li>\n  \n<\/ol>\n\n<\/div>\n<br>\n\n***","1e48fc9e":"## <center> 7.6 What's data Science Munching In Companies?:f)what is the make of food tank? <\/center>","4df2b84e":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 3.4. and Table 3.4: <\/b><br>\n\n<ol>\n    <li>Male respondents who has completed College degree has shown a noticable increase in number since 2020.<\/li>\n    <li>Also male male resondents with bachelors, masters and doctor's degree are gaining traction in the data science skills.<\/li>\n    <li>Females with , masters and doctor's degree are also pacing up in the analytical industry.<\/li>\n    <li>Does this indicate that since covid, many companies are indroducing Analytical\/ML pipelines?<\/li>\n\n<\/ol>\n\n<\/div>\n\n<br>\n    \n    \n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; Now that we have undertood that there is gender divide in the kaaggle survey, Let us dig down further in to the pipeline to understand how the data science industry is satiating the mass wrt experience. Are Females and males paid equally?\n<\/div>","be733cf1":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 6.2, 6.3,6.4: <\/b><br>\nThis following trend observed about the companies that kagglers work\n<ol>Behinners & Seasoned Coders\n    <li> Majority of them work in a company were total employee count is about 50.<\/li>\n    <li> Companies with 50-300 also have decent kaggler count<\/li>\n    <li>since 2019 , the undertaking of data science individuals with more than 10,000 employees is also rising<\/li>\n    <li> Also, beginners are found to have been working in companies where data science workloads is distruted among varing number of people. <\/li>\n    <li>As of 2021 20% beginners work in comapnies where they are exploring ML methods, this is the state since 2018<\/li>\n    <li>As of 2021 28% beginners don't have clear idea whether ML feture adoption in the company.<\/li>\n\n<\/ol>\n\n<ol>Experienced data science individuals \n    <li> Are well saught in companies with more number of employees. this proves that data is the new fuel for many industrial segments<\/li>\n    <li> Also, Experienced are found to have been working in companies where data science workloads is distruted among varing number of people. <\/li>\n    <li>  Experienced data scince minds cold purse consultation jobs in STARTUP companies where employee count could be less. <\/li>\n    <li> As the xperience in data job increaase, many are fond to work for well established compannies. <\/li>\n    <li> Whilst being experienced, many cann be advisors, mentors to companies where in data science adoption is in nascent stages. <\/li> \n<\/ol>\n\n<\/div>\n\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc\ud83d\udc40  &nbsp; Data scientists could be found in varying companies and data workload distributions.\n\n***","b4218c7a":"## <center> 5.10. The armories of Data Scientists:h)Computer Vision Methods? <\/center>","3d4ccd2a":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 8.1, Fig 8.2: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>\n    <li> Both in India and USA IT , Academics\/Education, Finance & Accounting are the 3 topmost fields<\/li>\n    <li>State of ML in India, the % of Verticals exploring ML is far greater than Well established ML in any field.<\/li>\n    <li>USA on the other hand has % of well established ML as compared o those still exploring on the field. <\/li>\n    \n<\/ol>\n\n<\/div>\n<br>\n\n***","87d6e9ab":"## <center> 5.6 The armories of Data Scientists:e)Any Specialized Hardware\ud83d\udcdd? <\/center>","7deec9bf":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.3,7.4: <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf This following trend observed about the Cloud computing platforms used and Experience\n<ol><b>Beginners<\/b>\ud83c\udfaf\n    <li> ~30% don't work on cloud computing platforms<\/li>\n    <li> ~20% use Goodle computing platform<\/li>\n    <li> ~16.5% use AWS<\/li>\n    <li> ~14% use AZure, Azure has 2 % increase in usage since 2018<\/li>\n    <li> AWS is voted to be having good user experience followed by GCP.<\/li>\n<\/ol>\n<ol><b>Experienced<\/b>\n    <li> ~30% use AWS, usage decreased since 2018(4% downfall)<\/li>\n    <li> ~23% use Goodle computing platform (GCP),4% increased usage <\/li>\n    <li> ~20% use Azure<\/li>\n    <li> ~16% use None<\/li>\n<\/ol>\n\n\n\n<\/div>\n\n<br>\n\n***","f6749372":"## <center> 3.4 Kagglers get introduced: \ud83d\udc68\u200d\ud83d\udcbb What are their education backgrounds\ud83c\udfc6?  <\/center>","337e1128":"## <center> 5.2 The armories of Data Scientists:The programming languages recommend for future geneerations \ud83d\udcdd? <\/center>","b360ca3c":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 8.5 a), Fig 8.5 b) & Fig 8.5 c): <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf \ud83c\udfaf  \n<ol>\n    <li>The implementation rate of AutoML features in both India and USA is similar. <\/li>\n     <li>Both countries have bee using AutoML products increasily to automate specific ML tasks.<\/li>\n\n       \n<\/ol>\n\n<\/div>\n<br>\n\n***","3bb9a40d":"## <center> 7.15 What's data Science Munching In Companies?:n)Favorite Media Sources? <\/center>","231805f5":"<div style=\"font-size:15px; font-family:verdana;\"><b>Inferences from the above Fig 7.8 a) and fig 7.8b): <\/b><br>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf This following trend observed about Big Dta Products used \n<ol><b>Beginners<\/b>\ud83c\udfaf\n   As of 2021 following points noted\n    <li> ~21% don't use any big data products <\/li>\n    <li> ~20% use MysQl , probably because much ot it is free and open sourse database<\/li>\n    <li> ~9%  use MSSQl, MSSQl express with 10gb is free,so its kind of populr among beginners <\/li>\n    <li> ~9% use PostgresSQl<\/li>\n   \n<\/ol>\n<ol><b>Experienced<\/b>\n    <li> ~20% MySQl because of its opensoure and wide available features<\/li>\n    <li> ~12% PostgrsSQL is gaining popularity because of its edging capabilities over other bigdata products<\/li>\n    <li> ~10% use MSSQL <\/li>\n \n<\/ol>\n<\/div>\n \ud83d\udccc\ud83d\udc40 \ud83c\udfaf Open Source big data products like MySQl, postgressSQl re mostly pervasive among both begnners and expreienced.\n<br>\n\n***"}}