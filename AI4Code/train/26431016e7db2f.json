{"cell_type":{"7b180026":"code","b3db92b1":"code","5edbc532":"code","0c0c2db1":"code","f3ff3f18":"code","8484b08f":"code","e1354a95":"code","8a0f7e78":"code","a4ad612c":"code","cdaf9cad":"code","f63ffe28":"code","50450b97":"code","45868ecf":"code","caaf2ebe":"code","82a2b939":"code","23597f06":"code","307ada96":"code","78aa30a8":"code","c649860e":"code","3a8d78fd":"code","32badb21":"code","fcbf378c":"code","19a920e6":"code","3392df78":"code","b83330df":"code","ddacd491":"code","70b492f1":"code","1286acb4":"code","3c639648":"code","731f4f23":"code","2dc00e7f":"code","78079a66":"code","ce002194":"code","08629757":"code","0b17ba00":"code","08f56522":"code","5cfca59d":"code","3a0fe1f3":"markdown","269feb06":"markdown","ae722417":"markdown","39985a6d":"markdown","1aa4e1eb":"markdown","69f091ba":"markdown","d8f87b71":"markdown","66f0f572":"markdown","98899e06":"markdown","7497615a":"markdown","a6672952":"markdown","b980943a":"markdown","4c636c57":"markdown","95940f44":"markdown","0c7c2632":"markdown","444caf2e":"markdown","c83be297":"markdown","fd7be81e":"markdown"},"source":{"7b180026":"\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport os\nprint(os.listdir(\"..\/input\"))\n","b3db92b1":"data= pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata.head()","5edbc532":"data.info()","0c0c2db1":"data.isna().sum()\n#we have missing values ","f3ff3f18":"#replacing missing datas with mean\nfrom sklearn.impute import SimpleImputer \nimputer = SimpleImputer(missing_values=np.nan, \n                  strategy=\"mean\") \nimputer = imputer.fit(data.iloc[:,5:7]) \ndata.iloc[:,5:7] = imputer.transform(data.iloc[:,5:7])\ndata.head()","8484b08f":"#let's check\ndata.isna().sum()\n# so there are no missing values now\n#there are still some missing values in cabin column but cabin might not have have that effect on survival","e1354a95":"sns.scatterplot(data[\"Fare\"],data[\"Age\"],color='Green')","8a0f7e78":"sns.factorplot(x=\"Sex\",col=\"Survived\", data=data , kind=\"count\",size=7, aspect=.7,palette=['red','green'])","a4ad612c":"sns.catplot(x=\"Survived\", hue=\"SibSp\", col = 'Sex',kind=\"count\", data=data,height=7);\nsns.catplot(x=\"Survived\", hue=\"Parch\", col = 'Sex', kind=\"count\", data=data,height=7);","cdaf9cad":"surv =data.groupby('Survived').size() #this gives us total passengers survived and died\nfig = plt.figure()\nax1 = fig.add_axes([0, 0, 1, 1], aspect=1)\nax1.pie(surv.values,labels=['Died','Survived'],startangle=90,autopct='%1.1f%%')\nplt.title('Survivors to Casualties Ratio',bbox={'facecolor':'0.8', 'pad':5})\nplt.show() \n    ","f63ffe28":"data[\"Sex\"].replace([\"male\",\"female\"],[0,1],inplace=True)\ndata[\"Embarked\"].replace([\"S\",\"C\",\"Q\"],[0,1,2],inplace=True)\ndata.head()","50450b97":"data.isna().sum() #this says that there are still null values ... we had previously handled missing values but there are still null values left...","45868ecf":"#Checking null values and fill 0 at the place of NaN.\ndata.isnull().sum()\n\ndata.fillna(0,inplace=True)\ndata.isna().sum()","caaf2ebe":"data.drop(['PassengerId','Name','Ticket','Cabin'], axis=1, inplace=True)\n#these are unnecessary columns\ndata.head()","82a2b939":"data['Age Band']=0\n\ndata.loc[data['Age']<=16,'Age Band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32), 'Age Band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age Band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age Band']=3\ndata.loc[data['Age']>64,'Age Band']=4\n\ndata.head(10)","23597f06":"data['Age Band'].value_counts()","307ada96":"data[\"Fare Range\"]= pd.qcut(data[\"Fare\"],4)\n\ndata[\"Fare Range\"].value_counts()","78aa30a8":"data[\"Fare_grp\"]=0\ndata.loc[data[\"Fare\"]<=7.91,\"Fare_grp\"]=0\ndata.loc[(data[\"Fare\"]>7.91)&(data[\"Fare\"]<=14.454),\"Fare_grp\"]=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_grp']=2\ndata.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_grp']=3\ndata.head()","c649860e":"data['Fare_grp'].value_counts()","3a8d78fd":"data.drop(['Fare Range','Age'], axis=1, inplace=True)","32badb21":"#Splitting the dataset\nX=data[data.columns[1:]]\nY=data['Survived']\nprint(\"y data=\",Y.head())\nprint(\"---------------------------------------------------------------\")\nprint(\"x data\",X.head())","fcbf378c":"X.isna().sum()","19a920e6":"#Importing ML libraries.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.3, random_state=0)","3392df78":"\nfrom sklearn.linear_model import LogisticRegression\nlogr= LogisticRegression()\nlogr.fit(X_train,Y_train)\nY_pred=logr.predict(X_test)\n#MAKING CONFUSION MATRIX\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix( Y_pred ,Y_test )\nscore = metrics.accuracy_score(Y_pred,Y_test)\nprint(\"accuracy=\",score)\nprint(\"Confusion matrix = \",cm)#(contain both correct and incorrect predictions)\n#here correct predictions are 154+10 and incorrect ones are 14+90","b83330df":"from sklearn.neighbors import KNeighborsClassifier\nmodel_knn= KNeighborsClassifier()\nmodel_knn.fit(X_train,Y_train)\nY_knn=model_knn.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix( Y_knn,Y_test)\nscore = metrics.accuracy_score(Y_knn,Y_test)\nprint(\"accuracy=\",score)\nprint(\"Confusion matrix = \",cm)","ddacd491":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion='entropy' , random_state = 0)\nclassifier.fit(X_train,Y_train)\nY_dt = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test , Y_dt)\nscore = metrics.accuracy_score(Y_dt,Y_test)\nprint(\"accuracy=\",score)\nprint(\"Confusion matrix = \",cm)","70b492f1":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10 , criterion = 'entropy' , random_state=0) \nclassifier.fit(X_train,Y_train)\nY_rf = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test , Y_rf)\nscore = metrics.accuracy_score(Y_rf,Y_test)\nprint(\"accuracy=\",score)\nprint(\"Confusion matrix = \",cm)","1286acb4":"data_train= pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_sur = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","3c639648":"data_train.drop(['Name','Ticket','Cabin'], axis=1, inplace=True)\ndata_test.drop(['Name','Ticket','Cabin'], axis=1, inplace=True)","731f4f23":"#categorical to numerical values\ndata_train[\"Sex\"].replace([\"male\",\"female\"],[0,1],inplace=True)\ndata_train[\"Embarked\"].replace([\"S\",\"C\",\"Q\"],[0,1,2],inplace=True)\ndata_test[\"Sex\"].replace([\"male\",\"female\"],[0,1],inplace=True)\ndata_test[\"Embarked\"].replace([\"S\",\"C\",\"Q\"],[0,1,2],inplace=True)","2dc00e7f":"#replacing missing datas with mean\nfrom sklearn.impute import SimpleImputer \nimputer = SimpleImputer(missing_values=np.nan, \n                  strategy=\"mean\") \nimputer = imputer.fit(data_train.iloc[:,5:7]) \ndata_train.iloc[:,5:7] = imputer.transform(data_train.iloc[:,5:7])\nprint(data_train.head())\nprint(\"-------------------------------------------------------------------------------------\")\nfrom sklearn.impute import SimpleImputer \nimputer = SimpleImputer(missing_values=np.nan, \n                  strategy=\"mean\") \nimputer = imputer.fit(data_test.iloc[:,5:7]) \ndata_test.iloc[:,5:7] = imputer.transform(data_test.iloc[:,5:7])\nprint(data_test.head())","78079a66":"#Checking null values and fill 0 at the place of NaN.\ndata_train.isnull().sum()\n\ndata_train.fillna(0,inplace=True)\nprint(\"TRAINING DATA\")\nprint(data_train.isna().sum())\nprint(\"----------------------------------------------------------\")\ndata_test.isnull().sum()\n\ndata_test.fillna(0,inplace=True)\nprint(\"TEST DATA\")\nprint(data_test.isna().sum())","ce002194":"#Splitting the dataset\nX= data_train.drop(['Survived'], axis=1)\n#X=data_train[data_train.columns[2:]]\nY=data_train['Survived']\nX.head()","08629757":"#Importing ML libraries.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.3, random_state=0)","0b17ba00":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10 , criterion = 'entropy' , random_state=0) \nclassifier.fit(X_train,Y_train)\nY_rf = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test , Y_rf)\nscore = metrics.accuracy_score(Y_rf,Y_test)\nprint(\"accuracy=\",score)\nprint(\"Confusion matrix = \",cm)","08f56522":"final_prediction = classifier.predict(data_test)","5cfca59d":"output = pd.DataFrame({\"PassengerId\":data_test.PassengerId , \"Survived\" : final_prediction})\noutput.to_csv(\"..\/submission_\"  + \".csv\",index = False)","3a0fe1f3":"**we found that RANDOM FOREST has best accuracy.. now predicting score w.r.f test dataset**","269feb06":"**repeating same steps for test**","ae722417":"**since there are so many ages and fares we convert them into several groups**","39985a6d":"**here we can conclude the girls survived more **","1aa4e1eb":"#  **Look for some insights**","69f091ba":"# **Handling the missing values**","d8f87b71":"**Now, this gives a very good insight about family and gender relations. Female passengers were helped by there family males so they have higher survival rate in case if there family is onboard. But it was just the opposite for males as they were not prefered and thus most of them died saving there families**","66f0f572":"# Testing on different models","98899e06":"## SUBMISSION FOR KAGGLE COMPETETION","7497615a":"**Comparision of both datas after preprocessing**","a6672952":"**LOGISTIC REGRESSION**","b980943a":"### TESTING ON TEST DATASET","4c636c57":"**WE CAN CONCLUDE THAT RANDOM FOREST HAS HIGHEST ACCURACY**","95940f44":"**There are some categorical values we need to convert them into numerical values**","0c7c2632":"**DECISION TREE CLASSIFIER**","444caf2e":"# **check for missing values**","c83be297":" **KNN**","fd7be81e":"**RANDOM FOREST CLASSIFIER**"}}