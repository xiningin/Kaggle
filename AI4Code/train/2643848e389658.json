{"cell_type":{"630ac343":"code","63419d77":"code","f7972d56":"code","c9a8f5ee":"code","1719cfe4":"code","23dccff9":"code","dae884fd":"code","579ea998":"code","53749a92":"code","354b542a":"code","cfe549fd":"code","fa56ae41":"code","6d1f7f2b":"code","0e9fea2f":"code","91570c35":"code","765455a5":"code","a1456c8e":"code","2b3519e8":"code","55da134b":"code","88ef74cf":"code","8ee4bcc1":"code","bb4e2147":"code","d8f2080d":"code","3b8867bd":"code","52a770d5":"code","f63891ab":"code","4077f10d":"code","5223b551":"code","24f180ea":"code","9f5a1c05":"code","91e4dbfd":"code","facc53bd":"code","63808686":"code","06a33fc6":"code","9caf569a":"code","b8f77dfd":"code","ab576ba8":"code","a39c999b":"code","9bfc6b9c":"code","e8185f12":"code","ead44d59":"code","68534855":"code","de27a338":"code","14a0cc56":"code","59286a5d":"markdown","f2deb119":"markdown","c87cdf05":"markdown","1049ff48":"markdown","274a1fee":"markdown","275dd525":"markdown","eb26ff98":"markdown","969c83ec":"markdown","0bf6a8f6":"markdown","563693e3":"markdown","82ec3c2c":"markdown","e78f093b":"markdown","d2c06101":"markdown","fc704bcc":"markdown","169ab037":"markdown","0c13854a":"markdown","7847e5f4":"markdown","e1e05c8b":"markdown","18eb0ba8":"markdown","a50c7a12":"markdown","9568594f":"markdown","e5f385cd":"markdown","5c096758":"markdown","455db4fd":"markdown","d61c4cba":"markdown","06712008":"markdown","6117e73b":"markdown","165654a0":"markdown","874e4e10":"markdown","fc10b351":"markdown","c3867bea":"markdown","2b7e7ff1":"markdown","e5f44064":"markdown","68caf9d8":"markdown","86530001":"markdown","5bbd58c4":"markdown","8d2df148":"markdown","094a9b05":"markdown"},"source":{"630ac343":"import pandas as pd\nfrom time import time\nimport datetime\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import  init_notebook_mode\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import confusion_matrix\nimport colorlover as cl\nfrom tqdm import tqdm_notebook as tqdm\nsns.set(rc={'figure.figsize':(11.7,8.27)})\ninit_notebook_mode(connected=True)","63419d77":"train_labels_df = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv\")\ntrain_df = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/train.csv\")\nspecs_df = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/specs.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv\")","f7972d56":"print (\"In train dataset we have total of \" + str(train_df['installation_id'].nunique()) + \" unique Installation ID\")\nprint (\"In test dataset we have total of \" + str(test_df['installation_id'].nunique()) + \" unique Installation ID\")","c9a8f5ee":"train_df.head()","1719cfe4":"specs_df.head()","23dccff9":"train_labels_df.head()","dae884fd":"temp_df = train_labels_df.accuracy_group.value_counts(normalize = True) *100\ntemp_df = temp_df.round(2)\ntext = [str(x) + \"%\" for x in temp_df.values]\nfig = go.Figure(data = go.Bar(x = temp_df.index,y = temp_df.values, text = text,textposition='auto'))\nfig.update_traces(marker_color='#D95219', marker_line_color='#D95219',marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title={'text': \"Percentage of accuracy group\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'} )\nfig.show()","579ea998":"temp_df = train_df['world'].value_counts(normalize = True) * 100\ntemp_df = temp_df.round(2)\ntext = [str(x) + \"%\" for x in temp_df.values]\nfig = go.Figure(data = go.Bar(x = temp_df.values,y = temp_df.index, text = text,textposition='auto',orientation='h'))\nfig.update_traces(marker_color='#611F8D', marker_line_color='#611F8D',marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title={'text': \"Percentage of World\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'} )\nfig.show(title = \"Percentage of world\")","53749a92":"temp_df = train_df.groupby('world')['type'].value_counts(normalize = True).reset_index(name=\"percentage\")\ntemp_df['percentage'] = temp_df['percentage'] *100\ntemp_df = temp_df.round(2)\ndata = []\ntype_ = temp_df['type'].unique()\ncolors = [x.replace(\")\",\"\").replace(\"rgb(\",\"\") for x in cl.scales['4']['qual']['Paired']]\ncount = 0\nfor i in type_:\n\n    text = [str(x) + \"%\" for x in temp_df[temp_df['type'] == i]['percentage'].values]\n    data.append(go.Bar(name = i, x =temp_df[temp_df['type'] == i]['world'].values,text = text,textposition='auto',\n                      y =  temp_df[temp_df['type'] == i]['percentage'].values,marker=dict(\n        color='rgba(' + colors[count] + ',0.6)',\n        line=dict(color='rgba(' + colors[count] + ',1.0)', width=1)\n    )))\n    count = count + 1\nfig = go.Figure(data=data)\nfig.update_layout(barmode='stack')\nfig.update_layout(title={'text': \"Percentage of media types in each world\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'} )\nfig.show()\n","354b542a":"# lets see the fav title\ntemp_df = train_df['title'].str.replace(\"\\(Activity\\)\",\"\").replace(\"\\(Assessment\\)\",\"\")\ntext = ' ' .join(val for val in temp_df)\nwordcloud = WordCloud(width=1600, height=800, stopwords = {'None','etc','and','other'}).generate(text)\nplt.figure(figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","cfe549fd":"train_df['timestamp'] = pd.to_datetime(train_df.timestamp)\ntrain_df['date'] = train_df['timestamp'].dt.date\ntrain_df['month'] = train_df['timestamp'].dt.month_name()\ntrain_df['weekday_name'] = train_df['timestamp'].dt.weekday_name\ntrain_df['hour'] = train_df['timestamp'].dt.hour\ntrain_df['minute'] = train_df['timestamp'].dt.minute","fa56ae41":"date_df = train_df.groupby(\"date\")['event_id'].count()\nmonth_df = train_df.groupby(\"month\")['event_id'].count().reset_index(name=\"count\")\nmonth_df['month'] = pd.Categorical(month_df['month'],categories=['December','November','October','September','August','July','June','May','April','March','February','January'],ordered=True)\nmonth_df = month_df.sort_values('month',ascending=False)\n\nweekday_df = train_df.groupby(\"weekday_name\")['event_id'].count().reset_index(name=\"count\")\nweekday_df['weekday'] = pd.Categorical(weekday_df['weekday_name'],categories=['Saturday','Friday','Thursday','Wednesday','Tuesday','Monday','Sunday'],ordered=True)\nweekday_df = weekday_df.sort_values('weekday',ascending=False)\n\nhour_df = train_df.groupby(\"hour\")['event_id'].count()\nminute_df = train_df.groupby(\"minute\")['event_id'].count()\nfig = make_subplots(rows = 5,cols = 1)\n\ninstallation_df = train_df.groupby(\"date\")['installation_id'].count()\nfig.append_trace(go.Scatter(x = minute_df.index, y = minute_df.values, mode = \"lines\", name = \"Minute\"),row=1,col=1)\nfig.append_trace(go.Scatter(x = hour_df.index, y = hour_df.values, mode = \"markers\", name = \"Hour\"),row=2,col=1)\nfig.append_trace(go.Scatter(x = weekday_df['weekday'], y = weekday_df['count'], mode = \"lines+markers\", name = \"Week Day\"),row=3,col=1)\nfig.append_trace(go.Scatter(x = date_df.index, y = date_df.values, mode = \"lines+markers\", name = \"Date\"),row=4,col=1)\nfig.append_trace(go.Scatter(x = month_df['month'], y = month_df['count'], mode = \"lines\", name = \"Month\"),row=5,col=1)\n\n\n\n\nfig.update_layout(height=1000)\nfig.show()","6d1f7f2b":"temp_df = train_labels_df.groupby('title')['accuracy_group'].value_counts(normalize=True).reset_index(name=\"percentage\")\ntemp_df['percentage'] = temp_df['percentage']*100\ntemp_df = temp_df.round(2)\ntemp_df['title'] = temp_df['title'].str.replace(\"\\(Assessment\\)\",\"\")\ncolors = [x.replace(\")\",\"\").replace(\"rgb(\",\"\") for x in cl.scales['4']['qual']['Dark2']]\ndata = []\nfor i in range(4):\n    text = [str(x) + \"%\" for x in temp_df[temp_df['accuracy_group'] == i]['percentage'].values]\n    data.append(go.Bar(name = i, x = temp_df[temp_df['accuracy_group'] == i]['title'].values,\n                       text = text,textposition='auto',\n                      y = temp_df[temp_df['accuracy_group'] == i]['percentage'].values,marker=dict(\n        color='rgba(' + colors[i] + ',0.6)',\n        line=dict(color='rgba(' + colors[i] + ',1.0)', width=1)\n    )))\nfig = go.Figure(data=data)\nfig.update_layout(barmode='stack', title={'text': \"Percentage of accuracy group for different type of Assessment\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'})\nfig.show()","0e9fea2f":"temp_df =  train_labels_df['title'].value_counts()\ndata = go.Bar(x = temp_df.index,y = temp_df.values,text = temp_df.values,  textposition='auto')\nfig = go.Figure(data = data)\nfig.update_traces(marker_color='#C5197D', marker_line_color='#8E0052',marker_line_width=1.5, opacity=0.6)\nfig.update_layout(barmode='stack', title={'text': \"Different typess of Assessment\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'})\nfig.show()","91570c35":"temp_df = train_df[train_df.installation_id==\"0001e90f\"]\ntemp_df","765455a5":"print(\"Out of 1357 rows we have \" + str(temp_df.event_id.nunique()) + \" unique event ID and \" + str(temp_df.game_session.nunique()) + \" unique game session\")","a1456c8e":"temp_df[temp_df.game_session == \"0848ef14a8dc6892\"]","2b3519e8":"specs_df","55da134b":"train_df['game_time_log'] = train_df['game_time'].apply(np.log1p)\ntrain_df = train_df.head(1000000)\n# fig = px.box(train_df, y=\"game_time_log\",x = \"type\",color='month',title={'text': \"Distribution of game_time by type based on month\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},\n#              color_discrete_sequence=cl.scales['3']['qual']['Dark2'])\n# fig.show()\nax = sns.catplot(x=\"type\", y=\"game_time_log\", data=train_df,col=\"month\",kind=\"box\", aspect=.7)","88ef74cf":"# fig = px.box(train_df, y=\"game_time_log\",x = \"type\",color='weekday_name',title={'text': \"Distribution of game_time by type based on weekday\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},\n#              color_discrete_sequence=cl.scales['3']['qual']['Dark2'])\n# fig.show()\nax = sns.catplot(x=\"type\", y=\"game_time_log\", data=train_df,col=\"weekday_name\",kind=\"box\", aspect=.7)","8ee4bcc1":"# fig = px.box(train_df, y=\"game_time_log\",x = \"type\",color='world',title={'text': \"Distribution of game_time by type based on world\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},\n#              color_discrete_sequence=cl.scales['3']['qual']['Dark2'])\n# fig.show()\nplt.figure(figsize=(16, 6))\n\nax = sns.catplot(x=\"type\", y=\"game_time_log\", data=train_df,col=\"world\",kind=\"strip\", aspect=.7)","bb4e2147":"# fig = px.strip(train_df, y=\"game_time_log\",x = \"world\",title={'text': \"Distribution of game_time by world\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},\n#              color_discrete_sequence=cl.scales['3']['qual']['Dark2'])\n# fig.show()\n\nax = sns.catplot(x=\"world\", y=\"game_time_log\", data=train_df,kind=\"strip\", aspect=.7)","d8f2080d":"incorrect = train_labels_df.groupby(['title','accuracy_group'])['num_incorrect'].value_counts().reset_index(name=\"count\")\ncorrect = train_labels_df.groupby(['title','accuracy_group'])['num_correct'].value_counts().reset_index(name=\"count\")","3b8867bd":"px.scatter(incorrect[incorrect['title'] == \"Bird Measurer (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_incorrect\",size = \"count\",hover_name=\"accuracy_group\",title=\"Bird Measurer incorrect answers\")","52a770d5":"px.scatter(correct[correct['title'] == \"Bird Measurer (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_correct\",size = \"count\",hover_name=\"accuracy_group\",title=\"Bird Measurer correct answers\")","f63891ab":"px.scatter(incorrect[incorrect['title'] == \"Mushroom Sorter (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_incorrect\",size = \"count\",hover_name=\"accuracy_group\",title=\"Mushroom Sorter incorrect answers\")","4077f10d":"px.scatter(correct[correct['title'] == \"Mushroom Sorter (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_correct\",size = \"count\",hover_name=\"accuracy_group\",title=\"Mushroom Sorter correct answers\")","5223b551":"px.scatter(incorrect[incorrect['title'] == \"Cauldron Filler (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_incorrect\",size = \"count\",hover_name=\"accuracy_group\",title=\"Cauldron Filler incorrect answers\")","24f180ea":"px.scatter(correct[correct['title'] == \"Cauldron Filler (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_correct\",size = \"count\",hover_name=\"accuracy_group\",title=\"Cauldron Filler correct answers\")","9f5a1c05":"px.scatter(incorrect[incorrect['title'] == \"Chest Sorter (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_incorrect\",size = \"count\",hover_name=\"accuracy_group\",title=\"Chest Sorter incorrect answers\")","91e4dbfd":"px.scatter(correct[correct['title'] == \"Chest Sorter (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_correct\",size = \"count\",hover_name=\"accuracy_group\",title=\"Chest Sorter correct answers\")","facc53bd":"px.scatter(incorrect[incorrect['title'] == \"Cart Balancer (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_incorrect\",size = \"count\",hover_name=\"accuracy_group\",title=\"Cart Balancer incorrect answers\")","63808686":"px.scatter(correct[correct['title'] == \"Cart Balancer (Assessment)\"], x=\"accuracy_group\", y=\"count\",color = \"num_correct\",size = \"count\",hover_name=\"accuracy_group\",title=\"Cart Balancer correct answers\")","06a33fc6":"def qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    O = confusion_matrix(act,pred)\n    O = np.divide(O,np.sum(O))\n    \n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            W[i][j] = ((i-j)**2)\/((n-1)**2)\n            \n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E))\n    \n    num = np.sum(np.multiply(W,O))\n    den = np.sum(np.multiply(W,E))\n        \n    return 1-np.divide(num,den)","9caf569a":"list_of_user_activities = list(set(train_df['title'].unique()).union(set(test_df['title'].unique())))\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n\ntrain_df['title'] = train_df['title'].map(activities_map)\ntest_df['title'] = test_df['title'].map(activities_map)\ntrain_labels_df['title'] = train_labels_df['title'].map(activities_map)\n\nwin_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110\n\ntrain_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\ntest_df['timestamp'] = pd.to_datetime(test_df['timestamp'])","b8f77dfd":"def get_data(user_sample, test_set=False):\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    durations = []\n    for i, session in user_sample.groupby('game_session', sort=False):\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        if test_set == True:\n            second_condition = True\n        else:\n            if len(session)>1:\n                second_condition = True\n            else:\n                second_condition= False\n            \n        if (session_type == 'Assessment') & (second_condition):\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            features = user_activities_count.copy()\n            features['session_title'] = session['title'].iloc[0] \n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            features['accumulated_accuracy'] = accumulated_accuracy\/counter if counter > 0 else 0\n            accuracy = true_attempts\/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n\n            features.update(accuracy_groups)\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group\/counter if counter > 0 else 0\n            features['accumulated_actions'] = accumulated_actions\n            accumulated_accuracy_group += features['accuracy_group']\n            accuracy_groups[features['accuracy_group']] += 1\n            if test_set == True:\n                all_assessments.append(features)\n            else:\n                if true_attempts+false_attempts > 0:\n                    all_assessments.append(features)\n                \n            counter += 1\n\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activity = session_type\n\n    if test_set:\n        return all_assessments[-1] \n    return all_assessments","ab576ba8":"compiled_data = []\ninstallation_id = train_df['installation_id'].nunique()\nfor i, (ins_id, user_sample) in tqdm(enumerate(train_df.groupby('installation_id', sort=False)), total=installation_id):\n    compiled_data += get_data(user_sample)","a39c999b":"new_train = pd.DataFrame(compiled_data)\ndel compiled_data\nnew_train.shape","9bfc6b9c":"new_train.head()","e8185f12":"all_features = [x for x in new_train.columns if x not in ['accuracy_group']]\ncat_features = ['session_title']\nX, y = new_train[all_features], new_train['accuracy_group']\ndel train_df","ead44d59":"clf = CatBoostClassifier(loss_function='MultiClass',task_type=\"CPU\",learning_rate=0.05,iterations=3000,od_type=\"Iter\",early_stopping_rounds=500,random_seed=21)\nclf.fit(X, y, verbose=500, cat_features=cat_features)\ndel X, y","68534855":"new_test = []\nfor ins_id, user_sample in tqdm(test_df.groupby('installation_id', sort=False), total=1000):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    \nX_test = pd.DataFrame(new_test)\ndel test_df","de27a338":"preds = clf.predict(X_test)\ndel X_test","14a0cc56":"submission_df['accuracy_group'] = np.round(preds).astype('int')\nsubmission_df.to_csv('submission.csv', index=None)\nsubmission_df.head()","59286a5d":"game seesion is a total period of time devoted to an activity<br>\nlets take a game session example and see what does it have","f2deb119":"Now we are clear with few things.<br>\nWe will be given information of kids game play data and we need to predict accuracy_group","c87cdf05":"The game mainy have three worlds <br>\nMAGMAPEAK (Capacity)<br>\nCRYSTALCAVES (Weight)<br>\nTREETOPCITY (Height & Length)<br>\n44.3% of kids spent their time in Magmapeak world","1049ff48":"<img src=\"https:\/\/bento.cdn.pbs.org\/hostedbento-prod\/blog\/20170114_200556_794501_pk-channel-16x9.jpeg\"\/>","274a1fee":"# Introduction","275dd525":"The event id belongs to a specific table called specs_df.<br>\nThis table has 368 unique events.<br>\nThese events can be anything line users x,y cordinates or when a tutorial is played or when a player clicks someting","eb26ff98":"Cart Balancer is the most easily solved assessment.<br>\nMost the kids have not finised the Chest Sorter<br>","969c83ec":"# lets build the model","0bf6a8f6":"Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n2: the assessment was solved on the second attempt\n1: the assessment was solved after 3 or more attempts\n0: the assessment was never solved","563693e3":"This session entirely belongs to Sandcastle Builder","82ec3c2c":"# How hard the assignments are?","e78f093b":"Chow Time is the famous activity","d2c06101":"> Kids are more active from 10 AM till midnight.<br>\n> Kids are more active on Friday (Weekend starts)<br>\n> September has more traffic<br>","fc704bcc":"\nspecs.csv<br>\nThis file gives the specification of the various event types.<br>\n\nevent_id - Global unique identifier for the event type. Joins to event_id column in events table.<br>\ninfo - Description of the event.<br>\nargs - JSON formatted string of event arguments. Each argument contains:<br>\nname - Argument name.<br>\ntype - Type of the argument (string, int, number, object, array).<br>\ninfo - Description of the argument.<br>","169ab037":"PBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life. In this challenge, you\u2019ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes.","0c13854a":"Now lets discuss about event id","7847e5f4":"# Let's start with analysis","e1e05c8b":"Few notes.\n* For a given assessment and accuracy group 3 it is clear that count of incorrect is equal to count of correct. Example for Bird Measurer(accuracy group = 3) has 693 count for incorrect and it has 693 count for correct\n* Chest sorter is the most toughest assessment.\n* A kid has attempted 85 times to solve Bird Measure","18eb0ba8":"Please upvote this kernel too https:\/\/www.kaggle.com\/mhviraf\/a-new-baseline-for-dsb-2019-catboost-model<br>\n(Feature engineering code is taken from here) @mhviraf thankyou for this amazing kernel","a50c7a12":"lets understand the data distribution of test set. taking only 1000000 records of the train set<br>\nwe are applying np.log1p to the game time is to understand the skewness to the large value\n\n> for interactive visualization please uncomment the code","9568594f":"# Game time distribution","e5f385cd":"train.csv & test.csv<br>\nThese are the main data files which contain the gameplay events.<br>\n<br>\nevent_id - Randomly generated unique identifier for the event type. Maps to event_id column in specs table.<br>\ngame_session - Randomly generated unique identifier grouping events within a single game or video play session.<br>\ntimestamp - Client-generated datetime<br>\nevent_data - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and game_time; otherwise fields are determined by the event type.<br>\ninstallation_id - Randomly generated unique identifier grouping game sessions within a single installed application instance.<br>\nevent_count - Incremental counter of events within a game session (offset at 1). Extracted from event_data.<br>\nevent_code - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.<br>\ngame_time - Time in milliseconds since the start of the game session. Extracted from event_data.<br>\ntitle - Title of the game or video.<br>\ntype - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.<br>\nworld - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media. Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length\/Height), 'MAGMAPEAK' (Capacity\/Displacement), 'CRYSTALCAVES' (Weight).<br>\n<br>\n<br>","5c096758":"More information about game play can be found in https:\/\/www.kaggle.com\/c\/data-science-bowl-2019\/discussion\/117019#latest-680222","455db4fd":"So from the above three graphs we are sure that Magmapeak is most played world and Sandcastle builder is most famous activity","d61c4cba":"There are totally 4 media types -- activty,game,assessment,clip. lets see how they are distributed in the world","06712008":"50% of the kids finsih the assessment in one go.<br>\n23.91% of kids havent solve the assessment<br>\n3: the assessment was solved on the first attempt<br>\n2: the assessment was solved on the second attempt<br>\n1: the assessment was solved after 3 or more attempts<br>\n0: the assessment was never solved<br>","6117e73b":"The Public Broadcasting Service (PBS) is an American public broadcaster and television program distributor..<br>It is a nonprofit organization and the most prominent provider of educational television programming to public television stations in the United States.<br>\n<b>Subsidiary:<\/b> PBS KIDS, World<br>\n<b>Geographic scope:<\/b> United States<br>\nPBS Kids is the brand for most of the children's programming aired by the Public Broadcasting Service (PBS) in the United States.","165654a0":"train_labels.csv<br>\nThis file demonstrates how to compute the ground truth for the assessments in the training set.<br>","874e4e10":"NONE indicated the start of the app and it has only one media type which is clip<br>\nMAGMAPEAK & TREETOPCITY have more activites<br>\nCrystalcaves have more games<br>","fc10b351":"# What is PBS Kids","c3867bea":"> WIP<br>\n> Please upvote if you find this kernel intresting","2b7e7ff1":"More about QWK https:\/\/www.kaggle.com\/c\/data-science-bowl-2019\/discussion\/114133","e5f44064":"We are gonna use CatBoostClassifier","68caf9d8":"# Lets understand the relationship of installation_id, game_session, event_id","86530001":"# Time based analysis ","5bbd58c4":"Installation_id it is a single app installation instance<br>\nNow lets say the installation ID 0001e90f belongs to Alex\nWe can see that Alex has total of 1357 rows in train data set","8d2df148":"The outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):<br>\n<br>\n3: the assessment was solved on the first attempt<br>\n2: the assessment was solved on the second attempt<br>\n1: the assessment was solved after 3 or more attempts<br>\n0: the assessment was never solved<br>","094a9b05":"lets use catboost "}}