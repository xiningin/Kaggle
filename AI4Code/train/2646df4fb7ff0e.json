{"cell_type":{"4785dd21":"code","ad43616c":"code","03e0010a":"code","6eb0fba2":"code","41a82d8b":"code","28bda5a6":"code","f2597b52":"code","27a2be4d":"code","b29fa745":"code","e9ffb43e":"code","f27d0d31":"code","3ab82d91":"code","82caa159":"code","f0cc2937":"code","6e81702c":"code","f099fd6c":"code","e1622418":"code","79368760":"code","00f89e0a":"code","bdd3e823":"code","198a26ef":"code","11cf3ec9":"code","9265321a":"code","b6173a93":"code","8ff3faf3":"code","107a5a3d":"code","94c57317":"code","c7d8c5ac":"code","aa535a87":"code","7853a017":"code","dee129de":"code","260aa8df":"code","46f9e12d":"markdown","4edc14c1":"markdown","2195b471":"markdown","17716e1a":"markdown","f7300994":"markdown"},"source":{"4785dd21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#pd.set_option(\"display.max_rows\", 999)\n#pd.set_option(\"display.max_columns\", 999)\n#pd.reset_option(\"display.max_rows\")\n#pd.reset_option(\"display.max_columns\")\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad43616c":"# Data loading\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","03e0010a":"# Lets look at 5 rows of train set\ntrain.head()\n","6eb0fba2":"# Lets look at 5 rows of test set\ntest.head()","41a82d8b":"# Target variable\ny = train.pop(\"label\")\n","28bda5a6":"# Lets look at 5 rows of the target variable\ny.head()","f2597b52":"# Unique values and their frequiencies in the target variable\ny.value_counts()","27a2be4d":"# train set has 784 feature(pixels) and 42000 photos, test set has 784 feature and 28000 photos.  \ntrain.shape,y.shape,test.shape","b29fa745":"test.info()","e9ffb43e":"train.info()","f27d0d31":"y.dtype","3ab82d91":"# scale the input values to type float32\n\ntrain = train.astype('float32')\ntest = test.astype('float32')\ny = y.astype('float32')","82caa159":"# scale(normalize) the input values within the interval [0, 1]\ntrain \/= 255\ntest \/= 255","f0cc2937":"# Converting pandas Dataframe to numpy array\n\"\"\"\nKeras models accept three types of inputs:\n\nNumPy arrays, just like Scikit-Learn and many other Python-based libraries. This is a good option if your data fits in memory.\n\nTensorFlow Dataset objects. This is a high-performance option that is more suitable for datasets that do not fit in memory and that are streamed from disk or from a distributed filesystem.\n\nPython generators that yield batches of data (such as custom subclasses of the keras.utils.Sequence class).\n\"\"\"\ntrain = pd.DataFrame.to_numpy(train)\ntest = pd.DataFrame.to_numpy(test)","6e81702c":"# alternative normalization method by using KERAS\n\"\"\"\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\n\nnormalizer = Normalization(axis=-1)\nnormalizer.adapt(train)\n\ntrain = normalizer(train)\ntest = normalizer(test)\nprint(\"var: %.4f\" % np.var(train))\nprint(\"mean: %.4f\" % np.mean(train))\nprint('*'*20)\nprint(\"var: %.4f\" % np.var(test))\nprint(\"mean: %.4f\" % np.mean(test))\n\"\"\"\n\n","f099fd6c":"# Splitting training set into train and dev set \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.33, random_state=42)","e1622418":"# Importing libraries\nfrom keras.models import Sequential              # creates sequential model\nfrom keras.layers.core import Dense, Activation # creates layers and calls activation functions","79368760":"# Builing ANN model with KERAS Sequential\nmodel = keras.Sequential([\n    layers.Dense(64, activation = 'relu', name = 'layer1', input_shape = (X_train.shape[1],)), # layer with 64 nodes and activation function is RELU\n    layers.Dense(64, activation = 'relu', name = 'layer2'),\n    layers.Dense(10, activation = 'softmax', name = 'layer_pred')\n\n])","00f89e0a":"# Shows layers of the model\nmodel.layers","bdd3e823":"# Shows weights of the model (w,b)\nmodel.weights","198a26ef":"# Used to see the content of the model. It gives a summary of the model.\n# Here is the total number of parameters entering the nodes in each layer, which is called params. \n# There is 784 inputs in the first layer,that is, 784 w and 2 b and since there are 2 nodes, the total parameter entered into the nodes = 2 * 784 +2 = 1570\nmodel.summary()","11cf3ec9":"# To add debugging mode to the model, we need to write \"run_eagerly = True\"  in the model loss and metrics\nmodel.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n              loss=keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])","9265321a":"# Y variable has 10 different classes. Therefore we need to represent each values in y as vector. \n# This converst for example  1 to [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.] vector. \n\"\"\"\n# label encoding to y variable\nfrom keras.utils import to_categorical\ny = to_categorical(y, num_classes=10)\n\"\"\"\ny_onehot = tf.one_hot(y, depth=10)\ny_onehot_train = tf.one_hot(y_train, depth=10)\ny_onehot_test = tf.one_hot(y_test, depth=10)","b6173a93":"# model fitting\nmodel.fit(X_train, y_onehot_train,\n          batch_size=100, epochs=10)   #\u00a0epochs = number of iterations","8ff3faf3":"# Model evaluation\ntest_loss, test_acc = model.evaluate(X_test, y_onehot_test)\n","107a5a3d":"# Plot confusion matrix \n\n# Note: This code snippet for confusion-matrix is taken directly from the SKLEARN website.\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=30)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Actual class')\n    plt.xlabel('Predicted class')","94c57317":"from collections import Counter\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred, axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_onehot_test, axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10))","c7d8c5ac":"# Predict () method returns a vector containing predictions for all data set items.\npredictions = model.predict(X_test)","aa535a87":"# We know which class gives the most probability of belonging with Numpy's argmax function, which returns the index of the position containing the highest value of the vector.\nnp.argmax(predictions[9])","7853a017":"# We can use sum to see that all values in a vector are zero. Because these are probability values.\nnp.sum(predictions[11])","dee129de":"# Predict () method returns a vector containing predictions for all data set items.\ntest_result = model.predict(test)","260aa8df":"# Saving the results into the csv file\n\n# Convert one-hot vector to number\nresults = np.argmax(test_result,axis = 1) # this gives us the corresponding y value for the highest probability in the prediction vector e.g. 2 or 3\n\n\nresults = pd.Series(results,name=\"Label\")\n\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"test_submission.csv\",index=False)","46f9e12d":"# Ann Model With KERAS","4edc14c1":"## Prediction dev-set","2195b471":"# Prediction","17716e1a":"## Prediction test-set","f7300994":"# Confusion Matrix"}}