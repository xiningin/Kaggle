{"cell_type":{"caf6479d":"code","a7888434":"code","f46004de":"code","aa9d6063":"code","a281eabc":"code","0ddb4842":"code","aa64b822":"code","e56cb66c":"code","fe21ddc2":"markdown","01f4abfb":"markdown","4c763276":"markdown","f5b5ad28":"markdown","e04639fa":"markdown","c0aba4c9":"markdown","a31d6561":"markdown"},"source":{"caf6479d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npathToData = '..\/input\/lhcb-jet-data-separation2021\/'\n\nimport os\nfor dirname, _, filenames in os.walk(pathToData):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a7888434":"signalData = pd.read_csv(pathToData+\"bjet_train.csv\") # signal has mc_flavour = 5\nbackgroundData = pd.concat([pd.read_csv(pathToData+\"cjet_train.csv\"), \n                            pd.read_csv(pathToData+\"ljet_train.csv\")]) # background has mc_flavour != 5\nprint(\"First of {} signal rows\".format(signalData.shape[0]))\ndisplay(signalData.iloc[0])\nprint(\"First of {} background rows\".format(backgroundData.shape[0]))\ndisplay(backgroundData.iloc[0])","f46004de":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplotCols = list(signalData.columns)\n\nfor i in range(len(plotCols)):\n    print(\"Plotting {}\".format(plotCols[i]))\n    plt.hist(signalData[plotCols[i]],label = \"Sig\")\n    plt.hist(backgroundData[plotCols[i]],label = \"Bkg\")\n    plt.legend()\n    plt.xlabel(plotCols[i])\n    plt.show()\n\n# Note if the plots do not display minimise then maximise the output area below (double arrow button to the top right)","aa9d6063":"# Try fdChi2 as log10, others as linear\nlogCol = ['fdChi2']\nlinCol = ['PT', 'ETA', 'drSvrJet', 'fdrMin', \n          'm', 'mCor', 'mCorErr', 'pt', 'ptSvrJet',\n          'tau', 'ipChi2Sum', 'nTrk', 'nTrkJet'] # Note skip Id as that is not helpful\n# redefine columns as log10(col), so ranges are more similar between variables\nfor l in logCol:\n    signalData[l] = np.log10(signalData[l])\n    backgroundData[l] = np.log10(backgroundData[l])\n\nnTrainSig = signalData.shape[0]\/\/2 # half the rows for training, half for evaluation\nnTrainBkg = backgroundData.shape[0]\/\/2\n\n# first half as training\nx_data = np.concatenate([signalData[logCol+linCol][:nTrainSig].values,\n                         backgroundData[logCol+linCol][:nTrainBkg].values])\ny_data = np.concatenate([(signalData[\"mc_flavour\"][:nTrainSig]==5).values.astype(np.int),\n                         (backgroundData[\"mc_flavour\"][:nTrainBkg]==5).values.astype(np.int)])\n\n#second half as evaulation\nx_eval = np.concatenate([signalData[logCol+linCol][nTrainSig:].values,\n                         backgroundData[logCol+linCol][nTrainBkg:].values])\ny_eval = np.concatenate([(signalData[\"mc_flavour\"][nTrainSig:]==5).values.astype(np.int),\n                         (backgroundData[\"mc_flavour\"][nTrainBkg:]==5).values.astype(np.int)])","a281eabc":"# Simple 2 layer Keras network:\n# import Keras overall\nimport keras\n# a single NN layer of type \"Dense\" i.e. all inputs connected to all outputs\nfrom keras.layers import Dense\n# The input layer, takes x and starts NN processing\nfrom keras.layers import Input\n# Keras functional methods for defining a NN model\nfrom keras.models import Model\n\n# optimiser to use\nAdam = keras.optimizers.get('Adam') # defaults for optimiser\n\n# define a Functional keras NN model\n# define input layer\nnVal = x_data.shape[1]\ninputs = Input(shape=(nVal,)) \n# input->internal layer with nVal nodes\niLayer = Dense(nVal, activation='relu')(inputs) \n# prev layer -> output (1 node)\noutput = Dense(1, activation='sigmoid')(iLayer) \n# a model is created from connecting all the layers from input to output\nmodel = Model(inputs=inputs, outputs=output)\n# Compiling the model sets up how to optimise it\nmodel.compile(optimizer=Adam, \n              loss='binary_crossentropy', # define what is to be optimised\n              metrics=['accuracy']) # what to store at each step\n\n# run an evaluation before optimisation to see what the random initialisation\n# gave as an output\nscore = model.evaluate(x_eval, y_eval, verbose=1)\nprint('Initial loss:', score[0])\nprint('Inital accuracy:', score[1]) # note random get you to ~60% accuracy if the data are 60% true\n","0ddb4842":"# Choose a batch size \nbatchSize = 4096\n# Rather than run a fixed number of rounds, stop when the output stops improving\nfrom keras.callbacks import EarlyStopping\n# stop training early if after 10 iterations the result has not improved\nearly_stopping = EarlyStopping(monitor=\"loss\", patience=10)\n# Now run the optimisation, taking events from the generator\n# Each epoch is one pass through the data, if not stopped do 20 epochs\nhistory=model.fit(x=x_data,y=y_data,batch_size=batchSize,\n                  verbose=1,\n                  epochs=25,\n                  shuffle=True, # needed as training is all true then all false\n                  callbacks=[early_stopping])\nprint(\"Stopped after \",history.epoch[-1],\" epochs\")","aa64b822":"# Now evaluate the model after training on the evaluation sample, should be better (but could be much improved still)\nscore = model.evaluate(x_eval, y_eval, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","e56cb66c":"testData = pd.read_csv(pathToData+\"competitionData.csv\")\n# apply log10 to columns that need it\nfor l in logCol:\n    testData[l] = np.log10(testData[l])\nx_test = testData[logCol+linCol].values\n\npredMCFloat = model.predict(x_test)\n# predMCFloat is a float: need to convert to an int\npredMC = (predMCFloat>0.5).astype(np.int)\ntestData[\"Prediction1\"] = predMC\n\n# solution to submit\ndisplay(testData[[\"Id\",\"Prediction1\"]]) # display 5 rows\n# write to a csv file for submission\ntestData.to_csv(\"submit.csv.gz\",index=False,columns=[\"Id\",\"Prediction1\"],compression=\"gzip\") # Output a compressed csv file for submission: see \/kaggle\/working to the right","fe21ddc2":"### Now fit the model to half of the training data","01f4abfb":"# Notebook configured for local compressed input","4c763276":"## Plot the data in each column","f5b5ad28":"## Using Keras make a simple one hidden layer model","e04639fa":"## Do some input processing and put data into a numpy array for use in Keras","c0aba4c9":"# Now load data for the competition","a31d6561":"## Load the data"}}