{"cell_type":{"f6c92674":"code","95749c44":"code","71ff1a8f":"code","ec48a09e":"code","da55e8bc":"code","afa0e259":"code","dfef3055":"code","e3f6505d":"code","78098dd4":"code","dcfd56ea":"code","c12fe749":"code","2d12ab54":"code","bfaa396c":"code","efed4ed7":"code","9e6edaaa":"code","57405d4c":"code","a5c8a437":"code","8144fe05":"code","8f23018c":"code","bcb9b1ea":"code","65cc983f":"code","1110baf6":"code","5cfb086e":"code","24bf24ab":"code","08e124ae":"code","631ecd0e":"code","8f34f5e2":"code","e43959c1":"code","fb309ebc":"code","aec7a7b4":"code","f01bf6c8":"markdown","ccbd0abd":"markdown","1f82394d":"markdown","3f8804fe":"markdown","018f370b":"markdown","c46d2f10":"markdown","0a9e67e4":"markdown","2bce7d43":"markdown","195f2324":"markdown","0e53c078":"markdown","2ef8cb11":"markdown","17399acb":"markdown","16cf3efa":"markdown","8864ce89":"markdown","b3a6624e":"markdown","11220233":"markdown"},"source":{"f6c92674":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt # plotting\n%matplotlib inline \nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nmatplotlib.style.use('seaborn')\n\nimport seaborn as sns\nprint(\"seaborn version: {}\". format(sns.__version__))\n\nimport sklearn # machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95749c44":"# read competition data files\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\ndf_all = df_train.append(df_test, ignore_index = True) ","71ff1a8f":"print(\"Size of training data: \",df_train.shape)\ndf_train.head()","ec48a09e":"print(\"Size of testing data: \",df_test.shape)\ndf_test.head()","da55e8bc":"feature_cols = [col for col in df_train.columns if col.startswith(\"feat\")]","afa0e259":"df_train.describe().transpose()\\\n        .drop(\"id\")\\\n        .style.bar(subset=['mean','std'])\\\n        .background_gradient(subset=['max'])","dfef3055":"df_test.describe().transpose()\\\n        .drop(\"id\")\\\n        .style.bar(subset=['mean','std'])\\\n        .background_gradient(subset=['max'])","e3f6505d":"# number of rows with any values below zero\ndisplay(df_train[(df_train.drop([\"target\"],axis=1) < 0).any(1)].shape)\ndf_test[(df_test < 0).any(1)].shape","78098dd4":"# check the target variable\ntarget_absolute = df_train.target.value_counts()\ntarget_percent = df_train.target.value_counts(normalize=True)\ntarget_distribution = pd.DataFrame(data={'absolute':target_absolute, 'percent': target_percent})\ntarget_distribution[['percent']] = target_distribution[['percent']].applymap(lambda x: \"{0:.2f}%\".format(x*100))\ntarget_distribution","dcfd56ea":"plt.figure( figsize=(12,6))\nax= target_distribution['absolute'].sort_values(ascending=True).plot(kind='barh')\nax.set_title(\"Distribution of Target Variable\")\nax.set_xlabel(\"Count\")\n\nrects = ax.patches\nlabels = target_distribution['absolute'].sort_values(ascending=True)\nfor rect, label in zip(rects, labels):\n    width = rect.get_width()\n    ax.text(width +1500 ,rect.get_y() + rect.get_height() \/ 2, label,\n            ha='center', va='center')\nplt.show()","c12fe749":"# check for true duplicates, i.e. where features and target match\ndf_dupli_f = df_train[df_train.drop(columns=[\"id\"]).duplicated(keep=\"first\")]\nprint(\"Number of duplicates: \", df_dupli_f.shape[0]) \nprint(\"Number of duplicates per class: \\n\", df_dupli_f.target.value_counts())\n# drop duplicates\ndf_train = df_train.drop(columns=[\"id\"]).drop_duplicates()\ndf_train.shape","2d12ab54":"# check for duplicates in the feature columns, only possible for training data\ndf_dupli_f = df_train[df_train.drop(columns=[\"target\"]).duplicated(keep=\"first\")].copy() \ndf_dupli_f[\"f_sum\"] = df_dupli_f.sum(axis=1, numeric_only=True)\n#display(df_dupli_f.sort_values(by=\"f_sum\")) # sort the df to find matching duplicates easier\nprint(\"Number of duplicates per class if first duplicate is kept: \\n\", df_dupli_f.target.value_counts())\n\ndf_dupli_l = df_train[df_train.drop(columns=[\"target\"]).duplicated(keep=\"last\")].copy()\ndf_dupli_l[\"f_sum\"] = df_dupli_l.sum(axis=1, numeric_only=True)\n#display(df_dupli_l.sort_values(by=\"f_sum\"))\nprint(\"Number of duplicates per class if last duplicate is kept: \\n\",df_dupli_l.target.value_counts())","bfaa396c":"#df_train.drop(columns=[\"target\"]).loc[131686] == df_train.drop(columns=[\"target\"]).loc[66469] # this is a feature dupliacte pair, row 131686 \u2014> Class_2 vs. row 66469 \u2014> Class_3","efed4ed7":"# look at test set only\ndf_test[df_test.drop(columns=[\"id\"]).duplicated(keep=\"first\")]","9e6edaaa":"# check for samples with identical features in train and test data\ndf_train_temp = df_train.drop(columns=\"target\").drop_duplicates()\ndf_test_temp = df_test.drop(columns=\"id\").drop_duplicates()\ndf_all_temp = df_train_temp.append(df_test_temp, ignore_index = True) ","57405d4c":"df_all_temp[df_all_temp.duplicated(keep=\"last\")] # this shows the rows from the training data, keep=\"last\" would show the rows from the testing data","a5c8a437":"# thanks to Maxim Kazantsev (@maximkazantsev) for this function! I adapted it slighty\ndef make_data_plots(df, i=0):\n    \"\"\"\n    Makes value distribution histogram plots for a given dataframe features\n    df should contain only the features to be plotted\n    \"\"\"\n    columns = df.columns.values\n\n    cols = 4\n    rows = (len(columns) - i) \/\/ cols + 1\n\n    fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=True)\n    \n    plt.subplots_adjust(hspace = 0.2)\n    for r in np.arange(0, rows, 1):\n        for c in np.arange(0, cols, 1):\n            if i >= len(columns):\n                axs[r, c].set_visible(False)\n            else:\n                axs[r, c].hist(df[columns[i]].values, bins = 30)\n                axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            i+=1\n            \n            \nmake_data_plots(df_train[feature_cols])","8144fe05":"make_data_plots(df_test[feature_cols])","8f23018c":"# let's check if there are as many unique feature values as the range of values\npd.options.display.max_rows = 75\ndf_features = df_all[feature_cols] # use df_all, df_test here depending on what you want to see\n\nfeature_range = df_features.max() - df_features.min()\nno_unique_values = df_features.nunique()\n\nunique_values = pd.DataFrame(data={\"feature_range\": feature_range, \"no_unique_values\": no_unique_values})","bcb9b1ea":"unique_values.plot(kind=\"barh\", figsize=(12,24), color=['tab:blue', 'tab:orange'])\nplt.show()","65cc983f":"# check how many % of feature values are 0\nzerolist = []\n\nfor col in feature_cols:\n    zeroperc = df_train[col].value_counts()[0]\/df_train.shape[0]\n    zerolist.append(zeroperc)\n    \nzeros = round(pd.Series(data=zerolist)*100,2)","1110baf6":"zeros.sort_values(ascending=False).plot(kind='bar', figsize=(20,6))\nplt.axhline(y=50)\nplt.title(\"Percentage of Zeros in each feature\")\nplt.xlabel(\"Feature Number\")\nplt.ylabel(\"%\")\nplt.annotate(zeros.max(),xy=(0,zeros.max()+2))\nplt.annotate(zeros.min(),xy=(73,zeros.min()+2))\nplt.show()","5cfb086e":"# choose feature for a closer look\ncurrent_feature = \"feature_17\"\ncurrent_df = df_train\nprint(current_feature)\n\nfig = plt.figure(figsize=fsize) # create figure\nfsize = (10,6)\nax0 = fig.add_subplot(2, 1, 1) # add subplot 1 (2 rows, 1columns, first plot)\nax1 = fig.add_subplot(2, 1, 2)\n#current_df[current_feature].hist(figsize=fsize, ax=ax0)\nsns.histplot(x=current_feature, data=current_df, ax=ax0) # just an alternative with sns instead of plt\nsns.boxplot(x=current_feature, data=current_df, ax=ax1)\nplt.show()\n\nprint(current_df[current_feature].value_counts())","24bf24ab":"df_train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv') # read again, because of dropped duplicates\ntarget_col = df_train.target # store the target column, it interfers with the calcluations below\ndf_train.drop(columns=\"target\", inplace=True)\nif \"id\" in df_train.columns.to_list(): # if the id is still present, remove it beause it screws computations\n    df_train.drop(columns=\"id\", inplace=True)","08e124ae":"rownumber = 3 # enter the row you want to analyse\npd.Series(data=df_train[feature_cols].loc[rownumber].values).plot(kind='bar', figsize=(16,6))\nplt.title(\"Sample {} Values\".format(rownumber))\nplt.show()","631ecd0e":"# get the number of entries that are not 0 in each row\nnumber_nz = np.count_nonzero(df_train, axis=1) #dont assign directly to df_train, this will inclued number_nz in sum_nz! (learning by mistakes...)\n# get the sum of the entered values in each row\nsum_nz = df_train.sum(axis=1, numeric_only=True) \ndf_train[\"number_nz\"] = number_nz \ndf_train[\"sum_nz\"] = sum_nz\ndf_train.tail()","8f34f5e2":"df_train.number_nz.value_counts()#.plot(kind='barh', figsize=(16,12))\n# ... there are 5422 rows where 27 features have entries (to be preceise: non zero entries)\n# ... there is 1 row where 65 features have entries\n# ... and there are 9 rows where no features have entries","e43959c1":"# add the target column back to check for target when all features are 0\ndf_train['target'] = target_col\ndf_train[df_train.number_nz == 0]","fb309ebc":"# repeat for test data\ndf_test.drop(columns=\"id\", inplace=True)\n# get the number of entries that are not 0 in each row\nnumber_nz = np.count_nonzero(df_test, axis=1) #dont assign directly to df_train, this will inclued number_nz in sum_nz! (learning by mistakes...)\n# get the sum of the entered values in each row\nsum_nz = df_test.sum(axis=1, numeric_only=True) \ndf_test[\"number_nz\"] = number_nz \ndf_test[\"sum_nz\"] = sum_nz\n# check for all zero samples\ndf_test[df_test.number_nz == 0]","aec7a7b4":"# predict like train set probabilites\nfor col in sample_submission.drop(columns=\"id\"):\n    sample_submission.loc[:,col] = target_distribution.percent.loc[col]\n\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","f01bf6c8":"# About\nThis notebook is my EDA for the Tabular Playground Series June 2021. The June competitions has quite some similarities to the May competition. For the May EDA please see my notebook [TPS5 - EDA raising more questions than answers](https:\/\/www.kaggle.com\/melanie7744\/tps5-eda-raising-more-questions-than-answers).\n\nHere is a summary of my findings, please find the details below. \n\n**The most obivous similarities are:**\n- multi-class classification problem\n- mulit-class log loss as evaluation metric\n- anonymized (obfuscated) features\n- most feature values are 0\n- there are no binary features\n\n**The differences lie in the details:**\n- 9 classes vs 4 in TPS5\n- 75 features vs 50 in TPS5\n- training and testing data are twice as big as in TPS5\n- the features do not have any negative values\n- there are much more \"feature duplicates\" than in TPS5\n\n\nIf you like my analysis, please upvote!\n\nLet's load the environment and look at training and testing data.","ccbd0abd":"In the training data there are 9 rows where all features are 0. These 9 rows have different target classes: 5x Class_2, 2x Class_6, 1x Class_5, 1x Class_3.\n\nThere are also 6 rows with 0 for all features in the test set. Ids: 221601, 232129, 245875, 250216, 284469, 295642. So at least train and test have the same properties... Although the number of zero features is a big higher than expected in the test set. \n\nSome further investigation made me believe that this is an artifact from making a synthetic dataset. So no use trying to make sense out of those rows, \"just\" decide how to deal with them.","1f82394d":"We can see here that all distributions are right skewed. Some very heavily.","3f8804fe":"# Feature Value Analysis","018f370b":"We can see here, that there are no missing values. The variable statistics are comparable between training and testing data. Most features are in a range from 0 to 100, with a few exceptions. The highest value of any feature is 352. This is much higher than the highest value in TPS5. Like in TPS5 0 is by far the most common value for any feature. ","c46d2f10":"The target variable is imbalanced with Class_6 and Class_8 sharing half of the values. ","0a9e67e4":"# Viz per Feature","2bce7d43":"# Data Overview","195f2324":"# Set Baseline","0e53c078":"# Duplicates","2ef8cb11":"Here we see a quite different picture than in TPS5. Now, in TPS6, there are no low cardinality features. There are many features with a seizable difference between their value range and their number of unique features. I am still puzzeld what to make out of this observation. Any ideas?","17399acb":"In the **training data** we have 106 true duplicates. That is rows where features and target match.\n\nBut we also have 118 \"feature duplicates\" here. That is, rows where the features are the same but the target variable is different! An example would be: \n- row 131686-> Class_2 vs. \n- row  66469-> Class_3\n\nAnd just by counting the target variable values for the feature dupliates it can be seen that they do not match. \n\nIn the **test set** we have 79 rows indicated as duplicates. Comparing them to the number of dupicates in the train set 106+118 = 224, their number is a bit lower than expected.\n\nIf we **combine the training and testing set** we have 101 rows with identical features. So the model gets the exact same data in the test set that it learned from in the training phase. Would be interesting to check exactly those predictions. \n\nWhile TPS5 with a low number of feature duplicates led me to just drop them, I will think twice if this is a good approach here, for TPS6.\n\nSome further investigation made me believe that this is an artifact from making a synthetic dataset. So no use trying to make sense out of those rows, \"just\" decide how to deal with them.","16cf3efa":"There feature with the highest number of zeros has 86.09% zeros in it. The feature with the lowest number of zeros has 28.97% of zeros in it. Only 10 features have less than 50% of zeros. ","8864ce89":"# Sample Analysis","b3a6624e":"# Target Variable Analysis","11220233":"# Feature Value Distributions"}}