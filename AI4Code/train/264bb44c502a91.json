{"cell_type":{"d7941960":"code","1d5cbe41":"code","5c3eae34":"code","1027b826":"code","d825a824":"code","bc060618":"code","81980b6b":"code","cdb46a9b":"code","09b68999":"code","5a7820bd":"code","4e67381b":"code","49fa9c50":"code","a1ec9379":"code","dbc9d849":"code","dbf94787":"code","029491e5":"code","b4677982":"code","5f38d9d0":"code","1c93f186":"code","d0307920":"code","605e53b0":"code","91ac86c8":"code","8e7dff87":"markdown","6dc70244":"markdown","980ebc01":"markdown","5c3ddc9f":"markdown","0864e98a":"markdown","6713b259":"markdown","b6ce041e":"markdown","3180a43e":"markdown","0994f73f":"markdown","19cc74e6":"markdown","c35c0c75":"markdown"},"source":{"d7941960":"import pandas as pd\nimport numpy as np\nimport datetime\nimport random\nimport time\nimport os\nimport gc\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, confusion_matrix, classification_report\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import mode, skew, kurtosis\n\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#----------\npd.options.display.max_rows = 50\npd.options.display.max_columns = 50\n\nimport warnings\nwarnings.simplefilter('ignore')","1d5cbe41":"tf.__version__, tfa.__version__","5c3eae34":"CFG = {\n    'target': 'target',\n    'n_class': 9,\n    'lr': 1e-4,\n    'batch_size': 256,\n    'epochs': 50,\n    'verbose': 1,\n    'patience': 5,\n    'n_splits': 10,\n    'seed': 2021\n}","1027b826":"def seed_everything(seed=2021):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(CFG['seed'])","d825a824":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\n\nall_df = pd.concat([train, test]).reset_index(drop=True)","bc060618":"cat_features = [col for col in all_df.columns if 'feature_' in col]\ncnt_features = []","81980b6b":"all_df['mean'] = np.mean(all_df[cat_features], axis=1)\nall_df['std'] = np.std(all_df[cat_features], axis=1)\nall_df['skew'] = skew(all_df[cat_features], axis=1)\nall_df['kurtosis'] = kurtosis(all_df[cat_features], axis=1)\ncnt_features += ['mean', 'std', 'skew', 'kurtosis']","cdb46a9b":"scaler = MinMaxScaler()\nall_df[cnt_features] = scaler.fit_transform(all_df[cnt_features])","09b68999":"km = KMeans(n_clusters=CFG['n_class']*2, random_state=CFG['seed'], n_jobs=-1)\nall_df['cluster'] = km.fit_predict(all_df[cat_features])\ncat_features += ['cluster']","5a7820bd":"all_features = cat_features + cnt_features","4e67381b":"train_npy = all_df.iloc[:train.shape[0]][all_features].to_numpy()\ntest_npy = all_df.iloc[train.shape[0]:][all_features].to_numpy()\ntarget = train[CFG['target']].apply(lambda x: int(x.split(\"_\")[-1])-1).to_numpy()","49fa9c50":"plt.figure(figsize=(16, 4))\n\nfor pos in range(10):\n    i = random.sample(list(range(train.shape[0])), 1)\n    \n    plt.tight_layout()\n    plt.subplot(2, 5, pos+1)\n    plt.plot(train_npy[i[0]].reshape(-1))\n    plt.title(f\"row {i[0]}\")","a1ec9379":"plt.figure(figsize=(16, 4))\n\nfor pos in range(10):\n    i = random.sample(list(range(test.shape[0])), 1)\n\n    plt.tight_layout()\n    plt.subplot(2, 5, pos+1)\n    plt.plot(test_npy[i[0]].reshape(-1))\n    plt.title(f\"row {i[0]}\")","dbc9d849":"plt.figure(figsize=(16, 2))\nplt.hist(target, bins=CFG['n_class'])\nplt.title(\"Target distribution\")\nplt.xlabel('Label')\nplt.ylabel('freq #')","dbf94787":"X_train = train_npy.copy()\nX_test = test_npy.copy()\ny_train = tf.keras.utils.to_categorical(target, num_classes=CFG['n_class'])","029491e5":"def create_model(cat_shape=(76, ), cnt_shape=(4, )):\n    cat_input = tf.keras.layers.Input(shape=cat_shape, name='cat_input')\n    cnt_input = tf.keras.layers.Input(shape=cnt_shape, name='cnt_input')\n\n    x = tf.keras.layers.Embedding(1024, 16, name='embedding_1')(cat_input)\n    x1 = tf.keras.layers.Flatten(name='flatten')(x)\n  \n    x = tf.keras.layers.Dense(32, activation='relu', name='dense_1')(cnt_input)\n    x2 = tf.keras.layers.Dropout(0.2, name='dropout_1')(x)\n\n    x = tf.keras.layers.Concatenate(axis=1)([x1, x2])\n    \n    x = tf.keras.layers.Dropout(0.4, name='dropout_2')(x)\n    x = tf.keras.layers.Dense(128, activation='relu', name='dense_2')(x)\n    x = tf.keras.layers.Dense(64, activation='relu', name='dense_3')(x)\n    x = tf.keras.layers.Dense(32, activation='relu', name='dense_4')(x)\n    outputs = tf.keras.layers.Dense(CFG['n_class'], activation='softmax', name='output')(x)\n    \n    model = tf.keras.Model([cat_input, cnt_input], outputs)\n\n    metrics = tf.keras.metrics.CategoricalCrossentropy(\n        from_logits=False,\n        label_smoothing=0,\n        name='categorical_crossentropy'\n    )\n    loss = tf.keras.losses.CategoricalCrossentropy(\n                from_logits=False,\n                label_smoothing=0,\n                reduction='auto',\n                name='categorical_crossentropy'\n    )\n    optimizer = tfa.optimizers.AdamW(\n        weight_decay=1e-7,\n        learning_rate=CFG['lr'],\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-07,\n        amsgrad=True,\n        name='AdamW',\n    )\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    \n    return model\n\ncreate_model().summary()","b4677982":"scheduler_cb = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=2,\n    verbose=0,\n    mode='auto',\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0\n)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=CFG['patience'],\n    verbose=1,\n    mode='auto',\n    baseline=None,\n    restore_best_weights=True\n)\n","5f38d9d0":"kf = StratifiedKFold(n_splits=CFG['n_splits'], shuffle=True, random_state=CFG['seed'])\nhistory = []\n\nnn_oof = np.zeros((X_train.shape[0], CFG['n_class']))\nnn_pred = 0\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=X_train, y=target)):\n    print(f\"===== FOLD {fold} =====\")\n    X_tr, y_tr = X_train[trn_idx], y_train[trn_idx]\n    X_va, y_va = X_train[val_idx], y_train[val_idx]\n\n    start = time.time()\n    K.clear_session()\n    \n    model = create_model(cat_shape=X_tr[:, :len(cat_features)].shape[1], cnt_shape=X_tr[:, len(cat_features):].shape[1])\n    \n    log_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n    history.append(\n        model.fit(\n            x=[X_tr[:, :len(cat_features)], X_tr[:, len(cat_features):]],\n            y=y_tr,\n            batch_size=CFG['batch_size'],\n            epochs=CFG['epochs'],\n            validation_data=([X_va[:, :len(cat_features)], X_va[:, len(cat_features):]],\n                             y_va),\n            callbacks=[scheduler_cb, early_stopping_cb, tensorboard_cb],\n            verbose=CFG['verbose']\n        )\n    )\n    \n    nn_oof[val_idx] = model.predict([X_va[:, :len(cat_features)], X_va[:, len(cat_features):]])\n    nn_pred += model.predict([X_test[:, :len(cat_features)], X_test[:, len(cat_features):]]) \/ CFG['n_splits']\n    \n    nn_logloss = log_loss(target[val_idx], nn_oof[val_idx])\n    min_loss = min(history[fold].history['loss'])\n    min_val_loss = min(history[fold].history['val_loss'])\n    print(f\"FOLD {fold:d}: logloss score {nn_logloss:.6f} (train loss={min_loss:.6f}, validation loss={min_val_loss:.6f})\")\n    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n    \n    del model\n    _ = gc.collect()\n\nnn_logloss = log_loss(target, nn_oof)\nprint(f\"logloss score {nn_logloss}\")","1c93f186":"submission.iloc[:, 1:] = nn_pred\nsubmission.to_csv(\"submission.csv\", index=False)","d0307920":"plt.figure(figsize=(16, 8), tight_layout=True)\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    plt.title(f\"Class_{i+1}\")\n    submission[f'Class_{i+1}'].hist(bins=int(submission.shape[0]\/1000))","605e53b0":"cm = confusion_matrix(target, nn_oof.argmax(axis=1))\n\nplt.figure(figsize=((16,8)))\nsns.heatmap(cm, annot=True, fmt='5d', cmap='Blues')\nplt.savefig(\"confusion_matrix.png\")","91ac86c8":"print(classification_report(target, nn_oof.argmax(axis=1), digits=4))","8e7dff87":"# Combining discrete and continuous features in neural networks\n---\n\n## Reference\n* [Simple Keras embedding in 10 folds](https:\/\/www.kaggle.com\/pourchot\/simple-keras-embedding-in-10-folds) by [@pourchot](https:\/\/www.kaggle.com\/pourchot)","6dc70244":"## Submission","980ebc01":"## Libraries","5c3ddc9f":"## Training","0864e98a":"## Create CNN models","6713b259":"### Check target distribution","b6ce041e":"### Check train data","3180a43e":"## Load and check data","0994f73f":"## Configuration","19cc74e6":"## Check results","c35c0c75":"### Check test data"}}