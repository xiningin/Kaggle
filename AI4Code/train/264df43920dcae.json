{"cell_type":{"ab05938a":"code","0a0758a6":"code","2702fe89":"code","1bbf741a":"code","6510417b":"code","cf282a14":"code","649b5622":"code","d99a37a9":"code","3d8136e6":"code","5623cbdc":"code","cd2ed04a":"code","0c8ffc8a":"code","bf911571":"code","34d06350":"code","ca17a151":"code","459833bc":"code","8e3df2d9":"code","26bced18":"code","22bc6538":"code","0e959610":"code","e014727a":"code","375b6baa":"code","f2df1d08":"code","4caae6cd":"code","8afb11d1":"code","239acb83":"code","dd623b0f":"code","1ae5de7a":"markdown","4f56f04b":"markdown","018c2596":"markdown","d5c052dd":"markdown","afd6615e":"markdown","192c6f09":"markdown","f052eae5":"markdown","a569b2b8":"markdown","9eef8da6":"markdown","4d63bf05":"markdown","e744aed7":"markdown","d5af5f8c":"markdown","e19523eb":"markdown","7b78992a":"markdown","dc568b12":"markdown","1db53a22":"markdown","f6c84198":"markdown","beaf9c98":"markdown","9236c9ba":"markdown","53d94bff":"markdown","3f55fa06":"markdown","eeef9911":"markdown","c0d7c52d":"markdown","34e8f5f0":"markdown","e5cdcfc6":"markdown","39b15ef5":"markdown","4eb26ae8":"markdown","4f5a43ca":"markdown","c0884ac9":"markdown","54772b2b":"markdown","b169c3b6":"markdown","17cdf5c1":"markdown","498eb181":"markdown","7296db8f":"markdown","10c4b9c0":"markdown","dd3056cb":"markdown","6bc1a828":"markdown","beb5dfc1":"markdown","56bb234e":"markdown","f100cd34":"markdown","d8910294":"markdown","e122cccf":"markdown","6cce1d1f":"markdown","d84dba42":"markdown","944a50f0":"markdown","de6a1dd7":"markdown"},"source":{"ab05938a":"!conda install -c anaconda swig dask[distributed] --yes\n!pip install deap update_checker tqdm stopit xgboost\n!pip install tpot\n!pip install auto-sklearn\n!pip install 'ray[default]'\n!pip install scikit-optimize\n!pip install scipy==1.7.0","0a0758a6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, PolynomialFeatures\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.model_selection import ShuffleSplit, RepeatedKFold, cross_val_score, GridSearchCV, cross_val_predict\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, ElasticNet, SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import explained_variance_score\nimport warnings\nfrom scipy.linalg import LinAlgWarning\nfrom sklearn.exceptions import ConvergenceWarning\nimport autosklearn.regression\nfrom tpot import TPOTRegressor","2702fe89":"dataset = 'https:\/\/raw.githubusercontent.com\/stedy\/Machine-Learning-with-R-datasets\/master\/insurance.csv'","1bbf741a":"df = pd.read_csv(dataset)","6510417b":"print(df.head())","cf282a14":"print(df.shape)","649b5622":"print(df.info())","d99a37a9":"df.columns = df.columns.str.capitalize()","3d8136e6":"num_cols = df.select_dtypes('number').columns.tolist()\ncat_cols = df.select_dtypes('object').columns.tolist()","5623cbdc":"oe = OrdinalEncoder()\nsc = StandardScaler()\ndf2 = df.iloc[:,:-1]\nnum_features = num_cols[:-1]\ndf2[cat_cols] = oe.fit_transform(df2[cat_cols])\ndf2[num_features] = sc.fit_transform(df2[num_features])","cd2ed04a":"features = df2.columns\ntarget = df.columns[-1]\nX = df2\nY = df[target]\nss = ShuffleSplit(n_splits=1, test_size=.2, random_state=0)\ntrain_indecies = list(ss.split(X,y=Y))\ntrain_index, test_index = train_indecies[0][0], train_indecies[0][1]\nX_train, X_test = X.loc[train_index], X.loc[test_index]\ny_train, y_test = Y.loc[train_index], Y.loc[test_index]","0c8ffc8a":"print(df.describe())","bf911571":"print(df[cat_cols].describe().T)","34d06350":"df['Age'].hist(bins=5);","ca17a151":"x = df['Age']\ny = df['Charges']\nplt.scatter(x,y)\nplt.xlabel('Age')\nplt.ylabel('Charges');","459833bc":"sns.countplot(data=df, x='Region',order=df['Region'].value_counts().index);","8e3df2d9":"sns.boxplot(y=df['Region'], x=df['Charges'],order=df['Region'].value_counts().index);","26bced18":"sns.pairplot(df[num_cols], plot_kws=dict(alpha=.1, edgecolor='none'));","22bc6538":"sns.heatmap(df[num_cols]);","0e959610":"corr = df.corr()\nmask = np.triu(corr)\nsns.heatmap(corr, mask=mask, cmap='Wistia', center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5},  annot= True);","e014727a":"fe = ExtraTreesRegressor(n_estimators=10)\nfe.fit(X, Y)\nfedf = pd.DataFrame({'Feature':features,'Feature_importance %' : fe.feature_importances_ * 100})\nfedf = fedf.sort_values(by=['Feature_importance %'], ascending=False)\nprint(fedf)\nfedf.plot.bar(x='Feature',y='Feature_importance %');","375b6baa":"#training the models\nwarnings.filterwarnings('ignore', category=LinAlgWarning)\nwarnings.filterwarnings('ignore', category=ConvergenceWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('Polynomial', LinearRegression()))\nmodels.append(('Regularization', ElasticNet()))\nmodels.append(('SGD', SGDRegressor()))\nmodels.append(('DT', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('KNN', KNeighborsRegressor()))\nresults = []\nnames = []\nfor name, model in models:\n    if name == 'Polynomial':\n        evss = []\n        degrees = np.arange(1, 10)\n        max_evs, min_deg = 1 , 0\n        for deg in degrees:\n            poly_features = PolynomialFeatures(degree=deg, include_bias=False)\n            x_poly_train = poly_features.fit_transform(X_train)\n            poly_reg = model\n            poly_reg.fit(x_poly_train, y_train)\n            x_poly_test = poly_features.fit_transform(X_test)\n            poly_predict = poly_reg.predict(x_poly_test)\n            poly_evs = explained_variance_score(y_test, poly_predict)\n            evss.append(poly_evs)\n            if max_evs > poly_evs:\n                max_evs = poly_evs\n                min_deg = deg\n                \n        poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\n        x_poly_train = poly_features.fit_transform(X_train)\n        pipeline_p = Pipeline(steps=[('regressor',model)])\n        rkfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n        cv_results_p = cross_val_score(pipeline, x_poly_train, y_train, cv=rkfold , scoring='explained_variance')\n        results.append(cv_results.mean())\n        names.append(name)\n    elif name == 'Regularization':\n        params = {'normalize':[True, False], 'selection':['cyclic', 'random'],\n                  'l1_ratio':np.arange(0, 1, 0.01), 'alpha':np.logspace(-4, 0, 100)}\n        rkfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n        search_r = GridSearchCV(model, params, scoring='explained_variance', cv=rkfold, n_jobs=-1)\n        search_r.fit(X_train, y_train)\n        results.append(search_r.best_score_)\n        names.append(name)\n    elif name == 'KNN':\n        weights = ['uniform', 'distance']\n        algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n        leaf_size = list(range(1,50))\n        n_neighbors = list(range(1,30))\n        p=[1,2]\n        hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p, algorithm=algorithm, weights=weights)\n        rkfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n        search_k = GridSearchCV(model, hyperparameters, scoring='explained_variance', cv=rkfold, n_jobs=-1)\n        search_k.fit(X_train, y_train)\n        results.append(search_k.best_score_)\n        names.append(name)\n        #dfgrid = pd.DataFrame(search_k.cv_results_)\n    else:\n        pipeline = Pipeline(steps=[('regressor',model)])\n        rkfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n        cv_results = cross_val_score(pipeline, X_train, y_train, cv=rkfold , scoring='explained_variance')\n        results.append(cv_results.mean())\n        names.append(name)","f2df1d08":"# Compare Algorithms\nresults_df = pd.DataFrame({'Regressor': names, 'Explained_Variance': results})\nresults_df = results_df.sort_values(by=['Explained_Variance'], ascending=False)\nprint(results_df)\nresults_df.plot.bar(x='Regressor',y='Explained_Variance');\nplt.title('Algorithm Comparison');","4caae6cd":"rkfold_rf = RepeatedKFold(n_splits=3, n_repeats=3, random_state=1)\ntuned_parameters = {'max_depth': [x for x in range(1, 8)] + [None],\n    'max_features': [x for x in range(1, X_train.shape[1])],\n    'min_samples_split': np.linspace(0.1, 1.0, 10),\n    'n_estimators': [x for x in range(1, 100)]}\nsearch_rf = GridSearchCV(RandomForestRegressor(), tuned_parameters, scoring='explained_variance', cv=rkfold_rf, n_jobs=-1)\nsearch_rf.fit(X_train, y_train)\ntesting_predictions = search_rf.predict(X_test)\ntest_accuracy = explained_variance_score(y_test,testing_predictions)\nprint(\"Test-predictions accuracy: \",test_accuracy)","8afb11d1":"automl = autosklearn.regression.AutoSklearnRegressor(\n    n_jobs=4,\n    tmp_folder='\/tmp\/autosklearn_regression_example_tmp',\n)\nautoml.fit(X_train, y_train, dataset_name='insurance')\n#print(automl.leaderboard())\n#print(automl.show_models())\ntrain_predictions = automl.predict(X_train)\nasd = {'Regressor': 'AutoSklearn', 'Explained_Variance': explained_variance_score(y_train, train_predictions)}\n#print(\"Train Explained_Variance_Score:\", as['Explained_Variance'])\n#print(\"Test Explained_Variance_Score:\", sklearn.metrics.explained_variance_score (y_test, test_predictions))","239acb83":"rkfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\ntpot = TPOTRegressor(n_jobs=-1, generations=10, population_size=10, offspring_size=10, verbosity=0, cv=rkfold, scoring='explained_variance',random_state=1)\ntpot.fit(X_train, y_train)\ntpd = {'Regressor': 'TPOT', 'Explained_Variance': tpot.score(X_test, y_test)}\n#print('Best Pipeline Score= ',tp['Explained_Variance'])","dd623b0f":"results_df = results_df.append(asd, ignore_index=True)\nresults_df = results_df.append(tpd, ignore_index=True)\nresults_df = results_df.sort_values(by=['Explained_Variance'], ascending=False)\nprint(results_df)\nresults_df.plot.bar(x='Regressor',y='Explained_Variance');\nplt.title('Algorithm Comparison');","1ae5de7a":"train regression models on dataset and choosing the best model based on accuracy","4f56f04b":"heatmap of numerical features","018c2596":"number of candidates in each region","d5c052dd":"correlation between Age and Charges","afd6615e":"**Suggestions for next steps in analyzing this data, which may highlight possible flaws in the model and a plan of action to revisit this analysis with additional data or different predictive modeling techniques to achieve a better explanation or a better prediction**","192c6f09":"**Brief description of the data set you chose and a summary of its attributes**","f052eae5":"the default configuration for automated machine learning give better results than the model that manually selected and modified","a569b2b8":"## Load the dataset","9eef8da6":"Summary Statistics for Categorical columns","4d63bf05":"* being Smoker is main factor in deciding the insurance charges for the candidate\n* the Random Forest Regressor gave the best results even without GridSearch for the training set, so the model had to be optimized further for the prediction to give the best results possible\n* Polynomial, Regularization, KNN have been tweaked to provide best results on training set (finding best degree, gridsearch of parameters, best k) but stil they produce less accurate results","e744aed7":"Visual Exploration of Categorical columns","d5af5f8c":"Feature Importance","e19523eb":"**Plan for Data Exploration, Feature Engineering and Modelling**","7b78992a":"number of rows and coulmns in dataset","dc568b12":"**A paragraph explaining which of your regressions you recommend as a final model that best fits your needs in terms of accuracy and explainability.**","1db53a22":"Capitalize column names","f6c84198":"sampling the data","beaf9c98":"1. tpot\n2. auto-sklearn\n3. scipy","9236c9ba":"The steps in solving the Regression Problem are as follows:\n1. Packages to be installed\n2. Load the libraries\n3. Load the dataset\n4. General information about the dataset\n5. Exploratory Data Analysis (EDA)\n6. Modeling\n7. Recommendations","53d94bff":"distribution of Age for candidates","3f55fa06":"1. numpy\n2. pandas\n3. matplotlib\n4. seaborn\n5. sklearn\n6. autosklearn\n7. tpot","eeef9911":"dataset information","c0d7c52d":"**Summary Key Findings and Insights, which walks your reader through the main drivers of your model and insights from your data derived from your linear regression model.**","34e8f5f0":"## Recommendations","e5cdcfc6":"Classifying columns as Numerical or Categorical","39b15ef5":" Split the data into test and train","4eb26ae8":"## Load the libraries","4f5a43ca":"**Summary of training at least three linear regression models which should be variations that cover using a simple linear regression as a baseline, adding polynomial effects, and using a regularization regression. Preferably, all use the same training and test splits, or the same cross-validation method.**","c0884ac9":"using automated machine learning yield better results than manual or gridseached models\n\nfor this dataset will use auto-sklearn and TPOT and compared thier results to results obtained before","54772b2b":"correlation plot of numerical features","b169c3b6":"## Exploratory Data Analysis (EDA)","17cdf5c1":"## Packages to be installed","498eb181":"location of dataset","7296db8f":"Pair plot of numerical features","10c4b9c0":"**Main objective of the analysis that specifies whether your model will be focused on prediction or interpretation.**","dd3056cb":"regression models used for the training dataset and the results\n1. LinearRegression\n2. Polynomial\n3. Regularization(ElasticNet)\n4. SGD\n5. Decision Tree \n6. Random Forest\n7. KNN","6bc1a828":"Features Encoding","beb5dfc1":"reading the dataset into dataframe","56bb234e":"**Actions taken for data cleaning and feature engineering**","f100cd34":"Summary Statistics for Numerical columns","d8910294":"Random Forest Regressor have the best score of all regression models for the training sets\n\nso it will be chosen to make the prediction on the test set","e122cccf":"The data contains medical information and costs billed by health insurance companies. It contains 1338 rows of data and the following columns: age, gender, BMI, children, smoker, region, insurance charges.\n\n| S No. | Column | Description| Data Type | Category|\n| --- | --- | --- | --- | --- |\n|1 | Age | age of primary beneficiary | Int | Discrete |\n|2 | Sex | insurance contractor gender, female, male | String | Nominal |\n|3 | BMI | Body mass index, providing an understanding of body, weights that are relatively high or low relative to height | Float | Continuous |\n|4 |Children | Number of children covered by health insurance \/ Number of dependents | Int | Discrete |\n|5 | Smoker | Smoking status of contractor, yes, no | String | Nominal |\n|6 | Region | the beneficiary's residential area in the US, northeast, southeast, southwest, northwest. | String | Nominal |\n|7 | Charges | Individual medical costs billed by health insurance| Float | Continuous |","6cce1d1f":"Visual Exploration of Numerical Columns","d84dba42":"## Modeling","944a50f0":"## General information about the dataset","de6a1dd7":"distribution of charges for each region"}}